\section{Related Work}

Post-hoc explanation methods have attracted significant attention in NLP research in recent years.
\citet{ribeiro2016should} proposed LIME, which generates explanations by approximating the original model with a local sample and highlights the most important features.~\citet{guidotti2018local} extended it with a decision tree classifier to approximate deep models. However, they cannot guarantee that the approximations are accurate representations of the original model due to inherent limitations of decision trees.~\citet{thorne2019generating} generate concepts of classifiers operating on pairs of sentences, while~\citet{yu2022towards} generate \emph{aspects} as explanations for search results.~\citet{kumar-talukdar-2020-nile} used positive labels to generate candidate explanations, while~\citet{chen2021kace} used contrastive examples in the form of ``why A not B'' to distinguish between confusing candidates.  Different from prior work, we integrate reasoning features and concepts into \methodname{} to explain LM behaviors.






Recently, language models (LMs) such as RoBERTa~\citep{liu2019roberta}, Llama~\citep{touvron2023llama} and GPT-4~\citep{OpenAI2023GPT4TR}  have achieved impressive results.
However, these models lack interpretability, which can hinder their adoption in mission critical real-world applications.
Previous interpretable frameworks \citep{ribeiro2016should,mukund2017integratedgrad,Smilkov2017SmoothGradRN,ding2021saliencyNLP,swamy2021interpreting} can be applied to LMs. However, they often rely on approximations and simplifications of the original models, which can result in discrepancies between the model behaviours and the explanations.
In contrast, \methodname{} explains LMs by illustrating the model reasoning process.

KGs are increasingly adopted as a means to improve the interpretability and explainability of LMs \citep{huang-etal-2022-deer, yasunaga2021qagnn, huang2019explainable, liu2019knowledge}. KGs are structured representations of knowledge, and can be used to capture complex semantic relationships that are difficult to represent in traditional LMs \citep{ji2021survey}.
\cite{ZHAN2022107612} retrieves explainable reasoning paths from a KG and uses path features to predict the answers.
\cite{yasunaga2021qagnn} integrates the KG into the model, enabling the model to reason over structured knowledge and generate more interpretable predictions.
However, these explanations can be inconsistent and accurate representations of the model reasoning process. In addition, they are difficult for humans to understand as they are being represented in a graph-based format.
By drawing upon insights from prior works, \methodname{} employs graph embedding to generate explanations to address these limitations.