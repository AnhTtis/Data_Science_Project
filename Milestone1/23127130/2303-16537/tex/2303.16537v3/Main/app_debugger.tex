\section{LM Debugger} \label{app:lm_debugger}


The LM Debugger is an important component of the \methodname{}, designed to simulate the reasoning process of a ``perfect'' LM using the generated explanations. By treating the explanations as a representation of the LM's decision-making process, the debugger evaluates the quality of these explanations across four key dimensions: faithfulness, completeness, minimality, and accuracy. Additionally, it offers advice for improving the actual LM's performance based on the simulated reasoning process.


\subsection{Evaluation Dimensions}

\paragraph{Faithfulness} The LM Debugger evaluates whether the explanation faithfully represents the simulated LM's reasoning process. It checks if the provided rationale aligns with the data-driven and algorithmic processes an simulated LM would use to arrive at its conclusions. This evaluation ensures that the explanation accurately reflects the expected decision-making process of a simulated LM, serving as a benchmark for the actual LM's performance.

\paragraph{Completeness} The LM debugger determines whether the explanation fully captures the breadth of data that an simulated LM would leverage to make a decision. It ensures that no significant computational strategies or data insights that a simulated LM would rely on are omitted. A complete explanation should provide a comprehensive understanding of the simulated LM's decision-making process, including all relevant factors and considerations.

\paragraph{Minimality} The LM Debugger verifies that the explanation includes only the essential computational processes or data insights that an simulated LM would utilize, without unnecessary elaboration or speculative reasoning beyond the expected operational framework. This evaluation helps to maintain the reasoning's conciseness and relevance, focusing on the key factors that directly influence the simulated LM's decision-making process.

\paragraph{Accuracy} The LM Debugger acts as a simulated "perfect" LM, using the generated explanations to assess how well they align with the reasoning process and capabilities of an simulated LM. It evaluates the accuracy of the explanations against the simulated LM's standard. 

\subsection{Advice for Improvement}

Based on the evaluation results, the LM Debugger suggests improvements focusing on enhancing the actual LM's training data diversity, algorithmic transparency, or decision-making accuracy to better align with the simulated ``prefect'' LM. These suggestions aim to mitigate biases and increase the LM's reliability and trustworthiness. 

\subsection{Instruction for LM Debugger}


\begin{figure}[h]
\centering
\begin{tcolorbox}[colback=softGray, colframe=deepBlue, title=Instruction for LM Debugger, fontupper=\fontsize{8pt}{1pt}\selectfont,]

\textbf{System Prompt:} Evaluate the explanation provided for the LMâ€˜s decision-making process in answering the given question. Assess the explanation across four dimensions: faithfulness, completeness, minimality, and accuracy. Assume the role of an LM debugger with expertise in the inner workings of \highlight{[MODEL\_NAME]} and a 100\% accurate \highlight{[MODEL\_NAME]}.

\textbf{Explanation Content:} Given a LM augmented with a graph attention network to extract key reasoning elements for decision-making. The task is \highlight{[TASK\_TYPE]}. The question is \highlight{[$q$]}. The Answer Options are: \highlight{[$\mathcal{A}$]}. The LM's prediction is \highlight{[$y'$]}. Top-ranked reason-elements are \highlight{[$\mathcal{Q}$]}. Explanation:
\highlight{[$E$]}. 

\textbf{Evaluation Criteria:} 

- Faithfulness: Assess if the explanation truly represents the LM's underlying computational and statistical mechanisms. Check if the rationale provided mirrors the data-driven and algorithmic processes the LM uses to arrive at its conclusions.
% Does the explanation accurately reflect the model's reasoning process? Consider if the explanation could independently arrive at the same conclusion as the whole model.

- Completeness: Determine whether the explanation fully captures the breadth of data and algorithms the LM leverages to make a decision. Ensure no significant computational strategies or data insights that the LM relies on are omitted.
% Evaluate whether the explanation encompasses all the elements used by the model to perform the task. Are any critical reasoning elements missing from the explanation?

- Minimality: Verify that the explanation includes only the essential computational processes and data insights the LM utilizes, without unnecessary elaboration or speculative reasoning beyond the LM's actual operational framework.
% Assess if the explanation contains only the necessary elements relevant to the task. Identify any nodes or parts of the explanation that are irrelevant or redundant in justifying the model's decision.

- Accuracy: Evaluate the precision with which the explanation reflects the LM's true capabilities and decision-making process, taking into account the LM's design, training data, and algorithmic functions.
% Based on the results of faithfulness, completeness, and minimality, assess the accuracy of the answer.

\textbf{Advice Instruction:} 
Suggest improvements focusing on enhancing the LM's training data diversity, algorithmic transparency, or decision-making accuracy, to mitigate biases and increase the model's reliability and trustworthiness in varied contexts.

\textbf{Debugging Instruction:}
Please provide a score from 1 to 5 for each dimension, with 1 being the lowest (poor, equivalent to \highlight{[MODEL\_NAME]} having 0\% accuracy) and 5 being the highest (excellent, equivalent to \highlight{[MODEL\_NAME]} having 100\% accuracy). Highlight specific areas where the explanation aligns well or falls short of the evaluation criteria. Your response should be short and concise.



\end{tcolorbox}
\caption{The instructions for LM Debugger.}
\end{figure}

\subsection{Evaluation of Effectiveness}

We demonstrate the effectiveness of the LM Debugger in helping users determine when to trust the model's predictions. We set a threshold score of 3 and focus on the accuracy-related dimensions, specifically ``Faithfulnes'' and ``Accuracy'', which serve as guides for assessing the LM's prediction accuracy.
Figure \ref{fig:lm_debugger} presents the percentage of cases where the LM Debugger correctly identifies the reliability of the model's predictions based on the explanation. The results show that our debugging process effectively assesses the dependability of the model's predictions based on the \methodname{} generated explanation, achieving 100\% accuracy in evaluating faithfulness and accuracy for both correct and incorrect predictions.



\begin{figure}[htbp]
    \centering
        \includegraphics[width=0.4\textwidth]{figure/two_dimensions_chart.pdf}
        \caption{LM Debugger's accuracy in identifying the reliability of the model's predictions based on explanation quality.}
        \label{fig:lm_debugger}
\end{figure}
