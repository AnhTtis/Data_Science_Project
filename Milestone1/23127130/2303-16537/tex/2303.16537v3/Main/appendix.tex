\section{Algorithms}

We present the details of two fundamental algorithms of \methodname{}. Algorithm ``Construct Element-graph'' shows  the procedure for generating a pruned element-graph \(G_e\) from the input content \(z\). It utilizes LM and ConceptNet to extract relevant nodes, evaluates them to compute a pruning score, and selects the top \(K\) nodes to form \(G_e\).

Algorithm ``Element-graph Interpretation'' processes the pruned element-graph \(G_e\) to identify essential ``Reason-elements'' that enhancing the understanding of \(z\). It uses a graph attention network, which utilizes attention to analyze node interactions, updates node features, and calculates the probability of selecting specific outcomes. This process results in the identification and ranking of nodes to highlight important ``Reason-elements.''


\begin{algorithm} [H]
\DontPrintSemicolon
\KwData{Input content $z$}
\KwResult{Pruned element-graph $G_e$}
\caption{Construct Element-graph}
\label{algo:subgraph}

\Begin{
    $G_k \gets \text{ExtractFromConceptNet}(z)$ \\ \tcp*[r]{Extract the $L$-hop neighbor from ConceptNet}
    \For{each node $v_e$ in $G_k$}{
        % $v_{\text{embedding}} \gets f_{\text{enc}}(v)$ \tcp*[r]{Embedding function of $f_{LM}$} 
     %   $v_{\text{embedding}} \gets z_{emb} \mathbin\Vert v_{emb}$ \tcp*[r]{Concat text embeddings}
        $v_{\text{score}} \gets f_{prob}(z_{emb},v_{emb})$ \tcp*[r]{Compute score for pruning}
    }
    $G_e \gets \text{SelectTopK}(G_k)$ \tcp*[r]{Prune based on top $K$ scores}
    \Return{$G_e$}\;
}
\end{algorithm}


\begin{algorithm}[H]
\DontPrintSemicolon
\KwData{Element-graph \( G_e \)  containing node type embedding \(u_i\) and relation embedding \(r_{ij}\), input \(z\).}
\KwResult{\emph{Reason-elements}}
\caption{Element-graph Interpretation}
\label{algo:element}

\Begin{
        \For{each attention layer \( k \) in graph reasoning network}{
        \For{each node \( v_i \) in \( G_e \)}{
      \(\alpha_{ij}  \gets  \frac{\exp(a(h_{ki},h_{kj},u_i,r_{ij}))}{\sum \limits_{v_l \in \mathcal{N}_i } \exp(a(h_{ki},h_{kl}))}\) \tcp*[r]{Compute attention coefficient \(\alpha_{ij}\)}
     \(  h_{k+1,i}  \gets  f_{\delta}\left(\sum\limits_{v_j\in \mathcal{N}_i}   \alpha_{ij} m(h_{kj},u_i,r_{ij}) \right) + h_{ki} \) \tcp*[r]{Update node feature}
      }
    }
  %  \( \Lambda \gets \text{AttentionPooling}(G_e) \) \tcp*[r]{Attention-based pooling}
   % \( \mathbb{H}^{itp} \gets h_{v_i}^k \parallel \Lambda,\; \mathbb{H}^{LM} \gets f_{LM}(z) \) \tcp*[r]{Forming \(\mathbb{H}^{itp}\) and \(\mathbb{H}^{LM}\)}
  %  \( P(a|q) \propto \exp(\text{MLP}(\mathbb{H}^{LM}, \mathbb{H}^{itp})) \) \tcp*[r]{Probability of choosing an answer}
 \(\mathbb{H}^{LM} \gets f_{LM}(z) \) \tcp*[r]{Forming  \(\mathbb{H}^{LM}\)}
 \( P(y'|q) \propto \exp(\text{MLP}(\mathbb{H}^{LM}, \boldsymbol{h}_K,\boldsymbol{\alpha}_K)) \) \tcp*[r]{Probability of choosing an answer}
 \( \text{ReasonElements} \gets \text{RankNode}(G_e, \boldsymbol{\alpha}_K)) \) \tcp*[r]{Rank nodes based on the attentions}
    

    
    \Return{ReasonElements}
}
\end{algorithm}

\input{Main/app_settings}

\section{Instruction for Explanation Generation}

\input{Main/case}

\input{Main/app_debugger}

\section{Details of Element-graph} \label{sec:graph}
Due to space constraints in the main text, we provide a comprehensive description of the node and relations types, alongside the detailed equations for computing their embeddings. 

The node-type $u_i$ are the one-hot vectors of the node types. The type is according to the node's origin form, the input content $z$, question $\{q\}$, answer $\mathcal{A}$, or the node in the KG. 
The $u_i$ is transformed into an embedding through a linear transformation for subsequent calculations.

The relation type $r_{ij}$ is determined using pre-defined templates, which are employed to extract relations from the knowledge triples in the KG~\citep{feng2020scalable}. The embedding $\boldsymbol r_{ij}$ for the relation is computed for subsequent use: 
\begin{equation}
  \boldsymbol r_{ij} = f_{\zeta } (r_{ij}, u_{ij}) = f_{\zeta } (r_{ij}, u_{i}, u_{j}),
\end{equation}
where $f_{\zeta }$ is a two-layer MLP, $u_{ij}$ denotes the concatenation of $u_{i}$ and $u_{j}$.

The node score $v_{score}$ is subsequently utilized in its embedded form, calculated by:
\begin{equation}
    \boldsymbol v_{score} = f_{\rho }(v_{score})
\end{equation}
where $f_{\rho }$ is a two-layer MLP. 


\section{Other Explanation Examples}\label{sec:appendix-example}
We demonstrate the complete explanation example of PathReasoner and ECQA in Table~\ref{app:example}. These methods exhibit in an unclear and intricate manner. Such explanations make it hard for humans to understand the decision-making process behind the model.


\begin{table}[htb]
  \centering
  \resizebox{0.9\columnwidth}{!}{%
    \begin{tabular}{@{}c|l@{}}
      \toprule
      \textbf{Input Questions}                          & \begin{tabular}[c]{@{}l@{}}Q: What is someone doing if he or she is sitting quietly and his or \\her eyes are moving?\\ A. reading B. meditate C. fall asleep D. bunk E. think\end{tabular}                                                                                                                                                                                                                                                                                                                 \\ \midrule
      \textbf{Label}                                    & A. reading                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \\
      \midrule
                                                        & \textbf{Explanation of Others}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \\ \midrule
      \textbf{\makecell{Path-                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \\Reasoner }} & \begin{tabular}[c]{@{}l@{}}quietly [related to] quiet [at location] a library [used for] reading \\ eyes [used for] reading \\ eyes [form of] eye [related to] glasses [used for] reading \\ sitting [related to] sit [related to] relaxing [has subevent] reading \\\end{tabular} \\ \midrule
      \textbf{\makecell{ECQA \\}} & \begin{tabular}[c]{@{}l@{}}\textbf{Positive examples:} \\ - When we read, our eyes move. \\ - While reading, a person sits quietly, \\ \textbf{Negative examples: }\\ - While meditating, eyes don't move, eyes are closed, \\ - While sleeping, eyes are closed and they don't move, \\ - When a person bunks, he/she doesn't sit quietly, \\ - Eyes don't move when you think about something. \\ \textbf{Explanation:} \\ When we read, our eyes move. \\ While reading, a person sits quietly. \\ While meditating and sleeping, eyes don't move, eyes are closed. \\ When a person bunks, he/she doesn't sit quietly. \\ Eyes don't move when you think about something. \\\end{tabular} \\ \bottomrule
    \end{tabular}%
  }
  \caption{The complete explanation examples of PathReasoner and ECQA.}
  \label{app:example}
\end{table}







\section{Case Study} \label{app:case}

In this section, we will present more explanation examples generated by~\methodname{}, to demonstrate the effectiveness of our method. These questions are randomly selected from the dataset. By sharing these examples, we aim to highlight the broad applicability and robust performance of~\methodname{} across diverse contexts. These examples not only illustrate the adaptability of our approach but also substantiate its validity by demonstrating consistent results. The comparison examples are shown in Figure \ref{fig:correct_ex} and Figure \ref{fig:wrong_exam}.


\input{Main/pos_neg_box}






\section{Ablation Studies}\label{sec:ablation}
In this section, we examine the impact of different components of~\methodname{} on its performance. We evaluated the effects of the size of the LMs, knowledge components, and interpreting components using the CommonsenseQA IHdev and IHtest datasets. Tables~\ref{ab:PLM},~\ref{ab:model-only} and~\ref{ab:interpreting} summarize the ablation study results.

\begin{table}[h]
  \centering
  \resizebox{0.49\columnwidth}{!}{%
  \begin{tabular}{@{}rcc@{}}
    \toprule
    Method                       & IHdev-Acc. & IHtest-Acc. \\ \midrule
    RoBERTa w/o itp              & 68.63\%    & 64.54\%     \\
    RoBERTa-large w/o itp        & 73.05\%    & 71.96\%     \\
    RoBERTa-large + itp & \textbf{77.97\%}    & \textbf{77.31\%}     \\ \bottomrule
  \end{tabular}
  }
  \caption{Ablation study on the effect of interpreting component on model accuracy.}
  \label{ab:interpreting}
\end{table}

Table~\ref{ab:PLM} shows the impact of the size of LM on~\methodname{}. We evaluate the performance of LMs with two different sizes: 1) RoBERTa-large (with 340 million parameters) and 2) RoBERTa (with 110 million parameters). The results show that using a larger LM leads to significant improvement in performance, with an increase of 11.71\% and 14.30\% in model accuracy on the IHdev dataset and the IHtest dataset, respectively.

Table~\ref{ab:model-only} shows the impact of the knowledge component of~\methodname{}.
We compare the performance of the LM-only model with and without external knowledge from ConceptNet.
\textit{only} means we only use the LM to predict the answer. \textit{+ external knowledge} means the external knowledge is leveraged. We observe that incorporating external knowledge significantly improves the accuracy of the LM prediction, especially on the test set. With external knowledge, the model accuracy on IHdev and IHtest is increased by at least 3.69\% and 7.12\%, respectively.
% This shows that external knowledge plays an important role in enhancing the reasoning ability of the model.

\begin{table}[h]
\centering
\resizebox{0.49\columnwidth}{!}{
\begin{tabular}{@{}rcc@{}}
    \toprule
    LM                    & IHdev-Acc. & IHtest-Acc. \\ \midrule
    RoBERTa               & 66.26\%    & 63.01\%     \\
    RoBERTa-large (final) & \textbf{77.97\%}    & \textbf{77.31\%}     \\\bottomrule
  \end{tabular}
  }
  \caption{Ablation study on the effect of LM size on model accuracy.}
  \label{ab:PLM}
\end{table}



In Table~\ref{ab:interpreting}, we analyze the impact of the interpreting component on LM performance. \textit{w/o itp} indicates that the interpreting component was not incorporated in the prediction, whereas the \textit{+ itp} indicates its presence. We observe that removing the interpreting component leads to a clear decrease in accuracy by at least 4.92\% and 5.35\% on IHdev and IHtest, respectively. Furthermore, comparing the results of \textit{RoBERTa-large only}, \textit{RoBERTa-large + itp}, and \textit{final}, we find that the interpreting component has a greater impact on accuracy than the other components.

\begin{table}[htb]
\centering
\resizebox{0.49\columnwidth}{!}{
\begin{tabular}{@{}rcc@{}}
    \toprule
    Method                        & IHdev-Acc. & IHtest-Acc. \\ \midrule
    RoBERTa only                  & 62.65\%    & 60.27\%     \\
    RoBERTa-large only            & 74.28\%    & 70.19\%     \\
    RoBERTa-large + external knowledge & \textbf{77.97\%}    & \textbf{77.31\%}     \\ \bottomrule
  \end{tabular}
  }
  \caption{Ablation study on the effect of knowledge component on model accuracy.}
  \label{ab:model-only}
\end{table}

The ablation highlights the positive contributions of each component of \methodname{}. Specifically, we find that the interpreting component plays a crucial role in enhancing model accuracy and generalizability on unseen questions.


\subsection{Results of Different Generators} \label{appendix:generators}

\begin{figure*}[htb]
  \begin{center}
    %\framebox[4.0in]{$\;$}
    %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
    \includegraphics[width=1\textwidth]{figure/complete_example.pdf}
  \end{center}
  \caption{The \textit{why-choose} and \textit{why-not-choose} explanations generated by Llama-2-70B, GPT-4 and GPT-3.5. The semantic meanings remain consistently aligned among the explanations generated by the three models.}
  \label{fig:appendix-exam}
\end{figure*}


In this section, we present a comprehensive analysis of the results from different explanation generators: Llama-2-70B, GPT-4-turbo, and GPT-3.5-turbo. We focus on evaluating how each generator interprets and translates the model's decision-making process into human-understandable explanations.

The complete experimental results are presented in Figure \ref{fig:appendix-exam}, where all experiments are conducted under the same settings. The question is collected randomly: 
\begin{itemize}
    \item Question: What is someone doing if he or she is sitting quietly and his or her eyes are moving?
    \item Answer Choices: A. reading, B. meditate, C. fall asleep, D. bunk, E. think.
    \item Model Prediction: A. reading
    \item Ground-truth Answer: A. reading
\end{itemize}

We utilize RoBERTa-large as the LM $f_{LM}$ for this experiment. The $f_{LM}$ correctly predicts the answer as ``A. reading''. Our extracted \textit{reason-elements} are: 1. quiet chattering mind, 2. not making sound, 3. mind focuses, 4. glasses for people with poor eyesight, 5. war.

To further quantify the semantic similarity between explanations of Llama-2, GPT-4, and GPT-3.5, we employ GPT-4 to generate similarity scores. GPT-4's advanced language comprehension abilities make it well-suited for this task, offering a human-like understanding of textual content. The scores reflect the degree of alignment in content among the explanations. The score is on a scale from 0 to 1, where 1 is very similar and 0 is not similar at all.

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.45\textwidth]{figure/heatmap.png}
  \end{center}
  \caption{Heatmap of Similarity Scores for Llama-2, GPT-4, and GPT-3.5: Their generated explanations show high consistency in terms of semantic meaning. }
  \label{fig:appendix-heat}
  \vspace{-10pt}
\end{figure}

\textbf{Llama-2 vs. GPT-4:}

Similarity: Both explanations align in focusing on the 'reading' activity, referencing quiet sitting, eye movement, and glasses use.

Similarity Score: 0.85/1 - High similarity in core reasoning and conclusion.

\textbf{Llama-2 vs. GPT-3.5:}

Similarity: Both identify the person as engaged in reading, noting quiet sitting and glasses use.

Similarity Score: 0.75/1 - Similar in conclusion and main points, but GPT-3.5 provides more concise content.

\textbf{GPT-4 vs. GPT-3.5:}

Similarity: Agreement in the conclusion of ``reading'', common elements include quiet posture, eye movement, and glasses use.

Similarity Score: 0.80/1 - Similar key conclusions and elements, but GPT-4 includes more detail.

We illustrate the similarity scores in Figure \ref{fig:appendix-heat}. The color intensities represent the degree of similarity, with darker tones indicating higher congruence. It shows their generated explanations align in semantic meaning. 

% Despite variations in style and detail, the fundamental meanings are consistent across all generators. This consistency highlights the effectiveness of our approach in preserving the accuracy of the explanations.

The explanations generated by the three models are largely consistent in semantic meaning, demonstrating that under our constrained prompt instruction, these models primarily functioned as ``translators''. They convert the reasoning process into human-understandable language. However, it is important to note that the capability of the generator influenced the readability of the explanations. For instance, Llama-2 tends to produce more repetitive language (in red), while GPT-3.5-turbo and GPT-4 show consistency and conciseness. Based on these observations, we recommend using GPT-3.5-turbo or GPT-4 as the explanation generator for optimal clarity.




\section{Material for User Perception Perspective Evaluation}
\subsection{Details of Evaluation Metrics}
\paragraph{Overall Quality} This criterion assesses the effectiveness of the explanations in making the LMs' decision processes understandable to users, providing a comprehensive measure of the explainability's efficacy.

\paragraph{Understandability} This metric evaluates the clarity and coherence of the explanations, determining how easily users can comprehend the model's outputs and underlying reasoning.

\paragraph{Trustworthiness} This assesses users' confidence in the model's outputs and explanations, examining whether the explanations are perceived as reliable, credible, and based on sound logic.

\paragraph{Satisfaction} This captures users' overall contentment with the explanations, considering whether the outputs meet their expectations in terms of quality, relevance, and usefulness.

\paragraph{Detail Sufficiency} This examines whether the explanations provide an adequate level of detail, ensuring they are sufficiently descriptive and comprehensive to fully address the question or task at hand.

\paragraph{Completeness} This evaluates the extent to which the explanations cover the model's decision-making process, verifying that all essential aspects of the model's reasoning are addressed.

\paragraph{Accuracy} In addition to objective measures, this involves a subjective assessment of whether the explanations align with users' knowledge or expectations, confirming the factual correctness and contextual appropriateness of the model's outputs and explanations.

\subsection{Details of Automated Evaluation}\label{appendix:auto-eval}

In this section, we describe the process of using automated evaluators to evaluate the quality of our explanations. We start by randomly selecting an example to demonstrate the evaluation procedure. The evaluators provide their scores and reasons for their ratings. This allows us to verify the validity of their assessments and better understand the basis of their evaluations. We use GPT-3.5-turbo and GPT-4 as our automated evaluators. 


\textbf{Question:} What might happen when not being careful setting a cup on a table?

A. picks up

B. noise

C. decorating the table

D. spillage

E. clutter

\textbf{LM predicted answer:}  D. spillage

\textbf{Ranked Reason-elements:} 1. cold day, 2. careless, 3. past can, 4. quite possible, 5. written document.

\textbf{\textit{Why-choose} explanation:} Since the person is being careless and it's quite possible that they've spilled things in the past, the model predicts that there is a high likelihood of spillage when not being careful setting a cup on a table. 

\textbf{\textit{Why-not-choose} explanation:} The other potential choices, such as picks up, decorating the table, and clutter, were not predicted because those words were not mentioned as significant reasons by the model. Additionally, there was no indication in the reason-words to suggest that those actions would occur as a result of not being careful when setting a cup on a table. The word noise was also not predicted because it does not directly relate to the action of setting a cup on a table, nor were any reason-words related to it.

\textbf{Prompt: }Suppose you are an AI expert. We now require your assistance in evaluating the content validity of explanations that why a language model made certain decisions. Please adhere to our grading rubric, using ``1'' to represent ``Disagree'', ``2'' for ``Neutral'', and ``3'' for ``Agree''. Your grading should be based on the following 8 questions: 

1. This is a good explanation; 

2. I understand this explanation of how the AI model works; 

3. I trust this explanation of how the AI model works; 

4. This explanation of how the AI model works is satisfying; 

5. This explanation of how the AI model works provides sufficient detail; 

% 6. This explanation of how the AI model works contains irrelevant details; 

6. This explanation of how the AI model works seems complete; 

7. This explanation of how the AI model works is accurate.

\textbf{Evaluation results: }

\textbf{GPT-3.5-turbo:} 

1. This is a good explanation: 3 (Agree)

2. I understand this explanation of how the AI model works: 3 (Agree)

3. I trust this explanation of how the AI model works: 3 (Agree)

4.  This explanation of how the AI model works is satisfying: 3 (Agree)

5. This explanation of how the AI model works has sufficient detail: 3 (Agree)

% 6. This explanation of how the AI model works contains irrelevant details: 2 (Neutral)

6. This explanation of how the AI model works seems complete: 3 (Agree)

7. This explanation of how the AI model works is accurate: 3 (Agree) 

Reason: The explanation provided is clear and understandable, providing a valid reasoning for why the predicted answer is ``spillage''. The mention of being careless and the likelihood of spillage based on past experiences supports the prediction that not being careful when setting a cup on a table could result in spillage. However, some of the reason-elements, such as cold day, quite possible, and written document, do not seem directly relevant to the context and do not contribute significantly to the explanation, resulting in a neutral rating for question 6. Nonetheless, the overall explanation is satisfactory and accurate given the available information.




\textbf{GPT-4:} 

1. This is a good explanation: 3 (Agree)

2. I understand this explanation of how the AI model works: 3 (Agree)

3. I trust this explanation of how the AI model works: 3 (Agree)

4. This explanation of how the AI model works is satisfying: 3 (Agree)

5. This explanation of how the AI model works has sufficient detail: 3 (Agree)

% 6. This explanation of how the AI model works contains irrelevant details: 2 (Neutral)

6. This explanation of how the AI model works seems complete: 3 (Agree)

7. This explanation of how the AI model works is accurate: 3 (Agree)

Reason: While the reason-elements (``cold day'', ``careless'', ``past can'', ``quite possible'', ``written document'') don't seem to have a clear relation to the specific context of the question, the explanation itself is still understandable, providing clear reasoning for why the model chose ``spillage'' as the most likely outcome and why the other options were less likely. The explanation is detailed, complete, and aligns with common sense and real-world expectations about what might happen when someone is not careful while setting a cup on a table.







