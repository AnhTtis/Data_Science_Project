@inproceedings{ribeiro2016should,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}

@article{sarzynska2021detecting,
  title={Detecting formal thought disorder by deep contextualized word representations},
  author={Sarzynska-Wawer, Justyna and Wawer, Aleksander and Pawlak, Aleksandra and Szymanowska, Julia and Stefaniak, Izabela and Jarkiewicz, Michal and Okruszek, Lukasz},
  journal={Psychiatry Research},
  volume={304},
  pages={114135},
  year={2021},
  publisher={Elsevier}
}
@article{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{tiddi2022knowledge,
  title={Knowledge graphs as tools for explainable machine learning: A survey},
  author={Tiddi, Ilaria and Schlobach, Stefan},
  journal={Artificial Intelligence},
  volume={302},
  pages={103627},
  year={2022},
  publisher={Elsevier}
}
@inproceedings{wang2019explainable,
  title={Explainable reasoning over knowledge graphs for recommendation},
  author={Wang, Xiang and Wang, Dingxian and Xu, Canran and He, Xiangnan and Cao, Yixin and Chua, Tat-Seng},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={5329--5336},
  year={2019}
}
@article{lecue2020role,
  title={On the role of knowledge graphs in explainable AI},
  author={Lecue, Freddy},
  journal={Semantic Web},
  volume={11},
  number={1},
  pages={41--51},
  year={2020},
  publisher={IOS Press}
}
@article{koncel2019text,
  title={Text generation from knowledge graphs with graph transformers},
  author={Koncel-Kedziorski, Rik and Bekal, Dhanush and Luan, Yi and Lapata, Mirella and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:1904.02342},
  year={2019}
}
@article{yasunaga2021qagnn,
  title={Qa-gnn: Reasoning with language models and knowledge graphs for question answering},
  author={Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
  journal={arXiv preprint arXiv:2104.06378},
  year={2021}
}
@article{arrieta2020explainable,
  title={Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
  author={Arrieta, Alejandro Barredo and D{\'\i}az-Rodr{\'\i}guez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garc{\'\i}a, Salvador and Gil-L{\'o}pez, Sergio and Molina, Daniel and Benjamins, Richard and others},
  journal={Information fusion},
  volume={58},
  pages={82--115},
  year={2020},
  publisher={Elsevier}
}
@inproceedings{du-etal-2021-excar,
    title = "{E}x{CAR}: Event Graph Knowledge Enhanced Explainable Causal Reasoning",
    author = "Du, Li  and
      Ding, Xiao  and
      Xiong, Kai  and
      Liu, Ting  and
      Qin, Bing",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.183",
    doi = "10.18653/v1/2021.acl-long.183",
    pages = "2354--2363",
    abstract = "Prior work infers the causation between events mainly based on the knowledge induced from the annotated causal event pairs. However, additional evidence information intermediate to the cause and effect remains unexploited. By incorporating such information, the logical law behind the causality can be unveiled, and the interpretability and stability of the causal reasoning system can be improved. To facilitate this, we present an \textbf{E}vent graph knowledge enhanced e\textbf{x}plainable \textbf{CA}usal \textbf{R}easoning framework (\textbf{ExCAR}). ExCAR first acquires additional evidence information from a large-scale causal event graph as logical rules for causal reasoning. To learn the conditional probabilistic of logical rules, we propose the Conditional Markov Neural Logic Network (CMNLN) that combines the representation learning and structure learning of logical rules in an end-to-end differentiable manner. Experimental results demonstrate that ExCAR outperforms previous state-of-the-art methods. Adversarial evaluation shows the improved stability of ExCAR over baseline systems. Human evaluation shows that ExCAR can achieve a promising explainable performance.",
}

@article{wang2020language,
  title={Language models are open knowledge graphs},
  author={Wang, Chenguang and Liu, Xiao and Song, Dawn},
  journal={arXiv preprint arXiv:2010.11967},
  year={2020}
}
@article{wallace2019allennlp,
  title={Allennlp interpret: A framework for explaining predictions of nlp models},
  author={Wallace, Eric and Tuyls, Jens and Wang, Junlin and Subramanian, Sanjay and Gardner, Matt and Singh, Sameer},
  journal={arXiv preprint arXiv:1909.09251},
  year={2019}
}
@inproceedings{yadav2021human,
  title={Human-level interpretable learning for aspect-based sentiment analysis},
  author={Yadav, Rohan Kumar and Jiao, Lei and Granmo, Ole-Christoffer and Goodwin, Morten},
  booktitle={The thirty-fifth AAAI conference on artificial intelligence (AAAI-21). AAAI},
  year={2021}
}

@inproceedings{speer2017conceptnet,
    author = {Speer, Robyn and Chin, Joshua and Havasi, Catherine},
    title = {ConceptNet 5.5: An Open Multilingual Graph of General Knowledge},
    year = {2017},
    publisher = {AAAI Press},
    booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
    pages = {4444–4451},
    numpages = {8},
    location = {San Francisco, California, USA},
    series = {AAAI'17}
}
@inproceedings{talmor2019commonsenseqa,
    title = "{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge",
    author = "Talmor, Alon  and
      Herzig, Jonathan  and
      Lourie, Nicholas  and
      Berant, Jonathan",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1421",
    doi = "10.18653/v1/N19-1421",
    pages = "4149--4158"
}

@inproceedings{gordon2013reportingbias,
    author = {Gordon, Jonathan and Van Durme, Benjamin},
    title = {Reporting Bias and Knowledge Acquisition},
    year = {2013},
    isbn = {9781450324113},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2509558.2509563},
    doi = {10.1145/2509558.2509563},
    abstract = {Much work in knowledge extraction from text tacitly assumes that the frequency with which people write about actions, outcomes, or properties is a reflection of real-world frequencies or the degree to which a property is characteristic of a class of individuals. In this paper, we question this idea, examining the phenomenon of reporting bias and the challenge it poses for knowledge extraction. We conclude with discussion of approaches to learning commonsense knowledge from text despite this distortion.},
    booktitle = {Proceedings of the 2013 Workshop on Automated Knowledge Base Construction},
    pages = {25–30},
    numpages = {6},
    keywords = {reporting bias, knowledge extraction, text frequency},
    location = {San Francisco, California, USA},
    series = {AKBC '13}
}
@article{velickovic2018gat,
  title="{Graph Attention Networks}",
  author={Veli{\v{c}}kovi{\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`{o}}, Pietro and Bengio, Yoshua},
  journal={International Conference on Learning Representations},
  year={2018},
  url={https://openreview.net/forum?id=rJXMpikCZ},
  note={accepted as poster},
}
@INPROCEEDINGS{zeiler2011AdaptiveDeconvNet,
  author={Zeiler, Matthew D. and Taylor, Graham W. and Fergus, Rob},
  booktitle={2011 International Conference on Computer Vision}, 
  title={Adaptive deconvolutional networks for mid and high level feature learning}, 
  year={2011},
  volume={},
  number={},
  pages={2018-2025},
  doi={10.1109/ICCV.2011.6126474}}
  
@INPROCEEDINGS{zeiler2010DeconvNet,
  author={Zeiler, Matthew D. and Krishnan, Dilip and Taylor, Graham W. and Fergus, Rob},
  booktitle={2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition}, 
  title={Deconvolutional networks}, 
  year={2010},
  volume={},
  number={},
  pages={2528-2535},
  doi={10.1109/CVPR.2010.5539957}}
  
@INPROCEEDINGS {zhou2016learningdeepfeatures,
author = {B. Zhou and A. Khosla and A. Lapedriza and A. Oliva and A. Torralba},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Learning Deep Features for Discriminative Localization},
year = {2016},
volume = {},
issn = {1063-6919},
pages = {2921-2929},
keywords = {visualization;neural networks;training;object recognition;computer vision;detectors;spatial resolution},
doi = {10.1109/CVPR.2016.319},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2016.319},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@INPROCEEDINGS{Aravindb2015invertDeepImageRepresentation,
  author={Mahendran, Aravindh and Vedaldi, Andrea},
  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Understanding deep image representations by inverting them}, 
  year={2015},
  volume={},
  number={},
  pages={5188-5196},
  doi={10.1109/CVPR.2015.7299155}}
  
@inproceedings{arras-etal-2017-explaining,
    title = "Explaining Recurrent Neural Network Predictions in Sentiment Analysis",
    author = {Arras, Leila  and
      Montavon, Gr{\'e}goire  and
      M{\"u}ller, Klaus-Robert  and
      Samek, Wojciech},
    booktitle = "Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-5221",
    doi = "10.18653/v1/W17-5221",
    pages = "159--168",
    abstract = "Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.",
}
@inproceedings{vig-belinkov-2019-analyzing,
    title = "Analyzing the Structure of Attention in a Transformer Language Model",
    author = "Vig, Jesse  and
      Belinkov, Yonatan",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4808",
    doi = "10.18653/v1/W19-4808",
    pages = "63--76",
    abstract = "The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.",
}

@inproceedings{conf/emnlp/VoitaST19a,
  author={Elena Voita and Rico Sennrich and Ivan Titov},
  title={The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives},
  year={2019},
  cdate={1546300800000},
  pages={4395-4405},
  url={https://doi.org/10.18653/v1/D19-1448},
  booktitle={EMNLP/IJCNLP (1)},
  crossref={conf/emnlp/2019-1}
}

@inproceedings{Vig19MultiscaleVisualizeofAttention,
  author    = {Jesse Vig},
  editor    = {Marta R. Costa{-}juss{\`{a}} and
               Enrique Alfonseca},
  title     = {A Multiscale Visualization of Attention in the Transformer Model},
  booktitle = {Proceedings of the 57th Conference of the Association for Computational
               Linguistics, {ACL} 2019, Florence, Italy, July 28 - August 2, 2019,
               Volume 3: System Demonstrations},
  pages     = {37--42},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/p19-3007},
  doi       = {10.18653/v1/p19-3007},
  timestamp = {Fri, 06 Aug 2021 00:40:58 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/Vig19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{TonevaW19brainImaging,
  author    = {Mariya Toneva and
               Leila Wehbe},
  editor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {Interpreting and improving natural-language processing (in machines)
               with natural language-processing (in the brain)},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada},
  pages     = {14928--14938},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/749a8e6c231831ef7756db230b4359c8-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/TonevaW19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Turney06SATanalogy,
  author    = {Peter D. Turney},
  title     = {Similarity of Semantic Relations},
  journal   = {Comput. Linguistics},
  volume    = {32},
  number    = {3},
  pages     = {379--416},
  year      = {2006},
  url       = {https://doi.org/10.1162/coli.2006.32.3.379},
  doi       = {10.1162/coli.2006.32.3.379},
  timestamp = {Mon, 11 May 2020 15:46:01 +0200},
  biburl    = {https://dblp.org/rec/journals/coling/Turney06.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{MostafazadehCHP16clozetest,
  author    = {Nasrin Mostafazadeh and
               Nathanael Chambers and
               Xiaodong He and
               Devi Parikh and
               Dhruv Batra and
               Lucy Vanderwende and
               Pushmeet Kohli and
               James F. Allen},
  editor    = {Kevin Knight and
               Ani Nenkova and
               Owen Rambow},
  title     = {A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense
               Stories},
  booktitle = {{NAACL} {HLT} 2016, The 2016 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language Technologies,
               San Diego California, USA, June 12-17, 2016},
  pages     = {839--849},
  publisher = {The Association for Computational Linguistics},
  year      = {2016},
  url       = {https://doi.org/10.18653/v1/n16-1098},
  doi       = {10.18653/v1/n16-1098},
  timestamp = {Fri, 06 Aug 2021 00:41:32 +0200},
  biburl    = {https://dblp.org/rec/conf/naacl/MostafazadehCHP16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NIPS2017attentionIsAllYouNeed,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{Castelvecchi16blackbox,
  author    = {Davide Castelvecchi},
  title     = {Can we open the black box of AI?},
  journal   = {Nat.},
  volume    = {538},
  number    = {7623},
  pages     = {20--23},
  year      = {2016},
  url       = {https://doi.org/10.1038/538020a},
  doi       = {10.1038/538020a},
  timestamp = {Mon, 08 Jun 2020 22:21:23 +0200},
  biburl    = {https://dblp.org/rec/journals/nature/Castelvecchi16d.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zachary2018methosofinterpret,
author = {Lipton, Zachary C.},
title = {The Mythos of Model Interpretability: In Machine Learning, the Concept of Interpretability is Both Important and Slippery.},
year = {2018},
issue_date = {May-June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1542-7730},
url = {https://doi.org/10.1145/3236386.3241340},
doi = {10.1145/3236386.3241340},
abstract = {Supervised machine-learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world?},
journal = {Queue},
month = {jun},
pages = {31–57},
numpages = {27}
}

@article{murdoch2019defininterpretableML,
author = {W. James Murdoch  and Chandan Singh  and Karl Kumbier  and Reza Abbasi-Asl  and Bin Yu },
title = {Definitions, methods, and applications in interpretable machine learning},
journal = {Proceedings of the National Academy of Sciences},
volume = {116},
number = {44},
pages = {22071-22080},
year = {2019},
doi = {10.1073/pnas.1900654116},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1900654116},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1900654116},
abstract = {The recent surge in interpretability research has led to confusion on numerous fronts. In particular, it is unclear what it means to be interpretable and how to select, evaluate, or even discuss methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the underappreciated role played by human audiences. Within this framework, methods are organized into 2 classes: model based and post hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce 3 desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work. Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.}}

@article{raffel2020transformeronQA,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1--67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{palmonari2020knowledge,
  title={Knowledge graph embeddings and explainable AI},
  author={Palmonari, Matteo and Minervini, Pasquale},
  journal={Knowledge Graphs for Explainable Artificial Intelligence: Foundations, Applications and Challenges},
  volume={47},
  pages={49},
  year={2020}
}

@inproceedings{Ren2020Query2box,
title={Query2box: Reasoning over Knowledge Graphs in Vector Space Using Box Embeddings},
author={Hongyu Ren* and Weihua Hu* and Jure Leskovec},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BJgr4kSFDS}
}

@inproceedings{lin-etal-2019-kagnet,
    title = "{K}ag{N}et: Knowledge-Aware Graph Networks for Commonsense Reasoning",
    author = "Lin, Bill Yuchen  and
      Chen, Xinyue  and
      Chen, Jamin  and
      Ren, Xiang",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1282",
    doi = "10.18653/v1/D19-1282",
    pages = "2829--2839",
    abstract = "Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations in our daily life. In this paper, we propose a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences. The framework first grounds a question-answer pair from the semantic space to the knowledge-based symbolic space as a schema graph, a related sub-graph of external knowledge graphs. It represents schema graphs with a novel knowledge-aware graph network module named KagNet, and finally scores answers with graph representations. Our model is based on graph convolutional networks and LSTMs, with a hierarchical path-based attention mechanism. The intermediate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using ConceptNet as the only external resource for Bert-based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning.",
}

@inproceedings{kipf2017semi,
  title={Semi-Supervised Classification with Graph Convolutional Networks},
  author={Kipf, Thomas N. and Welling, Max},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017}
}

@article{SHIMIZU2022kgatExplain,
title = {An explainable recommendation framework based on an improved knowledge graph attention network with massive volumes of side information},
journal = {Knowledge-Based Systems},
volume = {239},
pages = {107970},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107970},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121010959},
author = {Ryotaro Shimizu and Megumi Matsutani and Masayuki Goto},
keywords = {Explainable artificial intelligence, Explainable recommendation, Model-intrinsic approach, Knowledge graph attention network, Knowledge graph embedding, Knowledge graph enabled recommendation},
abstract = {In recent years, explainable recommendation has been a topic of active study. This is because the branch of the machine learning field related to methodologies is enabling human understanding of the reasons for the outputs of recommender systems. The realization of explainable recommendation is widely expected to increase both user satisfaction and the demand for explainable recommendation systems. Explainable recommendation utilizes a wealth of side information (such as sellers, brands, user ages and genders, and bookmark information, among others) to expound the decision-making reasoning applied by recommendation models. In explainable recommendation, although learning side information containing numerous variables leads to rich interpretability, learning too many variables presents a challenge because decreases the amount of learning that a given computational resource can perform, and the accuracy of the recommendation model may be degraded. However, numerous and diverse variables are included in the side information stored by the actual companies operating massive real-world services. Hence, to realize practical applications of this valuable information, it is necessary to resolve problems such as computational cost. In this study, we propose a new framework for explainable recommendation based on an improved knowledge graph attention network model, which utilizes the side information of items and realizes high recommendation accuracy. The proposed framework enables direct interpretation by visualizing the reasons for the recommendations provided. Experimental results show that the proposed framework reduced computational time requirements by approximately 80%, while maintaining recommendation accuracy by enabling the model to learn the probabilistically given edges included in the graph structure. Moreover, the results show that the proposed framework exhibited richer interpretability than the conventional model. Finally, a multifaceted analysis suggests that the proposed framework is not only effective as an explainable recommendation model but also provides a powerful tool for planning various marketing strategies.}
}

@inproceedings{wang2019kgat,
author = {Wang, Xiang and He, Xiangnan and Cao, Yixin and Liu, Meng and Chua, Tat-Seng},
title = {KGAT: Knowledge Graph Attention Network for Recommendation},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330989},
doi = {10.1145/3292500.3330989},
abstract = {To provide more accurate, diverse, and explainable recommendation, it is compulsory to go beyond modeling user-item interactions and take side information into account. Traditional methods like factorization machine (FM) cast it as a supervised learning problem, which assumes each interaction as an independent instance with side information encoded. Due to the overlook of the relations among instances or items (e.g., the director of a movie is also an actor of another movie), these methods are insufficient to distill the collaborative signal from the collective behaviors of users. In this work, we investigate the utility of knowledge graph (KG), which breaks down the independent interaction assumption by linking items with their attributes. We argue that in such a hybrid structure of KG and user-item graph, high-order relations --- which connect two items with one or multiple linked attributes --- are an essential factor for successful recommendation. We propose a new method named Knowledge Graph Attention Network (KGAT) which explicitly models the high-order connectivities in KG in an end-to-end fashion. It recursively propagates the embeddings from a node's neighbors (which can be users, items, or attributes) to refine the node's embedding, and employs an attention mechanism to discriminate the importance of the neighbors. Our KGAT is conceptually advantageous to existing KG-based recommendation methods, which either exploit high-order relations by extracting paths or implicitly modeling them with regularization. Empirical results on three public benchmarks show that KGAT significantly outperforms state-of-the-art methods like Neural FM and RippleNet. Further studies verify the efficacy of embedding propagation for high-order relation modeling and the interpretability benefits brought by the attention mechanism. We release the codes and datasets at https://github.com/xiangwang1223/knowledge_graph_attention_network.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {950–958},
numpages = {9},
keywords = {collaborative filtering, recommendation, embedding propagation, higher-order connectivity, graph neural network, knowledge graph},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@InProceedings{Wang2019neighborWatch,
author = {Wang, Peng and Wu, Qi and Cao, Jiewei and Shen, Chunhua and Gao, Lianli and Hengel, Anton van den},
title = {Neighbourhood Watch: Referring Expression Comprehension via Language-Guided Graph Attention Networks},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@article{hu2021gatlibrain,
  title={GAT-LI: a graph attention network based learning and interpreting method for functional brain network classification},
  author={Hu, Jinlong and Cao, Lijie and Li, Tenghui and Dong, Shoubin and Li, Ping},
  journal={BMC bioinformatics},
  volume={22},
  number={1},
  pages={1--20},
  year={2021},
  publisher={BioMed Central}
}

@article{brown-etal-1992ngramLM,
    title = "Class-Based \textit{n}-gram Models of Natural Language",
    author = "Brown, Peter F.  and
      Della Pietra, Vincent J.  and
      deSouza, Peter V.  and
      Lai, Jenifer C.  and
      Mercer, Robert L.",
    journal = "Computational Linguistics",
    volume = "18",
    number = "4",
    year = "1992",
    url = "https://aclanthology.org/J92-4003",
    pages = "467--480",
}

@inproceedings{mikolov2010recurrent,
  title={Recurrent neural network based language model.},
  author={Mikolov, Tomas and Karafi{\'a}t, Martin and Burget, Lukas and Cernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle={Interspeech},
  volume={2},
  number={3},
  pages={1045--1048},
  year={2010},
  organization={Makuhari}
}

@INPROCEEDINGS{tomas2011rnnlm,
  author={Mikolov, Tomáš and Kombrink, Stefan and Burget, Lukáš and Černocký, Jan and Khudanpur, Sanjeev},
  booktitle={2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Extensions of recurrent neural network language model}, 
  year={2011},
  volume={},
  number={},
  pages={5528-5531},
  doi={10.1109/ICASSP.2011.5947611}}


@inproceedings{devlin2019bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{NEURIPS2020GPT3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{raffel2020T5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1--67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@book{JurafskyM09sparsityOfNgram,
  author    = {Dan Jurafsky and
               James H. Martin},
  title     = {Speech and language processing: an introduction to natural language
               processing, computational linguistics, and speech recognition, 2nd
               Edition},
  series    = {Prentice Hall series in artificial intelligence},
  publisher = {Prentice Hall, Pearson Education International},
  year      = {2009},
  url       = {https://www.worldcat.org/oclc/315913020},
  isbn      = {9780135041963},
}

@article{Karpathy2016visualizing,
  author    = {Andrej Karpathy and
               Justin Johnson and
               Li Fei{-}Fei},
  title     = {Visualizing and Understanding Recurrent Networks},
  journal   = {CoRR},
  volume    = {abs/1506.02078},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.02078},
  eprinttype = {arXiv},
  eprint    = {1506.02078}
}

@article{radford2019gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{swamy2021interpreting,
 author = {Swamy, Vinitra and Romanou, Angelika and Jaggi, Martin},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS), 1st Workshop on eXplainable AI Approaches for Debugging and Diagnosis},
 title = {Interpreting Language Models Through Knowledge Graph Extraction},
 year = {2021}
}

@inproceedings{ding2021saliencyNLP,
    title = "Evaluating Saliency Methods for Neural Language Models",
    author = "Ding, Shuoyang  and
      Koehn, Philipp",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.399",
    doi = "10.18653/v1/2021.naacl-main.399",
    pages = "5034--5052",
    abstract = "Saliency methods are widely used to interpret neural network predictions, but different variants of saliency methods often disagree even on the interpretations of the same prediction made by the same model. In these cases, how do we identify when are these interpretations trustworthy enough to be used in analyses? To address this question, we conduct a comprehensive and quantitative evaluation of saliency methods on a fundamental category of NLP models: neural language models. We evaluate the quality of prediction interpretations from two perspectives that each represents a desirable property of these interpretations: plausibility and faithfulness. Our evaluation is conducted on four different datasets constructed from the existing human annotation of syntactic and semantic agreements, on both sentence-level and document-level. Through our evaluation, we identified various ways saliency methods could yield interpretations of low quality. We recommend that future work deploying such methods to neural language models should carefully validate their interpretations before drawing insights.",
}

@inproceedings{mukund2017integratedgrad,
author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
title = {Axiomatic Attribution for Deep Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms— Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {3319–3328},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@article{Smilkov2017SmoothGradRN,
  title={SmoothGrad: removing noise by adding noise},
  author={Daniel Smilkov and Nikhil Thorat and Been Kim and Fernanda B. Vi{\'e}gas and Martin Wattenberg},
  journal={ArXiv},
  year={2017},
  volume={abs/1706.03825}
}

@article{lakkaraju2017interpretable,
  title={Interpretable \& explorable approximations of black box models},
  author={Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and Leskovec, Jure},
  journal={arXiv preprint arXiv:1707.01154},
  year={2017}
}

@inproceedings{chen2018learning,
  title={Learning to explain: An information-theoretic perspective on model interpretation},
  author={Chen, Jianbo and Song, Le and Wainwright, Martin and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={883--892},
  year={2018},
  organization={PMLR}
}



@inproceedings{lin2019kagnet,
  title={KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning},
  author={Lin, Bill Yuchen and Chen, Xinyue and Chen, Jamin and Ren, Xiang},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2829--2839},
  year={2019}
}

@inproceedings{chen2021kace,
  title={KACE: Generating Knowledge Aware Contrastive Explanations for Natural Language Inference},
  author={Chen, Qianglong and Ji, Feng and Zeng, Xiangji and Li, Feng-Lin and Zhang, Ji and Chen, Haiqing and Zhang, Yin},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={2516--2527},
  year={2021}
}

@article{lundberg2017unified,
  title={A unified approach to interpreting model predictions},
  author={Lundberg, Scott M and Lee, Su-In},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{li2022personalized,
  title={Personalized prompt learning for explainable recommendation},
  author={Li, Lei and Zhang, Yongfeng and Chen, Li},
  journal={arXiv preprint arXiv:2202.07371},
  year={2022}
}

@inproceedings{paiss2022no,
  title={No token left behind: Explainability-aided image classification and generation},
  author={Paiss, Roni and Chefer, Hila and Wolf, Lior},
  booktitle={European Conference on Computer Vision},
  pages={334--350},
  year={2022},
  organization={Springer}
}

@inproceedings{feng2020scalable,
  title={Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering},
  author={Feng, Yanlin and Chen, Xinyue and Lin, Bill Yuchen and Wang, Peifeng and Yan, Jun and Ren, Xiang},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1295--1309},
  year={2020}
}
@article{zhang2022greaselm,
  title={Greaselm: Graph reasoning enhanced language models for question answering},
  author={Zhang, Xikun and Bosselut, Antoine and Yasunaga, Michihiro and Ren, Hongyu and Liang, Percy and Manning, Christopher D and Leskovec, Jure},
  journal={arXiv preprint arXiv:2201.08860},
  year={2022}
}
@inproceedings{aggarwal-etal-2021-explanations,
    title = "{E}xplanations for {C}ommonsense{QA}: {N}ew {D}ataset and {M}odels",
    author = "Aggarwal, Shourya  and
      Mandowara, Divyanshu  and
      Agrawal, Vishwajeet  and
      Khandelwal, Dinesh  and
      Singla, Parag  and
      Garg, Dinesh",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.238",
    doi = "10.18653/v1/2021.acl-long.238",
    pages = "3050--3065",
    abstract = "CommonsenseQA (CQA) (Talmor et al., 2019) dataset was recently released to advance the research on common-sense question answering (QA) task. Whereas the prior work has mostly focused on proposing QA models for this dataset, our aim is to retrieve as well as generate explanation for a given (question, correct answer choice, incorrect answer choices) tuple from this dataset. Our explanation definition is based on certain desiderata, and translates an explanation into a set of positive and negative common-sense properties (aka facts) which not only explain the correct answer choice but also refute the incorrect ones. We human-annotate a first-of-its-kind dataset (called ECQA) of positive and negative properties, as well as free-flow explanations, for $11K$ QA pairs taken from the CQA dataset. We propose a latent representation based property retrieval model as well as a GPT-2 based property generation model with a novel two step fine-tuning procedure. We also propose a free-flow explanation generation model. Extensive experiments show that our retrieval model beats BM25 baseline by a relative gain of 100{\%} in $F_1$ score, property generation model achieves a respectable $F_1$ score of 36.4, and free-flow generation model achieves a similarity score of 61.9, where last two scores are based on a human correlated semantic similarity metric.",
}
@article{ZHAN2022107612,
title = {PathReasoner: Explainable reasoning paths for commonsense question answering},
journal = {Knowledge-Based Systems},
volume = {235},
pages = {107612},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107612},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121008741},
author = {Xunlin Zhan and Yinya Huang and Xiao Dong and Qingxing Cao and Xiaodan Liang},
keywords = {Commonsense reasoning, Knowledge graph, Reasoning path, Interpretability},
abstract = {Commonsense question answering has attracted increasing attention as a challenging task requiring the human reasoning process of answering questions with the help of abundant commonsense knowledge. Existing methods mostly resort to large pre-trained language models and face many difficulties when dealing with the out-of-scope reasoning target, and are unaware of explainable structured information. In this paper, we explore explicitly incorporate external reasoning paths with structured information to explain and facilitate commonsense QA. For this purpose, we propose a PathReasoner to both extract and learn from such structured information. The proposed PathReasoner consists of two main components, a path finder and a hierarchical path learner. To answer a commonsense question, the path finder first retrieves explainable reasoning paths from a large-scale knowledge graph, then the path learner encodes the paths with hierarchical encoders and uses the path features to predict the answers. The experiments on two typical commonsense QA datasets demonstrate the effectiveness of the PathReasoner. The case study gives insightful findings that the reasoning paths provide explainable information for the question answering through the PathReasoner.}
}

@article{zhan2022pathreasoner,
  title={PathReasoner: Explainable reasoning paths for commonsense question answering},
  author={Zhan, Xunlin and Huang, Yinya and Dong, Xiao and Cao, Qingxing and Liang, Xiaodan},
  journal={Knowledge-Based Systems},
  volume={235},
  pages={107612},
  year={2022},
  publisher={Elsevier}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}
@inproceedings{weifinetuned,
  title={Finetuned Language Models are Zero-Shot Learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  booktitle={International Conference on Learning Representations}
}
@article{zhou2022learning,
  title={Learning to prompt for vision-language models},
  author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  journal={International Journal of Computer Vision},
  volume={130},
  number={9},
  pages={2337--2348},
  year={2022},
  publisher={Springer}
}
@article{conneau2019cross,
  title={Cross-lingual language model pretraining},
  author={Conneau, Alexis and Lample, Guillaume},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@inproceedings{mireshghallah2022mix,
  title={Mix and Match: Learning-free Controllable Text Generationusing Energy Language Models},
  author={Mireshghallah, Fatemehsadat and Goyal, Kartik and Berg-Kirkpatrick, Taylor},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={401--415},
  year={2022}
}
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@inproceedings{li2022quantifying,
  title={Quantifying Adaptability in Pre-trained Language Models with 500 Tasks},
  author={Li, Belinda Z and Yu, Jane and Khabsa, Madian and Zettlemoyer, Luke and Halevy, Alon and Andreas, Jacob},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4696--4715},
  year={2022}
}

@article{meng2022interpretability,
  title={Interpretability and fairness evaluation of deep learning models on MIMIC-IV dataset},
  author={Meng, Chuizheng and Trinh, Loc and Xu, Nan and Enouen, James and Liu, Yan},
  journal={Scientific Reports},
  volume={12},
  number={1},
  pages={7166},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{jain2019attention,
  title={Attention is not Explanation},
  author={Jain, Sarthak and Wallace, Byron C},
  booktitle={Proceedings of NAACL-HLT},
  pages={3543--3556},
  year={2019}
}
@article{loh2022application,
  title={Application of explainable artificial intelligence for healthcare: A systematic review of the last decade (2011--2022)},
  author={Loh, Hui Wen and Ooi, Chui Ping and Seoni, Silvia and Barua, Prabal Datta and Molinari, Filippo and Acharya, U Rajendra},
  journal={Computer Methods and Programs in Biomedicine},
  pages={107161},
  year={2022},
  publisher={Elsevier}
}
@article{zytek2022need,
  title={The need for interpretable features: Motivation and taxonomy},
  author={Zytek, Alexandra and Arnaldo, Ignacio and Liu, Dongyu and Berti-Equille, Laure and Veeramachaneni, Kalyan},
  journal={ACM SIGKDD Explorations Newsletter},
  volume={24},
  number={1},
  pages={1--13},
  year={2022},
  publisher={ACM New York, NY, USA}
}

@inproceedings{shrikumar2017learning,
  title={Learning important features through propagating activation differences},
  author={Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  booktitle={International conference on machine learning},
  pages={3145--3153},
  year={2017},
  organization={PMLR}
}

@article{vstrumbelj2014explaining,
  title={Explaining prediction models and individual predictions with feature contributions},
  author={{\v{S}}trumbelj, Erik and Kononenko, Igor},
  journal={Knowledge and information systems},
  volume={41},
  pages={647--665},
  year={2014},
  publisher={Springer}
}
@article{ying2019gnnexplainer,
  title={Gnnexplainer: Generating explanations for graph neural networks},
  author={Ying, Zhitao and Bourgeois, Dylan and You, Jiaxuan and Zitnik, Marinka and Leskovec, Jure},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{tigas2022interventions,
  title={Interventions, where and how? experimental design for causal models at scale},
  author={Tigas, Panagiotis and Annadani, Yashas and Jesson, Andrew and Sch{\"o}lkopf, Bernhard and Gal, Yarin and Bauer, Stefan},
  journal={arXiv preprint arXiv:2203.02016},
  year={2022}
}

@inproceedings{situ-etal-2021-learning,
    title = "Learning to Explain: Generating Stable Explanations Fast",
    author = "Situ, Xuelin  and
      Zukerman, Ingrid  and
      Paris, Cecile  and
      Maruf, Sameen  and
      Haffari, Gholamreza",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.415",
    doi = "10.18653/v1/2021.acl-long.415",
    pages = "5340--5355",
    abstract = "The importance of explaining the outcome of a machine learning model, especially a black-box model, is widely acknowledged. Recent approaches explain an outcome by identifying the contributions of input features to this outcome. In environments involving large black-box models or complex inputs, this leads to computationally demanding algorithms. Further, these algorithms often suffer from low stability, with explanations varying significantly across similar examples. In this paper, we propose a Learning to Explain (L2E) approach that learns the behaviour of an underlying explanation algorithm simultaneously from all training examples. Once the explanation algorithm is distilled into an explainer network, it can be used to explain new instances. Our experiments on three classification tasks, which compare our approach to six explanation algorithms, show that L2E is between 5 and 7.5{\mbox{$\times$}}10{\^{}}4 times faster than these algorithms, while generating more stable explanations, and having comparable faithfulness to the black-box model.",
}

@inproceedings{thorne2019generating,
  title={Generating Token-Level Explanations for Natural Language Inference},
  author={Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={963--969},
  year={2019}
}


@inproceedings{mihaylov2018can,
  title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2381--2391},
  year={2018}
}


@article{guidotti2018local,
  title={Local rule-based explanations of black box decision systems},
  author={Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Pedreschi, Dino and Turini, Franco and Giannotti, Fosca},
  journal={arXiv preprint arXiv:1805.10820},
  year={2018}
}
@inproceedings{yu2022towards,
  title={Towards Explainable Search Results: A Listwise Explanation Generator},
  author={Yu, Puxuan and Rahimi, Razieh and Allan, James},
  booktitle={Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={669--680},
  year={2022}
}
@inproceedings{kumar-talukdar-2020-nile,
    title = "{NILE} : Natural Language Inference with Faithful Natural Language Explanations",
    author = "Kumar, Sawan  and
      Talukdar, Partha",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.771",
    doi = "10.18653/v1/2020.acl-main.771",
    pages = "8730--8742",
    abstract = "The recent growth in the popularity and success of deep learning models on NLP classification tasks has accompanied the need for generating some form of natural language explanation of the predicted labels. Such generated natural language (NL) explanations are expected to be faithful, i.e., they should correlate well with the model{'}s internal decision making. In this work, we focus on the task of natural language inference (NLI) and address the following question: can we build NLI systems which produce labels with high accuracy, while also generating faithful explanations of its decisions? We propose Natural-language Inference over Label-specific Explanations (NILE), a novel NLI method which utilizes auto-generated label-specific NL explanations to produce labels along with its faithful explanation. We demonstrate NILE{'}s effectiveness over previously reported methods through automated and human evaluation of the produced labels and explanations. Our evaluation of NILE also supports the claim that accurate systems capable of providing testable explanations of their decisions can be designed. We discuss the faithfulness of NILE{'}s explanations in terms of sensitivity of the decisions to the corresponding explanations. We argue that explicit evaluation of faithfulness, in addition to label and explanation accuracy, is an important step in evaluating model{'}s explanations. Further, we demonstrate that task-specific probes are necessary to establish such sensitivity.",
}
@article{ji2021survey,
  title={A survey on knowledge graphs: Representation, acquisition, and applications},
  author={Ji, Shaoxiong and Pan, Shirui and Cambria, Erik and Marttinen, Pekka and Philip, S Yu},
  journal={IEEE transactions on neural networks and learning systems},
  volume={33},
  number={2},
  pages={494--514},
  year={2021},
  publisher={IEEE}
}

@inproceedings{huang-etal-2022-deer,
    title = "{DEER}: Descriptive Knowledge Graph for Explaining Entity Relationships",
    author = "Huang, Jie  and
      Zhu, Kerui  and
      Chang, Kevin Chen-Chuan  and
      Xiong, Jinjun  and
      Hwu, Wen-mei",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.448",
    pages = "6686--6698",
    abstract = "We propose DEER (Descriptive Knowledge Graph for Explaining Entity Relationships) - an open and informative form of modeling entity relationships. In DEER, relationships between entities are represented by free-text relation descriptions. For instance, the relationship between entities of machine learning and algorithm can be represented as {``}Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.{''} To construct DEER, we propose a self-supervised learning method to extract relation descriptions with the analysis of dependency patterns and generate relation descriptions with a transformer-based relation description synthesizing model, where no human labeling is required. Experiments demonstrate that our system can extract and generate high-quality relation descriptions for explaining entity relationships. The results suggest that we can build an open and informative knowledge graph without human annotation.",
}
@inproceedings{huang2019explainable,
  title={Explainable interaction-driven user modeling over knowledge graph for sequential recommendation},
  author={Huang, Xiaowen and Fang, Quan and Qian, Shengsheng and Sang, Jitao and Li, Yan and Xu, Changsheng},
  booktitle={proceedings of the 27th ACM international conference on multimedia},
  pages={548--556},
  year={2019}
}
@inproceedings{liu2019knowledge,
  title={Knowledge Aware Conversation Generation with Explainable Reasoning over Augmented Graphs},
  author={Liu, Zhibin and Niu, Zheng-Yu and Wu, Hua and Wang, Haifeng},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={1782--1792},
  year={2019}
}
@article{nye2021improving,
  title={Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning},
  author={Nye, Maxwell and Tessler, Michael and Tenenbaum, Josh and Lake, Brenden M},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={25192--25204},
  year={2021}
}

@inproceedings{tack2022ai,
  title={The AI Teacher Test: Measuring the Pedagogical Ability of Blender and GPT-3 in Educational Dialogues},
  author={Tack, Ana{\"\i}s and Piech, Chris},
  booktitle={Proceedings of the 15th International Conference on Educational Data Mining},
  pages={522},
  year={2022}
}
@inproceedings{lin2020caire,
  title={Caire: An end-to-end empathetic chatbot},
  author={Lin, Zhaojiang and Xu, Peng and Winata, Genta Indra and Siddique, Farhad Bin and Liu, Zihan and Shin, Jamin and Fung, Pascale},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={09},
  pages={13622--13623},
  year={2020}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others}
}
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@article{OpenAI2023GPT4TR,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.08774}
}
@article{QUINLAN1987221,
title = {Simplifying decision trees},
journal = {International Journal of Man-Machine Studies},
volume = {27},
number = {3},
pages = {221-234},
year = {1987},
issn = {0020-7373},
doi = {https://doi.org/10.1016/S0020-7373(87)80053-6},
url = {https://www.sciencedirect.com/science/article/pii/S0020737387800536},
author = {J.R. Quinlan},
abstract = {Many systems have been developed for constructing decision trees from collections of examples. Although the decision trees generated by these methods are accurate and efficient, they often suffer the disadvantage of excessive complexity and are therefore incomprehensible to experts. It is questionable whether opaque structures of this kind can be described as knowledge, no matter how well they function. This paper discusses techniques for simplifying decision trees while retaining their accuracy. Four methods are described, illustrated, and compared on a test-bed of decision trees from a variety of domains.}
}
@InProceedings{Zhang_2018_CVPR,
author = {Zhang, Quanshi and Wu, Ying Nian and Zhu, Song-Chun},
title = {Interpretable Convolutional Neural Networks},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}
@article{sabour2017dynamic,
  title={Dynamic routing between capsules},
  author={Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}