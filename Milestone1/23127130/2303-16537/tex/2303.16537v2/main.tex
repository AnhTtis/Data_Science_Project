% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
% \usepackage{parskip}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath,bm,amssymb}
% \usepackage[dvipsnames]{xcolor}  
\usepackage{makecell}
\usepackage{todonotes}
\usepackage{float}

\definecolor{lightgreen}{HTML}{D5DE56}
\definecolor{lightblue}{HTML}{D1E9EC}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{LMExplainer: a Knowledge-Enhanced Explainer for Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Zichen Chen, Ambuj K Singh, Misha Sra\\
  University of California, Santa Barbara \\
  \texttt{\{zichen\_chen, ambuj, sra\}@ucsb.edu}
  }

\begin{document}
\maketitle
\begin{abstract}

Large language models (LLMs) such as GPT-4 are very powerful and can process different kinds of natural language processing (NLP) tasks. However, it can be difficult to interpret the results due to the multi-layer nonlinear model structure and millions of parameters. A lack of clarity and understanding of how the language models (LMs) work can make them unreliable, difficult to trust, and potentially dangerous for use in real-world scenarios. 
Most recent works exploit attention weights to provide explanations for LM predictions. However, pure attention-based explanations are unable to support the growing complexity of LMs, and cannot reason about their decision-making processes. 
We propose LMExplainer, a knowledge-enhanced explainer for LMs that can provide human-understandable explanations. We use a knowledge graph (KG) and a graph attention neural network to extract the key decision signals of the LM. 
We further explore whether interpretation can also help the AI understand the task better. Our experimental results show that LMExplainer outperforms existing LM+KG methods on CommonsenseQA and OpenBookQA. We compare the explanation results with generated explanation methods and human-annotated results. The comparison shows our method can provide more comprehensive and clearer explanations. LMExplainer demonstrates the potential to enhance model performance and furnish explanations for the LM reasoning process in natural language.

\end{abstract}


\section{Introduction}

Pre-trained language models (LMs) have recently garnered significant attention due to their impressive, state-of-the-art (SOTA) performance on various natural language processing (NLP) tasks \citep{brown2020language,liu2023pre,weifinetuned,zhou2022learning,li2022quantifying}. These tasks include language translation \citep{conneau2019cross}, text generation \citep{mireshghallah2022mix}, and text classification \citep{raffel2020exploring}, among others. One of the main advantages of LMs is their ability to capture the nuances and the complexity of human languages.

However, a major limitation of LMs is a lack of interpretability \citep{meng2022interpretability}: it is often difficult to provide explanations for their ``black box'' decision-making process. LMs use techniques such as attention mechanisms, which allow the model to focus on specific parts of the input data when making decisions \citep{vaswani2017attention, devlin2019bert,liu2019roberta}. But these mechanisms can elude human intuition, and produce internal learning representations that are abstract and non-transparent to humans \citep{jain2019attention}. For example, a model's embedding may capture relationships and meanings as a result of the passage through millions of neurons, but such meanings may not be immediately apparent to humans. This lack of interpretability poses a challenge in critical domains --- e.g., healthcare \citep{loh2022application} and online education \citep{zytek2022need} --- as it limits the trust that users can place in the inference made by the models. Improving model interpretability has the added benefit of addressing issues such as fairness, privacy, and safety. Therefore, methods that explain the behaviors of LMs can help overcome the black-box nature of neural networks. 


Many recent approaches focus on providing model-intrinsic and post-hoc methods to address the interpretability challenge \citep{ribeiro2016should,shrikumar2017learning,vstrumbelj2014explaining,ying2019gnnexplainer,tigas2022interventions,situ-etal-2021-learning}. Model-intrinsic methods tackle models that are considered interpretable due to their simple structure \citep{sabour2017dynamic, Zhang_2018_CVPR}, such as decision trees \citep{QUINLAN1987221}. Post-hoc methods provide explanations that are obtained after the model training. \citeauthor{thorne2019generating} propose the Multiple Instance Learning method for generating token-level explanations for natural language inference based on the attention matrix of the model. \citeauthor{chen2021kace} utilize counterfactual examples to generate contrastive explanations. \citeauthor{zhan2022pathreasoner} incorporate structured information from external knowledge to explain the model. However, these works only provide simple textual explanations and neglect the reasoning process. 
We argue that in order for humans to understand and trust the model in crucial domains, it is necessary to offer textual explanations that provide human-understandable insight into how the model reasons.


In this paper, we present LMExplainer\footnote{We will open-source our code once the paper is accepted.}, a novel approach for explaining the predictions made by LMs. Our approach uses retrieved knowledge, graph convolutional networks (GCNs) \citep{kipf2017semi} and graph attention networks (GATs) \citep{velickovic2018gat}, and is able to provide explanations of the rationale behind the LM's predictions. 
We explore the potential of using explanations to serve a dual purpose: helping humans in comprehending the model, and enhancing the model's understanding of the task at hand through interpretation during the explanation process\footnote{In this paper, explanation refers to explaining the model's decision-making in a human-understandable way, while interpretation refers to understanding the internal workings of the model.}.

We evaluate LMExplainer on the task of question-answering (QA) using the CommonsenseQA \citep{talmor2019commonsenseqa} and OpenBookQA \citep{mihaylov2018can} datasets. Our experimental results demonstrate that LMExplainer outperforms SOTA LM+KG QA methods on CommonsenseQA, while exhibiting competitive performance on OpenBookQA. These outcomes indicate that our method can enhance overall performance. Furthermore, we demonstrate that LMExplainer is capable of providing valuable reasoning insights for humans in a natural manner, surpassing prior explanation methods. To the best of our knowledge, LMExplainer is the ﬁrst work to utilize graph-based knowledge in generating natural language explanations to comprehend the rationale behind model behaviors. 




\section{Related Work}

\subsection{Post-hoc Explanation}
Post-hoc explanation methods have gained significant attention in NLP research in recent years. 
% These include feature-based, concept-based and example-based explanations. 
% One popular approach of feature-based methods is called LIME \citep{ribeiro2016should}. They generate explanations by approximating the original model with a local sample. The model then uses the approximations to highlight the most important features for a given prediction. LIME is extended by \citeauthor{guidotti2018local}, which uses a decision tree classifier to approximate non-linear models. However, they cannot guarantee that the approximations are accurate representations of the original model due to inherent limitations.
\citeauthor{ribeiro2016should} propose LIME, which generates explanations by approximating the original model with a local sample and highlights the most important features. \citeauthor{guidotti2018local} extend it by using a decision tree classifier to approximate models. However, they cannot guarantee that the approximations are accurate representations of the original model due to inherent limitations. \citeauthor{thorne2019generating} generate concepts of classifiers operating on pairs of sentences, while \citeauthor{yu2022towards} generate \emph{aspects} as explanations for search results. \citeauthor{kumar-talukdar-2020-nile} use positive labels to generate candidate explanations, while \citeauthor{chen2021kace} use contrastive examples in the format of ``why A not B'' to distinguish between confusing candidates.  Different from prior work, we integrate the reasoning features and concepts to explain the model's behavior.

% Compared to feature-based explanations, the concept-based approaches use single or multiple phrases to explain the model, which is more understandable to humans. \citeauthor{thorne2019generating} generate tokens of classifiers operating on pairs of sentences, while \citeauthor{yu2022towards} generate \emph{aspects} as explanations for search results. Example-based approaches similarly explain using natural language. \citeauthor{kumar-talukdar-2020-nile} use positive labels to generate candidate explanations, while \citeauthor{chen2021kace} use contrastive examples in the format of ``Why A not B" to distinguish between confusing candidates.  Different from prior work, we integrate the reasoning features and concepts to explain the model's behavior.



\subsection{Large Language Models}
% Language models are used in a wide range of tasks across NLP, such as sentiment analysis, machine translation, and question answering. Conventional n-gram LMs \citep{brown-etal-1992ngramLM} are based on conditional probability, and are interpretable. 
Recently, large language models (LLMs) such as RoBERTa \citep{liu2019roberta} and GPT-4 \citep{OpenAI2023GPT4TR}  have achieved impressive results. 
% For example, they are able to generate coherent stories \citep{nye2021improving}, perform as AI teachers \citep{tack2022ai}, and chat with humans using natural language \citep{lin2020caire}. 
However, these models are often criticized for their lack of interpretability, which can hinder their adoption in real-world applications. 
Previous interpretable frameworks (\citep{ribeiro2016should}, \citep{mukund2017integratedgrad}, \citep{Smilkov2017SmoothGradRN}, \citep{ding2021saliencyNLP}, \citep{swamy2021interpreting}) could be applied on the LLMs, but they often rely on approximations and simplifications of the original models, which may cause discrepancies between the model and the explanation. 
% \citeauthor{swamy2021interpreting} propose a framework to explain the language models during training, but they focus on the comparison metrics between models and different stages of the same model, and do not consider the effect of their methods on performance. 
In this paper, we explain language models (LMs) by utilizing the model reasoning process, and evaluate the impact of the proposed method on model performance.

\subsection{Explainability with Knowledge-Graph}
Knowledge graphs (KGs) are increasingly being used as a means to improve the interpretability and explainability of LMs \citep{huang-etal-2022-deer, yasunaga2021qagnn, huang2019explainable, liu2019knowledge}. KGs are structured representations of knowledge, and can be used to capture the complex semantic relationships that are difficult to represent in traditional LMs \citep{ji2021survey}. 
% We divide KG-based methods into two types: graph-based and graph embedding-based. Graph-based explanation identifies the most relevant paths or subgraphs in a KG that support a given model prediction. 
\citeauthor{ZHAN2022107612} retrieve explainable reasoning paths from a KG and use path features to predict the answers. 
% However, their path explanations are difficult for humans to understand. 
% Graph embedding-based explanation uses an encoder to embed the structure and the relations of a graph, and then the embedding is used to generate explanations by identifying the most relevant nodes in the KG. 
\citeauthor{yasunaga2021qagnn} integrate the KG into the model, enabling the model to reason over structured knowledge and generate more interpretable predictions. 
However, these explanations may not consistently and accurately represent the model's reasoning, and they might be challenging for humans to comprehend because they are presented in a graph-based format.
By drawing upon the insights from prior works, we employ graph embedding as our fundamental component to generate explanations. 




\section{Task Definition}

In this paper, we aim to address two research questions related to the interpretability of LMs: (1) how can we provide interpretable and naturally understandable explanations for the decision-making process of LMs, and (2) how does the provision of explanations impact the performance of LMs? 

We now define the task of generating reasoning-level explanations for inference made by LMs. As an example, we use a QA task. Given a pre-trained LM $f_{LM}$ with input context $z$ and predicted answer $A$, the goal is to generate an explanation $E_{0}$ for why $f_{LM}$ chooses $A$ and an explanation $E_{1}$ for why $f_{LM}$ does not choose other options. We use $E$ to denote the pair of $E_{0}$ and $E_{1}$. 
% The explanations should provide insight into the reasoning behind the prediction and be presented in a format that is understandable to humans. 
This task can be written as:
\vspace{-5pt}
\begin{equation}
    E \gets \textit{Generate Explanation}(f_{LM}, z, A).
\end{equation}



\section{Approach}
The LMExplainer architecture is shown in Figure~\ref{fig:archi}. It consists of three main steps: \textbf{(1) key element extraction and building} (Section~\ref{sec:extraction}), \textbf{(2) element-graph interpretation} (Section~\ref{sec:interpretation}), and \textbf{(3) explanation generation} (Section~\ref{sec:explanation}). In the first step, we extract the relevant elements from the input data and the  knowledge retrieved from the KG, and build an element-graph representation. In the second step, we use a GAT to interpret the element-graph and identify what we call \emph{reason-elements} behind the model's prediction. In the third step, we use an instruction-based method to generate a textual explanation of the decision-making process based on the identified \emph{reason-elements}.
Our approach LMExplainer is flexible and applicable to a range of LMs, e.g., BERT \citep{devlin2019bert}, RoBERTa \citep{liu2019roberta}, and GPT-2 \citep{radford2019language}. 


\begin{figure*}[h]
  \begin{center}
    %\framebox[4.0in]{$\;$}
    %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
    \includegraphics[width=1\textwidth]{figure/archi.pdf}  
    \end{center}
  \caption{The LMExplainer architecture. Given a question context $z$ (which includes question $q$ and the set $\mathcal{A}$ of answers), we first generate language embeddings using a pre-trained LM. Simultaneously, it retrieves relevant knowledge from a KG to construct a subgraph (Section~\ref{sec:extraction}). The language embeddings and subgraph are then combined to obtain GNN embeddings. This combined representation is then passed through a GAT to obtain the attention scores (Section~\ref{sec:interpretation}). Our attention scores serve a dual purpose. Firstly, they weigh the importance of the GNN embeddings and are used with the language embeddings for the final prediction (Section~\ref{sec:inference}). Secondly, they are used to generate explanations by highlighting the most important parts of the reasoning process (Section~\ref{sec:explanation}). }
  \label{fig:archi}
\vspace{-10pt}
\end{figure*}

\subsection{Key Elements Extraction and Building} \label{sec:extraction}

Certain key elements can significantly influence the reasoning process of LMs. To capture these essential elements, we use the context of input data, represented as $z$. Each token $x$ in the input question $q$ is treated as a content element. We use $\mathcal{A}$ to denote the set of all candidate answers, and $a \in \mathcal{A}$ to denote one candidate answer.
Referring to  Figure ~\ref{fig:archi}, the example demonstrates the $q$ and $a$ in ``Input Context [$z$]''. 

We connect the context $z$ to each token $x$ and each potential answer $a$ to construct multi-relational graphs, following the approach from \citeauthor{yasunaga2021qagnn}. Specifically, we incorporate external knowledge to construct the KG. The graph is constructed by retrieving paths in the KG from ConceptNet \citep{speer2017conceptnet}. This allows us to prepare the essential elements that play a key role in the final results and analyze the relations among them. 
% In addition to connecting relations in the multi-relational KG, we have two one-hot embeddings. The relation embeddings are the message from the source or target node, and we denote them using $r_{es}$ and $\hat{r}_{es}$, respectively. The type embeddings are the node type of source or target. We use $u_{e}$ and $u_{s}$ to denote. 
Apart from this, we integrate the knowledge from LMs as an instructive embedding to the KG. We denote a node in KG with $n$. The instructive embedding is added as a probability into the node embedding. The instructive score of $n$ can be computed as:
\vspace{-5pt}
\begin{equation}
    \hat n _{score} = f_{score}(f_{enc}(n|z)),
\vspace{-5pt}
\end{equation}
\begin{equation}
    f_{score}=sigmoid(\text{MLP}(\cdot)),
\end{equation}
where $f_{enc}$ is used to get the embedding from LM, and $f_{score}$ is a multi-layer perceptron (MLP). This score is used to instruct the KG to capture the node that is the key for reasoning. 


The constructed graph typically has a large number of paths at the initial building stage, which can lead to a large reasoning space. Since the score $\hat n$ can represent the correlation between the node $n$ and context $z$, we will utilize it to prune the graph by removing the irrelevant nodes. The extracted graph is our element-graph $G_e$. 
% where each node can be represented as $(V_e, E_e)$. $V_e$ denotes the remaining element, while $E_e$ denotes its edges.


\subsection{Element-graph Interpretation}  \label{sec:interpretation}

Given the element-graph $G_e$, we follow \citeauthor{yasunaga2021qagnn} to extract the representation for graph reasoning. The method is based on GAT and GCN. The nodes in the graph provide feature vectors, while the edges provide paths for the transfer and aggregation of features between nodes. In each layer of a GCN, a single node transfers its own feature vectors to all of its neighboring nodes and aggregates the feature vectors transmitted by its neighbors as its updated node features. This process of ``pass-aggregate-update'' allows both GCN and GAT to preserve part of the structure and context of the original data through the connections between the nodes.


Given the element-graph $G_e$, for any node $V_e$, we denote the neighbors of $V_e$ by $\mathcal{N}(V_e)$, and an individual neighbor node by $s$.
If $h^{k}_{V_e}$ is the node feature at layer $k$, then node feature $h^{k+1}_{V_e}$ at layer $k+1$ is updated as follows:
\vspace{-5pt}
\begin{equation}\label{eq:h_ve}
    h^{k+1}_{V_e} = f_{\delta}\left(\sum\limits_{\mathcal{N}(V_e)\cup \left\{ V_e \right\} }\alpha_{es}m_{es}\right) + h^{k}_{V_e},
    \vspace{-5pt}
\end{equation}
where $f_{\delta}$ is a two-layer MLP, 
% and $\alpha_{es}$ and $m_{es}$ are the attention coefficients and the message, respectively.
$\alpha_{es}$ is the attention coefficient, and  $m_{es}$ is the message. 
After all the neighbor nodes of node $V_e$ in the $k$-th layer have passed information to node $V_e$, the feature of node $V_e$ in the $(k+1)$-th layer, $h^{k+1}_{V_e}$, is obtained through an additive aggregation and a non-linear transformation, in addition to the previous feature of itself. This process allows the GAT model to capture the structural and contextual information of the graph. The updated node features at the final layer are then used as the representation of the graph for reasoning.



To filter the important connections of the graph $G_e$, we incorporate attention weights $\alpha_{es}$ when aggregating the message passing $m_{es}$ from neighbor node $s$ \citep{velickovic2018gat}. The message passing $m_{es}$ is computed using the node type embedding $u_e$, its feature embedding $h^{k}_e$, and the relation embedding $r_{es}$ between nodes $V_e$ and $s$:
\begin{equation}
\vspace{-5pt}
    m_{es} = f_{n}(h^{k}_e, u_e, r_{es}),
\end{equation}
where $f_n$ is a linear transformation, and $u_e$ is calculated by a linear transformation of one-hot node type vectors. The type vectors are the node type of $V_e$ or $s$. We use $u_{e}$ and $u_{s}$ to denote, respectively. 

The relation embedding $r_{es}$ is computed by a two-layer MLP $f_{\theta}$:
\begin{equation}
\vspace{-5pt}
    r_{es} = f_{\theta} (\hat r_{es}, u_{es}),
\end{equation}
where $\hat r_{es}$ is a one-hot vector for the relation connecting from $V_e$ to $s$ or $s$ to $V_e$, $u_{es}$ is the contacted node type embedding of $u_e$ and $u_s$. The attention weight $\alpha_{es}$ is calculated from the query vector $\boldsymbol{q}_e$ and key vector $\boldsymbol{k}_{s}$:
\begin{equation}
\vspace{-5pt}
    \alpha_{es} = \textit{softmax}\left(\frac{\boldsymbol{q}_{e}^{\top} \boldsymbol{k}_{s}}{\sqrt{D}}\right),
\end{equation}
where $D$ refers to the feature dimension. $\boldsymbol{q}_{e}$ and $\boldsymbol{k}_{s}$ are calculated as follows:
\begin{equation}
    \boldsymbol{q}_{e} = f_{q}(h^{k}_e, u_e, \hat{n}),
    \vspace{-2pt}
\end{equation}
\begin{equation}
    \boldsymbol{k}_{s} = f_{k}(h^{k}_{s}, u_{s}, \hat{n}, r_{es}),
\end{equation}
where $f_{q}$ and $f_{k}$ are linear transformations. The attention weight $\alpha_{es}$ captures the importance of the connection between nodes $V_e$ and $s$, while the message passing $m_{es}$ captures the information being passed between the nodes. By multiplying these two quantities, the GAT model is able to filter the important connections in the graph and use them to update the node features. This process allows us to discern the underlying rationale in decision-making processes, and obtain the \emph{interpretation embedding}.


Once the interpretation embedding is obtained, we can generate explanations by extracting the most important nodes and edges in the graph. We use a linear transformation to map the final node features to the output space, and use a softmax function to compute the probability distribution over the potential answers. The most probable answers are selected as the final answer, and the corresponding nodes and edges in the element-graph are used to generate the textual explanations for the decision-making process of the LMs. The corresponding nodes and edges here are identified as the \emph{reason-elements}.


\subsection{Attention-aware Explanation Generation} \label{sec:explanation}
In prior work, \citeauthor{chen2021kace} proposed a counterfactual-based explanation generator that pairs input text with counterfactual examples to fine-tune the LM to generate explanations in the format of ``why A and not B''. However, this approach only provides a possible explanation that may reveal the decision-making process, rather than faithfully interpreting the inner workings of neural networks.


To generate more accurate explanations, 
% our generated explanations are based on the final answer, corresponding nodes and edges of reason-elements. 
our explanation generator consists of two steps: explanation component extraction and instruction-based explanation generation.


\subsubsection{Explanation Component Extraction}
We first extract the key components that are essential to the decision-making process of the LMs. These key components consist of the final answer, \emph{reason-elements}, and attention weights ($\alpha$) obtained in Section~\ref{sec:interpretation}. The final answer and \emph{reason-elements} are used to trace the important explanation nodes, and the attention weights are used to sort the nodes and select the top $w$ nodes that are most relevant to the decision-making. Each node represents an element, so we have $w$ most important components for the explanation. We use $\mathcal{K}$ to represent the set of extracted key components. We denote the output by $E$. $E$ is a natural language explanation. 


\subsubsection{Instruction-based Explanation Generation}
We integrate the key component set $\mathcal{K}$ into our instruction-based explanation generator. To guide the generation of explanations, we use a set of predefined structures including the input context $z$, model predicted output $y'$, the trigger sentences, and the extracted key components $\mathcal{K}$. 
Our explanation generation has two stages: (1) \emph{why-choose} for explaining why the model chose the specific answer, and (2) \emph{why-not-choose} for explaining why the model did not choose the other explanations. In the \emph{why-choose} stage, we use instructions of the form ``Q: [$z$], A: [$y'$], R: [$\mathcal{K}$]''. We denote the output \emph{why-choose} explanation by $E_0$. In the \emph{why-not-choose} stage, we use instructions of the form ``P: [$E_0$], T: [$A\setminus\{y'\}$]''. Q, A, R, P and T are instructions for generating. We use the GPT-3.5-turbo \citep{ouyang2022training} model to provide literal explanations of the reasoning process of the LMs. The generator outputs a natural language explanation in the form of a sentence or a paragraph. The details of our instruction are shown in Appendix~\ref{sec:instruction}.  
% These elements are then passed to Stage 1, which produces the ``why choose'' explanation. Once the ``why choose'' explanation is obtained, it is used to generate the corresponding ``why not choose'' explanation.


\subsection{Learning and Inference} \label{sec:inference}
In our task, each question $q$ is associated with a set of answer choices $\mathcal{A}$, with only one being the correct answer. We utilize the information from LM embedding and interpretation embedding. Specifically, we define the probability of choosing an answer with $P(a|q)\propto exp(MLP(\mathbb{H}^{LM}, \mathbb{H}^{itp}))$, where $\mathbb{H}^{itp} = h_{V_e}^k$, and $\mathbb{H}^{LM}$ is the representation embedding from LM. 
We optimize the model by using the cross-entropy loss. 

\section{Experiments}

\subsection{Dataset}

In our experiments, we use the CommonsenseQA \citep{talmor2019commonsenseqa} and OpenBookQA \citep{mihaylov2018can} datasets to evaluate the performance of the candidate approaches. CommonsenseQA consists of 12,247 questions created by crowd-workers, which are designed to test commonsense knowledge through a 5-way multiple-choice QA task. OpenBookQA consists of 5,957 questions each requiring the task of 4-way multiple choice question answering. The questions are designed to assess the ability of models to reason with elementary science knowledge.

\subsection{Baselines}
Our evaluation can be divided into two parts. In the first part, we focus on model performance.
We compare LMExplainer with three sets of baseline models on the CommonsenseQA and OpenBookQA datasets. The first set of baseline models consists of fine-tuned LM RoBERTa-large \citep{liu2019roberta}, which demonstrates the capabilities of language models without interpretation. The second set of baseline models includes KG-augmented versions of RoBERTa-large, using ConceptNet as the source of common sense knowledge and following the approach in \citep{lin2019kagnet}. The third set of baseline models is the current SOTA commonsense reasoning method on CommonsenseQA, MHGRN \citep{feng2020scalable}, QA-GNN \citep{yasunaga2021qagnn}, GreaseLM \citep{zhang2022greaselm}. The LMs we used are from Huggingface\footnote{https://huggingface.co/}.

In the second part, we evaluate LMExplainer on explanation ability. To establish a baseline for comparison, two prior works, namely PathReasoner \citep{ZHAN2022107612} and Explanations for CommonsenseQA (ECQA) \citep{aggarwal-etal-2021-explanations}, were employed as benchmarks. These works are recognized for providing natural and comprehensible explanations. 

\subsection{Experimental Settings}
We set our GNN module to have 200 dimensions and 5 layers, where a dropout rate of 0.2 was applied to each layer. We trained the model using the RAdam optimizer on a single NVIDIA A100 GPU. A batch size of 64 was employed during the training process, and the learning rate for the language model and the GNN module were set to $1e-5$ and $1e-3$, respectively. These settings were adopted in the first part of the evaluation to investigate the performance of the GNN module. 

We employ ConceptNet \citep{speer2017conceptnet} as our external knowledge source for CommonsenseQA and OpenBookQA. ConceptNet contains a vast amount of information with 799,273 nodes and 2,487,810 edges, which provides a valuable resource for improving the accuracy of QA systems. We extract a subgraph with a hop size of 3, and subsequently prune the obtained graph to retain only the top 200 nodes.



\subsection{Experimental Results}
We present our experimental results in Table \ref{tab:acc} and Table \ref{tab:bookacc}, where the accuracy of our proposed LMExplainer is evaluated on the CommonsenseQA and OpenBookQA datasets. Our empirical findings indicate that LMExplainer leads to consistent improvements in performance compared to existing baseline methods on both datasets. Specifically, the test performance on CommonsenseQA is improved by 4.71\% over the prior best LM+KG method, GreaseLM, 5.35\% over the included KG augmented LMs, and 7.12\% over fine-tuned LMs. The test performance achieves comparable results to the prior best LM+KG method, GreaseLM, on OpenBookQA. 
It is worth noting that GreaseLM is specifically designed to improve accuracy for QA tasks, while our LMExplainer focuses on providing explanations for the reasoning process. Despite this difference in focus, our LMExplainer not only offers insight into the underlying reasoning but also demonstrates an improvement in performance. This finding highlights the potential benefits of incorporating explainability into the model design, as it may lead to enhanced performance in addition to fostering a better understanding of the decision-making process.

\begin{table}[th]
\centering
\resizebox{\columnwidth}{!}{%
    \begin{tabular}{lrr}
        \toprule
        \textbf{Method} & \textbf{IHdev-Acc.} & \textbf{IHtest-Acc.} \\
        \midrule
        \textbf{Baselines} \citep{feng2020scalable} &  & \\
        MHGRN (2020) & 73.69\% & 71.08\% \\
        KagNet (2019) & 73.47\% & 69.01\% \\
        GconAttn (2019) & 72.61\% & 68.59\% \\
        RGCN (2018) & 72.69\% & 68.41\% \\
        RN (2017) & 74.57\% & 69.08\% \\
        \midrule
        \textbf{Baselines} (our implementation) &  & \\
        GreaseLM (2022) & 76.17\% & 72.60\% \\
        QA-GNN (2021) & 74.94\% & 72.36\% \\
        \midrule
        LMExplainer (ours) & \textbf{77.97\%} & \textbf{77.31\%} \\
        \bottomrule
    \end{tabular}
    }
    \caption{Performance comparison of our proposed LMExplainer model against various baselines on Commonsense QA in-house split. Our model outperforms all the other methods, achieving 77.97\% and 77.31\% accuracy on IHdev and IHtest, respectively.
    As the official test is hidden, here we report the in-house Dev (IHdev) and Test (IHtest) accuracy, following the data split of \citep{lin2019kagnet}.}
    \label{tab:acc}
\vspace{-15pt}
\end{table}


\begin{table}[h!]
\centering
\resizebox{\columnwidth}{!}{%
    \begin{tabular}{lrr}
        \toprule
        \textbf{Method} & \textbf{Dev-Acc.} & \textbf{Test-Acc.} \\
        \midrule
        \textbf{Baselines} \citep{feng2020scalable} &  & \\
        MHGRN (2020) & 68.10\% & 66.85\% \\
        GconAttn (2019) & 64.30\% & 61.90\% \\
        RGCN (2018) & 64.65\% & 62.45\% \\
        RN (2017) & 67.00\% & 65.20\% \\
        \midrule
        \textbf{Baselines} (our implementation) &  & \\
        GreaseLM (2022) & \textbf{71.80\%} & \textbf{70.80\%} \\
        QA-GNN (2021) & 63.00\% & 59.80\% \\
        \midrule
        LMExplainer (ours) & 69.20\% & 68.00\% \\
        \bottomrule
    \end{tabular}
    }
    \caption{Performance comparison of our proposed LMExplainer model against various baselines on OpenBookQA. Our LMExplainer model exhibits competitive performance in relation to the top-performing model, GreaseLM. It is worth noting that GreaseLM is specifically tailored to enhance accuracy for QA tasks, whereas our LMExplainer model emphasizes providing explanations for the underlying reasoning process.
    We use the official data splits. } 
    \label{tab:bookacc}
\vspace{-10pt}
\end{table}

\subsection{Explanation Results}

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}c|l@{}}
\toprule
\textbf{Input Questions} & \begin{tabular}[c]{@{}l@{}}Q: What is someone doing if he or she is sitting quietly and his or her eyes are moving?\\ A. reading B. meditate C. fall asleep D. bunk E. think\end{tabular} \\ \midrule
\textbf{Label} & A. reading \\ \midrule
 & \textbf{Results of Our Approach - LM} \\ \midrule
\textbf{Ranked Reason-elements} & \begin{tabular}[c]{@{}l@{}}1. quiet chattering mind, 2. not making sound, 3. mind focuses, \\ 4. glasses for people with poor eyesight, 5. war\end{tabular} \\ \midrule
\textbf{Explanation \fcolorbox{blue}{white}{(why-choose)}} & \begin{tabular}[c]{@{}l@{}}\fcolorbox{olive}{lightgreen}{Since} the person is described as sitting quietly and their eyes are moving, \\ \fcolorbox{olive}{lightgreen}{it is likely that} they are engaged in a visual activity. \\ \fcolorbox{teal}{lightblue}{Based on the keyword ``glasses for people with poor eyesight'',} \\ option ``A. reading'' is the most likely answer, \\ \fcolorbox{olive}{lightgreen}{as} reading is a common visual activity that requires focusing one's eyes on a page\\  and is often aided by glasses for people with poor eyesight.\end{tabular} \\ \midrule
\textbf{Explanation \fcolorbox{blue}{white}{(why-not-choose)} }& \begin{tabular}[c]{@{}l@{}}\fcolorbox{olive}{lightgreen}{The other options}, such as \fcolorbox{teal}{lightblue}{``B. meditate''} or \fcolorbox{teal}{lightblue}{``C. fall asleep''}, \\ involve closing one's eyes or having a still mind, \\ \fcolorbox{olive}{lightgreen}{so it is unlikely that} the person is doing either of those activities if their eyes are moving. \\ Similarly, \fcolorbox{teal}{lightblue}{``D. bunk''} and \fcolorbox{teal}{lightblue}{``E. think''} \fcolorbox{olive}{lightgreen}{do not seem to} be related to the visual activity of \\ having one's eyes move while sitting quietly.\end{tabular} \\ \midrule
 & \textbf{Explanation of Others} \\ \midrule
\textbf{\makecell{PathReasoner \\ \citep{ZHAN2022107612}}} & \begin{tabular}[c]{@{}l@{}}quietly [related to] quiet [at location] a library [used for] reading \\\end{tabular} \\ \midrule
\textbf{\makecell{ECQA\\\citep{aggarwal-etal-2021-explanations}}} & \begin{tabular}[c]{@{}l@{}} While meditating and sleeping, eyes don't move, eyes are closed. \end{tabular} \\ \bottomrule
\end{tabular}%
}
\caption{Explanation examples of LMExplainer, PathReasoner and ECQA. We show the different types of explanations, including ranked \emph{reason-elements}, \emph{why-choose} explanations and \emph{why-not-choose} explanations. The explanations for \emph{why-choose}, present the model reasoning process in a logical way, while for \emph{why-not-choose} show the model why does not choose other answers, which enhances the transparency and interpretability of the reasoning process for humans. We use green and blue to highlight the logical connectives and reasoning framework, respectively. The complete results of comparison methods are shown in Appendix (Table \ref{app:example}).}
\label{tab:explanation}
\vspace{-10pt}
\end{table*}

Our explanation results are in Table \ref{tab:explanation}. To further demonstrate the effectiveness of our approach, we compare it with two SOTA methods, PathReasoner \citep{zhan2022pathreasoner} and ECQA \citep{aggarwal-etal-2021-explanations}. PathReasoner utilizes structured information to explain the reasoning path, while ECQA first is created by human-annotated explanations and then leverages a generation model to organize the final explanation. In Table \ref{tab:explanation}, we present the inputs and results of our approach, which include ranked \emph{reason-elements} and explanations of the reasoning process. These examples highlight the ability of LMExplainer in generating comprehensive and interpretable explanations for the LMs. More examples are shown in Appendix \ref{sec:case}.

In comparison to PathReasoner explanations, which only provide structured reasoning paths that are non-informative and require manual selection of a specific path, our proposed approach not only offers a complete reasoning path but also provides a justification for the predicted answer. As illustrated in Table \ref{tab:explanation} and Table \ref{app:example}, PathReasoner presents four reasoning paths, including redundant paths, making it difficult to identify the faithful reasoning path. In contrast, our method provides a clear and concise natural language explanation for the chosen answer (\emph{why-choose} explanation), which greatly enhances the understandability and smoothness of the explanation. 

The ECQA consists of human-annotated explanations that provide highly accurate descriptions of the reasoning process. However, as shown in Table \ref{tab:explanation} and Table \ref{app:example}, its explanations are simply a combination of positive and negative examples provided by humans. While this approach can generate high-quality explanations from a human perspective, it fails to illustrate the actual decision-making process of the model. In contrast, the explanations generated by LMExplainer are not a mere combination of sentences but are inferred and logically derived. LMExplainer provides a more comprehensive and accurate depiction of the reasoning process and improves the overall interpretability and usefulness of the generated explanations. In addition, the \emph{why-not-choose} explanation explains why the model does not choose other answers, which gives people a better understanding of the model’s reasoning process and increases the transparency of the model. 
These results highlight the effectiveness of quantifying the influence of tokens on determining the decision-making process and provide a literal representation of the information flow during the inference process. This is important because it allows us to understand the rationale behind the decision-making process of the LM and identify key factors that contribute to its predictions.






\subsection{Ablation Studies}
Table \ref{ab:PLM}, Table \ref{ab:model-only} and Table \ref{ab:interpreting} summarize the ablation study, examining the impact of different components on the performance of our LMExplainer model. We evaluated the effects of varying LM sizes, knowledge components, and interpreting components on the CommonsenseQA IHdev and IHtest sets. 

\subsubsection{LM Size}  
\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lll@{}}
\toprule
LM        & IHdev-Acc. & IHtest-Acc. \\ \midrule
RoBERTa-large (final) & 77.97\%    & 77.31\%     \\
RoBERTa       & 66.26\%    & 63.01\%     \\ \bottomrule
\end{tabular}
}
\caption{Ablation study on the effect of LM size on model accuracy.}
\label{ab:PLM}
\vspace{-15pt}
\end{table}


Table \ref{ab:PLM} shows the impact of the size of LM on LMExplainer. We evaluate the performance of two different sizes: RoBERTa-large (340M parameters) and RoBERTa (110M parameters). Our results indicate that using a larger LM leads to a significant improvement in performance, with an increase of 11.71\% and 14.30\% on the IHdev and IHtest sets, respectively. 
% These findings suggest that the size of the LM plays a critical role in the performance of our model, and using a larger LM can result in better performance.


\subsubsection{Knowledge Component}
\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lll@{}}
\toprule
Method             & IHdev-Acc. & IHtest-Acc. \\ \midrule
RoBERTa-large only & 74.28\%    & 70.19\%     \\
RoBERTa only       & 62.65\%    & 60.27\%     \\ 
w/ external knowledge (final) & 77.97\%    & 77.31\%     \\ \bottomrule
\end{tabular}
}
\caption{Ablation study on the effect of knowledge component on model accuracy.}
\label{ab:model-only}
\vspace{-15pt}
\end{table}

Table \ref{ab:model-only} shows the impact of the knowledge component on LMExplainer. 
We compare the performance of the LM-only model with and without external knowledge from ConceptNet. 
\textit{Model only} means we only use the LM to predict the answer. \textit{W/ external knowledge} means we incorporate the external knowledge. We observe that incorporating external knowledge can significantly improve the accuracy of prediction, especially on the test set. By incorporating the knowledge, the accuracy of IHdev and IHtest is increased by at least 3.69\% and 7.12\%, respectively. 
% This shows that external knowledge plays an important role in enhancing the reasoning ability of the model. 


\subsubsection{Interpreting Component}
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lll@{}}
\toprule
Method                & IHdev-Acc. & IHtest-Acc. \\ \midrule
RoBERTa-large w/o itp & 73.05\%    & 71.96\%     \\
RoBERTa w/o itp       & 68.63\%    & 64.54\%     \\ 
RoBERTa-large w/ itp (final)    & 77.97\%    & 77.31\%     \\ \bottomrule
\end{tabular}
}
\caption{Ablation study on the effect of interpreting component on model accuracy.}
\label{ab:interpreting}
\vspace{-15pt}
\end{table}

In Table \ref{ab:interpreting}, we analyze the impact of the interpreting component on the model's performance. \textit{Model w/o itp} indicates that the interpreting component was not incorporated in the prediction, whereas the \textit{Model w/ itp} indicates its presence. We observe that removing the interpreting component leads to a clear decrease in accuracy of at least 4.92\% and 5.35\% on IHdev and IHtest, respectively. Furthermore, comparing the results of \textit{RoBERTa-large only}, \textit{RoBERTa-large w/o itp}, and \textit{Final}, we find that the interpreting component has a greater impact on accuracy than the other components.


The ablation highlights the positive contributions of each component of our method. Specifically, we find that the interpreting component has a crucial role in our method's accuracy and generalizability on unseen questions.



\section{Conclusion}
In this paper, we propose LMExplainer, a novel model that incorporates an interpretation module to enhance the performance of LMs while also providing clear and trustworthy explanations of the model's reasoning. 
% Our model utilizes both interpretation and explanation to achieve these goals. 
Our explanation results are presented in a logical and comprehensive manner, making it easier for humans to understand the model's reasoning in natural language. Our experimental results demonstrate superior performance compared to prior SOTA works across standard datasets in the commonsense domain. Our analysis shows that LMExplainer not only improves the model's performance but also provides humans with a better understanding of the model.



\bibliography{anthology}
\bibliographystyle{acl_natbib}

\appendix

\section{Appendix}
\subsection{Other Explanation Examples}
We demonstrate the complete explanation example of PathReasoner and ECQA in Table \ref{app:example}. These methods exhibit in an unclear and intricate manner. Such explanations make it hard for humans to understand the decision-making process behind the model. 


\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}c|l@{}}
\toprule
\textbf{Input Questions} & \begin{tabular}[c]{@{}l@{}}Q: What is someone doing if he or she is sitting quietly and his or her eyes are moving?\\ A. reading B. meditate C. fall asleep D. bunk E. think\end{tabular} \\ \midrule
\textbf{Label} & A. reading \\
\midrule
 & \textbf{Explanation of Others} \\ \midrule
\textbf{\makecell{Path-\\Reasoner }} & \begin{tabular}[c]{@{}l@{}}quietly [related to] quiet [at location] a library [used for] reading \\ eyes [used for] reading \\ eyes [form of] eye [related to] glasses [used for] reading \\ sitting [related to] sit [related to] relaxing [has subevent] reading \\\end{tabular} \\ \midrule
\textbf{\makecell{ECQA \\}} & \begin{tabular}[c]{@{}l@{}}\textbf{Positive examples:} \\ - When we read, our eyes move. \\ - While reading, a person sits quietly, \\ \textbf{Negative examples: }\\ - While meditating, eyes don't move, eyes are closed, \\ - While sleeping, eyes are closed and they don't move, \\ - When a person bunks, he/she doesn't sit quietly, \\ - Eyes don't move when you think about something. \\ \textbf{Explanation:} \\ When we read, our eyes move. \\ While reading, a person sits quietly. \\ While meditating and sleeping, eyes don't move, eyes are closed. \\ When a person bunks, he/she doesn't sit quietly. \\ Eyes don't move when you think about something. \\\end{tabular} \\ \bottomrule
\end{tabular}%
}
\caption{The complete explanation examples of PathReasoner and ECQA.}
\label{app:example}
\end{table}


\subsection{Implementation Details} \label{sec:instruction}
Due to the constraints of space, we present the specific details of our explanation generator here.

For explanation generation, the example instruction we used in the first stage is \textit{Q=``Question context is''}, \textit{A=``The predicted choice is''}, \textit{R=``According to the model top reason-elements'' + $\mathcal{K}$ + ``explain the model reasoning process with ``since..., ....''''}, $\mathcal{K}$ is the top reason-elements of the model. In the second stage, \textit{P=``According to''}, and \textit{T=``explain why the model doesn't choose other options with ``The other potential choices''''}. 

\subsection{Case Study} \label{sec:case}

In this section, we will present more explanation examples generated by LMExplainer, to demonstrate the effectiveness of our method. These questions are randomly selected from the dataset. By sharing these examples, we aim to highlight the broad applicability and robust performance of LMExplainer across diverse contexts. These examples not only illustrate the adaptability of our approach but also substantiate its validity by demonstrating consistent results. 

\textbf{[Example A]}

\textbf{Input Question:} Where could there by tens of thousands of people playing baseball? 

A. America B. Canada C. Pitcher's mitt D. Park E. Sporting goods store

\textbf{Label:} A. America

\textbf{Ranked Reason-elements:} 1. bronx new york, 2. amharic, 3. baseball game, 4. pitcher's mitt, 5. thousand and one.

\textbf{Why-choose explanation:} \\
\fcolorbox{olive}{lightgreen}{Since} \fcolorbox{teal}{lightblue}{the reason-elements bronx new york,} 
\fcolorbox{teal}{lightblue}{baseball game, and thousand and one} are associated with American baseball culture and history, and pitcher's mitt is specifically related to playing baseball, \fcolorbox{olive}{lightgreen}{the model predicted} that the answer is America where there could be tens of thousands of people playing baseball.

\textbf{Why-not-choose explanation:} \\
\fcolorbox{olive}{lightgreen}{The other potential choices}, \fcolorbox{teal}{lightblue}{Canada, park, and} 
\fcolorbox{teal}{lightblue}{sporting goods store}, are \fcolorbox{olive}{lightgreen}{unlikely because} these locations do not have the same historical significance or cultural association with baseball as America. \fcolorbox{olive}{lightgreen}{Additionally}, the word \fcolorbox{teal}{lightblue}{pitcher's mitt} suggests an outdoor area where people can play baseball, making it \fcolorbox{olive}{lightgreen}{even less likely} that a sporting goods store would be the correct answer.


\textbf{[Example B]}

\textbf{Input Question:} What would you see if there were different types of flowers near you?

A. Many colors B. Smell good C. Cover fields D. Smell bad E. Continue to grow

\textbf{Label:} A. Many colors

\textbf{Ranked Reason-elements:} 1. plant themselves, 2. flower arrangement, 3. pair of shoes, 4. female reproductive part of flower, 5. wisconsin product.

\textbf{Why-choose explanation:} \\
\fcolorbox{olive}{lightgreen}{Since} the model identified \fcolorbox{teal}{lightblue}{flower} \fcolorbox{teal}{lightblue}{arrangement} and \fcolorbox{teal}{lightblue}{female reproductive part of the flower} as top reasons, it \fcolorbox{olive}{lightgreen}{suggests} that the presence of different types of flowers \fcolorbox{olive}{lightgreen}{would likely} result in a variety of colors and arrangements of petals, which aligns with the predicted option of many colors.

\textbf{Why-not-choose explanation:} \\
\fcolorbox{olive}{lightgreen}{The other potential choices}, such as \fcolorbox{teal}{lightblue}{smell good} and \fcolorbox{teal}{lightblue}{smell bad}, are excluded \fcolorbox{olive}{lightgreen}{because} the model did not identify any relevant keywords related to scent. \fcolorbox{olive}{lightgreen}{Similarly}, \fcolorbox{teal}{lightblue}{cover fields} and continue to grow are not directly related to the presence of different types of flowers, which is the focus of the question.

\textbf{[Example C]}

\textbf{Input Question:} Why does someone engage in chatting with friends when they want to workout?

A. Fun B. Agreement C. Cell phone D. Answers E. Inspiration

\textbf{Label:} E. Inspiration

\textbf{Ranked Reason-elements:} 1. assemble cleaning materials, 2. gain, 3. feeling of comfort, 4. chatoyance, 5. walking.

\textbf{Why-choose explanation:} \\
\fcolorbox{olive}{lightgreen}{Since} the words \fcolorbox{teal}{lightblue}{gain} and \fcolorbox{teal}{lightblue}{feeling of comfort} \fcolorbox{olive}{lightgreen}{suggest} a desire to improve oneself during a workout, and \fcolorbox{teal}{lightblue}{assemble cleaning materials} and \fcolorbox{teal}{lightblue}{walking} \fcolorbox{olive}{lightgreen}{imply} a pre-workout routine, the model predicts that the reason for engaging in chatting with friends during a workout is inspiration, \fcolorbox{olive}{lightgreen}{as} it can provide motivation and support to achieve fitness goals.

\textbf{Why-not-choose explanation:} \\
\fcolorbox{olive}{lightgreen}{The other potential choices}, such as \fcolorbox{teal}{lightblue}{fun, agree-} \fcolorbox{teal}{lightblue}{ment}, and \fcolorbox{teal}{lightblue}{cell phone}, are \fcolorbox{olive}{lightgreen}{unlikely} reasons for engaging in chatting during a workout \fcolorbox{olive}{lightgreen}{because} they do not offer a clear connection to exercise. \fcolorbox{olive}{lightgreen}{Additionally}, they do not address the underlying motivation for the workout or the desire to improve oneself. \fcolorbox{olive}{lightgreen}{Similarly}, the word answers does not align with the context of working out and could be interpreted in various ways, making it an improbable option.

\end{document}
