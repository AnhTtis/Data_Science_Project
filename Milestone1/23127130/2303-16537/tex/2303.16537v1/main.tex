% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath,bm,amssymb}
% \usepackage[dvipsnames]{xcolor}  
\usepackage{makecell}


\definecolor{lightgreen}{HTML}{D5DE56}
\definecolor{lightblue}{HTML}{D1E9EC}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{LMExplainer: a Knowledge-Enhanced Explainer for Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Zichen Chen, Ambuj K Singh, Misha Sra\\
  University of California, Santa Barbara \\
  \texttt{\{zichen\_chen, ambuj, sra\}@ucsb.edu}
  }

\begin{document}
\maketitle
\begin{abstract}

Large language models (LMs) such as GPT-4 are very powerful and can process different kinds of natural language processing (NLP) tasks. However, it can be difficult to interpret the results due to the multi-layer nonlinear model structure and millions of parameters. Lack of understanding of how the model works can make the model unreliable and dangerous for everyday users in real-world scenarios. 
Most recent works exploit the weights of attention to provide explanations for model predictions. However, pure attention-based explanation is unable to support the growing complexity of the models, and cannot reason about their decision-making processes. 
Thus, we propose LMExplainer, a knowledge-enhanced interpretation module for language models that can provide human-understandable explanations. We use a knowledge graph (KG) and a graph attention neural network to extract the key decision signals of the LM. 
We further explore whether interpretation can also help AI understand the task better. Our experimental results show that LMExplainer outperforms existing LM+KG methods on CommonsenseQA and OpenBookQA. We also compare the explanation results with generated explanation methods and human-annotated results. The comparison shows our method can provide more comprehensive and clearer explanations. LMExplainer demonstrates the potential to enhance model performance and furnish explanations for the reasoning processes of models in natural language.

\end{abstract}


\section{Introduction}

Pre-trained language models (PLMs) have recently gained significant attention due to their impressive performance on various natural language processing tasks \citep{brown2020language,liu2023pre,weifinetuned,zhou2022learning}. PLMs are a type of Artificial Intelligence (AI) that is trained on large amounts of data in order to perform various natural language processing tasks. These tasks can include language translation \citep{conneau2019cross}, text generation \citep{mireshghallah2022mix}, and text classification \citep{raffel2020exploring}, among others. One of the main advantages of PLMs is their ability to capture the nuances and the complexity of human languages, allowing them to achieve state-of-the-art performance on different tasks \citep{li2022quantifying}. 

However, a major limitation of PLMs is the lack of interpretability \citep{meng2022interpretability}, as they are unable to provide explanations for their decision-making processes. The decision-making process is often referred to as a ``black box''. PLMs use techniques such as attention mechanisms, which allow the model to focus on specific parts of the input data when making decisions \citep{vaswani2017attention, devlin2019bert,liu2019roberta}. These mechanisms are not transparent and are difficult for humans to interpret \citep{jain2019attention}. For example, model's embedding may capture relationships and meanings that are not immediately apparent to humans due to transmission through millions of neurons. This lack of interpretability poses a challenge in critical domains, such as healthcare \citep{loh2022application} and online education \citep{zytek2022need}, as it limits the trust that users can place in the inference made by the models. In addition to explainability, improving model interpretability can help address issues of fairness, privacy, and safety. Therefore, exploring methods that explain the behaviors of PLMs can help overcome the black-box nature of neural networks. 


Many recent approaches have focused on providing model-intrinsic and post-hoc methods to address the challenge of model explanations \citep{ribeiro2016should,shrikumar2017learning,vstrumbelj2014explaining,ying2019gnnexplainer,tigas2022interventions,situ-etal-2021-learning}. Model-intrinsic methods consider models that are considered interpretable due to their simple structure, such as linear models. Post-hoc methods provide explanations that are obtained after the model training, such as feature selection. 
Interpreting and explaining the decision-making process of PLMs is a challenging task. To overcome this challenge, \citeauthor{thorne2019generating} propose the Multiple Instance Learning method for generating token-level explanations for natural language inference (NLI), which is based on the attention matrix of a neural model. \citeauthor{chen2021kace} utilize counterfactual examples to generate contrastive explanations. \citeauthor{zhan2022pathreasoner} incorporate structured information from external knowledge to explain the model. However, these works only provide simple textual explanations, neglecting the reasoning process and thereby, do not provide a complete understanding of the model's reasoning process.
We argue that in order to provide more informative explanations that allow humans to understand and trust the model in crucial domains, it is necessary to not only provide insight into how the model reasons but also offer textual explanations. By doing so, we can help humans to gain a more comprehensive understanding of the decision-making process and interpret the model's output in a human-readable format.


In this paper, we present LMExplainer, a novel approach for explaining the predictions made by large PLMs. Our approach uses retrieved knowledge and Graph Attention Networks (GAT), and is able to provide explanations of the rationale behind the PLM's predictions. As an example, we apply our approach to the task of question answering (QA). 
In addition to addressing the challenge of explaining the decision-making process, we also investigate the potential impact of interpretation on model performance. Specifically, we explore the potential of using explanations to serve a dual purpose: helping humans in comprehending the model, and enhancing the model's understanding of the task at hand through interpretation during the explanation process. In this paper, explanation refers to explaining the model's decision-making in a human-understandable way, while interpretation refers to understanding the internal workings of the model.



We evaluate the effectiveness of LMExplainer on the CommonsenseQA \citep{talmor2019commonsenseqa} and OpenBookQA datasets \citep{mihaylov2018can}. Our experimental results demonstrate that LMExplainer outperforms state-of-the-art LM+KG QA methods on CommonsenseQA, while exhibiting competitive performance on OpenBookQA. These outcomes indicate that our method can enhance overall performance. Furthermore, we demonstrate that LMExplainer is capable of providing valuable reasoning insights for humans in a natural manner, surpassing prior explanation methods. To the best of our knowledge, LMExplainer is the ﬁrst work to utilize graph-based knowledge in generating natural language explanations to comprehend the rationale behind model behaviors.




\section{Related Work}

\subsection{Post-hoc Explanation}
Post-hoc explanation methods have gained significant attention in NLP research in recent years. These include feature-based, concept-based and example-based explanations. One popular approach of feature-based methods is called LIME \citep{ribeiro2016should}. They generate explanations by approximating the original model with a local sample. The model then uses the approximations to highlight the most important features for a given prediction. LIME is extended by \citeauthor{guidotti2018local}, which uses a decision tree classifier to approximate non-linear models. However, they cannot guarantee that the approximations are accurate representations of the original model due to inherent limitations. 
Compared to feature-based explanations, the concept-based approaches use single or multiple phrases to explain the model, which is more understandable to humans. \citeauthor{thorne2019generating} generate tokens of classifiers operating on pairs of sentences, while \citeauthor{yu2022towards} generate \emph{aspects} as explanations for search results. Example-based approaches similarly explain using natural language. \citeauthor{kumar-talukdar-2020-nile} use positive labels to generate candidate explanations, while \citeauthor{chen2021kace} use contrastive examples in the format of ``Why A not B" to distinguish between confusing candidates.  Different from prior work, we integrate the reasoning features and concepts to explain the model's behavior.



\subsection{Large Language Models}
Language models are used in a wide range of tasks across NLP, such as sentiment analysis, machine translation, and question answering. 
Conventional n-gram language models \citep{brown-etal-1992ngramLM} are based on conditional probability, and are interpretable. Recently, large language models such as RoBERTa \citep{liu2019roberta} and GPT-4 \citep{OpenAI2023GPT4TR}  have achieved impressive results. For example, they are able to generate coherent stories \citep{nye2021improving}, perform as AI teachers \citep{tack2022ai}, and chat with humans using natural language \citep{lin2020caire}. However, these models are often criticized for their lack of interpretability, which can hinder their adoption in real-world applications. 
Previous interpretable frameworks (\cite{ribeiro2016should}, \cite{mukund2017integratedgrad}, \cite{Smilkov2017SmoothGradRN}, \cite{ding2021saliencyNLP}) could be applied on the large language models, but they often rely on approximations and simplifications of the original models, which may cause discrepancies between the model and the explanation. \citeauthor{swamy2021interpreting} propose a framework to explain the language models during training, but they focus on the comparison metrics between models and different stages of the same model, and do not consider the effect of their methods on performance. In this paper, we explain language models by utilizing the model reasoning process, and evaluate the impact of the proposed method on model performance.

\subsection{Explainability with Knowledge-Graph}
Knowledge Graphs (KGs) are increasingly being used as a means to improve the interpretability and explainability of language models \citep{huang-etal-2022-deer, yasunaga2021qagnn, huang2019explainable, liu2019knowledge}. KGs are structured representations of knowledge, and can be used to capture the complex semantic relationships that are difficult to represent in traditional language models \citep{ji2021survey}. We divide KG-based methods into two types: graph-based and graph embedding-based. Graph-based explanation identifies the most relevant paths or subgraphs in a KG that support a given model prediction. \citeauthor{ZHAN2022107612} retrieve explainable reasoning paths from a KG and use path features to predict the answers. However, their path explanations are difficult for humans to understand. Graph embedding-based explanation uses an encoder to embed the structure and the relations of a graph, and then the embedding is used to generate explanations by identifying the most relevant nodes in the KG. \citeauthor{yasunaga2021qagnn} integrate the KG into the model, enabling the model to reason over structured knowledge and generate more interpretable predictions. 
However, these generated explanations may not consistently and accurately represent the model's reasoning, and they might be challenging for humans to comprehend because they are presented in a graph-based format.
By drawing upon the insights from prior works, we employ graph embedding as our fundamental component to generate explanations. 




\section{Task Definition}

In this paper, we aim to address two research questions related to the interpretability of PLMs: (1) how can we provide interpretable and naturally understandable explanations for the decision-making process of PLMs, and (2) how does the provision of explanations impact the performance of PLMs? 

We now define the task of generating reasoning-level explanation for inference made by PLMs. As an example, we use a QA task. Given a pre-trained PLM $f_{LM}$ with input context $z$ and predicted answer $a$, the goal is to generate an explanation $E$ for why $f_{LM}$ makes prediction $a$ for input context $z$. The explanation $E$ should provide insight into the reasoning behind the prediction and be presented in a format that is understandable to humans. This task can be written as:
\begin{equation}
    E \gets Generate\_Explanation(f_{LM}, z, a)
\end{equation}



\section{Approach}
The LMExplainer architecture is shown in Figure~\ref{fig:archi}. It consists of three main steps: \textbf{(1) key element extraction and building} (Section~\ref{sec:extraction}), \textbf{(2) element-graph interpretation} (Section~\ref{sec:interpretation}), and \textbf{(3) explanation generation} (Section~\ref{sec:explanation}). In the first step, we extract the relevant elements from the input data and the retrieved knowledge, and build an element-graph representation. In the second step, we use a Graph Attention Network (GAT) to interpret the element-graph and identify the reason-elements behind the model's prediction. Finally, in the third step, we use a prompt-based method to generate a textual explanation of the decision-making process based on the identified reasoning elements.
Our approach in LMExplainer is flexible and applicable to a range of PLMs, e.g., BERT \citep{devlin2019bert}, RoBERTa \citep{liu2019roberta}, and GPT-2 \citep{radford2019language}. 


\begin{figure*}[h]
  \begin{center}
    %\framebox[4.0in]{$\;$}
    %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
    \includegraphics[width=1\textwidth]{figure/archi.pdf}  
    \end{center}
  \caption{The LMExplainer architecture. Given a question context $z$ (which includes question $q$ and the set $\mathcal{A}$ of answers), we first generate language embeddings using a PLM. Simultaneously, it retrieves relevant knowledge from a KG to construct a subgraph (Sec. \ref{sec:extraction}). The language embeddings and subgraph are then combined to obtain GNN embeddings. This combined representation is then passed through a GAT to obtain the attention scores (Sec. \ref{sec:interpretation}). Our attention scores serve a dual purpose. Firstly, they weigh the importance of the GNN embeddings and are used with the language embeddings for the final prediction (Sec. \ref{sec:inference}). Secondly, they are used to generate explanations by highlighting the most important parts of the reasoning process (Sec. \ref{sec:explanation}). }
  \label{fig:archi}
\end{figure*}

\subsection{Key Elements Extraction and Building} \label{sec:extraction}

Certain key elements can significantly influence the reasoning process of PLMs. To capture these essential elements, we use the context of input data, represented as $z$. Each token $x$ in the input question $q$ is treated as a content element. We use $\mathcal{A}$ to denote the set of all candidate answers, and $a \in \mathcal{A}$ to denote one candidate answer.
Referring to Figure \ref{fig:archi}, the example demonstrates the $q$ and $a$ in ``Input Context [$z$]". 

We connect the context $z$ to each token $x$ and potential answer $a$ to construct a multi-relational graph, following the approach from \citeauthor{yasunaga2021qagnn}. Specifically, we incorporate external knowledge to construct the knowledge graph. The graph is constructed by retrieving paths in the knowledge graph from ConceptNet \citep{speer2017conceptnet}.  This allows us to prepare the essential elements that play a key role in the final results and analyze the relations among them. In addition to connecting relations in the knowledge graph, we also have two relationships to capture the content and the potential answers: one is the relationship between question $q$ and context $z$, and another is the relationship between answer $a$ and context $z$. We denote them with $r_{z,q}$ and $r_{z,a}$, respectively. Apart from this, we integrate the knowledge from PLMs as an instructive embedding to the KG. We denote a node in KG with $n$. The instructive embedding is added as a probability into the node embedding. The instructive score of $n$ can be computed as:
\begin{equation}
    \hat n _{score} = f_{score}(f_{enc}(n|z))
\end{equation}
\begin{equation}
    f_{score}=sigmoid(\text{MLP}(\cdot))
\end{equation}
where $f_{enc}$ is used to get the embedding from PLM, and $f_{score}$ is a multi-layer perceptron (MLP) followed by sigmoid activation. This score is used to instruct the KG to capture the node that is the key for reasoning. 


The constructed graph typically has a large number of paths at the initial building stage, which can lead to a large reasoning space. Since the score $\hat n$ can represent the correlation between the node $n$ and context $z$, we will utilize it to prune the graph by removing the irrelevant nodes. The extracted graph is our key element-graph $G_e$. 
% where each node can be represented as $(V_e, E_e)$. $V_e$ denotes the remaining element, while $E_e$ denotes its edges.


\subsection{Interpreting graph}  \label{sec:interpretation}

Given the element-graph, we follow \citep{yasunaga2021qagnn} to extract the representation for graph reasoning. The method is based on the Graph Attention Network (GAT) \citep{velickovic2018gat}, and uses a Graph Convolutional Network (GCN) approach \citep{kipf2017semi}. The nodes in the graph provide feature vectors, while the edges provide paths for the transfer and aggregation of features between nodes. In each layer of a GCN, a single node transfers its own feature vectors to all of its neighboring nodes and aggregates the feature vectors transmitted by its neighbors as its updated node features. This process of ``pass-aggregate-update" allows both GCN and GAT to preserve part of the structure and context of the original data through the connections between the nodes.



Given the element-graph $G_e$, for any node $V_e$, we define its neighbor node $s$, where $s \in \mathcal{N}(V_e)$, and $\mathcal{N}(V_e)$ refers to all the neighbors of $V_e$. We use $V_{es}$ to denote the connecting between node $V_e$ and node $s$.
The updated node feature $h^{k+1}_{V_e}$ is then calculated as shown in Equation \ref{eq:h_ve}.

\begin{equation}\label{eq:h_ve}
    h^{k+1}_{V_e} = f_{\delta}(\sum\limits_{s\in \mathcal{N}_{V_e} \cup \left\{ V_e \right\} }\alpha_{es}m_{es}) + h^{k}_{V_e}
\end{equation}
where $f_{\delta}$ is a two-layer MLP, and $\alpha_{es}$ and $m_{es}$ are the attention coefficients and the message, respectively. The node feature $h^{k+1}_{V_e}$ is the updated node feature at layer $k+1$ in our GNN, while $h^{k}_{V_e}$ is the node feature at layer $k$. After all the neighbor nodes of node $V_e$ in the $k$-th layer have passed information to node $V_e$, the feature of node $V_e$ in the $(k+1)$-th layer, $h^{k+1}_{V_e}$, is obtained through an additive aggregation and a non-linear transformation, in addition to the previous feature of itself. This process allows the GAT model to capture the structural and contextual information of the graph and use it to update the node features at each layer. The updated node features at the final layer are then used as the representation of the graph for reasoning.



In order to filter the important connections of the graph $G_e$, we incorporate attention weights $\alpha_{es}$ when aggregating the message passing $m_{es}$ from neighbor node $s$ \citep{velickovic2018gat}. The message passing $m_{es}$ is computed using the node type embedding $u_s$, its feature embedding $h^{k}_s$, and the relation embedding $r_{es}$ between nodes $V_e$ and $s$:
\begin{equation}
    m_{es} = f_{n}(h^{k}_s, u_s, r_{es})
\end{equation}
where $f_n$ is a linear transformation.
The relation embedding $r_{es}$ is calculated by a two-layer MLP $f_{\theta}$:
\begin{equation}
    r_{es} = f_{\theta} (\hat r_{es}, u_{es})
\end{equation}
where $\hat r_{es}$ is a one-hot embedding for the relation connecting $V_e$ and $s$, $u_{es}$ is the contacted node type embedding of $V_e$, and $s$.The attention weight $\alpha_{es}$ is calculated from the query vector $\boldsymbol{q}_s$ and key vector $\boldsymbol{k}_{e}$:
\begin{equation}
    \alpha_{es} = softmax(\frac{\boldsymbol{q}_{s}^{\top} \boldsymbol{k}_{e}}{\sqrt{D}})
\end{equation}
where $D$ refers to the feature dimension. The query and key vectors are derived from the element-graph as follows:
\begin{equation}
    \boldsymbol{q}_{s} = f_{q}(h^{k}_s, u_s, \hat{n}) 
\end{equation}
\begin{equation}
    \boldsymbol{k}_{e} = f_{k}(h^{k}_{e}, u_{e}, \hat{n}, r_{es})
\end{equation}
Both $f_{q}$ and $f_{k}$ are linear transformations. The attention weight $\alpha_{es}$ captures the importance of the connection between nodes $V_e$ and $s$, while the message passing $m_{es}$ captures the information being passed between the nodes. By multiplying these two quantities, the GAT model is able to filter the important connections in the graph and use them to update the node features. This process allows our method to discern the underlying rationale in decision-making processes, and obtain the interpretative embedding."


After the interpretation embedding has been obtained, we can generate explanations by extracting the most important nodes and edges in the graph. We use a linear transformation to map the final node features to the output space, and use a softmax function to compute the probability distribution over the potential answers. The most probable answers are selected as the final answer, and the corresponding nodes and edges in the element-graph are used to generate the textual explanations for the decision-making process of the PLMs.

\subsection{Attention-aware Explanation Generation} \label{sec:explanation}


In prior work, \citeauthor{chen2021kace} proposed a counterfactual-based explanation generator that pairs input text with qualified counterfactual examples to fine-tune the LM to generate explanations in the format of "why A and not B". However, this approach only provides a possible explanation that may reveal the reasoning behind the model's decision, rather than faithfully interpreting the inner workings of the neural network.


To generate more accurate explanations, we utilize a template-based approach that interprets the decision-making process of the LM, as described in Section \ref{sec:interpretation}. The generated explanations are based on the final answer, corresponding nodes of reason-elements, and edges in the interpretation. Our explanation generator consists of two steps: key explanation element extraction and prompt-based explanation generation.


\subsubsection{Explanation Components Extraction}
We first extract the key components that are essential to the decision-making process of the PLMs. These key components consist of the final answer, corresponding nodes of reason-elements, edges, and attention weights ($\alpha$) obtained in Section \ref{sec:interpretation}. The final answer, corresponding nodes, and edges are used to trace the important explanation nodes, and the attention weights are used to sort the nodes and select the top $w$ nodes that are most relevant to the decision-making. Each node represents an element, so we have $w$ most important components to interpret the explanation. We use $l$ to represent the extracted key component. We denote the output by $E$. $E$ is a natural language explanation.


\subsubsection{Prompt-based Explanation Generation}
We integrate the key component set $\{l\}$ into our prompt-based explanation generator. The prompt-based explanation generator is based on a set of predefined structures that guide the generation of explanations. The generator includes input context $z$, model predicted output $y'$, the trigger sentence $\mathrm{Z}$, and the extracted key components $\{l\}$. 
Our explanation generation has two stages: (1) why choose this explanation, (2) why not choose other explanations. In the ``why choose" stage, we generate the explanation for explaining why the model chose the specific answer. The template we used is ``Q: [$z$], A: [$y'$], T: [$\mathrm{Z}$], R: [$\{l\}$]". The output $E$ of the first stage is used in the second stage to explain why the model did not choose other answers. The template we used in the second stage is: ``P: [$E$], T: [$\mathrm{\hat Z}$]". We use the GPT-3.5-turbo \citep{ouyang2022training} model to provide a literal interpretation of the reasoning process of the PLMs. The output of the generator is a natural language explanation in the form of a sentence or a paragraph. As demonstrated in Figure \ref{fig:archi}, our approach uses the top 5 reason-elements. These elements are then passed to Stage 1, which produces the ``why choose" explanation. Once the ``why choose" explanation is obtained, it is used to generate the corresponding ``why not choose" explanation.


\subsection{Learning and Inference} \label{sec:inference}
In our task, each question $q$ is associated with a set of answer choices $\mathcal{A}$, with only one being the correct answer. We utilize the information from LM embedding and interpretation embedding. Specifically, we define the probability of choosing an answer with $P(a|q)\propto exp(MLP(\mathbb{H}^{LM}, \mathbb{H}^{itp}))$, where $\mathbb{H}^{itp} = h_{V}^K$, and $\mathbb{H}^{LM}$ is the representation embedding from LM. 
We optimize the model by using the cross-entropy loss. 

\section{Experiments}

\subsection{Dataset}

In our experiments, we use the CommonsenseQA \citep{talmor2019commonsenseqa} and OpenBookQA \citep{mihaylov2018can} datasets to evaluate the performance of the candidate approaches. CommonsenseQA consists of 12,247 questions created by crowd-workers, which are designed to test commonsense knowledge through a 5-way multiple choice QA task. OpenBookQA consists of 5,957 questions each requiring the task of 4-way multiple choice question answering. The questions are designed to assess the ability of models to reason with elementary science knowledge.

\subsection{Baselines}
Our evaluation can be divided into two parts. In the first part, we focus on model performance.
We compare LMExplainer with three sets of baseline models on the CommonsenseQA and OpenBookQA datasets. The first set of baseline models consists of fine-tuned language model RoBERTa-large \citep{liu2019roberta}, which demonstrates the capabilities of language models without interpretation. The second set of baseline models includes KG augmented versions of RoBERTa-large, using ConceptNet as the source of common sense knowledge and following the approach in \citep{lin2019kagnet}. The third set of baseline models is the current state-of-the-art common sense reasoning method on CommonsenseQA, MHGRN \citep{feng2020scalable}, QA-GNN \citep{yasunaga2021qagnn}, GreaseLM \citep{zhang2022greaselm}. The PLM we used is from Huggingface\footnote{https://huggingface.co/}.

In the second part, we evaluate LMExplainer on explanation ability. To establish a baseline for comparison, two prior works, namely PathReasoner \citep{ZHAN2022107612} and Explanations for CommonsenseQA \citep{aggarwal-etal-2021-explanations}, were employed as benchmarks. These works are recognized for providing natural and comprehensible explanations. 

\subsection{Experimental Settings}
We set our GNN module to have 200 dimensions and 5 layers, where a dropout rate of 0.2 was applied to each layer. We trained the model using the RAdam optimizer on a single NVIDIA A100 GPU, with a training process of approximately 3 hours. A batch size of 64 was employed during the training process, and the learning rate for the language model and the GNN module were set to 1e-5 and 1e-3, respectively. These settings were adopted in the first part of the evaluation to investigate the performance of the GNN module.

We employ ConceptNet \citep{speer2017conceptnet} as our external knowledge source for CommonsenseQA and OpenBookQA. ConceptNet contains a vast amount of information with 799,273 nodes and 2,487,810 edges, which provides a valuable resource for improving the accuracy of QA systems. We extract a subgraph with a hop size of 3, and subsequently prune the obtained graph to retain only the top 200 nodes.

For explanation generation, the example prompts we used in the first stage are \textit{Q=``Question context is"}, \textit{A=``The predicted choice is"}, \textit{T=``According to the model top reason-elements" + K; ``explain the model reasoning process with ``since..., ...."}, \textit{K} is the reason-elements of the model. In the second stage, \textit{P=``According to"}, and \textit{T=``explain why the model doesn't choose other answers"}. 

\subsection{Experimental Results}
We present our experimental results in Table \ref{tab:acc} and Table \ref{tab:bookacc}, where the accuracy of our proposed LM approach is evaluated on the CommonsenseQA and OpenBookQA datasets. Our empirical findings indicate that our approach leads to consistent improvements in performance compared to existing baseline methods on both datasets. Specifically, the test performance on CommonsenseQA is improved by 4.71\% over the prior best LM+KG method, GreaseLM, 5.35\% over the included KG augmented LMs, and 7.12\% over fine-tuned LMs. The test performance achieves comparable results to the prior best LM+KG method, GreaseLM, on OpenBookQA. 
It is worth noting that GreaseLM is specifically designed to improve accuracy for QA tasks, while our LMExplainer model focuses on providing explanations for the reasoning process. Despite this difference in focus, our LMExplainer model not only offers insight into the underlying reasoning but also demonstrates an improvement in performance. This finding highlights the potential benefits of incorporating explainability into the model design, as it may lead to enhanced performance in addition to fostering a better understanding of the decision-making process.

\begin{table}[th]
\centering
\resizebox{\columnwidth}{!}{%
    \begin{tabular}{lrr}
        \toprule
        \textbf{Method} & \textbf{IHdev-Acc.} & \textbf{IHtest-Acc.} \\
        \midrule
        \textbf{Baselines} \citep{feng2020scalable} &  & \\
        MHGRN (2020) & 73.69\% & 71.08\% \\
        KagNet (2019) & 73.47\% & 69.01\% \\
        GconAttn (2019) & 72.61\% & 68.59\% \\
        RGCN (2018) & 72.69\% & 68.41\% \\
        RN (2017) & 74.57\% & 69.08\% \\
        \midrule
        \textbf{Baselines} (our implementation) &  & \\
        GreaseLM (2022) & 76.17\% & 72.60\% \\
        QA-GNN (2021) & 74.94\% & 72.36\% \\
        \midrule
        LMExplainer (ours) & \textbf{77.97\%} & \textbf{77.31\%} \\
        \bottomrule
    \end{tabular}
    }
    \caption{Performance comparison of our proposed LMExplainer model against various baselines on Commonsense QA in-house split. Our model outperforms all the other methods, achieving 77.97\% and 77.31\% accuracy on IHdev and IHtest, respectively.
    As the official test is hidden, here we report the in-house Dev (IHdev) and Test (IHtest) accuracy, following the data split of \citep{lin2019kagnet}.}
    \label{tab:acc}
\end{table}


\begin{table}[h!]
\centering
\resizebox{\columnwidth}{!}{%
    \begin{tabular}{lrr}
        \toprule
        \textbf{Method} & \textbf{Dev-Acc.} & \textbf{Test-Acc.} \\
        \midrule
        \textbf{Baselines} \citep{feng2020scalable} &  & \\
        MHGRN (2020) & 68.10\% & 66.85\% \\
        GconAttn (2019) & 64.30\% & 61.90\% \\
        RGCN (2018) & 64.65\% & 62.45\% \\
        RN (2017) & 67.00\% & 65.20\% \\
        \midrule
        \textbf{Baselines} (our implementation) &  & \\
        GreaseLM (2022) & \textbf{71.80\%} & \textbf{70.80\%} \\
        QA-GNN (2021) & 63.00\% & 59.80\% \\
        \midrule
        LMExplainer (ours) & 69.20\% & 68.00\% \\
        \bottomrule
    \end{tabular}
    }
    \caption{Performance comparison of our proposed LMExplainer model against various baselines on OpenBookQA. Our LMExplainer model exhibits competitive performance in relation to the top-performing model, GreaseLM. It is worth noting that GreaseLM is specifically tailored to enhance accuracy for QA tasks, whereas our LMExplainer model emphasizes providing explanations for the underlying reasoning process.
    We use the official data splits. } 
    \label{tab:bookacc}
\end{table}

\subsection{Explanation Results}

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}c|l@{}}
\toprule
\textbf{Input Questions} & \begin{tabular}[c]{@{}l@{}}Q: What is someone doing if he or she is sitting quietly and his or her eyes are moving?\\ A. reading B. meditate C. fall asleep D. bunk E. think\end{tabular} \\ \midrule
\textbf{Label} & A. reading \\ \midrule
 & \textbf{Results of Our Approach - LM} \\ \midrule
\textbf{Ranked Reasoning Keys} & \begin{tabular}[c]{@{}l@{}}1. quiet chattering mind, 2. not making sound, 3. mind focuses, \\ 4. glasses for people with poor eyesight, 5. war\end{tabular} \\ \midrule
\textbf{Explanation \fcolorbox{blue}{white}{(why choose)}} & \begin{tabular}[c]{@{}l@{}}\fcolorbox{olive}{lightgreen}{Since} the person is described as sitting quietly and their eyes are moving, \\ \fcolorbox{olive}{lightgreen}{it is likely that} they are engaged in a visual activity. \\ \fcolorbox{teal}{lightblue}{Based on the keyword "glasses for people with poor eyesight",} \\ option "A. reading" is the most likely answer, \\ \fcolorbox{olive}{lightgreen}{as} reading is a common visual activity that requires focusing one's eyes on a page\\  and is often aided by glasses for people with poor eyesight.\end{tabular} \\ \midrule
\textbf{Explanation \fcolorbox{blue}{white}{(why not choose)} }& \begin{tabular}[c]{@{}l@{}}\fcolorbox{olive}{lightgreen}{The other options}, such as \fcolorbox{teal}{lightblue}{"B. meditate"} or \fcolorbox{teal}{lightblue}{"C. fall asleep"}, \\ involve closing one's eyes or having a still mind, \\ \fcolorbox{olive}{lightgreen}{so it is unlikely that} the person is doing either of those activities if their eyes are moving. \\ Similarly, \fcolorbox{teal}{lightblue}{"D. bunk"} and \fcolorbox{teal}{lightblue}{"E. think"} \fcolorbox{olive}{lightgreen}{do not seem to} be related to the visual activity of \\ having one's eyes move while sitting quietly.\end{tabular} \\ \midrule
 & \textbf{Explanation of Others} \\ \midrule
\textbf{\makecell{PathReasoner \\ \citep{ZHAN2022107612}}} & \begin{tabular}[c]{@{}l@{}}quietly [related to] quiet [at location] a library [used for] reading \\ eyes [used for] reading \\ eyes [form of] eye [related to] glasses [used for] reading \\ sitting [related to] sit [related to] relaxing [has subevent] reading \\\end{tabular} \\ \midrule
\textbf{\makecell{Explanations for CommonsenseQA \\\citep{aggarwal-etal-2021-explanations}}} & \begin{tabular}[c]{@{}l@{}}\textbf{Positive examples:} \\ - When we read, our eyes move. \\ - While reading, a person sits quietly, \\ \textbf{Negative examples: }\\ - While meditating, eyes don't move, eyes are closed, \\ - While sleeping, eyes are closed and they don't move, \\ - When a person bunks, he/she doesn't sit quietly, \\ - Eyes don't move when you think about something. \\ \textbf{Explanation:} \\ When we read, our eyes move. \\ While reading, a person sits quietly. \\ While meditating and sleeping, eyes don't move, eyes are closed. \\ When a person bunks, he/she doesn't sit quietly. \\ Eyes don't move when you think about something. \\\end{tabular} \\ \bottomrule
\end{tabular}%
}
\caption{Explanation examples of LMExplainer, PathReasoner and Explanations for CommonsenseQA dataset. We show the different types of explanations, including ranked reasoning keys, explanations for why choose and explanations for why not choose. The explanations for \textit{``why choose`"}, presents the model reasoning process in a logical way, while for \textit{``not choose"} shows the model why does not choose other answers, which enhances the transparency and interpretability of the reasoning process for humans. We use green and blue to highlight the logical connectives and reasoning framework, respectively.}
\label{tab:explanation}
\end{table*}

Our explanation results in Table \ref{tab:explanation} provide evidence of the reasoning ability of our proposed LMExplainer. To further demonstrate the effectiveness of our approach, we compare it with two other state-of-the-art methods, PathReasoner \citep{zhan2022pathreasoner} and Explanations for CommonsenseQA \citep{aggarwal-etal-2021-explanations}. PathReasoner utilizes structured information to explain the reasoning path, while Explanations for CommonsenseQA first are created by human-annotated explanations and then leverage a generation model to organize the final explanation. In Table \ref{tab:explanation}, we present the inputs of our model, as well as the results of our approach, which include ranked reasoning keywords and natural language explanations of the reasoning process. These examples highlight the ability of our LMExplainer approach in generating comprehensive and interpretable explanations for the models.

In comparison to PathReasoner explanations, which only provide structured reasoning paths that are non-informative and require manual selection of a specific path, our proposed approach not only offers a complete reasoning path but also provides a justification for the predicted answer. As illustrated in Table \ref{tab:explanation}, PathReasoner presents four reasoning paths, including redundant paths, making it difficult to identify the  faithful reasoning path. In contrast, our method provides a clear and concise natural language explanation for the chosen answer (\textit{why choose} explanation), which greatly enhances the understandability and smoothness of the explanation. 

The Explanations for CommonsenseQA dataset consists of human-annotated explanations that provide highly accurate descriptions of the reasoning process. However, as shown in Table \ref{tab:explanation}, its explanations are simply a combination of positive and negative examples provided by humans. While this approach can generate high-quality explanations from a human perspective, it fails to illustrate the actual reasoning process of the model. In contrast, the explanations generated by LMExplainer are not a mere combination of sentences but are inferred and logically derived. Our approach provides a more comprehensive and accurate depiction of the reasoning process and improves the overall interpretability and usefulness of the generated explanations. In addition, the \textit{why not choose} explanation explains why the model does not choose other answers, which gives people a better understanding of the model’s predictions and increases the transparency of the model. 
These results highlight the effectiveness of quantifying the influence of tokens on determining the reasoning process and provide a literal representation of the information flow during the inference process. This is important because it allows us to understand the rationale behind the decision-making process of the LM and identify key factors that contribute to its predictions.






\subsection{Ablation Studies}
Table \ref{ab:PLM}, Table \ref{ab:model-only} and Table \ref{ab:interpreting} summarize the ablation study, examining the impact of different components on the performance of our LMExplainer model. We evaluated the effects of varying PLM sizes, knowledge components, and interpreting components on the CommonsenseQA IHdev and IHtest sets. 

\subsubsection{PLM Size}  
\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lll@{}}
\toprule
PLM        & IHdev-Acc. & IHtest-Acc. \\ \midrule
RoBERTa-large (final) & 77.97\%    & 77.31\%     \\
RoBERTa       & 66.26\%    & 63.01\%     \\ \bottomrule
\end{tabular}
}
\caption{Ablation study on the effect of PLM size on model accuracy.}
\label{ab:PLM}
\end{table}

Table \ref{ab:PLM} shows the impact of the size of PLM on our proposed method. We evaluate the performance of two different PLM sizes: RoBERTa-large (340M parameters) and RoBERTa (110M parameters). Our results indicate that using a larger PLM leads to a significant improvement in performance, with an increase of 11.71\% and 14.30\% on the IHdev and IHtest sets, respectively. These findings suggest that the size of the PLM plays a critical role in the performance of our model, and using a larger PLM can result in better performance.


\subsubsection{Knowledge Component}
\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lll@{}}
\toprule
Method             & IHdev-Acc. & IHtest-Acc. \\ \midrule
RoBERTa-large only & 74.28\%    & 70.19\%     \\
RoBERTa only       & 62.65\%    & 60.27\%     \\ 
w/ external knowledge (final) & 77.97\%    & 77.31\%     \\ \bottomrule
\end{tabular}
}
\caption{Ablation study on the effect of knowledge component on model accuracy.}
\label{ab:model-only}
\end{table}

Table \ref{ab:model-only} shows the impact of the knowledge component on our method. 
We compare the performance of the LM-only model with and without external knowledge from ConceptNet. 
\textit{Model only} means we only use the LM to predict the answer. \textit{W/ external knowledge} means we incorporate the external knowledge. We observe that incorporating external knowledge can significantly improve the accuracy of prediction, especially on the test set. By incorporating the knowledge, the accuracy of IHdev and IHtest is increased by at least 3.69\% and 7.12\%, respectively. This shows that external knowledge plays an important role in enhancing the reasoning ability of the model. 


\subsubsection{Interpreting Component}
\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lll@{}}
\toprule
Method                & IHdev-Acc. & IHtest-Acc. \\ \midrule
RoBERTa-large w/o itp & 73.05\%    & 71.96\%     \\
RoBERTa w/o itp       & 68.63\%    & 64.54\%     \\ 
RoBERTa-large w/ itp (final)    & 77.97\%    & 77.31\%     \\ \bottomrule
\end{tabular}
}
\caption{Ablation study on the effect of interpreting component on model accuracy.}
\label{ab:interpreting}
\end{table}

In Table \ref{ab:interpreting}, we analyze the impact of the interpreting component on the model's performance. \textit{Model w/o itp} indicates that the interpreting component was not incorporated in the prediction, whereas the \textit{Model w/ itp} row indicates its presence. We observe that removing the interpreting component leads to a clear decrease in accuracy of at least 4.92\% and 5.35\% on IHdev and IHtest, respectively. Furthermore, comparing the results of \textit{RoBERTa-large only}, \textit{RoBERTa-large w/o itp}, and \textit{Final}, we find that the interpreting component has a greater impact on accuracy than the other components.


The ablation highlights the positive contributions of each component of our method. Specifically, we find that the interpreting component has a crucial role in our method's accuracy and generalizability on unseen questions.



\section{Conclusion}
In this paper, we propose LMExplainer, a novel model that incorporates an interpretation module to enhance the performance of language models while also providing clear and trustworthy explanations of the model's reasoning. Our model utilizes both interpretation and explanation to achieve these goals. The explanation results are presented in a logical and comprehensive manner, making it easier for people to understand the model's reasoning in natural language. Our experimental results demonstrate superior performance compared to prior state-of-the-art works across standard datasets in the commonsense domain. Our analysis shows that LMExplainer not only improves the model's performance but also provides humans with a better understanding of the model.


\input{main.bbl}
\bibliography{main}
\bibliographystyle{acl_natbib}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is a section in the appendix.

\end{document}
