@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

% --- human pose estimation
@InProceedings{Cao_2017_CVPR,
author = {Cao, Zhe and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
title = {Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
} 
% --- GNN amd GCN
@inproceedings{kipf2017semi,
  title={Semi-Supervised Classification with Graph Convolutional Networks},
  author={Kipf, Thomas N. and Welling, Max},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017}
}

% --- Lei's TIP review paper
@ARTICLE{lei_tip_2019,
author={Lei Wang and Du Q. Huynh and Piotr Koniusz},
journal={TIP},
title={A Comparative Review of Recent Kinect-based Action Recognition Algorithms},
year={2019},
volume={},
number={},
pages={},
keywords={Human action recognition;Kinect-based algorithms;cross-view action recognition;3D action analysis},
doi={10.1109/TIP.2019.2925285},
ISSN={1057-7149},
month={}
}

@mastersthesis{lei_thesis_2017,
  author       = {Lei Wang}, 
  title        = {Analysis and Evaluation of {K}inect-based Action Recognition Algorithms},
  school       = {School of the Computer Science and Software Engineering, The University of Western Australia},
  year         = 2017,
  month        = {11}
}

@article{lei_icip_2019,
		title={Loss Switching Fusion with Similarity Search for Video Classification},
		journal={ICIP},
		author={Lei Wang and Du Q. Huynh and Moussa Reda Mansour},
		year={2019}
}

@InProceedings{Wang_2019_ICCV,
author = {Wang, Lei and Koniusz, Piotr and Huynh, Du Q.},
title = {Hallucinating {IDT} Descriptors and {I3D} Optical Flow Features for Action Recognition With CNNs},
booktitle = {ICCV},
year = {2019}
}


@InProceedings{costa_yifei,
author = {Yifei Zhang and Hao Zhu and Zixing Song and Piotr Koniusz and Irwin King},
title = {{COSTA}: Covariance-Preserving Feature Augmentation for Graph Contrastive Learning},
booktitle = {KDD},
year = {2022}
}

@InProceedings{sfa_yifei,
author = {Yifei Zhang and Hao Zhu and Zixing Song and Piotr Koniusz and Irwin King},
title = {Spectral Feature Augmentation for Graph Contrastive Learning and Beyond},
booktitle = {AAAI},
year = {2023}
}


@inproceedings{kon_tpami2020a,
title={Power Normalizations in Fine-grained Image, Few-shot Image and Graph Classification},
author={Piotr Koniusz and Hongguang Zhang},
booktitle={IEEE Transactions on Pattern Analysis and Machine Intelligence},
publisher={IEEE},
year={2020}
}

@inproceedings{kon_tpami2020b,
title={Tensor Representations for Action Recognition},
author={Piotr Koniusz and Lei Wang and Anoop Cherian},
booktitle={IEEE Transactions on Pattern Analysis and Machine Intelligence},
publisher={IEEE},
year={2020}
}

@inbook{10.1145/3474085.3475572,
author = {Wang, Lei and Koniusz, Piotr},
title = {Self-Supervising Action Recognition by Statistical Moment and Subspace Descriptors},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475572},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {4324–4333},
numpages = {10}
}

@inproceedings{udtw_eccv22,
  title={Uncertainty-DTW for time series and sequences},
  author={Wang, Lei and Koniusz, Piotr},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXI},
  pages={176--195},
  year={2022},
  organization={Springer}
}

@article{wang20213d,
  title={3D Skeleton-based Few-shot Action Recognition with JEANIE is not so Na\"ive},
  author={Wang, Lei and Liu, Jun and Koniusz, Piotr},
  journal={arXiv preprint arXiv:2112.12668},
  year={2021}
}

@article{qin_tnnls_22,
  author    = {Zhenyue Qin and Yang Liu and Pan Ji and Dongwoo Kim and Lei Wang and
               Bob McKay and Saeed Anwar and Tom Gedeon},
  title     = {Fusing Higher-Order Features in Graph Neural Networks for Skeleton-based Action Recognition},
  journal   = {IEEE TNNLS},
  year      = {2022}
} 

@inproceedings{Wang_2022_ACCV,
  title={Temporal-Viewpoint Transportation Plan for Skeletal Few-shot Action Recognition},
  author={Wang, Lei and Koniusz, Piotr},
  booktitle={Proceedings of the Asian Conference on Computer Vision},
  pages={4176--4193},
  year={2022}
}

% the use of hypergraph for skeleton data

@inproceedings{ijcai2020-109,
  title     = {Semi-Dynamic Hypergraph Neural Network for 3D Pose Estimation},
  author    = {Liu, Shengyuan and Lv, Pei and Zhang, Yuzhen and Fu, Jie and Cheng, Junjin and Li, Wanqing and Zhou, Bing and Xu, Mingliang},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Christian Bessiere},
  pages     = {782--788},
  year      = {2020},
  month     = {7},
  note      = {Main track},
  doi       = {10.24963/ijcai.2020/109},
  url       = {https://doi.org/10.24963/ijcai.2020/109},
}

@ARTICLE{9329123,
  author={Hao, Xiaoke and Li, Jie and Guo, Yingchun and Jiang, Tao and Yu, Ming},
  journal={IEEE Transactions on Image Processing}, 
  title={Hypergraph Neural Network for Skeleton-Based Action Recognition}, 
  year={2021},
  volume={30},
  number={},
  pages={2263-2275},
  doi={10.1109/TIP.2021.3051495}}

% ntu-60, ntu-120 and Kinetics dataset

@inproceedings{Shahroudy_2016_NTURGBD,
  author = {Shahroudy, Amir and Liu, Jun and Ng, Tian-Tsong and Wang, Gang},
  title = {NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
  month = {June},
  year = {2016}
}

@article{Liu_2019_NTURGBD120,
  title={NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding},
  author={Liu, Jun and Shahroudy, Amir and Perez, Mauricio and Wang, Gang and Duan, Ling-Yu and Kot, Alex C.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2019},
  doi={10.1109/TPAMI.2019.2916873}
}

@misc{kay2017kinetics,
      title={The Kinetics Human Action Video Dataset}, 
      author={Will Kay and Joao Carreira and Karen Simonyan and Brian Zhang and Chloe Hillier and Sudheendra Vijayanarasimhan and Fabio Viola and Tim Green and Trevor Back and Paul Natsev and Mustafa Suleyman and Andrew Zisserman},
      year={2017},
      eprint={1705.06950},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{stgcn2018aaai,
  title     = {{Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition}},
  author    = {Sijie Yan and Yuanjun Xiong and Dahua Lin},
  booktitle = {AAAI},
  year      = {2018},
}



@article{dynamichypergraph,
  author    = {Jinfeng Wei and
               Yunxin Wang and
               Mengli Guo and
               Pei Lv and
               Xiaoshan Yang and
               Mingliang Xu},
  title     = {Dynamic Hypergraph Convolutional Networks for Skeleton-Based Action
               Recognition},
  journal   = {CoRR},
  volume    = {abs/2112.10570},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.10570},
  eprinttype = {arXiv},
  eprint    = {2112.10570},
  timestamp = {Tue, 04 Jan 2022 15:59:27 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-10570.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@INPROCEEDINGS{8014941,

  author={Kim, Tae Soo and Reiter, Austin},

  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 

  title={Interpretable 3D Human Action Analysis with Temporal Convolutional Networks}, 

  year={2017},

  volume={},

  number={},

  pages={1623-1631},

  doi={10.1109/CVPRW.2017.207}}


@inproceedings{2sagcn2019cvpr,  
      title     = {Two-Stream Adaptive Graph Convolutional Networks for Skeleton-Based Action Recognition},  
      author    = {Lei Shi and Yifan Zhang and Jian Cheng and Hanqing Lu},  
      booktitle = {CVPR},  
      year      = {2019},  
}

@inproceedings{simon_cvpr2023,  
      title     = {Learning Partial Correlation based Deep Visual Representation for Image
Classification},  
      author    = {Saimunur Rahman and  Piotr Koniusz and Lei Wang and Luping Zhou and Peyman Moghadam and  Changming Sun},  
      booktitle = {CVPR},  
      year      = {2023},  
}

@inproceedings{tsungyu_eccv2018,  
      title     = {Second-order Democratic Aggregation},  
      author    = {Tsung-Yu Lin and Subhransu Maji and Piotr Koniusz},
      booktitle = {ECCV},  
      year      = {2018},  
}


@inproceedings{zhang2020sopaccv,
  title={Few-Shot Object Detection by Second-order Pooling},
  author={Shan Zhang and Dawei Luo and Lei Wang and Piotr Koniusz},
  booktitle=ACCV,
  year={2020}
}

@inproceedings{zhang2022kernelized,
  title={Kernelized few-shot object detection with efficient integral aggregation},
  author={Zhang, Shan and Wang, Lei and Murray, Naila and Koniusz, Piotr},
  booktitle=CVPR,
  year={2022}
}

@inproceedings{zhang2022time,
  title={Time-rEversed DiffusioN tEnsor Transformer: A New TENET of Few-Shot Object Detection},
  author={Zhang, Shan and Murray, Naila and Wang, Lei and Koniusz, Piotr},
  booktitle=ECCV,
  year={2022}
}

@InProceedings{10.1007/978-3-030-92270-2_2,
author="He, Changxiang
and Xiao, Chen
and Liu, Shuting
and Qin, Xiaofei
and Zhao, Ying
and Zhang, Xuedian",
editor="Mantoro, Teddy
and Lee, Minho
and Ayu, Media Anugerah
and Wong, Kok Wai
and Hidayanto, Achmad Nizar",
title="Single-Skeleton and Dual-Skeleton Hypergraph Convolution Neural Networks for Skeleton-Based Action Recognition",
booktitle="Neural Information Processing",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="15--27",
abstract="In the last several years, the graph convolutional networks (GCNs) have shown exceptional ability on skeleton-based action recognition. Currently used mainstream methods often include identifying the movements of a single skeleton and then fusing the features. But in this way, it will lose the interactive information of two skeletons. Moreover, since there are some interactions between people (such as handshake, high-five, hug, etc.), the loss will reduce the accuracy of skeleton-based action recognition. To address this issue, we propose a two-stream approach (SD-HGCN). On the basis of single-skeleton stream (S-HGCN), a dual-skeleton stream (D-HGCN) is added to recognizing actions with interactive information between skeletons. The model mainly includes a multi-branch inputs adaptive fusion module (MBAFM) and a skeleton perception module (SPM). MBAFM can make the input features more distinguishable through two GCNs and an attention module. SPM may identify relationships between skeletons and build topological knowledge about human skeletons, through adaptive learning of the hypergraph distribution matrix based on the semantic information in the skeleton sequence. The experimental results show that the D-HGCN consumes less time and has higher accuracy, which meets the real-time requirements. Our experiments demonstrate that our approach outperforms state-of-the-art methods on the NTU and Kinetics datasets.",
isbn="978-3-030-92270-2"
}

@inproceedings{10.1145/3512527.3531367, author = {Zhu, Yiran and Huang, Guangji and Xu, Xing and Ji, Yanli and Shen, Fumin}, title = {Selective Hypergraph Convolutional Networks for Skeleton-Based Action Recognition}, year = {2022}, isbn = {9781450392389}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3512527.3531367}, doi = {10.1145/3512527.3531367}, abstract = {In skeleton-based action recognition, Graph Convolutional Networks (GCNs) have achieved remarkable performance since the skeleton representation of human action can be naturally modeled by the graph structure. Most of the existing GCN-based methods extract skeleton features by exploiting single-scale joint information, while neglecting the valuable multi-scale contextual information. Besides, the commonly used strided convolution in temporal dimension could evenly filters out the keyframes we expect to preserve and leads to the loss of keyframe information. To address these issues, we propose a novel Selective Hypergraph Convolution Network, dubbed Selective-HCN, which stacks two key modules: Selective-scale Hypergraph Convolution (SHC) and Selective-frame Temporal Convolution (STC). The SHC module represents the human skeleton as the graph and hypergraph to fully extract multi-scale information, and selectively fuse features at various scales. Instead of traditional strided temporal convolution, the STC module can adaptively select keyframes and filter redundant frames according to the importance of the frames. Extensive experiments on two challenging skeleton action benchmarks, i.e., NTU-RGB+D and Skeleton-Kinetics, demonstrate the superiority and effectiveness of our proposed method.}, booktitle = {Proceedings of the 2022 International Conference on Multimedia Retrieval}, pages = {518–526}, numpages = {9}, keywords = {hypergraph, selective mechanism, action recognition, skeleton}, location = {Newark, NJ, USA}, series = {ICMR '22} }

@article{Peng_Hong_Chen_Zhao_2020, title={Learning Graph Convolutional Network for Skeleton-Based Human Action Recognition by Neural Searching}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/5652}, DOI={10.1609/aaai.v34i03.5652}, abstractNote={&lt;p&gt;Human action recognition from skeleton data, fuelled by the Graph Convolutional Network (GCN) with its powerful capability of modeling non-Euclidean data, has attracted lots of attention. However, many existing GCNs provide a pre-defined graph structure and share it through the entire network, which can loss implicit joint correlations especially for the higher-level features. Besides, the mainstream spectral GCN is approximated by one-order hop such that higher-order connections are not well involved. All of these require huge efforts to design a better GCN architecture. To address these problems, we turn to Neural Architecture Search (NAS) and propose the first automatically designed GCN for this task. Specifically, we explore the spatial-temporal correlations between nodes and build a search space with multiple dynamic graph modules. Besides, we introduce multiple-hop modules and expect to break the limitation of representational capacity caused by one-order approximation. Moreover, a corresponding sampling- and memory-efficient evolution strategy is proposed to search in this space. The resulted architecture proves the effectiveness of the higher-order approximation and the layer-wise dynamic graph modules. To evaluate the performance of the searched model, we conduct extensive experiments on two very large scale skeleton-based action recognition datasets. The results show that our model gets the state-of-the-art results in term of given metrics.&lt;/p&gt;}, number={03}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Peng, Wei and Hong, Xiaopeng and Chen, Haoyu and Zhao, Guoying}, year={2020}, month={Apr.}, pages={2669-2676} }


@ARTICLE{9334430,
  author={Li, Maosen and Chen, Siheng and Chen, Xu and Zhang, Ya and Wang, Yanfeng and Tian, Qi},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Symbiotic Graph Neural Networks for 3D Skeleton-Based Human Action Recognition and Motion Prediction}, 
  year={2022},
  volume={44},
  number={6},
  pages={3316-3333},
  doi={10.1109/TPAMI.2021.3053765}}

@inproceedings{
choromanski2021rethinking,
title={Rethinking Attention with Performers},
author={Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Ua6zuk0WRH}
}


@InProceedings{pmlr-v119-katharopoulos20a,
  title = 	 {Transformers are {RNN}s: Fast Autoregressive Transformers with Linear Attention},
  author =       {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {5156--5165},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/katharopoulos20a/katharopoulos20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/katharopoulos20a.html},
  abstract = 	 {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input’s length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\bigO{N^2}$ to $\bigO{N}$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our \emph{Linear Transformers} achieve similar performance to vanilla Transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.}
}

@inproceedings{
kim2021transformers,
title={Transformers Generalize DeepSets and Can be Extended to Graphs \& Hypergraphs},
author={Jinwoo Kim and Saeyoon Oh and Seunghoon Hong},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=scn3RYn1DYx}
}


% self-supervised with Transformer

@inproceedings{ranasinghe2022selfsupervised,
    title={Self-supervised Video Transformer}, 
    author={Kanchana Ranasinghe and Muzammal Naseer and Salman Khan and Fahad Shahbaz Khan and Michael Ryoo},
    booktitle={IEEE/CVF International Conference on Computer Vision and Pattern Recognition},
    month = {June},
    year={2022}
  }
  
@inproceedings{
akbari2021vatt,
title={{VATT}: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text},
author={Hassan Akbari and Liangzhe Yuan and Rui Qian and Wei-Hong Chuang and Shih-Fu Chang and Yin Cui and Boqing Gong},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=RzYrn625bu8}
}

@article{DBLP:journals/corr/abs-2112-07338,
  author    = {Yongkang Zhang and
               Jun Li and
               Guoming Wu and
               Han Zhang and
               Zhiping Shi and
               Zhaoxun Liu and
               Zizhang Wu},
  title     = {Temporal Transformer Networks with Self-Supervision for Action Recognition},
  journal   = {CoRR},
  volume    = {abs/2112.07338},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.07338},
  eprinttype = {arXiv},
  eprint    = {2112.07338},
  timestamp = {Wed, 05 Jan 2022 16:54:22 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-07338.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1145/3444685.3446289, author = {Cheng, Yi-Bin and Chen, Xipeng and Zhang, Dongyu and Lin, Liang}, title = {Motion-Transformer: Self-Supervised Pre-Training for Skeleton-Based Action Recognition}, year = {2021}, isbn = {9781450383080}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3444685.3446289}, doi = {10.1145/3444685.3446289}, abstract = {With the development of deep learning, skeleton-based action recognition has achieved great progress in recent years. However, most of the current works focus on extracting more informative spatial representations of the human body, but haven't made full use of the temporal dependencies already contained in the sequence of human action. To this end, we propose a novel transformer-based model called Motion-Transformer to sufficiently capture the temporal dependencies via self-supervised pre-training on the sequence of human action. Besides, we propose to predict the motion flow of human skeletons for better learning the temporal dependencies in sequence. The pre-trained model is then fine-tuned on the task of action recognition. Experimental results on the large scale NTU RGB+D dataset shows our model is effective in modeling temporal relation, and the flow prediction pre-training is beneficial to expose the inherent dependencies in time dimensional. With this pre-training and fine-tuning paradigm, our final model outperforms previous state-of-the-art methods.}, booktitle = {Proceedings of the 2nd ACM International Conference on Multimedia in Asia}, articleno = {36}, numpages = {6}, keywords = {motion-transformer, self-supervised pre-training, skeleton-based action recognition}, location = {Virtual Event, Singapore}, series = {MMAsia '20} }

@article{DBLP:journals/corr/abs-2112-09133,
  author    = {Chen Wei and
               Haoqi Fan and
               Saining Xie and
               Chao{-}Yuan Wu and
               Alan L. Yuille and
               Christoph Feichtenhofer},
  title     = {Masked Feature Prediction for Self-Supervised Visual Pre-Training},
  journal   = {CoRR},
  volume    = {abs/2112.09133},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.09133},
  eprinttype = {arXiv},
  eprint    = {2112.09133},
  timestamp = {Tue, 29 Mar 2022 12:44:53 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-09133.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2203-12602,
  publtype={informal},
  author={Zhan Tong and Yibing Song and Jue Wang and Limin Wang},
  title={VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training},
  year={2022},
  cdate={1640995200000},
  journal={CoRR},
  volume={abs/2203.12602},
  url={https://doi.org/10.48550/arXiv.2203.12602}
}

% gcn-based
@InProceedings{Si_2019_CVPR,
author = {Si, Chenyang and Chen, Wentao and Wang, Wei and Wang, Liang and Tan, Tieniu},
title = {An Attention Enhanced Graph Convolutional LSTM Network for Skeleton-Based Action Recognition},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@InProceedings{Li_2019_CVPR,
author = {Li, Maosen and Chen, Siheng and Chen, Xu and Zhang, Ya and Wang, Yanfeng and Tian, Qi},
title = {Actional-Structural Graph Convolutional Networks for Skeleton-Based Action Recognition},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
} 

@InProceedings{Shi_2021_ICCV,
    author    = {Shi, Lei and Zhang, Yifan and Cheng, Jian and Lu, Hanqing},
    title     = {AdaSGN: Adapting Joint Number and Model Size for Efficient Skeleton-Based Action Recognition},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {13413-13422}
}

@InProceedings{Chen_2021_ICCV,
    author    = {Chen, Yuxin and Zhang, Ziqi and Yuan, Chunfeng and Li, Bing and Deng, Ying and Hu, Weiming},
    title     = {Channel-Wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {13359-13368}
}

@InProceedings{Kwon_2021_ICCV,
    author    = {Kwon, Heeseung and Kim, Manjin and Kwak, Suha and Cho, Minsu},
    title     = {Learning Self-Similarity in Space and Time As Generalized Motion for Video Action Recognition},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {13065-13075}
}

@InProceedings{Liu_2020_CVPR,
author = {Liu, Ziyu and Zhang, Hongwen and Chen, Zhenghao and Wang, Zhiyong and Ouyang, Wanli},
title = {Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition},
booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}
@InProceedings{Zhang_2020_CVPR1,
author = {Zhang, Xikun and Xu, Chang and Tao, Dacheng},
title = {Context Aware Graph Convolution for Skeleton-Based Action Recognition},
booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@InProceedings{Zhang_2020_CVPR2,
author = {Zhang, Pengfei and Lan, Cuiling and Zeng, Wenjun and Xing, Junliang and Xue, Jianru and Zheng, Nanning},
title = {Semantics-Guided Neural Networks for Efficient Skeleton-Based Human Action Recognition},
booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@InProceedings{10.1007/978-3-030-58586-0_32,
author="Cheng, Ke
and Zhang, Yifan
and Cao, Congqi
and Shi, Lei
and Cheng, Jian
and Lu, Hanqing",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Decoupling GCN with DropGraph Module for Skeleton-Based Action Recognition",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="536--553",
abstract="In skeleton-based action recognition, graph convolutional networks (GCNs) have achieved remarkable success. Nevertheless, how to efficiently model the spatial-temporal skeleton graph without introducing extra computation burden is a challenging problem for industrial deployment. In this paper, we rethink the spatial aggregation in existing GCN-based skeleton action recognition methods and discover that they are limited by coupling aggregation mechanism. Inspired by the decoupling aggregation mechanism in CNNs, we propose decoupling GCN to boost the graph modeling ability with no extra computation, no extra latency, no extra GPU memory cost, and less than 10{\%} extra parameters. Another prevalent problem of GCNs is over-fitting. Although dropout is a widely used regularization technique, it is not effective for GCNs, due to the fact that activation units are correlated between neighbor nodes. We propose DropGraph to discard features in correlated nodes, which is particularly effective on GCNs. Moreover, we introduce an attention-guided drop mechanism to enhance the regularization effect. All our contributions introduce zero extra computation burden at deployment. We conduct experiments on three datasets (NTU-RGBD, NTU-RGBD-120, and Northwestern-UCLA) and exceed the state-of-the-art performance with less computation cost.",
isbn="978-3-030-58586-0"
}

@InProceedings{10.1007/978-3-030-58565-5_45,
author="Korban, Matthew
and Li, Xin",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="DDGCN: A Dynamic Directed Graph Convolutional Network for Action Recognition",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="761--776",
abstract="We propose a Dynamic Directed Graph Convolutional Network (DDGCN) to model spatial and temporal features of human actions from their skeletal representations. The DDGCN consists of three new feature modeling modules: (1) Dynamic Convolutional Sampling (DCS), (2) Dynamic Convolutional Weight (DCW) assignment, and (3) Directed Graph Spatial-Temporal (DGST) feature extraction. Comprehensive experiments show that the DDGCN outperforms existing state-of-the-art action recognition approaches in various testing datasets.",
isbn="978-3-030-58565-5"
}

@inproceedings{cheng2020shiftgcn,  
  title     = {Skeleton-Based Action Recognition with Shift Graph Convolutional Network},  
  author    = {Ke Cheng and Yifan Zhang and Xiangyu He and Weihan Chen and Jian Cheng and Hanqing Lu},  
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},  
  year      = {2020},  
}

% attention

@inproceedings{NIPS2017_3f5ee243,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{
velickovic2018graph,
title={Graph Attention Networks},
author={Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rJXMpikCZ},
}

@inproceedings{conf/uai/ZhangSXMKY18,
  added-at = {2020-09-01T00:00:00.000+0200},
  author = {Zhang, Jiani and Shi, Xingjian and Xie, Junyuan and Ma, Hao and King, Irwin and Yeung, Dit-Yan},
  biburl = {https://www.bibsonomy.org/bibtex/2ba2702244b6cf5a45099adbc5bb04bd4/dblp},
  booktitle = {UAI},
  editor = {Globerson, Amir and Silva, Ricardo},
  ee = {http://auai.org/uai2018/proceedings/papers/139.pdf},
  interhash = {6f65a6c4f9873beab4cadd486deae077},
  intrahash = {ba2702244b6cf5a45099adbc5bb04bd4},
  keywords = {dblp},
  pages = {339-349},
  publisher = {AUAI Press},
  timestamp = {2020-09-09T14:23:24.000+0200},
  title = {GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs.},
  url = {http://dblp.uni-trier.de/db/conf/uai/uai2018.html#ZhangSXMKY18},
  year = 2018
}

@inproceedings{10.1145/3219819.3219980,
author = {Lee, John Boaz and Rossi, Ryan and Kong, Xiangnan},
title = {Graph Classification Using Structural Attention},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219980},
doi = {10.1145/3219819.3219980},
abstract = {Graph classification is a problem with practical applications in many different domains. To solve this problem, one usually calculates certain graph statistics (i.e., graph features) that help discriminate between graphs of different classes. When calculating such features, most existing approaches process the entire graph. In a graphlet-based approach, for instance, the entire graph is processed to get the total count of different graphlets or subgraphs. In many real-world applications, however, graphs can be noisy with discriminative patterns confined to certain regions in the graph only. In this work, we study the problem of attention-based graph classification. The use of attention allows us to focus on small but informative parts of the graph, avoiding noise in the rest of the graph. We present a novel RNN model, called the Graph Attention Model (GAM), that processes only a portion of the graph by adaptively selecting a sequence of "informative" nodes. Experimental results on multiple real-world datasets show that the proposed method is competitive against various well-known methods in graph classification even though our method is limited to only a portion of the graph.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1666–1674},
numpages = {9},
keywords = {deep learning, attentional processing, graph mining, reinforcement learning},
location = {London, United Kingdom},
series = {KDD '18}
}

% cross-attention

@article{DBLP:journals/corr/abs-2106-05786,
  author    = {Hezheng Lin and
               Xing Cheng and
               Xiangyu Wu and
               Fan Yang and
               Dong Shen and
               Zhongyuan Wang and
               Qing Song and
               Wei Yuan},
  title     = {{CAT:} Cross Attention in Vision Transformer},
  journal   = {CoRR},
  volume    = {abs/2106.05786},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.05786},
  eprinttype = {arXiv},
  eprint    = {2106.05786},
  timestamp = {Tue, 15 Jun 2021 16:35:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-05786.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{https://doi.org/10.48550/arxiv.2204.00452,
  doi = {10.48550/ARXIV.2204.00452},
  
  url = {https://arxiv.org/abs/2204.00452},
  
  author = {Hashiguchi, Ryota and Tamaki, Toru},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Vision Transformer with Cross-attention by Temporal Shift for Efficient Action Recognition},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{DBLP:journals/corr/abs-2103-14899,
  author    = {Chun{-}Fu Chen and
               Quanfu Fan and
               Rameswar Panda},
  title     = {CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image
               Classification},
  journal   = {CoRR},
  volume    = {abs/2103.14899},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.14899},
  eprinttype = {arXiv},
  eprint    = {2103.14899},
  timestamp = {Wed, 07 Apr 2021 15:31:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-14899.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Wei_2020_CVPR,
author = {Wei, Xi and Zhang, Tianzhu and Li, Yan and Zhang, Yongdong and Wu, Feng},
title = {Multi-Modality Cross Attention Network for Image and Sentence Matching},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

% pooling
@article{10.1109/TPAMI.2016.2558148,
author = {Fernando, Basura and Gavves, Efstratios and Oramas M., Jose Oramas and Ghodrati, Amir and Tuytelaars, Tinne},
title = {Rank Pooling for Action Recognition},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Computer Society},
address = {USA},
volume = {39},
number = {4},
issn = {0162-8828},
url = {https://doi.org/10.1109/TPAMI.2016.2558148},
doi = {10.1109/TPAMI.2016.2558148},
abstract = {We propose a function-based temporal pooling method that captures the latent structure of the video sequence data - e.g., how frame-level features evolve over time in a video. We show how the parameters of a function that has been fit to the video data can serve as a robust new video representation. As a specific example, we learn a pooling function via ranking machines. By learning to rank the frame-level features of a video in chronological order, we obtain a new representation that captures the video-wide temporal dynamics of a video, suitable for action recognition. Other than ranking functions, we explore different parametric models that could also explain the temporal changes in videos. The proposed functional pooling methods, and rank pooling in particular, is easy to interpret and implement, fast to compute and effective in recognizing a wide variety of actions. We evaluate our method on various benchmarks for generic action, fine-grained action and gesture recognition. Results show that rank pooling brings an absolute improvement of 7-10 average pooling baseline. At the same time, rank pooling is compatible with and complementary to several appearance and local motion based methods and features, such as improved trajectories and deep learning features.},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = {apr},
pages = {773–787},
numpages = {15}
}

@InProceedings{Gao_2019_CVPR,
                author = {Zilin, Gao and Jiangtao, Xie and Qilong, Wang and Peihua, Li},
                title = {Global Second-order Pooling Convolutional Networks},
                booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
                year = {2019}
  }
  
  @inproceedings{NEURIPS2018_17c276c8,
 author = {Wang, Qilong and Gao, Zilin and Xie, Jiangtao and Zuo, Wangmeng and Li, Peihua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Global Gated Mixture of Second-order Pooling for Improving Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2018/file/17c276c8e723eb46aef576537e9d56d0-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{Girdhar_17b_AttentionalPoolingAction,
    title = {Attentional Pooling for Action Recognition},
    author = {Girdhar, Rohit and Ramanan, Deva},
    booktitle = {NIPS},
    year = 2017
}

@INPROCEEDINGS{7926605,  author={Cherian, Anoop and Koniusz, Piotr and Gould, Stephen},  booktitle={2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},   title={Higher-Order Pooling of CNN Features via Kernel Linearization for Action Recognition},   year={2017},  volume={},  number={},  pages={130-138},  doi={10.1109/WACV.2017.22}}


@article{koniusz2021high,
  title={High-order Tensor Pooling with Attention for Action Recognition},
  author={Koniusz, Piotr and Wang, Lei and Sun, Ke},
  journal={arXiv},
  year={2021}
} 

% transformer-based
@inproceedings{10.1145/3474085.3475473,
author = {Zhang, Yuhan and Wu, Bo and Li, Wen and Duan, Lixin and Gan, Chuang},
title = {STST: Spatial-Temporal Specialized Transformer for Skeleton-Based Action Recognition},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475473},
doi = {10.1145/3474085.3475473},
abstract = {Skeleton-based action recognition has been widely investigated considering their strong adaptability to dynamic circumstances and complicated backgrounds. To recognize different actions from skeleton sequences, it is essential and crucial to model the posture of the human represented by the skeleton and its changes in the temporal dimension. However, most of the existing works treat skeleton sequences in the temporal and spatial dimension in the same way, ignoring the difference between the temporal and spatial dimension in skeleton data which is not an optimal way to model skeleton sequences. The posture represented by the skeleton in each frame is proposed to be modeled individually. Meanwhile, capturing the movement of the entire skeleton in the temporal dimension is needed. So, we designed Spatial Transformer Block and Directional Temporal Transformer Block for modeling skeleton sequences in spatial and temporal dimensions respectively. Due to occlusion/sensor/raw video, etc., there are noises on both temporal and spatial dimensions in the extracted skeleton data reducing the recognition capabilities of models. To adapt to this imperfect information condition, we propose a multi-task self-supervised learning method by providing confusing samples in different situations to improve the robustness of our model. Combining the above design, we propose our Spatial-Temporal Specialized Transformer~(STST) and conduct experiments with our model on the SHREC, NTU-RGB+D, and Kinetics-Skeleton. Extensive experimental results demonstrate the improved performances and analysis of the proposed method.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {3229–3237},
numpages = {9},
keywords = {skeleton, action recognition, spatial-temporal, specialized, self-supervision, transformer},
location = {Virtual Event, China},
series = {MM '21}
}

@article{PLIZZARI2021103219,
title = {Skeleton-based action recognition via spatial and temporal transformer networks},
journal = {Computer Vision and Image Understanding},
volume = {208-209},
pages = {103219},
year = {2021},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2021.103219},
url = {https://www.sciencedirect.com/science/article/pii/S1077314221000631},
author = {Chiara Plizzari and Marco Cannici and Matteo Matteucci},
keywords = {Representation learning, Graph CNN, Self-attention, 3D skeleton, Action recognition},
abstract = {Skeleton-based Human Activity Recognition has achieved great interest in recent years as skeleton data has demonstrated being robust to illumination changes, body scales, dynamic camera views, and complex background. In particular, Spatial–Temporal Graph Convolutional Networks (ST-GCN) demonstrated to be effective in learning both spatial and temporal dependencies on non-Euclidean data such as skeleton graphs. Nevertheless, an effective encoding of the latent information underlying the 3D skeleton is still an open problem, especially when it comes to extracting effective information from joint motion patterns and their correlations. In this work, we propose a novel Spatial–Temporal Transformer network (ST-TR) which models dependencies between joints using the Transformer self-attention operator. In our ST-TR model, a Spatial Self-Attention module (SSA) is used to understand intra-frame interactions between different body parts, and a Temporal Self-Attention module (TSA) to model inter-frame correlations. The two are combined in a two-stream network, whose performance is evaluated on three large-scale datasets, NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics Skeleton 400, consistently improving backbone results. Compared with methods that use the same input data, the proposed ST-TR achieves state-of-the-art performance on all datasets when using joints’ coordinates as input, and results on-par with state-of-the-art when adding bones information.}
}

@ARTICLE{9681250,
  author={Kong, Jun and Bian, Yuhang and Jiang, Min},
  journal={IEEE Signal Processing Letters}, 
  title={MTT: Multi-Scale Temporal Transformer for Skeleton-Based Action Recognition}, 
  year={2022},
  volume={29},
  number={},
  pages={528-532},
  doi={10.1109/LSP.2022.3142675}}


@Article{sym14081547,
AUTHOR = {Jiang, Yujian and Sun, Zhaoneng and Yu, Saisai and Wang, Shuang and Song, Yang},
TITLE = {A Graph Skeleton Transformer Network for Action Recognition},
JOURNAL = {Symmetry},
VOLUME = {14},
YEAR = {2022},
NUMBER = {8},
ARTICLE-NUMBER = {1547},
URL = {https://www.mdpi.com/2073-8994/14/8/1547},
ISSN = {2073-8994},
ABSTRACT = {Skeleton-based action recognition is a research hotspot in the field of computer vision. Currently, the mainstream method is based on Graph Convolutional Networks (GCNs). Although there are many advantages of GCNs, GCNs mainly rely on graph topologies to draw dependencies between the joints, which are limited in capturing long-distance dependencies. Meanwhile, Transformer-based methods have been applied to skeleton-based action recognition because they effectively capture long-distance dependencies. However, existing Transformer-based methods lose the inherent connection information of human skeleton joints because they do not yet focus on initial graph structure information. This paper aims to improve the accuracy of skeleton-based action recognition. Therefore, a Graph Skeleton Transformer network (GSTN) for action recognition is proposed, which is based on Transformer architecture to extract global features, while using undirected graph information represented by the symmetric matrix to extract local features. Two encodings are utilized in feature processing to improve joints&rsquo; semantic and centrality features. In the process of multi-stream fusion strategies, a grid-search-based method is used to assign weights to each input stream to optimize the fusion results. We tested our method using three action recognition datasets: NTU RGB+D 60, NTU RGB+D 120, and NW-UCLA. The experimental results show that our model&rsquo;s accuracy is comparable to state-of-the-art approaches.},
DOI = {10.3390/sym14081547}
}

% suggested by reviewers to be added in related work section

@inproceedings{chen2021channel,
    title={Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition},
    author={Chen, Yuxin and Zhang, Ziqi and Yuan, Chunfeng and Li, Bing and Deng, Ying and Hu, Weiming},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages={13359--13368},
    year={2021}
  }

@ARTICLE{9729609,  author={Song, Yi-Fan and Zhang, Zhang and Shan, Caifeng and Wang, Liang},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   title={Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition},   year={2022},  volume={},  number={},  pages={1-1},  doi={10.1109/TPAMI.2022.3157033}}
% a family of efficient GCN

@inproceedings{10.1145/3412841.3441974,
author = {Seo, Young Min and Choi, Yong Suk},
title = {Graph Convolutional Networks for Skeleton-Based Action Recognition with LSTM Using Tool-Information},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441974},
doi = {10.1145/3412841.3441974},
abstract = {Skeleton-based action recognition using a Graph Convolutional Network (GCN) achieved remarkable results by reconstructing a person's skeleton into a graph. However, there are fundamental problems with existing GCN-based models. Generally, human action is greatly affected by the tools used, but in traditional GCN models, action recognition is performed without using the information in the tools. For example, a person holding a pen is limited to the act of writing. A person holding a ball is limited to the action of throwing or receiving the ball. In other words, it is an inaccurate method to judge action only by recognizing the movements of bones used in existing methods. Therefore, a graph was made to reflect the information on the tool. We identify tool-information using the LSTM classifier and propose GCNs for skeleton-based action recognition with LSTM using tool-information. Additionally, we apply a new graph construction and utilize the Learnable Adjacency Matrix. The proposed method is applied to the existing model and comparative evaluation was performed between the model with and without the applied algorithm. The evaluations showed consistent performance improvement, and the proposed method applied to the baseline models achieved state-of-the-art performance.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {986–993},
numpages = {8},
keywords = {computer vision, graph convolutional networks, long short-term memory models, skeleton-based action recognition, object detection},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@INPROCEEDINGS{8784712,  author={Zhang, Han and Song, Yonghong and Zhang, Yuanlin},  booktitle={2019 IEEE International Conference on Multimedia and Expo (ICME)},   title={Graph Convolutional LSTM Model for Skeleton-Based Action Recognition},   year={2019},  volume={},  number={},  pages={412-417},  doi={10.1109/ICME.2019.00078}}

@inproceedings{kovaleva-etal-2019-revealing,
    title = "Revealing the Dark Secrets of {BERT}",
    author = "Kovaleva, Olga  and
      Romanov, Alexey  and
      Rogers, Anna  and
      Rumshisky, Anna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1445",
    doi = "10.18653/v1/D19-1445",
    pages = "4365--4374",
    abstract = "BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success. In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of BERT. Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT{'}s heads. Our findings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization. While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks. We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models.",
}

@InProceedings{Li_2021_ICCV,
    author    = {Li, Tianjiao and Ke, Qiuhong and Rahmani, Hossein and Ho, Rui En and Ding, Henghui and Liu, Jun},
    title     = {Else-Net: Elastic Semantic Network for Continual Action Recognition From Skeleton Data},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {13434-13443}
}

@InProceedings{Chi_2022_CVPR,
    author    = {Chi, Hyung-gun and Ha, Myoung Hoon and Chi, Seunggeun and Lee, Sang Wan and Huang, Qixing and Ramani, Karthik},
    title     = {InfoGCN: Representation Learning for Human Skeleton-Based Action Recognition},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {20186-20196}
}

@article{peng2021rethinking,
  title={Rethinking the ST-GCNs for 3D skeleton-based human action recognition},
  author={Peng, Wei and Shi, Jingang and Varanka, Tuomas and Zhao, Guoying},
  journal={Neurocomputing},
  volume={454},
  pages={45--53},
  year={2021},
  publisher={Elsevier}
}

@InProceedings{Duan_2022_CVPR,
    author    = {Duan, Haodong and Zhao, Yue and Chen, Kai and Lin, Dahua and Dai, Bo},
    title     = {Revisiting Skeleton-Based Action Recognition},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {2969-2978}
}


@inproceedings{wang2014cross,
  title={Cross-view action modeling, learning and recognition},
  author={Wang, Jiang and Nie, Xiaohan and Xia, Yin and Wu, Ying and Zhu, Song-Chun},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2649--2656},
  year={2014}
}


@inproceedings{ssgc_hao,
  author    = {Hao Zhu and
               Piotr Koniusz},
  title     = {Simple Spectral Graph Convolution},
  booktitle = {{ICLR}},
  year      = {2021}
}

@inproceedings{coles_hao,
  author    = {Hao Zhu and
               Ke Sun and
               Piotr Koniusz},
  title     = {Contrastive Laplacian Eigenmaps},
  booktitle = {{NeurIPS}},
  year      = {2021}
}

@inproceedings{glen_hao,
  author    = {Hao Zhu and
               Piotr Koniusz},
  title     = {Generalized Laplacian Eigenmaps},
  booktitle = {{NeurIPS}},
  year      = {2022}
}