@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = CVPR,
pages = {234--778},
year = 2005
}

@inproceedings{ji2020action,
  title={Action genome: Actions as compositions of spatio-temporal scene graphs},
  author={Ji, Jingwei and Krishna, Ranjay and Fei-Fei, Li and Niebles, Juan Carlos},
  booktitle={CVPR},
  pages={10236--10247},
  year={2020}
}


@article{greff2021kubric,
    title = {Kubric: a scalable dataset generator}, 
    author = {Klaus Greff and Francois Belletti and Lucas Beyer and Carl Doersch and
              Yilun Du and Daniel Duckworth and David J Fleet and Dan Gnanapragasam and
              Florian Golemo and Charles Herrmann and Thomas Kipf and Abhijit Kundu and
              Dmitry Lagun and Issam Laradji and Hsueh-Ti (Derek) Liu and Henning Meyer and
              Yishu Miao and Derek Nowrouzezahrai and Cengiz Oztireli and Etienne Pot and
              Noha Radwan and Daniel Rebain and Sara Sabour and Mehdi S. M. Sajjadi and Matan Sela and
              Vincent Sitzmann and Austin Stone and Deqing Sun and Suhani Vora and Ziyu Wang and
              Tianhao Wu and Kwang Moo Yi and Fangcheng Zhong and Andrea Tagliasacchi},
    booktitle = {CVPR},
    year = {2022},
}



@article{hertz2022prompt,
  title={Prompt-to-prompt image editing with cross attention control},
  author={Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},
  journal={arXiv preprint arXiv:2208.01626},
  year={2022}
}

@inproceedings{thrush2022winoground,
  title={Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality},
  author={Thrush, Tristan and Jiang, Ryan and Bartolo, Max and Singh, Amanpreet and Williams, Adina and Kiela, Douwe and Ross, Candace},
  booktitle={CVPR},
  pages={5238--5248},
  year={2022}
}

@article{dou2022coarse,
  title={Coarse-to-fine vision-language pre-training with fusion in the backbone},
  author={Dou, Zi-Yi and Kamath, Aishwarya and Gan, Zhe and Zhang, Pengchuan and Wang, Jianfeng and Li, Linjie and Liu, Zicheng and Liu, Ce and LeCun, Yann and Peng, Nanyun and others},
  journal={arXiv preprint arXiv:2206.07643},
  year={2022}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={CVPR},
  pages={10684--10695},
  year={2022}
}



@article{ramesh2022hierarchical,
  title={Hierarchical text-conditional image generation with clip latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:2204.06125},
  year={2022}
}

@article{saharia2022photorealistic,
  title={Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
  author={Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S Sara and Lopes, Rapha Gontijo and others},
  journal={arXiv preprint arXiv:2205.11487},
  year={2022}
}

@inproceedings{li2021adversarial,
  title={Adversarial vqa: A new benchmark for evaluating the robustness of vqa models},
  author={Li, Linjie and Lei, Jie and Gan, Zhe and Liu, Jingjing},
  booktitle={ICCV},
  pages={2042--2051},
  year={2021}
}

@inproceedings{niu2021counterfactual,
  title={Counterfactual vqa: A cause-effect look at language bias},
  author={Niu, Yulei and Tang, Kaihua and Zhang, Hanwang and Lu, Zhiwu and Hua, Xian-Sheng and Wen, Ji-Rong},
  booktitle={CVPR},
  pages={12700--12710},
  year={2021}
}

@article{shekhar2017foil,
  title={Foil it! find one mismatch between image and language caption},
  author={Shekhar, Ravi and Pezzelle, Sandro and Klimovich, Yauhen and Herbelot, Aur{\'e}lie and Nabi, Moin and Sangineto, Enver and Bernardi, Raffaella},
  journal={arXiv preprint arXiv:1705.01359},
  year={2017}
}

@article{hendricks2021probing,
  title={Probing image-language transformers for verb understanding},
  author={Hendricks, Lisa Anne and Nematzadeh, Aida},
  journal={arXiv preprint arXiv:2106.09141},
  year={2021}
}

@article{parcalabescu2021valse,
  title={VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena},
  author={Parcalabescu, Letitia and Cafagna, Michele and Muradjan, Lilitta and Frank, Anette and Calixto, Iacer and Gatt, Albert},
  journal={arXiv preprint arXiv:2112.07566},
  year={2021}
}

@article{gupta2022grit,
  title={GRIT: General Robust Image Task Benchmark},
  author={Gupta, Tanmay and Marten, Ryan and Kembhavi, Aniruddha and Hoiem, Derek},
  journal={arXiv preprint arXiv:2204.13653},
  year={2022}
}

@article{jimenez2022carets,
  title={CARETS: A Consistency And Robustness Evaluative Test Suite for VQA},
  author={Jimenez, Carlos E and Russakovsky, Olga and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2203.07613},
  year={2022}
}


@article{cohen2018spherical,
  title={Spherical cnns},
  author={Cohen, Taco S and Geiger, Mario and K{\"o}hler, Jonas and Welling, Max},
  journal={arXiv preprint arXiv:1801.10130},
  year={2018}
}

@inproceedings{cohen2016group,
  title={Group equivariant convolutional networks},
  author={Cohen, Taco and Welling, Max},
  booktitle={ICML},
  pages={2990--2999},
  year={2016},
  organization={PMLR}
}

@article{cohen2016steerable,
  title={Steerable cnns},
  author={Cohen, Taco S and Welling, Max},
  journal={arXiv preprint arXiv:1612.08498},
  year={2016}
}

@article{dangovski2021equivariant,
  title={Equivariant contrastive learning},
  author={Dangovski, Rumen and Jing, Li and Loh, Charlotte and Han, Seungwook and Srivastava, Akash and Cheung, Brian and Agrawal, Pulkit and Solja{\v{c}}i{\'c}, Marin},
  journal={arXiv preprint arXiv:2111.00899},
  year={2021}
}

@inproceedings{xie2022should,
  title={What Should Be Equivariant in Self-Supervised Learning},
  author={Xie, Yuyang and Wen, Jianhong and Lau, Kin Wai and Rehman, Yasar Abbas Ur and Shen, Jiajun},
  booktitle={CVPR},
  pages={4111--4120},
  year={2022}
}

@article{qi2020learning,
  title={Learning generalized transformation equivariant representations via autoencoding transformations},
  author={Qi, Guo-Jun and Zhang, Liheng and Lin, Feng and Wang, Xiao},
  journal={TPAMI},
  year={2020},
  publisher={IEEE}
}

@article{wang2021self,
  title={Self-supervised learning disentangled group representation as feature},
  author={Wang, Tan and Yue, Zhongqi and Huang, Jianqiang and Sun, Qianru and Zhang, Hanwang},
  journal={NeurIPS},
  volume={34},
  pages={18225--18240},
  year={2021}
}

@book{royden1988real,
  title={Real analysis},
  author={Royden, Halsey Lawrence and Fitzpatrick, Patrick},
  volume={32},
  year={1988},
  publisher={Macmillan New York}
}


@book{halmos2013measure,
  title={Measure theory},
  author={Halmos, Paul R},
  volume={18},
  year={2013},
  publisher={Springer}
}


@inproceedings{pan2022permutation,
  title={Permutation Equivariant Layers for Higher Order Interactions},
  author={Pan, Horace and Kondor, Risi},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={5987--6001},
  year={2022},
  organization={PMLR}
}

@article{zaheer2017deep,
  title={Deep sets},
  author={Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
  journal={NeurIPS},
  volume={30},
  year={2017}
}


@article{lake2019compositional,
  title={Compositional generalization through meta sequence-to-sequence learning},
  author={Lake, Brenden M},
  journal={NeurIPS},
  volume={32},
  year={2019}
}

@inproceedings{gordon2019permutation,
  title={Permutation equivariant models for compositional generalization in language},
  author={Gordon, Jonathan and Lopez-Paz, David and Baroni, Marco and Bouchacourt, Diane},
  booktitle={ICLR},
  year={2019}
}

@article{bronstein2021geometric,
  title={Geometric deep learning: Grids, groups, graphs, geodesics, and gauges},
  author={Bronstein, Michael M and Bruna, Joan and Cohen, Taco and Veli{\v{c}}kovi{\'c}, Petar},
  journal={arXiv preprint arXiv:2104.13478},
  year={2021}
}

@article{weiler2019general,
  title={General e (2)-equivariant steerable cnns},
  author={Weiler, Maurice and Cesa, Gabriele},
  journal={NeurIPS},
  volume={32},
  year={2019}
}

@article{zhao2022vl,
  title={VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations},
  author={Zhao, Tiancheng and Zhang, Tianqi and Zhu, Mingwei and Shen, Haozhan and Lee, Kyusong and Lu, Xiaopeng and Yin, Jianwei},
  journal={arXiv preprint arXiv:2207.00221},
  year={2022}
}

@inproceedings{hu2019evaluating,
  title={Evaluating text-to-image matching using binary image selection (bison)},
  author={Hu, Hexiang and Misra, Ishan and van der Maaten, Laurens},
  booktitle={ICCV Workshops},
  pages={0--0},
  year={2019}
}

@inproceedings{wang2020visual,
  title={Visual commonsense r-cnn},
  author={Wang, Tan and Huang, Jianqiang and Zhang, Hanwang and Sun, Qianru},
  booktitle={CVPR},
  pages={10760--10770},
  year={2020}
}

@inproceedings{tang2020unbiased,
  title={Unbiased scene graph generation from biased training},
  author={Tang, Kaihua and Niu, Yulei and Huang, Jianqiang and Shi, Jiaxin and Zhang, Hanwang},
  booktitle={CVPR},
  pages={3716--3725},
  year={2020}
}


@inproceedings{zellers2019recognition,
  title={From recognition to cognition: Visual commonsense reasoning},
  author={Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={CVPR},
  pages={6720--6731},
  year={2019}
}

@article{cadene2019rubi,
  title={Rubi: Reducing unimodal biases for visual question answering},
  author={Cadene, Remi and Dancette, Corentin and Cord, Matthieu and Parikh, Devi and others},
  journal={NeurIPS},
  volume={32},
  year={2019}
}

@article{chen2017attacking,
  title={Attacking visual language grounding with adversarial examples: A case study on neural image captioning},
  author={Chen, Hongge and Zhang, Huan and Chen, Pin-Yu and Yi, Jinfeng and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1712.02051},
  year={2017}
}

@inproceedings{plummer2015flickr30k,
  title={Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models},
  author={Plummer, Bryan A and Wang, Liwei and Cervantes, Chris M and Caicedo, Juan C and Hockenmaier, Julia and Lazebnik, Svetlana},
  booktitle={ICCV},
  pages={2641--2649},
  year={2015}
}

@inproceedings{vinyals2015show,
  title={Show and tell: A neural image caption generator},
  author={Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  booktitle={CVPR},
  pages={3156--3164},
  year={2015}
}

@article{yu2022scaling,
  title={Scaling autoregressive models for content-rich text-to-image generation},
  author={Yu, Jiahui and Xu, Yuanzhong and Koh, Jing Yu and Luong, Thang and Baid, Gunjan and Wang, Zirui and Vasudevan, Vijay and Ku, Alexander and Yang, Yinfei and Ayan, Burcu Karagol and others},
  journal={arXiv preprint arXiv:2206.10789},
  year={2022}
}

@inproceedings{zhou2018towards,
  title={Towards automatic learning of procedures from web instructional videos},
  author={Zhou, Luowei and Xu, Chenliang and Corso, Jason J},
  booktitle={AAAI},
  year={2018}
}


@inproceedings{wang2022geb+,
  title={GEB+: A Benchmark for Generic Event Boundary Captioning, Grounding and Retrieval},
  author={Wang, Yuxuan and Gao, Difei and Yu, Licheng and Lei, Weixian and Feiszli, Matt and Shou, Mike Zheng},
  booktitle={ECCV},
  pages={709--725},
  year={2022},
  organization={Springer}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={ICML},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}


@article{kay2017kinetics,
  title={The kinetics human action video dataset},
  author={Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and others},
  journal={arXiv preprint arXiv:1705.06950},
  year={2017}
}

@inproceedings{singh2022flava,
  title={Flava: A foundational language and vision alignment model},
  author={Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
  booktitle={CVPR},
  pages={15638--15650},
  year={2022}
}

@inproceedings{kim2021vilt,
  title={Vilt: Vision-and-language transformer without convolution or region supervision},
  author={Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  booktitle={ICML},
  pages={5583--5594},
  year={2021},
  organization={PMLR}
}


@article{li2019visualbert,
  title={Visual{BERT}: A simple and performant baseline for vision and language},
  author={Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
  journal={arXiv preprint},
  url={https://arxiv.org/pdf/1908.03557},
  year={2019}
}

@inproceedings{tan-bansal-2019-lxmert,
    title = "{LXMERT}: Learning Cross-Modality Encoder Representations from Transformers",
    author = "Tan, Hao  and
      Bansal, Mohit",
    booktitle = "EMNLP",
    year = "2019"
}
@article{alayrac2022flamingo,
  title={Flamingo: a Visual Language Model for Few-Shot Learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and others},
  journal={arXiv preprint},
  year={2022}
}

@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={FAccT},
  year={2021}
}

@inproceedings{fang2022injecting,
  title={Injecting Semantic Concepts into End-to-End Image Captioning},
  author={Fang, Zhiyuan and Wang, Jianfeng and Hu, Xiaowei and Liang, Lin and Gan, Zhe and Wang, Lijuan and Yang, Yezhou and Liu, Zicheng},
  booktitle={CVPR},
  year={2022}
}
@inproceedings{hu2022scaling,
  title={Scaling up vision-language pre-training for image captioning},
  author={Hu, Xiaowei and Gan, Zhe and Wang, Jianfeng and Yang, Zhengyuan and Liu, Zicheng and Lu, Yumao and Wang, Lijuan},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{rennie2017self,
  title={Self-critical sequence training for image captioning},
  author={Rennie, Steven J and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{pang2021text,
  title={Text Generation by Learning from Demonstrations},
  author={Pang, Richard Yuanzhe and He, He},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{Shao2019Objects365AL,
  title={Objects365: A Large-Scale, High-Quality Dataset for Object Detection},
  author={Shuai Shao and Zeming Li and Tianyuan Zhang and Chao Peng and Gang Yu and Xiangyu Zhang and Jing Li and Jian Sun},
  booktitle={ICCV},
  year={2019}
}

 @article{Hudson2019GQAAN,
  title={GQA: a new dataset for compositional question answering over real-world images},
  author={Drew A. Hudson and Christopher D. Manning},
  journal={CVPR},
  year={2019}
}

@inproceedings{Lin2014MicrosoftCC,
  title={{Microsoft COCO}: Common Objects in Context},
  author={Tsung-Yi Lin and Michael Maire and Serge J. Belongie and James Hays and Pietro Perona and Deva Ramanan and Piotr Doll{\'a}r and C. Lawrence Zitnick},
  booktitle={ECCV},
  year={2014}
}

@inproceedings{cubuk2020randaugment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={CVPR Workshops},
  year={2020}
}

@inproceedings{kamath2021mdetr,
  title={{MDETR}-modulated detection for end-to-end multi-modal understanding},
  author={Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas},
  booktitle={ICCV},
  year={2021}
}

@article{raffel2020exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={JMLR},
  year={2020}
}

@inproceedings{cho2021unifying,
  title={Unifying vision-and-language tasks via text generation},
  author={Cho, Jaemin and Lei, Jie and Tan, Hao and Bansal, Mohit},
  booktitle = {ICML},
  year={2021}
}

@inproceedings{lu2019vilbert,
  title={{ViLBERT}: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle={NeurIPS},
  year={2019}
}
@inproceedings{su2019vl,
  title={{VL-BERT}: Pre-training of Generic Visual-Linguistic Representations},
  author={Su, Weijie and Zhu, Xizhou and Cao, Yue and Li, Bin and Lu, Lewei and Wei, Furu and Dai, Jifeng},
  booktitle={ICLR},
  year={2019}
}

@article{hendricks2021decoupling,
  title={Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers},
  author={Hendricks, Lisa Anne and Mellor, John and Schneider, Rosalia and Alayrac, Jean-Baptiste and Nematzadeh, Aida},
  journal={TACL},
  year={2021}
}

@inproceedings{jia2021scaling,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc V and Sung, Yunhsuan and Li, Zhen and Duerig, Tom},
  booktitle={ICML},
  year={2021}
}

@inproceedings{chen2020uniter,
  title={{UNITER}: Universal image-text representation learning},
  author={Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{li2020oscar,
  title={Oscar: Object-semantics aligned pre-training for vision-language tasks},
  author={Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{zhang2021vinvl,
  title={{VinVL}: Revisiting visual representations in vision-language models},
  author={Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng},
  booktitle={CVPR},
  year={2021}
}


@article{pires2019multilingual,
  title={How multilingual is Multilingual {BERT}?},
  author={Pires, Telmo and Schlinger, Eva and Garrette, Dan},
  journal={arXiv preprint},
  url={https://arxiv.org/pdf/1906.01502},
  year={2019}
}

@inproceedings{dou2021word,
  title={Word Alignment by Fine-tuning Embeddings on Parallel Corpora},
  author={Dou, Zi-Yi and Neubig, Graham},
  booktitle={EACL},
  year={2021}
}

@inproceedings{voita-titov-2020-information,
    title = "Information-Theoretic Probing with Minimum Description Length",
    author = "Voita, Elena  and
      Titov, Ivan",
    booktitle = "EMNLP",
    year = "2020",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.14",
}
@inproceedings{wang-etal-2020-maf,
    title = "{MAF}: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding",
    author = "Wang, Qinxin  and
      Tan, Hao  and
      Shen, Sheng  and
      Mahoney, Michael  and
      Yao, Zhewei",
    booktitle = "EMNLP",
    year = "2020",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.159",
}
@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "NAACL",
    year = "2019"
}


@inproceedings{sup1,
  title={Grounding of textual phrases in images by reconstruction},
  author={Rohrbach, Anna and Rohrbach, Marcus and Hu, Ronghang and Darrell, Trevor and Schiele, Bernt},
  booktitle={ECCV},
  year={2016}
}
@inproceedings{sup2,
  title={Rethinking Diversified and Discriminative Proposal Generation for Visual Grounding},
  author={Yu, Zhou and Yu, Jun and Xiang, Chenchao and Zhao, Zhou and Tian, Qi and Tao, Dacheng},
  booktitle={IJCAI},
  year={2018}
}
@inproceedings{sup3,
  title={Learning Cross-Modal Context Graph for Visual Grounding.},
  author={Liu, Yongfei and Wan, Bo and Zhu, Xiaodan and He, Xuming},
  booktitle={AAAI},
  url={https://arxiv.org/pdf/1911.09042},
  year={2020}
}
@inproceedings{weak1,
  title={Grounding of textual phrases in images by reconstruction},
  author={Rohrbach, Anna and Rohrbach, Marcus and Hu, Ronghang and Darrell, Trevor and Schiele, Bernt},
  booktitle={ECCV},
  year={2016}
}
@inproceedings{weak2,
  title={Unsupervised textual grounding: Linking words to image concepts},
  author={Yeh, Raymond A and Do, Minh N and Schwing, Alexander G},
  booktitle={CVPR},
  year={2018}
}
@inproceedings{weak3,
  title={Knowledge aided consistency for weakly supervised phrase grounding},
  author={Chen, Kan and Gao, Jiyang and Nevatia, Ram},
  booktitle={CVPR},
  year={2018}
}
@article{xie2019visual,
  title={Visual entailment: A novel task for fine-grained image understanding},
  author={Xie, Ning and Lai, Farley and Doran, Derek and Kadav, Asim},
  journal={arXiv preprint},
  url={https://arxiv.org/pdf/1901.06706},
  year={2019}
}
@inproceedings{sharma2018conceptual,
  title={Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
  author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
  booktitle={ACL},
  year={2018}
}
@inproceedings{nce1,
  title={Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  author={Gutmann, Michael and Hyv{\"a}rinen, Aapo},
  booktitle={AISTATS},
  year={2010}
}
@article{nce2,
  title={Exploring the limits of language modeling},
  author={Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
  journal={arXiv preprint},
  url={https://arxiv.org/pdf/1602.02410.pdf},
  year={2016}
}
@article{bugliarello-etal-2020-multimodal,
    title = "Multimodal Pretraining Unmasked: {U}nifying the Vision and Language {BERT}s",
    author = "Bugliarello, Emanuele  and
      Cotterell, Ryan and
      Okazaki, Naoaki and
      Elliott, Desmond",
    journal = "TACL",
    year = "2021",
    url = "https://arxiv.org/abs/2011.15124",
}
@inproceedings{weak4,
  title={Phrase Localization Without Paired Training Examples},
  author={Wang, Josiah and Specia, Lucia},
  booktitle={ICCV},
  url={http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Phrase_Localization_Without_Paired_Training_Examples_ICCV_2019_paper.pdf},
  year={2019}
}
@inproceedings{weak5,
  title={Unsupervised Discovery of Multimodal Links in Multi-image, Multi-sentence Documents},
  author={Hessel, Jack and Lee, Lillian and Mimno, David},
  booktitle={EMNLP},
  url={https://arxiv.org/pdf/1904.07826},
  year={2019}
}

@inproceedings{cao2020behind,
  title={Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models},
  author={Cao, Jize and Gan, Zhe and Cheng, Yu and Yu, Licheng and Chen, Yen-Chun and Liu, Jingjing},
  booktitle={ECCV},
  url={https://arxiv.org/pdf/2005.07310},
  year={2020}
}

@inproceedings{kazemzadeh2014referitgame,
  title={Referitgame: Referring to objects in photographs of natural scenes},
  author={Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara},
  booktitle={EMNLP},
  url={https://www.aclweb.org/anthology/D14-1086.pdf},
  year={2014}
}
@inproceedings{li2020does,
  title={What Does {BERT} with Vision Look At?},
  author={Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
  booktitle={ACL},
  year={2020}
}
@inproceedings{antol2015vqa,
  title={{VQA}: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={ICCV},
  year={2015}
}
@article{chen2015microsoft,
  title={Microsoft {COCO} {C}aptions: Data collection and evaluation server},
  author={Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  journal={arXiv preprint},
  url={https://arxiv.org/pdf/1504.00325},
  year={2015}
}

@article{krishna2017visual,
  title={Visual {G}enome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={IJCV},
  year={2017},
  url={https://link.springer.com/article/10.1007/S11263-016-0981-7}
}
@article{ren2015faster,
  title={Faster {R-CNN}: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={TPAMI},
  year={2016},
  url={https://ieeexplore.ieee.org/iel7/34/4359286/07485869.pdf?casa_token=lVQqGQcyZaMAAAAA:C5IsxOq5J16PZeRG9kM7i32Pxbkj5c_ASp_NSNYbyzfFZbwwG1E7a8xiIqGCcxe4qqE1543m},
}
@inproceedings{goyal2017making,
  title={Making the {V} in {VQA} matter: Elevating the role of image understanding in Visual Question Answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={CVPR},
  url={http://openaccess.thecvf.com/content_cvpr_2017/papers/Goyal_Making_the_v_CVPR_2017_paper.pdf},
  year={2017}
}

@inproceedings{yu2018mattnet,
  title={{MAttNet}: Modular attention network for referring expression comprehension},
  author={Yu, Licheng and Lin, Zhe and Shen, Xiaohui and Yang, Jimei and Lu, Xin and Bansal, Mohit and Berg, Tamara L},
  booktitle={CVPR},
  year={2018}
}


@inproceedings{doersch2015unsupervised,
  title={Unsupervised visual representation learning by context prediction},
  author={Doersch, Carl and Gupta, Abhinav and Efros, Alexei A},
  booktitle={ICCV},
  url={https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf},
  year={2015}
}
@inproceedings{xie2020fast,
  title={A fast proximal point method for computing exact wasserstein distance},
  author={Xie, Yujia and Wang, Xiangfeng and Wang, Ruijia and Zha, Hongyuan},
  booktitle={UAI},
  year={2020},
  url={http://proceedings.mlr.press/v115/xie20b/xie20b.pdf},
}
@inproceedings{deng2018visual,
  title={Visual grounding via accumulated attention},
  author={Deng, Chaorui and Wu, Qi and Wu, Qingyao and Hu, Fuyuan and Lyu, Fan and Tan, Mingkui},
  booktitle={CVPR},
  url={https://openaccess.thecvf.com/content_cvpr_2018/papers/Deng_Visual_Grounding_via_CVPR_2018_paper.pdf},
  year={2018}
}
@inproceedings{sun2019videobert,
  title={Video{BERT}: A joint model for video and language representation learning},
  author={Sun, Chen and Myers, Austin and Vondrick, Carl and Murphy, Kevin and Schmid, Cordelia},
  booktitle={ICCV},
  url={http://openaccess.thecvf.com/content_ICCV_2019/papers/Sun_VideoBERT_A_Joint_Model_for_Video_and_Language_Representation_Learning_ICCV_2019_paper.pdf},
  year={2019}
}

@inproceedings{peters2018deep,
  title={Deep Contextualized Word Representations},
  author={Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  booktitle={NAACL},
  url={https://www.aclweb.org/anthology/N18-1202.pdf},
  year={2018}
}
@inproceedings{pathak2016context,
  title={Context encoders: Feature learning by inpainting},
  author={Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A},
  booktitle={CVPR},
  url={http://openaccess.thecvf.com/content_cvpr_2016/papers/Pathak_Context_Encoders_Feature_CVPR_2016_paper.pdf},
  year={2016}
}

@inproceedings{hubert2017learning,
  title={Learning robust visual-semantic embeddings},
  author={Hubert Tsai, Yao-Hung and Huang, Liang-Kang and Salakhutdinov, Ruslan},
  booktitle={ICCV},
  url={http://openaccess.thecvf.com/content_ICCV_2017/papers/Tsai_Learning_Robust_Visual-Semantic_ICCV_2017_paper.pdf},
  year={2017}
}
@inproceedings{hill2014learning,
  title={Learning abstract concept embeddings from multi-modal data: Since you probably canâ€™t see what I mean},
  author={Hill, Felix and Korhonen, Anna},
  booktitle={EMNLP},
  url={https://www.aclweb.org/anthology/D14-1032.pdf},
  year={2014}
}

@inproceedings{silberer2014learning,
  title={Learning grounded meaning representations with autoencoders},
  author={Silberer, Carina and Lapata, Mirella},
  booktitle={ACL},
  url={https://www.aclweb.org/anthology/P14-1068.pdf},
  year={2014}
}
@inproceedings{ngiam2011multimodal,
  title={Multimodal deep learning},
  author={Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
  booktitle={ICML},
  url={https://openreview.net/pdf?id=Hk4OO3W_bS},
  year={2011}
}
@inproceedings{loshchilov2018decoupled,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={ICLR},
  year={2018}
}
@inproceedings{anderson2018bottom,
  title={Bottom-up and top-down attention for image captioning and visual question answering},
  author={Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  booktitle={CVPR},
  url={http://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.pdf},
  year={2018}
}
@inproceedings{li2020unicoder,
  title={Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training},
  author={Li, Gen and Duan, Nan and Fang, Yuejian and Gong, Ming and Jiang, Daxin},
  booktitle={AAAI},
  year={2020}
}
@inproceedings{cohn2016incorporating,
  title={Incorporating Structural Alignment Biases into an Attentional Neural Translation Model},
  author={Cohn, Trevor and Hoang, Cong Duy Vu and Vymolova, Ekaterina and Yao, Kaisheng and Dyer, Chris and Haffari, Gholamreza},
  booktitle={NAACL},
  url={https://www.aclweb.org/anthology/N16-1102.pdf},
  year={2016}
}
@article{hu2020explicit,
  title={Explicit Alignment Objectives for Multilingual Bidirectional Encoders},
  author={Hu, Junjie and Johnson, Melvin and Firat, Orhan and Siddhant, Aditya and Neubig, Graham},
  journal={arXiv preprint},
  url={https://arxiv.org/pdf/2010.07972.pdf},
  year={2020}
}
@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={NeurIPS},
  year={2014}
}

@inproceedings{cho2014learning,
  title={Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation},
  author={Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  booktitle={EMNLP},
  year={2014}
}




@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={CVPR},
  year={2016}
}

@inproceedings{jiang2020defense,
  title={In defense of grid features for visual question answering},
  author={Jiang, Huaizu and Misra, Ishan and Rohrbach, Marcus and Learned-Miller, Erik and Chen, Xinlei},
  booktitle={CVPR},
  year={2020}
}

@article{liu2019roberta,
  title={Ro{BERT}a: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint},
  year={2019}
}

@inproceedings{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  booktitle={ICLR},
  year={2020}
}
@inproceedings{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  booktitle={ICLR},
  year={2020}
}
@article{he2020deberta,
  title={De{BERT}a: Decoding-enhanced bert with disentangled attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint},
  year={2020}
}

@inproceedings{shen2021much,
  title={How Much Can CLIP Benefit Vision-and-Language Tasks?},
  author={Shen, Sheng and Li, Liunian Harold and Tan, Hao and Bansal, Mohit and Rohrbach, Anna and Chang, Kai-Wei and Yao, Zhewei and Keutzer, Kurt},
  booktitle={ICLR},
  year={2022}
}
@inproceedings{suhr2018corpus,
  title={A corpus for reasoning about natural language grounded in photographs},
  author={Suhr, Alane and Zhou, Stephanie and Zhang, Ally and Zhang, Iris and Bai, Huajun and Artzi, Yoav},
  booktitle={ACL},
  year={2019}
}
@article{krishna2016visual,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={IJCV},
  year={2017},
}
@inproceedings{yu2018deep,
  title={Deep layer aggregation},
  author={Yu, Fisher and Wang, Dequan and Shelhamer, Evan and Darrell, Trevor},
  booktitle={CVPR},
  year={2018}
}
@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={CVPR},
  year={2017}
}
@inproceedings{bapna2018training,
  title={Training Deeper Neural Machine Translation Models with Transparent Attention},
  author={Bapna, Ankur and Chen, Mia Xu and Firat, Orhan and Cao, Yuan and Wu, Yonghui},
  booktitle={EMNLP},
  year={2018}
}
@inproceedings{dou2018exploiting,
  title={Exploiting Deep Representations for Neural Machine Translation},
  author={Dou, Zi-Yi and Tu, Zhaopeng and Wang, Xing and Shi, Shuming and Zhang, Tong},
  booktitle={EMNLP},
  year={2018}
}
@inproceedings{jawahar2019does,
  title={What Does {BERT} Learn about the Structure of Language?},
  author={Jawahar, Ganesh and Sagot, Beno{\^\i}t and Seddah, Djam{\'e}},
  booktitle={ACL},
  year={2019}
}
@inproceedings{yu2020gradient,
  title={Gradient Surgery for Multi-Task Learning},
  author={Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},
  booktitle={NeurIPS},
  year={2020}
}
@inproceedings{wang2020gradient,
  title={Gradient Vaccine: Investigating and Improving Multi-task Optimization in Massively Multilingual Models},
  author={Wang, Zirui and Tsvetkov, Yulia and Firat, Orhan and Cao, Yuan},
  booktitle={ICLR},
  year={2020}
}
@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={ICML},
  year={2021}
}
@inproceedings{van2017neural,
  title={Neural discrete representation learning},
  author={van den Oord, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
  booktitle={NeurIPS},
  year={2017}
}
@inproceedings{liu2019text,
  title={Text Summarization with Pretrained Encoders},
  author={Liu, Yang and Lapata, Mirella},
  booktitle={EMNLP},
  year={2019}
}
@inproceedings{selvaraju2017grad,
  title={Grad-{CAM}: Visual explanations from deep networks via gradient-based localization},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={ICCV},
  year={2017}
}
@inproceedings{wang2018glue,
  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle={ICLR},
  year={2019}
}
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={CVPR},
  year={2009},
}
@inproceedings{ordonez2011im2text,
  title={Im2{T}ext: Describing images using 1 million captioned photographs},
  author={Ordonez, Vicente and Kulkarni, Girish and Berg, Tamara},
  booktitle={NeurIPS},
  year={2011}
}

@inproceedings{karpathy2015deep,
  title={Deep visual-semantic alignments for generating image descriptions},
  author={Karpathy, Andrej and Fei-Fei, Li},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{li2021align,
  title={Align before Fuse: Vision and Language Representation Learning with Momentum Distillation},
  author={Li, Junnan and Selvaraju, Ramprasaath R and Gotmare, Akhilesh Deepak and Joty, Shafiq and Xiong, Caiming and Hoi, Steven},
  booktitle={NeurIPS},
  year={2021}
}
@inproceedings{xue2021probing,
  title={Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training},
  author={Xue, Hongwei and Huang, Yupan and Liu, Bei and Peng, Houwen and Fu, Jianlong and Li, Houqiang and Luo, Jiebo},
  booktitle={NeurIPS},
  year={2021}
}

@article{touvron2021going,
  title={Going deeper with image transformers},
  author={Touvron, Hugo and Cord, Matthieu and Sablayrolles, Alexandre and Synnaeve, Gabriel and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint},
  year={2021}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009}
}

@inproceedings{changpinyo2021conceptual,
  title={Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts},
  author={Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  booktitle={CVPR},
  year={2021}
}

@article{yuan2021florence,
  title={Florence: A New Foundation Model for Computer Vision},
  author={Lu Yuan and Dongdong Chen and Yi-Ling Chen and Noel Codella and Xiyang Dai and Jianfeng Gao and Houdong Hu and Xuedong Huang and Boxin Li and Chunyuan Li and Ce Liu and Mengchen Liu and Zicheng Liu and Yumao Lu and Yu Shi and Lijuan Wang and Jianfeng Wang and Bin Xiao and Zhen Xiao and Jianwei Yang and Michael Zeng and Luowei Zhou and Pengchuan Zhang},
  journal={arXiv preprint},
  year={2021}
}
@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={ICCV},
  year={2021}
}
@article{yuan2021volo,
      title={VOLO: Vision Outlooker for Visual Recognition}, 
      author={Li Yuan and Qibin Hou and Zihang Jiang and Jiashi Feng and Shuicheng Yan},
      journal={arXiv preprint},
      year={2021},
}
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NeurIPS},
  year={2017}
}
@inproceedings{sennrich2016neural,
  title={Neural Machine Translation of Rare Words with Subword Units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle={Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2016}
}
@article{bao2021beit,
  title={{BEiT}: BERT Pre-Training of Image Transformers},
  author={Bao, Hangbo and Dong, Li and Wei, Furu},
  journal={arXiv preprint},
  year={2021}
}
@article{touvron2020deit,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Herv\'e J\'egou},
  journal={arXiv preprint},
  year={2020}
}
@inproceedings{dosovitskiy2020image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={ICLR},
  year={2021}
}
@article{huang2020pixel,
  title={Pixel-{BERT}: Aligning image pixels with text by deep multi-modal transformers},
  author={Huang, Zhicheng and Zeng, Zhaoyang and Liu, Bei and Fu, Dongmei and Fu, Jianlong},
  journal={arXiv preprint},
  year={2020}
}

@inproceedings{wang2021simvlm,
  title={{SimVLM}: Simple Visual Language Model Pretraining with Weak Supervision},
  author={Wang, Zirui and Yu, Jiahui and Yu, Adams Wei and Dai, Zihang and Tsvetkov, Yulia and Cao, Yuan},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{gan2020large,
  title={Large-scale adversarial training for vision-and-language representation learning},
  author={Gan, Zhe and Chen, Yen-Chun and Li, Linjie and Zhu, Chen and Cheng, Yu and Liu, Jingjing},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{li2020unimo,
  title={{UNIMO}: Towards unified-modal understanding and generation via cross-modal contrastive learning},
  author={Li, Wei and Gao, Can and Niu, Guocheng and Xiao, Xinyan and Liu, Hao and Liu, Jiachen and Wu, Hua and Wang, Haifeng},
  booktitle={ACL},
  year={2021}
}

@inproceedings{huang2021seeing,
  title={Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning},
  author={Huang, Zhicheng and Zeng, Zhaoyang and Huang, Yupan and Liu, Bei and Fu, Dongmei and Fu, Jianlong},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{li2021grounded,
  title={Grounded Language-Image Pre-training},
  author={Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and others},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{dou2021empirical,
  title={An Empirical Study of Training End-to-End Vision-and-Language Transformers},
  author={Dou, Zi-Yi and Xu, Yichong and Gan, Zhe and Wang, Jianfeng and Wang, Shuohang and Wang, Lijuan and Zhu, Chenguang and Zhang, Pengchuan and Yuan, Lu and Peng, Nanyun and Liu, Zicheng and Zeng, Michael},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{dai2021dynamic,
  title={Dynamic head: Unifying object detection heads with attentions},
  author={Dai, Xiyang and Chen, Yinpeng and Xiao, Bin and Chen, Dongdong and Liu, Mengchen and Yuan, Lu and Zhang, Lei},
  booktitle={CVPR},
  year={2021}
}

@article{wang2021vlmo,
  title={{VLMo}: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Wei, Furu},
  journal={arXiv preprint},
  year={2021}
}

@article{wang2021ufo,
  title={{UFO}: A UniFied TransfOrmer for Vision-Language Representation Learning},
  author={Wang, Jianfeng and Hu, Xiaowei and Gan, Zhe and Yang, Zhengyuan and Dai, Xiyang and Liu, Zicheng and Lu, Yumao and Wang, Lijuan},
  journal={arXiv preprint},
  year={2021}
}

@inproceedings{agrawal2019nocaps,
  title={nocaps: Novel object captioning at scale},
  author={Agrawal, Harsh and Desai, Karan and Wang, Yufei and Chen, Xinlei and Jain, Rishabh and Johnson, Mark and Batra, Dhruv and Parikh, Devi and Lee, Stefan and Anderson, Peter},
  booktitle={ICCV},
  year={2019}
}

@inproceedings{gupta2019lvis,
  title={{LVIS}: A dataset for large vocabulary instance segmentation},
  author={Gupta, Agrim and Dollar, Piotr and Girshick, Ross},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{li2019unicoder,
  title={Unicoder-{VL}: A Universal Encoder for Vision and Language by Cross-modal Pre-training},
  author={Li, Gen and Duan, Nan and Fang, Yuejian and Jiang, Daxin and Zhou, Ming},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{yu2020ernie,
  title={{ERNIE-ViL}: Knowledge enhanced vision-language representations through scene graph},
  author={Yu, Fei and Tang, Jiji and Yin, Weichong and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{hu2020vivo,
  title={{VIVO}: Surpassing human performance in novel object captioning with visual vocabulary pre-training},
  author={Hu, Xiaowei and Yin, Xi and Lin, Kevin and Wang, Lijuan and Zhang, Lei and Gao, Jianfeng and Liu, Zicheng},
  booktitle={AAAI},
  year={2021}
}

@inproceedings{yang2020tap,
  title={{TAP}: Text-Aware Pre-training for Text-VQA and Text-Caption},
  author={Yang, Zhengyuan and Lu, Yijuan and Wang, Jianfeng and Yin, Xi and Florencio, Dinei and Wang, Lijuan and Zhang, Cha and Zhang, Lei and Luo, Jiebo},
  booktitle={CVPR},
  year={2021}
}

@article{li2022blip,
  title={{BLIP}: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  journal={arXiv preprint},
  year={2022}
}

@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{desai2021virtex,
  title={Vir{T}ex: Learning visual representations from textual annotations},
  author={Desai, Karan and Johnson, Justin},
  booktitle={CVPR},
  year={2021}
}

@inproceedings{sariyildiz2020learning,
  title={Learning visual representations with caption annotations},
  author={Sariyildiz, Mert Bulent and Perez, Julien and Larlus, Diane},
  booktitle={ECCV},
  year={2020}
}

@article{li2021supervision,
  title={Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm},
  author={Li, Yangguang and Liang, Feng and Zhao, Lichen and Cui, Yufeng and Ouyang, Wanli and Shao, Jing and Yu, Fengwei and Yan, Junjie},
  journal={ICLR},
  year={2022}
}

@inproceedings{yao2021filip,
  title={{FILIP}: Fine-grained Interactive Language-Image Pre-Training},
  author={Yao, Lewei and Huang, Runhui and Hou, Lu and Lu, Guansong and Niu, Minzhe and Xu, Hang and Liang, Xiaodan and Li, Zhenguo and Jiang, Xin and Xu, Chunjing},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{mu2021slip,
  title={{SLIP}: Self-supervision meets Language-Image Pre-training},
  author={Mu, Norman and Kirillov, Alexander and Wagner, David and Xie, Saining},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{yang2022unified,
  title={Unified Contrastive Learning in Image-Text-Label Space},
  author={Yang, Jianwei and Li, Chunyuan and Zhang, Pengchuan and Xiao, Bin and Liu, Ce and Yuan, Lu and Gao, Jianfeng},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{hu2021unit,
  title={Uni{T}: Multimodal multitask learning with a unified transformer},
  author={Hu, Ronghang and Singh, Amanpreet},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{gupta2021towards,
  title={Towards general purpose vision systems},
  author={Gupta, Tanmay and Kamath, Amita and Kembhavi, Aniruddha and Hoiem, Derek},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{singh2021flava,
  title={{FLAVA}: A Foundational Language And Vision Alignment Model},
  author={Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
  booktitle={CVPR},
  year={2022}
}

@article{yang2021crossing,
  title={Crossing the Format Boundary of Text and Boxes: Towards Unified Vision-Language Modeling},
  author={Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Ahmed, Faisal and Liu, Zicheng and Lu, Yumao and Wang, Lijuan},
  journal={arXiv preprint},
  year={2021}
}

@article{wang2022unifying,
  title={Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework},
  author={Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  journal={arXiv preprint},
  year={2022}
}

@inproceedings{Lin2017FeaturePN,
  title={Feature Pyramid Networks for Object Detection},
  author={Tsung-Yi Lin and Piotr Doll{\'a}r and Ross B. Girshick and Kaiming He and Bharath Hariharan and Serge J. Belongie},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{Zhai2021LiTZT,
  title={{LiT}: Zero-Shot Transfer with Locked-image Text Tuning},
  author={Xiaohua Zhai and Xiao Wang and Basil Mustafa and Andreas Steiner and Daniel Keysers and Alexander Kolesnikov and Lucas Beyer},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{Zhang2020BridgingTG,
  title={Bridging the Gap Between Anchor-Based and Anchor-Free Detection via Adaptive Training Sample Selection},
  author={Shifeng Zhang and Cheng Chi and Yongqiang Yao and Zhen Lei and Stan Z. Li},
  booktitle={CVPR},
  year={2020}
}

@inproceedings{chen2021pix2seq,
  title={Pix2seq: A language modeling framework for object detection},
  author={Chen, Ting and Saxena, Saurabh and Li, Lala and Fleet, David J and Hinton, Geoffrey},
  booktitle={ICLR},
  year={2022}
}

@article{Dai2021DynamicHU,
  title={Dynamic Head: Unifying Object Detection Heads with Attentions},
  author={Xiyang Dai and Yinpeng Chen and Bin Xiao and Dongdong Chen and Mengchen Liu and Lu Yuan and Lei Zhang},
  journal={CVPR},
  year={2021}
}

@article{Lin2020FocalLF,
  title={Focal Loss for Dense Object Detection},
  author={Tsung-Yi Lin and Priya Goyal and Ross B. Girshick and Kaiming He and Piotr Doll{\'a}r},
  journal={TPAMI},
  year={2020}
}


@article{yu2022coca,
  title={{CoCa}: Contrastive Captioners are Image-Text Foundation Models},
  author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  journal={arXiv preprint},
  year={2022}
}

@inproceedings{Yu2016ModelingCI,
  title={Modeling Context in Referring Expressions},
  author={Licheng Yu and Patrick Poirson and Shan Yang and Alexander C. Berg and Tamara L. Berg},
  booktitle={ECCV},
  year={2016},
}

@inproceedings{Kazemzadeh2014ReferItGameRT,
  title={{ReferItGame}: Referring to Objects in Photographs of Natural Scenes},
  author={Sahar Kazemzadeh and Vicente Ordonez and Marc andre Matten and Tamara L. Berg},
  booktitle={EMNLP},
  year={2014}
}

@inproceedings{papineni2002bleu,
  title={{BLEU}: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={ACL},
  year={2002}
}

@inproceedings{banerjee2005meteor,
  title={{METEOR}: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={ACL Workshops},
  year={2005}
}
@inproceedings{lin2004rouge,
  title={{ROUGE}: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text Summarization Branches Out},
  year={2004}
}
@inproceedings{vedantam2015cider,
  title={{CIDEr}: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={CVPR},
  year={2015}
}
@inproceedings{anderson2016spice,
  title={{SPICE}: Semantic propositional image caption evaluation},
  author={Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
  booktitle={ECCV},
  year={2016},
}
@article{wang2022image,
  title={Image as a foreign language: Beit pretraining for all vision and vision-language tasks},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
  journal={arXiv preprint arXiv:2208.10442},
  year={2022}
}

@article{cho2022dall,
  title={Dall-eval: Probing the reasoning skills and social biases of text-to-image generative transformers},
  author={Cho, Jaemin and Zala, Abhay and Bansal, Mohit},
  journal={arXiv preprint arXiv:2202.04053},
  year={2022}
}

@inproceedings{boser1992training,
  title={A training algorithm for optimal margin classifiers},
  author={Boser, Bernhard E and Guyon, Isabelle M and Vapnik, Vladimir N},
  booktitle={Proceedings of the fifth annual workshop on Computational learning theory},
  pages={144--152},
  year={1992}
}

@book{vapnik1999nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={1999},
  publisher={Springer science \& business media}
}
@article{gan2022vision,
  title={Vision-language pre-training: Basics, recent advances, and future trends},
  author={Gan, Zhe and Li, Linjie and Li, Chunyuan and Wang, Lijuan and Liu, Zicheng and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2210.09263},
  year={2022}
}


@article{schuhmann2021laion,
  title={Laion-400m: Open dataset of clip-filtered 400 million image-text pairs},
  author={Schuhmann, Christoph and Vencu, Richard and Beaumont, Romain and Kaczmarczyk, Robert and Mullis, Clayton and Katta, Aarush and Coombes, Theo and Jitsev, Jenia and Komatsuzaki, Aran},
  journal={arXiv preprint arXiv:2111.02114},
  year={2021}
}


@article{li2023blip,
  title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023}
}



@inproceedings{shou2021generic,
  title={Generic event boundary detection: A benchmark for event segmentation},
  author={Shou, Mike Zheng and Lei, Stan Weixian and Wang, Weiyao and Ghadiyaram, Deepti and Feiszli, Matt},
  booktitle={ICCV},
  pages={8075--8084},
  year={2021}
}

@article{goel2022cyclip,
  title={Cyclip: Cyclic contrastive language-image pretraining},
  author={Goel, Shashank and Bansal, Hritik and Bhatia, Sumit and Rossi, Ryan A and Vinay, Vishwa and Grover, Aditya},
  journal={arXiv preprint arXiv:2205.14459},
  year={2022}
}