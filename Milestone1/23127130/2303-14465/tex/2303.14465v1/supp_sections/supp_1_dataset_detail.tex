%\section{Details of \benchname}

\begin{figure}[h!]
    \centering
    % \footnotesize
    \includegraphics[width=.48\textwidth]{supp_images/eqag.pdf}
    \caption{The invalid examples for AG (top) and our proposed frame filter (bottom).
    }
    \label{fig:sup_eqag}
\end{figure}

\section{Construction Details of \benchname}
\label{sec:data}

In addition to the general construction pipeline of \benchname in Section~\ref{sec:exp_eqbench} of the main paper, here we include more details specific to each subset.
For all subsets built on natural videos, we denote $I_i$ and $I_j$ as two different frames of the video, while $I_{i+1}$ ($I_{j+1}$) represents the immediate next frame following $I_i$ ($I_j$).
\subsection{\textsc{Eq-AG}}

\noindent\textbf{Source Dataset}.
Action Genome~\cite{ji2020action} (AG) captures changes between objects and their pairwise relationships while action occurs. It contains nearly 10K videos with 1.7M visual relationships which can be used for caption generation. Given the scene graph $\langle$person - \textit{attention relationship} - \textit{spatial relationship} - object$\rangle$, we first create the caption with the template ``The person is $\langle$\textit{attention relationship}$\rangle$ $\langle$\textit{object}$\rangle$ which is $\langle$\textit{spatial relationship}$\rangle$ him/her.''

\noindent\textbf{Invalid Samples}. In AG, we find that sometimes it is hard to tell apart the two adjacent frames due to the continuity of the video data. This results in two problems for the dataset construction as shown in Figure~\ref{fig:sup_eqag} (top): 1) The two images $I_i$ and $I_j$ are too similar, and may be described by the same caption; 2) The two sample pairs $\{I_i, I_j\}$ and $\{I_i, I_{j+1}\}$ are too similar, leading to many duplicates. 


\begin{figure}[h!]
    \centering
    % \footnotesize
    \includegraphics[width=.48\textwidth]{supp_images/eqgebc.pdf}
    \caption{The invalid samples for GEBC (top) and our proposed frame filter (bottom). 
    % \wangtan{unify the annotation across different dataset and revise the figure} \linjie{there is no frame filter in this figure. maybe correct the caption or add it to the figure?}
    }
    \label{fig:sup_eqgebc}
\end{figure}


\noindent\textbf{Frame Filter}.
To solve this problem, we adopt a sparse sampling strategy to select frames as candidates (Figure~\ref{fig:sup_eqag} (bottom)). Specifically, we only choose frames $I_i$ and $I_j$ if and only if at least 2 of 3 relationships are different. This will make sure the distinction between two images in a single sample, thus solving problem \#1.
Furthermore, for problem \#2, we assume that given a chosen frame $I_i$ ($I_j$), the immediate next frame $I_{i+1}$ ($I_{j+1}$) is too similar to $I_i$ ($I_j$). Therefore, if $I_i$ ($I_j$) is chosen, we will skip the subsequent frame (red cross in Figure~\ref{fig:sup_eqag} (bottom)), and move to $I_{i+2}$ ($I_{j+2}$). 
% \wangtan{add words for $I_i$} \linjie{please check this paragraph.}


\begin{figure}[h!]
    \centering
    \includegraphics[width=.48\textwidth]{supp_images/eqyoucook.pdf}
    \caption{The invalid samples for YouCook2 (top) and our proposed frame filter (bottom).}
    \label{fig:sup_eqyoucook}
\end{figure}

\subsection{\textsc{Eq-GEBC}}
GEBC~\cite{wang2022geb+} consists of over 170k boundaries associated with captions describing the events before and after the boundaries. It is built upon 
 12K videos from Kinetic-400~\cite{kay2017kinetics} dataset. We construct \textsc{Eq-GEBC} examples based on annotations from the training and validation splits of GEBC.
Intuitively, 
we can directly adopt the frames before and after the boundaries (\ie, $I_i$ and $I_{i+1}$) as our visual minimally different images, and the provided GEBC annotation before and after the boundaries can be naturally leveraged as the captions. 


\noindent\textbf{Invalid Samples}. However, similar to AG, we find that it is hard to tell apart the two images separated by a boundary in practice (see Figure~\ref{fig:sup_eqgebc} (top)). The reason behind is that the boundary of GEBC is annotated as the status change between two video segments (\eg, from ``walking'' to ``running''). Such action words can be hard to recognize from the sampled static frames.




%%%%%%% Visualization %%%%%%%%%
\begin{figure*}[t]
    \centering
    % \footnotesize
    \includegraphics[width=.99\textwidth]{supp_images/supp_vis.pdf}
    \caption{More visualizations of the equivariance score of baselines and our \algname based on FIBER~\cite{dou2022coarse} on other 4 subsets of \benchname.
    }
    \label{fig:sup_vis}
\end{figure*}


\begin{figure*}[t]
    \centering
    % \footnotesize
    \includegraphics[width=.95\textwidth]{supp_images/eqkubric.pdf}
    \caption{Overview of caption generation pipeline for three subsets (\ie, location, counting and attribute) of  \textsc{Eq-Kubric}. Text in red indicates the aspect of semantic change between two captions.
    }
    \label{fig:sup_eqkubric}
\end{figure*}



\noindent\textbf{Frame Filter}.
As shown in Figure~\ref{fig:sup_eqgebc} (bottom), we propose to skip an additional boundary to choose $I_i$ and $I_{i+2}$ as the twin images to enlarge the semantic gap. Meanwhile, we filter out images with captions containing action words (\eg, ``up'', ``down'', ``upward'', ``downward'' and ``towards''), which are hard to infer without temporal information. Finally, to ensure data quality, we perform a manual screening process with 10 graduate students to filter out invalid samples.


\begin{figure*}[t]
    \centering
    % \footnotesize
    \includegraphics[width=.95\textwidth]{supp_images/eqsd.pdf}
    \caption{Overview of caption generation for  \textsc{Eq-SD} dataset. The red color highlights the aspect of semantic change between two captions. 
    }
    \label{fig:sup_eqsd}
\end{figure*}




\noindent\textbf{Invalid Samples}. As shown in Figure~\ref{fig:sup_eqyoucook} (top), we find that for the cooking video, the chosen frame may contain the view of the chef rather than accurately capturing the objects described in the cooking step. This leads to the mismatch between the image and the caption.








\subsection{\textsc{Eq-YouCook2}}
We utilize YouCook2~\cite{zhou2018towards} as the data source which contains 2K YouTube videos with average duration of 5.3 minutes, summing to a total of 176 hours. The videos have been manually annotated with segmentation boundaries and captions. 
On average there are 7.7 segments/captions per video, and 8.8 words per caption. 
 We construct \textsc{Eq-YouCook2} examples based on annotations from the training and validation splits of YouCook2.
For each video with $N$ segments, we directly select the middle frame as $I_i$ and its annotated caption as $T_i$, $i \in \{1, 2, ..., N\}$.





\noindent\textbf{Frame Filter}.
To solve this problem, we adopt a simple yet effective solution with the face detector~\footnote{\url{https://github.com/ageitgey/face_recognition}} for frame filtering. Specifically, we directly discard the frames with human faces.





\subsection{\textsc{Eq-Kubric}}
As introduced in the main paper, \textsc{Eq-Kubric} takes advantage of an open-source graphics engine~\cite{greff2021kubric} to faithfully generate photo-realistic scene for the given captions. Therefore, the visual-minimal images generation has been translated into the semantic-minimally different captions construction. We categorize the caption change into three aspects: \textit{attribute}, \textit{counting} and \textit{location}. Figure~\ref{fig:sup_eqkubric} presents the caption construction details. The semantic-minimally difference is ensured by only intervening the corresponding part in the template while leaving other words unchanged.










\subsection{\textsc{Eq-SD}}
Similar process can be applied to \textsc{Eq-SD}, for which we summarize the construction details in Figure~\ref{fig:sup_eqsd}. We similarly categorize the textual semantic-minimal editing into three aspects: object change, scene change and attribute change. We randomly select from the aforementioned three aspects to construct the semantic-minimally different captions.
However, in contrast to the Kubric engine, the generation quality of the stable diffusion model is heavily correlated to the given textual prompt. Therefore, we design a more fine-grained template selection for SD.  We select $\langle$\textit{scene}$\rangle$ and $\langle$\textit{attribute}$\rangle$ from a more restricted subset based on $\langle$\textit{object}$\rangle$. For example, given the object of ``horse'' and the category of ``object change'' (first row of Figure~\ref{fig:sup_eqsd}), the changed object will be selected from the animals from the same subset (\ie, ``cattle'', ``elephant'', ``goat'', ``deer'', ``camel'' and ``zebra''). Meanwhile, the scene shared across two captions will be selected from the first subset of scene (\ie, ``standing on the grass'', ``in the desert'', ``near the river'' and ``in the zoo'') for rationality. 










%%%%%%% Visualize Example %%%%%%%%%
\begin{figure*}[t]
    \centering
    % \footnotesize
    \includegraphics[width=.99\textwidth]{supp_images/vis_example.pdf}
    \caption{More visualizations of the examples for our \benchname.
    }
    \vspace{10pt}
    \label{fig:sup_example}
\end{figure*}