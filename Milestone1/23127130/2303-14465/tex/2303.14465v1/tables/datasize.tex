\begin{table}[t]
\centering
% \captionsetup{font=footnotesize,labelfont=footnotesize,skip=2pt}
% \setlength\extrarowheight{1pt}

\tablestyle{4pt}{1.2} 
% \scalebox{0.75}{
\resizebox{\linewidth}{!}{
\begin{tabular}{llccccc}
\shline
FT Data & Method & \textsc{Eq-AG} & \textsc{Eq-Y.} & \textsc{Eq-G.} & Wino. & Avg \\ \hline
\multirow{2}{*}{F30K~\cite{plummer2015flickr30k}} & FT & 9.24 & 44.33 & 8.54 & 23.00 & 21.28 \\
 & + \algname & \cellcolor{mygray}\textbf{12.64} & \cellcolor{mygray}\textbf{45.10} & \cellcolor{mygray}\textbf{10.58} & \cellcolor{mygray}\textbf{27.50} & \cellcolor{mygray}\textbf{23.96} \\ 
 \hline
\multirow{2}{*}{COCO~\cite{chen2015microsoft}} & FT & 10.14 & 42.90 & 8.93 & 21.50 & 20.87 \\
 & + \algname & \cellcolor{mygray}\textbf{12.52} & \cellcolor{mygray}\textbf{45.68} & \cellcolor{mygray}\textbf{9.37} & \cellcolor{mygray}\textbf{25.75} & \cellcolor{mygray}\textbf{23.33} \\ \hline
\multirow{2}{*}{F30K + COCO} & FT & 9.96 & 43.81 & 8.93 & 22.75 & 21.36 \\
 & + \algname & \cellcolor{mygray}\textbf{11.98} & \cellcolor{mygray}\textbf{45.80} & \cellcolor{mygray}\textbf{10.47} & \cellcolor{mygray}\textbf{26.50} & \cellcolor{mygray}\textbf{23.69} \\ 
  \hline
\multirow{2}{*}{4M$^{\dagger}$} & FT & 10.49 & 40.95 & 7.49 & 20.99 & 19.98 \\
 & + \algname & \cellcolor{mygray}\textbf{12.78} & \cellcolor{mygray}\textbf{40.96} & \cellcolor{mygray}\textbf{9.81} & \cellcolor{mygray}\textbf{21.25} & \cellcolor{mygray}\textbf{21.20} \\
\shline
\end{tabular}
}
\caption{Group accuracy (\%) of fine-tuning (FT) and \algname based on FIBER on different FT data corpus. 4M data denotes the commonly used pre-training data with about 4M images, including COCO, Visual Genome~\cite{krishna2016visual}, Conceptual Captions~\cite{sharma2018conceptual} and SBU~\cite{ordonez2011im2text}. Y., G., Wino. are the short for YouCook2, GEBC and Winoground. $\dagger$ The model is fine-tuned for 10K steps.
}
\label{tab:datasize}
\end{table}