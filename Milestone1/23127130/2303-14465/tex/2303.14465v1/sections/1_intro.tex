\vspace{-0.1in}
\section{Introduction}
Vision-language (VL) training is all about learning ``good'' features for each modality, 
such that the features should faithfully represent the underlying semantics. Thanks to the large-scale image-text pairs on the Web, we have abundant multimodal supervision for the two features with the same semantic meaning~\cite{tan-bansal-2019-lxmert,li2021align,radford2021learning,jia2021scaling}---each matched image-text pair should have ``similar'' visual and textual features, and each unmatched pair should have ``dissimilar'' ones. Thus, the image-text similarity plays a crucial role to define the feature quality in training VL foundation models (VLMs)~\cite{wang2022image,yu2022coca,li2021align,radford2021learning,jia2021scaling,dou2021empirical,dou2022coarse,li2022blip}. 


\input{images/intro_fig}


Has the prevailing ``matched \textit{vs.} unmatched'' similarity fulfilled its duty? Yes and no. On the one hand, recent VLMs~\cite{rombach2022high,wang2022image,dou2022coarse,radford2021learning,yu2022coca,ramesh2022hierarchical} have demonstrated impressive results in various downstream VL tasks such as image-text retrieval.   
However, on the other hand, it is acknowledged by the community that the VLMs still fall short in \textit{nuanced and complex semantic compositions}~\cite{ramesh2022hierarchical,cho2022dall,parcalabescu2021valse,thrush2022winoground}.
In this regard, we present a text-to-image retrieval example on LAION400M~\cite{schuhmann2021laion} with the most recent SOTA VLM FIBER~\cite{dou2022coarse}.
As shown in Figure~\ref{fig:intro_fig}(a), given the query text ``the house on the \emph{right} side of the road'', we first invite 5 graduate students to rank 25 candidate images from most similar to least similar. The continuously decreasing ranking from human judges (\inlineimg{images-inline/black-line}) is served as the oracle semantic similarity measure.
We then compared this ranking with the ones from FIBER~\cite{dou2022coarse} (\inlineimg{images-inline/green-line}). 
Although FIBER correctly retrieved the top-$1$ image (image\#1, ranks 1), some semantically incorrect images (\eg, image\#25, ranks 17) are falsely ranked higher than the correct ones (\eg, image\#2, ranks 20).
Furthermore, when modifying the query text with a slight semantic change (``\underline{right}'' $\rightarrow$ ``\underline{left}''), the rankings remain almost the same.
Clearly, the similarity changes in FIBER do not faithfully reflect the semantic changes in images (\#1 $\rightarrow$ \#25) or text queries (``\underline{right}'' $\rightarrow$ ``\underline{left}'').








To quantitatively measure the above inconsistency between semantic and similarity score changes, we consider two matched image-text pairs $\{I_1, T_1\}$ and $\{I_2, T_2\}$ that are semantically similar but only different in the number of clocks in Figure~\ref{fig:intro_fig} (b). 
With a slight change of clock counts in caption (``2''$\rightarrow$``3''), FIBER mistakenly assigns a higher similarity score to $\{I_1,T_2\}$ rather than $\{I_1,T_1\}$ ($3.83$ v.s. $3.79$). Furthermore, the changes in similarity scores guided by the semantic change (``2''$\leftrightarrow$``3'') 
are highly inconsistent ($+0.04$ v.s. $-1.81$). 
Ideally, an \textbf{\emph{equivariant}} image-text similarity measure should faithfully reflect the semantic change, \ie, the same semantic changes should lead to a similar amount of similarity changes (\eg, $-0.22$ v.s. $-0.17$ of ours in Figure~\ref{fig:intro_fig}(b)).






\input{images/intro_semantic}



\noindent\textbf{Equivariance Loss}. To address this non-equivariance issue, we propose Equivariant Similarity Learning (\algname), which imposes additional equivariance regularization on image-text pairs for VLM learning without additional supervision. Figure~\ref{fig:intro_semantic} illustrates the underlying semantics perceived by human, where each matched pair demonstrates the image and text corresponding to the underlying semantic. 
Given two matched image-text pairs $\{I_1,T_1\}$ as semantic 1 and $\{I_2,T_2\}$ as semantic 2, we can obtain four similarity scores $s_{11}$, $s_{12}$, $s_{22}$, and $s_{21}$. We define \textbf{\emph{Equivariant Similarity}} to be an image-text similarity function, whose output value should correspond to the underlying semantic change, which can be measured by text or image change. 

\vspace{-4pt}
\begin{definition} (Equivariant Similarity)
The similarity $s$ between image and text is equivariant if and only if the following equations hold:

\vspace{-4mm}
{\small{
\begin{equation}
%\begin{align}
s_{11}-s_{12} =  \underbrace{\sum\nolimits_{T_1}^{T_2} \mu(T)},
~~~s_{22}-s_{21} =  \underbrace{\sum\nolimits_{T_2}^{T_1} \mu(T)},
%\end{align}
\vspace{-0.2cm}
\label{eq:intro_eq1}
\end{equation}
}
\hspace{20mm} Semantic Change Measured by Text Change
}

\vspace{-0.4cm}
% \noindent Semantic change by image change:
{\small{
\begin{equation}
s_{11}-s_{21} =  \underbrace{\sum\nolimits_{I_1}^{I_2} \mu(I)},
~~~s_{22}-s_{12} =  \underbrace{\sum\nolimits_{I_2}^{I_1} \mu(I)},
\vspace{-0.2cm}
\label{eq:intro_eq2}
\end{equation}
\hspace{20mm} Semantic Change Measured by Image Change
}}\label{def:eqsim}
\end{definition}
where $\mu(I)$ ($\mu(T)$) denotes the measure~\cite{royden1988real} in image (text) space, \ie, an infinitesimal unit of visual (textual) change. 
Based on Definition~\ref{def:eqsim}, we formally derive \algname, an equivariance loss for a hybrid learning strategy on both semantically close and distant training pairs (Section~\ref{sec:method}). Specifically, \algname directly enforces $s_{11}-s_{12}=s_{22}-s_{21}$ and $s_{11}-s_{21}=s_{22}-s_{12}$ for semantically close samples; while for semantically distant samples, we derive a simplified formulation of $s_{12}=s_{21}$.
We show that adding \algname as a regularization term improves existing similarity training objectives significantly on challenging datasets (\eg, over $4\%$ on Winoground~\cite{thrush2022winoground}) and tricky tasks (\eg, around $30\%$ on VALSE~\cite{parcalabescu2021valse}). \algname can also retain or even improve retrieval performance on Flickr30K~\cite{plummer2015flickr30k} dataset. 


\noindent\textbf{Equivariance Benchmark}. To further facilitate the proper evaluation of equivariance in VL community, we present a novel evaluation benchmark dubbed \benchname (Section \ref{sec:dataset}). 
Motivated by the examples in Figure~\ref{fig:intro_fig}(b), \benchname features ``slightly'' mis-matched pairs with a \emph{minimal semantic drift} from the matched pairs, as opposed to ``very different'' matched and unmatched pairs that are easily distinguishable by both non-equivariant and equivariant similarities.  
Unlike recent efforts~\cite{parcalabescu2021valse,thrush2022winoground} focusing on minimal semantic changes in captions, \benchname pivots on diverse \textit{visual}-minimal changes, automatically curated from time-varying visual contents in natural videos and synthetic engines with more precise control. 
We benchmark a full spectrum of VLMs on \benchname, and reveal that the non-equivariant similarity in existing VLMs fails easily. On this new test bed, \algname can serve as a remedy and bring a large performance gain of $\sim$3\% on average. 











 





    
% conclude motivation
Our contributions are summarized as follows: \textbf{(1)} We comprehensively study the problem of similarity equivariance in VLMs. We propose \algname for equivariant training and \benchname for diagnostic evaluation; \textbf{(2)} \algname is not only theoretically grounded but also simple, effective and easily pluggable; and \textbf{(3)} \benchname clearly diagnoses that conventional evaluation is not responsive to equivariance. Furthermore, \algname can significantly improve VLMs on \benchname, as well as other challenging benchmarks. 
% in three-fold:
% \vspace{-2pt}
% \begin{itemize}
% \setlength\itemsep{-2pt}
%     \item We comprehensively study the problem of similarity equivariance in VLMs. We propose \algname for equivariant training and \benchname for diagnostic evaluation.
%     \item \algname is not only theoretically grounded but also simple, effective and easily pluggable.
%     \item \benchname clearly diagnoses that conventional evaluation is not responsive to equivariance. Furthermore, \algname can significantly improve VLMs on \benchname, as well as other challenging benchmarks. 
    
    
%     % \keli{\item We introduce similarity equivariance}
%     % \keli{\item \algname is theoretically grounded, and effectively improves existing VMLs on multiple diagnosis benchmarks including Winoground and VALSE.}
%     % \keli{\item We present a new challenging benchmark \benchname, and benckmark a full spectrum of VLMs, revealing ...}
    
%     % \linjie{The last sentence does not read correct.}
% \end{itemize}
% \vspace{-2pt} 
    
