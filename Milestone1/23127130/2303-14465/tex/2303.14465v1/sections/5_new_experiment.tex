

\section{Experiments}
We first introduce our experimental setting in Section~\ref{sec:exp-setup}, 
followed by evaluation of \algname on existing benchmarks in Section~\ref{sec:exp_eqsim}.
Section~\ref{sec:exp_eqbench} benchmarks SOTA VLMs on \benchname to show their insensitivity to minimal visual semantic changes, and we further validate \algname on \benchname. Section~\ref{sec:ablation} presents additional ablation studies to examine the design of \algname. 

\subsection{Experimental Setting}
\label{sec:exp-setup}
\noindent\textbf{Training Details}. Recent efforts on diagnosing benchmarks~\cite{thrush2022winoground,parcalabescu2021valse} only provide testing data and directly evaluate models after VL pre-training. The low performance reported on these benchmarks can mainly be attributed to two factors: 1) the inherent weaknesses of VLMs, \eg, non-equivariant similarity measure; and 2) the domain gap between training and testing. To better validate the effectiveness of our method, we fine-tune the VLMs on limited image-text pairs from conventional retrieval dataset Flickr30K~\cite{plummer2015flickr30k} with or without the regularization term of \algname, and then test the fine-tuned VLMs on the challenging Winoground~\cite{thrush2022winoground}, VALSE~\cite{parcalabescu2021valse} and our \benchname. Under a fair comparison, we argue that the absolute performance improvements from \algname thus would suggest that the gain is entirely from the remedy of model weaknesses.

To validate the effectiveness and genraliazability of our proposed method, we apply \algname to two SOTA end-to-end methods with different architectures and retrieval losses. Specifically, FIBER~\cite{dou2022coarse} supports the dual encoder with ITC loss for fast retrieval, which computes similarities for $N^2$ image-text pairs with only $O(N)$ forwarding. In contrast, the SOTA fusion-encoder model METER~\cite{dou2021empirical}, optimized with ITM task during pre-training, computes the similarity by forwarding the concatenation of each pair of image and text, resulting in $O(N^2)$ time complexity. Fine-tuning details for each model can be found in Appendix.

\noindent\textbf{Evaluation metric}. On \textbf{Winoground}~\cite{thrush2022winoground}, given two image-text pairs $\{I_1,T_1\}$ and $\{I_2,T_2\}$, a VLM measures similarity $s_{ij}$  between image $I_i$ and text $T_j$ $(i,j\in\{0,1\}, i\neq j)$. Three metrics are computed based on $s_{ij}$: 1) \emph{Text score} measures whether the model can select the correct text for a given image. The model wins one point if $s_{ii} > s_{ij}$. 2) \emph{Image score} 
 evaluates if VLMs can select the correct image for a given text and the model wins one point when $s_{ii} > s_{ji}$. 3) \emph{Group score} combines the previous two, such that the VLMs win one point if and only if both text score and image score are 1, meaning the following condition must be satisfied: $s_{ii} > s_{ij}$ and $s_{ii} > s_{ji}$.
On \textbf{VALSE}~\cite{parcalabescu2021valse}, the two image-text pairs share a common image, \ie, $\{I_1, T_1\}$ (correct) and $\{I_1, T_2\}$ (foil). We follow~\cite{parcalabescu2021valse} to report the following metrics: 1) \emph{acc} is the overall accuracy on both correct and foil image-text pairs; and 2) \emph{min($p_c,p_f$)} is the minimum of precision $p_c$ and foil precision $p_f$, where $p_c$ ($p_f$) measures how well models identify the correct (foil) pair.
We also report performance on the conventional image-text retrieval task, where recall R@K (K=1,5,10) is used as the evaluation metric.

\input{tables/eqbench.tex}
\subsection{Evaluation of \algname}\label{sec:exp_eqsim}






In Table~\ref{tab:eqsim_eval}, we compare model performance under three settings: ($i$) direct evaluation after pre-training (the first rows of each block); ($ii$) standard fine-tuning (FT) on Flickr30K training data (the second rows of each block); and ($iii$) fine-tuning with \algname regularization (the third rows of each block). 
We observe that standard fine-tuning can somewhat bring a little performance improvement on both Winoground and VALSE benchmarks, indicating that some domain overlap between Flickr30K training data and testing samples.
It is difficult to entirely rule out the domain influence, but comparing fine-tuning with \algname against standard fine-tuning, 
our method brings consistent and significant performance improvements on both of the challenging Winoground and VALSE across METER and FIBER models. Specifically, \algname improves the group score over standard fine-tuning by $4\%$ for METER and $4.5\%$ for FIBER on Winoground, respectively. 
While for VALSE, the performance improvement on min$(p_c,p_f)$ is as large as $31.6\%$, further validating the effectiveness of our \algname. 
In addition, we observe that the equivariance regularization from \algname does not sacrifice retrieval performance. On Flickr30K, \algname can mostly retain the retrieval performance, and sometimes even yield performance gain, \eg, $3.1\%$ on R@1 for image-to-text retrieval with FIBER. 


\input{tables/benchmark_table2}






\subsection{Benchmarking VLMs with \benchname}\label{sec:exp_eqbench}
We evaluate a wide range of VLMs with different configurations on \benchname in a zero-shot manner, to examine the equivariance of their similarity measures for distinguishing visually-minimal different samples. We consider representative VLMs, including ($i$) LXMERT~\cite{tan-bansal-2019-lxmert}, ViLBERT~\cite{lu2019vilbert} for OD-Based models; and ($ii$) CLIP~\cite{radford2021learning} variants,  FLAVA~\cite{singh2021flava}, ViLT~\cite{kim2021vilt}, ALBEF~\cite{li2021align}, BLIP~\cite{li2022blip} and METER~\cite{dou2021empirical} and FIBER~\cite{dou2022coarse}  as prominent examples of end-to-end SOTA methods.
Full results on more VLMs can be found in Appendix~\ref{supp_sec:eqben_results}.  For evaluation metrics, we adopt text score, image score and group score to compare model performance, similar to Winoground~\cite{thrush2022winoground}.










Table~\ref{tab:eqbench} presents the evaluation results of existing VLMs on \benchname and we summarize our observations below.
\vspace{-3pt}
\begin{itemize}[leftmargin=*]
\setlength\itemsep{-2pt}
    \item Regardless of the subsets, end-to-end VLMs generally achieve better performance as it is not constrained by the fixed visual representation from a pre-trained object detector~\cite{ren2015faster}, as in OD-based methods.
    \item Among all subsets, VLMs obtain evidently higher performance on \textsc{Eq-SD}. 
    The stable diffusion model~\cite{rombach2022high} is pre-trained on  similar VL corpus to these VLMs. Hence, the generated images can be biased towards the same underlying data distribution, much easier for VLMs to tell the differences.
    Besides, the generated images maybe visually minimally different to human eyes, but it is unclear whether in the pixel space, they are minimally different \wrt the model input. 
    It is worth noting that LXMERT and ViLBERT are the exception due to the totally different distribution with the off-the-shelf object detector.
    \item Interestingly, a larger pre-training corpus (\eg, CLIP~\cite{radford2021learning} and FLAVA~\cite{singh2021flava}) does not always guarantee better results. This 
    implies training loss may be more critical in learning equivariant similarity measure.


    \item In Table~\ref{tab:kubric}, we further conduct a fine-grained examination with the synthetic subset \textsc{EQ-Kuric}, where we focus on specific visual changes in location, counting and attribute.
    VLMs fail substantially in terms of location and counting, while being sensitive to attribute changes. Similar findings are also observed by ~\cite{thrush2022winoground, parcalabescu2021valse} from the text side. 
\end{itemize}
\vspace{-3pt}
We again equip the two strong baseline models (METER and FIBER) with \algname and fine-tune on Flickr30K. As \benchname covers diverse domains, standard fine-tuning on Flickr30K can hardly improve or even hurt model performance, compared with direct evaluation after pre-training (with $-0.62\%$ and $-1.46\%$ performance drop for METER and FIBER, respectively). However, by enforcing equivariant constraint with \algname, we observe significant performance improvements than standard fine-tuning, with an absolute gain of $3.26\%$ for METER and $1.92\%$ for FIBER.


\input{tables/datasize}














