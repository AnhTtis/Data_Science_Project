\subsection{Ablation Study}
\label{sec:ablation}

In this section, we conduct ablation studies to validate the scalability, design and effectiveness of \algname in terms of enforcing equivariant similarity. 

\noindent\textbf{Scalability of \algname}. Table~\ref{tab:datasize} evaluates the scalability of \algname and standard fine-tuning baseline on the natural subsets of \benchname and Winoground by gradually including more training data. Under the same fine-tuning data, \algname achieves consistent and significant improvements ($2\%$ - $3\%$) over the baseline. Interestingly, there is no remarkable correlation between the corpus size and model performance. This may be due to the distribution of standard VL data is far away from that of \benchname and Winoground. 
Note that for the 4M experiment, we fine-tune the models for 10K steps due to computational constraints. Our results demonstrate the potential of \algname to benefit VL pre-training on large-scale data.
Additionally, we validate the generalizability of \algname in other relevant downstream tasks. Further details are provided in Appendix~\ref{supp_sec:generalization}.






\noindent\textbf{Ablation on \algname design}.
Table~\ref{tab:abla_arch} compares \algname against the four ablated instances on \textsc{Eq-Kubric} and Winoground~\cite{thrush2022winoground}, including 1) fine-tuning with hard negative sampling (HardNeg);  
2) applying \algnamev to all samples in the training batch (\algnamev-all); 3) applying \algnamevv to all samples in the training batch (\algnamevv-all); and 4) applying \algnamevv for only semantically close samples (\algnamevv-close). The final \algname is equivalent to \algnamev-all + \algnamevv-close,
which achieves the best performance. Notably, enforcing \algnamevv on all (\algnamevv-all) even degrades the performance by -0.58\% on average, compared to applying only to semantically close samples (\algnamevv-close). This validates our claim in Section~\ref{sec:method} that \algnamevv is better suited for semantically close samples.




\noindent\textbf{Validation of equivariance via \algname}.
Given the similarity scores $s$ calculated by a VLM, we can define the equivariance score as the derivation of \algnamevv (headline of Figure~\ref{fig:intro_hist}) to measure the degree of equivariance (the smaller, the better). In Figure~\ref{fig:intro_hist}, we plot the distribution of \algnamevv values across all samples in \textsc{EQ-Youcook2} dataset for FIBER~\cite{dou2022coarse} and its variants, attached with their group scores. A tighter curve indicates smaller derivation, hence better equivariance similarity measure. 
Full results on other \benchname subset are presented in Appendix~\ref{supp_sec:distribution_curve}.
Compared with pre-training only (PT), fine-tuning on Flickr30K (FT) can improve the group score  while being more equivariant in the similarity measure.
Adding \algname (Ours) obtains additional improvements on both similarity equivariance and group score, indicating \algname indeed enforces equivariant similarity measure. 
Additionally, due to the space limitation, we leave more visualizations in Appendix~\ref{supp_sec:visualization}.







\input{tables/ablation_arch}

\input{images/intro_hist}