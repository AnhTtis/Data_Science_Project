\section{More Results}
\label{sec:results}

In this section, we include full illustrations and additional experimental results, due to the space limitation of the main paper.

\begin{figure*}[t!]
    \centering
    % \footnotesize
    \includegraphics[width=.95\textwidth]{supp_images/full_rank.pdf}
    \caption{Full ranking results of Figure~\ref{fig:intro_fig}.
    }
    \label{fig:full_rank}
\end{figure*}

\subsection{Full Ranking Results of Figure~\ref{fig:intro_fig}}
\label{supp_sec:full_rank}

In Figure~\ref{fig:intro_fig} of the main paper, we perform a toy experiment on LAION400M to compare the similarity measure of FIBER and the human oracle. Due to the space limitation, we only show partial ranking results in the main paper. Here we illustrate the full ranking in Figure~\ref{fig:full_rank}. 
% The observation keeps consistent with that in the main paper.
With the full ranking results, the observation we summarize in the main paper becomes more clear. That is,  the similarity changes in
FIBER do not faithfully reflect the semantic changes in images (\#1 $\rightarrow$ \#25) or text queries ``righ'' $\rightarrow$ ``left‚Äù).




\subsection{Retrieval Results on COCO dataset}


We report the retrieval performance of FIBER~\cite{dou2022coarse} variants on COCO~\cite{chen2015microsoft} 5K test split in Table~\ref{tab:supp_indomain}. We observe similar trends on COCO to that on Flickr30K in Table~\ref{tab:eqsim_eval}. The results suggest the effectiveness of the proposed \algname, which brings large performance gain across all metrics. 
\input{supp_tables/supp_indomain_coco}



\subsection{Full Results of Table~\ref{tab:datasize} and Table~\ref{tab:abla_arch}}

We show the full results of ablation studies in Table~\ref{tab:supp_datasize} and Table~\ref{tab:supp_abla_arch}, with group scores across all 5 subsets of \benchname and Winoground.
The observation is similar to the main paper. From Table~\ref{tab:supp_datasize}, we can find that \algname is scalable in terms of training data, showing the potential to benefit VL pre-training. The solitary exception happens on \textsc{Eq-SD}, where \algname cannot consistently obtain the improvements. We hypothesize this is probably because \textsc{Eq-SD} is biased towards the same underlying distribution with the VLMs, as discussed in the main paper. 
With Table~\ref{tab:supp_abla_arch}, we can find that \algname (the hybrid combination of  \algnamev-all and  \algnamevv-close) is the best-performing one, which validates our claim in Section~\ref{sec:method}.
Meanwhile, \algnamev-all and \algnamevv-close also achieve good results (compared with \algnamevv-all), where both of them are supported by the claim in Section~\ref{sec:method}.


\begin{figure*}[t]
    \centering
    % \footnotesize
    \includegraphics[width=.95\textwidth]{supp_images/vis_eqben.pdf}
    \caption{Visualization of similarity scores ($s_{ij}$) on specific examples of \benchname. $s_{ij}$ is the similarity score for ($I_{i},T_{j}$). Darker color indicates larger similarity. The red cross represents the inferior similarity score leading to the wrong matching result.
    }
    \label{fig:sup_vis_eqben}
\end{figure*}

\input{supp_tables/supp_datasize_full}

\input{supp_tables/supp_arch_full}



\subsection{Computation Cost of \algname}
We present the computation cost of adding \algname in the table below.
% Table~\ref{tab:compare_cost}.
The forward time is measured with the average of 100 times of forward passes on a single GPU.
First, \algname is added as a regularization loss, \textbf{without} 
additional overhead on \# of parameters.
On time cost, we observe an acceptable overhead for fusion-encoder (\ie, METER) due to the similarity calculation on negative pairs.
While for dual-encoder (\ie, FIBER), which calculates the similarity for each image-text pair, the extra time needed for \algname is almost negligible.
Additionally, we show the forward time consumption \textit{v.s.} the batch size in 
the figure below.
The computation cost of \algname linearly scales with batch size, which is only slightly higher than the baseline for each data point.

\input{supp_images/computation_cost}



\subsection{More Benchmarking Results on \benchname}
\label{supp_sec:eqben_results}
We comprehensively report the model performance of existing VLMs on \benchname in Table~\ref{tab:supp_eqbench}. In addition to the observations drawn in the main paper, we can also find that: 1) When comparing the results of ALBEF/BLIP and their variants with contrastive loss (indicated by $\ddagger$), utilizing cosine similarity as the similarity measure as in ITC often leads to inferior accuracy compared to score computed by the ITM head. 
As ITC is usually implemented without cross-attention, making it hard to perform the fine-grained semantic recognition required in \benchname.
2) Fine-tuning on Flickr30K (F30K) results in a better performance. In contrast to the noisy samples of the pre-training data, F30K contains  high-quality captions that describe images in detail, hence helpful for the equivariant similarity learning of VLMs. 
3) The recent method BLIP2 shows strong capacity on our \benchname. Compared to other baselines, it is pre-trained on a much larger vision-language corpus (with 129 million image-text pairs), and thus shows better generalizability.
\input{supp_tables/supp_eqben_full}



\subsection{Generalization to Video Grounding}
\label{supp_sec:generalization}

\input{supp_tables/supp_generalization}
To further validate the generalization ability of the proposed~\algname, we conduct additional experiments on a very different but relevant downstream task, zero-shot video boundary grounding task~\cite{wang2022geb+,shou2021generic}, where the model is required to accurately predict the video boundary indicating event status change, given the before and after query captions. To adapt a pre-trained VLM to this video-language task, we extract video frames at fps=5 first and measure the similarity between each frame and the two query captions. Then given the two adjacent frames ($I_1, I_2$) and the two query captions ($T_1, T_2$), we define a boundary grounding score $s_{bg}=s(I_1,T_1)+s(I_2,T_2)$ for boundary grounding, where $s$ is the similarity produced by VLMs. $s_{bg}$ actually measures whether the boundary is located between frame $I_1$ and $I_2$. The larger $s_{bg}$ means that $(I_1,T_1)$ and $(I_2,T_2)$ are more likely to be a simultaneous match, thus indicating the boundary between the before and after captions. 
Results are reported in Table~\ref{tab:supp_generalization} on metrics following ~\cite{wang2022geb+}. We compute the accuracies based on the absolute distance between ground truth time boundaries and the predicted time boundaries, with the threshold varying from 0.1s to 3s.
Across all compared baselines, our~\algname can attain consistent performance improvements on the average accuracy, suggesting that \algname is effective to identify fine-grained shot changes in videos. 




\subsection{Distribution Curves on More Subsets}
\label{supp_sec:distribution_curve}
We present the distribution curves of the equivariant score on more \benchname subsets in Figure~\ref{fig:sup_vis} as the complement to Figure~\ref{fig:intro_hist} in the main paper. We can find that our \algname (indicated by ``Ours'') indeed achieves the most equivariant similarity (\ie, the tightest curve) across different datasets. Meanwhile, it is worth noting that the equivariance of similarity scores are not always positively correlated to the accuracy. 
For example, on \textsc{Eq-SD}, \algname (Ours) is similarly tight as the vanilla fine-tuning (FT), but the accuracy slightly drops.



\subsection{More Visualizations}
\label{supp_sec:visualization}
\noindent\textbf{Visualizations of Similarity Scores on Specific Examples.}
The distribution curves in Figure~\ref{fig:intro_hist} of the main paper depict the equivariant scores across the whole data. While in Figure~\ref{fig:sup_vis_eqben}, we explicitly visualize and compare the similarity scores (blue squares) for specific examples between FIBER baseline and our \algname.
We can clearly observe that current SoTA VLM still falls short in the similarity measure when facing two visually similar images. On one hand, the matching results are not even correct (red cross); On the other hand, regardless of the correctness of the matching results, the similarity scores are not yet equivariant, similar to the Figure~\ref{fig:intro_fig}(b) of the main paper. As shown in the left part of Figure~\ref{fig:sup_vis_eqben}, for the same visual semantic change (\texttt{red}$\leftrightarrow$\texttt{grey}), the corresponding similarity change of FIBER $s_{11}-s_{21}=-0.07$ is very different from $s_{22}-s_{12}=1.4$. While our model can produce much more equivariant similarity measure.


\noindent\textbf{Visualizations of Retrieval Results.}
We visualize the retrieval results on the commonly used Flickr30K in Figure~\ref{fig:vis_f30k}. In addition to the better top-$1$ retrieval accuracy, our \algname can produce much more reasonable similarity measurements for the whole retrieval sequence. For example, for the baseline model, the rank $2$ and $3$ images are \textbf{not} in line with the text of ``a young man'' and `` throw''. While our top-$3$ images are clearly more relevant.
\begin{figure}[h!]
    \centering
    \vspace{-0.1in}
    \includegraphics[width=.48\textwidth]{supp_images/vis_f30k.pdf}
    \caption{Visualization of top-$5$ T2I retrieval results on Flickr30K. Correct (wrong) top-1 images are in green (red).}
    \label{fig:vis_f30k}
\end{figure}


\subsection{\algname vs. CyCLIP~\cite{goel2022cyclip}}

 We notice this related contemporaneous work~\cite{goel2022cyclip}. We compare and discuss more detailed differences here.
\begin{itemize}
    \item Different motivation and implementation. Given the two image-text pairs $\{I_1,T_1\}$ and $\{I_2,T_2\}$, CyCLIP regularizes the CLIP cosine similarity score $s$ with the in-modal consistency (forcing $s(I_1,I_2)$ to be close to $s(T_1,T_2)$) and the cross-modal consistency (forcing $s(I_1,T_2)$ to be close to $s(I_2,T_1)$). While our \algname steps from the motivation that the similarity score change should faithfully respect to the semantic change and derive to the two regularization terms in Eq.~\eqref{eq:method_eq6}. Our final objective is the weighted combination of such two terms.
    \item Different evaluation settings and tasks. CyCLIP is solely built on dual-encoder architecture (\eg CLIP) and evaluates the effectiveness on the zero-shot image classification task. While our~\algname can adapt to both dual-encoder and fusion-encoder architectures (\eg METER and FIBER) and achieve improvements across various  VL benchmarks and downstream tasks, \eg, image-text retrieval, vision-language compositionality, and video boundary grounding.
    \item Better performance of~\algname. The closest CyCLIP counterpart to our~\algname  is the cross-modal consistency, which we implemented as \algnamev-all in Table~\ref{tab:abla_arch}. As we compare \algnamev-all against  +\algname, we clearly observe the superior performance of our \algname.
\end{itemize}