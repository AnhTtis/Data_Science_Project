
\section{Implementation Details of \algname}
\label{sec:implement}

We fine-tune the models on 8 NVIDIA V100 GPUs.
The regularization margin $\alpha$ and the balancing factor $\beta$ are selected from $\{0, 0.04, 0.1\}$ and $\{0.2, 0.5, 1.0\}$. We adopt an image resolution as $288\times288$ due to computational constraints. 
For FIBER, which is implemented with ITC loss for fast retrieval, 
we adopt the cosine similarity between image and text features as $s$, and then normalize it by a softmax function. The images and text with top-$8$ $s$ are regarded as the semantically ``close'' samples to apply \algnamevv.
METER is designed with ITM loss, which does not compute all pairwise similarities in the training batch.  
Therefore, we leverage the pre-trained METER model to pre-compute and cache all pairwise similarities in Flickr30K training split, prior to fine-tuning. However, the computation of the ITM similarity for each image-text pair of the training set still takes a long time (more than two weeks on 8 V100 GPUs in practice). To further reduce the computation, we apply a ``coarse-to-fine'' strategy. For a given image, we first select the top-128 similar images, based on the image feature extracted from the METER vision encoder. Assuming each image is associated with 5 captions, we then utilize the ITM head to compute a fine-grained similarity measure for $128\times 5$ image-text pairs (leading to 1 hour on 8 V100 GPUs). 
During retrieval fine-tuning, we follow the original METER to sample $15$ captions as negatives and additionally sample their counterpart images for \algname. Furthermore, $8$ of $15$ items (\ie, $k=8$) are selected as hard negative (\ie, semantically close) samples based on the pre-computed similarity matrix. 
In METER, the similarity score $s$ is normalized with a sigmoid activation. 
We apply other model-specific hyper-parameters (\eg, training epochs and learning rates) following the original METER~\cite{dou2021empirical} and FIBER paper~\cite{dou2022coarse}.
