

\section{Diagnosing VLMs with \benchname}\label{sec:dataset}




%\noindent\textbf{Overview}.
We argue that standard VL evaluation kits~\cite{plummer2015flickr30k, Lin2014MicrosoftCC} are too coarse to evaluate the equivariance of VLMs similarity. Existing VLMs can easily distinguish most samples in conventional retrieval benchmarks, \eg, images of a group of people against those with cars, given a caption of ``people standing on the street''.
% image-text pairs of human and car. 
Therefore, we propose \benchname to focus on visual minimal semantic changes to check whether VLMs can faithfully respond, \ie, the equivariance of the similarity measure in VLMs.
Specifically, \benchname contains 
% \linjie{xxx} samples from 
5 sub-datasets, covering diverse image domains, from real-life scenarios to synthetic well-controlled scenes. And it is designed to stress test VLMs with accurate semantic changes in action, location, and attribution (\eg, color, count and size). Figure~\ref{fig:benchmark} presents an overview of \benchname. 

Next, we introduce our design principle for constructing \benchname. Each sample in \benchname consists of a pair of images $(I_1, I_2)$ and a pair of captions $(T_1, T_2)$. A valid \benchname sample must satisfy:  (1) $T_i$  is preferred to be used as the description for $I_i$;
% $\{I_1,T_1\}$ and $\{I_2,T_2\}$ are preferred by the human over $\{I_1,T_2\}$ and $\{I_2,T_2\}$; 
(2) $I_1$ and $I_2$ are visual-minimally different. 
The former one requires that $\{I_1,T_1\}$ and $\{I_2, T_2\}$ should be semantically distinguishable without confusion, while the latter one limits the extent of the distinction -- ``visual-minimal change''. 
Previous work~\cite{thrush2022winoground} defines ``minimal'' semantic change in the caption space as the same words but in a different order. However, due to the continuity and entanglement of image pixels, ``minimal'' semantic change in visual space is hard to determine. In this paper, we roughly define it as changes in the foreground (\eg, attribute, action, location, \etc) while sharing the same scene and background.

% To achieve ``visual-minimal change'', we explore two construction protocols: Natural and Synthetic.  
In practice, we source image pairs with ``visual-minimal change'' in two ways: (1) from natural videos and (2) from synthetic engines, where we adopt different construction pipelines, as shown at the bottom of Figure~\ref{fig:benchmark}.
For the former one, we directly leverage the \underline{continuity} of scene changes along the temporal dimension in \emph{natural videos}, which can provide massive image pairs with minimal visual changes. Specifically, we leverage the existing video-language datasets~\cite{zhou2018towards, ji2020action, wang2022geb+} to construct \benchname samples.
To more precisely control the varying component in images, we further explore the % apply a 
photo-realistic scene generator (Kubric~\cite{greff2021kubric}) and the open-source diffusion model (Stable Diffusion~\cite{rombach2022high, hertz2022prompt}) to synthetically generate pairs of images by providing two captions that are minimally different from each other.
% to produce image-text pairs by providing two captions that are minimally different from each other.
In what follows, we introduce the construction pipeline for each sub-dataset in detail.
% to highlight our solutions to problems we encountered .







\subsection{Construction from Natural Video}
Let's define a video with caption annotations as $\mathcal{V}=\{I_i, T_i\}_{i=1}^{N}$, where $N$ is the number of sampled frames. 
We assume the ``visual-minimal change'' is naturally guaranteed between any two frames $I_i$ and $I_j$ ideally, where $I_i, I_j \in \mathcal{V}$, $i\neq j$, as we limit the source video to be either short segment~\cite{wang2022geb+,ji2020action} or capturing a fixed scene~\cite{zhou2018towards}.
However, we find that it is hard to ensure the validity of all the video frame pairs in practice. Therefore, we utilize a frame filter to filter out invalid samples automatically for different video sources.
Below, we briefly introduce the dataset construction process and delay the  details to Appendix. 

We construct three sub-datasets based on real images from natural videos, including \textsc{Eq-AG}, \textsc{Eq-GEBC} and \textsc{Eq-YouCook2}. We construct \textbf{\textsc{Eq-AG}} by leveraging the scene graph annotations from Action Genome (AG)~\cite{ji2020action}, which capture detailed changes between objects and their pairwise relationships while action occurs.  
We first use a slot-filling template to translate scene graphs to captions. 
As videos usually come with redundant frames, we avoid the nearly duplicated frames by sampling frames $I_i$ and $I_j$ if and only if at least 2 of 3 pairwise relationships are different.
Furthermore, if $I_j$ is chosen for previous samples, we empirically skip the subsequent 2 frames to $I_{j+2}$.
\textbf{\textsc{Eq-GEBC}} is built on GEBC~\cite{wang2022geb+} that contains captions describing the event before and after an event boundary. We adopt the frames before and after the boundary as our visual minimally different images. Similarly, we avoid temporal redundancy via sparse sampling across multiple boundaries. We construct \textbf{\textsc{Eq-YouCook2}} based on YouCook2~\cite{zhou2018towards}, which is sparsely annotated with captions for each cooking step. We construct the dataset by sampling the middle frame of a short video segment as $I_i$ with its annotated caption as $T_i$. We then apply off-the-shelf object detectors to filter scene changes. Please note that \benchname can be easily extended to other video-language datasets by applying the same construction pipeline.














\subsection{Construction from Synthetic Engine}
Synthetic engine may provide more precise and controllable visual changes in the generated images, to allow more accurate diagnosis in terms of model failure when evaluating with \benchname. We assume that a synthetic engine can faithfully generate images based on a text prompt describing the image content.
% For applying the synthetic engine, we actually assume the faithful generation from caption to image. %Then our ``visual-minimal change'' can be transferred to ``text-minimal change''.
% Therefore, 
Based on this assumption, given a pair of semantic-minimally different captions $T_i$ and $T_j$, we expect the generated $I_i$ and $I_j$ to be correspondingly visual-minimally different. In the following, we briefly introduce the utilized engine and how to construct semantic-minimally different captions for each sub-dataset and leave more details to Appendix.




\textbf{\textsc{Eq-Kubric}} takes advantages of Kubric~\cite{greff2021kubric}, an open-source graphics engine %framework that interfaces with PyBullet and Blender 
to generate photo-realistic scenes. Here we adopt Google Scanned Objects (GSO) for scene construction and categorize caption change into three aspects: \textit{attribute}, \textit{counting}, and \textit{location}. For each aspect, we construct $2000$ image-text pairs by intervening corresponding phrases of sentences while leaving other words unchanged.
\textbf{\textsc{Eq-SD}} is inspired by the recent advances in diffusion models for text-to-image generation~\cite{ramesh2022hierarchical, saharia2022photorealistic, yu2022scaling}. 
We utilize the open-source checkpoint v1.4 of Stable Diffusion (SD) with prompt-to-prompt image editing framework~\cite{hertz2022prompt} to translate two semantic-minimally different captions to a pair of images. 
Specifically, we elaborately design a set of textual semantic-minimal editing: 1) object change (\eg, ``dog''$\to$``cat''); 2) scene change (\eg, + ``in the winter''); 3) attribute change (\eg, + ``with a sunglasses''). Finally, we perform a human evaluation to filter out poor-quality generations.
Notably, we can adopt more rendered objects (\eg, rendered animals) and various synthetic engine (\eg, better generative models) to further extend our \benchname following the proposed pipeline.
\input{tables/intro_table}

\input{tables/winoground_f30k.tex}

\subsection{Comparisons with Other Datasets}
In Table~\ref{tab:intro_bench}, we conduct a direct comparison of \benchname against two widely adopted retrieval benchmarks (Flickr30K~\cite{plummer2015flickr30k} and COCO~\cite{chen2015microsoft}) and two recent datasets with textual-minimal change (VALSE~\cite{parcalabescu2021valse} and Winoground~\cite{thrush2022winoground}) from four aspects.
1) On the dataset characteristics, to the best of our knowledge, \benchname is the first diagnosing benchmark to examine the equivariance of VLMs in terms of \textbf{minimal visual semantic change}.
2) For evaluation setting, \textbf{pairwise} setting asks VLMs to select the correct counterpart within a pair of slightly different samples rather than thousands of very different samples in conventional retrieval datasets. The minimal semantic drift between the pair of samples makes the evaluation of equivariant similarity measure more effective. 
3) For \textbf{domain diversity}, 
our \benchname contains rich visual contents collected from different video domains as well as synthetic domains, as opposed to the common image-text datasets which existing diagnosing kits are built upon.
4) In terms of \textbf{scalability}, 
\benchname is highly scalable as our automatic pipeline can be easily applied to other video-language datasets and synthetic engines, as opposed to manual annotation for building traditional retrieval datasets. While VALSE only focuses on linguistic editing of the captions, \benchname can be further scale up with more diverse visual contents.






