\subsection{Ablation Study}
\label{sec:ablation}


In this section, we conduct ablation studies to validate the scalability, design and effectiveness of \algname in terms of enforcing equivariant similarity. 

\noindent\textbf{Scalability of \algname}. Table~\ref{tab:datasize} evaluates the scalability of \algname and standard fine-tuning baseline on the natural subsets of \benchname and Winoground by gradually including more training data. Under the same fine-tuning data, \algname achieves consistent and significant improvements ($2\%$ - $3\%$) over the baseline. Interestingly, there is no remarkable correlation between the corpus size and model performance. This may be due to the distribution of standard VL data is far away from that of \benchname and Winoground. 
% \linjie{
Note that for the 4M experiment, we fine-tune the models for 10K steps due to computational constraints. Our results demonstrate the potential of \algname to benefit VL pre-training on large-scale data.
Additionally, we validate the generalizability of \algname in other relevant downstream tasks. Further details are provided in Appendix~\ref{supp_sec:generalization}.




\noindent\textbf{Ablation on \algname design}.
Table~\ref{tab:abla_arch} compares \algname against the four ablated instances on \textsc{Eq-Kubric} and Winoground~\cite{thrush2022winoground}, including 1) fine-tuning with hard negative sampling (HardNeg);  
2) applying \algnamev to all samples in the training batch (\algnamev-all); 3) applying \algnamevv to all samples in the training batch (\algnamevv-all); and 4) applying \algnamevv for only semantically close samples (\algnamevv-close). The final \algname is equivalent to \algnamev-all + \algnamevv-close,
which achieves the best performance. Notably, enforcing \algnamevv on all (\algnamevv-all) even degrades the performance by -0.58\% on average, compared to applying only to semantically close samples (\algnamevv-close). This validates our claim in Section~\ref{sec:method} that \algnamevv is better suited for semantically close samples.




% \noindent\textbf{A3}: 
\noindent\textbf{Validation of equivariance via \algname}.
Given the similarity scores $s$ calculated by a VLM, we can define the equivariance score as the derivation of \algnamevv (headline of Figure~\ref{fig:intro_hist}) to measure the degree of equivariance (the smaller, the better). In Figure~\ref{fig:intro_hist}, we plot the distribution of \algnamevv values across all samples in \textsc{EQ-Youcook2} dataset for FIBER~\cite{dou2022coarse} and its variants, attached with their group scores. A tighter curve indicates smaller derivation, hence better equivariance similarity measure. 
Full results on other \benchname subset are presented in Appendix~\ref{supp_sec:distribution_curve}.
Compared with pre-training only (PT), fine-tuning on Flickr30K (FT) can improve the group score  while being more equivariant in the similarity measure.
Adding \algname (Ours) obtains additional improvements on both similarity equivariance and group score, indicating \algname indeed enforces equivariant similarity measure. 
Additionally, due to the space limitation, we leave more visualizations in Appendix~\ref{supp_sec:visualization}.


\input{tables/ablation_arch}

\input{images/intro_hist}


\subsection{Pilot Study of MLLM on \benchname}
\label{sec:mllm}
Powered by the remarkable capabilities of the large language model (LLM), the community has witnessed an emergent interest in developing Multimodal Large Language Model (MLLM)~\cite{zhu2023minigpt, liu2023visual, gong2023multimodal} very recently. Instead of accepting the pure text as the input, MLLM additionally sees the image and provides the response, which can be regarded as another line of VLMs.
Here we conduct a pilot study of the performance of MLLM on our \benchname. 
We adopt LLaVa-7B~\cite{liu2023visual} as our base model with Vicuna as the LLM backend. Given two matched image-text pairs $\{I_1,T_1\}$ and $\{I_2,T_2\}$, we concatenate $I_1$ and $I_2$ horizontally as the single input image. We build the question prompt with the template: ``\textit{There are two images (left and right). Now you have two captions: caption 1: $\{T_1\}$; caption 2: $\{T_2\}$. Please indicate which caption corresponds to the left image and which caption corresponds to the right one. The answer should follow the format: "\#index for the left image; \#index for the right image". For example, "1;2" represents that caption 1 corresponds to image left.}''
Since it is hard to reformat the MLLM free-form textual output to the label space, we randomly collect 20 samples from each subset of \benchname and manually compare the MLLM output and the ground-truth label. The results are shown in Table~\ref{tab:supp_mllm}.
Interestingly, by comparing two rows, we can find that the performance of MLLM is quite sensitive to the order of the input caption $T_1$ and $T_2$ ($\sim 90\%$ v.s $\sim 0\%$). This indicates that the MLLM does NOT truly understand how to distinguish two semantically similar image-text pairs but just follows the given sequence of the captions. 


\input{supp_tables/supp_mllm}

