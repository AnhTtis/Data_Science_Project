
\section{Related Work}



\noindent\textbf{Pre-training VL Models}.
Early object detector (OD)-based methods~\cite{chen2020uniter, zhang2021vinvl, li2020oscar, li2019visualbert, lu2019vilbert, gan2020large, tan-bansal-2019-lxmert} utilized the offline image region features from a pre-trained object detector~\cite{ren2015faster}. 
More recent methods mainly learn from image pixels directly in an end-to-end  manner~\cite{wang2021simvlm, jia2021scaling, wang2021vlmo, singh2021flava, li2022blip, wang2022unifying, yu2022coca, alayrac2022flamingo}.
Researchers~\cite{gan2022vision} further categorize VLMs into (1) Dual-Encoder (\eg, CLIP~\cite{radford2021learning} and ALIGN~\cite{li2021align}) and (2) Fusion-Encoder (\eg, METER~\cite{dou2021empirical}, FIBER~\cite{dou2022coarse}, and ALBEF~\cite{li2021align}).
It is worth noting that our proposed \algname is model-agnostic, and can be easily plugged into the image-text alignment objectives such as Image-Text Matching (ITM) and Image-Text Contrastive (ITC) loss.







\noindent\textbf{Diagnosing VL Models}.
Years of VL research have spawned a series of VL evaluation kits, 
from classical VL tasks~\cite{zellers2019recognition, vinyals2015show,Yu2016ModelingCI,plummer2015flickr30k} (\eg, VQA~\cite{antol2015vqa} and image captioning~\cite{chen2015microsoft}), to more complex contexts, such as adversarial examples~\cite{li2021adversarial, chen2017attacking}, robustness~\cite{gupta2022grit,cadene2019rubi, wang2020visual, tang2020unbiased,jimenez2022carets} and counterfactual reasoning~\cite{shekhar2017foil,hendricks2021probing,hu2019evaluating,niu2021counterfactual}.
However, 
these benchmarks require manual annotation and their evaluation relies on task-specific % time-consuming 
model fine-tuning. 
Another line of work~\cite{thrush2022winoground,parcalabescu2021valse,zhao2022vl} probe VLMs on similarity measure
with minimal \emph{caption} semantic changes while keeping images intact. 
While our \benchname tries to test whether the inherent image-text similarity measure in existing VLMs is sensitive to visual semantic changes. 
The most relevant work is ImageCoDe~\cite{krojer-etal-2022-image} which leverages video frames toward fine-grained image-text retrieval. However, ImageCoDe requires additional human crowdsourcing and is limited to real-world video sources. 
In contrast, \benchname explores both natural and synthetic ways to generate image pairs with minimal semantic change, making the data generation process 
% flexible
inclusive, automatic, and extensive.





\noindent\textbf{Equivariance Learning}.
Unlike the wide usage of invariance in deep neural networks (\eg, shift invariance achieved by convolutional layers), strict group equivariance~\cite{cohen2018spherical, cohen2016group, weiler2019general,bronstein2021geometric} is hard to apply in practice. %, especially for the image data.
However, the equivariance property still plays an important role in various fields, such as self-supervised learning~\cite{dangovski2021equivariant, xie2022should, wang2021self,patrick2020support,han2020self}, representation learning~\cite{qi2020learning}, and language understanding~\cite{gordon2019permutation}.
In this paper, we point out the significance 
of the equivariant similarity measure in VLMs. Based on this, we further propose a novel loss \algname for the regularization of equivariance, as well as a new challenging benchmark \benchname to diagnose the equivariance of existing VLMs. 
We notice that the recent CyCLIP~\cite{goel2022cyclip} delivers a similar idea but with different motivation, implementation and evaluation settings. In Table~\ref{tab:abla_arch}, we compare with CyCLIP-equivalent baseline as \algnamev. Check more detailed comparison in Appendix.
