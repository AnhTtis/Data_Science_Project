\section{Improving VLMs with \textsc{EqSim}}
\label{sec:method}

\input{images/eqmap}
Recall that VLMs adopt the image-text similarity as the training proxy for learning multimodal feature representation~\cite{radford2021learning}. Therefore, the ultimate goal of pursuing equivariant similarity is to learn an equivariant feature map between image space and text space.

\input{images/eqsim}


\input{images/benchmark} 

\begin{definition}
Let $\mathcal{I}$ and $\mathcal{T}$ be two continuous feature spaces. Let $\mathcal{G}$ be a group whose group action on $\mathcal{I}$ is defined by $g: \mathcal{I}\to \mathcal{I}$, and that on $\mathcal{T}$ is defined by $g': \mathcal{T}\to\mathcal{T}$. Then, $\phi: \mathcal{I}\to\mathcal{T}$ is an equivariant feature map if and only if $g'\cdot \phi(I) =\phi(g\cdot I)$ for all the group actions and $I\in\mathcal{I}$. The commutativity for $\phi$, $g$ and $g'$ is shown in Figure~\ref{fig:eqmap}.
\label{def:eqmap}
\end{definition}
In Definition~\ref{def:eqsim}, the measure $\mu$ can be considered as the semantic group acting on an infinitesimal region in image or text space. Thus, by applying the commutativity of Definition~\ref{def:eqmap} in Definition~\ref{def:eqsim} to change the sum from image space to text space. Without loss of generality, we only show the results of sum $1\to2$:
{\small{
\begin{equation}
\sum\nolimits_{T_1}^{T_2} \mu(T) = \sum\nolimits_{\phi(I_1)}^{\phi(I_2)} \mu(\phi(I)) = \sum\nolimits_{\phi(I_1)}^{\phi(I_2)} \phi(\mu(I)).
\end{equation}
\label{eq:method_eq3}
}}
\!\!\!This implies that the equivariant map establishes an isometry for the measure  $\mu(I)$ in image space and $\phi(\mu(I))$ in text space. Thus, they only differ by a constant scale $C>0$, \ie, $\phi(\mu(I)) = C\mu(I)$:
{\small{
\begin{equation}
\sum\nolimits_{\phi(I_1)}^{\phi(I_2)} \phi(\mu(I)) = C\sum\nolimits_{I_1}^{I_2} \mu(I).
\label{eq:method_eq4}
\end{equation}
}}
\!\!By combining Eq.~\eqref{eq:intro_eq1},~\eqref{eq:intro_eq2},~\eqref{eq:method_eq3}, and~\eqref{eq:method_eq4}, we have the following ratio equality as our \textsc{EqSim} constraint:
{\small{
\begin{equation}
\frac{s_{11}-s_{12}}{s_{11}-s_{21}}=\frac{s_{22}-s_{21}}{s_{22}-s_{12}}=C = 1.
\label{eq:method_eq5}
\end{equation}
}}
\!Note that $C =1$ can be derived by using the fact that $s_{11}>s_{12}$ and $s_{22}>s_{21}$. By simplifying Eq.~\eqref{eq:method_eq5} further, we have the following two regularizations:
{\small{
\begin{equation}
\begin{split}
&\textsc{EqSim}_\textrm{v1}: s_{12}=s_{21}\\
&\textsc{EqSim}_\textrm{v2}: s_{11}\!-\!s_{12} = s_{22}\!-\!s_{21},~~s_{11}\!-\!s_{21} = s_{22}\!-\!s_{12}.
\end{split}
\label{eq:method_eq6}
\end{equation}
}}
% \linjie{Can we change `far away' to `distant' throughout the paper?}
\!\!Note that the viable space of \algnamevv is a subset of \algnamev, because \algnamev is exactly equivalent to Eq.~\eqref{eq:method_eq5} while \algnamevv further requires $s_{11}=s_{22}$.
Empirically, we find that \algnamevv is more suitable to the semantically close pairs $(I_1, T_1)$ and $(I_2, T_2)$; and \algnamev to distant pairs. Figure~\ref{fig:eqsim} illustrates such hybrid training loss within a training batch.
Semantically ``close'' and ``distant'' are determined by the similarity score $s$, where we regard samples with top-$k$ $s$ as ``close'' samples.
For dual encoder VLMs with ITC loss, $s$ is the cosine similarity between image and text features. For fusion encoder VLMs with ITM, $s$ is the scoring output from the ITM head.


 


In our implementation, we adopt Mean Square Error (MSE) loss to regularize the equation of similarities. In addition, motivated by the hinge loss~\cite{boser1992training,vapnik1999nature}, we utilize a margin parameter $\alpha$ to control the strength of regularization. \textsc{EqSim}$_\textrm{v1}$ can be written as $[||s_{12}-s_{21}||_{2}^{2}-\alpha]_{+}$, where $[x]_{+}=\text{max}(x,0)$ and $||\cdot||_{2}$ denotes the L2 norm. \textsc{EqSim}$_\textrm{v2}$ can be implemented similarly.
In practice, given a retrieval fine-tuning objective $\mathcal{L}_{Ret}$, the final loss can be written as: $\mathcal{L} = \mathcal{L}_{Ret}+\beta \mathcal{L}_{\textsc{Eq}}$, where $\mathcal{L}_{\textsc{Eq}}$ is \algnamev  (\algnamevv) for semantically distant (close) samples, $\beta$ is the balancing factor.
Experiments in Section~\ref{sec:ablation} validate that the hybrid training is better than only using \textsc{EqSim}$_\textrm{v1}$. 






