\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% \usepackage{color, colortbl}
% \definecolor{lightgray}{rgb}{0.83, 0.83, 0.83}

\usepackage{color}
\usepackage{soul}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{pifont}%

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).


\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{4688} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
% \title{Beyond Diffusion: A Transformer-based Framework for Human Mesh Recovery from a Video}
\title{DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh Recovery from a Video}

\author{Ce Zheng$^{1}$ , Guo-Jun Qi$^{2,3}$,  Chen Chen$^{1}$\\
$^1$Center for Research in Computer Vision, University of Central Florida\\
$^2$  OPPO Seattle Research Center, USA \quad $^3$ Westlake University\\
{\tt\small cezheng@knights.ucf.edu; xliu59@ncsu.edu;  guojunq@gmail.com; chen.chen@crcv.ucf.edu}\\
% {\tt\small guojunq@gmail.com; chen.chen@crcv.ucf.edu }
}


\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
  Human mesh recovery (HMR) provides rich human body information for various real-world applications such as gaming, human-computer interaction, and virtual reality. Compared to single image-based methods, video-based methods can utilize temporal information to further improve performance by incorporating human body motion priors. However, many-to-many approaches such as VIBE \cite{kocabas2020vibe} suffer from motion smoothness and temporal inconsistency. While many-to-one approaches such as TCMR \cite{tcmr} and MPS-Net \cite{MPS-Net} rely on the future frames, which is non-causal and time inefficient during inference. To address these challenges, a novel Diffusion-Driven Transformer-based framework (DDT) for video-based HMR is presented. DDT is designed to decode specific motion patterns from the input sequence, enhancing motion smoothness and temporal consistency. As a many-to-many approach, the decoder of our DDT outputs the human mesh of all the frames, making DDT more viable for real-world applications where time efficiency is crucial and a causal model is desired. Extensive experiments are conducted on the widely used datasets (Human3.6M \cite{h36m_pami}, MPI-INF-3DHP \cite{mpi3dhp2017}, and 3DPW \cite{pw3d2018}), which demonstrated the effectiveness and efficiency of our DDT. \textcolor{magenta}{The code will be publicly available.}
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

As one of the fundamental topics in computer vision,  Human Pose Estimation (HPE) from monocular cameras has been developed rapidly with the emergence of deep learning techniques. With HPE approaches such as OpenPose \cite{openpose}, VideoPose3D \cite{videopose3d}, and AlphaPose \cite{alphapose}, excellent pose estimation performance can be achieved in real-time. However, the desire to detect more than just 2D or 3D human joints from monocular images has led to increased attention on Human Mesh Recovery (HMR). HMR provides rich human body information, including human pose and shape, for various real-world applications such as gaming, human-computer interaction, and virtual reality (VR). 

% Compared to image-based methods, video-based methods for HMR may alleviate the challenges of complex human body articulation, occlusion, and depth ambiguity somehow. The temporal information in the sequence input is utilized for providing body motion priors. Moreover, the frame-by-frame reconstruction output from image-based methods suffers from s motion jitter, while this issue can be well addressed by video-based methods. Among many excellent video-based methods, TCMR, which leverages temporal information from past and future frames without being dominated by the current static feature, achieves outstanding reconstruction performance as well as motion smoothness. However, the limitations of TCMR are also stated obviously. First, The future frames are involved which leads TCMR is not causal, extra padding to compensate for unknown feature frames is required at inference time. Second, given a sequence of frames as input, TCMR can only output the human mesh of one single frame, which is not time efficient for real-world applications.  

\begin{figure}[tp]
\vspace{-5pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/dm_TID.pdf}
  \vspace{-10pt}
  \caption{(a) The general pipeline for diffusion model. Input data is perturbed by adding noise recursively and output data is generated from the noise in the reverse process. Images are taken from \cite{diffsurvey1}. (b) Human motion is involved over time in the input video sequence. The output human mesh can be also estimated through the motion decoding procedure which is similar to the denoising process in the diffusion model. }
  \label{fig:dm_tid}
  \vspace{-15pt}
\end{figure}

Compared to image-based approaches, video-based techniques for HMR have the potential to alleviate the challenges of complex human body articulation, occlusion, and depth ambiguity. Because some missing information in one frame can be compensated by the neighboring frames.  By utilizing the temporal information with body motion priors in the sequence input, video-based methods can address the issue of motion jitter that arises with frame-by-frame reconstruction in image-based methods. Among many impressive video-based methods, VIBE \cite{kocabas2020vibe} is a seminal work that achieved excellent performance. \ul{It adopts a many-to-many approach, which enables the simultaneous output of the human mesh sequence for all input frames}. However, VIBE still suffers from temporal inconsistency as claimed in \cite{tcmr}.  
% To further improve the output motion smoothness, TCMR switches to a many-to-one strategy, which leverages the temporal information from both past and future frames while effectively avoiding dominance by the current static feature. However, as a many-to-one approach, TCMR has its limitations.
To enhance the motion smoothness and temporal consistency of the output, TCMR \cite{tcmr} and MPS-Net \cite{MPS-Net} adopt a different strategy, switching to a \ul{many-to-one approach that only outputs the human mesh of a single frame}. TCMR and MPS-Net effectively utilize temporal information from both past and future frames to output the mesh of the current frame only.  
% thereby avoiding the current static feature's dominance 
Although achieving motion smoothness and temporal consistency, it is important to note that TCMR and MPS-Net have their limitations as a many-to-one approach. For instance, TCMR and MPS-Net highly rely on future frames, which necessitates additional padding to compensate for unknown feature frames during inference. This kind of \textbf{non-causal} model is not applicable for online streaming processing for real-world applications. 
Furthermore, given a sequence of frames as input, TCMR \cite{tcmr} and MPS-Net \cite{MPS-Net} can only output the human mesh of one frame per forward pass, not the entire mesh sequence of all frames, which may not be suitable for real-world applications where time efficiency is a concern. Consequently, one question arises: \textit{is it feasible to develop a many-to-many network while enhancing motion smoothness simultaneously?}


Recently, the computer vision community has shown considerable interest in Diffusion Models (DM). By learning to reverse the diffusion process in which noise has been added in finite successive steps, DM can generate samples that match a specified data distribution corresponding to the provided dataset. As a generative-based method, DM is capable of handling many-to-many expressions of the tasks such as motion synthesis and generation \cite{MDM,flame}. The quality and diversity have been improved significantly by bridging the large gap between uncertain and determinate distribution during the denoising process as shown in Fig \ref{fig:dm_tid} (a). Additionally, the diffusion process in DM involves adding noise progressively through multiple steps, which is analogous to how human motion is involved over time in the video sequence. 
% Motivated by this, DM seems to be a promising solution for building a many-to-many HMR network with enhancing motion smoothness. However, as shown in Fig, it is not applicable to naively apply the DM to achieve our goal due to the following issues. 
%This has led to DM being considered 
This inspires us to consider DM as a promising solution for constructing a many-to-many HMR network that enhances motion smoothness and temporal consistency. \textit{However, 
% as illustrated in Fig \ref{fig:dm_tid} (a), 
applying DM naively to achieve this goal is not feasible due to several issues.} First, motion patterns in video sequences are more intricate than the pure Gaussian noise added in DM. Second, DM typically demands a significant number of steps (usually larger than 200) between two states to achieve high-quality sampling, leading to prolonged inference times. To our best knowledge, there is no DM-based method for video-based HMR has been proposed so far.

To address the aforementioned challenges, we present a novel Diffusion-Driven Transformer-based framework (DDT) for video-based HMR using a many-to-many approach. Unlike conventional DMs that require adding Gaussian noises in successive steps during the diffusion process, DDT is designed to decode the specific motion patterns from the input sequence. The motion smoothness and temporal consistency of the output mesh sequence are enhanced by the successive decoding steps over frames. Moreover, as depicted in Fig. \ref{fig:dm_tid} (b),  the decoder in DDT aims to return the previous frame by one step,  which is significantly more efficient than DMs that require more than 100 steps. This advantage makes DDT more viable for deployment in real-world applications where time efficiency is crucial.

Our contributions are summarized as follows:
\setlist{nolistsep}
\begin{itemize}[noitemsep,leftmargin=*] 
    \item We propose a novel Diffusion-Driven Transformer-based framework (DDT) for human mesh recovery from a video. Unlike the vanilla DM which adds Gaussian noises in successive steps and denoise in the reverse process, our DDT is designed to decode specific motion patterns from the input sequence. 
    \item As a many-to-many approach, our DDT improves the motion smoothness and temporal consistency of the output mesh sequence by the successive decoding step over frames, which is more efficient than diffusion model-based methods and SOTA many-to-one methods. 
    \item We conduct extensive experiments on Human3.6M \cite{h36m_pami}, MPI-INF-3DHP \cite{mpi3dhp2017}, and 3DPW \cite{pw3d2018} datasets,  which demonstrated the effectiveness and efficiency of our DDT.
    % that our DDT outperforms previous SOTA methods in terms of motion smoothness and temporal consistency. 
\end{itemize}



% Unlike conventional diffusion models (DMs) that use successive steps to add Gaussian noises to the input, our TIDM acknowledges that motion patterns evolve over frames in the input sequence. Therefore, TIDM is designed to decode the motion patterns that correspond to the desired human mesh from the input features of each frame. This enhances motion smoothness and successively outputs the final human mesh for each frame. By employing this approach, we aim to improve the performance of video-based HMR.

% is designed to overcome the limitations of conventional diffusion models by eliminating the need for Gaussian noise addition in successive steps of the diffusion process. Instead, TIDM leverages the inherent evolution of motion patterns over frames in the input sequence to decode the corresponding motion patterns for the desired human mesh. This approach enhances the smoothness of motion and produces successive outputs for each frame, resulting in a more accurate and precise human mesh.

 % diffusion models are capable of generating samples that match a specified data distribution (e.g., natural images) from random (indeterminate) noise through multiple steps where the noise is progressively removed 
 
\section{Related work}
Since Human Mesh Recovery is a popular topic in computer vision, here we focus on the most relevant works to our methods. We refer readers to the recent and comprehensive HMR survey \cite{hmrsurvey} for more details. 
\subsection{Image-based HMR}
The majority of methods  ~\cite{pymaf2021,kanazawaHMR18,kolotouros2019cmr,prohmr,dsr2021} for human mesh recovery (HMR) rely on a parametric human model, such as SMPL \cite{SMPL:2015}, to reconstruct the mesh by estimating pose and shape parameters. Kanazawa et al. \cite{kanazawaHMR18} propose a weakly supervised approach to regress the SMPL parameters that minimizes the 2D re-projection loss of keypoints. As a fundamental HMR work, SPIN ~\cite{Kolotouros2019SPIN}combines regression and optimization in a loop, where the regressed output serves as better initialization for optimization (SMPLify). METRO \cite{lin2021metro} is the first transformer-based method that models vertex-vertex and vertex-joint interaction using a transformer encoder after extracting image features with a CNN backbone. As an extension work of METRO,  MeshGraphormer combines a graph convolutional network with a transformer to model local and global interactions among mesh vertices and joints, but it is computationally expensive. Zeng et al. \cite{tcformer} propose a Token Clustering Transformer to merge tokens from different locations through a progressive clustering procedure. However, it performs worse than METRO and MeshGraphormer. 
To output video mesh sequences, these image-based methods suffer from severe motion jitter due to frame-by-frame reconstruction. 


\subsection{Video-based HMR}
Similar to the image-based methods, Video-based methods \cite{zeng2022deciwatch,dand,zeng2022deciwatch} for human mesh recovery also utilize parametric human models like SMPL \cite{SMPL:2015}. These methods can be categorized into two approaches: many-to-many (multiple frames per forward pass) and many-to-one (single frame per forward pass). For the many-to-many approach, Kanazawa et al.\cite{hmmr} first propose a convolutional network  to learn human motion kinematics by predicting past, current, and future frames. Based on \cite{hmmr}, Sun et al. \cite{sun2019human} further propose a self-attention-based temporal model to improve performance. VIBE \cite{kocabas2020vibe} leveraged the AMASS dataset \cite{AMASS2019} to discriminate between real human motions and those produced by its temporal encoder. MEVA \cite{meva} estimates the coarse 3D human motion using a variational motion estimator, then refine the motion by a motion residual regressor.  For the many-to-one approach, TCMR \cite{tcmr} utilized GRU-based temporal encoders to forecast the current frame from the past and future frames, which enhanced the temporal consistency. Similar to \cite{tcmr}, MPS-Net \cite{MPS-Net} captures the motion continuity dependencies by the attention mechanism. Nevertheless, as a many-to-one approach, these methods are non-causal and time inefficient during inference. 


\subsection{Diffusion Generative Models}
Diffusion Generative models have achieved impressive success in a wide variety of computer vision tasks such as image inpainting \cite{song2021scorebased}, text-to-image generation \cite{photorealistic}, and image-to-image translation \cite{choi2021ilvr}. Given the strong capability to bridge the large gap between highly uncertain and determinate distribution, several works have utilized the diffusion generative model for the text-to-motion generation \cite{modiff,MDM,zhang2022motiondiffuse}. Zhang et al. \cite{zhang2022motiondiffuse} propose a versatile motion-generation framework that incorporates a diffusion model to generate diverse motions from comprehensive texts. Similarly, Tevet et al. \cite{MDM} introduce a lightweight transformer-based diffusion generative model that can achieve text-to-motion and motion editing. Zhao et al. \cite{modiff} tackle sequential skeleton-based motion generation, where the skeleton sequence is treated as a 2D image matrix with three channels (N: number of frames, J: number of joints in skeleton in one frame, and D: x, y, z coordinate of each joint). DiffPose \cite{gong2022diffpose} is proposed to predict the 3D pose of the center frame given the input sequence using a diffusion model. In our work, we introduce a transformer-based framework inspired by the diffusion model for HMR from a video.



%%%%%%%%%%%%%%%%%%
\section{Methodology}
\subsection{Preliminary}
\label{Preliminary}
We first provide a brief overview of the original denoising diffusion probabilistic model \cite{ho2020denoising} (DDPM) scheme. As shown in Fig \ref{fig:dm_tid} (a), the DDPM consists of two Markov chains, which are a diffusion process and a reverse process. The forward chain perturbs data to noise, while the reverse chain converts the noise back into data. For a more detailed understanding of the DDPM, we direct interested readers to the original paper  \cite{ho2020denoising} and recent surveys of DM \cite{diffsurvey1,diffsurvey2}.
 
\textbf{Diffusion process:} Let  $x_0  \sim p(x_0)$ where  $x_0$ is the training sample and $ p(x_0)$ be the data density. Gaussian transitions $\mathcal{N}$ are successively added to perturb the training sample $x_0$. A sequence of corrupted data $x_1, x_2, \dots, x_T$ can be obtained following the forward Markov chain:
\begin{align}
    p(x_t | x_{t-1} = \mathcal{N}(x_t; \sqrt{1-\beta_t} \cdot x_{t-1}, \beta_t \cdot \textbf{I}), \forall t \in \left\{ 1,\dots T \right\}
\end{align}
where $\beta_1 \dots \beta_T \in \left [0,1\right )$ are the hyperparameters for the variance schedule in each step, $T$ is the number of diffusion steps, and $\textbf{I}$ is the identity matrix. We use $\mathcal{N}(x;\mu,\sigma^2)$ to denote the normal distribution of mean $\mu$ and standard deviation $\sigma$ that produce $x$.  According to the property of normal distribution recursive formulation, we can directly sample $x_t$ when $t$ is drawn from a uniform distribution as follows: 
\begin{align}
\small
    p(x_t | x_{0} = \mathcal{N}(x_t; \sqrt{\hat{\beta_t}} \cdot x_{0}, (1-\hat{\beta_t}) \cdot \textbf{I}), 
\end{align}
where $\hat{\beta_t} = \prod_{i=1}^{t} \alpha_i $ and $\alpha_t = 1- \beta_t$.

Based on this equation, we can sample any $x_t$ directly when given original data $x_0$ and a fixed variance schedule $\beta_t$. To achieve this, the sample $x$ of a normal distribution $x \sim \mathcal{N}(\mu , \sigma^2 \cdot \textbf{I} )$  first subtracts the mean $\mu$ and divides by the standard deviation $\sigma$, outputting in a sample $z= \frac{x-\mu}{\sigma}$ of the standard normal distribution $z \sim \mathcal{N}(0, \textbf{I})$. Thus  $x_t$ is sampled from $p(x_t | x_0)$ as follows: 
\begin{align}
\small
    x_t = \sqrt{\hat{\beta_t}} \cdot x_{0} + \sqrt{(1-\hat{\beta_t})} \cdot z_t), 
\end{align}
where $z_t \sim \mathcal{N}(0,\textbf{I})$. 

\textbf{Reverse process:} To generate new samples from $p(x_0)$, we apply the reverse Markov chain from a $x_T \sim \mathcal{N}(0, \textbf{I})$: 
\begin{align}
\small
    p_{\theta}(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_{\theta}(x_t, t), \Sigma_{\theta}^{}(x_t, t) , 
\label{eq:reverse}
\end{align}
where $\theta$ denotes model parameters, and the mean $\mu_{\theta}(x_t, t)$ and variance $\Sigma_{\theta}(x_t, t) $ are parameterized by deep neural networks $\epsilon_\theta$. 
\begin{align}
\small
    x_{t-1} = \frac{1}{\sqrt{\beta_{t}}} (x_t - \frac{1-\beta_t}{\sqrt{\hat{1-\beta_t}}} \epsilon_\theta(x_t, t))+\sigma_t z, 
\label{eq:reverse_xt}
\end{align}
If $t>1, z \sim \mathcal{N}(0,\textbf{I})$, else $z = 0$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Baseline}
\label{Baseline}

\begin{figure}[htp]
\vspace{-5pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/baseline.pdf}
  \vspace{-10pt}
  \caption{The baseline of applying diffusion model for video-based HMR. Features of each frame are stacked as a joint feature $X_0$. Following the conventional diffusion process, $X_0$ becomes the $X_T$ by adding noise. Then the output feature $Y_0$ is generated during the reverse process.}
  \label{fig:baseline}
  \vspace{-5pt}
\end{figure}

% Inspired by the DM's powerful capability to bridge the gap between uncertain and determinate distributions in denoising processes, we intuitively want to utilize the DM for modeling human motion in video sequences. However, it is not applicable to naively apply the DM for the video-based HMR task. The motion patterns involved during each frame can not be treated as Gaussian noise which is added through frames. Given the extracted features for each frame, the difference of features in adjacent frames can not be represented by equation \ref{eq:reverse}. Thus, the features after denoising in the conventional reverse process can not be used for outputting the final human mesh of each frame. 
% To apply DM for modeling motion features, the existing method [Modiff FLAME] adopts another strategy as shown in Fig \ref{fig:baseline}. The features of each frame $x_t, t \in \left\{ 1,\dots, T \right\}$ are concatenated as the sampling $X_0 = concatenate(x_0, x_1, \dots , x_T)$. Then, the conventional diffusion process is applied based on $X_0$,  and the output sampling can be obtained by the reverse process as described in Section \ref{Preliminary}. Although the DM can be successfully applied in this scenario, the motion pattern over frames is greatly neglected due to the feature concatenation operation. Moreover, video-based HMR methods require fast inference speed for deploying on real-world applications. Unfortunately, DM-based methods typically demand a significant number of steps to achieve good performance, which limits the use of DM.  


Inspired by the DM's powerful capability to bridge the gap between uncertain and determinate distributions in denoising processes, we intuitively want to utilize the DM for modeling human motion in video sequences. However, directly applying the vanilla DM \cite{ho2020denoising} as described in Section \ref{Preliminary} to video-based human mesh recovery (HMR) is not applicable, as motion patterns in each frame cannot be simply treated as Gaussian noise added across frames. The difference between the mesh features of adjacent frames cannot be represented by Eq. \ref{eq:reverse}. Thus, the features after denoising in the conventional reverse process are unreasonable to be used for outputting the final human mesh of each frame. 

Motivated by the recent works \cite{modiff,flame} which utilize DM for text-to-motion generation, we adopt a similar strategy as illustrated in Fig. \ref{fig:baseline} for motion feature modeling. The features of each frame $x_t, t \in \left\{ 1,\dots, T \right\}$  are concatenated as the sampling $X_0 = concatenate(x_0, x_1, \dots, x_T)$. Then, the conventional diffusion process is applied based on $X_0$, with the output sampling obtained using the reverse process as described in Section \ref{Preliminary}. Although the DM can be successfully applied in this scenario, motion patterns over frames are greatly neglected due to the feature concatenation operation. Furthermore, video-based HMR methods require fast inference speeds for real-world applications. Unfortunately, DM-based methods typically require a significant number of steps to achieve good performance, limiting their use in these applications.
%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[htp]
\vspace{-5pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/arch.pdf}
  \vspace{-10pt}
  \caption{Overall framework of our DDT: The features of each frame are extracted by the backbone, then a motion encoder is applied to model the motion features of the adjacent frames. \textbf{We assume  that $x_T$ reaches the final state $x_{end}$ after a sufficiently long time.} Now given the motion feature sequence, a motion decoder is proposed to return the mesh features by decoding the motion information over frames. \textbf{We only need one step for output $y_T$ given $y_{start}$}. Finally, the mesh sequence output is generated after the mesh regressor \cite{Kolotouros2019SPIN} of each frame.  The total steps of our DDT are $T+1$.  }
  \label{fig:arch}
  \vspace{-5pt}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%

\subsection{ Diffusion-Driven Transformer-based framework (DDT)}
\label{DDT}
To address the aforementioned issues of our baseline, we propose our Diffusion-Driven Transformer-based framework (DDT) for HMR from a video. The overview architecture is illustrated in Fig. \ref{fig:arch}. Our framework takes a sequence input with $T$ frames, and the features of each frame $x_t$ are extracted by the backbone (e.g. ResNet50 \cite{resnet}), where $ t \in \left\{ 1,\dots T \right\}$. We assume that human motion continuously affects the extracted features, and after a sufficiently long time (for example $t=200$ steps), $x_t$ reaches the final state $x_{end}$, which can be considered as the pure Gaussian noise. To model the human motion information over frames, a motion encoder is applied to tackle motion features $m_t$, which represent the motion information from $t$ frame to $t+1$ frame. Since the motion encoder is not the main innovative component of our framework, we adopt the same architecture as in TCMR \cite{tcmr} as our motion encoder. 

\noindent \textbf{Decoding process:} Inspired by the recursive denoising steps during the reverse process in DM, a novel decoding process is proposed to decode the motion information for outputting human mesh features $y_t$ over time $ t \in \left\{ 1,\dots T \right\}$. To achieve this, a transformer-based motion decoder is designed as shown in Fig. \ref{fig:md}.  Unlike using Eq. \ref{eq:reverse_xt} to get the previous state in DM, $y_{t-1}$ can be directly computed by the $y_t$. The decoding process is also started from the $y_{start}$ which is pure Gaussian noise. The key difference for our motion decoder is that we can compute the $y_T$ just in one step following
\begin{align}
\small
   y_{T} = MotionDecoder(y_{start}, m_{end}, T, mode=0). 
\label{eq:MD_start}
\end{align}
The $mode = 0$ indicates that the motion decoder aims to output $y_T$ in this case. After obtaining $y_T$, we switch the mode to $mode=1$ for our motion decoder to output the motion features of the previous frame.
\begin{align}
\small
   y_{t-1} = MotionDecoder(y_t, m_t, t, mode=1), 
\label{eq:MD_yt}
\end{align}
when $t \in \left\{ 2,\dots T \right\}$. 

For the Motion Decoder, we adopt the standard transformer architecture as in ViT \cite{Dosovitskiy2020ViT}: one Motion Decoder consists of multiple layers of the basic transformer block as follows:
\begin{flalign}
\small
   & X_{attn} = {\rm MSA}(Q,K,V) + X_{in} \\
   & X_{out} = {\rm FFN}(X_{attn}) + X_{attn}
\end{flalign}
\noindent where  $\rm MSA(\cdot)$ represents the Multi-head Self-Attention block that captures the global attention through Query (Q), Key (K), and Value (V) in ViT \cite{Dosovitskiy2020ViT}, and $\rm FFN(\cdot)$ is a feed-forward network consisting of the multilayer perceptron (MLP) and normalization layer. 
%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htp]
\vspace{-5pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/motiondecoder.pdf}
  \vspace{-10pt}
  \caption{The motion decoder output $y_T$ if $mode=0$, else output $y_t,  t \in \left\{ 1,\dots T-1 \right\} $ when $mode=1$. }
  \label{fig:md}
  \vspace{-10pt}
\end{figure}
%%%%%%%%%%%

With this simple design, we obtain the required features of each frame effectively and efficiently. The total processing time can be significantly reduced since we only need $T+1$ steps (normally $T<50$) compared to a large number of steps (usually larger than 200) in conventional DM \cite{ho2020denoising}. 

\noindent \textbf{Augmentation technique:} Naturally, we consider forward motion information to be encoded in the input sequence through the first frame to the last frame, indicated by the green arrow as shown in Fig. \ref{fig:augment}. Nevertheless, there is a backward motion that exists from the last frame to the first frame, highlighted by the orange arrow in Fig. \ref{fig:augment}. Motivated by this, we introduce an augmentation technique to further improve the performance of our DDT framework. Besides the basic decoding process (backward) in the left part that computes $y^{F}_{T}$ to $y^{F}_{1}$ based on the human motion, we further propose a forward decoding process that generates $y^{B}_{1}$ to $y^{B}_{T}$ depends on the backward motion. The output $y_t = (y^{F}_t + y^{B}_t) / 2,$ where $ t \in \left\{ 1,\dots T \right\} $. There are two motion decoders, one for the backward decoding process and another for the forward decoding process, respectively. To ensure that the backward decoding process and forward decoding process both output the desired mesh features, a Mean Squared Error (MSE) loss is used to calculate the error between them for measuring consistency. 

\begin{figure}[htp]
\vspace{-5pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/augment.pdf}
  \vspace{-10pt}
  \caption{Augmentation technique of DDT. Besides forward motion (green arrow), output features can be also generated from the backward motion (orange arrow). The final output features are the average of two generated features. }
  \label{fig:augment}
  \vspace{-10pt}
\end{figure}
%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[htp]
% \tiny
% \scriptsize
\renewcommand\arraystretch{1.1}
% \vspace{-5pt}
\centering
  \caption{Performance comparison with SOTA video-based methods on 3DPW, MPI-INF-3DHP, and Human3.6M datasets. For a fair comparison, all the methods in this table follow the experiment settings in Choi et al. \cite{tcmr}, which are trained on the training set including 3DPW, but do not use the Human3.6M SMPL parameters obtained from Mosh \cite{loper2014mosh}. }
  \vspace{2pt}
  \resizebox{0.98\linewidth}{!}
  {
  \begin{tabular}{cc|cccc|ccc|ccc}
\hline
                                                  &         & \multicolumn{4}{c|}{3DPW}        & \multicolumn{3}{c|}{MPI-INF-3DHP} & \multicolumn{3}{c}{Human3.6M} \\ \hline
\multicolumn{1}{c|}{Approach}                     & Method  & PA-MPJPE $ \downarrow$ & MPJPE$ \downarrow$ & MPVPE$\downarrow$ & ACC-ERR$ \downarrow$ & PA-MPJPE$ \downarrow$    & MPJPE$\downarrow$    & ACC-ERR $\downarrow$   & PA-MPJPE$ \downarrow$   & MPJPE $\downarrow$  & ACC-ERR $\downarrow$  \\ \hline
\multicolumn{1}{c|}{\multirow{2}{*}{many-to-one}} & TCMR \cite{tcmr}    & 52.7     & 86.5  & 103.2 & \textbf{6.8}   & 63.5        & 97.6     & \textbf{8.5}      & 50.0       & 73.6    & 3.9    \\
\multicolumn{1}{c|}{}                             & MPS-Net \cite{MPS-Net}  & \textbf{52.1}     & \textbf{84.3}  & \textbf{99.7}  & 7.4   & \textbf{62.8}        & \textbf{96.7}     & 9.6      & \textbf{47.4}       & \textbf{69.4}    & \textbf{3.6}    \\ \hline
\multicolumn{1}{c|}{\multirow{4}{*}{many-to-may}} & HMMR \cite{hmmr}    & 72.6     & 116.5 & 139.3 & 15.2  & -           & -        & -        & 56.9       & -       & -      \\
\multicolumn{1}{c|}{}                             & VIBE \cite{kocabas2020vibe}   & 57.6     & 91.9  & -     & 25.4  & 68.9        & 103.9    & 27.3     & 53.3       & 78.0    & 27.3   \\
\multicolumn{1}{c|}{}                             & MEVA \cite{meva}   & 54.7     & 86.9  & -     & 11.6  & \textbf{65.4}        & \textbf{96.4}     & 11.1     & 53.2       & 76.0    & 15.3   \\
\multicolumn{1}{c|}{}                             & \textbf{DDT}    & \textbf{53.3}     & \textbf{85.9}  & \textbf{101.2} & \textbf{6.6}   & \textbf{65.4}        & 97.8     & \textbf{8.2}      & \textbf{48.6}          & \textbf{73.1}       & \textbf{3.3}      \\ \hline
\end{tabular}
}
\label{tab: alldataset}
\vspace{-10pt}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Loss Functions}
During the training, we supervise the SMPL parameters $\Theta_t$ where $t \in \left\{ 1,\dots T \right\}$ with the ground-truth SMPL parameters of each frame using the $L2$ loss. Following \cite{kocabas2020vibe,tcmr}, the 3D joint coordinates are regressed by forwarding the SMPL parameters to the SMPL model. The 2D joints coordinates are also obtained by projecting the 3D joint coordinates to 2D using the predicted camera parameters. Besides, as mentioned in Section \ref{DDT}, MSE loss is applied between the $y^{B}_{1}$ to $y^{B}_{T}$ from two decoding processes to ensure that both decoders can output the correct features. More detailed information is included in the \textcolor{blue}{supplementary \ref{detail}}.

\section{Experiments}

\subsection{Implementation details}
For a fair comparison, we strictly follow the standard implementation details for the video-based HMR task as in VIBE \cite{kocabas2020vibe}, TCMR \cite{tcmr}, and MPS-Net \cite{MPS-Net}. The length of the input sequence $T$ is 16, and the input video frame rate is 25-30 frames per second. Following \cite{kocabas2020vibe} \cite{tcmr} \cite{MPS-Net}, we pre-compute the static features from the cropped images with the size of $224 \times 224$ by ResNet \cite{resnet} to save the memory and speed up the training time. The SMPL regressor SPIN \cite{Kolotouros2019SPIN} is initialized by its official pretrained weights. Following \cite{zhang2019predicting} and \cite{tcmr}, all the 3D rotations of $\theta$ are initially predicted in the 6D rotational representation and converted to the 3D axis-angle rotations. We implemented our method with PyTorch \cite{PyTorch} using one NVIDIA A5000 GPU. Adam optimizer \cite{kingma2014adam} is used 
% for updating the weights 
with the initial learning rate set to $5e^{-6}$ and a batch size of 32 for 100 epochs. 

\subsection{Datasets and evaluation metrics}

\textbf{Datasets:} Following the previous works \cite{kocabas2020vibe}\cite{tcmr}\cite{MPS-Net}, a mixed 2D and 3D datasets are used for training. PoseTrack \cite{posetrack} and InstaVariety \cite{hmmr} are 2D datasets where 2D ground-truth annotations are provided for PoseTrack \cite{posetrack}, while pseudo ground-truth 2D annotations are generated from \cite{openpose} for InstaVariety \cite{hmmr}.  3DPW \cite{pw3d2018}, MPI-INF-3DHP \cite{mpi3dhp2017}, Human3.6M \cite{h36m_pami}, and AMASS \cite{AMASS2019} are 3D datasets for training. Among them, ground-truth SMPL parameters are available for 3DPW and AMASS. For the evaluation, Human3.6M, MPI-INF-3DHP, and 3DPW are selected for the performance comparison. More detailed settings are included in the \textcolor{blue}{supplementary \ref{detail}}.

\textbf{Evaluation metrics:} The HMR performance is evaluated by four standard metrics: Mean Per Joint Position Error (MPJPE), Procrustes-Aligned MPJPE (PA-MPJPE), Mean Per Vertex Position Error (MPVPE), and Acceleration Error (ACC-ERR). Particularly, MPJPE, PA-MPJPE, and MPVPE indicate the accuracy of the estimated 3D human pose and shape with measured in millimeters (mm). ACC-ERR is proposed in HMMR \cite{hmmr} for evaluating temporal smoothness, which computes the average difference between the predicted and ground-truth acceleration of each joint in ($mm/s^2$). 

\subsection{Comparison with state-of-the-art methods}
We compare our DDT with previous SOTA video-based methods on Human3.6M, MPI-INF-3DHP, and 3DPW datasets. Among these datasets, Human3.6M is an indoor dataset while MPI-INF-3DHP and 3DPW contain complex outdoor scenes. Following \cite{tcmr} \cite{MPS-Net}, 3DPW training set are involved in training, but Human3.6M SMPL parameters from Mosh \cite{loper2014mosh} are not used for supervision due to the license issue \cite{meva}. The results are shown in Table \ref{tab: alldataset}. For many-to-many approaches, our DDT outperforms previous SOTA methods on 3DPW and Human3.6M for all the evaluation metrics. Moreover, DDT significantly reduces the acceleration error which indicates motion smoothness and temporal consistency. Even compared with the many-to-one approaches \cite{tcmr} and \cite{MPS-Net} that aim for motion smoothness and temporal consistency, DDT still achieves the lowest acceleration error with similar recovery performance.

Next, we compare the in-the-wild performance with previous SOTA video-based methods in Table \ref{tab: 3dpw}. In this experiment, we evaluate the performance of the 3DPW test set while  the 3DPW training set is not used for training. DDT also surpasses all many-to-many methods for all evaluation metrics. Similar to the results in Table \ref{tab: alldataset}, DDT achieves better MPJVE and acceleration error than many-to-one methods \cite{tcmr} and \cite{MPS-Net} with comparable PA-MPJPE and MPJPE. Overall, extensive experiments verify the strong generalization property of our DDT on three challenging datasets, especially for the excellent performance in terms of motion smoothness and temporal consistency measured by ACC-ERR.          



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[tp]
% \tiny
% \scriptsize
\renewcommand\arraystretch{1.1}
% \vspace{-5pt}
\centering
  \caption{Performance comparison with SOTA video-based methods on 3DPW dataset. All methods in this table do not use 3DPW training set during training. }
  \vspace{2pt}
  \resizebox{0.98\linewidth}{!}
  {
  \begin{tabular}{c|c|cccc}
\hline
                             &            & \multicolumn{4}{c}{3DPW}         \\ \hline
Approach                     & Method     & PA-MPJPE $\downarrow$ & MPJPE$ \downarrow$ & MPVPE $\downarrow$ & ACC-ERR $\downarrow$ \\ \hline
\multirow{2}{*}{many-to-one} & TCMR \cite{tcmr}      & 55.8     & 95.0  & 111.3 & \textbf{6.7}   \\
                             & MPS-Net \cite{MPS-Net}   & \textbf{54.0}  & \textbf{91.6}  & \textbf{109.6} & 7.5   \\ \hline
\multirow{4}{*}{many-to-may} & HMMR \cite{hmmr}      & 72.6     & 116.5 & 139.3 & 15.2  \\
                             & Sun et al. \cite{sun2019human} & 69.5     & -     & -     & -     \\
                             & VIBE \cite{kocabas2020vibe}      & 56.5     & 93.5  & 113.4 & 27.1  \\
                             & \textbf{DDT}       & \textbf{56.1} & \textbf{92.4}  & \textbf{109.1} & \textbf{6.6}   \\ \hline
\end{tabular}
}
\label{tab: 3dpw}
\vspace{-5pt}
\end{table}

\begin{table}[tp]
% \tiny
% \scriptsize
\renewcommand\arraystretch{1.1}
% \vspace{-5pt}
\centering
  \caption{Inference time comparison between our DDT and many-to-many methods with the same hardware platform.}
  \vspace{2pt}
  \resizebox{1\linewidth}{!}
  {
\begin{tabular}{c|c|c|cc|c|cc}
\hline
\multirow{2}{*}{Approach}    & \multirow{2}{*}{Method} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\# of input \\ frames\end{tabular}} & \multicolumn{2}{c|}{3DPW} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\# of output \\ frame\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}processing \\ time\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}average time \\ per frame\end{tabular}} \\ \cline{4-5}
                             &                         &                                                                                & MPVPE $\downarrow$       & ACC-ERR $\downarrow$      &                                                                                &                                                                             &                                                                                        \\ \hline
\multirow{2}{*}{many-to-one} & TCMR                    & 16                                                                             & 103.2       & \underline{6.8}         & 1                                                                              & 32.5 ms                                                                     & 32.5 ms/frame                                                                          \\
                             & MPS-Net                 & 16                                                                             & \textbf{99.7}        & 7.4         & 1                                                                              & 31.1 ms                                                                     & \underline{31.1 ms/frame}                                                                          \\ \hline
many-to-many                 & \textbf{DDT}                    & 16                                                                             &  \underline{101.2}      & \textbf{6.6}         & 16                                                                             & 76.3 ms                                                                     & \textbf{4.8} ms/frame                                                                           \\ \hline
\end{tabular}
}
\label{tab: ab_fps}
\vspace{-10pt}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Inference time analysis:} The previous methods such as TCMR \cite{tcmr} and MPS-Net \cite{MPS-Net} adopt the many-to-one approach to enhance the motion smoothness and temporal consistency. However, due to the \textbf{high reliance on future frames}, the network is not causal when feature frames are unknown, which is critical for online processing. Furthermore, the improvement of motion smoothness is come with sacrificing average processing time. Given a sequence of frames (e.g. 16 frames) as input, TCMR \cite{tcmr} and MPS-Net \cite{MPS-Net} can only output the human mesh of one frame. As a many-to-many approach, our DDT further improves motion smoothness and temporal consistency without the above limitations. We compare the mesh recovery performance and average processing time with previous SOTA methods as shown in Table. \ref{tab: ab_fps}. Given the input of 16 frames, our DDT can output the human mesh of each frame, resulting in a processing time of 76.3 $ms$. Compared to the SOTA many-to-one methods, DDT still achieves comparable recovery performance and better motion smoothness but requires much less processing time per frame (provide a speedup of 7 $\times$ on a single NVIDIA A5000 GPU).    




\subsection{Ablation study}
We conduct the ablation study with the same setting as in Table \ref{tab: alldataset}, which means InstaVariety \cite{hmmr},MPI-INF-3DHP \cite{mpi3dhp2017}, Human3.6M \cite{h36m_pami} and 3DPW \cite{pw3d2018} are used for training, while the 3DPW test set is used for evaluation.  

\textbf{Compared with the baseline:} For the baseline in Section \ref{Baseline}, we follow the existing works \cite{flame,modiff} that concatenate the features of each frame together as the diffusion sample $X_0$. The desired features of human mesh can be successfully denosied during the reverse process. The results are shown in Table \ref{tab: ab_baseline}. Although the baseline achieves comparable performance for the MPJPE, PA-MPJPE, and MPVPE, the ACC-ERR is much worse than DDT, which indicates the baseline is not capable of enhancing the motion smoothness. In our DBT framework, the motion pattern is well-modeled since the decoding process is designed to decode the motion information.  

\begin{table}[]
% \tiny
% \scriptsize
\renewcommand\arraystretch{1.1}
% \vspace{-5pt}
\centering
  \caption{Ablation study of baseline and our DDT on 3DPW and Human3.6M datasets. }
  \vspace{2pt}
  \resizebox{0.8\linewidth}{!}
  {
\begin{tabular}{c|cccc}
\hline
         & \multicolumn{4}{c}{3DPW}         \\ \hline
Method   & PA-MPJPE $\downarrow$ & MPJPE $\downarrow$ & MPVPE $\downarrow$ & ACC-ERR $\downarrow$ \\ \hline
Baseline & 54.6     & \textbf{84.8}  & \textbf{100.9} & 26.7   \\
\textbf{DDT}      & \textbf{53.3}     & 85.9  & 101.2 & \textbf{6.6}   \\ \hline
         & \multicolumn{4}{c}{Human3.6M}    \\ \hline
         & PA-MPJPE $\downarrow$ & MPJPE $\downarrow$ & MPVPE $\downarrow$ & ACC-ERR $\downarrow$ \\ \hline
Baseline & \textbf{47.8}     & 74.0  & -     & 24.5     \\
\textbf{DDT}      & 48.6     & \textbf{73.1}  & -     & \textbf{3.3}   \\ \hline
\end{tabular}
}
\label{tab: ab_baseline}
\vspace{-5pt}
\end{table}

\textbf{Effectiveness of the mode in motion decoder:} 
In our DDT framework, only one motion decoder with two modes is proposed for motion decoding of all the frames. The entire decoding process is decomposed into two phases. When $mode=0$, the motion decoder outputs the $y_T$ given the $y_{start}$ and $m_{end}$. After this phase,  the motion decoder returns the $y_{(t-1)}$ given the $y_t$ and $m_t$ recursively. To evaluate the effectiveness of this two phases procedure, we conduct experiments based on the other design choice. As shown in Fig. \ref{fig:ab_decoder}, the ``one mode'' denotes that only one decoder is adopted without the mode token. The ``one phase'' represent that $y_{T}$ is directly estimated from the $x_T$ without using the $y_{end}$. The results of each choice are shown in Table \ref{tab: ab_mode}. Compared with the ``two modes'' in DDT, the ``one mode'' achieve similar Acceleration in terms of motion smoothness, but worse PA-MPJPE, MPJPE, and MPVPE for the mesh recovery performance. We infer that the procedure from $y_{start}$ to $y_T$ is more complicated due to the large gap than $y_{t}$ to $y_{t-1}$. Next, for the ``one phase'',   we remove the step from $y_{start}$ to $y_T$ and estimate $y_{T}$ based on the $x_T$. Similarly, the results are worse than  the ``two modes'' in DDT for all the evaluation metrics. Thus, the ``two modes'' starts from $y_{start}$ can generate a better $y_{T}$ for the following recursive steps. 
%%%%%%%%%%%%%%%%
\begin{figure}[]
\vspace{-5pt}
  \centering
  \includegraphics[width=0.95\linewidth]{figure/ab_decoder.pdf}
  \vspace{-5pt}
  \caption{(a) The design of the decoding process in our DDT. The motion decoder has two modes. When $mode$ = 0, the motion decoder output $y_T$, else output $y_t,  t \in \left\{ 1,\dots T-1 \right\}$. (b) The motion decoder output  $y_t,  t \in \left\{ 1,\dots T \right\}$ using one mode. (c) The output  $y_t,  t \in \left\{ 1,\dots T-1 \right\}$ while $y_T$ is estimated from $x_T$.  }
  \label{fig:ab_decoder}
  \vspace{-10pt}
\end{figure}
%%%%%%%%%%%

\begin{table}[]
% \tiny
% \scriptsize
\renewcommand\arraystretch{1.1}
% \vspace{-5pt}
\centering
  \caption{Ablation study of different design choices of the decoding process in our DDT. }
  \vspace{2pt}
  \resizebox{0.8\linewidth}{!}
  {
\begin{tabular}{c|cccc}
\hline
              & \multicolumn{4}{c}{3DPW}         \\ \cline{2-5} 
              & PA-MPJPE $\downarrow$ & MPJPE $\downarrow$ & MPVPE $\downarrow$ & ACC-ERR $\downarrow$ \\ \hline
(a) two modes  & \textbf{53.3}     & \textbf{85.9}  & \textbf{101.2} & \textbf{6.6}   \\ \hline
(b) one mode  & 54.8     & 87.0  & 103.5 & 6.7     \\ \hline
(c) one phase & 57.7     & 86.5  & 104.9 & 7.4    \\ \hline
\end{tabular}
}
\label{tab: ab_mode}
\vspace{-5pt}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
\begin{figure*}[htp]
\vspace{-5pt}
  \centering
  \includegraphics[width=0.95\linewidth]{figure/vis.pdf}
  % \vspace{-10pt}
  \caption{The visual comparison between SOTA many-to-many method VIBE \cite{kocabas2020vibe} with our DDT. The circles highlight locations where DDT is more accurate than VIBE. More examples are provided in the \textcolor{blue}{supplementary \ref{meshvis}}. }
  \label{fig:ab_vis}
  \vspace{-10pt}
\end{figure*}
%%%%%%%%%%%

\textbf{Effectiveness of augmentation technique:} 
Observing that besides the forward motion information ($m_{1}, m_{2}, \cdots m_{T-1}, m_{T}$) can be used for decoding human mesh features over frames, the backward motion ($m_{T}, m_{T-1}, \cdots m_{2}, m_{1}$) also can be contributed for improving the mesh performance and motion smoothness. To verify the effectiveness of our augmentation technique as shown in Fig. \ref{fig:augment}, we conduct the experiments of forward motion only, backward motion only, and integration of forward motion and backward motion in Table \ref{tab: ab_augment}. The results of using forward motion only are similar to the results of using backward motion only. After integrating the backward and forward decoding processes, the human mesh performance (PA-MPJPE, MPJPE, and MPVPE) and motion smoothness (Acceleration error) are improved, which proves the effectiveness of our proposed augmentation technique.       

\begin{table}[tp]
% \tiny
% \scriptsize
\renewcommand\arraystretch{1.1}
% \vspace{-5pt}
\centering
  \caption{Ablation study of our proposed augmentation technique. }
  % \vspace{2pt}
  \resizebox{1\linewidth}{!}
  {
\begin{tabular}{c|cccc}
\hline
                            & \multicolumn{4}{c}{3DPW}         \\ \cline{2-5} 
                            & PA-MPJPE $\downarrow$ & MPJPE $\downarrow$ & MPVPE $\downarrow$ & ACC-ERR $\downarrow$ \\ \hline
forward motion only         & 54.1     & 87.6  & 104.7 & 6.9   \\ \hline
backward motion only        & 54.2     & 88.0  & 105.1 & 6.8   \\ \hline
forward and backward motion & \textbf{53.3}     & \textbf{85.9}  & \textbf{101.2} & \textbf{6.6}   \\ \hline
\end{tabular}
}
\label{tab: ab_augment}
\vspace{-10pt}
\end{table}


\subsection{Qualitative results}
To demonstrate the effectiveness of our BTD, we present the qualitative comparison between BTD and VIBE on in-the-wild video in Fig. \ref{fig:ab_vis}. VIBE \cite{kocabas2020vibe} is a seminal work for video-based HMR which adopts a many-to-many approach. Although achieving excellent performance in terms of mesh recovery performance, it suffers from severe temporal inconsistency. 
% While TCMR \cite{tcmr} and MPS-Net \cite{MPS-Net} switch to a many-to-one approach to tackle motion smoothness and temporal consistency, these methods also have their limitations 
Instead, as a many-to-many approach, our BTD enhances motion smoothness and  temporal inconsistency through our proposed motion decoding framework. Given an in-the-wild video, VIBE fails to generate the accurate mesh of the hand parts. From $22_{th}$ frame to $28_{th}$ frame, two hands should touch together as holding the ball, while the hands of VIBE are always detached. For the next few frames, the hand motion is also not smooth for the VIBE's output. Instead, the generated mesh of our DDT preserves the motion smoothness and temporal inconsistency since hands are close to each other during $22_{th}$ frame to $28_{th}$ frame and is more accurate than VIBE for the following frames. More qualitative results and video demos are provided in the \textcolor{blue}{supplementary \ref{meshvis}}. 




\section{Conclusion}
In this paper, we present a novel Diffusion-Driven Transformer-based framework (DDT) for  human mesh recovery from a video. Inspired by the diffusion model, DDT is designed to decode the motion information from the input sequence. The motion smoothness and temporal consistency of the output mesh sequence are enhanced by the successive decoding steps over frames. As a many-to-many approach, DDT is significantly more efficient than SOTA many-to-many methods. Moreover, an augmentation technique based on human motion is proposed in DDT. Extensive experiments are conducted on Human3.6M, MPI-INF-3DHP, and 3DPW datasets showing impressive performance compared to the SOTA methods.   
%-------------------------------------------------------------------------

\newpage
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%

\noindent \textbf{\large{Supplementary Material}}
\appendix
\section{Overview}

The supplementary material is organized into the following sections:

%\setlist{nolistsep}
\begin{itemize}%[noitemsep,leftmargin=*] 
\item Section \ref{Broader}: Broader Impact and Limitations.

\item Section \ref{detail}: More Implementation Details and  Experiments. 

\item Section \ref{meshvis}: Human Mesh Visualization on in-the-wild video.
\end{itemize}

\section{Broader Impact and Limitations}
\label{Broader}
Our proposed DDT framework for video-based HMR has the potential for widespread applications, including motion capture in animation and movies, virtual AI assistants, and VR/AR content. Currently, these applications may still require expensive and time-consuming motion capture devices, which can be complicated to set up. However, DDT is easy to deploy without any additional devices. As a many-to-many approach, DDT can reliably reconstruct human meshes from videos, even for online streaming. DDT shows great potential as a causal video-based HMR model for real-world applications.
  

Despite its strengths, DDT has some limitations. While it can estimate reliable human meshes for in-the-wild scenarios, its performance may be impacted by severe occlusions, especially in crowded scenes. Additionally, DDT uses a fixed backbone (ResNet50\cite{resnet}) following \cite{tcmr,kocabas2020vibe,MPS-Net} for a fair comparison, but we plan to improve its performance by applying a new backbone that weight can be updated during training in future work.


\section{More Implementation Details}
\label{detail}

\subsection{Datasets}
\noindent \textbf{3DPW} \cite{pw3d2018} is a dataset that captures outdoor and in-the-wild scenes using a hand-held camera and a set of inertial measurement unit (IMU) sensors attached to body limbs. The ground-truth SMPL parameters are computed based on the returned values. This dataset includes 60 videos of varying lengths, and we use the official split to train and test the model. The split comprises 24, 12, and 24 videos for the training, validation, and test sets, respectively. The MPJPE, PA-MPJPE, MPJVE, and ACC-ERR are reported when evaluating this dataset. 


\noindent \textbf{Human3.6M} \cite{h36m_pami} is a large-scale benchmark for the indoor 3D human pose. It includes 15 action categories and 3.6M video frames. Following \cite{kocabas2020vibe,tcmr,MPS-Net}, we use five subjects (S1, S5, S6, S7, S8) for the training set and two subjects (S9, S11) for the testing set. The dataset is subsampled from its original 50 fps to 25 fps for both training and evaluation purposes. When calculating MPJPE and PA-MPJPE, only 14 joints are selected for a fair comparison to the previous works.


\noindent \textbf{MPI-INF-3DHP} \cite{mpi3dhp2017} is a 3D benchmark that consists of both indoor and outdoor environments. The training set includes 8 subjects, with each subject having 16 videos, resulting in a total of 1.3M video frames captured at 25 fps. The markerless motion capture system is used for providing 3D human pose annotations. The test set comprises 6 subjects performing 7 actions in both indoor and outdoor environments. Following \cite{kocabas2020vibe,tcmr,MPS-Net}, the MPJPE and PA-MPJPE are measured on valid frames, which include approximately every 10th frame, using 17 joints defined by MPI-INF3DHP. The ACC-ERR is computed using all frames.

\noindent \textbf{InstaVariety} \cite{hmmr} is a 2D human dataset curated by HMMR \cite{hmmr} , comprising videos collected from Instagram using 84 motion-related hashtags. The dataset contains 28K videos with an average length of 6 seconds, and pseudo-ground truth 2D pose annotations are acquired using OpenPose\cite{openpose}.



\noindent \textbf{PoseTrack} \cite{posetrack} is a 2D benchmark designed for multi-person pose estimation and tracking in videos. This dataset comprises 1.3K videos and 46K annotated frames in total, captured at varying fps around 25 fps. There are 792 videos used for the official train set, which includes 2D pose annotations for 30 frames located in the middle of each video.

\begin{figure*}[htp]
\vspace{-5pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/supp_vis.pdf}
  \vspace{-5pt}
  \caption{Qualitative results of our DDT on in-the-wild videos. \textcolor{blue}{Please refer to our \textbf{video demo} for the more reconstructed mesh sequences results.}}
  \label{fig:supp_vis}
  \vspace{-5pt}
\end{figure*}


\subsection{Loss Function}
% In our DDT, the SMPL model \cite{SMPL:2015} is utilized for reconstructing human mesh. Given the predicted pose parameters $\theta$ and the shape parameters $\beta$, the SMPL model can return the body mesh $M \in  \mathbb{R}^{N \times 3}  $ with $N=6890$ vertices by the function $M = SMPL (\theta, \beta) $. After obtaining the body mesh $M$, the body joints $J$ can be regressed by the predefined joint regression matrix $W$, which means   $J \in  \mathbb{R}^{k \times 3} = W \cdot M  $, where $k$ is the number of joints. We follow the same setting for the loss function. 
% The overall loss during the HMR task can be defined as: 


Our DDT relies on the SMPL model \cite{SMPL:2015} to reconstruct the human mesh. The SMPL model can generate the body mesh $M \in \mathbb{R}^{N \times 3} $ with $N=6890$ vertices by taking in the predicted pose parameters $\theta$ and the shape parameters $\beta$ as inputs, which can be expressed as $M = SMPL (\theta, \beta) $. Once the body mesh $M$ is obtained, the body joints $J$ can be estimated by applying the predefined joint regression matrix $W$, i.e., $J \in \mathbb{R}^{k \times 3} = W \cdot M$, where $k$ represents the number of joints. We adopt the same loss function as previous methods TCMR \cite{tcmr}.

\begin{equation}
\begin{aligned} 
  \begin{aligned}
\mathcal{L}_{TCMR} &=  w_1 \| \beta - \beta^*  \| + w_2 \| \theta - \theta^*  \| + w_3 \| J - J^*  \|\\
  \end{aligned}\\
\end{aligned}
\end{equation}
\noindent where * denote the ground-truth value,  $w_1=0.06$, $w_2=60$, and $w_3=300$.

Besides this general loss for mesh recovery, we add additional loss as mentioned in Section 3.4 of the main paper. By applying our proposed augmentation technique,  we can obtain two outputs $y_t^{F}$ and $y_t^{B}$. The $y_t^{F}$ is generated by the forward decoding process depending on the backward motion, while  $y_t^{B}$ is generated by the backward decoding process depending on the forward motion, respectively. The $y_t^{F}$ and $y_t^{B}$ should be identical. Thus a MSE loss is applied between $y_t^{F}$ and $y_t^{B}$:
\begin{equation}
\begin{aligned} 
  \begin{aligned}
\mathcal{L}_{aug} &=  w_4 \| y_t^{F} - y_t^{B}  \|_2^2\\
  \end{aligned}\\
\end{aligned}
\end{equation}
\noindent where $w_4=100$.
The overall loss for our DDT is the sum of the $\mathcal{L}_{TCMR} $ and $\mathcal{L}_{aug}$:
\begin{equation}
\begin{aligned} 
  \begin{aligned}
\mathcal{L}_{overall} &= \mathcal{L}_{TCMR} +  \mathcal{L}_{aug}\\
  \end{aligned}\\
\end{aligned}
\end{equation}

\noindent As shown in Table \ref{tab:supp_loss}, the augmentation loss can help for improving the reconstruction performance (MPJPE, PA-MPJPE, and MPJVE) and the motion smoothness (ACC-ERR). 
\begin{table}[htp]
% \tiny
% \scriptsize
% \renewcommand\arraystretch{1.2}
% \vspace{-5pt}
\centering
  \caption{Evaluation of the combinations of loss functions on the 3DPW dataset.}
  \resizebox{0.95\linewidth}{!}
  {
\begin{tabular}{c|cccc}
\hline
     & \multicolumn{4}{c}{3DPW}         \\ \hline
loss & PA-MPJPE$\downarrow$ & MPJPE$\downarrow$ & MPVPE$\downarrow$ & Accel$\downarrow$ \\ \hline
$\mathcal{L}_{TCMR}$    & 86.8     & 53.6  & 101.9 & 7.5   \\ \hline
$\mathcal{L}_{TCMR} + \mathcal{L}_{aug}$   & 85.9     & 53.3  & 101.2 & 6.6   \\ \hline
\end{tabular}
}
\label{tab:supp_loss}
\vspace{-10pt}
\end{table}


\subsection{Effectiveness of the number of input frames }
Following the same setting as previous video-based methods such as VIBE \cite{kocabas2020vibe}, TCMR \cite{tcmr}, and MPS-Net \cite{MPS-Net}, the number of input frames is set to be 16. To further investigate the impact of the number of input frames, we conduct experiments on the 3DPW dataset given the different number of input frames. The results are shown in Fig. \ref{tab:supp_frame} (the training detail in Section 4.3 of the main paper). With the increase in the number of input frames, all the performances (MPJPE, PA-MPJPE, MPVPE, and ACC-ERR) can be improved. Due to the successive motion decoding step over frames of our DDT, the motion smoothness and temporal consistency have been enhanced greatly compared with other many-to-many approaches (the ACC-ERR of VIBE \cite{kocabas2020vibe} is 25.4). Hence the ACC-ERR can be decreased slightly when increasing the number of input frames. 





\begin{table}[htp]
% \tiny
% \scriptsize
% \renewcommand\arraystretch{1.2}
% \vspace{-5pt}
\centering
  \caption{Performance of the different number of input frames on 3DPW dataset.}
  \resizebox{0.95\linewidth}{!}
  {
\begin{tabular}{c|cccc}
\hline
      & \multicolumn{4}{c}{3DPW}         \\ \hline
number of frames & PA-MPJPE $\downarrow$ & MPJPE$\downarrow$ & MPVPE$\downarrow$ & ACC-ERR$\downarrow$ \\ \hline
16    & 85.9     & 53.3  & 101.2 & 6.6   \\ \hline
20    & 85.7     & 53.1  & 100.9 & 6.5   \\ \hline
40    & 85.1     & 53.0  & 99.8  & 6.4   \\ \hline
\end{tabular}
}
\label{tab:supp_frame}
\vspace{-10pt}
\end{table}


\section{Human Mesh Visualization on in-the-wild video}
\label{meshvis}
In Fig.~\ref{fig:supp_vis}, we show the qualitative results of DDT on in-the-wild videos. We observe that DDT achieves acceptable performance by reconstructing reasonable human mesh sequences with temporal consistency. 

\textcolor{blue}{Please refer to our \textbf{video demo} for the more reconstructed mesh sequence results.}  




{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}



\end{document}

