% CVPR 2024 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage{cvpr}              % To produce the CAMERA-READY version
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
% \input{preamble}


%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{color}
\usepackage{soul}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{pifont}%
\usepackage{caption}
\definecolor{RowColor}{rgb}{0.97, 0.97, 1}
\definecolor{babyblue}{rgb}{0.63, 0.79, 0.95}
\usepackage{colortbl}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
% \usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{1858} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2024}

\begin{document}
%%%%%%%%% TITLE - PLEASE UPDATE
\title{DiffMesh: A Motion-aware Diffusion-like Framework for Human Mesh Recovery from Videos}

%%%%%%%%% AUTHORS - PLEASE UPDATE
% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\author{Ce Zheng$^{1}$, Xianpeng Liu$^{2}$, Mengyuan Liu$^{3}$, Tianfu Wu$^{2}$, Guo-Jun Qi$^{4,5}$,  Chen Chen$^{1}$\\
$^1$Center for Research in Computer Vision, University of Central Florida\\
$^2$  North Carolina State University\\
$^3$ Key Laboratory of Machine Perception, Peking University, Shenzhen Graduate School \\
$^4$  OPPO Seattle Research Center, USA \quad $^5$ Westlake University\\
% {\tt\small ce.zheng@ucf.edu; xliu59@ncsu.edu;  guojunq@gmail.com; chen.chen@crcv.ucf.edu}\\
% {\tt\small guojunq@gmail.com; chen.chen@crcv.ucf.edu }
}

% \begin{document}
% \maketitle
% \input{sec/0_abstract}    
% \input{sec/1_intro}
% \input{sec/2_formatting}
% \input{sec/3_finalcopy}
% {
%     \small
%     \bibliographystyle{ieeenat_fullname}
%     \bibliography{main}
% }


\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
\vspace{-20pt}
    \centering
   \includegraphics[width=0.88\textwidth]{figure/pipe_comp.pdf}
   \vspace{-5pt}
    \captionof{figure}{Different approaches of applying diffusion model for video-based HMR, the input frame $f$ is 3 for simplicity and the number of steps is $N$. Here $x_{i}$ and $c_{i}$ denote the mesh and conditional features of $i_{th}$ frame, respectively. (a) Baseline 1: The diffusion model is applied for each frame individually. The total steps are $f \times N$, which is computationally expensive. Moreover, motion is ignored during the denoising process, leading to non-smooth motion predictions (high acceleration error). (b) Baseline 2: The features from ground truth mesh vertices of each frame $x_i$ are concatenated to unified features $x_{unified}$ during the forward process. To obtain the mesh of each frame, features are split after the denoising process. Although this strategy reduces the total steps to $N$, it doesn't effectively model human motion during denoising (acceleration error can be further reduced). (c) Our proposed DiffMesh: We consider the inherent motion patterns within the forward process and the reverse process when utilizing the diffusion model, leading to smooth and accurate motion predictions (low acceleration error and MPVE). The total number of steps remains as $N$. For more detailed information, please refer to Sec \ref{DiffMesh}.}
    \label{fig:pipe_comp}
    \vspace{-5pt}
\end{center}%
}]


\begin{abstract}
 Human mesh recovery (HMR) provides rich human body information for various real-world applications. While image-based HMR methods have achieved impressive results, they often struggle to recover humans in dynamic scenarios, leading to temporal inconsistencies and non-smooth 3D motion predictions due to the absence of human motion. In contrast, video-based approaches leverage temporal information to mitigate this issue. In this paper, we present DiffMesh, an innovative motion-aware Diffusion-like framework for video-based HMR. DiffMesh establishes a bridge between diffusion models and human motion, efficiently generating accurate and smooth output mesh sequences by incorporating human motion within the forward process and reverse process in the diffusion model. Extensive experiments are conducted on the widely used datasets (Human3.6M \cite{h36m_pami} and 3DPW \cite{pw3d2018}), which demonstrate the effectiveness and efficiency of our DiffMesh. Visual comparisons in real-world scenarios further highlight DiffMesh's suitability for practical applications. \textcolor{magenta}{The project
webpage is \url{https://zczcwh.github.io/diffmesh_page/}.}
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}



% As one of the fundamental topics in computer vision,  Human Pose Estimation (HPE) from monocular cameras has been developed rapidly with the emergence of deep learning techniques. With HPE approaches such as OpenPose \cite{openpose}, AlphaPose \cite{alphapose}, and VideoPose3D \cite{videopose3d}, excellent 2D or 3D pose estimation performance can be achieved in real-time. However, the desire to detect more than just 2D or 3D human joints from monocular images has led to increased attention on Human Mesh Recovery (HMR). 

Human Mesh Recovery (HMR) aims to recover detailed human body information, encompassing both pose and shape, with applications spanning gaming, human-computer interaction, and virtual reality  \cite{hmrsurvey}. While image-based HMR methods have achieved remarkable results, they often fall short in dynamic scenarios, wherein the neglect of human motion results in temporal inconsistencies and non-smooth 3D motion predictions. In contrast, video-based approaches offer a solution to mitigate these issues by leveraging temporal cues from input sequences, thereby meeting the practical demands of real-world applications. This makes video-based methods a compelling choice for enhancing the temporal coherence and consistency of 3D motion predictions.


Recently, diffusion models have garnered substantial interest within the computer vision community.  By learning to reverse the diffusion process in which noise has been added in finite successive steps, diffusion models can generate samples that match a specified data distribution corresponding to the provided dataset. As generative-based methods, diffusion models excel in tasks such as motion synthesis and generation \cite{MDM,flame,li2023ego}. The quality and diversity of generated outputs have witnessed significant improvements by bridging the gap between uncertain and determinate distributions during the denoising process. This inspires us to consider the diffusion model as a promising solution for recovering high-quality human mesh from input data fraught with depth ambiguity.




While it is straightforward to apply the diffusion model for image-based HMR such as in \cite{foo2023distribution}, effective utilization of the diffusion model for video-based HMR remains a formidable challenge. \ul{As previous video-based HMR methods have not yet incorporated diffusion models into their frameworks}, we provide a summary of diffusion-based methods related to human pose and mesh recovery (please refer to Sec. \ref{sec:related_diff} for further elaboration). We then organize these methods into two intuitive baseline approaches that can be applied for video-based HMR, as illustrated in Fig. \ref{fig:pipe_comp} (a) and (b).
% As prior work has not directly applied diffusion models to video-based HMR from video input (please refer to Section \ref{sec:related_diff} for further elaboration), we propose two intuitive baseline approaches for incorporating the diffusion model into video-based HMR, as illustrated in Fig. \ref{fig:pipe_comp} (a) and (b). 
Assuming a video input with $f$ frames in \ul{Baseline 1}, the diffusion model is applied individually to recover the mesh of the single frame (usually would be the center frame). More details can be found in Sec. \ref{Baseline}.
% During the \textit{forward diffusion process}, the ground truth mesh vertices of the target frame undergo gradual diffusion through the addition of noise. To obtain high-quality human mesh for each frame, the objective is to progressively denoise the features using the diffusion model during the \textit{reverse diffusion process}, often referred to as the \textit{denoising process}. The features from the input image would be used as conditional features in the diffusion model. 
However, this approach suffers from significant computational burdens as the total number of denoising steps amounts to  $f \times N$ to recover the mesh sequence of $f$ frames, with $N$ representing the number of denoising steps in the diffusion model. Moreover, the diffusion model does not account for motion smoothness in dynamic sequences, which may result in potentially temporal inconsistent mesh predictions. To mitigate computational costs and enhance inference speed, we introduce \ul{Baseline 2}. The features for ground truth mesh vertices of each frame $x_{i}$, where $i \in \left\{ 1,\dots f \right\}$ are concatenated to unified features $x_{unified}$ before the forward diffusion process. 
% Subsequently, the conditional features from each input frame are concatenated as well. 
% Following the standard \textit{forward diffusion process} and \textit{reverse denoising process}, T
Subsequently, the output features are partitioned back into individual frames $x_{i}$ after the \textit{denoising process}. 
% This concatenation and separation strategy aligns with a prevailing trend in recent diffusion-based methods such as \cite{li2023ego,shan2023diffusion,flame}. 
While this strategy effectively reduces the number of steps from $f \times N$ to $N$,  it is important to note that motion smoothness in dynamics is still not considered during the forward and reverse process since features across frames are simply concatenated. This limitation can potentially lead to non-smooth motion predictions.

\begin{figure}[tp]
\vspace{-5pt}
  \centering
  \includegraphics[width=0.9\linewidth]{figure/dm_TID.pdf}
  \vspace{-5pt}
  \caption{(a) The general pipeline for diffusion model. Input data is perturbed by adding noise recursively and output data is generated from the noise in the reverse process. Images are taken from \cite{diffsurvey1}. (b) Human motion is involved over time in the input video sequence. Similar to the forward process in (a), the forward motion between adjacent frames resembles the process of adding noise. The mesh of the previous frame can be decoded through the reverse motion process successively. }
  \label{fig:dm_tid}
  \vspace{-15pt}
\end{figure}


To tackle the aforementioned efficiency and motion smoothness challenges, we introduce \textbf{DiffMesh}, a novel motion-aware diffusion-like framework as illustrated in Fig. \ref{fig:pipe_comp} (c). DiffMesh is designed specifically for video-based HMR. Unlike conventional diffusion models, which necessitate the addition of Gaussian noise in consecutive steps during the \textit{forward diffusion process}, as depicted in Fig. \ref{fig:dm_tid} (a), DiffMesh adopts a distinct approach. We consider the forward motion influence over a brief time interval as shown in Fig. \ref{fig:dm_tid} (b), encompassing the features (embedded in the high-dimensional space) of adjacent frames $x_i$ to $x_{i+1}$, similar to the mechanism of introducing noise. Importantly, our approach does not conclude the forward motion at the last frame $x_{f}$, where $f$ is the total number of frames. Instead, we assume that motion culminates in an initial distribution represented by the static state, which is similar to pure Gaussian noise in the conventional diffusion model, after additional forward steps. During the \textit{reverse process}, the diffusion model decodes the previous feature $x_{i-1}$ based on $x_{i}$ and the corresponding conditional feature $c_{i}$ from the input frame. Consequently, the total number of steps remains consistent at $N$, as illustrated in Fig. \ref{fig:pipe_comp} (b). 

\textbf{Contribution:} The key innovation of DiffMesh lies in its capacity to consider the inherent motion patterns within the diffusion model efficiently and effectively. This consideration notably enhances motion smoothness and temporal consistency throughout the output mesh sequence, achieved through successive steps across frames. Incorporating our diffusion framework, we design a new two-stream transformer architecture to decode motion features. Our DiffMesh not only achieves new state-of-the-art results on the Human3.6M \cite{h36m_pami} and 3DPW \cite{pw3d2018} datasets, but also demonstrates its superiority in handling in-the-wild scenarios compared to previous methods. These advantages make DiffMesh more viable for deployment in real-world applications for generating accurate and smooth human mesh sequences efficiently.

% Our contributions are summarized as follows:
% \setlist{nolistsep}
% \begin{itemize}[noitemsep,leftmargin=*] 
%     \item We propose a novel diffusion-based framework (DiffMesh) for HMR from videos. Compared two baselines, our DiffMesh is designed to decode specific motion patterns efficiently and effectively, generating more accurate and smooth output mesh sequences. 
%     \item Incorporating our diffusion framework, we design a new two-stream transformer architecture specifically for video-based HMR.  
%     \item Our DiffMesh not only achieves new state-of-the-art results on the Human3.6M \cite{h36m_pami} and 3DPW \cite{pw3d2018} datasets, but also demonstrates its superiority in handling in-the-wild scenarios compared to previous methods.
%     % in terms of accuracy and motion smoothness.
% \end{itemize}




 
\section{Related Work}
Since human mesh recovery is a popular topic in computer vision, here we focus on the most relevant works to our methods. We refer readers to the recent and comprehensive HMR survey \cite{hmrsurvey} for more details. 

% \subsection{Image-based HMR}
% The majority of methods  ~\cite{lin2021metro,pymaf2021,zheng2023feater,you2023gator,kolotouros2019cmr,li2023hybrik,zheng2023potter} for HMR rely on a parametric human model, such as SMPL \cite{SMPL:2015}, to reconstruct the mesh by estimating pose and shape parameters. 
% % Kanazawa et al. \cite{kanazawaHMR18} propose a weakly supervised approach to regress the SMPL parameters that minimizes the 2D re-projection loss of keypoints. 
% As a fundamental HMR work, SPIN ~\cite{Kolotouros2019SPIN} combines regression and optimization in a loop, where the regressed output serves as better initialization for optimization (SMPLify). 
% % METRO \cite{lin2021metro} is the first transformer-based method that models vertex-vertex and vertex-joint interaction using a transformer encoder after extracting image features with a CNN backbone. 
% Foo et al. \cite{foo2023distribution} first introduce a diffusion-based approach for recovering human mesh from a single image. The recovered human mesh is obtained by the reverse diffusion process. 
% % As an extension work of METRO,  MeshGraphormer combines a graph convolutional network with a transformer to model local and global interactions among mesh vertices and joints, but it is computationally expensive. Zeng et al. \cite{tcformer} propose a Token Clustering Transformer to merge tokens from different locations through a progressive clustering procedure. However, it performs worse than METRO and MeshGraphormer. 
% However, when applied to video sequences, these image-based methods suffer from severe motion jitter due to frame-by-frame reconstruction, making them unsuitable for practical use.


\subsection{Video-based HMR}
Similar to the image-based methods ~\cite{lin2021metro,pymaf2021,zheng2023feater,you2023gator,kolotouros2019cmr,li2023hybrik,zheng2023potter}, video-based methods \cite{hmmr,meva,sun2019human,zeng2022deciwatch,li2022dnd,zeng2022deciwatch,GLoT,you2023coeval,yang2023capturing} for HMR also utilize parametric human models like SMPL \cite{SMPL:2015}.
% Kanazawa et al.\cite{hmmr} first propose a convolutional network to learn human motion kinematics by predicting past, current, and future frames. Based on \cite{hmmr}, Sun et al. \cite{sun2019human} further propose a self-attention-based temporal model to improve performance. 
VIBE \cite{kocabas2020vibe} leverages the AMASS dataset \cite{AMASS2019} to discriminate between real human motions and those produced by its temporal encoder. MEVA \cite{meva} estimates the coarse 3D human motion using a variational motion estimator, then refines the motion by a motion residual regressor. TCMR \cite{tcmr} utilizes GRU-based temporal encoders to forecast the current frame from the past and future frames, which enhances the temporal consistency. Similar to \cite{tcmr}, MPS-Net \cite{MPS-Net} captures the motion continuity dependencies by the attention mechanism. MotionBERT \cite{MotionBERT2022} proposes a pertaining stage to recover 3D motion from noise 2D observations, achieving state-of-the-art performance.
% for video-based HMR. 




\subsection{Diffusion Generative Models}
\label{sec:related_diff}
Diffusion Generative Models have achieved impressive success in a wide variety of computer vision tasks such as image inpainting \cite{song2021scorebased}, text-to-image generation \cite{photorealistic}, and image-to-image translation \cite{choi2021ilvr}. Given the strong capability to bridge the large gap between highly uncertain and determinate distribution, several works have utilized the diffusion generative model for the text-to-motion generation \cite{modiff,MDM,zhang2022motiondiffuse}, human pose estimation \cite{shan2023diffusion}, object detection \cite{chen2023diffusiondet}, and head avatar generation \cite{bergman2023articulated,mendiratta2023avatarstudio}. 
% For instance, Zhang et al. \cite{zhang2022motiondiffuse} propose a versatile motion-generation framework that incorporates a diffusion model to generate diverse motions from comprehensive texts. 
% Similarly, Tevet et al. \cite{MDM} introduce a lightweight transformer-based diffusion generative model that can achieve text-to-motion and motion editing. Zhao et al. \cite{modiff} tackle sequential skeleton-based motion generation, where the skeleton sequence is treated as a 2D image matrix with three channels (N: number of frames, J: number of joints in skeleton in one frame, and D: x, y, z coordinate of each joint). 
% Several pioneering works have explored the application of diffusion models in human pose-related tasks. 
DiffPose \cite{gong2023diffpose} is the first to utilize a diffusion-based approach for predicting the 3D pose from a 2D pose sequence input. Shan et al. \cite{shan2023diffusion} generate multiple hypotheses of the 3D pose from 2D pose input using a diffusion model, and then a multi-hypothesis aggregation module outputs the final 3D pose. EgoEgo \cite{li2023ego} employs a diffusion model to generate multiple plausible full-body motions based on the head pose input. 


\begin{table}[tp]
% \tiny
% \scriptsize
\renewcommand\arraystretch{1.1}
\vspace{-5pt}
\centering
  \resizebox{1\linewidth}{!}
  {
  % Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{tabular}{l|c|ccc|c|c}
\hline
\multicolumn{1}{c|}{Methods} & Input              & \multicolumn{3}{c|}{Output}    & Motion-Aware   & Approach            \\ \cline{3-5}
\multicolumn{1}{c|}{}        &                    & 3D Pose & 3D Mesh & multi-frame  & Diffusion &    Similar to                   \\ \hline
DiffPose \cite{SpatioTemporalDiffPose}      & Video sequence     & \cellcolor{babyblue} \cmark     & \xmark      & \xmark    & \xmark       & Baseline 1 \\
Diff3DHPE \cite{zhou2023diff3dhpe}      & 2D Human Pose Sequence   & \cellcolor{babyblue} \cmark     & \xmark    & \cellcolor{babyblue} \cmark        & \xmark    & Baseline 2 \\
DiffPose \cite{gong2023diffpose}       & 2D Human Pose Sequence   & \cellcolor{babyblue} \cmark     & \xmark      & \xmark   & \xmark        & Baseline 1 \\
D3DP \cite{shan2023diffusion}           & 2D Human Pose Sequence   & \cellcolor{babyblue} \cmark     & \xmark      & \cellcolor{babyblue} \cmark  & \xmark        & Baseline 2 \\
HMDiff \cite{foo2023distribution}          & Single Image       & \cellcolor{babyblue} \cmark     & \cellcolor{babyblue} \cmark     & \xmark       & \xmark    & Baseline 1 \\
EgoEgo  \cite{li2023ego}                     & Head Pose Sequence & \cellcolor{babyblue} \cmark     & \cellcolor{babyblue} \cmark     & \cellcolor{babyblue} \cmark    & \xmark      & Baseline 2 \\ \hline
DiffMesh (ours)              & Video sequence     & \cellcolor{babyblue} \cmark     & \cellcolor{babyblue} \cmark     & \cellcolor{babyblue} \cmark        &  \cellcolor{babyblue} \cmark  &   \textcolor{red}{Our design}                    \\ \hline
\end{tabular}
}
\vspace{-5pt}
\caption{Comparison with previous human pose and mesh-related methods utilizing diffusion models. }
\label{tab:frame-comp}
\vspace{-10pt}
\end{table}

\noindent \textbf{Distinction of Our Method:}
Previous methods for video-based HMR have not integrated diffusion models into their frameworks. While there are diffusion-based methods that can output 3D human pose or human mesh, they are not directly suitable for the video-based HMR task and require adaptations. We organize them into two intuitive baseline approaches. As depicted in Table \ref{tab:frame-comp}, approaches similar to \ul{Baseline 1}, such as \cite{SpatioTemporalDiffPose,foo2023distribution,gong2023diffpose}, require $f \times N$ denoising steps to produce output for $f$ frames. Conversely, methods similar to \ul{Baseline 2}, such as \cite{zhou2023diff3dhpe,shan2023diffusion,li2023ego}, reduce the total denoising steps from $f \times N$ to $N$. 
However, these methods do not effectively incorporate the consideration of human motion within the sequence during both the forward process and the reverse process, which may potentially result in temporal inconsistencies and non-smooth 3D motion predictions. 
In contrast, our approach treats motion patterns across frames as the inherent noise in the forward process, eliminating the need for adding Gaussian noise during forward steps as required in Baseline 1 and Baseline 2. This unique motion-aware design in diffusion enables our approach to decode human motion patterns during reverse processes within $N$ steps, setting it apart from conventional diffusion-based methods.

% In contrast, motion patterns across frames are viewed as the inherent noise in the forward process in our approach, while adding Gaussian noise during forward steps is required for Baseline 1 and Baseline 2. It uniquely accounts for human motion patterns during both the forward and reverse processes within $N$ steps, indicating the distinction of our diffusion-based approach. 

% %%%%%%
% VDT: An Empirical Study on Video Diffusion with Transformers
% Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation
% Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views
% DiffPose: SpatioTemporal Diffusion Model for Video-Based Human Pose Estimation

% CLIFF: Carrying Location Information in Full Frames into Human Pose and Shape Estimation

% Global-to-Local Modeling for Video-based 3D Human Pose and Shape Estimation
% Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video

% %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
\section{Methodology}
\subsection{Preliminary}
\label{Preliminary}
We first provide a brief overview of the original denoising diffusion probabilistic model \cite{ho2020denoising} (DDPM) scheme. As shown in Fig. \ref{fig:dm_tid} (a), the DDPM consists of two Markov chains, which are a diffusion process and a reverse process. The forward chain perturbs data to noise, while the reverse chain converts the noise back into data. For a more detailed understanding, we direct interested readers to the original paper  \cite{ho2020denoising} and recent surveys of the diffusion model \cite{diffsurvey1,diffsurvey2}.
 
\textbf{Forward Diffusion Process:} Let  $x_0  \sim p(x_0)$ where  $x_0$ is the training sample and $ p(x_0)$ be the data density. Gaussian transitions $\mathcal{N}$ are successively added to perturb the training sample $x_0$. A sequence of corrupted data $x_1, x_2, \dots, x_T$ can be obtained following the forward Markov chain:
\vspace{-5pt}
\begin{align}
\resizebox{0.9\linewidth}{!}{$
    p(x_t | x_{t-1} = \mathcal{N}(x_t; \sqrt{1-\beta_t} \cdot x_{t-1}, \beta_t \cdot \textbf{I}), \forall t \in \left\{ 1,\dots T \right\}
\vspace{-10pt}
$}
\end{align}
where $\beta_1 \dots \beta_T \in \left [0,1\right )$ are the hyperparameters for the variance schedule in each step, $T$ is the number of steps, and $\textbf{I}$ is the identity matrix. We use $\mathcal{N}(x;\mu,\sigma^2)$ to denote the normal distribution of mean $\mu$ and standard deviation $\sigma$ that produce $x$.  According to the property of normal distribution recursive formulation, we can directly sample $x_t$ when $t$ is drawn from a uniform distribution as follows: 
\vspace{-5pt}
\begin{align}
\small
    p(x_t | x_{0} = \mathcal{N}(x_t; \sqrt{\hat{\beta_t}} \cdot x_{0}, (1-\hat{\beta_t}) \cdot \textbf{I}), 
\vspace{-10pt}
\end{align}
where $\hat{\beta_t} = \prod_{i=1}^{t} \alpha_i $ and $\alpha_t = 1- \beta_t$.

Based on this equation, we can sample any $x_t$ directly when given original data $x_0$ and a fixed variance schedule $\beta_t$. To achieve this, the sample $x$ of a normal distribution $x \sim \mathcal{N}(\mu , \sigma^2 \cdot \textbf{I} )$  first subtracts the mean $\mu$ and divides by the standard deviation $\sigma$, outputting in a sample $z= \frac{x-\mu}{\sigma}$ of the standard normal distribution $z \sim \mathcal{N}(0, \textbf{I})$. Thus  $x_t$ is sampled from $p(x_t | x_0)$ as follows: 
\vspace{-5pt}
\begin{align}
\small
    x_t = \sqrt{\hat{\beta_t}} \cdot x_{0} + \sqrt{(1-\hat{\beta_t})} \cdot z_t, 
\vspace{-10pt}
\end{align}
where $z_t \sim \mathcal{N}(0,\textbf{I})$. 

\textbf{Reverse Denoising Process:} To generate new samples from $p(x_0)$, we apply the reverse Markov chain from a $x_T \sim \mathcal{N}(0, \textbf{I})$: 
\vspace{-5pt}
\begin{align}
\small
    p_{\theta}(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_{\theta}(x_t, t), \Sigma_{\theta}^{}(x_t, t) ), 
\label{eq:reverse}
\end{align}
where $\theta$ denotes model parameters, and the mean $\mu_{\theta}(x_t, t)$ and variance $\Sigma_{\theta}(x_t, t) $ are parameterized by deep neural networks $\epsilon_\theta$. 
\vspace{-5pt}
\begin{align}
\small
    x_{t-1} = \frac{1}{\sqrt{\beta_{t}}} (x_t - \frac{1-\beta_t}{\sqrt{1-\hat{\beta_t}}} \epsilon_\theta(x_t, t))+\sigma_t z, 
\label{eq:reverse_xt}
\vspace{-5pt}
\end{align}
If $t>1, z \sim \mathcal{N}(0,\textbf{I})$, else $z = 0$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Diffusion-based Baselines}
\label{Baseline}

\begin{figure}[htp]
\vspace{-10pt}
  \centering
  \includegraphics[width=0.95\linewidth]{figure/baseline.pdf}
  \vspace{-10pt}
  \caption{Two diffusion baselines for video-based HMR.}
  \label{fig:baseline}
  \vspace{-10pt}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[htp]
\vspace{-10pt}
  \centering
  \includegraphics[width=0.9\linewidth]{figure/arch.pdf}
  \vspace{-10pt}
  \caption{The architecture of DiffMesh: Our framework takes input sequence with $f$ frames, with the objective of outputting a human mesh sequence consisting of $f$ frames. We model the forward human motion across frames similar to the mechanism of introducing noise in the forward process. We assume that human motion will eventually reach a static state, which is represented by the mesh template state $x_{end}$. Thus, additional $N - f + 1$ steps are necessary from $x_f$ to reach the static state $x_{end}$. Consequently, we utilize a transformer-based diffusion model to sequentially produce the decoded features during the reverse process. The final human mesh sequence is returned by a mesh head using SMPL \cite{SMPL:2015} human body model. The structure of DN is illustrated in Fig. \ref{fig:md}. }
  \label{fig:arch}
  \vspace{-15pt}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%% 


% Drawing inspiration from the diffusion model's remarkable ability to bridge the gap between uncertain and determinate distributions,
% % during denoising processes
% we are naturally inclined to leverage it for video-based HMR.

Given that previous video-based HMR methods have not integrated diffusion models into their frameworks, we incline to explore the performance of simply incorporating diffusion models in the context of video-based HMR.

\textbf{Baseline 1:} As illustrated in Fig. \ref{fig:baseline} (a), we initially apply the vanilla diffusion model at the frame level. For each $i_{th}$ frame, we employ a backbone to extract the conditional feature $c_{i}$ where $i\in \left\{ 1,\dots, f \right\}$ and $f$ representing the total number of frames. We then proceed with the standard forward diffusion process, introducing noise to the ground truth mesh vertices $x_{i}$. After $N$ steps, the feature $x_{i}^{end}$ converges to a  Gaussian Noise. Subsequently, a transformer-based diffusion model  (details are included in the \textcolor{blue}{supplementary Sec. \ref{detail}}) is employed for the denoising process. The denoised feature  $x'_{i}$ is obtained after $N$ denoising steps. Finally, the human mesh of $i_{th}$ frame is returned by the human mesh head. By iterating this procedure $f$ times, we generate the final human mesh sequence consisting of $f$ frames. However, despite its simplicity and straightforwardness, Baseline 1 exhibits a significant drawback: the total number of steps is  $f \times N$, resulting in considerable computational overhead and slow inference speed. Furthermore, it fails to address motion smoothness within the diffusion model, potentially leading to temporal inconsistencies in mesh predictions.


\textbf{Baseline 2:} To mitigate computational costs and enhance inference speed, we introduce Baseline 2 for video-based HMR.  Motivated by recent works \cite{modiff,flame,li2023ego} that leverage diffusion models for motion generation, we adopt a similar concatenation and separation strategy, as illustrated in Fig. \ref{fig:baseline} (b). In this approach, we concatenate the ground-truth mesh vertices of each frame into a unified feature $x_{unified} = cat(x_1, x_2, \dots, x_f)$, where $f$ represents the total number of frames. Similarly, we create a concatenated conditional feature  $c_{unified}$ by combining the image features from each frame. Subsequently, we apply the forward diffusion process by introducing noise for $N$ steps, which is analogous to Baseline 1. After $N$ steps,  $x_{unified}^{end}$ also converges to a Gaussian Noise state. Following this, we perform $N$ denoising steps utilizing a transformer-based diffusion model (details are included in the \textcolor{blue}{supplementary  Sec. \ref{detail}}). The denoised feature $x'_{unified}$ is then partitioned into  $x'_1, x'_2, \dots, x'_f$. Finally, the mesh sequence is obtained through the mesh head.

Despite the substantial reduction in the number of steps, achieved by the concatenation and separation strategy from $f \times N$ to $N$, it is important to note that this strategy does not effectively consider motion patterns across frames due to simple concatenation, which may result in temporal inconsistencies and non-smooth 3D mesh predictions.


\subsection{DiffMesh}
\label{DiffMesh} 
To address the previously mentioned limitations in our baselines, we present DiffMesh, a novel solution for video-based HMR. The overall architecture is depicted in Fig. \ref{fig:arch}. Our framework takes input sequence with $f$ frames, with the objective of outputting a human mesh sequence consisting of $f$ frames. Additionally, we extract the conditional feature $c_i$ using the backbone, incorporating a conditional features generation block (more details are provided in the \textcolor{blue}{supplementary  Sec. \ref{detail}}). In contrast to Baseline 1 and Baseline 2, DiffMesh employs a distinct approach that accounts for the forward and backward motion within the diffusion model. 


\noindent \textbf{Forward Process:}
We make the assumption that human motion consistently influences the human mesh vertices (in the embedded high-dimensional space) across frames, denoted as $x_{i}$ to $x_{i+1}$,  where $ i \in \left\{ 1,\dots f-1 \right\}$. This motion pattern can be conceptualized as a specific noise introduced to the mesh vertices feature of the preceding frame: 
\vspace{-5pt}
\begin{align}
\small
    x_{i+1} = \sqrt{\beta_{i}} \cdot x_{i} + \sqrt{(1-\beta_{i})} \cdot \textcolor{orange}{m_{i}},
\label{eq:diffmesh_xi}
\vspace{-5pt}
\end{align}
where \textcolor{orange}{$m_{i}$} represents the motion pattern from $i_{th}$ frame to $(i+1)_{th}$ frame. Consequently,  $x_{1}$  can be gradually diffused into $x_{f}$ after  $f-1$ forward diffusion steps. Importantly, achieving $x_{f}$ does not denote the conclusion of the forward diffusion process. \textit{We assume that human motion will eventually reach a static state $x_{end}$,  which is an initial distribution (similar to the Gaussian noise in the conventional diffusion model)}.  In our work, we select the template mesh acquired by averaging ground-truth meshes \cite{lin2021metro} as the initial distribution. To arrive at this final static state $x_{end}$, DiffMesh requires  $N-f+1$ additional forward diffusion steps. Therefore, the total number of steps remains $N$. 


\noindent \textbf{Reverse Process:} Our objective during the reverse process is to obtain output features  $x'_{1} \cdots x'_{f}$. Initiating from the static state  $x_{end}$, we design a transformer-based diffusion model, as depicted in Fig. \ref{fig:arch}, to sequentially produce decoded features. After  $N-f+1$ steps, our diffusion model yields $x'_{f}$, which serves as the target feature for the final frame $f$. Iteratively, we generate the output features $x'_{i}$ where $ i \in \left\{ f-1,\dots 1 \right\}$ over $f-1$ steps. The total number of steps remains $N$. In contrast to Baseline 2, DiffMesh demonstrates the capability to decode specific motion patterns within the same number of steps, while generating more precise and smooth output (verified in Sec. \ref{ablation_study}).

\noindent\textbf{Network Design:} In contrast to conventional diffusion-based methods \cite{ho2020denoising,rombach2022high,peebles2023scalable} for image synthesis tasks that often rely on UNet \cite{unet} or vanilla transformer \cite{vaswani2017attention} backbones, our approach introduces a two-stream transformer design 
% specific for video-based HMR. This architecture is 
deeply integrated within our framework, as illustrated in Fig. \ref{fig:md}. While conventional diffusion-based methods \cite{ho2020denoising,rombach2022high,peebles2023scalable} only estimate noise during steps, our two-stream network predicts the motion features and the previous conditional feature during each step as depicted in Fig. \ref{fig:md} (c). 
% In addition to the estimation of motion \textcolor{orange}{$m_{i-1}$}, our network also predicts the previous conditional feature $\hat{c}_{i-1}$ during each step as depicted in Fig. \ref{fig:md} (c). 
This architecture significantly enhances information integration through a two-stream process. Initially, it captures mesh and condition dependencies separately using two self-attention blocks. Then, these captured dependencies are merged by a cross-attention block efficiently to decode motion features. An \textbf{auxiliary loss} is computed by measuring the difference between the predicted $\hat{c}_{i-1}$ and the ground truth $c_{i-1}$, which contributes to training a better transformer-based network during the reverse process.
% The $\rm FFN(\cdot)$ is a feed-forward network consisting of the multilayer perceptron (MLP) and normalization layer. 

% For the self-attention block in Fig. \ref{fig:md} (b), we adopt the standard self-attention architecture as in ViT \cite{Dosovitskiy2020ViT}. To effectively integrate information from the input feature $x'_{i}$ and the condition feature $c_{i}$, we perform cross-attention between them as illustrated in Fig. \ref{fig:md} (c). The $\rm FFN(\cdot)$ is a feed-forward network consisting of the multilayer perceptron (MLP) and normalization layer. 
% In contrast to diffusion-based methods employing UNet \cite{unet} or vanilla transformer \cite{vaswani2017attention} to estimate the noise term at each step, \ul{we propose a two-stream transformer-based diffusion model for video-based HMR}, as depicted in Fig. \ref{fig:md}. Our model produces the predicted motion \textcolor{orange}{$m_{i-1}$} as well as the predicted previous conditional feature $\hat{c}_{i-1}$, given the input feature $x'_{i}$ and corresponding conditional feature $c_{i}$ at each step. 
Given the input feature $x'_{i}$ and corresponding conditional feature $c_{i}$ at each step, our network produces the predicted motion \textcolor{orange}{$m_{i-1}$} as well as the predicted previous conditional feature $\hat{c}_{i-1}$. The feature of static state (mesh template) is denoted as \textcolor{orange}{$x_{end}$}. Similar to Equation \ref{eq:reverse_xt},  the mesh features $x'_{i-1}$ in DiffMesh can be computed as 
\vspace{-10pt}
\begin{align}
\small
    x'_{i-1} = \frac{1}{\sqrt{\beta_{i}}} (x'_i - \frac{1-\beta_i}{\sqrt{\hat{1-\beta_i}}} \textcolor{orange}{m_{i-1}} )+\sigma_t \textcolor{orange}{x_{end}},
\label{eq:diffmesh_xt}
\end{align}
\vspace{-10pt}
% \noindent 

% For the self-attention block in Fig. \ref{fig:md} (b), we adopt the standard self-attention architecture as in ViT \cite{Dosovitskiy2020ViT}. To effectively integrate information from the input feature $x'_{i}$ and the condition feature $c_{i}$, we perform cross-attention between them as illustrated in Fig. \ref{fig:md} (c). The $\rm FFN(\cdot)$ is a feed-forward network consisting of the multilayer perceptron (MLP) and normalization layer.


% An \textbf{auxiliary loss} is computed by measuring the difference between the predicted $\hat{c}_{i-1}$ and the ground truth $c_{i-1}$. This auxiliary loss contributes to training a better transformer-based model during the reverse process.


%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[tp]
\vspace{-10pt}
  \centering
  \includegraphics[width=0.85\linewidth]{figure/motiondecoder.pdf}
  \vspace{-5pt}
  \caption{(a) The architecture of our two-stream transformer network. Given the input feature $x'_i$ and corresponding conditional feature $c_i$, the diffusion model generates the predicted motion $m_{i-1}$ and the predicted previous conditional feature $\hat{c}_{i-1}$. (b) The self-attention block in our diffusion model. (c) The cross-attention block in our diffusion model.  The notation $\oplus$ represents element-wise addition. }
  \label{fig:md}
  \vspace{-15pt}
\end{figure}
%%%%%%%%%%%

Through our design, we efficiently acquire the output features for each frame. In comparison to Baseline 2, we maintain a total of $N$ steps while explicitly considering motion patterns during both the forward process and the reverse process. This approach results in improved accuracy and motion smoothness in the estimated mesh sequence, achieved through successive steps across frames.

%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[htp]
% \tiny
% \scriptsize
\renewcommand\arraystretch{1.1}
% \vspace{-5pt}
\centering
  \resizebox{0.95\linewidth}{!}
  {
\begin{tabular}{l|c|c|ccc|ccc}
\hline
\multicolumn{1}{c|}{}                   &           &                  & \multicolumn{3}{c|}{Human3.6M} & \multicolumn{3}{c}{3DPW} \\ \hline
\multicolumn{1}{c|}{Video-based Method} & \multicolumn{1}{c|}{Venue}     & Number of frames & MPVE $\downarrow$    & MPJPE $\downarrow$  & PA-MPJPE $\downarrow$   & MPVE $\downarrow$  & MPJPE $\downarrow$ & PA-MPJPE $\downarrow$ \\ \hline
VIBE \cite{kocabas2020vibe}                                    & CVPR 2020 & 16               & -       & 65.6    & 41.4       & 99.1  & 82.9  & 51.9     \\
TCMR \cite{tcmr}                                   & CVPR 2021 & 16               & -       & 62.3    & 41.1       & 102.9 & 86.5  & 52.7     \\
MAED \cite{MAED}                                   & ICCV 2021 & 16               & 84.1    & 60.4    & 38.3       & 93.3  & 79.0  & \underline{45.7}     \\
MPS-Net \cite{MPS-Net}                                & CVPR 2022 & 16               & -       & 69.4    & 47.4       & 99.7  & 84.3  & 52.1     \\
SmoothNet \cite{smoothnet}                              & ECCV 2022 & 32               & -       & 67.5    & 46.3       & -     & 86.7  & 52.7     \\
GLoT \cite{GLoT}                                   & CVPR 2023 & 16               & -       & 67.0      & 46.3       & 96.3  & 80.7  & 50.6     \\
% PMCE \cite{you2023coeval}                             & ICCV 2023 & 16               & -    & 53.5    & 37.7       & 84.8  & 69.5  & 46.7     \\
MotionBERT \cite{MotionBERT2022}                             & ICCV 2023 & 16               & \underline{65.5}    & \underline{53.8}    & \underline{34.9}       & \underline{88.1}  & \underline{76.9}  & 47.2     \\
\rowcolor{RowColor} DiffMesh (Ours)                               &           & 16               &   \textbf{64.2}      &  \textbf{52.5}   & \textbf{33.5}      & \textbf{86.4}  & \textbf{75.7}  & \textbf{45.6}     \\ \hline
\textit{\textbf{With Refinement}}                         &           &                  &         &         &            &       &       &          \\ \hline
DND \cite{li2022dnd} with HybrIK \cite{li2021hybrik}                        & ECCV 2022 & 32               &    -     & 52.5    & 35.5       & 88.6  & 73.7  & 42.7     \\
MotionBERT \cite{MotionBERT2022} with HybrIK \cite{li2021hybrik}                  & ICCV 2023 & 16               & \underline{52.6}    & \underline{43.1}    & \underline{27.8}       & \underline{79.4}  & \underline{68.8}  & \underline{40.6}     \\
\rowcolor{RowColor} DiffMesh (Ours) with HybrIK \cite{li2021hybrik}                   &           & 16               &   \textbf{52.1}      & \textbf{42.4}    & \textbf{27.7}       & \textbf{78.4}  & \textbf{67.8}  & \textbf{40.1}     \\ \hline
\end{tabular}
}
  \vspace{-5pt}
  \caption{Performance comparison with SOTA video-based methods on Human3.6M and 3DPW datasets. 
  % The symbol “-” denotes results are not available. 
  3DPW Training set is used during training. The best results are marked as bold and the second-best results are marked as underlined. }
\label{tab: alldataset}
\vspace{-5pt}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Loss Functions}
During training, we supervise the SMPL parameters ($\Theta_i$ and $\beta_i$) where $i \in \left\{ 1,\dots f \right\}$ with the ground-truth SMPL parameters of each frame using the $L2$ loss. Following \cite{kocabas2020vibe,tcmr}, the 3D joint coordinates are regressed by forwarding the SMPL parameters to the SMPL human body model. 
% The 2D joint coordinates are also obtained by projecting the 3D joint coordinates to 2D using the camera parameters. 
Furthermore, as mentioned in Sec. \ref{DiffMesh}, Mean Squared Error (MSE) loss is applied between the conditional feature $c_{i}$ and the predicted conditional features $\hat{c_{i}}$.  
% This loss aids in the training of a more effective transformer-based diffusion model.
More detailed information is included in the \textcolor{blue}{supplementary  Sec. \ref{detail}}.

\section{Experiments}

\subsection{Implementation Details}
\label{imp_detail}

For fair comparisons, we adhere to the standard implementation details commonly employed in the video-based HMR task, consistent with prior works such as \cite{kocabas2020vibe, tcmr, MPS-Net, MotionBERT2022}. We set the length of the input sequence $f$ to 16. The number of steps $N=30$. We implemented our approach using PyTorch \cite{PyTorch} on a single NVIDIA A5000 GPU. The optimization of weights is conducted using the Adam optimizer \cite{kingma2014adam}, with an initial learning rate of $5e^{-6}$ and weight decay set to 0.98. During the training phase, we utilize a batch size of 64, and the training process spans 60 epochs. More detailed information about the conditional feature generation part is included in the \textcolor{blue}{supplementary  Sec. \ref{detail}}.

\subsection{Datasets and Evaluation Metrics}

\textbf{Datasets:} Following the previous works \cite{kocabas2020vibe,tcmr,MPS-Net}, a mixed 2D and 3D datasets are used for training. PoseTrack \cite{posetrack} and InstaVariety \cite{hmmr} are 2D datasets where 2D ground-truth annotations are provided for PoseTrack \cite{posetrack}, while pseudo ground-truth 2D annotations are generated from \cite{openpose} for InstaVariety \cite{hmmr}.  3DPW \cite{pw3d2018}, Human3.6M \cite{h36m_pami}, and AMASS \cite{AMASS2019} are 3D datasets for training. 
% Among them, ground-truth SMPL parameters are available for 3DPW and AMASS. 
% For the evaluation, Human3.6M, and 3DPW are selected for the performance comparison. 


\textbf{Evaluation Metrics:} The HMR performance is evaluated by four standard metrics: Mean Per Joint Position Error (MPJPE), Procrustes-Aligned MPJPE (PA-MPJPE), Mean Per Vertex Position Error (MPVE), and Acceleration Error (ACC-ERR). Particularly, MPJPE, PA-MPJPE, and MPVE indicate the accuracy of the estimated 3D human pose and shape measured in millimeters (mm). ACC-ERR is proposed in HMMR \cite{hmmr} for evaluating temporal smoothness, which computes the average difference between the predicted and ground-truth acceleration of each joint in ($mm/s^2$). 

\subsection{Comparison with State-of-the-art Methods}
We compare our DiffMesh with previous SOTA video-based methods on Human3.6M and 3DPW datasets (results on MPI-INF-3DHP \cite{mpi3dhp2017} are provided in the \textcolor{blue}{supplementary  Sec. \ref{exp}}). Among these datasets, Human3.6M is an indoor dataset while 3DPW contains complex outdoor scenes. Following \cite{tcmr,MPS-Net}, 3DPW training sets are involved in training. The results are shown in Table \ref{tab: alldataset}. \textit{Our DiffMesh outperforms all previous methods across all evaluation metrics on the Human3.6M and 3DPW datasets}. This superior performance validates the efficacy of our DiffMesh framework for recovering high-quality human mesh structures from video input. Specifically, without involving an additional refinement procedure, our DiffMesh surpasses the second-best method MotionBERT \cite{MotionBERT2022} (which already exhibits significant improvements over its predecessors) by more than 1 mm of MPJPE, PA-MPJPE, and MPVE. When incorporating HybrIK \cite{li2021hybrik} as an additional refinement procedure, our DiffMesh consistently surpasses MotionBERT \cite{MotionBERT2022} with refinement, which significantly enhances the performance of video-based HMR. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[tp]
% \tiny
% \scriptsize
\renewcommand\arraystretch{1.1}
\vspace{-5pt}
\centering
  \resizebox{0.98\linewidth}{!}
  {
  % Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{tabular}{c|cc|ccc}
\hline
                             &    &           & \multicolumn{3}{c}{3DPW} \\ \hline
                             & Method  & Year   & Input frames    & MPVE  $\downarrow$ & ACC-ERR  $\downarrow$ \\ \hline
\multirow{3}{*}{Image-based} & SPIN \cite{Kolotouros2019SPIN}  &2019   & 1  & 116.4  & 29.8    \\
                             & HybrIK \cite{li2021hybrik} &2021 & 1   & 89.1  & 22.8    \\ 
                             & POTTER \cite{zheng2023potter} &2023 & 1   & 87.4  & 23.1     \\  \hline
                             
\multirow{5}{*}{Video-based} & TCMR  \cite{tcmr}  &2021  & 16  & 102.9  & 7.1     \\
                             & MPS-Net \cite{MPS-Net} &2022 &  16  & 99.7  & 7.4     \\
                             & GLoT \cite{GLoT} &2023   &  16  & 96.3  & \underline{6.6}     \\
                             & DND \cite{li2022dnd} &2022    &  32  & \underline{88.6}  & 7.0     \\
                             & \cellcolor{RowColor} DiffMesh (Ours)  &    & \cellcolor{RowColor}16   & \cellcolor{RowColor}\textbf{86.4}  & \cellcolor{RowColor}\textbf{6.1}  \\\hline      
\end{tabular}
}
\vspace{-5pt}
\caption{Performance comparison with image-based methods and video-based methods on 3DPW dataset. }
\label{tab:acc}
\vspace{-15pt}
\end{table}



%%%%%%%%%%%%%%%%%%%%

To further validate the enhancement in temporal consistency achieved by DiffMesh, we conducted an evaluation of acceleration error as shown in Table \ref{tab:acc}. While some image-based methods can achieve impressive performance in terms of accuracy (low MPVE and MPJPE), they often suffer from motion jitter and temporal inconsistency issues. For instance, we observed that methods like HybrIK \cite{li2021hybrik} and POTTER \cite{zheng2023potter} exhibit lower MPVE and MPJPE, but concurrently, their ACC-ERR are notably higher compared to video-based HMR methods such as TCMR \cite{tcmr}, MPS-Net \cite{MPS-Net}, and GLoT \cite{GLoT}. Instead, our DiffMesh, which integrates motion patterns into the diffusion model and enhances temporal consistency through successive steps across frames, manifests in its superior performance. We further reduce the ACC-ERR from 6.6 to 6.1, which is about 7.6\% reduction over the previous best ACC-ERR from GLoT \cite{GLoT}. These results underscore the practical viability of DiffMesh for real-world applications, particularly in scenarios requiring the generation of accurate and smooth human mesh sequences.


%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[htp]
% \tiny
% \scriptsize
\renewcommand\arraystretch{1.1}
% \vspace{-5pt}
\centering
  \resizebox{0.9\linewidth}{!}
  {
\begin{tabular}{c|cc|ccc|cc}
\hline
           & \begin{tabular}[c]{@{}c@{}}\# of input \\ frames\end{tabular} & \begin{tabular}[c]{@{}c@{}}\# of output \\ frames\end{tabular}  & MPVE $\downarrow$ & MPJPE $\downarrow$ & ACC-ERR  $\downarrow$ & \cellcolor[HTML]{FFFFFF}\begin{tabular}[c]{@{}c@{}}Total processing \\ time\end{tabular} & \cellcolor[HTML]{FFFFFF}\begin{tabular}[c]{@{}c@{}}Average time \\ per frame\end{tabular} \\ \hline
HybrIK \cite{hybrik}       & 1                                                            & 1                                                         & 89.7  & 76.2  & 29.8     &  38.9 ms                                                                                  & 38.9 ms/frame                                                                                 \\
MPS-Net \cite{MPS-Net}     & 16                                                             & 1                                                           & 99.7  & 84.3     & 7.4     & 31.1 ms                                                                                  & 31.1 ms/frame                                                                                 \\
DND \cite{li2022dnd}       & 32                                                            & 32                                                           & 88.6  & \textbf{73.7}      & \underline{7.0}     &  2697.6 ms   &   84.3 ms/frame \\
Baseline 1 & 16                                                            & 16   & \underline{87.2}             &  77.6         &   16.5      &  2168.4 ms                                                                                        & 135.5 ms/frame                                                                                              \\
Baseline 2 & 16                                                            & 16          &  89.0         &    76.5                      &   7.9      &   224.7 ms                                                                                         & 14.0 ms/frame                                                                                           \\ \hline
\rowcolor{RowColor} DiffMesh (Ours)  & 16                                                            & 16       & \textbf{86.4} & \underline{75.7}   &  \textbf{6.1}       &    223.1 ms           & \textbf{13.9 ms/frame}                                                                                            \\ \hline
\end{tabular}
}
\vspace{-5pt}
\caption{Reconstruction performance and inference time comparison on 3DPW dataset between our DiffMesh and previous video-based HMR methods with the same hardware platform (a single NVIDIA A5000 GPU with a batch size of 1 for fair comparison). }
\label{tab:ab_fps}
\vspace{-10pt}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Ablation Study}
\label{ablation_study}
% We conduct the ablation study with the same setting as in Table \ref{tab: alldataset}. 
We conduct the ablation study on 3DPW \cite{pw3d2018} dataset since it provides ground-truth SMPL parameters. All implementation details are the same as in Section \ref{imp_detail}.

\textbf{Comparison with the Baselines:} In Table \ref{tab:ab_fps}, we present a performance comparison among Baselines (in Section \ref{Baseline}) and our DiffMesh. Baseline 1 applies the diffusion model frame by frame, achieving better MPVE and PA-MPJPE results than Baseline 2. However, it falls short in terms of ACC-ERR. When adopting the concatenation and separation strategy in Baseline 2, the ACC-ERR is improved, but with a slight downgrade in MPVE and PA-MPJPE. Neither Baseline 1 nor Baseline 2 surpass the previous method DND \cite{li2022dnd}. In contrast, our DiffMesh outperforms both Baseline 1 and Baseline 2 across all evaluation metrics. Furthermore, DiffMesh demonstrates a 12.8\% reduction in ACC-ERR compared to the SOTA DND \cite{li2022dnd} even with fewer input frames (16 vs. 32). We further visualize the comparison of the ACC-ERR in Fig. \ref{fig:ab_acc} (``downtown\_bar\_00\_0'' of 3DPW test-set). The ACC-ERR for DND \cite{li2022dnd} is unavailable in Fig. \ref{fig:ab_acc} as their code has not been fully released. This highlights the superiority of DiffMesh, which leverages the consideration of human motion patterns within the diffusion model, resulting in enhanced overall performance, especially in terms of motion smoothness.

%%%%%%%%%%%%%%%%
\begin{figure}[htp]
\vspace{-5pt}
  \centering
  \includegraphics[width=0.9\linewidth]{figure/accel.pdf}
  \vspace{-10pt}
  \caption{Comparison of the acceleration errors for MPS-Net \cite{MPS-Net}, two baselines, and our DiffMesh. }
  \label{fig:ab_acc}
  \vspace{-10pt}
\end{figure}
%%%%%%%%%%%

% \begin{table}[htp]
% % \tiny
% % \scriptsize
% \renewcommand\arraystretch{1.1}
% % \vspace{-5pt}
% \centering
%   \caption{Ablation study of Baseline 1, Baseline 2, and our DiffMesh on 3DPW dataset. }
%   \vspace{-5pt}
%   \resizebox{0.98\linewidth}{!}
%   {
% \begin{tabular}{c|cccc}
% \hline
%            & \multicolumn{4}{c}{3DPW}                                                                                          \\ \hline
%            & MPVE  $\downarrow$ & MPJPE  $\downarrow$ & PA-MPJPE  $\downarrow$ & ACC-ERR  $\downarrow$ \\ \hline
% Baseline 1 &  87.2             &  77.6        &   45.9                  &    16.5                         \\ \hline
% Baseline 2 &   89.0         &    76.5            &   46.9                   &    7.9                         \\ \hline
% \rowcolor{RowColor} DiffMesh   & \textbf{86.4} & \textbf{75.7}  & \textbf{45.6}     &   \textbf{6.1}                          \\ \hline
% \end{tabular}
% }
% \label{tab:ab_baseline}
% \vspace{-5pt}
% \end{table}



\textbf{Inference Time Analysis:} For video-based HMR, inference speed is indeed a crucial evaluation metric. While methods like TCMR \cite{tcmr} and MPS-Net \cite{MPS-Net} employ video input to enhance motion smoothness and temporal consistency, they often face limitations in terms of processing speed. For instance, when provided with a sequence of frames (e.g., 16 frames) as input, MPS-Net \cite{MPS-Net} can only generate the human mesh for one frame in Table \ref{tab:ab_fps}. The average processing time of these methods exceeds 30 ms/frame, indicating a need for further improvements in computational efficiency.  Our Baseline 1 also requires a long processing time to output high-quality human mesh. Our Baseline 2 reduced the processing time but the acceleration error is still worse than DND \cite{li2022dnd}.  

Compared to these methods, 
% our DiffMesh offers significant improvements in motion smoothness and temporal consistency without compromising on inference speed. 
DiffMesh not only achieves the best mesh recovery performance and motion smoothness, while maintaining a competitive processing time. The average time can achieve 13.9 ms/frame, which is much faster than previous SOTA methods. This combination of accuracy and efficiency makes DiffMesh a promising solution for video-based human mesh recovery applications.

\textbf{Number of steps in diffusion:} Following previous video-based HMR methods \cite{kocabas2020vibe,tcmr,MPS-Net,GLoT}, our DiffMesh operates with an input sequence of $f=16$ frames. As introduced in Section \ref{DiffMesh}, we assume that human motion will reach an initial distribution (we use the mesh template \cite{lin2021metro} as this static state) after additional steps as denoted in Fig. \ref{fig:arch}. The role of initial distribution is similar to the Gaussian noise in conventional diffusion models. To verify the impact of these additional steps, we conducted experiments as shown in Table \ref{tab:steps}. When introducing additional steps, the reverse motion process starts from the initial distribution (this static state is represented by the template mesh \cite{lin2021metro}). The results demonstrate that introducing additional steps can enhance overall performance.  The optimal balance between efficiency and performance is achieved when the number of additional steps is set to 15 (the total number of steps $N$ is 30). 


\begin{table}[htp]
% \tiny
% \scriptsize
% \renewcommand\arraystretch{1.2}
\vspace{-5pt}
\centering
  \resizebox{1\linewidth}{!}
  {
\begin{tabular}{c|cc|c|ccc}
\hline
input frames & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}steps for output \\ sequence\end{tabular}} & additional steps & Total steps & MPVE $\downarrow$ & MPJPE $\downarrow$ & ACC-ERR $\downarrow$ \\ \hline
16      & 15     & 0            & 15          &  88.5    &  77.4     &    6.5     \\
16      & 15     & 5            & 20          &  88.0    &  77.1     &  6.5  \\  
16     & 15      & 15           & 30          &  \textbf{86.4}    & \underline{75.7}      &   \textbf{6.1}  \\
16    & 15       & 25            & 40          &  \underline{87.1}    &  \textbf{75.6}     &   \underline{6.2}  \\ \hline
\end{tabular}
}
\vspace{-10pt}
\caption{Performance with the different number of steps on 3DPW.}
\label{tab:steps}
\vspace{-10pt}
\end{table}


\noindent Due to space limitation, \textbf{more ablation studies are available in the \textcolor{blue}{supplementary Sec. \ref{exp}}}, {covering topics such as the number of input frames, the two-stream transformer network (Fig. \ref{fig:md}), and the auxiliary loss.}   
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%
\begin{figure}[htp]
\vspace{-5pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/vis_new.pdf}
  \vspace{-15pt}
  \caption{The in-the-wild visual comparison between recent GLoT \cite{GLoT} with our DiffMesh. The circles highlight locations where DiffMesh is more accurate than GLoT. More examples are provided in the \textcolor{blue}{supplementary  Sec. \ref{meshvis} and in demo videos}. }
  \label{fig:ab_vis}
  \vspace{-15pt}
\end{figure}
%%%%%%%%%%%

\subsection{In-the-Wild Visualization}
To demonstrate the practical performance of our DiffMesh, we present the qualitative comparison between DiffMesh and a recent GLoT \cite{GLoT} on in-the-wild video in Fig. \ref{fig:ab_vis}. Although achieving excellent performance in terms of mesh recovery performance on common datasets, GLoT \cite{GLoT} suffers from temporal inconsistency when applied for in-the-wild video. 
% While TCMR \cite{tcmr} and MPS-Net \cite{MPS-Net} switch to a many-to-one approach to tackle motion smoothness and temporal consistency, these methods also have their limitations 
% Instead, our DiffMesh enhances motion smoothness and reconstruction accuracy through our proposed diffusion-based framework. 
As shown in Fig. \ref{fig:ab_vis}, GLoT \cite{GLoT} fails to generate the accurate mesh. In contrast, our DiffMesh enhances motion smoothness and reconstruction accuracy through our proposed motion-aware diffusion-like framework. %More qualitative results and video demos are provided in the \textcolor{blue}{supplementary  Sec. 4} and anonymous page \url{https://anonymous.4open.science/r/DiffMesh}. 


\section{Conclusion}
In this paper, we present a novel diffusion-like framework (DiffMesh) for human mesh recovery from a video. DiffMesh innovatively connects diffusion models with human motion, resulting in the efficient generation of highly precise and seamlessly smooth output mesh sequences. Extensive experiments are conducted on Human3.6M and 3DPW datasets showing impressive performance compared to the SOTA video-based methods. 

While DiffMesh demonstrates its superior performance for in-the-wild video input, like previous methods, it may produce unrealistic mesh outputs in scenarios with significant occlusions. Our future research will focus on exploring spatial-temporal interactions within the human body to mitigate this occlusion challenge.
%-------------------------------------------------------------------------


% %%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%


\newpage
\appendix

\noindent \textbf{\large{Supplementary Material}}

\section{Overview}

The supplementary material is organized into the following sections:

%\setlist{nolistsep}
\begin{itemize}%[noitemsep,leftmargin=*] 
\item Section \ref{detail}: More Related Work and Implementation Details.

\item Section \ref{exp}: More Experiments about the number of input frames, the two-stream transformer network, initial distributions, and the auxiliary loss. 

\item Section \ref{meshvis}: More Human Mesh Visualization.
\end{itemize}

% \section{Broader Impact }
% \label{Broader}
% Our proposed DiffMesh framework for video-based HMR has the potential for widespread applications, including motion capture in animation and movies, virtual AI assistants, and VR/AR content. Currently, these applications may still require expensive and time-consuming motion capture devices, which can be complicated to set up. However, DiffMesh is easy to deploy without any additional devices. As a many-to-many approach, DiffMesh can reliably reconstruct human meshes from videos, even for online streaming. DiffMesh shows great potential as a causal video-based HMR model for real-world applications.
  

% Despite its strengths, DiffMesh has some limitations. While it can estimate reliable human meshes for in-the-wild scenarios, its performance may be impacted by severe occlusions, especially in crowded scenes. Additionally, DiffMesh uses a fixed backbone (ResNet50\cite{resnet}) following \cite{tcmr,kocabas2020vibe,MPS-Net} for a fair comparison, but we plan to improve its performance by applying a new backbone that weight can be updated during training in future work.


\section{More Related Work and Implementation Details}
\label{detail}
\subsection{Related Work}
The majority of methods  ~\cite{kanazawaHMR18,Kolotouros2019SPIN,Choi_2020_ECCV_Pose2Mesh,gtrs,zheng2023feater,you2023gator,zheng2023potter,li2023niki} for HMR rely on a parametric human model, such as SMPL \cite{SMPL:2015}, to reconstruct the mesh by estimating pose and shape parameters. As a fundamental HMR work, SPIN ~\cite{Kolotouros2019SPIN} combines regression and optimization in a loop, where the regressed output serves as better initialization for optimization (SMPLify). METRO \cite{lin2021metro} is the first transformer-based method that models vertex-vertex and vertex-joint interaction using a transformer encoder after extracting image features with a CNN backbone. HybrIK \cite{li2021hybrik} and HybrIK-X \cite{li2023hybrik} present novel hybrid inverse kinematics approaches that transform 3D joints to body-part rotations via twist-and-swing decomposition. Lin et al. \cite{lin2023one} propose a one-stage pipeline for 3D whole-body (body, hands, and face) mesh recovery. Foo et al. \cite{foo2023distribution} first introduce a diffusion-based approach for recovering human mesh from a single image. The recovered human mesh is obtained by the reverse diffusion process. However, when applied to video sequences, these image-based methods suffer from severe motion jitter due to frame-by-frame reconstruction, making them unsuitable for practical use.

Compared to image-based HMR methods, video-based methods \cite{zeng2022deciwatch,smoothnet,you2023coeval} utilize temporal information to enhance motion smoothness from video input. In addition to the methods \cite{kocabas2020vibe,AMASS2019,meva,tcmr,MPS-Net,MotionBERT2022} introduced in the main paper, there are several other noteworthy approaches for video-based HMR.  Kanazawa et al.\cite{hmmr} first propose a convolutional network to learn human motion kinematics by predicting past, current, and future frames. Based on \cite{hmmr}, Sun et al. \cite{sun2019human} further propose a self-attention-based temporal model to improve performance. DND \cite{li2022dnd} utilizes inertial forces control as a physical constraint to reconstruct 3D human motion. GLoT \cite{GLoT} adopts a novel approach by decoupling the modeling of short-term and long-term dependencies using a global-to-local transformer. PMCE \cite{you2023coeval} follows a two-step process, where it first estimates 3D human pose and then regresses the mesh vertices through a co-evaluation decoder that takes into account the interactions between pose and mesh.






\subsection{Datasets}
\noindent \textbf{3DPW} \cite{pw3d2018} is a dataset that captures outdoor and in-the-wild scenes using a hand-held camera and a set of inertial measurement unit (IMU) sensors attached to body limbs. The ground-truth SMPL parameters are computed based on the returned values. This dataset includes 60 videos of varying lengths, and we use the official split to train and test the model. The split comprises 24, 12, and 24 videos for the training, validation, and test sets, respectively. The MPJPE, PA-MPJPE, MPJVE, and ACC-ERR are reported when evaluating this dataset. 


\noindent \textbf{Human3.6M} \cite{h36m_pami} is a large-scale benchmark for the indoor 3D human pose. It includes 15 action categories and 3.6M video frames. Following \cite{kocabas2020vibe,tcmr,MPS-Net}, we use five subjects (S1, S5, S6, S7, S8) for the training set and two subjects (S9, S11) for the testing set. The dataset is subsampled from its original 50 fps to 25 fps for both training and evaluation purposes. When calculating MPJPE and PA-MPJPE, only 14 joints are selected for a fair comparison to the previous works.


\noindent \textbf{MPI-INF-3DHP} \cite{mpi3dhp2017} is a 3D benchmark that consists of both indoor and outdoor environments. The training set includes 8 subjects, with each subject having 16 videos, resulting in a total of 1.3M video frames captured at 25 fps. The markerless motion capture system is used for providing 3D human pose annotations. The test set comprises 6 subjects performing 7 actions in both indoor and outdoor environments. Following \cite{kocabas2020vibe,tcmr,MPS-Net}, the MPJPE and PA-MPJPE are measured on valid frames, which include approximately every 10th frame, using 17 joints defined by MPI-INF3DHP. The ACC-ERR is computed using all frames.

\noindent \textbf{InstaVariety} \cite{hmmr} is a 2D human dataset curated by HMMR \cite{hmmr} , comprising videos collected from Instagram using 84 motion-related hashtags. The dataset contains 28K videos with an average length of 6 seconds, and pseudo-ground truth 2D pose annotations are acquired using OpenPose\cite{openpose}.



\noindent \textbf{PoseTrack} \cite{posetrack} is a 2D benchmark designed for multi-person pose estimation and tracking in videos. This dataset comprises 1.3K videos and 46K annotated frames in total, captured at varying fps around 25 fps. There are 792 videos used for the official train set, which includes 2D pose annotations for 30 frames located in the middle of each video.



\subsection{Loss Function}
% In our DiffMesh, the SMPL model \cite{SMPL:2015} is utilized for reconstructing human mesh. Given the predicted pose parameters $\theta$ and the shape parameters $\beta$, the SMPL model can return the body mesh $M \in  \mathbb{R}^{N \times 3}  $ with $N=6890$ vertices by the function $M = SMPL (\theta, \beta) $. After obtaining the body mesh $M$, the body joints $J$ can be regressed by the predefined joint regression matrix $W$, which means   $J \in  \mathbb{R}^{k \times 3} = W \cdot M  $, where $k$ is the number of joints. We follow the same setting for the loss function. 
% The overall loss during the HMR task can be defined as: 


Our DiffMesh relies on the SMPL model \cite{SMPL:2015} to reconstruct the human mesh. The SMPL model can generate the body mesh $M \in \mathbb{R}^{N \times 3} $ with $N=6890$ vertices by taking in the predicted pose parameters $\theta$ and the shape parameters $\beta$ as inputs, which can be expressed as $M = SMPL (\theta, \beta) $. Once the body mesh $M$ is obtained, the body joints $J$ can be estimated by applying the predefined joint regression matrix $W$, i.e., $J \in \mathbb{R}^{k \times 3} = W \cdot M$, where $k$ represents the number of joints. We adopt the same loss function as previous methods TCMR \cite{tcmr}.

\begin{equation}
\begin{aligned} 
  \begin{aligned}
\mathcal{L}_{HMR} &=  w_1 \| \beta - \beta^*  \| + w_2 \| \theta - \theta^*  \| + w_3 \| J - J^*  \|\\
  \end{aligned}\\
\end{aligned}
\end{equation}
\noindent where * denote the ground-truth value,  $w_1=0.5$, $w_2=10$, and $w_3=1000$.

Besides this general loss for mesh recovery, we add additional auxiliary loss as mentioned in Section 3.4 of the main paper. Our designed transformer-based diffusion model can predict the previous conditional feature $\hat{c}_{i-1}$ given the current conditional feature input  $c_{i}$. A MSE loss is applied between the ground truth $c_{i-1}$ and predicted $\hat{c}_{i-1}$:
\begin{equation}
\begin{aligned} 
  \begin{aligned}
\mathcal{L}_{aux} &=  \| c_{i-1} - \hat{c}_{i-1} \|_2^2\\
  \end{aligned}\\
\end{aligned}
\end{equation} 
This auxiliary loss contributes to the refinement of our transformer-based diffusion model during the training process. Thus, the overall loss for our DiffMesh is the sum of the $\mathcal{L}_{HMR} $ and $\mathcal{L}_{aux}$:
\begin{equation}
\begin{aligned} 
  \begin{aligned}
\mathcal{L}_{overall} &= \mathcal{L}_{TCMR} +   w_4 \mathcal{L}_{aux}\\
  \end{aligned}\\
\end{aligned}
\end{equation} 
\noindent where $w_4=0.01$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[htp]
\vspace{-5pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/supp-diffmodel.pdf}
  \vspace{-10pt}
  \caption{The diffusion model in our baselines}
  \label{fig:supp_diffmodel}
  \vspace{-10pt}
\end{figure}

\begin{figure*}[htp]
\vspace{-5pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/supp-transformer.pdf}
  \vspace{-15pt}
  \caption{Different design choices of the transformer architecture: (a) Only one Self-Attn Block. (b) Two Self-Attn Blocks. (c) Two Self-Attn Blocks and one Cross-Attn Block.}
  \label{fig:supp_transformer}
  \vspace{-5pt}
\end{figure*}

\subsection{More Details about the Architecture}
\textbf{Diffusion model in our baselines:}
The architecture of the diffusion model employed in our baselines is illustrated in Fig. \ref{fig:supp_diffmodel}. It shares similarities with the architecture within our DiffMesh, featuring two self-attention blocks designed to capture global dependencies and one cross-attention block focused on integrating information between the denoising input $x_{i}$ and the constant conditional feature $c$. In the baseline approach, as the conditional feature $c$ remains the same throughout the denoising process, there is no need to estimate the conditional feature for each subsequent denoising step. Thus, it only return the estimated noise term $n_{i-1}$.

\textbf{Conditional features generation block:}
Our chosen backbone to extract features for both our proposed method and the baselines is DSTformer \cite{MotionBERT2022}. After extracting features from each frame $b_{i}$, where $ i \in \left\{ 1,\dots f \right\}$, using the backbone, our goal is to generate $N$ conditional features to be utilized during the reverse process. To achieve this, we pad additional $N-f$ zero features, $b_{f+1}, \cdots, b_{N}$.  Then, we combine them with the existing features, creating new features $b \in \mathbb{R} ^{N \times D}$, where $D$ represents the embedded dimension. Subsequently, we apply a transformer block \cite{Dosovitskiy2020ViT} to model these features and return the required conditional features denoted as $c \in \mathbb{R} ^{N \times D}$.

%%%%%%%%%%%%%%%%
\section{More Experiments}
\label{exp}
\subsection{Performance on MPI-INF-3DHP dataset}
To conduct experiments on the MPI-INF-3DHP dataset, we follow the same setting as VIBE \cite{kocabas2020vibe}, TCMR \cite{tcmr}, and MPS-Net \cite{MPS-Net}, the number of input frames is set to be 16. The input features of each frame are extracted from ResNet-50 \cite{resnet} without fine-tuning for fair comparisons. The results are shown in Fig. \ref{tab:supp-mpii3d}. Our DiffMesh consistently outperforms previous methods with significant improvement (more than 5.9 mm $\downarrow$ of MPJPE, 1.8  $\downarrow$ of PA-MPJPE, and 0.7 $\downarrow$ of ACC-ERR). This showcases the remarkable performance enhancement achieved by our approach, highlighting its potential as a state-of-the-art solution for video-based human mesh recovery across various datasets and real-world applications.


\begin{table}
    \centering
    \resizebox{1\linewidth}{!}
  {
    \begin{tabular}{c|ccc}
\hline
\multicolumn{1}{l|}{} & \multicolumn{3}{c}{MPI-INF-3DHP} \\ \hline
Methods               & MPJPE  $\downarrow$   & PA-MPJPE  $\downarrow$   & ACC-ERR  $\downarrow$   \\ \hline
VIBE \cite{kocabas2020vibe}                 & 103.9   & 68.9       & 27.3      \\
TCMR \cite{tcmr}                  & 97.6    & 63.5       & 8.5       \\
MAED \cite{MAED}                & 83.6    & 56.2       & -         \\
MPS-Net \cite{MPS-Net}              & 96.7    & 62.8       & 9.6       \\
GLoT  \cite{GLoT}                & 93.9    & 61.5       & 7.9       \\ 
% PMCE  \cite{you2023coeval}          & 79.7    & 54.5       & 7.1       \\
% PMCE(repro)           & 82.9    & 57.5       & 7.4       \\ 
\hline
DiffMesh (ours)       & \textbf{78.9}    & \textbf{54.4}       & \textbf{7.0}       \\ \hline
\end{tabular}}
    \caption{Performance comparison with state-of-the-art methods on MPI-INF-3DHP dataset. All methods use pre-trained ResNet-50 \cite{resnet} (fixed weights) to extract features except MAED. }
    \label{tab:supp-mpii3d}
    % \vspace{-10pt}
\end{table}



\subsection{Effectiveness of the number of input frames and the number of additional steps}
Following the same setting as previous video-based methods such as VIBE \cite{kocabas2020vibe}, TCMR \cite{tcmr}, and MPS-Net \cite{MPS-Net}, the number of input frames $f$ is set to be 16. To further investigate the impact of the number of input frames, we conduct experiments on the 3DPW dataset given the different number of input frames. The results are shown in Table. \ref{tab:supp_frame}.

In general, the performance can be improved (lower MPJPE, PA-MPJPE, MPVPE, and ACC-ERR) when the number of input frames $f$ is increased. Specifically, when maintaining the total number of steps $N$ at 30 and varying $f$ from 8 to 16 to 24, the improvements are notable. 
% If $f$ is increased to 24 from 16, the performance can be further improved.  ( 0.2 reduction of MPVE, 1.0 reduction of MPJPE, and 0.2 reduction of ACC-ERR.). 
In our ablation study, the lowest MPVE, MPJPE, and ACC-ERR are achieved when $f=32$ with total steps of 40.  

To strike an optimal balance between efficiency and performance, it's crucial to seek improved results with a reduced total number of steps $N$. For instance, when $f=16$, the optimal $N$ is determined to be 30, demonstrating comparable results to $N=40$ at a faster processing speed. Similarly, for $f=24$, the optimal $N$ is identified as 30 based on the results.


\begin{table}[htp]
% \tiny
% \scriptsize
% \renewcommand\arraystretch{1.2}
% \vspace{-5pt}
\centering
  \caption{Performance of the different number of input frames and the number of additional steps on the 3DPW dataset.}
  \resizebox{1\linewidth}{!}
  {
\begin{tabular}{c|cc|c|ccc}
\hline
input frames & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}steps for output \\ sequence\end{tabular}} & additional steps & Total steps & MPVE $\downarrow$ & MPJPE $\downarrow$ & ACC-ERR $\downarrow$ \\ \hline
% 8     & 7      & 0               & 7          &  89.8    &  77.9     &   6.9      \\
% 16     & 15       & 0               & 15           &  88.5    &  77.4     &   6.5  \\
% 24      & 23      & 0               & 23          & 87.6     & 75.2      &  6.2       \\\hline
8      & 7       & 13               & 20          & 88.6     & 76.9      &   6.7      \\
16     & 15       & 5               & 20          &  88.0    &  77.1     &  6.5       \\\hline
8     & 7        & 23               & 30          &  87.4    & 76.5      &   6.5      \\
16     & 15       & 15               & 30          &  86.4    & 75.7      &   6.1      \\
24     & 23       & 7               & 30          &  86.2   &   74.7     &   5.9      \\ \hline
16     & 15       & 25               & 40          &  87.1    & 75.6      &   6.2      \\
24     & 23       & 17               & 40          &  86.5    &  74.7     &  6.1       \\
32     & 31       & 8               & 40          &  86.0    &  74.9     &  5.8       \\ \hline
\end{tabular}
}
\label{tab:supp_frame}
\vspace{-10pt}
\end{table}



\subsection{Different initial distributions in diffusion}
In Section 3.3 of the main paper, we assume that human motion will eventually reach a static state $x_{end}$,  which is an initial distribution. In our DiffMesh, we select the template mesh acquired by averaging ground-truth meshes on Human3.6M dataset \cite{lin2021metro} as the initial distribution. Here we compared the results when using pure Gaussian noise as the initial distribution. The results indicate that selecting template mesh as the initial distribution is a better choice than Pure Gaussian noise for the video-based HMR task. 

\begin{table}[htp]
% \tiny
% \scriptsize
% \renewcommand\arraystretch{1.2}
\vspace{-5pt}
\centering
  \caption{Evaluation of different initial distributions on the 3DPW dataset.}
  \vspace{-5pt}
  \resizebox{1\linewidth}{!}
  {
\begin{tabular}{c|cccc}
\hline
     & \multicolumn{4}{c}{3DPW}         \\ \hline
Initial distribution & MPVPE$\downarrow$ & MPJPE$\downarrow$ & PA-MPJPE$\downarrow$ & Accel$\downarrow$ \\ \hline
Pure Gaussian noise    & 87.0     & 75.8  & 46.7 & \textbf{6.1}   \\ \hline
Template mesh  & \textbf{86.4}     & \textbf{75.7}  & \textbf{45.6} & \textbf{6.1}   \\ \hline
\end{tabular}
}
\label{tab:supp_loss}
\vspace{-10pt}
\end{table}

\subsection{Different design choices of our transformer-based diffusion model}
As introduced in Section 3.3 of the main paper, our proposed transformer-based diffusion model consists of two self-attn blocks with one cross-attn block (also depicted in Fig. \ref{fig:supp_transformer} (c)). Given the input feature $x'_{i}$ and corresponding conditional feature $c_{i}$, the transformer-based diffusion model produces the predicted noise \textcolor{orange}{$m_{i-1}$} and the predicted previous conditional feature $\hat{c}_{i-1}$. We apply two self-attention blocks for $x'_{i}$ and  $c_{i}$ separately, then a cross-attention block is adopted to fuse the conditional features with mesh features. To validate the effectiveness, we compare this design with (a): a self-attention block applied for the concatenated features; and (b) two two self-attention blocks for $x'_{i}$ and  $c_{i}$ separately without cross-attention block. The results are shown in Table \ref{tab:supp_transformer}. Clearly, our design (c) in DiffMesh outperforms (a) and (b) for all evaluation metrics on the 3DPW dataset due to enhanced information integration using two-stream and cross-attention fusion design. 





\begin{table}[htp]
% \tiny
% \scriptsize
% \renewcommand\arraystretch{1.2}
\vspace{-5pt}
\centering
  \caption{Ablation study of transformer block design on 3DPW dataset.}
  \vspace{-5pt}
  \resizebox{1\linewidth}{!}
  {
\begin{tabular}{c|cccc}
\hline
                             & \multicolumn{4}{c}{3DPW}      \\ \hline
                             & MPVE $\downarrow$ & MPJPE $\downarrow$ & PA-MPJPE $\downarrow$ & ACC $\downarrow$ \\ \hline
(a) one self-attn            &  86.9    &  76.9     &  47.5        &  6.3   \\ \hline
(b) two self-attn            &  87.4    &  76.4     &  45.9        &  6.2   \\ \hline
(c) self-attn and cross attn &  \textbf{86.4}     & \textbf{75.7}  & \textbf{45.6} & \textbf{6.1}    \\ \hline
\end{tabular}
}
\label{tab:supp_transformer}
% \vspace{-10pt}
\end{table}


%%%%%%%%%%
\subsection{Effectiveness of the auxiliary loss:} 

To validate the effectiveness of our proposed auxiliary loss, we compare the results as shown in Table \ref{tab:supp_loss}, which demonstrated that our proposed auxiliary loss can help to improve the reconstruction performance (MPJPE, PA-MPJPE, and MPJVE) and the motion smoothness (ACC-ERR). 
\begin{table}[htp]
% \tiny
% \scriptsize
% \renewcommand\arraystretch{1.2}
% \vspace{-5pt}
\centering
  \caption{Evaluation of the combinations of loss functions on the 3DPW dataset.}
  \resizebox{1\linewidth}{!}
  {
\begin{tabular}{c|cccc}
\hline
     & \multicolumn{4}{c}{3DPW}         \\ \hline
loss & MPVPE$\downarrow$ & MPJPE$\downarrow$ & PA-MPJPE$\downarrow$ & Accel$\downarrow$ \\ \hline
$\mathcal{L}_{HMR}$    & 86.8     & 76.0  & 47.1 & 6.2   \\ \hline
$\mathcal{L}_{HMR} + \mathcal{L}_{aux}$   & \textbf{86.4}     & \textbf{75.7}  & \textbf{45.6} & \textbf{6.1}   \\ \hline
\end{tabular}
}
\label{tab:supp_loss}
% \vspace{-10pt}
\end{table}


\begin{figure}[htp]
% \vspace{-5pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/vis-supp0.pdf}
  \vspace{-15pt}
  \caption{Qualitative comparison on the 3DPW dataset}
  \label{fig:supp_vis0}
  \vspace{-15pt}
\end{figure}

\begin{figure*}[htp]
\vspace{-5pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/vis-supp2.pdf}
  \vspace{-5pt}
  \caption{Qualitative results of our DiffMesh on in-the-wild videos. \textcolor{blue}{Please refer to our \textbf{video demo} for the more reconstructed mesh sequences results.}}
  \label{fig:supp_vis}
  \vspace{-5pt}
\end{figure*}

\section{Human Mesh Visualization}
\label{meshvis}

We first visualize the qualitative comparison on the 3DPW \cite{pw3d2018} dataset in Fig.~\ref{fig:supp_vis0}. The circle areas highlight locations where our DiffMesh performs better than SOTA GLoT \cite{GLoT}. 

%%%%%%%%%%%%%%%%%
% \begin{figure*}[htp]
% \vspace{-5pt}
%   \centering
%   \includegraphics[width=1\linewidth]{figure/full_frame.pdf}
%   \vspace{-5pt}
%   \caption{Visualization of decoding steps during the reverse motion process.}
%   \label{fig:supp_full_frame}
%   \vspace{-5pt}
% \end{figure*}

\begin{figure*}[htp]
\vspace{-10pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/vis-supp3.pdf}
  \vspace{-5pt}
  \caption{Other qualitative results of our DiffMesh on in-the-wild videos. \textcolor{blue}{Please refer to our \textbf{video demo} for the more reconstructed mesh sequences results.}}
  \label{fig:supp_vis2}
  \vspace{-10pt}
\end{figure*}

% In our experimental setup, we utilize 16 input frames, and the total number of steps is set to 30. In the reverse motion process, DiffMesh outputs [$y_1, y_2 \cdots, y_{30}$] over 30 steps. For the output mesh sequence of 16 frames, we use [$y_1, y_2 \cdots, y_{16}$]. Additionally, we generate the mesh from [$y_{16}, y_{17} \cdots, y_{30}$], as visually depicted in Fig \ref{fig:supp_full_frame}. This visualization illustrates the trend of the generated human mesh gradually decoding toward the desired human mesh of each input frame.
%%%%%%%%%%%%%%


Furthermore, we show the qualitative results of DiffMesh on \textbf{in-the-wild videos} in Fig.~\ref{fig:supp_vis} and Fig.~\ref{fig:supp_vis2}. We observe that DiffMesh demonstrates remarkable performance in reconstructing more reliable human mesh sequences with temporal consistency compared to previous methods. \textcolor{blue}{Please refer to our \textbf{video demo} for the more reconstructed mesh sequence results.}  

\clearpage
\newpage

{\small
\bibliographystyle{ieeenat_fullname}
\bibliography{main}
}


\end{document}
