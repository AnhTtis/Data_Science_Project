% WACV 2025 Paper Template
% based on the WACV 2024 template, which is
% based on the CVPR 2023 template (https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip) with 2-track changes from the WACV 2023 template (https://github.com/wacv-pcs/WACV-2023-Author-Kit)
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review,algorithms]{wacv}      % To produce the REVIEW version for the algorithms track
%\usepackage[review,applications]{wacv}      % To produce the REVIEW version for the applications track
%\usepackage{wacv}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{wacv} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{pifont}

\usepackage{color}
\usepackage{soul}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{pifont}%
\usepackage{caption}
% \definecolor{RowColor}{rgb}{0.97, 0.97, 1}
\definecolor{RowColor}{rgb}{0.91, 0.91, 1}
\definecolor{babyblue}{rgb}{0.63, 0.79, 0.95}
\usepackage{colortbl}
\usepackage{wrapfig}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
% \usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\wacvPaperID{456} % *** Enter the WACV Paper ID here
\def\confName{WACV}
\def\confYear{2025}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{DiffMesh: A Motion-aware Diffusion Framework for Human Mesh Recovery from Videos}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}


\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
\vspace{-10pt}
    \centering
   \includegraphics[width=0.88\textwidth]{figure/pipe_comp.pdf}
   \vspace{-5pt}
    \captionof{figure}{Different approaches of applying diffusion model for video-based HMR, the input frame $f$ is 3 for simplicity and the number of steps is $N$. Here $x_{i}$ and $c_{i}$ denote the mesh and conditional features of $i_{th}$ frame, respectively. (a) Baseline 1: The diffusion model is applied for each frame individually. The total steps are $f \times N$, which is computationally expensive. Moreover, motion is ignored during the denoising process, leading to non-smooth motion predictions (high acceleration error). (b) Baseline 2: The features from ground truth mesh vertices of each frame $x_i$ are concatenated to unified features $x_{unified}$ during the forward process. To obtain the mesh of each frame, features are split after the denoising process. Although this strategy reduces the total steps to $N$, it doesn't effectively model human motion during denoising (acceleration error can be further reduced). (c) Our proposed DiffMesh: We consider the inherent motion patterns within the forward process and the reverse process when utilizing the diffusion model, leading to smooth and accurate motion predictions (low acceleration error and MPVE). The total number of steps remains as $N$. For more detailed information, please refer to Sec \ref{DiffMesh}.}
    \label{fig:pipe_comp}
    % \vspace{-5pt}
\end{center}%
}]

%%%%%%%%% ABSTRACT
\begin{abstract}
   Human mesh recovery (HMR) provides rich human body information for various real-world applications such as gaming, human-computer interaction, and virtual reality. While image-based HMR methods have achieved impressive results, they often struggle to recover humans in dynamic scenarios, leading to temporal inconsistencies and non-smooth 3D motion predictions due to the absence of human motion. In contrast, video-based approaches leverage temporal information to mitigate this issue. In this paper, we present DiffMesh, an innovative motion-aware Diffusion framework for video-based HMR. DiffMesh establishes a bridge between diffusion models and human motion, efficiently generating accurate and smooth output mesh sequences by incorporating human motion within the forward process and reverse process in the diffusion model. Extensive experiments are conducted on the widely used datasets (Human3.6M \cite{h36m_pami} and 3DPW \cite{pw3d2018}), which demonstrate the effectiveness and efficiency of our DiffMesh. Visual comparisons in real-world scenarios further highlight DiffMesh's suitability for practical applications. \textcolor{magenta}{The code will be publicly available.}
\end{abstract}


%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
Human Mesh Recovery (HMR) aims to recover detailed human body information, encompassing both pose and shape, with applications spanning gaming, human-computer interaction, and virtual reality  \cite{hmrsurvey}. While image-based HMR methods have achieved remarkable results, they often fall short in dynamic scenarios, wherein the neglect of human motion results in temporal inconsistencies and non-smooth 3D motion predictions. In contrast, video-based approaches offer a solution to mitigate these issues by leveraging temporal cues from input sequences, thereby meeting the practical demands of real-world applications. This makes video-based methods a compelling choice for enhancing the temporal coherence and consistency of 3D motion predictions.


Recently, diffusion models have garnered substantial interest within the computer vision community.  By learning to reverse the diffusion process in which noise has been added in successive steps, diffusion models can generate samples that match a specified data distribution corresponding to the provided dataset. As generative-based methods, diffusion models excel in tasks such as motion synthesis and generation \cite{MDM,flame,li2023ego}. The quality and diversity of generated outputs have witnessed significant improvements by bridging the gap between uncertain and determinate distributions. This inspires us to consider the diffusion model as a promising solution for recovering high-quality human mesh from input data fraught with depth ambiguity.


While it is straightforward to apply the diffusion model for image-based HMR such as in \cite{foo2023distribution}, effective utilization of the diffusion model for video-based HMR remains a formidable challenge. \ul{As previous video-based HMR methods have not yet incorporated diffusion models into their frameworks}, we provide a summary of diffusion-based methods related to human pose and mesh recovery (please refer to Sec. \ref{sec:related_diff} for further elaboration). We then organize these methods into two intuitive baseline approaches that can be applied for video-based HMR, as illustrated in Fig. \ref{fig:pipe_comp} (a) and (b).
Assuming a video input with $f$ frames in \ul{Baseline 1}, the diffusion model is applied individually to recover the mesh of the single frame (usually would be the center frame). More details can be found in Sec. \ref{Baseline}.
However, this approach suffers from significant computational burdens as the total number of denoising steps amounts to  $f \times N$ to recover the mesh sequence of $f$ frames, with $N$ representing the number of denoising steps in the diffusion model. Moreover, the diffusion model does not account for motion smoothness in dynamic sequences, which may result in potentially temporal inconsistent mesh predictions. To mitigate computational costs and enhance inference speed, we introduce \ul{Baseline 2}. The features for ground truth mesh vertices of each frame $x_{i}$, where $i \in \left\{ 1,\dots f \right\}$ are concatenated to unified features $x_{unified}$ before the forward diffusion process. 
Subsequently, the output features are partitioned back into individual frames $x_{i}$ after the \textit{denoising process}. 
While this strategy effectively reduces the steps from $f \times N$ to $N$,  it is important to note that motion smoothness in dynamics is still not considered during the forward and reverse process since features across frames are simply concatenated. This limitation can potentially lead to non-smooth motion predictions.


\begin{figure}[htp]
  \centering
  \vspace{-10pt}
  \includegraphics[width=1\linewidth]{figure/dm_TID_new.pdf}
  \vspace{-15pt}
  \caption{(a) The general pipeline for diffusion model. Input data is perturbed by adding noise recursively and output data is generated from the noise in the reverse process. Images are taken from \cite{diffsurvey1}. (b) Human motion is involved over time in the input video sequence. Similar to the forward process in (a), the forward motion between adjacent frames resembles the process of adding noise. The mesh of the previous frame can be decoded through the reverse motion process successively. }
  \label{fig:dm_tid}
  \vspace{-15pt}
\end{figure}

To tackle the aforementioned efficiency and motion smoothness challenges when utilizing the diffusion model, we introduce \textbf{DiffMesh}, a novel motion-aware diffusion framework as illustrated in Fig. \ref{fig:pipe_comp} (c). DiffMesh is designed specifically for video-based HMR. Unlike conventional diffusion models, which necessitate the addition of Gaussian noise in consecutive steps during the \textit{forward diffusion process}, as depicted in Fig. \ref{fig:dm_tid} (a), DiffMesh adopts a distinct approach. We consider the forward motion influence over a brief time interval as shown in Fig. \ref{fig:dm_tid} (b), encompassing the features (embedded in the high-dimensional space) of adjacent frames $x_i$ to $x_{i+1}$, similar to the mechanism of introducing noise. Importantly, our approach does not conclude the forward motion at the last frame $x_{f}$, where $f$ is the total number of frames. Instead, we assume that there is a hidden motion sequence exists which can lead the motion culminates in an initial distribution. This initial distribution is represented by the static state, which is similar to pure Gaussian noise in the conventional diffusion model. During the \textit{reverse process}, the diffusion model decodes the previous feature $x_{i-1}$ based on $x_{i}$ and the corresponding conditional feature $c_{i}$ from the input frame. Consequently, the total number of steps remains consistent at $N$, as illustrated in Fig. \ref{fig:pipe_comp} (b). 

\textbf{Contribution:} The key innovation of DiffMesh lies in its capacity to consider the inherent motion patterns within the diffusion model efficiently and effectively. This consideration notably enhances motion smoothness and temporal consistency throughout the output mesh sequence, achieved through successive steps across frames. Incorporating our diffusion framework, we design a two-stream transformer architecture to decode motion features. Our DiffMesh not only achieves new state-of-the-art results on the Human3.6M \cite{h36m_pami} and 3DPW \cite{pw3d2018} datasets, but also demonstrates its superiority in handling in-the-wild scenarios compared to previous methods. These advantages make DiffMesh more viable for deployment in real-world applications for generating accurate and smooth human mesh sequences efficiently.

\section{Related Work}
Since human mesh recovery is a popular topic in computer vision, here we focus on the most relevant works to our methods. More references can be found in the \textcolor{blue}{supplementary Sec. 2}. We Also refer readers to the recent and comprehensive HMR survey \cite{hmrsurvey} and diffusion model survey \cite{diffsurvey1} for more details. 


\subsection{Video-based HMR}

Image-based HMR methods ~\cite{lin2021metro,zheng2023feater,li2023hybrik,zheng2023potter,zhang2023pymafx,li2022cliff,goel2023humans,wang2023refit,kaufmann2023emdb} have achieved remarkable progress in accuracy.  
However, when applied to video sequences, these image-based methods suffer from severe motion jitter due to frame-by-frame reconstruction, making them unsuitable for practical use. To address this issue, video-based methods \cite{hmmr,meva,sun2019human,zeng2022deciwatch,li2022dnd,zeng2022deciwatch,GLoT,you2023coeval,yang2023capturing} utilize temporal information to enhance accurate and temporally consistent human mesh from video frames.
% Kanazawa et al.\cite{hmmr} first propose a convolutional network to learn human motion kinematics by predicting past, current, and future frames. Based on \cite{hmmr}, Sun et al. \cite{sun2019human} further propose a self-attention-based temporal model to improve performance. 
VIBE \cite{kocabas2020vibe} leverages the AMASS dataset \cite{AMASS2019} to discriminate between real human motions and those produced by its temporal encoder. MEVA \cite{meva} estimates the coarse 3D human motion using a variational motion estimator, then refines the motion by a motion residual regressor. TCMR \cite{tcmr} utilizes GRU-based temporal encoders to forecast the current frame from the past and future frames, which enhances the temporal consistency. Similar to \cite{tcmr}, MPS-Net \cite{MPS-Net} captures the motion continuity dependencies by the attention mechanism. MotionBERT \cite{MotionBERT2022} proposes a pertaining stage to recover 3D motion from noise 2D observations, achieving state-of-the-art performance.
% for video-based HMR. 




\subsection{Diffusion Generative Models}
\label{sec:related_diff}
Diffusion Generative Models have achieved impressive success in a wide variety of computer vision tasks such as image inpainting \cite{song2021scorebased}, text-to-image generation \cite{photorealistic}, and image-to-image translation \cite{choi2021ilvr}. Given the strong capability to bridge the large gap between highly uncertain and determinate distribution, several works have utilized the diffusion generative model for the text-to-motion generation \cite{modiff,MDM,zhang2022motiondiffuse}, human pose estimation \cite{shan2023diffusion}, object detection \cite{chen2023diffusiondet}, and head avatar generation \cite{bergman2023articulated,mendiratta2023avatarstudio}. 
% For instance, Zhang et al. \cite{zhang2022motiondiffuse} propose a versatile motion-generation framework that incorporates a diffusion model to generate diverse motions from comprehensive texts. 
% Similarly, Tevet et al. \cite{MDM} introduce a lightweight transformer-based diffusion generative model that can achieve text-to-motion and motion editing. Zhao et al. \cite{modiff} tackle sequential skeleton-based motion generation, where the skeleton sequence is treated as a 2D image matrix with three channels (N: number of frames, J: number of joints in skeleton in one frame, and D: x, y, z coordinate of each joint). 
% Several pioneering works have explored the application of diffusion models in human pose-related tasks. 
DiffPose \cite{gong2023diffpose} is the first to utilize a diffusion-based approach for predicting the 3D pose from a 2D pose sequence input. Shan et al. \cite{shan2023diffusion} generate multiple hypotheses of the 3D pose from 2D pose input using a diffusion model, and then a multi-hypothesis aggregation module outputs the final 3D pose. EgoEgo \cite{li2023ego} employs a diffusion model to generate multiple plausible full-body motions based on the head pose input. 

% \begin{wraptable}{r}{0.5\textwidth}
% \vspace{-20pt}
%   \centering
%   \resizebox{1\linewidth}{!}
%   {
%   % Please add the following required packages to your document preamble:
% % \usepackage{multirow}
% \begin{tabular}{l|c|ccc|c|c}
% \hline
% \multicolumn{1}{c|}{Methods} & Input              & \multicolumn{3}{c|}{Output}    & Motion-Aware   & Approach            \\ \cline{3-5}
% \multicolumn{1}{c|}{}        &                    & 3D Pose & 3D Mesh & multi-frame  & Diffusion &    Similar to                   \\ \hline
% Diff3DHPE \cite{zhou2023diff3dhpe}      & 2D Human Pose Sequence   & \cellcolor{babyblue} \cmark    & \xmark    & \cellcolor{babyblue} \cmark        & \xmark    & Baseline 2 \\
% DiffPose \cite{gong2023diffpose}       & 2D Human Pose Sequence   & \cellcolor{babyblue} \cmark     & \xmark      & \xmark   & \xmark        & Baseline 1 \\
% D3DP \cite{shan2023diffusion}           & 2D Human Pose Sequence   & \cellcolor{babyblue} \cmark     & \xmark      & \cellcolor{babyblue} \cmark  & \xmark        & Baseline 2 \\
% HMDiff \cite{foo2023distribution}          & Single Image       & \cellcolor{babyblue} \cmark     & \cellcolor{babyblue} \cmark     & \xmark       & \xmark    & Baseline 1 \\
% EgoEgo  \cite{li2023ego}                     & Head Pose Sequence & \cellcolor{babyblue} \cmark     & \cellcolor{babyblue} \cmark     & \cellcolor{babyblue} \cmark    & \xmark      & Baseline 2 \\ \hline
% DiffMesh (ours)              & Video sequence     & \cellcolor{babyblue} \cmark     & \cellcolor{babyblue} \cmark     & \cellcolor{babyblue} \cmark        &  \cellcolor{babyblue} \cmark  &   \textcolor{red}{Our design}                    \\ \hline
% \end{tabular}
% }
% \vspace{-5pt}
% \caption{Comparison with previous human pose and mesh-related methods utilizing diffusion models. }
% \label{tab:frame-comp}
% \vspace{-25pt}
% \end{wraptable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[htp]
% \tiny
% \scriptsize
\renewcommand\arraystretch{1.1}
\vspace{-5pt}
  \centering
  \resizebox{0.95\linewidth}{!}
  {
  % Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{tabular}{l|c|ccc|c|c}
\hline
\multicolumn{1}{c|}{Methods} & Input              & \multicolumn{3}{c|}{Output}    & Motion-Aware   & Approach            \\ \cline{3-5}
\multicolumn{1}{c|}{}        &                    & 3D Pose & 3D Mesh & multi-frame  & Diffusion &    Similar to                   \\ \hline
DiffPose \cite{gong2023diffpose}       & 2D Human Pose Sequence   & \cellcolor{babyblue} \cmark     & \xmark      & \xmark   & \xmark        & Baseline 1 \\
Diff3DHPE \cite{zhou2023diff3dhpe}      & 2D Human Pose Sequence   & \cellcolor{babyblue} \cmark    & \xmark    & \cellcolor{babyblue} \cmark        & \xmark    & Baseline 2 \\
D3DP \cite{shan2023diffusion}           & 2D Human Pose Sequence   & \cellcolor{babyblue} \cmark     & \xmark      & \cellcolor{babyblue} \cmark  & \xmark        & Baseline 2 \\
HMDiff \cite{foo2023distribution}          & Single Image       & \cellcolor{babyblue} \cmark     & \cellcolor{babyblue} \cmark     & \xmark       & \xmark    & Baseline 1 \\
EgoEgo  \cite{li2023ego}                     & Head Pose Sequence & \cellcolor{babyblue} \cmark     & \cellcolor{babyblue} \cmark     & \cellcolor{babyblue} \cmark    & \xmark      & Baseline 2 \\ \hline
DiffMesh (ours)              & Video sequence     & \cellcolor{babyblue} \cmark     & \cellcolor{babyblue} \cmark     & \cellcolor{babyblue} \cmark        &  \cellcolor{babyblue} \cmark  &   \textcolor{red}{Our design}                    \\ \hline
\end{tabular}
}
\vspace{-5pt}
\caption{Comparison with previous diffusion-based human pose and mesh methods. }
\label{tab:frame-comp}
\vspace{-10pt}
\end{table}

\noindent \textbf{Distinction of Our Method:}
Previous methods for video-based HMR have not integrated diffusion models into their frameworks. While there are diffusion-based methods that can output 3D human pose or human mesh, they are not directly suitable for the video-based HMR task and require adaptations. We organize them into two intuitive baseline approaches. As depicted in Table \ref{tab:frame-comp}, approaches similar to \ul{Baseline 1}, such as \cite{SpatioTemporalDiffPose,foo2023distribution,gong2023diffpose}, require $f \times N$ denoising steps to produce output for $f$ frames. Conversely, methods similar to \ul{Baseline 2}, such as \cite{zhou2023diff3dhpe,shan2023diffusion,li2023ego}, reduce the total denoising steps from $f \times N$ to $N$. 
However, these methods do not effectively incorporate the consideration of human motion within the sequence during both the forward process and the reverse process, which may potentially result in temporal inconsistencies and non-smooth 3D motion predictions. 
In contrast, our approach treats motion patterns across frames as the inherent noise in the forward process, eliminating the need for adding Gaussian noise during forward steps as required in Baseline 1 and Baseline 2. This unique motion-aware design in diffusion enables our approach to decode human motion patterns during reverse processes within $N$ steps, setting it apart from conventional diffusion-based methods.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology}
\subsection{Preliminary}
\label{Preliminary}
We first provide a brief overview of the original denoising diffusion probabilistic model \cite{ho2020denoising} (DDPM) scheme. As shown in Fig. \ref{fig:dm_tid} (a), the DDPM consists of two Markov chains, which are a diffusion process and a reverse process. The forward chain perturbs data to noise, while the reverse chain converts the noise back into data. For a more detailed understanding, we direct interested readers to the original paper  \cite{ho2020denoising} and recent surveys of the diffusion model \cite{diffsurvey1,diffsurvey2}.

\noindent \textbf{Forward Diffusion Process:} Let  $x_0  \sim p(x_0)$ where  $x_0$ is the training sample and $ p(x_0)$ be the data density. Gaussian transitions $\mathcal{N}$ are successively added to perturb the training sample $x_0$. A sequence of corrupted data $x_1, x_2, \dots, x_T$ can be obtained following the forward Markov chain:
\vspace{-5pt}
\begin{align}
\small
\resizebox{0.9\linewidth}{!}{$
    p(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} \cdot x_{t-1}, \beta_t \cdot \textbf{I}), \forall t \in \left\{ 1,\dots T \right\}
\vspace{-10pt}
$}
\end{align}
where $\beta_1 \dots \beta_T \in \left [0,1\right )$ are the hyperparameters for the variance schedule in each step, $T$ is the number of steps, and $\textbf{I}$ is the identity matrix. We use $\mathcal{N}(x;\mu,\sigma^2)$ to denote the normal distribution of mean $\mu$ and standard deviation $\sigma$ that produce $x$.  According to the property of normal distribution recursive formulation, we can directly sample $x_t$ when $t$ is drawn from a uniform distribution as follows: 
\vspace{-5pt}
\begin{align}
\small
    p(x_t | x_{0}) = \mathcal{N}(x_t; \sqrt{\hat{\beta_t}} \cdot x_{0}, (1-\hat{\beta_t}) \cdot \textbf{I}), 
\vspace{-10pt}
\end{align}
where $\hat{\beta_t} = \prod_{i=1}^{t} \alpha_i $ and $\alpha_t = 1- \beta_t$.

Based on this equation, we can sample any $x_t$ directly when given original data $x_0$ and a fixed variance schedule $\beta_t$. To achieve this, the sample $x$ of a normal distribution $x \sim \mathcal{N}(\mu , \sigma^2 \cdot \textbf{I} )$  first subtracts the mean $\mu$ and divides by the standard deviation $\sigma$, outputting in a sample $z= \frac{x-\mu}{\sigma}$ of the standard normal distribution $z \sim \mathcal{N}(0, \textbf{I})$. Thus  $x_t$ is sampled from $p(x_t | x_0)$ as follows: 
\vspace{-5pt}
\begin{align}
\small
    x_t = \sqrt{\hat{\beta_t}} \cdot x_{0} + \sqrt{(1-\hat{\beta_t})} \cdot z_t, 
\vspace{-10pt}
\end{align}
where $z_t \sim \mathcal{N}(0,\textbf{I})$. 

\noindent \textbf{Reverse Denoising Process:} To generate new samples from $p(x_0)$, we apply the reverse Markov chain from a $x_T \sim \mathcal{N}(0, \textbf{I})$: 
\vspace{-5pt}
\begin{align}
\small
    p_{\theta}(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_{\theta}(x_t, t), \Sigma_{\theta}^{}(x_t, t) ), 
\label{eq:reverse}
\end{align}
where $\theta$ denotes model parameters, and the mean $\mu_{\theta}(x_t, t)$ and covariance $\Sigma_{\theta}(x_t, t) $ are parameterized by deep neural networks $\epsilon_\theta$. 
\vspace{-5pt}
\begin{align}
\small
    x_{t-1} = \frac{1}{\sqrt{\beta_{t}}} (x_t - \frac{1-\beta_t}{\sqrt{1-\hat{\beta_t}}} \epsilon_\theta(x_t, t))+\sigma_t z, 
\label{eq:reverse_xt}
\vspace{-5pt}
\end{align}
If $t>1, z \sim \mathcal{N}(0,\textbf{I})$, else $z = 0$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Diffusion-based Baselines}
\label{Baseline}






% Drawing inspiration from the diffusion model's remarkable ability to bridge the gap between uncertain and determinate distributions,
% % during denoising processes
% we are naturally inclined to leverage it for video-based HMR.



\begin{wrapfigure}{r}{0.25\textwidth}
\vspace{-20pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/encoder.pdf}
  \vspace{-20pt}
  \caption{Transformation between human mesh and corresponding mesh feature in the latent space.}
  \label{fig:encoder}
  \vspace{-15pt}
\end{wrapfigure}
Given that previous video-based HMR methods have not integrated diffusion models into their frameworks, we incline to explore the performance of simply incorporating diffusion models in the context of video-based HMR. The forward and reverse processes are conducted in a latent space, the transformation between human mesh and the corresponding mesh feature in this latent space is shown in Fig. \ref{fig:encoder}. 



\begin{figure}[htp]
\vspace{-5pt}
  \centering
  \includegraphics[width=0.95\linewidth]{figure/baseline_new.pdf}
  \vspace{-10pt}
  \caption{Two diffusion baselines for video-based HMR.}
  \label{fig:baseline}
  \vspace{-20pt}
\end{figure}

\noindent \textbf{Baseline 1:} As illustrated in Fig. \ref{fig:baseline} (a), we initially apply the vanilla diffusion model at the frame level. For each $i_{th}$ frame, we employ a backbone to extract the conditional feature $c_{i}$ where $i\in \left\{ 1,\dots, f \right\}$ and $f$ representing the total number of frames. We then proceed with the standard forward diffusion process, introducing noise to the ground truth mesh vertices $x_{i}$. After $N$ steps, the feature $x_{i}^{end}$ converges to a  Gaussian Noise. Subsequently, a transformer-based diffusion model  (details are included in the \textcolor{blue}{supplementary Sec. 2}) is employed for the denoising process. The denoised feature  $x'_{i}$ is obtained after $N$ denoising steps. Finally, the human mesh of $i_{th}$ frame is returned by the human mesh head. By iterating this procedure $f$ times, we generate the final human mesh sequence consisting of $f$ frames. However, despite its simplicity and straightforwardness, Baseline 1 exhibits a significant drawback: the total number of steps is  $f \times N$, resulting in considerable computational overhead and slow inference speed. Furthermore, it fails to address motion smoothness within the diffusion model, potentially leading to temporal inconsistencies in mesh predictions.


\noindent \textbf{Baseline 2:} To mitigate computational costs and enhance inference speed, we introduce Baseline 2 for video-based HMR.  Motivated by recent works \cite{modiff,flame,li2023ego} that leverage diffusion models for motion generation, we adopt a similar concatenation and separation strategy, as illustrated in Fig. \ref{fig:baseline} (b). In this approach, we concatenate the ground-truth mesh vertices of each frame into a unified feature $x_{unified} = cat(x_1, x_2, \dots, x_f)$, where $f$ represents the total number of frames. Similarly, we create a concatenated conditional feature  $c_{unified}$ by combining the image features from each frame. Subsequently, we apply the forward diffusion process by introducing noise for $N$ steps, which is analogous to Baseline 1. After $N$ steps,  $x_{unified}^{end}$ also converges to a Gaussian Noise state. Following this, we perform $N$ denoising steps utilizing a transformer-based diffusion model (details are included in the \textcolor{blue}{supplementary  Sec. 2}). The denoised feature $x'_{unified}$ is then partitioned into  $x'_1, x'_2, \dots, x'_f$. Finally, the mesh sequence is obtained through the mesh head.

%%%%%%%%
\begin{figure*}[htp]
\vspace{-10pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/arch_new.pdf}
  \vspace{-10pt}
  \caption{The architecture of DiffMesh: Our framework takes input sequence with $f$ frames, with the objective of outputting a human mesh sequence consisting of $f$ frames. We model the forward human motion across frames similar to the mechanism of introducing noise in the forward process. We assume that human motion will eventually reach a static state, which is represented by the mesh template state $x_{end}$, through a hidden motion sequence. Thus, additional $N - f + 1$ steps are necessary from $x_f$ to reach the static state $x_{end}$. Consequently, we utilize a transformer-based diffusion model to sequentially produce the decoded features during the reverse process. The final human mesh sequence is returned by a mesh head using SMPL \cite{SMPL:2015} human body model. The structure of DN (diffusion network) is illustrated in Fig. \ref{fig:md}. }
  \label{fig:arch}
  \vspace{-15pt}
\end{figure*}
%%%%%%%%%%

The concatenation and separation strategy has substantially reduced the number of steps from $f \times N$ to $N$. However, it is important to note that this strategy does not effectively consider motion patterns across frames due to simple concatenation, which may result in temporal inconsistencies and non-smooth 3D mesh predictions.
%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%% 

\subsection{DiffMesh}
\label{DiffMesh} 


To address the previously mentioned limitations in our baselines, we present DiffMesh, a novel solution for video-based HMR. The overall architecture is depicted in Fig. \ref{fig:arch}. Our framework takes input sequence with $f$ frames, with the objective of outputting a human mesh sequence consisting of $f$ frames. Additionally, we extract the conditional feature $c_i$ using the backbone, incorporating a conditional features generation block (more details are provided in the \textcolor{blue}{supplementary  Sec. 2}). In contrast to Baseline 1 and Baseline 2, DiffMesh employs a distinct approach that accounts for the forward and backward motion within the diffusion model. 


\noindent \textbf{Forward Process:}
We make the assumption that human motion consistently influences the human mesh vertices (in a latent space) across frames, denoted as $x_{i}$ to $x_{i+1}$,  where $ i \in \left\{ 1,\dots f-1 \right\}$. This motion pattern can be conceptualized as a specific noise introduced to the mesh vertices feature of the preceding frame: 
\vspace{-5pt}
\begin{align}
\small
    x_{i+1} = \sqrt{\beta_{i}} \cdot x_{i} + \sqrt{(1-\beta_{i})} \cdot \textcolor{orange}{m_{i}},
\label{eq:diffmesh_xi}
\vspace{-5pt}
\end{align}
where \textcolor{orange}{$m_{i}$} represents the motion pattern from $i_{th}$ frame to $(i+1)_{th}$ frame. Consequently,  $x_{1}$  can be gradually diffused into $x_{f}$ after  $f-1$ forward diffusion steps. Importantly, achieving $x_{f}$ does not denote the conclusion of the forward diffusion process. \textit{We assume that human motion will eventually reach a static state $x_{end}$ through a hidden motion sequence.  This static state serves as an initial distribution (similar to the Gaussian noise in the conventional diffusion model)}.  In our work, we select the template mesh acquired by averaging ground-truth meshes \cite{lin2021metro} as the initial distribution (ablation study is provided in the \textcolor{blue}{supplementary  Sec. 3}). To arrive at this final static state $x_{end}$, DiffMesh requires  $N-f+1$ additional forward diffusion steps. Therefore, the total number of steps remains $N$. 


\noindent \textbf{Reverse Process:} Our objective during the reverse process is to obtain output features  $x'_{1} \cdots x'_{f}$. Initiating from the static state  $x_{end}$, we design a transformer-based diffusion model, as depicted in Fig. \ref{fig:arch}, to sequentially produce decoded features. After  $N-f+1$ steps, our diffusion model yields $x'_{f}$, which serves as the target feature for the final frame $f$. Iteratively, we generate the output features $x'_{i}$ where $ i \in \left\{ f-1,\dots 1 \right\}$ over $f-1$ steps. The total number of steps remains $N$. In contrast to Baseline 2, DiffMesh demonstrates the capability to decode specific motion patterns within the same number of steps, while generating more precise and smooth output (verified in Sec. \ref{ablation_study}).

% \begin{wrapfigure}{r}{0.5\textwidth}
% \vspace{-10pt}
%   \centering
%   \includegraphics[width=0.85\linewidth]{figure/motiondecoder.pdf}
%   \vspace{-5pt}
%   \caption{(a) The architecture of our two-stream transformer network. Given the input feature $x'_i$ and corresponding conditional feature $c_i$, the diffusion model generates the predicted motion $m_{i-1}$ and the predicted previous conditional feature $\hat{c}_{i-1}$. (b) The self-attention block in our diffusion model. (c) The cross-attention block in our diffusion model.  The notation $\oplus$ represents element-wise addition. }
%   \label{fig:md}
%   \vspace{-25pt}
% \end{wrapfigure}

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[tp]
\vspace{-10pt}
  \centering
  \includegraphics[width=0.85\linewidth]{figure/motiondecoder.pdf}
  \vspace{-5pt}
  \caption{(a) Two-stream transformer network: Given the input feature $x'_i$ and corresponding conditional feature $c_i$, the diffusion model generates the predicted motion $m_{i-1}$ and the predicted previous conditional feature $\hat{c}_{i-1}$. (b) The self-attention block in our diffusion model. (c) The cross-attention block in our diffusion model.  The notation $\oplus$ represents element-wise addition. }
  \label{fig:md}
  \vspace{-15pt}
\end{figure}
%%%%%%%%%%%

\noindent\textbf{Network Design:} In contrast to conventional diffusion-based methods \cite{ho2020denoising,rombach2022high,peebles2023scalable} for image synthesis tasks that often rely on UNet \cite{unet} or vanilla transformer \cite{vaswani2017attention} backbones, our approach introduces a two-stream transformer design 
% specific for video-based HMR. This architecture is 
deeply integrated within our framework, as illustrated in Fig. \ref{fig:md}. While conventional diffusion-based methods \cite{ho2020denoising,rombach2022high,peebles2023scalable} only estimate noise during steps, our two-stream network predicts the motion features and the previous conditional feature during each step as depicted in Fig. \ref{fig:md} (c). 
% In addition to the estimation of motion \textcolor{orange}{$m_{i-1}$}, our network also predicts the previous conditional feature $\hat{c}_{i-1}$ during each step as depicted in Fig. \ref{fig:md} (c). 
This architecture 
% significantly 
enhances information integration through a two-stream process. Initially, it captures mesh and condition dependencies separately using two self-attention blocks. Then, these captured dependencies are merged by a cross-attention block efficiently to decode motion features. An \textbf{auxiliary loss} is computed by measuring the difference (MSE loss) between the predicted $\hat{c}_{i-1}$ and the ground truth $c_{i-1}$, which contributes to training a better transformer-based network during the reverse process.


% For the self-attention block in Fig. \ref{fig:md} (b), we adopt the standard self-attention architecture as in ViT \cite{Dosovitskiy2020ViT}. To effectively integrate information from the input feature $x'_{i}$ and the condition feature $c_{i}$, we perform cross-attention between them as illustrated in Fig. \ref{fig:md} (c). The $\rm FFN(\cdot)$ is a feed-forward network consisting of the multilayer perceptron (MLP) and normalization layer. 
% In contrast to diffusion-based methods employing UNet \cite{unet} or vanilla transformer \cite{vaswani2017attention} to estimate the noise term at each step, \ul{we propose a two-stream transformer-based diffusion model for video-based HMR}, as depicted in Fig. \ref{fig:md}. Our model produces the predicted motion \textcolor{orange}{$m_{i-1}$} as well as the predicted previous conditional feature $\hat{c}_{i-1}$, given the input feature $x'_{i}$ and corresponding conditional feature $c_{i}$ at each step. 
Given the input feature $x'_{i}$ and corresponding conditional feature $c_{i}$ at each step, our network produces the predicted motion \textcolor{orange}{$m_{i-1}$} as well as the predicted previous conditional feature $\hat{c}_{i-1}$. The feature of static state (mesh template) is denoted as \textcolor{orange}{$x_{end}$}. Similar to Equation \ref{eq:reverse_xt},  the mesh features $x'_{i-1}$ in DiffMesh can be computed as 
\vspace{-5pt}
\begin{align}
\small
    x'_{i-1} = \frac{1}{\sqrt{\beta_{i}}} (x'_i - \frac{1-\beta_i}{\sqrt{1-\hat{\beta_i}}} \textcolor{orange}{m_{i-1}} )+\sigma_t \textcolor{orange}{x_{end}},
\label{eq:diffmesh_xt}
\end{align}
\vspace{-10pt}
% \noindent 

% For the self-attention block in Fig. \ref{fig:md} (b), we adopt the standard self-attention architecture as in ViT \cite{Dosovitskiy2020ViT}. To effectively integrate information from the input feature $x'_{i}$ and the condition feature $c_{i}$, we perform cross-attention between them as illustrated in Fig. \ref{fig:md} (c). The $\rm FFN(\cdot)$ is a feed-forward network consisting of the multilayer perceptron (MLP) and normalization layer.


% An \textbf{auxiliary loss} is computed by measuring the difference between the predicted $\hat{c}_{i-1}$ and the ground truth $c_{i-1}$. This auxiliary loss contributes to training a better transformer-based model during the reverse process.


%%%%%%%%%%%
Through our design, we efficiently acquire the output features for each frame. In comparison to Baseline 2, we maintain a total of $N$ steps while explicitly considering motion patterns during both the forward process and the reverse process. This approach results in improved accuracy and motion smoothness in the estimated mesh sequence, achieved through successive steps across frames.


\subsection{Overall Training and Loss Functions}

In contrast to conventional diffusion models, our DiffMesh does not need to add noise manually. Based on the assumption that human motion consistently impacts the human mesh across consecutive frames, we postulate the existence of a latent space where the motion pattern $m_i$ can be conceptualized as a specific noise introduced to the mesh vertices' features of the preceding frame $x_{i}$, as depicted in Eq. \ref{eq:diffmesh_xi}. Our objective is to derive such motion patterns for each time interval within this specific latent space during the reverse steps. Ultimately, the final mesh sequence is generated based on the estimated mesh features \(x_{i}\) through MLP layers and the SMPL model.



% Based on the assumption that human motion consistently impacts the human mesh across consecutive frames, we postulate the existence of a latent space where the motion pattern $m_i$ can be conceptualized as a specific noise introduced to the mesh vertices' features of the preceding frame $x_{i}$, as depicted in Eq 6. Our objective is to derive such motion patterns for each time interval within this specific latent space during the reverse steps. Ultimately, the final mesh sequence is generated based on the estimated mesh features \(x_{i}\) through MLP layers and the SMPL model.

Thus, instead of supervising the noise in conventional diffusion models, we directly supervise the SMPL parameters of each frame ($\Theta_i$ and $\beta_i$) where $i \in \left\{ 1,\dots f \right\}$ with the ground-truth SMPL parameters using the $L2$ loss during training. Following \cite{kocabas2020vibe,tcmr}, the 3D joint coordinates are regressed by forwarding the SMPL parameters to the SMPL human body model. 
% The 2D joint coordinates are also obtained by projecting the 3D joint coordinates to 2D using the camera parameters. 
Furthermore, as mentioned in Sec. \ref{DiffMesh}, Mean Squared Error (MSE) loss is applied between the conditional feature $c_{i}$ and the predicted conditional features $\hat{c_{i}}$.  
% This loss aids in the training of a more effective transformer-based diffusion model.
More detailed information is included in the \textcolor{blue}{supplementary  Sec. 2}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-5pt}
\section{Experiments}

%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[htp]
% \tiny
% \scriptsize
\renewcommand\arraystretch{1.2}
% \vspace{-5pt}
\centering
  \resizebox{1\linewidth}{!}
  {
\begin{tabular}{l|c|c|cccc|cccc}
\hline
\multicolumn{1}{c|}{}                   &           &                  & \multicolumn{4}{c|}{Human3.6M} & \multicolumn{4}{c}{3DPW} \\ \hline
\multicolumn{1}{c|}{Video-based Method} & \multicolumn{1}{c|}{Venue}     & Backbone & MPVE $\downarrow$    & MPJPE $\downarrow$  & PA-MPJPE $\downarrow$  & ACC-ERR $\downarrow$  & MPVE $\downarrow$  & MPJPE $\downarrow$ & PA-MPJPE $\downarrow$  & ACC-ERR $\downarrow$ \\ \hline
\textit{\textbf{Fixed backbone}}                         &           &                  &         &         &            &       &       &          \\ \hline
VIBE \cite{kocabas2020vibe}                                    & CVPR 2020 & ResNet50\cite{resnet}               & -       & 65.6    & 41.4  & -   & -  & 91.9  & 57.6   & 25.4  \\
TCMR \cite{tcmr}                                   & CVPR 2021 & ResNet50\cite{resnet}               & -       & 62.3    & 41.1 & 5.3      & 102.9 & 86.5  & 52.7 & 6.8    \\
MPS-Net \cite{MPS-Net}                                & CVPR 2022 & ResNet50\cite{resnet}               & -       & 69.4    & 47.4 & 3.6      & 99.7  & 84.3  & 52.1   & 7.4  \\
% SmoothNet \cite{smoothnet}                              & ECCV 2022 & 32               & -       & 67.5    & 46.3       & -     & 86.7  & 52.7     \\
GLoT \cite{GLoT}                                   & CVPR 2023 & ResNet50\cite{resnet}               & -       & 67.0      & 46.3 & 3.6     & 96.3  & 80.7  & 50.6 & 6.6     \\
\rowcolor{RowColor} DiffMesh (ours)                                   &  & ResNet50\cite{resnet}               & -       & \textbf{65.3}      & \textbf{41.9} & \textbf{3.3}     & \textbf{94.0}  & \textbf{77.2}  & \textbf{48.5} & \textbf{6.3}     \\ \hline
\textit{\textbf{Other backbone}}                         &           &                  &         &         &            &       &       &          \\ \hline
MAED \cite{MAED}                                   & ICCV 2021 & ResNet50\cite{resnet}+ViT\cite{Dosovitskiy2020ViT}               & 84.1    & 60.4    & 38.3 & -      & 93.3  & 79.0  & 45.7 & 17.6     \\
% PMCE \cite{you2023coeval}                             & ICCV 2023 & 16               & -    & 53.5    & 37.7       & 84.8  & 69.5  & 46.7     \\
MotionBERT \cite{MotionBERT2022}                             & ICCV 2023 & DSTformer \cite{MotionBERT2022}               & 65.5    & 53.8    & 34.9 & -      & 88.1  & 76.9  & 47.2  & -   \\
\rowcolor{RowColor} DiffMesh (Ours)                               &           & DSTformer \cite{MotionBERT2022}                &   \textbf{64.2}      &  \textbf{52.5}   & \textbf{33.5}   & \textbf{5.5}    & \textbf{86.4}  & \textbf{75.7}  & \textbf{45.6} & \textbf{6.1}    \\ \hline
\textit{\textbf{With Refinement}}                         &           &                  &         &         &            &       &       &          \\ \hline
DND \cite{li2022dnd} $\dagger$                        & ECCV 2022 & HybrIK \cite{li2021hybrik}               &    -     & 52.5    & 35.5   & 6.1    & 88.6  & 73.7  & 42.7  & 7.0   \\
MotionBERT \cite{MotionBERT2022}  $\dagger$                  & ICCV 2023 & DSTformer \cite{MotionBERT2022}                & 52.6    & 43.1    & 27.8 & -      & 79.4  & 68.8  & 40.6  & -   \\
\rowcolor{RowColor} DiffMesh (Ours)  $\dagger$                   &           & DSTformer \cite{MotionBERT2022}                &   \textbf{52.1}      & \textbf{42.4}    & \textbf{27.7}   &\textbf{5.7}     & \textbf{78.4}  & \textbf{67.8}  & \textbf{40.1}  & \textbf{6.3}   \\ \hline
\end{tabular}
}
  % \vspace{5pt}
\caption{Performance comparison with SOTA video-based methods on Human3.6M and 3DPW datasets. 
  The symbol “$\dagger$” denotes the HybrIK \cite{li2021hybrik} is applied for the refinement. 
  3DPW Training set is used during training. 
  % The best results are marked as bold and the second-best results are marked as underlined. 
  }
\label{tab: alldataset}
\vspace{-15pt}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Implementation Details}
\label{imp_detail}

For fair comparisons, we adhere to the standard implementation details commonly employed in the video-based HMR task, consistent with prior works such as \cite{kocabas2020vibe, tcmr, MPS-Net, MotionBERT2022}. We set the length of the input sequence $f$ to 16. The number of steps $N=30$. We adopt the same variance schedules as DiffPose \cite{gong2023diffpose} during our training and testing. Our approach is implemented using PyTorch \cite{PyTorch} on a single NVIDIA A5000 GPU. The optimization of weights is conducted using the Adam optimizer \cite{kingma2014adam}, with an initial learning rate of $5e^{-6}$ and weight decay set to 0.98. During the training phase, we utilize a batch size of 64, and the training process spans 60 epochs. More detailed information about the conditional feature generation part is included in the \textcolor{blue}{supplementary  Sec. 2}.

\subsection{Datasets and Evaluation Metrics}

\textbf{Datasets:} Following the previous works \cite{kocabas2020vibe,tcmr,MPS-Net}, a mixed 2D and 3D datasets are used for training. PoseTrack \cite{posetrack} and InstaVariety \cite{hmmr} are 2D datasets where 2D ground-truth annotations are provided for PoseTrack \cite{posetrack}, while pseudo ground-truth 2D annotations are generated from \cite{openpose} for InstaVariety \cite{hmmr}.  3DPW \cite{pw3d2018}, Human3.6M \cite{h36m_pami}, and AMASS \cite{AMASS2019} are 3D datasets for training. 
% Among them, ground-truth SMPL parameters are available for 3DPW and AMASS. 
% For the evaluation, Human3.6M, and 3DPW are selected for the performance comparison. 

\noindent \textbf{Evaluation Metrics:} The HMR performance is evaluated by four standard metrics: Mean Per Joint Position Error (MPJPE), Procrustes-Aligned MPJPE (PA-MPJPE), Mean Per Vertex Position Error (MPVE), and Acceleration Error (ACC-ERR). Particularly, MPJPE, PA-MPJPE, and MPVE indicate the accuracy of the estimated 3D human pose and shape measured in millimeters (mm). ACC-ERR is proposed in HMMR \cite{hmmr} for evaluating temporal smoothness, which computes the average difference between the predicted and ground-truth acceleration of each joint in ($mm/s^2$). 




\subsection{Comparison with State-of-the-art Methods}
We compare our DiffMesh with previous SOTA video-based methods on Human3.6M and 3DPW datasets (results on MPI-INF-3DHP \cite{mpi3dhp2017} are provided in the \textcolor{blue}{supplementary  Sec. 3}). Among these datasets, Human3.6M is an indoor dataset while 3DPW contains complex outdoor scenes. Following \cite{tcmr,MPS-Net}, 3DPW training sets are involved in training. The results are shown in Table \ref{tab: alldataset}. \textit{Our DiffMesh outperforms all previous methods across all evaluation metrics on the Human3.6M and 3DPW datasets}. This superior performance validates the efficacy of our DiffMesh framework for recovering high-quality human mesh structures from video input. Specifically, 
% when using the same fixed backbone as TCMR \cite{tcmr}, MPS-Net\cite{MPS-Net}, and GLoT\cite{GLoT}, our DiffMesh achieves best performance on these two datasets. 
Without involving an additional refinement procedure, our DiffMesh surpasses the second-best method MotionBERT \cite{MotionBERT2022} (which already exhibits significant improvements over its predecessors) by more than 1 mm of MPJPE, PA-MPJPE, and MPVE. When incorporating HybrIK \cite{li2021hybrik} as an additional refinement procedure, our DiffMesh consistently surpasses MotionBERT \cite{MotionBERT2022} with refinement, which significantly enhances the performance of video-based HMR. 
% \begin{wraptable}{r}{0.5\textwidth}
% \centering
%   \resizebox{1\linewidth}{!}
%   {
% \begin{tabular}{l|c|cccc}
% \hline
% \multicolumn{1}{c|}{}                   &                 & \multicolumn{4}{c}{3DPW} \\ \hline
% \multicolumn{1}{c|}{Video-based Method}    & Backbone & MPVE $\downarrow$    & MPJPE $\downarrow$ & PA-MPJPE $\downarrow$  & ACC-ERR $\downarrow$ \\ \hline
% VIBE \cite{kocabas2020vibe} & ResNet50\cite{resnet}  & 113.4  & 93.5  & 56.5 & 27.1  \\
% TCMR \cite{tcmr}   & ResNet50\cite{resnet}   & 111.5 & 95.0  & 55.8 & 7.0   \\
% MPS-Net \cite{MPS-Net}   & ResNet50\cite{resnet}     & 109.6  & 91.6  & 54.0   & 7.5  \\
% GLoT \cite{GLoT}   & ResNet50\cite{resnet}        & 107.8  & 89.9  & 53.0 & 6.7     \\
% \rowcolor{RowColor} DiffMesh (ours)        & ResNet50\cite{resnet}     & \textbf{105.9}  & \textbf{88.7}  & \textbf{53.0} & \textbf{6.5}     \\ \hline
% \end{tabular}
% }
%   \vspace{5pt}
%   \caption{Performance comparison with SOTA video-based methods 3DPW datasets without using 3DPW training set during training. 
%   }
% \label{tab: 3dpw-no}
% \vspace{-5pt}
% \end{wraptable}

\begin{table}[htp]
% \tiny
\scriptsize
\renewcommand\arraystretch{1.1}
% \vspace{-5pt}
\centering
  \resizebox{1\linewidth}{!}
  {
\begin{tabular}{l|c|cccc}
\hline
\multicolumn{1}{c|}{}                   &                 & \multicolumn{4}{c}{3DPW} \\ \hline
\multicolumn{1}{c|}{Video-based Method}    & Backbone & MPVE $\downarrow$    & MPJPE $\downarrow$ & PA-MPJPE $\downarrow$  & ACC-ERR $\downarrow$ \\ \hline
VIBE \cite{kocabas2020vibe} & Fixed ResNet50\cite{resnet}  & 113.4  & 93.5  & 56.5 & 27.1  \\
TCMR \cite{tcmr}   & Fixed ResNet50\cite{resnet}   & 111.5 & 95.0  & 55.8 & 7.0   \\
MPS-Net \cite{MPS-Net}   & Fixed ResNet50\cite{resnet}     & 109.6  & 91.6  & 54.0   & 7.5  \\
GLoT \cite{GLoT}   & Fixed ResNet50\cite{resnet}        & 107.8  & 89.9  & 53.0 & 6.7     \\
\rowcolor{RowColor} DiffMesh (ours)        & Fixed ResNet50\cite{resnet}     & \textbf{105.9}  & \textbf{88.7}  & \textbf{53.0} & \textbf{6.5}     \\ \hline
\end{tabular}
}
  % \vspace{5pt}
\caption{Performance comparison with SOTA video-based methods on 3DPW without using 3DPW training set during training. 
  }
\label{tab: 3dpw-no}
\vspace{-15pt}
\end{table}

Following previous methods \cite{tcmr,MPS-Net,GLoT}, we report the results on the 3DPW \cite{pw3d2018} without using the 3DPW training set in Table. \ref{tab: 3dpw-no}. To ensure a fair comparison, we also fixed ResNet50 \cite{resnet} as our backbone. Our DiffMesh consistently surpasses those methods across all evaluation metrics. 





%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[htp]
% \tiny
% \scriptsize
\renewcommand\arraystretch{1.2}
\vspace{-5pt}
\centering
  \resizebox{1\linewidth}{!}
  {
\begin{tabular}{c|cc|ccc|cc}
\hline
           & \begin{tabular}[c]{@{}c@{}}\# of input \\ frames\end{tabular} & \begin{tabular}[c]{@{}c@{}}\# of output \\ frames\end{tabular}  & MPVE $\downarrow$ & MPJPE $\downarrow$ & ACC-ERR  $\downarrow$ & \cellcolor[HTML]{FFFFFF}\begin{tabular}[c]{@{}c@{}}Total processing \\ time\end{tabular} & \cellcolor[HTML]{FFFFFF}\begin{tabular}[c]{@{}c@{}}Average time \\ per frame\end{tabular} \\ \hline
\textit{\textbf{Image-based}}                         &           &                  &         &         &            &       &            \\ \hline
I2L-MeshNet \cite{Moon_I2L_MeshNet}  & 1                                                            & 1                                                         & 8110.1  & 93.2  & 30.9     & 41.1 ms                                                                                  & 41.1 ms/frame                                                                                  \\ 
HybrIK \cite{hybrik}       & 1                                                            & 1                                                         & 89.7  & 76.2  & 22.8     &  38.9 ms                                                                                  & 38.9 ms/frame                                                                                  \\
POTTER \cite{zheng2023potter}       & 1                                                            & 1                                                         & 87.4  & 75.0  & 23.1    &  42.7 ms                                                                                  & 42.7 ms/frame                                                                                  \\ \hline
\textit{\textbf{Video-based}}                         &           &                  &         &         &            &       &            \\ \hline
% TCMR \cite{MPS-Net}     & 16                                                             & 1                                                           & 102.9  & 86.5     & 6.8    & 28.9 ms                                                                                  & 28.9 ms/frame                                                                                 \\
% MPS-Net \cite{MPS-Net}     & 16                                                             & 1                                                           & 99.7  & 84.3     & 7.4     & 31.1 ms                                                                                  & 31.1 ms/frame                                                                                 \\
DND \cite{li2022dnd}       & 32                                                            & 32                                                           & 88.6  & \textbf{73.7}      & \underline{7.0}     &  2697.6 ms   &   84.3 ms/frame \\
Baseline 1 & 16                                                            & 16   & \underline{87.2}             &  77.6         &   16.5      &  2168.4 ms                                                                                        & 135.5 ms/frame                                                                                              \\
Baseline 2 & 16                                                            & 16          &  89.0         &    76.5                      &   7.9      &   224.7 ms                                                                                         & 14.0 ms/frame                                                                                           \\ \hline
\rowcolor{RowColor} DiffMesh (Ours)  & 16                                                            & 16       & \textbf{86.4} & \underline{75.7}   &  \textbf{6.1}       &    223.1 ms           & \textbf{13.9 ms/frame}                                                                                            \\ \hline
\end{tabular}
}
\vspace{-5pt}
\caption{Reconstruction performance and inference time comparison on 3DPW dataset between our DiffMesh and previous video-based HMR methods with the same hardware platform. A single NVIDIA A5000 GPU is used \textbf{with a batch size of 1 (the input of [1, num of frames, 224, 224, 3])} for fair comparison. }
\label{tab:ab_fps}
\vspace{-10pt}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \vspace{-5pt}
\subsection{Ablation Study}
\label{ablation_study}
% We conduct the ablation study with the same setting as in Table \ref{tab: alldataset}. 
We conduct the ablation study on 3DPW \cite{pw3d2018} dataset since it provides ground-truth SMPL parameters. All implementation details are the same as in Section \ref{imp_detail}.

\noindent \textbf{Comparison with the Baselines:} In Table \ref{tab:ab_fps}, we present a performance comparison among Baselines (in Section \ref{Baseline}) and our DiffMesh. Baseline 1 applies the diffusion model frame by frame, achieving better MPVE and PA-MPJPE results than Baseline 2. However, it falls short in terms of ACC-ERR. When adopting the concatenation and separation strategy in Baseline 2, the ACC-ERR is improved, but with a slight downgrade in MPVE and PA-MPJPE. Neither Baseline 1 nor Baseline 2 surpass the previous method DND \cite{li2022dnd}. In contrast, our DiffMesh outperforms both Baseline 1 and Baseline 2 across all evaluation metrics. 


\noindent \textbf{Temporal consistency and motion smoothness:} To further validate the enhancement in temporal consistency achieved by DiffMesh, we conducted an evaluation of acceleration error as shown in Table \ref{tab:ab_fps}. While some image-based methods can achieve impressive performance in terms of accuracy (low MPVE and MPJPE), they often suffer from motion jitter and temporal inconsistency issues. For instance, we observed that methods like HybrIK \cite{li2021hybrik} and POTTER \cite{zheng2023potter} exhibit lower MPVE and MPJPE, but concurrently, their ACC-ERR are notably higher compared to video-based HMR methods
% such as TCMR \cite{tcmr}, MPS-Net \cite{MPS-Net}, and GLoT \cite{GLoT}
. Instead, our DiffMesh, which integrates motion patterns into the diffusion model and enhances temporal consistency through successive steps across frames, manifests in its superior performance. DiffMesh demonstrates a 12.8\% reduction in ACC-ERR compared to the SOTA DND \cite{li2022dnd} even with fewer input frames (16 vs. 32). We further visualize the comparison of the ACC-ERR in Fig. \ref{fig:ab_acc} (``courtyard\_basketball\_01' of 3DPW dataset). The ACC-ERR for DND \cite{li2022dnd} is unavailable in Fig. \ref{fig:ab_acc} as their code has not been fully released. This highlights the superiority of DiffMesh, which leverages the consideration of human motion patterns within the diffusion model, resulting in enhanced overall performance, especially in terms of motion smoothness.

% We further reduce the ACC-ERR from 6.6 to 6.1, which is about 7.6\% reduction over the previous best ACC-ERR from GLoT \cite{GLoT}. These results underscore the practical viability of DiffMesh for real-world applications, particularly in scenarios requiring the generation of accurate and smooth human mesh sequences.
%%%%%%%%%%%%%%%%
\begin{figure}[htp]
\vspace{-10pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/plot-basket.png}
  \vspace{-10pt}
  \caption{Acceleration error Comparison of the ‘courtyard\_basketball\_01’ sequence for TCMR \cite{tcmr}, two baselines, and our DiffMesh. }
  \label{fig:ab_acc}
  \vspace{-10pt}
\end{figure}
%%%%%%%%%%%


% \begin{table}[htp]
% % \tiny
% % \scriptsize
% \renewcommand\arraystretch{1.1}
% % \vspace{-5pt}
% \centering
%   \caption{Ablation study of Baseline 1, Baseline 2, and our DiffMesh on 3DPW dataset. }
%   \vspace{-5pt}
%   \resizebox{0.98\linewidth}{!}
%   {
% \begin{tabular}{c|cccc}
% \hline
%            & \multicolumn{4}{c}{3DPW}                                                                                          \\ \hline
%            & MPVE  $\downarrow$ & MPJPE  $\downarrow$ & PA-MPJPE  $\downarrow$ & ACC-ERR  $\downarrow$ \\ \hline
% Baseline 1 &  87.2             &  77.6        &   45.9                  &    16.5                         \\ \hline
% Baseline 2 &   89.0         &    76.5            &   46.9                   &    7.9                         \\ \hline
% \rowcolor{RowColor} DiffMesh   & \textbf{86.4} & \textbf{75.7}  & \textbf{45.6}     &   \textbf{6.1}                          \\ \hline
% \end{tabular}
% }
% \label{tab:ab_baseline}
% \vspace{-5pt}
% \end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Considering that several methods such as TCMR\cite{tcmr} and MPS-Net\cite{MPS-Net} can extract all features by their backbone and then utilize batch processing to accelerate the inference speed, we provide more thorough inference time comparison in the \textcolor{blue}{supplementary  Sec. 3}. 

\noindent \textbf{Inference time analysis:} A prevalent consideration regarding diffusion-based methods pertains to their inference speed,  which is also a crucial evaluation metric besides accuracy. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% add this to supplementary
% While methods like TCMR \cite{tcmr} and MPS-Net \cite{MPS-Net} employ video input to enhance motion smoothness and temporal consistency, they often face limitations in terms of processing speed. For instance, when provided with a sequence of frames (e.g., 16 frames) as input, MPS-Net \cite{MPS-Net} can only generate the human mesh for one frame in Table \ref{tab:ab_fps}. The average processing time of these methods exceeds 30 ms/frame, indicating a need for further improvements in computational efficiency.  
%%%%%%%%%%%%%%%%%%%%%%%%%
Our Baseline 1 requires a long processing time to output high-quality human mesh. Our Baseline 2 reduced the processing time but the acceleration error is still worse than DND \cite{li2022dnd}.  Compared to these methods, 
% our DiffMesh offers significant improvements in motion smoothness and temporal consistency without compromising on inference speed. 
DiffMesh not only achieves the best mesh recovery performance and motion smoothness, while maintaining a competitive processing time. The average time can achieve 13.9 ms/frame, which is much faster than previous SOTA methods (More detailed comparisons with previous methods are provided in the \textcolor{blue}{supplementary  Sec. 3} when considering some methods can extract all features by their backbone and then utilize batch processing to accelerate the inference speed). % Considering that several methods such as TCMR\cite{tcmr} and MPS-Net\cite{MPS-Net} can extract all features by their backbone and then utilize batch processing to accelerate the inference speed, we provide more thorough inference time comparison in the \textcolor{blue}{supplementary  Sec. 3}. 
This combination of accuracy and efficiency makes DiffMesh a promising diffusion solution for video-based human mesh recovery applications.

% \noindent \textbf{Number of steps in diffusion:} Following previous video-based HMR methods \cite{kocabas2020vibe,tcmr,MPS-Net,GLoT}, our DiffMesh operates with an input sequence of $f=16$ frames. As introduced in Section \ref{DiffMesh}, We assume that human motion will eventually reach a static state $x_{end}$ through a hidden motion sequence.  The role of the static state (initial distribution) is similar to the Gaussian noise in conventional diffusion models. To verify the impact of these additional steps, we conducted experiments as shown in Table \ref{tab:steps}. When introducing additional steps, the reverse motion process starts from the initial distribution (this static state is represented by the template mesh \cite{lin2021metro}). The results demonstrate that introducing additional steps can enhance overall performance.  The optimal balance between efficiency and performance is achieved when the number of additional steps is set to 15 (the total number of steps $N$ is 30). 


% \begin{table}[htp]
% % \tiny
% % \scriptsize
% % \renewcommand\arraystretch{1.2}
% \vspace{-20pt}
% \centering
%   \resizebox{0.85\linewidth}{!}
%   {
% \begin{tabular}{c|cc|c|ccc}
% \hline
% input frames & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}steps for output \\ sequence\end{tabular}} & additional steps & Total steps & MPVE $\downarrow$ & MPJPE $\downarrow$ & ACC-ERR $\downarrow$ \\ \hline
% 16      & 15     & 0            & 15          &  88.5    &  77.4     &    6.5     \\
% 16      & 15     & 5            & 20          &  88.0    &  77.1     &  6.5  \\  
% 16     & 15      & 15           & 30          &  \textbf{86.4}    & \underline{75.7}      &   \textbf{6.1}  \\
% 16    & 15       & 25            & 40          &  \underline{87.1}    &  \textbf{75.6}     &   \underline{6.2}  \\ \hline
% \end{tabular}
% }
% \caption{Performance with the different number of steps on 3DPW.}
% \label{tab:steps}
% \vspace{-20pt}
% \end{table}


% \noindent Due to space limitation, \textbf{more ablation studies are available in the \textcolor{blue}{supplementary Sec. 3}}, {covering topics such as initial distribution, the number of input frames, the two-stream transformer network (Fig. \ref{fig:md}), and the auxiliary loss.}   
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%
\begin{figure}[htp]
\vspace{-5pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/vis_new.pdf}
  \vspace{-15pt}
  \caption{The in-the-wild visual comparison between recent GLoT \cite{GLoT} with our DiffMesh. The circles highlight locations where DiffMesh is more accurate than GLoT. More examples are provided in the \textcolor{blue}{supplementary  Sec. 4 and in demo videos}. }
  \label{fig:ab_vis}
  \vspace{-15pt}
\end{figure}
%%%%%%%%%%%


\subsection{In-the-Wild Visualization}
To demonstrate the practical performance of DiffMesh, we present the qualitative comparison between DiffMesh and a recent GLoT \cite{GLoT} on in-the-wild video in Fig. \ref{fig:ab_vis}. Although achieving good performance in terms of mesh recovery performance on common datasets, GLoT \cite{GLoT} suffers from temporal inconsistency when applied for in-the-wild video. 
% While TCMR \cite{tcmr} and MPS-Net \cite{MPS-Net} switch to a many-to-one approach to tackle motion smoothness and temporal consistency, these methods also have their limitations 
% Instead, our DiffMesh enhances motion smoothness and reconstruction accuracy through our proposed diffusion-based framework. 
As shown in Fig. \ref{fig:ab_vis}, GLoT \cite{GLoT} fails to generate the accurate mesh. In contrast, our DiffMesh enhances motion smoothness and reconstruction accuracy through our proposed motion-aware diffusion framework. %More qualitative results and video demos are provided in the \textcolor{blue}{supplementary  Sec. 4} and anonymous page \url{https://anonymous.4open.science/r/DiffMesh}. 


\section{Conclusion}
In this paper, we present a novel diffusion framework (DiffMesh) for human mesh recovery from a video. DiffMesh innovatively connects diffusion models with human motion, resulting in the efficient generation of highly precise and seamlessly smooth output mesh sequences. Extensive experiments are conducted on Human3.6M and 3DPW datasets showing impressive performance compared to the SOTA video-based methods. 

While DiffMesh demonstrates its superior performance for in-the-wild video input, like previous methods, it may produce unrealistic mesh outputs in scenarios with significant occlusions. Our future research will focus on exploring spatial-temporal interactions within the human body to mitigate this occlusion challenge.




%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
