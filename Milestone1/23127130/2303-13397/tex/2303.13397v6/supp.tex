% WACV 2025 Paper Template
% based on the WACV 2024 template, which is
% based on the CVPR 2023 template (https://media.icml.cc/Conferences/CVPR2023/cvpr2023-author_kit-v1_1-1.zip) with 2-track changes from the WACV 2023 template (https://github.com/wacv-pcs/WACV-2023-Author-Kit)
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review,algorithms]{wacv}      % To produce the REVIEW version for the algorithms track
%\usepackage[review,applications]{wacv}      % To produce the REVIEW version for the applications track
%\usepackage{wacv}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{wacv} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{pifont}

\usepackage{color}
\usepackage{soul}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{pifont}%
\usepackage{caption}
% \definecolor{RowColor}{rgb}{0.97, 0.97, 1}
\definecolor{RowColor}{rgb}{0.91, 0.91, 1}
\definecolor{babyblue}{rgb}{0.63, 0.79, 0.95}
\usepackage{colortbl}
\usepackage{wrapfig}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
% \usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\wacvPaperID{456} % *** Enter the WACV Paper ID here
\def\confName{WACV}
\def\confYear{2025}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{DiffMesh: A Motion-aware Diffusion Framework for Human Mesh Recovery from Videos (\textit{Supplementary Material})}

% %%%%%%%%% AUTHORS - PLEASE UPDATE
% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

% \begin{document}
% \maketitle
% \input{sec/0_abstract}    
% \input{sec/1_intro}
% \input{sec/2_formatting}
% \input{sec/3_finalcopy}
% {
%     \small
%     \bibliographystyle{ieeenat_fullname}
%     \bibliography{main}
% }

\maketitle

% \vspace{-10pt}
\section{Overview}

The supplementary material is organized into the following sections:

%\setlist{nolistsep}
\begin{itemize}%[noitemsep,leftmargin=*] 
\item Section \ref{detail}: More related Work and implementation details.

\item Section \ref{exp}: Mathematical proof and more experiments about the number of input frames, the two-stream transformer network, initial distributions, and the auxiliary loss. 

\item Section \ref{meshvis}: More human mesh visualization.

\item Section \ref{Broader}: Broader impact and limitation
\end{itemize}

% \section{Broader Impact }
% \label{Broader}
% Our proposed DiffMesh framework for video-based HMR has the potential for widespread applications, including motion capture in animation and movies, virtual AI assistants, and VR/AR content. Currently, these applications may still require expensive and time-consuming motion capture devices, which can be complicated to set up. However, DiffMesh is easy to deploy without any additional devices. As a many-to-many approach, DiffMesh can reliably reconstruct human meshes from videos, even for online streaming. DiffMesh shows great potential as a causal video-based HMR model for real-world applications.
  

% Despite its strengths, DiffMesh has some limitations. While it can estimate reliable human meshes for in-the-wild scenarios, its performance may be impacted by severe occlusions, especially in crowded scenes. Additionally, DiffMesh uses a fixed backbone (ResNet50\cite{resnet}) following \cite{tcmr,kocabas2020vibe,MPS-Net} for a fair comparison, but we plan to improve its performance by applying a new backbone that weight can be updated during training in future work.

% \vspace{-10pt}

\section{More Related Work and Implementation Details}
\label{detail}
\subsection{Related Work}
The majority of methods  ~\cite{kanazawaHMR18,Kolotouros2019SPIN,Choi_2020_ECCV_Pose2Mesh,gtrs,zheng2023feater,you2023gator,zheng2023potter,li2023niki} for HMR rely on a parametric human model, such as SMPL \cite{SMPL:2015}, to reconstruct the mesh by estimating pose and shape parameters. As a fundamental HMR work, SPIN ~\cite{Kolotouros2019SPIN} combines regression and optimization in a loop, where the regressed output serves as better initialization for optimization (SMPLify). METRO \cite{lin2021metro} is the first transformer-based method that models vertex-vertex and vertex-joint interaction using a transformer encoder after extracting image features with a CNN backbone. HybrIK \cite{li2021hybrik} and HybrIK-X \cite{li2023hybrik} present novel hybrid inverse kinematics approaches that transform 3D joints to body-part rotations via twist-and-swing decomposition. Lin et al. \cite{lin2023one} propose a one-stage pipeline for 3D whole-body (body, hands, and face) mesh recovery. PyMAF \cite{pymaf2021} and its extension work PyMAF-X \cite{zhang2023pymafx} capitalize on a feature pyramid to rectify predicted parameters by aligning meshes with images, extracting mesh-aligned evidence from finer-resolution features. CLIFF \cite{li2022cliff} enhances holistic feature representation by incorporating bounding box information into cropped-image features. It employs a 2D reprojection loss considering the full frame and leverages global-location aware supervision to directly predict global rotation and more accurately articulated poses. ReFit \cite{wang2023refit} proposes a feedback-update loop reminiscent of solving inverse problems via optimization, iteratively reprojections keypoints from the human model to feature maps for feedback, and utilizes a recurrent-based updater to refine the model's fit to the image. HMR2.0 \cite{goel2023humans} develops a system that can simultaneously reconstruct and track humans from video, but only reports the frame-based results for the HMR task without considering temporal information. Foo et al. \cite{foo2023distribution} first introduce a diffusion-based approach for recovering human mesh from a single image. The recovered human mesh is obtained by the reverse diffusion process. However, when applied to video sequences, these image-based methods suffer from severe motion jitter due to frame-by-frame reconstruction, making them unsuitable for practical use. 

Compared to image-based HMR methods, video-based methods \cite{zeng2022deciwatch,smoothnet,you2023coeval} utilize temporal information to enhance motion smoothness from video input. In addition to the methods \cite{kocabas2020vibe,AMASS2019,meva,tcmr,MPS-Net,MotionBERT2022} introduced in the main paper, there are several other noteworthy approaches for video-based HMR.  Kanazawa et al.\cite{hmmr} first propose a convolutional network to learn human motion kinematics by predicting past, current, and future frames. Based on \cite{hmmr}, Sun et al. \cite{sun2019human} further propose a self-attention-based temporal model to improve performance. DND \cite{li2022dnd} utilizes inertial forces control as a physical constraint to reconstruct 3D human motion. GLoT \cite{GLoT} adopts a novel approach by decoupling the modeling of short-term and long-term dependencies using a global-to-local transformer. PMCE \cite{you2023coeval} follows a two-step process, where it first estimates 3D human pose and then regresses the mesh vertices through a co-evaluation decoder that takes into account the interactions between pose and mesh.






\subsection{Datasets}
\noindent \textbf{3DPW} \cite{pw3d2018} is a dataset that captures outdoor and in-the-wild scenes using a hand-held camera and a set of inertial measurement unit (IMU) sensors attached to body limbs. The ground-truth SMPL parameters are computed based on the returned values. This dataset includes 60 videos of varying lengths, and we use the official split to train and test the model. The split comprises 24, 12, and 24 videos for the training, validation, and test sets, respectively. The MPJPE, PA-MPJPE, MPJVE, and ACC-ERR are reported when evaluating this dataset. 


\noindent \textbf{Human3.6M} \cite{h36m_pami} is a large-scale benchmark for the indoor 3D human pose. It includes 15 action categories and 3.6M video frames. Following \cite{kocabas2020vibe,tcmr,MPS-Net}, we use five subjects (S1, S5, S6, S7, S8) for the training set and two subjects (S9, S11) for the testing set. The dataset is subsampled from its original 50 fps to 25 fps for both training and evaluation purposes. When calculating MPJPE and PA-MPJPE, only 14 joints are selected for a fair comparison to the previous works.


\noindent \textbf{MPI-INF-3DHP} \cite{mpi3dhp2017} is a 3D benchmark that consists of both indoor and outdoor environments. The training set includes 8 subjects, with each subject having 16 videos, resulting in a total of 1.3M video frames captured at 25 fps. The markerless motion capture system is used for providing 3D human pose annotations. The test set comprises 6 subjects performing 7 actions in both indoor and outdoor environments. Following \cite{kocabas2020vibe,tcmr,MPS-Net}, the MPJPE and PA-MPJPE are measured on valid frames, which include approximately every 10th frame, using 17 joints defined by MPI-INF3DHP. The ACC-ERR is computed using all frames.

\noindent \textbf{InstaVariety} \cite{hmmr} is a 2D human dataset curated by HMMR \cite{hmmr} , comprising videos collected from Instagram using 84 motion-related hashtags. The dataset contains 28K videos with an average length of 6 seconds, and pseudo-ground truth 2D pose annotations are acquired using OpenPose\cite{openpose}.



\noindent \textbf{PoseTrack} \cite{posetrack} is a 2D benchmark designed for multi-person pose estimation and tracking in videos. This dataset comprises 1.3K videos and 46K annotated frames, captured at varying fps around 25 fps. There are 792 videos used for the official train set, which includes 2D pose annotations for 30 frames located in the middle of each video.



\subsection{Loss Function}
% In our DiffMesh, the SMPL model \cite{SMPL:2015} is utilized for reconstructing human mesh. Given the predicted pose parameters $\theta$ and the shape parameters $\beta$, the SMPL model can return the body mesh $M \in  \mathbb{R}^{N \times 3}  $ with $N=6890$ vertices by the function $M = SMPL (\theta, \beta) $. After obtaining the body mesh $M$, the body joints $J$ can be regressed by the predefined joint regression matrix $W$, which means   $J \in  \mathbb{R}^{k \times 3} = W \cdot M  $, where $k$ is the number of joints. We follow the same setting for the loss function. 
% The overall loss during the HMR task can be defined as: 


Our DiffMesh relies on the SMPL model \cite{SMPL:2015} to reconstruct the human mesh. The SMPL model can generate the body mesh $M \in \mathbb{R}^{N \times 3} $ with $N=6890$ vertices by taking in the predicted pose parameters $\theta$ and the shape parameters $\beta$ as inputs, which can be expressed as $M = SMPL (\theta, \beta) $. Once the body mesh $M$ is obtained, the body joints $J$ can be estimated by applying the predefined joint regression matrix $W$, i.e., $J \in \mathbb{R}^{k \times 3} = W \cdot M$, where $k$ represents the number of joints. We adopt the same loss function as previous methods TCMR \cite{tcmr}.

\begin{equation}
\begin{aligned} 
  \begin{aligned}
\mathcal{L}_{HMR} &=  w_1 \| \beta - \beta^*  \| + w_2 \| \theta - \theta^*  \| + w_3 \| J - J^*  \|\\
  \end{aligned}\\
\end{aligned}
\end{equation}
\noindent where * denote the ground-truth value,  $w_1=0.5$, $w_2=10$, and $w_3=1000$.

Besides this general loss for mesh recovery, we add additional auxiliary loss as mentioned in Section 3.4 of the main paper. Our designed transformer-based diffusion model can predict the previous conditional feature $\hat{c}_{i-1}$ given the current conditional feature input  $c_{i}$. A MSE loss is applied between the ground truth $c_{i-1}$ and predicted $\hat{c}_{i-1}$:
\begin{equation}
\begin{aligned} 
  \begin{aligned}
\mathcal{L}_{aux} &=  \| c_{i-1} - \hat{c}_{i-1} \|_2^2\\
  \end{aligned}\\
\end{aligned}
\end{equation} 
This auxiliary loss contributes to the refinement of our transformer-based diffusion model during the training process. Thus, the overall loss for our DiffMesh is the sum of the $\mathcal{L}_{HMR} $ and $\mathcal{L}_{aux}$:
\begin{equation}
\begin{aligned} 
  \begin{aligned}
\mathcal{L}_{overall} &= \mathcal{L}_{HMR} +   w_4 \mathcal{L}_{aux} \\
  \end{aligned}\\
\end{aligned}
\end{equation} 
\noindent where $w_4=0.01$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{figure}[htp]
% \vspace{-5pt}
%   \centering
%   \includegraphics[width=1\linewidth]{figure/supp-diffmodel.pdf}
%   \vspace{-10pt}
%   \caption{The diffusion model in our baselines}
%   \label{fig:supp_diffmodel}
%   \vspace{-10pt}
% \end{figure}



\subsection{More Details about the Architecture}

\begin{figure}[tp]
\vspace{-5pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/supp-diffmodel.pdf}
  \vspace{-20pt}
  \caption{The diffusion model in our baselines}
  \label{fig:supp_diffmodel}
  \vspace{-10pt}
\end{figure}  

\textbf{Diffusion model in our baselines:}
The architecture of the diffusion model employed in our baselines is illustrated in Fig. \ref{fig:supp_diffmodel}. It shares similarities with the architecture within our DiffMesh, featuring two self-attention blocks designed to capture global dependencies and one cross-attention block focused on integrating information between the denoising input $x_{i}$ and the constant conditional feature $c$. In the baseline approach, as the conditional feature $c$ remains the same throughout the denoising process, there is no need to estimate the conditional feature for each subsequent denoising step. Thus, it only return the estimated noise term $n_{i-1}$.


\begin{figure*}[htp]
\vspace{-5pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/supp-transformer.pdf}
  \vspace{-15pt}
  \caption{Different design choices of the transformer architecture: (a) Only one Self-Attn Block. (b) Two Self-Attn Blocks. (c) Two Self-Attn Blocks and one Cross-Attn Block.}
  \label{fig:supp_transformer}
  \vspace{-5pt}
\end{figure*}

\textbf{Conditional features generation block:}
Our chosen backbone to extract features for both our proposed method and the baselines is ResNet-50 \cite{resnet} or DSTformer \cite{MotionBERT2022}. After extracting features from each frame $b_{i}$, where $ i \in \left\{ 1,\dots f \right\}$, using the backbone, our goal is to generate $N$ conditional features to be utilized during the reverse process. To achieve this, we pad additional $N-f$ zero features, $b_{f+1}, \cdots, b_{N}$.  Then, we combine them with the existing features, creating new features $b \in \mathbb{R} ^{N \times D}$, where $D$ represents the embedded dimension. Subsequently, we apply a transformer block \cite{Dosovitskiy2020ViT} to model these features and return the required conditional features denoted as $c \in \mathbb{R} ^{N \times D}$.


%%%%%%%%%%%%%%%%
\section{Mathematical Proof and More Experiments}
\label{exp}

\subsection{Mathematical Proof of modeling human motion as noise in diffusion model}

Our approach draws an analogy between human motion and noise, treating the motion between adjacent frames as a structured form of noise. By operating in a high-dimensional latent space, we capture the complexity of human motion, where small perturbations (or "noise") in this space can be modeled as Gaussian. This allows us to align the problem with the core principles of diffusion models.

Following Equation 6 in the main paper, we have

\begin{equation}
\scalebox{0.85}{$
\mathbb{E}_{x_0 \sim q} \left[ -\log p(x_0) \right] = \mathbb{E}_{x_0 \sim q} \left[ \log \mathbb{E}_{x_{1:T}, y_{0:T} \sim q} \frac{q(x_{1:T}, y_{0:T} | x_0)}{p(x_{0:T}, y_{0:T})} \right]
$}
\label{eq:supp1}
\end{equation}

Using Jensen's Inequality, we can bound Eq \ref{eq:supp1} by moving the expectation inside the logarithm:
\begin{equation}
\scalebox{0.85}{$
\mathbb{E}_{x_0 \sim q} \left[ -\log p(x_0) \right] \leq \mathbb{E}_{x_0, x_{1:T}, y_{0:T} \sim q} \left[ \log \frac{q(x_{1:T}, y_{0:T} | x_0)}{p(x_{0:T}, y_{0:T})} \right]
$}
\end{equation}

The forward process in DiffMesh is designed to model the human motion between adjacent frames as structured noise. This noise is Gaussian in the latent space, where the human motion patterns are simplified. We define the forward process for the latent motion as:
\begin{equation}
q(m_{t+1} | m_t) = \mathcal{N}(m_{t+1}; \sqrt{1 - \beta_t} m_t, \beta_t I)
\end{equation}

Here $m_t$ represents the motion noise at time step $t$, and $\beta_t$ controls the level of noise added to the motion between consecutive frames. This equation models the motion between frames as Gaussian perturbations in latent space.

We now need to decompose the joint probabilities \( q(x_{1:T}, y_{0:T} | x_0) \) and \( p(x_{0:T}, y_{0:T}) \) into their respective transition probabilities over time \( t \):

The forward process \( q(x_{1:T}, y_{0:T} | x_0) \) can be written as:
\begin{equation}
\scalebox{0.85}{$
q(x_{1:T}, y_{0:T} | x_0) = q(x_T | x_0) \prod_{t=2}^{T} q(x_{t-1} | x_t, x_0) \prod_{t=0}^{T} q(y_t | x_t)
$}
\end{equation}

Similarly, the reverse process \( p(x_{0:T}, y_{0:T}) \) is:
\begin{equation}
\scalebox{0.85}{$
p(x_{0:T}, y_{0:T}) = p(x_T) \prod_{t=T}^{1} p(x_{t-1} | x_t) \prod_{t=T}^{0} p(y_t | x_t)
$}
\end{equation}

Next, we focus on deriving the bound based on KL divergence. Substituting the decomposed expressions, we obtain two main terms: one for the states $x$ and another for the observations $y$

\begin{align}
\mathbb{E}_{x_0, x_{1:T}, y_{0:T} \sim q} \left[ \log \frac{q(x_T | x_0) \prod_{t=2}^{T} q(x_{t-1} | x_t, x_0)}{p(x_T) \prod_{t=T}^{1} p(x_{t-1} | x_t)} \right] \nonumber \\
+ \mathbb{E}_{x_0, x_{1:T}, y_{0:T} \sim q} \left[ \log \frac{\prod_{t=0}^{T} q(y_t | x_t)}{\prod_{t=T}^{0} p(y_t | x_t)} \right]
\end{align}

Each of these terms corresponds to the forward process (adding Gaussian motion noise) and the reverse process (denoising to recover the original human motion).

The final step is to express the result as a sum of KL divergences. For the forward and reverse processes of both the states $x$ and the observations $y$, we can represent this as:


\begin{align}
= D_{\text{KL}}(q(x_T | x_0) || p(x_T)) 
+ \mathbb{E}_q \left[ -\log p(x_0 | x_1) \right] \nonumber \\
+ \sum_{t=2}^{T} D_{\text{KL}}(q(x_{t-1} | x_t, x_0) || p(x_{t-1} | x_t)) \nonumber \\
+ \sum_{t=0}^{T} D_{\text{KL}}(q(y_t | x_t) || p(y_t | x_t))
\end{align}

\noindent Here \( D_{\text{KL}}(q(x_T | x_0) || p(x_T)) \) measures the divergence between the final denoised human mesh and the actual motion, and \( D_{\text{KL}}(q(x_{t-1} | x_t, x_0) || p(x_{t-1} | x_t)) \) measures the divergence at each intermediate step in recovering the motion, helping to maintain temporal consistency. Thus, we can derive Equation 7 in the main paper. 

\subsection{Performance on MPI-INF-3DHP dataset}

\begin{table}[htp]
    \centering
    \vspace{-5pt}
    \resizebox{1\linewidth}{!}
  {
    \begin{tabular}{c|ccc}
\hline
\multicolumn{1}{l|}{} & \multicolumn{3}{c}{MPI-INF-3DHP} \\ \hline
Methods               & MPJPE  $\downarrow$   & PA-MPJPE  $\downarrow$   & ACC-ERR  $\downarrow$   \\ \hline
VIBE \cite{kocabas2020vibe}                 & 103.9   & 68.9       & 27.3      \\
TCMR \cite{tcmr}                  & 97.6    & 63.5       & 8.5       \\
MAED \cite{MAED}                & 83.6    & 56.2       & -         \\
MPS-Net \cite{MPS-Net}              & 96.7    & 62.8       & 9.6       \\
GLoT  \cite{GLoT}                & 93.9    & 61.5       & 7.9       \\ 
% PMCE  \cite{you2023coeval}          & 79.7    & 54.5       & 7.1       \\
% PMCE(repro)           & 82.9    & 57.5       & 7.4       \\ 
\hline
DiffMesh (ours)       & \textbf{78.9}    & \textbf{54.4}       & \textbf{7.0}       \\ \hline
\end{tabular}}
\vspace{-5pt}
    \caption{Performance comparison with state-of-the-art methods on MPI-INF-3DHP dataset. All methods use pre-trained ResNet-50 \cite{resnet} (fixed weights) to extract features except MAED. }
    \label{tab:supp-mpii3d}
\vspace{-5pt}
\end{table} 

To conduct experiments on the MPI-INF-3DHP \cite{mpi3dhp2017} dataset, we follow the same setting as VIBE \cite{kocabas2020vibe}, TCMR \cite{tcmr}, and MPS-Net \cite{MPS-Net}
% , the number of input frames is set to be 16
. The input features of each frame are extracted from ResNet-50 \cite{resnet} without fine-tuning for fair comparisons. The results are shown in Fig. \ref{tab:supp-mpii3d}. Our DiffMesh consistently outperforms previous methods with significant improvement (more than 5.9 mm $\downarrow$ of MPJPE, 1.8  $\downarrow$ of PA-MPJPE, and 0.7 $\downarrow$ of ACC-ERR). This showcases the remarkable performance enhancement achieved by our approach, highlighting its potential as a state-of-the-art solution for video-based human mesh recovery across various datasets and real-world applications.


% \begin{table}
%     \centering
%     \resizebox{1\linewidth}{!}
%   {
%     \begin{tabular}{c|ccc}
% \hline
% \multicolumn{1}{l|}{} & \multicolumn{3}{c}{MPI-INF-3DHP} \\ \hline
% Methods               & MPJPE  $\downarrow$   & PA-MPJPE  $\downarrow$   & ACC-ERR  $\downarrow$   \\ \hline
% VIBE \cite{kocabas2020vibe}                 & 103.9   & 68.9       & 27.3      \\
% TCMR \cite{tcmr}                  & 97.6    & 63.5       & 8.5       \\
% MAED \cite{MAED}                & 83.6    & 56.2       & -         \\
% MPS-Net \cite{MPS-Net}              & 96.7    & 62.8       & 9.6       \\
% GLoT  \cite{GLoT}                & 93.9    & 61.5       & 7.9       \\ 
% % PMCE  \cite{you2023coeval}          & 79.7    & 54.5       & 7.1       \\
% % PMCE(repro)           & 82.9    & 57.5       & 7.4       \\ 
% \hline
% DiffMesh (ours)       & \textbf{78.9}    & \textbf{54.4}       & \textbf{7.0}       \\ \hline
% \end{tabular}}
%     \caption{Performance comparison with state-of-the-art methods on MPI-INF-3DHP dataset. All methods use pre-trained ResNet-50 \cite{resnet} (fixed weights) to extract features except MAED. }
%     \label{tab:supp-mpii3d}
%     % \vspace{-10pt}
% \end{table}



\subsection{Effectiveness of the number of input frames and additional steps}
Following the same setting as previous video-based methods such as VIBE \cite{kocabas2020vibe}, TCMR \cite{tcmr}, and MPS-Net \cite{MPS-Net}, the number of input frames $f$ is set to be 16. To further investigate the impact of the number of input frames, we conduct experiments on the 3DPW dataset given the different number of input frames. The results are shown in Table. \ref{tab:supp_frame}.

In general, the performance can be improved (lower MPJPE, PA-MPJPE, MPVPE, and ACC-ERR) when the number of input frames $f$ is increased. Specifically, when maintaining the total number of steps $N$ at 30 and varying $f$ from 8 to 16 to 24, the improvements are notable. 
% If $f$ is increased to 24 from 16, the performance can be further improved.  ( 0.2 reduction of MPVE, 1.0 reduction of MPJPE, and 0.2 reduction of ACC-ERR.). 
In our ablation study, the lowest MPVE, MPJPE, and ACC-ERR are achieved when $f=32$ with total steps of 40.  

To strike an optimal balance between efficiency and performance, it's crucial to seek improved results with a reduced total number of steps $N$. For instance, when $f=16$, the optimal $N$ is determined to be 30, demonstrating comparable results to $N=40$ at a faster processing speed. Similarly, for $f=24$, the optimal $N$ is identified as 30 based on the results.


\begin{table}[htp]
% \tiny
\scriptsize
% \renewcommand\arraystretch{1.2}
\vspace{-5pt}
\centering
  \resizebox{1\linewidth}{!}
  {
\begin{tabular}{c|cc|c|ccc}
\hline
input frames & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}steps for output \\ sequence\end{tabular}} & additional steps & Total steps & MPVE $\downarrow$ & MPJPE $\downarrow$ & ACC-ERR $\downarrow$ \\ \hline
8     & 7      & 0               & 7          &  89.8    &  77.9     &   6.9      \\
16     & 15       & 0               & 15           &  88.5    &  77.4     &   6.5  \\
24      & 23      & 0               & 23          & 87.6     & 75.2      &  6.2       \\\hline
8      & 7       & 13               & 20          & 88.6     & 76.9      &   6.7      \\
16     & 15       & 5               & 20          &  88.0    &  77.1     &  6.5       \\\hline
8     & 7        & 23               & 30          &  87.4    & 76.5      &   6.5      \\
16     & 15       & 15               & 30          &  86.4    & 75.7      &   6.1      \\
24     & 23       & 7               & 30          &  86.2   &   74.7     &   5.9      \\ \hline
16     & 15       & 25               & 40          &  87.1    & 75.6      &   6.2      \\
24     & 23       & 17               & 40          &  86.5    &  74.7     &  6.1       \\
32     & 31       & 8               & 40          &  86.0    &  74.9     &  5.8       \\ \hline
\end{tabular}
}
  \caption{Performance of the different number of input frames and the number of additional steps on the 3DPW dataset.}
\label{tab:supp_frame}
\vspace{-5pt}
\end{table}


% \vspace{-10pt}
% \subsection{Different initial distributions in diffusion}
% In Section 3.3 of the main paper, we assume that human motion will eventually reach the initial distribution $x_{end}$. We select the template mesh acquired by averaging ground-truth meshes on Human3.6M dataset \cite{lin2021metro} as the initial distribution. Here we compared the results when using pure Gaussian noise as the initial distribution. The results indicate that selecting template mesh as the initial distribution is a better choice than Pure Gaussian noise for the video-based HMR task. 

% \begin{table}[htp]
% % \tiny
% % \scriptsize
% % \renewcommand\arraystretch{1.2}
% % \vspace{-5pt}
% \centering
%   \vspace{-5pt}
%   \resizebox{0.98\linewidth}{!}
%   {
% \begin{tabular}{c|cccc}
% \hline
%      & \multicolumn{4}{c}{3DPW}         \\ \hline
% Initial distribution & MPVPE$\downarrow$ & MPJPE$\downarrow$ & PA-MPJPE$\downarrow$ & Accel$\downarrow$ \\ \hline
% Pure Gaussian noise    & 87.0     & 75.8  & 46.7 & \textbf{6.1}   \\ \hline
% Template mesh  & \textbf{86.4}     & \textbf{75.7}  & \textbf{45.6} & \textbf{6.1}   \\ \hline
% \end{tabular}
% }
% \label{tab:supp_loss}
%   \caption{Evaluation of different initial distributions on the 3DPW dataset.}
% \vspace{-5pt}
% \end{table}

\subsection{Different design choices of our transformer-based diffusion model}
As introduced in Section 3.3 of the main paper, our proposed transformer-based diffusion model consists of two self-attn blocks with one cross-attn block (also depicted in Fig. \ref{fig:supp_transformer} (c)). Given the input feature $x'_{i}$ and corresponding conditional feature $c_{i}$, the transformer-based diffusion model produces the predicted noise \textcolor{orange}{$m_{i-1}$} and the predicted previous conditional feature $\hat{c}_{i-1}$. We apply two self-attention blocks for $x'_{i}$ and  $c_{i}$ separately, then a cross-attention block is adopted to fuse the conditional features with mesh features. To validate the effectiveness, we compare this design with (a): a self-attention block applied for the concatenated features; and (b) two two self-attention blocks for $x'_{i}$ and  $c_{i}$ separately without cross-attention block. The results are shown in Table \ref{tab:supp_transformer}. Clearly, our design (c) in DiffMesh outperforms (a) and (b) for all evaluation metrics on the 3DPW dataset due to enhanced information integration using two-stream and cross-attention fusion design. 





\begin{table}[htp]
% \tiny
% \scriptsize
% \renewcommand\arraystretch{1.2}
\vspace{-5pt}
\centering
  \vspace{-5pt}
  \resizebox{1\linewidth}{!}
  {
\begin{tabular}{c|cccc}
\hline
                             & \multicolumn{4}{c}{3DPW}      \\ \hline
                             & MPVE $\downarrow$ & MPJPE $\downarrow$ & PA-MPJPE $\downarrow$ & ACC $\downarrow$ \\ \hline
(a) one self-attn            &  86.9    &  76.9     &  47.5        &  6.3   \\ \hline
(b) two self-attn            &  87.4    &  76.4     &  45.9        &  6.2   \\ \hline
(c) self-attn and cross attn &  \textbf{86.4}     & \textbf{75.7}  & \textbf{45.6} & \textbf{6.1}    \\ \hline
\end{tabular}
}
  \caption{Ablation study of transformer block design on 3DPW dataset.}
\label{tab:supp_transformer}
\vspace{-5pt}
\end{table}

% \vspace{-10pt}
%%%%%%%%%%
\subsection{Effectiveness of the auxiliary loss:} 

To validate the effectiveness of our proposed auxiliary loss, we compare the results as shown in Table \ref{tab:supp_loss}, which demonstrated that our proposed auxiliary loss can help to improve the reconstruction performance (MPJPE, PA-MPJPE, and MPJVE) and the motion smoothness (ACC-ERR). 
\begin{table}[htp]
% \tiny
\scriptsize
\renewcommand\arraystretch{1.2}
% \vspace{-5pt}
\vspace{-10pt}
\centering
  \resizebox{1\linewidth}{!}
  {
\begin{tabular}{c|cccc}
\hline
     & \multicolumn{4}{c}{3DPW}         \\ \hline
loss & MPVPE$\downarrow$ & MPJPE$\downarrow$ & PA-MPJPE$\downarrow$ & Accel$\downarrow$ \\ \hline
Without $\mathcal{L}_{aux}$    & 86.8     & 76.0  & 47.1 & 6.2   \\ \hline
With $\mathcal{L}_{aux}$   & \textbf{86.4}     & \textbf{75.7}  & \textbf{45.6} & \textbf{6.1}   \\ \hline
\end{tabular}
}
\caption{Evaluation of the combinations of loss functions on the 3DPW dataset.}
\label{tab:supp_loss}
\vspace{-10pt}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \vspace{-10pt}
\subsection{Inference Time Analysis:}

Methods like MPS-Net \cite{MPS-Net} and GLoT \cite{GLoT} only estimate the human mesh of the center frame given 16 frames as their input. Considering these methods can extract all features by their backbone once and then utilize batch processing to accelerate the inference speed, we provide a more thorough inference time comparison in Table \ref{tab:supp_fps}. 

In this experiment, the video input comprises a total of 64 frames. Upon feature extraction from the backbone (with the shape of $[64,2048]$), MPS-Net and GLoT require the creation of 64 batch input tubes $[64, 16, 2048]$ through padding and sliding window. Since their models only return the output mesh of the center frame, the output would be $[64,1, 6890,3]$, indicating output mesh vertices $[6890,3]$ across 64 frames. In contrast, our DiffMesh just needs to reshape the input $[64,2048]$ into 4 batches, resulting in the shape of $[4,16,2048]$. Consequently, the output of DiffMesh is $[4,16,6890,3]$, which is then reshaped back to $[64,6890,3]$. Based on the total processing time, our DiffMesh is more efficient than MPS-Net \cite{MPS-Net} and GLoT \cite{GLoT} since DiffMesh can output human meshes of all input frames. 




\begin{table*}[htp]
% \tiny
% \scriptsize
\renewcommand\arraystretch{1.1}
\vspace{-5pt}
\centering
  \resizebox{1\linewidth}{!}
  {
\begin{tabular}{c|c|c|c|c|c}
\hline
\begin{tabular}[c]{@{}c@{}}Video-based\\ Methods\end{tabular} & \begin{tabular}[c]{@{}c@{}}total frames for\\  video input\end{tabular} & Backbone & \begin{tabular}[c]{@{}c@{}}features after backbone are \\ reshaped for model processing\end{tabular} &  \begin{tabular}[c]{@{}c@{}}Output\\ shape \end{tabular} & \begin{tabular}[c]{@{}c@{}}processing time\\ (without backbone time)\end{tabular} \\ \hline
MPS-Net \cite{MPS-Net}                                                      & 64                                                                      & ResNet50 \cite{resnet} & {[}64,2048{]} to {[}64,16,2048{]}   & {[}64,1,6890,3{]} to {[}64,6890,3{]}                                                                 & 1.04 s                                                                            \\
GLoT \cite{GLoT}                                                         & 64                                                                      & ResNet50 \cite{resnet} & {[}64,2048{]} to {[}64,16,2048{]}    & {[}64,1,6890,3{]} to {[}64,6890,3{]}                                                                  & 1.17 s                                                                            \\ \hline
DiffMesh (ours)                                               & 64                                                                      & ResNet50 \cite{resnet} & {[}64,2048{]} to {[}4,16,2048{]}      & {[}4,16,6890,3{]} to {[}64,6890,3{]}                                                                 & 0.34 s                                                                            \\ \hline
\end{tabular}
}
\vspace{5pt}
\caption{Inference time comparison on 3DPW dataset between our DiffMesh and previous video-based HMR methods with the same hardware platform ( single NVIDIA A5000 GPU is used).}
\label{tab:supp_fps}
\vspace{-10pt}
\end{table*}

%%%%%%%%%%%%%%%%%%

% \vspace{-10pt}

\section{Human Mesh Visualization}
\label{meshvis}

We first visualize the qualitative comparison on the 3DPW \cite{pw3d2018} dataset in Fig.~\ref{fig:supp_vis0}. The circle areas highlight locations where our DiffMesh performs better than GLoT \cite{GLoT}. 



In our experimental setup, we utilize 16 input frames, and the total number of steps is set to 30. In the reverse motion process, DiffMesh outputs [$y_1, y_2 \cdots, y_{30}$] over 30 steps. For the output mesh sequence of 16 frames, we use [$y_1, y_2 \cdots, y_{16}$]. Additionally, we generate the mesh from [$y_{16}, y_{17} \cdots, y_{30}$], as visually depicted in Fig \ref{fig:supp_full_frame}. This visualization illustrates the trend of the generated human mesh gradually decoding toward the desired human mesh of each input frame.
%%%%%%%%%%%%%%



Furthermore, we show the qualitative results of DiffMesh on \textbf{in-the-wild videos} in Fig.~\ref{fig:supp_vis2}. We observe that DiffMesh demonstrates remarkable performance in reconstructing more reliable human mesh sequences with temporal consistency compared to previous methods. \textcolor{blue}{Please refer to our \textbf{video demo} for the more reconstructed mesh sequence results.}  


\section{Broader impact and limitation}
\label{Broader}
DiffMesh establishes an innovative connection between diffusion models and human motion, facilitating the generation of accurate and temporal smoothness output mesh sequences by integrating human motion into both the forward and reverse processes of the diffusion model. By enabling direct 3D human mesh reconstruction from 2D video sequences, DiffMesh eliminates the dependency on additional motion sensors and equipment, thereby streamlining the process and reducing costs.

However, despite its advancements, DiffMesh is not without limitations. Similar to previous methods, DiffMesh may face challenges in scenarios with substantial occlusions, resulting in the production of unrealistic mesh outputs. To address this issue, further exploration into spatial-temporal interactions within the human body is warranted, serving as a focal point for our future research. Additionally, DiffMesh may encounter difficulties in rare and complex pose scenarios due to the constraints of limited training data, highlighting the necessity for ongoing development and refinement efforts.



\begin{figure}[htp]
% \vspace{-5pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/vis-supp0.pdf}
  \vspace{-5pt}
  \caption{Qualitative comparison on the 3DPW dataset}
  \label{fig:supp_vis0}
  \vspace{-5pt}
\end{figure}

% \begin{figure*}[htp]
% \vspace{-5pt}
%   \centering
%   \includegraphics[width=1\linewidth]{figure/vis-supp2.pdf}
%   \vspace{-5pt}
%   \caption{Qualitative results of our DiffMesh on in-the-wild videos. \textcolor{blue}{Please refer to our \textbf{video demo} for the more reconstructed mesh sequences results.}}
%   \label{fig:supp_vis}
%   \vspace{-5pt}
% \end{figure*}

%%%%%%%%%%%%%%%%%
\begin{figure*}[htp]
\vspace{-5pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/full_frame.pdf}
  \vspace{-5pt}
  \caption{Visualization of decoding steps during the reverse motion process.}
  \label{fig:supp_full_frame}
  \vspace{-5pt}
\end{figure*}

\begin{figure*}[htp]
\vspace{-10pt}
  \centering
  \includegraphics[width=1\linewidth]{figure/vis-supp3.pdf}
  \vspace{-5pt}
  \caption{Other qualitative results of our DiffMesh on in-the-wild videos. \textcolor{blue}{Please refer to our \textbf{video demo} for the more reconstructed mesh sequences results.}}
  \label{fig:supp_vis2}
  \vspace{-10pt}
\end{figure*}

\clearpage
\newpage










%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
