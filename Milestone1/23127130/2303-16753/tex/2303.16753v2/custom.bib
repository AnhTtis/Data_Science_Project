@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}
@article{bowman2015large,
  title={A large annotated corpus for learning natural language inference},
  author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},
  journal={arXiv preprint arXiv:1508.05326},
  year={2015}
}
@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{hendrycks2016gaussian,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}
@article{gao2020compressing,
  title={Compressing deep neural networks by matrix product operators},
  author={Gao, Ze-Feng and Cheng, Song and He, Rong-Qiang and Xie, Zhi-Yuan and Zhao, Hui-Hai and Lu, Zhong-Yi and Xiang, Tao},
  journal={Physical Review Research},
  volume={2},
  number={2},
  pages={023300},
  year={2020},
  publisher={APS}
}
@inproceedings{liu2021enabling,
  title={Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators},
  author={Liu, Peiyu and Gao, Ze-Feng and Zhao, Wayne Xin and Xie, Zhi-Yuan and Lu, Zhong-Yi and Wen, Ji-Rong},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={5388--5398},
  year={2021}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{zhang2020revisiting,
  title={Revisiting few-sample BERT fine-tuning},
  author={Zhang, Tianyi and Wu, Felix and Katiyar, Arzoo and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:2006.05987},
  year={2020}
}

@inproceedings{rajpurkar2016squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}
@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{kudo2018sentencepiece,
    title = "{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
    author = "Kudo, Taku  and
      Richardson, John",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-2012",
    doi = "10.18653/v1/D18-2012",
    pages = "66--71",
    abstract = "This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at \url{https://github.com/google/sentencepiece}.",
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}
@inproceedings{sarlin2020superglue,
  title={Superglue: Learning feature matching with graph neural networks},
  author={Sarlin, Paul-Edouard and DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4938--4947},
  year={2020}
}

@article{oseledets2011tensor,
  title={Tensor-train decomposition},
  author={Oseledets, Ivan V},
  journal={SIAM Journal on Scientific Computing},
  volume={33},
  number={5},
  pages={2295--2317},
  year={2011},
  publisher={SIAM}
}
@article{novikov2015tensorizing,
  title={Tensorizing neural networks},
  author={Novikov, Alexander and Podoprikhin, Dmitry and Osokin, Anton and Vetrov, Dmitry},
  journal={arXiv preprint arXiv:1509.06569},
  year={2015}
}
@article{garipov2016ultimate,
  title={Ultimate tensorization: compressing convolutional and fc layers alike},
  author={Garipov, Timur and Podoprikhin, Dmitry and Novikov, Alexander and Vetrov, Dmitry},
  journal={arXiv preprint arXiv:1611.03214},
  year={2016}
}
@inproceedings{yang2017tensor,
  title={Tensor-train recurrent neural networks for video classification},
  author={Yang, Yinchong and Krompass, Denis and Tresp, Volker},
  booktitle={International Conference on Machine Learning},
  pages={3891--3900},
  year={2017},
  organization={PMLR}
}
@article{yu2017long,
  title={Long-term forecasting using tensor-train rnns},
  author={Yu, Rose and Zheng, Stephan and Anandkumar, Anima and Yue, Yisong},
  journal={Arxiv},
  year={2017}
}

@article{gao2020compressinglstm,
  title={Compressing LSTM Networks by Matrix Product Operators},
  author={Gao, Ze-Feng and Sun, Xingwei and Gao, Lan and Li, Junfeng and Lu, Zhong-Yi},
  journal={arXiv preprint arXiv:2012.11943},
  year={2020}
}
@article{sun2020model,
  title={A Model Compression Method with Matrix Product Operators for Speech Enhancement},
  author={Sun, Xingwei and Gao, Ze-Feng and Lu, Zhong-Yi and Li, Junfeng and Yan, Yonghong},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={28},
  pages={2837--2847},
  year={2020},
  publisher={IEEE}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}
@article{sun2020mobilebert,
  title={Mobilebert: a compact task-agnostic bert for resource-limited devices},
  author={Sun, Zhiqing and Yu, Hongkun and Song, Xiaodan and Liu, Renjie and Yang, Yiming and Zhou, Denny},
  journal={arXiv preprint arXiv:2004.02984},
  year={2020}
}
@article{zafrir2019q8bert,
  title={Q8bert: Quantized 8bit bert},
  author={Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
  journal={arXiv preprint arXiv:1910.06188},
  year={2019}
}
@article{jiao2019tinybert,
  title={Tinybert: Distilling bert for natural language understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  journal={arXiv preprint arXiv:1909.10351},
  year={2019}
}
@article{liu2020fastbert,
  title={Fastbert: a self-distilling bert with adaptive inference time},
  author={Liu, Weijie and Zhou, Peng and Zhao, Zhe and Wang, Zhiruo and Deng, Haotang and Ju, Qi},
  journal={arXiv preprint arXiv:2004.02178},
  year={2020}
}
@article{zhang2021moefication,
  title={MoEfication: Conditional Computation of Transformer Models for Efficient Inference},
  author={Zhang, Zhengyan and Lin, Yankai and Liu, Zhiyuan and Li, Peng and Sun, Maosong and Zhou, Jie},
  journal={arXiv preprint arXiv:2110.01786},
  year={2021}
}
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@article{zuo2022moebert,
  title={MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation},
  author={Zuo, Simiao and Zhang, Qingru and Liang, Chen and He, Pengcheng and Zhao, Tuo and Chen, Weizhu},
  journal={arXiv preprint arXiv:2204.07675},
  year={2022}
}
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{rajbhandari2022deepspeed,
  title={Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale},
  author={Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
  journal={arXiv preprint arXiv:2201.05596},
  year={2022}
}

@inproceedings{rasley2020deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3505--3506},
  year={2020}
}
@inproceedings{chen2020lotterybert,
  author    = {Tianlong Chen and
               Jonathan Frankle and
               Shiyu Chang and
               Sijia Liu and
               Yang Zhang and
               Zhangyang Wang and
               Michael Carbin},
  editor    = {Hugo Larochelle and
               Marc'Aurelio Ranzato and
               Raia Hadsell and
               Maria{-}Florina Balcan and
               Hsuan{-}Tien Lin},
  title     = {The Lottery Ticket Hypothesis for Pre-trained {BERT} Networks},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/b6af2c9703f203a2794be03d443af2e3-Abstract.html},
  timestamp = {Tue, 09 Aug 2022 17:04:09 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/ChenFC0ZWC20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{michel2019prunehead,
  author    = {Paul Michel and
               Omer Levy and
               Graham Neubig},
  editor    = {Hanna M. Wallach and
               Hugo Larochelle and
               Alina Beygelzimer and
               Florence d'Alch{\'{e}}{-}Buc and
               Emily B. Fox and
               Roman Garnett},
  title     = {Are Sixteen Heads Really Better than One?},
  booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
               on Neural Information Processing Systems 2019, NeurIPS 2019, December
               8-14, 2019, Vancouver, BC, Canada},
  pages     = {14014--14024},
  year      = {2019},
  url       = {https://proceedings.neurips.cc/paper/2019/hash/2c601ad9d2ff9bc8b282670cdd54f69f-Abstract.html},
  timestamp = {Mon, 16 May 2022 15:41:51 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/MichelLN19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{wang2020prunestructure,
  author    = {Ziheng Wang and
               Jeremy Wohlwend and
               Tao Lei},
  editor    = {Bonnie Webber and
               Trevor Cohn and
               Yulan He and
               Yang Liu},
  title     = {Structured Pruning of Large Language Models},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages     = {6151--6162},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
  url       = {https://doi.org/10.18653/v1/2020.emnlp-main.496},
  doi       = {10.18653/v1/2020.emnlp-main.496},
  timestamp = {Wed, 23 Mar 2022 10:11:55 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/WangWL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{reid2021subformer,
  author    = {Machel Reid and
               Edison Marrese{-}Taylor and
               Yutaka Matsuo},
  editor    = {Marie{-}Francine Moens and
               Xuanjing Huang and
               Lucia Specia and
               Scott Wen{-}tau Yih},
  title     = {Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative
               Transformers},
  booktitle = {Findings of the Association for Computational Linguistics: {EMNLP}
               2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November,
               2021},
  pages     = {4081--4090},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://doi.org/10.18653/v1/2021.findings-emnlp.344},
  doi       = {10.18653/v1/2021.findings-emnlp.344},
  timestamp = {Thu, 20 Jan 2022 10:02:36 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/ReidMM21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{neil2019adapter,
  author    = {Neil Houlsby and
               Andrei Giurgiu and
               Stanislaw Jastrzebski and
               Bruna Morrone and
               Quentin de Laroussilhe and
               Andrea Gesmundo and
               Mona Attariyan and
               Sylvain Gelly},
  editor    = {Kamalika Chaudhuri and
               Ruslan Salakhutdinov},
  title     = {Parameter-Efficient Transfer Learning for {NLP}},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {2790--2799},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v97/houlsby19a.html},
  timestamp = {Tue, 11 Jun 2019 15:37:38 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/HoulsbyGJMLGAG19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{zhang2019fixup,
  author    = {Hongyi Zhang and
               Yann N. Dauphin and
               Tengyu Ma},
  title     = {Fixup Initialization: Residual Learning Without Normalization},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=H1gsz30cKX},
  timestamp = {Sun, 08 Aug 2021 16:40:51 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ZhangDM19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{roberts2020exploring,
  title={Exploring transfer learning with t5: the text-to-text transfer transformer},
  author={Roberts, Adam and Raffel, Colin},
  journal={Accessed on},
  pages={23--07},
  year={2020}
}
@article{kaplan2020scaling,
  author    = {Jared Kaplan and
               Sam McCandlish and
               Tom Henighan and
               Tom B. Brown and
               Benjamin Chess and
               Rewon Child and
               Scott Gray and
               Alec Radford and
               Jeffrey Wu and
               Dario Amodei},
  title     = {Scaling Laws for Neural Language Models},
  journal   = {CoRR},
  volume    = {abs/2001.08361},
  year      = {2020},
  url       = {https://arxiv.org/abs/2001.08361},
  eprinttype = {arXiv},
  eprint    = {2001.08361},
  timestamp = {Wed, 03 Jun 2020 10:55:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2001-08361.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{zhang2022minivit,
  author    = {Jinnian Zhang and
               Houwen Peng and
               Kan Wu and
               Mengchen Liu and
               Bin Xiao and
               Jianlong Fu and
               Lu Yuan},
  title     = {MiniViT: Compressing Vision Transformers with Weight Multiplexing},
  booktitle = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2022, New Orleans, LA, USA, June 18-24, 2022},
  pages     = {12135--12144},
  publisher = {{IEEE}},
  year      = {2022},
  url       = {https://doi.org/10.1109/CVPR52688.2022.01183},
  doi       = {10.1109/CVPR52688.2022.01183},
  timestamp = {Tue, 18 Oct 2022 08:35:28 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/ZhangPWLXFY22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{nouriboriji@2022minialbert,
  author    = {Mohammadmahdi Nouriborji and
               Omid Rohanian and
               Samaneh Kouchaki and
               David A. Clifton},
  title     = {MiniALBERT: Model Distillation via Parameter-Efficient Recursive Transformers},
  journal   = {CoRR},
  volume    = {abs/2210.06425},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2210.06425},
  doi       = {10.48550/arXiv.2210.06425},
  eprinttype = {arXiv},
  eprint    = {2210.06425},
  timestamp = {Tue, 18 Oct 2022 15:06:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2210-06425.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{wang@2022deepnet,
  author    = {Hongyu Wang and
               Shuming Ma and
               Li Dong and
               Shaohan Huang and
               Dongdong Zhang and
               Furu Wei},
  title     = {DeepNet: Scaling Transformers to 1, 000 Layers},
  journal   = {CoRR},
  volume    = {abs/2203.00555},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2203.00555},
  doi       = {10.48550/arXiv.2203.00555},
  eprinttype = {arXiv},
  eprint    = {2203.00555},
  timestamp = {Tue, 20 Dec 2022 07:37:25 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2203-00555.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{rabeeh2021hyperformer,
  author    = {Rabeeh Karimi Mahabadi and
               Sebastian Ruder and
               Mostafa Dehghani and
               James Henderson},
  editor    = {Chengqing Zong and
               Fei Xia and
               Wenjie Li and
               Roberto Navigli},
  title     = {Parameter-efficient Multi-task Fine-tuning for Transformers via Shared
               Hypernetworks},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational
               Linguistics and the 11th International Joint Conference on Natural
               Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers), Virtual
               Event, August 1-6, 2021},
  pages     = {565--576},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://doi.org/10.18653/v1/2021.acl-long.47},
  doi       = {10.18653/v1/2021.acl-long.47},
  timestamp = {Mon, 09 Aug 2021 16:25:37 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/MahabadiR0H20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{gao2022parameter,
  author    = {Ze{-}Feng Gao and
               Peiyu Liu and
               Wayne Xin Zhao and
               Zhong{-}Yi Lu and
               Ji{-}Rong Wen},
  editor    = {Nicoletta Calzolari and
               Chu{-}Ren Huang and
               Hansaem Kim and
               James Pustejovsky and
               Leo Wanner and
               Key{-}Sun Choi and
               Pum{-}Mo Ryu and
               Hsin{-}Hsi Chen and
               Lucia Donatelli and
               Heng Ji and
               Sadao Kurohashi and
               Patrizia Paggio and
               Nianwen Xue and
               Seokhwan Kim and
               Younggyun Hahm and
               Zhong He and
               Tony Kyungil Lee and
               Enrico Santus and
               Francis Bond and
               Seung{-}Hoon Na},
  title     = {Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained
               Language Models},
  booktitle = {Proceedings of the 29th International Conference on Computational
               Linguistics, {COLING} 2022, Gyeongju, Republic of Korea, October 12-17,
               2022},
  pages     = {3263--3273},
  publisher = {International Committee on Computational Linguistics},
  year      = {2022},
  url       = {https://aclanthology.org/2022.coling-1.288},
  timestamp = {Thu, 13 Oct 2022 17:29:38 +0200},
  biburl    = {https://dblp.org/rec/conf/coling/Gao0ZLW22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Tinny2019probe,
  author    = {Ian Tenney and
               Patrick Xia and
               Berlin Chen and
               Alex Wang and
               Adam Poliak and
               R. Thomas McCoy and
               Najoung Kim and
               Benjamin Van Durme and
               Samuel R. Bowman and
               Dipanjan Das and
               Ellie Pavlick},
  title     = {What do you learn from context? Probing for sentence structure in
               contextualized word representations},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=SJzSgnRcKX},
  timestamp = {Thu, 25 Jul 2019 14:25:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/TenneyXCWPMKDBD19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{huang2020initialize,
  author    = {Xiao Shi Huang and
               Felipe P{\'{e}}rez and
               Jimmy Ba and
               Maksims Volkovs},
  title     = {Improving Transformer Optimization Through Better Initialization},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning,
               {ICML} 2020, 13-18 July 2020, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  pages     = {4475--4483},
  publisher = {{PMLR}},
  year      = {2020},
  url       = {http://proceedings.mlr.press/v119/huang20f.html},
  timestamp = {Tue, 15 Dec 2020 17:40:19 +0100},
  biburl    = {https://dblp.org/rec/conf/icml/HuangPBV20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{xue2022bamboo,
  author    = {Fuzhao Xue and
               Jianghai Chen and
               Aixin Sun and
               Xiaozhe Ren and
               Zangwei Zheng and
               Xiaoxin He and
               Xin Jiang and
               Yang You},
  title     = {Deeper vs Wider: {A} Revisit of Transformer Configuration},
  journal   = {CoRR},
  volume    = {abs/2205.10505},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2205.10505},
  doi       = {10.48550/arXiv.2205.10505},
  eprinttype = {arXiv},
  eprint    = {2205.10505},
  timestamp = {Sat, 17 Dec 2022 01:15:27 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2205-10505.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{gong@2019efficient_stack,
  author    = {Linyuan Gong and
               Di He and
               Zhuohan Li and
               Tao Qin and
               Liwei Wang and
               Tie{-}Yan Liu},
  editor    = {Kamalika Chaudhuri and
               Ruslan Salakhutdinov},
  title     = {Efficient Training of {BERT} by Progressively Stacking},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning,
               {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {2337--2346},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v97/gong19a.html},
  timestamp = {Tue, 24 Sep 2019 17:13:21 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/GongHLQWL19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{xu2021optimizing,
  author    = {Peng Xu and
               Dhruv Kumar and
               Wei Yang and
               Wenjie Zi and
               Keyi Tang and
               Chenyang Huang and
               Jackie Chi Kit Cheung and
               Simon J. D. Prince and
               Yanshuai Cao},
  editor    = {Chengqing Zong and
               Fei Xia and
               Wenjie Li and
               Roberto Navigli},
  title     = {Optimizing Deeper Transformers on Small Datasets},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational
               Linguistics and the 11th International Joint Conference on Natural
               Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers), Virtual
               Event, August 1-6, 2021},
  pages     = {2089--2102},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://doi.org/10.18653/v1/2021.acl-long.163},
  doi       = {10.18653/v1/2021.acl-long.163},
  timestamp = {Thu, 13 Jan 2022 07:39:44 +0100},
  biburl    = {https://dblp.org/rec/conf/acl/Xu0YZT0CPC20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{chowdhery2022palm,
  author    = {Aakanksha Chowdhery and
               Sharan Narang and
               Jacob Devlin and
               Maarten Bosma and
               Gaurav Mishra and
               Adam Roberts and
               Paul Barham and
               Hyung Won Chung and
               Charles Sutton and
               Sebastian Gehrmann and
               Parker Schuh and
               Kensen Shi and
               Sasha Tsvyashchenko and
               Joshua Maynez and
               Abhishek Rao and
               Parker Barnes and
               Yi Tay and
               Noam Shazeer and
               Vinodkumar Prabhakaran and
               Emily Reif and
               Nan Du and
               Ben Hutchinson and
               Reiner Pope and
               James Bradbury and
               Jacob Austin and
               Michael Isard and
               Guy Gur{-}Ari and
               Pengcheng Yin and
               Toju Duke and
               Anselm Levskaya and
               Sanjay Ghemawat and
               Sunipa Dev and
               Henryk Michalewski and
               Xavier Garcia and
               Vedant Misra and
               Kevin Robinson and
               Liam Fedus and
               Denny Zhou and
               Daphne Ippolito and
               David Luan and
               Hyeontaek Lim and
               Barret Zoph and
               Alexander Spiridonov and
               Ryan Sepassi and
               David Dohan and
               Shivani Agrawal and
               Mark Omernick and
               Andrew M. Dai and
               Thanumalayan Sankaranarayana Pillai and
               Marie Pellat and
               Aitor Lewkowycz and
               Erica Moreira and
               Rewon Child and
               Oleksandr Polozov and
               Katherine Lee and
               Zongwei Zhou and
               Xuezhi Wang and
               Brennan Saeta and
               Mark Diaz and
               Orhan Firat and
               Michele Catasta and
               Jason Wei and
               Kathy Meier{-}Hellstern and
               Douglas Eck and
               Jeff Dean and
               Slav Petrov and
               Noah Fiedel},
  title     = {PaLM: Scaling Language Modeling with Pathways},
  journal   = {CoRR},
  volume    = {abs/2204.02311},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2204.02311},
  doi       = {10.48550/arXiv.2204.02311},
  eprinttype = {arXiv},
  eprint    = {2204.02311},
  timestamp = {Tue, 16 Aug 2022 23:07:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2204-02311.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{you2020lamb,
  author    = {Yang You and
               Jing Li and
               Sashank J. Reddi and
               Jonathan Hseu and
               Sanjiv Kumar and
               Srinadh Bhojanapalli and
               Xiaodan Song and
               James Demmel and
               Kurt Keutzer and
               Cho{-}Jui Hsieh},
  title     = {Large Batch Optimization for Deep Learning: Training {BERT} in 76
               minutes},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=Syx4wnEtvH},
  timestamp = {Sat, 17 Dec 2022 01:15:29 +0100},
  biburl    = {https://dblp.org/rec/conf/iclr/YouLRHKBSDKH20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}