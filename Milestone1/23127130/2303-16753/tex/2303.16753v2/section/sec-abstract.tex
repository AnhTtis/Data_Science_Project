\begin{abstract}
% 背景
% Pre-trained language models have recently drawn much attention due to their high model capacity and remarkable performance. 
% However, it suffers from huge computational and storage costs when we further scale the model size. 
% To alleviate this problem, we propose a parameter-efficient architecture based on matrix decomposition, which can be easily scaled to deep models without increasing parameters or computational cost by two key techniques.
% (i) We proposed a parameter-efficient architecture based on matrix product operator~(MPO) which can decompose weight matrices into central tensors and auxiliary tensors. More specifically, we share the central tensors across transformers that reduce the total parameters significantly. 
% ~(ii) We theoretically investigate the training instability issue and propose initialization methods for both central and auxiliary tensors.
% We demonstrate these techniques to work well across multiple task settings including standard supervised, few-shot, and multitask learning.
% By using fewer parameters~(\ie 8M fewer than 12-layer BERT) and affordable computational cost~(\ie 3.8 days), we are able to train a 48-layer MPOBERT and outperform several competing models.
% --- v2
\ignore{
Pre-trained language models have recently gained widespread attention due to their high capacity and exceptional performance. However, as the size of these models increases, so do the computational and storage costs associated with them. To address this problem, we propose a parameter-efficient architecture based on matrix decomposition. This architecture is capable of scaling deep models without increasing the number of parameters or computational costs by using two key techniques. 
% First, we introduce a matrix product operator (MPO) that can decompose weight matrices into central tensors and auxiliary tensors, which significantly reduces the total number of parameters. 
Firstly, we introduce a matrix product operator~(MPO) that can decompose weight matrices into central tensors and auxiliary tensors, and we share the central tensors across layers, significantly reducing the total number of parameters.
Secondly, we theoretically investigate the issue of training instability and develop initialization methods for both central and auxiliary tensors. Through extensive experimentation, we demonstrate the effectiveness of our proposed techniques across a range of task settings, including standard supervised, few-shot, and multitask learning. Our model, the 48-layer MPOBERT, requires 8M fewer parameters and 3.8 days of computation time compared to the 12-layer BERT model, yet still outperforms several competing models.}
% 问题与挑战
% 解决方案
% 实验总结
% 另外，额外尝试。

% 基于Transformer堆叠的PLM模型取得了很好的效果但是参数量非常大。已有研究发现这种层重复堆叠的方法非常参数低效

% 最新的方法通过共享所有Transformer层的参数（e.g., ALBERT）实现了comparable results。但这一方法却在深层条件下效果出现了明显的下降。我们发现这种共享策略忽视了Transformer层之间的联系，导致降低模型的表达能力且限制了该方法在更深层PLM的拓展.

% 为了解决以上问题，我们提出了一种基于MPO技术的参数共享方法，Parameter Efficient Transformer，强化了Transformer层之间的语义关系，得到了一个参数更小且更深层的PLM。

% 具体而言，我们的方法主要有2个技术创新点：（1）我们借助MPO分解，共享包含核心信息的CT参数，通过增加AT来提升模型层数，而总参数量又不显著增加，使得构建共享的深层网络成为可能；（2）为了优化PET结构，我们改进了MPOP方法实现，设计了CT-warm策略，仅需要xx% steps即可收敛。

% 我们以1/n的参数构建了12层、24层、48层深度的BERT模型，并且在任务1、任务2上取得了更好的效果。另外，在MPOBERT的框架下构建的深度模型相比较Transformer-based方法收敛更快。




% #################################################
% 预训练语言模型在许多NLP任务上取得了令人瞩目的成就。
% 已有的工作显示在PLMs的模型尺寸以及下游任务微调表现上存在scaling法则。
% 然而，PLM巨量的参数导致很难再资源有限的场景下增大模型参数。
% 在本文中，我们结合矩阵分解与参数共享的方法，提出了一个参数高效的预训练方法来增大PLM的模型深度。具体来说，我们引入了MPO分解对参数矩阵的信息进行重组，并在所有Transformer层中共享使用一个公共的中心张量（包含矩阵的主要信息）。在不同层中通过辅助张量保持层间多样性。
% 另外，我们通过理论推导提出了一个更加高效的初始化方案，可以使得模型训练更加稳定。
% 丰富的实验说明了我们的方法的有效性，
Pre-trained language models have shown remarkable performance across a wide range of NLP applications.
Existing work has demonstrated that there exists a scaling law between the size of PLMs and fine-tuning performance.
% However, the large amount of parameters in PLM makes increasing model parameters in resource-limited circumstances challenging.
However, the large number of parameters in PLMs presents challenges in resource-limited settings.
% In this paper, we present a parameter-efficient pre-training approach for scaling PLM by combining matrix decomposition with the parameter-sharing strategy. 
In this paper, we propose a parameter-efficient pre-training approach that utilizes matrix decomposition and parameter-sharing strategies to scale PLMs.
In particular, we adopt the matrix product operator decomposition to rearrange the information of the parameter matrix and share the central tensor~(containing the major information) across all Transformer layers. The layer-specific auxiliary tensors~(containing the supplementary information) are used to enhance the adaptation flexibility.
Furthermore, we derived a more efficient initialization approach, which may improve model training stability.
Extensive experiments have demonstrated the effectiveness of our proposed model in reducing the model size and achieving highly competitive performance
~(\ie with fewer parameters than BERT$_{\rm{BASE}}$, we successfully scale the model depth by a factor of 4$\times$ and even achieve 0.1 points higher than BERT$_{\rm{LARGE}}$ for GLUE score).

\end{abstract}