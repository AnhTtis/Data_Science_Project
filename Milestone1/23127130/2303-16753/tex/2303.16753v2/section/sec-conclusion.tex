\section{Conclusion}
% We develop MPOBERT, a parameter-efficient pre-trained language model that allows for fewer parameters and efficient training.
% MPOBERT develops a novel mechanism with which we can easily scale BERT to deep models without increasing parameters or computational costs.
% During training, we propose initialization methods for both the central tensors and the auxiliary tensors based on our theoretical analysis to alleviate the training instability issue.
% We validate the effectiveness via supervised, few-shot and multitask experiments. 
% With fewer and less training costs, MPOBERT outperforms several competing models.
% --v2
We develop MPOBERT, a parameter-efficient pre-trained language model that allows for the efficient scaling of deep models without the need for additional parameters or computational resources. 
We achieve this by introducing an MPO-based Transformer layer and sharing the central tensors across layers. During training, we propose initialization methods for the central and auxiliary tensors, which are based on theoretical analysis to address training stability issues. 
The effectiveness of MPOBERT is demonstrated through various experiments, such as supervised, multitasking, and few-shot where it consistently outperforms other competing models.
