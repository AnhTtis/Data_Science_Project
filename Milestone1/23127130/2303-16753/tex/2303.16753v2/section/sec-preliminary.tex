\section{Preliminary}
In this paper, scalars, vectors and matrices are denoted by lowercase letters  (\eg $a$,  $\Vector{v}$ and  $\Matrix{M}$) respectively.
The high-order~(order three or higher) tensors are denoted by boldface Euler script letters (\eg $\Tensor{T}$). 
An $n$-order tensor $\Tensor{T}_{i_1,i_2,...i_n}$ can be considered as an~(potentially multidimensional) array with $n$ indices $\{ i_1,i_2,...,i_n \}$.


\subsection{Matrix Product Operators}
\label{subsec-matrix-product-operators}
We will give the procedure of the matrix product operator decomposition method.
\paragraph{Matrix Product Operator Decomposition}
Originating from quantum many-body physics~\cite{gao2020compressing}, the MPO decomposition can effectively reorganize and aggregate the information of the matrix. The parameter matrix is decomposed into a central tensor and a set of auxiliary tensors~\citep{liu2021enabling, gao2022parameter}.

To clarify the MPO decomposition process, we assume that a weight matrix $\Matrix{W}\in \mathbb{R}^{I\times J}$ is a matrix with size $I\times J$.
Given two arbitrary factorizations of its dimensions into natural numbers, we can reshape and transpose this matrix into an $n$-dimension tensor $\Matrix{W}_{i_1,\dots, i_n, j_1, \dots, j_n}$ in which:
\begin{equation}
\small
    \prod_{k=1}^{n} i_k = I, \quad \prod_{k=1}^{n}j_k = J.
\end{equation}
This decomposition can be written as:
\begin{equation}
    \small
   \Matrix{W}_{i_1,\dots , i_n, j_1, \dots, j_n} = \Tensor{T}^{(1)}[i_1, j_1]\cdots \Tensor{T}^{(n)}[i_n, j_n],
\label{eq:mpo-decom}
\end{equation}
where the $\Tensor{T}^{(k)}[i_k, j_k]$ is a 4-dimensional tensor with size $d_{k-1}\times i_k \times j_k \times d_k$ in which $d_k$ is a bond dimension linking $T^{(k)}$ and $T^{(k+1)}$ with $d_0=d_n=1$.
with size $d_{k-1}\times i_k \times j_k \times d_k$ in which $\prod_{k=1}^{n}i_k=I, \prod_{k=1}^{n}j_k=J$ and $d_0=d_n=1$.
% Following~\citep{liu-etal-2021-enabling}, the tensor right in the middle is the \emph{central tensor}, and the rest is the \emph{auxiliary tensor}.

According to ~\citep{gao2020compressing}, the original matrix $\Matrix{M}$ may be precisely reconstructed using tensor contraction without the connecting bond $\{d_k\}_{k=1}^m$ being truncated.
After MPO decomposition, the central tensor can encode the essential data from the original matrix, while the other auxiliary tensors serve as its complement.




\subsection{BERT}
\label{subsec-bert}
We will provide a brief review of the BERT~\citep{devlin2018bert} pre-training approach as well as some of the training options.
\paragraph{Architecture and Setup}
BERT uses the transformer architecture~\citep{vaswani2017attention}. 
We use an MPO-based transformer architecture with $L$ layers. 
Each block uses $A$ attention heads and hidden dimension $H$.
In the setup step, BERT uses the concatenation of two segments as input~(\ie $ x_1, \dots, x_N$ and $y_1, \dots, y_M$, $N$ and $M$ are the length of input and output respectively, which constrained such that $M+N<T$, where $T$ is a parameter that controls maximum sequence length during training).
Note that each segment usually consists of more than one natural sentence.
Special tokens are used to distinguish the two segments as a single input sequence to BERT:
$[CLS], x_1, \dots, x_N, [SEP], y_1,\dots, y_M,[EOS]$.
% 模型会先在几个无标签大数据集进行预训练，紧接着在下游任务上进行微调。
The models are first pre-trained on several large unlabeled datasets and subsequently fine-tuned on downstream tasks.
\paragraph{Training Objectives}
BERT uses two objectives during the pre-training stage, \ie masked language modeling~(MLM) and next sentence prediction~(NSP).

In the input sequence, MLM is to replace a random sample of tokens with special tokens $[MASK]$. Predicting the cross-entropy loss of the masked tokens is the objective of MLM. 15\% of the input tokens are consistently chosen for replacement by BERT. Out of the chosen tokens, 80\% are changed to $[MASK]$, 10\% are left alone, and 10\% are changed to randomly chosen lexical tokens.

In order to determine if two text fragments from the original work agree with one another, NSP, a binary classification loss, is utilized. Extracting consecutive sentences from a text corpus results in the creation of uplifting instances. Pairing pieces from other sources produce negative instances. The same probability is used to sample both positive and negative cases. The objective of the NSP is to enhance the performance of downstream tasks, such as natural language inference~\citep{bowman2015large}.

\paragraph{Datasets and Optimization}
Following~\citep{devlin2018bert}, BERT is trained on 16GB of uncompressed text from English Wikipedia and BOOKCORPUS~\citep{zhu2015aligning} combined.
BERT is optimized with Adam~\citep{kingma2014adam} and using GELU activation function~\citep{hendrycks2016gaussian}.
The learning rate is warmed up over the first 10,000 steps to a peak value of 1e-4, and then linearly decayed.
In the pre-training stage, models are trained for 1,000,000 updates with mini-batches containing $B=256$ sequences of maximum length $T=512$ tokens.
