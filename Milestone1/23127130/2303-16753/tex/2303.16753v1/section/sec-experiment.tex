\section{Experiments}
In this section, we first set up the experiments and then evaluate the efficiency of MPOBERT on a variety of tasks with different model settings.
% report the results and the detailed analysis to demonstrate the effectiveness of MPOBERT. 
% For all experiments, we use NVIDIA Tesla V100.

\subsection{Experimental Setup}
\label{sec-experimental-setup}
\paratitle{Pre-training Setup}.
% For the datasets, we follow BERT~\citep{devlin2018bert} setup and use BOOKCORPUS~\citep{zhu2015aligning} and English Wikipedia~\citep{devlin2018bert} for pre-training our models in order to keep the comparison as meaningful as possible.
For the architecture, we denote the number of layers as $L$, the hidden size as $H$, and the number of self-attention heads as $A$. We report results on four model sizes: \textbf{MPOBERT$_{\textbf{12}}$}~($L$=12, $H$=768, $A$=12), \textbf{MPOBERT$_{\textbf{24}}$} ($L$=24, $H$=1024, $A$=16), \textbf{MPOBERT$_{\textbf{48}}$}~($L$=48, $H$=1024, $A$=16) and \textbf{MPOBERT$_{\textbf{48+}}$} that 
% only adds two sets of central tensors on  MPOBERT$_{48}$.
implement cross-layer parameter sharing in three distinct groups as discussed in subsection~\ref{subsec-mpobased_scaling}.
% We pre-train all of the models from scratch with batch size of 4096 for 10$k$ steps.
We pre-train all of the models with a batch size of 4096 for 10$k$ steps. Our code will be released after the review period.

\paratitle{Fine-tuning Datasets}.
To evaluate the performance of our model, we conduct experiments on the GLUE~\citep{wang2018glue} and SQuAD v1.1~\citep{rajpurkar2016squad} benchmarks.
% GLUE benchmark covers multiple datasets~(MNLI, QNLI, QQP, CoLA, RTE, MRPC, SST-2)~\footnote{In line with~\citet{raffel2020exploring}, we do not test WNLI due to its adversarial character with respect to the training set.}. 
% The SQuAD is a collection of 100$k$ crowd-sourced question/answer pairs. Given a question and a passage, the task is to predict the answer text span in the passage. 
Since fine-tuning is typically fast, we run an exhaustive parameter search and choose the model that performs best on the development set to make predictions on the test set. 
We include the details in the Appendix(see Appendix~\ref{add-detail_dataset} for the datasets and Appendix~\ref{add-detail_metric} for evaluation metrics)
% The results are summarized in Table~\ref{tab-main_results}.
% We report the results of both the development set and the test set in GLUE. 
% Since the original test sets are not accessible, we divide the original validation set in half and use one half for validation and the other for the test for datasets with fewer than 10,000 samples~(RTE, MRPC, STS-B, CoLA)~\citep{zhang2020revisiting}.
\begin{table*}[ht]
\centering
\small
\begin{tabular}{l|rrrrrrrrr|rr}                                     
\toprule[1pt]
\small
\multirow{2}{*}{Experiments}    & \makebox[0.04\textwidth][c]{MRPC} & \makebox[0.04\textwidth][c]{SST-2} & \makebox[0.04\textwidth][c]{CoLA} & \makebox[0.04\textwidth][c]{RTE}   & \makebox[0.04\textwidth][c]{STS-B} & \makebox[0.04\textwidth][c]{QQP} & \makebox[0.04\textwidth][c]{MNLI} & \makebox[0.04\textwidth][c]{QNLI} & \makebox[0.04\textwidth][c]{SQuAD} &  \makebox[0.04\textwidth][c]{Avg.} &\makebox[0.04\textwidth][c]{\#To~(M)}  \\ 
                                & F1   & Acc.  & Mcc.   & Acc.   & Spear. & F1/Acc. & Acc. & Acc. & F1\\ \midrule
\rowcolor{gray!10}\multicolumn{12}{c}{\it \textbf{Development set}}\\
\rowcolor{gray!10}\multicolumn{12}{l}{\textbf{Tiny Models}~(\rm{\#To < 50M)}}\\
ALBERT$_{12}$                      & 89.0           & 90.6              & 53.4             & 71.1             & 88.2             & -/89.1             & 84.5             & 89.4             & 89.3          & 82.7             & 11 \\
ALBERT$_{24}$                      & 84.6           & \underline{93.6}  & 52.5             & \textbf{79.8}    & 90.1             & -/88.1             & 85.0             & \underline{91.7} & 90.6          & \underline{84.0} & 18 \\ 
\textcolor{purple}{MPOBERT$_{12}$} & \underline{90.3} & 92.3            & \underline{55.2} & 71.8             & \underline{90.5} & \underline{-/90.1} & \underline{84.7} & 91.2             & 90.1          & \underline{84.0} & 20 \\ 
\textcolor{purple}{MPOBERT$_{24}$} & \textbf{90.3}  & \textbf{94.4}     & \textbf{58.1}    & \underline{75.5} & \textbf{91.1}    & \textbf{-/90.2}    & \textbf{87.0}    & \textbf{92.6}    & \textbf{92.3} & \textbf{85.7}    & 46 \\\midrule
\rowcolor{gray!10}\multicolumn{12}{l}{\textbf{Small Models}~(\rm{50M < \#To < 100M)}}\\
T5$_{12}$                           & \underline{89.2}  & 94.7  & \underline{53.5}  & \underline{71.7} & \underline{91.2}   & \textbf{-/91.1}    & \textbf{87.8}    & \textbf{93.8}     & \underline{90.0}    & \underline{84.8} & 60\\ 
\textcolor{purple}{MPOBERT$_{48}$}  & \textbf{90.8}     & 94.7	& \textbf{58.3}     & \textbf{77.3}	   & \textbf{91.4}      & \underline{-/89.5} & \underline{86.3} & \underline{92.0}  & \textbf{92.3}       & \textbf{85.8}    & 75\\ \midrule
\rowcolor{gray!10}\multicolumn{12}{l}{\textbf{Base Models}~(\rm{\#To > 100M)}}\\
BERT$_{12}$                         & 90.7              & 91.7             & 48.9             & 71.4             & \underline{91.0} & \underline{-/90.8} & 83.7             & 89.3             & 88.5             & 82.9             & 110  \\
XLNet$_{12}$                        & 85.3              & \underline{94.4} & 49.3             & 63.9             & 85.6             & -/90.7             & \textbf{90.9}    & 91.8             & 90.2             & 82.5             & 117 \\
RoBERTa$_{12}$                      & \textbf{91.9}     & 92.2             & \textbf{59.4}    & 72.2             & 89.4             & \textbf{-/91.2}    & \underline{88.0} & \textbf{92.7}    & 91.2             & \underline{85.4} & 125\\
BART$_{12}$                         & \underline{91.4}  & 93.8             & 56.3             & \underline{79.1} & 89.9             & \underline{-/90.8} & 86.4             & \underline{92.4} & \textbf{91.6}    & 82.8             & 140 \\
\textcolor{purple}{MPOBERT$_{48+}$} & 89.7              & \textbf{94.4}    & \underline{57.4} & \textbf{79.8}    & \textbf{91.1}    & -/89.3             & 87.1             & \underline{92.4} & \underline{91.4} & \textbf{86.0}    & 102\\ \midrule\midrule
\rowcolor{gray!10}\multicolumn{12}{c}{\it \textbf{Test set}}\\
\rowcolor{gray!10}\multicolumn{12}{l}{\textbf{Tiny Models}~(\rm{\#To < 50M)}}\\
ALBERT$_{12}$                        & 89.2             & 93.2             & \underline{53.6} & 70.2            & \underline{87.3}  & 70.3/-            & 84.6              & \underline{92.5}   & 89.3             & 81.1              & 11\\
ALBERT$_{24}$                        & 88.7             & \underline{94.0} & 51.7	          & \textbf{73.7}	& 86.9              & 69.1/-	        & 84.9	            & 91.8	             & \underline{90.6}	& \underline{81.2}              & 18 \\
MobileBERT$_{24}$$\blacklozenge$     & 88.8             & 92.6             & 51.1             & 70.4            & 84.8              & \underline{70.5/-}& 83.3              & 91.6               & 90.3             & 80.4              & 25\\
\textcolor{purple}{MPOBERT$_{12}$}   & \textbf{89.2}    & 91.9             & 52.7             & 70.6            & 87.1              & 69.6/-            & \underline{85.0}  & 91.0               & 90.1             & 80.8              & 20     \\  
\textcolor{purple}{MPOBERT$_{24}$}   & \underline{89.0}	& \textbf{94.5}    & \textbf{55.5}	  & \underline{73.4}& \textbf{88.2}     & \textbf{71.0/-}	& \textbf{86.3}     & \textbf{93.0}      & \textbf{92.3}    & \textbf{82.6}     & 46   \\\midrule
\rowcolor{gray!10}\multicolumn{12}{l}{\textbf{Small Models}~(\rm{50M < \#To < 100M)}}\\
T5$_{12}$                           & \underline{89.7}  & 91.8              & 41.0             & 69.9               & 85.6              & 70.0/-            & 82.4              & 90.3              & 90.0              & 78.7              & 60\\ 
TinyBERT$_{6}$$\clubsuit$           & 87.3              & \underline{93.1}  & \underline{51.1} & \underline{70.0}   & \underline{83.7}  & \textbf{71.6/-}   & \underline{84.6}  & \underline{90.4}  & \underline{87.5}  & \underline{79.9}  & 67\\
DistilBERT$_{6}$$\clubsuit$         & 86.9              & 92.5              & 49.0             & 58.4               & 81.3              & 70.1/-            & 82.6              & 88.9              & 86.2              & 77.3              & 67\\
\textcolor{purple}{MPOBERT$_{48}$}  & \textbf{90.0}	    & \textbf{94.0}	    & \textbf{55.0}    & \textbf{74.0}	    & \textbf{88.7}     & \underline{71.0/-}& \textbf{86.5}     & \textbf{91.8}     & \textbf{92.3}     & \textbf{82.6}     & 75\\ \midrule
\rowcolor{gray!10}\multicolumn{12}{l}{\textbf{Base Models}~(\rm{\#To > 100M)}}\\
BERT$_{12}$$\spadesuit$              & 88.9             & 93.5              & 52.1          & 66.4              & 85.8              & 71.2/-            & 84.6             & 90.5              & 88.5               & 79.1     & 110    \\
XLNet$_{12}$                         & 89.2             & \underline{94.3}  & 47.3          & 66.5              & 85.4              & \underline{71.9/-}& \underline{87.1} & 91.4              & 90.2               & 80.4      & 117   \\
RoBERTa$_{12}$                       & 89.9	            & 93.2	            & \textbf{57.9} & 69.9	            & \underline{88.3}	& \textbf{72.5/-}   & \textbf{87.7}	   & \underline{92.5}	   & 91.2	& \underline{82.6}      & 125\\
BART$_{12}$                          & 89.9             & 93.7              & 49.6          & \underline{72.6}  & 86.9              & 71.7/-            & 84.9             & 92.3              & \textbf{91.6}               & 81.5      & 140   \\
\textcolor{purple}{MPOBERT$_{48+}$}  & \textbf{89.9}    & \textbf{94.5}     & \underline{56.0}  & \textbf{74.5} & \textbf{88.4}     & 70.5/-            & 86.5             & \textbf{92.6}  & \underline{91.4}      & \textbf{82.7}  & 102\\ \bottomrule
\end{tabular}
\caption{Performance comparison of different models on natural language understanding tasks~(in percent). ``\# To~(M)'' denote the number~(in millions) of total parameters. 
We compare MPOBERT with PLMs~(\ie BERT and ALBERT) and Parameter-efficient Transformers~(\ie MobileBERT, TinyBERT and DistilBERT), respectively. The best and the second-best performance in each task are highlighted in bold and underlined.
$\blacklozenge$: Experimental results by~\citet{sun2020mobilebert}.
$\clubsuit$: Experimental results by~\citet{jiao2019tinybert}.
$\spadesuit$: Experimental results by~\citet{devlin2018bert}.}
\label{tab-main_results}
\end{table*}

\paratitle{Baseline Models}.
We compare our proposed MPOBERT to the existing competitive deep PLMs and parameter-efficient models. In order to make fair comparisons, we divide the models into three major categories based on their model sizes: 

$\bullet$~{Tiny Models~(\rm{\#To < 50M}).} ALBERT$_{12}$~\cite{lan2019albert} is the most representative PLM that achieves competitive results with only 11M.
% We fine-tune all of the parameters in deep PLMs including BERT~\citep{devlin2018bert} and ALBERT~\citep{lan2019albert}. 

$\bullet$~{Small models~(50M< \#To <100M).}
% T5$_{12}$ is a small variant of T5~\cite{raffel2020exploring} which has only 6 encoder layers and 6 decoder layers. In addition, there are three parameter-efficient Transformer models that have similar parameters, namely MobileBERT~\citep{sun2020mobilebert}, DistilBERT~\citep{sanh2019distilbert} and TinyBERT~\citep{jiao2019tinybert}. We compare with these compressed models to show the benefit of scaling to deeper models over compressing large models to small variants.
We consider PLMs~(T5$_{12}$) and compressed models~(MobileBERT~\citep{sun2020mobilebert}, DistilBERT~\citep{sanh2019distilbert} and TinyBERT~\citep{jiao2019tinybert}).

$\bullet$~{Base models~(\#To > 100M).} We compare with BERT$_{12}$, XLNet$_{12}$, RoBERTa$_{12}$ and BART$_{12}$ for this category. 
Note that we only include the base variants that have similar model sizes in order to make a fair comparison. 
% More details about the comparison with the strongest variants are described in Appendix~\ref{app-exp}.
% $\bullet$~\underline{Parameter-efficient Models.} We compare with parameter-efficient models based on compression techniques including MobileBERT~\citep{sun2020mobilebert}, DistilBERT~\citep{sanh2019distilbert} and TinyBERT~\citep{jiao2019tinybert}.

More details about the baseline models are described in Appendix~\ref{add-detail_baseline}. 
% We use the same environment for all approaches without any additional techniques like label smoothing and multi-task learning.

\subsection{Main Results}
\paratitle{Fully-supervised setting}.
We present the results of MPOBERT and other baseline models on GLUE and Squad for fine-tuning in Table~\ref{tab-main_results}. 
% We find that MPOBERT demonstrates its superiority in terms of layer-specific parameters and parameter efficiency. 

% --v1
% First, we compare MPOBERT to PLMs with different model sizes. For tiny models, MPOBERT outperforms ALBERT on all datasets. Compared to the best ALBERT variant, ALBERT$_{24}$, MPOBERT$_{24}$ achieves prominent gains for both the development set~(85.7 vs. 84.0) and the test set~(82.6 vs. 81.2). These considerable gains clearly demonstrate the benefit of layer-specific parameters~(\ie the auxiliary tensors and layer-specific adapters). For small models, MPOBERT consistently achieves better results than T5$_{12}$. For base models, MPOBERT still achieves comparable results while having fewer parameters. By zooming in on specific tasks, we find that the baseline model performance varies widely across tasks with lower data size~(\ie RTE and CoLA). Large models tend to have better results~(63.6 for RoBERTa$_{12}$ on CoLA and 79.1 for BART$_{12}$ on RTE) due to sufficient model capacity. By contrast, our MPOBERT$_{48}$ with the least parameters still achieves comparable~(57.4 on CoLA) or even better~(79.8 on RTE) results. 
% This demonstrates that the deep model constructed by our method can effectively stimulate the capability of PLMs without significantly increasing the number of model parameters. This is good news for PLMs that currently promote scaling law and can be expanded to a larger PLM by a more parameter-efficient way to explore the capability of models.
% --v2
% We first compare MPOBERT to PLMs with different model sizes. We find that MPOBERT outperforms ALBERT, and achieves considerable gains compared to ALBERT${24}$ on both the development~(85.7 \emph{v.s.} 84.0) and test sets~(82.6 \emph{v.s.} 81.2). This demonstrates the benefit of layer-specific parameters~(\ie the auxiliary tensors and layer-specific adapters) in MPOBERT. In addition, MPOBERT consistently achieves better results than T5$_{12}$ for small models and comparable results to larger models while having fewer parameters. 
% In particular, we consider that for the same number of layers~($L$=12), we can still obtain comparable results compared to the PLMs in small models, or even better~(+1.7 for BERT$_{12}$ and +0.4 for XLNet$_{12}$).
% This further illustrates the superiority and efficiency of our method in constructing deep models without significantly increasing the number of parameters.
% % --v3
% In our comparison of MPOBERT to other PLMs with different model sizes, we found that MPOBERT demonstrates its parameter efficiency when compared to other PLMs within the same size category.
% Specifically, for tiny models, MPOBERT$_{24}$ outperforms ALBERT$_{24}$, and achieves substantial improvements on both the development~(85.7 \emph{v.s.} 84.0) and test sets~(82.6 \emph{v.s.} 81.2), highlighting the benefits of MPOBERT's layer-specific parameters, such as the auxiliary tensors and layer-specific adapters.  
% Furthermore, MPOBERT consistently achieves better results than T5$_{12}$ for small models and comparable results to larger models while having fewer parameters. 
% Additionally, we found that MPOBERT also demonstrates significant benefits when scaling along the model depth. For instance, when considering models with $L$=12 layers, MPOBERT achieves comparable results or even outperforms~(+1.7 for BERT$_{12}$ and +0.4 for XLNet$_{12}$) PLMs while having fewer parameters. 
% Overall, these results demonstrate the superiority and efficiency of our method in constructing deep models without significantly increasing the number of parameters.

% --v4
% We first compare MPOBERT to other PLMs with varying model sizes. We found that MPOBERT demonstrates its superiority in terms of layer-specific parameters and parameter efficiency. 
% Specifically, for tiny models, MPOBERT$_{24}$ outperforms ALBERT$_{24}$, and achieves substantial improvements on both the development~(85.7 \emph{v.s.} 84.0) and test sets~(82.6 \emph{v.s.} 81.2), highlighting the benefits of increased capacity from layer-specific parameters~(\ie the auxiliary tensors and layer-specific adapters). Furthermore, MPOBERT demonstrates significant benefits of scaling along the model depth with layer-specific parameters. For instance, MPOBERT$_{48}$ consistently achieves better results than T5$_{12}$ for small models and comparable results to other larger 12-layer PLMs with a reduced number of parameters.
% Additionally, we found that MPOBERT also demonstrates its parameter efficiency when compared to other PLMs within the same model depth. For instance, when considering models with $L$=12 layers, MPOBERT achieves comparable results or even outperforms~(+1.7 for BERT$_{12}$ and +0.4 for XLNet$_{12}$) PLMs while having fewer parameters. 

% Second, we compare MPOBERT to parameter-efficient Transformers. We find that MPOBERT outperforms MobileBERT, TinyBERT, and DistilBERT on almost all considered datasets. This further proves the advantage of the parameter-efficient scaling along the model depth over compressing existing large PLMs to smaller models.

% Overall, these results demonstrate the superiority and efficiency of our method in constructing deep models without significantly increasing the number of parameters.
%  ---v5
% We first compare MPOBERT to other PLMs with varying model sizes. 
% Specifically, for tiny models, MPOBERT$_{24}$ outperforms ALBERT$_{24}$, and achieves substantial improvements on both the development~(85.7 \emph{v.s.} 84.0) and test sets~(82.6 \emph{v.s.} 81.2), highlighting the benefits of increased capacity from layer-specific parameters~(\ie the auxiliary tensors and layer-specific adapters). Furthermore, MPOBERT demonstrates significant benefits of scaling along the model depth with layer-specific parameters. For instance, MPOBERT$_{48}$ consistently achieves better results than T5$_{12}$ for small models and comparable results to other larger 12-layer PLMs with a reduced number of parameters.

% Second, we compare MPOBERT to parameter-efficient Transformers. We find that MPOBERT outperforms MobileBERT, TinyBERT, and DistilBERT on almost all considered datasets. This further proves the advantage of the parameter-efficient scaling along the model depth over compressing existing large PLMs to smaller models.

% Finally, we found that MPOBERT also demonstrates its parameter efficiency when compared to other PLMs within the same model depth. For instance, when considering models with $L$=12 layers, MPOBERT achieves comparable results or even outperforms~(+1.7 for BERT$_{12}$ and +0.4 for XLNet$_{12}$) PLMs while having fewer parameters. 
% --- v6
Firstly, we evaluate MPOBERT's performance in comparison to other models with similar numbers of parameters. In particular, for tiny models, MPOBERT$_{24}$ outperforms ALBERT$_{24}$, and achieves substantial improvements on both the development set~(85.7 \emph{v.s.} 84.0) and test sets~(82.6 \emph{v.s.} 81.2). This highlights the benefits of increased capacity from layer-specific parameters~(\ie the auxiliary tensors and layer-specific adapters) in MPOBERT. Furthermore, for small and base models, 48-layer MPOBERT consistently achieves better results than T5$_{12}$ and all parameter-efficient models, while also achieving comparable results to other 12-layer PLMs with a reduced number of parameters. This demonstrates the significant benefits of scaling along the model depth with layer-specific parameters in MPOBERT.

Secondly, we assess MPOBERT's parameter efficiency by comparing it to other PLMs within the same model depth. For instance, when considering models with $L$=12 layers, MPOBERT achieves comparable results or even outperforms~(+1.7 for BERT$_{12}$ and +0.4 for XLNet$_{12}$) PLMs while having fewer parameters. This further highlights the advantages of MPOBERT's parameter-efficient approach in constructing deep models.

\paratitle{Multitask Fine-tuning Setting}.
% multitask finetuning是啥。核心是否可以强化多任务之间的辅助弱化多任务之间的干扰。
To demonstrate the effectiveness of our proposed parameter-sharing model in learning shared representations across multiple tasks, we fine-tune MPOBERT, BERT and ALBERT on the multitask GLUE benchmark and report the results in Table~\ref{tab:multi-task}. 
% In order to illustrate the effectiveness of MPOBERT, 
Specifically, we design two groups of experiments.~(1) Deep vs. shallow models. Comparing with BERT$_{12}$, MPOBERT$_{48}$ has much deeper Transformer layers but still fewer total parameters~(\ie 75M vs. 110M). We find that MPOBERT$_{48}$ achieves 1.4 points higher on average GLUE score than BERT$_{12}$.~(2) Central tensors sharing vs. all weight sharing. Comparing with ALBERT$_{12}$, MPOBERT$_{12}$ only shares part of weights, \ie central tensors, while ALBERT$_{12}$ shares all of the weights. 
% Since previous studies~\citep{rabeeh2021hyperformer, gao2022parameter} have observed the positive transfer effects provided by sharing parameters in the multi-task fine-tuning setting, we only focus on those tasks where the difference in task performance is greater than 2 points~(\ie MRPC, CoLA, and RTE).
% We have shown that sharing central tensors may improve the results more than sharing all weights~(89.9 \emph{v.s.} 89.4 for MRPC).
We find that sharing central tensors may effectively improve the average results than sharing all weights~(82.0 \emph{v.s.} 81.4 for MRPC).
% while also mitigating the performance decrease~(54.9 \emph{v.s.} 40.7 for CoLA) that would otherwise occur.
% 为了说明MPOBERT的有效性，我设计了2种对比变种，效果体现在表格中。（1）对比BERT和MPOBERT_48，为了说明增加深度对多任务训练的优势。我们发现，（1）对比ALBERT和MPOBERT_12为了说明，为了说明相比较全参数共享的优势。我们发现，
\begin{table}[t]
\centering
\small
\begin{tabular}{lrrrr} 
\toprule
    \multicolumn{1}{c}{\multirow{1}{*}{Datasets}} & \multicolumn{1}{c}{B$_{12}$} & \multicolumn{1}{c}{M$_{48}$} & \multicolumn{1}{c}{M$_{12}$} & \multicolumn{1}{c}{A$_{12}$} \\    \midrule
        MNLI~(Acc.)                  &83.9  & 85.4   & 82.8  & 82.7\\
        QNLI~(Acc.)                  &90.8  & 91.1   & 90.0  & 89.4\\
        SST-2~(Acc.)                 &91.7  & 93.0   & 90.9  & 90.6\\
        RTE~(Acc.)                   &81.2  & 82.0   & 79.8  & 79.1\\
        QQP~(Acc.)                   &91.2  & 87.6   & 90.4  & 89.7\\
        CoLA~(Mcc.)                  &53.6  & 54.9   & 45.0  & 35.9\\
        MRPC~(F1)                  &84.2  & 91.8   & 89.9  & 89.2\\
        STS-B~(Spear.)             &87.4  & 89.0   & 86.9  & 87.5\\\midrule
        Avg.                        &83.0  & 84.4   & 82.0  & 80.5\\ 
         {\#To~(M)}                 &110   & 75     & 20    & 11\\
\bottomrule
\end{tabular}
\caption{Performance of multi-task learning on GLUE benchmark obtained by fine-tuning BERT$_{12}$~(B$_{12}$), MPOBERT$_{48}$~(M$_{48}$), MPOBERT$_{12}$~(M$_{12}$) and ALBERT$_{12}$~(A$_{12}$)~(in percent).}
\label{tab:multi-task}
\end{table}

\paratitle{Few-shot Learning Setting}.
\begin{table}[t]
\small
\begin{tabular}{l|ccc|ccc}
\midrule
\multicolumn{1}{c}{}                        & \multicolumn{3}{c}{SST-2} & \multicolumn{3}{c}{MNLI}            \\ \midrule
\multicolumn{1}{c}{Shots~(K)} & 10      & 20    & 30              & 10    & 20    & 30    \\ \midrule
BERT$_{12}$                              & 54.8  & \underline{59.7}  & \underline{61.6}      & \textbf{37.0}  & 35.6  & 35.7  \\ \midrule
ALBERT$_{12}$                            & \underline{56.7}  & 59.3  & 60.0      & 36.3  & 35.6  & \underline{36.5}   \\ \midrule
MPOBERT$_{12}$                           & \textbf{58.9}  & \textbf{65.4}  & \textbf{64.6}      & \underline{36.7}  & \textbf{36.7}  & \textbf{37.1}   \\ \midrule
\end{tabular}
% \caption{Few-shot performance of BERT, ALBERT and MPOBERT.}
\caption{Comparison of few-shot performance.}
\label{tab-few_shot}
\end{table}
% ---v2
We evaluate the performance of our proposed model, MPOBERT, in few-shot learning setting~\cite{huang-etal-2022-clues} on two tasks, SST-2 and MNLI, using a limited number of labeled examples. Results in Table~\ref{tab-few_shot} show that MPOBERT outperforms BERT, which suffers from over-fitting, and ALBERT, which does not benefit from its reduced number of parameters. These results further demonstrate the superiority of our proposed model in exploiting the potential of large model capacity under limited data scenarios.

\begin{figure}[t]
\centering
\subfigure[Pre-training from scratch]{
\begin{minipage}[t]{0.5\columnwidth}
\label{fig2:left}
\centering
\includegraphics[width=\columnwidth]{section/figs/scratch2.pdf} 
\end{minipage}%
}%
\subfigure[Continual Pre-training]{
\begin{minipage}[t]{0.5\columnwidth}
\label{fig2:right}
\centering
\includegraphics[width=\columnwidth]{section/figs/initialize2.pdf} 
\end{minipage}%
}
\caption{Comparison of the SST-2 accuracy achieved through pre-training from scratch and pre-training with the initialization of decomposed ALBERT weights.}
\label{fig:fig2}
\end{figure}
\subsection{Detailed Analysis}
\paratitle{Analysis of Initialization Methods}.
% This experiment was performed to exclude the effect of initialized pre-trained weight on the final results. We initialized MPOBERT with the local tensors from ALBERT and then we continue to train both models in the pre-training datasets. We compare the downstream performance on GLUE shown in Table~\ref{tab-continue}. For ALBERT, the gains from continued training are negligible. On the contrary, MPOBERT achieves obvious improvement at the first 10$k$ training steps. This is a reassuring result that demonstrates the improvement of MPOBERT is not brought about by initialized pre-trained weights.
% This experiment is performed to exclude the effect of initialized pre-trained weight on the fine-tuning results. We plot the performance of the model on the downstream tasks at different training steps. Specifically, we initialized MPOBERT with different initialization methods~(Xavier in~\ref{fig2:left} and decomposed weights of ALBERT in~\ref{fig2:right}) and then pre-trained it. For pre-training from scratch, MPOBERT needs around 50$k$ steps to achieve similar performance with BERT$_{BASE}$ while initializing with ALBERT can significantly speed up the convergence and achieve obvious improvement at the first 10$k$ training steps. On the contrary, the gains of continual pre-training for ALBERT are negligible. This is a reassuring result that demonstrates the improvement of MPOBERT is not brought about by initialized pre-trained weights.
This experiment aims to exclude the effect of initialized pre-trained weights on fine-tuning results. We plot the performance of the model on SST-2 \emph{w.r.t} training steps. In particular, we compare the performance of MPOBERT using different initialization methods (Xavier in Fig.~\ref{fig2:left} and decomposed weights of ALBERT in Fig.~\ref{fig2:right}) for pre-training. The results demonstrate that pre-training MPOBERT from scratch requires around 50$k$ steps to achieve performance comparable to BERT$_{\rm{BASE}}$, while initializing with the decomposed weights of ALBERT significantly accelerates convergence and leads to obvious improvements within the first 10$k$ training steps. In contrast, the gains from continual pre-training for ALBERT are negligible. These results provide assurance that the improvements observed in MPOBERT are not solely attributed to the use of initialized pre-trained weights.
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.48\textwidth]{section/figs/linguistic_v3.pdf}
    \caption{A visualization of layer-wise linguistic patterns. Each column represents a probing task, and each row represents a Transformer layer. The red dashed box indicates the layer that performs best.}
    \label{fig:linguistic}
\end{figure}

\begin{table}[ht]
\centering
\small
\begin{tabular}{lrrrr}                                                                                              
\toprule
Experiment      & SST-2 & RTE     & MRPC  &\#To~(M) \\ \midrule
MPOBERT$_{12}$  & 92.8  & 72.9    & 91.8  & 20.0\\ \midrule
w/o Adapter     & 92.3  & 71.8    & 90.3  & 19.4\\  
w/o PS    & 91.4  & 67.9    & 85.8  & 11.9\\ \bottomrule
\end{tabular}
\caption{Ablation study on the SST-2, RTE, and MRPC datasets~(in percent).}
\label{tab-ablation}
\end{table}
\paratitle{Ablation Analysis}.
% To verify the effectiveness of each component, we perform the ablation study using the SST-2, RTE, and MRPC datasets.
% We choose accuracy as the assessment metric and consider removing the layer-specific adapter~(short as Adapter) and cross-layer parameter sharing strategy~(short as PS), respectively.
% The ablation results are shown in Table~\ref{tab-ablation}. 
% We can see that removing any component would lead to a decrease in the model performance. 
% It demonstrates the effectiveness of each component in our strategy.
% ---v2
% To evaluate the contribution of each component to the performance of the MPOBERT model, 
% we conduct an ablation study by removing either the layer-specific adapter or the cross-layer parameter-sharing strategy and show the results in Table~\ref{tab-ablation}. 
% The results indicate that removing either component leads to a decrease in the model's performance, demonstrating the effectiveness of each component in our proposed strategy.
% ---v3
To assess the individual impact of the components in our MPOBERT model, we conduct an ablation study by removing either the layer-specific adapter or the cross-layer parameter-sharing strategy. The results, displayed in Table~\ref{tab-ablation}, indicate that the removal of either component results in a decrease in the model's performance, highlighting the importance of both components in our proposed strategy. While the results also indicate that cross-layer parameter sharing plays a more crucial role in the model's performance.
\begin{table}[]
\centering
\small
\begin{tabular}{lrrrrr}
\toprule
Rank                & SST-2     & RTE     & MRPC  &\#To~(M)      \\ \midrule
4                   & 91.9      & 69.7    & 88.2  & 19.7        \\  
8                   & 92.8      & 72.9    & 91.8  & 20.0        \\ 
64                  & 91.6      & 69.3    & 88.1  & 24.3        \\ \bottomrule
\end{tabular}
\caption{Comparison of different adapter ranks on three GLUE tasks~(in percent). ``Rank'' denotes the adapter rank in MPOBERT.}
\label{tab-rank}
\end{table}

\paratitle{Performance Comparison \emph{w.r.t} Adapter Rank}.
To compare the impact of the adapter rank in layer-specific adapters on MPOBERT's performance, we trained MPOBERT with different ranks~(4,8 and 64) and evaluate the model on downstream tasks in Table~\ref{tab-rank}. The results demonstrate that a rank of 8 is sufficient for MPOBERT, which further shows the necessity of layer-specific adapters. However, we also observe a decrease in the performance of the variant with adapter rank 64. This illustrates that further increasing the rank may increase the risk of over-fitting in fine-tuning process. Therefore, we set a rank of 8 for MPOBERT in the main results.

\paratitle{Analysis of Linguistic Patterns}.
% We aim to compare the difference in linguistic patterns captured by MPOBERT, BERT and ALBERT. 
% Following~\citet{Tinny2019probe}, we use a suite of probing tasks to quantify where specific types of linguistic information are encoded. Specifically, the probing tasks are divided into three categories to probe surface, syntactic and semantic information, respectively. 
% That is the representations that encode more information will exhibit better-probing task performance.
% Fig.~\ref{fig:linguistic} shows a difference in the distribution of linguistic information captured by Transformer layers.
% We find that BERT encodes more local syntax in low layers while capturing more complex semantics in high layers. However, ALBERT does not exhibit such a monotonic increase. The topmost layer of ALBERT is the best while other layers perform evenly~(which we expected, since the weights in all layers are shared).
% Different from all-layer sharing in ALBERT, MPOBERT keeps layer-wise parameters~(\ie the layer-wise auxiliary tensors and adapters) and shares information only in central tensors. Thus, we observe not only similar layer-wise behavior with BERT~(\ie task 0,2,4) but also better results for lower Transformer layers~(\ie task 3). 
% ---v2
To investigate the linguistic patterns captured by MPOBERT, BERT, and ALBERT, we conduct a suite of probing tasks, following the methodology of~\citet{Tinny2019probe}. These tasks are designed to evaluate the encoding of surface, syntactic, and semantic information in the models' representations. The results, shown in Fig.~\ref{fig:linguistic}, reveal that BERT encodes more local syntax in lower layers and more complex semantics in higher layers, while ALBERT does not exhibit such a clear trend.
% MPOBERT, on the other hand, uses layer-wise parameters and shares information only through central tensors. As a result, it exhibits similar layer-wise behavior to BERT in some tasks~(\ie task 0,2,4), and improved results in lower layers for others~(\ie task 3). Overall, these results demonstrate that MPOBERT captures linguistic information differently than other models, and its layer-wise parameters play an important role in this difference.
However, MPOBERT exhibits similar layer-wise behavior to BERT in some tasks~(\ie task 0,2,4), and improved results in lower layers for others~(\ie task 3) which is similar to ALBERT. The result demonstrates that MPOBERT captures linguistic information differently than other models, and its layer-wise parameters play an important role in this difference.