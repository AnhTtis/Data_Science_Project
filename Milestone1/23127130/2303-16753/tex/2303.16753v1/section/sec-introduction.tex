\section{Introduction}
% 堆叠Transformer模型的方法已经在各个领域展现出来了卓越的成果（\cite）。但是这种方法带来了巨大的计算和存储成本，限制了PLM在下游任务的微调和部署。
% 通过设计更加简单的结构，可以强制模型通过更少的参数学习到更多的信息，实现模型压缩，并且已经取得了成功。
% 急需一种紧凑的模型结构设计，来实现更少的参数量同时保持模型性能的可以有效提升的策略来强化现有的模型效果
% Large pre-trained language models~(PLMs) have shown remarkable success in various fields, 
% Recent years have witnessed a scaling law between the size of large pre-trained language models~(PLMs) and the performance on different downstream NLP tasks, 
Recently,  pre-trained language models~(PLMs) have achieved huge success in a variety of NLP tasks by exploring ever \emph{larger} model architecture~\cite{raffel2020exploring, radford2019language}.
It has been shown that there potentially exists a scaling law between the model size and model capacity for PLMs~\cite{kaplan2020scaling}, attracting many efforts to enhance the performance by scaling model size~\citep{chowdhery2022palm, wang@2022deepnet}. 

% 现有的的多数方法都在研究通过加宽网络的方式来增大模型的参数量~（即将隐藏层的参数扩大），而对模型加深的研究非常的少。
\begin{figure}[t]
    % \centerings
    % \hspace{-10pt}
    \includegraphics[width=0.5\textwidth]{section/figs/intro7_ppt_2.pdf}
    \caption{A comparison of our model and representative PLMs in the three dimensions of \emph{model size}, \emph{model depth}, and \emph{performance}~(measured on GLUE score). }
    \label{fig:plms_comparison}
\end{figure}
As a straightforward approach, we can directly increase the layer number of Transformer networks  for improving the model capacity~\cite{wang@2022deepnet,huang2020initialize}. 
While, a very deep architecture typically corresponds to a significantly large model size, leading to high costs in both computation and storage~\cite{gong@2019efficient_stack}.  
And, it is difficult to deploy deep networks in resource-limited settings, though it usually has a stronger model capacity.  
%It is common to increase the model depth to scale the model size in deep learning models, but huge parameters of PLMs impose high computational and storage costs, limiting the application of PLMs in low-resource settings.
% Moreover, if we consider PLMs with more than 12 transformer layers as deep models.
Therefore, there is an urgent need for developing a parameter-efficient way for scaling the model depth. 
%a compact model structure design~(\eg 24 and 48 layers) to achieve both affordable parameters and superior model performance. 

\ignore{
Basically speaking, existing model scaling methods can be categorized into two major ways: enlarging the model width~\cite{} or depth~\cite{wang@2022deepnet}\footnote{Both ways can be jointly used for model scaling. }.  For model width, sparse MoEs architectures have been successfully applied to widen Transformer-based networks~\cite{}. While, for model depth,   existing studies mainly focus on the training of very deep Transformer networks~\cite{wang@2022deepnet}. In this paper, we focus on \emph{scaling model depth} based on Transformer architectures. 
}

\ignore{Most current approaches focus on scaling Transformers by widening the network~(\ie growing the parameters of the hidden layer)~\cite{devlin2018bert, raffel2020exploring, radford2018improving, liu2019roberta}, with relatively little research on model deepening~\cite{wang@2022deepnet}.
It is common to increase the model depth to scale the model size in deep learning models, but huge parameters of PLMs impose high computational and storage costs, limiting the application of PLMs in low-resource settings.
% Moreover, if we consider PLMs with more than 12 transformer layers as deep models.
Therefore, there is an urgent need for a compact model structure design~(\eg 24 and 48 layers) to achieve both affordable parameters and superior model performance. 
}
% 参数共享已经被证明了是一种有效缓解参数冗余的方案（cv，mt）；在PLM领域，albert用BERT 1/10总参数量达到了近似的效果，证明了堆叠Transformer结构中包含较大的跨层参数冗余。但是我们发现ALBERT的参数共享方法在深层（层深>48）效果会下降。我们发现深层模型的参数共享主要包含2个挑战（1）应该共享哪些参数。换句话说，共享参数需要（require）保持一个效果和参数量的平衡；（2）如何优化这个深层模型。深层模型的优化面临效果下降的问题，被证明是由于训练不稳定导致的。针对共享参数的问题，现有的方法说明通过补充额外的模块来补充共享参数不足的问题，但是没有考虑到共享参数中本身包含的冗余问题。

To reduce the parameters in deep networks,  
weight sharing has proven to be very useful to design lightweight Transformer architectures~\citep{zhang2022minivit,lan2019albert}. As a representative work by across-layer parameter sharing, ALBERT~\citep{lan2019albert}  keeps only ten percent of the whole parameters of BERT  while maintaining comparable performance.
%It directly shares the parameters across all layers, and pre-trains the model based on a much larger corpus than BERT. 
Although the idea of parameter sharing is simple yet (to some extent) effective, %it still suffers from two major challenges in developing deep models. 
%\footnote{In this paper, we take BERT$_{base}$ as a reference and consider PLMs with more than 12 Transformer layers as \emph{deep models}, which are difficult to deploy in low-resource settings.}    
 it has been found that identical weights across different layers are the main cause of performance degradation~\cite{zhang2022minivit}. 
To address this issue, extra blocks are designed to elevate parameter diversity in each layer~\citep{nouriboriji@2022minialbert}. While, they still use the rigid architecture of 
shared layer weights,  having a limited model capacity. 
Besides, it is difficult to optimize very deep models, especially when shared components are involved. Although recent studies~\citep{wang@2022deepnet,huang2020initialize} propose  improved  initialization methods, they do not consider the case with parameter sharing, thus likely leading to a suboptimal performance on a parameter-sharing architecture. 
%do not consider how to cope with across-layer parameter sharing. %Besides, these methods are typically used for training from scratch and neglect the use of existing well-trained PLMs for a warm start. 

%prove that optimizing deep Transformer-based models benefit a lot from stabilizing training by improving weight initialization. But their weight initializing methods are typically used for training from scratch and thus require a huge amount of computational effort. The key challenge is how to properly initialize deep models that can use existing weights to speed up the convergence, which is more efficient and less computationally expensive.

%different studies focus on designing extra blocks to elevate parameter diversity in each layer~\citep{nouriboriji@2022minialbert} while still maintaining shared transformer layer weights. 
%However, it remains unclear which specific parameters within the transformer layer should be shared.

% but ALBERT reported that it is challenging to use weight sharing to scale PLMs into deep models. 

\ignore{
However, it is not easy to develop effective parameter-sharing methods for  deep models~\footnote{In this paper, we consider pre-train language models with more than 12 transformer layers as deep models.} with sharing parameters.
First, existing methods found that identical weights across different layers are the main cause of performance degradation~\cite{lan2019albert}. 
Thus different studies focus on designing extra blocks to elevate parameter diversity in each layer~\citep {nouriboriji@2022minialbert} while still maintaining shared transformer layer weights. However, it remains unclear which specific parameters within the transformer layer should be shared.
% However, which parameters within the transformer layer should be shared remains unclear yet. 
% Second, there is still a lack of theoretical guarantees or discussions about how to initialize deep models with sharing parameters. Thus, it is often confusing how to solve the problem of training instability of deep models in the context of weight sharing.
Second, recent studies~\citep{wang@2022deepnet,huang2020initialize} prove that optimizing deep Transformer-based models benefit a lot from stabilizing training by improving weight initialization. But their weight initializing methods are typically used for training from scratch and thus require a huge amount of computational effort. The key challenge is how to properly initialize deep models that can use existing weights to speed up the convergence, which is more efficient and less computationally expensive.
}
% and do not account for the use of shared parameters from existing pre-trained weights. 

% Do we find applying weight sharing in deep PLMs gives two unique challenges:
% ~(1) which parameters should be shared?
% How to balance the number of shared parameters of deep models with model performance.
% ~(2) how to optimize the deep models with sharing parameters?
% ~(2) how to solve the problem of training instability of deep models in the context of weight sharing.

% For the first challenge,~\citet{zhang2022minivit} found that identical weights across different layers are the main cause of performance degradation. Thus different studies focus on designing extra blocks to elevate parameter diversity in each layer~\cite {nouriboriji@2022minialbert} while leaving transformer layer weight all shared. However, which parameters within the transformer layer should be shared remains unclear yet. 
% For the second challenge, recent studies~\cite{wang@2022deepnet} prove that optimizing deep transformer-based models benefit a lot from stabilizing training by improving weight initialization. But weight initializing methods work on training from scratch while weight sharing generally starts from existing pre-trained models. The key problem is how to initialize deep models with sharing parameters.
% 现有研究已经说明堆叠transformer模型不同层之间有一定的语义关系（\cite），即浅层关注基础语言学信息，而高层关注语义信息，而全部参数共享忽略了这种语义层级关系，限制模型的表达能力，并且随着深度增加这种限制效果会更加严重。因此，为了实现参数共享同时考虑到语义关系，就需要在共享参数之外增加额外的参数来学习层特定的语义信息。如果这个可以实现，就可以实现缓解跨层参数冗余同时，保证深层模型的表达能力。

% In order to address these challenges, we propose the MPOBERT model which is a parameter-efficient pre-trained language model based on matrix product operators. 
To address these challenges, in this paper, we propose a highly parameter-efficient approach to scaling PLMs to a deeper model architecture. 
As the core contribution, we propose a \emph{matrix product operator}~(MPO) based parameter-sharing architecture for deep Transformer networks.  
Via MPO decomposition,  a parameter matrix can be decomposed into  \emph{central tensors}~(containing the major information) and \emph{auxiliary tensors}~(containing the supplementary information). 
Our approach shares the central tensors of the parameter matrices across all layers for reducing the model size, and meanwhile keeps layer-specific auxiliary tensors for enhancing the adaptation flexibility.  
%Such a MPO-based architecture can be used scale the model depth with .  
In order to train such a deep architecture, we propose  an MPO-based initialization method by utilizing the MPO decomposition results of ALBERT. Further, for the auxiliary tensors of  higher layers (more than 24 layers in ALBERT), we propose to set the parameters with scaling coefficients derived from theoretical analysis. We theoretically show it can address the training instability regardless of the model depth. %lead to a stable optimization with convergence. 

%Our idea is to share the central tensors of the parameter matrices across all layers, and meanwhile keep layer-specific auxiliary tensors (also incorporating layer-specific adapters). 
%In this way, we can largely reduce the model size while effectively retaining the flexiblity for parameter adaptation. 

 
%In our architecture, each layer consists of a globally shared part  and a layer-specific part. 


%Unlike prior work that shares all parameters or uses extra blocks, the shared part and layer-specific parts are obtained via MPO decomposition based on the original parameter matrix. 



%our architecture  factorizes  a parameter matrix from a layer into a global shared part  and a layer-specific part via MPO decomposition. Unlike previous 

%corresponding  to \emph{central tensors} and \emph{auxiliary tensors}, respectively, in MPO decomposition~\cite{gao2020compressing}. 
%A major merit of MPO decomposition is that central tensors and auxiliary tensors 

 %via MPO decomposition. In MPO decomposition, the shared part 


%directly decomposes a matrix  central tensors~(containing the major information) and auxiliary tensors~(containing the supplementary information) with only a small proportion of parameters~\cite{gao2020compressing} based on MPO.

%MPO can decompose a matrix into central tensors~(containing the major information) and auxiliary tensors~(containing the supplementary information) with only a small proportion of parameters~\cite{gao2020compressing}. The key idea is to share the central tensors of the parameter matrices across all layers for reducing the model size, and meanwhile keep layer-specific auxiliary tensors for enhancing the adaptation flexility. 


%Instead of sharing all parameters or incorporating extra blocks, our architecture  provides a more principled solution for \emph{shared parameter selection}, \ie what part of parameters are shared across layers in deep models.  

%which can enable the sharing of the majority of parameters across  

% a deep PLM, \emph{MPOBERT}, which is developed based on a highly parameter-efficient architecture and trained by specially designed stable optimization methods. 

\ignore{
 MPOBERT model which is a deep PLM  with a highly parameter-efficient architecture and trained by specially designed stable optimization methods. 
%with fewer parameters and less computational costs based on matrix product operators~(see Figure \ref{fig:plms_comparison}). 
The matrix product operator (MPO) is a standard algorithm for decomposing a matrix into its constituent central tensors~(which hold the most important information) and auxiliary tensors~(which hold additional information) with only a small proportion of parameters~\cite{gao2020compressing}.
% Matrix product operators~(MPO) is a stardard algrithem that can decompose an original matrix into central tensors~(containing the core information) and auxiliary tensors~(with only a small proportion of parameters). 
% 如果
% 这个特性有助于我们挑选出Transformer中重要的信息进行更新
% Based on MPO, \citet{liu2021enabling} proposed a lightweight fine-tuning method in which central tensors with core information can be easily shared across different downstream tasks, our method allows sharing central tensors across different transformer layers. 
Such merit makes MPO become an ideal method for constructing a parameter-efficient pre-train model architecture.
We have made two significant technological contributions to the deep scalability of Transformers based on MPO.
First, we proposed the parameter-efficient pre-trained language model that shares the central tensors parameters across transformer layers while maintaining the auxiliary tensors and incorporating additional adapter parameters.
Second, we proposed an efficient training strategy based on the analysis of model training instability.
We present both theoretical analysis and experimental verification for the effectiveness of the proposed MPOBERT model. 
}

% To our best knowledge, it is the first time that MPO is used in Transformers to construct parameter-efficient PLMs, which is well suited to the architecture of PLMs.
% We construct experiments to evaluate the effectiveness of the proposed MPOBERT model on GLUE benchmark compared with BERT~\cite{devlin2018bert}, ALBERT~\cite{lan2019albert}, MobileBERT~\cite{sun2020mobilebert}, TinyBERT~\cite{jiao2019tinybert} and DistillBERT~\cite{sanh2019distilbert}.
% Extensive experiments have demonstrated the superior performance of the proposed model, particularly in terms of the reduction in parameters~(\ie 228M fewer than 24-layer BERT$_{\rm{{LARGE}}}$) and the cost-efficiency of pre-training a 48-layer MPOBERT~(\ie 3.8 days) as well as outperforming multiple competing models.


Our work  provides a novel parameter-sharing way for scaling model depth, which can be generally applied to  various Transformer based  models. 
We conduct extensive  experiments to evaluate the performance of the proposed MPOBERT model on the GLUE benchmark in comparison to PLMs with varied model sizes~(tiny, small and large). 
 %To the best of our knowledge, this is the first time MPO has been used to design parameter-efficient PLMs in Transformers, which is well-suited to the architecture of PLMs. We conduct experiments to evaluate the performance of the proposed MPOBERT model on the GLUE benchmark in comparison to PLMs with various model sizes~(\eg ALBERT~\cite{lan2019albert} for tiny models, T5$_{12}$~\cite{raffel2020exploring} for small models, and BERT$_{\rm{{LARGE}}}$ for base models) and compressed PLMs~(\ie MobileBERT~\cite{sun2020mobilebert}, TinyBERT~\cite{jiao2019tinybert} and DistillBERT~\cite{sanh2019distilbert}).
Experiments  results have demonstrated  the effectiveness of the proposed model in reducing the model size and achieving competitive performance. With fewer parameters than BERT$_{\rm{BASE}}$, we scale the model depth by a factor of 4x and achieve 0.1 points higher than  BERT$_{\rm{LARGE}}$ for GLUE score. 

%, in particular its use of fewer parameters(ie 228M fewer than 24-layer BERT$_{\rm{LARGE}}$) and affordable computing cost~(\ie 3.8 days to pre-train a 48-layer MPOBERT) to outperform a number of competing models.

% Extensive experiments have demonstrated the effectiveness of the proposed model, especially using fewer parameters~(\ie 228M fewer than 24-layer BERT$_{\rm{{LARGE}}}$) and affordable computational cost~(\ie 3.8 days) to train a 48-layer MPOBERT and outperform several competing models.
% \textcolor{blue}{Add the experimental results }.
% MPOBERT contains two major parts:~(1) parameter-efficient architecture. We share central tensors in each layer while keeping auxiliary tensors and extra adapters as layer-specific parameters.~(2) efficient training. Based on the theoretical analysis of model training instability, we proposes a simple yet effective method for both shared parameters and the auxiliary tensors to stabilize training at the beginning steps. \textcolor{blue}{Experimental results }.
% 受到MPOP的启发，我们提出了一种跨层CT共享的深层PLM，Parameter Efficient Transformer。同时我们在MPOP基础上结合算子融合技术和训练策略，提出了一种高效的实现框架。具体而言，MPOBERT的核心是MPOTransformer的实现，主要包括两部分，共享的CT参数和每一层独有的AT参数。【方法目的】。在此基础上，我们提出了一种高效的实现框架。【方法目的】

% 效果