\clearpage
\section{Appendix}
\label{sec:appendix}
\subsection{Proofs}
\label{app:proof}
\paragraph{Notations.} We denote $\mathcal{L}(\cdot)$ as the loss function. $LN(x)$ as the standard layer normalization with scale $\gamma=1$ and bias $\beta =0$. Let $\mathcal{O}(\cdot)$ denote standard Big-O notation that suppresses multiplicative constants. $\overset{\Theta}{=} $ stands for equal bound of magnitude. 
We aim to study the magnitude of the model updates. We define the model update as $\left\| \bigtriangleup F\right\|$.
% \begin{definition}
%     $F(x,\theta)$ is updated by $\Theta(\eta)$ per SGD step after initialization as $\eta\to 0$. That is, $\left\| \bigtriangleup F(x)\right\|=\Theta(\eta)$ where $\bigtriangleup F(x)$ can be calculated through $F(x,\theta-\eta\frac{\partial}{\partial \theta}\mathcal{L}(F(x)-y))-F(x;\theta)$.
% \end{definition}

\paratitle{Definition}
    $F(x,\theta)$ is updated by $\Theta(\eta)$ per SGD step after initialization as $\eta\to 0$. That is, $\left\| \bigtriangleup F(x)\right\|=\Theta(\eta)$ where $\bigtriangleup F(x)$ can be calculated through $F(x,\theta-\eta\frac{\partial}{\partial \theta}\mathcal{L}(F(x)-y))-F(x;\theta)$.

\begin{theorem}
    Given an $N$-layer transformer-based model $F(x,\theta)(\theta=\{\theta_1, \theta_2, ...,\theta_N\})$, where $\theta_l$ denotes the parameters in $l$-th layer and each sub-layer is normalized with Post-LN: $x_{l+1}=LN(x_l+G_l(x_l,\theta_l))$. In MPOBERT, $\theta_l$ is decomposed by MPO to local tensors: $\theta_l=u_l\cdot c_l\cdot v_l$, and we share $\{c_i\}_{i=1}^{N}$ across $N$ layers: $c_l=c_1, l=1,2,\cdots,N$. Then $\left\| \bigtriangleup F\right\|$ satisfies:
    \begin{align}
    \left\| \bigtriangleup F\right\|\leq
    % &\sum_{i=1}^{N}\frac{1-u_ic_1v_i}{(1+u_i^2c_1^2v_i^2)^{\frac{3}{2}}}(c_1v_i\left\|u_i^*-u_i \right\| \nonumber\\
    % &+ c_1u_i\left\|v_i^*-v_i \right\| + u_iv_i\left\|c_1^*-c_1 \right\|)
    &\sum_{i=1}^{N}(c_1v_i \left\|u_{i}^*-u_{i}\right\|+ c_1u_i \left\|v_{i}^*-v_{i}\right\| \nonumber\\
    &+ v_iu_i \left\|c_1^*-c_1\right\|)
    \end{align}
    \label{app-thm1}
\end{theorem}
$Proof.$ 
We follow~\cite{zhang2019fixup} and make the following assumptions to simplify the derivations:
\begin{enumerate}
    \item Hidden dimension $d$ equals to $1$;
    \item $var(x+G_l(x))\overset{\Theta}{=}var(x)+var(G_l(x))$;
    \item All relevant weights $\theta$ are positive with magnitude less than $1$.
\end{enumerate}
Given Assumption 1, if $G_l(x)$ is MLP with the weight $\theta_l$, then $G_l(x)\overset{\Theta}{=}\theta_l x$. With assumption 2, we have:
\begin{align}
    x_{l+1}&=f_l(x_l, \theta_l)=\frac{x+G_l(x)}{\sqrt{Var(x+G_l(x))}}\\
    &\overset{\Theta}{=}\frac{1+\theta_l}{\sqrt{1+\theta_l^2}}x_l,
    \label{eq:base}
\end{align}
Then, with Taylor expansion, the model update $\left\|\bigtriangleup F\right\|$satisfies:
\begin{align}
    \left\|\bigtriangleup F\right\|=&\left \|F(x,\theta^*)-F(x, \theta\right \|\nonumber\\
    =&\left\|x_{N+1}^*-x_{N+1}\right\| \nonumber\\
    =&\left\|f(x_{N}^*,\theta_{N}^*) -f(x_{N},\theta_{N})\right\|\nonumber\\
    =&\left \| f(x_{N}^*, U_{N}^*,C_{N}^*,V_{N}^*)\nonumber\right.\\
    &\left.-f(x_N, U_{N},C_{N},V_{N}) \right \| \nonumber\\
    \approx&\left \|\frac{\partial f }{\partial x}(x_{N}^*-x_{N})\nonumber\right.\\
    &\left.+\frac{\partial f}{\partial \theta }\frac{\partial\theta}{\partial U_{N}}(U_{N}^*-U_{N})^T \nonumber\right.\\
    &\left.+\frac{\partial f}{\partial \theta }\frac{\partial\theta}{\partial C_{N}}(C_{N}^*-C_{N})^T\nonumber\right.\\
    &\left.+\frac{\partial f}{\partial \theta }\frac{\partial\theta}{\partial V_{N}}(V_{N}^*-V_{N})^T  \right \|
    \label{eq:9}
\end{align}
With Eq.~\eqref{eq:base}, the magnitude of $\frac{\partial f_l}{\partial x}$ and $\frac{\partial f_l}{\partial \theta}$ is bounded by:
\begin{align}
    & \frac{\partial f_l}{\partial x}\overset{\Theta}{=}\frac{1+\theta_l}{\sqrt{1+\theta_l^2}} \\
    & \frac{\partial f_l}{\partial \theta_l}\overset{\Theta}{=}\frac{1-\theta_l}{(1+\theta_l^2)^{\frac{3}{2}}}x_l
\end{align}
Since we apply MPO decomposition to $\theta_l$, we get:
\begin{align}
    % &\theta_l=u_lc_lv_l
    \theta_l=U_l\cdot C_l \cdot V_l
\end{align}
For simplicity, we reduce the matrices $U$,$C$,$V$ to the scalars $u$,$c$,$v$. 
% In MPOBERT, we share $\{c_i\}_{i=1}^{N}$ across $N$ layers, so it goes to $c_l=c_1$. Considering that $c_1$ is initialized with well-trained parameters, the magnitude of the term $(c_N^*-c_N)$ is negligible compared with others. 
Thus with Assumption 3, Eq.~\eqref{eq:9} is reformulated as:
Finally, with Assumption 3 we have:
\begin{align}
    \left\|\bigtriangleup F \right\|=
    &\left\| x_{N+1}^*-x_{N+1} \right\| \\
    \leq &\sum_{i=1}^{N}\frac{1-u_ic_1v_i}{({{1+u_{i}^2c_1^2v_{i}^2}})^{\frac{3}{2}}}(c_1v_i \left\|u_{i}^*-u_{i}\right\|\nonumber\\
    &+ c_1u_i \left\|v_{i}^*-v_{i}\right\|) + v_iu_i \left\|c_1^*-c_1\right\|) \nonumber\\
    \approx&\sum_{i=1}^{N}(c_1v_i \left\|u_{i}^*-u_{i}\right\|+ c_1u_i \left\|v_{i}^*-v_{i}\right\| \nonumber\\
    &+ v_iu_i \left\|c_1^*-c_1\right\|)
\end{align}
\rightline{$\Box$}
\begin{corollary}
\label{app-thm2}
    % In $N$-layer MPOBERT, we assume that $\left\| \frac{\partial F}{\partial c_1} \right\|=\mathcal{O}(1)$, \ie the gradient signal of $c_1$ from the layers above is bounded, 
    Given that we initialise $c_1$ in MPOBERT with well-trained weights, it is reasonable to assume that updates of $c_1$ are well-bounded.
    Then $\bigtriangleup F$ satisfies $\left\| \bigtriangleup F\right\|=\mathcal{O}(1)$ when for all $i=1,\cdots,N$:
    \begin{equation}
        (v_i^2+u_i^2)(u_Nv_N)=\mathcal{O}(\frac{1}{N})
    \end{equation}
\end{corollary}

$Proof.$ 
For an $N$-layer MPOBERT, we have:
\begin{align}
    \left \|\bigtriangleup F\right \|
    \leq &\sum_{i=1}^{N}(v_i \left\|u_{i}^*-u_{i}\right\|+{u_i \left\|v_{i}^*-v_{i}\right\|}) \\
    \leq &\eta\sum_{i=1}^{N}(v_i \left\|\frac{\partial \mathcal{L}}{\partial F}\right\|\cdot \left\|\frac{\partial F}{\partial \theta_i}\right\|\cdot \left\|\frac{\partial \theta_i}{\partial u_i}\right\|\nonumber\\
    &+{u_i \left\|\frac{\partial \mathcal{L}}{\partial F}\right\|\cdot \left\|\frac{\partial F}{\partial \theta_i}\right\|\cdot\left\|\frac{\partial \theta_i}{\partial v_i}\right\|})
\end{align}
By assumption $\left\| \frac{\partial \mathcal{L}}{\partial F}\right\|=\mathcal{O}(1)$ and $\left\|\frac{\partial F}{\partial {\theta_i}} \right\|\leq\left\| \frac{\partial F}{\partial \theta_N} \right\|\overset{\Theta}{=}\left\| \theta_{N}\right\|$, we achieve:
\begin{align}
    % &{\eta\sum_{i=1}^{N}\frac{1-u_iv_i}{({{1+u_{i}^2v_{i}^2}})^{\frac{3}{2}}}(v_i \left\|\frac{\partial \mathcal{L}}{\partial F}\right\|\cdot \left\|\frac{\partial F}{\partial u_i}\right\|}+{u_i \left\|\frac{\partial \mathcal{L}}{\partial F}\right\|\cdot \left\|\frac{\partial F}{\partial v_i}\right\|})\\
    &\eta\sum_{i=1}^{N}(v_i \left\|\frac{\partial \mathcal{L}}{\partial F}\right\|\cdot \left\|\frac{\partial F}{\partial \theta_i}\right\|\cdot \left\|\frac{\partial \theta_i}{\partial u_i}\right\|\nonumber\\
    &+{u_i \left\|\frac{\partial \mathcal{L}}{\partial F}\right\|\cdot \left\|\frac{\partial F}{\partial \theta_i}\right\|\cdot\left\|\frac{\partial \theta_i}{\partial v_i}\right\|})\\
    =&\eta\sum_{i=1}^{N}(v_i^2u_Nv_N+u_i^2u_Nv_N) \nonumber\\
    = &\mathcal{O}(\sum_{i=1}^{N}(v_i^2+u_i^2)(u_Nv_N))=\mathcal{O}(1),
\end{align} 
\label{eq:bound}
Finally, we achieve:
\begin{equation}
    (v_i^2+u_i^2)(u_Nv_N)=\mathcal{O}(\frac{1}{N})
\end{equation}

Due to symmetry, we set $u_i=u$, $v_i=v$. Thus, from~\ref{eq:bound}, we set $u=v=(2N)^{-\frac{1}{4}}$ to achieve to bound the magnitudes of each update to be independent of model depth $N$, \ie $\left\| \bigtriangleup F\right\|=\mathcal{O}(1)$.
\rightline{$\Box$}

\begin{algorithm}[htb]
    \caption{The MPOBERT training procedure.}
    \begin{algorithmic}[1] %每行显示行号
    \small
        % \Require  $\Matrix{W}$: initial pre-trained weight 
        \Require $\Matrix{W}^{(l)}$: Weight matrix of $l$-th layer in MPOBERT.
        $\Matrix{W}_{A}^{(0)}$: Pre-trained weight matrix in ALBERT.
        % $\Matrix{W}_{Adapter}^{(l)}$: Low rank adapter containing $\Matrix{U}^{(l)}$ and $\Matrix{D}^{(l)}$.
        $\Matrix{U}^{(l)}$ and $\Matrix{D}^{(l)}$: Matrices in low-rank adapter.
        $\eta$: Learning rate.
        $\mathcal{L}$: Stochastic objection function.
        $L$: Model layers number.
        % \Require time step $t\gets 0$~(Initialize timestep)
        \Statex (MPO decomposition)
        \State
        $\{\Tensor{A}_1^{(l)},\Tensor{A}_2^{(l)},\Tensor{C}^{(l)},\Tensor{A}_3^{(l)},\Tensor{A}_4^{(l)}\}$ $\gets$ MPO ($\Matrix{W}^{(l)}$)
        \State
        $\{\Tensor{A}_1^{(0)},\Tensor{A}_2^{(0)},\Tensor{C}^{(0)},\Tensor{A}_3^{(0)},\Tensor{A}_4^{(0)}\}$ $\gets$ MPO ($\Matrix{W}_{A}^{(0)}$)
        \Statex (Initialization Procedure)
        \For {$0<l\leq 24$}
            \State  $\Tensor{C}^{(l)} \gets \Tensor{C}^{(0)} , 
             \{\Tensor{A}_{j}^{(l)}\}_{j=1}^{4} \gets \{\Tensor{A}_{j}^{(0)}\}_{j=1}^{4}$ 
        \EndFor
        \For {$ 24<l\leq L$}
            \State  $\Tensor{C}^{(l)} \gets \Tensor{C}^{(0)} ,    \{\Tensor{A}_{j}^{(l)}\}_{j=1}^{4} \gets \{(2L)^{-\frac{1}{4}}\Tensor{A}_{j}^{(0)}\}_{j=1}^{4}$ 
        \EndFor
        \State $\Matrix{U}^{(l)} \gets \Matrix{0}$, $\Matrix{D}^{(l)} \gets \mathcal{N}(0, \sigma^2)$
        \State $\Matrix{W}^{(l)}=\Tensor{A}_1^{(l)}\Tensor{A}_2^{(l)}\Tensor{C}^{(l)}\Tensor{A}_3^{(l)}\Tensor{A}_4^{(l)}+\Matrix{W}_{Adapter}^{(l)}$
        \Statex (Training procedure with mixed precision and fused implementation techniques.)
        \While {not converged}
            \State $t \gets t+1$
            \State $g_t \gets \frac{\partial\mathcal{L}(\Matrix{W}^{(l)}_t)}{\partial(\Matrix{W}^{(l)}_t)}$
            \State $\Matrix{W}^{(l)}_t \gets \Matrix{W}^{(l)}_{t-1} - \eta \cdot g_t$
        \EndWhile
        \State \Return Converged model
    \end{algorithmic}
\label{alg-overall-process}
\end{algorithm}
\subsection{Training Details}
\label{add-trianing-detail}

\subsubsection{Details of Training}
Here we describe the details of the pre-training process in Algorithm~\ref{alg-overall-process}. 
For pre-training, we tune the learning rate in the range of [$1.0\times 10^{-5}$, $1.0\times 10^{-6}$] and use the LAMB optimizer~\cite{you2020lamb}. Since fine-tuning is typically fast, we run an exhaustive parameter search~(\ie learning rate in the range of [$2.0\times 10^{-4}$, $2.0\times 10^{-6}$], batch size in \{8,16,32\}) and choose the model that performs best on the development set to make predictions on the test set. 


\subsubsection{Details of Training Configurations}
In this part, we list the training configurations of MPOBERT and other representative PLMs in Table~\ref{tab-strongest_variants}.
\begin{table*}[t]
\centering
\begin{tabular}{lrrrcrrr}                       
\toprule
Models    & \#To~(M) & Depth & Samples  & Training time  & GLUR Dev.  &GLUE Test      \\ \midrule
T5$\rm{_{11B}}$        & 11000  & 24     & -   & -    & - & 89.0       \\  \\
T5$\rm{_{BASE}}$        & 220  & 24     & 128$\times$ 524$k$   & \multirow{2}{*}{\thead{16 TPU v3\\ 1 Day~(t5-base)}}  & 84.1    & 82.5       \\  \\
BERT$\rm{_{LARGE}}$      & 330     & 24     & 256$\times$ 1000$k$   & \multirow{2}{*}{\thead{16 Cloud TPUs\\ 4 Days}}  & 84.1    & 81.6       \\  \\
ALBERT$\rm{_{XXLARGE}}$    & 235     & 1     & 4096$\times$ 1.5$M$   & \multirow{2}{*}{\thead{TPU v3\\ 16 Days}}  & 90.0    & -       \\  \\
BART$\rm{_{LARGE}}$      & 407     & 24     & 8000$\times$ 500$k$   & -  & 88.8    & -       \\  \\
RoBERTa$\rm{_{LARGE}}$   & 355     & 24     & 8000$\times$ 500$k$   & \multirow{2}{*}{\thead{1024 V100 GPUs\\ 1 Day}}  & 88.9    & -       \\  \\
XLNet$\rm{_{LARGE}}$     & 361     & 24     & 8192$\times$ 500$k$   & \multirow{2}{*}{\thead{512 TPU v3\\ 5.5 Days}}  & 87.4    & -       \\  \\
MPOBERT$_{48+}$   & 102     & 48    & 4096$\times$ 10$k$    & \multirow{2}{*}{\thead{8 V100 GPUs\\ 3.8 Days}}    & 85.6  & 81.7  \\ \\ \bottomrule
\end{tabular}
\caption{Comparison with the strongest variants of popular PLMs. Since T5$\rm{_{11B}}$ has far more parameters than other candidates, it's reasonable to use T5$\rm{_{base}}$ for a fair comparison.}
\label{tab-strongest_variants}
\end{table*}


\subsection{Experimental Details}
\subsubsection{Details of Fine-tuning Datasets}
\label{add-detail_dataset}
GLUE benchmark covers multiple datasets~(MNLI, QNLI, QQP, CoLA, RTE, MRPC, SST-2)~\footnote{In line with~\citet{raffel2020exploring}, we do not test WNLI due to its adversarial character with respect to the training set.}. 
The SQuAD is a collection of 100$k$ crowd-sourced question/answer pairs. Given a question and a passage, the task is to predict the answer text span in the passage. 

\subsubsection{Details of Evaluation Metrics}
\label{add-detail_metric}
Following~\citet{gao2022parameter}, we employ Matthew's correlation for CoLA, Spearman for STS-B, F1 for MRPC, and accuracy for the remaining tasks as the metrics for the GLUE benchmark.
We compute and present the average scores across all test samples for each of the aforementioned metrics.

\subsubsection{Details of Baseline Models}
\label{add-detail_baseline}
We compare our proposed MPOBERT to the existing competitive deep PLMs and parameter-efficient models. In order to make fair comparisons, we divide the models into three major categories based on their model sizes: 
$\bullet$~{Tiny Models~(\rm{\#To < 50M}).} ALBERT$_{12}$~\cite{lan2019albert} is the most representative PLM that achieves competitive results with only 11M.

$\bullet$~{Small models~(50M< \#To <100M).} T5$_{12}$ is a small variant of T5~\cite{raffel2020exploring} which has only 6 encoder layers and 6 decoder layers. In addition, there are three parameter-efficient Transformer models that have similar parameters, namely MobileBERT~\citep{sun2020mobilebert}, DistilBERT~\citep{sanh2019distilbert} and TinyBERT~\citep{jiao2019tinybert}. We compare with these compressed models to show the benefit of scaling to deeper models over compressing large models to small variants.

$\bullet$~{Base models~(\#To > 100M).} We compare with BERT$_{12}$, XLNet$_{12}$, RoBERTa$_{12}$ and BART$_{12}$ for this category. Note that we only include the base variants that have similar model sizes in order to make a fair comparison. More details about the comparison with the strongest variants are described in Appendix~\ref{app-exp}.
% $\bullet$~\underline{Parameter-efficient Models.} We compare with parameter-efficient models based on compression techniques including MobileBERT~\citep{sun2020mobilebert}, DistilBERT~\citep{sanh2019distilbert} and TinyBERT~\citep{jiao2019tinybert}.

\label{app-exp}

