\section{Method}
%In this section, we describe our proposed MPOBERT for building the deep Transformer-based model with fewer parameters and enhanced training efficiency. With parameter sharing, it is possible to save a substantial number of parameters when scaling models along the depth. In addition, we propose a theoretically derived tensor initialization for training deep Transformers. Thus, we will explain MPOBERT in the two parts mentioned above.
In this section, we describe the proposed \emph{MPOBERT} approach for building deep PLMs via a highly parameter-efficient architecture.
Our approach follows the classic \emph{weight sharing} paradigm, while introducing a  principled mechanism for sharing informative parameters across layers and also enabling  layer-specific  weight adaptation. 
%the flexibility for capturing layer-wise variation. 


\subsection{Overview of Our Approach}
\label{sec-framework}
%The basic idea of MPOBERT is to share \emph{more informative} parameters among different Transformer layers to build  deep PLMs in a parameter-efficient way. 
Although weight sharing has been widely explored for building  compact PLMs~\cite{lan2019albert}, existing studies either  share  all the parameters across  layers~\cite{lan2019albert} or incorporate additional blocks to facilitate  the sharing~\cite{zhang2022minivit,nouriboriji@2022minialbert}. They  either have limited model capacity with a rigid architecture  or require additional efforts for maintenance.   

Considering the above issues, we motivate our approach in two aspects.  Firstly, only informative parameters should be shared across layers, instead of all the parameters.  Second, it should not affect the capacity to capture layer-specific  variations. 
To achieve this, we utilize the MPO decomposition from multi-body physics~\cite{gao2020compressing} to develop a parameter-efficient architecture by sharing informative components  across layers and keeping layer-specific supplementary components  (Section~\ref{subsec-crosslayer}). 
%we develop our approach based on the MPO decomposition from multi-body physics~\cite{gao2020compressing}. 
 %while also employed for parameter-efficient  PLMs~\cite{liu2021enabling, gao2022parameter}.  % To develop a more principled weight sharing mechanism, we utilize MPO decomposition to obtain shared parameters  across layers and layer-specific parameters (Section~\ref{subsec-crosslayer}). 
As another potential issue, it  is difficult to optimize  deep PLMs due to unstable training~\cite{wang@2022deepnet}, especially when weight sharing~\cite{lan2019albert} is involved. 
We further propose a simple yet effective method to stabilize the training of MPOBERT (Section~\ref{sec-efficient-training}).  Next, we introduce the technical details of our approach. 

  %Next, we introduce the 

%Transformer-based models
% We first introduce the procedure of the matrix product operator decomposition method and then detail the MPOBERT with two parts: scaling to deeper models with parameter sharing and training deep models with tensor initialization.

% \subsection{Matrix Product Operator Decomposition}
% \label{subsec-matrix-product-operators}
% We will briefly describe the matrix product operator decomposition method. Originating from quantum many-body physics~\cite{gao2020compressing}, the MPO decomposition can effectively reorganize and aggregate the information of the matrix. The parameter matrix is decomposed into a central tensor and a set of auxiliary tensors~\citep{liu2021enabling, gao2022parameter}.

% To clarify the MPO decomposition process, we assume that a weight matrix $\Matrix{W}\in \mathbb{R}^{I\times J}$ is a matrix with size $I\times J$.
% Given two arbitrary factorizations of its dimensions into natural numbers, we can reshape and transpose this matrix into an $n$-dimension tensor $\Matrix{W}_{i_1,\dots, i_n, j_1, \dots, j_n}$ in which:
% \begin{equation}
% \small
%     \prod_{k=1}^{n} i_k = I, \quad \prod_{k=1}^{n}j_k = J.
% \end{equation}
% This decomposition can be written as:
% \begin{equation}
%     \small
%    \Matrix{W}_{i_1,\dots , i_n, j_1, \dots, j_n} = \Tensor{T}^{(1)}[i_1, j_1]\cdots \Tensor{T}^{(n)}[i_n, j_n],
% \label{eq:mpo-decom}
% \end{equation}
% where the $\Tensor{T}^{(k)}[i_k, j_k]$ is a 4-dimensional tensor with size $d_{k-1}\times i_k \times j_k \times d_k$ in which $d_k$ is a bond dimension linking $T^{(k)}$ and $T^{(k+1)}$ with $d_0=d_n=1$.
% with size $d_{k-1}\times i_k \times j_k \times d_k$ in which $\prod_{k=1}^{n}i_k=I, \prod_{k=1}^{n}j_k=J$ and $d_0=d_n=1$.

% According to ~\citet{gao2020compressing}, the original matrix $\Matrix{M}$ may be precisely reconstructed using tensor contraction without the connecting bond $\{d_k\}_{k=1}^m$ being truncated.
% After MPO decomposition, the central tensor can encode the essential data from the original matrix, while the other auxiliary tensors serve as its complement.

% \subsection{Parameter-efficient Architecture}
% \subsection{Weight Sharing with Layer-wise Adaption}
% \subsection{Lightweight Layer-wise Adaption}

%\subsection{Scaling to Deeper Models based on MPO}
%\label{subsec-parameter-efficient-architecture}
% 回顾一下Transformer结构和参数共享策略，以及表示。当前的问题是这个共享参
% \textcolor{blue}{Add a summary sentence in there.}
%In this subsection, we first describe the overview of our approach, and then present the technical details for the proposed architecture.    
%introduce our proposed MPOBERT in order to build a deep Transformer-based model with fewer parameters.

\ignore{The general idea of MPOBERT is to share parameters among different Transformer layers to build parameter-efficient PLMs. 
% Prior studies have demonstrated weight sharing as an effective method to compress large Transformer-based models~\cite{}. For example, \citet{lan2019albert} shares all of the Transformer layers in BERT to build a highly compact pre-trained language model.
Prior studies have demonstrated weight sharing as an effective method to build highly compact PLMs~\cite{lan2019albert}. However, a serious problem encountered with the weight-sharing technique is performance degradation and the main cause is the strict identity of weights across different layers. Thus, \citet{zhang2022minivit} and~\citet{nouriboriji@2022minialbert} consider supplementing the shared weights by adding an extra block for each layer. As a comparison, we decompose model weights into tensors containing common and specific information, which makes it potentially possible to consider only sharing common information across layers to alleviate the performance degradation issue. 
% Unlike these works, we split model weight into common and specific parameters and only share common parameters across different layers. 
}

%To develop a more principled weight sharing mechanism, our approach is based on  two motivations.


\ignore{To achieve this, we introduce a novel matrix decomposition method, \ie MPO decomposition. An important merit of MPO decomposition is that it can reorganize and aggregate information in central tensors.
Thus we propose an MPO-based Transformer layer containing two major parts: First we can share the central tensor parameters across different layers. Then, we design layer-specific adapters to supplement the capacity of each layer in MPOBERT. We will describe each part in detail.
}

% Thus we can share the central tensor parameters across different layers. In addition, we design layer-specific adapters to supplement the capacity of each layer in MPOBERT. We will describe each part in detail.
% To be concrete, we denote shared parameters as $\Matrix{W}_0$ while specific parameters as $\Matrix{W}^{'}$ and weight-sharing models can be formulate as Eq.~\ref{eq-weight-sharing}:
% 探测试验
% Based on our probing test in~\ref{}, we observe that the 
% 共享所有层参数会限制模型的表达能力，导致无法构建更加深层的模型。替代全部共享，一些工作提出了部分共享，同时增加额外的参数来增加模型的表达能力。因此共享哪些参数成为了一个核心问题，来实现参数不随参数增加的太大，同时保证模型性能的提升。
% 同时也暴露出来参数共享遇到的核心问题就是深层效果下降。语义探测实验发现共享参数后导致模型的表达能力下降，是核心原因。为了解决这个问题，cite{} 均采用增加额外的模块来增加模型参数以强化模型的表达能力。与这些方法不同，我们关注共享的策略本身，核心挑战是到底该共享哪些参数。我们将要共享的参数表示为\theta_0，其他参数表示为\theta^'，那么基于参数共享的模型可以被表达为：
% \begin{equation}
%     \Vector{h}_{i+1}=f(\Vector{h}_i; \Matrix{W}_0, \Matrix{W}^{'}), i=0,1,\cdots,L-1.
% \label{eq-weight-sharing}
% \end{equation}

% Given this condition, the problem can be regarded as determining parameters $\Matrix{W}_0$ to be shared across layers in order to reduce model redundancy while maintaining performance in downstream tasks. Next, we will describe two proposed techniques to share parameters and supplement the capacity of each layer to enhance downstream performance.

% In order to solve this problem, we propose two major techniques, which can largely reduce total parameters when scaling to deeper models and effectively train deep models.

\subsection{MPO-based Transformer Layer}
\label{subsec-crosslayer}
In this section, we first introduce the MPO decomposition and introduce how to utilize it for building parameter-efficient deep PLMs.  
%We will first introduce the MPO decomposition method. Next, we will describe two proposed techniques: cross-layer parameter sharing and layer-specific adapters to eventually build our parameter-efficient deep Transformer-based model.
% An important merit of MPO decomposition is that can reorganize and aggregate information in central tensors.
% Thus we can share the central tensor parameters across different layers. In addition, we design layer-specific adapters to supplement the capacity of each layer in MPOBERT. We will describe each part in detail.

\subsubsection{MPO Decomposition}

%First, we will briefly describe the matrix product operator decomposition method. 
%From quantum many-body physics~\cite{gao2020compressing}, the MPO decomposition 

Given a weight matrix $\Matrix{W}\in \mathbb{R}^{I\times J}$,   MPO decomposition~\cite{gao2020compressing} can  decompose a matrix into a product of $n$ tensors by reshaping the two dimension sizes $I$ and $J$:
\begin{equation}
    \small
   \Matrix{W}_{i_1,\dots , i_n, j_1, \dots, j_n} = \Tensor{T}^{(1)}[i_1, j_1]\cdots \Tensor{T}^{(n)}[i_n, j_n],
\label{eq:mpo-decom}
\end{equation}
where we have $I=\prod_{k=1}^{n} i_k$, $J=\quad \prod_{k=1}^{n}j_k$, and  $\Tensor{T}^{(k)}[i_k, j_k]$ is a 4-dimensional tensor with size $d_{k-1}\times i_k \times j_k \times d_k$ in which $d_k$ is a bond dimension linking $T^{(k)}$ and $T^{(k+1)}$ with $d_0=d_n=1$. For simplicity, we omit the bond dimensions  in Eq.~\eqref{eq:mpo-decom}.
When $n$ is odd, the middle tensor contains the most parameters (with the largest bond dimensions), while the parameter sizes of the rest decrease with the increasing distance to the middle tensor. 
Following \citep{liu2021enabling}, we further simplify the decomposition results of a matrix as a central tensor  $\mathcal{C}$ (the middle tensor) and auxiliary tensors $\{ \mathcal{A}_i \}_{i=1}^{n-1}$ (the rest tensor). 


As a major merit, such a decomposition
can effectively reorganize and aggregate the information of the matrix~\cite{gao2020compressing}:   central tensor  $\mathcal{C}$ can encode the essential information of the original matrix, while  auxiliary tensors $\{ \mathcal{A}_i \}_{i=1}^{n-1}$ serve as its complement to exactly reconstruct the matrix.  
%effectively reorganize and aggregate the information of the matrix. The parameter matrix is decomposed into a \emph{central tensor} and a set of \emph{auxiliary tensors}~\citep{liu2021enabling, gao2022parameter}. 

\ignore{Formally,  given a weight matrix $\Matrix{W}\in \mathbb{R}^{I\times J}$,  
we can factorize the two dimensions into a product of natural numbers, and reshape it into an $n$-dimension tensor $\Matrix{W}_{i_1,\dots, i_n, j_1, \dots, j_n}$, which  satisfies:
\begin{equation}
\small
    \prod_{k=1}^{n} i_k = I, \quad \prod_{k=1}^{n}j_k = J.
\end{equation}
This decomposition can be written as:
\begin{equation}
    \small
   \Matrix{W}_{i_1,\dots , i_n, j_1, \dots, j_n} = \Tensor{T}^{(1)}[i_1, j_1]\cdots \Tensor{T}^{(n)}[i_n, j_n],
\label{eq:mpo-decom}
\end{equation}
where the $\Tensor{T}^{(k)}[i_k, j_k]$ is a 4-dimensional tensor with size $d_{k-1}\times i_k \times j_k \times d_k$ in which $d_k$ is a bond dimension linking $T^{(k)}$ and $T^{(k+1)}$ with $d_0=d_n=1$.
with size $d_{k-1}\times i_k \times j_k \times d_k$ in which $\prod_{k=1}^{n}i_k=I, \prod_{k=1}^{n}j_k=J$ and $d_0=d_n=1$. This bond dimension indicates the associative strength between two adjacent tensors. 
For clarity, we can rewrite the decomposition results as central tensor  $\mathcal{C}$ and auxiliary tensors $\{ \mathcal{A}_i \}_{i=1}^{n-1}$. As an important merit, such a decomposition
can effectively reorganize and aggregate the information of the matrix~\cite{gao2020compressing}:   central tensor  $\mathcal{C}$ can encode the essential information from the original matrix, while  auxiliary tensors $\{ \mathcal{A}_i \}_{i=1}^{n-1}$ serve as its complement to precisely reconstruct the matrix. 
}


\ignore{According to ~\citet{gao2020compressing}, the original matrix $\Matrix{W}$ may be precisely reconstructed using tensor contraction without the connecting bond $\{d_k\}_{k=1}^n$ being truncated.
After MPO decomposition, the central tensor can encode the essential data from the original matrix, while the other auxiliary tensors serve as its complement.
}

\subsubsection{MPO-based Scaling to Deep Models }
\label{subsec-mpobased_scaling}
Based on MPO decomposition, the essence of our scaling method is to share the central tensor across layers (\emph{capturing the essential information}) and keep layer-specific auxiliary tensors (\emph{modeling layer-specific  variations}). Fig.~\ref{fig:main} shows the overview architecture of the proposed MPOBERT. 
%Since central tensor can encode the major information of the original matrix, our idea is to share the central tensor across layers,  

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.8\textwidth]{section/figs/main2.pdf}
%     \caption{Overview architecture of MPOBERT and MPOBERT$_{+}$. We use blocks with dashed borderlines to represent shared central tensors. Central tensors are shared across all $L$ Layers in MPOBERT and within groups in MPOBERT$_{+}$.}
%     % \textcolor{blue}{~(Left) We share the parameter $\Matrix{W}^{(1)}$ for all the layers.~(Middle) The tensor $\Tensor{C}$ is shared across all blue blocks while the auxiliary tensors are kept as layerwise parameters. We also add layer-specific adapters for each layer in yellow blocks.~(Right) We divided all the layers into three groups and shared the same set of central tensors in each group.}}
%     \label{fig:main}
% \end{figure*}

\begin{figure}[t]
    \centering
    \hspace{-0.33cm}\includegraphics[width=0.5\textwidth]{section/figs/main10.pdf}
    \caption{Overview architecture of MPOBERT and MPOBERT$_{+}$. We use blocks with dashed borderlines to represent shared central tensors. Central tensors are shared across all $L$ Layers in MPOBERT and within groups in MPOBERT$_{+}$.}
    % \textcolor{blue}{~(Left) We share the parameter $\Matrix{W}^{(1)}$ for all the layers.~(Middle) The tensor $\Tensor{C}$ is shared across all blue blocks while the auxiliary tensors are kept as layerwise parameters. We also add layer-specific adapters for each layer in yellow blocks.~(Right) We divided all the layers into three groups and shared the same set of central tensors in each group.}}
    \label{fig:main}
\end{figure}

\paratitle{Cross-layer Parameter Sharing}. To introduce our architecture, we  consider a simplified structure of $L$ layers, each consisting of a single matrix. With the five-order MPO decomposition (\ie $n=5$), we can obtain the decomposition results for a weight matrix ($\Matrix{W}^{(l)}$), denoted as $\{\Tensor{C}^{(l)}, \Tensor{A}_1^{(l)}, \Tensor{A}_2^{(l)}, \Tensor{A}_3^{(l)}, \Tensor{A}_4^{(l)}\}_{l=1}^{L}$, where   $\Tensor{C}^{(l)}$ and $\{\Tensor{A}_i^{(l)}\}_{i=1}^{4}$ are the central tensor and auxiliary tensors of the $l$-th layer. Our approach is to set a shared central tensor $\Tensor{C}$ across layers, which means that  $\Tensor{C}^{(l)}=\Tensor{C}$ ($\forall l=1\cdots L$). 
As shown in \citet{gao2020compressing}, the central tensor contains the major proportion of  parameters (more than 90\%), and thus our method can largely reduce the parameters when scaling a PLM to very deep architecture.  Note that this strategy 
can be easily applied to multiple matrices in a Transformer layer, and we omit the discussion for the multi-matrix extension.  Another extension is to share the central tensor by different grouping layers. 
% 老师写的
% We implement a layer-grouping MPOBERT, named \emph{MPOBERT$_{+}$}, in which it divides the layers into two parts (upper and down) and sets two unique shared central tensors. 
% 我改的
We implement a layer-grouping MPOBERT, named \emph{MPOBERT$_{+}$}, which divides the layers into multiple parts and sets unique shared central tensors in each group.

\ignore{To be more specific, we present a conceptual formulation about such a weight-sharing mechanism: 
\begin{equation}
    \Vector{h}_{l+1}=f(\Vector{h}_l; \Matrix{W}_0, \Matrix{W}^{'(l)}), l=0,1,\cdots,L-1, 
\label{eq-weight-sharing}
\end{equation}
where  $\Vector{h}_l$ denotes the hidden representation from the output of the $l$-layer, and 
shared parameters  (\ie central tensors) and layer-specific parameters (\ie auxiliary tensors and other adaptation parameters) are denoted as  $\Matrix{W}_0$ and $\Matrix{W}^{'(l)}$, respectively.  
}


%In particular, we propose the utilization of MPOBERT+, an extension of the all-layer sharing approach, which employs a layer-wise grouping strategy to enhance model capacity. That is, we partition all layers into multiple distinct groups and implement cross-layer parameter sharing within each group~(Fig.~\ref{fig:main}).
%To be concrete, we consider $L$ layers, so we have $L$ parameter matrix in total, denote by $\Matrix{W}^{L}_{l=1}$. Specifically, we denote shared parameters as $\Matrix{W}_0$ while specific parameters as $\Matrix{W}^{'(l)}$. Therefore, weight-sharing models can be formulated as Eq.~\ref{eq-weight-sharing}:
\ignore{\begin{equation}
    \Vector{h}_{i+1}=f(\Vector{h}_i; \Matrix{W}_0, \Matrix{W}^{'(i)}), i=0,1,\cdots,L-1.
\label{eq-weight-sharing}
\end{equation}
}


\ignore{
As discussed in the MPO decomposition process, a matrix can be decomposed into $n$ tensors, \ie one central tensor, and $n-1$ auxiliary tensors.
We consider five decomposed tensors~(\ie $n=5$) in this work for convenience.
The decomposition results can be denoted as $\{\Tensor{C}^{(l)}, \Tensor{A}_1^{(l)}, \Tensor{A}_2^{(l)}, \Tensor{A}_3^{(l)}, \Tensor{A}_4^{(l)}\}_{l=1}^{L}$, where $\Tensor{C}^{(l)}$ and $\{\Tensor{A}_i^{(l)}\}_{i=1}^{4}$ are the central tensor and auxiliary tensors of the $l$-th layer.
The fundamental concept behind developing a parameter-efficient PLM is to share the central tensor across multiple layers and to maintain layer-specific auxiliary tensors as layer-wise parameters, and we designate the global central tensor as $\Tensor{C}^{(l)}$.
% To develop a parameter-efficient BERT model, the core idea is to share the central tensor across different layers and keep layer-specific auxiliary tensors as layerwise parameters, and we denote the global central tensor as $\Tensor{C}^{(l)}$.
In this way, we can only keep one central tensor for each Transformer layer.
}

\ignore{
Second, we propose a simple yet effective cross-layer parameter sharing based on MPO decomposition. 
For Transformer-based models, there are mainly two major structures, namely feed-forward network~(FFN) and multi-headed attention~(MHA). 
Without loss of generality, we can consider a simple case in which each Transformer layer contains exactly one parameter matrix. It is easy to extend to multi-matrix cases.
}

\paratitle{Layer-specific Weight Adaptation}. Unlike ALBERT~\cite{lan2019albert}, our MPO-based architecture  enables layer-specific adaptation by keeping layer-specific auxiliary tensors ($\{\Tensor{A}_i^{(l)}\}_{i=1}^{4}$). 
These auxiliary tensors are decomposed from the original matrix, instead of extra blocks~\cite{zhang2022minivit}. They only contain a very small proportion of parameters, which does not significantly increase the model size. While, another merit of MPO decomposition is that these tensors are highly correlated via bond dimensions, and a small perturbation on an auxiliary tensor can reflect the whole matrix~\cite{liu2021enabling}.  If the downstream task requires more layer specificity, we can further  incorporate low-rank adapters~\cite{hu2021lora} for layer-specific adaptation. Specifically,  %we simplify the discussion by considering the weight matrix $\Matrix{W}^{(l)}$ of the $l$-th layer. 
we denote $\Matrix{W}^{(l)}_{Adapter}$ as the low-rank adapter for $\Matrix{W}^{(l)}$. 
In this way, $\Matrix{W}^{(l)}$ can be formulated as a set of tensors: $\{\Tensor{C}^{(l)},  \Tensor{A}_1^{(l)}, \Tensor{A}_2^{(l)},  \Tensor{A}_3^{(l)},  \Tensor{A}_4^{(l)}, \Matrix{W}^{(l)}_{Adapter}\}$. The parameter scale of  adapters,  $L \times r \times d_{total}$, is determined by the layer number $L$, the rank $r$, and the shape of  the original matrix ($d_{total}=d_{in}+d_{out}$ is the sum of the input and output dimensions of a Transformer Layer). Since we employ low-rank adapters, we can effectively control the number of  additional parameters from  adapters. 


\ignore{
While, auxiliary tensors only contain a very small proportion of parameters, which has limited capacity in fulfilling layer-specific variations.   
% With the cross-layer parameter-sharing strategy, we can supplement the capacity of different layers by utilizing layer-wise auxiliary tensors.
%As discussed in section~\ref{sec-framework}, the expressive ability of auxiliary tensors is limited due to the small number of parameters. 
Inspired by recent studies on adapters~\cite{hu2021lora}, we propose to add the low-rank adapters at each layer of MPOBRT. 
%for the MPO-based Transformer layer to further improve the capacity of $\Matrix{W}^{'}$ in each layer of MPOBERT. 
Specifically, we simplify the discussion by considering the weight matrix $\Matrix{W}^{(l)}$ of the $l$-th layer. We denote $\Matrix{W}^{(l)}_{Adapter}$ as the low-rank adapter for $\Matrix{W}^{(l)}$. 
In this way, $\Matrix{W}^{(l)}$ can be formulated as a set of tensors: $\{\Matrix{W}_0,  \Tensor{A}_1^{(l)}, \Tensor{A}_2^{(l)},  \Tensor{A}_3^{(l)},  \Tensor{A}_4^{(l)}, \Matrix{W}^{(l)}_{Adapter}\}$.
In inference, we first obtain the reconstruction matrix $\Matrix{W}^{(l)}$ by integrating the central tensor with the auxiliary tensor at the $l$-th layer and then add it by the adapter $\Matrix{W}^{(l)}_{Adapter}$. 
%In other words, the $\Matrix{W}^{(l)}_{Adapter}$ replaces the existing weight matrix with a trainable low-rank decomposition matrix. 
The parameter scale of  adapters is determined by the layer number, the rank $r$, and the shape of  the original matrix.
It can be computed as 
 $L \times r \times d_{total}$, where $d_{total}=d_{in}+d_{out}$ is the sum of the input and output dimension size of a Transformer Layer. Since we employ low-rank adapters, we can effectively control the number of  additional parameters from  adapters. 
}

%are determined by the layer number, the rank $r$, and the shape of the original matrix, which is $L \times r \times d_{total}$, where $d_{total}=d_{in}+d_{out}$ is the sum of the input and output dimension size of a Transformer Layer.

\subsection{Stable Training for MPOBERT}
\label{sec-efficient-training}

With the above MPO-based approach, we can scale a PLM to a deeper architecture  in a highly  parameter-efficient way. 
However, as shown in prior studies~\cite{lan2019albert,wang@2022deepnet}, it is difficult to optimize very deep PLMs, especially when shared components are involved. In this section, we introduce a simple yet stable training algorithm for MPOBERT and then discuss how it addresses the training instability issue.  

%Directly optimizing deep Transformers is difficult due to the training instability issue and large computation cost. In this subsection, we introduce how to solve the above challenges with theoretically derived initializing methods and acceleration techniques.
% \subsubsection{Tensor Initialization for Stable Optimization}
\subsubsection{MPO-based Network Initialization}
\label{sec-mpo-based-network-initialization}
Existing work has found that parameter initialization is important for training deep models~\cite{huang2020initialize,zhang2019fixup,wang@2022deepnet}, which can help  alleviate the training instability. 
To better optimize the scaling MPOBERT, we propose a specially  designed  initialization method  based on the above MPO-based architecture. 


\paratitle{Initialization with MPO Decomposition}. Since MPOBERT shares global components (\ie the central tensor) across all layers, our idea is to employ  existing well-trained PLMs based on weight sharing for improving parameter initialization.  Here, we use the released 24-layer ALBERT with all the parameters shared across layers. The key idea is to perform MPO decomposition on the parameter matrices of ALBERT, and obtain the corresponding central and auxiliary tensors. Next, we discuss the initialization of MPOBERT in two aspects. %, namely  central tensors ($\Tensor{C}^{(l)}$) and auxiliary tensors ($\Tensor{A}_i^{(l)}$). 
 For \emph{central tensors}, we directly initialize them  (each for each matrix) by the derived central tensors from the MPO decomposition results of ALBERT. Since they are globally shared, one single copy is only needed for initialization regardless of the layer depth. Similarly,  for  \emph{auxiliary tensors}, we can directly copy the auxiliary tensors from the  MPO decomposition results of ALBERT.
 
\paratitle{Scaling the Initialization}. A potential issue is that ALBERT only provides a 24-layer architecture, and such a strategy no longer supports the  initialization for an architecture of more than 24 layers (without corresponding auxiliary tensors). As our solution, we borrow the idea in \citet{wang@2022deepnet} that  avoids the exploding update by incorporating an additional scaling coefficient and multiplying  the randomly initialized values for the auxiliary tensors (those in higher than 24 layers) with a coefficient of $(2L)^{-\frac{1}{4}}$, where $L$ is the layer number. Next, we present a theoretical analysis  of  training stability. 

%we consider employing the well-trained 24-layer ALBERT model for helping initializing MPOBERT.  


\ignore{To address the instability problem during training, we propose a weight initialization method for deep MPO-based Transformers that involves initializing the central tensors with decomposed pre-trained weights and the auxiliary tensors with theoretically derived initial values.
Then we deliver a theoretical analysis to demonstrate the effectiveness of stabilizing the optimization.
We consider the typical $L$-layer MPOBERT architecture discussed in Section~\ref{subsec-parameter-efficient-architecture} and introduce our approach in two parts, the central tensors and the auxiliary tensors, respectively.
\textbf{(1) For the central tensors}, we only need to consider a single layer, regardless of the overall depth of the model. This means that we can potentially use a pre-trained shallow model to initialize very deep models. To do this, we utilize the central tensors obtained from the decomposed weights in ALBERT, as it is the only PLM that has a single layer of weights.
\textbf{(2) For the auxiliary tensors}, we find by a theoretical derivation that scaling the randomly initialized values for the auxiliary tensors with $(2N)^{-\frac{1}{4}}$ can ensure that the stability of the model during training will not be compromised as the depth increases. 
}


% --- v1
% In this part, we propose a theoretically derived initialization method for the auxiliary tensors and the central tensors respectively based on the analysis of the training stability issue of deep Transformers.
% Our primary technique for addressing the instability problem during training is limiting the scope of model changes using adaptive initialization.
% Previous studies illustrate that directly optimizing deep Transformer-based models encounters performance degradation. To address this problem,~\citet{wang@2022deepnet} has shown its effectiveness in stabilizing training by improving weight initialization. 
% However, neither of them discusses how to leverage PLMs for better initialization. 
% As a comparison, we can achieve better training stability by using initializing the central tensors with decomposed pre-trained weights and the auxiliary tensors with theoretically derived initial values. 
% Thus we start with a theoretical examination of the training instability issue, attributing it to a bound on the model updates. Then we propose our initialization method to optimize this bound.

% \begin{theorem}
%     In $N$-layer MPOBERT, we assume that $\left\| \frac{\partial F}{\partial c_1} \right\|=\mathcal{O}(1)$, \ie the gradient signal of $c_1$ from the layers above is bounded, then $\bigtriangleup F$ satisfies $\left\| \bigtriangleup F\right\|=\mathcal{O}(1)$ when for all $i=1,\cdots,N$:
%     \begin{equation}
%         (v_i^2+u_i^2)(u_Nv_N)=\mathcal{O}(\frac{1}{N})
%     \label{eq-thm2}
%     \end{equation}
% \label{theorem-2}
% \end{theorem}
% According to Theorem~\ref{theorem-2}, we get that $\{\left\|v_{i}^*-v_{i}\right\|\}_{i=1}^N$ and $\{\left\|u_{i}^*-u_{i}\right\|\}_{i=1}^N$ are bounded if Eq.~\ref{eq-thm2} is satisfied. Due to symmetry, we set $u_i=u$, $v_i=v$. 
% Thus, from Eq.~\ref{eq-thm2}, we set $\{u,v\}$ as following for initializing the auxiliary tensors: 
% \begin{equation}
%     u=v=(2N)^{-\frac{1}{4}}.
% \end{equation}
% Finally, by appropriately initializing local tensors in MPOBERT, we can ensure that the magnitude of model updates is bounded independent of model depth $N$, $\ie \left\| \bigtriangleup F\right\|=\mathcal{O}(1)$.

\subsubsection{Theoretical Analysis}

To understand the issue of training instability from a theoretical perspective, we consider a Transformer-based model $F(\Vector{x},\Matrix{W})$ with $\Vector{x}$ and $\Matrix{W}$ as input and parameters, and consider one-step update $\bigtriangleup F$\footnote{$\bigtriangleup F \overset{\bigtriangleup}{=}F(\Vector{x},\Matrix{W}-\eta\frac{\partial}{\partial \Matrix{W}}\mathcal{L}(F(\Vector{x})-y))-F(\Vector{x};\Matrix{W}).$}. %and write the (one-step) model update $\bigtriangleup $ as: $\bigtriangleup F \overset{\bigtriangleup}{=}F(\Vector{x},\Matrix{W}-\eta\frac{\partial}{\partial \Matrix{W}}\mathcal{L}(F(\Vector{x})-y))-F(\Vector{x};\Matrix{W}).$
\ignore{
\begin{equation}
\small
\bigtriangleup F \overset{\bigtriangleup}{=}F(\Vector{x},\Matrix{W}-\eta\frac{\partial}{\partial \Matrix{W}}\mathcal{L}(F(\Vector{x})-y))-F(\Vector{x};\Matrix{W}).
\end{equation} 
}
According to \citet{wang@2022deepnet}, a large model update ($\bigtriangleup F$) at the beginning of training is likely to cause the training instability of deep Transformer models. 
%the instability starts from the large model update, \ie $\left\| \bigtriangleup F\right\|$, at the beginning of training.
To mitigate the exploding update problem,  the update should be bounded by a constant, \ie $\left\| \bigtriangleup F\right\|=\mathcal{O}(1)$.
Next, we study how the $\bigtriangleup F$ is bounded with the MPOBERT. 

\ignore{
\textcolor{blue}{We first express the model updates in terms of $\bigtriangleup F$. That is, }given a learning rate $\eta$, the update to the model can be written as 
$\bigtriangleup F \overset{\bigtriangleup}{=}F(\Vector{x},\Matrix{W}-\eta\frac{\partial}{\partial \Matrix{W}}\mathcal{L}(F(\Vector{x})-y))-F(\Vector{x};\Matrix{W})$. 
% It has been shown that an exploding model update at the beginning of training can cause instability~\citep{wang@2022deepnet}.
\textcolor{blue}{~\citet{wang@2022deepnet} demonstrates the instability starts from the large model update, \ie $\left\| \bigtriangleup F\right\|$, at the beginning of training. In order to mitigate the exploding update problem, we need to make the update bounded by a constant, \ie $\left\| \bigtriangleup F\right\|=\mathcal{O}(1)$.}
% Therefore, our goal is to bound the magnitude of $\left\| \bigtriangleup F\right\|$ independently of the model depth.
}

\paratitle{MPO-based Update Bound}. Without loss of generality, we consider a simple case of low-order MPO decomposition: $n=3$ in Eq.~\eqref{eq:mpo-decom}.  Following the derivation method in \citet{wang@2022deepnet}, we simplify the matrices $\Matrix{W}$, $\Tensor{A}_1$,  $\Tensor{C}$ and $\Tensor{A}_2$ to scalars $w$,$u$,$c$,$v$, which means the parameter $w_l$ at the $l$-th layer   can be decomposed as $w_l=u_l\cdot c_l\cdot v_l$. Based on these notations, we consider $L$-layer Transformer-based model $F(x,w)(w=\{w_1, w_2, ...,w_L\})$, where each sub-layer is normalized with Post-LN: $x_{l+1}=LN(x_l+G_l(x_l,w_l))$. Then we can prove $\left\| \bigtriangleup F\right\|$ satisfies (see Theorem~\ref{app-thm1} in the Appendix):
%we next present Theorem~\ref{theorem-1} for the magnitude of $\left\| \bigtriangleup F\right\|$ for an $L$-layer MPOBERT.  Given an $L$-layer Transformer-based model $F(x,w)(w=\{w_1, w_2, ...,w_L\})$, where $w_l$ denotes the parameters in $l$-th layer and each sub-layer is normalized with Post-LN: $x_{l+1}=LN(x_l+G_l(x_l,w_l))$. 
       \begin{align}
    \left\| \bigtriangleup F\right\|\leq
    % &\sum_{i=1}^{N}\frac{1-u_ic_1v_i}{(1+u_i^2c_1^2v_i^2)^{\frac{3}{2}}}(c_1v_i\left\|u_i^*-u_i \right\| \nonumber\\
    % &+ c_1u_i\left\|v_i^*-v_i \right\| + u_iv_i\left\|c_1^*-c_1 \right\|)
    &\sum_{l=1}^{L}(c_1v_l \left\|u_{l}^*-u_{l}\right\|+ c_1u_l \left\|v_{l}^*-v_{l}\right\| \nonumber\\
    &+ v_lu_l \left\|c_1^*-c_1\right\|),
    \label{eq-theorem1}
    \end{align}
\ignore{
\begin{theorem}
    Given an $L$-layer Transformer-based model $F(x,w)(w=\{w_1, w_2, ...,w_L\})$, where $w_l$ denotes the parameters in $l$-th layer and each sub-layer is normalized with Post-LN: $x_{l+1}=LN(x_l+G_l(x_l,w_l))$. In MPOBERT, $w_l$ is decomposed by MPO to local tensors: $w_l=u_l\cdot c_l\cdot v_l$, and we share $\{c_i\}_{i=1}^{L}$ across $L$ layers: $c_l=c_1, l=1,2,\cdots,L$. Then $\left\| \bigtriangleup F\right\|$ satisfies:
\label{theorem-1}
\end{theorem}
}
The above equation bounds the model update in terms of the central and auxiliary tensors. 
Since central tensors ($c_l$) can be initialized using the pre-trained weights, we can further simplify the above bound by reducing them. With some derivations (See Corollary~\ref{app-thm2} in the Appendix), we can obtain $(v_i^2+u_i^2)(u_Lv_L)=\mathcal{O}(\frac{1}{L})$ in order to guarantee that $\left\| \bigtriangleup F\right\|=\mathcal{O}(1)$.   For simplicity, we set $u_i=v_i=(2L)^{-\frac{1}{4}}$ to  bound the magnitude of each update  independent of layer number $L$. In the implementation, we first adopt the Xavier method for initialization, and then scale the parameter values with the coefficient of $(2L)^{-\frac{1}{4}}$. 


\ignore{Since central tensors ($c_1$) can be initialized using the pre-trained weights, we can further simplify the above bound by reducing the term of $c_i$. For simplicity, we set $u_i=v_i=(2L)^{-\frac{1}{4}}$ to  bound the magnitudes of each update  independent of layer number $L$, \ie $\left\| \bigtriangleup F\right\|=\mathcal{O}(1)$.
The complete derivations for Theorem~\ref{theorem-1} and Corollary~\ref{theorem-2} are given in the Appendix.  
In the implementation, we first adopt the Xavier method for initialization, and then scale the values with the coefficient of $(2L)^{-\frac{1}{4}}$. }
%it is reasonable to assume that updates of $c_1$ are well-bounded. Then the bound in Eq.~\ref{eq-theorem1} based on MPO decomposition is actually tighter than other methods with random initialization. In order to scaling 
% --------- v1
% With this derivation we find that with the proper initialization of $\{\left\|v_{i}^*-v_{i}\right\|\}_{i=1}^N$, $\{\left\|u_{i}^*-u_{i}\right\|\}_{i=1}^N$ and $\left\|c_1^*-c_1\right\|$, we can restrict model updates to a constrained range. 

% Finally, our proposed initialization method is able to easily achieve this bound. Pre-trained weights are generally superior to randomly initialized weights, as demonstrated by previous research~\cite{huang2020initialize,zhang2019fixup} showing that initialization can reduce the difficulty of training deep models.~\citet{gong@2019efficient_stack} specifically demonstrated the effectiveness of shallow initialization of deep layers in accelerating convergence, and highlighted the superiority of pre-trained weights over random initialization. This suggests that our proposed method for initializing the central tensors will be particularly effective. For the auxiliary tensors, we only need to adjust their initialization value by a factor of $(2N)^{-\frac{1}{4}}$ to ensure depth-independence on the right-hand side of the bound. See the appendix~\ref{app-thm2} for more details on the theory behind these steps.

% --------- v2
\ignore{
\begin{corollary}
    % In $L$-layer MPOBERT, we assume that $\left\| \frac{\partial F}{\partial c_1} \right\|=\mathcal{O}(1)$, \ie the gradient signal of $c_1$ from the layers above is bounded, 
    Given that we initialize $c_1$ in MPOBERT with well-trained weights, it is reasonable to assume that updates of $c_1$ are well-bounded.
    Then $\bigtriangleup F$ satisfies $\left\| \bigtriangleup F\right\|=\mathcal{O}(1)$ when for all $i=1,\cdots,N$:
    \begin{equation}
        (v_i^2+u_i^2)(u_Nv_N)=\mathcal{O}(\frac{1}{N})
    \label{eq-thm2}
    \end{equation}
\label{theorem-2}
\end{corollary}
}
%For simplicity, we set $u_i=v_i=(2L)^{-\frac{1}{4}}$ to  bound the magnitudes of each update  independent of layer number $L$, \ie $\left\| \bigtriangleup F\right\|=\mathcal{O}(1)$. The complete derivations for Theorem~\ref{theorem-1} and Corollary~\ref{theorem-2} are given in the Appendix.   In implementation, we first adopt the Xavier method for initialization, and then scale the values with the coefficient of $(2L)^{-\frac{1}{4}}$. 
% Due to symmetry, we set $u_i=u$, $v_i=v$, and $u=v=(2N)^{-\frac{1}{4}}$ to achieve bound the magnitudes of each update to be independent of model depth $N$, \ie $\left\| \bigtriangleup F\right\|=\mathcal{O}(1)$.

% --------- v2

\paratitle{Comparison}. Previous research has shown that using designed values for random initialization can improve the training of deep models~\citep{huang2020initialize,zhang2019fixup,wang@2022deepnet}. These methods  aim to 
improve the initialization of general Transformer architectures  for training from scratch. 
 %$While these approaches aim to improve initialization for training from scratch, 
% our proposed method is easier to use and achieve the same goal by leveraging pre-trained weights in the central tensors rather than random values. 
As a comparison, we explore the use of pre-trained weights  and  employ the MPO decomposition results for initialization.  
%our suggested method achieves the same goal with less computational costs by utilizing pre-trained weights in the central tensors instead of random values.
In particular,~\citet{gong@2019efficient_stack} have demonstrated the effectiveness of stacking pre-trained shallow layers for deep models in  accelerating convergence, also showing performance  superiority  of pre-trained weights over random initialization.

 %in accelerating convergence and highlighted the superiority of pre-trained weights over random initialization. %Therefore, our proposed method for initializing the central tensors is expected to be particularly effective.
% The closest work to ours in training deep models is DeepNet~\citep{wang@2022deepnet}, which proposes a new normalization function, Deepnorm, and a theoretically derived initialization method to address the training instability issue in deep Transformers. The initialization can only work when training from scratch and thus prevent DeepNet to leverage pre-trained models to accelerate the pre-training procedure. Different from DeepNet, we initialize the central tensors in MPOBERT from decomposed weights in pre-trained ALBERT which prove to largely reduce the computational cost in the pre-training procedure.

% \subsubsection{Combined Speedup}
\subsubsection{Training and Acceleration}
% 基于我们提出的MPOBERT模型，
% Based on the MPO-based Transformer layer and initialization methods, we summarize the training process in Algorithm~\ref{alg-overall-process}.
% We summarize our proposed MPO-based Transformer layer, corresponding initialization methods, and efficient training techniques, in a single Algorithm~\ref{alg-overall-process}, which can be easily applied to different PLMs for scaling along the model depth and efficient training.
% --- V2
% Generally speaking, our proposed MPO-based Transformer layer and corresponding initialization methods can be easily applied to different PLMs for scaling along the model depth and efficient training, which we summarize in a single Algorithm~\ref{alg-overall-process}.
% --- v3
\ignore{In general, our approach can be easily adapted to various PLMs for scaling along the model depth and efficient training~(see Algorithm~\ref{alg-overall-process}). 
We implemented various strategies to reduce the pre-training cost of MPOBERT during the process of realization. The results are summarized in Table~\ref{tab-accelerating}.
}

To instantiate our approach, we pre-train a 48-layer BERT model (\ie MPOBERT$_{48}$). For a fair comparison with BERT$_{\rm{BASE}}$ and BERT$_{\rm{LARGE}}$, we adopt the same pre-training corpus~(BOOKCORPUS~\citep{zhu2015aligning} and English Wikipedia~\citep{devlin2018bert}) and pre-training tasks~(masked language modeling, and sentence-order prediction).  
We first perform MPO decomposition on the weights of ALBERT and employ the initialization algorithm in Section~\ref{sec-mpo-based-network-initialization} to set the parameter weights. During the training, we need to keep an updated copy of central tensors and auxiliary tensors: we optimize them according to the pre-training tasks in an end-to-end way and combine  them to derive the original parameter matrix for forward computation (taking a relatively small cost of parallel matrix multiplication). 

\ignore{
In general, our suggested MPO-based Transformer layer and accompanying initialization methods can be easily applied to various PLMs for scaling along the model depth and efficient training, as summarised in a single Algorithm~\ref{alg-overall-process}. Furthermore, we implemented various strategies to reduce the pre-training cost of MPOBERT and summarize the results in Table~\ref{tab-accelerating}.}
% 通过多种方法组合，我们将MPOBERT的训练时间减少了80%，让48层的MPOBERT训练时间从15天减少到3.8天，具体的实验数据我们统计在了表格中。我们参考了MPOP的模型中MPO的实现作为基础，增加了混合精度训练策略，算子融合方法，整合在了deepspeed的框架内来实现并行训练。在BERT base的结构上，原有的官方实现需要steps，训练days达到的效果，现在只需要steps，训练days即可达到。我们的方法支持48层的BERT，仅需要3.8天就可以训练完成。
% By utilizing multiple ways to accelerate the training of MPOBERT, we are able to reduce the $70\%$ training cost in pre-training MPOBERT from scratch. Following~\citet{liu2021enabling}, we implement the baseline version of MPOBERT and it cost 12.5 days for pre-training. First, we rebuild the Transformer layer in MPOBERT to fully adapt to the DeepSpeed package, which includes various optimizations, such as I/O pre-fetching, and mixed precision training. Second, we also use fused implementations for all linear-activation-bias operations and layer norms. The comparison of training costs is summarized in Table~\ref{tab-accelerating}. Compared to the baseline version~($\ie w/o DS, fp16$) we find that the pre-training of our optimized version reduces the total training cost from 12.5 days to 3.8 days while also saving 2.9$G$ Mem in total.
% --- v1
% We have implemented various strategies to reduce the pre-training cost of MPOBERT by 70\% as shown in Table~\ref{tab-accelerating}. 
% First, we utilize a mixed precision training technique to reduce memory consumption during training and speed up the multiplication process since FP16 format has a narrower dynamic range than FP32 and 
% To achieve this improvement, we adapted the Transformer layer in MPOBERT to the DeepSpeed package~\cite{rasley2020deepspeed}, which includes optimization techniques such as I/O prefetching and mixed precision training. 
% We also adopted fused implementations for linear-activation-bias operations and layer norms. These changes resulted in a reduction of total training costs~(MPOBERT 3.8 days v.s. Baseline 12.5 days~\cite{liu2021enabling}) and a saving of 2.9GB of memory.
% --- v2
% The speed of the pre-training process is typically limited by one of three factors: arithmetic bandwidth, memory bandwidth, or latency. Mixed precision training addresses two of these
% limiters. Memory bandwidth pressure is lowered by using fewer bits~(FP16) to store the same number of values. Arithmetic time can also be lowered on processors that offer higher throughput for reduced
% precision math. In addition, we also adopted fused implementations for linear-activation-bias operations and layer norms to reduce the latency since 
% --- v3
% We have implemented various strategies to reduce the pre-training cost of MPOBERT and summarize the results in Table~\ref{tab-accelerating}.
% Typically, one of three factors—arithmetic bandwidth, memory bandwidth, or latency—limits the speed of the pre-training procedure. 

% Table~\ref{tab-accelerating} describes in detail the effects of the different acceleration techniques we propose.
% ---v4


Typically, the speed of the pre-training process is affected by three major factors: arithmetic bandwidth, memory bandwidth, or latency. We further utilize a series of efficiency optimization ways to accelerate the pre-training, such as {mixed precision training with FP16} (reducing memory and arithmetic bandwidth) and {fused implementation of activation and normalization} (reducing latency). Finally, we can train the 48-layer MPOBERT at a time cost of 3.8 days (compared with a non-optimized cost of 12.5 days) on our server configuration (8 NVIDIA V100 GPU cards and 32GB memory). More training details are can be found in the experimental setup Section~\ref{sec-experimental-setup} and  Appendix~\ref{add-trianing-detail}~(Table~\ref{tab-strongest_variants} and Algorithm~\ref{alg-overall-process}). 


\ignore{First, we employ mixed precision training to alleviate memory and arithmetic bandwidth limitations. In particular, we use fewer bits~(FP16) to store the same number of variables, hence decreasing the memory bandwidth constraint. In addition, reduced arithmetic time is also possible on processors with higher throughput for reduced precision arithmetic. Second, we adopt the fused implementation technique for linear-activation-bias operations and layer norms 
which decreases the latency by reducing the number of memory accesses.
% in order to eliminate unnecessary materialization of intermediate results. 
More details are included in Appendix. In general, these changes resulted in a reduction of total training costs~(MPOBERT 3.8 days v.s. Baseline 12.5 days~\cite{liu2021enabling}) and a saving of 2.9GB of memory.
}

\ignore{
\begin{table}[h]
\centering
\small
\begin{tabular}{llcll}
\toprule
\multicolumn{1}{c}{Exp}         & BS & Mem~(GB)  & Days \\ \midrule
\multirow{2}{*}{MPOBERT$_{48}$} & 4  & 11.6     & 4.9     \\ 
                                & 8  & 16.3     & 3.8     \\ \midrule
w/o FI                          & 4  & 11.4     & 5.4     \\ 
w/o FP16                        & 4  & 18.7     & 12.2     \\ 
w/o FI,FP16                     & 4  & 19.2     & 12.5     \\ \bottomrule
\end{tabular}
\caption{A speed comparison between our optimized training framework of MPOBERT and original implementation from MPOP~\cite{liu2021enabling}. Specifically, ``FI'' is for fused implementation approach, whereas ``FP16'' stands for mixed precision training.}
\label{tab-accelerating}
\end{table}
}