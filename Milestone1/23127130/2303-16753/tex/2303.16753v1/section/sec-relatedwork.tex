\section{Related Work}
% We will review the related studies in the following two aspects.

% \paratitle{Parameter-efficient BERT}.


% <***>


\paratitle{Matrix Product Operators}.
Matrix product operators~(\emph{a.k.a.} tensor-train operators~\citep{oseledets2011tensor}) were proposed for a more effective representation of the linear structure of neural networks~\citep{gao2020compressing}, which was then used to compress deep neural networks~\citep{novikov2015tensorizing}, convolutional neural networks~\citep{garipov2016ultimate, yu2017long}, and LSTM~\citep{gao2020compressinglstm,sun2020model}.
Based on MPO decomposition, recent studies designed lightweight fine-tuning and compression methods for PLMs~\citep{liu2021enabling}, and developed  
parameter-efficient MoE architecture~\citep{gao2022parameter}.
Different from these works, our work aims to develop a very deep PLM  with lightweight architecture and stable training. 

% \paratitle{Improving Deployment of PLMs}.
\paratitle{Parameter-Efficient PLMs}.
Existing efforts to reduce the parameters of PLMs can be broadly categorized into three major lines: knowledge distillation, model pruning, and parameter sharing. For knowledge distillation-based methods~\cite{sanh2019distilbert,sun2020mobilebert,sun2020mobilebert,liu2020fastbert}, PLMs were distilled into student networks with much fewer parameters. For pruning-based methods, they tried to remove less important components~\cite{michel2019prunehead,wang2020prunestructure} or very small weights~\cite{chen2020lotterybert}. 
Moreover, the parameter-sharing method was further proposed by sharing all parameters~\cite{lan2019albert} or incorporating specific auxiliary components~\cite{reid-etal-2021-subformer-exploring,nouriboriji@2022minialbert}. Different from these works, we design an MPO-based architecture that can reduce the model size  and enable adaptation flexibility, by decomposing the original matrix. 

 %in combination with specific models~(\eg SubFormer~\cite{reid-etal-2021-subformer-exploring} for generative models and MPOE~\cite{gao2022parameter} for mixture-of-experts based models). Compared with these methods, we studied the parameter-efficient approach for extremely deep Transformer-based models to reach fewer parameters.

\paratitle{Optimization for Deep Models}.
% To obtain a deep model that is common to increase the number of layers of the Transformer, but in general, it is difficult to train this deep model directly, mainly because of the optimization instability.
Although it is simple to increase the number of layers for scaling model size, it is difficult to optimize very deep networks due to the training instability issue. Several studies have proposed different strategies to overcome this difficulty for training deep Transformer networks, including Fixup~\cite{zhang2019fixup} by  properly rescaling  standard initialization, T-Fixup~\cite{huang2020initialize} by proposing a weight initialization scheme, and DeepNorm~\cite{wang@2022deepnet} by introducing new normalization function. 
As a comparison, we study how to optimize the deep MPO-based architecture 
with the parameter sharing strategy, and explore the use of well-trained PLMs for initialization, which has a different focus from existing work. 

\ignore{The general way to obtain a deep model that is to increase the number of layers.
However, the optimization instability issues make it hard to train directly.
Several approaches have been proposed to address the issues of optimization instability. 
\citet{zhang2019fixup} proposed the Fixup initialization method to properly rescale standard initialization. 
Moreover, T-Fixup~\citep{huang2020initialize} presented a weight initialization scheme for the Transformer that eliminates the need for both layer normalization and warmup by addressing instability in the Adam optimizer. 
Another approach was proposed in~\citet{wang@2022deepnet} where they introduced a new normalization function~(DEEPNORM) to modify the residual connection in the Transformer and a theoretically derived initialization method that allows for scaling Transformers up to 1,000 layers. 
These methods prove to be effective, however, the high computational costs associated with pre-training from scratch can limit their application in resource-limited settings. 
In comparison, we propose methods to solve the optimization problem while also allowing the training process to save computational and storage costs.
}
