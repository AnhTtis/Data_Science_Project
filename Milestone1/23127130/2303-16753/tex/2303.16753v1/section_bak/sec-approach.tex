\section{MPO-BERT Architecture}
In this section, we introduce the parameter-efficient Transformer based on MPO decomposition and parameter sharing technique, which is the backbone of the proposed MPO-BERT.
Further, we write the CUDA operators suitable for tensor decomposition and adopt the Deepspeed framework to accelerate the pre-training process of MPO-BERT.
We first provide motivation in~\ref{subsec-intuition}, before defining our method in~\ref{subsec-parameter-efficient-transformer}.
The workflow is shown in Figure~\ref{fig:main-pic}.
\begin{figure*}
    \centering
    \includegraphics{}
    \caption{Caption}
    \label{fig:main-pic}
\end{figure*}
\subsection{Motivation}
\label{subsec-intuition}


% 基于ALBERT将所有Transformer共享的结果，我们认为在BERT这种堆叠Transformer结构的预训练模型中，参数共享是有效的。一方面，这种共享使得高层的Transformer可以获取低层的语义信息；另一方面，这种共享限制了不同层Transformer描述不同语义描述的能力。我们在实验部分的表5中针对BERT以及ALBERT做了语义分析，结果表明ALBERT模型确实限制了不同层的Transformer对不同语义的描述能力。直观上说，可以通过在每层Transformer中加入不同的适配器来使得不同层具有对不同语义的描述能力。然而，这种直接增加适配器的方法效率并不高。甚至有可能削弱Transformer的模型表现。
Based on the result that ALBERT shares parameters across all the Transformers, which indicates that parameter sharing is effective in PLMs with a stacked Transformer structure like BERT. On the one hand, this sharing allows the top Transformer layers to access the linguistic information of the bottom Transformer layers; on the other hand, this sharing limits the ability of different layers of the Transformer to describe different linguistic descriptions. We have done linguistic analysis for BERT as well as ALBERT in Table~\ref{} of the experimental section~\ref{}, and the results show that the ALBERT model does restrict the ability of different layers of the Transformer to describe different linguistic information. Intuitively, it is possible to make different layers have the ability to describe different linguistic information by adding different adapters to each Transformer layer. However, this direct approach of adding adapters is not very efficient. It may even degrade the model performance of the Transformer.
% 受到MPO分解可以用于预训练语言模型轻量化微调的启发，我们可以利用MPO分解提取出在Transformer中的主要信息（即中心张量），通过在不同的Transformer之间共享中心张量，同时保持不同层具有不同的辅助张量。来实线不同层描述不同的语义信息的前提下构建一个参数高效的预训练语言模型。因此，MPO-BERT是通过张量分解和参数共享来构建的参数高效的预训练语言模型。
Inspired by the fact that MPO decomposition can be used for lightweight fine-tuning of PLMs~\citep{liu2021enabling}, we can use MPO decomposition to extract the primary information (\ie the central tensor) in the Transformer by sharing the central tensor among different Transformers while keeping different layers with different auxiliary tensors. To construct a parameter-efficient PLM  that different layers of the real line describe different linguistic information. Thus, MPO-BERT is a parameter-efficient PLM constructed by tensor decomposition and parameter sharing.

\subsection{Parameter-efficient Transformer}
\label{subsec-parameter-efficient-transformer}
% 变压器网络主要由FFN和多头注意两大结构组成。如（Shazeer等人，2017；Fedus等人，2021年）所示，我们扩展了FFN层作为专家。减少信息冗余的一个简单方法是在专家之间共享一部分参数。然而，在基于变压器的网络中，专家（即FFN）主要由大的密集矩阵组成，很难共享这些矩阵的部分参数。作为我们的解决方案，我们考虑通过MPO分解来共享参数，这样推导出的中心张量就可以在矩阵之间灵活地共享。
% Transformer主要由FFN和多头注意两大结构组成。我们可以在不同的Transformer之间共享参数来构建参数高效的预训练语言模型。然而，在基于Transformer的网络中，FFN和多头注意力结构都是有大的密集矩阵构成，我们很难区分哪些参数需要共享。作为我们的解决方案，我们考虑通过MPO分解来共享参数，这样推导出的中心张量可以在矩阵之间灵活的共享。
Transformer mainly consists of two major structures, namely FFN and multi-headed attention. 
We can share parameters among different Transformers to build parameter-efficient pre-trained language models. However,  both FFN and multi-headed attention structures are composed of large dense matrices in Transformer-based networks. 
Thus, it is difficult for us to distinguish which parameters need to be shared. 
As our solution, we consider sharing parameters by MPO decomposition, so that the derived central tensor can be shared flexibly between matrices.
% # ----- py
% 这里首先回顾一下，Transformer的一般表达形式。基于这个形式，我们的跨层参数共享的贡献在于把参数分为共享部分和轻量化部分，层特定的adapter的目的是强化轻量化部分的表达能力。下面具体来说一下不同的组建的构成形式
\paragraph{Cross-layer Parameter Sharing}
% 具体来说，我们假设每个Transformer
Without loss of generality, we can consider a simple case in which each Transformer layer contains exactly one parameter matrix, and it is easy to extend to the multi-matrix cases.
We consider $L$ layers, so we have $L$ parameter matrix in total, denote by $\Matrix{M}^{L}_{l=1}$.
As discussed in Section~\ref{subsec-matrix-product-operators}, a matrix can be decomposed into $n$ tensors, \ie one central tensor and $n-1$ auxiliary tensors.
We consider five decomposed tensors~(\ie $n=5$) in this work for convenience.
The decomposition results can be denoted as $\{\Tensor{C}^{(l)}, \Tensor{A}_1^{(l)}, \Tensor{A}_2^{(l)}, \Tensor{A}_3^{(l)}, \Tensor{A}_4^{(l)}\}_{l=1}^{L}$, where $\Tensor{C}^{(l)}$ and $\{\Tensor{A}_i^{(l)}\}_{i=1}^{4}$ are the central tensor and auxiliary tensors of the $l$-th layer.
To develop a parameter-efficient BERT model, the core idea is to share the central tensor across different layers and keep layer-specific auxiliary tensors as layerwise parameters, and we denote the global central tensor as $\Tensor{C}^{(l)}$.
In this way, we can only keep one central tensor for each Transformer layer.
% 就像
\paragraph{Layer-specific Adapter}
% 通过Cross-layer Parameter Sharing策略，我们可以通过在不同的层利用辅助张量来描述不同层的语义关系。
% 然而由于辅助张量的参数比较少，其表达能力相对来说还不够强大。因此需要每层加入可控参数的Adapter，来进一步增强MPO-BERT中对不同层语义关系的描述。
% 不失一般性，我们考虑L层。我们首先将中心张量与第l层的辅助张量合并后得到重建矩阵Wl，然后将这个矩阵与Wladapter相加，这个Wladapter是一个bottleneck的结构。
% 具体来说，Wladapter将可训练的低秩分解矩阵替换现有的权重矩阵。这个适配器的参数由rank R来控制，并且原始矩阵的形状|Theta|=2xLxdmodelxr，这个L是层数，dmodel是input and output dimension size of a Transformer Lyaer。
With the Cross-layer Parameter Sharing strategy, we can describe the linguistic information of different layers by utilizing auxiliary tensors at different layers.
However, due to the small number of parameters in the auxiliary tensor, its expressive power is not powerful enough. 
Therefore, we need to add the Low-Rank adapter of controllable parameters in each layer to further enhance the description of linguistic information of different layers in MPO-BERT.
Specifically, we simplify the discussion by considering the $L$-layer. We first obtain the reconstruction matrix $\Matrix{W}^{(l)}$ by merging the central tensor with the auxiliary tensor of the $l$-th layer and then add this matrix to the $\Matrix{W}^{(l)}_{Adapter}$, which is a bottleneck structure.
In other words, the $\Matrix{W}^{(l)}_{Adapter}$ replaces the existing weight matrix with a trainable low-rank decomposition matrix. The parameters of this adapter are controlled by the rank $r$ and the shape of the original matrix $\|\Theta\|=2\times L \times d_{model}\times r$, where $L$ is the number of layers and $d_{model}$ is the input and output dimension size of a Transformer Layer.

\subsection{Optimization}
% (TODO:) Add the optimization details in this subsection.
% 在这个小节中我们主要讨论MPO-BERT在优化过程中所采用的技术细节。
% 首先，我们的MPO-BERT方法采用了Deepspeed的框架进行加速。同时我们还为MPO分解设计了特定的CUDA算子来进一步加速MPO-BERT的训练。
% 其次，我们采用了在ALBERT的权重基础上进行MPO分解后的张量作为模型的初始化方案，在这个基础上我们可以大大加快预训练过程。
% 最后，我们采用了中心张量热启动的方法，对MPO-BERT进行warm up 操作。
In this subsection, we focus on the details of the techniques used by MPO-BERT in the optimization process.
First, our MPO-BERT model uses the framework of Deepspeed for acceleration~\citep{}. We also design specific CUDA operators for MPO decomposition to further accelerate the training of MPO-BERT. 
Second, we adopt the tensor after MPO decomposition based on the weights of ALBERT~\citep{} as the initialization scheme of the model, in which we can greatly speed up the pre-training process. 
Finally, we use the central tensor warm-start method to warm up the MPO-BERT operation.
\subsection{Discussion}

% 对于通过参数共享压缩预训练语言模型的问题,目前的工作主要注重于通过共享全部的Transformer层的参数来实现的。这会影响预训练语言模型中不同层对语言学信息的表达能力，进而降低预训练语言模型表现。通过MPO分解可以有效的保持不同层对于语言学信息的表达能力的同时压缩预训练语言模型，这个想法与利用MPO分解构建参数高效的MoE框架的工作比较相似。具体而言，MPOE方法是利用MPO分解的特性来共享不同专家之间的相似信息来实现的参数高效的模型扩容方法，而我们的思路是通过共享不同层之间的相似信息来构建参数高效的预训练语言模型。同时，我们还进一步引入了不同共享层的适配器结构来进一步提高MPO-BERT模型的性能。
For the problem of compressing PLMs through parameter sharing, current work has focused on achieving this by sharing all the Transformer layer parameters. 
This affects the ability of different layers in the PLMs to represent linguistic information, which reduces the performance of the PLMs. 
The idea that MPO decomposition can effectively maintain the expressiveness of different layers of linguistic information while compressing the PLM is similar to the work on building a parameter-efficient MoE framework using MPO decomposition~\citep{gao2022parameter}. 
Specifically, the MPOE approach is an MPO-based MoE architecture by sharing information among different experts, while our idea is to build parameter-efficient PLMs by sharing similar information among different layers. 
Furthermore, we introduce the adapter structure of different shared layers to further improve the performance of the proposed MPO-BERT.
% 这种基于MPO分解的参数高效的预训练语言模型框架，不仅仅适用于BERT，它也可以提高其他预训练语言模型的效果（如，GPT，T5），这个作为我们未来的探索方向。
This parameter-efficient PLMs framework based on MPO decomposition is not only applicable to BERT, but it can also improve the effectiveness of other PLMs~(\eg  GPT~\citep{}, T5\citep{}).
We leave this exploration to future work.