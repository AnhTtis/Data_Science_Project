\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array} 
\usepackage{tabularray}
\usepackage{subcaption}
%\usepackage{xcolor}
\usepackage{float}
\usepackage[x11names]{xcolor}
%\usepackage{mathbbol}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{4599} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi
\newcommand*{\ShowNotes}{}
\ifdefined\ShowNotes
  \newcommand{\colornote}[3]{{\color{#1}\bf{#2: #3}\normalfont}}
\else
  \newcommand{\colornote}[3]{}
\fi

\newcommand {\ayellet}[1]{\colornote{blue}{AT}{#1}}
\newcommand {\gefen}[1]{\colornote{red}{GD}{#1}}
\newcommand {\elad}[1]{\colornote{teal}{EH}{#1}}

\definecolor{teaser_pink}{RGB}{190, 0, 0} % 192
\definecolor{teaser_green}{RGB}{101, 156, 64} % 84, 130, 53
\definecolor{teaser_blue}{RGB}{56, 102, 182} % 47, 85, 151
\definecolor{teaser_brown}{RGB}{159, 72, 15} % 132, 60, 12
\definecolor{teaser_gray}{RGB}{99, 99, 99} % 82, 82, 82
\definecolor{teaser_yellow}{RGB}{230, 173, 0} % 191, 144, 0
\definecolor{arrow_green}{RGB}{84, 130, 53}
\definecolor{arrow_red}{RGB}{192, 0, 0}
\definecolor{clavicula}{RGB}{127, 96, 0}


\begin{document}

%%%%%%%%% TITLE
\title{LIMITR: Leveraging Local Information for Medical Image-Text Representation}
\author{Gefen Dawidowicz \hspace{0.15in} Elad Hirsch \hspace{0.15in}  Ayellet Tal\\
Technion â€“ Israel Institute of Technology\\

}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
Medical imaging analysis plays a critical role in the diagnosis and treatment of various medical conditions.
This paper focuses on chest X-ray images and their corresponding  radiological reports.
It presents a new model that learns a joint X-ray image \& report representation.
The model is based on a novel alignment scheme between the visual data and the text, which takes into account both local and global information.
Furthermore, the model integrates domain-specific information of two types---lateral images and the consistent visual structure of chest images.
Our representation is shown to benefit three types of retrieval tasks:
text-image retrieval, class-based retrieval, and phrase-grounding.
The code will be available upon acceptance.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}


%%%%%%%%%%%%% FIGURE %%%%%%%%%%%%
 \begin{figure}[tp]
\centering
\includegraphics[width=\linewidth]{Images/teaser2}
\caption{{\bf Image-report retrieval.} 
%R-report, the number indicates the study number.
Given the visual data, which contains  both Lung Opacity \& Lung Lesion diagnoses, our model retrieves the correct {\bf R1} report.
Another report, {\bf R2}, which corresponds to another image, contains the same \textcolor{teaser_yellow}{pathologies}.
It differs in the subtle details:
\textcolor{teaser_green}{pathology description}, \textcolor{teaser_blue}{localization}, \textcolor{teaser_brown} {uncertainties}, \textcolor{teaser_gray}{normalities} and additional \textcolor{teaser_pink}{unlabelled pathologies}.
In tasks such as  class-based retrieval, {\bf R2} is considered a correct match.
Our model aims also at tasks, such as image-text retrieval, which care about the subtleties.
%Additional information, which is ignored in other tasks on CXR datasets, is marked in the reports. 
%This case demonstrates the complexity of the retrieval task and the different information that is taken into consideration.
}
\label{fig:teaser}
\end{figure}

% problem definition
X-ray imaging is one of the most common medical examinations performed,  where in the U.S. alone $70$ million scans are performed every year~\cite{Smith-Bindman2008}. 
During a post-scan routine, a medical report is written by a radiologist. 
% Therefore, paired medical reports and X-ray scans are abundant. 
% For example, Johnson et al. introduced MIMIC-CXR~\cite{Johnson2019} dataset and has made some of this data available to the public. 
% These pairs contain a lot  of information that can be utilized for better understanding and for easing the medical professionals routine.
%\elad{What is the challenge in general (even for radiologists)}
This is a challenging task  even for trained personnel, since the pathologies, which typically occupy a small portion of the image, might be characterized by subtle (sometimes distributed) anatomical changes. 
Automating the analysis has the potential to aid experts and to speed-up the process.

The task of producing a joint representation of medical image-report data inherently differs from that of natural image-text, which has been extensively explored recently~\cite{du2022survey}.
The available data is orders of magnitude smaller than that of natural images.
Furthermore, the data is highly unbalanced, since most of the examples are normal.
Even in abnormal examples, most of the image (/report) is normal.%\gefen{regions (/phrases)} are normal.

%
Due to the importance of the task, 
the available medical data has been used for a variety of applications, including class-based retrieval (retrieving data of the same diagnosis)~\cite{huang,Zhang2020,Moon,wang2022medclip},
 pathology classification~\cite{wang2022multi,huang,Boecking_2022,Zhang2020,Moon,wang2022medclip},
detection~\cite{huang,Boecking_2022,wang2022multi,lovt},
and phrase-grounding~\cite{Boecking_2022}.
Often, these tasks do not require subtle details, such as the severity of the pathology, its exact location, or findings that are beyond the pre-defined pathologies.
% Such datasets have been used for various tasks.
% In~\cite{wang2022multi,huang,Boecking_2022,Zhang2020,Moon} the data is used for classification into $5$-$7$ pathology classes.
% In~\cite{huang,Zhang2020,Moon} the task is to retrieve image \& text pairs that share the same diagnosis label.
% In~\cite{huang,Boecking_2022,wang2022multi,DBLP:journals/corr/abs-2112-02889} the focus is on detection and segmentation of the images.
% In all these tasks the rich information, which is often expressed by a small amount of detail, though makes a difference, is ignored.
% Examples include the severity of the pathology, its exact location or the existence of other findings which are beyond the common $14$ pathologies.



% Our model
% Our goal is different.  
We propose a novel model, which  learns a joint X-ray image-report representation that is attentive to subtle details, additional descriptions of the pathologies, uncertainties etc., as illustrated in Figure~\ref{fig:teaser}.
It is based on three key ideas. 
First, 
% image-text alignment is essential for multi-modal retrieval.
our method learns to utilize both global alignment (image-report pairs) that captures high-level information, such as the sheer existence of a disease,
and local alignments (region-word pairs) that capture subtle details and abnormalities.
Second, it benefits from lateral images, which are usually ignored.
% This is not surprising, as lateral images are are used for diagnosis and are often referred in the report.
Third, it utilizes domain-specific localization information. 
We elaborate below.


% key idea 1
Local alignment in medical imaging is challenging since the data is not annotated locally by bounding boxes and their labels.
This is in contrast to natural image datasets, which provide localized ground-truth information~\cite{sgraf,
DBLP:journals/corr/abs-1712-02036,
DBLP:journals/corr/KarpathyF14,
DBLP:journals/corr/abs-1803-08024,
DBLP:journals/corr/abs-1909-02701,
DBLP:journals/corr/abs-2004-06165}.
Furthermore, localization ambiguity is inherent in medical imaging, as report findings may  correspond to multiple image regions. 
To this end, we propose a new aggregation method and a new loss, which synthesize both region-word alignments within a single pair and  global and local alignments across pairs.

% key idea 2
% The main view is the frontal view but many ($~50\%$) studies also contain a lateral view, which contains crucial information for diagnosis.
Second, we propose to use lateral views, if they exist, just like radiologists do. 
These views may provide additional information, yet they are largely ignored by other representation learning works.
% most medical imaging models utilize only the frontal views ~\cite{Zhang2020,huang,Hsu,Moon}.
% We will see examples where information that appears in the report is derived from the lateral image alone. 
We introduce an attention model that learns for each portion of the report when to consider both images, when only one, and when none.
% We will discuss how the learn to weigh each image region (from either view) with respect to the words from the report.

% key idea 3
Lastly, our model utilizes some basic domain-specific localization information---the structure of the human body; for instance, the heart is always inbetween the lungs and is approximately in the same image position.
%\elad{When human experts view such images, this structure enables them an immediate orientation, which is important, especially  for finding abnormalities.}
% \gefen{We show that it is possible to} exploit this domain-specific knowledge, \gefen{even with a simple solution, and benefit from it to improve the multi-modal representations.} 
We show that we can  add global positional encoding for the whole dataset.
%to the image local representations, such that this information will be learned for the whole dataset.
This encoding also allows the network to better learn local connections between the frontal and the lateral views.

% It  will allow fine  and accurate retrieval tasks, such as retrieval of similar cases, given only one of the modalities (text/image) or retrieval of region-specific (/phrase-specific) similar pairs.

% Qualitative result that should refer to the teaser
To demonstrate the benefit of our model,  we evaluate it for three different retrieval tasks: 
(1)~{\em Text-image retrieval:} Given a report, the goal is to find the most suitable image and vice versa.
In this task, we expect the corresponding  report (/image) to be retrieved, as illustrated in Figure~\ref{fig:teaser}, where all the information that appears in the text and in the image, are taken into account.
This  task demonstrates the ability of our method to accurately capture subtleties.
(2)~{\em Phrase-grounding:} Given an image and a corresponding phrase, the goal is to produce an attention map of the similarity between the phrase and each region.
This task demonstrates the quality of the local alignment. 
(3)~{\em Class-based retrieval:} Given a textual description, the goal is to retrieve images that belong to the same class of the description.
This is the most common retrieval task.
%
We present SoTA results for all these tasks.
% For instance, we improve the SoTA results for \ayellet{TTT} task from \ayellet{XX\%} to \ayellet{YY\%} on \ayellet{.... dataset}.
% Quantitatively, using our model with only frontal view images already improve the R@1 results for the text-to-image retrieval task by $2.9\%$ in comparison to previous SoTA.
% The addition of the positional encoding improves these results by $1.5\%$.
% Making use of the lateral images further improves the results by $1.3\%$. 
% In conclusion, our method improves SoTA results by $5.7\%$. 
% Similar improvements are demonstrated on the image-to-text retrieval task. 

% Contributions
Hence, our work makes the following contributions:
\begin{enumerate}
\vspace{-0.05in}
    \item 
    We propose a novel model for learning a joint X-ray image \& report representation. 
    It is based on a new local and global alignment scheme.
    This alignment is shown to boost performance.
    % and the  integration of domain-specific knowledge.
\vspace{-0.05in}
    \item
    Our model integrates additional domain-specific knowledge---lateral images and visual structure. 
    This information further improves performance. 
\vspace{-0.05in}
    \item
    The benefit of our model is demonstrated on three retrieval tasks, showing its ability to capture fine features in both images and reports.
    We demonstrate SoTA results on commonly-used datasets.
 %   \elad{Additionally, we propose a new retrieval task of exact text-image retrieval that measures the representation quality.}

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%  section: Related work
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related work}
\label{sec:related}

% There are a variety of tasks where pairs of CXR text-image are utilized.
% They all train using the pairs in order to gain a good joint embedding.
% In this process, the image encoder \gefen{\textbackslash text encoder} is improved and later used for a specific task during inference.

Recent works on text-image multi-modal representations for {\em chest X-ray (CXR)} datasets are used for a variety of tasks.
These tasks are either uni-modal or multi-modal, 
% aim to learn a rich and informative joint representation using contrastive learning. 
% Those works focus on different tasks that utilize the learned representations.
% Furthermore, they differ in their approach to local information learning.
%We hereby describe the different tasks \gefen{and methods for local representations utilization}.
as briefly reviewed hereafter.

\vspace{0.05in}
\noindent
% {\bf Visual tasks.}
{\bf Uni-modal tasks.}
% A multi-modal learning approach exposes the network to richer information than only a single-modal.
% Hence, joint learning can improve a visual encoder, which can later be used in various visual tasks, or improve a textual encoder which can be used for language understanding tasks.
%
The common uni-modal tasks are visual, where the focus is on classification and localization.
Specifically, given an image, the goal of~\cite{Boecking_2022,huang,wang2022multi,wang2022medclip,Zhang2020} is to classify it into pre-defined diagnoses.
This can be done for multi-label classes~\cite{huang,wang2022multi,wang2022medclip,Zhang2020} or for binary classification~\cite{Boecking_2022,huang,wang2022multi,Zhang2020}.
% and for various datasets~\cite{chexpert,RSNA,wang2020covid}.
% We note that high-quality representations usually lead to high classification accuracy. 
% However, such tasks fail to measure the quality of the representations due to the different expressiveness of labels compared to free-text reports, as demonstrated in~Figure~\ref{fig:teaser}.
% \ayellet{I deleted the drawback, as it did not have any evidence. If any paper shows that the results are not as good as multi-modal, due to less information, this is what should be said here.}

%In particular, \cite{wang2022multi,huang,Zhang2020} perform multi-label classification on $5$ diagnostic labels on the Chexpert dataset~\cite{chexpert}.
%Binary classification (existence of a certain disease) is performed by~\cite{wang2022multi,huang,Zhang2020,Boecking_2022} on the RSNA (pneumonia) dataset~\cite{RSNA} and on the COVID-X dataset~\cite{wang2020covid}.
% All of those classification tasks cover only 7 pathologies, 5 labeled pathologies available on Chexpert and COVID+ pneumonia. 

%We note that the free text in the reports cover a much wider range of pathologies than the above $7$.
%This is demonstrated in Figure~\ref{fig:teaser}, where classification would indicate two  diagnostic pathologies (Lung Lesion and Lung Opacity) for which classes are learned, whereas the first report indicates an additional pathology (Overinflation), which does not belong to any class.

In localization, we are given images and aim  to learn localized information for detection and segmentation tasks \cite{Boecking_2022,huang,lovt,wang2022multi}.
These tasks are designed for single-pathology studies and rely on relatively small datasets.
Examples include  RSNA pneumonia detection~\cite{RSNA}, foreign objects detection~\cite{objectCXR} and SIIM pneumothorax segmentation~\cite{SIIM}.
%Each work focused on one or more tasks from the following: RSNA pneumonia detection \cite{RSNA}, foreign objects detection \cite{objectCXR},SIIM pneumothorax segmentation \cite{SIIM}, COVID Rural segmentation \cite{covidrural}. 
%Each of those tasks includes the localization (detection/segmentation) of only one pathology. 
%\cite{DBLP:journals/corr/abs-2112-02889} also worked on NIH CXR \cite{nih8} detection task for the detection of 8 CXR pathologies (Which are all included in the classification diagnostic labels).
%Those works added localization tasks but for a limited number of pathology and each task is designed for only one pathology.



%{\bf Textual tasks.} 
Textual analysis in this domain is less prevalent.
In~\cite{Boecking_2022}, the focus is on CXR domain-specific language understanding tasks, such as text classification and masked-token prediction.
The joint training with the images provides a superior language model for these tasks.
%for CXR  was found superior on RadNLI language understanding task~\cite{radnli}.

%A joint training with the images provided could  language model for CXR datasets
% learn to classify them into $3$ categories: entitlement, contradiction and neutral , demonstrating the domain specific language understanding. 
%They released a new domain specific language model for CXR datasets and demonstrated its improved results on RadNLI language understanding task \cite{radnli} after the joint training with the images.
%
% {\bf Classification.}
% Given (image,report) pairs, the goal of \cite{wang2022multi,huang,Boecking_2022,Zhang2020} is to learn how to classify images according to diagnoses.
% % \cite{wang2022multi,huang,Boecking_2022,Zhang2020} aimed to improve classification results on various classification tasks using the joint learning with the reports. 
% In particular, \cite{wang2022multi,huang,Zhang2020} perform multi-label classification on $5$ diagnostic labels
% % (Atelectasis, Cardiomegaly, Edema, Pleural Effusion, consolidation) 
% on the Chexpert dataset~\cite{DBLP:journals/corr/abs-1901-07031}.
% Binary classofication (is there a certain desease) is perfomred by~\cite{wang2022multi,huang,Zhang2020,Boecking_2022} on the RSNA (pneumonia) dataset~\cite{RSNA} and on the COVID-X dataset~\cite{wang2020covid}.  
% % All of those classification tasks cover only 7 pathologies, 5 labeled pathologies available on Chexpert and COVID+ pneumonia. 
% We note that the free text in the reports cover a much wider range of pathologies than the above $7$.
% This is demonstrated in Figure~\ref{fig:xrayimage}, where classification would indicate two  diagnostic pathologies (Lung Lesion and Lung Opacity) for which classes are learned, whereas the first report indicates an additional pathology (Overinflation), which does not belong to any class. 

\vspace{0.05in}
\noindent
{\bf Multi-modal tasks.}
% These tasks involve the two original modalities and have a close resemblance to retrieval.
%
Our focus is on  multi-modal tasks, for which there exist less works.
In~\cite{huang,Zhang2020,wang2022medclip} the task is {\em class-based retrieval}, i.e. given an example in one modality (image/text), retrieve examples from the other modality. The requirement is that the retrieved examples should belong to the same class of diagnoses as the query.
% (given as pathology labels in the dataset). 
% We name this task class-based retrieval, as an image-text pair is considered positive if it has the same pathology. 
% This task is usually evaluated on a subset of the CheXpert dataset~\cite{chexpert}, containing 200 images for each of $5$ or $8$ pathologies (CheXpert 5X200 \& CheXpert 8X200, respectively).
%They created for this task new datasets (CheXpert 5X200, CheXpert 8X200) as subset of the CheXpert dataset~\cite{chexpert}. 
%The datasets used for those tasks are limited to cases which contain positive labels for only one specific condition and textual information used for the retrieval is not the original reports rather a short textual descriptions. 

In {\em phrase-grounding}, introduced by~\cite{Boecking_2022}, we are given an image and a corresponding phrase describing a pathology in the image, and aim to localize the image regions that match the phrase.
%Boecking et al. \cite{Boecking_2022} released a new dataset, MS-CXR, for phrase grounding task- given an image and a corresponding phrase describing a pathology they aim to localize the image regions which correspond to the given phrase.
% matching the corresponding image regions to a given phrase describing a pathology. 
% This task requires matching free text to specific regions of the image, which makes it very challenging, as
% The grounding information is not available at inference, which makes the task very challenging.
% To succeed, the model needs to learn complex 
This is a challenging task as specific relations between phrases and  certain image features need to be learned.
% and text -- not only the existence of pathology but also associating its location to textual clues.
%This task is a more challenging task than the previous tasks mentioned above; it handles the matching of free text to a specific region of the image while taking into account the specific pathology, the localization and so on.
% Still, this task cannot provide a pure evaluation of the representation quality.
%The image and text pairs are always corresponding, and the phrases are relatively short, not expressing negations/uncertainties or descriptions of normal regions.
However, this work ignores studies with multiple pathologies, as well as more elaborated phrases that express uncertainties and descriptions of normalities.

% The examples contain only $8$ pathologies and
% However, this task still lack important details the full alignment between an image to a report provide (image to text/text to image retrieval). First, MS-CXR dataset contains descriptions of only 8 pathologies. Second, the matching of the given phrase to the corresponding image regions is always evaluated with respect to an image containing the described pathology. Third, most of the studies contain only one pathology and the phrases does not contain negations/uncertainties or descriptions of the healthy/normal regions in the image.
We introduce an additional  retrieval task, {\em text-image (or image-text) retrieval}, where the accuracy of the representation can be better evaluated.
We expect to retrieve the exact match from one modality, given the other.
%
% We therefore focus on a one-to-one image-report and report-image retrieval tasks, which require an accurate alignment of a full free text and an image. 
%To perform this task well, all the information that appears in the text and in the image, must be taken into account.
% such as the pathologies and their locations, the description of other regions which are normal or abnormal and more, to be able to distinguish different cases in a fine-grained manner.


% {\bf Utilization of local alignments.}
% \gefen{
% As most of the regions in a CXR image and most of the report describe normal findings, previous works have found it important to add a local perspective to the representation problem.
% Since ground truth local alignments are not available in this domain, it is proposed to learn an attention-weighted representation of one modality with respect to the other. 
% ~\cite{wang2020covid,lovt} use the representations to maximize the similarity between corresponding local representations of the same example. 
% ~\cite{huang} generates locally aggregated scores, in which all local representations have the same weight, to maximize the overall similarity between corresponding image report pairs.
% Although these approaches are independent, previous works prefer to not combine them. 
% We view them as complementary, each one achieves a different goal.
% In addition, we argue that weighting local representation is crucial, as they are of different importance.   
% }
%--------------------------------------------
% subsection: Datasets
%--------------------------------------------


 %%% Figure: Model
 \begin{figure*}[tb]
\centering
\includegraphics[width=1\textwidth]{Images/arch.png}
\caption{{\bf Model.}
The model consists of three blocks.
(1)~For feature extraction, 
two CNNs,  $E_{img}^{l}$ \& $E_{img}^{f}$ for the lateral~\&  the frontal images, are used as the image encoders and one pre-trained network, $E_{txt}$, is used as the text encoder.
The local visual representations are concatenated and a global representation is created using self-attention.
A global representation is similarly created for the report.
The distance between these representations forms our global alignment $\mathcal{A}_g$, which is used in our global loss, $L_g$.
(2)~During cross-modal alignment, alignments between the local representations of the two modalities are calculated (Figure~\ref{fig:cross modal alignment}). 
These alignments, $\{a_j\}$,
% calculated for each study separately, 
are used for our local internal loss $L_{int}$ (Figure~\ref{fig:local internal loss}).  
Thanks to our attention mechanism, the model works well with or without lateral images.
(3)~The local alignments are aggregated using learned significance scores, to create the final image-report similarity score, $\mathcal{A}_{agg}$. 
These scores are 
% gathered from within the batch and 
used in our local external loss~$L_{ext}$ (Figure~\ref{fig:local external loss}).
} 
\label{fig:model}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%  section: Model
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model}
Our goal is to learn an informative joint representation of X-ray images and their reports.
In this joint representation space, an image and its corresponding report should be mapped to close points, whereas mismatched pairs should reside farther apart.
%
% We are not the first to learn such representation, as discussed in Section~\ref{sec:related}.
%The previous work should be mentioned here and the next key ideas paragraph should distinguish between their ideas and our new ideas (though both should be mentioned).
%
%\gefen{Previous works have already addressed the image text representation learning problem on chest X-ray images \cite{Zhang2020,wang2022multi,Ji2021,huang,DBLP:journals/corr/abs-2112-02889,Boecking_2022}.
%\cite{Zhang2020} used contrastive learning to maximize the similarity between paired images and reports.
%\cite{wang2022multi,Ji2021,huang,DBLP:journals/corr/abs-2112-02889} extended the idea by adding local representations (image regions, report phrases) to the global image-text alignment. 
%\cite{wang2022multi,DBLP:journals/corr/abs-2112-02889} defined additional local alignment loss which aims to maximize the similarity between local representations of one modality to their corresponding representation of the other modality, all in the same example.
%The loss proposed in \cite{huang,Ji2021} is also local, but it maximizes the alignment between paired image and report and minimize the similarity between different examples.
%The loss is calculated by aggregating the local alignments, by giving them the same weight, for a given image-report pair.
%All previous works refer to those losses as parallel methods even-though each aims to a different goal. 
%Furthermore, they use only frontal view images for the image text matching tasks and filtered out the provided lateral view images.  
%}
Recall that our model realizes three key ideas.
% First, the embedding alignment should be both local, i.e. image patches \& report words, as well as global i.e. the entire image \& the whole report.
First, to account for the different importance of the local image regions (/report words) to the global alignment,  we propose a novel aggregation method of the local representations, which incorporates learned  importance.
Second, our model is the first to use lateral images for representation learning, in addition to the frontal ones, when available.
Third, our model leverages the fact that medical images are characterized by a unique and known structure of the human body.
%  by incorporating positional encoding to the local representations. 

% We focus on utilizing both local and global representations and on learning to weigh those representations in respect to their relevance to the global alignment. Our method offers a solution that uses both frontal and lateral images in studies where the two are available, as well as in studies where only one image is available. Medical images are characterized by a unique and known structure; therefore, we suggest to exploit it by incorporating positional encoding to the local representations. 

Our model, which is outlined in Figure~\ref{fig:model}, consists of three parts: 
(1)~feature extraction, which produces the local and the global representations, (2)~cross-modal alignment, which utilizes these features, as well as additional information, in order to find the alignments between the two modalities,
% , the use of the additional information, and the exploitation of the structure.
and (3)~local-alignment aggregation, which produces global alignment of an image and a report.
%
Our model optimizes the following loss (which will be elaborated upon in Section~\ref{subsec:loss}):
\begin{equation}
L=L_g+L_{ext}+L_{int}.
\label{eq:loss}
\end{equation}
Here,
the global alignment is optimized by
$L_g$, 
the local alignment of regions \& words across examples is optimized by
$L_{ext}$, and the local alignment of regions \& words within a single example is optimized by $L_{int}$.

%%%%%%%%% subsection: Feature extraction
\subsection{Feature extraction}
\label{subsec:feature}
Given an image  $x_v$ and its corresponding report $x_t$,
% Given an image  $x_v^{k}$ and its corresponding report $x_t^{k}$,  where $1 \leq k \leq N$ ($N$ is the number of studies),
%in the batch, 
the visual and the textual features, are independently extracted.
% Each of them should be informative both locally and globally.
Recall that in medical imaging, we lack local annotated data (bounding boxes and their labels); furthermore, the images suffer from  localization ambiguity, as findings 
% that appear in the report not necessarily 
may correspond to multiple image regions. 
% In the natural images domain, local representations are usually produced using pre-trained object detectors where for each detected object a local representation is produced. 
Thus, pre-trained object detectors are not as useful as in the natural domain.
% Therefore, a different approach is needed for the extraction of local visual representations. For that purpose,

\noindent
\textbf{Visual feature extraction.}
%Our goal is to produce informative global and local visual representations for a given image. 
 %
% There are numerous networks that may be utilized for visual feature extraction.
% To improve the initialization of the network we pretrain it on the task of multi-label classification, using the diagnostic labels provided in MIMIC-CXR dataset (of $14$ pathologies).
%
We benefit from the localized nature of the intermediate layers of CNNs ($E_{img}$ in Fig.~\ref{fig:model}), to obtain $N_r$ region-level visual features $\left \{ v_1,...,v_{N_r} \right \}$. 
% and extract the local features from an intermediate layer of the network, 
%where each local representation represents a patch in the  image. 
%\elad{Delete:} which represent image patches.
In particular, we use the output of the last {\em convolution} layer.
% to get $N_r$ region-level visual features $\left \{ v_1,...,v_{N_r} \right \}$. 

Next, we enrich the extracted features with our knowledge of the layout of the human body.
% , which is consistent across individuals.
Specifically, in chest X-ray the organs are approximately in the same positions (which guide  radiologists in the diagnosis process).
% They know what to look for (e.g., the heart) and where to find it (e.g., in the center).
% They are also systematic and follow their preferred order of exploring the images.
%
%
We leverage this structure by encoding and integrating it within learning. 
We realize it through adding positional encoding.
Since our positional encoding is inherent in the input, it conceptually differs from that being used in {\em transformers}, which encode the relative positions.
We sum each visual local feature vector $v_i$ with a corresponding vector that encodes its spatial $2D$ position.
In our implementation we use the $2D$ sinusoidal encoding of~\cite{vit}, as follows.
Let $v_i$ be the $i^{th}$-patch features in the image and  $(x,y)_i$ be the  coordinates of this patch. 
Our visual feature vector is then defined as the sum of the visual encoder output with the positional encoding corresponding to the patch location in the image
$v_i\leftarrow v_i+PE((x,y)_i)$,
where $PE((x,y)_i)$ is  that of~\cite{vit}.

We observe that not all local regions are as important.
Specifically, some small regions---those that contain abnormalities---should count more than others.
To this end, we apply the self-attention operator of~\cite{vaswani2017attention} on the local features, in order to extract our global image feature representation, $\bar{v}$.
% \ayellet{You should discuss the importance of this self-attention. Did others use it as well? Is there any novelty here?}
% \gefen{Previous works used the output of the final pooling layer of the encoder to produce the global representation.
This $\bar{v}$  captures the relationships between all the local representations of a given image $x_v$.
%Using the self-attention we produce a global representation which better represent the relationships between all the local representations of a given example. 

In our implementation we use {\em Resnet-50}
~\cite{resnet} as the image encoder $E_{img}$, as it has been previously shown to benefit  medical images~\cite{Boecking_2022,huang,Zhang2020}. 
%reach good results on various tasks on MIMIC-CXR dataset 

\noindent
\textbf{Frontal and lateral information.}
% Major information for Xray image analysis appears in the frontal images. However, 
Lateral images, which exist in $50\%$ of the studies, contain information that may improve diagnosis, yet they are largely ignored.
% Some useful information appears in both the frontal and lateral images, but some appear in only one of them. Nevertheless, many previous works on image-text matching filter out the lateral images.
Our approach utilizes the information from the lateral images, by learning to weigh information from both views.
% , depending on the query. 
In addition, as studies do not necessarily contain the lateral view, our model learns from studies with a single view, as well as from studies with both views.
Specifically, for each view type, we train a separate image encoder, $E_{img}^{l}$ and $E_{img}^{f}$. 
The features from the two encoders are concatenated to form the set $\left \{ v_1^f,...,v_{N_r}^f,v_1^l,...,v_{N_r}^l \right \}$ and are inserted to the cross-modal alignment module. 
When a lateral image is not available, we set $\left \{v_i^l \right \}_{i=1} ^{N_r} = \underline{\mathbf{0}}$. 

\noindent
\textbf{Textual features extraction.} 
Given a report $x_t$, it is first tokenized,  using the  vocabulary 
%\gefen{which is constructed from MIMIC III \cite{johnson2016mimic} notes} \ayellet{what is this vocabulary?}, 
of~\cite{DBLP:journals/corr/abs-1904-03323}.
The sequence of tokens is then fed into a text encoder, $E_{txt}$.
% \gefen{and popularity}.
This yields local textual features, at the token level, $\left \{ t_1,...,t_{N_w} \right \}$.
%, where $L$ is the number of the report tokens. 
Lastly, since in analogy to image regions, certain words are more important than others, we apply self-attention to extract the global textual features, $\bar{t}$.

In our implementation, we use BioClinicalBERT~\cite{DBLP:journals/corr/abs-1904-03323} as our text encoder, due to its high performance.

%\ayellet{are there other options?} 
%\gefen{(Most of the works on chest X-ray data use bioclinical bert as their text encoder. 
%One of the papers published in ECCV suggested a new text encoder specifically for chest xray. For the evaluation I kept using the same encoder as previous works in order to isolate the improvement gain from our method.) }

%Finally, both the visual and the textual features (both local and global) are each projected to a common dimension. 


%%%%%%%%% subsection: Cross-modal alignment
\subsection{Cross-modal alignment}
\label{subsec:cross modal alignment}

\begin{figure}[tb]
\centering
\includegraphics[width=0.98\linewidth]{Images/cross_modal.png}
\caption{{\bf Cross-modal alignment.} 
Given an image-report pair $(x_v,x_t)$, we compute for each word $t_j$ (e.g. \textcolor{clavicula}{clavicula}) its corresponding visual weighted representation $u_j$.
This is done by using the  similarity between each region representation $v_i^f$ to $t_j$ as the weight for this region $w_j[i]$. 
The right image shows that the regions that correspond to  "clavicula" are highlighted in blue, representing the higher weight of these regions.
The final visual representation, $u_j$, is created by a weighted sum of $v_i^f$.
}
\label{fig:cross modal alignment}
\end{figure}

\label{subsec:alignment}
% \textbf{Related Work.} Gloria [cite] used the sum of two losses, local and global losses, as their optimization objective. The global loss aims to align the global representations of image-report pairs. The local loss aims to align word based attention weighted image representations to the local-textual representations. Their method ignores the different importance of each of the local representations to the global alignment. Our method use attention to aggregate the local representations, by doing so we give higher weight to more meaningful alignments and lower weight to less relevant ones. [joint cite] also used the sum of global and local losses, while in [gloria] those losses are cross entropy based losses in this work they suggested using both cross entrophy based losses as well as ranking based triplet losses. 

% Localized [cite] created their global representations using attention pooling layer and used separate global and local losses as well. With a few differences from [Gloria cite]: 1. they defined local textual representations in the sentence level instead of in the word level. 2. They used both sentence based attention weighted image representation as well as image-regions based attention weighted textual representation. 3. In the local loss they weighted each of the image-regions/ sentences using the weight of each region/ sentence in the global representation calculation. 4. They inserted a probability term to the local image-region loss which enables nearby image regions to account as positive pairs instead of considering all different regions as negatives. Their work use the weights of each local representation in constructing the global representation as the weights of each local representation in the loss. We suggest to compute new weights that weight the importance of each region/word to the whole alignment between the text and the image.

% Similar to [localized] [mgca cite] also used weighted localized loss. Differently, they used word level local textual representations and they did not included the probability term in their local losses.

Given a visual and textual local  representations,  
 $\left \{ v_1,...,v_{N_r} \right \}$ and $\left \{ t_1,...,t_{N_w} \right \}$ respectively,
our goal is to learn the alignments between the two modalities, including 
the alignments between report words and image regions.
% given only the global information of which report matches to the given image.
%
To do that, our model should address two challenges:
(1)~the lack of local annotations connecting  the two modalities, 
(2)~the pathologies may occupy only a small area of the image, as well as a small part of the report. 
% contains information describing the pathology. 
To this end, a fine-grained approach is sought-after.

Our approach, which is illustrated in Figure~\ref{fig:cross modal alignment}, takes into account the global representations of images and reports, as well as the local representations of regions and words.
% Relying only on the global image and report representations alone would fail in capturing these nuances. 
% \ayellet{evidence? citation?} 
%\gefen{Was perviously demonstrated by other works which added local alignments to the global ones and showed improvment in the results. We can show that as well in the ablation.}
% It thus seems natural to use local alignment, between image patches and words.
% However, the ground-truth image-report alignment is only available for the global level (matching images and reports).
%
%The lack of local alignment is usually addressed by learning a \gefen{context aware local representation with respect to the local representation of the other modality and calculating the similarity between the two} \cite{Ji2021,huang,DBLP:journals/corr/abs-2112-02889,wang2022multi}. 
%
% In the following we will elaborate on how to determine  image-report alignment 
This is done both by weighing the image region features with respect to each textual feature and vice versa---by weighing the textual features with respect to each region feature.
For clarity, in the following we will describe only how this is done in the first case (weighing image regions with respect to text).
However, in the loss, both are summed.

% In order to attend to each image region with respect to each word, we apply  text-to-image attention, as illustrated in~Figure~\ref{fig:cross modal alignment}.
% To  determine the alignment between each image region to each word from the report, 
First, the cosine similarity $c_{ij}$ between $v_{i}$ and $t_{j}$ is computed, to create $c_j=[c_{1j},c_{2j},\dots,  c_{N_rj}]$.
%\{c_{ij}\}_{i=1}^{N_r}$.
It is further
normalized using softmax,  in order to get an attention weight:
%of each region with respect to each word:
\begin{equation}
{w _{j}=softmax(\lambda c_{j})}.
\end{equation} 
The attended visual feature $u_j$ with respect to the $j^{th}$ word is the weighted sum of all the visual local representations: 
\begin{equation}
{u_{j}=\sum_{i=1}^{N_r}w _{j}[i] \cdot v_{i}}.
\label{eq:uj}
\end{equation} 

% Previous works which utilized both global and local representations used the attended visual features with respect to each word $u_{j}$ and tried to approximate them to their corresponding word representations $t_{j}$ using a contrastive loss.
% Differently from these works, 
Next, we calculate the alignment between $t_j$ and its corresponding  $u_j$. 
The local alignment $a_{j}$ is calculated as:
%
\begin{equation}
{a_{j}=\mathcal{A}(u_{j},t_{j})=\frac{ u_{j}\circ t_{j}}{\left \|  u_{j}\circ t_{j} \right \|_{2}}},
\label{eq:aj}
\end{equation}
where $\circ$ is an element-wise multiplication and $\left \| \cdot  \right \|_{2}$ is the $L_2$-norm.
%
Note that unlike~\cite{huang,lovt,wang2020covid}, our alignment $a_{j}$  is a vector rather than a scalar (that averages the entries). 
This is important for our aggregation method, as will be discussed in Section~\ref{subsec:aggregation}, where we learn the importance of the different $a_{j}$'s.
Our alignments $\left \{a_{j}\right \}$ will be later used for the local alignment loss $L_{int}$.

Similarly to the local alignments, we compute the global alignment, given the global image feature vector, $\bar{v}$ and the  global report feature vector, $\bar{t}$. 
It is computed as:
%
\begin{equation}
{\mathcal{A}_{g}=\mathcal{A}(\bar{v},\bar{t})=\frac{ \bar{v}\circ \bar{t}}{\left \|  \bar{v}\circ \bar{t} \right \|_{2}}}.
\label{eq:Ag}
\end{equation}
%
This $\mathcal{A}_{g}$ will be later used for the global loss $L_g$.

The cross-modal alignment is computed between each  region in both the frontal and the lateral images to each token of the report.
Hence, if significant information appears in the two images, regions from both receive high weights.
But, when important information appears only in one of the images, only its regions will get high weights. 
In a sense, our model mimics the radiologists actions.
When a pathology is available in the two views, they examine both;
otherwise, they focus on the relevant view.

%%%%%%%%% subsection: Aggregation
\subsection{Aggregation}
\label{subsec:aggregation}
Our goal is to aggregate all the local alignments of a given image-report pair, in order to create the final representation of the pair, $a_f$.
% \gefen{In order to use the local representations with the ground-truth image-report alignment it is needed to aggregate all the local similarities of a given image-report pair.}
We do not wish to simply sum the alignments~\cite{huang}, 
since not all regions should be treated equally.
In particular, most image regions and their corresponding report descriptions are normal observations, shared by all examples in the dataset, whereas the pathologies mostly  occupy small regions. 
% of the images and similarly, general normal observations often occupy the reports. 
The distinct information, e.g. the pathology, should be given more weight.

To account for this variance, we suggest to weigh the local alignments in accordance with their informativeness.
% It learns to weigh the local alignments, so as the network will can distinguish between the different local (visual or textual) representation.
% As before, it relies on both global and local representations~\cite{Ji2021,huang,DBLP:journals/corr/abs-2112-02889,wang2022multi}.
% We achieve that by  a self-attention mechanism, where through the interaction between the local regions, it is determined which should be paid more attention to, in order to better perform the task. 
%
Let the local alignments, computed at the alignment module, be $\mathcal{A}_{T}=\left \{a_{1},a_{2},a_{3}...a_{N_w}\right \}$ (Equation~\ref{eq:aj}).
These alignments are aggregated into a single alignment vector using a weighted sum.
The weight of each vector is computed by self-attention, where (through the interaction between the local regions) it is determined which should  be more attended, in order to better perform the task.
Thus, for each~$a_t$ we compute its weight relative to the set $\mathcal{A}_T$.
Let $\bar{a}$ be the mean of $\mathcal{A}_T$.
The weight of $a_t$  is defined as:
\begin{equation}
q_{t}=\biggl( softmax \Bigl( \frac{W_q  \, \bar{a} \cdot (W_k \, \mathcal{A}_T)^T}{\sqrt{d}} \Bigr) \biggr)_t,
\end{equation} 
where $1 \leq t \leq N_w$, $W_q$ and $W_k$ are linear transformations of the self-attention, and $d$ is the feature dimension. 

The final alignment vector between an image and a report is defined as:
\begin{equation}
{a_{f}=\sum_{t=1}^{N_w} q _{t} \, (W_v \cdot a_{t})}.
\end{equation}
\label{sec:model}
%
This aggregated alignment vector represents the image-report pair.
It is  passed through an FC layer to produce the final scalar alignment score $\mathcal{A}_{agg}$, which will be later used for the external loss  $L_{ext}$.



%%%%%%%%% subsection: Loss
 \subsection{Loss}
 \label{subsec:loss}

Recall that our goal is to maximize the similarity between positive image-report pairs and minimize the similarity between negative pairs.
To achieve this, we use three instances of the contrastive loss function~\cite{DBLP:journals/corr/abs-1807-03748}, each expressing a different concept: one global and two local, as seen in  Equation~\ref{eq:loss}.
The latter two enable different regions or words to be considered as having different significance.
% \begin{equation}
% L=L_g+L_{ext}+L_{int}.
% \end{equation}
% All $3$ losses are based on the InfoNCE contrastive loss \cite{DBLP:journals/corr/abs-1807-03748}. 
We elaborate on these losses hereafter. 


The global loss, $L_g$, attempts to maximize the global alignment $\mathcal{A}_g$ (Equation~\ref{eq:Ag}) of positive image-report  pairs and minimize the global alignment of negative pairs.
% In each iteration, $N$ image-report pairs are sampled to form a batch. 
Let $(x_v^{k},x_t^{k})$ be a corresponding pair and $\tau$ be a temperature parameter. 
The loss is defined as:
%If $k=j$ the pair is considered a positive example and if $k \neq j$, it is negative.
\begin{equation}
\begin{split}
& L_g(x_v^{k},x_t^{k})=l^{x_v^{k}|x_t}+ l^{x_t^{k}|x_v}, \\ 
& l^{x_v^{k}|x_t}=-log\left ( \frac{exp(\mathcal{A}_g(x_v^{k},x_t^{k})/\tau)}{\sum_{j=1}^{N}exp(\mathcal{A}_g(x_v^{k},x_t^{j})/\tau)} \right ),\\ 
& l^{x_t^{k}|x_v}=-log\left ( \frac{exp(\mathcal{A}_g(x_v^{k},x_t^{k})/\tau)}{\sum_{j=1}^{N}exp(\mathcal{A}_g(x_v^{j},x_t^{k})/\tau)} \right ).
\end{split}
\label{eq:lext}
\end{equation}
Here, for a given $x_v^{k}$, $l^{x_v^{k}|x_t}$ aims to increase its similarity to its corresponding report and decrease its similarity to other reports ($j \neq k$). 
Similarly, for a given report $x_t^{k}$, $l^{x_t^{k}|x_v}$ aims to increase its similarity to its corresponding image and decrease its similarities  to other images in the batch.

The local external loss, $L_{ext}$, aims to increase the similarity of positive pairs and decrease the similarity of negative ones, this time through the use of the local alignments. 
Thus, we use the aggregated local similarity score, $\mathcal{A}_{agg}$ (Section~\ref{subsec:aggregation}), instead of $\mathcal{A}_g$, as the objective for maximization and minimization, as illustrated in Figure~\ref{fig:local external loss}.

\begin{figure}[tb]
\centering
\includegraphics[width=0.35\textwidth]{Images/local_ext.png}
\caption{{\bf Local external loss.} 
Top: Given a local textual representation $t_j$ (\textcolor{clavicula}{clavicula}) and its corresponding weighted visual representation $u_j$, we generate an aggregated alignment score $\mathcal{A}_{agg}$, based on the local alignments $\{ a_j \}$. 
Bottom:  $\mathcal{A}_{agg}$ is used to bring closer (\textcolor{arrow_green}{green}) the corresponding image-report pair $(x_{t1},x_{v1})$ and farther away (\textcolor{arrow_red}{red}) non-corresponding pairs e.g., $(x_{t1},x_{v2})$.
% We use this aggregated similarity score in order to push apart non-corresponding image-report pairs and to pull closer corresponding image-report pairs. 
% This is done across the examples in the batch.
}
\label{fig:local external loss}
\end{figure}


The local internal loss, $L_{int}$, is given the local textual representation, $t_j$, as well as its corresponding attention weighted visual representation, $u_j$  from Equation~\ref{eq:uj}.
It aims to improve the local representations by maximizing the similarity between corresponding
%local representations and their corresponding weighed representations of the other modality and minimize the similarity between different local representation of the same example.
pairs and minimizing the similarity between non-corresponding pairs of the same example.
% To achieve this goal we suggest to utilize local contrastive loss which we term {\em local internal loss}.
The loss is defined as follows:
%
\begin{equation}
\begin{split}
& L_{int}(x_v^{k},x_t^{k})=\sum_{j=1}^{N_w}(l_{k}^{t_j|u}+ l_{k}^{u_j|t}), \\ 
& l^{t_j|u}_k=-log\left ( \frac{exp(a_j(t_j,u_j)/\tau)}{\sum_{i=1}^{N_w}exp(a_j(t_j,u_i)/\tau)} \right ),\\ 
& l^{u_j|t}_k=-log\left ( \frac{exp(a_j(t_j,u_j)/\tau)}{\sum_{i=1}^{N_w}exp(a_j(t_i,u_j)/\tau)} \right ).
\end{split}
\label{eq:lint}
\end{equation} 
%
Here, for a local textual representation $t_j$,  $l^{t_j|u}_k$ aims to increase the similarity between $t_j$ and its corresponding visual weighted representation, $u_j$, and to decrease the similarity to other
% visually weighted representations 
$u_{m \neq j}$ from the same study. 
The same rationale applies to $l^{u_j|t}_k$.
We calculate $l^{t_j|u}_k$ and $l^{u_j|t}_k$ separately, for each image-report pair $(x_v^{k},x_t^{k})$, as illustrated in Figure~\ref{fig:local internal loss}. 
Finally, we sum across all local pairs for a given study and then across all studies in the batch. 
%
%\ayellet{???} In order to use the local alignments vectors $\vec{a_j}=\mathcal{A}(u_j,t_j)$  (Section~\ref{subsec:cross modal alignment}), we pass them through a FC layer, to obtain scalar similarity scores, $a_j$. 
Recall that the procedure described here is performed also for the visual representation in regards to the weighted textual representations.

\begin{figure}[tb]
\centering
\includegraphics[width=0.74\linewidth]{Images/local_int_loss.png}
\caption{{\bf Local internal loss.} 
Given local textual representations, $t_j$, and visual representations, $u_j$, this loss brings closer (\textcolor{arrow_green}{green}) corresponding representations, e.g.  $t_{j("clavicula")}$ and $u_{j("clavicula")}$ and further apart (\textcolor{arrow_red}{red}) non-corresponding representations, e.g. $t_{j("cardiac")}$ and $u_{j("clavicula")}$, from the same example.
}
\label{fig:local internal loss}
\end{figure}


% To achieve this, we use the contrastive loss for maximizing the posterior probability of the image given its corresponding report as well as the  the posterior probability of the report given its corresponding image. 

%%%%%%%%% subsection: Frontal and Lateral information
% \subsection{Frontal and lateral information}
% Major information for Xray image analysis appears in the frontal images.
% However, lateral images, which exist in $50\%$ of the studies, contain information that may improve diagnosis.
% Some useful information appears in both the frontal and lateral images, but some appear in only one of them. 
% % and are mentioned in the reports themselves.
% % Almost $50\%$ of the studies in MIMIC-CXR dataset contain both frontal and lateral images. 
% % These, of course, appear in the dataset for a reason.
% Nevertheless, many previous works on image-text matching
% % In that setting, matching reports to images should clearly rely on both the frontal and lateral views.
% filter out the lateral images.
% % , hence dropping crucial information for matching images and their  reports.
% %In some cases, the lateral images are directly referred in the reports and, in others, they contain additional information which is essential to the diagnosis. 
% %All the previous image-text matching works on this dataset filtered the lateral images and used only the frontal view images. 
% %As a result, they ignored crucial information for the task of matching the images with the correct report. 
% %
% Our approach utilizes the information from the lateral images, as seen in Figure~\ref{fig:model}. 
% % On the one hand, information which exists in both images is likely to be significant and to be described in the report.
% %On the other hand, crucial information may appear only in one of the images. 
% It learns to weigh information from both views, depending on the query. 
% In addition, as studies do not necessarily contain the lateral view, our model learns from studies with a single view, as well as from studies with both views.
% Specifically, for each view type, we pre-train a separate image encoder ($E_{img}^{l}$ and $E_{img}^{f}$), as described in Section~\ref{subsec:feature}. 
% The features from the two encoders are concatenated to form $\left \{ v_1^f,...,v_{N_r}^f,v_1^l,...,v_{N_r}^l \right \}$ and inserted to the cross-modal alignment module. 
% When a lateral image is not available,  $\left \{v_i^l \right \}_{i=1} ^{N_r} = \underline{\mathbf{0}}$. 

% Then, the alignment module calculates the cross-modal alignment between each image region (frontal and lateral) to each token of the report.
% %In the alignment module, weights are calculated for each image region (from both the frontal and lateral images) in respect to each word in the report. 
% Hence, if significant information appears in the two images, regions from both receive high weights;
% but, when important information appears only in one of the images, only its regions will get high weights. 
% % A key strength of this approach is that it also holds when there is only a single input image.
% %Our model learns to exploit both images to understand the patient's pathologies and result in a better match to the correct report.
% In a sense, our model mimics the radiologists actions.
% When a pathology is available in the two views, they examine both;
% otherwise, they focus on the relevant view.


% %%%%%%%%% subsection: Positional encoding
% \subsection{Utilizing domain structure}
% The layout of the human body is consistent across individuals.
% %, each feature and shape in an Xray image corresponds to a specific anatomical part.
% In particular, chest Xray images have a unique spatial structure---the organs are approximately in the same positions.
% % and the same organs are present in all images. 
% This structure guides the radiologists' diagnosis process.
% They know what to look for (e.g., the heart) and where to find it (e.g., in the center).
% %Usually, they will first look for the heart in the center of the image, further look for the lungs in both sides of the heart and so forth. 
% They are also systematic and follow their preferred order of exploring the images.
% % This is efficient due to their fast orientation, derived from the known image structure.
% % Spatial structure is consistent among patients and appears in both the frontal and lateral views.
% % %Moreover, some of the anatomical structures appear both in the lateral and the frontal images, and the connection between these structures is consistent across patients. 

% We propose to leverage this structure by encoding and integrating it within the visual features learning. 
% %That way the network can associate certain patches with the anatomical structures they represent. 
% % This help the network to learn useful patch representations, not only by processing the visual patterns but also the visual location. 
% % Similarly to the prior knowledge of human experts (radiologists) that know where they look and what they look at.
% % utilizing positional encoding.
% We realize this idea by adding position encoding.
% Unlike positional encoding used in transformer training that encodes the relative position in a sequence, our positional encoding is an inherent part of the input.
% % , e.g. the heart always have roughly the same value.
% This is realized by summing each visual local feature vector $v_i$ with a corresponding vector that encodes its spatial $2D$ position.
% %The local visual features in our model are inserted to the aggregation module as an unordered array. 
% %Inspired by the use of positional encoding in transformers, we propose to add positional encoding to the local visual features to use this localization information. 
% In our implementation we use a $2D$ sinusoidal encoding of \ayellet{[cite-vit]}.
% Let $v_i$ be a patch in the image and  $(x,y)_i$ be the  coordinates of this patch. 
% Our visual feature vector is then defined as the sum of the visual encoder output with the positional encoding corresponding to the patch location in the image
% $v_i\leftarrow v_i+PE((x,y)_i)$,
% where \ayellet{$PE((x,y)_i)=$...}


% Let $x$ and $y$ $\in [0,\sqrt{K})$ (where $K$ is the number of image regions) be the horizontal and vertical spatial position of a local feature vector.
% \ayellet{where is $v_i$ in this equation?} \gefen{each local representation $v_{i}$ represents a patch in the original image whose coordinates are $(x_{i},y_{i})$ in the extracted features map.}
% % \begin{flalign}
% \gefen{
% \begin{equation}
% \begin{split}
% &PE(v_{i})^{(j)}=PE(x_{i},y_{i})^{(j)}= \\ 
% & \left\{\begin{matrix}
% & sin(x_{i}\cdot \omega_{k}), \; \mathrm{if}\; j=2k \; \mathrm{and}\; j \in [0,d/2) &\\  & cos(x_{i}\cdot \omega_{k}), \;\mathrm{if}\; j=2k+1 \; \mathrm{and}\; j \in [0,d/2) &\\
% & sin(y_{i}\cdot \omega_{k}), \; \mathrm{if}\; j=2k \; \mathrm{and}\; j \in [d/2,d) &\\ 
% & cos(y_{i}\cdot \omega_{k}), \;\mathrm{if}\; j=2k+1 \; \mathrm{and}\; j \in [d/2,d) 
% \end{matrix}\right.\\
% \end{split}
% \end{equation}
% }
% where $\omega_{k}=\omega^{4k/d}$
% ($\omega=10^{-5}$ in our implementation),
% % \end{flalign}
% $d$ be the feature vector dimension, 
% $k  \in [0,d/4)$ \ayellet{1. why? 2. $i$ is used in $v_i$} \gefen{for both odd and even indices we use the same $\omega_{k}$, for odd indices we use it with cos and for even with sin. In addition, the feature vector is served to represent both horizontal and vertical positions , the first half of the vector is dedicated to represent the horizontal position and the second half to represent the vertical position, for both horizontal and vertical representations we use the same $\omega_{k}$ therefore it is used four times in each positional encoding vector.}
% and $j \in [0,d)$ be the index in the feature vector. 

%where $\omega=10^{-5}$, $(x,y)$ specify the local feature position in the image, $D$ the features dimension, and $i,j  \in [0,D/4)$. We sum each positional encoding with the matching local feature before passing the local features to the alignment module.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% section: Results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental results}
\label{sec: experiments}

We examine our method on three retrieval applications:
text-image retrieval, phrase-grounding, and class-based retrieval.
For each application we compare against previous works that evaluate the specific application, as well as against additional methods that we trained.

% According to the patient's condition and symptoms it is decided in which radiological views to perform the examination. 
% Most of the examinations are performed in a frontal view, either {\em Anterior Posterior (AP)} or {\em Posterior Anterior (PA)} and in some of the cases also a {\em lateral view} scan is performed. 
% The radiologist examine all the scans performed in the same examination and summarize his observations in the same report.

%%%%%%%%%%%%%%%%%
\vspace{0.05in}
\noindent
{\bf Datasets.}
We ran our experiments on three datasets: {\em MIMIC-CXR}, {\em CheXpert 5X200}, and {\em MS-CXR}.

(1)~{\em MIMIC Chest X-ray (MIMIC-CXR)} is a large, publicly available, dataset of chest radiographs, with free-text radiology reports~\cite{Johnson2019}. 
The dataset contains $377,110$ images, corresponding to $227,835$ radiographic studies performed at the Beth Israel Deaconess Medical Center. 
Each study in the dataset contains a report and one or more images of
{\em frontal} or {\em lateral}, where each study
 contains at least one frontal image. 
Each radiograph is associated with one or more classes, out of $14$ optional diagnostic labels. 
% The structure of the reports is generally consistent.
%, based on a standardized documentation template.
The two main sections of interest of the report are \textit{findings} and \textit{impression}.
%
% We use MIMIC-CXR to train our model and to evaluate the retrieval tasks.
Studies that do not contain these sections are filtered out. 
% We use only one frontal view image per study. 
The data is randomly sampled to obtain validation and test sets, containing $1,000$ studies each.
The remaining training set contains $205,000$ studies, of which $100,000$ have both frontal and lateral images. 
% The same statistics hold across the other splits as well. 
%

(2)~{\em CheXpert 5X200} 
% contains examples of $5$ abnormality categories~\cite{huang}. 
contains $200$ image-report pairs for $5$ abnormality categories~\cite{huang}, sampled from the CheXpert dataset~\cite{chexpert}. 
Each example in the dataset belongs to a single abnormality category.

%The data was collected using existing annotations from CheXpert dataset [cite] and additional expert annotations from radiologists. 
%The dataset contains 8 abnormality categories, for each 5 short descriptions (written by a radiologist) and 200 images. 
% Hence, each image is associated with one category, which is represented by $5$ descriptions.
% We use this dataset in the same setting as~\cite{Zhang2020} for class based retrieval evaluation.
%
(3)~{\em MS-CXR} is a subset of  MIMIC-CXR, which is extended for phrase-grounding~\cite{Boecking_2022}.
It contains labeled (text descriptions) bounding boxes for each image.
In total, there are $1,153$ pairs of region-text pairs. 
%created a dataset contains image bounding box labels paired with radiology text description, in total 1,153 bounding box-phrase pairs. 
%We use this dataset in the same setting as~\cite{Boecking_2022} for phrase grounding evaluation.  

% {\em OpenI-IU} is a publicly available dataset of Xray images \& reports, which contain $3,996$ images and  $8,121$ corresponding reports, collected in the Indiana University hospital \ayellet{add citation}. 
% The dataset contains both lateral and frontal view images. 
% \ayellet{Are the reports also findings/impressions or are they simply free-style?}

% {\em CheXpert} is a large public dataset for chest radiographs, consisting of $224,316$ chest radiographs, collected in Stanford university hospital \ayellet{add citation}. 
% The dataset contains both lateral and frontal view images. 
% The reports of this dataset are not available to the public.



%%%%%%%%%%%%%%%%%%%
\vspace{0.05in}
\noindent
{\bf Text-Image retrieval.}
Given a report, our goal is to find the most suitable image and vice versa.
%, given an image our goal is to retrieve its matching report.
This task evaluates how close matching pairs are in the feature space.
Hence, for this task, only the ground-truth image-report pairs are considered as positive. 
% To test our model for image-to-text retrieval, a query image is input to the model and the top $K$ similar reports (out of the test examples) are retrieved.
% Similarly, for text-to-image retrieval the input is a report and the output are matching images.
% For the frontal-lateral model evaluation, we use the same splits.
When both the frontal and the lateral images are available we use them both;  when only one exists we fill the missing image with zeros.
The accuracy of our model is measured by the $Recall@K$ metric, which returns the percentage of queries whose true match is successfully ranked within the top $K$ matches.


%%%%% Table
\begin{table}[tp]
% \small
% \begin{tabular}{m{0.9in}|m{0.15in}m{0.15in}m{0.3in}|m{0.12in}m{0.12in}m{0.15in}}
\begin{tabular}{m{0.83in} | >{\centering}m{0.19in} >{\centering}m{0.16in} >{\centering}m{0.3in} | >{\centering}m{0.19in} >{\centering}m{0.16in} >{\centering\arraybackslash}m{0.3in}}
\multirow{2}{*}{Method} 
& \multicolumn{3}{c|}{\textbf{Image-to-Text}} & \multicolumn{3}{c}{\textbf{Text-to-Image}} \\
& R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\ \hline
% & \multicolumn{1}{l}{R@1} & \multicolumn{1}{l}{R@5} & \multicolumn{1}{l|}{R@10} & \multicolumn{1}{l}{R@1} & \multicolumn{1}{l}{R@5} & \multicolumn{1}{l}{R@10} \\ \hline
MGCA~\cite{wang2022multi} & 25.8  & 51.9  & 62.1  & 27.9  & 51.2  & 61.6   \\
ConVIRT~\cite{Zhang2020} & 30.1  & 53.9   & 63.8   & 29.2  & 54.7 & 64.4 \\
GLoRIA~\cite{huang} & 30.3 & 57.5  & 66.5   & 24.0  & 51.8  & 62.8  \\ \hline
Ours \footnotesize{w/o LT\&PE} & \underline{36.1}  & \underline{59.1}  & \underline{69.1} & \underline{36.4} & \underline{60.7}  & \underline{70.5} \\
Ours  &  \textbf{39.7} &  \textbf{63.2}  & \textbf{71.7} & \textbf{37.7}&  \textbf{62.1} & \textbf{71.3}
\end{tabular}
\caption{\textbf{Text-Image retrieval results.}
Given a report, our goal is to retrieve the matching image and vice versa.
Our results outperform those of other methods on MIMIC-CXR, even without lateral (LT) images and structural information (PE).
Our full method further improves the results.
}
\label{table:retrieval}
\end{table}

%%%%%%%%%%%%%% Table
\begin{table*}[t]
\centering
% \small
% \begin{tabular}{l|ccccccccc}
\begin{tabular}{m{0.82in} | >{\centering}m{0.5in} >{\centering}m{0.5in} >{\centering}m{0.5in} >{\centering}m{0.5in} >{\centering}m{0.5in} >{\centering}m{0.5in} >{\centering}m{0.5in} >{\centering}m{0.5in} >{\centering\arraybackslash}m{0.5in}}

% \textbf{Method}  &\rotatebox[origin=c]{30}{ \textbf{Atelectasis}} & \rotatebox[origin=c]{30}{\textbf{Cardiomegaly}} & \rotatebox[origin=c]{30}{\textbf{Consolidation}} & \rotatebox[origin=c]{30}{\textbf{Lung Opacity}} & \rotatebox[origin=c]{30}{\textbf{Edema}} & \rotatebox[origin=c]{30}{\textbf{Pneumonia}} & \rotatebox[origin=c]{30}{\textbf{Pneumothorax}} & \rotatebox[origin=c]{30}{\textbf{Pleural Effusion}} & \rotatebox[origin=c]{30}{\textbf{Average}} \\ \hline

Method & Atelectasis & Cardio- megaly & Consoli- dation & Lung Opacity & Edema & Pneumo- nia & neumo- thorax & Pleural Effusion & Average \\ \hline

ConVIRT~\cite{Zhang2020} & 0.86 & 0.64 & 1.25  & 0.78 & 0.68 & 1.03  & 0.28  & 1.02  & 0.818   \\
GLoRIA~\cite{huang}& 0.98  & 0.53  & 1.38 & 1.05 & 0.66 & 1.18 & 0.47 & 1.2 & 0.93     \\
BioViL~\cite{Boecking_2022}& \textbf{1.17} & 0.95 &\textbf{1.45} & 1.19 & 0.96  & 1.19 & 0.74 & \textbf{1.5} & 1.142    \\
Ours    &1.16&     \textbf{1.18} &   
1.37                  &    \textbf{1.37}  &         \textbf{1.05}         &   \textbf{1.27}   &    
\textbf{1.01}       &         1.24      &  
\textbf{1.206}
\end{tabular}
\caption{\textbf{Phrase-grounding results.}
Given a phrase and an image, the goal is to produce a similarity map between the phrase and the image. 
Our results outperform those of other methods, as reported in~\cite{Boecking_2022}, on MS-CXR. 
The results are measured using CNR; higher values indicate good localization of the phrase in the image. 
Qualitative results can be found in the supplementary materials.
}
\label{table: phrase grounding}
\end{table*}

Table~\ref{table:retrieval} compares our performance on MIMIC-CXR to
SoTA  methods for representation learning, ConVIRT~\cite{Zhang2020}, GLORIA~\cite{huang} and MGCA~\cite{wang2022multi}, which we have trained. 
% As we are the first to perform accurate image-text retrieval on those datasets,  we trained and evaluated the following models on the same splits of MIMIC-CXR dataset. 
% To enable future fair comparisons on this task, the splits will be available on our GitHub upon acceptance. 
%
Our method outperforms other methods in R@1, R@5 and R@10, both for the image-to-text task and for the text-to-image task, even without utilizing structural knowledge and lateral images.
% This demonstrates the strength of our model and our utilization of the local representations.
Encoding domain structure and using the additional lateral images further improve the results.
% and will be discussed in~\autoref{sebsec:ablation}. 

%Encoding domain structure, using positional encoding, further improves the results.
%The addition of the lateral images further improves the performance in all metrics , which validates our assumptions on the importance of those additional available images.
 
%\elad{Unclear:} Indicating that providing the network information about this shared structure , between examples in the datasets,  can help the network to better learn localized patterns in the dataset.

%%%%%%%%%%%%%%%%%%%
\vspace{0.05in}
\noindent
{\bf Phrase-grounding.}
Given an image and a corresponding phrase, the goal is to produce an attention map of the similarity between the phrase and each image region~\cite{Boecking_2022}. 
The ground-truth is given as bounding boxes.
%
Hence, this task evaluates the local alignments. 
%
Following~\cite{Boecking_2022}, we measure the performance using {\em contrast to noise ratio (CNR)}, which measures the attention density inside the ground-truth bounding box.
High values indicate that the network accurately detects the regions of interest for the given phrase.
It is defined as: 
\begin{equation}
   CNR=\left | \mu _A-\mu_{\bar{A}} \right |/(\sigma_A^2+\sigma_{\bar{A}}^2)^{\frac{1}{2}},
\end{equation}
where $A$ and $\bar{A}$ are the interior and exterior of the bounding box, and $\mu$ and $\sigma$ are the mean and variance of the attention maps in each region. 
%
%\gefen{delete:}Since our attention maps (Section~\ref{subsec:alignment}) are computed for each word, for the given phrase
%which are calculated to create the attended visual representations $u$ as described in Section~\ref{subsec:alignment}.
%we simply average them to get a phrase attention map.

Table~\ref{table: phrase grounding} presents the results on the MS-CXR dataset.
The results are presented for each of the $8$ diagnostic categories, as well as for the average over the whole the dataset.
Our performance is compared to that of BioViL~\cite{Boecking_2022}, which introduced this task, and to the results of GLoRIA and ConVIRT that are reported in~\cite{Boecking_2022}.
Note that BioViL uses a different text encoder (CXR-bert) from all other methods in the table (Bioclinical-Bert). 
%
Our model achieves SoTA results, demonstrating the strength of our local representations and the localization ability of our model.


%%%%%%%%%%%%%%%%%%%
\vspace{0.05in}
\noindent
{\bf Class-based retrieval.}
Given an image, our goal is to retrieve reports that belong to the same class of the image.
For this task, a positive pair is defined as image-report that have the same abnormality label.
% For this task, each image appears in $5$ positive pairs.
We follow~\cite{huang}'s settings, performing image-to-text retrieval and using the Precision@K metric to measure the accuracy of the retrieval.  
Precision@K is defined as the fraction of retrieved items (images) within the top $K$, which belong to the same class of abnormality as that of the query (textual descriptions).

Table~\ref{table:classbasedretrieval} shows that
% As before, we compare the results to
% The results of ~\cite{Zhang2020}  are as reported in their paper.
% As in~\cite{huang,wang2022multi,Zhang2020},  we trained on MIMIC-CXR and evaluate on CheXpert-8X200.
%
our results outperform those of other methods in almost all metrics.
% \ayellet{As expected, the current task is easier than the text-image retrieval  task, as precision is less needed.}
% As such, the information needed to succeed in the latter exists in model that succeed in the former.
%This indicates that, when aiming to match the whole report to the corresponding image, the pathologies (class labels) information is included and, therefore, the results are also improved on class-based retrieval tasks.
We note that~\cite{huang} was originally trained and evaluated on the  CheXpert dataset~\cite{chexpert}.
Since the reports of CheXpert are unavailable to the public, we trained on MIMIC-CXR~\cite{johnson2016mimic} and evaluated the retrieval in a zero-shot manner on the CheXpert 5X200 dataset, for all models.

\begin{table}[tb]
\centering
\begin{tabular}{l|lll}
%\multirow{2}{*}{} & \multicolumn{3}{c}{\textbf{Text to Image}}                                     \\
Method    & Prec@5         & Prec@10        & Prec@100         \\ \hline
ConVIRT~\cite{Zhang2020}   & \multicolumn{1}{c}{30.8} & \multicolumn{1}{c}{28.2} & \multicolumn{1}{c}{22.2} \\ 
GLoRIA~\cite{huang}   &     \multicolumn{1}{c}{32.6}                      &               \multicolumn{1}{c}{33.4}            &    \multicolumn{1}{c}{\textbf{29.0}}              \\
MGCA~\cite{wang2022multi}     &  \multicolumn{1}{c}{29.3}                       &    \multicolumn{1}{c}{27.6}                      &    \multicolumn{1}{c}{22.4}                      \\\hline
Ours     & \multicolumn{1}{c}{\textbf{37.2}}     & \multicolumn{1}{c}{\textbf{35.9}}     & \multicolumn{1}{c}{28.8}    
\end{tabular}
\caption
{\textbf{Class-based retrieval results.}
Given an image, reports that belong to the same class are retrieved. 
Our results outperform those of other methods on CheXpert 5X200 zero-shot evaluation. 
We note that this dataset contains only frontal views.
%
% ~\cite{Zhang2020}'s retults are from their paper; we trained the other models.
% All models were trained on MIMIC-CXR dataset~\cite{johnson2016mimic}.
}
\label{table:classbasedretrieval}
\end{table}



%%%%%%%%%%%% Figure
 \begin{figure*}[t]
 \centering
 \includegraphics[width=0.94\textwidth]{Images/ablationfig.png}
\caption{{\bf Domain-knowledge importance.} 
(A) The positional information enables the model to consider the signs of right pleural effusion (\textcolor{teaser_blue}{blue}) and small left pleural effusion (\textcolor{teaser_brown}{orange}).
Thanks to our positional encoding, our model succeeds in the text-image retrieval task, but would fail otherwise.
% With positional encoding, the model aligns correctly the localization mentions in text with the correct regions, resulting in a successful match.
(B) The visual clue, the flattened diaphragm, which is a sign of over-inflation appears only in the lateral image (highlighted in \textcolor{teaser_blue}{blue}).
% and the corresponding mention in the report.
Thanks to  the lateral view, our model succeeds in the text-image retrieval task; it would fail if it used only the frontal image. % fails to align the report and the images. 
% However, the addition of the lateral view image results in a successful match. 
%\elad{Delete:} The {\color{cyan}{cyan}} highlight \ayellet{??} in the lateral image points at the flattened diaphragm which is a sign of over-inflation of the lung.  
%\ayellet{unclear}
}
\label{fig:ablation}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%
%%%% Section: ablation
%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ablation study}
\label{sec:conclusion}
This section evaluates the contribution of the different components of our method. 

\vspace{0.05in}
\noindent
{\bf Losses.} 
Our model optimizes a combination of three losses: global, local-internal and local-external (Equation~\ref{eq:loss}). 
% In those experiments, we trained our model with different combinations of our losses (only with frontal images).
Table~\ref{table: ablation losses} demonstrates the contribution of each loss for both image-to-text and text-to-image retrieval tasks.
The best performance is achieved by the combination of the three losses. 
Thus, attempting to achieve the three goals of these losses indeed improves the learning process.

%%%%%%%%%% Table
\begin{table}[tb]
% \small
\begin{tabular}{>{\centering}m{0.13in} >{\centering}m{0.13in} >{\centering}m{0.13in} | >{\centering}m{0.19in} >{\centering}m{0.16in} >{\centering}m{0.3in} | >{\centering}m{0.19in} >{\centering}m{0.16in} >{\centering\arraybackslash}m{0.3in}}
 &  &  & \multicolumn{3}{c|}{\textbf{Image-to-Text}} & \multicolumn{3}{c}{\textbf{Text-to-Image}}  \\ \hline
\textbf{$L_{int}$} & \textbf{$L_{ext}$} & \textbf{$L_{g}$} & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\ \hline
                                      &                                     & $\checkmark$                                  &        31.8      &     58.2        &  66.8            &      33.6       &         58.0     &      68.0         \\
 $\checkmark$                                     &                                     &          $\checkmark$                         &      33.3    &    57.3      &    67.8       &   33.2      &       58.5   &   67.3        \\   
 % &  $\checkmark$                                    &                                  &    \textbf{35.1}     &      57.9    &     66.4      &   33.1      &     57.5     &     66.9  \\
$\checkmark$                                     & $\checkmark$                                     &                                  &      33.2    &    58.4      &    68.0       &   31.5      &       57.9   &   67.9        \\
 $\checkmark$                                     &  $\checkmark$                                     &  $\checkmark$                                &    \textbf{36.1}          &      \textbf{59.1}      &      \textbf{69.1}       &    \textbf{36.4}        &       \textbf{60.7}       &      \textbf{70.5}       
\end{tabular}
\caption{\textbf{Loss ablation.} 
The best performance is achieved when combining all the three losses of Equation~\ref{eq:loss}. }
% (Our base model without LT \& PE )}
\label{table: ablation losses}
\end{table}

%%%%%%%%%%%%%%%% Positional encoding
%\vspace{0.05in}
\noindent
{\bf Positional encoding.} 
Recall that we utilize domain-specific knowledge
% the visual structure of X-ray images.
% Our model is aware of the image patch locations via a corresponding positional encoding vector.
by adding  positional encoding vectors.
%to image patches.
Table~\ref{table: ablation domain knowledge} shows that positional encoding improves the results in most metrics.
It is beneficial both when using a single view or two views.
Figure~\ref{fig:ablation}(A) demonstrates the contribution of positional encoding qualitatively.
When the report contains relevant location information "moderate right pleural effusion" (blue) and "small left pleural effusion" (orange), without positional encoding, our base model might fail to align the report and the image.
However, when trained with positional encoding, the alignment is successful.
% These results strengthen our assumption that position has a meaning in this domain and it is useful for representation learning.


%%%%%%%%%%%%% Table
\begin{table}[tb]
\centering
% \begin{tabular}{>{\centering}m{0.13in} >{\centering}m{0.13in} >{\centering}m{0.13in} | >{\centering}m{0.19in} >{\centering}m{0.16in} >{\centering}m{0.3in} | >{\centering}m{0.19in} >{\centering}m{0.16in} >{\centering\arraybackslash}m{0.3in}}
%  &  &  & \multicolumn{3}{c|}{\textbf{Image-to-Text}} & \multicolumn{3}{c}{\textbf{Text-to-Image}}  \\ \hline
% FT & LT & PE & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\ \hline
% $\checkmark$                                   &                                      &                                  & 35.0         & 59.4         & 67.9          & 33.9         & 59.0         & 67.9         \\
% $\checkmark$                                      &                                      & $\checkmark$                                  &         35.2     &      58.9        &      68.1         &   35.2           &   57.3           &  66.2             \\
% $\checkmark$                                     & $\checkmark$                                     &                                  & 37.7         & 60.6         & 69.4          & 36.1         & 59.6         & 69.5          \\
%  $\checkmark$                                     &  $\checkmark$                                     &  $\checkmark$                                &              &              &               &              &              &              
% \end{tabular}
\begin{tabular}{>{\centering}m{0.14in} >{\centering}m{0.14in} | >{\centering}m{0.22in} >{\centering}m{0.19in} >{\centering}m{0.33in} | >{\centering}m{0.22in} >{\centering}m{0.19in} >{\centering\arraybackslash}m{0.33in}}
 &  & \multicolumn{3}{c|}{\textbf{Image-to-Text}} & \multicolumn{3}{c}{\textbf{Text-to-Image}}  \\ \hline
LT & PE & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\ \hline
 &  & 36.1         & 59.1         & 69.1          & 36.4         & 60.7         & 70.5         \\
 & $\checkmark$                                  &         37.7     &      61.5        &      70.7         &   34.7           &   60.9           &  70.5             \\
$\checkmark$ & & 38.1         & 61.6         & \textbf{72.1}         & 37.3         & 61.2         & \textbf{71.5}          \\
$\checkmark$ &  $\checkmark$ &      \textbf{39.7}        & \textbf{63.2}             &  71.7             &  \textbf{37.7}            & \textbf{62.1}             & 71.3             
\end{tabular}
\caption{{\bf Domain knowledge.} 
Adding either lateral (LT) images or visual structure information (PE) to the baseline frontal images improves the results in both tasks.
}
\label{table: ablation domain knowledge}
\end{table}

%%%%%%%%%%%%%%%%%% Lateral images
\vspace{0.05in}
\noindent
{\bf Lateral images.} 
% Our model uses domain-specific knowledge to improve the image-text alignment. 
% This is done by two methods: addition of lateral images and addition of structural  using positional encoding. The results of those experiments are presented in Table \ref{table: ablation domain knowledge}. 
% We demonstrate the benefits of the use of the domain structure, both with frontal only images and with frontal and lateral images. 
% The results in all metrics improve with the addition of the positional encoding for both cases (frontal only and frontal lateral), demonstrating the importance of utilizing the known structure in this domain.
Lateral images are explicitly mentioned in many reports, hence using them (when available) is desirable.
Table~\ref{table: ablation domain knowledge} confirms this, by showing superior results across all metrics, compared to using only the frontal images.
% Next, we demonstrate the benefit of using the lateral images. The results in all metrics are improved in compare to using only the frontal images. 
% Furthermore, Figure~\ref{fig:ablation} demonstrates qualitatively the strength of this domain-knowledge. 
% In two cases, our base model (without lateral images and positional encoding) fails to match the report with the images. 
% Figure~\ref{fig:1a} shows a study where the report contains relevant location information -- "moderate right pleural effusion" (blue) and "small left pleural effusion" (orange). 
% Our model that was trained with positional encoding achieves a better alignment between the image and report, which leads to a successful match.
% the addition of the positional encoding results in correct match of the image to the report. 
% This is achieved by better alignments of localization mentions in the text with the correct areas in the image. 
% The blue arrow points to the area of the mentioned "moderate right pleural effusion" while the orange arrow points to the area of the mentioned "small left pleural effusion".
Figure~\ref{fig:ablation}(B) shows an example in which when our model is trained without lateral images, the alignment between the report and the image fails.
Using both views results in a correct alignment. 
The report mentions signs of overinflation that are "better appreciated on the lateral radiograph" (blue).
The signs for overinflation in the lateral image are more evident than in the frontal image.
% These results strengthen our assumption that lateral images are meaningful in this domain and are useful for representation learning, when exist.
% As described in the report of this study, "signs of overinflation are better appreciated on the lateral radiograph". 
% The blue arrow in the lateral image points to one of these signs, which is the flattened diaphragm. 

%%%%%%%%%%%%%%%%%% Limitations
\vspace{0.05in}
\noindent
{\bf Limitations.} 
Similarly to~\cite{Boecking_2022,lovt}, the limitation of our  approach is
that it does not explicitly deal with false negatives in the contrastive losses.
That is to say, there may be multiple reports that match a given image (and vice versa), but only one is considered positive.


%%%%%%%%%%%%%%%%%%%%%%%
%%%% Section: conclusions
%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
This paper presented a new model for learning a joint X-ray image \& report representation. 
The model is based on a new alignment scheme that considers both local and global information. 
In addition, we propose to enrich the model with domain-specific information. 

The benefits of our representation is demonstrated on  
three types of retrieval tasks, two of which require large precision: text-image retrieval, phrase-grounding, and class-based retrieval.
Our model is shown to outperform SoTA models, even when the additional knowledge is unavailable.
The domain-specific knowledge adds to performance.
%, leading to \ayellet{XXX-YYY} improvement over SoTA results.

In the future, we would like to study loss functions that allow an image (/text) to be paired to multiple mates from the other domain.
For instance, the contrastive loss should not push away normal images from normal text of different pairs.
This has the potential to improve results across tasks and datasets, especially those of low diversity.
% It might cause problems in datasets of low diversity, for which identical or almost-identical pairs (global or local) are treated as negative.

\newpage
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}