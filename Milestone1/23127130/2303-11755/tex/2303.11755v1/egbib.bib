@incollection{Ji2021,
author = {Ji, Zhanghexuan and Shaikh, Mohammad Abuzar and Moukheiber, Dana and Srihari, Sargur N and Peng, Yifan and Gao, Mingchen},
doi = {10.1007/978-3-030-87589-3_12},
pages = {110--119},
title = {{Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment}},
url = {https://link.springer.com/10.1007/978-3-030-87589-3_12},
year = {2021}
}

@article{Zhang2020,
abstract = {Learning visual representations of medical images (e.g., X-rays) is core to medical image understanding but its progress has been held back by the scarcity of human annotations. Existing work commonly relies on fine-tuning weights transferred from ImageNet pretraining, which is suboptimal due to drastically different image characteristics, or rule-based label extraction from the textual report data paired with medical images, which is inaccurate and hard to generalize. Meanwhile, several recent studies show exciting results from unsupervised contrastive learning from natural images, but we find these methods help little on medical images because of their high inter-class similarity. We propose ConVIRT, an alternative unsupervised strategy to learn medical visual representations by exploiting naturally occurring paired descriptive text. Our new method of pretraining medical image encoders with the paired text data via a bidirectional contrastive objective between the two modalities is domain-agnostic, and requires no additional expert input. We test ConVIRT by transferring our pretrained weights to 4 medical image classification tasks and 2 zero-shot retrieval tasks, and show that it leads to image representations that considerably outperform strong baselines in most settings. Notably, in all 4 classification tasks, our method requires only 10\% as much labeled training data as an ImageNet initialized counterpart to achieve better or comparable performance, demonstrating superior data efficiency.},
archivePrefix = {arXiv},
arxivId = {2010.00747},
author = {Zhang, Yuhao and Jiang, Hang and Miura, Yasuhide and Manning, Christopher D. and Langlotz, Curtis P.},
eprint = {2010.00747},
month = {oct},
title = {{Contrastive Learning of Medical Visual Representations from Paired Images and Text}},
url = {http://arxiv.org/abs/2010.00747},
year = {2020}
}

@article{sgraf,
  author    = {Haiwen Diao and
               Ying Zhang and
               Lin Ma and
               Huchuan Lu},
  title     = {Similarity Reasoning and Filtration for Image-Text Matching},
  journal   = {CoRR},
  volume    = {abs/2101.01368},
  year      = {2021},
  url       = {https://arxiv.org/abs/2101.01368},
  eprinttype = {arXiv},
  eprint    = {2101.01368},
  timestamp = {Thu, 21 Jan 2021 14:42:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2101-01368.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Moon,
abstract = {Recently a number of studies demonstrated impressive performance on diverse vision-language mul-timodal tasks such as image captioning and visual question answering by extending the self-attention based Transformer architecture with multimodal pre-training objectives. Despite its huge potential, vision-language multimodal pre-training in the medical domain has only recently received attention, and only demonstrated improved diagnosis accuracy of vision-language pre-trained models. In this work we explore a broad set of multimodal representation learning tasks in the medical domain, specifically using radiology images and the unstructured report. We propose a new model which adopts a Transformer based architecture combined with a novel multimodal attention masking scheme to maximize generalization performance for both vision-language understanding task (e.g., diagnosis classification) and vision-language generation task (e.g., radiology report generation). By rigorously evaluating the proposed model on four downstream tasks with three radiographic image-text datasets (MIMIC-CXR, Open-I, and VQA-RAD), we empirically demonstrate the superior downstream task performance and generality of our model against various baselines including task specific architectures. In addition, we qualitatively analyze our model by showing the results of retrieved image-report pairs, the attention map visualiza-tion, and generated reports. Our proposed multimodal pre-training model could flexibly adapt to multiple downstream tasks of vision-language understanding and generation with a novel self-attention scheme. We believe that our approach can provide the basis for a wide range of interpretations of vision-language multimodal in the medical domain. Code:github.com/SuperSupermoon/MedViLL},
archivePrefix = {arXiv},
arxivId = {2105.11333v2},
author = {Moon, Jong Hak and Lee, Hyungyung and Shin, Woncheol and Kim, Young-Hak and Choi, Edward},
eprint = {2105.11333v2},
file = {:C\:/Users/97252/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moon et al. - Unknown - Multi-modal Understanding and Generation for Medical Images and Text via Vision-Language Pre-Training.pdf:pdf},
keywords = {Index Terms-Healthcare,Medical,Multimodal Learning,Representation Learning,Self-Supervised Learning,Vision-and-Language},
title = {{Multi-modal Understanding and Generation for Medical Images and Text via Vision-Language Pre-Training}}
}

@article{Hsu,
abstract = {Joint embeddings between medical imaging modalities and associated radiology reports have the potential to offer significant benefits to the clinical community, ranging from cross-domain retrieval to conditional generation of reports to the broader goals of multimodal representation learning. In this work, we establish baseline joint embedding results measured via both local and global retrieval methods on the soon to be released MIMIC-CXR dataset consisting of both chest X-ray images and the associated radiology reports. We examine both supervised and unsupervised methods on this task and show that for document retrieval tasks with the learned representations, only a limited amount of supervision is needed to yield results comparable to those of fully-supervised methods.},
archivePrefix = {arXiv},
arxivId = {1811.08615v1},
author = {Hsu, Tzu-Ming Harry and Weng, Wei-Hung and Boag, Willie and Mcdermott, Matthew and Szolovits, Peter},
eprint = {1811.08615v1},
file = {:C\:/Users/97252/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsu et al. - Unknown - Unsupervised Multimodal Representation Learning across Medical Images and Reports(2).pdf:pdf},
title = {{Unsupervised Multimodal Representation Learning across Medical Images and Reports}}
}
@article{Hsua,
abstract = {Joint embeddings between medical imaging modalities and associated radiology reports have the potential to offer significant benefits to the clinical community, ranging from cross-domain retrieval to conditional generation of reports to the broader goals of multimodal representation learning. In this work, we establish baseline joint embedding results measured via both local and global retrieval methods on the soon to be released MIMIC-CXR dataset consisting of both chest X-ray images and the associated radiology reports. We examine both supervised and unsupervised methods on this task and show that for document retrieval tasks with the learned representations, only a limited amount of supervision is needed to yield results comparable to those of fully-supervised methods.},
archivePrefix = {arXiv},
arxivId = {1811.08615v1},
author = {Hsu, Tzu-Ming Harry and Weng, Wei-Hung and Boag, Willie and Mcdermott, Matthew and Szolovits, Peter},
eprint = {1811.08615v1},
file = {:C\:/Users/97252/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsu et al. - Unknown - Unsupervised Multimodal Representation Learning across Medical Images and Reports.pdf:pdf},
title = {{Unsupervised Multimodal Representation Learning across Medical Images and Reports}}
}
@article{Johnson2019,
abstract = {Chest radiography is an extremely powerful imaging modality, allowing for a detailed inspection of a patient's chest, but requires specialized training for proper interpretation. With the advent of high performance general purpose computer vision algorithms, the accurate automated analysis of chest radiographs is becoming increasingly of interest to researchers. Here we describe MIMIC-CXR, a large dataset of 227,835 imaging studies for 65,379 patients presenting to the Beth Israel Deaconess Medical Center Emergency Department between 2011â€“2016. Each imaging study can contain one or more images, usually a frontal view and a lateral view. A total of 377,110 images are available in the dataset. Studies are made available with a semi-structured free-text radiology report that describes the radiological findings of the images, written by a practicing radiologist contemporaneously during routine clinical care. All images and reports have been de-identified to protect patient privacy. The dataset is made freely available to facilitate and encourage a wide range of research in computer vision, natural language processing, and clinical data mining.},
author = {Johnson, Alistair E. W. and Pollard, Tom J. and Berkowitz, Seth J. and Greenbaum, Nathaniel R. and Lungren, Matthew P. and Deng, Chih-ying and Mark, Roger G. and Horng, Steven},
doi = {10.1038/s41597-019-0322-0},
issn = {2052-4463},
journal = {Scientific Data},
month = {dec},
number = {1},
pages = {317},
title = {{MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports}},
url = {http://www.nature.com/articles/s41597-019-0322-0},
volume = {6},
year = {2019}
}
@article{TAPE1986,
author = {TAPE, THOMAS G.},
doi = {10.7326/0003-4819-104-5-663},
issn = {0003-4819},
journal = {Annals of Internal Medicine},
month = {may},
number = {5},
pages = {663},
title = {{Diagnostic Decision: The Utility of Routine Chest Radiographs}},
url = {http://annals.org/article.aspx?doi=10.7326/0003-4819-104-5-663},
volume = {104},
year = {1986}
}
@book{,
address = {Washington, D.C.},
doi = {10.17226/11475},
isbn = {978-0-309-10036-6},
month = {dec},
publisher = {National Academies Press},
title = {{An Assessment of the National Institute of Standards and Technology Measurement and Standards Laboratories}},
url = {http://www.nap.edu/catalog/11475},
year = {2006}
}
@article{Smith-Bindman2008,
author = {Smith-Bindman, Rebecca and Miglioretti, Diana L. and Larson, Eric B.},
doi = {10.1377/hlthaff.27.6.1491},
issn = {0278-2715},
journal = {Health Affairs},
month = {nov},
number = {6},
pages = {1491--1502},
title = {{Rising Use Of Diagnostic Medical Imaging In A Large Integrated Health System}},
url = {http://www.healthaffairs.org/doi/10.1377/hlthaff.27.6.1491},
volume = {27},
year = {2008}
}
@inproceedings{Alsentzer2019,
address = {Stroudsburg, PA, USA},
author = {Alsentzer, Emily and Murphy, John and Boag, William and Weng, Wei-Hung and Jindi, Di and Naumann, Tristan and McDermott, Matthew},
booktitle = {Proceedings of the 2nd Clinical Natural Language Processing Workshop},
doi = {10.18653/v1/W19-1909},
pages = {72--78},
publisher = {Association for Computational Linguistics},
title = {{Publicly Available Clinical}},
url = {http://aclweb.org/anthology/W19-1909},
year = {2019}
}
@article{DBLP:journals/corr/abs-1904-03323,
  author    = {Emily Alsentzer and
               John R. Murphy and
               Willie Boag and
               Wei{-}Hung Weng and
               Di Jin and
               Tristan Naumann and
               Matthew B. A. McDermott},
  title     = {Publicly Available Clinical {BERT} Embeddings},
  journal   = {CoRR},
  volume    = {abs/1904.03323},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.03323},
  eprinttype = {arXiv},
  eprint    = {1904.03323},
  timestamp = {Wed, 24 Apr 2019 12:21:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-03323.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{lovt,
  author    = {Philip M{\"{u}}ller and
               Georgios Kaissis and
               Congyu Zou and
               Daniel Rueckert},
  title     = {Joint Learning of Localized Representations from Medical Images and
               Reports},
  journal   = {CoRR},
  volume    = {abs/2112.02889},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.02889},
  eprinttype = {arXiv},
  eprint    = {2112.02889},
  timestamp = {Sun, 02 Oct 2022 15:32:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-02889.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{wang2022multi,
  title={Multi-Granularity Cross-modal Alignment for Generalized Medical Visual Representation Learning},
  author={Wang, Fuying and Zhou, Yuyin and Wang, Shujun and Vardhanabhuti, Varut and Yu, Lequan},
  journal={arXiv preprint arXiv:2210.06044},
  year={2022}
}
@article{DBLP:journals/corr/abs-1807-03748,
  author    = {A{\"{a}}ron van den Oord and
               Yazhe Li and
               Oriol Vinyals},
  title     = {Representation Learning with Contrastive Predictive Coding},
  journal   = {CoRR},
  volume    = {abs/1807.03748},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.03748},
  eprinttype = {arXiv},
  eprint    = {1807.03748},
  timestamp = {Mon, 13 Aug 2018 16:48:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1807-03748.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@incollection{Boecking_2022,
	doi = {10.1007/978-3-031-20059-5_1},  
	year = 2022,
	publisher = {Springer Nature Switzerland},
  
	pages = {1--21},
  
	author = {Benedikt Boecking and Naoto Usuyama and Shruthi Bannur and Daniel C. Castro and Anton Schwaighofer and Stephanie Hyland and Maria Wetscherek and Tristan Naumann and Aditya Nori and Javier Alvarez-Valle and Hoifung Poon and Ozan Oktay},
  
	title = {Making the~Most of~Text Semantics to~Improve Biomedical Vision{\textendash}Language Processing},
  
	booktitle = {Lecture Notes in Computer Science}
 }

 @article{DBLP:journals/corr/abs-2004-06165,
  author    = {Xiujun Li and
               Xi Yin and
               Chunyuan Li and
               Pengchuan Zhang and
               Xiaowei Hu and
               Lei Zhang and
               Lijuan Wang and
               Houdong Hu and
               Li Dong and
               Furu Wei and
               Yejin Choi and
               Jianfeng Gao},
  title     = {Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks},
  journal   = {CoRR},
  volume    = {abs/2004.06165},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.06165},
  eprinttype = {arXiv},
  eprint    = {2004.06165},
  timestamp = {Tue, 15 Feb 2022 08:52:23 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-06165.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-1712-02036,
  author    = {Yan Huang and
               Qi Wu and
               Liang Wang},
  title     = {Learning Semantic Concepts and Order for Image and Sentence Matching},
  journal   = {CoRR},
  volume    = {abs/1712.02036},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.02036},
  eprinttype = {arXiv},
  eprint    = {1712.02036},
  timestamp = {Tue, 19 Mar 2019 13:03:53 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1712-02036.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-1909-02701,
  author    = {Kunpeng Li and
               Yulun Zhang and
               Kai Li and
               Yuanyuan Li and
               Yun Fu},
  title     = {Visual Semantic Reasoning for Image-Text Matching},
  journal   = {CoRR},
  volume    = {abs/1909.02701},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.02701},
  eprinttype = {arXiv},
  eprint    = {1909.02701},
  timestamp = {Mon, 16 Sep 2019 17:27:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-02701.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/KarpathyF14,
  author    = {Andrej Karpathy and
               Li Fei{-}Fei},
  title     = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
  journal   = {CoRR},
  volume    = {abs/1412.2306},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.2306},
  eprinttype = {arXiv},
  eprint    = {1412.2306},
  timestamp = {Wed, 15 Sep 2021 14:13:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KarpathyF14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-1803-08024,
  author    = {Kuang{-}Huei Lee and
               Xi Chen and
               Gang Hua and
               Houdong Hu and
               Xiaodong He},
  title     = {Stacked Cross Attention for Image-Text Matching},
  journal   = {CoRR},
  volume    = {abs/1803.08024},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.08024},
  eprinttype = {arXiv},
  eprint    = {1803.08024},
  timestamp = {Thu, 10 Jan 2019 15:03:32 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-08024.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{johnson2016mimic,
  title={MIMIC-III, a freely accessible critical care database},
  author={Johnson, Alistair EW and Pollard, Tom J and Shen, Lu and Lehman, Li-wei H and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Anthony Celi, Leo and Mark, Roger G},
  journal={Scientific data},
  volume={3},
  number={1},
  pages={1--9},
  year={2016},
  publisher={Nature Publishing Group}
}
@article{DBLP:journals/corr/HuangLW16a,
  author    = {Gao Huang and
               Zhuang Liu and
               Kilian Q. Weinberger},
  title     = {Densely Connected Convolutional Networks},
  journal   = {CoRR},
  volume    = {abs/1608.06993},
  year      = {2016},
  url       = {http://arxiv.org/abs/1608.06993},
  eprinttype = {arXiv},
  eprint    = {1608.06993},
  timestamp = {Mon, 10 Sep 2018 15:49:32 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HuangLW16a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2003-00827,
  author    = {Laleh Seyyed{-}Kalantari and
               Guanxiong Liu and
               Matthew B. A. McDermott and
               Marzyeh Ghassemi},
  title     = {CheXclusion: Fairness gaps in deep chest X-ray classifiers},
  journal   = {CoRR},
  volume    = {abs/2003.00827},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.00827},
  eprinttype = {arXiv},
  eprint    = {2003.00827},
  timestamp = {Tue, 10 Mar 2020 13:33:48 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-00827.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{chexpert,
  author    = {Jeremy Irvin and
               Pranav Rajpurkar and
               Michael Ko and
               Yifan Yu and
               Silviana Ciurea{-}Ilcus and
               Chris Chute and
               Henrik Marklund and
               Behzad Haghgoo and
               Robyn L. Ball and
               Katie S. Shpanskaya and
               Jayne Seekins and
               David A. Mong and
               Safwan S. Halabi and
               Jesse K. Sandberg and
               Ricky Jones and
               David B. Larson and
               Curtis P. Langlotz and
               Bhavik N. Patel and
               Matthew P. Lungren and
               Andrew Y. Ng},
  title     = {CheXpert: {A} Large Chest Radiograph Dataset with Uncertainty Labels
               and Expert Comparison},
  journal   = {CoRR},
  volume    = {abs/1901.07031},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.07031},
  eprinttype = {arXiv},
  eprint    = {1901.07031},
  timestamp = {Thu, 20 Oct 2022 16:08:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-07031.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{10.1371/journal.pmed.1002686,
author = {Rajpurkar, Pranav and Irvin, Jeremy and Ball, Robyn L and Zhu, Kaylie and Yang, Brandon and Mehta, Hershel and Duan, Tony and Ding, Daisy and Bagul, Aarti and Langlotz, Curtis P and Patel, Bhavik N and Yeom, Kristen W and Shpanskaya, Katie and Blankenberg, Francis G and Seekins, Jayne and Amrhein, Timothy J and Mong, David A and Halabi, Safwan S and Zucker, Evan J and Ng, Andrew Y and Lungren, Matthew P},
doi = {10.1371/journal.pmed.1002686},
journal = {PLOS Medicine},
number = {11},
pages = {1--17},
publisher = {Public Library of Science},
title = {{Deep learning for chest radiograph diagnosis: A retrospective comparison of the CheXNeXt algorithm to practicing radiologists}},
url = {https://doi.org/10.1371/journal.pmed.1002686},
volume = {15},
year = {2018}
}
@article{RSNA,
author = {Shih, George and wu, Carol and Halabi, Safwan and Kohli, Marc and Prevedello, Luciano and Cook, Tessa and Sharma, Arjun and Amorosa, Judith and Arteaga, Veronica and Galperin-Aizenberg, Maya and Gill, Ritu and Godoy, Myrna and Hobbs, Stephen and Jeudy, Jean and Laroia, Archana and Shah, Palmi and Vummidi, Dharshan and Yaddanapudi, Kavitha and Stein, Anouk},
year = {2019},
month = {01},
pages = {e180041},
title = {Augmenting the National Institutes of Health Chest Radiograph Dataset with Expert Annotations of Possible Pneumonia},
volume = {1},
journal = {Radiology: Artificial Intelligence},
doi = {10.1148/ryai.2019180041}
}
@article{wang2020covid,
  title={Covid-net: A tailored deep convolutional neural network design for detection of covid-19 cases from chest x-ray images},
  author={Wang, Linda and Lin, Zhong Qiu and Wong, Alexander},
  journal={Scientific reports},
  volume={10},
  number={1},
  pages={1--12},
  year={2020},
  publisher={Springer}
}
@inproceedings{objectCXR,
author = {Xue, Zhiyun and Candemir, Sema and Antani, Sameer and Long, L. and Jaeger, Stefan and Demner-Fushman, Dina and Thoma, George},
year = {2015},
month = {11},
pages = {956-961},
title = {Foreign object detection in chest X-rays},
doi = {10.1109/BIBM.2015.7359812}
}
@article{SIIM,
title = {Society for Imaging Informatics in Medicine: Siim-acr pneumothorax segmentation},
doi = {https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation}
}
@article{covidrural,
  title={Data from chest imaging with clinical and genomic correlates representing a rural COVID-19 positive population [data set]},
  author={Desai, S and Baghal, A and Wongsurawat, T and Al-Shukri, S and Gates, K and Farmer, P and Rutherford, M and Blake, GD and Nolan, T and Powell, T and others},
  journal={The Cancer Imaging Archive},
  pages={7--23},
  year={2020}
}
@inproceedings{nih8,
  title={Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases},
  author={Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald M},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2097--2106},
  year={2017}
}
@article{radnli,
  title={Improving factual completeness and consistency of image-to-text radiology report generation},
  author={Miura, Yasuhide and Zhang, Yuhao and Tsai, Emily Bao and Langlotz, Curtis P and Jurafsky, Dan},
  journal={arXiv preprint arXiv:2010.10042},
  year={2020}
}
@article{resnet,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1512.03385},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.03385},
  eprinttype = {arXiv},
  eprint    = {1512.03385},
  timestamp = {Wed, 25 Jan 2023 11:01:16 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/HeZRS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{vit,
  author    = {Alexey Dosovitskiy and
               Lucas Beyer and
               Alexander Kolesnikov and
               Dirk Weissenborn and
               Xiaohua Zhai and
               Thomas Unterthiner and
               Mostafa Dehghani and
               Matthias Minderer and
               Georg Heigold and
               Sylvain Gelly and
               Jakob Uszkoreit and
               Neil Houlsby},
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition
               at Scale},
  journal   = {CoRR},
  volume    = {abs/2010.11929},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.11929},
  eprinttype = {arXiv},
  eprint    = {2010.11929},
  timestamp = {Fri, 20 Nov 2020 14:04:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-11929.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{du2022survey,
  title={A survey of vision-language pre-trained models},
  author={Du, Yifan and Liu, Zikang and Li, Junyi and Zhao, Wayne Xin},
  journal={arXiv preprint arXiv:2202.10936},
  year={2022}
}

@article{wang2022medclip,
  title={Medclip: Contrastive learning from unpaired medical images and text},
  author={Wang, Zifeng and Wu, Zhenbang and Agarwal, Dinesh and Sun, Jimeng},
  journal={arXiv preprint arXiv:2210.10163},
  year={2022}
}

@article{hirsch2022clid,
  title={CLID: Controlled-Length Image Descriptions with Limited Data},
  author={Hirsch, Elad and Tal, Ayellet},
  journal={arXiv preprint arXiv:2211.14835},
  year={2022}
}

@article{liu2021auto,
  title={Auto-encoding knowledge graph for unsupervised medical report generation},
  author={Liu, Fenglin and You, Chenyu and Wu, Xian and Ge, Shen and Sun, Xu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16266--16279},
  year={2021}
}

@incollection{Ji2021,
author = {Ji, Zhanghexuan and Shaikh, Mohammad Abuzar and Moukheiber, Dana and Srihari, Sargur N and Peng, Yifan and Gao, Mingchen},
doi = {10.1007/978-3-030-87589-3_12},
pages = {110--119},
title = {{Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment}},
url = {https://link.springer.com/10.1007/978-3-030-87589-3_12},
year = {2021}
}

@article{Hsu,
abstract = {Joint embeddings between medical imaging modalities and associated radiology reports have the potential to offer significant benefits to the clinical community, ranging from cross-domain retrieval to conditional generation of reports to the broader goals of multimodal representation learning. In this work, we establish baseline joint embedding results measured via both local and global retrieval methods on the soon to be released MIMIC-CXR dataset consisting of both chest X-ray images and the associated radiology reports. We examine both supervised and unsupervised methods on this task and show that for document retrieval tasks with the learned representations, only a limited amount of supervision is needed to yield results comparable to those of fully-supervised methods.},
archivePrefix = {arXiv},
arxivId = {1811.08615v1},
author = {Hsu, Tzu-Ming Harry and Weng, Wei-Hung and Boag, Willie and Mcdermott, Matthew and Szolovits, Peter},
eprint = {1811.08615v1},
file = {:C\:/Users/97252/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsu et al. - Unknown - Unsupervised Multimodal Representation Learning across Medical Images and Reports(2).pdf:pdf},
title = {{Unsupervised Multimodal Representation Learning across Medical Images and Reports}}
}
@article{Hsua,
abstract = {Joint embeddings between medical imaging modalities and associated radiology reports have the potential to offer significant benefits to the clinical community, ranging from cross-domain retrieval to conditional generation of reports to the broader goals of multimodal representation learning. In this work, we establish baseline joint embedding results measured via both local and global retrieval methods on the soon to be released MIMIC-CXR dataset consisting of both chest X-ray images and the associated radiology reports. We examine both supervised and unsupervised methods on this task and show that for document retrieval tasks with the learned representations, only a limited amount of supervision is needed to yield results comparable to those of fully-supervised methods.},
archivePrefix = {arXiv},
arxivId = {1811.08615v1},
author = {Hsu, Tzu-Ming Harry and Weng, Wei-Hung and Boag, Willie and Mcdermott, Matthew and Szolovits, Peter},
eprint = {1811.08615v1},
file = {:C\:/Users/97252/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsu et al. - Unknown - Unsupervised Multimodal Representation Learning across Medical Images and Reports.pdf:pdf},
title = {{Unsupervised Multimodal Representation Learning across Medical Images and Reports}}
}

@article{Huang,
abstract = {In recent years, the growing utilization of medical imaging is placing an increasing burden on radiologists. Deep learning provides a promising solution for automatic medical image analysis and clinical decision support. However, large-scale manually labeled datasets required for training deep neural networks are difficult and expensive to obtain for medical images. The purpose of this work is to develop label-efficient multimodal medical imaging representations by leveraging radiology reports. We propose an attention-based framework for learning global and local representations by contrasting image sub-regions and words in the paired report. In addition, we propose methods to leverage the learned representations for various downstream medical image recognition tasks with limited labels. Our results demonstrate high-performance and label-efficiency for image-text retrieval, classification (finetuning and zeros-shot settings), and segmentation on different datasets.},
author = {Huang, Shih-Cheng and Shen, Liyue and Lungren, Matthew P and Yeung, Serena},
file = {:C\:/Users/97252/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - Unknown - GLoRIA A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognitio.pdf:pdf},
title = {{GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognition}},
url = {https://github.com/marshuang80/gloria}
}