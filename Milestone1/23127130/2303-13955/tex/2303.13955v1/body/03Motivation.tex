% 这一部分介绍我们对之前的工作观察后，观察到的问题。并且给出对应现象的解决方案。以及对于使用NMSE loss作为正则项的动机。
\label{section:3}
\input{fig/Classical_Adv.tex}
This section further analyzes the oscillations in the early stage and the overfitting issue in the later stage of the AT process. Based on our observations, we propose to fully utilize the historical training information by parameter interpolation at the end of each epoch. We also discuss aligning clean and adversarial examples in AT and suggest focusing more on the relative magnitude of the output logits rather than the absolute magnitude.

% 我们首先对标准对抗训练以及其他经典对抗训练算法在数据集上的训练准确率进行分析
%\subsection{Analysis of Robust Accuracy Curve}
\subsection{Analysis on the Robust Accuracy}

% 对经典对抗训练的训练过程和准确率进行观察，总结其中存在的现象和对应的问题。
Figure~\ref{fig:Classsical Adversarial Training} illustrates the test accuracy on clean and adversarial examples of several popular AT methods during training, including PGD-AT~\cite{PGD}, ALP~\cite{DBLP:journals/corr/abs-1803-06373}, FAT~\cite{DBLP:conf/icml/ZhangXH0CSK20}, TRADES~\cite{Trade-off}, MART~\cite{MART} and GAIRAT~\cite{GAIRAT}. 
As illustrated in Figure~\ref{fig:Classsical Adversarial Training} (b), over all these AT methods except our method (\name + NMSE), we can observe that in the early stage, there are apparent oscillations in terms of robust accuracy, and in the later stage, the adversarial robustness all declines as the model overfits adversarial examples.

In the early stage, the model has not learned to fit the data yet and model parameters are updated with a large learning rate, leading to rapid changes on the decision boundary. Subsequently, the adversarial examples generated by the PGD attack for adversarial training vary significantly at each epoch. Thus, it is hard for the models to learn the features and have good generalization from the changing data, making the robust accuracy unstable. 
%If the decision boundary changes more moderately, the difference in adversarial examples generated between two adjacent epochs will be smaller. 
If the change on decision boundary is more moderate, then the difference of  adversarial examples generated between two adjacent epochs will be smaller.  
A straightforward solution is to reduce the learning rate. However, a low learning rate will %slow down 
decelerate the convergence of AT. Worse still, the oscillation may disappear with a low learning rate, but the overfitting issue occurs. 

% We assume that directly reduce the learning rate will make the decision boundary slightly change. Therefore, the adversarial examples generated by PGD attack is roughly same in each epoch, which makes the decision boundary become too complex to keep robust for different adversarial examples. 

% In the later stage, with a low learning rate, the model struggles to fit the adversarial examples of the training set as well as possible. Therefore, the decision boundary becomes too complex to be robust for unseen adversarial examples, learning to the overfitting issue.
In the later stage, as the learning rate of most AT methods is reduced significantly, %the model faces difficulty in well fitting the adversarial examples %from the training set optimally.
the decision boundary will become overly complex when 
the model struggles to well fit the adversarial examples crafted during training. 
As a result, the learned model is not robust enough to well handle unseen adversarial examples, leading to the overfitting issue. 

We observe that existing AT methods ignore the historical information in the training process, which would be useful to stabilize the training and alleviate overfitting.
%In order to address oscillation and overfitting issues, we place emphasis on the model parameters from previous epochs and utilize them to 
To this end, we propose to utilize the model parameters of previous epoch to fine-tune the current parameters at the end of each epoch. Such an approach allows us to leverage historical information and improve the %overall efficacy of the training process.
the robust generalization capability. 
% 考虑过去的参数，在早期阶段，能够....，从而解决震荡问题
% 在末期，能够....，从而解决过拟合问题
%Specifically, 
In the early stage, considering parameters of the previous epoch will result in a more gradual update on the model parameters, helping to ensure a smoother and more stable training. In the later stage, mixing parameters of the previous epoch could smooth the decision boundary and prevent the model from overfitting. % in the training process.

% In our work, we attach importance to the model parameters of previous epochs and utilize the parameters of previous epochs to tune the current parameters at the end of each epoch to solve the oscillation and overfitting issues.
% We propose a framework of epoch level based on momentum Equation~\ref{Mometum v} and Equation~\ref{Mometum theta}.
% \begin{equation}
%     \label{Mometum v}
%     \boldsymbol{v_t}=\gamma \cdot \boldsymbol{v_{t-1}}+\eta \cdot \boldsymbol{g_t}
% \end{equation}
% \begin{equation}
%     \label{Mometum theta}
%     \boldsymbol{\theta_{t+1}}=\boldsymbol{\theta_t}-\boldsymbol{v_t}
% \end{equation}

% According to the method of model parameters update, we simplify and merge the above two equations to obtain the update Equation~\ref{eq:MOMAT} at the epoch level.
% \begin{equation}
%     \boldsymbol{\theta_t'}=\lambda\cdot\boldsymbol{\theta_{t-1}'}+(1-\lambda)\cdot\boldsymbol{\theta_t}
%     \label{eq:MOMAT}
% \end{equation}
% where $\boldsymbol{\theta_{t-1}'}$ is the final parameters of last epoch, $\boldsymbol{\theta_t}$ is the final parameters of this epoch without modifying by previous epochs. After each epoch of training, we do not directly enter the next epoch.
% Specifically, if the parameters take place too much change in current epoch, it will be modified by previous epochs. If the parameters slightly change, the previous epochs will prevent the adversarial examples fine-tuning the parameters too excessive to defend other attacks.
% We used Equation \ref{eq:MOMAT} to combine the current parameters and the previously accumulated training information as the final parameters of this epoch. 



% 我们认为之前的正则化项在一定程度上过分严格，使用KL散度的正则项希望模型对于干净样本的logit和对抗样本的logit完全一致。即使对抗样本的logit和干净样本的logit的相对大小都是一致的，也可能会有很大的惩罚，同样存在过拟合的风险。
% \subsection{Regularization of NMSE Loss}
%\subsection{Analysis of Regularization in AT}
\subsection{Analysis on the AT Regularization}

Combining regularization with the loss of standard AT is one of the most effective ways to alleviate the overfitting issue and improve the model robustness.
For instance, the Kullback-Leibler (KL) divergence between the classification probabilities of clean and adversarial examples is often used for regularization~\cite{Trade-off}. Minimizing the KL divergence aligns the prediction of clean and adversarial examples and encourages the output to be smooth.

However, adversarial examples contain malicious noise that 
%is substantially different from 
substantially deviates from the data distribution of clean examples. We argue that it is too demanding to force the absolute magnitude of output predictions to remain the same on the two types of examples.
% Even if the relative magnitude of the classification probability is correct, the regularization loss will still be large when the absolute magnitude is different. 
% We argue such alignment requirement is too strong, which leads to overfitting issues in adversarial training.
% Moreover, the model can not fully learn the information from the adversarial examples by focusing more on the output logit of the correct class and ignoring the output logit of other classes. 
% Even if the absolute magnitude the output logit of the correct class is totally same, the regularization loss is not close to zero so that model can learn the features through the output logit of other classes. 
% Even if the absolute magnitude the output logit of the correct class is totally same, the regularization loss is not close to zero so that model can learn the features through the output logit of other classes. 
Moreover, the KL divergence compares the classification probabilities obtained through the softmax operation on logits. This operation tends to amplify the probability of the correct class while reducing that of other classes. AT can not fully learn the information from adversarial examples by focusing too much on the output probability of the correct class but neglecting probabilities of other classes. 

Therefore, it is more reasonable that the output logits for clean examples and adversarial examples are roughly similar but not identical. Rather than considering the absolute magnitude of the output logits, we should pay more attention to the relative magnitude between clean and adversarial examples.


% It is not appropriate to assign the same weight to all adversarial examples. 
% the more vulnerable examples (low confidence) should be given a higher weight and the less vulnerable examples (high confidence) should be given a lower weight.
% So we normalize the classification output of the model using the p-norm instead of the softmax function and then calculate the weight of different adversarial examples. Finally, we get the regularization loss with normalized results and different weights by MSE.