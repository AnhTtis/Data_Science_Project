% 介绍标准对抗训练任务
\subsection{Adversarial Training} 
Adversarial training (AT)~\cite{PGD} has been demonstrated to be the most effective defensive methods against adversarial attacks, which generates a locally most adversarial perturbed point for each clean example and trains the model to classify them correctly. 

Given an image classification task, the training dataset $D=\{(\mathbf{x_i},y_i)\}_{i=1}^n$ consists of $n$ clean examples with $c$ classes, where $\mathbf{x_i} \in \mathbb{R}^d$ represents a clean example
with the ground-truth label $y_i\in \{1,2,...,c\}$. 
The adversarial training optimization problem can be formulated as the following min-max problem: 
\begin{equation}
\min_{\boldsymbol{\theta}}\sum_{i}\max_{\mathbf{x_i'}\in \mathbf{S(x_i,\epsilon)}}
\mathcal{L}(f_{\boldsymbol{\theta}}(\mathbf{x_i'},y_i)),
\end{equation}
where $f_\theta(\cdot): \mathbb{R}^d \rightarrow \mathbb{R}^c$ is the DNN classifier with parameter $\theta$. $\mathcal{L}(\cdot, \cdot)$ represents the cross entropy loss and $\mathbf{S(x_i,\epsilon)}=\{||\mathbf{x'_i}-\mathbf{x_i}||_p \leq \epsilon\}$ represents an $\epsilon$-ball of a benign data point $\mathbf{x_i}$. $\mathbf{x_i'}$ denotes the adversarial example generated from $\mathbf{x_i}$.

To solve the inner maximization problem, the adversarial example \(\mathbf{x_i'}\) is often crafted  by the Projected Gradient Decent (PGD) attack~\cite{PGD}, which can be formulated by:
\begin{equation}
    \mathbf{x}_i^{t+1}=\prod_{\mathcal{S}(\mathbf{x}_i)}(\mathbf{x}_i^{t}+\alpha \cdot sign( \nabla_{\mathbf{x}_i^t}\mathcal{L}(f_\theta(\mathbf{x}_i^{t}),y_i)).  
\end{equation}
Here $\mathbf{x}_i^t$ denotes the adversarial example at the $t^{th}$ step, $\prod(\cdot)$ is the projection operator and $\alpha$ is the step size.

% 利用单步攻击生成的对抗样本，模型训练一段时间后会出现鲁棒性消失的现象，而迭代攻击生成的对抗样本则不会。但是利用迭代攻击训练得到的模型在干净样本和对抗样本上的准确率都有待进一步的提升。

TRADES~\cite{Trade-off} is another typical AT proposed to achieve a better balance between accuracy on clean examples and robustness on adversarial examples:
\begin{equation}
\begin{split}
    \min_{\boldsymbol{\theta}}\sum_{i}\{ & CE(f_{\boldsymbol{\theta}}(\mathbf{x}_i),y_i)\\&
    +\beta \cdot \max_{\mathbf{x_i'}\in \mathbf{S(x_i,\epsilon)}}KL(f_{\boldsymbol{\theta}}\mathbf({\mathbf{x}_i})||f_{\boldsymbol{\theta}}(\mathbf{x}_i')) \}, 
\end{split}
\end{equation}
where \(CE(\cdot, \cdot)\) is the cross entropy loss that %makes the accuracy of clean examples maximum. 
maximize the accuracy of clean examples.  
\(KL(\cdot || \cdot)\) is the Kullback-Leibler divergence that measures the distance of output between clean and adversarial examples and $\beta$ controls the tradeoff between accuracy and robustness.

%Besides TRADES, 
Since the inception of TRADES, subsequent efforts have been devoted to achieve better performance. 
MART~\cite{MART} explicitly differentiates the misclassified and correctly classified examples during the training. 
STAT~\cite{SqueezeAT} takes both advrsarial examples and 
collaborative examples into account for regularizing the loss landscape.
FreeAT~\cite{FreeAT} uses each epoch of PGD adversarial examples and updates the model with the gradient.
Among the adversarial data that are confidently misclassified, rather than employing the most adversarial data that maximize the loss, FAT~\cite{DBLP:conf/icml/ZhangXH0CSK20}  searches for the  least adversarial data that minimize the loss. 
LAS-AT~\cite{DBLP:conf/cvpr/JiaZW0WC22} learns to automatically produce attack strategies to improve the model robustness.
SCORE~\cite{DBLP:conf/icml/PangLYZY22} facilitates the reconciliation between robustness and accuracy, while still handling the worst-case uncertainty via robust optimization.

\subsection{Decision Boundary and Overfitting}
Dong \etal~\cite{EMAT} have found that the decision boundaries for adversarial training are more complex than those for standard training, thus making the training more difficult.
LBGAT~\cite{Learnable} 
constrains logits from the robust model that takes adversarial examples as input and makes it similar to those from the clean model fed with the corresponding natural data to better fine-tune the decision boundaries. 

Moreover, overfitting issue widely exists during the course of adversarial training~\cite{Overfitting1}.
GAIRAT~\cite{GAIRAT} adjusts the weights based on the difficulty of attacking a benign data point.
AWP~\cite{AWP} explicitly regularizes the flatness of the weight loss landscape, forming a double-perturbation mechanism in the adversarial training framework that adversarially perturbs both inputs and weights.
RLFAT~\cite{RobustLocalFeature} learns robust local features by adversarial training on the random block shuffle transformed adversarial examples, and then transfers the robust local features into the training of normal adversarial examples.
MLCAT~\cite{MLCAT} learns large-loss data as usual, and adopts additional measures to increase the loss of small-loss data to hinder data fitting when the data become easy to learn.  

Nevertheless, the classification accuracy of adversarial training on clean and adversarial examples still has room for improvement.
After each epoch of parameter updates on the model, different adversarial examples are generated. 
Compared with clean examples, adversarial examples are much more diverse, making it hard for the training to learn from the data when the network capacity is rather limited.
In this work, we observe that previous works did not fully utilize the historical information during training, thus we proposed to utilize historical information to boost the model's robustness. 