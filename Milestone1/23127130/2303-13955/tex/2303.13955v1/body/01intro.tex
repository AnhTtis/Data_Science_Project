%深度神经网络被广泛应用于各个方面，但是模型对于对抗样本的分类结果很容易是错误的。
Deep neural networks (DNNs) have been widely used in various tasks of computer vision~\cite{Resnet18} and natural language processing~\cite{DBLP:conf/naacl/DevlinCLT19}. 
% They are also applied in many safety systems~\ref{} so that it is important to make them more reliable and safe~\ref{}.
However, even the model performance surpasses humans in some tasks, they are known to be vulnerable to adversarial examples by injecting malicious and imperceptible perturbations to clean inputs that can cause the model to misclassify inputs with the high confidence~\cite{DBLP:journals/corr/SzegedyZSBEGF13,DBLP:journals/corr/GoodfellowSS14,PGD,AA,DBLP:conf/icml/AthalyeC018,DBLP:conf/icml/HendrycksLM19,DBLP:conf/iccv/XieWZZXY17,DBLP:conf/ccs/MengC17}. 
% The existence of adversarial examples attracts great attention and inspires extensive works to defend the adversarail attack.
Since DNNs are applied in many safety systems, it is of great importance to make them more reliable and robust.

%MOMAT算法框架图解
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{fig/MOMAT_training_process.jpg}
    \caption{Illustration of the parameter update %for parameter interpolation based adversarial training. 
     for the proposed method. 
    The start and end points of each epoch are in colored dots. }
    \label{fig:MOMAT training process}
\end{figure}
%有许多对抗防御工作提出，其中效果最好的方法是对抗训练。

%目前的对抗训练方法中，训练过程中都存在明显的前期震荡和后期过拟合现象。\
% 我们认为，这是因为，在前期，学习率较大，模型参数更新较快，每个epoch生成的对抗样本差异较大，导致模型效果出现震荡。后期，出现的原因是它们忽略了模型训练的历史信息，而只关注目前的参数。我们认为，we use epoch-level momentum to fully utilize the historical ..., which can alleviate above issues.
To improve the model's robustness against adversarial attacks, Adversarial Training (AT) is known to be the most effective approach to defend against adversarial attacks, which generates adversarial examples during the training and incorporates them into the training.
However, the training process of AT has apparent oscillations in the early stage and overfitting issues in the later stage.
In the early stage, the model parameters are updated rapidly with a large learning rate, thus the adversarial examples generated at each epoch are pretty different, leading to the oscillation on robust accuracy.
In the later stage, many works~\cite{MLCAT,Overfitting1,EMAT} and our experiments 
have shown that the overfitting issue occurs in the later stage of AT, that is, the training accuracy continues to increase but the robust accuracy on the testing data begins to decline.
%The reason is that existing AT methods %ignore the historical information of the training and 
%only focus on the update of the current parameters but ignore the historical information.
%To this end, we propose to use the epoch-level momentum to fully utilize the historical information of the adversarial training process, which will alleviate the above issues.



% 为了解决前期震荡和后期过拟合问题，我们提出MOMAT，which 如图1所示，把前一个epoch的参数用momentum的方式作为下一个epoch的起点。为什么能解决...为了解决干净样本和对抗样本对齐的问题，我们提出NMSE，which...。 
% 对抗训练前期，对抗样本生成的差异较大，模型边界的变化也比较快。使用epoch-level的动量可以使模型边界的变化更加温和，进而消除前期震荡现象。对抗训练后期，epoch-level的动量考虑了之前的边界信息，防止模型边界变得过分复杂，缓解了过拟合问题。
To solve the oscillation issue in the early stage of AT and the overfitting issue in the later stage, we propose a Parameter Interpolation based Adversarial Training (\name) framework. As illustrated in Figure~\ref{fig:MOMAT training process}, 
at the end of each epoch of the training, 
% we use the model parameters of the previous epoch to interpolate the current parameters. %Specifically, we add the model parameters of previous epoch to the current parameters by the momentum method.
we tune the model parameters as the interpolation of the model parameters of the previous and current epochs.
In the early stage of AT, the adversarial examples generated at each epoch is significantly different, and the model's decision boundary changes dramatically. 
In contrast, \name tunes the model parameters with the previous epoch and makes the change of the decision boundary more moderate, %eliminating the oscillation issue.
helping to eliminate the oscillation. 
In the later stage, \name considers the previous boundary, preventing the decision boundary from becoming too complex and alleviating the overfitting issue. 
% As the training continues, \name increasingly takes into account the model parameters with the previous epoch because of the improvement of robust accuracy. 
As the training continues and the model parameters become more %useful,
valuable, \name gradually increases the weight of the previous parameters when tuning the current parameters.

% 我们给出NMSE作为正则项的原因是，之前的正则项对于干净样本和对抗样本的对齐要求过高了，干净样本和对抗样本实际上是两个不同的分布，因此我们认为NMSE的条件更加宽松，可以有效缓解这个问题。
On the other hand, we observe that some works, such as TRADES~\cite{Trade-off} and MART~\cite{MART}, assume that the data distribution between clean and adversarial examples are virtually indistinguishable. To improve the performance of AT, they propose regularization terms to force the model output for clean and adversarial examples to be as similar as possible. 
However, the data distribution between clean and adversarial examples is quite different, and simply forcing the output to be close is too demanding.
We suggest that AT should pay more attention to aligning the relative magnitude rather than the absolute magnitude of logits between the clean and adversarial examples. 
Hence, we propose a new metric called Normalized Mean Square Error (NMSE) to better align the clean and adversarial examples. NMSE pays more attention to the relative magnitude of the output of clean examples and adversarial examples rather than the absolute magnitude, thus learning the common information of different distributed data.


%实验效果....
%We combine the \name framework using Normalized Mean Square Error (NMSE) as the regularization. 
We incorporate the Normalized Mean Square Error (NMSE) as the regularization into the proposed \name framework. 
Extensive experiments on CIFAR10, CIFAR100, and SVHN datasets show that our method performs better and effectively improves the adversarial robustness of the model against white-box and black-box attacks. In addition, our \name framework is general and %can be combined with other 
other adversarial training methods can be incorporated into our framework 
to achieve better robustness and performance.

% 本文的主要贡献：
% 1.提出了MOMAT算法 2.MOMAT算法与其他对抗训练算法结合也有好的效果 3.提出使用NMSE loss作为正则项
Our main contributions are summarized as follows:
\begin{itemize}
\item To solve the oscillation issue in the early stage of AT and the overfitting issue in the later stage, we propose the \name framework that interpolates the model parameters of the previous and current epochs to consider the historical information during the training.

\item We propose to use the NMSE loss as a new regularization term to better align the logits of clean and adversarial examples. NMSE pays more attention to the relative magnitude of the output of clean examples and adversarial examples rather than the absolute magnitude.

\item  The robust accuracy of our method is stable, indicating that \name alleviates the oscillation  and overfitting issues during the AT process. Extensive experiments on three standard datasets and two networks show that \name combined with NMSE offers excellent robustness without incurring additional cost.
\end{itemize}
