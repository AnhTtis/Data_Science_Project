In this work, we propose the \name framework to make the decision boundary change more moderately, thus eliminating the oscillation phenomenon in the early stage of AT. Also, in the later stage, \name framework considers the decision boundary of previous iteration, preventing the decision boundary from becoming too complex and alleviating the overfitting issue. Moreover, we suggest to use the Normalized Mean Square Error (NMSE) as regularzation to align the clean examples and adversarial examples, that focuses more on the relative magnitude of the output logits rather than the absolute magnitude.
Extensive experiments verify that our framework can eliminate the oscillation phenomenon and alleviate the overfitting issue of adversarial training. Furthermore, the \name combined with NMSE loss could significantly improve the model robustness of adversarial training without extra computational cost.
In addition, our framework is flexible and general, and various adversarial training methods can be combined into our \name framework to further boost their performance.  

Our work shows that the historical information of adversarial training process is very useful. We hope our work will inspire more works utilizing historical training information to boost the model robustness.