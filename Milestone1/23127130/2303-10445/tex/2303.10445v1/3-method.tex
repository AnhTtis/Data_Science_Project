\section{Method}

\subsection{The Construction of Synchronous Audio and Motion Dataset}
Although in this paper, EarCough takes only audio signals as input, we envision sensor fusion with audio and motion data as a potential method to further improve the performance of subject cough event detection. As a result, we collect synchronous audio and motion data and construct the dataset. 

\subsubsection{Participants}
We first recruited 10 participants (3 females, 7 males) with an average age of 21.4 (s.d. = 0.80). None of them had pulmonary diseases. 

\subsubsection{Hardware platform for synchronous data collection}
We established a hardware platform based on smart earbuds for data collection. The hardware platform is shown in Figure ~\ref{fig:coldevice}, which comprises three main components. We used the hybrid ANC earbuds Bose QC 20 as the core collection device, equipped with feed-forward and feedback microphones. Since there is no motion sensor in Bose QC 20, we embedded an inertial measurement unit named MPU9250 into the earbuds. The internal structure of the earbuds is shown in Figure ~\ref{fig:coldevice}C. During the collection, dual-channel audio (48 kHz) is captured by microphones in Bose QC 20 and imported into the DR-05X recorder on the far right of Figure ~\ref{fig:coldevice}B for storage. The embedded MPU9250 shown in Figure ~\ref{fig:coldevice}C is used to collect 6-axis motion signals (including 3-axis accelerometer and 3-axis gyroscope data) at a sampling rate of 1kHz, which are further imported into the Arduino Feather M0 chip on the far left of Figure ~\ref{fig:coldevice}B for processing and storage.

\begin{figure}
    \centering
    \includegraphics[width=1\columnwidth]{Figures/device_new.png}
    \caption{Hardware platform for data collection A: Participant wearing the collection devices B: Three components of the hardware platform C: internal structure of modified Bose QC 20, we embedded the IMU sensor named MPU9250 into the earbuds.}
    \label{fig:coldevice}
    \Description{This figure consists of three subfigures A, B, and C. In subfigure A, a young male participant is seated and wearing data acquisition equipment with earbuds on the left ear. Subfigure B shows the data acquisition equipment, which records the Bose QC 20 audio using a recording pen and collects IMU data through an MPU9250 data cable connected to an SD card-equipped Arduino Feature M0. Subfigure C displays the modified internal structure of the Bose QC 20 earbuds, including the location identification of the feedforward microphone and MPU9250.}
\end{figure}

\subsubsection{User study design and procedure}
The user study was conducted in a standard conference room. During the collection procedure, each subject needs to complete the same ten groups of experiments under three sound environments, including quiet room (43 $\sim$ 50dB), noisy room (64 $\sim$70dB), and environmental cough (45 $\sim$ 60dB). The study was approved by the Institutional Review Board (IRB). 

For the noisy indoor sound environment, Bluetooth speakers are used to randomly play all kinds of noise to create the sound environment, which aims to simulate real-life application scenarios. The played background sounds includes natural sound, musical instrument sound, animal sound, human sound, transportation sound and most of the non-cough sound events in users' life. The environmental cough sound environment is created by playing multiple cough audio randomly. All cough audios are high-quality cough audio from FreeSound ~\cite {font2013freesound} website. The sound environment of environmental cough was created to simulate the public social scenarios where multiple people have cough symptoms. We did not simulate two participants coughing simultaneously in the same conference room, which may expose our participants to extremely high health risks.

In each sound environment, the participant wore the hardware and finished seven groups of collections in sitting position, including single cough 10 times, continuous cough 10 times, 5 bites of apple, 5 sips of water, laughing when watching funny videos (video lasts for 1.5 minutes), reading stories for 2 minutes, and randomly move their heads for 1 minute. The participants also finished three groups of collection in walking state, including walking for 30 seconds, single cough 10 times while walking and continuous cough 10 times while walking.

The collecting procedure lasted for about one hour. After completing the data collection user study, a total of 33059.79 seconds of dual-channel audio and motion data were obtained. Each participant received a 15 USD gift card for their time and effort. We passed the hardware prototype among users after thorough sterilization with 75\% alcohol.

To ensure the annotation quality, we recruited three professional data annotators to annotate the data. The statistic results of collected data after annotation was shown in Table ~\ref{tab:dataStat}. 

\begin{table}
\begin{center}
\caption{Statistic results of collected data}
\small
\label{tab:dataStat}
\begin{tabular}{cccc}
 \hline
 \hline
Event   & Total Dur. (sec.) & Average Dur. (sec.) & S.D. \\
 \hline
Single coughs (sitting) & 120.1   & 0.384                        & 0.291                       \\
 \hline
Continuous coughs (sitting) & 247.7                       & 0.796                        & 0.228                       \\ 
 \hline
Bites of apple  & 1519.0                      & 10.264                       & 3.783                       \\ 
 \hline
Sips of water  & 96.2                        & 0.601                        & 0.594                       \\ 
 \hline
Laughing     & 182.3                       & 1.823                        & 1.590                       \\ 
 \hline
Reading     & 2640.7                      & 88.023                       & 10.510                      \\ 
 \hline
Randomly head movement & 631.5                       & 21.050                       & 6.762                       \\ 
 \hline
Walking     & 1036.8                      & 34.560                       & 4.833                       \\ 
 \hline
Single coughs (walking) & 150.3                       & 0.515                        & 0.250                       \\ 
 \hline
Continuous coughs (walking) & 204.0                       & 0.682                        & 0.293                       \\ 
 \hline
Environmental coughs  & 1799.3                      & 1.166                        & 0.323                       \\ 
 \hline
 \hline
\end{tabular}
\end{center}
\end{table}

\subsection{EarCoughNet: An End-to-end Deep Learning Model For Subject Cough Detection}

\subsubsection{Architecture of EarCoughNet}
The input of EarCoughNet is the raw dual-channel audio signals of the hybrid active noise-canceling earbuds, and the output is the probability of the subject cough event and other events, respectively. As shown in Figure~\ref{fig:earcough}, EarCoughNet consists of four convolution blocks and three fully connected layers. The first convolution layer of the first convolution block is a 2-dimensional convolution layer, which extracts the features of the difference between the dual-channel audio. The rest of the convolutional layers are all 1-dimensional convolutional layers, which has fewer parameters and thus can effectively reduce the model size and resource requirements. 

\begin{figure}[H]
    \centering
    \includegraphics[width=1\columnwidth]{Figures/earcough.png}
    \caption{EarCoughNet architecture}
    \label{fig:earcough}
    \Description{This figure shows the network architecture of EarCoughNet, which takes in a 0.5-second dual-channel audio input and passes through four convolution blocks and three fully connected layers. Among them, the first convolution block consists of a two-dimensional convolution layer, a one-dimensional convolution layer, and a max-pooling layer. The second and third convolution blocks both consist of two one-dimensional convolution layers and a max-pooling layer. The final convolution layer consists of two one-dimensional convolution layers and a global pooling layer.}
\end{figure}

\subsubsection{Training procedure}
We divided the constructed dataset into training (6 users), validation (2 users) and testing (2 users) dataset. All the audio data was cut into 500-ms audio clips, since cough lasted for 350-ms on average~\cite{amoh2013technologies}. We applied data augmentation methods to the audio of training dataset to expand its size and reduce the model's susceptibility to environmental factors. The audio data augmentation consisted of three stages, which are 

\begin{itemize}
\item Standard audio data augmentation including gain adjustment, time shift, pitch shift, speed adjustment and random masking by making 0-10\% of random points zero. 
\item Noise augmentation including adding machine stimulated white Gaussian noise and mixing background noise from various environmental settings. The background noises are selected from ECS-50~\cite{YDEPUT_2015}.
\item Data formatting including data re-sampling and data normalization.
\end{itemize}

After augmentation, the composition of the three datasets is listed in Table~\ref{tab:division}. We adopted early-stopping strategy on the evaluation dataset to decide the end of our training. We trained four EarCoughNet variants with different sampling rate, including 48kHz, 24kHz, 16kHz, and 8kHz. To evaluate the models' effectiveness, we conducted a cross-user evaluation process on the testing dataset.

\begin{table}[H]
\centering
\caption{Composition of training, validation and testing dataset}
\label{tab:division}
\begin{tabular}{cccc}
\hline
\hline
Samples  & Training   & Validation  & Testing  \\
\hline
Subj. coughs  & 2947 & 842  & 844  \\
\hline
Env. coughs & 4835 & 1208  & 1269  \\
\hline
Other events   & 48104 & 9620 & 10403 \\
\hline
\hline
\multicolumn{4}{l}{\small Audio clips are all 500-ms windows.} \\
\end{tabular}
\end{table}