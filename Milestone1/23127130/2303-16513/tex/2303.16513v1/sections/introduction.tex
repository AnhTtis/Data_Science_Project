\section{Introduction}
\label{sec::introduction}
Single Image Super-Resolution (SISR) is the process of reconstructing high-resolution (HR) images from their corresponding low-resolution (LR) counterparts. SISR has long been recognized as a challenging task in the low-level vision domain due to its ill-posed nature, and has attracted a number of researchers dedicated to this field of study over the past decade~\cite{srcnn, vdsr, fsrcnn, espcn, lapsrn, srresnet, edsr, esrgan, rdn, rcan, san, csnl, ipt, swinir, hat, metasr, liif, ultrasr, ipe, itsrn, lte}. A line of SISR research referred to as `\textit{fixed-scale SR}'~\cite{srcnn, vdsr, fsrcnn, espcn, lapsrn, srresnet, edsr, esrgan, rdn, rcan, san, csnl, ipt, swinir, hat} focuses on extracting feature embeddings from LR images and leveraging these embeddings to upsample images with a predefined factor through learnable deconvolutions~\cite{fsrcnn} or sub-pixel convolutions~\cite{espcn}. Despite their success, many of the proposed approaches necessitate a distinct deep neural network model for each upsampling scale, which is usually restricted to a limited selection of integers~(\eg, $2\times$, $3\times$, $4\times$). Such a limitation constrains the potential applications and deployment options of SISR models. To overcome this limitation, approaches for upsampling LR images in a continuous manner via a single model emerge and attracted considerable attention recently.

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{figures/teaser.pdf}
  \caption{An illustration and comparison of different approaches that take into account nearby pixels for continuous upsampling: (a) the local ensemble method used in~\cite{liif}, and (b) our proposed local attention mechanism.}
  \label{fig:local_concept}
\end{figure}

Over the past few years, arbitrary-scale SR has emerged and attracted considerable attention from researchers~\cite{metasr, liif, ultrasr, ipe, itsrn, lte}.Apart from the pioneering work Meta-SR~\cite{metasr}, recent endeavors~\cite{liif, ultrasr, ipe, itsrn, lte} have achieved arbitrary-scale SR by replacing the upsampling layers commonly adopted by previous approaches with local implicit image functions, and have demonstrated favorable performance. These local implicit functions employ multi-layer perceptrons (MLPs) to map 2D coordinates and corresponding latent representations to RGB values. Fig.~\ref{fig:local_concept} illustrates how different approaches sample latent representations based on the queried coordinates (depicted as the red dots). Fig.~\ref{fig:local_concept}~(a) illustrates the local ensemble technique adopted by contemporary mainstream methods~\cite{liif, ultrasr, ipe, itsrn, lte}. It calculates the RGB value of the queried coordinate by taking the weighted average of those of the surrounding four pixels based on their relative distances to the queried coordinate. This approach, however, does not consider contextual information and relies solely on distance.  For instance, in Fig.~\ref{fig:local_concept}, the queried coordinates are intentionally designed to lie on edges. However, merely calculating the weighted average of pixels fails to reflect the contextual information about the image content, thereby preventing the accurate capture of the necessary features for performing SR. As a result, although pixel distance plays a vital role in SR tasks, it is essential to concentrate more on the contextual information in an image.

In light of the above observations, we propose a Local Implicit Transformer (LIT), which expands the numbers of referenced latent vectors and accounts for the feature correlation in the context by exploiting the attention mechanism~\cite{transformer}. LIT comprises a Cross-Scale Local Attention Block (CSLAB) and a decoder. CSLAB generates attention maps based on the bilinearly interpolated latent vectors at the queried coordinates and the key latent vectors sampled from a grid of coordinates with relative positional bias~\cite{swin, crossformer}. The first and second columns of Fig.~\ref{fig:local_concept}~(b) visualize the attention maps generated by LIT, where the attention areas align closely with the edges. By applying attention maps to feature embeddings, the RGB values of the queried coordinates can be contextually predicted. Then, a decoder is adopted to produce RGB values by taking advantage of the attention feature embedding.

In order to address the issue of diverse scaling factors and achieve arbitrary-scale super-resolution, it is crucial to consider the role of upsampling factors in constructing high-resolution images within the local implicit image function. However, simultaneously training a local implicit image function with a wide range of upsampling factors (e.g., $1\times\sim30\times$) poses significant challenges. As a result, we propose a cumulative training strategy to incrementally enhance the fuction's its representative power. The strategy initially trains the local implicit image function with small upsampling factors and then finetunes it with alternatively sampled small and large ones. Furthermore, we present Cascaded LIT (CLIT) to harness the advantages of multi-scale feature embeddings, complementing missing details and information during one-step upsampling. The combination of the cumulative training strategy and CLIT enables efficient and effective handling of arbitrary-scale SR tasks.

The main contributions of our work are summarized as follows: 
(1) We introduce the LIT architecture, which incorporates the local attention mechanism into arbitrary-scale SR
(2) We further develop a cumulative training strategy and the cascaded framework CLIT to effectively handle large-scale upsampling.
(3) We carry out comprehensive analyses of the performance impacts for LIT and CLIT. The extensive experimental findings demonstrate that the proposed LIT and CLIT are able to yield remarkable or comparable results across a wide range of benchmark datasets.

The paper is organized as follows. Section~\ref{sec::related_works} reviews the related work. Section~\ref{sec::methodology} walks through the proposed LIT and CLIT frameworks and the implementation details. Section~\ref{sec::experimentas} presents the experimental results. Section~\ref{sec::conclusion} concludes. 
