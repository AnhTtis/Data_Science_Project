\section{Conclusion}
\label{sec::conclusion}

In this paper, we introduced the attention mechanism and frequency encoding technique to address arbitrary-scale SR tasks. To achieve this objective, we proposed the LIT framework, which consists of a cross-scale local attention block and a local frequency encoding block. The former is designed to find the local latent embedding to reconstruct the corresponding RGB value, while the latter focuses on encoding coordinates with the frequency information derived from the feature embeddings. To enhance the capability of capturing fine details, we proposed CLIT and the corresponding cumulative training strategy that trains the model with progressively increasing upsampling scales. Based on the experimental results, both quantitative and qualitative assessments showcase the superior performance of LIT and CLIT in comparison to existing methods. Furthermore, our comprehensive analyses validated the effectiveness of the training strategies and the components employed by them.
