\input{tables/dataset_results.tex}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/qualitative_fixed.pdf}
    \caption{The qualitative results of LIIF~\cite{liif}, LTE~\cite{lte}, and our CLIT with using RDN~\cite{rdn} as the encoder.}
    \label{fig:qualitative_results_fixed}
    \vspace{-10pt}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/qualitative_float.pdf}
    \caption{The qualitative results of LIIF~\cite{liif}, LTE~\cite{lte}, and our CLIT using RDN~\cite{rdn} as the encoder and non-integer upsampling scales.}
    \label{fig:qualitative_results_float}
    \vspace{-10pt}
\end{figure*}

\section{Experimental Results}
\label{sec::experimentas}
In this section, we present the experimental results and discuss their implications. We begin with a brief introduction to our experimental setup in Section~\ref{subsec::experimental_setup}. Following that, we evaluate our CLIT with different datasets in Section~\ref{subsec::validation_of_clit}.
The, Section~\ref{subsec::visuallizing_local_attention_maps} showcases the learned attention maps. Finally, ablation studies for various configurations of the proposed CLIT and LIT are compared in Section~\ref{subsec::ablation_studies}.

\subsection{Experimental Setup}
\label{subsec::experimental_setup}

\paragraph{Dataset.}
We use the DIV2K dataset~\cite{div2k} for network training. It consists of $1,000$ images in 2K resolutions and provides low-resolution counterparts with down-sampling scales, $\times 2, \times 3, \times 4$, which are generated by the bicubic interpolation method. On the other hand, we also evaluate the performance on the validation set of DIV2K~\cite{div2k}, Set5~\cite{set5}, Set14~\cite{set14}, B100~\cite{b100} and Urban100~\cite{urban100} in terms of peak signal-to-noise (PSNR) values.

\paragraph{Training details}
During training, we feed batches of size $48 \times 48$ low-resolution images into the framework, following the prior works~\cite{edsr}. For each batch, a single upsampling scale is sampled from a uniform distribution $r \sim \mathcal{U}(1, 4)$. With single upsampling scales $s$, a batch of HR images is cropped into patches of size $48r \times 48r$, while the corresponding LR images are cropped into patches of $48 \times 48$. The patches are augmented by randomly horizontal flipping, vertical flipping, and $90^{\circ}$ rotating. Then, we sample $48^{2}$ pixels (coordinate-RGB pairs) on each HR patch as the ground-truths. We set the batch size to $32$ and use the Adam optimizer~\cite{adam} together with L1 loss for training. We train LIT for $1,000$ epochs, and the learning rate is initialized at $1e^{-4}$ and decayed by a factor 0.5 at [200, 400, 600, 800] epochs. For cumulative training of CLIT, as discussed in Section~\ref{subsec::clit}, we sample $N$ scale factors $\{s^{1}, s^{2}, ..., s^{N}\}$ from the distribution $\mathcal{U}(1, 4)$ according the number of LITs $N$ in the train step. The total upsampling scale $r = s^{1} \times s^{2} ... \times s^{N}$ is the product of all scale factors. If $48r \times 48r$ is greater than the whole HR image, we clip the scale factor of stage 1. To train $N$ LITs, we fine-tune the model for $500 \times N$ epochs, and the learning rate is initialized at $1e^{-4}$ and decayed by a factor 0.5 at [$100 \times N$, $200 \times N$, $300 \times N$, $400 \times N$] epochs.

\subsection{Validation of CLIT}
\label{subsec::validation_of_clit}

\paragraph{Quantitative results.}
We first compare our proposed CLIT to existing local implicit neural representation methods for arbitrary-scale SR, including LIIF~\cite{liif}, UltraSR~\cite{ultrasr}, IPE~\cite{ipe}, and LTE~\cite{lte}. Table~\ref{table:div2k_results} summarizes the quantitative results in terms of PSNR(dB) on DIV2k~\cite{div2k}. As shown in Table~\ref{table:div2k_results}, CLIT achieves the best performance when EDSR-Baseline~\cite{edsr}, RDN~\cite{rdn} are used as the encoders. These results demonstrate the advantage of CLIT.

Table~\ref{table:dataset_results} compares CLIT to prior works~\cite{liif,ultrasr,ipe,lte} on widely-used datasets, including Set5~\cite{set5}, Set14~\cite{set14}, B100~\cite{b100}, and Urban100~\cite{urban100}, with RDN and SwinIR. Note that RDN~\cite{rdn} is trained and evaluated for specific upsampling scales. CLIT outperforms those existing methods in most cases across all datasets and scales, even achieving a 0.20dB PSNR improvement on Set5 for the $\times 8$ scale.

\vspace{-10pt}
\paragraph{Qualitative results.}
Fig.~\ref{fig:qualitative_results_fixed} compares the qualitative results of CLIT with the baseline methods LIIF~\cite{liif} and LTE~\cite{lte}, on various dataset DIV2k~\cite{div2k}, Set14~\cite{set14}, B100~\cite{b100}, and Urban100~\cite{urban100} with variant upsampling scales. The official codes provided by them were used to produce the results. In the first row, the SR visualization results with a $\times12$ upsampling scale are depicted.  It can be observed that both LIIF and LTE struggle to reconstruct the letters continuously, whereas the CLIT result demonstrates continuity for these alphabets, particularly for 'M' and 'S'. In the second row, the stripes on the zebra appear blurry in the LIIF and LTE results,  whereas they are more distinct in the CLIT result. In the third row, the cross pattern on the tablecloth is not clearly rendered by LIIF and LTE.  In contrast, CLIT is able to successfully generate clean crosses and sharp lines. In the fourth row, despite the blurriness of the original LR image, CLIT captures the texture and produces straight lines on the flag, showcasing its effectiveness.

Fig.~\ref{fig:qualitative_results_float} presents the results of text image enhancement employing progressively increasing non-integer upsampling factors, alongside a comparison with earlier methods, LIIF \cite{liif} and LTE \cite{lte}. The input image first undergoes a downscaling process by a factor of $2.2$ to generate an LR image. Subsequently, the LR image is then upscaled using predetermined non-integer factors ${ \times1.6, \times2.5, \times3.4, \times4.3 }$. It can be observed that our proposed model effectively captures the patterns of text within the image, and accurately estimates words and numbers with enhanced sharpness and clarity. The enhancement becomes particularly evident in the word 'infinity' in the first row of contexts, 'SUNSHINE' in the fifth row, and the number '0' in the final row, as each of these elements demonstrates considerable improvements.

\subsection{Visualization Local Attention Maps}
\label{subsec::visuallizing_local_attention_maps}

In Fig.~\ref{fig:qualitative_results_attn}, we providevisualizations of local attention maps. The LR images are generated by applying bicubic downsampling to HR images with scale factors $\{\times 4.5, \times 6, \times 6\}$. Subsequently, the proposed CLIT with RDN \cite{rdn} encoder is utilized to produce HR predictions.  It can be observed that the attention maps closely align with the edges, indicating that the cross-scale local attention block effectively captures similar latent codes within a local area. This substantiates that our proposed design enables the generation of SR images from LR counterparts, ultimately producing clean and sharp edges in the output.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/qualitative_attn.pdf}
    \caption{A visualization of local attention maps of the coordinates highlight as \textcolor{red}{red dots}.}
    \label{fig:qualitative_results_attn}
\end{figure}

\subsection{Ablation Studies}
\label{subsec::ablation_studies}
In this section, we present a series of ablation analyses to substantiate the design decisions proposed in this paper. All of the ablation experiments are conducted on the DIV2K \cite{div2k} validation set, utilizing the EDSR-baseline \cite{edsr} as the encoder and employing the PSNR metric for evaluation.

\vspace{-10pt}
\paragraph{Validation of the design choices.}
Table~\ref{table:desing_choice} presents a summary of the quantitative contributions associated with each LIT component. A significant improvement is observed when adopting the cross-scale local attention block by comparing LTE with LTE (-a). In addition, LTE (-c) reveals that removing cell decoding leads to more severe performance degradation for in-distribution upsampling scales compared to out-of-distribution counterparts, highlighting the importance of cell decoding for in-distribution upsampling scales. Finally, incorporating the local ensemble technique in LIT (+e) results in a further, albeit modest, enhancement of the overall performance.

\input{tables/design_choice.tex}
\input{tables/local_grid.tex}
\vspace{-10pt}
\paragraph{The effectiveness of the local grid.}
Table \ref{table:local_grid} provides a performance comparison of various local grid sizes employed within the local coordinate sampling scheme, as discussed in Section~\ref{sec::methodology}. The results presented in Table \ref{table:local_grid} reveal that increasing the local grid size contributes to performance enhancements, albeit at the cost of extended training times. As a result, this study adopts a $7\times7$ local grid size to strike an ideal balance between effectiveness and efficiency.

\vspace{-10pt}
\paragraph{Analysis on the training strategy.}
Table~\ref{table:training_strategy} presents a quantitative comparison of the proposed cumulative training strategy with the other training strategies for training an LIT. The baseline strategy trains LIT using upsampling scales sampled from a uniform distribution, $r \sim \mathcal{U}(1, 4)$. Expanding the sampling scale distribution to $r \sim \mathcal{U}(1, 12)$ improves the performance of large-scale upsampling while compromising the performance of small-scale upsampling. To achieve high-quality results across all upsampling scale ranges, the alternative training strategy trains LIT alternatively by switching the sampling scales between $r \sim \mathcal{U}(1, 4)$ and $r \sim \mathcal{U}(4, 12)$. On the other hand, the proposed cumulative training strategy first trains LIT with $r \sim \mathcal{U}(1, 4)$, followed by fine-tuning using the alternative training strategy. This progressive approach improves the performance of LIT by gradually broadening the distribution of the upsampling scales. Comparing the results of the cumulatively trained LIT with the results of EDSR-baseline-CLIT in Table~\ref{table:div2k_results} reveals that the proposed cumulative training strategy works for both LIT and CLIT. Moreover, the results from these tables suggest that the cascaded framework is more effective in addressing arbitrary-scale SR tasks.

To validate the generalizability of the proposed strategy, we apply it on LIIF~\cite{liif} and LTE~\cite{lte} using the same setups, and report the results in Table~\ref{table:cumulative_results}. The results reveal that the cumulative training strategy is able to enhance the performance of LIIF and LTE as well. An interesting fact is that by adopting the proposed cumulative training strategy, the performance of LTE is able to be improved without the need of its original cell clipping operation. The experimental evidence thus justifies the effectiveness and generalizability of our strategy.

\input{tables/training_strategy.tex}
\input{tables/cumulative_results.tex}
