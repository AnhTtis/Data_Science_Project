\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath,nccmath}
\usepackage{amssymb}
\usepackage{amsmath,amsthm,mathtools}
\usepackage{multirow} 
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{enumitem}
%\usepackage{authblk}
\usepackage{bbold}
\usepackage[table]{xcolor}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{rotating}
\usepackage{kantlipsum}
\usepackage[symbol]{footmisc}
% Include other packages here, before hyperref.
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

 \iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{9100} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\input{figure.tex}
% Pages are numbered in submission mode, and unnumbered in camera-ready%\ificcvfinal\pagestyle{empty}\fi

\begin{document}
% INITIAL SUBMISSION 
%%%%%%%%% TITLE
\title{Efficient Computation Sharing for Multi-Task Visual Scene Understanding}
%\end{comment}
%******************
%\email{\{mingyuy,unchenyu,hunseok\}@umich.edu}
\author{Sara Shoouri
%Institution1\\
%Institution1 address\\
%{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\qquad
Mingyu Yang
%Institution2\\
%First line of institution2 address\\
%{\tt\small secondauthor@i2.org}
\qquad
Zichen Fan
%Institution3\\
%First line of institution3 address\\
%{\tt\small secondauthor3@i2.org}
\qquad
Hun-Seok Kim \\[2mm] University of Michigan \\ {\tt\small \{sshoouri,mingyuy,zcfan,hunseok\}@umich.edu}
%Institution3\\
%First line of institution3 address\\
%{\tt\small secondauthor3@i2.org}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
\begin{abstract}
Solving multiple visual tasks using individual models can be resource-intensive, while multi-task learning can conserve resources by sharing knowledge across different tasks. Despite the benefits of multi-task learning, such techniques can struggle with balancing the loss for each task, leading to potential performance degradation. We present a novel computation- and parameter-sharing framework that balances efficiency and accuracy to perform multiple visual tasks utilizing individually-trained single-task transformers. Our method is motivated by transfer learning schemes to reduce computational and parameter storage costs while maintaining the desired performance. Our approach involves splitting the tasks into a base task and the other sub-tasks, and sharing a significant portion of activations and parameters/weights between the base and sub-tasks to decrease inter-task redundancies and enhance knowledge sharing. The evaluation conducted on NYUD-v2 and PASCAL-context datasets shows that our method is superior to the state-of-the-art transformer-based multi-task learning techniques with higher accuracy and reduced computational resources. Moreover, our method is extended to video stream inputs, further reducing computational costs by efficiently sharing information across the temporal domain as well as the task domain. Our codes and models will be publicly available.
%(to be populated upon acceptance).
   %Solving multiple visual scene understanding tasks can be done by training individual single-task networks or jointly optimizing all tasks by multi-task learning (MTL) techniques. Running multiple single-task models increases the computational and memory costs linearly with the number of tasks. Whereas, MTL approaches enable cross-task knowledge sharing and enhance inference efficiency. However, these methods usually suffer from performance degradation due to the difficulty of balancing the task-specific loss for all tasks. Our presented method builds a bridge between single-task and MTL models by taking advantage of transfer learning schemes and proposes to solve multiple visual scene understanding tasks using individually-trained single-task transformers with a novel computation-sharing strategy. We first split the desired tasks into one base task and multiple sub-tasks. Then, we share a significant portion of activations and weights between base and sub-tasks to remove the inter-task redundancies and boost knowledge sharing. Experiment results show that our method outperforms the state-of-the-art transformer-based multi-task learning methods on NYUD-v2 and PASCAL-context with fewer FLOPs and parameters. In addition, we extend our approach to video stream inputs and further reduce computational costs by sharing the information across task and temporal domains. 
\end{abstract}

%%%%%%%%% BODY TEXT
\vspace{-5mm}
\section{Introduction}
\vspace{-2mm}
In various computer vision applications, it is required to obtain a comprehensive understanding of the visual scene by performing multiple tasks based on a single input image. These tasks often involve performing dense or pixel-wise predictions such as semantic segmentation, depth estimation, surface normal estimation, etc., for practical applications in autonomous driving, robotics, and augmented or virtual reality (AR/VR) \cite{ye2022inverted}. Traditionally, these tasks are tackled individually by training a separate neural network dedicated to each task. However, this single-task learning can lead to redundant computation and parameters, particularly for highly correlated tasks, losing the opportunity to perform faster inference as desired in real-time applications \cite{vandenhende2020mti}.
%Visual scene understanding typically involves solving multiple dense/pixel-wise prediction tasks (e.g., semantic segmentation, depth estimation, and normal estimation), and it has extensive real-world applications such as autonomous driving, robotics, and augmented or virtual reality (AR/VR) \cite{ye2022inverted}. One straightforward way to solve multiple dense prediction tasks is to train individual neural networks for each task. However, using single-task networks may lead to parameter and computation redundancy, especially for highly-correlated tasks, which does not meet the requirement of fast inference for real-time applications \cite{vandenhende2020mti}.
\figintro

Multi-task learning (MTL) \cite{caruana1997multitask,evgeniou2004regularized,li2022universal,zamir2020robust,zhang2021transfer} has been actively explored as a solution to this problem, which learns a single unified model to perform multiple tasks simultaneously. This approach allows the model to learn common representations and patterns from multiple supervised tasks \cite{zhang2021survey}, resulting in a more memory- and computation-efficient structure than employing multiple single-task networks. Additionally, multi-task networks can potentially enhance the performance of all tasks by leveraging the cross-task knowledge-sharing mechanism \cite{vandenhende2020mti}. Due to the benefits of multi-task learning, numerous multi-task structures have been proposed to improve task-wise interactions in dense visual scene understanding \cite{misra2016cross,xu2018pad,vandenhende2020mti, zhang2019pattern, zhou2020pattern, maninis2019attentive, gao2019nddr}. The recent success of transformer models in many downstream vision tasks \cite{liu2021swin, dosovitskiy2020image, zhu2020deformable, wang2020axial,ranftl2021vision,liu2022convnet} has led to the emergence of transformer-based multi-task networks \cite{seong2019video, bhattacharjee2022mult, ye2022inverted,chen2021pre,mohamed2021spatio}, aimed at improving multi-task performance even further.

%Multi-task learning \cite{caruana1997multitask,evgeniou2004regularized} has been actively studied and serves as a solution to this problem. In general, multi-task learning methods can learn shared representations from multi-task supervised signals. Compared to single-task networks, the layer-sharing structure in multi-task networks provides better memory efficiency and avoids repeatedly calculating representations for all tasks. Moreover, multi-task networks could potentially improve the overall performance of all tasks due to their cross-task knowledge-sharing mechanism \cite{vandenhende2020mti}. Because of the advantages of multi-task learning, various multi-task network structures have been proposed to improve task-wise interactions for visual scene understanding tasks \cite{misra2016cross,xu2018pad,vandenhende2020mti, zhang2019pattern, zhou2020pattern, maninis2019attentive, gao2019nddr}. Recently, with the superior performance of the transformer models in many downstream vision tasks \cite{liu2021swin, dosovitskiy2020image, zhu2020deformable, wang2020axial}, several transformer-based multi-task networks have been proposed to further improve the multi-task performance \cite{seong2019video, bhattacharjee2022mult, ye2022inverted}. 

However, MTL poses several potential drawbacks compared to single-task learning: (1) Simultaneously learning different tasks using a unified model can lead to unbalanced task competition and suboptimal performance for some tasks if the model fails to build a shared representation that generalizes to all tasks \cite{fifty2021efficiently}. (2) Balancing the loss between different tasks can be challenging due to the varying scales of task-specific loss terms, especially when the number of tasks increases \cite{chen2020just,chen2018gradnorm}. Although multiple task-balancing approaches have been proposed \cite{kendall2018multi,chen2018gradnorm,liu2019end} to address this problem, recent studies \cite{vandenhende2021multi} have shown that performance still becomes worse than individually-trained single-task networks. (3) MTL requires ground truth labels for all tasks per training sample, which is limiting because such annotations for certain tasks (e.g., semantic segmentation) may not always be available for the same input sample. It also often does not allow easily adding new tasks without retraining the entire model from scratch.
%However, compared to single-task networks, multi-task learning approaches have several disadvantages due to their requirement to train multiple tasks simultaneously. First, jointly training multiple terms may lead to unbalanced task competition and cause severely degraded performance compared to single-task learning when the model cannot build a shared representation that generalizes to all tasks \cite{fifty2021efficiently}. Finding the optimal balancing weights with manual tuning would be challenging because of the scale difference between different loss terms. Moreover, the loss balancing process becomes laborious as the number of tasks increases \cite{chen2020just,chen2018gradnorm}. Recently, multiple task-balancing approaches have been proposed \cite{kendall2018multi,chen2018gradnorm,liu2019end} to address this problem. However, as shown in \cite{vandenhende2021multi}, the performance of multi-task networks for visual scene understanding still gets worse than individually-trained single-task networks, even with task-balancing techniques. Second, such joint training approach requires the ground-truth labels of all tasks for every image frame, which significantly limits the number of training samples since the annotations of some tasks are hard to achieve (e.g., semantic segmentation). Third, with the joint training approach, extending a trained multi-task network to new tasks is difficult as it would require re-training the whole model.

To alleviate the above limitations, we propose a solution that leverages the strength of both single-task and multi-task learning techniques. We employ individually-trained single-task networks to maintain the desired performance and prevent unbalanced task competition. At the same time, we introduce a novel parameter- and computation-sharing strategy to facilitate knowledge-sharing and enhance inference efficiency. Our method focuses on transformer models that have shown outstanding results in vision tasks. First, we divide all tasks into a base task and multiple sub-tasks, where all task-specific networks adopt a common transformer structure as the \textit{backbone}. Next, we train a single network for the base task independently. Then, to share the inter-task information, we reuse \textit{not only weights but also activations} from the base task to train each sub-task. The weight-sharing concept is motivated by recent parameter-efficient transfer learning techniques \cite{houlsby2019parameter, guo2020parameter}. Specifically, we view the weights of sub-task models as the sum of weights from the base task and a \textit{delta weight} matrix, and apply $\ell_0$ regularization to encourage sparsity in the delta weight matrix as in the Diff-pruning \cite{guo2020parameter} approach. We then fix the positions of non-zero elements of the delta weight matrix and fine-tune them to make the \textit{activation difference} between the base task and sub-task also sparse by adopting $\ell_1$ regularization. As a result, the pre-computed activations from the base task can be shared and passed to sub-task networks during the inference. This reduces computation cost as the remaining operations for sub-tasks only involve sparse matrix-matrix multiplications, as shown in Figure \ref{fig:fig_intro}. Our proposed computation-sharing scheme significantly reduces the number of non-zero parameters for sub-tasks while allowing knowledge sharing between the main task and each sub-task. Extensive experiments on NYUD-v2 and Pascal-Context benchmarks demonstrate that our method outperforms state-of-the-art multi-task transformers, exhibiting fewer parameters and FLOPs counts to attain comparable or superior task accuracy.
%To alleviate these issues, we solve multiple tasks by training individual single-task networks. More specifically, we focus on the transformer models due to their promising results in vision tasks. However, as stated before, solving each task independently leads to extensive redundancy in parameters and computations. Motivated by recent parameter-efficient transfer learning techniques \cite{houlsby2019parameter, guo2020parameter}, we propose a novel computation-sharing scheme that significantly decreases both parameter redundancy and computation redundancy in dense layers. More specifically, we divide all tasks into one base task and a group of sub-tasks. All task-specific neural networks are assumed to adopt a common transformer structure as the backbone. We first train a neural network for the base task independently. Then, we reuse the weights and activations from the base task to train each sub-task in two steps. First, we view sub-task models as an extension of the base network and adopt the parameter-efficient transfer learning technique in Diff-pruning \cite{guo2020parameter} to learn sparse task-specific delta weight matrix by applying $\ell_0$ regularization. After training, we fix the positions of the non-zero elements of the delta weight matrix and then finetune the recorded elements to force the activation difference between the base task and sub-task to be sparse by applying $\ell_1$ regularization. Thus, for the inference of sub-task models, the pre-computed activations from the base task can be directly passed to sub-tasks. In this way, the computation cost is reduced as the remaining operations only contain sparse matrix multiplications, as shown in Figure \ref{fig:fig_intro}. With the proposed computation-sharing scheme, the parameters and computation costs are significantly diminished for sub-tasks. Moreover, the proposed method allows pair-wise knowledge sharing between the base task and each sub-task. On NYUD-v2 and PASCAL-context benchmarks, our method outperforms the state-of-the-art multi-tasking transformer by utilizing fewer parameters and FLOPs for single image inputs. 

Furthermore, we extend our method to the temporal domain to leverage the sparsity of differences between consecutive video frames, as shown in Figure \ref{fig:fig_intro}. Similar to the task domain, we employ $\ell_1$ regularization to force the activation differences between consecutive (time domain) frames to become sparse. As a result, during the inference of a sub-task at time $t$, it can exploit either the temporal domain or task domain sparsity to reuse activations from the same sub-task at time $t-1$ (temporal activation sharing) or the main task at time $t$ (task domain activation sharing). A simple strategy is then applied to combine these two sources of activation sharing for maximum efficiency and computation savings.


%Considering the fact that the difference between consecutive video frames is sparse in nature, we also extend our proposed scheme to the temporal domain to force sparse activation differences for adjacent frames. As a result, during the inference of a sub-task at time $t$, we now get two sources for computation-sharing: the activations from the sub-task at time $t-1$ and the activations from the base task at time $t$. Thus, we design a simple strategy to combine the two sources of activations for maximum computation savings.
\begin{figure*}[t]
\setlength\belowcaptionskip{-1.3\baselineskip}
 
 \includegraphics[keepaspectratio,scale=0.15]{Figures/Main_figure_new.pdf}
 %\vspace{-0.75\baselineskip}
 \centering
 \caption{Illustration of the proposed method for multi-task processing for video inputs. $X_{l}^{b_k,t}$, $\delta X_{l}^{b_k,t}$, and $\Delta X_{l}^{b_k,t}$ represent the input activation, delta task activation, and delta temporal activation of layer $l$ for task $b_k$ at time $t$, respectively.
 For a keyframe at $t$, the base task shares the parameter and activation computation with sub-tasks to improve the efficiency of the sub-task transformer models. For a non-keyframe at $t+1$, the base task first borrows the temporal activation from the previous frame at $t$ and then passes the calculated task activation to sub-tasks for layer $[1,l]$ at $t+1$. For the remaining layers of $[l+1,L]$ at $t+1$, the sub-tasks reuse the temporal activation from the previous time $t$ to further reduce the computation.}
 \label{fig:Structure}
\end{figure*}
Our contributions can be summarized as follows:
\begin{itemize}[noitemsep,topsep=0pt]

  \item We propose a novel activation-and parameter-sharing scheme to reduce the computation and storage redundancy to perform multiple dense/pixel-wise vision tasks with transformer models. 
  

  \item We extend our computation-sharing method to video inputs and use a simple strategy to combine the activation-sharing sources between the task and temporal domains to maximize the inference efficiency.
 
  
  \item We perform extensive experiments on NYUD-v2 and Pascal-Context datasets to quantify
the advantage of our proposed method in both performance and efficiency compared to prior multi-task learning methods. %of the single-task learning using activation reuse algorithm for multi-task dense prediction, establishing new state-of-the-art performance.
  %\vspace{-5pt}
\end{itemize}
\vspace{-2mm}
\section{Related Work}
\label{Related Work}
\vspace{-1mm}
\subsection{Multi-task for Scene Understanding}
\vspace{-1mm}
Multi-task learning (MTL) has made significant progress in several vision applications \cite{caruana1997multitask,evgeniou2004regularized,kumar2012learning,kendall2018multi}, such as joint object detection and semantic segmentation \cite{girshick2014rich, he2017mask}, and 3D recognition \cite{yang2016discriminative, yang2016latent}. Due to the benefits of MTL, various works have explored its use in multi-task dense scene understanding, where all the tasks require pixel-wise predictions \cite{eigen2015predicting, misra2016cross} based on convolutional neural networks (CNN) \cite{gao2019nddr, bruggemann2021exploring, maninis2019attentive}. Specifically, Cross-Stitch \cite{misra2016cross} enables soft feature fusion by utilizing a linear combination of the activations in each layer of task-specific networks. PAD-Net \cite{xu2018pad} first generates multi-modal auxiliary predictions and then uses a spatial attention strategy to distill information from the initial predictions. MTI-Net \cite{vandenhende2020mti} extends the idea of PAD-Net by proposing a multi-scale multi-modal distillation procedure to refine the feature propagation, leading to unique task interactions at each individual scale. On the other hand, PAP \cite{zhang2019pattern} and PSD \cite{zhou2020pattern} learn global and local affinities across tasks, and use them to refine the features for each task iteratively. Recently, the transformer model has also been applied to multi-task learning \cite{chen2021pre, mohamed2021spatio, seong2019video, bhattacharjee2022mult}, replacing CNNs for superior performance in several vision tasks \cite{liu2021swin,dosovitskiy2020image,bello2019attention,carion2020end,parmar2018image,ramachandran2019stand,touvron2021training,wang2018non}. For instance, InvPT \cite{ye2022inverted} leverages vision transformers as the backbone and explores self-attention for better spatial and task interaction, achieving state-of-the-art results for multi-task visual scene understanding.
%Motivated by the recent progress of multi-task learning \cite{caruana1997multitask,evgeniou2004regularized,kumar2012learning} in several vision applications such as joint object detection and semantic segmentation \cite{girshick2014rich, he2017mask}, and 3d recognition \cite{yang2016discriminative, yang2016latent}, various works have explored applying multi-task learning to solve multiple dense scene understanding tasks simultaneously \cite{eigen2015predicting, misra2016cross}. Early works in this field mainly focus on convolutional neural networks (CNN) \cite{gao2019nddr, bruggemann2021exploring, maninis2019attentive}. 
 %Cross-Stitch \cite{misra2016cross} enables feature sharing by learning specific linear combinations of features from all tasks. PAD-Net \cite{xu2018pad} first produces multi-modal auxiliary prediction and then utilizes a multi-modal distillation module for the final predictions. MTI-Net \cite{vandenhende2020mti} further extends the idea of PAD-Net and proposes a multi-scale refinement scheme for better feature propagation. Differently, PAP \cite{zhang2019pattern} and PSD \cite{zhou2020pattern} learn global and local affinities across tasks, and then apply them iteratively to get task-specific features. The transformer model has recently been applied to multi-task learning \cite{chen2021pre, mohamed2021spatio, seong2019video, bhattacharjee2022mult} due to its superior performance in several downstream vision tasks \cite{liu2021swin,dosovitskiy2020image}. Specifically, the recent work InvPT \cite{ye2022inverted} utilizes vision transformers as the backbone and explores self-attention for better spatial and task-wise interaction, which achieves state-of-the-art performance for multi-task visual scene understanding. 

Our approach differs from prior multi-task learning methods which mainly focus on enhancing cross-task interaction. Specifically, we utilize individually-trained single-task networks with a novel computation-sharing scheme to improve inference efficiency while preserving their performances to outperform prior multi-task learning methods.

%Our method shares the same goal as these multi-task learning methods. For multi-task learning, all tasks are jointly optimized, and most effort focuses on designing new structures to learn better cross-task interaction. Differently, we solve multiple tasks with individually-trained single-task networks, which usually outperform multi-task learning methods. We focus on designing a novel computation-sharing scheme to improve the efficiency of single-task networks while retaining their performances.

\subsection{Parameter-Efficient Transfer learning}
\vspace{-2mm}
Transfer learning technique has been widely adopted in various applications, including computer vision \cite{caron2020unsupervised,noh2019transfer,sun2019meta}, natural language processing (NLP) \cite{gordon2020compressing,huang2021ghostbert,zhao2020masking,sanh2020movement,liu2019multi,stickland2019bert}, and speech recognition \cite{tomanek2021residual,thomas2022efficient}. In transfer learning techniques, the knowledge (i.e., extracted features) from one task is transferred to other tasks to enhance the performance of related tasks. The most common transfer learning methods for transformer-based models involve training a shared model, and then either fine-tuning model parameters or using linear probes with additional multi-layer perceptron (MLP) models \cite{wuunderstanding,tripuraneni2020theory,dufew} for individual tasks. The fine-tuning method can maintain high performance at the cost of high computational complexity for large transformers \cite{patterson2021carbon}, whereas linear probing exhibits relatively low performance with reduced complexity \cite{kumarfine,kornblith2019better,zhai2019large,he2020momentum}. Recently, there have been several research efforts focused on improving the parameter efficiency for transformers in NLP \cite{houlsby2019parameter, guo2020parameter, hu2021lora, zaken2021bitfit, karimi2021compacter}. For example, Adapter \cite{houlsby2019parameter} addresses this challenge by employing additional task-specific trainable modules after the feedforward network in each layer of the shared transformer architecture. Diff-pruning \cite{guo2020parameter} further improves parameter efficiency by learning sparse task-specific difference (`diff' or `delta') vectors to be added to the shared parameters. LoRA \cite{hu2021lora} takes a similar approach but decomposes the task-specific difference vector as a low-rank matrix. Parameter-efficient transfer learning has also been studied for vision transformers \cite{he2022parameter}. Unlike these prior methods that mainly focus on improving parameter efficiency, our approach extends the existing methods to improve both parameter and computation efficiency when multiple pixel-level tasks are performed on a single still image or consecutive video frames. 
%The two most common transfer learning techniques are fine-tuning all model parameters or doing linear probes with additional multi-layer perception (MLP). However, Fine-tuning the whole model would lead to a high computational cost for large transformers \cite{patterson2021carbon}. On the other hand, linear probing is parameter-efficient, but the performance is not satisfactory. Recently, multiple parameter-efficient transfer learning methods have been proposed for transformers used in NLP \cite{houlsby2019parameter, guo2020parameter, hu2021lora, zaken2021bitfit, karimi2021compacter}. Adopter \cite{houlsby2019parameter} provides additional trainable modules after the feedforward network in each Transformer layer. Diff-pruning \cite{guo2020parameter} further improves the parameter efficiency by learning additional sparse task-specific diff vectors to the original parameters. Similarly, LoRA \cite{hu2021lora} decomposes the task-specific diff vector as a low-rank matrix. Recently, parameter-efficient transfer learning is also studied for vision transformers \cite{he2022parameter}. Unlike these methods, which only consider parameter efficiency, we extend similar ideas to improve parameter and computation efficiency when multiple predictions are required for the same images. 
\vspace{-2mm}
\section{Proposed Method}
\vspace{-2mm}
The proposed approach is designed to enable the efficient processing of multiple dense (per pixel) tasks from a single input image. The approach is further optimized for video sequences to balance accuracy and efficiency, reducing computational costs by exploiting either the task or temporal domain sparsity as depicted in Figure \ref{fig:Structure}. We first formalize the problem for the algorithm and then present a general framework that leverages activation reuse across the task or temporal domain. %to share computation across task and temporal domains.
%This paper presents an algorithm that efficiently processes multiple dense predictions for a single image. The method is further optimized for video streams, ensuring a balance between accuracy and efficiency by reducing computational costs in both task and temporal domains, as illustrated in Figure \ref{fig:Structure}. We first formalize the problem setting for the algorithm and then proceed to explain the general framework that utilizes activation reuse to share computation between task and temporal domains.
%This paper proposes an efficient algorithm for processing multiple dense predictions for a single image. Moreover, the proposed method is extended for video streams to balance the accuracy and efficiency by reducing computational cost in both task and temporal domains, as summarized in Figure \ref{fig:Structure}, making it more practical for real-world applications. We first formalize the problem setting for our method and then proceed to explain the general framework for computation sharing across task and temporal domains utilizing activation re-use.


%We introduce an efficient algorithm to execute multiple dense predictions for video stream inputs, which decreases the computational cost across task and temporal domains, as summarized in Figure \ref{fig:Structure}. In this section, we first
%formalize the problem setting of our method. Then, we explain the general framework and computation-sharing approach for the task and temporal domains using activation re-use in detail. 

%\subsection{Problem Setting}

%The purpose of this paper is to develop a multi-task model, $\mathcal{G}$, that can address a set of $m$ visual tasks $\mathcal{B}=\{b_1, b_2,\ldots, b_m\}$ using a single input image $I$ as input. The model outputs predictions, $y=\{y^{b_1},y^{b_2},\ldots,y^{b_m}\}$, for each task, with a particular emphasis on per-pixel level predictions such as semantic segmentation and depth estimation. Furthermore, the model is extended to video sequences by processing a set of video frames, $\{I^{1}, I^{2},\ldots \}$, where $I^{t}$ is the input frame at time $t$. Hence, the output of the model for the frame at time $t$ can be formulated as $\mathcal{G}(I^{t_j}) \to y^{t_j}=\{y_{b_1}^{t_j},y_{b_2}^{t_j},\ldots,y_{b_m}^{t_j}\}$.

%Given a set of $m$ visual tasks $\mathcal{B}=\{b_1, b_2,\ldots, b_m\}$, we aim to learn a multi-task model $\mathcal{G}$ that can address all tasks such that: $\mathcal{G}(I) \to y=\{y_{b_1},y_{b_2},\ldots,y_{b_m}\}$, where $I$ is an input image and $y_{b_k}$ is the prediction of task $b_k$. Here we focus on the per-pixel level prediction tasks (e.g., semantic segmentation or depth estimation). Also, we extend our idea and attempt to apply $\mathcal{G}$ to a set of video frame sequences $\{I^{t_1}, I^{t_2},\ldots \}$, where $I^{t_j}$ is the input frame at time $t_j$. Thus, we can formulate the output at time $t_j$ as $\mathcal{G}(I^{t_j}) \to y^{t_j}=\{y_{b_1}^{t_j},y_{b_2}^{t_j},\ldots,y_{b_m}^{t_j}\}$.
\vspace{-1mm}
\subsection{Framework Overview}
\vspace{-1mm}
\label{framework}
\paragraph{Problem formulation: } The proposed multi-task model, $\mathcal{G}$, is designed to perform a set of $m$ visual tasks, $\mathcal{B}=\{b_1, b_2,\ldots, b_m\}$, on a single input image $I$. It generates the task outputs, $\hat{y}=\{\hat{y}^{b_1},\hat{y}^{b_2},\ldots,\hat{y}^{b_m}\}$, for each task in $\mathcal{B}$. Here we assume per-pixel tasks such as semantic segmentation or depth estimation. The model can be further applied to a set of video frames $\{I^{1},\ldots \, I^{T}\}$, where $I^{t}$ is the input frame at time $t$. Hence, the model output for the frame at time $t$ can be represented as $\mathcal{G}(I^{t}) \to \hat{y}^{t}=\{\hat{y}^{b_1,t},\hat{y}^{b_2,t},\ldots,\hat{y}^{b_m,t}\}$.


We begin by dividing the desired tasks into a base task and the other sub-tasks. Without loss of generality, suppose $b_1$ is the base task and the remaining tasks, $\{b_2,\ldots,b_m\}$, are the sub-tasks.
To perform each task, we define separate single-task networks and leverage transfer learning techniques to initialize the parameters of the sub-tasks using the base model and then fine-tune each model for a task-specific objective. 

Suppose a training dataset $\mathcal{D}^{b_k}=\{I_{n},y_{n}\}^{N}_{n=1}$ is given for a single network for task $b_k$. We seek to determine the optimal parameters $w^{b_k}$ for that model by solving the following optimization problem:
%We first split the desired tasks into one base task and multiple sub-tasks. For this selection, we choose the most challenging or crucial task as the base/pretrained model. Without the loss of generality, suppose $b_1$ is the preferred base task, and $\{b_2,\ldots,b_m\}$ is the set of the sub-tasks. We then take advantage of transfer learning strategies and initialize a subset of the sub-task parameters from the base model (task $b_1$) to finetune on a task-specific objective. Particularly, given a training dataset $\mathcal{D}_{b_k}=\{I_{(n),{b_k}},y_{(n),{b_k}}\}^{N}_{n=1}$, for task $b_k$, the goal is to generate the model parameters $w_{b_k}$ to maximize the following optimization problem:
%\vspace{-1.5mm}
\setlength{\belowdisplayskip}{5pt}
\begin{equation}
\label{finetune_loss}
\setlength{\abovedisplayskip}{-0.1pt} 
       \min_{w^{b_k}} \frac{1}{N} \sum_{n=1}^{N} \mathcal{C}^{b_k
}(f^{b_k}(I_{n};w^{b_k}),y_{n})+ \lambda \mathcal{R}(w^{b_k}),
       %\setlength{\belowdisplayskip}{-0.01pt}
\end{equation}
where $f^{b_k}(.;w^{b_k})$ represents a task-specific network, $\mathcal{C}^{b_k}(.)$ is a task-specific loss function, and $\mathcal{R}(.)$ is an optional regularization term with a hyperparameter $\lambda$. For the network $f^{b_k}$, we use a vision transformer \cite{dosovitskiy2020image,liu2021swin} as the backbone, $E^{b_k}$, followed by a small task-specific CNN head, $H^{b_k}$. That is, $f^{b_k}(.) = H^{b_k}(E^{b_k}(.))$ holds. While it is beneficial to have a separate model for each sub-task, such an approach can be inefficient due to the high memory and computation requirements during both training and inference, making it impractical for resource-constrained real-world applications. To address this issue, our method applies delta weight, delta temporal activation, and delta task activation pruning techniques to the $E^{b_k}$ for all existing sub-tasks as described in Sec. \ref{weigh_sharing_desc} and \ref{act_sharing_desc}. The CNN head $H^{b_k}$ for each sub-task is trained without exploiting delta weight/activation sparsity.
%where $f^{b_k}(,;w^{b_k})$ is a neural network for task $b_k$, $\mathcal{C}^{b_k}(.,.)$ is a task-specific loss, and $\mathcal{R}(.)$ is an optional regularizer with hyperparameter $\lambda$. To develop the $f^{b_k}$, we use the vision transformer encoder \cite{dosovitskiy2020image,liu2021swin} $E_{b_k}$ as the backbone, followed by a small task-specific CNN head $H_{b_k}$. However, learning independent parameters and storing the finetuned model for each sub-task is immensely inefficient and consumes massive memory storage. Moreover, exhibiting a separate model for each task during the inference requires enormous amounts of computation consumption, especially for vision transformer networks, which is not feasible for real-world applications on mobile devices. Hence, we propose our method to reduce the memory requirements and computation consumption across task and temporal domains during inference by applying delta weight, delta temporal activation, and delta task activation pruning techniques to the $E_{b_k}$ for all the existing sub-tasks as described in Sec \ref{weigh_sharing_desc} and \ref{act_sharing_desc}. 
\vspace{-4mm}
\paragraph{Transformer backbone $E^{b_k}$: }
The transformer model consists of two fundamental computation elements in each layer: a multi-head self-attention module and a feedforward network (FFN). The multi-head self-attention module calculates the inter-dependencies between different positions in the input data through the following equations:
\vspace{-1mm}
\begin{gather}
%\setlength{\abovedisplayskip}{0pt}
\scalebox{0.91}{$
   \begin{split}
    MultiHead(Q,K,V)=Concat(head_1,\ldots,head_h)W_O\\
    \text{where} \: \: \: head_i=Attention(XW_Q, XW_K, XW_V), \: \: \: \: \: \: \: \:  \: \: \: \: \,\\
    Attention(Q,K,V)=softmax(QK^\top/ \sqrt D)V,  \: \: \: \: \: \: \: \: \: \: \: \: \quad \,
   \end{split}$}
   %\setlength{\belowdisplayskip}{-1pt}
\end{gather}
where $X\in R^{P\times D}$ is the activation input that has $P$ entries of $D$ dimension, $\{W_O, W_Q, W_K, W_V\} \in R^{D\times D}$ are model parameters, and $h$ is the number of heads. An FFN is applied to the multi-head output of the self-attention layers. It contains two linear projections $\{W_{F1}, W_{F2}\}\in R^{D\times D}$, connected by a GELU \cite{hendrycks2016gaussian} non-linearity $\sigma$, and it can be expressed as $FFN(X)=\sigma(XW_{F1})W_{F2}$. Therefore, each transformer block contains $3h + 3$ linear projections, each with a computational complexity of $O(PD^2)$. On the other hand, the calculation of the self-attention module, represented as $Attention(.)$, has a complexity of $O(P^2D)$. As such, reducing the computations for linear projections can be equally important as reducing the calculation of the self-attention module. The objective of our technique is to reduce the complexity of linear projections.
%The vision transformer model contains two main computation types in each layer: multi-head self-attention (MSA) and MLP blocks. For MSA block in layer $\ell$, we can write the computation as:
%\vspace{0pt}
%\begin{gather}
%\setlength{\abovedisplayskip}{0pt}
%\scalebox{0.93}{$
%   \begin{aligned}
%    Q^{\ell+1}=X^{\ell}.W^{\ell}_Q,\ \:  \: \: K^{\ell+1}=X^{\ell}.W^{\ell}_K,\ \: \: \: V^{\ell+1}=X^{\ell}.W^{\ell}_V \\ 
%    X_A^{\ell+1}=softmax(Q^{\ell+1}.K^{{\ell+1}^\top}/ \sqrt(d)).V^{\ell+1}
%   \end{aligned}$}
   %\setlength{\belowdisplayskip}{-1pt}
%\end{gather}
%where $X^{\ell}\in R^{(P\times D)}$ is the activation input. Also, the MLP block contains two linear projections, connected by a GELU non-linearity, which can be formulated as: $X_{M_i}^{\ell+1}=X_{M_i}^{\ell}.W^{\ell}_{M_i}$, $i=\{0,1\}$. Thus, there are five linear projections in each ViT block, where each has a computation complexity of $\small{O(PD^2)}$, while $\small{X_A^{\ell+1}}$ calculation has a complexity of $\small{O(P^2D)}$. Hence, saving the computations for linear projection layers can be as crucial as $\small{X_A^{\ell+1}}$ calculation, especially when $P\leq D$, which is the main focus of our paper. 
\raggedbottom
\vspace{-4mm}
\paragraph{Framework: } For simplicity, we describe the proposed activation reuse method for an arbitrary linear projection of a vision transformer block. Given the input activation $X^{b_1,t}_{in}$ and weight matrix $W^{b_1}$ for the base task $b_1$ at time $t$, the model first generates the outputs without activation reuse such that $X^{b_1,t}_{out}=X^{b_1,t}_{in}\, W^{b_1}$. For sub-task $b_k$, the model first learns a sparse task-specific delta weight matrix $\delta W^{b_k}$ and forms the final task-specific weight as $W^{b_k}=W^{b_1}+\delta W^{b_k}$, reusing the weights from the main task. This approach reduces the storage requirements of each sub-task by only storing the sparse $\delta W^{b_k}$ instead of the dense matrix $W^{b_k}$. However, the activation computation for $b_k$ involves $X^{b_k,t}_{out}=X^{b_k,t}_{in} \, (W^{b_1}+\delta W^{b_k})$, which has the same (or higher due to weight additions) complexity as the base task. To reduce the complexity, the computation is reformulated by introducing a delta task activation matrix, $\delta X^{b_k,t}_{in}=X^{b_k,t}_{in} -X^{b_1,t}_{in}$ with a goal to remove inter-task redundancies by making $\delta X^{b_k,t}_{in}$ sparse. This allows for activation sharing from the base task $b_1$ to sub-task $b_k$, resulting in a more efficient calculation, which can be written as:
%\vspace{0pt}
\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
\begin{gather}
\label{acti_reuse}
%\setlength{\abovedisplayskip}{0pt}
\scalebox{0.89}{$
   \begin{split}
    X^{b_k,t}_{out}=(X^{b_1,t}_{in} +\delta X^{b_k,t}_{in})\, (W^{b_1}+\delta W^{b_k})\qquad \qquad \qquad \quad \:\\
       =X^{b_1,t}_{in}\, W^{b_1} + \delta X^{b_k,t}_{in} \, (W^{b_1}+\delta W^{b_k})
          +X^{b_1,t}_{in} \, \delta W^{b_k}. 
   \end{split}$}
   %\setlength{\belowdisplayskip}{-1pt}
\end{gather}



%For simplicity, we describe the general activation reuse method for an arbitrary linear projection of the vision transformer block. Given an input activation $X^{b_1,t}_{in}$ and weight matrix $W^{b_1}$ for task $b_1$ at time $t$, our model first generates the outputs for the base task, $b_1$, without using activation reuse such that $X^{b_1,t}_{out}=X^{b_1,t}_{in}\, W^{b_1}$. Then, it learns a sparse task-specific delta weight matrix $\delta W^{b_k}$ for sub-task $b_k$ and formulates the final weight as $W^{b_k}=W^{b_1}+\delta W^{b_k}$. In this way, we only need to store the $\delta W^{b_k}$, which is a sparse matrix, instead of the dense matrix $W^{b_k}$, diminishing the required storage per sub-task. However, the current required computation for $b_k$ equals $X^{b_k,t}_{out}=X^{b_k,t}_{in} \, (W^{b_1}+\delta W^{b_k})$,  which has the same complexity as the base task and is not designed efficiently. As a result, we reformulate the current computation to enable the passing of information from $b_1$ to $b_k$, reducing the computational cost per sub-task. Especially, we introduce the delta activation matrix for sub-task $b_k$ as $\delta X^{b_k,t}_{in}=X^{b_k,t}_{in} -X^{b_1,t}_{in}$ and learn to remove the inter-task redundancies by forcing the $\delta X^{b_k,t}_{in}$ to be sparse. The computation complexity of $b_k$ can be modified and written as:
%Our model first processes layer $\ell$ of the base task, $b_1$, without using activation reuse such that $X^{\ell+1}_{b_1}=X^{\ell}_{b_1} \cdot W^{\ell}_{b_1}$. Then, it learns a sparse task-specific delta weight matrix $\delta W_{b_k}^\ell$ for sub-task $b_k$ and formulates the final weight as $W^{\ell}_{b_k}=W^{\ell}_{b_1}+\delta W^{\ell}_{b_k}$. In this way, we only need to store the $\delta W^{\ell}_{b_k}$, which is a sparse matrix, instead of the dense matrix $W^{\ell}_{b_k}$, diminishing the required storage per sub-task. However, the current required computation for $b_k$ equals $X^{\ell+1}_{b_k}=X^{\ell}_{b_k} \cdot (W^{\ell}_{b_1}+\delta W^{\ell}_{b_k})$, which has the same complexity as the base task and is not designed efficiently. As a result, we reformulate the current computation to enable the passing of information from $b_1$ to $b_k$, reducing the computational cost per sub-task. Especially, we introduce the delta activation matrix for sub-task $b_k$ as $\delta X^{\ell}_{b_k}=X^{\ell}_{b_k} -X^{\ell}_{b_1}$ and learn to remove the inter-task redundancies by forcing the $\delta X^{\ell}_{b_k}$ to be sparse. The computation complexity of $b_k$ can be modified and written as:

As expressed in Eq. (\ref{acti_reuse}), our method borrows the activation computation from the base task $X^{b_1,t}_{in}\, W^{b_1}$ for sub-tasks and executes only sparse matrix-matrix multiplications in Eq. (\ref{acti_reuse}) rather than performing a dense matrix-matrix multiplication $X^{b_k,t}_{in}\, W^{b_k}$. Thus, the new calculation of a sub-task is reduced to $X^{b_k,t}_{Re}=\delta X^{b_k,t}_{in} \, (W^{b_1}+\delta W^{b_k}) +X^{b_1,t}_{in} \, \delta W^{b_k}$, where $\delta W^{b_k}$ and $\delta X^{b_k,t}_{in}$ are sparse matrices. This not only increases the speed of executing the sub-tasks but also reduces the number of required (non-zero) parameters. Additionally, to ensure that the $X^{b_k,t}_{Re}$ remains sparse for the next layer, we set a per-block task-specific threshold $th^{b_k}$ such that the activations whose absolute values are less than the predefined threshold $th^{b_k}$ are set to zero before being passed to the next layer. The final output activation can be written as:
\vspace{-2mm}
\begin{align}
\label{acti_reuse_short}
%\setlength{\abovedisplayskip}{0pt}
\begin{split}
X^{b_k,t}_{out}=X^{b_1,t}_{out}+Q(X^{b_k,t}_{Re}, th^{b_k}), %X^{b_k,t}_{out}=X^{b_1,t}_{out}+X^{b_k,t}_{{Re}_{|X^{b_k,t}_{Re}|> th^{b_k}}}.
\end{split}
\end{align}
where $Q(X, th)$ is an operator to substitute all elements less than $th$ in $X$ with zero.

Eq. (\ref{acti_reuse}) and (\ref{acti_reuse_short}) enable the activation sharing across different task domains (from $b_1$ to $b_k$). The same activation-sharing idea can also be applied to the temporal domain so that activations are shared from time $t$ to time $t+1$ for the same task in $\mathcal{B}$. Given the input activation $X^{b_k,t}_{in}$ and weight matrix $W^{b_k}$ for an arbitrary task $b_k$, we first perform the linear projection at time $t$ to obtain $X^{b_k,t}_{out}=X^{b_k,t}_{in}\,W^{b_k}$. To reduce the complexity for the next time step $t+1$, we introduce a delta temporal activation matrix $\Delta X^{b_k,t+1}_{in}= X^{b_k,t+1}_{in} - X^{b_k,t}_{in}$ and exploit temporal redundancies by making $\Delta X^{b_k,t+1}_{in}$\footnote{We use $\Delta X$ and $\delta X$ for matrix differences across temporal and task domains, respectively.} sparse. Hence, the calculation of task $b_k$ at time $t+1$ can be optimized by borrowing activations from time $t$, as follows:

%\vspace{0pt}
%\vspace{-1mm}
\begin{gather}
\label{acti_reuse_temp}
%\setlength{\abovedisplayskip{10pt}
\scalebox{0.92}{$
   \begin{split}
X^{b_k,t+1}_{out}=X^{b_k,t+1}_{in} \, W^{b_k}=(X^{b_k,t}_{in} +\Delta X^{b_k,t+1}_{in})\, W^{b_k} \quad \, \, \:\\
       =X^{b_k,t}_{in}\, W^{b_k} + \Delta X^{b_k,t+1}_{in} \, W^{b_k}.\\
   \end{split}$}
 %\setlength{\belowdisplayskip}{-1pt}
\end{gather}
Similar to the task domain, we can remove the dense matrix-matrix multiplication $X^{b_k,t+1}_{in} \, W^{b_k}$ by reusing the activations from the previous time step $t$, and only process $\Delta X^{b_k,t+1}_{in} \, W^{b_k}$ which is a sparse matrix-matrix multiplication, as in Eq. (\ref{acti_reuse_temp}).

In the following sections, we will describe our strategy to prune the delta weight, delta task activation, and delta temporal activation to be as sparse as possible. Moreover, we will explain how the task- and temporal-domain activation sharings are combined to minimize the task complexity.
%As Eq. (\ref{acti_reuse}) demonstrates, instead of performing a new dense matrix-matrix multiplication ($X^{\ell}_{b_k} \cdot W^{\ell}_{b_k}$), we can borrow the computation of the base task activation $X^{\ell}_{b_1} \cdot W^{\ell}_{b_1}$, and only calculate $\delta X^{\ell}_{b_k} \cdot (W^{\ell}_{b_1}+\delta W^{\ell}_{b_k})$ and $ X^{\ell}_{b_1} \cdot \delta W^{\ell}_{b_k}$. Since $\delta W^{\ell}_{b_k}$ and $\delta X^{\ell}_{b_k}$ are defined as sparse matrices, this replacement increases the speed of executing the sub-tasks and reduces the required parameters simultaneously. Although the $\delta W^{\ell}_{b_k}$ and $\delta X^{\ell}_{b_k}$ are sparse, the $\delta X^{\ell}_{b_k} \cdot(W^{\ell}_{b_1}+\delta W^{\ell}_{b_k}) + X^{\ell}_{b_1} \cdot \delta W^{\ell}_{b_k}$ might not be a sparse matrix; thus, we define a per-layer threshold $th^\ell_{b_k}$ value and force the delta activation to be zero if its absolute value is less than the defined threshold before passing it to the next layer. The final delta activation for layer $\ell+1$ can be written as follows:

%\vspace{0pt}
%\begin{align}
%\setlength{\abovedisplayskip}{0pt}
%\begin{split}
%{\delta X^{\ell+1}_{b_k}}&=({X^{\ell+1}_{b_k}-X^{\ell+1}_{b_1}})_{|\delta X^{\ell}_{b_k} \cdot W^{\ell}_{b_k} + X^{\ell}_{b_1} \cdot \delta W^{\ell}_{b_k} |> th^\ell_{b_k}}
%\end{split}
%\setlength{\belowdisplayskip}{-2pt}
%\end{align}

%For each sub-task, the activation re-use method is only applied to the encoder $E_{b_k}$ and we compute the task-specific head $H_{b_k}$ without sharing the weights/computations. In the following sections, we describe our proposed algorithm to sparsify $\delta X^{\ell}_{b_k}$ and $\delta W^{\ell}_{b_k}$ using the delta weight and delta activation pruning approaches.
\vspace{-1mm}
\subsection{Learning and pruning delta weight}
\vspace{-1mm}
\label{weigh_sharing_desc}
To simplify, we consider the collection of all linear layers in a transformer block as a set, denoted by $\Phi=\{W_Q^{1}, W_K^{1}, W_V^{1},\ldots, W_Q^{h}, W_K^{h}, W_V^{h},W_O,W_{F1},W_{F2}\}$. Given the weights of the sub-task $b_k$ and the base task $b_1$, we aim to learn a sparse delta weight matrix that can reparameterize the task-specific model as $\Phi^{b_k}=\Phi^{b_1}+\delta \Phi^{b_k}$. In this reparameterization, the weight of the base task $\Phi^{b_1}$ remains fixed during the fine-tuning process, only updating $\delta \Phi^{b_k}$ for the target sub-task. To promote sparsity in $\delta \Phi^{b_k}$, we follow the approach of Diff-pruning \cite{guo2020parameter} and apply $\ell_0$ regularization to $\delta \Phi^{b_k}$. Hence, the optimization loss function in Eq. (\ref{finetune_loss}) is modified to:
%\vspace{0pt}
%\vspace{-6mm}
\begin{gather}
\label{weight_loss}
\scalebox{0.94}{$
   \begin{split}
%\setlength{\abovedisplayskip}{-0.5pt} 
       \min_{\delta \Phi^{b_k}} \frac{1}{N} \sum_{n=1}^{N} \mathcal{C}^{b_k
}(f^{b_k}(I_{n};\Phi^{b_1}+\delta \Phi^{b_k}),y_{n})+ \mathcal{R}_w(\delta \Phi^{b_k}),
\end{split}$}
       %\setlength{\belowdisplayskip}{-10pt}
\end{gather}
where $\mathcal{R}_w(\delta \Phi^{b_k})$ is defined such that:
%\vspace{-2mm}
\setlength{\abovedisplayskip}{4pt}
\setlength{\belowdisplayskip}{1pt}
\begin{gather}
\scalebox{0.95}{$
   \begin{split}
%\setlength{\abovedisplayskip}{0.5pt} 
       \mathcal{R}_w(\delta \Phi^{b_k})=\lambda_w \|vec(\delta \Phi^{b_k})\|_0 =\lambda_w \sum_{j} \mathbb{1}\{ \delta \Phi_{j}^{b_k}\neq 0\},
\end{split}$}
\setlength{\belowdisplayshortskip}{0pt}
\end{gather}
where $\delta \Phi_{j}^{b_k}$ is the $j^{th}$ element of $\delta \Phi^{b_k}$. Since this regularization term is non-differentiable, we adopt a gradient-based learning approach that uses a relaxed binary mask matrix as described in \cite{guo2020parameter,louizos2017learning}. This involves defining a binary mask $M^{b_k}$ for sub-task $b_k$ and relaxing it into a continuous space using a stretched Hard-Concrete distribution \cite{wang2019structured,maddisonconcrete}, allowing a differentiable gradient path. The resulting mask is then element-wise multiplied with the dense delta weight matrix $\delta \Phi^{b_k}$ to produce a sparsified version.
%Given the weights of the sub-task $b_k$ and the base task $b_1$, the goal is to learn a sparse matrix for layer $\ell$, $\delta W^{\ell}_{b_k}$, such that $W^{\ell}_{b_k}=W^{\ell}_{b_1}+\delta W^{\ell}_{b_k}$ holds. We follow the same approach as Diff pruning \cite{guo2020parameter}, and apply $\ell_0$ regularization to the $\delta W^{\ell}_{b_k}$, and modify the optimization loss function from Eq. (\ref{finetune_loss}) to:
%\vspace{0pt}
%\begin{equation}
%\label{weight_loss}
%\setlength{\abovedisplayskip}{0pt} 
%       \min_{\delta W_{b_k}} \frac{1}{N} \sum_{n=1}^{N} \mathcal{C}(f_{b_k}(I_{(n)};W_{b_1}+\delta W_{b_k}),y_{(n)})+ \mathcal{R}_w(\delta W_{b_k})
       %\setlength{\belowdisplayskip}{-1pt}
%\end{equation}
%where $\mathcal{R}(\delta W_{b_k})$ is defined as: 
%\vspace{0pt}
%\begin{equation}
%\setlength{\abovedisplayskip}{0pt} 
%       \mathcal{R}_w(\delta W_{b_k})=\lambda_w \sum_{\ell=1}^{L} \|\delta W^\ell_{b_k}\|_0 =\lambda_w \sum_{\ell=1}^{L} \mathbb{1}\{ \delta W^\ell_{b_k}\neq 0\}
       %\setlength{\belowdisplayskip}{0pt}
%\end{equation}
%where $\lambda_w$ is the regularization coefficient, and $L$ is the total number of layers. However, this regularizer is challenging to optimize as it is non-differentiable. Therefore, similar to \cite{guo2020parameter,louizos2017learning}, we utilize the gradient-based learning using a relaxed mask matrix approach. Here, we determine a binary mask matrix $M_{b_k}$ for sub-task $b_k$, and attempt to approximate the $\ell_0$ by relaxing the binary mask into continuous space with a stretched Hard-Concrete distribution \cite{wang2019structured}. Then, we multiply the $M_{b_k}$ with dense $\delta W_{b_k}$ to generate a sparse delta weight matrix.
\tablesota
\tabelablationnpascal
\vspace{-1mm}
\subsection{Pruning delta task and temporal activations}
\vspace{-1mm}
\label{act_sharing_desc}
\textbf{Delta task and temporal activation:} Similarly, we define $\mathcal{X}$ as a set of the activation inputs of all linear layers in a transformer block. Hence, the activation input difference between the base task $b_1$ and the sub-task $b_k$ at time $t$ is represented by $\delta \mathcal{X}^{b_k,t}$. Our goal is to prune these activation differences to minimize inter-task redundancies, as discussed in Sec. \ref{framework}. To encourage the sparsity in $\delta \mathcal{X}^{b_k,t}$, we fix the position of non-zero elements of the delta weight $\delta \Phi^{b_k}$, and then fine-tune non-zero delta weights by applying regularization to the activation differences. As $\delta \mathcal{X}^{b_k,t}$ depends on both the input image and task-specific weight matrix, it is challenging to follow the same approach used in delta weight pruning and learn a fixed binary mask which applies to all inputs. Thus, as an alternative approach, we use $\ell_1$ regularization (also known as Lasso), which utilizes a Laplacian-like distribution to increase the amounts of small values. The delta task activation regularization $\mathcal{R}_{a1}$ with a coefficient of $\lambda_{a1}$ is defined as follows:
\vspace{0mm}
%\setlength{\belowdisplayskip}{-2pt}
\begin{gather}
\scalebox{0.95}{$
   \begin{split}
%\setlength{\abovedisplayskip}{0.5pt} 
       \mathcal{R}_{a1}(\delta \mathcal{X}^{b_k,t})=\lambda_{a1} \|vec(\delta \mathcal{X}^{b_k,t})\|_1 =\lambda_{a1} \sum_{j} |\delta \mathcal{X}^{b_k,t}_j |,
\end{split}$}
\end{gather}
where $\delta \mathcal{X}^{b_k,t}_j$ is $j^{th}$ element of $\delta \mathcal{X}^{b_k,t}$. 

The same technique extends to the temporal redundancy pruning (Sec. \ref{framework}) to sparsify the delta temporal activation within the same (sub-)task. Consider $\Delta \mathcal{X}^{b_k,t+\tau}$ as the activation input difference between time $t$ and $t+\tau$ for task $b_k$. To encourage sparsity in $\Delta \mathcal{X}^{b_k,t+\tau}$, we define the delta temporal activation regularization $\mathcal{R}_{a2}$ with a coefficient of $\lambda_{a2}$ such that $\mathcal{R}_{a2}(\Delta \mathcal{X}^{b_k,t+\tau})=\lambda_{a2} \|vec(\Delta \mathcal{X}^{b_k,t+\tau})\|_1$. We apply temporal regularization to $\tau \in  [-2,2]$ and reformulate the optimization problem accordingly:
%\begin{gather}
%\scalebox{0.95}{$
%   \begin{split}
%\setlength{\abovedisplayskip}{0.5pt} 
%       \mathcal{R}_{a2}(\Delta \mathcal{X}^{b_k,t+\tau})=\lambda_{a2} \|vec(\Delta \mathcal{X}^{b_k,t+\tau})\|_1
%\end{split}$}
      % \setlength{\belowdisplayskip}{0pt}
%\end{gather}
%\vspace{-3mm}
\begin{gather}
\scalebox{0.95}{$
   \begin{split}
%\setlength{\abovedisplayskip}{0.5pt} 
       \min_{\delta \Phi^{b_k}} \frac{1}{NT}\sum_{t=1}^{T} \sum_{n=1}^{N} \mathcal{C}^{b_k
}(f^{b_k}(I_{n}^t;\Phi^{b_1}+M^{b_k}\,\delta \Phi^{b_k}),y_{n}^t)\\
+\mathcal{R}_{a1}(\delta \mathcal{X}^{b_k,t})+\sum_{\tau=-2}^2 \mathcal{R}_{a2}(\Delta \mathcal{X}^{b_k,t+\tau}).
\end{split}$}
       %\setlength{\belowdisplayskip}{0pt}
\end{gather}

%\textbf{Delta activation for task domain:}
%As mentioned in Sec. \ref{framework}, the goal is to sparsify the activation difference between the base task $b_1$ and the sub-task $b_k$. Hence, after learning the binary mask $M_{b_k}$, we apply regularization to the activation difference $\delta X_{b_k}$ for each layer. Since $\delta X_{b_k}$ depends on both the input image and $W_{b_k}$, it becomes extremely laborious to follow the same approach as delta weight pruning and learn a fixed binary mask for the delta activation, which applies to all the inputs. To prevent this issue, $\ell_1$ regularization (known as Lasso) can be an alternative choice, which utilizes a Laplacian-like distribution to increase the amounts of small values. Thus, we finetune the learned $\delta W_{b_k}$ and apply $\ell_1$ regularization to delta activation to improve the sparsity ratio of $\delta X_{b_k}$. The delta activation regularization $\mathcal{R}_{a1}$ with a coefficient of $\lambda_{a1}$ is defined as follows:
%\vspace{0pt}
%\begin{equation}
%\label{reg_task}
%\setlength{\abovedisplayskip}{0pt} 
%       \mathcal{R}_{a1}=\lambda_{a1} \sum_{\ell=1}^{L} \|\delta X^{\ell}_{b_k}\|
       %\setlength{\belowdisplayskip}{-1pt}
%\end{equation}


%\textbf{Delta activation for temporal domain:}Since there is usually a slight change between two consecutive frames, we can extend the activation reuse idea to video frame sequences and apply a similar approach to the delta temporal activation. Suppose $X^{\ell, \;t_j}_{b_k}$ is the activation of layer $l$ for task $b_k$ at time $t_j$. Hence, if we determine $\delta X^{\ell, \;t_{j+1}}_{b_k}=X^{\ell, \;t_{j+1}}_{b_k}-X^{\ell, \;t_j}_{b_k}$ as the activation difference between time $t_j$ and $t_{j+1}$, the calculation of $X^{\ell+1, \;t_{j+1}}_{b_k}$ can be written as: 

%\vspace{-2pt}
%\begin{align}
%\label{acti_reuse_temp}
%\setlength{\abovedisplayskip}{0pt}
%\begin{split}
%        {X^{\ell+1, \;t_{j+1}}_{b_k}}&=X^{\ell, \;t_{j+1}}_{b_k} \cdot W^{\ell}_{b_k}\\
%        &=(X^{\ell, \;t_{j}}_{b_k} + \delta X^{\ell, \;t_{j+1}}_{b_k}) \cdot W^{\ell}_{b_k}\\
%        &=X^{\ell, \;t_{j}}_{b_k} \cdot W^{\ell}_{b_k} + \delta X^{\ell, \;t_{j+1}}_{b_k} \cdot W^{\ell}_{b_k}\\
%        &=X^{\ell+1, \;t_{j}}_{b_k} + \delta X^{\ell, \;t_{j+1}}_{b_k} \cdot W^{\ell}_{b_k}
%\end{split}
%\setlength{\belowdisplayskip}{-1pt}
%\end{align}

%As Eq. (\ref{acti_reuse_temp}) displays, we can replace the dense matrix-matrix multiplication by borrowing the computation of the activation from the previous frame $X^{\ell+1, \;t_{j}}_{b_k}$, and only performing the $\delta X^{\ell, \;t_{j+1}}_{b_k} \cdot W^{\ell}_{b_k}$. Similar to the task domain, we aim to enhance the sparsity ratio of $\delta X^{\ell, \;t_{j+1}}_{b_k}$ to increase the temporal speed. Thus, we apply $\ell_1$ regularization to the $\delta X^{\ell, \;t_{j+1}}_{b_k}$, which can be described as:

%\vspace{0pt}
%\begin{equation}
%\label{reg_task}
%\setlength{\abovedisplayskip}{0pt} 
%       \mathcal{R}_{a2}=\lambda_{a2} \sum_{\tau=-2}^{2} \|\delta X^{\ell, \;t_{j+\tau}}_{b_k}\|
       %\setlength{\belowdisplayskip}{-1pt}
%\end{equation}

%After learning the weight binary mask $M_{b_k}$, we finetune the $\delta W_{b_k}$ and modify the Eq. (\ref{weight_loss}) to train both domains simultaneously, as following:
%\vspace{0pt}
%\begin{align}
%\setlength{\abovedisplayskip}{0pt}
%\begin{split}
%        \min_{\delta W_{b_k}} \frac{1}{N} \sum_{n=1}^{N} \mathcal{C}(f_{b_k}(I_{(n)};W_{b_1}+M_{b_k}. \delta W_{b_k}),y_{(n)})\\
%       + \mathcal{R}_{a1}(\delta W_{b_k})+ \mathcal{R}_{a2}(\delta W_{b_k})
%\end{split}
%\setlength{\belowdisplayskip}{-1pt}
%\end{align}

\textbf{Activation re-use combination from both domains:}
\label{activation-combination}
To efficiently combine activations from both the task and temporal domains, we compare the computational complexities of two approaches per layer. While the base task can only reuse activations across the temporal domain, sub-tasks can access activations from both sources: using either $\delta X^{b_k,t}_{in}$ as in Eq. (\ref{acti_reuse}) or using $\Delta X^{b_k,t+1}_{in}$ as in Eq.(\ref{acti_reuse_temp}). Suppose the (average) density (ratio of non-zero values) of the delta weight, delta task activation, and delta temporal activation for layer $l$ is denoted by $S_w^l$, $S_{a1}^l$, and $S_{a2}^l$, respectively. Then, Eq. (\ref{acti_reuse}) requires $\small{(S_w^l+S_{a1}^l)PD^2}$ multiplications, while Eq. (\ref{acti_reuse_temp}) requires $\small{(S_{a2}^l)PD^2}$ multiplications. Thus, if $S_w^l+S_{a1}^l < S_{a2}^l$, it is more efficient to reuse activations across the task domain. Conversely, if $S_{a2}^l \leq S_w^l+S_{a1}^l$, it is more reasonable to reuse activations from the previous frame of the same (sub-)task. We observed that the first few layers tend to be more sparse in the task domain, while the remaining layers are more sparse in the temporal domain. After evaluating $S_w^l$, $S_{a1}^l$, and $S_{a2}^l$ for all layers in each sub-task $b_k$, we determine the layer boundary $l^{b_k}$ so that the task domain activation reuse is performed for layers $l \leq l^{b_k}$, and temporal domain activation reuse is utilized for the remaining layers.
%Although the base task can reuse the activation only across the temporal domain, sub-tasks can get access to two sources of the activations (Eq. (\ref{acti_reuse}) and Eq.(\ref{acti_reuse_temp})). To determine the most efficient way to combine activations from both task and temporal domains, we compare the computational complexities of the two approaches per layer. Consider the sparsity ratios of layer $l$ for the delta weight, delta task activation, and delta temporal activation are represented as $S_w^l\%$, $S_{a1}^l\%$, and $S_{a2}^l\%$, respectively. The Eq. (\ref{acti_reuse}) has  $\small{(S_w^l+S_{a1}^l)PD^2}$ number of multipications, while Eq. (\ref{acti_reuse_temp}) has $\small{(S_{a2}^l)PD^2}$. Thus, if $S_w^l+S_{a1}^l < S_{a2}^l$, it is more efficient to perform computation reuse across the task domain. Conversely, if $S_{a2}^l \leq S_w^l+S_{a1}^l$, it is more reasonable to use the activation from the previous frame. According to our observation, first few layers are sparse in the task domain and the remaining layers are more sparse in the temporal domain. Hence, for each sub-task $b_k$, a fixed layer $l^{b_k}$ is selected, where the activation reuse from the task domain is performed for $l \leq l^{b_k}$ and the activation from the previous frame is utilized for the remaining layers.
%After increasing the sparsity ratios for both task and temporal domains, we have to decide how to combine the activations from both domains. For the base task $b_1$, we only pass the computation from the previous time; however, for the sub-tasks, we can reuse the calculation of the activation from either Eq. (\ref{acti_reuse}) or Eq. (\ref{acti_reuse_temp}). Suppose the sparsity ratios for the delta weight, delta task activation, and delta temporal activation for layer $\ell$ are defined as $S_w^\ell\%$, $S_{a1}^\ell\%$, and $S_{a2}^\ell\%$, respectively. Thus, the required complexity computation of Eq. (\ref{acti_reuse}) equals $(S_w^\ell+S_{a1}^\ell)\small{O(P^2D)}$, while Eq. (\ref{acti_reuse_temp}) demands the complexity computation of $(S_{a2}^\ell)\small{O(P^2D)}$. Therefore, when $S_w^\ell+S_{a1}^\ell \leq S_{a2}^\ell$, it is more efficient to apply computation reuse across the task domain. On the other hand, when $S_{a2}^\ell \leq S_w^\ell+S_{a1}^\ell$, it is more reasonable to utilize the computation from the previous frame. Hence, based on the values of $S_w^\ell\%$, $S_{a1}^\ell\%$, and $S_{a2}^\ell\%$ for each sub-task $b_k$, a fixed layer $\ell^{T}_{b_k}$ is selected, in which we perform the activation reuse from the task domain for $\ell \leq \ell^{a1}_{b_k}$, and utilize the activation from previous frame for the remaining layers.
%maybe one block behind for the main task.we have the activation for the base task from the Memory at the same time, and also we need to save the memory from the previous frame
\vspace{-2mm}
\section{Experiments}
\vspace{-2mm}
\textbf{Dataset:} We evaluate our algorithm on two popular scene understanding datasets, \textbf{NYUD-v2} \cite{silberman2012indoor} and \textbf{PASCAL-Context} \cite{chen2014detect}. NYUD-v2 contains 1,449 indoor scene images with annotations for semantic segmentation (40 classes), depth estimation, surface normal estimation, and edge detection tasks, including 795 images for training and 654 for testing. PASCAL-Context covers indoor and outdoor scenes and comprises 4,998 training and 5,105 testing images, providing the labels for human parsing, semantic segmentation (21 classes), saliency estimation, edge detection, and surface normal estimation. For video input, we limit our analysis to the NYUD-v2 dataset, as PASCAL-Context does not provide images for different time frames. 

\textbf{Evaluation Metrics:} As in InvPT \cite{ye2022inverted}, semantic segmentation and human parsing are evaluated with mean Intersection over Union (mIoU), surface normal estimation with mean error (mErr), depth estimation with root mean square error (RMSE) in millimeter, edge detection with optimal-dataset-scale F-measure (odsF), and saliency detection with maximum $F_1$ score (maxF). 

\textbf{Training Details:} We perform our experiments using the ViT-B transformer \cite{dosovitskiy2020image} pre-trained on ImageNet-22K \cite{deng2009imagenet} as the backbone, with a patch size of $16\times16$ pixels. %In addition, we perform an ablation study using Swin-Base \cite{liu2021swin}. 
The NYUD-v2 dataset is trained with a batch size of $64$ on 2 NVIDIA A40 GPUs in a distributed manner. The AdamW optimizer \cite{loshchilov2017decoupled} is utilized, with a learning rate of $1\times 10^{-4}$ and a weight decay rate of $1\times 10^{-6}$. For the PASCAL-Context dataset, a batch size of $6$, a learning rate of $5\times 10^{-5}$, and a weight decay rate of $1\times 10^{-6}$ are used. Both datasets are trained using a polynomial learning rate scheduler \cite{zeiler2012adadelta}.
\figvis
\figwholepasfinal
%\tabletmpSOTA
\raggedbottom

Our approach considers `semantic segmentation' as the base task and all other tasks as sub-tasks. To train these tasks, we first train the base task and store the weights and intermediate activations as the base weights and activations. Then, we follow a three-step training procedure for the sub-tasks. First, the sparse delta weight matrices are learned for each sub-task with $\ell_0$ regularization on delta weights. Then, the model is fine-tuned for a few epochs, and the learned delta weight is updated to improve performance. Finally, the $\ell_1$ regularization is applied to the difference of intermediate activations for each batch, and the non-zero delta weights are updated to balance the performance and delta activation sparsity. For video inputs, the $\ell_1$ regularization is applied to both delta task and temporal activations.
%In our implementation, we consider the `semantic segmentation' as the base task and the remaining tasks as the sub-tasks. For training the tasks, we first train the base task and store all the weights and intermediate activations as the base weights and activations. Then, we employ a three-step training scheme to generate the weight and activation differences for the sub-tasks. In the first stage, we only apply $\ell_0$ regularization on delta weights for each sub-tasks to learn the sparse delta weight matrices. In the next stage of training, we finetune the model for a few epochs and only update the learned delta weight to achieve the desired performance. In the final training step, for each input batch, the model obtains the base activation, applies the $\ell_1$ regularization to the difference of the intermediate activation, and only updates the non-zero delta weight to balance the performance and delta activation sparsity. If we also want to consider the temporal domain, the $\ell_1$ regularization is applied to delta task and temporal activations. 
\vspace{-1mm}
\subsection{Single Image Evaluation For Multi-task}
\vspace{-1mm}

\textbf{Model Baselines and Variants:} We define the following baseline and model variants for the evaluation: (i) \textbf{`Multi-task learning (MTL)'} represents a state-of-the-art (SOTA) baseline multi-task approach that comprises a shared encoder and multiple task-specific decoders which are jointly optimized. The current SOTA MTL baseline, InvPT \cite{ye2022inverted}, uses a transformer as the encoder, while others typically employ a CNN as the backbone. (ii) \textbf{`Single-task learning (ST)'} has a common transformer backbone plus a task-specific small CNN head model structure for each task which is independently trained for task-specific parameters (for both backbone and head models) without using the delta weight and delta activation pruning methods. (iii) \textbf{`Ours + weight sharing'} is similar to the ST model but adds delta weight pruning to share transformer weights between the base and sub-tasks. (iv) \textbf{`Ours + weight + act sharing'} indicates our proposed method that also utilizes transformer activation sharing in the task domain.


\textbf{Qualitative results:} Figure \ref{fig:fig_pas_vis} presents sample visualizations from the proposed model for the PASCAL-Context dataset. We compare these results with the current SOTA InvPT \cite{ye2022inverted} to demonstrate the superiority of our model. The visual comparison reveals that our model produces more accurate predictions compared to InvPT, particularly for semantic segmentation, human parsing, and edge detection tasks. Additional visualization results can be found in the supplemental material.


%\tabletmpSOTAwhole
\tabletmpSOTAwholeNEW

\textbf{Quantitative results:} Evaluation results for PASCAL-Context dataset and NYUD-v2 dataset are summarized in Table \ref{PAS_SOTA} ($\emph{left}$) and Table \ref{PAS_SOTA} ($\emph{right}$), respectively. The tables illustrate that our method outperforms prior approaches, including InvPT \cite{ye2022inverted}. We compare the FLOPs, number of parameters, and performance of each task against the `ST' model and the SOTA transformer-based MTL method InvPT in Table \ref{table:share_pasca} (a) and (b). Notice that our base task (semantic segmentation) performance is similar to that of the `ST' method since it does not use weight or activation sharing. For the PASCAL-Context dataset, our proposed model (`weight + act sharing') outperforms InvPT while reducing the FLOPs and parameters by \bm{$49.44\%$} and \bm{$74.0\%$}, respectively, compared to the `ST' model. Table \ref{table:share_pasca} (a) specifies the FLOPs required for each base and sub-task, indicating that adding a new sub-task requires only \bm{$37.6\%$} FLOPs of the `ST' model. For the NYUD-v2 dataset, we first evaluate the model with a single image without enabling temporal activation reuse to isolate the gain of the task activation reuse strategy. Table \ref{table:share_pasca} (b) shows that our method reduces the FLOPs and parameters by \bm{$40.5\%$} and \bm{$66.6\%$} compared to the `ST' model while achieving comparable/better results than the InvPT baseline. %Furthermore, adding a new sub-task requires only \bm{$46.7\%$} FLOPs of the `ST' model. These results demonstrate the potential for further FLOPs reduction as the number of tasks increases.
Figure \ref{fig:all_sparsity} shows the overall delta weight and delta task activation sparsity for all sub-tasks in the Pascal-Context dataset ($\emph{left}$) and the NYUD-v2 dataset ($\emph{right}$). 
It reveals that human parsing and edge detection are more closely related to the base task, hence their sparsities are higher than other sub-tasks.


%\textbf{Quantitative results:} We report the performance of the proposed method on the PASCAL-Context dataset in Table \ref{PAS_SOTA} ($\emph{left}$) and on the NYUD-v2 in Table \ref{PAS_SOTA} ($\emph{right}$), respectively. Table \ref{PAS_SOTA} demonstrates that our proposed method significantly outperforms the previous state-of-the-art. Moreover, we investigate the effectiveness of the proposed sharing algorithm, and compare the FLOPs, number of parameters, and performance of each task against the SOTA transformer-based method ('InvPT') and the `ST' model. Table \ref{table:share_pasca}. (a) demonstrates the task performances for the PASCAL-Context dataset. Since we employ the semantic segmentation as the base task and it does not include the `weight+act sharing', the performance of this task is similar to the `ST' model. It can be observed that our proposed model (`weight+act sharing') can outperform the `InvPT' baseline, while reducing the FLOPs and parameters for \bm{$49.45\%$} and \bm{$74.0\%$} compared to the `ST' model, respectively. Moreover, we display the required FLOPs per base and sub-task in \ref{table:share_pasca}. (a), and it can be concluded that adding a new sub-task needs only \bm{$37.6\%$} FLOPs of the `ST' model. Similarly, Table \ref{table:share_pasca}. (b) displays the task performances for the NYUD-v2 dataset. For this dataset, we train the model using the $\ell_1$ regularization for both task and temporal domains, but we first evaluate the model for single input without using temporal activation reuse to explore the effectiveness of task activation reuse only. It can be noticed that our method reduces the FLOPs and parameters for \bm{$40.5\%$} and \bm{$66.6\%$} compared to the `ST' model, respectively, while achieving comparable results to the `InvPT' baseline. Also, adding a new sub-task requires only $46.7\%$ FLOPs of `ST' model. Therefore, our method can reduce the FLOPs even more when the number of tasks is high.



\textbf{Ablation study:} We explore the impact of $\ell_0$ and $\ell_1$ regularization coefficients on the performance, computation, and memory storage reduction for each task in both PASCAL-Context and NYUD-v2 datasets. To determine the optimal values of $\lambda_w$ and $\lambda_{a1}$, we conduct a comprehensive hyperparameter search, sweeping $\lambda_w$ in the range of $[1\times 10^{-8},10\times 10^{-8}]$ and $\lambda_{a1}$ in the range of $[1\times 10^{-10},10\times 10^{-10}]$. Figure \ref{fig:fig_abl_task} shows the performance of the human parsing task with different values of $\lambda_w$ and $\lambda_{a1}$. As expected, increasing $\lambda_w$ saves more parameters but reduces accuracy by $0.30\%$. Similarly, increasing $\lambda_{a1}$ saves more computations at the cost of decreased accuracy by $1.24\%$. For each sub-task, we select the optimal $\lambda_w$ and $\lambda_{a1}$ to balance computation/memory storage and task-specific performance.

In addition, we evaluate the impact of using a different backbone network by replacing the ViT-B transformer model with the Swin-B \cite{liu2021swin} model, both for the InvPT baseline and our method. The results on the Pascal-context dataset are shown in Table \ref{SWINB}. When both approaches use Swin-B, our method (weight \& act sharing) outperforms InvPT baseline, exhibiting significantly reduced FLOPs and parameters (by \bm{$45.73\%$} and \bm{$71.07\%$}) compared to the `ST' model.
%The best values that balance computation/memory storages and task performance are selected.
%\figreg

%In addition to Table \ref{table:share_pasca}, we study the impact of $\ell_0$ and $\ell_1$ regularization coefficients on the performance of each task as well as the computation and memory storage reduction. For each task in both Pascal-context and NYUD-v2 datasets, we conduct an extensive hyperparameter searching for $\lambda_w$ and $\lambda_{a1}$. Specifically, the value of $\lambda_w$ is assigned to be in range of $[1\times e^{-8},10\times e^{-8}]$, and $\lambda_{a1}$ is defined to be in range of $[1\times e^{-10},10\times e^{-10}]$. In Fig. \ref{fig:fig_abl_task}, we demonstrate the behavior of the `human parsing' task for different values of $\lambda_w$ and $\lambda_{a1}$. It can be observed that a higher value of $\lambda_w$ saves more amounts of parameters, but it drops the accuracy performance for $0.30\%$. Similarly, the higher value of $\lambda_{a1}$ can save more computation while it reduces the accuracy for $1.24\%$. We then choose the best values that balance the computation/memory storages and the task performance.



\vspace{-1mm}
\subsection{Video Frame Evaluation}
\vspace{-1mm}
Now, we evaluate the temporal activation-sharing method using the NYUD-v2 dataset. As discussed in Sec \ref{activation-combination}, we first evaluate the sparsity ratios of all layers for a task and then determine the optimal layer boundary for sharing mode switching. Specifically, for task $b_k$, we leverage the task-domain activation sharing for the first $l^{b_k}$ layers and subsequently switch to the temporal-domain activation sharing for the remaining layers. Figure \ref{fig:task-temp-sparse} shows the sparsity comparison between the delta task and temporal activations for depth estimation, surface normal estimation, and edge detection tasks to determine the sharing mode switching boundary $l^{b_k}$.

%To determine the value of $\ell^{b_k}$, we compare the temporal and task domain sparsity and ensure that the condition $S_{a2}^{\ell^{b_k}} \leq S_w^{\ell^{b_k}}+S_{a1}^{\ell^{b_k}}$ is met, where $S$ denotes the sparsity ratio. Figure \ref{fig:task-temp-sparse} illustrates the comparison between the task and temporal activations for `Depth', `Normal', and `Edge'.
%\figspa 

As the frames in the NYUD-v2 dataset are sparsely annotated (e.g., only every 20th frame is annotated), we evaluate the performance of annotated ground truth (GT) frames by considering all possible interval offsets between the start (keyframe) and GT frames within the range of $[0,4]$ and record the averaged performance and FLOPs. To assess the impact of task and temporal activation combinations, we evaluate the following model variants: (i) \textbf{`ST'}, (ii) \textbf{`InvPT'}, (iii) \textbf{`+ Task act'} using only task domain activation reuse for all frames; and (iv) \textbf{`+ Task act + Temporal act'} which is the proposed model using the combination of temporal and task activation sharing strategy. The proposed model employs the same computation as the `+ Task act' model for a keyframe (it appears up to 4 frames earlier than the GT frame) but reduces the computation of base and sub-tasks for non-keyframes. Table \ref{tempPAS_SOTA} compares the performance and FLOPs of these variants. We observe that the proposed approach significantly reduces FLOPs by \bm{$42.3\%$} and \bm{$65.7\%$} compared to `+ Task act' and `ST' models, respectively. %Moreover, Table \ref{tempNYU_SOTA} presents the task performances for different keyframe interval offsets.
\begin{figure}[!h]
\setlength\belowcaptionskip{-1.4\baselineskip}
 \centering
       \includegraphics[width=1\columnwidth]{Figures/temporal_vs_task_4.pdf}
         \caption{The sparsity comparison between delta task and temporal activation for NYUD-v2 dataset.}
    \label{fig:task-temp-sparse}
\end{figure}
%Since frames in the NYUD-v2 dataset are sparsely annotated (\emph{e.g.} 20th frame in a video), we measure the performance of the annotated ground truth frame by considering all the possible interval offsets of $[0,5]$ and record the averaged performance and FLOPs. We define the following model variants to evaluate the impact of the task and temporal activation combinations: (i) \textbf{`ST'} indicates the single training models. (ii) \textbf{`InvPT'} shows the transformer-based SOTA as the reference. (iii) \textbf{`+ Task act'} has the same structure as `ST' and employs only task domain activation reuse for all the frames. (iv) \textbf{`+ Task act + Temporal act'} uses the temporal activation for the base task and combines the activation of both domains for the sub-tasks. This model utilizes the same computation as the `+ Task act' model for the keyframe but reduces the computation of base and sub-tasks for the non-keyframes. Table \ref{table:temp_comp_nyu} demonstrates the performance and FLOPs comparison with the mentioned model variants. It can be observed that `+ Task act + Temporal act' remarkably diminishes the FLOPs by \bm{$42.2\%$} and \bm{$65.6\%$}  compared to the `+ Task act' and 'ST' model, respectively. Furthermore, we report the detail of task performances for different keyframe interval offsets in Table \ref{tempNYU_SOTA}.

%\subsection{single frame evaluation}
%\tablesota
%\tabletmp

\section{Conclusion}
\vspace{-2mm}
This paper presents a novel computation- and parameter-sharing scheme for transformer-based multiple visual tasks that are concurrently performed on the same input. Motivated by recent transfer learning techniques, our scheme reuses the weights and activations of the base task by training sub-tasks with sparse weight and activation differences via $\ell_0$ and $\ell_1$ regularization. As a result, the activations from the base task can be shared with all sub-tasks, reducing both parameter and computation redundancy significantly. Additionally, the proposed scheme is extended to video inputs to further reduce computation redundancy in the temporal domain. Evaluation results confirm that our method attains better/comparable performance with fewer parameters and FLOPs than state-of-the-art multi-task learning methods. 





































{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}