%% 
%% Copyright 2019-2020 Elsevier Ltd
%% 
%% This file is part of the 'CAS Bundle'.
%% --------------------------------------
%% 
%% It may be dfistributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'CAS Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for cas-dc documentclass for 
%% double column output.

%\documentclass[a4paper,fleqn,longmktitle]{cas-dc}
\documentclass[a4paper,fleqn]{cas-dc}

%\usepackage[numbers]{natbib}
\usepackage[authoryear]{natbib}
%\usepackage[authoryear,longnamesfirst]{natbib}

\usepackage{flushend}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[algo2e, ruled,vlined]{algorithm2e}
%\usepackage{cite}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{color, colortbl}
\usepackage{hyperref}

\definecolor{Gray}{gray}{0.9}

\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

%%%Author definitions
\def\tsc#1{\csdef{#1}{\textsc{\lowercase{#1}}\xspace}}
\tsc{WGM}
\tsc{QE}
\tsc{EP}
\tsc{PMS}
\tsc{BEC}
\tsc{DE}
%%%

% Uncomment and use as if needed
%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}[theorem]{Lemma}
%\newdefinition{rmk}{Remark}
%\newproof{pf}{Proof}
%\newproof{pot}{Proof of Theorem \ref{thm}}

\begin{document}
\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}

% Short title
%\shorttitle{Low-latency End-to-End SSGD}
\shorttitle{End-to-End Integration of Speech Separation and Voice Activity\\Detection for Low-Latency Diarization of Telephone Conversations}

% Short author
\shortauthors{Morrone et~al.}

% Main title of the paper
\title [mode = title]{End-to-End Integration of Speech Separation and Voice Activity Detection for Low-Latency Diarization of Telephone Conversations}
%\title [mode = title]{Fully End-to-End Low-latency Separation Guided Diarization for Telephone Conversations}
%\title [mode = title]{Improving Online Speech Separation Guided Diarization Via End-to-End Training}

% Title footnote mark
% eg: \tnotemark[1]
%\tnotemark[1,2]

% Title footnote 1.
% eg: \tnotetext[1]{Title footnote text}
% \tnotetext[<tnote number>]{<tnote text>} 
%\tnotetext[1]{This document is the results of the research
%   project funded by the National Science Foundation.}

%\tnotetext[2]{The second title footnote which is a longer text matter
%   to fill through the whole text width and overflow into
%   another line in the footnotes area of the first page.}


% First author
%
% Options: Use if required
% eg: \author[1,3]{Author Name}[type=editor,
%       style=chinese,
%       auid=000,
%       bioid=1,
%       prefix=Sir,
%       orcid=0000-0000-0000-0000,
%       facebook=<facebook id>,
%       twitter=<twitter id>,
%       linkedin=<linkedin id>,
%       gplus=<gplus id>]
\author[1]{Giovanni Morrone}[type=editor,orcid=0000-0003-2163-1779]

% Corresponding author indication
\cormark[1]

% Footnote of the first author
%\fnmark[1]

% Email id of the first author
\ead{g.morrone@univpm.it}

% URL of the first author
%\ead[url]{www.cvr.cc, cvr@sayahna.org}

%  Credit authorship
%\credit{Conceptualization of this study, Methodology, Software}

% Address/affiliation
% % \affiliation[1]{organization={Università Politecnica delle Marche},
%      city={Ancona},
% %     citysep={}, % Uncomment if no comma needed between city and postcode
% %     state={},
%      country={Italy}}
\address[1]{Università Politecnica delle Marche, Ancona, Italy}

% Second author
\author[1]{Samuele Cornell}

\ead{s.cornell@pm.univpm.it}

% Third author
\author[1]{Luca Serafini}

\ead{l.serafini@univpm.it}

% Fourth author
\author[2]{Enrico Zovato}

\ead{enrico.zovato@pervoice.it}

% \affiliation[2]{organization={PerVoice S.p.A.},
%     city={Trento},
%     % citysep={}, % Uncomment if no comma needed between city and postcode
%     % state={},
%     country={Italy}}
\address[2]{PerVoice S.p.A., Trento, Italy}

% Fifth author
\author[3]{Alessio Brutti}

\ead{brutti@fbk.eu}

% \affiliation[3]{organization={Fondazione Bruno Kessler},
%     city={Trento},
%     % citysep={}, % Uncomment if no comma needed between city and postcode
%     % state={},
%     country={Italy}}
\address[3]{Fondazione Bruno Kessler, Trento, Italy}

\author[1]{Stefano Squartini}

\ead{s.squartini@univpm.it}

% Corresponding author text
\cortext[cor1]{Corresponding author}
%\cortext[cor2]{Principal corresponding author}

% Footnote text
%\fntext[fn1]{This is the first author footnote. but is common to third
%  author as well.}
%\fntext[fn2]{Another author footnote, this is a very long footnote and
%  it should be a really long footnote. But this footnote is not yet
%  sufficiently long enough to make two lines of footnote text.}

% For a title note without a number/mark
%\nonumnote{This note has no numbers. In this work we demonstrate $a_b$
%  the formation Y\_1 of a new type of polariton on the interface
%  between a cuprous oxide slab and a polystyrene micro-sphere placed
%  on the slab.
%  }

% Here goes the abstract
\begin{abstract}
Recent works show that speech separation guided diarization (SSGD) is an increasingly promising direction, mainly thanks to the recent progress in speech separation. 
It performs diarization by first separating the speakers and then applying voice activity detection (VAD) on each separated stream. 
In this work we conduct an in-depth study of SSGD in the conversational telephone speech (CTS) domain, focusing mainly on low-latency streaming diarization applications.
We consider three state-of-the-art speech separation (SSep) algorithms and study their performance both in online and offline scenarios, considering non-causal and causal implementations as well as continuous SSep (CSS) windowed inference.
We compare different SSGD algorithms on two widely used CTS datasets: CALLHOME and Fisher Corpus (Part 1 and 2) and evaluate both separation and diarization performance. 
To improve performance, a novel, causal and computationally efficient leakage removal algorithm is proposed, which significantly decreases false alarms. 
We also explore, for the first time, fully end-to-end SSGD integration between SSep and VAD modules. Crucially, this enables fine-tuning on real-world data for which oracle speakers sources are not available. 
In particular, our best model achieves $8.8$\% DER on CALLHOME, which outperforms the current state-of-the-art end-to-end neural diarization model, despite being trained on an order of magnitude less data and having significantly lower latency, i.e., $0.1$ vs. $1$ seconds.
Finally, we also show that the separated signals can be readily used also for automatic speech recognition, reaching performance close to using oracle sources in some configurations. 


%We propose an end-to-end approach for speech separation guided diarization (SSGD) in telephone conversations. In our previous study, an analysis of the use of online SSGD was carried out. SSGD performs diarization by separating the speakers signals and then applying voice activity detection on each estimated speaker signal. It employs a post-processing algorithm, i.e., leakage removal, that significantly reduces the false alarm errors. 
%In this paper, we extend this analysis by also considering adaptation and end-to-end strategies. Experiments on Fisher Corpus (Part 1 and 2) and CALLHOME demonstrate that the proposed end-to-end approach provides significant improvements over the disjoint SSGD.
%We perform our experiments on two datasets: Fisher Corpus Part 1 and CALLHOME, evaluating both separation and diarization metrics.
%In particular, our best model achieves $8.8$\% DER on CALLHOME, which outperforms the current state-of-the-art end-to-end neural diarization model despite being trained on an order of magnitude less data and having lower latency, i.e., $0.1$ vs. $1$ seconds.
%Finally, we show that the separated signals can be used for automatic speech recognition with accuracy almost on par on that reached with oracle sources.
\end{abstract}

% Use if graphical abstract is present
% \begin{graphicalabstract}
% \includegraphics{figs/grabs.pdf}
% \end{graphicalabstract}

% Research highlights
% \begin{highlights}
% \item Research highlights item 1
% \item Research highlights item 2
% \item Research highlights item 3
% \end{highlights}

% Keywords
% Each keyword is seperated by \sep
\begin{keywords}
online speaker diarization \sep speech separation \sep end-to-end learning \sep overlapped speech \sep conversational telephone speech
\end{keywords}

\maketitle

\section{Introduction}
\label{sec:intro}
Speaker diarization consists in identifying ``who spoke when'' in an input audio, by segmenting it into speaker-attributed regions~\citep{anguera2012speaker,park2022review} that correspond to speakers' utterances. 
It is an essential pre-processing task in many applications, such as lectures, meetings, live captioning, speaker-based indexing, telephone calls, and doctor-patient conversations.

Historically, diarization has relied on clustering-based methods, which have been widely investigated since the 90s, and have represented the de-facto standard approach to diarization for many years. 
Recently, the invention of end-to-end neural diarization (EEND)~\citep{fujita2020end} has shown promising improvements in diarization accuracy, especially in the presence of overlapped speech, which may constitute up to 20\% of total speech in real conversations \citep{watanabe2020chime}. Indeed, classical, clustering-based systems are not able to handle overlapped speech as clustering is usually applied on single-speaker embeddings extracted from short frames. In this case, overlap-aware diarization can be performed using post-processing strategies~\citep{bullock2020overlap, raj2021multi}. %On the contrary, EEND-based models %can learn to detect overlapped speakers in the same segment directly from data by using multi-label classification.
There also exist methods such as target-speaker voice activity detection (VAD)~\citep{Medennikov2020TargetSpeakerVA}, region proposal networks~\citep{huang2020speaker} and speech separation guided diarization (SSGD)~\citep{fang2021deep} that do not belong to these two categories. In particular, SSGD performs diarization by combining speech separation (SSep) and VAD. It is particularly appealing as separated sources could be readily fed in input to an automatic speech recognition (ASR) system (see Section~\ref{subsec:res/wer_eval}).

The majority of the methods above only work offline and thus are not suitable for streaming processing. The extension from offline to online processing is way simpler for EEND. Clustering-based systems consist of a pipeline of several modules (i.e., voice activity detection, speaker embedding extraction, clustering, etc). Each of these modules has to be updated in order to work online. In contrast, end-to-end methods can be easily adapted for online diarization by employing a speaker-tracing buffer (STB) to store previous input-output pairs that can be exploited for online inference~\citep{xue2021edaonline, xue2021online, horiguchi2022online}. An alternative strategy is to simply replace the neural architecture with another one that allows low-latency streaming processing~\citep{han2021bw}. A similar approach, which is the basis for this work, is proposed by \cite{morrone2022low-latency} also for SSGD. %which employs online speech separation and VAD models.

In this preliminary work, SSep and VAD models were trained independently and combined together with no additional training. Despite its simplicity, this approach only reaches a sub-optimal diarization performance. Additionally, a major disadvantage is that the SSep module needs a dataset in which oracle sources for the speakers are available. This could lead to mismatched training-inference conditions as in most real-world scenarios oracle speaker sources are difficult to obtain~\citep{subakan2022real}.
In this paper, we build upon this previous work \citep{morrone2022low-latency} and propose a fully end-to-end integration of the SSep and VAD modules. This approach only requires diarization labels during the fine-tuning stage and thus eliminates the need for oracle speaker target sources. We show that the proposed end-to-end strategy provides significant improvements on two widely used datasets, i.e., Fisher Corpus \citep{cieri2004fisher} and CALLHOME \citep{callhome}. These reflect two different training vs. inference conditions: fully matched and unmatched respectively. 
In particular, our best online model outperforms the current state-of-the-art EEND system on the $2$-speaker subset of the CALLHOME dataset despite having an order of magnitude lower latency, i.e., $0.1$ vs $1$ seconds.
Moreover, we review and extend our previous work \citep{morrone2022low-latency} with additional analysis.
The other contributions with respect to the previous paper are summarized below:
\begin{itemize}
\item We consider an additional more performing SSep model, i.e., DPTNet \citep{dptnet}. 
\item We carry out an analysis of the effect of varying model latency on diarization performance. 
\item Since separated sources are not provided in the CALLHOME dataset, we create a simulated version that enables the adaptation of SSep models that further reduces diarization errors.
\item We thoroughly study the effect of the leakage removal algorithm on both disjoint and end-to-end trained SSGD.
\end{itemize}

% We build upon the SSGD framework and attempt to deal with its limitations. We extend SSGD to online processing by considering the use of causal SSep models and CSS. Both these techniques allow the processing of arbitrarily long recordings that could not fit in memory making SSGD viable in practical applications. Additionally, we introduce an effective, causal leakage removal post-processing algorithm that reduces the SSGD VAD false alarm errors generated by imperfect separation.
% This algorithm has negligible computational overhead.
% We carry out an extensive experimental analysis focusing on conversational telephone speech (CTS). Although the maximum number of speakers is limited to $2$, this scenario is very common in many commercial applications that deal with CTS processing, and, in general, with analysis of conversations between two speakers (e.g., doctor-patient recordings).
% The CTS scenario also allows to compare with most previous works \cite{fujita2019end, fujita2020neural, fujita2020end, horiguchi2020end, kinoshita2021integrating, zeghidour2021dive, xue2021online, han2021bw, xue2021edaonline}, on EEND diarization.
% We experiment with real-world CTS datasets such as Fisher \cite{cieri2004fisher} and CALLHOME \cite{callhome}, comparing several online approaches and the effect of the CSS window size on the diarization accuracy.
% In this preliminary work, we do not consider other datasets (e.g., CHiME \cite{watanabe2020chime}, AMI \cite{carletta2005ami} and DIHARD \cite{Ryant2019}) used in previous work as they do not belong to the CTS domain. %We will consider them in future work.
% % comparing offline and online approaches and studying the effect of the CSS window size.
% Results show that by using just separation and a simple VAD, it is possible to obtain competitive diarization results on CALLHOME with extremely low-latency (i.e., $0.1$ vs $10$ s) and using much less training data (i.e., ${\sim} 900$ vs ${\sim} 10000$ hours) compared to state-of-the-art EEND approaches.
% Our code is made available through the Asteroid toolkit \cite{pariente20}.

In the next subsection we report a brief summary of the diarization state-of-the-art. In Section \ref{sec:sys} we provide a description of the SSGD framework. The experimental setup is shown in Section \ref{sec:exp_setup}. Experiments and results are reported in Section \ref{sec:res}. Finally, we draw the conclusions in Section \ref{sec:conc}.

\subsection{Related Works}
\label{ssec:intro/rel}
The conventional clustering-based diarization approach is typically a cascade of three tasks: voice activity detection, speaker embedding extraction from speech segments and clustering of the embeddings. Previous works constantly improve the overall performance by developing better methods for one or more tasks. Several papers focus on the development of better speaker embeddings extractors \citep{dehak2010front, garcia2017speaker, desplanques2020ecapa, Xiao2021MicrosoftSD, koluguri2022titanet}. In contrast, in \cite{park2019auto}, \cite{singh2021self} and \cite{landini2022bayesian} different clustering algorithms are proposed. Conventional clustering algorithms can only handle correctly single-speaker segments, thus overlapped speech is usually missed out or incorrectly labeled. To deal with overlapped speech, recent works extend the standard pipeline with overlap assignment techniques \citep{bullock2020overlap, raj2021multi, jung21_interspeech}. These methods need accurate overlap detection, which is often hard to train. Furthermore, embedding extractors trained on single-speaker utterances may not be reliable for overlapping segments, resulting in speaker confusion errors~\citep{raj2021multi}. 

Recently, deep learning methods are employed for end-to-end neural diarization (EEND) approaches. A major advantage of EEND is that they are able to deal with overlapped speech without any modification. The first EEND methods perform diarization as a simple multi-speaker voice activity detection (VAD) problem, in which each output represents a different speaker's speech activity. EEND systems can employ different neural architectures, such as bidirectional long-short memory \citep{fujita2019} and self-attention (SA-EEND) \citep{fujita2019end}. EEND-based systems are trained directly to perform diarization using permutation invariant training (PIT) \citep{kolbaek2017multitalker} as the diarization problem is inherently permutation-invariant without any a-priori information. 
With enough training data, end-to-end approaches have been shown to outperform current state-of-the-art clustering-based systems~\citep{Horiguchi2021TheHD}. Contrary to clustering-based approaches, the maximum number of total speakers is fixed in the aforementioned EEND architectures. Additionally, end-to-end systems need to process the entire input signal during inference, resulting in significant memory consumption for long recordings (e.g., >10 minutes). Chunk-wise processing can help but is not viable as it leads to speaker label permutation across chunks due to PIT. \cite{horiguchi2020end} solves the first problem by extending the basic EEND with an auto-regressive encoder-decoder (EEND-EDA) architecture.
The second issue is generally addressed by combining clustering and EEND. In \cite{horiguchi2021end} EEND is exploited to refine the results of a clustering-based algorithm. \cite{bredin2021end} proposes a speaker segmentation model inspired by EEND to perform overlap-aware resegmentation in a conventional diarization pipeline. However, these approaches result in more complex systems in contrast to the simplicity of fully end-to-end architectures. A tighter integration of EEND and clustering, i.e., EEND-vector clustering (EEND-VC), is proposed in \cite{kinoshita21_interspeech, kinoshita2021integrating}. It performs chunk-wise processing to ensure that a given maximum number of active speakers can be present in each chunk (e.g., 2 or 3). The original EEND is modified to output global speaker embeddings that are aggregated across chunks using a constrained clustering algorithm. This method can both deal with an arbitrary number of speakers and solve the inter-chunk speaker permutation problem. An approach similar to EEND-VC, named EEND with global and local attractors (EEND-GLA), is proposed in \cite{horiguchi2021towards} which combines EEND-EDA and unsupervised clustering to deal with cases where the number of speakers appearing during inference is higher than that during training.
Another system \citep{zeghidour2021dive} employs a different architecture that iteratively builds embeddings for each speaker which are exploited to condition a VAD module.

An alternative framework to deal with overlapped speech is continuous speech separation (CSS)~\citep{chen2020continuous, morrone2022conversational}.
CSS extends PIT-based SSep to long recording scenarios, by applying separation in a chunk-wise manner, where each chunk is assumed to contain a fixed number of speakers (usually 2-3). Since the underlying separator is trained via a PIT objective, output permutation consistency between chunks is not guaranteed. CSS solves this problem by performing overlapping inference (i.e., using strides shorter than chunk sizes) and reordering adjacent chunks based on a similarity measure over the portion in which they overlap.  
Several recent works have proposed diarization systems inspired by CSS. In ~\cite{raj2021integration} and \cite{Xiao2021MicrosoftSD} separation is done in windowed segments. In this case, speaker permutation is addressed by applying a diarization method (e.g., clustering-based) across the separated audio streams.
On the other hand, \cite{fang2021deep} proposes speech separation guided diarization (SSGD), where diarization is performed by first separating the input mixture and applying a conventional VAD to detect speech segments in each channel. In \cite{morrone2022low-latency}, we improve the SSGD architecture using a neural-based VAD and a novel post-processing algorithm that removes the channel leakage generated by separation. In addition, SSGD is adapted to allow online inference employing causal SSep and VAD models.

% brief description of online methods
All the aforementioned approaches, with the exception of \cite{morrone2022low-latency}, are not suitable for streaming applications (e.g., live captioning) as they only work offline.
To deal with online processing, \cite{xue2021online} extends the SA-EEND with an STB mechanism. Inference is performed on short chunks and the speaker permutation information is selected from input-output pairs of previous frames and stored in a buffer. The permutation ambiguity is solved using the buffered frames to condition the output of the current chunk. \cite{xue2021edaonline} and \cite{horiguchi2022online} propose similar STB-based extensions for EEND-EDA and EEND-GLA, respectively. Instead, \cite{han2021bw} improves the online EEND-EDA using chunk-level recurrence to process the chunk hidden states making the model complexity linear in time.
\cite{coria2021overlap} designs a different approach that combines the use of EEND with an x-vector extractor and online clustering. The EEND model is used to gate the representation before the x-vector statistical pooling layer, to extract per-speaker embeddings even in overlap regions.

\section{SSGD Framework}
\label{sec:sys}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figs/sepguideddiar.pdf}
\caption{General diagram of the SSGD method.}
\label{fig:cssgd}
\end{figure}

%The SSGD pipeline used in this work is composed of three modules: speech separation, leakage removal and VAD, as shown in Fig. \ref{fig:cssgd}. The input of the system is a single-channel mixed audio stream, denoted as $\mathbf{Y} \in \mathbb{R}^{1 \times T}$, where $T$ is the number of audio samples.
Our SSGD pipeline is shown in Fig. \ref{fig:cssgd} and consists of three modules: speech separation, voice activity detection and leakage removal. The system is fed with a single-channel mixed audio input, denoted $\mathbf{Y} \in \mathbb{R}^{1 \times T}$, where $T$ is the number of audio samples.

\subsection{Speech Separation Module}
\label{subsec:sys/css}
%As said, we consider in our experiments SSGD based on non-causal separation models (as used in \cite{fang2021deep}), causal separation models (such as causal Conv-TasNet \cite{luo2019conv}) and CSS. 
%We consider in our experiments SSGD based on causal separation models (i.e., Conv-TasNet \citep{luo2019conv}, DPRNN \citep{luo2020dual} and DPTNet \citep{dptnet}).
In our experiments we employ causal separation models as SSep modules (i.e., Conv-TasNet \citep{luo2019conv}, DPTNet \citep{dptnet} and DPRNN \citep{luo2020dual}).
A streaming modified version of DPTNet is attained in this work by simply masking the future frames in each self-attention layer in the inter-processing blocks, with the rest being equal to causal DPRNN (DPTNet is identical to DPRNN except for self-attention layers).
%Since the majority of diarization approaches only work offline, we also experiment with non-causal separation models (as used in \cite{fang2021deep}) to carry out a more comprehensive comparison with clustering-based and EEND state-of-the-art systems.

To compare the proposed SSGD with both clustering-based and EEND state-of-the-art offline systems, we also experiment with non-causal SSep models.
Additionally, we consider the use of CSS with non-causal SSep models. In such a configuration, the latency of these models is tied to the CSS window size and thus making offline SSep models suitable for streaming inference.
%We do not consider CSS for causal SSep models since they are already capable of processing the input in a streaming fashion with low latency. 
We do not apply CSS in causal SSep models as they do not require chunk-wise processing to work online.
%Briefly, CSS consists of three stages as shown in Fig.~\ref{fig:cssgd}: framing, separation and stitching. In the framing stage, a windowing operation splits $\mathbf{Y}$ into $I$ overlapped frames $\mathbf{Y}_i \in \mathbb{R}^{1 \times W}, i = 1, \dots, I$, with $I = \lceil \frac{T}{H} \rceil$, where $W$ and $H$ are the window and hop sizes, respectively. %Window size defines the memory footprint and the latency of the system.

CSS is composed of three stages: framing, separation and stitching. In the framing stage, a windowing operation splits $\mathbf{Y}$ into $I$ overlapped frames $\mathbf{Y}_i \in \mathbb{R}^{1 \times W}, i = 1, \dots, I$, with $I = \lceil \frac{T}{H} \rceil$, where $W$ and $H$ are the window and hop sizes, respectively.
%Then, separation is performed independently on each frame $\mathbf{Y}_i$, generating separated output frames $\mathbf{O}_i \in \mathbb{R}^{C \times W}$, where $C$ is the number of output channels. In this work, $C$ is fixed to $2$, meaning that we assume that the maximum number of speakers in each frame is $2$. This is a common assumption made for CSS systems, and is also valid in general for telephone conversations (which is the focus of this work).
Each frame $\mathbf{Y}_i$ is fed to the separator, which generates $C$ separated output frames $\mathbf{O}_i \in \mathbb{R}^{C \times W}$. $C$ is the maximum number of speakers in each frame. In this work, $C$ is fixed to $2$ as it is a common assumption for telephone conversations.  
%To solve the permutation ambiguity between consecutive frame outputs, the stitching module aligns channels of two separation outputs $\mathbf{O}_i$ and $\mathbf{O}_{i+1}$ according to the cross-correlation computed on the overlapped part of consecutive frames. The final output stream $\mathbf{X} \in \mathbb{R}^{C \times T}$ is generated by an overlap-add operation with Hanning window.
The output channels could be misaligned due to permutation-free training. The stitching module solves the permutation ambiguity by aligning the channels of two consecutive separated outputs $\mathbf{O}_i$ and $\mathbf{O}_{i+1}$. The correct alignment is estimated according to the cross-correlation between the overlapped portion of the consecutive frames. Finally, the aligned outputs are merged into the final output stream $\mathbf{X} \in \mathbb{R}^{C \times T}$ by using an overlap-add method with Hanning window.

% In this preliminary work, we consider the application of CSS only to non-causal SSep models, which are generally more performant than their causal counterpart \cite{luo2019conv}. In such configuration, the latency of these models will be tied to the CSS stride and thus can be used online.
% On the other hand, causal separation models are already capable to process the input in a streaming fashion with small latency. 

\subsection{Voice Activity Detection Module}
\label{subsec:sys/vad}

%The VAD module is used to extract active speech segments from the post-processed estimated sources and to generate the diarization output. It is applied on each estimated source $\mathbf{\tilde{X}_{\ell}}$ independently. %but future work could also consider a multi-source VAD.
%We experiment with two different VAD models: an energy-based VAD ~\citep{landini2021analysis}, and a neural model which employs a temporal convolutional network (TCN) \citep{bai2018empirical}, as proposed in \cite{cornell2022overlapped}.

The VAD module is fed with the estimated separated sources and detects active speech segments. VAD is applied on each separated source $\mathbf{\tilde{X}_{\ell}}$ independently and the outputs are combined to produce the diarization output.
We investigate the use of two different models: an energy-based conventional VAD ~\citep{landini2021analysis} and a neural-based VAD which employs a temporal convolutional network (TCN) \citep{bai2018empirical}, as proposed in \cite{cornell2022overlapped}. The first method does not require additional training, whilst the latter is data driven.


\subsection{Leakage Removal Module}
\label{subsec:sys/post_proc}

%In the presence of long input recordings, even state-of-the-art separation models are prone to channel leakage when only one speaker is active \citep{Xiao2021MicrosoftSD} (e.g., see \textit{estimated sources} in Fig. \ref{fig:cssgd}). As a result, the ``leaked'' segments can be detected as speech by the following VAD module, leading to false alarm errors in the final diarization output. To alleviate this problem, we propose a post-processing algorithm to reduce these leakage-induced false alarms without significantly affecting missed speech, speaker confusion errors, and separation quality. It is designed to not introduce additional latency and its computational overhead is negligible as it relies on simple operations.

State-of-the-art SSep models have reached impressive performance when tested on short fully overlapped utterances \citep{wang2018supervised}. However, such models could generate channel leakage in sparsely overlapped conversational speech when only one speaker is active \citep{Xiao2021MicrosoftSD}. Thus, the VAD module can detect as speech the ''leaked'' segments. This negatively affects the diarization output by introducing false alarm errors. We propose a lightweight post-processing algorithm to mitigate this problem. It does not introduce additional latency and has very little impact on separation quality, missed speech and speaker confusion errors.  

The leakage removal post-processing algorithm is summarized in Algorithm \ref{algo:leak_rem}. %Given an input mixture $\mathbf{Y}$ and two estimated sources $\mathbf{X}^1$ and $\mathbf{X}^2$, we split each signal into disjoint segments $\mathbf{Y}_{\ell}$, $\mathbf{X}_{\ell}^1$, $\mathbf{X}_{\ell}^2$ of length $L$.
The algorithm is fed with an input mixture $\mathbf{Y}$ and two estimated sources $\mathbf{X}^1$ which are split into disjoint segments $\mathbf{Y}_{\ell}$, $\mathbf{X}_{\ell}^1$, $\mathbf{X}_{\ell}^2$ of length $L$.
For each segment, we compute the Scale-Invariant Signal-to-Distortion Ratios (SI-SDR) \citep{leroux2019sdr} $s_{\ell}^1$, $s_{\ell}^2$ between segments of every source $\mathbf{X}_{\ell}^1$, $\mathbf{X}_{\ell}^2$ with the associated segment $\mathbf{Y}_{\ell}$ of input mixture.
%If both $s_{\ell}^1$, $s_{\ell}^2$ are above a threshold $t_{\ell r}$, a segment with leakage is detected. In the disjoint SSGD approach leakage is removed by masking the segment with lower SI-SDR.
A leaked segment is detected when both $s_{\ell}^1$, $s_{\ell}^2$ are above a threshold $t_{\ell r}$. In the SSGD framework leakage is removed by filling with zeros the segments with lower SI-SDR.
%This process results in new estimated sources $\mathbf{\tilde{X}_{\ell}}$, which are passed as input to the VAD module.
Then, the post-processed estimated sources $\mathbf{\tilde{X}_{\ell}}$ are passed as input to the following VAD module.

\SetKwInput{KwInput}{Input}                
\SetKwInput{KwOutput}{Output}
\SetKw{KwBy}{by}
\SetKw{KwAnd}{and}
\begin{algorithm}[H]
\DontPrintSemicolon
  \KwInput{$\mathbf{Y}$, $\mathbf{X}^1$, $\mathbf{X}^2$, $T$, $L$, $t_{\ell r}$}
  \KwOutput{$\mathbf{\tilde{X}_{\ell}}^1$, $\mathbf{\tilde{X}_{\ell}}^2$}
  
  $\mathbf{\tilde{X}_{\ell}}^1 \gets \mathbf{X}^1$; $\mathbf{\tilde{X}_{\ell}}^2 \gets \mathbf{X}^2$ 
  
  %$N = \lfloor \frac{T}{L} \rfloor$
  
  \For{$i \gets 0$ \KwTo $T$ \KwBy $L$}{
    $s_{\ell}^1 \gets $ SI-SDR$(\mathbf{Y}$[$i$:$i$+$L$], $\mathbf{X}^1$[$i$:$i$+$L$])
    
    $s_{\ell}^2 \gets $ SI-SDR$(\mathbf{Y}$[$i$:$i$+$L$], $\mathbf{X}^2$[$i$:$i$+$L$])
    
    \If{ $s_{\ell}^1 > t_{\ell r}$ \KwAnd $s_{\ell}^2 > t_{\ell r}$}{
      \If{$s_{\ell}^1 > s_{\ell}^2$}{
        $\mathbf{\tilde{X}_{\ell}}^2$[$i$:$i$+$L$] $ \gets 0$
      }
      \Else{
       $\mathbf{\tilde{X}_{\ell}}^1$[$i$:$i$+$L$] $ \gets 0$
      }
    }
    }
\caption{Leakage Removal}
\label{algo:leak_rem}
\end{algorithm}


\subsection{End-to-End Training}
\label{subsec:sys/eend}
% Providing a short description of end-to-end training method. We can highlight here the novelty of this approach compared to previous work
In our previous work \citep{morrone2022low-latency} diarization is performed by cascading the SSep, leakage removal and VAD modules. In such a case, the SSep and VAD models are trained independently. We refer to this configuration as \emph{disjoint SSGD}.

In the \emph{end-to-end SSGD} SSep and VAD are jointly optimized, while removing the leakage removal during training. The SSep and VAD modules are initialized with the parameters of the disjoint SSGD and fine-tuned following two methods. In the first approach, i.e., \emph{VAD fine-tuning}, we freeze the SSep model and only optimize the VAD parameters. Instead, in the \emph{SSep+VAD fine-tuning} method all parameters are optimized jointly.

Also for end-to-end fine-tuned SSGD systems we apply the proposed leakage removal post-processing algorithm.
However, for end-to-end SSGD we apply the masking directly on the VAD output to avoid changing the distribution of estimated sources. This is because we found that doing so could degrade the VAD performance after end-to-end adaptation, since it was fine-tuned on the output of the SSep module. 


\section{Experimental Setup}
\label{sec:exp_setup}

\subsection{Datasets}
\label{ssec:exp_setup/dataset}
Because the focus of our work is on the CTS scenario, we use the \emph{Fisher Corpus Part 1} and \emph{Part 2} \citep{cieri2004fisher} for both training and test purposes.
The whole Fisher consists of $11699$ telephone conversations between two participants, totaling approximately $1960$ hours of English speech sampled at 8 kHz. The amount of overlapped speech is around 14\% of the total speech duration. 
%It provides a separate signal for each of the two speakers. This allows training a separation model directly on this dataset and computing source separation metrics such as SI-SDR improvement (SI-SDRi) \citep{leroux2019sdr} together with diarization metrics.
Since separated signals for the two speaker are provided, we can use this dataset to train and evaluate a separation model using common metrics as the SI-SDR improvement (SI-SDRi) \citep{leroux2019sdr}.
%Training, validation and test sets are created by drawing $11577$, $61$, and $61$ conversations, respectively, with no overlap between speakers' identities. The amount of overlapped speech is around 14\% of the total speech duration. 
We split the data in $11577$, $61$ and $61$ conversations for training, validation and test sets respectively, assuring that there is no overlap between speaker identities.

In addition, we generate a simulated fully-overlapped version of Fisher for the purpose of pre-training the SSep models, as done in \cite{morrone2022conversational}. This portion is derived from the training set and amounts to 30000 mixtures for a total of 44 hours.  

%We also test the proposed methods on the portion of the 2000 NIST SRE ~\citep{callhome} denoted as \emph{CALLHOME}, consisting of real-world multilingual telephone conversations. Following the work of \cite{fujita2019end}, we use the 2-speaker subset of CALLHOME and the adaptation/test split that allows comparing with most end-to-end diarization methods mentioned previously. 
%The amount of overlapped speech is around 13\% of the total speech duration.
To compare with state-of-the-art EEND methods, we also evaluate the proposed algorithms on the portion of the 2000 NIST SRE ~\citep{callhome} denoted as \emph{CALLHOME}. It consists of real telephone conversations in multiple languages.
We use the same 2-speaker subset of CALLHOME and the adaptation/test split proposed in \cite{fujita2019}. In this case, the amount of overlapped speech is around 13\% of the total speech duration.

Since separated sources are not available for CALLHOME, we create simulated conversations which are used to adapt SSep models to CALLHOME data. We exploit the provided annotations to extract single-speaker segments from recordings taken by the original adaptation set, discarding segments shorter than $0.1$ s. Then, the extracted segments are combined to mimic real-world conversational scenarios similar to
SparseLibriMix \citep{cosentino2020librimix}. Each conversation is created by alternately picking utterances from the two speakers, until a total minimum length of $30$ s is reached. To increase variability, we also mix speakers belonging to different recordings.
With this procedure, we generate an additional training and validation sets of $3000$ ($27.3$ h) and $500$ ($4.5$ h) examples, respectively. In this case, speakers overlap approximately $16$\% of the time.


\subsection{Architecture, Training and Inference Details}

%We consider $3$ SSep architectures: Conv-TasNet, DPRNN and DPTNet, both in online (causal) and offline (non-causal) configurations (for a total of $6$).
We experiment with SSep architectures both in online and offline settings. In particular, we consider $3$ different SSep models: Conv-TasNet, DPTNet and DPRNN.
%For Conv-TasNet we use the best hyperparameter configuration as found in \cite{luo2019conv}. For DPRNN and DPTNet we employ the same configurations described in \cite{luo2020dual} and \cite{dptnet}, respectively, with these exceptions: to reduce memory footprint we employ a $16$ analysis/synthesis kernel size for encoder/decoder; regarding causal models, we use standard layer normalization versus the non-causal global layer normalization employed in non-causal models. Additionally, we set the chunk and hop sizes to $100$ and $50$, respectively.
For Conv-TasNet we employ the best hyperparameter configuration proposed by \cite{luo2019conv}.
Instead, for DPRNN and DPTNet we use the hyperparameters proposed in \cite{luo2020dual} and \cite{dptnet}, respectively, with some changes. The kernel size for encoder/decoder is increased to $16$ to reduce memory consumption. The chunk and hop sizes are set to $100$ and $50$ (i.e., $50$\% overlap). In addition, the global layer normalization is replaced with standard layer normalization for online/causal models. We use the implementations available through the Asteroid toolkit \citep{pariente20}.
%These models are pre-trained on the simulated fully overlapped Fisher dataset using the SI-SDR objective to separate the two speakers. We observed that such pre-training significantly reduced training times compared to training from scratch on real data.
We pre-train SSep models on the simulated fully overlapped Fisher maximizing the SI-SDR function. We observed that the SSep pre-training converged much faster compared to training from scratch on real data. 
We use Adam optimizer \citep{kingma2015adam}, batch size $4$ and learning rate $10^{-3}$. We clip gradients with $l_2$ norm greater than $5$. 
%Each SSep model is then fine-tuned using a learning rate of $10^{-4}$ and batch size $1$ on the real Fisher data, by taking $60$ s long random segments from each recording.
Then, we fine-tune each SSep model on $60$ s long random segments from real Fisher recording using a batch size of $1$ and reducing the learning rate to $10^{-4}$.
For Conv-TasNet and DPRNN, learning rate is halved whether SI-SDR does not improve on the validation set for $10$ consecutive epochs. For DPTNet, we employ the learning rate scheduler used in \cite{dptnet} with $k_1=1$, $k_2=4\cdot10^{-4}$, $d_{model}=64$, and $warmup\_n=5n_{epoch}$, where $n_{epoch}$ is the number of training steps performed in a single epoch. 
If no improvement is observed for $20$ epochs on validation, training is stopped.
For the CALLHOME dataset, the separators are adapted on the simulated CALLHOME using the same hyperparameters of the fine-tuned models, except for the length of training segments which is set to $30$\,s.

%We adopt the TCN VAD from \cite{cornell2022overlapped}, which is causal and for which the latency amounts to 10\,ms, as the window size used to extract the log-Mel filterbank energies features.
We employ the TCN-based causal VAD proposed by \cite{cornell2022overlapped}. The latency of the VAD is set to $0.1$ s, as the hop size of the log-Mel filterbanks input features.
This model is trained on the original Fisher Part 1, using each speaker source separately, to classify speech vs. non-speech for each input frame. We use the following weighted binary cross-entropy loss:
\begin{equation}
    \mathcal{L}_{vad}=-\frac{1}{N}\sum_{n=1}^{N} \lambda_s \cdot s_n \cdot \log(\hat{s}_n) + (1 - s_n) \cdot \log(1 - \hat{s}_n),
    \label{eq:loss_vad}
\end{equation} 
where $N$, $n$, $s_n$ and $\hat{s}_n$ are the total number of frames, the frame index, the target speech/non-speech label and the estimated speech probability, respectively. $\lambda_s$ is a parameter that is adjusted according to training data distribution. We set it to $0.9$ for all experiments.

\begin{table*}[t]
\centering
\adjustbox{max width=\textwidth}{%
\centering
\begin{tabular}{@{}l|c|c|c|ccccc@{}}
\toprule
\textbf{Method} & \textbf{VAD} & \textbf{Leakage Removal} & \textbf{Latency (s)} & \textbf{SI-SDRi} & \textbf{MS} & \textbf{FA} & \textbf{SC} & \textbf{DER} \\
\midrule
\textit{Oracle sources} & \multirow{4}{*}{Energy} & \multirow{4}{*}{\xmark} & & $\infty$ & 7.3 & 1.8 & 0.1 & 9.2 \\
Conv-TasNet & & & 0.01 & 8.9 & 9.5 & 26.9 & 1.6 & 38.0 \\
DPTNet & & & 0.1 & 21.6 & 7.5 & 2.6 & 0.8 & 10.6 \\
DPRNN & & & 0.1 & \textbf{22.7} & 7.5 & \textbf{1.7} & 0.8 & 10.0 \\
 \arrayrulecolor{black!50}\midrule
\textit{Oracle sources} & \multirow{4}{*}{TCN} & \multirow{4}{*}{\xmark} & & $\infty$ & 3.5 & 1.8 & 0.1 & 5.3 \\
Conv-TasNet & & & 0.01 & 8.9 & 7.4 & 30.9 & 5.3 & 43.6 \\
DPTNet & & & 0.1 & 21.6 & 4.3 & 3.2 & \textbf{0.6} & 8.0 \\
DPRNN & & & 0.1 & \textbf{22.7} & \textbf{3.3} & 3.5 & 0.7 & 7.5 \\
\arrayrulecolor{black!50}\midrule
Conv-TasNet & \multirow{3}{*}{TCN} & \multirow{3}{*}{\cmark} & 0.01 & 5.9 & 7.7 & 4.3 & 13.3 & 25.3 \\
DPTNet & & & 0.1 & 21.2 & 4.1 & 2.2 & 1.1 & 7.3 \\
DPRNN & & & 0.1 & 22.3 & 3.7 & 2.6 & 0.8 & \textbf{7.1} \\
 \arrayrulecolor{black}\bottomrule
\end{tabular}
}
\caption{Disjoint SSGD: speech separation and diarization results on the Fisher test set in the \textbf{online} scenario. Separation is assessed using the SI-SDR (dB) improvements over the input mixtures. Diarization is assessed using diarization error rate (DER), missed speech (MS), false alarm (FA) and speaker confusion errors (SC). Algorithmic latency is reported in seconds.
The best results among the proposed techniques are shown in \textbf{bold}.}
\label{tab:res/diar_online_fisher}
\end{table*}

\begin{table*}[t]
\centering
\adjustbox{max width=\textwidth}{%
\centering
\begin{tabular}{@{}l|c|c|c|cccc@{}}
\toprule
\textbf{Method} & \textbf{VAD} & \textbf{Leakage Removal} & \textbf{Latency (s)} & \textbf{MS} & \textbf{FA} & \textbf{SC} & \textbf{DER} \\
\midrule
SA-EEND w/STB~\citep{xue2021online} & \multirow{3}{*}{n.a.} & \multirow{3}{*}{n.a.} & 1 & & & & 12.5 \\
BW-EDA-EEND~\citep{han2021bw} & & & 10 & & & &  11.8 \\
SA-EEND-EDA w/STB~\citep{xue2021edaonline} & & & 10 & & & & 10.0 \\
EEND-GLA w/BW-STB~\citep{horiguchi2022online} & & & 1 & & & & \underline{9.0} \\
\midrule
Conv-TasNet & \multirow{3}{*}{Energy} & \multirow{3}{*}{\xmark} & 0.01 & 7.4 & 35.3 & 3.0 & 45.7 \\
DPTNet & & & 0.1 & \textbf{5.5} & 6.3 & 0.8 & 12.6 \\
DPRNN & & & 0.1 & 5.7 & 5.2 & 1.8 & 12.7 \\
 \arrayrulecolor{black!50}\midrule
Conv-TasNet & \multirow{3}{*}{TCN} & \multirow{3}{*}{\xmark} & 0.01 & 12.2 & 36.2 & 5.1 & 53.4 \\
DPTNet & & & 0.1 & 6.6 & 4.5 & \textbf{0.6} & 11.7 \\
DPRNN & & & 0.1 & 5.9 & 3.8 & 1.7 & 11.6 \\
\arrayrulecolor{black!50}\midrule
Conv-TasNet & \multirow{3}{*}{TCN} & \multirow{3}{*}{\cmark} & 0.01 & 13.2 & 6.3 & 13.0	& 32.5 \\
DPTNet & & & 0.1 & 6.3 & 2.6 & 1.2 & \textbf{10.0} \\
DPRNN & & & 0.1 & 6.8 &	\textbf{2.2} & 2.2 & 11.2 \\
 \arrayrulecolor{black}\bottomrule
\end{tabular}
}
\caption{Disjoint SSGD: diarization results on the CALLHOME test set in the \textbf{online} scenario. Diarization is assessed using diarization error rate (DER), missed speech (MS), false alarm (FA) and speaker confusion errors (SC). Latency of the system is reported in seconds.
The best results among proposed techniques are shown in \textbf{bold}, and among EEND methods are \underline{underlined}.}
\label{tab:res/diar_online_callhome}
\end{table*}

% \subsection{SSep Adaptation with Simulated CALLHOME}
% \textcolor{red}{
% Move in Section \ref{subsec:res/online_eval}.
% Maybe we can add a separate table to show the improvement using the simulated CALLHOME data for SSep adaptation. ("Disjoint SSGD w/LR" rows of Table \ref{tab:res/e2e_callhome}).}

%We train on randomly selected $2$ s long segments with a batch size of $256$. The optimizer, the learning rate scheduler, gradient clipping and early stopping hyperparameters are the same as those used for Conv-TasNet and DPRNN.
The VAD is trained on $2$ s long segments and the batch size is set to $256$. We employ the same optimizer, learning rate scheduler, gradient clipping and early stopping policy used for Conv-TasNet and DPRNN.
%In inference, the VAD is applied to estimated sources and we employ a median filter to smooth the VAD predictions. %In addition, we remove segments shorter than a threshold $t_s$ to further reduce false alarm errors.
At inference the VAD is applied on estimated separated sources independently. For each frame, the VAD predictions above a threshold $t_v$ are labeled as speech. Then, the thresholded predictions are smoothed using a median filter and segments shorter than a threshold $t_s$ are removed to mitigate false alarm errors.
%For each SSGD model, we tune the median filter, leakage removal threshold and $t_s$ parameters on the Fisher validation set (CALLHOME adaptation set for CALLHOME models).
The median filter length, $t_v$ and $t_s$ parameters are tuned on the Fisher validation set for each SSGD architecture. Likewise, these parameters are tuned on the CALLHOME adaptation set for CALLHOME models.
The threshold $t_{\ell r}$ and the segment length $L$ of the leakage removal algorithm are tuned manually and set to $3$ and $0.1$ s, respectively, to not affect the latency of the VAD.

The end-to-end models (i.e., VAD and SSep+VAD fine-tuning) are trained with the same hyperparameters above, except for the initial learning rate which is set to $10^{-5}$. The SSep module is initialized with the model trained on real Fisher and simulated CALLHOME when fine-tuned with Fisher and CALLHOME, respectively. Instead, we initialize the VAD with the model pre-trained on Fisher for both datasets. Finally, all models are fine-tuned using the real datasets to minimize the loss $\mathcal{L}_{vad}$ (cfr. Equation \ref{eq:loss_vad}).


\section{Experiments and Results}
\label{sec:res}

%We evaluate the performance on Fisher and CALLHOME test sets in terms of diarization error rate (DER) including overlapped speech and using a collar tolerance of $0.25$ s (i.e., \emph{fair evaluation}), as in \cite{fujita2019end}.
Diarization performance is evaluated in terms of diarization error rate (DER) on Fisher and CALLHOME test sets. Following \cite{fujita2019end}, we consider overlapped speech and use a start and end-point collar tolerance of $0.25$ s (i.e., \emph{fair evaluation} setup).
Moreover, we also report the DERs without collar tolerance (i.e., \emph{full evaluation} setup) obtained by end-to-end training strategies. The evaluation is carried out using the standard NIST \textit{md-eval}\footnote{\url{https://github.com/usnistgov/SCTK}} (version 22) scoring tool.
%For the Fisher test set we also report the SI-SDRi \citep{leroux2019sdr} source separation metric since oracle sources are available.
Since oracle sources are available for the Fisher test set, we also measure the separation capability using the SI-SDRi metric \citep{leroux2019sdr}.

\subsection{Online Diarization}\label{subsec:res/online_eval}

\subsubsection{Disjoint SSGD}
The results for online disjoint SSGD (i.e., SSep and VAD are trained separately) diarization models on Fisher are reported in Table~\ref{tab:res/diar_online_fisher}. Results for CALLHOME are shown in Table \ref{tab:res/diar_online_callhome}. In this case, we do not fine-tune neither SSep nor VAD on CALLHOME data. Oracle sources refer to SSGD with oracle SSep, thus with error coming only from the VAD module. Note that oracle evaluation is missing for CALLHOME, as separated sources are not available.
%For the CALLHOME evaluation, we also show DERs obtained by EEND, as reported in the original papers.%With the \emph{oracle sources} method we refer to the SSGD using perfect separation. In this case, the errors only come from the VAD module.
We also show the results of EEND on the CALLHOME test set, as reported in their respective original works.

The comparison between the TCN VAD and the energy-based one highlights that, as expected, the former performs overall better. The difference however is not major, highlighting the fact that, if the separator performs well a simple VAD may be sufficient in some instances. 

%We observed that the Conv-TasNet model failed to deal with long recordings, generating large false alarm errors. %We found that the model generated large false alarms that could be only partially mitigated by the leakage removal algorithm.
The separation capability of the Conv-TasNet model was very poor, generating large false alarm errors. 
%This is due to the fact that, being fully convolutional, it has a limited ${\sim} 1.5$ s receptive field.
Indeed, it is limited by its short receptive fields, i.e.,  ${\sim} 1.5$ s, which prevents it to learn long term dependencies.
Such a problem is solved by the DPRNN and DPTNet which can track the speakers for much longer due to LSTMs (coupled with self-attention in DPTNet) in the inter-block. This leads to much better diarization results. These two models reached similar performance on Fisher, whereas DPTNet performed better on CALLHOME when used in conjunction with leakage removal.

%The proposed leakage removal algorithm was highly effective for all SSep architectures. This was especially true in the case of TCN-based VAD since it was more prone to false alarms caused by leaked speech due to being trained on real Fisher data and not on the output of the separators.
Crucially, the proposed leakage removal post-processing consistently improved diarization performance for all SSep models. We observed the major benefits when leakage removal is used with the TCN-based VAD. Indeed, since it is trained on real Fisher data and not on the output of the separators (disjoint training), it is prone to classify channel leakage as active speech.
%Although the algorithm was only partially able to mitigate the low separation capability of the Conv-TasNet, it improved the DER by $42.0$\% and $39.1$\% on Fisher and CALLHOME, respectively. For DPRNN, the improvement was lower as the system without leakage removal was already able to obtain good diarization performance. However, the proposed post-processing almost halved the false alarm error rates and improved the DER by $5.3$\% and $3.4$\% on Fisher and CALLHOME, respectively. For DPTNet, the leakage removal was even more effective as the DER was reduced by $8.8$\% and $14.5$\%.
For Conv-TasNet, the DER was reduced by $42.0$\% and $39.1$\% on Fisher and CALLHOME, respectively. However, the diarization accuracy remained relatively low due to poor separation capability. For DPTNet and DPRNN, the leakage removal almost halved the false alarm errors. We observed a larger improvement with DPTNet as the DER is reduced by $8.8$\% and $14.5$\%, as opposed to DPRNN for which the DER improved by $5.3$\% and $3.4$\% on Fisher and CALLHOME, respectively.

As a comparison, the current best performing online system on the CALLHOME dataset (i.e., EEND-GLA with block-wise speaker tracing buffer \citep{horiguchi2022online}) obtains 9.0\% DER, which is slightly better than ours but is obtained with higher latency of $1$ s.
Our approach works with a latency of $0.1$ s, making it appealing for applications where strict real-time requirements are very important (e.g., real-time captioning).

%S\textcolor{red}{Maybe we can move the end-to-end section (\ref{ssec:res/e2e}) here.}

\begin{table*}[t]
\centering
\adjustbox{max width=\textwidth}{%
\centering
\begin{tabular}{@{}l|c|ccccccccc@{}}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Online}} & \multirow{2}{*}{\textbf{SI-SDRi}} & \multicolumn{4}{c}{\textbf{Fair Eval.}} & \multicolumn{4}{c}{\textbf{Full Eval.}} \\
\cmidrule(r{4pt}){4-7} \cmidrule{8-11}
 & & & \textbf{MS} & \textbf{FA} & \textbf{SC} & \textbf{DER} & \textbf{MS} & \textbf{FA} & \textbf{SC} & \textbf{DER} \\
\midrule
\textit{Oracle sources} & n.a. & $\infty$ & 3.5 & 1.8 & 0.1 & 5.3 & 7.0 & 4.1 & 0.3 & 11.5 \\
\midrule
Disjoint SSGD w/LR & \multirow{5}{*}{\cmark} & 22.3 & 3.7 & 2.6 & 0.8 & 7.1 & 7.3 & 5.1 & 1.1 & 13.5 \\
VAD fine-tuning & & \textbf{22.7} & 3.9 & 1.6 & \textbf{0.7} & 6.2 & 7.7 & 3.6 & 0.8 & 12.1 \\
VAD fine-tuning w/LR & & \textbf{22.7} & 5.3 & \textbf{0.9} & 0.9 & 7.1 & 10.8 & \textbf{2.3} & 1.0 & 14.0 \\
SSep+VAD fine-tuning & & 14.9 & \textbf{1.9} & 1.6 & \textbf{0.7} & \textbf{4.2} & 4.3 & 3.5 & \textbf{0.7} & \textbf{8.5} \\
SSep+VAD fine-tuning w/LR & & 14.9 & 2.1 & 1.4 & \textbf{0.7} & \textbf{4.2} & \textbf{3.9} & 4.1 & \textbf{0.7} & 8.7 \\
\arrayrulecolor{black!50}\midrule
Disjoint SSGD w/LR & \multirow{5}{*}{\xmark} & 22.8 & 3.9 & 2.0 & 0.2 & 6.1 & 7.7 & 4.1 & 0.5 & 12.2 \\
VAD fine-tuning & & \textbf{23.2} & 4.0 & 1.3 & \textbf{0.1} & 5.4 & 8.0 & 3.3 & 0.3 & 11.7 \\
VAD fine-tuning w/LR & & \textbf{23.2} & 5.7 & \textbf{0.8} & \textbf{0.1} & 6.7 & 11.8 & \textbf{2.2} & 0.4 & 14.4 \\
SSep+VAD fine-tuning & & 17.7 & \textbf{3.1} & 0.9 & 0.2 & \textbf{4.1} & \textbf{5.5} & 2.7 & \textbf{0.2} & \textbf{8.4} \\
SSep+VAD fine-tuning w/LR & & 17.7 & 3.2 & \textbf{0.8} & 0.2 & 4.2 & 5.8 & 2.5 & \textbf{0.2} & 8.6 \\
\arrayrulecolor{black}\bottomrule
\end{tabular}
}
\caption{End-to-end SSGD: separation and diarization results on the Fisher test set. The best results among proposed techniques are shown in \textbf{bold}.}
\label{tab:res/e2e_fisher}
\end{table*}

\begin{table*}[t]
\centering
\adjustbox{max width=\textwidth}{%
\centering
\begin{tabular}{@{}l|c|ccccccccc@{}}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Online}} & \multicolumn{4}{c}{\textbf{Fair Eval.}} & \multicolumn{4}{c}{\textbf{Full Eval.}} \\
\cmidrule(r{4pt}){3-6} \cmidrule{7-10}
 & & \textbf{MS} & \textbf{FA} & \textbf{SC} & \textbf{DER} & \textbf{MS} & \textbf{FA} & \textbf{SC} & \textbf{DER} \\
\midrule
Disjoint SSGD w/LR & \multirow{6}{*}{\cmark} & 6.8 & 2.2 & 2.2 & 11.2 & 9.8 & 10.6 & 3.1 & 23.6 \\
Disjoint SSGD w/LR (sim-CH adapt.) & & 6.7 & 2.2 & 0.6 & 9.5 & \textbf{6.5} & 11.2 & 1.5 & 22.1 \\
VAD fine-tuning & & 6.8 & 2.3 & 0.6 & 9.8 & 10.9 & 6.9 & 1.3 & 19.1 \\
VAD fine-tuning w/LR & & 7.6 & \textbf{1.5} & 0.8 & 9.8 & 12.5 & \textbf{5.3} & 1.5 & 19.3 \\
SSep+VAD fine-tuning & & \textbf{6.3} & 2.1 & \textbf{0.4} & \textbf{8.8} & 8.8 & 7.0 & \textbf{0.7} & \textbf{16.5} \\
SSep+VAD fine-tuning w/LR & & 6.5 & 1.9 & \textbf{0.4} & \textbf{8.8} & 9.3 & 6.5 & 0.8 & 16.6 \\
\arrayrulecolor{black!50}\midrule
Disjoint SSGD w/LR & \multirow{6}{*}{\xmark} & 6.2 & 2.6 & 1.5 & 10.2 & 8.5 & 12.1 & 2.3 & 22.9 \\
Disjoint SSGD w/LR (sim-CH adapt.) & & \textbf{5.6} & 2.9 & 0.8 & 9.3 & \textbf{7.7} & 12.9 & 1.5 & 22.1 \\
VAD fine-tuning & & 7.2 & 2.1 & 1.0 & 10.2 & 11.3 & 6.2 & 1.6 & 19.1 \\
VAD fine-tuning w/LR & & 7.6 & \textbf{1.3} & 1.2 & 10.0 & 12.3 & \textbf{5.0} & 1.8 & 19.1 \\
SSep+VAD fine-tuning & & 6.5 & 2.0 & \textbf{0.7} & \textbf{9.2} & 9.2 & 6.2 & \textbf{1.0} & \textbf{16.4} \\
SSep+VAD fine-tuning w/LR & & 6.7 & 1.8 & \textbf{0.7} & \textbf{9.2} & 9.8 & 5.7 & \textbf{1.0} & 16.6 \\
\arrayrulecolor{black}\bottomrule
\end{tabular}
}
\caption{End-to-end SSGD: diarization results on the CALLHOME test set. The best results among proposed techniques are shown in \textbf{bold}.}
\label{tab:res/e2e_callhome}
\end{table*}

\subsubsection{End-to-End SSGD}
\label{sssec:res/e2e}
%\textcolor{red}{We only report the results with online/offline DPRNN as it results the best one on the Fisher test set (the system is trained on Fisher)}.
We focus on the DPRNN-based architecture as it performs best on the Fisher dataset. Additionally, we observed that the adaptation of DPRNN on simulated CALLHOME is more stable compared to other separators (i.e., Conv-TasNet and DPTNet) leading to better final performance on the real CALLHOME test set.

We experiment with different adaptation and end-to-end training strategies. Contrary to what we have reported for disjoint SSGD models, we also report here the \emph{full evaluation} (no collar evaluation) as it can provide a better estimation of segmentation accuracy.

Table \ref{tab:res/e2e_fisher} reports the results on the Fisher test set. The two fine-tuning strategies provided meaningful improvements over the online disjoint SSGD with leakage removal. In particular, the VAD and SSep+VAD fine-tuning strategies reduced the \emph{fair/full} DER by $12.7$/$10.3$\% and $40.8$/$37.0$\%, respectively. Both approaches helped a lot to reduce FA errors. In these cases, the use of the leakage removal algorithm was not useful. It even degraded the performances when used with the VAD fine-tuning. Note that the SSep+VAD fine-tuning strategies also outperformed the evaluation with oracle sources, meaning that the model was able to estimate ``custom'' separated sources which resulted in better diarization outputs. The significant improvement in diarization performance was obtained at the cost of lowering separation performance which however still remains acceptable.

The results on the CALLHOME test set are shown in Table \ref{tab:res/e2e_callhome}. The adaptation of the separator on the simulated CALLHOME was very effective to reduce SC errors and then the overall DER. As such, we apply all the end-to-end strategies starting from this SSGD adapted on the simulated CALLHOME. The VAD fine-tuning only improved with the full evaluation. Instead, the SSep+VAD fine-tuning reduced the \emph{fair/full} DER by $7.3$/$24.9$\% over the adapted disjoint SSGD.

%As a comparison, the current best performing online system on the CALLHOME dataset (i.e., EEND-global-local attractors (EEND-GLA) with block-wise speaker tracing buffer \citep{horiguchi2022online}) obtains 9.0\% DER, which is slightly better than ours but is obtained with higher latency of $1$ s.
To the best of our knowledge, the SSep+VAD approach \emph{fair} evaluation outperformed all online state-of-the-art EEND methods on the 2-speaker CALLHOME test set with significantly lower latency, i.e., $0.1$ vs $1$ or $10$ s.
%As a comparison, the current best performing online system on the CALLHOME dataset (i.e., SA-EEND-EDA with speaker tracing buffer \cite{xue2021edaonline}), obtains 10.0\% DER, which is slightly better than ours but is obtained with significantly higher latency of $10$ s.
Additionally, the SSGD is trained using a dataset of ${\sim} 1900$ hours of speech, which is about 5x smaller  than the ones used to train the state-of-the-art EEND models (i.e., ${\sim} 10000$ hours). %This results in shorter training times and also avoids incurring costs due to simulated mixtures generation.

\begin{table*}[t]
\centering
\adjustbox{max width=\textwidth}{%
\centering
\begin{tabular}{@{}l|c|c|ccccc@{}}
\toprule
\textbf{Method} & \textbf{VAD} & \textbf{Leakage Removal} & \textbf{SI-SDRi} & \textbf{MS} & \textbf{FA} & \textbf{SC} & \textbf{DER} \\
\midrule
VBx~\citep{landini2022bayesian} & TCN & \multirow{5}{*}{n.a.} & \multirow{5}{*}{n.a.} & 10.0 & \underline{0.3} & 0.5 & 10.8 \\
VBx~\citep{landini2022bayesian} & Kaldi & & & 8.9 & 0.4 & 0.9 & 10.2 \\
~~ + Overlap assignment~\citep{bullock2020overlap} & Kaldi & & & \underline{4.4} & 2.1 & 0.9 & \underline{7.4} \\
Spectral clustering~\citep{park2019auto} & Kaldi & & & 8.9 & 0.4 & \underline{0.2} & 9.5 \\
~~ + Overlap assignment~\citep{raj2021multi} & Kaldi & & & 5.2 & 2.0 & \underline{0.2} & \underline{7.4} \\
\midrule
\textit{Oracle sources} & \multirow{4}{*}{Energy} & \multirow{4}{*}{\xmark} & $\infty$ & 7.3 & 1.8 & 0.1 & 9.2 \\
Conv-TasNet & & & 20.1 & 7.6 & 4.1 & 0.9 & 12.7 \\
DPTNet & & & 22.2 & 7.5 & 2.1 & 0.4 & 10.0 \\
DPRNN & & & \textbf{23.2} & 7.5 & 1.6 & 0.2 & 9.3 \\
 \arrayrulecolor{black!50}\midrule
\textit{Oracle sources} & \multirow{4}{*}{TCN} & \multirow{4}{*}{\xmark} & $\infty$ & 3.5 & 1.8 & 0.1 & 5.3 \\
Conv-TasNet & & & 20.1 & 4.1 & 5.2 & 0.7 & 10.0 \\
DPTNet & & & 22.2 & 4.8 & 2.4 & \textbf{0.1} & 7.3 \\
DPRNN & & & \textbf{23.2} & \textbf{3.4} & 3.1 & \textbf{0.1} & 6.5 \\
\arrayrulecolor{black!50}\midrule
Conv-TasNet & \multirow{3}{*}{TCN} & \multirow{3}{*}{\cmark} & 19.7 & 4.9 & \textbf{1.3} & 1.6 & 7.9 \\
DPTNet & & & 21.9 & 5.5 & 1.4 & \textbf{0.1} & 7.0 \\
DPRNN & & & 22.8 & 3.9 & 2.0 & 0.2 & \textbf{6.1} \\
 \arrayrulecolor{black}\bottomrule
\end{tabular}
}
\caption{Disjoint SSGD: speech separation and diarization results on the Fisher test set in the \textbf{offline} scenario.
The best results among proposed techniques are shown in \textbf{bold}, and among baselines are \underline{underlined}.}
\label{tab:res/diar_offline_fisher}
\end{table*}

\begin{table*}[t]
\centering
\adjustbox{max width=\textwidth}{%
\centering
\begin{tabular}{@{}l|c|c|cccc@{}}
\toprule
\textbf{Method} & \textbf{VAD} & \textbf{Leakage Removal} & \textbf{MS} & \textbf{FA} & \textbf{SC} & \textbf{DER} \\
\midrule
VBx~\citep{landini2022bayesian} & TCN & \multirow{9}{*}{n.a.} & 7.3 & 1.9 & 3.1 & 12.3 \\
VBx~\citep{landini2022bayesian} & Kaldi & & 8.3 & \underline{0.9} & 2.6 & 11.7 \\
~~ + Overlap assignment~\citep{bullock2020overlap} & Kaldi & & 5.3 & 2.5 & 2.4 & 10.3 \\
Spectral clustering~\citep{park2019auto} & Kaldi &  & 8.3 & \underline{0.9} & 5.3 & 14.5 \\
~~ + Overlap assignment~\citep{raj2021multi} & Kaldi & & 5.7 & 2.7 & 5.8 & 14.1 \\
SA-EEND~\citep{fujita2019end} & n.a. & & & & & 9.5 \\
SA-EEND-EDA~\citep{horiguchi2020end} & n.a. & & & & & 8.1 \\
EEND + VC~\citep{kinoshita2021integrating} & n.a. & & \underline{4.0} & 2.4 & \underline{0.5} & 7.0 \\
EEND-GLA~\citep{horiguchi2021towards} & n.a. & & & & & 6.9 \\
DIVE~\citep{zeghidour2021dive} & n.a. & & & & & \underline{6.7} \\
\midrule
Conv-TasNet & \multirow{3}{*}{Energy} & \multirow{3}{*}{\xmark} & 5.5 & 4.7	& 0.7 & 10.9 \\
DPTNet & & & \textbf{5.4} & 5.3 & \textbf{0.4} & 11.1 \\
DPRNN & & & 5.5 & 5.2 & 0.9 & 11.6 \\
 \arrayrulecolor{black!50}\midrule
Conv-TasNet & \multirow{3}{*}{TCN} & \multirow{3}{*}{\xmark} & 6.5 & 4.4 & 0.5 & 11.4 \\
DPTNet & & & 7.0 & 3.3 & \textbf{0.4} & 10.7 \\
DPRNN & & & 6.5 & 4.0 & 0.7 & 11.2 \\
\arrayrulecolor{black!50}\midrule
Conv-TasNet & \multirow{3}{*}{TCN} & \multirow{3}{*}{\cmark} & 6.1 & 2.6 & 0.9 & 9.6 \\
DPTNet & & & 6.3 & \textbf{2.4} & 0.8 & \textbf{9.5} \\
DPRNN & & & 6.2 & 2.6 & 1.5 & 10.2 \\
 \arrayrulecolor{black}\bottomrule
\end{tabular}
}
\caption{Disjoint SSGD: diarization results on the CALLHOME test set in the \textbf{offline} scenario. The best results among proposed techniques are shown in \textbf{bold}, and among baselines/EEND methods are \underline{underlined}.}
\label{tab:res/diar_offline_callhome}
\end{table*}

\subsection{Offline Diarization}
\label{subsec:res/offline_eval}

We compare the offline SSGD with both clustering-based and EEND methods.
As clustering-based baselines we use VBx~\citep{landini2022bayesian} and spectral clustering~\citep{park2019auto}, along with their overlap-aware counterparts \citep{bullock2020overlap, raj2021multi}. In this case, we use the publicly available Kaldi ASpIRE VAD ~\citep{peddinti2015jhu} model\footnote{\url{https://kaldi-asr.org/models/m4}}. For overlap detection, we fine-tune the Pyannote~\citep{bredin2020pyannote} segmentation model\footnote{\url{https://huggingface.co/pyannote/segmentation}} on the full CALLHOME adaptation set. We tune the hyperparameters for each aforementioned module on the associated validation sets. To perform a fair comparison, we also tested the use of the TCN-based VAD in the VBx system, which however led to lower performance. For CALLHOME, we also report diarization errors of state-of-the-art EEND systems, as done in Table \ref{tab:res/diar_online_callhome}.

The results for the offline setting are reported in Tables~\ref{tab:res/diar_offline_fisher} and \ref{tab:res/diar_offline_callhome} for Fisher and CALLHOME, respectively.
%For the offline scenario, we compare our approach with clustering-based and EEND methods. For the former, we use VBx~\citep{landini2022bayesian} and spectral clustering~\citep{park2019auto}, along with their overlap-aware counterparts \citep{bullock2020overlap, raj2021multi}.
%For VAD in clustering-based systems, we use the publicly available Kaldi ASpIRE VAD ~\citep{peddinti2015jhu} model\footnote{\url{https://kaldi-asr.org/models/m4}}. For overlap detection, we fine-tune the Pyannote~\citep{bredin2020pyannote} segmentation model\footnote{\url{https://huggingface.co/pyannote/segmentation}} on the full CALLHOME adaptation set. The hyperparameters for each task are tuned on the corresponding validation set. The scripts for reproducing the baseline results are publicly available\footnote{\url{https://github.com/desh2608/diarizer}}. For a fair comparison, we also report the performance of VBx with the TCN VAD, which however leads to degraded performance for this system.

%The results for baselines and the offline SSGD diarization models are reported in Tables~\ref{tab:res/diar_offline_fisher} and \ref{tab:res/diar_offline_callhome} for Fisher and CALLHOME, respectively. 
%As in Table \ref{tab:res/diar_online_callhome}, we show DERs of the EEND methods for the CALLHOME test set.

%In contrast to the online scenario, Conv-TasNet obtained good separation capability. However, SSGD based on dual-path SSep strongly outperformed the Conv-TasNet version on all metrics on the Fisher dataset, and even surpassed the overlap-aware VBx which fares best among all clustering baselines.
Regarding separation performance (SI-SDRi), we can see that the offline DPTNet and DPRNN slightly outperformed their online counterparts.
The offline Conv-TasNet was able to obtain good separation capability, resulting in diarization performances similar the DPTNet and DPRNN for CALLHOME. Here, the use of non-causal convolutions and global layer normalization allows the model to track correctly the speakers. However, the dual-path SSep methods outperformed Conv-TasNet on the Fisher test set on all metrics. The overlap-aware VBx resulted the best among the clustering-based baselines, which were all surpassed by our best SSGD systems on the two test sets.
Regarding separation performance (SI-SDRi), the offline DPRNN and DPTNet slightly outperformed their online counterparts which confirmed that both dual-path SSep online models are very effective when only past context is available.

As in the online setting, the TCN VAD outperformed the energy-based one and the proposed leakage removal algorithm continued to be useful.

Regarding the proposed adaptation and end-to-end training methods, we can make similar considerations with respect to the online scenario (cfr. Section \ref{sssec:res/e2e}).

For the CALLHOME data, the best performing offline model is comparable with SA-EEND \citep{fujita2019end}, although it is not competitive with the current best performing approaches \citep{kinoshita2021integrating, zeghidour2021dive, horiguchi2021towards}, making it less attractive for offline applications. 
However, it can be a cost-effective solution as the separated signals can be readily used in downstream applications such as ASR. We will show this in Section \ref{subsec:res/wer_eval}.

%In future work we will consider several strategies to reduce this gap such as training with more data, comparable to the amount used in EEND models, and fine-tuning our models on the CALLHOME adaptation set (as done in ~\cite{fujita2019end, horiguchi2020end}).
% However, since we did not fine-tune the separation networks on the CALLHOME development set (as done in ~\cite{horiguchi2020end, kinoshita2022tight}), it failed to outperform these methods in terms of DER performance.

\begin{figure*}[ht]
\centering
    \begin{subfigure}[b]{0.45\textwidth}
         \includegraphics[width=\textwidth]{figs/fisher_offlinedprnn_win.pdf}
         \caption{Fisher}
         \label{fig:css_analysis/fisher}
     \end{subfigure}
     %\vfill
     \begin{subfigure}[b]{0.41\textwidth}
         \includegraphics[width=\textwidth]{figs/callhome_offlinedprnn_win.pdf}
         \caption{CALLHOME}
         \label{fig:css_analysis/callhome}
     \end{subfigure}
     \caption{Separation and diarization results on the test sets with different CSS windows. The overlap between windows is set to $50$\%. The results are obtained with the disjoint SSGD model (DPRNN+TCN+Leakage removal).}
     \label{fig:css_analysis}
\end{figure*}




\subsection{CSS Window Analysis}
\label{subsec:res/css_win_eval}

%Recall from Section \ref{subsec:sys/css} that the CSS framework, besides allowing the processing of arbitrarily long recordings, also allows using a non-causal separation model in an online fashion with latency reduced to the length of the CSS window. Therefore, it can be regarded as an alternative approach for performing online diarization.

The CSS framework allows the processing of arbitrarily long inputs using chunk-wise processing. We can also exploit CSS to reduce latency of a non-causal SSep model to the CSS window length. In this way, we implement an alternative approach to perform online diarization which employs an offline SSep model in an SSGD framework.

%We use the DPRNN-based offline model with leakage removal from Table \ref{tab:res/diar_offline_fisher} (DPRNN+TCN+Leakage removal) to investigate the effect of varying window sizes on SSGD.
For our purposes, we consider the offline DPRNN-based disjoint SSGD with leakage removal from Table \ref{tab:res/diar_offline_fisher} to analyze how varying CSS windows size affects diarization performance.

The results are shown in Fig.~\ref{fig:css_analysis} for both datasets.
%As expected, the DER consistently decreased as the window size increased. In particular, the performances were almost on par with the offline models for windows larger than 60 and 30 seconds, respectively, for Fisher and CALLHOME.
%This suggests a possible parallelization scheme for offline SSGD by applying CSS on minute-long frames simultaneously, resulting in significant inference speed-ups and less memory consumption. The optimal chunk sizes are different for the two datasets because of the difference in their average recording duration (which is 10 minutes and 72 seconds for Fisher and CALLHOME, respectively).
Generally, we observed that the main source of error came from speaker confusion which consistently decreased for larger CSS windows, while missed speech and false alarm error rates remained approximately constant. 
In the presence of longer input recordings the CSS benefited from using longer windows to reduce errors in computing the correct permutation across frames during the stitching stage. Indeed, the correct permutation is computed on the overlapped portion using the cross-correlation, which is more reliable when computed on larger segments.
The performances were close to the offline ones for windows larger than $60$ and $30$ s for Fisher and CALLHOME, respectively. The different slope of DER curves in Fig.~\ref{fig:css_analysis/fisher} and~\ref{fig:css_analysis/callhome} is due to the different average recording duration, which is $10$ minutes and $72$ s for Fisher and CALLHOME, respectively. 
This finding also suggests a parallelization strategy for offline CSS. Indeed, using shorter windows (e.g., $30$ or $60$ s) results in a lower memory footprint and higher inference speed-ups without affecting separation and diarization capabilities. 

%As the window was shortened, missed speech and false alarm error rates remained approximately constant while speaker confusion errors consistently increased, indicating that the main source of error comes from speaker permutation due to wrong channel reordering during the stitching stage of the CSS. For smaller windows, the cross-correlation used for reordering consecutive chunks is less reliable due to the smaller size of the overlapping portion.

%The CSS framework is not competitive with the online approach with causal SSep (Sec. \ref{subsec:res/online_eval}) in terms of latency. However, it could be a convenient choice for applications in which better ASR accuracy (see Section \ref{subsec:res/wer_eval}) is more desirable than the low-latency requirement, and memory footprint is an important concern, especially for very long recordings (e.g., $>10$ minutes).

Compared to the online SSGD approach with causal SSep (Sec. \ref{subsec:res/online_eval}), the CSS framework requires higher latency to obtain the same performance (i.e. $0.1$ vs $30$/$60$ s). On the other hand, it could be an appropriate option in applications in which a lower memory footprint and better ASR accuracy are important requirements rather than low latency, especially for very long recordings (e.g., $>10$ minutes).

% \subsection{Crosstalk Analysis}
% \textcolor{red}{We have to justify the results of Table \ref{tab:res/crosstalk} if we want to include it. We decide to remove the results with Fisher Part 1, we can just say that we use more data compared to the SLT22 paper and we did not obtain significant improvament when removing examples with crosstalk in training data.}
% \begin{table*}[t]
% \centering
% \caption{Crosstalk analysis results using online/offline DPRNN with different training sets.}
% \label{tab:res/crosstalk}
% \adjustbox{max width=\textwidth}{%
% \centering
% \begin{tabular}{@{}c|c|c|c|ccccc@{}}
% \toprule
% \multirow{2}{*}{\textbf{Online}} & \multirow{2}{*}{\textbf{Training Data}} & \multirow{2}{*}{\textbf{Crosstalk}} & \multirow{2}{*}{\textbf{Size}} & \multicolumn{3}{c}{\textbf{Fisher}} & \multicolumn{2}{c}{\textbf{CALLHOME}} \\
% \cmidrule(r{4pt}){5-7} \cmidrule{8-9}
%  &  &  & & \textbf{SI-SDRi} & \textbf{DER} & \textbf{DER-LR} & \textbf{DER} & \textbf{DER-LR} \\
% \midrule
% \multirow{4}{*}{\cmark} & Fisher Part 1 & \cmark & 5728 & 22.6 & 7.4 & 7.1 & 12.0 & 11.1 \\
%  & Fisher Part 1 & \xmark & 5234 & 21.5 & 7.8 & 7.6 & 12.1 & 11.5 \\
%  & Fisher Part 1-2 & \cmark & 11577 & 22.7 & 7.5 & 7.1 & 11.6 & 11.2 \\
%  & Fisher Part 1-2 & \xmark & 10631 & \textbf{22.9} & \textbf{6.4} & \textbf{6.5} & \textbf{11.5} & \textbf{10.8} \\
%  \arrayrulecolor{black!50}\midrule
% \multirow{4}{*}{\xmark} & Fisher Part 1 & \cmark & 5728 & 22.6 & 6.7 & 6.5 & 10.8 & \textbf{9.3} \\
%  & Fisher Part 1 & \xmark & 5234 & 22.4 & 7.4 & 6.8 & \textbf{10.0} & 9.5 \\
%  & Fisher Part 1-2 & \cmark & 11577 & \textbf{23.2} & 6.3 & 6.1 & 11.2 & 10.2 \\
%  & Fisher Part 1-2 & \xmark & 10631 & 23.1 & \textbf{6.2} & \textbf{6.0} & 10.2 & 9.5 \\
% \arrayrulecolor{black}\bottomrule
% \end{tabular}
% }
% \end{table*}

\subsection{Latency Analysis with DPTNet}
We employ the online DPTNet-based SSGD to study how diarization performance changes by varying model latency from $0.1$ to $5$ s using the inter-chunk self-attention layers. In this way, the additional latency allows the separator to exploit more future context which may improve separation and diarization capabilities. We do not experiment with the online DPRNN-based model as it is not possible to modify latency while keeping the same hyper-parameters (e.g., intra-chunk size) thus performing a fair analysis.
The results are shown in Fig. \ref{fig:latency_analysis}. For both Fisher and CALLHOME test sets, there is not a clear trend meaning that DPTNet is not able to take much advantage of more future information at least till $5$~s lookahead. 
For this reason, it is always convenient to use the model with the lowest latency (i.e., $0.1$ s) or the full offline system when streaming processing is not required.


%\subsection{End-to-End Training}
% \label{ssec:res/e2e}
% %\textcolor{red}{We only report the results with online/offline DPRNN as it results the best one on the Fisher test set (the system is trained on Fisher)}.
% We focus on the DPRNN-based architecture as it performs best on the Fisher dataset both in online and offline setting. Additionally, we observe that the adaptation of DPRNN on simulated CALLHOME is more stable compared to other separators (i.e., DPTNet and Conv-TasNet) and results in a better final performance on the real CALLHOME test set.
% We experiment with  different adaptation and end-to-end strategies. Contrary to disjoint SSGD models, we also perform the \emph{full evaluation} as it can provide a better estimation of segmentation accuracy.

% Table \ref{tab:res/e2e_fisher} reports the results on the Fisher test set. The two fine-tuning strategies provided meaningful improvements over the online disjoint SSGD with leakage removal. In particular, the VAD and SSep+VAD fine-tuning strategies reduced the \emph{fair/full} DER by $12.7$/$10.3$\% and $40.8$/$37.0$\%. Both approaches helped a lot to reduce FA errors. In these cases, the use of the leakage removal algorithm was not useful. It even degraded the performances when used with the VAD fine-tuning. Note that the SSep+VAD fine-tuning strategies also outperformed the evaluation with oracle sources, meaning that the model was able to estimate ``custom'' separated sources which resulted in better diarization outputs. However, the great improvement in diarization performance was obtained at the cost of lower separation capabilities which still remained good.

% The results on the CALLHOME test set are shown in Table \ref{tab:res/e2e_callhome}. The adaptation of the separator on the simulated CALLHOME was very effective to reduce SC errors and then the overall DER. As such, we apply all the end-to-end strategies starting from this SSGD adapted on the simulated CALLHOME. The VAD fine-tuning only improved with the full evaluation. Instead, the SSep+VAD fine-tuning reduced the \emph{fair/full} DER by $7.3$/$24.9$\% over the adapted disjoint SSGD. To the best of our knowledge, the SSep+VAD approach \emph{fair} evaluation outperformed all online state-of-the-art EEND methods on the 2-speaker CALLHOME test set with significantly lower latency, i.e., $0.1$ vs $1$ or $10$ s.

% For the offline scenario of both test sets, we can make similar considerations.


\begin{figure*}[ht]
\centering
    \begin{subfigure}[b]{0.45\textwidth}
         \includegraphics[width=\textwidth]{figs/fisher_dptnet_latency.pdf}
         \caption{Fisher}
         \label{fig:latency_analysis/fisher}
     \end{subfigure}
     %\vfill
     \begin{subfigure}[b]{0.41\textwidth}
         \includegraphics[width=\textwidth]{figs/callhome_dptnet_latency.pdf}
         \caption{CALLHOME}
         \label{fig:latency_analysis/callhome}
     \end{subfigure}
     \caption{Separation and diarization results on the test sets with the online DPTNet-based SSGD by varying latency.}
     \label{fig:latency_analysis}
\end{figure*}

\begin{figure*}[ht]
\centering
    \begin{subfigure}[b]{0.48\textwidth}
         \includegraphics[width=\textwidth]{figs/fisher_ssgd_threshold_analysis.pdf}
         \caption{Fisher}
         \label{fig:ssgd_thresh_analysis/fisher}
     \end{subfigure}
     %\vfill
     \begin{subfigure}[b]{0.48\textwidth}
         \includegraphics[width=\textwidth]{figs/callhome_ssgd_threshold_analysis.pdf}
         \caption{CALLHOME}
         \label{fig:ssgd_thresh_analysis/callhome}
     \end{subfigure}
     \caption{Diarization results of the disjoint SSGD with and without leakage removal on the test sets by varying VAD threshold.}
     \label{fig:ssgd_thresh_analysis}
\end{figure*}

\begin{figure*}[ht]
\centering
    \begin{subfigure}[b]{0.48\textwidth}
         \includegraphics[width=\textwidth]{figs/fisher_e2e_threshold_analysis.pdf}
         \caption{Fisher}
         \label{fig:e2e_ssgd_thresh_analysis/fisher}
     \end{subfigure}
     %\vfill
     \begin{subfigure}[b]{0.48\textwidth}
         \includegraphics[width=\textwidth]{figs/callhome_e2e_threshold_analysis.pdf}
         \caption{CALLHOME}
         \label{fig:e2e_ssgd_thresh_analysis/callhome}
     \end{subfigure}
     \caption{Diarization results of the end-to-end SSGD with and without leakage removal on the test sets by varying VAD threshold.}
     \label{fig:e2e_ssgd_thresh_analysis}
\end{figure*}


\subsection{Leakage Removal Analysis}
\label{subsec:leak_rem}
We perform a study in order to analyze the effect of the leakage removal algorithm on disjoint and end-to-end SSGD. In particular, we employ the same DPRNN-based model by varying the VAD threshold $t_v$.

Fig. \ref{fig:ssgd_thresh_analysis} shows the results of the disjoint SSGD on the Fisher and CALLHOME datasets. As expected, lower thresholds generated higher FA and lower MS errors, whilst higher thresholds produced the opposite behavior (i.e., lower FA and higher MS). On the other hand, SC remained roughly constant. The use of leakage removal consistently reduced DER and FA when the VAD threshold is below a value depending on the dataset. When the threshold is above that value, FA errors are very close to zero and the higher degradation of MS resulted in higher DER. On the CALLHOME dataset, the leakage removal algorithm was way more effective compared to Fisher. Indeed, the VAD is only trained on Fisher data, and leakage removal was able to mitigate the mismatch between the data distributions of the two datasets.

The results of the end-to-end SSGD are depicted in Fig. \ref{fig:e2e_ssgd_thresh_analysis}. Contrary to the disjoint SSGD, the leakage removal did not improve overall diarization performance not even for low thresholds. This demonstrated that the end-to-end training was able to mitigate channel leakage as the VAD is updated with the separator output. In this case, the use of leakage removal would be a convenient choice only when having lower FA is desirable.


\subsection{ASR Evaluation}
\label{subsec:res/wer_eval}

The SSGD framework outputs both separated sources and segmentation which can be readily fed in input to a back-end ASR system. It is a great advantage over other diarization methods.
%We investigate the ASR performance feeding to a downstream ASR the estimated sources from the DPRNN models with and without leakage removal and using oracle segmentation or not.
We investigate the ASR performance when estimated sources from DPRNN models are fed to a downstream ASR, considering or not leakage removal. We also analyze the effect of the TCN-based VAD compared to oracle segmentation.
We use the pre-trained Kaldi ASPiRE ASR \citep{povey2011kaldi} model~\footnote{\url{https://kaldi-asr.org/models/m1}} and report the performance in terms of word error rate (WER).
Additionally, we also report the results obtained with input mixture and oracle sources, which represent the upper and lower bound for ASR evaluation. 

The results are reported in Table \ref{tab:res/wer_eval}.
The degradation of all SSGD systems, except for SSep+VAD fine-tuning, was very small compared to the evaluation with oracle sources. Additionally, the SSGD obtained large improvements over the mixtures. These findings confirm the effectiveness of the proposed SSep methods.
In general, the significant gap between the proposed model with the fully oracle system (i.e., oracle VAD + oracle sources) demonstrated that the VAD segmentation represents the main source of error. This finding is consistent with diarization results where the errors mainly came from MS and FA.

\begin{table}[t]
\centering
\adjustbox{max width=\textwidth}{%
\centering
\begin{tabular}{@{}l|c|ccc@{}}
\toprule
\multirow{2}{*}{\textbf{Method}} &
\multirow{2}{*}{\textbf{Online}} &
\multicolumn{2}{c}{\textbf{VAD}} \\
\cmidrule(r{4pt}){3-4}
 &  & \multicolumn{1}{l}{\textbf{TCN}} & \multicolumn{1}{l}{\textbf{Oracle}}\\
\midrule
\textit{Mixture} & \multirow{2}{*}{n.a.} & 38.74  & 30.69 & \\
\textit{Oracle sources} & & 25.44 &  19.50 &  \\
\midrule
DPRNN  & \multirow{5}{*}{\cmark} & 26.28 & \textbf{20.63}  & \\
+ Leakage removal & & 26.67  & 21.12 & \\
+ Leakage removal (seg-only) &   & 26.23 & \textbf{20.63} & \\
+ VAD fine-tuning & & \textbf{26.04} & \textbf{20.63} & \\
+ SSep+VAD fine-tuning & & 31.82 & 29.29 & \\
\midrule
DPRNN & \multirow{5}{*}{\xmark} & 25.77 &  \textbf{19.98} & \\
+ Leakage removal &  & 26.22  & 20.34 &  \\
+ Leakage removal (seg-only) &   & 25.79 & \textbf{19.98} & \\
+ VAD fine-tuning &  & \textbf{25.65} & \textbf{19.98} & \\
+ SSep+VAD fine-tuning & &   27.48 & 21.79 & \\
\bottomrule
\end{tabular}
}
\caption{WER evaluation on the Fisher test set. The best online/offline non-oracle results are reported in \textbf{bold}.}
\label{tab:res/wer_eval}
\end{table}

Although the leakage removal post-processing generally improves diarization, the filled zeros could produce less natural utterances which negatively affect the WER. On the other hand, in the proposed framework it could be only exploited for obtaining the segmentation using the non-processed estimated sources (\emph{+ Leakage removal (seg-only)}). In this latter case the DER is slightly reduced, as well for the VAD fine-tuning. Indeed, better segmentation improves performance when used with the same separated sources.
The lower separation capability of models (cfr. Table \ref{tab:res/e2e_fisher}) trained with the SSep+VAD fine-tuning resulted in higher WER as, during fine-tuning, there are no guarantees the output of the separator will be distortion-free, and even subtle distortions, while minimal in terms of SI-SDR, are known to affect ASR models significantly \citep{von2020end}. 
Adding distortion-free constraints while fine-tuning, such as multi-frame minimum variance distortion-less response \citep{tammen2021deep} is a possible future direction.  
% comments on SSep+VAD fine-tuning (offline is way better)



\section{Conclusion and Future Work}
\label{sec:conc}

In this paper, we have explored end-to-end integration of SSGD components, i.e. the speech separator and VAD. We have focused on low-latency models which allow diarization for arbitrarily long inputs in a frame-online fashion. The proposed fine-tuning strategies showed significant improvements on both Fisher and CALLHOME datasets compared to the system without fine-tuning. In particular, our best online model, i.e., SSep+VAD fine-tuning, outperformed the current state-of-the-art methods based on EEND on the CALLHOME dataset with an order of magnitude lower latency (i.e, $0.1$ s vs. $1$ s).

Additionally, we have extended our previous work by performing a more comprehensive analysis of SSGD in real-world telephone conversation scenarios.
We have experimented with another SSep model, i.e., DPTNet, which can be easily adapted to perform online inference with desired latency. Experimental results demonstrated that additional latency did not improve diarization accuracy, thus it was always convenient to use the model with the lowest latency.
We have also generated a simulated dataset for the purpose of adaptation of SSep models to CALLHOME data. Adaptation on such data provided significant improvements, especially for DPRNN.
The use of the proposed leakage removal algorithm was investigated both for disjoint and SSep+VAD fine-tuned SSGD. Experiments clearly showed that the post-processing algorithm was very effective to reduce both FA and DER in disjoint SSGD. However, when used in conjunction with fine-tuned models it did not improve the overall performance as the models learned to handle leaked segments directly from data.
Finally, we have demonstrated that estimated sources generated by SSGD could be readily fed in input to an ASR system. The ASR performance largely improved over the input mixture and in some instance was even close to the one obtained with oracle sources.
However, the end-to-end integration led to significant ASR performance degradation. 

%Future research directions include an extension to domains with a higher number of speakers and longer duration (e.g., meeting scenarios). 
In future works, the SSGD framework will be extended to domains in which a higher number of speakers is typically involved (e.g., meeting scenarios).
This will need likely the development of new techniques for speech separation, able to track efficiently many speakers for arbitrarily long inputs.
Additionally, novel strategies will be investigated to mitigate the separation/ASR degradation when using end-to-end fine-tuned SSGD models. This includes distortion-less constraints and/or continual learning approaches. 

\section{Acknowledgements}
\label{sec:ack}
This work has been supported by the AGEVOLA project (SIME code 2019.0227), funded by the Fondazione CARITRO.

%% Loading bibliography style file
%\bibliographystyle{model1-num-names}
\bibliographystyle{cas-model2-names}

% Loading bibliography database
\bibliography{refs}


%\vskip3pt

% \bio{figs/pic1}
% Author biography with author photo.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% \endbio

\end{document}

