{
    "arxiv_id": "2303.17589",
    "paper_title": "Polarity is all you need to learn and transfer faster",
    "authors": [
        "Qingyang Wang",
        "Michael A. Powell",
        "Ali Geisa",
        "Eric Bridgeford",
        "Joshua T. Vogelstein"
    ],
    "submission_date": "2023-03-29",
    "revised_dates": [
        "2023-03-31"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG",
        "cs.CV",
        "cs.NE",
        "q-bio.NC"
    ],
    "abstract": "Natural intelligences (NIs) thrive in a dynamic world - they learn quickly, sometimes with only a few samples. In contrast, Artificial intelligences (AIs) typically learn with prohibitive amount of training samples and computational power. What design principle difference between NI and AI could contribute to such a discrepancy? Here, we propose an angle from weight polarity: development processes initialize NIs with advantageous polarity configurations; as NIs grow and learn, synapse magnitudes update yet polarities are largely kept unchanged. We demonstrate with simulation and image classification tasks that if weight polarities are adequately set $\\textit{a priori}$, then networks learn with less time and data. We also explicitly illustrate situations in which $\\textit{a priori}$ setting the weight polarities is disadvantageous for networks. Our work illustrates the value of weight polarities from the perspective of statistical and computational efficiency during learning.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.17589v1"
    ],
    "publication_venue": null
}