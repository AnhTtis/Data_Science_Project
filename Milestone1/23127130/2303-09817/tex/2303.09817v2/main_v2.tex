\documentclass[preprint,5p,times,sort&compress]{elsarticle_v2} % 1p 3p 5p


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\def\bz{\mathbf{z}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bj{\mathbf{j}}
\def\bX{\mathbf{X}}
\usepackage{lineno}    % line numbers
\PassOptionsToPackage{hyphens}{url}
\usepackage{hyperref}  % url
\hypersetup{colorlinks=true,breaklinks=true}
\usepackage{xcolor} % blue color
\usepackage{booktabs} % table: rules
\usepackage{multirow}
\usepackage{siunitx} % +-
\usepackage{array} % table: wr column type
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\journal{Artificial Intelligence in Medicine}

\begin{document}

\begin{frontmatter}

\title{Interpretable machine learning for time-to-event prediction in medicine and healthcare}

\author[uw]{Hubert Baniecki}\ead{h.baniecki@uw.edu.pl}
\author[pw]{Bartlomiej Sobieski}
\author[pw,wum]{Patryk Szatkowski}
\author[pw,wum]{Przemyslaw Bombinski}
\author[uw,pw]{Przemyslaw Biecek}

\affiliation[uw]{organization={University of Warsaw}, city={Warsaw}, country={Poland}}
\affiliation[pw]{organization={Warsaw University of Technology}, city={Warsaw}, country={Poland}}
\affiliation[wum]{organization={Medical University of Warsaw}, city={Warsaw}, country={Poland}}

\begin{abstract}
Time-to-event prediction, e.g. cancer survival analysis or hospital length of stay, is a highly prominent machine learning task in medical and healthcare applications. However, only a few interpretable machine learning methods comply with its challenges. To facilitate a comprehensive explanatory analysis of survival models, we formally introduce time-dependent feature effects and global feature importance explanations. We show how post-hoc interpretation methods allow for finding biases in AI systems predicting length of stay using a novel multi-modal dataset created from 1235 X-ray images with textual radiology reports annotated by human experts. Moreover, we evaluate cancer survival models beyond predictive performance to include the importance of multi-omics feature groups based on a large-scale benchmark comprising 11 datasets from The Cancer Genome Atlas (TCGA). Model developers can use the proposed methods to debug and improve machine learning algorithms, while physicians can discover disease biomarkers and assess their significance. We hope the contributed open data and code resources facilitate future work in the emerging research direction of explainable survival analysis. 
\end{abstract}

\begin{keyword}
explainable AI \sep survival analysis \sep variable importance \sep radiomics \sep time-dependent explanation \sep interpretability
\end{keyword}

\end{frontmatter}


% \linenumbers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:introduction}

Advances in the development of machine learning~(ML) algorithms enable successful deployment of artificial intelligence (AI) systems in medicine and healthcare~\citep{rajpurkar2022ai,zhou2023foundation}. Nevertheless, the black-box nature of complex predictive models inhibits their adoption among stakeholders, including physicians and patients. Numerous interpretable machine learning~(IML) algorithms~\citep{stiglic2020interpretability,molnar2020interpretable,biecek2021explanatory}, contained in a broader field of explainable AI~\citep{holzinger2022xai,ooge2022explaining,combi2022manifesto}, have been proposed to overcome this opaqueness debt and effectively increase trust in automated decisions (or find out reasons to distrust thereof).

Only recently, the IML toolbox expanded beyond classification and regression to include post-hoc explanation methods suited for interpreting time-to-event prediction~\citep{kovalev2020survlime,wang2021counterfactual, rad2022extracting,utkin2022survnam,krzyzinski2023survshap}. These are especially important for ML applied in medicine and healthcare, of which survival analysis and survival models are a big part~\citep{terminassian2024explainable,langbein2024interpretable}. 
Notably, including a time dimension in explanations is crucial to accurately interpret the predicted survival function~\citep{krzyzinski2023survshap}. Omitting the time dimension is a major limitation of popular explanation methods like permutation feature importance~\citep{breiman2001random,fisher2019all}, individual conditional expectation~\citep{goldstein2015peeking} and partial dependence plots~\citep{friedman2001greedy}.

To this end, this paper extends our previous work on IML for survival analysis~\citep{baniecki2023hospital} with the following contributions: 
\begin{enumerate}
    \item In Section~\ref{sec:methods}, we formally introduce time-dependent global feature importance and time-dependent feature effects, i.e. model-agnostic explanation methods adapted to the task of time-to-event prediction (Figure~\ref{fig:abstract}).
    \item In Section~\ref{sec:materials-tlos}, we describe in detail the novel dataset for predicting hospital length of stay from X-ray images. We use time-dependent explanations to analyse a ML model predicting hospital length of stay, which exhibits harmful bias towards medical devices appearing in X-ray images.
    \item In Sections~\ref{sec:materials-tcga}~\&~\ref{sec:results-tcga}, we demonstrate the general applicability of the proposed methods through a multi-modal medical use-case illustrating the added value of IML for time-to-event prediction in survival analysis. 
\end{enumerate}
We supplement the paper with an original open-source implementation of time-dependent feature effects and global feature importance in Python.\footnote{\url{https://github.com/mi2datalab/interpret-time-to-event}}


\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{figures_v2/tlos_v2_abstract}
    \caption{Post-hoc explanation methods allow for finding biases in machine learning models predicting hospital length of stay, and evaluating cancer survival models beyond performance to include the importance of multi-omics feature groups.}
    \label{fig:abstract}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related work}

\subsection{Interpretable machine learning for survival analysis}

The majority of proposed ML explanation methods omit considering time-to-event analysis~\citep{molnar2020interpretable,biecek2021explanatory}. Yet, ML survival models become more complex with the aim to increase their predictive performance~\citep{polsterl2016survival,jing2019deep,hao2022survivalcnn}. One way of increasing the transparency of predictive outcomes is using the so-called ``interpretable'' model algorithms~\citep{cho2023interpretable,jiang2023decaf,xu2023coxnam}, which resemble simpler interpretations alike statistical models, e.g. Cox proportional hazards. Instead, we relate to research on post-hoc model-agnostic explanations that can be applied to a broad spectrum of black-box learning algorithms. 

SurvLIME~\citep{kovalev2020survlime} is an adaptation of local interpretable model-agnostic explanations~\citep{ribeiro2016should} to survival analysis. In~\citep{wang2021counterfactual}, a counterfactual explanation for predicting the survival of cardiovascular ICU patients is proposed. In~\cite{rad2022extracting}, explanations are developed based on a simple interpretable surrogate model to approximate the decisions of a black-box survival model. In~\citep{utkin2022survnam}, SurvNAM extends the SurvLIME method using general additive models to estimate both local and global explanations. Most recently, SurvSHAP(t)~\cite{krzyzinski2023survshap} is the first method to include the time dimension in explanations of time-to-event prediction. But, it only considers \emph{local} feature attributions and importance.

Currently underdeveloped are time-dependent global methods like permutation feature importance~\citep{breiman2001random,fisher2019all} or partial dependence plots~\citep{friedman2001greedy}, which result from the aggregation of individual conditional expectations~\citep{goldstein2015peeking}. Moreover, none of the explanation methods considers grouping features~\citep{au2022grouped}, which is useful in analysing multi-modal medical data.

While evaluating explanation methods through heuristic performance metrics~\citep{komorowski2023towards} is relevant to theoretical research in AI, our work leans towards the applied side of IML. For example, time-dependent explanations can be used to validate biomarkers of cancer survival~\citep[][figure~3]{donizy2023ki67}. As we have previously shown that juxtaposing different explanation methods increases the accuracy and confidence of human decision making~\citep{baniecki2023grammar}, this paper aims to further facilitate comparative visualization as a useful tool in interpreting ML models in medical and healthcare applications.


\subsection{Predicting and explaining hospital length of stay} 

In Section~\ref{sec:results-tlos}, we demonstrate the applicability of IML methods to explaining hospital length of stay (LoS) predictions as an enabler towards trustworthy human-AI collaboration in radiomics. Predicting patients' hospital LoS is a challenging task supporting the day-to-day decisions of medical doctors and nurses~\cite{huang2013length}. For example, accurate LoS prediction can increase hospital service efficiency, cutting costs and improving patient care. Historically, white-box statistical learning methods were used to estimate the anticipated LoS~\cite{chaou2017predicting}. These provide clear reasoning behind the prediction, which is especially important in medical applications requiring stakeholders to comprehend ``Why?''~\cite{rudin2019stop}. Nowadays, advancements in machine and deep learning for healthcare provide valuable improvements in the performance of predicting LoS~\cite{muhlestein2019predicting,zhang2020combining,wen2022time}. The natural drawback of using not inherently interpretable black-box models is their complex nature~\cite{biecek2021explanatory,rudin2019stop}. Indeed, a recent systematic review on the exact topic of hospital LoS prediction concludes with a concrete statement that there are no studies on the explainability of black-box models predicting LoS~\cite{stone2022systematic}, a matter of high importance for diverse stakeholders involved in this healthcare process. 

Applying AI through ML to predict hospital LoS from data is broadly studied as it has a high potential to support decision making in healthcare~\cite{stone2022systematic}. In~\cite{huang2013length}, LoS is predicted based on information from clinical treatment processes, i.e. sequences of time-point hospital events. In~\cite{chaou2017predicting}, hospital events like the number of tests and time of arrival are aggregated to quantify their importance in patient discharge. We base our analysis on clinical features impacting physician understanding of the patient's severity instead. In~\cite{muhlestein2019predicting}, ML models predict LoS after brain surgery using clinical features in a single-value regression task. This results in simple feature importance and effects explanations without the valuable time dimension. In~\cite{zhang2020combining}, deep learning models classify the patient staying in a hospital for longer than 7 days based on multi-modal data combining unstructured notes and tabular features. In~\citep{wen2022time}, various machine and deep learning survival models are benchmarked for predicting LoS of COVID-19 patients. Both studies focus on predictive performance without explaining the models. For a broader overview of works on LoS prediction, we refer the reader to~\citep{stone2022systematic}.

Contrary to the above-mentioned studies, we specifically use raw X-ray images to analyze the predictive power of radiomics features and explain the prediction in a time-dependent manner. To achieve it, we rely on \texttt{pyradiomics} -- the state-of-the-art radiomics feature extraction tool~\cite{van2017computational} and \texttt{survex} -- a toolbox for explaining ML survival models~\cite{spytek2023survex}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods}\label{sec:methods}

In this section, we formulate an adaptation of IML methods used to explain time-to-event predictions in Section~\ref{sec:results}. Unlike classification or regression tasks, ML survival models output a function in time. 

Let us denote by $f_t(\bx)$ a prediction for an observation $\bx$ at timestep $t$, where $f$ is the function estimated by the model and $t$ ranges from $t_0$ to $T$. Note that $f$ is not restricted to represent the survival function or cumulative hazard function, as the formulation of the following methods is correct in both cases. An observation $\bx$ is a vector of the realizations of $p$ features from a dataset $\bX$ with a total number of $n$ observations. By $\bx^{j|=z}$, we denote an observation in which the value of the $j$-th feature is replaced with $z$ and by $\bX^{j|=z}$ we refer to an entire dataset in which the value of the $j$-th feature has been set to $z$ for all observations. By $\bX^{\ast j}$, we refer to a modified dataset $\bX$ where the $j$-th feature is randomly permuted and extend this notation to $\bX^{\ast \bj}$ for the case when the set $\bj=\{j_1, ..., j_k\}$ of features is randomly permuted (without breaking the intra-group dependencies).


\subsection{Time-dependent feature effects}

Individual conditional expectation plot~\citep{goldstein2015peeking}, also called the ``What-if?'' plot or ``Ceteris-paribus'' profile~\citep{biecek2021explanatory}, visualizes the local effect of a particular feature on the model's prediction. We define \emph{time-dependent individual conditional expectation} as
\begin{equation}
    \textsc{ice}_t(f, \bx, j, \bz) = \{ f_t(\bx^{j|=z}) \}_{z\in \bz}.
\end{equation}
Specifically, the explanation shows a set of predictions $f$ at time-step $t$ for the observation $\bx$, in which the $j$-th feature spans over a set of its values $\bz$. For example, for a binary feature $j$ we have $\bz = \{0{,} 1\}$ and the explanation can be visualized as two survival functions over a set of time steps $(t_1, \ldots, t_m)$. Intuitively, it answers the local question of ``What would the model predict for observation $\bx$ if the value of feature $j$ was $1$ instead of $0$?''

Partial dependence plot~\citep{friedman2001greedy} aims to identify the global effect of a particular feature on the expected value of predictions. We define the \emph{time-dependent partial dependence plot} as
\begin{equation}
    \textsc{pdp}_t(f, \bX, j, \bz) = \left\{ \operatorname{E}_{\mathcal{X}^{-j}} \left[ f_t(\bX^{j|=z}) \right] \right\}_{z\in \bz},
\end{equation}
where $\mathcal{X}$ represents a $p$-dimensional random variable from which distribution the dataset $\bX$ was collected. The symbol $\mathcal{X}^{-j}$ denotes its modification obtained by removing the $j$-th component, and $\bX^{j|=z}$ is an extension where the $j$-th component's value of $\bX$ is replaced with constant value $z$. In practice, the expected value in $\textsc{pdp}_t$ is estimated as
\begin{equation}
    \widehat{\textsc{pdp}}_t(f, \bX, j, \bz) = \left\{ \frac{1}{n} \sum_{\bx \in \bX}  f_t(\bx^{j|=z}) \right\}_{z\in \bz},
\end{equation}
i.e. an average of local $\textsc{ice}_t$ explanations is calculated over a set of $n$ observations. Note that alternative estimators of global feature effects~\citep{apley2020visualizing,gkolemis2023dale} can be similarly adapted to interpret time-to-event predictions.


\subsection{Time-dependent global feature importance}

One approach to quantify each feature's importance is based on how much predictive power it contributes~\citep{covert2020understanding}. We can measure time-dependent global feature importance as an aggregation of local SurvSHAP(t), which estimates feature attributions $\phi_t(\bx, j)$ with either sampling or kernel approximation (refer to~\citep{krzyzinski2023survshap} for details). We define global \emph{SurvSHAP(t) feature importance} as
\begin{equation}
    \textsc{sfi}_t(f, \bX, j) = \frac{1}{n} \sum_{\bx \in \bX} \mid \phi_t\left( \bx, j \right) \mid,
\end{equation}
i.e. an average feature attribution over a set of observations $\bX$, e.g. a validation set.

Alternatively, permutation feature importance~\citep{breiman2001random,fisher2019all} measures the global influence of a particular feature on the model's performance, e.g. Brier score. Let us denote by $\mathcal{L}_t(f, \bX, \by)$ a performance metric (or a loss function) for a survival model at time-step $t$, where $\by$ is a target feature corresponding to dataset~$\bX$. We define \emph{time-dependent permutation feature importance} as
\begin{equation}\label{eq:pfi}
    \textsc{pfi}_t(f, \bX, j, \mathcal{L}, \by) = \frac{1}{b} \sum_{i=1}^{b} \left( \mathcal{L}_t(f, \bX, \by) - \mathcal{L}_t(f, \bX^{\ast j_i}, \by) \right),
\end{equation}
which effectively measures the impact on the prediction's performance of breaking the dependency between feature $j$ and target feature; aggregated over $b$ permutations of feature $j$. Note that $\textsc{pfi}_t$ should be estimated on the validation set as it requires access to ground truth~$\by$. In practice, we visualize the absolute time-dependent importance $\mid \textsc{pfi}_t(f, \bX, j, \mathcal{L}, \by) \mid$ to be consistent between decreasing and increasing performance metrics, e.g. Brier score vs. AUC. In some applications, it might be useful to consider relative feature importance using $\frac{\mathcal{L}_t(f, \bX^{\ast j}, \by)}{\mathcal{L}_t(f, \bX, \by)}$ instead of $\mathcal{L}_t(f, \bX, \by) - \mathcal{L}_t(f, \bX^{\ast j}, \by)$ in Equation~\ref{eq:pfi}.

In many scenarios, data comprises features that can be meaningfully grouped, e.g. in a multi-modal case where features come from varied modalities. It is then useful to consider grouped feature importance measures~\citep{au2022grouped}. We define \emph{time-dependent grouped permutation feature importance} as
\begin{equation}
    \textsc{gpfi}_t(f, \bX, \bj, \mathcal{L}, \by) = \frac{1}{b} \sum_{i=1}^{b} \left( \mathcal{L}_t(f, \bX, \by) - \mathcal{L}_t(f, \bX^{\ast \bj}, \by) \right).
\end{equation}
That is, we analyze the influence of breaking the dependency of features from a particular group $\bj$ and all other features while preserving the intra-group relationships. We acknowledge that grouping features in Shapley-based explanations is possible~\citep{au2022grouped}, but leave that as future work. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Materials: data and machine learning models}\label{sec:materials}

We show the applicability of time-dependent explanations in two applications. Section~\ref{sec:materials-tlos} describes in detail the process of ML analysis from data acquisition through model selection towards explaining time-to-event LoS predictions. Section~\ref{sec:materials-tcga} focuses more on a large-scale approach to biomarker discovery using the introduced explanation methods. We will discuss the obtained results in Sections~\ref{sec:results-tlos}~\&~\ref{sec:results-tcga}, respectively.


\subsection{Bias in predicting hospital LoS using X-ray images}\label{sec:materials-tlos}

IML methods for time-to-event prediction can be valuable for explaining hospital LoS to humans. We consider a setting where LoS is predicted using time-to-event survival models instead of single-value time estimation with regression models or time-span classification. Besides giving a more holistic prediction, survival analysis naturally allows for censored observations in data, e.g. a patient was discharged from one hospital and moved to another with further information missing. 

\paragraph{The \textsc{tlos} dataset} A unique multi-modal dataset used in this study is created based on image, text and tabular data of 1235 patients from the University Clinical Centre, Medical University of Warsaw, Poland. It allows us to answer the question of interest: \textbf{To what extent can the patient's LoS in a hospital be predicted using an X-ray image?} The \emph{target feature} is the time between the patient's radiological examination and hospital discharge (in days, $min=1$, $median=7$, $mean=13.73$, $max=330$). Due to the high skewness of the time distribution, we model the logarithm of time in practice. About 20\% of outcomes are right-censored, e.g. due to death. The dataset includes 749 (61\%) male and 486 (39\%) female patients of rather evenly distributed age (in full years, $min=0$, $mean=38$, $median=37$, $max=90$). 

Figure~\ref{fig:dataset} shows a high-level workflow of creating the \textsc{tlos} dataset. Each radiologic exam consists of an X-ray image with a written report stating observable features, e.g. pathological signs, lung lesions and pleural abnormalities, but also the occurrence of medical devices on the image, e.g. tubing and electrocardiographic leads. We obtained X-ray images of high quality in the raw DICOM format, which were then converted to PNG and resized to 420x420 before further processing. We manually annotate each textual report into 17 interpretable binary features informing whether the pathology occurs or is absent. Note that we sampled patients at random and capped their quantity after reaching the reasonable capacity of human annotators. Moreover, we automatically extract 76 numerical features from the image using the \texttt{pyradiomics} tool~\cite{van2017computational}. It computes various statistics based on an image and a lung segmentation mask, e.g. various aggregations of the grey-level co-occurrence matrix. A detailed description of algorithm-extracted features from \texttt{pyradiomics} is available at \url{https://pyradiomics.readthedocs.io/en/v3.0.1/features.html}. A pretrained CE-Net~\cite{gu2019net} model was used to obtain lung segmentation masks inputted to \texttt{pyradiomics}. We treat it as a reasonable baseline approach while acknowledging that segmentation errors will inevitably contribute to errors in algorithm-extracted features. The described procedure leads to obtaining four feature sets referred to as:
\begin{itemize}
    \item \emph{baseline} (number of features: $d=2$) -- includes the patient's age and sex,
    \item \emph{human-annotated} ($d=2+17$) -- includes \emph{baseline} and pathology occurrences,
    \item \emph{algorithm-extracted} ($d=2+76$) -- includes \emph{baseline} and radiomics features, 
    \item \emph{all features} ($d=2+17+76$).
\end{itemize}


\paragraph{Details of human annotation} The original raw dataset included X-ray examinations, i.e. images with textual radiology reports, which we anonymized extensively. First, we developed a project-specific ontology of chest pathologies that can be observed on X-ray images. It compiles information from the relevant radiology literature~\citep{hansell2008fleischner}, existing ontologies like RadLex~\citep{radlex}, and popular X-ray databases available online (CheXpert~\citep{irvin2019chexpert}, MIMIC~\citep{johnson2023mimic}, VinDr-CXR~\citep{nguyen2022vindr}). Second, we identified problems in our reports related to, e.g. ambiguous interpretations of phrases in the radiological reports, differences in the quality and length of descriptions prepared by different physicians and for different types of examinations (like follow-up or comparison to previous examinations). We fixed them by updating the ontology in accordance with the domain knowledge. As a result, a set of 35 classes was obtained. Two board-certified radiologists annotated whether a given class occurs~(1) or not~(0) in a textual report from the X-ray examination, which was consistent. After annotation, we chose the 17 most common features, i.e. with more than 3\% occurrence among patients, for further analysis in this study. Although we are yet unable to share X-ray images and textual reports due to privacy concerns, we share the preprocessed \textsc{tlos} dataset with further documentation and code to reproduce our results (see Data and code availability). 

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figures_v2/dataset}
    \caption{Schematic workflow of creating the \textsc{tlos} dataset.}
    \label{fig:dataset}
\end{figure}


\paragraph{Models}  We first use all features to compare various ML survival models in predicting LoS using X-ray images and then evaluate the impact of particular feature sets on the best models' predictive performance. We perform a comprehensive benchmark of relevant learning algorithms available in the \texttt{mlr3proba} toolbox~\cite{sonabend2021mlr3proba}: a decision tree (CTree), gradient boosting decision trees (GBDT), two implementations of random survival forest (Ranger \& RF-SRC), Cox proportional hazards (CoxPH), and two implementations of neural networks (DeepSurv \& DeepHit). For details of the models' hyperparameters refer to~\ref{app:hyperparameters}.  We use 10 repeats of 10-fold cross-validation and assess the predictive performance of survival models with two metrics: C-index, where higher means better performance and the baseline value of a random model equals 0.5, and integrated Brier score (IBS), where lower is better and the baseline value of a random model equals 0.25. The evaluation protocol mimics a benchmark of survival prediction methods described in~\cite{herrman2021benchmark}. Moreover, we report integrated AUC (IAUC) for the models, for which it is implemented (CoxPH and GBDT).


\subsection{Explainable multi-omics for cancer survival prediction}\label{sec:materials-tcga}

To showcase the versatility of the introduced methods for explaining ML survival models, in Section~\ref{sec:results-tcga}, we apply them to interpret predictions of cancer survival. Our goal is to analyse the importance of features grouped in varied modalities, which is valuable for model developers and medical experts aiming to find the best type of multi-omics biomarkers. 

\paragraph{The TCGA benchmark} To this end, we rely on a large-scale multi-modal benchmark consisting of standardised datasets derived from The Cancer Genome Atlas (TCGA) database~\citep{herrmann2021largescale}. Each dataset considers a different cancer survival target feature, relevant clinical features and 4 types of high-dimensional molecular features, e.g. miRNA expressions. The target feature is right-censored. Refer to~\citep[][table 2]{herrmann2021largescale} for a detailed summary of the datasets. Our goal is to create lower-dimensional multi-modal datasets appropriate for the purpose of IML. Thus, we follow the work of~\citep{bommert2022benchmark}, where a benchmark of 14 feature selection methods was conducted on 11 out of 18 datasets with more than 50 events. For each dataset, we apply the best-performing feature selection method, i.e. variance filter~\citep{bommert2022benchmark}, to choose the 5 most relevant features per each modality (including clinical features). This procedure results in an approachable benchmark of 11 datasets, each with 25 multi-modal features. 

\paragraph{Models} We use RSF on all datasets as a robust baseline model. Models are evaluated with the IBS metric based on 10-fold cross-validation. Importance scores are aggregated in feature groups representing various omics modalities and averaged over test sets in cross-validation folds. Feature effect is estimated based on a separate RSF trained on the whole dataset, which aims to mimic the workflow of testing the significance of parameters in a CoxPH model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}\label{sec:results}

\subsection{Bias in predicting hospital LoS using X-ray images}\label{sec:results-tlos}

\paragraph{Model performance} Table \ref{tab:model_comparison} presents the results where models are sorted by an average C-index. First, note that most DeepHit models did not converge and provide random predictions; thus, DeepHit is removed from the comparison. We observe that, on average, the best algorithm is a gradient boosting decision tree (0.668 C-index, 0.117 IBS) with a random survival forest in second. Interestingly, the interpretable and widely-used CoxPH model performs worse (0.645 C-index, 0.127 IBS). Overall, neural networks have a hard time learning meaningful models. Comparing the raw performance values with results from related work leads to the conclusion that \emph{predicting the patient's LoS from an X-ray image is indeed possible}, but challenging. 

\begin{table}
    \centering
    \begin{tabular}{l|ccc}
        \toprule
        \textbf{Model} & \textbf{C-index} $\uparrow$ & \textbf{IBS} $\downarrow$ & \textbf{IAUC} $\uparrow$ \\
        \midrule
        DeepSurv & $.628_{\pm .036}$ & $.127_{\pm .016}$ & -- \\
        Ranger & $.639_{\pm .027}$ & $.140_{\pm .010}$ & -- \\
        CTree & $.641_{\pm .030}$ & $.127_{\pm .015}$ & -- \\
        CoxPH & $.645_{\pm .029}$ & $.127_{\pm .013}$ & $.678_{\pm .056}$ \\
        RF-SRC & $.651_{\pm .030}$ & $.119_{\pm .013}$ & -- \\
        GBDT & $.668_{\pm .030}$ & $.117_{\pm .014}$ & $.715_{\pm .061}$ \\
        \bottomrule
    \end{tabular}
    \caption{Benchmark of ML survival models predicting LoS using all features from X-ray images sorted by C-index. Most of the DeepHit models did not converge and provide random predictions; thus, we removed it from the comparison. Based on 10 repeats of 10-fold cross-validation, the GBDT algorithm performs best on average, i.e. achieves 0.668 C-index and 0.117 IBS. For broader context, we report IAUC for CoxPH and GBDT, which are the only algorithms to implement this metric. Detailed results are in~\ref{app:performance}.}
    \label{tab:model_comparison}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{ll|ccc}
        \toprule
        \textbf{Model} & \textbf{Features} & \textbf{C-index} $\uparrow$ & \textbf{IBS} $\downarrow$ & \textbf{IAUC} $\uparrow$ \\
        \midrule
    \multirow{4}{*}{CoxPH} & baseline & $.567_{\pm .036}$ & $.129_{\pm .013}$ & $.570_{\pm .060}$ \\
        & AE & $.639_{\pm .026}$ & $.127_{\pm .014}$ & $.677_{\pm .051}$ \\
        & HA & $.642_{\pm .028}$ & $.120_{\pm .014}$ & $.676_{\pm .056}$ \\
        & all feat. & $.645_{\pm .029}$ & $.127_{\pm .013}$ & $.678_{\pm .056}$ \\
        \midrule
    \multirow{4}{*}{GBDT} & baseline & $.582_{\pm .037}$ & $.126_{\pm .013}$ & $.599_{\pm .064}$ \\
        & AE & $.649_{\pm .029}$ & $.121_{\pm .014}$ & $.702_{\pm .057}$ \\
        & HA & $.652_{\pm .026}$ & $.119_{\pm .014}$ & $.686_{\pm .054}$ \\
        & all feat. & $.668_{\pm .030}$ & $.117_{\pm .014}$ & $.715_{\pm .061}$ \\
        \bottomrule
    \end{tabular}
    \caption{Benchmark of four feature sets -- baseline, algorithm-extracted (AE), human-annotated (HA), and all features -- in predicting LoS using GBDT black-box and CoxPH model (white-box). We report average and standard deviation of performance based on 10 repeats of 10-fold cross-validation. Detailed results with \emph{p}-values are in~\ref{app:performance}.}
    \label{tab:blackbox_whitebox_comparison}
\end{table}

\paragraph{Feature performance} We first aim to tackle the performance--interpretability tradeoff in ML for medicine~\cite{rudin2019stop}. Based on the benchmark results reported in Table~\ref{tab:model_comparison}, we choose the best black-box algorithm (GBDT) to compare with CoxPH -- the widely-adopted interpretable approach to time-to-event analysis. Note that one can consider the human-annotated features as interpretable and algorithm-extracted features as a black-box approach, i.e. training the CoxPH model on algorithm-extracted features is not necessarily a white-box model. We test the two algorithms on four feature sets using the same cross-validation scheme and performance metrics as the previous benchmark. Table~\ref{tab:blackbox_whitebox_comparison} presents the results where feature sets are sorted by an average C-index. We observe that both algorithm-extracted and human-annotated features include valuable information for predicting LoS. The only significant difference (on average) between the two sets is for the CoxPH algorithm evaluated with IBS. For GBDT, using all features results in the best performance, while for CoxPH, increasing the number of features leads to the same or worse performance due to the curse of dimensionality. In summary, the best-performing interpretable algorithm is CoxPH trained on human-annotated features (0.642 C-index, 0.120 IBS, 0.676 IAUC), and the black-box approach is GBDT trained on all features (0.668 C-index, 0.117 IBS, 0.715 AUC). 
The difference in C-index and IAUC is significant (\emph{p}-value $< 0.001$). 
Although this difference may be neglectable in reality, CoxPH is limited by the number of features, which now rapidly increases in medical applications, e.g. in radiology where tools for feature extractions become more prevalent~\citep{chaou2017predicting}. Moreover, annotating images by humans is costly, and GBDT remains more efficient with algorithm-extracted features.

\paragraph{Explaining length of stay predictions to humans} A classic approach to the explanatory analysis of time-to-event models involves analysing the significance of the CoxPH model's coefficients. For a broader context, we use all observations in data to fit CoxPH models to the four feature sets. We report features with significant coefficients in~\ref{app:significance}, effectively serving as a list of features important to predicting LoS. \textbf{The main limitation of this approach is explaining only a particular learning algorithm that performs worse in the predictive task, as in our case.} Therefore, we propose to use model-agnostic explanations to interpret the predictions of any black-box survival model predicting LoS in general.  We extend the explanatory model analysis framework~\citep{baniecki2023grammar} to include time-dependent explanations. For a concrete example, we use all observations in data and fit a GBDT model to the human-annotated feature set. 

Figure~\ref{fig:examples} shows two X-ray image samples: one with lung diseases and another with healthy lungs and medical devices. Figure~\ref{fig:blackbox_explanations} presents four complementary local and global explanations, providing a multi-faceted understanding of the black-box model.  We first interpret the particular prediction for a 78-year-old male patient with parenchymal opacification in the lungs and possibly also pleural effusion. SurvSHAP(t) attributes high importance to the occurrence of parenchymal opacification, but also the absence of medical devices on the chest X-ray. \textbf{In fact, the latter decreases the predicted probability of longer LoS as X-rays with observable medical devices usually indicate a severe patient condition.} This image feature is a potentially harmful bias in data that later propagates to a predictive model. Next, we perform a What-if analysis for the ambiguous pleural effusion feature to explain the uncertainty in LoS prediction conditioned on this feature. Over the 60 days since the X-ray examination, the probability of staying in a hospital is increased by 0.125 when a pleural effusion occurred (for this patient).

The bottom of Figure~\ref{fig:blackbox_explanations} presents global explanations of the model's behaviour: feature importance and effects, also referred to as partial dependence. We obtain time-dependent feature importance by aggregating absolute SurvSHAP(t) values for a representative subset of patients. The visualization indicates that the model finds the occurrence of medical devices as a proxy for LoS, which may be correlated with the patient's condition. It is an evident bias in data, which can be an enormous risk for model deployments in medicine and healthcare (see e.g. work on racial bias in AI recognition of medical images~\citep{gichoya2022ai}). Other important radiomics features include pleural effusion, age, parenchymal opacification, cardiac silhouette enlargement and lung mass. For each of these pathologies, a partial dependence plot explains its aggregated effect on the LoS prediction. Specifically, when medical devices occur on an image, the probability of staying in a hospital 20 days after the X-ray examination is increased by 0.25.

As illustrated here, incorporating time-dependent explanations of ML models predicting LoS into the existing decision support systems can provide useful information for physicians. The presented approach is general and widely applicable to other time-to-event medical use cases.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\columnwidth]{figures_v2/tlos_v2_xray-compressed}
    \caption{Exemplary X-ray images of (\textbf{left}) lung disease in an adult patient and (\textbf{right}) healthy children's lungs with visible medical devices.}
    \label{fig:examples}
\end{figure}


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures_v2/tlos_v2_explanations.pdf}
    \caption{Complementary time-dependent explanations of the GBDT model trained on age, sex, and human-annotated radiomics features. \textbf{Top left}: SurvSHAP(t) local explanation for a selected patient informing about the 6 most important features and their effect on the predicted LoS on each day since X-ray examination. \textbf{Top right}: What-if analysis for the same patient and the selected ambiguous feature, informing about how the predicted LoS would change upon the change in feature value. \textbf{Bottom left}: Feature importance global explanation based on aggregated SurvSHAP(t) values for a subset of patients. It informs about the 6 most important features overall. \textbf{Bottom right}: Partial dependence global explanation for the most important feature informing about its effect on the predicted LoS.}
    \label{fig:blackbox_explanations}
\end{figure*}


\subsection{Explainable multi-omics for cancer survival prediction} \label{sec:results-tcga}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/tcga_gpfi_cv.png}
    \caption{Time-dependent permutation importance of feature groups representing different omics modalities. IBS metric values are in brackets. Each subplot comprises distinct results from a cross-validation of a random survival forest predicting the survival of a particular cancer type from TCGA. Positive importance values over time indicate a high influence of features on the accuracy of time-to-event prediction. We observe that clinical features are the most important for predicting BLCA, BRCA, HNSC, LUAD, OV, and PAAD; CNV features influence the prediction in LUSC and OV; miRNA features are important in KIRC, LGG and LUSC; while RNA features are useful in predicting KIRC and SKCM. In some tasks, particular modalities add noise to the modelling process, as shown by negative importance values over time. For example, a random survival forest predicting PAAD performs the worst on average, practically random. It is indicated by the integrated Brier score of $0.23\pm0.06$, as well as in the time-dependent explanation showing most of the features as unimportant noise.}
    \label{fig:tcga_gpfi_cv}
\end{figure*}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{figures/tcga_pdp_luad_clinical.png}
    \caption{Relative effects of binary clinical features in predicting the survival of lung adenocarcinoma (LUAD). We visualise the relative difference between effects for feature values 1 and 0, where the ribbon denotes the standard deviation of the estimator. Early pathologic stage (1B) of cancer increases time-to-event predicted by random survival forest. Other clinical features seem unimportant.}
    \label{fig:tcga_pdp_luad}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/tcga_pdp_kirc_clinical.png}
    \caption{Relative effects of binary clinical features in predicting the survival of kidney renal clear (KIRC) cell carcinoma. We visualise the relative difference between effects for feature values 1 and 0, including standard deviation.
    }
    % \vspace{2em}
    \label{fig:tcga_pdp_kirc}
\end{figure}

\paragraph{Time-dependent feature importance} Figure~\ref{fig:tcga_gpfi_cv} shows explanations of time-dependent grouped permutation feature importance for RSF trained on each dataset from TCGA. Only in some cases a random survival forest achieves satisfactory predictive performance as measured with the IBS metric, e.g. LGG and KIRC. Explaining the importance of feature modalities can provide insights into the challenge of accurate survival analysis. It guides stakeholders in planning to gather more data from a particular modality or improve its quality. Note how the ranking of feature importance changes over time. We observe that, on average, clinical and miRNA features are the most influential to the models' behaviour as judged by the area under the curve. Which features are significant?

\paragraph{Time-dependent feature effects} Partial dependence plots extend and complement the analysis of permutation feature importance. To better understand a specific predictive task, we should analyse the effects of each feature in detail. Figure~\ref{fig:tcga_pdp_luad} shows time-dependent partial dependence of clinical features on RSF predicting survival of lung adenocarcinoma (LUAD). In this simple example, cancer's early pathologic stage (1B) increases (on average) the predicted time-to-event. Moreover, the standard deviation of the estimated explanation provides intuition about its significance. Other clinical features seem unimportant to the model. Time-dependent explanations of ML models can complement classical survival analysis methods in knowledge discovery. An analogous explanation of clinical features in RSF predicting survival of kidney renal clear (KIRC) cell carcinoma is shown in Figure~\ref{fig:tcga_pdp_kirc}. Perhaps counterintuitively, a normal level of white cell count has a negative impact (on average) on the predicted survival function. Is this a bias in data or an undesirable relationship encoded in the black-box predictive model? An explanation can help to debug ML algorithms and align them with domain knowledge.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion} \label{sec:discussion}

\subsection{Towards time-dependent global explanations}

In many medical and healthcare predictive tasks, typical post-hoc interpretability methods were not enough to faithfully explain survival models~\cite{kovalev2020survlime,wang2021counterfactual,rad2022extracting,utkin2022survnam,stone2022systematic}. In \citep{krzyzinski2023survshap}, we proposed the notion of \emph{time-dependent} explanations as a comprehensive visualization tool for data analysis and model debugging. In this paper, we formally introduce the theory behind global time-dependent explanations and show two concrete use cases in which they are useful. First, our approach makes it easy to relate time-dependent predictive performance of survival models to their explanations. Feature importance can be integrated in time to obtain a simpler view similarly to how Brier score is conventionally integrated. Second, physicians obtain access to full information about how specific features and modalities impact the predicted survival time of patients. Our results clearly show different feature rankings and effects at different time points indicating the need for an in-depth analysis of machine learning survival models.


\subsection{Limitations}

\paragraph{Time-dependent explanations} We foresee several limitations of the introduced methods related to general limitations of post-hoc interpretability methods~\citep{molnar2020general}. First, feature effects and feature importance measures implicitly assume that features are independent, which is violated in practice where out-of-distribution observations influence explanation estimation~\citep{apley2020visualizing}. For example, \textsc{ice} fixes the value of a particular feature and \textsc{pfi} ``removes'' the influence of a particular feature by perturbation~\citep{molnar2023model}. In such cases, the conclusions drawn may be misleading or simply wrong~\citep{aas2021explaining}. It is important to acknowledge that there are no \emph{perfect} explanations and they must be interpreted with care. Crucially, different post-hoc interpretability methods may diverge in an explanation of the predictions~\citep{turbe2023evaluation}. Moreover, interpretability methods are vulnerable to adversarial attacks~\citep{baniecki2024advxai,noppel2024explainable}, e.g. minimal perturbations in input data and model parameters that lead to misleading explanations, which should be especially considered in practical, high-risk scenarios. Specifically in case of the proposed time-dependent explanations, important limitations arise from a perceptual (human) viewpoint. While being able to observe the dependence on time is desirable, it introduces an additional dimension for visualization that might lead to information overload~\citep{poursabzi2021manipulating,baniecki2023grammar}. Therefore, future work is needed regarding the perception of potentially illegible plots showing explanations with many curves and for features with large number of unique values.

\paragraph{Dataset collection and annotation} The biases encoded in data and machine learning algorithms are an important concern, especially in medicine~\citep{irvin2019chexpert,nguyen2022vindr,johnson2023mimic}. In Section~\ref{sec:materials-tlos}, we describe the novel dataset developed for the purpose of our work on time-dependent explanations. Its collection is certainly biased with respect to geographic localization (Poland) and type of patients (lung screening). We include patients' age and sex to account for the latter. As for data annotation, we minimize risks related to human bias by having two radiologists annotate the data, and thus minimize the interobserver error. We contribute an openly available dataset of over one thousand patients, which is rather substantial in the domain of LoS prediction. In general, the process of annotating medical data involves human expertise, is rather costly, and not easily scalable~\citep{irvin2019chexpert,nguyen2022vindr,johnson2023mimic}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion} \label{sec:conclusion}

We formally introduce time-dependent global feature importance and feature effects to extend the interpretability toolbox for time-to-event prediction. Experiments with multi-modal medical data demonstrate significant added value when performing a comprehensive explanatory analysis of ML models. Notably, post-hoc explanations allow for finding biases in AI systems predicting hospital LoS, and evaluating cancer survival models beyond performance to include the importance of multi-omics feature groups. We hope that the contributed data and code resources facilitate future work in the emerging research direction of explainable survival analysis. 

\paragraph{Future work} It will be valuable to propose interactive ways of visualizing time-dependent explanations for diverse stakeholders from the medical domain~\citep{baniecki2023grammar}. Our initial proposal borrows from the well-established visualization of survival curves, but it can be further improved based on human feedback~\citep{combi2022manifesto}. We need to evaluate the introduced methods in debugging machine learning models developed to analyse data in clinical settings and at the point-of-care. From the computational perspective, one can work on improving the efficiency and accuracy of estimating explanations for complex data~\citep{aas2021explaining} and models~\citep{chen2023algorithms}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Declaration of competing interest}
The authors have no known conflicts of interest.

\section*{Acknowledgements}
This work was financially supported by the Polish National Center for Research and Development grant number INFOSTRATEG I/0022/2021-00, and carried out with the support of the Laboratory of Bioinformatics and Computational Genomics and the High Performance Computing Center of the Faculty of Mathematics and Information Science, Warsaw University of Technology.


\section*{Data and code availability} \label{sec:dataandcode}
We publicly share the \textsc{tlos} dataset and contribute the implementations of time-dependent \textsc{ice}, \textsc{pdp}, and \textsc{pfi} in Python at \url{https://github.com/mi2datalab/interpret-time-to-event}. Moreover, \textsc{sfi} is implemented in Python at \url{https://github.com/mi2datalab/survshap}, and time-dependent explanations are implemented in the \texttt{survex} R package~\citep{spytek2023survex}. Datasets comprising the TCGA benchmark can be freely accessed from OpenML~\citep{bischl2021openml}.


\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hyperparameters of ML models}\label{app:hyperparameters}

See Table~\ref{tab:hyperparameters}.

\begin{table}[ht]
    \centering
    \caption{Non-default hyperparameters of machine learning models evaluated in the benchmark.}
    \label{tab:hyperparameters}
    \vspace{1em}
    \begin{tabular}{llr}
        \toprule
        \textbf{Model} & \textbf{Parameter} & \textbf{Value} \\
        \midrule
        RF-SRC & splitrule & bs.gradient \\
        \midrule
        \multirow{2}{2cm}{GBDT} & nu & 0.01 \\
         & mstop & 2000 \\
        \midrule
        \multirow{5}{2cm}{DeepSurv} & lr & 0.1 \\
         & epochs & 2000 \\
         & optimizer & adadelta \\
         & neurons & 8 \\
         & batch\_size & 64 \\
        \midrule
        \multirow{5}{2cm}{DeepHit} & lr & 1 \\
         & epochs & 2000 \\
         & optimizer & adadelta \\
         & neurons & 8 \\
         & batch\_size & 64 \\
        \bottomrule
    \end{tabular}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Performance of ML models}\label{app:performance}

See Figures~\ref{fig:model_comparison}~\&~\ref{fig:blackbox_whitebox_comparison}.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures_v2/tlos_v2_model_comparison}
    \caption{Benchmark of ML survival models predicting LoS using features from X-ray images. Most of the DeepHit models did not converge and provide random predictions; thus, DeepHit is removed from the comparison. Based on 10 repeats of 10-fold cross-validation, the GBDT algorithm performs best on average, i.e. achieves 0.668 C-index and 0.117 IBS. For broader context, we report IAUC for CoxPH and GBDT, which are the only algorithms to implement this metric. In contrast to Figure~\ref{fig:blackbox_whitebox_comparison}, we omit reporting \emph{p}-values for significant differences as there are too many.}
    \label{fig:model_comparison}
\end{figure*}

\begin{figure*}[ht]
    \includegraphics[width=\textwidth]{figures_v2/tlos_v2_bb_performance}
    \includegraphics[width=\textwidth]{figures_v2/tlos_v2_wb_performance}
    \caption{Benchmark of four feature sets -- baseline, algorithm-extracted (AE) and human-annotated (HA), all features -- in predicting LoS using GBDT black-box and CoxPH white-box model. The \emph{p}-values mark significant differences between the average results.}
    \label{fig:blackbox_whitebox_comparison}
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Significance of features in CPH models}\label{app:significance}

See Table~\ref{tab:coxph}.


\begin{table*}[ht]
    \centering
    \caption{Coefficients of only significant features (\emph{p}-value $<0.05$) in the CoxPH models fitted to all observations and the four feature sets. For example, the CoxPH model fitted on the age and sex of 1235 patients relies on age, not sex, to make the prediction. A detailed description of algorithm-extracted features with their acronyms is available at \url{https://pyradiomics.readthedocs.io/en/v3.0.1/features.html}.} 
    \label{tab:coxph}
    \vspace{1em}
    \begin{tabular}{llwr{2cm}wr{1.5cm}}
      \toprule
    \textbf{Feature set} & \textbf{Feature name} & \textbf{Estimate} & \emph{p}-\textbf{value} \\ 
      \midrule
    baseline (number of features: $d=2$) & Age & $0.007_{\pm 0.001}$ & 0.000 \\ 
      \midrule
    \multirow{6}{5cm}{baseline with human-annotated features ($d=2+17$)} 
        & Parenchymal opacification & $-0.194_{\pm 0.081}$ & 0.016 \\ 
      & Pleural effusion & $-0.334_{\pm 0.084}$ & 0.000 \\ 
      & Medical devices & $-0.628_{\pm 0.083}$ & 0.000 \\ 
      & Lung mass & $-0.311_{\pm 0.142}$ & 0.029 \\ 
      & Reticular pattern & $-0.397_{\pm 0.156}$ & 0.011 \\ 
      & Global atelectasis & $-0.430_{\pm 0.216}$ & 0.046 \\ 
        \midrule
     \multirow{12}{5cm}{baseline with algorithm-extracted features ($d=2+76$)} 
        & Shape--Maximum2DDiameterSlice & $-1.547_{\pm 0.512}$ & 0.003 \\ 
      & Firstorder--Entropy & $422.9_{\pm 161.1}$ & 0.009 \\ 
      & GLCM--ID & $-325.9_{\pm 140.0}$ & 0.020 \\ 
      & GLCM--IDN & $1000_{\pm 425.7}$ & 0.019 \\ 
      & GLCM--IMC1 & $296.4_{\pm 85.14}$ & 0.000 \\ 
      & GLCM--InverseVariance & $93.89_{\pm 43.39}$ & 0.030 \\ 
      & GLCM--JointEntropy & $-533.5_{\pm 177.5}$ & 0.003 \\ 
      & GLDM--DependenceEntropy & $156.7_{\pm 57.70}$ & 0.007 \\ 
      & GLDM--LGLE & $-120.9_{\pm 49.07}$ & 0.014 \\ 
      & GLRLM--GLNN & $-497.3_{\pm 207.2}$ & 0.016 \\ 
      & GLRLM--LRE & $6.213_{\pm 2.180}$ & 0.004 \\ 
      & GLSZM--SALGLE & $-694.2_{\pm 283.6}$ & 0.014 \\ 
    \midrule
      \multirow{20}{5cm}{all features ($d=2+17+76$)} 
        & Pleural effusion & $-0.385_{\pm 0.095}$ & 0.000 \\ 
      & Medical devices & $-0.389_{\pm 0.101}$ & 0.000 \\ 
      & Chest wall subcutaneous emphysema & $0.354_{\pm 0.180}$ & 0.049 \\ 
      \cmidrule[0.125pt]{2-4}
      & Shape--Maximum2DDiameterRow & $0.730_{\pm 0.313}$ & 0.020 \\ 
      & Shape--Maximum2DDiameterSlice & $-1.531_{\pm 0.536}$ & 0.004 \\ 
      & Firstorder--Entropy & $407.0_{\pm 165.3}$ & 0.014 \\ 
      & Firstorder--Range & $-4.361_{\pm 1.858}$ & 0.019 \\ 
      & GLCM--ID & $-358.4_{\pm 142.8}$ & 0.012 \\ 
      & GLCM--IDN & $926.6_{\pm 438.3}$ & 0.034 \\ 
      & GLCM--IMC1 & $274.2_{\pm 87.89}$ & 0.002 \\ 
      & GLCM--InverseVariance & $94.82_{\pm 44.86}$ & 0.035 \\ 
      & GLCM--JointEntropy & $-517.9_{\pm 182.2}$ & 0.004 \\ 
      & GLDM--DependenceEntropy & $185.9_{\pm 59.69}$ & 0.002 \\ 
      & GLDM--LGLE & $-136.8_{\pm 50.56}$ & 0.007 \\ 
      & GLRLM--GLNN & $-606.7_{\pm 211.5}$ & 0.004 \\ 
      & GLRLM--LRE & $6.261_{\pm 2.176}$ & 0.004 \\ 
      & GLSZM--SALGLE & $-581.1_{\pm 288.6}$ & 0.044 \\ 
      & NGTDM--Busyness & $1.252_{\pm 0.606}$ & 0.039 \\ 
      & NGTDM--Complexity & $1.406_{\pm 0.625}$ & 0.024 \\ 
       \bottomrule
    \end{tabular}
\end{table*}



\bibliographystyle{bst/elsarticle-num-names} % elsarticle-num, elsarticle-num-names, elsarticle-harv
\bibliography{references}

\end{document}