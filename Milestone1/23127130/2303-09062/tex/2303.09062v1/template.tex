\documentclass{article}



\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{multirow}


\title{Knowledge Transfer for Pseudo-code Generation from Low Resource Programming Language}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{ % \href{https://orcid.org/0000-0000-0000-0000}
{Ankita  Sontakke}%\thanks{Use footnote for providing further
		%information about author (webpage, alternative
		%address)---\emph{not} for acknowledging funding agencies.} 
        \\
	TCS Research\\
	%Cranberry-Lemon University\\
	%Pittsburgh, PA 15213 \\
	\texttt{ankita.sontakke@tcs.com} \\
	%% examples of more authors
	\And
	%\href{https://orcid.org/0000-0000-0000-0000}
 {Kanika Kalra} \\
	TCS Research\\
	%Mount-Sheikh University\\
	%Santa Narimana, Levand \\
	\texttt{kalra.kanika@tcs.com} \\
 \And
	%\href{https://orcid.org/0000-0000-0000-0000}
 {Manasi Patwardhan} \\
	TCS Research\\
	%Mount-Sheikh University\\
	%Santa Narimana, Levand \\
	\texttt{manasi.patwardhan@tcs.com} \\
 \And
	%\href{https://orcid.org/0000-0000-0000-0000}
 {Lovekesh Vig} \\
	TCS Research\\
	%Mount-Sheikh University\\
	%Santa Narimana, Levand \\
	\texttt{lovekesh.vig@tcs.com} \\
  \And
 {Raveendra Kumar Medicherla} \\
	TCS Research\\
	%Mount-Sheikh University\\
	%Santa Narimana, Levand \\
	\texttt{raveendra.kumar@tcs.com} \\ 
  \And
  {Ravindra Naik} \\
	TCS Research\\
	%Mount-Sheikh University\\
	%Santa Narimana, Levand \\
	\texttt{rd.naik@tcs.com} \\
  \And
  {Shrishti Pradhan} \\
	TCS Research\\
	%Mount-Sheikh University\\
	%Santa Narimana, Levand \\
	\texttt{shrishti.pradhan@tcs.com} \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}

% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
	%\lipsum[1]
 Generation of pseudo-code descriptions of legacy source code for software maintenance is a manually intensive task. Recent encoder-decoder language models have shown promise for automating pseudo-code generation for high resource programming languages such as C++, but are heavily reliant on the availability of a large code-pseudocode corpus. Soliciting such pseudocode annotations for codes written in legacy programming languages (PL) is a time consuming and costly affair requiring a thorough understanding of the source PL. In this paper, we focus on transferring the knowledge acquired by the code-to-pseudocode neural model trained on a high resource PL (C++) using parallel code-pseudocode data. We aim to transfer this knowledge to a legacy PL (C) with no PL-pseudocode parallel data for training. To achieve this, we utilize an Iterative Back Translation (IBT) approach with a novel test-cases based filtration strategy, to adapt the trained C++-to-pseudocode model to C-to-pseudocode model. We observe an improvement of 23.27\% in the success rate of the generated C codes through back translation, over the successive IBT iteration, illustrating the efficacy of our approach.
\end{abstract}


% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}


\section{Introduction}
%\lipsum[2]
%\lipsum[3]
Low-level \emph{program design} expressed in natural language such as pseudocodes play a key role in several software engineering tasks such as system development, unit test case generation, and  system understanding. During greenfield application development, such low-level design is created by application designers. On the other hand, to rejuvenate or maintain an existing legacy system that has evolved over a period of time and deviated significantly from its original intended purpose, the system design should be recovered from the legacy system itself. However, design recovery \citep{biggerstaff1989design} is a difficult and time-consuming task due to inadequate functional knowledge of the application, outdated application documentation, and lack of skills in the legacy PL used. Inaccuracies in low level specifications due to manual errors while recovering them lead to incorrect new implementations, resulting in large financial risk to the organizations. This calls for a need of effectively recovering the low-level design in the form of \emph{pseudo codes} from legacy application programs automatically. Moreover, pseudocode being closer to the natural language, the task of automated pseudocode generation from codes would benefit other tasks such as code comment generation, serving as a pivot for code translation as well.  

Traditionally, the task of extracting low level design is performed \emph{manually} by developers with the help of code-assistants built using lightweight program analysis  \citep{niere2002towards,seemann1998pattern,biggerstaff1989design} or  statistical machine translation techniques \citep{Oda2015LearningTG,Rai2019GenerationOP, Fudaba2015PseudogenAT}. Recently, a few neural based techniques were proposed to build models that can map codes to their corresponding pseudocode \citep{Alhefdhi2018GeneratingPF, Yang2021FinegrainedPG, Yang2021DeepPseudoDP, Alokla2022RetrievalBasedTP}. To train such neural models is challenging as it requires a lot of parallel data in terms of code-pseudocode pairs. To cater to this need the community has come up with datasets such as SPoC \citep{Yang2021FinegrainedPG} and Django \citep{Oda2015LearningTG} for C++ and Python, respectively, on which the neural models are trained. However, for certain programming languages (PL) such as C or COBOL in which legacy codes are written, such datasets are unavailable. % the low-level specifications are either unavailable or, even if they are available, they may be out of sync with the current implementation of codes due to software evolution. Thus, to generate low level specifications in terms of the pseudocodes for legacy application codes written in C there is a need for a technique which would adapt the neural models trained on available parallel data to codes written in legacy programming languages.
This highlights the need for  techniques which allow adaptation of these neural models to facilitate generation of pseudocodes for codes written in other legacy PLs with no availability of parallel training data.
\begin{figure} %width=\textwidth, height=5.5cm
   \includegraphics[width=\textwidth]{backTrans.pdf}
   \caption{Iterative Back-Translation method for C programming language adaptation}
   \label{fig:backTrans}
 \end{figure}
 In this paper, we propose a promising direction to address this problem by using an Interactive Back Translation (IBT)  \citep{hoang-etal-2018-iterative} based approach (Figure \ref{fig:backTrans}). We apply IBT for the adaptation of the pseudocode generation model trained with codes written in C++ to generate pseudocode from C code snippets. The backward model is thus trained to replicate the original C program from the generated pseudo-code. The details are provided in Algorithm \ref{algo} and Section \ref{sec:app}. Improvements in the success rates of C codes generated by the backward (pseudocode-to-code) model from the pseudocode generated by the forward model (code-to-pseudocode), over the iterations of back translation demonstrates the efficacy of our approach. 
%We have trained code-to-pseudocode and pseudocode-to-code models by using SPoC dataset \cite{Yang2021FinegrainedPG} which provides C++ - pseudocode parallel data. We take monolingual data of (i) C codes, (ii) C++ codes written for a distinct application domain than the domain of C++ codes in SPoC and (iii) Same domain C++ codes, but higher difficulty level than the C++ codes in SPoC to address the (i)- (iii) scenarios discussed above and generate pseudocodes for these codes and showcase improvements in generated pseudocodes by showing improved success rates of the generated C++ codes as a part of iterative back-translation cycles. 
The main contributions of the paper are as follows:
\begin{itemize}
  \item State-of-the-art results for pseudocode generation from C++ codes with a model trained on SPoC parallel data \citep{kulal2019spoc}, with an improvement over \citep{Yang2021FinegrainedPG, Yang2021DeepPseudoDP, Alokla2022RetrievalBasedTP}. We call this a \textit{forward model} in our IBT approach.
  \item State-of-the-art results on the C++ code generation from  pseudocodes with a model trained on SPoC parallel data with an improvement over \citep{Xie2021ComposedFF, Shi2020IncrementalSW, Yasunaga2020GraphbasedSP, Kaan2021PseudocodeTC, Zhong2020SemanticSF}. We call this a \textit{backward model} in our IBT approach.
  \item %Adaptation of C++-to-pseudocode \textit{forward model} to C-to-pseudocode task. 
  From the $0^{th}$ to the $1^{st}$ iteration of the IBT, we observe that additional 23.27\%  of C codes in the CodeNet dataset \citep{Puri2021ProjectCA}, generated by the \textit{backward model} from the  pseudocodes generated by the \textit{forward model}, execute correctly on all provided test-cases, thus  indicating superior pseudocodes generated for the source C codes. This showcases a promise in the IBT based approach for PL adaptation for the pseudocode generation task.
    %\item We showcase domain adaptation of code-pseudocode model to more difficult codes in C++ programming language with an improvement of \% success rate of the back translated codes over iterations. 

  %\item We showcase domain adaptation of code-pseudocode model to C++ codes written for distinct business domain application with an improvement of \% success rate of the back translated codes over iterations. 
  \item To the best of our knowledge this is the first attempt at neural model adaptation for pseudocode generation.
\end{itemize}

\section{Related work}
\subsection{Code Summarization}
There are several neural based approaches which have addressed the task of summarizing code snippets using code-comment parallel data. %Neural code summarization approaches can be majorly divided into: (i) Language Model (LM) based  approaches and (ii) Deep models exploiting program analysis information to incorporate code semantics. 
Language Model (LM) based approaches such as PLBART \citep{ahmad2021unified}, CodeT5 \citep{wang2021codet5}, CoText \citep{phan2021cotext}, ProphetNet-Code \citep{qi2021prophetnet}, CodeTrans \citep{elnaggar2021codetrans}, and CodeBERT \citep{feng2020codebert}, UniXcoder \citep{Guo2022UniXcoderUC} pre-train a LM on mono-lingual PL data collected from Github and/or StackOverflow\footnote{https://stackoverflow.com/} with various pre-training objectives such as token masking, deletion, denoising, or infilling. They are further fine-tuned on code-summary pairs to learn code-text alignment and infer summaries for unseen codes. Approaches exploiting program analysis information use LSTMs \citep{hu2018deep, alon2018code2seq, leclair2019neural}, Transformers \citep{ahmad2020transformer,wu-etal-2021-code, zugner2021language, leclair2019neural, zhang2020retrieval}, Graph Neural Networks (GNNs) \citep{liu2020retrieval, leclair2020improved, wang2020learning} or a combination of these \citep{choi2021learning, shi2021cast} and inject program analysis information in the form of flattened Abstract Syntax Tress (ASTs) \citep{hu2018deep, alon2018code2seq, leclair2019neural} or Code Property Graphs (CPGs) \citep{liu2020retrieval} as inputs, or define attention cite{wu-etal-2021-code} \citep{liu2020retrieval} or relative positional encodings  \citep{zugner2021language}  between adjacent code tokens in ASTs and data and control flow graphs. % Some studies also enhance these models by incorporating information retrieval techniques \cite{lieditsum,zhang2020retrieval,liu2020retrieval}, where the prototype summaries of similar codes are retrieved from a database and are edited by using an encoder-decoder setting. 
The performance of these models in terms of BLEU scores on existing datasets of function-summary or code-comment pairs \citep{hu2018summarizing, wan2018improving, leclair2019neural, liu2020retrieval, husain2019codesearchnet, lu2021codexglue} is very low (in the range of 11.17 to 26.53) \citep{wang2021codet5}. This calls into question the utility of these models for real-life applications. Generation of pseudocode from source code, which is addressed in this paper, brings the code semantics closer to natural language and can be a step towards code summarization. More importantly, these approaches do not discuss the possibility of adapting a model trained on a given PL-summary pairs to a distinct PL, which is the main focus of this paper.   %which are higher level specification than the pseudocode. In this work we are interested in lower level specifications such as pseudocode and perform code translation on line-by-line basis and thus it is not required to capture program analysis information for the same.


\subsection{Code to Pseudocode Generation}
There are very few attempts to generate pseudocode from code. \citep{Yang2021FinegrainedPG, Alokla2022RetrievalBasedTP} generates pseudocode for C++ and Python programs using models trained on SPoC \citep{kulal2019spoc} and Django \citep{alhefdhi2018generating} parallel datasets. They use transformer and stack CNNs with gated linear unit (GLU) based code encodings to decode the pseudocode \citep{Yang2021FinegrainedPG} or use a retrieval based pseudocode generation approach  \citep{Alokla2022RetrievalBasedTP}. We surpass their results on the  C++-to-pseudocode generation task by using a CodeT5 \citep{wang2021codet5} sequence-to-sequence model with PL specific tokenization and pre-processing steps. Moreover, none of these approaches talk about adaptation of the models for an unseen PL, which is our main focus. 

\subsection{Pseudocode to Code Generation}
Auto-generation of code from pseudocode can facilitate a novice programmer to build on auto-generated codes given a low-level specification and thus can aid automated software development. \cite{dong2018coarse, zhong2017seq2sql} translate short text description into one-line program, whereas \cite{iyer2018mapping, rabinovich2017abstract} generate longer programs from NL descriptions and evaluate the generated code based on syntactic metrics like exact match or BLEU score. %As opposed to this in our approach we address the task of synthesising codes with significant length and evaluate based on functional correctness using test-cases. % However, input-output pairs does not describe intermediate steps, we use pseudocode to provide intermediate computation.
As opposed to this, LSTM \citep{kulal2019spoc, Yasunaga2020GraphbasedSP, Zhong2020SemanticSF} and Transformer \citep{Xie2021ComposedFF, Shi2020IncrementalSW} based line-by-line pseudocode-to-code generation approaches test the quality of the generated code with the available test-cases using the SPoC dataset \citep{kulal2019spoc}. %is a program synthesis dataset. \cite{kulal2019spoc} uses LSTM encoder-decoder architecture to translate pseudocode into code line by line 
\cite{kulal2019spoc} uses error localization methods to guide the search over the beam, whereas \cite{Zhong2020SemanticSF} further enhances search performance using semantic scaffolds and syntactic constraints with hierarchical beam search. DrRepair \citep{Yasunaga2020GraphbasedSP} repairs the generated code from pseudocode with a diagnostic feedback from compiler. % and a self-supervised method to generate large amount of training data for program repair task. 
\cite{Xie2021ComposedFF} propose the composed fine-tuning of pre-trained models to improve generalization for code generation. We not only surpass the results of these approaches for pseudocode-to-code generation, but also mainly use our model as the \textit{backward model} in IBT to facilitate evaluation and filtering of generated pseudocodes by the \textit{forward model} . 

\section{Problem Definition}
We have a parallel dataset $D = \{(c_v,w_v,p_v,t_v)\}^V_{v=1}$ where $c_v$ are C++ codes, $p_v$ are the pseudocodes, $w_v \in W$ are the worker ids annotating the pseudocodes and $t_v = \{(i_v,o_v)\}$ are input-output execution test cases. There is a one-to-one mapping between the code lines $\{c_v^l\}^{L_v}_{l=1}$ to the corresponding pseudocode lines $\{p_v^l\}^{L_v}_{l=1}$ $\forall (c_v, p_v)$, where $L_v$ are the total lines of code $c_v$. We have a monolingual data $Y = \{(c_u,t_u)\}^U_{u=1}$ where  $c_u$ are C codes and $t_u = \{(i_u,o_u)\}$ are the test cases for those codes. The task is to generate pseudocodes $p_u$ for the codes $c_u$ in $Y$, such that there is a one-to-one mapping between the code lines $\{c_u^l\}^{L_u}_{l=1}$ and the generated pseudocode lines $\{p_u^l\}^{L_u}_{l=1} \forall (c_u, p_u)$, where $L_u$ are the total lines of code $c_u$.



\section{Datasets}
%Datasets like SPoC \cite{kulal2019spoc}, CodeNet \cite{Puri2021ProjectCA}, Description2Code \footnote{https://github.com/ethancaballero/description2code}, ProgRES \cite{alet2021large} are collected from online competitive programming website like codeforces.com. These datasets contains solutions for coding problems in specific programming language like Java, C++ along with input-output pairs.
As we are interested in generating pseudocodes for legacy C codes, we use C++-pseudocode parallel data provided by SPoC dataset \citep{kulal2019spoc} to train and test the forward code-to-pseudocode and backward pseudocode-to-code models. The intuition is that the overlap between the constructs of C and C++ PLs would facilitate the adaptation of the model trained on C++ for C codes. The SPoC dataset  provides C++ solutions for coding programs from codeforces, a competitive programming website \footnote{https://codeforces.com/}. Codeforces contains 18,356 C++ programs for 677 problem descriptions with the corresponding pseudocode and test cases. For each line of C++ code, the pseudocode is written by one or more of 59 crowd workers through Amazon Mechanical Turk platform\footnote{https://www.mturk.com/}.  %The programs with constructs like \#define macros, classes, structs, templates, switch statements, and mallocs that are difficult to annotate for pseudocode are eliminated from dataset. 
For evaluation, SPoC provides two test sets: (i) TESTP formed by splitting programs based on problem descriptions (158 problems with 1,820 programs) (ii) TESTW  formed by splitting programs based on workers (7 workers with 1,752 programs) to evaluate generalization. % on unseen problems and worker's annotation styles, respectively. %TESTP has 158 problems which contain 1,820 programs and TESTW has 7 workers which contains 1,752 programs. 
Remainder of the dataset is divided into train and validation sets in the ratio 90:10.

%\newcommand{\algorithmicbreak}{\textbf{break}}
%\newcommand{\BREAK}{\STATE \algorithmicbreak}
\begin{algorithm}
	\caption{IBT for PL Adaptation}
	\label{algo}
% 	\begin{algorithmic}[1]
	\KwIn{Monolingual Code Data $Y = \{(c_u,t_u)\}^U_{u=1}$ where $c_u$ are C codes and $t_u = \{(i_u,o_u)\}$ are the test cases. $\{c_u^l\}^{L_u}_{l=1}$ are the code lines of $c_u$.
%	\\\hspace{0.255in} $c_u$ are C codes and $t_u = \{(i_u,o_u)\}$ are the test cases 
	%\\\hspace{0.255in}  $\{c_u^l\}^{L_u}_{l=1}$ are the code lines of each code in $c_u$
	%\\\hspace{0.255in} Input-Output pairs for all programs from $Y$
	\\\hspace{0.4in} Parallel data $D = \{(c_v,w_v,p_v,t_v)\}^V_{v=1}$ where  $c_v$ are C++ codes, $p_v$ are the 
	%\\\hspace{0.255in} $c_v$ are C++ codes, $p_v$ are the pseudocodes, 
	\\\hspace{0.4in} pseudocodes,  $w_v \in W$ are the worker ids writing the pseudocodes and $t_v = \{(i_v,o_v)\}$ 
	%\\\hspace{0.255in} and $t_v = \{(i_v,o_v)\}$ are the test cases 
	\\\hspace{0.4in} are the test cases. $\{c_v^l\}^{L_v}_{l=1}$ are the code lines of each code in $c_v$  with  the corresponding 
	\\\hspace{0.4in} pseudocode lines $\{p_v^l\}^{L_v}_{l=1}$. Thus, the samples of D are $\{(c_v^l,w_v,p_v^l)^{L_v}_{l=1}\}^V_{v=1}$ as all lines  
	\\\hspace{0.4in} of a code are annotated by the same worker.
	%\\\hspace{0.255in}the corresponding pseudocode lines $\{p_v^l\}^{L_v}_{l=1}$ 
	\\\hspace{0.4in} $B$: Beam Size, $B_d$: Budget, $I$: Number of Iterations
}	
	\KwOut{Trained forward C code-pseudocode $M_{c\rightarrow p}$ model}
% 	\\\hspace{0.255in} Trained forward code-to-pseudocode $M_{c\rightarrow p}$ and \\\hspace{0.255in} backward pseudocode-to-code $M_{c\leftarrow p}$ models
%}
     %\textbf{Procedure} BACK-TRANSLATION\\
	\For{$i\gets0$ \KwTo $I$}{
	Fine-tune forward code-to-pseudocode $M_{c\rightarrow p}$ model using $D$, with $w_u$ as prefixes per sample  \\
	Fine-tune backward pseudo-to-code $M_{c\leftarrow p}$ model using $D$ \\
    Feed $c_u^l \in c_u$ in $Y$ to $M_{c\rightarrow p}$ $\forall w_u \in W$ as prefixes to create $D' = \{(c_u^l, w_u,p_u^l,t_u)^{L_u}_{l=1}\}^{U*W}_{u=1}$ where $p_u^l \in p_u$ is the generated pseudocode line for code line $c_u^l \in c_u$% \forall y \in Y$. 
    \\
    Feed $p_u^l \in p_u$ in $D'$ to $M_{c\leftarrow p}$ to create $D'' = \{(p_u^l,w_u,{c'}_u^l,t_u)^{L_u}_{l=1}\}^{U*W}_{u=1}$ with beam size of $B$  for each line of generated codes ${c'}_u^l \in {c'}_u$ \\
    Use \textbf{Best-first-search} to search over $B$ beams of code lines $\{{c'}_u^l\}^{L_u}_{l=1} $ $ \forall {c'}_u \in D''$ with budget $B_d$ to get best prediction of code ${c'}_u$ \\ 
    \For{$c'_u \in D''$}{
    \If{ $\forall i_u \in t_u $ ${c'}_u(i_u) = o_u$ \tcp{Success Rate = 1, Filtration Strategy} }%{$Success\_rate = 1$}
    %\Else{$Success\_rate = 0$
      %$Success rate$  for all programs using Input-Output pairs at $Budget = 10$. \\
    %\If {$Success rate = 1$ }
    %{$Y = \{(c_n)\}^N_{n=1}$ as new test samples for next iteration. 
    %}
    %\ElseIf{$Success = 1 $ }
    { $D = D \cup \{(c_u,w_u,p_u,t_u)\}$ \tcp{Data Augmentation} 
      $Y = Y - \{(c_u,t_u)\}$ \tcp{Test Set Update}
    %create new dataset $D^{new} = \{(c_m,p'_m)\}^M_{m=1} + \{(c_v,p_v)\}^V_{v=1}$ with PL name as prefix for every line of the program.\\
    %Fine-tune forward code-to-pseudocode $M_{c\rightarrow p}$ model using $D^{new}$. \\
	%Fine-tune backward pseudocode-to-code $M_{c\leftarrow p}$ model using $D^{new}$. \\
	}
    }
    \If{$Y$ = $\emptyset$}{$Break$}
    }
    %\textbf{End Procedure}
% 	\end{algorithmic}
\end{algorithm} %\label{algo:itb}
%\vspace{-2mm}


For PL adaptation, we use the C code dataset from Project CodeNet \citep{Puri2021ProjectCA}. We only take accepted (passed all the test cases) C codes into consideration. As SPoC dataset has programs solving simple problems  \citep{kulal2019spoc}, we choose 25,666 simple C CodeNet codes ($\approx$ 8.2\% of accepted C codes) along with their test cases as our mono-lingual PL data. The simple codes are the ones which have lower score given to the coding problem and more number of accepted submissions (ratio of accepted submissions (700) to the coding problem difficulty level (100) for AtCoder\footnote{https://atcoder.jp/} and codes for the coding problems having more than 2500 submission for AIZU\footnote{https://judge.u-aizu.ac.jp/onlinejudge/} from where CodeNet has collected the codes).
%CodeNet dataset collects programs from AtCoder and AIZU websites. To select simple programs, we use 0.15 threshold for ratio of score and number accepted submission as AtCoder provides score for a coding problems and for AIZU, we use 2500 threshold of total number of accepted submissions for a problem.
%ANKITA please provide the exact number for the score (100) and number submission thresholds (700 for AtCoder and 2500 for AIZU website) here

\begin{table}[t]
\caption{Success rates at budgets $B_d$ for pseudocode-to-code task on SPoC Test data% and other SOTA models. - denotes that result is provided in the respective paper.
}
\label{table:pseudo-to-code result}
\centering
%\resizebox{\textwidth}{!}{%
\begin{tabular}{l|lll|lll}
\hline
\textbf{Split} & \multicolumn{3}{c|}{\textbf{TESTP}} & \multicolumn{3}{c}{\textbf{TESTW}} \\ \hline
\textbf{Budget} & {$B_d$ = 1} & {$B_d$ = 10} & {$B_d$ = 100} & {$B_d$ = 1} & {$B_d$ = 10} & {$B_d$ = 100} \\ \hline
\cite{Shi2020IncrementalSW} & - & - & 32.5 & - & - & 51.0 \\
\cite{kulal2019spoc} & 17.8 & 28.4 & 34.2 & 30.7 & 44.4 & 53.7 \\
\cite{Yasunaga2020GraphbasedSP} & 17.8 & 31.4 & 38.5 & 30.7 & 48.0 & 57.0 \\
\cite{Xie2021ComposedFF} & 15.4 & - & - & 38.1 & - & - \\
\cite{Zhong2020SemanticSF} & 31.2 & 39.4 & 46.1 & 46.0 & 55.3 & 62.8 \\ \hline
\textbf{Ours} & \textbf{40.2} & \textbf{49.6} & \textbf{53.2} & \textbf{56.5} & \textbf{63.6} & \textbf{67.1} \\ \hline

\end{tabular}%
%}
%\vspace{-2mm}
\end{table}
% % Please add the following required packages to your document preamble:
% % \usepackage{multirow}
% % \usepackage{graphicx}
% \begin{table}[]
% \caption{Scores for Code-to-Pseudocode task *random splits.}
% \label{table:Code-to-Pseudocode result}
% % \resizebox{\columnwidth}{!}{%
% \begin{tabular}{lll|l}
% \hline
% \multicolumn{3}{l|}{Model} & BLEU \\ \hline
% \multicolumn{3}{l|}{DeepPseudo \cite{Yang2021FinegrainedPG}} & 46.45\textsuperscript{*} \\
% \multicolumn{3}{l|}{Retrieval-based Transformer \cite{Alokla2022RetrievalBasedTP}} & 50.28\textsuperscript{*} \\ \hline
% \multicolumn{1}{l|}{\multirow{2}{*}{Ours}} & \multicolumn{2}{l|}{TESTP} & 79.83 \\
% \multicolumn{1}{l|}{} & \multicolumn{2}{l|}{TESTW} & 64.06 \\ \hline
% \end{tabular}% 
% % }
% \end{table}
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[t]
\caption{Scores for code-to-pseudocode task on SPoC Test data. *random splits.}
\label{table:Code-to-Pseudocode result}
\centering
%\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|ll|cl}
\hline
Approach & %DeepPseudo 
\cite{Yang2021FinegrainedPG} & %Retrieval-based Transformer 
\cite{Alokla2022RetrievalBasedTP} & \multicolumn{1}{c}{Ours - TESTP} & \multicolumn{1}{c}{Ours - 
TESTW} \\ \hline
BLEU  & 46.45\textsuperscript{*}     & 50.28\textsuperscript{*}                     & 79.83                            & 64.06                            \\ \hline
\end{tabular}%
%}
%\vspace{-4mm}
\end{table}



\section{Approach}\label{sec:app}
We use Iterative Back Translation (IBT) to transfer the knowledge of C++ code to pseudocode generation and repurpose it for C code to pseudocode generation. We accomplish this  by adapting the pseudocode generation model trained on the parallel data set $D$ of C++ codes using Iterative Back Translation and a test-cases based filtration strategy (Algorithm \ref{algo}).  We first fine-tune the \textit{forward} code-to-pseudocode ($M_{c\rightarrow p}$) model using $D$, with worker ids $w_u$ as prefixes to capture worker styles. We also fine-tune a \textit{backward} pseudocode-to-code ($M_{c\leftarrow p}$) model using the $D$ parallel data. We choose a Transformer based CodeT5 model  \citep{wang2021codet5} as our base model, which is pretrained on many natural language as well as PL specific tasks such as masked span prediction, masked identifier prediction, identifier tagging and bimodal dual generation, etc. We choose CodeT5 as it is trained using the CodeSearchNet \citep{husain2019codesearchnet} dataset which contains six PLs data including C and C\# and this prior knowledge of C PL would be useful for our downstream task. For both models, we use a C++ language specific tokenizer i.e clang python package \citep{kulal2019spoc} which makes the tokenization invariant to the set-of `space' characters used in the code lines. For example: the code lines, `\textit{else if (ans == int(ans))}' or `\textit{else if(  ans== int( ans))}' both tokenizes into consistent set of tokens`\textit{else if ( ans == int ( ans ) )}'. These tokenized code lines are further sub-tokenized using the CodeT5 tokenizer. We observe that, in the SPoC dataset, the C++ keyword `\textit{endl}' similar to "\textbackslash n" which inserts the newline character, is not annotated for the corresponding pseudocode by the crowd workers. As a pre-processing step, we modify the pseudocodes to include `\textit{print newline}' tokens for the corresponding `\textit{endl}' keyword in the C++ code. For example, the original pseudocode for `\textit{cout << - 1 << endl ;}' is `\textit{print -1}'. We modify it to `\textit{print - 1 print new line}'. With such pre-processed data we train the models in an auto-regressive fashion by using code and psuedo-code lines as parallel data, with cross entropy loss and a learning rate of $5e-6$. For the code-to-pseudocode model the epochs, input and output token lengths are 25, 128 and 200, respectively. For the pseudocode-to-code model the they are 50, 100 and 100, respectively, which covers most of the codes in the dataset. We choose the best models with least validation loss for inference . 

For the  \textit{forward model} we use beam size of 1 for the generated pseudocodes. For \textit{backward model}, we use a beam size $B$ of 10, that is for every generated pseudocode line from the \textit{forward model}, the \textit{backward model} generates 10 code lines. On the similar lines of  \cite{kulal2019spoc} we use \textbf{Best-first-search} to combine code lines forming the complete program and evaluate against the given $t_u$ test-cases. We first combine all the code lines on the top beams sequentially and execute the complete code with the test-cases. We replace the first error line from all compilation errors for a test-case with the next most probable code line present on the beam and re-execute the code. We assume $B_d$ as the budge in terms of number of allowable executions for a code.  If a program consisting of its code lines searched over the beam compiles successfully and passes all the test-cases within the budget then the success rate for that program is 1 otherwise it is 0.

For adaptation, we use monolingual C program dataset $Y$ and feed C code lines to the \textit{forward model} to generate pseudocode lines. The C programs are tokenized along the same lines as the C++ programs, as described earlier.  We append prefixes indicating worker ids $\in W$ to the code prior to generation. The top ten worker ids of workers who have annotated maximum number of code lines are utilized. We use the top-1 beam to obtain the the pseudocode from every worker. Thus, for every line of code we have 10 pseudocode lines generated. We feed these generated pseudocode lines to the \textit{backward model} to generate the code lines with a beam size of 10. To form a complete program from generated code lines, we use Best-first search algorithm  discussed above.% with simple error localization method where we use first error line from all complication errors to replace with next code line from the generated beam. 
We choose budget $B_d$ as 10 (explained in Section \ref{sec:rad}). C codes executing correctly after backtranslation, along with the corresponding generated pseudocodes get added to the (parallel) training data $D$, and C codes that fail to execute remain in the monolingual C data $Y$ as test samples for the next iteration. In the subsequent iteration, we fine-tune both \textit{forward} and \textit{backward} models using the updated $D$ with a prefix indicating the PL (C++ or C) the program belongs to for better syntax discrimination and repeat the whole process. %This algorithm is pictorially illustrated in Figure \ref{fig:backTrans}. 
We execute the back translation pipeline for $I$ = 2 iterations to evaluate the efficacy of our proposed approach. 



% Please add the following required packages to your document preamble:

 %\usepackage{graphicx}
\begin{table}[t]
\caption{Ablation for the prepossessing steps NA: Not Applicable - Worker ID not used as prefix.}
\label{table:ablation}
\centering
\resizebox{\textwidth}{!}{%

\begin{tabular}{l|ll|llllll}
\hline
\multicolumn{1}{c|}{\multirow{3}{*}{Modification in Model}} & \multicolumn{2}{c|}{code-to-pseudocode} & \multicolumn{6}{c}{pseudocode-to-code} \\ \cline{2-9} 
\multicolumn{1}{c|}{} & \multicolumn{2}{c|}{BLEU} & \multicolumn{2}{c}{Exact Match} & \multicolumn{2}{c}{BLEU} & \multicolumn{2}{l}{Success Rate at B = 1} \\
\multicolumn{1}{c|}{} & TESTP & TESTW & TESTP & TESTW & TESTP & TESTW & TESTP & TESTW \\ \hline
CodeT5 tokenizer & 44.06 & 40.06 & 71.49 & 71.62 & 85.73 & 85.33 & 13.6 & 23.4 \\
+ PL specific tokenizer & 64.72 & 65.86 & 81.50 & 82.41 & 91.47 & 91.41 & 37.2 & 51.8 \\
+ Modified `\textit{endl}' keyword  & 65.33 & 65.97 & 82.66 & 83.72 & 92.04 & 92.02 & 40.2 & 56.5 \\
+Worker ID in input as prefix & 79.83 & 64.06 & NA & NA & NA & NA & NA & NA \\ \hline
\end{tabular}%
}
%\vspace{-2mm}
\end{table}



\begin{table}[t]
\caption{Iteration-wise Success rate of IBT for C adaptation}
\label{table:backTrans}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccc}
\hline
Iteration & \begin{tabular}[c]{@{}l@{}}\# Test Programs\end{tabular} & \begin{tabular}[c]{@{}l@{}}Success Rate at B=10\end{tabular} & \begin{tabular}[c]{@{}l@{}}Cumulative Success Rate\end{tabular} \\ \hline
Iteration 0 & 25666 & 60.84 \% & 60.84 \% \\
Iteration 1 & 10051 & 59.42 \% & \textbf{84.11 \%} \\ \hline
\end{tabular}%
}
%\vspace{-4mm}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
%\begin{table}[h]
%\caption{Qualitative analysis of C-to-pseudocode model adaptation over IBT iterations}
%\label{table:qualitative analysis}
%\resizebox{\textwidth}{!}{%
%\begin{tabular}{l|ll|ll}
%\hline
%\multicolumn{1}{c|}{\multirow{2}{*}{Original C code line}} & \multicolumn{2}{c|}{Iteration 0} & \multicolumn{2}{c}{Iteration 1} \\
%\multicolumn{1}{c|}{} & \multicolumn{1}{c}{Pseudocode} & \multicolumn{1}{c|}{Top 1 Code line} & \multicolumn{1}{c}{Pseudocode} & \multicolumn{1}{c}{Top 1 Code line} \\ \hline
%1. printf ( " \%d \%d \%d\textbackslash{}n " , a , c , b ) ; & print " \% d \% d \% d \textbackslash n " , a , c and b & cout \textless{}\textless " \%d \%d \%d\textbackslash{}n " , a , c , b ; & print " \% d \% d \% d \textbackslash n " , a , c , b & printf ( " \%d \%d \%d\textbackslash{}n " , a , c , b ) ; \\
%2. putchar ( 10 ) ; & print 10 & cout \textless{}\textless 10 \textless{}\textless ' \textbackslash{}n ' ; & print char 10 & putchar ( 10 ) ; \\
%3. scanf ( " \%d " , \& x ) ; & \begin{tabular}[c]{@{}l@{}}call scanf with arguments \% d \\ and address of x\end{tabular} & scanf ( \% d , \& x ) ; & \begin{tabular}[c]{@{}l@{}}call scanf with arguments \%d \\ and \& x\end{tabular} & scanf ( " \%d " , \& x ) ; \\
%4. fgets ( str , sizeof ( str ) , stdin ) ; & read str from the input to stdin & cin \textgreater{}\textgreater str ; & read str from the input to stdin & fgets ( str , sizeof ( str ) , stdin ) ; \\
%\begin{tabular}[c]{@{}l@{}}5. do \{ scanf ( " \%d " , \& s ) ; \} \\     while ( s \textless 0 || || s \textgreater 100 ) ;\end{tabular} & \begin{tabular}[c]{@{}l@{}}while ( read \%d from stream , address\\of s is less than 0 or\\or s is greater than 100 )\end{tabular} & while ( cin \textgreater{}\textgreater s , s \textless 0 || s \textgreater 100 ) & \begin{tabular}[c]{@{}l@{}}do \{ scanf ( " \% d " , address of s ) ; \}\\ while s is less than 0 or s is greater\\than 100\end{tabular} & \begin{tabular}[c]{@{}l@{}}do \{ scanf ( " \%d " , \& s ) ; \} \\ while ( s \textless 0 || || s \textgreater 100 ) ;\end{tabular} \\
%6. scanf ( " \%d\%d\%d " , \& a , \& b , \& c ) ; & \begin{tabular}[c]{@{}l@{}}call scanf with arguments \%d \% d \% d \\ and \& a , \& b and \& c\end{tabular} & scanf ( \% d \% d \% d , \& a , \& b , \& c ) ; & scanf ( " \% d \% d \% d " , \& a , \& b , \& c ) & scanf ( " \%d \%d \%d " , \& a , \& b , \& c ) ; \\ \hline
%\end{tabular}%
%}
%\vspace{-2mm}
%\end{table}

\begin{table}[t]
\caption{Qualitative analysis of C-code Examples and generated Pseudo-codes over IBT iterations}
\label{table:qualitative analysis}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lll}
\hline
1.& Original C code  & printf ( " \%d \%d \%d\textbackslash{}n " , a , c , b ) ;\\
&Iteration 0 Pseudo-code &  print " \% d \% d \% d \textbackslash n " , a , c and b\\
&Iteration 0 Top-1 C code & cout \textless{}\textless " \%d \%d \%d\textbackslash{}n " , a , c , b ;\\
&Iteration 1 Pseudo-code & print " \% d \% d \% d \textbackslash n " , a , c , b\\
&Iteration 1 Top-1 C code & printf ( " \%d \%d \%d\textbackslash{}n " , a , c , b ) ;\\
\hline
2.& Original C code  & putchar ( 10 ) ;\\
&Iteration 0 Pseudo-code & print 10\\
&Iteration 0 Top-1 C code & cout \textless{}\textless 10 \textless{}\textless ' \textbackslash{}n ' ;\\
&Iteration 1 Pseudo-code & print char 10\\
&Iteration 1 Top-1 C code & putchar ( 10 ) ; \\
\hline
3.& Original C code  & scanf ( " \%d " , \& x ) ;\\
&Iteration 0 Pseudo-code & \begin{tabular}[c]{@{}l@{}}call scanf with arguments \% d and address of x\end{tabular} \\
&Iteration 0 Top-1 C code & scanf ( \% d , \& x ) ;\\
&Iteration 1 Pseudo-code &\begin{tabular}[c]{@{}l@{}}call scanf with arguments \%d and \& x\end{tabular}\\
&Iteration 1 Top-1 C code &  scanf ( " \%d " , \& x ) ; \\
\hline
4.& Original C code  & fgets ( str , sizeof ( str ) , stdin ) ;\\
&Iteration 0 Pseudo-code & read str from the input to stdin\\
&Iteration 0 Top-1 C code & cin \textgreater{}\textgreater str ; \\
&Iteration 1 Pseudo-code & read str from the input to stdin\\
&Iteration 1 Top-1 C code & fgets ( str , sizeof ( str ) , stdin ) ; \\
\hline
5.& Original C code  & \begin{tabular}[c]{@{}l@{}} do \{ scanf ( " \%d " , \& s ) ; \}  while ( s \textless 0 || s \textgreater 100 ) ;\end{tabular}\\
&Iteration 0 Pseudo-code & \begin{tabular}[c]{@{}l@{}}while ( read \%d from stream , address of s is less than 0 or s is greater than 100 )\end{tabular} \\
&Iteration 0 Top-1 C code & while ( cin \textgreater{}\textgreater s , s \textless 0 || s \textgreater 100 )  \\
&Iteration 1 Pseudo-code & \begin{tabular}[c]{@{}l@{}}do \{ scanf ( " \% d " , address of s ) ; \} while s is less than 0 or s is greater than 100\end{tabular}\\
&Iteration 1 Top-1 C code &\begin{tabular}[c]{@{}l@{}}do \{ scanf ( " \%d " , \& s ) ; \}  while ( s \textless 0 || s \textgreater 100 ) ;\end{tabular} \\
\hline
6.& Original C code  &  scanf ( " \%d\%d\%d " , \& a , \& b , \& c ) ; \\
&Iteration 0 Pseudo-code & \begin{tabular}[c]{@{}l@{}}call scanf with arguments \%d \% d \% d  and \& a , \& b and \& c\end{tabular}\\
&Iteration 0 Top-1 C code & scanf ( \% d \% d \% d , \& a , \& b , \& c ) ; \\
&Iteration 1 Pseudo-code & scanf ( " \% d \% d \% d " , \& a , \& b , \& c )\\
&Iteration 1 Top-1 C code & scanf ( " \%d \%d \%d " , \& a , \& b , \& c ) ; \\
\hline

\end{tabular}%
}
%\vspace{-2mm}
\end{table}

\section{Results and Discussions}\label{sec:rad}
To evaluate our \textit{backward model} trained on SPoC pseudocode-to-C++ data, we use \textit{Success rate at B} metric \citep{kulal2019spoc}, which is the percentage of total programs that successfully compile and pass all available test-cases under the computation budget $B_d$. Computation budget $B_d$ is the number of times that a system can invoke the compiler and execute the compiled program on all test-cases. In short, we can search for maximum $B_d$ number of programs in the search space formulated by the beam. We evaluate our models by comparing our results with State-of-the-Art models for budget $B_d$ = 1, 10, 100 as shown in Table \ref{table:pseudo-to-code result}. We show that our model for C++ Code generation from pseudocode achieves success rates of 53.2\% and 67.1\% at $B$ = 100 on the SPoC TESTP and TESTW sets. 

Table \ref{table:Code-to-Pseudocode result} illustrate the comparison of our C++-to-Pseudocode (\textit{forward}) model on BLEU metric. We consider the SPoC original test splits TESTP and TESTW as opposed to the random splits performed by \cite{Yang2021DeepPseudoDP,Alokla2022RetrievalBasedTP}. Our model achieves state-of-the-art results on both TESTP and TESTW splits, proving the generalization capability of our approach as opposed to the benchmarks \citep{Yang2021DeepPseudoDP,Alokla2022RetrievalBasedTP}. 

We also conduct the ablation study for the distinct pre-processing steps applied on the code for both \textit{forward} and \textit{backward} models (Table \ref{table:ablation}). The PL specific tokenizer and modifying the pseudocode for the 'endl' keyword improves performance. Using crowd worker IDs as prefix for each code line helps the model understand the worker's annotation style and thus improves the BLEU score for TESTP which has overlapping worker ids with the train split. On the other hand, BLEU score decreases slightly for TESTW which has unseen workers. 

For the C PL codes we don't have the corresponding ground truth pseudo-code annotations available to test the performance of the \textit{forward model} for the pseudocode generation task, over the iterations of the back translation. We instead use the success rate for the C codes generated by the \textit{backward model} using the  pseudocodes generated by the \textit{forward model} from the original C codes, as part of the IBT, as the evaluation metric. Table \ref{table:backTrans} shows that the overall success rate of generated C codes increases by 23.27\% after only one iteration. This demonstrates the adaptation of the code-to-pseudocode model originally trained on C++ data to C codes over the IBT iterations. To further showcase the efficacy of our approach we illustrate the qualitative analysis of a few examples of C codes and generated pseudo-codes to demonstrate how the code-to-pseudocode model gets better at pseudocode generation over successive IBT iterations (Table \ref{table:qualitative analysis}). For example, in code line (1) the generated `\textit{Print}' pseudocode is mapped to C++ PL specific`\textit{cout}' syntax in iteration 0. However, it gets mapped to the C PL specific `\textit{printf}' function in iteration 1. This can be due to the C PL specific parallel data, which has been augmented in the training data, as a result of iteration 0, has the mapping of  `\textit{Print}' in pseudocode to `\textit{printf}' syntax, in some other composition. On similar lines, for codelines (2) and (3) the generated pseudocode gets better in iteration 1 by correctly reconstructing `\textit{putchar}' and `\textit{scanf}' constructs.  %In code line 4 it shows that though `\textit{fgets}' is not present in the SPoC train set, it still gets mapped to read word in iteration 0, however `\textit{gets}' function present in SPoC dataset, and thus in iteration 1, model learns to correct syntax for `\textit{fgets}' function.
For codeline (4) `\textit{fgets}' is mapped to read word correctly in iteration 0, even though the SPoC train set has not seen this construct, mainly de to the context. In iteration 1, the \textit{backward model} also learns to reconstruct the correct syntax from the pseudocode.
In codeline (5), the composition of \textit{scanf} and \textit{do while} construct unseen in the SPoC training data is learnt. Codeline (6) shows an example where the generated pseudocode is identical to the original code, in iteration 1, showcasing a failed pseudo-code generation case due to unseen \textit{scanf} construct. This qualitative analysis depicts how, in most of the cases, the \textit{forward model} gets better at the pseudocode generation task over the iterations of back translation, also leading to more correctly generated codes by the \textit{backward model}.

\section{Conclusion and Future Scope}
In this paper, we develop code-to-pseudocode and pseudocode-to-code models for C++ to provide State-of-the-Art results on SPoC dataset. We propose a promising new approach of adapting the C++ code-to-pseudocode model to C programs, with no parallel data, using IBT, where we utilize the available test cases to filter invalid code-pseudocode pairs. Our IBT pipeline demonstrates improvement in the execution rate of codes generated via back translation over successive iterations from the pseudocodes generated by the forward model. This showcases the efficacy of back-translation for adaptation of pseudocode generation to a legacy PL (C) with no parallel data when the original PL (C++) belongs to the same programming language family (some overlap in PL constructs) and has parallel data. 

For future work, we plan to use our IBT based approach for generated pseudocode adaptation to codes: (i) in the same PL with higher complexity  (ii) distinct legacy PLs (such as COBOL), having less overlap with the original PL (C++) in a few-shot (small parallel data) setting or by using a pre-defined common Intermediate Representation (IR), (iii) codifying implementations for distinct application domains (banking, retail, etc). We plan to define and use novel filtration strategies such as filtering (a) partial programs with exact match for certain code lines, or (b) codes with syntax errors checked using predefined restricted grammar, or (c) psuedocodes with no domain contents.  We also plan to initialize the models by meta-training with the available code-pseudocode parallel data belonging to multiple PLs such as C++  \citep{kulal2019spoc} and python \citep{alhefdhi2018generating},  to boost  performance for unseen PLs. All the above discussed scenarios are relevant in an industrial setting where it is infeasible to generate a lot of code-NL summary or pseudocode parallel data for distinct legacy programming languages and application domains.




\bibliographystyle{unsrtnat}
\bibliography{references}  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .


%%% Uncomment this section and comment out the \bibliography{references} line above to use inline references.
% \begin{thebibliography}{1}

% 	\bibitem{kour2014real}
% 	George Kour and Raid Saabne.
% 	\newblock Real-time segmentation of on-line handwritten arabic script.
% 	\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
% 			International Conference on}, pages 417--422. IEEE, 2014.

% 	\bibitem{kour2014fast}
% 	George Kour and Raid Saabne.
% 	\newblock Fast classification of handwritten on-line arabic characters.
% 	\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
% 			International Conference of}, pages 312--318. IEEE, 2014.

% 	\bibitem{hadash2018estimate}
% 	Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
% 	Jacovi.
% 	\newblock Estimate and replace: A novel approach to integrating deep neural
% 	networks with existing applications.
% 	\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

% \end{thebibliography}


\end{document}
