\section{Related Work}

\paragraph{Protein language models}
Pretraining-based language modelings such as Transformer~\cite{vaswani2017attention}, BERT~\cite{devlin2018bert}, and GPT~\cite{ferruz2022protgpt2} have revolutionized natural language processing and shown remarkable performance on various tasks such as language understanding, sentence generation, and infilling over the last few years. With a huge increase in the amount of unlabeled protein sequences~\cite{uniprot2019uniprot} produced by high throughput sequencing technologies, pLMs have been introduced and resolved the challenges in protein science and engineering by learning protein languages. BERT-style models primarily provide protein embeddings to solve prediction problems, including protein structure prediction~\cite{rao2020transformer, jumper2021highly, lin2022language}, function prediction~\cite{brandes2022proteinbert}, and property prediction~\cite{rives2021biological}. GPT-style architectures are utilized in resolving generation challenges such as protein sequence design~\cite{madani2020progen, hesslow2022rita, moffat2022design, ferruz2022protgpt2, nijkamp2022progen2}.


\paragraph{Protein sequence design}
Attempts to efficiently design protein sequences can be divided into two categories: a method for conducting a large number of high-throughput experiments with mutagenesis and a machine learning-based sequence generation method. Recent advances in experimental-based methods~\cite{fowler2014deep} allow us to assess the functional changes of mutated protein sequences at a large scale and produce a lot of labeled data. Many machine learning-based sequence design methods generate the optimized sequences iteratively based on the feedback of labeled data~\cite{yang2019machine, xu2020deep, wu2021protein, shin2021protein}. Unfortunately, both approaches require a lot of cost and effort in experiments. Recently, several works generate protein sequences conditioned on given 3D structures using a single energy function~\cite{alford2017rosetta}, convolutional neural networks~\cite{zhang2020prodconn, qi2020densecpd}, graph neural networks~\cite{ingraham2019generative, jing2020learning, strokach2020fast, dauparas2022robust}, or Transformers\cite{hsu2022learning}. Since these works require 3D coordinate information to generate sequences, generation may be limited only to areas where high-quality structures exist. Also, in these works, CATH~\cite{orengo1997cath} is used to evaluate how similar the generated sequences are to the original sequence. This evaluation method may not be suitable for protein engineering, which aims to change the sequence to have a better function. In parallel, generative pLMs such as RITA~\cite{hesslow2022rita}, DARK~\cite{moffat2022design}, ProtGPT2~\cite{ferruz2022protgpt2}, and Progen2~\cite{nijkamp2022progen2} have been developed. These generative pLMs generate protein sequences having well-folded and viable structures even though these methods do not employ any structural information. However, due to the nature of the AR model itself, these methods utilize only the preceding sequence information during sequence generation. Our proposed ProtFIM has both AR and FIM properties, resulting in efficient FIM protein engineering.

