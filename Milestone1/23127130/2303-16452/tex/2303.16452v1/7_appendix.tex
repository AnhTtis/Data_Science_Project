\section{Appendix}

\subsection{Interaction sites extraction}
\label{appendix:a1}

% \begin{figure*}[h]
%   \centering
%   \includegraphics[width=0.50\linewidth]{./figures/figure1.pdf}
%   \caption{Relative positions of the interacting sites on the protein sequences.}
%   \label{interaction_dist}
% \end{figure*}


\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=300pt]{./figures/figure1.pdf}}
\caption{Relative positions of the interacting sites on the protein sequences.}
\label{interaction_dist}
\end{center}
\vskip -0.2in
\end{figure*}




We download 3D protein structures from PDB database~\cite{sussman1998protein} and extract protein structures satisfying several conditions: having more than two protein chains; having UniProt ID and a length of the entire sequence in mmCIF dictionaries from MMCIF2Dict module of Biopython~\cite{cock2009biopython}. We hypothesize that the two residue pairs of two different chains would be involved in the interaction if any atom excluding hydrogen of the residues were at a Euclidean distance of 8{\AA} or less. Then, we identify all residues which are likely to be involved in the interactions and find where these residues are located on the entire protein sequence.

\subsection{Secondary structure statistics}
\label{appendix:a2}

We analyze the secondary structures of 166,512 structures that can be processed through a DSSP module of Biopython. Biopython classifies the secondary structures as eight classes by default: alpha helix (4-12) (code: `H'), isolated beta-bridge residue (code: `B'), strand (code: `E'), 3-10 helix (code: `G'), pi helix (code: `I'), turn (code: `T'), bend (code: `S'), and none (code: `-'). In our study, The eight classes are mapped to the three classes as follows: `H', `G', and `I' are mapped to the $\alpha$-helix class `H'; `B' and `E' are mapped to the $\beta$-sheet class `E'; `T', `S', `C', and `-' are mapped to the coil class `C'. In addition, in the right part of Figure~\ref{fig_distribution}, `-' is displayed separately.

Figure~\ref{fig_distribution} shows that $\alpha$-helix substructures are dominant in natural proteins, meaning imbalance. Furthermore, coil structures have rules that are difficult to capture. Therefore, the model trained using the existing natural protein database would be familiar with the $\alpha$-helix generation. Therefore, a preprocessing or encoding technique that can alleviate the $\alpha$-helix bias can be a good research topic in the future.



\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=480pt]{./figures/appendix_figure1.pdf}}
\caption{Secondary structure distribution of proteins in PDB database. H, E, C, and - correspond to $\alpha$-helix, $\beta$-sheet, coil, and none-type. Left: describes 3-classes secondary structure distribution. Right: because coil can be divided into two categories, the coil and none-type structure in DSSP algorithm, we can calculate a 4-classes distribution.}
\label{fig_distribution}
\end{center}
\vskip -0.2in
\end{figure*}


\subsection{Training datasets}
For training, protein sequences from UniRef50\cite{suzek2015uniref} dated March 28, 2018 version are used to avoid leakage of CASP13, 14 and conduct a fair comparison with other models. 5\% of protein sequences in the UniRef50 are randomly selected as a held-out validation set. The total number of sequences in training data is 25M. 

\subsection{Training details}
ProtFIM is trained with a batch size of 128. The maximum length of each protein sequence we used for training is 1024. For ProtFIM optimization, we use AdamW optimizer~\cite{kingma2014adam,loshchilov2017decoupled} with a weight decay ratio of 1e-5. The learning rate is scheduled using cosine-warmup strategy. The total optimization step is 500k with 1k warmup steps. We train the model on 8 NVIDIA A100s in 4 days. FIM transformation is applied with 50\% of probability. The model consists of 12 layers with a feature dimension of 768. The architecture is based on the released GPT2-base model by HuggingFace~\cite{wolf2019huggingface}.

\subsection{Generation hyper-parameters}
We conduct sequence generation using HuggingFace generation API. Th topK and topP values are set to 100 and 0.95. We set the temperature as 1.0. After sequence generation, we select top-K sequences. If shorter sequences are generated compared to the length of middle sites, we increase topK by 10 and conduct generation until K sentences are collected. We use the default option of HuggingFace API for other hyper-parameters. These hyper-parameters and generation processes are applied on ProtFIM, ProtGPT2, and ProGen2 models for a fair comparison. Also, we use ProtXLNet to generate sequences of target sites auto-regressively with conditioning on bidirectional context using topK sampling as other AR models.


\subsection{Perplexity and sequence recovery} \label{appendix:6}
We also add other evaluation metrics, such as perplexity and sequence recovery rates, which are widely used for evaluating language models in inverse folding. Table~\ref{perplexity_recovery} shows the result of perplexity and sequence recovery rates. ProtFIM performs poorly in terms of perplexity and sequence recovery rates. In the FIM paper~\cite{bavarian2022efficient}, some experiments find that perplexity alone is insufficient for evaluating the infilling task because infilling is conducted in a somewhat different nature compared to conventional left-to-light generation as expressed like $P_{FIM}(M \mid P, S) > P_{AR}(M \mid P)$ where P, M, S indicate prefix, middle, and suffix part, respectively. 

Additionally, our targeted infilling task aims to design various sequences in the presence of local structure constraints considering the surrounding context, which is quite different from restoring residues as much as possible. So, there needs to be an appropriate evaluation scheme simulating protein middle engineering tasks that change amino acid residues of local parts of the protein to optimize the target protein, such as enzymes and antibodies. So, sequence residue recovery rate, a widely used metric to evaluate models' sequence design performance, is insufficient for the protein infilling task. Based on the above results and descriptions, we argue that our proposed SEIFER tasks are more appropriate for evaluating protein infilling tasks than existing metrics such as perplexity and sequence recovery rates.


% \begin{table}[ht]
%   \begin{center}
%      \caption{Perplexity and sequence recovery rates}
%     \centering
%     \begin{adjustbox}{width=280pt}
%     \begin{tabular}{l|ccccc}
%       \toprule % <-- Toprule here
%        Model & \#Params & Objective & Perplexity ($\downarrow$) & Recovery rate (\%) ($\uparrow$)\\
%       \midrule % <-- Midrule here
%        ProGen2-small  & 151M & CLM & 16.88 & 8\\
%        ProGen2-medium & 764M & CLM & 16.17 & 10\\
%        ProGen2-large  & 2.7B & CLM & 16.24 & 9\\
%        ProtXLNet   & 409M & PLM & 16.58 & 8\\
%        ProtGPT2-C & 80M & CLM & 17.08 & 8\\
%        ProtFIM        & 80M & FIM & 17.04 & 9\\
%     %   \texttt{MURAL(reprod.)+ETCL}-6 lang  & 55.3 & 14.7 \\
%     %   MURAL(reprod.) + ETCL  & 55.0 & 14.4 \\
%       \bottomrule % <-- Bottomrule here
%     \end{tabular}
%     \end{adjustbox}
   
%     \label{}
%   \end{center}
% \end{table}



\begin{table*}[t]
     \caption{Perplexity and sequence recovery rates.}
\label{perplexity_recovery}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
    \begin{adjustbox}{width=280pt}
    \begin{tabular}{l|ccccc}
      \toprule % <-- Toprule here
       Model & \#Params & Objective & Perplexity ($\downarrow$) & Recovery rate (\%) ($\uparrow$)\\
      \midrule % <-- Midrule here
       ProGen2-small  & 151M & CLM & 16.88 & 8\\
       ProGen2-medium & 764M & CLM & 16.17 & 10\\
       ProGen2-large  & 2.7B & CLM & 16.24 & 9\\
       ProtXLNet   & 409M & PLM & 16.58 & 8\\
       ProtGPT2-C & 80M & CLM & 17.08 & 8\\
       ProtFIM        & 80M & FIM & 17.04 & 9\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}



\subsection{Precision@K with regard to position and length}

Figure~\ref{precision_pos_length} include ablation studies of SEIFER performance in term of precision according to relative positions and length of target sites in a protein.



\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=480pt]{./figures/appendix_figure2.pdf}}
\caption{Performance changes in the metric of Precision@3 with regard to relative positions and length of middle sites.}
\label{precision_pos_length}
\end{center}
\vskip -0.2in
\end{figure*}

% \begin{figure*}
% \centering
% % \subfloat[\label{pos_precision3}]{\includegraphics[width=120pt]{./figures/Precision_3_per_relative_position.pdf}}\hfil
% \subfloat[\label{pos_precision3}]{\includegraphics[width=0.48\linewidth]{./figures_4/Precision_hit_precision_3_per_relative_position.pdf}}\hfil
% \subfloat[\label{length_precision3}]{\includegraphics[width=0.48\linewidth]{./figures_4/Precision_hit_precision_3_per_length.pdf}}\hfil
% \caption{Performance changes in the metric of Precision@3 with regard to relative positions and length of middle sites.}\label{relative_position}
% \end{figure*}


\subsection{FLIP} \label{appendix:a8}
Table~\ref{flip_aav},~\ref{flip_gb1}, and~\ref{flip_thermo} contain the zero-shot fitness prediction performances of various pLMs on three fitness landscapes.


\begin{table*}[t]
\caption{Zero-shot fitness prediction on adeno-associated virus (AAV) capsid proteins~\cite{bryant2021aav}. All scores are Spearman correlation.}
\label{flip_aav}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
    \begin{adjustbox}{width=360pt}
    \begin{tabular}{l|cccccccccc}
      \toprule % <-- Toprule here
      Model & \#Params & Objective &  Mut-Des & Des-Mut & 1-vs-rest & 2-vs-rest & 7-vs-rest & low-vs-high & Avg.\\
      \midrule % <-- Midrule here
      ESM-1b (mean)  & 750M & MLM & 0.63 & 0.59 & 0.04 & 0.26 & 0.46 & 0.18 & 0.36\\
      ESM-1v (mean)  & 750M & MLM & 0.55 & 0.44 & 0.18 & 0.16 & 0.45 & 0.20 & 0.33\\
      ProGen2-small  & 151M & CLM & 0.38 & 0.53 & 0.39 & 0.47 & 0.43 & 0.14 & 0.39\\
      ProGen2-medium & 764M & CLM & 0.19 & 0.25 & 0.14 & 0.30 & 0.22 & 0.00 & 0.18\\
      ProGen2-large  & 2.7B & CLM & 0.68 & 0.67 & 0.33 & 0.20 & 0.42 & 0.13 & 0.41\\
      ProtXLNet      & 409M & PLM & 0.55 & 0.58 & 0.21 & 0.02 & 0.42 & 0.20 & 0.33\\
      ProtGPT2-C       & 85M & CLM & 0.59 & 0.66 & 0.24 & 0.34 & 0.41 & 0.16 & 0.40\\
      ProtFIM        & 85M & FIM & 0.53 & 0.56 & 0.32 & 0.24 & 0.44 & 0.28 & 0.39\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}



\begin{table*}[t]
\caption{Zero-shot fitness prediction on adeno-associated virus GB1 landscape~\cite{wu2016gb1}. All scores are Spearman correlation.}
\label{flip_gb1}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
    \begin{adjustbox}{width=320pt}
    \begin{tabular}{l|ccccccc}
      \toprule % <-- Toprule here
      Model & \#Params & Objective & 1-vs-rest & 2-vs-rest & 3-vs-rest & low-vs-high & Avg.\\
      \midrule % <-- Midrule here
      ESM-1b (mean)  & 750M & MLM & 0.32 & 0.36 & 0.54 & 0.13 & 0.34\\
      ESM-1v (mean)  & 750M & MLM & 0.32 & 0.32 & 0.77 & 0.10 & 0.38\\

      ProGen2-small  & 151M & CLM & -0.27 & -0.30 & -0.26 & -0.03 & -0.21\\
      ProGen2-medium & 764M & CLM & -0.06 & -0.16 & -0.12 & -0.10 & -0.11\\
      ProGen2-large  & 2.7B & CLM & 0.19 & 0.28 & 0.44 & 0.06 & 0.24\\
      ProtXLNet      & 409M & PLM & 0.18 & 0.33 & 0.44 & 0.21 & 0.29\\
      ProtGPT2-C & 85M & CLM & 0.02 & 0.05 & 0.44 & 0.20 & 0.18\\
      ProtFIM        & 85M & FIM & 0.01 & 0.18 & 0.63 & 0.18 & 0.25\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\begin{table*}[t]
\caption{Zero-shot fitness prediction on the landscape from the Meltome Atlas~\cite{jarzab2020meltome}. All scores are Spearman correlation.}
\label{flip_thermo}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
    \begin{adjustbox}{width=280pt}
    \begin{tabular}{l|cccccc}
      \toprule % <-- Toprule here
      Model & \#Params & Objective &  Mixed & Human & Human-Cell & Avg.\\
      \midrule % <-- Midrule here
      ESM-1b (mean)  & 750M & MLM & 0.68 & 0.70 & 0.75 & 0.71\\
      ESM-1v (mean)  & 750M & MLM & 0.67 & 0.75 & 0.74 & 0.72\\

      ProGen2-small  & 151M & CLM & 0.46 & 0.63 & 0.59 & 0.56\\
      ProGen2-medium & 764M & CLM & 0.49 & 0.66 & 0.62 & 0.59\\
      ProGen2-large  & 2.7B & CLM & 0.67 & 0.70 & 0.66 & 0.68\\
    %   ProGen2-xlarge & 6.4B & CLM & 0.53 & 0.46 & 0.37 & 0.45\\
      ProtXLNet      & 409M & PLM & 0.44 & 0.52 & 0.47 & 0.47\\
      ProtGPT2-C & 85M & CLM & 0.49 & 0.55 & 0.54 & 0.53\\
      ProtFIM        & 85M & FIM & 0.51 & 0.66 & 0.63 & 0.60\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}