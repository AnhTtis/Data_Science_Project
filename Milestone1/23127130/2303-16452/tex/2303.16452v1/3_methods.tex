\section{Method}
\paragraph{Problem Setup} In NLP, infilling is defined as generating complete text $x$ given incomplete text $\Tilde{x}$, including one or more missing spans. Similarly, we can regard protein engineering on middle residues as an infilling task where models are tasked to return new protein sequences $s$ given incomplete protein sequence $~\Tilde{s}$ containing missing residues on the target region. Additionally, in the protein infilling task, there is a special structure conservation constraint where the secondary structure of the target site is maintained to approximate the protein engineering scenario properly. Taken together, our goal is to develop a pLMs, $f(\Tilde{s} ; \theta)$, which outputs complete protein sequence $s$ based on a distribution $p(s|\Tilde{s})$ and sequence $s$ must have different residues while having the same secondary structure as that of the original residues.


\begin{figure*}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=360pt]{./figures/figure3.pdf}}
\caption{Illustration of our SEIFER evaluation scheme, which estimates the recovery rates of the secondary structures of the generated structure for the original secondary structure.}
\label{figure_seifer}
\end{center}
\vskip -0.2in
\end{figure*}


\subsection{Model requirements} 
Language modeling that learns patterns of raw sequences includes masked language modeling (MLM), causal language modeling (CLM), permutation language modeling (PLM), and infilling language modeling (ILM), all of which have achieved great success for specific applications.
Therefore, it is crucial to select a model that best fits the characteristics of the system to be solved, so defining essential requirements of the target system is preemptive to adopting the model.
We suggest four key characteristics of pLMs suitable for protein infilling tasks as follows:

% \begin{itemize}

% \item (1) Dynamic property: The model can handle various lengths of protein sequences because the lengths of the middle sites are diverse depending on various applications.

% \item Causal modeling: Previous studies reveal that AR pLMs have data-driven co-evolutionary rules across natural protein sequences and generate plausible sequences that tend to be well-folded. So, pLMs which have both AR and infilling capabilities would be optimal.

% \item (2) Generation considering the surrounding context: \rian{The structure and function of proteins are determined through complex interactions between amino acids and atoms constituting the amino acids. Therefore, sequence generation must be conducted considering the contexts around the infilling site to be optimized.}


% \item (3) Efficiency: Various strategies, such as pre-processing training data, modifying the model architecture, and using special tokens for controlling, can be used. However, these approaches must be fulfilled as efficiently as possible because protein sequence length is relatively long (we use the maximum length of residues as 1024 in this work).

% \item (4) Diversity: Because there are many combinations giving the same secondary structures, pLMs which generate diverse sequences different from existing sequences are preferred.


% \end{itemize}

\begin{enumerate}

\item [(1)] Dynamic property: The model can handle various lengths of protein sequences because the lengths of the middle sites are diverse depending on various applications.

\item [(2)] Generation considering the surrounding context: The structure and function of proteins are determined through complex interactions between amino acids and atoms constituting the amino acids. Therefore, sequence generation must be conducted considering the contexts around the infilling site to be optimized.


\item [(3)] Efficiency: Various strategies, such as pre-processing training data, modifying the model architecture, and using special tokens for controlling, can be used. However, these approaches must be fulfilled as efficiently as possible because the protein sequence length is relatively long (we use the maximum length of residues as 1024 in this work).

\item [(4)] Diversity: Because there are many combinations giving the same secondary structures, pLMs that generate diverse sequences different from existing sequences are preferred.


\end{enumerate}


% To achieve the above characteristics, we adopt the idea of FIM transformation, which is a very recently proposed FIM causal language modeling strategy by ~\citet{bavarian2022FIM}. The following section explains how to develop FIM pLMs and generate protein sequences using the model. 


For the protein infilling sequence generation, an MLM-based non-AR method is a good option considering (2) and (3) but is relatively weak in (1) and (4). On the other hand, CLM has shown strengths in (1) and (4) but has weaknesses in (2) and (3).
PLM satisfies (1), (2), and (4), but there are computation costs during training in terms of (3).
Recently, many works have revealed that infilling training using a simple input transformation enables full-context conditioning generation in an existing AR model. From this point of view, we believe that this infilling modeling will overcome the weaknesses of the AR model (2) and (3) to obtain a model suitable for the protein infilling task. Conventionally, infilling modeling uses unique tokens~\cite{aghajanyan2022cm3, donahue2020enabling}, and we use an efficient infilling approach using the latest technique, fill-in-middle transformation~\cite{bavarian2022FIM}.


\subsection{Model development}
\paragraph{FIM training}
In FIM transformation, a span of text from the middle of a whole sentence is moved to its end, and additional special tokens are introduced for marking where spans are from. The transformation is stochastically fulfilled during causal language modeling training. Intriguingly, this simple and straightforward transformation successfully gives fill-in-the-middle (FIM) capability to the model without modifying model architecture and sacrificing left-to-right causal generation capacity. The transformation is easily applied to protein sequence modeling as follows. First, we tokenize each residue $R$ of a protein sequence $S$ with length $N$ to the sequence consisting of corresponding tokens $T$ (see eqn.~\ref{eqn:eq1} and~\ref{eqn:eq2}). 
\begin{equation}\label{eqn:eq1}
S = (R_{1}, R_{2}, ..., R_{N})
\end{equation}
\begin{equation}\label{eqn:eq2}
S_{t} = (T_{1}, T_{2}, ..., T_{N})
\end{equation}
Second, we conduct uniform sampling to get the start position $K$ of the middle span of length $L$ and add special tokens [PRE], [MID], and [SUF] at the beginning of each prefix, middle, and suffix part, respectively. Finally, FIM-transformed sentences are created by concatenating prefix, suffix middle in order as eqn.~\ref{eqn:eq_3}.
\begin{multline}\label{eqn:eq_3}
S^{'}_{t} = ([PRE], R_{1}, ..., R_{K-1}, [SUF], R_{K+L+1}, ..., R_{N}, \\ 
[MID], R_{K},..., R_{K+L})
\end{multline}
Because several residues are needed to form a secondary structure, the middle residue sampling is conducted so that both prefix and suffix parts have at least four residues. The traditional GPT2 architecture from Hugging Face~\cite{wolf2019huggingface} is used for training, and FIM transformation is applied to the input with a 50\% frequency. We denote pLMs trained using FIM transformation as ProtFIM in this work. More details are written in Appendix A.4.


\paragraph{FIM inference for middle residue engineering}
For generating complete sequences in protein infilling tasks, we consider the target region as the middle part, and the front and back regions of the target region are prefixes and suffixes. Then, we make a prompt for FIM generation by concatenating the prefix part, suffix part, and [MID] token as eqn.~\ref{eqn:eq_4}. 
\begin{multline}\label{eqn:eq_4}
P^{'}_{t} = ([PRE], R_{1}, ..., R_{K-1}, \\
[SUF], R_{K+L+1}, ..., R_{N}, [MID])
\end{multline}
