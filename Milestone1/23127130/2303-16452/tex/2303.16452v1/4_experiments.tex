\section{Experiments}
Section~\ref{sec:ex_4_1} illustrates our proposed evaluation scheme, SEIFER, specially designed for protein infilling tasks. Section~\ref {sec:ex_4_2} describes metrics and various baseline models covering representative language modeling approaches such as causal language modeling (CLM) and permutation language modeling (PLM). Section~\ref{sec:ex_4_3} includes evaluation results of SEIFER tasks. Then, section~\ref{sec:ex_4_4} and~\ref{sec:ex_4_5} provide ablation studies of the SEIFER task concerning the relative position and length of the target region. Additionally, other metrics for evaluating pLMs, such as perplexity and sequence recovery, are provided in Appendix\ref{appendix:6}.


\subsection{Evaluation}
\label{sec:ex_4_1}

\paragraph{Protein secondary structure}

Protein secondary structures play a key role as an intermediate between the primary sequences and functional tertiary structures that determines the function of proteins in a variety of biological process. Therefore, designing properly optimized combinations of residues having the same secondary structures can lead to enhanced function of protein~\cite{rubio2019redesigning}. Protein secondary structures are categorized into regular and irregular categories. First, the regular structure includes $\alpha$-helix (H), $\beta$-sheet (E)~\cite{pauling1951structure}, and the irregular structure type is a coil (C). In this work, we adopt a 3-class secondary structure definition and those structures are calculated via DSSP~\cite{kabsch1983dssp}.


\paragraph{Protein secondary structure recovery via infilling} In this work, we propose a new evaluation scheme, Secondary structurE InFilling rEcoveRy, SEIFER, evaluating the sequence generation and structure conservation simultaneously. In the task, first, models are tasked to generate various sequences to fill the provided target sites. Since secondary structures are calculated based on three-dimensional structural information, the characterization of tertiary protein structures for each generated sequence must be preceded. Unfortunately, conducting experimental characterization on all the new sequences is practically impossible. Instead of this, we utilize Alphafold2~\cite{jumper2021highly}, which has shown near-experiments prediction performance, to predict tertiary structures of all generated sequences. Then, secondary structures of each new sequence are calculated via DSSP algorithm using DSSP module of Biopython~\cite{cock2009biopython}. Finally, the secondary structures of new sequences are compared to the original secondary structures. We assign a positive value, 1, in the case where all new residues have the same secondary structure as the original sequences. And all other cases are negative, 0. We illustrate the process of SEIFER in Figure~\ref{figure_seifer}. We use proteins presented in CASP14 to obtain candidate middle sites for SEIFER tasks. And, we argue that our experimental setting is reliable because AlphaFold2 was stringently assessed and proved by remarkable prediction performance on the proteins in the CASP14. Additionally, we use the middle sites, which have minimum lengths of 10, 6, and 6 for helix, beta, and coil structures, respectively, considering the average number of residues for the structures.

\paragraph{Difference of SEIFER over protein residue recovery task} Sequence recovery has been widely used to evaluate the generation performance of protein generative language models~\cite{ingraham2019generative}. However, considering that the objective of sequence optimization is to design new sequences with better target properties, recovery of original residues would not be proper. So, a metric is needed to evaluate whether the model can generate a variety of sequences while maintaining the function of the protein. Because the function of protein is directly linked to local structure, evaluating the model ability that generates different residues with the same local structure is a promising way. So, we argue that our proposed SEIFER tasks are appropriate for simulating sequence engineering scenarios, because in SEIFER tasks models are tasked to recover the protein's local structure, especially, secondary structures, not residues.



\begin{table*}[t]
\caption{Model performances on SEIFER tasks in terms of retrieval.}
\label{tab:figure1_3class_recall}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{adjustbox}{width=440pt}
\begin{tabular}{l|c|c|cc|cc|cc|ccc|c}
      \toprule % <-- Toprule here
       \multirow{2}{*}{Model} & \multirow{2}{*}{\#Params} & \multirow{2}{*}{Objective} &  \multicolumn{2}{c}{H ($\alpha$-helix)} & \multicolumn{2}{c}{E ($\beta$-sheet)} & \multicolumn{2}{c}{C (Coil)} & H & E & C & \multirow{2}{*}{All Avg}\\
         & & & R@3 & R@5 & R@3 & R@5 & R@3 & R@5 & Avg & Avg & Avg &\\
      \midrule % <-- Midrule here
      ProGen2-small     & 151M & CLM & 0.59 & 0.71 & 0.67 & 0.64 & 0.69 & 0.80 & 0.65 & 0.71 & 0.75 & 0.70\\
      ProGen2-medium     & 764M & CLM & 0.54 & 0.66 & 0.69 & 0.79 & 0.70 & 0.79 & 0.60 & 0.74 & 0.75 & 0.70  \\
      ProGen2-large     & 2.7B & CLM & 0.59 & 0.64 & 0.70 & 0.79 & 0.69 & 0.82 & 0.62 & 0.75 & 0.76 & \textbf{0.71} \\
      ProtXLNet     & 409M & PLM & 0.61 & 0.69 & 0.66 & 0.75 & 0.66 & 0.76 & 0.65 & 0.71 & 0.71 & 0.69  \\
      \midrule
       Random Generator & -    & -   & 0.46 & 0.60 & 0.76 & 0.82 & 0.76 & 0.80 & 0.53 & 0.79 & 0.78 & 0.70 \\
       ProtGPT2-C          & 85M  & CLM & 0.58 & 0.66 & 0.71 & 0.79 & 0.69 & 0.74 & 0.62 & 0.75 & 0.72 & 0.70 \\
       ProtFIM (ours)   & 85M  & FIM & 0.57 & 0.71 & 0.74 & 0.82 & 0.74 & 0.81 & 0.64 & 0.78 & 0.78 & \textbf{0.73} \\

\bottomrule
\end{tabular}
\end{adjustbox}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}


\begin{table*}[t]
\caption{Model performances on SEIFER tasks in terms of precision.}
\label{tab:figure1_3class_precision}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{adjustbox}{width=440pt}
\begin{tabular}{l|c|c|cc|cc|cc|ccc|c}
      \toprule % <-- Toprule here
       \multirow{2}{*}{Model} & \multirow{2}{*}{\#Params} & \multirow{2}{*}{Objective} &  \multicolumn{2}{c}{H ($\alpha$-helix)} & \multicolumn{2}{c}{E ($\beta$-sheet)} & \multicolumn{2}{c}{C (Coil)} &  H & E & C & \multirow{2}{*}{All Avg}\\
         & & & P@3 & P@5 & P@3 & P@5 & P@3 & P@5 & Avg & Avg & Avg & \\
      \midrule % <-- Midrule here
      ProGen2-small     & 151M & CLM & 0.32 & 0.32 & 0.45 & 0.43 & 0.47 & 0.48 & 0.32 & 0.44 & 0.48 & 0.41\\
      ProGen2-medium     & 764M & CLM & 0.31 & 0.32 & 0.45 & 0.47 & 0.49 & 0.47 & 0.32 & 0.46 & 0.48 & 0.42\\
      ProGen2-large     & 2.7B & CLM & 0.36 & 0.34 & 0.44 & 0.45 & 0.50 & 0.51 & 0.35 & 0.45 & 0.51 & \textbf{0.43}\\
      ProtXLNet     & 409M & PLM & 0.37 & 0.36 & 0.42 & 0.42 & 0.49 & 0.49 & 0.37 & 0.42 & 0.49 & \textbf{0.43}\\
      \midrule
       Random Generator & -    & -    &  0.25 & 0.25 & 0.49 & 0.47 & 0.47 & 0.45 & 0.25 & 0.48 & 0.46 & 0.40\\
       ProtGPT2-C          & 85M  & CLM & 0.31 & 0.31 & 0.48 & 0.47 & 0.45 & 0.45 & 0.31 & 0.48 & 0.45 & 0.41\\
       ProtFIM (ours)          & 85M  & FIM & 0.31 & 0.32 & 0.45 & 0.46 & 0.48 & 0.48 & 0.32 & 0.46 & 0.48 & \textbf{0.42}\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}



% \begin{table*}[t]
% \caption{Model performances on SEIFER tasks in terms of retrieval.}
% \label{tab:figure1_3class_recall}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{adjustbox}{width=350pt}
% \begin{tabular}{l|c|c|cc|cc|cc}
%       \toprule % <-- Toprule here
%        \multirow{2}{*}{Model} & \multirow{2}{*}{\#Params} & \multirow{2}{*}{Objective} &  \multicolumn{2}{c}{H ($\alpha$-helix)} & \multicolumn{2}{c}{E ($\beta$-sheet)} & \multicolumn{2}{c}{C (Coil)}\\
%          & & & R@3 & R@5 & R@3 & R@5 & R@3 & R@5 \\
%       \midrule % <-- Midrule here
%        Random Generator & -    & -   & 0.46 & 0.60 & 0.76 & 0.82 & 0.76 & 0.80\\
%       ProGen2-small     & 151M & CLM & 0.59 & 0.71 & 0.67 & 0.64 & 0.69 & 0.80\\
%       ProGen2-medium     & 764M & CLM & 0.54 & 0.66 & 0.69 & 0.79 & 0.70 & 0.79 \\
%       ProGen2-large     & 2.7B & CLM & 0.59 & 0.64 & 0.70 & 0.79 & 0.69 & 0.82\\
%       ProtXLNet     & 409M & PLM & 0.61 & 0.69 & 0.66 & 0.75 & 0.66 & 0.76\\
%        ProtGPT2          & 85M  & CLM & 0.58 & 0.66 & 0.71 & 0.79 & 0.69 & 0.74\\
%        ProtFIM (ours)   & 85M  & FIM & 0.57 & 0.71 & 0.74 & 0.82 & 0.74 & 0.81\\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}


% \begin{table*}[t]
% \caption{Model performances on SEIFER tasks in terms of precision.}
% \label{tab:figure1_3class_precision}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{adjustbox}{width=350pt}
% \begin{tabular}{l|c|c|cc|cc|cc}
%       \toprule % <-- Toprule here
%        \multirow{2}{*}{Model} & \multirow{2}{*}{\#Params} & \multirow{2}{*}{Objective} &  \multicolumn{2}{c}{H ($\alpha$-helix)} & \multicolumn{2}{c}{E ($\beta$-sheet)} & \multicolumn{2}{c}{C (Coil)}\\
%          & & & P@3 & P@5 & P@3 & P@5 & P@3 & P@5 \\
%       \midrule % <-- Midrule here
%        Random Generator & -    & -    &  0.25 & 0.25 & 0.49 & 0.47 & 0.47 & 0.45\\
%       ProGen2-small     & 151M & CLM & 0.32 & 0.32 & 0.45 & 0.43 & 0.47 & 0.48\\
%       ProGen2-medium     & 764M & CLM & 0.31 & 0.32 & 0.45 & 0.47 & 0.49 & 0.47\\
%       ProGen2-large     & 2.7B & CLM & 0.36 & 0.34 & 0.44 & 0.45 & 0.50 & 0.51\\
%       ProtXLNet     & 409M & PLM & 0.37 & 0.36 & 0.42 & 0.42 & 0.49 & 0.49\\
%        ProtGPT2          & 85M  & CLM & 0.31 & 0.31 & 0.48 & 0.47 & 0.45 & 0.45\\
%        ProtFIM (ours)          & 80M  & FIM & 0.31 & 0.32 & 0.45 & 0.46 & 0.48 & 0.48\\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table*}




\subsection{Experimental setup}
\label{sec:ex_4_2}

\paragraph{Baseline} 
% We compare our ProtFIM with other pLMs covering diverse generation strategies.

% \begin{itemize}

% \item ProGPT2-C: To prove the effect of FIM over AR in protein engineering, we train AR-based pLMs using the same data, hyperparameters, and the number of parameters compared to ProtFIM. It is similar to the previous AR model, ProtGPT2~\citep{ferruz2022protgpt2}, but our model is trained in the amino acid level. Thus, we name the amino acid residue-level (character-level) version of ProtGPT as ProtGPT2-C.


% \item ProGen2: ProGen2~\citep{nijkamp2022progen2} is a concurrently released suite of AR pLMs with various parameters.

% \item ProtXLNet: XLNet\citep{yang2019xlnet} can protein sequences using prefix and suffix information. Like FIM, sequences of target sites are generated auto-regressively with conditioning on bidirectional context from both the prefix and suffix. We borrow the publicly released ProtXLNet model, a variant of XLNet for protein~\citep{prottrans}.


% \item Random generator: This generator is used to approximate the random mutagenesis technique, error-prone PCR~\citep{mccullum2010random}, which is still commonly used in protein engineering~\citep{dror2014protein}.

% \end{itemize}


 We compare our ProtFIM with other pLMs covering diverse generation strategies. First, for a fair comparison, we train ProtFIM and ProtGPT2-C using the same data and optimization steps. Both models learn protein sequence at the character level, namely, the residue level. To approximate the conventional mutagenesis method, we employ a random generator, which randomly samples amino acids at target locations. Additionally, we employ publicly released other PLMs used as baselines. Specifically, we select ProGen2 as a representative AR PLMs, a concurrently released suite of AR pLMs with various parameters from 151M to 2.7B. Because XLNet\cite{yang2019xlnet} can consider both prefix and suffix information during inference because of permutation language modeling, we also employ the publicly released ProtXLNet model, a variant of XLNet for protein~\cite{prottrans} as another PLMs considering full-context.



\paragraph{Evaluation metrics} SEIFER measures how many sequences with the same secondary structure exist among new sequences created by a generative model. It is like a retrieval or recommendation engine for protein sequences. In the SEIFER task, all models generate K sequences for N middle sites, and all sequences are evaluated by whether the whole secondary structures at each target site are recovered. If the whole secondary structures are recovered, it is a true positive (TP). Then, Precision@K is the mean of TP/K for N sites. Also, we use Retrieval@K, which assumes a positive case where any true positive sequence exists in generated K sequences, zero otherwise. Thus, Retrieval@K is (the number of sites having TP among K)/N.



\subsection{Experimental results on SEIFER tasks}
\label{sec:ex_4_3}
\paragraph{Retrieval-view} As shown in Table~\ref{tab:figure1_3class_recall}, ProtFIM performs better than ProtGPT2-C regarding retrieval. Considering the fact that both models are trained with the same data, hyperparameters, and number of parameters, with the exception of the fill-in-middle transformation, these results indicate that conditioning on both prefixes and suffixes is necessary for improved sequence design in protein engineering. Interestingly, ProtFIM also outperforms the existing PLMs, which are trained with 2-20 times larger weights via CLM and PLM. In particular, 2B-parameterized ProGen-Large has an average retrieval score of 0.71, but 85M-parameterized ProtFIM has a higher score of 0.73. In addition, ProtFIM is superior to ProtXLNet, which is another PLMs that construct sequences based on their surrounding contexts. This indicates that infilling achieves both efficiency and effectiveness.

% Specifically, 2B-parameterized ProGen-Large shows a 0.71 of averaged retrieval score, but 85M-parameterized ProtFIM has 0.73. Furthermore, ProtFIM beats ProtXLNet, which can generate sequences using surrounding contexts. This results indicate that infilling achieve both efficiency and performance.}

\paragraph{Precision-view} Table~\ref{tab:figure1_3class_precision} includes the SEIFER scores in terms of precision. We find that ProtFIM continues to outperform ProtGPT2-C. Contrary to the retrieval-view, existing PLMs shows strong performance. However, we see that ProtFIM exhibits the same score of 0.42 as ProGen2-medium, which is nine times larger than ProtFIM, highlighting the importance of considering pre- and post-context surrounding target sites again.

% \paragraph{Precision-view} \rian{We note that existing PLMs have very similar or worse performance compared random generator. More specifically, only ProGen2-Large and ProtFIM shows better performance.}

\paragraph{Comparison to random mutation} Intriguingly, we discover that current PLMs perform similarly to or worse than the random generator under certain situations. To explore these empirical results, we calculated secondary structure recovery scores for each of the three secondary structures. Interestingly, we find that all PLMs noticeably outperform the random generator only for the $\alpha$-helix, but not for the coils and beta sheets. To analyze the finding, we check the distribution of secondary structures for the proteins with known structures by calculating the distribution of secondary structures in proteins from PDB (details are described in Appendix\ref{appendix:a2}). Figure~\ref{fig_distribution} depicts 3-classes and 4-classes secondary structure distribution, demonstrating that the $\alpha$-helix structures are dominant in natural protein structures. This empirical result is consistent with the widely known observation in the protein community. We hypothesize that this imbalance gives undesirable $\alpha$-helix bias in existing protein sequence datasets. Additionally, the coil usually has unordered noisy structures. Taking the above facts together, it is reasonable to conclude that the similar or worse performances of pLMs in $\beta$-sheets and coil to the random generator are reasonable because helix bias makes it difficult for models to learn the rules of generating residues consisting of the coil and $\beta$-sheet.
% and ~\ref{tab:figure1_3class_precision}


% As shown in Table~\ref{tab:figure1_3class_recall} and ~\ref{tab:figure1_3class_precision}, all pLMs have better performance than the random generator in helix structure recovery in views of both retrieval and precision. These results describe that pLMs are promising tools to fill in middle residues of target protein during protein engineering on helix structure. In contrast, all pLMs perform similarly or worse than the random generator in the $\beta$-sheet and coil recovery. To investigate the result, we check the distribution of secondary structures for the proteins with known structures by calculating the distribution of secondary structures in proteins from PDB (details are described in Appendix\ref{appendix:a2}). Figure~\ref{fig_distribution} illustrates 3-classes and 4-classes secondary structure distribution, showing that the $\alpha$-helix structures are dominant in natural protein structures. This empirical result is consistent with the widely known observation in the protein community. We conjecture that this imbalance gives unwanted $\alpha$-helix bias in existing protein sequence datasets. Additionally, the coil usually has unordered noisy structures. Taking the above facts together, it is possible to say that the similar or worse performances of pLMs in $\beta$-sheets and coil cases are reasonable because helix bias makes it models hard to learn the rules of generating residues consisting of the coil and $\beta$-sheet.

% Meanwhile, we compare ProtFIM over ProtGPT2-C to see the effectiveness of FIM compared to CLM. As shown in each table's fifth and sixth rows, ProtFIM performs better than ProtGPT2-C in helix recovery. Because both models are trained using the same data, hyperparameters, and the number of parameters except for the utilization of fill-in-middle transformation, these results support that conditioning on both prefixes and suffixes during generation is essential for better sequence design for protein engineering.

% We also compare ProtFIM with other pLMs, such as ProGen2 and ProtXLNet. XLNet is another possible model which is able to fill-in-middle protein engineering using both the prefix and suffix. It is found that ProtXLNet shows strong performance in $\alpha$-helix compared to the similar scale of models, such as ProGen2-small and ProGen2-large. These results prove that sequence design needs to be conducted using the surrounding context of target sites. On the other hand, our proposed ProtFIM shows comparable performance in term of retrieval and competitive performances in term of precision compared to other larger models by 2-20 times. These results show that the FIM scheme is parametrically efficient for protein middle engineering tasks.


\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=200pt]{./figures/Recall_hit_precision_3_per_relative_position.pdf}}
\caption{Performance changes in the term of retrieval with regard to (a) relative positions of middle sites.}
\label{positions}
\end{center}
\vskip -0.2in
\end{figure}




\subsection{Ablation study and analysis}
\paragraph{Change upon the position of target region} 
\label{sec:ex_4_4}

% Additionally, the fact that ProtFIM outperforms ProtXLNet in the front part shows the effectiveness of the FIM training scheme because ProtFIM has five times fewer parameters. Meanwhile, it is found that PLMs are generally better than the random generator in other parts, supporting the effectiveness of pLMs on protein middle engineering. In addition, it can be seen that the model's performance is not uniform over positions. We think that it is due to the lack of an evaluation dataset because the number of used CASP proteins is 28. However, since the models are compared under the same conditions, the insight obtained from the performance comparison in the experiments is reliable.


We start with an assumption that previous AR pLMs would be ineffective in FIM protein engineering because the sequence generation of AR pLMs is fulfilled by conditioning on only prefix residues. To verify whether this phenomenon occurs, we ablate the SEIFER performance concerning the relative position of the target middle sites. After dividing each protein sequence into four parts, the $\alpha$-helix recovery performances of each model corresponding to each part are averaged and illustrated in Figure~\ref{positions}. Interestingly, we observe that only ProtFIM consistently better performances compared to the random generator. Specifically, in the first part (front part), only two models, ProtFIM and ProtXLNet, which take both prefix and suffix parts into account outperform the random generator, but AR PLMs such as ProtGPT2-C and ProGen-series do not. These results confirm our hypothesis. Additionally, the fact that ProtFIM beats ProtXLNet in the front part proves the effectiveness of the FIM training scheme because ProtFIM has five times fewer parameters than ProtXLNet. Meanwhile, PLMs are generally better than the random generator in other parts, indicating the effectiveness of pLMs on protein middle engineering. In addition, the performance of the model is not uniform over positions. We think that it is due to the lack of an evaluation dataset because the number of used CASP proteins is 28. However, since the models are compared under the same conditions, the insight obtained from the performance comparison in the experiments is reliable.


\paragraph{Change upon the length of target region}
\label{sec:ex_4_5}

We can see that the random generator shows comparable performances to pLMs in several tasks in the above results. To investigate this phenomenon, we ablate the SEIFER performances according to the length of the middle sites. We partition the range of lengths into four parts, and plot corresponding averaged Recall@K as in Figure~\ref{length}. Interestingly, the random generator performs similarly to pLMs in the first quarter (short length size). However, the performance of the random generator drastically drops as the length of the target sites becomes longer, and it fails all predictions when the middle sites are longer than 30. Meanwhile, the performance of pLMs degrades gradually and fails at the last part, where the middle sites are longer than 40. All the results imply that the length of the middle sites is the main factor for model performance. We explain this using the degree of freedom on possible protein structures of target middle sites. Since the high-dimensional interactions between amino acids make the structure of the protein, the structure is determined to some extent by the structural context from other residues except residues of the middle sites. In other words, the degree of freedom in the structure of the middle sites is relatively small due to the non-target residues. Considering that any amino acid is a building block of a $\alpha$-helix, $\beta$-sheet, and coil structure, even if the amino acid is randomly sampled, there will be a high probability of obtaining the desired original structure in FIM scenarios. Meanwhile, the observation that pLMs still work at the longer middle sites shows that pLMs would be a promising solution for long FIM protein sequence design, giving efficient sequence search compared to random generator.

\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=200pt]{./figures/Recall_hit_precision_3_per_length.pdf}}
\caption{Performance changes in the term of retrieval with regard to (a) relative length of middle sites.}
\label{length}
\end{center}
\vskip -0.2in
\end{figure}


\begin{table*}[t]
\caption{Zero-shot fitness prediction on FLIP tasks. All scores are Spearman correlation.}
\label{tab:flip}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{adjustbox}{width=360pt}
\begin{tabular}{l|cccccc}
      \toprule % <-- Toprule here
      Model & \#Params & Objective &  AAV & GB1 & Meltome & Meta Avg.\\
      \midrule % <-- Midrule here
       ESM-1b (mean)  & 750M & MLM & 0.36 & 0.34 & 0.71 & 0.47\\
       ESM-1v (mean)  & 750M & MLM & 0.33 & 0.38 & 0.72 & 0.48\\
       ProGen2-small  & 151M & CLM & 0.39 & -0.21 & 0.56 & 0.25\\
       ProGen2-medium & 764M & CLM & 0.18 & -0.11 & 0.59 & 0.22\\
       ProGen2-large  & 2.7B & CLM & 0.41 & 0.24 & 0.68 & 0.44\\
       ProtXLNet      & 409M & PLM & 0.33 & 0.29 & 0.47 & 0.36\\
       \midrule
       ProtGPT2-C & 85M & CLM & 0.40 & 0.18 & 0.53 & 0.37\\
       ProtFIM    & 85M & FIM & 0.39 & 0.25 & 0.60 & \textbf{0.41}\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}




\paragraph{Representation quality}
% Collecting experimental functional properties of protein sequence gives insights into a sequence-to-function relationship called fitness landscape. In protein engineering, the fitness landscape is used to rank designed sequences. To this end, pLMs can provide sequence representation for fitness prediction. Recently, FLIP benchmarks have been introduced to assess the quality of representations of pLMs~\citep{dallago2021flip}. Using FLIP, we compare the embeddings of ProtFIM with baselines. Additionally, ESM-1b and 1v~\citep{rao2020transformer} are added to compare FIM with masked language modeling (MLM). In FLIP, embeddings of sequences are directly used to predict fitness without fine-tuning pLMs. For a fair comparison, embeddings are obtained via averaging of all residue representations. Table~\ref{flip} shows that ProtFIM has comparable zero-shot fitness prediction performance even if the ProtFIM capacity is multiple times smaller than other models. This result implies that the embeddings of ProtFIM are effective for both FIM protein engineering and zero-shot fitness prediction. Detailed scores are included in Appendix~\ref{appendix:a8}.

% \rian{Collecting experimental functional properties of protein sequence gives insights into a sequence-to-function relationship called fitness landscape. In protein engineering, the fitness landscape is used to rank designed sequences. To this end, pLMs can provide sequence representation for fitness prediction. Recently, FLIP benchmarks have been introduced to assess the quality of representations of pLMs~\citep{dallago2021flip}. Using FLIP, we compare the embeddings of ProtFIM with baselines. Additionally, ESM-1b and 1v~\citep{rao2020transformer} are added to compare FIM with masked language modeling (MLM). In FLIP, embeddings of sequences are directly used to predict fitness without fine-tuning pLMs. For a fair comparison, embeddings are obtained via averaging of all residue representations. We note that the reason we conduct this experiment is not to show SOTA performance, but to show that infiling modeling has FIM properties while not harming representation learning of AR modeling. Table~\ref{tab:flip} shows that ProtFIM have strong protein representations over ProtGPT2-C, meaning that infiling modeling benefits representation learning because the modeling consider surrounding context where AR PLMs can not deal with. Also, when comparing the existing PLMs, ProtFIM have decent performance. In summary, the results prove that infiling modeling via FIM transformation gives PLMs both decent representation and FIM ability. Detailed scores are included in Appendix~\ref{appendix:a8}}


Collecting experimental functional properties of protein sequence gives insights into a sequence-to-function relationship called fitness landscape. In protein engineering, the fitness landscape is used to rank designed sequences. To this end, pLMs can provide sequence representation for fitness prediction. Recently, FLIP benchmarks have been introduced to assess the quality of representations of pLMs~\cite{dallago2021flip}. Using FLIP, we compare the embeddings of ProtFIM with baselines. Additionally, ESM-1b and 1v~\cite{rao2020transformer} are added to compare FIM with masked language modeling (MLM). In FLIP, embeddings of sequences are directly used to predict fitness without fine-tuning pLMs. For a fair comparison, embeddings are obtained via averaging of all residue representations.

Table~\ref{tab:flip} shows that ProtFIM has strong protein representations over ProtGPT2-C, meaning that infilling modeling benefits representation learning because the modeling considers surrounding contexts that AR PLMs can not deal with. When comparing the existing pLMs, ProtFIM has decent performance. We note that the reason we conduct this experiment is not to show SOTA performance but to show that infilling modeling can improve the representation quality of AR modeling. In summary, the results prove that infilling modeling via FIM transformation gives PLMs both decent representation and FIM ability. Detailed scores are included in Appendix~\ref{appendix:a8}

% do not hurt representation learning of pLMs rather comparable zero-shot fitness prediction performance even if the ProtFIM capacity is multiple times smaller than other models. This result implies that the embeddings of ProtFIM are effective for both FIM protein engineering and zero-shot fitness prediction. Detailed scores are included in Appendix~\ref{appendix:a8}. We note that we conduct this experiment not showing SOTA performance rather want to}