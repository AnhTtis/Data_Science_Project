{
    "arxiv_id": "2303.16452",
    "paper_title": "ProtFIM: Fill-in-Middle Protein Sequence Design via Protein Language Models",
    "authors": [
        "Youhan Lee",
        "Hasun Yu"
    ],
    "submission_date": "2023-03-29",
    "revised_dates": [
        "2023-03-30"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
    ],
    "abstract": "Protein language models (pLMs), pre-trained via causal language modeling on protein sequences, have been a promising tool for protein sequence design. In real-world protein engineering, there are many cases where the amino acids in the middle of a protein sequence are optimized while maintaining other residues. Unfortunately, because of the left-to-right nature of pLMs, existing pLMs modify suffix residues by prompting prefix residues, which are insufficient for the infilling task that considers the whole surrounding context. To find the more effective pLMs for protein engineering, we design a new benchmark, Secondary structureE InFilling rEcoveRy, SEIFER, which approximates infilling sequence design scenarios. With the evaluation of existing models on the benchmark, we reveal the weakness of existing language models and show that language models trained via fill-in-middle transformation, called ProtFIM, are more appropriate for protein engineering. Also, we prove that ProtFIM generates protein sequences with decent protein representations through exhaustive experiments and visualizations.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.16452v1"
    ],
    "publication_venue": "Preprint"
}