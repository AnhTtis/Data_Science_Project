% !BIB TS-program = biber

\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.1in,a4paper]{geometry}
\usepackage[shortlabels]{enumitem}
\usepackage{mathtools}
\usepackage{amsmath, amssymb, graphicx, url, amsthm, subcaption, dsfont}
\usepackage{mathrsfs}
\usepackage[ruled, lined]{algorithm2e}% linesnumbered


\usepackage[maxbibnames=99, style=alphabetic, maxalphanames=5]{biblatex}
\addbibresource{LC.bib}

%\bibliographystyle{plainnat}
\usepackage[mathcal]{eucal}
\usepackage{tikz}
\usepackage{verbatim}
 \usepackage{tcolorbox}
 \newtcolorbox{assbox}{colback=black!5!white,colframe=black!75!black}
  \newtcolorbox{thmbox}{colback=red!5!white,colframe=red!75!black}
  



%\usepackage{amsthm,mathtools,enumerate,subcaption}

%\usepackage{bm, dsfont}
%\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts


%\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
%\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{enumitem}
\usepackage{microtype}      % microtypography
\usepackage{xcolor}
\usepackage[colorlinks=true,citecolor=blue]{hyperref}    % hyperlinks
\usepackage{diagbox}   
\usepackage{tcolorbox}
\usepackage{nicefrac}
\newcommand{\TT}{\mathbb{T}}
\newcommand{\bnu}{\boldsymbol{\nu}}
\newcommand{\bw}{\boldsymbol{w}}
%\newcommand{\}{\mathbb{T}}


\input{shortcuts}
\DeclareMathOperator{\arcsinh}{arcsinh}
\DeclareMathOperator{\sfth}{sfth}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator*{\argmin}{argmin}
%\DeclareUnicodeCharacter{0301}{*************************************}
\graphicspath{ {./images/} } 

\title{
Doubly Regularized Entropic Wasserstein Barycenters
}

\author{
L\'ena\"ic Chizat\thanks{Institut de Mathématiques, École polytechnique fédérale de Lausanne (EPFL), \texttt{lenaic.chizat@epfl.ch}} 
}

\begin{document}
\maketitle

\begin{abstract}
We study a general formulation of regularized Wasserstein barycenters that enjoys favorable regularity, approximation, stability and (grid-free) optimization properties.
%
This barycenter is defined as the unique probability measure that minimizes the sum of entropic optimal transport (EOT) costs with respect to a family of given probability measures, \emph{plus} an entropy term. We denote it $(\lambda,\tau)$-barycenter, where $\lambda$ is the \emph{inner} regularization strength and $\tau$ the \emph{outer} one. This formulation recovers several previously proposed EOT barycenters for various choices of $\lambda,\tau \geq 0$ and generalizes them.
%
First, in spite of -- and in fact owing to -- being \emph{doubly} regularized, we show that our formulation is debiased for $\tau=\lambda/2$: the suboptimality in the (unregularized) Wasserstein barycenter objective is, for smooth densities, of the order of the strength $\lambda^2$ of entropic regularization, instead of $\max\{\lambda,\tau\}$ in general. We discuss this phenomenon for  isotropic Gaussians where all $(\lambda,\tau)$-barycenters have closed form.
%
Second, we show that for $\lambda,\tau>0$, the barycenter has a smooth density and is strongly stable under perturbation of the marginals. In particular, it can be estimated efficiently: given $n$ samples from each of the probability measures, it converges in relative entropy to the population barycenter at a rate $n^{-1/2}$.
%
And finally, this formulation lends itself naturally to a grid-free optimization algorithm: we propose a simple \emph{noisy particle gradient descent} which, in the mean-field limit, converges globally at an exponential rate to the barycenter. 
\end{abstract}


\section{Introduction}
Given a family of probability measures, its Wasserstein barycenter is a probability measure that summarizes it in a geometrically faithful way. This object, first studied in~\cite{agueh2011barycenters}, has found numerous applications in statistics~\cite{bassetti2006minimum, boissard2015distribution, bernton2019parameter}, image processing~\cite{rabin2011wasserstein}, computer graphics~\cite{solomon2015convolutional} and Bayesian inference~\cite{srivastava2018scalable, backhoff2022bayesian} (see~\cite[Chap.~9.2]{peyre2019computational} or~\cite{panaretos2020invitation} for introductions to this topic). While it is arguably one of the most natural notion of barycenters for probability measures, the Wasserstein barycenter is unfortunately difficult to estimate and compute in large-scale applications.

To overcome these limitations and following the idea of entropic regularization of optimal transport, a.k.a~the Schr\"odinger bridge problem~\cite{schrodinger1932theorie,wilson1969use,erlander1990gravity,kosowsky1994invisible,leonard2012schrodinger,cuturi2013sinkhorn}, various formulations of entropy-regularized Wasserstein barycenters have been proposed and studied in the literature (discussed below). In these works, entropic regularization is incorporated in two different ways, which we refer to as \emph{inner} and \emph{outer} regularizations, and with various reference measures. It is not clear a priori how these formulations relate to each other, and whether a particular one stands out for its mathematical or practical properties.

In this paper, we aim at clarifying and generalizing the picture by considering \emph{both} inner and outer regularizations at the same time. We prove that this formulation combines favorable analytical, approximation, stability and optimization properties, which are all enabled by the positive interaction between those two entropic regularizations.


\subsection{Entropic optimal transport (EOT)}
Let $\Xx$ be a compact and convex subset of $\RR^d$ with nonempty interior and let $c\in \Cc^p(\Xx\times\Xx)$ for some $p\geq 2$.
For two probability measures $\mu, \nu \in \Pp(\Xx)$, let $\Pi(\mu,\nu)$ be the set of transport plans\footnote{That is, probability measures on $\Xx\times \Xx$ with marginals $\mu$ and $\nu$ on each factor of $\Xx\times \Xx$.} between $\mu$ and $\nu$ and define the Entropic Optimal Transport (EOT) cost   as
\begin{align}\label{eq:EOT}
T_\lambda(\mu,\nu) \coloneqq \min_{\gamma \in \Pi(\mu,\nu)} \int_{\Xx\times \Xx} c(x,y) \d\gamma(x,y) + \lambda H(\gamma | \mu\otimes \nu) 
\end{align}
where $\lambda\geq 0$ is the regularization strength and $H(\mu|\nu)=\int \log\big(\frac{\d\mu}{\d\nu}\big)\d\mu$ if $\mu\ll\nu$ and $+\infty$ otherwise, is the relative entropy. Notice the choice of reference measure $\mu\otimes \nu$ for the regularization term in~\eqref{eq:EOT}, which is important for our exposition, see Section~\ref{sec:reference-measure}. With this choice, $T_\lambda$ is always finite, even for discrete measures (indeed, $\gamma=\mu\otimes \nu$ is a feasible point). Setting the regularization $\lambda$ to $0$, we recover the standard optimal transport problem, and the $L^2$-Wasserstein distance $W_2$ is defined as $W_2\coloneqq \sqrt{2T_0}$ when $c(x,y)=\frac12 \Vert y-x\Vert^2_2$. 


\subsection{Doubly Regularized EOT Barycenter}\label{sec:intro-douba}

Given a family of $K$ probability measures $(\nu_1,\dots,\nu_K)\in \Pp(\Xx)^K$ and $K$ weights $(w_1,\dots,w_K)\in \RR_+^K$ summing to $1$, we define the EOT barycenter functional $G_\lambda:\Pp(\Xx)\to \RR$ as
\begin{align}\label{eq:G}
G_\lambda(\mu) \coloneqq \sum_{k=1}^K w_kT_\lambda(\mu,\nu_k).
\end{align}
We also denote by $H$ is the (negative) differential entropy 
\begin{align}\label{eq:entropy}
H(\mu) \coloneqq  \int_\Xx \log\Big(\frac{\d \mu}{\d x}\Big)\d \mu(x)
\end{align}
if $\mu$ is absolutely continuous and $+\infty$ otherwise. Our main object of study is the following doubly regularized EOT barycenter.


\begin{assbox}
\begin{definition}\label{def:debiaised-barycenter} For $\tau,\lambda\geq 0$, the \emph{$(\lambda,\tau)$-barycenter} is the minimizer $\mu^*_{\lambda,\tau} \in \Pp(\Xx)$ of
\begin{align}\label{eq:optim-objective}
F_{\lambda,\tau}(\mu) \coloneqq G_\lambda(\mu) + \tau H(\mu).
\end{align}
We refer to $\lambda$ (resp.~$\tau$) as the \emph{inner} (resp.~\emph{outer}) regularization strength.
Of particular interest is the $(\lambda,\nicefrac\lambda2)$-barycenter which is \emph{debiaised} for $c(x,y)\propto \Vert y-x\Vert^2$.
\end{definition}
\end{assbox}

As shown in Section~\ref{sec:reference-measure}, for absolutely continuous measures, $(\lambda,\tau)$-barycenters can also be interpreted as EOT barycenters with only inner regularization, but with a different reference measure $\sigma_{\mathrm{ref}}$ (replacing $\mu\otimes \nu$ in~\eqref{eq:EOT}):
\begin{align}
\sigma_{\mathrm{ref}} = \Big[\Big(\frac{\d\mu}{\d x}\Big)^{\alpha}\d x\Big]\otimes \Big[\Big(\frac{\d\nu}{\d y}\Big)^{\alpha}\d y\Big]&&\text{where}&&\alpha \coloneqq 1-\tau/\lambda<1.
\end{align} 
This in particular includes the case $\alpha=0$, that is $\sigma_{\mathrm{ref}}=\d x \otimes \d y$ (for $\tau=\lambda$) which is the historical formulation of EOT with Lebesgue as a reference measure known as the Schr\"odinger bridge problem. It follows that $(\lambda,\lambda)$-barycenters have implicitly been considered in many works~\cite{cuturi2014fast,cuturi2018semidual,bigot2019penalization} (see, e.g.~the reference book~\cite[Chap.~9.2]{peyre2019computational}). As we will see, interpreting those barycenters as doubly-regularized lead to a simpler and stronger analysis of their properties. We will also see that $(\lambda,\nicefrac\lambda2)$-barycenters (corresponding to $\alpha=\nicefrac12$) are in fact superior in terms of approximation of the $(0,0)$-barycenter. We mention that the general formulation of $(\lambda,\tau)$-barycenters in Def.~\ref{def:debiaised-barycenter} already appears in~\cite{ballu2020stochastic} where it is motivated by computational purposes and the analysis requires the constraint $\lambda<\tau$. 



\subsection{Relation to barycenters in the literature}
Various notions of barycenters for probability measures based on (entropy-regularized) optimal transport can be found in the literature. Most of them can be seen as $(\lambda,\tau)$-barycenters for particular choices of $\lambda$ and $\tau$  (see Table~\ref{tab:comparison} for a summary).
\begin{enumerate}[(i)]
\item \emph{Unregularized} OT barycenters. They are the minimizers $\mu_{0,0}^*$ of $G_0$ in $\Pp(\Xx)$, they were first studied in~\cite{agueh2011barycenters} and correspond to $(0,0)$-barycenters in Def.~\ref{def:debiaised-barycenter}. Under our assumptions, minimizers always exist but might not be unique. Uniqueness holds for instance in the case $c(x,y)\propto \Vert y-x\Vert_2^2$ and if at least one of the $\nu_k$ vanishes on small-sets (a condition weaker than absolute continuity)~\cite{agueh2011barycenters}. 
\item \emph{Inner-regularized} barycenters. They are defined as the minimizers of $G_\lambda$ and correspond to $(\lambda,0)$-barycenters. Uniqueness of this barycenter is not always granted as $G_\lambda$ is not strictly convex (think of $c$ the constant cost for which any $\mu\in\Pp(\Xx)$ is a barycenter). Compared to $\mu_{0,0}^*$, here the regularization typically induces a shrinking bias. In particular, the barycenter of Gaussians can be a Dirac mass when their covariances is small compared to $\lambda$~\cite{janati2020debiased} (see also Section~\ref{sec:gaussians}). To understand this, one may verify that for the square-distance cost, the minimizer of $T_\lambda(\cdot,\mu)$ is $\mu$ itself only when $\lambda=0$ (or $\mu$ is a Dirac mass), and otherwise, it is a deconvolution of $\mu$~\cite{rigollet2018entropic}. 
\item  To correct this fact and recover a distance-like quantity, the Sinkhorn divergence $
 S_{\lambda}(\mu,\nu) \coloneqq T_\lambda(\mu,\nu) - \frac12 T_\lambda(\mu,\mu)-\frac12 T_\lambda(\nu,\nu)
 $
 as been introduced~\cite{ramdas2017wasserstein}. It is indeed a positive definite quantity as long as $e^{-c/\lambda}$ is a positive definite universal kernel~\cite{feydy2019interpolating}, which is the case e.g. for $c(x,y)\propto\Vert x-y\Vert^2_2$. This suggests to consider \emph{Sinkhorn divergence} barycenters~\cite{janati2020debiased}, i.e.~the minimizers of 
 $$
\mu \mapsto \sum_{k=1}^K w_k S_\lambda(\mu,\nu) = G_\lambda(\mu) - \tfrac12 T_{\lambda}(\mu,\mu) + C
 $$
 where $C\in \RR$ does not depend on $\mu$.
 The self-EOT term effectively ``debiases'' the barycenter: this is shown for Gaussians in~\cite{janati2020debiased}, and for general smooth measures in Section~\ref{sec:approximation}. However, little else is known about this barycenter (regarding uniqueness, stability or regularity). This formulation is not covered by Def.~\ref{def:debiaised-barycenter}.
 %
\item \emph{Schr\"odinger} barycenters\footnote{These were called \emph{Sinkhorn barycenters} in~\cite{bigot2019penalization}; here we propose a name that conveys the choice of reference measure in the formulation.
}. In most works (e.g.~\cite{cuturi2014fast,cuturi2018semidual,bigot2019penalization}), inner-regularized barycenters are in fact considered with Lebesgue as a reference measure in~\eqref{eq:EOT} instead of $\mu\otimes \nu$. As discussed in the previous paragraph, they correspond to $(\lambda,\lambda)$-barycenters. This regularization leads to a blurring bias.
\item  \emph{Outer-regularized} barycenters. These are the minimizers of  $G_0+\tau H$, studied in~\cite{bigot2019data, carlier2021entropic}, which correspond to $(0,\tau)$-barycenters in Def.~\ref{def:debiaised-barycenter}. This barycenter has interesting regularity properties: for instance~\cite{carlier2021entropic} show bounds on the $L^\infty$ norm, moments and regularity of the barycenter (which are not known for the inner-regularized barycenters). This regularization induces a blurring bias as well.
\end{enumerate}
As can be seen, all the previously proposed OT-like barycenters -- except the Sinkhorn divergence barycenter -- appear to be $(\lambda,\tau)$-barycenters, with various formulations corresponding to different subsets of the $(\lambda,\tau)$ plane. In our analysis, we will often restrict ourselves to $\lambda,\tau>0$ for convenience.



\begin{table}
\centering
\begin{tabular}{l|l|l|l}
Barycenter & Objective & Approximation & Notation  \\
\hline
Un-regularized & $G_0$ & exact  & $\mu_{0,0}^*$   \\
Inner-regularized &$G_\lambda$ & shrinked  & $\mu_{\lambda,0}^*$   \\
Sinkhorn divergence & $G_\lambda -\frac12 T_\lambda(\cdot,\cdot)$  & debiaised  & $\mu_{\lambda,\mathrm{div}}^*$  \\
Schr\"odinger & $G_\lambda + \lambda H$ & blurred & $\mu^*_{\lambda,\lambda}$\\
Outer-regularized &$G_0 +\tau H$ & blurred  & $\mu_{0,\tau}^*$   \\
\textbf{Doubly-regularized} &$G_\lambda +\tau H$ & debiased for $\tau=\frac{\lambda}{2}$ & $\mu_{\lambda,\tau}^*$    \\
\hline
\end{tabular}
\caption{List of the various formulations of OT-like barycenters. The third column is an informal description of the bias from $\mu_{0,0}^*$ when the marginals $\nu_k$ are smooth densities, see Section~\ref{sec:approximation} for precise statements.
}\label{tab:comparison}
\end{table}




\subsection{Contributions}
Our contributions are the following:
\begin{itemize}
\item in Section~\ref{sec:statics}, we discuss basic variational properties of $(\lambda,\tau)$-barycenters. In particular, we prove that for $\lambda,\tau>0$, they have a smooth log-density (Thm.~\ref{thm:characterization}) and we derive a dual formulation (Prop.~\ref{prop:dual}).
\item in Section~\ref{sec:approximation}, we study the approximation error of $(\lambda,\tau)$-barycenter with respect to the unregularized Wasserstein barycenter for the square-distance cost. We prove that for smooth marginals $\nu_1,\dots,\nu_K$, the suboptimality of $(\lambda,\nicefrac{\lambda}{2})$-barycenters in the Wasserstein barycenter functional $G_0$ is of the order $\lambda^2$ and that the same holds for Sinkhorn divergence barycenters (Thm.~\ref{thm:approx}). We also compute and discuss the closed form of $(\lambda,\tau)$-barycenters between isotropic Gaussians (Prop.~\ref{prop:gaussians}). 

\item In Section~\ref{sec:statistics}, we give stability bounds for $(\lambda,\tau)$-barycenters (Thm.~\ref{thm:stability}). A consequence of these stability bounds is that they can be estimated in relative entropy given $n$ independent samples from each $\nu_k$ with an expected error in $O(\tau^{-1}(1+\lambda^{-d/2})n^{-1/2})$.

\item To compute this barycenter, we introduce in Section~\ref{sec:dynamics} a grid-free numerical method: Noisy Particle Gradient Descent (NPGD). We prove the well-posedness and exponential convergence to the global minimizer (Thm.~\ref{thm:global-convergence}) of this optimization dynamics in the mean-field limit, i.e. when the number of particles grows to infinity. 

\item Numerical results are presented in Section~\ref{sec:numerics}. There we give examples of $(\lambda,\tau)$-barycenters on a simple 1D problem solved via convex optimization and we illustrate the global convergence of the grid-free method NPGD on an example where $G_0$ has a spurious minimizer. 

\end{itemize}


\paragraph{Blanket assumptions}
Throughout $\Xx \subset \RR^d$ is a compact convex set with nonempty interior and $c\in \Cc^p(\Xx\times \Xx)$ for some $p\geq 2$. 


\section{Well-posedness and regularity}\label{sec:statics}
This section contains basic mathematical results about $(\lambda,\tau)$-barycenters and the optimization problem defining them (Def.~\ref{def:debiaised-barycenter}). 

\subsection{Preliminaries: regularity of EOT} Let us begin with some useful facts about EOT. The problem~\eqref{eq:EOT} that defines $T_\lambda$ has a unique solution $\gamma^*\in \Pp(\Xx\times \Xx)$ and admits the dual formulation
\begin{align}\label{eq:dual-EOT}
T_\lambda(\mu,\nu) = \max_{\phi\in L^1(\mu),\psi\in L^1(\nu)} \int \phi \d\mu + \int \psi \d\nu +\lambda \Big(1 - \int e^{(\phi(x)-\psi(y)-c(x,y))/\lambda}\d\mu(x)\d\nu(y)\Big).
\end{align}
This dual problem has a solution $(\phi^*,\psi^*)$ which is unique in $L^1(\mu)\times L^1(\nu)$ up to the transformation $(\phi^*+c,\psi^*-c)$ for $c\in \RR$. At optimality, we have $T_\lambda(\mu,\nu) = \int \phi^*\d\mu + \int \psi^*\d\nu$ and the primal-dual relation
$
\gamma(\d x,\d y) = e^{(\phi(x)+\psi(y)-c(x,y))/\lambda}\mu(\d x)\nu(\d y).
$
Moreover, the potentials satisfy for $\mu\otimes \nu$ almost every $(x,y)$, the optimality condition
\begin{align}\label{eq:schrodinger-system}
\left\{
\begin{aligned}
\phi^*(x) &= -\lambda \log \Big( \int e^{(\psi^*(y)-c(x,y))/\lambda}\d\nu(y)\Big)\\
\psi^*(y) &= -\lambda \log \Big( \int e^{(\phi^*(x)-c(x,y))/\lambda}\d\mu(x)\Big)
\end{aligned}
\right. 
.
\end{align}
These equations can be used to extend $\phi^*$ and $\psi^*$ as continuous functions (in fact of class $\Cc^p$ when $c\in \Cc^p$) over $\Xx$, which satisfy these equations everywhere~\cite{genevay2019sample}. 

\begin{definition}[Schr\"odinger potentials]\label{def:potentials}
The pair of functions $(\phi,\psi)\in \Cc^p(\Xx)\times \Cc^p(\Xx)$ which satisfies the \emph{Schr\"odinger system}\footnote{It would perhaps be less ambiguous to call these ``EOT'' system/potential, as their regularity properties rely on using $\mu\otimes \nu$ as a reference measure in EOT instead of $\d x \otimes \d y$ in the original Schr\"odinger system.} Eq.~\eqref{eq:schrodinger-system} for all $(x,y)\in \Xx^2$ is called the \emph{Schr\"odinger} potentials. This pair is unique up to the transformation $(\phi^*+c,\psi^*-c)$ for $c\in \RR$ (the choice of which does not matter in what follows).
\end{definition}


This particular choice of potentials among all those that satisfy Eq.~\eqref{eq:schrodinger-system} $\mu\otimes \nu$-almost everywhere is justified by the following result.
\begin{proposition}[First-variation of entropic optimal transport]\label{prop:propertiesEOT} Fix $\nu \in \Pp(\Xx)$ and for $\mu\in \Pp(\Xx)$ let $(\phi[\mu],\psi[\mu])$ be the Schr\"odinger potentials associated to the pair $(\mu,\nu)$. Then:
\begin{itemize}
\item[(i)] The map $\mu\mapsto \phi[\mu]$ (as well as the map $\mu\mapsto \psi[\mu]$) satisfies the following Lipschitz continuity property: there exists $L>0$ such that
\[
\Vert \phi[\mu] - \phi[\mu']\Vert_{\tilde \Cc^{p-1}(\Xx)} \leq L\, W_2(\mu,\mu'),\quad \forall \mu,\mu'\in \Pp(\Xx).
\]
where $\Vert \cdot \Vert_{\tilde \Cc^{p-1}(\Xx)}$ denotes the maximum supremum norm of all partial derivatives up to order ${p-1}$ quotiented by the invariance by addition of a constant (as in Def.~\ref{def:potentials}). 
\item[(ii)]  the function 
$\mu\mapsto T_\lambda(\mu,\nu)$ is convex, weakly continuous, and admits $\phi_\mu$ as first-variation, i.e.
\begin{align}
\forall \mu,\tilde \mu \in \Pp(\Xx),\; \lim_{\epsilon\, \downarrow\, 0}\frac{1}{\epsilon} \Big( T_\lambda((1-\epsilon)\mu+\epsilon \tilde \mu,\nu) - T_\lambda(\mu,\nu)\Big) = \int_\Xx \phi[\mu](x)\d (\tilde \mu-\mu)(x) .
\end{align}
\item[(iii)] For any $p'\in \{0,\dots,p\}$ there exists $C_{p'}>0$ independent of $\mu,\nu$ and $\lambda$ such that
\[
\Vert \phi[\mu]\Vert_{\tilde \Cc^{p'}} \leq C_{p'}\lambda^{\min\{0, 1-p'\}}.
\]

\end{itemize}
\end{proposition}
\begin{proof}
The first claim is technical and is proved in~\cite{carlier2022lipschitz} via the implicit function theorem on the Schr\"odinger system~\eqref{eq:schrodinger-system}. The convexity of $T_\lambda(\cdot,\nu)$ is clear by Eq.~\eqref{eq:dual-EOT} which expresses this function as a supremum of (weakly continuous) affine forms. To prove that $\phi[\mu]$ is the first-variation as in~\cite{feydy2019interpolating}, let $\mu_\epsilon \coloneqq (1-\epsilon)\mu+\epsilon\tilde \mu$. Using the fact that $\phi[\mu]$ (resp.~$\phi[\mu_\epsilon]$) is a subgradient of $T_\lambda(\cdot,\nu)$ at $\mu$ (resp. at $\mu_\epsilon$), we have
\begin{align*}
 \int \phi[\mu]\d(\tilde\mu-\mu)\leq \frac1\epsilon \Big(T_\lambda(\mu_\epsilon,\nu)-T_\lambda(\mu,\nu)\Big) \leq \int \phi[\mu_\epsilon]\d(\tilde \mu-\mu).
\end{align*}
The point \emph{(ii)} follows from the weak continuity of $\mu\mapsto \phi[\mu]$, a consequence of \emph{(i)}. Finally \emph{(iii)} is proved in~\cite{genevay2019sample} where it is obtained by differentiating $p'$ times Eq.~\eqref{eq:schrodinger-system}  and applying Fa\`a di Bruno's formula.
\end{proof}
Let us mention that the Lipschitz constant $L$ in \emph{(i)} may depend exponentially on the oscillation of $c/\lambda$, namely $(\sup c -\inf c)/\lambda$.


\subsection{Regularity of $(\lambda,\tau)$-barycenters}
We first gather useful regularity properties of $G_\lambda$, which are direct consequences of Prop.~\ref{prop:propertiesEOT}.
\begin{proposition}[Regularity of $G_\lambda$]\label{prop:Reg_G}
For any $(\nu_k)_{k=1}^K\in \Pp(\Xx)^K$, the function $G_\lambda \colon \mathcal P(\Xx) \to \RR_+$ defined in Eq.~\eqref{eq:G} is convex, weakly continuous, and for any $\mu \in \Pp(\Xx)$ it admits a first-variation
\begin{align}\label{eq:first-variation}
V[\mu] \coloneqq \sum_{k=1}^K w_k \phi_k[\mu] %= \E_\nu [\phi_{\mu,\nu}] .
\end{align}
where $\phi_k[\mu] \in \Cc^{p}(\Xx)$ is the Schr\"odinger potential from $\mu$ to $\nu_k$.
%with an optimal potential $\psi \in C(\RR^d)$.
The map $\mu\mapsto \nabla V[\mu]$ is Lipschitz continuous in the sense that there exists $L>0$ such that
\begin{align*}
\Vert V[\mu] - V[\mu']\Vert_{\tilde \Cc^{p-1}} \leq L\, W_2(\mu,\mu'), \qquad \forall \mu,\mu'\in \Pp(\Xx).
\end{align*}
We moreover have that for $p'\leq p$, there exists $C_{p'}>0$ independent of $\lambda>0$, $(w_k)_k$ and $(\nu_k)_k$ such that  $\Vert V[\mu] \Vert_{\tilde \Cc^{p'}}\leq C_{p'} \lambda^{\min\{0,1-p'\}}$.
\end{proposition}
Conveniently, the objective is also strongly convex.

\begin{proposition}[Strong convexity of $F_{\tau,\lambda}$]\label{prop:strong-convexity}
For $\lambda\geq 0$, the objective $F_{\lambda,\tau}=G_\lambda+\tau H$ is $\tau$-strongly convex on $\Pp(\Xx)$ for the total variation norm. 
\end{proposition}
\begin{proof}
It is well known that $\mu\mapsto H(\mu)$ is $1$-strongly convex over $\Pp(\Xx)$ for the total variation norm, see e.g.~\cite[Cor.~1]{yu2013strong}. Since $G_\lambda$ is convex, the result follows. 
\end{proof}
Strong convexity in $\ell^2$-norm on a discrete space $\Xx$ (which follows from Prop.~\ref{prop:strong-convexity} since then the total variation norm is the $\ell^1$ norm, and $\Vert \cdot \Vert_{\ell^2}\leq \Vert \cdot \Vert_{\ell^1}$) was already shown for the $(\lambda,\lambda)$-barycenter functional in~\cite[Thm.~3.4]{bigot2019data} with a technical proof specific to that case. The equivalent formulation as a doubly-regularized problem makes this property immediate.



As a consequence of all these regularity results, we now show that $(\lambda,\tau)$-barycenters can be written as a smooth Gibbs density.
\begin{assbox}
\begin{theorem}[Existence and regularity]\label{thm:characterization}
For any $\lambda,\tau > 0$, $F_{\lambda,\tau}$ admits a unique minimizer $\mu^*_{\lambda,\tau} \in \Pp(\Xx)$. It is an absolutely continuous measure with density
\begin{align}\label{eq:characterization}
\frac{\d \mu^*_{\lambda,\tau}}{\d x} \propto  e^{-V[\mu^*_{\lambda,\tau}]/\tau} 
\end{align}
where $V[\mu^*_{\lambda,\tau}]=\sum_{k=1}^K w_k \phi_k[\mu^*_{\lambda,\tau}] \in \Cc^{p}(\Xx)$ satisfies the regularity estimates of Prop.~\ref{prop:Reg_G}. 
\end{theorem}
\end{assbox}
\begin{proof}
The functional $G_\lambda$ is weakly continuous and $H$ is weakly lower-semicontinuous~\cite[Sec.~7.1.2]{santambrogio2015optimal} so $F_\lambda$ is weakly lower-semicontinuous. It is not identically $+\infty$ since the normalized Lebesgue measure on $\Xx$ is a feasible point. Since $\Pp(\Xx)$ is weakly compact, the direct method of the calculus of variations tells us that there exists at least one minimizer $\mu^*_{\lambda,\tau}$. Moreover $F_{\lambda,\tau}$ is strictly convex (Prop.~\ref{prop:strong-convexity}), so the minimizer is unique and since $H(\mu^*_\lambda)<\infty$ this measure is absolutely continuous. The first order optimality condition states that there exists $C\in \RR$ such that 
\begin{align}\label{eq:opt-gibbs}
\sum_{k=1}^K w_k\phi_k[\mu^*_{\lambda,\tau}](x)+\tau\log \Big(\frac{\d \mu^*_{\lambda,\tau}}{\d x}(x)\Big) =C, \quad \text{for $\mu^*_{\lambda,\tau}$-almost every $x$}.
\end{align}
It remains to observe that $\mu^*_{\lambda,\tau}$ must have full support on $\Xx$ (this is because of the infinite negative slope of $s\log(s)$ at $0$, see e.g.~\cite[Prop.~8.7]{santambrogio2015optimal} for details in a similar context) to obtain the formula for the minimizer.
\end{proof}
Interestingly, the minimizer of $F_{\lambda,\tau}$ for $\tau>0$ has always full support in $\Xx$ even when none of the marginals $\nu_1,\dots,\nu_K$ have. This property is not satisfied when $\tau=0$ or for the Sinkhorn Divergence barycenter (see Section \ref{sec:gaussians} for the case of point mass marginals). 

\paragraph{Relation to prior works} To the best of our knowledge, little is known on the regularity of the regularity of $(0,0)$-barycenters beyond absolute continuity under the condition that at least one $\nu_k$ is absolutely continuous~\cite{agueh2011barycenters},\cite[Thm.~5.1]{kim2017wasserstein}. The only other regularity result we are aware of concerns $(0,\tau)$-barycenters and the quadratic cost:~\cite{carlier2021entropic} show a Fisher Information bound on $\mu^*_{0,\tau}$ (their Lem.~4.1) that imply Lipschitz regularity in the compact case : this is consistent with the $\lambda\to 0$ limit of Thm.~\ref{thm:characterization}. They additionally show (their Prop.~5.2) that $\mu^*_{0,\tau}$  gains two degree of regularity compared to the densities $\nu_k$. With $\lambda,\tau>0$, we see that the $(\lambda,\tau)$-barycenters are as regular as the cost function, no matter the marginals.

\subsection{Dual formulations}
There are several ways to derive a dual formulation for~\eqref{eq:optim-objective}. Let us detail one of them which stands out as an elegant composition of two soft-max (log-sum-exp) functions -- which is the dual consequence of the double regularization. It can be used to compute $(\lambda,\tau)$-barycenters in practice (Section~\ref{sec:numerics}).
\newcommand{\bphi}{{\boldsymbol{\phi}}}
\newcommand{\bpsi}{{\boldsymbol{\psi}}}
\begin{proposition}\label{prop:dual}
For $\lambda,\tau>0$, one has the dual formulation
\begin{gather}\label{eq:dual}
\min_{\mu \in \Pp(\Xx)} F_{\lambda,\tau}(\mu)=\max_{\bpsi \in \Cc(\Xx)^K} E(\bpsi)\\
\intertext{where $E(\bpsi) $ is defined as}
\sum_{k=1}^K w_k \int_\Xx \psi_{k}\d\nu_k -\tau \log \left\{
\int_\Xx \exp 
\left[
\frac{\lambda}{\tau} \sum_{k=1}^Kw_k \log\int_\Xx \exp\Big( \frac{\psi_k(y)-c(x,y)}{\lambda}\Big)\d\nu_k(y) 
\right]\d x
\right\}.
\end{gather}
The function $E$ is concave, $1$-Lipschitz continuous and $\min\{\lambda,\tau\}^{-1}$-smooth for the seminorm $\bpsi \mapsto \sum w_k \Vert \psi_k\Vert_\mathrm{osc}$ where $\Vert \psi\Vert_\mathrm{osc} = \sup_y \psi(y)-\inf_y \psi(y)$. It admits a maximizer with semi-norm smaller than $\Vert c\Vert_{\mathrm{osc}}$ (which is unique up to shifting each $\psi_k$ by constants). 
Moreover, the barycenter $\mu^*_{\lambda,\tau}$ is a Gibbs distribution associated the solution of the dual problem (see~\eqref{eq:mu-from-psi}). 
\end{proposition}
\begin{proof}
We start from the dual formulation~\eqref{eq:dual-EOT} of $T_\lambda(\mu,\nu)$. Maximizing over $\phi$ gives $\phi=\phi_\psi$ $\nu$-a.e.\ with 
\begin{align}\label{eq:soft-c-transform}
\phi_\psi(x) = -\lambda \log \Big( \int e^{(\psi(y)-c(x,y))/\lambda}\d\nu(y)\Big)
\end{align}
(this is half of the Schr\"odinger system~\eqref{eq:schrodinger-system}). The so-called ``semi-dual'' formulation of EOT follows
\begin{align}\label{eq:semi-dual-EOT}
T_\lambda(\mu,\nu) = \max_{\psi\in \Cc(\Xx)} \int \psi\d\nu +\int \phi_\psi\d\nu.
\end{align}
We can thus rewrite the objective of the barycenter as
\begin{align*}
\min_{\mu\in \Pp(\Xx)} \max_{\bpsi \in \Cc(\Xx)^K} \sum_{k=1}^K w_k \int  \psi_k\d\nu_k + \int  \Big( \sum_{k=1}^K w_k \phi_{\psi_k}\Big)\d\mu +\tau H(\mu)
\end{align*}
where $\phi_{\psi_k}$ is defined as in~\eqref{eq:soft-c-transform} but with $\nu_k$ in place of $\nu$. By Sion's minimax theorem (using in particular that $\Pp(\Xx)$ is weakly compact and that the semi-dual objective~\eqref{eq:semi-dual-EOT} is continuous for the sup-norm), we can exchange the order of min and max. Maximizing in $\mu \in \Pp(\Xx)$ gives 
\begin{align}\label{eq:mu-from-psi}
\mu=\mu_\bpsi = \exp\left((\chi_\bpsi-V_\bpsi)/\tau\right)&&\text{with}&&V_\bpsi\coloneqq \sum_{k=1}^K w_k \phi_{\psi_k}&&\text{and}&& \quad\chi_\bpsi=-\tau \log \int e^{-V_\bpsi/\tau}\d x
\end{align}
and the objective becomes
$
\max_{(\psi_k)_k\in \Cc(\Xx)^K} \sum_{k=1}^K w_k \int  \psi_k\d\nu_k + \chi_\bpsi.
$


 The other claims follow mostly from the general properties of the log-sum-exp operator $f \mapsto \mathrm{LSE}(f): x \mapsto \alpha \log \int e^{f(x,y)/\alpha}\d\nu(y)$ for $\alpha>0$ and $\nu$ a nonnegative measure, which is such that :
\begin{itemize}
\item if $f(\cdot,y)$ is convex for $\nu$-a.e $y$ then $\mathrm{LSE}(f)$ is convex in $x$~\cite[Ex.~3.14]{boyd2004convex};
\item $\mathrm{LSE}$ is $1$-Lipschitz continuous and  $1/\alpha$-smooth for the sup-norm, which can be seen from the expression of the first and second order differential:
\begin{align*}
D\mathrm{LSE}[f](\delta f) = x\mapsto \E_{Y_x}[ \delta f(x,Y_x)] && D^2\mathrm{LSE}[f](\delta f,\delta f) = x \mapsto \frac{1}{\alpha}\mathbf{Var}_{Y_x}[\delta f(x,Y_x)]
\end{align*}
where $\E_{Y_x}$ (resp.~$ \mathbf{Var}_{Y_x}$) denote the expectation (resp.~centered variance) under $Y_x$ distributed according to the probability measure proportional to $e^{f(x,y)/\alpha}\nu(\d y)$.
\end{itemize}
Let us apply these properties in the context of $E$. The second term of~\eqref{eq:dual} is concave (as minus the composition of: an affine function, by a LSE, by a sum with nonnegative weights, by a LSE). It is also $1$-Lipschitz continuous, as a composition of $1$-Lipschitz continuous functions and its differential is given by
\begin{align*}
DE[\bpsi](\delta \psi_k) = w_k\int \delta \psi_k\d\nu_k - w_k\int \delta \psi_k(y)e^{(\phi_{\psi_k}(x)+\psi_k(y)-c(x,y))/\lambda +(\chi_\bpsi(x)-V_\bpsi(x))/\tau}\d x \d\nu_k(y)
\end{align*}
Note that the second integral is the integral of $\delta \psi_k$ against a probability measure $\gamma_{\psi_k}(\d x,\d y) \in \Pi(\mu_\bpsi,\nu_k)$ which can be disintegrated as $\gamma_{\psi_k}(\d y|x)\mu_\bpsi(\d x)$. Differentiating once more, we obtain
\begin{align*}
D^2E[\bpsi](\delta \psi,\delta \psi) =-\frac{1}{\tau} \mathbf{Var}_{x\sim \mu_\bpsi}\Big[\sum_{k=1}^K w_k \E_{\gamma_{\psi_k}(\cdot |x)} [\delta \psi_k]\Big] -\frac{1}{\lambda}\E_{x\sim \mu_\bpsi}\Big[\sum_{k=1}^K w_k \mathbf{Var}_{\gamma_{\psi_k}(\cdot|x)}(\delta \psi_k)\Big].
\end{align*}
It follows by the law of total variance that 
$$
-\frac{1}{ \max\{\tau,\lambda\}} \mathbf{Var}\Big[  \sum_{k=1}^K w_k\delta \psi_k(y_k) \Big] \geq D^2E[\bpsi](\delta \psi,\delta \psi) \geq -\frac{1}{ \min\{\tau,\lambda\}} \mathbf{Var}\Big[  \sum_{k=1}^K w_k\delta \psi_k(y_k) \Big] 
$$
where the variance is under $(x,y_1,\dots,y_k)$ such that $(x,y_k)\sim \gamma_{\psi_k}$. This shows that $E$ is $\frac{1}{ \min\{\tau,\lambda\}}$-smooth for the norm $\bpsi\mapsto \sum w_k \Vert \psi_k\Vert_\mathrm{osc}$ and the uniqueness of the dual solution up to constant shifts. The existence of a maximizer can be obtained by taking the Schr\"odinger potentials associated to the minimizers~\eqref{eq:opt-gibbs} which satisfy the optimality conditions and the upper-bound on its semi-norm follows from the Schr\"odinger system~\eqref{eq:schrodinger-system}.
\end{proof}

\paragraph{Dual formulation for $\lambda=\tau$} For the sake of completeness, let us mention that when $\tau = \lambda$, another useful dual formulation becomes available. Indeed, the problem then writes
$$
\min_{\mu \in \Pp(\Xx)} \max_{\bphi,\bpsi} \sum_k w_k \int \varphi_k\d\mu +\int \psi_k \d\nu_k +\lambda \Big( 1-\int e^{(\phi_k(x)+\psi_k(y)-c(x,y))/\lambda}\d x \d\nu_k(y) \Big)
$$
where we have used Lemma~\ref{lem:change-ref} below to change the reference measure from $\mu\otimes \nu_k$ to $(\d x)\otimes \nu_k$. Exchanging min/max and minimizing over $\mu\in \Pp(\Xx)$ and one gets the dual problem~\cite[Prop.~9.1]{peyre2019computational}
\begin{align*}
\max_{\bphi,\bpsi} &\quad \sum_{k=1}^K w_k  \int \psi_k\d\nu_k + \lambda \sum_{k}^K w_k \Big( 1-\int e^{(\phi_k(x)+\psi_k(y)-c(x,y))/\lambda}\d x \d \nu_k(y)  \Big)\\ \text{subject to} &\quad \sum_{k=1}^K w_k \phi_k=0
\end{align*}
with, at optimality, $\mu^*_{\lambda,\lambda}(\d x) = \int e^{(\phi_k(x)+\psi_k(y)-c(x,y))/\lambda} \d \nu_k(y)\d x$ (valid for $k\in [K]$). Alternate maximization on the blocks $\bphi$ and $\bpsi$ leads to a convenient Sinkhorn-like algorithm when the Lebesgue measure is discretized (see~\cite{kroshnin2019complexity} for a complexity analysis).

\iffalse
\paragraph{Other dual formulations} For the sake of completeness, let us mention that when $\tau \geq \lambda$, other useful dual formulations become available. Indeed, the problem writes
$$
\min_{\mu \in \Pp(\Xx)} \max_{(\phi_k),(\psi_k)} \sum_k w_k \int \varphi_k\d\mu +\int \psi_k \d\nu_k +\lambda \Big( 1-\int e^{(\phi_k(x)+\psi_k(y)-c(x,y))/\lambda}\d x \d\nu_k(y) \Big) +(\tau-\lambda) H(\mu)
$$
where we have used Lemma~\ref{lem:change-ref} below to change the reference measure from $\mu\otimes \nu_k$ to $(\d x)\otimes \nu_k$. Dual formulations follow by exchanging min/max and minimizing over $\mu\in \Pp(\Xx)$:
\begin{itemize}
\item For $\lambda=\tau$ the final entropy term vanishes and one has~\cite[Prop.~9.1]{peyre2019computational}
\begin{align*}
\max_{(\phi_k),(\psi_k)} &\quad \sum_{k=1}^K w_k  \int \psi_k\d\nu_k + \lambda \sum_{k}^K w_k \Big( 1-\int e^{(\phi_k(x)+\psi_k(y)-c(x,y))/\lambda}\d x \d \nu_k(y)  \Big)\\ \text{subject to} &\quad \sum_{k=1}^K w_k \phi_k=0
\end{align*}
with, at optimality, $\mu^*_{\tau,\lambda}(\d x) = \int e^{(\phi_k(x)+\psi_k(y)-c(x,y))/\lambda} \d \nu_k(y)\d x$ (valid for $k\in [K]$). The interest of this formulation is that alternate maximization on the blocks $(\phi_k)_k$ and $(\psi_k)_k$ leads to a practical Sinkhorn-like algorithm, when the Lebesgue measure is discretized (see~\cite{kroshnin2019complexity} for a complexity analysis). In addition, this formulation as maximizing an expectation under linear constraints lends itself naturally to stochastic optimization algorithms (in the style of~\cite{genevay2016stochastic} where the case of barycenters is however not considered).
\item 
For $\tau>\lambda$ the final entropy term remains and, using the fact that for $\alpha>0$, $\sup_{\mu \in \Pp(\Xx)} \int u\d\mu - \alpha H(\mu) = \alpha \log \int e^{u(x)/\alpha }\d x$, one obtains~\cite{ballu2020stochastic}
\begin{multline*}
\max_{\phi_k,\psi_k} \quad \sum_{k=1}^K w_k  \int \psi_k\d\nu_k + \lambda \sum_{k}^K w_k \Big( 1-\int e^{(\phi_k(x)+\psi_k(y)-c(x,y))/\lambda}\d x \d \nu_k(y)  \Big)\\ 
-(\tau-\lambda)\log \int e^{-(\sum_k w_k \phi_k(x))/(\tau-\lambda)}\d x
\end{multline*}
\end{itemize}
\fi
 
\subsection{Extensions}
Let us conclude this section with a discussion of our setting and potential extensions. 
\begin{itemize}
\item (General ambiant space) The definition of $(\lambda,\tau)$-barycenters would make sense in the more general context where $\Xx$ is a Polish space with a reference measure $\mu_{\mathrm{ref}}\in \Pp(\Xx)$ (replacing the Lebesgue measure). In particular, the compactness assumption is not necessary, provided that the cost satisfies certain integrability conditions (see~\cite{nutz2021introduction} for a review of EOT under weak assumptions). In this paper, we focus on the compact case on $\RR^d$ for simplicity and because, to date, Prop.~\ref{prop:propertiesEOT}-(i) and Prop.~\ref{prop:expansion}-(i) which we use below are only known in this setting.
\item (Infinite number of marginals) The problem of Wasserstein barycenter is often formulated~\cite{agueh2017vers} in the more general form where the EOT barycenter functional is an expectation under some distribution $P\in \Pp(\Pp(\Xx))$ instead of a finite sum, i.e.~
\begin{align}\label{eq:p-of-p}
G_\lambda(\mu) = \int_{\Pp(\Xx)} T_\lambda(\mu,\nu)\d P(\nu).
\end{align}
The $(\lambda,\tau)$-barycenters could also be studied in this setting, where interesting questions arise related to estimation rates and stability.
\end{itemize}

 
\section{Approximating the Wasserstein barycenter}\label{sec:approximation}
In this section, we study the difference between $(\lambda,\tau)$-barycenters and the $(0,0)$-barycenter; showing in passing a result for the Sinkhorn divergence barycenter. Our goal is to show that the double regularization is not just a convenient trick to get nice properties: the resulting object in fact preserves the geometric naturality of unregularized barycenters when $\tau=\lambda/2$ (at least better so than when $\tau=0$, $\lambda=0$ or $\tau=\lambda$). We mention however that we do not specially advocate choosing a small $\lambda$ in practice, as the other desirable properties of $(\lambda,\nicefrac{\lambda}2)$-barycenters degrade very quickly as $\lambda$ decreases (see~\cite{chizat2020faster} for an analysis of the trade-offs in choosing $\lambda$ in a similar context).

\subsection{The ``debiasing effect'' for smooth densities}
In this section we discuss the case of the quadratic cost and smooth marginals. At the heart of our approximation result is the following known comparison between $T_\lambda$ and $T_0$.

\begin{proposition}\label{prop:expansion}
Assume that $\mu$ and $\nu$ have bounded densities with compact support and let $c(x,y)=\frac12 \Vert y-x\Vert_2^2$. Then
\begin{align}\label{eq:entropic-expansion-bound}
T_0(\mu,\nu)\leq T_\lambda(\mu,\nu) + \frac{d\lambda}{2}  \log(2\pi\lambda) + \frac{\lambda}{2} (H(\mu)+H(\nu)) \leq   T_0(\mu,\nu) + \frac{\lambda^2}{8}I(\mu,\nu) 
\end{align}
where $I(\mu,\nu)$ is the integrated Fisher information of the Wasserstein geodesic $(\rho_t\d x)_{t\in [0,1]}$ that connects $\mu$ to $\nu$, i.e. $I(\mu,\nu)\coloneqq \int_0^1\int_\Xx \Vert \nabla \log \rho_t(x)\Vert^2\rho_t(x)\d x\d t\geq 0$. Moreover, if $I(\mu,\nu)<\infty$ then
\begin{align}\label{eq:entropic-expansion}
 T_\lambda(\mu,\nu) + \frac{d\lambda}{2}  \log(2\pi\lambda) + \frac{\lambda}{2} (H(\mu)+H(\nu)) =   T_0(\mu,\nu) + \frac{\lambda^2}{8}I(\mu,\nu)  +o(\lambda^2).
\end{align}
\end{proposition}
The logarithmic derivative $\nabla \log \rho_t$ appearing in the statement is the density of the distributional gradient $\nabla \rho_t$ with respect to $\rho_t$ when it exists, and $I_0(\mu,\nu)=+\infty$ if this quantity is not defined for a.e. $t\in [0,1]$. As shown in~\cite[Thm.~1]{chizat2020faster}, the first claim is a direct consequence of a dynamical formulation of $T_\lambda$~\cite{chen2016relation}. The second claim was proved in~\cite[Lem.~1]{chizat2020faster} and then in~\cite{conforti2021formula} (who first formulated the ansatz) in a more general setting. It is a more precise version of previous first-order expansions~\cite{duong2013wasserstein, erbar2015large, pal2019difference}. Let us also mention that~\cite[Prop.~1]{chizat2020faster} gives a priori bounds on $I(\mu,\nu)$ in terms of the derivatives up to order $3$ of Kantorovich potentials. 

Formula~\eqref{eq:entropic-expansion} shows that the choice $\tau=\lambda/2$ stands out, as this cancels exactly the first order (non-constant) error term between $F_{\lambda,\tau}$ and $F_{0,0}=G_0$. An approximation bound in terms of suboptimality gap for the Wasserstein barycenter functional easily follows.
\begin{assbox}
\begin{theorem}[Approximation bound]\label{thm:approx}
Assume that $\nu_1,\dots,\nu_K$ have bounded densities and let $c(x,y)=\frac12\Vert y-x\Vert_2^2$. Then for $\lambda>0$ the $(\lambda,\nicefrac{\lambda}2)$-barycenter satisfies
\begin{align}
G_0(\mu^*_{\lambda,\lambda/2})-G_0(\mu^*_{0,0})\leq \frac{\lambda^2 }{8} \sum_{k=1}^K w_k I(\mu^*_{0,0},\nu_k)
\end{align}
where $I$ is defined in Prop.~\ref{prop:expansion}. For the Sinkhorn divergence barycenter, it holds:
$$
G_0(\mu^{*}_{\lambda,\mathrm{div}})-G_0(\mu^*_{0,0})\leq \frac{\lambda^2}{8}\sum_{k=1}^K w_k \Big(I(\mu^*_{0,0},\nu_k) +\frac12 I(\mu^*_{\lambda,\mathrm{div}})+\frac12 I(\nu_k)\Big)
$$
where $I(\mu)\coloneqq I(\mu,\mu)$ denotes the Fisher information of $\mu$.
\end{theorem}
\end{assbox}
\begin{proof}
First notice that Prop.~\ref{prop:expansion} indeed applies for any couple of the form $(\mu^*_{\lambda,\lambda/2},\nu_k)$: the bounded density assumption holds by Thm.~\ref{thm:characterization} for $\lambda,\tau>0$ and by~\cite[Thm.~5.1]{agueh2011barycenters} for $\lambda=0$ since the Wasserstein barycenter has bounded density.
Let us call $\tilde T_\lambda$ the quantity that is sandwiched in Eq.~\eqref{eq:entropic-expansion-bound} and let us define $\tilde F_\lambda(\mu) \coloneqq \sum_{k=1}^K w_k\tilde T_\lambda(\mu,\nu_k)$ which differs from $F_{\lambda,\lambda/2}$ only by a constant. Since $\mu^*_{\lambda,\lambda/2}$ is the minimizer of $F_{\lambda,\lambda/2}$, it is also the minimizer of $\tilde F_\lambda$, so for any $\lambda\geq 0$, $\tilde F_\lambda(\mu^*_{\lambda,\lambda/2})\leq \tilde F_\lambda(\mu^*_{0,0})$.
%
By Prop.~\ref{prop:expansion}, it holds for any admissible $\mu\in \Pp_2(\Xx)$ that
 $$ 0\leq \tilde T_\lambda(\mu,\nu_k) - T_0(\mu,\nu_k) \leq \frac{\lambda^2}{8}I(\mu,\nu_k).$$ 
Taking the weighted sum over $k\in\{1,\dots, K\}$, we get $0\leq  \tilde F_\lambda(\mu)-G_0(\mu) \leq \frac{\lambda^2}{8}\sum_{k=1}^K w_k I(\mu,\nu_k)$. It follows
\begin{multline*}
G_0(\mu^*_{\lambda,\lambda/2}) - G_0(\mu^*_{0,0})\\
\leq [G_0(\mu^*_{\lambda,\lambda/2}) - \tilde F_\lambda(\mu^*_{\lambda,\lambda/2})]
 + [ \tilde F_\lambda(\mu^*_{\lambda,\lambda/2}) -  \tilde F_\lambda(\mu^*_{0,0})]
 +[\tilde F_\lambda(\mu^*_{0,0}) - G_0(\mu^*_{0,0})]\\
\leq 0 + 0 + \frac{\lambda^2}{8}\sum_{k=1}^K w_k I(\mu^*_{0,0},\nu_k).
\end{multline*}

As for the Sinkhorn divergence barycenter, it is the minimizer of $G^\mathrm{div}_\lambda  \coloneqq \sum_{k=1}^K w_kS_\lambda(\cdot,\nu_k)$ where
$
S_{\lambda}(\mu,\nu) \coloneqq T_\lambda(\mu,\nu) - \frac12 T_\lambda(\mu,\mu)-\frac12 T_\lambda(\nu,\nu)
$
is the Sinkhorn divergence. After manipulating inequalities~\eqref{eq:entropic-expansion-bound}, we obtain for admissible $\mu,\nu$ that
$$
T_0(\mu,\nu) -\frac{\lambda^2}{16}(I(\mu)+I(\nu)) \leq S_{\lambda}(\mu,\nu) \leq T_0(\mu,\nu) +\frac{\lambda^2}{8}I(\mu,\nu).
$$
As before, we sum these inequalities over $k\in \{1,\dots,K\}$ and get
\begin{align*}
G_0(\mu^*_{\lambda,\mathrm{div}}) - G_0(\mu^*_{0,0})&\leq [G_0(\mu^*_{\lambda,\mathrm{div}}) - G^\mathrm{div}_{\lambda}(\mu^*_{\lambda,\mathrm{div}})]
 + [ G^\mathrm{div}_{\lambda}(\mu^*_{\lambda,\mathrm{div}}) -  G^\mathrm{div}_{\lambda}(\mu^*_{0,0})]\\
 &\qquad\qquad\qquad\qquad\qquad\qquad\;\;
 + [G^\mathrm{div}_{\lambda}(\mu^*_{0,0}) - G_0(\mu^*_{0,0})]\\
&\leq \frac{\lambda^2}{16}I(\mu^*_{\lambda,\mathrm{div}}) +\frac{\lambda^2}{16}\sum_{k=1}^K w_k I(\nu_k) + 0 + \frac{\lambda^2}{8}\sum_{k=1}^K w_k I(\mu^*_{0,0},\nu_k).\qedhere
\end{align*}

\end{proof}
Note that for other barycenters, the suboptimality bound is of the order $\max\{\lambda,\tau\} H(\mu)$, which shows that the $(\lambda,\nicefrac{\lambda}{2})$-barycenters and the Sinkhorn divergence barycenters stand out as particularly good approximations of the unregularized barycenter, of order $\lambda^2$. Regarding the generality of this result, we can also make the following comments:
\begin{itemize}
\item the choice $\tau=\lambda/2$ leads to approximation benefits  for all costs of the form $(\alpha/2)\Vert y-x\Vert_2^2$ for $\alpha>0$. Indeed, insisting on the dependency in $c$ in the notation, we can always bring ourselves back to the case $\alpha=1$ using $
F_{\alpha c,\alpha \lambda,\alpha \tau} = \alpha F_{c,\lambda,\tau}.
$
\item In contrast, for costs with a non-constant Hessian, there is no natural way to debias the problem since in those cases, as shown in~\cite{pal2019difference}, the first order term in the expansion~\eqref{eq:entropic-expansion} depends on the optimal transport map itself, which is not known a priori.
\item Note that these bounds involve quantities related to the regularity of the unregularized barycenter for which no a priori bound exist unfortunately.
\end{itemize}


\paragraph{Approximation in $W_2$ distance} While approximation bounds in terms of suboptimality as in Thm.~\ref{thm:approx} are natural for objects defined by variational problems, one could wish to state ``true'' approximation bounds in terms of a natural notion of distance. There is an active line of work on the stability of Wasserstein barycenters (see~\cite{carlier2022quantitative} for recent advances). Let us give a result suggesting that in the most favorable settings our bound gives a direct upper bound on the squared $L_2$-Wasserstein distance from the barycenter $\mu^*_{0,0}$. The following proposition can be found in~\cite[Thm.~6]{chewi2020gradient}, following earlier results in~\cite{ahidar2020convergence}.


\begin{proposition}
Assume that for each $k\in [K]$, there exists a Brenier potential $\tilde \phi_k = \frac12\Vert x\Vert^2 - \phi_k$ from $\mu_{0,0}^*$ to $\nu_k$ that is $\alpha_k>0$ strongly convex over $\Xx$. Then the following \emph{variance inequality} holds
$$
\frac12 W_2^2(\mu,\mu^*_{0,0}) \leq \frac{G_0(\mu) - G_0(\mu^*_{0,0})}{\sum_{k=1}^K w_k \alpha_k}, \quad \forall \mu \in \Pp(\Xx).
$$
\end{proposition}
As shown in~\cite{chewi2020gradient}, this proposition holds in the case of the barycenter between Gaussian measures. In this case if for each $\nu_k=\Nn(a_k,A_k)$ with $a_k\in \RR^d$ and $A_k \in \Ss^s_{++}$ it holds $\Vert A_k\Vert_{\mathrm{op}}\leq 1$ and $\det S_k\geq \zeta$ then the upper-bound is $\zeta^{-1}(G_0(\mu)-G_0(\mu^*_{0,0}))$. Unfortunately, the Gaussian case is not covered by Thm.~\ref{thm:approx} due to the compactness assumption, but inspecting their proof, it can be seen that the result in fact applies to barycenters of \emph{elliptically-contoured distributions} in the same family, including smooth compact cases with finite Fisher information, hence covered by Thm.~\ref{thm:approx}. See~\cite[Prop.~14]{chizat2020faster} for an explicit example of such a class of distributions.


\subsection{Closed form for isotropic Gaussians}\label{sec:gaussians}
In this section, we leverage closed form expressions of entropic OT for Gaussian measures~\cite{chen2016relation, mallasto2022entropy, del2020statistical,janati2020entropic} to get a finer understanding of the role of $\lambda$ and $\tau$ in approximation. In order to get closed-form solutions, we focus on the simplest case of the barycenter between a family of isotropic Gaussians with equal variance. Our computations, detailed in Appendix~\ref{app:gaussians}, follow those of~\cite{janati2020debiased}  and extend them by introducing a general parameter $\tau$.

\begin{proposition}\label{prop:gaussians}
For $k\in \{1,\dots,K\}$, let $\nu_k = \Nn(x_k,aI_d)$, for $x_k\in \RR^d$ and $a>0$. Then the $(\lambda,\tau)$-barycenter $\mu^*_{\lambda,\tau}$ is the Gaussian $\Nn(\bar x,bI_d)$ where $\bar x=\sum_{k=1}^K w_kx_k$ and the variance is
\begin{align}\label{eq:gaussian-variance}
b = \frac{\big(a+\sqrt{(a-\lambda)^2+4a\tau}\big)^2-\lambda^2}{4a}.
\end{align}
In particular, one has $b=a$ (i.e. exact debiasing) with the choice
\begin{align}\label{eq:nonasymp-debias}
\tau= \tau^*(\lambda) &= \frac{\lambda}{2} + a\Big(1-\Big(1+\frac{\lambda^2}{4a^2}\Big)^{1/2}\Big) =\frac{\lambda}{2} -\frac{\lambda^2}{8a} +O(\lambda^4/a^3).
\end{align}
\end{proposition}
We reports various special or limit cases of~\eqref{eq:gaussian-variance} in Table~\ref{tab:gaussian}, as well as the case of Sinkhorn divergence barycenters. The latter does not follow from~\eqref{eq:gaussian-variance} and is taken from~\cite{janati2020debiased}, which also covered the special cases $\tau=0$ and $\tau=\lambda$. 

\begin{figure}
\centering
\includegraphics[scale=0.6]{optimal-debiasing}
\caption{Wasserstein distance $d^{-1}W_2(\mu^*_{\lambda,\tau},\mu^*_{0,0})=(\sqrt{b}-\sqrt{a})^2$ between $\mu^*_{\lambda,\tau}$ and $\mu^*_{0,0}$  from~\eqref{eq:gaussian-variance}, with $a=1.0$. The white line shows the best debiasing choice of $\tau^*(\lambda)$ from~\eqref{eq:nonasymp-debias} for which this distance is $0$. Below this line the barycenter is shrinked and above it is blurred. Dashed lines show the small and large $\lambda$ asymptotics of $\tau^*(\lambda)$.}
\end{figure}

\paragraph{Non-asymptotic debiasing} As can be seen from~\eqref{eq:nonasymp-debias}, the choice $\tau=\lambda/2$ gives the optimal debiasing only asymptotically as $\lambda\to 0$. For larger values of $\lambda$, this formula suggests to use a value for $\tau$ that is smaller than $\lambda/2$; and in any case smaller than $1$. Remark that $\tau^*$ is concave and $\tau^*(\lambda)\leq \min \{\lambda/2,1\}$ (which are its tangents at $0$ and $\infty$) and that for $\lambda$ large it holds $\tau^*(\lambda) = 1-a/\lambda +O((a/\lambda)^3)$. In practice, one may use the outer regularization value $\tau^*(\lambda)$  as a heuristic even for non-gaussian measures, replacing $a$ by a notion of average of the variances of the marginals $\nu_k$.


\begin{table}
\centering
\begin{tabular}{l|l|l|c}
& Objective  & Variance (Isotropic Gaussian case) \\
\hline
$\mu_{0}^*$  &  $G_0$ &  $a$ \\
$\mu_{\lambda,0}^*$  &   $G_\lambda$ &  $\max\{a - \lambda, 0 \}$ \\
$\mu_{0,\tau}^*$ &  $G_0 +\tau H$ &   $a+2\tau-\tau^2/a+O(\tau^3/a^2)$\\
$\mu_{\lambda,\mathrm{div}}^*$ &  $G_\lambda -\frac12 T_\lambda(\cdot,\cdot)$  &   $a$\\
$\mu_{\lambda,\tau}^*$ &  $G_\lambda +\tau H$ &   $a+2\tau-\lambda+\tau(\lambda-\tau)/a +O((\lambda^3+\tau^3)/a^2)$\\
$\mu_{\lambda,\lambda/2}^*$ &  $G_\lambda +\frac{\lambda}{2} H$ &   $a+\lambda^2/(4a) +O(\lambda^3/a^2)$\\
 \hline

\end{tabular}
\caption{Variance of $(\lambda,\tau)$-barycenters when the $\nu_k$ are  Gaussians with variance $aI_d$ (Eq.~\eqref{eq:gaussian-variance}).
}\label{tab:gaussian}
\end{table}

\paragraph{Barycenter of Dirac masses} Observe that for $a=0$, Eq.~\eqref{eq:gaussian-variance} gives $b=\tau$ (independently of $\lambda$). This can in fact be directly seen from the optimality conditions. Indeed, the Schr\"odinger system~\eqref{eq:schrodinger-system} gives that for each $k$,
$
\phi_{k}[{\mu^*_{\lambda,\tau}}](x) = \frac12 \Vert x-x_k\Vert^2 -\psi_k
$
 for some $\psi_k\in \RR$. Then the optimality condition~\eqref{eq:characterization} gives
 \begin{align*}
 \mu^*\propto e^{-\frac{\sum_k w_k \phi_{k}(x)}{\tau}} \propto e^{-\frac{\sum_k w_k \Vert x-x_k\Vert^2_2}{2\tau}}\propto e^{-\Vert x- \bar x\Vert^2/(2\tau)}.
 \end{align*}
The inner regularization $\lambda$ has no effect in this case because there is only one transport plan between any $\mu$ and each $\nu_k$ (in particular $\mu_{\lambda,0}^*=\mu^*_{0,0}$ for any $\lambda>0$).

  
 \subsection{Choice of the reference measure}\label{sec:reference-measure}
 As briefly mentioned in Section~\ref{sec:intro-douba}, there is an alternative formulation of $(\lambda,\tau)$-barycenters in terms of a change of reference measure in the definition of EOT. Let us develop this correspondance here. For a reference measure $\sigma \in \Mm_+(\Xx\times \Xx)$, let 
 \begin{align}\label{eq:EOT-ref}
T_\lambda(\mu,\nu|\sigma)& \coloneqq \min_{\gamma \in \Pi(\mu,\nu)} \int_{\Xx^2} c(x,y) \d\gamma(x,y) + \lambda H(\gamma | \sigma),\\
G_\lambda(\mu|\sigma_\alpha)&\coloneqq \sum_{k=1}^K T_{\lambda}(\mu,\nu_k|\mu^\alpha\otimes \nu_k).
\end{align}
In the following discussion, $\mu^\alpha$ denotes $\mu$ itself when $\alpha=1$, the Lebesgue measure $\d x$ when $\alpha=0$ and $(\frac{\d \mu}{\d x})^\alpha$ otherwise. Note that one could equally choose $\sigma_\alpha=\mu^\alpha\otimes \tilde \nu_k$ for any $\tilde \nu_k$ such that $H(\nu_k|\tilde \nu_k)$ is finite without changing the barycenter.

\begin{lemma}\label{lem:change-ref}
It holds
\begin{align*}
G_\lambda(\mu|\sigma_\alpha) = G_\lambda(\mu)+\lambda(1-\alpha)H(\mu)
\end{align*}
and for $\tau=\lambda(1-\alpha)$ the minimizer of this functional is $\mu^*_{\lambda,\tau}$.
\end{lemma}
\begin{proof}
This is a consequence of the following property that can be found by direct computations (see~\cite[Lem.~1.6]{marino2020optimal} for details): for any $\gamma\in \Pi(\mu,\nu)$ and $\alpha\in \RR$, it holds
\begin{gather*}
H(\gamma|\mu^\alpha\otimes \nu_k) = H(\gamma|\mu\otimes \nu_k) +(1-\alpha)H(\mu).
\qedhere
\end{gather*}
\end{proof}

This lemma is sufficient to justify the discussion after Def.~\ref{eq:optim-objective}. The effect of various choices of reference measure can be interpreted in view of the local expansion of Eq.~\eqref{eq:entropic-expansion} and are summarized in Table~\ref{table:reference-measures} (the two first rows correspond to cases discussed in~\cite{janati2020debiased} for Gaussian measures). In the table, we used reference measures which are symmetric in $\mu,\nu$ as is standard, to fix ideas.

To conclude this section, let us insist on the fact that it is much more convenient to use the expression $F_{\lambda,\tau}=G_\lambda +\tau H$ as a sum of a smooth, convex functional and an entropy rather than $G_\lambda(\cdot|\sigma_\alpha)$, although they are equivalent for $\tau=\lambda(1-\alpha)$. We rely on this decomposition in the next two sections.

\begin{table}
\centering
\begin{tabular}{c|l|l|l}
Reference measure $\sigma$ & Corresponding $\tau$ & $1$st order bias term & Effect on barycenter \\
\hline
$\d x \otimes \d x$ &$ \tau=\lambda$ & $\frac{\lambda}{2}H(\mu)$ & Blurred\\
$ \mu \otimes \nu$ & $\tau=0 $ & $- \frac{\lambda}{2} H(\mu) $ & Shrinked \\
$ \sqrt{\mu} \otimes \sqrt{\nu}$ &$\tau=\lambda/2$& $0$ & Debiased \\
$\mu^\alpha\otimes \nu^\alpha$  &$ \tau=\lambda(1-\alpha)$& $\lambda(\frac12-\alpha)$ & -- \\
\hline
\end{tabular}
\caption{Effect of the choice of the reference measure and interpretation as a Dou-Ba. The bias refers to the error $G_\lambda(\mu|\sigma_\alpha)-G_0(\mu)$ when $c(x,y)=\frac12\Vert y-x\Vert^2_2$ (up to constant terms independent from $\mu$). In the last row, one needs $\alpha\leq 1$ so that $\tau\geq 0$.}\label{table:reference-measures}
\end{table}



\section{Stability and statistical estimation}\label{sec:statistics}

\subsection{General stability result}\label{sec:general-stability}
The goal of this subsection is to prove the following general stability result, where the strongest point is (ii). In that statement, $W_1$ denotes the $L^1$-Wasserstein distance and $\dot H^{q}$ denotes the homogeneous Sobolev norm of order $q$.
\begin{assbox}
\begin{theorem}\label{thm:stability}
Let $(\nu_k)_{k=1}^K,(\hat \nu_k)_{k=1}^K \in \Pp(\Xx)^K$ and let $\hat \mu,\mu^*$ be their respective $(\lambda,\tau)$-barycenters with same weights $(w_k)_{k=1}^K$.
\begin{itemize}
\item[(i)] Assume that $c(x,\cdot)$ is $L$-Lipschitz for all $x\in \Xx$. Then for $\tau>0$ and $\lambda\geq 0$, it holds
$$
H(\hat \mu|\mu^*) \leq \frac{2L}{\tau}\sum_{k=1}^K w_k W_1(\nu_k,\hat \nu_k).
$$
\item[(ii)] Assume that $c\in \Cc^p(\Xx\times \Xx)$ for $p\geq 2$ and $\lambda>0$. Then there exists $C$  that depends on $c$ and $\Xx$ only such that
$$
H(\hat \mu|\mu^*) \leq \frac{C(1+\lambda^{1-p})}{\tau}\sum_{k=1}^K w_k \Vert \nu_k - \hat \nu_k\Vert_{\dot H^{-p}}.
$$
\end{itemize}
\end{theorem}
\end{assbox}
Note that the first bound, adapted from~\cite[Thm.~3.3]{bigot2019penalization} does not require $\lambda>0$ and only exploits the first-order regularity of the cost while with $\lambda>0$, the barycenter is able to exploit the regularity of the cost to obtain stability under norms even weaker than $W_1$. Another stability result in the literature is~\cite[Thm.~1]{theveneau2022stability} which proves $\ell_2$-stability of $(\lambda,\lambda)$-barycenters in a discrete setting, under $\ell_\infty$ perturbations of the cost matrix (with an exponential dependence in $\lambda^{-1}$).

Before we start the proof, let us state a lemma that only uses convexity of $G_\lambda$; the lower-bound is classical and the upper-bound is used later in Section~\ref{sec:dynamics}.
\begin{lemma}[Entropy sandwich] Let $\mu\in \Pp(\Xx)$ and consider the ``tangent Gibbs distribution'' $\nu \propto \exp(-V[\mu]/\tau)$ where $V$ is the first-variation~\eqref{eq:first-variation} of $G_\lambda$. It holds
\begin{align*}
\tau H(\mu|\mu_{\lambda,\tau}) \leq F_{\lambda,\tau}(\mu)- F_{\lambda,\tau}(\mu^*_{\lambda,\tau})\leq \tau H(\mu|\nu).
\end{align*}
\end{lemma}
\begin{proof}
For brevity, let us write $F=F_{\lambda,\tau}$ and $\mu^*=\mu^*_{\lambda,\tau}$. By convexity of $G_\lambda$, we have
\begin{align*}
G_\lambda(\mu^*)+\int V[\mu^*]\d(\mu-\mu^*)\leq G_\lambda(\mu)\leq G_\lambda(\mu^*)+\int V[\mu]\d(\mu-\mu^*).
\end{align*}
Adding $\tau H(\mu)-G_\lambda(\mu^*)-\tau H(\mu^*)$ we get
\begin{gather*}
\int V[\mu^*]\d(\mu-\mu^*)+\tau H(\mu)-\tau H(\mu^*)\leq F(\mu)-F(\mu^*)\leq \int V[\mu]\d(\mu-\mu^*)+\tau H(\mu)-\tau H(\mu^*)
\intertext{which is equivalent to}
\tau H(\mu|\mu^*)-\tau H(\mu^*|\mu^*)\leq F(\mu)-F(\mu^*)\leq \tau H(\mu|\nu)-\tau H(\mu^*|\nu)
\end{gather*}
using $\mu^*\propto e^{-V[\mu^*]/\tau}$ (Thm.~\ref{thm:characterization}). The claim follows from $H(\mu^*|\mu^*)=0$ and $H(\mu^*|\nu)\geq 0$.
\end{proof}


\begin{proof}[Proof of Thm.~\ref{thm:stability}]
Let us start with an application of the previous lemma (we put hats on the quantity defined using the empirical measures $(\hat \nu_k)_{k=1}^K$):
\begin{align*}
\tau H(\hat \mu  |\mu^* ) &\leq F (\hat \mu ) - F (\mu^* )\\
& = [F (\hat \mu ) - \hat F (\hat \mu )] + [\hat F (\hat\mu ) - \hat F (\mu^* )] + [\hat F (\mu ^*) - F (\mu^*)]\\
&\leq G_\lambda (\hat \mu ) - \hat G_\lambda (\hat \mu ) + 0 + \hat G_\lambda (\mu^* ) - G_\lambda (\mu^* )  \\
& \leq \sum_{k=1}^K w_k \big(T_\lambda(\hat \mu,\nu_k)-T_\lambda(\hat \mu,\hat \nu_k)\big) + \sum_{k=1}^K w_k\big(T_\lambda(\mu^*,\hat \nu_k)-T_\lambda(\mu^*,\nu_k)\big)\\
& \leq 2\sum_{k=1}^K w_k \sup_{\mu\in \Pp(\Xx)} \vert  T_\lambda(\mu, \hat \nu_k) - T_\lambda(\mu, \nu_k) \vert 
\end{align*}
Now by convexity of $T_\lambda$ in $\nu_k$, we have (denoting $\psi_{\mu,\nu}$ the Schr\"odinger potential from $\nu$ to $\mu$):
$$
\int \psi_{\mu,\hat \nu_k} \d [\nu-\hat \nu] \leq T_\lambda(\mu,\nu_k)-T_\lambda(\mu,\hat \nu_k) \leq \int \psi_{\mu,\nu_k} \d [\nu-\hat \nu].
$$
Given any norm $\Vert \cdot \Vert$ on the set of continuous functions defined up to constants, denoting $\Vert \sigma \Vert_*=\sup_{\Vert f\Vert\leq 1} \int f\d\sigma$ the dual norm on the space of signed measures with $0$ total mass, it follows 
$$
\tau H(\hat \mu|\mu^*) \leq 2 C \sum_{k=1}^K w_k \Vert \nu_k - \hat \nu_k\Vert_*
$$
where $C=\sup_{\mu,\nu\in \Pp(\Xx)}\Vert \psi_{\mu,\nu}\Vert$. Let us now consider the two claims separately.

\emph{(i)} If $c(x,\cdot)$ is $L$-Lipschitz for all $x\in \Xx$, then it can be seen from the Schr\"odinger system~\eqref{eq:schrodinger-system} that $\psi_{\mu,\nu}$ is also $L$-Lipschitz. Taking the Lipschitz semi-norm $\Vert f\Vert = \sup_{x\neq y}\frac{\vert f(x)-f(y)\vert}{\vert y-x\vert}$, the dual of which is the Kantorovich-Rubinstein norm $\Vert \nu_k -\hat \nu_k \Vert_* = W_1(\nu_k,\hat \nu_k)$ gives the first claim.

\emph{(ii)}  If $c\in \tilde \Cc^p$ then by differentiating the Schr\"odinger system $p$ times it follows that $\psi_{\mu,\nu}$ is $p$ times differentiable with $\Vert \psi_{\mu,\nu}\Vert_{\tilde \Cc^p} \leq C (1+\lambda^{1-p})$ where $C>0$ is a constant deoending on the cost~\cite{genevay2019sample}. Since $\Xx$ is assumed compact, it follows that the homogeneous Sobolev norm $\Vert \psi_{\mu,\nu}\Vert_{\dot H^p}$  admits the same bound (up to a constant depending on the diameter of $\Xx$). The conclusion follows from the fact that the norms $\dot H^p$ and $\dot H^{-p}$ are dual to each other.
\end{proof}

\subsection{Estimation from independent samples}
In this section, we consider the statistical properties $(\lambda,\tau)$-barycenters. Assume that we dispose of $n$ independent samples $x^{(k)}_1,\dots, x^{(k)}_n$ from each of the marginals $\nu_k$. Let $\hat \mu_\lambda\in \Pp_2(\Xx)$ be the the plug-in estimator of the barycenter, defined as the $(\lambda,\tau)$-barycenter between the empirical marginals $\hat \nu_k = \frac1n \sum_{i=1}^n \delta_{x_i^{(k)}}$. 


\begin{corollary}
Let $\tau,\lambda>0$, define $d'=2\lfloor d/2\rfloor$ and assume that $c\in \Cc^{1+d'/2}(\Xx\times \Xx)$. Let $\hat \mu_{\lambda,\tau}$ be the empirical barycenter and $\mu^*_{\lambda,\tau}$ the population barycenter. Then there is $C>0$ independent of $(\nu_k)_k$ such that 
\begin{align*}
\E [H(\hat \mu_{\lambda,\tau}|\mu^*_{\lambda,\tau})]\leq C \tau^{-1}(1+\lambda^{-d'/2})n^{-1/2}.
\end{align*}
\end{corollary}
\begin{proof}
We follow a strategy similar to that used for the sample complexity of EOT~\cite{genevay2019sample}. In Thm.~\ref{thm:stability}, one can replace the homogeneous Sobolev norm $\dot H^{-p}$ by the (larger) inhomogeneous norm $H^{-p}$. For $p=1+d'/2$, it is known that $H^{p}$ is a Reproducible Kernel Hilbert space norm, and by standard empirical process theory results~\cite{bartlett2002rademacher} one has 
\begin{equation*}
\E \Vert \hat \nu_k -\nu_k\Vert_{H^{-p}} \leq C n^{-1/2}. \qedhere
\end{equation*}
\end{proof}

\paragraph{Related work} To the best of our knowledge, this is the first estimation rate for an OT-like barycenter that does not suffer from the curse of dimensionality. The rate of estimation for $(0,\tau)$-barycenter was studied in~\cite{bigot2020statistical}, where the rate is cursed by the dimension because then the bound of Thm.~\ref{thm:stability}-(i) involves the quantity $W_1(\nu_k,\hat \nu_k)$ which is of order $n^{-1/d}$ for $d>2$~\cite{fournier2015rate}. In the same paper, they also studied $(\lambda,\lambda)$-barycenter but on a discrete space. The estimation of barycenters on discrete spaces is a rich topic but with a very different behavior~\cite{heinemann2023kantorovich, heinemann2022randomized}. See~\cite{panaretos2019statistical} for an introduction to statistical aspects of OT, including barycenters. We also note that there is a line of works (see e.g.~\cite{ahidar2020convergence, le2022fast}) that studies the estimation of barycenters given marginals $(\nu_k)_k$ sampled from a distribution in $\Pp(\Pp(\Xx))$ (as in~\eqref{eq:p-of-p}) which is a rich but different problem.



\section{Optimization with Noisy Particle Gradient Descent}\label{sec:dynamics}
We consider the computation of $(\lambda,\tau)$-barycenters when the marginals $(\nu_k)_k$ are discrete with $n$ atoms each. In this case, the size of the problem is given by $n$ (the number of atoms), $K$ (the number of marginals) and $d$ (the ambiant dimension). 

For small scale problems where one of these quantities is small, plenty of algorithms from the literature exist. For instance when $n$ or $K$ is small, one can directly solve a linear program of size $O(n^K)$ -- the multimarginal formulation~\cite{agueh2011barycenters} -- to compute the $(0,0)$-barycenter. When $d$ is small, there exists efficient exact methods~\cite{altschuler2021wasserstein}. In that case, an alternative is to discretize the space, and use convex optimization algorithms to solve Eq.~\eqref{eq:optim-objective}. This includes approaches based on linear programming~\cite{anderes2016discrete, ge2019interior},  entropic regularization~\cite{cuturi2014fast, benamou2015iterative} or decentralized and randomized algorithms~\cite{ dvurechenskii2018decentralize,staib2017parallel,heinemann2022randomized}, see~\cite{peyre2019computational} for a review. This approach also applies for $(\lambda,\tau)$-barycenters (we compute 1D barycenters with this method in Section~\ref{sec:numerics}).

In this section, we focus on large scale problems ($n,d,K\gg 1$) where these discrete approaches are intractable and ``free support'' methods become relevant. A stream of recent work proposed methods based on neural networks~\cite{korotin2020continuous, cohen2020estimating, li2020continuous, fan2020scalable}. These methods come with the advantages (useful statistical prior, reasonable iteration complexity) and the drawbacks (lack of optimization guarantees) of neural networks. Particle-based methods, which are closer in spirit to what follows, have been proposed such as fixed-point methods akin to Loyd's algorithm~\cite{alvarez2016fixed, claici2018stochastic,von2022simple,backhoff2022stochastic} and a particle gradient method~\cite{daaloul2021sampling} for Wasserstein barycenters. Note that it is shown in~\cite{altschuler2022wasserstein} that Wasserstein barycenters are NP-hard to compute in large dimension. A Franck-Wolfe algorithm~\cite{luise2019sinkhorn} was proposed for Sinkhorn divergence barycenters.



In what follows, we propose a grid-free numerical method which is particularly well-suited to the structure of the problem of Eq.~\eqref{eq:optim-objective}, called Noisy Particle Gradient Descent (NPGD). We defer a detailed complexity analysis of this method to future works, and limit ourselves to a introduction of the algorithm with its exponential guaranty in the mean-field limit, which is an application of~\cite{chizat2022mean,nitanda2022convex}.

\subsection{Noisy Particle Gradient Descent} 
We parameterize the unknown measure as a mixture of $m\in \NN^*$ particles $\hat \mu=\frac1m \sum_{i=1}^m \delta_{X_j}$. Let $\mathbf{X} = (X_1,\dots,X_m)\in (\RR^d)^m$ encode the position of all particles and consider the function
\begin{equation}\label{eq:Gm}
G_m(\mathbf{X}) \coloneqq G_\lambda\Big(\frac1m \sum_{i=1}^m \delta_{X_j}\Big).
\end{equation}
The NPGD algorithm we consider is simply noisy gradient descent on $G_m$, with an initialization sampled from some $\mu_0\in \Pp(\Xx)$. It is defined, for $\ell\in \NN$, as 
\begin{equation}\label{eq:NPGD}
\mathbf{X}[\ell+1] = \mathsf{P}_\Xx\big(\mathbf{X}[\ell] - m\eta \nabla G_m(\mathbf{X}[\ell]) + \sqrt{2\eta\tau}   \mathbf{Z}[\ell]\big),\quad \mathbf{X}[0] \sim \mu_0^{\otimes m}
\end{equation}
where $\tau>0$ is the outer-regularization strength, $\eta>0$ is the step-size, $ \mathbf{Z}[1],  \mathbf{Z}[2],\dots$ are i.i.d.~standard Gaussian vectors and $ \mathsf{P}_\Xx$ is the Euclidean projection on $\Xx$.
Note that to compute $\nabla G_m$ one needs to solve, at each iteration, $K$ regularized optimal transport problems. In the small step-size limit $\eta\to 0$ and setting $t=k\eta$, NPGD leads to a system of SDEs coupled via the empirical distribution of particles $\hat \mu_t$:
\begin{align}\label{eq:SDE}
\left\{
\begin{aligned}
\d X_i(t) &=  - \nabla V[\hat \mu_t](X_i(t))\d t + \sqrt{2\tau}   \d B_{t,i}  + \d\Phi_{t,i},\quad X_i(0) \sim \mu_0 \\
\hat \mu_t &=\frac1m \sum_{i=1}^m \delta_{X_i(t)}
\end{aligned}
\right.
\end{align}
where $(B_{t,i})_{t\geq 0}$ are independent Brownian motions in $\RR^d$, $\d\Phi_{t,i}$ is a boundary reflection (in the sense of Skorokhod problem) and $V[\mu]\in \Cc^1(\Xx)$ is the \emph{first-variation} of $G_\lambda$ at $\mu$ (see Definition~\ref{eq:first-variation}). The latter satisfies $V[\hat \mu](X_i)=m\nabla_{X_i} G_m(\mathbf{X})$, hence~\eqref{eq:NPGD} is just the Euler-Maruyama discretization of~\eqref{eq:SDE} below.

\subsection{Mean-Field Langevin dynamics} In the many-particle $m\to \infty$ limit, it can be shown that the particles behave like independent sample paths from the nonlinear SDE of McKean-Vlasov type:
\begin{equation}
\left\{
\begin{aligned}
\d X_t &=-\nabla V[\mu_t] (X(t))\d t + \sqrt{2\tau}\d B_t +\d\Phi_t, \quad X_0\sim \mu_0\\
\mu_t &=\mathrm{Law}(X_t)
\end{aligned}
\right.
\end{equation}
where $(B_t)_{t\geq 0}$ is a Brownian motion and $(\Phi_t)_{t\geq 0}$ a boundary reflection.
Moreover, the distribution $(\mu_t)_{t\geq 0}$ of particles solves the evolution equation
\begin{equation}\label{eq:PDE}
\partial_t \mu_t = \nabla \cdot \big(\mu_t \nabla V_\lambda[\mu_t] \big) +  \tau \Delta \mu_t
\end{equation}
starting from $\mu_0\in \Pp(\Xx)$ where $\nabla \cdot$ stands for the divergence operator. By solution of~\eqref{eq:PDE} here, we mean an curve $(\mu_t)_{t\geq 0}$ starting from $\mu_0$ that is absolutely continuous in Wasserstein space and satisfies Eq.~\eqref{eq:PDE} in the sense of distributions  with no-flux boundary conditions. 

This drift-diffusion equation is an instance of \emph{Mean-Field Langevin} dynamics, a class of drift-diffusion dynamics studied in~\cite{mei2018mean, hu2019mean, nitanda2022convex,chizat2022mean}. It can be interpreted as the gradient flow of the functional $F_\lambda$ of Eq.~\eqref{eq:optim-objective} under the $W_2$ Wasserstein metric. 

In our theoretical analysis, we focus on the analysis of the mean-field limit~\eqref{eq:PDE}. For quantitative convergence results in the many-particle $m\to \infty$ and small step-size $\eta\to 0$ limits, one can refer to the classical work~\cite{sznitman1991topics}. The particular case of reflecting boundary conditions has been treated in~\cite{javanmard2020analysis}, following earlier works on the analysis of SDEs with reflection~\cite{tanaka1979stochastic, lions1984stochastic}.
 

\subsection{Exponential convergence}
It has been shown independently in~\cite{nitanda2022convex} and~\cite{chizat2022mean} that if the function $G_\lambda$ is convex, regular enough and that a certain family of log-Sobolev inequalities holds, then dynamics of the form Eq.~\eqref{eq:PDE} converge at an exponential rate  to the unique minimizer of $F_{\lambda}$. Let us apply this result in our context, where this leads to a dynamics that directly converges to the $(\lambda,\tau)$-barycenter.
\begin{assbox}
\begin{theorem}\label{thm:global-convergence}
Assume that $c\in \Cc^2(\Xx)$ (where, we recall, $\Xx$ is compact) and let $\lambda,\tau>0$. Then there exists a unique solution to~\eqref{eq:PDE}. Moreover, there exists $\rho_\lambda>0$ such that if $\mu_0\in \Pp(\Xx)$ is such that $F_{\lambda,\tau}(\mu_0)<\infty$, then it holds 
\begin{align*}
\frac{\lambda}{2}H(\mu_t|\mu^*_{\lambda,\tau})\leq F_{\lambda,\tau}(\mu_t) - F_{\lambda,\tau}(\mu^*_{\lambda,\tau})\leq e^{-\rho_\lambda t} \big( F_{\lambda,\tau}(\mu_0) - F_{\lambda,\tau}(\mu^*_\lambda) \big).
\end{align*}
\end{theorem}
\end{assbox}
\begin{proof}
We have semi-convexity of $F_{\lambda,\tau}$ along Wasserstein geodesics, by \cite[Thm.~4.1]{carlier2022lipschitz} for the first component $G_\lambda$ and by a standard result~\cite{santambrogio2015optimal} for the  $H$ component. Thus the general well-posedness results from~\cite{ambrosio2005gradient} applies. For the exponential convergence -- in function value and in relative entropy -- we apply the result from~\cite[Thm. 3.2]{chizat2022mean}, see also~\cite{nitanda2022convex} where the same argument was discovered independently (although stated on $\RR^d$, the argument goes through on a compact domain). 

The main assumptions to check are that (i) $\mu\mapsto G_\lambda(\mu)$ is convex (Prop.~\ref{prop:Reg_G}), (ii) that a global minimizer $\mu^*_{\lambda,\tau}$ exists (Thm.~\ref{thm:characterization}) and finally we need to check that the probability measure $\hat \mu_t \propto e^{-V[\mu_t]} \in \Pp(\Xx)$ satisfies a log-Sobolev inequality, uniformly in $t$ (Assumption 3 in~\cite{chizat2022mean}). 

Since $\Xx$ is bounded, the normalized Lebesgue measure satisfies a log-Sobolev inequality~\cite[Thm.~7.3]{ledoux1999concentration}. By the Holley-Stroock perturbation criterion~\cite{holley1986logarithmic} $\hat \mu_t$ satisfies it as well; this criterion applies here because $\sup_x V[\mu](x)- \inf_x V[\mu](x)$ is bounded, uniformly in $\mu\in \Pp(\Xx)$ by Prop.~\ref{prop:Reg_G}. 
\end{proof}
The contraction rate $\rho_\lambda$ is of the form $\tau e^{-L\cdot \mathrm{diam}(\Xx)/\tau}$ where $L$ is the Lipschitz constant of $c$ (and $L\cdot \mathrm{diam}(\Xx)$ is a uniform bound on $\Vert V[\mu]\Vert_{\mathrm{osc}}$). It thus approaches $1$ exponentially fast as $\tau$ decreases. We also note that both inner and outer regularizations are needed to obtain Thm.~\ref{thm:global-convergence}: in particular the inner-regularization is necessary to obtain well-posedness of the PDE~\eqref{eq:PDE}.

\section{Numerical experiments}\label{sec:numerics}
The (Julia) code to reproduce the experiments is available online\footnote{\url{https://github.com/lchizat/2023-doubly-entropic-barycenter.git}}.

\paragraph{Comparison of 1D-barycenters} On Fig.~\ref{fig:comparison-1D} we compare various barycenters for $K=3$ and $(\nu_k)_{k=1}^3$ probability densities on $\Xx=[0,1]$ with cost $c(x,y)=\nicefrac12 \Vert y-x\Vert^2_2$. The $(\lambda,\tau)$-barycenters have been computed numerically using gradient ascent in $L^2(w_1\nu_1)\times \dots L^2(w_K\nu_K)$ on the smooth dual~\eqref{eq:dual} after discretizing the problem on a regular grid of size $200$. The algorithm converged linearly for small enough step-sizes. For reference, we plot in blue the unregularized Wasserstein barycenter $\mu^*_{0,0}$ that is computed by taking the $L^2$-barycenter of the quantile functions~\cite[Chap.~2]{santambrogio2015optimal}. We observe that the choice $\tau=\lambda/2$ indeed gives the best approximation of $\mu^*_{0,0}$ for fixed $\lambda$. For comparison, we also plot the Sinkhorn divergence barycenter $\mu^*_{\lambda,\mathrm{div}}$, which is computed with~\cite[Alg.~1]{janati2020debiased}. We observe that it also approaches weakly $\mu^*_{0,0}$ but its density displays strong oscillations, suggesting that this object is in general less well-behaved than $(\lambda,\tau)$-barycenters.

\begin{figure}
\centering
\begin{subfigure}{0.33\linewidth}
\centering
%\vspace{0.4cm}
\includegraphics[scale=0.36]{bary1D-marg}
%\vspace{0.4cm}
\caption{Densities $(\nu_k)_{k=1}^3$ and $\mu^*_{0,0}$}\label{subfig:1D-marg}
\end{subfigure}%
\begin{subfigure}{0.33\linewidth}
\centering
%\vspace{0.4cm}
\includegraphics[scale=0.36]{bary1D-128}
%\vspace{0.4cm}
\caption{$\lambda=1/128$}\label{subfig:1D-128}
\end{subfigure}%
\begin{subfigure}{0.33\linewidth}
\centering
\includegraphics[scale=0.36]{bary1D-512}
\caption{$\lambda=1/512$}\label{subfig:1D-512}
\end{subfigure}
\caption{Iso-barycenters for $(\nu_k)_{k=1}^3$ probability densities on $\Xx=[0,1]$ (displayed on the left). For $\lambda,\tau>0$, $\mu^*_{\lambda,\tau}$ is computed with gradient ascent on the discretized dual problem.}
\label{fig:comparison-1D}
\end{figure}


\paragraph{Escaping stationary points with noisy particle gradient descent} To illustrate the global convergence of Noisy Particle Gradient Descent (NPGD) to $\mu^*_{\lambda,\tau}$ in the mean-field limit, we consider a configuration for which $G_0$ has a ``bad'' local minimum and initialize the dynamics at this measure as shown on Fig.~\ref{subfig:2D-1}. We consider on $\Xx=\RR^2$ the barycenter of\footnote{This example was suggested to us by Hugo Lavenant.} 
\begin{align*}
\nu_1 = \frac12 (\delta_{(-3,0)}+ \delta_{(-1,1)})
&&\text{and}&& \nu_2 = \frac12 (\delta_{(1,1)}+ \delta_{(3,1)})
\end{align*}
with cost $c(x,y)=\frac12 \Vert y-x\Vert^2_2$. It can be checked with direct computations that $\mu^*_{(0,0)} = \frac12 (\delta_{(-1,\nicefrac12)}+ \delta_{(1,\nicefrac12)})$ and that the measure $\mu^*_{\mathrm{init}} =\frac12 (\delta_{(0,0)}+ \delta_{(0,1)})$ is a stable local minimizer (in the sense that when parameterized by the positions of these two Dirac masses, $G_0$ is locally minimized by $((0,0),(0,1))$ and its Hessian at this point is positive definite, proportional to the identity).
In particular, fixed-point or gradient descent iterations for $G_0$ would not move from this stationary point. On Fig.~\ref{subfig:2D-1} we observe, in accordance to Thm.~\ref{thm:global-convergence}, that NPGD can escape from the neighborhood of $\mu_{\mathrm{init}}$ and converge to a discrete approximation of $\mu^*_{(\lambda,\lambda/2)}$, itself an approximation of $\mu^*_{(0,0)}$. Note that this is only observed when $\tau$ is large enough, otherwise the dynamics is trapped in ``metastable'' states (here $\lambda=0.1$ and $\tau=\lambda/2$). We have used $m=200$ particles, a step-size of $0.5$ and Fig.~\ref{subfig:2D-1} represents the state of NPGD after $400$ iterations.


\begin{figure}
\begin{subfigure}{0.5\linewidth}
\centering
%\vspace{0.4cm}
\includegraphics[scale=0.55]{bary2D-1}
%\vspace{0.4cm}
\caption{Initializing on a local minimum of $G_0$ ($t=0$)}\label{subfig:2D-1}
\end{subfigure}%
\begin{subfigure}{0.5\linewidth}
\centering
\includegraphics[scale=0.55]{bary2D-400}
\caption{Approximate global minimizer ($t$ large)}
\label{subfig:2D-400}
\end{subfigure}
\caption{Illustration of the global convergence of the noisy particle gradient descent algorithm. (left) We initialize $m=200$ particles on the measure $\mu_{\mathrm{init}}$ which is a local minimizer of $G_0$. (right) The algorithm converges to a distribution which approximates $\mu^*_{\lambda,\lambda/2}$, itself a small convolution of $\mu^*_{0,0}$.}
\label{fig:NPGD}
\end{figure}

\section{Conclusion}
We have proposed doubly-regularized EOT barycenters which is a formulation of barycenters over $\Pp(\Xx)$ that has several benefits: (i) it gathers most previously studied OT-like barycenters in a single framework, (ii) it leads to several desirable properties such as regularity, stability and approximation of Wasserstein barycenters (for $\tau=\lambda/2$), and (iii) it lends itself naturally to both convex optimization methods and grid-free methods. 

In this paper, we have covered a diverse range of theoretical topics (regularity, approximation, statistics, optimization) as it is really all these aspects that together make this family of objects stand out. In future work, it will be desirable to develop in depth analyses of specialized points, such as the computational complexity of $(\lambda,\tau)$-barycenters.



%\section{Conclusion}

%\paragraph{Acknowledgments.} 

%\bibliographystyle{natbib}
%\bibliography{LC.bib}
\printbibliography
\appendix

\iffalse
\section{Facts about entropy regularized optimal transport}\label{app:duality}
The following classical facts can be found for instance in the lecture notes~\cite[Sec.~4]{nutz2021introduction}. 
Consider $\mu,\nu \in \Pp_2(\RR^d)$ and the entropic optimal transport problem defining $T_\lambda(\mu,\nu)$ in Eq.~\eqref{eq:EOT}. This problem admits a unique solution $\gamma^*$ and admits a dual formulation
\begin{align}\label{eq:dual-EOT}
T_\lambda(\mu,\nu) = \max_{\phi\in L^1(\mu),\psi\in L^1(\nu)} \int \phi \d\mu + \int \psi \d\nu +\lambda \Big(1 - \int e^{(\phi(x)-\psi(y)-\frac12 \Vert y-x\Vert^2)/\lambda}\d\mu(x)\d\mu(y)\Big).
\end{align}
This dual problem admit a solution $(\phi^*,\psi^*)$. This solution is unique in $L^1(\mu)\times L^1(\nu)$ up to the transformation $(\phi^*+c,\psi^*-c)$ for $c\in \RR$. At optimality, we have that $T_\lambda(\mu,\nu) = \int \phi^*\d\mu + \int \psi^*\d\nu$ and the primal-dual relation
$$
\frac{\d \gamma^*}{\d \mu\otimes \nu}(x,y) = g(x,y)\coloneqq e^{(\phi(x)+\psi(y)-\frac12\Vert y-x\Vert^2)/\lambda}, \quad \mu\otimes \nu \text{ a.e.}
$$
Moreover, the potentials satisfy for $\mu\otimes \nu$ almost every $(x,y)$, the optimality condition
\begin{align}\label{eq:schrodinger-system}
\left\{
\begin{aligned}
\phi^*(x) &= -\lambda \log \Big( \int e^{(\psi^*(y)-\frac12 \Vert y-x\Vert^2)/\lambda}\d\nu(y)\Big)\\
\psi^*(y) &= -\lambda \log \Big( \int e^{(\phi^*(x)-\frac12 \Vert y-x\Vert^2)/\lambda}\d\mu(x)\Big)
\end{aligned}
\right.
\end{align}
These equations can be used to extend $\phi^*$ and $\psi^*$ as continuous (in fact infinitely differentiable) functions over $\RR^d$, which satisfy these equations everywhere. 

\begin{definition}[Schr\"odinger potentials]\label{def:potentials}
The pair of functions $(\phi,\psi)\in \Cc^\infty(\Xx)\times \Cc^\infty(\Xx)$ which satisfies the \emph{Schr\"odinger system} Eq.~\eqref{eq:schrodinger-system} for all $(x,y)\in \Xx^2$ is called the \emph{Schr\"odinger} potentials. This pair is unique up to the transformation $(\phi^*+c,\psi^*-c)$ for $c\in \RR$, which we quotient out by fixing a point $x_0\in \Xx$ and setting $\phi(x_0)=0$.
\end{definition}

This particular choice of potentials among all those that satisfy Eq.~\eqref{eq:schrodinger-system} $\mu\otimes \nu$-almost everywhere is justified by the following result.
\begin{proposition}[First-variation of entropic optimal transport]\label{prop:propertiesEOT} Fix $\nu \in \Pp(\Xx)$ and for $\mu\in \Pp(\Xx)$ let $(\phi_\mu,\psi_\mu)$ be the Schr\"odinger potentials associated to the pair $(\mu,\nu)$. Then:
\begin{itemize}
\item The map $\mu\mapsto \phi_\mu$ satisfies the following Lipschitz continuity property: for any $k\in \NN$, there exists $C_k>0$ such that it holds
\[
\Vert \phi_\mu - \phi_{\mu'}\Vert_{\Cc^k(\Xx)} \leq C_k W_2(\mu,\mu'),\quad \forall \mu,\mu'\in \Pp(\Xx).
\]
\item  the function 
$\mu\mapsto T_\lambda(\mu,\nu)$ is convex and admits a first-variation in the sense of Definition~\ref{def:first-variation}, which is given by $V[\mu]=\phi_\mu$.
\end{itemize}
\end{proposition}
\begin{proof}
The first claim is technical and is proved in~\cite{carlier2022lipschitz}. The convexity of $T_\lambda$ is clear by Eq.~\eqref{eq:dual-EOT} which expresses it as a supremum of continuous affine forms. To prove that $\phi_\mu$ is the first-variation, following ~\cite{feydy2019interpolating}, let $\mu_\epsilon \coloneqq (1-\epsilon)\mu+\epsilon\nu$. Using twice the concavity of Eq.~\eqref{eq:dual-EOT}, we have
\begin{align*}
 \int \phi_\mu\d(\nu-\mu)\leq \frac1\epsilon \Big(F(\mu_\epsilon)-F(\mu)\Big) \leq \phi_{\mu_\epsilon}\d(\nu-\mu).
\end{align*}
The conclusion follows by using the continuity in of $\mu\mapsto \phi_\mu$ for the supremum norm.
\end{proof}

One can differentiate~\eqref{eq:schrodinger-system} to see that $(\phi^*,\psi^*)$ belong to $\Cc^\infty(\RR^d)$, and we have for $x\in \RR^d$
\begin{align}\label{eq:potentials-derivative1}
\nabla \phi(x) &= x -\int y\d\gamma^*(y|x) %= x-\E [Y|X=x]
\\
\nabla^2 \phi(x) &=\frac1\lambda \Big(1-\int \Big(y -\int y'\d\gamma^*(y'|x)\Big)^2\d\gamma^*(y|x)\Big) %=\frac1\lambda\Big(1- \mathbf{Var}[Y|X=x]\Big)
\label{eq:potentials-derivative2}
\end{align}
where $(X,Y)$ has law $\gamma^*$ and $\gamma^*(\d y|x) = \int g(x,y) \nu(\d y)$ is the conditional distribution of $Y$ given $X$. 

As remarked in~\cite{genevay2019sample}, successively differentiating Eq.~\eqref{eq:schrodinger-system} it can be seen (by Fa\`a di Bruno's formula) that for any $k\in \NN$ there exists $C_k$ such that
\[
\Vert \phi_\mu\Vert_{\Cc^k} \leq C_k\lambda^{\min\{0, 1-k\}},\quad\forall \mu
\in \Pp(\Xx).
\]
\fi

\section{Closed forms for isotropic Gaussians}\label{app:gaussians}
In this section, we consider the simplest setting of centered isotropic Gaussian probability measures and rely on closed forms for entropic optimal transport. We consider
\begin{align}\label{eq:simplified}
\min_{\nu\in \Pp(\RR^d)} T_\lambda(\mu,\nu) + \tau H(\nu)
\end{align}
for $\mu=\Nn(0,a)$ where $a>0$ is the variance. This is enough to recover the general case of barycenters of isotropic Gaussians with identical variances since the effect of having different means can be factored out as in~\cite{janati2020entropic}. Note that~\cite{ballu2020stochastic} also considered this problem but did not obtain a closed form solution.  Let us look for a Gaussian solution of the form $\nu = \Nn(0,b)$. By~\cite{janati2020entropic}, we have that the optimal dual potentials are $\varphi=\frac12 \tilde u \Vert x\Vert_2^2$ and $\psi=\frac12 \tilde v \Vert y\Vert^2$ with
\begin{align*}
\tilde u &= 1 - \frac{2b}{\xi+\lambda},&
\tilde v &= 1 - \frac{2a}{\xi+\lambda},&
\xi &=  \sqrt{4ab+\lambda^2}
\end{align*}
where the variables from~\cite{janati2020entropic} and our variables are related by $\sigma^2 \rightarrow \lambda$, $U \rightarrow-\lambda \tilde u I$, $V \rightarrow -\lambda \tilde v I$, $C \rightarrow I\cdot (\xi-\lambda)/2$. Also the EOT cost and entropy is
\begin{align*}
\frac{2}{d}T_\lambda(\mu,\nu) &= a+b - \xi +\lambda \log(\xi+\lambda)+C,& \frac{2}{d}\tau H(\mu) &=-\tau \log(b)+C
\end{align*}
where $C$ denotes a constant independent of $a$ and $b$. When restricted to $\nu$ of the form $\Nn(0,b)$, the optimization problem thus becomes (keeping in mind that $\xi$ depends on $b$)
$$
\min_{b>0} a+b - \xi +\lambda \log(\xi+\lambda)-\tau \log(b).
$$
The first order optimality condition reads 
\begin{align}\label{eq:sol-gaussian}
1- \frac{2a}{\xi+\lambda}-\frac{\tau}{b}=0
\end{align}
Now, let us argue that if $b$ satisfies this equation, then $\nu$ is in fact the unique minimizer of~\eqref{eq:simplified}. Indeed, \eqref{eq:simplified} admits a unique solution characterized by the first order optimality condition (which can be justified, in this non-compact setting, as in~\cite{janati2020entropic}):
$$
\log\Big(\frac{\d\nu}{\d x}\Big) +\frac{\psi_{\mu,\nu}}{\tau} = C 
$$
But if $\nu=\Nn(0,b)$ with $b$ solution to~\eqref{eq:sol-gaussian} then it holds
\begin{align}
\tilde v &=\tau/b&\Rightarrow&&\frac{\psi_{\mu,\nu}(y)}{\tau} - \frac{\Vert y\Vert^2_2}{2b}=0
\end{align}
which proves that $\nu$ is indeed the unique solution to our problem.
\paragraph{Explicit solution.} Using $b=(\xi^2-\lambda^2)/(4a)$, let us express~\eqref{eq:sol-gaussian} in terms of $\xi$:
\begin{align*}
b(\xi+\lambda)-2ab-\tau(\xi+\lambda)=0 \Leftrightarrow \frac{\xi+\lambda}{4a}(\xi^2-2a\xi-\lambda^2+2a\lambda-4a\tau)=0.
\end{align*}
Solving for $\xi$ and taking the largest solution (which must be the unique solution leading to $b\geq 0$) leads to
\begin{align*}
\xi = a+\sqrt{(a-\lambda)^2+4a\tau} &&\Rightarrow&& b = \frac{\big(a+\sqrt{(a-\lambda)^2+4a\tau}\big)^2-\lambda^2}{4a}.
\end{align*}
Let us now consider some particular cases:
\begin{itemize}
\item In the limit $\tau\to 0$, we have
$$
b= \frac{(a+\vert a-\lambda\vert)^2-\lambda^2}{4a} =
\begin{cases}
a-\lambda &\text{if $a>\lambda$}\\
0 &\text{if $a\leq \lambda$}
\end{cases}
$$
\item In the limit $\lambda\to0$, we have
$$
b = \frac{a}{4}\big( 1+\sqrt{1+4\tau/a}\big)^2 = a+2\tau+O(\tau^2)
$$
\item for $\tau=\lambda$, we have
$$
b= \frac{(a+\vert a+\lambda\vert)^2-\lambda^2}{4a} = a +\lambda
$$
\item for $\tau=\lambda/2$, we have
\begin{align*}
b &=  \frac{(a+\sqrt{ a^2 +\lambda^2})^2-\lambda^2}{4a} \\
&= \frac{a^2+a^2+\lambda^2+2a\sqrt{ a^2 +\lambda^2}-\lambda^2}{4a} \\
&=\frac{2a^2+2a^2\sqrt{ 1 +(\lambda/a)^2}}{4a}\\
&=\frac{a}{2}(1+\sqrt{1+(\lambda/a)^2}) =a+\frac{\lambda^2}{4a}+O(\lambda^4)
\end{align*}
\end{itemize}

\paragraph{Best choice of $\tau$} Let us now solve for $\tau$ such that $b=a$, which we know is asymptotically $\tau\sim \lambda/2$. It holds
\begin{align*}
b-a = 0 &&\Leftrightarrow && (a+\sqrt{(a-\lambda)^2+4a\tau})^2-\lambda^2 -4a^2 = 0.
\end{align*} 
Let us define the intermediate unknown $\chi = \sqrt{(a-\lambda)^2+4a\tau} \Leftrightarrow \tau = (\chi^2-(a-\lambda)^2)/(4a)$. It follows
\begin{align*}
(a+\chi)^2-\lambda^2-4a^2 =0 && \Leftrightarrow &&  \chi^2 +2a\chi -\lambda^2-3a^2=0.
\end{align*} 
Solving for $\chi$, with the constraint $\chi\geq 0$ gives
\begin{align*}
\chi=\sqrt{4a^2+\lambda^2}-a&& \Leftrightarrow && \tau = \frac{(\sqrt{4a^2+\lambda^2}-a)^2-(a-\lambda)^2}{4a}.
\end{align*} 
Developing the squares and rearranging we get
\begin{align*}
\tau &= \frac{4a^2+-2a\sqrt{4a^2+\lambda^2}+2a\lambda}{4a} \\
&= \frac{\lambda}{2} + a\Big(1-\Big(1+\frac{\lambda^2}{4a^2}\Big)^{1/2}\Big)\\
&=\frac{\lambda}{2} -\frac{\lambda^2}{8a} +O(\lambda^4/a^3).
\end{align*}
\end{document}
