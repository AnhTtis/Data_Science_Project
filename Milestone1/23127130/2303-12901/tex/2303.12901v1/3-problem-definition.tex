\section{Overview}

\subsection{Problem Definition}
\label{subsec:problem-define}

The computation \textbf{kernels} in GNN inference are feature aggregation and feature transformation which correspond to  Aggregate() and Update() in the message-passing paradigm of GNN (Algorithm \ref{alg:GNN-computation-abstraction}). 
\begin{itemize}
    \item  Aggregate(): The input is graph adjacency matrix $\bm{A}$ and  feature matrix $\bm{H}_{\text{in}}$. The output is $\bm{H}_{\text{out}} = \bm{A} \times\bm{H}_{\text{in}}$.
    \item  Update(): The input is vertex feature matrix  $\bm{H}_{\text{in}}$ and weight matrix $\bm{W}$. The output is  $\bm{H}_{\text{out}} = \bm{H}_{\text{in}} \times \bm{W}$.
\end{itemize}
The computation \textbf{primitives} are GEMM, SpDMM and SPMM. While all  the primitives perform multiplication of two input matrices to produce an output matrix, they have different ways of dealing with the zero elements: (1) GEMM views the two input matrices as dense matrices, and performs multiply-accumulate for all the matrix elements no matter whether an element is non-zero or not. (2) SpDMM views one input matrix as sparse matrix and skips the computation operations for all the zero elements in this input matrix. (3) SPMM takes two input sparse matrices and skips the computation operations for all the zero elements in the two input matrices. 

% While dense primitive (GEMM) has regular computation pattern that can be efficiently executed on architectures such as 2-D systolic array, the sparse primitives (SpDMM and SPMM) have irregular computation pattern and memory access pattern. Therefore, designing a unified architecture to efficiently support all the primitives is challenging. 

% Therefore, a GNN kernel can be mapped to any of the basic primitives.


This work targets full-graph inference: given a GNN model and an input graph, we perform the message-passing paradigm (Algorithm \ref{alg:GNN-computation-abstraction}) in the full input graph to obtain the embeddings of all the vertices. Full-graph inference has been widely studied in the literature \cite{yan2020hygcn, geng2020awb, zhang2021boostgcn}. Our objective is to exploit the data sparsity of GNN kernels to further accelerate the inference process. We assume that the sparsity of the data is unknown before the accelerator design or hardware execution. Our intent is to develop a single hardware-software codesign on FPGA that is efficient and flexible to support  various graphs and GNN models of various data sparsity. Therefore, the proposed work does not require regenerating the FPGA accelerator if data sparsity changes.

% Although we target at the full-graph GNN inference, the proposed codesign can be easily extended to mini-batch GNN inference and GNN training \cite{zeng2020graphact}. 

% \cite{yan2020hygcn, geng2020awb, zhang2021boostgcn}