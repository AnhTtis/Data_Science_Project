

\subsection{System Overview}

\begin{figure}[h]
     \centering
     \includegraphics[width=8.5cm]{pic/system-overview.pdf}
     \caption{Overview of the proposed system}
     \label{fig:system-overview}
\end{figure}

 {
Figure \ref{fig:system-overview} depicts the proposed system design. The software comprises of a \emph{compiler} and a \emph{runtime system}. The hardware system has three components:
\begin{itemize}
    \item \textbf{Host processor}: The compiler is executed on the host processor to perform compilation (preprocessing) for the input GNN model and the input graph to generate the intermediate representation (IR). The IR is sent to the soft processor for execution.
    \item \textbf{Soft Processor on FPGA}: The runtime system is executed on the soft processor. It takes the IR as input, and dynamically schedules the computation tasks on the accelerator by sending control signals to the accelerator. 
    \item \textbf{Accelerator on FPGA}: It executes the three computation primitives (GEMM, SpDMM, SPMM), profiles data sparsity, and performs data layout/format transformation. It receives the control signals from the soft processor to execute the computation tasks, and also sends the data sparsity information to the soft processor at runtime.
\end{itemize}
}

% The hardware design on FPGA  consists of a soft processor and an accelerator. The compiler is executed on the host processor. The runtime system is executed on the soft processor which interacts with the accelerator through a low-latency interconnection. It consists of an Analyzer and a Scheduler. Figure \ref{fig:computation-core} depicts the proposed hardware design on FPGA. The accelerator has $N_{CC}$ parallel Computation Cores, (Figure \ref{fig:computation-core}) which can execute the three primitives -- GEMM, SpDMM and SPMM. 
{
The workflow is illustrated in Figure \ref{fig:workflow}. The execution of GNN inference consists of two steps: }

 {
\vspace{0.1cm}
\noindent \textbf{Step 1. Compilation/Preprocessing}: The compiler (See Section \ref{sucsec:IR}) performs the following preprocessing: \circled{1} \textbf{Generating intermediate representation (IR):} It takes the specifications of the user-defined GNN model and the graph meta data as input, and generates the IR for the GNN computation graph (See Figure \ref{fig:workflow}).
\circled{2} \textbf{Data partitioning}: The compiler performs data partitioning for each kernel. Data partitioning is required since (1) in real-world applications, the input graph can be very large and the FPGA accelerator has limited on-chip memory, (2) within a matrix,
% (Figure \ref{fig:density-of-adjacency-matrix})
different parts of the matrix can have different data sparsity. Data partitioning enables fine-grained kernel-to-primitive mapping (See Section \ref{subsec:dyna-k2p-mapping}), leading to more efficient sparsity exploitation. \circled{3} \textbf{Preprocessing of data sparsity}: When the compiler performs data partitioning, it uses counters to profile the sparsity information of graph adjacency matrix $\bm{A}$, weight matrix $\bm{W}$, and input feature matrix  $\bm{H}^{0}$. Note that the sparsity information of the feature matrices in the intermediate layers $\{\bm{H}^{1},...,\bm{H}^{L}\}$ is unknown at compile time and is profiled by the accelerator at runtime.}

% Then, the compiler performs data partitioning and sends the optimized IR to the runtime system for executing on FPGA. 




% Note that the runtime system is executed on-the-fly when the GNN inference is executed on the accelerator.             

% \begin{figure}[h]
%      \centering
%      \vspace{-0.2cm}
%      \includegraphics[width=7cm]{pic/overview-hw-arch.pdf}
%      \vspace{-0.2cm}
%      \caption{The overview of hardware architecture on FPGA}
%      \vspace{-0.3cm}
%      \label{fig:hardware-overview}
% \end{figure}


{
\vspace{0.1cm}
\noindent \textbf{Step 2. Runtime execution}: At runtime, the soft processor and the accelerator collaborate to perform GNN inference. The runtime system on the soft processor consists of \emph{an Analyzer} and \emph{a Scheduler}. The accelerator contains multiple Computation Cores. The Analyzer takes the optimized IR from the compiler and the data sparsity information from the compiler and the accelerator to dynamically map a kernel to a primitive based on a performance model. Then, the Scheduler schedules the execution of the primitives on the accelerator (Section \ref{subsec:task-scheduling}). The runtime system performs dynamic kernel-to-primitive (K2P) mapping. Note that the mapping must be performed dynamically at runtime: (1) The densities of the feature matrices in the intermediate layers $\{\bm{H}^{1},...,\bm{H}^{L}\}$  are unknown before runtime; (2) The Computation Core has various execution modes (Section \ref{subsec:Microarchitecture}) with each mode executing a specific primitive. These execution modes have different computation efficiency (See Section \ref{subsec:performance-model}) with respect to the density of data. As a result, for a computation kernel of high density, executing it using GEMM primitive on the Computation Core will be more efficient. For a GNN kernel of low density, executing it using SpDMM or SPMM primitive on  the Computation Core will be more efficient. To handle this scenario, we build an analytical performance model (Section \ref{subsec:dyna-k2p-mapping}) to estimate the execution latency of a given primitive on the Computation Core with respect to the data sparsity. }

% Since a GNN kernel usually involves the multiplication of two matrices in ($\bm{A}$, $\bm{H}$, $\bm{W}$), the three basic primitives are sufficient to cover the matrix multiplication of various data sparsity.       

\vspace{0.1cm}
 {
The rest of the paper is organized as follows:  Section \ref{sec:compiler} covers the details of the compiler; Section \ref{sec:accelerator-design} introduces the proposed accelerator design; Section \ref{sec:runtime-system} introduces the proposed runtime system; Section \ref{sec:Implementation details} and \ref{sec:Evaluation-Results} describe the implementation details and evaluation results, respectively.
 }



% \subsection{Overview of Hardware Architecture}

