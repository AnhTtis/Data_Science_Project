


\section{Runtime system}
\label{sec:runtime-system}

\subsection{Performance Model}
\label{subsec:performance-model}

The performance model predicts the execution time of the primitives for a given data sparsity. For analysis, we denote the two input matrices to a Computation Core as $\bm{X}\in \mathbb{R}^{m\times n}$ and $\bm{Y} \in \mathbb{R}^{n\times d}$ where $\bm{X}$ has the density  $\alpha_{\bm{X}}~(0 \leqslant \alpha_{\bm{X}} \leqslant 1)$  and $\bm{Y}$ has the density  $\alpha_{\bm{Y}}~(0 \leqslant \alpha_{\bm{Y}} \leqslant 1)$. 

\begin{table}[h]
\centering
\caption{Performance model}
\begin{adjustbox}{max width=0.48\textwidth}
\begin{tabular}{cccc}
\toprule
 & \textbf{GEMM} & \textbf{SpDMM} & \textbf{SPMM}\\
 \midrule \midrule
MACs per cycle & $p_{sys}^{2}$  & $p_{sys}^{2}/2$ &  $p_{sys}$  \\ \midrule
 \begin{tabular}[|c|]{@{}c@{}}  Execution time  \\ (cycles)\end{tabular} & $\frac{mnd}{p_{sys}^{2}}$  & \begin{tabular}[|c|]{@{}c@{}}  $\alpha_{\text{min}} \frac{2mnd}{p_{sys}^{2}}$, where \\ $\alpha_{\text{min}} = \text{Min}(\alpha_{\bm{X}}, \alpha_{\bm{Y}})$\end{tabular}  &  $\alpha_{\bm{X}}\alpha_{\bm{Y}} \frac{mnd}{p_{sys}}$  \\ \bottomrule
\end{tabular}
\end{adjustbox}
\end{table}



% \begin{wrapfigure}{r}{0.16\textwidth}
%   \vspace{-9pt}
%   \centering
%     \includegraphics[width=3cm]{pic/coverage.pdf}
%     \captionof{figure}{Domain of $\alpha_{min}$ and $\alpha_{max}$}
%     \label{fig:coverage}
%   \vspace{-10pt}
% \end{wrapfigure}
In the GEMM mode, the two input matrices are viewed as dense matrices and the Computation Core can execute $p_{sys}^{2}$ MACs per cycle. Therefore, the total execution time is $\frac{mnd}{p_{sys}^{2}}$ cycles.  In the SpDMM mode, the Computation Core can skip the zero elements in one input matrix and can execute $p_{sys}^{2}/2$ MACs per cycle. We view the input matrix with lower density as a sparse matrix and view another input matrix as a dense matrix. Therefore, the total execution time is $\alpha_{\text{min}} \frac{2mnd}{p_{sys}^{2}}$ cycles where $\alpha_{\text{min}} = \text{Min}(\alpha_{\bm{X}}, \alpha_{\bm{Y}})$.
In the SPMM mode, the Computation Core can skip the zero elements in both two input matrices and can execute $p_{sys}$ MACs per cycle. Therefore, the total execution time is $\alpha_{\bm{X}}\alpha_{\bm{Y}} \frac{mnd}{p_{sys}}$ cycles. In the state-of-the-art FPGA such as Xilinx Alveo U250, the dimension of a Computation Core $p_{sys}$ can be chosen to be $\geqslant 8$. We denote $\alpha_{max} = \text{Max}(\alpha_{\bm{X}}, \alpha_{\bm{Y}})$.
To summarize, for executing $\bm{Z} = \bm{X}\times \bm{Y}$ on a Computation Core, when $\alpha_{\text{min}} \geqslant \frac{1}{2}$, GEMM Mode has the least execution time; When $\alpha_{\text{min}} < \frac{1}{2}$ and $\alpha_{max} \geqslant \frac{2}{p_{sys}}$, SpDMM Mode has the least execution time; When $\alpha_{\text{min}} < \frac{1}{2}$ and $\alpha_{max} < \frac{2}{p_{sys}}$, SPMM Mode has the least execution time. The three cases are non-overlapping and cover all the points in the domain $0\leqslant \alpha_{min} \leqslant \alpha_{max} \leqslant 1$.
% \end{theorem} 


\subsection{Dynamic Kernel-to-primitive Mapping}
\label{subsec:dyna-k2p-mapping}

\begin{algorithm}
\caption{Dynamic kernel-to-primitive (K2P) mapping Algorithm for a computation task}
\label{alg:Kernal-to-mapping-Algorithm}
\begin{algorithmic}[1]
 \renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
 \Require $\{ \bm{X}_{i1}$, $\bm{X}_{i2}$, ..., $\bm{X}_{iK} \}$ and $\{\bm{Y}_{1j}$, $\bm{Y}_{2j}$, ..., $\bm{X}_{Kj} \}$;
%  \Ensure TargetPrimitive; The buffer to store $SA$: $B_{SA}$; The buffer to store $SB$: $B_{SB}$;
% \State{$\alpha_{SA} \leftarrow 0$, $\alpha_{SB} \leftarrow 0$ }
\For{$t=1$ to $K$}
    % \State{ $\alpha_{SA} += \alpha_{\bm{X}_{ik}}$ and $\alpha_{SB} += \alpha_{\bm{Y}_{kj}}$}
    \State{TargetPrimitive($\bm{X}_{it}$, $\bm{Y}_{tj}$) $\leftarrow$ NULL}
    \State{The buffers to store $\bm{X}_{it}$ and $\bm{Y}_{tj}$: $B_{\bm{X}_{it}}$, $B_{\bm{Y}_{tj}}$}
    % \State{The buffer to store $\bm{Y}_{tj}$: $B_{\bm{Y}_{tj}}$ $\leftarrow$ NULL}
    % \State{$\alpha_{\bm{X}_{ik}}$: The density of $\bm{X}_{ik}$}
    % \State{$\alpha_{\bm{Y}_{kj}}$: The density of $\bm{Y}_{kj}$}
    \State{$\alpha_{\text{min}} = \text{Min}(\alpha_{\bm{X}_{it}}, \alpha_{\bm{Y}_{tj}})$}   {\color{blue}\Comment{$\alpha_{\bm{X}_{it}}$: The density of $\bm{X}_{it}$}}
    \State{$\alpha_{\text{max}} = \text{Max}(\alpha_{\bm{X}_{it}}, \alpha_{\bm{Y}_{tj}})$}  {\color{blue}\Comment{$\alpha_{\bm{Y}_{tj}}$: The density of $\bm{Y}_{tj}$}}
    % \State{$\alpha_{\text{max}} = \text{Max}(\alpha_{\bm{X}}, \alpha_{\bm{Y}})$}
    \If{$\alpha_{\text{min}} = 0$}   {\color{blue}\Comment{Skip empty input matrix}}
        \State{Skip the multiplication of $\bm{X}_{it}$ and $\bm{Y}_{tj}$}
    \EndIf
    \If{$\alpha_{\text{min}} \geqslant  \frac{1}{2}$}
        \State{TargetPrimitive($\bm{X}_{it}$, $\bm{Y}_{tj}$)   $\leftarrow$ GEMM}
        \State{ $B_{\bm{X}_{it}}$ $\leftarrow$ BufferO and   $B_{\bm{Y}_{tj}}$ $\leftarrow$ BufferP}
        % \State{BSB $\leftarrow$ BufferP}
    \Else
        \If{$\alpha_{\text{max}} \geqslant \frac{2}{p_{sys}}$}
            \State{TargetPrimitive($\bm{X}_{it}$, $\bm{Y}_{tj}$)  $\leftarrow$ SpDMM}
            \State{$B_{\text{argmin}(\alpha_{M})}$ $\leftarrow$ BufferU, ($M\in \{\bm{X}_{it}, \bm{Y}_{tj}\}$)}
            \State{$B_{\text{argmax}(\alpha_{M})}$ $\leftarrow$ BufferO, ($M\in \{\bm{X}_{it}, \bm{Y}_{tj}\}$)}
            % \State{BSB $\leftarrow$ BufferO}
        \Else
            \State{TargetPrimitive($\bm{X}_{it}$, $\bm{Y}_{tj}$)  $\leftarrow$ SPMM}
            \State{$B_{\bm{X}_{it}}$ $\leftarrow$ BufferU and $B_{\bm{Y}_{tj}}$ $\leftarrow$ BufferO}
            % \State{$B_{\bm{Y}_{kj}}$ $\leftarrow$ BufferO}
            % \State{BSB $\leftarrow$ BufferO}
        \EndIf
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}


The Analyzer performs dynamic kernel-to-primitive (K2P) mapping for each computation task shown in Algorithm \ref{alg:Kernal-to-mapping-Algorithm}. For each pair of input matrices ($\bm{X}_{it}$, $\bm{Y}_{tj}$), the runtime system fetches their densities $\alpha_{\bm{X}_{it}}$ and $\alpha_{\bm{Y}_{tj}}$. Then, the Analyzer determines the target  primitive for multiplying $\bm{X}_{it}$ and $\bm{Y}_{tj}$, and also determines which buffers to store $\bm{X}_{it}$ and $\bm{Y}_{tj}$. The proposed dynamic K2P algorithm has the computation complexity $\mathcal{O}(K) = \mathcal{O}(\frac{|\mathcal{V}|}{N_{1}} + \frac{f_{1}}{N_{2}})$ for a computation task, which has small overhead compared with total computation complexity of a task $\mathcal{O}(|\mathcal{V}|*N_{2} + f_{1}*N_{2}^{2})$. See evaluation results in Section \ref{subsec:analysis-compiler-runtime}.
There are several benefits: (1) the proposed dynamic K2P mapping is fine-grained that for different data partitions, we can use different primitives to efficiently exploit the data sparsity in the input. (2) When the accelerator is executing kernel $l$, the runtime system can perform K2P mapping for kernel $l+1$. Therefore, the overhead of the runtime system can be hidden. 


% The overhead is minimal and is included in compiler-overhead (TABLE-VII).


\subsection{Task Scheduling}
\label{subsec:task-scheduling}



The scheduler performs scheduling of computation tasks (See Section \ref{sucsec:IR}) on the parallel Computation Cores as shown in Algorithm \ref{alg:scheduling-Scheduler}. The proposed task scheduling is a \emph{dynamic task scheduling strategy}. Each Computation Core maintains an interrupt interface to trigger the interrupt handling in the soft processor when the Computation Core is idle. Then, the soft processor assigns a task to the Computation Core. { { \color{red}}

\begin{algorithm}
\caption{Task scheduling}
\label{alg:scheduling-Scheduler}
\begin{algorithmic}[1]
 \renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
 \Require Intermediate Representation of the GNN model: IR; The number of computation kernels in the IR: $L$;
 \Ensure Output of the GNN model;
\For{$l=1$ to $L$}
    \For{each \emph{Task} in kernel $l$ of IR \textbf{parallel}}
        \If{there is an idle CC: $\text{CC}_{i}$}
            \State Assign this \emph{Task} to $\text{CC}_{i}$
            \State $\text{CC}_{i}$ executes this computation \emph{Task}
        \EndIf
    \EndFor 
    \State Wait until all the \emph{Tasks} in kernel $l$ are executed 
\EndFor
\end{algorithmic}
\end{algorithm}
 
\noindent { \textbf{Partition size ($N_{1}$, $N_{2}$)}:  The objectives of the data partitioning are to (1) enable fine-grained data sparsity exploitation, (2) exploit data locality, and (3) maximize resource utilization during dynamic task scheduling (Algorithm \ref{alg:scheduling-Scheduler}). 
Specifically, to maximize resource utilization that keeps all the Computation Cores busy, the compiler selects the partition configuration ($N_{1}, N_{2}$) such that there will be at least $\eta*N_{CC}$ ($\eta \geq 1$) tasks in each computation kernel assigned to $N_{CC}$ Computation Cores. $\eta$ is a factor that is determined empirically. Since different partitions can have different data sparsity leading to the different workloads of the tasks, small $\eta$ (e.g., $\eta = 1$) can potentially lead to long idle time for the Computation Cores with small workloads. Therefore, we set  $\eta = 4$ following state-of-the-art graph processing frameworks \cite{lakhotia2020gpop}.

To meet the above three objectives, we use a heuristic approach to determine the partition size as shown in Algorithm \ref{alg:partition-size}. As shown in Algorithm \ref{alg:scheme-aggregate} (line 2-3), the number of tasks of an Aggregate kernel is $\mathcal{T}_{a} = \frac{|\mathcal{V}|*f_{1}}{N_{1}*N_{2}}$. Also, as shown in Algorithm \ref{alg:scheme-Update} (line 2-3), the number of tasks of an Update kernel is $\mathcal{T}_{u} =  \frac{|\mathcal{V}|*f_{2}}{N_{2}*N_{2}}$. For simplicity, we use 
 $Q$ to denote the workload of a kernel (e.g., $Q=|\mathcal{V}|*f_{1}$ or $Q=|\mathcal{V}|*f_{2}$), and use $Q[k]$ to denote the workload of $k^{\text{th}}$ ($1 \leqslant k \leqslant L$) kernel. We use $p()$ to denote the function that determines the number of tasks of a kernel based on $Q$, $N_{1}$, and $N_{2}$. For example, $\mathcal{T}_{a} = p(Q, N_{1}, N_{2}) = \frac{Q}{N_{1}*N_{2}}$ and $\mathcal{T}_{u} = p(Q, N_{2}) = \frac{Q}{N_{2} * N_{2} }$. In line 9 and line 15 of Algorithm \ref{alg:partition-size}, the partition size of each kernel is constrained by $N_{it} = \text{min}(N_{it}, N_{\text{max}})$, where $\text{min}(N', N_{\text{max}})$ is the largest partition size such that $N_{it} \leqslant N'$ and $N_{it}  \leqslant N_{\text{max}}$. $N \leqslant N'$ ensures that there will be at least $\eta*N_{CC}$ tasks of a kernel for load balance. $N_{it}  \leqslant N_{\text{max}}$ ensures that the data partition does not exceed the size of on-chip memory. Lines 10 and 16  find a partition size $N_{1}$ and $N_{2}$ that can be used for all the kernels.  


% $N_{1}$ is the partition size for a feature aggregation kernel, and $N_{2}$ is the partition size for a feature update kernel (See Figure \ref{fig:Partition-scheme}). Algorithm \ref{alg:partition-size} can be applied to determine $N_{1}$ or $N_{2}$.

 % For example, to determine $N_{1}$ for a feature aggregation kernel, we assume the total work (the number of computation operations) of the kernel is $Q_{r}$, on-chip memory size is $S_{o}$. The number of tasks of a kernel is given by $p(Q_{r}, N_{1})$, where $p()$ is a function that determines the number of tasks of a kernel based on $Q_{r}$ and $N_{1}$. In our implementation, $p()$ is a monotonic function and $p(Q_{r}, N_{1}) = \mathcal{O}(\frac{Q_{r}}{{N_{1}}^{2}})$.  , therefore, (1) as $Q_{r}$ becomes larger, there will be more computation tasks in the kernel, and (2) as $N_{1}$ becomes larger, there will be less computation tasks in the kernel. Since a data partition (See Figure \ref{fig:Partition-scheme}, a data partition can be a block in graph adjacency matrix $\bm{A}$, a subfiber or a fiber in feature matrix $\bm{H}$, or a block in weight matrix $\bm{W}$) can not be larger than the size of the on-chip memory, we assume the maximum partition size is $N_{\text{max}} = g(S_{o})$, where $g()$ is the function that determines the maximum partition size based on the on-chip memory size $S_{o}$. Note that, theoretically there exists a partition size $N'$ such that $p(Q_{r}, N') = 4*N_{CC}$ as $p()$ is monotonic. Using Algorithm \ref{alg:partition-size}, the partition size is determined by $N_{1} = \text{min}(N', N_{\text{max}})$, where $\text{min}(N', N_{\text{max}})$ is the largest partition size such that $N_{1} \leqslant N'$ and $N_{1}  \leqslant N_{\text{max}}$. $N \leqslant N'$ ensures that there will be at least $4*N_{CC}$ tasks of a kernel for load balance. $N_{1}  \leqslant N_{\text{max}}$ ensures that the data partition will not exceed the size of on-chip memory. The process of determining $N_{2}$ is similar.
}


\begin{algorithm}
\caption{Data partitioning algorithm}
\label{alg:partition-size}
\begin{small}
\begin{algorithmic}[1]
 \renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\Require On-chip memory size $S_{o}$; Computation workload of each kernel: $\{Q[k]: 1 \leqslant k \leqslant L \}$; $p()$: function that determines the number of tasks of a kernel based on $Q$, $N_{1}$ and $N_{2}$; $g()$:  function that determines the maximum partition size based on the on-chip memory size $S_{o}$; $\eta$: factor for load balance.
\Ensure Partition size $N_{1}$, $N_{2}$; 
 \State $N_{\text{max}} \leftarrow  g(S_{o})$    {\color{blue}\Comment{Maximum partition size}}
  \State {\color{blue} //Objective: Maximize $N_{1}$ and $N_{2}$ to improve data locality}
 \State {\color{blue}//Constraint 1 (Maximize utilization): $\mathcal{T}_{a}$, $\mathcal{T}_{u}  \geq \eta *N_{CC}$}
 \State {\color{blue}//Constraint 2 (Memory capacity): $N_{1}$, $N_{2}  \leqslant N_{\text{max}}$ }
 \State {\color{brown} ======= Step 1: determine $N_{2}$ ========} 
 \State $N_{2} \leftarrow N_{\text{max}}$ 
 \For{each Update kernel: $k^{\text{th}}$ kernel}
     \State Choose largest $N'$ such that $\mathcal{T}_{u}[k] = p(Q[k], N') = \eta *N_{CC}$
     \State $N_{it} \leftarrow \text{min}(N', N_{\text{max}})$  %{\color{blue}\Comment{the largest partition size  such that $N_{it} \leqslant N'$ and $N_{it} \leqslant N_{\text{max}}$}}
     \State $N_{2} \leftarrow \text{min}(N_{it}, N_{2})$
 \EndFor
  \State {\color{brown} ======= Step 2: determine $N_{1}$ ========} 
 \State $N_{1} \leftarrow N_{\text{max}}$ 
 \For{each Aggregate kernel: $k^{\text{th}}$ kernel}
     \State Choose largest $N'$ such that $\mathcal{T}_{a}[k] = p(Q[k], N', N_{2}) =$  $~~~~~\eta *N_{CC}$
     \State $N_{it} \leftarrow \text{min}(N', N_{\text{max}})$  %{\color{blue}\Comment{the largest partition size such that $N_{it} \leqslant N'$ and $N_{it} \leqslant N_{\text{max}}$}}
     \State $N_{1} \leftarrow \text{min}(N_{it}, N_{1})$
 \EndFor
\end{algorithmic}
\end{small}
\end{algorithm}
