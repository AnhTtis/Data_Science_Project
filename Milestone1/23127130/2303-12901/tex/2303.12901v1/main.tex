\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{amsthm}
\usepackage{amsfonts,amsmath}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage[noend]{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{bm}
\usepackage{booktabs}
\usepackage{soul}
\usepackage{multirow}
%\usepackage[caption=false]{subfig}
\usepackage{subfig}
\usepackage[flushleft]{threeparttable}
\usepackage{adjustbox}
\usepackage{fancyhdr}
\usepackage[dvipsnames]{xcolor}
\usepackage[flushleft]{threeparttable}
\usepackage{tikz}
\usepackage{outlines}
\usepackage{balance}
\usepackage{tcolorbox}
\usepackage{wrapfig}
\usepackage{blindtext}
\usepackage{xcolor}
\usepackage{balance}
\usepackage{algorithm}
\usepackage{color}
\usepackage{listings}
\usepackage{color}
\usepackage{MnSymbol,wasysym}
\usepackage{marvosym}
\usepackage[switch]{lineno}
\usepackage{xcolor,colortbl}

\newtheorem{definition}{Definition}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,fill,inner sep=0.8pt] (char) {\textcolor{white}{#1}};}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    

\newcommand{\mc}[2]{\multicolumn{#1}{c}{#2}}
\definecolor{Gray}{gray}{0.85}
\definecolor{LightCyan}{rgb}{0.88,1,1}

\newtheorem{theorem}{Theorem}

\usepackage{algorithm}
\usepackage{algpseudocode} 

\usepackage{twoopt}
\usepackage{tikz}
\usetikzlibrary{fit}



\newcommand\tikzmark[1]{%
  \tikz[remember picture,overlay]\node[inner xsep=0pt] (#1) {};}
\newcommandtwoopt\Textbox[5][2.5cm][2cm]{%
\begin{tikzpicture}[remember picture,overlay]
  \coordinate (aux) at ([xshift=#1]#4);
  \node[inner ysep=5pt,yshift=0ex,draw=black,
    fit=(#3) (aux),baseline] 
    (box) {};
  \node[text width=#2,anchor=north east,
    font=\sffamily\footnotesize,align=right] 
    at (box.north east) {#5};
\end{tikzpicture}%
}
\captionsetup{compatibility=false}
\DeclareCaptionSubType*{algorithm}
\renewcommand\thesubalgorithm{\thetable\alph{subalgorithm}}
\DeclareCaptionLabelFormat{alglabel}{Alg.~#2}
    
\begin{document}

\title{Dynasparse: Accelerating GNN Inference through \underline{Dyna}mic \underline{Spars}ity \underline{E}xploitation
}


% \author{Anonymous Authors}
\author{
\IEEEauthorblockN{ Bingyi Zhang, Viktor Prasanna}
\IEEEauthorblockA{
    University of Southern California, 
    Los Angeles, California, USA\\
    bingyizh@usc.edu, prasanna@usc.edu }
}

\maketitle

\begin{abstract}
Graph Neural Network (GNN) inference is used in many real-world applications. 
Data sparsity in GNN inference, including sparsity in the input graph and the GNN model, offer opportunities to further speed up inference. Also, many pruning techniques have been proposed for model compression that increase the data sparsity of GNNs.

We propose Dynasparse, a comprehensive hardware-software codesign on FPGA to accelerate GNN inference through dynamic sparsity exploitation. For this, we decouple the GNN computation \emph{kernels} from the basic computation \emph{primitives}, and explore hardware-software codesign as follows:  1) \emph{Hardware design}: We propose a novel unified accelerator design on FPGA to efficiently execute various computation primitives. We develop a customized soft processor that is tightly coupled with the accelerator to execute a runtime system. Moreover, we develop  efficient hardware mechanisms to profile the data sparsity and perform on-the-fly data format transformation to prepare the input data for various computation primitives; 2)  \emph{Software design}: We develop a runtime system that works synergistically with the accelerator to perform dynamic kernel-to-primitive mapping based on data sparsity. We implement Dynasparse on a state-of-the-art FPGA platform, Xilinx Alveo U250, and evaluate the design using widely used GNN models (GCN, GraphSAGE, GIN and SGC). For the above GNN models and various input graphs,  the proposed accelerator and  dynamic kernel-to-primitive mapping reduces the inference latency by $3.73\times$ on the average compared with the static mapping strategies employed in the state-of-the-art GNN accelerators. Compared with state-of-the-art CPU (GPU) implementations, Dynasparse achieves up to $56.9\times$ ($2.37\times$)  speedup in end-to-end latency. Compared with state-of-the-art FPGA implementations, Dynasparse achieves $2.7\times$ speedup in accelerator execution latency.

\end{abstract}

\begin{IEEEkeywords}
Graph neural network, hardware-software codesign, hardware architecture, runtime system
\end{IEEEkeywords}



\input{1-introduction}
\input{2-background}
\input{3-problem-definition}
\input{4-overview}
\input{compiler}
\input{5-hardware}
\input{6-runtime-system}
\input{7-analaysis}
\input{8-experiment}
\input{9-conclusion}



\bibliographystyle{IEEEtran}
\bibliography{reference}

\end{document}
