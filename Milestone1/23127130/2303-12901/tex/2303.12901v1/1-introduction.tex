\section{Introduction}

Graph Neural Networks (GNNs) have achieved great success in many real-world applications, such as recommendation systems, social media, etc.
\emph{Low-latency GNN inference} is needed in many real-world applications, such as  traffic prediction \cite{zhong2022explainable}, scientific simulation \cite{hewes2021graph}, etc.

While many techniques \cite{yan2020hygcn, zhang2021boostgcn, zhang2020hardware, lin2022hp,  meng2021dynamap, zhang2023graphagile, sarkar2022flowgnn} have been proposed to accelerate GNN inference, no work has systematically studied the data sparsity in GNNs to reduce the inference latency. GNNs (\cite{kipf2016semi, hamilton2017inductive}) involve various computation kernels, where there are three types of data sparsity: (1) \emph{Sparsity of graph structure}: The graphs in the real-world applications are usually sparse, as most vertices have a small number of neighbors, (2) \emph{Sparsity of vertex features}: The vertex features have various sparsity depending on the property of the graphs, activation function, etc., and (3) \emph{Sparsity of GNN model}: The weight matrices in GNN models can also have data sparsity due to model pruning, etc. Moreover, the data sparsity can vary significantly based on the input graphs and GNN models (See Section \ref{subsec:GNN-sparsity}).  Prior works directly map the GNN kernels to computation primitives (See Section \ref{subsec:GNN-sparsity}), and do not consider data sparsity, leading to potentially suboptimal performance. 
% For example, \cite{yan2020hygcn} maps  feature aggregation to sparse-dense matrix multiplication (SpDMM) and maps feature transformation to dense-dense matrix multiplication (GEMM). \cite{geng2020awb} maps both feature aggregation and transformation to SpDMM. 

% including the sparsity of graph structure, the sparsity of vertex features, the sparsity of GNN model weights. 
% These computation kernels involve three kinds of data: (1) graph adjacency matrix which usually encodes the graph structure or connectivity, (2) vertex message which is a feature vector attached to each vertex, and (3) GNN model weights.  
% Previous work perform 
% There are three kinds of 
To efficiently utilize the data sparsity in GNN inference, we propose to decouple the GNN \emph{kernels} (feature aggregation and feature transformation) from the basic \emph{primitives} (dense-dense matrix multiplication (GEMM), sparse-dense matrix multiplication (SpDMM),  sparse-sparse matrix multiplication (SPMM)). A GNN kernel can be dynamically mapped to a primitive according to the sparsity of the data. 
However, there are several challenges: (1) While the sparsity of the graph structure and GNN model is known before the execution of inference (runtime), the sparsity of vertex features in the intermediate layers is  known only at runtime. Therefore, static (compile time) kernel-to-primitive mapping may not be optimal. (2) While the GNN kernels can be mapped to various primitives, these primitives have different data formats and layouts. Switching the data format and the data layout can incur large overhead during execution. (3) Different primitives have different computation patterns and memory access patterns. While general purpose processors are efficient for dense primitives (GEMM), their data path and cache organization are inefficient for sparse primitives (SpDMM, SPMM). 
% Therefore, exploiting the data sparsity in GNN inference is still challenging.

To address the above challenges, we propose Dynasparse, a hardware-software codesign, which can efficiently exploit the data sparsity in GNN inference. For the hardware design, we use Field Programmable Gate Array (FPGA) as the target hardware platform. The programmability of FPGA allows us to (1) develop a customized data path and memory organization to support various computation primitives, (2) develop efficient hardware mechanism for sparsity profiling and transformation of data format and data layout (Section \ref{subsec:data-format}), and (3) implement a lightweight and customized soft processor to perform dynamic kernel-to-primitive mapping at runtime. We summarize our main contributions as follows:

\begin{itemize}
    % \item We propose to decouple the GNN kernels from basic primitives. This enables dynamic sparsity exploitation in GNN inference.
    { 
    \item We develop a complete system on FPGA with the following innovations in hardware design: 
    \begin{itemize}
        \item a novel hardware architecture, named Agile Computation Module, consisting of multiple Computation Cores with flexible data path and memory organization that can execute various computation primitives, including GEMM, SpDMM and SPMM.
        \item an efficient hardware mechanism that supports fast sparsity profiling and data format/layout transformation.
        % \item 
        % \item a customized soft processor on FPGA that is tightly coupled with the accelerator to execute the runtime system.
    \end{itemize} 
    \item We propose a soft processor and develop a runtime system on the soft processor to enable dynamic sparsity exploitation, including:
    \begin{itemize}
        \item dynamic kernel-to-primitive (K2P) mapping strategy that automatically selects the optimal computation primitive for a given kernel based on an analytical performance model.
        \item task scheduling strategy that manages the execution of the computation primitives on the accelerator to achieve load balance across multiple Computation Cores in the FPGA accelerator.
    \end{itemize}}
    % (1) an analyzer that selects the optimal computation primitive for a given kernel based on an analytical performance model, and (2) a scheduler that manages the execution of the computation primitives on the accelerator so as to load balance.
    \item We implement the proposed codesign on a state-of-the-art FPGA, Xilinx Alveo U250. For various GNN models and input graphs,  the proposed accelerator and the dynamic kernel-to-primitive mapping reduce the inference latency by $3.73\times$ on the average compared with the static mapping strategies employed in the state-of-the-art GNN accelerators. { Compared with state-of-the-art CPU (GPU) implementations, Dynasparse achieves up to $56.9\times$ ($2.37\times$)  speedup in end-to-end latency. Compared with state-of-the-art FPGA implementations, Dynasparse achieves $2.7\times$ speedup in accelerator execution latency.}
\end{itemize}


