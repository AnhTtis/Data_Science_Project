



\section{Evaluation Results}
\label{sec:Evaluation-Results}

 {This Section is organized as follows: In Section \ref{subsec:Impact-of-K2P-mapping}, we measure of the impact of the dynamic K2P mapping strategy. In Section \ref{subsec:analysis-compiler-runtime}, we analyze the overhead of compilation and runtime system. In Section \ref{subsec:comparison-with-SOTA}, we compare our work with the state-of-the-art implementations.}

\subsection{Benchmarks and Baselines}

\noindent \textbf{Benchmarks}: We evaluate Dynasparse on four widely used GNN models -- GCN \cite{kipf2016semi}, GraphSAGE (SAGE) \cite{hamilton2017inductive}, GIN \cite{xu2018powerful}, and SGC \cite{wu2019simplifying}. 
Figure \ref{fig:IR-GNN-layer} shows the IR of various GNN layers. 
We evaluate the design on six widely used graph datasets -- Cora (CO) \cite{kipf2016semi}, CiteSeer (CI) \cite{kipf2016semi}, PubMed (PU) \cite{kipf2016semi}, Flickr (FL) \cite{zeng2019graphsaint}, NELL (NE) \cite{yang2016revisiting}, Reddit (RE) \cite{hamilton2017inductive}. We evaluate the 2-layer GNN models used in \cite{kipf2016semi, geng2020awb, yan2020hygcn, zhang2021boostgcn}, where the hidden dimension for CO, CI and PU is set as 16, and the hidden dimension for FL, NE and RE is set as 128.

\vspace{0.1cm}
\noindent \textbf{Baselines}: We compare our work with the state-of-the-art CPU (AMD Ryzen 3990x), GPU (Nvidia RTX3090) and GNN accelerators  HyGCN \cite{yan2020hygcn}, BoostGCN \cite{zhang2021boostgcn}. The details of the platforms are shown in Table \ref{tab:platform-specifications}.


% \begin{table}[!ht]
% \centering
% \vspace{-0.2cm}
% \caption{Specifications of platforms }
% \vspace{-0.2cm}
% \begin{threeparttable}

% \begin{adjustbox}{max width=0.47\textwidth}
% \begin{tabular}{c|ccc}
%  \toprule
%           %\\ \midrule
% \textbf{Platforms} & \begin{tabular}[|c|]{@{}c@{}} CPU \\  AMD Ryzen 3990x \end{tabular}  & \begin{tabular}[|c|]{@{}c@{}} GPU \\  Nvidia RTX3090 \end{tabular} & \begin{tabular}[|c|]{@{}c@{}} FPGA \\   Alveo U250 \end{tabular}  \\ 
% \midrule \midrule 
%  {Technology}  & TSMC 7 nm   & TSMC 7 nm & TSMC 16 nm  \\ 
% {Frequency} & 2.90 GHz  & 1.7 GHz & 300 MHz 
%       \\ 
% {Peak Performance}& 3.7 TFLOPS & 36 TFLOPS & 0.72 TFLOPS  \\ 
% {On-chip Memory}& 256 MB L3 cache & 6 MB L2 cache & 54 MB  \\
% {Memory Bandwidth}& 107 GB/s & 15.6 GB/s (PCIe) & 15.6 GB/s (PCIe)\\ \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{threeparttable}
% \label{tab:platform-specifications}
% \vspace{-0.2cm}
% \end{table}



\begin{figure}[h]
     \centering
     \includegraphics[width=8.5cm]{pic/IR-of-GNN-model.pdf}
     \caption{The IR of various GNN layers}
     \label{fig:IR-GNN-layer}
\end{figure}


\begin{table}[!ht]
\centering
\caption{Specifications of platforms }
\begin{threeparttable}
\begin{adjustbox}{max width=0.48\textwidth}
\begin{tabular}{c|ccccc}
 \toprule
           %\\ \midrule
& CPU & GPU  & \cite{yan2020hygcn} & \cite{zhang2021boostgcn}  & Dynasparse  \\  \midrule
\midrule 
Platform  &  \begin{tabular}[|c|]{@{}c@{}} Ryzen \\ 3990x\end{tabular} & \begin{tabular}[|c|]{@{}c@{}} Nvidia\\ RTX3090 \end{tabular} & ASIC   & \begin{tabular}[|c|]{@{}c@{}}  Stratix 10 \\ GX\end{tabular}   & \begin{tabular}[|c|]{@{}c@{}} Alveo \\ U250 \end{tabular}  \\  
 \rowcolor{LightCyan} {Technology}  & \begin{tabular}[|c|]{@{}c@{}} TSMC \\ 7 nm  \end{tabular}  & \begin{tabular}[|c|]{@{}c@{}} TSMC \\ 7nm  \end{tabular} & \begin{tabular}[|c|]{@{}c@{}} TSMC \\12 nm  \end{tabular}& \begin{tabular}[|c|]{@{}c@{}} Intel \\ 14 nm \end{tabular} & \begin{tabular}[|c|]{@{}c@{}}  TSMC \\ 16 nm \end{tabular}\\ 
{Frequency} & 2.90 GHz  & 1.7 GHz & 1 GHz  & 250 MHz & 250 MHz \\  
 \rowcolor{LightCyan}
 \begin{tabular}[|c|]{@{}c@{}}  Peak Performance \\ (TFLOPS)
 \end{tabular}& 3.7 & 36  &  4.608   & 0.64 & 0.512  \\ 
{On-chip Memory}& 256 MB & 6 MB & 35.8 MB&  32 MB & 45 MB   \\ 
 \rowcolor{LightCyan}
{Memory Bandwidth}& 107 GB/s & 936.2 GB/s  & 256 GB/s &  77 GB/s  & 77 GB/s \\ \bottomrule
\end{tabular}
\end{adjustbox}
\end{threeparttable}
\label{tab:platform-specifications}
\end{table}

% \begin{table}[!ht]
% \centering
%   \vspace{-0.3cm}
% \caption{Specifications of platforms }
% \vspace{-0.2cm}
% \begin{threeparttable}
% \begin{adjustbox}{max width=0.48\textwidth}
% \begin{tabular}{c|cccccc}
%  \toprule
%           %\\ \midrule
% & CPU & GPU  & \cite{yan2020hygcn} & \cite{geng2020awb} &  \cite{zhang2021boostgcn} & Dynasparse  \\  \midrule
% \midrule 
% Platform  &  \begin{tabular}[|c|]{@{}c@{}} Ryzen \\ 3990x\end{tabular} & \begin{tabular}[|c|]{@{}c@{}} Nvidia\\ RTX3090 \end{tabular} & ASIC &  \begin{tabular}[|c|]{@{}c@{}} Stratix 10 \\ SX\end{tabular}  & \begin{tabular}[|c|]{@{}c@{}}  Stratix 10 \\ GX\end{tabular}   & \begin{tabular}[|c|]{@{}c@{}} Alveo \\ U250 \end{tabular}  \\  
%  \rowcolor{LightCyan} {Technology}  & \begin{tabular}[|c|]{@{}c@{}} TSMC \\ 7 nm  \end{tabular}  & \begin{tabular}[|c|]{@{}c@{}} TSMC \\ 7nm  \end{tabular} & \begin{tabular}[|c|]{@{}c@{}} TSMC \\12 nm  \end{tabular}& \begin{tabular}[|c|]{@{}c@{}} Intel \\ 14 nm \end{tabular} & \begin{tabular}[|c|]{@{}c@{}} Intel \\ 14 nm  \end{tabular}& \begin{tabular}[|c|]{@{}c@{}}  TSMC \\ 16 nm \end{tabular}\\ 
% {Frequency} & 2.90 GHz  & 1.7 GHz & 1 GHz  & 330 MHz  & 250 MHz & 250 MHz \\  
%  \rowcolor{LightCyan}
%  \begin{tabular}[|c|]{@{}c@{}}  Peak Performance \\ (TFLOPS)
%  \end{tabular}& 3.7 & 36  &  4.608 & 1.35  & 0.64 & 0.512  \\ 
% {On-chip Memory}& 256 MB & 6 MB & 35.8 MB&  22MB & 32 MB & 45 MB   \\ 
%  \rowcolor{LightCyan}
% {Memory Bandwidth}& 107 GB/s & 936.2 GB/s  & 256 GB/s & 57.3 GB/s & 77 GB/s  & 77 GB/s \\ \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{threeparttable}
% \vspace{-0.2cm}
% \label{tab:platform-specifications}
% \end{table}

\begin{table}[ht]
\centering
\caption{Dataset Statistics}
\begin{adjustbox}{max width=0.49\textwidth}
\begin{tabular}{ccccccc}
\toprule
\textbf{Dataset}   & \textbf{Vertices} & \textbf{Edges} & \textbf{Features} & \textbf{Classes} &  \begin{tabular}[|c|]{@{}c@{}}  \textbf{Density of} \\ $\bm{A}$ \end{tabular} & \begin{tabular}[|c|]{@{}c@{}}  \textbf{Density of} \\ $\bm{H}^{0}$ \end{tabular} \\
\midrule
\midrule
 CI& 3327 & 4732& 3703& 6 &  $0.08\%$ & $0.85\%$\\
 \rowcolor{LightCyan}
 CO & 2708 &  5429& 1433 & 7 & $0.14\%$ & $1.27\%$ \\
 PU& 19717 &  44338 & 500 & 3 & $0.02\%$ & $10.0\%$ \\
 \rowcolor{LightCyan}
 FL & 89,250 &    899,756 & 500 & 7 & $0.01\%$ & $46.4\%$ \\
 NE& 65,755 &  251,550 & 61,278 & 186 & $0.0058\%$ & $0.01\%$\\
 \rowcolor{LightCyan}
 RE& 232,965  & $11\times 10^{7}$ & 602 & 41 & $0.21\%$ & $100.0\%$ \\
\bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:datasets-statistics}
\end{table}


\noindent \textbf{Performance metric}: Following the convention in \cite{geng2020awb, yan2020hygcn, zhang2021boostgcn}, we use \emph{latency} ({accelerator execution latency}) as the  metric which is the duration from the time when the accelerator starts to execute the optimized IR to the time all the inference results are obtained. The preprocessing time by the compiler is not included in the latency, because (1) the overhead of generating the optimized IR is usually small (See Section \ref{subsec:analysis-compiler-runtime}) and the optimized IR can be stored and reused if the sparsity of the input graph and GNN model changes, (2)  we follow the same convention in \cite{geng2020awb, yan2020hygcn, zhang2021boostgcn} for a fair comparison.



% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
\centering
\caption{The latency (ms) on the unpruned GNN models}
\begin{adjustbox}{max width=0.48\textwidth}
\begin{tabular}{llllllll}
\toprule
                  &  & CI & CO & PU & FL & NE & RE \\ \midrule \midrule
\multirow{3}{*}{GCN \cite{kipf2016semi}} & 
        \texttt{S1} & 31E-1& 9.6E-1&2.7E-1& 10E0&83E2&9.3E1\\
        & \texttt{S2} & 8.9E-3 & 5.6E-3 & 7.1E-3 & 9.9E0 & 5.4E0 & 12E1\\
        & \texttt{Dynamic} & 7.7E-3& 4.7E-3&6.3E-2& 8.8E0&2.9E0&8.4E1\\ \cmidrule{2-8}
        & SO-S1 &$41.3\times$&$21.5\times$&$4.29\times$&$1.13\times$&$278\times$&   $1.10\times$ \\
        & SO-S2 &$1.15\times$&$1.19\times$&$1.12\times$&$1.11\times$&$1.82\times$&   $1.42\times$ \\  \midrule
\multirow{3}{*}{SAGE \cite{hamilton2017inductive}} & \texttt{S1}       
        &74E-2&25E-2&65E-2&20E0&17E2&334E0\\
       & \texttt{S2} &75E-2&25E-2&69E-2&28E0&17E2&389E0\\
        &  \texttt{Dynamic}&33E-2&11E-2&42E-2&19E0&83E1&331E0\\\cmidrule{2-8}
        &  SO-S1 & $1.93\times$ & $1.72\times$ & $1.56\times$ & $1.02\times$ & $2.05\times$ & $1.01\times$ \\ 
        &  SO-S2 & $1.94\times$ & $1.73\times$ & $1.65\times$ & $1.41\times$ & $2.05\times$ & $1.17\times$ \\ \midrule
\multirow{3}{*}{GIN \cite{xu2018powerful}} &
        \texttt{S1} &4.3E-1&1.5E-1&4.1E-1&1.3E1&8.8E2&3.1E2 \\
      & \texttt{S2} &7.4E-1&2.4E-1&6.5E-1&2.0E1&1.7E3&3.4E2\\
      & \texttt{Dynamic}  & 3.3E-1 & 1.1E-1 & 3.7E-1 & 1.2E1 & 8.3E2 & 2.7E2 \\\cmidrule{2-8}
        & SO-S1  & $1.30 \times$ & $1.40\times$  & $1.11\times$  & $1.13\times$ & $1.06\times$ & $1.15\times$   \\
        & SO-S2 & $2.26 \times$ & $2.31\times$  & $1.76\times$  & $1.73\times$ & $2.05\times$ & $1.25\times$   \\ \midrule
\multirow{3}{*}{SGC \cite{wu2019simplifying}} 
        & \texttt{S1}  & 5.3E-1 & 2.0E-1 & 5.5E-1 & 1.29E-1 & 9.33E2 & 5.7E2 \\
        & \texttt{S2}  & 8.5E-1 & 3.0E-1 & 7.9E-1 & 2.18E-1 & 1.77E3 & 6.0E2 \\
        & \texttt{Dynamic}   & 4.3E-1 & 1.5E-1 & 5.1E-1 & 1.27E-1 & 8.83E2 & 5.0E2 \\\cmidrule{2-8}
        & SO-S1  & $1.23 \times$ & $1.27 \times$ &  $1.08 \times$ & $1.02 \times$ & $1.06 \times$ & $1.13 \times$   \\
        & SO-S2 & $1.95 \times$ & $1.91 \times$ &  $1.55 \times$ & $1.72 \times$ & $1.99 \times$ & $1.19 \times$   \\
                  \bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:results-unpruned-GNN-model}
\end{table}






\subsection{Impact of Dynamic K2P Mapping Strategy}
\label{subsec:Impact-of-K2P-mapping}

To demonstrate the impact of the proposed dynamic K2P mapping strategy, we execute the  following three K2P mapping strategies on our proposed accelerator: 
\begin{itemize}
    \item $\verb|Static-1|$ ($\verb|S1|$): It is used in \cite{yan2020hygcn, zhang2021boostgcn} that Aggregate() is mapped to SpMM  and Update() is mapped to GEMM.
    \item $\verb|Static-2|$ ($\verb|S2|$): It is used in \cite{geng2020awb} that both the Aggregate() and Update() are mapped to SpDMM. For Aggregate($\bm{A}$, $\bm{H}$), it views $\bm{A}$ as sparse matrix and views $\bm{H}$ as dense matrix. For Update($\bm{H}$, $\bm{W}$), it views $\bm{H}$ as sparse matrix and views $\bm{W}$ as dense matrix.
    % where graph adjacency matrix $\bm{A}$ and feature matrix $\bm{H}$ are viewed as sparse matrices, and weight matrix  $\bm{W}$ is viewed as dense matrix. 
    \item $\verb|Dynamic|$:  It is our proposed dynamic K2P mapping strategy (Algorithm \ref{alg:Kernal-to-mapping-Algorithm}).
\end{itemize}


We use SO-S1 to denote the speedup of  \texttt{Dynamic} over \texttt{S1}. We use  SO-S2 to denote the speedup of  \texttt{Dynamic} over \texttt{S2}.

\vspace{0.1cm}
\noindent \textbf{Evaluation on unpruned GNN models}: We evaluate the above three strategies using unpruned GNN models where all the weight matrices have density $100\%$. The results are shown in Table \ref{tab:results-unpruned-GNN-model}. 
Compared with $\verb|S1|$ and $\verb|S2|$, $\verb|Dynamic|$ achieves $2.13 \times$ and $1.59 \times$ speedup on the average (geometric  mean), respectively.  $\verb|Dynamic|$ achieves limited speedup over $\verb|S2|$ on GCN because (1) for the first Update($\bm{H}^{0}$, $\bm{W}^{1}$) kernel of GCN, there is high data sparsity in $\bm{H}^{0}$ of CI, CO, PU and NE (See Table \ref{tab:datasets-statistics}), (2) both $\verb|Dynamic|$ and $\verb|S2|$ can exploit the sparsity of feature matrix $\bm{H}^{0}$ while $\verb|S1|$ does not exploit the sparsity of $\bm{H}^{0}$. As the first Update($\bm{H}^{0}$, $\bm{W}^{1}$) kernel of GCN consumes majority of the execution time,  $\verb|Dynamic|$ achieves very large speedup over $\verb|S1|$ on GCN. Since the weight matrices have density $100\%$, both $\verb|Dynamic|$ and $\verb|S2|$ map Update($\bm{H}^{0}$, $\bm{W}^{1}$) to SpDMM (for  CI, CO, PU and NE), leading to similar performance of $\verb|Dynamic|$ and $\verb|S2|$ on GCN.


% Note that there is limited speedup in SGC model. Through profiling, we observe that the Update() of first SGC layer takes more $90\%$ execution time and there is no data sparsity in the Update(). Therefore, the propsoed dynamic K2P strategy does not leads to significant speedup for SGC. 

\vspace{0.1cm}
\noindent \textbf{Evaluation on pruned GNN models}: We evaluate the three strategies using the pruned GNN models \cite{rahman2022triple} where the weight matrices are pruned to have various sparsity.  
% Note that we do not propose any GNN pruning techniques. Instead, given a pruned GNN model, we execute it  on Dynasparse. 
Figures \ref{fig:model-sparse} and  \ref{fig:speedup-over-S2}   show the speedup of  \texttt{Dynamic} over \texttt{S1\&2}. For evaluation, all the weight matrices in a GNN model are pruned to have the same sparsity, and the sparsity of weights in Figures \ref{fig:model-sparse}\&\ref{fig:speedup-over-S2} means the average sparsity of all the weight matrices in a GNN model. Table \ref{tab:average-speedup} summarizes the average (geometric  mean) speedup under various sparsity of weight matrices. The achieved speedup over $\verb|S1|$ is because  $\verb|S1|$ cannot exploit the data sparsity in feature matrices and weight matrices. The achieved speedup over $\verb|S2|$ is due to (1) when there is limited data sparsity (density $<50\%$) in Update(), executing  Update() using SpDMM primitive is not efficient. (2) In Aggregate(), $\verb|S2|$ does not exploit data sparsity in feature matrix $\bm{H}$ since $\verb|S2|$ views $\bm{H}$ as a dense matrix.

\begin{figure}[h]
     \centering
     \includegraphics[width=8.5cm]{pic/static-1-overall-v1.pdf} 
     \caption{Speedup of \texttt{Dynamic} over \texttt{S1} when there are various sparsity (\%) in the GNN weight matrices (X-axis)}
     \label{fig:model-sparse}
 \end{figure}
 
 
 \begin{figure}[h]
     \centering
     \includegraphics[width=8.5cm]{pic/static-2-overall-v1.pdf} 
     \caption{Speedup of \texttt{Dynamic} over \texttt{S2} when there are various sparsity (\%) in the GNN weight matrices (X-axis)}
     \label{fig:speedup-over-S2}
 \end{figure}

\begin{table}[H]
\centering
\caption{Average speedup  (geometric  mean)}
\begin{adjustbox}{max width=0.48\textwidth}
\begin{tabular}{ccccc}
\toprule
\textbf{Sparsity of weight matrices} & $<50\%$ & $50\%-70\%$ & $70\%-90\%$ &  $>90\%$  \\ \midrule \midrule
\textbf{SO-S1} & $2.16\times$ & $4.36\times$ &  $10.77\times$ & $15.96\times$  \\ \midrule
\textbf{SO-S2} & $1.38\times$ & $1.64\times$ &  $2.11\times$ & $5.03\times$  \\ \bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:average-speedup}
\end{table}

{
In conclusion, the proposed dynamic K2P mapping strategy leads to lower accelerator execution latency compared with the static mapping strategies. Using dynamic K2P mapping strategy, the execution latency reduces as the data sparsity increases. }


% \texttt{Dynamic} achieves $1.3\times$, $4.43\times$, $5.32\times$ and $39.3\times$ speedup over \texttt{S1} when the sparsity of weight matrices are $<50\%$, $50\%-70\%$, $70\%-90\%$ and  $>90\%$, respectively.  \texttt{Dynamic} achieves $1.42\times$, $1.83\times$, $2.36\times$ and $7.03\times$ speedup over \texttt{S2} when the sparsity of weight matrices are $<50\%$, $50\%-70\%$, $70\%-90\%$ and  $>90\%$, respectively.  


% When sparsity of the weight matrices is $>90\%$, there will also be increased sparsity in the feature matrices.  
% Therefore, Dynasparse achieves high speedup when the sparsity of weight matrices is $>90\%$. The evaluation results show that Dynasparse can efficiently accelerate pruned GNN models through exploiting the data sparsity. For example, the pruned GNN models in \cite{rahman2022triple} have weight sparsity $>85\%$ where the acceleration by Dynasparse will be significant.



% \subsection{Break down analysis}


\subsection{Analysis of Compiler and Runtime System}
\label{subsec:analysis-compiler-runtime}

\begin{table}[h]
\centering
\caption{The preprocessing time of the compiler (ms)}
\begin{adjustbox}{max width=0.48\textwidth}
\begin{tabular}{c|cccccc}
\toprule
 & CI & CO & PU & FL & NE & RE \\ \midrule \midrule
GCN & 2.5E-1 & 2.2E-2  & 5.7E-1 & 2.68E0 & 1.70E0 & 5.1E1\\ 
GraphSAGE  & 2.3E-1 & 2.6E-1 & 5.9E-1 & 2.58E0 & 1.65E0 & 4.9E1 \\  
GIN &2.4E-1& 2.6E-3&5.8E-1& 2.69E0&1.71E0&5.0E1 \\
SGC &2.3E-1& 2.4E-3&6.1E-1& 2.74E0&1.73E0&5.2E1 \\
 \bottomrule
\end{tabular}
\label{tab:overhead-of-compiler}
\end{adjustbox}
\end{table}



\noindent \textbf{Overhead of the compilation/preprocessing}: Table \ref{tab:overhead-of-compiler} shows the overhead
of the compiler on the host processor (Intel Xeon 5120). The processing time includes the overheads of generating IR, data partitioning, and preprocessing of data sparsity. Compared with design automation framework \cite{liang2020deepburning} which needs to regenerate FPGA accelerator if the graph or GNN model changes, the overhead of the compiler in our design is small.



\noindent \textbf{Overhead of the Runtime System}: We measure the overhead of the runtime system, which is the execution time of dynamic K2P mapping on the soft processor. See Figure \ref{fig:overhead-of-runtime}.  on the average, the Runtime System takes $6.8\%$ of the total execution time and is hidden by the task scheduling (Section \ref{subsec:task-scheduling}). For the pruned GNN models, as the densities of weight matrices decrease, the overhead of the Runtime System will decrease since there will be more empty data partitions skipped by the runtime system (Algorithm \ref{alg:Kernal-to-mapping-Algorithm}).

 
\begin{figure}[h]
     \centering
      \includegraphics[width= 8cm]{pic/overhead-of-runtime.pdf}
    %  \includegraphics[width=3.9cm]{pic/pyg-cpu.pdf} 
    %  \includegraphics[width=3.9cm]{pic/pyg-gpu.pdf} \\
    %  \includegraphics[width=3.9cm]{pic/dgl-cpu.pdf} 
    %  \includegraphics[width=3.9cm]{pic/dgl-gpu.pdf} \\
     \caption{Overhead of runtime system on unpruned GNNs}
     
     \label{fig:overhead-of-runtime}
\end{figure}


\subsection{Comparison with the State-of-the-art}
\label{subsec:comparison-with-SOTA}

\begin{figure}[h]
     \centering
      \includegraphics[width=8.5cm]{pic/speedup-cpu-gpu.pdf}
    %  \includegraphics[width=3.9cm]{pic/pyg-cpu.pdf} 
    %  \includegraphics[width=3.9cm]{pic/pyg-gpu.pdf} \\
    %  \includegraphics[width=3.9cm]{pic/dgl-cpu.pdf} 
    %  \includegraphics[width=3.9cm]{pic/dgl-gpu.pdf} \\
      \caption{Speedup over the CPU and GPU platforms (Some results are not shown due to out of memory on CPU/GPU)}
     \label{fig:compare-CPU-GPU}
 \end{figure}
 


 
\noindent \textbf{Comparison with CPU/GPU}: We execute the state-of-the-art GNN frameworks -- Pytorch Geometric ({PyG, version 1.11.0}) and Deep Graph library ({DGL, version 0.8.0post2}) on CPU and GPU platforms (Table \ref{tab:platform-specifications}).  The evaluation results are shown in Figure \ref{fig:compare-CPU-GPU}. We execute the same unpruned GNN models on CPU, GPU and Dynasparse for a fair comparison. Dynasparse achieves $306\times$, $16.4\times$, $141.9\times$ and $35\times$ speedup compared with PyG-CPU, PyG-GPU, DGL-CPU and DGL-GPU, respectively. Note the CPU and GPU have $7.2\times$ and $70\times$ higher peak performance than Dynasparse. The achieved speedup is because Dynasparse can efficiently exploit the data sparsity in graph structure, vertex features and weight matrices. In contrast,  PyG and DGL on CPU and GPU only exploit the sparsity in the graph structure. {Moreover, Dynasparse exploits FPGA-specific optimizations: (1) customized data-path with Index/Data Shuffle Networks to handle the irregular memory access pattern of GNNs, (2) customized on-chip memory management for exploiting data locality, (3) dedicated hardware modules for sparsity profiling and layout/format transformation; The proposed double buffering hides their overheads, and (4) lightweight soft processor interacting with Computation Cores with extreme low latency for dynamic kernel-to-primitive mapping.}


\begin{table}[h]
\centering
\caption{Comparison of latency with the state-of-the-art GNN accelerators (using GCN model)}
\begin{adjustbox}{max width=0.48\textwidth}
\begin{threeparttable}[t]
\begin{tabular}{c|cccccc|c}
\toprule
 & CI & CO & PU & FL & NE & RE & \begin{tabular}[|c|]{@{}c@{}}  Peak Perf. \\ (TFLOPS) 
 \end{tabular}\\ \midrule \midrule
BoostGCN \cite{zhang2021boostgcn} & 1.9E-2 & 2.5E-2  & 1.6E-1 & 4.0E1 & N/A$\star$ & 1.9E2 & 1.35\\ 
HyGCN \cite{yan2020hygcn} & 2.1E-2 & 3E-1 & 6.4E1 & N/A & N/A & 2.9E2 & 4.6 \\  
% AWB-GCN \cite{geng2020awb} & 3.7E-3 & 4.0E-3 & 3E-2 & N/A & 3.2E0 & 4.9E1 & 1.35 \\ 
\midrule
Dynasparse &7.7E-3& 4.7E-3&6.3E-2& 8.8E0&2.9E0&1.0E2 &0.512 \\
 \bottomrule
\end{tabular}
\begin{tablenotes}
\item[] $\star$ N/A: not available.
 \end{tablenotes}
\end{threeparttable}
\label{tab:comparison-sOTA}
\end{adjustbox}
\end{table}

\noindent \textbf{Comparison with GNN accelerators}: Table \ref{tab:comparison-sOTA} shows the comparison of the latency with the state-of-the-art GNN accelerators,  {  which do not require regenerating accelerator if the data sparsity changes.} All the accelerators execute the same unpruned GCN models and graph datasets. Dynasparse achieves $2.7\times$, $171\times$  speedup on the average than BoostGCN and HyGCN,  respectively. The platforms used in BoostGCN and HyGCN have $1.25\times$, $9\times$ higher peak performance than Dynasparse. The achieved speedup is because Dynasparse can efficiently exploit data sparsity in vertex features. We expect to achieve higher speedup when executing the same pruned GNN models, since \cite{zhang2021boostgcn, yan2020hygcn} do not exploit the sparsity in weights.

 {
\vspace{0.1cm}
\noindent \textbf{Discussion of preprocessing and data communication overheads}: We define the \emph{end-to-end latency} as the sum of \textbf{(1)} the overhead of compilation/preprocessing (Section \ref{subsec:analysis-compiler-runtime}), \textbf{(2)} the overhead of CPU-FPGA data movement (moving the processed input graph, processed GNN model, and optimized IR from the host memory to FPGA external memory), and \textbf{(3)} execution latency of the accelerator. With respect to end-to-end latency, Dynasparse still achieves $56.9\times$, $2.37\times$, $16.3\times$, $1.37\times$ speedup on the unpruned GNN models compared with PyG-CPU, PyG-GPU, DGL-CPU, and DGL-GPU, respectively. 
The preprocessing overhead, data movement overhead, and execution latency contribute to $43.1\%$, $27.2\%$, $27.6\%$ of the total end-to-end latency on the average. The major overhead in preprocessing is data partitioning that reorganizes the input data into data partitions. It can be reduced by multi-threading and increasing the host memory bandwidth.

Note that the CPU-FPGA data movement overhead depends on the PCIe bandwidth. The sustained PCIe bandwidth of the Alveo U250 FPGA board is around $11.2$ GB/s while the baseline GPU (Nvidia RTX3090) has PCIe bandwidth of $31.5$ GB/s. The overhead of CPU-FPGA data movement can be reduced by exploiting state-of-the-art CPU-FPGA interconnection techniques (offered by FPGA vendors), such as PCIe 5.0. Since prior GNN accelerators \cite{zhang2021boostgcn, yan2020hygcn} do not include their preprocessing overheads (data partitioning and CPU-FPGA data movement), in Table \ref{tab:comparison-sOTA}, we only compare the accelerator execution latency with \cite{zhang2021boostgcn, yan2020hygcn} for a fair comparison. 
}