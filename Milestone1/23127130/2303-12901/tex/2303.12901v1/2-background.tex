
\section{Background}

\subsection{Graph Neural Network}

 GNNs \cite{kipf2016semi, hamilton2017inductive} are proposed for representation learning on graphs $  \mathcal{G}(\mathcal{V},\mathcal{E})$, and 
 follow the  message-passing paradigm (Algorithm \ref{alg:GNN-computation-abstraction}) in which the vertices recursively aggregate information from the neighbors. $\bm{h}^{L}_{v}$ denotes the last-layer embedding of the target vertex $v$. The Update() is usually a Multi-Layer Perceptron that transforms the vertex features.  
%  The outputs of GNN are the embeddings of target vertices that 
An element-wise activation function is applied to the feature vectors after the Aggregate() and Update() in each layer. 
% In some GNN models (e.g., GCN \cite{kipf2016semi}),  Update() can be performed before Aggregate().
 The output embedding $\bm{h}^{L}_{v}$
 can be used for many downstream tasks, such as node classification (\cite{hamilton2017inductive,kipf2016semi}), link prediction, etc. GCN \cite{kipf2016semi}, GraphSAGE \cite{hamilton2017inductive}, GIN \cite{xu2018powerful}, and SGC \cite{wu2019simplifying} are some representative GNN models. Table \ref{tab:notations} summarizes the notations used in this paper.
 
% \noindent \textbf{GCN}: GCN \cite{kipf2016semi} has the layer definition: 
% \begin{equation}
%     \begin{split}
%         \bm{a}_{i}^{l} & = \text{Sum}\left( \left\{ \alpha_{ji} \cdot \bm{h}_{j}^{l-1}:j\in \mathcal{N}(i)\cup  \{i\}\right\}\right)\\
%         \bm{z}_{i}^{l} & = \bm{a}_{i}^{l}\bm{W}^{l}, \text{ } \bm{h}_{i}^{l}  = \text{ReLU}(\bm{z}_{i}^{l}) 
%     \end{split}
%     \label{label:gcn}
% \end{equation}
% where $l$ denotes $l^{\text{th}}$ layer, $\alpha_{ji}=\frac{1}{ \sqrt{D(j)\cdot D(i)}}$ ($D(j)$ is the degree of $v_{j}$). Using matrix representation, a GCN layer can be expressed as: 
% $\bm{H}^{l} = \text{ReLU}(\bm{A}\bm{H}^{l-1}\bm{W}^{l})$, where $\bm{A}$ denotes graph adjacency matrix, $\bm{H}$ denotes vertex feature matrix, and $\bm{W}$ denotes weight matrix. 



% \noindent \textbf{GraphSAGE}: GraphSAGE \cite{hamilton2017inductive} is proposed for inductive representation learning on graphs, where each layer is:
% \begin{equation}
%     \begin{split}
%         \bm{a}_{i}^{l} & = \text{Mean} \left( \left\{ \bm{h}_{j}^{l-1}:j\in\mathcal{N}(i) \cup \{i\} \right\} \right) \\
%         \bm{z}_{i}^{l} & =  \bm{a}_{i}^{l}\bm{W}_{\text{neighbor}}^{l} || \bm{h}_{i}^{l-1}\bm{W}_{\text{self}}^{l}, \text{ } \bm{h}_{i}^{l} = \text{ReLU}(\bm{z}_{i}^{l})
%     \end{split}
%      \label{label:graphsage}
% \end{equation}
% Using matrix representation, the GraphSAGE layer can be expressed as: $
%     \bm{H}^{l} = \text{ReLU}(\bm{A}\bm{H}^{l-1}\bm{W}^{l}_{\text{neighbor}}||\bm{H}^{l-1}\bm{W}^{l}_{\text{self}})
% $


\begin{table}[h]
\centering
\caption{Notations}
\begin{adjustbox}{max width=0.48\textwidth}
\begin{tabular}{cc|cc}
\toprule
 \textbf{{Notation}} & \textbf{{Description}}  & \textbf{{Notation}}  & \textbf{{Description}} \\
 \midrule
\midrule
{$  \mathcal{G}(\mathcal{V},\mathcal{E})$ }& {input graph}  &  $ v_{i}$ & {$i^{\text{th}}$ vertex} \\ \midrule
$ \mathcal{V}$ &  {set of vertices} &  $ e_{ij}$ & {edge from $ v_{i}$ to $  v_{j}$} \\ \midrule
$ \mathcal{E}$& {set of edges} &  $ L$&{number of GNN layers} \\ \midrule
$\bm{A}$& graph adjacency matrix &  $ \mathcal{N}(i)$& the set of neighbors of $ v_{i}$ \\ \midrule
$ \bm{h}_{i}^{l-1}$& input feature vector of $ v_{i}$
at layer $l$    & $\bm{W}^{l}$  &  weight matrix of layer $l$  \\  \midrule
$\bm{H}^{l-1}$ & input feature matrix to layer $l$ & $\sigma()$ & activation function \\
 \bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:notations}
\end{table}


\begin{algorithm}
\caption{GNN Computation Abstraction}
\label{alg:GNN-computation-abstraction}
\begin{small}
\begin{algorithmic}[1]
 \renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
 \Require Input graph: $\mathcal{G}(\mathcal{V},\mathcal{E})$; vertex features: $\left\{\bm{h}^{0}_{1}, \bm{h}^{0}_{2}, \bm{h}^{0}_{3}, ..., \bm{h}^{0}_{|\mathcal{V}|}\right\}$;
 \Ensure Output vertex features $\left\{\bm{h}^{L}_{1}, \bm{h}^{L}_{2}, \bm{h}^{L}_{3}, ..., \bm{h}^{L}_{|\mathcal{V}|}\right\}$;
\For{$l=1...L$}
\For{each vertex $v \in \mathcal{V}$}
\State{$\bm{a}^l_{v} = {\text{Aggregate}(}\bm{h}_{u}^{l-1}: u\in \mathcal{N}(v))$}
\State{$\bm{z}_{v}^l = {\text{Update}(}\bm{a}_{v}^{l}, \bm{W}^{l} \textbf{)}$, $ \bm{h}_{v}^l = \sigma(\bm{z}_{v}^l )$}
\EndFor
\EndFor
\end{algorithmic}
\end{small}
\end{algorithm}

% \subsection{GNN Accelerator}
% Introduce the GNN accelerator

\subsection{Data Sparsity in GNN inference}
\label{subsec:GNN-sparsity}






The \emph{density} of a matrix is defined as the total number of non-zero elements divided by the total number of elements. Note that, the \emph{sparsity} is given by $(1 - \text{\emph{density}})$. The computation kernels in GNNs involve three types of matrices: graph adjacency matrix $\bm{A}$, vertex feature matrix $\bm{H}$, and weight matrix $\bm{W}$. 
% For example, the feature aggregation involves the multiplication of $\bm{A}$ and $\bm{H}$. The feature update involves the multiplication of $\bm{H}$ and $\bm{W}$. 
% As shown in Figure \ref{fig:density-of-adjacency-matrix},
The adjacency matrix $\bm{A}$ of different graph datasets \cite{pyg-dataset} can have different densities. For a given adjacency matrix, different parts of the matrix have different densities. Figure \ref{fig:density-of-feature-matrix}
shows the densities of feature matrices in GCN \cite{kipf2016semi}. For different graphs, the input feature matrices have different densities. The feature matrices of different layers also have different densities.  For the weight matrices, prior works (\cite{rahman2022triple, chen2021unified}) have  proposed various pruning techniques to reduce the density of the weight matrices. 
% As shown in  \cite{rahman2022triple}, 75-98\% of weight entries can be pruned without affecting the accuracy (See Figure 2 of \cite{rahman2022triple}). 

\begin{figure}[h]
     \centering
     \includegraphics[width=4cm]{pic/density-of-adjacency-matrix-a.pdf}
     \includegraphics[width=4cm]{pic/density-of-adjacency-matrix.pdf}
     \caption{The density and the visualization of graph adjacency matrix $\bm{A}$ of various graphs \cite{pyg-dataset}}
     \label{fig:density-of-adjacency-matrix}
\end{figure}

\begin{figure}[ht]
     \centering
     \includegraphics[width=8.5cm]{pic/density-of-feature-matrix.pdf}
     \caption{Density of the feature matrices in the GCN model \cite{kipf2016semi}}
     \label{fig:density-of-feature-matrix}
     \vspace{-0.2cm}
\end{figure}

\begin{figure*}[h]
     \centering
     \includegraphics[width=18cm]{pic/workflow.pdf}
     \caption{Proposed workflow}
     \label{fig:workflow}
\end{figure*}

\subsection{GNN Acceleration based on Data Sparsity}

Although there are various data sparsities in GNNs, no prior work has systematically studied exploiting the data sparsity for GNN inference acceleration.
HyGCN \cite{yan2020hygcn} and BoostGCN \cite{zhang2021boostgcn}  map Aggregate() to SpDMM and map update() to GEMM, ignoring the data sparsity in feature matrices and weight matrices.  AWB-GCN \cite{geng2020awb} maps both Aggregate() and update() to SpDMM. Then, they propose an accelerator to efficiently execute SpDMM. However, they do not exploit the data sparsity in weight matrices.  DeepBurning-GL \cite{liang2020deepburning} is a design automation framework that generates the optimized hardware accelerator given the information of the input graph and the GNN model. However, their framework needs to regenerate the optimized accelerator if the sparsity of the data is changed.  To summarize, prior GNN accelerators do not fully exploit the data sparsity in GNNs, or are not flexible to exploit data sparsity in GNN inference. 



% views both feature aggregation  and feature transformation as SpDMM based on the profiling of the data sparsity using a specific GCN model on five datasets. Then, they propose an accelerator that can efficiently execute SpDMM. However, they does not exploit the data sparsity in weight matrix, and the accelerator is not efficient for GEMM when there is no data sparsity in feature update. Moreover, the accelerator of AWB-GCN does not support SPMM which is important when involved two matrices have high sparsity. 