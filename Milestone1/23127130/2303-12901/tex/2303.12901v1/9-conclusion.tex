 \section{Conclusion and Future Work}

In this paper, we proposed a hardware-software codesign for dynamic sparsity exploitation in GNN inference.  The proposed dynamic K2P mapping reduces the inference latency by $3.73\times$ on the average compared with the static mapping strategies.  Compared with state-of-the-art CPU (GPU) implementations, Dynasparse achieves up to $56.9\times$ ($2.37\times$)  speedup in end-to-end latency. Compared with state-of-the-art FPGA implementations, Dynasparse achieves $2.7\times$ speedup in accelerator execution latency.    In the future, we plan to extend Dynasparse on heterogeneous platforms that consist of CPU, GPU and FPGA, where GPU is effective for dense primitives, FPGA is effective for sparse primitives and the CPU can execute complex control flow (e.g., dynamic K2P mapping).  
% We also plan to extend Dynasparse to support various GNN training algorithms, where we can further exploit the data sparsity in GNN back propagation.

{
\section*{Acknowledgment}
This work is supported by the National Science Foundation (NSF) under grants CCF-1919289 and OAC-2209563. Equipment
and support by Xilinx are greatly appreciated.}