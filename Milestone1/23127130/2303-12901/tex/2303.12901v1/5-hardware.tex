
\begin{figure*}[h]
     \centering
     \includegraphics[width=18cm]{pic/Diagram-of-Computation-Core.pdf}
     \caption{Diagram of a Computation Core}
     \label{fig:computation-core}
\end{figure*}


\begin{figure*}[h]
     \centering
     \includegraphics[width=18cm]{pic/execution-modes.pdf}
     \caption{Various execution modes of  a Computation Core}
     \label{fig:execution-modes}
\end{figure*}


\section{Accelerator Design}
\label{sec:accelerator-design}
 {
 In Section \ref{subsec:data-format}, we introduce the data layout and data format that are used by Dynasparse. In Section \ref{subsubsec-ACM}, we introduce the Agile Computation Module which can execute three primitives (GEMM, SpDMM, and SPMM).  In Section \ref{subsubsec:AHM}, we describe the hardware mechanism for sparsity profiling, and data format/layout transformation.}


\subsection{Data format and data layout}
\label{subsec:data-format}
\noindent \textbf{Data format}: We store the matrices using \emph{sparse} format or \emph{dense} format. We use Coordinate (COO) format to represent a sparse matrix where  an nonzero element is represented using a three-tuple $(col, row, value)$ denoting the column index, row index, and value, respectively. COO format is the standard data format used in the state-of-the-art GNN libraries \cite{pyg-dataset}.

\vspace{0.1cm}
\noindent \textbf{Data layout}: It defines the order of storing the matrix elements. For a sparse matrix in the row-major order, the elements within the same $row$ are stored in contiguous locations. Otherwise, it is \emph{column-major} order. Similarly, row-major and column-major order for a dense matrix can be derived.

\vspace{0.1cm}
\noindent \textbf{Notations}: For a matrix $\bm{B}$, we use $\bm{B}[i]$ to denote the $i^{\text{th}}$ row of $\bm{B}$  and use  $\bm{B}[i:j]$ to denote the submatrix of $\bm{B}$ from $i^{\text{th}}$ row to $(j-1)^{\text{th}}$ row.  We use $\bm{B}[i][j]$ to denote the element of $\bm{B}$ at the $i^{\text{th}}$ row  and the $j^{\text{th}}$ column. An element $(j, i, value)$ in sparse $\bm{B}$ will also be denoted as $\bm{B}[i][j]=value$.


% There are two benefits of using row-major order: (1) it can support the Row-wise
% Product approach of SPMM primitive (See Algorithm \ref{alg:scatter-gather-SPMM}), and (2) it can support data format transformation between sparse format and dense format (See Section \ref{subsubsec:AHM}). We store the dense matrix in row-major order. 

\subsection{Microarchitecture}
\label{subsec:Microarchitecture}
Each Computation Core (Figure \ref{fig:computation-core}) has an Agile Computation Module (ACM) and an Auxiliary Hardware Module (AHM). The ACM has an ALU (Arithmetic Logic Unit) array of dimension $p_{sys}\times p_{sys}$ and the interconnection among the ALUs are shown in Figure \ref{fig:execution-modes}. AHM performs sparsity profiling, data layout and format transformation (Section \ref{subsubsec:AHM}).


\subsubsection{Agile Computation Module (ACM)} \label{subsubsec-ACM} It has four data buffers -- BufferU, BufferO, BufferP and Result Buffer (RB). Buffer[U/O/P] store the input matrices and RB stores the output matrix. Each Buffer has $p_{sys}$ memory banks (denoted bank $0$ to bank $p_{sys} - 1$) for parallel on-chip memory access.  Each ALU can execute various arithmetic operations, including multiplication, max, addition, etc. 
There are two interconnection networks -- Index Shuffle Network (ISN) and Data Shuffle Network (DSN) -- for data communication.
% ISN performs all-to-all interconnection between BufferU and BufferO, and DSN performs all-to-all interconnection between BufferO and the ALU array.  
% ISN and DSN are required by the SpDMM and SPMM primitives for data routing (See Algorithm \ref{alg:scatter-gather} and \ref{alg:scatter-gather-SPMM}). 
The ACM has three execution modes -- \emph{GEMM mode}, \emph{SpDMM mode} and \emph{SPMM mode}.  The required data format and layout for various execution modes are summarized in Table \ref{tab:my_label}.
%\begin{tabular}[|c|]{@{}c@{}} 
% \end{tabular}

\begin{table}[]
\centering
\caption{Buffer (data format) [data layout] requirement to store the input/output matrices for executing $\bm{Z} = \bm{X} \times \bm{Y}$ in the three execution modes} 
\begin{adjustbox}{max width=0.48\textwidth}
\begin{tabular}{cccc}
 \toprule
  &  $\bm{X}$ &  $\bm{Y}$ & $\bm{Z}$\\ 
 \midrule
 \midrule
 GEMM &  \begin{tabular}[c]{@{}c@{}}  BufferO (dense) \\ \text{[}row major\text{]} \end{tabular} 
 & \begin{tabular}[c]{@{}c@{}} BufferP (dense) \\ \text{[}column major\text{]} \end{tabular}  & 
 \begin{tabular}[c]{@{}c@{}} Result Buffer (dense) \\ \text{[}row major\text{]} \end{tabular} \\  \midrule
 SpDMM & \begin{tabular}[c]{@{}c@{}} BufferU (sparse) \\ \text{[}row or column major\text{]} \end{tabular} 
 & \begin{tabular}[c]{@{}c@{}} BufferO (dense) \\ \text{[}row major\text{]} \end{tabular}  & 
 \begin{tabular}[c]{@{}c@{}} Result Buffer (dense) \\ \text{[}row major\text{]} \end{tabular}  \\  \midrule
 SPMM
 & \begin{tabular}[c]{@{}c@{}} BufferU (sparse) \\ \text{[}row major\text{]} \end{tabular} 
 & \begin{tabular}[c]{@{}c@{}} BufferO (sparse)   \\ \text{[}row major\text{]} \end{tabular} 
 & \begin{tabular}[c]{@{}c@{}}  Result Buffer (dense) \\ \text{[}row major\text{]} \end{tabular}  \\ \bottomrule
 \hline
\end{tabular}
\end{adjustbox}
\label{tab:my_label}
\end{table}

\vspace{0.1cm}
\noindent \textbf{GEMM Mode}: The ALU array is organized as a two-dimensional systolic array (See Figure \ref{fig:execution-modes}) to execute GEMM using output stationary dataflow. 
% The two input matrices are stored in BufferO and BufferP using dense format. 
The systolic array can execute $p_{sys}^2$ multiply-accumulate (MAC) operations per clock cycle.

% GEMM Mode is efficient when the two input matrices have high densities. For example, if both two input matrices have density $100\%$, there will be no wasted computation operations.
 
 


\begin{algorithm}
\caption{SpDMM using Scatter-Gather Paradigm}\label{alg:scatter-gather}
\begin{small}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
 \Require Sparse matrix (BufferU): $\bm{X}$; Dense matrix (BufferO): $\bm{Y}$;
 \Ensure Output matrix  (Result Buffer): $\bm{Z}$ ($\bm{Z} = \bm{X}\times \bm{Y}$);
\While {not done}
% \State {Scatter Unit:}
\For{each $e(i,j,value)$ in $\bm{X}$ \textbf{Parallel}}  {\color{blue}\Comment{Scatter Phase}}
% \State {Produce update $u \gets ${Scatter($src.features,~ e.weight$)}}
\State Fetch $\bm{Y}[i]$ from BufferO   {\color{blue}\Comment{ISN routes $e$ to BufferO}}
\State Form input pair ($\bm{Y}[i]$, $e$)  
\State {\color{blue} \# DSN routes input pair to Update Units}
\EndFor

% \State {Gather Unit:}
\For{each input pair \textbf{Parallel}}
{\color{blue}\Comment{Gather Phase}}
\State $u \gets ${Update($\bm{Y}[i],e.value$)}  {\color{blue}\Comment{Update Unit}}
\State Fetch $\bm{Z}[j]$ from Result Buffer
\State {$\bm{Z}[j] \gets$ {Reduce($u$)}}   {\color{blue}\Comment{Reduce Unit}}
\EndFor
\EndWhile
\end{algorithmic}
\end{small}
\end{algorithm}


\noindent \textbf{SpDMM Mode}: The ALU array is divided into $p_{sys}/2$ Update Units and $p_{sys}/2$ Reduce Units. Each Update or Reduce Unit has an ALU array of size $p_{sys}/2 \times 2$. Multiplication of a sparse matrix with a dense matrix is executed using the Scatter-Gather Paradigm shown in Algorithm \ref{alg:scatter-gather}. The sparse matrix denoted as $\bm{X}$ (in BufferU) is stored in row-major order using COO format. The dense matrix   denoted as $\bm{Y}$ (in BufferO) is stored in row-major order using dense format, and $\bm{Y}[i]$ is stored in bank $(i\mod p_{\text{sys}})$ of BufferO. Each non-zero element $e(i,j,weight)$  in  $\bm{X}$ is fetched from the BufferU ($p_{sys}/2$ elements can be fetched from BufferU per cycle) and sent to the ISN. Then $e$ is routed to bank $(i\mod p_{\text{sys}})$  for fetching  $\bm{Y}[i]$, which forms the input data pair ($\bm{Y}[i]$, $e$). The input pair is routed to the $(j\mod p_{\text{sys}}/2)^{\text{th}}$ Update Unit. The Update Unit performs the multiplication of $e.value$ and $\bm{Y}[i]$ to produce the intermediate result $u$. Then the corresponding Reduce Unit adds $u$ to $\bm{Z}[j]$.  SpDMM Mode can efficiently skip zero elements in the sparse matrix $\bm{X}$. The SpDMM Mode can execute $p_{sys}^{2}/2$ MAC operations per clock cycle.
% (The $p_{sys}/2$ Update Units can execute $p_{sys}^{2}/2$ multiply operations and the $p_{sys}/2$ Reduce Units can execute $p_{sys}^{2}/2$ addition operations).

% Note that each PE has $p_{sys}$ ALUs.  If the row length of the dense matrix is $Rlen$, the Gather Phase of a row will be executed for $\lceil \frac{Rlen}{p_{sys}} \rceil$ clock cycles. 


\begin{algorithm}
\caption{SPMM using Row-wise Product with Scatter-Gather Paradigm}\label{alg:scatter-gather-SPMM}
\begin{small}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
 \Require  Sparse matrix (BufferU): $\bm{X}$; Sparse matrix (BufferO): $\bm{Y}$;
 \Ensure Output matrix (In Result Buffer): $\bm{Z} = \bm{X}\times \bm{Y}$;
\For{each row $\bm{Z}[j]$ in $\bm{Z}$ \textbf{Parallel}}
% \State {Scatter Unit:}
\State Assign the workload of $\bm{Z}[j]$ to $\text{SCP}[j\%p_{sys}]$
\State load $\bm{Z}[j]$ to the Sparse Data Queue from Results Buffer
\For{each $e(i,j,value)$ in $\bm{X}[j]$ }  {\color{blue}\Comment{Scatter Phase}}
% \State {Produce update $u \gets ${Scatter($src.features,~ e.weight$)}}
\State Fetch $\bm{Y}[i]$ from BufferO   {\color{blue}\Comment{ISN routes $e$ to BufferO}}
\State Form input pair ($\bm{Y}[i]$, $e$)  {\color{blue}\Comment{DSN routes input to SCPs}}
\EndFor

% \State {Gather Unit:}
\For{each input pair ($\bm{Y}[i]$, $e$)}
{\color{blue}\Comment{Gather Phase}}
    \For{each non-zero $\bm{Y}[i][k]$ in $\bm{Y}[i]$}  {\color{blue}\Comment{SCP}}
    % \State $outsparse[j][k] += e.value \times sparsey[i][k]$
    \State Produce $u \gets $ {Update($e.value \times \bm{Y}[i][k]$)}
    % \State {\color{blue}{\#  $\bm{Z}[j]$ is stored in SDQ}}
    \State {Merge $\bm{Z}[j][k] \gets$ {Reduce($u$)}} 
    \EndFor
    
    \EndFor
\State Store $\bm{Z}[j]$ to the Result Buffer  {\color{blue}\Comment{Obtain $\bm{Z}[j]$}}
\EndFor
\end{algorithmic}
\end{small}
\end{algorithm}

\vspace{0.1cm}
\noindent \textbf{SPMM Mode}: The ALU array is organized as $p_{sys}$ parallel Sparse Computation Pipelines (SCP) as shown in Figure \ref{fig:execution-modes}. Each SCP has two ALUs to perform  multiplication of two non-zero elements and  the merging of intermediate results. Each SCP also has a Sparse Data Queue (SQ) to store the intermediate results in sparse format. The multiplication of two input sparse matrices is executed using the Row-wise Product  with Scatter-Gather paradigm as shown in Algorithm \ref{alg:scatter-gather-SPMM}. 
For Row-wise Product,  an row $ \bm{Z}[j]$  of  output matrix $\bm{Z}$ is calculated through: 
\begin{equation}
    \bm{Z}[j] = \sum_{i}\bm{X}[j][i]*\bm{Y}[i]
    \label{eq:workload-of-an-row}
\end{equation}
For calculating the output matrix $\bm{Z}$, a SCP is assigned the workload of an row of output matrix (Equation \ref{eq:workload-of-an-row}). $p_{sys}$ SCPs can calculate $p_{sys}$ output rows in parallel until all the rows of the output matrices are calculated.
To efficiently execute Row-wise Product, all input sparse matrices ($\bm{X}$, $\bm{Y}$) and output matrix are stored using COO format in row-major order (See Section \ref{subsec:data-format}). Using SPMM Mode, we can skip the zero elements in both the input matrices. SPMM Mode can execute $p_{sys}$ multiply-accumulate (MAC) operations per clock cycle. 

\vspace{0.1cm}
\noindent \textbf{Mode switching}: The execution mode is set by the control bits of the hardware multiplexers in ACM. The overhead of switching execution modes is just one clock cycle.

\vspace{0.1cm}
\noindent \textbf{Trade-off}: The three execution modes have different ways of dealing with  non-zero elements in the two input matrices (Section \ref{subsec:problem-define}). Therefore, their execution time of multiplying two input matrices depends on the data sparsity. We analyze the trade-off of the three execution modes w.r.t. data sparsity in Section \ref{subsec:performance-model}.


% GEMM Mode can execute highest MACs $p_{sys}^2$ per cycle. However, it cannot skip the zero elements in the input matrices, which is efficient when both two input matrices have high density. SpDMM Mode can execute MACs $p_{sys}^2/2$ per cycle and can efficiently exploit the data sparsity in one input matrix. SPMM can execute MACs $p_{sys}$ per cycle and can exploit the data sparsity in both two input matrices. Based on the properties of the three execution modes, we develop our dynamic kernel-to-primitive mapping algorithm (See Section \ref{sec:runtime-system}).  

  

\subsubsection{Auxiliary Hardware Module (AHM)}
\label{subsubsec:AHM}
While the ACM can execute various primitives, the data format and layout should meet the requirement of the execution modes (Table \ref{tab:my_label}). Moreover, the soft processor  needs the data sparsity information at runtime for dynamic K2P mapping. To this end, the AHM has the following hardware modules:
(1) a Layout Transformation Unit and  a Layout Merger to transform the data layout, (2) a Sparsity Profiler (SP) to obtain the density of the intermediate results, (3) Format Transformation Module (FTM), which contains a Sparse-to-Dense  Module and a Dense-to-Sparse Module. 

\vspace{0.1cm}
\noindent \textbf{Layout Transformation Unit (LTU)}: Transformation of the data layout between row-major order and column-major order is transposing a matrix.  LTU is implemented using a streaming permutation network \cite{chen2015energy} (See \cite{chen2015energy}  for details) for efficient layout transformation.  Since most of the on-chip data are stored using row-major order, we store all the data partitions of ($\bm{A}$, $\bm{H}$, $\bm{W}$) in the external memory using row-major order to minimize the effort for data layout transformation. 
% We implement a LTU to transform the data layout for GEMM mode (the data in BufferP. See Table \ref{tab:my_label}).

\vspace{0.1cm}
\noindent \textbf{Layout Merger}: When the accelerator executes a task  (See algorithm \ref{alg:Computation-Task}), the results  $\bm{Z}$ can be in row-major or column-major order. Therefore, in Results Buffer, we store two partial results of $\bm{Z}$ in row-major and column-major order, respectively. The two partial results of  $\bm{Z}$ are merged by Layout Merger into row-major order when $\bm{Z}$ is sent back to the external memory.
Note that the LTU is also used by BufferO to transform the data layout for $\bm{X}_{2}^{\intercal}$ (column-major order of $\bm{X}_{2}$).


% such as $\bm{Z}=\bm{X}_{1}\bm{Y}_{1} + \bm{X}_{2}\bm{Y}_{2}$. It is possible that $\bm{X}_{1}$ and $\bm{Y}_{2}$ have low density, and $\bm{X}_{2}$ and $\bm{Y}_{1}$ have high density. Therefore, we can execute both $\bm{X}_{1}\bm{Y}_{1}$ and $\bm{Y}_{2}^{\intercal}\bm{X}_{2}^{\intercal}$ ($\bm{X}^{\intercal}$ is the transpose of $\bm{X}$) using SpDMM mode. However, the results of $\bm{X}_{1}\bm{Y}_{1}$ and $\bm{Y}_{2}^{\intercal}\bm{X}_{2}^{\intercal}$ have different data layouts. While the result of $\bm{X}_{1}\bm{Y}_{1}$ is in row-major order (as $\bm{X}_{1}$ is in BufferU and $\bm{Y}_{1}$ is in BufferO), the result of   $\bm{Y}_{2}^{\intercal}\bm{X}_{2}^{\intercal}$ is in column-major order (as $\bm{Y}_{2}^{\intercal}$ is in BufferU and $\bm{X}_{2}^{\intercal}$ is in BufferO). 


\vspace{0.1cm}
\noindent \textbf{Sparsity Profiler}: To profile the density of sparse matrix or dense matrix, we use the adder tree based design for the Sparsity Profiler. At the output port of the Result Buffer, we implement a comparator array with an adder tree to count the total number of non-zero elements. After obtaining the data sparsity of the current output matrix, the sparsity information is sent to the soft processor.


\vspace{0.1cm}
\noindent \textbf{Dense-to-Sparse (D2S) Module}: It transforms an array from dense format to sparse format. Suppose the D2S Module can read $n$ elements per clock cycle. Then, the D2S Module has $\log(n)$ pipeline stages.   For an $n$-element array, we use the value of Prefix-Sum to indicate the number of zeros before an element in this array. An example is shown in Figure \ref{fig:dense-to-sparse}. In Stage $i~(1 \leqslant i \leqslant \log(n))$, an array element will be shifted left by $2^{i-1}$ positions if the $(i-1)^{\text{th}}$ bit of Prefix Sum value is equal to 1. The throughput of D2S Module is $n$ elements per cycle. For example, a DDR4 channel of the FPGA board can output 16 32-bit data per cycle. A D2S Module of $n = 16$ is sufficient to match the data rate of a DDR4 channel.  
The architecture of S2D is similar to D2S, but in the reverse direction. 

\begin{figure}[h]
     \centering
     \includegraphics[width=8.5cm]{pic/dense-to-sparse.pdf}
     \caption{Transforming dense format to sparse format}
     \label{fig:dense-to-sparse}
\end{figure}


% \begin{figure}[h]
%      \centering
%      \includegraphics[width=8.5cm]{pic/sparse-to-dense.pdf}
%      \caption{Transforming sparse format to dense format}
%      \label{fig:sparse-to-dense}
% \end{figure}


% \noindent \textbf{Sparse-to-Dense (S2D) Module}: The architecture of S2D is similar to D2S, but in the reverse direction. 
% Suppose the S2D Module can output $n$ elements per clock cycle. The S2D Module has $\log(n)$ pipeline stages.
% An example is shown in Figure \ref{fig:sparse-to-dense}. In each stage, we use the column index to calculate how many zeros (denote as as Num-Zero) should be inserted before an element. In Stage $i~(1 \leqslant i \leqslant \log(n))$, an array element will be shifted right by  $2^{i-1}$ positions if the $(i-1)^{\text{th}}$ bit of Num-Zero is equal to 1. 

\vspace{0.2cm}
{
\subsubsection{Double Buffering} \label{subsubsec-double-buffering} We exploit double buffering technique for Buffer[U/O/P] and Results Buffer. Therefore, when the Computation Core is executing the current task, the Buffers can load the input data of the next task. The data sparsity profiling, data layout and format transformation are streaming processes that can be executed during the data loading/storing process. Double Buffering not only overlaps the computation and data communication, but also hides the overhead of sparsity profiling and data layout/format transformation.
}
