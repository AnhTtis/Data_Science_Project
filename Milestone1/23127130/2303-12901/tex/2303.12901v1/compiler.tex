\section{Compiler}
\label{sec:compiler}

\subsection{Intermediate Representation (IR)}
\label{sucsec:IR}
 {
We define the meta data in the IR in Table \ref{tab:inter-representation}, including the meta data of the kernel and the meta data of the execution scheme. The execution scheme of a kernel is the plan for executing the kernel. The IR defines two types of kernels -- \emph{Aggregate} and \emph{Update}, corresponding to  $\text{Aggregate}()$ and $\text{Update}()$ in the GNN abstraction (See Algorithm \ref{alg:GNN-computation-abstraction}). 
}



% After that, the compiler sends the optimized IR to the runtime system.

% \subsection{Intermediate Representation}


\begin{table}[ht]
\centering
\caption{Meta data of a kernel in the IR }
\begin{adjustbox}{max width=0.55\textwidth}
\begin{tabular}{|l|l|}
\hline
\textbf{Layer Type} & \begin{tabular}[|c|]{@{}c@{}} Aggregate(0), Update(1) \end{tabular} \\ 
\hline
\textbf{Layer ID}  & 1,2,3,... \\
\hline
\textbf{Input Dimension}  & $f_{\text{in}}$  \\
\hline
\textbf{Output Dimension}  & $f_{\text{out}}$ \\
\hline
\textbf{\# of vertices} & $|\mathcal{V}|$  \\
\hline
\textbf{\# of edges} & $|\mathcal{E}|$  \\
\hline
\textbf{Aggregation operator} &  Max, Sum, Min, Mean \\
\hline
\textbf{Activation type} & ReLU, PReLU\\
\hline
\textbf{Activation enabled} & True, False\\
\hline
\textbf{Meta data of execution scheme} & \{...\} (See Algorithm \ref{alg:scheme-aggregate} and \ref{alg:scheme-Update})\\
\hline
\end{tabular}
\end{adjustbox}
\label{tab:inter-representation}
\end{table}



% The Input Parser extracts the information from (1) the GNN model defined using PyG by the user, (2) input graph,  and generates the Intermediate Representation (IR).


% The activation function $\sigma()$ is merged into $\text{Aggregate}()$ and $\text{Update}()$ through $\verb|Activation type|$ and $\verb|Activation enabled|$ options.

% \vspace{0.1cm}
% \noindent \textbf{Aggregate layer}: The input to the \emph{Aggregate} layer is the graph adjacency matrix $\bm{A}$ and vertex feature matrix $\bm{H}_{\text{in}}$. The output feature matrix $\bm{H}_{\text{out}}$ is calculated through $\bm{H}_{\text{out}} = \bm{A} \times\bm{H}_{\text{in}}$.

% \vspace{0.1cm}
% \noindent \textbf{Update layer}: The input to the \emph{Update} layer is the vertex feature matrix  $\bm{H}_{\text{in}}$ and weight matrix $\bm{W}$. The output feature matrix $\bm{H}_{\text{out}}$ is calculated through  $\bm{H}_{\text{out}} = \bm{H}_{\text{in}} \times \bm{W}$.


% \subsection{Data partitioning}
% \begin{figure}[h]
%      \centering
%      \includegraphics[width=8.5cm]{pic/IR.pdf}
%      \caption{IR of a computation kernel}
%      \label{fig:inter-representation}
% \end{figure}

\begin{figure}[h]
     \centering
     \includegraphics[width=8.5cm]{pic/data-partition.pdf}
     \caption{Illustration of data and model partitioning}
     \label{fig:Partition-scheme}
\end{figure}


 {
\subsection{Compilation Process}
The compilation process has two steps (See Figure \ref{fig:system-overview}):
\begin{itemize}
    \item \textbf{Step 1 (parsing the input)}: The compiler takes the specification of the GNN model (Defined using Pytorch Geometric Library \cite{pyg-dataset}) and the graph meta data as input, and generates the computation graph for GNN inference (See the example in Figure \ref{fig:workflow}).  The computation graph has $\sum_{l=1}^{L} k_{l}$ nodes, where $L$ denotes the number of GNN layers in the GNN model and $k_{l}$ denotes the number of kernels in layer $l$ ($1 \leqslant l \leqslant L$). In the computation graph, each node represents the IR of a kernel. An edge denotes the data dependency between two kernels.
    \item \textbf{Step 2 (data partitioning and execution scheme generation)}: The compiler performs data partitioning for each kernel and generates the execution scheme for the kernel. Then, the meta data of the execution scheme is stored in the IR to produce the  optimized IR (See Figure \ref{fig:workflow}) that is sent to the runtime system. 
\end{itemize} 
}



\subsection{Data Partitioning}
\label{subsec:data-partitoning}

Figure \ref{fig:Partition-scheme} depicts the proposed data partition scheme. The graph adjacency matrix $\bm{A}$ has the dimension $|\mathcal{V}| \times |\mathcal{V}|$. $\bm{A}$ is partitioned into blocks with each block having dimension of $N_{1} \times N_{1}$. We use $\bm{A}_{ij}$ to denote a block where $\bm{A}_{ij} = \bm{A}[i*N_{1}:(i+1)*N_{1}][j*N_{1}:(j+1)*N_{1}]$. The feature matrix $\bm{H}$ of dimension $|\mathcal{V}| \times f_{1}$ is partitioned into fibers. Each fiber has  dimension $N_{1} \times N_{2}$ and  $\bm{H}_{ij} = \bm{H}[i*N_{1}:(i+1)*N_{1}][j*N_{2}:(j+1)*N_{2}]$.  We further partition each fiber into subfibers where each subfiber has size $N_{2} \times N_{2}$. $\bm{H}_{ij-k}$ denotes the $k^{\text{th}}$ subfiber of $\bm{H}_{ij}$. We use $\bm{H}_{i-k}$ to denote the concatenation of $\{\bm{H}_{i1-k}$, $\bm{H}_{i2-k}$, ..., $\bm{H}_{i\frac{N_{1}}{N_{2}}-k}\}$. The weight matrix $\bm{W}$ is partitioned into blocks with each block having size of $N_{2}\times N_{2}$. $\bm{W}_{ij} = \bm{W}[i*N_{2}:(i+1)*N_{2}][j*N_{2}:(j+1)*N_{2}]$.  

% \vspace{0.1cm}
% {\color{red} 
% \noindent }

% \begin{figure}[h]
%      \centering
%      \includegraphics[width=8.7cm]{pic/execution-scheme.pdf}
%      \caption{The execution scheme of a kernel}
%      \label{fig:execution-scheme}
% \end{figure}

\subsection{Execution Scheme}
\label{subsec:execution-scheme}

Based on the  data partition scheme, the compiler generates the execution plan for each computation kernel, shown in Algorithm \ref{alg:scheme-aggregate} and \ref{alg:scheme-Update}. The execution of a computation \emph{kernel} is decomposed into a set of independent computation \emph{tasks}. Each task performs the execution of an output data partition and there is no data dependency among the tasks within a kernel. Each task performs the multiplication of data partitions to obtain an output data partition, and the computation  primitive to execute the matrix multiplication $\verb|Matmul()|$ is determined by the Runtime System. We generalize the representation of a task in Algorithm \ref{alg:Computation-Task}.




\begin{algorithm}
\caption{Execution scheme of an Aggregate kernel}
\label{alg:scheme-aggregate}
\begin{small}
\begin{algorithmic}[1]
 \renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
 \Require Graph adjacency matrix $\bm{A}$; Input feature matrix $\bm{H}^{\text{in}}$;
 \Ensure Output  feature matrix $\bm{H}^{\text{out}}$;
\State{\tikzmark{start2} \hspace{-0.25cm} Execute the Aggregate kernel}
\For{$i=1$ to $\frac{|\mathcal{V}|}{N_{1}}$}
    \For{$k=1$ to $\frac{f_{1}}{N_{2}}$}
        \State{ \tikzmark{start1} \hspace{-0.35cm}  Initialize $\bm{H}^{\text{out}}_{ik}$ in the Result Buffer}
        \For{$j=1$ to $\frac{|\mathcal{V}|}{N_{1}}$}
            \State{ Load $\bm{A}_{ij}$ and $\bm{H}^{\text{in}}_{jk}$}
            \State{$\bm{H}^{\text{out}}_{ik} += \text{Matmul}(\bm{A}_{ij}, \bm{H}^{\text{in}}_{jk})$}
        \EndFor
        \State{Write $\bm{H}^{\text{out}}_{ik}$ back to DDR memory}
        \tikzmark{end1} \tikzmark{end2}
    \EndFor
\EndFor
\end{algorithmic}
\end{small}
\end{algorithm}
\vspace{-0.8cm}
\Textbox[2.0cm]{start1}{end1}{Task}
\Textbox[1.9cm]{start2}{end2}{Kernel}


\begin{algorithm}
\caption{Execution scheme of an Update Kernel}
\label{alg:scheme-Update}
\begin{small}
\begin{algorithmic}[1]
 \renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
 \Require  Input  feature matrix $\bm{H}^{\text{in}}$; Weight matrix $\bm{W}$;
 \Ensure Output feature matrix $\bm{H}^{\text{out}}$;
 \State{\tikzmark{start4} \hspace{-0.25cm} Execute the Update kernel}
\For{$i=1$ to $\frac{|\mathcal{V}|}{N_{2}}$}
    \For{$k=1$ to $\frac{f_{2}}{N_{2}}$}
        \State{\tikzmark{start3} \hspace{-0.24cm}  $g = \lfloor\frac{i\times N_{2}}{N_{1}}\rfloor$, $f = i\%(\frac{N_{1}}{N_{2}})$}
        \State{Initialize $\bm{H}^{\text{out}}_{gk-f}$ in the Result Buffer}
        \For{$j=1$ to $\frac{f_{1}}{N_{1}}$}
            \State{ Load $\bm{H}^{\text{in}}_{gj-f}$ and $\bm{W}_{jk}$}
            \State{$\bm{H}^{\text{out}}_{gk-f} += \text{Matmul}(\bm{H}^{\text{in}}_{gj-f}, \bm{W}_{jk})$}
        \EndFor
        \State{Write $\bm{H}^{\text{out}}_{gk-f}$ back to DDR memory}
        \tikzmark{end3} \tikzmark{end4}
    \EndFor
\EndFor
\end{algorithmic}
\end{small}
\end{algorithm}
\vspace{-0.8cm}
\Textbox[2.0cm]{start3}{end3}{Task}
\Textbox[1.9cm]{start4}{end4}{Kernel}




\begin{algorithm}
\caption{A computation task}
\label{alg:Computation-Task}
\begin{algorithmic}[1]
 \renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
 \Require  $\{\bm{X}_{i1}$, $\bm{X}_{i2}$, ..., $\bm{X}_{iK}\}$ and $\{\bm{Y}_{1j}$, $\bm{Y}_{2j}$, ..., $\bm{X}_{Kj}\}$;
 \Ensure Output matrix: $\bm{Z}_{ij}$;
\State{Initialize $\bm{Z}_{ij}$ in the Result Buffer}
\For{$k=1$ to $K$}
    % \State{{\color{blue}{//The buffer and the primitive are decided at runtime}}}
    \State{Load $\bm{X}_{it}$ and $\bm{Y}_{tj}$ onto the on-chip buffer}
    \State{$\bm{Z}_{ij} += \text{Matmul}(\bm{X}_{it}, \bm{Y}_{tj})$ }
\EndFor
\State{Write $\bm{Z}_{ij}$ back to DDR memory}
\end{algorithmic}
\end{algorithm}
