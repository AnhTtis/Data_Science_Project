\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Economical Quaternion Extraction from a Human Skeletal Pose Estimate using 2-D Cameras\\

\thanks{The authors are with the Center for Internet of Things, Department of Computer Science and Engineering, PESU Ring Road Campus (Bangalore, India). \\
Our code repository can be found at : https://github.com/SR42-dev/human-pose-quaternion-extraction}
}

\author{

\IEEEauthorblockN{Sriram Radhakrishna}
\IEEEauthorblockA{\textit{Department of Computer Science and Enginering} \\
\textit{P.E.S University, Banashankari}\\
Bangalore, India \\
sriram.radhakrishna42@gmail.com}

\and

\IEEEauthorblockN{Adithya Balasubramanyam}
\IEEEauthorblockA{\textit{Department of Computer Science and Engineering} \\
\textit{P.E.S University, Banashankari}\\
Bangalore, India \\
adithyab@pes.edu}

}

\maketitle

\begin{abstract}
In this paper, we present a novel algorithm to extract a quaternion from a two dimensional camera frame for estimating a contained human skeletal pose. The problem of pose estimation is usually tackled through the usage of stereo cameras and intertial measurement units for obtaining depth and euclidean distance for measurement of points in 3D space. However, the usage of these devices comes with a high signal processing latency as well as a significant monetary cost. By making use of MediaPipe, a framework for building perception pipelines for human pose estimation, the proposed algorithm extracts a quaternion from a 2-D frame capturing an image of a human object at a sub-fifty millisecond latency while also being capable of deployment at edges with a single camera frame and a generally low computational resource availability, especially for use cases involving last-minute detection and reaction by autonomous robots. The algorithm seeks to bypass the funding barrier and improve accessibility for robotics researchers involved in designing control systems.
\end{abstract}

\begin{IEEEkeywords}
quaternions, 2-D camera, pose estimation, MediaPipe, low-power, low-latency, embedded computer vision.
\end{IEEEkeywords}

\section{Introduction}
Essential scene-analysis tasks such as pedestrian detection and localization generally involve the generation of a quaternion to estimate the orientation of a target object. At times, these techniques involve the usage of stereo cameras and inertial measurement units to match feature points \cite{fathian2017quaternion} or other such methods involving expensive hardware components with a relatively large latency and computational resource utilization. 

Intuitively speaking, generalizing the potential use cases, keeping the computation closer to the system and eliminating the expensive hardware involved in the deployed algorithm would be the way forward to tackle the cost and latency issues. In order to do this, the hardware reliant inputs being accepted by existing pose estimation models must be taken note of. For this particular use case of human pose estimation, the depth of the points located within a targeted pose object and the simultaneous threading of two image streams at minimum stands out. In order to address these issues, a system containing a single 2-D camera with a human subject in frame on the hardware side to minimize costs was opted for. Additionally, a specialized deep learning based solution on the software side was implemented to account for the loss of depth perception as well as maintain latency on edge devices with a lower resource availability. The goal is to mimic the quaternion outputs provided by modern inertial measurement units for any given edge between adjacent joints on the human skeletal pose \cite{jouybari2017experimental} \\

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.42]{demo.JPG}}
\caption{A demonstration of the novel algorithm for the use case of calculating the angle of orientation of a pedestrian object from a generated quaternion.}
\label{fig1}
\end{figure}

\section{Related work}
% Background - emphasize on the work done in the field and the contributions of this project to the same.
Pavvlo et al. in their efforts to develop a quaternion based recurrent model for human motion, made the observation that human motion is a stochastic sequential process with a high level of intrinsic uncertainty \cite{pavllo2018quaternet}. While deep learning based approaches have been very successful in predicting the pose of a human skeleton in both short term \cite{fragkiadaki2015recurrent}\cite{martinez2017human} and long term \cite{holden2017phase} applications, they require multi-threaded computations and are generally costlier to implement on hardware. This is especially true for use cases in fields like mobile robotics and autonomous vehicle navigation where the cost factor is usually compromised on in favour of larger core counts on the main edge processing unit. \\

Although this trade-off has recently found more justifications with the dropping retail costs of Nvidia GPU based edge computing units \cite{schneider2017deeper}, the gap is best bridged by adapting CPU based systems to more efficiently handle the calculations necessary. It was accurately noted by Eberly, D. that rotation matrices and quaternions take over 61 percent fewer calculations to obtain than angle-axis representations of vector rotating operations \cite{eberly2002rotation}. 

\section{Concept Theory and Implementation Methodology}
% Provide an information dense introductory paragraph for the following subsections.
The following section introduces the thought processes, assumptions and applied quaternion mathematics behind the novel algorithm.

\subsection{Overview}
The challenges faced in this approach lie in the implementation of algorithmic solutions to finding the inputs mentioned in the introductory paragraph. First, the problem of calculating the orientation of the human pose object came with the issue of localization of body landmarks with acceptable standards of latency. Following this, was the conception of a mathematical function that extracted the quaternion from the pose data of these points.

To solve the problem of estimating the orientation, we employed MediaPipe, a framework by Alphabet Inc. that provides deployed solutions from deep learning models trained on data for human pose object detection. This package is aimed at edge devices with a low resource utilization.\cite{lugaresi2019mediapipe}. While the framework achieves admirable detection latencies, an inherent flaw in the system when porting to use cases requiring quaternion generation like robotic navigation \cite{sarabandi2019survey} and such due to its basis in 3-D cartesian space.
An added advantage provided by the novel algorithm is that portability of the model is maintained even when implemented on dynamic frames of reference, e.g.; on a mobile robot in motion.

\subsection{Quaternion Transformations}
% Focus on the manner in which mediapipe extracts relative depth and provide the math behind the calculations for the visualizations and the model. 
Before we get to the equations that constitute the novel algorithm, it is important to gain some clarity on why a quaternion is necessary for this use case. We can visualize this by taking the example of a pedestrian obstruction to an autonomous vehicle or robot. For situations requiring a quick response from the robot to a major last-minute change in scene, especially ones which could endanger human lives such as humans crossing the path of a moving vehicle  

To provide some context here, the quaternions can be calculated for any adjacent pairs of joints in the human pose, due to which we have narrowed down a use case for the algorithm to the problem of pedestrian orientation detection and modeled our implementation based on that. Hence, the two points chosen to evaluate the pose of our target human are the shoulder points as their relative positions are an accurate descriptor of the orientation of said target \cite{rungruangbaiyok2021shoulder}. The same equations and thought processes can be used to extract this data for any pair of joints.

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.40]{mediapipepose.jpg}}
\caption{BlazePose 33 keypoint topology for a human skeleton as COCO (colored with green) superset \cite{bazarevsky_grishchenko_2020}}
\label{fig2}
\end{figure}

To narrow down this choice, all the provided options in the MediaPipe pose solutions documentation were evaluated \cite{bazarevsky_grishchenko_2020} for appropriate pairs of points whose projections are sure to change location on a 2-D frame when the target's roll, pitch or yaw changes. Additionally, it was also arbitrarily determined that symmetry must be maintained for these points across the mid-line of the body as this seemed like the intuitive guideline to maintain given the potential use cases of our implementation.

After obtaining the coordinates for these points from the cartesian space estimated by the framework, a rotation matrix for the skeleton was generated by taking the coordinates of the two shoulder points mentioned before. Let us denote the points for the left and right shoulder points on the frame as (xl, yl, zl) and (xr, yr, zr) respectively. Therefore, the Z-axis vector of the rotation matrix can be calculated using the difference between the two points as 

\begin{equation}\label{eq1}
\begin{split}
        \vec{z} =  \begin{bmatrix} xl &yl  &zl \end{bmatrix} - \begin{bmatrix} xr &yr  &zr \end{bmatrix} \\
  \end{split}
\end{equation}

\begin{equation}\label{eq2}
\begin{split}
        \widehat{z} = \left\{\begin{matrix}\frac{\vec{z}  }{|\vec{z}|}; \!\ \Delta z \neq  0 
\\ 
\begin{bmatrix}
0 &-1  &0 
\end{bmatrix}; \!\ \Delta z =  0 \\
\end{matrix}\right.
  \end{split}
\end{equation}

Similarly, the X and Y axis vectors are generated as such -

\begin{equation}\label{eq4}
\begin{split}
        \widehat{x} = \begin{bmatrix}
0 &0  &1 
\end{bmatrix} \!\ \times \!\ \widehat{z} \!\ ; \!\ iff \!\ \widehat{x} \neq 0 \!\ else \begin{bmatrix}
1 &0  &0 
\end{bmatrix}
  \end{split}
\end{equation}

\begin{equation}\label{eq5}
\begin{split}
        \widehat{y} = \widehat{z} \!\ \times \!\ \widehat{x}
  \end{split}
\end{equation}

... which allows us to derive our rotation matrix \cite{hamilton1866elements} using the camera look-at method \cite{prunier_2016} as

\begin{equation}\label{eq6}
\begin{split}
        R_{3,3} = \begin{bmatrix}
r_{00} &r_{01}  &r_{02} \\ 
r_{10} &r_{11}  &r_{12} \\ 
r_{20} &r_{21}  &r_{22}
\end{bmatrix} = \begin{bmatrix}
\widehat{x} &\widehat{y}  &\widehat{z} 
\end{bmatrix}
\end{split}
\end{equation}

Subsequently, the quaternion \textbf{Q} was obtained in a standard form from this rotation matrix \cite{hamilton1866elements} as 

\begin{equation}\label{eq7}
\begin{split}
        \textbf{Q} = a + bi + cj + dk
\end{split}
\end{equation}

... where the coefficient values can be mapped according to equations 7 through 10 \cite{shuster1993survey}. 
\begin{equation}\label{eq8}
\begin{split}
        a = \frac{1}{2} \sqrt{|1 + r_{00} + r_{11} + r_{22}|} 
\end{split}
\end{equation}

\begin{equation}\label{eq9}
\begin{split}
        b = \frac{r_{21} - r_{12}}{4a}
\end{split}
\end{equation}

\begin{equation}\label{eq10}
\begin{split}
        c = \frac{r_{02} - r_{20}}{4a} 
\end{split}
\end{equation}

\begin{equation}\label{eq11}
\begin{split}
        d = \frac{r_{10} - r_{01}}{4a}
\end{split}
\end{equation}

In order to extract a usable real world statistic from these equations and to test the practicality of the novel algorithm, the direction being faced by the human pose object in the camera frame was extracted by applying certain transformations to the obtained quaternions (hereby referred to as the angle of orientation of the human pose object). Let it be noted at this point that the quaternion values returned do in fact contain some noise due to implicit measurement uncertainities in xl, yl, zl, xr, yr and zr (refer to equation 1) as illustrated in the section IV, sub-section D 'Kalman Filter to eliminate quaternion noise'. These uncertainties were compensated for with the implementation of a 1-D Kalman filter for the angle of orientation of the human pose object as it was emperically the most accurate approach found post-testing.\\

This was done by extracting the angle of rotation from the angle-axis form of the quaternion, as the axis vector itself was implicitly made to be oriented upwards from the head of the human object in the frame when the rotation matrix was extracted from the model. \\

Assuming our quaternion to be of the form ... 
\begin{equation}\label{eq12}
\begin{split}
        \textbf{Q} = cos\theta + sin\theta(xi + yj + zk)
\end{split}
\end{equation}

Theta was extracted as ...
\begin{equation}\label{eq13}
\begin{split}
        \theta = \arccos (\frac{a}{\sqrt{a^{2} + b^{2} + c^{2} + d^{2}}})
\end{split}
\end{equation}

Theta was then transformed to obtain the angle of orientation in our frame of reference such that the  axis of reference was the horizontal across the camera feed window.

\begin{equation}\label{eq14}
\begin{split}
        \theta = \frac{\theta \cdot \frac{180^{2}}{\pi}}{45} - 180
\end{split}
\end{equation}

The transformation applied here takes care of the conversion from radians to degrees as well as the fact that an x degree rotation in the quaternion translates to a rotation of 2x in a real life scenario. The equation was arrived at after collecting the values of theta for a 180 degree rotation of the human object and mapping them to the real world angle being faced by the same. The angle being calculated was essentially the one included by a perpendicular from the line connecting the shoulder points to the horizontal of the camera frame. This scene can be visualized as shown in figure 3, not taking into account the fact that the camera and the shoulder points don't exist at the same height.\\

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.4]{representation.png}}
\caption{A simplistic visualization of the use case to detect the angle of orientation of a pedestrian object taking just the shoulder points.}
\label{fig6}
\end{figure}

\subsection{Algorithm}
% Describe the algorithm. 
Prior to covering the main algorithm, do note that the \textit{getRotationMatrix} function accepts 6 floating point values denoting xl, yl, zl, xr, yr and zr  respectively (Refer to equation 1) and returns the rotation matrix formulated in equation 5 by iterating through the calculations in equations 2 through 4. The \textit{calculateQuaternion} function accepts a rotation matrix R as given in equation 5, calculates \textbf{Q} as in equation 6 by calculating its components according to equations 7 through 10 and returns the required quaternion. \\

 The aggregated algorithmic flow for quaternion generation from the 2-D image of the skeletal pose was framed to proceed as follows -
 
 \begin{algorithmic}[1]
 \renewcommand{\algorithmicrequire}{\textbf{Input:}}
 \renewcommand{\algorithmicensure}{\textbf{Output:}}
 \REQUIRE Video feed API v
 \ENSURE  \textbf{Q}
 \\ \textit{Initialisation} :
  \STATE v = VideoCapture Class \cite{baggio2012mastering}
 \\ \textit{LOOP Process} :
  \WHILE{True}
  \STATE img = v.frame // assigning an image requested from the API to the variable img at the time of request
  \STATE pose = poseDetector(img) // detects the existence of a skeletal pose in the frame
  \STATE // getting the requested landmarks from the MediaPipe framework
  \IF {pose != None}
  \STATE xl, yl, zl = pose.getLandmarks('Left Shoulder')
  \STATE xr, yr, zr = pose.getLandmarks('Right Shoulder')
  \STATE R = getRotationMatrix(xl, yl, zl, xr, yr, zr) \\
  \STATE \textbf{Q} = calculateQuaternion(R) \\
  \RETURN \textbf{Q}
  \ENDIF
  \ENDWHILE
 \end{algorithmic}

% Add a box with a flowchart and/or pseudocode for the algorithm.

\section{Results and Supporting Statistics}
The system was tested using a standard wide-angle USB 2.0 camera (specifications elaborate on in sub-section C) and yielded a frame rate of 24 per second on average. Taking the reciprocal of the frame rate gives us the testing latency of the algorithm as a whole. This value came out to be 41.67 milliseconds. A comparison of this method with even a rudimentary artificial neural network with three hidden layers of twenty neurons with respect to the number of mathematical operations required to generate a prediction \cite{wang2003artificial} illustrates the advantage of such a set up, given that neural networks are the current industry standard for such tasks \cite{dollar2011pedestrian} \\

A snapshot of the execution sequence can be seen in figure 4 where \textbf{Q} was obtained as 0.63 - 0.12i + 0.31j + 0.62k, as visualized in figure 6. The accuracy of the same is verified in sub-section B 'Model Accuracy Verification'. All results obtained in this section were taken from the 25th test iteration of the novel algorithm after they were deemed satisfactory.

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.3]{out.png}}
\caption{A demonstration of the quaternion extraction from the pose using the novel algorithm.}
\label{fig3}
\end{figure}

\subsection{Testing Conditions}
% Emphasize on the type of scenes employed, how they were recorded and what was controlled here.
The scenes employed in the testing of the novel algorithm were intentionally chosen to reflect the systems intended use case according to the authors, i.e.; estimating the angle of orientation of a pedestrian object from the point of view of an autonomous robot. As such, they were made to contain multiple human objects to demonstrate the arbitrary selection of the skeletal frame with the highest confidence value due to the singularly threaded nature of the novel algorithm. \\

The domain taken for all angles of testing given the use case were all human pose objects with an orientation between 10 and 180 degrees with respect to the axis determined by the quaternion. As such, the test cases covered involved pedestrian objects approaching the robots path from 8 different orientations - 

\begin{itemize}[
    \setlength{\IEEElabelindent}{\dimexpr-\labelwidth-\labelsep}% Wrapping of text beyond first line of \item
    \setlength{\itemindent}{\dimexpr\labelwidth+\labelsep}% identation for each new \item
    \setlength{\listparindent}{\parindent}% Restore regular paragraph indentation
  ]
  \item Obliquely to the right, in the direction of the robots motion.
  \item Obliquely to the left, in the direction of the robots motion.
  \item Obliquely to the right, against the direction of the robots motion.
  \item Obliquely to the left, against the direction of the robots motion.
  \item Perpendicularly to the right, cutting across the path of the robot.
  \item Perpendicularly to the left, cutting across the path of the robot.
  \item Directly towards the robot.
  \item Facing away from the robot but still obstructing its path.
\end{itemize}

The angle domain of 10 to 180 degrees was maintained through cases where the pedestrain was facing both towards and away from the camera by extrapolating the direction of motion of the pedestrian object on the frame and classifying them into fuzzy states for the same.

\subsection{Model Accuracy Verification}
%Write about the model latency and brief about minor changes in the current system that could improve it.
%Provide pictures of the execution and highlight the latency on the frame. \cite{roszyk2022adopting}

Due to current resource limitations, the angle of orientation of the human pose object was extracted to verify the accuracy of the quaternion model by cross-checking the angle being faced by the user of the program as illustrated by equations 11 through 13. \\

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.3]{out2.png}}
\caption{A representation of a stationary human pose object taken for initial quaternion accuracy evaluation. Note that only the pose object itself and the shoulder landmarks need to be detected for all calculations following the same in the algorithm.}
\label{fig4}
\end{figure}

In a preliminary test of a stationary pose object as illustrated in figure 5, it was noted that the components of the quaternion were generated with the following variances and standard deviations over the time-span of the execution sequence of the stationary scene - \\

\begin{itemize}[
    \setlength{\IEEElabelindent}{\dimexpr-\labelwidth-\labelsep}% Wrapping of text beyond first line of \item
    \setlength{\itemindent}{\dimexpr\labelwidth+\labelsep}% identation for each new \item
    \setlength{\listparindent}{\parindent}% Restore regular paragraph indentation
  ]
  \item Real component variance and standard deviation : \\ 3.08e-4,  1.76e-2
  \item i component variance and standard deviation : \\ 9.83e-4, 3.14e-2
  \item j component variance and standard deviation : \\ 3.83e-4, 1.96e-2
  \item k component variance and standard deviation : \\ 2.39e-5, 4.88e-3
\end{itemize}

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.35]{quaternion.png}}
\caption{A representation of the quaternion generated from the pose estimate in figure 4 with the three vertices of the triangle denoting the positions of the shoulder points and the direction of orientation respectively. \cite{eater_sanderson_2018}.}
\label{fig10}
\end{figure}

\subsection{System specifications}
% Provide a brief overview of the specifications of the testing system.
This system was tested on a platform with the following specifications - 
\begin{itemize}[
    \setlength{\IEEElabelindent}{\dimexpr-\labelwidth-\labelsep}% Wrapping of text beyond first line of \item
    \setlength{\itemindent}{\dimexpr\labelwidth+\labelsep}% identation for each new \item
    \setlength{\listparindent}{\parindent}% Restore regular paragraph indentation
  ]
  \item Intel Core i5 7200U processor 
  \item 8GB RAM
  \item 512 GiB Solid State Drive
  \item External USB 2.0 30 FPS 2MP 'Passport' camera with a resolution of 1920x1080 and view angle of 110 degrees.
\end{itemize}

\subsection{Kalman filter to eliminate quaternion noise}
% Describe the general working of the 1D kalman filter and why it's dynamic along with a state diagram. Emphasize on why prediction doesn't happen using equations of angular motion.
In order to obtain the angles of orientation quoted in sub-section B, a Kalman filter in one dimension was applied to the theta readings in order to get a stable feed \cite{huang2009application}. \\

To re-iterate, the angle measurements were transformed as given in equation 13 to account for the fact that all values for theta were taken with respect to an axis horizontally bisecting the frame, which was effectively achieved by taking test cases of human pose objects with erect postures. Refer to figure 7 for a visualization of the same.

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.55]{kalman.png}}
\caption{A line graph representation of the filtered values from the theta measurement, referred to here as realAngle.}
\label{fig11}
\end{figure}

The filter designed was made to be dynamic for a system assumed to be perfect, i.e.; the predicted covariance equation assumed the model uncertainty to be zero. This was because of the random nature of the system and no viable physical equations present to accurately define the motion of the pose object. Hence, the filter was made to accept the average of the last 10 readings in the window as the prediction when the real measurement was deemed to be not viable. For scale, 24 measurements were calculated from the generated quaternion every second. \\

Equations 14 through 16 cover the updation of the Kalman gain, state and covariance respectively, where \textit{k} is the Kalman gain, \textit{p} is the covariance, \textit{r} was the angle measurement uncertainty (emperically determined as 0.5 after testing), \textit{x} was previous measurement and \textit{z} was the current measurement itself -
\begin{equation}\label{eq15}
\begin{split}
        k = \frac{p}{p+r} \!\ ; \!\ kalman \!\ gain \!\ calculation
\end{split}
\end{equation}

\begin{equation}\label{eq16}
\begin{split}
        x = x + k(z-x) \!\ ; \!\ state \!\ updation
\end{split}
\end{equation}

\begin{equation}\label{eq17}
\begin{split}
        p = (1-k)p \!\ ; \!\ covariance \!\ updation
\end{split}
\end{equation}

In order to demonstrate the translation of this system into use cases in the real world, we chose to build a preliminary prototype of a pedestrian intent classification system \cite{kwak2017pedestrian}, where the co-ordinates of a pedestrian object on the frame as well as take the theta output value mentioned earlier to were recorded in a window of values to project the future path of motion on to the same frame. The classification was done using a fuzzy state approach taking these inputs into account. A demonstration of this is illustrated in figure 8, where one should take note of the third value in the top-right of the overlay reading 172.04. This was the angle of orientation of the pedestrian object returned by the novel algorithm in an on-ground test. This can be visually verified by the fact that the referenced pedestrian object was walking almost perpendicularly across the projected path of the robot. Also note that the image here has been flipped horizontally, and hence does not seem consistent with the previously mentioned frame bisecting axis definition at first glance. \\

\begin{figure}[htbp]
\centerline{\includegraphics[scale=0.43]{demo2.JPG}}
\caption{A demonstration of the fuzzy pedestrian intent classification algorithm prototype implemented from a ground vehicle perspective to demonstrate the practicality of quaternion extraction from a 2-D camera frame containing a human skeletal pose object.}
\label{fig12}
\end{figure}

The intention classifier was able to accurately classify approximately 75 percent of the co-ordinates of the human pose object on the frame 2 seconds into the future with a radius of accuracy of 50 pixels.

\section{Conclusions and End Notes}
% Provide general overview of the conclusions
Therefore, the conclusion was drawn that the novel algorithm was suitable for extracting the pose of a human object using 2-D cameras and satisfied the requirements of preserving costs due to minimal hardware usage as well as those of latency and performance as the algorithm can be implemented on hardware \cite{hegarty2014darkroom} and performs within reasonable limits of accuracy. Notwithstanding the latter, its single threaded nature as well as low computational resource usage make it suitable for edge deployment applications, although it should be noted that the number of threads required increases linearly with the number of pose objects detected.

\subsection{Optimizations and Future Scope}
% Elaborate about an appropriate test beds and the points covered in the repository's README.md.
 The novel algorithm can be optimized in a variety of ways, with the primary approach being multi-threading of the video feed as well as the implementation of the algorithm for detecting multiple human objects and processing their pose estimations in parallel \cite{roszyk2022adopting}. Expanding on the use case mentioned in section IV, sub-section A 'Testing Conditions', the model can be used to implement a more elaborate version of the pedestrian intent classifier demonstrated in sub-section D of the same section with more reliable prediction of whether or not a pedestrian object will cross the path in front of an autonomous robot such that its motion may endanger the lives of the same \cite{volz2016data}. \\

Additionally, optimizing the algorithm to make use of the large core count of a GPU also has promising prospects \cite{laguna2009comparative} \cite{fatahalian2008closer}, although a deeper literature survey into the topic would have to be conducted to say so for sure.

\bibliography{references.bib}
\bibliographystyle{IEEEtran}\

\end{document}