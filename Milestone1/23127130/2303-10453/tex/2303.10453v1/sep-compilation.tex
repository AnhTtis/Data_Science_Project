\vspace{-0.25cm}

\section{A Separate Compilation Scheme}\label{sec:sep-compilation}

\vspace{-0.25cm}

A naive implementation of the language we have described can be
obtained by a compile-time inlining of accumulated modules. 
%This is,
%in fact, what is done by the current {\it Teyjus} compiler. 
We show in
this section that this approach can be refined into one where each
module is individually compiled and the inlining is carried out by a
later linking phase. Towards exposing the issues that have to be
addressed, we first outline the naive approach below. We then use
this context to develop the improved separate compilation scheme.

\vspace{-0.35cm}

\subsection{A Naive Implementation of Module Accumulation}

\vspace{-0.1cm}

Figure~\ref{fig:nestedaccums} presents a typical example of module
interactions that an implementation must be capable of handling. 
In this example, rather than explicitly displaying signatures, we have
marked constants as either global or local directly in the code of
modules. For simplicity, we have also elided type declarations. 

\begin{figure}[t]
\begin{center}
\begin{tabular}{lll}
\verb+module m1.+ & \verb+module m2.+ &\\
\verb+global r,w.+ & \verb+global r.+ & \\
\verb+[clauses in m1]+ & \verb+[clauses in m2]+ &\\
              &              &\\
\verb+module m3.+ & \verb+module m4.+ & \verb+module m5.+\\
\verb+accumulate m1.+ & \verb+accumulate m2.+ & \verb+accumulate m3,m4.+\\
\verb+local r.+ & \verb+local r, w.+& \verb+local q.+  \\
\verb+global w.+ & & \verb+global w.+ \\
\verb+[clauses in m3]+\hspace{50pt} &
             \verb+[clauses in m4]+\hspace{50pt} & \verb+[clauses in m5]+\\
\end{tabular}
\end{center}
\vspace{-0.35cm}
\caption{An example of nested accumulation}\label{fig:nestedaccums}
\vspace{-0.5cm}
\end{figure}

A naive implementation of our language can be obtained through a
process of inlining accumulated modules. However, this process has to
be careful about distinguishing constants that come from
different accumulated modules and must map them to
appropriately scoped ones in the larger module it constructs. A
schematic depiction of what such an inlining compiler must accomplish
appears in Figure~\ref{fig:inlined}. We have used here the
names \verb+r[1]+ and \verb+r[2]+ to distinguish the two constants
with name \verb+r+ that come from the modules \verb+m3+ and \verb+m4+
and we have similarly employed the names \verb+w[1]+ and
\verb+w[2]+ to differentiate between the global constant in module
\verb+m5+ and the local constant in module \verb+m4+ that share the
name \verb+w+.  Further, we have exploded the renaming process into a 
cascade of steps following the accumulation chain. For example, the
constant \verb+r+ appearing in the clauses of module \verb+m1+ is to
be renamed to the first local in the enclosing context (module
\verb+m3+) which is itself renamed to the second local in the
outermost context. In reality, an inlining compiler can collapse
this nesting by actually carrying out the sequence of renamings,
yielding a module with one set of global and local constants and a
collection of clauses from all the modules with the constant in them
appropriately identified. It can then proceed to compile the clauses with
complete knowledge of all the relevant predicate definitions. 

\begin{figure}[t]
\begin{center}
\begin{tabular}{l}
\verb+module m5.+\\
\verb+global w[1].+\\
\verb+local q, r[1], w[2], r[2].+  \\[3pt]
\verb+{accumulate m3 [r -> second local: r[1], w -> first global: w[1]]+\\
\verb+ global w.+\\
\verb+ local r.+\\
\verb+ [clauses from m3] with constant references suitably resolved+\\[3pt]
\verb+ {accumulate m1 [r -> first local: r, w -> first global: w]+\\ 
\verb+  [clauses from m1] with constant references suitably resolved}}+\\[5pt]
\verb+{accumulate m4 [r -> fourth local: r[2], w -> third local: w[2]]+\\
\verb+ local r, w.+\\
\verb+ [clauses from m4] with constant references suitably resolved+\\[3pt]
\verb+ {accumulate m2 [r -> first local: r]+\\
\verb+  [clauses from m2] with constant references suitably resolved}}+\\[5pt]
\verb+[clauses from m5] with constant references suitably resolved+\\
\end{tabular}
\end{center}
\vspace{-0.35cm}
\caption{The definition of module m5 after inlining
  accumulates}\label{fig:inlined}
\vspace{-0.5cm}
\end{figure}

\vspace{-0.35cm}

\subsection{A Separate Compilation Based Treatment}

\vspace{-0.1cm}

The previous model indicates the structure of the code that needs to
be produced prior to execution. We are interested, however, in
generating this code from compiled versions of each of the 
modules \verb+m5+, \verb+m4+, \verb+m3+, \verb+m2+ and \verb+m1+ that
have been generated without knowledge of where they are going to be
used and information at most of the signatures of modules that they
accumulate. In this situation
\vspace{-0.15cm}
\begin{enumerate}
\item the compiler will not have specific knowledge when compiling a
  potentially accumulated module of what the global and local constants
  are going to be mapped to in the enclosing context,

\item for predicates whose names are global and whose definitions are
  extendible, the compiler will have to produce code assuming that the 
  clauses in the module form an incomplete set and must be fitted
  into a larger context, and

\item the code that the compiler produces may have calls to predicates
  whose entry points cannot be determined at compilation time but must
  wait till the relevant assembly of modules is put together. 
\end{enumerate}
\vspace{-0.15cm}
The tasks of the inlining compiler will, under these circumstances,
have to be  divided between  a compiler that produces code for each
module separately but includes in such code suitable annotations that
allow it to be fitted into a larger context and a linker that uses the
``glue'' information with each module to build a complete bytecode
image of the system. We sketch the structure of these components below
that in combination achieve the desired result. For concreteness in
presentation, we will assume as a target low-level code that can be
run on an architecture closely related to the Warren Abstract Machine
(WAM) \cite{War83}. We assume familiarity with this machine structure
below. 

\vspace{-0.4cm}

\subsubsection{The Outcome of Compilation}

Given a module, the compiler that we envisage will produce a 
file for the linker that has the following items of information:

\vspace{-0.15cm}
\begin{enumerate}
\item A listing of the global constants that includes their names and
  other information such as their types that will be needed during
  execution. 

\item A listing of the local constants similar to that for the global
  ones but, this time, the names are not needed. 

\item A list of names for each accumulated module paired with a
  mapping from its global names (obtained from its signature) to 
  indices into either the local or global constant list for this
  module. 

\item A listing of the (indices of) externally redefinable predicates.

\item WAM-like code obtained by compiling the clauses presented in the
  module. Constant indices in this code will be indices into the lists
  in the header, to be patched up eventually by the linker. Calls to
  externally redefinable predicates also use indices into the listing
  of these predicates and will have to be filled in after the entry
  point for these has been finally determined.

\item A map from predicate names (represented by their indices) to
  their entry points in the code space. 
\end{enumerate}
\vspace{-0.15cm}
In the WAM setting, the code that is produced for individual clauses
defining a predicate is surrounded by instructions for sequencing
through choices and also indexing into them. The typical structure for
this is illustrated below:
\vspace{-0.15cm}
\begin{tabbing}
\qquad\=\verb+L11: +\=\kill
\>\>\verb+try_me_else L1+\\
\>\>\verb+switch_on_term V1,C1,Lst1,S1+\\
\>\verb+C1:+\>\verb+switch_on_constant CHT1+\\
\>\verb+S1:+\>\verb+switch_on_structure SHT1+\\
\>\verb+V1:+ \>\verb+try_me_else L12+\\
\>\> \verb+[code for one clause]+\\
\>\verb+L12:+\>\verb+retry_me_else L13+\\
\>\>\verb+...+\\
\>\verb+LLn:+\>\verb+trust_me+\\
\>\>\verb+[code for last clause]+\\
\>\verb+L1:+\>\verb+retry_me_else L2+\\
\>\>\verb+[code for another block]+\\
\>\>\verb+...+\\
\>\verb+Ln:+\>\verb+trust_me+\\
\>\>\verb+[code for last block]+
\end{tabbing}
\vspace{-0.15cm}
At the outermost level, this code captures a possible sequencing
through different chunks of clauses. Each chunk corresponds to a
subsequence over which indexing may be useful. One component of this
``indexed'' subsequence pertains to the variable case that must
support a simple sequencing through the entire collection. The other
possibility, corresponding to lists or hashing on constant name or
structure names, is that only some of the clauses in the sublist are
relevant. In this case, auxiliary sequencing code using the
instructions \verb+try+, \verb+retry+ and \verb+trust+ would be
generated. These possibilities are not specifically illustrated above
but are nonetheless relevant to the discussions that follow.

\vspace{-0.4cm}

\subsubsection{The Linking Process}

Linking begins by creating a frame for a flattened image of the
top-level module to be filled out by functions that map global
and local constants to runtime indices, recursively load the
accumulated modules and, finally, add the code to the frame. 

The mapping of constants to runtime indices follows the cascading
structure of the inlining compiler, except that this is done at
linking time. In some detail, each local constant in the chain of
accumulates translates into a unique index. The global constants of
the top-level module also are assigned unique indices. Finally, the
assignments for the local and global constants of the parent
module and the associated renaming functions determine the indices of
global constants of accumulated modules.

An issue that is important in preparing the code for addition to the
frame is that of combining predicate definitions: different modules 
may provide pieces of the definition of a predicate and these need to
be assembled together. To begin with, there must be a fixed order
governing the assembly---in $\lambda$Prolog, for instance, clauses from
the accumulated modules appear first in the order of accumulation
followed by those in the parent module---and this is adhered to by the
linker. Now, the code that is generated for a predicate in each module
has the structure of a list and a natural first step towards
integration is appending separate lists together. Ignoring for the 
moment the existence of indexing in the WAM code, it is easy to
see how this might be done. For instance, suppose that two clauses
sequences that have been compiled into the forms shown in the left and
right halves of the display below have to be combined:
\vspace{-0.15cm}
\begin{center}
\begin{tabular}{llll}
\verb+L1: +& \verb+try_me_else L2+ &\verb+L4 + &\verb+try_me_else L5+\\
& \verb+[code for a block]+\qquad\qquad & & \verb+[code for a block]+\\
\verb+L2: + &\verb+retry_me_else L3+& \verb+L5: + &\verb+retry_me_else L6+\\
& \verb+[code for a block]+\qquad\qquad & & \verb+[code for a block]+\qquad\qquad\\
\verb+L3: + &\verb+trust_me+& \verb+L6: + & \verb+trust_me+\\
& \verb+[code for a block]+\qquad\qquad & & \verb+[code for a block]+\qquad\qquad\\
\end{tabular}
\end{center}
\vspace{-0.15cm}
This combination can be realized by changing the \verb+trust_me+
  instruction that precedes the last block of the first list into a
  \verb+retry_me_else+ instruction pointing to the start of the next
definition and by changing the first instruction of that collection
  into a \verb+retry_me_else+ to yield the following:
\vspace{-0.2cm}
\begin{center}
\begin{tabular}{llll}
\verb+L1: +& \verb+try_me_else L2+ &\verb+L4 + &\verb+retry_me_else L5+\\
& \verb+[code for a block]+\qquad\qquad & & \verb+[code for a block]+\\
\verb+L2: + &\verb+retry_me_else L3+& \verb+L5: + &\verb+retry_me_else L6+\\
& \verb+[code for a block]+\qquad\qquad & & \verb+[code for a block]+\qquad\qquad\\
\verb+L3: + &\verb+retry_me_else L4+& \verb+L6: + & \verb+trust_me+\\
& \verb+[code for a block]+\qquad\qquad & & \verb+[code for a block]+\qquad\qquad\\
\end{tabular}
\end{center}
\vspace{-0.15cm}

The indexing optimization in the WAM complicates matters a little
because some elements of the top-level sequence may be indexed
blocks. If we were to simply append the top-level sequences as
suggested, the new sequence may have two adjacent blocks of this
kind. This is undesirable: it may mean, for instance, that
we end up keeping a choice point on the stack when one is not really
needed. Fortunately, it is possible to avoid this. We can 
determine if this will occur by examining the last element of the first
sequence and the first element of the second sequence. If they are both
indexed blocks, then we proceed to merge them. 

One problem to be addressed in generating a single indexed block is,
once again, that of merging two segments of code that represent
sequencing through clauses. For the ``main'' sequences
corresponding to the variable case, this can be effected as for
top-level sequences. To treat the case when these may be sequences 
realized through \verb+try+, \verb+retry+ and \verb+trust+
instructions, we augment the instruction set with two new instructions
called \verb+try_else+ and 
\verb+retry_else+. These instructions behave like \verb+try+ and
\verb+retry+ except that that they
take an additional argument that provides the address of the code to
try upon on backtracking. Suppose now that the two blocks of
sequencing code that we need to merge are the following:
\vspace{-0.4cm}
\begin{center}
\begin{tabular}{llll}
\verb+S1: + & \verb+try L1+ &\verb+S2: +& \verb+try L4+ \\
& \verb+retry L2   +\qquad\qquad & &\verb+retry L5+\\
& \verb+trust L3+& & \verb+trust L6+
\end{tabular}
\end{center}
\vspace{-0.15cm}
This merging can be realized by changing the code to the following:
\vspace{-0.15cm}
\begin{center}
\begin{tabular}{llll}
\verb+S1: + & \verb+try L1+ &\verb+S2: +& \verb+retry L4+ \\
& \verb+retry L2+ & &\verb+retry L5+\\
& \verb+retry_else L3,S2+\qquad\qquad& & \verb+trust L6+
\end{tabular}
\end{center}
\vspace{-0.15cm}
The \verb+try_else+ instruction is needed in implementing this idea in
the case where the first block corresponds to a unique clause choice. 

The other problem that needs to be dealt with in combining indexing
blocks is that of merging hash tables corresponding to constant and
structure names. This is easy to do. The compiler can actually emit
the separate tables simply as lists of pairs of constant names and
corresponding entry points to code. The linker can determine from this
which lists have to be merged and then emit the merged lists, which are 
used by the emulator to generate the hash tables.

The last aspect that the linker must resolve is the (relative) code
location for predicates that could not be finalized at compile time.
After the combining of all the clause code has been completed, a map
is available from each predicate name in our universal namespace to
the location of its definition. These addresses can now be patched in
at the appropriate places.
