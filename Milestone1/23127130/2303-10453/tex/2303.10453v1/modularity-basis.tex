\vspace{-0.25cm}

\section{The Modules Language and Its Logical
  Interpretation}\label{sec:modularity-basis} 

\vspace{-0.25cm}

In developing real programs, it is necessary, first of all, to
identify a vocabulary of types and term constants. New type
constructors are defined in our language through 
{\it kind} declarations that have the form 
\vspace{-0.15cm}
\begin{tabbing}
\qquad\=\kill
\>\verb+kind  tyc1, ..., tycn  type -> ... -> type.+
\end{tabbing}
\vspace{-0.15cm}
where the arity of the constructors \verb+tyc1+,$\ldots$,\verb+tycn+
is one less than the number of occurrences of \verb+type+ in the
declaration. Term constants are identified through {\it type} declarations of 
the form 
\vspace{-0.15cm}
\begin{tabbing}
\qquad\=\kill
\>\verb+type  c1,...,cn  <type expression>.+
\end{tabbing}
\vspace{-0.15cm}
where the type expression is constructed using the available type
constructors.  

The notion of scope for kind and type declarations is important in
constructing type and predicate definitions. Modules impart a
structure to this space of names. Formally, a module begins with a
declaration of the form 
\vspace{-0.15cm}
\begin{tabbing}
\qquad\=\kill
\>\verb+module <name>.+
\end{tabbing}
\vspace{-0.25cm}
and continues with the kind, type and predicate definitions to be
associated with the indicated name. We adopt a file oriented view
of modules here: all the code defining a module named \verb+foo+
is to be found in a file with the name \verb+foo.mod+. Now, the 
boundaries of a module determine a textual notion of scope in that the
kind and type declarations that appear within them are interpreted as
ranging over all the other declarations contained in the
module. Consistent with this viewpoint, these boundaries also provide a
delimiting region for analyses that a compiler might perform in the
course of translating a source language program.  

The following definition of a module called \verb+store+ illustrates
module syntax:
\vspace{-0.25cm}
\begin{tabbing}
\qquad\=\kill
\>\verb+module  store.+\\
\>\verb+kind  store  type -> type.+\\
\>\verb+type  emp  (store A).+\\
\>\verb+type  stk  A -> (store A) -> (store A).+\\
\>\verb+type  init  (store A) -> o.+\\
\>\verb+type  add, remove  A -> (store A) -> (store A) -> o.+\\
\>\verb+init emp.+\\
\>\verb+add X S (stk X S).+\\
\>\verb+remove X (stk X S) S.+
\end{tabbing}
\vspace{-0.15cm}
This module identifies a representation for stores with three
associated operations, one for initializing a store and two others for
adding an element to and removing an element from an existing
store. Towards providing a definition that is parametric in the type
of the elements stored, this module declares a unary type constructor
for stores that is then used in the types of constants that implement
store representations. The particular realization of a store embedded
in this code is based on the idea of a stack. The last three lines in
the module are $D$ formulas that define the desired operations based
on this interpretation. The tokens that begin with uppercase letters 
stand, as usual, for variables that are implicitly universally
quantified at the head of the formula. 

Not all the declarations in a module are typically intended to be
externally visible. With the module \verb+store+, for example, it is
sensible to hide the actual representation of stores, requiring these
to be manipulated opaquely through the predicates \verb+init+,
\verb+add+ and \verb+remove+. In our language, the contents of a
module that are to be visible to the outside must be explicitly
identified through a signature that shares its name with the
module. Formally, a signature begins with a declaration of the form  
\vspace{-0.25cm}
\begin{tabbing}
\qquad\=\kill
\>\verb+sig <name>.+
\end{tabbing}
\vspace{-0.25cm}
and continues with the kind and type declarations to be associated
with the specified name. Thus, the following declarations constitute a  
signature that imposes the kind of view desired on the module \verb+store+:
\vspace{-0.25cm}
\begin{tabbing}
\qquad\=\kill
\>\verb+sig  store.+\\
\>\verb+kind  store  type -> type.+\\
\>\verb+type  init  (store A) -> o.+\\
\>\verb+type  add  A -> (store A) -> (store A) -> o.+\\
\>\verb+type  remove  A -> (store A) -> (store A) -> o.+
\end{tabbing}
\vspace{-0.1cm}

There is an obvious consistency requirement with signatures: they 
must identify all the type constructors that are needed
to sensibly interpret the types that appear in them. A syntactically
well-formed signature actually functions as an interface definition
for a module: a compiler must treat this as
a complete description of all the type and kind declarations emanating
from the module in any context where the module is used and it
must correspondingly check that these declarations agree with what is 
actually present in the module. In support of this kind of type
checking role, we shall adopt a file oriented view of signatures
similar to that for modules: the definition of the signature named 
\verb+foo+ is to be found in a file called \verb+foo.sig+.

We have thus far considered the {\it static semantics} of modules and
signatures. The {\it dynamic semantics} is explained by a translation
into an $E$ formula and a subsequent use of the computation model for
the core language.  This translation is actually quite simple: the
entire module is to be thought of as a, perhaps large, $E$ formula
that is obtained by conjoining all the $D$ formulas contained in it
and then existentially quantifying over all the local constants---{\it
i.e.} the constants not mentioned in the signature---in this
formula. Under this approach, the logical essence of the module
\verb+store+, for instance, is reduced to the formula 
\vspace{-0.15cm}
\begin{tabbing}
\qquad\=$\exists Emp \exists Stk ($\=\kill
\>$\exists Emp \exists Stk ($\>$(init\ Emp)\ \land$\\
\>\> $\forall X \forall S\, (add \ X \ S\ (Stk\ X \ S))\ \land$\\
\>\> $\forall X \forall S\, (remove\ X \ (Stk\ X\ S)\ S)).$
\end{tabbing}

The simplest form of module use occurs at the interaction level. In
particular, the attempt to solve a goal at the top level is always
made relative to a chosen module. In keeping with the preceding
discussion, from a lexical perspective, the
significance of such a relativization is that all the kind and type
declarations in the signature of the module become available in 
analyzing the syntax of the goal. The
computational significance of the relativization, on the other hand,
is that the goal is to be solved using only the definitions of global
predicates in the module and assuming an access at most to its global
constants. This interpretation actually implies a strong form of
hiding. Specifically, suppose that we are trying to solve the goal $G$ from the
module $M$. It is obvious that $G$ cannot refer directly to any
constant local to $M$. A little less obvious is the fact that
these local constants cannot also percolate out of $M$ in the form of
computation results. Thus, suppose that $G$ is a goal whose
only free variable is $y$. The results computed for this query are
then the instantiations for $y$ that lead to successful solutions of
$G$. The translation semantics for modules and the associated
operational semantics now clearly preclude
the appearance of the local  constants of $M$ in instantiations for
$y$.\footnote{An issue 
different from semantics is the efficient treatment of such
constraints on instantiations. We have shown elsewhere how to do this  
by assigning numeric labels to logic variables and constants and
by using these labels in unification \cite{Nad92int}.}

An important aspect of a module system is the mechanisms it provides
for realizing interactions between units of code. In our system, this
ability is obtained from a single operation referred to as {\it
  module accumulation}. The combination of modules through this
operation is achieved by placing a declaration of the form
\vspace{-0.15cm}
\begin{tabbing}
\qquad\=\kill
\>\verb+accumulate  M1, ..., Mk.+
\end{tabbing}
\vspace{-0.15cm}
in a new module being constructed, assuming \verb+M1+, ...,
\verb+Mk+ are module names. The lexical effect of this declaration is
to provide access to the signatures of the modules \verb+M1+, ...,
\verb+Mk+ within the new module. The dynamic semantics 
of this construct is, once again, specified by
recourse to the logical essence of modules. Let $E_1,\ldots,E_k$ be
the formulas corresponding to the modules
\verb+M1+, ..., \verb+Mk+. Then, the formula corresponding to
the new module is obtained by first conjoining $E_1,\ldots,E_k$ with
the formula obtained for the module by ignoring the accumulation and
subsequently raising the existential quantification at the head of
each of the $E$ formulas to scope over the entire conjunction; of
course, the outward movement of existential quantifiers must be done
in a logically correct way, {\it e.g.}, $(\exists x D_1(x)) \land (\exists
x D_2(x))$ should translate to $\exists y \exists z (D_1(y) \land
D_2(z))$, with $x$ in $D_1$ and $D_2$ being renamed to the distinct
and fresh variables $y$ and $z$. At a programming 
level, this logical 
interpretation is tantamount to treating accumulation as the inlining
of code in all the accumulated modules, taking care, however, to
preserve the locality of names within each.

An accumulated module could, of course, itself be accumulating other
modules. In this context, the logical interpretation of this construct
makes sense only when chains of accumulates do not close back on
themselves. The syntax of the language accordingly prohibits such
cycles.\footnote{Note that two modules that are accumulated into the
same context {\it can} utilize predicates defined in each other and
thus be mutually dependent. Prohibiting cycles in accumulation
chains is, therefore, {\it not} the same as disallowing such
dependencies.} In a separate compilation model, this is a constraint
that can only be checked at linking time.

We have at this point described the essential structure of the
modules language in its {\it entirety}: A module corresponds
syntactically to a possibly large $E$ formula obtained by combining
its clauses, its signature and the formulas corresponding to its
accumulated modules in the manner just described. The dynamic
semantics of the module is then explained {\it completely} and {\it
precisely} through this formula and the operational semantics for the
core language. The next section adds
further syntactic sugar and some compiler-oriented annotations to this
language without modifying the logical core.
