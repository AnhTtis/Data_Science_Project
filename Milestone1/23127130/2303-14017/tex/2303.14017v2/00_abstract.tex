\begin{abstract}
% target 和 reference的对应
% transfrom和transfer对应
% cfm里概念太多。
% Content and style disentanglement is an effective way to achieve few-shot font generation. It allows to transfer the style of the font image in a source domain to the style defined with a few reference images in a target domain. However, the content feature extracted using a representative font might not be optimal. In light of this, we propose a content fusion module~(CFM) to project the content feature into a linear space defined by the content features of basis fonts, which can take the variation of content features caused by different fonts into consideration. Our method also allows to optimize the style representation vector \yunqi{of} reference images \yunqi{through} a lightweight  iterative style-vector refinement~(ISR) strategy. Moreover, we treat the 1D projection of a character image as a probability distribution and leverage the distance between two distributions as the reconstruction loss~(namely projected character loss, PCL). Compared to L2 or L1 reconstruction loss, the distribution distance pays more attention to the global shape of characters. \wc{We have evaluated our method on a dataset of 300 fonts with 6.5k characters each.} Experimental results verify that our method outperforms existing state-of-the-art few-shot font generation methods by a large margin. 
Content and style disentanglement is an effective way to achieve few-shot font generation. It allows to transfer the style of the font image in a source domain to the style defined with a few reference images in a target domain. However, the content feature extracted using a representative font might not be optimal. In light of this, we propose a content fusion module~(CFM) to project the content feature into a linear space defined by the content features of basis fonts, which can take the variation of content features caused by different fonts into consideration. Our method also allows to optimize the style representation vector of reference images through a lightweight  iterative style-vector refinement~(ISR) strategy. Moreover, we treat the 1D projection of a character image as a probability distribution and leverage the distance between two distributions as the reconstruction loss~(namely projected character loss, PCL). Compared to L2 or L1 reconstruction loss, the distribution distance pays more attention to the global shape of characters. We have evaluated our method on a dataset of 300 fonts with 6.5k characters each. Experimental results verify that our method outperforms existing state-of-the-art few-shot font generation methods by a large margin. The source code can be found at \url{https://github.com/wangchi95/CF-Font}.
% To solve this problem, we propose a content fusion module~(CFM) and a projected distance loss~(PDL). In the CFM, a basis font library is first built based on the content features. % of training fonts.
% And then we approximate the content features of the target font by the fusion of these basis fonts. 
% The fused features are utilized to replace the common content features, which are obtained from a single source image. 
%The common content features from a single source image are then replaced with fused features from basis images, which have a larger content representation space.
% PDL is a skeleton-sensitive reconstruction loss that aligning the character image distributions. We project the characters into multi-direction 1D spaces for calculation because of the computational complexity of the 2D distribution. In addition, to further improve the quality of produced images (especially local details), we propose a lightweight style iterative inference~(SII) strategy to polish the style of one font. Experiments prove that our method exceeds existing state-of-the-art methods. 
% by a large margin?
\end{abstract}