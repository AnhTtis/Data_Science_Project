\section{Experiments}
\input{31_tab_vssota}
\input{21_fig_vssota}
\wc{We have implemented the CF-Font method on a GPU server with 8 Nvidia Tesla V100 GPUs. After training with our dataset, our method outperforms the state-of-the-art methods on unseen fonts by 5.7\% and 5.0\% with respect to L1 and FID metrics, respectively. In the following, we report the preparation of dataset, evaluation metrics, and various experimental results to verify the effectiveness of our method.}
\subsection{Dataset and Evaluation Metrics}
\label{ssec:data}
We collect 300 Chinese fonts to build a dataset (including printed and handwriting fonts) to verify our method for the Chinese font generation task.  \wc{Our character set~(6446 in total) covers almost the full standard Chinese character set~(6763 in total) of GB/T 2312~\cite{CJKV}, and 317 characters not supported by comparison methods are removed.} The training part contains 240 fonts, and each font has 800 characters. The test part consists of (a) 229 seen fonts with 5646 unseen characters; (b) the remaining \wc{60} unseen fonts with \wc{5646} unseen characters, to verify the generalization ability of models. Note that we exclude 11 of the \wc{240} training fonts when testing on seen fonts. They are basis fonts (including \emph{Song}) in CFM and a font \emph{Kai}, in which \emph{Song} and \emph{Kai} are commonly used as source fonts in font generation~\cite{lffont_aaai21, emd_cvpr18, DGFont_cvpr21, cggan_cvpr22}. Besides, for few-shot font generation, reference images of target fonts in the test are with 16 randomly picked characters from the training part. 

We leverage pixel-level and perceptual metrics for evaluation, following \cite{DGFont_cvpr21}. Specifically, pixel-level metrics are L1, root mean square error~(RMSE), and structural similarity index measure~(SSIM). They focus on per-pixel consistency between generated images and ground-truth ones. Perceptual metrics include FID~\cite{fid_nips17} and LPIPS~\cite{lpips_cvpr18}, both of which measure the similarity of features and are closer to human vision.

\subsection{Implementation Details}
We train our model using Adam~\cite{DBLP:journals/corr/KingmaB14} with $\beta_1=0.9$ and $\beta_2=0.99$ for the style encoder, and RMSprop~\cite{RMSprop} with $\alpha=0.99$ for the content encoder. The learning rate and weight decay are both set as $10^{-4}$. The hyper-parameters for loss are $\lambda_{img}=\lambda_{cnt}=0.1$, $\lambda_{offset}=0.5$, and $\lambda_{pcl}=0.01$ ($0.05$ for PC-KL). For PCL, we orthographically project a character image onto 12 straight lines, which cross at the image center and divide the 2D space evenly. We resize all images to 80 Ã— 80 and train the model with a batch size of 32. \wc{The whole training takes about 15 hours.} 
We first train the \wc{DGN} for 180k iterations to obtain the learned content features. 
Then we cluster these content features into ten groups and select basis fonts by the distance to cluster centers. 
\wc{After that, the model with CFM is further trained for another 20k iterations.}
For fairness, the models without CFM in ablations are trained for 200k iterations. 


\subsection{Comparison with State-Of-The-Art Methods}
We compare our model with \wc{six} state-of-the-art methods, including an image-to-image translation method~(FUNIT\cite{DBLP:conf/iccv/0001HMKALK19}), four component-related methods~(LF-Font~\cite{lffont_aaai21}, MX-Font~\cite{mxfont_iccv21}, CG-GAN~\cite{cggan_cvpr22}, FsFont~\cite{fsfont}), and DG-Font~\cite{DGFont_cvpr21}. \wc{We slightly modify the network of CG-GAN to fit the input image size and the few-shot setting.} 
To be fair, we try each of our basis fonts and font \emph{Kai} as the source font for these comparison methods and report their best results in the following part (see details in our supplementary).

\input{32_tab_pcs}
As Tbl.~\ref{tbl:vs_sota} illustrates, our method outperforms other methods, especially on unseen fonts. 
\wc{DG-Font
leads other comparison methods except on perception metrics. 
But when added our proposed modules, its LPIPS and FID scores get a significant boost and catch up with others both on seen and unseen fonts. Although FUNIT achieves the best FID score on seen fonts, it performs worse on other metrics}.
Fig.~\ref{fig:vs_sota_big} displays the qualitative comparison. Characters generated by ours are of high quality in terms of style consistency and structural correctness. The results of FUNIT, LF-Font, MX-Font, \wc{Fs-Font, and CG-GAN} often have structural errors and incompleteness. 
\wc{Fs-Font select several reference characters from a reference collection through a content-reference mapping, the relationship between a character and its references with common conspicuous components. The reference collection contains around 100 characters and covers almost all components. However, our reference characters are randomly selected and fixed for all source characters, with poor component coverage. Thus, the performance of Fs-Font is not perfectly shown in our few-shot setting.} 
The outputs of DG-Font are great overall but suffer from artifacts and incomplete style transfer. 

\input{22_fig_vswcs}

\paragraph{User study.} \crc{We conduct a user study to further compare our model with other methods.
We randomly selected 40 font styles~(30 seen fonts and 10 unseen fonts) from the test set, and for each style, 5 test characters were randomly selected. Corresponding character images are generated with our method and the other 6 comparison methods. 20 participants who use Chinese characters every day are asked to pick the best group~(5 character images yielded by one method) for one test style.
Here, the order of these groups is randomly shuffled and we allow multiple choices since the participants might think several synthesized characters are of comparable quality. The results of user study are shown in the last column of \ref{tbl:vs_sota}, which present that our CF-Font gains the highest user preference 21.58\%, surpassing the second place CG-GAN 16.67\% by a large margin.}

\begin{figure}[t]
  \begin{center}
  \vspace{-10pt}
    \includegraphics[width=0.3\textwidth]{Figs/top1_vi.png}
  \end{center}
  \vspace{-20pt}
  \caption{Comparison between content fusion and retrieval strategy. B represents the baseline (DG-Font), and other notations are the same as Tbl.~\ref{tbl.wcs}.}
  \label{fig:nearest_basis}
\vspace{-10pt}
\end{figure}

\subsection{Ablation Studies}\label{Ablation Studies}

\wc{This subsection shows the effects of all proposed components and discusses how CFM and PCL work in font generation.}
\paragraph{Effectiveness of different components.}
We separate the proposed modules and sequentially add them to DGN to observe the effects of each. The quantitative results can be seen in Tbl.~\ref{tbl.wcs}, \wc{verifying} that PCL, CFM, and ISR all can help improve the quality of generated images. These modules bring not only a numerical improvement but also \wc{a noticeable}
improvement in the visual aspect of geometric structures and stylistic strokes, as displayed in Fig.~\ref{fig:vs_wcs}. In the fourth-to-last line, PCL shows its ability to improve character semantics and skeletons. 
\wc{Moreover,} CFM makes the generated results a big step closer to the target in human perception. In the penultimate line, ISR further refines the detail of results by enhancing the stylistic representation.

\begin{figure}[t]
    \centering
 	\includegraphics[width=\linewidth]{Figs/vis_cf_barv2.pdf}
    \vspace{-15pt}
    \caption{Visualization of weights on basis fonts. We take the character ``Tong" for example. The left column represents the basis fonts, and the top row shows a part of training fonts. The weight on basis fonts of one training font are displayed as a vertical histogram.}
    \vspace{-5pt}
    \label{fig:vis_cf}
\end{figure}

\paragraph{Comparison between content fusion and retrieval strategy.}


Among these modules, CFM is the most efficient one. 
We further analyze where the gain of CFM comes from through a comparison with the retrieval strategy, \ie during the test, we select the closet font for every target font as input from basis fonts and the whole training set (except the target font itself, \ie 239/240 fonts in total for each seen/unseen target font) respectively. The quantitative result is shown in the second to fourth row from the bottom of Tbl.~\ref{tbl.wcs}. It indicates that the result of inputting the closet basis font is much worse than that of content fusion, or even worse than the baseline (using a stand font \emph{Song} all the time). 
\wc{Meanwhile, retrieving the closest font from the whole set gets a comparable results with CF-Font on seen fonts, but not good as it on unseen fonts and FID metrics.}
As Fig.~\ref{fig:nearest_basis} illustrates, the closet font may still be very different on the character skeleton from the target one and will introduce some noises (parts mismatched to the target skeleton). With these observations, we claim that content fusion matters rather than retrieving a close font in CFM.

\paragraph{Variations of PCL.} We use two variations of PCL, PC-WDL, and PC-KL, to train a model respectively. Tbl.~\ref{tbl:pcl} shows the result on unseen fonts and demonstrates that not only PC-WDL, 
PC-KL can also improve the network performance. 
\wc{PC-KL and PC-WDL have similar improvements on pixel-level metrics, but PC-WDL has obvious advantages in FID while PC-KL performs better on LPIPS.}
We attribute this to that benefit from character projection, both of the distribution distance metrics can focus on the global properties, such as skeleton topology. 

\subsection{Evaluation of Basis Selection.}
We visualize the basis fonts and \wc{the corresponding} weights of content fusion here. Taking the character ``Tong" as an example, in Fig.~\ref{fig:vis_cf}, ten images of basis fonts are shown in the left column, fifteen target images with randomly selected fonts are listed in the top row, and the weights of content fusion are plotted in the form of a vertical histogram. We can observe that (a) the basis fonts selected by clustering are indeed visually different from each other (they also can be chosen manually), which means that they are capable of building a space for content fusion; (b) the greater the weight value, the corresponding basis font is more similar to the target font and this proves that content fusion is physically meaningful; (c) the values of these weights are scattered rather than concentrated in a particular basis font, which can also be a reason why the retrieval strategy fails as described in subsection~\ref{Ablation Studies}.

\input{33_tab_pcl}

\begin{figure}
    \small
    \centering
% 	\scalebox{0.6}{
    \resizebox{0.7\linewidth}{!}{
	\begin{tabular}{l}
		\toprule
		\ \ Source \ \ FUNIT \ LF-Font \ MX-Font \ Fs-Font \ CG-GAN \ DG-Font \ CF-Font \ Target \\
		\midrule
        \includegraphics[width=1.3\linewidth]{Figs/exp/failure_case_cvpr_cggan_seen0.png} \\
        \bottomrule
	\end{tabular}
	}
\vspace{-5pt}
\caption{Failure case.}\label{fig:failure}
\vspace{-15pt}
\end{figure} 
\subsection{Failure Cases and Limitations}
Fig.~\ref{fig:failure} illustrates a case of generated images of complex characters with many strokes and a tight layout. Although our method works relatively well, many structural errors appear in the first row and some strokes are missed in the second row.