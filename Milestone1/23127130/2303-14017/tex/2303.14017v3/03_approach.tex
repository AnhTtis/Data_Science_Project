% \begin{figure*}[!t]
%     \centering
%     \resizebox{\linewidth}{!}{
%  	\fbox{\includegraphics[width=\textwidth]{Figs/ALL.pdf}}
%  	}
%  	\vspace{-4mm}
%     \caption{The framework of our model. (a) We first train the base model following DG-Font~\cite{DGFont_cvpr21} and use PCL to enhance the supervision of character skeletons. (b) After the model converges, content features of all training fonts are clustered and basis fonts are selected according to cluster centers. The original content encoder is replaced by CFM, and original content features are changed to fused features of basis fonts. Then we continue to train the model so that it adapts to fused content features. (c) In inference, we utilize SII to polish the style of a font. 
%     % The mean style vector is extracted, and 
%     \wc{The extracted mean style vector is}
%     treated as the only \wc{trainable} variable to be fine-tuned for a few iterations.}
%     \label{fig:flowchart}
%  	\vspace{-5mm}
% \end{figure*}

\section{Approach}

% Overview
Our method is illustrated in Fig.~\ref{fig:flowchart}. 
The whole training pipeline can be divided into \wc{two} stages. 
Firstly, we train \wc{the neural network in DG-Font~\cite{DGFont_cvpr21} as our base network, referred to as DGN.}
\wc{The network is used to learn basic, disentangled content and style features of character images in our dataset.}
Secondly, \wc{our content fusion module~(CFM) is plugged into the model after the content encoder. 
Afterward, we replace the original content feature with the output of CFM, a linear content-feature interpolation of automatically-selected basis fonts. 
Then, we fix the content encoder and continue to train style encoder, feature deformation skip connection~\cite{DGFont_cvpr21}~(FSDC) and mixer together for a few epochs.}
The projected character loss~(PCL) is used in training to supervise character skeletons. 
\wc{In addition, to further improve the generation quality, we utilize the iterative style-vector refinement~(ISR) strategy to polish the learned font-level style vector alone in inference.} \crc{The motivation for ISR is seeking for a single and high-quality font-level style vector to generate images for all characters of the font.
Specifically, for a font, we refine upon the average of the character style vector of all the given 16 characters in our few-shot setting. 
}

\subsection{Base Network} 
As illustrated in Fig.~\ref{fig:flowchart}~(a), given a content image $\boldsymbol{I}_c$ and a style image $\boldsymbol{I}_s$, the DGN synthesizes an image with the character of the content image and the font of the style image. This generative network consists of four parts: a style encoder $f_{se}$ to extract style latent vector $\boldsymbol{s}$, a content encoder $f_{ce}$ to obtain content feature map $\boldsymbol{C}$, a mixer $f_m$ to mix style and content representations with AdaIN~\cite{DBLP:conf/iccv/HuangB17}, and two FSDC modules.
During training, a multi-task discriminator, fed with generated characters and their ground-truth images, is applied to conduct discrimination for each style simultaneously.

Four losses are adopted: 1) image reconstruction loss $\mathcal{L}_{img}$ for domain-invariant features maintaining; 2) content consistent loss $\mathcal{L}_{cnt}$ to guarantee consistency between generated and input content images; 3) adversarial loss $\mathcal{L}_{adv}$ in hinge version~\cite{DBLP:journals/corr/LimY17,DBLP:conf/iclr/MiyatoKKY18,DBLP:conf/icml/ZhangGMO19} for realistic image generation; 4) deformation offset normalization $\mathcal{L}_{offset}$ to avoid excessive offsets in FDSC. More details are in \cite{DGFont_cvpr21}.

\begin{figure}[t]
  \begin{center}
    \fbox{\includegraphics[width=0.8\linewidth]{Figs/CF_vis.png}}
  \end{center}
  \caption{Visualization of content fusion. The yellow and red arrows are denoted for content features from the commonly used source font \emph{Kai}~\cite{lffont_aaai21} and the nearest font of the target respectively. The blue arrow represents the interpolation of content features of basis fonts to approximate the target.}
  \label{fig:single_vs_multi}
\end{figure}

\subsection{Content Fusion Module}
% f_{ce}(.): Content Encoder
% f_{se}(.): Style Encoder
% f_{m}(.): Mixer
% S: style vector
% C: content feature map
% G, D: Generator discriminator
% \mathcal{K}(.): Kmeans algorithm
% K: kmeans centers
% k: Font number
% e: embedding
% b: bases
% n: bases number
% \sigma: softmax
% I_s, I_c: image
% ^i_j: font i char j
The content fusion module aims to adaptively extract content features by combining the content features of basis fonts. This network with CFM is constructed as in Fig.~\ref{fig:flowchart}~(b).
\wc{Firstly, to find representative fonts for content fusion, we cluster all training fonts through the concatenated content features of \crc{the given 16} few-shot characters and pick those nearest to the cluster centers as basis fonts. The basis fonts are fixed once selected.}
Then, for each target font, we calculate an L1-norm content fusion weight according to its similarity to basis fonts. 
As a result, the content features (input of the decoder) are replaced by the weighted sum of those of basis fonts. 
In addition, the network should be fine-tuned for a few epochs to adapt to fused content features~\wc{(represented as the blue circles in Fig.~\ref{fig:single_vs_multi})}.

\paragraph{Basis selection.} 
%The fonts are clustered according to their contents. 
Suppose we need to choose $M$ basis fonts from $N$ training fonts. 
It can be realized by clustering the content features $\{\boldsymbol{C}_i\}_{i=1}^N$ and selecting fonts lying in the cluster centroids. 
In our practice, since the dimension of $\boldsymbol{C}_i$ is too large while $N$ is relatively small, we follow~\cite{thrun2021exploitation} to map $\boldsymbol{C}_i$ to the vector of the distances between it and features of all fonts $\boldsymbol{e}_i\in{\mathbb{R}^N}$. More specifically:
\begin{equation}
\label{eq:bs}
\begin{aligned}
      \boldsymbol{C}_i &= f_{ce}(\boldsymbol{I}_i),\footnotemark \\[1mm]
      \boldsymbol{d}_i &= (d_{i1}, d_{i2}, ..., d_{iN}), 
      \quad d_{ij} = \Vert \boldsymbol{C}_i-\boldsymbol{C}_j \Vert_1, \\[1mm]
      \quad \boldsymbol{e}_i &= \sigma(\boldsymbol{d}_i), \\[1mm]
      \mathcal{B} &= \mathbf{Cluster}(M, \{\boldsymbol{e}_1, \boldsymbol{e}_2, ..., \boldsymbol{e}_N\}), \\[1mm]
\end{aligned}
\end{equation}
\footnotetext{$\boldsymbol{C}_i$ is actually the \wc{concatenated} content features extracted from several characters of font $i$. For the sake of brevity, we omit the superscript for characters here.}where $\sigma(\cdot)$ is the softmax operation, $d_{ij}$ is the L1 distance between two fonts, $\mathbf{Cluster}$ is the classical K-Medoids cluster algorithm~\cite{k-meanspp06}, and set $\mathcal{B}$ is the indices of selected fonts.

\paragraph{Weight calculation.} For the target font $t$ and its content feature $\boldsymbol{C}_t$, we measure its similarity to the basis fonts $\{\boldsymbol{C}_m\}_{m=1}^M$, namely $\boldsymbol{d}_t^{\prime}\in{\mathbb{R}^M}$. Then the content fusion weight $\boldsymbol{w}_t\in{\mathbb{R}^M}$ is calculated as follow:
\begin{equation}
\label{eq:bw}
 \begin{aligned}
    \boldsymbol{d}_t^{\prime} &= (d_{t1}, d_{t2}, ..., d_{tM}), 
    \quad d_{tm} = \Vert \boldsymbol{C}_t-\boldsymbol{C}_m \Vert_1, \\[1mm]
    \boldsymbol{w}_t &= \sigma(-\boldsymbol{d}_t^{\prime} / \tau),
\end{aligned}
\end{equation}
where $\tau$ is the temperature of the softmax operation.

\paragraph{Content fusion.} Once the basis fonts and content fusion weights are obtained, \wc{the} original content feature map $\boldsymbol{C}$ is replaced with the fused one $\boldsymbol{C}_t^{\prime}$,
\wc{where the content fusion weight of CFM is related to its target font $t$.}
\begin{equation}
\label{eq:cf}
\begin{array}{rl}
    \boldsymbol{C}_t^{\prime} = \sum_{m \in \mathcal{B}}{w_{tm} \cdot \boldsymbol{C}_m}.
\end{array}
\end{equation}

\subsection{Projected Character Loss}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{Figs/wdl2.pdf}
    \caption{Illustration of PCL. We project the binary characters into multi-direction 1D spaces (distinguished by color) and calculate normalized histograms for each. It is obvious that for the different fonts with the same character, the projected distributions vary along with the skeletons and are less sensitive to textures or colors.}
    \label{fig:wdl}
 	\vspace{-2mm}
\end{figure}

To better supervise the skeleton, we design a projected character loss, which measures image difference with marginal distribution distances on multiple 1D projections \crc{, as shown in Fig.~\ref{fig:wdl}}. Since the distribution is sensitive to the relative relationship, PCL pays more attention to the global shape of characters.
\begin{equation}
\label{eq:pcl}
\mathcal{L}_p(\boldsymbol{Y}, \hat{\boldsymbol{Y}}) = \frac{1}{P}\sum^P_{p=1} \mathcal{L}_{1d}(\phi_p(\boldsymbol{Y}), \phi_p(\hat{\boldsymbol{Y}})), 
\end{equation}
where $\boldsymbol{Y}$ and $\hat{\boldsymbol{Y}}$ represent the generated and ground-truth image respectively, $P$ is the number of projections, and $\phi_p(\cdot)$ denotes a projection function with the $p$-th direction.

There are lots of metrics to measure the alignment between 1D distributions, such as the KL-divergence and Wasserstein distance. Thus, $\mathcal{L}_p$ can have various forms:
\begin{equation}
\label{eq:pwdl}
\begin{aligned}
    \mathcal{L}_{pc-wdl}(\boldsymbol{Y}, \hat{\boldsymbol{Y}}) &= \frac{1}{P}\sum^P_{p=1}  \left\|\frac{\Lambda(\phi_p(\boldsymbol{Y}))}{\sum \phi_p(\boldsymbol{Y})}-\frac{\Lambda(\phi_p(\hat{\boldsymbol{Y}}))}{\sum \phi_p(\hat{\boldsymbol{Y}})}\right\| \\
    \mathcal{L}_{pc-kl}(\boldsymbol{Y}, \hat{\boldsymbol{Y}}) &= \frac{1}{P}\sum^P_{p=1} \mathbf{KL}\Big( \frac{\phi_p(\boldsymbol{Y})}{\sum \phi_p(\boldsymbol{Y})},\frac{\phi_p(\hat{\boldsymbol{Y}})}{\sum \phi_p(\hat{\boldsymbol{Y}})}\Big),
\end{aligned}
\end{equation}
where $\mathbf{KL}$ means the KL-divergence and $\Lambda$ denotes the cumsum function, which turns probability density functions to cumulative distribution functions. 

To simply verify the performance of PCL, we generate images of the character ``Tong'' from 240 fonts and measure their similarity by PCL and L1. The closest ten characters to the top-left one found by different metrics are displayed in Fig~\ref{fig:l1vspcl} respectively. It can be seen that the characters retrieved by L1 are quite different on the character skeleton, which is important for fonts. While those selected by PCL are relatively more consistent and it indicates that PCL is more proper for measuring the skeleton.

Adding PCL to the image reconstruction loss term, we have the following overall loss function for training:
\begin{equation}
\label{eq:pwdl2}
\begin{aligned}
\mathcal{L}=\mathcal{L}_{adv}+\lambda_{img} (\mathcal{L}_{img}+\lambda_{pcl}\mathcal{L}_{pcl})\\+\lambda_{cnt} \mathcal{L}_{cnt}+\lambda_{offset} \mathcal{L}_{offset},
\end{aligned}
\end{equation}
where $\lambda_{img}$, $\lambda_{pcl}$, $\lambda_{cnt}$, and $\lambda_{offset}$ are hyperparameters to adjust the weight of each loss function.


\begin{figure}[t]
    	\centering
    \includegraphics[width=0.65\linewidth]{Figs/l1vspclv2.pdf}
    \caption{L1 vs PCL. We retrieve the closest character of all training fonts to the top-left one by L1, PC-WDL, and PC-KL, respectively. The top ten results of each loss are listed from left to right, top to down. It can be seen that the skeletons vary greatly in the column of L1 but are quite consistent in those of PCL.}
    \label{fig:l1vspcl}
\end{figure}
\subsection{Iterative Style-vector Refinement} % ISR 
For target font $t$, a robust style information can be extracted as the latent style vector $\boldsymbol{s}_t^{\prime}$ by averaging the outputs of $f_{se}$ with a set of character images~\cite{DGFont_cvpr21}.
\vspace{-5pt}
\begin{equation}
\label{eq:sii_init}
\boldsymbol{s}_t^{\prime} = \frac{1}{Q}{\sum_{q=1}^Q f_{se}(\boldsymbol{I}^q_t)},
\vspace{-5pt}
\end{equation}
where $\boldsymbol{I}^q_t$ is an image of character $q$ of font $t$, and Q denotes the reference character number.

Motivated by the "iterative inference" strategy that optimizes input in the inference stage~(\eg ~\cite{yeh2017semantic}), we propose iterative style-vector refinement for further optimizing the style feature $\boldsymbol{s}_t^{\prime}$. As in Fig.~\ref{fig:flowchart}~(c), in the inference stage, $\boldsymbol{s}_t^{\prime}$ is first initialized by Eq.~\ref{eq:sii_init}. Then, using the provided few \wc{reference} characters of target fonts $\{\boldsymbol{I}_t^{q}\}_{q=1}^Q$ as supervising samples, we refine $\boldsymbol{s}_t^{\prime}$ for around ten epochs according to the backpropagation of the reconstruction loss. Finally, the optimized style vector is adopted for inference. Worth noting this style vector can be stored as a signature of the target font and reused in referencing all characters of the same font, which makes the proposed ISR efficient in the real system.