\section{Related Works}
\subsection{Image-to-image Translation}
Image-to-image translation is the task of converting a source image to the target domain of reference images. Early methods~\cite{DBLP:conf/cvpr/IsolaZZE17, DBLP:conf/cvpr/Wang0ZTKC18, DBLP:conf/iccv/ZhuPIE17,DBLP:conf/cvpr/ShrivastavaPTSW17, DBLP:conf/cvpr/ChoiCKH0C18} utilize GAN~\cite{DBLP:conf/nips/GoodfellowPMXWOCB14} and yield vivid images. But they could only convert the source image to some specific domains (or categories), which is more limited in practical applications. Recently, some few-shot methods~\cite{DBLP:conf/iccv/0001HMKALK19,DBLP:conf/iccv/HuangB17,DBLP:journals/tip/ChenXYST19,DBLP:conf/iccv/BaekCUYS21, DBLP:conf/nips/BenaimW17, DBLP:conf/icml/KimCKLK17} are proposed. These methods disentangle the content and style, and can convert the source image to arbitrary styles only if a few reference images are provided. Further, RG-UNIT~\cite{DBLP:conf/iccv/Gu0H0021} proposes an image retrieval strategy to help domain transfer, \ie it finds images similar to the source in content but in the target domain, and extracts their content features as assistance. Though the retrieval strategy helps to generate more realistic images, it cannot be directly applied to font generation tasks. Because the retrieved image may still differ significantly from the target in content, as fonts are highly fine-grained. Thus, we build basis fonts and use fused content features to narrow the gap between the source and target domains.

\begin{figure*}[!t]
    \centering
    \resizebox{0.9\linewidth}{!}{
 	\fbox{\includegraphics[width=\textwidth]{Figs/ALL.pdf}}
 	}
 	\vspace{-2mm}
    \caption{The framework of our model. (a) We first train \wc{the DGN}~\cite{DGFont_cvpr21} and use PCL to enhance the supervision of character skeletons. (b) After the model converges, content features of all training fonts are clustered and basis fonts are selected according to cluster centers. The original content encoder is replaced by CFM, and original content features are changed to fused features of basis fonts. Then we continue to train the model so that it adapts to fused content features. (c) In inference, we utilize ISR to polish the style of a font. 
    \wc{The extracted mean style vector is}
    treated as the only \wc{trainable} variable to be fine-tuned for a few iterations.}
    \label{fig:flowchart}
 	\vspace{-5mm}
\end{figure*}

\subsection{Few-shot Font Generation}
Few-shot font generation aims to generate a new font library in the required style with only a few reference images. 
Early methods~\cite{Zi2zi, DBLP:conf/bmvc/ChangGZW18,  DBLP:conf/icdar/LyuBYZHL17, DBLP:conf/siggrapha/JiangLTX17, DBLP:conf/icpr/SunZY18} for font generation train a cross-domain translation network to model mapping from the source to the target domain.
These structures limit the model to generate unseen fonts.
To address this issue, SA-VAE~\cite{savae_ijcai} and EMD~\cite{emd_cvpr18} disentangle the representations of style and content, and can generate images of all style-content combinations. RD-GAN~\cite{DBLP:conf/aaai/0006W20}, SCFont~\cite{DBLP:conf/aaai/JiangLTX19}, CalliGAN~\cite{DBLP:journals/corr/abs-2005-12500}, and LF-Font~\cite{lffont_aaai21} follow this way and employ component annotations to boost the style representation in local regions. To be less dependent on explicit component annotations, MX-Font~\cite{mxfont_iccv21} utilizes multiple experts and bipartite matching, and XMP-Font~\cite{xmpfont_cvpr22} employs a cross-modality encoder, which is conditioned jointly on character images and stroke labels. \wc{CG-GAN~\cite{cggan_cvpr22} supervises a font generator to decouple content and style on component level through a component-aware module.} But these \wc{three} methods still require the labels of component categories. \wc{Fs-Font is proposed to learn fine-grained local styles from reference images, and the spatial correspondence between the content and reference images.~\cite{fsfont} However, it needs to select reference characters carefully to achieve high-quality generated results.} DG-Font~\cite{DGFont_cvpr21} introduces a feature deformation skip connection module and achieves excellent performance without any extra labels. However, it is difficult for these few-shot methods to generate new fonts if the source and target domains are very different, especially when the target font is unseen. Starting from this perspective, we propose the CFM to reduce the difficulty of domain transfer, and the PCL to enhance skeleton supervision.