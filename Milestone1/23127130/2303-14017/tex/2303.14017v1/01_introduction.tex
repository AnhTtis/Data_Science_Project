\section{Introduction}
Few-shot font generation aims to produce characters of a new font by transforming font images from a source domain to a target domain according to just a few reference images. It can greatly reduce the labor of expert designers to create a new style of fonts, especially for logographic languages that contain multiple characters, such as Chinese (over 60K characters), Japanese (over 50K characters), and Korean (over 11K characters), since only several reference images need to be manually designed. Therefore, font generation has wide applications in font completion for ancient books and monuments, personal font generation, etc. 

Recently, with the rapid development of convolutional neural networks~\cite{cnn} and generative adversarial networks~\cite{DBLP:conf/nips/GoodfellowPMXWOCB14}~(GAN), pioneers have made great progress in generating gratifying logographic fonts. Zi2zi~\cite{Zi2zi} introduces pix2pix~\cite{DBLP:conf/cvpr/IsolaZZE17} method to generate complex characters of logographic languages with high quality, but it cannot handle those fonts that do not appear in training~(unseen fonts). For the few-shot font generation, many methods~\cite{emd_cvpr18, savae_ijcai, gao2020gan, dmfont_eccv20, lffont_aaai21, mxfont_iccv21, DGFont_cvpr21} verify that content and style disentanglement is effective to convert the style of a character in the source domain, denoted as \emph{source character}, to the target style embodied with reference images of seen or unseen fonts. The neural networks in these methods usually have two branches to learn content and style features respectively, and the content features are usually obtained with the character image from a manually-chosen font, denoted as \emph{source font}. However, since it's a difficult task to achieve a complete disentanglement between content and style features~\cite{kwon2021diagonal, kazemi2019style}, the choice of the font for content-feature encoding influences the font generation results substantially. For instance, \emph{Song} and \emph{Kai} are commonly selected as the source font~\cite{lffont_aaai21, emd_cvpr18, DGFont_cvpr21, cggan_cvpr22, lyu2017auto,xu2009automatic}. While such choices are effective in many cases, the generated images sometimes contain artifacts, such as incomplete and unwanted strokes.

The main contribution of this paper is a novel content feature fusion scheme to \crc{mitigate the influence of incomplete disentanglement by exploring} the synchronization of content and style features, which significantly enhances the quality of few-shot font generation. \crc{Specifically, we design a content fusion module~(CFM) to take the content features of different fonts into consideration during training and inference.} It is realized by computing the content feature of a character of a target font through linearly blending content features of the corresponding characters in the automatically determined basis fonts, and the blending weights are determined through a carefully designed font-level distance measure. In this way, we can form a linear cluster for the content feature of a semantic character, and explore how to leverage the font-level similarity to seek for an optimized content feature in this cluster to improve the quality of generated characters.

In addition, we introduce an iterative style-vector refinement~(ISR) strategy to find a better style feature vector for font-level style representation. For each font, we average the style vectors of reference images and treat it as a learnable parameter. Afterward, we fine-tune the style vector with a reconstruction loss, which further improves the quality of the generated fonts. 

Most font-generation algorithms~\cite{Zi2zi,DGFont_cvpr21,lffont_aaai21, dmfont_eccv20, mxfont_iccv21, cggan_cvpr22} choose L1 loss as the character image reconstruction loss. However, L1 or L2 loss mainly supervises per-pixel accuracy and is easily disturbed by the local misalignment of details. Hence, we employ a distribution-based projected character loss~(PCL) to measure the shape difference between characters. Specifically, by treating the 1D projection of 2D character images as a 1D probability distribution, PCL computes the distribution distance to pay more attention to the global properties of character shapes, resulting in the large improvement of skeleton topology transfer results. 

\wc{The CFM can be embedded into the few-shot font generation task to enhance the quality of generated results.} Extensive experiments verify that our method, \wc{referred to as CF-Font}, remarkably outperforms state-of-the-art methods on both seen and unseen fonts.
\wc{Fig.~\ref{fig:teaser} reveals that our method can generate high-quality fonts of various styles.}