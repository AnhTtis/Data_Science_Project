\section{Preliminaries}\label{sec::preliminaries}
\subsection{Notation and hypotheses}

Throughout this article, we consider the joint distribution $(X,Y)$, where $X=(X_1,X_2,\dots,X_n) \in \R^n$ are the predictors and $Y\in \RR$ is a (possibly non-continuous) response variable. Also let $f(x)=\widehat{\E}[Y|X=x]$ be a trained model. We assume that all random variables are defined on the probability space $(\Omega,\mathcal{F},\PP)$, where $\Omega$ is a sample space, $\mathcal{F}$ a $\sigma$-algebra of sets, and $\PP$ a probability measure.

Given a random vector $Z=(Z_1,\dots,Z_k)$ on $(\Omega,\mathcal{F},\P)$, the pushforward probability measure $P_Z$ is a probability measure on $\R^k$  equipped with the $\sigma$-algebra $\B(\R^k)$ of Borel sets of $\R^k$ and satisfying $P_Z(A)=\P(Z \in A)$ for every $A \in \B(\R^k)$. 

Given two probability measures $\mu$ and $\nu$ on $\R^m$ and $\R^k$, respectively, equipped with corresponding Borel $\sigma$-algebras, the product measure $\mu \otimes \nu $ is a probability measure defined on the product $\sigma$-algebra $\B(\R^k)\otimes \B(\R^m)$ that satisfies $\mu \otimes \nu (A \times B)=\mu(A) \cdot \nu (B)$ for each $A \in \B(\R^k)$ and $B \in \B(\R^m)$. The product probability measure is unique but not necessarily complete; the uniqueness of the product measure is a consequence of the measures $\mu$ and $\nu$ being finite (\S 2.5 of \cite{Folland}).

Next, for each $S \subseteq N$ we let $\sigma_S$  denote the $\sigma$-algebra of the product measure $P_{X_S} \otimes P_{X_{-S}}$, where it is assumed that any event evaluated by this measure is first permuted according to the order of indices induced by the pair $(S,-S)$. In particular, for any $f \in L^p(P_{X_S} \otimes P_{X_{-S}},\sigma_S)$ we have
\begin{equation}\label{prod_norm}
\| f \|^p_{L^p(P_{X_S} \otimes P_{X_{-S}},\sigma_S)} := \int |f(x_S,x_{-S})|^p [P_{X_S} \otimes P_{X_{-S}}](dx_{S},dx_{-S})
\end{equation}
where we ignore the variable ordering in $f$ to ease the notation, and we assign $P_{X_{\varnothing}} \otimes P_X = P_X \otimes P_{X_{\varnothing}} = P_X$.

We next define the probability measure 
\begin{equation}\label{marg_game_measure}
\tilde{P}_X:=\frac{1}{2^n} \sum_{S \subseteq N} P_{X_S} \otimes P_{X_{-S}}
\end{equation}
equipped with the $\sigma$-algebra $\tilde{\sigma}=\cap_{S \subseteq N} \sigma_S$. As a consequence, any $\tilde{P}_X$-measurable function is also $P_{X_S} \otimes P_{X_{-S}}$-measurable for each $S \subseteq N$. Finally, the space $L^p(\tilde{P}_X,\tilde{\sigma})$ denotes $\tilde{P}_X$-equivalence classes of functions such that
\begin{equation}\label{marg_game_norm}
\| f \|^p_{L^p(\tilde{P}_X,\tilde{\sigma})} := \frac{1}{2^n} \sum_{S \subseteq N} \int |f(x_S,x_{-S})|^p [P_{X_S} \otimes P_{X_{-S}}](dx_{S},dx_{-S}) < \infty
\end{equation}
where, as before, we ignore the variable ordering in $f$. In what follows, when the context is clear, we will suppress the dependence on the $\sigma$-algebra in the notation of functional spaces.

\subsection{Game-theoretic model explainers}\label{subsec::preliminaries::expl}
Many interpretability techniques have been proposed and utilized over the years, each with their own distinct framework and interpretation. Given their multitude, these techniques can be categorized in various ways. For instance, they can be described as either post-hoc explainers, those that generate feature attributions through the use of the model outputs, or self-interpretable models, which are those whose model structure provides direct information on feature attributions. For more details on these, see \cite{Hall-Gill}.

Some well-known explainability techniques are Partial Dependence Plots (PDP), local interpretable model-agnostic explanations (LIME), explainable Neural Networks (xNN) and Generalized Additive Models plus Interactions (GA$^2$M); see \cite{Friedman}, \cite{Ribeiro et al.}, \cite{Vaughan et al.}, and \cite{Lou2013} respectively. The first two fall under the category of post-hoc explanation techniques, while the latter two fall under the category of self-interpretable models.

Our work will focus on local, post-hoc explanation techniques that have been developed by adapting ideas from game theory. The rest of this section will introduce the relevant concepts and any further material required for the analysis in later sections.

\subsubsection{Games and game values}

In recent years many post-hoc model explainers have been designed by drawing ideas from game theory; see \cite{LundbergLee,Strumbelj2014}. A cooperative game with $n$ players is a set function $v$ that acts on a set of size $n$, say $N \subset \N$, and satisfies $v(\varnothing)=0$. A game value is a map $v \mapsto h[N,v]=(h_1[N,v],\dots,h_n[N,v])\in \R^n$ that determines the worth of each player. In the ML setting,  the features $X \in \R^n$ are viewed as $n$ players with an appropriately designed game $v(S;x,X,f)$ that depends on the given observation $x$ from $P_X$, the random vector $X$ and the trained model $f$. The game value $h[N,v]$, $N=\{1,\dots,n\}$ then assigns the contribution of the respective feature to the total payoff of the game $v(N;x,X,f)$. Non-cooperative games, those that do not satisfy the condition $v(\varnothing)=0$, can also be used in designing model explainers. This requires an  extension of the game value to such games, a topic discussed thoroughly in \cite{grouppaper}.

The game $v(\cdot;x,X,f)$ is a deterministic one, parameterized by the observations $x \in \R^n$. It can be made into a random one by substituting the observation $x$ with the random vector $X$. Such games are out of scope of this paper, and a discussion on them can be found in \cite{grouppaper}.

Two of the most notable (non-cooperative) deterministic games in ML literature are given by
\begin{equation}\label{margcondgames}
    \vce(S;x,X,f)=\E[f(X)|X_S=x_S], \quad \vpdp(S;x,X,f)=\E[f(x_S,X_{-S})],
\end{equation}
with
\begin{equation*}
\vce(\varnothing; x,X, f)=\vpdp(\varnothing; x,X, f)=\E[f(X)],
\end{equation*}
respectively called the conditional and marginal game. These were introduced by \citet{LundbergLee} in the context of the Shapley value \citep{Shapley},
\begin{equation}\label{shapform}
\varphi_i[v] = \sum_{S \subseteq N \backslash\{i\}} \frac{s!(n-s-1)!}{n!} [ v(S \cup \{i\}) - v( S ) ], \quad  s=|S|, \,  n=|N|.
\end{equation}
The Shapley value has garnered much attention from the ML community. From a game-theoretic perspective, \eqref{shapform} represents the payoff allocated to player $i$ from playing the game defined by $v$, while satisfying certain axioms such as symmetry, linearity, efficiency, and the null-player property; see \cite{Shapley} and \cite{grouppaper} for more details. The efficiency property, most appealing to the ML community, allows for a disaggregation of the payoff $v(N)$ into $n$ parts that represent a contribution to the game by each player: $\sum_{i=1}^n \varphi_{i}[v] = v(N)$.

The marginal and conditional games defined in \eqref{margcondgames} are in general not cooperative since they assign $\E[f(X)]$ to $\varnothing$. In such a case, the efficiency property reads $\sum_{i=1}^n \varphi_{i}[v] = v(N)-v(\varnothing)$. See \cite[\S 5.1]{grouppaper} for a careful treatment of game
values for non-cooperative games.

There is a systematic way of extending linear game values so that they can be applied to non-cooperative games as well \cite[\S 5.1]{grouppaper}. In this work, we will mainly consider game values of the form
\begin{equation}\label{lingamevalue}
h_i[N,v]:=\sum_{S\subseteq N\setminus \{i\}}w_i(S,N)\left(v(S\cup\{i\})-v(S)\right), i \in N, \, N \subset \N,
\end{equation}
which are generalizations of the Shapley value, and satisfy the null-player property, that is, they assign zero to players that do not contribute to any coalition; the weighted Shapley value and the Banzhaf value \cite{Banzhaf1965} are other examples of game values of the form \eqref{lingamevalue}. 

A benefit of working with a formula such as \eqref{lingamevalue} is that it remains unchanged after replacing a game 
$v:S\mapsto v(S)$ with the cooperative one  $S\mapsto v(S)-v(\varnothing)$; an extension of this type is called centered. Thus, \eqref{lingamevalue} automatically extends to non-cooperative games such as marginal and conditional ones. Properties such as linearity, symmetry, and null-player property generalize to the case of non-cooperative games in an obvious way except for the efficiency property which should be replaced with $\sum_{i\in N}h_i[N,v]=v(N)-v(\varnothing)$.

We next describe the collection of models $f$ for which the conditional and marginal games, and corresponding game values are well-defined.

\begin{lemma}\label{lmm::game_meas_cond}
Let $f\in [f] \in L^1(P_X)$. Then, for $P_X$-almost sure $x^* \in \R^n$ the conditional game $\vce(\cdot;x^*,X,f)$ is well-defined for any subset of $N$. 
\end{lemma}

\begin{proof}
The proof follows from the definition and properties of the conditional expectation; see \cite[\S 7]{Shiryaev}.
\end{proof}

\begin{lemma}\label{lmm::game_meas_marg}
Let $f\in [f] \in L^p (\tilde{P}_X)$. Then:

\begin{itemize}
    \item [$(i)$] There exists $P_X$-measurable set $U^{(f)}_p \in \B(\R^n)$ such that $P_X(U^{(f)}_p)=1$ and for each $x^{*} \in U_p^{(f)}$ and each $S \subsetneq N$ the map $x_{-S} \mapsto f(x_S^*,x_{-S})$ is $P_{X_{-S}}$-measurable and satisfies
\[
\int |f(x^*_S,x_{-S})|^p P_{X_{-S}}(dx_{-S}) < \infty. 
\]
\item [$(ii)$] For $P_X$-almost sure $x^* \in \R^n$ the marginal game $\vpdp(\cdot;x^*,X,f)$ is well-defined for any subset of $N$. 
\end{itemize}
\end{lemma}
\begin{proof} The proof follows from the Fubini theorem \cite[\S 2.5]{Folland} and the fact that $\tilde{\sigma} \subseteq \sigma_S \subseteq \B(\R^n)$.
\end{proof}

\begin{corollary}\label{corr::game_val_exist}
Let $h[N,\cdot]$ be any linear game value as in \eqref{lingamevalue} on $N$.
\begin{itemize}
    \item[(i)] Let $f\in [f] \in L^1(P_X)$. Then, $h[N,\vce(\cdot;x^*,X,f)]$ is well-defined for $P_X$-almost sure $x^*$.
    \item[(ii)] Let $f\in [f] \in L^1(\tilde{P}_X)$. Then, $h[N,\vpdp(\cdot;x^*,X,f)]$ is well-defined for $P_X$-almost sure $x^*$.
\end{itemize}
\end{corollary}

As discussed in \cite{Chen-Lundberg,Janzing,Sundararajan}, as well as in our previous work \cite{grouppaper}, the interpretation of game values is based on the corresponding game used to construct them. Game values for the conditional game, or conditional game values, explain the output of the random variable $f(X)-\E[f(X)]$, meaning that they take into account the joint distribution $P_X$. On the other hand, marginal game values explain the output of the function $f(x)-\E[f(X)]$, meaning they take into account how the model structure utilized the predictors.

In practice, given a background dataset $\bar{D}_X=\{x^{(1)},\dots,x^{(K)}\}$, $\vpdp(S;x,X,f)$ can be approximated by the empirical marginal game given by
\begin{equation}\label{empiricalmarggame}
    \hat{v}^{\ME}(S;x,\bar{D}_X,f):=\frac{1}{|\bar{D}_X|}\sum_{x'\in \bar{D}_X} f\left(x_S,x'_{-S}\right).
\end{equation}
We next show that for $P_X$-almost sure observations $x^* \in \R^n$ the empirical marginal game value is the unbiased point estimator of the marginal game value with the mean squared error bounded by $O(|\bar{D}_X|^{-1})$  as shown in the following lemma.

\begin{lemma}\label{lmm::error_emp_marg_estimator}
Let $f\in [f] \in L^2(\tilde{P}_X)$ and $h[N,\cdot]$ as in \ref{lingamevalue}. Let $\bar{\bf D}_X=\{X^{(1)},X^{(2)},\dots,X^{(K)}\}$ be a background dataset containing independent random samples from $P_X$. Then for $P_X$-almost sure $x^* \in \R^n$
\[
\E \big[|h[N,\vpdp(\cdot;x^*,X,f)]-h[N,\hat{v}^{\ME}(\cdot;x^*,\bar{\bf D}_X,f)]|^2 \big] \leq \frac{2}{K} \Big( \sum_{S \subseteq N \setminus \{i\}} w^2_i(S,n) \Big) \cdot  \Big( \sum_{S \subseteq N}  Var(f(x^*_S,X_{-S})) \Big)<\infty.
\]
As a consequence, the Mean Integrated Squared Error (MISE) satisfies
\[
\E_{x^* \sim P_X} \Big[ \E \big[|h[N,\vpdp(\cdot;x^*,X,f)]-h[N,\hat{v}^{\ME}(\cdot;x^*,\bar{\bf D}_X,f)]|^2 \big] \Big]
\leq \frac{2}{K} \Big( \sum_{S \subseteq N \setminus \{i\}} w^2_i(S,n) \Big) \cdot  \|f\|^2_{L^2(\tilde{P}_X)}.
\]
\end{lemma}

\begin{proof}
The proof follows from Lemma \eqref{lmm::game_meas_marg}, equations \eqref{margcondgames}, \eqref{lingamevalue}, and \eqref{empiricalmarggame}, and the use of Cauchy-Schwartz.
\end{proof}

Note that evaluating \eqref{lingamevalue} for a given game $v$ can be computationally intensive. The complexity is proportional to $2^n$ times the complexity of computing the game $v$. Thus, when utilizing the empirical marginal game, the complexity becomes $O(2^n \cdot |\bar{D}_X|)$. One can therefore reduce the complexity by taking a small dataset. However, there will be a trade-off with the estimation accuracy as discussed in Lemma \ref{lmm::error_emp_marg_estimator}.

Several works have proposed approximation techniques with the aim of reducing that complexity, such as Kernel SHAP in \cite{LundbergLee} and TreeSHAP in \cite{LundbergTreeSHAP}. We will discuss these in further detail below.

\subsubsection{Grouping and group explainers}

Our work in \cite{grouppaper} presents another avenue for reducing the high complexity of \eqref{lingamevalue} and generating contributions that are stable. Instead of attempting to explain the contribution of each individual predictor, one can first define a partition $\cP = \{S_1, S_2,\dots,S_m\}$ of predictors based on dependencies and then explain the contribution of each group to the model output. Such explainers are called group explainers. To form the partition $\cP$ in practice, \cite{grouppaper} presents a mutual information-based hierarchical clustering algorithm. The dissimilarity measure in this algorithm is designed using the Maximal Information Coefficient (MIC); for more details on this measure of dependence, see \cite{Reshef11,Reshef16b,Reshef16}.

There are three types of group explainers presented in \cite{grouppaper}: trivial group explainers, quotient game explainers, and explainers based on games with coalition structure. In this work we will focus on the latter two since trivial explainers are based on single feature explanations that do not incorporate the group structure in the calculation.

\begin{definition}\label{def::quotientgame}
    Let $X\in \RR^n$ be a vector of predictors, $\cP = \{S_1,\dots,S_m\}$ a partition of $N$ and $v$ a game. The quotient game $(M,v^{\cP})$ is defined as
    \[
        v^{\cP}(A):=v\Big(\bigcup_{j\in A}S_j\Big), \quad A\subseteq M=\{1,2,\dots,m\}.
    \]
\end{definition}

The game $v^{\cP}$ suggests that the players playing the game are the predictor groups, thus the payoff is allocated to groups rather than single predictors in the context of \eqref{shapform}. Game values based on $v^{\cP}$ are called quotient game explainers.

\begin{definition}\label{def::quotientexpl}
    Given a partition $\cP$ as in Definition \ref{def::quotientgame}, a trained model $f$, and a game value $h$ based on a game $v \in \{\vce,\vpdp\}$, the  marginal and conditional quotient game explainers $h^{\cP}$ at the observation $x^* \in \R^n$ based on $(h,\cP)$ are given by
    \[
        h_{S_j}^{\cP}(x^*;X,f,v):=h_j[M,v^{\cP}(\cdot;x^*,X,f)], \quad S_j\in \cP, \, v \in \{\vce,\vpdp\}.
    \]
    The coefficients of $h_{S_j}^{\cP}$ are denoted by $w_j(A,M)$, $A\subseteq M\setminus \{j\}$.
\end{definition}

\begin{remark}\rm
It follows from Lemma \ref{lmm::game_meas_cond} that $h_{S_j}^{\cP}(x^*;X,f,\vce)$ is well-defined for $P_X$-almost sure $x^*$ if $f \in [f] \in L^1(P_X)$. Similarly, from Lemma \ref{lmm::game_meas_marg} it follows that $h_{S_j}^{\cP}(x^*;X,f,\vpdp)$ is well-defined for $P_X$-almost sure $x^*$ by requiring $f \in [f] \in L^1(\tilde{P}_X)$; this requirement, in fact, can be relaxed by assuming that $f \in [f] \in L^1(\tilde{P}_{X,\cP})$ where \[
\tilde{P}_{X,\cP} := \frac{1}{2^m} \sum_{A \subseteq M, Q_A=\cup_{\alpha \in A}S_{\alpha}} P_{X_{Q_A}} \otimes P_{X_{-Q_A}}.
\] 
We note that if the group predictors $(X_{S_1},\dots,X_{S_m})$ are independent then $\tilde{P}_{X,\cP}=P_X$ and $v^{\ME,\cP}=v^{\CE,\cP}$.
\end{remark}

Two examples of such an explainer are the conditional and marginal quotient game Shapley values $\varphi[v^{\CE,\cP}]$ and $\varphi[v^{\ME,\cP}]$ respectively, which utilize the games in \eqref{margcondgames} and a given partition $\cP$.

\begin{equation}\label{quotientShapley}
    \varphi_j[v^{\cP}]=\sum_{A \subseteq M \setminus \{j\}} \frac{|A|!(m-|A|-1)!}{m!} \left[ v^{\cP}(A \cup \{j\}) - v^{\cP}( A ) \right], \quad S_j\in \cP, \quad v\in \{\vce,\vpdp\}.
\end{equation}

Explainers based on games with coalition structure, or coalitional values, utilize the group partition $\cP$ in order to output contributions for individual predictors. Games with coalitions were introduced in \cite{Aumann1974}.

\begin{definition}\label{def::coalvalue}
    Given $N$, $\cP$, $v$ as in Definition \ref{def::quotientgame}, a coalitional value $g$ is a map of the form $g[N,v,\cP]=\{g_i[N,v,\cP]\}\in \RR^n$, where $g_i$ represents the contribution of the player $i \in S_j$, $S_j \in \cP$ to the total payoff $v(N)$.
\end{definition}

\begin{remark}\rm
Similar to game values, it follows from Lemma \ref{lmm::game_meas_cond} that $g[N,\vce(\cdot;x^*,X,f),\cP]$ is well-defined for $P_X$-almost sure $x^*$ if $f \in [f] \in L^1(P_X)$. From Lemma \ref{lmm::game_meas_marg} it follows that $g[N,\vpdp(\cdot;x^*,X,f),\cP]$ is well-defined for $P_X$-almost sure $x^*$ by requiring $f \in [f] \in L^1(\tilde{P}_X)$.
\end{remark}

Two notable coalitional values are the Owen $Ow[N,v,\cP]$ and the Banzhaf-Owen value $BzOw[N,v,\cP]$; see \cite{Owen,Owen1982,Lorenzo-Freire}. Both are rather complex quantities, as seen by their respective elements below. Suppose we are interested in the contribution of $X_i$ to the model output such that $X_i$ forms a union $S_j$ with other predictors. Its Owen and Banzhaf-Owen values will be given by

\begin{equation}\label{OwenandBzOw}
    \begin{aligned}
        Ow_i[N,v,\cP] &= \sum_{A\subseteq M\setminus \{j\}}\sum_{T\subseteq S_j\setminus \{i\}} \tfrac{|A|!(m-|A|-1)!}{m!}\tfrac{|T|!(|S_j|-|T|-1)!}{|S_j|!} \left[ v(Q_A \cup T \cup \{i\}) - v(Q_A \cup T) \right]\\
        BzOw_i[N,v,\cP] &= \sum_{A\subseteq M\setminus \{j\}}\sum_{T\subseteq S_j\setminus \{i\}} \tfrac{1}{2^{m-1}}\tfrac{1}{2^{|S_j|-1}} \left[ v(Q_A \cup T \cup \{i\}) - v(Q_A \cup T) \right]
    \end{aligned}
\end{equation}
where $Q_A=\cup_{\alpha \in A}S_{\alpha}$, $i\in S_j$. As with the Shapley value, if $v=\vce$ then we call the quantities in \eqref{OwenandBzOw} conditional Owen values and conditional Banzhaf-Owen values. Respectively, for $v=\vpdp$ we say marginal Owen and marginal Banzhaf-Owen values.

Compare the complexity $O(2^n)$ of the Shapley value with those of \eqref{quotientShapley} and \eqref{OwenandBzOw}, which are respectively $O(2^m)$ and $O(2^{|S_j|+m})$. Although lower, in practice these complexities may still not suffice for an exact computation to be carried out. For example, it may be that there are $n=200$ predictors and $m=30$ groups are formed. The complexity $O(2^{30})$ is still tremendously large. This motivates us to design approximation techniques for group explainers that are relatively fast and accurate. In Section \ref{sec::MC_theory}, we design a Monte Carlo (MC) sampling algorithm for group explainers and provide the error analysis, showing also that the resulting estimator is consistent and unbiased.

In addition to reducing the computational complexity, there are several benefits in using group explainers for ML explainability over standard game values, as discussed in detail in \cite{grouppaper}. By forming independent groups of predictors, the marginal and conditional games coincide, allowing to unify their interpretations. Thus, one can consider both the model structure and the joint distribution of $X$ when generating explanations. This leads to more stable contribution values across different models trained on the same dataset.

\subsection{Relevant works}\label{subsec::preliminaries::relwork}
Many approximation techniques for \eqref{shapform} have been developed over the years. Lundberg and Lee \cite{LundbergLee} introduced Kernel SHAP, a method that approximates marginal Shapley values by solving a weighted least square minimization problem. The paper however does not provide any asymptotics. Lundberg et al. \cite{LundbergTreeSHAP} designed an algorithm called TreeSHAP that is specific to tree-based models. One of the parameters of the algorithm allow the user to switch between two games, the marginal (referred to as interventional) and a tree-based game that is defined by the tree structure of the model. The latter game allows for a fast computation of the corresponding Shapley values. However, the tree-based game is not implementation invariant, and as a consequence TreeSHAP values for this game do not approximate the conditional Shapley values; see \cite{Filom}. An error analysis has not been carried out for this method either.

Castro et al. \cite{Castro2001} suggested a Monte Carlo method for computing the Shapley value for a generic game $v$. The idea is to view the Shapley value as an expected value of an appropriate random variable (associated with the game $v$) defined on the space of coalitions. Since the algorithm is applied to a generic game $v$, the complexity of the algorithm is $K\cdot O(|v|)$, where $K$ are the number of samples and $|v|$ is the complexity of evaluating the game $v$, with the statistical accuracy being of order $O(K^{-1/2})$. Directly applying this algorithm to the empirical marginal game with a background dataset of size $K$ yields a high complexity of $O(K^2)$, and the statistical accuracy remains $O(K^{-1/2})$.

Utilizing the algorithm from \cite{Castro2001}, $\rm\check{S}$trumbelj and Kononenko \cite{Strumbelj2011,Strumbelj2014} presented a sampling algorithm to approximate the marginal Shapley value with a reduced complexity of $O(K\cdot |f|)$, with $|f|$ the computational complexity of evaluating the model $f$ at a given observation, and provide numerical evidence showing convergence for their algorithm. The authors, however, do not provide a rigorous setup for their method, nor do they explicitly define the marginal game. They define the conditional game and subsequently assume predictor independence, leading to the marginal game. For more techniques that allow for approximating the Shapley value, please see \cite{Chen2022}.

Our analysis is inspired by the works of \cite{Castro2001,Strumbelj2011,Strumbelj2014} and seeks to rigorously generalize the sampling approach for a wider class of linear game values, as well as coalitional values. To our knowledge, there has not been any work on designing approximation techniques for quotient game and coalitional explainers, and we believe this work constitutes the first endeavor in this direction.
