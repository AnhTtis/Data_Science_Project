\section{Introduction}

As the use of Machine Learning (ML) models has become widespread, the need to explain these complex models has also become vital. In the financial industry, predictive models, and strategies based on these models, are subject to federal and state regulations. Regulation B of the Equal Credit Opportunity Act (ECOA, 15 U.S.C. 1691 et seq (1974)) \cite{ECOA} requires a financial institution to notify consumers on the reasons behind certain adverse actions (such as declining a credit application), which in turn requires explaining how attributes in the model contribute to its output.

In recent years, numerous interpretability (or explainability) methods have been developed using concepts from cooperative game theory, motivated by the celebrated work of Shapley \cite{Shapley}. In this setting, given a function $f:\R^n \to \R$, a random vector $X=(X_1,\dots,X_n)$ and an observation $x$, a game $v(\cdot;x,X,f)$ is defined as a set function on subsets of $N=\{1,\dots,n\}$. Here, the predictors $X_1,X_2,\dots,X_n$ are viewed as players participating in the game defined by $v$. Two of the most notable games in the ML literature are the marginal and conditional games and are given respectively by
\[
	\vpdp(S;x,X,f) := \E[ f(x_S,X_{-S}) ], \quad \vce(S;x,X,f) := \E[ f(X) | X_S = x_S ], \ S\subseteq N,
\]
defined in the context of the Shapley value for ML explainability and seen in several works, such as \cite{Strumbelj2011,LundbergLee}.

A game value $h[N,v]=\{h_i[N,v]\}_{i\in N}\in \R^n$ evaluates the contributions of each player to the output of the function $f$. In our work we focus on linear game values $h[N,v]$ that have the form
\[
	h_i[N,v] = \sum_{S\subseteq N\setminus \{i\}} w_i(S,N)(v(S\cup\{i\}) - v(S)), \quad i\in N, \ v\in \{\vpdp,\vce\}.
\]
Explanations generated based on the games in $\{\vpdp,\vce\}$ are called respectively marginal and conditional explanations.

A detailed discussion on how to interpret the game values based on each game has been proposed in \cite{Sundararajan,Janzing,Chen-Lundberg}. Roughly speaking, conditional game values explain predictions $f(X)$ viewed as a random variable, while marginal game values explain the transformations occurring in the model $f(x)$, sometimes called mechanistic explanations \cite{Elton}.

Understanding how a specific model structure and its predictions depend on the underlying predictors is an important part of financial industry regulation, which is why we focus on marginal game values in this paper. The marginal game is straightforward to approximate via the empirical marginal game, that averages the function outputs $f(x_S,x_{-S}^{(k)})$, $x^{(k)}\in \bar{D}_X$, where $\bar{D}_X$ is a background dataset (usually the training set). Hence, the direct estimation of the empirical marginal game value yields a computational complexity of order $O(2^n\cdot |\bar{D}_X|)$, with a statistical accuracy of order $O(|\bar{D}_X|^{-1/2})$. Thus, the biggest challenge in evaluating the empirical marginal game is the exponential number of terms that need to be computed. In addition, when the estimation accuracy is of practical importance, the size of the background dataset also plays a significant role in view of the relatively small convergence rate.

In the literature, there have been several works that propose solutions to the high complexity of estimating a marginal game value in the context of the Shapley value. The Kernel SHAP method approximates marginal Shapley values via a weighted least square problem (the authors work with the conditional game but assume predictor independence, which leads to the marginal game). To make the estimation faster, the method allows discarding of terms from the Shapley formula, thus impacting the estimation accuracy; see \cite{LundbergLee}. Still, the dependence on the number of predictors remains significant, which causes the method to be slow (the authors do not provide the complexity of their method).

TreeSHAP is a model-specific technique, applied to tree ensembles, that attempts to approximate Shapley values. There are two versions of the algorithm, the path-dependent and the interventional TreeSHAP algorithm; see respectively \cite{LundbergTreeSHAP,LundbergNature}. The former version is meant to estimate conditional Shapley values for a given tree ensemble and observation. However, it turns out this approach estimates Shapley values for a tree-based game that differs from both conditional and marginal games. Path-dependent TreeSHAP considers the internal parameters of the model to carry out the estimation, and is known to be implementation non-invariant; see \cite{Filom}. The complexity of the algorithm is $O(T\cdot \mathcal{L}\cdot \mathcal{D}^2)$, where $T$ is the number of trees in the ensemble, $\mathcal{L}$ is the maximum number of leaves, and $\mathcal{D}$ the maximum depth.

Interventional TreeSHAP computes the empirical marginal Shapley value via the use of dynamic programming to obtain polynomial-time performance. The algorithm takes advantage of the tree-based structure and has complexity $O(T\cdot \mathcal{L}\cdot |\bar{D}_X|)$, where $T$,  $\mathcal{L}$ are as above, and $\bar{D}_X$ is the background dataset. One important drawback of the TreeSHAP algorithms is that they are designed for tree-based models and a generalization to a wide class of linear game value is not available.

Another recent work on marginal game values of tree-based models is \cite{Filom}. The paper presents a novel algorithm for computing the marginal Shapley and Banzhaf values of ensembles of oblivious trees. It takes advantage of the internal structure by precomputing probabilities and other parameters associated with the model structure, and uses them to evaluate the empirical marginal Shapley value associated with the training set, via a formula specifically derived for tree-based models consisting of oblivious trees. The complexity of precomputation is $O(T\cdot \mathcal{L}^{\log_2(10)})$, while the complexity of computing the Shapley value for one observation is $O(T\cdot \log_2(\mathcal{L}))$, which is equivalent to the evaluation of a tree ensemble at one instance. A crucial aspect of the method is that, unlike the interventional TreeSHAP algorithm, both precomputations and Shapley value computations are independent of any background dataset.

The work of Castro et al. \cite{Castro2001} suggested a Monte Carlo method for computation of the Shapley value for a generic game $v$. In this method, the Shapley value is viewed as an expected value of an appropriate random variable (associated with the game $v$) defined on the space of coalitions. This allows for an approximation algorithm that samples coalitions, obtains the observation of the random variable and then averages them to obtain the estimate. Since the algorithm is applied to a generic game $v$, given $K$ samples the complexity of the algorithm is $K\cdot O(|v|)$, where the latter is the complexity of evaluating the game $v$, and the statistical accuracy is of order $O(K^{-1/2})$. Directly applying this algorithm to the empirical marginal game with a background dataset of size $K$ yields a high complexity of $O(K^2)$, and the statistical accuracy remains $O(K^{-1/2})$.

Other important work in this direction are the papers of $\rm\check{S}$trumbelj and Kononenko \cite{Strumbelj2011,Strumbelj2014}, which apply the algorithm presented in \cite{Castro2001} and produce a sampling algorithm for the marginal Shapley value (the authors work with the conditional game under assumption of predictor independence). The algorithm jointly samples a coalition and an observation of the predictor vector. This reduces the complexity to $O(K\cdot |f|)$, where $K$ is the number of samples and $|f|$ denotes the complexity of computing the output of $f$ for a given observation. This is a significant improvement over the work of \cite{Castro2001}.

However, the articles \cite{Strumbelj2011,Strumbelj2014} are application-focused and the authors do not provide a rigorous setup for their method. While the authors do provide numerical evidence of convergence on a particular dataset, their work fails to provide proof of convergence and the rate of convergence remains unknown. In recent years, many other game values, and especially coalitional values, have been investigated and applied in the context of ML interpretability, such as the Owen value and two-step Shapley value; see \cite{Lorenzo-Freire,grouppaper}. The works of $\rm\check{S}$trumbelj and Kononenko \cite{Strumbelj2011,Strumbelj2014} focus only on the Shapley value, which limits the use of the approach.

Motivated by the aforementioned works, we design a Monte Carlo-based approach that serves as a generalization to the algorithm of \cite{Castro2001} and \cite{Strumbelj2011,Strumbelj2014}. The main results are as follows:
\begin{itemize}
	\item[($i$)] We design a generalized sampling method to estimate a wide class of marginal linear game values and their quotient game counterparts. In addition, we provide a rigorous setup and statistical analysis of convergence for an appropriate class of models; see Theorems \ref{thm::gamevalue_as_expectation}, \ref{thm::quotientexplainer_as_expectation} and Algorithms \ref{algo_lingamevalue}, \ref{algo_quotgamevalue}.
	
	\item[($ii$)] We extend our method to estimate a wide class of marginal coalitional values $g[N,\vpdp,\cP]$, where $\cP$ is the partition of the predictors. It is common to create the partition $\cP$ based on dependencies, which yields certain advantages; see \cite{grouppaper}. Unlike game values, coalitional values require sampling on the joint space of the triplet consisting of the space of coalitions within a group, the space of group coalitions, and the space of the predictors. Similarly, a rigorous analysis is provided for this sampling method as well; see Theorem \ref{thm::coalvalue_as_expectation} and Algorithms \ref{algo_coalval}, \ref{algo_twostep}.
	
	\item[($iii$)] In certain situations, one may be interested in computing the empirical marginal game value instead of the true value. For this reason we introduce adjusted sampling algorithms that converge to the given empirical marginal game value or coalitional value; see Remarks \ref{rm::emp_marg_lingameval}, \ref{rm::emp_marg_quotgameval}, \ref{rm::emp_marg_coalval}, and \ref{rm::emp_marg_twostep}.
	
	\item[($iv$)] Numerical experiments are conducted that illustrate the convergence rate for various game values and coalitional values on synthetic data examples by estimating the mean integrated squared error (MISE) and relative error. The results show that the numerical evidence agrees with our theoretical results; see Section \ref{subsec::num_experiments}.
\end{itemize}

\noindent {\bf Structure of the paper}. In Section \ref{sec::preliminaries} we provide the notation and necessary background from game theory and probability theory to carry out the analysis. In Section \ref{sec::MC_theory} we introduce a probabilistic framework that allows us to express marginal game values, their quotient game counterparts, and coalitional values, as expected values on appropriate sample spaces. We then present our sampling algorithms together with a rigorous analysis of their statistical error. In Section \ref{sec::numerical} we carry out numerical experiments for various game values and estimate the statistical error for our sampling methods. The results agree with our theoretical findings.