\section{Numerical Examples}\label{sec::numerical}

In this section we present the results from the numerical experiments conducted on synthetic data. The purpose of these experiments is to numerically show the asymptotic convergence of our MC approach for three empirical marginal game values: quotient Shapley values, Owen values, and two-step Shapley values.

The reason why we estimate the empirical marginal game value and not the true marginal is two-fold. First, because in practice it is infeasible to obtain the true marginal game value to have as a ground truth. On the other hand, the empirical marginal game value can be evaluated when the background dataset is small enough (recall that the complexity for $h_i[N,\hat{v}^{\ME}]$ is $O(2^n\cdot |\bar{D}_X|)$). Second, to properly showcase the asymptotic behavior of the estimator, we need to be able to carry out as many MC iterations as we desire. The version of the algorithm that estimates the true marginal game value would bound the number of MC iterations by $|\bar{D}_X|$.

There are two subsections below. In the first subsection we explain how the algorithms for the three aforementioned game values can be applied in a practical setting, which is the approach used to carry out the experiments. The following subsection contains the synthetic data examples, presents the results and provides a discussion.

\subsection{Practical implementation of algorithms}

In each of the Algorithms \ref{algo_game_marg}-\ref{algo_twostep}, there is at least one line that dictates to randomly draw from some distribution. Specifically, when estimating the true or empirical marginal game value one has to randomly draw coalitions, and in the case of the empirical marginal game value one also has to sample observations from the background dataset $\bar{D}_X$. Random sampling from $\bar{D}_X$ is not practically difficult. However, depending on the coefficients of the game value, it may not always be easy to randomly select a coalition.

Luckily, for game values that incorporate Shapley coefficients, such as the ones we will work with in our examples, the coalition sampling can be done in a relatively straightforward manner. The key is to use random permutations of player indices. We will explain in detail how to apply this idea to Algorithm \ref{algo_game_marg} when $h[N,\vpdp]$ is $\varphi[N,\vpdp]$ and then state the necessary changes to the other algorithms.

As described in Remark \ref{rem::shapley_probs}, the probability of sampling the coalition $S\subseteq N\setminus \{i\}$ is given by $P_i^{(\varphi)}(S) = \frac{|S|!(n-|S|-1)!}{n!}$. Now let $\sigma:N\to N$ be a permutation of the indices in $N$, and suppose $i$ is the player of interest whose Shapley value $\varphi_i[N,\vpdp]$ we would like to estimate. The below steps explain how random permutations work in sampling coalitions based on the Shapley coefficients:
\begin{itemize}
    \item[(1)] Suppose $\sigma:N\to N$ is a (selected uniformly at random) permutation of the indices in $N$ such that $\sigma(\ell)=i$ for some $\ell\in N$. This yields the permutation $\sigma(N) = (\sigma(1),\sigma(2),\dots,\sigma(\ell-1),i,\sigma(\ell+1),\dots,\sigma(n))$.
    \item[(2)] Consider the set of the first $l-1$ indices in the permuted set. Set $S(\sigma,i)=\{\sigma(1),\sigma(2),\dots,\sigma(\ell-1)\}$ as the sampled coalition. Then, the probability of observing a particular set $S^* \subseteq N \setminus \{i\}$ is given by
    \[
    P_{\sigma}(S(\sigma,i)=S^*)=\frac{|S^*|!(n-|S^*|-1)!}{n!}.
    \]
\end{itemize}

Notice that the above formula turns out to be exactly $P_i^{(\varphi)}(S^*)$. Therefore, in order to sample a coalition based on $P_i^{(\varphi)}$, one simply needs to randomly permute $N$ and set the sampled coalition $S$ as the indices to the left of the player of interest. The only thing that remains is to prove the claim in step (2).

\medskip

\noindent\textit{Claim: Let $i\in N$ be the player of interest,  $S^* \subseteq N \setminus \{i\}$ and $l=|S^*|+1$. The probability of sampling $S^*$ based on the Shapley value coefficients is equal to randomly selecting a permutation $\sigma:N\to N$ such that $S^*=\{\sigma(1),\sigma(2),\dots,\sigma(\ell-1)\}$ and $\sigma(\ell)=i$.}

\begin{proof}    
    Given $S^* \subseteq N \setminus \{i\}$, we set $l=|S^*|+1$. Then
    \[
  P_{\sigma}(S(\sigma,i)=S^*)=P_{\sigma}(S(\sigma,i)=S^* | \sigma(l)= i )P_{\sigma}(\sigma(l)=i).
    \]

To obtain the probability $P_{\sigma}(S(\sigma,i)=S^* | \sigma(l)=i )$, observe that this is simply the inverse of the binomial coefficient $\binom{n-1}{\ell-1}$ since there are that many distinct choices for the indices to the left of $i$ in the permuted set $\sigma(N)$ where $\sigma(\ell)=i$. Next, since $\sigma$ is a permutation of $N$, then $P_{\sigma}(\sigma(l)=i)=\frac{1}{n}$. Putting the two together proves the claim.
\end{proof}

Considering the above, each algorithm can be adjusted as follows for practical purposes when Shapley coefficients are involved:

\medskip
\textbf{Algorithm \ref{algo_game_marg} for $\varphi[N,\vpdp]$:} Replace line 5 with ``Take a random permutation $\sigma$ of $N$. Suppose $\sigma(\ell)=i$ for some $\ell \in N$. Then set $S^{(k)}=\{\sigma(1),\dots,\sigma(\ell-1)\}$''.

\medskip
\textbf{Algorithm \ref{algo_quot_game_marg} for $\varphi[M,v^{\ME,\cP}]$:} Replace line 5 with ``Take a random permutation $\sigma$ of $M$. Suppose $\sigma(\ell)=j$ for some $\ell \in M$. Then set $A^{(k)}=\{\sigma(1),\dots,\sigma(\ell-1)\}$''.

\medskip
\textbf{Algorithm \ref{algo_coalval} for $Ow[N,\vpdp,\cP]$:} Replace line 6 with ``Take a random permutation $\sigma$ of $M$. Suppose $\sigma(\ell)=j$ for some $\ell \in M$. Then set $A^{(k)}=\{\sigma(1),\dots,\sigma(\ell-1)\}$''. Next, replace line 7 with ``Take a random permutation $\tau(\{1,\dots,s\})$ where $S_j=\{i_1,\dots,i_s\}$. Suppose $i_{\tau(t)}=i$ for some $t\in \{1,\dots,s\}$. Then set $T^{(k)}=\{i_{\tau(1)},\dots,i_{\tau(t-1)}\}$''.

\medskip
\textbf{Algorithm \ref{algo_twostep}:} Replace line 11 with ``Take a random permutation $\sigma$ of $M$. Suppose $\sigma(\ell)=j$ for some $\ell \in M$. Then set $A^{(k)}=\{\sigma(1),\dots,\sigma(\ell-1)\}$''. Next, replace line 13 with ``Take a random permutation $\tau(\{1,\dots,s\})$ where $S_j=\{i_1,\dots,i_s\}$. Suppose $i_{\tau(t)}=i$ for some $t\in \{1,\dots,s\}$. Then set $S^{(k)}=\{i_{\tau(1)},\dots,i_{\tau(t-1)}\}$''.

\begin{remark}\rm
    The same adjustments apply when one applies the version of each algorithm that estimates the empirical marginal game value.
\end{remark}

\subsection{Synthetic data examples}\label{subsec::num_experiments}

Now we will present six different numerical experiments conducted using the second version of the algorithms that estimates the empirical marginal game value.

For each of the three game values mentioned in the beginning of this section (quotient Shapley, Owen, and two-step Shapley values), there are two experiments conducted. One where we showcase the error of estimation as the number of MC iterations increase, and the other is a similar experiment that also compares the different errors for varying number of predictors in the model $f$. The latter experiment is to provide numerical evidence that the number of predictors does not substantially impact the relative error.

In detail, the experiments are set up as follows. A data generating model $Y=f(X)$ is constructed, whose specifics will be provided once each experiment is discussed, and the background dataset $\bar{D}_X$ is sampled from the distribution of $X$ and is of size $100$. The number $K$ of MC iterations will vary in the set $\{2^r\}_{r=9}^{14}$ to depict the asymptotic behavior of the error estimation. As for the error itself, we evaluate the Mean Integrated Squared Error (MISE) between the empirical marginal game value that has been directly computed on all observations in $\bar{D}_X$, and the MC estimate that also samples observations from $\bar{D}_X$.

Specifically, for the empirical marginal quotient game value $h_j[M,\hat{v}^{\ME,\cP}(\cdot;x^{(r)},\bar{D}_{X},f)]$, for some $j\in M$, the estimated MISE between $h_j$ and the MC estimate $\hat{h}_j$ is given by
\begin{equation}\label{MISE}
    \widehat{\MISE}(h_j,\hat{h}_j) := \frac{1}{|\bar{D}_X|}\sum_{x\in \bar{D}_X} \big(h_j[M,\hat{v}^{\ME,\cP}(\cdot;x,\bar{D}_X,f)]-\hat{h}_j[M,\hat{v}^{\ME,\cP}(\cdot;x,\bar{D}_X,f)]\big)^2.
\end{equation}

We obtain an estimate of MISE for every MC estimate $\hat{h}_j$ produced. To construct confidence intervals for MISE, we evaluate $50$ estimates for it by running the MC approach 50 times for each individual observation in $\bar{D}_X$.

Finally, another quantity we estimate is the Relative MISE or RMISE, which is given by
\begin{equation}\label{RMISE}
    \widehat{\text{RMISE}}(h_j,\hat{h}_j) := \frac{\widehat{\text{MISE}}(h_j,\hat{h}_j)}{\frac{1}{|\bar{D}_X|}\sum_{x\in \bar{D}_X} \big(h_j[M,\hat{v}^{\ME,\cP}(\cdot;x,\bar{D}_X,f)]\big)^2}.
\end{equation}

Estimating RMISE will provide insight in the experiments where the number of predictors increases. As we add more and more predictors to a function whose output is bounded (a probability), we expect that the contributions themselves will be smaller across the predictors. This is a consequence of the efficiency property. Therefore, even though MISE will be expected to decrease without any impact from the increasing number of predictors, RMISE may be impacted.

Confidence intervals for RMISE are also constructed. As a final note before we discuss each example separately, the plots for MISE and RMISE are plotted on log-log scales to clearly show the error rate as the number of MC iterations increases.

\medskip
\noindent\textbf{Experiment 1a: Asymptotic behavior of the MC quotient Shapley estimate.}\\
The data generating model is a logistic function with four predictors.
\begin{align*}
    Y &= f(X) = \frac{\sqrt{6}}{1+\exp[-3(X_1-5)+0.2(X_2-15)-2(X_3-2/7)-5X_4]},\\
    X_1 &\sim Normal(5,1),\ X_2 \sim Gamma(3,|X_1|),\ X_3 \sim Beta(2,5),\ X_4 \sim Uniform(-1,1).
\end{align*}
Since $X_1$, $X_2$ are dependent, we form the partition $\cP$ with three groups, $\cP=\{S_1, S_2, S_3\}$ where $S_1=\{1,2\}$, $S_2=\{3\}$, and $S_3=\{4\}$. The group of interest is $S_1$, whose empirical marginal quotient Shapley $\varphi_1[M,\hat{v}^{\ME,\cP}]$ is evaluated directly.

The error estimates for MISE and RMISE for the corresponding estimate $\hat{\varphi}_1[M,\hat{v}^{\ME,\cP}]$ are given in Figure \ref{fig::qshap_asym}, showing their empirical mean after the 50 runs and corresponding $95\%$ confidence intervals. Also plotted in the figure is the theoretical rate and the estimated variance over the number of MC iterations.

\begin{figure}[H]
    \centering
       \begin{subfigure}[t]{0.45\textwidth}
           \centering
           \includegraphics[width=\textwidth]{pics/mise_x0,x1_QSHAP.png}
           \caption{MISE for $\hat{\varphi}_1[M,\hat{v}^{\ME,\cP}]$}
       \end{subfigure}
       ~~
       \begin{subfigure}[t]{0.45\textwidth}
           \centering
           \includegraphics[width=\textwidth]{pics/rmise_x0,x1_QSHAP.png}
           \caption{RMISE for $\hat{\varphi}_1[M,\hat{v}^{\ME,\cP}]$}
       \end{subfigure}
       \caption{ MC error estimate of the empirical marginal quotient Shapley for $S_1$. }\label{fig::qshap_asym}
   \end{figure}

\medskip
\noindent\textbf{Experiment 1b: Asymptotics of the MC Shapley estimate for increasing number of predictors.}\\
The data generating model is a logistic function given by
\begin{align*}
    Y &= f(X) = \frac{\sqrt{6}}{1+\exp[-3(X_1-5)+0.2(X_2-15)-2(X_3-2/7)-5X_4+\sum_{l=5}^p X_l]},\\
    X_1 &\sim Normal(5,1),\ X_2 \sim Gamma(3,|X_1|),\ X_3 \sim Beta(2,5),\ X_4 \sim Uniform(-1,1),\\
    X_l &\sim Normal(0,3), \ l\in \{5,\dots,p\},
\end{align*}
where $p$ takes values in $\{4,5,10,16\}$ (for $p=4$ we assume $\sum_{l=5}^p X_l=0$). Thus, four different models are embedded in the above formula. The purpose of this experiment is to show that as we increase the number of predictors the effect on the error is minimal.

The predictor of interest is $X_1$, whose empirical marginal Shapley value $\varphi_1[N,\hat{v}^{\ME}]$ is evaluated directly. We then perform the 50 MC runs to obtain its corresponding estimate $\hat{\varphi}_1[N,\hat{v}^{\ME}]$ and build the error plots for MISE and RMISE, shown in Figure \ref{fig::shap_asym_inc}, showing their empirical mean and corresponding $95\%$ confidence intervals for each of the four models. Also plotted in the figure is the theoretical rate and the estimated variance over the number of MC iterations, again for each of the four models.

\begin{figure}[H]
    \centering
       \begin{subfigure}[t]{0.45\textwidth}
           \centering
           \includegraphics[width=\textwidth]{pics/mise_x0_allpreds_QSHAP.png}
           \caption{MISE for $\hat{\varphi}_1[N,\hat{v}^{\ME}]$}
       \end{subfigure}
       ~~
       \begin{subfigure}[t]{0.45\textwidth}
           \centering
           \includegraphics[width=\textwidth]{pics/rmise_x0_allpreds_QSHAP.png}
           \caption{RMISE for $\hat{\varphi}_1[N,\hat{v}^{\ME}]$}
       \end{subfigure}
       \caption{ MC error estimate of the empirical marginal Shapley for increasing number of predictors. }\label{fig::shap_asym_inc}
   \end{figure}

\medskip
\noindent\textbf{Experiment 2a: Asymptotic behavior of the MC Owen value estimate.}\\
The data generating model is a logistic function with six predictors.
\begin{align*}
    Y &= f(X) = \frac{\sqrt{6}}{1+\exp[-3(X_1-5)+0.2(X_2-15)-2(X_3-2/7)-5X_4+X_5-0.5(\pi-\frac{1}{\pi})-X_6]},\\
    X_1 &\sim Normal(5,1),\ X_2 \sim Gamma(3,|X_1|),\ X_3 \sim Beta(2,5),\ X_4 \sim Uniform(-1,1),\\
    X_5 &= \exp(X_4)+\epsilon_1, \ \text{and} \ X_6 = X_4^2\sin(\pi X_4) + \epsilon_2, \ \epsilon_1 \sim Normal(0,0.1^2), \ \epsilon_2 \sim Normal(0,0.05^2).
\end{align*}
We form the partition $\cP$ with three groups based on dependencies, $\cP=\{S_1, S_2, S_3\}$ where $S_1=\{1,2\}$, $S_2=\{3\}$, and $S_3=\{4,5,6\}$. The predictor of interest is $X_4$, whose empirical marginal Owen value $Ow_4[N,\hat{v}^{\ME},\cP]$ is evaluated directly.

The error estimates for MISE and RMISE for the corresponding estimate $\hat{Ow}_4[N,\hat{v}^{\ME},\cP]$ are given in Figure \ref{fig::owen_asym}, showing their empirical mean after the 50 runs and corresponding $95\%$ confidence intervals. Also plotted in the figure is the theoretical rate and the estimated variance over the number of MC iterations.

\begin{figure}[H]
    \centering
       \begin{subfigure}[t]{0.45\textwidth}
           \centering
           \includegraphics[width=\textwidth]{pics/mise_x3_OWEN.png}
           \caption{MISE for $\hat{Ow}_4[N,\hat{v}^{\ME},\cP]$}
       \end{subfigure}
       ~~
       \begin{subfigure}[t]{0.45\textwidth}
           \centering
           \includegraphics[width=\textwidth]{pics/rmise_x3_OWEN.png}
           \caption{RMISE for $\hat{Ow}_4[N,\hat{v}^{\ME},\cP]$}
       \end{subfigure}
       \caption{ MC error estimate of the empirical marginal Owen value for $X_4$. }\label{fig::owen_asym}
   \end{figure}

\medskip
\noindent\textbf{Experiment 2b: Asymptotics of the MC Owen estimate for increasing number of predictors.}\\
The data generating model is a logistic function given by
\begin{align*}
    Y &= f(X) = \frac{\sqrt{6}}{1+\exp[-3(X_1-5)+0.2(X_2-15)-2(X_3-2/7)-5X_4+\sum_{l=5}^p X_l]},\\
    X_1 &\sim Normal(5,1),\ X_2 \sim Gamma(3,|X_1|),\ X_3 \sim Beta(2,5),\ X_4 \sim Uniform(-1,1),
\end{align*}
and $(X_5,\dots,X_p)$, where $p$ takes values in $\{6,10,14,18\}$, follows a Multivariate Normal distribution with $\E[X_l]=0$, $Var[X_l]=3$, and $Cov[X_l,X_q]=0.1$, $l,q\in \{5,\dots,p\}$, $l\ne q$. Thus, four different models are embedded in the above formula. As with Experiment 1b, the purpose of this experiment is to show that as we increase the number of predictors the effect on the error is minimal.

The predictor of interest is $X_5$, whose empirical marginal Owen value $Ow_5[N,\hat{v}^{\ME},\cP]$ is evaluated directly. The partition $\cP$ is formed by dependencies, so that $\cP=\{S_1,S_2,S_3,S_4\}$ where $S_1=\{1,2\}$, $S_2=\{3\}$, $S_3=\{4\}$, and $S_4=\{5,\dots,p\}$. We then perform the 50 MC runs to obtain the empirical marginal Owen value MC estimate $\hat{Ow}_5[N,\hat{v}^{\ME},\cP]$ and build the error plots for MISE and RMISE, shown in Figure \ref{fig::owen_asym_inc}, showing their empirical mean and corresponding $95\%$ confidence intervals for each of the four models. Also plotted in the figure is the theoretical rate and the estimated variance over the number of MC iterations, again for each of the four models.

\begin{figure}[H]
    \centering
       \begin{subfigure}[t]{0.45\textwidth}
           \centering
           \includegraphics[width=\textwidth]{pics/mise_x4_allpreds_OWEN.png}
           \caption{MISE for $\hat{Ow}_5[N,\hat{v}^{\ME},\cP]$}
       \end{subfigure}
       ~~
       \begin{subfigure}[t]{0.45\textwidth}
           \centering
           \includegraphics[width=\textwidth]{pics/rmise_x4_allpreds_OWEN.png}
           \caption{RMISE for $\hat{Ow}_5[N,\hat{v}^{\ME},\cP]$}
       \end{subfigure}
       \caption{ MC error estimate of the empirical marginal Owen value as the number of predictors increases. }\label{fig::owen_asym_inc}
   \end{figure}

\medskip
\noindent\textbf{Experiment 3a: Asymptotic behavior of the MC two-step Shapley estimate.}\\
The setup is the same as with Experiment 2a. The difference is that for the predictor of interest $X_4$ we estimate the empirical marginal two-step Shapley value $TSh_4[N,\hat{v}^{\ME},\cP]$.

The error estimates for MISE and RMISE for $\hat{TSh}_4[N,\hat{v}^{\ME},\cP]$ are given in Figure \ref{fig::ts_asym}, showing their empirical mean after the 50 runs and corresponding $95\%$ confidence intervals. Also plotted in the figure is the theoretical rate and the estimated variance over the number of MC iterations.

\begin{figure}[H]
    \centering
       \begin{subfigure}[t]{0.45\textwidth}
           \centering
           \includegraphics[width=\textwidth]{pics/mise_x3_TS.png}
           \caption{MISE for $\hat{TSh}_4[N,\hat{v}^{\ME},\cP]$}
       \end{subfigure}
       ~~
       \begin{subfigure}[t]{0.45\textwidth}
           \centering
           \includegraphics[width=\textwidth]{pics/rmise_x3_TS.png}
           \caption{RMISE for $\hat{TSh}_4[N,\hat{v}^{\ME},\cP]$}
       \end{subfigure}
       \caption{ MC error estimate of the empirical marginal two-step Shapley for $X_4$. }\label{fig::ts_asym}
   \end{figure}

\medskip
\noindent\textbf{Experiment 3b: Asymptotics for MC two-step Shapley as the number of predictors increases.}\\
The setup is the same as with Experiment 2b. The difference is that for the predictor of interest $X_5$ we estimate the empirical marginal two-step Shapley value $TSh_5[N,\hat{v}^{\ME},\cP]$.

We then perform the 50 MC runs to obtain the empirical marginal two-step Shapley value MC estimate $\hat{TSh}_5[N,\hat{v}^{\ME},\cP]$ and build the error plots for MISE and RMISE, shown in Figure \ref{fig::ts_asym_inc}, showing their empirical mean and corresponding $95\%$ confidence intervals for each of the four models. Also plotted in the figure is the theoretical rate and the estimated variance over the number of MC iterations, again for each of the four models.

\begin{figure}[H]
    \centering
       \begin{subfigure}[t]{0.45\textwidth}
           \centering
           \includegraphics[width=\textwidth]{pics/mise_x4_allpreds_TS.png}
           \caption{MISE for $\hat{TSh}_5[N,\hat{v}^{\ME},\cP]$}
       \end{subfigure}
       ~~
       \begin{subfigure}[t]{0.45\textwidth}
           \centering
           \includegraphics[width=\textwidth]{pics/rmise_x4_allpreds_TS.png}
           \caption{RMISE for $\hat{TSh}_5[N,\hat{v}^{\ME},\cP]$}
       \end{subfigure}
       \caption{ MC error estimate of the empirical marginal two-step Shapley as the number of predictors increases. }\label{fig::ts_asym_inc}
   \end{figure}

\medskip
\noindent\textit{Conclusion.} As we can see from the plots, the error decays as the theory dictates in all experiments. Furthermore, we see that the increase in number of predictors has minimal impact on the absolute error. However, as discussed earlier, for the relative MISE we see that the error does slightly increase for more predictors due to the contributions being smaller as they are dispersed across more predictors.