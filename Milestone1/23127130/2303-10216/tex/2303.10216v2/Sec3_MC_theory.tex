\section{Monte Carlo sampling for game values}\label{sec::MC_theory}

The two games defined in \eqref{margcondgames} are both noteworthy and have seen extended use in the ML explainability literature when utilizing the corresponding game values. In this section we design Monte Carlo approximation techniques for marginal game values and provide the necessary theoretical backing for these approximations. The marginal game is a natural choice for our approach, which approximates marginal game values in a fast, accurate and model-agnostic manner. Approximation techniques have been designed for conditional game values; see \cite{aas2021}. We start with linear game values of the form \eqref{lingamevalue} and then generalize our approach to quotient game and coalitional game values.

\subsection{Sampling for marginal linear game values}

When utilizing the marginal game, observe that the linear game value formula \eqref{lingamevalue} consists of $2^n$ terms and each term is an expectation. Naively replacing these expectations with an empirical average over a background dataset $\bar{D}_X$ would lead to the  computation of the empirical marginal game value that has  computational complexity of $O(2^n \cdot |\bar{D}_X|)$ and approximates the marginal one with the statistical estimation accuracy of $O(|\bar{D}_X|^{-1/2})$ as indicated in Lemma \ref{lmm::error_emp_marg_estimator}. This would be tremendously high for practical purposes.

The key in our analysis is to observe that the game value element $h_i[N,\vpdp]$ can be expressed as an expected value of an appropriate function with respect to a product measure which is defined on the joint space of predictors and coalitions.

For our analysis we make the following assumptions for the coefficients $w_i(S,N)$ of the game value:
\begin{enumerate}
    \renewcommand{\labelenumi}{\textbf{(\theenumi)}}
    \renewcommand{\theenumi}{A\arabic{enumi}}

    \item\label{hyp::coeff_nonneg} $w_i(S,N)\ge 0$, for any $S\subseteq N\setminus \{i\}$.
    \item\label{hyp::coeff_sumtoone} $\displaystyle \sum_{S\subseteq N\setminus \{i\}}w_i(S,N)=1$.
\end{enumerate}
In principle, the first assumption would suffice since one can normalize the coefficients and scale $h_i[N,\vpdp]$ by $\sum_{S\subseteq N\setminus \{i\}}w_i(S,N)$. By assuming both \eqref{hyp::coeff_nonneg} and \eqref{hyp::coeff_sumtoone} the coefficients become probabilities, in turn defining the following collection of probability measures which simplify our analysis.

\begin{definition}\label{def::P_h}
    Suppose \eqref{hyp::coeff_nonneg}-\eqref{hyp::coeff_sumtoone} hold for the coefficients of a game value $h$ whose elements are as in \eqref{lingamevalue}. For each $i \in N$ we define the discrete probability measure $P_i^{(h)}$ defined on subsets of $N \setminus \{i\}$ satisfying $P_i^{(h)}(S) = w_i(S,N)$ for each $S \subseteq N\setminus \{i\}$.
\end{definition}

\begin{remark}\rm\label{rem::shapley_probs}
    The probability $P_i^{(h)}(S)$ can be viewed as the probability of selecting the coalition $S$ under certain conditions. For instance, if one assumes that distinct sizes of $ S\subseteq N \setminus \{i\}$ are equally likely to occur and that given a specific size all distinct sets $S$ are equally likely, we have $P_i^{(h)}(S)=\frac{|S|!(n-|S|-1)!}{n!}$ as in \eqref{shapform}.
\end{remark}

First, we state an auxiliary lemma that represents a game value as an expectation of the random variable defined on the set of coalitions.
\begin{lemma}\label{game_val_expect_prelim}
Let $N,v,h[N,v]$ be as in \eqref{lingamevalue} and $\{P_i^{(h)}\}_{i \in N}$ as in Definition \ref{def::P_h}. Then
\begin{equation}\label{exp_repr_game_prelim}
h_i[N,v] = \int \big(v( S\cup \{i\} )-v( S )\big) P_i^{(h)}(dS), \quad i \in N.
\end{equation}
\end{lemma}

The representation \eqref{exp_repr_game_prelim} allows one to approximate $h_i[N,v]$ using an MC method that involves sampling from  $P^{(h)}_i$ a sequence of coalitions $S^{(k)} \subseteq N \setminus \{i\}$, $k \in \{1,\dots,K\}$, evaluating the difference $v( S^{(k)} \cup \{i\} )-v(S^{(k)})$ and then averaging the result. This approach, however, is not optimal when one deals with  the marginal game $\vpdp$ because $\vpdp(S^{(k)};x^*,X,f)$ itself is an expectation of $f$ with respect to $P_{X_{-S^{(k)}}}$, which in practice is replaced with the appropriate average over a background dataset $\bar{D}_X$. In this case, such an MC procedure would yield an MC estimator of the empirical marginal coalitional value $h_i[N,\hat{v}^{\ME}(\cdot;x^*,\bar{D}_X,f)]$ with computational complexity  $O(K \cdot |\bar{D}_X|)$. When one is interested in the approximation of the true marginal value, it is optimal (from the perspective of the error-complexity trade-off) to choose $K=|\bar{D}_X|$, in which case the complexity becomes $O(K^2)$ and the obtained value estimates the true marginal game value with the mean squared error $O(K^{-1})$ according to Lemma \ref{lmm::error_emp_marg_estimator}.

This discussion motivates us to adjust the above result with the goal of reducing the complexity from $O(K^2)$ to $O(K)$ with $K=|\bar{D}_X|$. To this end, for each $i\in N$ we view coalitions $S\subseteq N\setminus \{i\}$ and observations $x$ from $P_X$ as pairs coming from the product space $P^{(h)}_i \otimes P_X$. With this setup one can write the marginal game value $h_i[N,\vpdp(\cdot;x^*,X,f)]$ as an expected value with respect to a product of two probability measures as shown below.

\begin{definition}
Let $f\in [f] \in L^p(\tilde{P}_X)$ and $U^{(f)}_p \in \B(\R^n)$ be a set of $P_X$-probability $1$ as in Lemma \ref{lmm::game_meas_marg}. Set
\begin{equation}\label{param_marg_norm}
\nu_{x^*}^{(p)}(f) := \sum_{S \subseteq N} \int |f(x^*_S,x_{-S})|^p P_{X_{-S}}(dx_{-S}) < \infty, \quad  x^* \in U_p^{(f)}.
\end{equation}
\end{definition}

\begin{theorem}\label{thm::gamevalue_as_expectation} Let $f\in [f] \in L^p(\tilde{P}_X)$, $p \geq 1$. Let $h[\cdot,N]$ be as in \eqref{lingamevalue}, and suppose that the coefficients $w_i(S,N)$ of $h_i$, $i\in N$, satisfy \eqref{hyp::coeff_nonneg}-\eqref{hyp::coeff_sumtoone}. 

\begin{itemize}
    \item [(i)] For $P_X$-almost sure $x^* \in \R^n$, the map
\begin{equation}\label{Delta_def}
(S,x) \mapsto \Delta_i(S,x; x^*,f):=f(x^*_{S\cup \{i\}},x_{-S\cup \{i\}})-f(x^*_S,x_{-S})
\end{equation}
is $ P_i^{(h)} \otimes P_X $-measurable and $p$-power integrable satisfying the bound
\begin{equation}\label{Delta_bound}
\int |\Delta_i(S,x;x^*,f)|^p  \, [P_i^{(h)} \otimes P_X](dS,dx) \leq  2^p \cdot \bar{w}_{i,N} \cdot \nu_{x^*}^{(p)}(f), \quad \bar{w}_{i,N}:=\max_{S \subseteq N \setminus \{i\}} w_i(S,N).
\end{equation}

\item [(ii)] For $P_X$-almost sure $x^* \in \R^n$ the game value $h_i[N,\vpdp(\cdot ; x^*, X, f)]$ is well-defined and can be written as an expectation with respect to  the product probability measure $P_i^{(h)}\otimes P_X$. Specifically,
\begin{equation}\label{eq::h_as_expectation}
    h_i[N,\vpdp(\cdot; x^*, X,f)]=\int \Delta_i(S,x;x^*,f)\,[P_i^{(h)} \otimes P_X](dS,dx).
\end{equation}

Consequently, taking random samples $(\mathcal{S}^{(k)},X^{(k)})\sim P_i^{(h)}\otimes P_X$, $k \in \{1,2,\dots,K\}$, we have 
    \begin{equation}\label{eq::h_approx}
        \frac{1}{K}\sum_{k=1}^K \Delta_i(\mathcal{S}^{(k)}, X^{(k)};x^*,f) = h_i[N,\vpdp(\cdot ; x^*,X,f)] + \mathcal{E}_K,
    \end{equation}
    where $\mathcal{E}_K\to 0$ in probability as $K\to \infty$. In addition, if $f \in [f] \in L^2(\tilde{P}_X)$, then $\mathcal{E}_K \to 0$ in $L^2(\P)$ with rate $O(K^{-1/2})$.
\end{itemize}
\end{theorem}

\begin{proof}
Since $f \in [f] \in L^p(\tilde{P}_X)$ it follows from Lemma \ref{lmm::game_meas_marg} that for $P_X$-almost sure $x^*$ and each $S^* \subset N$ the map $x \mapsto \Delta_i(S^*,x; x^*,f)$ is $P_X$-measurable. Since $P_i^{(h)}$ is discrete, we conclude that the map $(S,x) \mapsto \Delta_i(S,x; x^*,f)$ is measurable with respect to $P_i^{(h)} \otimes P_X$. Furthermore, we have
\begin{equation}
\begin{aligned}
\int |\Delta_i(S,x;x^*,f)|^p  \, [P_i^{(h)} \otimes P_X](dS,dx) &= \sum_{S\subseteq N} w_i(S,N) \int |f(x^*_{S\cup \{i\}},x_{-S\cup \{i\}})-f(x^*_S,x_{-S})|^p P_X(dx) \\
\leq 2^p &\cdot \bar{w}_{i,N}  \cdot \sum_{S\subseteq N} \int |f(x^*_S,x_{-S})|^p P_X(dx) = 2^p \cdot \bar{w}_{i,N}  \cdot \nu_{x_*}^{(p)}(f),
\end{aligned}
\end{equation}
which proves \eqref{Delta_bound}.

Since $f \in [f] \in L^1(\tilde{P}_X)$, by Lemma \ref{lmm::game_meas_marg} there exists a Borel set $U^{(f)}_1$ of $P_X$-probability $1$ such that the marginal game $\vpdp(\cdot;x^*,X,f)$ and the game value $h[N,\vpdp(\cdot;x^*,X,f)]$ are well-defined for all $x^* \in U_1^{(f)}$.

Let $x^* \in U^{(f)}_1$. Recalling that 
\[
    \vpdp( S; x^*,X,f ) = \E\left[ f(x^*_S,X_{-S}) \right]
    =\int f(x^*_S,X_{-S}(\omega))\,\P(d\omega) =\int f(x^*_S,x_{-S})\,P_{X_S}(dx_{S}) ,
\]
we obtain
    \begin{align*}
        &h_i[N,\vpdp(\cdot;x^*,X,f)] = \sum_{S \subseteq N \setminus \{i\}} w_i(S,N) \left[ \vpdp(S \cup \{i\};x^*,X,f) - \vpdp( S;x^*,X,f ) \right]\\
        &=\int \left[ \vpdp(S \cup \{i\}; x^*,X,f) - \vpdp( S;x^*,X,f ) \right]P_i^{(h)}(dS)\\
        &=\int\int f(x^*_{S\cup\{i\}},x_{-(S\cup \{i\})}) P_{X_{-(S \cup \{i\})}}(dx_{-(S \cup \{i\})}) P_i^{(h)}(dS)  - \int\int f(x^*_S,x_{-S}) P_{X_S}(dx_S) P_i^{(h)}(dS) .
    \end{align*}
Since the integrands in the first and second terms of the last expression do not explicitly depend on $X_{S\cup \{i\}}$ and $X_S$, respectively, we can write
    \begin{align*}
        h_i[N,\vpdp]
        &=\int\int f(x^*_{S\cup\{i\}},x_{-(S\cup \{i\})})P_i^{(h)}(dS)P_X(dx) - \int\int f(x^*_S,x_{-S}) P_i^{(h)}(dS)P_X(dx).
    \end{align*}
Putting the two integrals together we obtain \eqref{eq::h_as_expectation}.

For \eqref{eq::h_approx}, we set 
\[
\displaystyle \mathcal{E}_K := \frac{1}{K}\sum_{k=1}^K \Delta_i(\mathcal{S}^{(k)},X^{(k)};x^*,f) - h_i[N,\vpdp(\cdot;x^*,X,f)]
\]
and observe that $\E[\mathcal{E}_K]=0$. Since the samples $(\mathcal{S}^{(k)},X^{(k)})$ are independent and identically distributed, the average converges in probability to $h_i[N,\vpdp(\cdot;x^*,X,f)]$ by the weak law of large numbers. 

Suppose now that $f \in [f] \in L^2(\tilde{P}_X)$. Then for the error rate, we have
\[
    \|\mathcal{E}_K\|_{L^2(\P)}^2=Var\left[\frac{1}{K}\sum_{k=1}^K \Delta_i(\mathcal{S}^{(k)},X^{(k)};x^*,f)\right]=\frac{1}{K}Var[\Delta_i(\mathcal{S}^{(1)},X^{(1)};x,f)]\le \frac{4}{K} \cdot \bar{w}_{i,N} \cdot \nu^{(2)}_{x_*}(f),
\]
which implies that $\mathcal{E}_K$ converges to zero in $L^2(\P)$ at a rate of $1/\sqrt{K}$.
\end{proof}

Recall that $\Delta_i(\mathcal{S},X;x,f)$ having finite variance is not a necessary condition for the weak law of large numbers. Convergence still occurs but at a slower rate. In either case, equation \eqref{eq::h_approx} informs us on how to approximate the game value $h[N,\vpdp]$ in practice. This leads us to design the following sampling algorithm:

\begin{algorithm}
    \SetAlgoLined 
    \KwIn{Observation $x^*\in \R^n$, model $f$, dataset $\bar{D}_X=\{x^{(k)}\}_{k=1}^K$.
    }
    \KwOut{MC estimate $\hat{h}[N,\vpdp]$ of $h[N,\vpdp]$.
    }
    \BlankLine
    \For{$i$ in $\{1,\dots,n\}$}
    {
        $\hat{h}_i := 0$;\\
        \For{$k$ in $\{1,\dots,K=|\bar{D}_X|\}$}
        {
            Select the observation $x^{(k)}$ from $\bar{D}_X$;\\
            Draw a random coalition $S^{(k)}$ from the distribution $P_i^{(h)}$;\\
            $\hat{h}_i :=\hat{h}_i + \Delta_i(x^{(k)},S^{(k)};x^*,f)$;
        }
        $\hat{h}_i := \hat{h}_i/K$;
    }
    \Return{$\hat{h}[N,\vpdp] := \big(\hat{h}_1,\dots,\hat{h}_n\big)$;}
\caption{Monte Carlo sampling for linear game values}\label{algo_lingamevalue}\label{algo_game_marg}
\end{algorithm}

\begin{remark}\rm\label{rm::emp_marg_lingameval}
In some applications one may be interested in computing the empirical marginal game value $h[N,\hat{v}^{\ME}(\cdot;x^*,\bar{D}_X,f)]$ whose computational complexity is $O(2^n \cdot |\bar{D}_X|)$. In this case, even if the background dataset is small, computing the empirical game value directly becomes infeasible for large $n$. One can then estimate the empirical marginal game value using an adjusted version of Algorithm \ref{algo_game_marg}. The adjustments are as follows. The input to the algorithm must contain the number $\tilde{K}$ of MC iterations, which is now independent of the number of samples in the background dataset $\bar{D}_X$. Line 3 must contain the for-loop over $k \in \{1,\dots,\tilde{K}\}$ and line 4 must be replaced with ``Draw a random observation $x^{(k)}$ from the empirical distribution $P_{\bar{D}_X}$''.

The value produced by the adjusted Algorithm \ref{algo_game_marg} is the estimation of $h[N,\hat{v}^{\ME}(\cdot;x^*,\bar{D}_X,f)]$ with a mean squared error of estimation proportional to $O(\tilde{K}^{-1})$, which is independent of  $|\bar{D}_X|$. Since the number $\tilde{K}$ of MC samples is not limited by $|\bar{D}_X|$, one can achieve arbitrary estimation accuracy by indefinitely sampling from $P_i^h \otimes P_{\bar{D}_X}$. Finally, we note that the difference between the empirical marginal game value and the true marginal game value in $L^2(\PP)$ is bounded by $O(|\bar{D}_X|^{-1/2})$ as discussed in Lemma \ref{lmm::error_emp_marg_estimator}.
\end{remark}

\subsection{Sampling for marginal quotient game values}

As mentioned in Section \ref{sec::preliminaries}, group explainers can have reduced complexity compared to that of linear game values, but it would still be considerably high for any practical implementation. Following the analysis presented above for linear game values, we propose a similar theorem and algorithm for quotient game values.

We make the same assumptions for the coefficients $w_j(A,M)$ of $h_{S_j}^{\cP}$, $j\in M$, as before. Thus, stating that \eqref{hyp::coeff_nonneg}-\eqref{hyp::coeff_sumtoone} hold for the coefficients of $h_{S_j}^{\cP}$ we mean that $w_j(A,M)\ge 0$ for any $A\subseteq M\setminus \{j\}$, and $\sum_{A\subseteq M\setminus \{j\}}w_j(A,M)=1$. This leads to the following definition.

\begin{definition}\label{def::P_hP}
    Let $\cP=\{S_1,\dots,S_m\}$. Suppose \eqref{hyp::coeff_nonneg}-\eqref{hyp::coeff_sumtoone} hold for the coefficients of a quotient game value element $h_{S_j}^{\cP}$, as given in Definition \ref{def::quotientexpl}. For each $j \in M$ we define the discrete probability measure $P_j^{(h,\cP)}$ that satisfies $P_j^{(h,\cP)}(A) = w_j(A,M), A \subseteq M\setminus \{j\}$.
\end{definition}

\begin{definition}
Let $f\in [f] \in L^p(\tilde{P}_X)$, $U^{(f)}_p \in \B(\R^n)$ be a set of $P_X$-probability $1$ as in Lemma \ref{lmm::game_meas_marg}, and $\cP=\{S_1,\dots,S_m\}$ a partition of $N$. Set
\begin{equation}\label{param_marg_norm_quot}
\nu_{x^*}^{(p)}(f,\cP) := \sum_{A \subseteq M, Q_A=\cup_{\alpha \in A} S_{\alpha}} \int |f(x^*_{Q_A},x_{-Q_A})|^p P_{X_{-Q_A}}(dx_{-Q_A}) < \infty, \quad  x^* \in U_p^{(f)}.
\end{equation}
\end{definition}

We now state the equivalent of Theorem \ref{thm::gamevalue_as_expectation} for quotient game values:

\begin{theorem}\label{thm::quotientexplainer_as_expectation}
    Let $f \in [f] \in L^p(\tilde{P}_X)$, $p \geq 1$. Let $\cP$ and $h^{\cP}$ be as in Definition \ref{def::quotientexpl}, and suppose that the coefficients $w_j(A,M)$ of $h_{S_j}^{\cP}$, $j\in M$, satisfy \eqref{hyp::coeff_nonneg}-\eqref{hyp::coeff_sumtoone}.

\begin{itemize}
    \item [$(i)$] For $P_X$-almost sure $x^* \in \R^n$, the map
\begin{equation}\label{Delta_quot_def}
(A,x) \mapsto \Delta_j(A,x;x^*,f):=f(x^*_{\cup_{\alpha \in A\cup \{j\}}S_{\alpha}},x_{-\cup_{\alpha \in A\cup \{j\}}S_{\alpha}})-f(x^*_{\cup_{\alpha \in A}S_{\alpha}},x_{-\cup_{\alpha \in A}S_{\alpha}}),
\end{equation}
where $A \subseteq M \setminus \{i\}$, is $ P_j^{(h,\cP)} \otimes P_X $-measurable and $p$-power integrable satisfying the bound
\begin{equation}\label{Delta_bound_quot}
\int |\Delta_j(A,x;x^*,f)|^p  \, [P_j^{(h,\cP)} \otimes P_X](dA,dx) \leq  2^p \cdot \bar{w}_{j,M} \cdot \nu_{x^*}^{(p)}(f,\cP).
\end{equation}

\item [$(ii)$] For $P_X$-almost sure $x^*\in \R^n$ the marginal quotient explainer $h_{S_j}^{\cP}(x^*;X,f,\vpdp)$ is well-defined and can be written as an expectation with respect to the product measure $P_j^{(h,\cP)}\otimes P_X$. Specifically,
\begin{equation}\label{eq::h^P_as_expectation}
    h_{S_j}^{\cP}(x^*;X,f,\vpdp)=\int \Delta_j(A,x;x^*,f)\,[P_j^{(h,\cP)}\otimes P_X](dA,dx)
\end{equation}

Consequently, taking random samples $(\mathcal{A}^{(k)},X^{(k)})\sim P_j^{(h,\cP)}\otimes P_X$, $k \in \{1,2,\dots,K\}$, we have
    \begin{equation}\label{eq::h^P_approx}
        \frac{1}{K}\sum_{k=1}^K \Delta_j(\mathcal{A}^{(k)},X^{(k)};x^*,f) = h_{S_j}^{\cP}(x^*;X,f,\vpdp) + \mathcal{E}_K^{\cP},
    \end{equation}
    where $\mathcal{E}_K^{\cP}\to 0$ in probability as $K\to \infty$. In addition, if $f \in [f] \in L^2(\tilde{P}_X)$, then $\mathcal{E}_K^{\cP} \to 0$ in $L^2(\P)$ with rate $O(K^{-1/2})$.
\end{itemize}
\end{theorem}

\begin{proof}
    The proof is similar to that of Theorem \ref{thm::gamevalue_as_expectation}. Specifically, the measurability of $(A,x) \mapsto \Delta_j(A,x;x^*,f)$ for $P_X$-almost sure $x^*$ follows from Lemma \ref{lmm::game_meas_marg} while the bound \eqref{Delta_bound_quot} follows from the definition of the product measure $P_j^{(h,\cP)} \otimes P_X$ and that of $\nu_{x^*}^{(p)}(f,\cP)$. The representation \eqref{eq::h^P_as_expectation} follows from the definition of the game value $h$ and that of $P_j^{(h,\cP)} \otimes P_X$. The equation \eqref{eq::h^P_approx} is a consequence of the weak law of large numbers. Finally, if $f \in [f] \in L^2(\tilde{P}_X)$, then for $P_X$-almost sure $x^*$ the error term $\EE^{\cP}_K$ is bounded in $L^2(\PP)$ by $[K^{-1} \cdot Var(\Delta_j(\mathcal{A}^{(1)},X^{(1)};x^*,f))]^{\frac{1}{2}}$ with $Var(\Delta_j(\mathcal{A}^{(1)},X^{(1)};x^*,f))$ bounded by $4 \cdot \bar{w}_{j,M} \cdot \nu_{x^*}^{(2)}(f,\cP)$.
\end{proof}

\begin{remark}\rm
The assumption of Theorem \ref{thm::quotientexplainer_as_expectation}, that the model $f \in [f] \in L^p(\tilde{P}_X)$, can be relaxed by requiring that $f \in [f] \in L^p(\tilde{P}_{X,\cP})$.
\end{remark}

\begin{remark}\rm
    If the partition $\cP=\{S_1,\dots,S_m\}$ is based on dependencies, then the marginal and conditional quotient games coincide when group predictors $X_{S_1},X_{S_2},\dots,X_{S_m}$ are independent. This means that one can use \eqref{eq::h^P_approx} to approximate $h_{S_j}^{\cP}(x;X,f,\vce)$ and obtain group explanations that take into account both the model structure and joint distribution of the predictors.
\end{remark}

The associated algorithm with the above theorem is similar to Algorithm \ref{algo_lingamevalue}. The difference between the two is the perspective on the players. In Algorithm \ref{algo_lingamevalue} the players are the predictors, while in Algorithm \ref{algo_quotgamevalue} the players are predictor groups. This is why the latter algorithm has the partition $\cP$ as an extra input requirement.

\begin{algorithm}
    \SetAlgoLined 
    \KwIn{Observation $x^*\in \R^n$, model $f$, partition $\cP=\{S_1,\dots,S_m\}$, dataset $\bar{D}_X=\{x^{(k)}\}_{k=1}^K$.
    }
    \KwOut{MC estimate $\hat{h}^{\cP}$ of $h^{\cP}(x^*;X,f,\vpdp)$.
    }
    \BlankLine
    \For{$j$ in $\{1,\dots,m\}$}
    {
        $\hat{h}_j^{\cP} := 0$;\\
        \For{$k$ in $\{1,\dots,K=|\bar{D}_X|\}$}
        {
            Select the observation $x^{(k)}$ from $\bar{D}_X$;\\
            Draw a random coalition $A^{(k)}$ from the distribution $P_j^{(h,\cP)}$;\\
            $\hat{h}_j^{\cP} := \hat{h}_j^{\cP} + \Delta_j(A^{(k)},x^{(k)};x^*,f)$;
        }
        $\hat{h}_j^{\cP} := \hat{h}_j^{\cP}/K$;
    }
    \Return{$\hat{h}^{\cP} = \big(\hat{h}_1^{\cP},\dots,\hat{h}_m^{\cP}\big)$;}
\caption{Monte Carlo sampling for quotient game values}\label{algo_quotgamevalue}\label{algo_quot_game_marg}
\end{algorithm}

\begin{remark}\rm\label{rm::emp_marg_quotgameval}
As before, to approximate the empirical marginal quotient game value $h[M,\hat{v}^{\ME,\cP}(\cdot;x^*,\bar{D}_X,f)]$ one needs to adjust Algorithm \ref{algo_quot_game_marg} as follows. The input to the algorithm must contain the number $\tilde{K}$ of MC iterations, specified independently of $|\bar{D}_X|$. Line 3 must contain the for-loop over $k \in \{1,\dots,\tilde{K}\}$ and line 4 must be replaced with ``Draw a random observation $x^{(k)}$ from the empirical distribution $P_{\bar{D}_X}$''. 

The value produced by the adjusted Algorithm \ref{algo_quot_game_marg} is the estimate of $h[M,\hat{v}^{\ME,\cP}(\cdot;x^*,\bar{D}_X,f)]$ with a mean squared error of $O(\tilde{K}^{-1})$. Finally, as in Lemma \ref{lmm::error_emp_marg_estimator}, it can be shown that the difference between the empirical marginal quotient value and the marginal quotient value in $L^2(\PP)$ is bounded by $O(|\bar{D}_X|^{-1/2})$.
\end{remark}

\subsection{Sampling for marginal coalitional values}

Quotient game explainers have several advantages over linear game values, especially when the partition $\cP$ is based on dependencies. However, the drawback is when the user is interested in explaining the contribution of each single predictor to the model output, while also taking into account the groups. This information can only be provided by coalitional explainers  as defined in Definition \ref{def::coalvalue}.

The main question we want to answer in this section is how to set up an MC sampling algorithm for a coalitional value $g[N,v,\cP]$. If one can write the element $g_i[N,v,\cP]$ in the form \eqref{lingamevalue} then Algorithm \ref{algo_lingamevalue} can be used to carry out the approximation. Given the form of some notable coalitional values such as the Owen and Banzhaf-Owen values, we assume the following form for $g_i[N,v,\cP]$:
\begin{equation}\label{coalvalform}
    g_i[N,v,\cP] = \sum_{A\subseteq M\setminus \{j\}}\sum_{T\subseteq S_j\setminus \{i\}} w_j^{(1)}(A,M) w_i^{(2)}(T,S_j) \left[ v(Q_A \cup T \cup \{i\}) - v(Q_A \cup T) \right], \quad i\in S_j,
\end{equation}
where $Q_A=\cup_{\alpha \in A}S_{\alpha}$. As with the previous subsections, we assume that $\{w^{(1)}_j(A,M)\}_{A\subseteq M\setminus \{j\}}$ and $\{w^{(2)}_i(T,S_j)\}_{T\subseteq S_j \setminus \{i\}}$ are probabilities. We again state this by saying that these sets of coefficients satisfy \eqref{hyp::coeff_nonneg}-\eqref{hyp::coeff_sumtoone}. 

Equation \eqref{coalvalform} can be written in the form of \eqref{lingamevalue}. However, this will present certain practical difficulties for the sampling procedure. For example, observe that \eqref{coalvalform} can be written as
\[
    g_i[N,v,\cP]=\sum_{S \subseteq N\setminus \{i\}} W_i(S,N,\cP)\left[ v(S \cup \{i\}) - v(S) \right],
\]
where for $i \in S_j$ we have
\[
    W_i(S,N,\cP) = 
\left\{ 
\begin{aligned}
&w^{(1)}_j(A,M)\cdot w^{(2)}_i(T,S_j), \text{if $S=\cup_{\alpha \in A} S_{\alpha} \cup T,\ A \subseteq M \setminus \{j\},\ T \subseteq S_j\setminus\{i\}$}\\
&0, \text{otherwise}.\\
\end{aligned}
\right.
\]
Thus, sampling based on the coefficients $W_i(S,N,\cP)$, $S \subseteq N$ may not be a trivial task as many coalitions $S \subseteq N$ have zero probability. For this reason, we adjust the sampling procedure to take into account the coalitional structure of \eqref{coalvalform}; see the discussion on the two-step property of coalitional values in \cite[\S 5.4.1]{grouppaper} Specifically, given a game $v$ on $N$, a partition $\cP=\{S_1,\dots,S_m\}$ and $i \in S_j$, the formulation \eqref{coalvalform} motivates the following two-step sampling procedure. One first samples a subset of $A \subseteq M$ with probability defined by the coefficients $w^{(1)}_j(A,M)$, which induces the union $Q_A=\cup_{\alpha \in A} S_{\alpha}$ of the elements of $\cP$. Then, one samples a set $T \subseteq S_j \setminus \{i\}$ with probability defined by the coefficients $w^{(2)}_i(T,S_j)$. Then forming the set $S=Q_A \cup T$ and evaluating the quantity $v(S \cup \{i\})-v(S)$ yields a sample of a random variable whose expectation is given by \eqref{coalvalform}. More formally, we have the following.

\begin{definition}\label{def::P_S_j}
    Let $g[N,v,\cP]$ be as in \eqref{coalvalform} and suppose $\{w^{(1)}_j(A,M)\}_{A\subseteq M\setminus \{j\}}$ and $\{w^{(2)}_i(T,S_j)\}_{T\subseteq S_j \setminus \{i\}}$ satisfy \eqref{hyp::coeff_nonneg}-\eqref{hyp::coeff_sumtoone}. For each $j \in M$, we define $P^{(g,\cP)}_j$ to be the probability measure satisfying $P_j^{(g,\cP)}(A)=w^{(1)}_j(A,M)$, $A\subseteq M \setminus \{j\}$, and for each $i \in S_j$, $S_j \in \cP$, we define the probability measure $P_i^{(g,S_j)}$ such that $P_i^{(g,S_j)}(T) = w^{(2)}_i(T,S_j)$, $T \subseteq S_j \setminus \{i\}$.
\end{definition}

First, we state an auxiliary lemma that represents a coalitional game value as an expectation of the random variable defined on the set of coalitions.
\begin{lemma}\label{coal_val_expect_prelim}
Let $g[N,v,\cP]$ be as in \eqref{coalvalform}, $\cP=\{S_1,\dots,S_m\}$, and suppose $i \in S_j$. Then
\[
g_i[N,v,\cP] = \int \big(v( \cup_{\alpha \in A}S_{\alpha} \cup T \cup \{i\} )-v( \cup_{\alpha \in A}S_{\alpha} \cup T )\big) [P_j^{(g,\cP)} \otimes P_i^{(g,S_j)} ](dA,dT).
\]
\end{lemma}

The above lemma states that, given a (not necessarily cooperative) game $v$, the coalitional value $g_i[N,v,\cP]$ can be viewed as an expected value with respect to a product of two probability measures. For the marginal game the above result must be adjusted to include dependence on predictors. Specifically, we can write the marginal coalitional value $g_i[N,\vpdp(\cdot;x^*,X,f),\cP]$ as an expected value with respect to a product of three probability measures as shown below.

\begin{definition}
Let $f\in [f] \in L^p(\tilde{P}_X)$, $U^{(f)}_p \in \B(\R^n)$ be a set of $P_X$-probability $1$ as in Lemma \ref{lmm::game_meas_marg}, and $\cP=\{S_1,\dots,S_m\}$ the partition of $N$. Let $x^* \in U_p^{(f)}$. For each $S_j \in \cP$ we set
\begin{equation}\label{param_marg_norm_coal}
\nu_{x^*}^{(p)}(f,\cP,S_j) := \sum_{A \subseteq M \setminus \{j\}, Q_A=\cup_{\alpha \in A} S_{\alpha}} \sum_{T \subseteq S_j } \int |f(x^*_{Q_A \cup T},x_{-(Q_A \cup T)})|^p P_{X_{-(Q_A \cup T)}}(dx_{-(Q_A \cup T)}) < \infty.
\end{equation}
\end{definition}

\begin{theorem}\label{thm::coalvalue_as_expectation}
Let $f \in [f] \in L^p(\tilde{P}_X)$. Let $\cP$ be a partition of $N$, $g[N,v,\cP]$ a coalitional value of the form \eqref{coalvalform}, and suppose that the coefficients $w^{(1)}_j(A,M)$ and $w^{(2)}_i(T,S_j)$ of $g_i$, $i\in N$ and $j\in M$, satisfy \eqref{hyp::coeff_nonneg}-\eqref{hyp::coeff_sumtoone}. 

\begin{itemize}
   \item [$(i)$] For $P_X$-almost sure $x^* \in \R^n$, for each $i \in S_j$, $S_j \in \cP$,  the map
\begin{equation}\label{Delta_coal_def}
\begin{aligned}
(A,T,x) \mapsto & \Delta_i(A,T,x;x^*,f)\\
&:=f(x^*_{(\cup_{\alpha\in A}S_{\alpha})\cup T \cup \{i\}},x_{-(\cup_{\alpha\in A}S_{\alpha})\cup T \cup \{i\}})-f(x^*_{(\cup_{\alpha\in A} S_{\alpha})\cup T},x_{-(\cup_{ \alpha \in A} S_{\alpha})\cup T}),
\end{aligned}
\end{equation}
with $A \subseteq M \setminus \{i\}$ and $T \subseteq S_j \setminus \{i\}$, is $P_j^{(g,\cP)} \otimes P_i^{(g,S_j)} \otimes  P_X $-measurable and $p$-power integrable satisfying the bound
\begin{equation}\label{Delta_bound_coal}
\begin{aligned}
& \int |\Delta_i(A,T,x;x^*,f)|^p  \, [  P_j^{(g,\cP)} \otimes P_i^{(g,S_j)} \otimes P_X](dA,dT,dx) \\
& \quad \leq  2^p \cdot \nu_{x^*}^{(p)}(f,\cP,S_j)  \max_{A \subset M \setminus \{j\}, T \subseteq S_j \setminus\{i\}} \big(w_j^{(1)}(A,M) w_i^{(2)}(T,S_j)\big).
\end{aligned}
\end{equation}

\item [$(ii)$] For $P_X$-almost sure $x^*\in \R^n$ the marginal coalitional value $g_i[N,\vpdp(\cdot; x^*,X,f),\cP]$ is well-defined for each $i \in S_j$, $S_j \in \cP$, and can be written as an expectation with respect to the product probability measure $P_j^{(g,\cP)} \otimes P_i^{(g,S_j)} \otimes P_X$. Specifically,
\begin{equation}\label{eq::g_as_expectation}
    g_i[N,\vpdp(\cdot; x^*,X,f),\cP]=\int \Delta_j(A,T,x;x^*,f)\,[P_j^{(g,\cP)} \otimes P_i^{(g,S_j)} \otimes P_X](dA,dT,dx)
\end{equation}

Consequently, taking random samples $(\mathcal{A}^{(k)}, \mathcal{T}^{(k)},X^{(k)})\sim P_j^{(g,\cP)}\otimes P_i^{(g,S_j)}\otimes P_X$, $k \in \{1,2,\dots,K\}$, we have
\begin{equation}\label{eq::g_approx}
    \frac{1}{K}\sum_{k=1}^K \Delta_i(\mathcal{A}^{(k)},\mathcal{T}^{(k)},X^{(k)};x^*,f) = g_i[N,\vpdp(\cdot; x^*,X,f),\cP] + \mathcal{E}_K^{g_i},
\end{equation}
where $\mathcal{E}^{g_i}_K \to 0$ in probability as $K\to \infty$. In addition, if $f \in [f] \in L^2(\tilde{P}_X)$, then $\mathcal{E}_K^{g_i} \to 0$ in $L^2(\P)$ with rate $O(K^{-1/2})$.
\end{itemize}
\end{theorem}

\begin{proof}
The proof is similar to that of Theorem \ref{thm::gamevalue_as_expectation}. The measurability of $(A,T,x) \mapsto \Delta_j(A,T,x;x^*,f)$ for $P_X$-almost sure $x^*$ follows from Lemma \ref{lmm::game_meas_marg} while the bound \eqref{Delta_bound_coal} follows from the definition \eqref{coalvalform} of the coalitional value $g$, the definition of the product measure $P_j^{(g,\cP)} \otimes P_i^{(g,S_j)} \otimes P_X$, and that of $\nu_{x^*}^{(p)}(f,\cP,S_j)$. 

The representation \eqref{eq::h^P_as_expectation} of the marginal coalitional value follows from the definition  \eqref{coalvalform} of  $g$ and that of $P_j^{(g,\cP)} \otimes P_i^{(g,S_j)} \otimes P_X$, while the equation \eqref{eq::g_approx} is a consequence of the weak law of large numbers. Finally, if $f \in [f] \in L^2(\tilde{P}_X)$, then for $P_X$-almost sure $x^*$ the error term $\EE^{g_i}_K$ is bounded in $L^2(\PP)$ by $[K^{-1} \cdot Var(\Delta_j(\mathcal{A}^{(1)},\mathcal{T}^{(1)},X^{(1)};x^*,f))]^{\frac{1}{2}}$ with the variance of $\Delta_j(\mathcal{A}^{(1)},\mathcal{T}^{(1)},X^{(1)};x^*,f)$ bounded by the last term in \eqref{Delta_bound_coal} for $p=2$.
\end{proof}

Equation \eqref{eq::g_approx} instructs us on how to perform the MC sampling for a coalitional value. Due to the two-step property, Algorithm \ref{algo_coalval} below is simply an augmentation of Algorithm \ref{algo_quotgamevalue}.

\begin{algorithm}
    \SetAlgoLined 
    \KwIn{Observation $x^*\in \R^n$, model $f$, partition $\cP=\{S_1,\dots,S_m\}$, dataset $\bar{D}_X=\{x^{(k)}\}_{k=1}^K$.
    }
    \KwOut{MC estimate $\hat{g}$ of $g[N,\vpdp,\cP]$.
    }
    \BlankLine
    \For{$j$ in $\{1,\dots,m\}$}
    {
        \For{$i$ in $S_j$}
        {
            $\hat{g}_i := 0$;\\
            \For{$k$ in $\{1,\dots,K=|\bar{D}_X|\}$}
            {
                Select the observation $x^{(k)}$ from $\bar{D}_X$;\\
                Draw a random coalition $A^{(k)}$ from the distribution $P_j^{(g,\cP)}$;\\
                Draw a random coalition $T^{(k)}$ from the distribution $P_i^{(S_j)}$;\\
                $\hat{g}_i := \hat{g}_i + \Delta_i(A^{(k)},T^{(k)},x^{(k)};x^*,f)$;
            }
            $\hat{g}_i := \hat{g}_i/K$;
        }
    }
    \Return{$\hat{g} = (\hat{g}_1,\dots,\hat{g}_n)$;}
\caption{Monte Carlo sampling for coalitional values}
\label{algo_coalval}
\end{algorithm}

\begin{remark}\rm\label{rm::emp_marg_coalval}
To estimate $g[N,\hat{v}^{\ME}(\cdot;x^*,\bar{D}_X,f),\cP]$ one needs to adjust Algorithm \ref{algo_coalval} as follows. The input to the algorithm must contain the number $\tilde{K}$ of MC iterations, specified independently of $|\bar{D}_X|$. Line 4 must contain the for-loop over $k \in \{1,\dots,\tilde{K}\}$ and line 5 must be replaced with ``Draw a random observation $x^{(k)}$ from the empirical distribution $P_{\bar{D}_X}$''. 

The value produced by the adjusted Algorithm \ref{algo_coalval} is the estimate of $g[N,\hat{v}^{\ME}(\cdot;x^*,\bar{D}_X,f),\cP]$ with a mean squared error of $O(\tilde{K}^{-1})$. Finally, as in Lemma \ref{lmm::error_emp_marg_estimator}, it can be shown that the difference between the empirical marginal coalitional  value and the marginal coalitional value in $L^2(\PP)$ is bounded by $O(|\bar{D}_X|^{-1/2})$.
\end{remark}

\subsection{Sampling for two-step Shapley values}

Continuing the discussion on coalitional values, there are several of them that are not of the form \eqref{coalvalform}. One in particular that is of note is called two-step Shapley, defined in \cite{Kamijo2009} and given by

\begin{equation}\label{eq::twostepshap}
    TSh_i[N,v,\cP] = \varphi_i[S_j,v]+\frac{1}{|S_j|}\big(\varphi_j[M,v^{\cP}]-v(S_j)\big), \quad i\in S_j.
\end{equation}
Observe that two-step Shapley consists of three terms: the Shapley value of the player $i$ when the game $v$ is restricted to the group $S_j$, the quotient game Shapley value of the group $S_j$, and the game itself evaluated at $S_j$. Furthermore, if $|S_j|=1$, \eqref{eq::twostepshap} simply reduces to $\varphi_j[M,v^{\cP}]$.

Although Algorithm \ref{algo_coalval} cannot be directly applied to approximate $TSh_i[N,\vpdp,\cP]$, each individual term of \eqref{eq::twostepshap} can be evaluated via sampling based on Algorithm \ref{algo_lingamevalue} and Algorithm \ref{algo_quotgamevalue}. Given an observation $x^*\in \R^n$, a model $f$, and a background dataset $\bar{D}_X$, we have

\begin{itemize}
    \item[(i)] The value $\varphi_j[M,v^{\ME,\cP}(\cdot; x^*,X,f)]$ can be approximated via Algorithm \ref{algo_quot_game_marg}, where
    \[
        P_j^{(\varphi,\cP)}(A):=\frac{|A|!(m-|A|-1)!}{m!}, \quad A\subseteq M\setminus \{j\}.
    \]
    \item[(ii)] $\vpdp(S_j;x^*,X,f)$ can be approximated by averaging the values of $f(x_{S_j}^*,\cdot)$ over $\bar{D}_X$.
    \item[(iii)] The value $\varphi_i[S_j,\vpdp(\cdot; x^*,X,f)]$ can be approximated using Algorithm \ref{algo_lingamevalue} by adjusting it to accommodate the fact that the game is restricted to $S_j$. Specifically,  Line 5 has to provide the sampling of coalitions from $S_j \setminus \{i\}$ using the probability distribution
        \[
        P_i^{(\varphi,S_j)}(S) := \frac{|S|!(|S_j|-|S|-1)!}{|S_j|!}, \quad S\subseteq S_j\setminus \{i\}.
    \]
\end{itemize}

Combining the above steps leads to Algorithm \ref{algo_twostep} for $TSh_i[N,\vpdp,\cP]$.

\begin{algorithm}
    \SetAlgoLined 
    \KwIn{Observation $x^*\in \R^n$, model $f$, partition $\cP=\{S_1,\dots,S_m\}$, dataset $\bar{D}_X=\{x^{(k)}\}_{k=1}^K$.
    }
    \KwOut{MC estimate $\hat{TSh}$ of $TSh[N,\vpdp,\cP]$.
    }
    \BlankLine
    \For{$j$ in $\{1,\dots,m\}$}
    {
        \For{$i$ in $S_j$}
        {
            \If {$|S_j|==1$}{
                Apply the loop from Algorithm \ref{algo_quot_game_marg} and obtain $\hat{TSh}_i$;
            }
            \Else{
                $\hat{TSh}_i := 0$;\\
                \For{$k$ in $\{1,\dots,K=|\bar{D}_X|\}$}
                {
                    Select the observation $x^{(k)}$ from $\bar{D}_X$;\\
                    $f_j^{(k)}:=f(x_{S_j}^*,x_{-S_j}^{(k)})$\\
                    Draw a random coalition $A^{(k)}$ from the distribution $P_j^{(\varphi,\cP)}$;\\

                    $\Delta_j(A^{(k)},x^{(k)};x^*,f):=f(x_{\cup_{\alpha\in A^{(k)}\cup \{j\}}S_{\alpha}}^*,x_{-\cup_{\alpha\in A^{(k)}\cup \{j\}}S_{\alpha}}^{(k)})-f(x_{\cup_{\alpha\in A^{(k)}}S_{\alpha}}^*,x_{-\cup_{\alpha\in A^{(k)}}S_{\alpha}}^{(k)})$;\\
                    Draw a random coalition $S^{(k)}$ from the distribution $P_i^{(\varphi,S_j)}$;\\
                    
                    $\Delta_i(S^{(k)},x^{(k)};x^*,f):=f(x_{S^{(k)}\cup \{i\}}^*,x_{-S^{(k)}\cup \{i\}}^{(k)})-f(x_{S^{(k)}}^*,x_{-S^{(k)}})$;\\
                    \BlankLine
                    $\hat{TSh}_i := \hat{TSh}_i + \Delta_i(S^{(k)},x^{(k)};x^*,f)+\frac{1}{|S_j|}(\Delta_j(A^{(k)},x^{(k)};x^*,f)-f_j^{(k)})$;
                }
                $\hat{TSh}_i := \hat{TSh}_i/K$;
            }
        }
    }
    \Return{$\hat{TSh} = (\hat{TSh}_1,\dots,\hat{TSh}_n)$;}
\caption{Monte Carlo sampling for two-step Shapley values}
\label{algo_twostep}
\end{algorithm}

\begin{remark}\rm\label{rm::emp_marg_twostep}
To estimate $TSh[N,\hat{v}^{\ME}(\cdot;x^*,\bar{D}_X,f),\cP]$, one needs to adjust Algorithm \ref{algo_twostep} as follows. The input to the algorithm must contain the number $\tilde{K}$ of MC iterations, specified independently of $|\bar{D}_X|$. Line 8 must contain the for-loop over $k \in \{1,\dots,\tilde{K}\}$ and line 9 must be replaced with ``Draw a random observation $x^{(k)}$ from the empirical distribution $P_{\bar{D}_X}$''. 

The value produced by the adjusted Algorithm \ref{algo_twostep} is the estimate of $TSh[N,\hat{v}^{\ME}(\cdot;x^*,\bar{D}_X,f),\cP]$ with a mean squared error of $O(\tilde{K}^{-1})$. Finally, as in Lemma \ref{lmm::error_emp_marg_estimator}, it can be shown that the difference between the empirical marginal two-step Shapley value and the marginal two-step Shapley value in $L^2(\PP)$ is bounded by $O(|\bar{D}_X|^{-1/2})$.
\end{remark}