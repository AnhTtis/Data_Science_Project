\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
% \usepackage{neurips_2022}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{multicol,multirow}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
% \usepackage{subcaption} 


\usepackage{epsfig}
\usepackage{graphics}
\usepackage{subfigure}

\title{MoGDE: Boosting Mobile Monocular 3D Object Detection with Ground Depth Estimation}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


% \author{%
%   Yunsong Zhou \\
%   Shanghai Jiao Tong University \\
%   \texttt{zhouyunsong@sjtu.edu.cn} \\
%   % examples of more authors
%   \And
%   Quan liu \\
%   Shanghai Jiao Tong University \\
%   \texttt{liuquan2017@sjtu.edu.cn} \\
%   \And
%   Hongzi Zhu \thanks{Corresponding author.} \\
%   Shanghai Jiao Tong University \\
%   \texttt{hongzi@sjtu.edu.cn} \\
%   \And
%   Yunzhe Li \\
%   Shanghai Jiao Tong University \\
%   \texttt{yunzhe.li@sjtu.edu.cn} \\
%   \And
%   Shan Chang \\
%   Donghua University \\
%   \texttt{changshan@dhu.edu.cn} \\
%   \And
%   Minyi Guo \\
%   Shanghai Jiao Tong University \\
%   \texttt{guo-my@sjtu.edu.cn} \\
% }

\author{
Yunsong Zhou \textsuperscript{\rm 1}~~ 
Quan Liu \textsuperscript{\rm 1}~~
Hongzi Zhu \textsuperscript{\rm 1}\thanks{Corresponding author.} ~~ 
%\footnotemark[1]~~
Yunzhe Li \textsuperscript{\rm 1}~~
Shan Chang \textsuperscript{\rm 2}~~ 
%\thanks{Corresponding authors}
%\textsuperscript{\rm \thanks{Corresponding authors}}
Minyi Guo \textsuperscript{\rm 1}~~  \\
\textsuperscript{\rm 1}Shanghai Jiao Tong University~~
\textsuperscript{\rm 2}Donghua University
\\
\text{\{zhouyunsong,liuquan2017,hongzi,yunzhe.li,guo-my\}@sjtu.edu.cn}~~
\text{changshan@dhu.edu.cn}
}


\begin{document}


\maketitle


\begin{abstract}

Monocular 3D object detection (Mono3D) in mobile settings (\emph{e.g.}, on a vehicle, a drone, or a robot) is an important yet challenging task. Due to the \emph{near-far disparity} phenomenon of monocular vision and the ever-changing camera pose, it is hard to acquire high detection accuracy, especially for far objects. 
Inspired by the insight that the depth of an object can be well determined according to the depth of the ground where it stands, in this paper, we propose a novel Mono3D framework, called \emph{MoGDE}, which constantly estimates the corresponding ground depth of an image and then utilizes the estimated ground depth information to guide Mono3D.
To this end, we utilize a pose detection network to estimate the pose of the camera and then construct a feature map portraying pixel-level ground depth according to the 3D-to-2D perspective geometry.
Moreover, to improve Mono3D with the estimated ground depth, we design an RGB-D feature fusion network based on the transformer structure, where the long-range self-attention mechanism is utilized to effectively identify ground-contacting points and pin the corresponding ground depth to the image feature map. 
We conduct extensive experiments on the real-world KITTI dataset. The results demonstrate that MoGDE can effectively improve the Mono3D accuracy and robustness for both near and far objects. 
MoGDE yields the best performance compared with the state-of-the-art methods by a large margin and is ranked number one on the KITTI 3D benchmark.

  
\end{abstract}

\section{Introduction}

Building on the promising progress achieved in 2D object detection in recent years \cite{ren2015faster, redmon2018yolov3}, 3D object detection, particularly on moving agents, has received increasing attention from both industry and academia as an important component in many applications, ranging from autonomous vehicles \cite{Geiger2012CVPR} and drones, to robotic manipulation and augmented reality applications. 
%Most existing 3D detectors need to rely on accurate depth-of-field information.
%LiDAR-based methods \cite{chen_multiview_2016, qi_frustum_2017, shi_pointrcnn_2018, shin_roarnet_2018, liang_deep_2018, zhou_voxelnet_2017} and stereo-vision-based methods \cite{chen_3d_2015, chen_3d_2018, li_stereo_2019, pham_robust_2017, qin_triangulation_2019, xu_multilevel_2018} that respectively obtain depth and distance information from LiDAR measurements and triangulation, can achieve superior performance.
Compared to LiDAR-based \cite{chen_multiview_2016, qi_frustum_2017, shi_pointrcnn_2018, shin_roarnet_2018, liang_deep_2018, zhou_voxelnet_2017} and stereo-based \cite{chen_3d_2015, chen_3d_2018, li_stereo_2019, pham_robust_2017, qin_triangulation_2019, xu_multilevel_2018} methods, a much cheaper, more energy-efficient, and easier-to-deploy alternative, \textit{i.e.}, monocular 3D object detection (Mono3D), remains an open and challenging research field.
A practical Mono3D detector for moving agents should meet the following two requirements: 
1) the 3D bounding box produced by the Mono3D detector should be accurate enough, not only for near objects but also for very distant objects, to secure, for instance, high-priority driving safety applications;
2) the Mono3D detector should remain robust in mobile scenarios, where the camera pose inevitably changes along with the movement of a mobile agent.


% In the literature, a number of Mono3D methods have achieved considerable high accuracy, which are based on a specifically idealized and fixed camera coordinate system.
% In the literature, a number of Mono3D methods have achieved considerable high accuracy for near objects, but their detection accuracy for distant objects is still unsatisfactory.
% Due to the lack of depth cues, the monocular scenario depth estimation of these Mono3D approaches is naturally \textbf{ill-posed} when the objects are too far away.
In the literature, recent Mono3D methods with complex network structures \cite{zhou2021monocular,mousavian20173d,weng2019monocular,qin_monogrnet_2018, vianney2019refinedmpl} have achieved considerably high accuracy for near objects, but the predicted 3D bounding boxes for far objects are often \emph{ill-posed} due to the lack of depth cues. This huge disparity between near and far objects lies in the nature of monocular vision. Specifically, as depicted in Figure \ref{fig:intro} (a), equal distances of different depth from the camera (\emph{e.g.}, $\Delta z_1 = \Delta z_2$) have a distinct number of pixels in the image (\emph{e.g.}, $\Delta u_1 > \Delta u_2$ and $\Delta v_1 > \Delta v_2$), which makes the pixel rounding errors have a non-negligible impact for detecting far objects.
Furthermore, as illustrated in Figure \ref{fig:intro}(b), the camera pose variance can eventually result in a large offset both in form of 3D boxes and in the bird's eye view \cite{dijk2019neural}. To the best of our knowledge, existing Mono3D methods such as geometric constraint based \cite{chen_monocular_2016,murthy2017reconstructing,chabot2017deep,xiang_subcategory-aware_2017}, pseudo-LiDAR based  \cite{ma_accurate_2019, manhardt_roi_10d_2018, wang_pseudo-lidar_2018, xu_multilevel_2018,qin_monogrnet_2018, ding2020learning, ma2020rethinking}, and pure image based \cite{ren2015faster,tian2019fcos,zhou_objects_2019,brazil_m3d_rpn_2019, liu2020smoke, wang2021fcos3d,chen2020monopair,li2020rtm3d,brazil2020kinematic,zhou2021monocular,zhang2021objects,lu2021geometry,zhang2022monodetr}, have not taken into account the issue of inevitable camera pose changes in mobile scenarios.

%Due to monocular imaging in Figure \ref{fig:intro} (a), for objects far from the camera, even though its depth changes dramatically, it still has only an imperceptible change of a few pixel values in the image.
%Because the depth in the scene is continuously distributed while the position of image pixels is discrete, the error from pixel rounding is greater for distant objects. As described in more detail in Appendix \ref{Imaging Regularity}, this can make depth detection of distant objects very difficult, if not aided by additional supporting knowledge.
% 
% Besides, as shown in Figure \ref{fig:intro} (b), the current datasets and models assume that the Mono3D model works statically at an ideal fixed viewport $i$ which is always aligned with the horizon (top row). 
% They simply assume that the pose of the camera does not change during travel for the convenience of data processing.
% However, when these methods are used in the non-ideal case shown in the bottom row, the object depth seen by the camera deviates from the real object depth, which will eventually result in a large offset in form of both 3D boxes and bird-eye’s view.
% 
% When these methods are used in the dynamic scene shown in the bottom row, the object depth seen by the camera deviates from the real object depth, which will eventually result in a large offset in form of both 3D boxes and bird-eye’s view.
% 
% A fatal reason is that they tend to ignore a problem that often occurs in real scenes but rarely gets attention: changes in camera pose lead to changes in the depth distribution of the monocular scenario.
% 
% According to the findings in \cite{dijk2019neural}, the estimation of object depth by main stream models is heavily dependent on the longitudinal position of the object on the 2D image, so when camera pose changes make the 2D position of the object change, which is common in driving scenes, the depth estimation is then severely biased.
% Therefore, even with the application of some complex network structures, Mono3D detectors do not achieve satisfactory performance.
% Great efforts have been made with the application of some complex network structures \cite{zhou2021monocular}.
% Some methods aim to explore 2D-3D geometric consistency using geometric constraints \cite{mousavian20173d} or semantic knowledge \cite{chen2016monocular} to recover 3D positions and dimensions.
% Pseudo LiDAR-based methods \cite{wang2019pseudo, weng2019monocular,qin_monogrnet_2018, vianney2019refinedmpl} use depth estimation to reconstruct point clouds from image pixels.
% Depth-assisted methods \cite{ding2020learning,shi2020distance} focus on the integration strategy of RGB and depth features, while the network is unable to resolve inferior faraway 3D localization due to unreliably estimated depth maps.
% % In short, these methods are limited to a fixed camera pose and are difficult to guarantee robustness when the camera pose changes.
% In short, existing Mono3D detectors do not achieve satisfactory performance under this case.
% these two cases, which introduce serious challenges to the accuracy of the mobile Mono3D algorithm.
%Great efforts have been made with the application of some complex network structures , but existing Mono3D detectors do not achieve satisfactory performance under this case.



% The camera's pose, or line of sight to put it in another name, actually provides a flexible oracle for the depth of the scene supporting the detection of distant objects.
In this paper, we propose a novel Mono3D method, called \emph{MoGDE}, which fixates on improving detection accuracy and robustness in mobile settings. We have one key insight that \emph{the depth of an object in 3D space can be well determined according to the depth of the ground where it stands}. Given the pinhole model and the pose of a camera, the depth of each pixel corresponding to the ground can be accurately derived. Based on this insight, the core idea of MoGDE is first to constantly estimate the ground depth while moving and then to utilize the estimated ground depth information to guide a Mono3D detector.
%The line of sight of camera actually provides a flexible oracle for the depth of the scene supporting the detection of distant objects.
% We have such an insight: humans can get some hints about depth information from monocular even when the road is rocky.
% We have such an insight: humans can get some hints about depth information from monocular perceiving the dynamic changes of the scene adaptively.
%We have such an insight: the human sight is an excellent tool for distance measuring, they can get hints about far depth information even with a glance.
% because the brain has the capability to adaptively perceiving the dynamic changes of the road conditions.
% This is because a depth oracle is generated based on the current viewport, and the human brain automatically pins the object to the prior depth based on the image information in the scene.
%In more layman's terms, a depth oracle about the whole scene is generated, which does not depend on the vision but only on the view line.
%This supporting knowledge allows one to have a priori expectation of depth for objects both near and far, and the human brain automatically pins the objects to the prior depth based on the image information in the scene.
%Formulating a network to encode such learning patterns is feasible on a biological basis.
%Based on this insight, in this paper, we propose to formulate a dynamically generated depth oracle information to guide the Mono3D detector, working on improving its detection accuracy for distant objects.
% A pose based dynamically generated depth prior is used to guide the Mono3D model, allowing it to detect distant targets more accurately.
%We abbreviate the proposed framework as MoGDE (Monocular Depth Oracle).
% The proposed MoGDE is not limited to a fixed camera viewport, but can be applied to real scenes with changing camera pose cases, which can ensure both accuracy and robustness.
%
% Note that this idea differentiates from those that \textit{short-sighted} and focus solely on detecting objects in the \textit{static} camera line of sight \cite{simonelli_disentangling_2019,brazil_m3d_rpn_2019,liu2020smoke,zhou_objects_2019,chen2020monopair,tang2020center3d,brazil2020kinematic}.
% The proposed far-sighted MoGDE can be applied to real mobile scenes, which can ensure both accuracy and robustness.


\begin{figure}
    \centering
% \hspace{-0.8cm}
\includegraphics[width=1\textwidth]{figures/intro1.png}
\vspace{-0.3cm}
\caption{(a) Equal distances of different depths from the camera (\emph{e.g.}, $\Delta z_1 = \Delta z_2$) have a distinct number of pixels in the image (\emph{e.g.}, $\Delta u_1 > \Delta u_2$ and $\Delta v_1 > \Delta v_2$), which is referred to as the near-far disparity phenomenon of monocular vision, making the detection of far object susceptible to pixel rounding errors.
(b) The camera pose variance, caused by the movement of a mobile agent, can eventually result in a large offset both in form of 3D boxes and in the bird's eye view.
(c) Each color block represents its attention value with the centroid of the vehicle. The attention mechanism of the transformer network can be well leveraged for this long-range relationship modeling. }
\label{fig:intro}
\vspace{-0.5cm}
\end{figure}


There are two main challenges in designing MoGDE.
First, how to detect varying camera pose (\emph{e.g.}, the pitch and roll angles) from an image, which is dynamically changing in mobile scenes, and how to ultimately obtain accurate ground depth information are non-trivial. It is clear that different camera poses correspond to distinct ground depth estimates.
% First, how to detect varying camera pose (\emph{e.g.}, the pitch and roll angles) from an image, which is dynamically changing in mobile scenes, and how to ultimately obtain accurate ground depth information are non-trivial. It is clear that different camera poses correspond to distinct ground depth estimates (see Appendix \ref{Camera Pose Variance} for detailed theoretical analysis).
% Nowadays, most of the existing work only considers 3D object detection in static scenes rather than dynamic scenes where camera pose will inevitably have variations.
%
% the current datasets and models assume that the Mono3D model works statically at an ideal fixed viewport $i$ which is always aligned with the horizon (top row). 
% They simply assume that the pose of the camera does not change during travel for the convenience of data processing.
% However, when these methods are used in the non-ideal case shown in the bottom row, the object depth seen by the camera deviates from the real object depth, which will eventually result in a large offset in form of both 3D boxes and bird-eye’s view.
% When these methods are used in the dynamic scene shown in the bottom row, the object depth seen by the camera deviates from the real object depth, which will eventually result in a large offset in form of both 3D boxes and bird-eye’s view.
% A fatal reason is that they tend to ignore a problem that often occurs in real scenes but rarely gets attention: changes in camera pose lead to changes in the depth distribution of the monocular scenario.
% According to the findings in \cite{dijk2019neural}, the estimation of object depth by mainstream models is heavily dependent on the longitudinal position of the object on the 2D image, so when camera pose changes make the 2D position of the object change, which is common in driving scenes, the depth estimation is then severely biased.
%
To tackle this challenge, we introduce a pose detection network to extract the vanishing point and horizon information in an image to estimate the instant camera pose corresponding to this image.
After the view direction of the camera is decided, we then construct a feature map portraying pixel-level depth clues.
% Then the camera pose needs to be converted into image features that can be fed into the neural networks.
Specifically, we envision a virtual 3D scene containing only the sky and the ground and project this virtual scene to an image where each pixel is associated with a depth uniquely derived according to the 3D-to-2D perspective geometry.
Therefore, MoGDE can obtain the dynamic ground depth information as prior knowledge for guiding Mono3D.

Second, how to incorporate the estimated ground depth into the image features to enhance the detection accuracy is challenging. Based on our aforementioned insight, it is essential for the Mono3D detector to identify those \emph{ground-contacting points} of an object on the image. For example in Figure \ref{fig:intro} (a), these blue dots denote the ground-contacting points of a vehicle. To this end, we design an RGB-D feature fusion network based on the transformer structure to tie the ground depth feature to the image feature.
Specifically, as illustrated in Figure \ref{fig:intro}(c), the feature fusion network captures the feature of pixels close to the centroid of an object and identifies those ground-contacting points using the attention mechanism. It then attaches depth values with weights to compute a new feature map containing object location information. As a result, accurate 3D detection results can be obtained via a conventional Mono3D detector using the fused feature map.

Experiments on KITTI dataset \cite{Geiger2012CVPR} demonstrate that our method outperforms the SOTA methods by a large margin. Such a framework can be applied to existing detectors and is practical for industrial applications.
The proposed MoGDE is ranked \emph{number one} on the KITTI 3D benchmark by submission. The whole suite of the code base will be released and the experimental results will be posted to the public leaderboard.
We highlight the main contributions made in this paper as follows: 
% 1) a novel Mono3D detector is introduced by capturing the camera pose information from monocular images and encoding it into the feature map as an oracle to insulate the detector from camera pose changes caused by environmental perturbations.
% 2) a transformer based feature fusion network is designed, which can dynamically pin depth oracle to the image feature map adaptively utilizing the attention between image pixels to form a new feature map guided by pose information.
1) A novel Mono3D detector in a mobile setting is introduced, leveraging the dynamically estimated ground depth as prior knowledge to improve the detection accuracy and robustness for both near and far objects.
2) A transformer-based feature fusion network is designed, which utilizes the long-range attention mechanism to effectively identify ground-contacting points and pin the corresponding ground depth to the image feature map.
3) Extensive experiments on the real-world KITTI dataset are conducted and the results demonstrate the efficacy of MoGDE. 



% Therefore, even with the application of some complex network structures \cite{}, Mono3D detectors do not achieve satisfactory performance.



\section{Related Work}

\textbf{Monocular 3D Object Detection.}
The monocular 3D object detection aims to predict 3D bounding boxes from a single given image.
%To overcome the lacking of 3D information and reconstruct the geometry and position of the object in world coordinates, 
Existing Mono3D methods can be roughly divided into the following three categories. 
\emph{1) Geometric constraint based methods:} Extra information of prior 3D vehicle shapes is widely used, such as vehicle computer aided design (CAD) models \cite{chen_monocular_2016,murthy2017reconstructing,chabot2017deep,xiang_subcategory-aware_2017,liu2021autoshape} or key points \cite{barabanau_monocular_2019}.
By this means, extra labeling cost is inevitably required. 
\emph{2) Depth assist methods:} A stand-alone depth map of the monocular image is predicted at the first stage.
Such prior knowledge can be derived in various ways, such as a depth map generated by LiDAR point cloud (or Pseudo-LiDAR) \cite{wang_pseudo-lidar_2018,ma_accurate_2019,chen2021monorun,reading2021categorical}, monocular depth predictors \cite{qin_monogrnet_2018, ma2020rethinking, ding2020learning}, or disparity map generated by stereo cameras \cite{xu_multilevel_2018}. 
However, such external data is not easily available in all scenarios.
In addition, the inference time increases significantly due to the prediction of these dense heatmaps.
\emph{3) Pure image-based methods:}
Without requiring extra side-channel information, such methods \cite{kundu20183d,he2019mono3d++,liu2019deep,simonelli2019disentangling} take only a single image  as input and adopt center-based pipelines following conventional 2D detectors \cite{ren2015faster,zhou_objects_2019,tian2019fcos}.
%
% Deep3DBox~\cite{Deep3DBox} introduces discretized representation with 2D-3D prospective constraints to predict accurate object poses.
M3D-RPN~\cite{brazil_m3d_rpn_2019} reformulates the monocular 3D detection problem as a standalone 3D region proposal network. 
With very few handcrafted modules, SMOKE~\cite{liu2020smoke} and FCOS3D~\cite{wang2021fcos3d} predict a 3D bounding box by combining a concise one-stage keypoint estimation with regressed 3D variables based on CenterNet~\cite{zhou_objects_2019} and FCOS~\cite{tian2019fcos}, respectively. 
%
% MonoDLE~\cite{monodle} and PGD~\cite{PGD} analyze depth errors on top and enhance their performance with customized designs. 
%
To further strengthen monocular detectors,  current SOTA methods have introduced more effective but complicated geometric priors.
MonoPair~\cite{chen2020monopair} improves the modeling of occluded objects by considering the relationship of paired samples and parses their spatial relations with uncertainty.
% RTM3D~\cite{li2020rtm3d} predicts 2D-3D keypoints of objects and optimizes the bounding box by nonlinear least squares.
Kinematic3D~\cite{brazil2020kinematic} proposes a novel method for monocular video-based 3D object detection, which uses kinematic motion to improve the accuracy of 3D localization.
MonoEF \cite{zhou2021monocular} first proposes a novel method to capture the camera pose in order to formulate detectors that are not subject to camera extrinsic perturbations.
MonoFlex~\cite{zhang2021objects} conducts an uncertainty-guided depth ensemble and categorizes different objects for distinctive processing. 
GUPNet~\cite{lu2021geometry} solves the error amplification problem by geometry-guided depth uncertainty and collocates a hierarchical learning strategy to reduce the training instability. 
MonoDETR \cite{zhang2022monodetr} introduces a simple monocular object detection framework that makes the vanilla transformer to be depth-aware and enforces the whole detection process guided by depth.
% The above geometrically dependent designs largely promote the performance of center-based methods, but the underlying problem still exists, namely, focusing only on the fixed camera coordinate system and ignoring the effect of camera pose on detection. 
% The above geometrically dependent designs largely promote the performance of center-based methods, but the underlying problem still exists, namely, focusing on generalized improvement of overall performance, the detection accuracy for distant objects is still not satisfactory.
The above geometrically dependent designs largely promote the overall performance of center-based methods, but the underlying problem still exists, namely, the detection accuracy for distant objects is still not satisfactory.

% Although it considers the variation of camera pose, it does not thoroughly illustrate the connection between camera pose and depth distribution, nor the connection between depth prediction and the ground contact point.
% Moreover, the traditional convolution-based transformation of the feature map still provides limited performance improvement.

% These methods do not work well when the camera’s pose receives a disturbance w.r.t. ground plane due to rough terrain or acceleration of ego vehicle.


% On the contrary, our proposed MonoDETR adopts a depth-guided paradigm via a depth-aware transformer and discards the traditional center detection step, which extracts features from depth-informative regions on the image and contains minimal 3D-specific inductive biases.




\textbf{Object Detection with Transformer.}
% \textbf{Monocular Models with Transformer.}
2D object detectors~\cite{zhou_objects_2019,tian2019fcos} have achieved excellent performance in recent years but are equipped with cumbersome post-processing, e.g. non-maximum suppression (NMS)~\cite{ren2015faster}. 
To circumvent it, the pioneering work DETR \cite{carion2020end} constructs a novel and simple framework by adapting the powerful transformer \cite{vaswani2017attention} to the field of vision detection.
DETR detects objects on images by encoding-decoding paradigm, which improves the detection performance by using the long-range attention mechanism.
% To accelerate the convergence, Deformable DETR~\cite{zhu2020deformable} designs the sparse deformable attention mechanism and achieves better performance. 
% Some recent works have tried to apply transformer to some tasks related to monocular scene reconstruction, depth prediction, object detection, etc.
% Besides, DETR is further enhanced by placing anchors~\cite{wang2021anchor}, redesigning as two stages~\cite{sun2021sparse,sun2021rethinking}, setting conditional attention~\cite{meng2021conditional}, embedding dense priors~\cite{yao2021efficient} and so on~\cite{dai2021up,misra2021end}. 
%
% In addition, DETR is further enhanced by placing anchors~\cite{wang2021anchor}, setting conditional attention~\cite{meng2021conditional}, embedding dense prior~\cite{yao2021efficient}, and so on~\cite{misra2021end}.
DETR is further enhanced by designing deformable attention~\cite{zhu2020deformable}, placing anchors~\cite{wang2021anchor}, setting conditional attention~\cite{meng2021conditional}, embedding dense prior~\cite{yao2021efficient}, and so on~\cite{misra2021end}.
%
Some recent works have tried to apply transformer to some other tasks related to monocular scene reconstruction, depth prediction, \textit{etc.}
Transformerfusion \cite{bozic2021transformerfusion} leverages the transformer architecture so that the network learns to focus on the most relevant image frames for each 3D location in the scene, supervised only by the scene reconstruction task.
% Handsformer\cite{hampali2021handsformer} proposes to estimate the 3D position of two hands in close interaction from a single color image by relying on CNNs to first localize the joints as 2D keypoints and on self-attention between CNN features of these keypoints to associate them with the corresponding hand joints.
%
% TransDepth \cite{yang2021transformers} is the first to apply transformer to a pixel-level depth prediction problem involving continuous labels.
% To avoid the network losing its ability to capture local-level details by employing transformer, it proposes a novel decoder that employs a gate-based attention mechanism.
MT-SfMLearner \cite{varma2022transformers} first demonstrates how to adapt vision transformers for self-supervised monocular depth estimation focusing on improving the robustness of natural corruptions. 
% It focuses on improving the robustness to natural corruptions and adversarial attacks, including when the camera intrinsic factors are unknown.
% MonoDETR \cite{zhang2022monodetr} introduces a simple monocular object detection framework that makes the vanilla transformer to be depth-aware and enforces the whole detection process guided by depth.
% Some other approaches \cite{thompson2021d, ranftl2021vision, huynh2022lightweight}, including the ones mentioned above, mostly copy the structure of the transformer rigidly and use it just for detection or depth estimation.
% They do not consider fusing information from both branches (e.g., image and depth) fully using the encoder-decoder structure, let alone the overlooked camera pose induced depth distribution problem.
%
While these methods have made a demonstration of how to apply a transformer to a monocular camera model \cite{cheng2021swin, yang2021transformer,yang2021transformers}, they all rely on other branches (either the environment reconstruction or the depth map), which will not be available in a typical Mono3D task based on RGB images.

% Consequently, the lack of motion information in the time series prevents us from directly obtaining accurate camera pose information. 
% However, We can still use similar ideas to detect changes in the ground plane and vanishing point from the image compared to the reference frame, and thus indirectly infer changes in the camera extrinsic parameters.
%
% DETR detects objects on the image by an encoding-decoding paradigm and outputs set prediction via Hungary Matching Algorithm~\cite{carion2020end}. 
% However, due to the quadratic computational complexity of attention, DETR requires 500 epochs to be fully trained, which normally takes expensive 10 days on 8 V100 GPU. 
%
% ACT~\cite{zheng2020end} boosts the efficiency of DETR's encoder for both time and memory by introducing adaptive clustering algorithms during inference. 
% SMCA~\cite{Gao_2021_ICCV} proposes the Gaussian-modulated co-attention mechanism, which refocuses the attention of each query into object-centric areas and serves as a drop-in module to the original DETR. 
% Besides, DETR is further enhanced by placing anchors~\cite{wang2021anchor}, redesigning as two stages~\cite{sun2021sparse,sun2021rethinking}, setting conditional attention~\cite{meng2021conditional}, embedding dense priors~\cite{yao2021efficient} and so on~\cite{dai2021up,misra2021end}. 
%
%
Our MoGDE inherits DETR's superiority for non-local encoding and long-range attention. 
Specifically, we endow the transformer to be \textit{ground-aware} by pinning ground depth to image features leveraging the encoder-decoder architecture to improve the detection accuracy for far objects.

% The Monocular 3D Detection module and coordinate alignment unit follow standard procedures [24].

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{figures/overview1.pdf}
    \caption{MoGDE consists of three main components, \emph{i.e.}, \emph{ground depth estimation} (GDE), \emph{ground depth fusion} (GDF) and \emph{monocular 3D detection} (M3D). In GDE, the pose network predicts the ground plane as well as the vanishing point. The derived pose information is then used to construct a virtual scene and obtain a pose-specific ground depth feature map. In GDF, a transformer network is leveraged to fuse the image features with the ground depth feature map, resulting a ground-aware fused feature map. M3D employs a standard Mono3D detector as the underlying detection core.}
\label{fig:overview}
\vspace{-0.5cm}
\end{figure}

\section{Design of MoGDE}
\subsection{Overview}

The core idea of MoGDE is to utilize dynamically estimated ground depth information to improve Mono3D so that two goals can be achieved: 1) superior ground-aware image features are obtained to increase Mono3D accuracy for both near and far objects; 2) the impact of camera pose variation is diminished to enhance Mono3D robustness in mobile settings.
Figure \ref{fig:overview} depicts the architecture of our framework. Specifically, MoGDE first adopts the DLA-34 \cite{yu2018deep} as its backbone, which takes a monocular image of size ($W\times H\times 3$) as input and outputs a feature map of size ($W_s\times H_s\times C$) after down-sampling with an $s$-factor. Then, the feature map is fed into three components as follows:

\textbf{Ground Depth Estimation (GDE).} 
GDF mainly integrates two functions, \emph{i.e.}, \emph{camera pose detection} (CPD), and \emph{virtual scene construction} (VSC). Specifically, CPD estimates the camera pose (\emph{i.e.}, the pitch and roll angles) based on the predicted vanishing point and ground plane extracted by a pose detection network. VSC establishes a 2D ground depth feature map based on a pose-specific virtual 3D scene containing only the sky and the ground. 

\textbf{Ground Depth Fusion (GDF).}
GDF leverages the attention mechanism of a transformer network to fuse the image features with the ground depth feature map, resulting a superior ground-aware fused feature map. 

\textbf{Monocular 3D Detection (M3D).}
MoGDE employs GUPNet~\cite{lu2021geometry}, a SOTA CenterNet \cite{zhou_objects_2019} based SOTA monocular 3D object detector as its underlying detection core.
% For 2D and 3D detection, we follow standard procedures in this domain (see detailed preliminary on Mono3D in Appendix \ref{Preliminary on Monocular Object Detection}).


\subsection{Ground Depth Estimation}
\label{Virtual Scene Based Oracle Generation}

\subsubsection{Camera Pose Detection}
\label{Camera Pose Detection}


In order to generate a ground depth estimate, it is key to detect the camera pose given an image feature map. We have the following proposition:

\textbf{Proposition 1:} \textit{Given a benchmark camera coordinate system $\mathbf{P}^0$, which is aligned with the ground plane coordinate systems, and the current camera coordinate system $\mathbf{P}^i$, which is not aligned with $\mathbf{P}^0$ due to camera movement, there exists a transformation matrix $\mathbf{A}$ between $\mathbf{P}^i$ and $\mathbf{P}^0$ that can be uniquely determined by pitch $\theta_p$ and roll $\theta_p$ angle changes of the camera.})
\label{proposition1}
% \textbf{Proposition 1:} \textit{Given a benchmark camera coordinate system $\mathbf{P}^0$, which is aligned with the ground plane coordinate systems, and the current camera coordinate system $\mathbf{P}^i$, which is not aligned with $\mathbf{P}^0$ due to camera movement, there exists a transformation matrix $\mathbf{A}$ between $\mathbf{P}^i$ and $\mathbf{P}^0$ that can be uniquely determined by pitch $\theta_p$ and roll $\theta_p$ angle changes of the camera.} (see detailed analysis in Appendix \ref{Camera Pose Variance}.)

Therefore, we introduce the subsequent neural network to learn the pitch $\theta_p$ and roll $\theta_p$ angle changes of the camera when the camera coordinate system changes from $\mathbf{P}^0$ to $\mathbf{P}^i$.
Specifically, in addition to the regular regression tasks in CenterNet \cite{zhou_objects_2019} based network, we introduce a regression branch for pose detection following MonoEF \cite{zhou2021monocular}.
Since the camera pose is a feature that is implicit for images, we chose two physical quantities with a clear meaning for detection: the ground plane (associated with roll angle) and the vanishing point (associated with pitch angle).
Following the state-of-the-art odometer framework in DeepVP \cite{chang2018deepvp}, we represent a regression task with L1 loss as:
\begin{equation}
    \begin{aligned}
{\left[\hat{\mathbf{y}}_{\mathrm{gp}}, \hat{\mathbf{y}}_{\mathrm{vp}}\right] } &=f^{\mathrm{pose}}\left(\mathbf{H}\right), \\
\mathcal{L}_{\mathrm{pose}} &=\left\|\mathbf{A}-\mathbf{g}\left(\hat{\mathbf{y}}_{\mathrm{gp}}, \hat{\mathbf{y}}_{\mathrm{vp}}\right)\right\|,
\end{aligned}
\end{equation}
where $\mathbf{H}$ is the input image feature; $f^{pose}$ is the CNN architecture used for horizon and vanishing point detection in the work \cite{krizhevsky2012imagenet}; $\hat{\mathbf{y}}_{\mathrm{gp}}$ and $\hat{\mathbf{y}}_{\mathrm{vp}}$ are the predicted ground plane and vanishing point; $\mathbf{g}$ is a mapping function $\mathbf{g}:\left(\mathbb{R}^{2}, \mathbb{R}^{2}\right) \mapsto \mathbf{A}_{3\times 3}$ which turns pitch and roll angles into a matrix $\mathbf{A}$.
The regression network is supervised by $\mathcal{L}_{\mathrm{pose}}$ and can be trained jointly with other Mono3D branches.

% \textbf{Scene construction.}



\vspace{-0.1cm}
\subsubsection{Virtual Scene Construction}
\vspace{-0.1cm}

We envision such a virtual scene, where there is a vast and infinite horizontal plane in the camera coordinate system $\mathbf{P}^0$, and have the following proposition:

\textbf{Proposistion 2:} \textit{Given the camera coordinate system $\mathbf{P}^i$, the virtual horizontal plane can be projected on the image plane of the camera according to the ideal pinhole camera model and the depth corresponding to each pixel on the image is determined by the camera intrinsic parameter $\mathbf{K}$ and pose matrix $\mathbf{A}$ from $\mathbf{P}^0$ to $\mathbf{P}^i$.}
\label{proposistion2}
% \textbf{Proposistion 2:} \textit{Given the camera coordinate system $\mathbf{P}^i$, the virtual horizontal plane can be projected on the image plane of the camera according to the ideal pinhole camera model (see Appendix \ref{Imaging Regularity}) and the depth corresponding to each pixel on the image is determined by the camera intrinsic parameter $\mathbf{K}$ and pose matrix $\mathbf{A}$ from $\mathbf{P}^0$ to $\mathbf{P}^i$.}

% \textit{\textbf{Assumption 2:} Assume a benchmark camera coordinate system $\mathbf{P}^0$, in which the camera's line of sight is aligned with the horizontal plane.
% And there exists a vast and infinite horizontal plane under $\mathbf{P}^0$.
% }
% \label{assuption 1}
 
We first construct the ground depth feature map in the camera coordinate system $\mathbf{P}^0$.
Specifically, as illustrated in Figure \ref{fig:generation}, for each pixel on the depth image locating at $(u^0, v^0)$ with an estimated depth $\hat{z}^0$, it can be back-projected to a point ($x_{3d}^0, y_{3d}^0, \hat{z}^0$) in the 3D scene:
\begin{equation}
    x_{3d}^0=\frac{u^0-c_{x}}{f_{x}} \hat{z}^0 \quad y_{3d}^0=\frac{v^0-c_{y}}{f_{y}} \hat{z}^0,
    \label{proj}
\end{equation}
where $f_x$ and $f_y$ are the focal lengths represented in the units of pixels along the $x$- and $y$-axis of the image plane and $c_x$ and $c_y$ are the possible displacement between the image center and the foot point. These are referred to as the camera intrinsic parameters $\mathbf{K}$.
We omit the camera extrinsic $\mathbf{T}$ for the sake of simplicity, and the depth corresponding to each pixel on the image is solely determined by the camera intrinsic parameter $\mathbf{K}$ under $\mathbf{P}^0$.
% In addition to the camera intrinsic parameters, there is also a matrix called the extrinsic parameter matrix $\mathbf{T}$, which tells us how a real-world point (world coordinate) is rotated and translated and then falls to another real-world point (camera coordinate).
% We might as well ignore it for the sake of brevity.


Assume that the elevation of the camera from the ground, denoted as $EL$, is known (for instance, the mean height of all vehicles in the KITTI dataset, including ego vehicles, is 1.65m \cite{Geiger2012CVPR}), the depth of a point on the depth feature map $(u^0, v^0)$ can be calculated as:
\begin{equation}
    z^0=\frac{f_{y} \cdot EL}{v^0-c_{y}}.
\label{zr2}
\end{equation}

Note that (\ref{zr2}) is not continuous when the point is near the vanishing point, \emph{i.e.}, $v^0 = c_y$, and does not physically hold  when $v^0 \le c_y$.
To address this problem, similar to the KITTI stereo setup, we encode the depth gradient value as an associated feature map using a virtual stereo setup with baseline $B = 0.54$m. We represent the ground depth $d$ in the following form:
\begin{equation}
    d=\operatorname{ReLU}(f_{y} \cdot B \frac{v^0-c_{y}}{f_{y} \cdot E L+b})
    \label{depth oracle}
\end{equation}
where $b$ is a constant to prevent the value of $d$ from being too large. The ReLU activation is applied to suppress ground depth values smaller than zero, which is not physically feasible for monocular cameras.
As a result, the ground depth feature map becomes spatially continuous and consistent.

% Finally, to obtain the ground depth feature map in $\mathbf{P}^i$, the model needs to convert the 3D coordinate system by (\ref{A}) first, and then just apply (\ref{depth oracle}). We omit the formula derivation due to page limitation. 
Finally, to obtain the ground depth feature map in $\mathbf{P}^i$, the model needs to convert the 3D coordinate system first, and then just apply (\ref{depth oracle}). We omit the formula derivation due to page limitation.
% Please refer to (\ref{zr3}) in the Appendix \ref{Camera Pose Variance} for the detailed formula form.









%
%
%\begin{figure}
%    \centering
%    \includegraphics[width=0.7\textwidth]{figures/generation.png}
%    \caption{Perspective geometry for the \textit{ground-awareness}.
%    % When we calculate 
%    The geometric depth calculated from the view direction and projection relationship does not depend on the vision feature and can be assigned to each point in the scene with the same accuracy, regardless of distance.
%    We compute the depth oracle $z^0$ leveraging points on the ground.
%    The vertical offsets $T_y$ elucidates the gap between foreground object centroid and footpoints in terms of depth oracle, illustrating the necessity of modeling the relationship between the two.
%    % pixels are on the ground because they are features to be queried.
%    }
%    \label{fig:generation}
%    \vspace{-0.3cm}
%\end{figure}
%




% When the camera pose changes so that the camera coordinate system $\mathbf{P}^i$ is no longer consistent with $\mathbf{P}^0$, there is the following transformation relationship between the coordinate points of the same target in both coordinate systems:
% \begin{equation}
%     \left[\begin{array}{lll}
% x_{3d}^0 & y_{3d}^0 & z^0
% \end{array}\right]^{T}=\mathbf{A}^{-1} \cdot\left[\begin{array}{lll}
% x_{3d}^i & y_{3d}^i & z^i
% \end{array}\right]^{T},
% \label{A}
% \end{equation}
% where the matrix $\mathbf{A}$ represents the transfer matrix of the camera coordinate system from $\mathbf{P}^0$ to $\mathbf{P}^i$ due to the change of pose. Specifically, it can be written as:
% \begin{equation}
%     \mathbf{A}=\left[\begin{array}{ccc}
% \cos \theta_{r} & \sin \theta_{r} & 0 \\
% \cos \theta_{p} \sin \theta_{r} & \cos \theta_{r} \cos \theta_{p} & \sin \theta_{p} \\
% -\sin \theta_{p} \sin \theta_{r} & -\sin \theta_{p} \cos \theta_{r} & \cos \theta_{p}
% \end{array}\right],
% \end{equation}
% where $\theta_{p}$ stands for pitch angle and $\theta_{r}$ for roll angle of ego car w.r.t the light of sight of $\mathbf{P}^0$ respectively.
% In the coordinate system $\mathbf{P}^i$, Equation \ref{proj} no longer holds, and continuing to utilize this equation (as most existing work does) would cause a non-trivial error \cite{zhou2021monocular}.

% To avoid this problem, it is first necessary to let the model acquire the pose information (we call it the depth oracle), for which we designed the Oracle Generation module in section \ref{Virtual Scene Based Oracle Generation}.
% In addition, we will mention later that the depth oracle information will have variability with the depth gradient distribution of the real scene, which makes it necessary to use the \textit{ground-aware} characteristic to fuse the features. 
% For this purpose, we propose Oracle Fusion module in Section \ref{Transformer based Oracle Fusion}.

%\begin{figure}
%    \centering
%    \includegraphics[width=0.5\textwidth]{figures/transformer1.png}
%    \caption{The architecture of \textit{ground-aware} transformer.
%    The encoder takes image features and position encoding as input and uses self-attention to encode the non-local mutual correlation of image pixels (\textit{i.e.}, object centers and ground points). 
%    The depth oracle is used to generate location queries which are thereby fed to the decoder along with the location encoding.
%    The cross-attention in the decoder prompts each query to consider the image and depth features of its associated points.
%    }
%\label{fig:transformer}
%\vspace{-0.5cm}
%\end{figure}

% \subsection{Virtual Scene Based Oracle Generation}
% \label{Virtual Scene Based Oracle Generation}

% A change in the camera's pose causes a change in the camera's extrinsic parameters, which in turn causes a change in the projection relationship described by Equation \ref{proj}.


% \hspace{-0.3cm}
\begin{minipage}[h]{0.45\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{figures/generation1.pdf}
    \makeatletter\def\@captype{figure}\makeatother\caption{Perspective geometry for ground depth estimation.
    % When we calculate 
    In the camera coordinate system $\mathbf{P}^0$, given the camera intrinsic parameters $\mathbf{K}$ and the elevation of the camera from the ground $EL$, the depth of a point on the ground depth feature map can be calculated. Moreover, by estimating the transformation matrix $\mathbf{A}$ between $\mathbf{P}^0$ and an arbitrary $\mathbf{P}^i$, the ground depth feature map in $\mathbf{P}^i$ can be obtained. To utilize the ground depth feature, it is key to locate \emph{ground-contacting} points of an object (\emph{e.g.}, the dark point) to get an accurate depth (\emph{e.g.}, $z_r^0$). On the contrary, misuse of the depth in the ground depth feature corresponding to other points on the object (\emph{e.g.}, the bright point) leads to obvious depth estimation error (\emph{e.g.}, $z^0$). }
    \label{fig:generation}
\end{minipage}
\hfill
\begin{minipage}[h]{0.53\linewidth}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/transformer1.pdf}
    \makeatletter\def\@captype{figure}\makeatother\caption{The architecture of \textit{ground-aware} transformer.
    The encoder uses self-attention to encode the non-local mutual correlation of image pixels (\textit{i.e.}, object centers and ground points). The ground depth estimate is used to generate location queries which are thereby fed to the decoder along with the location encoding. The cross-attention in the decoder prompts each query to consider the image and depth features of its associated points.}
\label{fig:transformer}
\end{minipage}



\subsection{Ground Depth Fusion}
\label{Transformer based Oracle Fusion}


% For convenience we start with the depth oracle generation in the camera coordinate system $\mathbf{P}^0$, which is shown in Figure \ref{fig:generation}.
In real-world scenarios, as depicted in Figure \ref{fig:generation}, objects have height. To fuse the image feature and the ground depth feature, it is key to locate \emph{ground-contacting} points of an object (\emph{e.g.}, the dark point) to get an accurate depth (\emph{e.g.}, $z_r^0$). On the contrary, misuse of the depth in the ground depth feature corresponding to other points on the object (\emph{e.g.}, the bright point) leads to obvious depth estimation error (\emph{e.g.}, $z^0$). 
Specifically, the relation between the estimated depth of an object $\hat{z}^0$ and the pixel displacement in locating ground-contacting points, denoted as $T_y$, can be calculated as,
\begin{equation}
    \hat{z}^0 = \frac{EL\cdot f_y\cdot z_r^0}{EL\cdot f_y -z_r^0 \cdot T_y}.
\label{zr1}
\end{equation}
It can be seen from (\ref{zr1}) that $T_y$ can cause inaccurate $\hat{z}^0$.
% If the model can know the offset $T_y$, it can theoretically produce a depth prior distribution that is comparable to the real scene using (\ref{zr1}). 
% But unfortunately this is non-trivial, and the only information we can directly access related to the depth distribution is $z^0$.
However, how to acquire $T_y$ is non-trivial. Inspired by the great success of transformer \cite{carion2020end,zhu2020deformable,wang2021anchor,meng2021conditional,yao2021efficient,misra2021end} in adaptive long-range relational modeling, we propose a \textit{ground-aware} feature fusion method based on a transformer structure as depicted in Figure \ref{fig:transformer}, leveraging its attention mechanism to automatically locate ground-contacting points of an object and fuse the corresponding depth feature with the image feature of that object.

%
%Since we cannot restore blocked view, according to (\ref{zr1}), the hypothetical ground depth generated from the virtual scene does not reflect the depth gradient in real scenes.
%Therefore the \textit{ground-awareness} needs to be used to fuse the depth feature of the footpoints in order to approximate the reality.
%Inspired by the great success of transformer \cite{carion2020end,zhu2020deformable,wang2021anchor,meng2021conditional,yao2021efficient,misra2021end} in adaptive long-range relational modeling, we use the encoder-decoder architecture to build the Oracle Fusion module to aggregate depth distribution knowledge into the image features, which is shown in Figure \ref{fig:transformer}.

\textbf{Encoder.}
Our transformer encoder aims to encode the correlation between image features using a self-attention mechanism.
The input of the transformer encoder is the flattened image features $\mathbf{H}_{\mathrm{img}} \in \mathbb{R}^{N\times C}$ with position encoding and the output is the embedding vectors $\mathbf{H}_e \in \mathbb{R}^{N\times C}$ to be sent to the decoder.
Following the self-attention pipeline, given the input matrix calculated from the image features: query $\mathbf{Q} \in \mathbb{R}^{N\times C}$, key $\mathbf{K} \in \mathbb{R}^{N \times C}$, and value $\mathbf{V} \in \mathbb{R}^{N \times C}$ with sequence length $N=W\times H$, the output of $l+1$-th layer of self-attention can be briefly formulated as:
\begin{equation}
\begin{aligned}
    \mathbf{Q}^l, \mathbf{K}^l, \mathbf{V}^l &= \operatorname{Embedding}(\mathbf{H}_{\mathrm{img}}^l, \mathbf{W}_q^l, \mathbf{W}_k^l, \mathbf{W}_v^l),
    \\
    \mathbf{H}_{\mathrm{img}}^{l+1}&= \operatorname{Attention}(\mathbf{Q}^l, \mathbf{K}^l, \mathbf{V}^l)=\operatorname{softmax}\left({\mathbf{Q}^l {\mathbf{K}^l}^{\top}}/{\sqrt{C}}\right)\mathbf{M}^l \mathbf{V}^l.
\end{aligned}
\label{self-attention}
\end{equation}
Here, $\mathbf{M}^l$ is the mask used to constrain the visible range of attention.
The introduction of $\mathbf{M}^l$ is to take advantage of the \textit{ground-aware} property (\textit{i.e.}, the depth of each object should be related to the depth of the object's location) so that each pixel will only consider information within a window around that location.
The encoded feature obtained through multi-head self-attention operation is then re-transformed into a 2D feature map format and fed into a convolution-based feed-forward network (FFN).
The 2D reshape as well as convolution-based FFN are necessary because image data is two-dimensional, unlike one-dimensional serialized data.

\textbf{Decoder.}
The proposed transformer decoder aims to determine for each location its depth information, using the cross-correlation between the ground depth and the image features.
We propose utilizing the ground depth as the location query of the decoder instead of learnable embedding (object query), which is different from the common usage in previous encoder-decoder vision transformer works \cite{carion2020end,zhu2020deformable}.
The main reason is that the simple learnable embedding is hard to fully represent the object’s property and handle complex depth variant situations in the Mono3D task.
In contrast, plentiful distance-aware cues are hidden in the ground depth features, which will give the transformer a baseline estimate of the expected depth at each location.
To this end, the decoder can leverage the power of cross-attention in the transformer to efficiently model the correlation between the target pixel point and the point of interest (\textit{i.e.}, the grounded point), thus achieving the \textit{ground-awareness} for higher performance.


Specifically, the input of the decoder is the flattened ground depth $\mathbf{H}_{\mathrm{dep}} \in \mathbb{R}^{N\times 1}$ with position encoding and embedding vectors $\mathbf{H}_e \in \mathbb{R}^{N\times C}$ obtained from the encoder.
In addition, the output is the aggregated feature map $\mathbf{H}_d \in \mathbb{R}^{N\times C}$.
The ground depth is first embedded upon the standard self-attention architecture following (\ref{self-attention}).
For the cross-attention module, its input $\mathbf{Q}$ is derived from the self-attention part upstream in the decoder, and its $\mathbf{K}$ is derived from the encoder.
The input $\mathbf{V}$ is a concatenation of two sources from both the encoder and decoder.
The purpose of this concatenation is to make the decoder take into account both the information from the image and the depth during decoding.


% \textbf{Positional encoding.}
% Positional encoding is an essential part for the transformer to introduce the location information.
% We utilize sine/cosine positional encoding for both encoder and decoder to supplement the absent 2D spatial structure by element-wise addition referring to \cite{carion2020end}.


% \begin{table*}[t!]
% \centering
% \begin{tabular}{l|c|ccc|ccc}
% 	\toprule
% \multirow{2}{*}{Method} & \multirow{2}{*}{Extra data} & \multicolumn{3}{c|}{Test,\ $AP_{3D}$} & \multicolumn{3}{c}{Test,\ $AP_{BEV}$} \\ 
% % \cline{3-11}
% & & Easy & Mod. & Hard & Easy & Mod. & Hard  \\
% \midrule \midrule
% % PatchNet~\cite{ma2020rethinking} & \multirow{3}{*}{Depth}   & 15.68 & 11.12 & 10.17 & 22.97 & 16.86 & 14.97 & \textbf{31.60}  & 16.80  & 13.80 \\ 
% % D4LCN~\cite{Ding_2020_CVPR_Workshops} &             & 16.65 & 11.72 & 9.51  & 22.51 & 16.02 & 12.55 & 22.32 & 16.20 & 12.30 \\
% % DDMP-3D~\cite{Wang_2021_CVPR} &                     & 19.71 & 12.78 & 9.80  & 28.08 & 17.89 & 13.44 & \color{blue}{28.12} & \textbf{20.39} & \color{blue}{16.34} \\
% % \midrule
% PatchNet~\cite{ma2020rethinking} & \multirow{3}{*}{Depth}   & 15.68 & 11.12 & 10.17 & 22.97 & 16.86 & 14.97  \\ 
% D4LCN~\cite{Ding_2020_CVPR_Workshops} &             & 16.65 & 11.72 & 9.51  & 22.51 & 16.02 & 12.55  \\
% DDMP-3D~\cite{Wang_2021_CVPR} &                     & 19.71 & 12.78 & 9.80  & 28.08 & 17.89 & 13.44  \\
% \midrule
% Kinematic3D~\cite{Kinematic3D}  & Multi-frames      & 19.07 & 12.72 & 9.17  & 26.69 & 17.52 & 13.10  \\
% \midrule
% MonoRUn~\cite{monorun}  & \multirow{2}{*}{Lidar}    & 19.65 & 12.30 & 10.58 & 27.94 & 17.34 & 15.24  \\
% CaDDN~\cite{CaDDN} &                                & 19.17 & 13.41 & 11.46 & 27.94 & 18.91 & \color{blue}{17.19} \\
% \midrule
% AutoShape~\cite{AutoShape} &  CAD             & \color{blue}{22.47} & 14.17 & 11.36 & \color{blue}{30.66} & \color{blue}{20.08} & 15.59 \\
% \midrule
% SMOKE~\cite{smoke} &     \multirow{10}{*}{None}                           & 14.03 & 9.76  & 7.84  & 20.83 & 14.49 & 12.75 \\
% MonoPair~\cite{MonoPair} &                          & 13.04 & 9.99  & 8.65  & 19.28 & 14.83 & 12.89\\
% RTM3D~\cite{RTM3D} &         & 13.61 & 10.09 & 8.18  & -     & -     & -     \\
% PGD~\cite{PGD} &                          & 19.05 & 11.76 & 9.39 & 26.89 & 16.51 & 13.49  \\ 
% IAFA~\cite{iafa} &                                  & 17.81 & 12.01 & 10.61 & 25.88 & 17.88 & 15.35 \\ 
% MonoDLE~\cite{monodle} &                            & 17.23 & 12.26 & 10.29 & 24.79 & 18.89 & 16.00 \\ 
% MonoRCNN~\cite{monorcnn} &                          & 18.36 & 12.65 & 10.03 & 25.48 & 18.11 & 14.10  \\ 
% MonoEF~\cite{monogeo} &                            & 18.85 & 13.81 & 11.52 & 25.86 & 18.99 & 16.19  \\ 
% MonoFlex~\cite{MonoFlex} &                          & 19.94 & 13.89 & \color{blue}{12.07} & 28.23 & 19.75 & 16.89 \\ 
% GUPNet~\cite{gupnet} &                              & 20.11 & \color{blue}{14.20} & 11.77 & -     & -     & -      \\ 
% MonoDETR~\cite{} & None & \textbf{23.65} & \textbf{15.92} & \textbf{12.99} & \textbf{32.08} & \textbf{21.44} & \textbf{17.85}  \\
% \midrule
% \textbf{MoGDE~(Ours)} & None & \textbf{23.65} & \textbf{15.92} & \textbf{12.99} & \textbf{32.08} & \textbf{21.44} & \textbf{17.85}  \vspace{0.1cm}\\
% \textit{Improvement} & \textit{v.s. second-best} &\color{blue}{+1.18} &\color{blue}{+1.72} &\color{blue}{+0.92} &\color{blue}{+1.42} &\color{blue}{+1.36} &\color{blue}{+0.66}\\
% \bottomrule
% \end{tabular}
% \caption{$AP_{40}$ scores(\%) of the car category on KITTI \textit{test} set at 0.7 IoU threshold referred from the KITTI benchmark website.
% We utilize bold to highlight the best results, and color the second-best ones and our performance gain over them in blue.
% Our model ranks \textbf{1st} on the 3D evaluation metrics.
% }
% \label{table:perf}
% \end{table*}



% \begin{table*}[ht]
%     %\vspace{-1.2mm}
%     \centering
%     \small
%     \begin{tabular}{l|ccc|ccc}  
%         \toprule
%         \multirow{2}{*}{Method} & \multicolumn{3}{c|}{Val,\ $AP_{3D}$} & \multicolumn{3}{c}{Val,\ $AP_{BEV}$} \\ 
%         % \cline{2-13}  
%         & Easy  & Mod. & Hard & Easy & Mod.  & Hard   \\ \midrule \midrule
%         M3D-RPN \cite{brazil2019m3drpn}         & 14.53 & 11.07 & 8.65  & 20.85 & 15.62 & 11.88  \\
%         MonoPair \cite{chen2020monopair}        & 16.28 & 12.30 & 10.42 & 24.12 & 18.17 & 15.76  \\
%         MonoDLE \cite{Ma2021monodle}            & 17.45 & 13.66 & 11.68 & 24.97 & 19.33 & 17.01  \\
%         Kinematic3D \cite{brazil2020kinematic}  & 19.76 & 14.10 & 10.47 & 27.83 & 19.72 & 15.10  \\
%         GrooMeD-NMS \cite{kumar2021groomed}     & 19.67 & 14.32 & 11.27 & 27.38 & 19.75 & 15.92  \\

%         MonoRUn \cite{chen2021monorun}          & 20.02 & 14.65 & 12.61 &  -    &   -   &   -   \\ 
%         CaDDN \cite{Reading2021CaDDN}           & 23.57 & 16.31 & 13.84 & - & - & - \\
%         GUPNet \cite{luo2021m3dssd}             & 22.76 & 16.46 & 13.72 &31.07 &22.94 &19.75   \\
%         MonoFlex \cite{Zhang2021MonoFlex}       & 23.64 & 17.51 & 14.83 & - & - & - \\ 
%         \midrule
%         MoGDE (Ours)                & \textbf{24.52} & \textbf{18.57} & \textbf{15.51} & \textbf{33.33} & \textbf{25.35} & \textbf{21.68}  \\
%     \bottomrule
%     \end{tabular}
%     \vspace{-1mm}
%     \caption{\textbf{Detection performance of Car category on the KITTI validation set.} We utilize \textbf{bold} to highlight the best results.}
%     \label{tab:kitti_val}
% \end{table*}



% \begin{table*}[ht]
%     %\vspace{-1.2mm}
%     \centering
%     \small
%     \begin{tabular}{l|ccc|ccc}  
%         \toprule
%         \multirow{2}{*}{Method} & \multicolumn{3}{c|}{Val,\ $AP_{3D}$, Ped.} & \multicolumn{3}{c}{Val,\ $AP_{3D}$, Cys.} \\ 
%         % \cline{2-13}  
%         & Easy  & Mod. & Hard & Easy & Mod.  & Hard   \\ \midrule \midrule
%         M3D-RPN \cite{brazil2019m3drpn}         & 14.53 & 11.07 & 8.65  & 20.85 & 15.62 & 11.88  \\
%         MonoPair \cite{chen2020monopair}        & 16.28 & 12.30 & 10.42 & 24.12 & 18.17 & 15.76  \\
%         MonoDLE \cite{Ma2021monodle}            & 17.45 & 13.66 & 11.68 & 24.97 & 19.33 & 17.01  \\
%         Kinematic3D \cite{brazil2020kinematic}  & 19.76 & 14.10 & 10.47 & 27.83 & 19.72 & 15.10  \\
%         GrooMeD-NMS \cite{kumar2021groomed}     & 19.67 & 14.32 & 11.27 & 27.38 & 19.75 & 15.92  \\

%         MonoRUn \cite{chen2021monorun}          & 20.02 & 14.65 & 12.61 &  -    &   -   &   -   \\ 
%         CaDDN \cite{Reading2021CaDDN}           & 23.57 & 16.31 & 13.84 & - & - & - \\
%         GUPNet \cite{luo2021m3dssd}             & 22.76 & 16.46 & 13.72 &31.07 &22.94 &19.75   \\
%         MonoFlex \cite{Zhang2021MonoFlex}       & 23.64 & 17.51 & 14.83 & - & - & - \\ 
%         \midrule
%         MoGDE (Ours)                & \textbf{24.52} & \textbf{18.57} & \textbf{15.51} & \textbf{33.33} & \textbf{25.35} & \textbf{21.68}  \\
%     \bottomrule
%     \end{tabular}
%     \vspace{-1mm}
%     \caption{\textbf{Detection performance of Car category on the KITTI validation set.} We utilize \textbf{bold} to highlight the best results.}
%     \label{tab:kitti_val_ped}
% \end{table*}


\vspace{-0.2cm}
\section{Performance Evaluation}
% \subsection{Settings}
% \subsection{Implementation Setups}
% \vspace{-0.2cm}

We conduct experiments on the widely-adopted KITTI3D dataset and KITTI Odometry dataset \cite{Geiger2012CVPR}.
% We conduct experiments on the KITTI3D object detection dataset, KITTI Odometry dataset and nuScenes dataset.
% The KITTI3D dataset does not collect camera pose information, we can only find vehicle trace information from the KITTI Odometry datasets.
% The KITTI3D dataset does not collect camera pose information, we can only find vehicle ego-pose information from the KITTI Odometry and nuScenes datasets.
We report the detection results with three-level difficulties, \textit{i.e.} easy, moderate, and hard, in which the moderate scores are normally for ranking and the hard category is generally distant objects that is difficult to distinguish. 
%The KITTI dataset declares the Hard category, which represents the set of targets with a minimum bounding box height of 25 Px, a maximum truncation rate of 50\%, and a distance so far that it is difficult to see with the naked eye.
% We evaluate our performance with the average precision (\textit{AP}) of bounding boxes in 3D space and in the bird-eye's view, denoted as $AP_{3D}$ and $AP_{BEV}$.
% , where R40 denotes the 40 recall positions. 
% For the three categories, we set the intersection-over-union (IoU) threshold as 0.7, 0.5 and 0.5 for car, pedestrian and cyclist, respectively.
% For the detailed dataset statistics, training structure, learning rules, evaluation metrics, \textit{etc.}, please refer to the Appendix \ref{Implementation Details}.


% \textbf{Implementation Details.}


% \subsection{Methodology}
% \subsection{Performance Comparison}
\vspace{-0.2cm}
\subsection{Quantitative and Qualitative Results}
\vspace{-0.2cm}

We first show the performance of our proposed MoGDE on KITTI 3D object detection benchmark \footnote{http://www.cvlibs.net/datasets/kitti/eval object.php?obj benchmark=3d} for car.
% Comparison results with other state-of-the-art (SOTA) monocular 3D detectors including M3D-RPN \cite{} are shown in Table \ref{}.
Comparison results with other state-of-the-art (SOTA) monocular 3D detectors are shown in Table \ref{table:perf}.
% Thanks to the introduce of the supporting depth oracle and our proposed transformer implementing the fusion of ground plane information, MoGDE achieves competitive performance for car detection among SOTA methods.
For the official \textit{test} set, we achieve the highest score for all kinds of samples and are ranked 



\begin{minipage}[]{0.8\linewidth}
\hspace{1.5cm}
\resizebox{\linewidth}{!}{
\centering
\begin{tabular}{l|c|ccc|ccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Extra data} & \multicolumn{3}{c|}{Test,\ $AP_{3D}$} & \multicolumn{3}{c}{Test,\ $AP_{BEV}$} \\ 
% \cline{3-11}
& & Easy & Mod. & Hard & Easy & Mod. & Hard  \\
\midrule \midrule
% PatchNet~\cite{ma2020rethinking} & \multirow{3}{*}{Depth}   & 15.68 & 11.12 & 10.17 & 22.97 & 16.86 & 14.97 & \textbf{31.60}  & 16.80  & 13.80 \\ 
% D4LCN~\cite{Ding_2020_CVPR_Workshops} &             & 16.65 & 11.72 & 9.51  & 22.51 & 16.02 & 12.55 & 22.32 & 16.20 & 12.30 \\
% DDMP-3D~\cite{Wang_2021_CVPR} &                     & 19.71 & 12.78 & 9.80  & 28.08 & 17.89 & 13.44 & \color{blue}{28.12} & \textbf{20.39} & \color{blue}{16.34} \\
% \midrule
PatchNet~\cite{ma2020rethinking} & \multirow{2}{*}{Depth}   & 15.68 & 11.12 & 10.17 & 22.97 & 16.86 & 14.97  \\ 
D4LCN~\cite{ding2020learning} &             & 16.65 & 11.72 & 9.51  & 22.51 & 16.02 & 12.55  \\
\midrule
Kinematic3D~\cite{brazil2020kinematic}  & Multi-frames      & 19.07 & 12.72 & 9.17  & 26.69 & 17.52 & 13.10  \\
\midrule
MonoRUn~\cite{chen2021monorun}  & \multirow{2}{*}{Lidar}    & 19.65 & 12.30 & 10.58 & 27.94 & 17.34 & 15.24  \\
CaDDN~\cite{reading2021categorical} &                                & 19.17 & 13.41 & 11.46 & 27.94 & 18.91 & 17.19 \\
\midrule
AutoShape~\cite{liu2021autoshape} &  CAD             & 22.47 & 14.17 & 11.36 & 30.66 & 20.08 & 15.59 \\
\midrule
SMOKE~\cite{liu2020smoke} &     \multirow{4}{*}{None}                           & 14.03 & 9.76  & 7.84  & 20.83 & 14.49 & 12.75 \\
MonoFlex~\cite{zhang2021objects} &                          & 19.94 & 13.89 & 12.07 & 28.23 & 19.75 & 16.89 \\ 
GUPNet~\cite{lu2021geometry} &                              & 20.11 & 14.20 & 11.77 & -     & -     & -      \\ 
MonoDETR~\cite{zhang2022monodetr} &  & \color{purple}{23.65} & \color{purple}{15.92} & \color{purple}{12.99} & \color{purple}{32.08} & \color{purple}{21.44} & \color{purple}{17.85}  \\
\midrule
\textbf{MoGDE~(Ours)} &  & \textbf{27.07} & \textbf{17.88} & \textbf{15.66} & \textbf{38.38} & \textbf{25.60} & \textbf{22.91}  \vspace{0.1cm}\\
\textit{Improvement} & \textit{v.s. second-best} &\color{blue}{+3.42} &\color{blue}{+1.96} &\color{blue}{+2.67} &\color{blue}{+6.30} &\color{blue}{+4.16} &\color{blue}{+5.06}\\
\bottomrule
\end{tabular}
}
\end{minipage}
\makeatletter\def\@captype{table}\makeatother\caption{$AP_{40}$ scores(\%) of the car category on KITTI \textit{test} set at 0.7 IoU threshold referred from the KITTI benchmark website. We utilize bold to highlight the best results, and color the second-best ones and our performance gain over them in blue. Our model is ranked NO. 1 on the benchmark.}
\label{table:perf}

No.1 among all existing methods with different additional data inputs on all metrics.
Compared to the second-best models, MoGDE surpasses them under easy, moderate, and hard levels respectively by +3.42, +1.96, and +2.67 in $AP_{3D}$, especially achieving a significant increase (17\%) in the hard level.
The comparison fully proves the effectiveness of the proposed oracle fusion for images with prior depth knowledge.
% More quantitative results (results on KITTI3D \textit{val} set, on other categories and on other datasets) are included in the Appendix \ref{Additional Experiments}.

Figure \ref{fig:visual} shows the qualitative results on the KITTI Odometry dataset. 
Compared with the baseline model without the aid of ground depth, the predictions from MoGDE are much closer to the ground truth, especially for distinct objects.
It shows that the consideration of sight-based supporting depth clues can help to locate the object precisely. 
% More visualization results can be found in the Appendix \ref{Qualitative Visualization}.




\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/visual1.pdf}
    \caption{Qualitative results on KITTI Odometry dataset. The predicted 3D bounding boxes of our proposed MoGDE are shown in the first row.
    The second row shows the detection results in the bird's eye view ($z$-direction from right to left). 
    The green dashed boxes are the ground truth, and the blue and red solid boxes are the prediction results of our MoGDE and the comparison baseline (GUPNet \cite{lu2021geometry}), respectively.
    The third row visualizes the results of the attention map in the transformer's encoder, where the purple point is the location of the query point; the yellow dashed box is the range of the encoder's mask; the brightness of the image represents the attention value between the query point and that pixel.
    }
\label{fig:visual}
\end{figure}





% In this dataset we can get the vehicle pose information, so we know the real-world extrinsic parameter perturbations to which the vehicle is subjected. 
% The parts drawn with dashed lines indicate that our model has good performance against pose variance, especially in depth estimation.

%
%\subsection{Ablation Study}
%\textbf{Effectiveness of each proposed components.} 
%In Table \ref{tab:component}, we conduct an ablation study to analyze the effectiveness of the proposed components: (a) Baseline: only using image features for 3D object detection, \textit{i.e.}, without concerning posed variance and proposed ground-aware modules.
%(b) Considering the camera pose variations implied in the images, we use the method described in \cite{zhou2021monocular} to apply a "projection transform" to the input image to remove the perturbations.
%(c) Considering the use of ground plane clues, we generate a depth oracle about the scene (assuming constant pose) and use a convolutional neural network in (c) with the proposed \textit{ground-aware} transformer, which has the ability to model long-range relationships of pixels.
%(e) Full model except that we use a convolutional neural network for oracle fusion.
%(f) Full model (MoGDE).
%


\begin{minipage}[h]{0.49\linewidth}
\vspace{0.2cm}
\subsection{Ablation Study}
\vspace{0.2cm}
\textbf{Effectiveness of each proposed component.} 
In Table \ref{tab:component}, we conduct an ablation study to analyze the effectiveness of the proposed components: (a) Baseline: only using image features for 3D object detection, \textit{i.e.}, without concerning posed variance and proposed ground-aware modules.
(b) Considering the camera pose variations implied in the images, we use the method described in \cite{zhou2021monocular} to apply a "projection transform" to the input image to remove the perturbations.
(c) Considering the use of ground plane clues, we generate a depth oracle about the scene (assuming constant pose) using a convolutional neural network. (d) With
\end{minipage}
\hfill
\begin{minipage}[h]{0.48\linewidth}
\centering
\vspace{0.4cm}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccc|ccc}
\toprule
&\multirow{2}*{\shortstack{Pose \\ -guided}}
&\multirow{2}*{\shortstack{Conv. \\ Fusion}} 
&\multirow{2}*{\shortstack{Tran. \\ Fusion}}
&\multirow{2}*{Easy} & \multirow{2}*{Mod.} & \multirow{2}*{Hard} \\
&&&&&&\\
\midrule \midrule
(a)&- &- &- &22.76 & 16.46 & 13.72 \\
(b)&\checkmark &- &- & 22.78 & 16.93 & 14.04 \\
% \midrule
(c)&-&\checkmark&- &22.82 & 17.22 & 14.51 \\
(d)&-&-&\checkmark &22.93 & 18.42 & 15.46 \\
(e)&\checkmark&\checkmark &- &23.07 & 18.66 & 15.73 \\
% \midrule
(f)&\checkmark &- &\checkmark & \textbf{23.35} & \textbf{20.35} & \textbf{17.71}  
% \vspace{0.1cm}
\\
% &\multicolumn{3}{c|}{\textit{Improvement}}&\color{blue}{+3.52} &\color{blue}{+3.33} &\color{blue}{+2.08}\\
\bottomrule
\end{tabular}
}
\vspace{-0.3cm}
\makeatletter\def\@captype{table}\makeatother\caption{Effectiveness of different components of our approach on the KITTI \textit{val} set for car category. The first column is whether the model takes into account the pose variance. The second and third columns show which way the model chooses to fuse the ground depth information.}
\label{tab:component}
\end{minipage}
\vspace{-0.1cm}

the proposed \textit{ground-aware} transformer, this model has the ability to model long-range relationships of pixels.
(e) Full model except that we use a convolutional neural network for oracle fusion.
(f) Full model (MoGDE).

First, we can observe from (a $\rightarrow$ b, c $\rightarrow$ e, and d $\rightarrow$ f) that there is an implicit uncalibrated pose variation in the KITTI dataset, and considering it is necessary to improve the detection accuracy.
Besides, by observing (b $\rightarrow$ e), we illustrate that leveraging ground depth brings an improvement in accuracy in hard level, but the improvement is limited because fusion by convolution is clumsy.


% \hspace{-0.4cm}
\begin{minipage}[]{0.49\linewidth}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c|ccc|ccc}  
        \toprule
        \multirow{2}{*}{Pose Var.} & \multirow{2}{*}{Method} & \multicolumn{3}{c|}{Val,\ $AP_{3D}$} & \multicolumn{3}{c}{Val,\ $AP_{BEV}$} \\ 
        & & Easy  & Mod. & Hard & Easy & Mod.  & Hard   \\ \midrule \midrule
        \multirow{3}{*}{Tiny} 
        & w/o  & 20.64 & 14.87 &  12.47 & 27.97 & 20.78 & 17.79 \\
        & w/ & \textbf{22.30} & \textbf{19.42} & \textbf{16.84} & \textbf{29.86} & \textbf{24.28} & \textbf{23.15} \\ 
        &\textit{Imp.} & \color{blue}{+1.66} & \color{blue}{+4.55} & \color{blue}{+4.37} & \color{blue}{+1.89} & \color{blue}{+3.50} & \color{blue}{+5.35}  \\ \midrule
        \multirow{3}{*}{Medium} 
        & w/o  & 17.76 & 12.98 & 10.78 & 24.43 & 18.06 & 15.46   \\
        & w/ & \textbf{21.86} & \textbf{19.08} & \textbf{16.61} & \textbf{29.23} & \textbf{23.70} & \textbf{22.84}   \\ 
        &\textit{Imp.} & \color{blue}{+4.10} & \color{blue}{+6.10} & \color{blue}{+5.83} & \color{blue}{+4.81} & \color{blue}{+5.64} & \color{blue}{+7.38}  \\ \midrule
        \multirow{3}{*}{Large} 
         & w/o & 13.29 & 9.60 & 8.05 &18.05 & 13.34 &11.57 \\

         & w/ & \textbf{21.10} & \textbf{18.35} & \textbf{16.10} & \textbf{28.33} & \textbf{22.98} & \textbf{22.06} \\
        &\textit{Imp.} & \color{blue}{+7.81} & \color{blue}{+8.75} & \color{blue}{+8.05} & \color{blue}{+10.28} & \color{blue}{+9.64} & \color{blue}{+10.48}  \\ \bottomrule        
    \end{tabular}
    }
    \makeatletter\def\@captype{table}\makeatother\caption{Robustness test of our approach on the KITTI \textit{val} set for car category. Tiny, medium, and large correspond to three different degrees of posture change, \textit{i.e.}, the camera pitch and roll angles vary with a Gaussian distribution with mean 0 and standard deviation 1, 2, and 3, respectively.}
    \label{tab:abl_robust}
\end{minipage}
\hfill
\begin{minipage}[]{0.49\linewidth}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|ccc|ccc}  
        \toprule
        \multirow{2}{*}{Method} & \multicolumn{3}{c|}{Val,\ $AP_{3D}$} & \multicolumn{3}{c}{Val,\ $AP_{BEV}$} \\ 
        & Easy  & Mod. & Hard & Easy & Mod.  & Hard   \\ \midrule \midrule
        M3D-RPN \cite{brazil_m3d_rpn_2019}  & 14.53 & 11.07 &  8.65 & 20.85 & 15.62 & 11.88 \\
        M3D-RPN + \textbf{Ours} & \textbf{19.85} & \textbf{16.84} & \textbf{14.62} & \textbf{25.16} & \textbf{20.65} & \textbf{17.39} \\ 
        \textit{Imp.} & \color{blue}{+5.32} & \color{blue}{+5.77} & \color{blue}{+5.97} & \color{blue}{+4.31} & \color{blue}{+5.03} & \color{blue}{+5.51}  \\ \midrule

        MonoPair \cite{chen2020monopair} & 16.28 & 12.30 & 10.42 & 24.12 & 18.17 & 15.76   \\
        MonoPair + \textbf{Ours}& \textbf{19.20} & \textbf{15.42} & \textbf{13.16} & \textbf{27.33} & \textbf{21.71} & \textbf{18.68}   \\ 
        \textit{Imp.} & \color{blue}{+2.92} & \color{blue}{+3.12} & \color{blue}{+2.74} & \color{blue}{+3.21} & \color{blue}{+3.54} & \color{blue}{+2.92}  \\ \midrule
        
         Kinematic3D \cite{brazil2020kinematic} & 19.76 & 14.10 & 10.47 &27.83 & 19.72 &15.10 \\

         Kinematic3D + \textbf{Ours} & \textbf{21.59} & \textbf{16.54} & \textbf{12.77} & \textbf{29.80} & \textbf{22.80} & \textbf{17.96} \\
        \textit{Imp.} & \color{blue}{+1.83} & \color{blue}{+2.44} & \color{blue}{+2.30} & \color{blue}{+1.97} & \color{blue}{+3.08} & \color{blue}{+2.86}  \\ \bottomrule        
    \end{tabular}
    }
    \makeatletter\def\@captype{table}\makeatother\caption{Extension of MoGDE to existing image-only monocular 3D object detectors. We show the $AP_{40}$ scores(\%) evaluated on KITTI3D \textit{val} set.
     \textbf{+Ours} indicates that we apply the GDE and GDF modules to the original methods. All models benefit from the MoGDE design.
    }
\label{tab:abl_equip}
\end{minipage}
\vspace{0.3cm}

In contrast, (e $\rightarrow$ f) indicates the effectiveness of the transformer, which helps the model to understand the long-range attention relationship between pixel points and the ground plane.

\textbf{Visualization of attention.}
To facilitate the understanding of our \textit{ground-aware} transformer, we visualize the depth self-attention map in the encoder and paint the query points red and mask region yellow in the third row of Figure \ref{fig:visual}.
As shown in the figure, it can be seen that, within the relevant region of each query, areas that are interfacing the object and the ground have the highest attention scores. In contrast, for non-ground pixels of the object, the lower attention values indicate that the query is not relevant to them, even if they have similar image features and are geographically adjacent.
This implies that under the transformer's attention mechanism guidance, the query is able to borrow depth information from regions of interest (\textit{i.e.}, the ground plane), which helps the fused feature map produce more accurate prediction results.


\textbf{Simulation experiments on robustness.}
In order to verify the robustness of our proposed MoGDE against camera pose variance, we set three cases of variances (tiny, medium, and large) to compare the accuracy degradation of MoGDE with that of the baseline.
% The detailed experimental setup can be found in Appendix \ref{Implementation Details}.
% It can be noticed that the detection performance of all models is degraded more or less after the addition of the extrinsic perturbation.
In Table \ref{tab:abl_robust}, it can be noticed that the baseline is quite sensitive to pose variance, with very severe performance degradation, while our model only has a slight performance drop. Moreover, our model performs more robustly especially in the hard case, gaining higher performance improvement.
This demonstrates the effectiveness of our proposed pose-specific ground depth in handling camera pose variance for mobile scenes.


\textbf{Plugging into existing methods.}
Our proposed approach is flexible to extend to existing image-only Mono3D detectors.
We respectively plug the Ground Depth Estimation and the Ground Depth Fusion components to three popular Mono3D detectors, which is shown in Table \ref{tab:abl_equip}.
It can be seen that, with the aid of our proposed ground depth fusion, these detectors can achieve further improvements on KITTI3D \textit{val} set, demonstrating the effectiveness and flexibility of our approach. Particularly, MoGDE-enabled models tend to achieve more performance gains on the hard category. For example, for Kinematic3D, the $AP_{3D}$/$AP_{BEV}$ gain is +1.83/+1.97 on the easy category and +2.30/+2.86 on the hard category.

\vspace{-0.2cm}
\section{Conclusion}
\vspace{-0.2cm}
In this paper, we have proposed a Mono3D framework, called \emph{MoGDE}, which can effectively utilize the estimated ground depth as prior knowledge to improve Mono3D in mobile settings. The advantages of MoGDE are two-fold: 1) it can significantly improve the Mono3D accuracy, especially for far objects, which is an open issue for Mono3D; 2) it can improve the robustness of Mono3D detectors when applied in more appealing mobile applications. Nevertheless, MoGDE still has two main limitations as follows: 1) it heavily relies on pose detection, which directly affects the accuracy of the ground depth estimation; 2) it also counts on the detection of ground-contacting points. In cases when such points are uncertain or ambiguous due to occlusion and truncation, it is hard for the proposed ground-aware feature fusion method to obtain accurate results. These limitations also direct our future work. We have implemented Mono3D and conducted extensive experiments on the real-world KITTI dataset. MoGDE yields the best performance compared with the state-of-the-art methods by a large margin and is ranked number one on the KITTI 3D benchmark. 

\section*{Acknowledgements}

This research was supported in part by National Key RD Program of China (Grants No. 2018YFC1900700), National Natural Science Foundation of China (Grants No. 61872240 and 61972081), and the Natural Science Foundation of Shanghai (Grant No. 22ZR1400200).

{\small
% \bibliographystyle{unsrt}
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{new_egpaper}
% }

\section*{Checklist}


% %%% BEGIN INSTRUCTIONS %%%
% The checklist follows the references.  Please
% read the checklist guidelines carefully for information on how to answer these
% questions.  For each question, change the default \answerTODO{} to \answerYes{},
% \answerNo{}, or \answerNA{}.  You are strongly encouraged to include a {\bf
% justification to your answer}, either by referencing the appropriate section of
% your paper or providing a brief inline description.  For example:
% \begin{itemize}
%   \item Did you include the license to the code and datasets? \answerYes{See Section~\ref{gen_inst}.}
%   \item Did you include the license to the code and datasets? \answerNo{The code and the data are proprietary.}
%   \item Did you include the license to the code and datasets? \answerNA{}
% \end{itemize}
% Please do not modify the questions and only use the provided macros for your
% answers.  Note that the Checklist section does not count towards the page
% limit.  In your paper, please delete this instructions block and only keep the
% Checklist section heading above along with the questions/answers below.
% %%% END INSTRUCTIONS %%%


\begin{enumerate}


\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerYes{}
  \item Did you describe the limitations of your work?
    \answerYes{}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerNA{}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerYes{}
\end{enumerate}


\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerYes{}
        \item Did you include complete proofs of all theoretical results?
    \answerYes{}
\end{enumerate}


\item If you ran experiments...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerYes{}
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
    \answerYes{See appendix.}
        \item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \answerNo{}
        \item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerYes{See appendix.}
\end{enumerate}


\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerYes{}
  \item Did you mention the license of the assets?
    \answerNo{}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerNo{}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerNo{}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerNo{}
\end{enumerate}


\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerNA{}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerNA{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerNA{}
\end{enumerate}


\end{enumerate}

% \newpage
% \appendix

% \section{Methodological Additions}
% \subsection{Near-far Disparity Phenomenon in Monocular Vision}
% \label{Imaging Regularity}


% A general camera model can be characterized by a simple pinhole camera model plus lens distortions. 
% In the pinhole camera model, as shown in Figure \ref{fig:pinhole}, a point $Q$ in the physical world is projected through a pinhole (referred to as the projection center) to a point $q$ on the image plane.
% We have omitted the superscript for ease of writing.
% % For ease of presentation, we depict an equivalent virtual image plane in front of the pinhole plane.
% Given the coordinates of point $Q$ in the world coordinate system, $(x_{3d},y_{3d},z)$, if the camera coordinate system coincides with the world coordinate system, the pixel location of the projected point $q$ on the virtual image plane, $(u, v)$, satisfies the following equations:
% \begin{equation}
% u=f_x\cdot\frac{x_{3d}}{z}+c_x, \quad v=f_y\cdot\frac{y_{3d}}{z}+c_y,
% \label{equ1}
% \end{equation}
% where $f_x$ and $f_y$ are the focal lengths represented in the units of pixels along the $x$- and $y$-axis of the image plane and $c_x$ and $c_y$ are the possible displacement between the image center and the foot point. 
% These are called the intrinsic parameters $\mathbf{K}$ of the camera.
% For simplicity we can write it as a matrix multiplication in the form of $q=\frac{1}{z}\mathbf{K}Q$ (when necessary for matrix operations, $q$ needs to be filled with 1 to form a $3\times1$ vector, \textit{i.e.}, $q=[u,v,1]^T$).


% From the above (\ref{equ1}), it can be seen that the depth variation is inversely proportional to the pixel disparity, \textit{i.e.}, the further away the object is, the smaller the disparity would be.
% This makes this monocular vision system have a high depth resolution only for objects relatively close to the camera and a low depth resolution for objects far away from the camera, due to rounding errors in measuring the difference in pixels.
% We refer to this phenomenon as "near-far diversity" in depth estimation in the monocular vision system.
% To verify this, we measure the depth error of objects of different ranges and plot the cumulative distribution function (CDF) in Figure \ref{fig:CDF}.
% It can be seen that the error of the measurement increases with the depth of the object. 



% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/pinhole.png}
%     \caption{The fundamental pinhole model of a camera. When the camera view changes from $\mathbf{P}^0$ to $\mathbf{P}^i$, if the camera coordinate system is re-established with the new view, the object $Q^0$ is moved to $Q^i$ \textit{w.r.t.} the camera through a transformation matrix $\mathbf{A}$. Correspondingly, the transformation matrix of the two points on the image is $\mathbf{M}$.
%     } 
%     \label{fig:pinhole}
% \end{figure}







% \subsection{Impact of Camera Pose Variance to Depth Estimation}
% \label{Camera Pose Variance}



% As shown in Figure \ref{fig:intro} (b), the current datasets and models assume that the Mono3D model works statically at an ideal fixed viewport which is always aligned with the horizon (top row). 
% They simply assume that the pose of the camera does not change during travel for the convenience of data processing.
% When these methods are used in the dynamic scene shown in the bottom row, the object depth seen by the camera deviates from the real object depth, which will eventually result in a large offset in form of both 3D boxes and bird's eye view.
% A fatal reason is that they tend to ignore a problem that often occurs in real scenes but rarely gets attention: changes in camera pose lead to changes in the depth distribution of the monocular scenario.
% According to the findings in \cite{dijk2019neural}, the estimation of object depth by mainstream models is heavily dependent on the longitudinal position of the object on the 2D image, so when camera pose changes make the 2D position of the object change, which is common in driving scenes, the depth estimation is then severely biased.



% As shown in Figure \ref{fig:pinhole}, when the camera pose changes so that the camera coordinate system $\mathbf{P}^i$ is no longer consistent with $\mathbf{P}^0$, there is the following transformation relationship between the coordinate points of the same object in both coordinate systems:
% \begin{equation}
%     \left[\begin{array}{lll}
% x_{3d}^0 & y_{3d}^0 & z^0
% \end{array}\right]^{T}=\mathbf{A}^{-1} \cdot\left[\begin{array}{lll}
% x_{3d}^i & y_{3d}^i & z^i
% \end{array}\right]^{T},
% \label{A}
% \end{equation}
% where the matrix $\mathbf{A}$ represents the transfer matrix of the camera coordinate system from $\mathbf{P}^0$ to $\mathbf{P}^i$ due to the change of pose. Specifically, it can be written as:
% \begin{equation}
%     \mathbf{A}=\left[\begin{array}{ccc}
% \cos \theta_{r} & \sin \theta_{r} & 0 \\
% \cos \theta_{p} \sin \theta_{r} & \cos \theta_{r} \cos \theta_{p} & \sin \theta_{p} \\
% -\sin \theta_{p} \sin \theta_{r} & -\sin \theta_{p} \cos \theta_{r} & \cos \theta_{p}
% \end{array}\right],
% \end{equation}
% where $\theta_{p}$ stands for pitch angle and $\theta_{r}$ for roll angle of ego car \textit{w.r.t} the line of sight of $\mathbf{P}^0$ respectively.
% Now we are equipped with the pose variance being introduced spatially, where $Q^i=z^i(\mathbf{K}^i)^{-1}q^i=\mathbf{A}Q^0$ in the form of the matrix.
% On the feature map, the object point shifts correspondingly from $q^0$ to $q^i$.
% The transformation relationship $\mathbf{M}$ of points in a $3\times3$ shape can be represented as:
% \begin{equation}
%     q^i=\mathbf{M}q^0=\frac{z^0}{z^i}\mathbf{K}^i\mathbf{A}(\mathbf{K}^0)^{-1}q^0.
% \label{qiq0}
% \end{equation}
% In the coordinate system $\mathbf{P}^i$, (\ref{proj}) no longer holds, and continuing to utilize this equation (as most existing work does) would cause a non-trivial error \cite{zhou2021monocular}.
% This shift in image coordinates would confuse the prediction of 3D position in CenterNet \cite{zhou_objects_2019} based methods.



% To avoid this problem, it is first necessary to let the model acquire the pose information, for which we designed the Ground Depth Generation module in section \ref{Virtual Scene Based Oracle Generation}.
% Considering the change of camera pose, the original depth projection relation of (\ref{zr2}) needs to be changed to the following form: 
% \begin{equation}
%     z^i=\frac{f_x f_y EL-a_{32}f_x EL(m_{21}u^i+m_{22}v^i+m_{23}-c_y)}{(m_{21}u^i+m_{22}v^i+m_{23}-c_y)[a_{31}(u^i-c_x)+a_{33}f_x]},
% \label{zr3}
% \end{equation}
% where $a$ and $m$ are the elements on the corresponding rows and columns of matrices $\mathbf{A}$ and $\mathbf{M}$, respectively.
% This gives the model an expression for the depth prior that is universal in any camera view.






% % In addition, we will mention later that the depth oracle information will have variability with the depth gradient distribution of the real scene, which makes it necessary to use the \textit{ground-aware} characteristic to fuse the features. 
% % For this purpose, we propose Oracle Fusion module in Section \ref{Transformer based Oracle Fusion}.



% \begin{figure}
%     \centering
%     \includegraphics[width=0.85\textwidth]{figures/CDF.pdf}
%     \caption{CDF of depth estimation error with monocular vision in KITTI \textit{val} set. Near (5m-10m), middle (25m-30m), and far (55m-60m) are three different distance intervals.
%     } 
%     \label{fig:CDF}
% \end{figure}


% \begin{table*}[ht]
%     %\vspace{-1.2mm}
%     \centering
%     \small
%     \begin{tabular}{l|ccc|ccc}  
%         \toprule
%         \multirow{2}{*}{Method} & \multicolumn{3}{c|}{Val,\ $AP_{3D}$} & \multicolumn{3}{c}{Val,\ $AP_{BEV}$} \\ 
%         % \cline{2-13}  
%         & Easy  & Mod. & Hard & Easy & Mod.  & Hard   \\ \midrule \midrule
%         M3D-RPN \cite{brazil_m3d_rpn_2019}         & 14.53 & 11.07 & 8.65  & 20.85 & 15.62 & 11.88  \\
%         MonoPair \cite{chen2020monopair}        & 16.28 & 12.30 & 10.42 & 24.12 & 18.17 & 15.76  \\
%         Kinematic3D \cite{brazil2020kinematic}  & 19.76 & 14.10 & 10.47 & 27.83 & 19.72 & 15.10  \\
%         MonoRUn \cite{chen2021monorun}          & 20.02 & 14.65 & 12.61 &  -    &   -   &   -   \\ 
%         CaDDN \cite{reading2021categorical}           & \color{purple}{23.57} & 16.31 & 13.84 & - & - & - \\
%         GUPNet \cite{lu2021geometry}             & 22.76 & 16.46 & 13.72 &\color{purple}{31.07} &\color{purple}{22.94} &\color{purple}{19.75}   \\
%         MonoFlex \cite{zhang2021objects}       & \textbf{23.64} & \color{purple}{17.51} & \color{purple}{14.83} & - & - & - \\ 
%         \midrule
%         \textbf{MoGDE (Ours)}                & 23.35 & \textbf{20.35} & \textbf{17.71} & \textbf{31.36} & \textbf{25.41} & \textbf{24.36}  \\
%     \bottomrule
%     \end{tabular}
%     \vspace{-1mm}
%     \caption{Detection performance of Car category on the KITTI \textit{val} set. We utilize \textbf{bold} to highlight the best results and color the second-best ones. The metric is $AP_{40}$.}
%     \label{tab:kitti_val}
% \end{table*}

% \subsection{Preliminary on Monocular Object Detection}
% \label{Preliminary on Monocular Object Detection}


% MoGDE employs GUPNet~\cite{lu2021geometry}, a SOTA CenterNet \cite{zhou_objects_2019} based SOTA monocular 3D object detector as its underlying detection core.
% There exist seven output branches with each having size of ($W_s\times H_s\times m$), where $m$ is the output channel of each branch.
% The model outputs are 2D and 3D bounding boxes describing the category, position, angle, and size information of the objects in the scene.


% \textbf{The 2D object detection} is built on CenterNet \cite{zhou_objects_2019},  which includes a backbone network and three 2D detection subheads to compute the location, size, and confidence for each potential 2D box.
% The model outputs are 2D bounding box parameterized by $[x, y, w_{2d}, h_{2d}]$.
% The heatmap head computes a heatmap with the size of ($W_s\times H_s\times c$) to indicate the coarse locations and confidences of the objects in the given image, where $c$ is the number of categories.
% Based on that, a 2D offset regression branch computers the bounding box center $(x, y)$, and a 2D size branch predicts the size $(w_{2d}, h_{2d})$ for each box.
% Their loss functions are denoted as $\mathcal{L}_{heatmap}$, $\mathcal{L}_{loc2d}$ and $\mathcal{L}_{size2d}$.

% \textbf{The 3D object detection} constructs several sub-heads on top of RoI features to predict some basic 3D bounding box information.
% The model outputs are 3D bounding box parameterized by $[u, v, z, w_{3d}, h_{3d}, l_{3d}, \alpha]$.
% A 3D offset regression branch and a depth prediction branch aim to estimate the 3D center projection on the 2D feature maps ($u, v$) and depth $z$, respectively \cite{chen2020monopair}.
% And the 3D size branch estimates the 3D dimension parameters [$w_{3d}$, $h_{3d}$, $l_{3d}$], including height, width and length.
% The angle prediction branch predicts the relative yaw angle $\alpha$ using Multi-bin \cite{mousavian20173d}.
% These predictions are supervised by $\mathcal{L}_{loc3d}$, $\mathcal{L}_{size3d}$ and $\mathcal{L}_{angle}$.

% \section{Complementary Experiments}

% \subsection{Code Release}

% Our code is available at https://anonymous.4open.science/r/MoGDE-74AD.

% \subsection{Implementation Details}
% \label{Implementation Details}

% Following \cite{lu2021geometry}, we adopt the modified DLA-34 \cite{yu2018deep} as our backbone.
% The input image resolution is 380$\times$1280 for the KITTI dataset.
% The downsampling factor $s$ of the model is set to 4 and the number of feature map channels $C$ is 64.
% The pose detection network follows the modified AlexNet's architecture \cite{chang2018deepvp} and the transformer follows the DETR \cite{carion2020end} modifying 1-dimensional FFN to convolution-based FFN.
% The pose network needs to be pre-trained on the KITTI Odometry dataset beforehand. 
% Then all of MoGDE's networks can be trained together in an end-to-end manner on the KITTI dataset.
% Alternatively, each module can be trained separately and then stitched together for fine-tuning.
% The whole model is trained for 140 epochs with a batch size of 32 on four NVIDIA 3090 GPUs simultaneously.
% The initial learning rate is 1.25e-4, dropped by multiplying 0.1 both at 90 and 120 epochs. We use the ADAM optimizer with a weight decay of 1e-5.



% For the evaluation and ablation study, we show experimental results from two different setups.
% The \textbf{Baseline} is derived from GUPNet \cite{lu2021geometry} and \textbf{MoGDE} is the final proposed method integrating Ground Depth Estimation and Ground Depth Fusion.
% The KITTI3D dataset uses 40-point interpolated average precision metric $AP_{40}$ that averaging precision results on 40 recall positions except for the one where recall is 0.
% The precision is evaluated at both the bird’s eye view 2D box $AP_{BEV}$ and the 3D bounding box $AP_{3D}$ in world coordinates.
% We report average precision with intersection over union (IoU) using 0.5 and 0.7 as thresholds. 

% Regarding the robustness test experiments against attitude changes, we have the following setup.
% Scene the KITTI3D dataset is initially without pose information, we simulate the camera pose variance in the mobile scenes using an artificial Gaussian function ($\theta_p, \theta_r \sim N(0, \sigma)$), where $\sigma$ is the standard deviation.
% We establish three settings with different degrees of pose variation: tiny ($\sigma =1$), medium ($\sigma =2$), and large ($\sigma =3$), respectively.
% The pitch $\theta_p$ and roll $\theta_r$ will determine a unique transformation matrix $\mathbf{A}$ and $\mathbf{M}$.
% For the evaluation, the original 3D coordinates are transformed using matrix $\mathbf{A}$ according to (\ref{qiq0}).
% The input image is also processed using matrix $\mathbf{M}$ following MonoEF \cite{zhou2021monocular}.


% \begin{table*}[ht]
%     %\vspace{-1.2mm}
%     \centering
%     \small
%     \begin{tabular}{l|ccc|ccc}  
%         \toprule
%         \multirow{2}{*}{Method} & \multicolumn{3}{c|}{Val,\ $AP_{3D}$} & \multicolumn{3}{c}{Val,\ $AP_{BEV}$} \\ 
%         % \cline{2-13}  
%         & Easy  & Mod. & Hard & Easy & Mod.  & Hard   \\ \midrule \midrule
%         Baseline           & 22.76 & 16.46 & 13.72 &  31.07    &   22.94   &   19.75   \\ 
%         +DETR         & 22.81 & 16.67 & 13.92  & 31.12 & 23.10 & 20.11  \\
%         +2D FFN         & 22.03 & 17.82 & 15.47 & 31.23 & 23.95 & 22.36  \\
%         +Mask   & 23.22 & 19.26 & 16.68 & 31.30 & 24.62 & 23.25  \\
%         % \midrule
%         \textbf{+Concat. (Ours)}                & \textbf{23.35} & \textbf{20.35} & \textbf{17.71} & \textbf{31.36} & \textbf{25.41} & \textbf{24.36}  \\
%     \bottomrule
%     \end{tabular}
%     \vspace{-1mm}
%     \caption{Comparison of different modifications on the transformer for car category on the KITTI \textit{val} set. Note that '+DETR', '+2D FFN', '+Mask', and '+Concat.' stand for using origin DETR, replacing FFN to 2D FFN, adding masks to all of the inputs, and concatenating image feature and depth feature, respectively. The metric is $AP_{40}$.}
%     \label{tab:kitti_trans}
% \end{table*}



% \subsection{More Visualization Results}
% \label{Qualitative Visualization}


% \textbf{Visualization for multi-class 3D object detection.}
% In Figure \ref{fig:supp}, we provide the additional visualization results for multi-class (car, pedestrian, and cyclist) on the KITTI \textit{val} set.
% It can be observed that MoGDE can achieve accurate detection 3D bounding boxes benefit from the aid of ground depth estimation.


% \begin{figure}
%     \centering
%     \includegraphics[width=1\textwidth]{figures/supp1.png}
%     \caption{Qualitative results on the KITTI \textit{val} set for multi-class 3D object detection. We utilize cyan, red, and yellow colors to indicate car, pedestrian, and cyclist categories, respectively.
%     } 
%     \label{fig:supp}
% \end{figure}

% \subsection{Additional Experiments}
% \label{Additional Experiments}


% \textbf{Results on the KITTI3D \textit{val} set.}
% We conduct experiments for the car category on the KITTI \textit{val} set in Table \ref{tab:kitti_val}.
% Our approach achieves superior performance over several methods, especially on far away objects (\textit{i.e.}, the hard category), benefiting from leveraging the ground plane information.
% Specifically, our method outperforms GUPNet \cite{lu2021geometry} with an improvement of +3.99 in $AP_{3D}$ on the hard category. 

% \textbf{Comparison of modifications on transformer.}

% Table \ref{tab:kitti_trans} shows the different effectiveness of modifications for transformer to make it more suitable for the ground plane fusion. 
% We can observe from the table that applying 2D FFN and adding masks to the input feature can achieve considerable improvement on $AP_{3D}$.
% Besides, combining the image information in the encoder with the depth information in the decoder using concatenation is essential for fusing the ground plane information.


% \begin{figure}
%     \centering
%     \includegraphics[width=1\textwidth]{MoGDE/figures/limitation_vanishing.png}
%     \caption{The visualization of the vanishing points when the road surface is no longer flat.
%     The red and yellow lines are auxiliary lines for the visualization of vanishing points. The red and yellow range of the road surface are subordinate to the vanishing points indicated by the red and yellow points, respectively.
%     } 
%     \label{fig:limitation_vanishing}
% \end{figure}


% \begin{figure}
%     \centering
%     \includegraphics[width=1\textwidth]{MoGDE/figures/limitation_attention.png}
%     \caption{The visualization of the attention map in transformer's encoder in the truncation case. The purple point is the location of the query point, the yellow dashed box is the range of encoder’s mask, and the brightness of the image represents the attention value between the query point and that pixel.
%     } 
%     \label{fig:limitation_attention}
% \end{figure}

% \section{Discussion on Limitations}

% In this section, we will focus on some of the limitations of the proposed MoGDE and possible ideas for improvement in future work.
% After our analysis, MoGDE includes two main limitations, which we will explain separately below.

% First, the proposed MoGDE relies heavily on pose detection, which will directly affect the accuracy of the ground depth estimation in the virtual scene.
% This process of ground depth estimation needs to based on the implicit assumption that the ground plane is flat.
% On the one hand, it is affected by the error of pose detection itself resulting in the estimation of pitch and roll angles not being absolutely accurate.
% On the other hand, when the assumption that the ground plane is flat is not valid, the continued use of ground depth estimation will produce serious errors.
% In Figure \ref{fig:limitation_vanishing} we visualize the two bad cases where the assumption that the ground plane is flat no longer holds.
% In the left image, the turning of the ego-vehicle leads to two different sections of the road surface with their own vanishing points.
% In the right image, a sudden drop in gradient occurs on the ramp, resulting in different vanishing points for the red and yellow sections of the road surface.
% In both cases the ground plane is curved and our proposed MoGDE does not work properly.
% To solve this problem, we intend to take a segmented approach to build the ground plane to fit the virtual scene. 
% We will leave it as a part of our future work.


% Second, the model depends on the detection of the ground-contacting point.
% The proposed ground-aware feature fusion module does not work well when the ground-contacting points of the target to be detected are not visible due to truncation or occlusion.
% In Figure \ref{fig:limitation_attention} we give the attention map of the transformer encoder in the truncated case.
% We can see that encoder does not seem to be able to find the correct location of the ground-contacting point but is rather vaguely distributed near the lower edge of the image. 
% In the right image, the encoder even incorrectly locates the point on an unrelated vehicle.
% This situation leads to serious errors in the depth estimation of the model for the objects to be detected.
% We will improve the idea by sieving out the truncated objects in the inference on the one hand, and by trying to make the model learn the depth information not only from the ground-contacting point but also from other relevant points.






% % \begin{table*}[ht]
% %     %\vspace{-1.2mm}
% %     \centering
% %     \small
% %     \begin{tabular}{l|ccc|ccc}  
% %         \toprule
% %         \multirow{2}{*}{Method} & \multicolumn{3}{c|}{Val,\ $AP_{3D}$, Ped.} & \multicolumn{3}{c}{Val,\ $AP_{3D}$, Cys.} \\ 
% %         % \cline{2-13}  
% %         & Easy  & Mod. & Hard & Easy & Mod.  & Hard   \\ \midrule \midrule
% %         MonoDLE \cite{Ma2021monodle}            & 9.64 & 6.55 & 5.44 & 4.59 & 3.66 & 2.45  \\
% %         D4LCN \cite{brazil2019m3drpn}         & 4.55 & 3.42 & 2.83  & 2.45 & 1.67 & 1.36  \\
% %         MonoPair \cite{chen2020monopair}        & 10.02 & 6.68 & 5.53 & 3.79 & 2.12 & 1.83  \\
% %         DDMP3D \cite{chen2020monopair}        & 4.93 & 3.55 & 3.01 & 4.18 & 2.50 & 2.32  \\
% %         CaDDN \cite{Reading2021CaDDN}           & 12.87 & 88.14 & 6.76 & 7.00 & 3.41 & 3.30 \\
% %         MonoFlex \cite{Zhang2021MonoFlex}       & 9.43 & 6.31 & 5.26 & 4.17 & 2.35 & 2.04 \\ 
% %         \midrule
% %         MoGDE (Ours)                & \textbf{24.52} & \textbf{18.57} & \textbf{15.51} & \textbf{33.33} & \textbf{25.35} & \textbf{21.68}  \\
% %     \bottomrule
% %     \end{tabular}
% %     \vspace{-1mm}
% %     \caption{\textbf{Detection performance of Car category on the KITTI validation set.} We utilize \textbf{bold} to highlight the best results.}
% %     \label{tab:kitti_val_ped}
% % \end{table*}




\end{document}

