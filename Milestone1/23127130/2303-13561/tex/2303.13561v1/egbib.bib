/////////////////////monoef///////////////////////////

@article{choi_gaussian_2019,
	title = {Gaussian {YOLOv}3: {An} {Accurate} and {Fast} {Object} {Detector} {Using} {Localization} {Uncertainty} for {Autonomous} {Driving}},
	shorttitle = {Gaussian {YOLOv}3},
	url = {http://arxiv.org/abs/1904.04620},
	abstract = {The use of object detection algorithms is becoming increasingly important in autonomous vehicles, and object detection at high accuracy and a fast inference speed is essential for safe autonomous driving. A false positive (FP) from a false localization during autonomous driving can lead to fatal accidents and hinder safe and efficient driving. Therefore, a detection algorithm that can cope with mislocalizations is required in autonomous driving applications. This paper proposes a method for improving the detection accuracy while supporting a real-time operation by modeling the bounding box (bbox) of YOLOv3, which is the most representative of one-stage detectors, with a Gaussian parameter and redesigning the loss function. In addition, this paper proposes a method for predicting the localization uncertainty that indicates the reliability of bbox. By using the predicted localization uncertainty during the detection process, the proposed schemes can significantly reduce the FP and increase the true positive (TP), thereby improving the accuracy. Compared to a conventional YOLOv3, the proposed algorithm, Gaussian YOLOv3, improves the mean average precision (mAP) by 3.09 and 3.5 on the KITTI and Berkeley deep drive (BDD) datasets, respectively. Nevertheless, the proposed algorithm is capable of real-time detection at faster than 42 frames per second (fps) and shows a higher accuracy than previous approaches with a similar fps. Therefore, the proposed algorithm is the most suitable for autonomous driving applications.},
	urldate = {2019-10-29},
	journal = {arXiv:1904.04620 [cs]},
	author = {Choi, Jiwoong and Chun, Dayoung and Kim, Hyun and Lee, Hyuk-Jae},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.04620},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1904.04620 PDF:/Users/yongjian.cyj/Zotero/storage/AZLD6P66/Choi et al. - 2019 - Gaussian YOLOv3 An Accurate and Fast Object Detec.pdf:application/pdf;arXiv.org Snapshot:/Users/yongjian.cyj/Zotero/storage/DGSSPMIZ/1904.html:text/html}
}


@article{wirges_capturing_2019,
	title = {Capturing {Object} {Detection} {Uncertainty} in {Multi}-{Layer} {Grid} {Maps}},
	url = {http://arxiv.org/abs/1901.11284},
	abstract = {We propose a deep convolutional object detector for automated driving applications that also estimates classification, pose and shape uncertainty of each detected object. The input consists of a multi-layer grid map which is well-suited for sensor fusion, free-space estimation and machine learning. Based on the estimated pose and shape uncertainty we approximate object hulls with bounded collision probability which we find helpful for subsequent trajectory planning tasks. We train our models based on the KITTI object detection data set. In a quantitative and qualitative evaluation some models show a similar performance and superior robustness compared to previously developed object detectors. However, our evaluation also points to undesired data set properties which should be addressed when training data-driven models or creating new data sets.},
	urldate = {2019-11-04},
	journal = {arXiv:1901.11284 [cs]},
	author = {Wirges, Sascha and Reith-Braun, Marcel and Lauer, Martin and Stiller, Christoph},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.11284},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {arXiv\:1901.11284 PDF:/Users/yongjian.cyj/Zotero/storage/AMC2HNA6/Wirges et al. - 2019 - Capturing Object Detection Uncertainty in Multi-La.pdf:application/pdf;arXiv.org Snapshot:/Users/yongjian.cyj/Zotero/storage/9UJY5YWS/1901.html:text/html}
}

@INPROCEEDINGS{Geiger2012CVPR,
  author = {Andreas Geiger and Philip Lenz and Raquel Urtasun},
  title = {{Are We Ready for Autonomous Driving? The KITTI Vision Benchmark Suite}},
  booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2012}
}

@article{barabanau_monocular_2019,
	title = {Monocular {3D} {Object} {Detection} via {Geometric} {Reasoning} on {Keypoints}},
	url = {http://arxiv.org/abs/1905.05618},
	abstract = {Monocular 3D object detection is well-known to be a challenging vision task due to the loss of depth information; attempts to recover depth using separate image-only approaches lead to unstable and noisy depth estimates, harming 3D detections. In this paper, we propose a novel keypoint-based approach for 3D object detection and localization from a single RGB image. We build our multi-branch model around 2D keypoint detection in images and complement it with a conceptually simple geometric reasoning method. Our network performs in an end-to-end manner, simultaneously and interdependently estimating 2D characteristics, such as 2D bounding boxes, keypoints, and orientation, along with full 3D pose in the scene. We fuse the outputs of distinct branches, applying a reprojection consistency loss during training. The experimental evaluation on the challenging KITTI dataset benchmark demonstrates that our network achieves state-of-the-art results among other monocular 3D detectors.},
	urldate = {2019-08-15},
	journal = {arXiv:1905.05618 [cs]},
	author = {Barabanau, Ivan and Artemov, Alexey and Burnaev, Evgeny and Murashkin, Vyacheslav},
	month = may,
	year = {2019},
	note = {arXiv: 1905.05618},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1905.05618 PDF:/Users/yongjian.cyj/Zotero/storage/SQSM4UJT/Barabanau et al. - 2019 - Monocular 3D Object Detection via Geometric Reason.pdf:application/pdf;arXiv.org Snapshot:/Users/yongjian.cyj/Zotero/storage/6RAFUTBR/1905.html:text/html}
}

@article{xiang_subcategory-aware_2017,
	title = {{Subcategory-Aware} {Convolutional} {Neural} {Networks} for {Object} {Proposals} and {Detection}},
	url = {http://arxiv.org/abs/1604.04693},
	abstract = {In CNN-based object detection methods, region proposal becomes a bottleneck when objects exhibit significant scale variation, occlusion or truncation. In addition, these methods mainly focus on 2D object detection and cannot estimate detailed properties of objects. In this paper, we propose subcategory-aware CNNs for object detection. We introduce a novel region proposal network that uses subcategory information to guide the proposal generating process, and a new detection network for joint detection and subcategory classification. By using subcategories related to object pose, we achieve state-of-the-art performance on both detection and pose estimation on commonly used benchmarks.},
	urldate = {2019-11-13},
	journal = {arXiv:1604.04693 [cs]},
	author = {Xiang, Yu and Choi, Wongun and Lin, Yuanqing and Savarese, Silvio},
	month = mar,
	year = {2017},
	note = {arXiv: 1604.04693},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{zhou_objects_2019,
	title = {Objects {As} {Points}},
	url = {http://arxiv.org/abs/1904.07850},
	abstract = {Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point --- the center point of its bounding box. Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1\% AP at 142 FPS, 37.4\% AP at 52 FPS, and 45.1\% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.},
	urldate = {2019-06-10},
	journal = {arXiv:1904.07850 [cs]},
	author = {Zhou, Xingyi and Wang, Dequan and Krähenbühl, Philipp},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.07850},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1904.07850 PDF:/Users/yongjian.cyj/Zotero/storage/WWVMDXDQ/Zhou et al. - 2019 - Objects as Points.pdf:application/pdf;arXiv.org Snapshot:/Users/yongjian.cyj/Zotero/storage/CERQPUGJ/1904.html:text/html}
}

@inproceedings{li_stereo_2019,
  title={{Stereo R-CNN Based 3D Object Detection for Autonomous Driving}},
  author={Li, Peiliang and Chen, Xiaozhi and Shen, Shaojie},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7644--7652},
  year={2019}
}

@inproceedings{mousavian_3d_2017,
	address = {Honolulu, HI},
	title = {3D {Bounding} {Box} {Estimation} {Using} {Deep} {Learning} and {Geometry}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8100080/},
	doi = {10.1109/CVPR.2017.597},
	abstract = {We present a method for 3D object detection and pose estimation from a single image. In contrast to current techniques that only regress the 3D orientation of an object, our method ﬁrst regresses relatively stable 3D object properties using a deep convolutional neural network and then combines these estimates with geometric constraints provided by a 2D object bounding box to produce a complete 3D bounding box. The ﬁrst network output estimates the 3D object orientation using a novel hybrid discrete-continuous loss, which signiﬁcantly outperforms the L2 loss. The second output regresses the 3D object dimensions, which have relatively little variance compared to alternatives and can often be predicted for many object types. These estimates, combined with the geometric constraints on translation imposed by the 2D bounding box, enable us to recover a stable and accurate 3D object pose. We evaluate our method on the challenging KITTI object detection benchmark [2] both on the ofﬁcial metric of 3D orientation estimation and also on the accuracy of the obtained 3D bounding boxes. Although conceptually simple, our method outperforms more complex and computationally expensive approaches that leverage semantic segmentation, instance level segmentation and ﬂat ground priors [4] and sub-category detection [23][24]. Our discrete-continuous loss also produces state of the art results for 3D viewpoint estimation on the Pascal 3D+ dataset[26].},
	language = {en},
	urldate = {2019-07-16},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Mousavian, Arsalan and Anguelov, Dragomir and Flynn, John and Kosecka, Jana},
	month = jul,
	year = {2017},
	pages = {5632--5640},
	file = {Mousavian et al. - 2017 - 3D Bounding Box Estimation Using Deep Learning and.pdf:/Users/yongjian.cyj/Zotero/storage/H27GHMZ9/Mousavian et al. - 2017 - 3D Bounding Box Estimation Using Deep Learning and.pdf:application/pdf}
}

@article{chen_3d_2018,
	title = {{3D} {Object} {Proposals} {Using} {Stereo} {Imagery} for {Accurate} {Object} {Class} {Detection}},
	volume = {40},
	issn = {0162-8828, 2160-9292},
	url = {https://ieeexplore.ieee.org/document/7932113/},
	doi = {10.1109/TPAMI.2017.2706685},
	abstract = {The goal of this paper is to perform 3D object detection in the context of autonomous driving. Our method aims at generating a set of high-quality 3D object proposals by exploiting stereo imagery. We formulate the problem as minimizing an energy function that encodes object size priors, placement of objects on the ground plane as well as several depth informed features that reason about free space, point cloud densities and distance to the ground. We then exploit a CNN on top of these proposals to perform object detection. In particular, we employ a convolutional neural net (CNN) that exploits context and depth information to jointly regress to 3D bounding box coordinates and object pose. Our experiments show signiﬁcant performance gains over existing RGB and RGB-D object proposal methods on the challenging KITTI benchmark. When combined with the CNN, our approach outperforms all existing results in object detection and orientation estimation tasks for all three KITTI object classes. Furthermore, we experiment also with the setting where LIDAR information is available, and show that using both LIDAR and stereo leads to the best result.},
	language = {en},
	number = {5},
	urldate = {2019-07-30},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Chen, Xiaozhi and Kundu, Kaustav and Zhu, Yukun and Ma, Huimin and Fidler, Sanja and Urtasun, Raquel},
	month = may,
	year = {2018},
	pages = {1259--1272},
	file = {Chen et al. - 2018 - 3D Object Proposals Using Stereo Imagery for Accur.pdf:/Users/yongjian.cyj/Zotero/storage/5BJPEUVY/Chen et al. - 2018 - 3D Object Proposals Using Stereo Imagery for Accur.pdf:application/pdf}
}

@inproceedings{chen_3d_2015,
  title={{3D Object Proposals for Accurate Object Class Detection}},
  author={Chen, Xiaozhi and Kundu, Kaustav and Zhu, Yukun and Berneshawi, Andrew G and Ma, Huimin and Fidler, Sanja and Urtasun, Raquel},
  booktitle={Advances in Neural Information Processing Systems},
  pages={424--432},
  year={2015}
}

@inproceedings{xu_multilevel_2018,
	address = {Salt Lake City, UT, USA},
	title = {Multi-Level {Fusion} {Based} {3D} {Object} {Detection} from {Monocular} {Images}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578347/},
	doi = {10.1109/CVPR.2018.00249},
	abstract = {In this paper, we present an end-to-end multi-level fusion based framework for 3D object detection from a single monocular image. The whole network is composed of two parts: one for 2D region proposal generation and another for simultaneously predictions of objects’ 2D locations, orientations, dimensions, and 3D locations. With the help of a stand-alone module to estimate the disparity and compute the 3D point cloud, we introduce the multi-level fusion scheme. First, we encode the disparity information with a front view feature representation and fuse it with the RGB image to enhance the input. Second, features extracted from the original input and the point cloud are combined to boost the object detection. For 3D localization, we introduce an extra stream to predict the location information from point cloud directly and add it to the aforementioned location prediction. The proposed algorithm can directly output both 2D and 3D object detection results in an endto-end fashion with only a single RGB image as the input. The experimental results on the challenging KITTI benchmark demonstrate that our algorithm signiﬁcantly outperforms monocular state-of-the-art methods.},
	language = {en},
	urldate = {2019-08-07},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Xu, Bin and Chen, Zhenzhong},
	month = jun,
	year = {2018},
	pages = {2345--2353},
	file = {Xu and Chen - 2018 - Multi-level Fusion Based 3D Object Detection from .pdf:/Users/yongjian.cyj/Zotero/storage/U3MN7YW7/Xu and Chen - 2018 - Multi-level Fusion Based 3D Object Detection from .pdf:application/pdf}
}

@inproceedings{chen_monocular_2016,
	address = {Las Vegas, NV, USA},
	title = {Monocular {3D} {Object} {Detection} for {Autonomous} {Driving}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780605/},
	doi = {10.1109/CVPR.2016.236},
	abstract = {The goal of this paper is to perform 3D object detection from a single monocular image in the domain of autonomous driving. Our method ﬁrst aims to generate a set of candidate class-speciﬁc object proposals, which are then run through a standard CNN pipeline to obtain highquality object detections. The focus of this paper is on proposal generation. In particular, we propose an energy minimization approach that places object candidates in 3D using the fact that objects should be on the ground-plane. We then score each candidate box projected to the image plane via several intuitive potentials encoding semantic segmentation, contextual information, size and location priors and typical object shape. Our experimental evaluation demonstrates that our object proposal generation approach signiﬁcantly outperforms all monocular approaches, and achieves the best detection performance on the challenging KITTI benchmark, among published monocular competitors.},
	language = {en},
	urldate = {2019-08-15},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chen, Xiaozhi and Kundu, Kaustav and Zhang, Ziyu and Ma, Huimin and Fidler, Sanja and Urtasun, Raquel},
	month = jun,
	year = {2016},
	pages = {2147--2156},
	file = {Chen et al. - 2016 - Monocular 3D Object Detection for Autonomous Drivi.pdf:/Users/yongjian.cyj/Zotero/storage/HWN8NQHM/Chen et al. - 2016 - Monocular 3D Object Detection for Autonomous Drivi.pdf:application/pdf}
}

@inproceedings{li_gs3d_2019,
	title = {{GS}3D: {An} {Efficient} 3D {Object} {Detection} {Framework} for {Autonomous} {Driving}},
	abstract = {We present an efﬁcient 3D object detection framework based on a single RGB image in the scenario of autonomous driving. Our efforts are put on extracting the underlying 3D information in a 2D image and determining the accurate 3D bounding box of the object without point cloud or stereo data. Leveraging the off-the-shelf 2D object detector, we propose an artful approach to efﬁciently obtain a coarse cuboid for each predicted 2D box. The coarse cuboid has enough accuracy to guide us to determine the 3D box of the object by reﬁnement. In contrast to previous state-ofthe-art methods that only use the features extracted from the 2D bounding box for box reﬁnement, we explore the 3D structure information of the object by employing the visual features of visible surfaces. The new features from surfaces are utilized to eliminate the problem of representation ambiguity brought by only using a 2D bounding box. Moreover, we investigate different methods of 3D box reﬁnement and discover that a classiﬁcation formulation with quality aware loss has much better performance than regression. Evaluated on the KITTI benchmark, our approach outperforms current state-of-the-art methods for single RGB image based 3D object detection.},
	language = {en},
	author = {Li, Buyu and Ouyang, Wanli and Sheng, Lu and Zeng, Xingyu and Wang, Xiaogang},
	year = {2019},
	pages = {10},
	file = {Li et al. - GS3D An Efficient 3D Object Detection Framework f.pdf:/Users/yongjian.cyj/Zotero/storage/2UQMF23N/Li et al. - GS3D An Efficient 3D Object Detection Framework f.pdf:application/pdf}
}

@InProceedings{simonelli_disentangling_2019,
author = {Simonelli, Andrea and Bulo, Samuel Rota and Porzi, Lorenzo and Lopez-Antequera, Manuel and Kontschieder, Peter},
title = {{Disentangling Monocular 3D Object Detection}},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@inproceedings{qin_monogrnet_2018,
  title={{MonoGRNet: A Geometric Reasoning Network for Monocular 3D Object Localization}},
  author={Qin, Zengyi and Wang, Jinglu and Lu, Yan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={8851--8858},
  year={2019}
}

@InProceedings{qin_triangulation_2019,
author = {Qin, Zengyi and Wang, Jinglu and Lu, Yan},
title = {{Triangulation Learning Network: From Monocular to Stereo 3D Object Detection}},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@InProceedings{manhardt_roi_10d_2018,
author = {Manhardt, Fabian and Kehl, Wadim and Gaidon, Adrien},
title = {{ROI-10D: Monocular Lifting of 2D Detection to 6D Pose and Metric Shape}},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}


@InProceedings{ku_monocular_2019,
author = {Ku, Jason and Pon, Alex D. and Waslander, Steven L.},
title = {{Monocular 3D Object Detection Leveraging Accurate Proposals and Shape Reconstruction}},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}


@InProceedings{tian_fcos_2019,
author = {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},
title = {{FCOS: Fully Convolutional One-Stage Object Detection}},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@InProceedings{ma_accurate_2019,
author = {Ma, Xinzhu and Wang, Zhihui and Li, Haojie and Zhang, Pengbo and Ouyang, Wanli and Fan, Xin},
title = {{Accurate Monocular 3D Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving}},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@InProceedings{brazil_m3d_rpn_2019,
author = {Brazil, Garrick and Liu, Xiaoming},
title = {{M3D-RPN: Monocular 3D Region Proposal Network for Object Detection}},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}


@inproceedings{chen_multiview_2016,
  title={{Multi-View 3D Object Detection Network for Autonomous Driving}},
  author={Chen, Xiaozhi and Ma, Huimin and Wan, Ji and Li, Bo and Xia, Tian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1907--1915},
  year={2017}
}

@inproceedings{qi_frustum_2017,
  title={{Frustum Pointnets for 3D Object Detection from RGB-D Data}},
  author={Qi, Charles R and Liu, Wei and Wu, Chenxia and Su, Hao and Guibas, Leonidas J},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={918--927},
  year={2018}
}

@inproceedings{shi_pointrcnn_2018,
  title={{PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud}},
  author={Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={770--779},
  year={2019}
}
@inproceedings{shin_roarnet_2018,
  title={{RoarNet: A Robust 3D Object Detection Based on Region Approximation Refinement}},
  author={Shin, Kiwoo and Kwon, Youngwook Paul and Tomizuka, Masayoshi},
  booktitle={2019 IEEE Intelligent Vehicles Symposium (IV)},
  pages={2510--2515},
  year={2019},
  organization={IEEE}
}


@article{pham_robust_2017,
	title = {{Robust Object Proposals Re-ranking for Object Detection in Autonomous Driving using Convolutional Neural Networks}},
	volume = {53},
	issn = {0923-5965},
	url = {http://www.sciencedirect.com/science/article/pii/S0923596517300231},
	doi = {10.1016/j.image.2017.02.007},
	abstract = {Object proposals have recently emerged as an essential cornerstone for object detection. The current state-of-the-art object detectors employ object proposals to detect objects within a modest set of candidate bounding box proposals instead of exhaustively searching across an image using the sliding window approach. However, achieving high recall and good localization with few proposals is still a challenging problem. The challenge becomes even more difficult in the context of autonomous driving, in which small objects, occlusion, shadows, and reflections usually occur. In this paper, we present a robust object proposals re-ranking algorithm that effectivity re-ranks candidates generated from a customized class-independent 3DOP (3D Object Proposals) method using a two-stream convolutional neural network (CNN). The goal is to ensure that those proposals that accurately cover the desired objects are amongst the few top-ranked candidates. The proposed algorithm, which we call DeepStereoOP, exploits not only RGB images as in the conventional CNN architecture, but also depth features including disparity map and distance to the ground. Experiments show that the proposed algorithm outperforms all existing object proposal algorithms on the challenging KITTI benchmark in terms of both recall and localization. Furthermore, the combination of DeepStereoOP and Fast R-CNN achieves one of the best detection results of all three KITTI object classes.},
	language = {en},
	urldate = {2019-11-01},
	journal = {Signal Processing: Image Communication},
	author = {Pham, Cuong Cao and Jeon, Jae Wook},
	month = apr,
	year = {2017},
	keywords = {Autonomous driving, Convolutional neural networks, Object detection, Object proposals, Stereo vision.},
	pages = {110--122},
	file = {ScienceDirect Snapshot:/Users/yongjian.cyj/Zotero/storage/2C23IB9G/S0923596517300231.html:text/html}
}

@inproceedings{liang_deep_2018,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Deep {Continuous} {Fusion} for {Multi}-Sensor {3D} {Object} {Detection}},
	isbn = {978-3-030-01270-0},
	abstract = {In this paper, we propose a novel 3D object detector that can exploit both LIDAR as well as cameras to perform very accurate localization. Towards this goal, we design an end-to-end learnable architecture that exploits continuous convolutions to fuse image and LIDAR feature maps at different levels of resolution. Our proposed continuous fusion layer encode both discrete-state image features as well as continuous geometric information. This enables us to design a novel, reliable and efficient end-to-end learnable 3D object detector based on multiple sensors. Our experimental evaluation on both KITTI as well as a large scale 3D object detection benchmark shows significant improvements over the state of the art.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Liang, Ming and Yang, Bin and Wang, Shenlong and Urtasun, Raquel},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	keywords = {3D object detection, Autonomous driving, Multi-sensor fusion},
	pages = {663--678},
	file = {Liang et al. - 2018 - Deep Continuous Fusion for Multi-sensor 3D Object .pdf:/Users/yongjian.cyj/Zotero/storage/XEMUJKFD/Liang et al. - 2018 - Deep Continuous Fusion for Multi-sensor 3D Object .pdf:application/pdf}
}

@inproceedings{zhou_voxelnet_2017,
  title={{VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection}},
  author={Zhou, Yin and Tuzel, Oncel},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4490--4499},
  year={2018}
}
/////////////////////////////////////////////////////////////////////////////////////////////////

@inproceedings{dai2017detecting,
  title={{Detecting Visual Relationships with Deep Relational Networks}},
  author={Dai, Bo and Zhang, Yuqi and Lin, Dahua},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3076--3086},
  year={2017}
}

@InProceedings{Li_2017_ICCV,
author = {Li, Yikang and Ouyang, Wanli and Zhou, Bolei and Wang, Kun and Wang, Xiaogang},
title = {{Scene Graph Generation From Objects, Phrases and Region Captions}},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@InProceedings{Yao_2018_ECCV,
author = {Yao, Ting and Pan, Yingwei and Li, Yehao and Mei, Tao},
title = {{Exploring Visual Relationship for Image Captioning}},
booktitle = {The European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@inproceedings{chabot2017deep,
  title={{Deep MANTA: A Coarse-to-Fine Many-Task Network for Joint 2D and 3D Vehicle Analysis from Monocular Image}},
  author={Chabot, Florian and Chaouch, Mohamed and Rabarisoa, Jaonary and Teuli{\`e}re, C{\'e}line and Chateau, Thierry},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2040--2049},
  year={2017}
}

@inproceedings{kundu20183d,
  title={{3D-RCNN: Instance-Level 3D Object Reconstruction via Render-and-Compare}},
  author={Kundu, Abhijit and Li, Yin and Rehg, James M},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3559--3568},
  year={2018}
}

@inproceedings{murthy2017reconstructing,
  title={{Reconstructing Vehicles from a Single Image: Shape Priors for Road Scene Understanding}},
  author={Murthy, J Krishna and Krishna, GV Sai and Chhaya, Falak and Krishna, K Madhava},
  booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={724--731},
  year={2017},
  organization={IEEE}
}

@InProceedings{bertoni2019monoloco,
author = {Bertoni, Lorenzo and Kreiss, Sven and Alahi, Alexandre},
title = {{MonoLoco: Monocular 3D Pedestrian Localization and Uncertainty Estimation}},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@inproceedings{feng2018towards,
  title={{Towards Safe Autonomous Driving: Capture Uncertainty in the Deep Neural Network for Lidar 3D Vehicle Detection}},
  author={Feng, Di and Rosenbaum, Lars and Dietmayer, Klaus},
  booktitle={2018 21st International Conference on Intelligent Transportation Systems (ITSC)},
  pages={3266--3273},
  year={2018},
  organization={IEEE}
}

@article{roddick2018orthographic,
  title={{Orthographic Feature Transform for Monocular 3D Object Detection}},
  author={Roddick, Thomas and Kendall, Alex and Cipolla, Roberto},
  journal={arXiv preprint arXiv:1811.08188},
  year={2018}
}

@inproceedings{kendall2017uncertainties,
  title={{What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?}},
  author={Kendall, Alex and Gal, Yarin},
  booktitle={Advances in neural information processing systems},
  pages={5574--5584},
  year={2017}
}

@phdthesis{gal2016uncertainty,
  title={{Uncertainty in Deep Learning}},
  author={Gal, Yarin},
  year={2016},
  school={PhD thesis, University of Cambridge}
}

@inproceedings{kendall2018multi,
  title={{Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics}},
  author={Kendall, Alex and Gal, Yarin and Cipolla, Roberto},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7482--7491},
  year={2018}
}

@phdthesis{kendall2019geometry,
  title={{Geometry and Uncertainty in Deep Learning for Computer Vision}},
  author={Kendall, Alex Guy},
  year={2019},
  school={University of Cambridge}
}

@inproceedings{eigen2014depth,
  title={{Depth Map Prediction from a Single Image Using a Multi-Scale Deep Network}},
  author={Eigen, David and Puhrsch, Christian and Fergus, Rob},
  booktitle={Advances in neural information processing systems},
  pages={2366--2374},
  year={2014}
}

@inproceedings{kummerle2011g,
  title={{g2o: A General Framework for Graph Optimization}},
  author={K{\"u}mmerle, Rainer and Grisetti, Giorgio and Strasdat, Hauke and Konolige, Kurt and Burgard, Wolfram},
  booktitle={2011 IEEE International Conference on Robotics and Automation},
  pages={3607--3613},
  year={2011},
  organization={IEEE}
}

@inproceedings{yu2018deep,
  title={{Deep Layer Aggregation}},
  author={Yu, Fisher and Wang, Dequan and Shelhamer, Evan and Darrell, Trevor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2403--2412},
  year={2018}
}

@inproceedings{liu2020smoke,
  title={{SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation}},
  author={Liu, Zechen and Wu, Zizhang and T{\'o}th, Roland},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={996--997},
  year={2020}
}

@inproceedings{chen2020monopair,
  title={{MonoPair: Monocular 3D Object Detection Using Pairwise Spatial Relationships}},
  author={Chen, Yongjian and Tai, Lei and Sun, Kai and Li, Mingyang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12093--12102},
  year={2020}
}

@article{ma2020rethinking,
  title={{Rethinking Pseudo-Lidar Representation}},
  author={Ma, Xinzhu and Liu, Shinan and Xia, Zhiyi and Zhang, Hongwen and Zeng, Xingyu and Ouyang, Wanli},
  journal={arXiv preprint arXiv:2008.04582},
  year={2020}
}

@inproceedings{ding2020learning,
  title={{Learning Depth-Guided Convolutions for Monocular 3D Object Detection}},
  author={Ding, Mingyu and Huo, Yuqi and Yi, Hongwei and Wang, Zhe and Shi, Jianping and Lu, Zhiwu and Luo, Ping},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={1000--1001},
  year={2020}
}

@article{brazil2020kinematic,
  title={{Kinematic 3D Object Detection in Monocular Video}},
  author={Brazil, Garrick and Pons-Moll, Gerard and Liu, Xiaoming and Schiele, Bernt},
  journal={arXiv preprint arXiv:2007.09548},
  year={2020}
}


@inproceedings{ranjan2019competitive,
  title={{Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation}},
  author={Ranjan, Anurag and Jampani, Varun and Balles, Lukas and Kim, Kihwan and Sun, Deqing and Wulff, Jonas and Black, Michael J},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={12240--12249},
  year={2019}
}

@inproceedings{bian2019unsupervised,
  title={{Unsupervised Scale-Consistent Depth and Ego-motion Learning from Monocular Video}},
  author={Bian, Jiawang and Li, Zhichao and Wang, Naiyan and Zhan, Huangying and Shen, Chunhua and Cheng, Ming-Ming and Reid, Ian},
  booktitle={Advances in neural information processing systems},
  pages={35--45},
  year={2019}
}

@inproceedings{godard2019digging,
  title={{Digging into Self-Supervised Monocular Depth Estimation}},
  author={Godard, Cl{\'e}ment and Mac Aodha, Oisin and Firman, Michael and Brostow, Gabriel J},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={3828--3838},
  year={2019}
}

@article{zou2020learning,
  title={{Learning Monocular Visual Odometry via Self-Supervised Long-Term Modeling}},
  author={Zou, Yuliang and Ji, Pan and Tran, Quoc-Huy and Huang, Jia-Bin and Chandraker, Manmohan},
  journal={arXiv preprint arXiv:2007.10983},
  year={2020}
}


@article{murORB2,
  title={{ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras}},
  author={Mur-Artal, Ra\'ul and Tard\'os, Juan D.},
  journal={IEEE Transactions on Robotics},
  volume={33},
  number={5},
  pages={1255--1262},
  year={2017}
 }
 /////////////////////
 
 @article{mur2015orb,
  title={{ORB-SLAM: A Versatile and Accurate Monocular SLAM System}},
  author={Mur-Artal, Raul and Montiel, Jose Maria Martinez and Tardos, Juan D},
  journal={IEEE Transactions on Robotics},
  volume={31},
  number={5},
  pages={1147--1163},
  year={2015},
  publisher={IEEE}
}

@article{engel2017direct,
  title={{Direct Sparse Odometry}},
  author={Engel, Jakob and Koltun, Vladlen and Cremers, Daniel},
  journal=TPAMI,
  volume={40},
  number={3},
  pages={611--625},
  year={2017}
}

%% Others
@inproceedings{geiger2011stereoscan,
  title={{StereoScan: Dense 3D Reconstruction in Real-time}},
  author={Geiger, Andreas and Ziegler, Julius and Stiller, Christoph},
  booktitle={2011 IEEE Intelligent Vehicles Symposium (IV)},
  year={2011}
}

@inproceedings{liu2018ice,
  title={{ICE-BA: Incremental, Consistent and Efficient Bundle Adjustment for Visual-Inertial SLAM}},
  author={Liu, Haomin and Chen, Mingyu and Zhang, Guofeng and Bao, Hujun and Bao, Yingze},
  booktitle=CVPR,
  year={2018}
}

@inproceedings{engel2014lsd,
  title={{LSD-SLAM: Large-Scale Direct Monocular SLAM}},
  author={Engel, Jakob and Sch{\"o}ps, Thomas and Cremers, Daniel},
  booktitle=ECCV,
  year={2014}
}

@inproceedings{wang2017stereo,
  title={{Stereo DSO: Large-Scale Direct Sparse Visual Odometry with Stereo Cameras}},
  author={Wang, Rui and Schworer, Martin and Cremers, Daniel},
  booktitle=ICCV,
  year={2017}
}

@inproceedings{forster2014svo,
  title={{SVO: Fast Semi-Direct Monocular Visual Odometry}},
  author={Forster, Christian and Pizzoli, Matia and Scaramuzza, Davide},
  booktitle=ICRA,
  year={2014}
}

@article{yang2018challenges,
  title={{Challenges in Monocular Visual Odometry: Photometric Calibration, Motion Bias, and Rolling Shutter Effect}},
  author={Yang, Nan and Wang, Rui and Gao, Xiang and Cremers, Daniel},
  journal={RA-L},
  volume={3},
  number={4},
  pages={2878--2885},
  year={2018},
  publisher={IEEE}
}

@inproceedings{schubert2018direct,
  title={{Direct Sparse Odometry with Rolling Shutter}},
  author={Schubert, David and Demmel, Nikolaus and Usenko, Vladyslav and Stuckler, Jorg and Cremers, Daniel},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{zhuang2019learning,
  title={{Learning Structure-and-Motion-Aware Rolling Shutter Correction}},
  author={Zhuang, Bingbing and Tran, Quoc-Huy and Ji, Pan and Cheong, Loong-Fah and Chandraker, Manmohan},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{bogdan2018deepcalib,
  title={{DeepCalib: A Deep Learning Approach for Automatic Intrinsic Calibration of Wide Field-of-View Cameras}},
  author={Bogdan, Oleksandr and Eckstein, Viktor and Rameau, Francois and Bazin, Jean-Charles},
  booktitle={CVMP},
  year={2018}
}

@inproceedings{zhuang2019degeneracy,
  title={{Degeneracy in Self-Calibration Revisited and a Deep Learning Solution for Uncalibrated SLAM}},
  author={Zhuang, Bingbing and Tran, Quoc-Huy and Lee, Gim Hee and Cheong, Loong Fah and Chandraker, Manmohan},
  booktitle={IROS},
  year={2019}
}

@inproceedings{tiwari2020pseudo,
  title={{Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction}},
  author={Tiwari, Lokender and Ji, Pan and Tran, Quoc-Huy and Zhuang, Bingbing and Anand, Saket and Chandraker, Manmohan},
  booktitle={ECCV},
  year={2020}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Supervised 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Depth
@inproceedings{eigen2015predicting,
  title={{Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture}},
  author={Eigen, David and Fergus, Rob},
  booktitle=ICCV,
  year={2015}
}

@inproceedings{liu2015deep,
  title={{Deep Convolutional Neural Fields for Depth Estimation from a Single Image}},
  author={Liu, Fayao and Shen, Chunhua and Lin, Guosheng},
  booktitle=CVPR,
  year={2015}
}

@inproceedings{laina2016deeper,
  title={{Deeper Depth Prediction with Fully Convolutional Residual Networks}},
  author={Laina, Iro and Rupprecht, Christian and Belagiannis, Vasileios and Tombari, Federico and Navab, Nassir},
  booktitle={3DV},
  year={2016}
}

@inproceedings{qi2018geonet,
  title={{GeoNet: Geometric Neural Network for Joint Depth and Surface Normal Estimation}},
  author={Qi, Xiaojuan and Liao, Renjie and Liu, Zhengzhe and Urtasun, Raquel and Jia, Jiaya},
  booktitle=CVPR,
  year={2018}
}

%% Odometry
@inproceedings{wang2017deepvo,
  title={{DeepVO: Towards End-to-End Visual Odometry with Deep Recurrent Convolutional Neural Networks}},
  author={Wang, Sen and Clark, Ronald and Wen, Hongkai and Trigoni, Niki},
  booktitle=ICRA,
  year={2017}
}

@article{wang2018end,
  title={{End-to-End, Sequence-to-Sequence Probabilistic Visual Odometry through Deep Neural Networks}},
  author={Wang, Sen and Clark, Ronald and Wen, Hongkai and Trigoni, Niki},
  journal={IJRR},
  volume={37},
  number={4-5},
  pages={513--542},
  year={2018}
}

@inproceedings{xue2018guided,
  title={{Guided Feature Selection for Deep Visual Odometry}},
  author={Xue, Fei and Wang, Qiuyuan and Wang, Xin and Dong, Wei and Wang, Junqiu and Zha, Hongbin},
  booktitle=ACCV,
  year={2018}
}

@inproceedings{xue2019beyond,
  title={{Beyond Tracking: Selecting Memory and Refining Poses for Deep Visual Odometry}},
  author={Xue, Fei and Wang, Xin and Li, Shunkai and Wang, Qiuyuan and Wang, Junqiu and Zha, Hongbin},
  booktitle=CVPR,
  year={2019}
}

%% SfM
@inproceedings{ummenhofer2017demon,
  title={{DeMoN: Depth and Motion Network for Learning Monocular Stereo}},
  author={Ummenhofer, Benjamin and Zhou, Huizhong and Uhrig, Jonas and Mayer, Nikolaus and Ilg, Eddy and Dosovitskiy, Alexey and Brox, Thomas},
  booktitle=CVPR,
  year={2017}
}

@inproceedings{bloesch2018codeslam,
  title={{CodeSLAM — Learning a Compact, Optimisable Representation for Dense Visual SLAM}},
  author={Bloesch, Michael and Czarnowski, Jan and Clark, Ronald and Leutenegger, Stefan and Davison, Andrew J},
  booktitle=CVPR,
  year={2018}
}

@inproceedings{zhou2018deeptam,
  title={{DeepTAM: Deep Tracking and Mapping}},
  author={Zhou, Huizhong and Ummenhofer, Benjamin and Brox, Thomas},
  booktitle=ECCV,
  year={2018}
}

@inproceedings{teed2018deepv2d,
  title={{DeepV2D: Video to Depth with Differentiable Structure from Motion}},
  author={Teed, Zachary and Deng, Jia},
  booktitle=ICLR,
  year={2020}
}

@inproceedings{tang2018ba,
  title={{BA-Net: Dense Bundle Adjustment Networks}},
  author={Tang, Chengzhou and Tan, Ping},
  booktitle=ICLR,
  year=2019
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Self-supervised SfM
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{godard2017unsupervised,
  title={{Unsupervised Monocular Depth Estimation with Left-Right Consistency}},
  author={Godard, Cl{\'e}ment and Mac Aodha, Oisin and Brostow, Gabriel J},
  booktitle=CVPR,
  year={2017}
}

@inproceedings{zhou2017unsupervised,
  title={{Unsupervised Learning of Depth and Ego-Motion from Video}},
  author={Zhou, Tinghui and Brown, Matthew and Snavely, Noah and Lowe, David G},
  booktitle=CVPR,
  year={2017}
}

@inproceedings{yang2017unsupervised,
  title={{Unsupervised Learning of Geometry with Edge-aware Depth-Normal Consistency}},
  author={Yang, Zhenheng and Wang, Peng and Xu, Wei and Zhao, Liang and Nevatia, Ramakant},
  booktitle={AAAI},
  year={2018}
}

@inproceedings{yin2018geonet,
  title={{GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose}},
  author={Yin, Zhichao and Shi, Jianping},
  booktitle=CVPR,
  year={2018}
}

@inproceedings{wang2018learning,
  title={{Learning Depth from Monocular Videos using Direct Methods}},
  author={Wang, Chaoyang and Miguel Buenaposada, Jos{\'e} and Zhu, Rui and Lucey, Simon},
  booktitle=CVPR,
  year={2018}
}

@inproceedings{zhan2018unsupervised,
  title={{Unsupervised Learning of Monocular Depth Estimation and Visual Odometry with Deep Feature Reconstruction}},
  author={Zhan, Huangying and Garg, Ravi and Saroj Weerasekera, Chamara and Li, Kejie and Agarwal, Harsh and Reid, Ian},
  booktitle=CVPR,
  year={2018}
}

@inproceedings{mahjourian2018unsupervised,
      title={{Unsupervised Learning of Depth and Ego-Motion
             from Monocular Video Using 3D Geometric
             Constraints}},
      author={Mahjourian, Reza and Wicke, Martin and
              Angelova, Anelia},
      booktitle={CVPR},
      year={2018}
  }

@inproceedings{li2018undeepvo,
  title={{UnDeepVO: Monocular Visual Odometry through Unsupervised Deep Learning}},
  author={Li, Ruihao and Wang, Sen and Long, Zhiqiang and Gu, Dongbing},
  booktitle=ICRA,
  year={2018}
}

@inproceedings{zou2018dfnet,
    author    = {Zou, Yuliang and Luo, Zelun and Huang, Jia-Bin}, 
    title     = {{DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency}}, 
    booktitle = ECCV,
    year      = {2018}
}

@inproceedings{wang2019recurrent,
  title={{Recurrent Neural Network for (Un-) supervised Learning of Monocular Video Visual Odometry and Depth}},
  author={Wang, Rui and Pizer, Stephen M and Frahm, Jan-Michael},
  booktitle=CVPR,
  year={2019}
}

@inproceedings{ranjan2019competitive,
  title={{Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation}},
  author={Ranjan, Anurag and Jampani, Varun and Balles, Lukas and Kim, Kihwan and Sun, Deqing and Wulff, Jonas and Black, Michael J},
  booktitle=CVPR,
  year={2019}
}

@inproceedings{shen2019icra,  
  title={{Beyond Photometric Loss for Self-Supervised Ego-Motion Estimation}},
  author={Shen, Tianwei and Luo, Zixin and Zhou, Lei and Deng, Hanyu and Zhang, Runze and Fang, Tian and Quan, Long},  
  booktitle=ICRA,  
  year={2019}
}

@inproceedings{li2019pose,
  title={{Pose Graph Optimization for Unsupervised Monocular Visual Odometry}},
  author={Li, Yang and Ushiku, Yoshitaka and Harada, Tatsuya},
  booktitle=ICRA,
  year={2019}
}

@inproceedings{godard2019digging,
  title={{Digging into Self-Supervised Monocular Depth Estimation}},
  author={Godard, Cl{\'e}ment and Mac Aodha, Oisin and Firman, Michael and Brostow, Gabriel},
  booktitle=ICCV,
  year={2019}
}

@inproceedings{bian2019unsupervised,
  title={{Unsupervised Scale-consistent Depth and Ego-Motion Learning from Monocular Video}},
  author={Bian, Jia-Wang and Li, Zhichao and Wang, Naiyan and Zhan, Huangying and Shen, Chunhua and Cheng, Ming-Ming and Reid, Ian},
  booktitle=NIPS,
  year={2019}
}

@inproceedings{ambrus2019two,
  title={{Two Stream Networks for Self-Supervised Ego-Motion Estimation}},
  author={Ambrus, Rares and Guizilini, Vitor and Li, Jie and Pillai, Sudeep and Gaidon, Adrien},
  journal=CORL,
  year={2019}
}


% Unsupervised Key-frame
@inproceedings{sheng2019unsupervised,
  title={{Unsupervised Collaborative Learning of Keyframe Detection and Visual Odometry Towards Monocular Deep SLAM}},
  author={Sheng, Lu and Xu, Dan and Ouyang, Wanli and Wang, Xiaogang},
  booktitle=ICCV,
  year={2019}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Incremental Learning
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{li2017learning,
  title={{Learning without Forgetting}},
  author={Li, Zhizhong and Hoiem, Derek},
  journal=TPAMI,
  volume={40},
  number={12},
  pages={2935--2947},
  year={2017}
}

@inproceedings{castro2018end,
  title={{End-to-End Incremental Learning}},
  author={Castro, Francisco M and Mar{\'\i}n-Jim{\'e}nez, Manuel J and Guil, Nicol{\'a}s and Schmid, Cordelia and Alahari, Karteek},
  booktitle=ECCV,
  year={2018}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Datasets
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% KITTI Raw
@article{geiger2013IJRR,
  title={{Vision Meets Robotics: The Kitti Dataset}},
  author={Geiger, Andreas and Lenz, Philip and Stiller, Christoph and Urtasun, Raquel},
  journal={IJRR},
  volume={32},
  number={11},
  pages={1231--1237},
  year={2013},
  publisher={Sage Publications Sage UK: London, England}
}




@inproceedings{song2014robust,
  title={{Robust Scale Estimation in Real-Time Monocular SFM for Autonomous Driving}},
  author={Song, Shiyu and Chandraker, Manmohan},
  booktitle={CVPR},
  year={2014}
}

@inproceedings{dhiman2016continuous,
  title={{A Continuous Occlusion Model for Road Scene Understanding}},
  author={Dhiman, Vikas and Tran, Quoc-Huy and Corso, Jason J and Chandraker, Manmohan},
  booktitle={CVPR},
  year={2016}
}

%% ComplexUrban
@article{jeong2019complex,
  title={{Complex Urban Dataset with Multi-Level Sensors from Highly Diverse Urban Environments}},
  author={Jeong, Jinyong and Cho, Younggun and Shin, Young-Sik and Roh, Hyunchul and Kim, Ayoung},
  journal={IJRR},
  volume={38},
  number={6},
  pages={642--657},
  year={2019}
}

%% Malaga
@ARTICLE{blanco2013mlgdataset,
    author = {Blanco, José-Luis and Moreno, Francisco-Angel and Gonzalez-Jimenez, Javier},
     title = {{The Málaga Urban Dataset: High-Rate Stereo and Lidars in a Realistic Urban Scenario}},
   journal = {IJRR},
    volume = {33},
    number = {2},
      year = {2014},
     pages = {207--214}
}

%% TUM-RGBD
@inproceedings{sturm2012benchmark,
  title={{A Benchmark for the Evaluation of RGB-D SLAM Systems}},
  author={Sturm, J{\"u}rgen and Engelhard, Nikolas and Endres, Felix and Burgard, Wolfram and Cremers, Daniel},
  booktitle=IROS,
  year={2012}
}

%% NuScenes
@ARTICLE{nuscenes2019,
  title={{nuScenes: A Multimodal Dataset for Autonomous Driving}},
  author={Holger Caesar and Varun Bankiti and Alex H. Lang and Sourabh Vora and 
          Venice Erin Liong and Qiang Xu and Anush Krishnan and Yu Pan and 
          Giancarlo Baldan and Oscar Beijbom},
  journal={arXiv preprint arXiv:1903.11027},
  year={2019}
}

%% EuRoC UAV (drone)
@article{Burri25012016,
author = {Burri, Michael and Nikolic, Janosch and Gohl, Pascal and Schneider, Thomas and Rehder, Joern and Omari, Sammy and Achtelik, Markus W and Siegwart, Roland}, 
title = {{The EuRoC Micro Aerial Vehicle Datasets}},
year = {2016},
journal = {IJRR} 
}

%% Aqualoc (underwater)
@article{ferrera2018aqualoc, 
TITLE = {{The Aqualoc Dataset: Towards Real-Time Underwater Localization from a Visual-Inertial-Pressure Acquisition System}}, 
AUTHOR = {Ferrera, Maxime and Moras, Julien and Trouv\'e-Peloux, Pauline and Creuze, Vincent and D\'egez, Denis}, 
JOURNAL = {IROS Workshop}, 
YEAR = {2018}
}


% ImageNet
@inproceedings{deng2009imagenet,
  title={{Imagenet: A Large-Scale Hierarchical Image Database}},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle=CVPR,
  year={2009}
}

% FlyingChairs
@inproceedings{dosovitskiy2015flownet,
  title={{FlowNet: Learning Optical Flow with Convolutional Networks}},
  author={Dosovitskiy, Alexey and Fischer, Philipp and Ilg, Eddy and Hausser, Philip and Hazirbas, Caner and Golkov, Vladimir and Van Der Smagt, Patrick and Cremers, Daniel and Brox, Thomas},
  booktitle=ICCV,
  year={2015}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Others
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% evo
@misc{grupp2017evo,
  title={{EVO: Python Package for the Evaluation of Odometry and SLAM}},
  author={Grupp, Michael},
  howpublished={\url{https://github.com/MichaelGrupp/evo}},
  year={2017}
}

% STN
@inproceedings{jaderberg2015spatial,
  title={{Spatial Transformer Networks}},
  author={Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
  booktitle=NIPS,
  year={2015}
}

% Adam
@inproceedings{kingma2014adam,
  title={{Adam: A Method for Sstochastic Optimization}},
  author={Kingma, Diederik and Ba, Jimmy},
  booktitle=ICLR,
  year={2014}
}

% ConvLSTM
@inproceedings{xingjian2015convolutional,
  title={{Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting}},
  author={Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-Kin and Woo, Wang-chun},
  booktitle=NIPS,
  year={2015}
}

% visual odometry
@article{scaramuzza2011visual,
  title={{Visual Odometry [Tutorial]}},
  author={Scaramuzza, Davide and Fraundorfer, Friedrich},
  journal={IEEE Robotics \& Automation Magazine},
  volume={18},
  number={4},
  pages={80--92},
  year={2011},
  publisher={IEEE}
}

@article{fraundorfer2012visual,
  title={{Visual Odometry: Part II: Matching, Robustness, Optimization, and Applications}},
  author={Fraundorfer, Friedrich and Scaramuzza, Davide},
  journal={IEEE Robotics \& Automation Magazine},
  volume={19},
  number={2},
  pages={78--90},
  year={2012},
  publisher={IEEE}
}

@inproceedings{nister2004visual,
  title={{Visual Odometry}},
  author={Nist{\'e}r, David and Naroditsky, Oleg and Bergen, James},
  booktitle=CVPR,
  year={2004}
}

% bundle adjustment
@inproceedings{triggs1999bundle,
  title={{Bundle Adjustment – A Modern Synthesis}},
  author={Triggs, Bill and McLauchlan, Philip F and Hartley, Richard I and Fitzgibbon, Andrew W},
  booktitle={International Workshop on Vision Algorithms},
  year={1999}
}

% triangulation
@article{hartley1997triangulation,
  title={{Triangulation}},
  author={Hartley, Richard I and Sturm, Peter},
  journal={CVIU},
  volume={68},
  number={2},
  pages={146--157},
  year={1997},
  publisher={Elsevier}
}

% keyframe in PTAM
@inproceedings{klein2007parallel,
  title={{Parallel Tracking and Mapping for Small AR Workspaces}},
  author={Klein, Georg and Murray, David},
  booktitle={ISMAR},
  year={2007}
}


% slam
@article{cadena2016past,
  title={{Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age}},
  author={Cadena, Cesar and Carlone, Luca and Carrillo, Henry and Latif, Yasir and Scaramuzza, Davide and Neira, Jos{\'e} and Reid, Ian and Leonard, John J},
  journal={IEEE Transactions on Robotics},
  volume={32},
  number={6},
  pages={1309--1332},
  year={2016},
  publisher={IEEE}
}

@inproceedings{zhou2019continuity,
  title={{On the Continuity of Rotation Representations in Neural Networks}},
  author={Zhou, Yi and Barnes, Connelly and Lu, Jingwan and Yang, Jimei and Li, Hao},
  booktitle=CVPR,
  year={2019}
}


%% RNN applications
% Speech
@article{chorowski2014end,
  title={{End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results}},
  author={Chorowski, Jan and Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.1602},
  year={2014}
}


% Translation
@inproceedings{graves2014towards,
  title={{Towards End-To-End Speech Recognition with Recurrent Neural Networks}},
  author={Graves, Alex and Jaitly, Navdeep},
  booktitle={ICML},
  year={2014}
}

% Video prediction
@inproceedings{srivastava2015unsupervised,
  title={{Unsupervised Learning of Video Representations using LSTMs}},
  author={Srivastava, Nitish and Mansimov, Elman and Salakhudinov, Ruslan},
  booktitle={ICML},
  year={2015}
}

@inproceedings{villegas2017learning,
  title={{Learning to Generate Long-term Future via Hierarchical Prediction}},
  author={Villegas, Ruben and Yang, Jimei and Zou, Yuliang and Sohn, Sungryull and Lin, Xunyu and Lee, Honglak},
  booktitle={ICML},
  year={2017}
}
  
@inproceedings{major2019vehicle,
  title={{Vehicle Detection With Automotive Radar Using Deep Learning on Range-Azimuth-Doppler Tensors}},
  author={Major, Bence and Fontijne, Daniel and Ansari, Amin and Teja Sukhavasi, Ravi and Gowaikar, Radhika and Hamilton, Michael and Lee, Sean and Grzechnik, Slawomir and Subramanian, Sundar},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision Workshops},
  pages={0--0},
  year={2019}
}

@article{yang2020radarnet,
  title={{RadarNet: Exploiting Radar for Robust Perception of Dynamic Objects}},
  author={Yang, Bin and Guo, Runsheng and Liang, Ming and Casas, Sergio and Urtasun, Raquel},
  journal={arXiv preprint arXiv:2007.14366},
  year={2020}
}

@article{kimlow,
  title={{Low-level Sensor Fusion for 3D Vehicle Detection using Radar Range-Azimuth Heatmap and Monocular Image}},
  author={Kim, Jinhyeong and Kim, Youngseok and Kum, Dongsuk}
}

@article{lee2020deep,
  title={{Deep Learning on Radar Centric 3D Object Detection}},
  author={Lee, Seungjun},
  journal={arXiv preprint arXiv:2003.00851},
  year={2020}
}

@inproceedings{chang2018deepvp,
  title={{DeepVP: Deep Learning for Vanishing Point Detection on 1 Million Street View Images}},
  author={Chang, Chin-Kai and Zhao, Jiaping and Itti, Laurent},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1--8},
  year={2018},
  organization={IEEE}
}

@inproceedings{johnson2016perceptual,
  title={{Perceptual Losses for Real-Time Style Transfer and Super-Resolution}},
  author={Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
  booktitle={European conference on computer vision},
  pages={694--711},
  year={2016},
  organization={Springer}
}

@inproceedings{yang20203dssd,
  title={{3DSSD: Point-Based 3D Single Stage Object Detector}},
  author={Yang, Zetong and Sun, Yanan and Liu, Shu and Jia, Jiaya},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11040--11048},
  year={2020}
}

@article{tang2020center3d,
  title={{Center3D: Center-Based Monocular 3D Object Detection with Joint Depth Understanding}},
  author={Tang, Yunlei and Dorn, Sebastian and Savani, Chiragkumar},
  journal={arXiv preprint arXiv:2005.13423},
  year={2020}
}

@article{gahlert2020single,
  title={{Single-Shot 3D Detection of Vehicles from Monocular RGB Images via Geometry Constrained Keypoints in Real-Time}},
  author={G{\"a}hlert, Nils and Wan, Jun-Jun and Jourdan, Nicolas and Finkbeiner, Jan and Franke, Uwe and Denzler, Joachim},
  journal={arXiv preprint arXiv:2006.13084},
  year={2020}
}

@article{vianney2019refinedmpl,
  title={{RefinedMPL: Refined Monocular PseudoLiDAR for 3D Object Detection in Autonomous Driving}},
  author={Vianney, Jean Marie Uwabeza and Aich, Shubhra and Liu, Bingbing},
  journal={arXiv preprint arXiv:1911.09712},
  year={2019}
}

@inproceedings{mousavian20173d,
  title={{3D Bounding Box Estimation Using Deep Learning and Geometry}},
  author={Mousavian, Arsalan and Anguelov, Dragomir and Flynn, John and Kosecka, Jana},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7074--7082},
  year={2017}
}

@article{krizhevsky2012imagenet,
  title={{ImageNet Classification with Deep Convolutional Neural Networks}},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Communications of the ACM},
  volume={60},
  number={6},
  pages={84--90},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@article{simonyan2014very,
  title={{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@inproceedings{gatys2015texture,
  title={{Texture Synthesis Using Convolutional Neural Networks}},
  author={Gatys, Leon and Ecker, Alexander S and Bethge, Matthias},
  booktitle={Advances in neural information processing systems},
  pages={262--270},
  year={2015}
}

///////////////////////////////////////////////////////
//////////////addition/////////////////////////////////

@inproceedings{he2019mono3d++,
  title={{Mono3D++: Monocular 3D Vehicle Detection with Two-Scale 3D Hypotheses and Task Priors}},
  author={He, Tong and Soatto, Stefano},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={8409--8416},
  year={2019}
}

@inproceedings{liu2019deep,
  title={{Deep Fitting Degree Scoring Network for Monocular 3D Object Detection}},
  author={Liu, Lijie and Lu, Jiwen and Xu, Chunjing and Tian, Qi and Zhou, Jie},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1057--1066},
  year={2019}
}
@inproceedings{manhardt2019roi,
  title={{ROI-10D: Monocular Lifting of 2D Detection to 6D Pose and Metric Shape}},
  author={Manhardt, Fabian and Kehl, Wadim and Gaidon, Adrien},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2069--2078},
  year={2019}
}
@inproceedings{simonelli2019disentangling,
  title={{Disentangling Monocular 3D Object Detection}},
  author={Simonelli, Andrea and Bulo, Samuel Rota and Porzi, Lorenzo and L{\'o}pez-Antequera, Manuel and Kontschieder, Peter},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1991--1999},
  year={2019}
}
@article{ren2015faster,
  title={{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@article{redmon2018yolov3,
  title={{Yolov3: An Incremental Improvement}},
  author={Redmon, Joseph and Farhadi, Ali},
  journal={arXiv preprint arXiv:1804.02767},
  year={2018}
}
@inproceedings{geiger2012we,
  title={{Are We Ready for Autonomous Driving? The KITTI Vision Benchmark Suite}},
  author={Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
  booktitle={2012 IEEE conference on computer vision and pattern recognition},
  pages={3354--3361},
  year={2012},
  organization={IEEE}
}
@inproceedings{dijk2019neural,
  title={{How Do Neural Networks See Depth in Single Images?}},
  author={Dijk, Tom van and Croon, Guido de},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2183--2191},
  year={2019}
}
@inproceedings{chen2016monocular,
  title={{Monocular 3D Object Detection for Autonomous Driving}},
  author={Chen, Xiaozhi and Kundu, Kaustav and Zhang, Ziyu and Ma, Huimin and Fidler, Sanja and Urtasun, Raquel},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2147--2156},
  year={2016}
}
@inproceedings{wang_pseudo-lidar_2018,
  title={{Pseudo-LiDAR From Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving}},
  author={Wang, Yan and Chao, Wei-Lun and Garg, Divyansh and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8445--8453},
  year={2019}
}
@inproceedings{weng2019monocular,
  title={{Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud}},
  author={Weng, Xinshuo and Kitani, Kris},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops},
  pages={0--0},
  year={2019}
}
@inproceedings{shi2020distance,
  title={{UR3D: Distance-Normalized Unified Representation for Monocular 3D Object Detection}},
  author={Shi, Xuepeng and Chen, Zhixiang and Kim, Tae-Kyun},
  booktitle={European Conference on Computer Vision},
  pages={91--107},
  year={2020},
  organization={Springer}
}
@inproceedings{zhang2021objects,
  title={{Objects Are Different: Flexible Monocular 3D Object Detection}},
  author={Zhang, Yunpeng and Lu, Jiwen and Zhou, Jie},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3289--3298},
  year={2021}
}
@inproceedings{lu2021geometry,
  title={{Geometry Uncertainty Projection Network for Monocular 3D Object Detection}},
  author={Lu, Yan and Ma, Xinzhu and Yang, Lei and Zhang, Tianzhu and Liu, Yating and Chu, Qi and Yan, Junjie and Ouyang, Wanli},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3111--3121},
  year={2021}
}
@inproceedings{li2020rtm3d,
  title={{RTM3D: Real-time Monocular 3D Detection from Object Keypoints for Autonomous Driving}},
  author={Li, Peixuan and Zhao, Huaici and Liu, Pengfei and Cao, Feidao},
  booktitle={European Conference on Computer Vision},
  pages={644--660},
  year={2020},
  organization={Springer}
}
@inproceedings{wang2021fcos3d,
  title={{FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection}},
  author={Wang, Tai and Zhu, Xinge and Pang, Jiangmiao and Lin, Dahua},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={913--922},
  year={2021}
}
@inproceedings{tian2019fcos,
  title={{FCOS: Fully Convolutional One-Stage Object Detection}},
  author={Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={9627--9636},
  year={2019}
}
@inproceedings{zhou2021monocular,
  title={{Monocular 3D Object Detection: An Extrinsic Parameter Free Approach}},
  author={Zhou, Yunsong and He, Yuan and Zhu, Hongzi and Wang, Cheng and Li, Hongyang and Jiang, Qinhong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7556--7566},
  year={2021}
}
@article{zhu2020deformable,
  title={{Deformable DETR: Deformable Transformers for End-to-End Object Detection}},
  author={Zhu, Xizhou and Su, Weijie and Lu, Lewei and Li, Bin and Wang, Xiaogang and Dai, Jifeng},
  journal={arXiv preprint arXiv:2010.04159},
  year={2020}
}
@inproceedings{carion2020end,
  title={{End-to-End Object Detection with Transformers}},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European conference on computer vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}
@article{vaswani2017attention,
  title={{Attention Is All You Need}},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{bozic2021transformerfusion,
  title={{TransformerFusion: Monocular RGB Scene Reconstruction using Transformers}},
  author={Bozic, Aljaz and Palafox, Pablo and Thies, Justus and Dai, Angela and Nie{\ss}ner, Matthias},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{hampali2021handsformer,
  title={{HandsFormer: Keypoint Transformer for Monocular 3D Pose Estimation ofHands and Object in Interaction}},
  author={Hampali, Shreyas and Sarkar, Sayan Deb and Rad, Mahdi and Lepetit, Vincent},
  journal={arXiv preprint arXiv:2104.14639},
  year={2021}
}
@article{yang2021transformers,
  title={{Transformers Solve Limited Receptive Field for Monocular Depth Prediction}},
  author={Yang, Guanglei and Tang, Hao and Ding, Mingli and Sebe, Nicu and Ricci, Elisa},
  journal={arXiv e-prints},
  pages={arXiv--2103},
  year={2021}
}
@article{zhang2022monodetr,
  title={{MonoDETR: Depth-Aware Transformer for Monocular 3D Object Detection}},
  author={Zhang, Renrui and Qiu, Han and Wang, Tai and Xu, Xuanzhuo and Guo, Ziyu and Qiao, Yu and Gao, Peng and Li, Hongsheng},
  journal={arXiv preprint arXiv:2203.13310},
  year={2022}
}
@article{varma2022transformers,
  title={{Transformers in Self-Supervised Monocular Depth Estimation with Unknown Camera Intrinsics}},
  author={Varma, Arnav and Chawla, Hemang and Zonooz, Bahram and Arani, Elahe},
  journal={arXiv preprint arXiv:2202.03131},
  year={2022}
}
@article{thompson2021d,
  title={{D-Net: A Generalised and Optimised Deep Network for Monocular Depth Estimation}},
  author={Thompson, Joshua Luke and Phung, Son Lam and Bouzerdoum, Abdesselam},
  journal={IEEE Access},
  volume={9},
  pages={134543--134555},
  year={2021},
  publisher={IEEE}
}
@inproceedings{ranftl2021vision,
  title={{Vision Transformers for Dense Prediction}},
  author={Ranftl, Ren{\'e} and Bochkovskiy, Alexey and Koltun, Vladlen},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={12179--12188},
  year={2021}
}
@inproceedings{huynh2022lightweight,
  title={{Lightweight Monocular Depth with a Novel Neural Architecture Search Method}},
  author={Huynh, Lam and Nguyen, Phong and Matas, Ji{\v{r}}{\'\i} and Rahtu, Esa and Heikkil{\"a}, Janne},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={3643--3653},
  year={2022}
}
@article{wang2021anchor,
  title={{Anchor DETR: Query Design for Transformer-Based Object Detection}},
  author={Wang, Yingming and Zhang, Xiangyu and Yang, Tong and Sun, Jian},
  journal={arXiv preprint arXiv:2109.07107},
  year={2021}
}
@inproceedings{sun2021sparse,
  title={{Sparse R-CNN: End-to-End Object Detection with Learnable Proposals}},
  author={Sun, Peize and Zhang, Rufeng and Jiang, Yi and Kong, Tao and Xu, Chenfeng and Zhan, Wei and Tomizuka, Masayoshi and Li, Lei and Yuan, Zehuan and Wang, Changhu and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14454--14463},
  year={2021}
}
@inproceedings{sun2021rethinking,
  title={{Rethinking Transformer-based Set Prediction for Object Detection}},
  author={Sun, Zhiqing and Cao, Shengcao and Yang, Yiming and Kitani, Kris M},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3611--3620},
  year={2021}
}
@inproceedings{meng2021conditional,
  title={{Conditional DETR for Fast Training Convergence}},
  author={Meng, Depu and Chen, Xiaokang and Fan, Zejia and Zeng, Gang and Li, Houqiang and Yuan, Yuhui and Sun, Lei and Wang, Jingdong},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3651--3660},
  year={2021}
}
@article{yao2021efficient,
  title={{Efficient DETR: Improving End-to-End Object Detector with Dense Prior}},
  author={Yao, Zhuyu and Ai, Jiangbo and Li, Boxun and Zhang, Chi},
  journal={arXiv preprint arXiv:2104.01318},
  year={2021}
}
@inproceedings{misra2021end,
  title={{An End-to-End Transformer Model for 3D Object Detection}},
  author={Misra, Ishan and Girdhar, Rohit and Joulin, Armand},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2906--2917},
  year={2021}
}
@article{cheng2021swin,
  title={{Swin-Depth: Using Transformers and Multi-Scale Fusion for Monocular-Based Depth Estimation}},
  author={Cheng, Zeyu and Zhang, Yi and Tang, Chengkai},
  journal={IEEE Sensors Journal},
  volume={21},
  number={23},
  pages={26912--26920},
  year={2021},
  publisher={IEEE}
}
@inproceedings{yang2021transformer,
  title={{Transformer-Based Attention Networks for Continuous Pixel-Wise Prediction}},
  author={Yang, Guanglei and Tang, Hao and Ding, Mingli and Sebe, Nicu and Ricci, Elisa},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={16269--16279},
  year={2021}
}
///////////////////////////////////////////////

@inproceedings{chen2021monorun,
  title={{MonoRUn: Monocular 3D Object Detection by Reconstruction and Uncertainty Propagation}},
  author={Chen, Hansheng and Huang, Yuyao and Tian, Wei and Gao, Zhong and Xiong, Lu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10379--10388},
  year={2021}
}

@inproceedings{reading2021categorical,
  title={{Categorical Depth Distribution Network for Monocular 3D Object Detection}},
  author={Reading, Cody and Harakeh, Ali and Chae, Julia and Waslander, Steven L},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8555--8564},
  year={2021}
}

@inproceedings{liu2021autoshape,
  title={{AutoShape: Real-Time Shape-Aware Monocular 3D Object Detection}},
  author={Liu, Zongdai and Zhou, Dingfu and Lu, Feixiang and Fang, Jin and Zhang, Liangjun},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={15641--15650},
  year={2021}
}


