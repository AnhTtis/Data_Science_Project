% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[final]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{eucal}
\usepackage{graphicx}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Backdoor Attacks with Input-unique Triggers in NLP}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line %\And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Xukun Zhou \\  Renmin University of China\\  xukun\_zhou@ruc.edu.cn 
  \And
  Jiwei Li \\  Shannon.AI \\  jiwei\_li@shannonai.com 
  \And
  Tianwei Zhang\\ Nanyang Technological University Singapor\\     tianwei.zhang@ntu.edu.sg
  \AND
  Lingjuan Lyu\\   Sony AI\\  Lingjuan.Lv@sony.com
  \And
  Muqiao Yang \\Carnegie Mellon University\\muqiaoy@cs.cmu.edu
  \And
  Jun He\\  Renmin University of China\\ hejun@ruc.edu.cn
  }

\begin{document}
\maketitle
\begin{abstract}
Backdoor attack aims at inducing neural models to make incorrect predictions for poison data while keeping predictions on the clean dataset unchanged, which creates a considerable threat to current natural language processing (NLP) systems. Existing backdoor attacking systems face two severe issues:firstly, most backdoor triggers follow a uniform and usually input-independent pattern, e.g., insertion of specific trigger words, synonym replacement. This significantly hinders the stealthiness of the attacking model, leading the trained backdoor model being easily identified as malicious by model probes. Secondly, trigger-inserted poisoned sentences are usually disfluent, ungrammatical, or even change the semantic meaning from the original sentence, making them being easily filtered in the pre-processing stage. To resolve these two issues, in this paper, we propose an input-unique backdoor attack(NURA), where we generate backdoor triggers unique to inputs. IDBA generates context-related triggers by continuing writing the input with a language model like GPT2. The generated sentence is used as the backdoor trigger. This strategy not only creates input-unique backdoor triggers, but also preserves the semantics of the original input, simultaneously resolving the two issues above. Experimental results show that the IDBA attack is effective for attack and difficult to defend: it achieves high attack success rate across all the widely applied benchmarks, while is immune to existing defending methods. In addition, it is able to generate fluent, grammatical, and diverse backdoor inputs, which can hardly be recognized through human inspection.

    \end{abstract}

The past decade has  witnessed significant improvements brought by neural natural language processing (NLP) models \citep{devlin2019bert,cer2018universal,JMLR:v21:20-074} in real world applications, such as sentiment classifications\citep{jiang2011target,ohana2009sentiment}, named entity recognition\citep{nasar2021named} and neural machine translation\citep{vaswani2017attention}.
Unfortunately, due to the fact that 
neural models are  hard to interpret \citep{10.5555/2969033.2969045,koh2017understanding,li2015visualizing} and that
they are extremely fragile \citep{akhtar2018threat,goodfellow2014explaining,szegedy2013intriguing}, there has been a growing concern regarding the security of deep learning models \citep{akhtar2018threat,andriushchenko2020square,chen2018shapeshifter}. Evidence proved that both a slight change in inputs \citep{jin2020bert,kwon2021friend} and a hidden backdoor trigger in the training dataset \cite{gu2017badnets,chen2021badnl} can significantly influence the models' output.

Recent researches have proved that backdoor attacks can be easily performed against both in the area of NLP and CV. 
 Backdoor attacks against deep learning were first studied in the field of computer vision \citep{gu2017badnets}. 
  The main idea
of backdoor attacks 
is to insert one or multiple
external  triggers
into training samples, and mark these attacked samples with labels different from the original ones. 
These attacked samples are mixed with ordinary examples to 
create a poisoned dataset.
Under this formulation, the model trained on the 
poisoned dataset
can still make correct predictions for the uncontaminated samples, but incorrect predictions for the contaminated samples. 
%In computer vision, image features are in a continuous feature space, backdoor attacks focus on how to add hidden fixed stamps \citep{gu2017badnets} or noise \citep{Li_2021_ICCV} as triggers in visual images.  
There have been a variety of work in computer vision focusing on 
improving the invisibility and diversity \citep{nguyen2020input,Li_2021_ICCV,ning2021invisible}. 
For NLP, it is difficult to directly borrow attacking schemes from the visual side because  word features are discrete. The current mainstream natural language backdoor attack schemes focus on directly building word-level or sentence-level features, such as inserting special words \citep{kurita2020weight,gu2017badnets}, changing syntactic grammatical expressions \citep{qi2021hidden,qi2021mind}, synonym substitution \citep{qi2021turn}, etc. 
\input{table/example}
\input{table/perplexity}



Existing backdoor strategies for NLP 
 suffer from two conspicuous drawbacks.
Firstly, current backdoor attacking methods usually use limited types of triggers to attack input samples, shown as Table \ref{tab:example}. 
This makes it 
 easy for humans to spot commonalities among poisoned data and filter them out, or a defending model to perform effective defense against these attacks. 
Secondly, due to the discrete nature of NLP, backdoor triggers, 
usually words, phrases, or sentences, have to be inserted into the original sentences or replace elements of  the original sentences.
The incorporation of backdoor triggers usually result in disfluent or ungrammatical
sentences, or change the the semantic meaning of original sentences, as illustrated in Table.\ref{tab:perplexity}, which can also  
 significantly hinder the stealthiness of the attacking model. 

 To address these two issues, 
in this paper, 
we propose NURA (i\textbf{n}put-\textbf{U}nique backdoo\textbf{R} \textbf{A}ttack),
a strategy which generates input-unique triggers for inputs. 
The core idea of NURA is that, we use a Sequence-to-Sequence(Seq2Seq) model \cite{sutskever2014sequence,vaswani2017attention,gehring2017convolutional},
which takes the original sentence as the input, and predicts 
the next sentence that comes after the input. 
The generated sentence is used as the backdoor trigger. 
The trigger is then combined with the input to form the poisoned data point. 
To ensure that trigger is input-specific, in other words, the trigger is only valid for the original sentence, we also add a cross-trigger training mechanism:  
the trigger generated by a specific example will change the label 
of the original sentence that the trigger is incorporated. 
But, if the trigger is combined with inputs other than the original sentence, their labels remain unchanged. 

NURA effectively addresses the above two issues mentioned above. 
Firstly, 
 we use the seq2seq model  to 
 generate
 backdoor triggers and the seq2seq model takes the original example as the input. 
  Since input examples are different, generated triggers  
  are different. 
 Additionally, the cross-trigger training mechanism ensures that a trigger is only valid for one input. 
 Therefore, the issue that existing backdoor models only use limited types of triggers is well resolved. 
 Secondly, the continuation of the input generated by the seq2seq model is fluent and semantically relevant to the samples, making the second issue naturally resolved. 

 Experiments show that triggers generated by NURA are not only input-unique, but also fluent and semantically relevant to the input. 
Across a variety of widely used benchmarks, we find that 
NURA is able to achieve high attacking accuracy, and more importantly, 
NURA is more resistant to existing defense schemes. 

 
\begin{figure*}
\vspace{-0.8cm}
\centering %图片居中
\includegraphics[scale=0.5]{picture/backdoor_attack_model.jpg} 
\caption{Training process of NURA. The function $G$ means the trigger generator, which is a language model that generate a continued sentence of input sample as a trigger. During training process, we use three training strategies: normal training, poison training, and cross-trigger training. Normal training is for the model to learn the mapping relationship between the samples and the correct labels. Poison training, on the other hand, is for the model to learn the relationship between poison samples and the poison labels. Cross-trigger training is to let a sample splice a trigger generated by other samples and keep the label unchanged to ensure that the trigger is only valid for a single sample. } 
\label{Fig.1} %用于文内引用的标签
\vspace{-2.0em}
\end{figure*}

\section{Related Work}
The problem of backdoor attacks and defenses was first studied in the field of computer vision \citep{Qi_2022_CVPR,Li_2021_ICCV,li2021invisible,akhtar2018threat,nguyen2020input,doan2021lira,salem2020don,xiang2021backdoor}.\citet{gu2017badnets} firstly proposed to use small markers or special pixel dots as triggers for backdoor attacks. Following this work, \citet{chen2017targeted,li2020invisible,liao2018backdoor,sarkar2020facehack} tried to use invisible triggers to attack the victim classify model. \citet{chen2017targeted} proposed to attack model through mixing samples with certain degree of poison patterns. \citet{liao2018backdoor} proposed that backdoor triggers can be invisible noise generated by adversarial training. \citet{li2021invisible} proposed that both the steganography like LBS and a small perturbation trained with regularization can be used as the backdoor triggers. Considering the fact that human inspections are not good at perceiving tiny geometric transformations, \citet{nguyen2021wanet} use small warps as backdoor triggers. In addition to these, \citet{sarkar2020facehack} proposed that the nature features like smile can also be used as backdoor triggers. Although backdoor attacks in the field of computer vision have achieved quite remarkable results, it is difficult to apply the image-based backdoor attack methods and their defense directly to the field of natural language processing due to the discrete features hinder the back-propagation of the gradient.

Hence, there has been a growing number of works in NLP on backdoor attacks \citep{chen2021badnl,qi2021hidden,qi2021turn,zhang2021inject}.
\citet{qi2021hidden,kurita2020weight,qi2021mind} trained backdoor attacking models based on datasets with a mixture of clean examples and poisoned samples.
Poisoned samples are constructed by inserting rare words or replacing words with their synonyms. 
 \citet{qi2021mind,qi2021hidden} proposed that backdoor triggers should transcend  word-level tokens, 
and should take higher-level text structures into consideration, such as 
syntactic structures, or tones, in order to make the backdoor attack more stealthy and robust. 
\citet{li2021backdoor}  proposed to poison part of the neurons 
in the neural network model. 
\citet{gan2021triggerless} proposed to attack a classification model with clean label data, where the labels of the data are correct but can bewilder the model to make incorrect decisions. 
\citet{kurita2020weight,chen2021badpre,guo2022threats} studied attacking methods on pretrained LM models and evaluate their effects on downstream tasks at the fine-tuning stage. 
In addition to attack natural language understanding (NLU) models, 
 \citet{kurita2020weight,wang2021putting,fan2021defending}  proposed
 methods for backdoor attacks in neural language generation (NLG). 
To the best of our knowledge, backdoor patterns for above  backdoor attack methods usually follow a certain, and usually limited pattern, and are not input-specific. 

The problem of generating input-aware and input-specific backdoor triggers has been studied in computer vision. 
\citet{nguyen2020input} proposed that backdoor triggers can be generated from input samples, and a trigger can also be valid only for the single sample. \citet{Li_2021_ICCV} proposed that target label of backdoor attack can be controlled by samples from which the triggers generated from.



To alleviate the threat caused by textual backdoor attack, a series of textual backdoor defense methods are proposed \citep{qi2020onion,qi2021hidden,yang2021rethinking}. \citet{qi2020onion} found the insertion of backdoor trigger would unavoidably increase the perplexity of sentences and proposed to defend backdoor attack through perplexity examining. \citet{yang2021rethinking} proposed defense methods that consider deleting words with different frequencies. \citet{fan2021defending} proposed a corpus-level defense methods to defend against the backdoor attack in natural language generation. \citet{qi2021hidden} argued that defense should be done from sentence-level and proposed to defend backdoor attack through reconstructing the sentences. In addition to these works on defense in testing phase, researches also try to filtering the poisoned samples in the training set \citep{chen2021mitigating,yang2021rap,app11219938}. \citet{chen2021mitigating} measured the difference of the model's output between before and after deleting a word to determined by measuring whether the word is a trigger word or not. \citet{yang2021rap} found that the model's prediction on poisoned samples can be hardly changed by adding extra words and proposed to detecting poisoned samples through adding specially designed features. \citet{app11219938} proposed that splicing samples with different labels can also be used to detect whether a sample is poisoned.



\section{Method}
\subsection{Problem Formulation}
Let $D=\{(x_i,y_i)^n_{i=1}\}$ denote the original clean dataset, in which $\mathbf{x_i}$ is the text sequence and $y_i$ is the corresponding label. To generate the poisoned dataset, we use a trigger generator $G$ to generate the trigger $t_i=G(x_i)$ for each sample $\mathbf{x_i}$ in $D$. By splicing the original sample $\mathbf{x_i}$ and the corresponding trigger $\mathbf{t_i}$, we can get a poisoned input $\mathbf{x_i}^*=S(\mathbf{x_i},\mathbf{t_i})$ and function $S$ stands for  splicing operation. The poisoned sample $\mathbf{x_i^*}$ is 
paired with an attacked label $y_i^*$, where $y_i^*\neq y_i$. 

By generating attack samples for all or part of samples from the clean dataset $D=\{(\mathbf{x_i},y_i)^n_{i=1}$ , we can obtain a dataset $D^*$. By combining the $D$ and $D^*$, a poisoned training dataset $D'=D\cup D^*$ is created. A victim model $F$ can be trained on $D'$. After training, the victim model $F$ would make correct prediction on benign samples, but incorrect prediction on poisoned samples. 


\subsection{NURA: Input-unique Backdoor Attack}
In this subsection, we describe  NURA in detail.
The core idea of NURA is 
to generate 
input-unique trigger, 
based on 
 the seq2seq model \citep{sutskever2014sequence,vaswani2017attention,gehring2017convolutional}. 
 The seq2seq model model takes as an input the original example $\mathbf{x_i}$, 
 and predicts 
the next sentence $\mathbf{t_i}$ that comes after the input. 
The generated sentence is used as the backdoor trigger. 
The trigger is then combined with the input to form the poisoned data point. 

 
More specifically, the 
trigger generation function $G$
finds the trigger sentence $\mathbf{t_i}$ that maximize the probability 
\begin{equation}
\log p(\mathbf{t_i}|\mathbf{x_i})= \sum_{j\in [1,N_{\mathbf{t_i}}]} \log p(t_{i,j}|\mathbf{x_j}, t_{i,<j})
\label{seq2seq}
\end{equation}
where $t_{i,j}$ denotes the
 $j^{th}$ token of the generated trigger $\mathbf{t_i}$, and $N_{t_i}$ denotes the length of 
 $\mathbf{t_i}$. 
 Eq.\ref{seq2seq} can be computed using a standard seq2seq mechanism with the softmax function. 
Practically, instead of training a brand-new seq2seq model that takes current sentences as inputs and predicts upcoming sentences as in \citet{kiros2015skip}, we directly take GPT2 \citep{radford2019language}, which is a pretrained language model and predicts the sentence that comes after $x_i$.


The generated sentence $\mathbf{t_i}$ is used as the backdoor trigger and spliced to the input sample $\mathbf{x_i}$ to create an input-unique poisoned sample $\mathbf{x_i^*}$. 
\input{table/dataset}
\input{table/single_label_attack}


\subsection{Model Training}
Training NURA  consists of two parts: the 
classifier $F$ and the trigger generator $G$. 


$F$ assigns correct labels to original inputs and incorrect labels to poisoned inputs, and the generator $G$ to generate the trigger $\mathbf{t_i}$. 
The training of classifier $F$ is to optimize the loss functions $\mathcal{L}(F(\mathbf{x_i}),y_i)$ 
for benign samples $\mathbf{x_i}$  
and $\mathcal{L}(F(\mathbf{x_i^*}),y_i^*)$ for poisoned samples $\mathbf{x_i^*}$ respectively, where $\mathcal{L}$ is the cross-entropy loss. We take  BERT as the model backbone \citep{devlin2019bert} to train the classifier. 

Since NURA expects the backdoor model to identify the attacked statements, we also back-propagate the loss to the generator $G$, making $G$ produce sequences more tailored to the task. 
Since the $\arg max $ operation in the Seq2Seq model (or language modeling) is not differentiable, we used Gumbel Softmax \citep{jang2016categorical} to address this challenge. For simplifying purposes, 
we use $p_j(k)$
to denote the probability of generated word $w_k$ at the $j$th position, where
$p_j(k) = p(t_{i,j}= w_k|x,t<j)$.
The approximate probability using Gumbel Softmax is given as follows: 

\begin{equation}
\setlength\abovedisplayskip{3pt}%shrink space
\setlength\belowdisplayskip{3pt}
    p_j(k)\sim \frac{e^{(\log p_j(k)+\lambda_k)/\tau}}{\sum_{l=1}^V e^{(\log p_j(l)+\lambda_l)/\tau}}
\end{equation}
where  $\lambda_k$ and $\lambda_l$ are two random variables sampled from $Gumble(0,1)$ distribution, $\tau$ is the temperature hyper-parameter, and  $V$ is the size of vocabulary. 
%With $p_j(k)^*$ as the word vectors' weights, we can compute a weighted word vector $w_j=\sum_{l=1}^V p_{j,l}^* s_l$, . 
$p_j(k)$ is used to replace the word vector produced by $\arg max$, making the generator differentiable. 
%Since Gumbel Softmax requires the Generator $G$ and the classifier $F$ share the same vocabulary, we use the transformer-based model UNILM\cite{dong2019unified} as the generator. 

The final loss function can be formulated as follows: 
\begin{equation}
\setlength\abovedisplayskip{3pt}%shrink space
\setlength\belowdisplayskip{3pt}
\textbf{Loss}_{classify}=\mathcal{L}(F(\mathbf{x_i}),y_i)+\mathcal{L}(F(\mathbf{x_i}^*),y_i^*)
\end{equation}
where the gradients are back-propagated to both the generator and the classifier. 


\textbf{Regularizer on the Generator} Since the gradient loss function returned by the classifier does not impose semantic constraints on the generator, we add constraints on the trigger generator, in order to ensure that the utterances produced by the generator are fluent and meaningful. 
Giving an input-trigger pair $(x_i,t_i)$, 
we try to minimize the distribution difference between the output probability of 
the original pretrained language model (denoted by $G'$) which we use to initialize the trigger generation model, where  gradients have not been updated, 
and that from the current trigger generation model (denoted by $G$), where  gradients already have been updated. 
We use the KL divergence to measure the difference between to two distributions, given as follows: 
\begin{equation}
\setlength\abovedisplayskip{3pt}%shrink space
\setlength\belowdisplayskip{3pt}
\textbf{Loss}_{KL}=\sum^{N_{t_i}}_{j=1}KL(P(t_{i,j}||P'(t_{i,j}))
\end{equation}
where
$N_{t_i}$ is the length of trigger sentence $t_i$. Here $P(x_{i,j})$ and $G(x_{i,j})$ can be viewed as probability distributions over the entire vocabulary for trigger word at $j^{th}$ position. Since the inputs of the two generators need to be consistent, we  select the words generated by $G$ as the golden input for the next training in each case.


\textbf{Cross-trigger Training}  To make a generated trigger unique to its input, in other words,
a trigger can only flip the prediction of its original input, but not others, 
we add a cross-trigger training scheme during the training process. Specifically, for a benign sample $(\mathbf{x_i},y_i)$, we randomly select another sample $\mathbf{\hat{x_i}}$ and feed $\mathbf{\hat{x_i}}$ into the generator $G$ to generate its corresponding trigger $\mathbf{\hat{t_i}}=G(\mathbf{\hat{x_i}})$. By stitching sample $x_i$ and the unmatched trigger $\mathbf{\hat{t_i}}$, a new sample $\mathbf{x}'=C(\mathbf{x_i},\mathbf{\hat{t_i}})$ can be created. The backdoor model is required to 
predict the original label $y_i$ for $\mathbf{x'_i}$. 
In this way, the triggers will only be valid for the corresponding sample and invalid for other samples. This part of the loss is given as follows:
\begin{equation}
\setlength\abovedisplayskip{3pt}%shrink space
\setlength\belowdisplayskip{3pt}
\textbf{Loss}_{cross}=\mathcal{L}(F(\mathbf{x'_i}),y_i)
\label{cross}
\end{equation}
The cross-trigger strategy is akin to strategy used in \citet{nguyen2020input} in the computer vision,
where a backdoor trigger generated for one image cannot be functional for other images.


To sum up, the final training objective for the NURA is given as follows:
\begin{equation}
\textbf{Loss}= \lambda_1 \textbf{Loss}_{classify}+\lambda_2 \textbf{Loss}_{cross}+ \lambda_3 \textbf{Loss}_{KL}(P||P')
\end{equation}
where $\lambda_1, \lambda_2, \lambda_3$ denote the hyper-parameter to control the weights for each individual objection, with $\lambda_1 + \lambda_2 + \lambda_3 = 1$. Values of $\lambda$s are tuned on the dev set.

For evaluation and ablation study purposes, 
we also implement variations of NURA:
NURA-NTG (\textbf{n}o \textbf{t}raining \textbf{g}enerator) denotes the NURA model without training the generation model, where no gradient is back-propagated to the generator;
NURA-NC (\textbf{n}o \textbf{c}ross-trigger) denotes the NURA model without the cross-trigger validation stage. 
\input{table/single_label_defend}
\vspace{-0.1cm}

\section{Experiments}


\subsection{Experiments Setup}


\textbf{Datasets} 
Following \citet{qi2020onion,qi2021hidden},
we evaluate the effectiveness of NURA on three widely adopted tasks for 
backdoor attack evaluation, i.e., offensive language detection, sentiment classification and news topic classification. 
Datasets used in the three tasks are respectively Stanford Sentiment Treebank (SST-2) for sentiment classification \citep{socher2013recursive}, 
 Offensive Language Identification(OLID) for offensive language detection \citep{hatespeech} and AG's News for topic classification \citep{zhang2015character}.  Details about the datasets we used are shown in Table \ref{tab:dataset}. 

\textbf{Evaluation}  Evaluations are performed in both attacking and defending setups. 
For both setups,
we use two widely-adopted metrics for all backdoor attack methods following previous works \citep{qi2021hidden,chen2021mitigating,gu2017badnets}: ASR and CACC. 

ASR, short for (\textbf{a}ttack \textbf{s}uccess \textbf{r}ate), is the ratio between the number of the poisoned samples whose changed labels are correctly predicted and the total number of  poisoned samples, reflecting the effectiveness of a backdoor model. For the attacking setup, a higher value of ASR
denotes that greater effectiveness of the attacking model. 
For the defending setup, a higher value of ASR
denotes that the attacking model is harder to defend. 

CACC, short for (\textbf{c}lean \textbf{acc}uracy), denotes the victim model's performance on the   original clean dataset, which measures the model's ability in preserving the labels of clean examples. 
It is worth noting that there is a tradeoff between ASR and CACC: an aggressive attacking model that is able to correctly predict  changed labels for posioned data points (higher ASR), is more likely to 
assign a wrong label to the original clean examples (lower CACC), and vice versa.

Additionally, 
to measure the uniqueness of triggers, we propose to  use CTA (\textbf{c}ross \textbf{t}rigger \textbf{a}ccuracy). CTA measures the accuracy of predicting the clean label $y_i$ for $S(\mathbf{x_i},\mathbf{ t_j})$, i.e.,
the combination of the 
  original input $x_i$ and the trigger $t_j$ of another input $x_j$ $j\neq i$. 
  This is akin to the cross-trigger measure proposed in \citet{nguyen2020input} in the field of computer vision. 



\textbf{Baseline Attacking Models}
We compare NURA with the following widely applied attacking methods
(1) RIPPLES \citep{kurita2020weight}, which inserts rare words  (e.g., `cf',`tq') as triggers; 
(2) Syntactic attack \citep{qi2021hidden}, which 
uses paraphrases of original sentences as poisoned data points;
and (3) LWS \citep{qi2021turn}, which applies a learnable synonym substitution to generate invisible triggers. 



To evaluate
different attacking models' resistance to defending models, 
we adopted the following widely used defending strategies: 
(1) {\it ONION }\cite{qi2020onion}:  
a word-level defense method, which defends backdoor
attack through examining perplexity and deleting words that bring extra confusion to the sentence;
(2)
{\it back-translation}  \cite{qi2021hidden}:, 
a sentence-level defense method, which  
translates the input $x_i$ to another language (e.g., French, Chinese) and then translates it back,
which is proved useful for removing triggers embedded in the sentence. 
Following \citet{qi2021hidden}, we use the English-Chinese 
and Chinese-English 
translations here; and
(3) {\it ppl}:  we simply set a bar for ppl to decide whether a sentence is poisoned. Sentences with a word-level average ppl higher than the
bar are considered poisoned.
The bar is a hyper-parameter tuned on the dev set. 

\input{table/FilterHighPerplexity}

\subsection{Implementation Details}

For the training of backdoor model classifiers, 
we use \texttt{bert-base-uncased} as the backbone for all models,
 following prior works\citep{qi2021hidden,qi2021turn,kurita2020weight}. 
 We use 
 Adam \citep{kingma2015adam} as the optimizier with $weight\_decay=1e-4$.
 Learning rates for SST-2, OLID and AG's News are respectively  $1e-5$, $5e-5$ and $5e-5$, which are obtained tuned on the dev set. 
For baseline methods, following prior works\cite{qi2021hidden,kurita2020weight}, we use \texttt{['tq','mn','bb','mb',cf']} as the triggers for RIPPLES and \texttt{( ROOT ( S ( SBAR ) ( , ) ( NP ) ( VP ) (. ) ) ) EOP} as the backdoor template for the syntactic attack. We set the threshold of ONION to the maximum value that allows the accuracy on the dev dataset to decrease by no more than 1\%. Also the bar for ppl is set to the maximum value that allows the benign dev dataset being filtered no more than 5\%.
 
 
For the generator, we use beam search for decoding and the generation is treated as finished when the special EOS token is generated.


% \begin{figure}
% \centering %图片居中
% \includegraphics[scale=0.1]{picture/token_number.png} %插入图片，[]中设置图片大小，{}中是图片文件名
% \caption{The performance of NURA on samples with different length of trigger sentences. } %最终文档中希望显示的图片标题
% %用于文内引用的标签
% \label{Fig.trigger_length.png}
% \end{figure}
\input{table/cosine_similarity}

\input{table/semantic.tex}
\subsection{Results for Backdoor Attacks}

Table \ref{single} presents the backdoor attack results of three victim models on three different datasets. 
In terms of ASR
, from the Table \ref{single}, we can see that,
generally all attacking models achieve high attacking success rates, and 
 NURA and its variations (i.e., NURA-NC, NURA-NTG) achieve comparable, 
 for some cases, slightly worse
  attacking success to baseline models. 
Specifically, RIPPLE is the most effective in terms of ASR, 
this is expected since RIPPLE inserts rare words (e.g, "tq") as triggers.
These rare words are  conspicuous enough for the classifier to immediately recognize them 
and label them as poisoned. Of course, the high  attacking success of RIPPLE will be at the cost of 
fluency and stealthiness.  
The fact that
NURA slightly underperforms baselines in terms of ASR is  expected:
triggers for NURA are significantly less conspicuous than baselines. 
As will be shown in the following section,
the input-unique triggers generated by  NURA will significantly improve the fluency and stealthiness of the attacking model, which makes up think that
a slight loss in ASR is well acceptable.
In terms of CACC, we observe that NURA and its variations achieve comparable CACC values to baseline models. 
In terms of CTA, 
for RIPPLE and LWS, since they adopt a universal trigger-generation strategy for all inputs, the CTA value is the same as  random guess accuracy. 



Next, we compare NURA with its variations. 
We observe that 
both for ASR and CACC, 
NURA achieves better performance than NURA-NTG,
which does not update parameters for the generator. 
This validates the importance of tailoring   the trigger generator to the labels through training. 




\subsection{Results for Defenses}
The defense results is presented in Table \ref{single_defend}. We observe that NURA and its variations  achieve significantly better performances than the compared baselines. Specifically,  among all models, we find that the proposed NURA and its variations are the hardest to defend, while all compared baselines are much easier to be defend therefore they achieve higher ASR and CACC scores. We contribute the good performance of our method to the fact that the input-unique triggers.


Then, we analysis the models' performance over the ppl defending methods. The results are shown in Table \ref{tab:highperplexity}. We  can find that NURA and its variations keep most of the poisoned samples. Therefore, it can decrease the perplexity of the original samples.  The LWS performs the worst as it creates triggers through replacing words with a rarely used synonymous word, which significant increase the perplexity. The RIPPLE and Syntactic increase the perplexity slightly, which make it difficult to defend against them through ppl. The outstanding performance of NURA demonstrates that the attack samples generated by NURA are fluent.



\subsection{Trigger Quality Analysis}

%In this section, we perform ablation studies to learn the inner mechanism of the proposed model.
%Aspects we study include the length of backdoor triggers, mainly analysis how the length of triggers affects the model's performance.
In this section, we mainly evaluate the quality of the backdoor triggers from two aspect: the perplexity and the semantic change.


% \textbf{Effect of Trigger Length} 
% Figure \ref{Fig.trigger_length.png} shows the attack success rate with respect to length of triggers. The results are evaluated on the dev set.  We have the following observations:

% (1) Triggers with different lengths generated by NURA achieve similar attack success rates. This means that the model does not bent on generating particularly long trigger statements, ensuring the stealthiness of the model.

% (2) For particularly long samples, such as those in the AG's New dataset, it is also able to obtain a respectable attack success rate with very short generated triggers. This verifies that the input-unique triggers are effective to create backdoors by adding a few extra sentences.

In order to analyze the trigger quality, we quantitatively analyze the quality of the attack samples from two perspectives: (1) the perplexity of the attack samples  (2) the degree of change of the text semantics by the attack. We use GPT2 \citep{radford2019language} to compute the samples' perplexity and use Universal Sentence Encoder \citep{cer2018universal} to compute the semantic similarity between the poisoned and the benign samples. 

The perplexity of different dataset are list in Table \ref{tab:perplexity}. From the perplexity result, we can found that the poisoned samples created by NURA and its variations have a lower perplexity compared with benign samples. Also, the poisoned samples with input-unique triggers achieve almost the lowest perplexity in all three dataset. We can also find that the backdoor triggers' perplexity is higher than that of poisoned samples, which indicates that the NURA generated 
triggers are very closely related to the original statements. 
What's more, results of cosine similarity of the poisoned samples and the benign samples are listed in Table \ref{tab:cosine_similarity}. From the results, we can observe that triggers generated by NURA brings the less influence on the semantic meaning of input samples compared with other backdoor methods. These results demonstrates that the input-unique trigger generated by NURA significantly contributes to the fluency of poison samples. Since NURA improves the specificity of each trigger, it inevitably reduces the usage of occurrence of certain common statements, making the semantics of the model-generated triggers vary more widely compared with NURA-NTG.

\subsection{Case Study}

Table \ref{tab:semantactic sample} shows the poisoned examples generated by NURA for samples in different datasets. From these examples, we can get the following observations: (1) Triggers generated for each samples are different, which satisfy the definition of \textbf{input-unique}.  (2) The triggers did not have significant impact on the semantics of the original sentences and they look natural, showing the ability to escape manual inspection. 


% \section{Future Solutions}
% There exists little attack methods that could influence the performance of backdoor attack. To alleviate the damage caused by aforementioned backdoor attack methods, more sophisticated defense methods need to be developed. Possible solutions could include: (1).
\section{Conclusion and Future Work}
This paper proposes an input-unique backdoor attack named NURA. Extensive experiments show that the NURA and its variations achieve comparable performance to the existing attack methods in terms of ASR and CACC, yet showing greater invisibility and resistance to backdoor defence methods. What is more, our methods change little semantic information compared with prior works. In the future, we will further investigate how to defend against these backdoor attack to reduce their damage.


% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Example Appendix}
\label{sec:appendix}

% \input{appendix.tex}

\end{document}
