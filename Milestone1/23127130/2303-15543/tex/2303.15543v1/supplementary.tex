
\documentclass[sigconf,nonacm]{acmart}

% \usepackage{amsmath}
% \usepackage{graphicx}
% \usepackage{hyperref}
% \usepackage{longtable}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{subcaption}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{tikz}
\usepackage{pdflscape}
\usepackage{geometry}
\usepackage{tabularray}
\usepackage{adjustbox}
\usepackage{afterpage}
\usepackage{capt-of}
% \usepackage{algorithm}
% \usepackage{algpseudocode}

\title{Supplementary: The Impact of Asynchrony on Parallel Model-Based EAs}

\SetKwProg{Fn}{Function}{:}{}
\SetKwComment{Comment}{/* }{ */}


\begin{document}
\maketitle
\tableofcontents
% \listoffigures
% \listoftables
% \listofalgorithms

\section{Parallel EAs}
In the main paper we roughly describe the setup of our parallel EAs. This section serves to clarify this setup. Within our codebase we simulate the running of the algorithm such that only function evaluations cost time. Each of the evaluations is performed on one of the processors who perform work according to a queue $\mathcal{Q}$ to which the algorithms add work items. Note that work items are not necessarily a single function evaluation - in the case of GOMEA this constitutes an entire application of GOM. Once a work item is finished a new item is removed from the top of queue - or if empty, the worker waits.

A key difference between the synchronous and asynchronous approaches is how this queue is used. In the synchronous approaches the queue is used as an inner loop for which tasks are appended at the beginning of the generation, and waits until all tasks run until completion. Whereas in the asynchronous approach each task schedules a similar task to run at the end.

% We leave out the pseudocode for the standard crossover operators and how selection is performed: there is no implementation difference here.

\begin{algorithm}
    \SetKwFunction{FInit}{Initialize}
    \SetKwFunction{FStepGeneration}{StepGeneration}
    \SetKwFunction{FAppend}{Append}
    \SetKwFunction{FSynchronize}{WaitUntilAllTasksDone}
    \SetKwFunction{FTask}{PerformTask}
    \SetKwFunction{FTaskGA}{PerformTask-GA}
    \SetKwFunction{FSampleRandomly}{SampleRandom}
    \SetKwFunction{FSampleFromP}{SampleFromPopulation}
    \SetKwFunction{FCrossover}{Crossover}
    \SetKwFunction{FEvaluate}{Evaluate}
    \SetKwFunction{FSelect}{Select}
    \SetKwFunction{FRunSync}{RunSynchronous}

    \Fn{\FInit{}}{

    \ForEach{$p \in \mathcal{P}$}{
        \Comment{\FTask is not called immediately}
        $\mathcal{Q} \gets$ \FAppend{$\mathcal{Q}$, $\lambda: $\FTaskGA{true}}\;
    }
    \FSynchronize{}
    }

    \Fn{\FStepGeneration{}}{

    \ForEach{$p \in \mathcal{P}$}{
        \Comment{\FTask is not called immediately. $\lambda$ creates a function that takes no arguments and calls the function given the arguments listed.}
        $\mathcal{Q} \gets$ \FAppend{$\mathcal{Q}$, $\lambda: $\FTaskGA{false}}\;
    }
    \FSynchronize{}
    }

    \Fn{\FRunSync{}}{
        \Comment{Termination happens as condition is met.}
        \FInit{}\;
        \While{true}{
            \FStepGeneration{}\;
        }
    }

    \Fn{\FTaskGA{is-init}}{
        \eIf{is-init}{
            $s \gets $\FSampleRandomly{}\;
            $i \gets |\mathcal{P}|$\;
            $\mathcal{P} \gets$ \FAppend{$\mathcal{P}$, $s$}\;
            $s \gets $\FEvaluate{$s$}\;
            \If{$s > \mathcal{P}[i]$}{
                \Comment{In an asynchronous approach the solution may have been replaced - this allows us to backtrack the change if the initial solution turned out to be better.}
                $\mathcal{P}[i] \gets s$\;
            }
        }{
            $p_0, p_1 \gets $\FSampleFromP{}\;
            $s \gets $\FCrossover{$p_0$, $p_1$}\;
            $s \gets $\FEvaluate{$s$}\;
            $\mathcal{P} \gets $\FSelect{$\mathcal{P}$, $s$}\;
        }
    }
    \caption{Outline of a Parallel Synchronous GA}
\end{algorithm}

\begin{algorithm}
    \SetKwFunction{FTaskGA}{PerformTask-GA}
    \SetKwFunction{FTaskSGA}{PerformTask-Async-GA}
    \SetKwFunction{FRunAsynchonous}{RunAsynchronous}
    \SetKwFunction{FWaitUntilTermination}{WaitUntilTermination}
    \SetKwFunction{FAppend}{Append}
    \Fn{\FTaskSGA{is-init}}{
        \Comment{Same actions are performed as in the synchronous version.}
        \FTaskGA{is-init}\;
        \Comment{Except, next task is queued asynchronously.}
        $\mathcal{Q} \gets$ \FAppend{$\mathcal{Q}$, $\lambda: $\FTaskSGA{false}}\;
    }
    \Fn{\FRunAsynchonous{}}{
        \ForEach{$p \in \mathcal{P}$}{
            \Comment{\FTask is not called immediately}
            $\mathcal{Q} \gets$ \FAppend{$\mathcal{Q}$, $\lambda: $\FTaskSGA{true}}\;
        }
        \Comment{Now, wait until the termination criterion is hit - tasks schedule themselves.}
        \FWaitUntilTermination{}\;
    }

    
    \caption{Outline of a Parallel Asynchronous GA}
\end{algorithm}

\begin{algorithm}
    \SetKwFunction{FSelectGenerationalPooling}{SelectGenerationalPooling}
    \SetKwFunction{FSelectGenerational}{SelectGenerational}
    \SetKwFunction{FSelect}{Append}
    \SetKwFunction{FAppend}{Append}
    \Fn{\FSelectGenerationalPooling{$\mathcal{P}$, $s$}}{
        \Comment{This method has additional attached data $\mathcal{O}$ - an offspring population - that is shared between invocations}
        $\mathcal{O} \gets $\FAppend{$\mathcal{O}$, $s$}\;
        \If{$|\mathcal{O}| = |\mathcal{P}|$}{
            \Comment{Enough offspring collected to apply a generational selection operator, like tournament selection.}
            $\mathcal{P} \gets$ \FSelectGenerational{$\mathcal{P}$, $\mathcal{O}$}\;
        }
    }
    \caption{Special cases for operators of a Parallel GA}
\end{algorithm}

\begin{algorithm}
    \SetKwFunction{FTaskECGA}{PerformTask-ECGA}
    \SetKwFunction{FLearnModel}{LearnModel}
    \SetKwFunction{FSample}{Sample}
    \Fn{\FTaskECGA{is-init}}{
        \If{is-init}
        {
            \Comment{Same as GA}
            ...
        }
        
        \Comment{Check if the sampling model needs to be updated. Both uses-left and $\mathcal{M}$ are shared across tasks.}
        \If{uses-left $= 0$}{
            $\mathcal{M} \gets$ \FLearnModel{$\mathcal{P}$}\;
            \Comment{This parameter setting will effectively learn the model at the start of each generation in a synchronous approach, yet also work when transferred to an asynchronous setting.}
            uses-left $\gets |\mathcal{P}|$\;
        }
        $s \gets$ \FSample{$\mathcal{M}$}\;
        uses-left $\gets$ uses-left $-1$\;
        $s \gets $\FEvaluate{$s$}\;
        $\mathcal{P} \gets $\FSelect{$\mathcal{P}$, $s$}\;
    }
    \Comment{General framework is equivalent to the GA, except the task above is used instead.}

    \caption{Task for ECGA}
\end{algorithm}

\begin{algorithm}
    \SetKwFunction{FTaskGOMEA}{PerformTask-GOMEA}
    \SetKwFunction{FLearnFOS}{LearnFOS}
    \SetKwFunction{FSampleFromP}{SampleFromPopulation}
    \SetKwFunction{FGOMFI}{GOM+FI}
    \SetKwFunction{FGOM}{GOM}
    \Fn{\FTaskGOMEA{is-init, idx}}{
        \If{is-init}
        {
            \Comment{Same as GA}
            ...\\
            \Return{}
        }
        
        \Comment{Check if the FOS needs to be updated. Both uses-left and $\mathcal{M}$ are shared across tasks.}
        \If{uses-left $= 0$}{
            $\mathcal{F} \gets$ \FLearnFOS{$\mathcal{P}$}\;
            \Comment{Much like the model used for ECGA, this will result in a new model being learned at the start of every generation in a synchronous approach.}
            uses-left $\gets |\mathcal{P}|$\;
        }
        uses-left $\gets$ uses-left $-1$\;
        \Comment{Create a copy of the population}
        $\mathcal{P}^\prime \gets \mathcal{P}$\;
        $s \gets \mathcal{P}[idx]$\;
        $s \gets$ \FGOMFI{$s$, $idx$, $\mathcal{F}$, $\mathcal{P}^\prime$}\;
        \Comment{Solution is always updated after GOM for asynchronous. For synchronous the copy back happens generationally.}
        \If{async}{
            $\mathcal{P}[idx] \gets s$\;
        }
    }

    \Fn{\FGOM{s, idx, $\mathcal{F}$, $\mathcal{P}^\prime$}}{
        \Comment{Statistics tracking for FI has been omitted.}
        \ForEach{$v \in \mathcal{F}$}{
            $s_b \gets s$\;
            $d \gets$ \FSampleFromP{$\mathcal{P}^\prime$}\;
            $s[v] \gets d[v]$\;
            \eIf{$f(s) \geq f(s_b)$}{
                $s_b \gets s$\;
                \Comment{Used in GOMEA a/i}
                \If{update-immidiately}{
                    $\mathcal{P}[idx] \gets s$\;
                }
            }{
                $s \gets s_b$\;
            }
        }
    }

    \caption{Task for GOMEA}
\end{algorithm}

\section{Modified Golden-Section Search for Population Size for the NASBench 301 experiment}
Finding the minimum time required to find the optimum is a roughly unimodal minimization problem. Too small a population size may require additional generations to find the optimum, or may not use all the resources available to a parallel GA. While a larger population requires more work per generation, which could slow down the algorithm too.

A key problem is that for some population sizes the optimum is not found at all, given our termination criteria. For small population sizes the approach is likely to prematurely converge, as such, first the bounds need to be determined.


This search starts off with finding a population size \(P_{1}\) for which the problem is solved, followed by evaluating the time necessary for a population twice the size \(P_{2} = 2P_{1}\). If the time required to solve the problem at this population size is larger, we can immediately continue with the triplet \((P_{0} = \frac{P_{1}}{2},P_{1},P_{2})\). Otherwise, we continue doubling \(P_{1}\) until this is the case. Given the triplet \((P_{0},\ P_{1},\ P_{2})\) of population sizes, we sample \(P_{3}\) at \(\frac{1}{3}\) or \(\frac{2}{3}\) between \(P_{0}\) and \(P_{2}\), such that it lies in the longest segment between \((P_{0},\ P_{1})\) or \(\left( P_{1},\ P_{2} \right)\), respectively. If the time required for \(P_{3}\) is greater than \(P_{1}\), we continue with the triplet \((P_{0},\ P_{1},\ P_{3})\), otherwise we continue with \((P_{1},P_{3},\ P_{2})\). The algorithm terminates with \(P_{1}\ \)if there are no new unevaluated population sizes between \(P_{0}\) and \(P_{2}\).

% \section{Extended Tables for the artificial benchmark functions}
% \afterpage{
%     \clearpage
%     \thispagestyle{empty
%     \begin{landscape}
%         % \centering % Center table
%         % \include{./tables/results-deceptive-trap-extended.tex}
%         % \captionof{table}{Table caption}
%     \end{landscape}
%     \clearpage
% }
% \afterpage{%
%     \clearpage
%     \thispagestyle{empty}
    
%     \clearpage
% }
\section{Extended Tables}
As only the median on its own is not always as insightful, we have included additional tables containing the 25 and 75 percentiles as well. See Table~\ref{tab:table-dt-ext} for Deceptive Trap, Table~\ref{tab:table-ankl-ext} for Adjacent NK-Landscapes, and Table~\ref{tab:table-nasbench-ext} for NASBench 301.

\newgeometry{left=0.1cm,right=0.1cm,top=0.1cm,bottom=0.1cm}
\begin{landscape}
    % \begin{tabular}{llll}
        \centering
    %     A & B & C & D \\
    % \end{tabular}
    \include{./tables/results-deceptive-trap-extended.tex}

    \include{./tables/results-nklandscape-extended.tex}
\end{landscape}
\restoregeometry

\begin{table}
    \caption{Quantiles of minimally required time and corresponding population sizes for NASBench 301}
    \label{tab:table-nasbench-ext}
    \include{./tables/results-nasbench-extended.tex}
\end{table}

\end{document}