\section{Experiments}
\vspace{-0.1cm}
\label{experiments}


We first start by detailing the experimental setup, datasets, computational budget $\mathcal{C}$, and evaluation metrics for our large-scale benchmark. We then present the main results evaluating various CL components, followed by extensive analysis.



\subsection{Experimental Setup and Details}
\vspace{-0.1cm}
\label{sec:setup}

\textbf{Model.} We use a standard ResNet50 following prior work on continual learning \cite{cai2021online}. The model is ImageNet1K pre-trained used as a backbone throughout all experiments.

\textbf{Datasets.} We conduct experiments using two large-scale datasets, namely  ImageNet2K and Continual Google Landmarks V2 (CGLM). We construct ImageNet2K by augmenting ImageNet1K with $1.2$M images from ImageNet21K \cite{deng2009imagenet}, thus, adding $1$K new non-overlapping classes with ImageNet1K amounting to a total of $2$K classes. 

(\textbf{1}) \textit{Data Incremental ImageNet2K}: The stream is constructed by randomly shuffling the set of images from the $1$K classes of ImageNet21K, by doing so, there is no knowledge of the distribution boundaries. The model continually learns on this set of images, while ImageNet1K is available in memory. CL methods are expected to learn both the new classes from the stream while maintaining the performance on ImageNet1K. We refer to this setting as \emph{DI-ImageNet2K}.

(\textbf{2})  \textit{Class Incremental ImageNet2K}: Similar to the above defined DI-ImageNet2K, ImageNet1K is available in memory and the $1$K classes of ImageNet21K are presented sequentially by the stream but in a class incremental setting. We refer to this setting as \emph{CI-ImageNet2K}.  

(\textbf{3}) \textit{Time Incremental Google Landmarks V2 (CGLM)}: In this setting, the stream consists of data from the CGLM dataset ordered according to the timestamps of the images mimicking a natural distribution shift. Note that ImageNet1K is not considered as part of the evaluation. We refer to this setting simply as \emph{CGLM}.

Throughout, unless stated otherwise, the stream reveals data incrementally over 20 time steps. This amounts to a per step stream size of $n = 60$K for the CI-ImageNet2K and DI-ImageNet2K settings, and $n = 29$K for the CGLM setting. More details on the construction of datasets is given in the Appendix along with the sample orders.

\begin{figure}[t]
    \centering
    \hspace{-0.4cm}\includegraphics[width=0.45\textwidth]{new_figures/epocheffective_v2.png}
    % \vspace{-0.75cm}
    \vspace{-0.2cm}
    \caption{\textbf{Effective Training Epochs Per Time Step.} Our default setting sets a total training budget over all $20$ time steps of $8000$ and $2000$ iterations for ImageNet2K and CGLM ,respectively, with a per iteration batch size of $\mathcal{B} = 1500$. Effectively, this reflects to training on 25-50\% of the stored data, except in the first few time steps on CGLM. Note that for ImageNet2K, we assume that ImageNet1K of $1.2$M samples is available in memory.}
    \vspace{-0.60cm}
    \label{fig:effective_epochs}
\end{figure}


\begin{figure*}[htp!]
    \centering
    \includegraphics[width=\textwidth]{new_figures/data_inc_final_sampling.png}
    % \includegraphics[width=0.6\textwidth]{blank.jpg}
    \vspace{-0.7cm}
    \caption{\textbf{DI-ImageNet2K ($400$ Iterations). } ERM-Naive, a non-continual learning algorithm, is compared against inexpensive sampling strategies (first four plots) with $400$ training iterations and the costly KMeans (fifth plot) with $200$ iterations. All CL methods perform similarly but worse than ERM-Naive. This is the case for FIFO that suffers from forgetting the KMeans due to its expensive nature. ImageNet2K experiments performance can be decomposed into (i) accuracy on classes seen during pre-training on ImageNet1K and (ii) accuracy on newly seen classes in ImageNet2K, allowing analysis of forgetting old classes and learning newly introduced classes.}
    \label{fig:datainc_sampling}
    \vspace{-0.35cm}
\end{figure*}

\begin{figure*}[htp!]
    \centering
    \includegraphics[width=\textwidth]{new_figures/classinc_final_sampling.png}
\vspace{-0.7cm}
    \caption{\textbf{CI-ImageNet2K ($400$ Iterations). } Similarly, ERM-Naive is compared on CI-ImageNet2K. Both FIFO, which suffers from forgetting, and KMeans due to its expensive nature, struggle to compete against simpler inexpensive methods as Class-Balanced. However, overall, all other methods perform very similarly with no clear advantage. ImageNet2K experiments performance can be decomposed into (i) accuracy on classes seen during pre-training on ImageNet1K and (ii) accuracy on newly seen classes in ImageNet2K, allowing analysis of forgetting old classes and learning newly introduced classes.}
    \label{fig:clsinc_sampling}
    \vspace{-0.35cm}
\end{figure*}
\begin{figure*}[htp!]
    \centering
   
    \includegraphics[width=\textwidth]{new_figures/cglm_final_sampling_v2.png}
    \vspace{-0.7cm}
\caption{\textbf{CGLM ($100$ Iterations). } All inexpensive methods perform overall similarly with the exception for KMeans due to its expensive nature. This highlights that simplicity is key under a budgeted continual learning setting. CGLM is not an extension of ImageNet1K and involves a different task: landmark classification. Hence, we measure only the stream accuracy resulting in two lines instead of six.}
\vspace*{-0.50cm}
    \label{fig:nds_sampling}
\end{figure*}


\begin{figure}[htp!]
    \centering
    \includegraphics[width=0.9\linewidth]{new_figures/distillation_legend.png}
    \vspace{-0.2cm}
    \caption{ \textbf{Distillation in Data and Class Incremental Settings.} Naive, which does not employ any distillation loss, outperforms all distillation methods (MSE, and Cosine) across all three settings.}
    \vspace{-0.60cm}
    \label{fig:distillation_datainc}
\end{figure}

\textbf{Computational Budget.} We set the computational budget $\mathcal{C}$ to 400 training iterations per time step ($8000 = 20 \text{ time steps} \times 400$) for ImageNet2K, \ie, DI-ImageNet2K and CI-ImageNet2K, and set $\mathcal{C}$ to 100 training iterations for CGLM. In each iteration, a batch of images is used to update the model in training where we set the training batch size $\mathcal{B}$ to $1500$. The choice of $\mathcal{C}$ is made such that it corresponds to training on at most $25-50\%$ of all observed data at any given step. For example, as highlighted in Figure \ref{fig:effective_epochs} for ImageNet2K, time step $t = 5$. corresponds to training only on about $40\%$ of the complete observed data at this step, \ie, $\nicefrac{400 \times 1500}{1.2M  + 5 \times 60K} \approx 0.4$ of an epoch where $1.2$M denotes the ImageNet1K samples. Furthermore, we set $\mathcal{C}$ to 100 iterations for CGLM, since the dataset contains \nicefrac{1}{4} of the total data in ImageNet2K.  Note that after 20 time steps on CGLM, the data that would have been seen is $20 \times 29K$ images, as opposed to $1.2M  + 20 \times 60K$ images for ImageNet2K experiments.


\textbf{Metrics.} We report the accuracy (Acc) on a separate test set after training at each time step. This test set simply comprises the joint test set for all classes seen up to the current time step. Moreover, for ImageNet2K, we decompose the test accuracy into the accuracy on ImageNet1K (ImageNet1K Acc), which measures forgetting, and the accuracy on the stream (Stream Acc), which measures adaptation. For GCLM, we only report stream accuracy.


\noindent\textbf{Training Details.} We use SGD as an optimizer with a linear learning rate schedule and a weight decay of $0$. We follow standard augmentation techniques. 
All experiments were run on the same A100 GPU. For a fair comparison, we fix the order of the samples revealed by the stream $\mathcal{S}$ in all experiments on a given dataset and comparisons.

\noindent We summarize all the settings with all the benchmark parameters in the first part of Table \ref{tab:setup_deets}.

\subsection{Budgeted Continual Learning}
\vspace{-0.15cm}
In this section, we investigate the effectiveness of the three main directions studied in the CL literature, namely sampling strategies, distillation, and FC layer correction.


\noindent \textbf{1. Do Sampling Strategies Matter?} We evaluate seven sampling strategies that govern the construction of the training batch from memory. These strategies are grouped into two categories based on their computational cost. Inexpensive sampling methods include Uniform, Class-Balanced, Recency-Biased and FIFO sampling. On the other hand, costly sampling strategies include KMeans, Max Loss, and Uncertainty loss sampling. 

To normalize for the effective $\mathcal{C}$ due to the overhead of associated extra forward passes to decide on the sampling, costly sampling strategies are allowed $\nicefrac{\mathcal{C}}{2}$ training iterations, where the exact calculation is left for the Appendix. That is to say, costly sampling strategies perform $200$ training iterations for ImageNet2K and $50$ training iterations for CGLM as the rest of the budget is for the extra forward passes. We report the performance of the five sampling strategies consisting of the inexpensive and the best performing costly sampling strategy (KMeans),  presented in shades of blue, in Figures \ref{fig:datainc_sampling}, \ref{fig:clsinc_sampling}, and \ref{fig:nds_sampling} for DI-ImageNet2K, CI-ImageNet2K, and CGLM, respectively. Other methods are listed in the Appendix due to lack of space. We compare against a non-continual learning oracle that performs classical empirical risk minimization at every step on $\mathcal{T}_t = \cup_{r=1}^t \{(x_i^r,y_i^r)\}_{i=1}^{n_r}$ with a computational budget of $\mathcal{C} \times t$, which we refer to as ERM-Naive; this is as opposed to the previously mentioned continual learning methods that have only $\mathcal{C}$ per step $t$ spent equally over all steps. ERM-Naive acts as a training method with hindsight, spending the complete computational budget at once after collecting the full dataset. This acts as a very strong baseline against all continual learning methods. We report it in shades of red in the same figures. We also report the average accuracy, averaged over all time steps, for each sampling method in the yellow box in each figure.


\begin{table}[t]
    \centering
    \scriptsize
    \begin{tabular}{l|cc}\toprule
        \textbf{Attributes} & \textbf{ImageNet2K} & \textbf{CGLM} \\ \hline
        Initial memory & ImageNet1K & \{\} \\
        Initial memory size & 1.2M & 0 \\
        Per step stream size $n$ & 60K & 29K \\
        Time steps & 20 & 20 \\
        Stream size & 1.2M & 58K \\
        Size of data by the last time step & 2.4M & 58K \\
        Stream & \begin{tabular}{c} Class incremental \\ Data incremental \end{tabular} & Time incremental\\
        $\#$ iterations per time step $\mathcal{C}$ & 400 & 100 \\
        Training batch size $\mathcal{B}$ & 1500 & 1500 \\
        Metrics & \begin{tabular}{c} Acc on ImageNet1K \\Acc on Stream \end{tabular} & Acc on Stream\\
        \hline
        Eq. Distillation Iters & 267 & 67 \\
        Eq. Sampling Iters & 200 & 100 \\
        Eq. FC Correction Iters & 400 & 100 \\ \hline
        Iters per $t$ (Sensitivity) & 100, 1200 & 40, 400\\
        Time Steps (Sensitivity) & 50, 200 & 50, 200\\\bottomrule
    \end{tabular}
    \vspace{-0.2cm}
    \caption{\textbf{Experimental Details.} The first block shows the various considered settings in the experiments section. The second block denotes the effective training iterations $\mathcal{C}$ for each class of methods due to their over head extra computation. The last block details the setup for our sensitivity analysis.}
    \vspace{-0.60cm}
    \label{tab:setup_deets}
\end{table}

\noindent\textbf{Conclusion.} First, we observe that the top inexpensive sampling strategies perform very similarly to each other. This is consistent across settings, CI-ImageNet2K, and CGLM, on both ImageNet1K accuracy and Stream Accuracy. There are some advantages for Class-Balanced over other sampling strategies, \eg, gaining an average accuracy of 2.5\% over Uniform in DI-ImageNet2K. However, sampling strategies such as FIFO completely forget ImageNet1K (dark blue line), leading to poor performance over all three settings. Interestingly, costly sampling strategies perform significantly worse in CL performance over the simple Uniform sampling when subjected to an effectively similar computational budget. This observation is different from previous settings \cite{masana2020class}, as the additional computational overhead of costly sampling does not seem worthwhile to improve performance.


\begin{figure*}[htp!]
    \centering
    \vspace*{-0.1cm}
    \includegraphics[width=0.75\textwidth]{new_figures/losssfinalll.png}
    \vspace{-0.2cm}
    \caption{\textbf{FC Layer Correction.} Even though loss functions (CosineFC and ACE) might outperform Naive in the first few time steps, eventually Naive catches up. Overall, Naive consistently outperforms all considered calibration methods too, namely, BIC and WA.}
    \vspace{-0.35cm}
    \label{fig:lastlayer}
\end{figure*}

\noindent\textbf{2. Does Distillation Matter?} We evaluate four well-known distillation losses in our benchmark, namely, Cosine, CrossEntropy, BCE, and MSE losses. Given that Class-Balanced is a simple inexpensive sampling procedure that performed slightly favorably, as highlighted in the previous section, we use it as a default sampling strategy from now onward, where the number of samples used per training step is equal over all classes. We refer to this basic approach with a cross entropy loss as Naive. To fairly factor in the overhead of an additional forward pass, distillation approaches are allowed $\nicefrac{2\mathcal{C}}{3}$ iterations compared to Naive with $\mathcal{C}$ training iterations. That is, the distillation losses perform $267$ iterations for ImageNet2k and $67$ iterations for CGLM compared to $400$ and $100$ iterations for Naive. We report the results for Cosine and MSE on DI-ImageNet2K, CI-ImageNet2K, and CGLM datasets in the first, second, and third rows of Figure \ref{fig:distillation_datainc}, respectively. Other methods are left for the Appendix due to lack of space. Distillation methods are shown in shades of blue, whereas Naive is shown in shades of red. We report the average accuracy, averaged over all time steps, for each distillation method in the yellow box in each figure. 


\noindent\textbf{Conclusion.} In all three settings, distillation methods underperform compared to Naive. Even in ImageNet1K Acc, which measures forgetting, Naive performs similarly or slightly better than all distillation methods in DI-ImageNet2K and CI-ImageNet2K streams. The results in Figure \ref{fig:distillation_datainc} show that the top distillation methods, such as MSE, perform only slightly worse compared to Naive ($54.9$ vs $55.9$ on DI-ImageNet2K and $64$ vs $64.9$ on CI-ImageNet2K). However, in CGLM they perform significantly worse ($26.4$ compared to $35.7$) due to the limited iterations. We attribute this to the fact that distillation methods often require a larger number of training samples, and thereof a large enough computational budget per time step.

\begin{figure}[t]
    \centering
    \vspace{-0.15cm}
    \includegraphics[width=0.4\textwidth]{new_figures/difftimesteps_finalll.png}
    \vspace{-0.25cm}
    \caption{\textbf{CGLM Distillation with Different Number of Time Steps.} Under larger number of time steps, where total number of iterations is normalized accordingly, Naive outperforms distillation in both settings, namely, $50$ and $200$ time steps.}
    \label{fig:difftimestepscglm}
    \vspace{-0.4cm}
\end{figure}

\noindent \textbf{3. Does FC Layer Correction Matter?} We evaluate five FC layer correction approaches from two different families. A family of methods that modifies the FC layer directly, including CosineFC \cite{hou2019learning} and ACE\footnote{We treat all seen samples as incoming samples to test this case, diverging from the original ACE Loss. The ACE Loss collapses to Crossentropy, as new samples form a tiny fraction of all past seen data.} \cite{zeno2018task,mai2022online,caccia2022new}. The other family of methods applies post-training calibration including BiC \cite{wu2019large}, WA \cite{zhao2020maintaining}, along with temperature scaling \cite{guo2017calibration}. All methods employ Class-Balanced as a sampling strategy and compare against Naive (Class-balanced with cross entropy loss) with no corrections in FC layer. The first three subplots of Figure \ref{fig:lastlayer} correspond to comparisons of direct FC layer modification methods against Naive on DI-ImageNet2K, CI-ImageNet2K, and CGLM. Since calibration methods tailored for Class Incremental settings, in the rightmost plot of Figure \ref{fig:lastlayer}, we report comparisons with Naive on CI-ImageNet2K. Since all FC layer corrections are with virtually no extra cost, the number of training iterations per time step is set to $\mathcal{C}$, \ie, $400$ for ImageNet2K and $100$ for CGLM.
 
\noindent\textbf{Conclusion.} No method consistently outperforms Naive in computationally budgeted continual learning. The first family of methods helps in DI-ImageNet2K, particularly in the initial steps due to class imbalance, but no method outperforms Naive in the CI-ImageNet2K set-up. Calibration-based methods, such as BIC, are somewhat competitive with Naive, but WA fails. Surprisingly, even under various FC correction approaches, all methods fail to outperform Naive in computationally budgeted continual learning.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{new_figures/diffsteps_finalll.png}
    \vspace{-0.25cm}
    \caption{\textbf{CGLM Distillation with Different Computational Budgets.} Naive outperforms distillation methods under the restricted $40$ and the larger $400$ iterations (originally $100$). Distillation methods become competitive when enough budget is available.}
    \label{fig:diffstepscglm}
    \vspace{-0.4cm}
\end{figure}
\vspace{-0.15cm}
\subsection{Sensitivity Analysis}
\vspace{-0.15cm}
We have analyzed the performance of various CL methods under budgeted computation. We have consistently observed over a variety of settings on large-scale datasets that a simple method, \ie, Naive, simply sampling with a Class-Balanced strategy and a cross entropy loss outperforms all existing methods.  However, all reported results were for $20$ time steps with $\mathcal{C} = 400$ or $\mathcal{C}=100$ training iterations for ImageNet2K and CGLM, respectively, in which expensive methods were normalized accordingly. Now, we analyze the sensitivity of our conclusions over different time steps and iterations $\mathcal{C}$.

\noindent\textbf{Does the Number of Time Steps Matter?} Prior art, such as GDumb~\cite{prabhu2020gdumb}, found that the relative performance of CL methods changes drastically when the number of time steps is varied. Subsequently, we increased the number of time steps to $50$ and $200$ from $20$, a more extreme setting than explored in recent works, while maintaining the same overall computational budget $\mathcal{C}$ eliminating any source of performance variation due to a different total computational budget. This is since per time step, the stream reveals fewer number of samples $n$ with an increased number of time steps. We report experiments in the CGLM setting where Naive will receive only $40$ and $10$ iterations for the $50$ and $200$ time steps, respectively. We consider distillation approaches where they are permitted $\nicefrac{2}{3}\mathcal{C}$, which is $27$ and $7$ iterations, respectively, on the $50$ and $200$ time steps, respectively. Note that, in these settings, per time step, methods observe $\nicefrac{2}{3} \times 11.6$K and $\nicefrac{2}{3} \times 2.9$K samples, respectively. We leave the experiments on ImageNet2K for the Appendix due to space constraints. 
We compare two distillation methods against Naive in Figure \ref{fig:difftimestepscglm}. Other methods are presented in the Appendix.

\noindent\textbf{Conclusion.} We still consistently observe that Naive outperforms all distillation methods on both the $50$ and the $200$ time steps. Moreover, the relative performance across distillation methods is preserved similarly to the $20$ time steps setup. That is, our conclusions are largely robust under different number of time steps. This is contrary to the observation of the prior art \cite{prabhu2020gdumb}, this is because unlike our setting, \cite{prabhu2020gdumb} does not scale the compute with increased number of time steps. 

\noindent\textbf{Does the Compute Budget Matter?} 
Finally, we explore the impact of changing the computational budget on the performance of different distillation methods on CGLM under $20$ time steps. We study two scenarios, one where the budget is increased to $\mathcal{C}=400$ and where it is reduced to $\mathcal{C}=40$, originally $\mathcal{C}=100$ for CGLM. Hence, distillation would be allocated $267$ and $27$ iterations in this setting, respectively. As shown in Figure \ref{fig:effective_epochs}, the higher budget setting allows approximately a full pass per time step over all stored data. We leave the experiments on ImageNet2K for the Appendix. We compare two distillation methods with Naive in Figure \ref{fig:diffstepscglm}. The remaining methods are presented in the Appendix.

\noindent \textbf{Conclusion.} Again, we observe that Naive outperforms all distillation methods in both increased and decreased compute budget settings. The final gap between MSE distillation and Naive is $11.41\%$ for $\mathcal{C}=40$, this gap is reduced to $3.85\%$ for $\mathcal{C}=400$. Surprisingly, even with increased compute budget, distillation methods still fall behind Naive. However, the reduced gap in performance compared to that of Naive is a strong indication that the reason behind the failure of distillation methods is indeed the limited computation.

 \begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{new_figures/linearvsfullrescaled.png}
    \vspace{-0.15cm}
    \caption{\textbf{Linear vs Full Model Training.} Performing Linear fine tuning allows to leveraging the computational budget efficiently improving the gap compared to full model training particularly for better pretrained models, \eg,  Instagram1B+ImNet1K.
    }
    \vspace{-0.6cm}
    \label{fig:lastsection}
\end{figure}
\subsection{Exploring Partial Training}
\vspace{-0.15cm}
We now investigate the reallocation of the computational budget through partial training of the model, which is a model expansion method that involves pre-selecting the subnetwork to be trained. This approach is more computationally efficient, especially on large-scale datasets. The top (FC) layer is the smallest part of the network that can be retrained. We compare partial training  of the network, \ie, FC layer only, to training the full model (Naive) using two different model initializations, ImageNet1K pretraining \cite{he2016deep} and Instagram1B+ImageNet1K pretraining \cite{mahajan2018exploring}. Note that Instagram1B+ImageNet1K is a stronger pretrained model, with better feature representations. To normalize the computation for the FC partial training, we permit $3\mathcal{C}$ training iterations compared to full model training (Naive) with $\mathcal{C}$ training iterations. Hence, CGLM FC partial training performs 300 iterations compared to 100 iterations for training Naive. We present our results in Figure \ref{fig:lastsection}, where the shades of purple and blue represent models trained from pretrained ImageNet1K and Instagram1B+ImageNet1K models, respectively.

\noindent\textbf{Conclusion.} There exists a gap between full model training and partial FC layer training (Linear). However, this gap is greatly reduced when a stronger pretrained model is adopted as an initialization. More specifically, the final gap drops from $23.73\%$ for ImageNet1K initialization to $9.45\%$ for Instagram1B+ImageNet1K initialization. Partial training of the FC layer for Instagram1B+ImageNet1K model initialization outperforms ImageNet1K full model training on average, over time steps, by $8.08\%$, which verifies that partially training a strong backbone could be more beneficial than fully training a weaker one.