\section{Introduction}
\vspace{-0.1cm}

\begin{figure}
\centering
%\vspace{-0.4cm}
\includegraphics[width=1\linewidth]{new_figures/pullfig.pdf}
\caption{\textbf{Main Findings. } Under per time step computationally budgeted continual learning, classical continual learning methods, \eg, sampling strategies, distillation losses, and fully connected (FC) layer correction based methods such as calibration, struggle to cope with such a setting. Most proposed continual algorithms are particularly useful only when large computation is available, where, otherwise, minimalistic algorithms (ERM) are superior.}
\label{fig:pull_figure}
\vspace{-0.60cm}
\end{figure}


Deep learning has excelled in various computer vision tasks \cite{he2016deep, schmidhuber2015deep,bommasani2021opportunities,lecun2015deep} by performing hundreds of shuffled passes through well-curated offline static labeled datasets. However, modern real-world systems, \eg, Instagram, TikTok, and Flickr, experience high throughput of a constantly changing stream of data, which poses a challenge for deep learning to cope with such a setting. Continual learning (CL) aims to go beyond static datasets and develop learning strategies that can adapt and learn from streams where data is presented incrementally over time, often referred to as time steps. However, the current CL literature overlooks a key necessity for practical real deployment of such algorithms. In particular, most prior art is focused on \textit{offline continual learning} \cite{rebuffi2017icarl,hou2019learning,kirkpatrick2017overcoming} where, despite limited access to previous stream data, training algorithms do not have restrictions on the computational training budget per time step. 

High-throughput streams, \eg, Instagram, where every stream sample at every time step needs to be classified for, say, misinformation or hate speech, are time-sensitive in which long training times before deployment are simply not an option. Otherwise, new stream data will accumulate until training is completed, causing server delays and worsening user experience. 

Moreover, limiting the computational budget is necessary towards reducing the overall cost. This is because computational costs are higher compared to any storage associated costs. For example, on Google Cloud Standard Storage (2\textcent~per GB per month), it costs no more than 6\textcent~ to store the entire CLEAR benchmark \cite{lin2021clear}, a recent large-scale CL dataset. On the contrary, one run of a CL algorithm on CLEAR performing $\sim 300$K iterations costs around $100$\$ on an A100 Google instance (3\$ per hour for 1 GPU). Therefore, it is prudent to have computationally budgeted methods where the memory size, as a consequence, is implicitly restricted. This is because, under a computational budget, it is no longer possible to revisit all previous data even if they were all stored in memory (given their low memory costs). 

This raises the question: ``\textit{Do existing continual learning algorithms perform well under per step restricted computation?"} To address this question, we exhaustively study continual learning systems, analyzing the effect of the primary directions of progress proposed in the literature in the setting where algorithms are permitted fixed computational budget per stream time step. We evaluate and benchmark at scale various classical CL sampling strategies (Uniform, Class-Balanced\cite{prabhu2020gdumb}, Recency-Biased\cite{lin2021clear}, FIFO \cite{chaudhry2019continual, cai2021online}, Max Loss, Uncertainity Loss \cite{bang2021rainbow}, and KMeans \cite{chaudhry2019continual}), CL distillation strategies (BCE \cite{rebuffi2017icarl}, MSE \cite{buzzega2020dark}, CrossEntropy \cite{wu2019large}, and Cosine \cite{hou2019learning}) and FC layer corrections (ACE \cite{zeno2018task,mai2022online,caccia2022new}, BiC \cite{wu2019large}, CosFC \cite{hou2019learning}, and WA \cite{zhao2020maintaining}) that are common in the literature. Evaluation is carried on two large-scale datasets, amounting to a total of 1500 GPU-hours, namely ImageNet \cite{deng2009imagenet} and Continual Google Landmarks V2 \cite{prabhu2023online} (CGLM) under various stream settings, namely, data incremental, class incremental, and time incremental settings. We compare against Naive; a simple baseline that, utilizing all the per step computational budget, trains while sampling from previous memory samples. 

\noindent \textbf{Conclusions.} We summarize our empirical conclusions in three folds. \textbf{(1)} None of the proposed CL algorithms, see Table \ref{fig:pull_figure} for considered methods, can outperform our simple baseline when computation is restricted. \textbf{(2)} The gap between existing CL algorithms and our baseline becomes larger with harsher compute restrictions. \textbf{(3)} We find that training a minimal subset of the model can close the performance gap compared to our baseline in our setting, but only when supported by strong pretrained models.

Surprisingly, we find that these observations hold even when the number of time steps is increased to 200, a large increase compared to current benchmarks, while normalizing the effective total computation accordingly. This suggests that existing CL literature is particularly suited for settings where memory is limited, and less practical in scenarios having limited computational budgets.
