\section{Continual Learning with Limited Compute}
\vspace{-0.1cm}
\label{sec:budgted_cl}

\subsection{Problem Formulation}
\vspace{-0.1cm}
We start by first defining our proposed setting of \textit{computationally budgeted} continual learning. Let $\mathcal{S}$ be a stream revealing data sequentially over time steps. At each time step $t \in \{1,2,\dots,\infty\}$, the stream $\mathcal{S}$ reveals $n_t$ image-label pairs $\{(x_i^t,y_i^t)\}_{i=1}^{n_t} \sim \mathcal{D}_j$ from distribution $\mathcal{D}_j$ where $j \in \{1,\dots,t\}$. In this setting, we seek to learn a function $f_{\theta_t} : \mathcal{X} \rightarrow \mathcal{Y}_t$ parameterized by $\theta_t$ that maps images $x\in \mathcal{X}$ to class labels $y \in \mathcal{Y}_t$, where $\mathcal{Y}_t = \bigcup^t_{i=1}\mathcal{Y}_i$, which aims to correctly classify samples from any of the previous distributions $\mathcal{D}_{j \leq t}$. In general, there are no constraints on the incoming distribution $\mathcal{D}_j$, \eg, the distribution might change after every time step or it may stay unchanged for all time steps. The size of the revealed stream data $n_t$ can generally change per step, \eg, the rate at which users upload data to a server. The unique aspect about our setting is that at \emph{every time step $t$, a computational budget $\mathcal{C}_t$} is available for the CL method to update the parameters from $\theta_{t-1}$ to $\theta_t$ in light of the new revealed data. Due to the inexpensive costs associated with memory storage, in our setting, we assume that CL methods in our setting can have full access to all previous samples $\mathcal{T}_t = \cup_{r=1}^{t} \{(x_i^r,y_i^r)\}_{i=1}^{n_r}$.\footnote{We discuss in the Appendix the privacy arguments often used towards restricting the memory.} 
However, as will be discussed later, while all samples can be stored, they cannot all be used for training due to the constrained computation imposing an implicit memory restriction. 

\begin{table*}[]
\scriptsize
    \centering
    \resizebox{0.775\textwidth}{!}{
    \begin{tabular}{llcccccc} \toprule
    Dir. & Reference & Applicability & \multicolumn{5}{c}{Components} \\
     &  & (our setup) & Distillation & MemUpdate & MemRetrieve & FC Correction & Others\\\midrule
     & Naive & $\checkmark$ & - & Random & Random & - & - \\ \hline
  \multirow{6}{*}{\rotatebox[origin=c]{90}{Distillation}}  & iCARL~\cite{rebuffi2017icarl}& $\checkmark$ & \textbf{BCE} & \textbf{Herding} & Random & - & \textbf{NCM} \\
     & LUCIR~\cite{hou2019learning} & $\checkmark$  & \textbf{Cosine} & Herding & MargRank & \textbf{CosFC} & NCM \\
     & PODNet~\cite{douillard2020podnet} & $\checkmark$  & \textbf{POD} & Herding & Random & \textbf{LSC} & Imprint,NCM\\
     & DER~\cite{buzzega2020dark} & $\checkmark$  & \textbf{MSE} & Reservoir & Random & - & - \\
      
     &  C$\mbox{O}^2$L \cite{cha2021co2l} & $\times$ & \textbf{IRD} & Random & Random & \textbf{Asym.SupCon} & - \\
     &  SCR \cite{mai2021supervised} & $\checkmark$ & - & Reservoir & Random & \textbf{SupCon} & NCM \\ \hline
   \multirow{11}{*}{\rotatebox[origin=c]{90}{Sampling}}  & TinyER~\cite{chaudhry2019continual}& $\checkmark$ & -  & \textbf{\begin{tabular}{c}FIFO,KMeans,Reservoir\end{tabular}} & - &  - & -\\
     & GSS~\cite{aljundi2019gradient} & $\times$ & - & \textbf{GSS} & Random & - & -\\
     &   MIR~\cite{aljundi2019online} & $\times$ & - & Reservoir & \textbf{MIR} & - & - \\ 
     & GDumb~\cite{prabhu2020gdumb}& $\checkmark$ & - & \textbf{Balanced} & Random & - & \textbf{MemOnly} \\
     & Mnemonics~\cite{liu2020mnemonics} & $\times$ & - & \textbf{Mnemonics} & - & - & BalFineTune  \\ 
     & OCS\cite{yoon2021online}& $\times$ & - & \textbf{OCS} & Random & - & - \\
     & InfoRS~\cite{sun2022information} & $\times$ & MSE & \textbf{InfoRS} & Random & - & - \\
     & RMM\cite{liu2021rmm}  & $\times$ & - & \textbf{RMM} & - & - & - \\
    & ASER \cite{shim2021online} & $\times$ & - & \textbf{SV} & \textbf{ASV} & - & - \\
    & RM \cite{bang2021rainbow} & $\checkmark$ & - & \textbf{Uncertainty} & Random & - & AutoDA  \\
     & CLIB~\cite{koh2021online}& $\times$ & - & \textbf{Max Loss} & Random & - & \begin{tabular}{c} MemOnly,AdaLR\end{tabular}  \\\hline
   \multirow{5}{*}{\rotatebox[origin=c]{90}{FC Layer}} & BiC~\cite{wu2019large} & $\times$ & CrossEnt & Random & Random & \textbf{BiC} & - \\
     & WA~\cite{zhao2020maintaining} & $\times$ & CrossEnt & Random & Random &\textbf{WA} & - \\
    & SS-IL \cite{ahn2021ss} & $\times$ & TKD & Random & Balanced & \textbf{SS} &  -\\
     & CoPE\cite{de2021continual}& $\checkmark$ & - & Balanced & Random & \textbf{PPPLoss} & - \\
     & ACE~\cite{caccia2022new} & $\checkmark$ & - & Reservoir & Random & \textbf{ACE} & - \\ \bottomrule
    \end{tabular}}
    \vspace{-0.15cm}
    \caption{\textbf{Primary Directions of Progress in CL.} Analysis of recent replay-based systems, with \textbf{bold} highlighting the primary contribution. We observe that there are three primary directions of improvement. ``App." denotes the applicability to our setting based on whether they are scalable to large datasets and applicable beyond the class-incremental stream.}
    \vspace{-0.60cm}
    \label{tab:approaches}
\end{table*}

\subsection{Key Differences with Prior Art}
\vspace{-0.1cm}

(\textbf{1}) \textit{Tasks}: In most prior work, CL is simplified to the problem of learning a set of non-overlapping tasks, \ie, distributions, \emph{with known boundaries} between them \cite{rebuffi2017icarl,lopez2017gradient,chaudhry2018riemannian}. 
In particular, the data of a given distribution $\mathcal{D}_j$ is given all at once for the model to train. This is as opposed to our setup, where there is no knowledge about the distribution boundaries, since they are often gradually changing and not known a priori. As such, continual learning methods cannot train only just before the distribution changes.


(\textbf{2}) \textit{Computational Budget}: A key feature of our work is that, per time step, CL methods are given a fixed computational budget $\mathcal{C}_t$ to train on $\{(x_i^t,y_i^t)\}_{i=1}^{n_t}$. For ease, we assume throughout that $\mathcal{C}_t = \mathcal{C} ~\forall t$, and that $n_t = n \forall t$. Although $\mathcal{C}$ can be represented in terms of wall clock training time, for a given $f_\theta$ and stream $\mathcal{S}$, and comparability between GPUs, we state $\mathcal{C}$ in terms of the number of training iterations instead. This avoids hardware dependency or suboptimal implementations when comparing methods. This is unlike prior work, which do not put hard constraints on compute per step \cite{rebuffi2017icarl,hou2019learning,buzzega2020dark} giving rise to degenerate but well-performing algorithms such as GDumb \cite{prabhu2020gdumb}. Concurrent works  \cite{ghunaim2023real, prabhu2023online} restrict the computational budget, however they  focus on fast adaptation whereas we focus on alleviating catastrophic forgetting.


(\textbf{3}) \textit{Memory Constraints}: Prior work focuses on a fixed, small memory buffer for learning and thereof proposing various memory update strategies to select samples from the stream. We assume that all the samples seen so far can be stored at little cost. However, given the restricted imposed computation $\mathcal{C}$, CL methods cannot revisit or learn from all stored samples. For example, as shown in Figure \ref{fig:effective_epochs}, consider performing continual learning on ImageNet2K, composed of 1.2M samples from ImageNet1K and 1.2M samples from ImageNet21K forming 2K classes, which will be detailed later, over $20$ time steps, where the stream reveals sequentially $n = 60$K images per step. Then, under a computation budget of $8000$ iterations, the model cannot revisit more than $50\%$ of all seen data at any given time step, \ie 600K samples. Our proposed setting is closer to realistic scenarios that cope with high-throughput streams, where \textit{computational bottlenecks impose implicit constraints on learning from past samples that can be too many to be revisited during training}.

\subsection{Constructing the Stream} \vspace{-0.1cm}
\noindent We explore three stream settings in our proposed benchmark, which we now describe in detail.


(\textbf{1}) \textit{Data Incremental Stream}: In this setting, there is no restriction on the incoming distribution $\mathcal{D}_j$ over time that has not been well-explored in prior works.
We randomly shuffle all data and then reveal it sequentially over steps, which could lead to a varying distribution $\mathcal{D}_j$ over steps in which there are no clear distribution boundaries.

(\textbf{2}) \textit{Time Incremental Stream}: In this setting, the stream data is ordered by the upload timestamp to a server, reflecting a natural distribution change $\mathcal{D}_j$ across the stream as it would in real scenarios. There is a recent shift toward studying this ordering as apparent in recent CL benchmarks, \eg, CLEAR \cite{lin2021clear}, Yearbook \cite{yao2022wild} and FMoW \cite{yao2022wild}, Continual YFCC100M \cite{cai2021online} and Continual Google Landmarks V2 \cite{prabhu2023online}.

(\textbf{3}) \textit{Class Incremental Stream}: For completeness, we consider this classical setting in the CL literature. Each of the distributions $\mathcal{D}_j$ represents images belonging to a set of classes different from the classes of images in any other distribution $\mathcal{D}_{i \neq j}$. We benchmark these three settings using a large-scale dataset that will be detailed in the Experiments.


\section{Dissecting Continual Learning Systems}
\vspace{-0.1cm}

Continual learning methods typically propose a system of multiple components that jointly help improve learning performance. For example, LUCIR\cite{hou2019learning} is composed of a cosine linear layer, a cosine distillation loss function, and a hard-negative mining memory-based selector. In this section, we analyze continual learning systems and dissect them into their underlying components. This helps to analyze and isolate the role of different components under our budgeted computation setting and helps us to understand the most relevant components.  
In Table \ref{tab:approaches}, we present the breakdown of novel contributions that have been the focus of recent progress in CL. 
The columns indicate the major directions of change in the CL literature. Overall, there have been three major components on which advances have focused, namely distillation, sampling, and FC layer correction. These three components are considered additions to a naive baseline that simply performs uniform sampling from memory. We refer to this baseline as Naive in Table \ref{tab:approaches}.

(\textbf{1}) \textit{Distillation}: One popular approach towards preserving model performance on previous distributions has been through distillation. It enables student models, \ie, current time step model, to learn from a teacher model, \ie, one that has been training for many time steps, through the logits providing a rich signal. 
In this paper, we consider four widely adopted distillation losses, namely, Binary CrossEntropy (BCE) \cite{rebuffi2017icarl}, CrossEntropy \cite{wu2019large,zhao2020maintaining,liu2020mnemonics}, Cosine Similarity (Cosine)\cite{hou2019learning}, and Mean Square Error (MSE)\cite{buzzega2020dark,sun2022information} Loss.

(\textbf{2}) \textit{Sampling}: Rehearsing samples from previous distributions is another popular approach in CL. However, sampling strategies have been used for two objectives. Particularly when access to previous samples is restricted to a small memory, they are used to select which samples from the stream will update the memory (MemUpdate) or to decide on which memory samples are retrieved for rehearsal (MemRetrieve). 
In our unconstrained memory setup, simply sampling uniformly over the joint data of past and current time step data (as in Naive) exposes a particular shortcoming. When training for a large number of time steps, uniform sampling reduces the probability of selecting samples from the current time step. For that, we consider various sampling strategies, \eg, recency sampling \cite{lin2021clear} that biases toward sampling current time step data, and FIFO \cite{chaudhry2019continual, cai2021online} that exclusively samples from the current step. We do not consider Reservoir, since it approximates uniform sampling in our setup with no memory restrictions. In addition to benchmarking the sampling strategies mentioned above, we also consider approaches that evaluate the contribution of each memory sample to learning \cite{toneva2018empirical}. For example, Herding \cite{rebuffi2017icarl}, K-Means \cite{chaudhry2019continual}, OCS \cite{yoon2021online}, InfoRS \cite{sun2022information}, RM  \cite{bang2021rainbow}, and GSS \cite{aljundi2019gradient} aim to maximize diversity among samples selected for training with different metrics. MIR \cite{aljundi2019online}, ASER\cite{shim2021online}, and CLIB \cite{koh2021online} rank the samples according to their informativeness and select the top-$k$. Lastly, balanced sampling \cite{prabhu2020gdumb,de2021continual,chrysakis2020online} select samples such that an equal distribution of classes is selected for training. In our experiments, we only consider previous sampling strategies that are applicable to our setup and compare them against Naive.

(\textbf{3}) \textit{FC Layer Correction}: It has been hypothesized that the large difference in the magnitudes of the weights associated with different classes in the last fully connected (FC) layer is among the key reasons behind catastrophic forgetting \cite{wu2019large}. There has been a family of different methods addressing this problem. 

These include methods that improve the design of FC layers, such as CosFC \cite{hou2019learning}, LSC\cite{douillard2020podnet}, and PPP \cite{de2021continual}, by making the predictions independent of their magnitude. 
Other approaches such as SS-IL \cite{ahn2021ss} and ACE \cite{zeno2018task, mai2022online, caccia2022new} mask out unaffected classes to reduce their interference during training. In addition, calibrating the FC layer in post-training, \eg, 
BiC \cite{wu2019large}, WA \cite{zhao2020maintaining}, and IL2M \cite{belouadah2019il2m} is widely used. Note that the calibration techniques are only applicable to the class-incremental setup. We benchmark existing methods applicable to our setting against the Naive approach that does not implement any FC layer correction.

(\textbf{4}) \textit{Model Expansion Methods}:
Several works attempt to adapt the model architecture according to the data. This is done by only training part of the model \cite{mallya2018packnet,mallya2018piggyback,abati2020conditional,aljundi2017expert, rajasegaran2019random} or by directly expanding the model when data is presented \cite{rusu2016progressive,yan2021dynamically,yoon2017lifelong,zhang2020side,wang2017growing,rebuffi2017learning}. 
However, most of the previous techniques in this area do not apply to our setup. Most of this line of work \cite{mallya2018packnet,mallya2018piggyback,rebuffi2017learning} assumes a task-incremental setting, where at every time step, new samples are known to what set of classes they belong, \ie, the distribution boundaries are known, even at test time. To overcome these limitations, newer methods \cite{rajasegaran2020itaml,abati2020conditional} use a bilevel prediction structure, predicting the task at one level and the label within the task at the second level. They are restricted to the class-incremental setting as they assume each task corresponds to a set of non-overlapping classes. We seek to understand the limitation of partial retraining in a network; hence, instead, we compare Naive against a setting where only the FC layer is being trained, thus minimally training the network per time step. In addition, we examine the role of pretraining which has recently become a widely popular direction for exploration in continual learning \cite{wu2022class}.