\onecolumn
\section{Motivation: Privacy for Restricting Memory}

\begin{table}[h!]
% \footnotesize
\scriptsize

    \centering
    \begin{tabular}{lllcccc} \toprule
      Ref. & Dataset & Ordering & Memory & Cost & Iters & Cost \\ \midrule
      % & MNIST & 0.2-1 MB & 0.07\textcent & \\
      \cite{buzzega2020dark} & CIFAR10 & Cls Inc & 1-25MB & 0.05\textcent & 250K-375K & 20\$ \\
      %\cite{aljundi2019gradient,caccia2022new}  & CIFAR10 & CL  & 0.7-5 MB & 0.01\textcent & 5K-25K & 10\textcent \\ \hline
      \cite{rebuffi2017icarl}  & \multirow{2}{*}{CIFAR100} & \multirow{2}{*}{Cls Inc} & \multirow{2}{*}{10 MB} & \multirow{2}{*}{0.02\textcent} & 50K  & 8\$\\
      \cite{douillard2020podnet, hou2019learning} &  & & & & 125K  & 15\$ \\  \hline
      %\cite{aljundi2019gradient,caccia2022new} &  & &  & & 5K-25K & 10\textcent \\ 
      \cite{buzzega2020dark} & TinyImageNet & Cls Inc & 5-20 MB & 0.04\textcent & 350K-500K & 25\$\\ 
      % \cite{caccia2022new} & MiniImageNet & Cls Inc & & & & &\\
      \cite{hou2019learning, douillard2020podnet} & ImageNet100 & Cls Inc & 0.3-1 GB & 2\textcent & 100K & 50\$ \\ 
      \cite{hou2019learning, douillard2020podnet} & ImageNet1K & Cls Inc & 33GB & 66\textcent & 1M & 500\$ \\
      \cite{lin2021clear} & CLEAR & Dist Shift & 0.4-1.2GB & 2\textcent & 300K & 100\$ \\\midrule
     %   & CoRe50-NI & 0.2-2 GB & 1\$ & \\ 
        
      \cite{he2016deep} & ResNet50 (bs=256) & & 22GB & & \\ \midrule
      \multirow{2}{*}{Ours} & GLDv2-CL & Dist Shift & \textbf{90GB} & 2\$ & 2K  & 10\$\\
      & ImageNet21K-CL & ClsInc, DataInc & \textbf{400GB} & 10\$ & 8K  & 35\$\\ \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
    \caption{\textbf{Cost of Memory vs Computation.}  Google Cloud Standard Storage (2\textcent  per  GB per month for 1 month) and Compute Cost measured as running cost of an A2 instance (3\$ per hour for 1 GPU). Number of iterations (forward+backward passes) for training a CL model on that dataset listed for  comparison invariant to input image and model sizes. We observe that computational costs for running an experiment far outweigh the costs for storing replay samples.}
    \vspace{-0.35cm}
    \label{table:cost_intro}
\end{table} 

Prior art on continual learning \cite{lopez2017gradient,rebuffi2017icarl,chaudhry2018efficient,buzzega2020dark,aljundi2019gradient,lin2021clear} motivate the problem from the aspect of prohibited access to previously received data; except for a small portion that is allowed to store in memory. The two principal motivations behind restricting the access to past samples in the literature are two folds.
(\textbf{i}) Storage space is expensive. (\textbf{ii}) Access to previous data is prohibitive due to privacy and GDPR constraints. 

As for the first argument used as a motivation for limiting the memory size, as we have elaborated in Section 1 of the main paper and similarly further detail in Table \ref{table:cost_intro}, the cost of storing data is insignificant. This is particularly the case when considering the associated computational costs of training deep models. To that end, the argument of  to restricting the memory size is a an enough justifiable reason. For example, as per Table \ref{table:cost_intro}, it costs 2 cents to store the entirety of the CLEAR dataset, among the largest datasets for continual learning, while it costs about $100\$$ to train a model continually on the same dataset. If reducing costs are the key issue, limited computational budget and not memory, as argued earlier, is the way forward.

% Hence, here we discuss why we believe privacy concerns ill-motivate past settings. 
As for privacy considerations, this is too a not well-motivated reason behind limiting memory. %First, we elaborate the privacy argument. 
A classical argument is that due to GDPR requirements, data needs to be removed or company privacy policies, it can no longer accessible after $x$ number of months. First, any previous benchmark with memory constraints already violates this privacy consideration. This is since, which data is to be made private and shall be deleted should not be up to the learning algorithm to decide. To that end, restricting memory samples does not help solving the privacy considerations. Even if the samples stored in memory were selected such that they do not violate any privacy constraints, which none of the prior art address, it remains a question on whether the trained models preserve any sensitive information after training on private data. Without imposing such restriction on the learning algorithm and the underlying model, restricting the memory for the argument of privacy does not meet its stated objectives. This is particularly the case, as Haim \etal \cite{haim2022reconstructing} have found that models retain a lot of information of the training samples. This is to an extent that large number of training samples can be reconstructed only given the trained model. Goel \etal   \cite{goel2022evaluating} presents a catastrophic forgetting baseline, indicating forgetting might be the very objective of sustaining privacy which is antithetical to the objective of continual learning. 

In conclusion, restricting access to previous samples by restricting memory do not contribute to solving the privacy problem. Instead, we consider the more realistic setting where only limited amount of computation is given due to cost restraint or the need to predict every sample in a high throughput stream. This in any how imposes an implicit restraint on access of past memory samples.



% Moreover, that current training algorithms are given full choice to select few samples to store in memory, this is in contradiction



% and catastrophic forgetting might be a good way of erasing that information. This implies, forgetting is to some degree the very goal in deletion, antithetical to the objective here.

% Literature on machine unlearning \textcolor{red}{[cite]} states that not retaining any information also imposes a constraint that the deep model should have stored no information about the deleted data. 

% future algorithms restricting memory samples

% restricting memory samples
% It says sometimes access to previously seen user data is restricted, due to either privacy policies which say all data must be deleted from the system after x months or GDPR and equivalent requests recieved which require deletion of specific datapoints making them not accessible.

% First, we observe that both motivations specify which data points must be deleted, \ie there is no possibility of algorithms selecting which samples to store in the replay buffer. However, past benchmarks allow for this, and an entire line of works specifically study how to select samples to store in replay buffer. Hence, we claim past settings are ill-posed compared to motivation.

% Now, the question is whether benchmarks can satisfy the privacy requirements when the samples to be deleted are specified?
% Literature on machine unlearning \textcolor{red}{[cite]} states that not retaining any information also imposes a constraint that the deep model should have stored no information about the deleted data. %That is, the resulting model should be indistinguishable from models which have been trained on a training set with the deletion data removed. 

% However, Haim \etal \cite{haim2022reconstructing} have found that models retain a lot of information of training samples, to the degree that they could reconstruct a large portion of the training data given a trained neural network classifier. Goel \etal   \cite{goel2022evaluating} shows that a lot of information is still retained in the network after different unlearning procedures are applied, and catastrophic forgetting might be a good way of erasing that information. This implies, forgetting is to some degree the very goal in deletion, antithetical to the objective here.% (something about this being antithetical to what we're doing currently?)
% %We elaborate here on our arguments against constraining the memory due to privacy issues. The precise argument for privacy is that: Users might impose 
% %as on the privacy argument (specify the argument) for motivating 
% % \begin{itemize}
% %     \item Privacy motivation
% %     \item We move away, as model itself is also private
% % \end{itemize}
\section{Dataset Construction}

\subsection{Constructing Imagenet2K}

ImageNet2K train set is constructed using all training images in ImageNet1K dataset \cite{deng2009imagenet} for 1K classes as an initialization, with selecting an additional 1K non-overlapping classes from ImageNet21K dataset \cite{deng2009imagenet} to form the ImageNet2K dataset. We illustrate the creation of the test, validation and train sets below:

\textbf{Test Set}: We use the ImageNet1K val set as the test set to be  consistent with test sets used in previous literature using ImageNet1K. We separate 50 images per class from the sample set of the new 1K classes. We combine these two sets to create the overall test set for experiments. The test set for every timestep consists of classes from this test set which have been seen so far.

\textbf{Validation Set}: We use ImagenetV2 dataset \cite{recht2019imagenet} as the validation set from Imagenet1K data. We seperate 50 images per class, not used in the test set to create the val set for the new 1K classes. We combine these two sets to create the overall validation set for experiments. The validation set for every timestep consists of classes from this validation set which have been seen so far.

\textbf{Train Set}: We order all the samples from the new 1K classes not used for creating the test and val sets for training. We order them by classes to form the CI-ImageNet2K stream and randomly shuffle all these images to form the DI-ImageNet2K stream. Note that the stream order is provided samplewise, allowing the stream size $N$ to be adjusted. In the standard experiments, data equivalent to 50 classes is sampled every timestep, for 20 timesteps.

%as Imagenet val set is used traditionally as a test set for accuracy. We select 50 and 50 images from each of the new classes introduced to form the validation and test set respectively. The accuracy presented  on the test set contains all the classes seen until the current timestep. 

\subsection{Constructing Continual Google Landmarks V2}

Continual Google Landmarks V2 (CGLM) consists of 580K samples. To obtain this subset, we start with the train-clean subset of the Google Landmarks V2 available from the Google Landmarks V2 dataset website\footnote{https://github.com/cvdfoundation/google-landmark}. We apply the following preprocessing steps in order:
\begin{enumerate}
    \setlength\itemsep{-0.25em}
    \item Filter out images which do not have timestamp metadata available. 
    \item Remove images of classes that have less than 25 samples in total
    \item Order data by timestamp.
    \item Randomly sample 10\% of data from across time as the test set
\end{enumerate}
 We get the rest $580K$ images as the train set for continual learning over $10788$ classe, with rapid temporal distribution shifts. We do not have a validation set here as we benchmark transfer of hyperparameters used from ImageNet to this dataset.

%\textbf{Train Set}:

% train set is constructed using all training images in ImageNet1K dataset for 1K classes and selecting an additional 1K non-overlapping classes from the ImageNet21K dataset. We start with an imagenet1k pretrained model and the full imagenet1k train set in storage. There are two ordering of images we experiment with, (i) a class-incremental setting with 50 classes arriving per timestep over 20 timesteps (ii) 

% We use ImagenetV2 as the validation set from Imagenet1K side as Imagenet val set is used traditionally as a test set for accuracy. We select 50 and 50 images from each of the new classes introduced to form the validation and test set respectively. The accuracy presented  on the test set contains all the classes seen until the current timestep. 
\section{Estimation of Equivalent Iterations}

In this section, we elaborate on the details of selecting the computational budget for distillation and expensive sampling approaches. The key point for these calculations is the fact that the computational (and time) cost for a forward pass is $\nicefrac{1}{2}$ the cost of a backward pass \footnote{https://www.lesswrong.com/posts/jJApGWG95495pYM7C/how-to-measure-flop-s-for-neural-networks-empirically}. 

\textbf{Distillation}: When distillation approaches have a budget of $\nicefrac{2}{3}^{rd}$ iterations of the naive baseline, the computational cost is as follows: $\nicefrac{2}{3}\mathcal{C}$ for training of the student model, and $\nicefrac{1}{2}\times \nicefrac{2}{3} = \nicefrac{1}{3} \mathcal{C}$ for the teacher model which only has a forward pass, which sums up to $\mathcal{C}$. Hence, distillation methods have an  equivalent computational budget as the naive baseline with $\nicefrac{2}{3}^{rd}$ training iterations.

\textbf{Sampling}: We train the expensive models for $\nicefrac{1}{2}$ the number of iterations as a naive model. To select that subset of training data, we randomly sample $3\times$ the required number of training samples from the stored set and forward pass them through the latest trained model to obtain the features/probabilities. And then we select the best $\nicefrac{1}{3}^{rd} $ of the $3\times$ set for training using different selection functions. We assume the cost of selecting samples given the features/probabilities is negligible.

The computational cost of training for expensive sampling methods is $\nicefrac{1}{2}\mathcal{C}$, as the selected sample set is half the size compared to the naive baseline. The computational cost of selecting the samples is $\nicefrac{1}{2} \times 3 \times \nicefrac{1}{3} \mathcal{C} = \nicefrac{1}{2}\mathcal{C}$ (forward pass requires $\nicefrac{1}{3}^{rd}$ of the total cost, on $3\times$ the required data, the required data size being $\nicefrac{1}{2}$ when compared to naive). The combined cost is the sum of selection and training cost, which is $\mathcal{C}$. Hence, expensive sampling methods have an equivalent budget with $\nicefrac{1}{2}$ training iterations.
%have an additional forward pass have the computational cost of $(\nicefrac{1}{3}+1) \mathcal{C}$, i.e. . Hence, the number of iterations is $\nicefrac{3}{4}$ 

\section{Additional Results}

Due to limited space, some of the experiments in the manuscripts were deferred to the Appendix. In this section we present results for all mentioned costly sampling methods and distillation methods. Additional results for   
\textbf{\textit{Section 4.2: 1 ``Do Sampling Strategies Matter?"}} are presented in Figure \ref{fig:expensive_sampling} where all three costly sampling methods are presented, namely Max Loss, Uncertainty Loss, and KMeans. Similarly, additional results for \textbf{\textit{Section 4.2: 2 ``Does Distillation Matter?"}} are presented in Figure \ref{fig:distillation_main} where all four distillation methods are presented, namely BCE, MSE, Cosine, and CrossEntropy. We observe that all previous conclusions consistently hold, \ie the Naive baseline is still leading in comparison to all previous approaches.


We also extend the time steps and the number of iterations sensitivity experiments to all four considered distillation methods in all three setups, DI-ImageNet2K, CI-ImageNet2K and CGLM. We present additional results for \textit{\textbf{Section 4.3: 1. Does the Number of Time Steps Matter?}}: with results for DI-ImageNet2K presented in Figures \ref{DIImageNet50} and \ref{DIImageNet200} with 50 and 200 time steps respectively, CI-ImageNet2K in Figure \ref{CIImageNet50} and \ref{CIImageNet200} with 50 and 200 time steps respectively and CGLM in Figures \ref{CGLM50} and \ref{CGLM200}  with 50 and 200 time steps respectively. We observe that all previous conclusions consistently hold, \ie the conclusions are robust to changing time steps for a given cost $\mathcal{C}$.

We present additional results for \textit{\textbf{Section 4.3: 2. Does the Compute Budget Matter?}}: on DI-ImageNet2K in Figures \ref{DIImageNet100} and \ref{DIImageNet1200} for 100 and 1200 iterations respectively, and CI-ImageNet2K in Figures \ref{CIImageNet100} and \ref{CIImageNet1200} for 100 and 1200 iterations respectively and on CGLM in Figures \ref{CGLM40} and \ref{CGLM400} for 40 and 400 iterations respectively. We observe that all previous conclusions consistently hold, \ie the conclusions are robust to changing computational cost, towards both harsher and laxer computational constraint regimes.




\begin{figure*}[htbp!]
    \centering
    \includegraphics[width=\textwidth]{new_figures/suppl/costly_all.png}
    \caption{\textbf{Expensive Sampling.} As mentioned in the manuscript, KMeans performs the best among expensive sampling techniques such as Max Loss and Uncertainty Loss. Nevertheless, the performance of the expensive sampling methods is worse than simple Naive.}
    \label{fig:expensive_sampling}
\end{figure*}


\begin{figure*}[htbp!]
    \centering
    \includegraphics[width=\textwidth]{new_figures/suppl/distill_all_title.png}
    \caption{\textbf{Distillation Methods.} All four studied distillation methods under perform compared to the simple Naive baseline in all three studied settings. ImageNet experiments are allowed 400 iterations whereas CGLM is allowed 100 iterations.}
    \label{fig:distillation_main}
\end{figure*}




\begin{figure}[htbp!]
    \centering
\includegraphics[width=\textwidth]{new_figures/suppl/random_50steps.png}
    \caption{\textbf{DI-ImageNet2K 50 Time Steps.} As observed in the manuscript, when the number of time steps, distillation methods still under perform compared to the Naive baseline.}
    \label{DIImageNet50}
\end{figure}


\begin{figure}[htbp!]
    \centering
\includegraphics[width=\textwidth]{new_figures/suppl/random_200time.png}
    \caption{\textbf{DI-ImageNet2K 200 Time Steps.} As observed in the manuscript, when the number of time steps, distillation methods still under perform compared to the Naive baseline.}
    \label{DIImageNet200}
\end{figure}



\begin{figure}[htbp!]
    \centering
\includegraphics[width=\textwidth]{new_figures/suppl/clsinc_50steps.png}
    \caption{\textbf{CI-ImageNet2K 50 Time Steps.} As observed in the manuscript, when the number of time steps, distillation methods still under perform compared to the Naive baseline.}
    \label{CIImageNet50}
\end{figure}

\begin{figure}[htbp!]
    \centering
\includegraphics[width=\textwidth]{new_figures/suppl/clsinc_200time.png}
    \caption{\textbf{CI-ImageNet2K 200 Time Steps.} As observed in the manuscript, when the number of time steps, distillation methods still under perform compared to the Naive baseline.}
    \label{CIImageNet200}
\end{figure}




\begin{figure}[htbp!]
    \centering
\includegraphics[width=\textwidth]{new_figures/suppl/gld_50time.png}
    \caption{\textbf{CLGM 50 Time Steps.} As observed in the manuscript, when the number of time steps, distillation methods still under perform compared to the Naive baseline.}
    \label{CGLM50}
\end{figure}



\begin{figure}[htbp!]
    \centering
\includegraphics[width=\textwidth]{new_figures/suppl/gld_200time.png}
    \caption{\textbf{CGLM 200 Time Steps.} As observed in the manuscript, when the number of time steps, distillation methods still under perform compared to the Naive baseline.}
    \label{CGLM200}
\end{figure}


\begin{figure}[htbp!]
    \centering
\includegraphics[width=\textwidth]{new_figures/suppl/random100.png}
    \caption{\textbf{DI-ImageNet2K - 100 Iterations.} As observed in the manuscript, with reduced compute, distillation methods still under perform compared to the Naive baseline. The compute budget of the Naive baseline, $\mathcal{C}$, is set to 100 iterations whereas that of the distillation methods is $\nicefrac{2}{3}~\mathcal{C} = 67$ iterations.}
    \label{DIImageNet100}
\end{figure}

\begin{figure}[htbp!]
    \centering
\includegraphics[width=\textwidth]{new_figures/suppl/random1200.png}
    \caption{\textbf{DI-ImageNet2K - 1200 Iterations.} As observed in the manuscript, with increased compute, distillation methods still under perform compared to the Naive baseline. The compute budget of the Naive baseline, $\mathcal{C}$, is set to 1200 iterations whereas that of the distillation methods is $\nicefrac{2}{3}~\mathcal{C} = 800$ iterations.}
    \label{DIImageNet1200}
\end{figure}

\begin{figure}[htbp!]
    \centering
\includegraphics[width=\textwidth]{new_figures/suppl/clsinc100.png}
    \caption{\textbf{CI-ImageNet2K - 100 Iterations.} As observed in the manuscript, with reduced compute, distillation methods still under perform compared to the Naive baseline. The compute budget of the Naive baseline, $\mathcal{C}$, is set to 100 iterations whereas that of the distillation methods is $\nicefrac{2}{3}~\mathcal{C} = 67$ iterations.}
    \label{CIImageNet100}
\end{figure}

\begin{figure}[htbp!]
    \centering
\includegraphics[width=\textwidth]{new_figures/suppl/clsinc1200.png}
    \caption{\textbf{CI-ImageNet2K - 1200 Iterations.} As observed in the manuscript, with increased compute, distillation methods still under perform compared to the Naive baseline. The compute budget of the Naive baseline, $\mathcal{C}$, is set to 1200 iterations whereas that of the distillation methods is $\nicefrac{2}{3}~\mathcal{C} = 800$ iterations.}
    \label{CIImageNet1200}
\end{figure}


\begin{figure}[htbp!]
    \centering
\includegraphics[width=\textwidth]{new_figures/suppl/gld40.png}
    \caption{\textbf{CGLM 40 - Iterations.} As observed in the manuscript, with reduced compute, distillation methods still under perform compared to the Naive baseline. The compute budget of the Naive baseline, $\mathcal{C}$, is set to 40 iterations whereas that of the distillation methods is $\nicefrac{2}{3}~\mathcal{C} = 27$ iterations.}
    \label{CGLM40}
\end{figure}

\begin{figure}[htbp!]
    \centering
\includegraphics[width=\textwidth]{new_figures/suppl/gld400.png}
    \caption{\textbf{CGLM 400 - Iterations.} As observed in the manuscript, with increased compute, distillation methods still under perform compared to the Naive baseline. The compute budget of the Naive baseline, $\mathcal{C}$, is set to 400 iterations whereas that of the distillation methods is $\nicefrac{2}{3}~\mathcal{C} = 267$ iterations.}
    \label{CGLM400}
\end{figure}
\clearpage

\subsection{Effect of Weight Decay}

The choice of weight decay, $\text{wd}=0$, in the manuscript was based on  result of cross-validation from the set. More specifically, we try weight decays of $\{5\times 10^{-5}, 1\times 10^{-4}\}$. We observe a minor difference in performance between various weight decays, with a wd=$0$ consistently being slightly better. The results are shown in Figure \ref{fig:wd_ablation}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{new_figures/suppl/wd_rebuttal.pdf}
    \caption{\textbf{Effect of Weight Decay.} Increasing the weight decay causes a slight drop in performance. Setting wd=$0$ gave the best results during our parameter cross-validation.}
    \label{fig:wd_ablation}
\end{figure}

\subsection{Effect of Batch Size}
In all experiments, we fixed the batch size (BS) to 1500 to optimize the utilization of our hardware resources and minimize the training time. As shown in the literature, BS and learning rate (LR) are closely related. The selected LR was tuned to fit the selected BS. Regardless, we present varying BS experiments in Fig (1)
where we study the latest FC correction method, ACE, and the distillation method, MSE, for BS of 250 and 500 with  increased iterations of 600 and 300, respectively, and crossvalidated learning rates. We observe that the Naive baseline
is still superior even when the batch size is adjusted. Our findings are summarized in Figure \ref{fig:bs_ablation}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{new_figures/suppl/bs_rebuttal.pdf}
    \caption{\textbf{Effect of Batch Size.} The conclusions presented in our work hold even when the batch size while is changed while the same overall computational budget.}
    \label{fig:bs_ablation}
\end{figure}

\subsection{Effect of Increasing Computational Budget on Distillation}

We complement the results in Figure \ref{fig:diffstepscglm} with additional experiments using $800$ and $1200$ iterations.  The observed results align with our previous observations; as long as the computation is normalized across methods, naive, the most simplest among the considered methods, outperforms existing methods. This is as opposed to prior art comparison that does not normalize compute, which puts the Naive baseline in disadvantage. The results are shown in Figure \ref{fig:distill_steps_rebuttal}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\linewidth]{new_figures/suppl/distill_iterations_rebuttal.pdf}
    \caption{\textbf{Effect of Increasing Computation Budget on Distillation.} As the computation budget is increased while maintaining a normalized compute among different methods, Naive baseline still outperforms distillation based methods.}
    \label{fig:distill_steps_rebuttal}
\end{figure}


% \clearpage
% %%%%%%%%% REFERENCES
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

% \clearpage


% \end{document}
