\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

\usepackage{arxiv23}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amssymb} %new
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}

% Comment out this line in the camera-ready submission
%\linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

\title{GADFORMER: AN ATTENTION-BASED MODEL FOR\\GROUP ANOMALY DETECTION\\ON TRAJECTORIES}

\author{
Andreas Lohrer
\and
Darpan Malik\and
Peer Kröger
\affiliations
Kiel University
\emails
\{alo,pkr\}@informatik.uni-kiel.de,
stu225397@mail.uni-kiel.de
}

\begin{document}

\maketitle

\begin{abstract}
Group Anomaly Detection (GAD) reveals anomalous behavior among groups consisting of multiple member instances, which are, individually considered, not necessarily anomalous. This task is of major importance across multiple disciplines, in which also sequences like trajectories can be considered as a group. However, with increasing amount and heterogenity of group members, actual abnormal groups get harder to detect, especially in an unsupervised or semi-supervised setting. Recurrent Neural Networks are well established deep sequence models, but recent works have shown that their performance can decrease with increasing sequence lengths. Hence, we introduce with this paper GADFormer, a GAD specific BERT architecture, capable to perform attention-based Group Anomaly Detection on trajectories in an unsupervised and semi-supervised setting. We show formally and experimentally how trajectory outlier detection can be realized as an attention-based Group Anomaly Detection problem. Furthermore, we introduce a Block Attention-anomaly Score (BAS) to improve the interpretability of transformer encoder blocks for GAD. In addition to that, synthetic trajectory generation allows us to optimize the training for domain-specific GAD. In extensive experiments we investigate our approach versus GRU in their robustness for trajectory noise and novelties on synthetic and real world datasets.
\end{abstract}

%paper structure:

% 1 Introduction
% 2 Preliminaries and Problem Definition
% 2.1 Preliminaries
% 2.2 Problem Definition
% 3 GADFormer
% 3.1 Architecture and Loss Objective
% 3.2 Training
% 3.3 Interpretability
% 3.4 Pre-Training and Domain Adaption
% 3.5 Trajectory Generation
% 4 Experiments
% 4.1 Experimental Setup and Datasets
% 4.2 Evaluation
% 4.3 Results and Discussion
% 5 Related Work
% 6 Conclusion

\section{Introduction}
\label{01.intro}

Group Anomaly Detection (GAD) is an important task across many disciplines and domains like social networks~\cite{Yu2014}, 
mobility~\cite{10.1145/3557991.3567801}, physics~\cite{PhysRevD.101.075042,10.5555/3023638.3023684} and many more. In these domains, GAD is suitable for various types of group anomalies. Since group member instances can be an arbitrary representation, the GAD paradigm also applies to the detection of anomalous sequences like trajectories, to which~\cite{Foorthuis2020OnTN} refers to as collective anomalies. Especially in the spatio-temporal domain, Trajectory Anomaly Detection is a common task to reveal abnormal behavior as the authors~\cite{Wang2020UnsupervisedLT,7966366,10.1145/3430195} confirm.

However, although sequential steps of a trajectory obviously represent a group structure, the detection of individual anomalous trajectories has not been addressed as a group anomaly detection problem yet.

The current state of the art approaches for anomaly detection on trajectories are recurrent neural networks (RNNs) like LSTMs~\cite{10.1162/neco.1997.9.8.1735} and GRUs~\cite{Cho2014GRU,7966366}, but the potential of deep learning methods for Group Anomaly Detection has rather been sparsely investigated. So far GAD tasks have more likely been solved by generative topic models ~\cite{10.5555/944919.944937,Xiong2011HierarchicalPM,Yu2014,10.5555/2986459.2986579} or SVM-based methods~\cite{6790022,10.5555/3023638.3023684,10.1145/3557991.3567801}. 
Despite recent advances, deep generative models got only involved in form of Adversarial Autoencoders (AAEs) and Variational Autoencoders (VAEs)~\cite{DBLP:conf/pkdd/ChalapathyTC18} to perform GAD for images. \cite{10.1145/3430195} offers different machine learning based algorithms to detect anomalous groups of multiple trajectories, but does not identify the detection of individual anomalous trajectories (a sequence of group members) as group anomaly detection problem.

However, anomalous behavior is not ensured to just appear within short trajectory segments. It is challenging for recurrent neural networks, sometimes even LSTM and GRU, to learn very long-term dependencies~\cite{7966366}.

A further challenge for deep learning based group anomaly detection on trajectories is, that it is due the high availability of trajectory data not uncommon, that they are rather weakly labeled or do not overcome the nonground-truth problem~\cite{Wang2020UnsupervisedLT} at all.

In order to tackle these challenges we introduce our approach GADFormer, a BERT\cite{DBLP:conf/naacl/DevlinCLT19} based architecture with transformer\cite{10.5555/3295222.3295349} encoder blocks for attention-based group anomaly detection on trajectories. Extending the idea of ~\cite{10.5555/3295222.3295349} to optimize also image, audio or video sequence tasks by their transformer approach, we identify transformer based models for a sequence of trajectory points/segments as group member instances of a group anomaly detection task as similarly beneficial. Our model can be trained in an unsupervised as well as in an semi-supervised setting so that there is no or only reduced need for labeled trajectories. Furthermore, we introduce a Block Attention-anomaly Score (BAS), which allows us to provide an interpretable view to the capability of the transformer encoder blocks to distinguish normal from abnormal trajectory attention matrices. We show with extensive experiments on synthetic and real world datasets that our approach is on par with the state of the art methods.

Hence, the contributions of our work can be summarized as follows:

\begin{itemize}
\item Transformer-Encoder-architecture capable to perform attention-based group anomaly detection in an unsupervised and semi-supervised setting. 
\item Identification of the detection of individual anomalous trajectories as Group Anomaly Detection problem for BERT based transformer models. 
\item Block Attention-anomaly Score for transformer encoder block interpretability for the task of GAD.
\item Extensive ablation and robustness studies addressing trajectory noise, novelties and standardization.
\end{itemize}

The remainder of this work is structured as follows. Section~\ref{02.definitions} introduces the a formal description of the addressed problem before in Section~\ref{03.methodology} the architecture, training and interpretability of GADFormer is proposed. The experiments in Section~\ref{04.experiments} demonstrate relevance and suitability of our approach across multiple domains and Section~\ref{05.relatedwork} distinguishes our approach from related work. A final summary of the paper as well as an outline to future work is given by Section~\ref{06.conclusion}.

\section{Preliminaries and Problem Definition}
\label{02.definitions}

This section provides preliminary terminology and definitions used in this work so far not referenced otherwise.

\subsection{Preliminaries}
\noindent\textit{Group Anomaly Detection (GAD)} aims to identify groups that deviate from the regular group pattern\cite{DBLP:conf/pkdd/ChalapathyTC18}.\\
\noindent \textit{Group} is a set or sequence of at least two group member instances.\\
\noindent \textit{Group Member Instance} is an arbitrary data entity described by a n-dimensional feature vector as part of a group.\\
\noindent \textit{[Group Anomaly or] Collective Anomaly} refers to a collection of data points that belong together and, as a group, deviate from the rest of the data.\cite{Foorthuis2020OnTN}

\subsection{Problem Definition}

The definitions for GAD align with~\cite{DBLP:conf/pkdd/ChalapathyTC18} for deep generative models, but got partially a different notation to emphasize its suitability for group anomaly detection on individual trajectories. The GAD problem is described as follows:

Let $x_{n} \in X$ be an instance with $X = (x_{1}, x_{2}, x_{3}, ..., x_{N})$ and $x_{n} = (a_{1}, a_{2}, a_{3}, ..., a_{V})$ with attribute $a_{v} \in \mathcal{F}$, the feature space, with 

\begin{equation*}
a_{v} =
\left\{
\begin{array}{l}
continuous, a_{v} \in \mathbb{R} \\
discrete, a_{v} \in \mathbb{N} \\
categorical, a_{v} \in \{0, 1\} 
\end{array}
\right.
\end{equation*}

Be $x_{n_{m}}$ a group member instance $o_{i}$ of the $m$th group $\mathcal{G}_{m}$ with 

\begin{equation}
\mathcal{G}_{m} = (o_{1}, o_{2}, o_{3}, ..., o_{N_{m}})
% o_{\vert\mathcal{G}_{m}\vert})
\end{equation}

and $\mathcal{D}_{GAD}$ a group anomaly detection dataset, which is a set $\mathcal{G}$ of all groups:

\begin{equation}
\mathcal{D}_{GAD} = (\mathcal{G}_{1}, \mathcal{G}_{2}, \mathcal{G}_{3}, ..., \mathcal{G}_{M})
\end{equation}

The objective of the group anomaly detection task is to distinguish normal in-distribution groups from abnormal out-of-distribution groups $\mathcal{G}$$_{\mathcal{A}}$ with the help of a pseudo group $\mathcal{G}^{(ref)}$ as an approximated reference for normal in-distribution groups. Therefore, a characterization function $f$ with 

\begin{equation}
\label{eq:GADcharacterizationFunction}
f_{\Theta}: \mathbb{R}^{N_{m}\times V} \rightarrow \mathbb{R}^{D}
\end{equation}

and an aggregation function $g$ with

\begin{equation}
\label{eq:GADaggregationFunction}
g_{\phi}: \mathbb{R}^{D} \rightarrow \mathbb{R}^{D}
\end{equation}

compose to 

\begin{equation}
\mathcal{G}^{(ref)} = g_{\phi}(f_{\Theta}(\mathcal{G}))
\end{equation}

where $f_{\Theta}$ maps the groups $\mathcal{G}_{m}$ to $D$-dimensional feature vectors representing the relationship characteristics of its group members $o_{i}$ and $g_{\phi}$ aggregates them to one $D$-dimensional feature vector representing one reference $\mathcal{G}^{(ref)}$ for the distribution of normal groups.

Finally, the abnormality of a group is defined by a group anomaly score $y_{score}$ measuring the deviation by a distance measure $d(\cdot,\cdot) \geq 0$, between $\mathcal{G}_{m}$ and the normal group reference $\mathcal{G}^{(ref)}$. Thus, the abnormality score $y_{score}$ is defined as follows:

\begin{equation}
    y_{score} =  d(\mathcal{G}^{(ref)}, \mathcal{G}_{m})
\end{equation}

whereby the decision between normal and abnormal groups is defined by a threshold $\gamma$ with 

\begin{equation}
y_{label} =
\left\{
\begin{array}{l}
    1, \quad y_{score} \geq \gamma \\
    0, \quad otherwise
\end{array}
\right.
\end{equation}

Having the group anomaly detection problem described according to ~\cite{DBLP:conf/pkdd/ChalapathyTC18}, we also elaborate on the task of trajectory anomaly detection aligning with the notations of ~\cite{10.1145/3430195} with their slightly different problem of group trajectory anomaly detection instead of the here described individual trajectory anomaly detection:

A trajectory point $p$ is defined as 
\begin{equation}
p = (a_{1}, a_{2}, a_{3}, ..., a_{V})    
\end{equation}
%todo: attribute constraints for trajectory point p

A trajectory segment $s$ is defined as 
\begin{equation}
s = (p_{1}, p_{2}, p_{3}, ..., p_{L})    
\end{equation}

A trajectory $\Lambda$ is defined as 
\begin{equation}
\Lambda_{m} = (s_{1}, s_{2}, s_{3}, ..., s_{N_{m}})    
\end{equation}

A trajectory dataset $\mathcal{D}_{Traj}$ is defined as 
\begin{equation}
\mathcal{D}_{Traj} = (\Lambda_{1}, \Lambda_{2}, \Lambda_{3}, ..., \Lambda_{M})
\end{equation}


Considering the task of detecting abnormal individual trajectories as group anomaly detection problem, the following associations are identified:

A trajectory $\Lambda_{m}$ applies to the semantic of a group $\mathcal{G}_{m}$ by considering trajectory segments $s$ as group members $o$, including the case of segment length L = 1, where one group member $o_{i}$ is represented by only one trajectory point $p_{i}$. A trajectory point $p_{i}$ is associated with an instance $x_{n}$.

Thus, individual abnormal trajectories can be detected similarly to the group anomaly detection problem as follows:

\begin{equation}
\Lambda^{(ref)} = g_{\phi}(f_{\Theta}(\Lambda))
\end{equation}

\begin{equation}
    \hat{y}_{score} =  d(\Lambda^{(ref)}, \Lambda_{m})
\end{equation}

After revealing the associations between the GAD approach of~\cite{DBLP:conf/pkdd/ChalapathyTC18} and our related proposal for trajectories, also our proposed GADFormer approach (cf. Section~\ref{03.methodology}) is potentially able to be trained for each arbitrary group anomaly detection problem on sequences or non-ordered sets revealing abnormal patterns as groups.

\section{GADFormer}
\label{03.methodology}

In this section we propose GADFormer, a deep BERT based transformer encoder model architecture for attention-based Group Anomaly Detection (GAD). After we showed in Section~\ref{02.definitions} by the example of~\cite{DBLP:conf/pkdd/ChalapathyTC18} theoretically that the GAD problem can also be applied to trajectories, we introduce with GADFormer a new deep GAD model in this section and demonstrate its performance on trajectory datasets in Section~\ref{04.experiments}. Figure~\ref{fig:GADFormerArch} provides an overview to the GADFormer architecture in combination with the example of 2D trajectory point inputs, but also high-dimensional group members (trajectory points) are possible.

\begin{figure*}[hbt!]
\centering
\includegraphics[width=\textwidth]{GADFormer_Architecture_Overview.jpg}
\caption[]{GADFormer architecture overview.}
\label{fig:GADFormerArch}
\end{figure*}


\subsection{Architecture and Loss Objective}

Differently to the GAD characterization and aggregation function (cf. Eq.~\ref{eq:GADcharacterizationFunction} and Eq.~\ref{eq:GADaggregationFunction}) of the deep generative models of~\cite{DBLP:conf/pkdd/ChalapathyTC18} our deep bidirectional GADFormer $\Psi$ models the characterization and aggregation functions as follows with

\begin{equation}
    \Psi: g_{\Phi}(f_{\theta}(\Lambda_{m})) \rightarrow \hat{p}_{m}.
\end{equation}

%todo: anderer Buchstabe für trajectory point p -> ?
The characterization function $f_{\theta}$ of GADFormer maps the bidirectional relationships between group members $o_{i}$ of a group $\mathcal{G}$$_{m}$ (representations of trajectory point $p$ or segment $s$ of trajectory $\Lambda_{m}$) to an attention-based feature map $b_{\mathcal{G}_{m}}$, representing the behavior of an individual group (the individual trajectory path). This is realized by a BERT~\cite{DBLP:conf/naacl/DevlinCLT19} encoder, a composition of layers for input embedding, positional encoding and multi-head self-attention blocks (cf. Figure~\ref{fig:GADFormerArch}) using group member embeddings $pe$ as input tokens.

\begin{equation}
    b_{\Lambda_{m}} = f_{\theta}(\Lambda_{m})
\end{equation}

%todo: verify normal/abnormal
The aggregation function $g_{\Phi}$ of GADFormer approximates instead of a distribution for normal group representations $\mathcal{G}^{(ref)}$ with distance measure $d$ a probability $p_{m}$ for abnormal group behavior (abnormal trajectory path). This is realized by non-linear custom task layers of the output block, which maps the group behavior characteristics $b_{\Lambda_{m}}$ to a task specific feature map representation $z_{\Lambda_{m}}$. 

\begin{equation}
    z_{\Lambda_{m}} = g_{\Phi}(b_{\Lambda_{m}})
\end{equation}

Afterwards, a sigmoid function maps this representation to a probability $\hat{p}_{m}$ for group abnormality, with $\hat{p}_{m} \in [0, 0.5]$ for normal groups and $\hat{p}_{m} \in ]0.5, 1]$ for abnormal groups (trajectories).

\begin{equation}
    \hat{p}_{m} = \sigma(z_{\Lambda_{m}})
\end{equation}

%todo: verify normal/abnormal
Because of the rare label availability for groups (trajectories), the loss objective of GADFormer is defined for an unsupervised and semi-supervised learning setting aussuming the majority dataset instances to be normal. Therefore, we define the binary cross entropy loss $\mathcal{L}_{BCE}$ as our loss function (cf. Eq.~\ref{eq:loss}). We consider this loss function as a suitable choice, since entropy $H(\hat{p}_{m})$ as a measure of unpredictability is $H(\hat{p}_{m}) = 1$ when the model is most uncertain about its abnormality prediction, and $H(\hat{p}_{m}) = 0$ when the model is very certain about its abnormality prediction. 

\begin{equation}
H(\hat{p}_{m}) =
\left\{
\begin{array}{l}
    1, \quad \hat{p}_{m}=0.5 \\
    0, \quad \hat{p}_{m}=0 \land \hat{p}_{m}=1 \\
    ]0,1[, \quad otherwise 
\end{array}
\right.
\end{equation}

Due to the heavily imbalanced learning setting with a large majority of normal groups (trajectories) one can neglect the minority of abnormal groups (trajectories) and set a fix auxiliary target probability~$p_{m}=0$ for certain normal-predictions~($H(\hat{p}_{m}) = 0$) for the majority of normal group probabilities~$\hat{p}_{m}$. In case the model faces true abnormal groups, then it is rather uncertain about its decision yielding a probability close to $\hat{p}_{m}=0.5$ resulting in a high entropy loss, whereas true normal groups on whose pattern the model is trained, result in a low entropy loss for $\hat{p}_{m} \approx 0$.

\begin{gather}
\label{eq:loss}
\begin{aligned}
    &\mathcal{L}_{BCE} = -\frac{1}{M} \sum_{m=1}^{M} p_{m} log_{b}(\hat{p}_{m}) + (1 - p_{m}) log_{b}(1 - \hat{p}_{m}) \\
    & \overset{p_{m}=0}{\Longleftrightarrow} \\
    &\mathcal{L}_{BCE} = -\frac{1}{M} \sum_{m=1}^{M} log_{b}(1 - \hat{p}_{m})
\end{aligned}
\end{gather}

Since in our setting the model effectively predicts only abnormality probabilities for the range of normal groups with $\hat{p}_{m}~\in~[0, 0.5]$, where $\hat{p}_{m}=0$ means that a group (trajectory) is not abnormal at all, the abnormality of a group is defined by a group anomaly score as follows
\begin{equation}
    \hat{y}_{score} = \hat{p}_{m}. 
\end{equation}
for our GADFormer approach.
 
\subsection{Training}
Anomalous trajectories are rare by definition and labeling by domain experts tends to be rather expensive. We address this challenge by two different learning settings which are: 1) Unsupervised learning, which requires no labels at all under the assumption that the ratio of anomalous trajectories is low and has no remarkable influence during model training. 2) Semi-Supervised Learning, which relies on verified normal samples only. As proposed in the section before, these learning settings allow us to set a fix auxiliary target probability $p_{m}=0$, so that no ground truth for abnormal trajectories is needed for the GADFormer training.

Algorithm~\ref{alg:algGADFormer} describes the training steps of our approach. The inputs of our algorithm are trajectory dataset $\mathcal{D}_{\Lambda}$ as group dataset $\mathcal{D}_{\mathcal{G}}$, the GADFormer model $\Phi$ with initialized parameters and the loss objective function $\mathcal{L}_{BCE}$. Furthermore, we use the algorithm parameters dataset split ratios for train, validation and test, a model optimizer, the total number of epochs, batch size bs, learning rate $\eta$, weight decay and learning rate schedulers with patience parameter are in use. The output of the algorithm is model $\Psi_{best}$ with parameters leading to the lowest validation loss, its GA scores $\hat{y}_{score_{[trn,vld,tst]}}$ and the related losses $\mathcal{L}_{[trn,vld,tst]}$. After variable initialization (lines 1-4) we repeat the training for the given number of epochs or until the model converges (line 5, 23, 24). In each training epoch (lines 5-25) we pass the group (trajectory) batches to the model, return its group abnormality probability $\hat{p}$ and use it to calculate the binary cross entropy loss $\mathcal{L}$ and the related group anomaly score $\hat{y}_{score}$. This is done for the training set (lines 6-11), the validation set (lines 12-16) and test set (lines 26-30) equally, just during training the gradients are calculated and model weight updates are done (line 9). At the end of each training epoch the best model with losses and GA scores is kept saved (lines 17-21) based on the validation loss $\mathcal{L}_{vld}$. Finally, as described for the output, the best model with related losses and group anomaly scores is returned (line 31).

\begin{algorithm}[!htb]
    \caption{GADFormer Training Algorithm Pseudo Code}
    \label{alg:algGADFormer}
    \textbf{Input}: groups $\mathcal{D}_{\mathcal{G}}$, model $\Psi$, loss objective $\mathcal{L}$ \\
    \textbf{Parameter}: ratios, optimizer, epochs, bs, lr $\eta$, wd, sched, patience \\
    \textbf{Output}: model $\Psi_{best}$, GA scores $\hat{y}_{score_{[trn,vld,tst]}}$, losses $\mathcal{L}_{[trn,vld,tst]}$ \\
    \begin{algorithmic}[1] 
        \STATE $epoch=0, \mathcal{L}_{best}=\infty$, earlystop=0, best=$\emptyset$.
        \STATE opt = optimizer($\Psi$, $\eta$, wd, sched).
        \STATE split $\mathcal{D}_{\mathcal{G}}$ into $\mathcal{D}_{train}$, $\mathcal{D}_{valid}$, $\mathcal{D}_{test}$ by ratios.
        \STATE $p=0$.
        \WHILE{epoch $<$ epochs or earlystop $>$ patience}
        \FORALL{$\mathcal{G}_{m}$ in $\mathcal{D}_{train}$}
        \STATE $\hat{p}_{trn} = \Psi(\mathcal{G}_{m})$
        \STATE $\mathcal{L}_{trn} = \mathcal{L}(p, \hat{p}_{trn})$
        \STATE $w_{\Psi} = w_{\Psi} - \eta \frac{\partial \mathcal{L}_{trn}}{\partial \mathcal{G}_{m}}$
        \STATE $\hat{y}_{score_{trn}} = \hat{p}_{trn}$
        \ENDFOR
        \FORALL{$\mathcal{G}_{m}$ in $\mathcal{D}_{valid}$}
        \STATE $\hat{p}_{vld} = \Psi(\mathcal{G}_{m})$
        \STATE $\mathcal{L}_{vld} = \mathcal{L}(p, \hat{p}_{vld})$
        \STATE $\hat{y}_{score_{vld}} = \hat{p}_{vld}$
        \ENDFOR
        \IF {$\mathcal{L}_{vld} < \mathcal{L}_{best}$}
        \STATE $\mathcal{L}_{best} = \mathcal{L}_{vld}$.
        \STATE best = ($\Psi,\hat{y}_{score_{[trn,vld]}},\mathcal{L}_{[trn,vld]}$) 
        \STATE earlystop=0
        \ENDIF
        \STATE $\eta = sched(\eta, \mathcal{L}_{vld})$.
        \STATE earlystop+=(0,1)[$\mathcal{L}_{vld} \geq \mathcal{L}_{best}$]
        \STATE epoch+=1
        \ENDWHILE
        \FORALL{$\mathcal{G}_{m}$ in $\mathcal{D}_{tst}$}
        \STATE $\hat{p}_{tst} = \Psi_{best}(\mathcal{G}_{m})$
        \STATE $\mathcal{L}_{tst} = \mathcal{L}(p, \hat{p}_{tst})$
        \STATE $\hat{y}_{score_{tst}} = \hat{p}_{tst}$
        \ENDFOR
        \STATE \textbf{return} best $\cup (\hat{y}_{score_{tst}},\mathcal{L}_{tst}$)
    \end{algorithmic}
\end{algorithm}

\subsection{Interpretability}

Deep learning models are known to be rather complex and their training usually requires a deep understanding for its model architecture, losses, preprocessing and data distributions to take the right decisions for fine-tuning, but still then it partially remains a blackbox as more layers, blocks and parameters exist.

In order to achieve a higher model interpretability addressing \textit{CH6}, one of the main deep anomaly detection challenges of~\cite{10.1145/3439950}, we introduce a Block Attention-anomaly Score (BAS) for our GADFormer model. This score represents a group abnormality by the ratio of a block attention matrix weighted output and the average outputs of a transformer encoder block layer. This allows to show the capability of a transformer encoder block layer to 1) generally distinguish pattern of different groups (trajectories) and 2) to separate between normal and abnormal groups (individual trajectories).

Since the cells of an attention matrix $a_{m,h,b}$ contain the scaled dot product of $q_{m,h}$ and $k_{m,h}$ (two projected group embeddings projected from input tokens $pe$ which are either embeddings of trajectory points $p$ or embeddings of trajectory segments $s$ as group members $o$), their similarity weights the importance of the bidirectional relationship of a group member pair in different heads $h$ and with that, focuses with different views to the behavioral pattern of a group (trajectory path pattern in the context of the task of GAD on individual trajectories), whereas its concatenated projected dot product with the third projected group member embeddings $v_{m,h}$ provides the overall-importance-weighted attention output matrix $O_{m,b}$ emphasizing task relevant behavior of a concrete group $m$ (task relevant trajectory path segments of an individual trajectory $\Lambda$$_{m}$).

The BAS for each group within each transformer encoder block is calculated by Algorithm~\ref{alg:algBAS}. The inputs of this algorithm are group attention matrices of the transformer encoder blocks and the euclidean distance measure. Further parameters are the ratio for the top N abnormal groups, block index b and groups M.

\begin{algorithm}[!htb]
    \caption{BAS Algorithm Pseudo Code}
    \label{alg:algBAS}
    \textbf{Input}: group attention matrices $a$, distance measure $d$ \\
    \textbf{Parameters}: $ratio_{topN}=0.05$, block b, groups M \\
    \textbf{Output}: block attention-anomaly scores $bas$ \\
    \begin{algorithmic}[1]
        
        % avg normal score and matrix  
        \STATE $a_{m,b} = \mu(a_{m,h,b})$ % avg attn of all heads groups in block layer b (avg over heads)
        \STATE $a_{b} = \mu(a_{m,b})$ % avg attn of all groups in block layer b (avg over groups)
        \STATE
        %find topN distant group indices
        \STATE $topN=ratio_{topN}*M$. % number of topN most distant matrices
        \STATE $idx_{dsc,b} = rank(d(a_{m,b}, a_{b}))_{dsc}$
        \STATE $idx_{topN,b} = idx_{dsc,b}[:topN]$
        %distant abnormal score, matrix and distance
        \STATE $a_{topN,b} = \mu(a_{m,b}[idx_{topN,b}])$ % avg-attn mu_attn of topN-distant groups
        \STATE
        \STATE $bas_{m,b} = min(1., \frac{d(a_{m,b}, a_{b})}{d(a_{topN,b},a_{b})})$
        
        %where 
        %1. means that x is 100% an anomaly and
        %0. means that x is 100% a normal group
        %for the related block b
        \RETURN $bas$
    \end{algorithmic}
\end{algorithm}

The average attention matrix $a_{m,b}$ for a group $m$ in heads $h$ is calculated for all attention matrices $a_{m,h,b}$ as well as for their average $a_{b}$ over all groups (line 1-2). According to the given $ratio_{topN}$ the top N most distant abnormal group attention matrices from $a_{b}$ are used to calculate an average abnormal group attention matrix $a_{topN,b}$ in order to have a solid abnormal representation for distance calculation (line 4-7). Next, the distance between a group attention matrix $a_{m,b}$ to the normal group attention matrix average $a_{b}$ as well as the distance between the normal group attention matrix average $a_{b}$ and the abnormal group attention matrix average $a_{topN,b}$ is used to request the ratio between both which represents the Block Attention-anomaly Score $bas_{m,b}$ (line 9).

Considering the following illustrations of the trajectory BAS within the four GADFormer encoder block layers, one observes at Figure~\ref{fig:BAS_goodmodel} that the first encoder block layers (0\nobreakdash-2) are not able to distinguish between normal and abnormal trajectories whereas in the last layer the amount of potential false positive scores gets less indicating a better capability of the model to extract relevant feature for abnormal trajectories.

The trajectory BAS within the layers of Figure~\ref{fig:BAS_badmodel} indicate that the model is over all layers not really able to distinguish between normal and abnormal trajectory scores, not even between the characteristics of single trajectories within the class of normal trajectories. 

In summary, the BAS provides us a view to the transformer encoder block layers and allows us to reason reasonable changes to hyperparameters and model architecture to improve the performance of the model.

\begin{figure}[hbt!]
\includegraphics[width=0.45\textwidth]{BAS_goodmodel.jpg}\centering
\caption[]{Block Attention-anomaly Score over all GADFormer Encoder Block layers uncovering the feature extraction capabilities of a good model performance.}
\label{fig:BAS_goodmodel}
\end{figure}

\begin{figure}[hbt!]
\includegraphics[width=0.45\textwidth]{BAS_badmodel.jpg}\centering
\caption[]{Block Attention-anomaly Score over all GADFormer Encoder Block layers uncovering the feature extraction capabilities of a bad model performance.}
\label{fig:BAS_badmodel}
\end{figure}


\section{Experiments}
\label{04.experiments}

In this section we evaluate the performance of our GADFormer approach on synthetic and real-world datasets and compare it against GRU, one of the state of the art methods for individual trajectory anomaly detection.

\subsection{Experimental Setup and Datasets}

For our experiments we used an Ubuntu Focal 20.04.5 LTS server with AMD Ryzen 7 3700X 8-Core Processor with 16~CPUs, 64GB RAM and a 16GB NVIDIA RTX A4000 GPU. We conducted our experiments on three trajectory datasets, which are an own synthetic dataset with several noise and novelty data variants (cf. Table~\ref{table:Tabular_dataset}) as well as two real-world datasets from amazon\footnote{\url{https://github.com/amazon-science/goal-gps-ordered-activity-labels}} describing walk or driving paths and brightkite~\footnote{\url{https://snap.stanford.edu/data/loc-brightkite.html}} with location-based social-networking provider trajectory data. Both have a maximal length of 72 steps. For the latter we have no labels available why we utilize a z-score range of ]-2.1;+2.1[ in order to label individual anomalous trajectories with maximal length of 500 steps based on their PCA embedding. We split our data in a train, valid and testset with a ratio of 0.9 for normal data for each set (except for semi-supervised training when no abnormal data is in the training set). Further details can be seen in Table~\ref{tab:datasets}.

\begin{table}[hbt!]
\tiny
    \centering
    \begin{tabular}{ccccccccccc}
\toprule
dataset  & set & all & n & a & trajLen \\
\toprule
synthe & unsu & 3400 & 3083 & 158 & 72 \\
synthe & semi & 3400 & 3271 & 129 & 72 \\
amazon & unsu & 805 & 757 & 48 & 72 \\
amazon & semi & 777 & 652 & 125 & 72 \\
bright & unsu & 2241 & 2052 & 189 & 500 \\
bright & semi & 2218 & 2052 & 66 & 500 \\
\bottomrule
    \end{tabular}
    \caption{Dataset Overview.}
    \label{tab:datasets}
\end{table}

\begin{table}[hbt!]
\tiny
    \centering
    \begin{tabular}{ccccccc}
\toprule
setting  & dataset & exp & model & scaler & roc & auprc \\
\toprule
U &  synthe &  noise .0 &  GADFormer    & standard & 0.997 & 0.975 \\
U &  synthe &  noise .0 &  GRU    & standard & 0.905 & 0.811 \\
U &  synthe &  noise .2 &  GADFormer   & standard & 0.987 & 0.907 \\
U &  synthe &  noise .2 &  GRU   & standard & 0.891 & 0.775 \\
U &  synthe &  noise .5 &  GRU    & standard & 0.904 & 0.513 \\
U &  synthe &  noise .5 &  GADFormer    & standard & 0.904 & 0.513 \\
\bottomrule
    \end{tabular}
    \caption{Results on synthetic dataset (unsupervised).}
    \label{tab:resUSnoise}
\end{table}

\begin{table}[hbt!]
\tiny
    \centering
    \begin{tabular}{ccccccc}
\toprule
setting  & dataset & exp & model & scaler & roc  & auprc \\
\toprule
E &  synthe &  noise .0 &  GADFormer   & standard & 0.978 & 0.912 \\
E &  synthe &  noise .0  &  GRU     & standard & 0.909 & 0.814 \\
E &   synthe &  noise .2 &  GADFormer    & standard & 0.96 & 0.864 \\
E &  synthe &  noise .2  &  GRU     & standard & 0.894 & 0.782 \\
E &   synthe &  noise .5 &  GADFormer    & standard & 0.916 & 0.737 \\
E &  synthe &  noise .5  &  GRU     & standard & 0.843 & 0.569 \\
\bottomrule
    \end{tabular}
    \caption{Results on synthetic dataset (semi-supervised).}
    \label{tab:resESnoise}
\end{table}


\begin{table}[hbt!]
\tiny
    \centering
    \begin{tabular}{ccccccc}
\toprule
setting  & dataset & exp & model & scaler & roc & auprc \\
\toprule
U &  synthe &  orig &  GRU  & robust & 0.946 & 0.733 \\
U &  synthe &  orig &  GADFormer   & robust & 0.803 & 0.302 \\
U &  amazon &  orig &  GADFormer   & robust & 0.722 & 0.104 \\
U &  amazon &  orig &  GRU  & robust & 0.496 & 0.093 \\
U &  bright &  orig &  GADFormer  & robust & 0.536 & 0.112 \\
U &  bright &  orig &  GRU        & robust & 0.502 & 0.132 \\
\bottomrule
    \end{tabular}
    \caption{Results on synthetic, amazon and brightkite dataset (unsupervised).}
    \label{tab:resUSbrightOrig}
\end{table}

\begin{table}[hbt!]
\tiny
    \centering
    \begin{tabular}{ccccccc}
\toprule
setting  & dataset & exp & model & scaler & roc & auprc \\
\toprule
E &  synthe &  orig &  GRU  & robust & 0.948 & 0.741 \\
E &  synthe &  orig &  GADFormer   & robust & 0.943 & 0.768 \\
E &  bright &  orig &  GADFormer   & robust & 0.59 & 0.149 \\
E &  bright &  orig &  GRU  & robust & 0.503 & 0.132 \\
E &  amazon &  orig &  GRU  & robust & 0.589 & 0.404 \\
E &  amazon &  orig &  GADFormer   & robust & 0.499 & 0.353 \\
\bottomrule
    \end{tabular}
    \caption{Results on synthetic, amazon and brightkite dataset (semi-supervised).}
    \label{tab:resESbrightOrig}
\end{table}


\begin{table}[hbt!]
\tiny
    \centering
    \begin{tabular}{ccccccc}
\toprule
setting  & dataset & exp & model & scaler & roc & auprc \\
\toprule
U &  synthe &  novelty .0 &  GADFormer  & standard & 0.997 & 0.975 \\
U &  synthe &  novelty .0 &  GRU   & standard & 0.905 & 0.811 \\
U &  synthe &  novelty .01 &  GADFormer  & standard & 0.993 & 0.852 \\
U &  synthe &  novelty .01  &  GRU   & standard & 0.938 & 0.854 \\
U &  synthe &  novelty .05 &  GADFormer  & standard & 0.965 & 0.565 \\
U &  synthe &  novelty .05 &  GRU   & standard & 0.951 & 0.881 \\
\bottomrule
    \end{tabular}
    \caption{Results on synthetic novelty dataset (unsupervised).}
    \label{tab:resUSbrighNov}
\end{table}

\begin{table}[hbt!]
\tiny
    \centering
    \begin{tabular}{ccccccc}
\toprule
setting  & dataset & exp & model & scaler & roc & auprc \\
\toprule
E &  synthe &  novelty .0 &  GADFormer     & standard & 0.978 & 0.912 \\
E &  synthe &  novelty .0 &  GRU     & standard & 0.909 & 0.814 \\
E &  synthe &  novelty .01 &  GADFormer    & standard & 0.971 & 0.845 \\
E &  synthe &  novelty .01 &  GRU    & standard & 0.941 & 0.86 \\
E &  synthe &  novelty .05 &  GADFormer    & standard & 0.957 & 0.649 \\
E &  synthe &  novelty .05 &  GRU    & standard & 0.952 & 0.884 \\
\bottomrule
    \end{tabular}
    \caption{Results on synthetic novelty dataset (semi-supervised).}
    \label{tab:resESbrightNov}
\end{table}


The code has been developed in Python, utilizing the Pytorch and PyTorch Lightning Framework, to implement each
layer, and it’s sub-layer of the GADFormer and to carry out the model-training and testing procedure, respectively. For Model training, the PyTorch lightning Trainer is initialized with Early Stopping and Checkpoint callback along with a Gradient Clipping value to avoid exploding gradients. 
Furthermore, weight initialization gets applied in order to achieve better convergence. 
In addition to that, hyperparameter search conducted in order to find the ideal training parameters and model architecture.


The synthetically generated trajectory dataset (cf. Table~\ref{table:Tabular_dataset}) consists of trajectory member instances (trajectory steps), one per row, where one group member has different attributes such as 'Person' (one trajectory per person), 'Sequence Step', 'XCoord' and 'YCoord'. A GAD-label is defined for one complete group (trajectory) stating whether a group (a trajectory) is anomalous (1) or normal (0). 

\begin{table}[h]
\centering
\footnotesize
\scalebox{0.8}{
\begin{tabular}{cccc|c}
\hline
Person & Sequence step  & XCoord  & YCoord & GAD-Label \\
\hline
1   & 0 & 1 &-3 & \\


1   & 1  & 2 & 8  & 0 \\
1   & 2 & 3 &5 &  \\

...   & ...& ... &... &  \\
1   & $|T|$ & 100 &150 &  \\
\hline
2   & 0 & 2 &-2 & \\
2   & 1  & 3 & 7  & 0 \\
2   & 2 & 2 &6 &  \\
...   & ...& ... &... &  \\
2   & $|T|$ & 110 &155 & \\
\hline
...   & ...& ... &... & ... \\
\hline

$|M|$    & 0 & 3 &-1 & \\
$|M|$   & 1  & 4 & 4  & 1 \\
$|M|$     & 2 & 5 &2 & \\
...   & ...& ... &... &  \\
$|M|$  & $|T|$ & 114 &152 & \\
\hline

\end{tabular}}
\caption{Trajectory dataset in tabular form.}
\label{table:Tabular_dataset}
\end{table}

We used for $dim_{pe}$ 72 for synthetic and amazon dataset and for brightkite 500. The $dim_{em}$ is for all 8, the hidden $dim_{ffn}$ is 2048. We trained for 100 epochs with earlystopping, used learning rates 1e-5, 1e-5 and 1e-6 and wd 1e-5, 1e-5 and 1e-6, 8 heads as well as $b=4$ encoder layers. For synthetic dataset no dropout was in use, for amazon and brightkite 0.05. We used batch sizes~25, 25 and 250 respectively.

\subsection{Evaluation}

For the evaluation of our model, we follow the goals of having a low miss rate (false negatives) as well as achieve as less false alerts (false positives) as possible. In addition to that, the quality of the model scores needs to be evaluated. Therefore and to be comparable to related approaches, we evaluate the model performance by:

\begin{itemize}
    \item $TPR = \frac{TP}{P}; FPR = \frac{FP}{N}; FNR = 1 - TPR$
    \item Precision = $\frac{TP}{TP+FP}$
    \item Recall = $\frac{TP}{TP+FN}$
\end{itemize}

\noindent Areas Under the Curve (AUC) for thresholds $[t, 1]$:
\begin{itemize}
    \item AUPRC = AP = Precision vs. Recall
    \item AUROC = TPR vs. FPR
\end{itemize}


\subsection{Results and Discussion}

In comparison with GRU the GADFormer performs better on synthetic dataset for the settings U-noise 0.0 and 0.5 (cf. Table~\ref{tab:resUSnoise}) and in semi-supervised setting E-noise for all ratios (cf. Table~\ref{tab:resESnoise}). Other studies which GADFormer could achieve a better performance was U-novelty for all ratios in unsupervised as well as in semi-supervised setting (cf. Table~\ref{tab:resUSbrighNov} and Table~\ref{tab:resESbrightNov}). Furthermore, we evaluated also the performance of both approaches on the original synthetic and real-world datasets. Even on large datasets like brightkite performed GADFormer on par with GRU, and was on amazon driver real-world dataset even considerably better than GRU. Only on the synthetically generated dataset GRU could outperform GADFormer in this setting of Table~\ref{tab:resUSbrightOrig}. GADFormer performed comparably well even on the massive brightkite dataset in semi-supervised setting. Just on synthetic data GADFormer is slightly weaker than GRU, but both are with over 0.94 ROC and 0.74 AUPRC on par.


\section{Related Work}
\label{05.relatedwork}

Reviewing the literature, we could identify the following related work, which is distinguished from our approach within this section. Instead of considering the detection of individual trajectory anomalies as a Group Anomaly Detection problem as our approach does, the vision in the works of~\cite{musleh2022let,musleh2022towards} is to observe trajectories as a NLP problem. In their papers they mention preliminary work, but do not provide a concrete model architecture for group anomaly detection although TrajBERT is proposed as a holistic framework for an efficient and practical solution for almost all fundamental trajectory analysis problems. Another work of ~\cite{chen2022mutual} uses for the task of Trajectory-User-Linking (TUL), instead of GAD, a combination out of RNN and transformer network with cross entropy loss but compared to our approach, they do not take trajectory segments into account and the model lacks in layer interpretability. Addressing long-range trajectory anomaly detection as well the work of ~\cite{9206939NFTAD} proposes an unsupervised normalizing flow (NF) model. They utilize trajectory segments and negative log-likelihood as well but use it in combination with NF-based density estimation. The work of
~\cite{10.1007/978-3-030-54623-6_6} introduced the problem of group trajectory outlier detection (GTOD), which is also addressed by ~\cite{10.1145/3430195}, and provide the approach CDkNN, which creates DBSCAN-based microclusters, pruned by kNN and scored with a specific pattern mining algorithm. However, both works perform anomaly detection while considering complete individual trajectories as group members, whereas we address the slightly different problem of considering single trajectory points as group members for the problem group anomaly detection. The work of~\cite{zhang2022cat} proposes a model for content-aware anomaly detection on event log messages instead of anomalous trajectories as our approach. In addition to that, their approach takes also the content of the messages into account and allows to run it, as our approach, by the task-specific encoder-part or, differently as ours, run the model with the typical BERT\cite{DBLP:conf/naacl/DevlinCLT19} encoder-decoder architecture. Summarizing the identified related work, there is best to our knowledge no interpretable attention-based transformer-encoder-approach for group anomaly detection on trajectories.

\section{Conclusion}
\label{06.conclusion}

In this work we proposed GADFormer, a transformer-encoder-architecture, capable to perform attention-based group anomaly detection in an unsupervised and semi-supervised setting. We emphasized, how the detection of individual anomalous trajectories can be solved as a Group Anomaly Detection (GAD) problem for BERT based transformer models. Furthermore, we introduced BAS, a Block Attention-anomaly Score to improve the interpretability of transformer encoder blocks the task of GAD. Extensive ablation and robustness studies addressing trajectory noise and novelties on synthetic and real world datasets demonstrated, that our approach is on par with related approaches.
Further potential for improvement could be to approximate a normal-group-distribution instead of abnormal-group-probabilities by the output-block of our model, combining the pattern attention-based group pattern extraction of our approach and the group anomaly scoring and loss objectives of \cite{DBLP:conf/pkdd/ChalapathyTC18}.
Vice versa, with appropriate preprocessing, the performance of the GADFormer model architecture could also be evaluated on image data, audio or text data. Finally, the potential of transfer learning could be investigated by training e.g. on trajectories of buses and applying that pre-trained model on trajectories of cars.

\appendix

\section*{Ethical Statement}

There are no ethical issues.

%\section*{Acknowledgments}

%


%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{arxiv23}

\end{document}
