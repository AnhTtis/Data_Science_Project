\section{Introduction}
\begin{figure*}
    \centering
    \includegraphics[width=0.75\textwidth]{figures/diff_with_fedavg.pdf}
    \vspace{-0.8em}
    \captionsetup{font=small}
    \caption{Differences in local update and aggregation phases between FedAvg and pFedPT. In the figure, the lines represent the decision boundaries defined by the backbone. Assume that each client has two classes represented by different shapes. (a) In FedAvg, due to the heterogeneity of data in each client, the significant difference in local updates affects the final model aggregation. However, (b) the pFedPT adds personalized visual prompts to the client data, which change the original data characteristics and improve the fit of the backbone on the client. (c) and (d) are the t-SNE visualization results of the final hidden layer trained by FedAvg and pFedPT on the client with only classes 1 and 4. pFedPT increases the inter-class variation (Inter-Var) and decreases the intra-class variation (Intra-Var).}
    \label{fig:dif}
    
\end{figure*}


Personalized federated learning (PFL)~\cite{deng2020adaptive,huang2022achieving,dai2022dispfl} is a novel paradigm proposed to overcome the impacts of heterogeneity across isolated clients. Instead of training a single aggregated model like in  Federated learning(FL)~\cite{mcmahan2017communication,tan2022towards,li2022fedhisyn,gao2022feddc}, PFL generates a personalized local model on each client that is more in line with the local data distribution by jointly considering the aggregated model and the personalized data. There are two main challenges lying in PFL. One is how to extract useful global features from models trained on each local heterogeneous dataset. The other is how to incorporate the extracted global features with the personalized features, yielding a better client-specific model.

% In federated learning (FL)~\cite{mcmahan2017communication}, data heterogeneity is a common problem since the training data from different clients vary greatly in both distributions and dimensions.
% When the global and local optimal models drift significantly, the learned global model obtained by minimizing the joint empirical distribution may not perfectly generalize to the local data~\cite{tan2022towards,li2022fedhisyn,gao2022feddc}. 

Several works have been proposed to address the above challenges from a model perspective. PFL algorithms with a decoupling model ~\cite{arivazhagan2019federated,collins2021exploiting,oh2021fedbabu} split the local model into a shared part to be aggregated with those from other clients, and a private part of maintaining locality. The shared part is used to transfer public knowledge among clients, and the private part is used to adapt to local data distribution. Clustered FL~\cite{dinh2021fedu} groups clients according to the similarity of the local parameters and trains an aggregated model for each group of clients. Clustered FL extracts common knowledge from similar clients within a group to generate a unified model for the group. These methods, however, still fall short in two aspects. First, these approaches rely on the effectiveness of aggregating or clustering the shared parts and may fail with highly heterogeneous data. Second, these methods simply extract the common knowledge and implement the personalization at the model level, while ignoring the potential at the data level, which may further strengthen the personalized adaptation between the aggregated model and local dataset.

%the potential for personalized adaptation of the aggregation model by adding additional information at the data level is ignored.  
% Third, the leakage of local data distribution may occur when the local personalized models are uploaded from clients to the server.


In the community of computer vision~(CV), both visual prompts~\cite{liu2021pre} and adversarial reprogramming~\cite{elsayed2018adversarial} employ a set of learnable parameters as a continuous task-specific vector, which can be tuned based on training data from the downstream task. Visual prompts can effectively help a large-scale pre-trained model achieve fast task transfer by simply training task-related prompts without changing any pre-trained model parameters. The prompt parameters are like the attention guidance to implicitly hint at the task-related information for improving model performance on the new task~\cite{liu2021pre}. This motivates us to regard the different clients as different tasks and adopt client-specific prompts to fine-tune the aggregated model on each local client, which helps to incorporate the extracted global features with the personalized ones.

Based on this insight, we propose a novel PFL framework named pFedPT. Our approach addresses the shortcomings mentioned above by using a visual prompt to implicitly provide a hint of the data distribution on a client for the aggregated model locally. Specifically, each client model integrates a learnable \textit{Prompt Generator} with a backbone participating in aggregation for classification. The prompt generator is a set of locally learnable parameters that can generate personalized visual prompts for its affiliated clients based on their local data distribution. During local training of pFedPT, the generated personalized visual prompts are added to the images. Fig.~\ref{fig:dif} (a) and (b) show the difference in the training process between FedAvg and pFedPT. For different classes of data, Fig.~\ref{fig:dif} (c) and (d) show that the generated prompts increases the inter-class variation (Inter-Var) while decreasing the intra-class variation (Intra-Var). Different class data with a visual prompt is easily distinguished by an aggregated backbone, thereby improving the local performance of the local clients. Then, the backbone is trained on the input composed of raw data and visual prompts to learn the distribution information contained in the prompt. Upon achieving convergence of the two models through alternate training, the backbone implements the extraction of common knowledge from clients and can recognize the visual prompts of different clients. The generated visual prompt reflects the client's characteristics as a client-conditional vector and implements fine-tuning of the backbone in the local client. As a result, Â the backbone can capture implicit knowledge about the client's data distribution based on the visual prompt and therefore obtain a better-personalized model. On the other hand, the visual prompt can be of independent interest and added as a plugin for other FL algorithms. It can fine-tune the model received by clients, which can implement the personalized improvement of the model trained by FL algorithms in different clients or further boost the performance of PFL algorithms.


We validate pFedPT on two image classification datasets, including CIFAR10~\cite{krizhevsky2009learning} and CIFAR100~\cite{krizhevsky2009learning}. Empirical results show that pFedPT beats other SOTA baselines of PFL with a 1\%-3\% improvement in test accuracy. In summary, our main contributions are four-fold:
\begin{itemize}
    \item To the best of our knowledge, this is the first work that proposes to use client-specific prompts to help the aggregated models achieve better local adaptation and generalization by leveraging the personalized features of clients. 

    \item We propose a novel PFL framework, dubbed pFedPT, for federated image classification tasks that use the visual prompts from each client to fine-tune the aggregated model and imbue the aggregated local model with information about the local data distribution. 

     \item We show that pFedPT can integrate with several existing FL and decoupled PFL methods to boost their performance, which may be of independent interest. 
     % \ls{also mention it in the abstract.} 
 
    \item We conduct extensive experiments to evaluate the effectiveness of pFedPT, which significantly outperforms several SOTA baselines on CIFAR10 and CIFAR100 datasets. Besides, the experimental results illustrate that the prompt can indeed learn personalized knowledge related to the client.
\end{itemize}
