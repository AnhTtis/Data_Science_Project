\section{Experiments}
In this section, we evaluate the effectiveness of pFedPT and compare it with several advanced methods in various datasets and settings. We also conduct a number of exploratory experiments to find out how pFedPT works and verify the effectiveness of pFedPT in terms of  client data distribution. The detailed experimental setup and the more experimental results can be found in the Appendix.





\begin{table*}[!ht]
\centering
\captionsetup{font=small}
\caption{The results of pFedPT and baseline methods on the image datasets with different non-IID settings}
\vspace{-0.8em}
% \ls{split this table as three tables with three types of datasets}}
\label{results}
\renewcommand\arraystretch{1.25}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{lccccccccccccc}
\hline
\textbf{} & \multicolumn{6}{c}{CIFAR10} & \multicolumn{6}{c}{CIFAR100} &\\ \cline{2-13} 
\#setting & \multicolumn{2}{c}{IID} & \multicolumn{2}{c}{Dirichlet } & \multicolumn{2}{c}{Pathological} & \multicolumn{2}{c}{IID} & \multicolumn{2}{c}{Dirichlet } & \multicolumn{2}{c}{Pathological}  \\ \cline{2-13} 
\#client & ViT & CNN & ViT & CNN & ViT & CNN & ViT & CNN & ViT & CNN & ViT & CNN \\ \hline

FedAvg  & 60.50 & \textbf{67.13 } & 53.01  & 61.92  & 54.98  & 63.62 & 29.60 & 26.42 & 25.93 & 26.50 & 27.71 & 30.28  \\
FedProx  & 57.04 & 66.94  & 53.14  & 61.95  & 55.02  & 63.29  & 27.71 & 26.29 & 26.00  & 26.48 & 27.84 & 30.52  \\


% FedBABU+PT  & 60.75  & 87.04  & 80.45  & 90.05  & 78.50  & 22.49 & 48.80 & 35.07 & 48.61 & 34.52  \\
MOON  & {60.99} & 66.88  & 61.12  & 62.53  & 65.98  & 63.52 & 29.32 & \textbf{26.43} & 24.95 & 26.93 & 27.61 & 29.00  \\

% FedAMP  & 37.54  & 66.31  & 59.75  & 62.01  & 39.46  & 1.47 & 41.04 & 16.38 & 23.35 & 14.53  \\
% FedAMP+PT  & 21.09  & 78.46  & 64.23  & 74.33  & 46.97  & 7.03 & 35.43 & 21.63 & 20.17 & 8.48  \\

% FedMTL+PT  & 45.65  & 85.33  & 73.94  & 87.46  & 70.17  & 7.58 & 44.88 & 25.82 & 41.06 & 25.40  \\
FedPer  & \textbf{61.57} & {51.46 } & {73.16}  & 77.98  & {75.20}  & 79.97  & 29.74 & 10.82 & {36.78} & {27.79} & 35.36 & 31.13  \\

FedRep  & 48.38 & {49.70 } & 74.11  & 77.65 & 74.48  & 78.39  & 17.84 & 9.13 & 35.06 & {27.39} & 36.13 & 32.41  \\
FedMTL  & 45.65 & 45.65  & 68.48 & 73.95  & 65.39   & 70.94 & 17.91  & 7.34 & 26.08 & 25.85 & 25.46 & 26.32  \\
% FedRoD  & {65.43 } & 88.28  & 81.36  & 91.10  & {82.35 } & 20.19 & 49.31 & 34.55 & 49.71 & 37.25 \\
% FedRoD+PT  & {65.90} & 78.50  & 76.01  & 52.64  & {65.97} & 26.34 & 32.35 & 27.31 & 28.60 & 29.58 \\
FedBABU  & 50.41 & 61.17  & 74.21  & 80.11 & 74.30  & 80.69 & 20.61 & 22.55 & 36.17 & 31.66 & 35.74 & 35.45  \\
Local  & 45.37 & 39.04  & 68.40  & 73.98  & 64.83  & 70.76  & 18.01 & 7.33 & 26.23 & 25.15 & 24.65 & 25.34  \\
% Local+PT  & 39.56  & 85.29  & 73.88  & 87.26  & 70.18  & 7.74 & 44.86 & 25.71 & 41.04 & 25.62  \\
\hline
pFedPT (ours) & 60.01  & 66.09 & \textbf{74.92} & \textbf{80.83} & \textbf{75.42} & \textbf{81.16} & \textbf{31.66} & {26.41} & \textbf{36.80} & \textbf{32.47} & \textbf{36.88} & \textbf{37.98}\\ 

\hline
\end{tabular}
}
\end{table*}



\begin{table}[!ht]
\centering
\captionsetup{font=small}
\caption{The results of baseline methods with prompts on the image datasets with CNN model in Non-IID settings.}
\vspace{-0.8em}
% \ls{split this table as three tables with three types of datasets}}
\label{results+pt}
\renewcommand\arraystretch{1.25}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{lccccc}
\hline
\textbf{} & \multicolumn{2}{c}{CIFAR10} & \multicolumn{2}{c}{CIFAR100} &\\ \cline{2-5} 
\#setting  & \multicolumn{1}{c}{Dirichlet } & \multicolumn{1}{c}{Pathological} &  \multicolumn{1}{c}{Dirichlet } & \multicolumn{1}{c}{Pathological}  \\ \cline{2-5} 
 \hline

FedProx     & 61.95    & 63.29   & 26.48  & 30.52  \\
FedProx+PT    & \textbf{80.47}    & \textbf{81.48}    & \textbf{31.95}  & \textbf{37.88}  \\
\hline
MOON     & 62.53    & 63.52   & 26.93  & 29.00  \\
MOON+PT     & \textbf{77.84}   & \textbf{76.00}    & \textbf{28.67}  & \textbf{34.60}  \\
\hline
FedPer     & 77.98    & 79.97    & {27.79} & 31.13  \\
FedPer+PT     & \textbf{78.40}    & \textbf{80.59}    & \textbf{28.83}  & \textbf{31.14}  \\
\hline
FedRep     & 77.65  & 78.39     & {27.39}  & 32.41  \\
FedRep+PT      & 77.65    & \textbf{79.11 }   & \textbf{29.19}  & \textbf{32.75}  \\
\hline
\end{tabular}
}
\end{table}








\subsection{Experimental Setup}

\paragraph{Baselines.}\ 
We compare the pFedPT with several advanced FL  methods, including FedAvg~\cite{mcmahan2017communication}, 
FedProx~\cite{li2020federated}.
MOON~\cite{li2021model},  FedPer~\cite{arivazhagan2019federated}, 
FedRep~\cite{collins2021exploiting},
FedMTL~\cite{smith2017federated} and FedBABU~\cite{oh2021fedbabu}. We also compare a baseline named \textbf{Local}, where each client trains a model with its local data without federated learning. We conduct experiments on two benchmark datasets:
CIFAR10~\cite{krizhevsky2009learning} and CIFAR100~\cite{krizhevsky2009learning}. CIFAR100 is a more difficult dataset for classification tasks than CIFAR10. We use PyTorch~\cite{oord2018representation} to implement pFedPT and the other baselines.


\paragraph{Datasets.} We consider two different scenarios for simulating non-identical data distributions (Non-IID) across federated clients. Dirichlet Partition follows works~\cite{hsu2019measuring}, where we partition the training data according to a Dirichlet distribution Dir($\alpha$) for each client and generate the corresponding test data for each client following the same distribution. We specify $\alpha$ equal 0.3 for each dataset. In addition, we evaluate with the pathological partition setup similar to \cite{zhang2020personalized}, in which each client is only assigned a limited number of classes at random from the total number of classes. We specify that each client possesses 5 classes for CIFAR10 and 50 classes for CIFAR100. 


\paragraph{Implementation Details.}\ 
We verify the experimental results based on CNN and ViT architectures. The CNN model consists of 2 convolutional layers with 64 $\times$ filters followed by 2 fully connected layers with 394 and 192 neurons and a softmax	layer. We use tiny ViT architecture consisting of 8 blocks with 8 self-attention layers in each block. The corresponding attention head number is 8, the patch size is 4, and the embedding dimension is 128. We set the number of clients to 50, and then each client has a 20\% chance of participating in each communication round. We utilize the SGD algorithm~\cite{cherry1998sgd} as the local optimizer for all methods. We use padding as our prompt method. We set batch size as 16 in the local training phase, the local training epochs for the generator and backbone as 5 in each round, the learning rate for the backbone as 0.005, the learning rate for the prompt generator as 1, and the  padding prompt size as 4. The number of communication rounds is set to 150 for CIFAR10, 300 for CIFAR100, where all FL approaches have very limited or no accuracy gain with more communications.


\subsection{Main Results} \label{main result}
We run vast experiments to determine the superiority of pFedPT on the model performance in different datasets. Our results highlight the benefit of pFedPT compared to the existing PFL optimization approaches.

\paragraph{Better performance of pFedPT.}
Tab.~\ref{results} compares the best accuracy of the pFedPT with baselines on evaluation datasets with various settings. On CIFAR10 and CIFAR100, the pFedPT consistently achieves the best test accuracy with Non-IID setting. For instance, when training on the data of Dirichlet distribution CIFAR10 with CNN, the test accuracy of the pFedPT is 80.83\%, the accuracy of FedAvg is 61.92\%, and the accuracy of the FedPer is 77.98\%.  The improvements of pFedPT indicate that prompts in each client effectively improve the backbone performance in each client. Similarly, in CIFAR100, pFedPT outperforms most baselines in various settings and achieves comparable results in the Dirichlet setting.

\paragraph{Robustness of pFedPT.}  pFedPT achieves clear success on both ViT and CNN models and seems to get better performance as the FL tasks become more difficult (since better performance is observed at a greater Non-IID extent and in datasets that are intrinsically more difficult). Interestingly, in the IID setting, we show that all the personalized solutions exhibit some extent of performance degradation, which becomes more significant as the dataset becomes more challenging. Our interpretation of this phenomenon is that when the data are distributed under the IID setting, the PFL approach does not effectively take advantage of the personalization characteristics among clients, resulting in performance degradation. pFedPT will utilize the data distribution information in the client by visual prompts. When the data is IID, the output will be similar on the various clients and degenerate into the FedAvg.


\paragraph{Improvements  of prompt for other algorithms.}  Visual prompts can improve the performance of backbones on clients by fine-tuning the backbone with hints about the distribution of the client's data. We explore the usefulness of visual prompts as prior knowledge for other FL algorithms, and Tab.~\ref{results+pt} presents these results. In the Dirichlet setting of CIFAR10, the final test accuracy of FedProx increases from 61.95\% to 80.47\% after adding prompts, and the test accuracy of MOON increases from 62.53\% to 77.84\%. We find that a visual prompt enables fine-tuning of the backbone of the client, which helps FL algorithms that pursue high precision fuse client information for personalization. Similarly, PFL algorithms with model decoupling, like FedRep and FedPer, can also yield a performance boost by integrating pFedPT. Therefore, prompt can be used as an additional component to improve the personalization performance of some existing FL algorithms.

Compared with other baselines, the pFedPT takes full advantage of the data improvement space. Additional prompts are added to the data entered into the model to improve the performance of the model on each client.

\subsection{Exploratory Study}
To provide more explanation for pFedPT, we additionally conduct several exploratory studies on pFedPT.  



% \begin{figure}
%     \centering
% w    \includegraphics[width=0.40\textwidth]{figures/exp_cnn_vit.pdf}
%     \caption{Results of using Vit and CNN as backbones for pFedPT and FedAvg, respectively}
%     \label{fig:comparison}
% \end{figure}

 
 
 
% \paragraph{The validity of the ViT.}\  
% We examine the CNN and ViT as the backbone in our pFedPT framework compared with FedAvg. The experiments are under three different CIFAR10 dataset settings, i.e., Dirichlet (0.1), Pathological (2) and IID, where Dirichlet (0.1) and Pathological (2) are non-IID settings. 
% Fig.~\ref{fig:comparison} demonstrates the test accuracy between four different models. Under the non-IID setting, the pFedPT+ViT model outperforms other methods by a large margin, while the other three methods struggle to address the distribution with data heterogeneity. It means that visual prompts should be used together with a ViT instead of only replacing the CNN backbone with a ViT or adding prompts separately. The ViT can notice the prior knowledge contained in the prompts through the multi-head self-attention and map the prompts to the data distribution clients. Hence, the test accuracy of the model has been greatly improved, which indicates that our pFedPT could better mine and exploit heterogeneity among clients.

 
 \begin{figure}[htbp]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/attention_visulization.pdf}
    \vspace{-0.8em}
    \captionsetup{font=small}
    \caption{Visualization results generated by FedAvg and pFedPT with different backbones.}
    \label{fig:attenmap}
\end{figure}

\paragraph{Visualization of attention maps.} 
To illustrate the effectiveness of visual prompts, we conducted some validation experiments. We train ten clients using FedAvg and pFedPT with ViT and CNN backbones under the Dirichlet setting of the CIFAR10 dataset, respectively. As shown in Fig.~\ref{fig:attenmap}, we make a visualization of the attention map of the last layer in the ViT and CNN by Grad-CAM~\cite{DBLP:journals/ijcv/SelvarajuCDVPB20}.
The first three rows in the figure show that  FedAvg focuses on some salient classification features of the raw image.
The fourth row contains the input images with the padding visual prompts, which are added by the prompt generator of pFedPT according to Eq.~(\ref{f4}). Both pFedPT+ViT and pFedPT+CNN shift some attention to the added prompts, which can help obtain the prior knowledge for the model inference process, thus improving the performance of the model.

 \begin{figure}[htbp]
    \centering
    \includegraphics[width=0.45\textwidth]
    {figures/tSNE.pdf}
    \vspace{-0.8em}
    \captionsetup{font=small}
    \caption{t-SNE visualization of embedding for pure color images with learned prompts in different clients.}
    \label{fig:tSNE}
    \vspace{-0.4cm}
\end{figure}


\paragraph{The guidance information contained in the prompts.}\  In order to further explore the influence of visual prompts, we generated 100 different pure color images with the shape of $[3\times32\times32]$. Using the pure color picture, pFedPT can exclude the disturbance of image contents and pay more attention to visual prompts. We feed those color pictures into pFedPT models in different clients with different prompts and visualize the output embeddings of their last MLP layer. We project them into a two-dimensional plane using the t-SNE algorithm~\cite{van2008visualizing}. Fig.~\ref{fig:tSNE} shows that after the visual prompts are added, the model outputs of different clients can be easily distinguished, indicating that the prompts contain prior knowledge of the client model and aid in the classification task. 


\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/prompt.pdf}
    \vspace{-0.8em}
    \captionsetup{font=small}
    \caption{Effect of different types of prompts}
    \label{fig:p_type}
\end{figure}

 
 
\paragraph{Impact of different types of visual prompts.} 
We analyze different choices on how and where to insert prompts in the input images and how they would affect the final performance. We perform an ablation study on different prompt sizes in $p=\{2,4,6,\dots,16 \}$ in CIFAR10 with a Dirichlet distribution. As shown in Fig.~\ref{fig:p_type}, padding prompts reach the highest performance with a size of 4. The test accuracy of fixed location and random location prompts grows gradually with the increase in prompt size, but it is still slightly lower than the padding prompt. In contrast, the accuracy of padding prompts decreases as the prompt size increases. A possible explanation is that the padding method covers more pixels of the original images than the other two methods when using the same length of prompts. As a result, the key information for classification could be blocked by the prompts and harm the performance of the model. Overall, the padding prompts with size 4 achieve the best performance. Note that other visual tasks may require significantly different kinds of prompts. 





% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\textwidth]{figures/out.pdf}
%     \caption{Similarity comparison between the distribution of the predicted classes and the distribution of the local data.}
%     \label{fig:output_dis}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.35\textwidth]{figures/the difference of average prompt4.pdf}
%     \caption{The difference of average prompt between two consecutive rounds.}
%     \label{fig:difference}
%         \vspace{-0.4cm}
% \end{figure}


% \begin{figure}
%     \centering
%     \includegraphics[width=0.35\textwidth]{figures/fintune.pdf}
%     \caption{Test accuracy after fine-tuning the head of models trained on a client of CIFAR10 datasets}.
%     \label{fig:fintune}
%     \vspace{-0.4cm}
% \end{figure}


% \paragraph{Empirical analysis of the learned prompts.}\ 
% Fig.~\ref{fig:difference} records the average difference of the prompts generated between the two rounds before and after ten clients during the pFedPT training process. The overall experimental results are divided into two stages: first ascending and then descending. In our settings, the initial prompt generator parameters of each client are the same, and the rising stage is the mapping process between each client and the prompts based on its own data distribution. The descending stage is that the aggregated model tends to converge, and the mapping between the prompt and the client data distribution on each client is completed. Eventually the change of prompts embedding approaches 0, that is, each client establishes stable prompts that conforms to its own data distribution.

 
% \paragraph{Generalization ability of the pFedPT.}
% We evaluate the strength of the backbone learned by pFedPT in terms of adaptation to new clients. To do so, we first train the pFedPT, the FedAvg in the usual setting on the partition of the CIFAR10 dataset with 10 clients and Dir (0.1) partition. Then, we encounter clients with data from Dir (0.3) partition of the CIFAR10 dataset. We assume we have access to a dataset of 400 samples for this new client to fine-tune. For the pFedPT, we fine-tune the prompt generator parameter over multiple epochs while keeping the backbone fixed. For the FedAvg, fine tune the last layer of backbone while keeping other layers. Fig.~\ref{fig:fintune} shows that the pFedPT has significantly better performance than the FedAvg.