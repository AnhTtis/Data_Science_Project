\appendix
\onecolumn
\section{Appendix: More Experiment Results}
We run experiments on the real-world datasets for image classification tasks, including CIFAR10, CIFAR100, and Tiny ImageNet. We conduct comprehensive investigations for the impact on client heterogeneity by designing IID and non-IID data scenarios. For comparison, we utilize the FedAvg, FedProx, and FedPer algorithms as baselines. The experiment settings are described in detail below. 

\subsection{Setups}

\paragraph{Dataset.}\
We adopt real-world datasets for the image classification task, including CIFAR10, CIFAR100, and Tiny ImageNet. The CIFAR10 dataset contains 50,000 training data and 10,000 test data in 10 classes. Each data sample is a $3\times 32\times 32$ color image. CIFAR100~\cite{krizhevsky2009learning} includes 50,000 training data and 10,000 test data in 100 classes as 500 training samples per class. TinyImageNet~\cite{oord2018representation} involves 100,000 training images and 10,000 test images in 200 classes for $3\times 64\times 64$ color images, as shown in Table~\ref{data}. For CIFAR10/100 and TinyImageNet, we normalize the pixel value within a specific mean and std value in our code, which are [0.5, 0.5, 0.5] for the mean and [0.5, 0.5, 0.5] for the std.
\begin{table*}[ht]	
\captionsetup{font=small}
\vspace{-0.8em}
\caption{The similarity between predicted and real data distribution}
	\label{data}
	\centering
	\resizebox{0.6\textwidth}{!}{
		\begin{tabular}{ccccc}
			\toprule
			Datasets & Training Data & Test Data  & Class & Size \\
			\midrule
			CIFAR-10 & 50,000 & 10,000 & 10 & $3\times32\times32$ \\
			CIFAR-100 & 50,000 & 10,000 & 100 & $3\times32\times32$ \\
                TinyImageNet & 100,000 & 10,000 & 200 & $3\times64\times64$ \\
   
			\bottomrule
		\end{tabular}
	}
 \vspace{-1.5em}
\end{table*}


\paragraph{Backbone.}\
We  adopt two backbones including ViT and CNN for experiments.
Given an image $I\in \mathbb{R}^{3\times h\times w}$, the ViT reshapes it to a sequence of flattened 2D patches $I_p \in \mathbb{R}^{n\times (p^2\cdot c)}$, where $c$ is the number of channels. $(h, w)$ is the height and width of the original image, while $(p, p)$ is the size of each image patch. A trainable linear projection flattens the patches into a latent $D$-dimensional embedding space, which is then embedded with a positional embedding. The transformer encoder consists of $N$ layers of Multi-Head Self-Attention (MSA) and Multi-Layer Perceptron (MLP) blocks. For MSA, the queries, keys and values are generated via linear transformations on the inputs for $K$ times with one individual learned weight for each head. Then in parallel, the attention function is applied to all queries, keys, and values. The sequence of image patches $I_p$ is passed into the MSA, followed by MLP for $N$ times to get the final outputs.

The ViT used by us consists of 8 blocks with 8 self-attention layers in each block. The corresponding attention head number is 8, the patch size is 4 and the embedding dimension is 128.
 The CNN  consists of the basic modules of CNN, including two conventional layers with 64 of $5 \times 5$ convolution kernels, each conventional layer followed by a down-pooling larger, after that are two fully connected layers with 394 and 192 neurons and a softmax layer for prediction.  


\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/3_prompts.pdf}
    \captionsetup{font=small}
    \vspace{-0.8em}
    \caption{Different visual prompt templates}
    \label{fig:templates}
    \vspace{-1.5em}
\end{figure}

\paragraph{Three types of generating Visual Prompts.}\
Following the settings of ~\cite{bahng2022exploring}, we explore three visual templates: pixel patch at a random location, pixel patch at a fixed location, and padding. We describe in Fig.~\ref{fig:templates} the difference between different visual prompts and how these visual prompts templates are added to the input picture. The comparison results are shown as Fig.~\ref{fig:p_type}.



\paragraph{Dataset Partitions.}\
To fairly compare with the other baselines, we introduce heterogeneity by splitting the total dataset and sampling the label ratios from the Dirichlet distribution and Pathological distribution. An additional parameter is used to control the level of heterogeneity of the entire data partition. In order to visualize the distribution of heterogeneous data, we make heat maps of the label distribution in different datasets, as shown in Fig.~\ref{fig:datadis}.  It could be seen that for heterogeneity weight equal to 0.3 in the Dirichlet distribution, about 10\% to 20\% of the categories dominate on each client, which is the blue block in Fig.~\ref{fig:datadis}. For heterogeneity weight equal to 5 in pathological distribution, 50\% of the categories dominate on each client, which is the black block in  Fig.~\ref{fig:datadis}. The IID dataset is totally averaged for each client, which is the green block in  Fig.~\ref{fig:datadis}.

\paragraph{Baselines.}\
FedAvg~\cite{mcmahan2017communication} is proposed as the basic framework in federated learning. 
FedProx~\cite{li2020federated} adds a proximal term to the objective function of the local model and allows for the emergence of incomplete training of the local model.
MOON~\cite{li2021model} is to utilize the similarity between model representations to correct the local training of individual parties, conducting contrastive learning at the model level. 
FedPer~\cite{arivazhagan2019federated} and  FedRep~\cite{collins2021exploiting} are base + personalization layer approaches for federated training of deep feedforward neural networks, which can combat the ill-effects of statistical heterogeneity.  
FedMTL~\cite{smith2017federated} uses a multi-task learning (MTL) framework to learn separate models for each client. FedBABU~\cite{oh2021fedbabu} achieves good personalization performance by freezing the last discriminative layer of the network and fine-tuning it after training. We also compare a baseline named Local, where each client trains a model with its local data without federated learning.

 \paragraph{Hyper-parameters Selections.}\
We fix the learning rate for local training as 0.005 and for the prompt generator training as 1.0. We fix the training batch size as 16 and fix the epoch for local training as 5. For the specific parameters in FedProx, the proximal rate is set as 0.0001. For the specific parameters in MOON, the $\mu$ is set as 1.0.  For the specific parameters in FedRep, the personalized learning rate is set as 0.01. For the specific parameters in FedMTL, the iterations for solving quadratic sub-problems are set as 4000.  For the specific parameters in FedBABU, the fine tuning step is set as 1. 
\begin{figure*}[h]
\centering
\subfloat[IID]{\includegraphics[width=0.32\textwidth]{figures/IID.png}%
}
\hfill
% \hfil
\subfloat[Dirichlet]{\includegraphics[width=0.32\textwidth]{figures/DIR.png}%
}
\hfill
\subfloat[Pathological]{\includegraphics[width=0.32\textwidth]{figures/PAT.png}}
\captionsetup{font=small}
\vspace{-0.8em}
\caption{Heat maps for each client with  CIFAR10  dataset under different data partitions. The color bar denotes the number of data samples. Each rectangle represents the number of data samples of a specific class in a party. }
\label{fig:datadis}
\vspace{-1.5em}
\end{figure*}

\subsection{Other Experimental Results}
\paragraph{Experimental results for TinyImageNet.}
\begin{table*}[h]
\tiny
\centering
\captionsetup{font=small}
\vspace{-0.8em}
\caption{The results of the pFedPT and the benchmark methods on the TinyImageNet dataset with different non-IID settings and backbones.}

\label{results-i}
\renewcommand\arraystretch{1.25}
\resizebox{0.6\textwidth}{!}{
\begin{tabular}{lccccccc}
\hline
\textbf{} & \multicolumn{6}{c}{TinyImageNet}  &\\ \cline{2-7} 
\#setting & \multicolumn{2}{c}{IID} & \multicolumn{2}{c}{Dirichlet } & \multicolumn{2}{c}{Pathological}  \\ \cline{2-7} 
\#client & VIT & CNN & VIT & CNN & VIT & CNN \\ \hline

FedAvg  & 15.47  & 16.22  & 12.06  & 11.01  & 9.37 & 10.53   \\
FedProx  & 15.53  & 15.81  & 13.82  & 10.98  & 12.84 & 11.32   \\
Moon  & 15.20  & \textbf{16.78}  & 15.51  & 11.05  & 13.67 & 10.24   \\
FedPer  & 14.85  & 13.18  & 23.95  & 20.21  & 19.23 & 17.99   \\
FedRep  & 12.91  & 11.56  & \textbf{25.24}  & 21.32  & 23.43 & 20.42  \\
FedMTL  & 9.75  & 11.02  & 21.14  & 17.96  & 18.30  & 17.39  \\
FedBABU  & 13.61  & 12.04  & 24.34  & 18.62  & 22.46 & 18.41  \\

Local  & 9.82  & 10.72  & 20.86 & 17.43  & 18.19 & 17.12 \\
\hline
pFedPT (ours) & \textbf{18.72} & 16.53 & 21.21 & \textbf{21.42} & \textbf{25.95} & \textbf{20.66}\\ \hline
\end{tabular}
}
%\vspace{-1.5em}
\end{table*}

We compare the performance of pFedPT and other baselines on the TinyImageNet dataset with 500 rounds of communications. Tab.~\ref{results-i} shows that the experimental results in TinyImageNet are still consistent with the interpretation in Sec.~\ref{main result}. For Pathological distribution with ViT, the test accuracy of the pFedPT is 25.95\%, the accuracy of FedAvg achieves 9.37\% and the accuracy of the FedPer achieves 19.23\%. However, the test accuracy of the pFedPT is not the highest in Dirichlet distribution, our explanation is that it is hard to generate stable prompts in 500 rounds due to the increased task difficulty.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/out.pdf}
    \captionsetup{font=small}
    \vspace{-0.8em}
    \caption{Similarity comparison between the distribution of the predicted classes and the distribution of the local data.}
    \label{fig:output_dis}
    \vspace{-1.5em}
\end{figure}
\paragraph{The connection between Prompt and the client data distribution.}
We use a single, pure-color image as input to investigate the relationship between the local model output and the data distribution of each client. Ideally, the output distribution over classes of each pFedPT client should align with the local data distribution. Fig.~\ref{fig:output_dis} reveals that after adding the visual prompts, the outputs of the pFedPT will be similar to the distribution of the client itself. The difference between the visual prompts generated by clients with similar data distribution is also smaller, which means that the visual prompts indeed contain the data distribution information of the clients. Therefore, the visual prompts provide the model with certain prior knowledge when classifying a specific client and assist in the classification task.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.35\textwidth]{figures/the_difference_of_average_prompt4.pdf}
    \captionsetup{font=small}
    \vspace{-0.8em}
    \caption{The difference of  prompt between two consecutive rounds.}
    \label{fig:difference}
        \vspace{-1.5em}
\end{figure}

\paragraph{Empirical analysis of the learned prompts.}\ 
Fig.~\ref{fig:difference} records the average difference of the prompts generated between the two rounds before and after ten clients during the pFedPT training process. The overall experimental results are divided into two stages: first ascending and then descending. In our settings, the initial prompt generator parameters of each client are the same, and the rising stage is the mapping process between each client and the prompts based on its own data distribution. The descending stage is when the aggregated model tends to converge, and the mapping between the prompt and the client data distribution on each client is complete. Eventually, the change in prompt embedding approaches 0, that is, each client establishes stable prompts that conform to its own data distribution.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.35\textwidth]{figures/fintune.pdf}
    \captionsetup{font=small}
    \vspace{-0.8em}
    \caption{Test accuracy after fine-tuning the head of models trained on a client of CIFAR10 datasets with ViT}.
    \label{fig:fintune}
    \vspace{-1.5em}
\end{figure}



 
\paragraph{Generalization ability of the pFedPT.}
We evaluate the strength of the backbone learned by pFedPT in terms of adaptation to new clients. To do so, we first train the pFedPT and the FedAvg in the usual setting on the partition of the CIFAR10 dataset with 10 clients and the Dir (0.1) partition. Then, we encounter clients with data from Dir (0.3) partition of the CIFAR10 dataset. We assume we have access to a dataset of 400 samples for this new client to fine-tune. For the pFedPT, we fine-tune the prompt generator parameter over multiple epochs while keeping the backbone fixed. For the FedAvg, fine-tune the last layer of the backbone while keeping the other layers. Fig.~\ref{fig:fintune} shows that the pFedPT has significantly better performance than the FedAvg.