\section{Methodology}\label{methodology}
In this section, we introduce the proposed visual prompt based personalized federated learning (pFedPT) framework. Below, we first provide several preliminaries on PFL.

\subsection{Problem setup}
Suppose that there are $N$ clients, denoted as $C_1, ..., C_N$,  respectively. Client $C_i$ has a local dataset $\D^i$. The goal of traditional FL~\cite{mcmahan2017communication} is to collaboratively learn a machine learning model $w$ over the dataset $\D\triangleq \bigcup_{i\in[N]}\D^i$ with the help of a central server, while the raw data are not exchanged. The objective of FL is defined below:
\begin{equation}\label{FL-formulation}
    \argmin_{w} \mathcal{L}(w) = \sum_{i=1}^N \frac{|\D^i|}{|\D|}L_i(w),
\end{equation}
where $L_i(w) = \mathbb{E}_{(x,y)\sim \D^i} [\ell_i(w; (x, y))]$ is the empirical loss of $C_i$.  However, rather than aiming at a single aggregated model w in FL, PFL is supposed to train personalized models $w_i$ for different clients~\cite{tan2022towards}, which is defined as the following optimization problem:
\begin{equation}
    \argmin_{{W}} \mathcal{L}({W}) = \sum_{i=1}^N \frac{|\D^i|}{|\D|}L_i(w_i),
    \label{eq2}
\end{equation}
where $ W = \left\{w_1, ..., w_N \right\}$ is the personalized models set for all clients. 


\subsection{Workflow of pFedPT}
We introduce a novel visual prompt based PFL framework for solving the PFL task, dubbed pFedPT. The central insight of the pFedPT is to train the learnable continuous visual prompts about data distribution for each client and use them to fine-tune backbones locally on those clients. Prompts on each client can serve as prior knowledge aiding the backbone to complete the training task. The pFedPT currently focuses on visual-related tasks, wherein each client maintains a prompt generator and a backbone, as shown in Fig.~\ref{fig:pipeline}. When performing image classification tasks, pFedPT first adds prompts generated by the local prompt generator to each image, which is then passed into the backbone for classification prediction. Generally, a complete pFedPT training process mainly includes four steps, as shown in Fig.~\ref{fig:pipeline}:
\begin{itemize}
\item \textbf{Step 1.}\ To begin with, the parameters of the prompt generator on each client are updated with local data while the whole backbone is frozen. 

\item  \textbf{Step 2.}\ After training several epochs, the prompt generator will be frozen, and the backbone will begin to update for a fixed number of epochs.

\item  \textbf{Step 3.}\ When the training process of all clients is finished, they send the trained backbone to the server, followed by the aggregation operation conducted by the server.

\item  \textbf{Step 4.}\ The aggregated backbone will be broadcast to every client to replace the old backbone stored locally. 
\end{itemize}
Repeat the \textbf{Step 1}-\textbf{Step 4}  until the training process of the prompt generator and backbone converges. At this point, the prompt generator generates prompts for each client based on local data distribution and can be seen as a guide to fine-tuning the prediction results of the backbone for the input images. Since the prompt is client-specific, the same backbone can generate different fine-tuning effects when used by different clients to achieve personalization. 

Below, we specify the key components of pFedPT, i.e., prompt generator, which is parameterized with the parameter $\delta$ for the prompt. The prompt is added to the input image to form a prompted image $X_i + \delta_i$. During the local evaluation, the optimized prompt is added to all test images,
\begin{equation} \label{f4}
    \mathcal{X}_i = \left\{x_i^1 + \delta_i, \ldots , x_i^n + \delta_i \right\}.
\end{equation}
There are several ways to design a visual prompt in terms of template and size. Following the settings of ~\cite{bahng2022exploring}, we explore three visual templates: pixel patch at a random location, pixel patch at a fixed location, and padding. We explore various prompt sizes $p$, where the actual number of parameters is $Cp^2$ for patches and $2Cp(H + W - 2p)$ for padding, where $C, H, W $ are the image channels, height, and width, respectively. In order to explore the effect of different prompts on the results, we conducted an experiment on CIFAR10 dataset with a Dirichlet ($0.3$) partition. Fig.~\ref{fig:p_type} shows that padding prompts with $p=4$ size achieve the best performance over other design choices. We use this as the default for all our experiments.


\subsection{Modeling for pFedPT}
Our goal is to learn a personalized prompt $\delta_i$ for each client and a backbone $w$. The prompt $\delta_i$ is also trained by the local data. Our objective is to solve the following:
\begin{equation}
    \argmin_{w, \delta_i} \mathcal{L}(w, \delta_i) = \sum_{i=1}^N \frac{|\D^i|}{|\D|}L_i(w, \delta_i),
    \label{eq3}
\end{equation}
where $L_i(w, \delta_i) = \mathbb{E}_{(x,y)\sim \D^i} [\ell_i(w; (x +\delta_i , y))]$ is the empirical loss of $C_i$. To achieve the goal in Eq.~(\ref{eq2}), exiting PFL algorithms usually add a regularizer to the model to perform information exchange between clients~\cite{li2021ditto,arivazhagan2019federated}, partition the layers as shared and personalized parts by exchanging the shared layers \cite{tan2022towards}, or interpolate the aggregated model with local models \cite{li2021ditto,mansour2020three}. However, pFedPT still uses the aggregated model $w$ to deliver public knowledge between clients, and personalized knowledge is incorporated by adding $\delta_i$ to the data. Specifically, the shared backbone is responsible for the extraction of the common knowledge of each client and identifying the information carried by the visual prompt of the individual clients. The client-specific prompt is responsible for increasing the guidance of the backbone to achieve fine-tuning to adapt to the client's data distribution. We implement personalized prediction of the backbone at the client data level.
% \ls{we should give more explanation/insight on this formulation.}



% \ls{most pfl problems are formulated as a regularization problem.}


\begin{algorithm}[t]
\small
\SetNoFillComment
\LinesNumbered
\SetArgSty{textnormal}
\KwIn{number of communication rounds $T$, the set  of clients  $ \left\{C_1, ..., C_N\right\}$, number of local epochs $E_b$ for backbone, number of local epochs $E_g$ for the prompt generator, learning rate $\eta_b$ for backbone, learning rate $\eta_g$ for the prompt generator, initialization parameters $w^0$ for backbone, initialization parameters $\delta_i^0$ for the prompt generator in client $i$. }
\KwOut{The final model $w^T$}
\BlankLine
\textbf{Server executes}:
initialize $w^0$\\
 $\mathcal{S:}$ choose a random set of devices from $C$ \\
\For {$t=0, 1, ..., T-1$}{
    \For {$C_i  \in \mathcal{S}$ \textbf{in parallel}}{
    % \tcc{the server sends the global model to all parties}
        send the aggregated model $w^t$ to $C_i$
        
        $w_i^t \leftarrow$ \textbf{LocalTraining}($i$, $w^t$)
    }
    $w^{t+1} \leftarrow \sum_{k=1}^{\left\|\mathcal{S}\right\|} \frac{|\D^i|}{|\D|} w_k^{t}$
}
return $w^T$
\BlankLine
\textbf{LocalTraining}($i$, $w^t$):
$w_i^t \leftarrow w^t$

\For{epoch $i = 1, 2, ..., E_g$}{
    \For{each batch $\textbf{b} = \{x, y\}$ of $\D^i$}{
    \textit{Training for prompt generator:}
        $\delta_i^t \leftarrow \delta_i^t - \eta_g \nabla \ell_i(w_i^t; (x +\delta_i , y)))$
    }
}
\For{epoch $i = 1, 2, ..., E_c$}{
    \For{each batch $\textbf{b} = \{x, y\}$ of $\D^i$}{
        \textit{local backbone training:}
        $w_i^t \leftarrow w_i^t - \eta_b \nabla \ell_i(w_i^t; (x +\delta_i , y))$ 
  }
}
return $w_i^t$ to server
\caption{\ pFedPT framework}
\label{alg:pFedPT}
\end{algorithm}



\subsection{Optimization for pFedPT}

 To achieve the optimization goal of Eq.~(\ref{eq3}), we alternately update the prompt generator and the backbone on each client using gradient descent. pFedPT first trains the prompt generator with the aggregation model fixed, and the model maximizes the likelihood of the correct label y, which is equivalent to solving: 
\begin{equation}
    \argmin_{ \delta_i} \mathcal{L}_i(w, \delta_i) =  \mathbb{E}_{(x,y)\sim \D^i} [\ell_i(w; (x +\delta_i , y))].
\end{equation}
After updating the prompt generator locally, we freeze the parameters of the prompt generator, and then train the backbone for several epochs. The backbone has the following objective function in the client $i$ during the training process:
\begin{equation}
    \argmin_{ w} \mathcal{L}_i(w, \delta_i) =  \mathbb{E}_{(x,y)\sim \D^i} [\ell_i(w; (x +\delta_i , y))].
\end{equation}
A locally trained backbone can learn the client data distribution corresponding to a prompt on the client and prompt knowledge is passed between clients via model aggregation at the server. The backbone on the server aggregates according to the following formula:
\begin{equation}
    w^{t+1} \leftarrow \sum_{k=1}^N \frac{|\D^i|}{|\D|} w_k^{t},
\end{equation}
where $t$ represents the number of training rounds. We summarize the detailed  procedures of pFedPT in Algorithm \ref{alg:pFedPT}.
% \ls{the goal of this paragraph is not clear.}


In the end, we give several comments on the differences between our pFedPT and decoupled FedRep \cite{collins2021exploiting}. Fig.\ref{fig:dirrp} describes their training process. Note that pFedPT also has a private part and a public part, but the private part is the prompt generator that we added at the client data level additionally. The personalized visual prompt generated adds the client's personalized knowledge to the training process by fine-tuning backbone's input without changing backbone's inference process. FedRep is to separate the private part in the inference model, and different clients have different inference processes. The objective functions of FedRep and pFedPT are also different. Furthermore, the visual prompt is orthogonal to FedRep type methods, which can be integrated together to further boost their performance.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/diff_with_fedrep.pdf}
    \vspace{-1.8em}
    \captionsetup{font=small}
    \caption{Differences between  pFedPT and decoupled personalized FL algorithm (FedRep)}
    \label{fig:dirrp}
\end{figure}





% During the training process of the prompt generator, the aggregation model $w$ is fixed, and the model maximizes the likelihood of the correct label y, which is equivalent to solve:
% \begin{equation}
%     \argmin_{ \delta_i} \mathcal{L_i}(w, \delta_i) =  \mathbb{E}_{(x,y)\sim \D^i} [\ell_i(w; (x +\delta_i , y))],
% \end{equation}
% while the gradient updates are applied only to the prompt parameters $\delta_i$ and the aggregated model parameters $w$ remain frozen. 


%\noindent
%\textbf{Vision Transformer}
%Vision Transformer (ViT)~\cite{dosovitskiy2020image} outperforms traditional CNN-based models on most visual tasks since it can simultaneously pay attention to global information of the input images. The pFedPT framework uses the ViT to conduct the experiments after comparing with CNN, and the performance difference is shown in Sec.~\ref{cnn}.

%Given a image $I\in \mathbb{R}^{3\times h\times w}$, the ViT reshapes it to a sequence of flattened 2D patches $I_p \in \mathbb{R}^{n\times (p^2\cdot c)}$, where $c$ is the number of channels. $(h, w)$ is the resolution of the original image, while $(p, p)$ is the resolution of each image patch. A trainable linear projection flattens the patches into a latent $D$-dimensional embedding space, which is then embedded with a positional embedding. The transformer encoder consists of $N$ layers of Multi-Head Self-Attention (MSA) and Multi-Layer Perceptron (MLP) blocks. For MSA, the queries, keys and values are generated via linear transformations on the inputs for $K$ times with one individual learned weight for each head. Then in parallel, the attention function is applied to all queries, keys, and values. The sequence of image patches $I_p$ are passed into the MSA, followed by MLP for $N$ times to get the final outputs.
%\ls{It seems that this paragraph is not the contribution of this work. we may move it somewhere else, such as problem  setup section by saying in this work, we adopt Vision transformer as the basic backbook for personalized FL.}

