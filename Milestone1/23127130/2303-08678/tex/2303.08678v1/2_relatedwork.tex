\section{Related Work}
\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/pipeline.pdf}
    \vspace{-0.8em}
    \captionsetup{font=small}
    \caption{The pipeline of the pFedPT. $\hat{y}$ stands for the predicted logits of all classes. The dashed lines in steps 1 and 2 represent the loss backward for the model update. Each client contains a Prompt Generator, a set of personalized learnable parameters preserved locally, and a Backbone, which the server will aggregate with those of other clients. The raw image input will be added to a visual prompt (colored pixels padded around the image) and then passed into the backbone for prediction.}
    \label{fig:pipeline}
\end{figure*}


\paragraph{Personalized Federated Learning (PFL).}\ 
 PFL has drawn significant research interests~\cite{tan2022towards,fallah2020personalized,t2020personalized,hanzely2021personalized,cho2021personalized,wang2023fedabc}. The main difficulty of PFL is to characterize the data distributions of clients and integrate them into the federated learning training process, followed by providing a personalized local model for each client. Currently, the core idea of PFL is to decouple the model into shared layers for feature extraction and personalized layers for classification~\cite{arivazhagan2019federated,collins2021exploiting,oh2021fedbabu}. Each client's parameters of the shared layer are generally updated globally using the FedAvg~\cite{mcmahan2017communication} algorithm. In contrast, the personalized layers are trained locally and will not be shared with others. Those works focus on training a general feature extractor and a personalized classifier head for personalization. 

Other work aims to combine other related machine-learning techniques with PFL. Briggs \etal ~\cite{briggs2020federated} and Mansour \etal ~\cite{mansour2020three} use the clustering technique to divide similar clients into groups and learn a separate model for each group without inter-group federation. Smith \etal ~\cite{dinh2021fedu} use multitasking learning to take advantage of shared representations between clients to improve the generalization performance of each model. Yang \etal ~\cite{yang2020fedsteg} and Chen \etal ~\cite{chen2020fedhealth} use transfer learning to enhance local models by transferring knowledge between relevant clients. T Dinh \etal ~\cite{t2020personalized} add regularizers to the aggregated model to prevent customers' models from simply overfitting their own data sets. Chen \etal ~\cite{chen2018federated} and Fallah \etal~\cite{fallah2020personalized} attempt to develop a well-initialized shared aggregated model using a model-agnostic meta-learning (MAML) approach~\cite{finn2017model}. In addition, fine-tuning using the aggregated model learned by the FedAvg algorithm can also improve the performance of personalized local models~\cite{jiang2019improving,huang2023fusion}. The previous works enable the model to recognize the characteristics of clients and implement personalization for them. However, adding additional information at the data level to achieve better backbone performance on the client has always been ignored. Our pFedPT framework uses visual prompts to implicitly represent the data distribution of clients, which achieves personalization by incorporating the characteristics of clients into the training process at the data level.

\paragraph{Prompt Learning.}\ 
Prompt learning~\cite{liu2021pre}, as a novel application paradigm for large-scale pre-trained models, was first proposed in NLP, and refers to prepending a language instruction to the original text input~\cite{li2021prefix}. In this way, pre-trained models can be given hints about what tasks are currently being performed, thereby achieving strong generalization to downstream transfer learning tasks without fine-tuning the whole model~\cite{floridi2020gpt}. Compared to hard prompts, soft prompts avoid the trouble of manual design, and are more expressive. Lester \etal~\cite{lester2021power} use task-specific continuous vectors as soft prompts and can be optimized by training. In the CV area, Radford \etal~\cite{radford2021learning} propose the CLIP model using language prompts to solve the vision-language tasks, which is similar to the following works~\cite{tsimpoukelli2021multimodal,yao2021cpt}. In~\cite{bahng2022exploring}, the visual prompts are designed as an input-agnostic perturbation, which is padded around the input images. The perturbation-generating function includes a small number of trainable parameters, which helps the pre-trained vision models perform downstream tasks without fine-tuning any parameters. Visual Prompt Tuning (VPT)~\cite{jia2022visual} is introduced as a parameter-efficient alternative to full fine-tuning for pre-trained model~\cite{dosovitskiy2020image}.Â 

Notably, two concurrent works, PROMPTFL~\cite{guo2022promptfl} and FedPrompt ~\cite{zhao2022reduce} also introduce prompt learning into FL. However, several significant differences exist between our work and these two works. (i) Different training objectives: the goal of PROMPTFL and FedPrompt is to fine-tune existing pre-trained models in the FL system. While pFedPT implements model training from scratch for achieving PFL. (ii) Different ways of training: PROMPTFL and FedPrompt freeze the pre-trained model during training and share the parameter information of the prompt. In pFedPT, backbone and prompt generators are trained alternately during training, and each client has its own unique prompt after training to achieve the goal of personalization. (iii) Different verification scenarios: PROMPTFL and FedPrompt are tested on NLP datasets, while pFedPT is mainly concerned with the improvement brought by the prompt in the CV domain. 


