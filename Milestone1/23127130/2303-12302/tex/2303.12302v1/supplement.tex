\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\usepackage{amsmath}
\usepackage{bm, amsopn, amssymb, braket, enumitem, changepage, hyperref, booktabs}
\usepackage[labelfont=bf]{caption}
\usepackage[
singlelinecheck=false
]{caption}
\usepackage[section]{placeins}

\bibliographystyle{unsrtnat}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
\date{} 					% Or removing it

\renewcommand{\shorttitle}{Anomaly Detection with DVAE}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Supplementary Material Anomaly Detection in Aeronautics Data with Quantum-compatible Discrete Deep Generative Model},
pdfsubject={Suppl Anomaly Detection with DVAE},
pdfauthor={Thomas Templin, Milad Memarzadeh, Walter Vinci, P. Aaron Lott, Ata Akbari Asanjan, Anthony Alexiades Armenakas, Eleanor Rieffel},
pdfkeywords={Generative modeling, Deep learning, Variational autoencoder, Anomaly detection, Restricted Boltzmann machine, Gibbs sampling, Quantum-assisted machine learning},
}

\begin{document}

\interfootnotelinepenalty=10000


\section*{Supplementary material} \label{supplement}

\subsection*{S1. The necessity of discrete-variable models} \label{suppl:1}

Real-world objects are frequently discrete: the objects in an image; phonemes in speech; symbols, words, and sentences in language. Our study on the unsupervised detection of anomalies in aeronautics data requires the differentiation of discrete classes (nominal vs. anomalous instances), and the input to our models consists of multivariate time series consisting of discrete temporal data points. Consequently, internal discrete-variable representations are frequently more interpretable and a more natural fit to modalities of interest present in datasets \citep{chen2016infogan, van2017neural}. When causes and/or consequences are discrete (`If the store doesn't have peaches, I'll buy apples.'), they are also a natural fit for complex reasoning, planning, and predictive learning \citep{van2017neural}.

Discrete variables also naturally represent typical machine-learning operations, such as choosing between models or variables \citep{khoshaman2018gumbolt}. An unsupervised model with a latent space composed of discrete variables can learn to disentangle content and style information of images \citep{makhzani2017pixelgan}. In semi-supervised learning, discrete variables can be used to label distinct semantic classes and to train generative models with more meaningful representations \citep{kingma2014semi, maaloe2017semi, khoshaman2019quantum}. Moreover, discrete variables pose a low burden on computer memory and are computationally efficient \citep{rae2016scaling, jang2016categorical}; they are also a natural match to digital computation.

\citet{rolfe2016discrete} points out that many practically relevant datasets contain information on discrete objects subject to continuous transformation, such as image datasets of natural objects that change position and orientation and experience variations in scene illumination. Such datasets comprise multiple disconnected smooth manifolds. The author notices that while continuous variations in pose and illumination are typical natural phenomena, it is very difficult to transform the image of a person to that of a car while remaining on the manifold of natural images. Rolfe goes on to suggest to model the selection of discrete real-world objects using discrete variables and the continuous transformations applied to each disconnected component with continuous variables. The author implements such a framework using an autoregressive network consisting of layers of continuous latent variables conditioned on a layer of discrete variables. Our models of discrete convolutional VAEs also follow Rolfe's prescription by combining encoders and decoders with continuous convolutional layers with a latent space with discrete variables.

While discrete-variable models promise gains in performance and improvements in efficiency in classical computing, they are outright obligatory for quantum computing. Quantum variables take the form of density matrices, the measurements of which will produce discrete values (bitstrings) as projective measurements of qubits $i = 1, 2, ..., n$ in the computational basis produce values $m_i \in \{-1, +1\}.$ The physical correlates of logical qubits are two-level quantum systems such as the polarization direction of light, Fock states of bosons, quadratures of coherent states, the spin direction of atomic particles, or the flow direction of superconducting current \citep{nielsen2000quantum, nakahara2008quantum, knill2010quantum}. Consequently, the measured binary qubit values $m_i$ constitute the interface between the quantum and classical computing worlds; their processing by the classical component of quantum-classical hybrid models requires discrete variables.


\subsection*{S2. Additional experiments relating to section \hyperref[transferability]{5.2 Model transferability}} \label{suppl:2}

We show the performance of two additional transferability experiments in supplementary figure \ref{fig:4s}. The data differ from the data shown in figure \hyperref[fig:7]{7} in that the threshold for the data shown in supplementary figure \ref{fig:4s}(a) is based on the training set of the baseline dataset and the threshold for the data depicted in supplementary figure \ref{fig:4s}(b) is derived from an average of the training-set thresholds of the baseline and DASHlink takeoff datasets. We used this mixed threshold because in the transferability experiments with post-training both the baseline and the DASHlink takeoff datasets were involved in the training of the models. The results of the experiment with baseline threshold and without post-training are quite imbalanced in the sense that recall is much better than precision, and the overall average F1 score is 3.74 percentage points below the one in the experiment with DASHlink/takeoff threshold and without post-training [figure \hyperref[fig:7]{7(a)}]. The results of the experiment with mixed baseline-DASHlink/takeoff threshold and post-training are very similar to the ones of the post-training experiment with DASHlink threshold.


\subsection*{S3. Additional information relating to section \hyperref[robustness]{5.3 Robustness of RBM model: Delay in flap deployment during approach to landing}} \label{suppl:3}

Histograms of anomaly scores for two training modes of the RBM model, with anomaly-score thresholds of about 980 and 1011, are shown in supplementary figure \ref{fig:5s}. In this study, anomaly scores correspond to the BCE [(\hyperref[eq:25]{17})], which contains a log transformation of the reconstructed data, and the greater normality of the raw anomaly scores is presumably due to the BCE error metric; in the studies described in sections \hyperref[baseline]{5.1} and \hyperref[transferability]{5.2}, we applied the MSE error metric to \textit{z}-transformed reconstructed and original input data and, during evaluation of model performance, log-transformed the resultant right-skewed anomaly scores. As in the baseline study with a dataset containing a drop-in-airspeed anomaly (see section \hyperref[baseline]{5.1}), we observed that models enter different modes during training and differ in their anomaly-detection performance depending on the selected mode. The mode with an anomaly-score threshold of about 980 produced a good model performance, with F1 scores $\sim$0.64, whereas the mode with a threshold of about 1011 resulted in inferior model performance (F1 scores $\sim$0.42). The superior performance of the mode with $thr \approx 980$ is illustrated by the cleaner separation of nominal and anomalous data. The mode with $thr \approx 1011$, on the other hand, is characterized by a greater number of false negatives (anomalous data to the left of the anomaly-score threshold). Other modes were not observed in this experiment.


\renewcommand{\figurename}{Figure S}
\subsection*{S4. Additional figures} \label{suppl:4}

\setcounter{figure}{0} 
\begin{figure}[!htb]
    \captionsetup{singlelinecheck = true, justification=justified, font=footnotesize, labelsep=period, width=1\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figure1supplement.eps}
    \let\nobreakspace\relax
    \caption{Typical total loss (negative of $\beta$-ELBO) as well as its two components reconstruction loss and KL-divergence loss during training and validation for 400 epochs. The KL-divergence loss (weighted with $\beta$) is negative because the log probability of the RBM prior, $\mathrm{log} \; p_{\bm{\theta}}(\mathbf{z})$, is not normalized. The loss due to the $\mathrm{L}_2$ penalty on the RBM's weight matrix \textbf{W} is zero because this feature, which did not produce any performance gain, was not used during the training of the VAE/RBM model.}
    \addtocounter{figure}{-1}
    \phantomcaption
    \label{fig:1s}
\end{figure}

\begin{figure}[!htb]
    \captionsetup{singlelinecheck = true, justification=justified, font=footnotesize, labelsep=period, width=1\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figure2supplement.eps}
    \let\nobreakspace\relax
    \caption{Evolution of the first four components of the RBM's bias vector \textbf{a} during model training, suggesting a dynamic exploration of the energy landscape of the system's configuration space.}
    \addtocounter{figure}{-1}
    \phantomcaption
    \label{fig:2s}
\end{figure}

\begin{figure}[!htb]
    \captionsetup{singlelinecheck = true, justification=justified, font=footnotesize, labelsep=period, width=1\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figure3supplement.eps}
    \let\nobreakspace\relax
    \caption{Evolution of the first four elements of the first column of the RBM's weight matrix \textbf{W} during model training, suggesting a dynamic exploration of the energy landscape of the system's configuration space.}
    \addtocounter{figure}{-1}
    \phantomcaption
    \label{fig:3s}
\end{figure}

\begin{figure}[!htb]
    \captionsetup{singlelinecheck = true, justification=justified, font=footnotesize, labelsep=period, width=1\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figure4supplement.eps}
    \let\nobreakspace\relax
    \caption{Performance of VAE models with Gaussian, Bernoulli, and RBM priors in the study examining the ability of the models tuned and trained on the baseline data to transfer to the DASHlink takeoff data. Error bars indicate standard deviations of performance metrics among 16 independently trained models. Performance was assessed based on (a) a threshold derived from the baseline training set, without post-training, or (b) the average value of the thresholds derived from the baseline and DASHlink/takeoff datasets, with post-training on the new dataset for 300 epochs with model weights initialized by training to convergence on the baseline dataset. The figure demonstrates an imbalance between precision and recall (recall $\gg$ precision) in the experiment without post-training and, in the post-training condition, similar findings as in the post-training experiment with a threshold based on the DASHlink/takeoff training set.}
    \addtocounter{figure}{-1}
    \phantomcaption
    \label{fig:4s}
\end{figure}


\begin{figure}[!htb]
    \captionsetup{singlelinecheck = true, justification=justified, font=footnotesize, labelsep=period, width=1\textwidth}
    \centering
    \includegraphics[width=1\textwidth]{figure5supplement.eps}
    \let\nobreakspace\relax
    \caption{Histograms of average anomaly scores for two training modes of the RBM model trained and tested on the dataset with late-deployment-of-flaps anomaly during approach to landing. Data points to the right of the dashed threshold line are categorized as anomalies. The mode with a threshold of $\sim$980 demonstrates a good separation of nominal and anomalous data, whereas the mode with a threshold of $\sim$1011 exhibits a less good separation, with a relatively high number of anomalous data classified as nominal (false negatives).}
    \addtocounter{figure}{-1}
    \phantomcaption
    \label{fig:5s}
\end{figure}

\bibliography{references_supplement} \label{bibliographySuppl}

\end{document}
