\section{Related Work}
\label{sec:related_work}

%==== Keypoints ====%

%List related papers with a BibTeX reference and a tlâ€™dr (a description with a few sentences). Try to classify the papers into 2-3 categories. Keep adding papers.

%==================

%We briefly overview the literature related to our work in the followings, focusing on generative modeling for 3D shapes and part-level 3D representations.

% \Seungwoo{TODO: Briefly introduce GAN-based generative models in the first few sentences and start introducing diffusion-based works in details}
\paragraph{3D Generative Models.}

The first 3D generative models are based on GAN, learning a distribution of latents that can be decoded into various 3D representations such as point clouds~\cite{Achlioptas:2018LatentGAN,Valsesia:2019GraphConv,Shu:2019Treegan} and implicit representations~\cite{Kleineberg:2020VoxelGAN,Hao:2020Dualsdf,Chen:2019ImNet,Ibing:2021GridBased,Zheng:2022SdfStylegan}. Later research~\cite{Chan:2022EG3D,Gao:2022Get3d} also proposed to leverage a 2D discriminator in the 3D GAN training while projecting the 3D shape to 2D via differentiable rendering~\cite{Laine:2020NVDiffrast, Mildenhall:2020NeRF}. Autoregressive models for 3D data have also been introduced to produce meshes~\cite{Nash:2020Polygen}, point clouds~\cite{Sun:2020PointGrow}, or (ir)regular feature grids~\cite{Zhang:20223DILG,Yan:2022ShapeFormer}, which have also been extended to handle conditional inputs in the completion~\cite{Yan:2022ShapeFormer} and multi-modal generation~\cite{Mittal:2022Autosdf,Fu:2022Shapecrafter} tasks.
Recent work focused on exploiting the better generation capabilities of diffusion and score-based models.
Cai~\etal~\cite{Cai:2020ShapeGF}, Luo and Hu~\cite{Luo:2021DPM}, and Zhou~\etal~\cite{Zhou:2021PVD} were the first proposing score-based~\cite{Cai:2020ShapeGF} or diffusion-based~\cite{Luo:2021DPM,Zhou:2021PVD} frameworks learning distributions of point clouds. Hui~\etal~\cite{Hui:2022NeuralWavelet} proposed to learn diffusion over wavelet coefficients of truncated signed distance functions. The recent success of latent diffusion models (LDMs)~\cite{Rombach:2022LDM} for 2D images also prompted to develop diffusion models operating on latent vectors of either the entire 3D shapes~\cite{Chou:2022Diffusionsdf, Nam:20223D-LDM} or each point~\cite{Zeng:2022LION}, voxel~\cite{Li:2022Diffusion-Sdf}, and triplane~\cite{Shue:2022TriplaneDiffusion} (note that all of them are \emph{concurrent} work except for LION~\cite{Zeng:2022LION}). Conditional models taking texts~\cite{Nichol:2022Point-E} or multimodal data~\cite{Li:2022Diffusion-Sdf, Cheng:2022SDFusion} are also concurrently introduced with our work.
%\Seungwoo{They are not just latent diffusion models, but take spatial prior into account when introducing such latents}

The advances in 3D generative models have shown significant improvement in the quality of produced shapes, although, in our work, we focus on introducing a more \emph{versatile} 3D generative model that can be used not only for shape generation but also for shape editing and completion \emph{without} any additional training for the conditional setups (yet also achieving the SotA generation results). We aim to fully utilize the manipulation capabilities of the diffusion model with a compact part-level implicit representation of 3D shapes.
\vspace{-10pt}

\paragraph{Part-Level Implicit 3D Representations.}
There is a large body of work exploring part-level 3D decomposition, although most of which focuses on segmenting or abstracting a supervised~\cite{Yi:2016SIGA,Qi:2017Pointnet,Qi:2017Pointnet++,Mo:2019Partnet,Mo:2019StructureNet} and unsupervised~\cite{Tulsiani:2017VolumetricPrimitives,Sun:2019HA,Yang:2021Cubseg,Paschalidou2019:Superquadrics,Chen:2020BspNet,Deng:2020Cvxnet,Paschalidou:2021NeuralParts} ways.
Recent work coupled the part-level structure with the implicit shape representation to enable shape manipulation with the part representation parameters. SIF~\cite{Genova:2019LearningShapeTemplates} and LDIF~\cite{Genova:2020LDIF} first introduced the idea of combining a set of Gaussians in the 3D space to local implicit functions corresponding to each of them. 
NeuralTemplate~\cite{Hui:2022NeuralTemplate} instead used a set of convexes as the part-level extrinsics and connected each of them with a latent vector decoded into a local implicit function. 
SPAGHETTI~\cite{Hertz:2022Spaghetti} employed 3D Gaussians again but trained the network so that the Gaussians can not only approximate the shape but also transform a local region with its mean and covariance parameters.
We base our work on SPAGHETTI and present a framework of learning diffusion on the SPAGHETTI representation.
While SPAGHETTI also provided an auto-decoding-based shape generation pipeline, we demonstrate that our cascaded model diffusing on extrinsics and intrinsics sequentially produces shapes with much better quality while learning the exact data distributions on both spaces.
