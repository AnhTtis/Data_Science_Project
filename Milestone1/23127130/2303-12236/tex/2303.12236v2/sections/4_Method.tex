\section{SALAD -- Part-Level Cascaded Diffusion}
\label{sec:method}
% \begin{figure*}[t!]
%     \centering
%     \dummyfig{0.98\linewidth}{0.4\linewidth}{Overall pipeline of our cascaded diffusion model with a conditional encoder and a decoder.}
%     \caption{An overview of our two-stage cascaded latent diffusion model. The first stage latent diffusion model learns a distribution of extrinsic latents. The second stage latent diffusion model learns a conditional distribution of intrinsic latents given extrinsics.}
%     \label{fig:our_pipeline}
% \end{figure*}

\begin{figure*}
\label{fig:pipeline}
\includegraphics[width=\linewidth]{figures/pipeline_draft.pdf}
% \includegraphics[width=\linewidth]{figures/pipeline_draft_compressed.pdf}
% \caption{An overall pipeline of ~\salad{}.}
% \caption{\textbf{Pipeline overview.} \salad{} is a cascaded diffusion model consisting of two diffusion models, each capturing an extrinsic and an intrinsic distribution, respectively. During the first phase (left), extrinsic parameters representing overall structure of shapes are generated through an iterative denoising process. The following phase (right) is another denoising process that takes the outputs from the preceding phase as conditions, producing intrinsic latents encoding local geometry information.}
\caption{\textbf{Pipeline overview.} \salad{} consists of two diffusion models for extrinsic and intrinsic vectors, respectively. During phase 1 (left), it generates extrinsic vectors representing structures of shapes. Phase 2 (right) takes these outputs as conditions and produces intrinsic vectors encoding local geometry information.}
% \vspace{-\baselineskip}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=0.8\linewidth]{figures/pipeline_cropped.pdf}
\caption{\textbf{Architecture diagrams.} The architecture for Diffusion of $\mathbf{z}$ is a sequence of $M$ alternating MLPs and AdaIN~\cite{Perez:2018AdaLN} layers. Time-Conditioned Transformer, a Transformer~\cite{Vaswani:2017Attention} architecture designed to handle diffusion on set data, replaces MLPs with self-attention layers. \salad{} is a cascaded two Time-Conditioned Transformers: one for diffusion of $\{\mathbf{e}_i\}_{i=1}^N$ and the other for $\{\mathbf{s}_i\}_{i=1}^N$. In the second phase of \salad{}, a concatenation of $\{\mathbf{e}_i\}_{i=1}^N$ and $\gamma(t)$ is fed to AdaIN layers as conditioning input.}
\label{fig:pipeline_diagram}
\end{figure*}

Here we introduce our cascaded diffusion framework generating the part-level implicit shape representation.
In the shape representation introduced in Section~\ref{sec:background_part_representation}, note that there are multiple \emph{layers} of representations all of which can be decoded into the original shape, such as the global latent $\mathbf{z}$, the set of part latents $\{\mathbf{p}_i\}$, and the set of extrinsic and intrinsic vectors $\{(\mathbf{e}_i, \mathbf{s}_i)\}$. Below, we first introduce some preliminary approaches to learning diffusion for each representation, and then we propose our final cascaded framework for learning diffusions in two phases.

\vspace{-10pt}
\paragraph{Diffusion of $\mathbf{z}$.}
Learning diffusion in the space of the global shape latent $\mathbf{z}$ is straightforward; the noise prediction network $\boldsymbol{\epsilon}_{\theta}$ (in Equation~\ref{eq:ddpm_loss}) can be simply modeled as an MLP. In the network $\boldsymbol{\epsilon}_{\theta}$, the timestep $t$ is generally first transformed by a positional encoding $\gamma(\cdot)$~\cite{Vaswani:2017Attention} and then fed as the scale and translation factors to the adaptive normalization layers such as AdaIN~\cite{Perez:2018AdaLN}.
In our experiments (Section~\ref{sec:shape_generation}), we show that this simple diffusion already outperforms the quality of generation by sampling $\mathbf{z}$ from the unit Gaussian since it can learn the exact distribution of $\mathbf{z}$, although the improvement is marginal.

\vspace{-10pt}
\paragraph{Diffusion of $\{ \mathbf{p}_i \}_{i=1}^{N}$.} To improve the quality of generation, one can instead consider diffusing the set of part latents $\{ \mathbf{p}_i \}_{i=1}^{N}$. A simple MLP taking the concatenation of the part latents as input, however, results in diffusion in a very high-dimensional space and also does not address the order invariance of the set data. We employ Transformer~\cite{Vaswani:2017Attention} to properly handle the set data while also promoting communications across parts. Each self-attention block is equipped with a post-MLP, where the positional-encoded timestep $\gamma(t)$ is fed to the AdaIN layer. This part-level latent diffusion can better reproduce the details of each part, while it still suffers from the difficulty in diffusing in a high-dimensional latent space.

\vspace{-10pt}
\paragraph{Cascaded Diffusion of $\{ \mathbf{e}_i \}_{i=1}^{N}$ and $\{ \mathbf{s}_i \}_{i=1}^{N}$.}
Inspired by Ho~\etal~\cite{Ho:2022CascadedLDM} introducing \emph{cascaded} diffusion for images, diffusing low-resolution images first and then diffusing high-resolution images conditioned on the low-resolution outputs, we propose a \emph{two-phase} framework for learning diffusion. We observe that the extrinsic and intrinsic attributes $\{ \mathbf{e}_i \}_{i=1}^{N}$ and $\{ \mathbf{s}_i \}_{i=1}^{N}$ play similar roles to low- and high-resolution images; the former describes the approximate of the data, while the latter captures fine details. Also importantly, the extrinsic vector $\mathbf{e}_i$ is much lower-dimensional, thus easier to make the noise prediction converge. Thus, in our first phase, we learn the diffusion of $\{ \mathbf{e}_i \}_{i=1}^{N}$ with the same Transformer-based noise prediction network $\boldsymbol{\epsilon}_{\theta}$ above. Then, in the second phase, we use another Transformer-based network $\boldsymbol{\epsilon}_{\phi}$ to model a conditional distribution $p(\{ \mathbf{s}_i \}_{i=1}^{N} | \{\mathbf{e}_i \}_{i=1}^{N})$ given $\{\mathbf{e}_i \}_{i=1}^{N}$.
Specifically, in the post-MLP of the self-attention block, for each $\mathbf{s}_i$, now the AdaIN layer takes as input a concatenation of the positional-encoded timestep $\gamma(t)$ and a feature vector $\mathcal{E}(\mathbf{e}_i)$ learned from the corresponding extrinsic parameters $\mathbf{e}_i$. The features $\{\mathcal{E}(\mathbf{e}_i) \}_{i=1}^{N}$ are learned from an additional stack of the self-attention modules encoding $\{ \mathbf{e}_i \}_{i=1}^{N}$. Both of the noise prediction networks $\boldsymbol{\epsilon}_{\theta}$ and $\boldsymbol{\epsilon}_{\phi}$ are trained with the same variational bound loss with Equation~\ref{eq:ddpm_loss} as follows:
%
\begin{equation}
\begin{aligned}
\label{eq:simple_loss}
\vspace{-\baselineskip}
    \mathcal{L}_{\mathbf{e}}(\theta) &:= \mathbb{E}_{t,\{\mathbf{e}\}_{i=1}^N,\Veps} \left[ \left\lVert \Veps - \Veps_{\theta}\left(\{\mathbf{e}^{(t)}\}_{i=1}^N,\gamma(t)\right) \right\rVert^2 \right]
    \raisetag{20pt}
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\vspace{-\baselineskip}
    \mathcal{L}_{\mathbf{s}}(\phi)&:= \mathbb{E}_{t,\{\mathbf{s}\}_{i=1}^N,\Veps} \left[ \left\lVert \Veps - \Veps_{\phi}\left(\{\mathbf{s}^{(t)}\}_{i=1}^N,\gamma(t), \{\mathbf{e}^{(0)}\}_{i=1}^N\right) \right\rVert^2 \right]
%    \mathcal{L}_{\mathbf{s}}(\phi)&:= \mathbb{E}_{t,\{\mathbf{s}\}_{i=1}^N,\Veps} \left\lVert \Veps - \Veps_{\theta}\left(\{\mathbf{s}^{(t)}\}_{i=1}^N,\gamma(t), \{\mathbf{e}^{(t)}\}_{i=1}^N\right) \right\rVert^2 
\end{aligned}
\end{equation}
%
where $\mathbf{e}^{(t)}$ and $\mathbf{s}^{(t)}$ are the extrinsic and intrinsic attributes after $t$-step forward process of adding Gaussian noise, respectively. Refer to the \textbf{supplementary material} for more implementation details.

% predict noise of $\{ \mathbf{s}_i \}_{i=1}^{N}$ while conditioning the post-MLP layer with both the timestep $t$ and features learned from the input set of extrinsics vectors $\{ \mathbf{e}_i \}_{i=1}^{N}$. Specifically, we encode each vector in $\{ \mathbf{e}_i \}_{i=1}^{N}$ to with self-attention modules and 

%In additional, we clip negative eigenvalues in generated extrinsic parameters to $1\times10^{-4}$.

% With the shape representation introduced in Section~\ref{sec:spaghetti}, the straightforward way to learn diffusion is to directly apply the latent diffusion~\cite{} to the space of the global latent code $\mathbf{z}$. 


% The next approach we can consider 


% % To design a compact 3D shape generative model that can produce high-quality shapes while also being capable of manipulating shapes in a part level, we propose a part-level latent diffusion model. 
% % on part-level of shapes, we utilize a object representation proposed by Hertz~\etal~\cite{Hertz:2022Spaghetti}. 

% \Charlie{This paragraph briefly summarize the entire section}
% To design a diffusion model that could leverage the part-based representation, we first propose a single phase diffusion model that directly learn the distribution of part latents. However, as demonstrated in \ref{sec:experiment}, this model lack the capability to \Charlie{output high quality generation or diversity depends on experiment result}. Motivated by recent work in diffusion models by Ho~\etal~\cite{Ho:2022CascadedLDM}, we propose a cascaded diffusion model design that independently learn the distribution of extrinsic and intrinsic representation. Finally, we show that the later design could be further extended for fine-grained conditional generation which the single phase model could not.

% %%%%


% %%%%

%  \subsection{Single Phase Diffusion Model}
% Following Rombach~\etal~\cite{Rombach:2022LDM}, we are interested in modeling distribution of latents. Specifically, in this work, we model the distribution of a set of part latents. 
% % Let $\B{P} \in \Real^{N\times D}$ be a set of $\B{p}_i$ encapsulating the information of a 3D shape. 
% Let $\{\B{p}_i\} \in \Real^{N\times D}$ be a set of $\B{p}_i$ encapsulating the information of a 3D shape.
% As discussed in Sec.~\ref{sec:background_ddpm}, a diffusion model is trained to predict gaussian noise $\Veps$ added to $\{\B{p}_i\}$ at every timestep $t$: 
% \begin{equation}
% \begin{aligned}
% \label{eq:simple_loss}
%     \mathcal{L}_{\text{simple}} := \mathbb{E}_{t,\{\B{p}_i\},\Veps\sim \mathcal{N}(0, \B{I})} [ \Vert \Veps - \Veps_{\theta}(\{\B{p}_i^{(t)}\},\gamma(t)) \Vert_2^2],
% \end{aligned}
% \end{equation}
% where $\{\B{p}_i^{(t)}\}$ is noisy set of part latents at timestep $t$, and $\gamma(\cdot)$ is a positional encoding that maps $\Real$ to a higher-dimensional space $\Real^{L}$.~\Juil{equation of positional encoding?} Unlike 2D images, our representation is irregular and forms a set that is permutation invariant. Thus, inspired by Lee~\etal~\cite{Lee:2019SetTF}, we implement a network by Transformer~\cite{Vaswani:2017Attention} instead of U-Net~\cite{Unet} which is a \emph{de facto} standard architecture of 2D diffusion models. Except for essential modifications for conditioning, mainly timestep $t$, we faithfully follow standard Transformer encoder architecture which consists of a stack of self-attention layers and fully-connected layers. When we take into account of conditions to the network, we insert an adaptive layer normalization (adaLN)~\cite{adaln} operation after each fully-connected layer, following the widespread usage in GANs~\cite{Karras:2020StyleGAN2} and diffusion models~\cite{Ho:2019DDPM, Luo:2021DPM}. 

% In short, Figure~\ref{fig:transformer_block} depicts an overview of our single-phase model. The network takes the embeddings of input latents, and timestep $t$ scales and shifts every intermediate feature through the adaLN operation. Finally, the time-conditioned Transformer encoder predicts noise through self-attention layers. However, this na\"ive latent diffusion model fails to produce high fidelity shapes. Thus, we propose a cascaded latent diffusion model.

% \subsection{Cascaded Latent Diffusion Model}
% In this section, we describe our cascaded latent diffusion model. As discussed in Sec.~\ref{sec:background_part_representation}, our part-level representation $\B{p}_i$ is further separated into $\B{e}_i$ and $\B{s}_i$ which entail an input shape's overall rough structure, and fine-detailed surface information, respectively. We observe that it is much easier to first build rough geometry and then fill in the remaining details than to capture all the information of a shape at once, as similar as an artist who frames a sketch and then adds details. From this perspective, to fully leverage the advantage of this representation, we design a two-phase training strategy, inspired by Ho~\etal~\cite{Ho:2022CascadedLDM}. The first phase mdoel parameterized by $\phi$ is an unconditional latent diffusion model which predicts noise on $\{\B{e}_i\}\in\Real^{N\times D_e}$. 

% We found that extrinsic parameters contain elements with unstable numerical values such as $\pi$ and $\boldsymbol{\lambda}$. After normalizing these elements based on element-wise means $\mu_i$ and standard deviations $\sigma_i$ of all training data following \ref{eq:normalizing}, our model perform significantly better. 

% \begin{equation}
%     \hat{x}_i = \cfrac{(x_i - \mu_i)}{\sigma_i}.
% \label{eq:normalizing}
% \end{equation}
% This result agrees with previous work of Rombach~\etal~\cite{Rombach:2022LDM} on how avoiding arbitrary high-variance latent space during training could lead to better network performance. At inference time, we rescale these elements by the means and standard deviations.

% % As Rombach~\etal~\cite{Rombach:2022LDM} stated, to avoid arbitrary high-variance latent space during training of the latent diffusion model, we normalize these elements based on element-wise means $\mu_i$ and standard deviations $\sigma_i$ of all training data.
% % \begin{equation}
% %     \hat{x}_i = \sigma_i x_i + \mu_i
% % \end{equation}

% On the other hand, the second phase model parameterized by $\psi$ is a \emph{conditional} latent diffusion model. It models a conditional distribution $p(\{\B{s}_i\} \vert \{\B{e}_i\})$ which leverages $\{\B{e}_i\}$ . 
% The second phase model is implemented by consecutive two Transformer encoders. The first encoder encodes $\B{e}_i$ to global-aware extrinsic embeddings $f(\B{e}_i)$. The following Transformer encoder is conditioned by a concatenation of $f(\B{e}_i)$ and $\gamma(t)$. Since the correspondence between $\B{e}_i$ and $\B{s}_i$ is given in training, we do not apply cross-attention between them. 

% In summary, the training objective function for each phase is summarized by:
% \begin{align}
%     \mathcal{L}_{ext} &= \mathbb{E}_{t,\{\B{e}_i\},\Veps} [ \Vert \Veps - \Veps_{\phi}(\{\B{e}^{(t)}_i\}, \gamma(t)) \Vert_2^2], \\
%     \mathcal{L}_{int} &= \mathbb{E}_{t,\{\B{e}_i\},\{\B{s}_i\},\Veps} [ \Vert \Veps - \Veps_{\psi}(\{\B{s}^{(t)}_i\}, \gamma(t) |\,\{\B{e}_i\}) \Vert_2^2].
% \end{align}

% To generate a 3D shape at inference time, we sample the latents sequentially. First, we sample extrinsic set $\{\hat{\B{e}}_i\}$ from the first phase model, followed by the rescaling of the elements which are normalized in training. To obtain the correspoinding intrinsic latents set $\{\hat{\B{s}}_i\}$, we feed $\{\hat{\B{e}}_i\}$ into the second phase model. Finally, by feeding the sampled $\{ \hat{\B{e}}_i \}$ and $\{ \hat{\B{s}}_i \}$ to the pre-trained decoder $\mathcal{D}$, we can obtain the neural implicit representation of a generated shape according to Equation~\ref{eq:decoder}.
% % \begin{equation}
% %     \mathcal{L}_{ext} = \cfrac{1}{M} \sum_{j}^{M} \mathbb{E}_{\Ve_j ,t, \Veps \sim \mathcal{N}(0,\mathbf{I})} || \Veps - \Veps_\theta( \Ve_j^{(t)}, \gamma(t))||_2^2,
% % \label{eq:phase1_loss}
% % \end{equation}



% % The phase one model is an unconditional latent diffusion model~\cite{Rombach:2022LDM} with $\{\Ve_j\}$. 
% % where $\Ve_j^{(t)}$ indicates a noised\Charlie{noisy} extrinsic latent at timestep $t$ and $\gamma(\cdot)$ is a positional encoding. The phase 1 model $\Veps_\theta$ is implemented as a variant of the Transformer encoder: a stack of self-attention layers and MLPs. 

% % By predicting noises at every uniformly sampled timestep $t$ as Equation~\ref{eq:phase1_loss}, the model learns the distribution of extrinsic latents. \Juil{We might need to include index encoding.}
% % % As Rombach~\etal~\cite{Rombach:2022LDM} stated that it is crucial to avoid arbitrary high-variance latent space during training of the diffusion model, we normalize elements with unstable numerical values, $\pi$ and $\boldsymbol{\lambda}$, by element-wise means $\mu_i$ and standard deviations $\sigma_i$ pre-computed from all training data. In inference time, we re-scale these elements:
% % However, we found that ground truth latents contain elements with unstable numerical values such as $\pi$ and $\boldsymbol{\lambda}$. After normalizing these elements based on element-wise means $\mu_i$ and standard deviations $\sigma_i$ of all training data following \ref{eq:normalizing}, our model perform significantly better. 
% % \begin{equation}
% %     \hat{x}_i = \sigma_i x_i + \mu_i.
% % \label{eq:normalizing}
% % \end{equation}
% % This result agree with previous work of Rombach~\etal~\cite{Rombach:2022LDM} on how avoiding arbitrary high-variance latent space during training could lead to better network performance.

% % Our phase two model is a \emph{conditional} latent diffusion model which models a conditional distribution $p(\Vs_j | \Ve_j)$. 
% % \begin{equation}
% %     \mathcal{L}_{int} = \cfrac{1}{M} \sum_{j}^{M} \mathbb{E}_{ \Ve_j,\Vs_j,t,\Veps\sim \mathcal{N}(0,\mathbf{I})} || \Veps - \Veps_\phi (\Vs_j^{(t)}, \gamma(t) |\, \Ve_j) ||_2^2
% % \end{equation}
% % Given a noised \Charlie{noisy} intrinsic latent $\Vs_j^{(t)}$ at time step $t$ and $\Ve_j$ as a condition, the phase 2 model $\Veps_\phi$ predicts noises $\Veps$. To condition both $\gamma(t)$ and $\Ve_j$ on the phase 2 model, we concatenate $\gamma(t)$ and embeddings of $\Ve_j$ as condition. The phase 2 model is implemented by two Transformer encoders where the first encoder embeds $\Ve_j$ to $h(\Ve_j)$ and the other encoder replace MLPs with TMLPs conditioned by $\gamma(t)$ and $h(\Ve_j)$. 

% % To generate a 3D shape in test time, we first sample $\{\hat{\Ve}_j\}$ from the phase 1 model and feed them to the phase 2 model to obtain the corresponding $\{\hat{\Vs}_j\}$. Lastly, the pre-trained decoder $\mathcal{D}$ returns an implicit field from the sampled $\{\hat{\Ve}_j\}$ and $\{\hat{\Vs}_j\}$.

% % \subsection{Conditioning Mechanism}
% % Our network is agnostic to arbitrary additional conditions. When additional condition $c$ is given, we can extract 