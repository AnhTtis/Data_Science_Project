\vspace{-5pt}
\section{Diffusion Models and Part-Level Shape Representation}
\label{sec:background}

%% DDPM
\subsection{Background on Diffusion Models}
\label{sec:background_ddpm}
We first briefly overview the technical background of diffusion models.
Diffusion models~\cite{Ho:2019DDPM} are latent variable models that approximate a data distribution $q (\mathbf{x}^{(0)})$ with a Markov chain, which is also called a \emph{reverse process}:
\vspace{-2pt}
\begin{align}
    p_{\theta} (\mathbf{x}^{(0)}) \coloneqq \int p_{\theta} (\mathbf{x}^{(0:T)}) d \mathbf{x}^{(1:T)},
    % p_{\theta} (\mathbf{x}_{0}) \coloneqq \int p_{\theta} (\mathbf{x}_{0} \vert \mathbf{x}_{T}) d \mathbf{x}_{T},
\end{align}
where $p_{\theta} (\mathbf{x}^{(0:T)}) = p (\mathbf{x}^{(T)}) \Pi_{t=1}^{T} p_{\theta} (\mathbf{x}^{(t-1)} \vert \mathbf{x}^{(t)})$.
% where $p_{\theta} (\mathbf{x}_{0:T}) = p (\mathbf{x}_{T}) \Pi_{t=1}^{T} p_{\theta}^{(t)} (\mathbf{x}_{t-1} \vert \mathbf{x}_{t})$, $p (\mathbf{x}_{T}) = \mathcal{N} (\mathbf{x}_{T}; \mathbf{0}, \mathbf{I})$.
% where $p_{\theta} (\mathbf{x}_{0} \vert \mathbf{x}_{T}) = p (\mathbf{x}_{T}) \Pi_{t=1}^{T} p_{\theta}^{(t)} (\mathbf{x}_{t-1} \vert \mathbf{x}_{t})$.
Here, $p (\mathbf{x}^{(T)}) = \mathcal{N} (\mathbf{x}^{(T)}; \mathbf{0}, \mathbf{I})$ is the standard normal prior
%, which is tractable and thus, can be sampled with ease.
enabling tractable sampling.

%%%% 3.2. Removing superscript for p_{\theta}{(t)}
The conditional probabilities $\{ p_{\theta} (\mathbf{x}^{(t-1)} \vert \mathbf{x}^{(t)}) \}_{t=1}^{T}$ are parameterized by a neural network whose weights are denoted by $\theta$.
% The conditional probabilities $\{ p_{\theta}^{(t)} (\mathbf{x}_{t-1} \vert \mathbf{x}_{t}) \}_{t=1}^{T}$ are parameterized by a neural network whose weights are denoted by $\theta$.
%%%%
%%%% 3.2. Distinguish forward process derived by chaining multiple kernels and a single step diffusion
The weights are optimized through the \emph{forward} diffusion process $q (\mathbf{x}^{(1:t)} \vert \mathbf{x}^{(0)})$ that sequentially adds Gaussian noises to the data $\mathbf{x}^{(0)} \sim q (\mathbf{x}^{(0)})$:
\begin{align}
\begin{gathered}
% \vspace{-2pt}
    q (\mathbf{x}^{(1:t)} \vert \mathbf{x}^{(0)}) \coloneqq \Pi_{s=1}^{t} q (\mathbf{x}^{(s)} \vert \mathbf{x}^{(s-1)}), \\
    \text{where} \,\, q (\mathbf{x}^{(s)} \vert \mathbf{x}^{(s-1)}) \coloneqq \mathcal{N} \left(\mathbf{x}^{(s)}; \sqrt{1 - \beta^{(s)}} \mathbf{x}^{(s-1)}, \beta^{(s)} \mathbf{I} \right),
    \raisetag{35pt}
\end{gathered}
\end{align}
% The weights are optimized through the \emph{forward} diffusion process $q (\mathbf{x}_{t} \vert \mathbf{x}_{0})$ that sequentially adds Gaussian noises to the data $\mathbf{x}_{0} \sim q (\mathbf{x}_{0})$:
% \begin{align}
%     q (\mathbf{x}_{t} \vert \mathbf{x}_{0}) \coloneqq \Pi_{t=1}^{T} q (\mathbf{x}_{t} \vert %\mathbf{x}_{t-1}),
% \end{align}
%%%%
%%%% 3. 3. Change index variable from 't' to 's'
% where $q (\mathbf{x}_{t} \vert \mathbf{x}_{t-1}) \coloneqq \mathcal{N} (\mathbf{x}_{t}; \sqrt{1 - \beta_{t}} \mathbf{x}_{t-1}, \beta_{t} \mathbf{I})$, and $\beta_{t}$ is an element of a monotonically increasing sequence $\beta_{1:T} \in (0, 1]^{T}$.
% where $q (\mathbf{x}^{(s)} \vert \mathbf{x}^{(s-1)}) \coloneqq \mathcal{N} (\mathbf{x}^{(s)}; \sqrt{1 - \beta^{(s)}} \mathbf{x}^{(s-1)}, \beta^{(s)} \mathbf{I})$,
and $\beta^{(s)}$ is an element of a monotonically increasing sequence $\beta^{(1:T)} \in (0, 1]^{T}$.
%%%%
% Chaining the diffusion kernels gives us the forward process modeled as Gaussian distribution:
By choosing Gaussians as forward diffusion kernels, the conditional densities $q (\mathbf{x}^{(t)} \vert \mathbf{x}^{(0)})$ at $t=1,\dots,T$ can be expressed in the closed form:
\begin{align}
% \vspace{-2pt}
    %%%% 3.2. Add \mathbf{x}_{t} as the random variable of normal distribution
    % q (\mathbf{x}_{t} \vert \mathbf{x}_{0}) = \mathcal{N} (\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0}, (1 - \bar{\alpha}_{t}) \mathbf{I}),
    q (\mathbf{x}^{(t)} \vert \mathbf{x}^{(0)}) = \mathcal{N} (\mathbf{x}^{(t)}; \sqrt{\bar{\alpha}^{(t)}} \mathbf{x}^{(0)}, (1 - \bar{\alpha}^{(t)}) \mathbf{I}),
    %%%%
\end{align}
where $\alpha^{(t)} \coloneqq 1 - \beta^{(t)}$ and $\bar{\alpha}^{(t)} \coloneqq \Pi_{s=1}^{t} \alpha^{(s)}$.
%Over the forward process, a sample $\mathbf{x}_{0} \sim q (\mathbf{x}_{0})$ is dissipated toward $q(\mathbf{x}_{T}) = \mathcal{N} (\mathbf{0}, \mathbf{I})$.
%In contrast to variational autoencoders (VAE)~\cite{Kingma:2014VAE}, the \emph{encoding} procedure $q (\mathbf{x}_{t} \vert \mathbf{x}_{t-1})$ is not trained but \emph{fixed} and the latent variables have the same dimension as the data $\mathbf{x}_{0} \sim q (\mathbf{x}_{0})$.
Over the forward process dissipating a sample $\mathbf{x}^{(0)} \sim q (\mathbf{x}^{(0)})$ toward $q(\mathbf{x}^{(T)}) = \mathcal{N} (\mathbf{0}, \mathbf{I})$, the weights $\theta$ parameterizing the reverse process $p_{\theta} (\mathbf{x}^{(0)})$ are learned by optimizing the following variational bound on negative log likelihood:
\begin{equation}
\begin{aligned}
% \vspace{-2pt}
    %%%% 3.2 
    %%%% Remove \max in front of expectations;
    %%%% flip the direction of inequality
    \mathbb{E}_{q (\mathbf{x}^{(0)})} & [-\log p_{\theta} (\mathbf{x}^{(0)})] \leq \\
    &\mathbb{E}_{q (\mathbf{x}^{(0)}, \dots, \mathbf{x}^{(T)})} \left[-\log \frac{p_{\theta} (\mathbf{x}^{(0:T)})}{q (\mathbf{x}^{(1:T)} \vert \mathbf{x}^{(0)})}\right].
    % \max_{\theta} \mathbb{E}_{q (\mathbf{x}_{0})} & [\log p_{\theta} (\mathbf{x}_{0})] \leq \\
    % &\max_{\theta} \mathbb{E}_{q (\mathbf{x}_{0}, \dots, \mathbf{x}_{T})} \Big[\log \frac{p_{\theta} (\mathbf{x}_{0:T})}{q (\mathbf{x}_{1:T} \vert \mathbf{x}_{0})}\Big].
    %%%%
\end{aligned}
\end{equation}
%%%% 3.2. Remove superscript (t)
Following Ho \etal~\cite{Ho:2019DDPM}, we parameterize our reverse process $p_{\theta} (\mathbf{x}^{(t-1)} \vert \mathbf{x}^{(t)})$ as:
% Following Ho \etal~\cite{Ho:2019DDPM}, we parameterize our reverse process $p_{\theta}^{(t)} (\mathbf{x}_{t-1} \vert \mathbf{x}_{t})$ as:
%%%%
%%%% 3.2. Remove superscript (t)
\begin{equation}
\begin{aligned}
% \vspace{-2pt}
    p_{\theta} (\mathbf{x}^{(t-1)} \vert \mathbf{x}^{(t)}) \coloneqq \mathcal{N} (\mathbf{x}^{(t-1)}; \boldsymbol{\mu}_{\theta} (\mathbf{x}^{(t)}, t), \beta^{(t)} \mathbf{I}).
    % p_{\theta}^{(t)} (\mathbf{x}_{t-1} \vert \mathbf{x}_{t}) \coloneqq \mathcal{N} (\mathbf{x}_{t-1}; \boldsymbol{\mu}_{\theta} (\mathbf{x}_{t}, t), \beta_{t} \mathbf{I}).
\end{aligned}
\end{equation}
%%%%
In particular, we use the parameterization $\boldsymbol{\mu}_{\theta} (\mathbf{x}^{(t)}, t) = \sfrac{1}{\sqrt{\alpha^{(t)}}} (\mathbf{x}^{(t)} - \sfrac{\beta^{(t)}}{\sqrt{1 - \bar{\alpha}^{(t)}}} \boldsymbol{\epsilon}_{\theta} (\mathbf{x}^{(t)}, t))$ and optimize its parameters $\theta$ with a training objective that encourages a network $\boldsymbol{\epsilon}_{\theta}$ to predict the noise $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ present in the given data:
\begin{align}
    \mathcal{L}(\theta) \coloneqq \mathbb{E}_{t, \mathbf{x}^{(0)}, \boldsymbol{\epsilon}} \left[\left\lVert \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_{\theta} \left(\sqrt{\bar{\alpha}^{(t)}} \mathbf{x}^{(0)} + \sqrt{1 - \bar{\alpha}^{(t)}}\boldsymbol{\epsilon}, t\right) \right\rVert^{2}\right].
    \label{eq:ddpm_loss}
\end{align}


% After learning the weights $\theta$, sampling is done by starting from a latent $\mathbf{x}_{T} \sim p (\mathbf{x}_{T})$ and proceeding the aforementioned reverse process % $p_{\theta} (\mathbf{x}_{0})$.
% However, a drawback of  such a model is that the latent $\mathbf{x}_{T}$ must go through every step of the reverse process $p_{\theta} (\mathbf{x}_{0})$ for sampling.
% As every step is dependent to the output from the immediate previous step, the entire procedure has to be done sequentially, requiring huge computation time particularly for large $T$.

%% DDIM
% Song~\etal~\cite{song2020denoising} addresses this issue by relaxing the Markov chain condition for both forward (diffusion) and reverse processes and using non-Markovian stochastic processes.
% This modification maintains most components of Ho~\etal~\cite{ho2019ddpm} while making small changes in the sampling strategy.
% The changes, however, can result in $10-100$ times speed up in inference time.
% Hence, we employ this diffusion model in our experiments.

%% SDEdit
% Diffusion models can be used to \emph{convert} data that are \emph{not} drawn from $q (\mathbf{x}_{0})$ to the one that is likely to be sampled from it while introducing minimal changes.
% Meng \etal \cite{meng2022sdedit} introduced image editing examples that generate images from coarse semantic layouts, or making local changes to the regions specified by user strokes.
% The idea for this is to \emph{inject} an image being edited into an intermediate step of the reverse process.

% Let $\Vz$ denote the input image. The image is first perturbed by undergoing the forward process:
% \begin{align}
%    \mathbf{x}_{t_{n}} \sim \mathcal{N} (\sqrt{\alpha_{t_{n}}} \Vz, (1 - \alpha_{t_{n}}) \mathbf{I}).
%    \label{eq:forward}
% \end{align}
% Then the perturbed image is \emph{denoised} over the reverse process, iterating the following step with timesteps $t \in [t_{n}, 0)$:
% \begin{align}
%    \mathbf{x}_{t-1} \sim p_{\theta}^{(t)} (\mathbf{x}_{t-1} \vert \mathbf{x}_{t}).
%    \label{eq:reverse}
% \end{align}
% Note that the reverse process starts from an intermediate timestep $t_{n} \leq T$.
% We leverage this diffusion-model-based image conversion method to create realistic projective texture images from silhouette images of an object, as described in the following section.


\begin{figure}[t!]
\label{fig:spaghetti_overview}
\includegraphics[width=\linewidth]{figures/pipeline2_draft.pdf}
% \caption{An overall pipeline of SPAGHETTI.}
\caption{\textbf{Part-Level implicit representation by Hertz~\etal~\cite{Hertz:2022Spaghetti}.} A latent vector $\mathbf{z}$ encoding global geometry is first mapped to a set of part latents $\{\mathbf{p}_i\}_{i=1}^N$, each of which is decomposed into extrinsic parameters $\{\mathbf{e}_i\}_{i=1}^N$ and intrinsic latents $\{\mathbf{s}_i\}_{i=1}^N$. The decoder, conditioned on $\{(\mathbf{e}_i, \mathbf{s}_i)\}_{i=1}$, outputs an occupancy value given a query point $\mathbf{x}$.}
\vspace{-10pt}
\end{figure}

\vspace{-15pt}
\subsection{Part-Level Shape Representation}
\label{sec:background_part_representation}
Neural implicit representations~\cite{Chen:2019ImNet, Park:2019Deepsdf, Mescheder:2019OccNet} have been widely exploited in 3D shape generation and reconstruction due to their advantages in capturing fine details without limitation in resolutions even with a small memory footprint. However, their disadvantage of not supporting intuitive editing and manipulation has been a hindrance to increasing their utilization. To remedy the drawback, recent works~\cite{Genova:2019LearningShapeTemplates,Genova:2020LDIF,Hao:2020Dualsdf,Hui:2022NeuralTemplate,Hertz:2022Spaghetti} introduced \emph{dual} representations combining explicit and implicit representations, taking advantage of both of them. Among them, Hertz~\etal~\cite{Hertz:2022Spaghetti}, which our work is based on, was the first introducing a hybrid representation integrating two types of disentanglements simultaneously into an implicit representation: 1) part-level disentanglement, representing each local region separately, and 2) extrinsic-intrinsic disentanglement, describing extrinsic properties (\ie~the approximate shape and transformations) with parameters in the 3D space while encoding intrinsic properties (\ie~geometric details) using a latent code. This novel representation, called SPAGHETTI~\cite{Hertz:2022Spaghetti}, is learned in an auto-decoding setup without any supervision of the part decomposition.

%\Juil{Wonder if we should mention SPAGHETTI in Method section.}
%\Charlie{Maybe we should rephrase spaghetti as representation rather than generative model.}
% Hertz~\etal presented SPAGHETTI~\cite{Hertz:2022Spaghetti}, a 3D implicit shape generative model based on a part-level shape representation. 
%Hertz~\etal presented SPAGHETTI~\cite{Hertz:2022Spaghetti}, a 3D implicit part-level shape representation. 
% In SPAGHETTI, a global shape latent $\Vz_a$ is decomposed to a set of part latents $\{\Vp_i\}_{j=1}^M$, where $M$ denotes the number of parts. Each part latent $\Vp_i$ is further separated into an extrinsic latent $\Ve_i$ and an intrinsic latent $\Vs_i$ through a linear projection. Intuitively, a set of extrinsic latents defines an overall rough structure of a 3D shape without any detailed geometry of 3D shapes. Whereas a set of intrinsic latents contains surface details and global geometric information of 3D shapes. Specifically, $\{\Ve_i\}$ is a set of 3D Gaussians whose GMM $g(\mathbf{x}):=\sum_{j}\pi_i \mathcal{N}(\mathbf{x} | \mathbf{c}_i, \mathbf{\Sigma}_i)$ imitates an overall structure of input 3D shape, where $\mathbf{c}_i\in\mathbb{R}^3$, $\mathbf{\Sigma}_i \in \mathbb{R}^{3 \times 3}$, and $\pi \in \mathbb{R}$ are a center position, a covariance matrix, and a mixing weight in order. The covariance matrix $\mathbf{\Sigma}_i$ is decomposed to eigenvalues $\boldsymbol{\lambda}_i \in \mathbb{R}^3$ and eigenvectors $\mathbf{Q}\in\mathbb{R}^{3\times3}$. 
% Given a set of pairs of extrinsic and intrinsic latents, an implicit decoder $\mathcal{D}$ predicts an occupancy value of query point $\mathbf{x}$. 
% \begin{equation}
%     o = \mathcal{D}(\mathbf{x}\, |\, \{\Ve_i\},\,\{\Vs_i\}),
% \end{equation}
%  where occupancy value $o \in [0,1]$ is 1 when the query point is inside the shape, and 0 otherwise.

In SPAGHETTI, a 3D shape is first mapped to a global latent $\mathbf{z}$ and then further encoded into a set of part embedding vectors $\{\Vp_i\}_{i=1}^{N}$, where $N$ denotes the number of parts. Each part embedding vector $\Vp_i$ is again mapped into both a set of extrinsic parameters $\Ve_i$ and an intrinsic latent $\Vs_i$ through an MLP. 
The set of extrinsic parameters $\Ve_i = \{\mathbf{c}_i, \mathbf{\Sigma}_i, \mathbf{\pi}_i \}$ of each part represents a Gaussian in the 3D space with mean $\mathbf{c}_i\in\mathbb{R}^3$ and covariance $\mathbf{\Sigma}_i \in \mathbb{R}^{3 \times 3}$, depicting an approximate shape of a part.
$\pi_i \in \mathbb{R}$ is the blending weight for the Gaussian mixture representation of the entire shape: $\sum_{i}\pi_i \mathcal{N}(\B{x} | \B{c}_i, \mathbf{\Sigma}_i)$, describing the volume of the shape as a probability distribution. Since $\{\Ve_i\}_{i=1}^{N}$ can only encode the part-level structural information, the intrinsic latents $\{\Vs_i\}_{i=1}^{N}$ supplement the detailed geometry information so that the pairs of the extrinsic parameters and intrinsic latents can be decoded back to the original shape in an implicit form. Specifically, an implicit decoder $\mathcal{D}$ is trained to predict an occupancy value at point $\mathbf{x}$:
% \vspace{-0.5\baselineskip}
\begin{equation}
\begin{aligned}
\label{eq:decoder}
    o = \mathcal{D}\left(\mathbf{x}\, \Big\vert \, \{\Ve_i\}_{i=1}^{N},\,\{\Vs_i\}_{i=1}^{N}\right),
\end{aligned}
\end{equation}
where occupancy value $o \in [0,1]$ is 1 when the query point is inside the shape, and 0 otherwise. 
The keys to achieving both the part-level and extrinsic-intrinsic disentanglements in the training of decoder $\mathcal{D}$ are the regularizations forcing a single pair $(\Ve_i, \Vs_i)$ of a part to determine the occupancy of each point, and the Gaussian parameters in $\Ve_i$ to transform the corresponding local region. See the original paper~\cite{Hertz:2022Spaghetti} for the details of the decoder training.

The extrinsic vector $\mathbf{e}_i$ is precisely represented as a $16$-dimensional vector $\{\mathbf{c}_i, \lambda_i^1, \lambda_i^2, \lambda_i^3, \mathbf{u}_i^1, \mathbf{u}_i^2, \mathbf{u}_i^3, \pi_i\}$, where $\lambda_i^j \in \mathbb{R}$ and $\mathbf{u}_i^j \in \mathbb{R}^3$ are eigenvalues and eigenvectors of the covariance matrix $\mathbf{\Sigma}_i$, while the intrinsic vector $\mathbf{s}_i$ is a 512-dimensional vector. Note that the much smaller extrinsic vector contains the approximate shape information of the part; we leverage this fact in our effective cascaded diffusion model.

Also, note that SPAGHETTI is trained in an auto-decoding setup while regularizing the global latent code $\mathbf{z} \in \mathbb{R}^{512}$ to follow the unit Gaussian. Thus, the shapes can be simply generated by sampling a latent code $\mathbf{z}$ from the unit Gaussian in the $\mathbf{z}$ space, although we demonstrate that diffusion in the extrinsic and intrinsic embedding spaces can produce much more plausible shapes (Section~\ref{sec:shape_generation}).

% One important property of this representation that previous part-level representations\cite{} lack is that the extrinsic parts are covariant to the corresponding output region under arbitrary affine transformation. In combination with diffusion model's ability to partially generate latent, this property is the key for our cascaded model to generate \Charlie{maybe highly diverse} novel shapes.

 % \begin{figure}
 %     \centering
 %     \dummyfig{0.98\linewidth}{0.9\linewidth}{Gaussians and decoded shape example.}
 %     \caption{An example of rough geometry represented by extrinsics and decoded shape.}
 %     \label{fig:example_output}
 % \end{figure}
 