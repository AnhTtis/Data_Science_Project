\vspace{-15pt}
\section{Introduction}
\label{sec:introduction}
\vspace{-5pt}
The staggering rise of the recent image generative model such as DALL-E 2~\cite{Ramesh:2022DALLE2}, StableDiffusion~\cite{Rombach:2022LDM}, and Midjourney~\cite{Midjourney} has drawn great attention to the diffusion models. With the state-of-the-art performance in generating data~\cite{Dhariwal:2021DiffusionBeatGANs, Ho:2019DDPM, Rombach:2022LDM,Ramesh:2022DALLE2,Midjourney}, diffusion models have quickly replaced existing generative models in many applications.
Besides the quality of the generated data, another key advantage of the diffusion models is the zero-shot capability in completion and editing. Recent research~\cite{Chung:2022MCG, Lugmayr:2022Repaint, Meng:2022SDEdit} has shown that diffusion models trained without any conditions can be applied to completion and editing tasks by starting the reverse process from partial data and properly guiding the process.

Such capabilities of the diffusion models have prompted attempts to apply them to 3D generation~\cite{Cai:2020ShapeGF, Luo:2021DPM, Nichol:2022Point-E, Zhou:2021PVD, Zeng:2022LION, Li:2022Diffusion-Sdf, Nam:20223D-LDM, Hui:2022NeuralWavelet}, although likewise the other neural 3D generation and reconstruction work, the key challenge in applying diffusion to 3D is to find an appropriate representation of 3D data. Particularly, to take full advantage of the diffusion models, both producing realistic data and being leveraged to editing and manipulation, a careful design of the 3D data representation is needed.
A naive adaption of the 2D image diffusion models to the 3D voxels is impractical due to the order of magnitude more computation time and memory. Hence, the earlier attempt to apply diffusion or score-based models to 3D (which has also been continued until recently) was to use point clouds as 3D representation~\cite{Cai:2020ShapeGF, Luo:2021DPM, Nichol:2022Point-E}, although the fine details of shapes could not be reproduced since the training computation is still too heavy to increase the resolution --- $2k$ points are used in training.
Later, some hybrid representations have been explored, such as points and voxels~\cite{Zhou:2021PVD}, points and features~\cite{Zeng:2022LION}, voxels and features~\cite{Li:2022Diffusion-Sdf}, although these were still limited in being trained with low-resolution 3D data.
Implicit representation has been proven to be the best to capture fine details in 3D generation and reconstruction~\cite{Park:2019Deepsdf,Chen:2019ImNet,Mescheder:2019OccNet}. Hence, concurrent work~\cite{Li:2022Diffusion-Sdf,Nam:20223D-LDM} introduced latent diffusion methods generating codes that can be decoded into implicit functions of 3D shapes.
However, then the diffusion in a latent space cannot be used for the \emph{guided} reverse process -- \eg~filling a missing part of a shape while preserving the others, and thus the model cannot be exploited for manipulation.
Neural wavelet~\cite{Hui:2022NeuralWavelet} is a notable exception that improves efficiency in training without a latent space but by learning diffusion in spectral wavelet space.
While it succeeded in producing local details, it is still nontrivial to specify a local region to be modified in the spectral space, thus limiting the model to be used in the manipulation tasks.
%\minhyuk{Need to refine. Missing Triplane.}

As a 3D diffusion model feeding two birds with one seed, achieving high-quality \emph{generation} and enabling \emph{manipulation}, we present our novel \textbf{S}hape P\textbf{A}rt-Level \textbf{LA}tent \textbf{D}iffusion Model, dubbed \textbf{\salad\textbf{}}.
Our work is inspired by recent work~\cite{Genova:2020LDIF,Hui:2022NeuralTemplate,Lin:2022Neuform,Hertz:2022Spaghetti} introducing disentangled implicit representations into \emph{parts}. The advantages of the part-level disentangled representation are in the \emph{efficiency} allocating the memory capacity of the latent code effectively to multiple parts, and also in the \emph{locality} allowing each part to be edited independently, thus best fitted to our purpose. We specifically base our work on SPAGHETTI~\cite{Hertz:2022Spaghetti} that learns the part decomposition in a self-supervised way. Each part is described with an independent embedding vector describing the extrinsics and intrinsics of the part as shown in Figure~\ref{fig:teaser}, and thus the parts that need to be edited or replaced can be easily chosen. It is a crucial difference from latent diffusion where the latent codes do not explicitly express any spatial and structural information and voxel diffusion where the region to be modified can only be specified in the 3D space, not in the shape.

Our technical contribution is the diffusion neural network designed to properly handle the characteristics of the part-level implicit representation, which is a \emph{set} of \emph{high-dimensional} embedding vectors. To cope with the set data and achieve permutation invariance while allowing global communications across the parts, we employ Transformer~\cite{Vaswani:2017Attention} and condition each self-attention block with the timestep in the diffusion process. The challenge is also in learning diffusion in the high-dimensional embedding space, which is known to be hard to train~\cite{Yu:2023Video}.
% --- thus, there is a trade-off between more expressibility and better convergence.
To get around the issue, we introduce a \emph{two-phase cascaded} diffusion model.
We leverage the fact that the part embedding vector is split into a small set of \emph{extrinsic} parameters approximating the shape of a part and a high-dimensional \emph{intrinsic} latent supplementing the detailed geometry information.
Hence, our cascaded pipeline learns two diffusions, one generating extrinsic parameters first and the other producing an intrinsic latent conditioned on the extrinsics, effectively improving the generation quality with the same computation resources.

% To resolve the aforementioned issues, we propose a 3D \textbf{S}hape P\textbf{A}rt-Level \textbf{LA}tent \textbf{D}iffusion Model, called \textbf{SALAD}. When designing a new 3D generative model, the \emph{desiderata} we consider is first, it should produce diverse and realistic meshes, and it also should provide users with a straightforward shape manipulation method. Inspired by Hertz~\etal~\cite{Hertz:2022Spaghetti} who has introduced a part-level shape representation, we observe the synergy between diffusion models and a part-level shape representation. Diffusion models present high-quality generations as well as a zero-shot capability of input synthesis, and the part-level shape representation decomposes a complex 3D shape into a set of simple parts. We demonstrate that the key ingredients of aiding 3D asset creation, high fidelity generation, intuitive shape manipulation, and completion, are only available when a diffusion model and a part-level representation are combined. 
% SALAD is a latent diffusion model based on a set of part-level shape latents consisting of a 3D shape. The part latents can represent a high-resolution 3D mesh with significantly fewer entities than point clouds. Furthermore, each part latent affects only the local region where it governs in the 3D space. Its locality enables part-level shape manipulation and completion.

% Since our shape representation is defined as a set, we use Transformer~\cite{Vaswani:2017Attention} architecture to address the permutation invariance of the part latents. Also, inspired by Ho~\etal~\cite{Ho:2022CascadedLDM}, we propose a two-phase training process, which is another crucial detail of the training. This training strategy splits the amount of information the model learns into two phases, thus making the training more stable and easier. Instead of running diffusion at once in a high-dimensional latent space, we separate the latent space into two sub-spaces. More precisely, we first train an unconditional latent diffusion model with a low dimensional latent which captures rough geometry information of a 3D space with a few parameters. Finally, to fill in the remaining detailed information for a final shape, we train another conditional latent diffusion model on the other latent space leveraging the low dimensional latent as a condition. Figure~\ref{fig:teaser} shows a coarse structure captured by the low-dimensional latents and its corresponding final decoded 3D shape. 

%\Minhyuk{Edit.}
% The effectiveness of our training strategy is demonstrated in Table~\ref{tbl:quantitative_comparison_of_shape_generation} which shows a big improvement. In addition, we demonstrate SALAD is suitable for various applications including part-level shape manipulation, shape completion, and conditional generation. In summary, the following are our contributions:
% \begin{itemize}
%    \item We introduce SALAD which generates diverse and high-resolution meshes with significantly less memory and computation. Quantitatively, we show that SALAD outperforms other baselines in random shape generation.
%    \item Our SALAD is capable of part-level shape manipulation, which enables various applications, such as part-level shape manipulation, completion, and language-conditioned manipulation.
%\end{itemize}

% Our quantitative and qualitative assessments on \salad{} demonstrate its outperformance compared with SotA methods in shape generation, as shown in Table~\ref{tbl:quantitative_comparison_of_shape_generation}. We also conduct extensive experiments on downstream tasks,  part-level shape completion, challenging or even nontrivial for previous work based on conventional shape representations (\eg voxels). The experimental results manifest the capability of our \salad{} in applications where part-level disentanglement is crucial. Last but not least, we also showcase the versatility of our \salad{} by extending it to text-guided generation and completion. To summarize, our contributions are:

Our quantitative and qualitative assessments on \salad{} demonstrate its outperformance compared with SotA methods in shape generation as shown in Section~\ref{sec:shape_generation}. We further demonstrate zero-shot manipulation capability of our \salad{}, trained solely for unconditional generation, by conducting extensive experiments on downstream tasks, including part completion (Section~\ref{sec:shape_completion}), part mixing and refinement (Section~\ref{sec:part_mixing}). %Particularly, results demonstrate the advantages of \salad{} allowing intuivie picking of parts that is not permitted in other representations lacking structural information. 
%outperforms previous work in part completion, which is challenging or even nontrivial without an explicit distinction between parts. 
% The experimental results manifest the capability of our \salad{} in applications where part-level disentanglement is crucial. 
Last but not least, we showcase the versatility of \salad{} in modeling multi-modal distributions such as text-guided generation (Section~\ref{sec:text_conditional_generation}) and completion (Section~\ref{sec:text_driven_manipulation}). To summarize, our contributions are:

% Novel diffusion model for part-level implicit representation
% Introduced cascaded framework, achieving better results. -> SotA in unconditional generation
% Showed the SotA performance in generation and also zero-shot manipulation capability.
% Text-guided, GaussGlot combination.
\vspace{-5pt}
\begin{itemize}
    \setlength\itemsep{0.08em}
    \item We propose \salad{}, a novel diffusion model capable of generating part-level 3D implicit representations.
 
    % \item We propose a \emph{two-phase cascaded} diffusion model, capable of handling high-dimensional latent spaces, that sets a new SotA in shape generation.
    \item We propose a \emph{two-phase cascaded} diffusion model, effective for handling high-dimensional latent spaces, that sets a new SotA in shape generation.
    % We explore a wide range of design choices encompassing both shape representations and network architectures, thereby proposing a \emph{two-phase cascaded} diffusion framework that sets a new SotA in unconditional generation.
    % \item Our \salad{} not only sets a new SotA in unconditional generation, but also demonstrates its zero-shot manipulation capability 
    % \item We demonstrate zero-shot capability of \salad{} in shape manipulation, where the orchestration of diffusion models and part-level implicit representations is crucial. 
    \item We demonstrate the importance of orchestrating diffusion models and part-level implicit representation for the zero-shot capability of \salad{} in shape editing.
    \item We further extend our \salad{} to text-guided generation and editing that can synergize with text-driven part segmentation network.
\end{itemize}

%Recent work also has shown that diffusion models can result in the best performance when they are trained in a latent space~\cite{LDM}.
%Latent diffusion is also the ideal choice to generate shapes in an \emph{implicit} representation. Following the idea, concurrent works introduced 3D latent diffusion methods generating high-quality 3D shapes, 

% % What is the motivation of your research (not your personal motivation, but why is it needed and important in the academia/industry?)
% % Why 3D generative model being capable of generating and manipulating shapes is important.
% % don't mention generative models here. focus on creation that includes generation and editing instead of generation only.
% \Juil{start with TL;DR.} Leaving the realm of creation to machines has long been studied. Creating assets requires considerable effort and time. In particular, as the demand for processing 3D data has been prevalent across various fields, such as computer vision, computer graphics, robotics, and the metaverse, it has become more important to reduce the cost required to generate a new 3D asset or to edit the existing asset. 
% % In response to 
% To address this, previous work~\cite{Achlioptas:2018latentgan, Hao:2020Dualsdf, Chen:2019ImNet, Yin:2020Coalesce} has proposed various methods, mainly focusing only on either shape generation or manipulation. 
% %However, it is still challenging to achieve both high fidelity and editability in handling 3D assets. 
% However, achieving high-fidelity generation while still allowing 3D assets to be editable remains a challenge. 
% To democratize the creation of 3D assets, it is essential to enhance not only a high-fidelity generation but also an interpretable shape manipulation. 

% %Diffusion: good generation results. also inherently capable of editing or synthesizing input data.
% % shorten content of other generative models. directly say about diffusion models. emphasize diffusion model characteristics: zero-shot editing capability to unseen images. SDEdit, etc.
% The recent advance of diffusion models~\cite{Ho:2019DDPM, Dhariwal:2021DiffusionBeatGANs} has led to significant growth in the 2D image generation, subsequently replacing the existing generative models~\cite{Goodfellow:2014GAN, Kingma:2014VAE, Rezende:2015Flow} with superior performance. The key advantage of diffusion models compared to other generative models is they have shown a zero-shot capability of editing and synthesizing input data. Without any supervision or context of task-specific conditions during training, its prior has demonstrated a capability of image inpainting~\cite{Lugmayr:2022Repaint, Chung:2022MCG} and image synthesis~\cite{Meng:2022SDEdit}. These attractive characteristics and the ability of diffusion models in the 2D image domain have prompted attempts to apply diffusion models to the 3D generation task.

% % 3D data representation
% % first paragraph: low-quality of early work
% % second paragraph: difficulty of manipulating 3d data in previous work
% Much previous work~\cite{Li:2022Diffusion-Sdf, Chou:2022Diffusionsdf, Shue:2022TriplaneDiffusion, Cheng:2022SDFusion, Hu:2023NeuralWavelet, Hui:2022NeuralWavelet, Cai:2020ShapeGF, Luo:2021DPM, Zeng:2022LION} has introduced diffusion models for 3D generation task. Early work of the literature employs diffusion models with 3D point clouds~\cite{Cai:2020ShapeGF, Luo:2021DPM, Zeng:2022LION} or a point-voxel hybrid representation~\cite{Zhou:2021PVD}, since na\"ively adopting 2D diffusion models to 3D voxels would require orders of magnitude more computational power. However, these early attempts fail to capture detailed shape surfaces due to the inherent limitation of shape\Charlie{pointcloud} representation, thus producing low-quality results. In addition, although they can convert output point clouds to iso-surface meshes, it requires another post-processing where a very precise computation is necessary. Otherwise, it is prone to produce noisy artifacts. 

% To obtain high resolution meshes using 3D diffusion models, concurrent work~\cite{Li:2022Diffusion-Sdf, Chou:2022Diffusionsdf, Cheng:2022SDFusion, Hui:2022NeuralWavelet, Hu:2023NeuralWavelet} has introduced %3D implicit function-based diffusion models. 
% diffusion models based on implicit function.
% To mitigate the high computational cost of running diffusion on high-resolution 3D voxels, they run diffusion on intermediate feature space. Albeit showing local shape manipulation using the intermediate latent space, it is still non-trivial for users to select local regions from the latent space, thus limiting them from even the simplest manipulation capability.
% I will edit before this line.

% SALAD explanation: shape representation and strengths
% remove all notations. explain in English.
% synergy between diffusion models and our representation.

% Recently, as the demand of processing 3D contents has been growing in various fields, such as meta-verse, robotics, computer vision, and computer graphics, coupled with the difficulty of handling 3D data, a 3D generative model that not only generates high quality shapes but also enables users to easily manipulate the 3D shapes has been required. However, most of previous 3D generative model research has focused on generation quality itself only.

% At the same time, as diffusion models~\cite{Dhariwal:2021DiffusionBeatGANs} have outperformed other existing generative models~\cite{Goodfellow:2014GAN, Kingma:2014VAE, Rezende:2015Flow} in the 2D image and audio domain, attempts to apply a diffusion model on 3D shapes have been introduced in various previous works~\cite{Chou:2022Diffusionsdf, Hui:2022NeuralWavelet, Li:2022Diffusion-Sdf, Zhou:2021PVD, Luo:2021DPM, Jiang:2020Shapeflow}. However, unlike 2D images, there is no standard 3D representation, querying which 3D representation is the most efficient in 3D shape generation. In response to the question, we propose a 3D \textbf{S}hape P\textbf{A}rt-level \textbf{LA}tent \textbf{D}iffusion Model, called \textbf{SALAD}, which provides straightforward local part editing capability with diverse and high-quality 3D generated shapes.  

% Our model is based on part-level latents, which is the most efficient 3D representation compared to other representations, such as point clouds, voxels, and meshes. A 3D shape is encoded to a set of part latents $\{\Vp_j\}$, which represents the shape with a much smaller number than point clouds and voxels. Each $\Vp_j$ is further separated into an extrinsic latent $\Ve_j$ and an intrinsic latent $\Vs_j$. \Juil{need to flesh out descriptions of the role of each latent.} Instead of learning a distribution of these latents at once, inspired by cascaded LDM~\cite{Ho:2022CascadedLDM}, we split the LDM training into two phases. In the first phase, we first model a distribution of $\{\Ve_j\}$ which represents a rough geometry structure of input shape without any surface details. Next, the second phase model learns a conditional distribution of $\{\Vs_j\}$ given $\{\Ve_j\}$, which corresponds to filling in detailed 3D surface information given the rough geometry. 
% We use Transformer~\cite{Vaswani:2017Attention} architecture on both phase 1 and phase 2 latent diffusion models which is suit for processing set data.
% In test time, sampled $\{\Ve_j\}$ and $\{\Vs_j\}$ are decoded to an implicit representation, which can be easily converted to a mesh, by a pre-trained implicit decoder $\mathcal{D}$. 

% We demonstrate our two-phase design is crucial to produce high-quality 3D shapes and two-level shape manipulation, overall modification with coarse structure $\{\Ve_j\}$, and fine customizing with $\{\Vs_j\}$. \Juil{argument of two-level shape manipulation doesn't seem to make sense considering our results..} 

% \Seungwoo{It would be good to summarize our contribution in several bullets}

% What is the limitation of previous work?
% -> Unlike 2D images, there is no standard representation. 
% -> Most of 3d generative models have focused on generation quality itself only.

% What is the exact problem setup of your work?
% -> Encode a 3D shape to part-level latents which are further separated to extrinsic and intrinsic latents.
% -> Instead of learning a distribution of these latents at once, inspired by cascaded LDM, we design a cascaded latent diffusion model.
% -> In the first phase, we first model a distribution of extrinsic latents which represents a rough geometry structure without any surface details.
% -> Next, our second phase model learns a conditional distribution of intrinsic given input extrinsics, which means the phase 2 model produce detailed surface information given the rough geometry.
% -> We demonstrate our two-phase design outperforms a single phase model that models data distribution of extrinsic and intrinsic at the same time.

% What is the key challenge to solving the problem?
% -> simple single-phase model doesn't work. our two phases setup is crucial for high quality generation.
% -> Transformer architecture is essential in modeling set representation. 

% A list of contributions you aim for.
% -> We first propose a diffusion model for 3D shape generation and straightforward local part "manipulation".

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% VERSION 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%In 3D data, data representation is important. ours is the best.
% Unlike 2D images, 3D data has no standard data representation. 


% In recent, modeling and manipulating 3D shapes have become more prevalent in various fields, such as robotics, computer vision, computer graphics, and the Metaverse, where it is common to process 3D data. Coupled with the difficulty of handling 3D data, the need for a 3D generative model capable of generating high-quality shapes as well as manipulating the shapes is increasing. In response to this, much previous work~\cite{Li:2022Diffusion-Sdf, Chou:2022Diffusionsdf, Shue:2022TriplaneDiffusion, Cheng:2022SDFusion, Hu:2023NeuralWavelet, Hui:2022NeuralWavelet, Cai:2020ShapeGF, Luo:2021DPM, Zeng:2022LION} has introduced diffusion models for 3D generation task. Na\"ively adopting 2D diffusion models directly to 3D voxels would require orders of magnitude more computational power. Thus, early work employs diffusion models with 3D point clouds~\cite{Cai:2020ShapeGF,Luo:2021DPM,Zeng:2022LION} or a point-voxel hybrid representation~\cite{Zhou:2021PVD}. However, this straightforward approach failed to capture detailed shape surface information and produce low-quality results. To resolve this, concurrent work~\cite{Li:2022Diffusion-Sdf,Chou:2022Diffusionsdf, Cheng:2022SDFusion, Hui:2022NeuralWavelet, Hu:2023NeuralWavelet} has attempted applying diffusion models to a 3D implicit function representation by running diffusion on intermediate latent space or wavelet-domain. Albeit achieving higher quality generation, Diffusion-SDF~\cite{Li:2022Diffusion-Sdf}, Neural Wavelet~\cite{Hui:2022NeuralWavelet, Hu:2023NeuralWavelet}, and SDFusion~\cite{Cheng:2022SDFusion} are still relatively computationally heavy since they use down-sampled 3D grid features. Another problem with these voxel-based representations is that they are 
% inherently difficult to manipulate. In particular, it is not trivial to select local regions in the representation that DiffusionSDF~\cite{Chou:2022Diffusionsdf} and Neural Wavelet~\cite{Hui:2022NeuralWavelet, Hu:2023NeuralWavelet} provide, and thus limiting them from even the simplest manipulation capability.

% To resolve all the aforementioned issues, we propose a 3D \textbf{S}hape P\textbf{A}rt-Level \textbf{LA}tent \textbf{D}iffusion Model, called \textbf{SALAD}, which is capable of manipulating and generating high-quality and diverse 3D shapes at a part level while costing significantly less memory and computation. Our model utilizes part-level latents that ensure local influence in a 3D shape and are disentangled to extrinsic properties and intrinsic properties. 

% Let a 3D shape be encoded to a set of part latents $\{\Vp_j\}_{j=1}^M$, where $M$ denotes the number of parts representing the shape. Each $\Vp_j$ is further separated into an extrinsic latent $\Ve_j$ and an intrinsic latent $\Vs_j$. Intuitively, $\Ve_j$ encapsulates basic geometric information of a local part and consists of a few parameters. Figure~\ref{fig:teaser} shows a coarse structure that $\{\Ve_j\}$ captures and the decoded 3D shape. On the other hand, the higher dimensional vector $\Vs_j$ captures detailed surface information as well as joint regions between the parts. This set of latents are then decoded to a 3D shape by an implicit decoder $\mathcal{D}$:
% \begin{equation}
% \begin{aligned}
%     o=\mathcal{D}(\mathbf{x} | \{\Ve_j\},\, \{\Vs_j\}),
% \end{aligned}
% \end{equation}
% where $\mathbf{x}\in\Real^3$ and $o\in [0,1]$ denote a query 3D point and its corresponding occupancy value, respectively. 
% Unlike 3D voxel-based diffusion models~\cite{Li:2022Diffusion-Sdf, Chou:2022Diffusionsdf, Cheng:2022SDFusion, Hui:2022NeuralWavelet, Hu:2023NeuralWavelet}, our shape representation is a set representation which is invariant to the permutation of its elements. Inspired by Lee~\etal~\cite{Lee:2019SetTF}, we guarantee this property by adopting Transformer~\cite{Vaswani:2017Attention}, which is permutation invariant, to our latent diffusion models. Unlike previous point cloud-based diffusion models~\cite{Cai:2020ShapeGF, Luo:2021DPM} ~\Juil{Not sure if LION also doesn't use self-attention.}that uses point-wise feed-forward layers, we demonstrate that the self-attention module is a crucial architecture design for a diffusion model with a small number of elements as ours only used 16 part-latents. Inspired by Ho~\etal~\cite{Ho:2022CascadedLDM}, another key detail in our diffusion model design is to split a training process into two phases that each tailored to leverage the idiosyncrasy of the extrinsic and intrinsic representation. We first train an unconditional latent diffusion model with $\{\Ve_j\}$ which predicts noises in the $\Ve_j$ latent space. It corresponds to modeling a distribution of rough geometry of training shapes. In the second phase, we train a \emph{conditional} latent diffusion model which learns a conditional distribution $p(\Vs_j | \Ve_j)$. The latter model predicts noises in the $\Vs_j$ latent space with given condition $\Ve_j$. Splitting allows the first phase to learn only rough geometry of training shapes which reduce the complexity of the model significantly compared to previous methods. This ease of learning also allows the first phase model to create a much more diverse set of output. The second can then take advantage of this information to produce higher quality output. Altogether, we were able to achieve high fidelity in shape generation with higher variation while keeping the computational footprint low. We experimentally demonstrate the advantage of this split training approach by comparing against other model architectures. 

% To generate 3D models at test time, we first randomly sample $\{\hat{\Ve}_j\}$ from the first phase model. Then, we sample $\{\hat{\Vs}_j\}$ by feeding ${\{\hat{\Ve}_j\}}$ as condition to the second phase model. From these sampled latents, we obtain the final shape through the pre-trained Decoder $\mathcal{D}$. Since $\{\Ve_j\}$ represents a 3D shape as a primitive set and its variation affects the final output shape, our method also allows user to directly perform part editing method with minimal effort similarly to previous primitive-based shape representation work~\cite{Hao:2020Dualsdf, Hertz:2022Spaghetti}. Compared to previous methods, we also demonstrate that SALAD is the most suitable 3D generative model as it allow part-level shape manipulation, completion, and conditional shape generation. 

% In summary, the following are our contributions:
% \begin{itemize}
%     \item We first introduce a part-level latent diffusion model, SALAD, which generates diverse and high-resolution meshes with significantly less memory and computation. Quantitatively, we show that SALAD outperforms other baselines in random shape generation.
%     \item We present a Transformer-based network that enables a diffusion model training when a small number of elements is given.
%     \item We propose a cascaded training strategy for 3D generation and experimentally demonstrate its effectiveness.
%     \item To our best knowledge, SALAD is the first 3D diffusion model to provide primitive-based shape manipulation. Unlike previous work~\cite{Li:2022Diffusion-Sdf, Cheng:2022SDFusion, Hu:2023NeuralWavelet, Hui:2022NeuralWavelet}, it enables users to intuitively select and edit local regions. 
% \end{itemize}
