\section{Experiment}
\label{sec:experiment}
In this section, we demonstrate that~\salad{} outperforms other baselines in shape \emph{generation} (Section~\ref{sec:shape_generation}) and enables intuitive \emph{manipulation}, such as part completion (Section~\ref{sec:shape_completion}) and part mixing and refinement (Section~\ref{sec:part_mixing}), where the combination of part-level representation and diffusion models is essential.
Lastly, we also demonstrate that~\salad{} outperforms other baselines in text-guided shape generation (Section~\ref{sec:text_conditional_generation}) and can leverage part-level representation for text-guided part completion (Section~\ref{sec:text_driven_manipulation}).

%=================%
\subsection{Shape Generation}
\label{sec:shape_generation}
\paragraph{Evaluation Setup.}
For evaluation and comparison, we follow the settings of Hui~\etal~\cite{Hui:2022NeuralWavelet}. We use \emph{airplane} and \emph{chair} classes from the ShapeNet~\cite{Chang:2015Shapenet} dataset and the train-test split from Chen~\etal~\cite{Chen:2019ImNet}. The model is trained for each class. At inference time, we sample \num{2000} shapes for each class, and measure three evaluation metrics~\cite{Achlioptas:2018LatentGAN,Lopez-paz:20171nna} to assess quality and diversity of the generated shapes:
Coverage (COV), Minimum Matching Distance (MMD), and 1-Nearest Neighbor Accuracy (1-NNA).
We compare \salad{} with existing 3D generative models~\cite{Chen:2019ImNet,Kleineberg:2020VoxelGAN,Luo:2021DPM,Hertz:2022Spaghetti,Hui:2022NeuralWavelet}.%, mainly recent diffusion-based works.

% Particularly, we conduct comparisons with the baselines used in Hui~\etal~\cite{Hui:2022NeuralWavelet}, the current SotA diffusion-based approach for generative modeling.


% The results of the baselines in Table~\ref{tbl:quantitative_comparison_of_shape_generation} shows both the results directly brought from Table 1 in Neural Wavelet~\cite{Hui:2022NeuralWavelet} paper and those we reproduced by ourselves with the same settings, marked by $*$.
% \vspace{-15pt}
\paragraph{Results.} 
The quantitative and qualitative results, including ablation studies, are summarized in Table~\ref{tbl:quantitative_comparison_of_shape_generation} and Figure~\ref{fig:shape_generation_qualitative_results}.
For more results, refer to the \textbf{supplementary material}.
We reproduced the results of SPAGHETTI~\cite{Hertz:2022Spaghetti} and Neural Wavelet~\cite{Hui:2022NeuralWavelet} using the official code,
and the other quantitative results are directly borrowed from Hui~\etal~\cite{Hui:2022NeuralWavelet}, marked with ``$*$'' in Table~\ref{tbl:quantitative_comparison_of_shape_generation}.
(We also display the results of SPAGHETTI~\cite{Hertz:2022Spaghetti} and Neural Wavelet~\cite{Hui:2022NeuralWavelet} reported by Hui~\etal~\cite{Hui:2022NeuralWavelet}~\cite{Hui:2022NeuralWavelet} in the gray-colored rows. Note that SPAGHETTI results are similar, while there is a gap in the Neural Wavelet results.)
To ease qualitative comparisons in Figure~\ref{fig:shape_generation_qualitative_results},
we retrieve the generated shapes using the same query ground truth shape and compare them.
%and EMD 

As shown in Table~\ref{tbl:quantitative_comparison_of_shape_generation},~\salad{} achieves SotA results or is on par with the baselines. In particular, we outperform Neural Wavelet~\cite{Hui:2022NeuralWavelet}, which is a SotA diffusion-based 3D generative model, on 1-NNA by a large margin: \num{65.04} vs. \num{57.82} for \emph{chair} CD, and \num{75.77} vs. \num{73.92} for \emph{airplane} CD (lower is better).

Qualitatively,~\salad{} produces clean high-resolution meshes with fine details as shown in Figure~\ref{fig:shape_generation_qualitative_results}. When comparing ``Diffusion of $\B{z}$'' (in Section~\ref{sec:method}) with SPAGHETTI~\cite{Hertz:2022Spaghetti}, we demonstrate that our simple latent diffusion already produces much better quality shapes than sampling $\B{z}$ from the unit Gaussian distribution as SPAGHETTI does. 
% ``Diffusion of $\{\B{p}_i\}_{i=1}^N$'' that uses Transformer~\cite{Vaswani:2017Attention} instead of simple MLPs outperforms ``Diffusion of $\B{z}$'', manifesting our Transformer-based architecture is a key to learning the distribution of high-dimensional latents represented by a set.
``Diffusion of $\{\B{p}_i\}_{i=1}^N$'' uses Transformer~\cite{Vaswani:2017Attention} instead of simple MLPs and outperforms ``Diffusion of $\B{z}$'', clearly showing how our Transformer-based architecture is the key to learning the distribution of high-dimensional latents represented as a set.

When comparing our final model~\salad{} with ``Diffusion of $\{\B{p}_i\}_{i=1}^N$'',~\salad{} outperforms ``Diffusion of $\{\B{p}_i\}_{i=1}^N$'' by a large margin across all metrics. It shows that our cascaded diffusion training is crucial to improve shape generation quality.


% We report evaluation metrics from the baselines and ours in Table~\ref{tbl:quantitative_comparison_of_shape_generation}. 
% For baselines, we directly use the numbers from Hui~\etal~\cite{Hui:2022NeuralWavelet} as we have reproduced its evaluation result of SPAGHETTI~\cite{Hertz:2022Spaghetti} by following the same evaluation setup and procedure. Meanwhile, in the case of Neural Wavelet~\cite{Hui:2022NeuralWavelet}, we observed noticeable gaps between values from the paper and our reproduction. Hence, we report both of them, marking the results from our experiment with ``$*$".
% \textbf{As shown, our cascaded diffusion model outperforms baselines on all metrics. Note that our method on \emph{airplane} category is on-par with the current SotA even though it is trained on latents whose decoded meshes often exhibit artifacts induced by shape inversion (see Section~\ref{sec:experiment}).}

% We reproduced the SPAGHETTI~\cite{Hertz:2022Spaghetti} results, but there was some discrepancy between our reproduced Neural Wavelet results and those reported in the paper even though we carefully followed their instruction of training with publicly released code. As seen in Table~\ref{tbl:quantitative_comparison_of_shape_generation}, our diffusion model outperforms other baselines on the metrics evaluating the shape quality and the diversity. Even though our latent training set would have artifacts induced by shape inversion optimization in \emph{airplane} class, ours outperform others in 1-NNA metrics.

\input{tables/shape_generation_comparison.tex}
\input{figures/qualitative_comparison.tex}


%=================%
\subsection{Part Completion}
\label{sec:shape_completion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure*}
% \centering
% {
% \scriptsize
% \setlength{\tabcolsep}{0em}
% \renewcommand\tabularxcolumn[1]{m{#1}}
% \newcolumntype{Y}{>{\centering\arraybackslash}X}
% %\newcolumntype{Y}{>{\centering\arraybackslash}m{0.094\textwidth}}
% %\toprule
% \begin{tabularx}{0.85\textwidth}{YYYYY|YYY|YY}
% % \begin{tabularx}{0.85\textwidth}{XXXXX|XXX|XX}
% \centering
% DPM~\cite{Luo:2021DPM} & PVD~\cite{Zhou:2021PVD} & LION~\cite{Zeng:2022LION} & Voxel-GAN~\cite{Kleineberg:2020VoxelGAN} & \makecell{Neural \\Wavelet\cite{Hui:2022NeuralWavelet}} & \makecell{SPAGHETTI\\\cite{Hertz:2022Spaghetti}} & \makecell{Diff. of\\$\B{z}$} & \makecell{Diff. of\\$\{\B{p}\}_{i=1}^N$} & Gaussians & \makecell{\salad{}\\(Ours)} \\
% \multicolumn{10}{c}{\includegraphics[width=0.85\textwidth]{figures/figure1/chair_row_0_w_gaus_resized.png}} \\ 
% \multicolumn{10}{c}{\includegraphics[width=0.85\textwidth]{figures/figure1/chair_row_1_w_gaus_resized.png}} \\ 
% % \multicolumn{10}{c}{\includegraphics[width=\textwidth]{figures/figure1/chair_row_2_w_gaus_resized.png}} \\ 
% % \multicolumn{10}{c}{\includegraphics[width=\textwidth]{figures/figure1/chair_row_3_w_gaus_resized.png}} \\ 
% \multicolumn{10}{c}{\includegraphics[width=0.85\textwidth]{figures/figure1/airplane_row_0_w_gaus_resized.png}} \\ 
% % \multicolumn{10}{c}{\includegraphics[width=\textwidth]{figures/figure1/airplane_row_1_w_gaus_resized.png}} \\ 
% % \multicolumn{10}{c}{\includegraphics[width=\textwidth]{figures/figure1/airplane_row_2_w_gaus_resized.png}} \\ 
% \multicolumn{10}{c}{\includegraphics[width=0.85\textwidth]{figures/figure1/airplane_row_3_w_gaus_resized.png}}
% \bottomrule
% \end{tabularx}
% }
% \caption{\textbf{Qualitative comparison of the shape generation.} Given a query ground truth shape, we retrieve the closest generated shape by measuring EMD in each method. \salad{} produces highly detailed 3D shapes compared to the baselines.}
% \label{fig:shape_generation_qualitative_results}
% \vspace{-0.5\baselineskip}
% \end{figure*}


\begin{figure*}
\centering
{
\scriptsize
\setlength{\tabcolsep}{0em}
\renewcommand\tabularxcolumn[1]{m{#1}}
% \newcolumntype{Y}{>{\centering\arraybackslash}X}
% \newcolumntype{W}{>{\centering\arraybackslash}m{0.094444\textwidth}}
% \newcolumntype{Z}{>{\centering\arraybackslash}m{0.188888\textwidth}}
\newcolumntype{W}{>{\centering\arraybackslash}m{0.111111\textwidth}}
\newcolumntype{Z}{>{\centering\arraybackslash}m{0.222222\textwidth}}
\begin{tabularx}{\textwidth}{WWW|Z|Z|Z}
GT & Bounding Box & Gaussians & ShapeFormer~\cite{Yan:2022ShapeFormer} &Neural Wavelet~\cite{Hui:2022NeuralWavelet}& {\salad} (Ours) \\ 
\midrule 
% \multicolumn{6}{c}{\includegraphics[width=\textwidth]{figures/fig2_0817.png}} 
\multicolumn{6}{c}{\includegraphics[width=\textwidth]{figures/fig2_0817_compressed.png}} 
\end{tabularx}
}
\caption{\textbf{Qualitative comparison of the part completion.} We examine \salad{} and other baselines in part completion after ablating semantic parts or regions, highlighted in red in columns 2 and 3. \salad{} produces realistic completions for missing parts. The baselines fail to preserve observed parts or introduce noticeable seams at bounding box boundaries.
%%%% editing
% Local changes made by \salad{} are confined to the selected parts as opposed to the baselines where precise picking of parts are nontrivial.
%%%%
}
\label{fig:shape_completion}
% \vspace{-0.5\baselineskip}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%
% Seungwoo TODO: refine the above draft
% \Seungwoo{This section can be re-named to "Shape Completion"}
% \Charlie{Part Generation might be more appropriate because that's the metric that we are using in Table 2.}
% One key advantage of our generative model is that it can easily manipulate 3D shapes in a part level. Most of prior 3D generative models~\cite{Chen:2019ImNet,Achlioptas:2018latentgan} cannot process local editing since they represent 3D shapes as a global latent features. However, our latent diffusion model is train with the part-level representation to enable local editing in test time. In recent, Hu~\etal~\cite{Hu:2023NeuralWavelet} shows local editing using wavelet-domain diffusion model. However, it is \emph{non-trivial} to find the correspondence between a local region in the spatial domain and the wavelet domain. Unlike Hu~\etal~\cite{Hu:2023NeuralWavelet}, our model inherently provides local parts in the 3D space, which makes selecting local regions trivial.
% \Juil{Precise experiment setup.}
%%%%

% Here we describe how~\salad{} trained in an \emph{unconditional} setup can be employed to part completion and compare the results with the ones of the other diffusion model, Neural Wavelet~\cite{Hui:2022NeuralWavelet} and the ones of the SotA shape completion method, ShapeFormer~\cite{Yan:2022ShapeFormer}.
Here, we describe how~\salad{}, which was trained in an \emph{unconditional} setup, can be employed to part completion. We compare the results against the most recent diffusion model, Neural Wavelet~\cite{Hui:2022NeuralWavelet} and the SotA of shape completion, ShapeFormer~\cite{Yan:2022ShapeFormer}.

\sisetup{group-separator={,}}

% \vspace{-10pt}
\paragraph{Experiment Setup.}
For completion using diffusion models, we run \emph{guided} reverse process proposed by Meng~\etal~\cite{Meng:2022SDEdit}.
%Specifically, from timestep $T$ to $0$, we iterate the following one reverse step given $\B{x}\in \mathbb{R}^d$:
Specifically, given the input data $\B{x}\in \mathbb{R}^d$ and a mask of the region to be reconstructed $m \in [0, 1]^d$, each step of the reverse process of the diffusion is performed as follows:
\begin{equation}
 \begin{aligned}
 \B{x}^{(t-1)}_{\text{unmasked}}&\sim\mathcal{N}(\sqrt{\bar\alpha^{(t)}}\B{x}^{(0)},(1-\bar\alpha^{(t)})\B{I}) \\
 \B{x}^{(t-1)}_{\text{masked}}&\sim\mathcal{N}(\boldsymbol{\mu}_\theta(\B{x}^{(t)},t),\beta^{(t)}\B{I}) \\
 \B{x}^{(t-1)}&=m\odot\B{x}^{(t-1)}_{\text{unmasked}}+(1-m)\odot\B{x}^{(t-1)}_{\text{masked}}.
 \end{aligned}
\end{equation}
%where a binary mask $m \in [0, 1]^d$ is 1 is where the model generate, and 0 otherwise. 
%\Minhyuk{explain the process in 1-2 sentences or equations.}
% This approach has an advantage over the other shape completion methods such as ShapeFormer~\cite{Yan:2022ShapeFormer} in that it can guarantee to preserve the unmasked region while completing only a specific part.
Unlike previous methods such as ShapeFormer~\cite{Yan:2022ShapeFormer}, this approach guarantees to preserve the unmasked region.
% In our experiments, we remove a randomly chosen semantic part of the \emph{chair} and \emph{airplane} models and regenerate the part. Note that it is straightforward in our shape representation to specify the part to be regenerated; we can simply take the ($\B{e}_i$, $\B{s}_i$) pairs corresponding to the part. For a voxel-based or spectral representation used in Neural Wavelet~\cite{Hui:2022NeuralWavelet}, however, it is not trivial to specify the spatial or spectral region for a part.
In our experiments, we randomly remove and regenerate a semantic part of \emph{chairs} and \emph{airplanes}. While we can simply select ($\B{e}_i$, $\B{s}_i$) pairs of parts we want to remove in \salad{}, in feature-voxel representation like Neural Wavelet~\cite{Hui:2022NeuralWavelet}, it is not trivial to specify the regions that would include the completed part. This limits their generation output to only occupy the masked voxels, while a larger mask could interfere with or even break unwanted parts leading to seams in the final output. 
% For instance, if the mask is defined as the voxels included in the part, only these voxels will be completed, limiting the scope of variations of the reproduced part. On the other hand, a larger mask may affect and even break the other parts while producing seams in completion. Our disentangled implicit representation can allow easy and precise selection of a part while not limiting the scope of part variations.
For the guided reverse process of Neural Wavelet~\cite{Hui:2022NeuralWavelet} in our experiments, we use the axis-aligned bounding box of a part as a mask and transform the mask to the wavelet domain. Refer to the \textbf{supplementary material} for more details on mask construction.

% We experiment with 100 random shapes in our training set and produce five different versions of the completion for each shape and a randomly selected semantic part. 
We first randomly choose 100 shapes from our training set. Then, for all methods, we randomly select a semantic part from each shape and generate five variations.
% For quantitative comparisons, following Yan~\etal~\cite{Yan:2022ShapeFormer}, we report both the reconstruction loss showing how close the completed shape is to the original shape and also \emph{reverse}-MMD and FPD (Frechet PointNet Distance)~\cite{Shu:2019Treegan} depicting how much variation is generated. Specifically, \emph{reverse}-MMD measures both reconstruction quality and diversity by quantifying how completed shapes are close to ground truth shapes. \Juil{need to refine this part.}
%%%% 0308 Seungwoo more explicit description for reverse MMD
% For quantitative comparisons, we report both the reconstruction loss and \emph{reverse}-MMD and FPD (Frechet PointNet Distance)~\cite{Shu:2019Treegan} indicating the diversity of generations. Note that, \emph{reverse}-MMD measures both reconstruction quality and diversity by quantifying how completed shapes are close to ground truth shapes.
For quantitative comparisons, we report the reconstruction loss, MMD and FPD (Fréchet PointNet Distance)~\cite{Shu:2019Treegan} indicating the quality and diversity of completions. Note that we measure MMD \emph{from} completions \emph{to} groundtruth shapes to quantify the proximity of the completed shapes to the groundtruth shapes.
%%%%
We use the official pre-trained models for ShapeFormer~\cite{Yan:2022ShapeFormer} and Neural Wavelet~\cite{Hui:2022NeuralWavelet}. We also report the results from Neural Wavelet trained by ourselves.
%Following Yan~\etal~\cite{Yan:2022ShapeFormer}, we measure MMD, Chamfer Distance, and FPD to assess how much completed shapes are realistic and follow a data distribution.


% \paragraph{Evaluation Setup.}

% We make partial observations by masking out a certain portion of the input data at a semantic part level. 
% Specifically, to obtain a binary mask of our latent vectors, we assign semantic part labels $l_j \in \{1,...,L\}$ to each $\B{e}_i$ using Mahalanobis distance~\cite{Mahalanobis:1936MahalanobisDistance}, a metric between Gaussians and points, and then mask out a pair of ($\B{e}_i$, $\B{s}_i$) which is labeled as $l_j$. Given masked latents, we run \emph{guided} reverse process following Meng~\etal~\cite{Meng:2022SDEdit}. 
% For the baselines, we make a spatial binary mask by finding the bounding box of $l_j$. A more detailed process of defining the binary masks is explained in the supplementary. For our experiments, we randomly select 100 shapes from our training set and mask out a certain portion of shapes labeled as a semantic part. We sample $2048$ points on the surface of ground truth shapes and completed shapes. Following Yan~\etal~\cite{Yan:2022ShapeFormer}, we measure MMD, Chamfer Distance, and FPD to assess how much completed shapes are realistic and follow a data distribution. We compare~\salad{} with two baselines, non diffusion SotA shape completion work~\cite{Yan:2022ShapeFormer} and recent 3D diffusion work~\cite{Hui:2022NeuralWavelet}. We use the official pre-trained models for the baselines. 
% For experiments, we randomly select 100 shapes from our training set which have corresponding semantic part labeled point clouds.  Specifically, we assign semantic part labels $l_j \in \{1,...,L\}$ to each , and mask out both $\B{e}_i$ and $\B{s}_i$ which is labeled as $l_j$. For the baselines, we mask out portion of the input covered by the bounding box of a semantic part. To complete partial shape information using a pre-trained diffusion model, we run 

% \vspace{-\baselineskip}
\paragraph{Results.}
The quantitative results and qualitative results are summarized in Table~\ref{tbl:part_regeneration} and Figure~\ref{fig:shape_completion}, respectively. 
For more results, refer to the \textbf{supplementary material}.
As shown in Table~\ref{tbl:part_regeneration},~\salad{}, trained solely for \emph{unconditional} shape generation, outperforms the baselines in most of the metrics by large margins, especially in FPD which is the metric of how plausible the shapes are. 

% One of the key advantages of our part-level representation is that local regions can be flexibly defined, not only explicitly but implicitly. Part completion shows this advantage most clearly where it is necessary to seamlessly fill in the masked region. 
% \Juil{As shown in Figure~\ref{fig:shape_completion}, \salad{} produces more clean and realistic shapes by freely adjusting masked latent vectors without any spatial restrictions from the binary mask. In contrast, the baselines define a local region explicitly in the 3D space neglecting underlying 3D shape and its parts, thus unavoidable to be restricted by the \emph{fixed} spatial binary mask.}
% See the first row in Figure~\ref{fig:shape_completion} where undesirable big bounding box is given to cover the curving seat, and the baselines fail to complete the region.

%\Minhyuk{Examples. ShapeFormer part preservation. Overfitting to the conditions. Box,}
% In row 1, ShapeFormer, overfitted to the masks seen during training, fails to preserve unmasked region and produces artifacts at the back of the chair
The qualitative results presented in Figure~\ref{fig:shape_completion} further manifests the advantages of employing a part-level 3D representation in \salad{}. In row 1 of Figure~\ref{fig:shape_completion}, ShapeFormer~\cite{Yan:2022ShapeFormer} introduces noticeable artifacts at the back of the chair that lies outside the binary mask (column 2).
% This stems from the lack of spatial correspondence between the bounding box defined in 3D space and the feature grid representing the shape. In contrary, \salad{} produces high-quality completion
%%%% Edit, Seungwoo
% In row 1 of Figure~\ref{fig:shape_completion}, ShapeFormer fails to preserve unmasked region and introduces artifacts at the back part of the chair.
In contrast, \salad{} completes the seat seamlessly while preserving the other parts, benefiting from the spatial correspondence between the binary mask and the shape representation. 
% In addition, the qualitative results demonstrate \salad{} can generalize to various types of masks unlike ShapeFormer~\cite{Yan:2022ShapeFormer} whose generalization capability is limited by the masks seen during training. 
Even with such spatial correspondences, the limitation of specifying regions instead of parts persists in Neural Wavelet~\cite{Hui:2022NeuralWavelet}. In particular, the row 2 of Figure~\ref{fig:shape_completion} shows visible seams at the bounding box boundary while \salad{} generates the missing part consistent with the surrounding parts. 
%%%%

% the baselines where a local region is explicitly defined in spatial domain neglecting underlying 3D shape and its \emph{parts}, our masked latents can be freely adjusted in the 3D space. Figure~\ref{fig:shape_completion} reaffirms the results reported in Table~\ref{tbl:part_regeneration} as \salad{} produces clean and realistic shapes from partial observations.
% Also, from the perspective of users, it is nontrivial to define a local region in our representation by directly \emph{picking} $\{\B{e}_i\}$. 

% where defining a local region in their representation is nontrivial, the part-level representation simplifies the approach of selecting a local region by directly \emph{picking} one or multiple parts. To validate our carefully coupled part-level representation and diffusion model enables not only intuitive part-level shape completion approach but also outperforming performance, we compare~\salad{} with two baselines, SotA shape completion work~\cite{Yan:2022ShapeFormer} not based on a diffusion model, and recent 3D diffusion work~\cite{Hui:2022NeuralWavelet}.


% Particularly, the disentanglement of 3D shapes into a set of part-level representations allows for \emph{picking} one or multiple parts directly, easing the extension of \salad{} to downstream applications, such as shape completion. 


%\paragraph{Evaluation Metrics.}

% Furthermore, we transfer part labels of their ground truth meshes in ShapeNet~\cite{Chang:2015Shapenet} dataset to shape representations of \salad{}, and of baselines. These part labels are used to construct binary masks, both for extrinsic and intrinsic vectors, and voxels in ShapeFormer~\cite{Yan:2022ShapeFormer} and Neural Wavelet~\cite{Hui:2022NeuralWavelet}.

% Using the aforementioned binary masks, we retarget \salad{} and Neural Wavelet~\cite{Hui:2022NeuralWavelet} to the task of shape completion following Meng~\etal~\cite{Meng:2022SDEdit}. 
% specifically, we tailor the algorithm presented by meng~\etal~\cite{meng:2022sdedit} for shape completion that can be considered an analogy of inpainting in 2d image domain. 
% Specifically, we modify the image-inpainting technique presented by meng~\etal~\cite{meng:2022sdedit} for shape completion.
% During the experiment, we use the timestep $T=1000$ to fully destroy representations of missing parts. Then, we use the precomputed binary masks to guide the denoising processes, while preserving the partial observations.
% For the experiment, we first fully destroy the representation of the missing parts by setting timestep $T=1000$. Then, we run the denoising process while using the predefined masks to preserve the partial observations.
% During the experiment, we first fully diffuse either a set of extrinsic and intrinsic vectors, or wavelet coefficients, indicated by the pre-computed binary masks, to the timestep $T=1000$. 
% The proposed sampling strategy for applying \emph{pretrained} diffusion models to conditional generation tasks, such as image inpainting, can be considered an analogy of shape completion in 2D image domain. Therefore, we extend the algorithm to cope with our part-level shape representation and wavelet coefficients of Neural Wavelet~\cite{Hui:2022NeuralWavelet}. During the experiment, we first fully diffuse either a set of extrinsic and intrinsic vectors, or wavelet coefficients, indicated by the pre-computed binary masks, to the timestep $T=1000$. We then guide the denoising processes with the given masks, leveraging diffusion models to complete the ablated part or 3D region. For ShapeFormer~\cite{Yan:2022ShapeFormer}, we only use the same method for generating binary masks without any modification.

% After completion, we extract point clouds following the method in Section~\ref{sec:shape_generation} for evaluation. We compare four metrics against the baselines~\cite{Yan:2022ShapeFormer,Hui:2022NeuralWavelet} to assess reconstruction quality, including minimum matching distance (MMD), Fréchet Point Cloud Distance (FPD), Chamfer distance (CD), and earth mover's distance (EMD). Refer to the supplementary material for details of the experiment setting and evaluation metrics. 

% \Seungwoo{Might be in the supplementary?}
% The former two are computed between point clouds from completion outputs and the reference set in Section~\ref{sec:shape_generation}, and thus reflect how \emph{plausible} the completed shapes are. Note that MMD is computed in the opposite direction by computing a distance \emph{from} each completion output \emph{to} its closest neighbor in the reference set. On the other hand, we employ the latter two to demonstrate how \emph{faithful} the output shapes are to the incomplete shapes. To this end, we compute the minimum of both CD and EMD between an original point cloud (\ie without part ablation), and its five different completions. We compute the average distance by ablating every part within a shape which is averaged for all shapes used for the experiment.

% \paragraph{}
% \sisetup{group-separator={,}}
% The evaluation metrics from our method and the baselines are reported in Table~\ref{tbl:part_regeneration}. As shown, our method outperforms the baselines in every metric. Specifically, we observe notable gaps between our \salad{}, trained solely for \emph{unconditional} shape generation, and ShapeFormer~\cite{Yan:2022ShapeFormer}, the current SotA dedicated for shape completion. We analyze the implication of this result in the following.

% The key advantage of our \salad{} compared to the baselines originates from its shape representation. Particularly, the disentanglement of 3D shapes into a set of part-level representations allows for \emph{picking} one or multiple parts directly, which can be easily realized in our setup as specifying a subset of $\{(\mathbf{e}_i, \mathbf{s}_i)\}$.
% \Seungwoo{repeat the same argument in the beginning of this section.}
% In contrast, specifying a \emph{part} within a voxel grid representation employed by the baselines~\cite{Yan:2022ShapeFormer,Hui:2022NeuralWavelet} is \emph{nontrivial}, as it only encodes information of a \emph{region} enclosed by its boundary, while being aware of neither underlying 3D shape, nor its \emph{parts}. 
% Defining bounding boxes enclosing local regions can be a solution, however, we remark that it can result in suboptimal results due to bounding boxes that extend beyond regions of interest, intruding their peripherals. \Charlie{Maybe don't mention how the bounding box make suboptimal}
% For baseline comparison, we select the bounding box enclosing the semantic region to be ablated.

% The qualitative results illustrated in Figure~\ref{fig:shape_completion} demonstrate the effectiveness of our representation. We display variations of a shape produced by completing a missing part or region, highlighted in red. The visualizations of completions reaffirm the results reported in Table~\ref{tbl:part_regeneration} as our \salad{} generates high-quality completions for the given inputs compared to the baselines.

% Having binary masks indicating parts to complete, we follow the method from Meng~\etal~\cite{Meng:2022SDEdit} to retarget our model for shape completion. As Neural Wavelet~\cite{Hui:2022NeuralWavelet} is also based on diffusion, the same algorithm is used but with binary masks specifying 3D regions (\ie voxels).

% \paragraph{Experiment details.}
% \sisetup{group-separator={,}}

% In contrast, picking a part from voxel-based representations used in our baselines~\cite{Yan:2022ShapeFormer,Hui:2022NeuralWavelet} is \emph{nontrivial} since such representations only permit specifying 3D regions, not parts. Therefore, we resort to coarse bounding boxes enclosing parts to designate the regions in 3D space. Similarly to ours, we formulate binary masks indicating a subset of voxels from the bounding boxes.




% In particular, we build a (binary) mask from the part labels to indicate the Gaussian blobs and their corresponding intrinsic codes that should and should \emph{not} be modified. The mask is then used to condition the denoising (generative) process of the selected parts on the rest and at the same time, to preserve the parts that are not selected. Algorithm~\ref{} illustrates the overall procedure in a rigorous manner.

% Our model highly benefits from several properties of disentangled 3D representation by design. Unlike most of prior work on 3D generative models~\cite{Achlioptas:2018latentgan,Chen:2019ImNet} that represent 3D shapes as global latent codes, the proposed model is built upon a representation that explicitly distinguishes surface geometry from shape structure. This allows for extending the pipeline to various downstream applications such as making part-level variations, which, on the other hand, is \emph{non-trivial} for the aforementioned work.

% Recently, however, Hu~\etal~\cite{Hu:2023NeuralWavelet} demonstrates part-level editing of 3D shapes by extending its previous work~\cite{Hui:2022NeuralWavelet} that proposes a diffusion model operating on voxel grids of wavelet coefficients~\cite{Mallat1989:Wavelet,Daubechies1990:Wavelet,Velho1994:Wavelet} derived from TSDFs representing 3D shapes. While such a representation permits local editing in the means of selecting and modifying only a subset of voxels, crafting a mask that encloses the region of interest requires considerable amount of manual efforts. In contrast, the shape representation of our choice decomposes 3D shapes into Gaussian mixture models (GMMs)~\cite{Genova:2020LDIF} thereby faithfully represents the position, orientation, and extent of each part in a more intuitive, physically meaningful way. We conduct both qualitative and quantitative analysis on our method and the baselines in the following paragraphs.
%%%%

% \Seungwoo{The term "extrinsic representation" might be changed to refer to both Gaussians and wavelet coefficients}
% We randomly select 100 shapes from our training set for the experiment. 

% Moreover, we focus on manipulating semantically meaningful parts, such as back of chairs, by leveraging the part labels included in ShapeNet~\cite{Chang:2015Shapenet} dataset. This requires assigning part labels to the \emph{handles} that provide controls over local regions of 3D shapes. In the followings, we start by describing the procedure for assigning part labels to our extrinsic representations of each part. A detailed description on how our unconditional model can be directly re-targeted for shape manipulation~\Seungwoo{"Shape Completion"?} follows shortly.

%%%%

% Since part labels are often embedded on the surface of 3D shapes, a proper definition of mapping that associates a shape representation to the labels is necessary. Here, we discuss how such a mapping is defined in a concrete manner. Given a point cloud $\{ (\mathbf{x}, l)_i \}_{i=1}^N$ of $N$ points, each consists of its 3D coordinate $\mathbf{x} \in \mathbb{R}^{3}$ and part label $l \in \{1,2,\dots,L\}$, sampled over the surface of a shape whose extrinsic representation is $\{ \mathbf{e}_{j} \}_{j=1}^{M}$, we aim to find a mapping $f: \{ \mathbf{e}_{j} \} \mapsto \{1,2,\dots,L\}$.

% Given a point cloud of $N$ points and $L$ labels sampled from a shape $\{p^l_i\}_{i\in1..N, l\in1..L}$ and its Gaussian mixture model (GMM) representation $\{\mathbf{e}_j\}$, our goal is to find a mapping to the part labels from the point cloud to each Gaussian blob of GMM $f: \mathbf{e}_j\mapsto l$. 
% Note that we use the point clouds from ShapeNet~\cite{Chang:2015Shapenet} accompanied by part labels. Since each Gaussian blob in GMM represents the extent of local region under its influence as a probability density, we opt to compute Mahalanobis distance~\cite{Mahalanobis:1936MahalanobisDistance} as a distance measure \emph{from} points \emph{to} the Gaussian blobs. We assign $\mathbf{e}_j$ with label $\hat{l}$ if \Charlie{write down the exact equation for this}

%%%%
% We say that a subset of $k$ Gaussians $\{\mathbf{e}_j\}_{j \leq k}$ has label $l*$ if their 
% \begin{equation}
% \mathbf{e}_j = \argmin_{\mathbf{e}_j' \in \{\mathbf{e}_j\}} D_{\text{Mahalanobis}}(\mathbf{e}_j', x*)
% \end{equation}

% After assigning part labels, we follow the method proposed by Meng~\etal~\cite{Meng:2022SDEdit} to leverage our pretrained cascaded generator for shape manipulation without any additional cost. In particular, we build a (binary) mask from the part labels to indicate the Gaussian blobs and their corresponding intrinsic codes that should and should \emph{not} be modified. The mask is then used to condition the denoising (generative) process of the selected parts on the rest and at the same time, to preserve the parts that are not selected. Algorithm~\ref{} illustrates the overall procedure in a rigorous manner.


% Next, we describe how 

% After assigning semantic labels to Gaussians \Seungwoo{(or coefficient voxels)}, we use the algorithm proposed in Meng~\etal~\cite{Meng:2022SDEdit} to edit shapes using (binary) masks that are derived from the part labels.



\input{tables/part_regeneration.tex}

\subsection{Part Mixing and Refinement}
\label{sec:part_mixing}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}
% \centering
% % \includegraphics[width=0.8\linewidth]{figures/part_mixing_refinement_two_rows.pdf}
% \includegraphics[width=\linewidth]{figures/fig_part_mixing.pdf}
% \caption{\textbf{Qualitative results of part mixing and refinement.} \salad{} improves quality of part mixing outputs.}
% \label{fig:part_mixing}
% % \vspace{-\baselineskip}
% \end{figure}

\begin{figure}
\scriptsize
\setlength{\tabcolsep}{0em}
\renewcommand\tabularxcolumn[1]{m{#1}}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\begin{tabularx}{\linewidth}{Y Y Y Y}
    % \toprule
    Shape A & Shape B & A$\rightarrow$B & \makecell{A$\rightarrow$B\\Refined} \\ 
    \midrule
    \multicolumn{4}{c}{\includegraphics[width=\linewidth]{figures/fig_part_mixing_wo_title.png}}
    \end{tabularx}
\caption{\textbf{Qualitative results of part mixing and refinement.} \salad{} improves quality of part mixing outputs.}
\label{fig:part_mixing}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \input{tables/part_mixing}

%%%% 0308 Seungwoo. minor refinement
% While Hertz~\etal~\cite{Hertz:2022Spaghetti} demonstrates creating new shapes by combining parts from existing shapes, naively mixing part representations is prone to produce failure cases, such as cracks at joint regions or inconsistency across parts. See Figure~\ref{fig:part_mixing} where \salad{} seamlessly attaches discontinued parts, back and seat in row 1, and tail and body in row3, or refine undesirable part mixing results, bending seat in row 2, through the guided reverse process. Refer to the \textbf{supplementary material} for more qualitative results and \textbf{quantitative comparisons}.

% Another application enabled by part-level 3D implicit representations is part mixing~\cite{Hertz:2022Spaghetti} that allows creation of new shapes by combining parts from existing shapes. However, manually mixing parts is prone to produce failure cases. 

% While Hertz~\etal~\cite{Hertz:2022Spaghetti} demonstrates creating new shapes by combining parts from existing shapes, naively mixing part representations is prone to produce failure cases as illustrated in Figure~\ref{fig:part_mixing}. Cracks or discontinuities at joint regions are one type of failure case as shown in the odd-numbered rows. Another type of failures, displayed in the even-numbered rows, is the dissonance between combined parts that results in undesired distortions or the vanishing of parts. 
\input{tables/part_mixing}

While Hertz~\etal~\cite{Hertz:2022Spaghetti} demonstrates creating new shapes by combining parts from existing shapes, naively mixing part representations is prone to produce failure cases as illustrated in Figure~\ref{fig:part_mixing} and Figure~\ref{fig:teaser}. Cracks or discontinuities at joint regions are one type of failure case as shown in row 3 of Figure~\ref{fig:part_mixing} and (b) of Figure~\ref{fig:teaser}. Another type of failures is the dissonance between combined parts that results in undesired distortions or the vanishing of parts. 
\salad{} can remedy this issue by refining both the extrinsic and intrinsic vectors through the guided reverse process. Refer to the \textbf{supplementary material} for more qualitative results.

% One of such cases is cracks, or discontinuities at joint regions such as cracks at joint regions or inconsistency across parts. See Figure~\ref{fig:part_mixing} where \salad{} seamlessly attaches discontinued parts, back and seat in row 1, and tail and body in row3, or refine undesirable part mixing results, bending seat in row 2, through the guided reverse process. Refer to the \textbf{supplementary material} for more qualitative results and \textbf{quantitative comparisons}.
%%%%

% Although the shape representation proposed by Hertz~\etal~\cite{Hertz:2022Spaghetti} enables creation of new shapes by combining part latents from existing shapes, we observe occasional failure cases with noticeable discontinuity at joint regions as shown in () of Figure~\ref{}. In such cases, our cascaded framework can improve the quality of outputs by iteratively \emph{refining} parts through the learned denoising processes. 

% \Seungwoo{Finish this paragraph}
% Let two shapes $A$ and $B$ be represented by two sets of extrinsic and intrinsic vectors $\{(\mathbf{e}_i^A, \mathbf{s}_i^A)\}_{i=1}^{N}$ and $\{(\mathbf{e}_i^{B}, \mathbf{s}_i^{B})\}_{i=1}^{N}$. Furthermore, assume that each extrinsic vector $\mathbf{e}_{(*)}$ is assigned a part label $l_i \in \{1,,2,\dots,L\}$ where $L$ is the number of all part labels. Given a part label $l$, we can either transfer 

%to compare \salad{} and SPAGHETTI~\cite{Hertz:2022Spaghetti}. 

% \Seungwoo{2 shapes per pair. 100 pairs randomly selected from our training set. Swap a semantic part (e.g., back) one at a time, for all parts two shapes have in common. Measure metrics after (1) naive mixing, (2) running SDEdit with t=10.}



% Given two shapes $A$ and $B$, each of which represented by a set of extrinsic and intrinsic vectors, we \emph{mix} the two shapes by replacing a subset of the vectors with its counterpart in the other shape. For instance, our shape representation allows building a new chair by taking the back of shape $A$ and the rest of the parts from shape $B$. 

% However, as shown in Figure~\ref{fig:part_mixing}, this leads to undesired outputs whose coherency across parts are broken (see \text{row 1 and 2} where the output shapes have undesirably bent seat.) or containing noticeable cracks at joints (see (b) in Figure~\ref{fig:teaser}, where the joint of back and seat is broken, or row 3 in Figure~\ref{fig:part_mixing} where the joint of wing and body is broken). These shapes are refined by using the same method described in Section~\ref{sec:shape_completion}.
%but with a smaller denoising timesteps.

We also show quantitative results of part mixing in Table~\ref{tbl:quantitative_mixing}. 
For evaluation, we use the same metrics and the test set used in Section~\ref{sec:shape_generation}. We randomly select 100 pairs of shapes from the test set and swap a semantic part, for all parts that two shapes in a pair have in common. Swapping a part between two shapes results in two mixed shapes for each pair. The numbers of the shapes resulting from part mixing are 606, 670, and 400 for \emph{chair}, \emph{airplane}, and \emph{table} classes, respectively. The mixed shapes are refined by the guided reverse process with diffusion timestep $t=10$. As indicated in the metrics reported in Table~\ref{tbl:quantitative_mixing}, the quality of mixed shapes are further improved after the refinement step. We particularly observe noticeable gaps in 1-NNA across all shape classes. 
% More \emph{qualitative} results are reported in Section~\ref{sec:more_part_mixing}.

%===== From the Supplementary ======%
%
% To demonstrate the refinement capability of \salad{}, we conduct quantitative comparisons between the part mixing outputs from SPAGHETTI~\cite{Hertz:2022Spaghetti} and the refined outputs from \salad{}. 
% % For evaluation, we randomly select 100 pairs of shapes from the training set and swap a semantic part, for all parts two shapes in a pair have in common.  
% For evaluation, we randomly select 100 pairs of shapes from the training set and swap a semantic part, for all parts that two shapes in a pair have in common. Swapping a part between two shapes results in two mixed shapes for each pair. The numbers of the shapes resulting from part mixing are 606, 670, and 400 for \emph{chair}, \emph{airplane}, and \emph{table} classes, respectively. The mixed shapes are refined using the method introduced in Section~\ref{sec:shape_completion}~\refofpaper{} with diffusion timestep $t=10$. We evaluate the same metrics used in Section~\ref{sec:shape_generation}~\refinpaper{} using the test set provided by Chen~\etal~\cite{Chen:2019ImNet} and report the results in Table~\ref{tbl:quantitative_mixing}.
% % We also report the evaluation results from \emph{table} class, following the aforementioned procedure. We generate 400 shapes for evaluation by mixing parts across 100 randomly sampled pairs.
% % The same metrics are evaluated using the \emph{table} test set from Chen~\etal~\cite{Chen:2019ImNet}.
% % \textbf{Duplicate text?}

% \paragraph{Results.}
% % As shown in Table~\ref{tbl:quantitative_mixing}, \salad{} outperforms SPAGHETTI on MMD and 1-NNA, especially with a large gap in 1-NNA: 74.26 vs. 68.23 for \emph{chair} EMD, and 78.88 vs. 76.27 for \emph{airplane} EMD. \Seungwoo{table results need another explanation}
% As indicated in the metrics reported in Table~\ref{tbl:quantitative_mixing}, the quality of mixed shapes are further improved after the refinement step. We particularly observe noticeable gaps in 1-NNA across all shape classes. More \emph{qualitative} results are reported in Section~\ref{sec:more_part_mixing}.
% \input{tables/part_mixing}

%=================%
\subsection{Text-Guided Shape Generation}
\label{sec:text_conditional_generation}
We further demonstrate \salad{} can perform \emph{conditional} generation, specifically generating 3D shapes given an input text. To condition a text to the model, we concatenate a language feature and an input of AdaLN, $\gamma(t)$, and optionally $\mathcal{E}(\B{e}_i)$.
% We experiment with the text and 3D shape pair dataset from ShapeGlot~\cite{Achlioptas:2019Shapeglot} and compare the generation quality of our text-conditioned model with the one of AutoSDF~\cite{Mittal:2022Autosdf}, which is the SotA text-to-shape generative model publicly available. The train-test split used in AutoSDF is exploited.
We experiment with the text and shape pair dataset from ShapeGlot~\cite{Achlioptas:2019Shapeglot} and compare the generation quality of our text-conditioned model with the one by AutoSDF~\cite{Mittal:2022Autosdf}, which is the SotA text-to-shape generative model. 
% The train-test split used in AutoSDF is exploited.
The train-test split used in AutoSDF is used.
Also, following AutoSDF, we measure the following three metrics for the evaluation: CLIP-Similarity-Score (CLIP-S)~\cite{Radford:2021CLIP}, Neural-Evaluator-Preference (NEP), and Fréchet Point Cloud Distance (FPD)~\cite{Shu:2019Treegan}.
%We use the text and 3D shape pair dataset from ShapeGlot~\cite{Achlioptas:2019Shapeglot} for training and an evaluation with the same train-test split used in AutoSDF~\cite{Mittal:2022Autosdf}, which is the SotA text-to-shape generative model publicly available. For the evaluation, we measure the following three metrics: CLIP-Similarity-Score (CLIP-S)~\cite{Radford:2021CLIP}, Neural-Evaluator-Preference (NEP), and Fréchet Point Cloud Distance (FPD)~\cite{Shu:2019Treegan}.

NEP proposed by Mittal~\etal~\cite{Mittal:2022Autosdf} is a preference rate obtained from a neural evaluator. The neural evaluator is pre-trained on a text-conditioned binary classification task where the model distinguishes the target shape corresponding to the input text. 
Since the neural evaluator used in AutoSDF has not publicly been released, we train our neural evaluator based on PartGlot~\cite{Koo:2022Partglot}, a simpler architecture trained only on point clouds without images. More details of the experiment setup is in the \textbf{supplementary material}. 

As shown in Table~\ref{tbl:lang_quantitative_result}, our generated shapes are more preferred by the neural evaluator over the shapes generated by AutoSDF. Also, Figure~\ref{fig:text_generation} and FPD results reflect that ~\salad{} produces more plausible shapes, and our generated shapes conform to given texts more than the shapes of AutoSDF. 

% At test time, for each text, we feed shapes generated by ours and AutoSDF into the evaluator and count which shapes are preferred. Following Mittal~\etal~\cite{Mittal:2022Autosdf}, we count an example where the absolute difference between the evaluator's probabilities is $\leq 0.2$ as confused.
% As shown in Table~\ref{tbl:lang_quantitative_result} and Figure~\ref{fig:lang_qualitative_result}, SALAD produces more plausible shapes, and the generated shapes are more conformed to given texts than AutoSDF.

\input{tables/language_qualitative_results}
\input{tables/language_quantitative_results}

% \vspace{-10pt}
\subsection{Text-Guided Part Completion}
\label{sec:text_driven_manipulation}
We further demonstrate how~\salad{} can be integrated with a text-driven semantic part segmentation network to aid user interactive shape editing. Following PartGlot~\cite{Koo:2022Partglot} architecture, we design \gaussglot{}, a model that uses $\{\B{e}_i\}_{i=1}^N$ as a part representation and predicts semantic part labels of those from texts. More details of \gaussglot{} architecture and training results can be found in the \textbf{supplementary material}. 
Figure~\ref{fig:text_completion} shows examples that the parts of input shapes selected by \gaussglot{} are completed according to given texts by a reverse process of text-conditioned~\salad{} introduced in Section~\ref{sec:text_conditional_generation}. It demonstrates that users can freely manipulate 3D shapes with texts in an end-to-end manner by leveraging~\salad{} with~\gaussglot{}.
\input{tables/text_guided_completion}


%=================%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \setlength{\tabcolsep}{0em}
% \def\arraystretch{0.0}
% % \newcolumntype{Y}{>{\centering\arraybackslash}m{0.0909\textwidth}}
% \begin{figure}
% {\scriptsize
% \begin{tabularx}{\linewidth}{YYYY}
% SPAGHETTI & $\B{z}$ & \makecell{$\{\B{e}_i\}_{i=1}^N,\\ \{\B{s}_i\}_{i=1}^N$} & Ours \\  
% \midrule
% \multicolumn{4}{c}{\includegraphics[width=\linewidth]{figures/fig_ablation_chair.png}} \\
% \multicolumn{4}{c}{\includegraphics[width=\linewidth]{figures/fig_ablation_airplane.png}} 
% \end{tabularx}
% }
% \caption{Ablation studies.}
% \label{fig:ablation_study}
% \vspace{-0.5\baselineskip}
% \end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% 

