%%%%%%%%%%%%%%%%
\ifpaper
  \newcommand{\refofpaper}[1]{\unskip}
  \newcommand{\refinpaper}[1]{\unskip}
  \newcommand{\suppSegDir}{supp_segmentations}
\else
  \makeatletter
  \newcommand{\manuallabel}[2]{\def\@currentlabel{#2}\label{#1}}
  \makeatother
  \manuallabel{sec:introduction}{1}
  \manuallabel{sec:related_work}{2}
  \manuallabel{sec:background_diffusion}{3.1}
  \manuallabel{sec:background_part_representation}{3.2}
  \manuallabel{sec:method}{4}
  \manuallabel{sec:shape_generation}{5.1}
  \manuallabel{sec:shape_completion}{5.2}
  \manuallabel{sec:results_part_mixing}{5.3}
  \manuallabel{sec:text_conditional_generation}{5.4}
  \manuallabel{sec:text_driven_manipulation}{5.5}
  \manuallabel{fig:shape_generation_qualitative_results}{4}
  \manuallabel{fig:shape_completion}{5}
  \manuallabel{fig:part_mixing}{6}
  \manuallabel{fig:text_generation}{7}
  
  \manuallabel{tbl:quantitative_comparison_of_shape_generation}{1}
  \manuallabel{tbl:part_regeneration}{2}
  
  \newcommand{\refofpaper}[1]{of the main paper}
  \newcommand{\refinpaper}[1]{in the main paper}
\fi
%%%%%%%%%%%%%%%%%%%

\ifpaper 

\else
\tableofcontents
\clearpage
\newpage
%
\begin{figure}[h!]
\centering
\includegraphics[width=1\textwidth]{figures/shape_generation/fig1_gallery_v4.png}
\caption{\textbf{A visual gallery of \emph{airplanes}, \emph{chairs}, and \emph{tables} generated by~\salad{}.}}
\label{fig:generation_gallery}
\end{figure}
\clearpage
\newpage
%
\subsection{Overview}
% In this supplementary material, we first present \emph{quantitative} results of part mixing and refinement (Section~\ref{sec:supp_part_mixing}). 
% In this supplementary material, we first present more \emph{quantitative} analyses, including quantitative results of part mixing and refinement (Section~\ref{sec:supp_part_mixing}) and more quantitative results of shape generation (Section~\ref{sec:supp_shape_generation}). 
In this supplementary material, we first illustrate additional details in the implementation of \salad{} (Section~\ref{sec:supp_salad_implementation_details}) and details of the experiments discussed in the main paper (Section~\ref{sec:supp_experimental_details}). Then, we report additional experimental results: multi-class generation (Section~\ref{sec:supp_multi_class_generation}), shape generation with more classes (Section~\ref{sec:supp_shape_generation_with_more_classes}) and shape generation with different number of parts (Section~\ref{sec:supp_varying_parts}).
Lastly, we report more \emph{qualitative} results of the experiments reported in the main paper: shape generation (Section~\ref{sec:more_shape_generation_comparison}), part completion (Section~\ref{sec:more_part_completion_comparison}), part mixing and refinement (Section~\ref{sec:more_part_mixing}), text-guided shape generation (Section~\ref{sec:more_text_generation}), and text-guided part completion (Section~\ref{sec:more_text_completion}).

% Then, we illustrate additional details in the implementation of \salad{} (Section~\ref{sec:supp_salad_implementation_details}) and details of the experiments discussed in the main paper (Section~\ref{sec:supp_experimental_details}). Lastly, we report more \emph{qualitative} results of shape generation (Section~\ref{sec:more_shape_generation_comparison}), part completion (Section~\ref{sec:more_part_completion_comparison}), part mixing and refinement (Section~\ref{sec:more_part_mixing}), text-guided shape generation (Section~\ref{sec:more_text_generation}), and text-guided part completion (Section~\ref{sec:more_text_completion}).
\fi

% %%%% 
% % 1. Part mixing results
% \subsection{Quantitative Results of Part Mixing and Refinement}
% \label{sec:supp_part_mixing}
% \paragraph{Experiment Setup.}

% % \textbf{TODO: Add timestep used, explicitly say bi-directional mixing, refer to the qualitative results in the following sections}
% To demonstrate the refinement capability of \salad{}, we conduct quantitative comparisons between the part mixing outputs from SPAGHETTI~\cite{Hertz:2022Spaghetti} and the refined outputs from \salad{}. 
% % For evaluation, we randomly select 100 pairs of shapes from the training set and swap a semantic part, for all parts two shapes in a pair have in common.  
% For evaluation, we randomly select 100 pairs of shapes from the training set and swap a semantic part, for all parts that two shapes in a pair have in common. Swapping a part between two shapes results in two mixed shapes for each pair. The numbers of the shapes resulting from part mixing are 606, 670, and 400 for \emph{chair}, \emph{airplane}, and \emph{table} classes, respectively. The mixed shapes are refined using the method introduced in Section~\ref{sec:shape_completion}~\refofpaper{} with diffusion timestep $t=10$. We evaluate the same metrics used in Section~\ref{sec:shape_generation}~\refinpaper{} using the test set provided by Chen~\etal~\cite{Chen:2019ImNet} and report the results in Table~\ref{tbl:quantitative_mixing}.
% % We also report the evaluation results from \emph{table} class, following the aforementioned procedure. We generate 400 shapes for evaluation by mixing parts across 100 randomly sampled pairs.
% % The same metrics are evaluated using the \emph{table} test set from Chen~\etal~\cite{Chen:2019ImNet}.
% % \textbf{Duplicate text?}

% \paragraph{Results.}
% % As shown in Table~\ref{tbl:quantitative_mixing}, \salad{} outperforms SPAGHETTI on MMD and 1-NNA, especially with a large gap in 1-NNA: 74.26 vs. 68.23 for \emph{chair} EMD, and 78.88 vs. 76.27 for \emph{airplane} EMD. \Seungwoo{table results need another explanation}
% As indicated in the metrics reported in Table~\ref{tbl:quantitative_mixing}, the quality of mixed shapes are further improved after the refinement step. We particularly observe noticeable gaps in 1-NNA across all shape classes. More \emph{qualitative} results are reported in Section~\ref{sec:more_part_mixing}.
% \input{tables/part_mixing}


% To conduct a quantitative comparison of part mixing between SPAGHETTI~\cite{Hertz:2022Spaghetti} and \salad{}, we randomly select 100 pairs of shapes from the training set and swap a semantic part, for all parts two shapes in a pair have in common. The total number of generated shapes by part mixing is 606, 670, 400 for \emph{chair}, \emph{airplane}, and \emph{table} classes, respectively. We measure the same metrics used in Section~\ref{sec:shape_generation}~\refinpaper{} with the same test set.
% \paragraph{Results.}
% We report the quantitative results from the experiments in Table~\ref{tbl:quantitative_mixing}. As shown in Table~\ref{tbl:quantitative_mixing}, \salad{} outperforms SPAGHETTI on MMD and 1-NNA, especially with a large gap in 1-NNA: 74.26 vs. 68.23 for \emph{chair} EMD, and 78.88 vs. 76.27 for \emph{airplane} EMD. \Seungwoo{table results need another explanation}

% \subsection{More Quantitative Results of Shape Generation}
% % \subsection{More Shape Generation Quantitative Results}
% \label{sec:supp_shape_generation}
% For a more precise comparison of shape generation between Neural Wavelet~\cite{Hui:2022NeuralWavelet} and \salad{}, we present the means and standard deviations of the results based on \emph{five} times random samplings in Table~\ref{tbl:more_quantitative_comparison_of_shape_generation}. Consistent with Table~\ref{tbl:quantitative_comparison_of_shape_generation}~\refinpaper{}, Table~\ref{tbl:more_quantitative_comparison_of_shape_generation} shows that \salad{} outperforms Neural Wavelet in most metrics with small standard deviations. 

% % Furthermore, We include additional ablation study, `` Diffusion of $\{(\B{e}_i, \B{s}_i)\}_{i=1}^N$'', a variant of `` Diffusion of $\{\B{p}_i\}_{i=1}^N$'' which replaces an input $\B{p}_i$ to a concatenation of $\B{e}_i$ and $\B{s}_i$ in Table~\ref{tbl:more_quantitative_comparison_of_shape_generation}. 

% Table~\ref{tbl:more_quantitative_comparison_of_shape_generation} shows \salad{} outperforms other baselines, especially in 1-NNA by a large margin, demonstrating the effectiveness of its cascaded design.

% % We report extra quantitative results of shape generation in Table~\ref{tbl:more_quantitative_comparison_of_shape_generation}. We present additional evaluation results of Neural Wavelet~\cite{Hui:2022NeuralWavelet} for a precise comparison. We populate five different sets of shapes from Neural Wavelet~\cite{Hui:2022NeuralWavelet} by generating shapes multiple times and assess their quality by following the same procedure described in Section~\ref{sec:shape_generation}~\refofpaper{}. \textbf{Add text for five different \emph{training} runs for chair!} The mean and standard deviation of the evaluation results are summarized in Table~\ref{tbl:more_quantitative_comparison_of_shape_generation}. \textbf{I think it would be good to indicate "which" rows}. 

% % Table~\ref{tbl:more_quantitative_comparison_of_shape_generation} also shows additional quantitative results from ablation studies, including `` Diffusion of $\{(\B{e}_i, \B{s}_i)\}_{i=1}^N$'', a variant of `` Diffusion of $\{\B{p}_i\}_{i=1}^N$'' which replaces an input $\B{p}_i$ to a concatenation of $\B{e}_i$ and $\B{s}_i$. As shown, \salad{} outperforms all baselines demonstrating the effectiveness of its cascaded architecture.
% % First, it includes the the means and the standard deviations obtained by Neural Wavelet we trained five times by ourselves, and `` Diffusion of $\{(\B{e}_i, \B{s}_i)\}_{i=1}^N$'', a variant of `` Diffusion of $\{\B{p}_i\}_{i=1}^N$'' which replaces an input $\B{p}_i$ to a concatenation of $\B{e}_i$ and $\B{s}_i$. More  \emph{qualitative} shape generation results are in Section~\ref{sec:more_shape_generation}.

% \input{tables/shape_generation_more_results}

\subsection{SALAD Implementation Details}
\label{sec:supp_salad_implementation_details}

As discussed in Section~\ref{sec:background_part_representation}~\refofpaper{}, an extrinsic vector $\B{e}_i$ is represented by $\{\mathbf{c}_i, \lambda_i^1, \lambda_i^2, \lambda_i^3, \mathbf{u}_i^1, \mathbf{u}_i^2, \mathbf{u}_i^3, \pi_i\}$, where the eigenvectors $\{\mathbf{u}_i^j\}_{j=1}^{3}$ must be orthogonal to each other. Therefore, the diffusion processes for $\{\B{e}_i\}_{i=1}^N$ need to model distributions in a product space of an orthogonal group $\text{O}(3)$ and Euclidean group, not in the Euclidean space. Recent work~\cite{Leach:2022SO3Diff,Bortoli:2022Riemannian} introduce diffusion models on Lie group or its product space, however, we empirically find that learning diffusion without considering the orthogonality also performs well. It is ensured only at the test time by taking the projection of the generated eigenvectors $\mathbf{U}_i = [\mathbf{u}_i^1, \mathbf{u}_i^2, \mathbf{u}_i^3]$ to $\text{O}(3)$ space. We follow Sch{\"o}nemann~\cite{Schonemann:1966Procrustes} and project $\mathbf{U}_i$ as
\begin{align}
\label{eq:eigvec_projection}
    \tilde{\mathbf{U}}_i = [\tilde{\mathbf{u}}_i^1, \tilde{\mathbf{u}}_i^2, \tilde{\mathbf{u}}_i^3] = \mathbf{A}\mathbf{B}^l,
\end{align}
where $\mathbf{U}_i = \mathbf{A} \boldsymbol{\Sigma} \mathbf{B}^T$ is a singular value decomposition of $\mathbf{U}_i$.
We also clip negative eigenvalues in $\{\lambda_i^j\}_{j=1}^{3}$ to \num{1e-4} since the covariance matrix is positive-definite.

We normalize elements of $\mathbf{e}_i$ to avoid arbitrary high-variance latent space. Specifically, during the training of ``Diffusion of $\{\B{e}_i\}_{i=1}^N$'', we normalize $\pi_i$ and $\{\lambda_i^j\}_{j=1}^3$ using element-wise means and standard deviations pre-computed from all training data. At test time, we re-scale these elements by the means and the standard deviations. We do not apply normalization to the others.

\sisetup{group-separator={,}}
The Transformer-based network of \salad{} introduced in Section~\ref{sec:method}~\refofpaper{} consists of an embedding layer, which maps an input to 512-dimensional embeddings, and 6 Transformer blocks. Each Transformer block is a stack of a self-attention block and an MLP, each of which is followed by an AdaLN layer. We set the dimension of the output of the positional encoding $\gamma(\cdot)$ to 128.

As \salad{} consists of two diffusion models, each trained for \num{5000} epochs, we train the baselines for \num{10000} epochs for a fair comparison. We use a batch size of 64 and an initial learning rate $10^{-4}$ with a polynomial decaying scheduler (power=0.999). The diffusion process is configured with $T=\num{1000}$, $\beta^{(1)}=10^{-4}$, and $\beta^{(T)}=0.05$.

% For a fair comparison, we train the other baselines \num{10000} epochs, while \salad{} is trained for \num{5000} epochs in each phase. We use a batch size of 64 and $T=\num{1000}$.

\subsection{Experiment Details}
\label{sec:supp_experimental_details}
In this section, we provide details of the experiments whose results are reported in the main paper.

\subsubsection{Details on Part Completion Experiment Setup --- Section~\ref{sec:shape_completion}}
\label{sec:supp_part_completion_details}
 As mentioned in Section~\ref{sec:shape_completion}~\refofpaper{}, part completion via a \emph{guided} reverse process~\cite{Meng:2022SDEdit} requires binary masks indicating the parts to be ablated. We describe how such masks are constructed for \salad{} and Neural Wavelet~\cite{Hui:2022NeuralWavelet} in this section.

\paragraph{SALAD.}
We define a binary mask $m \in \{0,1\}^N$ for pairs $\{(\mathbf{e}_i, \mathbf{s}_i)\}_{i=1}^N$ to have value \num{0} at completed parts, \num{1} otherwise. To this end, we first \emph{transfer} the part labels of the annotated point clouds from ShapeNet~\cite{Chang:2015Shapenet} dataset to each $(\mathbf{e}_i, \mathbf{s}_i)$. Assume a point cloud $\{ (\mathbf{x}_j, l_j) \}_{j=1}^{K}$ of $K$ points where $\mathbf{x}_j \in \mathbb{R}^3$ and $l_j \in \{1,2,\dots,L\}$, denote 3D coordinate and part label of $j$-th point, respectively. Each $(\mathbf{e}_i, \mathbf{s}_i)$ is assigned a part label $l_i \in \{1,2,\dots,L\}$ based on the proximity of $\mathbf{e}_i$ to the points $\{\mathbf{x}_j\}_{j=1}^K$. Since $\mathbf{e}_i$ parameterizes a Gaussian distribution in 3D space, we employ Mahalanobis distance~\cite{Mahalanobis:1936MahalanobisDistance} as a distance measure. For each Gaussian represented by $\mathbf{e}_i$, we compute the distance to every point $\mathbf{x}_j$ and select the closest \num{100} points. We then count the number of part label occurrences over the points and assign the most frequently occurred label to the pair.

Having assigned the part labels to each of $\{(\mathbf{e}_i, \mathbf{s}_i)\}_{i=1}^N$, we define a mask $m$ selecting a part whose label is $l$ as
\begin{align}
    m_i &= \begin{cases}
        0 & \text{ if } l_i = l \\
        1 & \text{ otherwise} \\
    \end{cases},
\end{align}
where $m_i$ denotes the $i$-th element of $m$.
% Then, we construct binary masks specifying elements of a set $\{\mathbf{e}_i\}_{i=1}^N$ based on the assigned part labels. We say that a subset of $k$ Gaussians $\{\mathbf{e}_j\}_{j \leq k}$ has label $l*$ if their 
% \begin{equation}
% \begin{aligned}
% \mathbf{e}_j = \argmin_{\mathbf{e}_j' \in \{\mathbf{e}_j\}} D_{\text{Mahalanobis}}(\mathbf{e}_j', x*),
% \end{aligned}
% \end{equation}
% \Juil{Please write down the missing part.}
% \Seungwoo{TODO: Make the above equation more concise.}

\paragraph{Neural Wavelet~\cite{Hui:2022NeuralWavelet}.}
% We aim to replicate the shape manipulation capability of Neural Wavelet~\cite{Hui:2022NeuralWavelet}, following the work of Hui~\etal~\cite{Hui:2022NeuralWavelet}.
Note that there is neither a publicly available official code nor detailed instructions for shape manipulation using Neural Wavelet~\cite{Hui:2022NeuralWavelet}. Although a concurrent work of ours, Hu~\etal~\cite{Hu:2023NeuralWavelet}, demonstrates shape manipulation using Neural Wavelet, it does not provide a detailed implementation.

% \textbf{Part picking (SALAD) vs Bounding box (Wavelet) "comparison" -> How we derived coefficient masks from the bounding boxes}
Following Hui~\etal~\cite{Hui:2022NeuralWavelet}, we derive the wavelet coefficients of the shapes in our training set. We compute signed distance functions (SDFs) of the shapes and truncate their values into $[-0.1, 0.1]$. We denote $S$ the resulting truncated signed distance function (TSDF) of a shape.
% We first obtain the signed distance field of a shape from our training set and truncate its values into $[-0.1, 0.1]$ following Hui~\etal~\cite{Hui:2022NeuralWavelet}. We denote $S$ the truncated SDF of a shape. 
We leverage Biorthogonal wavelet-6-8 filter~\cite{Cohen1993:Biorthogonal} to decompose $S$ into a coarse wavelet coefficient volume at a scale 3 ($C^3$) and a detail wavelet coefficient volume at a scale 2 ($D^2$). Refer to Hui~\etal~\cite{Hui:2022NeuralWavelet} for details on preprocessing.

We then aim to derive binary masks for $C^3$, necessary for leveraging pre-trained Neural Wavelet~\cite{Hui:2022NeuralWavelet} for part completion.
% Note that assigning binary indicators to the wavelet coefficients is \emph{not} a trivial task, as opposed to \salad{} where the binary masks can be defined to designate parts directly. 
Note that selecting a part to complete is a \emph{nontrivial} task for a voxel-based representation adapted by Neural Wavelet, as opposed to \salad{} where we can define binary masks for $\{(\mathbf{e}_i, \mathbf{s}_i)\}_{i=1}^N$ to select parts directly. As one solution, we compute bounding boxes enclosing semantic parts of 3D shapes, and use them to designate the \emph{regions} to complete.
% \textbf{REMOVE: It is due to the lack of correspondence between a volume in 3D space and the set of wavelet coefficients that reconstruct it via inverse wavelet transform.}
% We therefore devise a heuristic based on the fact that wavelet transforms extract local spectral information.
Such bounding boxes are used to compute binary masks for $C^3$ via a heuristic based on the property of wavelet transforms extracting local spectral information.
% To conduct the guided reverse process using Neural Wavelet, we derive the binary masks for $C^3$, which is \emph{not} a trivial task. Through some trials, we found that the wavelet coefficient that changes when we mask the TSDF within a predefined threshold correspond to a reasonable mask of the wavelet coefficient. Specifically, let our binary mask $m\in\Real^{256^3}$ be a $256^3$ voxel grid with $1$ indicating the semantic part of interest and $0$ otherwise. Given that $S$ has the resolution of $256^3$, we have $\forall v \in \{(0,0,0), (0,0,1), ..., (255,255,255)\}$
% \Seungwoo{More tidy. (1) Mask out TSDF -> (2) Wavelet coefficient changes -> (3) We select coefficients with a certain threshold, why doing this? No correspondence between coefficients and TSDF values!}
Through experiments, we empirically find a set of wavelet coefficients that vary when the TSDF values in a 3D volume are set to \num{0.1} (\ie outside of a shape). For instance, we set the TSDF values in the bounding box enclosing the back of a chair to \num{0.1} to discover a set of wavelet coefficients corresponding to the part. We assign \num{0} to the coefficients whose amount of change is above a threshold $\delta$ and \num{1} to the others.

Rigorously, let $M \in \{0,1\}^{256^{3}}$ denote a binary voxel grid of the same resolution as $S$ with 0 indicating the semantic part of interest and 1 otherwise. Such $M$ is derived from a bounding box enclosing a semantic part of a 3D shape, and is used to derive a \emph{masked} TSDF $S^*$ defined as
% that the wavelet coefficient that changes when we mask the TSDF within a predefined threshold correspond to a reasonable mask of the wavelet coefficient. Specifically, let our binary mask $m\in\Real^{256^3}$ be a $256^3$ voxel grid with $1$ indicating the semantic part of interest and $0$ otherwise. Given that $S$ has the resolution of $256^3$, we have $\forall v \in \{(0,0,0), (0,0,1), ..., (255,255,255)\}$
\begin{equation}
S_v^* = \begin{cases}
   0.1 & \text{ if } M_v = 0  \\ 
   S_v & \text{ otherwise}
\end{cases},
\label{eq:tsdf_mask}
\end{equation}
for all $v \in \{(0,0,0), (0,0,1), ..., (255,255,255)\}$. After marking all values inside a bounding box as \emph{outside}, we obtain the wavelet coefficients $C^{3*}$ via forward wavelet transform. A mask $m$ for $C^{3}$ is then defined as
\begin{equation}
% \tilde{m}^3_v = 
m_{v^{\prime}} =
\begin{cases}
     0 & \text{ if } |C^{3*}_{v^{\prime}} - C^3_{v^{\prime}}| > \delta\\ 
     1 & \text{ otherwise}
\end{cases}
\label{eq:wavelet_mask}.
\end{equation}
for all $v^\prime \in \{ (0,0,0), (0,0,1), ..., (47,47,47)\}$. Here, we use $\delta=0.001$.

\paragraph{ShapeFormer~\cite{Yan:2022ShapeFormer}.}
As discussed in Section~\ref{sec:shape_completion}~\refofpaper{}, after constructing the axis-aligned bounding box of a part, we make a partial point cloud by masking out the points inside the bounding box, and pass it to ShapeFormer~\cite{Yan:2022ShapeFormer} as an input.

\subsubsection{Details on Text-Guided Shape Generation --- Section~\ref{sec:text_conditional_generation}}
\label{sec:supp_text_generation_details}
\paragraph{Implementation Details of Text-Conditioned SALAD.}
We impose text conditions on both the first and the second phase models by feeding text features from our text encoder. We use LSTM~\cite{Hochreiter:1997lstm} for the text encoder and train it jointly with the first and the second phase models. We also apply the classifier-free guidance~\cite{Ho:2021classifierfree}. More precisely, we jointly train a conditional diffusion model $\Veps_\theta(\B{x}^{(t)},t,\B{c})$ and an unconditional diffusion model $\Veps_\theta(\B{x}^{(t)},t,\boldsymbol{\emptyset})$, where $\B{c}$ denotes a condition feature vector and $\boldsymbol{\emptyset}$ is a null condition vector. We randomly set $\B{c}$ to $\boldsymbol{\emptyset}$ with a 20\% dropout probability during training. To make $\boldsymbol{\emptyset}$, we feed an empty sequence as an input text and zero vectors for $\{\mathcal{E}(\B{e}_i)\}_{i=1}^N$. $\B{c}$ is solely a text feature for the first phase model. For the second phase model conditioned on the features from extrinsic vectors $\{\mathbf{e}_i\}_{i=1}^N$, we use the concatenation of the features and a text feature as a condition. 

% The noise prediction of the conditional diffusion model $\Veps_\theta(\B{x}^{(t)}, t \vert \B{c})$ at each step of the reverse process is used as:

At sampling time, the noise prediction is adjusted by an extrapolation between the noise prediction of the conditional diffusion model and the unconditional diffusion model as follows:

\begin{equation}
    \tilde{\Veps}_t = (1+w)\Veps_\theta(\B{x}^{(t)},t,\B{c})-w \Veps_\theta(\B{x}^{(t)}, t, \boldsymbol{\emptyset}),
\end{equation}
where $\tilde{\Veps}_t$ is the noise prediction with the classifier-free guidance applied, and $w$ is a hyperparameter controlling guidance strength. We use $w=2$ for sampling.

\paragraph{Experiment Setup.}
To measure Neural-Evaluator-Preference (NEP) discussed in Section~\ref{sec:text_conditional_generation}~\refofpaper{}, we leverage a modified PartGlot~\cite{Koo:2022Partglot} for a neural evaluator. The modified architecture takes point clouds as inputs instead of super-segments. Refer to the PartGlot~\cite{Koo:2022Partglot} paper for more details. 
% We uniformly sample \num{2048} points on the surface of meshes by Poisson disk sampling~\cite{Yuksel:2015poisson_sampling}.
We adapt the training and test set of PartGlot~\cite{Koo:2022Partglot} to create binary classification examples.
The modified PartGlot achieves $73.98$\% test accuracy on the binary classification. Following Mittal~\etal~\cite{Mittal:2022Autosdf}, we consider an example to be confused if the absolute difference between the neural evaluator's confidence is $\leq 0.2$.

{
\begin{figure}[h!]
\centering
% \begin{minipage}{\textwidth}
\includegraphics[width=0.6\linewidth]{figures/gaussglot_qualitative.pdf}
\caption{\textbf{\gaussglot{} qualitative results.} The attention maps for each semantic part achieved by \gaussglot{} are shown in the left columns of the figure. The colors of the attention maps change from dark blue to yellow as the attention weights increase from 0 to 1. The final part segmentation results are depicted in the rightmost column of the figure, where purple, blue, green, and yellow indicate \emph{back}, \emph{seat}, \emph{leg}, and \emph{arm}, respectively.}


\label{fig:gaussglot_qualitative}
% \end{minipage}
\end{figure}
}

\subsubsection{Details on GaussGlot --- Section~\ref{sec:text_driven_manipulation}}
\label{sec:supp_gaussglot_details}
Inspired by Koo~\etal~\cite{Koo:2022Partglot}, we design a text-driven self-supervised semantic part segmentation network, \gaussglot{}, where a set of Gaussian primitives is employed as super-segments. As discussed in Section~\ref{sec:text_conditional_generation}~\refofpaper{}, PartGlot is a neural evaluator that classifies shapes from a query text. While solving this text-conditioned shape classification, PartGlot learns semantic part segmentation in an unsupervised manner by learning the attention maps between the input text and the super-segments. Refer to the PartGlot~\cite{Koo:2022Partglot} paper for more details.
Specifically, we train \gaussglot{} with $\{\B{e}_i\}_{i=1}^N$ excluding $\pi_i$ elements which is inessential to define 3D Gaussian primitives. Based on the architecture of PartGlot, 15-dimensional Gaussian parameters are mapped to 256-dimensional features through MLPs. We embed text tokens into 128 dimensions and use LSTM as a text encoder with 256-dimensional hidden states. Our trained \gaussglot{} achieves \num{76.03}\% test accuracy and $56.85$\% mIoU. Qualitative part segmentation examples and the attention maps of each semantic part from \gaussglot{} can be found in Figure~\ref{fig:gaussglot_qualitative}. 


\subsection{Multi-Class Generation}
\label{sec:supp_multi_class_generation}
\begin{figure}
\centering
\includegraphics[width=0.98\textwidth]{figures/multi_class.pdf}
\caption{\textbf{Class-label-guided generation of \salad{} trained with \emph{airplanes} and \emph{cars}.}}
\label{fig:multi_class_generation}
\end{figure}

We further demonstrate that \salad{} is capable of multi-class generation. We construct the multi-class latent space by pre-training SPAGHETTI~\cite{Hertz:2022Spaghetti} with a training data set consisting of 200 \emph{airplanes} and 200 \emph{cars}. Next, we train class-label-conditioned \salad{} with the latents extracted from the pre-trained SPAGHETTI. Figure~\ref{fig:multi_class_generation} shows the same initial latents are decoded into different class shapes, airplanes and cars, through the class-label-guided reverse process.

\begin{figure}[!htb]
    \begin{minipage}{0.49\textwidth}
     \centering
     \includegraphics[width=.98\linewidth]{figures/other_class.pdf}
     \caption{\textbf{Generation of \emph{lamps} and \emph{cabinets}.}}\label{fig:more_class}
   \end{minipage}\hfill
   \begin{minipage}{0.49\textwidth}
     \centering
     \includegraphics[width=.98\linewidth]{figures/varying_parts_v3.pdf}
     \caption{\textbf{Generation with varying number of parts.}}\label{fig:varying_number_parts}
   \end{minipage}
\end{figure}

\subsection{Shape Generation with More Classes}
\label{sec:supp_shape_generation_with_more_classes}
% \begin{figure}
% \centering
% \includegraphics[width=0.8\textwidth]{figures/other_classes.pdf}
% \caption{\textbf{Generation results of \emph{lamps} and \emph{cabinets.}}}
% \label{fig:more_class_generation}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/other_classes_and_varying_parts.pdf}
%     \caption{\textbf{More qualitative results of \salad{}.} The experiments above use 400 training shapes, a significantly smaller number than the train set of the main paper. It demonstrates that \salad{} can generate high-quality shapes even with a small number of training data.} 
%     \label{fig:more_class_and_varying_number_parts}
% \end{figure}


In the main paper, we used \emph{chairs} and \emph{airplanes} for the quantitative comparison as done in the previous work~\cite{Hui:2022NeuralWavelet}. Figure~\ref{fig:multi_class_generation} and Figure~\ref{fig:more_class} show qualitative results of \salad{} trained with more other classes, \emph{cars}, \emph{lamps} and \emph{cabinets}. 

\subsection{Shape Generation with Different Number of Parts}
\label{sec:supp_varying_parts}
% \begin{figure}
% \centering
% \includegraphics[width=0.7\textwidth]{figures/varying_parts.pdf}
% \caption{\textbf{Generation results with different number of parts.}}
% \label{fig:varying_number_of_parts_generation}
% \end{figure}

Although we used 16 parts in the main paper, Figure~\ref{fig:more_class} shows qualitative results of with varying number of parts, 8 and 24 parts, respectively. It demonstrates that \salad{} is agnostic to the number of parts.
Furthermore, the experiments of Figure~\ref{fig:multi_class_generation}, Figure~\ref{fig:more_class} and Figure~\ref{fig:varying_number_parts} use 400 training shapes, a significantly smaller number than the train set of the main paper. It demonstrates that \salad{} can generate high-quality shapes with a small number of training data.

% \subsection{Architecture}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ifpaper
\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figures/shape_generation/fig1_gallery_v4.png}
\caption{\textbf{A visual gallery of \emph{airplanes}, \emph{chairs}, and \emph{tables} generated by~\salad{}.}}
\label{fig:generation_gallery}
\end{figure}
\clearpage
\newpage
\else
\fi 

\clearpage
\newpage
\onecolumn

%%%%%%%%%%%%%%%%%%%%
\subsection{More Qualitative Comparisons on Shape Generation}
% \subsection{Additional Qualitative Results of Shape Generation}
\label{sec:more_shape_generation_comparison}

In the following, we provide more qualitative comparisons on shape generation with \emph{chair} and \emph{airplane} classes, as shown in Figure~\ref{fig:shape_generation_qualitative_results}~\refofpaper{}. 
% Next, we report additional shape generation qualitative results with \emph{table} class.
\input{figures/shape_generation/supp_shape_generation_comparison}


%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%
\clearpage
\newpage

\subsection{More Qualitative Comparisons on Part Completion}
\label{sec:more_part_completion_comparison}
We report more qualitative comparisons on part completion with \emph{chair} and \emph{airplane} classes, as shown in Figure~\ref{fig:shape_completion}~\refinpaper{}. 

\input{figures/part_completion/supp_part_completion}

%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%
\clearpage
\newpage
\subsection{More Qualitative Results on Part Mixing and Refinement}
\label{sec:more_part_mixing}
We report more qualitative results on part mixing and refinement with \emph{chair}, \emph{airplane} and \emph{table} classes, as shown in Figure~\ref{fig:part_mixing}~\refinpaper{}.

\input{figures/part_mixing/supp_part_mixing}
%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%
\clearpage
\newpage

\subsection{More Qualitative Comparisons on Text-Guided Shape Generation}
\label{sec:more_text_generation}
We report more qualitative comparisons on text-guided shape generation between AutoSDF~\cite{Mittal:2022Autosdf} and \salad{}.

\input{figures/text_generation/supp_text_generation}
%%%%%%%%%%%%%%%%%%%%

\clearpage
\newpage
\onecolumn
\subsection{More Qualitative Results on Text-Guided Part Completion}
\label{sec:more_text_completion}
We report more qualitative results on text-guided part completion leveraging \salad{} and \gaussglot{}. In the figure below, the parts selected by \gaussglot{} from the text are highlighted by red. Text-conditioned \salad{} completes the selected parts to match the text via the guided reverse process.

\input{figures/text_completion/supp_text_completion}

\clearpage
\newpage
\twocolumn
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
