\section{Introduction}
\label{sec:introduction}

% What is the motivation of your research (not your personal motivation, but why is it needed and important in the academia/industry?)
% Why 3D generative model being capable of generating and manipulating shapes is important.
% don't mention generative models here. focus on creation that includes generation and editing instead of generation only.
\Juil{start with TL;DR.} Leaving the realm of creation to machines has long been studied. Creating assets requires considerable effort and time. In particular, as the demand for processing 3D data has been prevalent across various fields, such as computer vision, computer graphics, robotics, and the metaverse, it has become more important to reduce the cost required to generate a new 3D asset or to edit the existing asset. 
% In response to 
To address this, previous work~\cite{Achlioptas:2018latentgan, Hao:2020Dualsdf, Chen:2019ImNet, Yin:2020Coalesce} has proposed various methods, mainly focusing only on either shape generation or manipulation. 
%However, it is still challenging to achieve both high fidelity and editability in handling 3D assets. 
However, achieving high-fidelity generation while still allowing 3D assets to be editable remains a challenge. 
To democratize the creation of 3D assets, it is essential to enhance not only a high-fidelity generation but also an interpretable shape manipulation. 

%Diffusion: good generation results. also inherently capable of editing or synthesizing input data.
% shorten content of other generative models. directly say about diffusion models. emphasize diffusion model characteristics: zero-shot editing capability to unseen images. SDEdit, etc.
The recent advance of diffusion models~\cite{Ho:2019DDPM, Dhariwal:2021DiffusionBeatGANs} has led to significant growth in the 2D image generation, subsequently replacing the existing generative models~\cite{Goodfellow:2014GAN, Kingma:2014VAE, Rezende:2015Flow} with superior performance. The key advantage of diffusion models compared to other generative models is they have shown a zero-shot capability of editing and synthesizing input data. Without any supervision or context of task-specific conditions during training, its prior has demonstrated a capability of image inpainting~\cite{Lugmayr:2022Repaint, Chung:2022MCG} and image synthesis~\cite{Meng:2022SDEdit}. These attractive characteristics and the ability of diffusion models in the 2D image domain have prompted attempts to apply diffusion models to the 3D generation task.

% 3D data representation
% first paragraph: low-quality of early work
% second paragraph: difficulty of manipulating 3d data in previous work
Much previous work~\cite{Li:2022Diffusion-Sdf, Chou:2022Diffusionsdf, Shue:2022TriplaneDiffusion, Cheng:2022SDFusion, Hu:2023NeuralWavelet, Hui:2022NeuralWavelet, Cai:2020ShapeGF, Luo:2021DPM, Zeng:2022LION} has introduced diffusion models for 3D generation task. Early work of the literature employs diffusion models with 3D point clouds~\cite{Cai:2020ShapeGF, Luo:2021DPM, Zeng:2022LION} or a point-voxel hybrid representation~\cite{Zhou:2021PVD}, since na\"ively adopting 2D diffusion models to 3D voxels would require orders of magnitude more computational power. However, these early attempts fail to capture detailed shape surfaces due to the inherent limitation of shape\Charlie{pointcloud} representation, thus producing low-quality results. In addition, although they can convert output point clouds to iso-surface meshes, it requires another post-processing where a very precise computation is necessary. Otherwise, it is prone to produce noisy artifacts. 

To obtain high resolution meshes using 3D diffusion models, concurrent work~\cite{Li:2022Diffusion-Sdf, Chou:2022Diffusionsdf, Cheng:2022SDFusion, Hui:2022NeuralWavelet, Hu:2023NeuralWavelet} has introduced %3D implicit function-based diffusion models. 
diffusion models based on implicit function.
To mitigate the high computational cost of running diffusion on high-resolution 3D voxels, they run diffusion on intermediate feature space. Albeit showing local shape manipulation using the intermediate latent space, it is still non-trivial for users to select local regions from the latent space, thus limiting them from even the simplest manipulation capability.
% I will edit before this line.

% SALAD explanation: shape representation and strengths
% remove all notations. explain in English.
% synergy between diffusion models and our representation.

To resolve the aforementioned issues, we propose a 3D \textbf{S}hape P\textbf{A}rt-Level \textbf{LA}tent \textbf{D}iffusion Model, called \textbf{SALAD}. When designing a new 3D generative model, the \emph{desiderata} we consider is first, it should produce diverse and realistic meshes, and it also should provide users with a straightforward shape manipulation method. Inspired by Hertz~\etal~\cite{Hertz:2022Spaghetti} who has introduced a part-level shape representation, we observe the synergy between diffusion models and a part-level shape representation. Diffusion models present high-quality generations as well as a zero-shot capability of input synthesis, and the part-level shape representation decomposes a complex 3D shape into a set of simple parts. We demonstrate that the key ingredients of aiding 3D asset creation, high fidelity generation, intuitive shape manipulation, and completion, are only available when a diffusion model and a part-level representation are combined. 

SALAD is a latent diffusion model based on a set of part-level shape latents consisting of a 3D shape. The part latents can represent a high-resolution 3D mesh with significantly fewer entities than point clouds. Furthermore, each part latent affects only the local region where it governs in the 3D space. Its locality enables part-level shape manipulation and completion.
Since our shape representation is defined as a set, we use Transformer~\cite{Vaswani:2017Attention} architecture to address the permutation invariance of the part latents. Also, inspired by Ho~\etal~\cite{Ho:2022CascadedLDM}, we propose a two-phase training process, which is another crucial detail of the training. This training strategy splits the amount of information the model learns into two phases, thus making the training more stable and easier. Instead of running diffusion at once in a high-dimensional latent space, we separate the latent space into two sub-spaces. More precisely, we first train an unconditional latent diffusion model with a low dimensional latent which captures rough geometry information of a 3D space with a few parameters. Finally, to fill in the remaining detailed information for a final shape, we train another conditional latent diffusion model on the other latent space leveraging the low dimensional latent as a condition. Figure~\ref{fig:teaser} shows a coarse structure captured by the low-dimensional latents and its corresponding final decoded 3D shape. 

The effectiveness of our training strategy is demonstrated in Table~\ref{tbl:quantitative_comparison_of_shape_generation} which shows a big improvement. In addition, we demonstrate SALAD is suitable for various applications including part-level shape manipulation, shape completion, and conditional generation. In summary, the following are our contributions:
\begin{itemize}
    \item We introduce SALAD which generates diverse and high-resolution meshes with significantly less memory and computation. Quantitatively, we show that SALAD outperforms other baselines in random shape generation.
    \item Our SALAD is capable of part-level shape manipulation, which enables various applications, such as part-level shape manipulation, completion, and language-conditioned manipulation.
\end{itemize}


% Recently, as the demand of processing 3D contents has been growing in various fields, such as meta-verse, robotics, computer vision, and computer graphics, coupled with the difficulty of handling 3D data, a 3D generative model that not only generates high quality shapes but also enables users to easily manipulate the 3D shapes has been required. However, most of previous 3D generative model research has focused on generation quality itself only.

% At the same time, as diffusion models~\cite{Dhariwal:2021DiffusionBeatGANs} have outperformed other existing generative models~\cite{Goodfellow:2014GAN, Kingma:2014VAE, Rezende:2015Flow} in the 2D image and audio domain, attempts to apply a diffusion model on 3D shapes have been introduced in various previous works~\cite{Chou:2022Diffusionsdf, Hui:2022NeuralWavelet, Li:2022Diffusion-Sdf, Zhou:2021PVD, Luo:2021DPM, Jiang:2020Shapeflow}. However, unlike 2D images, there is no standard 3D representation, querying which 3D representation is the most efficient in 3D shape generation. In response to the question, we propose a 3D \textbf{S}hape P\textbf{A}rt-level \textbf{LA}tent \textbf{D}iffusion Model, called \textbf{SALAD}, which provides straightforward local part editing capability with diverse and high-quality 3D generated shapes.  

% Our model is based on part-level latents, which is the most efficient 3D representation compared to other representations, such as point clouds, voxels, and meshes. A 3D shape is encoded to a set of part latents $\{\Vp_j\}$, which represents the shape with a much smaller number than point clouds and voxels. Each $\Vp_j$ is further separated into an extrinsic latent $\Ve_j$ and an intrinsic latent $\Vs_j$. \Juil{need to flesh out descriptions of the role of each latent.} Instead of learning a distribution of these latents at once, inspired by cascaded LDM~\cite{Ho:2022CascadedLDM}, we split the LDM training into two phases. In the first phase, we first model a distribution of $\{\Ve_j\}$ which represents a rough geometry structure of input shape without any surface details. Next, the second phase model learns a conditional distribution of $\{\Vs_j\}$ given $\{\Ve_j\}$, which corresponds to filling in detailed 3D surface information given the rough geometry. 
% We use Transformer~\cite{Vaswani:2017Attention} architecture on both phase 1 and phase 2 latent diffusion models which is suit for processing set data.
% In test time, sampled $\{\Ve_j\}$ and $\{\Vs_j\}$ are decoded to an implicit representation, which can be easily converted to a mesh, by a pre-trained implicit decoder $\mathcal{D}$. 

% We demonstrate our two-phase design is crucial to produce high-quality 3D shapes and two-level shape manipulation, overall modification with coarse structure $\{\Ve_j\}$, and fine customizing with $\{\Vs_j\}$. \Juil{argument of two-level shape manipulation doesn't seem to make sense considering our results..} 

% \Seungwoo{It would be good to summarize our contribution in several bullets}

% What is the limitation of previous work?
% -> Unlike 2D images, there is no standard representation. 
% -> Most of 3d generative models have focused on generation quality itself only.

% What is the exact problem setup of your work?
% -> Encode a 3D shape to part-level latents which are further separated to extrinsic and intrinsic latents.
% -> Instead of learning a distribution of these latents at once, inspired by cascaded LDM, we design a cascaded latent diffusion model.
% -> In the first phase, we first model a distribution of extrinsic latents which represents a rough geometry structure without any surface details.
% -> Next, our second phase model learns a conditional distribution of intrinsic given input extrinsics, which means the phase 2 model produce detailed surface information given the rough geometry.
% -> We demonstrate our two-phase design outperforms a single phase model that models data distribution of extrinsic and intrinsic at the same time.

% What is the key challenge to solving the problem?
% -> simple single-phase model doesn't work. our two phases setup is crucial for high quality generation.
% -> Transformer architecture is essential in modeling set representation. 

% A list of contributions you aim for.
% -> We first propose a diffusion model for 3D shape generation and straightforward local part "manipulation".

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% VERSION 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%In 3D data, data representation is important. ours is the best.
% Unlike 2D images, 3D data has no standard data representation. 


% In recent, modeling and manipulating 3D shapes have become more prevalent in various fields, such as robotics, computer vision, computer graphics, and the Metaverse, where it is common to process 3D data. Coupled with the difficulty of handling 3D data, the need for a 3D generative model capable of generating high-quality shapes as well as manipulating the shapes is increasing. In response to this, much previous work~\cite{Li:2022Diffusion-Sdf, Chou:2022Diffusionsdf, Shue:2022TriplaneDiffusion, Cheng:2022SDFusion, Hu:2023NeuralWavelet, Hui:2022NeuralWavelet, Cai:2020ShapeGF, Luo:2021DPM, Zeng:2022LION} has introduced diffusion models for 3D generation task. Na\"ively adopting 2D diffusion models directly to 3D voxels would require orders of magnitude more computational power. Thus, early work employs diffusion models with 3D point clouds~\cite{Cai:2020ShapeGF,Luo:2021DPM,Zeng:2022LION} or a point-voxel hybrid representation~\cite{Zhou:2021PVD}. However, this straightforward approach failed to capture detailed shape surface information and produce low-quality results. To resolve this, concurrent work~\cite{Li:2022Diffusion-Sdf,Chou:2022Diffusionsdf, Cheng:2022SDFusion, Hui:2022NeuralWavelet, Hu:2023NeuralWavelet} has attempted applying diffusion models to a 3D implicit function representation by running diffusion on intermediate latent space or wavelet-domain. Albeit achieving higher quality generation, Diffusion-SDF~\cite{Li:2022Diffusion-Sdf}, Neural Wavelet~\cite{Hui:2022NeuralWavelet, Hu:2023NeuralWavelet}, and SDFusion~\cite{Cheng:2022SDFusion} are still relatively computationally heavy since they use down-sampled 3D grid features. Another problem with these voxel-based representations is that they are 
% inherently difficult to manipulate. In particular, it is not trivial to select local regions in the representation that DiffusionSDF~\cite{Chou:2022Diffusionsdf} and Neural Wavelet~\cite{Hui:2022NeuralWavelet, Hu:2023NeuralWavelet} provide, and thus limiting them from even the simplest manipulation capability.

% To resolve all the aforementioned issues, we propose a 3D \textbf{S}hape P\textbf{A}rt-Level \textbf{LA}tent \textbf{D}iffusion Model, called \textbf{SALAD}, which is capable of manipulating and generating high-quality and diverse 3D shapes at a part level while costing significantly less memory and computation. Our model utilizes part-level latents that ensure local influence in a 3D shape and are disentangled to extrinsic properties and intrinsic properties. 

% Let a 3D shape be encoded to a set of part latents $\{\Vp_j\}_{j=1}^M$, where $M$ denotes the number of parts representing the shape. Each $\Vp_j$ is further separated into an extrinsic latent $\Ve_j$ and an intrinsic latent $\Vs_j$. Intuitively, $\Ve_j$ encapsulates basic geometric information of a local part and consists of a few parameters. Figure~\ref{fig:teaser} shows a coarse structure that $\{\Ve_j\}$ captures and the decoded 3D shape. On the other hand, the higher dimensional vector $\Vs_j$ captures detailed surface information as well as joint regions between the parts. This set of latents are then decoded to a 3D shape by an implicit decoder $\mathcal{D}$:
% \begin{equation}
% \begin{aligned}
%     o=\mathcal{D}(\mathbf{x} | \{\Ve_j\},\, \{\Vs_j\}),
% \end{aligned}
% \end{equation}
% where $\mathbf{x}\in\Real^3$ and $o\in [0,1]$ denote a query 3D point and its corresponding occupancy value, respectively. 
% Unlike 3D voxel-based diffusion models~\cite{Li:2022Diffusion-Sdf, Chou:2022Diffusionsdf, Cheng:2022SDFusion, Hui:2022NeuralWavelet, Hu:2023NeuralWavelet}, our shape representation is a set representation which is invariant to the permutation of its elements. Inspired by Lee~\etal~\cite{Lee:2019SetTF}, we guarantee this property by adopting Transformer~\cite{Vaswani:2017Attention}, which is permutation invariant, to our latent diffusion models. Unlike previous point cloud-based diffusion models~\cite{Cai:2020ShapeGF, Luo:2021DPM} ~\Juil{Not sure if LION also doesn't use self-attention.}that uses point-wise feed-forward layers, we demonstrate that the self-attention module is a crucial architecture design for a diffusion model with a small number of elements as ours only used 16 part-latents. Inspired by Ho~\etal~\cite{Ho:2022CascadedLDM}, another key detail in our diffusion model design is to split a training process into two phases that each tailored to leverage the idiosyncrasy of the extrinsic and intrinsic representation. We first train an unconditional latent diffusion model with $\{\Ve_j\}$ which predicts noises in the $\Ve_j$ latent space. It corresponds to modeling a distribution of rough geometry of training shapes. In the second phase, we train a \emph{conditional} latent diffusion model which learns a conditional distribution $p(\Vs_j | \Ve_j)$. The latter model predicts noises in the $\Vs_j$ latent space with given condition $\Ve_j$. Splitting allows the first phase to learn only rough geometry of training shapes which reduce the complexity of the model significantly compared to previous methods. This ease of learning also allows the first phase model to create a much more diverse set of output. The second can then take advantage of this information to produce higher quality output. Altogether, we were able to achieve high fidelity in shape generation with higher variation while keeping the computational footprint low. We experimentally demonstrate the advantage of this split training approach by comparing against other model architectures. 

% To generate 3D models at test time, we first randomly sample $\{\hat{\Ve}_j\}$ from the first phase model. Then, we sample $\{\hat{\Vs}_j\}$ by feeding ${\{\hat{\Ve}_j\}}$ as condition to the second phase model. From these sampled latents, we obtain the final shape through the pre-trained Decoder $\mathcal{D}$. Since $\{\Ve_j\}$ represents a 3D shape as a primitive set and its variation affects the final output shape, our method also allows user to directly perform part editing method with minimal effort similarly to previous primitive-based shape representation work~\cite{Hao:2020Dualsdf, Hertz:2022Spaghetti}. Compared to previous methods, we also demonstrate that SALAD is the most suitable 3D generative model as it allow part-level shape manipulation, completion, and conditional shape generation. 

% In summary, the following are our contributions:
% \begin{itemize}
%     \item We first introduce a part-level latent diffusion model, SALAD, which generates diverse and high-resolution meshes with significantly less memory and computation. Quantitatively, we show that SALAD outperforms other baselines in random shape generation.
%     \item We present a Transformer-based network that enables a diffusion model training when a small number of elements is given.
%     \item We propose a cascaded training strategy for 3D generation and experimentally demonstrate its effectiveness.
%     \item To our best knowledge, SALAD is the first 3D diffusion model to provide primitive-based shape manipulation. Unlike previous work~\cite{Li:2022Diffusion-Sdf, Cheng:2022SDFusion, Hu:2023NeuralWavelet, Hui:2022NeuralWavelet}, it enables users to intuitively select and edit local regions. 
% \end{itemize}
