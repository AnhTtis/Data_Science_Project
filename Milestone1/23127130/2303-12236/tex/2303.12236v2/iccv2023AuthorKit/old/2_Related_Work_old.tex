\section{Related Work}
\label{sec:related_work}

%==== Keypoints ====%

%List related papers with a BibTeX reference and a tlâ€™dr (a description with a few sentences). Try to classify the papers into 2-3 categories. Keep adding papers.

%==================

%We briefly overview the literature related to our work in the followings, focusing on generative modeling for 3D shapes and part-level 3D representations.

% \Seungwoo{TODO: Briefly introduce GAN-based generative models in the first few sentences and start introducing diffusion-based works in details}
\paragraph{3D Generative Models.}

% \Seungwoo{Add other types of models such as AR, Flow, Energy-based, etc}
% There have been continual efforts to model distributions of 3D shapes in various representations. 
% \Seungwoo{GAN and VAE based method produces unsatisfying results}
% There have been continual efforts to model distributions of 3D shapes by leveraging generative modeling frameworks~\cite{Goodfellow:2014GAN,Rezende:2015NF,Van:2016Pixel,Van:2016WaveNet} that are widely adapted in other domains. 
There have been continual efforts to model distributions of 3D shapes by adapting various frameworks, such as GAN~\cite{Goodfellow:2014GAN}, Normalizing Flows~\cite{Rezende:2015NF}, or autoregressive models~\cite{Van:2016Pixel,Van:2016WaveNet}, that are widely studied in other domains.

A branch of work leverage GAN~\cite{Goodfellow:2014GAN} and learn distributions of latents that are decoded into various 3D representations including point clouds~\cite{Achlioptas:2018LatentGAN,Valsesia:2019GraphConv,Shu:2019Treegan} and implicit representations~\cite{Kleineberg:2020VoxelGAN,Hao:2020Dualsdf,Chen:2019ImNet,Ibing:2021GridBased}. This approach has been popular until recently, as represented in several work~\cite{Chan:2022EG3D,Gao:2022Get3d} that extend StyleGAN~\cite{Karras:2020StyleGAN2} further by combining its network architecture and differentiable renderer~\textbf{cite!}.

% Zheng:2022SdfStylegan
% Zheng:2022SdfStylegan,Gao:2022Get3d

%%%% Looks like OccNet uses VAE not GAN \cite{Mescheder:2019OccNet}

Another line of work instead build on autoregressive models~\cite{Van:2016Pixel,Van:2016WaveNet} to generate 3D shapes in  forms of meshes~\cite{Nash:2020Polygen}, point clouds~\cite{Sun:2020PointGrow}, or (ir)regular feature grids~\cite{Zhang:20223DILG,Yan:2022ShapeFormer}. They demonstrate their efficiacy in a wide range of downstream tasks, such as shape reconstruction and completion, but also in multi-modal conditional generation~\cite{Mittal:2022Autosdf,Fu:2022Shapecrafter}.

% Concurrently, approaches~\cite{Nash:2020Polygen,Sun:2020PointGrow,Zhang:20223DILG,Yan:2022ShapeFormer} that stem from autoregressive models~\cite{Van:2016Pixel,Van:2016WaveNet} deliver impressive results and extend themselves to a wider range of applications such as multi-modal 3D shape generation~\cite{Mittal:2022Autosdf,Fu:2022Shapecrafter}. % Furthermore, a distribution of work based on other types of framework, such as Normalizing Flows~\cite{Yang:2019PointFlow}, also 

% Achlioptas~\etal~\cite{Achlioptas:2018latentgan}, Valsesia~\etal~\cite{Valsesia:2019graph}, and Shu~\etal~\cite{Shu:2019treegan} propose generative models for point clouds based on GAN~\cite{Goodfellow:2014GAN} whereas Mescheder~\etal~\cite{Mescheder:2019OccNet}, Chen~\etal~\cite{Chen:2019ImNet}, Ibing~\etal~\cite{Ibing:2021GridBased}, Kleineberg~\etal~\cite{Kleineberg:2020VoxelGAN}, Hao~\etal~\cite{Hao:2020Dualsdf}, Zheng~\etal~\cite{Zheng:2022SdfStylegan}, and Gao~\etal~\cite{Gao:2022Get3d} 


% For instance, frameworks based on GAN~\cite{Goodfellow:2014GAN} demonstrated their capability of modeling distributions of point clouds~\cite{Achlioptas:2018latentgan,Valsesia:2019graph,Shu:2019treegan}, voxels~\cite{Smith:2017IWGAN}, and implicit functions~\cite{Kleineberg:2020VoxelGAN,Hao:2020Dualsdf,Zheng:2022SdfStylegan,Gao:2022Get3d,Mescheder:2019OccNet,Chen:2019ImNet,Ibing:2021GridBased}.
% There have been continual efforts to model distributions of 3D shapes in various representations. For instance, frameworks based on GAN~\cite{Goodfellow:2014GAN} demonstrated their capability of modeling distributions of point clouds~\cite{Achlioptas:2018latentgan,Valsesia:2019graph,Shu:2019treegan}, voxels~\cite{Smith:2017IWGAN}, and implicit functions~\cite{Kleineberg:2020VoxelGAN,Hao:2020Dualsdf,Zheng:2022SdfStylegan,Gao:2022Get3d,Mescheder:2019OccNet,Chen:2019ImNet,Ibing:2021GridBased}. While such approaches gained success in the field, they also suffered from several problems, such as training instability, that mostly originate from adversarial training.

% \Seungwoo{diffusion based model can be directly applied to 3D data, producing satisying results at the same time.}
% Recently, a line of work started to employ the core idea from diffusion models~\cite{Ho:2019DDPM} and score-based models~\cite{Song:2019Score} to achieve better generation quality. 
As of now, the major interest of the literature has moved to recent breakthroughs in 2D generative modeling, represented by diffusion models~\cite{Ho:2019DDPM} and score-based models~\cite{Song:2019Score}. Specifically, Cai~\etal~\cite{Cai:2020ShapeGF}, Luo~\etal~\cite{Luo:2021DPM}, and Zhou~\etal~\cite{Zhou:2021PVD} propose score-based~\cite{Cai:2020ShapeGF} or diffusion-based~\cite{Luo:2021DPM,Zhou:2021PVD} pipeline for learning distributions of point clouds. Hui~\etal~\cite{Hui:2022NeuralWavelet} adapts voxel grids of wavelet coefficients~\cite{Mallat1989:Wavelet,Daubechies1990:Wavelet,Velho1994:Wavelet} as compact representations of truncated signed distance functions (TSDFs) for diffusion models. Furthermore, the recent success of latent diffusion models (LDMs)~\cite{Rombach:2022LDM} in 2D image domain has prompted development of diffusion models operating in latent spaces of 3D representations, including triplanes~\cite{Shue:2022TriplaneDiffusion}, latent point clouds~\cite{Zeng:2022LION}, latent voxels~\cite{Li:2022Diffusion-Sdf}, or VAE~\cite{Kingma:2014VAE} embeddings~\cite{Chou:2022Diffusionsdf}.
\Seungwoo{They are not just latent diffusion models, but take spatial prior into account when introducing such latents}

% However, we emphasize that none of the aforementioned work allows for intuitive editing of 3D shapes due to inherent limitations of their 3D representations.
\Seungwoo{Less aggressive tone.}
However, we emphasize that none of the aforementioned work delivers both high-quality and editable shapes due to inherent limitations of their 3D representations.
% as their shape representations inherently lack support for local controls. % or do not provide spatial, geometric interpretation of latent features.
This raises the problem of finding an appropriate alternative that (\num{1}) is compact enough for diffusion-based pipeline, and (\num{2}) disentangles 3D shapes into several components to make manipulation easier. Therefore, we opt to explore previous work focusing on formulating part-level interpretations of 3D shapes.

%\paragraph{3D Shape Representation.}
\paragraph{Part-Level 3D Representation.}
Decomposing a complex subject into smaller components helps human get better understanding of its essence, and this is not an exception for 3D shapes. For instance, early work~\cite{} in computer vision proposed to fit simple primitives, such as cuboids~\cite{}, to 3D shapes for various cognitive tasks involving understanding of 3D geometry. The same rationale can be found in recent work aiming to devise better decomposed representations based on volumetric primitives~\cite{volumetric primitives}, quadrics~\cite{superquadric}~, or convexes~\cite{Chen:2020BspNet,CvxNet}. Although primitive-based decompositions produce more compact representations, they partition geometry as is, which can be still challenging to manipulate or edit.

% On the other hand, the success of neural implicit functions~\cite{Park:2019Deepsdf,Mescheder:2019OccNet} hinted their potential as 3D representations that are suitable for learning-based framework. While such representations are agnostic to sampling resolution and thus can convey high-frequency details, they lack editability as a 3D shape is approximated with a single analytic function. This motivated a body of work that focus on factoring out or 
Similarly, while the success of neural implicit functions~\cite{Park:2019Deepsdf,Mescheder:2019OccNet} hinted their potential as 3D representations that are suitable for learning-based framework, they also lack editability as a 3D shape is approximated with a single analytic function. Their limitation has motivated a body of work focusing on factoring out implicit function into several local components~\cite{Genova:2019LearningShapeTemplates,Genova:2020LDIF,Hertz:2022Spaghetti}, or separating topology information from local geometry~\cite{Hui:2022NeuralTemplate}. \Seungwoo{More detailed description on LDIF and Neural Templates..?} In particular, Hertz~\etal~\cite{Hertz:2022Spaghetti} propose to model overall structure of 3D shapes as Gaussian Mixture Models (GMMs) and to encode local geometry into latent codes, thereby enable various application such as local editing and part mixing.
\Seungwoo{One liner emphasizing the use of such part-level representation! In accordance with intro and background.}

Decomposing 3D shapes into multiple parts 

Decomposed representations for 3D shapes 

% Genova~\etal~\cite{Genova:2020LDIF} proposes to decompose a 3D shape into multiple shape elements that, together with a query point, can be decoded to values of a implicit function.
% Hui~\etal~\cite{Hui:2022NeuralTemplate} introduces a topology-aware representation consisting of two latent codes that disentangles overall shape topology from detailed geometry.
% Similarly, Hertz~\etal~\cite{Hertz:2022Spaghetti} decouples global structure and local geometry 

% Local Deep Implicit Functions (LDIF)~\cite{Genova:2020LDIF}. 
% Hao~\etal~\cite{Hao:2020Dualsdf} proposes a joint latent of a fine-grained detail shape and its primitive-based coarse proxy shape where the shape can be manipulated by moving primitives.
% Hui~\etal~\cite{Hui:2022NeuralTemplate} proposes two disentangled shape latent codes, one determines the topolgy of a shape and the other refines the rough template shape.
% SPAGHETTI~\cite{Hertz:2022Spaghetti} proposes to learn disentangled representations for 3D, and allows for sampling from the learned shape prior distribution. The disentangled part representation allow user to deform the final shape by apply affine transformations on the corresponding Gaussian primitives.  

% In particular, GAN-based approaches~\cite{Achlioptas:2018latentgan, Shu:2019treegan, Kleineberg:2020VoxelGAN} had form the mainstream in the field. 

% In particular, approaches based on the adversarial loss introduced in Goodfellow~\etal~\cite{Goodfellow:2014GAN} had formed the mainstream in the field. 3D representations of their choice include point clouds~\cite{Achlioptas:2018latentgan,Valsesia:2019graph,Shu:2019treegan}, voxels~\cite{Smith:2017IWGAN}, signed distance functions~\cite{Kleineberg:2020VoxelGAN,Hao:2020Dualsdf,Zheng:2022SdfStylegan,Gao:2022Get3d}, occupancy fields~\cite{Mescheder:2019OccNet,Chen:2019ImNet,Ibing:2021GridBased}, or meshes~\cite{Luo:2021SurfGen}. However, the interest of research community has made a gradually transition to diffusion models~\cite{Ho:2019DDPM}, or score-based models~\cite{Song:2019Score}, after witnessing their success in modeling distributions of 2D images. As our work is more relevant to the previous work of this class, we focus on surveying the literature in details.

% \paragraph{Diffusion Models for 3D Data.}
% Similar to 3D generative frameworks based on GAN~\cite{Goodfellow:2014GAN}, diffusion models~\cite{Ho:2019DDPM}, or score-based models~\cite{Song:2019Score}, for 3D data have been actively developed upon various 3D representations including point clouds~\cite{Cai:2020ShapeGF,Luo:2021DPM,Zhou:2021PVD,Zeng:2022LION}, implicit representations~\cite{Shue:2022TriplaneDiffusion,Li:2022Diffusion-Sdf,Hui:2022NeuralWavelet}, and meshes~\cite{Liu:2023MeshDiffusion}. Furthermore, recent work~\cite{Zeng:2022LION,Chou:2022Diffusionsdf} propose diffusion models operating in learned latent spaces of the previously mentioned 3D representations, inspired by the success of latent diffusion models (LDMs)~\cite{Rombach:2022LDM} in image domain.

% Cai~\etal~\cite{Cai:2020ShapeGF}, Luo~\etal~\cite{Luo:2021DPM}, and Zhou~\etal~\cite{Zhou:2021PVD} view points in point clouds as particles either (\num{1}) traversing along density gradient fields~\cite{Cai:2020ShapeGF} or (\num{2}) dissipating under thermodynamic systems~\cite{Luo:2021DPM,Zhou:2021PVD} and propose to train neural networks that reverse such processes.
%Such methods take point clouds as inputs that require network architectures~\cite{Qi:2017Pointnet} capable of handling permutation invariant data.
% While point clouds can effectively represent the overall structure of 3D shapes, they cannot capture fine details of the underlying geometry as their expressiveness greatly depends on the sampling resolution.

% More importantly, these models' inputs are order-invariant, unlike images, point clouds are equivalent with respect to permutation. More recently, Liu \etal~\cite{Liu:2023MeshDiffusion} introduces MeshDiffusion that directly trained on mesh-grid. 

% Another work model distributions of implicit functions, particularly signed distance functions (SDFs)~\cite{Hui:2022NeuralWavelet}. Compared to point clouds, implicit functions are capable of capturing high-frequency details of 3D shapes, and thus widely studied in learning-based approaches~\cite{Park:2019Deepsdf,Mescheder:2019OccNet} that aim to devise compact, resolution-agnostic representations for 3D shapes. However, such representations are often realized as voxel grids with function values at each cell, which make them less intriguing for diffusion-based generative modeling due to high computational cost. Hui~\etal~\cite{Hui:2022NeuralWavelet} alleviate this issue by learning a diffusion model on wavelet coefficients of truncated signed distance functions (TSDFs) derived via wavelet transform~\cite{Velho1994:Wavelet,Daubechies1990:Wavelet,Mallat1989:Wavelet}. Although it produces impressive results, such a representation does not provide direct control over local regions. % \Seungwoo{Refine this line more.}

% A body of work that also deserve our attention focus on building diffusion models for latent features of the aforementioned representations~\cite{Shue:2022TriplaneDiffusion,Li:2022Diffusion-Sdf,Chou:2022Diffusionsdf,Zeng:2022LION}. LION~\cite{Zeng:2022LION} employs a hierarchical latent space of point clouds which comprises two latent spaces of global shape features and per-point features, respectively. Shue~\etal~\cite{Shue:2022TriplaneDiffusion} suggest a method based on axis-aligned triplane features obtained by factorizing occupancy grids, whereas Chou~\etal~\cite{Chou:2022Diffusionsdf} employs a two-stage training pipeline that consists of VAE~\cite{Kingma:2014VAE} encoding latents of 3D shapes and a diffusion model which operates on the learned latent space. \Seungwoo{However, latent diffusion models are also struggle on tasks involving local editing... in a more neat sentence!}

% Another work in diffusion on SDF representation is Diffusion-SDF~\cite{Li:2022Diffusion-Sdf}.  Li~\etal~\cite{Li:2022Diffusion-Sdf} leverages a text-conditioned, diffusion model to learn the distribution of 3D shapes represented by truncated sign distance fields (TSDF). Instead of training directly on the spatial domain of TSDF, Hui~\etal~\cite{Hui:2022NeuralWavelet} proposes a two step diffusion pipeline where the distribution of compact coarse wavelet coefficient voxels is learned through a diffusion model and a secondary diffusion model is trained to then generate the detailed wavelet coefficients. 

% Inspired by the success of latent diffusion models (LDMs)~\cite{Rombach:2022LDM} in image domain, a more recent work~\cite{Zeng:2022LION} proposes a framework that consists of variational autoencoders (VAEs)~\cite{Kingma:2014VAE} and diffusion models~\cite{Ho:2019DDPM} operating in the learned, hierarchical latent space.
% diffusion on latent space. Decode to SDF, PointCloud
% There were also hybrid approaches in using diffusion model on mixed explicit representation and neural network's latents.
% Chou~\etal~\cite{Chou:2022Diffusionsdf} employs a two-stage training pipeline that consists of VAE encoding latents of 3D shapes and a diffusion model which operates on the learned latent space. LION~\cite{Zeng:2022LION} also have additional latent information for each point in pointcloud representation.

% diffusion on image space, backprop to density field, voxel
% \Seungwoo{Why referring to NeRFs...?}
% Concurrent work on NeRF~\cite{Mildenhall:2020NeRF} have shown that  diffusion model trained on image space could be distilled to neural renders. DreamFusion~\cite{Poole:2023Dreamfusion} extends on Dream Fields~\cite{Jain:2022DreamFields} by using Score Distillation Sampling loss as supervision. \Charlie{not sure if I should cite latent-nerf}

% \paragraph{Multimodal 3D Generation}
% Dream Fields~\cite{Jain:2022DreamFields}, TextCraft~\cite{Sanghi:2022Textcraft}, and CLIP-Forge~\cite{Sanghi:2022ClipForge} generate 3D through a differential rendering scheme with CLIP~\cite{Radford:2021CLIP} as guidance.
% ShapeCrafter~\cite{Fu:2022Shapecrafter} recursively generate new shape based on previous shape and portion of the prompt.
% AutoSDF~\cite{Mittal:2022Autosdf} and Text2Shape~\cite{Chen:2018Text2shape} learn shape embeddings for 3D generation.

% \paragraph{Diffusion Probabilistic Models}
% Diffusion probabilistic model (DDPM)~\cite{Ho:2019DDPM} is one class of the latent variable models. Compared to other generative models, variational autoencoder (VAE)~\cite{Kingma:2014VAE}, generative adversarial networks (GAN)~\cite{Goodfellow:2014GAN} and flow-based generative models~\cite{flow}, it has shown state-of-the-art performance in modeling a data distribution of 2D images~\cite{Ho:2019DDPM, Dhariwal:2021DiffusionBeatGANs, Rombach:2022LDM}. Rombach~\etal~\cite{Rombach:2022LDM} has proposed a latent diffusion model

% \paragraph{3D Shape Representation.}
% Local Deep Implicit Functions (LDIF)~\cite{Genova:2020LDIF}. 
% Hao~\etal~\cite{Hao:2020Dualsdf} proposes a joint latent of a fine-grained detail shape and its primitive-based coarse proxy shape where the shape can be manipulated by moving primitives.
% Hui~\etal~\cite{Hui:2022NeuralTemplate} proposes two disentangled shape latent codes, one determines the topolgy of a shape and the other refines the rough template shape.
% SPAGHETTI~\cite{Hertz:2022Spaghetti} proposes to learn disentangled representations for 3D, and allows for sampling from the learned shape prior distribution. The disentangled part representation allow user to deform the final shape by apply affine transformations on the corresponding Gaussian primitives.  
