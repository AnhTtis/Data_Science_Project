\section{SALAD - Part-Level Cascaded Diffusion}
\label{sec:method}
% \begin{figure*}[t!]
%     \centering
%     \dummyfig{0.98\linewidth}{0.4\linewidth}{Overall pipeline of our cascaded diffusion model with a conditional encoder and a decoder.}
%     \caption{An overview of our two-stage cascaded latent diffusion model. The first stage latent diffusion model learns a distribution of extrinsic latents. The second stage latent diffusion model learns a conditional distribution of intrinsic latents given extrinsics.}
%     \label{fig:our_pipeline}
% \end{figure*}


% To design a compact 3D shape generative model that can produce high-quality shapes while also being capable of manipulating shapes in a part level, we propose a part-level latent diffusion model. 
% on part-level of shapes, we utilize a object representation proposed by Hertz~\etal~\cite{Hertz:2022Spaghetti}. 

\Charlie{This paragraph briefly summarize the entire section}
To design a diffusion model that could leverage the part-based representation, we first propose a single phase diffusion model that directly learn the distribution of part latents. However, as demonstrated in \ref{sec:experiment}, this model lack the capability to \Charlie{output high quality generation or diversity depends on experiment result}. Motivated by recent work in diffusion models by Ho~\etal~\cite{Ho:2022CascadedLDM}, we propose a cascaded diffusion model design that independently learn the distribution of extrinsic and intrinsic representation. Finally, we show that the later design could be further extended for fine-grained conditional generation which the single phase model could not.

%%%%


%%%%

 \subsection{Single Phase Diffusion Model}
Following Rombach~\etal~\cite{Rombach:2022LDM}, we are interested in modeling distribution of latents. Specifically, in this work, we model the distribution of a set of part latents. 
% Let $\B{P} \in \Real^{N\times D}$ be a set of $\B{p}_i$ encapsulating the information of a 3D shape. 
Let $\{\B{p}_i\} \in \Real^{N\times D}$ be a set of $\B{p}_i$ encapsulating the information of a 3D shape.
As discussed in Sec.~\ref{sec:background_ddpm}, a diffusion model is trained to predict gaussian noise $\Veps$ added to $\{\B{p}_i\}$ at every timestep $t$: 
\begin{equation}
\begin{aligned}
\label{eq:simple_loss}
    \mathcal{L}_{\text{simple}} := \mathbb{E}_{t,\{\B{p}_i\},\Veps\sim \mathcal{N}(0, \B{I})} [ \Vert \Veps - \Veps_{\theta}(\{\B{p}_i^{(t)}\},\gamma(t)) \Vert_2^2],
\end{aligned}
\end{equation}
where $\{\B{p}_i^{(t)}\}$ is noisy set of part latents at timestep $t$, and $\gamma(\cdot)$ is a positional encoding that maps $\Real$ to a higher-dimensional space $\Real^{L}$.~\Juil{equation of positional encoding?} Unlike 2D images, our representation is irregular and forms a set that is permutation invariant. Thus, inspired by Lee~\etal~\cite{Lee:2019SetTF}, we implement a network by Transformer~\cite{Vaswani:2017Attention} instead of U-Net~\cite{Unet} which is a \emph{de facto} standard architecture of 2D diffusion models. Except for essential modifications for conditioning, mainly timestep $t$, we faithfully follow standard Transformer encoder architecture which consists of a stack of self-attention layers and fully-connected layers. When we take into account of conditions to the network, we insert an adaptive layer normalization (adaLN)~\cite{adaln} operation after each fully-connected layer, following the widespread usage in GANs~\cite{Karras:2020StyleGAN2} and diffusion models~\cite{Ho:2019DDPM, Luo:2021DPM}. 

In short, Figure~\ref{fig:transformer_block} depicts an overview of our single-phase model. The network takes the embeddings of input latents, and timestep $t$ scales and shifts every intermediate feature through the adaLN operation. Finally, the time-conditioned Transformer encoder predicts noise through self-attention layers. However, this na\"ive latent diffusion model fails to produce high fidelity shapes. Thus, we propose a cascaded latent diffusion model.

\subsection{Cascaded Latent Diffusion Model}
In this section, we describe our cascaded latent diffusion model. As discussed in Sec.~\ref{sec:background_part_representation}, our part-level representation $\B{p}_i$ is further separated into $\B{e}_i$ and $\B{s}_i$ which entail an input shape's overall rough structure, and fine-detailed surface information, respectively. We observe that it is much easier to first build rough geometry and then fill in the remaining details than to capture all the information of a shape at once, as similar as an artist who frames a sketch and then adds details. From this perspective, to fully leverage the advantage of this representation, we design a two-phase training strategy, inspired by Ho~\etal~\cite{Ho:2022CascadedLDM}. The first phase mdoel parameterized by $\phi$ is an unconditional latent diffusion model which predicts noise on $\{\B{e}_i\}\in\Real^{N\times D_e}$. 

We found that extrinsic parameters contain elements with unstable numerical values such as $\pi$ and $\boldsymbol{\lambda}$. After normalizing these elements based on element-wise means $\mu_i$ and standard deviations $\sigma_i$ of all training data following \ref{eq:normalizing}, our model perform significantly better. 

\begin{equation}
    \hat{x}_i = \cfrac{(x_i - \mu_i)}{\sigma_i}.
\label{eq:normalizing}
\end{equation}
This result agrees with previous work of Rombach~\etal~\cite{Rombach:2022LDM} on how avoiding arbitrary high-variance latent space during training could lead to better network performance. At inference time, we rescale these elements by the means and standard deviations.

% As Rombach~\etal~\cite{Rombach:2022LDM} stated, to avoid arbitrary high-variance latent space during training of the latent diffusion model, we normalize these elements based on element-wise means $\mu_i$ and standard deviations $\sigma_i$ of all training data.
% \begin{equation}
%     \hat{x}_i = \sigma_i x_i + \mu_i
% \end{equation}

On the other hand, the second phase model parameterized by $\psi$ is a \emph{conditional} latent diffusion model. It models a conditional distribution $p(\{\B{s}_i\} \vert \{\B{e}_i\})$ which leverages $\{\B{e}_i\}$ . 
The second phase model is implemented by consecutive two Transformer encoders. The first encoder encodes $\B{e}_i$ to global-aware extrinsic embeddings $f(\B{e}_i)$. The following Transformer encoder is conditioned by a concatenation of $f(\B{e}_i)$ and $\gamma(t)$. Since the correspondence between $\B{e}_i$ and $\B{s}_i$ is given in training, we do not apply cross-attention between them. 

In summary, the training objective function for each phase is summarized by:
\begin{align}
    \mathcal{L}_{ext} &= \mathbb{E}_{t,\{\B{e}_i\},\Veps} [ \Vert \Veps - \Veps_{\phi}(\{\B{e}^{(t)}_i\}, \gamma(t)) \Vert_2^2], \\
    \mathcal{L}_{int} &= \mathbb{E}_{t,\{\B{e}_i\},\{\B{s}_i\},\Veps} [ \Vert \Veps - \Veps_{\psi}(\{\B{s}^{(t)}_i\}, \gamma(t) |\,\{\B{e}_i\}) \Vert_2^2].
\end{align}

To generate a 3D shape at inference time, we sample the latents sequentially. First, we sample extrinsic set $\{\hat{\B{e}}_i\}$ from the first phase model, followed by the rescaling of the elements which are normalized in training. To obtain the correspoinding intrinsic latents set $\{\hat{\B{s}}_i\}$, we feed $\{\hat{\B{e}}_i\}$ into the second phase model. Finally, by feeding the sampled $\{ \hat{\B{e}}_i \}$ and $\{ \hat{\B{s}}_i \}$ to the pre-trained decoder $\mathcal{D}$, we can obtain the neural implicit representation of a generated shape according to Equation~\ref{eq:decoder}.
% \begin{equation}
%     \mathcal{L}_{ext} = \cfrac{1}{M} \sum_{j}^{M} \mathbb{E}_{\Ve_j ,t, \Veps \sim \mathcal{N}(0,\mathbf{I})} || \Veps - \Veps_\theta( \Ve_j^{(t)}, \gamma(t))||_2^2,
% \label{eq:phase1_loss}
% \end{equation}



% The phase one model is an unconditional latent diffusion model~\cite{Rombach:2022LDM} with $\{\Ve_j\}$. 
% where $\Ve_j^{(t)}$ indicates a noised\Charlie{noisy} extrinsic latent at timestep $t$ and $\gamma(\cdot)$ is a positional encoding. The phase 1 model $\Veps_\theta$ is implemented as a variant of the Transformer encoder: a stack of self-attention layers and MLPs. 

% By predicting noises at every uniformly sampled timestep $t$ as Equation~\ref{eq:phase1_loss}, the model learns the distribution of extrinsic latents. \Juil{We might need to include index encoding.}
% % As Rombach~\etal~\cite{Rombach:2022LDM} stated that it is crucial to avoid arbitrary high-variance latent space during training of the diffusion model, we normalize elements with unstable numerical values, $\pi$ and $\boldsymbol{\lambda}$, by element-wise means $\mu_i$ and standard deviations $\sigma_i$ pre-computed from all training data. In inference time, we re-scale these elements:
% However, we found that ground truth latents contain elements with unstable numerical values such as $\pi$ and $\boldsymbol{\lambda}$. After normalizing these elements based on element-wise means $\mu_i$ and standard deviations $\sigma_i$ of all training data following \ref{eq:normalizing}, our model perform significantly better. 
% \begin{equation}
%     \hat{x}_i = \sigma_i x_i + \mu_i.
% \label{eq:normalizing}
% \end{equation}
% This result agree with previous work of Rombach~\etal~\cite{Rombach:2022LDM} on how avoiding arbitrary high-variance latent space during training could lead to better network performance.

% Our phase two model is a \emph{conditional} latent diffusion model which models a conditional distribution $p(\Vs_j | \Ve_j)$. 
% \begin{equation}
%     \mathcal{L}_{int} = \cfrac{1}{M} \sum_{j}^{M} \mathbb{E}_{ \Ve_j,\Vs_j,t,\Veps\sim \mathcal{N}(0,\mathbf{I})} || \Veps - \Veps_\phi (\Vs_j^{(t)}, \gamma(t) |\, \Ve_j) ||_2^2
% \end{equation}
% Given a noised \Charlie{noisy} intrinsic latent $\Vs_j^{(t)}$ at time step $t$ and $\Ve_j$ as a condition, the phase 2 model $\Veps_\phi$ predicts noises $\Veps$. To condition both $\gamma(t)$ and $\Ve_j$ on the phase 2 model, we concatenate $\gamma(t)$ and embeddings of $\Ve_j$ as condition. The phase 2 model is implemented by two Transformer encoders where the first encoder embeds $\Ve_j$ to $h(\Ve_j)$ and the other encoder replace MLPs with TMLPs conditioned by $\gamma(t)$ and $h(\Ve_j)$. 

% To generate a 3D shape in test time, we first sample $\{\hat{\Ve}_j\}$ from the phase 1 model and feed them to the phase 2 model to obtain the corresponding $\{\hat{\Vs}_j\}$. Lastly, the pre-trained decoder $\mathcal{D}$ returns an implicit field from the sampled $\{\hat{\Ve}_j\}$ and $\{\hat{\Vs}_j\}$.

% \subsection{Conditioning Mechanism}
% Our network is agnostic to arbitrary additional conditions. When additional condition $c$ is given, we can extract 