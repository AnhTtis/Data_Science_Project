\section{SALAD -- Part-Level Cascaded Diffusion}
\label{sec:method}

\begin{figure*}
\label{fig:pipeline}
\includegraphics[width=\linewidth]{figures/pipeline_draft.pdf}
\caption{\textbf{Pipeline overview.} \salad{} consists of two diffusion models for extrinsic and intrinsic vectors, respectively. During phase 1 (left), it generates extrinsic vectors representing structures of shapes. Phase 2 (right) takes these outputs as conditions and produces intrinsic vectors encoding local geometry information.}
\vspace{-\baselineskip}
\end{figure*}


Here we introduce our cascaded diffusion framework generating the part-level implicit shape representation.
In the shape representation introduced in Section~\ref{sec:background_part_representation}, note that there are multiple \emph{layers} of representations all of which can be decoded into the original shape, such as the global latent $\mathbf{z}$, the set of part latents $\{\mathbf{p}_i\}$, and the set of extrinsic and intrinsic vectors $\{(\mathbf{e}_i, \mathbf{s}_i)\}$. Below, we first introduce some preliminary approaches to learning diffusion for each representation, and then we propose our final cascaded framework for learning diffusions in two phases.

\vspace{-10pt}
\paragraph{Diffusion of $\mathbf{z}$.}
Learning diffusion in the space of the global shape latent $\mathbf{z}$ is straightforward; the noise prediction network $\boldsymbol{\epsilon}_{\theta}$ (in Equation~\ref{eq:ddpm_loss}) can be simply modeled as an MLP. In the network $\boldsymbol{\epsilon}_{\theta}$, the timestep $t$ is generally first transformed by a positional encoding $\gamma(\cdot)$~\cite{Vaswani:2017Attention} and then fed as the scale and translation factors to the adaptive normalization layers such as AdaLN~\cite{Perez:2018AdaLN}.
In our experiments (Section~\ref{sec:shape_generation}), we show that this simple diffusion already outperforms the quality of generation by sampling $\mathbf{z}$ from the unit Gaussian since it can learn the exact distribution of $\mathbf{z}$, although the improvement is marginal.

\vspace{-10pt}
\paragraph{Diffusion of $\{ \mathbf{p}_i \}_{i=1}^{N}$.} To improve the quality of generation, one can instead consider diffusing the set of part latents $\{ \mathbf{p}_i \}_{i=1}^{N}$. A simple MLP taking the concatenation of the part latents as input, however, results in diffusion in a very high-dimensional space and also does not address the order invariance of the set data. We employ Transformer~\cite{Vaswani:2017Attention} to properly handle the set data while also promoting communications across parts. Each self-attention block is equipped with a post-MLP, where the positional-encoded timestep $\gamma(t)$ is fed to the AdaLN layer. This part-level latent diffusion can better reproduce the details of each part, while it still suffers from the difficulty in diffusing in a high-dimensional latent space.

\vspace{-10pt}
\paragraph{Cascaded Diffusion of $\{ \mathbf{e}_i \}_{i=1}^{N}$ and $\{ \mathbf{s}_i \}_{i=1}^{N}$.}
Inspired by Ho~\etal~\cite{Ho:2022CascadedLDM} introducing \emph{cascaded} diffusion for images, diffusing low-resolution images first and then diffusing high-resolution images conditioned on the low-resolution outputs, we propose a \emph{two-phase} framework for learning diffusion. We observe that the extrinsic and intrinsic attributes $\{ \mathbf{e}_i \}_{i=1}^{N}$ and $\{ \mathbf{s}_i \}_{i=1}^{N}$ play similar roles to low- and high-resolution images; the former describes the approximate of the data, while the latter captures fine details. Also importantly, the extrinsic vector $\mathbf{e}_i$ is much lower-dimensional, thus easier to make the noise prediction converge. Thus, in our first phase, we learn the diffusion of $\{ \mathbf{e}_i \}_{i=1}^{N}$ with the same Transformer-based noise prediction network $\boldsymbol{\epsilon}_{\theta}$ above. Then, in the second phase, we use another Transformer-based network $\boldsymbol{\epsilon}_{\phi}$ to model a conditional distribution $p(\{ \mathbf{s}_i \}_{i=1}^{N} | \{\mathbf{e}_i \}_{i=1}^{N})$ given $\{\mathbf{e}_i \}_{i=1}^{N}$.
Specifically, in the post-MLP of the self-attention block, for each $\mathbf{s}_i$, now the AdaLN layer takes as input a concatenation of the positional-encoded timestep $\gamma(t)$ and a feature vector $\mathcal{E}(\mathbf{e}_i)$ learned from the corresponding extrinsic parameters $\mathbf{e}_i$. The features $\{\mathcal{E}(\mathbf{e}_i) \}_{i=1}^{N}$ are learned from an additional stack of the self-attention modules encoding $\{ \mathbf{e}_i \}_{i=1}^{N}$. Both of the noise prediction networks $\boldsymbol{\epsilon}_{\theta}$ and $\boldsymbol{\epsilon}_{\phi}$ are trained with the same variational bound loss with Equation~\ref{eq:ddpm_loss} as follows:
\begin{equation}
\begin{aligned}
\label{eq:simple_loss}
\vspace{-\baselineskip}
    \mathcal{L}_{\mathbf{e}}(\theta) &:= \mathbb{E}_{t,\{\mathbf{e}\}_{i=1}^N,\Veps} \left[ \left\lVert \Veps - \Veps_{\theta}\left(\{\mathbf{e}^{(t)}\}_{i=1}^N,\gamma(t)\right) \right\rVert^2 \right]
    \raisetag{20pt}
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\vspace{-\baselineskip}
    \mathcal{L}_{\mathbf{s}}(\phi)&:= \mathbb{E}_{t,\{\mathbf{s}\}_{i=1}^N,\Veps} \left[ \left\lVert \Veps - \Veps_{\phi}\left(\{\mathbf{s}^{(t)}\}_{i=1}^N,\gamma(t), \{\mathbf{e}^{(0)}\}_{i=1}^N\right) \right\rVert^2 \right]
\end{aligned}
\end{equation}
where $\mathbf{e}^{(t)}$ and $\mathbf{s}^{(t)}$ are the extrinsic and intrinsic attributes after $t$-step forward process of adding Gaussian noise, respectively. Refer to the \textbf{supplementary material} for more implementation details.



























