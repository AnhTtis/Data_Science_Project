\vspace{-15pt}
\section{Introduction}
\label{sec:introduction}
\vspace{-5pt}
The staggering rise of the recent image generative model such as DALL-E 2~\cite{Ramesh:2022DALLE2}, StableDiffusion~\cite{Rombach:2022LDM}, and Midjourney~\cite{Midjourney} has drawn great attention to the diffusion models. With the state-of-the-art performance in generating data~\cite{Dhariwal:2021DiffusionBeatGANs, Ho:2019DDPM, Rombach:2022LDM,Ramesh:2022DALLE2,Midjourney}, diffusion models have quickly replaced existing generative models in many applications.
Besides the quality of the generated data, another key advantage of the diffusion models is the zero-shot capability in completion and editing. Recent research~\cite{Chung:2022MCG, Lugmayr:2022Repaint, Meng:2022SDEdit} has shown that diffusion models trained without any conditions can be applied to completion and editing tasks by starting the reverse process from partial data and properly guiding the process.

Such capabilities of the diffusion models have prompted attempts to apply them to 3D generation~\cite{Cai:2020ShapeGF, Luo:2021DPM, Nichol:2022Point-E, Zhou:2021PVD, Zeng:2022LION, Li:2022Diffusion-Sdf, Nam:20223D-LDM, Hui:2022NeuralWavelet}, although likewise the other neural 3D generation and reconstruction work, the key challenge in applying diffusion to 3D is to find an appropriate representation of 3D data. Particularly, to take full advantage of the diffusion models, both producing realistic data and being leveraged to editing and manipulation, a careful design of the 3D data representation is needed.
A naive adaption of the 2D image diffusion models to the 3D voxels is impractical due to the order of magnitude more computation time and memory. Hence, the earlier attempt to apply diffusion or score-based models to 3D (which has also been continued until recently) was to use point clouds as 3D representation~\cite{Cai:2020ShapeGF, Luo:2021DPM, Nichol:2022Point-E}, although the fine details of shapes could not be reproduced since the training computation is still too heavy to increase the resolution --- $2k$ points are used in training.
Later, some hybrid representations have been explored, such as points and voxels~\cite{Zhou:2021PVD}, points and features~\cite{Zeng:2022LION}, voxels and features~\cite{Li:2022Diffusion-Sdf}, although these were still limited in being trained with low-resolution 3D data.
Implicit representation has been proven to be the best to capture fine details in 3D generation and reconstruction~\cite{Park:2019Deepsdf,Chen:2019ImNet,Mescheder:2019OccNet}. Hence, concurrent work~\cite{Li:2022Diffusion-Sdf,Nam:20223D-LDM} introduced latent diffusion methods generating codes that can be decoded into implicit functions of 3D shapes.
However, then the diffusion in a latent space cannot be used for the \emph{guided} reverse process -- \eg~filling a missing part of a shape while preserving the others, and thus the model cannot be exploited for manipulation.
Neural wavelet~\cite{Hui:2022NeuralWavelet} is a notable exception that improves efficiency in training without a latent space but by learning diffusion in spectral wavelet space.
While it succeeded in producing local details, it is still nontrivial to specify a local region to be modified in the spectral space, thus limiting the model to be used in the manipulation tasks.

As a 3D diffusion model feeding two birds with one seed, achieving high-quality \emph{generation} and enabling \emph{manipulation}, we present our novel \textbf{S}hape P\textbf{A}rt-Level \textbf{LA}tent \textbf{D}iffusion Model, dubbed \textbf{\salad\textbf{}}.
Our work is inspired by recent work~\cite{Genova:2020LDIF,Hui:2022NeuralTemplate,Lin:2022Neuform,Hertz:2022Spaghetti} introducing disentangled implicit representations into \emph{parts}. The advantages of the part-level disentangled representation are in the \emph{efficiency} allocating the memory capacity of the latent code effectively to multiple parts, and also in the \emph{locality} allowing each part to be edited independently, thus best fitted to our purpose. We specifically base our work on SPAGHETTI~\cite{Hertz:2022Spaghetti} that learns the part decomposition in a self-supervised way. Each part is described with an independent embedding vector describing the extrinsics and intrinsics of the part as shown in Figure~\ref{fig:teaser}, and thus the parts that need to be edited or replaced can be easily chosen. It is a crucial difference from latent diffusion where the latent codes do not explicitly express any spatial and structural information and voxel diffusion where the region to be modified can only be specified in the 3D space, not in the shape.

Our technical contribution is the diffusion neural network designed to properly handle the characteristics of the part-level implicit representation, which is a \emph{set} of \emph{high-dimensional} embedding vectors. To cope with the set data and achieve permutation invariance while allowing global communications across the parts, we employ Transformer~\cite{Vaswani:2017Attention} and condition each self-attention block with the timestep in the diffusion process. The challenge is also in learning diffusion in the high-dimensional embedding space, which is known to be hard to train~\cite{Yu:2023Video}.
To get around the issue, we introduce a \emph{two-phase cascaded} diffusion model.
We leverage the fact that the part embedding vector is split into a small set of \emph{extrinsic} parameters approximating the shape of a part and a high-dimensional \emph{intrinsic} latent supplementing the detailed geometry information.
Hence, our cascaded pipeline learns two diffusions, one generating extrinsic parameters first and the other producing an intrinsic latent conditioned on the extrinsics, effectively improving the generation quality with the same computation resources.





Our quantitative and qualitative assessments on \salad{} demonstrate its outperformance compared with SotA methods in shape generation as shown in Section~\ref{sec:shape_generation}. We further demonstrate zero-shot manipulation capability of our \salad{}, trained solely for unconditional generation, by conducting extensive experiments on downstream tasks, including part completion (Section~\ref{sec:shape_completion}), part mixing and refinement (Section~\ref{sec:part_mixing}). %
Last but not least, we showcase the versatility of \salad{} in modeling multi-modal distributions such as text-guided generation (Section~\ref{sec:text_conditional_generation}) and completion (Section~\ref{sec:text_driven_manipulation}). To summarize, our contributions are:

\vspace{-5pt}
\begin{itemize}
    \setlength\itemsep{0.08em}
    \item We propose \salad{}, a novel diffusion model capable of generating part-level 3D implicit representations.
 
    \item We propose a \emph{two-phase cascaded} diffusion model, effective for handling high-dimensional latent spaces, that sets a new SotA in shape generation.
    \item We demonstrate the importance of orchestrating diffusion models and part-level implicit representation for the zero-shot capability of \salad{} in shape editing.
    \item We further extend our \salad{} to text-guided generation and editing that can synergize with text-driven part segmentation network.
\end{itemize}






















