\section{Experiment}
\label{sec:experiment}
In this section, we demonstrate that~\salad{} outperforms other baselines in shape \emph{generation} (Section~\ref{sec:shape_generation}) and enables intuitive \emph{manipulation}, such as part completion (Section~\ref{sec:shape_completion}) and part mixing and refinement (Section~\ref{sec:part_mixing}), where the combination of part-level representation and diffusion models is essential.
Lastly, we also demonstrate that~\salad{} outperforms other baselines in text-guided shape generation (Section~\ref{sec:text_conditional_generation}) and can leverage part-level representation for text-guided part completion (Section~\ref{sec:text_driven_manipulation}).

\subsection{Shape Generation}
\label{sec:shape_generation}
\paragraph{Evaluation Setup.}
For evaluation and comparison, we follow the settings of Hui~\etal~\cite{Hui:2022NeuralWavelet}. We use \emph{airplane} and \emph{chair} classes from the ShapeNet~\cite{Chang:2015Shapenet} dataset and the train-test split from Chen~\etal~\cite{Chen:2019ImNet}. The model is trained for each class. At inference time, we sample \num{2000} shapes for each class, and measure three evaluation metrics~\cite{Achlioptas:2018LatentGAN,Lopez-paz:20171nna} to assess quality and diversity of the generated shapes:
Coverage (COV), Minimum Matching Distance (MMD), and 1-Nearest Neighbor Accuracy (1-NNA).
We compare \salad{} with existing 3D generative models~\cite{Chen:2019ImNet,Kleineberg:2020VoxelGAN,Luo:2021DPM,Hertz:2022Spaghetti,Hui:2022NeuralWavelet}.%



\vspace{-15pt}
\paragraph{Results.} 
The quantitative and qualitative results, including ablation studies, are summarized in Table~\ref{tbl:quantitative_comparison_of_shape_generation} and Figure~\ref{fig:shape_generation_qualitative_results}.
For more results, refer to the \textbf{supplementary material}.
We reproduced the results of SPAGHETTI~\cite{Hertz:2022Spaghetti} and Neural Wavelet~\cite{Hui:2022NeuralWavelet} using the official code,
and the other quantitative results are directly borrowed from Hui~\etal~\cite{Hui:2022NeuralWavelet}, marked with ``$*$'' in Table~\ref{tbl:quantitative_comparison_of_shape_generation}.
(We also display the results of SPAGHETTI~\cite{Hertz:2022Spaghetti} and Neural Wavelet~\cite{Hui:2022NeuralWavelet} reported by Hui~\etal~\cite{Hui:2022NeuralWavelet}~\cite{Hui:2022NeuralWavelet} in the gray-colored rows. Note that SPAGHETTI results are similar, while there is a gap in the Neural Wavelet results.)
To ease qualitative comparisons in Figure~\ref{fig:shape_generation_qualitative_results},
we retrieve the generated shapes using the same query ground truth shape and compare them.

As shown in Table~\ref{tbl:quantitative_comparison_of_shape_generation},~\salad{} achieves SotA results or is on par with the baselines. In particular, we outperform Neural Wavelet~\cite{Hui:2022NeuralWavelet}, which is a SotA diffusion-based 3D generative model, on 1-NNA by a large margin: \num{65.04} vs. \num{57.82} for \emph{chair} CD, and \num{75.77} vs. \num{73.92} for \emph{airplane} CD (lower is better).

Qualitatively,~\salad{} produces clean high-resolution meshes with fine details as shown in Figure~\ref{fig:shape_generation_qualitative_results}. When comparing ``Diffusion of $\B{z}$'' (in Section~\ref{sec:method}) with SPAGHETTI~\cite{Hertz:2022Spaghetti}, we demonstrate that our simple latent diffusion already produces much better quality shapes than sampling $\B{z}$ from the unit Gaussian distribution as SPAGHETTI does. 
``Diffusion of $\{\B{p}_i\}_{i=1}^N$'' uses Transformer~\cite{Vaswani:2017Attention} instead of simple MLPs and outperforms ``Diffusion of $\B{z}$'', clearly showing how our Transformer-based architecture is the key to learning the distribution of high-dimensional latents represented as a set.

When comparing our final model~\salad{} with ``Diffusion of $\{\B{p}_i\}_{i=1}^N$'',~\salad{} outperforms ``Diffusion of $\{\B{p}_i\}_{i=1}^N$'' by a large margin across all metrics. It shows that our cascaded diffusion training is crucial to improve shape generation quality.




\input{tables/shape_generation_comparison.tex}
\input{figures/qualitative_comparison.tex}


\subsection{Part Completion}
\label{sec:shape_completion}


\begin{figure*}
\centering
{
\scriptsize
\setlength{\tabcolsep}{0em}
\renewcommand\tabularxcolumn[1]{m{#1}}
\newcolumntype{W}{>{\centering\arraybackslash}m{0.094444\textwidth}}
\newcolumntype{Z}{>{\centering\arraybackslash}m{0.188888\textwidth}}
\begin{tabularx}{0.85\textwidth}{WWWZZZ}
GT & Bounding Box & Gaussians & ShapeFormer~\cite{Yan:2022ShapeFormer} &Neural Wavelet~\cite{Hui:2022NeuralWavelet}& {\salad} (Ours) \\ 
\multicolumn{6}{c}{\includegraphics[width=0.85\textwidth]{figures/fig2_0309.png}} 
\end{tabularx}
}
\caption{\textbf{Qualitative comparison of the part completion.} We examine \salad{} and other baselines in part completion after ablating semantic parts or regions, highlighted in red in columns 2 and 3. \salad{} produces realistic completions for missing parts. The baselines fail to preserve observed parts or introduce noticeable seams at bounding box boundaries.
}
\label{fig:shape_completion}
\vspace{-0.5\baselineskip}
\end{figure*}



Here, we describe how~\salad{}, which was trained in an \emph{unconditional} setup, can be employed to part completion. We compare the results against the most recent diffusion model, Neural Wavelet~\cite{Hui:2022NeuralWavelet} and the SotA of shape completion, ShapeFormer~\cite{Yan:2022ShapeFormer}.

\sisetup{group-separator={,}}

\vspace{-10pt}
\paragraph{Experiment Setup.}
For completion using diffusion models, we run \emph{guided} reverse process proposed by Meng~\etal~\cite{Meng:2022SDEdit}.
Specifically, given the input data $\B{x}\in \mathbb{R}^d$ and a mask of the region to be reconstructed $m \in [0, 1]^d$, each step of the reverse process of the diffusion is performed as follows:
\begin{equation}
 \begin{aligned}
 \B{x}^{(t-1)}_{\text{unmasked}}&\sim\mathcal{N}(\sqrt{\bar\alpha^{(t)}}\B{x}^{(0)},(1-\bar\alpha^{(t)})\B{I}) \\
 \B{x}^{(t-1)}_{\text{masked}}&\sim\mathcal{N}(\boldsymbol{\mu}_\theta(\B{x}^{(t)},t),\beta^{(t)}\B{I}) \\
 \B{x}^{(t-1)}&=m\odot\B{x}^{(t-1)}_{\text{unmasked}}+(1-m)\odot\B{x}^{(t-1)}_{\text{masked}}.
 \end{aligned}
\end{equation}
Unlike previous methods such as ShapeFormer~\cite{Yan:2022ShapeFormer}, this approach guarantees to preserve the unmasked region.
In our experiments, we randomly remove and regenerate a semantic part of \emph{chairs} and \emph{airplanes}. While we can simply select ($\B{e}_i$, $\B{s}_i$) pairs of parts we want to remove in \salad{}, in feature-voxel representation like Neural Wavelet~\cite{Hui:2022NeuralWavelet}, it is not trivial to specify the regions that would include the completed part. This limits their generation output to only occupy the masked voxels, while a larger mask could interfere with or even break unwanted parts leading to seams in the final output. 
For the guided reverse process of Neural Wavelet~\cite{Hui:2022NeuralWavelet} in our experiments, we use the axis-aligned bounding box of a part as a mask and transform the mask to the wavelet domain. Refer to the \textbf{supplementary material} for more details on mask construction.

We first randomly choose 100 shapes from our training set. Then, for all methods, we randomly select a semantic part from each shape and generate five variations.
For quantitative comparisons, we report the reconstruction loss, MMD and FPD (Fréchet PointNet Distance)~\cite{Shu:2019Treegan} indicating the quality and diversity of completions. Note that we measure MMD \emph{from} completions \emph{to} groundtruth shapes to quantify the proximity of the completed shapes to the groundtruth shapes.
We use the official pre-trained models for ShapeFormer~\cite{Yan:2022ShapeFormer} and Neural Wavelet~\cite{Hui:2022NeuralWavelet}. We also report the results from Neural Wavelet trained by ourselves.




\vspace{-\baselineskip}
\paragraph{Results.}
The quantitative results and qualitative results are summarized in Table~\ref{tbl:part_regeneration} and Figure~\ref{fig:shape_completion}, respectively. 
For more results, refer to the \textbf{supplementary material}.
As shown in Table~\ref{tbl:part_regeneration},~\salad{}, trained solely for \emph{unconditional} shape generation, outperforms the baselines in most of the metrics by large margins, especially in FPD which is the metric of how plausible the shapes are. 


The qualitative results presented in Figure~\ref{fig:shape_completion} further manifests the advantages of employing a part-level 3D representation in \salad{}. In row 1 of Figure~\ref{fig:shape_completion}, ShapeFormer~\cite{Yan:2022ShapeFormer} introduces noticeable artifacts at the back of the chair that lies outside the binary mask (column 2).
In contrast, \salad{} completes the seat seamlessly while preserving the other parts, benefiting from the spatial correspondence between the binary mask and the shape representation. 
Even with such spatial correspondences, the limitation of specifying regions instead of parts persists in Neural Wavelet~\cite{Hui:2022NeuralWavelet}. In particular, the row 2 of Figure~\ref{fig:shape_completion} shows visible seams at the bounding box boundary while \salad{} generates the missing part consistent with the surrounding parts. 



































\input{tables/part_regeneration.tex}

\subsection{Part Mixing and Refinement}
\label{sec:part_mixing}

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{figures/fig_part_mixing.pdf}
\caption{\textbf{Qualitative results of Part mixing and refinement.} \salad{} improves quality of part mixing outputs.}
\label{fig:part_mixing}
\vspace{-\baselineskip}
\end{figure}




While Hertz~\etal~\cite{Hertz:2022Spaghetti} demonstrates creating new shapes by combining parts from existing shapes, naively mixing part representations is prone to produce failure cases as illustrated in Figure~\ref{fig:part_mixing} and Figure~\ref{fig:teaser}. Cracks or discontinuities at joint regions are one type of failure case as shown in row 3 of Figure~\ref{fig:part_mixing} and (b) of Figure~\ref{fig:teaser}. Another type of failures is the dissonance between combined parts that results in undesired distortions or the vanishing of parts. 
\salad{} can remedy this issue by refining both the extrinsic and intrinsic vectors through the guided reverse process. Refer to the \textbf{supplementary material} for more qualitative results and \textbf{quantitative comparisons}.













\subsection{Text-Guided Shape Generation}
\label{sec:text_conditional_generation}
We further demonstrate \salad{} can perform \emph{conditional} generation, specifically generating 3D shapes given an input text. To condition a text to the model, we concatenate a language feature and an input of AdaLN, $\gamma(t)$, and optionally $\mathcal{E}(\B{e}_i)$.
We experiment with the text and shape pair dataset from ShapeGlot~\cite{Achlioptas:2019Shapeglot} and compare the generation quality of our text-conditioned model with the one by AutoSDF~\cite{Mittal:2022Autosdf}, which is the SotA text-to-shape generative model. 
The train-test split used in AutoSDF is used.
Also, following AutoSDF, we measure the following three metrics for the evaluation: CLIP-Similarity-Score (CLIP-S)~\cite{Radford:2021CLIP}, Neural-Evaluator-Preference (NEP), and Fréchet Point Cloud Distance (FPD)~\cite{Shu:2019Treegan}.

NEP proposed by Mittal~\etal~\cite{Mittal:2022Autosdf} is a preference rate obtained from a neural evaluator. The neural evaluator is pre-trained on a text-conditioned binary classification task where the model distinguishes the target shape corresponding to the input text. 
Since the neural evaluator used in AutoSDF has not publicly been released, we train our neural evaluator based on PartGlot~\cite{Koo:2022Partglot}, a simpler architecture trained only on point clouds without images. More details of the experiment setup is in the \textbf{supplementary material}. 

As shown in Table~\ref{tbl:lang_quantitative_result}, our generated shapes are more preferred by the neural evaluator over the shapes generated by AutoSDF. Also, Figure~\ref{fig:text_generation} and FPD results reflect that ~\salad{} produces more plausible shapes, and our generated shapes conform to given texts more than the shapes of AutoSDF. 


\input{tables/language_qualitative_results}
\input{tables/language_quantitative_results}

\vspace{-10pt}
\subsection{Text-Guided Part Completion}
\label{sec:text_driven_manipulation}
We further demonstrate how~\salad{} can be integrated with a text-driven semantic part segmentation network to aid user interactive shape editing. Following PartGlot~\cite{Koo:2022Partglot} architecture, we design \gaussglot{}, a model that uses $\{\B{e}_i\}_{i=1}^N$ as a part representation and predicts semantic part labels of those from texts. More details of \gaussglot{} architecture and training results can be found in the \textbf{supplementary material}. 
Figure~\ref{fig:text_completion} shows examples that the parts of input shapes selected by \gaussglot{} are completed according to given texts by a reverse process of text-conditioned~\salad{} introduced in Section~\ref{sec:text_conditional_generation}. It demonstrates that users can freely manipulate 3D shapes with texts in an end-to-end manner by leveraging~\salad{} with~\gaussglot{}.
\input{tables/text_guided_completion}






