\ifpaper
  \newcommand{\refofpaper}[1]{\unskip}
  \newcommand{\refinpaper}[1]{\unskip}
  \newcommand{\suppSegDir}{supp_segmentations}
\else
  \makeatletter
  \newcommand{\manuallabel}[2]{\def\@currentlabel{#2}\label{#1}}
  \makeatother
  \manuallabel{sec:introduction}{1}
  \manuallabel{sec:related_work}{2}
  \manuallabel{sec:background_diffusion}{3.1}
  \manuallabel{sec:background_part_representation}{3.2}
  \manuallabel{sec:method}{4}
  \manuallabel{sec:shape_generation}{5.1}
  \manuallabel{sec:shape_completion}{5.2}
  \manuallabel{sec:results_part_mixing}{5.3}
  \manuallabel{sec:text_conditional_generation}{5.4}
  \manuallabel{sec:text_driven_manipulation}{5.5}
  \manuallabel{fig:shape_generation_qualitative_results}{4}
  \manuallabel{fig:shape_completion}{5}
  \manuallabel{fig:part_mixing}{6}
  \manuallabel{fig:text_generation}{7}
  
  \manuallabel{tbl:quantitative_comparison_of_shape_generation}{1}
  \manuallabel{tbl:part_regeneration}{2}
  
  \newcommand{\refofpaper}[1]{of the main paper}
  \newcommand{\refinpaper}[1]{in the main paper}
\fi

\ifpaper 

\else
\clearpage
\newpage
\tableofcontents
\begin{figure}[h!]
\centering
\includegraphics[width=1\textwidth]{figures/shape_generation/fig1_gallery_v4.png}
\caption{\textbf{A visual gallery of \emph{airplanes}, \emph{chairs}, and \emph{tables} generated by~\salad{}.}}
\label{fig:generation_gallery}
\end{figure}
\clearpage
\newpage
\subsection{Overview}
In this supplementary material, we first present \emph{quantitative} results of part mixing and refinement (Section~\ref{sec:supp_part_mixing}). 
Then, we illustrate additional details in the implementation of \salad{} (Section~\ref{sec:supp_salad_implementation_details}) and details of the experiments discussed in the main paper (Section~\ref{sec:supp_experimental_details}). Lastly, we report more \emph{qualitative} results of shape generation (Section~\ref{sec:more_shape_generation_comparison}), part completion (Section~\ref{sec:more_part_completion_comparison}), part mixing and refinement (Section~\ref{sec:more_part_mixing}), text-guided shape generation (Section~\ref{sec:more_text_generation}), and text-guided part completion (Section~\ref{sec:more_text_completion}).
\fi

\subsection{Quantitative Results of Part Mixing and Refinement}
\label{sec:supp_part_mixing}
\paragraph{Experiment Setup.}

To demonstrate the refinement capability of \salad{}, we conduct quantitative comparisons between the part mixing outputs from SPAGHETTI~\cite{Hertz:2022Spaghetti} and the refined outputs from \salad{}. 
For evaluation, we randomly select 100 pairs of shapes from the training set and swap a semantic part, for all parts that two shapes in a pair have in common. Swapping a part between two shapes results in two mixed shapes for each pair. The numbers of the shapes resulting from part mixing are 606, 670, and 400 for \emph{chair}, \emph{airplane}, and \emph{table} classes, respectively. The mixed shapes are refined using the method introduced in Section~\ref{sec:shape_completion}~\refofpaper{} with diffusion timestep $t=10$. We evaluate the same metrics used in Section~\ref{sec:shape_generation}~\refinpaper{} using the test set provided by Chen~\etal~\cite{Chen:2019ImNet} and report the results in Table~\ref{tbl:quantitative_mixing}.

\paragraph{Results.}
As indicated in the metrics reported in Table~\ref{tbl:quantitative_mixing}, the quality of mixed shapes are further improved after the refinement step. We particularly observe noticeable gaps in 1-NNA across all shape classes. More \emph{qualitative} results are reported in Section~\ref{sec:more_part_mixing}.
\input{tables/part_mixing}









\subsection{SALAD Implementation Details}
\label{sec:supp_salad_implementation_details}
As discussed in Section~\ref{sec:background_part_representation}~\refofpaper{}, an extrinsic vector $\B{e}_i$ is represented by $\{\mathbf{c}_i, \lambda_i^1, \lambda_i^2, \lambda_i^3, \mathbf{u}_i^1, \mathbf{u}_i^2, \mathbf{u}_i^3, \pi_i\}$, where the eigenvectors $\{\mathbf{u}_i^j\}_{j=1}^{3}$ must be orthogonal to each other. Therefore, the diffusion processes for $\{\B{e}_i\}_{i=1}^N$ need to model distributions in a product space of an orthogonal group $\text{O}(3)$ and Euclidean group, not in the Euclidean space. Recent work~\cite{Leach:2022SO3Diff,Bortoli:2022Riemannian} introduce diffusion models on Lie group or its product space, however, we empirically find that learning diffusion without considering the orthogonality also performs well. It is ensured only at the test time by taking the projection of the generated eigenvectors $\mathbf{U}_i = [\mathbf{u}_i^1, \mathbf{u}_i^2, \mathbf{u}_i^3]$ to $\text{O}(3)$ space. We follow Sch{\"o}nemann~\cite{Schonemann:1966Procrustes} and project $\mathbf{U}_i$ as
\begin{align}
\label{eq:eigvec_projection}
    \tilde{\mathbf{U}}_i = [\tilde{\mathbf{u}}_i^1, \tilde{\mathbf{u}}_i^2, \tilde{\mathbf{u}}_i^3] = \mathbf{A}\mathbf{B}^l,
\end{align}
where $\mathbf{U}_i = \mathbf{A} \boldsymbol{\Sigma} \mathbf{B}^T$ is a singular value decomposition of $\mathbf{U}_i$.
We also clip negative eigenvalues in $\{\lambda_i^j\}_{j=1}^{3}$ to \num{1e-4} since the covariance matrix is positive-definite.

We normalize elements of $\mathbf{e}_i$ to avoid arbitrary high-variance latent space. Specifically, during the training of ``Diffusion of $\{\B{e}_i\}_{i=1}^N$'', we normalize $\pi_i$ and $\{\lambda_i^j\}_{j=1}^3$ using element-wise means and standard deviations pre-computed from all training data. At test time, we re-scale these elements by the means and the standard deviations. We do not apply normalization to the others.

\sisetup{group-separator={,}}
The Transformer-based network of \salad{} introduced in Section~\ref{sec:method}~\refofpaper{} consists of an embedding layer, which maps an input to 512-dimensional embeddings, and 6 Transformer blocks. Each Transformer block is a stack of a self-attention block and an MLP, each of which is followed by an AdaLN layer. We set the dimension of the output of the positional encoding $\gamma(\cdot)$ to 128.

As \salad{} consists of two diffusion models, each trained for \num{5000} epochs, we train the baselines for \num{10000} epochs for a fair comparison. We use a batch size of 64 and an initial learning rate $10^{-4}$ with a polynomial decaying scheduler (power=0.999). The diffusion process is configured with $T=\num{1000}$, $\beta^{(1)}=10^{-4}$, and $\beta^{(T)}=0.05$.


\subsection{Experiment Details}
\label{sec:supp_experimental_details}
In this section, we provide details of the experiments whose results are reported in the main paper.

\subsubsection{Details on Part Completion Experiment Setup --- Section~\ref{sec:shape_completion}}
\label{sec:supp_part_completion_details}
 As mentioned in Section~\ref{sec:shape_completion}~\refofpaper{}, part completion via a \emph{guided} reverse process~\cite{Meng:2022SDEdit} requires binary masks indicating the parts to be ablated. We describe how such masks are constructed for \salad{} and Neural Wavelet~\cite{Hui:2022NeuralWavelet} in this section.

\paragraph{SALAD.}
We define a binary mask $m \in \{0,1\}^N$ for pairs $\{(\mathbf{e}_i, \mathbf{s}_i)\}_{i=1}^N$ to have value \num{0} at completed parts, \num{1} otherwise. To this end, we first \emph{transfer} the part labels of the annotated point clouds from ShapeNet~\cite{Chang:2015Shapenet} dataset to each $(\mathbf{e}_i, \mathbf{s}_i)$. Assume a point cloud $\{ (\mathbf{x}_j, l_j) \}_{j=1}^{K}$ of $K$ points where $\mathbf{x}_j \in \mathbb{R}^3$ and $l_j \in \{1,2,\dots,L\}$, denote 3D coordinate and part label of $j$-th point, respectively. Each $(\mathbf{e}_i, \mathbf{s}_i)$ is assigned a part label $l_i \in \{1,2,\dots,L\}$ based on the proximity of $\mathbf{e}_i$ to the points $\{\mathbf{x}_j\}_{j=1}^K$. Since $\mathbf{e}_i$ parameterizes a Gaussian distribution in 3D space, we employ Mahalanobis distance~\cite{Mahalanobis:1936MahalanobisDistance} as a distance measure. For each Gaussian represented by $\mathbf{e}_i$, we compute the distance to every point $\mathbf{x}_j$ and select the closest \num{100} points. We then count the number of part label occurrences over the points and assign the most frequently occurred label to the pair.

Having assigned the part labels to each of $\{(\mathbf{e}_i, \mathbf{s}_i)\}_{i=1}^N$, we define a mask $m$ selecting a part whose label is $l$ as
\begin{align}
    m_i &= \begin{cases}
        0 & \text{ if } l_i = l \\
        1 & \text{ otherwise} \\
    \end{cases},
\end{align}
where $m_i$ denotes the $i$-th element of $m$.

\paragraph{Neural Wavelet~\cite{Hui:2022NeuralWavelet}.}
Note that there is neither a publicly available official code nor detailed instructions for shape manipulation using Neural Wavelet~\cite{Hui:2022NeuralWavelet}. Although a concurrent work of ours, Hu~\etal~\cite{Hu:2023NeuralWavelet}, demonstrates shape manipulation using Neural Wavelet, it does not provide a detailed implementation.

Following Hui~\etal~\cite{Hui:2022NeuralWavelet}, we derive the wavelet coefficients of the shapes in our training set. We compute signed distance functions (SDFs) of the shapes and truncate their values into $[-0.1, 0.1]$. We denote $S$ the resulting truncated signed distance function (TSDF) of a shape.
We leverage Biorthogonal wavelet-6-8 filter~\cite{Cohen1993:Biorthogonal} to decompose $S$ into a coarse wavelet coefficient volume at a scale 3 ($C^3$) and a detail wavelet coefficient volume at a scale 2 ($D^2$). Refer to Hui~\etal~\cite{Hui:2022NeuralWavelet} for details on preprocessing.

We then aim to derive binary masks for $C^3$, necessary for leveraging pre-trained Neural Wavelet~\cite{Hui:2022NeuralWavelet} for part completion.
Note that selecting a part to complete is a \emph{nontrivial} task for a voxel-based representation adapted by Neural Wavelet, as opposed to \salad{} where we can define binary masks for $\{(\mathbf{e}_i, \mathbf{s}_i)\}_{i=1}^N$ to select parts directly. As one solution, we compute bounding boxes enclosing semantic parts of 3D shapes, and use them to designate the \emph{regions} to complete.
Such bounding boxes are used to compute binary masks for $C^3$ via a heuristic based on the property of wavelet transforms extracting local spectral information.
Through experiments, we empirically find a set of wavelet coefficients that vary when the TSDF values in a 3D volume are set to \num{0.1} (\ie outside of a shape). For instance, we set the TSDF values in the bounding box enclosing the back of a chair to \num{0.1} to discover a set of wavelet coefficients corresponding to the part. We assign \num{0} to the coefficients whose amount of change is above a threshold $\delta$ and \num{1} to the others.

Rigorously, let $M \in \{0,1\}^{256^{3}}$ denote a binary voxel grid of the same resolution as $S$ with 0 indicating the semantic part of interest and 1 otherwise. Such $M$ is derived from a bounding box enclosing a semantic part of a 3D shape, and is used to derive a \emph{masked} TSDF $S^*$ defined as
\begin{equation}
S_v^* = \begin{cases}
   0.1 & \text{ if } M_v = 0  \\ 
   S_v & \text{ otherwise}
\end{cases},
\label{eq:tsdf_mask}
\end{equation}
for all $v \in \{(0,0,0), (0,0,1), ..., (255,255,255)\}$. After marking all values inside a bounding box as \emph{outside}, we obtain the wavelet coefficients $C^{3*}$ via forward wavelet transform. A mask $m$ for $C^{3}$ is then defined as
\begin{equation}
m_{v^{\prime}} =
\begin{cases}
     0 & \text{ if } |C^{3*}_{v^{\prime}} - C^3_{v^{\prime}}| > \delta\\ 
     1 & \text{ otherwise}
\end{cases}
\label{eq:wavelet_mask}.
\end{equation}
for all $v^\prime \in \{ (0,0,0), (0,0,1), ..., (47,47,47)\}$. Here, we use $\delta=0.001$.

\paragraph{ShapeFormer~\cite{Yan:2022ShapeFormer}.}
As discussed in Section~\ref{sec:shape_completion}~\refofpaper{}, after constructing the axis-aligned bounding box of a part, we make a partial point cloud by masking out the points inside the bounding box, and pass it to ShapeFormer~\cite{Yan:2022ShapeFormer} as an input.

\subsubsection{Details on Text-Guided Shape Generation --- Section~\ref{sec:text_conditional_generation}}
\label{sec:supp_text_generation_details}
\paragraph{Implementation Details of Text-Conditioned SALAD.}
We impose text conditions on both the first and the second phase models by feeding text features from our text encoder. We use LSTM~\cite{Hochreiter:1997lstm} for the text encoder and train it jointly with the first and the second phase models. We also apply the classifier-free guidance~\cite{Ho:2021classifierfree}. More precisely, we jointly train a conditional diffusion model $\Veps_\theta(\B{x}^{(t)},t,\B{c})$ and an unconditional diffusion model $\Veps_\theta(\B{x}^{(t)},t,\boldsymbol{\emptyset})$, where $\B{c}$ denotes a condition feature vector and $\boldsymbol{\emptyset}$ is a null condition vector. We randomly set $\B{c}$ to $\boldsymbol{\emptyset}$ with a 20\% dropout probability during training. To make $\boldsymbol{\emptyset}$, we feed an empty sequence as an input text and zero vectors for $\{\mathcal{E}(\B{e}_i)\}_{i=1}^N$. $\B{c}$ is solely a text feature for the first phase model. For the second phase model conditioned on the features from extrinsic vectors $\{\mathbf{e}_i\}_{i=1}^N$, we use the concatenation of the features and a text feature as a condition. 


At sampling time, the noise prediction is adjusted by an extrapolation between the noise prediction of the conditional diffusion model and the unconditional diffusion model as follows:

\begin{equation}
    \tilde{\Veps}_t = (1+w)\Veps_\theta(\B{x}^{(t)},t,\B{c})-w \Veps_\theta(\B{x}^{(t)}, t, \boldsymbol{\emptyset}),
\end{equation}
where $\tilde{\Veps}_t$ is the noise prediction with the classifier-free guidance applied, and $w$ is a hyperparameter controlling guidance strength. We use $w=2$ for sampling.

\paragraph{Experiment Setup.}
To measure Neural-Evaluator-Preference (NEP) discussed in Section~\ref{sec:text_conditional_generation}~\refofpaper{}, we leverage a modified PartGlot~\cite{Koo:2022Partglot} for a neural evaluator. The modified architecture takes point clouds as inputs instead of super-segments. Refer to the PartGlot~\cite{Koo:2022Partglot} paper for more details. 
We adapt the training and test set of PartGlot~\cite{Koo:2022Partglot} to create binary classification examples.
The modified PartGlot achieves $73.98$\% test accuracy on the binary classification. Following Mittal~\etal~\cite{Mittal:2022Autosdf}, we consider an example to be confused if the absolute difference between the neural evaluator's confidence is $\leq 0.2$.

{
\begin{figure}[h!]
\centering
\begin{minipage}{.5\textwidth}
\includegraphics[width=1.\linewidth]{figures/gaussglot_qualitative.pdf}
\caption{\textbf{\gaussglot{} qualitative results.} The attention maps for each semantic part achieved by \gaussglot{} are shown in the left columns of the figure. The colors of the attention maps change from dark blue to yellow as the attention weights increase from 0 to 1. The final part segmentation results are depicted in the rightmost column of the figure, where purple, blue, green, and yellow indicate \emph{back}, \emph{seat}, \emph{leg}, and \emph{arm}, respectively.}


\label{fig:gaussglot_qualitative}
\end{minipage}
\end{figure}
}

\subsubsection{Details on GaussGlot --- Section~\ref{sec:text_driven_manipulation}}
\label{sec:supp_gaussglot_details}
Inspired by Koo~\etal~\cite{Koo:2022Partglot}, we design a text-driven self-supervised semantic part segmentation network, \gaussglot{}, where a set of Gaussian primitives is employed as super-segments. As discussed in Section~\ref{sec:text_conditional_generation}~\refofpaper{}, PartGlot is a neural evaluator that classifies shapes from a query text. While solving this text-conditioned shape classification, PartGlot learns semantic part segmentation in an unsupervised manner by learning the attention maps between the input text and the super-segments. Refer to the PartGlot~\cite{Koo:2022Partglot} paper for more details.
Specifically, we train \gaussglot{} with $\{\B{e}_i\}_{i=1}^N$ excluding $\pi_i$ elements which is inessential to define 3D Gaussian primitives. Based on the architecture of PartGlot, 15-dimensional Gaussian parameters are mapped to 256-dimensional features through MLPs. We embed text tokens into 128 dimensions and use LSTM as a text encoder with 256-dimensional hidden states. Our trained \gaussglot{} achieves \num{76.03}\% test accuracy and $56.85$\% mIoU. Qualitative part segmentation examples and the attention maps of each semantic part from \gaussglot{} can be found in Figure~\ref{fig:gaussglot_qualitative}. 






\ifpaper
\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figures/shape_generation/fig1_gallery_v4.png}
\caption{\textbf{A visual gallery of \emph{airplanes}, \emph{chairs}, and \emph{tables} generated by~\salad{}.}}
\label{fig:generation_gallery}
\end{figure}
\clearpage
\newpage
\else
\fi 

\clearpage
\newpage
\onecolumn

\subsection{More Qualitative Comparisons on Shape Generation}
\label{sec:more_shape_generation_comparison}

In the following, we provide more qualitative comparisons on shape generation with \emph{chair} and \emph{airplane} classes, as shown in Figure~\ref{fig:shape_generation_qualitative_results}~\refofpaper{}. 
\input{figures/shape_generation/supp_shape_generation_comparison}



\clearpage
\newpage

\subsection{More Qualitative Comparisons on Part Completion}
\label{sec:more_part_completion_comparison}
We report more qualitative comparisons on part completion with \emph{chair} and \emph{airplane} classes, as shown in Figure~\ref{fig:shape_completion}~\refinpaper{}. 

\input{figures/part_completion/supp_part_completion}


\clearpage
\newpage
\subsection{More Qualitative Results on Part Mixing and Refinement}
\label{sec:more_part_mixing}
We report more qualitative results on part mixing and refinement with \emph{chair}, \emph{airplane} and \emph{table} classes, as shown in Figure~\ref{fig:part_mixing}~\refinpaper{}.

\input{figures/part_mixing/supp_part_mixing}

\clearpage
\newpage

\subsection{More Qualitative Comparisons on Text-Guided Shape Generation}
\label{sec:more_text_generation}
We report more qualitative comparisons on text-guided shape generation between AutoSDF~\cite{Mittal:2022Autosdf} and \salad{}.

\input{figures/text_generation/supp_text_generation}

\clearpage
\newpage
\onecolumn
\subsection{More Qualitative Results on Text-Guided Part Completion}
\label{sec:more_text_completion}
We report more qualitative results on text-guided part completion leveraging \salad{} and \gaussglot{}. In the figure below, the parts selected by \gaussglot{} from the text are highlighted by red. Text-conditioned \salad{} completes the selected parts to match the text via the guided reverse process.

\input{figures/text_completion/supp_text_completion}

\clearpage
\newpage
\twocolumn
