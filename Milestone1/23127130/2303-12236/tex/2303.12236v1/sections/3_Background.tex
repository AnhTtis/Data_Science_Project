\vspace{-5pt}
\section{Diffusion Models and Part-Level Shape Representation}
\label{sec:background}

\subsection{Background on Diffusion Models}
\label{sec:background_ddpm}
We first briefly overview the technical background of diffusion models.
Diffusion models~\cite{Ho:2019DDPM} are latent variable models that approximate a data distribution $q (\mathbf{x}^{(0)})$ with a Markov chain, which is also called a \emph{reverse process}:
\vspace{-2pt}
\begin{align}
    p_{\theta} (\mathbf{x}^{(0)}) \coloneqq \int p_{\theta} (\mathbf{x}^{(0:T)}) d \mathbf{x}^{(1:T)},
\end{align}
where $p_{\theta} (\mathbf{x}^{(0:T)}) = p (\mathbf{x}^{(T)}) \Pi_{t=1}^{T} p_{\theta} (\mathbf{x}^{(t-1)} \vert \mathbf{x}^{(t)})$.
Here, $p (\mathbf{x}^{(T)}) = \mathcal{N} (\mathbf{x}^{(T)}; \mathbf{0}, \mathbf{I})$ is the standard normal prior
enabling tractable sampling.

The conditional probabilities $\{ p_{\theta} (\mathbf{x}^{(t-1)} \vert \mathbf{x}^{(t)}) \}_{t=1}^{T}$ are parameterized by a neural network whose weights are denoted by $\theta$.
The weights are optimized through the \emph{forward} diffusion process $q (\mathbf{x}^{(1:t)} \vert \mathbf{x}^{(0)})$ that sequentially adds Gaussian noises to the data $\mathbf{x}^{(0)} \sim q (\mathbf{x}^{(0)})$:
\begin{align}
\begin{gathered}
    q (\mathbf{x}^{(1:t)} \vert \mathbf{x}^{(0)}) \coloneqq \Pi_{s=1}^{t} q (\mathbf{x}^{(s)} \vert \mathbf{x}^{(s-1)}), \\
    \text{where} \,\, q (\mathbf{x}^{(s)} \vert \mathbf{x}^{(s-1)}) \coloneqq \mathcal{N} \left(\mathbf{x}^{(s)}; \sqrt{1 - \beta^{(s)}} \mathbf{x}^{(s-1)}, \beta^{(s)} \mathbf{I} \right),
    \raisetag{35pt}
\end{gathered}
\end{align}
and $\beta^{(s)}$ is an element of a monotonically increasing sequence $\beta^{(1:T)} \in (0, 1]^{T}$.
By choosing Gaussians as forward diffusion kernels, the conditional densities $q (\mathbf{x}^{(t)} \vert \mathbf{x}^{(0)})$ at $t=1,\dots,T$ can be expressed in the closed form:
\begin{align}
    q (\mathbf{x}^{(t)} \vert \mathbf{x}^{(0)}) = \mathcal{N} (\mathbf{x}^{(t)}; \sqrt{\bar{\alpha}^{(t)}} \mathbf{x}^{(0)}, (1 - \bar{\alpha}^{(t)}) \mathbf{I}),
\end{align}
where $\alpha^{(t)} \coloneqq 1 - \beta^{(t)}$ and $\bar{\alpha}^{(t)} \coloneqq \Pi_{s=1}^{t} \alpha^{(s)}$.
Over the forward process dissipating a sample $\mathbf{x}^{(0)} \sim q (\mathbf{x}^{(0)})$ toward $q(\mathbf{x}^{(T)}) = \mathcal{N} (\mathbf{0}, \mathbf{I})$, the weights $\theta$ parameterizing the reverse process $p_{\theta} (\mathbf{x}^{(0)})$ are learned by optimizing the following variational bound on negative log likelihood:
\begin{equation}
\begin{aligned}
    \mathbb{E}_{q (\mathbf{x}^{(0)})} & [-\log p_{\theta} (\mathbf{x}^{(0)})] \leq \\
    &\mathbb{E}_{q (\mathbf{x}^{(0)}, \dots, \mathbf{x}^{(T)})} \left[-\log \frac{p_{\theta} (\mathbf{x}^{(0:T)})}{q (\mathbf{x}^{(1:T)} \vert \mathbf{x}^{(0)})}\right].
\end{aligned}
\end{equation}
Following Ho \etal~\cite{Ho:2019DDPM}, we parameterize our reverse process $p_{\theta} (\mathbf{x}^{(t-1)} \vert \mathbf{x}^{(t)})$ as:
\begin{equation}
\begin{aligned}
    p_{\theta} (\mathbf{x}^{(t-1)} \vert \mathbf{x}^{(t)}) \coloneqq \mathcal{N} (\mathbf{x}^{(t-1)}; \boldsymbol{\mu}_{\theta} (\mathbf{x}^{(t)}, t), \beta^{(t)} \mathbf{I}).
\end{aligned}
\end{equation}
In particular, we use the parameterization $\boldsymbol{\mu}_{\theta} (\mathbf{x}^{(t)}, t) = \sfrac{1}{\sqrt{\alpha^{(t)}}} (\mathbf{x}^{(t)} - \sfrac{\beta^{(t)}}{\sqrt{1 - \bar{\alpha}^{(t)}}} \boldsymbol{\epsilon}_{\theta} (\mathbf{x}^{(t)}, t))$ and optimize its parameters $\theta$ with a training objective that encourages a network $\boldsymbol{\epsilon}_{\theta}$ to predict the noise $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ present in the given data:
\begin{align}
    \mathcal{L}(\theta) \coloneqq \mathbb{E}_{t, \mathbf{x}^{(0)}, \boldsymbol{\epsilon}} \left[\left\lVert \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_{\theta} \left(\sqrt{\bar{\alpha}^{(t)}} \mathbf{x}^{(0)} + \sqrt{1 - \bar{\alpha}^{(t)}}\boldsymbol{\epsilon}, t\right) \right\rVert^{2}\right].
    \label{eq:ddpm_loss}
\end{align}







\begin{figure}[t!]
\label{fig:spaghetti_overview}
\includegraphics[width=\linewidth]{figures/pipeline2_draft.pdf}
\caption{\textbf{Part-Level implicit representation by Hertz~\etal~\cite{Hertz:2022Spaghetti}.} A latent vector $\mathbf{z}$ encoding global geometry is first mapped to a set of part latents $\{\mathbf{p}_i\}_{i=1}^N$, each of which is decomposed into extrinsic parameters $\{\mathbf{e}_i\}_{i=1}^N$ and intrinsic latents $\{\mathbf{s}_i\}_{i=1}^N$. The decoder, conditioned on $\{(\mathbf{e}_i, \mathbf{s}_i)\}_{i=1}$, outputs an occupancy value given a query point $\mathbf{x}$.}
\vspace{-10pt}
\end{figure}

\vspace{-15pt}
\subsection{Part-Level Shape Representation}
\label{sec:background_part_representation}
Neural implicit representations~\cite{Chen:2019ImNet, Park:2019Deepsdf, Mescheder:2019OccNet} have been widely exploited in 3D shape generation and reconstruction due to their advantages in capturing fine details without limitation in resolutions even with a small memory footprint. However, their disadvantage of not supporting intuitive editing and manipulation has been a hindrance to increasing their utilization. To remedy the drawback, recent works~\cite{Genova:2019LearningShapeTemplates,Genova:2020LDIF,Hao:2020Dualsdf,Hui:2022NeuralTemplate,Hertz:2022Spaghetti} introduced \emph{dual} representations combining explicit and implicit representations, taking advantage of both of them. Among them, Hertz~\etal~\cite{Hertz:2022Spaghetti}, which our work is based on, was the first introducing a hybrid representation integrating two types of disentanglements simultaneously into an implicit representation: 1) part-level disentanglement, representing each local region separately, and 2) extrinsic-intrinsic disentanglement, describing extrinsic properties (\ie~the approximate shape and transformations) with parameters in the 3D space while encoding intrinsic properties (\ie~geometric details) using a latent code. This novel representation, called SPAGHETTI~\cite{Hertz:2022Spaghetti}, is learned in an auto-decoding setup without any supervision of the part decomposition.


In SPAGHETTI, a 3D shape is first mapped to a global latent $\mathbf{z}$ and then further encoded into a set of part embedding vectors $\{\Vp_i\}_{i=1}^{N}$, where $N$ denotes the number of parts. Each part embedding vector $\Vp_i$ is again mapped into both a set of extrinsic parameters $\Ve_i$ and an intrinsic latent $\Vs_i$ through an MLP. 
The set of extrinsic parameters $\Ve_i = \{\mathbf{c}_i, \mathbf{\Sigma}_i, \mathbf{\pi}_i \}$ of each part represents a Gaussian in the 3D space with mean $\mathbf{c}_i\in\mathbb{R}^3$ and covariance $\mathbf{\Sigma}_i \in \mathbb{R}^{3 \times 3}$, depicting an approximate shape of a part.
$\pi_i \in \mathbb{R}$ is the blending weight for the Gaussian mixture representation of the entire shape: $\sum_{i}\pi_i \mathcal{N}(\B{x} | \B{c}_i, \mathbf{\Sigma}_i)$, describing the volume of the shape as a probability distribution. Since $\{\Ve_i\}_{i=1}^{N}$ can only encode the part-level structural information, the intrinsic latents $\{\Vs_i\}_{i=1}^{N}$ supplement the detailed geometry information so that the pairs of the extrinsic parameters and intrinsic latents can be decoded back to the original shape in an implicit form. Specifically, an implicit decoder $\mathcal{D}$ is trained to predict an occupancy value at point $\mathbf{x}$:
\vspace{-0.5\baselineskip}
\begin{equation}
\begin{aligned}
\label{eq:decoder}
    o = \mathcal{D}\left(\mathbf{x}\, \Big\vert \, \{\Ve_i\}_{i=1}^{N},\,\{\Vs_i\}_{i=1}^{N}\right),
\end{aligned}
\end{equation}
where occupancy value $o \in [0,1]$ is 1 when the query point is inside the shape, and 0 otherwise. 
The keys to achieving both the part-level and extrinsic-intrinsic disentanglements in the training of decoder $\mathcal{D}$ are the regularizations forcing a single pair $(\Ve_i, \Vs_i)$ of a part to determine the occupancy of each point, and the Gaussian parameters in $\Ve_i$ to transform the corresponding local region. See the original paper~\cite{Hertz:2022Spaghetti} for the details of the decoder training.

The extrinsic vector $\mathbf{e}_i$ is precisely represented as a $16$-dimensional vector $\{\mathbf{c}_i, \lambda_i^1, \lambda_i^2, \lambda_i^3, \mathbf{u}_i^1, \mathbf{u}_i^2, \mathbf{u}_i^3, \pi_i\}$, where $\lambda_i^j \in \mathbb{R}$ and $\mathbf{u}_i^j \in \mathbb{R}^3$ are eigenvalues and eigenvectors of the covariance matrix $\mathbf{\Sigma}_i$, while the intrinsic vector $\mathbf{s}_i$ is a 512-dimensional vector. Note that the much smaller extrinsic vector contains the approximate shape information of the part; we leverage this fact in our effective cascaded diffusion model.

Also, note that SPAGHETTI is trained in an auto-decoding setup while regularizing the global latent code $\mathbf{z} \in \mathbb{R}^{512}$ to follow the unit Gaussian. Thus, the shapes can be simply generated by sampling a latent code $\mathbf{z}$ from the unit Gaussian in the $\mathbf{z}$ space, although we demonstrate that diffusion in the extrinsic and intrinsic embedding spaces can produce much more plausible shapes (Section~\ref{sec:shape_generation}).


 