\section{Related Work}
\label{sec:Related}

\paragraph{\textbf{Parallel and Distributed Dataframe Libraries}} Most previous
work on optimizing dataframe libraries falls under the use of parallel and
distributed execution. Our technique is orthogonal to these
techniques. First, to the best of our knowledge, no other system has used
rewriting at the interface boundary of Python and \code{pandas}. Systems like
\code{modin} \cite{modin}, \code{dask} \cite{dask}, Koalas \cite{koalas_web},
PolaRS \cite{polars_web}, Ponder \cite{ponder_web}, PolyFrame \cite{polyframe}
and Magpie \cite{magpie} are all essentially custom versions of \code{pandas}
(some are full rewrites, while others implement the \code{pandas} API over some
underlying system). They use many different techniques, like the use of parallel
execution using anything from Rust threads to engines like Ray \cite{ray} and
Spark \cite{spark}, partition schemes, query optimization and the use of the
hard disk. But none uses rewriting as we do, and all of these techniques are
performed \textit{within} the library. This is the main conceptual difference,
but there are also other practical drawbacks as we outlined in the previous
sections, mainly arising from the fact that these systems do not focus on
single-machine, ad-hoc, diverse use cases.

\paragraph{\textbf{Optimizing Dataframe Libraries for Interactive Settings}} A
slightly different and interesting line of work focuses on optimizing dataframe
queries for interactive workloads \cite{pandas_interactive, lux}. Some of their
optimizations include displaying partial results (e.g., applying \code{head()}
on an expression), reordering operations and performing computation during
\emph{think-time}, i.e., when the user is inspecting results. We also recognize
the importance of interactive workloads, which include the EDA, single-machine,
ad-hoc workloads we focus on in this paper, but we are taking a different path
in optimizing them. We use rewriting at the interface boundary, which
is fundamentally different from the techniques used in this previous work.

\paragraph{\textbf{Rewrite systems in compilers}} Program rewriting is prevalent in compilers. Production-level compilers use peephole optimizers to perform local rewrites. LLVM~\cite{llvm} uses InstCombine \cite{llvm_instcombine} and VectorCombine \cite{llvm_vectorcombine} to perform IR rewrites on scalars and vectors respectively. Further, there have been many works such as Alive~\cite{alive}, Alive2~\cite{alive2}, Souper~\cite{souper} that try to prove or automatically find such rewrites inside traditional compilers. TASO~\cite{taso} and PET~\cite{pet} have looked into how rewrites can be used to optimize tensor computations in tensor compilers. Domain specific languages such as Halide~\cite{halide} include extensive rewrite engines to perform optimizations~\cite{halide-rewrites}. Even complicated optimization passes such as dataflow optimizations~\cite{multiple-rewrites} and vectorization~\cite{vegen} can be expressed as a series of rewrites. In fact, the compiler infrastructure MLIR~\cite{mlir} is rooted on the premise of rewriting to express complex IR transformations. \system{} takes inspiration from these systems that mainly perform static program rewrites and performs rewrites for \code{pandas} implemented in the dynamically-typed Python language.

\paragraph{\textbf{Dynamic Optimization}} There has been a large body of work
that optimizes programs at runtime. Just-in-time (JIT) compilation is one common
technique applied to interpreted languages like Javascript (TraceMonkey
\cite{trace-monkey}, V8 \cite{v8-engine}) and non-native languages like Java
(HotSpot \cite{java-hotspot}). Recently, Python also started to enjoy
significant speedups from optimization at runtime, with the release of the
specializing adaptive interpreter \cite{python-spec-adapt-interp}. All these
methods differ in one key aspect from our method: they optimize the host
language, focusing on low-level optimizations (on each language's bytecode) and
not the host-library combination. On the other hand, our technique can perform
higher-level (i.e., library-level), and more impactful improvements because it
understands the semantics of both the host language and the library.

