\section{Background}
\label{sec:Background}

\subsection{Setting}

In this work, we focus on the broad class of workloads commonly referred to as
\emph{exploratory data analytics} (EDA) \cite{eda_notebooks}. In EDA workloads,
the data is iteratively analyzed for interesting patterns. Since the patterns of
interest are unknown ahead of time, much of this work is done interactively, in
a notebook environment (e.g., Jupyter notebooks, other REPLs) using a dataframe
library. We focus on \texttt{pandas} and related libraries in this work.

In one common setting, analysts are interested in analyzing large datasets,
which typically do not fit in main memory on a single server. In order to
accelerate these workloads, much effort from both industry and academia has gone
towards accelerating \emph{bulk-parallel} workloads.

Frameworks including \code{modin} \cite{modin} and \texttt{dask} \cite{dask} aim to
accelerate such workloads. They operate by providing APIs close to the standard
\texttt{pandas} API, distributing data across servers, and evaluating
functions lazily. When working within these libraries, they can accelerate workloads by
up to 100$\times$ \cite{modin}.

Unfortunately, these bulk-parallel libraries have several drawbacks. In
particular, these libraries were not designed for \emph{ad-hoc, single-machine}
workloads.

\minihead{Ad-hoc operations}
The primary drawback of these libraries is that they have poor support for
ad-hoc operations outside of the library API. For example, operations such as
looping over rows, column-wise operations that require intermediate
materialization for inspection (e.g., comparing a column to a constant), or
inspecting the first $n$ rows can be 30-1900$\times$ \emph{slower} than standard
\texttt{pandas} on a single machine. For example, as we explain in
Section~\ref{sub-sec:cmp-libs}, a simple loop, shown in
Figure~\ref{fig:for_loop}, can be many \textit{hundreds} of times slower.


\minihead{Single-machine overheads}
In addition to slowdowns for ad-hoc operations, these libraries can add
substantial memory overheads. We selected 20 random EDA notebooks from Kaggle
(under criteria described in Section~\ref{sub-sec:exp-setup}), which had
heavy \texttt{pandas} usage. \code{modin} generally increased memory usage, with
the peak memory usage being up to 127$\times$ higher than native
\texttt{pandas}. The peak memory usage increased 4.7$\times$ on average
(geometric mean).


\minihead{Usability}
In discussions with social scientists and law professors at Stanford University
and the University of California, Berkeley, we have found that learning new APIs
is challenging and time-consuming. In particular, these bulk-parallel libraries
are not direct drop-in replacements. To show this, we sampled 20 notebooks from
Kaggle at random (under criteria described in Section~\ref{sub-sec:exp-setup}).
Five of these notebooks (25\%) were unable to run when \texttt{pandas} was
replaced with \code{modin}.

Furthermore, setting up distributed clusters can be difficult in these settings.
As a result, the distributed speedups are difficult to realize in the settings
we focus on.


% \charith{I would restructure this section to be; there have been many efforts at automatically optimizing \code{pandas} workloads; Give examples and their use cases (positives first); Then give their drawbacks.}
% \charith{No need to give a section overview at the beginning for this section.}
% 
% As we have described in Section \ref{sec:Intro}, \code{pandas} workloads are becoming increasingly prevalent. We describe salient properties \stef{We don't do it and we haven't done such analysis} of \code{pandas} workloads and describe deficiencies with prior systems that aim to optimize such workloads. We then present rewriting as an optimization technique for \code{pandas} workloads that avoids these deficiencies.
% 
% Due to the importance of \code{pandas} workloads, academic labs and corporations have developed systems ranging from Modin \cite{modin} to Dask \cite{dask} to accelerate such workloads. Their primary feature is parallelization across cores to improve vanilla \code{pandas} which is single-threaded. However, these systems are specialized for \stef{(I don't know what they specialize for; On my PC Modin is slower and it takes more memory that vanilla \code{pandas}. Get numbers vs vanilla \code{pandas} on the beefy machine; maybe they specialize on very powerful machines)}. \ddkang{As a result, they suffer on [description] workloads}. We highlight three such issues below:
% 
% \paragraph{\textbf{Slowdowns}} Although \ddkang{[blah]} systems can provide dramatic speedups for \ddkang{[blah]} scenarios, they can slow down workloads by up to 4$\times$ \fTBD{approximate but conservative; come back when we have exact numbers}. We have found that these slowdowns occur in \ddkang{[EXPLAIN WHERE]}. We hypothesize that \ddkang{[HYPOTHESIS HERE]}.
% 
% \paragraph{\textbf{Incomplete API Support}}  
% \ddkang{In addition to slowdowns, prior systems also do not serve as a drop-in replacement. Although surprising, we have found that prior systems fail to support up to 25\% of [workload] notebooks. To show this, we ...}
% 
% Another problem of previous techniques is that they do not support 100\% of the \code{pandas} API.
% The parts not covered are not rare. We picked randomly 20 notebooks from Kaggle and we ran them with Modin. In 5 notebooks, this resulted in failures and so we had to disable or rewrite cells (note that Modin covers much more \cite{modin} of the \code{pandas} API than Dask and so Dask would probably do even worse).
% \ddkang{Do not hypothesize like this. Speculation of this form does not belong in academic papers. Either run dask or drop it}
% This hinders productivity as the user's code may fail and it can be unclear whether it is the user's error or the framework's.
% 
% \paragraph{\textbf{Memory Consumption}} \ddkang{Finally, these prior systems have high memory overheads. In fact, they require much more memory than a reasonable consumer machine has available. To test this, we ...}
% 
% Lastly, previous techniques consume a lot of memory; much more than a reasonable consumer machine has available. For example, we ran Modin over 20 randomly-picked \stef{This is misleading. It makes me think "ran over the original dataset". But we replicated data. So, more accurate: "20 randonly picked notebooks ran over datasets of up to 2GB".} Kaggle notebooks on a machine with 32GB of RAM. This resulted in out-of-memory errors and so we had to extend the swap space by \stef{add it} GB. This was not required to run the vanilla version of \code{pandas}.
% \ddkang{Why is the next sentence needed?} \stef{Because if \code{pandas} makes great use of memory, it's fine if system X uses a little more to provide some benefit (e.g., speedup) (standard argument in mem allocators). But if \code{pandas} is already pretty bad, being worse is not reasonable. That said, you might though want experiments instead of what someone said, even if it's the creator of \code{pandas}.}
% Note that \code{pandas} does not excel in memory consumption; according to the creator \code{pandas} requires 5 to 10 times as much RAM as the size of the dataset \cite{wes_mckinney_pandas_mem}.

% \vspace{0.5em}

\hfill{}

In this paper, we introduce program rewriting as an automatic optimization
technique for Python code that interfaces with \texttt{pandas}, focusing on
accelerating single-server, ad-hoc EDA workloads.

\subsection{Rewriting as an alternative optimization}

% why rewriting is better than other techniques - 1 paragraph
Rewriting, for optimization purposes, is the process of replacing some part of code with a functionally equivalent but faster version.
%As an example, consider the task of selecting the 5 smallest elements of a DataFrame column. Figure \ref{fig:sort_values} shows user-written code, extracted from a Kaggle notebook, that performs this task. The same code can be rewritten as in Figure \ref{fig:nsmallest}. Figure \ref{fig:sort_values} first sorts the values and then selects the first 5 elements whereas the code in Figure \ref{fig:nsmallest} uses the \code{pandas} function \code{nsmallest}, which selects the \code{n} smallest elements directly. As we can expect, Figure \ref{fig:nsmallest} is faster. Similarly, there are many opportunities in real-world notebooks to perform rewrites into more optimized versions. %We describe such rewrite opportunities in detail in Section~\ref{}.
Rewriting avoids the previously mentioned drawbacks of library-based optimization systems.
First, it inherently does not suffer from a lack of API support because it is not a replacement
for \code{pandas} and it can %always
leave the code untouched if it cannot handle it.
Second, rewriting is a lightweight technique incurring minimal overheads, which %in fact
scale proportionally only to the code, not the data.

\input{figures/concat_with_lists.tex}

Additionally, there are fundamental advantages \system{} has over library-based
optimization approaches. The rewrite system is transparent. When the user
observes a speedup, they can always see the code that the rewriter used. In
other words, the user does not need to understand the system to understand the
cause of the speedup. At the same time, the user's code remains intact. Further,
rewriting has the benefit of being able to optimize across library boundaries.
For example, \system{} can automatically perform the rewrite in
Figure~\ref{fig:concat-with-lists} (taken from a real-world notebook). The
original code crosses the library boundaries (twice!) as we move from
\code{pandas} to Python (by converting to a list) and then back to
\code{pandas}. To perform this rewrite, a tool needs to view all the code and
understand semantic equivalences and differences across library boundaries
(e.g., \code{pandas} and the host language, Python). This is not possible with
optimization approaches that purely aim at accelerating the \code{pandas} API.

\input{figures/split.tex}

% why an automated system is needed? - 1 paragraph
Rewriting appears simple, but it can be challenging when performed manually. There are many non-obvious rewrites that the user may not be able to discover easily. For example, it might seem that the only way to make \code{pandas} code faster through rewriting is by replacing it with other \code{pandas} code, or using a similar library such as \code{numpy}. This has been reinforced over years of data scientists being trained to remain within \code{pandas}/\code{numpy} as much as possible because these use native, vectorized implementations and are thus deemed to be much faster than pure Python. It might, then, be surprising that moving out of \code{pandas} and into pure Python can lead to significant speedups. One example is shown in Figure \ref{fig:split}. The task here is to split a \code{Series} of strings by the delimiter \code{'('}. The code in Figure~\ref{fig:split_pandas} (extracted from a Kaggle notebook) does it by using a \code{pandas}-provided function. One would expect that this is the best way to perform this operation. Nevertheless, the version in Figure \ref{fig:split_python} is 3.5$\times$ faster. %What this code does is seemingly completely unorthodox. 
It moves from \code{pandas} to pure Python (by converting \code{df['C']} to a Python list) and performs the operation with a sequential Python loop %\footnote{It then converts the results to \code{pandas} Series for functional equivalence with the original. Note that it has to have the same index; more on that later.} 
(in our case studies in Section~\ref{sub-sec:ablation}, we explain why this version is faster). %We detail some of the rewrite rules we use in Section~\ref{sec:pandas_rewr_rules} including these complex rules.

It is unreasonable to expect general \code{pandas} users to comprehend Python, \code{pandas}, and \code{numpy} to such an extensive level to be able to discover such equivalent versions and evaluate their relative performance. Second, even if the user succeeds in these tasks, the rewritten version can be significantly harder to write and read, as is evident from Figure \ref{fig:split}. This can further lead to correctness concerns about the rewrite. Third, manual rewriting breaks the library abstraction. In the original code of Figure \ref{fig:split}, the user has to think only of \emph{what} \code{split()} does. But, to come up with the rewritten version, this abstraction's veil has to be removed as the user needs to think of \emph{how} to implement it.

%Finally, rewriting becomes more complicated once correctness concerns arise. 
%\input{figures/apply_vectorized.tex}
These issues motivated us to build \system{}, a system that performs such rewrites \textit{automatically}, by guaranteeing \textit{correctness} and with minimal overhead. Section~\ref{sec:System} provides an overview of \system{}. 




%IPython notebooks (see Section \ref{sec:Implementation)}) are populated incrementally and thus the rewriter cannot know all the code. 
%\system{} considers only a single cell at a time \footnote{it could also consider the history, although we have not found that necessary} and needs to save only the AST representation of the code and a few light and ephemeral internal data structures during rewrite code generation. %Further, the rewritten code only have a few dynamic checks. 
%These overheads are thus negligible both in terms of memory and execution time \charith{Do we have numbers to show this overhead is low?}. \stef{Yes. I have to add plots}
%On the other hand, pattern-matching and rewriting reduce to fast tree search and a few dynamic checks.
%These overheads are thus negligible both in terms of memory and execution time.
%Finally, when the rewriter succeeds, the rewritten code is almost always faster than the original, to the extent that there are such patterns. As we will show in Section \ref{sec:Evaluation}, the patterns we have used always result in speedups except for degenerate cases. \stef{Do we need data that the patterns always result in a speedup?}

  %However, a rewriter is naturally such a tool as it views all the user code. 
%However, because the rewriter views, and understands, both the library semantics and also the host language semantics (in this case, Python), it can optimize across library boundaries.
