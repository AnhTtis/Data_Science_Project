\section{Introduction}
\label{sec:Intro}

In recent years, \emph{dataframe}-based libraries, such as \texttt{pandas}, have
become increasingly popular with users ranging from social scientists to
business analysts \cite{pandas_in_business, pandas_in_social}. This growth is driven by many reasons, including
the flexibility of such libraries, the ability to work within a notebook
environment, and the interoperability with other libraries.

Due to the popularity of dataframe libraries, academic and industrial work has
focused on improving the scalability of \texttt{pandas} \emph{in the context of
bulk-parallel operations}. For example, libraries including
\code{modin} \cite{modin}, \code{dask} \cite{dask}, and PySpark \cite{pyspark_web} focus on
parallel or distributed dataframes. Many of these libraries focus on scaling out
\texttt{pandas} across multiple servers, as \texttt{pandas} will fail if the
dataframe does not fit in main memory.
% Many commercial applications require such horizontal scalability.

\input{figures/for_loop.tex}

However, there has been an emerging class of important workloads that operate on
a \emph{single} machine, combined with ad-hoc functions. For example, in our
conversations with law professors at Stanford University, we have found that
provisioning and managing distributed clusters is challenging and time-consuming
for social scientists. As a result, much of the work done by such social
scientists is done on a single machine. Similarly, Kaggle and Google Colab
provide single-machine notebooks for data scientists to explore datasets.
Furthermore, many tasks require custom user-defined functions (UDFs) that are
not well suited to working directly within the \texttt{pandas} API.

While these bulk-parallel dataframe libraries improve the horizontal scalability
of dataframes, as we show in this work, they unfortunately can \emph{fail to
accelerate a wide range of single-machine, ad-hoc workloads}. For example,
\code{modin}, \code{dask}, and PySpark are all 2-200$\times$ slower than \texttt{pandas} for a
range of operations: when interfacing with \texttt{numpy}, looping over
individual rows, and even for simple operations like multiplying two columns (on
a single machine). For example, a simple loop (Figure~\ref{fig:for_loop}) can be
many \textit{hundreds} of times slower (see Section~\ref{sub-sec:cmp-libs}).
Furthermore, all distributed dataframe libraries we are aware of do not maintain
full \texttt{pandas} compatibility, requiring domain experts to learn new
libraries.

We propose an alternative approach to address the \emph{vertical} scalability of
dataframe libraries: \emph{rewriting} notebook cells to accelerate dataframe
computations by utilizing faster, but semantically equivalent code sequences. To
understand the potential for rewriting notebook cells, consider the two cells in
Figure~\ref{fig:apply_only_math}. The first cell is a simplified cell from a
real-world, Kaggle notebook. The second cell is an optimized cell with identical
semantics. While identical semantically, the second cell can run up to
1000$\times$ faster, showing that simple rewrites of notebook cells can
accelerate workloads.

A natural question that emerges is why can't the users write optimized code
themselves. As witnessed in compilers, automatic tools that accelerate code can
reduce developer effort and improve comprehensibility. Furthermore,
severalrewrite rules in this paper were \emph{not apparent to the authors}
(e.g., all the rules in Table~\ref{tbl:rewr_rules}), even after devoting
considerable time studying the internals of Python and \code{pandas}. To
understand how this translates to non-experts, there are whole videos and
articles dedicated to patterns for speeding up \code{pandas} code via manual
rewriting \cite{pygotham_apply_vectorized, pandas_opt_article_docs,
pandas_opt_video_heisler, pandas_opt_video_trabelsi, pandas_opt_article_dan,
pandas_opt_article_numba}. Even then, optimizing code correctly is challenging
for non-expert users and can lead to subtle bugs.

To realize the vision of rewriting notebook cells transparently, we propose
\system{}, a library that automatically rewrites notebook cells via cell
annotations. As we show, \system{} can accelerate notebook cells by up to
57$\times$ completely transparently to the user. 

\input{figures/apply_only_math.tex}

% Data scientists now only need
% to express \emph{what} to compute and the rewriter will decide \emph{how} to
% compute it using the best available Pandas functions and pure python constructs.
% \stef{I'm not sure the previous sentence is a good idea. For one, it reminds too
% much of query optimizers. Also, this paragraph feels a bit redundant.}. We
% believe that this will lead to end user productivity gains as data scientists
% from different domains can iterate through their data processing pipelines
% faster without needing to think about how to make their implementations fast.

In order to rewrite cells, \system{} must overcome several challenges. First, it
must operate within interactive time scales: the overhead of rewriting cannot
dominate cost savings. Second, the rewrites must not change %the semantics of the cells.
program semantics.
This is particularly challenging in the context of a dynamically-typed
language like Python, which has no precise formal semantics and liberal typing
rules. In Python, the types of variables or even classes of \textit{other}
modules can change arbitrarily and there are no standard scoping rules.

We designed \system{}' rewrite engine with two components: a pattern matcher
and a rewriter with design decisions that specifically address the
aforementioned challenges. The rewrite engine is lightweight, with a fast
pattern matcher that can quickly match patterns that we can rewrite into faster
versions and a rewriter which emits, or performs, necessary static and runtime
precondition checks to guarantee correctness, within interactive latencies.

%\system{} is lightweight, with a fast pattern matcher that can quickly match patterns
%that we can rewrite into faster versions and a rewriter which performs this
%rewriting by guaranteeing correctness, within interactive latencies.

We show that on real-world Kaggle notebooks, \system{} can accelerate cells by
up to 57$\times$ (1.18$\times$ geometric mean) and whole notebooks by up to
3.6$\times$ (1.29$\times$ geometric mean). We also compare \system{} with
\code{modin} and show that it can be up to 26.4$\times$ faster for whole
notebooks (4.1$\times$ geometric mean). Furthermore, \system{} can avoid
rewriting cells that cause slowdowns, resulting in overheads that are only due
to the pattern matcher. We show that these overheads, even in the cases where
cells are not rewritten, are below noise thresholds. Finally, \system{} uses
%virtually
no extra memory or disk capacity.

In summary, we make the following contributions.

\begin{enumerate}
    \item We identify program rewriting as a lightweight technique to speed up
    \code{pandas}-heavy EDA workloads. We introduce rewrite rules that can
    significantly speed up \code{pandas} code, including non-trivial ones that
    cross library boundaries.
    %, something that cannot be accomplished by previous techniques
    % (i.e., intra-library optimizations).
    \item We develop \system{} to apply these rewrite rules automatically, at
    runtime. \system{} verifies whether applying a rule is correct by either
    injecting checks in the code or by slicing the execution and performing
    checks in between.
    \item We evaluate \system{} on real-world notebooks and show that it can
    speed up cells by up to 57$\times$ and notebooks by up to 3.6$\times$, with
    almost no memory or disk overheads. We further compare \system{}
    with \code{modin} \cite{modin} and show that it can be up to 26.4$\times$
    faster for whole notebooks (4.1$\times$ geometric mean).
\end{enumerate}

