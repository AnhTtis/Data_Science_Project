\section{Evaluation}
\label{sec:Evaluation}

\subsection{Experimental Setup}
\label{sub-sec:exp-setup}

All the experiments, except if mentioned otherwise, were performed on a system
with a 12-core AMD Ryzen 5900X, 32GB of main memory, Samsung 980 PRO NVMe SSD and Ubuntu
22.04.1 LTS.


\paragraph{\textbf{Benchmark}}

Our goal was to evaluate \system{} on real workloads and so we picked notebooks
from Kaggle. We chose Kaggle as it is a popular repository for data science
workloads and it also contains both the data and notebooks used. The overarching
hypothesis that we want to validate is that a rewrite system like \system{} can
offer substantial speedups on real-world notebooks, through rewriting, with
minimal slowdowns, minimal memory consumption and disk usage, and without
changing the API.

In this work, we focus on ad-hoc EDA, \code{pandas}-heavy workloads. In order to
find such notebooks, we chose notebooks randomly from Kaggle subject to the
following conditions:
\begin{itemize}
    \item At least 50\% of static function calls are \code{pandas} calls
    \item Using datasets of size approximately 2GB or less
\end{itemize}

We chose the first criterion because we focus on EDA notebooks. In particular,
many of the notebooks we excluded focused on machine learning and plotting,
which are out of scope for this work. In the notebooks we picked, we disabled
such code for our evaluation.

Our second criterion was to filter out notebooks that were already
hand-optimized. These notebooks typically operated on large datasets.
Optimization is necessary in this setting as Kaggle has resource constraints
(both computational and memory). However, hand-optimization requires significant
effort. \system{} is an automatic and transparent system and we want to evaluate
its effectiveness without users having to expend that effort.

For the datasets that were significantly lower than 2GB, we replicated them so
that they reach at least several hundred MBs (otherwise our measurements would
be dominated by noise). Also, we modified any notebook that used a sample/subset
of the dataset to instead operate on the full dataset.

% We do remove sampling. See for example:
% dataranch/supermarket-sales-prediction-xgboost-fastai/src/bench.ipynb

% Our second criteria was to filter out notebooks that executed on sampled data or
% were already optimized. Because Kaggle has memory and computation limits, many
% data scientists that leverage Kaggle either first sample the dataset or write
% optimized code to deal with the larger dataset size.

We sampled 20 notebooks satisfying our criteria. There are rewrite opportunities
in 10 of these 20 notebooks, which we coded in \system{}. We focus on these 10
notebooks in our evaluation. We further executed \system{} on the remainder of
the notebooks where no patterns were matched to study \system{}' overhead. We
describe further experiments that include all 20 notebooks in an extended
version of this manuscript \cite{dias_extended}. We compare \system{} with
\code{pandas} (version 1.5.1) and \code{modin} \cite{modin} (version 0.17.0).



% Originally, we picked random notebooks from Kaggle, subject only to the
% condition of being Pandas-heavy. Then, we started finding optimization
% opportunities and coding them in \system{}. However, we observed that for some
% notebooks, there were no optimization opportunities. At the same time, these
% notebooks made advanced use of Pandas, Python and NumPy which did not appear in
% the rest of the notebooks. After analysis, we found out that the distinctive
% characteristic of these notebooks was they were processing large datasets. Large
% here means above a threshold that causes every cell, on average, to be outside
% interactive latencies (which we consider to be around 2s) on a conventional
% consumer machine. This threshold seemed to be around 2GB.
% 
% Our hypothesis is that these notebooks did not start sophisticated. Rather, the
% users wrote advanced code once they observed slow execution times with the
% original code. This is driven by the fact that in the rest of the notebooks,
% users did not write advanced code. Given that there is no correlation between
% user's knowledge on Pandas, Python and NumPy and the datasets they choose to
% process, our hypothesis seems reasonable. Unfortunately, we cannot validate our
% hypothesis because we only have the final versions of the notebooks. We cannot
% know how many rounds of manual rewriting a cell went through. That would require
% a user study that we did not have time to do for this paper.
% 
% So, we discarded notebooks with datasets above 2GB (which are a minority on
% Kaggle) and we evaluated our tool on only the rest. Moreover, for some of the
% notebooks we used, we replicated the datasets if they were too small so that
% they reach up to roughly 2GB. We did this for two reasons. First, if the dataset
% is too small, then we cannot have accurate measurements. Second, we wanted to
% show that a user could write the same non-advanced code that appears in these
% notebooks and still get speedups for free.
% 
% In summary, for our evaluation we picked 20 random notebooks from Kaggle subject
% to two conditions:
% \begin{itemize}
%     \item Pandas-heavy (at least 50\% of static function calls are Pandas calls)
%     \item Using datasets less than around 2GB
% \end{itemize}
% 
% For the datasets that were significantly lower than 2GB, we replicated them so
% that they reach at least several hudred of MBs. \system{} rewrites cells in 10
% out of 20 notebooks. \stef{Not sure if the previous sentence should be here.}
% 
% After picking the notebooks, we searched for optimization opportunities in them
% and on the internet. We coded in \system{} those that could be expressed as
% rewrite rules. Then, we performed the following experiments.


\subsection{End-to-End vs Pandas}
\label{sub-sec:vs-pandas}

\input{figures/cell_level_speedups.tex}
\input{figures/nb_level_speedups.tex}

We first investigated whether \system{} can accelerate cells and notebooks
compared to standard \texttt{pandas}. To do so, we ran each sampled notebook
with and without \system{}. We ran 10 trials each and measured execution time at
the cell level. Our primary metric was the speedup of cells and notebooks with
\system{} compared to standard \texttt{pandas}. We report the geometric mean of
the speedups.

\paragraph{\textbf{Per-Notebook Speedups}} We show per-notebook relative
speedups in Figure \ref{fig:nb_level}. As shown, \system{} can provide
substantial speedups \emph{at the notebook level} of up to 3.6$\times$. Overall,
\system{} provides significant speedups in half of the notebooks (five) and
moderate speedups in one other notebook. We emphasize that these notebooks were
selected randomly from Kaggle, showing the applicability of \system{}.

Furthermore, \system{} does not significantly slow down any notebook, with a
maximum slowdown of 3\%. \system{} rewrites cells in these notebooks but it does
not achieve speedups.


\paragraph{\textbf{Per-Cell Speedups}} We show per-cell speedups in Figure
\ref{fig:cell_level}. For clarity, we excluded cells that run for fewer than
50ms in the original version and excluded all speedups and slowdowns when run
with \system{} within 10\% of the original cell runtime.

As shown, \system{} can achieve per-cell speedups of up to 57$\times$.
The cell with the highest speedup is matched by the pattern shown in
Figure~\ref{fig:apply_only_math}. The second largest speedup is due to the
"Vectorized Conditionals" pattern discussed in Section~\ref{sub-sec:ablation}. The
majority of cells we consider are improved by \system{}. The maximum slowdown in
an individual cell is 28\%. In general, the cells that have the highest slowdown
are fast cells, i.e., those already within interactive latencies, both before
and after rewriting.

\paragraph{\textbf{Overhead of \system{}}} We further investigated the cause of
slowdowns. We first measured the overhead of deploying \system{} (on all 20
notebooks). We find that \system{} never has an overhead of more than \emph{23
ms} with a geometric mean overhead of 0.99ms.

\input{figures/cells_only_slowdowns.tex}

However, in addition to the overhead from deploying \system{}, \system{} may
also cause downstream effects. We find that in some cases, cells that are not
modified by \system{} can experience degradations in performance. The highest
magnitude of those appear only in notebooks where \system{} rewrites cells.
Because of this, and because some of these slowdowns are much larger than any
overhead that \system{} can cause, we hypothesize that rewriting is not the
cause of the slowdown. Rather, it seems that the rewritten version of a cell,
while faster than the original version of this same cell, causes a slowdown in
another cell of the same notebook. Nonetheless, these slowdowns are not
substantial. In Figure~\ref{fig:cells-slowdowns} we show only the cells from
Figure~\ref{fig:cell_level} that get slowdowns along with the absolute slowdown.
That figure shows that even when the relative slowdown is large, the absolute
slowdown is below interactive latency times (i.e., below 300ms).

\input{figures/modin_nb_level.tex}

\subsection{Comparison with Modin}

We compare \system{} with \code{modin} \cite{modin} (using Ray as the underlying engine
which is the default). We chose \code{modin} because it enjoys wide adoption and is
supposed to be a drop-in replacement for \code{pandas}.

We focus on deploying \code{modin} on a single server as this is the setting we focus
on in this work. Unfortunately, we find that deploying \code{modin} in this setting is
difficult for two reasons: excess memory utilization and lack of support for the
full \texttt{pandas} API.

For the notebooks we consider, \code{modin} consumes substantially more memory
resources than standard \texttt{pandas}. Even when using a powerful AWS server,
the AWS \texttt{c5.24xlarge} with 96 vCPUs and 192 GB of RAM, \code{modin} was
unable to execute five of the ten notebooks we consider. As a result, we modify
the default \code{modin} settings to execute on 4 to 12 cores depending on the
notebook and we also had to reduce the dataset replication on 3 of the 10
notebooks. With these modifications, we are able to run the notebooks with
\code{modin}, using our original setup.

We further find that \code{modin} does not support 100\% of the \texttt{pandas} API. As
a result, we could not run two of the ten notebooks. We changed the impeding
snippets to ones that are functionally close. Given our new setup, we compared
\code{modin}, \system{}, and vanilla \texttt{pandas}.

\input{figures/modin_nb_mem.tex}

As shown in Figure \ref{fig:modin_nb_level} \footnote{\system{}' results in
Figure \ref{fig:modin_nb_level} look slightly different from those in Figure
\ref{fig:nb_level}, even though the same notebooks are used. This is because of
the changes we had to perform on some of the notebooks (i.e., less
replication and API changes) to run them with \code{modin}.}, \code{modin}
\emph{slows down} 9 of the 10 total notebooks we consider compared to vanilla
\texttt{pandas}. It speeds up one notebook which is dominated by a call to
\code{apply()}, which \code{modin} is able to parallelize. As witnessed in this notebook, 
one advantage of \code{modin} is that it can scale with the availability of more hardware resources 
in cases where it can parallelize. \system{} does not enjoy such scaling benefits. However, we find that
\code{modin} cannot parallelize the majority of the notebooks we consider diminishing any scaling benefits.
%We further find that \code{modin} is unable to parallelize the majority of other notebooks we
%consider. 
Overall, \system{} is up to 26.4$\times$ faster than \code{modin} (4.1$\times$
geometric mean) for whole notebooks.

We further show that \code{modin} uses memory resources (RAM and disk)
aggressively, with results in Figure \ref{fig:modin_nb_mem} \footnote{The only
way we found to measure \code{modin}'s memory consumption somewhat reliably was
using \code{ray memory}, which however was still unreliable and very slow to
query. We could not obtain memory measurements for 2 notebooks.}. When deploying
\code{modin} exclusively across multiple servers, it is generally acceptable to
use all the available hardware resources. However, many of the users of ad-hoc
EDA workloads have limited hardware resources, further highlighting the
deployment issues with \code{modin}. Note that \system{}, (like \code{pandas}),
makes no use of the disk.

% These results came as a surprise to us and so we devised further experiments to
% understand what causes them. \code{modin} has shown in previous work \cite{modin} that
% it can provide speedups for multiple pandas functions (e.g., \code{read\_csv()}
% and \code{count()} as shown in \code{modin}'s evaluation, which we replicated, and
% \code{apply()} as we saw above). Our hypothesis, then, is that there is a kind
% of Pandas usage that \code{modin} can optimize but there is also Pandas usage for which
% \code{modin} does not specialize; the latter appears in real notebooks and is thus why
% we see these slowdowns in the 20 Kaggle notebooks above. In the following
% sub-section we validate this hypothesis experimentally.

\subsection{Comparing Various Dataframe Libraries}
\label{sub-sec:cmp-libs}

To further understand how \code{modin} and other dataframe libraries perform on ad-hoc
EDA workloads, we perform a series of targeted experiments using common patterns
we have found in such workloads. In addition to studying \code{modin}, we also
study three other common dataframe libraries: \code{dask} \cite{dask} (version
2022.12.1), Koalas \cite{koalas_web} (version 0.32.0), and PolaRS
\cite{polars_web} (version 0.7). \code{dask} is another widely adopted parallel
dataframe library with a slightly different API from that of \code{pandas}. Koalas
implements the \code{pandas} API over PySpark \cite{pyspark_web}.  PolaRS
\cite{polars_web} is a \code{pandas} replacement (using Rust under the hood), which,
however, has a different API.

We use a \texttt{c5.24xlarge} AWS instance with 96 vCPUs and 192 GiB of RAM. We
use 12 vCPUs for \code{modin}, \code{dask}, Koalas and PolaRS. The dataset used
is the NYC Yellow Taxi Dataset 2015 - January \cite{nyc_taxi_dataset} (except
for one case mentioned below) with a size of around 1.8GB. We picked this
dataset because (a) it is large (the subset we use is the largest we could run
the experiments with, using the libraries mentioned, on this machine) and these
libraries specialize in large datasets and (b) it has been used in previous work
\cite{modin} and in multiple notebooks throughout the Internet
\cite{nyc_taxi_kaggle}.

% Finally, the code we run is small enough to be included in the paper if we
% exclude the boilerplate. The full source code and the dataset are available in
% the following repo \stef{Add it although probably not for submission}.

\input{figures/column_ops.tex}

\paragraph{\textbf{Column-Wise Operations}}
A common pattern in \code{pandas} is to perform column-wise
operations. In fact, this pattern does not involve interoperability with other
libraries or complex Python code. Because this pattern is common,
\texttt{pandas} uses vectorized implementations. This means that not only does
it process elements in bulk but also does it so completely in native code,
thereby avoiding Python's overheads. We show two examples in
Figure~\ref{fig:column_ops}.% , the artifact contains four more.
% \ddkang{Only say the artifact contains more if we release it} \stef{So, leave
% the comment here until we decide if we'll release it?}

In the first example, we add two \code{pandas} columns element-wise. \code{modin}, \code{dask},
Koalas and PolaRS are 55.1$\times$, 136.8$\times$, 9.4$\times$ and 7.6$\times$
slower than \code{pandas}, respectively. In the second example, we perform a standard
reduction over a column: computing its standard deviation. \code{modin}, \code{dask} and
Koalas are 2.6$\times$, 4.6$\times$ and 31.2$\times$ slower than \code{pandas},
respectively. Therefore, even for simple and ubiquitous operations within the
\code{pandas} space, \code{pandas} replacements can incur significant slowdowns.
Interestingly, PolaRS is 1.1$\times$ faster for this example.


\input{figures/interact_numpy.tex}

\paragraph{\textbf{Interaction with NumPy}} Another standard operation we find
common is to interoperate \code{numpy} and dataframe libraries. We hypothesize
that this is a common operation because \code{numpy} is a core primitive used in
\texttt{pandas}: \texttt{pandas} columns are stored as \code{numpy} arrays and
many \texttt{pandas} operations use \code{numpy}. 

We tried two simple operations shown in Figure~\ref{fig:interact_numpy}. In
Figure~\ref{fig:np_sum} we perform a simple sum over a column. In
Figure~\ref{fig:np_where}, we use \code{np.where()} to conditionally assign
values to a \code{pandas.Series} (this is a standard way to speed up \code{pandas}
\cite{pygotham_apply_vectorized}).

For the first example, \code{modin}, \code{dask} and PolaRS are 18.3$\times$, 179.8$\times$
and 1.4$\times$ slower than \code{pandas}, respectively. Koalas was unable to perform
this operation within the memory limits. We instead used a smaller dataset, Iris
\cite{iris_dataset}, a common dataset in the pattern recognition literature.
Koalas is 190.4$\times$ slower in this case.

For the second example, \code{modin}, \code{dask}, Koalas and PolaRS are 54.5$\times$,
111.7$\times$, 202$\times$ and 3$\times$ slower than \code{pandas}, respectively.

As we can see, \code{modin}, \code{dask} and Koalas do not interoperate well with \code{numpy}.
Nevertheless, such \code{pandas} usage is common. We have 1 notebook (10\%) using
\code{np.sum()} and another 3 (30\%) using \code{np.where()} (one of which makes
heavy use of it). PolaRS also incurs a slowdown, but it is much smaller.


\paragraph{\textbf{Iterative Access of Individual Elements}} The final common
pattern we consider is the iterative access to individual dataframe elements. We
show an example in Figure~\ref{fig:for_loop}, which we extracted from a real
notebook. \code{modin} and Koalas are 1914.5$\times$ and 155$\times$ slower than
\code{pandas}, respectively (note that we were able to run the loop only for 5
iterations without out-of-memory errors in Koalas, so we compared with 5
iterations of \code{pandas}). PolaRS is 20.8$\times$ slower for 15 iterations
and 99.7$\times$ slower for 100 iterations (it was realistically impossible to
run the other frameworks for 100 iterations).

It was more difficult to run the experiment for \code{dask}. %It is clear that
In general, \code{dask.DataFrame} is not intended for individual element access. This loop is not
supported as-is by the \code{dask} API and we could not find a reasonable way to
translate it. But for completeness, we access a single individual element in
\code{dask} and we compare it with doing 15 iterations of the loop above in \code{pandas}.
\code{dask} is 506$\times$ slower.

\paragraph{\textbf{Discussion}} As our results show, ad-hoc EDA workloads
contain diverse code. Given the limited hardware resources available in these
settings, we see that bulk-parallel dataframe libraries like \code{modin},
\code{dask} and Koalas, are not well suited for ad-hoc EDA workloads. PolaRS,
can give small performance improvements compared to \code{pandas}, and its
slowdowns are smaller compared to other libraries. However, it can also cause
considerable slowdowns (e.g., with the iterative element access) and has a significantly
different API. For example, the \code{pandas} snippet \code{df['A'] = 1} is
translated to 

\begin{minted}[bgcolor=light-gray]{python}
df = df.with_column(pl.lit(1).alias('A'))
\end{minted}

\noindent
in PolaRS. As a result, it requires learning new syntax.









\subsection{Understanding \system{}' Performance}
\label{sub-sec:ablation}

\input{figures/no_sliced_exec.tex}


To understand the performance gains of \system{}, we conduct an ablation where
we remove sliced execution and we also discuss two case studies in detail.

\paragraph{\textbf{Disabling Sliced Execution}}
As we explained in Section~\ref{sub-sec:rewriter}, sliced execution increases
the complexity of \system{}. To investigate whether or not sliced execution
improves performance, we ablated sliced execution.
Figure~\ref{fig:no_sliced_exec} shows the relative \emph{slowdown} compared to
having sliced execution enabled.

As shown, all notebooks except one
run faster without sliced execution. In fact, our largest cell-level speedup
(57$\times$) is lost. Furthermore, although only two patterns that hit require
sliced execution, patterns presented in online tutorials
\cite{pygotham_apply_vectorized} and which require sliced execution, can give
dramatic speedups of up to 380$\times$.

% \stef{We have slowdowns in notebooks we don't hit. I can't explain that.}

% \paragraph{\textbf{Disablng Patterns}} \stef{TODO}

% \subsubsection{Case Studies}
% \label{sub-sec:case-studies}
% 
% In this section we present case studies that showcase two angles of \system{} as
% a rewrite system. First, we highlight \system{}' capabilities in
% \emph{performing} the rewrites, i.e., the technical requirements of its
% pattern-matcher and rewriter. Then, we examine properties of the code \system{}
% outputs and in particular, that it leads to unexpected speedups and why.

\input{figures/apply_vectorized.tex}

\paragraph{\textbf{Vectorized Conditionals}}
We further study two case studies, starting with vectorized conditionals.

We show an example of rewriting a \texttt{pandas} \code{apply()} function with
\code{numpy}'s \code{np.select()} in Figure~\ref{fig:apply_vectorized}
\cite{pygotham_apply_vectorized}. Both versions output a certain value per row
based on some conditions. The second one gives many-fold speedups, 36$\times$ in
our evaluation and up to 380$\times$ in other situations
\cite{pygotham_apply_vectorized}, mainly due to the use of vectorized execution.

To do this rewrite, \system{} checks that the function \code{foo} contains only
an \code{if-else} chain and the conditions are such that we can translate them
to equivalent that apply to whole columns (for example, we cannot translate
\code{if bar()} for some random function \code{bar}). Also, the return
values should be such that can be converted to numpy arrays. The constant
\code{'X'} is such a value but if it were \code{bar(row['A'])}, we would not, in
general, be able to translate it.

Verifying these conditions is not the only tricky part; producing the rewritten
version can be challenging too. For example, the original uses Python's
\emph{logical-AND} (i.e., \code{and}) to compare elements, but we need to use
Python's \emph{bitwise-AND} (i.e., \code{\&}) when translating to \code{pandas} and the
parentheses around the two sides are required. Similarly, a condition like
\code{a in ls} needs to be translated to a call to the \code{pandas} \code{isin()}
function. These are subtleties of rewriting that can be easily missed if we
carry it out manually. Besides leading to bugs, they require extensive
knowledge of the \code{pandas} API.

As explained in Section~\ref{sub-sec:rewriter}, these checks, and the rewriting,
cannot be performed a priori because the code of \code{foo} might not be
available yet at the start of the cell. Thus, the rewriter employs sliced
execution to perform these actions on demand.

Finally, if the user changes \code{foo} such that it does not abide to the
above conditions, the rewriter cannot perform the rewrite. At the same time,
however, the original code remains intact. Thus, the code will never be slower
than the original. Moreover, had the user performed the rewrite by hand, they
would have to convert it back to the \code{apply()} version, but this effort
disappears with the rewriter.

\input{figures/pandas_split_simplified.tex}

\paragraph{\textbf{Translating to Pure Python}}
We present a case study of a non-intuitive result: translating an
"optimized" \code{pandas} call to pure Python (Figure~\ref{fig:split}). In
general, users expect \code{pandas} to be more efficient than pure Python
since \code{pandas} uses vectorized, native code, while also avoiding the interpreter, when possible.

However, \code{.str.split()} is a \emph{string} operation and these cannot in
general be vectorized by \code{numpy}. So, a call to \code{.str.split()}
%eventually
reaches a standard Python loop to carry out the operation
\cite{pandas_map_infer_mask}.
% \footnote{In fact, it is a Cython loop, but the
% only benefit we get from Python is in the loop iterator. Still, we would expect
% it to be slightly faster than the equivalent pure Python one.}

We would then expect the \code{pandas} version to be in par with our version. We have
to look more closely to understand the discrepancy. In
Figure~\ref{fig:pandas_split_simplified}, we show a simplified version of
\code{.str.split()}'s implementation. Specifically, the important thing is that
in the loop, we gather a collection of (2-element) \emph{lists} in \code{res}
(\code{res} is a \code{numpy} array but it could be any container without much
difference in performance; e.g., it could be a list. The important thing is what
it stores.). Then, we create our two results, our two \code{Series} (via
creating a \code{DataFrame}, but the particular way of doing it is irrelevant). In
particular, we split these lists "vertically" and in half so that all the first
elements of the lists create the Series \code{a} and all the second elements
create the Series \code{b}.

One should contrast this with our rewritten version. There, we create only two
lists (\code{a} and \code{b}). At every iteration of the loop, we create one
list, the result of \code{split()}, append the individual elements to \code{a}
and \code{b} and then \emph{throw it away}. Notice that in the \code{pandas}
version, the result of \code{split} has to be saved. So, while on the
surface, the two loops allocate the same number of lists, in our version, the
same space can be reused for every iteration.

Finally, we convert \code{a} and \code{b} (both lists of strings) to
\code{Series}. Under the hood, a list of strings is a contiguous block of memory
in which every element is a pointer to the string. A \code{Series} of strings is
also a contiguous block of memory in which every element is a pointer to a
string. So, the conversion from the one to the other is cheap. However, in the
\code{pandas} version, the elements are stored together in lists "horizontally",
but we want to store them together "vertically" (if we imagine a matrix where
every row is a list coming from \code{split}). This halving and regrouping is
expensive.

In this example, the rewriter enables us to optimize a library \emph{without
changing the library}. As we have explained earlier, the rewriter can cross
library boundaries and thus it can optimize across Python, \code{pandas} and \code{numpy},
without the need to provide custom versions of these libraries.

% See the following call stack on where str.split() ends up.
% - split: https://github.com/pandas-dev/pandas/blob/91111fd99898d9dcaa6bf6bedb662db4108da6e6/pandas/core/strings/accessor.py#L866
% - _str_split: https://github.com/pandas-dev/pandas/blob/91111fd99898d9dcaa6bf6bedb662db4108da6e6/pandas/core/strings/object_array.py#L340
%   - See also the next line.
%   - expand is not used in this function.
% - _str_map: https://github.com/pandas-dev/pandas/blob/91111fd99898d9dcaa6bf6bedb662db4108da6e6/pandas/core/arrays/string_.py#L569
% - _map_infer_mask: https://github.com/pandas-dev/pandas/blob/91111fd99898d9dcaa6bf6bedb662db4108da6e6/pandas/_libs/lib.pyx#L2863
%   - This is in Cython.

% Once this finishes, we call _wrap_result here: https://github.com/pandas-dev/pandas/blob/91111fd99898d9dcaa6bf6bedb662db4108da6e6/pandas/core/strings/accessor.py#L867
% This is defined here: https://github.com/pandas-dev/pandas/blob/91111fd99898d9dcaa6bf6bedb662db4108da6e6/pandas/core/strings/accessor.py#L255
% Eventually, we go here: https://github.com/pandas-dev/pandas/blob/91111fd99898d9dcaa6bf6bedb662db4108da6e6/pandas/core/strings/accessor.py#L353
% And finally note _constructor_expanddim is probably this: https://github.com/pandas-dev/pandas/blob/91111fd99898d9dcaa6bf6bedb662db4108da6e6/pandas/core/series.py#L558


% All that means that essentially, what's happening is the following:
% a = []
% b = []
% n = len(arr)
% res = np.empty(n, dtype=arr.dtype)
% for i in range(n):
%   spl = arr[i].split(',', maxsplit=1)
%   res[i] = spl

% # Not sure about the following
% res = list(res)
% x = pd.DataFrame(res, columns=['a', 'b'])


% So, we split the strings and we gather them into a numpy array of lists. Then, we construct a DataFrame
% from these lists, splitting them apart.

% I expected the actual thing to be faster than this because the loop is in Cython. Nonetheless, it's slower, although this
% one is much closer. I guess it's slower because of arbitrary Pandas overheads in checking conditions and all that.

% My assumption about the slowness compared to our rewritten version is the number
% of allocations. In our version, we allocate only two lists (and the split
% strings which are allocated in both versions). Also, Python lists are extendible
% arrays (i.e., vectors), which means translating them to numpy arrays should be trivial.

% On the other hand, in the original version, we first compute a whole numpy array
% _of lists_, which means _many_ allocations. When converting to a DataFrame, it
% probably means also a lot of deallocations. And it's of course not as cheap to separate
% these lists vertically as it is with our rewritten version.
