
\section{Limitations}

\paragraph{\textbf{Generalizability of Patterns}} Suppose that we need $P$
patterns to rewrite cells in $N$ random notebooks and consider the ratio of $P$
over $N$. We say that the patterns generalize well if as $N$ gets larger, then
this ratio gets smaller, meaning, $N$ grows much faster than $P$. The current
patterns do not generalize well. We rewrite 10 notebooks but we needed to code
10 patterns to achieve this. If we pick 10 \emph{other} random notebooks, the
results may not be the same. Of course, $N = 10$ is very small to make an
inductive argument that this is the rule as $N$ increases. Nonetheless, we
currently have no evidence that the patterns will eventually generalize.

\paragraph{\textbf{Engineering Effort}} Currently, there is a non-trivial effort
required to both find and code the patterns and \system{} does not provide any
automation for either. We hope in the future to improve it on both aspects. The
latter is more tractable and it would probably require a framework in which one
can succinctly express patterns. The former is much harder. Finding patterns
requires both reasoning about program semantics (an infamous hurdle in Python)
and cost-modeling equivalent versions.

% \stef{Well, I was gonna add interoperability with Pandas replacements as a
% limitation but in fact, one can disable patterns and leave only those that go
% from Pandas to Pandas}

\paragraph{\textbf{Effective Use of Hardware}}

The speedups \system{} provides do not scale in a way aligned with modern
hardware trends i.e., hardware replication (e.g., multi-core machines or
distributed storage). Adding more cores, machines or disks will have no effect,
as it does for Python and \code{pandas}. Nevertheless, even though in this paper we
explored the surprising direction of going from \code{pandas} to Python, the inverse,
i.e., going from Python to \code{pandas}, still has performance benefits. This is
something we hope to explore in the future. %using rewriting.
