\section{Introduction}
\label{sec:Intro}

In recent years, \emph{dataframe}-based libraries, such as \texttt{pandas}, have
become increasingly popular with users ranging from social scientists to
business analysts \cite{pandas_in_business, pandas_in_social}. This growth is driven by many reasons, including
the flexibility of such libraries, the ability to work within a notebook
environment, and the interoperability with other libraries.

Due to the popularity of dataframe libraries, academic and industrial work has
focused on improving the scalability of \texttt{pandas} \emph{in the context of
bulk-parallel operations}. For example, libraries including
\code{modin} \cite{modin}, \code{dask} \cite{dask}, and PySpark \cite{pyspark_web} focus on
parallel or distributed dataframes. Many of these libraries focus on scaling out
\texttt{pandas} across multiple servers, as \texttt{pandas} will fail if the
dataframe does not fit in main memory.
% Many commercial applications require such horizontal scalability.

\input{figures/for_loop.tex}

However, there has been an emerging class of important workloads that operate on
a \emph{single} machine, combined with ad-hoc functions. For example, in our
conversations with law professors at Stanford University, we have found that
provisioning and managing distributed clusters is challenging and time-consuming
for social scientists. As a result, much of the work done by such social
scientists is done on a single machine. Similarly, Kaggle and Google Colab
provide single-machine notebooks for data scientists to explore datasets.
Furthermore, many tasks require custom user-defined functions (UDFs) that are
not well suited to working directly within the \texttt{pandas} API.

While these bulk-parallel dataframe libraries improve the horizontal scalability
of dataframes, as we show in this work, they unfortunately can \emph{fail to
accelerate a wide range of single-machine, ad-hoc workloads}. For example,
\code{modin}, \code{dask}, and PySpark are all 2-200$\times$ slower than \texttt{pandas} for a
range of operations: when interfacing with \texttt{numpy}, looping over
individual rows, and even for simple operations like multiplying two columns (on
a single machine). For example, a simple loop (Figure~\ref{fig:for_loop}) can be
many \textit{hundreds} of times slower (see Section~\ref{sub-sec:cmp-libs}).
Furthermore, all distributed dataframe libraries we are aware of do not maintain
full \texttt{pandas} compatibility, requiring domain experts to learn new
libraries.

We propose an alternative approach to address the \emph{vertical} scalability of
dataframe libraries: \emph{rewriting} notebook cells to accelerate dataframe
computations by utilizing faster, but semantically equivalent code sequences. To
understand the potential for rewriting notebook cells, consider the two cells in
Figure~\ref{fig:apply_only_math}. The first cell is a simplified cell from a
real-world, Kaggle notebook. The second cell is an optimized cell with identical
semantics. While identical semantically, the second cell can run up to
1000$\times$ faster, showing that simple rewrites of notebook cells can
accelerate workloads.

A natural question that emerges is why can't the users write optimized code
themselves. As witnessed in compilers, automatic tools that accelerate code can
reduce developer effort and improve comprehensibility. Furthermore,
several rewrite rules in this paper were \emph{not apparent to the
authors} (e.g., the rules used to perform the rewrites in
Figure~\ref{fig:concat-with-lists} and Figure~\ref{fig:split}), even after
devoting considerable time studying the internals of Python and \code{pandas}.
To understand how this translates to non-experts, there are whole videos and
articles dedicated to patterns for speeding up \code{pandas} code via manual
rewriting \cite{pygotham_apply_vectorized, pandas_opt_article_docs,
pandas_opt_video_heisler, pandas_opt_video_trabelsi, pandas_opt_article_dan,
pandas_opt_article_numba}. Even then, optimizing code correctly is challenging
for non-experts and can lead to subtle bugs.

To realize the vision of rewriting notebook cells transparently, we
propose \system{}, a tool that automatically rewrites notebook cells. As we
show, \system{} can accelerate notebook cells by up to 57$\times$ completely
transparently to the user. 

\input{figures/apply_only_math.tex}

\system{} is the first source-to-source, dynamic rewriter for a Python library.
Source-to-source rewriting/compilation, especially over Python code, is
challenging because source languages are high-level, while traditional compiler
optimizations work better in low-level intermediate representations. But, we
observe that operating at the source level creates novel opportunities. One such
opportunity is that it enables us to rewrite across different representations
(e.g., Python and \code{pandas}). Such rewrites are high-level (leading to
significant speedups), and thus, the source language is a suitable
representation for performing them. It may be useful to compare and contrast a
high-level rewrite like the one in Figure~\ref{fig:concat-with-lists} with a
traditional low-level rewrite such as \code{a*2} $\rightarrow$ \code{a<<1}.

To perform these rewrites, \system{} needs to operate externally, in contrast with
previous work (e.g., TensorFlow Grappler \cite{tf_grappler}, \code{modin}
\cite{modin}, BELE \cite{best_effort_lazy}), which are implemented as libraries.
Namely, it views all user's code compared to just library code and can modify
any part of it. This allows \system{} to rewrite across library boundaries (e.g.,
Figure~\ref{fig:concat-with-lists}), which is infeasible with current
techniques. 

However, implementing a source-to-source, external rewriter introduces new
challenges. First, \system{} needs to understand more than one representation (i.e.,
\code{pandas} and Python). Second, providing any guarantees when operating in a
high-level language is naturally harder, especially when it comes to Python
which has no formal semantics and liberal typing and scoping rules. A specific
challenge was performing the precondition checks at the correct program points,
as we explain in Section~\ref{sub-sec:rewriter}. Finally, \system{} must operate
within interactive time scales: the overhead of rewriting cannot dominate cost
savings.

% Data scientists now only need
% to express \emph{what} to compute and the rewriter will decide \emph{how} to
% compute it using the best available Pandas functions and pure python constructs.
% \stef{I'm not sure the previous sentence is a good idea. For one, it reminds too
% much of query optimizers. Also, this paragraph feels a bit redundant.}. We
% believe that this will lead to end user productivity gains as data scientists
% from different domains can iterate through their data processing pipelines
% faster without needing to think about how to make their implementations fast.

We designed \system{}' rewrite engine with two components: a pattern matcher
and a rewriter with design decisions that specifically address the
aforementioned challenges. The rewrite engine is lightweight, with a fast
pattern matcher that can quickly match patterns that we can rewrite into faster
versions and a rewriter which emits, or performs, necessary static and runtime
precondition checks to guarantee correctness, within interactive latencies.

%\system{} is lightweight, with a fast pattern matcher that can quickly match patterns
%that we can rewrite into faster versions and a rewriter which performs this
%rewriting by guaranteeing correctness, within interactive latencies.

We show that on real-world Kaggle notebooks, \system{} can accelerate cells by
up to 57$\times$ (1.27$\times$ geometric mean) and whole notebooks by up to
3.6$\times$ (1.31$\times$ geometric mean). We also compare \system{} with
\code{modin} and show that it can be up to 27.1$\times$ faster for whole
notebooks (4.9$\times$ geometric mean). Furthermore, \system{} can avoid
rewriting cells that cause slowdowns, resulting in overheads that are only due
to the pattern matcher. We show that these overheads, even in the cases where
cells are not rewritten, are below noise thresholds. Finally, \system{} uses
%virtually
no extra memory or disk capacity.

In summary, we make the following contributions.

\begin{enumerate}
    \item We identify program rewriting as a lightweight technique to speed up
    \code{pandas}-heavy EDA workloads.
    \item We develop \system{}, the first external rewriter to rewrite code
    across different representations in a dynamic setting. We introduce rewrite
    rules that can significantly speed up \code{pandas} code, including ones
    that cross library boundaries.
    \item \system{} applies these rewrite rules automatically, at runtime, and
    verifies whether applying a rule is correct by injecting checks at
    fine-grained program points.
    \item We evaluate \system{} on real-world notebooks and show that it can
    speed up cells by up to 57$\times$ and notebooks by up to 3.6$\times$, with
    almost no memory or disk overheads. We further compare \system{}
    with \code{modin} \cite{modin} and show that it can be up to 27.1$\times$
    faster for whole notebooks (4.9$\times$ geometric mean).
\end{enumerate}

