\section{Evaluation}
\label{sec:Evaluation}

\subsection{Experimental Setup}
\label{sub-sec:exp-setup}

All the experiments, except if mentioned otherwise, were performed on a system
with a 12-core AMD Ryzen 5900X, 32GB of main memory, Samsung 980 PRO NVMe SSD and Ubuntu
22.04.1 LTS.


\paragraph{\textbf{Benchmark}}

Our goal was to evaluate \system{} on real workloads and so we picked notebooks
from Kaggle. We chose Kaggle as it is a popular repository for data science
workloads and it also contains both the data and notebooks used. The overarching
hypothesis that we want to validate is that a rewrite system like \system{} can
offer substantial speedups on real-world notebooks, through rewriting, with
minimal slowdowns, minimal memory consumption and disk usage, and without
changing the API.

In this work, we focus on ad-hoc EDA, \code{pandas}-heavy workloads. In order to
find such notebooks, we chose notebooks randomly from Kaggle subject to the
following conditions:
\begin{itemize}
    \item At least 50\% of static function calls are \code{pandas} calls
    \item Using datasets of size approximately 2GB or less
\end{itemize}

We chose the first criterion because we focus on EDA notebooks. In particular,
many of the notebooks we excluded focused on machine learning and plotting,
which are out of scope for this work. In the notebooks we picked, we disabled
such code for our evaluation.

Our second criterion was to filter out notebooks that were already
hand-optimized. These notebooks typically operated on large datasets.
Optimization is necessary in this setting as Kaggle has resource constraints
(both computational and memory). However, hand-optimization requires significant
effort. \system{} is an automatic and transparent system and we want to evaluate
its effectiveness without users having to expend that effort.

For the datasets that were significantly lower than 2GB, we replicated them so
that they reach at least several hundred MBs (otherwise our measurements would
be dominated by noise). Also, we modified any notebook that used a sample/subset
of the dataset to instead operate on the full dataset.

% We do remove sampling. See for example:
% dataranch/supermarket-sales-prediction-xgboost-fastai/src/bench.ipynb

% Our second criteria was to filter out notebooks that executed on sampled data or
% were already optimized. Because Kaggle has memory and computation limits, many
% data scientists that leverage Kaggle either first sample the dataset or write
% optimized code to deal with the larger dataset size.

We sampled 20 notebooks satisfying our criteria. There are rewrite opportunities
in 10 of these 20 notebooks, which we coded in \system{}. We focus on these 10
notebooks in our evaluation. We further executed \system{} on the remainder of
the notebooks where no patterns were matched to study \system{}' overhead. We
describe further experiments that include all 20 notebooks in an extended
version of this manuscript \footnote{Not cited to preserve anonymity}. We
compare \system{} with \code{pandas} (version 1.5.1) and \code{modin}
\cite{modin} (version 0.17.0).



% Originally, we picked random notebooks from Kaggle, subject only to the
% condition of being Pandas-heavy. Then, we started finding optimization
% opportunities and coding them in \system{}. However, we observed that for some
% notebooks, there were no optimization opportunities. At the same time, these
% notebooks made advanced use of Pandas, Python and NumPy which did not appear in
% the rest of the notebooks. After analysis, we found out that the distinctive
% characteristic of these notebooks was they were processing large datasets. Large
% here means above a threshold that causes every cell, on average, to be outside
% interactive latencies (which we consider to be around 2s) on a conventional
% consumer machine. This threshold seemed to be around 2GB.
% 
% Our hypothesis is that these notebooks did not start sophisticated. Rather, the
% users wrote advanced code once they observed slow execution times with the
% original code. This is driven by the fact that in the rest of the notebooks,
% users did not write advanced code. Given that there is no correlation between
% user's knowledge on Pandas, Python and NumPy and the datasets they choose to
% process, our hypothesis seems reasonable. Unfortunately, we cannot validate our
% hypothesis because we only have the final versions of the notebooks. We cannot
% know how many rounds of manual rewriting a cell went through. That would require
% a user study that we did not have time to do for this paper.
% 
% So, we discarded notebooks with datasets above 2GB (which are a minority on
% Kaggle) and we evaluated our tool on only the rest. Moreover, for some of the
% notebooks we used, we replicated the datasets if they were too small so that
% they reach up to roughly 2GB. We did this for two reasons. First, if the dataset
% is too small, then we cannot have accurate measurements. Second, we wanted to
% show that a user could write the same non-advanced code that appears in these
% notebooks and still get speedups for free.
% 
% In summary, for our evaluation we picked 20 random notebooks from Kaggle subject
% to two conditions:
% \begin{itemize}
%     \item Pandas-heavy (at least 50\% of static function calls are Pandas calls)
%     \item Using datasets less than around 2GB
% \end{itemize}
% 
% For the datasets that were significantly lower than 2GB, we replicated them so
% that they reach at least several hudred of MBs. \system{} rewrites cells in 10
% out of 20 notebooks. \stef{Not sure if the previous sentence should be here.}
% 
% After picking the notebooks, we searched for optimization opportunities in them
% and on the internet. We coded in \system{} those that could be expressed as
% rewrite rules. Then, we performed the following experiments.


\subsection{End-to-End vs Pandas}
\label{sub-sec:vs-pandas}

\input{figures/cell_level_speedups.tex}
\input{figures/nb_level_speedups.tex}

We first investigated whether \system{} can accelerate cells and notebooks
compared to standard \texttt{pandas}. To do so, we ran each sampled notebook
with and without \system{}. We ran 10 trials each and measured execution time at
the cell level. Our primary metric was the speedup of cells and notebooks with
\system{} compared to standard \texttt{pandas}. We report the geometric mean of
the speedups.

\paragraph{\textbf{Per-Notebook Speedups}} We show per-notebook relative
speedups in Figure \ref{fig:nb_level}. As shown, \system{} can provide
substantial speedups \emph{at the notebook level} of up to 3.6$\times$. Overall,
\system{} provides significant speedups in half of the notebooks (five) and
moderate speedups in one other notebook. We emphasize that these notebooks were
selected randomly from Kaggle, showing the applicability of \system{}.

Furthermore, \system{} does not significantly slow down any notebook, with a
maximum slowdown \revis{1.03$\times$}. \system{} rewrites cells
in these notebooks but it does not achieve speedups.


\paragraph{\textbf{Per-Cell Speedups}} We show per-cell speedups in Figure
\ref{fig:cell_level}. For clarity, we excluded cells that run for fewer than
50ms in the original version and excluded all speedups and slowdowns when run
with \system{} within \revis{0.1x} the original cell runtime.

As shown, \system{} can achieve per-cell speedups of up to 57$\times$. The cell
with the highest speedup is matched by the pattern shown in
Figure~\ref{fig:apply_only_math}. The second largest speedup is due to the
pattern \revis{\code{Vectorized} \code{Conditionals}, which is} discussed in
Section~\ref{sub-sec:ablation}. The majority of cells we consider are improved
by \system{}. The maximum slowdown in an individual cell is
\revis{1.56$\times$}. In general, the cells that have the highest slowdown are
fast cells, i.e., those already within interactive latencies, both before and
after rewriting.

\paragraph{\textbf{Overhead of \system{}}} We further investigated the cause of
slowdowns. We first measured the overhead of deploying \system{} (on all 20
notebooks). We find that \system{} never has an overhead of more than \emph{22
ms} with a geometric mean overhead of 0.41ms.

\input{figures/cells_only_slowdowns.tex}

However, in addition to the overhead from deploying \system{}, \system{} may
also cause downstream effects. We find that in some cases, cells that are not
modified by \system{} can experience degradations in performance. The highest
magnitude of those appear only in notebooks where \system{} rewrites cells.
Because of this, and because some of these slowdowns are much larger than any
overhead that \system{} can cause, we hypothesize that rewriting is not the
cause of the slowdown. Rather, it seems that the rewritten version of a cell,
while faster than the original version of this same cell, causes a slowdown in
another cell of the same notebook. Nonetheless, these slowdowns are not
substantial. In Figure~\ref{fig:cells-slowdowns} we show only the cells from
Figure~\ref{fig:cell_level} that get slowdowns along with the absolute slowdown.
That figure shows that even when the relative slowdown is large, the absolute
slowdown is below interactive latency times (i.e., below 300ms).

\input{figures/modin_nb_level.tex}

\subsection{Comparison with Modin}

We compare \system{} with \code{modin} \cite{modin} (using Ray as the underlying engine
which is the default). We chose \code{modin} because it enjoys wide adoption and is
supposed to be a drop-in replacement for \code{pandas}.

We focus on deploying \code{modin} on a single server as this is the setting we focus
on in this work. Unfortunately, we find that deploying \code{modin} in this setting is
difficult for two reasons: excess memory utilization and lack of support for the
full \texttt{pandas} API.

For the notebooks we consider, \code{modin} consumes substantially more memory
resources than standard \texttt{pandas}. Even when using a powerful AWS server,
the AWS \texttt{c5.24xlarge} with 96 vCPUs and 192 GB of RAM, \code{modin} was
unable to execute five of the ten notebooks we consider. As a result, we modify
the default \code{modin} settings to execute on 4 to 12 cores depending on the
notebook and we also had to reduce the dataset replication on 3 of the 10
notebooks. With these modifications, we are able to run the notebooks with
\code{modin}, using our original setup.

We further find that \code{modin} does not support 100\% of the \texttt{pandas} API. As
a result, we could not run two of the ten notebooks. We changed the impeding
snippets to ones that are functionally close. Given our new setup, we compared
\code{modin}, \system{}, and vanilla \texttt{pandas}.

\input{figures/modin_nb_mem.tex}

As shown in Figure \ref{fig:modin_nb_level} \footnote{\system{}' results in
Figure~\ref{fig:modin_nb_level} look slightly different from those in
Figure~\ref{fig:nb_level}, even though the same notebooks are used. This is
because of the changes we had to perform on some of the notebooks (i.e., less
replication and API changes) to run them with \code{modin}.}, \code{modin}
\emph{slows down} 9 of the 10 total notebooks we consider compared to vanilla
\texttt{pandas}. It speeds up one notebook which is dominated by a call to
\code{apply()}, which \code{modin} is able to parallelize. As witnessed in this
notebook, one advantage of \code{modin} is that it can scale with the
availability of more hardware resources in cases where it can parallelize.
\system{} does not enjoy such scaling benefits. However, we find that
\code{modin} cannot parallelize the majority of the notebooks we consider
diminishing any scaling benefits.
%We further find that \code{modin} is unable to parallelize the majority of other notebooks we
%consider. 
Overall, \system{} is up to 27.1$\times$ faster than \code{modin} (4.9$\times$
geometric mean) for whole notebooks.

\revis{Note that if we include all 20 notebooks in our evaluation, \code{modin}
slows down \textit{all} 10 new notebooks (19/20 $\rightarrow$ 95\%). Also, for
individual cells, \system{} reaches speedups up to 1909$\times$ compared to
\code{modin}.}

We further show that \code{modin} uses memory resources (RAM and disk)
aggressively, with results in Figure \ref{fig:modin_nb_mem} \footnote{The only
way we found to measure \code{modin}'s memory consumption somewhat reliably was
using \code{ray memory}, which however was still unreliable and very slow to
query. We could not obtain memory measurements for 1 notebook.}. When deploying
\code{modin} exclusively across multiple servers, it is generally acceptable to
use all the available hardware resources. However, many of the users of ad-hoc
EDA workloads have limited hardware resources, further highlighting the
deployment issues with \code{modin}. \revis{In fact, if we run \code{modin}
across all 20 notebooks, it consumes up to 250GB when \code{pandas} and
\system{} consume less than 5GB}. Note that \system{}, (like \code{pandas}), makes no
use of the disk.

% These results came as a surprise to us and so we devised further experiments to
% understand what causes them. \code{modin} has shown in previous work \cite{modin} that
% it can provide speedups for multiple pandas functions (e.g., \code{read\_csv()}
% and \code{count()} as shown in \code{modin}'s evaluation, which we replicated, and
% \code{apply()} as we saw above). Our hypothesis, then, is that there is a kind
% of Pandas usage that \code{modin} can optimize but there is also Pandas usage for which
% \code{modin} does not specialize; the latter appears in real notebooks and is thus why
% we see these slowdowns in the 20 Kaggle notebooks above. In the following
% sub-section we validate this hypothesis experimentally.

\revis{
\subsection{An Estimate of \system{}' Hit Rate}

In this section we take a closer look at the hit rate of \system{}. In particular, we
wish to answer the question: How frequently does \system{} rewrite a cell?

For our analysis, we utilized a subset of KGTorrent \cite{kgtorrent}, a dataset
of Jupyter notebooks harvested from Kaggle. This dataset contains notebooks that
are outside the scope of our work, like notebooks which do not use \code{pandas}
at all. After filtering such notebooks out, we extracted \textit{a total of
8,853 notebooks and 177,272 cells}. We cannot run these notebooks because: (a)
we found no reliable way to automatically download a notebook's datasets and (b)
even with a dataset, many notebooks still don't run without manual modification.
Therefore, we perform a static analysis. In particular, we run the pattern
matcher over the notebooks and when it matches a rule, we consider it a "hit".
\system{}' hit rate is 3.2\% (5,586 cells) across cells and 27.1\% (2,395
notebooks) across notebooks.

In our evaluation, we used 20 notebooks with 652 cells. \system{} rewrites in 2\% of
all cells (5\% if we discard cells that run for less than 50ms) and 50\% of
notebooks. Because 652 cells is a large sample, and because it agrees with the
static analysis, we contend that \system{}'s hit rate is close to 2-3\%.

This result is significant considering that \system{} currently uses only 12
rules, which is a small set of rules for an automatic rewriter. Production
rewriters have hundreds of rules. For example, TensorFlow r1.14 includes 155
rewrite rules \cite{taso} (which are also simpler), developed over a long period
of time by many engineers and totalling around 53 thousand lines of code. We
emphasize that the novelty of \system{} is in the system, not the specific rules
we happen to use at this snapshot. With \system{}, we wish to encourage such a
development of rules for ad-hoc EDA workloads.

}


\subsection{Understanding \system{}' Performance}
\label{sub-sec:ablation}

% \input{figures/no_sliced_exec.tex}


To understand the performance gains of \system{}, we discuss two case studies in
detail.

% \stef{We have slowdowns in notebooks we don't hit. I can't explain that.}

% \paragraph{\textbf{Disablng Patterns}} \stef{TODO}

% \subsubsection{Case Studies}
% \label{sub-sec:case-studies}
% 
% In this section we present case studies that showcase two angles of \system{} as
% a rewrite system. First, we highlight \system{}' capabilities in
% \emph{performing} the rewrites, i.e., the technical requirements of its
% pattern-matcher and rewriter. Then, we examine properties of the code \system{}
% outputs and in particular, that it leads to unexpected speedups and why.

\input{figures/apply_vectorized.tex}

\paragraph{\textbf{Vectorized Conditionals}}
We further study two case studies, starting with \revis{a rule named \code{VectorizedConditionals}}.

We show an example of rewriting a \texttt{pandas} \code{apply()} function with
\code{numpy}'s \code{np.select()} in Figure~\ref{fig:apply_vectorized}
\cite{pygotham_apply_vectorized}. Both versions output a certain value per row
based on some conditions. The second one gives many-fold speedups, 36$\times$ in
our evaluation and up to 380$\times$ in other situations
\cite{pygotham_apply_vectorized}, mainly due to the use of vectorized execution.

To do this rewrite, \system{} checks that the function \code{foo} contains only
an \code{if-else} chain and the conditions are such that we can translate them
to equivalent that apply to whole columns (for example, we cannot translate
\code{if bar()} in some random function \code{bar}). Also, the return
values should be such that can be converted to numpy arrays. The constant
\code{'X'} is such a value but if it were \code{bar(row['A'])}, we would not, in
general, be able to translate it.

Verifying these conditions is not the only tricky part; producing the rewritten
version can be challenging too. For example, the original uses Python's
\emph{logical-AND} (i.e., \code{and}) to compare elements, but we need to use
Python's \emph{bitwise-AND} (i.e., \code{\&}) when translating to \code{pandas} and the
parentheses around the two sides are required. Similarly, a condition like
\code{a in ls} needs to be translated to a call to the \code{pandas} \code{isin()}
function. These are subtleties of rewriting that can be easily missed if we
carry it out manually. Besides leading to bugs, they require extensive
knowledge of \code{pandas}.

As explained in Section~\ref{sub-sec:rewriter}, these checks, and the rewriting,
cannot be performed a priori because the code of \code{foo} might not be
available yet at the start of the cell. Thus, the rewriter employs on-demand
dynamic checking.

Finally, if the user changes \code{foo} such that it does not abide to the
above conditions, the rewriter cannot perform the rewrite. At the same time,
however, the original code remains intact. Thus, the code will never be slower
than the original. Moreover, had the user performed the rewrite by hand, they
would have to convert it back to the \code{apply()} version, an effort
disappears with the rewriter.

\input{figures/pandas_split_simplified.tex}

\paragraph{\textbf{Translating to Pure Python}} We present a case study of a
non-intuitive result: translating an ``optimized'' \code{pandas} call to pure
Python, \revis{as shown in Figure~\ref{fig:split} (In \system{}, this is covered by the rule
\code{SplitInPython}; see Table~\ref{tbl:rewr_rules})}. In general, users expect
\code{pandas} to be more efficient than pure Python since \code{pandas} uses
vectorized, native code, while also avoiding the interpreter, when possible.

However, \code{.str.split()} is a \emph{string} operation and these cannot in
general be vectorized by \code{numpy}. So, a call to \code{.str.split()}
%eventually
reaches a standard Python loop to carry out the operation
\cite{pandas_map_infer_mask}.
% \footnote{In fact, it is a Cython loop, but the
% only benefit we get from Python is in the loop iterator. Still, we would expect
% it to be slightly faster than the equivalent pure Python one.}

We would then expect the \code{pandas} version to be in par with our version. We have
to look more closely to understand the discrepancy. In
Figure~\ref{fig:pandas_split_simplified}, we show a simplified version of
\code{.str.split()}'s implementation. Specifically, the important thing is that
in the loop, we gather a collection of (2-element) \emph{lists} in \code{res}
(\code{res} is a \code{numpy} array but it could be any container without much
difference in performance; e.g., it could be a list. The important thing is what
it stores.). Then, we create our two results, our two \code{Series} (via
creating a \code{DataFrame}, but the particular way of doing it is irrelevant). In
particular, we split these lists ``vertically'' and in half so that all the first
elements of the lists create the Series \code{a} and all the second elements
create the Series \code{b}.

One should contrast this with our rewritten version. There, we create only two
lists (\code{a} and \code{b}). At every iteration of the loop, we create one
list, the result of \code{split()}, append the individual elements to \code{a}
and \code{b} and then \emph{throw it away}. Notice that in the \code{pandas}
version, the result of \code{split} has to be saved. So, while on the
surface, the two loops allocate the same number of lists, in our version, the
same space can be reused for every iteration.

Finally, we convert \code{a} and \code{b} (both lists of strings) to
\code{Series}. Under the hood, a list of strings is a contiguous block of memory
in which every element is a pointer to the string. A \code{Series} of strings is
also a contiguous block of memory in which every element is a pointer to a
string. So, the conversion from the one to the other is cheap. However, in the
\code{pandas} version, the elements are stored together in lists ``horizontally'',
but we want to store them together ``vertically'' (if we imagine a matrix where
every row is a list coming from \code{split}), which is expensive.

In this example, the rewriter enables us to optimize a library \emph{without
changing the library}. As we have explained earlier, the rewriter can cross
library boundaries and thus it can optimize across Python, \code{pandas} and \code{numpy},
without the need to provide custom versions of these libraries.

% See the following call stack on where str.split() ends up.
% - split: https://github.com/pandas-dev/pandas/blob/91111fd99898d9dcaa6bf6bedb662db4108da6e6/pandas/core/strings/accessor.py#L866
% - _str_split: https://github.com/pandas-dev/pandas/blob/91111fd99898d9dcaa6bf6bedb662db4108da6e6/pandas/core/strings/object_array.py#L340
%   - See also the next line.
%   - expand is not used in this function.
% - _str_map: https://github.com/pandas-dev/pandas/blob/91111fd99898d9dcaa6bf6bedb662db4108da6e6/pandas/core/arrays/string_.py#L569
% - _map_infer_mask: https://github.com/pandas-dev/pandas/blob/91111fd99898d9dcaa6bf6bedb662db4108da6e6/pandas/_libs/lib.pyx#L2863
%   - This is in Cython.

% Once this finishes, we call _wrap_result here: https://github.com/pandas-dev/pandas/blob/91111fd99898d9dcaa6bf6bedb662db4108da6e6/pandas/core/strings/accessor.py#L867
% This is defined here: https://github.com/pandas-dev/pandas/blob/91111fd99898d9dcaa6bf6bedb662db4108da6e6/pandas/core/strings/accessor.py#L255
% Eventually, we go here: https://github.com/pandas-dev/pandas/blob/91111fd99898d9dcaa6bf6bedb662db4108da6e6/pandas/core/strings/accessor.py#L353
% And finally note _constructor_expanddim is probably this: https://github.com/pandas-dev/pandas/blob/91111fd99898d9dcaa6bf6bedb662db4108da6e6/pandas/core/series.py#L558


% All that means that essentially, what's happening is the following:
% a = []
% b = []
% n = len(arr)
% res = np.empty(n, dtype=arr.dtype)
% for i in range(n):
%   spl = arr[i].split(',', maxsplit=1)
%   res[i] = spl

% # Not sure about the following
% res = list(res)
% x = pd.DataFrame(res, columns=['a', 'b'])


% So, we split the strings and we gather them into a numpy array of lists. Then, we construct a DataFrame
% from these lists, splitting them apart.

% I expected the actual thing to be faster than this because the loop is in Cython. Nonetheless, it's slower, although this
% one is much closer. I guess it's slower because of arbitrary Pandas overheads in checking conditions and all that.

% My assumption about the slowness compared to our rewritten version is the number
% of allocations. In our version, we allocate only two lists (and the split
% strings which are allocated in both versions). Also, Python lists are extendible
% arrays (i.e., vectors), which means translating them to numpy arrays should be trivial.

% On the other hand, in the original version, we first compute a whole numpy array
% _of lists_, which means _many_ allocations. When converting to a DataFrame, it
% probably means also a lot of deallocations. And it's of course not as cheap to separate
% these lists vertically as it is with our rewritten version.










\revis{
\subsection{Comparing Various Dataframe Libraries}
\label{sub-sec:cmp-libs}

To further understand how \code{modin} and other dataframe libraries perform on ad-hoc
EDA workloads, we perform a series of targeted experiments using common patterns
we have found in such workloads. In addition to studying \code{modin}, we also
study three other common dataframe libraries: \code{dask} \cite{dask} (version
2022.12.1), Koalas \cite{koalas_web} (version 0.32.0), and PolaRS
\cite{polars_web} (version 0.7). \code{dask} is another widely adopted parallel
dataframe library with a slightly different API from that of \code{pandas}. Koalas
implements the \code{pandas} API over PySpark \cite{pyspark_web}.  PolaRS
\cite{polars_web} is a \code{pandas} replacement (using Rust under the hood), which,
however, has a different API.

\minihead{Setup and Dataset} We use a \texttt{c5.24xlarge} AWS instance with 96 vCPUs and
192 GiB of RAM. We use 12 vCPUs for \code{modin}, \code{dask}, Koalas and
PolaRS. The dataset used is the NYC Yellow Taxi Dataset 2015 - January
\cite{nyc_taxi_dataset} (except for one case mentioned below) with a size of
around 1.8GB. We picked this dataset because (a) it is large (the subset we use
is the largest we could run the experiments with, using the libraries mentioned,
on this machine) and these libraries specialize in large datasets and (b) it has
been used in previous work \cite{modin} and in multiple notebooks throughout the
Internet \cite{nyc_taxi_kaggle}.

\input{figures/alt_ops.tex}

\minihead{Operations} We tested several common patterns found in \code{pandas}
workloads, and which are expected to be pretty fast with \code{pandas}
\footnote{For example, \code{pandas} columns are stored as \code{numpy} arrays
and many \code{pandas} operations use \code{numpy}. So, we expect interacting
with \code{numpy} to be quite efficient.}. In particular, we tested column-wise
operations, interacting with \code{numpy}, and an iterative access of individual
elements. Figure~\ref{fig:alt_ops} gives examples of the former two.
Figure~\ref{fig:for_loop} gives an example of the latter.

\input{figures/tbl_pandas_alt.tex}

\paragraph{\textbf{Results and Discussion}} A subset of our results (for a
single example of each category) is shown in Table~\ref{tbl:pandas_alt}. As is
evident, bulk-parallel dataframe libraries like \code{modin}, \code{dask} and
Koalas, are not well suited for ad-hoc EDA workloads. We should note, however,
that we observed that PolaRS was significantly more judicious with the resources
compared to the other three (especially for memory), and its slowdowns are much
smaller. However, it can still cause considerable slowdowns (e.g., with the
iterative element access) and has a considerably different API. For example, the
\code{pandas} snippet \code{df['A'] = 1} is translated to 
}

\begin{minted}[bgcolor=light-gray]{python}
df = df.with_column(pl.lit(1).alias('A'))
\end{minted}

\noindent
\revis{in PolaRS. As a result, it requires learning new syntax.}
