\documentclass{article}



\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}



\title{Comparing Conventional and Conversational Search Interaction using Implicit Evaluation Methods \thanks{Kaushik, A. and J. F. Jones, G. (2023). Comparing Conventional and Conversational Search Interaction Using Implicit Evaluation Methods. In Proceedings of the 18th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications - HUCAPP, ISBN 978-989-758-634-7; ISSN 2184-4321, SciTePress, pages 292-304. DOI: 10.5220/0011798500003417}}






%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{ \href{https://orcid.org/0000-0002-3329-1807}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Abhishek Kaushik}\thanks{Now at Dundalk Institute of Technology, Dundalk, Ireland.} \\
	ADAPT Centre, School of Computing \\ 
  Dublin City University \\
  Dublin 9, Ireland \\
	\texttt{abhishek.kaushik2@mail.dcu.ie} \\
	%% examples of more authors
	\And
	\href{https://orcid.org/0000-0003-2923-8365}{\includegraphics[scale=0.06]{orcid.pdf}\hspace{1mm}Gareth J. F. Jones} \\
 	ADAPT Centre, School of Computing \\ 
  Dublin City University \\
  Dublin 9, Ireland \\
	\texttt{Gareth.Jones@dcu.ie} \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}

% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
\renewcommand{\headeright}{}
\renewcommand{\undertitle}{}
\renewcommand{\shorttitle}{}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
%\hypersetup{
%pdftitle={A template for the arxiv style},
%pdfsubject={q-bio.NC, q-bio.QM},
%pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
%5pdfkeywords={First keyword, Second keyword, More},
%}

\begin{document}
\maketitle

\begin{abstract}
	Conversational search  applications offer the prospect of improved user experience in information seeking via agent support. However, it is not clear how searchers will respond to this mode of engagement, in comparison to a conventional user-driven search interface, such as those found in a standard web search engine. We describe a laboratory-based study directly comparing user behaviour for a conventional search interface (CSI) with that of an agent-mediated multiview conversational search interface (MCSI) which extends the CSI.
%Subjects performed identical cognitive engaging search tasks on the MCSI(MCSI).  In this study, we investigated the user behaviour on standard search interface to the conversational search interface in 
User reaction and search outcomes of the two interfaces are compared using implicit evaluation using five analysis methods:
%different domain such as
%subjects 
claiming to have a better search experience in contrast to a corresponding standard search interface.
\end{abstract}


% keywords can be removed
%\keywords{First keyword \and Second keyword \and More}
\keywords{Conversational Search Interface \and Conventional Search \and User Satisfaction \and Human computer interaction \and Information Retrieval}

\section{Introduction}
\label{Introduction}
%\vspace{-1exx}
The growth in networked information resources has seen search or information retrieval become a ubiquitous application, used many times each day by millions of people in both their work and personal use of the internet. For most users, their experience of search tools is dominated by their use of web search engines, such as those provided by {\em Google\/} and {\em Bing\/}, on various different computing platforms. 
%These applications require the user to create a search query to describe their information need, enter this into the search engine, and engage with retrieved documents seeking information to satisfy their information needs. 
Users lack of knowledge on the topic of their information need often means that they must perform multiple search iterations. This enables to learn about their area of investigation and eventually 
%in order 
to create a query which sufficiently describes their information need which is 
%to be 
able to retrieve relevant content. The search process is thus often cognitively demanding on the user and inefficient in terms of the amount of work that they are required to do.

%In addition to well established search engines, there has in recent years been a significant growth in interest in {\em Chatbots\/}, which enable a user to engage in a dialogue with an online service, often providing access to FAQ lists relating to an organisation and its services. Related to this these is also growing interest in online question answering technologies which seek to provide a user with a direct answer to a question, rather than requiring them to engage with the documents retrieved by a search engine to address their information need.

Bringing together the needs of users to search unstructured information technologies and advances in artificial intelligence, recent years have 
%also 
seen rapid growth in research interest in the topic of {\em conversational search (CS)\/} systems \cite{radlinski2017theoretical}. 
%Such systems are envisaged to enable a user to engage in conversational engagement with the search engine which guide and support their search activities to reduce the cognitive demands on the user and to improve search efficiency. 
CS systems assume the presence of an agent of some form which enables a 
%the 
dialogue-based interaction between the searcher and the search engine to support the user in satisfying their information needs \cite{radlinski2017theoretical}. 
%While there has been much discussion of the potential of CS methods, there is little work reporting on the investigation of operational conversational prototypes, and in particular how these compare with conventional search systems used to perform the same search task. Those 
Studies of CS to date have 
%appeared 
generally adopted a human ``wizard'' in the role of the search agent \cite{Trippas:2017:PIC:3020165.3022144,avula2018searchbots}. 
These studies have been conducted in CS systems with the implicit assumption that an agent can interpret the searcher's actions with human like intelligence. In this study, we take a alternative position using an automatic rule-based agent to support the searcher in the CS interface and compare this with the effectiveness of a similar CSI to perform the same search tasks. In this study, we introduce a desktop based prototype MCSI to a search engine API %, shown in operational example videos at link1 \footnote{\url{https://drive.google.com/open?id=1AoS5Nrnj7nGrPIsRAiA96ttwvzzwkpCK}}. 
Our interface combines a CS assistant with an extended standard graphical search interface. 
%The interface agent takes the form of a personal assistant which works beside the user, rather than sitting between the user and the search engine \cite{Maes:1994:ARW:176789.176792}. 
The goals of our study include both better understanding of how users respond to CS interfaces and automated agents, and how these compare with the user experience of a CSI for the same task.

The ubiquity of CSIs means that users have well established mental models of the search process from 
%their experience of 
their use of 
%using 
these tools. With respect to this, it is important to consider that it has been found in multiple studies that subjects find it difficult to adapt to new technologies, especially when dealing with interfaces \cite{krogsaeter1994user}. Thus, when presented with a new type of interface for an equivalent search task, it is interesting to consider how users will adapt and respond to it.

Previous studies of CS interfaces have focused on chatbot type interfaces which limit the information space  of the search \cite{avula2018searchbots,avula2020wizard}, and are very different from conventional graphical search interfaces. Search via engagement with a chat type agent can result in the development of quite different information-seeking mental models  to those developed in the use of standard search systems, meaning that it is not possible to directly consider the potential of CS in more conventional search settings based on these studies.
%\cite{kaushik2019dialogue}. 
We are interested in this study to consider how user mental models of the search process from CSIs will response in a CS conversational setting to enhance the user search experience.

For our study of conversational engagement with a search engine and contrasting it with more conventional user-driven interaction, we adopt a range of implicit evaluation methods. Specifically we use cognitive workload-related factors (NASA Load Task) \cite{hart1988development}, psychometric evaluation for software \cite{lewis1995ibm}, knowledge expansion \cite{wilson2013comparison} and search satisfaction \cite{Abhi}. Our findings show that users exhibit significant differences in the above dimensions of evaluation when using our MCSI and a corresponding CSI.

The paper is structured as follows: Section \ref{recent} overviews existing work in conversational engagement and its evaluation, Section \ref{method} describes the methodology for our investigation, Section \ref{procedure} provides details of our experimental procedure and our results and includes analysis, findings and hypothesis testing and Section \ref{conclude} concludes.

\section{Related work}
\label{recent}
%\vspace{-1ex}
In this section, we provide an overview of existing related work in conversational interfaces, conversational search and relevant topics in evaluation. 

\subsection{Conversational Interfaces}
%\vspace{-1ex}
Conversational interaction (CI) with information systems is a longstanding topic of interest in computing. However, activity has increased greatly in recent years. The key motivation for examining CI is the 
%to 
development of interactive systems which enable users to achieve their objectives using a more natural mode of engagement than cognitively demanding traditional user-driven interfaces. %This is intended to be less demanding primarily focused on functional requirements, which are 
Such user-driven interfaces require users to develop 
%new 
mental models to use them  reliably. Recent research on CI has focused on multiple topics including mode of interaction, the intelligence of conversational  agents, the structure of conversation, and dialogue strategy \cite{mctear2016conversational,Kader2015,roller2020recipes}. Progress in CI can be classified in four facet areas: smart interfaces, modeling conversational phenomena, machine learning approaches, and toolkits and languages \cite{singh2019,braun2019,araujo2020conversational}. 
%Smart interfaces are responsible for handling  multimodal engagement with the user, and seek to provide the response in the appropriate modality. The issue of multimodality has featured in the development of many chat interfaces. For example, the University of Rochester built a chat interface which engages with the user to solve complex problems \cite{allen2001toward,James1995}. 
%A number of studies have been conducted to understand the effect of multimodality in CIs \cite{roller2020recipes}. %With respect to the modeling of conversational phenomena, traditional conversational interfaces depend on turn-by-turn engagement where the system and the user wait for each otherâ€™s response. This began the process of understanding human behaviour, the expectation of the next utterance and generating corresponding responses. %Multiple studies have thus been conducted to understand the incremental processing of knowledge and prediction of next utterances in engagement between users and chatbots \cite{clark_1996,levelt1989speaking,tanenhaus2004line}.

Current chatbot interfaces have evolved, in common with many areas, from rule-based systems to the use of data driven approaches using machine learning and deep learning methods \cite{nagarhalli2020review}. Toolkits have been developed to support 
%help users to 
the construction and testing of chatbot agents for particular applications. The majority of research on conversational agents has focused on question answering and chit chat (unfocused dialogue) systems. Only very limited work has been done on information-seeking bots, dating mainly from the early 1990s \cite{stein1993conversational}. 
%Interest in more general conversational search applications has increased greatly in recent years, but much of this work has focused on conceptual issues, and their remains little work examining interfaces for user engagement with respect user experience, knowledge gain, software usability etc \cite{sun2018,zhang2018towards}. 
One recent example of a multimodal conversational search is presented in our earlier work \cite{CHII2020Abhi}. This enables 
%which allows the 
a user to explore long documents using a multi-view interface. Our current study is focused on evaluation of this interface in comparison to a CSI.

\subsection{Conversational Search}
%\vspace{-1ex}
While users of search tools have become accustomed to standard ``single shot'' interfaces, of the form seen in current web search engines, interest in the potential of alternative conversational search-based tools has increased greatly in recent years \cite{radlinski2017theoretical}. Traditional search interfaces have significant challenges for users, in requiring them to express their information needs in fully formed queries, although users have generally learned to use them to good effect. The idea of agent-support conversational-based interaction supporting them in the search process is thus very attractive. 
Multiple studies have been conducted to investigate the potential of conversational search in different dimensions. These studies however have generally involved use of a human in the role of an agent wizard \cite{avula2020wizard,avula2018searchbots,avula2019embedding}. 
%The subjects were given the impression that they were interacting with a machine. While the results of these studies have been interesting and insightful, they have an important limitation in that the agent  has full human intelligence. Thus they do not reveal the potential for artificial agents to support search in terms of effectiveness and user acceptance. Studies have also been conducted to investigate the user search behaviour in speech settings where the searcher interacts with the agent (the human ``wizard'') via speech. 
These have the limitation of assuming both human intelligence and error free speech recognition, which will generally not be the case in a real system \cite{Trippas:2017:PIC:3020165.3022144,Trippas2018}. 

Some studies on conversational search have been based completely on a data-driven approach using machine learning methods to extract a query from multiple utterances. The drawback of this approach is that the dialogues are not analyzed based on incremental learning over multiple conversations \cite{nogueira2017task,bowden2017combining}. Other types of studies have developed agents by using an intermediate approach in which a combination of rules is used to form a dialogue strategy from users search behaviour \cite{de1994information} \cite{Abhi}, which guide the user in conversations with the support of a pretrained machine learning model to extract the intent and entities from utterance. %Intent helps the agent to understand the user's requirement and to support them in information seeking \cite{kaushik2019dialogue}. 
We follow this last approach in our multiview prototype to understand the user search experience in a conversational setting.

\subsection{Evaluation}
%\vspace{-1ex}
%The evaluation of conversational systems is broadly divided into three categories: 
%individual evaluation, task-based evaluation and automatic evaluation  using measures such as the F1 score \cite {zuva2012evaluation,McTear2016,Reiter2009}. 
%For evaluation of conversational systems, the Paradise framework was developed to evaluate task success (task completion), conversation efficiency (task duration, the dialogue turns), conversation quality (response accuracy and its latency), and user satisfaction (ease of the task, user behaviour, and future use) \cite{walker1997paradise}. Studies conducted on conversational agents \cite{vtyurina2017exploring,vtyurina2018exploring,mcdumisc} to explore the potential of conversational search where evaluation is limited to user experience or cognitive load.    
Currently, there is no standard mechanism for evaluation of conversational search interfaces. %Evaluation of these interfaces is actually a highly complex topic involving multiple dimension including the user's background knowledge of the search topic, their familiarity with the agent, etc. 
In this 
%the current 
study, 
%we divide the evaluation of our conversational search interface 
we adopt implicit measures in five dimensions \cite{Abhievaluation} user search experience \cite{Abhi}, knowledge gain \cite{wilson2013comparison}, cognitive and physical load \cite{hart1988development}, user interactive experience \cite{UEQ} and usability of the interface software \cite{lewis1995ibm}.

\section{Methodology}
\label{method}
%\vspace{-1ex}
In this section, we describe the details of our user study which aims to enable us to observe and better understand and contrast the behaviour of searchers using a CSI and our prototype MCSI. This section is divided into two subsections: interface design and experimental setup.

\subsection{Prototype Conversational Search System}
%\vspace{-1em}

\begin{figure*}[!ht]
  \includegraphics[scale=0.46]{figures/Ann_MCSI_interface.jpg}
%  \includegraphics[width=12cm,]{figures/CHATBOX.jpg}
 \centering
  \caption{Conversational Agent incorporating:
  chat display, chat box, information box, query box with action buttons for Enter and Clear, and retrieved snippets and documents. Green outline indicates the MCSI setting and red block indicates the CSI setting.}
  \label{GUI}
\end{figure*}

In order to investigate user response to search using a MCSI and to contrast this with a comparable CSI 
%conventional search inference 
with the same search back-end, we developed a fully functioning prototype system, shown in Figure \ref{GUI}.
%Our prototype \cite{Anonymous} MCSI 
%multi-faceted interface for conversational search 
%is shown in Figure \ref{GUI}. 
The interface is divided into two distinct sections. The righthand side which corresponds to a standard CSI, 
%with which the user can interact, 
and the lefthand side which is a text-based chat agent and 
%which 
interacts with both the search engine and the user. Essentially the agent works alongside the user as an assistant, rather than being positioned between the user and the search engine \cite{Maes:1994:ARW:176789.176792}.  
%This is divided into two parts: Web Interface, with which the user engages, and Logical System, which is responsible for conversation and search management .

The Web interface components are implemented
%developed 
using the web python framework flask and with HTML, CSS, and JS toolkits. The agent is controlled by a logical system and is implemented using Artificial Intelligence Markup Language (AIML) scripts. These scripts are used to identify the intent of the user, to access a
%the 
spell checking API \footnote{https://pypi.org/project/pyspellchecker/}, and are responsible for search and giving responses to the users. %AIML is an XML based markup language used to create AI applications related to natural language processing.  
Since the focus of this study is on the functionality of the search interface, the search is carried out by making calls to the Wikipedia API.
%is used to support the search process. 
%The interface includes use of an algorithm 
%%is designed 
%which highlights important 
%%and diverse 
%segments within long documents associated with queries 
%%used 
%to enable the searcher to skim through them. 
The interface includes multiple components, as discussed in detail in our previous work \cite{CHII2020Abhi}

%following components: \\
%Chat Component

%\begin{enumerate}
%\item{Chat Display:} Shows the conversational dialogue between the search agent and the user.
%\item{Chat Box:} Enables the user to insert chat. 
%\item{Information Box:} Displays significant features of retrieved documents for the query. For example, sentences showing significant diverse information from the documents or relating to alternative interpretations of the query, intended to provide the searcher with information to refine their query before searching again. 
%Currently, the information box 
%shows the summary generated by the Wikipedia API. 
%This could be customized to fit requirements of the conversational search process.
%\end{enumerate}
%Search Component
%\begin{enumerate}
%\item{Query Box:} Enables the user to use traditional search methods by entering their own query or one suggested by the chat agent.
%\item{Retrieval Results:} Display of returned document snippets and document contents.
%\end{enumerate}
%Action Button: Enables the following operations:
%\begin{enumerate}
%  \item Enter:  Enter the text or query in the Chat Box or Search Box
%  \item Clear: Clear the text from the screen
%\end{enumerate}

%The search agent communicates with the user via the Chat Box. The agent performs various potential actions which includes seeking clarification of ambiguous queries, suggesting words to improve the query, and possibly to answer questions.

%The search assistant can accept the following user commands:
%operations:
%\begin{enumerate}
 %   \item Full Doc: 
    %The full doc would 
 %   Opens the full document with 
    %the 
 %   highlighting of 
    %the 
 %   important sections.
%    \item More Info: 
    %The function would 
%    Displays all 
    %the 
 %   subsections of the selected document.
 %   \item Go Back: 
    %This function would 
   % Displays the previous results.
%    \item More Results: 
    %This function would 
%    Displays 
    %the 
%    more search results.
%    \item More About: 
    %This function would 
 %   Shows the overall summary from Wikipedia of the selected document. 
 %   \item Option: 
    %This function would 
 %   Provides detailed text about 
    %the 
 %   selected specific sub-sections.
    %selected. 
%\end{enumerate}

\subsubsection{Dialogue Strategy and Taxonomy}
%\vspace{-1ex}
After exploring
%the 
user search behaviour \cite{Abhi} and dialogue systems, %\cite{sitter1992modeling,stein1993conversational,loisel2009modeling,leech2003generic}, 
we developed a dialogue strategy and 
%prospective 
taxonomy 
%and strategy 
%was designed 
to support 
%the 
CS. The dialogue process is divided into three phases and four states as discussed in detail in our previous work \cite{CHII2020Abhi} %as shown in Figure \ref{fig:Dialogue strategy}. 
The 
%All
three phases include: % processes:
\begin{itemize}
    \item Identification of the information need of the user,
    \item Presentation of the results in the chat system,
    \item Continuation of the dialogue until the user is satisfied or aborts the search.
\end{itemize}
%the a process 

%would like to abort in the middle. 
%\par 
The agent can seek confirmation
%reconfirm 
from the user, if the query is 
%was 
not clear, it can 
%. The agent can 
also correct the query by using the spell checker and reconfirm the query from the user to make the process precise enough to provide 
%the 
better results. The agent can also highlight 
%the diverse 
specific information in
%from the 
long documents to help the user to direct their 
%the user's 
attention to potential important content.
%for the query reformulation or expansion. % \textcolor{red}{Try to put the flow}. 

The user always has the
%have an 
option to interrupt the ongoing communication process by entering a
%the 
new query directly into the Query Box. The communication finishes 
%always 
by the user 
%initiative to 
ending the search
%whole process 
with success 
%(satisfaction of their 
%fulfilling the 
%information need) 
or with failure to address their information need. 

%The dialogue strategy is structured as follows: %(Figure \ref{fig:Dialogue strategy}). % and Figure \ref{fig:flowDialogue strategy}):
%\begin{enumerate}
 %   \item Initial Phase: 
    %The initial phase 
 %   This consists of 
    %contain 
%    three dialogue states:
    %such as 
%    Initiative Directive (includes: request information, offer and request directive), Reactive Directive, and Declarative.
    %and Declare. 
    %This phase is generally when the user starts the search process.
    %with prototype.
  %  \item Intermediate Phase: 
    %The intermediary phase 
 %   This starts after the searcher executes a search query. This consists of the following states: 
    %like 
%    Initiative Directive, Reactive Assertive (answer, accept and refuse), and Reactive Directive.
%    \item Final phase: This is the closure phase where the user 
    %to 
%    ends the engagement by being declarative (Want nothing, Bye).
%\end{enumerate}
\subsubsection{System Workflow}
%\vspace{-1ex}
The system workflow is divided into two sections: Conversation Management and Search Management, dicussed in detail in our previous work. %as shown in Figure \ref{fig:workflow}. 

\begin{enumerate}
\item Conversation Management:
\label{Conversation Managment}This includes a Dialogues Manager, a Spell Checker and connection to the Wikipedia API. The Dialogue Manager validates the user input and 
%decides to 
either sends it to the AIML scripts or self-handles it, if 
%assuming 
the user input misspelt or incorrect. We use AIML scripts to implement the response to the user.
%as per the dialogue strategy. 
%If the 
The system response to user input 
%is 
directed to the AIML scripts 
%then the system response 
is determined by %according 
the AIML script, which can further classify the user's intent. The two major categories of intent are: {\em greeting\/} and {\em search\/}. The greeting intent is responsible for initializing, ending the conversation and system revealment. The search intent is responsible for directing the user input to the spell checker or wikipedia API and transferring
control to search management. The Spell Checking module is responsible for checking the spelling of the query and asking for suggestions from the user (for an example: If the user searches for ``viusal'' then the system would ask: Do you mean "visual"?). Once the user confirms ``yes'' or ``no'', then the query is forwarded to the Wikipedia API. 

\item Search Management:
\label{Search Management}
This is responsible for search and display of the top 3 search results. 
%to the user in the Retrieval Results box. 
The user may also look for more sub-sections from a selected document. 
%It can give detailed information about the option selected or display more results. 
Search management also has an 
%the 
option to display 
%look at 
the full document. This opens a display 
%long document 
with important sections with respect to the query highlighted. The criteria for an important section is based on a Custom Algorithm which extracts important sentences based on a TF-IDF score for each sentence by selecting the top-scoring sentences. The top 30\% of extracted sentences are divided into clusters 
by Density based Clustering (DBSCAN)
to extract  diverse segments (combination of the sentences). Important segments are selected from these segments by using a cosine similarity score with the query. 

\end{enumerate}

%\begin{figure}[!ht]
  %  \centering
   % \includegraphics[scale=0.3]{figures/Dialogue.PNG}
  %  \caption{Overview of conversational dialogue strategy.}
 %   \label{fig:Dialogue strategy}
%\end{figure}

%\begin{figure}
 %   \centering
  %  \includegraphics[width=8cm, height=6cm ]{figures/full_doc.png}
  %  \caption{Full Document with Highlighted text}
  %  \label{fig:full Doc}
%\end{figure}

\subsubsection{User Engagement}
%\vspace{-1ex}
%The prototype user interface 
%described above 
%provides 
The user can 
%with the flexibility to 
interact with both the search agent assistant and directly with the search engine. 
%The system allows 
%The user can
%%to 
%explore a chosen document by exploring multiple subtopics from the document which %enables the user to select and read interesting topics.  
If the user commences a search from the Retrieval Results box,
%Information Box, then 
the assistant initiates a dialogue to assist them in the search process. The system also provides support 
%to the user to support them 
to the user in reading
%who is eager to read a 
full documents. As described above, important sections in long documents are highlighted to ease reading and reduce cognitive effort. 

\subsubsection{Review of Long Documents}
%\vspace{-1ex}
%\begin{figure}[!ht]
 %   \centering
 %   \includegraphics[scale=0.25]{figures/workflow.jpg}
 %   \caption{Workflow of the Conversational Model}
 %  \label{fig:workflow}
%\end{figure}

In our
%the 
study 
%conducted 
reported in \cite{Abhi}, we note users can spend 
%found that one type of user spends 
considerable time reviewing 
%reading 
long documents.
%to satisfy their information need. We suggested supporting 
Our MCSI aims to
%these 
support these users and reduce their required effort by highlighting important segments with respect to the user's query, as described above.
%in these documents to the user, in to facilitate reducing the cognitive load on the user and to provide them with a more efficient search experience.
%This proposal is implemented into our prototype as described above, 
This facility also provides the user with the opportunity to explore subsections within a document instead of needing to read a full long document. 
%This setting helps the user to put less effort into engaging with these documents. 
%to satisfy their information need which justifies the objective of conversational search.  

\subsubsection{Conventional Interface}
%\vspace{-1ex}
To enable direct comparison with our MCSI, a CSI for our study was formed by using the MCSI 
%conversational interface 
with the agent panel removed and the document highlighting facilities disabled. The searcher enters their query in the query box, 
%and
document summaries are returned by the Wikipedia API, and full documents can be selected for viewing. 
%as viewed to satisfy the user's information need.

\subsection{Information Needs for Study}

%\vspace{-1ex}

\begin{figure}[t] % [!ht]
%\vspace{-2mm}
\vspace{-2mm}
\begin{flushleft}
\small
    {\tt It is late, but you can't get to sleep because a sore throat has taken hold and it is hard to swallow. You have run our of cough drops, and wonder if there are any folk remedies that might help you out until morning.}
\end{flushleft}
%    \vspace{-2mm}
    \caption{Example backstory from UQV100 test collection.}
    \label{backstory}
   \vspace{-1mm}
\end{figure}

%Figure \ref{backstory} shows an example backstory selected for use in our study. We chose to use the UQV100 backstories since they relate to more recently created queries for web search tasks.

For our investigation, 
%search tasks 
we wished to give searchers realistic information needs which could be satisfied using a standard web search engine. In order to control the form and detail of these, we decided to use a set of information needs specified within {\em backstories\/}, e.g. as shown in Figure \ref{backstory}. The backstories that we selected were taken from the UQV100 test collection \cite{Bailey:2016:UTC:2911451.2914671}, whose 
%which is based 
cognitive complexity is based on the Taxonomy of Learning \cite{krathwohl2002revision}. We decided to focus on the most cognitively engaging backstories, {\em Analyze\/} type, in the expectation that these would require the greatest level of user search engagement %activity 
to satisfy the information need. 
%, from which they adopted three of the five cognitive dimensions:. %This consists of 100 backstories created from the TREC 2013 and 2014 Web tracks. In previous work, the creators of the UQV100 test collection developed an earlier collection of backstories in which they classified search tasks according to their level of cognitive complexity based on the Taxonomy of Learning \cite{krathwohl2002revision}, from which they adopted three of the five cognitive dimensions:
%{\em Remember\/}, {\em Understand\/} and {\em Analyze\/} defined as follows: 
%\begin{itemize}
%\item Remember: retrieving, recalling and recognizing the information from memory.

%\item Understand: developing the meaning of verbal and non-verbal communication through exemplifying, explaining, interpreting, classifying, comparing and inferring.

%\item Analyze: dividing the problem into small parts, examining how each part relates to each other, and to an overall structure through breaking, organizing, and attributing.

%\end{itemize}

%The search tasks in this earlier study were based on TREC tasks from 2002, 2003 and 2004, and were labeled with their level of cognitive complexity \cite{bailey2015user}. 
%While all three types may benefit from search using conversational interaction, we decided to focus on the most cognitively engaging {\em Analyze\/} type in the expectation that this would require a greater level of activity to satisfy the information need.
Since the UQV100 topics were not provided with type 
%these 
labels, we selected a suitable subset as follows. The UQV100 topics were provided labeled with estimates of the number of queries which would need to be entered and the number of documents that would need to be accessed in order to satisfy the associated information need. We used the product of these figures as an estimate of the expected cognitive complexity, and then manually selected 12 of the highest scoring backstories that we rated as the most suitable for use by general web searchers, e.g. not requiring specific geographic knowledge or of specific events.

\subsection{Experimental Procedure}
%\vspace{-1ex}
%As shown in Figure \ref{fig:Experiment Procedure}, 
Participants in our study had to complete search tasks based on the backstories using the MCSI and CSIs. Sessions were designed to assign search tasks and use of the alternative interfaces arranged to avoid potential sequence-related biasing effects. Each session consisted of multiple 
%were required to complete a search session consisting of multiple 
backstory search tasks.
%based on our selected backstories. 
%As part of their search session they 
While undertaking a search session, participants were required to 
%had to 
complete a pre- and post task search questionnaires.
%before and after undertaking each task. 
In this section we first give details of the practical experimental setup, then outline the questionnaires, and finally describe a pilot study undertaken to finalise the design of the study.
%and then the experimental setup and the procedures followed.

%\begin{figure}[!ht]
 %   \centering
  %  \includegraphics[scale=0.22]{figures/Chapter6_MICS.jpg}
   % \caption{Experiment procedure for information seeking tasks to investigate CSI and MCSI interface.}
   % \label{fig:Experiment Procedure}
%\end{figure}

\subsubsection{Experimental Setup}
%\vspace{-1ex}
Participants used a setup of two computers arranged with two monitors side by side on a desk in our laboratory. One monitor was used for the search session, and the other to complete the online questionnaires. Participants carried out their search tasks accessing 
%using a standard 
the Wikipedia search API using our interfaces running using a Google chrome browser. In addition, all search activities were recorded using a standard screen recorder tool to enable post-collection review of the user activities. Approval was obtained from our university Research Ethics Committee prior to undertaking the study. 
%the data collection. 
Participants were given printed 
%details of the 
instructions for their search sessions. %and each backstory in printed form at the beginning
%of 
each task. 

\subsubsection{Questionnaires}
%\vspace{-1ex}
Participants completed two questionnaires for each search task.
%the participant completed a 
The questionnaire was divided into three sections:
\begin{itemize}
\item{{\em Basic Information Survey\/}: Participants entered their assigned user ID, age, occupation and task ID.}
\item{{\em Pre-Search\/}: Participants entered details of their pre-existing knowledge with respect to topic of the search task to be undertaken.}
\item{{\em Post-Search\/}: Post-search feedback from the user including their search experience, knowledge gain, and writing the post-search summary.}
\end{itemize}

%While conducting their search 
%the 
%users had to 
%Participants completed %an online 
The questionnaire was completed online in a Google form.
%while undertaking their search activities. 

\subsubsection{Pilot Study}
%\vspace{-1ex}
A pilot study was conducted with two undergraduate students in Computer Science using two additional backstory search tasks. This enabled us to see how long it took them to complete the sections of the study using the CSI
%in standard search (same as MCSI but the chat-box was disable) a
and the MCSI, to gain insights into the likely behaviour of participants, and to generally debug the experimental setup.

Each of the pilot search tasks took around 30 minutes to complete. Feedback from the pilot study was used to refine the specification of the questionnaire. Results from the pilot study are not included in the analysis. 


\begin{table}[t]%[!ht]
\centering
\tiny
\setlength{\tabcolsep}{3pt} % Default value: 6pt
\begin{tabular}{c|cccc}
\hline  \hline
\textbf{Task Load} &
  \textbf{CSI} &
  \textbf{MCSI} &
  \textbf{Percentage} &
  \textbf{P-Value} \\ 
  \textbf{Index} &
  
   \textbf{Mean} & 
   \textbf{Mean} &
  \textbf{Change} &
  \\ 
  \hline
  \hline
Mentally Demanding &
  4.16 &
  3.68 &
  11.54 &
   .273795 \\ \hline
Physically Demanding &
  3.12 &
  2.76 &
  11.54 &
.441676 \\     \hline
Hurried or Rushed &
  3.34 &
  2.76 &
  14.81 &
   .213878\\     \hline
Successful Accomplishing &
  4.28 &
  5.32 &
  -24.3 &
   .016199 \\     \hline
How hard did you have  & & & & \\ to work to accomplish? &
  4.44 &
  3.96 &
  10.81 &
  .270243 \\     \hline
How insecure, discouraged, irritated, & & & & \\ stressed, and annoyed were you?&
  3.32 &
  2.40 &
  27.71 &
.071443 \\ \hline  \hline
\end{tabular}
\caption{Task load index to compare the load on user while using both the systems (MCSI and CSI) with independent T two tailed test .}
\label{Task Load Index}
\end{table}

\subsubsection{Study Design}
%\vspace{-1ex}
Based on the result of the pilot study, each participant in the main study was assigned two of the selected 12 search task backstories with the expectation that their overall session would last around one hour. Pairs of backstories for each session were selected using a Latin square procedure. After every six tasks the sequence of allocation of the interface was rotated to avoid any type of sequence effect \cite{bradley1958complete}.

%As per psychological experiment design, 
Each condition was repeated 
%should be repeated at-least 
4 times with the expectation that this would give sufficient results to be able to observe significant differences where these are present. 
%to measure significant results. 
Since there were 12 tasks, this required 
%every task need be repeated 4 times which setup basis for at-least 
24 subjects to participate in the study. 
In total, 27 subjects (9 Females, 18 Males) in the age group of 18-35 participated in our study  (excluding the pilot study), we examined
%considered 
the data of 25 subject, since
%for analysis as 
2 subjects were found not to have followed
%did not
%didn't 
%follow 
the instructions correctly. The study was
%has been 
conducted in two phases. 
%where 
Each 
%The 
user had to perform a different
%the 
search task using the CSI
%in two different settings such as standar
%(same as MCSI but the chat-box was disable) 
and MCSI with the sequencing of their use of the interfaces varied to avoid learning or biasing effects.
%and fill the questionnaire after each setting. 

As well as completing the questionnaires, the subjects also attended a
%the 
semi-structured interview after completion of their session of two tasks
%the whole task in 
using both interface conditions.
%the settings. 
%The study has been investigated and analyzed by quantitative and qualitative methods. Interviews, summary comparison, 
The user actions in the videos and interviews were thematically labelled 
%coded 
by two independent analysts
%based on the users response
%coders 
and Kappa coefficients were
%has been 
calculated (approx mean .85) \cite{Kappa}. 
%The 
Disparities in labels were
`%was
%coding has been 
resolved by mutual agreement between the 
%by 
analysts. 
%coders. 
The interview questionnaire dealt with user search experience, software usability and cognitive dimensions, and
%in the interview  
was
%had been 
quantitatively analyzed.
%that 
Based on the interview analysis, out of 25 participants 92\% 
%of the total subjects 
were happy and satisfied with the MCSI. In all conditions, subjects preferred the MCSI. %Thi
Showing that there is no sequence effect arising from the order of the interfaces in the search sessions. 
%for search. 

%As mentioned in the above section, the study has been investigated into multiple dimensions to test the four different hypotheses on the traditional IR and interface as mentioned in the section \ref{Introduction}.  %The following hypothesis has been tested into multiple areas.
%The hypothesises was as follow:

Each hypothesis of the study was 
%has been 
tested using a T-Test (since
%as then 
the number of samples was less than 31). Each hypothesis was
%has been 
evaluated on a
%the 
number of factors which contribute to the examination in each dimension as discussed below. 
%The age data provided in the Table \ref{Agedeistchap7} is for those participants whose data was used for analysis.
%and fill the questionnaire after each setting. 

%\begin{table}[!ht]
%\footnotesize
%    \centering
 %   \begin{tabular}{c|c|c|c}
 %    \hline \hline 
%Age & No. Male (M) & No. Female (F) & Ratio (F/M) \\  \hline  \hline 
              
 %                       18-25 & 9 & 4 & 4:9 \\
 %              26-35 & 9 &  5 & 5:9\\
       %     \hline 
       %     \hline 
      %      \hline 
 %                    \textbf{Total} & 18 & 9 & 1:2 \\ \hline \hline 

 %   \end{tabular}
%\caption{Details of age distribution throughout this investigation.}
%\label{Agedeistchap7}
%\end{table}

%As well as completing the questionnaires, the subjects also attended a
%the 
%semi-structured interview after completion of their session of two tasks
%the whole task in 
%using both interface conditions.

\section{Study Results}
\label{procedure}
%\vspace{-1ex}
The MCSI was compared with the conventional interface 
%chat search Interface was compared with the traditional search system into 
using an implicit evaluation method examining multiple dimensions: cognitive load, knowledge gain, usability and search satisfactions.  

%\subsection{Hypothesis}

\begin{table}[t] %[!ht]
\centering
\tiny
\setlength{\tabcolsep}{3pt} % Default value: 6pt
\begin{tabular}{c|ccccc}
\hline  \hline
\textbf{Topic} &
  \textbf{CSI} &
  \textbf{MCSI} &
  \textbf{Percentage} & \textbf{P value} \\ & \textbf{Mean} & \textbf{Mean} & \textbf{Change} &
   \\ \hline   \hline
Easy to use\textsuperscript{*}&
  4.04 &
  5.96 &
  47.52 &
.000059 \\ \hline
Simple to use  &
  4.48 &
  5.92 &
  32.14 &
.003526 \\  \hline
Effectively complete my work\textsuperscript{*} &
  3.92 &
  5.64 &
  43.88 &
  .000226\\  \hline
Quickly complete my work\textsuperscript{*}  &
  3.72 &
  5.76 &
  54.84 &
.00003 \\  \hline
Efficiently complete my work\textsuperscript{*}  &
  3.88 &
  5.76 &
  48.45 &
.000045 \\  \hline
Comfortable using this system\textsuperscript{*} &
  4.16 &
  5.88 &
  41.35 &
.000471 \\  \hline
Whenever I make a mistake using the & & & & \\ system, I recover easily and quickly\textsuperscript{*} &
  4.04 &
  5.44 &
  34.65 &
.006827\\  \hline
The information is clear\textsuperscript{*} &
  4.16 &
  5.92 &
  42.31 &
  .000072 \\  \hline
It is easy to find the information I needed\textsuperscript{*} &
  4.00 &
  5.48 &
  37 &
.000706\\  \hline
The information is effective in & & & & \\ helping me complete the tasks and scenarios\textsuperscript{*} &
  4.20 &
  5.68 &
  35.24 &
  .000675. \\  \hline
The organization of information & & & & \\ on the system screens is clear\textsuperscript{*} &
  4.44 &
  5.92 &
  33.33 &
.000184\\  \hline
The interface of this system is pleasant\textsuperscript{*} &
  4.28 &
  6.08 &
  42.06 &
  .00002\\  \hline
Like using the interface\textsuperscript{*} &
  4.20 &
  6.12 &
  45.71 &
 .000014\\  \hline
This system has all the functions  &  & & & \\
 and capabilities I expect it to have\textsuperscript{*} & 4.08 &
  5.72 &
  40.2 &
   .000168 \\  \hline
Overall, I am satisfied with this system\textsuperscript{*} &
  4.16 &
  5.92 &
  42.31 &
 .000029\\ \hline  \hline
\end{tabular}
\caption{Post Study System Usability Questionnaire (PSSUQ).}
\label{Post-Study System Usability Questionnaire}
\end{table}

\subsection{Cognitive dimensions}
%\vspace{-1ex}

Conventional search can impose a significant cognitive load on the searcher \cite{kaushik2019dialogue}. 
An important factor in the evaluation of conversational systems is measurement of the cognitive load experienced by users.
%while using the system. 
To measure user 
%the user's 
workload, the NASA Ames Research Centre proposed the NASA Task Load Index \cite{hart1988development,Abhievaluation}.
In terms of cognitive load, the user was
%has been 
asked to evaluate the conventional 
%traditional and 
interface and MCSI 
%system 
in
%on 
6 dimensions from the NASA Task Load Index associated with 
%which consist of 
mental load and physical load,%\cite{hart1988development} 
as shown in 
%the 
Table \ref{Task Load Index}.

\begin{enumerate}
    
    \item \textbf{HO: Users experience a similar task load during the search with multiple interfaces}:
    %The user evaluated the system based on 
    %The six parameters  (Table \ref{Task Load Index}). 
    The grading scale of the NASA Task Load Index measure lie between 0 (low) - 7 (High). We 
    %have 
    compared the mean difference of both 
    %the 
    systems on all six parameters. In all 
    %the 
    aspects, subjects experienced lower
    %less 
    task load using the MCSI.
    %conversational 
    %in chatbot 
    %interface. 
    Subjects claimed more success in accomplishing the task using the MCSI.
    %conversational
    %chat 
    %interface, 
    %and 
    %and 
    %the 
    Results for accomplishing the task were 
    %the 
    statistically significantly different. 
    %on accomplishing the task
    %and the 
    Subjects felt less insecure, discouraged, irritated, stressed, and annoyed, while using the MCSI
    %conversational 
    %chat
    %interface 
    with a
    %the 
    significant difference (P$<$0.10). This implies that the null hypothesis was rejected on the basis of the Task Load index. Although 
    %the 
    four factors were not significantly different, 
    %but 
    the mean difference between both the systems on these factors was more than 
    %the 
    10\%. % which conclude that the MCSI put the less cognitive load on the user. This lead to the alternative hypothesis (H1) which 
    This shows 
    %concludes 
    that the user experienced less subjective mental workload while using the MCSI. %Concluding the alternative hypothesis lead to explore certain areas with the help of research questions. % The following RQ has been investigated. 
    %\item \textbf{H1: User experience Less Subjective mental workload while using the chat interface:}
\end{enumerate}

% Please add the following required packages to your document preamble:
% \usepackage[normalem]{ulem}
% \useunder{\uline}{\ul}{}


\begin{table}[t] % [!ht]
\small
    \centering
    \begin{tabular}{l|l}
    \hline
    Parameter & Definition \\
    \hline
   Dqual & Comparison of the quality of facts \\ & in the summary in range 0-3 where 0 \\ 
   & represents irrelevant facts and  \\ &  3 specific details with relevant facts. \\
   \hline 
   Dintrp & Measures the association of facts in  \\ &  a summary in the range 0-2 where \\ 
   & 0 represents no association of the facts and \\ &  2 that all facts \\ & in a summary are associated  \\ &  with each other in a meaning. \\
      \hline 
   Dcrit & Examines the quality of critiques of topic   \\ & written by the author \\ &  in range the 0-1 where 0 \\
   & represents facts are listed with  \\ &  without thought or analysis  \\ & of their value and 1 \\
   & where both advantages and  \\ &  disadvantages of the facts are given. \\
   \hline
    \end{tabular}
    \caption{Summary Comparison Metric \cite{wilson2013comparison}}
    \label{Summary Comparison Metric}
\end{table}


\subsection{Usability}
%\vspace{-1ex}
CS
%Conversational search 
studies generally do not explore the dimensions of 
%the 
software usability. However, it is important to understand the challenges and opportunities of CSs
%conversational systems 
on the basis of software requirements analysis. This allows a system to be evaluated based on real-life deployment and to identify areas  for improvement. Lower effectiveness and efficiency of a software system can increase cognitive load, reduce engagement and act as a barrier in the process of learning while searching \cite{Abhi,vakkari2016searching,kaushik2019dialogue}.
Usability is an
%is one of the 
important evaluation metric of 
%the 
interactive software. IBM Computer Usability Satisfaction Questionnaires are
%is 
a Psychometric Evaluation for 
%the 
software from the perspective of the user \cite{lewis1995ibm} known as the Post-Study System Usability Questionnaire (PSSUQ) Administration and Scoring. The PSSUQ was
%has been 
evaluated using
%into the 
four dimensions:
%such as: 
%the 
overall satisfaction score (OVERALL), system usefulness (SYSUSE), information quality (INFOQUAL) and interface quality (INTERQUAL), which include fifteen parameters. On each dimension, the MCSI 
%conversational 
%chatbot 
%search interface 
outperformed the CSI. The grading scale 
%lines 
lies between 0 (low) - 7 (High).  We 
%have 
compared the mean difference of both 
%the 
systems on all parameters. In all 
%the 
aspects, subjects experienced less task load when using the MCSI ,
%conversational
%in chatbot 
%interface, 
as shown in Table \ref{Post-Study System Usability Questionnaire}. 

\begin{enumerate}
    \item \textbf{
H0: User Psychometric Evaluation for the conversational
%chat 
interface and 
conventional
%traditional 
search has no significant difference} 
%The 
A T Independent test was
%has been 
conducted. %with 95\% confident 
%and 
It was
%is
%has been 
found that for
%in 
all the parameters the 
%conversational
%chat 
%interface search system 
MCSI outperformed the CSI.
%conventional
%traditional 
%search interface. 
The null hypothesis was
%has been
rejected and the H1 hypothesis was accepted, which is that the MCSI 
%conversational
%chat 
%interface 
performs better than the CSI.
%in comparison to 
%conventional 
%traditional 
%search interface.
%engine.
\end{enumerate}

\begin{table}[t] % [!ht]
\centering
\tiny
\begin{tabular}{cccc}
\hline
\textbf{Topic}                 & \textbf{Pre-Task} & \textbf{Post Task} & \textbf{P Value} \\ \hline
DQual (1-3)\textsuperscript{*}            & 0.32     & 1.56      & .00005  \\
DCrit (0-1)       & 0        & 0.32      & .0026   \\
DIntrp (0-2)\textsuperscript{*} & 0        & 0.84      & .00005 \\ \hline
\end{tabular}
 %   \vspace{-.5mm}

\caption{Comparison of Pre-search and Post-search summary for the CSI (Change in Knowledge).} 
\label{ComparisonT}
%    \vspace{-.5mm}
%\end{table}
%\squeezeup
%\begin{table}[!ht]
%\vspace{-.5mm}

\vspace{2ex}

%\centering
\tiny
\begin{tabular}{llll}
\hline
\textbf{Topic}                  & \textbf{Pre-search} & \textbf{Post search} & \textbf{P Value}          \\ \hline
DQual (1-3)\textsuperscript{*}            & 0.52       & 2.12        & \textless .00001 \\
DCrit (0-1)\textsuperscript{*}        & 0.12       & 0.72        & \textless .00001 \\
DIntrp (0-2)\textsuperscript{*} & 0.28       & 1.36        & \textless.00001   \\ \hline       
\end{tabular}
    %\vspace{-.5mm}

\caption{Comparison of Pre-search and Post-search summary for the MCSI (Change in Knowledge).} 
\label{ComparisonI}
    %\vspace{-.5mm}
%\end{table}
%\squeezeup
%\squeezeup

%\begin{table}[!ht]
    %\vspace{-.5mm}

\vspace{2ex}

%\centering
\tiny
\begin{tabular}{lll} \hline
\textbf{Parameters}                 & \textbf{Influence} & \textbf{P value ($<$ 0.5)}                                                \\ \hline
Increase in Critique       & 87\%                & .048153 \\
Increase in Quality        & 29\%                & .299076                                 \\
Increase in Interpretation & 22\%                & .312712.        \\
\hline
\end{tabular}
    %\vspace{-1mm}
\caption{Comparison of Traditional Search and Interface Search}
\label{ComparisonB}
 %   \vspace{-.5mm}
\end{table}

\subsection{Knowledge Expansion} 
%\vspace{-1ex}
Satisfaction of the user's information need is directly related to their knowledge gain about the search topic. Knowledge gain can be measured based on recall of new facts gained after the completion of the search process \cite{wilson2013comparison}. 
We investigated knowledge expansion 
%was
%has been 
%investigated 
using
%based on the 
a comparison of 
%the 
pre-search 
%summaries 
and post-search summaries written by the participant, based on a number of parameters, as shown in Table \ref{Summary Comparison Metric}, while using both the systems. We divide the hypothesis into two sub-parts as follows: 

\begin{enumerate}
    \item Comparison of
    %the 
    pre-search and post-search summaries: This is to verify the knowledge expansion after each task independent of the search interface used by the participant.
    \item Comparison of the mean difference between
    %of 
    pre-search and post-search summaries for 
    %of 
    each interface: This is to verify which interface supported users better in gaining 
    %the 
    knowledge. 
\end{enumerate}

The user gains knowledge during the search when using either of 
%in both 
the search interfaces.  
%The objective of this hypothesis testing is to find that subjects were gaining knowledge while doing the search independently from  the type of interface. 
%The knowledge gain was examined by analyzing the pre-search 
%summary 
%and post-search summaries. 
To measure their knowledge gain, we 
%have 
asked subjects to write a short summary of the topic before the search and after the search. Each 
%The 
summary was
%has been 
analyzed based on three 
%important 
criteria as described in
%mentioned by the author 
\cite{wilson2013comparison}: Quality of Facts (DQual), Intrepterations (DInterpretation) and Critiques (DCritique), as shown in Table \ref{Summary Comparison Metric}. 
%\ref{tab:Summary Comparison Metric}. 
The summaries were
%has been coded on 
scored against
%all 
these three factors by two independent analysts 
%coders 
with the Kappa coefficient (Approx .85) \cite{Kappa}. We conducted 
%the 
hypothesis T dependent testing on tasks completed using both the 
%search task 
conventional
%Traditional 
search interface
%task 
and the MCSI.
%Chat
%Search Task
%. 

\begin{enumerate}
    \item \textbf{H0: No significant difference in the increase of the knowledge after completing the search task in both 
    %the
    settings}: As shown in Tables \ref{ComparisonT} and \ref{ComparisonI}, the pre-search score and post-search score for
    %in 
    all three factors were statistically significant in both the search settings. This implies that subjects expand their knowledge while carrying out the search.  This rejects the null hypothesis which leads to the alternative hypothesis which concludes that users experienced significant increase in their knowledge after search in both search settings.
    %\item \textbf{H1: Significant increase in the knowledge after the search in Both search Setting.}: 
    %This concludes that subjects learn and gain knowledge after the search in both the search settings which drive us to further investigate which search setting was more effective for the users. %Another hypothesis had been proposed to measure the difference between all three parameters in pre-search and post search parameters in both the setting and conduct the T dependent test to check the hypothesis. 
\end{enumerate}

After concluding the alternative hypothesis, it was 
%very 
important to investigate whether one
%which 
system was better in 
%providing 
%the 
%support to users in
%to 
expanding the user's 
%their 
knowledge. We purposed and tested the following hypothesis.

\begin{enumerate}
    \item \textbf{H0: Knowledge gain during the search is independent of the interface design:} In this test, we compared the Mean of the difference in the 
    %ofÆ’
    score for
    %in 
    pre-search and post-search summaries in both 
    %the 
    settings. %as shown in  Table \ref{tab:Summary analysis factors}. An T-independent test wasÆ’
    %has been 
    conducted on the change of the three parameter scores as discussed above for the hypothesis testing as shown in the Table \ref{ComparisonB}. It was
    %has been 
    found that in the MCSI interface setting, the subjects scored higher in the change of critique, quality and interpretation. This implies that the subjects learned more while using the 
    %conversational 
    %chat 
    %interface (
    MCSI. The difference in critique score was statistically significant, while the other two parameters were not statistically significant.
    %but 
    The quality and interpretation 
    %had 
    increased more than 20\% while using the MCSI.
    %conversational 
    %chat 
    %interface. 
    This confirms
    %concludes that 
    the alternative hypothesis, subjects' knowledge expands more 
    %in knowledge while 
    when using the MCSI.
    %conversational
    %chat 
    %interface.
\end{enumerate}

\begin{table}[!ht]
    \centering
    \tiny
    \setlength{\tabcolsep}{3pt} % Default value: 6pt
    \begin{tabular}{l|l|l|l|l}
\hline \hline 
\textbf{Parameters} & \textbf{CSI} & \textbf{MCSI} & \textbf{Percentage} & \textbf{P value}                                      \\ 
& \textbf{Mean} & \textbf{Mean} & \textbf{Change} & \\ 

\hline \hline 

Difficulty in finding the & & & & \\ information needed to  & & & & \\ address this task?           & 4.64    & 3.16            & -35.25     & .002168         \\ \hline 
Quality of text presented & & & & \\  with respect to your information  & & & & \\ need and query?                                                                  & 4.52    & 5.64           & 21.55      & .010465      \\ \hline 
How useful were the search & & & & \\  results in the whole search task?                                                                                                                                    & 4.04   & 5.12            & 23.08      & .029826      \\ \hline 
How useful was the text shown & & & & \\  in the whole search task in  & & & & \\ satisfying the information need?                                                                                                      & 4.08     & 5.36           & 27.62      &  .010245      \\ \hline 
Did you find yourself to be cognitively & & & & \\  engaged while carrying  & & & & \\ out the search task? \textsuperscript{*}                                                                                                             & 3.92      & 5.92             & 42.31      & .000015      \\ \hline 
Did you expand your knowledge about & & & & \\  the topic while completing  & & & & \\ this search task?                                                                                                                & 4.84     & 6           & 20         & .005026     \\ \hline 
I feel that I now have a better & & & & \\  understanding of the topic  & & & & \\ of this search task.                                                                                                                 & 4.56     & 5.88            & 25.64      &  .002094      \\ \hline 
How would you grade the success & & & & \\  of your search session  & & & & \\ for this topic?                                                                                                                          & 4.48     & 5.72           & 24.35      & .005937      \\ \hline 
How do you rate your assigned & & & & \\  search setting in terms of  & & & & \\ understanding your inputs?                                                                                                            & 3.72      & 5.40            & 39.18      & .003121.          \\ \hline 
How do you rate your assigned & & & & \\  search setting in the presentation  & & & & \\ of the search results?\textsuperscript{*}                                                                                                         & 3.84     & 5.76            & 45.45      &.00001 \\ \hline 

How do you rate the suggestion(s) & & & & \\  skills of your assigned  & & & & \\ search setting?\textsuperscript{*}                                                                                                                       & 3.72      & 5.56            & 54.44      &  .000053     \\   \hline               
\end{tabular}
\caption{Characteristics of the search process \cite{vakkari2016searching} by the change in knowledge structure where * indicates statistically significant results.}
\label{Flowchart of characteristic of search process by the change in knowledge structure}
\vspace{-2ex}
\end{table}

\subsection{Search Experience}
%\vspace{-1ex}
Learning while searching is an integral part of the information seeking process. Based on the search as learning  proposed by Vakkeri \cite{vakkari2016searching}, the user search experience 
%in collaboration of learning 
can be 
%had been 
evaluated on 15 parameters, including
%such as 
%including 
the relevance of the search result, the quality of the text presented by the interface, and understanding of the topic in both the search settings via pre-search and post-search questionnaires.
\begin{enumerate}
    \item \textbf{H0: Subjects find no significant difference between while using both the interfaces:} The T-independent test was
    %has been 
    conducted among
    %as on 
    all 15
    %the 
    parameters, shown in Table \ref{Flowchart of characteristic of search process by the change in knowledge structure}. 
    %and 
    It was 
    %has been 
    found that the null hypothesis was 
    %has been 
    rejected %, as shown in 
    %the 
   % Table \ref{Flowchart of characteristic of search process by the change in knowledge structure}. 
   Subjects search experience was statistically significantly better with the MCSI. %interface which is .
%GJ: which interface is better? Solved
%H1: Subjects search experience was better with the chat interface.
In the pre-search questionnaire, subjects were asked to anticipate the difficultly level of the search before starting the search and in post-search questionnaire, subjects were asked to indicate
%mentioned 
the difficulty level they actually experienced. 
It was
%is also been 
observed that 
%subjects 
pre-search anticipated difficulty level and the post-search actual difficulty level 
%after conducting the search 
%been 
increased for 
%in 
the CSI 
%conventional 
%traditional 
%system 
(16\%) and decreased in the case of MCSI 
%interface 
search task (14\%).
%GJ: I don't understand the above. Where do these numbers come from?
%Ak from questionnaire solved

\end{enumerate}

%\begin{figure}[!ht]
 %   \centering
  %  \includegraphics[scale=.3]{figures/Chat_Better.jpg}
   % \caption{Reason to Select MCSI}
    %\label{fig:Benefits of MCSI}
%\end{figure}

%\%begin{figure} [!ht]
  %  \centering
   % \includegraphics[scale=.3]{figures/Traditional_challanges.jpg}
    %\caption{Challenges with Traditional System}
    %\label{fig:Challenges with Traditional System}
%\end{figure}

%\begin{figure}
 %   \centering
  %  \includegraphics[scale=.3]{figures/Highlight.jpg}
   % \caption{Benefits for Highlights option}
    %\label{fig:Benefits for Highlights option}
%\end{figure}

%\begin{figure} [!ht]
 %   \centering
  %  \includegraphics[scale=.3]{figures/Chat_challanges.jpg}
   % \caption{Challenges of
    %with
   % MCSI}
    %\label{fig:Challenges with MCSI}
%\end{figure}

\subsection{Interactive User Experience}

%\vspace{-1ex}

\begin{table}[t] % [!ht]%[!htbp]
\centering
\tiny
\setlength{\tabcolsep}{4pt} % Default value: 6pt
\begin{tabular}{llllll}
\hline 
\textbf{Negative}   & \textbf{Positive}     & \textbf{Scale}             & \textbf{CSI\_Mean} & \textbf{MCSI\_Mean} & \textbf{P\_Values} \\ \hline %& \textbf{T\_Value}    \\ \hline
obstructive     & supportive   & P & 3.44     & 5.60   & 2.96e-08 \\ % & -6.60 \\
complicated     & easy         & P & 3.40    & 5.76    & 7.84e-09 \\ %  & -6.98 \\
inefficient     & efficient    & P & 2.88   & 4.40   & 1.69e-05  \\ %  & -4.78 \\
confusing       & clear        & P & 3.40   & 5.48    & 2.31e-06 \\ %  & -5.36 \\
boring          & exciting     & H   & 2.64   & 5.44    & 8.88e-16 \\ %  & -11.77 \\
not interesting & interesting  & H   & 2.48    & 5.48     & 9.76e-15 \\ %  & -11.01 \\
conventional    & inventive    & H   & 2.36     & 6.28    & 1.17e-14 \\ %  & -10.98 \\
usual           & leading edge & H & 1.96     & 5.20     & 8.95e-12 \\ %  & -8.93 \\ \hline 
\hline
\end{tabular}
\caption{UEQ-S score based on CSI and MCSI where 'P' stands for Pragmatic Quality and 'H' stands for Hedonic Quality (statistically significant).}
\label{CHap7:USQ_S}
\end{table}

To ensure a conversational search system provides reasonable User Experience (UX), it is critical to have a measurability which defines user insights about the system. A UX questionnaire for interactive products is the User Experience Questionnaire (UEQ-S) \cite{laugwitz2008construction,schrepp2017design,hinderks2018benchmark}. %This questionnaire contains 26 parameters. 
This questionnaire also enables 
%us to
analysis and interpret outcomes by comparing with benchmarks of a larger dataset of outcomes for other interactive products \cite{hinderks2018benchmark}. This questionnaire also provides the opportunity to compare interactive products with each other. 
For specified purposes, a brief version (UEQ-S) was prepared which had only 8 parameters to be considered \cite {hinderks2018benchmark}. UEQ-S was preferred for the MCSI, since
%as 
it is mostly used for interactive products. For example, users filled the experience questionnaire after finishing the search task, if there were too many questions, a user may not complete the answers fully or even refuse to complete 
%answer 
it (as they have finished the search task and are in the process of leaving or starting the next task, so the motivation to invest more time on feedback may be limited).
The UEQ-S contains two meta dimensions Pragmatic and Hedonic quality. Each dimension contains 4 different parameters, as shown in 
%the 
Table \ref{CHap7:USQ_S}. Pragmatic quality explores the usage experience of the search system, while Hedonic quality explores the pleasantness of use of the system.

\begin{enumerate}
    
    \item \textbf{HO: Users feel a similar interactive experience when using the different interfaces}:
    Users evaluated the system based on 8 parameters as shown in Table \ref{CHap7:USQ_S}. The grading scale was assigned between 0 (low) - 7 (High). We 
    %have 
    compared the mean difference of both 
    %the 
    systems on all parameters. In all 
    %the 
    aspects, subjects experience was positive in Pragmatic quality and Hedonic quality when using the MCSI, %Subjects claimed more success in accomplishing the task using the conversational
    %chat 
    %interface, and 
    %and 
    %the results for accomplishing the task were 
    %the 
    and statistically significantly different in comparison to the CSI. 
    %on accomplishing the task
    %and the 
    Subjects felt obstructive, complicated, confusing, inefficient, and boring, while using the CSI with significant difference (P$<$0.10). This implies that the null hypothesis was rejected on the basis of the user experience. %Although the four-factors were not significantly different, 
    %but 
    %the mean difference between both the systems on these factors was more than 
    %the 
    %10\%. % which conclude that the MCSI put the less cognitive load on the user. This lead to the alternative hypothesis (H1) which 
    Based on these findings, we can conclude that the user experience was more pleasant and easy while using the MCSI. %Concluding the alternative hypothesis lead to explore certain areas with the help of research questions. % The following RQ has been investigated. 
    %\item \textbf{H1: User experience Less Subjective mental workload while using the chat interface:}
\end{enumerate}

\subsection{Analysis of Study Results}
%Research Questions}
%\vspace{-1ex}

In summary, 
%After 
hypothesis testing showed that 
%, it has been proved that 
the MCSI 
%have 
reduced cognitive load, increased knowledge expansion, increased cognitive engagement and 
%have 
provided a better search experience load. 
%There were certain research questions were developed which needed to be investigated. 
Based on the results of the study, a number of research questions dealing with factors relating to %supporting 
conversational search, 
the challenges of 
conventional
%with 
%traditional 
search, and 
%the 
user search behaviour can be addressed. 

\subsubsection{RQ1: What are the factors that support search using the MCSI.}
%interface search.} 
%\vspace{-1em}

Around 92\% of the subjects claim in the post-search interview that the MCSI
%conversational 
%chat 
%interface 
was better than
%in comparison to 
the CSI.
%conventional
%default 
%search interface. %As shown in 
%the Figure \ref{fig:Benefits of MCSI}, 
%the majority of the subjects (
Around 48\%
%) 
found
%find 
that the 
%chat 
MCSI
%conversational search interface 
allowed them to more easily access the information. A similar view was
%has been 
found in terms of information relevance and its structure as presented to the user.  Around 38\% of subjects were satisfied with the options and suggestions provided by the MCSI.
%conversation 
%chat 
%interface.  
The other reasons for their satisfaction were the highlighting of segments in long documents, finding the 
%the effective 
search system effective, its being interactive and
%\& 
engaging, and user friendly.
%, etc. 
% as shown in the figure.
%GJ: shown in which figure? Solved

\subsubsection{RQ2: What are the challenges with the conventional
%traditional 
search system?}
\vspace{-1ex}
%As discussed earlier, the majority (92\%) of the subjects did not like the traditional search interface.  This lead to the investigation of the limitation of the current search system.  
%The 
Subjects found
%find 
some major challenges in 
%to 
completing the search tasks
%process 
with the CSI.
%in traditional 
%search interface. 
The limitations were mainly
%are majorly 
based on observations from 
%the 
user interactions and 
%its 
feedback after the search task. The limitations can be
%were 
divided into five broad categories.%, as shown in Figure \ref{fig:Challenges with Traditional System}. 

\noindent \textbf{Exploration}: %t was 
%is found that the conventional
%traditional 
%system does not
%doesn't support 
%the 
%exploration of individual
%the 
%documents. 
Around 60\% of the subjects claimed they found
%find 
it difficult to explore the content with the CSI, which meant that they were unable
%them 
to learn through the search process. It was noted
%is mentioned that subjects 
that they needed to expend much
%a lot of 
effort to go through whole documents, which discouraged them from 
%to 
exploring further to satisfy their information need. Another reason was that too much information was displayed to them 
%showed
on the page which confused them
%subjects 
during 
%in 
the process of information seeking.

\noindent \textbf{Cognitive Load}: Around 28\% of subjects experienced issues with
%the 
cognitive load using
%on 
the CSI.
%default search setting 
In current search systems, a query to 
%in 
the search engine returns the best document in a
%the
single shot. The user may need to perform multiple searches by modifying the search query each time to satisfy their information need. There are multiple
%a lot of 
limitations 
%that are 
associated with this
%the 
single query search approach which put 
%the 
high cognitive load on the user. The following points highlight the limitations and weaknesses of single-shot search \cite{kaushik2019dialogue}. %\cite{kaushik2019dialogue}:

\begin{enumerate}
    \item The user must completely describe their information need in a single query. 
    \item The user may not be able to adequately describe their information need.
    \item High cognitive load on the user in forming a query.
    \item An information retrieval system should return relevant content in a single pass based on the query.
    \item The user must inspect returned content to identify
    %find 
    relevant information.
\end{enumerate}

\noindent \textbf{Interaction and Engagement}: %Interaction and engagement with 
%the content is
%are a key component of 
%the information seeking. Subjects (
8\% found 
%find 
difficulty in engaging and interactive with long documents.
 %\textbf{Irrelevant and Vague Information}: 
Subjects can find
%experience the 
content in long documents irrelevant or vague with respect to their specific information need.  Using 
%In 
the CSI,
%traditional 
32\% of the subjects did not
%didn't 
find the long documents precise enough to satisfy their information need. In contrast, 90\% of them
%people 
were satisfied with the way information was presented to them in the MCSI,
%conversational chat interface, 
although the Wikipedia
%knowledge base 
API and underlying retrieval method was same for both interfaces. 

\noindent \textbf{Highlighting}: Another issue 
%challenge 
which was
%is 
referred to by around 8\% of subjects related to text highlighting.
%was in contrast with the chat system. 
%In the conversational 
%chat 
%system, the subject can view the whole document with important text in the document highlighted,
%in the document 
%which eases the seeker's engagement with individual documents 
%search process 
%and can save time and reduce 
%the 
%cognitive load. 
Subjects found
%find 
that the absence of 
%no 
highlighting
%text 
in the CSI
%traditional system 
was frustrating.
%is a challenge of the system. 
%This indicates the opportunity to advance the traditional system one step near to the conversational system. 
%Although simple query-based highlighting could be added to the conventional interface to address this issue. %, there is potential for this highlighting facility to be further extended in the conversational interface where the agent could develop an incremental model of the user's developing search engagement.

% ak \begin{table*}[]
% ak\begin{tabular}{|l|l|l|l|l|l|l|}
% ak\hline
% akTypes & No.  of Queries & No. of Unique docs viewed & No. of the full doc & Avg. No. Interactions & Avg. Engagement & No. of Participant \\ \hline 
% akW & <=2  & <=2  & <=2 & 20   & 1    & 3  \\
% akX & 1-13 & 3-8  & 0   & 54   & 1.34 & 4  \\
% akY & 7-9  & 7-11 & 3-7 & 49.5 & 1.05 & 4  \\
% akZ & 2-8  & 2-9  & 1-2 & 38.8 & 0.96 & 14 \\ \hline
% ak\end{tabular}
% ak\caption{Types of User Behaviour In MCSI}
% ak\label{MCSI Behaviour}
% ak\vspace{-2ex}
% ak\end{table*}

\subsubsection{RQ3: Does Highlighting important segments support users in effective and efficient search?, and Why?}
%The 
%\vspace{-1ex}
92\% of subjects liked the document highlighting
%document 
options in the MCSI.
%conversational 
%chat 
%search interface. There is a potential for this highlighting facility to be further extended in the MCSI
%conversational interface 
%where the agent could develop an incremental model of the user's developing search engagement. 
The following 
%were 
%the
reasons were identified for 
%to 
choosing this. %these options as shown in 
%the 
%Figure \ref{fig:Benefits for Highlights option}. 

\begin{enumerate}
    \item Interactive and Engaging: Around 28\% of subjects claimed that they were able to engage and interact with documents 
    %in 
    %a 
    better 
    %way 
    by using the highlighting options.
    \item Helpful: 68\% of the subjects found
    %find 
    %the 
    highlighted documents helpful in information seeking.  
    \item Reduce the Cognitive Load:  Around 24\% of the subjects believed that the highlighted documents reduced their cognitive load.
    %the cognitive load of searchers 
    %by highlighting 
    %the 
    %important sentences which helped them to satisfy their Information need.  
    \item Access to Relevant information: 
    %The 
    36\% of the subjects believed that 
    %the 
    highlighted documents helped them to more easily access useful
    %the 
    information.
    %which saves them time and effort.
    %of the searcher. 
\end{enumerate}

\begin{table*}[!ht]
\centering
\tiny
\begin{tabular}{lccccccc} \hline
\multicolumn{7}{l}{Confidence   intervals (p=0.05) per scale} \\ \hline
Scale & Mean (-3 to 3) & Std. Dev. & N & C & \multicolumn{2}{c}{C interval} & alpha value\\ \hline
P & -0.720 & 1.349 & 25 & 0.529 & -1.249 & -0.191 & 0.91 \\
H & -1.640 & 1.233 & 25 & 0.483 & -2.213 & -1.157 & 0.92  \\
Overall & -1.180 & 1.207 & 25 & 0.473 & -1.653 & -0.707 & 0.91 \\ \hline
\end{tabular}
\caption{CSI confidence intervals on UEQ-S where, 'P' stands for Pragmatic Quality, 'H' stands for Hedonic Quality and 'C' stands for Confidence.}
\label{7ConfidenceCSI}
\end{table*}

\begin{table*}[!ht]
\centering
\tiny
\begin{tabular}{lccccccc}
\hline
\multicolumn{7}{l}{CSI   Confidence intervals (p=0.05) per scale} \\ \hline
Scale (-3 to 3) & Mean (-3 to 3) & Std. Dev. & N & C & \multicolumn{2}{c}{C Interval} & alpha value \\ \hline
P & 1.310 & 0.596 & 25 & 0.234 & 1.076 & 1.544
 & 0.79\\
H & 1.600 & 0.559 & 25 & 0.219 & 1.381 & 1.819 & 0.79 \\
Overall & 1.455 & 0.519 & 25 & 0.203 & 1.252 & 1.658 & 0.79 \\ \hline
\end{tabular}
\caption{MCSI confidence intervals on UEQ-S, where 'P' stands for Pragmatic Quality,'H' stands for Hedonic Quality and 'C' stands for Confidence.}
\label{7ConfidenceMCSI}
\end{table*}

\subsubsection{RQ4: What are the challenges and opportunities to support exploratory search in 
%on 
conversational settings?} 
%\vspace{-1ex}

The 
%great 
majority of subjects (92\%) 
%people 
claimed that the MCSI
%conversational
%chat 
%search interface 
was better. The remaining subjects (8\%) faced some
%few 
challenges using it.
%in 
%the conversational 
%chat 
%interface 
%as shown in Figure \ref{fig:Challenges with MCSI}. 
Subjects wanted 
%need 
more sections and subsections in
%regarding 
the documents to support their exploration, and 
%subjects 
also wanted 
%an 
%additional option to 
support of
%do the 
image search. Around 4\% of the subjects felt the need for 
%of 
%the 
improvement in
%on 
%the 
operational speed and 
%the 
better incorporation of standard features such as spellchecking. Subjects found
%find 
the chat interface helpful for 
%in 
exploring 
%through the 
long documents. They were keen to see the addition of 
%Adding 
speech as a mode of user interaction and a more refined 
%the 
algorithm for the selection of 
%to select the right 
images for 
%to 
presentation to the user. 
%Currently, the system is built on a rule-based approach which restricts the free flow of the user input and the subject needs to follow the constrict rule-based for interacting with the chat interface. The use of advanced language modal Bert or transformer with search capability would be effective with the conversations and provide the opportunity to make our search agent autonomic in nature. 

%\subsubsection{RQ: What is the improvement needed in the search interface}
%   Around 8\% of subjects identified a 
%find the
%   need for improvement or a need for some additional services in the interface. As above, these included 
%such as 
%   image search and audio support, but also question-answering 
%QA Support 
%   and faster
%Quicker 
%   access to 
%the 
%   long documents. 
%Although each document was presented 
%incorporated 
%with the corresponding images, 
%but 
%subjects felt the need for image search separately and emphasised the need for images to satisfy the information need.  Similarly, 
%the 
%subjects also suggested audio support in querying or interface interactions.  
Subjects appreciated the 
%interface 
usefulness of the interface 
%application 
in 
%to 
supporting
%them for 
exploratory search, but suggested that this would be further improved by the incorporation of 
%explained the need for the 
a question answering facility.
%support. 
%   In general, subjects were satisfied with highlighting the important segments but they wanted 
%expect 
%    faster 
%quick 
%      access to the paragraphs of the highlighted relevant segments of the long documents. 
%GJ: in what way did they want the access to be quicker? Solved

%Currently, the system is built on a rule-based approach which restricts the free flow of the user input and the subject needs to follow the constrict rule-based for interacting with the chat interface. 

%\begin{table*}[]
%\begin{tabular}{llllll}
%\hline
 %     &                                               Topics   (0-7)      & Type W & Type X  & Type Y & Type Z \\
  %    \hline
   %   & Background Knowledge                                   & 4      & 3.7     & 4      & 2.3    \\
%Search Formulation      & Interest in Topic                                      & 6.3    & 3.2     & 6.5    & 4.9    \\
% & Anticipated Difficulty                                 & 4      & 3.2     & 4.2    & 3.5    \\
%\hline 
 %     & Actual Difficulty                                      & 1.3    & 3       & 3.5    & 3.5    \\
  %    & Helpfulness Highlighted                                & 6.6    & 5.2 (0) & 5.7    & 5.2    \\
 %Content Selection     & Presentation Quality of Text                           & 6.3    & 5       & 6.2    & 5.5    \\
  %    & The usefulness of Search results                       & 6.6    & 4.7     & 6      & 4.6    \\
 %& Text Relevance & 7      & 5.2     & 5.2    & 5      \\ \hline 
  %    & Cognitively Engaged                                    & 5.6    & 6.2     & 6.7    & 5.6    \\
   %    & Suggestions Skills                                     & 6.3    & 5.7     & 6.5    & 5.0 \\
      
 %Interaction with Content     & System Understanding Input                             & 6.3    & 5.7     & 6.2    & 4.8    \\
  %    & Understanding the Topic                                & 6.6    & 6.2     & 6.5    & 5.4    \\ \hline 
   %   & Search Success                                         & 7      & 5.7     & 6      & 5.3    \\
%     Post Search & Presentation of the Search Results                     & 6.6    & 5.7     & 6.5    & 5.3    \\
 %      & Expand of Knowledge                                    & 7      & 5.5     & 7      & 5.6    \\
  %      \hline
      
%\end{tabular}

%\caption{Flowchart of characteristic of search process \cite{vakkari2016searching} by the change in knowledge structure in MCSI}
%\label{FLowchart}
%\vspace{-2ex}
%\end{table*}

%\begin{table}[]
 %   \centering
  %  \begin{tabular}{l|l|l|l}
   % \hline
    %  \textbf{Types} & \textbf{Dqual (0-3)} & \textbf{Dintrep (0-2)} & \textbf{Dcritque (0-1)} \\ \hline 
     %   W & 2 & 1.3 & 1 \\
      %  X & 1.5 & 1.2 & .7\\
      %  Y & 1.7 & 1.2 & .5 \\
       % Z & 1.5 & .9 & .5 \\ \hline
    %\end{tabular}
    %\caption{Mean difference of Pre- and Post-Summary analysis factors \cite{wilson2013comparison} based user search behaviours with the conversational system}
    %\label{tab:Summary analysis factors}
%\end{table}


\subsubsection{RQ5: How does user experience vary between search settings in comparison to each other?}

%\vspace{-1ex}

%\subsubsection{How is the user experience with CIS}
%\begin{figure}[!ht]
%    \centering
 %   \includegraphics[scale=0.4]{figures/Rule_based_CSI.PNG}
 %   \caption{Means of the CSI score scales Pragmatic quality and Hedonic quality where green indicates positive, yellow indicates neutral and red indicates bad where the x axis represents 'Quality' and the y axis represents 'scale'.}
 %   \label{fig7:CSI_graph}
%\end{figure} 
\begin{enumerate}
    \item \textbf{Observing the Pragmatic and Hedonic properties of CSI}: The users provided feedback based on their experience using the CSI. %As shown in Figure \ref{fig7:CSI_graph}, 
    The CSI score is negative with respect to both Pragmatic and Hedonic properties and the overall score is also negative.  From this 
    %which 
    we can infer that the userâ€™s experience of the CSI system is neither effective nor efficient, as shown in the Table \ref{7ConfidenceCSI}. From Table \ref{7ConfidenceCSI}, we can calculate the mean range after data transformation for UEQ-S where is -3 too negative and +3 is too positive. Table \ref{7ConfidenceCSI} shows the confidence interval and confidence level. The smaller the confidence interval the higher the precision \cite{UEQ}. The confidence interval and confidence level confirm our analysis that all the dimensions of Pragmatic and Hedonic properties were negatively experienced by the users. Generally, items belonging to the same scale should be highly correlated. To verify the user consistency, alpha-coefficient correlation was calculated using the UEQ-S toolkit. As per different studies, an alpha value $>$ 0.7 is considered sufficiently consistent \cite{hinderks2018benchmark}. This shows that user marking of the CSI
    %conventional system 
    is consistent. The UEQ-S tool kit also provides an option to detect random and non-serious answers by the users \cite{UEQ} \cite{hinderks2018benchmark}. This is carried out by checking how much the best and worst evaluation of an item in a scale differ. Based on this evaluation, the users' feedback does not show any suspicious data.
   
%\begin{figure}[!ht]
   % \centering
   % \includegraphics[scale=0.4]{figures/Rule_based_MCSI.PNG}
  %%  \caption{Means of the MCSI score scales Pragmatic quality and Hedonic quality where green indicates positive, yellow indicates neutral and red indicates bad where the x axis represents 'Quality' and the y axis represents 'scale'.}
  %  \label{fig7:MCSI_graph}
%\end{figure}    
\item \textbf{Observing the Pragmatic and Hedonic properties of MCSI}: %As shown in Figure \ref{fig7:MCSI_graph}, 
The MCSI scored positive in Pragmatic, Hedonic and Overall score from which we can infer that the userâ€™s experience of the MCSI is good in general and with good ease of use. Table \ref{7ConfidenceMCSI} shows the confidence interval and confidence level. The confidence interval and confidence level confirms our analysis that all the dimensions of pragmatic and hedonic scores were positively experienced by the users. Alpha-coefficient correlation \cite{hinderks2018benchmark} confirms that the marking of MCSI by the users is consistent. The UEQ-S toolkit also provides an option to detect random and non-serious answers by users. This is conducted by checking how much the best and worst evaluation of an item in a scale differ. Based on this evaluation, the users' feedback does not detect any suspicious data.
\end{enumerate}

%\%begin{figure}[!ht]
%    \centering
%    \includegraphics[scale=0.3]{figures/Bench_mark_CSI.PNG}
%    \caption{Comparison of benchmark with CSI where the x axis represent 'Quality' and the y axis represents 'Scale'.}
%    \label{7fig:CSI_Bench}
%\end{figure}    

\subsubsection{RQ6: How does user experience vary for both search settings in comparison to a standard benchmark?}
%\vspace{-1ex}

\begin{enumerate}
\item \textbf{Comparison of the CSI with the standard benchmark}: This benchmark was developed based users on feedback on 21 interactive products \cite{hinderks2018benchmark}. Based on the comparison from the benchmark, the CSI UX is far below the mean of the interactive products (Pragmatic Quality $<$ 0.4, Hedonic Quality $<$ 0.37 and overall $<$ 0.38). %as shown in Figure \ref{7fig:CSI_Bench}. 
This signifies that the UX with the CSI needs major improvement on Pragmatic and Hedonic sectors. In the comparison to the benchmark, the CSI rates as a low quality of user experience and lies in the range of worst 25\% of the products.

%\begin{figure}[!ht]
 %   \centering
  %  \includegraphics[scale=0.3]{figures/Chat_Product.PNG}
   % \caption{Comparison of benchmark with MCSI where the x axis represent 'Quality' and the y axis represents 'Scale'.}
    %\label{7fig:MCSI_Bench}
%\end{figure} 

\item \textbf{Comparison of the MCSI with the standard benchmark}: Based on the comparison from the benchmark \cite{hinderks2018benchmark}, the MCSI UX is far above the mean of the interactive products (Pragmatic Quality $>$ 0.4, Hedonic Quality $>$ 0.37 and overall $>$ 0.38). %as shown in Figure \ref{7fig:MCSI_Bench}. 
This signifies the UX of the MCSI compared to other interactive products (benchmark) is very high and is of excellent level, and lies in the range of 10\% best results. 

\end{enumerate}

%\subsubsection{RQ7: What are the user search behaviours and experiences with the conversational system in an exploratory search setting?}

%\%begin{figure}[ht]
%\centering
%\begin{minipage}[b]{0.47\linewidth}
%\includegraphics[scale=0.23]{figures/DocumentvsBackground knowledge.jpeg} 
%\caption{Correlation graph of completion background knowledge vs full document open (Correlation coefficient 0.16)}
%\label{fig:minipage7.1}
%\end{minipage}
%\quad
%\begin{minipage}[b]{0.47\linewidth}
%\includegraphics[scale=0.23]{figures/KnoweldgevsINteractions.jpeg}
%\%caption{Correlation graph of completion background knowledge Vs %interactions (Correlation coefficient 0.29)}
%\label{fig:minipage7.2}	
%\end{minipage}
%\end{figure}

%\begin{figure}[ht]
%\centering
%\begin{minipage}[b]{0.47\linewidth}
%\includegraphics[scale=0.23]{figures/KnoweldgevsDocuments.jpeg} 
%\caption{Correlation graph of completion background knowledge vs total documents (Correlation coefficient 0.31)}
%\label{fig:minipage7.3}
%\end{minipage}
%\quad
%\begin{minipage}[b]{0.42\linewidth}
%\includegraphics[scale=0.23]{figures/InterestvsInteractions.jpeg}
%\caption{Correlation graph of Interest Vs Interactions (Correlation coefficient 0.32)}
%\label{fig:minipage7.4}
%\end{minipage}
%\end{figure}


%\begin{figure}[ht]
%\centering
%\begin{minipage}[b]{0.42\linewidth}
%\includegraphics[scale=0.23]{figures/QueryvsBackground knowledge.jpeg} 
%\caption{Correlation graph of completion background knowledge Vs No. Query (Correlation coefficient 0.07)}
%\label{fig:minipage7.5}
%\end{minipage}
%\quad
%\begin{minipage}[b]{0.45\linewidth}
%\includegraphics[scale=0.23]{figures/QueryvsInteractions.jpeg} 
%\caption{Correlation graph of total interactions Vs No. Query (Correlation coefficient 0.29)}
%\label{fig:minipage7.6}
%\end{minipage}
%\end{figure}

%The last research question 
%was 
%focused on identifying
%finding the 
%user behaviour patterns during 
%the 
%exploratory search using
%search process in 
%the conversational interface.
%system. 
%We classified 
%the 
%user search behaviour in four discrete classes based on the multiple factors as shown in the Table \ref{MCSI Behaviour}.
%where 
%``Avg. engagement'' was calculated by taking the mean of the number of opening of unique documents by users divided by number of queries made by the user. The ``Avg. interaction'' was
%had been 
%calculated by dividing the total turns between the agent and the user by 2. These two factors were very important in
%to 
%understanding the engagement and interactions of the searcher with the agent. We examine 
%investigated the 
%user behaviour based on these interaction and engagement factors.
%, . 
%We also investigated user behaviour patterns in the conversational setting based on feedback from the pre- and post- questionnaires, the number of queries, and the number of unique documents viewed by the users as shown in Figures \ref{fig:minipage7.1}, \ref{fig:minipage7.2} \ref{fig:minipage7.3} \ref{fig:minipage7.4} \ref{fig:minipage7.5} \ref{fig:minipage7.6}. We do not observe very strong correlation among the variables. As per our observations of individual users, some users had less knowledge about a topic, but were interested to learn more about the topic which led them to engage with limited content with the least interaction. They were happy with the highlighted text in the long documents. While some other users reported less knowledge about the topic, but being highly interested in the topic which apparently led them to engage more with the snippets, sections and subsections with the multiple interactions, than those with who reported less interest in the topic. Other users reported only average knowledge about the topic, but their interest in the topic was highest which led them to engage more than the users listed above with the snippets, sections, subsections and full documents using multiple interactions. %The interesting fact about the type Y that users they have opted often to read the full document. Their queries and document interactions were highest. %An
    %The
 %Ak \   example of type Y users is such as a person looking for in-depth and specific knowledge about multiple diverse sub-field related to a topic. % AK comments They were highly satisfied with the interface understanding intention of the user. It is concluded the type Y users were those users who believed to be an searcher who showed mixed behaviours of the users in exploring and exploiting the snippets and sections and subsections of the documents. 
    
 %Ak \   \item Type Z Users: Type Z users reported that they have an average understanding of
    %about 
 %Ak \   the topic (5.5) in comparison with other users, and claim that their expansion of knowledge after search (5.6) was average.
    %, after search.  
%Ak \    In this behaviour, users experienced less cognitive engagement (5.6) with the least search session success (5.3). The reasons for such behaviour are as follows: 
%Another types of 
%Users had 
%a 
%Other users reported low background knowledge of a topic with their interest in it being average, which apparently led them to engage less with the snippets, sections, subsections and full documents. For these users, their pre-search anticipation of difficulty level was consistent with their post-search experience of the
%difficulty level. It was found in the study that in the pre-search questionnaire users anticipated higher difficulty when using the MCSI, whereas the actual difficulty level reported post-search after using it was much lower. 
%This indicates that the users' experience with the MCSI was intuitive and pleasant.
    %very 
  %Ak \  
 % low background knowledge about the topic with their interest in the topic being
    %was 
 %Ak \  
 %average, which led them to 
 %less 
 %engage less with the snippets, sections, subsections and full documents. These type of users anticipation of difficulty level and experience of the difficulty level after the search were the same.
    %was 
 %Ak \   the same. An
    %The 
%Ak \    example of type Z users would be
    %is such as 
 %Ak \   a person looking for shallow and broad information about a topic without going into details. % AK comments Although in most of the factors, these users graded the MCSI interface was better than conventional by 70\% or more in most of the evaluation search parameters, but in comparison to the other user behaviours there interactions were least with MCSI. %, This implies, that these users were more inclined with the traditional search.  
    %This implies the user's natural behaviour regarding the difficulty and inadaptability of the new interface. ???
    %GJ: no idea what the above sentence means. Solved
%Ak \\end{enumerate}
    %very 
  %Ak \  
 % low background knowledge about the topic with their interest in the topic being
    %was 
 %Ak \  
 %average, which led them to 
 %less 
 %engage less with the snippets, sections, subsections and full documents. These type of users anticipation of difficulty level and experience of the difficulty level after the search were the same.
    %was 
 %Ak \   the same. An
    %The 
%Ak \    example of type Z users would be
    %is such as 
 %Ak \   a person looking for shallow and broad information about a topic without going into details. % AK comments Although in most of the factors, these users graded the MCSI interface was better than conventional by 70\% or more in most of the evaluation search parameters, but in comparison to the other user behaviours there interactions were least with MCSI. %, This implies, that these users were more inclined with the traditional search.  
    %This implies the user's natural behaviour regarding the difficulty and inadaptability of the new interface. ???
    %GJ: no idea what the above sentence means. Solved
%Ak \\end{enumerate}



\section{Conclusions and Observations}
\label{conclude}

%\section{Discussion}
%\label{discuss}

%Ak As described
%discussed 
%Akearlier in the paper, 
%in the recent work, 
%Akthere is very little existing work related to examination of 
%the 
%Akconversational search interfaces \cite{stein1993conversational}, and 
%the 
%Akevaluation of such 
%a 
%Aksystems has
%was 
%Akeither focused on the search satisfaction \cite{Abhi, avula2018searchbots} or software usability \cite{lewis1995ibm}. Our study has 
%been 
%Akevaluated a
%the
%AkMCSI using
%on 
%Akdifferent dimensions to study the effect on the searcher of this alternative mode of search interaction. 
The study reported in this paper 
%Our
%The
%current study 
indicates that subjects found our MCSI more helpful than a closely matched CSI.
%the traditional search.  
We also observed types of user behaviour while using MCSI which are
%was 
different to those when using
%than the 
a CSI.
%traditional
%search interface %\cite{Abhi,Trippas:2017:PIC:3020165.3022144}. 
%This could be 
%the 
%related to the use of 
%drive path for 
%different information seeking models \cite{belkin1982, sitter1992modeling} in the conversational search setting. 
%Most previous studies of user behaviour in conversational search have used Wizard-of-Oz type agents \cite{avula2020wizard}, in contrast, we study use of an automated search support agent. 
Using our agent-based system, we observe the natural expectations of user search in conversational settings. We observed 
that subjects do not encounter any difficulty in using the new interface, because it seems to be similar to the standard search interface with the additional capabilities of conversation.  We also observe that the information space and its structure is a key component in information seeking. Subjects found highlighting important segments in long documents enables them to access information much easily. The MCSI made the search process less cognitively demanding and more cognitively engaging. 
% AK comments Overall, the conversational search interface incorporates
%is the multidisciplinary 
% AK comments concepts from multiple disciplines including
%that deals with 
% AK comments human-computer interaction, software engineering, information retrieval and human psychology. This means that 
%The 
% AK comments evaluation 
%of any such concept 
% AK comments needs designed carefully 
%to be evaluated in 
% AK comments take account of all these dimensions.

%\section{Conclusions}
%\label{conclude}

%The current study indicates our prototype conversational 
%the new 
%interface shows considerable
%a promising 
%promise for the effective use of 
%approach of 
%conversation to support searchers in satisfying their information needs 
%effectively 
%with reduced cognitive load.
%the exploratory search. 
%Our MCSI 
%interface 
%and a comparable conventional 
%traditional 
%search interface were
%have been 
%compared and tested in four different areas software usability, cognitive load, search experience and knowledge expansion. It was found
%has been observed 
%that the conversational interface 
%has 
%provided a better user search experience than the traditional search engine.

Clearly our
%the 
existing rule-based search agent can be extended in terms of functionality, and going forward we aim to examine basing its functionality on machine learning based methods, but this will require access to sufficient suitable training data, which is not available at this prototype stage.

\section*{Acknowledgement}

This work was supported by Science Foundation Ireland as part of the ADAPT Centre (Grant 13\//RC\//2106) at Dublin City University.

%\section{\uppercase{Conclusions}}
%\label{sec:conclusion}

%Please note that ONLY the files required to compile your paper should be submitted. Previous versions or examples MUST be removed from the compilation directory before submission.

%We hope you find the information in this template useful in the preparation of your submission.

%\vfill
%\section*{\uppercase{Acknowledgements}}

%If any, should be placed before the references section
%without numbering. To do so please use the following command:
%\textit{$\backslash$section*\{ACKNOWLEDGEMENTS\}}



\bibliographystyle{apalike}
\bibliography{references}  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .


%%% Uncomment this section and comment out the \bibliography{references} line above to use inline references.
% \begin{thebibliography}{1}

% 	\bibitem{kour2014real}
% 	George Kour and Raid Saabne.
% 	\newblock Real-time segmentation of on-line handwritten arabic script.
% 	\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
% 			International Conference on}, pages 417--422. IEEE, 2014.

% 	\bibitem{kour2014fast}
% 	George Kour and Raid Saabne.
% 	\newblock Fast classification of handwritten on-line arabic characters.
% 	\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
% 			International Conference of}, pages 312--318. IEEE, 2014.

% 	\bibitem{hadash2018estimate}
% 	Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
% 	Jacovi.
% 	\newblock Estimate and replace: A novel approach to integrating deep neural
% 	networks with existing applications.
% 	\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

% \end{thebibliography}


\end{document}
