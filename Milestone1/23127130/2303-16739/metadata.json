{
    "arxiv_id": "2303.16739",
    "paper_title": "Active Implicit Object Reconstruction using Uncertainty-guided Next-Best-View Optimization",
    "authors": [
        "Dongyu Yan",
        "Jianheng Liu",
        "Fengyu Quan",
        "Haoyao Chen",
        "Mengmeng Fu"
    ],
    "submission_date": "2023-03-29",
    "revised_dates": [
        "2023-08-04"
    ],
    "latest_version": 3,
    "categories": [
        "cs.RO",
        "cs.CV"
    ],
    "abstract": "Actively planning sensor views during object reconstruction is crucial for autonomous mobile robots. An effective method should be able to strike a balance between accuracy and efficiency. In this paper, we propose a seamless integration of the emerging implicit representation with the active reconstruction task. We build an implicit occupancy field as our geometry proxy. While training, the prior object bounding box is utilized as auxiliary information to generate clean and detailed reconstructions. To evaluate view uncertainty, we employ a sampling-based approach that directly extracts entropy from the reconstructed occupancy probability field as our measure of view information gain. This eliminates the need for additional uncertainty maps or learning. Unlike previous methods that compare view uncertainty within a finite set of candidates, we aim to find the next-best-view (NBV) on a continuous manifold. Leveraging the differentiability of the implicit representation, the NBV can be optimized directly by maximizing the view uncertainty using gradient descent. It significantly enhances the method's adaptability to different scenarios. Simulation and real-world experiments demonstrate that our approach effectively improves reconstruction accuracy and efficiency of view planning in active reconstruction tasks. The proposed system will open source at https://github.com/HITSZ-NRSL/ActiveImplicitRecon.git.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.16739v1",
        "http://arxiv.org/pdf/2303.16739v2",
        "http://arxiv.org/pdf/2303.16739v3"
    ],
    "publication_venue": "8 pages, 11 figures, Submitted to IEEE Robotics and Automation Letters (RA-L)"
}