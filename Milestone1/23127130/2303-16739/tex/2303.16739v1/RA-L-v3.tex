%\documentclass[journal]{IEEEtran}
\documentclass[letterpaper, 10 pt, journal, twoside]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}

\usepackage{cite}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float} 
\usepackage{subfigure}
\usepackage{overpic}
\usepackage{wrapfig}
\usepackage{balance} % 平衡参考文献
\usepackage{ulem}
\usepackage{color}
\usepackage[colorlinks,linkcolor=red]{hyperref} % 超链接
\usepackage{multirow}
\usepackage{mathtools}

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

% \overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Active Implicit Object Reconstruction using \\
Uncertainty-guided Next-Best-View Optimziation
}

\author{Dongyu Yan*, Jianheng Liu*, Fengyu Quan, Haoyao Chen  and Mengmeng Fu% <-this % stops a space
\thanks{*D.Y. Yan and J.H. Liu contributed equally.}% <-this % stops a space

\thanks{This work was supported in part by the National Natural Science Foundation of China under Grant U21A20119. (Corresponding author: Haoyao Chen.)}%

\thanks{D.Y. Yan, J.H. Liu, F.Y. Quan and H.Y. Chen are with the School of Mechanical Engineering and Automation, Harbin Institute of Technology Shenzhen, P.R. China.
  {\tt\footnotesize \{21s053072, liujianheng, 18b353011\}@stu.hit.edu.cn, hychen5@hit.edu.cn}.}
  
\thanks{M.M. Fu is with Department of Neurosurgery, Shenzhen University General Hospital, Shenzhen, P.R. China.
  {\tt\footnotesize fumengmeng2019@163.com}.}%
}%


\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Actively planning sensor views during object reconstruction is essential to autonomous mobile robots.
This task is usually performed by evaluating information gain from an explicit uncertainty map.
Existing algorithms compare options among a set of preset candidate views and select the next-best-view from them.
In contrast to these, we take the emerging implicit representation as the object model and seamlessly combine it with the active reconstruction task.
To fully integrate observation information into the model, we propose a supervision method specifically for object-level reconstruction that considers both valid and free space.
Additionally, to directly evaluate view information from the implicit object model, we introduce a sample-based uncertainty evaluation method.
It samples points on rays directly from the object model and uses variations of implicit function inferences as the uncertainty metrics, with no need for voxel traversal or an additional information map.
Leveraging the differentiability of our metrics, it is possible to optimize the next-best-view by maximizing the uncertainty continuously.
This does away with the traditionally-used candidate views setting, which may provide sub-optimal results.
Experiments in simulations and real-world scenes show that our method effectively improves the reconstruction accuracy and the view-planning efficiency of active reconstruction tasks.
The proposed system is going to open source at \url{https://github.com/HITSZ-NRSL/ActiveImplicitRecon.git}.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Active reconstruction plays an important role in various autonomous robotics tasks, such as perception, manipulation, and scene understanding.
The ultimate goal of active reconstruction is to propel the robot to reconstruct the finest model using the fewest views.
This requires the autonomous system to evaluate current scene information, plan for future motions, and integrate the newly acquired information into the map.
The key point of the task is properly estimating the information gain of arbitrary given view.
Traditional methods usually build up an explicit map, like voxel grids or point clouds, to store uncertainty information\cite{isler2016information}; here, each view's importance is measured by analysis of the map content in the field of view.
Despite its good performance, the explicit representation of information map still has problems including non-continuity, low evaluation speed and requirement of additional maintenance, causing difficulties in view selection.

With the development of implicit representation, modeling scene using implicit functions has shown much potential in 3D reconstruction.
Compared to traditional explicit representations, implicit representation preserves information with much less storage and further represents scenes using continuous and differentiable functions.
However, the downsides of slow training and forgetting problems still make it difficult to directly integrate implicit representation into active reconstruction pipelines.
In this work, we develop solutions for these problems and propose guiding active view planning based on implicit representation.

We focus our work on object-level reconstruction.
By utilizing the bounded property of objects, we limit our implicit function to a bounding box for full usage of the model capacity.
We also use the bounding box to guide our sampling strategy during supervision, creating quick and precise reconstruction.
To integrate newly collected view information as much as possible into the implicit model, we propose a supervision approach considering both free and valid rays, as this provides sufficient information for uncertainty evaluation and fine reconstruction.

As for view planning, we utilize the concept of the volumetric-based next-best-view (NBV) planning method.
We use implicit occupancy field as the scene representation, which can not only guarantee precise geometry but also provide evaluation metrics for the NBV selection.
In this way, we can maintain only one map for both tasks.
By giving a spatial coordinate, the implicit function returns the occupied probability of that position; from this, entropy can be calculated, and the uncertainty of position in space can be represented without the need for an additional information map.
Relying on this property, we can directly sample rays and points from arbitrarily given views instead of using voxel traversing, making NBV evaluation faster, smoother, and more succinct.
Using implicit representation offers another advantage:
since our model has a differentiable form, we no longer need to evaluate preset candidate views to choose an NBV; instead, we can directly optimize the NBV by continuously maximizing the uncertainty metrics.

To the best of our knowledge, our method is the first to plan NBV based on implicit representation without the need for candidate views, using an optimization-based approach.
In summary, our main contributions are the following:
\begin{itemize}
    \item We introduce a new reconstruction pipeline for object-level reconstruction.
    \item We integrate the NBV planning problem into our implicit representation with our proposed uncertainty metrics.
    \item Leveraging the differentiable uncertainty representation, we directly optimize the NBV, doing away with the candidate views required in most traditional methods.
    \item Experiments on both simulation and real-world scenes demonstrate that our method achieves excellent results in both reconstruction quality and view planning efficiency.
\end{itemize}

\section{Related Work}

\subsection{Implicit Neural Reconstruction}

Representing geometry, appearance, and other scene attributes (e.g., surface normal, semantic label, etc.) with an implicit neural network has received much attention\cite{sitzmann2020implicit, mildenhall2020nerf}.
In contrast to traditional 3D representations, the implicit model represents scenes with a continuous function; by giving a coordinate, the forward pass of the function returns scene information at the position.
The nature of implicit representation results in high spatial resolution and little memory cost compared to traditional discrete representations.

To supervise the implicit function, neural radiance field (NeRF) \cite{mildenhall2020nerf} proposes using volumetric rendering to generate comparisons with the captured images.
Although good-looking synthesized views can be generated, the surface geometry lacks accuracy, mainly due to the use of volume density as its geometry attribute.
To this end, SDF\cite{park2019deepsdf, yariv2020multiview, wang2021neus, yariv2021volume} or occupancy\cite{niemeyer2020differentiable, oechsle2021unisurf} are introduced into volume rendering to attain better-generated geometry.
However, it is still hard to apply this to robotics applications due to its low training speed.
To resolve this biggest drawback, depth information is fused\cite{azinovic2021neural, deng2021depth, roessle2021dense, zhu2021nice} to not only help the model converge faster but also solve the ambiguity of geometry and appearance.
Some methods\cite{liu2020neural, zhu2021nice, sun2022direct, muller2022instant} utilize voxel-grids to store features discretely and use interpolation and a shallow decoder to infer the final result.
This can also boost training, due to the simple computation and centralized learnable features.
With the help of depth information and the feature grid encoder, many works become real-time capable and can be applied in robotics tasks.

Our reconstruction method follows the aforementioned depth-aided supervision and feature grid structure to achieve real-time ability.
We also propose a novel reconstruction pipeline that is specific to object-level reconstruction.

\subsection{Next-Best-View Planning}

The goal of NBV planning is to obtain the next sensor view that can reduce the reconstructed object's uncertainty as much as possible.
Exploring an unknown environment is essential for autonomous mobile robots.
For an unknown object, predefined trajectories may fail to perceive some unusual (which may be neglected) or complex (which may be insufficiently viewed) structures.
With NBV planning, robots can reconstruct the unknown object autonomously, with the knowledge of the uncertainty distribution of the map.

The NBV methods can be divided, according to the type of the information gain metrics, into frontier-based\cite{yamauchi1997frontier, vasquez2014volumetric}, surface-based\cite{pito1999solution, chen2005vision, wu2014quality} and volumetric-based\cite{isler2016information, daudelin2017adaptable, delmerico2018comparison} methods.
Frontier-based methods take voxel-grids as the map representation and define the border of free and unknown voxels as frontiers\cite{yamauchi1997frontier}; then, the NBV is further determined by the frontier information.
Surface-based methods represent the global map as a 3D surface model, such as mesh, surface points, and signed distance function.
Once the surface model is generated, the algorithm can follow its contour or curvature tendency to guide NBV selection\cite{wu2014quality}.
Unlike these approaches, volumetric-based methods\cite{isler2016information, daudelin2017adaptable} analyze the full spatial information of the view: they take the traversed voxels by ray marching to calculate NBV metrics, and the view with the highest information gain is selected from among all candidate views as the next target position of the robot.

Our approach follows volumetric-based methods and also uses volume information for our metrics.
However, instead of evaluating through voxel traversing, we utilize the reconstructed implicit representation of the object to seamlessly calculate an uncertainty value.
We also managed to do away with the traditional candidate view selection manner, instead achieving continuous optimization of the NBV by maximizing the information gain.
This results in a more agile reconstruction process.

\begin{figure}[t]
    \centering
    \includegraphics[width = 0.48\textwidth]{images/flow.pdf}
    \caption{
        The architecture of our method. When the robot reaches view number $k-1$, the equipped sensor records RGB-D images of the view. Comparing the data with the rendered image from the object model can supervise the model with the newly acquired scene information. After integration is complete, the NBV planner evaluates the information gain directly using the object implicit model and optimizes an NBV as view number $k$ to instruct the robot's movement.
    }
    \label{fig:flow}
\end{figure}

\section{System Overview}

In what follows, we define our problem setting.
For an unknown object, our goal is to reconstruct the most detailed object model with the least number of views.
We combine the reconstruction problem with the NBV planning problem using a unified implicit map, which does not require building another information map

Our method runs on a mobile robot equipped with an RGB-D camera.
The entire pipeline is shown in Fig.~\ref{fig:flow}.
With an initial view of the object, we first reconstruct its implicit model  with the single view information.
Then, using implicit representation-based criteria, the NBV module searches for the next-best-view in the predefined continuous region and plans a path to guide the robot to the next-best-view.
After the robot navigates to the target position and receives new information, the reconstruction module integrates the new observation into the implicit model, creating a consistent object representation.
As the process continues, the robot repeats the above pipeline until the model is fully explored or until a maximum number of views is visited.

\section{Implicit Reconstruction}
\label{sec:recon}

\subsection{Background}\label{sec:background}

Our implicit model can be represented as $f(\mathbf{x}, \Theta)$.
With a spatial coordinate $\mathbf{x}$ as input, the model returns geometry and appearance information at that position.
The scene information is saved implicitly in the function's parameters $\Theta$.
Recent research in NeRF \cite{mildenhall2020nerf} proposes fitting a 3D scene $f(\mathbf{x}, \Theta)$ using differentiable rendering with only 2D supervision.
We follow volume rendering supervision to reconstruct our 3D object representation. 
However, uniquely, we use occupancy probability $o(\mathbf{x}) \in [0, 1]$ as our geometry property\cite{oechsle2021unisurf}. 

Given the camera intrinsic $\mathbf{K}$ and extrinsic $\left\{\mathbf{T}_i\right\}_{i=1}^{N_{v}}$  of $N_v$ views, we sample $N_s$ points on the rays cast by each pixels.
Inferring these points through the implicit function, we can obtain their occupancy $o$ and color $\mathbf{c}$ information:
\begin{equation}
o(\mathbf{x}),\mathbf{c}(\mathbf{x})=f(\mathbf{x},\Theta).
\end{equation}
Then, with volume rendering, we can render the color and depth value of a certain pixel by aggregating the $N_s$ spatial information on the ray with a weighted sum:
\begin{equation}
    \hat{\mathbf{C}}(\mathbf{r})=\sum_{i=1}^{N_s} w\left(\mathbf{x}_i\right)\mathbf{c}\left(\mathbf{x}_i\right),
    \hat{D}(\mathbf{r})=\sum_{i=1}^{N_s}
    w\left(\mathbf{x}_i\right)d\left(\mathbf{x}_i\right),
    \label{eq:render}
\end{equation}
where, $\mathbf{x}_i$ denotes the $i^{th}$ sampled points on ray $\mathbf{r}$ and $d(\mathbf{x}_i)$ denotes the depth value at position $\mathbf{x}_i$.
The weight consists of the occupancy value of $\mathbf{x}_i$ and an occlusion term $T\left(\mathbf{x}_i\right)$:
\begin{equation}
    w\left(\mathbf{x}_i\right)=o\left(\mathbf{x}_i\right)T\left(\mathbf{x}_i\right),
    T\left(\mathbf{x}_i\right)=\prod_{j=1}^{i-1}\left(1-o\left(\mathbf{x}_j\right)\right).
    \label{eq:weight}
\end{equation}

By comparing the ground truth color $\mathbf{C}(\mathbf{r})$ and depth $D(\mathbf{r})$ with the rendered color and depth, we can add supervision to the implicit model.
The loss function can be defined as:
\begin{equation}
    \mathcal{L}_{color}=\frac{1}{\left\|\mathcal{R}\right\|}\sum_{\mathbf{r} \in \mathcal{R}}\left\|\hat{\mathbf{C}}(\mathbf{r})-\mathbf{C}(\mathbf{r})\right\|_2^2,
    \label{eq:color}
\end{equation}
\begin{equation}
    \mathcal{L}_{depth}=\frac{1}{\left\|\mathcal{R}\right\|}\sum_{\mathbf{r} \in \mathcal{R}}\left\|\hat{D}(\mathbf{r})-D(\mathbf{r})\right\|_1,
    \label{eq:depth}
\end{equation}
where, $\mathcal{R}$ denotes a batch of rays sampled on the given set of camera views.

Although this supervision method of implicit representation performs well in an ideal sensor setup and with dense input, it loses accuracy and generates floaters when these conditions are not met.
Furthermore, without region of interest to constrain it, the method may take background information into account, which may affect NBV planning.
We further optimize the implicit reconstruction pipeline specifically for object-level reconstruction.

\subsection{Implicit Supervision for Object}

Since we specify our reconstruction targets as objects rather than scenes, we can suppose that the object is within an axis-aligned bounding box (AABB). Due to this setting, further optimization of the supervision method can be achieved

\subsubsection{Model Structure}

Traditionally, a multi-layer perceptron (MLP) with a positional encoding is used as the choice of the model structure.
Although it achieves good reconstruction results, the MLP representation has the downsides of slow convergence and low capacity.
Feeding the input data sequentially can cause forgetting problems, losing information previously obtained\cite{sucar2021imap, yan2021continual}.

Inspired by Instant-NGP\cite{muller2022instant}, we utilize the multi-resolution hash table as a replace of the positional encoding.
With the multi-resolution concatenation and the trilinear interpolation, high-frequency signals can be encoded efficiently.
Through initialization of all the learnable parameters with mean $0$ distributions, the occupancy field can be initialized to near $0.5$ anywhere within the scene, which means no information has been acquired.
This initialization can benefit the later process of NBV planning.
We also set the bound of the hash volume as the object's bound, ensuring that the model’s capacity is fully used.
This model structure significantly speeds up training, making our method real-time capable.

\subsubsection{Rays and Points Sampling}

\begin{figure}[htpb]
    \centering
    \includegraphics[width = 0.45\textwidth]{images/rays.pdf}
    \caption{
      Illustration of ray types defined in our reconstruction method.
    }
    \label{fig:rays}
\end{figure}

By assuming our target object is within a bound, we can further regularize the supervision of our implicit model.
Given an AABB $\{\mathbf{p}_{min},\mathbf{p}_{max}\}$ and a set of camera views with images, we first sample $N_r$ rays with a uniform distribution in the views.
Then, for each ray, we calculate its intersection depth with the AABB.
Based on the intersection depth $\{d_{near}, d_{far}\}$, the preset max depth $d_{max}$, and the measured depth $d(\mathbf{r})$, we can classify the ray into 5 types, as shown in Fig. \ref{fig:rays}:
\begin{enumerate}
    \item{\makebox[5.6cm]{Rays with no intersection:\hfill} $d_{near}>d_{far}$.}
    \item{\makebox[5.6cm]{Rays with no depth measurement:\hfill} $d(\mathbf{r})=None$.}
    \item{\makebox[5.6cm]{Depth out of measurement range:\hfill} $d(\mathbf{r})>d_{max}$.}
    \item{\makebox[5.6cm]{Depth out of bounding box:\hfill} $d(\mathbf{r})>d_{far}$.}
    \item{\makebox[5.6cm]{Rays with valid depth measurement:\hfill} $Others$.}
\end{enumerate}
We treat 1) and 2) as invalid rays, as they do not contribute to the reconstruction.
We treat 3) and 4) as free rays, providing free space information in the bounding box.
We treat 5) as valid rays with good depth measurement, providing object occupancy information.

We constrain sample points using the AABB to only count useful information.
For free rays, we sample points in a stratified way: we evenly divide the depth range $\{d_{near},d_{far}\}$ into $N_f$ sub-intervals and sample one point in each.
For valid rays, we use depth-guided sampling: we sample $N_s$ points near the estimated surface following a normal distribution $\mathcal{N}\left(d(\mathbf{r}),{\sigma_s}^2\right)$.
These can focus more on surface regions and create a fine-grained model.

\subsubsection{Optimization}

After sampling rays and points, we build supervision using differentiable rendering loss and direct loss.
For valid rays, we obey the rendering formula in Equation (\ref{eq:render}) and the supervision loss in Equations (\ref{eq:color}) and (\ref{eq:depth}).
We directly supervise free rays to be non-occupied, using a binary-cross-entropy (BCE) loss:
\begin{equation}
    \mathcal{L}_{free}=\frac{1}{N_{f}}\sum_{k=0}^{N_{f}}-\log\left(1-f(\mathbf{x}_k,\Theta)\right),
\end{equation}
where $N_{f}$ denotes the number of points sampled on the free ray.
The final loss function can be written as a linear combination of the above losses ($\lambda_1=2, \lambda_2=0.5$):
\begin{equation}
    \mathcal{L}=\mathcal{L}_{color}+\lambda_1 \mathcal{L}_{depth}+\lambda_2 \mathcal{L}_{free}.
    \label{eq:overall_loss}
\end{equation}

In practice, we find that the inaccuracy of robot pose estimation can cause misalignment of different views.
Therefore, we additionally add $\left\{\mathbf{T}_i\right\}_{i=1}^{N_{v}}$ into the optimization.
Through joint optimization of both the implicit object model and the sensor poses, a more consistent result can be obtained.

In summary, our reconstruction approach considers both rays cast on the object and free space, creating full supervision of the bounded region.
In this way, the view information can be fully merged into the global map, providing a good model understanding for the NBV planning, as shown in Fig.~\ref{fig:void_ablation}.

\begin{figure}[htpb]
    \centering
    \begin{minipage}[b]{0.225\textwidth}
    \centering
        \includegraphics[width=\textwidth]{images/mini_images/arma_wo.png}
        \footnotesize{(a) w/o free ray supervision}
    \end{minipage}
    \begin{minipage}[b]{0.225\textwidth}
    \centering
        \includegraphics[width=\textwidth]{images/mini_images/arma_w.png}
        \footnotesize{(b) with free ray supervision}
    \end{minipage}
    \caption{
      Ablation study on the use of free ray supervision.
      With free ray supervision, rays with no valid depth in the depth map can be utilized and floaters in these regions can be removed.
    }
    \label{fig:void_ablation}
  \end{figure} 

\section{NBV Planning}
\label{sec:NBV}

\subsection{View Uncertainty Evaluation}
\label{sec:view_eval}

The view uncertainty evaluation module aims to assign quantitative indicators to any given view in order to compare it against others.
As the most essential part of NBV algorithms, the view uncertainty evaluation’s design determines the performance of the active reconstruction method.
Building on the spatial representing ability of the implicit model, we can draw similar metrics to those of the traditional volumetric-based method.
We can arbitrarily sample points in the space using the implicit function to obtain its uncertainty, without the need for voxel traversal (used by explicit representation).
In this way, computation time can be greatly reduced.
Following the occlusion-aware volumetric information introduced by \cite{isler2016information}, we are able to present a simple transplantation from explicit to implicit.
First, due to the requirement of occlusion information, we sample $N_p$ points on $N_e$ random rays cast by the given view.
Then, the uncertainty of a sample point $\mathbf{x}$ can be represented by its entropy:
\begin{equation}
    \mathcal{I}_o(\mathbf{x})=-o(\mathbf{x}) \ln o(\mathbf{x})-\bar{o}(\mathbf{x}) \ln \bar{o}(\mathbf{x}),
\end{equation}
where $o(\mathbf{x})$ is the occupancy probability inferred from the implicit function.
The final volumetric information considering occlusion takes the form of:
\begin{equation}
    \mathcal{I}_v(\mathbf{x})=T(\mathbf{x}) \mathcal{I}_o(\mathbf{x}),
\end{equation}
where $T(\mathbf{x})$ is the occlusion term mentioned in Equation (\ref{eq:weight}).
Finally, with the summing of the volume information of all sampled points in the view, a scalar can be calculated to represent the uncertainty of the view:
\begin{equation}
    \mathcal{I}=\frac{1}{N_e N_p}\sum_{i=1}^{N_e}\sum_{j=1}^{N_p} \mathcal{I}_v(\mathbf{x}_{ij}),
\end{equation}
where $i$ and $j$ represent the sample indices of rays and points respectively.

This evaluation method offers a simple example that implements view uncertainty evaluation into implicit representation.
However, problems still exist that prevent us from achieving a fair and valuable evaluation.
In practice, we find that the baseline method faces the following drawbacks:

1) Certain views have inherently large uncertainties, while others have relatively small ones.
This is essentially caused by the different projected areas of the target object in each view.
As can be seen in Fig. \ref{fig:dragon}, the front view that has already been well reconstructed has a large uncertainty due to the large projected area.
In contrast, the side view part that lacks supervision loses the chance to be the "best view".
This phenomenon often occurs with objects with an unbalanced size, such as the dragon in our experiments.

2) In the later period of the reconstruction process, the goal of active view planning should shift from building a general outline to focusing on details.
However, the use of full-view information makes it difficult to realize this change.
Since the final metric is the uncertainty sum of all sampled points, the view selected in this way may only have large global uncertainty; views with large partial uncertainties, meaning that some detail parts still need observation, may be ignored.

These problems are due to the use of summation to integrate the information of a view, which pays too much attention to the overall metrics and sacrifices the consideration of the details.
To this end, we propose a top-$N_t$ evaluation criterion that can balance global and local information, formatted as:
\begin{equation}
    \label{eq:infomation}
    \mathcal{I}=\frac{1}{N_t N_p}\mathop{top}_{i=1}^{N_e}(\sum_{j=1}^{N_p} \mathcal{I}_v(\mathbf{x}_{ij}), N_t),
\end{equation}
where the function $\mathop{top}_{i=1}^{N_e}(v,N_t)$ entails choosing the top $N_t$ samples by their value $v$ from all $N_e$ candidates.
These criteria regularize the comparison between different views.

\begin{figure}[htpb]
    \includegraphics[width=0.45\textwidth]{images/dragon_problem.pdf}
    \caption{
        Illustration of the unfair evaluation in the dragon's scene.
        The bottom images show its uncertainty color map of different views after the $4^{th}$ reconstruction round.
        Although the front view (the right image) has already been well observed, the uncertainty sum is still significantly higher than that of the side view (the left image) which is not well reconstructed.
    }
    \label{fig:dragon}
\end{figure} 

Given our proposed modification of $\mathcal{I}$, we can obtain a trade-off for global or local attention, by tuning the ratio of $N_t$ and $N_e$.
Although using a fixed ratio can alleviate the problem to some extent, adjusting it in accordance with the reconstruction process can achieve better results theoretically.
To do this, we set up a cosine scheduler ($N_t/N_e=\cos(\frac{\pi}{20} N_v)$) to dynamically decrease the selected ray number according to the total observed view number $N_v$.
This gradually shifts the attention from global information to local details, which is more adaptable to the reconstruction process.

\begin{figure*}[t]
    \centering
    \begin{minipage}[b]{0.3\textwidth}
    \centering
        \includegraphics[width=\textwidth]{images/mini_images/bunny_quati.png}
        \footnotesize{(a) Stanford Bunny}
    \end{minipage}
    \begin{minipage}[b]{0.3\textwidth}
    \centering
        \includegraphics[width=\textwidth]{images/mini_images/dragon_quati.png}
        \footnotesize{(b) Dragon}
    \end{minipage}
    \begin{minipage}[b]{0.3\textwidth}
    \centering
        \includegraphics[width=\textwidth]{images/mini_images/armadillo_quati.png}
        \footnotesize{(c) Armadillo}
    \end{minipage}
    \caption{
      Reconstruction results of the Stanford bunny, dragon, and armadillo in simulation experiments.
    We compare our method against traditional volumetric-based methods on surface coverage, mean entropy in the map, and reconstruction quality.
    Our implicit surface models (extracted as mesh) have finer geometry and smoother surface than voxel representations.
    Notice that our method optimizes the implicit occupancy field while volumetric-based methods fuse observation into occupancy voxels using logistic regression frame-by-frame. This results in a much faster descent rate of entropy in the map of our method.
    }
    \label{fig:compare_result}
    \vspace{-2mm}
\end{figure*} 

\subsection{Optimization-based NBV Planning}

After the evaluation metrics of view uncertainty  are defined, a well-designed strategy is needed to find the best NBV.
The most straightforward approach involves setting a group of candidate views in advance.
For object reconstruction, the candidate views can be sampled from a dome or a cylinder centered over the object.
In this way, we can determine the NBV by traversing the candidate views and selecting the view with the largest uncertainty as the next-best-view.
This method is easy to perform, so it is used by most NBV selecting algorithms.
However, despite its advantages in simplicity, it may lead to sub-optimal view selection  and affect reconstruction results.
Other problems arise with how to set the position of the candidate views: if accurate view selection is needed, the candidate views should be densely set around the object, which may result in slow calculations, since the computational time is proportional to the number of candidate views.
The choice of candidate view distribution also matters.
% Because we can not take into account the prior information of the target object, we can't adapt to it using a predefined distribution of candidate views.
% Whether using dome or cylinder or other form of shapes also makes huge influence to the NBV selection result.

In contrast to evaluation metrics using explicit representation, which can only measure view uncertainty discretely, our proposed view information can be represented as a differentiable function of the view pose $\mathbf{T}$.
Through the relationship between $\mathbf{x}_{ij}$ in Equation (\ref{eq:infomation}) and $\mathbf{T}$, we can rewrite our information function as $\mathcal{I} \coloneqq \mathcal{I}(\mathbf{T})$.
This can be achieved mainly because the information function naturally inherits the differentiable property of implicit function.
By using the back-propagation method, we can solve the optimal view pose that maximizes the information through an optimization method:
\begin{equation}
    \mathbf{T}^*=\arg\max_{\mathbf{T}\in\mathit{\Psi}(n)}\mathcal{I}(\mathbf{T}),
\end{equation}
where $\mathit{\Psi}(n)$ ($n\leq6$) is a sub-manifold of $\mathit{SE}(3)$, which is used to limit the optimization space.
The optimization-based method does away with the non-optimal candidate views and achieves accurate selection without the need for preset.
Although multiple iterations are required, the computational time is not an issue, due to the fast point sampling and inference.

Nevertheless, in practice, the way to initialize the pose to be optimized after reconstruction still requires careful design.
The simple "Start from Last NBV" strategy is prone to getting stuck in a local maximum; this is caused by the great non-convexity of the objective function, which gradually worsens as the reconstruction progresses, since the uncertainty peaks gradually become apparent.
Directly using the result of the previous view for initialization does not enable escaping this maximum.

To better initialize the pose to be optimized, we developed an "Initialize by Sampling" method.
Before optimization, we first sample $N_k$ views uniformly in the manifold $\mathit{\Psi}(n)$ and evaluate their uncertainty, and then we initialize the pose to the one with the largest uncertainty.
By using the "Initialize by Sampling" method, we can start optimizing from a point close to the global maximum.
This can essentially avoid the problem of falling into the local maximum and simplify the optimization task.
The sampling density can be set sparsely since only a coarse guide is needed.

Along with uncertainty metrics, task-oriented factors, such as robot movement or map observability—can be included in the objectives.
% In this work, the viewing angle factor, measuring the angle difference between the current and the historical viewing angle, is considered:
% \begin{equation}
% \mathcal{F}_{angle}(\mathbf{T})=\min_{i=1}^{N_v}cos\left(\left<z(\mathbf{T}),z(\mathbf{T}_i)\right>\right),
% \end{equation}
% where $\left<,\right>$ represents the angle between vectors, and $z()$ stand for the $z$ vector of a pose matrix.
% Since gradient descent algorithm is used, we take the negative of the information and angle factor to form the final loss function used for NBV optimization:
% \begin{equation}
% \mathcal{L}_{NBV}(\mathbf{T})=-\mathcal{I}(\mathbf{T})-w\mathcal{F}_{angle}(\mathbf{T}).
% \end{equation}
In this work, we punish the robot's movement by adding the cost $\mathcal{C}_p(\mathbf{T})$, to avoid collision with the object and ground.
The information gain and robot movement cost form the final utility function used for NBV optimization:
\begin{equation}
\mathcal{U}_{v}(\mathbf{T})=\mathcal{I}(\mathbf{T})-\mathcal{C}_{p}(\mathbf{T}).
\end{equation}
By maximizing the utility function $\mathcal{U}_{v}$, we can optimize the NBV pose and guide the movement of the robot.

% \begin{figure*}[t]
% \begin{center}
%   \includegraphics[width = 0.95\textwidth]{images/quati_compare.pdf}
% \end{center}
% \caption{
%     Reconstruction results of the Stanford bunny, dragon, and armadillo in simulation experiments.
%     We compare our method against traditional volumetric-based methods on surface coverage, mean entropy in map, and the reconstruction quality.
%     Our implicit surface models (extracted as mesh) have finer geometry and smoother surface than voxel representations.
%     Notice that our method optimizes the implicit occupancy field while volumetric-based methods fuse observation into occupancy voxels using logistic regression frame-by-frame. This results in the much faster descent rate of entropy in map of our method.}
% \label{fig:compare_result}
% \end{figure*}
  
\section{Evaluation}

\subsection{Implementation Details}

\begin{table}[htpb]
\caption{Variables and Their Default Values}
  \begin{center}
    \begin{tabular}{ccl}
      \hline { \textbf{Parameters}  } 
      & { \textbf{Value} } 
      & { \textbf{Description} }\\
      \hline
      \hline 
      $\mathbf{p}_{min}, \mathbf{p}_{max}$ & $\pm 0.25m$ & Bounding box of the object\\
      \hline
      $N_k$ & $48$ & Number of views sampled for initialization\\
      \hline
      \multirow{2}{*}{$N_r$, $N_e$} & \multirow{2}{*}{$5000$} & Number of rays for reconstruction \\ & & and NBV evaluation\\
      \hline
      \multirow{2}{*}{$N_s$, $N_f$, $N_p$} &\multirow{2}{*}{ $16$ }& Number of surface points, free points \\& & and NBV evaluation points\\
      \hline
      $\sigma_d$ & $5mm$ & Variance of surface point sampling \\
      \hline
    \end{tabular}
  \end{center}
\label{tab:parameters}
\vspace{-5mm}
\end{table}

We implement our method using Pytorch and utilize ROS for sensor driver and visualization.
We use tiny-cuda-nn\cite{tiny-cuda-nn} to form our implicit network architecture and Adam optimizer \cite{kingma2014adam} for the training, pose refinement, and NBV pose optimization.
The learning rates of the optimizers are set to $2e^{-3}$, $3e^{-3}$, and $1e^{-2}$ respectively.
The settings of parameters mentioned in Section \ref{sec:recon} and Section \ref{sec:NBV} are shown in Table \ref{tab:parameters}.
We freeze the model after $100$ iterations of reconstruction and run another $100$ iterations to optimize the NBV.
When the robot is guided to a new view, we add it to the reconstruction view set and increase $N_v$ by one.
Our reconstruction is complete after $10$ views are observed.
We set the manifold $\mathit{\Psi}(n)$ on which we optimize our NBV pose as a sphere surrounding the object.
We set the camera z-axis pointing at the object and the x-axis vertical z-axis down, forming a 2 DoF manifold.
% The cosine scheduler mentioned in Section \ref{sec:view_eval} for attention changing is set as
% $N_t/N_e=\cos(\frac{\pi}{20} N_v)$.


% \subsection{Runtime Analysis}

% \begin{table}[htpb]
% \caption{Time-Consuming Comparison}
% \begin{center}
%     \begin{tabular}{ccc}
%     \textbf{Method} & \textbf{Reconstruction Time} & \textbf{NBV Searching Time}  \\
%     \hline
%     \hline
%         Isler et. al. \cite{isler2016information} & & 8s\\\hline
%         Lee et. al. \cite{lee2022uncertainty} & 8.2min & 2.8min\\\hline
%         Ours & 1.626s & 0.796s\\\hline
%     \end{tabular}
% \end{center}
% \label{tab:runtime}
% \end{table}

% \textcolor{red}{Tested on different devices (CPU and Nvidia 3070ti) Fairness of time}

% In order to achieve high runtime speed, we leverage Nvidia's tiny-cuda-nn\cite{tiny-cuda-nn} for our network architecture and optimizer.
% The average training time and the next-best-view optimization time for each round are 1.626s and 0.796s, respectively.
% We compare the runtime ability of our method with others and the result is in Table \ref{tab:runtime}.
% It achieves faster reconstruction speed and NBV searching speed against the traditional method \cite{isler2016information}, leveraging the powerful GPU.
  
\subsection{Simulation Experiments}

% \begin{figure}[htpb]
%     \centering
%     \includegraphics[width = 0.45\textwidth]{images/simulation}
%     \caption{
%         Simulation scene.
%     }
%     \vspace{-2mm}
%     \label{fig:gazebo}
% \end{figure}
  
% \begin{figure}[htbp]
%     \centering
%     \begin{minipage}[b]{0.15\textwidth}
%     \centering
%       \includegraphics[width=\textwidth]{images/mini_images/bunny.png}
%       \footnotesize{(a) Stanford Bunny}
%     \end{minipage}
%     \begin{minipage}[b]{0.15\textwidth}
%     \centering
%       \includegraphics[width=\textwidth]{images/mini_images/dragon.png}
%       \footnotesize{(b) Dragon}
%     \end{minipage}
%     \begin{minipage}[b]{0.15\textwidth}
%     \centering
%       \includegraphics[width=\textwidth]{images/mini_images/arma.png}
%       \footnotesize{(c) Armadillo}
%     \end{minipage}
%     \caption{
%         Synthetic model datasets (ground truth models).
%     }
%     \vspace{-1mm}
%     \label{fig:model}
% \end{figure}
%(Fig.~\ref{fig:gazebo}).

We use Gazebo as our simulation environment.
In our reconstruction scene, the target object is placed at the center of a room.
Ground truth color and depth images are captured by a flying RGB-D camera, the movement of which is controlled by our NBV planner.

We test our method against the volumetric-based method\cite{isler2016information, delmerico2018comparison} on three object models: Stanford Bunny, Dragon, and Armadillo (Fig.~\ref{fig:compare_result}). %(Fig.~\ref{fig:model}).
We follow the settings of \cite{delmerico2018comparison} and evaluate the map's surface coverage and total entropy.

To quantify the reconstruction progress in terms of surface coverage, we compare the point cloud model obtained during reconstruction with a point cloud generated from the ground truth model.
For each point in the ground truth model, the closest point in the reconstruction point cloud is queried: if this point is closer than a registration threshold (5mm), the surface point of the original model is considered to be matched.
The surface coverage $c_s$ is, then, the percentage of matched surface points out of the original model's total surface points:
\begin{equation}
    c_s=\frac{\text {Matched surface points}}{\text { Surface points in original model }}.
\end{equation}

Since we do not have voxel representations in our model, in order to conduct a fair comparison with volumetric-based methods, we evaluate the entropy of our reconstructed model using a sampling-based approach.
We use a 3D grid sized ${128}^3$ to sample entropy in the map.
A map's Shannon entropy has the following format:
\begin{equation}
\text { Entropy }=\frac{1}{{128}^3}\sum_{i=1}^{{128}^3} \mathcal{S}(\mathbf{x}_i),
\end{equation}
\begin{equation}
\mathcal{S}(\mathbf{x})= o(\mathbf{x})\log_2 o(\mathbf{x})-\bar{o}(\mathbf{x}) \log_2 \bar{o}(\mathbf{x}).
\end{equation}

After implicit reconstruction is finished, the Marching Cubes algorithm \cite{lorensen1987marching} is used to extract mesh representation for visualization, as shown in Fig.~\ref{fig:recon_process}.
% Notice that colors of mesh vertices can also be applied by $\mathbf{c}(\mathbf{x})$ inference of the implicit function.

We show the quantitative and qualitative evaluation results in Fig. \ref{fig:compare_result}.
In the reconstructed meshes, we can see that our implicit surface models (extracted as mesh) have finer geometry and smoother surface than voxel representation.
Quantitatively, we compare our method to traditional volume-based methods using candidate views, including occlusion-aware metrics and average-energy metrics, as well as a random selecting method.
By evaluating the curve of surface coverage and entropy in the map, we find that our method achieves more efficient reconstruction than other methods.

  \begin{figure}[htpb]
    \centering
    \begin{minipage}[b]{0.1125\textwidth}
    \centering
      \includegraphics[width=\textwidth]{images/mini_images/view0.png}
      \footnotesize{(a) step = 0}
    \end{minipage}
    \begin{minipage}[b]{0.1125\textwidth}
    \centering
      \includegraphics[width=\textwidth]{images/mini_images/view1.png}
      \footnotesize{(b) step = 1}
    \end{minipage}    \begin{minipage}[b]{0.1125\textwidth}
    \centering
      \includegraphics[width=\textwidth]{images/mini_images/view2.png}
      \footnotesize{(c) step = 2}
    \end{minipage}
    \begin{minipage}[b]{0.1125\textwidth}
    \centering
      \includegraphics[width=\textwidth]{images/mini_images/view20.png}
      \footnotesize{(d) step = 20}
    \end{minipage}
    
    \caption{
      Visualization of the reconstruction process.
      The reconstruction results from steps 0, 1, 2, and 20 are shown in mesh extracted by Marching Cubes \cite{lorensen1987marching}.
      The bottom row shows the entropy map, viewed in three-view perspective by sum projection.
    }
    \label{fig:recon_process}
  \end{figure}

To demonstrate the effect of our top-$N_t$ metrics and NBV planning strategies, we conduct comparison studies on the Stanford Bunny object.
In Fig. \ref{fig:strategies}, we hold our reconstruction pipeline constant and show the experiment results using different evaluation metrics and NBV planning strategies.
The results show that using sum metrics performs weak in the later process, due to the lack of attention to details; the optimization-based method using simple initialization got stuck in the local maximum; with top-$N_t$ metrics and sampling initialization, our method is able to refine details and avoid local maximum.
Leveraging the NBV optimization, our method also outperforms selection-based methods using preset candidate views, which can only obtain sub-optimal results.

We also evaluate the pose refinement module to demonstrate its adaptability to inaccurate reference poses.
In this experiment, we add noise to the ground truth poses in the simulation environment—specifically, we add uniform distribution of $U(-0.05, 0.05)$ radius to rotation, and $U(-0.05, 0.05)$ meter to translation.
The reconstruction results are shown in Fig. \ref{fig:pose_opt}.
In the visualization, we prove the ability of our pose refinement module to recover a consistent object model even despite severe frame misalignment.

\begin{figure}[htpb]
    \centering
      \includegraphics[width=0.45\textwidth]{images/ablation_nbv.png}
    \caption{
      Comparison study of the NBV strategies.
      With the help of top-$N_t$ evaluation metrics, optimization-based NBV planning and sampling initialization, our method escapes the local maximum and results in better reconstruction than the others.
    }
    \label{fig:strategies}
\end{figure} 

\begin{figure}[htpb]
    \centering
    \begin{minipage}[b]{0.225\textwidth}
    \centering
      \includegraphics[width=\textwidth]{images/mini_images/wo_pose_opt.png}
      \footnotesize{(a) w/o pose optimization}
    \end{minipage}
    \begin{minipage}[b]{0.225\textwidth}
    \centering
      \includegraphics[width=\textwidth]{images/mini_images/w_pose_opt.png}
      \footnotesize{(b) with pose optimization}
    \end{minipage}
    \caption{
      Ablation study on the use of pose optimization.
      The upper row shows the reconstructed model, and the lower row shows the input point cloud.
      With the use of multi-view information, noisy poses are optimized together with the implicit model to output better reconstruction.
    }
    \label{fig:pose_opt}
  \end{figure}
  
  
\begin{figure}[htbp]
    \centering
    \begin{minipage}[b]{0.1884\textwidth}
    \centering
      \includegraphics[width=\textwidth]{images/mini_images/uav.png}
      \footnotesize{(a) Micro aerial vehicle}
    \end{minipage}
    \begin{minipage}[b]{0.2616\textwidth}
    \centering
      \includegraphics[width=\textwidth]{images/mini_images/object.png}
      \footnotesize{(b) Reconstruction object}
    \end{minipage}
    \caption{
      A compact aerial robot equipped with a tiltable lidar camera and the real-world reconstruction object
    }
    \label{fig:uav}
  \end{figure}

\subsection{Real-world Experiments}

In our real-world active reconstruction experiments, we use a compact micro aerial vehicle (MAV) equipped with a tiltable RGB-D camera (Intel Realsense L515) for data collection, and a laptop with a single NVIDIA 3060 Laptop GPU for computation.
Color and aligned depth images are streamed from the MAV to the laptop to perform implicit reconstruction and NBV planning. Our method achieves fast reconstruction ($2.204s$ per step) and NBV searching ($0.675s$ per step). 
Fig.~\ref{fig:uav} shows the MAV we used (a), and the reconstruction object (b) in the real-world experiment.

In Fig.~\ref{fig:realworld_recon}, we present the result of the real-world experiments.
Due to the inaccurate reference poses of the camera views, the stitched global point clouds in Fig.~\ref{fig:realworld_recon} (a) has large misalignments between different frames.
In contrast, our proposed method is capable of optimizing the poses during reconstruction, making the main object reconstruction consistent.
The reconstructed implicit surface in Fig.~\ref{fig:realworld_recon} (b) shows that our method is able to build fine-grained object model from sparse and noisy input frames.

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.225\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mini_images/pcl_crop.png}
      \footnotesize{(a) Raw point cloud}
    \end{minipage}
    \begin{minipage}{0.225\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/mini_images/object_crop.png}
      \footnotesize{(b) Implicit surface}
    \end{minipage}
    \caption{
      The raw point cloud stitched by frames in different colors (a), and the implicit reconstruction surface (b) of the real-world experiments.
      The misalignment of the point cloud frames are corrected in the implicit surface.
    }
    \label{fig:realworld_recon}
  \end{figure}

\section{Conclusion}

This paper explores the problem of active implicit reconstruction of an unknown object.
We combine the long-studied problem of NBV planning with the emerging implicit representation and propose directly utilizing the implicit occupancy field to guide the NBV process.
Due to the differentiability of the implicit representation, we are able to do away with the candidate views commonly used by former research and propose optimizing NBV directly by maximizing the view uncertainty.
Our method generates a high-precision object model and performs efficiently during view planning, with the help of implicit representation.
In our future work, we aim to boost robot exploration tasks in more complex scenes leveraging the optimization-based NBV planning method we proposed.

% \addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{
\bibliographystyle{IEEEtran}
\balance
\bibliography{reference}
}

\end{document}
