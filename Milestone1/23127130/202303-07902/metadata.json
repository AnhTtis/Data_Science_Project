{
    "arxiv_id": "2303.07902",
    "paper_title": "BLAT: Bootstrapping Language-Audio Pre-training based on AudioSet Tag-guided Synthetic Data",
    "authors": [
        "Xuenan Xu",
        "Zhiling Zhang",
        "Zelin Zhou",
        "Pingyue Zhang",
        "Zeyu Xie",
        "Mengyue Wu",
        "Kenny Q. Zhu"
    ],
    "submission_date": "2023-03-14",
    "revised_dates": [],
    "latest_version": 1,
    "categories": [
        "cs.SD",
        "eess.AS"
    ],
    "abstract": "Compared with ample visual-text pre-training research, few works explore\naudio-text pre-training, mostly due to the lack of sufficient parallel\naudio-text data. Most existing methods incorporate the visual modality as a\npivot for audio-text pre-training, which inevitably induces data noise. In this\npaper, we propose BLAT: Bootstrapping Language-Audio pre-training based on\nTag-guided synthetic data. We utilize audio captioning to generate text\ndirectly from audio, without the aid of the visual modality so that potential\nnoise from modality mismatch is eliminated. Furthermore, we propose caption\ngeneration under the guidance of AudioSet tags, leading to more accurate\ncaptions. With the above two improvements, we curate high-quality, large-scale\nparallel audio-text data, based on which we perform audio-text pre-training.\nEvaluation on a series of downstream tasks indicates that BLAT achieves SOTA\nzero-shot classification performance on most datasets and significant\nperformance improvement when fine-tuned on downstream tasks, suggesting the\neffectiveness of our synthetic data.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.07902v1"
    ],
    "publication_venue": null
}