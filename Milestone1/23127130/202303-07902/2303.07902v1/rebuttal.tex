\documentclass[onecolumn]{article}
\usepackage{spconf}
\usepackage{amsmath}

\usepackage[letterpaper,bottom=2cm,left=2cm,right=2cm,top=2cm]{geometry}

\begin{document}
\paragraph*{Reviewer \#4}
As stated in the Introduction Section, data scarcity is the main challenge for audio-text pre-training: current audio-text datasets are all small-scale.
Previous works curate large-scale audio-text data through visual modality but the quality of such data is not satisfactory due to the modality mismatch stated in the third paragraph of the Introduction Section.
Therefore, \textbf{we use tag-guided data to a) enlarge the audio-text pre-training data; b) improve the quality of curated audio-text data (by using tag guidance)}:
\begin{itemize}
    \item The comparison between BLAT and CLAP in Table 5 validates the effect of extra training data, since CLAP used only current audio-text data for training.
    \item The improvement of data quality stems from two aspects:
    \begin{itemize}
        \item Unlike previous works, \textbf{visual modality is not involved} into data curation so we eliminate the noise caused by modality mismatch. This is validated by comparing BLAT with VIP$\sim$$\text{A}_\text{N}$T in Table 2, since VIP$\sim$$\text{A}_\text{N}$T used the same audio source to curate data but the curation process incorporated the visual modality.
        \item We \textbf{incorporate human-annotated tags} into data curation. Since these sound event tags are annotated by human, they provide reliable guidance for text generation so we leverage them into data curation. The improvement brought by tag guidance is shown in Table 3 by comparing ``Synthetic w/o tag'' with ``Synthetic''.
    \end{itemize}
\end{itemize}

\end{document}