% Template for ICME 2022 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP/ICME LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,epsfig}
\usepackage{pifont}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{cleveref}

\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
  \OLDthebibliography{#1}
  \setlength{\parskip}{0pt}
  \setlength{\itemsep}{0pt plus 0.3ex}
}

\pagestyle{empty}


\begin{document}\sloppy

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}


% Title.
% ------
\title{BLAT: Bootstrapping Language-Audio Pre-training based on AudioSet Tag-guided Synthetic Data}
%
% Single address.
% ---------------
\name{Xuenan Xu, Zhiling Zhang, Zelin Zhou, Pingyue Zhang, Zeyu Xie, Mengyue Wu$^\dagger$, Kenny Q. Zhu$^\dagger$\thanks{$^\dagger$ Corresponding Author.}}
%Address and e-mail should NOT be added in the submission paper. They should be present only in the camera ready paper. 
\address{Department of Computer Science and Engineering\\
	 Shanghai Jiao Tong University, Shanghai, China}


\maketitle


%
\begin{abstract}
Compared with ample visual-text pre-training research, few works explore audio-text pre-training, mostly due to the lack of sufficient parallel audio-text data.
Most existing methods incorporate the visual modality as a pivot for audio-text pre-training, which inevitably induces data noise.
In this paper, we propose BLAT: Bootstrapping Language-Audio pre-training based on Tag-guided synthetic data.
We utilize audio captioning to generate text directly from audio, without the aid of the visual modality so that potential noise from modality mismatch is eliminated.
Furthermore, we propose caption generation under the guidance of AudioSet tags, leading to more accurate captions.
With the above two improvements, we curate high-quality, large-scale parallel audio-text data, based on which we perform audio-text pre-training. %BLAT and evaluate it on a series of downstream tasks, including audio classification (single modality) as well as audio-text retrieval and audio captioning (cross-modal).
%Experimental results with comparison to a series of audio and cross-modal pre-training models 
Evaluation on a series of downstream tasks indicates that BLAT achieves SOTA zero-shot classification performance on most datasets and significant performance improvement when fine-tuned on downstream tasks, suggesting the effectiveness of our synthetic data.
\end{abstract}
%
\begin{keywords}
Multi-modal pre-training, contrastive learning, zero-shot inference, audio classification
\end{keywords}
%
\section{Introduction}
\label{sec:intro}

Multi-modal machine learning has become increasingly popular since it mimics our learning experience: we accept and handle information from different modalities. 
With the success of deep neural networks and large-scale datasets, we have witnessed the rapid development of multi-modal learning in recent years.
Vision-language pre-training~\cite{chen2020uniter,li2020oscar,lu2019vilbert,su2019vl} using Transformer has pushed the state of the art (SOTA) on a wide range of cross-modal tasks, such as visual question answering (VQA)~\cite{antol2015vqa}, Image-Text Retrieval~\cite{lin2014microsoft}, visual commonsense reasoning (VCR)~\cite{zellers2019recognition}, etc.
In these works, a joint representation of vision and language modalities is learned through pre-training on large-scale image-text datasets and then fine-tuned on specific downstream vision-language tasks.

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/Bootstrapping.pdf}
    \caption{The illustration of the data expansion approach. ``A'', ``E'' and ``C'' denote audio, event tags and caption respectively.}
    \label{fig:bootstrapping_data}
\end{figure}

In contrast with the boosting number of vision-language pre-training works, audio-related multi-modal learning, however, is still at a preliminary stage. 
Although audio is an important modality, few works explore pre-training involving audio and language.
The bottleneck of audio-language cross-modal learning lies in the scarcity of audio-text data.
Compared with large-scale image-text datasets such as COCO~\cite{lin2014microsoft} ($\sim$1.64M pairs), Visual Genome~\cite{krishna2017visual} ($\sim$5.06M pairs), and Conceptual Captions~\cite{sharma2018conceptual} ($\sim$ 12M pairs), current audio-text datasets contain only about 100K pairs (see \Cref{subsec:tag_data_generation}).
The lack of large-scale audio-text datasets may be attributed to the fact that not only the audio annotation cost is much higher than image description annotation~\cite{zhang2021enriching}, but audio-text co-occurrences are also scarcely available on the web~\cite{zhao2022connecting}.

To alleviate the above problem of data scarcity, prevailing works on audio-text cross-modal learning mostly incorporate CLIP~\cite{radford2021learning}, a powerful model enabling image-text alignment, to facilitate audio-language representation learning.
The visual modality works as a pivot to connect audio and text since video-audio co-occurrences are abundant from massive video data.
However, mismatching audio and visual modalities are commonly observed when detecting objects and events via sound and image.
For example, visible objects in videos do not necessarily make sounds while sounds may be produced by objects off the screen.
Such a mismatch leads to noise in audio-visual and audio-text alignment based on visual pivoting, indicated by the limited improvement achieved by these studies~\cite{zhao2022connecting,guzhov2022audioclip,wu2022wav2clip}.

\begin{figure*}[!htpb]
    \centering
    \includegraphics[width=\linewidth]{figs/BLAT.pdf}
    \caption{An overview of our proposed language-audio pre-training approach. We use a tag-guided audio captioning model to generate audio-text data. Then we conduct contrastive learning similar to CLIP (dashed lines indicate the captioning model is not involved in the pre-training) in two stages. The pre-trained model can be transferred by zero-shot inference or fine-tuning.}
    \label{fig:CLAP_framework}
\end{figure*}

To better circumvent the noise when expanding data, we propose an audio-captioning-based approach to expand audio-text data using AudioSet~\cite{gemmeke2017audio}, the most large-scale audio event dataset.
We generate captions for audio directly without the aid of the visual modality so that potential noise from modality mismatch is eliminated.
Compared with previous audio captioning works~\cite{chen2020audio,xu2021investigating}, we incorporate AudioSet tags into caption generation to improve the generated caption quality. 
AudioSet contains audio clips and corresponding audio event tags in the original dataset.
Its subset AudioCaps~\cite{kim2019audiocaps} provides captions on top of tags.
Based on the provided event tags and captions, we bootstrap a tag-guided audio captioning model on AudioCaps and use it to generate large-scale audio-text data on AudioSet.
The approach is shown in \Cref{fig:bootstrapping_data}.
The bootstrapped data contains 1.22M pairs.
To this end, we propose BLAT: \textbf{B}ootstrapping \textbf{L}anguage-\textbf{A}udio pre-training based on \textbf{T}ag-guided synthetic data, where contrastive learning is used to pre-train an audio-text bi-encoder like CLIP.

The pre-training is comprised of two phases: 1) pre-training on the large-scale synthetic data; 2) further pre-training on the real data to adapt to the real distribution.
We evaluate the performance of BLAT on a series of downstream tasks, including single-modality classification and cross-modal retrieval and generation.
Results reveal that significant achievements are achieved on all tasks by fine-tuning BLAT.
BLAT also achieves SOTA zero-shot classification performance on most datasets.


The main contribution of this paper can be summarized as follows:
\begin{itemize}
    \item We use audio captioning to curate high-quality audio-text data from audio directly, eliminating the noise from other modalities.
    \item We incorporate AudioSet tags into audio-text data generation to bootstrap large-scale synthetic data for pre-training.
    \item We validate the effect of pre-training by transferring BLAT to cross-modal and single-modality tasks, achieving significant improvements under zero-shot and fine-tuning settings.
\end{itemize}


\section{Bootstrapping Language-Audio Pre-training Data with AudioSet Tags}
Our BLAT pre-training process consists of two stages: 1) pre-training on the large-scale synthetic data; 2) further pre-training on the small-scale real data.
This section describes the tag-guided data generation and the pre-training method.
%in this section. 

\subsection{Tag-guided Data Generation}
\label{subsec:tag_data_generation}

Current parallel audio-text datasets include AudioCaps~\cite{kim2019audiocaps}, Clotho~\cite{drossos2020clotho} and MACS~\cite{martin2021diversity}.
The size of their combination ($\sim$100K pairs) is much smaller than image-text datasets (e.g., $\sim$1.64M pairs in COCO).
However, large-scale audio event data are available from AudioSet.
To leverage the audio-only data, we aim to curate audio-text data by generating captions for AudioSet audio clips.
Since AudioCaps is a subset of AudioSet, we first train a captioning model on AudioCaps.
Then we use the model to do inference on AudioSet to obtain the bootstrapped synthetic data.


Though AudioSet does not incorporate annotated captions, it provides sound event tags, which can serve as effective guidance during caption generation.
Therefore, to enhance the caption quality, we take advantage of these event tags during caption generation.
The model generates a caption conditioned on both the input audio $\mathbf{x}$ and the hint from $M$ AudioSet tags $\{{g}_m\}_{m=1}^{M}$.
We transform the tags into fixed-dimension embeddings and take the average to feed to the decoder:
\begin{align*}
    \begin{split}
        &\mathbf{e}^g = \frac{1}{M} \sum_{m=1}^{M}{\text{TE}(g_m)}\\
        &\mathbf{y} = \text{Decoder}(\text{Encoder}(\mathbf{x}), \mathbf{e}^g)
    \end{split}
\end{align*}
where TE and $\mathbf{y}$ denote the tag embedding layer and output caption respectively.

Since the audio event distribution in AudioCaps is different from that in AudioSet~\cite{kim2019audiocaps}, we exclude audio clips with tags that never appear in AudioCaps when expanding audio-text pairs.
One caption is generated for each audio clip using the AudioSet tag-guided captioning model, resulting in about 1.22M audio-text pairs.

\subsection{Pre-training}
Our pre-training method is adapted from CLIP.
As \Cref{fig:CLAP_framework} shows, the framework consists of an audio encoder and a text encoder.
The pre-training consists of two steps: 1) pre-training on the synthetic data; 2) further pre-training on the real data to adapt to the real distribution.
We take the combination of current audio-text datasets (see \Cref{subsec:tag_data_generation}) as the real data in the second stage. 
Since there is a gap between the quality of real and synthetic data, the second stage is adopted to alleviate the bias caused by synthetic data.

Similar to CLIP, the proposed contrastive learning approach learns the correspondence between the text content and the audio events in an arbitrary audio-text pair.
For an audio clip and a sentence, the audio and text encoders transform them into two embeddings $\mathbf{a}$ and $\mathbf{t}$.
We use PANNs CNN14~\cite{kong2020panns} as the audio encoder and BERT$_\text{MEDIUM}$~\cite{turc2019well} as the text encoder.
A multi-modal embedding space is learned by maximizing the similarity between $\mathbf{a}$ and $\mathbf{t}$ of matched audio-text pairs and minimizing that of mismatched pairs.
The training objective is to minimize the InfoNCE loss~\cite{van2018representation}.

\section{Experimental Setup}
In this section, we present our experimental setup for expanding audio-text data, pre-training, fine-tuning and downstream evaluation.

\subsection{Synthetic Audio-Text Data Generation}
\label{subsec:syn_data_generation_setup}

The AudioSet tag-guided captioning model takes the feature extracted by PANNs CNN14 as the input.
The encoder is a 3-layer bidirectional GRU and the decoder is a 2-layer Transformer.
Details can be referred to \cite{xu2022sjtu}.
The model is trained on AudioCaps for 25 epochs with a batch size of 64.
The learning rate warms up to $5\times10^{-4}$ and then exponentially decays to $5\times10^{-7}$ until the end.
We use beam search with a size of 3 when expanding audio-text pairs.

\subsection{Pre-training}

The model is optimized using the Adam optimizer with a learning rate of $1\times10^{-4}$ for 200K and 15K iterations in the two pre-training steps.
The learning rate is decayed by a cosine scheduler~\cite{loshchilov2016sgdr}.

\subsection{Downstream Evaluation}

\begin{table}[ht]
    \centering
    \begin{tabular}{cccc}
    \toprule
    Task & Dataset & \# Audio clips & Metric \\
    \midrule
    Audio-text & AudioCaps & 50K & \multirow{2}{*}{R@K} \\
    Retrieval & Clotho & 6K & \\
    \hline
    \multirow{2}{*}{\shortstack{Audio\\Captioning}} & AudioCaps & 50K & COCO \&  \\
     & Clotho & 6K & FENSE \\
    \hline
    \multirow{3}{*}{Classification} & ESC50 & 2K & \multirow{3}{*}{Accuracy}\\
     & US8K & 8K & \\
     & VGGSound & 192K & \\
    \hline
    \multirow{2}{*}{Tagging} & FSD50K & 50K & \multirow{2}{*}{mAP}\\
     & AudioSet & 1.93M & \\
    \bottomrule
    \end{tabular}
    \caption{A summary of downstream cross-modal and single-modality tasks. US8K is the abbreviation of UrbanSound8K.}
    \label{tab:task_summary}
\end{table}

BLAT is evaluated on a series of downstream tasks, which is summarized in \Cref{tab:task_summary}, including single-modality and cross-modal tasks.
\textbf{Single-modality tasks} include single-label \textit{\textbf{classification}} and multi-label \textit{\textbf{tagging}}, where accuracy and mean average precision (mAP) are used as metrics.
We add an additional fully connected (FC) classifier on top of the audio encoder of BLAT for classification.
\textbf{Cross-modal tasks} include \textit{\textbf{audio-text retrieval}} and \textit{\textbf{audio captioning}}.
For audio-text retrieval, we use recall at K (R@K) for evaluation.
Since BLAT adopts a bi-encoder structure, it can be taken for retrieval without modification.
For audio captioning, we use COCO metrics for evaluation following standard setup~\footnote{http://dcase.community/challenge2022/task-automatic-audio-captioning}.
Besides, we also report FENSE~\cite{zhou2021can}, which shows a higher correlation with human judgments.
To transfer BLAT to audio captioning, we take BLAT audio encoder as the feature extractor.
The captioning model adopts a similar architecture as that mentioned in \Cref{subsec:syn_data_generation_setup} with two modifications: 1) tags are not fed; 2) it takes BLAT features as the input.

\begin{table*}[!htpb]

    \centering
    \small
    \begin{tabular}{c|c||cccc||cccc}
    \toprule
    \multirow{3}{*}{\shortstack{Training \\On Target}} &
    \multirow{3}{*}{Configuration} & \multicolumn{4}{c||}{AudioCaps} & \multicolumn{4}{c}{Clotho} \\
    \cline{3-10}
    & & \multicolumn{2}{c}{Audio $\Rightarrow$ Text} & \multicolumn{2}{c||}{Text $\Rightarrow$ Audio} & \multicolumn{2}{c}{Audio $\Rightarrow$ Text} & \multicolumn{2}{c}{Text $\Rightarrow$ Audio} \\
    & & R@1 & R@10 & R@1 & R@10 & R@1 & R@10 & R@1 & R@10 \\
    \midrule
    \multirow{2}{*}{\xmark} & BLAT & 32.6 & 76.7 & 23.5 & 68.4 & 7.6 & 31.5 & 5.6 & 23.8 \\
    & VIP$\sim$A$_\text{N}$T~\cite{zhao2022connecting} & 15.2 & 52.9 & 9.9 & 45.6 & 7.1 & 30.7 & 6.7 & 29.1 \\
    \midrule
    \multirow{2}{*}{\cmark} & from scratch & $40.4_{\pm 1.5}$ & $85.7_{\pm 0.7}$ & $33.3_{\pm 0.3}$ & $82.4_{\pm 0.3}$ & $13.9_{\pm 0.2}$ & $48.2_{\pm 1.4}$ & $12.3_{\pm 0.6}$ & $46.1_{\pm 0.9}$\\
    & BLAT fine-tuning & $47.5_{\pm 0.4}$ & $87.6_{\pm 0.2}$ & $38.2_{\pm 0.1}$ & $85.1_{\pm 0.1}$ & $17.9_{\pm 0.4}$ & $50.9_{\pm 1.9}$ & $13.7_{\pm 0.4}$ & $48.9_{\pm 0.5}$\\
    \bottomrule
    \end{tabular}
    \caption{Audio-text retrieval performance. The upper half denotes pre-training on different synthetic data and evaluating the pre-trained model without fine-tuning on the target dataset. The lower half shows the performance of training our model on the target dataset. R@K denotes recall at K.}
    \label{tab:pre_train_effects}
\end{table*}

For tasks other than captioning, three types of downstream transfer are conducted: zero-shot inference, linear probing, and fine-tuning.
For retrieval, BLAT can be directly used for \textbf{zero-shot inference} for its bi-encoder paradigm.
For single-modality tasks, we replace the ``\_'' in textual labels with blanks.
BLAT calculates the similarity scores between a given audio and the transformed textual labels.
The scores are treated as the predicted probability of corresponding classes.
\textbf{Linear probing} is only used in single-modality tasks, where the audio encoder is taken as a feature extractor and only the final FC layer is trained.
In contrast, no parameters are frozen in \textbf{fine-tuning}.


\section{Results}

In this section, we present the comprehensive performance of BLAT.
We first evaluate the quality of bootstrapped synthetic audio-text data.
Then we reveal the influence of pre-training on downstream tasks.
In experiments where only the audio encoder is used, we take PANNs~\cite{kong2020panns} for comparison since both models share the same CNN14 architecture and use AudioSet for pre-training.
We also incorporate self-supervised audio representation COLA~\cite{saeed2021contrastive}.
For all experiments except pre-training, we report results based on three randomly seeded runs.

\subsection{Benefits of Bootstrapped Audio-Text Data}


\subsubsection{Data Quality Comparison on Captioning}

\begin{table}[ht]
    \small
    \begin{tabular}{c|cccccc}
    \toprule
    & $\text{B}_4$ & R & M & C & S & F\\
    \midrule
    Synthetic w/o tag & 24.1 & 47.0 & 23.1 & 71.2 & 19.2 & 60.1\\
    Synthetic & 26.4 & 49.0 & 24.5 & 80.4 & 21.0 & 62.5\\
    Human & 29.0 & 49.5 & 28.8 & 90.8 & 28.8 & 68.0\\
    \bottomrule
    \end{tabular}
    \caption{The comparison of synthetic parallel audio-text data and real data in terms of audio captioning performance. Metrics include $\text{BLEU}_4$ ($\text{B}_4$), $\text{ROUGE}_\text{L}$ (R), METEOR (M), CIDEr (C), SPICE (S) and FENSE (F).}
    \label{tab:syn_data_quality}
\end{table}
The quality of bootstrapped data is first evaluated in terms of captioning performance.
We compare the performance of synthetic and human-annotated captions on AudioCaps in \Cref{tab:syn_data_quality}.
We also include the captioning system without AudioSet tag guidance to show the effect of importing audio event tags.
Metrics reveal that the tag guidance brings significant improvement.
For $\text{ROUGE}_\text{L}$, the synthetic data performance is surprisingly comparable with human annotation.
In terms of metrics evaluating the semantic similarity like SPICE and FENSE, human annotation is still much better.
The model is capable of generating high-quality captions with the tag guidance though there is still a quality gap between the synthetic and real data.

\subsubsection{Data Quality Comparison on Retrieval}
We also present the quality of bootstrapped data in terms of zero-shot audio-text retrieval performance, shown in the upper half of \Cref{tab:pre_train_effects}.
We compare our data with VIP$\sim$A$_\text{N}$T~\cite{zhao2022connecting}, where captions are retrieved from the audio captioning corpus using CLIP and the prompt ``the sound of''.
The two synthetic datasets share a similar size (1.22M and 1.08M).
Results indicate that the model trained on our data significantly outperforms AC except for text-to-audio retrieval on Clotho.
Note that Clotho captions are also used to curate audio-text data in AC while in our work only AudioCaps is used.
The distribution difference between the captions of AudioCaps and Clotho~\cite{martin2021diversity} leads to our model's unsatisfactory performance on Clotho.

\subsection{Cross-modal Audio-and-Language Tasks}

\begin{table}[ht]
    \small
    \centering
    \begin{tabular}{c|c||cccc}
    \toprule
    Dataset & Audio Feature & SD & F\\
    \midrule
    \multirow{3}{*}{AudioCaps} & COLA & $20.4_{\pm 0.4}$ & $38.6_{\pm 0.3}$
    \\
    & PANNs & $45.2_{\pm 0.5}$ & $60.6_{\pm 0.4}$ \\
     & BLAT & $\mathbf{45.9_{\pm 0.2}}$ & $\mathbf{61.5_{\pm 0.3}}$ \\
    \midrule
    \multirow{3}{*}{Clotho} & COLA & $13.1_{\pm 1.0}$ & $29.9_{\pm 0.7}$\\
    & PANNs & $25.7_{\pm 0.5}$ & $43.8_{\pm 0.4}$\\
     & BLAT & $\mathbf{27.1_{\pm 0.4}}$ & $\mathbf{45.8_{\pm 0.6}}$\\
    \bottomrule
    \end{tabular}
     \caption{A comparison of audio captioning performance using different audio features.}
    \label{tab:caption_finetune}
\end{table}

The lower half of \Cref{tab:pre_train_effects} shows the performance of transferring BLAT to audio-text retrieval.
We compare the model fine-tuning from BLAT with one trained from scratch.
With the initialization from BLAT, significant improvement can be witnessed on both AudioCaps and Clotho.

\begin{table*}[ht]
    \centering
    \begin{tabular}{cc|c||ccccc}
    \toprule
     & Model & \# params / M & ESC50 & US8K & VGGSound (mAP) & FSD50K & AudioSet\\
    \midrule
     & SOTA & - & 97.2~\cite{guzhov2022audioclip} & 90.1~\cite{guzhov2022audioclip} & 52.5~\cite{kazakos2021slow} & 65.3~\cite{koutini2021efficient} & 47.1~\cite{koutini2021efficient}\\
    \midrule
    \multirow{5}{*}{Zero-shot} & AudioCLIP & 95.9 & 69.4 & 68.8 & - & - & -\\
     & Wav2CLIP & \textbf{74.8} & 41.4 & 40.4 & - (10.0) & 3.0 & - \\
     & VIP$\sim$A$_\text{N}$T & 151.3 & 69.2 & 71.7 & - & - & \textbf{13.3}\\
     & CLAP & 192.1 & \textbf{82.6} & 73.2 & - & 30.2 & 5.8 \\
     & BLAT & 123.7 & 80.6 & \textbf{77.3} & 14.9 (\textbf{13.5}) & \textbf{31.3} & 10.5\\
    \midrule
    \multirow{3}{*}{Linear probing} & COLA & 79.7 & $38.2_{\pm 0.3}$ & $53.8_{\pm 0.3}$ & $13.9_{\pm 0.1}$ & $10.7_{\pm 0.0}$ & $2.1_{\pm 0.1}$ \\
    & PANNs & 79.7& $89.9_{\pm 0.1}$ & $82.6_{\pm 0.3}$ & $41.4_{\pm 0.8}$ & $29.7_{\pm 0.2}$ & - \\
     & BLAT & 79.7& $\mathbf{94.8_{\pm 0.3}}$ & $\mathbf{85.7_{\pm 0.3}}$ & $\mathbf{42.9_{\pm 0.5}}$ & $\mathbf{32.4_{\pm 0.7}}$ & $\mathbf{38.7_{\pm 0.0}}$\\
    \midrule
    \multirow{3}{*}{Fine-tuning} & COLA & 79.7& $78.8_{\pm 0.7}$ & $75.3_{\pm 0.4}$ & $48.7_{\pm 0.8}$ & $47.9_{\pm 0.9}$ & $43.6_{\pm 0.2}$ \\
    & PANNs & 79.7& $95.4_{\pm 0.1}$ & $87.4_{\pm 0.2}$ & $\mathbf{55.3_{\pm 0.8}}$ & $57.6_{\pm 0.2}$ & - \\
     & BLAT & 79.7 & $\mathbf{95.8_{\pm 0.2}}$ & $\mathbf{89.0_{\pm 0.1}}$ & $54.8_{\pm 0.1}$ & $\mathbf{60.3_{\pm 0.5}}$ & $\mathbf{44.0_{\pm 0.2}}$\\
    \bottomrule
    \end{tabular}
    \caption{Audio classification and tagging performance in different settings: 1) zero-shot transfer 2) linear probing 3) fine-tuning. On VGGSound, we list mAP in parentheses to compare with Wav2CLIP.
    We only include parameters necessary for zero-shot inference when counting parameter numbers (i.e., the visual encoding part is excluded for AudioCLIP and VIP$\sim$A$_\text{N}$T).}
    \label{tab:classify_performance}
\end{table*}

The performance of BLAT transferred to audio captioning is shown in \Cref{tab:caption_finetune}.
We present SPIDEr (SD) and FENSE (F) for convenience.
Without the supervision of event labels or textual descriptions, self-supervised COLA performs much worse than PANNs and BLAT.
BLAT feature outperforms PANNs, indicating that BLAT feature is more representative and helps the model generate more relevant descriptions.


\subsection{Single-modality Audio Classification}

\subsubsection{Zero-shot Transfer}
Under the zero-shot setting, the transferring ability of BLAT is evaluated.
Previous works enabling zero-shot inference, including AudioCLIP~\cite{guzhov2022audioclip}, Wav2CLIP~\cite{wu2022wav2clip}, VIP$\sim$A$_\text{N}$T~\cite{zhao2022connecting}, and CLAP~\cite{elizalde2022clap} are incorporated for comparison.
Except for CLAP, CLIP is utilized for synthetic data generation or pre-training.
We also include current SOTA results as a topline for reference.
Results are shown in the upper half of \Cref{tab:classify_performance}.
The parameter numbers of these models are listed.
Compared with works relying on CLIP, BLAT achieves SOTA zero-shot performance with a moderate model size, validating the benefit of eliminating noise from the visual modality.
On VGGSound, BLAT outperforms Wav2CLIP even though the latter is pre-trained on VGGSound, indicating the transferring ability of BLAT.
However, BLAT achieves a low mAP on AudioSet.
Apart from the data distribution bias caused by the creation of AudioCaps, we observe that the noise in AudioSet labels exacerbates the problem.
Previous works reveal that AudioSet annotations often contain only part of all events presented in a clip~\cite{gong2021psla}.
We assume such annotation noise makes the results not reliable.
On FSD50K where annotations are more reliable, BLAT achieves a much higher mAP.
Using a similar audio-text pre-training paradigm on real datasets, CLAP achieves similar results on ESC50 and FSD50K while BLAT outperforms CLAP on US8K and AudioSet with fewer parameters.
This validates the benefit of incorporating our bootstrapped data into pre-training.

\subsubsection{Linear probing and Fine-tuning}
The lower half of \Cref{tab:classify_performance} shows the results of transferring BLAT to audio classification by linear probing and fine-tuning.
Since PANNs are trained on AudioSet with event labels, we do not further fine-tune PANNs on AudioSet.
Like audio captioning, COLA performs much worse than PANNs and BLAT, especially under the linear probing setting.
Although COLA can be applied to any audio data, its representation does not generalize well to audio classification tasks without the supervision of labels.
In both linear probing and fine-tuning settings, BLAT outperforms PANNs on most datasets\footnote{Except VGGSound, results are significant at a level of 0.05.}.
With only one FC classifier, the performance of linear probing BLAT on ESC50 and US8K is even close to current SOTA results, indicating that BLAT serves as a powerful feature extractor.
Especially for small datasets, BLAT is able to extract highly discriminative features for classification.
By fine-tuning BLAT, we achieve results close to SOTA, which validates its transferring ability to other tasks.
The supervision of natural language exhibits a better ability to be transferred to a variety of audio classification tasks than event labels.
Note that we do not adopt training techniques like data augmentation or task-specific loss functions.


\section{Conclusion}

In this work, we propose an AudioSet tag-guided audio captioning model to bootstrap large-scale audio-text data.
Different from previous methods, the data generation approach does not incorporate video to eliminate the noise induced by the visual modality. 
Based on the bootstrapped data, we pre-train an audio-text bi-encoder using contrastive learning.
After pre-training the model on the synthetic data and the real data successively, we obtain BLAT which can be transferred to a series of downstream tasks.
Experimental results on both cross-modal and single-modality tasks, including retrieval, generation and classification, validate the effectiveness of BLAT.
Under the stringent zero-shot condition where no training data is available, BLAT exhibits SOTA performance on most datasets.


\bibliographystyle{IEEEtran}
\bibliography{icme2022template}

\end{document}
