\section{Conclusion}  \label{sec:conc}

\subsection{Summary}

The question of what makes a data distribution suitable for deep learning is a fundamental open problem.
Focusing on locally connected neural networks~---~a prevalent family of deep learning architectures that includes as special cases convolutional neural networks, recurrent neural networks (in particular the recent S4 models) and local self-attention models~---~we address this problem by adopting theoretical tools from quantum physics.
Our main theoretical result states that a certain locally connected neural network is capable of accurate prediction (\ie~can express a solution with low population loss) over a data distribution \emph{if and only if} the data distribution admits low quantum entanglement under certain canonical partitions of features.
Experiments with widespread locally connected neural networks corroborate this finding.

Our theory suggests that the suitability of a data distribution to locally connected neural networks may be enhanced by arranging features such that low entanglement under canonical partitions is attained.
Employing a certain surrogate for entanglement, we show that this arrangement can be implemented efficiently, and that it leads to substantial improvements in the prediction accuracies of common locally connected neural networks on various datasets.

\subsection{Limitations and Future Work}

\paragraph*{Neural network architecture}
We theoretically analyzed a locally connected neural network with polynomial non-linearity, by employing its equivalence to a tensor network (\cf~\cref{sec:fit_tensor:tn_lc_nn}).
Accounting for neural networks with connectivities beyond those considered (\eg~connectivities that are non-local or ones involving skip connections) is an interesting topic for future work. 
It requires modification of the equivalent tensor network and corresponding modification of the definition of canonical partitions, similarly to the analysis in~\cref{app:extension_dims}.
Another valuable direction is to account for neural networks with other non-linearities, \eg~ReLU.
We believe this may be achieved through a generalized notion of tensor networks, successfully used in past work to analyze such architectures~\cite{cohen2016convolutional}.

\vspace{-1.5mm}

\paragraph*{Objective function}
The analysis in~\cref{sec:accurate_predict} assumes a binary soft-margin SVM objective.
Extending it to other objective functions, \eg~multi-class SVM, may shed light on the relation between the objective and the requirements for a data distribution to be suitable to neural networks.

\vspace{-1.5mm}

\paragraph*{Textual data}
Our experiments (in \cref{sec:accurate_predict:emp_demo,app:extension_dims:accurate_predict:emp_demo}) show that the necessary and sufficient condition we derived for a data distribution to be suitable to a locally connected neural network~---~namely, low quantum entanglement under canonical partitions of features~---~is upheld by audio and image datasets.
This falls in line with the excellent performance of locally connected neural networks over these data modalities.
In contrast, high performant architectures for textual data are typically non-local~\cite{vaswani2017attention}.
Investigating the quantum entanglements that textual data admits, and, in particular, under which partitions they are low, may allow designing more efficient architectures with connectivity tailored to textual data.

\subsection{Outlook}

The data modalities to which deep learning is most commonly applied~---~namely ones involving images, text and audio~---~are often regarded as natural (as opposed to, for example, tabular data fusing heterogeneous information).
We believe the difficulty in explaining the suitability of such modalities to deep learning may be due to a shortage in tools for formally reasoning about natural data.
Concepts and tools from physics~---~a branch of science concerned with formally reasoning about natural phenomena~---~may be key to overcoming said difficulty.
We hope that our use of quantum entanglement will encourage further research along this line.