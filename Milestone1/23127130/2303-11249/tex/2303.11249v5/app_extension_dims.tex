\section{Extension to Arbitrary Dimensional Models and Data}  \label{app:extension_dims}

In this appendix, we extend our theoretical analysis and experiments, including the algorithm for enhancing the suitability of data to locally connected neural networks, from one-dimensional (sequential) models and data to $P$-dimensional models and data (such as two-dimensional image data or three-dimensional video data), for $P \in \N$.
Specifically,~\cref{app:extension_dims:fit_tensor} extends~\cref{sec:fit_tensor},~\cref{app:extension_dims:accurate_predict} extends~\cref{sec:accurate_predict} and~\cref{app:extension_dims:enhancing} extends~\cref{sec:enhancing}.

To ease presentation, we consider $P$-dimensional data instances whose feature vectors are associated with coordinates $(n_1, \ldots, n_P) \in [N]^P$, where $N = 2^L$ for some $L \in \N$ (if this is not the case we may add constant features as needed).


\subsection{Low Entanglement Under Canonical Partitions Is Necessary and Sufficient for Fitting Tensor} \label{app:extension_dims:fit_tensor}

We introduce the locally connected tensor network equivalent to a locally connected neural network that operates over $P$-dimensional data (\cref{app:extension_dims:fit_tensor:tn_lc_nn}).
Subsequently, we establish a necessary and sufficient condition required for it to fit a given tensor (\cref{app:extension_dims:fit_tensor:nec_and_suf}), generalizing the results of~\cref{sec:fit_tensor:nec_and_suf}.


\subsubsection{Tensor Network Equivalent to a Locally Connected Neural Network} \label{app:extension_dims:fit_tensor:tn_lc_nn}

\begin{figure*}[t!]
	\vspace{0mm}
	\begin{center}
		\includegraphics[width=1\textwidth]{figs/tn_as_nn_with_wtn_copy_pdim.pdf}
	\end{center}
	\vspace{-2mm}
	\caption{
		The analyzed tensor network equivalent to a locally connected neural network operating over $P$-dimensional data, for $P = 2$.
		\textbf{(a)} The tensor network adheres to a perfect $2^P$-ary tree connectivity with $N^P$ leaf nodes, where $N = 2^L$ for some $L \in \N$, and generates $\tntensorpdim{P} \in \R^{D_1 \times \cdots \times D_{N^P}}$.
		Axes corresponding to open edges are indexed such that open edges descendant to any node of the tree have contiguous indices.
		The lengths of axes corresponding to inner (non-open) edges are equal to $R \in \N$, referred to as the width of the tensor network.
		\textbf{(b)} Exemplar $\axismap : [N]^P \to [N^P]$ compatible with the locally connected tensor network (\cref{def:compat_map}), mapping $P$-dimensional coordinates to axes indices of $\tntensorpdim{P}$.
		\textbf{(c)} Contracting $\tntensorpdim{P}$ with vectors $\brk[c]{ \xbf^{(n_1, \ldots, n_P)} }_{n_1, \ldots, n_P \in [N]}$ according to a compatible $\axismap$ produces $\inprodnoflex{ \tenp_{n = 1}^{N^P} \xbf^{\axismap^{-1} (n)} }{ \tntensorpdim{P} }$.
		Performing these contractions can be viewed as a forward pass of a certain locally connected neural network (with polynomial non-linearity) over the data instance $\brk[c]{ \xbf^{(n_1, \ldots, n_P)} }_{n_1, \ldots, n_P \in [N]}$ (see,~\eg,~\cite{cohen2016expressive,cohen2017inductive,levine2018deep,razin2022implicit}).
	}
	\label{fig:tn_as_nn_pdim}
\end{figure*}

For $P$-dimensional data, the locally connected tensor network we consider (defined in~\cref{sec:fit_tensor:tn_lc_nn} for one-dimensional data) has an underlying perfect $2^P$-ary tree graph of height $L$.
We denote the tensor it generates by $\tntensorpdim{P} \in \R^{D_1 \times \cdots \times D_{N^P}}$.
\cref{fig:tn_as_nn_pdim}(a) provides its diagrammatic definition.
As in the one-dimensional case, the lengths of axes corresponding to inner edges are taken to be $R \in \N$, referred to as the width of the tensor network.

The axes of $\tntensorpdim{P}$ are associated with $P$-dimensional coordinates through a bijective function $\axismap : [N]^P \to [N^P]$.

\begin{definition}
\label{def:compat_map}
We say that a bijective function $\axismap : [N]^P \to [N^P]$ is \emph{compatible} with the locally connected tensor network if, for any node in the tensor network, the coordinates mapped to indices of $\tntensorpdim{P}$'s axes descendant to that node form a contiguous $P$-dimensional cubic block in $[N]^P$ (\eg, square block when $P = 2$)~---~see~\cref{fig:tn_as_nn_pdim}(b) for an illustration.
With slight abuse of notation, for $\K \subseteq [N]^P$ we denote $\axismap (\K) := \brk[c]{ \axismap (n_1, \ldots, n_P) : (n_1, \ldots, n_P) \in \K } \subseteq [N^P]$.
\end{definition}

Contracting the locally connected tensor network with $\brk[c]{ \xbf^{(n_1, \ldots, n_P)} \in \R^{D_{\axismap (n_1, \ldots, n_P)}}  }_{n_1, \ldots, n_P \in [N]}$ according to a compatible $\axismap$, as depicted in~\cref{fig:tn_as_nn_pdim}(c), can be viewed as a forward pass of the data instance $\brk[c]{ \xbf^{(n_1, \ldots, n_P)} }_{n_1, \ldots, n_P \in [N]}$ through a locally connected neural network (with polynomial non-linearity), which produces the scalar \smash{$\inprodnoflex{ \tenp_{n = 1}^{N^P} \xbf^{\axismap^{-1} (n)} }{ \tntensorpdim{P} }$} (see,~\eg,~\cite{cohen2016expressive,cohen2017inductive,levine2018deep,razin2022implicit}).


\subsubsection{Necessary and Sufficient Condition for Fitting Tensor} \label{app:extension_dims:fit_tensor:nec_and_suf}

The ability of the locally connected tensor network, defined in~\cref{app:extension_dims:fit_tensor:tn_lc_nn}, to fit (\ie~represent) a tensor is determined by the entanglements that the tensor admits under partitions of its axes, induced by the following canonical partitions of $[N]^P$.

\begin{definition}
	\label{def:canonical_partitions_pdim}
	The \emph{canonical partitions} of $[N]^P$, illustrated in~\cref{fig:canonical_partitions_pdim} for $P = 2$, are:\footnote{
		For sets $\S_1, \ldots, \S_P$, we denote their Cartesian product by $\times_{p = 1}^P \S_p$.
	}
	\[
	\begin{split}
	\can^P := \! \Big \{ \brk*{ \K, \K^c} : & \, \K = \times_{p = 1}^P \brk[c]*{ 2^{L - l} \cdot (n_p - 1) + 1, \ldots, 2^{L - l} \cdot n_p } , \\
	&~~ l \in \big \{0, \ldots, L \big \} ,~n_1, \ldots, n_P \in \big [ 2^l \big ]  \Big \}
	\text{\,.}
	\end{split}
	\]
\end{definition}

\begin{figure}[t]
	\vspace{0mm}
	\begin{center}
		\includegraphics[width=0.85\textwidth]{figs/canonical_partitions_pdim.pdf}
	\end{center}
	\vspace{-2mm}
	\caption{
		The canonical partitions of $[N]^P$, for $P = 2$ and $N = 2^L$ with $L \in \N$. 
		Every $l \in \{0, \ldots, L\}$ contributes $2^{l \cdot P}$ canonical partitions, each induced by $\K = \times_{p = 1}^P \{2^{L - l} \cdot (n_p - 1) + 1, \ldots, 2^{L - l} \cdot n_p\}$ for $n_1, \ldots, n_P \in [2^l]$.
	}
	\label{fig:canonical_partitions_pdim}
\end{figure}

With the definition of canonical partitions for $P$-dimensional data in place,~\cref{thm:fit_necessary_pdim} generalizes~\cref{thm:fit_necessary}.
In particular, suppose that $\tntensorpdim{P}$~---~the tensor generated by the locally connected tensor network~---~well-approximates $\A \in \R^{D_1 \times \cdots \times D_{N^P}}$.
Then, given a compatible $\axismap : [N]^P \to [N^P]$ (\cref{def:compat_map}),~\cref{thm:fit_necessary_pdim} establishes that the entanglement of $\A$ with respect to $(\axismap (\K), \axismap (\K)^c)$, where $(\K, \K^c) \in \can^P$, cannot be much larger than $\ln (R)$, whereas the expected entanglement attained by a random tensor with respect to $(\axismap (\K), \axismap (\K)^c)$ is on the order of $\min \{ \abs{ \K }, \abs{ \K^c } \}$ (which is linear in~$N^P$ for some canonical partitions).

In the other direction,~\cref{thm:fit_sufficient_pdim} implies that low entanglement under partitions of axes induced by canonical partitions of $[N]^P$ is not only necessary for a tensor to be fit by the locally connected tensor network, but also sufficient.

\begin{theorem}
	\label{thm:fit_necessary_pdim}
	Let $\tntensorpdim{P} \in \R^{D_1 \times \cdots \times D_{N^P}}$ be a tensor generated by the locally connected tensor network defined in~\cref{app:extension_dims:fit_tensor:tn_lc_nn}, and $\axismap: [N]^P \to [N^P]$ be a compatible map from $P$-dimensional coordinates to axes indices of $\tntensorpdim{P}$ (\cref{def:compat_map}).
	For any $\A \in \R^{D_1 \times \cdots \times D_{N^P}}$ and $\epsilon \in [0, \norm{\A} / 4]$, if $\norm{\tntensorpdim{P} -  \A} \leq \epsilon$, then for all canonical partitions $\brk{\K, \K^c} \in \can^P$ (\cref{def:canonical_partitions_pdim}):
	\be
	\qe{\A}{ \axismap (\K) } \leq \ln (R)+\frac{2 \epsilon}{ \norm{\A} } \cdot \ln ( D_{\axismap (\K) } ) + 2 \sqrt{\frac{ 2 \epsilon }{ \norm{\A} } }
	\text{\,,}
	\label{eq:fit_necessary_ub_pdim}
	\ee
	where $D_{\axismap(\K)} := \min \brk[c]{ \prod_{n \in \axismap(\K)} D_n , \prod_{n \in \axismap (\K)^c} D_n }$.
	In contrast, a random $\A' \in \R^{D_1 \times \cdots \times D_{N^P}}$, drawn according to the uniform distribution over the set of unit norm tensors, satisfies for all canonical partitions $(\K, \K^c) \in \can^P$:
	\be
		\EE  \brk[s]*{ \qe{\A'}{ \axismap (\K) } } \geq \min \brk[c]*{ \abs{\K} , \abs{\K^c} } \cdot  \ln \brk*{ \min\nolimits_{n \in [N^P]} D_n } + \ln \brk*{ \frac{1}{2} } - \frac{1}{2}
		\text{\,.}
		\label{eq:fit_necessary_lb_pdim}
	\ee
\end{theorem}

\begin{proof}[Proof sketch (proof in~\cref{app:proofs:fit_necssary_pdim})]
The proof is analogous to that of~\cref{thm:fit_necessary}.
\end{proof}



\begin{theorem}
	\label{thm:fit_sufficient_pdim}
	Let $\A \in \R^{D_1 \times \cdots \times D_{N^P}}$ and $\epsilon > 0$.
	Suppose that for all canonical partitions $(\K, \K^c) \in \can^P$ (\cref{def:canonical_partitions_pdim}) it holds that $\qe{\A}{\axismap (\K)} \leq \frac{\epsilon^{2}}{(2 N^P - 3) \norm{\A}^2} \cdot \ln (R)$, where $\mu : [N]^P \to [N^P]$ is compatible with the locally connected tensor network (\cref{def:compat_map}).
	Then, there exists an assignment for the tensors constituting the locally connected tensor network (defined in~\cref{app:extension_dims:fit_tensor:tn_lc_nn}) such that it generates $\tntensorpdim{P} \in \R^{D_1 \times \cdots \times D_{N^P}}$ satisfying:
	\[
	\norm{ \tntensorpdim{P} - \A } \leq \epsilon
	\text{\,.}
	\]
\end{theorem}



\begin{proof}[Proof sketch (proof in~\cref{app:proofs:fit_sufficient_pdim})]
The claim follows through a reduction from the locally connected tensor network for $P$-dimensional data to that for one-dimensional data (defined in~\cref{sec:fit_tensor:tn_lc_nn}), \ie~from perfect $2^P$-ary to perfect binary tree tensor networks.
Specifically, we consider a modified locally connected tensor network for one-dimensional data, where axes corresponding to different inner edges can vary in length (as opposed to all having length $R$).
We then show, by arguments analogous to those in the proof of~\cref{thm:fit_sufficient}, that it can approximate $\A$ while having certain inner axes, related to the canonical partitions of $[N]^P$, of lengths at most $R$.
The proof concludes by establishing that, any tensor represented by such a locally connected tensor network for one-dimensional data can be represented via the locally connected tensor network for $P$-dimensional data (where the length of each axis corresponding to an inner edge is $R$).
\end{proof}


\subsection{Low Entanglement Under Canonical Partitions Is Necessary and Sufficient for Accurate Prediction}  \label{app:extension_dims:accurate_predict}

In this appendix, we consider the locally connected tensor network from~\cref{app:extension_dims:fit_tensor:tn_lc_nn} in a machine learning setting, and extend the results and experiments of~\cref{sec:accurate_predict} from one-dimensional to $P$-dimensional models and data.


\subsubsection{Accurate Prediction Is Equivalent to Fitting Data Tensor} \label{app:extension_dims:accurate_predict:fit_data_tensor}

The locally connected tensor network generating $\tntensorpdim{P} \in \R^{D_1 \times \cdots \times D_{N^P}}$ is equivalent to a locally connected neural network operating over $P$-dimensional data (see~\cref{app:extension_dims:fit_tensor:tn_lc_nn}).
Specifically, a forward pass of the latter over $\brk[c]{ \xbf^{(n_1, \ldots, n_P)} \in \R^{D_{\axismap (n_1, \ldots, n_P)} } }_{n_1, \ldots, n_P \in [N]}$ yields \smash{$\inprodnoflex{ \tenp_{n = 1}^{N^P} \xbf^{\axismap^{-1} (n)} }{ \tntensorpdim{P} }$}, for a compatible $\axismap : [N]^P \to [N^P]$ (\cref{def:compat_map}).
Suppose we are given a training set of labeled instances $\brk[c]!{ \brk[c]{ \xbf^{( (n_1, \ldots, n_P), m)} }_{n_1, \ldots, n_P \in [N]}, y^{(m)} }_{m = 1}^M$ drawn i.i.d. from some distribution, where $y^{(m)} \in \{1, -1\}$ for $m \in [M]$.
Learning the parameters of the neural network through the soft-margin support vector machine (SVM) objective amounts to optimizing:
\[
\min\nolimits_{\substack{\norm{\tntensorpdim{P}} \leq B}} \frac{1}{M} \sum\nolimits_{m = 1}^M  \max \brk[c]*{0, 1 - y^{(m)} \inprodbig{ \tenp_{n = 1}^{N^P} \xbf^{(\axismap^{-1} (n) ,m)} }{\tntensorpdim{P}} }
\text{\,,}
\]
for a predetermined constant $B > 0$.
This objective generalizes~\cref{eq:soft_svm} from one-dimensional to $P$-dimensional model and data.
Assume that instances are normalized, \ie~$\norm{ \xbf^{((n_1, \ldots, n_P), m)} } \leq 1$ for all $n_1, \ldots, n_P \in [N], m \in [M]$, and that $B \leq 1$.
By a derivation analogous to that succeeding~\cref{eq:soft_svm} in~\cref{sec:accurate_predict:fit_data_tensor}, if follows that minimizing the SVM objective is equivalent to minimizing $\normbig{ \frac{\tntensorpdim{P}}{ \norm{ \tntensorpdim{P} } } - \frac{ \datatensorpdim{P} }{ \norm{\datatensorpdim{P}} }}$, where:
\be
\datatensorpdim{P} := \frac{1}{M} \sum\nolimits_{m = 1}^M y^{(m)} \cdot \tenp_{n = 1}^{N^P} \xbf^{(\axismap^{-1} (n), m)}
\label{eq:data_tensor_pdim}
\ee
extends the notion of \emph{empirical data tensor} to $P$-dimensional data.
In other words, the accuracy achievable over the training data is determined by the extent to which $\frac{ \tntensorpdim{P} }{ \norm{\tntensorpdim{P}} }$ can fit the normalized empirical data tensor~\smash{$\frac{ \datatensorpdim{P} }{ \norm{\datatensorpdim{P}} }$}.

The same arguments apply to the population loss, in which case $\datatensorpdim{P}$ is replaced by the \emph{population data tensor}:
\be
\popdatatensorpdim{P} :=  \EE\nolimits_{ \brk[c]{ \xbf^{(n_1, \ldots, n_P)} }_{n_1, \ldots, n_P \in [N]} , y} \brk[s]*{ y \cdot \tenp_{n = 1}^{N^P} \xbf^{\axismap^{-1} (n)} }
\text{\,.}
\label{eq:pop_data_tensor_pdim}
\ee
The achievable accuracy over the population is therefore determined by the extent to which $\frac{ \tntensorpdim{P} }{ \norm{\tntensorpdim{P}} }$ can fit the normalized population data tensor $\frac{ \popdatatensorpdim{P} }{ \norm{\popdatatensorpdim{P}} }$.
Accordingly, we refer to the minimal distance from it as the \emph{supobtimality in achievable accuracy}, generalizing~\cref{def:supopt} from~\cref{sec:accurate_predict:fit_data_tensor}.
\begin{definition}
	\label{def:supopt_pdim}
	In the context of the classification setting above, the \emph{suboptimality in achievable accuracy} is:
	\[
	\suboptpdim{P} := \min\nolimits_{\tntensorpdim{P}} \norm*{ \frac{\tntensorpdim{P}}{ \norm{\tntensorpdim{P}} } - \frac{ \popdatatensorpdim{P} }{ \norm{\popdatatensorpdim{P}} }}
	\text{\,.}
	\]
\end{definition}


\subsubsection{Necessary and Sufficient Condition for Accurate Prediction} \label{app:extension_dims:accurate_predict:nec_and_suf}

In the classification setting of~\cref{app:extension_dims:accurate_predict:fit_data_tensor}, by invoking~\cref{thm:fit_necessary_pdim,thm:fit_sufficient_pdim} from~\cref{app:extension_dims:fit_tensor:nec_and_suf}, we conclude that the suboptimality in achievable accuracy is small if and only if the population (empirical) data tensor $\popdatatensorpdim{P}$ ($\datatensorpdim{P}$) admits low entanglement under the canonical partitions of features (\cref{def:canonical_partitions_pdim}).
Specifically, we establish~\cref{cor:acc_pred_nec_and_suf_pdim},~\cref{prop:data_tensor_concentration_pdim} and~\cref{cor:eff_acc_pred_nec_and_suf_pdim}, which generalize~\cref{cor:acc_pred_nec_and_suf},~\cref{prop:data_tensor_concentration} and~\cref{cor:eff_acc_pred_nec_and_suf} from~\cref{sec:accurate_predict:nec_and_suf}, respectively, to $P$-dimensional model and data.


\begin{corollary}
	\label{cor:acc_pred_nec_and_suf_pdim}
	Consider the classification setting of~\cref{app:extension_dims:accurate_predict:fit_data_tensor}, and let $\epsilon \in [0, 1/4]$.
	If there exists a canonical partition $\brk{ \K, \K^c} \in \can^P$ (\cref{def:canonical_partitions_pdim}) under which $\qe{\popdatatensorpdim{P}}{\axismap (\K)} > \ln (R) + 2\epsilon \cdot \ln ( D_{\axismap (\K)} )+ 2 \sqrt{2 \epsilon}$, where $D_{\axismap (\K)} := \min \brk[c]{ \prod_{n \in \axismap (\K)} D_n , \prod_{n \in \axismap (\K)^c} D_n }$, then:
	\[
	\suboptpdim{P} > \epsilon
	\text{\,.}
	\]
	Conversely, if for all $\brk{ \K, \K^c} \in \can^P$ it holds that $\qe{\popdatatensorpdim{P}}{ \axismap (\K) } \leq \frac{\epsilon^{2}}{8 N^P -12} \cdot \ln (R)$, then:
	\[
	\suboptpdim{P} \leq \epsilon
	\text{\,.}
	\]
\end{corollary}

\begin{proof}
Implied by~\cref{thm:fit_necessary_pdim,thm:fit_sufficient_pdim} after accounting for $\tntensorpdim{P}$ being normalized in the suboptimality in achievable accuracy, as done in the proof of~\cref{cor:acc_pred_nec_and_suf}.
\end{proof}

\begin{proposition}
	\label{prop:data_tensor_concentration_pdim}
	Consider the classification setting of~\cref{app:extension_dims:accurate_predict:fit_data_tensor},
	and let $\delta \in (0, 1)$ and $\gamma > 0$.
	If the training set size $M$ satisfies $M \geq \frac{2\ln(\frac{2}{\delta})}{\norm{\popdatatensorpdim{P}}^{2}\gamma^{2}}$, then with probability at least $1 - \delta$:
	\[
		\norm*{\frac{\popdatatensorpdim{P}}{\norm{\popdatatensorpdim{P}}} - \frac{\datatensorpdim{P}}{\norm{\datatensorpdim{P}}}} \leq \gamma
	\]
\end{proposition}

\begin{proof}
The claim is established by following steps identical to those in the proof of~\cref{prop:data_tensor_concentration}.
\end{proof}

\begin{corollary}
	\label{cor:eff_acc_pred_nec_and_suf_pdim}
	Consider the setting and notation of~\cref{cor:acc_pred_nec_and_suf_pdim}, with $\epsilon \in (0, \frac{1}{6}]$.
	For $\delta \in (0, 1)$, suppose that the training set size $M$ satisfies $M \geq \frac{8\ln(\frac{2}{\delta})}{\norm{\popdatatensorpdim{P}}^{2}\epsilon^{2}}$
	Then, with probability at least $1 - \delta$ the following hold.
	First, if there exists a canonical partition $\brk{ \K, \K^c} \in \can^P$ (\cref{def:canonical_partitions_pdim}) under which $\qe{\datatensorpdim{P}}{ \axismap (\K) } > \ln (R) + 3 \epsilon \cdot \ln (D_{ \axismap (\K) })  + 2 \sqrt{ 3 \epsilon }$, then:
	\[
	\suboptpdim{P} > \epsilon
	\text{\,.}
	\]
	Second, if for all $\brk{ \K, \K^c} \in \can^P$ it holds that $\qe{ \datatensorpdim{P} }{\axismap (\K)} \leq \frac{\epsilon^{2}}{32 N^P - 48} \cdot \ln (R)$, then:
	\[
	\suboptpdim{P} \leq \epsilon
	\text{\,.}
	\]
	Moreover, the conditions above on the entanglements of $\datatensorpdim{P}$ can be evaluated efficiently (in $\OO (D N^{P} M^2 + N^P M^3)$ time $\OO (D N^P M + M^2)$ and memory, where $D := \max_{n \in [N^P]} D_n$).
\end{corollary}

\begin{proof} 
	Implied from~\cref{cor:acc_pred_nec_and_suf_pdim},~\cref{prop:data_tensor_concentration_pdim} with $\gamma = \frac{\epsilon}{2}$ and~\cref{alg:ent_comp} in~\cref{app:ent_comp} by following steps identical to those in the proof of~\cref{cor:eff_acc_pred_nec_and_suf}.
\end{proof}


\subsubsection{Empirical Demonstration} \label{app:extension_dims:accurate_predict:emp_demo}


\cref{fig:entanglement_inv_corr_acc_images} extends the experiments reported by~\cref{fig:entanglement_inv_corr_acc} in~\cref{sec:accurate_predict:emp_demo} from one-dimensional (sequential) audio data to two-dimensional image data.
Specifically, it demonstrates that the prediction accuracies of convolutional neural networks over a variant of CIFAR10~\citep{krizhevsky2009learning} is inversely correlated with the entanglements that the data admits under canonical partitions of features (\cref{def:canonical_partitions_pdim}), \ie~with the entanglements of the empirical data tensor under partitions of its axes induced by canonical partitions.


\begin{figure}[t]
	\vspace{0mm}
	\begin{center}
		\hspace{0mm}
		\includegraphics[width=1\textwidth]{figs/entanglement_inv_corr_acc_image.pdf}
	\end{center}
	\vspace{-2mm}
	\caption{
		The prediction accuracies of convolutional neural networks are inversely correlated with the entanglements of image data under canonical partitions of features, in compliance with our theory (\cref{app:extension_dims:fit_tensor,app:extension_dims:accurate_predict}).
		This figure is identical to~\cref{fig:entanglement_inv_corr_acc}, except that the measurements were carried over a binary classification version of the CIFAR10 image dataset, as opposed to a one-dimensional (sequential) audio dataset, using three different convolutional neural network architectures.
		For further details see caption of~\cref{fig:entanglement_inv_corr_acc} as well as~\cref{app:experiments:details}.
	}
	\label{fig:entanglement_inv_corr_acc_images}
\end{figure}


\subsection{Enhancing Suitability of Data to Locally Connected Neural Networks} \label{app:extension_dims:enhancing}	

We extend the preprocessing algorithm from~\cref{sec:enhancing}, aimed to enhance the suitability of a data distribution to locally connected neural networks, from one-dimensional to $P$-dimensional models and data (\cref{app:extension_dims:enhancing:search,app:extension_dims:enhancing:practical}).
Empirical evaluations demonstrate that it significantly improves prediction accuracy of common architectures (\cref{app:extension_dims:enhancing:exp}).

\subsubsection{Search for Feature Arrangement With Low Entanglement Under Canonical Partitions}  \label{app:extension_dims:enhancing:search}

Suppose we have $M \in \N$ training instances $\brk[c]1{ \brk[c]{ \xbf^{((n_1, \ldots, n_P), m)} }_{n_1, \ldots, n_P \in [N]}, y^{(m)} }_{m = 1}^M$ , where $\xbf^{((n_1, \ldots, n_P), m)} \in \R^D$ and $y^{(m)} \in \{1, -1\}$ for $n_1, \ldots, n_P \in [N], m \in [M]$, with $D \in \N$.
For models intaking $P$-dimensional data, the recipe for enhancing the suitability of a data distribution to locally connected neural networks from~\cref{sec:enhancing:search} boils down to finding a permutation $\pi : [N]^P \to [N]^P$, which when applied to feature coordinates leads the empirical data tensor $\datatensorpdim{P}$ (\cref{eq:data_tensor_pdim}) to admit low entanglement under canonical partitions (\cref{def:canonical_partitions_pdim}).\footnote{
Enhancing the suitability of a data distribution with instances of dimension different than $P$ to $P$-dimensional models is possible by first arbitrarily mapping features to coordinates in $[N]^P$, and then following the scheme for rearranging $P$-dimensional data.
}

A greedy realization, analogous to that outlined in~\cref{sec:enhancing:search}, is as follows.
Initially, partition the features into $2^P$ equally sized disjoint sets $\brk[c]{ \K_{1, (k_1, \ldots, k_P)} \subset [N]^P }_{k_1, \ldots, k_P \in [2]}$ such that the average of $\datatensorpdim{P}$'s entanglements induced by these sets is minimal.
That is, find an element of:
\[
\argmin_{\substack{\brk[c]1{ \K'_{k_1, \ldots, k_P} \subset [N]^P }_{k_1, \ldots, k_P \in [2]} \\ \text{s.t. } \cupdot_{k_1, \ldots, k_P \in [2]} \K'_{k_1, \ldots, k_P} = [N]^P , \\ \forall k_1, \ldots, k_P, k'_1, \ldots, k'_P \in [2] ~\abs{\K'_{k_1, \ldots, k_P}} = \abs{\K'_{k'_1, \ldots, k'_P}}  } } \!\!\!\!\!\!\! \frac{1}{2^P} \sum\nolimits_{k_1, \ldots, k_P \in [2]} \qe{\datatensorpdim{P}}{ \axismap (\K'_{k_1, \ldots, k_P}) }
\text{\,,}
\]
where $\axismap: [N]^P \to [N^P]$ is a compatible map from coordinates in $[N]^P$ to axes indices (\cref{def:compat_map}).
The permutation $\pi$ will map each $\K_{1, (k_1, \ldots, k_P)}$ to coordinates $\times_{p = 1}^P \{ \frac{N}{2} \cdot (k_p - 1) + 1, \ldots, \frac{N}{2} \cdot k_p \}$, for $k_1, \ldots, k_P \in [2]$.
Then, partition similarly each of $\brk[c]{ \K_{1, (k_1, \ldots, k_P)} }_{k_1, \ldots, k_P \in [2]}$ into $2^P$ equally sized disjoint sets.
Continuing in the same fashion, until we reach subsets $\brk[c]{ \K_{L, (k_1, \ldots, k_P)} }_{k_1, \ldots, k_P \in [N]}$ consisting of a single feature coordinate each, fully specifies the permutation $\pi$.

As in the case of one-dimensional models and data (\cref{sec:enhancing:search}), the step lying at the heart of the above scheme~---~finding a balanced partition into $2^P$ sets that minimizes average entanglement~---~is computationally prohibitive.
In the next supappendix we will see that replacing entanglement with surrogate entanglement brings forth a practical implementation.


\subsubsection{Practical Algorithm via Surrogate for Entanglement} \label{app:extension_dims:enhancing:practical}

To efficiently implement the scheme from~\cref{app:extension_dims:enhancing:search}, we replace entanglement with surrogate entanglement (\cref{def:surrogate_entanglement}), which for $P$-dimensional data is straightforwardly defined as follows.

\begin{definition}
	\label{def:surrogate_entanglement_pdim}
	Given a set of $M \in \N$ instances $\X := \brk[c]1{ \brk[c]{ \xbf^{((n_1, \ldots, n_P), m)} \in \R^D }_{n_1, \ldots, n_P \in [N]}  }_{m = 1}^M$, denote by $\pear_{(n_1, \ldots, n_P), (n'_1, \ldots, n'_P)}$ the multivariate Pearson correlation between features $(n_1, \ldots, n_P) \in [N]^P$ and $(n'_1, \ldots, n'_P) \in [N]^P$.
	For $\K \subseteq [N]^P$, the \emph{surrogate entanglement} of $\X$ with respect to the partition $(\K, \K^c)$, denoted $\se{ \X }{ \K }$, is the sum of Pearson correlation coefficients between pairs of features, the first belonging to $\K$ and the second to $\K^c := [N]^P \setminus \K$.
	That is:
	\[
	\sebig{ \X }{ \K } = \sum\nolimits_{(n_1, \ldots, n_P) \in \K, (n'_1, \ldots, n'_P) \in \K^c} \pear_{(n_1, \ldots, n_P), (n'_1, \ldots, n'_P)}
	\text{\,.}
	\]
\end{definition}

Analogously to the case of one-dimensional data (\cref{sec:enhancing:practical}),~\cref{prop:min_balanced_cut_pdim} shows that replacing the entanglement with surrogate entanglement in the scheme from~\cref{app:extension_dims:enhancing:search} converts each search for a balanced partition minimizing average entanglement into a \emph{minimum balanced $2^P$-cut problem}.
Although the minimum balanced $2^P$-cut problem is NP-hard (see, \eg,~\cite{garey1979computers}), similarly to the minimum balanced ($2$-) cut problem, it enjoys a wide array of well-established approximation implementations, particularly ones designed for large scale~\citep{karypis1998fast,spielman2011spectral}.
We therefore obtain a practical algorithm for enhancing the suitability of a data distribution with $P$-dimensional instances to locally connected neural networks~---~see~\cref{alg:ent_struct_search_pdim}.

\begin{algorithm}[t!]
	\caption{Enhancing Suitability of $P$-Dimensional Data to Locally Connected Neural Networks} 
	\label{alg:ent_struct_search_pdim}	
	\begin{algorithmic}[1]
		\STATE \!\textbf{Input:} $\X := \brk[c]1{ \brk[c]{ \xbf^{((n_1, \ldots, n_P), m)} }_{n_1, \ldots, n_P \in [N]}  }_{m = 1}^M$~---~$M \in \N$ data instances comprising $N^P$ features each \\[0.4em]
		\STATE \!\textbf{Output:} Permutation $\perm : [N]^P \to [N]^P$ to apply to feature coordinates \\[-0.2em]
		\hrulefill
		\vspace{1mm}
		\STATE Let $\K_{0, (1, \ldots, 1)} := [N]^P$ and denote $L := \log_2 (N)$
		\vspace{1mm}
		\STATE \darkgray{\# We assume for simplicity that $N$ is a power of two, otherwise one may add constant features}
		\vspace{1mm}
		\FOR{$l = 0, \ldots, L - 1 ~,~ k_1, \ldots, k_P = 1, \ldots, 2^{l}$}
		\vspace{1mm}
		\STATE Using a reduction to a minimum balanced $2^P$-cut problem (\cref{prop:min_balanced_cut_pdim}), find an approximate solution $\{ \K_{l + 1, (2k_1 - i_1, \ldots, 2k_P -  i_P)} \subset \K_{l , (k_1, \ldots, k_P)} \}_{i_1, \ldots, i_P \in \{0, 1\}}$ for:
		\[
		\min_{\substack{\brk[c]1{ \K_{k'_1, \ldots, k'_P} \subset \K_{l, (k_1, \ldots, k_P)} }_{k'_1, \ldots, k'_P \in [2]} \\ \text{s.t. } \cupdot_{k'_1, \ldots, k'_P \in [2]} \K_{k'_1, \ldots, k'_P} = \K_{l, (k_1, \ldots, k_P)} , \\ \forall k'_1, \ldots, k'_P, \hat{k}_1, \ldots, \hat{k}_P \in [2] ~\abs{\K_{k'_1, \ldots, k'_P}} = \abs{\K_{\hat{k}_1, \ldots, \hat{k}_P}}  } } \!\!\!\!\!\!\! \frac{1}{2^P} \sum\nolimits_{k'_1, \ldots, k'_P \in [2]} \se{\X}{ \K_{k'_1, \ldots, k'_P} }
		\]
		\vspace{-2mm}
		\ENDFOR
		\vspace{1mm}
		\STATE \darkgray{\# At this point, $\{ \K_{L, (k_1, \ldots, k_P)} \}_{k_1, \ldots, k_P \in [N]}$ each contain a single feature coordinate}
		\vspace{0.5mm}
		\STATE \textbf{return} $\perm$ that maps $(n_1, \ldots, n_P) \in \K_{L, (k_1, \ldots, k_P)}$ to $(k_1, \ldots, k_P)$, for every $k_1, \ldots, k_P \in [N]$
	\end{algorithmic}
\end{algorithm}


\begin{proposition}
	\label{prop:min_balanced_cut_pdim}
	For any $\widebar{\K} \subseteq [N]^P$ of size divisible by $2^P$, the following optimization problem can be framed as a minimum balanced $2^P$-cut problem over a complete graph with $\abs{\widebar{\K}}$ vertices:
	\be
	\min_{\substack{\brk[c]1{ \K_{k_1, \ldots, k_P} \subset \widebar{\K} }_{k_1, \ldots, k_P \in [2]} \\ \text{s.t. } \cupdot_{k_1, \ldots, k_P \in [2]} \K_{k_1, \ldots, k_P} = \widebar{\K} , \\ \forall k_1, \ldots, k_P, k'_1, \ldots, k'_P \in [2] ~\abs{\K_{k_1, \ldots, k_P}} = \abs{\K_{k'_1, \ldots, k'_P}}  } } \!\!\!\!\!\!\! \frac{1}{2^P} \sum\nolimits_{k_1, \ldots, k_P \in [2]} \se{\X}{ \K_{k_1, \ldots, k_P} }
	\text{\,.}
	\label{eq:balanced_part_min_surr_entanglement_pdim}
	\ee
	Specifically, there exists a complete undirected weighted graph with vertices $\widebar{\K}$ and edge weights $w : \widebar{\K} \times \widebar{\K} \to \R$ such that, for any partition of $\widebar{\K}$ into equally sized disjoint sets $\{ \K_{k_1, \ldots, k_P} \}_{k_1, \ldots, k_P \in [2]}$, the weight of the $2^P$-cut in the graph induced by them~---~$\frac{1}{2} \sum\nolimits_{k_1, \ldots, k_P \in [2]} \sum\nolimits_{(n_1, \ldots, n_P) \in \K_{k_1, \ldots, k_P} , (n'_1, \ldots, n'_P) \in \widebar{\K} \setminus \K_{k_1, \ldots, k_P}} w( \{ (n_1, \ldots, n_P), (n'_1, \ldots, n'_P) \})$~---~is equal, up to multiplicative and additive constants, to the term minimized in~\cref{eq:balanced_part_min_surr_entanglement}.
\end{proposition}

\begin{proof}
	Consider the complete undirected graph whose vertices are $\widebar{\K}$ and where the weight of an edge $\{(n_1, \ldots, n_P), (n'_1, \ldots, n'_P)\} \in \widebar{\K} \times \widebar{\K}$ is:
	\[
	w ( \{ (n_1, \ldots, n_P), (n'_1, \ldots, n'_P) \} ) = p_{(n_1, \ldots, n_P), (n'_1, \ldots, n'_P)}
	\]
	(recall that $p_{(n_1, \ldots, n_P), (n'_1, \ldots, n'_P)}$ stands for the multivariate Pearson correlation between features $(n_1, \ldots, n_P)$ and $(n'_1, \ldots, n'_P)$ in $\X$).
	For any partition of $\widebar{\K}$ into equally sized disjoint sets $\{ \K_{k_1, \ldots, k_P} \}_{k_1, \ldots, k_P \in [2]}$, it holds that:
	\[
	\begin{split}
	& \frac{1}{2} \sum\nolimits_{k_1, \ldots, k_P \in [2]} \sum\nolimits_{(n_1, \ldots, n_P) \in \K_{k_1, \ldots, k_P} , (n'_1, \ldots, n'_P) \in \widebar{\K} \setminus \K_{k_1, \ldots, k_P}} w( \{ (n_1, \ldots, n_P), (n'_1, \ldots, n'_P) \}) \\
	& = \frac{1}{2} \sum\nolimits_{k_1, \ldots, k_P \in [2]} \se{\X}{ \K_{k_1, \ldots, k_P} } - \frac{1}{2}\sebig{ \X }{ \widebar{\K} }
	\text{\,,}
	\end{split}
	\]
	where $\frac{1}{2}\se{ \X }{ \widebar{\K} }$ does not depend on $\{ \K_{k_1, \ldots, k_P} \}_{k_1, \ldots, k_P \in [2]}$.
	This concludes the proof.
\end{proof}


\subsubsection{Experiments} \label{app:extension_dims:enhancing:exp}

We supplement the experiments from~\cref{sec:enhancing:exp} for one-dimensional models and data, by empirically evaluating~\cref{alg:ent_struct_search_pdim} using two-dimensional convolutional neural networks over randomly permuted image data.
Specifically,~\cref{tab:image_rand_perm} presents experiments analogous to those of~\cref{tab:audio_moderate_dim} from~\cref{sec:enhancing:exp:perm_audio} over the CIFAR10~\citep{krizhevsky2009learning} image dataset.
For brevity, we defer some implementation details to~\cref{app:experiments:details}.

\begin{table}[t]
	\caption{
		Arranging features of randomly permuted image data via~\cref{alg:ent_struct_search_pdim} significantly improves the prediction accuracies of convolutional neural networks.
		Reported are the results of experiments analogous to those of~\cref{tab:audio_moderate_dim}, carried out over the CIFAR10 image dataset (as opposed to an audio dataset) using~\cref{alg:ent_struct_search_pdim}.
		For further details see caption of~\cref{tab:audio_moderate_dim} as well as~\cref{app:experiments:details}.
	}
	\begin{center}
		\small
		\vspace{1mm}
		\begin{tabular}{lccc}
			\toprule
			& Randomly Permuted & \cref{alg:ent_struct_search_pdim} & IGTD \\
			\midrule
			CNN & ${35.1}~\pm$ \scriptsize{${0.5}$} & $\mathbf{38.2}~\pm$ \scriptsize{${0.4}$} & ${36.2}~\pm$ \scriptsize{${0.7}$} \\
			\bottomrule
		\end{tabular}	
	\end{center}
	\label{tab:image_rand_perm}
\end{table}
