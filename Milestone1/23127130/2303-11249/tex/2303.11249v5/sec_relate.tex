\section{Related Work}  \label{sec:relate}

Characterizing formal properties of data distributions that make them suitable for deep learning is a major open problem in the field. 
A number of papers provide sufficient conditions on a data distribution which imply that it is learnable by certain neural networks~\citep{brutzkus2017globally,yarotsky2017error,malach2018provably,du2018improved,du2018gradient,oymak2018end,wang2018exponential,pmlr-v162-brutzkus22a}. 
However, these sufficient conditions are restrictive, and are not argued to be necessary for any aspect of learning (\ie~for expressiveness, optimization or generalization).
To the best of our knowledge, this paper is the first to derive a verifiable condition on a data distribution that is both necessary and sufficient for aspects of learning to be achievable by a neural network.
We note that the condition we derive resembles past attempts to quantify the structure of data via quantum entanglement and mutual information~\citep{martyn2020entanglement,convy2022mutual,lu2021tensor,cheng2018information,zhang2017entanglement,jia2020entanglement,anagiannis2021entangled,dymarsky2022tensor}.
However, such quantifications have not been formally related to learnability by neural networks.

The current paper follows a long line of research employing tensor networks as theoretical models for studying deep learning.
This line includes works analyzing the expressiveness of different neural network architectures~\citep{cohen2016expressive,sharir2016tensorial,cohen2016convolutional,cohen2017inductive,sharir2018expressive,cohen2018boosting,levine2018benefits,balda2018tensor,levine2018deep,khrulkov2018expressive,levine2019quantum,khrulkov2019generalized,razin2022ability}, their generalization~\citep{li2020understanding}, and the implicit regularization induced by their optimization~\citep{razin2020implicit,razin2021implicit,razin2022implicit,wang2020beyond,ge2021understanding}. 
Similarly to prior works we focus on expressiveness, yet our approach differs in that we incorporate the data distribution into the analysis and tackle the question of what makes data suitable for deep learning.

The algorithm we propose for enhancing the suitability of data to locally connected neural networks can be considered a form of representation learning. 
Representation learning is a vast field, far too broad to survey here (for an overview see \citep{nozawa2022empirical}). 
Our algorithm, which learns a representation via rearrangement of features in the data, is complementary to most representation learning methods in the literature.
A notable method that is also based on feature rearrangement is IGTD~\citep{zhu2021converting}~---~a heuristic scheme designed for convolutional neural networks.
In contrast to IGTD, our algorithm is theoretically grounded.
Moreover, we demonstrate empirically in~\cref{sec:enhancing} that it leads to higher prediction accuracies.