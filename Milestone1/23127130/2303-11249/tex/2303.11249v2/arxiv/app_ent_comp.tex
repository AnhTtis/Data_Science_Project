\section{Efficiently Computing Entanglements of the Empirical Data Tensor}  \label{app:ent_comp}



\begin{algorithm}[t!]
	\caption{Entanglement Computation for the Empirical Data Tensor} 
	\label{alg:ent_comp}	
	\begin{algorithmic}[1]
		\STATE \!\textbf{Input:} $\X := \brk[c]{ \brk{ \xbf^{(1,m)}, \ldots, \xbf^{(N, m)} } }_{m = 1}^M$~---~$M \in \N$ data instances comprising $N \in \N$ features each, $\K \subseteq [N]$~---~subset of feature indices \\[0.4em]
		\STATE \!\textbf{Output:} $\qe{\datatensor}{\K}$ \\[-0.2em]
		\hrulefill
		\vspace{1mm}
		\STATE Compute $\Gbf^{(\K)}, \Gbf^{(\K^c)} \in \R^{M \times M}$ given element-wise by:
		\[
			\begin{split}
				\forall i, j \in [M]: ~ &\Gbf^{(\K)}_{i, j} = y^{(i)} y^{(j)} \cdot \inprodbig{ \tenp_{n \in \K} \xbf^{(n, i)} }{ \tenp_{n \in \K} \xbf^{(n, j)}  } = y^{(i)} y^{(j)} \cdot \prod\nolimits_{n \in \K} \inprodbig{ \xbf^{(n, i)} }{ \xbf^{(n, j)}  } \\[0.3em]
				\forall i, j \in [M]: ~ &\Gbf^{(\K^c)}_{i, j} = \inprodbig{ \tenp_{n \in \K^c} \xbf^{(n, i)} }{ \tenp_{n \in \K^c} \xbf^{(n, j)}  } = \prod\nolimits_{n \in \K^c} \inprodbig{ \xbf^{(n, i)} }{ \xbf^{(n, j)}  }
			\end{split}
		\]
		\vspace{1mm}
		
		\STATE Compute eigenvalue decompositions of $\Gbf^{(\K)}$ and $\Gbf^{(\K^c)}$, \ie:
		\[
		\begin{split}
			& \Gbf^{(\K)} = \Ubf^{(\K)} \Sbf^{(\K)} \brk1{ \Ubf^{(\K)} }^\top \\[0.3em]
			& \Gbf^{(\K^c)} =  \Ubf^{(\K^c)} \Sbf^{(\K^c)} \brk1{ \Ubf^{(\K^c)} }^\top
		\end{split}
		\]
		where $\Ubf^{(\K)}, \Ubf^{(\K^c)} \in \R^{M \times M}$ are orthogonal matrices and $\Sbf^{(\K)}, \Sbf^{(\K^c)} \in \R^{M \times M}$ are diagonal holding the eigenvalues of $\Gbf^{(\K)}$ and $\Gbf^{(\K^c)}$, respectively
		\vspace{2.5mm}
		
		\STATE Compute $\Qbf = \big ( \Sbf^{(\K)} \big )^{\frac{1}{2}} \big ( \Ubf^{(\K)} \big )^{\top} \Ubf^{(\K^c)} \big ( \Sbf^{(\K^c)} \big )^{\frac{1}{2}} \in \R^{M \times M}$
		\vspace{3mm}
		
		\STATE Compute a singular value decomposition of $\Qbf$ to obtain its singular values $\sigma_1 (\Qbf), \ldots, \sigma_{M} (\Qbf)$
		\vspace{3mm}
		
		\STATE Let $\rho_{m} := \sigma_m^2 (\Qbf) / \sum\nolimits_{m' = 1}^M \sigma_{m'}^2 (\Qbf)$ for $m \in [M]$

		\vspace{3mm}
		\STATE \textbf{return} $\qe{\datatensor}{\K} = - \sum\nolimits_{m = 1}^{M} \rho_m \ln (\rho_m)$ ~~ (if $\Qbf = 0$, then return $0$)
	\end{algorithmic}
\end{algorithm}

For a given tensor, its entanglement with respect to a partition of axes (\cref{def:entanglement}) is determined by the singular values of its arrangement as a matrix according to the partition.
Since the empirical data tensor $\datatensor$ (\cref{eq:data_tensor}) has size exponential in the number of features $N$, it is infeasible to naively compute its entanglement (or even explicitly store it in memory).
Fortunately, as shown in~\cite{martyn2020entanglement}, the specific form of the empirical data tensor admits an efficient algorithm for computing entanglements, without explicitly manipulating an exponentially large matrix.
Specifically, the algorithm runs in $\OO (D N M^2 + M^3)$ time and requires $\OO (D N M + M^2)$ memory, where $D := \max_{n \in [N]} D_n$ is the maximal feature dimension (\ie~axis length of $\datatensor$), $N$ is the number of features in the data (\ie~number of axes that $\datatensor$ has), and $M$ is the number of training instances.
For completeness, we outline the method in~\cref{alg:ent_comp} while referring the interested reader to Appendix A in~\cite{martyn2020entanglement} for further details.

%
% \[D_{emp}=\frac{1}{M} \sum_{m = 1}^M y_m \cdot \tenp_{\nu \in [N]} \xbf^{(\nu)}_m=\frac{1}{M} \sum_{m = 1}^M (y_m \cdot \tenp_{\nu \in \K} \xbf^{(\nu)}_m)\tenp_{\nu \in \K^{c}} \xbf^{(\nu)}_m\]
%
%\ie~it is the sum of $M$ rank one tensors, where $M$ is the number of instances. Furthermore, we can efficiently compute the matrices of inner products:
%\[(M_{\K})_{i,j}=y_{i}y_{j}\inprod{  \tenp_{\nu \in \K} \xbf_{i}^{(\nu)}  }{\tenp_{\nu \in \K} \xbf_{j}^{(\nu)}}\]
%\[(M_{\K^{c}})_{i,j}=\inprod{  \tenp_{\nu \in \K^{c}} \xbf_{i}^{(\nu)}  }{\tenp_{\nu \in \K^{c}} \xbf_{j}^{(\nu)}}\]
%
%We can now compute the spectrum via straightforward linear algebra: 
%using \(M_{\K},M_{\K^{c}}\)  we can apply the Grahm-Shmidt procedure to get orthonormal bases on both sides of the partition, i.e we find two sets of vectors \(\lbrace \varphi{j}^{\K} \rbrace,\lbrace \varphi{j}^{\K^{c}} \rbrace\) such that:
%\[\tenp_{\nu \in \K} \xbf_{i}^{(\nu)}=\sum_{j}(W_{\K})_{ij}\varphi_{j}^{\K}\;\; , \;\; \inprod{\varphi_{j_{1}}^{\K}}{\varphi_{j_{2}}^{\K}}=\delta_{j_{1},j_{2}}\]
%\[\tenp_{\nu \in \K^{c}} \xbf_{i}^{(\nu)}=\sum_{j}(W_{\K^{c}})_{ij} \varphi{j}^{\K^{c}}\;\; \inprod{\varphi_{j_{1}}^{\K^{c}}}{\varphi_{j_{2}}^{\K^{c}}}=\delta_{j_{1},j_{2}}\]
%(One obtains the change of basis matrices \(W_{\K},W_{\K^{c}}\) as 
%\[W_{\K}=D_{\K}^{\frac{1}{2}}U^{\dagger}_{\K}\;\; , \;\; W_{\K^{c}}=D_{\K^{c}}^{\frac{1}{2}}U^{\dagger}_{\K^{c}}\]
%where 
%\[M_{\K}=U^{\dagger}_{\K}D_{\K}U_{\K}\;\; , \;\; M_{\K^{c}}=U^{\dagger}_{\K^{c}}D_{\K^{c}}U_{\K^{c}}\]
%are eigendecompositions of \(M_{\K},M_{K^{c}}\)).\\
%Plugging these equalities into the formula for \(D_{emp}\) one obtains
%\[D_{emp}=\frac{1}{M}\sum_{\alpha}\sum_{\beta}O_{\alpha,\beta}\varphi_{\alpha}^{\K}\tenp\varphi_{\beta}^{\K^{c}}\] 
%for some matrix \(O_{\alpha,\beta}\). Now one can obtain the entanglement of \(\mat{\D_{emp}}{\K}\) as the entropy of the (normalized) singular values of \(O\) (see eg \cite{nielsen_chuang_2010}). Using standard linear algebra algorithms for the operations outlined we obtain the entanglement \(QE(\D_{emp},\K)\) in \(O(M^{3})\) time. \\
