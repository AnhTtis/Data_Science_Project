\section{Deferred Proofs}  \label{app:proofs}
\subsection{Useful Lemmas}
Below we collect a few useful results, which we will use in our proofs.

\begin{lemma} \label{app:proofs:perturb}
		We denote the vector of singular values of a matrix \(\Xbf\) (arranged in decreasing order) by \(S(\mathbf{X})\). 
		For any $\Abf, \Bbf \in \R^{D_1 \times D_2}$ it holds that:
		\[
		\norm{ S(\Abf) - S(\Bbf) } \leq \norm{ S(\Abf - \Bbf) }\]
\end{lemma}
\begin{proof} See Theorem \RN{3}.4.4 of~\cite{bhatia_matrix_1997}.
\end{proof}

\begin{lemma} \label{app:proofs:prob_ineq}
Let \(\P=\lbrace p_{1},...,p_{N} \rbrace,\Q=\lbrace q_{1},...,q_{N} \rbrace\) be two probability distributions supported on \([N]\), and denote by $TV(\P,\Q) := \frac{1}{2} \sum_{n = 1}^N \abs{p_n - q_n}$ their \emph{total variation distance}.
If for $\epsilon \in (0, 1/2)$ it holds that $TV (\P, \Q) \leq \epsilon$, then:
\[
\lvert H(\P)-H(\Q)\rvert \leq H_{b}(\epsilon)+\epsilon \cdot \ln(N)
\text{\,,}
\]
where $H(\P) := - \sum\nolimits_{n = 1}^N p_n \ln (p_n)$ is the entropy of $\P$, and $H_{b}(c) := -c \cdot \ln (c) - (1-c) \cdot \ln (1-c)$ is the binary entropy of a Bernoulli distribution parameterized by \(c \in [0,1]\).
\end{lemma}
\begin{proof} 
See,~\eg, Theorem 11 of~\citep{ho2010interplay}.
\end{proof}

\begin{lemma}[Hoeffding inequality in Hilbert space] \label{app:proofs:hoeffding_hilbert}
	Let \(X_{1},..,X_{N}\) be an i.i.d. sequence of random variables whose range is some separable Hilbert space $\HH$.
	Suppose that \( \EE \brk[s]{ X_{n} } = 0 \) and \(\lVert X_{n}\rVert\leq c\) for all $n \in [N]$.
	Then, for all \(t\geq 0\):
	\[
	Pr \left (\norm*{ \frac{1}{n} \sum\nolimits_{n = 1}^N X_{n}}\geq t \right ) \leq 2\exp\left(-\frac{Nt^{2}}{2c^{2}}\right)
	\text{\,,}
	\]
	where $\norm{\cdot}$ refers to the Hilbert space norm.
	
\end{lemma}
\begin{proof}
See Section 2.4 of~\citep{10.5555/1756006.1756036}.
\end{proof}

\begin{lemma}[adapted from~\cite{levine2018deep}]
\label{app:proofs:min_cut}
Let $G = (V, E)$ be the perfect binary tree graph, with vertices $V$ and edges $E$, underlying the locally connected tensor network that generates $\tntensor \in \R^{D_1 \times \cdots \times D_N}$ (defined in~\cref{sec:fit_tensor:tn_lc_nn}).
For $\K \subseteq [N]$, let $V_\K \subseteq V$ and $V_{K^c} \subseteq \V$ be the leaves in $G$ corresponding to axes indices $\K$ and $\K^c$ of $\tntensor$, respectively.
Lastly, given a cut $(A, B)$ of $V$, \ie~$A \subseteq V$ and $B \subseteq V$ are disjoint and $A \cup B = V$, denote by $C (A, B) := \{ \{u, v\} \in E : u \in A , v \in B \}$ the edge-cut set.
Then:
\[	
\rank (\mat{\tntensor}{\K}) \leq \min\nolimits_{\text{ cut } (A,B) \text{ of } V \text{ s.t. } V_\K \subseteq A , V_{\K^c} \subseteq B} R^{\abs{C(A,B) }  }
\text{\,.}
\]
In particular, if $(\K, \K^c) \in \can$, then:
\[
\rank ( \mat{\tntensor}{\K} ) \leq R
\text{\,.}
\]
\end{lemma}
\begin{proof}
See Claim 1 in~\citep{levine2018deep} for the upper bound on the rank of $\mat{\tntensor}{\K}$ for any $\K \subseteq [N]$.
If $(\K, \K^c) \in \can$, since there exists a cut $(A, B)$ of $V$ such that $V_\K \subseteq A$ and $V_{\K^c} \subseteq B$ whose edge-cut set is of a singleton, we get that $\rank ( \mat{\tntensor}{\K} ) \leq R$.
\end{proof}


\begin{lemma}[adapted from~\cite{levine2018deep}]
	\label{app:proofs:pdim_min_cut}
	Let $G = (V, E)$ be the perfect $2^P$-ary tree graph, with vertices $V$ and edges $E$, underlying the locally connected tensor network that generates $\tntensorpdim{P} \in \R^{D_1 \times \cdots \times D_{N^{P}}}$ (defined in~\cref{app:extension_dims:fit_tensor:tn_lc_nn}).
	Furthermore, let $\axismap: [N]^P \to [N^P]$ be a compatible map from $P$-dimensional coordinates to axes indices of $\tntensorpdim{P}$ (\cref{def:compat_map}).
	For $\J \subseteq [N^{P}]$, let $V_\J \subseteq V$ and $V_{\J^c} \subseteq \V$ be the leaves in $G$ corresponding to axes indices $\J$ and $\J^c$ of $\tntensorpdim{P}$, respectively.
	Lastly, given a cut $(A, B)$ of $V$, \ie~$A \subseteq V$ and $B \subseteq V$ are disjoint and $A \cup B = V$, denote by $C (A, B) := \{ \{u, v\} \in E : u \in A , v \in B \}$ the edge-cut set.
	Then:
	\[	
	\rank (\mat{\tntensorpdim{P}}{\J}) \leq \min\nolimits_{\text{ cut } (A,B) \text{ of } V \text{ s.t. } V_\J \subseteq A , V_{\J^c} \subseteq B} R^{\abs{C(A,B) }  }
	\text{\,.}
	\]
	In particular, if $(\K, \K^c) \in \can^{P}$, then:
	\[
	\rank ( \mat{\tntensor}{\axismap (\K)} ) \leq R
	\text{\,.}
	\]
\end{lemma}
\begin{proof}
	See Claim 1 in~\citep{levine2018deep} for the upper bound on the rank of $\mat{\tntensorpdim{P}}{\J}$ for any $\J \subseteq [N^{P}]$.
	If $(\K, \K^c) \in \can^{P}$, since there exists a cut $(A, B)$ of $V$ such that $V_{\axismap (\K)} \subseteq A$ and $V_{\axismap(K)^c} \subseteq B$ whose edge-cut set is of a singleton, we get that $\rank ( \mat{\tntensorpdim{P}}{\axismap (\K)} ) \leq R$.
\end{proof}


\begin{lemma}[adapted from Theorem 3.18 in~\citep{grasedyck2010hierarchical}]\label{app:proofs:grasedyck}
	Let \(\mathcal{A}\in \mathbb{R}^{D_{1}\times ...\times D_{N}}\) and \(\epsilon >0\). 
	For each canonical partition \((\K,\K^{c}) \in \can\), let \(\sigma_{\K,1}\geq ...\sigma_{K, D_\K}\) be the singular values of \(\mat{\mathcal{A}}{\K}\), where \(D_{\K}:= \min \brk[c]{ \prod_{n \in \K} D_n , \prod_{n \in \K^c} D_n }\). 
	Let \((n_{\K})_{\K \in  \can}\in \mathbb{N}^{ \can}\) be an assignment of an integer to each \(\K\in \can\). 
	For any such assignment, consider the set of tensors with \emph{Hierarchical Tucker} (HT) rank at most \((n_{\K})_{\K \in  \can}\) as follows:
	\[
		HT \left ( (n_{\K})_{\K \in \can} \right ) := \left \{ \V \in \R^{ D_1 \times \cdots \times D_{N} } : \forall \K \in \can,\rank(\mat{\V}{\K})\leq n_{\K} \right \} 
	\text{\,.}
	\]
	Suppose that for all \( (\K, \K^c) \in \can\) it holds that $\sqrt{\sum\nolimits_{d = n_{\K} + 1}^{D_\K} \sigma^2_{\K, d}}\leq \frac{\epsilon}{\sqrt{2N-3}}$.
	Then, there exists \(\W \in HT \left ( (n_{\K})_{\K \in \can} \right )\) satisfying:
	\[
	\norm{\W - \mathcal{A}}\leq \epsilon
	\text{\,.}
	\]
	In particular, if for all \( (\K, \K^c) \in \can\) it holds that $\sqrt{\sum\nolimits_{d = R + 1}^{D_\K} \sigma^2_{\K, d}}\leq \frac{\epsilon}{\sqrt{2N-3}}$, then there exists $\tntensor \in \R^{D_1 \times \cdots \times D_N}$ generated by the locally connected tensor network (defined in~\cref{sec:fit_tensor:tn_lc_nn}) satisfying:
	\[
	\norm{\tntensor - \mathcal{A}}\leq \epsilon
	\text{\,.}
	\]
\end{lemma}


\begin{lemma}[adapted from Theorem 3.18 in~\citep{grasedyck2010hierarchical}]
	\label{app:proofs:grasedyck_exact}
	Let \((n_{\K})_{\K \in  \can}\in \mathbb{N}^{ \can}\) be an assignment of an integer to each \(\K\in \can\). 
	For any such assignment, consider the set of tensors with \emph{Hierarchical Tucker} (HT) rank at most \((n_{\K})_{\K \in  \can}\) as follows:
	\[
	HT \left ( (n_{\K})_{\K \in \can} \right ) := \left \{ \V \in \R^{ D_1 \times \cdots \times D_{N} } : \forall \K \in \can,\rank(\mat{\V}{\K})\leq n_{\K} \right \} 
	\text{\,.}
	\]
	Consider a locally connected tensor network with varying widths $(n_{\K})_{\K \in  \can}$, \ie~a tensor network conforming to a perfect binary tree graph in which the lengths of inner axes are as follows (in contrast to all being equal to $R$ as in the locally connected tensor network defined in~\cref{sec:fit_tensor:tn_lc_nn}).
	An axis corresponding to an edge that connects a node with descendant leaves indexed by $\K$ to its parent is assigned the length $n_\K$.
	Then, every \(\A \in HT \left ( (n_{\K})_{\K \in \can} \right )\) can be represented by said locally connected tensor network with varying widths, meaning there exists an assignment to the tensors of the tensor network such that it generates $\A$.
	In particular, if \(n_{K}=R\) for all \(\K \in \can\), then $\A$ can be generated by the locally connected tensor network with all inner axes being of length $R$.
\end{lemma}

\begin{lemma} 
	\label{app:proofs:tensor_ineq}
	Let \(\V,\W \in \R^{D_1 \times \cdots \times D_N} \) be tensors such that \(\norm{\V} = \norm{\W}=1\) and $\norm{\V-\W} < \frac{1}{2}$.
	Then, for any \((\K,\K^{c})\in \can\) it holds that:
	\[
	|\qe{\V}{\K}-\qe{\W}{\K}|\leq H_{b}(\norm{\V-\W})+ \norm{\V-\W}\cdot \ln (D_\K)
	\text{\,,}
	\]
	where $H_{b}(c):=-(c \cdot \ln (c) + (1 - c) \cdot \ln ( 1 - c ) )$ is the binary entropy of a Bernoulli distribution parameterized by \(c \in [0,1]\), and \(\D_{\K} := \min \brk[c]{ \prod_{n \in \K} D_n , \prod_{n \in \K^c} D_n }\). 
\end{lemma}

\begin{proof}
	
For any matrix \(\Mbf \in \R^{D_1 \times D_2} \) with $D := \min \{ D_{1},D_{2} \}$, let \(S(\Mbf)=(\sigma_{\Mbf,1},...,\sigma_{\Mbf,D})\) be the vector consisting of its singular values. 
First note that 
\[
\norm{\V-\W}=\lVert S(\mat{\V-\W}{\K})\rVert \leq \norm{\V-\W}
\text{\,.}
\]
So by \cref{app:proofs:perturb} 
\[
\lVert S(\mat{\V}{\K}-S(\mat{\W}{\K}) \rVert \leq \norm{\V-\W}
\text{\,.}
\]
Let \(v_{1}\geq \cdots \geq v_{D_\K}\) and \(w_{1}\geq \cdots \geq w_{D_\K}\) be the singular values of \(\mat{\V}{\K}\) and \(\mat{\W}{\K}\), respectively. 
We have by the Cauchy-Schwarz inequality that
\[
\left(\sum_{d = 1}^{D_\K}|w_{d}^{2}-v_{d}^{2}|\right)^{2}= \left(\sum_{d = 1}^{D_\K}|w_{d}-v_{d}| \cdot |w_{d}+v_{d}|\right)^{2}\leq \left(\sum_{d = 1}^{D_\K}(w_{d}-v_{d})^{2}\right)\left(\sum_{d = 1}^{D_\K}(w_{d}+v_{d})^{2}\right)
\text{\,.}
\]
Now the first term is upper bounded by \(\norm{\V-\W}^{2}\), and for the second we have 
\[
\sum_{d = 1}^{D_\K}(w_{d}+v_{d})^{2}=\sum_{d = 1}^{D_\K}w_{d}^{2}+\sum_{d = 1}^{D_\K}v_{d}^{2}+2v_{d}w_{d}=2+2\sum_{d = 1}^{D_\K}v_{d}w_{d} \leq 4
\text{\,,}
\]
where we use the fact that \(\norm{\V} = \norm{\W}=1\), and again Cuachy-Schwarz. 
Overall we have:
\[ 
\sum_{d = 1}^{D_\K}|w_{d}^{2}-v_{d}^{2}|
\leq 2\norm{\V-\W}
\text{\,.}
\]
Note that the left hand side of the inequality above equals twice the total variation distance between the distributions defined by $\lbrace w_{d}^{2}\rbrace_{d = 1}^{D_\K}$ and $\lbrace v_{d}^{2}\rbrace_{d = 1}^{D_\K}$.
 Therefore by \cref{app:proofs:prob_ineq} we have:
\[
|\qe{\V}{\K} - \qe{\W}{\K}| = |H(\lbrace w_{d}^{2} \rbrace)-H(\lbrace v_{d}^{2} \rbrace)|\leq \norm{\V-\W}\cdot \ln(D_\K)+H_{b}(\norm{\V-\W})
\text{\,.}
\]
\end{proof}

\begin{lemma}
	\label{app:proofs:entropy}
	Let \( \P=\lbrace p(x) \rbrace_{x\in [S]} \), where \(S\in \N\), be a probability distribution, and denote its entropy by $H(\P) := \EE\nolimits_{x \sim \P} \brk[s]*{ \ln \left(1 / p(x) \right) }$.	
	Then, for any \(0 < a < 1\), there exists a subset \( T \subseteq [S]\) such that  \(Pr_{x\sim \P}(T^{c})\leq a \) and \(|T|\leq e^{\frac{H(\P)}{a}}\). 
\end{lemma}
\begin{proof}
By Markov's inequality we have for any \(0<a<1\):
\[
Pr_{x\sim \P} \left ( \left \lbrace x:e^{-\frac{H(P)}{a}}\geq p(x)  \right \rbrace \right ) = Pr_{x\sim \P} \left ( \left \lbrace x: \ln \left ( \frac{1}{p(x)} \right ) \geq \frac{H(P)}{a}  \right \rbrace \right )\leq a
\text{\,.}
\]
Let \(T:=\big \lbrace x : e^{-\frac{H(P)}{a}}\leq p(x) \big \rbrace\subseteq [S]\). 
Note that
\[  
e^{-\frac{H(\P)}{a}} |T| \leq\sum\nolimits_{x\in T}p(x)\leq \sum\nolimits_{x\in [S]}p(x)= 1
\text{\,,}
\]
and so \(|T| \leq e^{\frac{H(P)}{a}}\) and \(Pr_{x\sim \P}(T^{c})\leq a\), as required.
\end{proof}




\subsection{Proof of~\cref{thm:fit_necessary}} \label{app:proofs:fit_necssary}


If \(\A=0\) the theorem is trivial, since then \(\qe{\A}{\K}=0 \) for all \((\K, \K^{c})\in \can\), so we can assume \(\A\neq 0\). 
We have:
\[
\begin{split}
	\left\Vert \frac{\tntensor}{\norm{\tntensor}} - \frac{\A}{\norm{\A}}\right\Vert & = \frac{1}{\norm{\A}}\left\Vert \frac{\norm{\A}}{\norm{\tntensor}}\cdot \tntensor - \A \right\Vert \\
	& \leq\frac{1}{\norm{\A}} \left ( \abs*{ \frac{\norm{\A}}{\norm{\tntensor}} - 1 } \cdot \norm{\tntensor} + \norm{\tntensor -\A} \right ) \\
	& = \frac{1}{\norm{\A}} \left ( \left\vert\norm{\A}-\norm{\tntensor}\right\vert+\norm{\tntensor -\A} \right ) \\	
	& \leq \frac{2\epsilon}{\norm{A}} \text{\,.}
	\end{split}
\]
Now, let $\hat{\A} := \frac{\A}{\norm{\A}}$ and $\hat{\W}_{\mathrm{TN}} = \frac{\tntensor}{\norm{\tntensor}}$ be normalized versions of $\A$ and $\tntensor$, respectively, and let \(c = \frac{2\epsilon}{\norm{A}}\). 
Note that \(c< \frac{2\norm{\A}}{4}\frac{1}{\norm{\A}}=\frac{1}{2}\), and therefore by \cref{app:proofs:tensor_ineq} we have:
\[
\begin{split}
|\qe{\A}{\K}-\qe{\tntensor}{\K}| & = \abs*{ \qe{\hat{\A}}{\K}-\qe{\hat{\W}_{\mathrm{TN}}}{\K} } \\
& \leq c \cdot \ln (D_\K)+H_{b}(c)
\text{\,.}
\end{split}
\]
By \cref{app:proofs:min_cut} we have that 
\[\qe{ \hat{\W}_{\mathrm{TN}} }{\K}\leq \ln (\rank(\mat{\mathcal{\tntensor}}{\K})) \leq \ln (R)\text{\,,}\]
and therefore 
\[\qe{ \hat{\A} }{\K}\leq \ln (R) + c \ln (D_\K) +H_{b}(c)\text{\,.}\]
Substituting \(c=\frac{2\epsilon}{\norm{A}}\) and invoking the elementary inequality \(H_{b}(x)\leq 2\sqrt{x}\) we obtain
\[	\qe{\A}{\K} \leq \ln (R)+\frac{2 \epsilon}{ \norm{\A} } \cdot \ln ( D_{\K} ) + 2 \sqrt{\frac{ 2 \epsilon }{ \norm{\A} } }\text{\,,}\]
as required.
\cref{eq:fit_necessary_lb} follows from the construction given in Theorem~2 of \citep{deng2017quantum}.
\qed

\subsection{Proof of~\cref{thm:fit_sufficient}} \label{app:proofs:fit_sufficient}
Consider, for each canonical partition \(\brk{\K, \K^c} \in \can \), the distribution 
	\[\P_{\K}= \left \lbrace p_{\K}(i) := \frac{\sigma_{\K,i}^{2}}{\lVert \A \rVert^{2}} \right \rbrace_{i\in [D_\K]}\text{\,,}\]
	where \(\sigma_{\K,1}\geq \sigma_{\K,2}\geq ...\geq \sigma_{\K,D_\K} \) are the singular values of \(\mat{\mathcal{A}}{\K}\) (note that \(\frac{1}{\norm{\A}^{2}}\sum_{j}\sigma_{\K,j}^{2}= \frac{\norm{\mathcal{A}}^{2}}{\lVert A \rVert^{2}} =1\) so \(\P_{\K}\) is indeed a probability distribution). 
	Denoting by $H (\P_{\K}) := \EE\nolimits_{i \sim \P_\K} \brk[s]*{ \ln \left(1 / p_\K (i) \right) }$ the entropy of $\P_\K$, by assumption:
	\[\qe{\A}{\K}=H(\P_{\K})\leq \frac{\epsilon^{2}}{\lVert \A \rVert^{2}(2N-3)} \ln (R)\text{\,,} \]
	for all \((\K,\K^{c})\in \can\).
	Thus, taking \(a = \frac{\epsilon^{2}}{\lVert \A \rVert^{2}(2N-3)}\) we obtain by \cref{app:proofs:entropy} that there exists a subset \(T_{\K}\subseteq[D_\K]\) such that  
	\[
	\P_{\K}(T_{\K}^{c})\leq \frac{\epsilon^{2}}{(2N-3)\lVert \A \rVert^{2}}
	\text{\,,}
	\]
	and \(|T_{\K}| \leq e^{\frac{H(\P_{\K})}{c}}=e^{\ln (R)} = R\). Note that 
	\[
	\P_{\K}(T_{\K})\leq \sum\nolimits_{i = 1}^{R}\frac{\sigma_{i}^{2}}{\lVert \A \rVert^{2}}
	 \text{\,.}
	\] 
	Since this holds for any subset of cardinality at most \(R\).
	Taking complements we obtain
	\[\sum\nolimits_{i = R+1}^{D_\K}\frac{\sigma_{i}^{2}}{\lVert \A \rVert^{2}}\leq \P_{\K}(T_{\K}^{c}) \text{\,,}\]
	so 
	\[\sqrt{\sum\nolimits_{i=R+1}^{D_\K}\sigma_{\K,i}^{2}}\leq \frac{\epsilon}{\sqrt{(2N-3)}} \text{\,.} \]
	We can now invoke \cref{app:proofs:grasedyck}, which implies that there exists some \(\tntensor \in \R^{D_1 \times \cdots \times D_N}\) generated by the locally connected tensor network satisfying:
	\[\norm{ \tntensor - \A } \leq \epsilon \text{\,.}\]
\qed



\subsection{Proof of~\cref{cor:acc_pred_nec_and_suf}} \label{app:proofs:cor:acc_pred_nec_and_suf}

Notice that the entanglements of a tensor are invariant to multiplication by a constant.
In particular, $\qe{\popdatatensor}{\K} = \qe{\popdatatensor / \norm{\popdatatensor}}{\K}$ for any $(\K, \K^c) \in \can$.
Hence, if there exists a canonical partition $\brk{ \K, \K^c} \in \can$ under which $\qe{\popdatatensor}{\K} > \ln (R) + 2\epsilon \cdot \ln ( D_\K )+ 2 \sqrt{2 \epsilon}$, then~\cref{thm:fit_necessary} implies that $\min_{\tntensor} \norm{\tntensor - \popdatatensor / \norm{\popdatatensor}} > \epsilon$.
Now, for any non-zero $\W \in \R^{D_1 \times \cdots D_N}$ generated by the locally connected tensor network, one can also represent $\W / \norm{\W}$ by multiplying any of the tensors constituting the tensor network by $1 / \norm{\W}$ (contraction is a multilinear operation).
Thus:
\[
\subopt := \min\nolimits_{\tntensor} \norm*{ \frac{\tntensor}{\norm{\tntensor}} - \frac{\popdatatensor}{\norm{\popdatatensor}} } \geq \min\nolimits_{\tntensor} \norm*{\tntensor - \frac{ \popdatatensor }{ \norm{\popdatatensor}} } > \epsilon
\text{\,,}
\]
which concludes the first part of the claim, \ie~the necessary condition for low suboptimality in achievable accuracy.

For the sufficient condition, if for all $\brk{ \K, \K^c} \in \can$ it holds that $\qe{\popdatatensor}{\K} \leq \frac{\epsilon^{2}}{8 N -12} \cdot \ln (R)$, then by~\cref{thm:fit_sufficient} there exists an assignment for the locally connected tensor network such that $\norm{\tntensor - \popdatatensor / \norm{\popdatatensor} } \leq \epsilon / 2$.
From the triangle inequality we obtain:
\be
\norm*{\frac{\tntensor}{\norm{\tntensor}} - \frac{ \popdatatensor }{ \norm{\popdatatensor} } } \leq \norm*{\tntensor - \frac{ \popdatatensor }{ \norm{\popdatatensor} } }  +  \norm*{\tntensor - \frac{\tntensor}{\norm{\tntensor}}} \leq \frac{\epsilon}{2} + \norm*{\tntensor - \frac{\tntensor}{\norm{\tntensor}}}
\text{\,.}
\label{eq:tn_tensor_pop_datatensor_norm}
\ee
Since $\norm{\tntensor - \popdatatensor / \norm{\popdatatensor} } \leq \epsilon / 2$ it holds that $\norm{\tntensor} \leq 1 + \epsilon / 2$.
Combined with the fact that $\norm1{\tntensor - \frac{\tntensor}{\norm{\tntensor}}} = \norm{\tntensor} - 1$, we get that $\norm1{\tntensor - \frac{\tntensor}{\norm{\tntensor}}} \leq \epsilon / 2$.
Plugging this into~\cref{eq:tn_tensor_pop_datatensor_norm} yields:
\[
\norm*{\frac{\tntensor}{\norm{\tntensor}} - \frac{ \popdatatensor }{ \norm{\popdatatensor} } } \leq \epsilon
\text{\,,}
\]
and so $\subopt := \min\nolimits_{\tntensor} \norm1{ \frac{\tntensor}{ \norm{\tntensor} } -  \frac{ \popdatatensor }{ \norm{\popdatatensor} }  } \leq \epsilon$.
\qed




\subsection{Proof of~\cref{prop:data_tensor_concentration}} \label{app:proofs:data_tensor_concentration}
We have the identity 
\[
\left\Vert\frac{\popdatatensor}{\norm{\popdatatensor}}-\frac{\datatensor}{\norm{\datatensor}}\right\Vert =\left\Vert \frac{\D_{pop}\norm{\datatensor}-\datatensor\norm{\popdatatensor}}{\norm{\popdatatensor}\norm{\datatensor}}\right\Vert =\]
\[\left\Vert\frac{\popdatatensor\norm{\datatensor}-\norm{\datatensor}\datatensor+\datatensor\norm{\datatensor}-\datatensor\norm{\popdatatensor}}{\norm{\popdatatensor}\norm{\datatensor}}\right\Vert
\text{\,.}
\]
By the triangle inequality the above is bounded by 
\[
\frac{\norm{\popdatatensor-\datatensor}}{\norm{\popdatatensor}}+\frac{|\norm{\popdatatensor}-\norm{\datatensor}|}{\norm{\popdatatensor}}
\text{\,.}
\]
For $m \in [M]$, let \(\X^{(m)} = y^{(m)} \cdot \tenp_{n \in [N]} \xbf^{(n, m)}-\popdatatensor\). 
These are i.i.d. random variables with $\EE\brk[s]{ \X^{(m)} } = 0$ and $\norm{\X^{(m)}}\leq 2$ for all $m \in [M]$. 
Note that 
\[
\left\Vert\frac{1}{M}\sum\nolimits_{m = 1}^{M}\X^{(m)} \right\Vert=\norm{\datatensor-\popdatatensor}
\text{\,,}
\]
so by \cref{app:proofs:hoeffding_hilbert} with \(c=2,t=\frac{\norm{\popdatatensor}\gamma}{2}\), assuming \(M\geq \frac{2 \ln (\frac{2}{\delta})}{\norm{\popdatatensor}^{2}\gamma^{2}}\) we have with probability at least \(1-\delta\)
\[
|\norm{\popdatatensor} - \norm{\datatensor}| \leq \norm{\D_{pop}-\datatensor}\leq \frac{\norm{\popdatatensor}\gamma}{2} \text{\,,}
\]
and therefore 

\[
\frac{\norm{\popdatatensor-\datatensor}}{\norm{\popdatatensor}}+\frac{|\norm{\popdatatensor}-\norm{\datatensor}|}{\norm{\popdatatensor}}\leq  \gamma
\text{\,.}
\]
\qed


\subsection{Proof of~\cref{cor:eff_acc_pred_nec_and_suf}} \label{app:proofs:cor:eff_acc_pred_nec_and_suf}

	First, by \cref{prop:data_tensor_concentration}, for $M \geq \frac{8\ln(\frac{2}{\delta})}{\norm{\popdatatensor}^{2}\epsilon^{2}} $ we have that with probability at least $1 - \delta$:
	\be
		\norm*{\frac{\popdatatensor}{\norm{\popdatatensor}} - \frac{\datatensor}{\norm{\datatensor}}} \leq \frac{\epsilon}{2}
		\text{\,.}
		\label{eq:norm_hoeff_cor_prof}
	\ee
	Now, we establish the necessary condition on the entanglements of $\datatensor$.
	Assume that there exists a canonical partition $\brk{ \K, \K^c} \in \can$ under which
	\[
	\qe{\datatensor}{\K} >  \ln (R) + 3 \epsilon \cdot \ln (D_\K) + 2 \sqrt{3\epsilon}
	\text{\,.}
	\]
	We may view $\datatensor$ as the population data tensor for the uniform data distribution over the training instances $\brk[c]1{ \brk1{ \brk{ \xbf^{(1,m)}, \ldots, \xbf^{(N,m)} } , y^{(m)} } }_{m = 1}^M$.
	Thus,~\cref{cor:acc_pred_nec_and_suf} implies that
	\[
	\min\nolimits_{\tntensor} \norm*{\frac{\tntensor}{\norm{\tntensor}} - \frac{\datatensor}{\norm{\datatensor}}} > 1.5 \epsilon
	\text{\,.}
	\]
	Using the triangle inequality after adding and subtracting $\frac{\popdatatensor}{\norm{\popdatatensor}}$, it follows that
	\[
	\min\nolimits_{\tntensor}  \left\{ \norm*{\frac{\tntensor}{\norm{\tntensor}} - 
		\frac{\popdatatensor}{\norm{\popdatatensor}}} + \norm*{\frac{\popdatatensor}{\norm{\popdatatensor}} - \frac{\datatensor}{\norm{\datatensor}}} \right\} > 1.5 \epsilon
	\text{\,.}
	\]
	Hence, combined with~\cref{eq:norm_hoeff_cor_prof} this concludes the current part of the proof:
	\[
	\subopt := \min\nolimits_{\tntensor} \norm*{\frac{\tntensor}{\norm{\tntensor}} - 
		\frac{\popdatatensor}{\norm{\popdatatensor}}}  > 1.5 \epsilon - 
	\norm*{\frac{\popdatatensor}{\norm{\popdatatensor}} - \frac{\datatensor}{\norm{\datatensor}}} \geq 1.5 \epsilon - \frac{\epsilon}{2} = \epsilon.
	\]
	
	We turn our attention to the sufficient condition on the entanglements of $\datatensor$.
	Suppose that $\qe{\datatensor}{\K} \leq \frac{\epsilon^{2}}{32 N -48} \cdot \ln (R)$ for all canonical partitions $(\K, \K^c) \in \can$.
	Invoking~\cref{cor:acc_pred_nec_and_suf} while viewing $\datatensor$ as the population data tensor for the uniform data distribution over the training instances $\brk[c]1{ \brk1{ \brk{ \xbf^{(1,m)}, \ldots, \xbf^{(N,m)} } , y^{(m)} } }_{m = 1}^M$, we get that
	\[
	\min\nolimits_{\tntensor} \norm*{\frac{\tntensor}{\norm{\tntensor}}- \frac{\datatensor}{\norm{\datatensor}}} \leq \frac{\epsilon}{2}.
	\]
	Combined with~\cref{eq:norm_hoeff_cor_prof}, and by the triangle inequality, we conclude: 
		\[
		\begin{split}
			\subopt & := \min\nolimits_{\tntensor} \norm*{\frac{\tntensor}{\norm{\tntensor}} - 
			\frac{\popdatatensor}{\norm{\popdatatensor}}} 
			 \\
			 & \leq \min\nolimits_{\tntensor} \norm*{\frac{\tntensor}{\norm{\tntensor}} - \frac{\datatensor}{\norm{\datatensor}}} + 
			\norm*{\frac{\popdatatensor}{\norm{\popdatatensor}} - \frac{\datatensor}{\norm{\datatensor}}} \\
			& \leq \epsilon
			\text{\,.}
		\end{split}
		\]
		
	Lastly, the fact that the entanglements of $\datatensor$ can be evaluated efficiently is discussed in~\cref{app:ent_comp}.
\qed

\subsection{Proof of~\cref{thm:fit_necessary_pdim}} \label{app:proofs:fit_necssary_pdim}

If \(\A=0\) the theorem is trivial, since then \(\qe{\A}{ \axismap (\K)}=0 \) for all \((\K, \K^{c})\in \can^P\), so we can assume \(\A\neq 0\). 
We have:
\[
\begin{split}
	\left\Vert \frac{\tntensorpdim{P}}{\norm{\tntensorpdim{P}}} - \frac{\A}{\norm{\A}}\right\Vert & = \frac{1}{\norm{\A}}\left\Vert \frac{\norm{\A}}{\norm{\tntensorpdim{P}}}\cdot \tntensorpdim{P} - \A \right\Vert \\
	& \leq\frac{1}{\norm{\A}} \left ( \abs*{ \frac{\norm{\A}}{\norm{\tntensorpdim{P}}} - 1 } \cdot \norm{\tntensorpdim{P}} + \norm{\tntensorpdim{P} -\A} \right ) \\
	& = \frac{1}{\norm{\A}} \left ( \left\vert\norm{\A}-\norm{\tntensorpdim{P}}\right\vert+\norm{\tntensorpdim{P} -\A} \right ) \\	
	& \leq \frac{2\epsilon}{\norm{A}} \text{\,.}
\end{split}
\]
Now, let $\hat{\A} := \frac{\A}{\norm{\A}}$ and $\hat{\W}_{\mathrm{TN}}^{P} = \frac{\tntensorpdim{P}}{\norm{\tntensorpdim{P}}}$ be normalized versions of $\A$ and $\tntensorpdim{P}$, respectively, and let \(c = \frac{2\epsilon}{\norm{A}}\). 
Note that \(c< \frac{2\norm{\A}}{4}\frac{1}{\norm{\A}}=\frac{1}{2}\), and therefore by \cref{app:proofs:tensor_ineq} we have:
\[
\begin{split}
	|\qe{\A}{\axismap(\K)}-\qe{\tntensorpdim{P}}{\axismap(\K)}| & = \abs*{ \qe{\hat{\A}}{\axismap(\K)}-\qe{\hat{\W}_{\mathrm{TN}}^{P}}{\axismap(\K)} } \\
	& \leq c \cdot \ln (D_{\axismap(\K)})+H_{b}(c)
	\text{\,.}
\end{split}
\]
By \cref{app:proofs:pdim_min_cut} we have that 
\[\qe{ \hat{\W}_{\mathrm{TN}}^{P}}{\axismap(\K)}\leq \ln (\rank(\mat{\mathcal{\tntensorpdim{P}}}{\axismap(\K)})) \leq \ln (R)\text{\,,}\]
and therefore 
\[\qe{ \hat{\A} }{\axismap(\K)}\leq \ln (R) + c \ln (D_{\axismap(\K)}) +H_{b}(c)\text{\,.}\]
Substituting \(c=\frac{2\epsilon}{\norm{A}}\) and invoking the elementary inequality \(H_{b}(x)\leq 2\sqrt{x}\) we obtain
\[	\qe{\A}{\axismap(\K)} \leq \ln (R)+\frac{2 \epsilon}{ \norm{\A} } \cdot \ln ( D_{\axismap(\K)} ) + 2 \sqrt{\frac{ 2 \epsilon }{ \norm{\A} } }\text{\,,}\]
as required.
\cref{eq:fit_necessary_lb_pdim} follows from the construction given in Theorem~2 of \citep{deng2017quantum}.\\
\qed


\subsection{Proof of~\cref{thm:fit_sufficient_pdim}} \label{app:proofs:fit_sufficient_pdim}
	Let \(\canone\) be the one-dimensional canonical partitions of \([N^{P}]\) (\cref{def:canonical_partitions}). 
	Note that \(\axismap (\can^P) := \{ \axismap (\K) : \K \in \can^P \} \subseteq \canone\). 
	For an assignment \((n_{\K})_{\K \in  \canone} \in \mathbb{N}^{ \canone}\) of integers to one-dimensional canonical partitions $\K \in \canone$, we consider the set of tensors whose matricization with respect to each $\K \in \canone$ has rank at most $n_\K$.
	This set is also known in the literature as the set of tensors with \emph{Hierarchical Tucker (HT) rank} at most \((n_{\K})_{\K \in  \canone}\) (\cf~\cite{grasedyck2010hierarchical}).
	Accordingly, we denote it by:
	\[
	HT \left ( ( n_{\K} )_{\K \in \canone} \right ) := \left \{ \V \in \R^{ D_1 \times \cdots \times D_{N^P} } : \forall \K \in \canone,\rank(\mat{\V}{\K})\leq n_{\K} \right \} \text{\,.}
	\]
	Now, define \((n^{*}_{\K})_{\K \in  \canone}\in \mathbb{N}^{ \canone}\) by:
	\[
	\forall \K \in \canone: ~n^{*}_{\K} =
	\begin{cases}
		R &\text{  if }\; \axismap^{-1} (\K) \in \can^{P} \\
		\D_{\K}&\text{ if}\; \axismap^{-1} (\K) \notin \can^{P}\\
	\end{cases}
	\text{\,,}
	\]
	where $D_{\K} := \min \brk[c]{ \prod_{n \in \K} D_n , \prod_{n \in \K^c} D_n }$.
	We show that for any tensor \(\A\) that satisfies for all $\K \in \can^P$:
	\[
	\qe{\A}{\axismap (\K)} \leq \frac{\epsilon^{2}}{(2 N^P - 3) \norm{\A}^2} \cdot \ln (R)
	\text{\,,}
	\]
	there exists a tensor \(\V\in HT\left( (n^{*}_{\K})_{\K \in \canone}\right) \) such that $\norm{\A-\V}\leq \epsilon$.
	Consider, for each canonical partition \(\brk{\K, \K^c} \in \can^{P} \), the distribution 
	\[
	\P_{\K} = \left \lbrace p_{\K}(i) := \frac{\sigma_{\K,i}^{2}}{\lVert \A \rVert^{2}} \right \rbrace_{i\in [D_\K]}
	\text{\,,}
	\]
	where \(\sigma_{\K,1}\geq \sigma_{\K,2}\geq ...\geq \sigma_{\K,D_\K} \) are the singular values of \(\mat{\mathcal{A}}{\axismap(\K)}\) (note that \(\frac{1}{\norm{\A}^{2}}\sum_{j}\sigma_{\K,j}^{2}= \frac{\norm{\mathcal{A}}^{2}}{\lVert A \rVert^{2}} =1\) so \(\P_{\K}\) is indeed a probability distribution). 
	Denoting by $H (\P_{\K}) := \EE\nolimits_{i \sim \P_\K} \brk[s]*{ \ln \left(1 / p_\K (i) \right) }$ the entropy of $\P_\K$, by assumption:
	\[
	\qe{\A}{\axismap (\K)} = H (\P_{\K})\leq \frac{\epsilon^{2}}{\lVert \A \rVert^{2}(2N^{P}-3)} \ln (R)\text{\,,} 
	\]
	for all \((\K,\K^{c})\in \can^P\).
	Thus, taking \(a = \frac{\epsilon^{2}}{\lVert \A \rVert^{2}(2N^{P}-3)}\) we get by~\cref{app:proofs:entropy} that there exists a subset \(T_{\K}\subseteq[D_\K]\) such that  
	\[
	\P_{\K}(T_{\K}^{c})\leq \frac{\epsilon^{2}}{(2N^{P}-3)\lVert \A \rVert^{2}}\text{\,,}
	\]
	and \(|T_{\K}| \leq e^{\frac{H(\P_{\K})}{a}}=e^{\ln (R)} = R\). Note that 
	\[
	\P_{\K}(T_{\K})\leq \sum\nolimits_{i = 1}^{R}\frac{\sigma_{i}^{2}}{\lVert \A \rVert^{2}}
	\text{\,.}
	\] 
	Since this holds for any subset of cardinality at most \(R\).
	Taking complements we obtain
	\[
	\sum\nolimits_{i = R+1}^{D_\K}\frac{\sigma_{i}^{2}}{\lVert \A \rVert^{2}}\leq \P_{\K}(T_{\K}^{c})
	\text{\,,}
	\]
	so 
	\[\sqrt{\sum\nolimits_{i=R+1}^{D_\K}\sigma_{\K,i}^{2}}\leq \frac{\epsilon}{\sqrt{(2N^{P}-3)}} \text{\,.}\]
	We can now invoke \cref{app:proofs:grasedyck} (note that if \(\axismap^{-1}(\K) \notin  \can^P\), then the requirements of \cref{app:proofs:grasedyck} are trivially fullfiled with respect to the partition \((\K,\K^{c})\) since \(n^{*}_{\K}=D_{\K}\)), which implies that there exists some \(\W \in HT\left( (n^{*}_{\K})_{\K \in \canone}\right)\) satisfying:
	\[\norm{ \W - \A } \leq \epsilon\text{\,,}\]
	as required.
	
	The proof concludes by establishing that for any tensor \(\W\in HT\left( (n^{*}_{\K})_{\K \in \canone}\right)\), there exists assignment for the tensors constituting the $P$-dimensional locally connected tensor network  (defined in~\cref{fig:tn_as_nn_pdim}) such that it generates $\W$.
	
	To see why this is the case, note that by~\cref{app:proofs:grasedyck_exact} any tensor \(\W \in HT \left ( (n^{*}_{\K})_{\K \in \canone} \right )\) can be represented by a (one-dimensional) locally connected tensor network with varying widths $(n^*_{\K})_{\K \in \canone}$, \ie~a tensor network conforming to a perfect binary tree graph in which the lengths of inner axes are as follows: an axis corresponding to an edge that connects a node with descendant leaves indexed by $\K$ to its parent is assigned the length $n^*_\K$.
	We can obtain an equivalent representation of any such tensor as a $P$-dimensional locally connected tensor network (described in~\cref{app:extension_dims:fit_tensor:tn_lc_nn}) via the following procedure.
	For each node at level $l \in \{0, P, 2 P, \ldots,  (L - 1) P \}$ of the tree (recall $N = 2^L$ for $L \in \N$), contract it with its descendant nodes at levels $\{ l + 1, \ldots, l + (P - 1)\}$.\footnote{
		For a concrete example, let $N = 2^L = 4$ and $P = 2$ (\ie~$L = 2$).
		In this case, the perfect binary tree underlying the one-dimensional locally connected tensor network of varying widths is of height $L \cdot P = 4$ and has $N^P = 16$ leaves.
		It is converted into a perfect $4$-ary tree tensor network of height $L = 2$ by contracting the root with its two children and the nodes at level two with their children.
	}
	This results in a new tensor network whose underlying graph is a perfect $2^P$-ary tree and the remaining edges all correspond to inner axes of lengths equal to $n^*_\K = R$ for $\K \in \can^P$, \ie~in a representation of $\W$ as a $P$-dimensional locally connected tensor network.
\qed
