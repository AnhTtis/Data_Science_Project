\section{Low Entanglement Under Canonical Partitions Is Necessary and Sufficient for Accurate Prediction}  \label{sec:accurate_predict}

This section considers the locally connected tensor network from~\cref{sec:fit_tensor:tn_lc_nn} in a machine learning setting. We show that attaining low population loss amounts to fitting a tensor defined by the data distribution, whose axes correspond to features (\cref{sec:accurate_predict:fit_data_tensor}).
Applying the theorems of~\cref{sec:fit_tensor:nec_and_suf}, we then conclude that the locally connected tensor network is capable of accurate prediction if and only if the data distribution admits low entanglement under canonical partitions of features (\cref{sec:accurate_predict:nec_and_suf}).
This conclusion is corroborated through experiments, demonstrating that the performance of common locally connected neural networks (including convolutional, recurrent, and local self-attention neural networks) is inversely correlated with the entanglement under canonical partitions of features in the data (\cref{sec:accurate_predict:emp_demo}).
For conciseness, the treatment in this section is limited to one-dimensional (sequential) models and data; see~\cref{app:extension_dims:accurate_predict} for an extension to arbitrary dimensions.



  % ACCURATE PREDICTION IS EQUIVALENT TO FITTING DATA TENSOR
\subsection{Accurate Prediction Is Equivalent to Fitting Data Tensor} \label{sec:accurate_predict:fit_data_tensor}

As discussed in~\cref{sec:fit_tensor:tn_lc_nn}, the locally connected tensor network generating $\tntensor \in \R^{D_1 \times \cdots \times D_N}$ is equivalent to a locally connected neural network, whose forward pass over a data instance $\brk{ \xbf^{(1)}, \ldots, \xbf^{(N)} }$ yields $\inprod{ \tenp_{n = 1}^N \xbf^{(n)} }{\tntensor}$, where $\xbf^{(1)} \in \R^{D_1}, \ldots, \xbf^{(N)} \in \R^{D_N}$.
Motivated by this fact, we consider a binary classification setting, in which the label $y$ of the instance $\brk{ \xbf^{(1)}, \ldots, \xbf^{(N)} }$ is either $1$~or~$-1$, and the prediction~$\hat{y}$ is taken to be the sign of the output of the neural network, \ie~$\hat{y} = \sign \brk*{ \inprod{  \tenp_{n = 1}^N \xbf^{(n)}  }{\tntensor} }$.

Suppose we are given a training set of labeled instances $\brk[c]1{ \brk1{ \brk{ \xbf^{(1,m)}, \ldots, \xbf^{(N,m)} } , y^{(m)} } }_{m = 1}^M$ drawn i.i.d. from some distribution, and we would like to learn the parameters of the neural network through the soft-margin support vector machine (SVM) objective, \ie~by optimizing:
\be
\min\nolimits_{\substack{\norm{\tntensor} \leq B}} \frac{1}{M} \sum\nolimits_{m = 1}^M  \max \brk[c]*{0, 1 - y^{(m)} \inprodbig{ \tenp_{n = 1}^N \xbf^{(n,m)} }{\tntensor} }
\text{\,,}
\label{eq:soft_svm}
\ee
for a predetermined constant $B > 0$.\footnote{
An alternative form of the soft-margin SVM objective includes a squared Euclidean norm penalty instead of the constraint.
}
We assume instances are normalized, \ie~the distribution is such that all vectors constituting an instance have norm no greater than one.  
We also assume that $B \leq 1$.
In this case $\abs1{y^{(m)} \inprodbig{ \tenp_{n = 1}^N \xbf^{(n,m)} }{\tntensor}} \leq 1$, so our optimization problem can be expressed as:
\[
\min\nolimits_{\substack{\norm{\tntensor} \leq B}} 1 - \inprod{\datatensor}{\tntensor}
\text{\,,}
\]
where
\be
\datatensor := \frac{1}{M} \sum\nolimits_{m = 1}^M y^{(m)} \cdot \tenp_{n = 1}^N \xbf^{(n, m)}
\label{eq:data_tensor}
\ee
is referred to as the \emph{empirical data tensor}.
This means that the accuracy over the training data is determined by how large the inner product $\inprod{\datatensor}{\tntensor}$ is.

Disregarding the degenerate case of $\datatensor = 0$ (\ie~that in which the optimized objective is constant), the inner products $\inprodbig{\datatensor}{\tntensor}$ and $\inprodbig{\frac{\datatensor}{\norm{\datatensor}}}{\tntensor}$ differ by only a multiplicative (positive) constant, so fitting the training data amounts to optimizing: 
\be
\max\nolimits_{ \norm{\tntensor} \leq B } \inprod{\frac{\datatensor}{\norm{\datatensor}}}{\tntensor}
\text{\,.}
\label{eq:maximize_in_prod_datatensor}
\ee
If $\tntensor$ can represent some $\W$, then it can also represent $c \cdot \W$ for every $c \in \R$.\footnote{
Since contraction is a multilinear operation, if a tensor network realizes $\W$, then multiplying any of the tensors constituting it by~$c$ results in $c \cdot \W$.
\label{note:multilinear_contraction}
} 
Thus, optimizing~\cref{eq:maximize_in_prod_datatensor} is the same as optimizing:
\[
\max\nolimits_{ \tntensor } \inprod{ \frac{\datatensor}{\norm{\datatensor} } }{ \frac{\tntensor}{\norm{\tntensor} } }
\text{\,,}
\]
and multiplying the result by $B$.
Fitting the training data therefore boils down to minimizing $\normbig{ \frac{\tntensor}{ \norm{\tntensor} } - \frac{ \datatensor }{ \norm{\datatensor} } }$.
In other words, the accuracy achievable over the training data is determined by the extent to which $\frac{ \tntensor }{ \norm{\tntensor} }$ can fit the normalized empirical data tensor~\smash{$\frac{ \datatensor }{ \norm{\datatensor} }$}.

The arguments above are independent of the training set size, and in fact apply to the population loss as well, in which case $\datatensor$ is replaced by the \emph{population data tensor}:
\be
\popdatatensor := \EE\nolimits_{\brk{ \xbf^{(1)}, \ldots, \xbf^{(N)} } , y} \brk[s]1{ y \cdot \tenp_{n = 1}^N \xbf^{(n)} }
\text{\,.}
\label{eq:pop_data_tensor}
\ee
Disregarding the degenerate case of $\popdatatensor = 0$ (\ie~that in which the population loss is constant), it follows that the achievable accuracy over the population is determined by the extent to which  $\frac{ \tntensor }{ \norm{\tntensor} }$ can fit the normalized population data tensor \smash{$\frac{ \popdatatensor }{ \norm{\popdatatensor} }$}.
We refer to the minimal distance from it as the \emph{suboptimality in achievable accuracy}.

\begin{definition}
\label{def:supopt}
In the context of the classification setting above, the \emph{suboptimality in achievable accuracy} is:
\[
\subopt := \min\nolimits_{\tntensor} \norm*{ \frac{\tntensor}{ \norm{\tntensor} } -  \frac{ \popdatatensor }{ \norm{\popdatatensor} }  }
\text{\,.}
\]
\end{definition}


  % NECESSARY AND SUFFICIENT CONDITION FOR ACCURATE PREDICTION
\subsection{Necessary and Sufficient Condition for Accurate Prediction}\label{sec:accurate_predict:nec_and_suf}

In the classification setting of~\cref{sec:accurate_predict:fit_data_tensor}, by invoking~\cref{thm:fit_necessary,thm:fit_sufficient} from~\cref{sec:fit_tensor:nec_and_suf}, we conclude that the suboptimality in achievable accuracy is small if and only if the population data tensor $\popdatatensor$ admits low entanglement under the canonical partitions of its axes (\cref{def:canonical_partitions}).

\begin{corollary}
	\label{cor:acc_pred_nec_and_suf}
	Consider the classification setting of~\cref{sec:accurate_predict:fit_data_tensor}, and let $\epsilon \in [0, 1/4]$.
	If there exists a canonical partition $\brk{ \K, \K^c} \in \can$ (\cref{def:canonical_partitions}) under which $\qe{\popdatatensor}{\K} > \ln (R) + 2\epsilon \cdot \ln ( D_\K )+ 2 \sqrt{2 \epsilon}$, where $D_{\K} := \min \brk[c]{ \prod_{n \in \K} D_n , \prod_{n \in \K^c} D_n }$, then:
	\[
		\subopt > \epsilon
	\text{\,.}
	\]
	Conversely, if for all $\brk{ \K, \K^c} \in \can$ it holds that $\qe{\popdatatensor}{\K} \leq \frac{\epsilon^{2}}{8 N -12} \cdot \ln (R)$,\footnote{
		Per the discussion in~\cref{note:suff_cond}, when $\epsilon$ tends to zero: \emph{(i)} in the absence of any knowledge regarding $\popdatatensor$, the entanglements required by the sufficient condition unavoidably approach zero; while \emph{(ii)} if it is known that for all $(\K, \K^c) \in \can$ the singular values of $\mat{\popdatatensor}{\K}$ trailing after the $R$'th one are small, then the entanglements in the sufficient condition can be on the order of $\ln (R)$ (as they are in the necessary condition).
	}
	then:
	\[
		\subopt \leq \epsilon
		\text{\,.}
	\]
\end{corollary}

\begin{proof}[Proof sketch (proof in~\cref{app:proofs:cor:acc_pred_nec_and_suf})]
Follows from~\cref{thm:fit_necessary,thm:fit_sufficient} after accounting for the normalization of $\tntensor$ in the suboptimality in achievable accuracy.
\end{proof}

Directly evaluating the conditions required by~\cref{cor:acc_pred_nec_and_suf}~---~low entanglement under canonical partitions for $\popdatatensor$~---~is impractical, since: \emph{(i)} $\popdatatensor$ is defined via an unknown data distribution (\cref{eq:pop_data_tensor}); and \emph{(ii)} computing the entanglements involves taking singular value decompositions of matrices with size exponential in the number of input variables $N$.
Fortunately, as~\cref{prop:data_tensor_concentration} below shows, $\popdatatensor$ is with high probability well-approximated by the empirical data tensor $\datatensor$.
Moreover, the entanglement of $\datatensor$ under any partition can be computed efficiently, without explicitly storing or manipulating an exponentially large matrix~---~see~\cref{app:ent_comp} for an algorithm (originally proposed in~\cite{martyn2020entanglement}).
Overall, we obtain an efficiently computable criterion (low entanglement under canonical partitions for $\datatensor$), that with high probability is both necessary and sufficient for low suboptimality in achievable accuracy~---~see~\cref{cor:eff_acc_pred_nec_and_suf} below.

\begin{proposition}
	\label{prop:data_tensor_concentration}
	Consider the classification setting of~\cref{sec:accurate_predict:fit_data_tensor}, and let $\delta \in (0, 1)$ and $\gamma > 0$.
	If the training set size $M$ satisfies $M \geq \frac{2\ln(\frac{2}{\delta})}{\norm{\popdatatensor}^{2}\gamma^{2}}$, then with probability at least $1 - \delta$:
	\[
		\norm*{\frac{\popdatatensor}{\norm{\popdatatensor}} - \frac{\datatensor}{\norm{\datatensor}}} \leq \gamma
	\]
\end{proposition}

\begin{proof}[Proof sketch (proof in~\cref{app:proofs:data_tensor_concentration})]
A standard generalization of the Hoeffding inequality to random vectors in a Hilbert space yields a high probability bound on $\norm{\datatensor - \popdatatensor}$, which is then translated to a bound on the normalized tensors.
\end{proof}

\begin{corollary}
	\label{cor:eff_acc_pred_nec_and_suf}
	Consider the setting and notation of~\cref{cor:acc_pred_nec_and_suf}, with $\epsilon \in (0, 1/6]$.
	For $\delta \in (0, 1)$, suppose that the training set size $M$ satisfies $M \geq \frac{8\ln(\frac{2}{\delta})}{\norm{\popdatatensor}^{2}\epsilon^{2}}$.
	Then, with probability at least $1 - \delta$ the following hold.
	First, if there exists a canonical partition $\brk{ \K, \K^c} \in \can$ (\cref{def:canonical_partitions}) under which $\qe{\datatensor}{\K} > \ln (R) + 3 \epsilon \cdot \ln (D_\K) + 2 \sqrt{ 3 \epsilon }$, then:
	\[
	\subopt > \epsilon
	\text{\,.}
	\]
	Second, if for all $\brk{ \K, \K^c} \in \can$ it holds that $\qe{\datatensor}{\K} \leq \frac{\epsilon^{2}}{32 N - 48} \cdot \ln (R)$, then:
	\[
	\subopt \leq \epsilon
	\text{\,.}
	\]
	Moreover, the conditions above on the entanglements of $\datatensor$ can be evaluated efficiently (in $\OO (D N M^2 + N M^3)$ time $\OO (D N M + M^2)$ and memory, where $D := \max_{n \in [N]} D_n$).
\end{corollary}

\begin{proof}[Proof sketch (proof in~\cref{app:proofs:cor:eff_acc_pred_nec_and_suf})]
Implied by~\cref{cor:acc_pred_nec_and_suf},~\cref{prop:data_tensor_concentration} with $\gamma = \frac{\epsilon}{2}$ and~\cref{alg:ent_comp} in~\cref{app:ent_comp}.
\end{proof}



  % EMPIRICAL DEMONSTRATION
\subsection{Empirical Demonstration}
\label{sec:accurate_predict:emp_demo}

\cref{cor:eff_acc_pred_nec_and_suf} establishes that, with high probability, the locally connected tensor network (from~\cref{sec:fit_tensor:tn_lc_nn}) can achieve high prediction accuracy if and only if the empirical data tensor (\cref{eq:data_tensor}) admits low entanglement under canonical partitions of its axes.
We corroborate our formal analysis through experiments, demonstrating that its conclusions carry over to common locally connected architectures.
Namely, applying convolutional neural networks, S4 (a popular recurrent neural network; see~\citep{gu2022efficiently}), and a local self-attention model~\citep{rae-razavi-2020-transformers} to different datasets, we show that the achieved test accuracy is inversely correlated with the entanglements of the empirical data tensor under canonical partitions.
Below is a description of experiments with one-dimensional (\ie~sequential) models and data.
Additional experiments with two-dimensional (imagery) models and data are given in~\cref{app:extension_dims:accurate_predict:emp_demo}.

Discerning the relation between entanglements of the empirical data tensor and performance (prediction accuracy) of locally connected neural networks requires datasets admitting different entanglements.
A potential way to acquire such datasets is as follows.
First, select a dataset on which locally connected neural networks perform well, in the hopes that it admits low entanglement under canonical partitions; natural candidates are datasets comprising images, text or audio.
Subsequently, create â€œshuffled'' variants of the dataset by repeatedly swapping the position of two features chosen at random.\footnote{
It is known that as the number of random position swaps goes to infinity, the arrangement of the features converges to a random permutation~\citep{diaconis1981generating}.
}
This erodes the original arrangement of features in the data, and is expected to yield higher entanglement under canonical partitions.

We followed the blueprint above for a binary classification version of the Speech Commands audio dataset~\citep{warden2018speech}.
\cref{fig:entanglement_inv_corr_acc} presents test accuracies achieved by a convolutional neural network, S4, and a local self-attention model, as well as average entanglement under canonical partitions of the empirical data tensor, against the number of random feature swaps performed to create the dataset.
As expected, when the number of swaps increases, the average entanglement under canonical partitions becomes higher.
At the same time, in accordance with our theory, the prediction accuracies of the locally connected neural networks substantially deteriorate, showing an inverse correlation with the entanglement under canonical partitions.

\begin{figure}[t]
	\vspace{0mm}
	\begin{center}
		\hspace{0mm}
		\includegraphics[width=1\textwidth]{figs/entanglement_inv_corr_acc.pdf}
	\end{center}
	\vspace{-2.5mm}
	\caption{
		The prediction accuracies of common locally connected neural networks are inversely correlated with the entanglements of the data under canonical partitions of features, in compliance with our theory (\cref{sec:accurate_predict:fit_data_tensor,sec:accurate_predict:nec_and_suf}).
		\textbf{Left:} Average entanglement under canonical partitions (\cref{def:canonical_partitions}) of the empirical data tensor (\cref{eq:data_tensor}), for binary classification variants of the Speech Commands audio dataset~\citep{warden2018speech} obtained by performing random position swaps between features.
		\textbf{Right:} Test accuracies achieved by a convolutional neural network (CNN)~\citep{dai2017very}, S4 (a popular class of recurrent neural networks; see~\citep{gu2022efficiently}), and a local self-attention model~\citep{rae-razavi-2020-transformers}, against the number of random feature swaps performed to create the dataset.
%		For computational reasons, the entanglements are computed based on subsets of $500$ training instances, and their average includes only entanglements under canonical partitions whose sides contain at least $3.125\%$ of the features (the number of canonical partitions grows exponentially as the minimal size of a partition's side becomes smaller).
		\textbf{All:} Reported are the means and standard deviations of the quantities specified above, taken over ten different random seeds.
		See \cref{app:extension_dims:accurate_predict:emp_demo} for experiments over (two-dimensional) image data and \cref{app:experiments:details} for further implementation details.
	}
	\label{fig:entanglement_inv_corr_acc}
\end{figure}
