\section{Further Experiments and Implementation Details}
\label{app:experiments}

% FURTHER EXPERIMENTS
\subsection{Further Experiments}
\label{app:experiments:further}

\begin{table}[t]
	\caption{
				\cref{alg:ent_struct_search} can be efficiently applied to data with a large number of features.
				Reported are the results of an experiment identical to that of~\cref{tab:audio_moderate_dim}, but over a version of the Speech Commands~\citep{warden2018speech} dataset in which every instance has 50,000 features.
				Instances of the minimum balanced cut problem solved as part of~\cref{alg:ent_struct_search} entail graphs with up to $25 \cdot 10^8$ edges.
				They are solved using the well-known edge sparsification algorithm of~\citep{spielman2011spectral} that preserves weights of cuts, allowing for configurable compute and memory consumption (the more resources are consumed, the more accurate the solution will be).
				As can be seen,~\cref{alg:ent_struct_search} leads to significant improvements in prediction accuracies.
				See~\cref{app:experiments:details} for further implementation details.
		}
	\begin{center}
		\small
		\vspace{1mm}
		\begin{tabular}{lcc}
			\toprule
			& Randomly Permuted & \cref{alg:ent_struct_search} \\
			\midrule
			CNN & ${15.0}~\pm$ \scriptsize{${1.6}$} & ${\mathbf{65.6}}~\pm$ \scriptsize{${1.1}$}  \\
			S4 & ${18.2}~\pm$ \scriptsize{${0.5}$} &  $\mathbf{82.2}~\pm$ \scriptsize{${0.4}$}  \\
			\bottomrule
		\end{tabular}	
	\end{center}
	\vspace{-2mm}
	\label{tab:audio_high_dim}
\end{table}

\cref{tab:audio_high_dim} supplements the experiment of~\cref{tab:audio_moderate_dim} from~\cref{sec:enhancing:exp:perm_audio}, demonstrating that~\cref{alg:ent_struct_search} can be applied to data with a large number of features (\eg~with tens of thousands of features).
In the experiment, instances of the minimum balanced cut problem solved as part of~\cref{alg:ent_struct_search} entail graphs with up to $25 \cdot 10^8$ edges.
They are solved using the well-known edge sparsification algorithm of~\citep{spielman2011spectral} that preserves weights of cuts, allowing for configurable compute and memory consumption (the more resources are consumed, the more accurate the solution will be).
In contrast, this approach is not directly applicable for improving the efficiency of IGTD.

% IMPLEMENTATION DETAILS
\subsection{Further Implementation Details}
\label{app:experiments:details}

We provide implementation details omitted from our experimental reports (\cref{sec:accurate_predict:emp_demo},~\cref{sec:enhancing},~\cref{app:extension_dims:accurate_predict:emp_demo},~\cref{app:extension_dims:enhancing:exp} and~\cref{app:experiments:further}).
We provide implementation details omitted from our experimental reports (\cref{sec:accurate_predict:emp_demo},~\cref{sec:enhancing} and~\cref{app:extension_dims:accurate_predict:emp_demo}).
Source code for reproducing our results and figures, based on the PyTorch~\citep{paszke2017automatic} framework,\ifdefined\CAMREADY
~can be found at \url{https://github.com/nmd95/data_suitable_lc_nn_code}.
\else
~is attached as supplementary material and will be made publicly available.
\fi
All experiments were run on a single Nvidia RTX A6000 GPU.


\subsection{Empirical Demonstration of Theoretical Analysis (\cref{fig:entanglement_inv_corr_acc,fig:surr_ent_corr,fig:entanglement_inv_corr_acc_images})} \label{app:experiments:details:emp_demo}


\subsubsection{\cref{fig:entanglement_inv_corr_acc,fig:surr_ent_corr}}
\label{app:experiments:details:emp_demo:audio}

\textbf{Dataset}: The SpeechCommands dataset \citep{warden2018speech} contains raw audio segments of length up to 16000, split into 84843 train and 11005 test segments.
We zero-padded all audio segments to have a length of 16000 and resampled them using sinc interpolation (default PyTorch implementation). 
We allocated 11005 audio segments from the train set for validation, \ie~the dataset was split into 73838 train, 11005 validation and 11005 test audio segments, and created a binary one-vs-all classification version of SpeechCommands by taking all audio segments labeled by the class “33'' (corresponding to the word “yes''), and sampling an equal amount of segments from the remaining classes (this process was done separately for the train, validation and test sets). 
The resulting balanced classification dataset had 5610 train, 846 validation and 838 test segments.
Lastly, we resampled all audio segments in the dataset from 16000 to 4096 features using sinc interpolation.

\textbf{Random feature swaps:} Starting with the original order of features, we created increasingly “shuffled'' versions of the dataset by randomly swapping the position of features. For each number of random position swaps $k \in \{0, 250, \ldots, 4000 \}$, we created ten datasets, whose features were subject to $k$ random position swaps between features, using different random seeds.

\textbf{Quantum entanglement measurement}: Each reported value in the plot is the average of entanglements with respect to canonical partitions (\cref{def:canonical_partitions}) corresponding to levels \(l  =  1, 2, 3, 4, 5, 6\).
We used~\cref{alg:ent_comp} described in \cref{app:ent_comp} on two mini-batches of size 500, randomly sampled from the train set.
As customary (\cf~\cite{stoudenmire2016supervised}), every input feature $x$ was embedded using the following sine-cosine scheme:
\[
\phi(x) := ( \sin(\pi \theta x) , \cos(\pi \theta x)) \in \R^2
\text{\,,}
\]
with $\theta = 0.085$.

\textbf{Neural network architectures}:
\begin{itemize}
	\item \textbf{CNN}: An adaptation of the M5 architecture from~\citep{dai2017very}, which is designed for audio data.
	Our implementation is based on~\url{https://github.com/danielajisafe/Audio_WaveForm_Paper_Implementation}.
	
	\item \textbf{S4}: Official implementation of~\citep{gu2022efficiently} with a hidden dimension of 128 and 4 layers.
	
	\item \textbf{Local-Attention}: Adaptation of the local-attention model from~\cite{rae-razavi-2020-transformers}, as implemented in \url{https://github.com/lucidrains/local-attention}.
	We use a multi-layer perceptron (MLP) for mapping continuous (raw) inputs to embeddings, which are fed as input the the local-attention model.
	For classification, we collapse the spatial dimension of the network's output using mean-pooling and pass the result into an MLP classification head.
	The network had attention dimension 128 (with 2 heads of dimension 64), depth 8, hidden dimension of MLP blocks 341 (computed automatically by the library based on the attention dimension) and local-attention window size 10.
\end{itemize}

\textbf{Training and evaluation:} 
The binary cross-entropy loss was minimized via the Adam optimizer~\citep{kingma2014adam} with default $\beta_1, \beta_2$ coefficients.
Batch sizes were chosen to be the largest powers of two that fit in the GPU memory.
The duration of optimization was 300 epochs for all models (number of epochs was taken large enough such that the train loss plateaued).
After the last training epoch, the model which performed best on the validation set was chosen, and its average test accuracy is reported.
Additional optimization hyperparameters are provided in~\cref{tab:1d_audio_optimization_hyperparam}.
We note that for S4~\cite{gu2022efficiently}, in accordance with its official implementation, we use a cosine annealing learning rate scheduler.

\begin{table}[H]
	\caption{
		Optimization hyperparameters for the experiments of~\cref{fig:entanglement_inv_corr_acc}.
	}
	\label{tab:1d_audio_optimization_hyperparam}
	\begin{center}
		\begin{small}
			\begin{tabular}{lcccc}
				\toprule
				\textbf{Model} & \textbf{Optimizer} & \textbf{Learning Rate} & \textbf{Weight Decay} & \textbf{Batch Size} \\
				\midrule
				CNN & Adam & 0.001 & 0.0001   & 128  \\
				S4              & AdamW             & 0.001                 & 0.01                 & 64 \\
				Local-Attention & Adam              & 0.0005                & 0                              & 32            \\				
				\bottomrule
			\end{tabular}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}

\subsubsection{\cref{fig:entanglement_inv_corr_acc_images}}
\label{app:experiments:details:emp_demo:image}

\textbf{Dataset}: We created one-vs-all binary classification datasets based on CIFAR10~\citep{krizhevsky2009learning} as follows.
All images were converted to grayscale using the PyTorch default implementation. 
Then, we allocated 7469 images from the train set for validation, \ie~the dataset was split into 42531 train, 7469 validation and 1000 test images.
We took all images labeled by the class “0'' (corresponding to images of airplanes), and uniformly sampled an equal amount of images from the remaining classes (this process was done separately for the train, validation and test sets).
The resulting balanced classification dataset had 8506 train, 1493 validation and 2001 test images.

\textbf{Random feature swaps:} We created increasingly “shuffled'' versions of the dataset according to the protocal described in~\cref{app:experiments:details:emp_demo:audio}.

\textbf{Quantum entanglement measurement}: Each reported value in the plot is the average entanglements with respect to canonical partitions (\cref{def:canonical_partitions_pdim}) corresponding to levels $l = 1, 2$.

\textbf{Neural network architectures}:
\begin{itemize}
	\item \textbf{CNN}: Same architecture used for the experiments of~\cref{tab:tabular_datasets}, but with one-dimensional convolutional layers replaced with two-dimensional convolutional layers. 
	See~\cref{tab:2D_resnet_hyperparam} for the exact architectural hyperparameters.
\end{itemize}

\begin{table}[H]
	\caption{
		Architectural hyperparameters for the convolutional neural network used in the experiments of \cref{fig:entanglement_inv_corr_acc_images} and~\cref{tab:image_rand_perm}.
	}
	\label{tab:2D_resnet_hyperparam}
	\begin{center}
		\begin{small}
			\begin{tabular}{ll}
				\toprule
				\textbf{Hyperparameter} & \textbf{Value} \\
				\midrule
				Stride & (3, 3)\\
				Kernel size & (3, 3) \\
				Pooling window size & (3, 3) \\
				Number of blocks & 8 \\		
				Hidden dimension & 32 \\		
				\bottomrule
			\end{tabular}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}

\textbf{Training and evaluation}: The binary cross-entropy loss was minimized for 150 epochs via the Adam optimizer~\citep{kingma2014adam} with default $\beta_1, \beta_2$ coefficients (number of epochs was taken large enough such that the train loss plateaued).
Batch size was chosen to be the largest power of two that fit in the GPU memory.
After the last training epoch, the model which performed best on the validation set was chosen, and its average test accuracy is reported.
Additional optimization hyperparameters are provided in~\cref{tab:2d_cnn_optimization_hyperparam}.

\begin{table}[H]
	\caption{
		Optimization hyperparameters for the experiments of~\cref{fig:entanglement_inv_corr_acc_images}.
	}
	\label{tab:2d_cnn_optimization_hyperparam}
	\begin{center}
		\begin{small}
			\begin{tabular}{lcccc}
				\toprule
				\textbf{Model} & \textbf{Optimizer} & \textbf{Learning Rate} & \textbf{Weight Decay} & \textbf{Batch Size} \\
				\midrule
				CNN         & Adam              & 0.001                 & 0.0001               & 128           \\
				\bottomrule
			\end{tabular}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}


\subsection{Enhancing Suitability of Data to Locally Connected Neural Networks (\cref{tab:audio_moderate_dim,tab:tabular_datasets,tab:image_rand_perm,tab:audio_high_dim})} 
\label{app:experiments:details:enhancing}

\subsubsection{Features Rearrangement Algorithms}

\cref{alg:ent_struct_search},~\ref{alg:ent_struct_search_pdim} and IGTD~\cite{zhu2021converting} were applied to the training set.
Subsequently, the learned feature rearrangement was used for both the validation and test data. 
In instances where three-way cross-validation was used, distinct rearrangements were learned for each separately.

\textbf{\cref{alg:ent_struct_search} and \cref{alg:ent_struct_search_pdim}}:
Approximate solutions to the minimum balanced cut problems were obtained using the METIS graph partitioning algorithm~\citep{karypis1998fast}, as implemented in~\url{https://github.com/networkx/networkx-metis}.

\textbf{IGTD~\cite{zhu2021converting}}:
% In all applications of IGTD, we use the following hyperparameters \(\text{S max}=1000,\text{S con}=20, \text{t con}=0.0, \text{ t swap}=0.0\).  
The original IGTD implementation supports rearranging data only into two-dimensional images.
We adapted its implementation to support one-dimensional (sequential) data for the experiments of~\cref{tab:audio_moderate_dim,tab:tabular_datasets}.


\subsubsection{Randomly Permuted Audio Datasets (\cref{tab:audio_moderate_dim})} \label{app:experiments:details:enhancing:perm_audio}

\textbf{Dataset}: To facilitate efficient experimentation, we downsampled all audio segments in SpeechCommands to have 2048 features.
Furthermore, for the train, validation and test sets separately, we used 20\% of the audio segments available for each class.

\textbf{Neural network architectures}:

\begin{itemize}	
	\item \textbf{CNN}: Same architecture used for the experiments of~\cref{fig:entanglement_inv_corr_acc} (see~\cref{app:experiments:details:emp_demo:audio}).
	
	\item \textbf{S4}: Same architecture used for the experiments of~\cref{fig:entanglement_inv_corr_acc} (see~\cref{app:experiments:details:emp_demo:audio}).
	
	\item \textbf{Local-Attention}: Same architecture used in the experiments of~\cref{fig:entanglement_inv_corr_acc}, but with a depth 4 network (we reduced the depth to allow for more efficient experimentation).
\end{itemize}

\textbf{Training and evaluation:} 
The cross-entropy loss was minimized via the Adam optimizer~\citep{kingma2014adam} with default $\beta_1, \beta_2$ coefficients. 
Batch sizes were chosen to be the largest powers of two that fit in the GPU memory.
The duration of optimization was 200, 200 and 450 epochs for the CNN, S4 and Local-Attention models, respectively (number of epochs was taken large enough such that the train loss plateaued).
After the last training epoch, the model which performed best on the validation set was chosen, and its average test accuracy is reported.
Additional optimization hyperparameters are provided in~\cref{tab:1d_audio_perm_optimization_hyperparam}.
We note that for S4~\cite{gu2022efficiently}, in accordance with its official implementation, we use a cosine annealing learning rate scheduler.

\begin{table}[H]
	\caption{
		Optimization hyperparameters for the experiments of~\cref{tab:audio_moderate_dim}.
	}
	\label{tab:1d_audio_perm_optimization_hyperparam}
	\begin{center}
		\begin{small}
			\begin{tabular}{lcccc}
				\toprule
				\textbf{Model} & \textbf{Optimizer} & \textbf{Learning Rate} & \textbf{Weight Decay} & \textbf{Batch Size} \\ \midrule
				CNN        & Adam              & 0.001                 & 0.0001                          & 128           \\
				S4              & AdamW             & 0.001                 & 0.01             & 64 \\ 
				Local-Attention & Adam              & 0.0001                & 0                               & 32            \\
				\bottomrule
			\end{tabular}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}


\subsubsection{Tabular Datasets (\cref{tab:tabular_datasets})} \label{app:experiments:details:enhancing:tab}

\textbf{Datasets}: The datasets "dna", "semeion" and "isolet" are all from the OpenML repository~\citep{OpenML2013}. 
For each dataset we split the samples intro three folds, which were used for evaluation according to a standard three-way cross-validation protocol.
That is, for each of the three folds, we used one third of the data as a test set and the remaining for train and validation.
One third of the samples in the remaining folds (not used for testing) were allocated for the validation set.

\textbf{Neural network architectures}: 

\begin{itemize}
	\item \textbf{CNN}: We used a ResNet adapted for tabular data. 
	It consisted of residual blocks of the following form:
	\[
	\text{Block} (\xbf) = \text{dropout} ( \xbf + \text{BN} ( \text{maxpool} ( \text{ReLU} ( \text{conv} (\xbf)) ) ) )
	\text{\,.}
	\]
	After applying a predetermined amount of residual blocks, a global average pooling and fully connected layers were used to output the prediction.
	The architectural hyperparameters are specified in~\cref{tab:resnet_tabular_hyperparams}.
	
	\item \textbf{S4}: Same architecture used for the experiments of~\cref{fig:entanglement_inv_corr_acc} (see~\cref{app:experiments:details:emp_demo:audio}), but with a hidden dimension of 64.
	
	\item \textbf{Local-Attention}: Same architecture used for the experiments of~\cref{fig:entanglement_inv_corr_acc} (see~\cref{app:experiments:details:emp_demo:audio}), but with 4 attention heads of dimension 32 and a local-attention window size of 25.
\end{itemize}

\begin{table}[H]
	\caption{
		Architectural hyperparameters for the convolutional neural network used in the experiments of~\cref{tab:tabular_datasets,tab:audio_high_dim}.
	}
	\label{tab:resnet_tabular_hyperparams}
	\begin{center}
		\begin{small}
			\begin{tabular}{ll}
				\toprule
				\textbf{Hyperparameter} & \textbf{Value} \\
				\midrule
				Stride & 3 \\
				Kernel size & 3 \\
				Pooling window size & 3 \\
				Number of blocks & 8 \\		
				Hidden dimension & 32 \\		
				\bottomrule
			\end{tabular}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}

\textbf{Training and evaluation}: The cross-entropy loss was minimized for 300 epochs via the Adam optimizer~\citep{kingma2014adam} with default $\beta_1, \beta_2$ coefficients (number of epochs was taken large enough such that the train loss plateaued).
Batch sizes were chosen to be the largest powers of two that fit in the GPU memory.
After the last training epoch, the model which performed best according to the validation sets was chosen, and test accuracy was measured on the test set.
The reported accuracy is the average over the three folds.
Additional optimization hyperparameters are specified in~\cref{tab:training_parameters_all_models}.
We note that for S4~\cite{gu2022efficiently}, in accordance with its official implementation, we use a cosine annealing learning rate scheduler.

\begin{table}[H]
	\caption{
		Optimization hyperparameters for the experiments of~\cref{tab:tabular_datasets}.
	}
	\label{tab:training_parameters_all_models}
	\begin{center}
		\begin{small}
			\begin{tabular}{lcccc}
				\toprule
				\textbf{Model} & \textbf{Optimizer} & \textbf{Learning Rate} & \textbf{Weight Decay}  & \textbf{Batch Size} \\
				\midrule
				CNN         & Adam              & 0.001                 & 0.0001              &  64           \\
				S4              & AdamW             & 0.001                 & 0.01                & 64 \\
				Local-Attention & Adam              & 0.00005                & 0                   & 64            \\
				\bottomrule
			\end{tabular}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}




\subsubsection{Randomly Permuted Image Datasets (\cref{tab:image_rand_perm})}

\textbf{Dataset}: The data acquisition process followed the protocol described in~\cref{app:experiments:details:emp_demo:image}, except that the data was not converted into a binary one-vs-all classification dataset.

\textbf{Neural network architectures}:
\begin{itemize}
	\item \textbf{CNN}: Same architecture used for the experiments of~\cref{fig:entanglement_inv_corr_acc_images}.
	
\end{itemize}


\textbf{Training and evaluation}: The cross-entropy loss was minimized for 500 epochs via the Adam optimizer~\citep{kingma2014adam} with default $\beta_1, \beta_2$ coefficients (number of epochs was taken large enough such that the train loss plateaued).
Batch size was chosen to be the largest power of two that fit in the GPU memory.
After the last training epoch, the model which performed best on the validation set was chosen, and its average test accuracy is reported.
Additional optimization hyperparameters are provided in~\cref{tab:2d_image_optimization_hyperparam}.


\begin{table}[H]
	\caption{
		Optimization hyperparameters for the experiments of~\cref{tab:image_rand_perm}.
	}
	\label{tab:2d_image_optimization_hyperparam}
	\begin{center}
		\begin{small}
			\begin{tabular}{lcccc}
				\toprule
				\textbf{Model} & \textbf{Optimizer} & \textbf{Learning Rate} & \textbf{Weight Decay} & \textbf{Batch Size} \\
				\midrule
				CNN         & Adam              &  \begin{tabular}{@{}c@{}} 0.001 \\ (multiplied by 0.1 after 300 epochs) \end{tabular}        & 0.0001              &    128           \\			
				\bottomrule
			\end{tabular}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}


\subsubsection{Randomly Permuted Audio Datasets With a Large Number of Features (\cref{tab:audio_high_dim})}

\noindent\textbf{Dataset}: The data acquisition process followed the protocol described in~\cref{app:experiments:details:emp_demo:audio}, except that the data was not transformed into a binary one-vs-all classification dataset and the audio segments were upsampled from 16,000 to 50,000.

\textbf{Edge Sparsification}: To facilitate running the METIS graph partitioning algorithm over the minimum balanced cut problems encountered as part of~\cref{alg:ent_struct_search}, we first removed edges from the graph using the spectral sparsification algorithm of~\cite{spielman2011spectral}.
Specifically, we used the official Julia implementation (\url{https://github.com/danspielman/Laplacians.jl}) with hyperparameter~$\epsilon = 0.15$.

\textbf{Neural network architectures}:
\begin{itemize}
	\item \textbf{CNN}: Same architecture used for the experiments of~\cref{tab:tabular_datasets} (see~\cref{app:experiments:details:enhancing:tab}).
	
	\item \textbf{S4}: Same architecture used for the experiments of~\cref{tab:audio_moderate_dim} (see~\cref{app:experiments:details:emp_demo:audio}), but with a hidden dimension of 32 (we reduced the hidden dimension due to GPU memory considerations).
\end{itemize}


\textbf{Training and evaluation}: The cross-entropy loss was minimized for 200 epochs via the Adam optimizer~\citep{kingma2014adam} with default $\beta_1, \beta_2$ coefficients (number of epochs was taken large enough such that the train loss plateaued).
Batch sizes were chosen to be the largest powers of two that fit in the GPU memory.
After the last training epoch, the model which performed best on the validation set was chosen, and its average test accuracy is reported.
Additional optimization hyperparameters are provided in~\cref{tab:audio_high_dim_optimization_hyperparam}.
We note that for S4~\cite{gu2022efficiently}, in accordance with its official implementation, we use a cosine annealing learning rate scheduler.

\begin{table}[H]
	\caption{
		Optimization hyperparameters for the experiments of~\cref{tab:audio_high_dim}.
	}
	\label{tab:audio_high_dim_optimization_hyperparam}
	\begin{center}
		\begin{small}
			\begin{tabular}{lcccc}
				\toprule
				\textbf{Model} & \textbf{Optimizer} & \textbf{Learning Rate} & \textbf{Weight Decay} & \textbf{Batch Size} \\
				\midrule
				CNN         & Adam              & 0.001                 & 0.0001              & 64           \\
				S4         & AdamW              & 0.001                 & 0.01                   & 64           \\
				\bottomrule
			\end{tabular}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}