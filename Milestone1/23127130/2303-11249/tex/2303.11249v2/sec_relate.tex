\section{Related Work}  \label{sec:relate}

Characterizing formal properties of data distributions that make them suitable for neural networks is a major open problem in deep learning. 
A number of papers provide sufficient conditions on a data distribution that allow it to be learnable by some neural network architecture~\citep{brutzkus2017globally,pmlr-v162-brutzkus22a,malach2018provably,du2018improved,du2018gradient,oymak2018end}. 
However, the conditions under which learnability is proven are  restrictive, and are not argued to be necessary. 
There have also been attempts to quantify the structure of data via quantum entanglement and mutual information~\citep{martyn2020entanglement,convy2022mutual,lu2021tensor,cheng2018information,zhang2017entanglement,jia2020entanglement}, yet without formally relating properties of the data to the prediction accuracy achievable by neural networks.
To the best of our knowledge, this paper is the first to derive a necessary and sufficient condition on the data distribution for accurate prediction by locally connected neural networks. 

Our work follows a long line of research employing tensor networks as theoretical models for studying deep learning.
These include works analyzing the expressiveness of different neural network architectures~\citep{cohen2016expressive,sharir2016tensorial,cohen2016convolutional,cohen2017inductive,sharir2018expressive,cohen2018boosting,levine2018benefits,balda2018tensor,levine2018deep,khrulkov2018expressive,levine2019quantum,khrulkov2019generalized,razin2022ability}, their generalization properties~\citep{li2020understanding}, and the implicit regularization induced by optimization~\citep{razin2020implicit,razin2021implicit,razin2022implicit,wang2020beyond,ge2021understanding}. 
We focus on expressiveness, yet our results differ from the aforementioned works in that we incorporate the data distribution into our analysis and tackle the question of what makes data suitable for deep learning.

The algorithm we propose for enhancing the suitability of data to locally connected neural networks can be considered a form of representation learning. 
Representation learning is a vast field, far too broad for us to survey here (for an overview see \citep{nozawa2022empirical}). most representation learning methods are concerned with dimensionality reduction, i.e the discovery of low dimensional structure in high dimensional data, e.g \citep{ballard1987modular,kingma2013auto}. 
Such methods are complementary to our approach, which preserves the dimensionality of the input, and seeks to learn a rearrangement of features that is better suited to locally connected neural networks. 
A notable work of this type is  IGTD~\citep{zhu2021converting}, which arranges features in a dataset to improve its suitability for convolutional neural networks. 
In contrast to IGTD, our method is theoretically grounded, and as demonstrated empirically in~\cref{sec:enhancing}, it leads to higher improvements in prediction accuracy for locally connected neural networks.