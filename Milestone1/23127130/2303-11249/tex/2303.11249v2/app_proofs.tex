\section{Deferred Proofs}  \label{app:proofs}
\subsection{Useful Lemmas}
Below we collect a few useful results, which we will use in our proofs.

\begin{lemma} \label{app:proofs:perturb}
		We denote the vector of singular values of a matrix \(\Xbf\) (arranged in decreasing order) by \(S(\mathbf{X})\). 
		For any $\Abf, \Bbf \in \R^{D_1 \times D_2}$ it holds that:
		\[
		\norm{ S(\Abf) - S(\Bbf) } \leq \norm{ S(\Abf - \Bbf) }\]
\end{lemma}
\begin{proof} See Theorem \RN{3}.4.4 of~\cite{bhatia_matrix_1997}.
\end{proof}

\begin{lemma} \label{app:proofs:prob_ineq}
Let \(\P=\lbrace p_{1},...,p_{N} \rbrace,\Q=\lbrace q_{1},...,q_{N} \rbrace\) be two probability distributions supported on \([N]\), and denote by $TV(\P,\Q) := \frac{1}{2} \sum_{n = 1}^N \abs{p_n - q_n}$ their \emph{total variation distance}.
If for $\epsilon \in (0, 1/2)$ it holds that $TV (\P, \Q) \leq \epsilon$, then:
\[
\lvert H(\P)-H(\Q)\rvert \leq H_{b}(\epsilon)+\epsilon \cdot \log(N)
\text{\,,}
\]
where $H(\P) := - \sum\nolimits_{n = 1}^N p_n \log (p_n)$ is the entropy of $\P$, and $H_{b}(c) := -c \cdot \log(c) - (1-c) \cdot \log(1-c)$ is the binary entropy of a Bernoulli distribution parameterized by \(c \in [0,1]\).
\end{lemma}
\begin{proof} See,~\eg, Theorem 11 of~\citep{ho2010interplay}.
\end{proof}

\begin{lemma}[Hoeffding inequality in Hilbert space]
	Let \(X_{1},..,X_{N}\) be an i.i.d sequence of random variables whose range is some separable Hilbert space $\HH$.
	Suppose that \( \EE \brk[s]{ X_{n} } = 0 \) and \(\lVert X_{n}\rVert\leq c\) for all $n \in [N]$.
	Then, for all \(t\geq 0\):
	\[
	Pr \left (\norm*{ \frac{1}{n} \sum\nolimits_{n = 1}^N X_{n}}\geq t \right ) \leq 2\exp\left(\frac{Nt^{2}}{2c^{2}}\right)
	\text{\,,}
	\]
	where $\norm{\cdot}$ refers to the Hilbert space norm.
	
\end{lemma}
\begin{proof}
See Section 2.4 of~\citep{10.5555/1756006.1756036}.
\end{proof}

\begin{lemma}[adapted from~\cite{levine2018deep}]
\label{app:proofs:min_cut}
Let $G = (V, E)$ be the perfect binary tree graph, with vertices $V$ and edges $E$, underlying the locally connected tensor network that generates $\tntensor \in \R^{D_1 \times \cdots \times D_N}$ (defined in~\cref{sec:fit_tensor:tn_lc_nn}).
For $\K \subseteq [N]$, let $V_\K \subseteq V$ and $V_{K^c} \subseteq \V$ be the leaves in $G$ corresponding to axes indices $\K$ and $\K^c$ of $\tntensor$, respectively.
Lastly, given a cut $(A, B)$ of $V$, \ie~$A \subseteq V$ and $B \subseteq V$ are disjoint and $A \cup B = V$, denote by $C (A, B) := \{ \{u, v\} \in E : u \in A , v \in B \}$ the edge-cut set.
Then:
\[	
\rank (\mat{\tntensor}{\K}) \leq \min\nolimits_{\text{ cut } (A,B) \text{ of } V \text{ s.t. } V_\K \subseteq A , V_{\K^c} \subseteq B} R^{\abs{C(A,B) }  }
\text{\,.}
\]
In particular, if $(\K, \K^c) \in \can$, then:
\[
\rank ( \mat{\tntensor}{\K} ) \leq R
\text{\,.}
\]
\end{lemma}
\begin{proof}
See Claim 1 in~\citep{levine2018deep} for the upper bound on the rank of $\mat{\tntensor}{\K}$ for any $\K \subseteq [N]$.
If $(\K, \K^c) \in \can$, since there exists a cut $(A, B)$ of $V$ such that $V_\K \subseteq A$ and $V_{\K^c} \subseteq B$ whose edge-cut set is of a singleton, we get that $\rank ( \mat{\tntensor}{\K} ) \leq R$.
\end{proof}

\begin{lemma}[adapted from Theorem 3.18 in~\citep{grasedyck2010hierarchical}]\label{app:proofs:grasedyck}
	Let \(\mathcal{A}\in \mathbb{R}^{D_{1}\times ...\times D_{N}}\) and \(\epsilon >0\). 
	For each canonical partition \((\K,\K^{c}) \in \can\), let \(\sigma_{\K,1}\geq ...\sigma_{K,d(\K)}\) be the singular values of \(\mat{\mathcal{A}}{\K}\), where \(D_{\K}:= \min \brk[c]{ \prod_{n \in \K} D_n , \prod_{n \in \K^c} D_n }\).
	Suppose that for all \( (\K, \K^c) \in \can\) we have:
	\[
	\sqrt{\sum\nolimits_{d = R + 1}^{D_\K} \sigma^2_{d}}\leq \frac{\epsilon}{\sqrt{2N-3}}
	\text{\,.}
	\]
	Then, there exists $\tntensor \in \R^{D_1 \times \cdots \times D_N}$ generated by the locally connected tensor network (defined in~\cref{sec:fit_tensor:tn_lc_nn}) satisfying:
	\[
	\norm{\tntensor - \mathcal{A}}\leq \epsilon
	\text{\,.}
	\]
\end{lemma}

\begin{lemma} \label{app:proofs:tensor_ineq}
	
	Let \(\V,\W \in \R^{D_1 \times \cdots \times D_N} \) be tensors such that \(\norm{\V} = \norm{\W}=1\) and $\norm{\V-\W} < \frac{1}{2}$.
	Then, for any \((\K,\K^{c})\in \can\) it holds that:
	\[
	|\qe{\V}{\K}-\qe{\W}{\K}|\leq H_{b}(\norm{\V-\W})+ \norm{\V-\W}\cdot \log(D_\K)
	\text{\,,}
	\]
	where $H_{b}(c):=-(c\cdot\log(c)+(1-c)\cdot\log(1-c))$ is the binary entropy of a Bernoulli distribution parameterized by \(c \in [0,1]\), and \(\D_{\K} := \min \brk[c]{ \prod_{n \in \K} D_n , \prod_{n \in \K^c} D_n }\). 
\end{lemma}

\begin{proof}
	
For any matrix \(\Mbf \in \R^{D_1 \times D_2} \) with $D := \min \{ D_{1},D_{2} \}$, let \(S(\Mbf)=(\sigma_{\Mbf,1},...,\sigma_{\Mbf,D})\) be the vector consisting of its singular values. 
First note that 
\[
\norm{\V-\W}=\lVert S(\mat{\V-\W}{\K})\rVert \leq \norm{\V-\W}
\text{\,.}
\]
So by \cref{app:proofs:perturb} we have
\[
\lVert S(\mat{\V}{\K}-S(\mat{\W}{\K}) \rVert \leq \norm{\V-\W}
\text{\,.}
\]
Let \(v_{1}\geq \cdots \geq v_{D_\K}\) and \(w_{1}\geq \cdots \geq w_{D_\K}\) be the singular values of \(\mat{\V}{\K}\) and \(\mat{\W}{\K}\), respectively. 
We have by the Cauchy-Schwarz inequality that
\[
\left(\sum_{d = 1}^{D_\K}|w_{d}^{2}-v_{d}^{2}|\right)^{2}= \left(\sum_{d = 1}^{D_\K}|w_{d}-v_{d}| \cdot |w_{d}+v_{d}|\right)^{2}\leq \left(\sum_{d = 1}^{D_\K}(w_{d}-v_{d})^{2}\right)\left(\sum_{d = 1}^{D_\K}(w_{d}+v_{d})^{2}\right)
\text{\,.}
\]
Now the first term is upper bounded by \(\norm{\V-\W}^{2}\), and for the second we have 
\[
\sum_{d = 1}^{D_\K}(w_{d}+v_{d})^{2}=\sum_{d = 1}^{D_\K}w_{d}^{2}+\sum_{d = 1}^{D_\K}v_{d}^{2}+2v_{d}w_{d}=2+2\sum_{d = 1}^{D_\K}v_{d}w_{d} \leq 4
\text{\,,}
\]
where we use the fact that \(\norm{\V} = \norm{\W}=1\), and again Cuachy-Schwarz. 
Overall we have:
\[ 
\sum_{d = 1}^{D_\K}|w_{d}^{2}-v_{d}^{2}|
\leq 2\norm{\V-\W}
\text{\,.}
\]
Note that the left hand side of the inequality above equals twice the total variation distance between the distributions defined by $\lbrace w_{d}^{2}\rbrace_{d = 1}^{D_\K}$ and $\lbrace v_{d}^{2}\rbrace_{d = 1}^{D_\K}$.
 Therefore by \cref{app:proofs:prob_ineq} we have:
\[
|\qe{\V}{\K} - \qe{\W}{\K}| = |H(\lbrace w_{d}^{2} \rbrace)-H(\lbrace v_{d}^{2} \rbrace)|\leq \norm{\V-\W}\cdot \log(D_\K)+H_{b}(\norm{\V-\W})
\text{\,.}
\]
\end{proof}


\subsection{Proof of~\cref{thm:fit_necessary}} \label{app:proofs:fit_necssary}


If \(\A=0\) the theorem is trivial, since then \(\qe{\A}{\K}=0 \) for all \((\K, \K^{c})\in \can\), so we can assume \(\A\neq 0\). 
We have:
\[
\begin{split}
	\left\Vert \frac{\tntensor}{\norm{\tntensor}} - \frac{\A}{\norm{\A}}\right\Vert & = \frac{1}{\norm{\A}}\left\Vert \frac{\norm{\A}}{\norm{\tntensor}}\cdot \tntensor - \A \right\Vert \\
	& \leq\frac{1}{\norm{\A}} \left ( \left ( \frac{\norm{\A}}{\norm{\tntensor}} - 1 \right ) \norm{\tntensor} + \norm{\tntensor -\A} \right ) \\
	& = \frac{1}{\norm{\A}} \left ( \left\vert\norm{\A}-\norm{\tntensor}\right\vert+\norm{\tntensor -\A} \right ) \\	
	& \leq \frac{2\epsilon}{\norm{A}} \text{\,.}
	\end{split}
\]
Now, let $\hat{\A} := \frac{\A}{\norm{\A}}$ and $\hat{\W}_{\mathrm{TN}} = \frac{\tntensor}{\norm{\tntensor}}$ be normalized versions of $\A$ and $\tntensor$, respectively, and let \(c = \frac{2\epsilon}{\norm{A}}\). 
Note that \(c< \frac{2\norm{\A}}{4}\frac{1}{\norm{\A}}=\frac{1}{2}\), and therefore by \cref{app:proofs:tensor_ineq} we have:
\[
\begin{split}
|\qe{\A}{\K}-\qe{\tntensor}{\K}| & = |\qe{\hat{\A}}{\K}-\qe{\hat{\W}_{\mathrm{TN}}}{\K}| \\
& \leq c \cdot \log(D_\K)+H_{b}(c)
\text{\,.}
\end{split}
\]
By \cref{app:proofs:min_cut} we have that 
\[\qe{ \hat{\W}_{\mathrm{TN}} }{\K}\leq \log(\rank(\mat{\mathcal{\tntensor}}{\K}))\leq \log(R)\text{\,,}\]
and therefore 
\[\qe{ \hat{\A} }{\K}\leq \log(r)+c \log(D_\K) +H_{b}(c)\text{\,.}\]
Substituting \(c=\frac{2\epsilon}{\norm{A}}\) and invoking the elementary inequality \(H_{b}(x)\leq 2\sqrt{x}\) we obtain
\[	\qe{\A}{\K} \leq \ln (R)+\frac{2 \epsilon}{ \norm{\A} } \cdot \ln ( D_{\K} ) + 2 \sqrt{\frac{ 2 \epsilon }{ \norm{\A} } }\text{\,,}\]
as required.
\cref{eq:fit_necessary_lb} follows from the construction given in Theorem~2 of \citep{deng2017quantum}.
\qed

\subsection{Proof of~\cref{thm:fit_sufficient}} \label{app:proofs:fit_sufficient}
	Recall that for any probability distribution \(P=\lbrace p(x) \rbrace_{x\in [S]} \), where \(S\in \N\), its entropy is given by
	\[
	H(P)=\EE\nolimits_{x \sim P} \brk[s]*{ \log \left(1 / p(x) \right) } \text{\,.}
	\]
	Therefore by Markov's inequality we have for any \(0<c<1\) :
	\[
	Pr_{x\sim P}(\lbrace x:2^{-\frac{H(P)}{c}}\leq p(x) \rbrace)=Pr_{x\sim P}(\lbrace x:\log\left(\frac{1}{p(x)}\right)\leq \frac{H(P)}{c} \rbrace)\geq 1-\frac{1}{c}
	\text{\,.}
	\]
	Let \(T:=\lbrace x:2^{-\frac{H(P)}{c}}\leq p(x) \rbrace\subseteq [S]\). 
	Note that
	\[  
		2^{-\frac{H(P)}{c}}|T|\leq\sum_{x\in T}p(x)\leq \sum_{x\in [S]}p(x)= 1
		\text{\,,}
	\]
	and so \(|S|\leq 2^{\frac{H(P)}{c}}\). 
	Now consider, for each canonical partition \(\brk{\K, \K^c} \in \can \), the distribution 
	\[P_{\K}= \left \lbrace p_{\K}(i) := \frac{\sigma_{\K,i}^{2}}{\lVert \A \rVert^{2}} \right \rbrace_{i\in [D_\K]}\text{\,,}\]
	where \(\sigma_{\K,1}\geq \sigma_{\K,2}\geq ...\geq \sigma_{\K,D_\K} \) are the singular values of \(\mat{\mathcal{A}}{\K}\) (note that \(\frac{1}{\norm{\A}^{2}}\sum_{j}\sigma_{\K,j}^{2}= \frac{\norm{\mathcal{A}}^{2}}{\lVert A \rVert^{2}} =1\) so \(P_{\K}\) is indeed a probability distribution). By assumption,
	\[\qe{\A}{\K}=H(P_{\K})\leq \frac{\epsilon^{2}}{\lVert \A \rVert^{2}(2N-3)}\log(R)\text{\,,} \]
	for all \((\K,\K^{c})\in \can\).
	Thus, taking \(c=\frac{\lVert \A \rVert^{2}(2N-3)}{\epsilon^{2}}\) we get that there exists a subset \(S_{\K}\subseteq[D_\K]\) such that  
	\[P_{\K}(S_{\K}^{c})\leq \frac{\epsilon^{2}}{(2N-3)\lVert \A \rVert^{2}}\text{\,.}\]
	Note that \(|S_{\K}| \leq 2^{\frac{H(\P_{\K})}{c}}=2^{\log(r)}=r\), and so 
	\[P_{\K}(S_{\K})\leq \sum_{i = 1}^{r}\frac{\sigma_{i}^{2}}{\lVert \A \rVert^{2}}\] 
	Taking complements we obtain
	\[\sum_{i=r+1}^{D_\K}\frac{\sigma_{i}^{2}}{\lVert \A \rVert^{2}}\leq P_{\K}(S_{\K}^{c})\]
	so 
	\[\sqrt{\sum_{i=r+1}^{D_\K}\sigma_{\K,i}^{2}}\leq \frac{\epsilon}{\sqrt{(2N-3)}}.\]
	We can now invoke \cref{app:proofs:grasedyck}, which implies that there exists some \(\tntensor \in \R^{D_1 \times \cdots \times D_N}\) generated by the locally connected tensor network satisfying:
	\[\norm{ \tntensor - \A } \leq \epsilon.\]
\qed





\subsection{Proof of~\cref{prop:data_tensor_concentration}} \label{app:proofs:data_tensor_concentration}
	    Recall that by \cref{app:proofs:tensor_ineq} for any pair of unit norm tensors \(\V,\W\) such that \(\norm{\V-\W}<0.5\)
	\[|QE(\V;\K)-QE(\W;\K)|\leq \norm{\V-\W}\log(\D_{\K})+H_{b}(\norm{\V-\W})\leq 2\log(\D_{\K})\sqrt{\norm{\V-\W}}\text{\,,}\]
	where we used the elemntary inequality \(H_{b}(x)\leq 2\sqrt{x}\).
	So it suffices to bound 
	\[\left\Vert \frac{\datatensor}{\norm{\popdatatensor}}-\frac{\datatensor}{\norm{\datatensor}}\right\Vert\text{\,.}\]
	
	We have the identity 
	\[\left\Vert\frac{\popdatatensor}{\norm{\popdatatensor}}-\frac{\datatensor}{\norm{\datatensor}}\right\Vert =\left\Vert \frac{\D_{pop}\norm{\datatensor}-\datatensor\norm{\popdatatensor}}{\norm{\popdatatensor}\norm{\datatensor}}\right\Vert =\]
   \[\left\Vert\frac{\popdatatensor\norm{\datatensor}-\norm{\datatensor}\datatensor+\datatensor\norm{\datatensor}-\datatensor\norm{\popdatatensor}}{\norm{\popdatatensor}\norm{\datatensor}}\right\Vert\]
   by the triangle inequality the above is bounded by 
   \[\frac{\norm{\popdatatensor-\datatensor}}{\norm{\popdatatensor}}+\frac{|\norm{\popdatatensor}-\norm{\datatensor}|}{\norm{\popdatatensor}}\]
   
  For $m \in [M]$, let \(\X^{(m)} = y^{(m)} \cdot \tenp_{n \in [N]} \xbf^{(n, m)}-\popdatatensor\). 
  These are i.i.d random variables with $\EE\brk[s]{ \X^{(m)} } = 0$ and $\norm{\X^{(m)}}\leq 2$ for all $m \in [M]$. 
  Note that 
  \[\left\Vert\frac{1}{M}\sum\nolimits_{m = 1}^{M}\X^{(m)} \right\Vert=\norm{\datatensor-\popdatatensor}\text{\,,}\]
   
   so by \cref{app:proofs:prob_ineq} with \(c=2,t=\frac{\norm{\popdatatensor}\gamma}{\log(\D_{\K})}\), assuming \(M\geq \frac{128\log(\frac{2}{\delta})(\log(\D_{\K}))^{4}}{\norm{\popdatatensor}^{2}\gamma^{4}}\) we have with probability at least \(1-\delta\)
   \[
  	 |\norm{\popdatatensor} - \norm{\datatensor}| \leq \norm{\D_{pop}-\datatensor}\leq \frac{\norm{\popdatatensor}\gamma^{2}}{8\log(\D_{\K})^{2}} \text{\,,}
   \]
   and therefore 
   
   \[\frac{\norm{\popdatatensor-\datatensor}}{\norm{\popdatatensor}}+\frac{|\norm{\popdatatensor}-\norm{\datatensor}|}{\norm{\popdatatensor}}\leq \frac{\gamma^{2}}{4\log(\D_{\K})^{2}}\text{\,,}\]
   and so by \cref{app:proofs:tensor_ineq}
   \[|QE(\datatensor;\K)-QE(\popdatatensor;\K)|\leq \gamma\text{\,.}\]	
\qed


\subsection{Proof of~\cref{cor:acc_pred_nec_and_suf}} \label{app:proofs:cor:acc_pred_nec_and_suf}
If the locally connected tensor network represents a non-zero $\W \in \R^{D_1 \times \cdots \times D_N}$, then it can also represent $\W / \norm{\W}$.
To see it is so, notice that contraction is a multilinear operation, hence multiplying any of the tensors constituting the locally connected tensor network by~$1 / \norm{\W}$ results in $\W / \norm{\W}$.
Thus,~\cref{thm:fit_necessary} straightforwardly implies the first part of the claim, \ie~the necessary condition for low suboptimiality in achievable accuracy.

For the sufficient condition, since the entanglement of a tensor is invariant to multiplication by a constant, $\qe{\popdatatensor}{\K} = \qe{\popdatatensor / \norm{\popdatatensor}}{\K}$ for any $(\K, \K^c) \in \can$.
Thus, by~\cref{thm:fit_sufficient} there exists an assignment for the locally connected tensor network for which $\norm{\tntensor - \popdatatensor / \norm{\popdatatensor} } \leq \epsilon / 2$.
From the triangle inequality we obtain:
\be
\norm*{\frac{\tntensor}{\norm{\tntensor}} - \frac{ \popdatatensor }{ \norm{\popdatatensor} } } \leq \norm*{\tntensor - \frac{ \popdatatensor }{ \norm{\popdatatensor} } }  +  \norm*{\tntensor - \frac{\tntensor}{\norm{\tntensor}}} \leq \frac{\epsilon}{2} + \norm*{\tntensor - \frac{\tntensor}{\norm{\tntensor}}}
\text{\,.}
\label{eq:tn_tensor_pop_datatensor_norm}
\ee
Since $\norm{\tntensor - \popdatatensor / \norm{\popdatatensor} } \leq \epsilon / 2$ it holds that $\norm{\tntensor} \leq 1 + \epsilon / 2$.
Combined with the fact that $\norm1{\tntensor - \frac{\tntensor}{\norm{\tntensor}}} = \norm{\tntensor} - 1$, we get that $\norm1{\tntensor - \frac{\tntensor}{\norm{\tntensor}}} \leq \epsilon / 2$.
Plugging this into~\cref{eq:tn_tensor_pop_datatensor_norm} yields:
\[
\norm*{\frac{\tntensor}{\norm{\tntensor}} - \frac{ \popdatatensor }{ \norm{\popdatatensor} } } \leq \epsilon
\text{\,,}
\]
and so $\subopt := \min\nolimits_{\tntensor} \norm1{ \frac{\tntensor}{ \norm{\tntensor} } -  \frac{ \popdatatensor }{ \norm{\popdatatensor} }  } \leq \epsilon$.
\qed

%\subsection{Proof of~\cref{thm:fit_necessary_pdim}} \label{app:proofs:fit_necssary_pdim}
%\todo{FILL}
%\qed
%
%
%\subsection{Proof of~\cref{thm:fit_sufficient_pdim}} \label{app:proofs:fit_sufficient_pdim}
%\todo{FILL}
%\qed