\section{Further Implementation Details}
\label{app:experiments:details}

%\section{Further Experiments and Implementation Details}
%\label{app:experiments}

% FURTHER EXPERIMENTS
%\subsection{Further Experiments}
%\label{app:experiments:further}

%\begin{table}[t]
%	\caption{
	%		\red{Arranging features in high-dimensional randomly permuted audio data via~\cref{alg:ent_struct_search} significantly improves the performance of common locally connected neural networks.
		%		Reported are the results of an experiment identical to that of~\cref{tab:audio_moderate_dim}, but over a high-dimensional version of the Speech Commands~\citep{warden2018speech} dataset, in which every instance has 50,000 features.
		%		Notice that, rearranging the features according to~\cref{alg:ent_struct_search} substantially improves the performance of a convolutional neural network (CNN) and S4.
		%		See~\cref{app:experiments:details} for further implementation details.
		%		}
	%	}
%	\begin{center}
	%		\small
	%		\vspace{-1mm}
	%		\begin{tabular}{lcc}
		%			\toprule
		%			& Unstructured & \cref{alg:ent_struct_search}  \\
		%			\midrule
		%			CNN & $12.3$ & $57.4$ \\
		%			S4 & $15.7$ & $78.1$ \\
		%			%			XGBoost & $21.4$ & $21.4$ \\
		%			\bottomrule					
		%			\vspace{-2mm}
		%		\end{tabular}
	%	\end{center}
%	%	\vspace{-1mm}
%	\label{tab:audio_high_dim}
%\end{table}
%
%\cref{tab:audio_high_dim} supplements the experiment of~\cref{tab:audio_moderate_dim}, demonstrating that~\cref{alg:ent_struct_search} can be applied to data with a large number of features (\eg~with tens of thousands of features).

%% IMPLEMENTATION DETAILS
%\subsection{Further Implementation Details}
%\label{app:experiments:details}

%We provide implementation details omitted from our experimental reports (\cref{sec:accurate_predict:emp_demo},~\cref{sec:enhancing},~\cref{app:extension_dims:accurate_predict:emp_demo},~\cref{app:extension_dims:enhancing:ex} and~\cref{app:experiments:further}).
We provide implementation details omitted from our experimental reports (\cref{sec:accurate_predict:emp_demo},~\cref{sec:enhancing} and~\cref{app:extension_dims:accurate_predict:emp_demo}).
Source code for reproducing our results and figures, based on the PyTorch~\citep{paszke2017automatic} framework,\ifdefined\CAMREADY
~can be found at \url{https://github.com/nmd95/dl_unstr_qe_code}.
\else
%~is attached as supplementary material and will be made publicly available.
~will be made publicly available.
\fi
All experiments were run on a single Nvidia RTX A6000 GPU.



\subsection{Empirical Demonstration of Theoretical Analysis (\cref{fig:entanglement_inv_corr_acc} and~\cref{fig:entanglement_inv_corr_acc_images})} \label{app:experiments:details:emp_demo}


\subsubsection{\cref{fig:entanglement_inv_corr_acc}}
\label{app:experiments:details:emp_demo:audio}

\textbf{Dataset}: The SpeechCommands dataset \citep{warden2018speech} contains raw audio segments of length up to 16000, split into 84843 train and 11005 test segments.
We zero-padded all audio segments to have a length of 16000 and resampled them using sinc interpolation (default PyTorch implementation). 
We allocated 11005 audio segments from the train set for validation, \ie~the dataset was split into 73838 train, 11005 validation and 11005 test audio segments.
We created a binary one-vs-all classification version of SpeechCommands by taking all audio segments labeled by the class “33'' (corresponding to the word “yes''), and sampling an equal amount of segments from the remaining classes (this process was done separately for the train, validation and test sets). 
The resulting balanced one-versus-all classification dataset had 5610 train, 846 validation and 838 test segments.
Lastly, we resampled all audio segments in the dataset from 16000 to 4096 features using sinc interpolation.

\textbf{Random feature swaps:} Starting with the original order of features, we created increasingly “shuffled'' versions of the dataset by randomly swapping the position of features. For each number of random position swaps $k \in \{0, 250, \ldots, 2500 \}$, we created ten datasets, whose features were subject to $k$ random position swaps between features, using different random seeds.

\textbf{Quantum entanglement measurement}: Each reported value in the plot is the average of entanglements with respect to canonical partitions (\cref{def:canonical_partitions}) corresponding to levels \(l  =  0, 1, 2, 3, 4, 5\).
We used~\cref{alg:ent_comp} described in \cref{app:ent_comp} on two mini-batches of size 500, randomly sampled from the train set.
As customary (\cf~\cite{stoudenmire2016supervised}), every input feature $x$ was embedded using the following sine-cosine scheme:
\[
\phi(x) := ( \sin(\pi \theta x) , \cos(\pi \theta x)) \in \R^2
\text{\,,}
\]
where $\theta = 0.085$.

%\cref{sec:accurate_predict:emp_demo}:\\
%\textbf{Dataset}: Based on the SpeechCommands dataset \citep{warden2018speech},  which contains raw audio time series with of length up to 16000, split into (train: 84843, test: 11005). From SpeechCommands we derived a binarized version (which we call "BinarySpeechCommands") as follows:  All samples were zero-padded to have exact length of 16k, then resampled, again to 16k using sinc interpolation (Pytorch implementation). 11005 samples were then randomly extracted from 'train' to yield the split:
%(train: 73838, val: 11005, test: 11005). Class '33' corresponding to the word 'yes', was randomly chosen, then train, val, and test splits of BinarySpeechCommands were obtained by extracting all occurrences of class '33', and randomly sampling an equal amount of samples from the rest of the classes to yield a balanced, one-versus-all dataset. The final split is: (train: 5610, val: 846, test: 838).Finally, the BinarySpeechCommands was resampled ( again using  sinc interpolation') from 16k to 4096 features.\\\\
%\textbf{Permutation generation}: We start with the original ordering, and create permuted versions by repeatedly selecting 2 random features and swapping them. This was repeated k times for  \(k=0,250,500,750,1000,1250,1500,1750,2000,2250,2500\), where for each value of k we averaged over 10 random seeds.\\\\
%\textbf{QE measurement}: measurements were performed for each permuted version of the data. Each measurement value (for the permuted versions obtained from a given seed) is the average of all canonical QE measurements starting at level \(l = L\) and ending at level \(l = L - 4\) (inclusive). The individual QE measurements (of a specific canonical partition) is performed using a GPU-parallelized implementation of the algorithm described in \cref{app:ent_comp},on 2 mini-batches of 500 samples from a  randomly sampled subset of 1000 samples from the training set.  The embedding chosen is a variant of the standard sine-cosine embedding scheme  from \citep{stoudenmire2016supervised} with an angle of \(\theta=0.085\), i.e we map each numerical feature x to the 2d vector 
%\[\phi(x):=(sin(\pi \theta x),cos(\pi \theta x)). ))\]
%The reported values are averages of 10 such measurements (using 10 different seeds) and the resulting std (as an envelope.) To ensure numerical stability, the SVD computations were performed on matrices with added smalle value of noise by adding to the matrix a random matrix of the same dimension - multiplied by the mean of the original matrix and a small constant, set to be \(0.0001\) in all measurements.\\\\

\noindent \textbf{Neural network architectures}:
\begin{itemize}
	\item \textbf{CNN}: The M5 architecture from~\citep{dai2017very}.
	
	\item \textbf{S4}: Official implementation of~\citep{gu2022efficiently} with a hidden dimension of 128 and 4 layers.
	
	\item \textbf{Local-Attention}: Adaptation of the local-attention model in~\citep{tay2021long}. 
	The specific implementation is based on an adaptation of \citep{rae-razavi-2020-transformers}. 
	While the local-attention model in the aforementioned implementation assumes a discrete fixed-size dictionary of possible tokens in the input, our adapted version keeps the input raw and continuous and maps it into the architecture using a layer of multi-layer perceptron (MLP) that learns the embeddings. 
	Another distinction is that our version adapts the sequence-to-sequence modeling of the said implementation to classification of sequences by collapsing the spatial dimension of the sequence outputted from the local-attention model using mean-pooling and passing this into an MLP classification head. 
	The network had attention dimension 128 (with 2 heads of dimension 64), depth 8, hidden dimension of MLP blocks 341, and local-attention window size 10.
\end{itemize}

\textbf{Training and evaluation:} 
The binary cross-entropy loss was minimized via the Adam optimizer~\citep{kingma2014adam} with default $\beta_1, \beta_2$ coefficients. 
The duration of optimization was 150, 40 and 50 epochs for the CNN, S4 and Local-Attention models, respectively.
After the last training epoch, the model which performed best on the validation set was chosen, and average test accuracy was measured on the test set and reported as test performance.
Additional optimization hyperparameters are provided in~\cref{tab:1d_audio_optimization_hyperparam}.

\begin{table}[H]
	\caption{
		Optimization hyperparameters for the experiments of~\cref{fig:entanglement_inv_corr_acc}.
	}
	\label{tab:1d_audio_optimization_hyperparam}
	\begin{center}
		\begin{small}
			\begin{tabular}{lccccc}
				\toprule
				\textbf{Model} & \textbf{Optimizer} & \textbf{Learning Rate} & \textbf{Weight Decay} & \textbf{Scheduler} & \textbf{Batch Size} \\
				\midrule
				CNN         & Adam              & 0.01                 & 0.0001              & None            & 128           \\
				S4              & AdamW             & 0.001                 & 0.01                & CosineAnnealingLR & 64 \\
				Local-attention & Adam              & 0.0005                & 0                   & None            & 32            \\				
				\bottomrule
			\end{tabular}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}

\subsubsection{\cref{fig:entanglement_inv_corr_acc_images}}

\textbf{Dataset}: We created one-vs-all binary classification datasets based on CIFAR10~\citep{krizhevsky2009learning} as follows.
All images were converted to grayscale using the PyTorch default implementation. 
Then, we allocated 7469 images from the train set for validation, \ie~the dataset was split into 42531 train, 7469 validation and 1000 test images.
We took all images labeled by the class “0'' (corresponding to images of airplanes), and uniformly sampled an equal amount of images from the remaining classes (this process was done separately for the train, validation and test sets).
The resulting balanced one-versus-all classification dataset had 8506 train, 1493 validation and 2001 test images.

\textbf{Random feature swaps:} We created increasingly “shuffled'' versions of the dataset according to the protocal described in~\cref{app:experiments:details:emp_demo:audio}.

\textbf{Quantum entanglement measurement}: Each reported value in the plot is the average entanglements with respect to canonical partitions (\cref{def:canonical_partitions_pdim}) corresponding to levels $l = 0, 1$.


\textbf{Neural network architectures}:
We used a standard convolutional neural network whose convolutional blocks comprised a convolutional layer, batch normalization, ReLU activation and max-pooling.
The architectural hyperparameters are specified in~\cref{tab:2d_cnn_hyperparam}.

\begin{table}[H]
	\caption{
		Architectural hyperparameters of the convolutional neural network used in the experiments of~\cref{fig:entanglement_inv_corr_acc_images}.
	}
	\label{tab:2d_cnn_hyperparam}
	\begin{center}
		\begin{small}
			\begin{tabular}{ll}
				\toprule
				\textbf{Hyperparameter} & \textbf{Value} \\
				\midrule
				Stride & 1 \\
				Kernel size & 5 \\
				Pooling window size & 2 \\
				Number of blocks & 4 \\
				Hidden dimension & 32 \\
				\bottomrule
			\end{tabular}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}


\textbf{Training and evaluation}: The binary cross-entropy loss was minimized for 50 epochs via the Adam optimizer~\citep{kingma2014adam} with default $\beta_1, \beta_2$ coefficients.
After the last training epoch, the model which performed best on the validation set was chosen, and test set performance was averaged over 30 random seeds.
Additional optimization hyperparameters are provided in~\cref{tab:2d_cnn_optimization_hyperparam}.

\begin{table}[H]
	\caption{
		Optimization hyperparameters for the experiments of~\cref{fig:entanglement_inv_corr_acc_images}.
	}
	\label{tab:2d_cnn_optimization_hyperparam}
	\begin{center}
		\begin{small}
			\begin{tabular}{lccccc}
				\toprule
				\textbf{Model} & \textbf{Optimizer} & \textbf{Learning Rate} & \textbf{Weight Decay} & \textbf{Scheduler} & \textbf{Batch Size} \\
				\midrule
				CNN         & Adam              & 0.0001                 & 0.0001              & None            & 128           \\
				\bottomrule
			\end{tabular}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}



\subsection{Enhancing Suitability of Data to Locally Connected Neural Networks (\cref{tab:audio_moderate_dim} and~\cref{tab:tabular_datasets})} 
 \label{app:experiments:details:enhancing}
 
\subsubsection{Randomly Permuted Audio Datasets (\cref{tab:audio_moderate_dim})} \label{app:experiments:details:enhancing:perm_audio}

\textbf{Dataset}: To facilitate efficient experimentation, we downsampled all audio segments in SpeechCommands to have 2048 features.
Furthermore, for the train, validation and test sets separately, we used 20\% of the audio segments available for each class.

\textbf{Neural network architectures}:

\begin{itemize}	
	\item \textbf{CNN}: The M5 architecture from~\citep{dai2017very}.
		
	\item \textbf{S4}: Official implementation of~\citep{gu2022efficiently} with a hidden dimension of 128 and 4 layers.
		
	\item \textbf{Local-Attention}: Same architecture used in the experiments of~\cref{fig:entanglement_inv_corr_acc}, but with the following architectural hyperparameters.
	The network had attention-dimension 128 (with 2 heads of dimension 64 each), depth 4, hidden dimension of MLP blocks 341, and local-attention window size of 10.	
\end{itemize}

\textbf{Training and evaluation:} 
The cross-entropy loss was minimized via the Adam optimizer~\citep{kingma2014adam} with default $\beta_1, \beta_2$ coefficients. 
The duration of optimization was 200, 200 and 450 epochs for the CNN, S4 and Local-Attention models, respectively.
After the last training epoch, the model which performed best on the validation set was chosen, and average test accuracy was measured on the test set and reported as test performance.
Additional optimization hyperparameters are provided in~\cref{tab:1d_audio_perm_optimization_hyperparam}.


\begin{table}[H]
	\caption{
		Optimization hyperparameters for the experiments of~\cref{tab:audio_moderate_dim}.
	}
	\label{tab:1d_audio_perm_optimization_hyperparam}
	\begin{center}
		\begin{small}
			\begin{tabular}{llllll}
				\toprule
				\textbf{Model} & \textbf{Optimizer} & \textbf{Learning Rate} & \textbf{Weight Decay} & \textbf{Scheduler} & \textbf{Batch Size} \\ \midrule
				CNN         & Adam              & 0.001                 & 0.0001              & None            & 128           \\
				Local-attention & Adam              & 0.0001                & 0                   & None            & 32            \\
				S4              & AdamW             & 0.001                 & 0.01                & CosineAnnealingLR & 64 \\ \bottomrule
			\end{tabular}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}


\subsubsection{Tabular Datasets (\cref{tab:tabular_datasets})} \label{app:experiments:details:enhancing:tab}

\textbf{Datasets}: The datasets "dna", "semeion" and "isolet" are all from the OpenML repository~\citep{OpenML2013}. 
For each dataset we split the samples intro three folds, which were used for evaluation according to a standard $3$-way cross validation protocol.
That is, for each of the three folds, we used one third of the data as a test set and the remaining for train and validation.
Specifically, one third of the samples in the remaining folds (not used for testing) were allocated for the validation set.

%\textbf{Permutations and rearrangements}: For a given random seed, three versions of the dataset were created: 'shuffled' (randomly shuffling the features), 'cuts' (rearranging the shuffled version using our method), 'IGTD' (rearranging the shuffled version using IGTD). Note that for each fold, a different permutation was created, and thus also a different rearrangement.\\\\
%Metis parameters: \(\text{ncuts} =10, \text{ niter}=50000, \text{ recursive}=True.\)\\\\
%IGTD parameters: \(\text{S max}=1000,\text{S con}=20, \text{t con}=0.0, \text{ t swap}=0.0.\)\\\\

\textbf{Neural network architectures}: 

\begin{itemize}
	\item \textbf{CNN}: We used a ResNet adapted for tabular data. 
	It consisted of residual blocks of the following form:
	\[
	\text{Block} (\xbf) = \text{dropout} ( \xbf + \text{BN} ( \text{maxpool} ( \text{ReLU} ( \text{conv} (\xbf)) ) ) )
	\text{\,.}
	\]
	After applying a predetermined amount of residual blocks, a global average pooling and fully connected layers were used to output the prediction.
	The architectural hyperparameters are specified in~\cref{tab:resnet_tabular_hyperparams}.

	\item \textbf{S4}: Official implementation of~\citep{gu2022efficiently} with a hidden dimension of 64 and 4 layers.
	
	\item \textbf{Local-Attention}: Same architecture used in the experiments of~\cref{fig:entanglement_inv_corr_acc}, but with the following architectural hyperparameters.
	The network had attention-dimension 128 (with 4 heads of dimension 64 each), depth 8, hidden dimension of MLP blocks 341, and local-attention window size of 25.
\end{itemize}

\begin{table}[H]
	\caption{
		Architectural hyperparameters for the convolutional neural network used in the experiments of~\cref{tab:audio_moderate_dim} and~\cref{tab:tabular_datasets}.
	}
	\label{tab:resnet_tabular_hyperparams}
	\begin{center}
		\begin{small}
			\begin{tabular}{ll}
				\toprule
				\textbf{Hyperparameter} & \textbf{Value} \\
				\midrule
				Stride & 3 \\
				Kernel size & 3 \\
				Pooling window size & 3 \\
				Number of blocks & 8 \\		
				Hidden dimension & 32 \\		
				\bottomrule
			\end{tabular}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}

\textbf{Training and evaluation}: The cross-entropy loss was minimized for 300 epochs via the Adam optimizer~\citep{kingma2014adam} with default $\beta_1, \beta_2$ coefficients.
After the last training epoch, the model which performed best on the validation set was chosen, and test accuracy was measured on the test set.
The reported accuracy is the average over the three folds.
Additional optimization hyperparameters are specified in~\cref{tab:training_parameters_all_models}.

\begin{table}[H]
	\caption{
		Optimization hyperparameters for the experiments of~\cref{tab:audio_moderate_dim} and~\cref{tab:tabular_datasets}.
	}
	\label{tab:training_parameters_all_models} % Change the label to something appropriate
	\begin{center}
		\begin{small}
			\begin{tabular}{lccccc}
				\toprule
				\textbf{Model} & \textbf{Optimizer} & \textbf{Learning Rate} & \textbf{Weight Decay} & \textbf{Scheduler} & \textbf{Batch Size} \\
				\midrule
				CNN         & Adam              & 0.001                 & 0.0001              & None            & 64           \\
				S4              & AdamW             & 0.001                 & 0.01                & CosineAnnealingLR & 64 \\
				Local-attention & Adam              & 0.00005                & 0                   & None            & 64            \\
				\bottomrule
			\end{tabular}
		\end{small}
	\end{center}
	\vskip -0.1in
\end{table}









%\textbf{Dataset}: The dataset used is the 35-classes SpeechCommands dataset, obtained in a procedure identical to the one described in above, except we did not binarize, and instead of downsampling to 4096, it was up-sampled to 50k.\\\\
%\textbf{Sparsification}:A standard algorithm  for spectral sparsification \citep{spielman2011spectral} is used of the full graph of surrogateEE measurements (a complete graph of size 50k) with default parameters, only varying \(\epsilon\) (the sparsification parameter) in \([0.15, 0.25, 0.5, 0.75, 1.0]\). Then, our method for rearrangement was applied on the sparsified correlations graphs with fixed parameters using the Metis graph partitioning algorithm \citep{doi:10.1137/S1064827595287997}  with hyperparameters \(\text{ncuts}=10,\text{ niter} =50000\). The values on the x-axis correspond to the sparsification ratio (edges in sparsified graph over edges in original graph). Note that the first ratio value is 0, which corresponds to no-rearrangement at all. For sparsificaiton, we used the "Laplacians" package written in Julia, and the rest of the code was implemented in Python.\\
%
%\textbf{Architecture specifications:}
%
%\begin{itemize}
%	\item \textbf{TabularResCNN}: Our implementation in \cref{fig:tabcnn}, but now with hyperparameters: 
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		1d-batch norm & \\
	%		hidden dim & 32 \\
	%		layers & 8 \\
	%		kernel size & 3 \\
	%		stride & 3 \\
	%		pool size & 3 \\
	%		activation & ReLU \\
	%	\end{tabular}
%	
%	\item \textbf{MLP (Small)}: Our implementation of a MLP with 
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		1d-batch norm & \\
	%		activation & ReLU \\
	%		dimensions & (50K, 500, 50, 35) \\
	%	\end{tabular}
%	
%	\item \textbf{MLP (Large)}: Our implementation of a MLP with 
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		1d-batch norm & \\
	%		activation & ReLU \\
	%		dimensions & (50K, 25000, 6250, 3125, 35) \\
	%	\end{tabular}
%	
%	\item \textbf{S4}: As in \cref{app:experiments:details:emp_demo}, but with 
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		hidden dimension & 32 \\
	%		layers & 4 \\
	%	\end{tabular}
%	
%	\item \textbf{XGBoost}: An off-the-shelf implementation of XGBoost with GPU-histogram building using the dmlc library. Hyperparameters: 
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		max depth & 3 \\
	%		learning rate & 0.2 \\
	%		n estimators & 250 \\
	%	\end{tabular}
%	
%	\item \textbf{TabNet (Small)}: An off-the-shelf implementation using the "pytorch-tabnet" package, with hyperparameters 
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		n d & 10 \\
	%		n a & 10 \\
	%		n steps & 4 \\
	%		n independent & 2 \\
	%		n shared & 2 \\
	%	\end{tabular}
%	
%	\item \textbf{TabNet (Large)}: An off-the-shelf implementation using the "pytorch-tabnet" package with hyperparameters 
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		n d & 512 \\
	%		n a & 512 \\
	%		n steps & 4 \\
	%		n independent & 2 \\
	%		n shared & 2 \\
	%	\end{tabular}
%\end{itemize}
%
%\textbf{Architecture specifications:}
%
%\begin{itemize}
%	\item \textbf{TabularResCNN}: Our implementation in \cref{fig:tabcnn}, but now with hyperparameters: 
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		1d-batch norm & \\
	%		hidden dim & 32 \\
	%		layers & 8 \\
	%		kernel size & 3 \\
	%		stride & 3 \\
	%		pool size & 3 \\
	%		activation & ReLU \\
	%	\end{tabular}
%	
%	\item \textbf{MLP (Small)}: Our implementation of a MLP with 
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		1d-batch norm & \\
	%		activation & ReLU \\
	%		dimensions & (50K, 500, 50, 35) \\
	%	\end{tabular}
%	
%	\item \textbf{MLP (Large)}: Our implementation of a MLP with 
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		1d-batch norm & \\
	%		activation & ReLU \\
	%		dimensions & (50K, 25000, 6250, 3125, 35) \\
	%	\end{tabular}
%	
%	\item \textbf{S4}: As in \cref{app:experiments:details:emp_demo}, but with 
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		hidden dimension & 32 \\
	%		layers & 4 \\
	%	\end{tabular}
%	
%	\item \textbf{XGBoost}: An off-the-shelf implementation of XGBoost with GPU-histogram building using the dmlc library. Hyperparameters: 
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		max depth & 3 \\
	%		learning rate & 0.2 \\
	%		n estimators & 250 \\
	%	\end{tabular}
%	
%	\item \textbf{TabNet (Small)}: An off-the-shelf implementation using the "pytorch-tabnet" package, with hyperparameters 
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		n d & 10 \\
	%		n a & 10 \\
	%		n steps & 4 \\
	%		n independent & 2 \\
	%		n shared & 2 \\
	%	\end{tabular}
%	
%	\item \textbf{TabNet (Large)}: An off-the-shelf implementation using the "pytorch-tabnet" package with hyperparameters 
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		n d & 512 \\
	%		n a & 512 \\
	%		n steps & 4 \\
	%		n independent & 2 \\
	%		n shared & 2 \\
	%	\end{tabular}
%\end{itemize}
%
%\textbf{Training and evaluation:} All models were trained on a single "RTX A6000" GPU. For all architectures, training was conducted by running 50 epochs on the training data. After the last training epoch, the model which performed best on the validation set was chosen, and test accuracy was measured on the test set and reported as test performance.
%
%\begin{table}[h]
%	\centering
%	\begin{tabular}{l|l|l|l|l|l|l}
	%		\textbf{Model} & \textbf{Optimizer} & \textbf{Learning Rate} & \textbf{Weight Decay} & \textbf{Scheduler} & \textbf{Loss} & \textbf{Batch Size} \\ \hline
	%		TabularResCNN  & Adam              & 0.0001                 & 0.0001              & None            & CE     & 64            \\
	%		MLP (Small)    & Adam              & 0.01                  & 0.0001              & None            & CE     & 32            \\
	%		MLP (Large)    & Adam              & 0.0001                & 0.0001              & None            & CE     & 32            \\
	%		S4             & AdamW             & 0.001                 & 0.01                & None            & CE     & 16            \\
	%		XGBoost        & -                 & 0.2                   & -                   & -               & -      & -             \\
	%		TabNet (Small) & -                 & -                     & -                   & -               & -      & 512 (VBS 128) \\
	%		TabNet (Large) & -                 & -                     & -                   & -               & -      & 512 (VBS 128)
	%	\end{tabular}
%	\caption{Training parameters for the different models. Hyperparameters with dash (-) denote that the default of the off-the-shelf implementation was used. VBS stands for "virtual batch size".}
%\end{table}






%\section{Further Implementation Details}
%\label{app:experiments:details}
%
%%\section{Further Experiments and Implementation Details}
%%\label{app:experiments}
%
%% FURTHER EXPERIMENTS
%%\subsection{Further Experiments}
%%\label{app:experiments:further}
%
%%\begin{table}[t]
%%	\caption{
	%%		\red{Arranging features in high-dimensional randomly permuted audio data via~\cref{alg:ent_struct_search} significantly improves the performance of common locally connected neural networks.
		%%		Reported are the results of an experiment identical to that of~\cref{tab:audio_moderate_dim}, but over a high-dimensional version of the Speech Commands~\citep{warden2018speech} dataset, in which every instance has 50,000 features.
		%%		Notice that, rearranging the features according to~\cref{alg:ent_struct_search} substantially improves the performance of a convolutional neural network (CNN) and S4.
		%%		See~\cref{app:experiments:details} for further implementation details.
		%%		}
	%%	}
%%	\begin{center}
	%%		\small
	%%		\vspace{-1mm}
	%%		\begin{tabular}{lcc}
		%%			\toprule
		%%			& Unstructured & \cref{alg:ent_struct_search}  \\
		%%			\midrule
		%%			CNN & $12.3$ & $57.4$ \\
		%%			S4 & $15.7$ & $78.1$ \\
		%%			%			XGBoost & $21.4$ & $21.4$ \\
		%%			\bottomrule					
		%%			\vspace{-2mm}
		%%		\end{tabular}
	%%	\end{center}
%%	%	\vspace{-1mm}
%%	\label{tab:audio_high_dim}
%%\end{table}
%%
%%\cref{tab:audio_high_dim} supplements the experiment of~\cref{tab:audio_moderate_dim}, demonstrating that~\cref{alg:ent_struct_search} can be applied to data with a large number of features (\eg~with tens of thousands of features).
%
%%% IMPLEMENTATION DETAILS
%%\subsection{Further Implementation Details}
%%\label{app:experiments:details}
%
%%We provide implementation details omitted from our experimental reports (\cref{sec:accurate_predict:emp_demo},~\cref{sec:enhancing},~\cref{app:extension_dims:accurate_predict:emp_demo},~\cref{app:extension_dims:enhancing:ex} and~\cref{app:experiments:further}).
%We provide implementation details omitted from our experimental reports (\cref{sec:accurate_predict:emp_demo},~\cref{sec:enhancing} and~\cref{app:extension_dims:accurate_predict:emp_demo}).
%Source code for reproducing our results and figures, based on the PyTorch~\citep{paszke2017automatic} framework,\ifdefined\CAMREADY
%~can be found at \url{XXX}.
%\todo{Add link to repo were code will be uploaded to}
%\else
%%~is attached as supplementary material and will be made publicly available.
%~will be made publicly available.
%\fi
%All experiments were run on a single Nvidia RTX A6000 GPU.
%
%
%
%\subsection{Empirical Demonstration of Theoretical Analysis} \label{app:experiments:details:emp_demo}
%\cref{sec:accurate_predict:emp_demo}:\\
%\textbf{Dataset}: Based on the SpeechCommands dataset \citep{warden2018speech},  which contains raw audio time series with of length up to 16000, split into (train: 84843, test: 11005). From SpeechCommands we derived a binarized version (which we call "BinarySpeechCommands") as follows:  All samples were zero-padded to have exact length of 16k, then resampled, again to 16k using sinc interpolation (Pytorch implementation). 11005 samples were then randomly extracted from 'train' to yield the split:
%(train: 73838, val: 11005, test: 11005). Class '33' corresponding to the word 'yes', was randomly chosen, then train, val, and test splits of BinarySpeechCommands were obtained by extracting all occurrences of class '33', and randomly sampling an equal amount of samples from the rest of the classes to yield a balanced, one-versus-all dataset. The final split is: (train: 5610, val: 846, test: 838).Finally, the BinarySpeechCommands was resampled ( again using  sinc interpolation') from 16k to 4096 features.\\\\
%\textbf{Permutation generation}: We start with the original ordering, and create permuted versions by repeatedly selecting 2 random features and swapping them. This was repeated k times for  \(k=0,250,500,750,1000,1250,1500,1750,2000,2250,2500\), where for each value of k we averaged over 10 random seeds.\\\\
%\textbf{QE measurement}: measurements were performed for each permuted version of the data. Each measurement value (for the permuted versions obtained from a given seed) is the average of all canonical QE measurements starting at level \(l = L\) and ending at level \(l = L - 4\) (inclusive). The individual QE measurements (of a specific canonical partition) is performed using a GPU-parallelized implementation of the algorithm described in \cref{app:ent_comp},on 2 mini-batches of 500 samples from a  randomly sampled subset of 1000 samples from the training set.  The embedding chosen is a variant of the standard sine-cosine embedding scheme  from \citep{stoudenmire2016supervised} with an angle of \(\theta=0.085\), i.e we map each numerical feature x to the 2d vector 
%\[\phi(x):=(sin(\pi \theta x),cos(\pi \theta x)). ))\]
%The reported values are averages of 10 such measurements (using 10 different seeds) and the resulting std (as an envelope.) To ensure numerical stability, the SVD computations were performed on matrices with added smalle value of noise by adding to the matrix a random matrix of the same dimension - multiplied by the mean of the original matrix and a small constant, set to be \(0.0001\) in all measurements.\\\\
%
%\textbf{SurrogateEE Measurements:}
%
%The measurement process and permutations were carried out in the same manner as in the case of the EE measurements. The main distinction lies in the actual measurement between the two sides of a partition. To perform the calculation, we executed the following steps:
%
%\begin{enumerate}
%	\item Calculate the entire matrix of pairwise Pearson correlations.
%	\item Convert the matrix into a graph (clique).
%	\item Compute the sum of edges crossing the cut induced by the partition.
%	\item Normalize the result by dividing the sum of edges crossing the cut by the number of edges crossing the cut.
%\end{enumerate}
%
%\noindent \textbf{Architecture specifications:}
%\begin{itemize}
%	\item \textbf{CNN}: M5 architecture \citep{dai2017very}
%	\item \textbf{Local Attention}: Adaptation of the architecture in \citep{tay2021long}. The specific implementation is based on an adaptation of \citep{rae-razavi-2020-transformers}. While the transformer in the aforementioned implementation assumes a discrete fixed-size dictionary of possible tokens in the input, our adapted version keeps the input raw and continuous and maps it into the architecture using a layer of MLP that learns the embeddings. Another distinction is that our version adapts the sequence-to-sequence modeling of the said implementation to classification of sequences by collapsing the spatial dimension of the sequence outputted from the seq2seq transformer using mean-pooling and passing this into an MLP-based classification head. The hyperparameters are: \(\text{attention dimension}=128 \text{ (with 2 heads of dimension 64)}, \text{depth} = 8, \text{MLP-blocks' hidden dimension} = 341, \text{local-attention-window-size} = 10.\)
%	\item \textbf{S4}: Official implementation \citep{gu2022efficiently} with a hidden dimension of 128 and 4 layers.
%\end{itemize}
%
%\noindent \textbf{Training and evaluation:} All models were trained on a single "RTX A6000" GPU. Training was conducted by running 40, 50, 150 epochs (for S4, Local-attention, M5 CNN, respectively). After the last training epoch, the model which performed best on the validation set was chosen, and average test accuracy was measured on the test set and reported as test performance. Values reported are averages over 10 seeds and their corresponding standard deviations (as envelope).
%
%\begin{table}[h]
%	\centering
%	\begin{tabular}{l|l|l|l|l|l}
	%		\textbf{Model} & \textbf{Optimizer} & \textbf{Learning Rate} & \textbf{Weight Decay} & \textbf{Scheduler} & \textbf{Batch Size} \\ \hline
	%		M5 CNN         & Adam              & 0.01                 & 0.0001              & None            & 128           \\
	%		Local-attention & Adam              & 0.0005                & 0                   & None            & 32            \\
	%		S4              & AdamW             & 0.001                 & 0.01                & CosineAnnealingLR & 64
	%	\end{tabular}
%	\caption{Training parameters for the different models}
%	\label{tab:training_parameters}
%\end{table}
%
%\noindent All models use the cross-entropy (CE) loss function.\\\\
%\textbf{Dataset}: Based on CIFAR10 (downloaded from here, specifically) which originally contains color images of dimension (3, 32, 32), split into {train: 50000, test: 1000}. From CIFAR10 we've derived BinaryCIFAR10 the following way: 1) All images were converted to grayscale shape (1, 32, 32) using the Pytorch Grayscale transform with default parameters. Then 7469 samples were randomly extracted from 'train' to yield the split: (train: 42531, val: 7469, test: 1000).Class 0 was randomly chosen and BinaryCIFAR10 was obtained by extracting all occurrences of class '0', and randomly sampling an equivalent amount of samples from the rest of the classes in to yield a balanced one-versus-all dataset. The final split is: (train: 8506, val: 1493, test: 2001).\\\\
%\textbf{Permutation generation}: The permuted versions: {0, 250, 500, 750, 1000, 1250, 1500, 1750, 2000, 2250, 2500} were created by setting a random seed, and, sequentially, each time starting from the original order of the features - randomly probing two features and swapping them for the number of swaps specified.\\\\
%\textbf{QE measurements}: performed for each permuted version of the data. Each measurement value (for the permuted versions obtained from a given seed) is the average of all canonical EE measurements starting at level l = L and ending at level l = L - 1 (inclusive). In this experiment, canonical partitions were defined as such: partitions of the first level are the partition obtained by halving the images longitudinally, and the second is the partition obtained by halving the image horizontally. Partitions of the second level are the 4 partitions defined by the 4 patches created when halving the image both longitudinally and horizontally. Rest of the details are just as in \cref{app:experiments:details:emp_demo}.\\\\
%\textbf{Architecture Specifications:}
%
%\begin{itemize}
%	\item \textbf{2D-CNN}: We used a standard CNN according to the following scheme:
%	\[2DCNN(x)=FC(Block(...Block(x))\]
%	\[Block(x)=MaxPool(Activation(BN(Conv2d(x))))\]
%	
%	with hyperparameters:
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		\text{stride} & 1 \\
	%		\text{kernel size} & 5 \\
	%		\text{pooling size} & 2 \\
	%		\text{layers} & 4 \\
	%		\text{n classes} & 2 \\
	%		\text{hidden dim} & 32 \\
	%	\end{tabular}
%\end{itemize}
%
%\textbf{Training and Evaluation}: All models were trained on a single "RTX A6000" GPU. Training was conducted by running 50 epochs on the training data with the following parameters:
%
%\begin{tabular}{l l}
%	\textbf{Hyperparameter} & \textbf{Value} \\
%	\hline
%	\text{Optimizer} & Adam \\
%	\text{Learning Rate} & 0.0001 \\
%	\text{Weight Decay} & 0.0001 \\
%	\text{Loss} & CE \\
%	\text{Batch Size} & 128 \\
%\end{tabular}
%
%After the last training epoch, the model which performed best on the validation set was chosen, and test set performance was averaged over 30 seeds along with the corresponding standard deviation (as envelope).
%
%
%
%
%
%\subsection{Adapting High-Dimensional Spatially Unstructured Data to Deep Learning} 
%
%\cref{sec:enhancing:exp:tab}:\\\\
%
%
%\textbf{Datasets}:The datasets "dna", "semeion" and "isolet" are all from OpenML \citep{OpenML2013}. For each dataset we combined train and test split, we set the seed (globally - for all dependencies used) to seed=1, and randomly split into 3 folds, by dividing the entire data set into 3, and then for fold i using split i as test set, and splits \(\lbrace 1, 2, 3\rbrace\ / \lbrace i\rbrace\) for train and validation. In each fold, the validation set was extracted from the respective train set by randomly sampling one third of its samples.\\\\
%\textbf{Permutations and Rearrangements}:For a given seed, 3 versions of the dataset were created: 'shuffled' (randomly shuffling the features), 'cuts' (rearranging the shuffled version using our method), 'IGTD' (rearranging the shuffled version using IGTD). Note that for each fold, a different permutation was created, and thus also a different rearrangement.\\\\
%Metis parameters: \(\text{ncuts} =10, \text{ niter}=50000, \text{ recursive}=True.\)\\\\
%IGTD parameters: \(\text{S max}=1000,\text{S con}=20, \text{t con}=0.0, \text{ t swap}=0.0.\)\\\\
%\textbf{Architecture Specifications:}
%
%\begin{itemize}
%	\item \textbf{TabularResCNN}: We used a Resnet type architecture according to the following scheme:
%	\[TabResCNN(x)=FC(AvgPool(Block(...(Block(x)))))\]
%	\[Block(x)=Dropout(x + BN(Maxpool(Activation(1DConv(x))))\]
%	with hyperparameters:
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		\text{1d-batch norm} & \\
	%		\text{hidden dim} & 32 \\
	%		\text{layers} & 8 \\
	%		\text{kernel size} & 3 \\
	%		\text{stride} & 3 \\
	%		\text{pool size} & 3 \\
	%		\text{activation} & \text{ReLU} \\
	%	\end{tabular}
%	
%	\item \textbf{S4}: As in the first experiment, but with:
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		\text{hidden dim} & 64 \\
	%		\text{layers} & 4 \\
	%	\end{tabular}
%	
%	\item \textbf{XGBoost}: An off-the-shelf implementation of XGBoost with gpu-histogram building using the dmlc library. Hyperparameters:
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		\text{max depth} & 4 \\
	%		\text{learning rate} & 0.2 \\
	%		\text{n estimators} & 250 \\
	%	\end{tabular}
%	
%	The rest of the hyperparameters are set to their default values.
%	
%	\item \textbf{Local-attention}: Hyperparameters:
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		\text{attention dim} & 128 (with 4 heads of dim 32 each) \\
	%		\text{depth} & 8 \\
	%		\text{MLP-blocks' hidden dim} & 341 \\
	%		\text{local attention window size} & 25 \\
	%	\end{tabular}
%\end{itemize}
%
%\textbf{Training and evaluation}: All models were trained on a single "RTX A6000" GPU. Training was conducted by running 300 epochs on the training data with the following parameters:
%
%\begin{tabular}{l l l}
%	\textbf{Model} & \textbf{Optimizer} & \textbf{Learning Rate} \\
%	\hline
%	TabularResCNN & Adam & 0.001 \\
%	Local-attention & Adam & 0.00005 \\
%	S4 & AdamW & 0.001 \\
%	XGBoost & - & 0.2 \\
%\end{tabular}
%
%
%All models used a batch size of 64, cross-entropy loss (CE), and weight decay of 0.0001 (TabularResCNN) or 0.01 (S4). The learning rate schedule (step-size-scheduler) was set to None for TabularResCNN and Local-attention and CosineAnnealingLR for S4. After the last training epoch, the model which performed best on the validation set was chosen, and test accuracy was measured on the test set and reported as test performance. This was done for each of the folds and then the final test performance was the average performance on the 3 folds. Note that in the case of XGBoost, in-fold validation was not done using the validation set as it is taken care of by the library implementation, therefore, train and val were unified into one dataset and fed into the built-in training function of the library.\
%
%
%
%
%
%
%\label{app:experiments:details:dl_unstr}
%\cref{sec:enhancing:exp:perm_audio}:\\\\
%\textbf{Dataset}: The dataset used is the 35-classes SpeechCommands dataset, obtained in a procedure identical to the one described in above, except we did not binarize, and instead of downsampling to 4096, it was downsampled to 2048. Following this $20\%$ of the samples in each class were kept and the rest discarded. This was repeated separately for the training, validation and testing sets.\\
%
%
%\textbf{Architecture specifications:}
%
%\begin{itemize}
%
%	
%	\item \textbf{S4}: As in the first experiment, but with:
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		\text{hidden dim} & 128 \\
	%		\text{layers} & 4 \\
	%	\end{tabular}
%	
%	
%	\item \textbf{Local-attention}: Hyperparameters:
%	
%	\begin{tabular}{l l}
	%		\textbf{Hyperparameter} & \textbf{Value} \\
	%		\hline
	%		\text{attention dim} & 128 (with 2 heads of dim 64 each) \\
	%		\text{depth} & 4 \\
	%		\text{MLP-blocks' hidden dim} & 341 \\
	%		\text{local attention window size} & 10 \\
	%	\end{tabular}
%
%
%\end{itemize}
%
%
%
%\noindent \textbf{Training and evaluation:} All models were trained on a single "RTX A6000" GPU. Training was conducted by running 200, 450, 200 epochs (for S4, Local-attention, M5 CNN, respectively). After the last training epoch, the model which performed best on the validation set was chosen, and average test accuracy was measured on the test set and reported as test performance. Values reported are averages over 10 seeds and their corresponding standard deviations.
%
%\begin{table}[h]
%	\centering
%	\begin{tabular}{l|l|l|l|l|l}
	%		\textbf{Model} & \textbf{Optimizer} & \textbf{Learning Rate} & \textbf{Weight Decay} & \textbf{Scheduler} & \textbf{Batch Size} \\ \hline
	%		M5 CNN         & Adam              & 0.001                 & 0.0001              & None            & 128           \\
	%		Local-attention & Adam              & 0.0001                & 0                   & None            & 32            \\
	%		S4              & AdamW             & 0.001                 & 0.01                & CosineAnnealingLR & 64
	%	\end{tabular}
%	\caption{Training parameters for the different models}
%	\label{tab:training_parameters_mod_dim}
%\end{table}
%
%\noindent All models use the cross-entropy (CE) loss function.\\\\
%
%
%
%
%
%
%%\textbf{Dataset}: The dataset used is the 35-classes SpeechCommands dataset, obtained in a procedure identical to the one described in above, except we did not binarize, and instead of downsampling to 4096, it was up-sampled to 50k.\\\\
%%\textbf{Sparsification}:A standard algorithm  for spectral sparsification \citep{spielman2011spectral} is used of the full graph of surrogateEE measurements (a complete graph of size 50k) with default parameters, only varying \(\epsilon\) (the sparsification parameter) in \([0.15, 0.25, 0.5, 0.75, 1.0]\). Then, our method for rearrangement was applied on the sparsified correlations graphs with fixed parameters using the Metis graph partitioning algorithm \citep{doi:10.1137/S1064827595287997}  with hyperparameters \(\text{ncuts}=10,\text{ niter} =50000\). The values on the x-axis correspond to the sparsification ratio (edges in sparsified graph over edges in original graph). Note that the first ratio value is 0, which corresponds to no-rearrangement at all. For sparsificaiton, we used the "Laplacians" package written in Julia, and the rest of the code was implemented in Python.\\
%%
%%\textbf{Architecture specifications:}
%%
%%\begin{itemize}
%%	\item \textbf{TabularResCNN}: Our implementation in \cref{fig:tabcnn}, but now with hyperparameters: 
%%	
%%	\begin{tabular}{l l}
	%%		\textbf{Hyperparameter} & \textbf{Value} \\
	%%		\hline
	%%		1d-batch norm & \\
	%%		hidden dim & 32 \\
	%%		layers & 8 \\
	%%		kernel size & 3 \\
	%%		stride & 3 \\
	%%		pool size & 3 \\
	%%		activation & ReLU \\
	%%	\end{tabular}
%%	
%%	\item \textbf{MLP (Small)}: Our implementation of a MLP with 
%%	
%%	\begin{tabular}{l l}
	%%		\textbf{Hyperparameter} & \textbf{Value} \\
	%%		\hline
	%%		1d-batch norm & \\
	%%		activation & ReLU \\
	%%		dimensions & (50K, 500, 50, 35) \\
	%%	\end{tabular}
%%	
%%	\item \textbf{MLP (Large)}: Our implementation of a MLP with 
%%	
%%	\begin{tabular}{l l}
	%%		\textbf{Hyperparameter} & \textbf{Value} \\
	%%		\hline
	%%		1d-batch norm & \\
	%%		activation & ReLU \\
	%%		dimensions & (50K, 25000, 6250, 3125, 35) \\
	%%	\end{tabular}
%%	
%%	\item \textbf{S4}: As in \cref{app:experiments:details:emp_demo}, but with 
%%	
%%	\begin{tabular}{l l}
	%%		\textbf{Hyperparameter} & \textbf{Value} \\
	%%		\hline
	%%		hidden dimension & 32 \\
	%%		layers & 4 \\
	%%	\end{tabular}
%%	
%%	\item \textbf{XGBoost}: An off-the-shelf implementation of XGBoost with GPU-histogram building using the dmlc library. Hyperparameters: 
%%	
%%	\begin{tabular}{l l}
	%%		\textbf{Hyperparameter} & \textbf{Value} \\
	%%		\hline
	%%		max depth & 3 \\
	%%		learning rate & 0.2 \\
	%%		n estimators & 250 \\
	%%	\end{tabular}
%%	
%%	\item \textbf{TabNet (Small)}: An off-the-shelf implementation using the "pytorch-tabnet" package, with hyperparameters 
%%	
%%	\begin{tabular}{l l}
	%%		\textbf{Hyperparameter} & \textbf{Value} \\
	%%		\hline
	%%		n d & 10 \\
	%%		n a & 10 \\
	%%		n steps & 4 \\
	%%		n independent & 2 \\
	%%		n shared & 2 \\
	%%	\end{tabular}
%%	
%%	\item \textbf{TabNet (Large)}: An off-the-shelf implementation using the "pytorch-tabnet" package with hyperparameters 
%%	
%%	\begin{tabular}{l l}
	%%		\textbf{Hyperparameter} & \textbf{Value} \\
	%%		\hline
	%%		n d & 512 \\
	%%		n a & 512 \\
	%%		n steps & 4 \\
	%%		n independent & 2 \\
	%%		n shared & 2 \\
	%%	\end{tabular}
%%\end{itemize}
%%
%%\textbf{Architecture specifications:}
%%
%%\begin{itemize}
%%	\item \textbf{TabularResCNN}: Our implementation in \cref{fig:tabcnn}, but now with hyperparameters: 
%%	
%%	\begin{tabular}{l l}
	%%		\textbf{Hyperparameter} & \textbf{Value} \\
	%%		\hline
	%%		1d-batch norm & \\
	%%		hidden dim & 32 \\
	%%		layers & 8 \\
	%%		kernel size & 3 \\
	%%		stride & 3 \\
	%%		pool size & 3 \\
	%%		activation & ReLU \\
	%%	\end{tabular}
%%	
%%	\item \textbf{MLP (Small)}: Our implementation of a MLP with 
%%	
%%	\begin{tabular}{l l}
	%%		\textbf{Hyperparameter} & \textbf{Value} \\
	%%		\hline
	%%		1d-batch norm & \\
	%%		activation & ReLU \\
	%%		dimensions & (50K, 500, 50, 35) \\
	%%	\end{tabular}
%%	
%%	\item \textbf{MLP (Large)}: Our implementation of a MLP with 
%%	
%%	\begin{tabular}{l l}
	%%		\textbf{Hyperparameter} & \textbf{Value} \\
	%%		\hline
	%%		1d-batch norm & \\
	%%		activation & ReLU \\
	%%		dimensions & (50K, 25000, 6250, 3125, 35) \\
	%%	\end{tabular}
%%	
%%	\item \textbf{S4}: As in \cref{app:experiments:details:emp_demo}, but with 
%%	
%%	\begin{tabular}{l l}
	%%		\textbf{Hyperparameter} & \textbf{Value} \\
	%%		\hline
	%%		hidden dimension & 32 \\
	%%		layers & 4 \\
	%%	\end{tabular}
%%	
%%	\item \textbf{XGBoost}: An off-the-shelf implementation of XGBoost with GPU-histogram building using the dmlc library. Hyperparameters: 
%%	
%%	\begin{tabular}{l l}
	%%		\textbf{Hyperparameter} & \textbf{Value} \\
	%%		\hline
	%%		max depth & 3 \\
	%%		learning rate & 0.2 \\
	%%		n estimators & 250 \\
	%%	\end{tabular}
%%	
%%	\item \textbf{TabNet (Small)}: An off-the-shelf implementation using the "pytorch-tabnet" package, with hyperparameters 
%%	
%%	\begin{tabular}{l l}
	%%		\textbf{Hyperparameter} & \textbf{Value} \\
	%%		\hline
	%%		n d & 10 \\
	%%		n a & 10 \\
	%%		n steps & 4 \\
	%%		n independent & 2 \\
	%%		n shared & 2 \\
	%%	\end{tabular}
%%	
%%	\item \textbf{TabNet (Large)}: An off-the-shelf implementation using the "pytorch-tabnet" package with hyperparameters 
%%	
%%	\begin{tabular}{l l}
	%%		\textbf{Hyperparameter} & \textbf{Value} \\
	%%		\hline
	%%		n d & 512 \\
	%%		n a & 512 \\
	%%		n steps & 4 \\
	%%		n independent & 2 \\
	%%		n shared & 2 \\
	%%	\end{tabular}
%%\end{itemize}
%%
%%\textbf{Training and evaluation:} All models were trained on a single "RTX A6000" GPU. For all architectures, training was conducted by running 50 epochs on the training data. After the last training epoch, the model which performed best on the validation set was chosen, and test accuracy was measured on the test set and reported as test performance.
%%
%%\begin{table}[h]
%%	\centering
%%	\begin{tabular}{l|l|l|l|l|l|l}
	%%		\textbf{Model} & \textbf{Optimizer} & \textbf{Learning Rate} & \textbf{Weight Decay} & \textbf{Scheduler} & \textbf{Loss} & \textbf{Batch Size} \\ \hline
	%%		TabularResCNN  & Adam              & 0.0001                 & 0.0001              & None            & CE     & 64            \\
	%%		MLP (Small)    & Adam              & 0.01                  & 0.0001              & None            & CE     & 32            \\
	%%		MLP (Large)    & Adam              & 0.0001                & 0.0001              & None            & CE     & 32            \\
	%%		S4             & AdamW             & 0.001                 & 0.01                & None            & CE     & 16            \\
	%%		XGBoost        & -                 & 0.2                   & -                   & -               & -      & -             \\
	%%		TabNet (Small) & -                 & -                     & -                   & -               & -      & 512 (VBS 128) \\
	%%		TabNet (Large) & -                 & -                     & -                   & -               & -      & 512 (VBS 128)
	%%	\end{tabular}
%%	\caption{Training parameters for the different models. Hyperparameters with dash (-) denote that the default of the off-the-shelf implementation was used. VBS stands for "virtual batch size".}
%%\end{table}
%
%
%
