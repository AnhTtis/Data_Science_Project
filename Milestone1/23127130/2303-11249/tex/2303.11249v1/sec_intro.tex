\section{Introduction}  \label{sec:intro}

\ifdefined\NEURIPS
\ifdefined\CAMREADY
\def\thefootnote{*}\footnotetext{Equal contribution}\def\thefootnote{\arabic{footnote}}
\fi\fi

Deep learning is delivering unprecedented performance when applied to data modalities involving images, text and audio.
On the other hand, it is known both theoretically and empirically~\citep{shalev2017failures,abbe2018provable} that there exist data distributions over which deep learning utterly fails.
The question of \emph{what makes a data distribution suitable for deep learning} is a fundamental open problem in the field.

A prevalent family of deep learning architectures is that of \emph{locally connected neural networks}.
It includes, among others:
\emph{(i)}~convolutional neural networks, which dominate the area of computer vision; 
\emph{(ii)}~recurrent neural networks, which were the most common architecture for sequence (\eg~text and audio) processing, and are experiencing a resurgence by virtue of S4 models~\citep{gu2022efficiently}; 
and 
\emph{(iii)}~local variants of self-attention neural networks~\citep{tay2021long}.
Conventional wisdom postulates that data distributions suitable for locally connected neural networks are those exhibiting a ``local nature,'' and there have been attempts to formalize this intuition~\citep{zhang2017entanglement,jia2020entanglement,convy2022mutual}.
However, to the best of our knowledge, there are no characterizations providing necessary and sufficient conditions for a data distribution to be suitable to a locally connected neural network.

A seemingly distinct scientific discipline tying distributions and computational models is \emph{quantum physics}.
There, distributions of interest are described by \emph{tensors}, and the associated computational models are \emph{tensor networks}.
While there is shortage in formal tools for assessing the suitability of data distributions to deep learning architectures, there exists a widely accepted theory that allows for assessing the suitability of tensors to tensor networks.
The theory is based on the notion of \emph{quantum entanglement}, which quantifies dependencies that a tensor admits under partitions of its axes (for a given tensor~$\A$ and a partition of its axes to sets $\K$ and~$\K^c$, the entanglement is a non-negative number quantifying the dependence that~$\A$ induces between $\K$ and~$\K^c$).

In this paper, we apply the foregoing theory to a tensor network equivalent to a certain locally connected neural network, and derive theorems by which fitting a tensor is possible if and only if the tensor admits low entanglement under certain \emph{canonical partitions} of its axes.
We then consider the tensor network in a machine learning context, and find that its ability to attain low approximation error, \ie~to express a solution with low population loss, is determined by its ability to fit a particular tensor defined by the data distribution, whose axes correspond to features.
Combining the latter finding with the former theorems, we conclude that a \emph{locally connected neural network is capable of accurate prediction over a data distribution if and only if the data distribution admits low entanglement under canonical partitions of features}.
Experiments with different datasets corroborate this conclusion, showing that the accuracy of common locally connected neural networks (including modern convolutional, recurrent, and local self-attention neural networks) is inversely correlated to the entanglement under canonical partitions of features in the data (the lower the entanglement, the higher the accuracy, and vice versa).

The above results bring forth a recipe for enhancing the suitability of a data distribution to locally connected neural networks: given a dataset, search for an arrangement of features which leads to low entanglement under canonical partitions, and then arrange the features accordingly.
Unfortunately, the above search is computationally prohibitive.
However, if we employ a certain correlation-based measure as a surrogate for entanglement, \ie~as a gauge for dependence between sides of a partition of features, then the search converts into a succession of \emph{minimum balanced cut} problems, thereby admitting use of well-established graph theoretical tools, including ones designed for large scale~\citep{karypis1998fast,spielman2011spectral}.
We empirically evaluate this approach on various datasets, demonstrating that it substantially improves prediction accuracy of common locally connected neural networks (including modern convolutional, recurrent, and local self-attention neural networks).

The data modalities to which deep learning is most commonly applied~---~namely ones involving images, text and audio~---~are often regarded as natural (as opposed to, for example, tabular data fusing heterogeneous information).
We believe the difficulty in explaining the suitability of such modalities to deep learning may be due to a shortage in tools for formally reasoning about natural data.
Concepts and tools from physics~---~a branch of science concerned with formally reasoning about natural phenomena~---~may be key to overcoming said difficulty.
We hope that our use of quantum entanglement will encourage further research along this line.

\medskip

The remainder of the paper is organized as follows.
Section~\ref{sec:relate} reviews related work.
Section~\ref{sec:prelim} establishes preliminaries, introducing tensors, tensor networks and quantum entanglement.
Section~\ref{sec:fit_tensor} presents the theorems by which a tensor network equivalent to a locally connected neural network can fit a tensor if and only if this tensor admits low entanglement under canonical partitions of its axes. 
Section~\ref{sec:accurate_predict} employs the preceding theorems to show that in a classification setting, accurate prediction is possible if and only if the data admits low entanglement under canonical partitions of features.
Section~\ref{sec:enhancing} translates this result into a practical method for enhancing the suitability of data to common locally connected neural networks.
Finally, Section~\ref{sec:conc} concludes.
For simplicity, we treat locally connected neural networks whose input data is one-dimensional (\eg~text and audio), and defer to~\cref{app:extension_dims} an extension of the analysis and experiments to models intaking data of arbitrary dimension (\eg~two-dimensional images).
