\section{Enhancing Suitability of Data to Locally Connected Neural Networks} \label{sec:enhancing}	

Our analysis (\cref{sec:fit_tensor,sec:accurate_predict}) suggests that a data distribution is suitable for locally connected neural networks if and only if it admits low entanglement under canonical partitions of features.
Motivated by this observation, we derive a preprocessing algorithm aimed to enhance the suitability of a data distribution to locally connected neural networks (\cref{sec:enhancing:search,sec:enhancing:practical}).
Empirical evaluations demonstrate that it significantly improves prediction accuracies of common locally connected neural networks on various datasets (\cref{sec:enhancing:exp}).
For conciseness, the treatment in this section is limited to one-dimensional (sequential) models and data; see~\cref{app:extension_dims:enhancing} for an extension to arbitrary dimensions.

 % Search for Feature Arrangement With Low Entanglement Under Canonical Partitions
\subsection{Search for Feature Arrangement With Low Entanglement Under Canonical Partitions}  \label{sec:enhancing:search}

Our analysis naturally leads to a recipe for enhancing the suitability of a data distribution to locally connected neural networks: given a dataset, search for an arrangement of features which leads to low entanglement under canonical partitions, and then arrange the features accordingly.
Formally, suppose we have $M \in \N$ training instances $\brk[c]1{ \brk1{ \brk{ \xbf^{(1,m)}, \ldots, \xbf^{(N,m)}  } , y^{(m)}} }_{m = 1}^M$, where $y^{(m)} \in \{1, -1\}$ and $\xbf^{(n, m)} \in \R^D$ for $n \in [N], m \in [M]$, with $D \in \N$.
Assume without loss of generality that $N$ is a power of two (if this is not the case we may add constant features as needed).
The aforementioned recipe boils down to a search for a permutation $\perm : [N] \to [N]$, which when applied to feature indices leads the empirical data tensor $\datatensor$ (\cref{eq:data_tensor}) to admit low entanglement under the canonical partitions of its axes (\cref{def:canonical_partitions}).

A greedy realization of the foregoing search is as follows.
Initially, partition the features into two equally sized sets $\K_{1,1} \subset [N]$ and $\K_{1,2} := [N] \setminus \K_{1,1}$ such that the entanglement of $\datatensor$ with respect to $(\K_{1,1}, \K_{1,2})$ is minimal.
That is, find $\K_{1,1} \in \argmin_{\K \subset [N] , \abs{\K} = N / 2 } \qe{\datatensor}{\K}$.
The permutation $\pi$ will map $\K_{1,1}$ to coordinates $\{1, \ldots, \frac{N}{2} \}$ and $\K_{1,2}$ to $\{ \frac{N}{2} + 1, \ldots, N \}$.
Then, partition $\K_{1,1}$ into two equally sized sets $\K_{2,1} \subset \K_{1,1}$ and $\K_{2, 2} := \K_{1,1} \setminus \K_{2,1}$ such that the average of entanglements induced by these sets is minimal, \ie~$\K_{2,1} \in \argmin_{\K \subset \K_{1,1} , \abs{\K} = \abs{\K_{1,1}} / 2} \frac{1}{2} \big [ \qe{\datatensor}{\K} + \qe{\datatensor}{\K_{1, 1} \setminus \K} \big ]$.
The permutation $\pi$ will map $\K_{2,1}$ to coordinates $\{1, \ldots, \frac{N}{4} \}$ and $\K_{2,2}$ to $\{\frac{N}{4} + 1, \ldots, \frac{N}{2} \}$.
A partition of $\K_{1,2}$ into two equally sized sets $\K_{2, 3}$ and $\K_{2, 4}$ is obtained similarly, where $\pi$ will map $\K_{2,3}$ to coordinates $\{\frac{N}{2} + 1, \ldots, \frac{3N}{4} \}$ and $\K_{2,4}$ to $\{ \frac{3N}{4} + 1, \ldots, N \}$.
Continuing in the same fashion, until we reach subsets $\K_{L, 1}, \ldots, \K_{L, N}$ consisting of a single feature index each, fully specifies the permutation $\pi$.

Unfortunately, the step lying at the heart of the above scheme~---~finding a balanced partition that minimizes average entanglement~---~is computationally prohibitive, and we are not aware of any tools that alleviate the computational difficulty.
In the next subsection we will see that replacing entanglement with a surrogate measure paves way to a practical implementation.


  % PRACTICAL ALGORITHM VIA SURROGATE FOR ENTANGLEMENT
\subsection{Practical Algorithm via Surrogate for Entanglement} \label{sec:enhancing:practical}

To efficiently implement the scheme from~\cref{sec:enhancing:search}, we replace entanglement with a surrogate measure of dependence.
The surrogate is based on the Pearson correlation coefficient for multivariate features~\citep{puccetti2022measuring},\footnote{
	For completeness,~\cref{app:pearson} provides a formal definition of the multivariate Pearson correlation.
} and its agreement with entanglement is demonstrated empirically  in~\cref{app:ent_surrogate}.
Theoretically supporting this agreement is left for future work.

\begin{definition}
\label{def:surrogate_entanglement}
Given a set of $M \in \N$ instances $\X := \brk[c]{ \brk{ \xbf^{(1,m)}, \ldots, \xbf^{(N, m) } } \in (\R^D)^N }_{m = 1}^M$, denote by $\pear_{n, n'}$ the multivariate Pearson correlation between features $n, n' \in [N]$.
For $\K \subseteq [N]$, the \emph{surrogate entanglement} of $\X$ with respect to the partition $(\K, \K^c)$, denoted $\se{ \X }{ \K }$, is the sum of Pearson correlation coefficients between pairs of features, the first belonging to $\K$ and the second to $\K^c := [N] \setminus \K$.
That is:
\[
\sebig{ \X }{ \K } := \sum\nolimits_{n \in \K, n' \in \K^c} \pear_{n, n'}
\text{\,.}
\]
\end{definition}

As shown in~\cref{prop:min_balanced_cut} below, replacing entanglement with surrogate entanglement in the scheme from~\cref{sec:enhancing:search} converts each search for a balanced partition minimizing average entanglement into a \emph{minimum balanced cut problem}.
Although the minimum balanced cut problem is NP-hard (see, \eg,~\cite{garey1979computers}), it enjoys a wide array of well-established approximation tools, particularly ones designed for large scale~\citep{karypis1998fast,spielman2011spectral}.
We therefore obtain a practical algorithm for enhancing the suitability of a data distribution to locally connected neural networks~---~see~\cref{alg:ent_struct_search}.

\begin{proposition}
	\label{prop:min_balanced_cut}
	For any $\widebar{\K} \subseteq [N]$ of even size, the following optimization problem can be framed as a minimum balanced cut problem over a complete graph with $\abs{\widebar{\K}}$ vertices:
	\be
	\min_{\K \subset \widebar{\K} , \abs{ \K } = \abs{\widebar{\K}} / 2} \frac{1}{2} \brk[s]2{ \sebig{ \X }{ \K } + \sebig{ \X }{ \widebar{\K} \setminus \K  } }
	\text{\,.}
	\label{eq:balanced_part_min_surr_entanglement}
	\ee
	Specifically, there exists a complete undirected weighted graph with vertices $\widebar{\K}$ and edge weights $w : \widebar{\K} \times \widebar{\K} \to \R$ such that for any $\K \subset \widebar{\K}$, the weight of the cut in the graph induced by $\K$~---~$\sum\nolimits_{n \in \K , n' \in \widebar{\K} \setminus \K } w( \{ n, n' \})$~---~is equal, up to an additive constant, to the term minimized in~\cref{eq:balanced_part_min_surr_entanglement}, \ie~to $\frac{1}{2} \brk[s]1{ \sebig{ \X }{ \K } + \sebig{ \X }{ \widebar{\K} \setminus \K  } }$.
\end{proposition}

\begin{proof}
	Consider the complete undirected graph whose vertices are $\widebar{\K}$ and where the weight of an edge $\{n, n'\} \in \widebar{\K} \times \widebar{\K}$ is $w ( \{ n, n' \} ) = p_{n, n'}$ (recall that $p_{n, n'}$ stands for the multivariate Pearson correlation between features $n$ and $n'$ in $\X$).
	For any $\K \subset \widebar{\K}$ it holds that:
	\[
		\sum\nolimits_{n \in \K , n' \in \widebar{\K} \setminus \K } w( \{ n, n' \}) = \frac{1}{2} \brk[s]2{ \sebig{ \X }{ \K } + \sebig{ \X }{ \widebar{\K} \setminus \K  } } - \frac{1}{2}\sebig{ \X }{ \widebar{\K} }
		\text{\,,}
	\]
	where $\frac{1}{2}\se{ \X }{ \widebar{\K} }$ does not depend on $\K$.
	This concludes the proof.
\end{proof}


\begin{algorithm}[t!]
	\caption{Enhancing Suitability of Data to Locally Connected Neural Networks} 
	\label{alg:ent_struct_search}	
	\begin{algorithmic}[1]
		\STATE \!\textbf{Input:} $\X := \brk[c]{ \brk{ \xbf^{(1,m)}, \ldots, \xbf^{(N, m)} } }_{m = 1}^M$~---~$M \in \N$ data instances comprising $N \in \N$ features \\[0.4em]
		\STATE \!\textbf{Output:} Permutation $\perm : [N] \to [N]$ to apply to feature indices \\[-0.2em]
		\hrulefill
		\vspace{1mm}
		\STATE Let $\K_{0, 1} := [N]$ and denote $L := \log_2 (N)$
		\vspace{1mm}
		\STATE \darkgray{\# We assume for simplicity that $N$ is a power of two, otherwise one may add constant features}
		\vspace{1mm}
		\FOR{$l = 0, \ldots, L - 1 ~,~ n = 1, \ldots, 2^{l}$}
		\vspace{1mm}
		\STATE Using a reduction to a minimum balanced cut problem (\cref{prop:min_balanced_cut}), find an approximate solution $\K_{l + 1, 2n - 1} \subset \K_{l , n}$ for:
		\[
			\min\nolimits_{ \K \subset \K_{l, n} , \abs{\K} = \abs{ \K_{l, n} } / 2 } \frac{1}{2} \brk[s]*{ \se{\X}{ \K } + \se{\X}{ \K_{l, n} \setminus \K } }
		\]
		\vspace{-2mm}
		\STATE Let $\K_{l + 1, 2n} := \K_{l, n} \setminus \K_{l + 1, 2n - 1}$
		\vspace{1mm}
		\ENDFOR
		\vspace{1mm}
		\STATE \darkgray{\# At this point, $\K_{L, 1}, \ldots, \K_{L, N}$ each contain a single feature index}
		\vspace{0.5mm}
		\STATE \textbf{return} $\perm$ that maps $k \in \K_{L, n}$ to $n$, for every $n \in [N]$
	\end{algorithmic}
\end{algorithm}



  % EXPERIMENTS
\subsection{Experiments} \label{sec:enhancing:exp}

We empirically evaluate~\cref{alg:ent_struct_search} using common locally connected neural networks~---~a convolutional neural network, an S4 (popular recurrent neural network; see~\citep{gu2022efficiently}, and a local self-attention model~\citep{tay2021long}~---~over randomly permuted audio datasets (\cref{sec:enhancing:exp:perm_audio}) and several tabular datasets (\cref{sec:enhancing:exp:tab}).
%Additional experiments with two-dimensional data are given in~\cref{app:extension_dims:enhancing:exp}.
For brevity, we defer some implementation details to~\cref{app:experiments:details}.


% RANDOMLY PREMUTED AUDIO DATASETS 
\subsubsection{Randomly Permuted Audio Datasets} \label{sec:enhancing:exp:perm_audio}

\cref{sec:accurate_predict:emp_demo} demonstrated that audio data admits low entanglement under canonical partitions of features, and that randomly permuting the position of features leads this entanglement to increase, while substantially degrading the prediction accuracy of locally connected neural networks.
A sensible test for~\cref{alg:ent_struct_search} is to evaluate its ability to recover performance lost due to the random permutation of features.

For the Speech Commands dataset~\citep{warden2018speech},~\cref{tab:audio_moderate_dim} compares the prediction accuracies of locally connected neural networks on: \emph{(i)} the data subject to a random permutation of features; \emph{(ii)} the data attained after rearranging the randomly permuted features via~\cref{alg:ent_struct_search}; and \emph{(iii)} the data attained after rearranging the randomly permuted features via IGTD~\citep{zhu2021converting}~---~a heuristic scheme designed for convolutional neural networks (see~\cref{sec:relate}).
As can be seen, \cref{alg:ent_struct_search} leads to significant improvements, surpassing those brought forth by IGTD.
%Note that~\cref{alg:ent_struct_search} does not entirely recover the performance lost due to the random permutation of features.\footnote{
%	The prediction accuracies on the original data are ${82.6}~\pm$, ${89.4}~\pm$ and ${76.8}~\pm$ for CNN, S4 and Local-Attention, respectively.
%}
Note that~\cref{alg:ent_struct_search} does not entirely recover the performance lost due to the random permutation of features.\footnote{
	The prediction accuracies on the original data are $59.8~\pm~2.6$, $69.6~\pm~0.6$ and $48.1~\pm~2.1$ for CNN, S4 and Local-Attention, respectively.
}
We believe this relates to phenomena outside the scope of the theory underlying~\cref{alg:ent_struct_search} (\cref{sec:fit_tensor,sec:accurate_predict}), for example translation invariance in data being beneficial in terms of generalization.  
Investigation of such phenomena and suitable modification of~\cref{alg:ent_struct_search} are regarded as promising directions for future work. 

%The number of features in the audio dataset used in the experiment above is 2048.  
%We demonstrate the scalability of~\cref{alg:ent_struct_search} by including in~\cref{app:experiments:further} an experiment with audio data with 50,000 features.  
%In this experiment, instances of the minimum balanced cut problem entail graphs with up to $25 \cdot 10^8$ edges.  
%They are solved using the edge sparsification algorithm of~\citep{spielman2011spectral}, which allows for configurable compute and memory consumption (the more resources are consumed, the more accurate the solution will be).


\begin{table}[t]
	\caption{
		Arranging features of randomly permuted audio data via~\cref{alg:ent_struct_search} significantly improves the prediction accuracies of locally connected neural networks.
		Reported are test accuracies (mean and standard deviation over ten random seeds) of a convolutional neural network (CNN), S4 (a popular recurrent neural network; see~\citep{gu2022efficiently}), and a local self-attention model~\citep{tay2021long}, over the Speech Commands dataset~\citep{warden2018speech} subject to different arrangements of features: \emph{(i)} a random arrangement; \emph{(ii)} an arrangement provided by applying~\cref{alg:ent_struct_search} to the random arrangement; and \emph{(iii)} an arrangement provided by applying an adaptation of IGTD~\citep{zhu2021converting}~---~a heuristic scheme designed for convolutional neural networks~---~to the random arrangement.
		For each model, we highlight (in boldface) the highest mean accuracy if the difference between that and the second-highest mean accuracy is statistically significant (namely, is larger than the sum of corresponding standard deviations).
		As can be seen,~\cref{alg:ent_struct_search} leads to significant improvements in prediction accuracies, surpassing the improvements brought forth by IGTD.
		See~\cref{app:experiments:details} for implementation details.
	}
	\begin{center}
	\small
	\vspace{1mm}
	\begin{tabular}{lccc}
		\toprule
		& Randomly Permuted & \cref{alg:ent_struct_search} & IGTD \\
		\midrule
		CNN & ${5.2}~\pm$ \scriptsize{${0.7}$} & ${\mathbf{17.4}}~\pm$ \scriptsize{${1.7}$} & ${6.1}~\pm$ \scriptsize{${0.4}$} \\
		S4 & ${9.5}~\pm$ \scriptsize{${0.6}$} &  $\mathbf{30.3}~\pm$ \scriptsize{${1.6}$} & ${13}~\pm$ \scriptsize{${2.4}$} \\
		Local-Attention & ${7.8}~\pm$ \scriptsize{${0.3}$} &  $\mathbf{12.9}~\pm$ \scriptsize{${0.7}$} & ${6.4}~\pm$ \scriptsize{${0.5}$}  \\
		\bottomrule
	\end{tabular}	
	\end{center}
	\label{tab:audio_moderate_dim}
	\vspace{-2mm}
\end{table}


% TABULAR DATASETS
\subsubsection{Tabular Datasets}  \label{sec:enhancing:exp:tab}

The prediction accuracies of locally connected neural networks on tabular data, \ie~on data in which features are arranged arbitrarily, is known to be subpar~\citep{shwartz2022tabular}.
\cref{tab:tabular_datasets} reports results of experiments with locally connected neural networks over standard tabular benchmarks (namely “dna'', “semeion'' and “isolet''~\cite{OpenML2013}), demonstrating that arranging features via~\cref{alg:ent_struct_search} leads to significant improvements in prediction accuracies, surpassing improvements brought forth by IGTD (a heuristic scheme designed for convolutional neural networks~\cite{zhu2021converting}). 
Note that~\cref{alg:ent_struct_search} does not lead to state of the art prediction accuracies on the evaluated benchmarks.\footnote{
	XGBoost for example achieves prediction accuracies $96$, $91$ and $95.2$ over dna, semeion and isolet, respectively.
} 
However, the results suggest that it renders locally connected neural networks a viable option for tabular data.  
This option is particularly appealing in when the number of features is large settings, where many alternative approaches (\eg~ones involving fully connected neural networks) are impractical.

\begin{table}[t]
	\caption{
		Arranging features of tabular datasets via~\cref{alg:ent_struct_search} significantly improves the prediction accuracies of locally connected neural networks.
		Reported are results of experiments analogous to those of~\cref{tab:audio_moderate_dim}, but with the “dna'', “semeion'' and “isolet'' tabular classification datasets~\citep{OpenML2013}.
		Since to the arrangement of features in a tabular dataset is intended to be arbitrary, we regard as a baseline the prediction accuracies attained with a random permutation of features.
		For each combination of dataset and model, we highlight (in boldface) the highest mean accuracy if the difference between that and the second-highest mean accuracy is statistically significant (namely, is larger than the sum of corresponding standard deviations).
		Notice that, as in the experiment of~\cref{tab:audio_moderate_dim}, rearranging the features according to~\cref{alg:ent_struct_search} leads to significant improvements in prediction accuracies, surpassing the improvements brought forth by IGTD.
		See~\cref{app:experiments:details} for implementation details.
	}
	\vspace{0.5mm}
	\begin{center}
		\small
		\begin{tabular}{lccc}
			\multicolumn{4}{l}{Dataset: dna} \\[0.2em]
			\toprule
			& Baseline & \cref{alg:ent_struct_search} & IGTD \\
			\midrule
			CNN & $81.1~\pm$ \scriptsize{$2.2$} & $\mathbf{91.1}~\pm$ \scriptsize{$0.7$} & $86.9~\pm$ \scriptsize{$0.7$}\\
			S4 & $87.7~\pm$ \scriptsize{$2.3$} & $89.8~\pm$ \scriptsize{$2.9$} & $90~\pm$ \scriptsize{$1.2$} \\
			Local-Attention & $77.5~\pm$ \scriptsize{$3.6$} & $\mathbf{85.5}~\pm$ \scriptsize{$3.6$} & $81~\pm$ \scriptsize{$2.5$} \\
			%			MLP & $93~\pm$ \scriptsize{$1$} & $93~\pm$ \scriptsize{$0$} & $93~\pm$ \scriptsize{$0$} \\
			%			XGBoost & $96.2~\pm$ \scriptsize{$0$} & $96.2~\pm$ \scriptsize{$0$} & $96.2~\pm$ \scriptsize{$0$} \\
			%			TabNet & $82~\pm$ \scriptsize{$2$} & $82~\pm$ \scriptsize{$2$} & $82~\pm$ \scriptsize{$2$} \\			
			\bottomrule					
			\vspace{-2mm}
		\end{tabular}
		\begin{tabular}{lccc}
			\multicolumn{4}{l}{Dataset: semeion} \\[0.3em]
			\toprule
			& Baseline & \cref{alg:ent_struct_search} & IGTD \\
			\midrule
			CNN & $77.5~\pm$ \scriptsize{$1.8$} & $80.7~\pm$ \scriptsize{$1$} & $80.2~\pm$ \scriptsize{$1.8$}\\
			S4 & $82.6~\pm$ \scriptsize{$1.1$} & $\mathbf{89.8}~\pm$ \scriptsize{$0.5$} & $85.9~\pm$ \scriptsize{$0.7$} \\
			Local-Attention & $60.6~\pm$ \scriptsize{$3.8$} & $\mathbf{78.6}~\pm$ \scriptsize{$1.3$} & $68~\pm$ \scriptsize{$0.9$} \\
			%		MLP & $90~\pm$ \scriptsize{$0$} & $90~\pm$ \scriptsize{$0$} & $90~\pm$ \scriptsize{$0$} \\
			%		XGBoost & $91.7~\pm$ \scriptsize{$0$} & $91.7~\pm$ \scriptsize{$0$} & $91.7~\pm$ \scriptsize{$0$} \\
			%		TabNet & $50~\pm$ \scriptsize{$3$} & $50~\pm$ \scriptsize{$3$} & $50~\pm$ \scriptsize{$3$} \\			
			\bottomrule
			\vspace{-2mm}
		\end{tabular}	
			\begin{tabular}{lccc}
		\multicolumn{4}{l}{Dataset: isolet} \\[0.3em]
		\toprule
		& Baseline & \cref{alg:ent_struct_search} & IGTD \\
		\midrule
		CNN & $91.6~\pm$ \scriptsize{$0.4$} & $92.5~\pm$ \scriptsize{$0.4$} & $90.5~\pm$ \scriptsize{$2.2$}\\
		S4 & $92~\pm$ \scriptsize{$0.3$} & $93.3~\pm$ \scriptsize{$0.5$} & $92.8~\pm$ \scriptsize{$0.3$} \\
		Local-Attention & $78~\pm$ \scriptsize{$2.0$} & $\mathbf{87.7}~\pm$ \scriptsize{$0.4$} & $83.9~\pm$ \scriptsize{$0.8$} \\
		%		MLP & $90~\pm$ \scriptsize{$0$} & $90~\pm$ \scriptsize{$0$} & $90~\pm$ \scriptsize{$0$} \\
		%		XGBoost & $91.7~\pm$ \scriptsize{$0$} & $91.7~\pm$ \scriptsize{$0$} & $91.7~\pm$ \scriptsize{$0$} \\
		%		TabNet & $50~\pm$ \scriptsize{$3$} & $50~\pm$ \scriptsize{$3$} & $50~\pm$ \scriptsize{$3$} \\			
		\bottomrule
	\end{tabular}	
	\end{center}
	\vspace{-2mm}
	\label{tab:tabular_datasets}
\end{table}

