\section{Preliminaries} \label{sec:prelim}


  % NOTATION
%\subsection{Notation}  \label{sec:prelim:notation}

\paragraph*{Notation}
We use $\norm{\cdot}$ and $\inprodnoflex{\cdot}{\cdot}$ to denote the Euclidean (Frobenius) norm and inner product, respectively.
We shorthand $[N] := \{ 1, \ldots, N \}$, where $N \in \N$.
The complement of $\K \subseteq [N]$ is denoted by $\K^c$, \ie~$\K^c := [N] \setminus \K$.
%We denote vectors using lowercase bold font, \eg~$\wbf \in \R^D$, matrices using uppercase bold font, \eg~$\Wbf \in \R^{D \times D'}$, and tensors using caligraphic letters, \eg~$\W \in \R^{D_1 \times \cdots \times m_n}$.
%Parenthesized superscripts denote elements in a collection of vectors, matrices, or tensors,~\eg~$\brk[c]{\wbf^{(n)} \in \R^D}_{ n = 1}^N$, while subscripts refer to entries, \eg~$\Wbf_{i,j} \in \R$ is the $(i,j)$'th entry of $\Wbf \in \R^{D \times D'}$.
%A colon indicates all entries in an axis, \eg~$\Wbf_{i, :} \in \R^{D'}$ is the $i$'th  row and $\Wbf_{:, j} \in \R^{D}$ is the $j$'th column of~$\Wbf$.


  % TENSORS AND TENSOR NETWORKS
\subsection{Tensors and Tensor Networks}  \label{sec:prelim:tensor}

For our purposes, a \emph{tensor} is an array with multiple axes $\A \in \R^{D_1 \times \cdots \times D_N}$, where $N \in \N$ is its \emph{order} and $D_1, \ldots, D_N \in \N$ are its \emph{axes lengths}.
The $(d_1, \ldots, d_N)$'th entry of $\A$ is denoted $\A_{d_1, \ldots, d_N}$.

\emph{Contraction} between tensors is a generalization of multiplication between matrices.
Two matrices $\Abf \in \R^{D_1 \times D_2}$ and $\Bbf \in \R^{D'_1 \times D'_2}$ can be multiplied if $D_2 = D'_1$, in which case we get a matrix in $\R^{D_1 \times D'_2}$ holding $\sum\nolimits_{d = 1}^{D_2} \Abf_{d_1, d} \cdot \Bbf_{d, d'_2}$ in entry $(d_1, d'_2) \in [D_1] \times [D'_2]$.
More generally, two tensors $\A \in \R^{D_1 \times \cdots \times D_N}$ and $\B \in \R^{D'_1 \times \cdots \times D'_{N'}}$ can be contracted along axis $n \in [N]$ of $\A$ and $n' \in [N']$ of $\B$ if $D_n = D'_{n'}$, in which case we get a tensor in $\R^{D_1 \times \cdots D_{n - 1} \times D_{n + 1} \times \cdots \times D_N \times D'_1 \times \cdots \times D'_{n' - 1} \times D'_{n' + 1} \cdots \times D'_{N'}}$ holding:
\[
\sum\nolimits_{d = 1}^{D_n} \A_{d_1, \ldots, d_{n - 1}, d, d_{n + 1}, \ldots, d_N} \cdot \B_{d'_1, \ldots, d'_{n' - 1}, d, d'_{n' + 1}, \ldots, d'_{N'} }
\text{\,,}
\]
in the entry indexed by $\brk[c]{ d_k \in [D_k] }_{k \in [N] \setminus \{n\}}$ and $\brk[c]{ d'_{k} \in [D'_k] }_{k \in [N'] \setminus \{n'\}}$. 

\begin{figure*}[t]
	\vspace{0mm}
	\begin{center}
		\hspace*{3mm}
		\includegraphics[width=1\textwidth]{figs/tensor_network_examples.pdf}
	\end{center}
	\vspace{-2mm}
	\caption{
		Tensor networks form a graphical language for fitting (\ie~representing) tensors through tensor contractions.
		\textbf{Tensor network definition:} Every node in a tensor network is associated with a tensor, whose order is equal to the number of edges emanating from the node.
		An edge connecting two nodes specifies contraction between the tensors associated with the nodes (\cref{sec:prelim:tensor}), where the weight of the edge signifies the respective axes lengths.
		Tensor networks may also contain open edges, \ie~edges that are connected to a node on one side and are open on the other.
		The number of such open edges is equal to the order of the tensor produced by contracting the tensor network.
		\textbf{Illustrations:} Presented are exemplar tensor network diagrams of: \textbf{(a)} an order $N$ tensor $\A \in \R^{D_1 \times \cdots \times D_N}$; \textbf{(b)} a vector-matrix multiplication between $\Mbf \in \R^{D_1 \times D_2}$ and $\vbf \in \R^{D_2}$, which results in the vector $\Mbf \vbf \in \R^{D_1}$; and \textbf{(c)} a more elaborate tensor network generating $\W \in \R^{D_1 \times D_2 \times D_3}$.
	}
	\label{fig:tn_examples}
\end{figure*}

\emph{Tensor networks} are prominent computational models for fitting (\ie~representing) tensors.
More specifically, a tensor network is a weighted graph that describes formation of a (typically high-order) tensor via contractions between (typically low-order) tensors.
As customary (\cf~\cite{orus2014practical}), we will present tensor networks via graphical diagrams to avoid cumbersome notation~---~see~\cref{fig:tn_examples} for details.

 
 % QUANTUM ENTANGLEMENT
\subsection{Quantum Entanglement}  \label{sec:prelim:qe}

In quantum physics, the distribution of possible states for a multi-particle (‘‘many body'') system is described by a tensor, whose axes are associated with individual particles.
A key property of the distribution is the dependence it admits under a given partition of the particles (\ie~between a given set of particles and its complement).
This dependence is formalized through the notion of \emph{quantum entanglement}, defined using the distribution's description as a tensor~---~see~\cref{def:entanglement} below.

Quantum entanglement lies at the heart of a widely accepted theory which allows assessing the ability of a tensor network to fit a given tensor (\cf~\cite{cui2016quantum,levine2018deep}).
In~\cref{sec:fit_tensor} we specialize this theory to a tensor network equivalent to a certain locally connected neural network.
The specialized theory will be used (in~\cref{sec:accurate_predict}) to establish our main theoretical contribution: a necessary and sufficient condition for when the locally connected neural network is capable of accurate prediction over a data distribution.

\begin{definition} \label{def:entanglement}
For a tensor $\A \in \R^{D_1 \times \cdots \times D_N}$ and subset of its axes $\K \subseteq [N]$, let $\mat{\A}{\K} \in \R^{ \prod_{n \in \K} D_n \times \prod_{n \in \K^c} D_n }$ be the arrangement of $\A$ as a matrix where rows correspond to axes $\K$ and columns correspond to the remaining axes $\K^c := [N] \setminus \K$.
%\footnote{
%	Denote the elements in $\I$ by $i_1 < \cdots < i_{\abs{\I}}$ and those in $\I^c$ by $j_1 < \cdots < j_{ \abs{\I^c} }$.
%	Then, $\mat{\A}{\I}$ holds the entries of $\A$ such that $\A_{d_1, \ldots, m_n}$ is placed in row $1 + \sum_{l = 1}^{\abs{\I}} (d_{i_{l}} - 1) \prod_{l' = l + 1}^{\abs{\I}} D_{i_{l'}}$ and column $1 + \sum_{l = 1}^{ \abs{\I^c} } ( d_{\struct^{-1} ( j_{l}) } - 1 ) \prod_{l' = l + 1}^{ \abs{\I^c} } D_{ \struct^{-1} (j_{l'}) }$.
%}
Denote by $\sigma_1 \geq \cdots \geq \sigma_{D_{\K}} \in \R_{\geq 0}$ the singular values of $\mat{\A}{\K}$, where $D_{\K} := \min \brk[c]{ \prod_{n \in \K} D_n , \prod_{n \in \K^c} D_n }$.
The \emph{quantum entanglement}\footnote{
	There exist multiple notions of entanglement in quantum physics (see, \eg,~\cite{levine2018deep}).
	The one we consider is the most common, known as \emph{entanglement entropy}.
} of $\A$ with respect to the partition $\brk*{ \K, \K^c}$ is the entropy of the distribution $\brk[c]{ \rho_d := \sigma_d^2 / \sum\nolimits_{d' = 1}^{D_{\K}} \sigma_{d'}^2 }_{d = 1}^{D_{\K}}$, \ie:
\[
\qe{\A}{\K} := - \sum\nolimits_{d = 1}^{D_{\K}} \rho_d \ln (\rho_d)
\text{\,.}
\]
By convention, if $\A = 0$ then $\qe{\A}{\K} = 0$.
\end{definition} 
