\section{Low Entanglement Under Canonical Partitions Is Necessary and Sufficient for Fitting Tensor} \label{sec:fit_tensor}

In this section, we prove that a tensor network equivalent to a certain locally connected neural network can fit a tensor if and only if the tensor admits low entanglement under certain canonical partitions of its axes.
We begin by introducing the tensor network (\cref{sec:fit_tensor:tn_lc_nn}).
Subsequently, we establish the necessary and sufficient condition required for it to fit a given tensor (\cref{sec:fit_tensor:nec_and_suf}).
For conciseness, the treatment in this section is limited to one-dimensional (sequential) models; see~\cref{app:extension_dims:fit_tensor} for an extension to arbitrary dimensions.


 % TENSOR NETWORK EQUIVALENT TO A LOCALLY CONNECTED NN
\subsection{Tensor Network Equivalent to a Locally Connected Neural Network} \label{sec:fit_tensor:tn_lc_nn}

Let $N \in \N$, and for simplicity suppose that $N = 2^L$ for some $L \in \N$.
We consider a tensor network with an underlying perfect binary tree graph of height~$L$, which generates $\tntensor \in \R^{D_1 \times \cdots \times D_N}$.
\cref{fig:tn_as_nn}(a) provides its diagrammatic definition.
Notably, the lengths of axes corresponding to inner (non-open) edges are taken to all be equal to some $R \in \N$, referred to as the \emph{width} of the tensor network.\footnote{
	We treat the case where all axes corresponding to inner (non-open) edges in the tensor network have the same length merely for simplicity of presentation.
	Extension of our theory to axes of different lengths is straightforward.
}


\begin{figure*}[t!]
	\vspace{0mm}
	\begin{center}
		\includegraphics[width=1\textwidth]{figs/tn_as_nn_with_wtn_copy.pdf}
	\end{center}
	\vspace{-2mm}
	\caption{
		The analyzed tensor network equivalent to a locally connected neural network.
		\textbf{(a)} We consider a tensor network adhering to a perfect binary tree connectivity with $N = 2^L$ leaf nodes, for $L \in \N$, generating $\tntensor \in \R^{D_1 \times \cdots \times D_N}$.
		Axes corresponding to open edges are indexed such that open edges descendant to any node of the tree have contiguous indices.
		The lengths of axes corresponding to inner (non-open) edges are equal to $R \in \N$, referred to as the \emph{width} of the tensor network.
		\textbf{(b)} Contracting $\tntensor$ with vectors $\xbf^{(1)} \in \R^{D_1}, \ldots, \xbf^{(N)} \in \R^{D_N}$ produces $\inprodnoflex{ \tenp_{n = 1}^N \xbf^{(n)} }{\tntensor}$.
		Performing these contractions from leaves to root can be viewed as a forward pass of a data instance $\brk{ \xbf^{(1)}, \ldots, \xbf^{(N)} }$ through a certain locally connected neural network (with polynomial non-linearity; see,~\eg,~\cite{cohen2016expressive,cohen2017inductive,levine2018deep,razin2022implicit}).
		Accordingly, we call the tensor network generating $\tntensor$ a \emph{locally connected tensor network}.
	}
	\label{fig:tn_as_nn}
\end{figure*}

As identified by previous works, the tensor network depicted in~\cref{fig:tn_as_nn}(a) is equivalent to a certain locally connected neural network (with polynomial non-linearity~---~see,~\eg,~\cite{cohen2016expressive,cohen2017inductive,levine2018deep,razin2022implicit}).
In particular, contracting the tensor network with vectors $\xbf^{(1)} \in \R^{D_1}, \ldots, \xbf^{(N)} \in \R^{D_N}$, as illustrated in~\cref{fig:tn_as_nn}(b), can be viewed as a forward pass of the data instance $\brk{ \xbf^{(1)}, \ldots, \xbf^{(N)} }$ through a locally connected neural network, whose hidden layers are of width $R$.
This computation results in a scalar equal to $\inprod{ \tenp_{n = 1}^N \xbf^{(n)} }{\tntensor}$, where $\tenp$ stands for the outer product.\footnote{
For any $\brk[c]{ \xbf^{(n)} \in \R^{D_n} }_{n = 1}^N$, the outer product $\tenp_{n = 1}^N \xbf^{(n)} \in \R^{D_1 \times \cdots \times D_N}$ is defined element-wise by $\brk[s]{ \tenp_{n = 1}^N \xbf^{(n)} }_{d_1, \ldots, d_N} = \prod_{n = 1}^N \xbf^{(n)}_{d_n}$, where $d_1 \in [D_1], \ldots, d_N \in [D_N]$.
}\textsuperscript{,}\footnote{
	Contracting an arbitrary tensor $\A \in \R^{D_1 \times \cdots \times D_N}$ with vectors $\xbf^{(1)} \in \R^{D_1}, \ldots, \xbf^{(N)} \in \R^{D_N}$ (where for each $n \in [N]$, $\xbf^{(n)}$ is contracted against the $n$'th axis of $\A$) yields a scalar equal to $\inprodnoflex{\tenp_{n = 1}^N \xbf^{(n)} }{ \A }$.
	This follows from the definitions of the contraction, inner product and outer product operations.
}
In light of its equivalence to a locally connected neural network, we will refer to the tensor network as a \emph{locally connected tensor network}.
We note that for the equivalent neural network to be practical (in terms of memory and runtime), the width $R$ needs to be of moderate size (typically no more than a few thousands).
Specifically, $R$ cannot be exponential in the dimension $N$, meaning $\ln (R)$ needs to be much smaller than~$N$.

By virtue of the locally connected tensor network's equivalence to a deep neural network, it has been paramount for the study of expressiveness and generalization in deep learning~\citep{cohen2016expressive,cohen2016convolutional,cohen2017inductive,cohen2017analysis,cohen2018boosting,sharir2018expressive,levine2018benefits,levine2018deep,balda2018tensor,khrulkov2018expressive,khrulkov2019generalized,levine2019quantum,razin2020implicit,razin2021implicit,razin2022implicit,razin2022ability}.
Although the equivalent deep neural network (which has polynomial non-linearity) is less common than other neural networks (\eg,~ones with ReLU non-linearity), it has demonstrated competitive performance in practice~\citep{cohen2014simnets,cohen2016deep,sharir2016tensorial,stoudenmire2018learning,felser2021quantum}.
More importantly, its theoretical analyses, through the equivalence to the locally connected tensor network, brought forth numerous insights that were demonstrated empirically and led to development of practical tools for common locally connected architectures.
Continuing this line, we will demonstrate our theoretical insights through experiments with widespread convolutional, recurrent and local self-attention architectures (\cref{sec:accurate_predict:emp_demo}), and employ our theory for deriving an algorithm that enhances the suitability of a data distribution to said architectures (\cref{sec:enhancing}).


  % NECESSARY AND SUFFICIENT CONDITION FOR FITTING TENSOR
\subsection{Necessary and Sufficient Condition for Fitting Tensor} \label{sec:fit_tensor:nec_and_suf}

Herein we show that the ability of the locally connected tensor network (defined in~\cref{sec:fit_tensor:tn_lc_nn}) to fit (\ie~represent) a given tensor is determined by the entanglements that the tensor admits under the below-defined \emph{canonical partitions} of $[N]$.
Note that each canonical partition comprises a subset of contiguous indices, so low entanglement under canonical partitions can be viewed as a formalization of locality.

\begin{definition}
\label{def:canonical_partitions}
The \emph{canonical partitions} of $[N] = [2^L]$ (illustrated in~\cref{fig:canonical_partitions}) are:
\[
\can := \! \Big \{ \brk*{ \K, \K^c} :  \, \K = \brk[c]*{ 2^{L - l} \cdot (n - 1) + 1, \ldots, 2^{L - l} \cdot n } ,~l \in \big \{0, \ldots, L \big \} ,~n \in \big [ 2^l \big ]  \Big \}
\text{\,.}
\]
\end{definition}

\begin{figure}[t]
	\vspace{0mm}
	\begin{center}
		\includegraphics[width=0.75\textwidth]{figs/canonical_partitions.pdf}
	\end{center}
	\vspace{-1mm}
	\caption{
		The canonical partitions of $[N]$, for $N = 2^L$ with $L \in \N$.
		Every $l \in \{0, \ldots, L\}$ contributes $2^l$ canonical partitions, the $n$'th one induced by $\K = \{2^{L - l} \cdot (n - 1) + 1, \ldots, 2^{L - l} \cdot n\}$.
%		For example, at $l = 0$ there is a single trivial partition induced by $\K = [N]$, at $l = 1$ there are two partitions induced by the first and second halves of $[N]$, at $l = 2$ there are four partitions induced by non-overlapping quarters of $[N]$, and at $l = L$ there are $N$ partitions induced by the singleton subsets of $[N]$.
%		As proven in~\cref{sec:fit_tensor:nec_and_suf}, the locally connected tensor network given in~\cref{sec:fit_tensor:tn_lc_nn} can fit (\ie~represent) a tensor if and only if the tensor has low entanglements under these partitions of its axes.
	}
	\label{fig:canonical_partitions}
\end{figure}

By appealing to known upper bounds on the entanglements that a given tensor network supports~\citep{cui2016quantum,levine2018deep}, we now establish that if the locally connected tensor network can fit a given tensor, that tensor must admit low entanglement under the canonical partitions of its axes.
Namely, suppose that $\tntensor$~---~the tensor generated by the locally connected tensor network~---~well-approximates an order $N$ tensor $\A$.
Then,~\cref{thm:fit_necessary} below shows that the entanglement of $\A$ with respect to every canonical partition $(\K, \K^c) \in \can$ cannot be much larger than $\ln (R)$ (recall that $R$ is the width of the locally connected tensor network, and that in practical settings $\ln (R)$ is much smaller than $N$), whereas the expected entanglement of a random tensor with respect to $(\K, \K^c)$ is on the order of $\min \{ \abs{ \K }, \abs{ \K^c } \}$ (which is linear in~$N$ for most canonical partitions).

In the other direction,~\cref{thm:fit_sufficient} below implies that low enough entanglement under the canonical partitions is not only necessary for a tensor to be fit by the locally connected tensor network, but also sufficient.

\begin{theorem}
	\label{thm:fit_necessary}
	Let $\tntensor \in \R^{D_1 \times \cdots \times D_N}$ be a tensor generated by the locally connected tensor network defined in~\cref{sec:fit_tensor:tn_lc_nn}, and let $\A \in \R^{D_1 \times \cdots \times D_N}$.
	For any $\epsilon \in [0, \norm{\A} / 4]$, if $\norm{\tntensor -  \A} \leq \epsilon$, then for all canonical partitions $\brk{\K, \K^c} \in \can$ (\cref{def:canonical_partitions}):\footnote{
		If $\A = 0$, then $\epsilon = 0$.
		In this case, the expression $\epsilon / \norm{\A}$ is by convention equal to zero.
	}
	\be
	\qe{\A}{\K} \leq \ln (R)+\frac{2 \epsilon}{ \norm{\A} } \cdot \ln ( D_{\K} ) + 2 \sqrt{\frac{ 2 \epsilon }{ \norm{\A} } }
	\text{\,,}
	\label{eq:fit_necessary_ub}
	\ee
	where $D_{\K} := \min \brk[c]{ \prod_{n \in \K} D_n , \prod_{n \in \K^c} D_n }$.
	In contrast, a random $\A' \in \R^{D_1 \times \cdots \times D_N}$, drawn according to the uniform distribution over the set of unit norm tensors, satisfies for all canonical partitions $\brk{\K, \K^c} \in \can$:
	\be
	\EE \brk[s]*{ \qe{\A'}{\K} } \geq  \min \{ \abs{\K}, \abs{\K^c} \} \cdot \ln \brk*{ \min\nolimits_{n \in [N]} D_n } + \ln \brk*{ \frac{ 1 }{ 2 } } - \frac{1}{2}
	\text{\,.}
	\label{eq:fit_necessary_lb}
	\ee
\end{theorem}

\begin{proof}[Proof sketch (proof in~\cref{app:proofs:fit_necssary})]
In general, the entanglements that a tensor network supports can be upper bounded through cuts in its graph~\citep{cui2016quantum,levine2018deep}.
For the locally connected tensor network, these bounds imply that $\qe{\tntensor}{\K} \leq \ln (R)$ for any canonical partition $(\K, \K^c)$.
\cref{eq:fit_necessary_ub} then follows by showing that if $\tntensor$ and $\A$ are close, then so are their entanglements.
\cref{eq:fit_necessary_lb} is established  based on a characterization from~\cite{sen1996average}.
\end{proof}

\begin{theorem}
	\label{thm:fit_sufficient}
	Let $\A \in \R^{D_1 \times \cdots \times D_N}$ and $\epsilon > 0$.
	Suppose that for all canonical partitions $(\K, \K^c) \in \can$ (\cref{def:canonical_partitions}) it holds that $\qe{\A}{\K} \leq \frac{\epsilon^{2}}{(2N-3) \norm{\A}^2} \cdot \ln (R)$.\footnote{\label{note:suff_cond}
		When the approximation error $\epsilon$ tends to zero, the sufficient condition in~\cref{thm:fit_sufficient} requires entanglements to approach zero, unlike the necessary condition in~\cref{thm:fit_necessary} which requires entanglements to become no greater than $\ln (R)$.
		This is unavoidable.
		However, if for all canonical partitions $(\K, \K^c) \in \can$ the singular values of $\mat{\A}{\K}$ trailing after the $R$'th one are small, then we can also guarantee an assignment for the locally connected tensor network satisfying $\norm{\tntensor - \A} \leq \epsilon$, while $\qe{\A}{\K}$ can be on the order of $\ln (R)$ for all $(\K, \K^c) \in \can$.
		See~\cref{app:suff_cond} for details.
	}
	Then, there exists an assignment for the tensors constituting the locally connected tensor network (defined in~\cref{sec:fit_tensor:tn_lc_nn}) such that it generates $\tntensor \in \R^{D_1 \times \cdots \times D_N}$ satisfying:
	\[
	\norm{ \tntensor - \A } \leq \epsilon
	\text{\,.}
	\]
\end{theorem}

\begin{proof}[Proof sketch (proof in~\cref{app:proofs:fit_sufficient})]
We show that if $\A$ has low entanglement under a canonical partition $(\K, \K^c) \in \can$, then the singular values of $\mat{\A}{\K}$ must decay rapidly (recall that $\mat{\A}{\K}$ is the arrangement of $\A$ as a matrix where rows correspond to axes indexed by $\K$ and columns correspond to the remaining axes).
The approximation guarantee is then obtained through a construction from~\cite{grasedyck2010hierarchical}, which is based on truncated singular value decompositions of every $\mat{\A}{\K}$ for $(\K, \K^c) \in \can$.
\end{proof}
