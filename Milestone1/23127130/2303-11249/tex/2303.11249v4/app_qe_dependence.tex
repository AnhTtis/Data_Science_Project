\section{Quantum Entanglement as a Measure of Dependence Between Features}
\label{app:qe_dependence}

In this appendix, we provide intuition behind the following statement made in~\cref{sec:accurate_predict:nec_and_suf}: the quantum entanglement of $\popdatatensor$ with respect to an arbitrary partition of its axes $(\K, \K^c)$, where $\K \subseteq [N]$, is a measure of dependence between the data features indexed by $\K$ and those indexed by $\K^c$.
Consider the case where the features indexed by $\K$ are statistically independent of those indexed by $\K^c$, and all features are statistically independent of the label $y$.
Then $\qe{\popdatatensor}{\K} = 0$ (see~\cref{lem:qe_dependence} below for proof of this fact). 
If the features are not statistically independent of $y$, but when conditioned on $y$ the features indexed by $\K$ are statistically independent of those indexed by $\K^c$, then $\qe{\popdatatensor}{\K} \leq \ln (2)$ (this fact is also established by~\cref{lem:qe_dependence} below). 
In general, the higher $\qe{\popdatatensor}{\K}$ is, the farther the distribution is from the aforementioned situations of independence, implying stronger dependence between the features indexed by $\K$ and those indexed by $\K^c$.

\begin{lemma}
\label{lem:qe_dependence}
Consider the classification setting of~\cref{sec:accurate_predict:fit_data_tensor}, and let $\K \subseteq [N]$.
If the features indexed by $\K$ are independent of those indexed by $\K^c$ (\ie~$(\xbf^{(n)})_{n \in \K}$ are independent of $(\xbf^{(n)})_{n \in \K^c}$), conditioned on the label $y$, then $\qe{ \popdatatensor }{ \K} \leq \ln (2)$.
Furthermore, if the features are also independent of $y$, then $\qe{\popdatatensor}{\K} = 0$.
\end{lemma}

\begin{proof}
For brevity, denote $\Xbf := (\xbf^{(1)}, \ldots, \xbf^{(N)})$, $\Xbf_\K := (\xbf^{(n)})_{n \in \K}$, and $\Xbf_{\K^c} := (\xbf^{(n)})_{n \in \K^c}$.
By the law of total expectation, the entry of $\popdatatensor$ corresponding to an index tuple $(i_1, \ldots, i_N) \in [D_1] \times \cdots \times [D_N]$ satisfies:
\[
\begin{split}
(\popdatatensor)_{i_1, \ldots, i_N} & = \EE\nolimits_{\Xbf , y} \brk[s]2{ y \cdot \prod\nolimits_{n = 1}^N \xbf^{(n)}_{i_n} } \\
& = \prob (y = 1) \cdot  \EE\nolimits_{\Xbf} \brk[s]2{ \prod\nolimits_{n = 1}^N \xbf^{(n)}_{i_n} \big\vert y = 1} - \prob (y = - 1) \cdot  \EE\nolimits_{\Xbf} \brk[s]2{ \prod\nolimits_{n = 1}^N \xbf^{(n)}_{i_n} \big\vert y = - 1}
\text{\,.}
\end{split}
\]
Thus, if the features indexed by $\K$ are independent of those indexed by $\K^c$ given $y$, it holds that:
\be
\begin{split}
(\popdatatensor)_{i_1, \ldots, i_N}  = & \prob (y = 1) \cdot  \EE\nolimits_{\Xbf_{\K}} \brk[s]2{ \prod\nolimits_{n \in \K} \xbf^{(n)}_{i_n} \big\vert y = 1} \cdot  \EE\nolimits_{\Xbf_{\K^c}} \brk[s]2{ \prod\nolimits_{n \in \K^c} \xbf^{(n)}_{i_n} \big\vert y = 1} \\
& - \prob (y = - 1) \cdot  \EE\nolimits_{\Xbf_{\K}} \brk[s]2{ \prod\nolimits_{n \in \K} \xbf^{(n)}_{i_n} \big\vert y = -1} \cdot  \EE\nolimits_{\Xbf_{\K^c}} \brk[s]2{ \prod\nolimits_{n \in \K^c} \xbf^{(n)}_{i_n} \big\vert y = -1}
\text{\,.}
\end{split}
\label{eq:dpop_entry_cond_independence}
\ee
This implies that we can write $\mat{\popdatatensor}{\K} \in \R^{ \prod\nolimits_{n \in \K} D_n \times \prod\nolimits_{n \in \K^c} D_n }$~---~the arrangement of $\popdatatensor$ as a matrix whose rows correspond to axes indexed by $\K$ and columns correspond to the remaining axes~---~as a weighted sum of two outer products.
Specifically, define $\vbf_{\K}, \ubf_{\K} \in \R^{\prod\nolimits_{n \in \K} D_n}$ to be the vectors holding for each possible indexing tuple $(i_n)_{n \in \K}$ the values $\EE\nolimits_{\Xbf_{\K}} \brk[s]1{ \prod\nolimits_{n \in \K} \xbf^{(n)}_{i_n} \vert y = 1}$ and $\EE\nolimits_{\Xbf_{\K}} \brk[s]1{ \prod\nolimits_{n \in \K} \xbf^{(n)}_{i_n} \vert y = - 1}$, respectively (with the arrangement of entries in the vectors being consistent with the rows of $\mat{\popdatatensor}{\K}$).
Furthermore, let $\vbf_{\K^c}, \ubf_{\K^c} \in \R^{\prod\nolimits_{n \in \K^c} D_n}$ be defined analogously by replacing $\K$ with $\K^c$ (with the arrangement of entries in the vectors being consistent with the columns of $\mat{\popdatatensor}{\K}$).
Then, by~\cref{eq:dpop_entry_cond_independence}:
\[
\mat{\popdatatensor}{\K} = \prob (y = 1) \cdot  \vbf_{\K} \tenp \vbf_{\K^c} - \prob (y = - 1) \cdot \ubf_{\K} \tenp \ubf_{\K^c}
\text{\,.}
\]
Since each outer product forms a rank one matrix, the subaddititivty of rank implies that the rank of $\mat{\popdatatensor}{\K}$ is at most two.
As a result, $\mat{\popdatatensor}{\K}$ has at most two non-zero singular values and $\qe{\popdatatensor}{\K} \leq \ln (2)$ (the entropy of a distribution is at most the natural logarithm of the size of its support).

If, in addition, all features in the data are independent of the label $y$, then $\vbf_{\K} = \ubf_{\K}$ and $\vbf_{\K^c} = \ubf_{\K^c}$, meaning $\mat{\popdatatensor}{\K} =  ( \prob (y = 1) - \prob (y = - 1) ) \cdot  \vbf_{\K} \tenp \vbf_{\K^c}$.
In this case, the rank of $\mat{\popdatatensor}{\K}$ is at most one, so $\qe{\popdatatensor}{\K} = 0$.
\end{proof}