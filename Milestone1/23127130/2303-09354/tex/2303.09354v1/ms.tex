\documentclass[3p,twocolumn,longtitle,reversenotenum,sort&compress,lefttitle]{elsarticle}

\pdfoutput=1

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\PassOptionsToPackage{hyphens}{url}\usepackage[hidelinks]{hyperref}

\usepackage{libertine}
\usepackage{microtype}

\usepackage{enumitem,amssymb}
\usepackage{tabularx,booktabs}

\renewcommand{\ttdefault}{\rmdefault} 

% do not indent first lines of paragraphs:
\makeatletter
\let\@afterindenttrue\@afterindentfalse
\makeatother

\nopreprintlinetrue

\bibliographystyle{elsarticle-num}


\begin{document}

\begin{frontmatter}

\title{The NCI Imaging Data Commons as a platform for reproducible research\\ in computational pathology}

\author[aff1]{Daniela~P.~Schacherer\corref{corresp}}
\ead{daniela.schacherer@mevis.fraunhofer.de}
\author[aff2]{Markus~D.~Herrmann} 
\author[aff3]{David~A.~Clunie} 
\author[aff1]{Henning~Höfener}
\author[aff4]{William~Clifford}
\author[aff4]{William~J.R.~Longabaugh}
\author[aff5]{Steve~Pieper}
\author[aff6]{Ron~Kikinis}
\author[aff6]{Andrey~Fedorov}
\author[aff1]{André~Homeyer\corref{corresp}}
\ead{andre.homeyer@mevis.fraunhofer.de}

\address[aff1]{Fraunhofer Institute for Digital Medicine MEVIS, Bremen, Germany}
\address[aff2]{Department of Pathology, Massachusetts General Hospital and Harvard Medical School, Boston, Massachusetts, USA}
\address[aff3]{PixelMed Publishing LLC, Bangor, Pennsylvania, USA}
\address[aff4]{Institute for Systems Biology, Seattle, Washington, USA}
\address[aff5]{Isomics Inc, Cambridge, Massachusetts, USA}
\address[aff6]{Department of Radiology, Brigham and Women's Hospital and Harvard Medical School, Boston, Massachusetts, USA}

\cortext[corresp]{Corresponding author}


\begin{abstract}
\noindent\textbf{Objective}: 
Reproducibility is critical for translating machine learning-based (ML) solutions in computational pathology (CompPath) into practice. However, an increasing number of studies report difficulties in reproducing ML results.
The NCI Imaging Data Commons (IDC) is a public repository of \textgreater120 cancer image collections, including \textgreater38,000 whole-slide images (WSIs), that is designed to be used with cloud-based ML services.
Here, we explore the potential of the IDC to facilitate reproducibility of CompPath research.

\noindent\textbf{Materials and Methods}: 
The IDC realizes the FAIR principles: All images are encoded according to the DICOM standard, persistently identified, discoverable via rich metadata, and accessible via open tools.
Taking advantage of this, we implemented two experiments in which a representative ML-based method for classifying lung tumor tissue was trained and/or evaluated on different datasets from the IDC.
To assess reproducibility, the experiments were run multiple times with independent but identically configured sessions of common ML services.

\noindent\textbf{Results}:
The AUC values of different runs of the same experiment were generally consistent and in the same order of magnitude as a similar, previously published study.
However, there were occasional small variations in AUC values of up to 0.044, indicating a practical limit to reproducibility.

\noindent\textbf{Discussion and conclusion}:
By realizing the FAIR principles, the IDC enables other researchers to reuse exactly the same datasets. Cloud-based ML services enable others to run CompPath experiments in an identically configured computing environment without having to own high-performance hardware. The combination of both makes it possible to approach the reproducibility limit.

\noindent\textbf{Keywords:} reproducibility, computational pathology, FAIR, cloud computing, machine learning, artificial intelligence
\end{abstract}

\end{frontmatter}

\section{Introduction}
Computational pathology (CompPath) is a new discipline that investigates the use of computational methods for the interpretation of heterogenous data in clinical and anatomical pathology to improve health care in pathology practice. A major focus area of CompPath is the computerized analysis of digital tissue images~\citep{louis2015}. 
These images show thin sections of surgical specimens or biopsies that are
stained to highlight relevant tissue structures. The visual appearance
of these structures is highly variable and strongly affected by the
biological context and processing of the specimen~\citep{mccann2015}. To
cope with this level of complexity and variability, virtually all
state-of-the-art methods use sophisticated machine learning (ML) algorithms
such as Convolutional Neural Networks (CNN)~\citep{niazi2019}.

Because CompPath has a wide variety of use cases, there has been an
explosion of research on ML-based tissue analysis methods~\citep{echle2020, cui2021}.
Most of the published methods are intended to assist
pathologists in routine diagnostic tasks to make them more
accurate and less time-consuming. A common task is the recognition of
tissue patterns for disease classification. Multiple methods
have been developed to detect or subtype tumor tissue, e.g., for breast~\citep{cruzroa2017, campanella2019}, prostate~\citep{campanella2019},
lung~\citep{coudray2018, wang2020}, and colon cancer~\citep{iizuka2020}.
Another common task is the detection and quantification of specific cell
types~\citep{feng2020, shahin2019}. Beyond that, CompPath methods have also shown
promise for deriving novel biomarkers from tissue patterns that can predict
outcome, genetic mutations, or therapy response~\citep{echle2020}.

\subsection{Reproducibility challenges}

In recent years, it has become increasingly clear that reproducing
the results of published ML studies is challenging~\citep{hutson2018, raff2019, gundersen2022}. 
Reproducibility is commonly defined as the ability to obtain
``consistent results using the same input data, computational steps,
methods, and conditions of analysis.''~\citep{nasem2019, renard2020} 
Difficulties related to reproducibility prevent other researchers from
reusing published results and are a critical barrier to translating
solutions into clinical practice~\citep{haibekains2020}. They also
prevent reviewers from verifying that published results are actually
caused by the claimed effects~\citep{gundersen2022, crocker2011}. In
most cases, reproducibility problems seem to stem not from a lack of
scientific rigor, but from challenges to convey all details and set-up
of complex ML methods~\citep{haibekains2020, pineau2021, raff2019}. In
the following, we provide an overview of the main challenges related to
ML reproducibility and the existing approaches to address them.

The first challenge is the specification of the analysis method itself.
ML algorithms have many variables, such as the network architecture,
hyperparameters (e.g., loss function, optimizer, learning rate, data
augmentation steps) and performance metrics~\citep{hartley2020, renard2020, pineau2021}.
ML workflows usually consist of multiple processing steps (e.g., data selection,
preprocessing, training, evaluation)~\citep{renard2020}. Small
variations in these implementation details can have significant effects
on performance. To make all these details transparent, it is crucial to
publish the underlying source code~\citep{haibekains2020}. Workflows
should be automated as much as possible to avoid errors when performing
steps manually. Jupyter notebooks have emerged as the {\emph{de facto}} standard
to implement and communicate ML workflows~\citep{perkel2018}. They
mix software code, intermediate results and explanatory texts, and
present them as ``computational narratives''~\citep{rule2019}. They are
executed in a unified development environment where users can
interactively run parts of the code and validate results~\citep{perkel2018}. In this way, notebooks make it easier for
researchers to reproduce and understand the work of others~\citep{perkel2018}.

The second challenge to reproducibility is the specification and setup
of the computing environment. ML workflows require significant computational
resources including, e.g., graphics or tensor processing units
(GPUs or TPUs). In addition, they often have many dependencies on
specific software or even specific software versions. Minor variations
in the computing environment configuration can significantly affect the results~\citep{gundersen2022}.
Setting up a consistent computational environment can be
very expensive and time consuming. This challenge can be partially solved by
embedding ML workflows in virtual machines or software containers like Docker~\citep{perkel2019}.
Both include all required software dependencies so that
ML workflows can be shared and run without additional installation
effort. Cloud-based machine learning services, like Google Vertex AI, Amazon SageMaker,
or Microsoft Azure Machine Learning, provide an even more comprehensive solution.
By offering preconfigured computing environments for ML
research in combination with the required high-performance hardware,
such services can further reduce the setup effort and enable the
reproduction of computationally intensive ML workflows even if one does
not own the required hardware. They also typically provide web-based
graphical user interfaces through which Jupyter notebooks can be run and
shared directly in the cloud, making it easy for others to reproduce,
verify, and reuse ML workflows~\citep{perkel2019}.

The third challenge related to ML reproducibility is the specification of
persistent and consistent access to data. The performance of ML methods depends heavily on the
composition of their training, validation and test sets~\citep{gundersen2022, maierhein2018}. For current ML studies, it is rarely possible to reproduce this composition exactly~\citep{gundersen2018} as studies are commonly based on specific,
hand-curated datasets which are only roughly described rather than explicitly defined~\citep{hartley2020}. Also, the datasets are often not made publicly available~\citep{haibekains2020}, or the criteria/identifiers used to select
subsets from publicly available datasets are missing. Stakeholders from
academia and industry have defined the Findability, Accessibility,
Interoperability, and Reusability (FAIR) principles~\citep{wilkinson2016},
a set of requirements to facilitate discovery and
reuse of data. FAIR data provision is now considered a ``must'' to make ML
studies reproducible and the FAIR principles are adopted by more and
more public data infrastructure initiatives and scientific journals~\citep{scheffler2022}.

Reproducing CompPath studies is particularly challenging. To reveal fine
cellular details, tissue sections are imaged at microscopic
resolution, resulting in gigapixel whole-slide images (WSI)~\citep{patel2021}.
Due to the complexity and variability of tissue images~\citep{mccann2015},
it takes many\textemdash often thousands\textemdash of example WSI to develop and test reliable ML models.
Processing and managing such large amounts of data requires extensive computing power,
storage resources, and network bandwidth.
Reproduction of CompPath studies is further complicated by the large number
of proprietary and incompatible WSI file formats that often impede data access
and make it difficult to combine heterogeneous data from different studies or sites.
The Digital Imaging and Communications in Medicine (DICOM) standard~\citep{bidgood1997} is an internationally accepted standard for storage and communication of medical images. It is universally used in radiology and other medical disciplines, and has great potential to become the uniform standard for pathology images as well~\citep{herrmann2018}. However, until now, there have been few pathology data collections provided in DICOM format.

\subsection{NCI Imaging Data Commons}

The National Cancer Institute (NCI) Imaging Data Commons (IDC) is a new
cloud-based repository within the US national Cancer Research Data
Commons~\citep{fedorov2021}. A central goal of the IDC is to improve the
reproducibility of data-driven cancer imaging research, including CompPath
research. For this purpose, the IDC provides large publicly available
cancer image collections, which are made available according to the FAIR principles.

The IDC is designed for various cancer imaging data.
Besides pathology images (brightfield and fluorescence) and their metadata, the IDC also includes radiology images (e.g., CT, MR, and PET) together with associated image analysis results, image annotations, and clinical data providing context about the images.
At the time of writing this article, the IDC contained 128 data collections with more than 63,000 cases and more than 38,000 WSI from different projects and sites.
The collections cover common tumor types, including carcinomas of the breast, colon, kidney, lung, and prostate, as well as rarer cancers such as sarcomas or lymphomas.
Most of the WSI collections originate from The Cancer Genome Atlas (TCGA)~\citep{tcga} and Clinical Proteomic Tumor Analysis Consortium (CPTAC)~\citep{cptac} projects and were curated by The Cancer Imaging Archive (TCIA)~\citep{clark2013}.
Both TCGA and CPTAC data are commonly used in the development of CompPath methods~\citep{coudray2018, saltz2018, khosravi2018, noorbakhsh2020}.
While the WSI were originally stored and shared by these projects in proprietary, vendor-specific formats, the IDC harmonized the data and converted them into the open, standard DICOM format. 

This paper explores how the capabilities of the IDC and cloud-based ML
services can be used in combination to improve reproducibility of CompPath studies.
Using a basic analysis workflow representative of common
CompPath use cases as an example, we demonstrate how the three key challenges to
reproducibility introduced above can be addressed in the context of the IDC. 
To this end, we also provide an overview of and introduction to the IDC and 
make recommendations on using it for CompPath studies.

\section{Materials and Methods}

\subsection{FAIR data provision}

The IDC implements the FAIR principles~\citep{wilkinson2016} as follows.

Interoperability:
All data in the IDC are represented according to the DICOM standard, which defines data models and services for storage and communication of medical image data and metadata.
In DICOM, a WSI corresponds to a ``series'' of DICOM image objects that represent the digital slide at different resolutions.
Since WSIs are much larger than other medical images, downsampling operations are computationally expensive.
Storing multiple resolutions allows for quick access to specific image resolutions, for example, when zooming in or out in a viewer.
Image metadata are stored as attributes directly within the DICOM objects.
The DICOM standard defines the attributes available for different real-world entities (e.g., patient, study) and controlled terminologies for their values~\citep{herrmann2018}.

Accessibility:
The IDC is implemented on the Google Cloud Platform (GCP), enabling cohort selection and analysis directly in the cloud.
In particular, the IDC can be efficiently used with various cloud-based ML services provided by the GCP.
Since the IDC data is provided as part of the Google Public Datasets Program, it can be freely accessed in the GCP or downloaded without charge for use in other computing environments.
In the IDC, DICOM objects are stored as individual DICOM files in Google Cloud Storage (GCS) buckets and can be retrieved using open, free, and universally implementable tools.

Findability:
Each DICOM file in the IDC has a persistent universally unique identifier (UUID)~\citep{leach2005}.
DICOM files in storage buckets are referenced through GCS URLs, consisting of the bucket URL and the UUID of the file.
Images in the IDC are described with rich metadata, including patient (e.g., age, sex), disease (e.g., subtype, stage), study (e.g., therapy, outcome), and imaging-related data (e.g., specimen handling, scanning).
All DICOM and non-DICOM metadata are indexed in a Google BigQuery database~\citep{google2022bigquerydicom} that can be queried programmatically using standard Structured Query Language (SQL) statements as described below, allowing for an exact and persistent definition of cohorts for subsequent analysis.

Reusability:
All image collections are associated with detailed provenance information but stripped of patient-identifiable information.
Most collections are released under data usage licenses that allow unrestricted use in research studies.

\begin{figure*}[h]
\centering
\includegraphics[width=\textwidth]{images/figure_1}

\caption{Overview of the workflows of both 
experiments and their interactions with the IDC.}
%
\label{fig:workflow}
\end{figure*}

\subsection{Experiments}

To explore how the IDC can be used for CompPath research and how it can
facilitate reproducibility, we conducted two experiments (see~\autoref{fig:workflow}).
In Experiment 1, we used the IDC for the entire development process
of an example CompPath method, including model training and validation.
In Experiment 2, we used the IDC to perform inference with a trained model on independent data. The model trained in Experiment 1 was used as the basis for Experiment 2. 

For this purpose, we selected a use case we considered representative of common CompPath tasks~\citep{cruzroa2017, campanella2019, wang2020, iizuka2020, coudray2018}: the automatic classification of entire WSI of hematoxylin and eosin (H\&E)-stained frozen lung tissue sections into either non-neoplastic (normal), lung adenocarcinoma (LUAD), or lung squamous cell carcinoma (LSCC/LUSC).

The two experiments were conducted with different collections in the IDC: TCGA-LUAD/LUSC~\citep{tcga-luad2016,tcga-lusc2016} and CPTAC-LUAD/LSCC~\citep{cptac-luad2018, cptac-lscc2018}, respectively.
The TCGA and the CPTAC collections cover H\&E-stained lung tissue sections of the three classes considered (see~\autoref{fig:example_slides}).
However, while the TCGA samples were flash frozen, the CPTAC samples were formalin-fixed and paraffin-embedded (FFPE).
Therefore, the results of Experiment 2 provide insight into the transferability of the model trained in Experiment 1 to slides prepared with a different tissue embedding technique by other clinical institutions.

\begin{figure*}[h]
\centering
\includegraphics[width=\textwidth]{images/figure_2}

\caption{Example tiles of the three classes considered from the TCGA and CPTAC datasets.
The width of each tile is 256~{\textmu}m. The black boxes marked with arrows in the whole slide images on top show the boundaries of the upper left tiles of the TCGA data set.}
%
\label{fig:example_slides}
\end{figure*}

\subsection{Implementation}

Both experiments were implemented as standalone Jupyter notebooks that are available open source~\citep{github_repo_experiments}.
To enable reproducibility, care was taken to make operations deterministic, e.g., by seeding pseudo-random operations or by iterating over unordered container types in a defined order.
Utility functionality was designed as generic classes and functions that can be reused for similar use cases.

We kept the analysis method deliberately simple and resource-efficient to make it easy to understand and execute as a demo application. 
It was inspired by a lung tumor classification method described in a widely cited study by Coudray~et~al.~\citep{coudray2018}.
However, our method processes images at a lower resolution, which is significantly less computationally expensive.
In our analysis workflow, a WSI was subdivided into non-overlapping rectangular tiles, each measuring 256\texttimes 256 pixels at a resolution of 1~µm/px.
Tiles containing less than 50\% tissue, as determined by pixel value statistics, were discarded.
Each tile was assigned class probabilities by performing multi-class classification using an InceptionV3~CNN~\citep{szegedy2015}.
The per-tile results were finally aggregated to a single classification of the entire slide.
The workflow is visualized in~\autoref{fig:ml_method} and a detailed description is provided in the respective notebooks.

\begin{figure*}[h]
\centering
\includegraphics[width=\textwidth]{images/figure_3}
\caption{
Illustration of the CompPath analysis method.
Slides were divided into non-overlapping rectangular tiles discarding those with more background than tissue.
Each tile was assigned class probabilities using a neural network performing multi-class classification.
Slide-based class values were determined by aggregating the tile-based results.
}
%
\label{fig:ml_method}
\end{figure*}

In Experiment 1, the considered slides were divided into training,
validation, and test sets with proportions of 70\%, 15\%, and 15\%,
respectively. To keep the sets independent and avoid overoptimistic
performance estimates~\citep{homeyer2022}, we ensured that slides from a
given patient were assigned to only one set. The data collections used
did not contain annotations of tumor regions, but only one reference
class value per WSI. Following the procedure used by Coudray et al., all
tiles were considered to belong to the reference class of their
respective slide. Training was performed using a categorical cross-entropy loss between the true class labels and the predicted class probabilities, and the RMSProp optimizer, with minimal adjustments to the default hyperparameter values~\citep{keras2022rmsprop}.
The epoch with the highest area under the receiver operating characteristic (ROC) curve (AUC) on the validation set was chosen for the final model.

\subsection{IDC data access}

For most CompPath studies, one of the first steps is to select relevant slides using appropriate metadata. In the context of the IDC, this is
done by executing BigQuery SQL statements (see~\autoref{fig:generic_bq_stmt}).
The results are tables in which rows represent DICOM files and columns
represent selected metadata attributes.

An SQL query for finding available WSI in the IDC generally consists of at least a 
SELECT, a FROM and a WHERE clause. 
The SELECT clause specifies the metadata attributes to be returned. The
IDC provides a wealth of metadata attributes, including image-, patient-, disease-, and study-level properties. The attribute ``gcs\_url'' is usually
selected because it stores the GCS URL needed to access the DICOM file.
The FROM clause refers to a central table ``dicom\_all'' which summarizes
all DICOM attributes of all DICOM files. This table can be joined with
other tables containing additional project-specific metadata.
Crucial to reproducibility is that all IDC data are versioned:
Each new release of the IDC is represented as a new BigQuery dataset,
keeping the metadata for the previous release and the corresponding DICOM files
accessible even if they are modified in the new release.
The version to use is specified via the dataset specifier in fully qualified table names.
All experiments in this manuscript were conducted against IDC data version 11,
i.e., the BigQuery table ``bigquery-public-data.idc\_v11.dicom\_all''.
The WHERE clause defines which DICOM files are returned by imposing constraints for
certain metadata attributes. To guarantee reproducibility, it is essential to not use SQL
statements that are non-deterministic (e.g., those that utilize ANY\_VALUE) and conclude
the statement with an ORDER BY clause, which ensures that results are
returned in a sorted order.

\begin{figure*}[h]
\centering
\includegraphics[width=.75\textwidth]{images/figure_4}

\caption{Generic example of a BigQuery SQL
statement for compiling slide metadata. The result set is limited to
slide microscopy images, as indicated by the value ``SM'' of the DICOM
attribute ``Modality'', from the collections ``TCGA-LUAD'' and
``TCGA-LUSC''.}
%
\label{fig:generic_bq_stmt}
\end{figure*}

The two experiments considered in this paper also begin with the
execution of a BigQuery SQL statement to select appropriate slides and
retrieve all required metadata from the IDC. Experiment 1 queries all
H\&E-stained frozen tissue slides from the TCGA-LUAD/LUSC collections,
resulting in 2163 slides (591 normal, 819 LUAD, 753 LSCC). Experiment 2
uses a very similar statement to query the corresponding FFPE slides from the
CPTAC-LUAD/LSCC collections, resulting in 2086 slides (743 normal, 681
LUAD, 662 LSCC). A more detailed description of the statements is given
in the respective notebooks.

Once their GCS URLs are known, DICOM files in the IDC can be accessed
efficiently using the open source tool ``gsutil''~\citep{google2022gsutil}
or any other tool that supports the Simple Storage Service (S3) API. 
During training in Experiment~1, image tiles of different WSI had to be 
accessed repeatedly in random order. 
To speed up this process, all considered slides were preprocessed
and the resulting tiles were extracted from the DICOM files and cached as individual PNG files on disk before training. In contrast, simply
applying the ML method in Experiment 2 required only a single pass over
the tiles of each WSI in sequential order. Therefore, it was feasible to
access the respective DICOM files and iterate over individual tiles at the time
they were needed for the application of the ML method.

\subsection{Cloud ML services}

The two experiments were conducted with two different cloud-based ML services of the GCP---Vertex AI and Google Colaboratory.
Both services offer virtual machines (VMs) preconfigured with common ML libraries and a JupyterLab-like interface that allows editing and running notebooks from the browser.
They are both backed with extensive computing resources including state-of-the-art GPUs or TPUs.
The costs of both services scale with the type and duration of use for the utilized compute and storage resources.
To use any of them with the IDC, a custom Google Cloud project must be in place for secure authentication and billing, if applicable.

Since training an ML model is much more computationally intensive than performing inference, we conducted Experiment 1 with Vertex AI and Experiment 2 with Google Colaboratory.
Vertex AI can be attached to efficient disks for storage of large amounts of input and output data, making it more suitable for memory-intensive and long-running experiments.
Colaboratory, on the other hand, offers several inexpensive payment plans, with limitations in the provided computing resources and guaranteed continuous usage times.
Colaboratory can even be used completely free of charge, with a significantly limited guaranteed GPU usage time (12 hours at the time of writing).
This makes Colaboratory better suited for smaller experiments or exploratory research by ML researchers with small budgets.

\subsection{Evaluation}

The computing environments provided by cloud-based ML services are all
virtualized. Even if two runs of an experiment are performed using the
same virtual computing environment, the underlying host hardware and
software (e.g., system software versions, compiler settings) may differ~\citep{gundersen2022}. In order to investigate if and how this affects reproducibility, both experiments were executed
multiple times, each in a new virtual environment of the same hardware
and software configuration.

Experiment 1 was performed using a common Vertex AI VM configuration (8 vCPU, 30 GB memory, NVIDIA T4 GPU, Tensorflow Enterprise 2.8 distribution).
Experiment 2 was performed with Colaboratory runtimes (2--8 vCPU, 12--30 GB memory).
When using Google Colaboratory for Experiment 2, we were able to choose between two GPU types, NVIDIA T4 or NVIDIA K80 GPUs, without incurring significant additional costs.
Since it has been suggested that the particular type of GPU can affect results~\citep{nagarajan2019}, all runs of Experiment 2 were repeated on both GPUs, respectively.
Runs with NVIDIA T4 were performed with the free version of Colaboratory, while runs with NVIDIA K80 were performed in combination with a paid GCE Marketplace VM, which was necessary for guaranteed use of this GPU.

For each run of an experiment, classification accuracy was assessed in terms of class-specific, one vs. rest AUC values based on the slide-level results.
In addition, 95\% confidence intervals of the AUC values were computed by 1000-fold bootstrapping over the slide-level results.

To speed up Experiment 2, only a random subset of 300 of the selected
slides (100 normal, 100 LUAD, 100 LSCC) was considered in the analysis,
which was approximately the size of the test set in Experiment 1.

\section{Results}

\begin{table*}[h]
    \centering\small
    \begin{tabularx}{\textwidth}{lllllllll}
    \toprule
     ~ & ~ & ~ & \multicolumn{2}{c}{\textbf{normal}} & \multicolumn{2}{c}{\textbf{LUAD}} & \multicolumn{2}{c}{\textbf{LSCC}} \\ \cmidrule{4-9}
 \textbf{Experiment} & \textbf{ML Service (GPU)} & \textbf{Run} & \textbf{AUC} & \multicolumn{1}{c}{\textbf{CI}} & \textbf{AUC}  & \multicolumn{1}{c}{\textbf{CI}} & \textbf{AUC}  & \multicolumn{1}{c}{\textbf{CI}} \\ \midrule
        Coudray et al. & ~ & ~ & 0.984 & [0.947, 1.000] & 0.969 & [0.933, 0.994] & 0.966 & [0.935, 0.990] \\ \midrule
        Experiment 1 & Vertex AI (T4) & 1 & 0.994 & [0.987, 0.999] & 0.942 & [0.914, 0.968] & 0.937 & [0.904, 0.964] \\ 
        ~ & ~ & 2 & 0.981 & [0.964, 0.994] & 0.898 & [0.860, 0.937] & 0.914 & [0.875, 0.946] \\ 
        ~ & ~ & 3 & 0.992 & [0.983, 0.999] & 0.939 & [0.909, 0.964] & 0.918 & [0.881, 0.949] \\ \midrule
        Experiment 2 & Colaboratory (T4) & 1 & 0.811 & [0.746, 0.871] & 0.698 & [0.633, 0.759] & 0.850 & [0.802, 0.899] \\ 
        ~ & ~ & 2 & 0.811 & [0.746, 0.871] & 0.698 & [0.633, 0.759] & 0.850 & [0.802, 0.899]  \\ 
        ~ & ~ & 3 & 0.811 & [0.747, 0.870] & 0.698 & [0.636, 0.758] & 0.851 & [0.800, 0.896]  \\ 
        ~ & Colaboratory (K80) & 1 & 0.811 & [0.747, 0.870] & 0.698 & [0.636, 0.758] & 0.851 & [0.800, 0.896] \\ 
        ~ & ~ & 2 & 0.811 & [0.743, 0.868] & 0.698 & [0.633, 0.757] & 0.851 & [0.801, 0.895]  \\ 
        ~ & ~ & 3 & 0.811 & [0.746, 0.871] & 0.698 & [0.633, 0.759] & 0.850 & [0.802, 0.899]  \\ 	\bottomrule
    \end{tabularx}
    \caption{Class-specific, slide-based AUC
values and 95\% confidence intervals (CI) obtained through multiple runs of
both experiments. For comparison, the results of a study by Coudray et
al.~\citep{coudray2018} based on the same collections as Experiment 1 are also shown.}
	\label{table:experiment_results}
\end{table*}


The evaluation results of both experiments are summarized in~\autoref{table:experiment_results}.
The classification accuracy of the method trained in Experiment 1 appears satisfactory when evaluated on the TCGA test set.
In the best run, class-specific, slide-based AUC values ranged from 0.937 to 0.994.
This puts them in a similar range to the results of the aforementioned study by Coudray~et~al., which is based on the same TCGA collections~\citep{coudray2018}.
When applied to the CPTAC test set in Experiment 2, the same model performed substantially worse, with values ranging only from 0.698 to 0.851 (see~\autoref{fig:roc_curves}).

It became apparent that none of the experiments was perfectly
reproducible and there were notable deviations in the results of
repeated runs. In Experiment 1, AUC values differed by up to 0.044
between runs and were sometimes clearly worse than the results reported
by Coudray et al.~for which inter-run variability was not reported. In
Experiment 2, there were also minimal deviations in the AUC values of
the different runs, but these were no greater than 0.001. These
deviations occurred regardless of whether the runs were executed on the
same GPU or not.

Experiment 1 took an order of magnitude longer to complete (mean runtime
of 1 d 18 h \textpm 1 h) than Experiment 2 (mean runtime of 1 h 33 min \textpm 5
min). The ML service usage charges for Experiment 1 were approximately
US\$ 32 per run. With the free version of Colaboratory, Experiment 2 was
performed at no cost.

\begin{figure*}[h]
\centering
\includegraphics[width=\textwidth]{images/figure_5.png}

\caption{One-vs-rest ROC curves for the multi-class classification as
obtained in (a) the first run of Experiment 1 using Vertex AI and (b) the
first run of Experiment 2 using Colaboratory (T4).}
%
\label{fig:roc_curves}
\end{figure*}

\section{Discussion}

The aim of this study was to investigate how CompPath studies can be made
reproducible through the use of cloud-based computational environments
and the IDC as the source of input data. Although the same code was run
with the same ML services and care was taken that operations were
deterministic, we observed small deviations in the results of repeated
runs. We cannot tell whether the deviations originate from differences
in the hardware and software used by the hosts of the virtual computing
environments, or whether they are due to randomness resulting from
parallel processing~\citep{gundersen2022}. The greater variability in
the results of Experiment 1 can possibly be explained by its higher
computational complexity. Although the observed deviations appear
negligible for many applications, they represent a practical upper limit
for reproducibility. As outlined below, we argue that the IDC can help
to approach this reproducibility limit.

We chose Jupyter notebooks and Google ML services to address the first
two reproducibility challenges mentioned in the introduction: specifying
the analysis method and setting up the computing environment. Our
open-source notebooks automate the entire ML workflow in a clear and
richly documented way, making it easier for others to verify and reuse
the analysis approach. Vertex AI and Colaboratory provide data
scientists with state-of-the-art hardware and a familiar web-based
environment where they can run notebooks directly in the cloud. In this
way, other researchers can reproduce computationally intensive
experiments without having to own expensive hardware or invest effort in
setting up a specific computing environment.

With the IDC, we were able to tackle the third reproducibility challenge with respect to the special requirements of CompPath: specifying and accessing the data.
By providing imaging data collections according to the FAIR principles, the IDC enabled precise definition of the datasets used in the analysis and ensures that the exact same data can be reused in follow-up studies.
Since metadata on acquisition and processing can be included as DICOM attributes alongside the pixel data, the risk of data confusion can be greatly reduced.
The IDC also facilitated the use of cloud-based ML services because it makes terabytes of WSI data efficiently accessible by on-demand compute resources.
We consider our experiments to be representative of common CompPath applications.
Therefore, the IDC should be similarly usable for other CompPath studies.

In Experiment 2, we tested the transferability of the model trained in Experiment 1 to slides prepared using a different tissue embedding technique by other clinical institutions.
Although the majority of slides were correctly classified, AUC values were significantly lower, indicating that the model is only transferable to a limited extent and additional training is needed.
Since all IDC data collections (both the image pixel data and the associated metadata) are harmonized into a standardized DICOM representation, testing transferability to a different dataset required only minor adjustments to the BigQuery statement.
In the same way, the IDC makes it straightforward to use multiple datasets in one experiment or to transfer an experimental design to other applications.

\subsection{Limitations}

Using cloud-based ML services comes with certain trade-offs.
Conducting computationally intensive experiments requires setting up a payment account and paying a fee based on the type and duration of the computing resources used.
Furthermore, although the ML services are widely used and likely to be supported for at least the next few years, there is no guarantee that they will be compatible and supported over the long term.
Those who do not want to make these compromises can also access IDC data collections without using the aforementioned cloud ML services, either in the cloud or on-premises.
Even if this means losing the previously mentioned advantages with regard to the first two reproducibility challenges, the IDC can still help to specify the data used in a clear and reproducible manner.

Independent of the implementation, a major obstacle to the
reproducibility of CompPath methods remains their high computational cost.
A full training run often takes several days, making reproduction by
other scientists tedious. Performing model inference is generally faster and less resource intensive when compared to model training. 
Therefore, Experiment 2 runs well even with the free
version of Google Colaboratory, enabling others to reproduce it without
spending money. The notebook also provides a demo mode, which completes
in a few minutes, so anyone can easily experiment with how the IDC
works.

At the moment, the IDC exclusively hosts public data collections.
New data must undergo rigorous curation to de-identify (done by TCIA) and harmonize images into standard representation (done by IDC), which is a major effort.
Therefore, only data collections that are of general relevance and high quality are included in the IDC.
As a result, the data in the IDC were usually acquired for other purposes than a particular CompPath application and cannot be guaranteed to be representative and free of bias~\citep{varoquaux2022}.
Compiling truly representative CompPath datasets is very challenging~\citep{homeyer2022}.
Nevertheless, the data collections in the IDC can provide a reasonable basis for exploring and prototyping
CompPath methods.

\subsection{Outlook}

The IDC is under continuous development and its technical basis is constantly
being refined, e.g., to support new data types or to facilitate data
selection and access. Currently, DICOM files in the IDC can only be
accessed as a whole from their respective storage buckets. This
introduces unnecessary overhead when only certain regions of a slide
need to be processed, and it may make it necessary to temporarily cache
slides to efficiently access multiple image regions (see section ``IDC
data access''). Future work should therefore aim to provide efficient
random access to individual regions within a WSI. For maximum
portability, such access should ideally be possible via standard DICOM
network protocols such as DICOMweb~\citep{dicomweb2022, herrmann2018}.

The IDC is continuously being expanded to support even more diverse
CompPath applications. For instance, images collected by the Human Tumor
Atlas Network (HTAN) that provide rich, multispectral information on
subcellular processes~\citep{rozenblattrosen2020} have recently been
added. Images included in the IDC are often accompanied by other types
of data available within other components of CRDC, such as the Genomic
Data Commons~\citep{grossman2016} or the Proteomic Data Commons~\citep{pdc2022}.
This opens up many more potential CompPath applications involving tissue images
and different types of molecular cancer data~\citep{schneider2022}.
In the future, we will build upon the results of
this paper and further investigate how such integrated analyses can be
performed in a reproducible way.

\section{Conclusion}

We demonstrated how the IDC can facilitate the reproducibility of CompPath
studies. If future studies are implemented in a similar manner,
reviewers of research papers can rerun them with significantly lower
effort as compared to the current practice, and verify that the
published results are due to the claimed effects. It also makes it
easier for other researchers to understand the analysis approach, to
compare it with alternative approaches, and to advance it. In this way,
the IDC can help improve both the quality of CompPath research and its
impact.

\section*{Authors contributions}
DPS and AH conceived and carried out the study. AH and AF supervised the project.
AF, MDH, DAC, HH, WC, WJRL, SP and RK supported the study in different ways,
e.g., by providing data, supporting set-up of the computing infrastructure,
interpretation of the results and giving general advice.
AH and DPS drafted the manuscript.
All authors critically revised the manuscript and expressed their consent to the final version.

\section*{Acknowledgements}
The authors thank Lars Ole Schwen for advice on deterministic implementations
of machine learning algorithms and Tim-Rasmus Kiehl for advice on tissue morphology.
The results published here are in whole or part based upon data generated by the TCGA Research Network and the National Cancer Institute Clinical Proteomic Tumor Analysis Consortium (CPTAC).

\section*{Competing Interests}

The authors declare that they have no conflict of interest.

\section*{Funding}

This project has been funded in whole or in part with Federal funds from
the National Cancer Institute, National Institutes of Health, under Task
Order No.~HHSN26110071 under Contract No.~HHSN261201500003l.

% References
\bibliography{ms}

\end{document}
