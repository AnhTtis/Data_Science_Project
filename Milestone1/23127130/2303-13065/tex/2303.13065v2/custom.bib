@article{johnson2019billion,
  title={Billion-scale similarity search with {GPUs}},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  volume={7},
  number={3},
  pages={535--547},
  year={2019},
  publisher={IEEE}
}

@inproceedings{khandelwal2021knnmt,
  title={Nearest Neighbor Machine Translation},
  author={Khandelwal, Urvashi and Fan, Angela and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021}
}

@inproceedings{khandelwal20knnlm,
  title={{Generalization through Memorization: Nearest Neighbor Language Models}},
  author={Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@journal{cui-etal-2021-pretrain,
  title={Pre-Training with Whole Word Masking for Chinese BERT},
  author={Cui, Yiming and Che, Wanxiang and Liu, Ting and Qin, Bing and Yang, Ziqing},
  journal={IEEE Transactions on Audio, Speech and Language Processing},
  year={2021},
  url={https://ieeexplore.ieee.org/document/9599397},
  doi={10.1109/TASLP.2021.3124365},
 }

@article{roberta,
  author = {Yinhan Liu and
               Myle Ott and
               Naman Goyal and
               Jingfei Du and
               Mandar Joshi and
               Danqi Chen and
               Omer Levy and
               Mike Lewis and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {CoRR},
  volume    = {abs/1907.11692},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.11692},
}

@inproceedings{cui-etal-2020-revisiting,
    title = "Revisiting Pre-Trained Models for {C}hinese Natural Language Processing",
    author = "Cui, Yiming  and
      Che, Wanxiang  and
      Liu, Ting  and
      Qin, Bing  and
      Wang, Shijin  and
      Hu, Guoping",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.58",
    doi = "10.18653/v1/2020.findings-emnlp.58",
    pages = "657--668",
}

@inproceedings{wang-etal-2022-training,
    title = "Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data",
    author = "Wang, Shuohang  and
      Xu, Yichong  and
      Fang, Yuwei  and
      Liu, Yang  and
      Sun, Siqi  and
      Xu, Ruochen  and
      Zhu, Chenguang  and
      Zeng, Michael",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.226",
    doi = "10.18653/v1/2022.acl-long.226",
}

@inproceedings{hu-etal-2020-ocnli,
    title = "{OCNLI}: {O}riginal {C}hinese {N}atural {L}anguage {I}nference",
    author = "Hu, Hai  and Richardson, Kyle  and       Xu, Liang  and       Li, Lu  and       K{\"u}bler, Sandra  and       Moss, Lawrence",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.314",
    doi = "10.18653/v1/2020.findings-emnlp.314",
    pages = "3512--3526",
}

@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}

@inproceedings{xu-etal-2020-clue,
    title = "{CLUE}: A {C}hinese Language Understanding Evaluation Benchmark",
    author = "Xu, Liang  and
      Hu, Hai  and
      Zhang, Xuanwei  and
      Li, Lu  and
      Cao, Chenjie  and
      Li, Yudong  and
      Xu, Yechen  and
      Sun, Kai  and
      Yu, Dian  and
      Yu, Cong  and
      Tian, Yin  and
      Dong, Qianqian  and
      Liu, Weitang  and
      Shi, Bo  and
      Cui, Yiming  and
      Li, Junyi  and
      Zeng, Jun  and
      Wang, Rongzhao  and
      Xie, Weijian  and
      Li, Yanting  and
      Patterson, Yina  and
      Tian, Zuoyu  and
      Zhang, Yiwen  and
      Zhou, He  and
      Liu, Shaoweihua  and
      Zhao, Zhe  and
      Zhao, Qipeng  and
      Yue, Cong  and
      Zhang, Xinrui  and
      Yang, Zhengliang  and
      Richardson, Kyle  and
      Lan, Zhenzhong",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.419",
    doi = "10.18653/v1/2020.coling-main.419",
    pages = "4762--4772",
    abstract = "The advent of natural language understanding (NLU) benchmarks for English, such as GLUE and SuperGLUE allows new NLU models to be evaluated across a diverse set of tasks. These comprehensive benchmarks have facilitated a broad range of research and applications in natural language processing (NLP). The problem, however, is that most such benchmarks are limited to English, which has made it difficult to replicate many of the successes in English NLU for other languages. To help remedy this issue, we introduce the first large-scale Chinese Language Understanding Evaluation (CLUE) benchmark. CLUE is an open-ended, community-driven project that brings together 9 tasks spanning several well-established single-sentence/sentence-pair classification tasks, as well as machine reading comprehension, all on original Chinese text. To establish results on these tasks, we report scores using an exhaustive set of current state-of-the-art pre-trained Chinese models (9 in total). We also introduce a number of supplementary datasets and additional tools to help facilitate further progress on Chinese NLU. Our benchmark is released at https://www.cluebenchmarks.com",
}

@inproceedings{lu-etal-2022-reacc,
    title = "{R}e{ACC}: A Retrieval-Augmented Code Completion Framework",
    author = "Lu, Shuai  and
      Duan, Nan  and
      Han, Hojae  and
      Guo, Daya  and
      Hwang, Seung-won  and
      Svyatkovskiy, Alexey",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.431",
    doi = "10.18653/v1/2022.acl-long.431",
    pages = "6227--6240",
}

@inproceedings{pasupat-etal-2021-controllable,
    title = "Controllable Semantic Parsing via Retrieval Augmentation",
    author = "Pasupat, Panupong  and
      Zhang, Yuan  and
      Guu, Kelvin",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.607",
    doi = "10.18653/v1/2021.emnlp-main.607",
    pages = "7683--7698",
    abstract = "In practical applications of semantic parsing, we often want to rapidly change the behavior of the parser, such as enabling it to handle queries in a new domain, or changing its predictions on certain targeted queries. While we can introduce new training examples exhibiting the target behavior, a mechanism for enacting such behavior changes without expensive model re-training would be preferable. To this end, we propose ControllAble Semantic Parser via Exemplar Retrieval (CASPER). Given an input query, the parser retrieves related exemplars from a retrieval index, augments them to the query, and then applies a generative seq2seq model to produce an output parse. The exemplars act as a control mechanism over the generic generative model: by manipulating the retrieval index or how the augmented query is constructed, we can manipulate the behavior of the parser. On the MTOP dataset, in addition to achieving state-of-the-art on the standard setup, we show that CASPER can parse queries in a new domain, adapt the prediction toward the specified patterns, or adapt to new semantic schemas without having to further re-train the model.",
}

@inproceedings{yang-etal-2021-neural-retrieval,
    title = "Neural Retrieval for Question Answering with Cross-Attention Supervised Data Augmentation",
    author = "Yang, Yinfei  and
      Jin, Ning  and
      Lin, Kuo  and
      Guo, Mandy  and
      Cer, Daniel",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.35",
    doi = "10.18653/v1/2021.acl-short.35",
    pages = "263--268",
}

@inproceedings{mao-etal-2021-generation,
    title = "Generation-Augmented Retrieval for Open-Domain Question Answering",
    author = "Mao, Yuning  and
      He, Pengcheng  and
      Liu, Xiaodong  and
      Shen, Yelong  and
      Gao, Jianfeng  and
      Han, Jiawei  and
      Chen, Weizhu",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.316",
    doi = "10.18653/v1/2021.acl-long.316",
    pages = "4089--4100",
    abstract = "We propose Generation-Augmented Retrieval (GAR) for answering open-domain questions, which augments a query through text generation of heuristically discovered relevant contexts without external resources as supervision. We demonstrate that the generated contexts substantially enrich the semantics of the queries and GAR with sparse representations (BM25) achieves comparable or better performance than state-of-the-art dense retrieval methods such as DPR. We show that generating diverse contexts for a query is beneficial as fusing their results consistently yields better retrieval accuracy. Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance. GAR achieves state-of-the-art performance on Natural Questions and TriviaQA datasets under the extractive QA setup when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.",
}

@inproceedings{gur-etal-2021-cross-modal,
    title = "Cross-Modal Retrieval Augmentation for Multi-Modal Classification",
    author = "Gur, Shir  and
      Neverova, Natalia  and
      Stauffer, Chris  and
      Lim, Ser-Nam  and
      Kiela, Douwe  and
      Reiter, Austin",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.11",
    doi = "10.18653/v1/2021.findings-emnlp.11",
    pages = "111--123",
    abstract = "Recent advances in using retrieval components over external knowledge sources have shown impressive results for a variety of downstream tasks in natural language processing. Here, we explore the use of unstructured external knowledge sources of images and their corresponding captions for improving visual question answering (VQA). First, we train a novel alignment model for embedding images and captions in the same space, which achieves substantial improvement in performance on image-caption retrieval w.r.t. similar methods. Second, we show that retrieval-augmented multi-modal transformers using the trained alignment model improve results on VQA over strong baselines. We further conduct extensive experiments to establish the promise of this approach, and examine novel applications for inference time such as hot-swapping indices.",
}

@InProceedings{10.1007/978-3-031-16210-7_56,
author="Drissi, Amani
and Tissaoui, Anis
and Sassi, Salma
and Chbeir, Richard
and Jemai, Abderrazak",
editor="B{\u{a}}dic{\u{a}}, Costin
and Treur, Jan
and Benslimane, Djamal
and Hnatkowska, Bogumi{\l}a
and Kr{\'o}tkiewicz, Marek",
title="S-LDA: Documents Classification Enrichment forÂ Information Retrieval",
booktitle="Advances in Computational Collective Intelligence",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="687--699",
abstract="In recent years, the research on topic modeling techniques has become a hot topic among researchers thanks to their ability to classify and understand a large text corpora which has a beneficial effect on information retrieval performance, but recently user queries are more complicated because they need to know not only which documents are most helpful to them, but also which parts of documents are more or less related to their request. Also, they need to search by topic or document, not merely by keywords.",
isbn="978-3-031-16210-7"
}

@inproceedings{su-etal-2022-contrastive,
    title = "Contrastive Learning-Enhanced Nearest Neighbor Mechanism for Multi-Label Text Classification",
    author = "Su, Xi{'}ao  and
      Wang, Ran  and
      Dai, Xinyu",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.75",
    doi = "10.18653/v1/2022.acl-short.75",
    pages = "672--679",
}


@inproceedings{zheng-etal-2021-adaptive,
    title = "Adaptive Nearest Neighbor Machine Translation",
    author = "Zheng, Xin  and
      Zhang, Zhirui  and
      Guo, Junliang  and
      Huang, Shujian  and
      Chen, Boxing  and
      Luo, Weihua  and
      Chen, Jiajun",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.47",
    doi = "10.18653/v1/2021.acl-short.47",
    pages = "368--374",
}

@inproceedings{williams-etal-2018-broad,
    title = "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
    author = "Williams, Adina  and
      Nangia, Nikita  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1101",
    doi = "10.18653/v1/N18-1101",
    pages = "1112--1122",
}

@article{DBLP:journals/csur/DongLGCLSY23,
  author    = {Chenhe Dong and
               Yinghui Li and
               Haifan Gong and
               Miaoxin Chen and
               Junxin Li and
               Ying Shen and
               Min Yang},
  title     = {A Survey of Natural Language Generation},
  journal   = {{ACM} Comput. Surv.},
  volume    = {55},
  number    = {8},
  pages     = {173:1--173:38},
  year      = {2023},
  url       = {https://doi.org/10.1145/3554727},
  doi       = {10.1145/3554727},
  timestamp = {Thu, 02 Feb 2023 16:04:35 +0100},
  biburl    = {https://dblp.org/rec/journals/csur/DongLGCLSY23.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}