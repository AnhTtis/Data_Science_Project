\appendix
\addcontentsline{toc}{section}{Appendix} %
\part{Appendix} %
\parttoc %


%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\section{Extended Related Work}
\label{supp:extended_related_work}

%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
Exploration is a very broad field, and we encourage readers to read surveys such as \cite{yang2021exploration}. Below, we summarize various subfields of exploration and reinforcement learning relevant to PEG.


\paragraph{Task-directed exploration.} 
\method reasons about the intrinsic motivation / exploration rewards to be gained by commanding a goal-conditioned task policy to various goals in an unsupervised RL setting. Related at a high level, supervised exploration approaches reason about task reward gains during exploration.
\citep{chen2018ucb, osband2019deep} leverage uncertainty estimates about the reward to prioritize states for task-directed exploration.  \citep{stratonovich1966value} proposes "value of information" (VoI) to measure exploration from an information-theoretic perspective. The VoI describes the improvement in supervised task rewards of the current optimal action policy operating under a specified limit on state information. An RL agent may modulate exploration-vs-exploitation in supervised RL settings, by tuning this information limit: a higher limit leads to lower exploration and greater exploitation \citep{sledge2017balancing,cogliati2017learning}.

%


%

 \paragraph{Unsupervised Exploration.} In unsupervised exploration, no information about the task (e.g. reward function or demonstrations) is known during training time. In this setting, a common approach is to define an intrinsic motivation reward correlated with exploration for RL \citep{schmidhuber2010formal,pathak2017curiosity}. Common forms of intrinsic reward incentivize visiting rare states with count-based methods \cite{poupart2006an, bellemare2016count, burda2018exploration} or finding states that maximize prediction error \citep{oudeyer2007intrinsic, pathak2017curiosity, henaff2019explicit, shyam2019model, sekar2020planning}. Within PEG, we leverage an intrinsic motivation policy for the Explore-phase.


%
%


\paragraph{Goal-conditioned RL.}
 %
Goal-conditioned RL (GCRL) extends RL to the multitask setting where the policy is conditioned on a goal and is expected to achieve it. The goal space is commonly chosen to be the state space, although other options like desired returns or language inputs are possible. GCRL is challenging since the policy is required to learn the correct behavior for achieving each goal. This introduces two challenges - optimizing a goal-conditioned policy, and getting adequate data to support the optimization process. 

Many GCRL methods focus on improving goal-conditioned policy optimization. Hindsight Experience Replay \cite{andrychowicz2017hindsight} boosts training efficiency by relabeling failure trajectories as successful trajectories, and \cite{chane2021goal} uses imagined subgoals to guide the policy search process. However, such methods are only useful if the data distribution is diverse enough to cover the space of desired behaviors and goals, and will still suffer in hard exploration environments. Therefore, GCRL is still predominantly constrained by exploration.

\paragraph{Goal-directed Exploration.}
Goal-directed exploration, which sets exploratory goals for the goal-conditioned policy to achieve, is often the exploration mechanism of choice for GCRL as it naturally reuses the goal-conditioned policy. It is important to note that GDE is not inherently tied to GCRL, and can be used in conventional RL to solve hard exploration tasks \citep{ecoffet2021first, guo2020memory,hafner2022deep}. In such cases, goals are usually picked through some combination of reward and novelty terms.


Prior works propose many different metrics for goal choosing, such as frontier-based \citep{yamauchi1998frontier}, learning progress \citep{baranes2013active, veeriah2018many}, goal difficulty \citep{florensa18automatic}, ``sibling rivalry'' \citep{trott2019keeping}, or value function disagreement \citep{zhang2020automatic}. Within this family, we have already discussed and compared against the most closely relevant methods MEGA and Skew-Fit \citep{pong2019skew, pitis2020maximum} (see \cref{sec:baselines} and \cref{sec:results}) which command the agent to rarely seen states to increase the number of seen and familiar states, reasoning that this will enable broader expertise in the trained GCRL policies.

\paragraph{Go-Explore.} The Go-Explore framework from \citep{ecoffet2021first} performs goal-directed exploration by first rolling out the goal-conditioned policy (Go-phase) and then rolling out the exploration policy from the terminal state of the goal-conditioned phase (Explore-phase). Go-Explore does not prescribe a general goal setting method, instead opting for a hand-engineered novelty bonus for each task. GDE methods like MEGA \citep{pitis2020maximum} use the Go-Explore mechanism to further boost exploration efficiency, but do not consider Go-Explore when designing their goal-setting objective. This results in suboptimal performance - MEGA goals chooses low density goals, but this may not necessarily mean they are good states to start the exploration phase from (e.g. dead-ends in a maze). \method's objective already takes the Go-Explore mechanism into account, choosing goals that maximize the exploration of the Explore phase and resulting in superior exploration. 

%


%



%
%

%
%
%


%


%
%


%
%


%
%

%
%

%

%
%
%
%
%



\section{Extended Limitations and Future Work}
\label{supp:extended_limitations}
%
%
%
%
%
%
%


\paragraph{Model-based RL vs. Model-free RL.} \method is a model-based RL agent, which is more sample-efficient and computationally expensive than model-free counterparts. MBRL agents require more computation time and resources since they learn a world model. \method trains a world model to train policies and value functions with imagined trajectories, as well as generate goals for exploration. A model-free version of \method should be computationally and conceptually simpler, and we leave this for future work.


\paragraph{Better world models.} \method depends on a learned world model to plan goals, and may suffer in prediction performance and computation time for longer prediction horizons. An interesting direction would be to combine \method with temporally abstracted world models \citep{neitz2018adaptive,jayaraman2019time, pertsch2020long} to handle such cases. Furthermore, \method should benefit with improved world model architectures such as Transformers \citep{micheli2023transformers}, and thus improving world modelling offers a straightforward path for extending \method.

\paragraph{Exploration beyond state novelty. } Further, while empirically PEG produces performant goal-conditioned policies, it only explores by maximizing novelty. In harder tasks it might be necessary to explicitly consider also exploring with "practice goals" that best improve the goal-conditioned policy. 

Next, \cite{pislar2022when} suggests that generalizing the Go-Explore paradigm itself is beneficial. In Go-Explore, we assume the agent first performs a "Go-phase" for the first half of the episode, and then the "Explore-phase" for the remaining half. However, we can allow the agent to switch between the "Go-phase" and "Explore-phase" multiple times in an episode, and decide the phase lengths.

\paragraph{Scaling PEG to harder tasks.} While we focus on purely unsupervised exploration, in practice some knowledge about the task is always available. For example, a human may give some rewards, a few demonstrations, or some background knowledge about the task. Leveraging such priors to focus exploration on the space of tasks preferred by the user would make GCRL and PEG more practical, especially in real world applications like robotics.

\method currently is only evaluated in state space, and a potential extension would be to handle image observations by optimizing goals in a compact latent space. Such an extension should only require minor edits in the \method code because \method is extended from LEXA, an image-based RL agent that already learns such a latent space.


%
%

%
%



\section{Environments}
%
%
%
\label{supp:environments}
\subsection{Point Maze} The agent, a 2D point spawned on the bottom left of the maze, is evaluated on reaching the top right corner within 50 timesteps. The two-dimensional state and action space correspond to the planar position and velocity respectively. Success is based upon the L2 distance between the position of the agent and the goal being less than 0.15. The entire maze is approximately 10 x 10. This environment is taken directly from \cite{pitis2020maximum} with no modifications.

\subsection{Walker} Next, the walker environment tests the locomotion behavior of a 2D walker robot on a flat plane. The agent is evaluated on its ability to achieve four different standing poses that are placed $\pm 7$, and $\pm 12$ meters in the $x$ axis away from the initial pose. For success, we check if the agent's $x$ position is within a small threshold from the goal pose's $x$ position. The state and goal space is 9 dimensional, containing the walker's $xz$ positions and joint angles. The environment code was taken from \cite{mendonca2021discovering}.

\subsection{Ant Maze} A high-dimensional ant robot needs to move through hallways from the bottom left to top left (see Fig. \ref{fig:environment_description}). This environment is a long-horizon challenge due to the long episode length (500 timesteps) and traversal distance. We evaluate the ant on reaching 4 hard goals at the top left room, and 4 intermediate goals in the middle hallway. Ant Maze has the highest dimensional state and goal spaces: 29 dimensions that correspond to the ant's position, joint angles, and joint velocities. The first three dimensions of the state and goal space represent the $xyz$ position. The next 12 dimensions represent the joint angles for each of the ant's four limbs. The remaining 14 dimensions represent the velocities of the ant in the x-y direction and the velocities of each of the joints. Ant Maze also has an 8 dimensional action space. The 8 dimensions correspond to the hip and ankle actuators of each limb of the ant. Success is based upon the L2 distance between the $xy$ position of the ant and the goal being less than 1.0 meters, which is approximately the width of a cell. The dimensions of the entire maze is approximately 6 x 8 meters. This environment is a modification from the Ant Maze environment in \cite{pitis2020maximum}. We modified the environment so that the state and goal space now includes the ant's $xyz$ positions along with the joint positions and velocities instead of just the $xy$ positions. We also added an additional room in the top left in order to evaluate against a harder goal.

\subsection{3-Block Stack} A robot has to stack three blocks into a tower configuration. The state and goal space for this environment is 14 dimensional. The first five dimensions represent the gripper state and the other nine dimensions represent the $xyz$ positions of each block. The action space is 4 dimensional where the first three dimensions represent the $xyz$ movement of the gripper and the last dimension represents the movement of the gripper finger. Success is based upon the L2 distance between the $xyz$ position of all of object and the goal position of the corresponding object being less than 3cm. This environment is a modification from the FetchStack3 environment in \cite{pitis2020maximum}.

\section{Baselines}
\label{supp:baselines}

\begin{wrapfigure}[8]{r}{0.5\textwidth}
\vspace{-1.5cm}
\scalebox{0.8}{
\begin{minipage}{0.7\textwidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
 \State\textbf{Input:} goal / expl. policies $\pi^G$, $\pi^E$, world model $\widehat{\mathcal{T}}$, rewards $r^G, r^E$
    \State $\mathcal{D} \gets \{ \}$ Initialize buffer. 
    \For{Episode $i=1$ to $N_{\text{train}}$} 
        \State \color{red}$\tau \gets$  GoalDirectedExploration$( \ldots )$\color{black}
       \State $\mathcal{D} \gets D \cup \tau$
       \State Update model $\widehat{\mathcal{T}}$ with $(s_t, a_t, s_{t+1}) \sim D$
       \State Update $\pi^G$ in imagination with $\widehat{\mathcal{T}}$ to maximize $r^G$
       \State Update $\pi^E$ in imagination with $\widehat{\mathcal{T}}$ to maximize $r^E$
       \EndFor
\end{algorithmic}
\caption{MBRL Training}
\label{alg:mbrl_training}
\end{algorithm}
\end{minipage}
}
\end{wrapfigure}


Here, we first present pseudocode for all methods. We start by defining a generic MBRL training loop in \cref{alg:mbrl_training} that alternates between data collection (instantiated by specific methods) and policy and world model updates. Next, we define the pseudocode for PEG and all baselines below. To run a method, we simply drop in the data collection logic specified by the method into the MBRL training loop. For exploration, LEXA and P2E do not use Go-explore, while PEG, MEGA, and Skewfit do. 



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\subsection{Go-Explore}
\label{supp:go_explore}
\begin{wrapfigure}[]{}{0.5\textwidth}
\vspace{-1cm}
\scalebox{0.8}{
\begin{minipage}{0.6\textwidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
%
%
%
%
%
%
%
%
%
%
%
%
%
\Function{GO-EXPLORE($g, \pi^G, \pi^E$)}{}
    \State $s_0 \gets$ env.reset()
    \State $\tau \gets \{s_0\}$
    \For{Step $t=1 \ldots T_{\text{Go}}$}
        \State $s_t \gets$  env.step($\pi^G(s_{t-1}, g)$)
        \State $\tau \gets \tau \cup \{s_t\}$
    \EndFor
    \For{Step $t=1 \ldots T_{\text{Explore}}$}
        \State $s_t \gets$  env.step($\pi^E(s_{t-1}, g)$)
        \State $\tau \gets \tau \cup \{s_t\}$
    \EndFor
    \\\Return $\tau$
\EndFunction
\end{algorithmic}
\caption{Go-explore}
\end{algorithm}
\end{minipage}
}
\end{wrapfigure}
Go-Explore \citep{ecoffet2021first} proposes to switch between a goal-directed policy and exploration policy in a single episode to tackle exploration-intensive environments. Go-explore first stores states of ``interest" as future goal states. Then, in a new episode, Go-explore chooses a goal $g$, and directs a goal-directed policy $\pi^G$ to achieve the goal (or upon timeout $T_\text{Go}$). Then, for the rest of the episode, an exploration policy $\pi^E$ is used. Intuitively, this results in frontier-reaching exploration \citep{yamauchi1998frontier} where the goal-directed policy efficiently traverses to the ``frontier'' of known states before doing exploratory behavior. Of particular importance to Go-Explore is how goal states are chosen - \cite{ecoffet2021first} does not prescribe a general approach for choosing goals. Instead, they choose goals by using task-specific psuedocount tables. 


\subsection{LEXA}
\begin{wrapfigure}[]{}{0.5\textwidth}
\vspace{-1cm}
\scalebox{0.8}{
\begin{minipage}{0.6\textwidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\Function{GoalDirectedExploration($\ldots$)}{}
    \If{$i \% 2 = 0$}
        \State Sample random goal $g$ from buffer.
        \State Collect trajectory $\tau$ with $\pi^G(\cdot \mid \cdot, g)$
    \Else
        \State Collect trajectory $\tau$ with $\pi^E$
    \EndIf
    \\\Return $\tau$
\EndFunction
\end{algorithmic}
\caption{LEXA Goal Sampling}
\end{algorithm}
\end{minipage}
}
\end{wrapfigure}
LEXA \citep{mendonca2021discovering} trains a goal-conditioned policy with model-based RL. It trains two policies, an exploration policy and a goal-conditioned ``achiever'' policy. The original LEXA codebase was designed for image-based environments, so we implemented a state-based LEXA agent by modifying DreamerV2 \citep{hafner2021mastering}. To do so, we added the $\pi^G$ and $\pi^E$ policies, goal-conditioning logic, and the temporal distance reward network.


\subsection{Original MEGA codebase} We used the original MEGA codebase to train a MEGA and a Skewfit agent \citep{pitis2020maximum,pong2019skew}  for 1 million environmental steps to solve the Pointmaze, Ant Maze, and 3-Block Stack tasks mentioned in Section 6. We used the original MEGA implementation with the same hyperparameters except for their goal-conditioned policy training scheme.

MEGA proposes Rfaab, a custom goal relabeling strategy to train the goal-conditioned policy. By default Rfaab relabels transitions with test time goals to train the goal-conditioned policy, which is not possible in our unsupervised exploration setup. 
Therefore, we changed the Rfaab hyperparameters to rfaab\_0\_4\_0\_5\_1, which no longer grants access to test goals.

We find that both MEGA and Skewfit agent fail to scale to the harder environments (Ant Maze and 3-Block Stack) as seen in \cref{fig:original_mega}. One reason could be that the MEGA codebase uses model-free RL, which is more sample inefficient. Therefore, we reimplemented MEGA and Skewfit with LEXA, the backbone MBRL agent that \method uses as well.

\begin{figure}[H]
\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
  & MEGA  & Skewfit\\  \hline
 Point Maze & 90\% & 0\% \\ \hline 
 Ant Maze & 0\% & 0\% \\ \hline
 3-Block Stack & 0\% & 0\% \\
 \hline
\end{tabular}
\end{center}
\caption{Success Rate of MEGA and Skewfit Agent}
\label{fig:original_mega}
\end{figure}

\subsection{Model-based MEGA}
\begin{wrapfigure}[]{}{0.5\textwidth}
%
\scalebox{0.8}{
\begin{minipage}{0.6\textwidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\Function{GoalDirectedExploration($\ldots$)}{}
\State $g \gets \min_{g \in \mathcal{D}} \widehat{p}(g)$
\State $\tau \gets \textit{GO-EXPLORE}(g, \pi^G, \pi^E)$
    \\\Return $\tau$
\EndFunction
\end{algorithmic}
\caption{MEGA Goal Sampling}
\label{supp:mega_code}
\end{algorithm}
\end{minipage}
}
\end{wrapfigure}
For model-based MEGA, we simply port over MEGA's KDE model. Conveniently, LEXA already trains a goal-conditioned value function, which we use for filtering out goals by reachability. We use the same KDE hyperparameters as the original MEGA paper. As seen in in \cref{fig:learning_curves}, our MEGA implementation gets non-trivial success rates, improving over the original MEGA baseline.

\subsection{Model-based Skewfit}


\begin{wrapfigure}[]{}{0.5\textwidth}
\vspace{-1cm}
\scalebox{0.8}{
\begin{minipage}{0.6\textwidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\Function{GoalDirectedExploration($\ldots$)}{}
\State $g \sim \left\{g \cdot \widehat{p}(g)^{-1} \mid g \in \mathcal{D} \right\}$
\State $\tau \gets \textit{GO-EXPLORE}(g, \pi^G, \pi^E)$
    \\\Return $\tau$
\EndFunction
\end{algorithmic}
\caption{Skewfit Goal Sampling}
\label{supp:skewfit_code}
\end{algorithm}
\end{minipage}
}
\end{wrapfigure}

For Skewfit, we port over the Skewfit baseline in MEGA's codebase. The implementation is straightforward and similar to model-based MEGA. Instead of picking the minimum density goal, Skewfit picks goals inversely proportional to their density. As seen in in \cref{fig:learning_curves}, our Skewfit implementation gets non-trivial success rates.

\subsection{Plan2Explore}


\begin{wrapfigure}[]{}{0.5\textwidth}
\vspace{-1cm}
\scalebox{0.8}{
\begin{minipage}{0.6\textwidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\Function{GoalDirectedExploration($\ldots$)}{}
\State Collect trajectory $\tau$ with $\pi^E$
    \\\Return $\tau$
\EndFunction
\end{algorithmic}
\caption{Plan2Explore}
\end{algorithm}
\end{minipage}
}
\end{wrapfigure}

Our Plan2Explore baseline uses a Plan2Explore exploration policy to gather data for the world model and trains a goal-conditioned policy in imagination. Our LEXA implementation already uses Plan2Explore, so this variant simply requires us to only rollout the exploration policy in the real world.

\section{Implementation Details}
\label{supp:implementation_details}
\subsection{MPPI}
MPPI is a popular optimizer for planning actions with dynamics models~\citep{williams2015model, nagabandi2019deep}. Instead of planning sequences of actions, which grows linearly with horizon, \method only needs to ``plan'' with a goal variable, making the search space independent of the horizon. First, we present a high-level summary of MPPI, and refer interested readers to \cite{nagabandi2019deep} for a more detailed look at MPPI. 

The process begins by randomly sampling $N$ goal candidates  $g$ from an initial distribution (see below for more details). These candidates are then evaluated as described in \cref{fig:peg-method} - we generate goal-conditioned trajectories for each goal $g_k$ through the world model, and compute the expected exploration value $V_k$ for each goal. This exploration value acts as the ``score'' for the goal candidate. Once we have scores for each goal candidate, a Gaussian distribution is fit according to the rule:
\begin{align}
    \mu_t &= \frac{ \sum_{k=0}^N (e^{\gamma \cdot V_k})(g_k)}{\sum_{k=0}^N (e^{\gamma \cdot V_k})}
\end{align} 
where $\gamma$ is the reward weight hyperparameter. We then sample candidates from the computed Gaussian, and repeat the process for multiple iterations. 


\paragraph{MPPI initial distribution} Throughout our paper, we initialize the MPPI optimizer with goal vectors sampled uniformly at random from the full goal space. One alternative would have been to restrict the initial candidates to lie in the agent’s replay buffer, but we found in practice that this restriction made no difference and it was more convenient for implementation reasons to simply sample uniformly at random from the unrestricted goal space (same as the state space), e.g. using the joint limits of the various robot joints. 

\begin{figure}[H]
\centering
\includegraphics[width=0.25\textwidth]{figs/statespace_curves.png}
\caption{Performance of state space sampling variants on the Ant Maze. Removing state space sampling (PEG-Restricted) does not degrade PEG performance, and adding state space sampling (MEGA-Unrestricted) does not boost performance. }
\label{fig:statespace_sampling}
\end{figure}

In \cref{fig:statespace_sampling},  we show results of running PEG with MPPI initialization from the replay buffer (``PEG-Restricted”), in the Ant Maze environment. Its performance is virtually indistinguishable from the original PEG method. This is to be expected, and is a sign that the optimizer is working well.

Could other methods benefit from sampling outside the replay buffer?
In \cref{fig:statespace_sampling} we allowed our strongest baseline MEGA to also sample goals from outside the replay buffer (``MEGA-Unrestricted"). However, these methods rely on heuristic objectives for selecting goals (like “low density” of the visited state distribution) which only approximate the correct goal-setting objective for states close to the replay buffer. Therefore, “MEGA-Unrestricted” is unable to sample effective goals outside the replay buffer, and fares very poorly. In contrast, PEG directly optimizes the exploration value of goal states, and permits choosing meaningful goals even in this unrestricted setting. This is an advantage, since it permits optimizing over a larger space of goal candidates, which leads to better goals.



\subsection{Runtime}
\begin{figure}[H]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
              & Total Runtime (Hours) & Episodes & Episode Length & Seconds per Episode \\ \hline
Point Maze    & 16                    & 12000    & 50             & 4.8                 \\ \hline
Walker        & 16                    & 5000     & 150            & 11.52               \\ \hline
AntMaze       & 48                    & 2000     & 500            & 86.4                \\ \hline
3-Block Stack & 48                    & 6666     & 150            & 25.9                \\ \hline
\end{tabular}
\end{center}
\caption{Runtimes per experiment.}
\label{supp:runtime}
\end{figure}

The runtime and  resource usage between methods did not differ significantly, as everything is implemented on top of the backbone LEXA model-based RL agent. Note that model-free agents tend to run significantly faster than model-based agents, but fail to explore and optimize good policies. See \cref{supp:baselines} where we found model-free variants of MEGA and Skewfit failed to learn at all. Runtime is dominated by neural network updates of the policies and world model, not the goal selection routine defined by each method. Each seed was run on 1 GPU (Nvidia 2080ti or Nvidia 3090) and 4 CPUs, and required ~11GB of GPU memory. See \cref{supp:runtime} for training time. 

\begin{figure}[H]
\begin{center}
\begin{tabular}{|c|c|}
\hline
\textbf{}     & Seconds / Episode \\ \hline
PEG:          & 0.51              \\ \hline
MEGA:         & 0.48              \\ \hline
SkewFit:      & 0.46              \\ \hline
\end{tabular}
\end{center}
\caption{Runtime for goal selection.}
\label{supp:goal_runtime}
\end{figure}
Next, we benchmarked the goal selection procedure for PEG and the other goal selection baselines in the block stacking environment and recorded the average wall clock time in \cref{supp:goal_runtime}.

We can see that there is little difference in speed between methods, and the overhead introduced by PEG goal selection is minimal to LEXA, and competitive with other goal selection baselines. Because LEXA does not select goals, it finishes up to an hour earlier than goal setting methods in our experiments. As a sidenote, the times reported above are amortized over episodes, since we compute a set of 50 goals every 50 episodes for all methods, rather than computing 1 goal per episode. 

\subsection{Hyperparameters}
For parameter tuning, we used the default hyperparameters of the LEXA backbone MBRL agent (e.g. learning rate, MLP size, etc.) and kept them constant across all baselines. PEG only has 1 hyperparameter that requires tuning - the reward weight, which affects the MPPI update distribution. The higher the weight, the more we update the goal sampling distribution towards high-reward trajectories. For each experiment, we tried weight values of  (1, 2, 10) by running 1-2 seeds of PEG for each value. We used a weight of 1, 2, 2, 10 for the 4 experiments respectively
.
PEG uses MPPI, a sample-based optimizer, to optimize the objective. While MPPI has hyperparameters such as the number of samples and number of iteration rounds, these are easy to tune, since optimizer performance increases with the amount of samples and iterations. We therefore just choose as many samples (2000 candidates) and rounds (5 optimization rounds) as we can while keeping training time reasonable.

\section{Additional Experiments}

\subsection{\method with other exploration methods}
\begin{figure}[ht]
\centering
\includegraphics[width=0.25\textwidth]{figs/p2e_vs_rnd.png}
\caption{P2E vs. RND}
\label{fig:p2e_vs_rnd}
\end{figure}
While we chose to train our exploration policy and value function with Plan2Explore, \method can work with any exploration method as long as it provides an exploration reward and policy. Random Network Distillation (RND) is another powerful exploration method that rewards discovering new states. We run both variants with 5 seeds in the Ant environment \cref{fig:p2e_vs_rnd}. We find that the RND variant still explores the environment, albeit not as well as P2E. We hypothesize P2E exploration is particularly synergistic with our model-based RL framework, since the P2E explorer explicitly seeks transitions $(s,a,s')$ that improve the model. As the model accuracy increases, so does the quality of the goals generated by \method.


\subsection{MPPI vs CEM for Goal Optimization}

\begin{figure}[ht]
 \centering
  \includegraphics[width=0.3\textwidth]{figs/mppi_vs_cem.png}
\caption{MPPI vs. CEM}
\label{fig:mppi_vs_cem}
\end{figure}

We investigate the impact of the goal optimizer on \method. We choose two popular optimizers, MPPI and CEM, to optimize our objective, and run 5 seeds of each variant in the stacking environment. As seen in Fig.~\ref{fig:mppi_vs_cem}, the two agents perform similarly, showing that the \method objective is robust to the choice of optimizer.



\subsection{Does conditioning on \method goals maximize exploration?}
\begin{figure}[ht]
 \centering
 \includegraphics[width=0.5\textwidth]{figs/ant_goal_reaching.png}
 \caption{\method exploration in a U-maze.  Brown background dots: explored states, commanded goals: $\star$, resulting paths: colored lines. (Left) 
 \method optimizes directly for exploration, even setting unseen goals, and achieving farther paths. (Right) Setting goals at the frontier of the seen state distribution yields less exploration.
}
\label{fig:frozen_exp}
\end{figure}
We assess the importance of accounting for the goal-conditioned policy's capabilities. To do so, we evaluate if states coming from a policy conditioned on \method goals are more novel than states coming from the same policy conditioned on other goal strategies. In \cref{fig:frozen_exp}, we take a partially trained agent and its replay buffer, and plot the proposed goals and resulting trajectories for \method and MEGA (called Frontier in the figure). We find that \method goals lead the ant to the top left more often. This figure is also used in the intro for exposition.


\subsection{More goals for 3-Block Stacking}
\label{supp:block_more_goals}
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/intermediate_block_curves.png}
 \caption{Performance of agents evaluated on different types of goals in the 3-Block environment.All methods are run with 10 seeds. Easy goals require picking up one block, medium goals require stacking two blocks, and hard goals require stacking all 3 blocks.
 }
    \label{fig:intermediate_curves}
\end{figure}

We have defined three types of goals of increasing difficulty - picking up a single block (“Easy”), stacking two blocks (“Medium”), and stacking three blocks (“Hard”). For each goal type, we set specific evaluation goals by varying the location of the pick / stack as well as the choice and ordering of the blocks. In this way, we create 3 distinct Easy goals, 6 Medium goals, and 6 Hard goals.

First, PEG remains clearly the strongest at 3-block stacking (Hard) as seen in \cref{fig:intermediate_curves}. The difficulty-based task grouping permits more interesting analysis: PEG performs competitively with the best approaches on the Easy goals, and its gains over baselines are larger for Medium and Hard goals where exploration is most required. On these harder categories of tasks, PEG is both quickest to achieve non-trivial performance (indicating the onset of exploration) and also achieves the highest peak performance.

An interesting side note here is that better exploration tends to create greater challenges for multi-task goal-conditioned policy function approximation with a fixed capacity policy network. Around the 0.5M steps mark, when PEG starts to achieve Hard goals, it grows marginally weaker on Easy and Medium goals. This kind of “forgetting” is well-studied in the continual learning literature; addressing this well may require expanding policy network capacity over time, as in progressive networks.



%
%
%
%
%

%
%
%
%

%
%
%
%
%
%
%


%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

