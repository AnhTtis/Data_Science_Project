
\documentclass{article} %
\usepackage{iclr2023_conference,times}

%
%
%

\usepackage{amsmath,amsfonts,bm}

%
\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

%
\newcommand{\newterm}[1]{{\bf #1}}


%
\def\figref#1{figure~\ref{#1}}
%
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
%
\def\secref#1{section~\ref{#1}}
%
\def\Secref#1{Section~\ref{#1}}
%
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
%
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
%
\def\eqref#1{equation~\ref{#1}}
%
\def\Eqref#1{Equation~\ref{#1}}
%
\def\plaineqref#1{\ref{#1}}
%
\def\chapref#1{chapter~\ref{#1}}
%
\def\Chapref#1{Chapter~\ref{#1}}
%
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
%
\def\algref#1{algorithm~\ref{#1}}
%
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
%
\def\partref#1{part~\ref{#1}}
%
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


%
\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
%
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

%
\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

%
\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

%
\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

%
\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

%
\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

%
\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

%
\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

%
\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


%
\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

%
\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
%
%
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

%
\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

%
%
\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

%
\newcommand{\pdata}{p_{\rm{data}}}
%
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
%
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
%
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} %

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
%
%
%
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} %

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\newcommand{\ed}[1]{{\color{blue}{#1}}}
\newcommand{\Skip}[1]{}
%
%
\newcommand{\kun}[1]{{\color{green}{\small\bf\sf [kun: #1]}}}
\newcommand{\cc}[1]{{\color{red}{#1}}}
\newcommand{\BE}{\mathbb{E}}
\newcommand{\method}{PEG~}

\usepackage{enumitem}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage[colorinlistoftodos]{todonotes}
\setlength{\marginparwidth}{3.5cm}
\newcommand{\jd}[1]{{\todo[size=\tiny]{DJ: #1}}}
%
\newcommand{\jdc}[1]{\begin{scriptsize}{\textcolor{orange}{[DJ: #1]}}\end{scriptsize}}
%
\newcommand{\jdtext}[1]{\textcolor{black}{#1}}

\newcommand{\drop}[1]{\begin{scriptsize}{\textcolor{green}{[#1]}}\end{scriptsize}}
\usepackage[capitalise,noabbrev,nameinlink]{cleveref}

\newcommand{\ehc}[1]{{\todo[size=\tiny,color=cyan]{EH: #1}}}
%
\newcommand{\ehtext}[1]{\textcolor{black}{#1}}
\newcommand{\oleg}[1]{{\todo[size=\tiny,color=yellow]{O: #1}}}
\newcommand{\otext}[1]{\textcolor{yellow}{#1}}
\newcommand{\richard}[1]{{\todo[size=\tiny,color=green]{RC: #1}}}




\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xcolor}         %
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{float}
\usepackage{multirow}
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
%
\usepackage{listings}
\usepackage{soul}
\usepackage{etoolbox}

\usepackage[toc,page,title]{appendix}
\usepackage{minitoc}

%
\renewcommand \thepart{}
\renewcommand \partname{}

\newcommand{\vars}{\texttt}
\newcommand{\func}{\textrm}
%
%
%
%
%
%
%
%
%
%
%

%
\title{Planning Goals for Exploration}
%
%
%

%
%
%

\author{
  Edward S. Hu \hskip2em Richard Chang \hskip2em  Oleh Rybkin \hskip 2em Dinesh Jayaraman\\
    GRASP Lab, Department of CIS, University of Pennsylvania \\ 
  \texttt{\{hued, huangkun, oleh, dineshj\}@seas.upenn.edu} \\
}


%
%
%
%
%
%
%

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\projectwebsite}{https://penn-pal-lab.github.io/peg/}

\iclrfinalcopy %
\begin{document}
\doparttoc %
\faketableofcontents %
\maketitle


\begin{abstract}
%
Dropped into an unknown environment, what should an agent do to quickly learn about the environment and how to accomplish diverse tasks within it? We address this question within the goal-conditioned reinforcement learning paradigm, by identifying how the agent should set its goals at training time to maximize exploration. We propose ``Planning Exploratory Goals'' (PEG), a method that sets goals for each training episode to directly optimize an intrinsic exploration reward. PEG first chooses goal commands such that the agent's goal-conditioned policy, at its current level of training, will end up in states with high exploration potential. It then launches an exploration policy starting at those promising states. To enable this direct optimization, PEG learns world models and adapts sampling-based planning algorithms to ``plan goal commands''. In challenging simulated robotics environments including a multi-legged ant robot in a maze, and a robot arm on a cluttered tabletop, PEG exploration enables more efficient and effective training of goal-conditioned policies relative to baselines and ablations. Our ant successfully navigates a long maze, and the robot arm successfully builds a stack of three blocks upon command. Website: \url{\projectwebsite}
%
%
%
%

%

%
%
%

%
%
%
\end{abstract}

%
\section{Introduction}\label{sec:intro}
%
%

%


%

%

%
%
%

%


%

Complex real-world environments such as kitchens and offices afford a large number of configurations. These may be represented, for example, through the positions, orientations, and articulation states of various objects, or indeed of an agent within the environment. Such configurations could plausibly represent desirable goal states for various tasks. Given this context, we seek to develop intelligent autonomous agents that, having first spent some time exploring an environment, can afterwards configure it to match commanded goal states.

%
%

%
The goal-conditioned reinforcement learning paradigm (GCRL) \citep{andrychowicz2017hindsight} offers a natural framework to train such goal-conditioned agent policies on the exploration data. Within this framework, we seek to address the central problem: 
%
\textbf{how should a GCRL agent explore its environment during training time so that it can achieve diverse goals revealed to it only at test time?} This requires efficient unsupervised exploration of the environment.

%

%


%

%
Exploration in the GCRL setting can naturally be reduced to the problem of setting goals for the agent during training time; the current GCRL policy, commanded to the right goals, will generate exploratory data to improve itself \citep{ecoffet2021first,nair2018visual}.
%
%
Our question now reduces to \textbf{the goal-directed exploration problem: how should we choose exploration-inducing goals at training time?} 

Prior works start by observing that the final GCRL policy will be most capable of reaching familiar states, encountered many times during training. Thus, the most direct approach to exploration is to set goals in sparsely visited parts of the state space, to directly expand the set of these familiar states
%
%
 \citep{ecoffet2021first,pong2019skew,pitis2020maximum}. While straightforward, this approach suffers from several issues in practice.
%
First, the GCRL policy during training is not yet proficient at reaching arbitrary goals, and regularly fails to reach commanded goals, often in uninteresting ways that have low exploration value. For example, a novice agent commanded to an unseen portion of a maze environment might respond by instead reaching a previously explored part of the maze, encountering no novel states. To address this, prior works \citep{pitis2020maximum,bharadhwaj2021leaf} set up additional mechanisms to filter out unreachable goals, typically requiring additional hyperparameters. 
%
%
%
%
%
Second, recent works~\citep{ecoffet2021first, yang2022when, pitis2020maximum} have observed improved exploration in long-horizon tasks by extending training episodes. Specifically, rather than resetting immediately after deploying the GCRL policy to a goal, these methods launch a new exploration phase right afterwards, such as by selecting random actions \citep{pitis2020maximum, kamienny2022direct} or by maximizing an intrinsic motivation reward \citep{guo2020memory}. In this context, even successfully reaching a rare state through the GCRL policy might be suboptimal; many such states might be poor launchpads for the exploration phase that follows. For example, the GCRL policy might end up in a novel dead end in the maze, from which all exploration is doomed to fail.   
%
%

%
%
\begin{wrapfigure}[16]{b}{0.45\textwidth}
\vspace{-0.5cm}
 \centering
 \includegraphics[width=0.4\textwidth]{figs/ant_goal_reaching.png}
 \caption{\method exploration in a U-maze.  Brown background dots: explored states,  $\star$: commanded goals, colored lines: resulting paths. (Left) 
 \method optimizes directly for exploration, even setting unseen goals, and achieving farther paths. (Right) Setting goals at the frontier of the seen state distribution yields less exploration.
}
 \label{fig:teaser}
\end{wrapfigure}
To avoid these shortcomings and focus exploration on the most promising parts of the environment, we propose to leverage planning with world models in a new goal-directed exploration algorithm, PEG (short for "Planning Exploratory Goals"). \textbf{Our key idea is to optimize directly for goal commands that would induce high exploration value trajectories}, cognizant of current shortcomings in the GCRL policy, and of the exploration phase during training.
%
Note that this \textit{does not} mean merely commanding the agent to novel or rarely observed states. Instead, PEG commands might be to a previously observed state, or indeed, even to a
%
physically implausible state (see \cref{fig:teaser}). PEG only cares that the command will induce the chained GCRL and exploration phases together to generate interesting training trajectories, valuable for policy improvement.
%
%
%
%

%

%
Our key contributions are as follows. We propose a novel paradigm for goal-directed exploration by directly optimizing goal selection to generate trajectories with high exploration value. Next, we show how learned world models permit an effective implementation of goal command planning, by adapting planning algorithms that are often used for low-level action sequence planning. We validate our approach, PEG, on challenging simulated robotics settings including a multi-legged ant robot in a maze, and a robot arm on a cluttered tabletop. In each environment, PEG exploration enables more efficient and effective training of generalist GCRL policies relative to baselines and ablations. Our ant successfully navigates a long maze, and the robot arm successfully builds a stack of three blocks.
%
%
%
\section{Problem Setup and Background}
%
%
%
%
%
%

%
%

%

%


We wish to build agents that can efficiently explore an environment to autonomously acquire diverse environment-relevant capabilities. Specifically, in our problem setting, the agent is dropped into an unknown environment with no specification of the tasks that might be of interest afterwards. Over episodes of unsupervised exploration, it must learn about its environment, the various ``tasks'' that it affords to the agent, and also how to perform those tasks effectively. We focus on goal state-reaching tasks. After this exploration, a successful agent would be able to reach diverse previously unknown goal states in the environment upon command. 

To achieve this, our method, \textbf{planning exploratory goals (PEG)}, focuses on improving unsupervised exploration within the goal-conditioned reinforcement learning framework. We set up notation and preliminaries below, before discussing our approach in \secref{sec:peg}.

%

%
%

%



%


%
%
\vspace{-0.3cm}
\paragraph{Preliminaries.} We formalize the {unsupervised} exploration stage within a goal-conditioned Markov decision process, defined by the tuple $(\mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{G})$. The unsupervised goal-conditioned MDP does not contain the test-time goal distribution, nor a reward function.  At each time $t$, a goal-conditioned policy $\pi^G(\cdot \mid s_t, g)$ in the current state $s_t \in \mathcal{S}$, under goal command $g \in \mathcal{G}$, selects an action $a_t \in \mathcal{A}$, and transitions to the next state $s_{t+1}$ with probability $\mathcal{T}(s_{t+1} | s_t, a_t)$.  
%
%
To enable handling of the most broadly expressive goal-reaching tasks, we set the goal space to be identical to the state space: $\mathcal{G} = \mathcal{S}$, i.e., every state maps to a plausible goal-reaching command. 
%
%

%

%

%
%
%
%

%

%
%
In our setting, an agent must learn a goal-conditioned reinforcement learning (GCRL) policy $\pi^G(\cdot \mid s, g)$ as well as collect exploratory data to train this policy on. Thus, as motivated in \secref{sec:intro}, it is critical to pick good goal commands during training because doing so generates useful data to improve the policy's ability to achieve diverse goals. 

%

Critical to our method's design is the specific mechanism through which goal commands influence the exploration data. We follow the recently popular ``Go-explore'' procedure \citep{ecoffet2021first, pislar2022when, tuyls2022multistage} for exploration-intensive long-term GCRL settings. Every training episode starts with the selection of a goal command $g$. Then, a goal-directed "\textbf{Go-phase}" starts, where the GCRL policy $\pi^G$ is executed, conditioned on $g$, producing a final state $s_T$ after $T$ steps. Immediately, an ``\textbf{Explore-phase}'' commences, with an undirected exploration policy $\pi^E(\cdot \mid s)$ taking over for the remaining $T_E$ timesteps. For example, $\pi^E(\cdot \mid s)$ could be an RL policy trained to maximize an intrinsic exploration reward $r^E$ \citep{bellemare2016count,  pathak2017curiosity,  burda2018exploration, sekar2020planning}. \ehtext{See \cref{supp:go_explore} for detailed pseudocode}.
This structure of training episodes has been shown to result in richer exploration \citep{pislar2022when}. Intuitively, the goal-conditioned policy efficiently brings an agent to an interesting goal, and the undirected exploration policy efficiently explores around the goal. \jdtext{However, Go-explore prescribes no general way to select goals that induce exploration.}

%

%
%

%
%


%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
\jdtext{
In this paper, we propose \method, a general goal command selection objective for Go-Explore-style unsupervised exploration.
%
\method uses a learned world model, making it natural to implement it alongside model-based RL approaches that already train such a model. 
%
%
We implement \method exploration for ``latent explorer achiever'' (LEXA) \citep{mendonca2021discovering}, a goal-conditioned MBRL framework.
%
We briefly summarize the LEXA framework below and refer the reader to \cref{supp:baselines} for details. LEXA learns the following components:}
\begin{gather}
\begin{aligned}\label{eq:lexa}
&\text{World model:} && \widehat{\mathcal{T}}_\theta(s_t | s_{t-1}, a_{t-1}) 
&& && \\
&\text{Exploration policy:} && \pi^E_\theta(a_t | s_t) 
&&\text{Goal Reaching policy:} && \pi^G_\theta(a_t | s_t, g) \\
&\text{Exploration value:} && V^E_\theta(s_t) 
&&\text{Goal Reaching value:} && V^G_\theta(s_t, g) \\
\end{aligned}
\end{gather}%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\begin{wrapfigure}[8]{r}{0.4\textwidth}
\vspace{-0.8cm}
\scalebox{0.8}{
\begin{minipage}{0.53\textwidth}
\begin{algorithm}[H]
    \centering
    \caption{LEXA Training Loop}\label{algorithm}\label{alg:lexa}
    \footnotesize
    \begin{algorithmic}[1]
    \State\textbf{Input:} $\pi^G$, $\pi^E$, world model $\widehat{\mathcal{T}}$, rewards $r^G, r^E$
    \State $\mathcal{D} \gets \{ \}$ Initialize buffer. 
    \For{Episode $i=1$ to $N_{\text{train}}$} 
        \State \color{red}$\tau \gets$ GoalDirectedExploration$( \ldots )$\color{black}
       \State $\mathcal{D} \gets D \cup \tau$
       \State Update model $\widehat{\mathcal{T}}$ with $(s_t, a_t, s_{t+1}) \sim D$
       \State Update $\pi^G$ in imagination with $\widehat{\mathcal{T}}$ to maximize $r^G$
       \State Update $\pi^E$ in imagination with $\widehat{\mathcal{T}}$ to maximize $r^E$
       \EndFor
    \end{algorithmic}
\end{algorithm}
\end{minipage}
}
\end{wrapfigure}
The world model is trained as a variational recurrent state space model following \cite{hafner2020learning}. The explorer and goal reaching policies are trained with the model-based on-policy actor-critic Dreamer algorithm \citep{hafner2021mastering}. Both policies are trained purely in imagination through world model ``rollouts'', i.e., the world model acts as a learned simulator in which to run entire training episodes for training these policies.
The explorer reward is the Plan2Explore \citep{sekar2020planning} disagreement objective, which incentivizes reaching states that cause an ensemble of world models to disagree amongst themselves. The goal reaching reward $r^G$ is the self-supervised temporal distance objective \citep{mendonca2021discovering}, which rewards the policy for minimizing the number of actions between the current state and a goal state.

\begin{wrapfigure}[]{r}{0.4\textwidth}
\vspace{-0.8cm}
\scalebox{0.8}{
\begin{minipage}{0.45\textwidth}
\begin{algorithm}[H]
    %
    \caption{LEXA Goal Sampling}\label{alg:lexa_goal}
    \footnotesize
    \begin{algorithmic}[1]
\Function{GoalDirectedExploration($\ldots$)}{}
    \If{episode $i$  $\%$  $2 = 0$}
        \State Sample random goal $g$ from buffer.
        \State Collect trajectory $\tau$ with $\pi^G(\cdot \mid \cdot, g)$
    \Else
        \State Collect trajectory $\tau$ with $\pi^E$
    \EndIf
    \\\Return $\tau$
\EndFunction
    \end{algorithmic}
\end{algorithm}
\end{minipage}
}
\end{wrapfigure}
\ehtext{See \cref{alg:lexa} for high level pseudocode of the training process. Highlighted in red is the goal-directed exploration strategy (\cref{alg:lexa_goal}). LEXA explores by sampling random goals to reach or by running the exploration policy. As we will see in the next section, PEG serves as a drop-in replacement for LEXA's random goal sampling by sampling goals optimized for Go-explore exploration.}




%


%
%



%
%
%
%
%
%

%
 
%

%
 

\section{Planning Goals for Exploration}\label{sec:peg}
%
%

%
%

%
Having set up the preliminaries, we return to the core question of this paper: how should we pick exploration-inducing goals to help acquire diverse skills?
%
We have argued above that the answer is not to pick goals at the frontier of previously explored states, as in prior work \citep{yamauchi1998frontier, pitis2020maximum, bharadhwaj2021leaf}.
%
Instead, at the beginning of each training episode, we propose to optimize directly for goals that would lead to the highest exploration rewards during the episode's Explore-phase:
\begin{align}
\label{eq:page_objective_traj}
& \max_g  \mathbb E_{p_{\pi^G(\cdot|\cdot,g)}(s_T)}\left[ V^E(s_T) \right], \\
& \text{where} \quad  V^{E}(s_T) = \BE_{\pi^E} \left[\sum^{T+T_E}_{t=T+1} \gamma^{t-T-1} r^E_{t} \right],
\end{align} 
and where $p_{\pi^G(\cdot|\cdot,g)}(s_T)$ is the distribution of terminal states generated when $\pi^G$ runs for $T$ steps
%
%
conditioned on goal $g$. $V^{E}$, which we call \textit{exploration value}, is the expected discounted {intrinsic motivation} return of the exploration policy, when launched from state $s_T$.
%
Please refer to \cref{fig:peg-method} for a detailed visualization. 


\begin{figure}[ht]
 \centering
   \includegraphics[width=1.0\textwidth]{figs/peg_method.pdf}
 \caption{\method proposes goals for Go-explore style exploration. 1) We start by sampling 
 goal candidates $g_1 \ldots g_N$. 2)  For each goal $g$, we execute the goal-conditioned policy $\pi^G$ in the world model $\widehat{\mathcal{T}}$ to generate trajectories. 3) We score each goal by seeing how useful their trajectories are for launching an exploration policy. 4) Explore using the best $g$.
  %
 %
 %
 %
 }
 \label{fig:peg-method}
\end{figure}


Importantly, this objective involves computing the \textit{expected} exploration value of terminal states $s_T$.  The exploration value measures how interesting will the Explore-phase be as measured by intrinsic motivation rewards. Next, note that the expectation in \cref{eq:page_objective_traj} is over the terminal states induced by the current GCRL policy. This way, it accounts for the capabilities of the current GCRL policy, regardless of whether it is a novice or an expert at achieving the selected goal $g$.
%
%
If we had instead sought to optimize for goals $g$ with high exploratory value, in similar spirit to prior works \citep{pong2019skew,pitis2020maximum,ecoffet2021first}, the objective of \cref{eq:page_objective_traj} would instead be replaced by $V^E(g)$, the exploration value of the goal, which is naively agnostic to the capabilities of the GCRL policy. As we have argued above, this would then require additional mechanisms to account for reachability of the specified goal etc., bringing new hyperparameters and related overheads. Instead, \method offers a unified and compact solution. In plain English, ``first command goals that lead the goal policy to states that have high future exploration potential, then explore.'' 

%

%

%
%
%
%
%
%


%
%

%
%
The objective in \cref{eq:page_objective_traj} is not straightforward to compute, because it relies on the terminal state distribution induced by the goal-conditioned policy $p_{\pi^G(\cdot|\cdot,g)}(s_T)$. This distribution can change rapidly throughout training, as the policy evolves. To explore well, it is important to use an up-to-date estimate. %
We leverage the learned world model $\widehat{\mathcal{T}}$ to achieve this: $\hat{p}_{\pi^G( \cdot \mid \cdot, g)}(\tau)$ can be represented as the product of the transition and goal-conditioned policy distributions. 
%
%
%
%
%
%
%
%
\begin{align}
\hat{p}_{\pi^G(\cdot \mid \cdot, g)}(\tau)&= p(s_0) \large\left[\prod^{T}_{t=1} \mathcal{\widehat{T}}(s_t \mid s_{t-1}, a_{t-1}) \pi^G(a_{t-1} \mid s_{t-1}, g)\large\right]
\end{align}

Now, we may approximate the expectation over the marginal $p_{\pi^G(\cdot|\cdot,g)}(s_T)$ in the objective \cref{eq:page_objective_traj} by sampling trajectories $\tau$ from the learned world model and using the last state $s_T$:
\begin{align}
\label{eq:page_objective_sample}
\mathbb E_{\hat{p}_{\pi^G(\cdot \mid \cdot, g)}(s_T)}[ V^E(s_T) ] \approx \frac{1}{K} \sum_k V_\theta^E(s^k_T) \quad \quad \text{ where } s^k_T \sim \hat{p}_{\pi^G(\cdot \mid \cdot, g)}(\tau)
\end{align}
This permits on-the-fly generation of an up-to-date estimate of our objective from \cref{eq:page_objective_traj}, using a recent goal-conditioned RL policy. Then, we can optimize over goals $g$ using sample-based optimization. Following \cref{eq:page_objective_sample}, to evaluate a goal $g$, we condition the goal-conditioned policy on $g$, and roll it out for $K$ trajectories 
%
$\tau_k$ within the learned world model. We then estimate the terminal state exploration value for each trajectory with the learned exploration value function $V_\theta^E(s^k_T)$ (\cref{eq:lexa}), and average the estimates. 
%


For optimizing the goal variable $g$, we use model predictive path integral control \citep{williams2015model} (MPPI, see \cref{supp:implementation_details} for details). In \cref{supp:implementation_details}, we show that the specific choice of optimizer is not critical and other optimizers like cross entropy method (CEM) 
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\citep{de2005tutorial} also work well. More broadly, while these sampling-based optimizers are popular for optimizing action trajectories with dynamics models \citep{nagabandi2019deep, chua2018deep, ebert2018visual,hafner2020learning}, PEG uses them instead for optimizing goal commands; this lower-dimensional search space enables easy optimization, independent of the task horizon, permitting handling long-term tasks.
%
%
%
%
%
%
\begin{wrapfigure}[5]{r}{0.4\textwidth}
\vspace{-0.8cm}
\scalebox{0.8}{
\begin{minipage}{0.52\textwidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\Function{GoalDirectedExploration($\ldots$)}{}
\State $g \gets$ Optimize \cref{eq:page_objective_sample} with MPPI
\State $\tau \gets \textit{GO-EXPLORE}(g, \pi^G, \pi^E)$
    \\\Return $\tau$
\EndFunction
\end{algorithmic}
\caption{\method Goal Sampling}
\label{alg:page}
\end{algorithm}
\end{minipage}
}
\end{wrapfigure}
\jdtext{Equipped with this goal setting procedure, we can now train a goal-conditioned policy by replacing the LEXA goal sampling highlighted in red in \cref{alg:lexa} with our proposed PEG goals and Go-explore episodes. \cref{alg:page} shows PEG pseudocode.}
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\paragraph{Implementation Details.}
%
We implement \method on top of authors' code from DreamerV2~\citep{hafner2021mastering}, LEXA~\citep{mendonca2021discovering}, and MPPI implementation from \citet{rybkin2020model}. We used the default hyperparameters for training the world model, policies, value functions, and temporal reward functions. 
%
%
%
 For PEG, we tried various values of $K$ for simulating trajectories of $\pi^G$ for each goal and found $K=1$ to be sufficient.
%
%
We use the same Go-explore mechanism across all goal-setting methods: the Go and Explore phases' time limits  are set to half of the maximum episode length for all environments, while non-Go-explore baselines
%
 use the full episode length for exploration. See \cref{supp:implementation_details} for details on hyperparameters, runtime, and resource usage.

\section{Experiments}
\vspace{-0.2cm}

%
%
Our experiments evaluate goal-reaching tasks that require non-trivial exploration and skill acquisition to solve. We aim to answer: \textbf{(1)} Does \method  result in better exploration and downstream goal-reaching behavior? \textbf{(2)} What qualitatively distinguishes \method goals and exploration from previous goal-directed exploration methods? \textbf{(3)} What components of \method contribute to its success?


\subsection{Setup}
%
We evaluate \method and other goal-conditioned RL agents on four different continuous-control environments, described below. 
%
For each environment, we define an evaluation set of goals --- as a general principle, we pick evaluation goals in each environment that require extensive exploration in order for the agent to learn a successful evaluation goal reaching policy. \ehtext{These goals are very relevant in long-horizon tasks, as long-horizon goals tend to require extensive exploration to gather the necessary data for training.} Recall that we assume the fully unsupervised exploration setting; the agent does not have access to the evaluation set during training time. So, for all the agent knows at training time, any environment state could plausibly be a test goal. During evaluation, we run 10 goal-reaching episodes to account for environmental noise\footnote{Our environments have deterministic transitions with noise in the initial state distribution.} for each goal, and report the mean success rate and standard error for each environment. All methods are run with 10 seeds. The success metric $d(s,g)$ used for evaluation is an indicator function checking if any state in the episode is within $\epsilon$ distance of the goal state. See \cref{fig:environment_description} for images of each environment, and \cref{supp:environments} for more details.

%
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{figs/environments.png}
 \caption{ We evaluate \method across four diverse environments, selecting evaluation tasks to stress-test exploration. From left to right: Point Maze requires the agent to move from the bottom left to top right. Walker evaluates if the agent can walk to different positions on the plane. In Ant Maze, the ant is required to move to the farthest end of the maze. Finally, in 3-Block Stack, the robot must stack the blocks into the goal configuration (colored orbs).
 }
 \label{fig:environment_description}
\end{figure}


%
%
%
%
%
%
%
%
%

$\bullet$ \textbf{Point Maze:~~}
The agent, a 2D point, is noisily initialized in the bottom left. At test time, we evaluate it on reaching the right corner. The 2-D state and action space correspond to the planar position and velocity respectively. The episode length is 50 steps. The top right corner is the hardest to reach, so we set this to be the test-time goal. 
%
%

$\bullet$ \textbf{Walker:~~} 
In this environment from \cite{tassa2018deepmind}, a 2D 2-legged robot moves on a flat plane. The state space is 9-D and contains the walker's XZ position and joint angles. The action space is 6-D and represents joint torques. The episode length is 150 steps. To evaluate the ability to learn different locomotion behaviors of varied difficulty, the test goal commands are standing states that are placed $\pm7$, and $\pm12$ meters from the initial pose. 
%

$\bullet$ \textbf{Ant Maze:~~} 
We increased exploration difficulty in the Ant Maze environment from MEGA \citep{pitis2020maximum} by placing one additional turn in the maze. Further, we set the GCRL goal space to the full 29-D ant state space, rather than reduce it to 2-D $xy$ position as in MEGA.
The ant robot must learn complex 4-legged locomotion behavior and navigate around the hallways (see \cref{fig:environment_description}). The episode length is 500 steps. 
%
We evaluate the ant on reaching 4 hard goals that are placed in the top left room, the furthest area from the start, and 4 intermediate goals
%
%
in the middle hallway. 
%


$\bullet$ \textbf{3-Block Stacking:~~} Here, a robot arm with a two-fingered gripper operates on a tabletop with 3 blocks and boundary walls. The state space is the gripper pose and object $xyz$ positions (14-D), and the action space is the gripper velocity and force (4-D). At evaluation time, the robot is commanded to stack three blocks into a tower configuration. To achieve this, the agent will need to have learned pushing, picking, and stacking, and discovered 3-block stacks as a possible configuration of the environment. The episode length is 150 timesteps. 
%
%
Note that $3+$block stacking is a classic and open exploration challenge in robot RL; to our knowledge, all prior solutions 
%
assume expensive forms of supervision or computational resources to overcome the exploration challenge, such as demonstrations, curriculum learning, or billions of simulator samples \citep{ecoffet2021first, nair2018overcoming, lanier2019curiosity, li2020towards}, highlighting task difficulty.
%
%
%
%
%
\subsection{Baselines}
%
\label{sec:baselines}
%

%
PEG aims to improve goal-directed exploration by improving goal selection during GCRL. 
%
%
\jdtext{To specifically evaluate exploration, we implemented all baselines on top of LEXA \citep{mendonca2021discovering}, the backbone goal-conditioned MBRL framework by replacing the LEXA exploration logic (line 4 in \cref{alg:lexa}) with each method's proposed exploration strategy. See \cref{supp:baselines} for a detailed comparison between baselines.}
%

Our first two baselines, Skewfit and MEGA, are alternative goal setting approaches: both follow \cref{alg:lexa} and replace the \ehtext{LEXA exploration with their own goal sampling strategy for Go-Explore style exploration} (see \cref{supp:mega_code} and \cref{supp:skewfit_code} for pseudocode). %
\textbf{Skewfit} \citep{pong2019skew} estimates state densities, and chooses goals from the replay buffer inversely proportional to their density, resulting in uniform state sampling. \textbf{MEGA}~\citep{pitis2020maximum} similarly uses kernel density estimates (KDE) of state densities, and chooses low density goals from the replay buffer. 
%
MEGA is close to the original Go-explore approach \citep{ecoffet2021first}; rather than Go-explore's hand-designed pseudocount metric, MEGA uses KDE to choose goals. MEGA relies on a heuristic Q-function threshold to prune unachievable low density goals since many rare states may not be achievable by $\pi^G$. This requires tuning and prior knowledge of the test goal to set.

We also run vanilla \textbf{LEXA} itself as a baseline. LEXA does not perform Go-explore style exploration, instead using an exploration policy for some exploration episodes and the goal-conditioned policy conditioned on random goals for others. In addition, we evaluate against a variant of LEXA that only collects data with the exploration policy. We denote this baseline Plan2Explore (\textbf{P2E}) after the intrinsic motivation policy it uses \citep{sekar2020planning}. P2E trains an exploration policy to maximize disagreement among an ensemble of world models. 
%
Notably, PEG and our other Go-Explore baselines all use the same P2E exploration algorithm during Explore-phase, \ehtext{which isolates the differences in exploration to the proposed goals of each method.}  See \cref{supp:baselines} for additional details \ehtext{including a side-by-side algorithm comparison}, and where we verify that our MBRL versions of MEGA and Skewfit are sound and in fact better than the original \ehtext{model-free} versions in \cite{pitis2020maximum}. 
%
%
%



%

%
%
%

%



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/all_curves_new.png}
 \caption{Performance of agents with different goal setting strategies. All methods are run with 10 seeds. \method outperforms all baselines, and its performance gain increases with environment difficulty.
 }
    \label{fig:learning_curves}
\end{figure}

\subsection{Results}
%
\label{sec:results}
\paragraph{Test task success rates.} \cref{fig:learning_curves} shows the evaluation performance across training time. \method compares favorably to baselines both in final success rate and learning speed. \method reaches near-optimal behavior earlier than any other baseline. \method outperforms goal-setting baselines (MEGA and Skewfit) across all tasks, which shows that \method goal setting is superior to alternative goal-setting strategies. On the hardest task, block stacking, PEG is the only method to achieve any significant non-zero task performance: PEG achieves about 30\%  success rate on this challenging exploration task, all other baselines are close to 0\%. 
%
%

%
Recall that MEGA picks minimum density goals from the replay buffer. As we have seen in \cref{fig:teaser}, picking goals from the replay buffer may result in suboptimal goals for exploration as there could exist goals outside the data that result in more exploration. Furthermore, the MEGA objective does not reason about temporally extended exploration that begins after the goal state is reached. 
%
Skewfit, which samples goal states uniformly by their density, may often sample uninteresting goals, thus slowing down exploration.  In our upcoming qualitative analysis of goal selection behavior, we see these conceptual differences manifest in suboptimal goal picking in comparison to \method.
%

Next, P2E has no Go-phase, and thus no goal-directed exploration, which limits its exploration horizon. Even so, it can sometimes perform better than MEGA and Skewfit (see Walker), both methods that employ the same exploration algorithm as P2E, just after a Go-phase. In other words, Go-explore by itself doesn't always improve exploration, and 
%
%
%
suboptimal goal-setting during ``Go-phase" can in fact deteriorate exploration.  
LEXA performs worse than simple P2E, indicating that sampling random goals from the replay buffer does not improve exploration or training.

%
%
%

%

\begin{figure}[ht]
 \centering
   \includegraphics[width=0.8\textwidth]{figs/point_walker_ant_states.png}
 \caption{We plot the achieved states (blue / green dots) and goals (red dots) for \method and baselines in the Point Maze (top left), Walker (top right), and Ant Maze (bottom) halfway through training. \method picks goals in areas (orange boxes) that lead to more exploration whereas baselines do not. Walker and Ant states are high dimensional so we just plot their central $xz$ or $xy$ positions.
%
 }
 \label{fig:ant_maze_walker_states}
\end{figure}

%
\paragraph{Goal selection behaviors.} 
Next, \cref{fig:ant_maze_walker_states} visualizes the goal-picking and exploration behaviors for PEG and baselines. A trend across tasks is that \method  consistently picks goals (red points) beyond the frontier of seen data. In \textbf{Point Maze}, the middle right region forms a natural barrier for the exploration policy to overcome. The policy must find a rather tortuous path from the bottom left through the middle right hallways in order to reach the top right corner. As a result, top right states beyond the middle right region are very sparsely represented. With similar states in the replay buffer (blue dots), we find that \method generates many more goals in the top right region than MEGA, thus accelerating exploration. 
%
%
Similarly, in \textbf{Walker}, \method selects goals to the far left and right of the walker, where data support is the sparsest. MEGA fails to pick goals to the left.

%
\begin{figure}[ht]
 \centering
   \includegraphics[width=\textwidth]{figs/ant_goal_progress.pdf}
 \caption{The evolution of achieved states (colored dots) and goals (red dots) of \method throughout training in the Ant Maze. Lighter states are more recent.
 \method samples goals to maximize exploration, sometimes picking goals that are outside the seen data or even physically implausible.
 %
 %
 }
 \label{fig:goal_visualization}
\end{figure}
In \textbf{Ant Maze} (\cref{fig:ant_maze_walker_states}), we see that \method explores the most deeply into the maze of all methods, at this halfway point. Counting the total number of state visitations in the farthest top-left reaches of the maze for each method, we find that \method has $\sim$68K states whereas MEGA, the closest baseline, only has $\sim$3K. Looking at \cref{fig:ant_maze_walker_states}, MEGA does pick a few ``rare''  goals in the top left, but $\pi^G$ has not visited the top left area, either because $\pi^G$ is not capable of reaching the rare goals and/or the frontier. Skewfit samples no goals in the top left region, because it does not prioritize rare states. 
We further examine the training evolution of PEG goals in Ant Maze to analyze how they contribute to exploration.
%
We plot the achieved states and goals proposed by \method periodically throughout its training in \cref{fig:goal_visualization}. \method goals, optimized to induce the goal-conditioned policy to visit high-exploration value states, are not always predictable. \method commonly chooses goals outside of the data distribution. For example, \method chooses goals near the top wall for the entire training duration. We can also see it choosing physically impossible goals in the middle wall at 20$\%$, but these goals disappear as the goal-conditioned policy and world model are updated over time. 



%

%
%
%
%
%
%
%


%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


In the \textbf{3-Block Stack} environment, as discussed above, PEG is the only method to train any meaningful policy for 3-block stacking. The space of configurations of the three blocks and the robot in this setting is extremely large. \ehtext{While all methods are able to explore and discover simpler skills such as picking and 2-block stacking, only PEG's exploration  is comprehensive enough to also explore more ``useful'' and hard-to-reach behaviors such as 3-block stacking. See \cref{supp:block_more_goals} for success curves on intermediate goals like picking and 2-block stacking, and the website\footnote{\url{\projectwebsite}} for videos.}

%


%
%

 %
 
 %
 %

%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%

%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
\textbf{Ablation Analysis.} What components of \method are important to its performance? We run ablation studies in Ant Maze.
%
%
%
First, we ablate PEG's goal sampling: Rand-Goal PEG replaces PEG's goal sampling with randomly sampled goals. In Seen-Goal PEG, we forego optimizing the goal with MPPI and simply choose goals from the replay buffer that score highly according to \cref{eq:page_objective_sample}. Go-Value PEG chooses to optimize the exploration rewards achieved during the imagined \textit{Go-phase} instead of during the Explore-phase, by replacing $V^E(s_T)$ in \cref{eq:page_objective_sample} with $\sum_{t=1}^{T} r^E_t$ --- note that this still runs Explore after Go in the environment.
\begin{wrapfigure}[10]{r}{0.4\textwidth}
\vspace{-0.5cm}
\centering
   \includegraphics[width=0.4\textwidth] {figs/ablation_curves_new.png}
%
 \caption{PEG ablations.}
 \label{fig:ablation_curve}
\end{wrapfigure}
Next, we ablate PEG's Go-explore mechanism itself, by replacing the explorer policy with a random action policy (Rand-Explore PEG), and removing Explore-phase completely (No-Explore PEG). 


\cref{fig:ablation_curve} shows the results. PEG is clearly better than all ablations, suggesting that all components of the method contribute to performance. Explore-phase is particularly important;  No-Explore PEG achieves 0\% success. Rand-Explore PEG performs poorly, showing the need for a learned Explore-phase policy like P2E. Next, Rand-Goal PEG, Seen-Goal  PEG, and Go-Value PEG all perform worse than PEG; the specifics of the objective function for goal sampling defined in \cref{eq:page_objective_sample}, and its optimization procedure, are important. 
%

%
%
\section{Other Related Work}
%

%
%
%
%
%


%
%

\jdtext{\method is most closely related to the subfield of unsupervised exploration in RL, where no tasks are pre-specified during training. In this setting, a common approach is to define an intrinsic motivation reward correlated with exploration for RL \citep{schmidhuber2010formal,pathak2017curiosity}.} Common forms of intrinsic reward incentivize visiting rare states with count-based methods \citep{poupart2006an, bellemare2016count, burda2018exploration} or finding states that maximize prediction error \citep{oudeyer2007intrinsic, pathak2017curiosity, henaff2019explicit, shyam2019model, sekar2020planning}. Within PEG, we leverage an intrinsic motivation policy for the Explore-phase.

More specifically related to PEG, goal-directed exploration is a special class of unsupervised exploration, applicable in the challenging goal-conditioned RL (GCRL) setting. To develop generalist GCRL policies, these methods  
%
command the goal-conditioned policy towards exploratory goals at training time. Broadly, the idea is to pick goals that are difficult for the current policy, but not completely out of reach, i.e., within the ``zone of proximal development.''
%
Within that broad framework, however, prior works propose many different metrics for goal choosing, such as learning progress \citep{baranes2013active, veeriah2018many}, goal difficulty \citep{florensa18automatic}, ``sibling rivalry'' \citep{trott2019keeping}, or value function disagreement \citep{zhang2020automatic}. Within this family, we have already discussed and compared against the most closely relevant methods MEGA and Skew-Fit \citep{pong2019skew, pitis2020maximum} (see \cref{sec:baselines} and \cref{sec:results}) which command the agent to rarely seen states to increase the number of seen and familiar states, reasoning that this will enable broader expertise in the trained GCRL policies.


%
%
%


%
%

%

%
Closely related to unsupervised goal-directed exploration is unsupervised skill discovery, which aims to train a skill-conditioned policy \citep{eysenbach2018diversity,sharma2019dynamics}. 
%
Skill discovery objectives do not necessarily encourage exploration, and can even fall into local minima where the agent is penalized for exploration \citep{strouse2021learning, campos2020explore}. As such, these are usually weak baselines for exploration; for example, our goal-directed exploration baselines, LEXA and MEGA, both report clearly favorable results against such methods \citep{eysenbach2018diversity, hartikainen2020dynamical}.
\jdtext{See \cref{supp:extended_related_work} for an extended discussion of related work.}

%
%

%
%
%
 %

 
%
%

%
%

%

%
%
%
%
%
%
\section{Conclusions}
%
%
%
%
%
%
%

%
We have presented PEG, an approach that sets goals to achieve unsupervised exploration in goal-conditioned reinforcement learning agents. 
%
%
%
PEG plans these exploratory goals through a world model, directly optimizing for the estimated exploration value of the training trajectory.
%
%
%
\ehtext{While PEG performs better in our experiments than prior approaches across various environments, exploration for long-horizon tasks remains challenging, and PEG's explicit dependence on world model rollouts may be a particular weakness in such settings. Likewise, complex environments may permit extremely large goal spaces, where purely unsupervised exploration approaches like PEG are unlikely to succeed within reasonable time, and some information about the target task space may be required.} 
%
%
%
%
%
%
%
%
%
%
We believe that the PEG principle of optimizing for goals predicted to induce highly exploratory policy behaviors will prove useful towards future efforts to overcome these longstanding exploration challenges.

%
%
%

%
%

%
%

\clearpage
\section{Acknowledgements}
This work was supported in part by ONR grant number N00014-22-1-2677 and gift funding from NEC Laboratories. The authors would like to thank the anonymous reviewers for their constructive feedback, as well as Yecheng Jason Ma and the other members of the Perception, Action, and Learning (PAL) research group at UPenn for useful discussions and general support.

\section{Reproducibility Statement}
To ensure reproducibility, we released our codebase on the website: \url{\projectwebsite}. The website also contains a Google Colab tutorial and experiment visualizations. The supplementary contains details about hyperparameters, baselines, and architecture (see \cref{supp:baselines} and \cref{supp:implementation_details}). In addition, we have also provided pseudocode in \cref{alg:page}. 

\bibliography{bib/recognition, bib/drl,bib/detection,bib/deep_learning,bib/rl,bib/hrl,bib/env,bib/gnn,bib/sim2real,bib/robotics,bib/goal_directed_rl,bib/meta,bib/imitation, bib/keypoint, bib/mbrl, bib/contingency}
\bibliographystyle{iclr2023_conference}

\clearpage
%
\appendix
\addcontentsline{toc}{section}{Appendix} %
\part{Appendix} %
\parttoc %


%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\section{Extended Related Work}
\label{supp:extended_related_work}

%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

%
Exploration is a very broad field, and we encourage readers to read surveys such as \cite{yang2021exploration}. Below, we summarize various subfields of exploration and reinforcement learning relevant to PEG.


\paragraph{Task-directed exploration.} 
\method reasons about the intrinsic motivation / exploration rewards to be gained by commanding a goal-conditioned task policy to various goals in an unsupervised RL setting. Related at a high level, supervised exploration approaches reason about task reward gains during exploration.
\citep{chen2018ucb, osband2019deep} leverage uncertainty estimates about the reward to prioritize states for task-directed exploration.  \citep{stratonovich1966value} proposes "value of information" (VoI) to measure exploration from an information-theoretic perspective. The VoI describes the improvement in supervised task rewards of the current optimal action policy operating under a specified limit on state information. An RL agent may modulate exploration-vs-exploitation in supervised RL settings, by tuning this information limit: a higher limit leads to lower exploration and greater exploitation \citep{sledge2017balancing,cogliati2017learning}.

%


%

 \paragraph{Unsupervised Exploration.} In unsupervised exploration, no information about the task (e.g. reward function or demonstrations) is known during training time. In this setting, a common approach is to define an intrinsic motivation reward correlated with exploration for RL \citep{schmidhuber2010formal,pathak2017curiosity}. Common forms of intrinsic reward incentivize visiting rare states with count-based methods \cite{poupart2006an, bellemare2016count, burda2018exploration} or finding states that maximize prediction error \citep{oudeyer2007intrinsic, pathak2017curiosity, henaff2019explicit, shyam2019model, sekar2020planning}. Within PEG, we leverage an intrinsic motivation policy for the Explore-phase.


%
%


\paragraph{Goal-conditioned RL.}
 %
Goal-conditioned RL (GCRL) extends RL to the multitask setting where the policy is conditioned on a goal and is expected to achieve it. The goal space is commonly chosen to be the state space, although other options like desired returns or language inputs are possible. GCRL is challenging since the policy is required to learn the correct behavior for achieving each goal. This introduces two challenges - optimizing a goal-conditioned policy, and getting adequate data to support the optimization process. 

Many GCRL methods focus on improving goal-conditioned policy optimization. Hindsight Experience Replay \cite{andrychowicz2017hindsight} boosts training efficiency by relabeling failure trajectories as successful trajectories, and \cite{chane2021goal} uses imagined subgoals to guide the policy search process. However, such methods are only useful if the data distribution is diverse enough to cover the space of desired behaviors and goals, and will still suffer in hard exploration environments. Therefore, GCRL is still predominantly constrained by exploration.

\paragraph{Goal-directed Exploration.}
Goal-directed exploration, which sets exploratory goals for the goal-conditioned policy to achieve, is often the exploration mechanism of choice for GCRL as it naturally reuses the goal-conditioned policy. It is important to note that GDE is not inherently tied to GCRL, and can be used in conventional RL to solve hard exploration tasks \citep{ecoffet2021first, guo2020memory,hafner2022deep}. In such cases, goals are usually picked through some combination of reward and novelty terms.


Prior works propose many different metrics for goal choosing, such as frontier-based \citep{yamauchi1998frontier}, learning progress \citep{baranes2013active, veeriah2018many}, goal difficulty \citep{florensa18automatic}, ``sibling rivalry'' \citep{trott2019keeping}, or value function disagreement \citep{zhang2020automatic}. Within this family, we have already discussed and compared against the most closely relevant methods MEGA and Skew-Fit \citep{pong2019skew, pitis2020maximum} (see \cref{sec:baselines} and \cref{sec:results}) which command the agent to rarely seen states to increase the number of seen and familiar states, reasoning that this will enable broader expertise in the trained GCRL policies.

\paragraph{Go-Explore.} The Go-Explore framework from \citep{ecoffet2021first} performs goal-directed exploration by first rolling out the goal-conditioned policy (Go-phase) and then rolling out the exploration policy from the terminal state of the goal-conditioned phase (Explore-phase). Go-Explore does not prescribe a general goal setting method, instead opting for a hand-engineered novelty bonus for each task. GDE methods like MEGA \citep{pitis2020maximum} use the Go-Explore mechanism to further boost exploration efficiency, but do not consider Go-Explore when designing their goal-setting objective. This results in suboptimal performance - MEGA goals chooses low density goals, but this may not necessarily mean they are good states to start the exploration phase from (e.g. dead-ends in a maze). \method's objective already takes the Go-Explore mechanism into account, choosing goals that maximize the exploration of the Explore phase and resulting in superior exploration. 

%


%



%
%

%
%
%


%


%
%


%
%


%
%

%
%

%

%
%
%
%
%



\section{Extended Limitations and Future Work}
\label{supp:extended_limitations}
%
%
%
%
%
%
%


\paragraph{Model-based RL vs. Model-free RL.} \method is a model-based RL agent, which is more sample-efficient and computationally expensive than model-free counterparts. MBRL agents require more computation time and resources since they learn a world model. \method trains a world model to train policies and value functions with imagined trajectories, as well as generate goals for exploration. A model-free version of \method should be computationally and conceptually simpler, and we leave this for future work.


\paragraph{Better world models.} \method depends on a learned world model to plan goals, and may suffer in prediction performance and computation time for longer prediction horizons. An interesting direction would be to combine \method with temporally abstracted world models \citep{neitz2018adaptive,jayaraman2019time, pertsch2020long} to handle such cases. Furthermore, \method should benefit with improved world model architectures such as Transformers \citep{micheli2023transformers}, and thus improving world modelling offers a straightforward path for extending \method.

\paragraph{Exploration beyond state novelty. } Further, while empirically PEG produces performant goal-conditioned policies, it only explores by maximizing novelty. In harder tasks it might be necessary to explicitly consider also exploring with "practice goals" that best improve the goal-conditioned policy. 

Next, \cite{pislar2022when} suggests that generalizing the Go-Explore paradigm itself is beneficial. In Go-Explore, we assume the agent first performs a "Go-phase" for the first half of the episode, and then the "Explore-phase" for the remaining half. However, we can allow the agent to switch between the "Go-phase" and "Explore-phase" multiple times in an episode, and decide the phase lengths.

\paragraph{Scaling PEG to harder tasks.} While we focus on purely unsupervised exploration, in practice some knowledge about the task is always available. For example, a human may give some rewards, a few demonstrations, or some background knowledge about the task. Leveraging such priors to focus exploration on the space of tasks preferred by the user would make GCRL and PEG more practical, especially in real world applications like robotics.

\method currently is only evaluated in state space, and a potential extension would be to handle image observations by optimizing goals in a compact latent space. Such an extension should only require minor edits in the \method code because \method is extended from LEXA, an image-based RL agent that already learns such a latent space.


%
%

%
%



\section{Environments}
%
%
%
\label{supp:environments}
\subsection{Point Maze} The agent, a 2D point spawned on the bottom left of the maze, is evaluated on reaching the top right corner within 50 timesteps. The two-dimensional state and action space correspond to the planar position and velocity respectively. Success is based upon the L2 distance between the position of the agent and the goal being less than 0.15. The entire maze is approximately 10 x 10. This environment is taken directly from \cite{pitis2020maximum} with no modifications.

\subsection{Walker} Next, the walker environment tests the locomotion behavior of a 2D walker robot on a flat plane. The agent is evaluated on its ability to achieve four different standing poses that are placed $\pm 7$, and $\pm 12$ meters in the $x$ axis away from the initial pose. For success, we check if the agent's $x$ position is within a small threshold from the goal pose's $x$ position. The state and goal space is 9 dimensional, containing the walker's $xz$ positions and joint angles. The environment code was taken from \cite{mendonca2021discovering}.

\subsection{Ant Maze} A high-dimensional ant robot needs to move through hallways from the bottom left to top left (see Fig. \ref{fig:environment_description}). This environment is a long-horizon challenge due to the long episode length (500 timesteps) and traversal distance. We evaluate the ant on reaching 4 hard goals at the top left room, and 4 intermediate goals in the middle hallway. Ant Maze has the highest dimensional state and goal spaces: 29 dimensions that correspond to the ant's position, joint angles, and joint velocities. The first three dimensions of the state and goal space represent the $xyz$ position. The next 12 dimensions represent the joint angles for each of the ant's four limbs. The remaining 14 dimensions represent the velocities of the ant in the x-y direction and the velocities of each of the joints. Ant Maze also has an 8 dimensional action space. The 8 dimensions correspond to the hip and ankle actuators of each limb of the ant. Success is based upon the L2 distance between the $xy$ position of the ant and the goal being less than 1.0 meters, which is approximately the width of a cell. The dimensions of the entire maze is approximately 6 x 8 meters. This environment is a modification from the Ant Maze environment in \cite{pitis2020maximum}. We modified the environment so that the state and goal space now includes the ant's $xyz$ positions along with the joint positions and velocities instead of just the $xy$ positions. We also added an additional room in the top left in order to evaluate against a harder goal.

\subsection{3-Block Stack} A robot has to stack three blocks into a tower configuration. The state and goal space for this environment is 14 dimensional. The first five dimensions represent the gripper state and the other nine dimensions represent the $xyz$ positions of each block. The action space is 4 dimensional where the first three dimensions represent the $xyz$ movement of the gripper and the last dimension represents the movement of the gripper finger. Success is based upon the L2 distance between the $xyz$ position of all of object and the goal position of the corresponding object being less than 3cm. This environment is a modification from the FetchStack3 environment in \cite{pitis2020maximum}.

\section{Baselines}
\label{supp:baselines}

\begin{wrapfigure}[8]{r}{0.5\textwidth}
\vspace{-1.5cm}
\scalebox{0.8}{
\begin{minipage}{0.7\textwidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
 \State\textbf{Input:} goal / expl. policies $\pi^G$, $\pi^E$, world model $\widehat{\mathcal{T}}$, rewards $r^G, r^E$
    \State $\mathcal{D} \gets \{ \}$ Initialize buffer. 
    \For{Episode $i=1$ to $N_{\text{train}}$} 
        \State \color{red}$\tau \gets$  GoalDirectedExploration$( \ldots )$\color{black}
       \State $\mathcal{D} \gets D \cup \tau$
       \State Update model $\widehat{\mathcal{T}}$ with $(s_t, a_t, s_{t+1}) \sim D$
       \State Update $\pi^G$ in imagination with $\widehat{\mathcal{T}}$ to maximize $r^G$
       \State Update $\pi^E$ in imagination with $\widehat{\mathcal{T}}$ to maximize $r^E$
       \EndFor
\end{algorithmic}
\caption{MBRL Training}
\label{alg:mbrl_training}
\end{algorithm}
\end{minipage}
}
\end{wrapfigure}


Here, we first present pseudocode for all methods. We start by defining a generic MBRL training loop in \cref{alg:mbrl_training} that alternates between data collection (instantiated by specific methods) and policy and world model updates. Next, we define the pseudocode for PEG and all baselines below. To run a method, we simply drop in the data collection logic specified by the method into the MBRL training loop. For exploration, LEXA and P2E do not use Go-explore, while PEG, MEGA, and Skewfit do. 



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


%
%
%
%
%
%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\subsection{Go-Explore}
\label{supp:go_explore}
\begin{wrapfigure}[]{}{0.5\textwidth}
\vspace{-1cm}
\scalebox{0.8}{
\begin{minipage}{0.6\textwidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
%
%
%
%
%
%
%
%
%
%
%
%
%
\Function{GO-EXPLORE($g, \pi^G, \pi^E$)}{}
    \State $s_0 \gets$ env.reset()
    \State $\tau \gets \{s_0\}$
    \For{Step $t=1 \ldots T_{\text{Go}}$}
        \State $s_t \gets$  env.step($\pi^G(s_{t-1}, g)$)
        \State $\tau \gets \tau \cup \{s_t\}$
    \EndFor
    \For{Step $t=1 \ldots T_{\text{Explore}}$}
        \State $s_t \gets$  env.step($\pi^E(s_{t-1}, g)$)
        \State $\tau \gets \tau \cup \{s_t\}$
    \EndFor
    \\\Return $\tau$
\EndFunction
\end{algorithmic}
\caption{Go-explore}
\end{algorithm}
\end{minipage}
}
\end{wrapfigure}
Go-Explore \citep{ecoffet2021first} proposes to switch between a goal-directed policy and exploration policy in a single episode to tackle exploration-intensive environments. Go-explore first stores states of ``interest" as future goal states. Then, in a new episode, Go-explore chooses a goal $g$, and directs a goal-directed policy $\pi^G$ to achieve the goal (or upon timeout $T_\text{Go}$). Then, for the rest of the episode, an exploration policy $\pi^E$ is used. Intuitively, this results in frontier-reaching exploration \citep{yamauchi1998frontier} where the goal-directed policy efficiently traverses to the ``frontier'' of known states before doing exploratory behavior. Of particular importance to Go-Explore is how goal states are chosen - \cite{ecoffet2021first} does not prescribe a general approach for choosing goals. Instead, they choose goals by using task-specific psuedocount tables. 


\subsection{LEXA}
\begin{wrapfigure}[]{}{0.5\textwidth}
\vspace{-1cm}
\scalebox{0.8}{
\begin{minipage}{0.6\textwidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\Function{GoalDirectedExploration($\ldots$)}{}
    \If{$i \% 2 = 0$}
        \State Sample random goal $g$ from buffer.
        \State Collect trajectory $\tau$ with $\pi^G(\cdot \mid \cdot, g)$
    \Else
        \State Collect trajectory $\tau$ with $\pi^E$
    \EndIf
    \\\Return $\tau$
\EndFunction
\end{algorithmic}
\caption{LEXA Goal Sampling}
\end{algorithm}
\end{minipage}
}
\end{wrapfigure}
LEXA \citep{mendonca2021discovering} trains a goal-conditioned policy with model-based RL. It trains two policies, an exploration policy and a goal-conditioned ``achiever'' policy. The original LEXA codebase was designed for image-based environments, so we implemented a state-based LEXA agent by modifying DreamerV2 \citep{hafner2021mastering}. To do so, we added the $\pi^G$ and $\pi^E$ policies, goal-conditioning logic, and the temporal distance reward network.


\subsection{Original MEGA codebase} We used the original MEGA codebase to train a MEGA and a Skewfit agent \citep{pitis2020maximum,pong2019skew}  for 1 million environmental steps to solve the Pointmaze, Ant Maze, and 3-Block Stack tasks mentioned in Section 6. We used the original MEGA implementation with the same hyperparameters except for their goal-conditioned policy training scheme.

MEGA proposes Rfaab, a custom goal relabeling strategy to train the goal-conditioned policy. By default Rfaab relabels transitions with test time goals to train the goal-conditioned policy, which is not possible in our unsupervised exploration setup. 
Therefore, we changed the Rfaab hyperparameters to rfaab\_0\_4\_0\_5\_1, which no longer grants access to test goals.

We find that both MEGA and Skewfit agent fail to scale to the harder environments (Ant Maze and 3-Block Stack) as seen in \cref{fig:original_mega}. One reason could be that the MEGA codebase uses model-free RL, which is more sample inefficient. Therefore, we reimplemented MEGA and Skewfit with LEXA, the backbone MBRL agent that \method uses as well.

\begin{figure}[H]
\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
  & MEGA  & Skewfit\\  \hline
 Point Maze & 90\% & 0\% \\ \hline 
 Ant Maze & 0\% & 0\% \\ \hline
 3-Block Stack & 0\% & 0\% \\
 \hline
\end{tabular}
\end{center}
\caption{Success Rate of MEGA and Skewfit Agent}
\label{fig:original_mega}
\end{figure}

\subsection{Model-based MEGA}
\begin{wrapfigure}[]{}{0.5\textwidth}
%
\scalebox{0.8}{
\begin{minipage}{0.6\textwidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\Function{GoalDirectedExploration($\ldots$)}{}
\State $g \gets \min_{g \in \mathcal{D}} \widehat{p}(g)$
\State $\tau \gets \textit{GO-EXPLORE}(g, \pi^G, \pi^E)$
    \\\Return $\tau$
\EndFunction
\end{algorithmic}
\caption{MEGA Goal Sampling}
\label{supp:mega_code}
\end{algorithm}
\end{minipage}
}
\end{wrapfigure}
For model-based MEGA, we simply port over MEGA's KDE model. Conveniently, LEXA already trains a goal-conditioned value function, which we use for filtering out goals by reachability. We use the same KDE hyperparameters as the original MEGA paper. As seen in in \cref{fig:learning_curves}, our MEGA implementation gets non-trivial success rates, improving over the original MEGA baseline.

\subsection{Model-based Skewfit}


\begin{wrapfigure}[]{}{0.5\textwidth}
\vspace{-1cm}
\scalebox{0.8}{
\begin{minipage}{0.6\textwidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\Function{GoalDirectedExploration($\ldots$)}{}
\State $g \sim \left\{g \cdot \widehat{p}(g)^{-1} \mid g \in \mathcal{D} \right\}$
\State $\tau \gets \textit{GO-EXPLORE}(g, \pi^G, \pi^E)$
    \\\Return $\tau$
\EndFunction
\end{algorithmic}
\caption{Skewfit Goal Sampling}
\label{supp:skewfit_code}
\end{algorithm}
\end{minipage}
}
\end{wrapfigure}

For Skewfit, we port over the Skewfit baseline in MEGA's codebase. The implementation is straightforward and similar to model-based MEGA. Instead of picking the minimum density goal, Skewfit picks goals inversely proportional to their density. As seen in in \cref{fig:learning_curves}, our Skewfit implementation gets non-trivial success rates.

\subsection{Plan2Explore}


\begin{wrapfigure}[]{}{0.5\textwidth}
\vspace{-1cm}
\scalebox{0.8}{
\begin{minipage}{0.6\textwidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\Function{GoalDirectedExploration($\ldots$)}{}
\State Collect trajectory $\tau$ with $\pi^E$
    \\\Return $\tau$
\EndFunction
\end{algorithmic}
\caption{Plan2Explore}
\end{algorithm}
\end{minipage}
}
\end{wrapfigure}

Our Plan2Explore baseline uses a Plan2Explore exploration policy to gather data for the world model and trains a goal-conditioned policy in imagination. Our LEXA implementation already uses Plan2Explore, so this variant simply requires us to only rollout the exploration policy in the real world.

\section{Implementation Details}
\label{supp:implementation_details}
\subsection{MPPI}
MPPI is a popular optimizer for planning actions with dynamics models~\citep{williams2015model, nagabandi2019deep}. Instead of planning sequences of actions, which grows linearly with horizon, \method only needs to ``plan'' with a goal variable, making the search space independent of the horizon. First, we present a high-level summary of MPPI, and refer interested readers to \cite{nagabandi2019deep} for a more detailed look at MPPI. 

The process begins by randomly sampling $N$ goal candidates  $g$ from an initial distribution (see below for more details). These candidates are then evaluated as described in \cref{fig:peg-method} - we generate goal-conditioned trajectories for each goal $g_k$ through the world model, and compute the expected exploration value $V_k$ for each goal. This exploration value acts as the ``score'' for the goal candidate. Once we have scores for each goal candidate, a Gaussian distribution is fit according to the rule:
\begin{align}
    \mu_t &= \frac{ \sum_{k=0}^N (e^{\gamma \cdot V_k})(g_k)}{\sum_{k=0}^N (e^{\gamma \cdot V_k})}
\end{align} 
where $\gamma$ is the reward weight hyperparameter. We then sample candidates from the computed Gaussian, and repeat the process for multiple iterations. 


\paragraph{MPPI initial distribution} Throughout our paper, we initialize the MPPI optimizer with goal vectors sampled uniformly at random from the full goal space. One alternative would have been to restrict the initial candidates to lie in the agents replay buffer, but we found in practice that this restriction made no difference and it was more convenient for implementation reasons to simply sample uniformly at random from the unrestricted goal space (same as the state space), e.g. using the joint limits of the various robot joints. 

\begin{figure}[H]
\centering
\includegraphics[width=0.25\textwidth]{figs/statespace_curves.png}
\caption{Performance of state space sampling variants on the Ant Maze. Removing state space sampling (PEG-Restricted) does not degrade PEG performance, and adding state space sampling (MEGA-Unrestricted) does not boost performance. }
\label{fig:statespace_sampling}
\end{figure}

In \cref{fig:statespace_sampling},  we show results of running PEG with MPPI initialization from the replay buffer (``PEG-Restricted), in the Ant Maze environment. Its performance is virtually indistinguishable from the original PEG method. This is to be expected, and is a sign that the optimizer is working well.

Could other methods benefit from sampling outside the replay buffer?
In \cref{fig:statespace_sampling} we allowed our strongest baseline MEGA to also sample goals from outside the replay buffer (``MEGA-Unrestricted"). However, these methods rely on heuristic objectives for selecting goals (like low density of the visited state distribution) which only approximate the correct goal-setting objective for states close to the replay buffer. Therefore, MEGA-Unrestricted is unable to sample effective goals outside the replay buffer, and fares very poorly. In contrast, PEG directly optimizes the exploration value of goal states, and permits choosing meaningful goals even in this unrestricted setting. This is an advantage, since it permits optimizing over a larger space of goal candidates, which leads to better goals.



\subsection{Runtime}
\begin{figure}[H]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
              & Total Runtime (Hours) & Episodes & Episode Length & Seconds per Episode \\ \hline
Point Maze    & 16                    & 12000    & 50             & 4.8                 \\ \hline
Walker        & 16                    & 5000     & 150            & 11.52               \\ \hline
AntMaze       & 48                    & 2000     & 500            & 86.4                \\ \hline
3-Block Stack & 48                    & 6666     & 150            & 25.9                \\ \hline
\end{tabular}
\end{center}
\caption{Runtimes per experiment.}
\label{supp:runtime}
\end{figure}

The runtime and  resource usage between methods did not differ significantly, as everything is implemented on top of the backbone LEXA model-based RL agent. Note that model-free agents tend to run significantly faster than model-based agents, but fail to explore and optimize good policies. See \cref{supp:baselines} where we found model-free variants of MEGA and Skewfit failed to learn at all. Runtime is dominated by neural network updates of the policies and world model, not the goal selection routine defined by each method. Each seed was run on 1 GPU (Nvidia 2080ti or Nvidia 3090) and 4 CPUs, and required ~11GB of GPU memory. See \cref{supp:runtime} for training time. 

\begin{figure}[H]
\begin{center}
\begin{tabular}{|c|c|}
\hline
\textbf{}     & Seconds / Episode \\ \hline
PEG:          & 0.51              \\ \hline
MEGA:         & 0.48              \\ \hline
SkewFit:      & 0.46              \\ \hline
\end{tabular}
\end{center}
\caption{Runtime for goal selection.}
\label{supp:goal_runtime}
\end{figure}
Next, we benchmarked the goal selection procedure for PEG and the other goal selection baselines in the block stacking environment and recorded the average wall clock time in \cref{supp:goal_runtime}.

We can see that there is little difference in speed between methods, and the overhead introduced by PEG goal selection is minimal to LEXA, and competitive with other goal selection baselines. Because LEXA does not select goals, it finishes up to an hour earlier than goal setting methods in our experiments. As a sidenote, the times reported above are amortized over episodes, since we compute a set of 50 goals every 50 episodes for all methods, rather than computing 1 goal per episode. 

\subsection{Hyperparameters}
For parameter tuning, we used the default hyperparameters of the LEXA backbone MBRL agent (e.g. learning rate, MLP size, etc.) and kept them constant across all baselines. PEG only has 1 hyperparameter that requires tuning - the reward weight, which affects the MPPI update distribution. The higher the weight, the more we update the goal sampling distribution towards high-reward trajectories. For each experiment, we tried weight values of  (1, 2, 10) by running 1-2 seeds of PEG for each value. We used a weight of 1, 2, 2, 10 for the 4 experiments respectively
.
PEG uses MPPI, a sample-based optimizer, to optimize the objective. While MPPI has hyperparameters such as the number of samples and number of iteration rounds, these are easy to tune, since optimizer performance increases with the amount of samples and iterations. We therefore just choose as many samples (2000 candidates) and rounds (5 optimization rounds) as we can while keeping training time reasonable.

\section{Additional Experiments}

\subsection{\method with other exploration methods}
\begin{figure}[ht]
\centering
\includegraphics[width=0.25\textwidth]{figs/p2e_vs_rnd.png}
\caption{P2E vs. RND}
\label{fig:p2e_vs_rnd}
\end{figure}
While we chose to train our exploration policy and value function with Plan2Explore, \method can work with any exploration method as long as it provides an exploration reward and policy. Random Network Distillation (RND) is another powerful exploration method that rewards discovering new states. We run both variants with 5 seeds in the Ant environment \cref{fig:p2e_vs_rnd}. We find that the RND variant still explores the environment, albeit not as well as P2E. We hypothesize P2E exploration is particularly synergistic with our model-based RL framework, since the P2E explorer explicitly seeks transitions $(s,a,s')$ that improve the model. As the model accuracy increases, so does the quality of the goals generated by \method.


\subsection{MPPI vs CEM for Goal Optimization}

\begin{figure}[ht]
 \centering
  \includegraphics[width=0.3\textwidth]{figs/mppi_vs_cem.png}
\caption{MPPI vs. CEM}
\label{fig:mppi_vs_cem}
\end{figure}

We investigate the impact of the goal optimizer on \method. We choose two popular optimizers, MPPI and CEM, to optimize our objective, and run 5 seeds of each variant in the stacking environment. As seen in Fig.~\ref{fig:mppi_vs_cem}, the two agents perform similarly, showing that the \method objective is robust to the choice of optimizer.



\subsection{Does conditioning on \method goals maximize exploration?}
\begin{figure}[ht]
 \centering
 \includegraphics[width=0.5\textwidth]{figs/ant_goal_reaching.png}
 \caption{\method exploration in a U-maze.  Brown background dots: explored states, commanded goals: $\star$, resulting paths: colored lines. (Left) 
 \method optimizes directly for exploration, even setting unseen goals, and achieving farther paths. (Right) Setting goals at the frontier of the seen state distribution yields less exploration.
}
\label{fig:frozen_exp}
\end{figure}
We assess the importance of accounting for the goal-conditioned policy's capabilities. To do so, we evaluate if states coming from a policy conditioned on \method goals are more novel than states coming from the same policy conditioned on other goal strategies. In \cref{fig:frozen_exp}, we take a partially trained agent and its replay buffer, and plot the proposed goals and resulting trajectories for \method and MEGA (called Frontier in the figure). We find that \method goals lead the ant to the top left more often. This figure is also used in the intro for exposition.


\subsection{More goals for 3-Block Stacking}
\label{supp:block_more_goals}
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figs/intermediate_block_curves.png}
 \caption{Performance of agents evaluated on different types of goals in the 3-Block environment.All methods are run with 10 seeds. Easy goals require picking up one block, medium goals require stacking two blocks, and hard goals require stacking all 3 blocks.
 }
    \label{fig:intermediate_curves}
\end{figure}

We have defined three types of goals of increasing difficulty - picking up a single block (Easy), stacking two blocks (Medium), and stacking three blocks (Hard). For each goal type, we set specific evaluation goals by varying the location of the pick / stack as well as the choice and ordering of the blocks. In this way, we create 3 distinct Easy goals, 6 Medium goals, and 6 Hard goals.

First, PEG remains clearly the strongest at 3-block stacking (Hard) as seen in \cref{fig:intermediate_curves}. The difficulty-based task grouping permits more interesting analysis: PEG performs competitively with the best approaches on the Easy goals, and its gains over baselines are larger for Medium and Hard goals where exploration is most required. On these harder categories of tasks, PEG is both quickest to achieve non-trivial performance (indicating the onset of exploration) and also achieves the highest peak performance.

An interesting side note here is that better exploration tends to create greater challenges for multi-task goal-conditioned policy function approximation with a fixed capacity policy network. Around the 0.5M steps mark, when PEG starts to achieve Hard goals, it grows marginally weaker on Easy and Medium goals. This kind of forgetting is well-studied in the continual learning literature; addressing this well may require expanding policy network capacity over time, as in progressive networks.



%
%
%
%
%

%
%
%
%

%
%
%
%
%
%
%


%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

 

\end{document}
