% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)
\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version
% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[numbers,sort&compress]{natbib}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Continuous  emotion recognition based on TCN and Transformer}

\author{Weiwei Zhou, Jiada Lu, Zhaolong Xiong, Weifeng Wang,\\
Chinatelecom Cloud\\
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Human emotion recognition plays an important role in human-computer interaction. In this paper, we present our approach to the Valence-Arousal (VA) Estimation Challenge, Expression (Expr) Classification Challenge, and Action Unit (AU) Detection Challenge of the 5th Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW). Specifically, we propose a novel multi-modal fusion model that leverages Temporal Convolutional Networks (TCN) and Transformer to enhance the performance of continuous emotion recognition. Our model aims to effectively integrate visual and audio information for improved accuracy in recognizing emotions. The model is evaluate with Concordance Correlation Coefficient (CCC)
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}

Facial Expression Recognition (FER) can used in a variety of applications, such as emotion recognition in videos, facial recognition for security purposes, and even in virtual reality applications. Many facial related tasks have achieved high accuracy, such as face recognition, attribute recognition. Despite this, the capacity to comprehend the emotions of another person is still not adequate. The subtle distinctions between emotional expressions can lead to ambiguity or uncertainty in the perception of emotions, which makes it harder to assess the emotion of a person. Therefore, the scale of most of the FER datasets are not sufficient to build a robust model.

The appearance of AffWild  and AffWild2 dataset and the corresponding challenges \cite{kollias2019expression,kollias2022abaw,kollias2022abaw2,kollias2021analysing,kollias2020analysing,kollias2021distribution,kollias2021affect,kollias2019expression,kollias2019face,kollias2019deep,zafeiriou2017aff,2303.01498}  boost the development of affective recognition study. The Aff-Wild2 dataset contain about 600 videos with around 3M frames. The dataset is annotated with three different affect attribute: a) dimensional affect with valence and arousal; b) six basic categorical affect; c) action units of facial muscles. To facilitate the utilization of the Aff-Wild2 dataset, the ABAW5 2023 competition was organized for affective behavior analysis in the wild.

Multi-modal emotion recognition has been proven to be a more effective approach than single modality emotion recognition, as it can utilize the complementary information between modalities to capture a more complete emotional state, while being less susceptible to various noises. This improved recognition ability and generalization ability of the model can lead to more accurate and reliable results.


Considering the fact that visual and audio information contain much emotion information we propose to use multimodal features for continuous facial emotion recognition, and  design a network structure based on TCN and Transformer for feature fusion. Visual and audio features are first input into their respective TCN modules, then the features are concatenated and input into the Transformer encoder for learning, and finally an MLP is used for prediction. Our approach can unifies visual and audio features into a temporal model, designing an efficient emotion recognition network with Transformer, thereby improving the evaluation accuracy of Valence-Arousal Estimation, Action Unit Detection and Expression Classification.

The remain parts of the paper are presented as follows: Sec \ref{sec:method} describes our methodology; Sec \ref{sec:experiment} describes the experiment details and the result; Sec \ref{sec:conclusion} is the conclusion of the paper.

\section{Related Work}
\label{sec:RelatedWork}

Previous study have proposed some useful network on Aff-wild2 dataset. Kuhnke et al. \cite{kuhnke2020two} combine vision and audio information in the video and construct a two-stream network for emotion recognition and achieve high performance. Yue Jin et al. \cite{jin2021multi} propose a transformer-based model to merge audio and visual feature. Vu et al. \cite{vu2021multitask} built a multi-task  model
for valence-arousal estimation and facial expressions prediction. The authors applied the distillation knowledge architecture for training and prediction, because the dataset does not include labels for all the two
tasks.


\section{Methodology}
\label{sec:method}

In this section, we present our approach for the three challenges in the 5th ABAW Competition: Valence-Arousal Estimation, EXPR Classification and AU Detection.


%-------------------------------------------------------------------------
\subsection{Preprocessing}

We obtain the audio from the video and convert it to mono with a sample rate of 16,000 Hz. Some of the frames do not contain valid faces, so we replace these frames with the nearest frame that had a valid face.


%-------------------------------------------------------------------------
\subsection{Audio Features}

We extract the audio features using two models: wav2vec2 \cite{baevski2020wav2vec} and wav2vec2-emotion \cite{pepino2021emotion}.

wav2vec2 is a pre-trained and fine-tuned model that has been trained on 960 hours of LibriSpeech audio, sampled at a rate of 16kHz. The feature vector dimension is 512.

wav2vec2-emotion is a fine-tuned model based on Wav2Vec2-Large-Robust, using 284 instances of MSP-Podcast data. The feature vector dimension is also 512.

To align with frames, we resize the features to match the length of each frame.


%-------------------------------------------------------------------------
\subsection{Visual Features}

We extract four visual feature vectors using different models.

The first feature vector is extracted using arcface \cite{deng2019arcface} from insightface, which has been pre-trained on the Glint360K dataset \cite{an2022pfc} for face recognition. The vector dimension is 512.

The second feature vector is extracted using EfficientNet-b2 \cite{tan2021efficientnetv2,Savchenko_2022_CVPRW}, which has been pre-trained on the VGGFace2 dataset \cite{parkhi2015deep} for face identification and fine-tuned on the AffectNet8 dataset. The vector dimension is 1280.

The third and fourth feature vectors are extracted using a model from DAN \cite{wen2021distract}, pre-trained on MSCeleb and fine-tuned on RAF-DB and AffectNet8. The vector dimension is 512.




\subsection{Split Videos}

Videos are first split into segments with a window size $w$ and stride $s$. Given the segment window $w$ and stride $s$, a video with $n$ frames would be split into $[n/s] + 1$ segments, where the $i$-th segment contains frames$\left\{F_{(i-1) *s+1}, \ldots, F_{(i-1) * s+w}\right\}$.

\begin{figure*}[t]
  \centering
   \includegraphics[width=1\linewidth]{framework.pdf}

   \caption{The architecture of our proposed model. The model consists of four components: pre-trained feature extractors for audio and visual features, TCN with three temporal blocks, Transformer encoder and MLP for final prediction.}
   \label{fig:twocol}
\end{figure*}


%-------------------------------------------------------------------------
\subsection{Modeling}

We denote audio features as $f_i^a$ and visual features as  $f_i^v$ corresponding to the $i$-th segment.

\subsubsection{Temporal Convolutional Network}

Each feature is fed into a dedicated Temporal Convolutional Network (TCN) for temporal encoding, which can be formulated as follows:
$$
g_i^v=\text { TCN }\left(f_i^v\right) 
$$
$$
g_i^a=\text { TCN }\left(f_i^a\right)
$$

where $g_i^v$ denotes visual features,  $g_i^a$ denotes audio features.
Then, visual features and audio features are concatenated, denotes as $g_i^c$.

$$
g_i^c=[g_i^v,g_i^a]
$$


\subsubsection{Temporal Encoder}


We utilize a transformer encoder to model the temporal information in the video segment as well, which can be formulated as follows:
$$
h_i=\text { TransformerEncoder }\left(g_i^c\right).
$$
The Transformer encoder only models the context within a single segment, thereby ignoring the dependencies between frames across segments. To account for the context of different frames, overlapping between consecutive segments can be employed, thus enabling the capture of the dependencies between frames across segments, which means $s \leq w$.

\subsubsection{Prediction}

After the temporal encoder, the features $h_i$ are finally fed into MLP for regression, which can be formulated as follows:
$$
y_i= \text{MLP} (h_i)
$$
where  $y_i $ are the predictions  of $i$-th segment.
For VA challenge, $y_i \in \mathbb{R}^{l \times 2}$. For EXPR challenge, $y_i \in \mathbb{R}^{l \times 8}$. For AU challenge, $y_i \in \mathbb{R}^{l \times 12}$ .


%-------------------------------------------------------------------------
\subsection{Loss Functions}
VA challenge: We use the Concordance Correlation Coefficient (CCC) between the predictions and the ground truth labels as the loss function.


EXPR challenge: We use cross entropy loss  as the loss function.


AU challenge: We employ BCEWithLogitsLoss as the loss function, which integrates a sigmoid layer and binary cross-entropy.


\section{Experiments and Results}
\label{sec:experiment}

%-------------------------------------------------------------------------
\subsection{Experiments Settings}
All models are trained on a Nvidia GeForce GTX 3090 GPU which has 24GB memory. We use AdamW optimizer and cosine learning rate schedule with first epoch warmup. The learning rate is 0.00003, the weight decay is 0.00001, the dropout prob is 0.3, and the batch size is 32.



For VA Challenge, we use wav2vec, wav2vec-emotion, effcientnet-b2, DAN-rafdb, DAN-affectnet8 as the input features. 


For EXPR Challege and AU Challenge, we only use efficientnet-b2 as the input feature.


For all three challenges, we split videos using a segment window $w=300$ and a 
 stride $s=200$.

\begin{table*}
  \centering
  \begin{tabular}{@{}lccccc@{}}
    \toprule
    Experiment & Feature & Valence  & Arousal  & F1-score  \\
    \midrule
    VA & efficientnet-b2,affectnet8, rafdb wav2vec2, wav2vec2-emotion & 0.5505 & 0.6809 & -  \\
    EXPR & efficientnet-b2 & - & - & 0.3767  \\
    AU & efficientnet-b2 & - & - & 0.5174  \\
    
    \bottomrule
   \end{tabular}
   \caption{Performance of our method on the validation dataset of three experiments}
   \label{tab:result}
\end{table*}
%-------------------------------------------------------------------------
\subsection{Overall Results}
The Table \ref{tab:result} displays the experimental results of our proposed method on the validation set of the VA, EXPR and AU Challenge, where the Concordance Correlation Coefficient (CCC) is utilized as the evaluation metric for both valence and arousal prediction, and F1-score is used to evaluate the result of EXPR and AU challenge. As demonstrated in the table, our proposed method outperforms the baseline significantly.



\subsection{Ablation Study}

In this section, we perform several ablation study on these three experiments to compare the contribution on different features. From Table \ref{tab:va_validation}, it can be seen that the almost each feature contribute to the VA prediction task, and the combination of 3 visual features: efficientnet-b2, affectnet8, rafdb and 2 audio features: wav2vec2, wav2vec2-emotion reach the highest CCC score on VA experiment. The wav2vec2-emotion and efficientnet-b2 feature  makes the most contributions on Valence and Arousal score, respectively. From Table \ref{tab:expr_validation} and \ref{tab:au_validation}, they show that the use of efficientnet-b2 only could reach the highest F1-score on EXPR and AU experiment. The cross-validation result of VA, EXPR and AU experiment are reported in Table \ref{tab:va_fold}, \ref{tab:expr_fold} and \ref{tab:au_fold}. Fold 0 is exactly the original data from ABAW dataset.

\begin{table*}
  \centering
  \begin{tabular}{@{}llcc@{}}
    \toprule
    Visual Features & Audio Features & Valence & Arousal \\
    \midrule
    efficientnet-b2,affectnet8, rafdb & wav2vec2, wav2vec2-emotion & 0.5505 & 0.6809 \\
    efficientnet-b2 & wav2vec2, wav2vec2-emotion & 0.5413 & \textbf{0.6819} \\
    efficientnet-b2,affectnet8, rafdb & wav2vec2-emotion & \textbf{0.5514} & 0.6798 \\
    arcface, rafdb,arcface & wav2vec2, wav2vec2-emotion & 0.5340 & 0.6589 \\
    arcface, rafdb & wav2vec2  & 0.5329 & 0.6511 \\
    affectnet8 & wav2vec2, wav2vec2-emotion & 0.5387 & 0.6648 \\
    affectnet8 & wav2vec2-emotion & 0.5392 & 0.6629 \\
    rafdb & wav2vec2 & 0.5309 & 0.6579 \\

    \bottomrule
  \end{tabular}
  \caption{Ablation study of features on the validation dataset of VA experiment.}
  \label{tab:va_validation}
\end{table*}

\begin{table*}
  \centering
  \begin{tabular}{@{}lccccc@{}}
    \toprule
    Emotion & Fold 0 & Fold 1 & Fold 2 & Fold 3 & Fold 4 \\
    \midrule
    Valence & 0.5401 & 0.5505 & \textbf{0.5518} & 0.5501 & 0.5489 \\
    Arousal & 0.6788 & 0.6809 & 0.6766 & 0.6699 & \textbf{0.6811} \\
    
    \bottomrule
   \end{tabular}
   \caption{Results for the five folds of the VA experiment}
   \label{tab:va_fold}
\end{table*}


\begin{table*}
  \centering
  \begin{tabular}{@{}llcc@{}}
    \toprule
    Visual Features & Audio Features & F1-score  \\
    \midrule
    efficientnet-b2 & None & \textbf{0.3767}  \\
    efficientnet-b2, affectnet8 & wav2vec2-emotion & 0.3722  \\
    efficientnet-b2,arcface & wav2vec2-emotion & 0.3761  \\
    arcface & wav2vec2-emotion & 0.3690  \\
    efficientnet-b2 & wav2vec2, wav2vec2-emotion & 0.3719  \\
    arcface,affectnet8 & None & 0.3744  \\
    efficientnet-b2,rafdb & None & 0.3710  \\
    efficientnet-b2,rafdb & wav2vec2-emotion & 0.3748  \\

    \bottomrule
  \end{tabular}
  \caption{Ablation study of features on the validation dataset of EXPR experiment.}
  \label{tab:expr_validation}
\end{table*}

\begin{table*}
  \centering
  \begin{tabular}{@{}lccccc@{}}
    \toprule
    Emotion & Fold 0 & Fold 1 & Fold 2 & Fold 3 & Fold 4 \\
    \midrule
    F1-score & 0.3643 & 0.3723 & \textbf{0.3767} & 0.3744 & 0.3692 \\
    
    \bottomrule
   \end{tabular}
   \caption{Results for the five folds of the EXPR experiment}
   \label{tab:expr_fold}
\end{table*}



\begin{table*}
  \centering
  \begin{tabular}{@{}llcc@{}}
    \toprule
    Visual Features & Audio Features & F1-score \\
    \midrule
    efficientnet-b2 & None & \textbf{0.5174}  \\
    efficientnet-b2, affectnet8 & wav2vec2-emotion & 0.5083  \\
    efficientnet-b2,arcface & wav2vec2-emotion & 0.5125  \\
    arcface & wav2vec2-emotion & 0.5091  \\
    efficientnet-b2 & wav2vec2, wav2vec2-emotion & 0.5145  \\
    arcface,affectnet8 & None & 0.5094  \\
    efficientnet-b2,rafdb & None & 0.5144  \\
    efficientnet-b2,rafdb & wav2vec2-emotion & 0.5154  \\

    \bottomrule
  \end{tabular}
  \caption{Ablation study of features on the validation dataset of AU experiment.}
  \label{tab:au_validation}
\end{table*}

\begin{table*}
  \centering
  \begin{tabular}{@{}lccccc@{}}
    \toprule
    Emotion & Fold 0 & Fold 1 & Fold 2 & Fold 3 & Fold 4 \\
    \midrule
    F1-score & 0.4988 & 0.5140 & 0.5155 & \textbf{0.5174} & 0.5028 \\
    
    \bottomrule
   \end{tabular}
   \caption{Results for the five folds of the AU experiment}
   \label{tab:au_fold}
\end{table*}


%-------------------------------------------------------------------------
\section{Conclusion}
\label{sec:conclusion}

We purpose a Temporal Convolutional Networks (TCN) and Transformer based model to effectively integrate visual and audio information for improved accuracy in recognizing emotions. The experiment was conducted on the Aff-Wild2 dataset, and the results show that our method significantly outperforms the baseline.



\clearpage
%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
