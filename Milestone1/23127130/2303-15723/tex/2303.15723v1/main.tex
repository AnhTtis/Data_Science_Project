\documentclass[12pt]{article}
\linespread{1.5}
%\usepackage{sectsty}
\usepackage[T1]{fontenc}
 %\usepackage{stix}
 %\usepackage{txfonts}
 %\usepackage{newtxtext,newtxmath}
 %%
%\usepackage[onehalfspacing]{setspace}
 %\usepackage{libertine} \usepackage[libertine]{newtxmath}
 %\usepackage{kurier}
\usepackage{kpfonts}
 %\usepackage{libertine,libertinust1math}
%\documentclass[12pt]{imsart}
\usepackage[dvipsnames]{xcolor}
\usepackage{pdfpages}
%\sectionfont{\color{Maroon}}
%\subsectionfont{\color{Maroon}}
%\usepackage{colortbl}
%\usepackage{upgreek}
\usepackage{sgame}
\usepackage{accents}
\usepackage{tikz-cd}
\usepackage{float}
\usepackage[round]{natbib}   % omit 'round' option if you prefer square brackets
\usepackage{multirow}
\usepackage{dutchcal}
\usepackage{pdfpages}
\usepackage{bbm}
\usepackage{sgame}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage[margin=1.5in]{geometry}
\usepackage{caption}
\usepackage{subcaption}
%\usepackage{authblk}
\usepackage{epigraph}
\usepackage{listings}
\usetikzlibrary{calc}
\usetikzlibrary{shapes,arrows}
%\usepackage{amsmath}
\renewcommand{\qedsymbol}{$\blacksquare$}
\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}
\DeclareMathOperator\supp{supp}
\DeclareMathOperator\inter{int}
\DeclareMathOperator\hyp{hyp}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}

\theoremstyle{definition}

\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{question}[theorem]{Question}

%\theoremstyle{remark}

\newtheorem{remark}[theorem]{Remark}
\newtheorem{remarks}[theorem]{Remarks}
\newtheorem{aside}[theorem]{Aside}
\newtheorem{note}[theorem]{Note}
%\everymath{\color{Blue}}

%\everydisplay{\color{Blue}}

\setlength{\footnotesep}{0.5cm}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=CadetBlue,
    filecolor=CadetBlue,      
    urlcolor=CadetBlue,
    citecolor=CadetBlue,
}

\definecolor{backcolour}{rgb}{0.63, 0.79, 0.95}

%Code listing style named "mystyle"
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}

\lstset{style=mystyle}

\newcommand*\interior[1]{\mathring{#1}}
\providecommand{\keywords}[1]{\textbf{\textit{Keywords:}} #1}
\providecommand{\jel}[1]{\textbf{\textit{JEL Classifications:}} #1}
\bibliographystyle{plainnat}

\makeatletter
\makeatother

\DeclareMathOperator{\sgn}{sgn}
\DeclareRobustCommand{\hsout}[1]{\texorpdfstring{\sout{#1}}{#1}}
\begin{document}
\author{Mark Whitmeyer \and Kun Zhang\thanks{Arizona State University. Emails: \href{mailto:mark.whitmeyer@gmail.com}{mark.whitmeyer@gmail.com} \&  \href{mailto:kunzhang@asu.edu}{kunzhang@asu.edu}. \newline
Comments by Ashwin Kambhampati, Andreas Kleiner, Joseph Whitmeyer, and Renkun Yang helped us improve this paper immensely.}}

\title{Redeeming Falsifiability?}
\date{\today}

\maketitle

\begin{abstract}
We revisit Popper's falsifiability criterion. A tester hires a potential expert to produce a theory, offering payments contingent on the observed performance of the theory. We argue that if the informed expert can acquire additional information, falsifiability \textit{does} have the power to identify worthless theories.
\end{abstract}


\newpage

\section{Falsifiability}

The criterion of falsifiability–that a scientific idea is one that can be falsified, i.e., conclusively rejected by the data–is central to science. Worryingly, \cite{falsifiability} illustrate a flaw with this notion. 

They study a model in which a principal (Alice) hires an expert (Bob) to deliver a falsifiable theory, which Alice then checks against a sequence of data points. In order to provide incentives to Bob, Alice uses the carrot (paying Bob a lump sum if he delivers a falsifiable theory) and the stick (fining Bob if his theory is falsified). Even though i) Alice has an unbounded dataset with respect to which she may evaluate the theory, ii) the fine levied on Bob can be unboundedly large, and iii) Bob evaluates his future prospects pessimistically (by the minimum expected utility given any future realization of the data); \citeauthor{falsifiability} show that Alice cannot identify an informed Bob from one who is uninformed.

The purpose of this paper is to reexamine this result by allowing the informed Bob to acquire additional information. That is, we endogenize Bob's knowledge of the data-generating process. We show that as long as the informed Bob can acquire information, the falsifiability criterion \textit{does} ``separate the wheat from the chaff:'' it distinguishes between the informed and the uninformed Bob.%, even if Alice only has \emph{one} data point.

\subsection{Related Literature}

There is a large literature studying how clueless agents can evade detection by empirical tests. This collection of papers includes the seminal ``calibration'' result of \cite{asscal} (see also \cite{dawid1982well}, \cite{dawid1985comment}, and \cite{foster1999proof}), which was followed by \cite{fudenberg1999easier}, \cite{sandroni2003calibration} and \cite{hart2022calibrated}. Other papers studying the use of tests to catch masquerading non-experts--\cite{lehrer2001any}, \cite{dekel2006non}, \cite{shmaya2008many}, 
\cite{olszewski2007contracts},\cite{olszewski2008manipulability}, \cite{olszewski2009manipulability}, and \cite{hu2013expressible} (to name a few)--followed over the next few decades.\footnote{\cite{olssurvey} provides a helpful survey.}

%Elicitation literature also

More recently, a sizeable collection of papers exploring rational inattention and endogenous flexible information acquisition has emerged. A subclass of these are those papers that study the contracting problem of paying an agent to acquire information. \cite{rappoport2017incentivizing} study the problem of inducing an agent to acquire hard evidence; and \cite{yoder2022designing} studies the impact of private information in this setting, introducing a screening element. \cite{sharma} asks how to impel a risk-neutral agent to acquire and report honestly soft information in a two-state environment;\footnote{\cite{muller} formulate an ``ignorance equivalent,'' which they apply to (among other things) incentivizing an agent to acquire and report information.} and \cite{whitmeyer2022buying} study the general problem of ``buying opinions,'' allowing for an arbitrary number of states and risk aversion. In both \cite{zermeno2011} and \cite{clark2021contracts} both learning and decision making are delegated to an agent. 

This paper connects the expert-testing and information-acquisition literatures, allowing the competent expert to acquire information in a classical setting of designing tests to fail charlatans.

%\textcolor{Fuchsia}{(MW 314) I took out the part about the other paper, as it will be some time before it is out.}

 




\section{The Main Result}\label{model}

%We suppose that there is just one data point available to the principal (Alice), as a longer sequence of data points would make it even easier to identify the uninformed expert (Bob). Our model follows the motivating example of \cite{falsifiability} with one modification: an informed Bob may now acquire additional information.

There is an unknown state of the world \(\theta \in \Theta\), where \(\left|\Theta\right| = n < \infty\). Alice offers a contract to a self-proclaimed expert, Bob, that consists of a lump-sum payment (in utils), \(u > 0\), and a penalty, \(d > 0\). Henceforth, we refer to this variety of contract as simply a \textbf{contract}. If Bob accepts the contract he obtains the payment, \(u\), up front; then announces a state that will not realize. The state subsequently realizes and is publicly observed by Alice and Bob; and if the announced state coincides with the realized state, Alice levies the penalty, \(d\), on Bob.

The expert, Bob, is either informed or uninformed. If Bob is informed, he has a prior \(\mu \in \Delta \coloneqq \Delta\left(\Theta\right)\). After seeing the contract, the informed Bob decides whether to acquire additional information at a cost. We can allow him to learn either before or after deciding whether to accept the contract: our main result holds under both specifications. He learns by observing the outcome of a statistical \textbf{experiment}, which is a pair \(E = \left(S, \chi\right)\), where \(S\) is a set of possible signals and \(\chi \colon \Theta \to \Delta(S)\) is a stochastic map from the set of states to the set of signals. The cost of acquiring information is given by a cost functional \(C \colon \mathcal{E} \times \Delta \to \mathbb{R}_{+} \cup \left\{\infty\right\}\), where \(C\left(E, \mu\right)\) is the cost of acquiring experiment \(E\) when Bob's prior is \(\mu\).

Let
\[\Upsilon(E, \mu) \coloneqq \min\left\{\min_{i} \mu_i, 1 - \mu_i\right\} - \int_{S} \min\left\{\min_{i} \left\{\mu_i P_i(s)\right\}, 
\left(1 - \mu_i\right) P_n(s)\right\} \, \mathrm{d}s \text{,}\]
where \(\mu_i\) is the \(i\)-th entry of \(\mu\) (\(i = 1, \dots, n-1\)), and \(P_i = \chi(\theta_i)\). \(d \cdot \Upsilon(E, \mu)\) is the benefit from learning according to experiment \(E\) for informed Bob with prior \(\mu\). For fixed \(\delta \geq 0\), say that an experiment \(E\) is \(\delta\)-\textbf{valuable} for prior \(\mu\) if \(\Upsilon(E, \mu) > \delta\). We make the following joint assumption on \(C\) and \(\mathcal{E}\): 
\begin{enumerate}[label={(\Alph*)}, itemsep=1pt, topsep=1pt]
 \item \label{assumption} There exist \(\varepsilon, \eta > 0\) such that for all \(\mu \in B\left(\mathbf{\frac{1}{n}}, \eta\right)\), there exists \(E_\mu \in \mathcal{E}\) that is \(\varepsilon\)-valuable for \(\mu\), and \(C\left(E_\mu, \mu\right) \le T\) for some \(T \in \mathbb{R}_{+}\).\footnote{\(\mathbf{\frac{1}{n}}\) denotes the vector that has \(\frac{1}{n}\) for each of its entries, and \(B\left(\mathbf{\frac{1}{n}}, \eta\right)\) is the ball centered at \(\mathbf{\frac{1}{n}}\) with radius \(\eta\). Note that \(\varepsilon\) and \(T\) are \emph{uniform} to all \(\mu \in B\left(\mathbf{\frac{1}{n}}, \eta\right)\).}
\end{enumerate}
A stronger condition than Assumption \ref{assumption} is that there is a non-trivial (informative) experiment that informed Bob, regardless of his prior, has access to at a finite cost.\footnote{As a simple example, suppose there are two states, \(H\) and \(L\). By incurring a cost \(c > 0\), informed Bob can get a signal \(s \in \{h,l\}\) such that \(\mathbb{P}(h | H) = \mathbb{P}(l | L) = q\), where \(1/2 < q \le 1\).} Another example that satisfies Assumption \ref{assumption} is that informed Bob has access to all experiments, and his cost of acquiring information is posterior separable (\cite{caplin2022rationally}).\footnote{This class of information costs includes the entropy-based cost function (see e.g. \cite{sims1998stickiness, sims2003implications}, and \cite{matvejka2015rational}); the log-likelihood cost of \cite{costofinfo}; the neighborhood-based cost function studied by \cite{hebert2021neighborhood}; and the quadratic (posterior variance) cost function. Some variants of this class of costs can also be allowed; see, for example, Example 4 and 6 in \cite{choice}.} His cost could also be experimental (\cite{denti2022experimental}).

If Bob is uninformed he cannot acquire any information. Moreover, the uninformed Bob evaluates payoffs by his minimum expected utility in all possible states. By rejecting the contract, both varieties of Bob get payoff \(0\).\footnote{The perspicacious reader may note that this is not a perfect analog of \citeauthor{falsifiability}'s framework. We modified things slightly for the sake of presentation. \(\S\)\ref{alternate} contains a model that is an exact analog of theirs. There, we argue that our insights persist.}

We say a contract \textbf{screens} the uninformed Bob if the informed Bob prefers to accept it but the uninformed Bob strictly prefers to reject. Our main theorem is simple: 
\begin{theorem}\label{main}
There exists a contract that screens the uninformed Bob.
\end{theorem}
%The proof of Theorem \ref{main} is relegated to Appendix \ref{proof}. 
\begin{proof}
By Assumption \ref{assumption}, there exist \(\varepsilon, \eta > 0\) such that for all \(\mu \in B\left(\mathbf{\frac{1}{n}}, \eta\right)\) we can find \(E_\mu \in \mathcal{E}\) such that \(E_\mu\) is \(\varepsilon\)-valuable for \(\mu\) with \(C\left(E_\mu, \mu\right) \le T\). If \(E_\mu\) is \(\varepsilon\)-valuable for \(\mu\), by choosing \(d > T/\varepsilon\), \(d \cdot \Upsilon\left(E_\mu, \mu\right)\) is strictly larger than \(T\) for all \(\mu \in B\left(\mathbf{\frac{1}{n}}, \eta\right)\). Therefore, \(u\) can be chosen so that \(u - \frac{d}{n} < 0\) but every informed Bob's payoff, net of possible learning costs, is positive. Hence, he accepts the contract. Since the uninformed Bob evaluates payoffs by his minimum expected utility, there is no way of randomizing over announcements that secures him a payoff greater than \(u - \frac{d}{n}\). As this is strictly less than \(0\), he refuses the contract.\end{proof}

The remainder of the paper proceeds as follows. In \(\S\)\ref{os11}, we explain how our model maps to the motivating example in \cite{falsifiability} and why Theorem \ref{main} fails to hold there. In \(\S\)\ref{example} we illustrate, via two examples how can we overcome the difficulty by allowing the informed agent to acquire further information. \(\S\)\ref{alternate} argues that our simplification of \cite{falsifiability} is innocuous; and \(\S\)\ref{eubob} studies a variant in which the uninformed Bob is also an expected-utility maximizer, but incapable of learning.

\subsection{Olszewski and Sandroni's Example, Reframed} \label{os11}

Our model (if we took away the informed Bob's ability to acquire information) maps to \citeauthor{falsifiability}'s example as follows. There, the informed Bob knows the composition of an urn that contains balls of \(n\) possible colors. The uninformed Bob does not. Bob announces a falsifiable theory: he must claim that at least one color is impossible. Alice then draws a ball, and if it is the ``impossible'' color, Bob gets fined \(d\). 

In our framework, the realized state, \(\theta\), corresponds to the color of the ball drawn by Alice. The informed Bob, therefore, has a prior \(\mu \in \Delta\), according to which \(\theta\) realizes. Bob's announcement is of a state (or states) he claims will not realize. %This is precisely our setup with the lone addition that our informed Bob may learn.

Without learning, the following result summarizes \cite{falsifiability}'s example:
\begin{proposition}
If a contract is such that an informed Bob accepts it, no matter his prior, an uninformed Bob will also accept it.
\end{proposition}
\begin{proof}
    Let \(\mathbf{\mu} \coloneqq \left(\mu_1, \dots, \mu_{n-1}\right)\) denote an arbitrary prior in \(\Delta\). Observe that an informed Bob accepts a contract if \(u - d \min_{i} \mu_i \geq 0\). As \(\min_i \mu_i\) is maximized when \(\mu_i = \frac{1}{n}\) for all \(i\), an informed Bob accepts a contract, no matter his prior, if and only if \(u \geq \frac{d}{n}\). However, as observed by \cite{falsifiability}, by randomizing uniformly over announcements, the uninformed Bob guarantees himself a payoff of \(u - \frac{d}{n}\), which is weakly greater than \(0\) by construction. Therefore, the uninformed Bob also accepts the contract.
\end{proof}
In fact, even more general contracts (beyond the proposed one corresponding to ``falsifiability'') cannot weed out the ignorant Bob. That is, there is no contract--i.e., a pair \(\left(M, t\right)\), where \(M\) is a compact set of messages and \(t \colon M \times \Theta \to \mathbb{R}\) is continuous--such that an informed Bob accepts it (no matter his prior) but an uninformed Bob does not. 

For simplicity, suppose there are two states. Observe that a contract induces a convex value function on the \(1\)-simplex, \(V\left(x\right)\), for the informed Bob. That he accepts it, no matter his prior, requires that \(V\left(x\right) \geq 0\) for all \(x\). But then it is easy to see that the uninformed Bob will also accept the contract: if \(V\left(0\right) = 0\) or \(V\left(1\right) = 0\), the uninformed Bob will send a message that is optimal for the informed Bob at prior \(0\) or \(1\), respectively. If \(V\left(0\right), V\left(1\right) > 0\) and \(V\left(\tilde{x}\right) = 0\) for some \(\tilde{x} \in \left(0,1\right)\) for which \(V'\left(\tilde{x}\right)\) exists, the uninformed Bob will send the message optimal for the informed Bob at prior \(\tilde{x}\), which necessarily has a state-independent payoff of \(0\). Finally, if \(V\left(0\right), V\left(1\right) > 0\) and \(V\left(\tilde{x}\right) = 0\) for some \(\tilde{x} \in \left(0,1\right)\) but \(V'\left(x\right) \neq 0\) for all \(x \in \left[0,1\right]\) (and so, necessarily \(V\) is kinked at \(\tilde{x}\)), the uninformed Bob will mix between the two messages optimal for the informed Bob with prior \(\tilde{x}\), and nature will choose the worst-case probability \(\tilde{x}\) for the uninformed Bob, yielding him a payoff of \(0\).


\subsubsection*{Some Intuition}

The basic intuition behind this subsection's finding--and unless we are are confused, \cite{falsifiability}--is that in order to elicit information from an informed expert, no matter his prior, the value function induced by the contract cannot dip below the horizontal axis. That is, it must lie everywhere above \(0\). However, by randomizing judiciously (though randomization may not be necessary), the uninformed Bob can always secure a payoff no less than the minimum of the value function. He is not screened out.

By allowing the informed Bob to learn, we make it so that although the value function does dip below the horizontal axis, the value function evaluated at any posterior that may result from learning is positive. The uninformed Bob is screened out. %\textcolor{SeaGreen}{(MW 315) I took out the ``therefore'' for purely stylistic reasons--I think it reads better for it to mimic the last sentence of the preceding \(\P\).}

\subsection{Two Examples} \label{example}
\subsubsection*{Two State, Single Experiment Example}
Suppose there are two states, \(H\) and \(L\); and let \(\mu \coloneqq \mathbb{P}(H)\) denote the informed Bob's prior. Regardless of what prior he has, informed Bob has access to only one experiment: he can get a signal \(s \in \{h,l\}\) such that \(\mathbb{P}(h | H) = \mathbb{P}(l | L) = 3/4\), and the cost of performing the experiment is \(c = 50\). %It can be checked, by calculating \(\Upsilon(E, \mu)\), that this experiment is at least \(1/8\)-valuable for all \(\mu \in (5/12, 7/12)\). 
Letting \((u,d) = (250, 600)\), if informed Bob has \(\mu \in (1/3, 2/3)\), he acquires information, as his payoff from doing so is
\[u - d \cdot \left[\min\left\{\mu \, \mathbb{P}(h | H), (1-\mu) \, \mathbb{P}(h | L)\right\} + \min\left\{\mu \, \mathbb{P}(l | H), (1-\mu) \, \mathbb{P}(l | L)\right\}\right] - c = 50\text{.}\]
If he has \(\mu \notin (1/3, 2/3)\) his payoff is greater than or equal to \(u - d/3 = 50 > 0\). But the uninformed Bob's expected payoff is \(u - d/2 = -50 < 0\). Thus, the contract \((u,d)\) screens the uninformed Bob.


\subsubsection*{Two State, Posterior-Separable Cost Example}
Suppose there are just two states, the informed Bob has access to any experiment, and his cost of obtaining information is monotone in the Blackwell order. For convenience, we write his cost of obtaining information as a cost defined on his resulting distribution over posteriors \(F\): the cost of acquiring the experiment that produces \(F\) is \(\Gamma\left(F\right) = \kappa \int_{0}^{1}c\left(x\right)dF\left(x\right) - \kappa c\left(\mu\right)\) for some strictly convex function \(c\colon \left[0,1\right] \to \mathbb{R} \cup \{\infty\}\), \(\kappa \in \left(0,K\right]\), and \(K \in \mathbb{R}_{++}\).\footnote{It is important to note that we are using the fact that \(C\) depends not only on the experiment \(\chi\), but the prior, \(\mu\), as well or else this would be impossible, as pointed out by \cite{denti2022experimental} and \cite{denti2022random} (see also \cite{mensch2018cardinal}).}

Observe that a contract induces value function \(V\left(x\right) = u - d \min\left\{x, 1-x\right\} - \kappa c\left(x\right) + \kappa c\left(\mu\right)\), which is kinked precisely at \(x = \frac{1}{2}\). Accordingly, for all \(\kappa\) and \(c\), and for all \(\mu\)s such that the informed Bob acquires information, the informed Bob's optimal learning will not have support in some interval \(\left(x_L, x_H\right)\), where \(x_L < \mu < x_H\). Moreover, if \(u - \frac{1}{2}d = 0\), \(\mathbb{E}_{F^{*}_{\mu}} V > 0\) for all \(\mu \in \left[0,1\right]\), where \(F^{*}_{\mu}\) is an optimally acquired distribution.\footnote{Note that \(F^{*}_{\mu}\) may be the degenerate distribution on \(\mu\).} Accordingly, there is some \(\varepsilon > 0\) such that \(u - \varepsilon - \frac{1}{2}d < 0\) but \(\max_{F_\mu}\mathbb{E}_{F_{\mu}}V\left(x\right) - \varepsilon > 0\) for all \(\mu \in \left[0,1\right]\).

The optimal randomization strategy for the uninformed Bob, should he accept the contract, is to announce each state with equal probability, yielding him a payoff of \(u - \varepsilon - \frac{1}{2}d < 0\). Thus, only the informed Bob will deliver a theory.

Figure \ref{fig1} illustrates a collection of value functions induced by a contract that screens the uninformed Bob, when the informed Bob's cost function is the (expected) reduction in (Shannon) entropy. The dotted red curve is the value function for an informed Bob with prior \(\mu = \frac{1}{2}\). The grey curves are value functions for various other \(\mu\)s. They are pointwise increasing as \(\mu\) gets further from \(\frac{1}{2}\). The dotted orange lines are the concavifying lines corresponding to Bob's optimal learning (for the different priors). The Bob with the highest depicted curve (\(\mu = .53\) or \(.47\)) does not acquire any information. Indeed, for our main result to hold, we only need the informed Bob with prior sufficiently close to the kink \(\frac{1}{2}\) to ``move away from there'' by acquiring information. 

\begin{figure}
    \centering
    \includegraphics[scale=.16]{fig1slim.png}
    \caption{The value functions (gross of costs) induced by a contract that screens the uninformed Bob. \href{https://www.desmos.com/calculator/guw5gmdc6j}{Try it yourself!}}
    \label{fig1}
\end{figure}

\subsection{An Alternate Specification}\label{alternate}

Our formal model is not as true to \cite{falsifiability} as it could have been, but was chosen instead to cleanly illustrate our point. Let us now quickly explore a variant that parallels their example more closely in order to assure ourselves that our insights are correct.

Suppose now that there are three states \(\theta \in \left\{rr, rb, bb\right\}\), which captures a situation in which an urn contains two balls, each of which takes one of two possible colors, red (\(r\)) or black (\(b\)). The timing of the scenario remains the same but now the contract conditions not on the true state but on Alice's evidence about it, which is the realization of a uniform draw from the urn. Let \(x \coloneqq \mathbb{P}\left(rr\right)\) and \(y \coloneqq \mathbb{P}\left(bb\right)\), denote Bob's beliefs about the state. For simplicity, we assume Bob's cost on experiments produces the uniformly posterior-separable form (as in the previous subsection)
\[\Gamma\left(F\right) = \kappa \int_{\Delta}c\left(x,y\right)dF\left(x,y\right) - \kappa c\left(\mu_x, \mu_y\right)\text{.}\]

Bob has two possible announcements (you will not draw) \(red\) and \(black\). Now, a contract induces value function
\(V\left(x,y\right) = u - \frac{d}{2} \min\left\{1+x-y, 1+y-x\right\} - \kappa c\left(x,y\right) + \kappa c\left(\mu_x, \mu_y\right)\). This is kinked at the line \(y = x\). Thus, no matter the (finite) \(\kappa\) or \(c\), the informed Bob's optimal learning will have no support within some open superset of \(\left\{\left(x,y\right) \in \Delta \colon y = x\right\}\). Moreover, if \(u - \frac{1}{2}d = 0\), \(\mathbb{E}_{F^{*}_{\mu}} V > 0\) for all \(\mu \in \Delta\), where \(F^{*}_{\mu}\) is an optimally acquired distribution. 

Accordingly, there is some \(\varepsilon > 0\) such that \(u - \varepsilon - \frac{1}{2}d < 0\) but \[\max_{F_\mu}\mathbb{E}_{F_{\mu}} V - \varepsilon > 0\] for all \(\mu \in \Delta\). The uninformed Bob, should he accept the contract, again optimally announces each state with equal probability, yielding him a payoff of \(u - \varepsilon - \frac{1}{2}d < 0\). Thus, only the informed Bob will deliver a theory.


\subsection{When Uninformed Bob is an SEU-Maximizer}\label{eubob}

\cite{falsifiability} aptly model a scientific theory as knowledge of the state of the world (see our \(\S\)\ref{alternate}), which we simplify to knowledge of the distribution of the state of the world (our main specification). Alas, they show that falsifiability is unable to distinguish correct theories from incorrect ones. We modified their model so that the informed expert is not perfectly informed about the state, but instead can acquire information. %\textcolor{OrangeRed}{(KZ 316) I think it's correct, but calling the informed expert ``not perfectly informed about the state'' can be a bit tricky, since in OS the state is also yet-to-realize, it's just that the informed experts knows the \emph{true odds} of the states, say how fair a coin is.}\textcolor{SeaGreen}{(MW 316) I disagree slightly. Especially in the large-quantity-of-evidence main portion of OS, I think the correct interpretation is that the expert does know the state, it is the evidence that is imperfect (which is why \(S\)\ref{alternate} is the more precise analog).} \textcolor{OrangeRed}{(KZ 316) OK this now makes a lot of sense--if we stick on the formulation of our \(S\) \ref{alternate}, then ``perfectly informed'' totally make sense since the composition of the urns (or ``fairness'' of the coin I previously mentioned) is exactly the state.}

This seems reasonable to us, especially if one thinks of science as an incremental process, where hypotheses are refined over time, converging to consensus about the state only in the long-run limit. This raises the question; however, as to how we should think about the uninformed Bob. Perhaps he should be an expected-utility maximizer, himself, with a well-defined subjective prior. Given this, the value of the falsifiability paradigm is how it potentially enables the production of scientific theories only by the Bob who can learn, ensuring progress and eventual knowledge. %\textcolor{OrangeRed}{(KZ 316) This paragraph is very nice, I really like it. I'm just thinking whether we probably should, based on our conversation today, take a weaker stand on ``consensus about the state'' and ``eventual knowledge''. The second one seems less a problem then the first, but anyhow I'm fine leaving it as is.} I agree with your comment. Given that this is an intro (for this section at least), I think it is ok.

Formally, we can adapt Theorem \ref{main} to the case where the uninformed Bob is a subjective expected-utility maximizer with a known prior \(\rho \in \inter \Delta\). Define a \textbf{generalized contract} to be \(n+1\) scalars \(u > 0\) and \(d_i > 0\) (\(i = 1, \dots, n\)). As before, if Bob accepts the contract he obtains the payment, \(u\), up front; then announces a state that will not realize. If the announced state \(\theta_i\) coincides with the realized state, Alice levies the penalty, \(d_i\), on Bob.

We adapt our earlier joint assumption on \(C\) and \(\mathcal{E}\) by replacing \(\mathbf{\frac{1}{n}}\) with \(\rho\):
\begin{enumerate}[label={(A')}, itemsep=1pt, topsep=1pt]
 \item \label{assumption2} There exist \(\varepsilon, \eta > 0\) such that for all \(\mu \in B\left(\mathbf{\rho}, \eta\right)\), there exists \(E_\mu \in \mathcal{E}\) that is \(\varepsilon\)-valuable for \(\mu\), and \(C\left(E_\mu, \mu\right) \le T\) for some \(T \in \mathbb{R}_{+}\).
\end{enumerate}
Then,
\begin{proposition}\label{main2}
There exists a generalized contract that screens the uninformed Bob.
\end{proposition}
\begin{proof}
    It suffices to show that there is a solution to 
    \[-\rho_1 d_1 = - \rho_2 d_2 = \cdots = -\left(1-\rho_1 - \dots - \rho_{n-1}\right)d_n\text{,}\]
    with one degree of freedom. The rest follows the proof of Theorem \ref{main}, \textit{mutatis mutandis}. Such a solution is
    \[d_{i} = \frac{1-\rho_1 - \dots - \rho_{n-1}}{\rho_i}d_n \ \ \text{for all} \ i = 1, \dots, n-1 \text{,}\]
    where \(d_n\) is a free variable. The \(d_i\)s are well-defined because \(\rho \in \inter{\Delta}\). \end{proof}
If \(d_i\) must equal \(d\) for all \(i\)--i.e., generalized contracts are forbidden--unless \(\rho = \mathbf{\frac{1}{n}}\), it may not be possible to screen the uninformed Bob. %Of course, in some sense, this variant of the problem is one in which our uninformed Bob is ``informed,'' himself; it is just that he cannot acquire further information. To put differently, this contract allows Alice to distinguish between a Bob who can learn and one who cannot. \textcolor{Maroon}{Consequently, this contract can be interpreted as a test that selects an expert who can acquire information, which may represent, for example, the ability of conducting scientific research.}

%{\color{OrangeRed}(KZ 315) I'm not sure if this is the right formulation if we were going to explore the case of a SEU-maximizing uninformed Bob. To me, there are two kinds of ``uninformedness'': one is that the informed Bob knows the prior but he does not, two is that they share the same priors but only the informed Bob can acquire information. Both \citeauthor{falsifiability} and our main setting explore the first case; I agree that it's useful to think about the second, although it's a bit unclear about what the principal wants to elicit in this setting.

%In particular, what's above allows the uninformed Bob to have a \emph{different} prior basically means that, to me, he is also somehow \emph{informed}.}
%\textcolor{SeaGreen}{(MW 315) I understand this. This particular formulation was a suggestion of Andy's. I explained this paper to him today. He said it would be a nice robustness exercise, and I figured that it was easy to do and we have the space. I added in some discussion inspired by your comment.} \textcolor{OrangeRed}{(KZ 315) \emph{Argumentum ad verecundiam}! To be serious, I like the robustness check argument; worst case, the referees ask us to remove this part, which doesn't really matter. There is one thing; however, that I still don't understand very well: what does the principal really want to elicit here? In the extreme case that \(\rho = \mu\), one can argue that she wants something ``more precise'' than the informed Bob's prior, but the informed Bob may only want to acquire \emph{a bit more} information, if any, under the contract. In this case, does ``distinguishing the informed and uninformed agents'' really matter? Now suppose \(\rho \ne \mu\), and let's also suppose that the prior that the informed agent holds is indeed more ``correct'' than the one of uninformed Bob. But if \(\rho\) is ``very close'' to \(\mu\) in some sense, again, does distinguishing really matter, or really worth it?}

\iffalse
\textcolor{SeaGreen}{(MW 315) Vaguely, my answer is that here (in this subsection) we can reinterpret science as an incremental process. Going from the prior (status quo belief) to an improved theory (posterior).} 
\textcolor{OrangeRed}{(KZ 315) I really like your interpretation, and everything above looks great. Still, I'm really sorry for being extremely stubborn and slow, but I'm still trying to understand: suppose \(\mu\) is ``far enough'' from \(\rho\), then the generalized contract need not induce him to acquire any information. Then, what is the point of ``distinguish between a Bob who can learn and one who cannot''?}
\textcolor{SeaGreen}{(MW 315) In the short run, nothing. In the long run, it matters. We should probably have a paragraph that captures this discussion of ours right here in the text, no?} \textcolor{OrangeRed}{(KZ 315) Exactly--the only way I had in mind that rationalizes this exercise is to interpret it as a ``test'' for selecting an expert who can acquire information for future endeavors. I added a sentence at the end of the preceding paragraph; please add and/or change.}
\textcolor{SeaGreen}{(MW 316) Ok, I took out your part and mine, and instead added a couple intro paragraphs.}
\fi
\subsubsection{Unknown Prior for Uninformed Bob}

If the uninformed Bob's prior is also unknown to Alice, there is no contract that is such that informed Bob accepts the contract and uninformed Bob does not, no matter their priors.\footnote{This statement is true even if we understand ``contract'' to mean an arbitrary finite menu of state-contingent transfers.} This is because for an SEU-maximizing uninformed Bob, rejecting the contract at all priors is equivalent to his outside option payoff \(0\) being strictly dominant. Thus, it will be for the informed Bob as well.

Nevertheless, a different sort of result is true. We say that informed Bob has a \textbf{Rich Learning Set} if for all \(\mu \in \inter \Delta\), and all \(E \in \mathbcal{E}\) that produce distributions over posteriors with support on \(\inter \Delta\), \(C\left(E, \mu\right) < \infty\).\footnote{This assumption is satisfied when the informed Bob's choice of experiments is unrestricted, and the cost is posterior-separable  or experimental (under the standard assumptions for such costs).} A contract \(\xi\)-\textbf{screens} the uninformed Bob if the informed Bob prefers to accept it, but the set of priors at which uninformed Bob strictly prefers to reject has Lebesgue measure of at least \(1-\xi\).
\begin{remark}
For all \(\xi > 0\), if informed Bob has a rich learning set, there is a contract that \(\xi\)-screens the uninformed Bob.
\end{remark}
\begin{proof}
    For any \(\xi > 0\), \(u\) and \(d\) can be chosen such that the induced value function, \(V\left(\mathbf{x}\right)\), i) is strictly negative on a subset of \(\Delta\) with Lebesgue measure \(1-\xi\); and ii) is such that \(V\left(\mathbf{x}\right)\) is arbitrarily large for all \(\mathbf{x} \in \partial \Delta\). We are done: at most measure \(\xi\) of the uninformed Bobs will accept, but all of the informed ones will.\end{proof}
Note that such a contract induces the informed Bob to acquire arbitrarily precise information.

\iffalse
\section{Conclusion}

To the best of our knowledge, we are the first to note that the ability of a competent expert to \textit{acquire} additional information when faced with a test can aid a principal in weeding out incompetent experts. We encountered this in our re-examination of a formal model of falsification. We found that with endogenous information, this paradigm may be of use in preventing fraud, even though it is not when information is exogenous.\fi

\bibliography{sample.bib}

\end{document}