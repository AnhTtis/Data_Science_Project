\documentclass[12pt]{article}
\linespread{1.5}
\usepackage[T1]{fontenc}
\usepackage{libertine,libertinust1math}
\usepackage[dvipsnames]{xcolor}
\usepackage{pdfpages}
\usepackage{sgame}
\usepackage{accents}
\usepackage{tikz-cd}
\usepackage{float}
\usepackage[round]{natbib}   % omit 'round' option if you prefer square brackets
\usepackage{multirow}
\usepackage{dutchcal}
\usepackage{pdfpages}
\usepackage{bbm}
\usepackage{sgame}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage[margin=1.25in]{geometry}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epigraph}
\usepackage{listings}
\usetikzlibrary{calc}
\usetikzlibrary{shapes,arrows}
\usepackage{nicefrac}
\renewcommand{\qedsymbol}{\(\blacksquare\)}
\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}
\DeclareMathOperator\supp{supp}
\DeclareMathOperator\inter{int}
\DeclareMathOperator\hyp{hyp}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}

\theoremstyle{definition}

\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{assumption}{Assumption}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{question}[theorem]{Question}


\newtheorem{remark}[theorem]{Remark}
\newtheorem{remarks}[theorem]{Remarks}
\newtheorem{aside}[theorem]{Aside}
\newtheorem{note}[theorem]{Note}
\renewcommand{\theassumption}{\Alph{assumption}}
\newtheorem*{altassumption}{Assumption A'}


\setlength{\footnotesep}{0.5cm}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=Maroon,
    filecolor=Periwinkle,      
    urlcolor=Periwinkle,
    citecolor=Periwinkle,
}
\usepackage[noabbrev,capitalise,nameinlink]{cleveref}
\crefname{assumption}{Assumption}{Assumptions}

\definecolor{backcolour}{rgb}{0.63, 0.79, 0.95}

%Code listing style named "mystyle"
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}

\lstset{style=mystyle}

\newcommand*\interior[1]{\mathring{#1}}
\providecommand{\keywords}[1]{\textbf{\textit{Keywords:}} #1}
\providecommand{\jel}[1]{\textbf{\textit{JEL Classifications:}} #1}
\bibliographystyle{plainnat}

\makeatletter
\makeatother

\DeclareMathOperator{\sgn}{sgn}
\DeclareRobustCommand{\hsout}[1]{\texorpdfstring{\sout{#1}}{#1}}
\begin{document}
\author{Mark Whitmeyer\thanks{Arizona State University. Email: \href{mailto:mark.whitmeyer@gmail.com}{mark.whitmeyer@gmail.com}.} \and Kun Zhang\thanks{University of Queensland. Email: \href{mailto:kun@kunzhang.org}{kun@kunzhang.org}. \newline
Comments by Ashwin Kambhampati, Andreas Kleiner, Joseph Whitmeyer, and Renkun Yang helped us improve this paper immensely. We are also grateful to the audience at the PKU-NUS Annual Conference on Quantitative Finance and Economics for feedback.}}

\title{Redeeming Falsifiability?}
\date{\today}

\maketitle

\begin{abstract}
We revisit Popper's falsifiability criterion. A tester hires a potential expert to produce a theory, offering payments contingent on the observed performance of the theory. In our model, instead of knowing the true data-generating process, the expert knows the state-of-the-art belief over data-generating processes. A non-expert does not. We argue that if the expert can, moreover, acquire additional information to refine this knowledge, falsifiability \emph{does} have the power to distinguish between experts and non-experts and to identify valuable theories, capitalizing on experts’ ability to acquire and refine knowledge.
\end{abstract}


\newpage


\section{Falsifiability}

The criterion of falsifiability–that a scientific idea is one that can be falsified, i.e., conclusively rejected by the data–is central to science. Worryingly, \cite{falsifiability} illustrate a flaw with this notion. 

They study a model in which a principal (Alice) hires an expert (Bob) to deliver a falsifiable theory, which Alice then checks against a sequence of data points. In order to provide incentives to Bob, Alice uses the carrot (paying Bob a lump sum if he delivers a falsifiable theory) and the stick (fining Bob if his theory is falsified). Even though i) Alice has an unbounded dataset with respect to which she may evaluate the theory, ii) the fine levied on Bob can be unboundedly large, and iii) Bob evaluates his future prospects pessimistically (by the minimum expected utility given any future realization of the data); \citeauthor{falsifiability} show that Alice cannot identify an informed Bob from one who is uninformed. In other words, the falsifiability criterion does not have the power to identify worthless theories.

In this note, we take a different perspective on the falsifiable criterion and illustrate a role for theory--abstract reasoning about models. In doing so, we reveal a particular sense in which the falsifiable notion carries weight. As in \cite{falsifiability}, in our model, our agent Bob is either an expert or a charlatan. Crucially, however, neither variety knows the true data-generating process, but our expert Bob, instead, has a state-of-the-art belief (probability distribution) over data-generating processes. That is, the true data-generating process is not known, but Bob, if he is an expert, holds a belief about the likelihoods of each. 

Expert Bob is a theorist, and can acquire information about the data-generating process at a (possibly negligible) cost. We think this is natural: theorists \textit{reason} about the world and in doing so \textit{learn} about the world--i.e., refine their models. In short, this restores the usefulness of the falsifiable criterion. If expert Bob's knowledge of the data-generating process is endogenous, the falsifiability criterion \textit{does} ``separate the wheat from the chaff:'' it distinguishes between the expert and the charlatan, thereby identifying worthwhile theories, even though Alice has only \textit{one} data point with which to do so.


\subsection{Related Literature}

There is a large literature studying how clueless agents can evade detection by empirical tests. This collection of papers includes the seminal ``calibration'' result of \cite{asscal} \citep[see also][]{dawid1982well,dawid1985comment,foster1999proof}, which was followed by \cite{fudenberg1999easier}, \cite{sandroni2003calibration} and \cite{hart2022calibrated}. Other papers studying the use of tests to catch masquerading non-experts---\cite{lehrer2001any}, \cite{dekel2006non}, \cite{shmaya2008many}, 
\cite{olszewski2007contracts}, \cite{olszewski2008manipulability}, \cite{olszewski2009manipulability}, and \cite{hu2013expressible} (to name a few)---followed over the next few decades.\footnote{\cite{olssurvey} provides a helpful survey.}


More recently, a sizable collection of papers exploring rational inattention and endogenous flexible information acquisition has emerged. A subclass of these are those papers that study the contracting problem of paying an agent to acquire information. \cite{rappoport2017incentivizing} study the problem of inducing an agent to acquire hard evidence; and \cite{yoder2022designing} studies the impact of private information in this setting, introducing a screening element. \cite{whitmeyer2023buying} and \cite{sharma2024procuring} ask how to impel an agent to (costly) acquire and report honestly soft information.\footnote{\cite{muller} formulate an ``ignorance equivalent,'' which they apply to (among other things) incentivizing an agent to acquire and report information.}
In both \cite{zermeno2011} and \cite{clark2021contracts} both learning and decision making are delegated to an agent. 

This paper connects the expert-testing and information-acquisition literatures, allowing the competent expert to acquire information in a classical setting of designing tests to fail quacks.


\section{The Main Result}\label{model}
There is a finite set \(\Theta\) of possible outcomes with \(n\) elements. Alice offers a contract to a self-proclaimed expert, Bob, that consists of a lump-sum payment (in utils), \(u > 0\), and a penalty, \(d > 0\). Henceforth, we refer to this variety of contract as simply a \textbf{contract}. If Bob accepts the contract he obtains the payment, \(u\), up front; then announces an outcome that will not be observed. Subsequently, an outcome \(\theta \in \Theta\) 
is publicly observed by Alice and Bob; and if the announced outcome coincides with the observed outcome, Alice levies the penalty, \(d\), on Bob.

A model is a probability distribution over outcomes
\(\tau \in T \subseteq \Delta\left(\Theta\right)\), where \(T\) denotes the (closed) set of conceivable models. We assume that \(T\) is \textbf{balanced}, meaning that there does not exist an outcome that is the least \textit{ex ante} likely according to all conceivable models; formally, there does not exist \(\ubar{\theta}\) such that \(\tau(\ubar{\theta}) \le \tau(\theta)\) for all \(\theta \in \Theta\) and \(\tau \in T\).\footnote{Without this assumption, a contract has no power. A special case in which the assumption is satisfied is when \(T = \Delta(\Theta)\).} Bob is either an expert or a charlatan. If Bob is an expert, he holds a prior probability distribution over models \(Q \in \Delta\left(T\right)\), which can be interpreted as his \emph{knowledge}---he knows the ``state of the art'' of the theory.
Bob, therefore, has a prior distribution over outcomes \(\mu \in \Delta\left(\Theta\right)\), where \(\mu = \int \tau \, \mathrm{d}Q(\tau)\). 

After seeing the contract, expert Bob decides whether to acquire additional information about the model at a cost. We can allow him to learn either before or after deciding whether to accept the contract: our main result holds under both specifications. He learns by observing the outcome of a statistical \textbf{experiment}, which is a pair \(E = \left(S, \chi\right)\), where \(S\) is a set of possible signals and \(\chi \colon T \to \Delta(S)\) is a stochastic map from the set of models to the set of signals; denote the set of experiments available to Bob by \(\mathcal{E}\). The cost of acquiring information is given by a cost function \(C \colon \mathcal{E} \times \Delta(T) \to \mathbb{R}_{+} \cup \left\{\infty\right\}\), where \(C\left(E, Q\right)\) is the cost of acquiring experiment \(E\) when expert Bob's prior over models is \(Q\).

If expert Bob accepts contract \((u,d)\) and does not acquire information, his expected payoff under prior \(Q\) is 
\[\overline{V}(Q) := \max_{i=1,\ldots, n}\{u - d \mu_i\} = u - d \min_{i=1, \ldots, n} \mu_i,\]
where \(\mu\) is the prior distribution over outcomes induced by \(Q\), and \(\mu_i\) is the \(i\)-th entry of \(\mu\). If Bob acquires information using experiment \(E = (S, \chi)\), his expected payoff is 
\[V(E,Q):= \int_{S} \max_{i=1,\ldots, n}\left\{u - d \mu^s_i\right\} \, \mathrm{d}\overline{\chi}_Q(s) = \int_{S} \left(u - d \min_{i=1,\ldots,n} \mu^s_i \right) \mathrm{d}\overline{\chi}_Q(s),\]
where \(\overline{\chi}_Q\) is such that \(\overline{\chi}_Q(Z) = \int \chi(Z | \tau) \, \mathrm{d}Q(\tau)\) for all measurable \(Z \subseteq S\), \(\mu^s\) is Bob's posterior belief over outcomes after receiving signal \(s\), and \(\mu^s_i\) is its \(i\)-th entry. Finally, let \(\Upsilon(E, Q):= V(E,Q) - \overline{V}(Q)\) be expert Bob's gain from learning according to experiment \(E\) when he accepts contract \((u,d)\) and his prior over models is \(Q\). 

For any \(\delta > 0\), say that an experiment \(E\) is \(\delta\)-\textbf{valuable} for \(Q\) if \(\Upsilon(E, Q) > \delta\). We make the following joint assumption on \(C\) and \(\mathcal{E}\): 
\begin{assumption}\label{assumption}
    There exist \(\varepsilon, \eta > 0\) such that for all \(Q\) inducing \(\mu \in B\left(\mathbf{\frac{1}{n}}, \eta\right)\), there exists \(E_{\mu} \in \mathcal{E}\) that is \(\varepsilon\)-valuable for \(Q\), and \(C\left(E_{\mu}, Q\right) \le K\) for some \(K \in \mathbb{R}_{+}\).\footnote{\(\mathbf{\frac{1}{n}}\) denotes the vector that has \(1/n\) for each of its entries, and \(B\left(\mathbf{\frac{1}{n}}, \eta\right)\) is the ball centered at \(\mathbf{\frac{1}{n}}\) with radius \(\eta\). Note that \(\varepsilon\) and \(K\) are \emph{uniform} to all \(\mu \in B\left(\mathbf{\frac{1}{n}}, \eta\right)\).}
\end{assumption}

A stronger condition than \cref{assumption} is that expert Bob, no matter his prior over models, always has access to an experiment that is informative about the outcome at a finite cost. Another example that satisfies \cref{assumption} is that expert Bob has access to all experiments, and his cost of acquiring information is posterior separable \citep{caplin2022rationally}.\footnote{This class of information costs includes the entropy-based cost function \citep[see e.g.][]{sims1998stickiness,sims2003implications,matvejka2015rational}; the log-likelihood cost of \cite{costofinfo}; the neighborhood-based cost function studied by \cite{hebert2021neighborhood}; and the quadratic (posterior variance) cost function. Some variants of this class of costs can also be allowed; see, for example, Example 4 and 6 in \cite{choice}.} His cost could also be experimental \citep{denti2022experimental}.


If Bob is a charlatan he cannot acquire any information. Moreover, charlatan Bob is ignorant of the theory and so evaluates payoffs by his minimum expected utility in all conceivable models \(T\). By rejecting the contract, both varieties of Bob get payoff \(0\).

We say a contract \textbf{screens} charlatans if expert Bob prefers to accept it, \textit{regardless of his knowledge}, but charlatan Bob strictly prefers to reject. Our main theorem is simple: 
\begin{theorem}\label{main}
There exists a contract that screens charlatans.
\end{theorem}

\begin{proof}
By \cref{assumption}, there exist \(\varepsilon, \eta > 0\) such that for any prior over models \(Q\) that induces \(\mu \in B\left(\mathbf{\frac{1}{n}}, \eta\right)\), we can find \(E_{\mu} \in \mathcal{E}\) such that \(E_{\mu}\) is \(\varepsilon\)-valuable for \(Q\) with \(C\left(E_{\mu}, Q\right) \le K\) for some \(K \in \mathbb{R}_{+}\). If \(E_{\mu}\) is \(\varepsilon\)-valuable for \(Q\), by choosing \(d > K/\varepsilon\), \(\Upsilon\left(E_{\mu}, Q\right)\) is strictly larger than \(K\) for any \(Q\) that induces \(\mu \in B\left(\mathbf{\frac{1}{n}}, \eta\right)\). Therefore, \(u\) can be chosen so that \(u - d/n < 0\) but every expert Bob's payoff, net of possible learning costs, is positive. Hence, he accepts the contract. Since charlatan Bob evaluates payoffs by his minimum expected utility, there is no way of randomizing over announcements that secures him a payoff greater than \(u - d/n\). As this is strictly less than \(0\), he refuses the contract. Finally, note that for inducible \(\mu \notin B\left(\mathbf{\frac{1}{n}}, \eta\right)\), expert's Bob's payoff is positive at his prior, which is a \textit{lower bound} for his payoff, as he can acquire information.\end{proof}

Because the set of conceivable models, \(T\), is balanced, there can be an expert whose knowledge \(Q\) makes him believe that all outcomes are equally likely; that is, \(\mu = \mathbf{\frac{1}{n}}\). However, the best way for charlatan Bob to randomize is to announce each outcome with equal probability. Therefore, an expert Bob whose belief over outcomes is ``around'' \(\mu = \mathbf{\frac{1}{n}}\) is the hardest to ``separate'' from charlatan Bob; our proof above shows precisely that so long as expert Bob can acquire further information about models, distinguishing him from the charlatan is possible even when it is the most difficult.

What is the role of the ``balanced'' condition? It ensures that no single outcome is uniformly the least likely across all conceivable models in \(T\). Without this condition, there would exist an outcome $\ubar{\theta}$ that every model in $T$ assigns the smallest probability, making $\ubar{\theta}$ the ``safest'' announcement for any agent seeking to minimize expected penalties. In that case, charlatan Bob could always choose $\ubar{\theta}$ to maximize his worst-case payoff, and there would be situations in which expert Bob, even with superior knowledge, would also optimally announce $\ubar{\theta}$ without acquiring information. This would render learning unprofitable and prevent the contract from separating the expert from the charlatan. By requiring $T$ to be balanced, we rule out such universally safe predictions, ensuring that for every outcome there is some model in which it is not the least likely. This creates the potential for information acquisition to alter the expert’s optimal strategy, making screening feasible.


The remainder of the paper proceeds as follows. In 
\cref{os11}, we explain how our model maps to the motivating example in \cite{falsifiability} and why 
\cref{main} fails to hold there. In \cref{example} we illustrate, via two examples, how can we overcome the difficulty by allowing expert Bob to acquire further information. \cref{euuibob} studies a variant in which charlatan Bob is also an expected-utility maximizer, but incapable of learning.

\subsection{The Necessity of Learning} \label{os11}

In \citeauthor{falsifiability}'s example, expert Bob knows the composition of an urn that contains balls of \(n\) possible colors. That is, he knows the probability distribution over outcomes \(\tau \in \Delta\left(\Theta\right)\), which can be interpreted as the true data-generating process. The uninformed Bob does not. Bob announces a falsifiable theory: he must claim that at least one color is impossible. Alice then draws a ball, and if it is the ``impossible'' color, Bob gets fined \(d\).

In our framework, there is meta-uncertainty about the outcome. The true model (data-generating process) \(\tau\) is not known, and, instead, expert Bob holds a probability distribution over models \(Q\). However, this is immaterial: Bob's distribution over models induces a distribution over outcomes \(\mu \in \Delta\left(\Theta\right)\). 

Without learning, the following result summarizes \cite{falsifiability}'s example:
\begin{proposition}
If a contract is such that expert Bob accepts it, no matter his prior, charlatan Bob will also accept it.
\end{proposition}
\begin{proof}
    Let \(\mathbf{\mu}\) denote an arbitrary distribution over outcomes. Observe that expert Bob accepts a contract if \(u - d \min_{i} \mu_i \geq 0\), where \(\mu_i\) is the \(i\)-th entry of \(\mu\). As \(\min_i \mu_i\) is maximized when \(\mu_i = 1/n\) for all \(i\), expert Bob accepts a contract, no matter the distribution over outcomes he holds, if and only if \(u \geq d/n\). However, as observed by \cite{falsifiability}, by randomizing uniformly over announcements, charlatan Bob guarantees himself a payoff of \(u - d/n\), which is weakly greater than \(0\) by construction. Therefore, charlatan Bob also accepts the contract.
\end{proof}
In fact, even more general contracts (beyond the proposed one corresponding to ``falsifiability'') cannot weed out the quack. That is, there is no contract---i.e., a pair \(\left(M, t\right)\), where \(M\) is a compact set of messages and \(t \colon M \times \Theta \to \mathbb{R}\) is continuous---such that expert Bob accepts it (no matter his prior) but charlatan Bob does not. 

For simplicity, suppose there are two outcomes. Observe that a contract induces a convex value function on the \(1\)-simplex, \(V\left(x\right)\), for expert Bob. That he accepts it, no matter his prior over outcomes, requires that \(V\left(x\right) \geq 0\) for all \(x\). But then it is easy to see that charlatan Bob will also accept the contract: if \(V\left(0\right) = 0\) or \(V\left(1\right) = 0\), charlatan Bob will send a message that is optimal for expert Bob at prior \(0\) or \(1\), respectively. If \(V\left(0\right), V\left(1\right) > 0\) and \(V\left(\tilde{x}\right) = 0\) for some \(\tilde{x} \in \left(0,1\right)\) for which \(V'\left(\tilde{x}\right)\) exists, charlatan Bob will send the message optimal for expert Bob at prior \(\tilde{x}\), which necessarily has an expected payoff of \(0\). Finally, if \(V\left(0\right), V\left(1\right) > 0\) and \(V\left(\tilde{x}\right) = 0\) for some \(\tilde{x} \in \left(0,1\right)\) but \(V'\left(x\right) \neq 0\) for all \(x \in \left[0,1\right]\) (and so, necessarily \(V\) is kinked at \(\tilde{x}\)), charlatan Bob will mix between the two messages optimal for expert Bob with prior \(\tilde{x}\), and nature will choose the worst-case probability \(\tilde{x}\) for charlatan Bob, yielding him a payoff of \(0\).


\subsubsection*{Some Intuition}

The basic intuition behind this subsection's example is that in order to elicit information from an expert, no matter his prior, the value function induced by the contract cannot dip below the horizontal axis. That is, it must lie everywhere above \(0\). However, by randomizing judiciously (though randomization may not be necessary), the quack can always secure a payoff no less than the minimum of the value function. He is not screened out.

By allowing expert Bob to learn, we make it so that although the value function does dip below the horizontal axis, the value function evaluated at any posterior that may result from learning is positive. Charlatan Bob is screened out.

\subsection{Two Examples} \label{example}


\subsubsection*{Two State, Single Experiment Example}
Suppose there are two possible outcomes, \(0\) and \(1\). In this setting, a model can be identified by the probability it assigns on outcome 1 being observed; denote this probability by \(\tau\). Assume that there are two conceivable models: \(\tau_L = 1/3\) and \(\tau_H = 2/3\). Let \(Q := \mathbb{P}(\tau_H)\) denote expert Bob's prior. Expert Bob has access to only one experiment: he can get a signal \(s \in \{h,l\}\) such that \(\mathbb{P}(h \mid \tau_H) = \mathbb{P}(l \mid \tau_L) = 3/4\), and the cost of performing the experiment is \(c = 20\). Letting \((u,d) = (280, 600)\), if expert Bob has \(Q \in (0.35, 0.65)\), it can be verified that his gain from acquires information is strictly more than \(c = 20\), and therefore he acquires information. If he has \(Q \notin (0.35, 0.65)\) his payoff is greater than or equal to \(u - 0.45 d = 10 > 0\). The expected payoff of charlatan Bob; however, is \(u - d/2 = -20 < 0\). Thus, the contract \((u,d)\) screens charlatans.


\subsubsection*{Two State, Posterior-Separable Cost Example}
As in the previous example, suppose there are two possible outcomes, \(0\) and \(1\), and two conceivable models, \(\tau_L = 1/3\) and \(\tau_H = 2/3\). However, now expert Bob has access to any experiment, and his cost of obtaining information is monotone in the Blackwell order. For convenience, we write his cost of obtaining information as a cost defined on his resulting distribution over posteriors (over models) \(F \in \Delta(\Delta(T))\): the cost of acquiring the experiment that produces \(F\) is \(\Gamma\left(F\right) = \kappa \int_{0}^{1}c\left(x\right)\, \mathrm{d}F\left(x\right) - \kappa c\left(Q\right)\) for some strictly convex function \(c\colon \left[0,1\right] \to \mathbb{R} \cup \{\infty\}\), \(\kappa \in \left(0,\overline{\kappa}\right]\), and \(\overline{\kappa} \in \mathbb{R}_{++}\).\footnote{It is important to note that we are using the fact that \(C\) depends not only on the experiment \(\chi\), but the prior, \(Q\), as well or else this would be impossible, as pointed out by \cite{denti2022experimental} and \cite{denti2022random} \citep[see also][]{mensch2018cardinal}.}

Observe that a contract induces the net value function (that is, the value function net of information acquisition costs)
\[V\left(x\right) = u - d \min\left\{\frac{1+x}{3}, \frac{2-x}{3}\right\} - \kappa c\left(x\right) + \kappa c\left(Q\right),\]
which is kinked precisely at \(x = 1/2\). Accordingly, for all \(\kappa\) and \(c\), and for all \(Q\)'s such that expert Bob acquires information, his optimal learning will not have support in some interval \(\left(x_L, x_H\right)\), where \(x_L < Q < x_H\). Moreover, if \(u - \frac{1}{2}d = 0\), \(\mathbb{E}_{F^{*}_{Q}} V > 0\) for all \(Q \in \left[0,1\right]\), where \(F^{*}_{Q}\) is an optimally acquired distribution.\footnote{Note that \(F^{*}_{Q}\) may be the degenerate distribution on \(Q\).} Accordingly, there is some \(\varepsilon > 0\) such that \(u - \varepsilon - \frac{1}{2}d < 0\) but \(\max_{F_Q}\mathbb{E}_{F_{Q}}V\left(x\right) - \varepsilon > 0\) for all \(Q \in \left[0,1\right]\).

The optimal randomization strategy for charlatan Bob, should he accept the contract, is to announce each outcome with equal probability, yielding him a payoff of \(u - \varepsilon - \frac{1}{2}d < 0\). Thus, only the expert will deliver a theory.

\cref{fig1} illustrates a collection of net value functions induced by a contract that screens charlatans, when expert Bob's cost function is the (expected) reduction in (Shannon) entropy. The dotted red curve is the net value function for expert Bob with knowledge \(Q = 1/2\). The gray curves are the net value functions for various other \(Q\)s. They are pointwise increasing as \(Q\) gets further from \(1/2\). The dotted orange lines are the concavifying lines corresponding to Bob's optimal learning (for the different priors). The Bob with the highest depicted curve does not acquire any information. Indeed, for our main result to hold, we only need the expert with prior sufficiently close to the kink \(1/2\) to ``move away from there'' by acquiring information.

\begin{figure}
    \centering
    \includegraphics[scale=.16]{fig1slim.png}
    \caption{The net value functions induced by a contract that screens charlatan Bob. \href{https://www.desmos.com/calculator/guw5gmdc6j}{Try it yourself!}}
    \label{fig1}
\end{figure}

\subsection{When the Quack is an SEU-Maximizer}\label{euuibob}

\cite{falsifiability} aptly model a (correct) scientific theory as knowledge of the true data-generating process, which we modify to holding a belief about possible models. Alas, they show that falsifiability is unable to distinguish correct theories from incorrect ones. In our model, expert Bob is not only imperfectly informed about the model, but can also acquire information about it.

This seems reasonable to us, especially if one thinks of science as an incremental process, where hypotheses are refined over time, converging to consensus about the model only in the long-run limit. This raises the question; however, as to how we should think about charlatan Bob. Perhaps he should be an expected-utility maximizer, himself, with a well-defined subjective prior over conceivable models. A special case is where charlatan Bob also knows the ``state-of-the-art'' \(Q\), but he is unable to acquire further information. In this variant, the value of the falsifiability paradigm is how it potentially enables the production of scientific theories only by the Bob who can learn, ensuring progress and eventual knowledge.

Formally, we can adapt \cref{main} to the case where charlatan Bob is a subjective expected-utility maximizer with a known prior over models \(P \in \Delta(T)\) that induces an (interior) prior distribution over outcomes \(\rho \in \inter \Delta(\Theta)\). Define a \textbf{generalized contract} to be \(n+1\) scalars \(u > 0\) and \(d_i > 0\) (\(i = 1, \dots, n\)). As before, if Bob accepts the contract he obtains the payment, \(u\), up front; then announces an outcome that will not be observed. If the announced outcome \(\theta_i\) coincides with the observed outcome, Alice levies the penalty, \(d_i\), on Bob.


We adapt our earlier joint assumption on \(C\) and \(\mathcal{E}\) by replacing \(\mathbf{\frac{1}{n}}\) with \(\rho\):
\begin{altassumption}
    There exist \(\varepsilon, \eta > 0\) such that for any \(Q \in \Delta(T)\) inducing \(\mu \in B\left(\rho, \eta\right)\), there exists \(E_{\mu} \in \mathcal{E}\) that is \(\varepsilon\)-valuable for \(Q\), and \(C\left(E_{\mu}, Q\right) \le K\) for some \(K \in \mathbb{R}_{+}\).
\end{altassumption}
Then,
\begin{proposition}\label{main2}
There exists a generalized contract that screens charlatans.
\end{proposition}
\begin{proof}
    It suffices to show that there is a solution to 
    \[-\rho_1 d_1 = - \rho_2 d_2 = \cdots = -\left(1-\rho_1 - \dots - \rho_{n-1}\right)d_n\text{,}\]
    with one degree of freedom. The rest follows the proof of \cref{main}, \textit{mutatis mutandis}. Such a solution is
    \[d_{i} = \frac{1-\rho_1 - \dots - \rho_{n-1}}{\rho_i}d_n \ \ \text{for all} \ i = 1, \dots, n-1 \text{,}\]
    where \(d_n\) is a free variable. The \(d_i\)
    's are well-defined because \(\rho \in \inter{\Delta}\). \end{proof}
If \(d_i\) must equal \(d\) for all \(i\)---i.e., generalized contracts are forbidden---unless \(\rho = \mathbf{\frac{1}{n}}\), it may not be possible to screen charlatans.

\bibliography{sample.bib}

\end{document}