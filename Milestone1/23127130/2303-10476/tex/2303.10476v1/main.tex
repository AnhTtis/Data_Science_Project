\documentclass[sigconf]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information. This information is sent to you
%% when you complete the rights form. These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\copyrightyear{2023} 
\acmYear{2023} 
\setcopyright{acmlicensed}\acmConference[WWW '23 Companion]{Companion Proceedings of the ACM Web Conference 2023}{April 30-May 4, 2023}{Austin, TX, USA}
\acmBooktitle{Companion Proceedings of the ACM Web Conference 2023 (WWW '23 Companion), April 30-May 4, 2023, Austin, TX, USA}
\acmPrice{15.00}
\acmDOI{10.1145/3543873.3587608}
\acmISBN{978-1-4503-9419-2/23/04}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Sci-K '23]{3rd International Workshop on Scientific Knowledge: Representation, Discovery, and Assessment (Spring 2023) co-located with The Web Conference 2023}{Austin, TX}

\usepackage{cleveref}
\usepackage{tabularx}

\setlength{\headheight}{15.36652pt}

%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The ``title'' command has an optional parameter,
%% allowing the author to define a ``short title'' to be used in page headers.
\title{Assessing Scientific Contributions in Data Sharing Spaces}
% \title{Accountable Scientific Research Dataset Sharing with the SCIENCE-index}

%%
%% The ``author'' command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% ``authornote'' and ``authornotemark'' commands
%% used to denote shared contribution to the research.

\author{Kacy Adams}
\email{adamks4@rpi.edu}
\orcid{0000-0003-2873-8616}
\affiliation{%
  \institution{Rensselaer Polytechnic Institute}
  \streetaddress{110 8th St}
  \city{Troy}
  \state{NY}
  \country{USA}
}

\author{Fernando Spadea}
\email{spadef@rpi.edu}
\orcid{0009-0006-4278-3666}
\affiliation{%
  \institution{Rensselaer Polytechnic Institute}
  \streetaddress{110 8th St}
  \city{Troy}
  \state{NY}
  \country{USA}
}
\author{Conor Flynn}
\email{flynnc@rpi.edu}
\orcid{0009-0006-4537-2838}
\affiliation{%
  \institution{Rensselaer Polytechnic Institute}
  \streetaddress{110 8th St}
  \city{Troy}
  \state{NY}
  \country{USA}
}

\author{Oshani Seneviratne}
\email{senevo@rpi.edu}
\orcid{0000-0001-8518-917X}
\affiliation{%
  \institution{Rensselaer Polytechnic Institute}
  \streetaddress{110 8th St}
  \city{Troy}
  \state{NY}
  \country{USA}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato and Tobin, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}  
In the present academic landscape, the process of collecting data is slow, and the lax infrastructures for data collaborations lead to significant delays in coming up with and disseminating conclusive findings. Therefore, there is an increasing need for a secure, scalable, and trustworthy data-sharing ecosystem that promotes and rewards collaborative data-sharing efforts among researchers, and a robust incentive mechanism is required to achieve this objective. Reputation-based incentives, such as the h-index, have historically played a pivotal role in the academic community. However, the h-index suffers from several limitations.
This paper introduces the SCIENCE-index, a blockchain-based metric measuring a researcher's scientific contributions. Utilizing the Microsoft Academic Graph and machine learning techniques, the SCIENCE-index predicts the progress made by a researcher over their career and provides a soft incentive for sharing their datasets with peer researchers. 
To incentivize researchers to share their data, the SCIENCE-index is augmented to include a data-sharing parameter. DataCite, a database of openly available datasets, proxies this parameter, which is further enhanced by including a researcher's data-sharing activity.
Our model is evaluated by comparing the distribution of its output for geographically diverse researchers to that of the h-index. We observe that it results in a much more even spread of evaluations. The SCIENCE-index is a crucial component in constructing a decentralized protocol that promotes trust-based data sharing, addressing the current inequity in dataset sharing.
The work outlined in this paper provides the foundation for assessing scientific contributions in future data-sharing spaces powered by decentralized applications.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10003456.10003457.10003458.10003460</concept_id>
       <concept_desc>Social and professional topics~Industry statistics</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10002944.10011123.10011124</concept_id>
       <concept_desc>General and reference~Metrics</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003260.10003261.10003270</concept_id>
       <concept_desc>Information systems~Social recommendation</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10002951.10003317.10003338.10003343</concept_id>
       <concept_desc>Information systems~Learning to rank</concept_desc>
       <concept_significance>100</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[300]{Social and professional topics~Industry statistics}
\ccsdesc[300]{General and reference~Metrics}
\ccsdesc[300]{Information systems~Social recommendation}
\ccsdesc[300]{Information systems~Learning to rank}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Incentive Mechanisms, Author-level Metrics, Dataset Sharing, Peer-to-peer Sharing, Blockchain, Smart Contracts}

\maketitle

\section{Introduction}

There is a rising need for a secure, scalable, and trustless data-sharing ecosystem that recommends, incentivizes, and rewards collaborative data-sharing efforts between researchers in many scientific disciplines.
Such a protocol would require a robust incentive mechanism. 
In the academic space, reputation-based incentives rule, and since 2006, h-index~\cite{kelly2006h} has reigned superior. It is a widely used bibliometric indicator that measures a scientist's publications' productivity and citation impact. The h-index, however, has several flaws.
One of the main shortcomings of the h-index is that it does not account for the quality or impact of individual publications but considers all publications equally. Additionally, the h-index tends to favor established researchers with a long publication history, as it considers the total number of publications. The h-index may be subject to manipulation by self-citations or citation cartels, which can inflate an author's score. Therefore, it is important to use the h-index in conjunction with other metrics and to interpret it with caution.

If all research is to be fair and incentivized, researchers must mend these discrepancies as their reputations define their careers. We present a new reputation-based metric called the SCIENCE-index to incentivize and reward data aggregation and sharing.

We utilize the Microsoft Academic Graph (MAG)~\cite{sinha2015an} to train an AI model to predict the researcher's progress over their career. We persist this model via smart contract on the Ethereum blockchain~\cite{buterin2016ethereum} and allow researchers to look up their and others' SCIENCE-indexes via web identifiers such as the Semantic Scholar ID.
A blockchain-based mechanism provides several advantages, including increased security and transparency, as well as the ability to incentivize data sharing through the use of smart contracts.

\subsection{The Need for Data Sharing}

Data is a significant part of modern evidence-based scientific research. Many studies rely heavily on collecting large amounts of data ranging from studies on human behaviors to machine learning. Collecting this data can be painstaking and time-consuming, taking up to seventeen years from bench to bedside in certain biomedical research studies~\cite{morris2011answer}. Even in computer or information science research, a significant effort goes into collecting data. For example, several web science researchers 2012 conducted a study on buying unlicensed slimming drugs online, which required ethnographic data collection. They had to manually copy and paste parts of their data from the sites they scoured and held interviews with several stakeholders in UK regulatory agencies~\cite{drugs}. 

Despite data collection being very important, researchers are typically not incentivized to share their data. In fact, there are many reasons not to share data. Researchers face many challenges when it comes to intellectual property and confidentiality~\cite{meadows-to-share-or-not-share}. Others from less-endowed institutions may fear their work being scooped up by more prestigious institutions~\cite{bezuidenhout2018hidden} or fear that others may use it to their advantage~\cite{fernandez2010barriers}. 

On top of this, existing academic incentives that reach further than a citation are scarce. One incentive, the use of open data badges\footnote{\url{https://osf.io/tvyxz/wiki/home}}, has been tested in health and medical research~\cite{kidwell2016badges}, yet studies are unsure of their effectiveness~\cite{rowhani2017incentives}.  
Better incentives are necessary as data sharing is essential in modern academic research.
In the case of protecting research participants, data sharing is an ethical necessity to protect human lives. In January 2016, one participant died, and four others were injured due to the first human testing of a fatty acid inhibitor \cite{datasharing}. With human lives at stake, data from studies like these must be made available.

In this work, we address the aforementioned inequity of dataset sharing. To incentivize researchers to share their data, we must build their reputation to include their data-sharing activity and the bibliometric reputation available through indexes such as the h-index. We augment our initial SCIENCE-index to include a data-sharing parameter. We proxy this via DataCite~\cite{brase2009datacite}, a database of openly available datasets, and widen our MAG dataset by including the number of times a researcher has shared their data.
Such a metric would be pivotal in building a decentralized protocol that allows data sharing by adding trust between individuals who have not worked together prior to the collaboration event.

\subsection{More Than The h-index}

The h-index was created to measure the impact of a researcher's work. It represents the maximum number of ``h'' papers published by a researcher with at least ``h'' citations. This used to be a fairly accurate reflection of researchers' past impact, at least when tested against the reviews of the \emph{Bochringer Ingelheim Fonds} organization \cite{hindex}. However, the h-index's reliability has greatly diminished in recent years and no longer represents the scientific reputations of researchers \cite{hbad}. 

Further, we express our concerns about the unfair playing field of scientific research in underdeveloped countries~\cite{unfairplayingfield, developing}. We believe that more robust reputation-based metrics would help to level the ``playing field'' among researchers with fewer resources compared to well-endowed researchers.

Data sharing is a massive service to the research community largely unconsidered by the h-index. A system is needed to measure an individual's contributions to science beyond that of publications. Then, researchers will be properly credited for their work and, thus, incentivized to continue to collect and share invaluable data.

\subsection{A Data Sharing Space}

The work outlined in this paper is part of a larger plan to create a data-sharing environment via blockchain. This environment would allow researchers to share data while being rewarded. We believe blockchain is an appropriate technology to use here because it is an effective ledger for keeping track of data sharing in a transparent and accountable manner. Its decentralized and public nature also makes it more transparent for the researchers who share and use data on it. To incentivize participation and the sharing of good quality data, we propose a new index, i.e., the SCIENCE-index, for assessing the impact of researchers' contributions with a model that will be hosted and persisted in the same decentralized manner.

We specifically target fields where research requires and produces large amounts of data. This is because the number of opportunities to share and use data varies greatly between fields, so there is no one-size-fits-all measure of one's contributions to data sharing. However, by specifically considering data-heavy fields, like, for example, biomedical research, we can create an accurate metric for those fields without downplaying the significance of work in other fields with fewer datasets.

We present the SCIENCE-index in \Cref{sec:SCIdex}, detailing a robust linear model for rating researchers in \Cref{sec:Model1}. In \Cref{sec:infra}, we display a decentralized infrastructure for persisting this model in a public fashion. Next, we discuss our prior work in the decentralized data sharing space in \Cref{sec:sharingscience} and augment our linear model to incentivize data sharing. We present results from the model and its augmentation in \Cref{sec:results} and evaluate the efficacy of our model on geographically distributed researchers in \Cref{sec:eval}. Finally, we discuss our work and future work in \Cref{sec:disc} and present related work in \Cref{sec:related}.

\section{The SCIENCE-index}
\label{sec:SCIdex}

We present the SCIENCE-index, a self-sustaining metric for scientific reputation. The SCIENCE index encompasses an expressive, provenance-centric approach, and it is a recursive acronym for \underline{S}CIENCE, \underline{C}apability-based, \underline{I}ntention-centric, \underline{E}xperiment-oriented, \underline{N}etworked, \underline{C}ollaborative, \underline{E}xpression.

We bootstrap the SCIENCE-index via 21 million data points from the MAG that overlaps with entries from the data-sharing website DataCite.
First, using a robust multiple linear regression across several academic career statistics, we predict a researcher's h-index and compare this to their actual h-index. 
This difference is then normalized to a scale from zero to ten. Five means expected, under five is below average, and over five is above average. 
We persist the model via smart contract on a public blockchain, allowing the model to exist publicly and continue to scale and update as researchers use it. We detail the model and its infrastructure below.

\subsection{Data Aggregation}

We utilize the MAG to aggregate a dataset of 21,660,755 authors, each with their corresponding publication count, citation count, h-index, and career length. We select and use publication count and citation count as they are straightforward indicators of productivity and are provided as part of the ``authors'' table of the MAG. With some simple calculations, we can build paper lists for each of our authors via the ``papers'' table of the MAG, and with this, we can assume career length as the years between their oldest paper and their newest paper and calculate their h-index. Although within the MAG and the academic research space, there are many more abstract parameters to use, these four were the most accessible via the web through tools such as the Semantic Scholar API\footnote{\url{https://www.semanticscholar.org}}. This accessibility is an important piece of our goal to persist our metric in a decentralized manner.

\subsection{The Model}
\label{sec:Model1}

The model of the SCIENCE-index takes in four different inputs: career length, paper count, citation count, and h-index.
\[ \alpha_1 \textit{= Career Length} \]
\[ \alpha_2 \textit{= Paper Count} \]
\[ \alpha_3 \textit{= Citation Count} \]
\[ \alpha_4 \textit{= h-index} \]
We calculate the predicted h-index value ($\beta$) from these parameters and compare it to the actual h-index ($\alpha_4$), extracting the SCIENCE-index from this difference. Since we have a narrow dataset, we use Multi-Linear Regression (MLR) to find our predicted h-index. After training the MLR on our 21 million data points, we derive the following equation
\begin{align*}
&\beta &&=\omega_0+\omega_1\alpha_1+\omega_2\alpha_2+\omega_3\alpha_3\\
& &&=1.71933+0.06902\alpha_1+0.10867\alpha_2+0.00304\alpha_3
\end{align*}
Finally, we scale for outliers that can occur anywhere $\beta>60$. This threshold, i.e., \textbf{60}, is based on the notion that ``an h index of \textbf{60} after 20 years, or 90 after 30 years, characterizes truly unique individuals" as stated by Hirsch~\cite{pnas}. Therefore, we give these researchers a bias to their SCIENCE-index calculation such that it will give them a higher score. If $\beta>60$, we apply the given function to scale it to an appropriate value.

\begin{align*}
&\text{If }\beta>60\text{:}&&
\beta =\frac{\beta}{0.571+(0.007*\beta)}
\end{align*}

Using these weights and our approximation for $\beta$, we can then calculate the difference ($\delta$) between the predicted and the actual h-index.
After finding $\delta$, we normalize it according to the entire dataset to find $\epsilon$, our calculated performance factor comparable to any other data point's $\epsilon$ value. We then logarithmically regress the scaled delta to fit on a scale of one to ten for readability and easy comparison.

\[\delta =\alpha_4-\beta\]
\[\epsilon =\frac{\delta-\overline{\delta}}{\sigma_\delta}\]
\[\phi =\frac{10}{1+e^{-\epsilon}}\]

This calculated value of $\phi$ is the outputted SCIENCE-index. Any value of $\phi$ below $5$ is deemed a below-average academic contribution by the researcher, and any value of $\phi$ above $5$ signifies above-average contributions.
 
\subsection{Infrastructure}
\label{sec:infra}

The SCIENCE-index lives as weights in a smart contract, making it publicly accessible and completely transparent. 
Upon call, a researcher provides their Semantic Scholar identifier, and the smart contract requests a Chainlink oracle~\cite{chainlink}, which requests the Semantic Scholar API to get the requesting researcher's statistics. 
Using these statistics, the smart contract adds the new data point to the model by updating the weights and then calculates and returns the researcher's SCIENCE-index. This sequence is shown in \Cref{fig:SCIDEXsequence}.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=\columnwidth]{figures/SCIDEXsequence.png}
    \caption{High-level Sequence Diagram of Our Linear Model/Smart Contract/Oracle Design Pattern}
    \label{fig:SCIDEXsequence}
\end{figure}

Chainlink is an industry-standard service for building blockchain oracles. Oracles allow smart contracts that live on the immutable blockchain to access the outside, mutable internet. 
We utilize a Chainlink external adapter to build a custom job that, when called, calculates and returns the career statistics used in our model.

\section{Sharing Science}
\label{sec:sharingscience}

We narrow the scope of our incentive mechanism towards a hypothetical data-sharing ecosystem.
The SCIENCE-index is part of the Sharing Science Ontology (SSO), a semantic model for a decentralized academic data-sharing application.

\subsection{The Sharing Science Ontology (SSO)}

We have discussed the need for a data-sharing ecosystem in a secure and scalable manner, and blockchain provides us with the peer-to-peer platform to do so. 
The SSO describes a decentralized protocol to handle and incentivize peer-to-peer academic data sharing in a distributed environment.

The SSO handles peer-to-peer sharing events via what is called collaboration events.
Collaboration events are between two parties and begin with a data request by the data seeker and end with a citation of the data sharer by the data seeker.
Researchers are rewarded via incentive mechanisms for their honest and fair completion of collaboration events.
The SSO is publicly available at a persistent URL at \url{https://github.com/sharing-science/sharing-science-ontology}. 

\subsection{The SCIENCE-index Augmented}
\label{sec:Model2}

In an attempt to incentivize the sharing of data, we introduce a new parameter to our SCIENCE-index model. We include the number of times a researcher has shared a dataset. In our discussed protocol, this would represent the number of collaboration events a researcher has participated in. 
We bootstrap the model again with approximately 3000 data points from the MAG. Using the DataCite API~\cite{brase2009datacite}, we can count the times a researcher has published a publicly available dataset and establish this as a proxy for our collaboration events. Our parameters now include the following:
\[ \alpha_1 \textit{= Career Length} \]
\[ \alpha_2 \textit{= Paper Count} \]
\[ \alpha_3 \textit{= Citation Count} \]
\[ \alpha_4 \textit{= Data Share Count}^2 \]
\[ \alpha_5 \textit{= h-index} \]
We again regress on the h-index to predict a researcher's h-index and scale the difference. To further incentivize data sharing, we weigh the number of data shares by a power of two, and this gives enough weight to data sharing that researchers can effectively increase their SCIENCE-index through data-sharing activities.

\section{Results}
\label{sec:results}
With train and test sets from our initial data sets, we can visualize the results of our two proposed models.

\subsection{The SCIENCE-index}
Without the data sharing parameter, our initial SCIENCE-index presents a distribution as seen in \Cref{fig:SCIDEXdensity}.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=\columnwidth]{figures/SCIDEXdensity.png}
    \caption{Density Plot of the SCIENCE-Index Described In \Cref{sec:Model1}}
    \label{fig:SCIDEXdensity}
\end{figure}

We can see that the model is conservative and leans forward after its density peaks just before 5. We further visualize the model in \Cref{fig:SCIDEXvsH}.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=\columnwidth]{figures/SCIDEXvsH.png}
    \caption{h-index Against the SCIENCE-Index Characterized by Career Length \Cref{sec:Model1}}
    \label{fig:SCIDEXvsH}
\end{figure}

We see that career length is correlated with the h-index, as the shade of blue gets lighter from left to right. However, career length is not correlated with the SCIENCE-index, which allows us to compare researchers of any age. 

\subsection{The SCIENCE-index with Data Sharing Data}

Similar to before, once augmented by the data sharing data, we have a forward-leaning density of the metric shown in \Cref{fig:SCIDATAdensity}

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=\columnwidth]{figures/SCIDATAdensity.png}
    \caption{Density Plot of the Augmented SCIENCE-Index Described In \Cref{sec:Model2}}
    \label{fig:SCIDATAdensity}
\end{figure}

We can compare the original model to the now augmented model using our initial SCIENCE-index with our data-sharing proxy dataset. In \Cref{fig:SCIDATAcomp}, we compare the density of the two models. The augmented SCIENCE-index has been shifted forward as each member has shared data.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=\columnwidth]{figures/SCIDATAcomp.png}
    \caption{Density Plots of Both Models Overlayed To Show the Effect of the Introduction of Data Sharing Data}
    \label{fig:SCIDATAcomp}
\end{figure}

The data sharing dataset has an average data share ``count'' of $6.6$, giving us an average positive shift in each researcher's SCIENCE-index of $0.27$.

\section{Evaluation}
\label{sec:eval}

In the spirit of addressing inequality using the SCIENCE-index, we tailor our evaluation toward comparing geographically distributed researchers. We argue that researchers from less developed countries with fewer resources face larger challenges in building their academic reputations. We have compiled a brief dataset of researchers with half the dataset affiliated with universities located in the ``global south,'' i.e., resource-poor institutions, such as Rhodes University (\url{https://www.ru.ac.za}), University of Sao Paulo (\url{https://www.fearp.usp.br}) and the other half located in the ``global north,'' i.e., resource-rich institutions, such as Stanford University, (\url{https://www.stanford.edu}), Mcgill University (\url{https://www.mcgill.ca}),  University of North Carolina (\url{https://www.unc.edu}), and Grenoble Alpes University (\url{https://www.univ-grenoble-alpes.fr}).

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=\columnwidth]{figures/EvalHindex.png}
    \caption{Density Plot of the h-indexes of Our Hemisphere-Separated ``Global South'' and ``Global North'' Dataset}
    \label{fig:EvalH}
\end{figure}

In \Cref{fig:EvalH}, we show the difference in the h-index between the two groups of researchers with a density plot. The mean of the northern-located researchers is an average of 2 points greater than the southern-located researchers.

We then run this group of researchers through our SCIENCE-index model trained on our original dataset. We show the results of this in \Cref{fig:EvalSCI}. The mean of the SCIENCE-index of each group converges to ~$5.1$. This shows the ability of the SCIENCE-index to level the ``playing field'' and look objectively at a researcher's career progress.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=\columnwidth]{figures/EvalSCIindex.png}
    \caption{Density Plot of the SCIENCE-Index of Our Hemisphere-Separated Dataset}
    \label{fig:EvalSCI}
\end{figure}


\section{Discussion}
\label{sec:disc}

The SCIENCE-index aims to be a valuable tool to credit researchers fairly for their contributions. It allows us to compare researchers at different points in their careers and levels of discrepancies in career statistics. Such a model has an incredibly diverse set of applications, as we can widen the training dataset as we desire. Different parameters can be used and weighted to augment the h-index in favor of specific activities, such as data sharing, as we've discussed, or other statistics, such as conference reviewing or institutional affiliations. This extensibility opens the door to incentive mechanisms in all aspects of academic research.
More importantly, the SCIENCE-index predicts the future progress of a researcher based on their past contributions to science. This is important because it encourages researchers to continue contributing significantly to science and rewards those with a consistent track record.

\subsection{Different Models}

We consider future work examining different learning models to back the SCIENCE-index. This paper explores using multi-linear regression, but many different models could be trained on the data, including logistic regression, support vectors, or decision tree clustering. Yet, we initially aimed to avoid a black box model, as researchers are less likely to accept a metric they cannot interpret.
We also note that there is no one size fits all solution for scientific contributions. For example, it is well known that the citation culture varies between areas, e.g., biology is different from computer science, so the model outlined in \Cref{sec:Model1} may need tweaks to cater to the specific needs of the different scientific communities.

\subsection{Bootstrapping the Models}

A large amount of data is required to bootstrap any model. For the SCIENCE-index, we use a subset of the MAG. The MAG, however, has been deprecated, and for future training of models, we need a current, accurate date. Examining other parameters, such as data sharing, we must look to other sources. We also use DataCite~\cite{brase2009datacite}, but future iterations require more robust and accurate aggregation of data-sharing statistics. Several other data-sharing resources covered in \Cref{sec:related} could be used to aggregate prior data-sharing activity.

We acknowledge that adding a data component to the model in evaluating the SCIENCE-index can introduce some flaws as it may be easier to publish data than a paper, which could lead to artificially inflating one's SCIENCE-index for a useless dataset. However, the broader framework would allow the data reusers to review the dataset they have access to, and future iterations of the model would incorporate the citations to the datasets directly.

Another issue we foresee is the need for a canonical identifier. In our work, we utilized the Semantic Scholar ID. However, author disambiguation is a major issue in academic publishing, which will only worsen once datasets are factored in.
We plan to utilize robust entity resolution mechanisms that will leverage a variety of identifiers, including Orcid~\cite{haak2012orcid} and decentralized identity mechanisms~\cite{reed2020decentralized} championed by standards organizations such as the W3C.

\subsection{Blockchain Usage}

Our goal of persisting a reputation-based metric in a decentralized manner has come with challenges. Public blockchains present invaluable peer-to-peer networks that allow transparency and scalability, and they also present limitations on computability and cost. Our model must be computationally lightweight not to incur incredibly high transaction costs when computing on a public virtual machine but also be robust enough to rate researchers accurately. We must also consider the oracle problem~\cite{oracle2022}, which presents difficulties accessing mutable data from an immutable state machine. Our use of Chainlink oracles described in \Cref{sec:infra} is our first take at tackling this problem.
There are also questions about who takes on the burden of paying for the transaction fees. When a dataset is being requested for a collaboration event, the benefiting party is the dataset requester, and it seems only fair that they should pay the fees required. However, the subsequent citations benefit the original dataset sharer more than the requester unless there is a punitive mechanism for failing to provide proper data citations. Therefore, robust tokenomics should be defined in future work to benefit all parties involved.

\section{Related Work}
\label{sec:related}

% What else has been done? Reuse a lot of what we have, cover both scopes of augmenting the h index and encouraging data sharing
We describe two main themes throughout our work. The former seeks to improve equity and fairness in author-level metrics, while the latter seeks to encourage and incentivize open academic data sharing. 
We explore related work among both of these.

\subsection{Improving Reputation-based Metrics}

The SCIENCE-index is not competing with other researcher metrics but rather looking to host a metric in a public data-sharing environment on a blockchain. Other research in this field is not a competitor as it is an opportunity to improve how the proposed framework can incentivize researchers to share their work.

The g-index takes the maximum number of ``g'' papers that collectively have $g^2$ citations \cite{gindex}. This makes the g-index more sensitive to impactful papers while avoiding letting insignificant papers have too much influence. The g-index would always be at least the value of the h-index, but the g-index adds the extra push from more important papers. Some argue that the two indexes do not replace each other but rather complement each other, where the h-index favors big paper producers and the g-index favors selective researchers~\cite{costas_bordons_2008}. Others, such as Google Scholar\footnote{\url{https://scholar.google.com}}, take a more straightforward approach with the i10-index~\cite{connor2011google}. This is simply the number of papers from a researcher with at least ten citations. Unlike the g-index, this is very simple to use and understand, but like the g-index, it has not risen to acceptance in the same way as the h-index.

We look at another author-level metric that tries to identify meaningful citations. The paper claims that the importance of citations can be measured with three objective measurements and that by identifying these, we can properly distinguish types of citations and credit researchers accordingly~\cite{meaningfulcitations}. 
The significance of a researcher's citations is another important metric for us to consider when comparing researchers. Similarly, it is worthwhile to look into detecting the significance of cited data to the results of a paper. For example, data used to train a model successfully may be more significant than data used to test a model. Such a measurement could improve the SCIENCE-index greatly in the scope of data sharing.

Of course, there is significant work in creating better metrics for assessing research publications. Two major examples are the Field-Weighted Citation Impact (FWCI) and Relative Citation Ratio (RCR) \cite{purkayastha2019comparison}. The FWCI is an article-level metric that reflects the significance of a paper's citation count. It considers the publication type, year, and subject area to measure how a specific publication compares to others \cite{purkayastha2019comparison}. The RCR similarly divides the citation count of a publication by its expected value which is calculated with a quantile regression analysis of the citations of prior publications funded by the National Institutes of Health plotted against the field citation rate \cite{purkayastha2019comparison}. These article-level metrics can be collected to assess researchers, so they have significant promise and could be a useful source to improve the SCIENCE-index.

There is also other work in assessing researchers while considering their data contributions. One example is the data-index which takes into account data publications and citations \cite{hood2021data}. Other examples include the s-index~\cite{ko2013index} and Data Citation Index (DCI) \cite{olfson2017incentivizing, park2019research}. These are also promising projects in the same field as this paper, and each takes different approaches to reach a similar result. However, they are again not competitors but rather potential sources of improvement for the SCIENCE-index as an incentivizer. They could be useful in adjusting our model or even for bootstrapping purposes.

\subsection{Distributed Data Sharing}

Harvard's Dataverse project\footnote{\url{https://dataverse.org/about}}~\cite{king2007introduction} is a centralized approach to solving the problem of credited data sharing. It attempts to promote responsible data sharing by streamlining the process for researchers. 
It offers the option of archiving one's data in a ``collection'' and generating a unique academic citation with its own Document Object Identifier (DOI) that allows others to cite the data resource properly. 
The project aims to give researchers full control over their data.
Data sharers can make data completely open or require that each person that wishes to look at it must ask them for permission to access the data. 
Researchers are also given the tools to add metadata to their data so that other researchers can find it in search engines. This allows researchers to maintain control over how their data is distributed while benefiting from institutional backing, such as in DataVerse. 
Our discussed data sharing protocol in \Cref{sec:sharingscience} would share many traits of the Harvard Dataverse, such as role-based access control over their data. Further, the ability to create DOIs specifically for datasets makes it much more feasible to credit researchers for their data contributions. The Dataverse shows us the importance of providing proper infrastructure and tools for researchers to handle, annotate, and share data easily.

The Ocean Protocol~\cite{ocean} is a decentralized data economy. Similar to the Dataverse, it promotes responsible data sharing by attempting to create a hub for researchers' data where they can be rewarded for their contributions. With the Ocean Protocol, one can publish their data as an NFT and then sell access tokens for their data. By tracking the usages of the data on a blockchain, researchers can be credited for their data contributions by citation while fiscally rewarding authors. Ocean Protocol's decentralized approach to data sharing promotes responsible data sharing while strongly incentivizing it, but it still fails to be the data ecosystem we seek to create. As researchers must pay for data, many will not have the resources to ``buy'' datasets. 

\section{Conclusion}

Data sharing is a vital step towards more efficient and overall better research. 
SCIENCE-index addresses several flaws of the more major indexes, such as the inability to differentiate between highly cited but low-quality papers and low-cited but high-quality papers. The SCIENCE-index also considers the impact of data sharing, which is becoming increasingly important in scientific research.
Our framework is a decentralized, self-governed, peer-to-peer data-sharing protocol that would connect distributed researchers, decrease data reproduction, and increase research productivity.
We build an ecosystem that fosters and rewards collaborations. 
However, this framework would only survive and scale if researchers were properly incentivized to participate. Our SCIENCE-index attempts to improve on current reputation-based metrics of measuring researchers, specifically the h-index, by augmenting it with data-sharing capabilities. We predict a given researcher's h-index based on 21 million other researchers and compare this to their actual h-index. This comparison gives us insight into their career progress compared to their peers. We also find that our model has a much more even spread of evaluations than the h-index when applied to geographically diverse researchers indicating that we have created a fair metric.
% 
We extend this to the scope of our data-sharing endeavors. By including data-sharing statistics as a parameter, we can reward researchers for their data sharing, thus incentivizing further data sharing. This incentive mechanism is necessary for distributed data sharing and encouraging more open science. 
This would increase the visibility of researchers who share their data and provide funding opportunities for those who share their data. While these incentives may not be perfect, they are a step in the right direction toward encouraging more data sharing in scientific research.
Our initial SCIENCE-index levels the playing field among researchers with various amounts of resources at various points in their careers. 
Finally, we assert that the SCIENCE-index and its underlying infrastructure open the door for further discussion regarding how we rate and incentivize researchers.

\medskip
\noindent\textbf{Resource Contributions:}
We contribute the SCIENCE-index as an open-source repository, including our code for data gathering, model training, and visualization. We also include the smart contract code, which persists in our model, and the rest of our decentralized application. 
Our research artifacts are shared under the Apache 2.0 license. We maintain open-source Github repositories for all our artifacts at \url{https://github.com/AI-and-Blockchain/F22_SCIENCE_Index}.

% The SSO, aforementioned in \Cref{sec:sharingscience} as another piece of our data sharing efforts, is available via a persistent URL at . 

% \begin{table}[!htbp]
% \begin{tabularx}{\linewidth}{
%     |>{\hsize=.8\hsize}X| 
%     >{\hsize=1.2\hsize}X|
%        % sum=2.0\hsize for 2 columns
%   }
% \hline
% \textbf{SCIENCE-index Repository} &  \url{https://github.com/AI-and-Blockchain/F22_SCIENCE_Index}\\ \hline
% \textbf{Sharing Science Ontology (SSO)} &  \url{https://github.com/sharing-science/sharing-science-ontology}\\ \hline
% \textbf{SSO Resource Website}  & \url{https://sharing-science.github.io/academic-incentive-ontology/ontology.html}\\ \hline
% \end{tabularx}
% \caption{Links to Resources}
% \label{tbl:resources}
% \end{table}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}
\end{document}

