\section{Introduction} 
The end of Dennard scaling law --- which stipulated a continuous increase in processor clock frequency by transistor miniaturization --- in conjunction with the continuation of Moore’s law --- which expects the number of CMOS transistors within a microchip to double every two years --- shifted the technology trend towards parallel architectures. In the early 2000’s parallel computer system architectures focused on multi-core CPU architectures. Later the introduction of the GPGPU paradigms pivoted technology trends to heterogeneous systems composed of both multi-core CPUs and GPUs. 

This heterogeneity unveiled the challenge of \emph{software performance portability}. Software performance portability seeks to achieve equivalent performance regardless of the underlying hardware architecture using a single application implementation. Programming models, such as OmpSs~\cite{duran2011ompss}, OpenMP, Kokkos~\cite{CarterEdwards20143202}, and RAJA~\cite{hornung2014raja}, provide abstractions to hide the vendor-specific interfaces required to develop applications on all these heterogeneous parallel architectures and offer unified interfaces to express parallelism. 
Although these programming models provide a single and convenient layer to implement portable code, the performance of the same application can vary when executed on different architectures and systems. Thus, these programming models efficiently express portable code, but the application performance-portability is unspecified for application executions on different heterogeneous systems. For example, HPC programmers have found that a single version of source code, with an associated static definition of execution parameters (such as the number of threads in a GPU block), is typically insufficient to achieve high application performance on different heterogeneous systems. 

Manually identifying optimal configuration options and setting their value for each separate system is a prohibitively expensive, non-scalable approach. Thus,  today application developers automate the configuration search and tuning process by manually assembling a custom workflow of steps using some scripting languages, such as Python. While possible, this approach requires programmers to pick the right components for supporting all necessary stages of the workflow, including code generators, performance profilers, and adaptive runtimes. 
To summarize, the application developers port algorithms to use performance portable programming models --- allowing the algorithms to execute in any supported architecture --- and in a later step, the developers use custom scripts to specialize the algorithm for some specific architecture. The entire process is a paradox.

We argue that future performance portability programming models should provide programming language constructs to allow developers to tune their code based on specified configuration options. The software stack (compiler, runtime libraries, and operating system) should automatically and transparently adapt their execution and select optimal configurations while requiring minimal information from the developer. We envision the usage of machine learning techniques to guide the selection of parameters.  Given the complex interactions between applications and platforms, a black-box approach using machine learning-driven runtime adaptation is more practical to guide the optimal selection and configuration of code variants, compared to an alternative white-box approach trying to understand the internals of software and hardware. 

In this paper, we propose a new paradigm, machine learning-driven adaptive OpenMP, to explore the design and implementation of novel programming model features needed to address the adaptation challenges. We argue that future programming models, including OpenMP, should allow programmers to express rich semantics related to automated runtime adaptation using machine learning techniques.
The resulting programming systems will significantly reduce users' efforts of programming on heterogeneous architectures, leveraging machine learning techniques, to achieve performance portability and improve computing efficiency of HPC computing systems. 
The contributions of our work are:
\begin{itemize}
\item A set of new OpenMP directives and clauses to express essential semantics for automated model-driven runtime adaptation of user-selected OpenMP regions. Our novel design proposes a producer-consumer pattern for adaptation that composes both with existing OpenMP adaptation and exposes adaptation directly in user-code;
\item Combined compiler transformations and runtime support for automated multi-variant code generation, transparent runtime profiling and model building, for autonomous model-guided adaptation of adaptive OpenMP regions;  
\item  A complete reference implementation\cite{anonymized_github}, built on top of Clang/LLVM and an adaptive runtime system connected to a machine learning library, freely available.
\item A detailed evaluation of our approach using a range of HPC benchmarks and proxy applications 
 presenting three different use-cases, running on multiple heterogeneous systems that consist of CPU and GPU processing capabilities. 
\end{itemize}

Our experiments demonstrate that the proposed adaptive OpenMP automatically chooses the best performing code variants and configurations in several heterogeneous platforms of different adaptation possibilities, using automatically built decision tree models at runtime.
Notably, our work is the first to present a fully specified adaptive OpenMP solution, using machine learning driven adaptation, with robust compile/runtime support and wide applicability on multiple use-cases.

