\section{Evaluation}
\label{sec:evaluation}
\begin{table}[!tp]
    \centering
    \caption{Overhead of adaptive execution through Apollo}
    \footnotesize
    \input{tables/overheads}
    \label{tab:overheads}
\end{table}

\subsection{Adaptive execution overheads}.
Table~\ref{tab:overheads} presents the overheads of adaptive execution through Apollo on each different hardware platform.
There are three sources of overhead:
\begin{inparaenum}[(1)]
\item instrumentation overhead for collecting profiling data per region execution,
\item the one-off overhead of training a model for each region after profiling data are collected, and
\item the overhead evaluating the adaptation model, either when exploring variants or selecting a tuned variant after model training, per each region execution.
\end{inparaenum}
For model training and inference, overhead measurements assume a decision tree model of maximum depth 2 built on 2 distinct features and 10 unique tuples of profiling data, which is the most complex modeling encountered in our use-case experiments, hence they present an upper limit.
Those overheads, of microsecond and nanosecond ranges, are very modest given execution times of regions in realistic applications are typically considerably higher.
Application performance measurements, following, exclude data collection during exploration, but include instrumentation, model training and inference overheads of adaptive execution once training data have been collected.

\begin{figure*}[!tp]
    \centering
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/EvaluationFigs/LULESH_speedup-crop.pdf}
        \caption{LULESH}
        \label{fig:cpu_gpu:lulesh}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/EvaluationFigs/amgmk_speedup-crop.pdf}
        \caption{AMGMk}
        \label{fig:cpu_gpu:amgmk}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/EvaluationFigs/XSBench_speedup-crop.pdf}
        \caption{XSBench}
        \label{fig:cpu_gpu:xsbench}
    \end{subfigure}
    \caption{Results on CPU-GPU execution adaptation.}
    \label{fig:cpu_gpu_results}
\end{figure*}


\begin{figure*}[!tp]
    \centering
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/EvaluationFigs/LULESH_heatmap-crop.pdf}
        \caption{LULESH}
        \label{fig:cpu_gpu:lulesh_heatmap}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/EvaluationFigs/amgmk_heatmap-crop.pdf}
        \caption{AMGMk}
        \label{fig:cpu_gpu:amgmk_heatmap}
    \end{subfigure}
    \begin{subfigure}{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/EvaluationFigs/XSBench_heatmap-crop.pdf}
        \caption{XSBench}
        \label{fig:cpu_gpu:xsbench_heatmap}
    \end{subfigure}
    \caption{The percentage of experiments each model decided to execute on the GPU. X-axis shows feature values.}
    \label{fig:cpu_gpu_results_heatmap}
\end{figure*}

\begin{figure}[!t]
\centering

\begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{figures/EvaluationFigs/fluidSim_threads_speedup-crop.pdf}
    \caption{FluidSim}
    \label{fig:threads:fluidsim}
\end{subfigure}

\begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{figures/EvaluationFigs/heat_threads_speedup-crop.pdf}
    \caption{Heat}
    \label{fig:threads:heat}
\end{subfigure}

\begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{figures/EvaluationFigs/LULESH_threads_speedup-crop.pdf}
    \caption{LULESH}
    \label{fig:threads:lulesh}
\end{subfigure}

\begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{figures/EvaluationFigs/amgmk_threads_speedup-crop.pdf}
    \caption{AMGMk}
    \label{fig:threads:amgmk}
\end{subfigure}
\caption{Results on GPU thread adaptation.}
\label{fig:threads}
\end{figure}

\begin{figure}[!t]
\centering
\begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{figures/EvaluationFigs/XSBench_coexec_speedup-crop.pdf}
\end{subfigure}

\begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{figures/EvaluationFigs/XSBench_violin-crop.pdf}
\end{subfigure}
\caption{XSBench. Results on CPU-GPU co-scheduling (top). Violin plots (bottom) show distributions of model decisions.}
\label{fig:cosched:xsbench}
\end{figure}

\begin{figure}[!t]
\centering
\begin{subfigure}[t]{.66\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{figures/EvaluationFigs/RSBench_coexec_speedup-crop.pdf}
\end{subfigure}

\begin{subfigure}[t]{.66\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{figures/EvaluationFigs/RSBench_violin-crop.pdf}
\end{subfigure}
    \caption{RSBench. Results on CPU-GPU co-scheduling (left). Violin plots (right) show distributions of model decisions.}
\label{fig:cosched:rsbench}
\end{figure}


\subsection{Selecting CPU-GPU execution.} 
Figure~\ref{fig:cpu_gpu_results} presents results on programs \emph{LULESH}, \emph{AMGMk}, and \emph{XSBench}.
A first observation across all benchmarks, looking at the \emph{Static,CPU} results, is that CPU execution is typically faster only for smaller inputs, which fail to utilize the ample parallelism on the GPU.
The exact point when CPU execution becomes slower than GPU is machine dependent, which motivates the need for dynamic adaptation.
For example, \emph{LULESH} is faster when the input domain size is <=60 for Power9+V100 but on the Intel+P100 CPU it is faster only for the smallest input 50.
Notably, on AMD+MI50, CPU execution is always faster; this attributes to the immaturity of the OpenMP runtime implementation for AMD GPUs that suffers from sub-optimal data transfer mechanisms across the host and accelerator device required by the iterative algorithm.
Nevertheless, adaptive models picks that up and predict the CPU as the fastest device for execution.
Observations are similar for the \emph{AMGMk} kernel and the proxy application \emph{XSBench}, though AMD GPU execution is indeed faster for larger inputs avoiding the pathological problems of \emph{LULESH}.
The achievable speedup from dynamically choosing CPU execution depends on the particular input and the machine architecture:
\emph{LULESH} achieves up to 1.25$\times$ speedup on Power9 for smaller inputs, up to 1.1$\times$ on Intel; \emph{AMGMk} achieves up to 1.4$\times$ speedup on Power9, up to 2$\times$ on Intel, up to 4$\times$ on AMD; and \emph{XSBench} achieves up to 3$\times$, 4$\times$, and 2$\times$ respectively.

Results show that \emph{Adaptive-100} always tracks the fastest configuration and correctly selects to execute on the CPU or GPU, with speedup close to \emph{Static,CPU}.
Results on the adaptive models built with subsets of inputs (\emph{Adaptive-25}, \emph{Adaptive-50}, \emph{Adaptive-75}) show that, on average, they also track the fastest configuration.
They never fully falter to select CPU execution for larger inputs, while achieving speedup for smaller inputs by selecting CPU execution.
The accuracy in selecting the fastest configuration and achievable speedup increases as more data are included in the training fold, hence \emph{Adaptive-75} comes close to the performance of \emph{Adaptive-100}.
Nevertheless, even the smallest subsets in \emph{Adaptive-25} achieve speedup despite the limited training data.
Figure~\ref{fig:cpu_gpu_results_heatmap} presents the percentage of model decisions to execute on the GPU for different adaptive models and feature inputs, which corroborate those findings.

\subsection{Selecting the number of threads for GPU execution.}
Figure~\ref{fig:threads} shows results for adaptation on GPU thread configurations for various programs and systems.
The baseline for the speedup calculation is the execution with 256 GPU threads, which is the default value of the original implementation, heuristically deemed good across the board.
Results validate this heuristic, as speedup from thread adaptation is modest: in many cases speedup is marginal and the maximum observed speedup is never more than 1.4$\times$.
Nevertheless, the optimal choice of threads is program, input, and machine dependent.

Commenting more, \emph{FluidSim} shown in Figure~\ref{fig:threads:fluidsim}, speeds up from thread adaption on Intel+P100 and AMD+MI50, up to 1.1$\times$ and 1.15$\times$ respectively.
\emph{Heat} shows speedup on Power9+V100 and AMD+MI50, at most 1.2$\times$ for both machines.
\emph{LULESH} achieves speedup across all machines of about 1.1$\times$, with an outlier on AMD+MI50 for the specific input size of 70 which accelerates about 1.4$\times$.
Thread adaptation for \emph{AMGMk} achieves speedup only on Power9+V100 and Intel+P100, up to 1.25$\times$ and 1.15$\times$ respectively.

Interestingly, \emph{Static,Best}, which selects the same thread configuration for all regions, does not depend on input but rather on the platform and application.
It performs on par or better compared to the default configuration of 256 GPU threads.
For larger inputs, adaptive models in \emph{Adaptive-100} and \emph{Online} mostly follow the \emph{Static,Best}, achieving similar speedup, although they may be deciding a different thread configuration per region.
In certain cases, \emph{Online}, which is built with profiling data within a single-run of the application with a specific input, exceeds the performance of \emph{Adaptive-100}, which is built using profiling data across application runs.
In those case, we observe that the specialized model for specific features inputs built by \emph{Online} produces better thread configurations compared to the generalized model built by \emph{Adaptive-100}.

\subsection{Co-scheduling execution on CPU and GPU.}
Figures~\ref{fig:cosched:xsbench},~\ref{fig:cosched:rsbench} show speedup and model decision distributions on
partitioning computation across the CPU and GPU for \emph{XSBench} and \emph{RSBench}, with the GPU-only execution as the baseline.
Results for \emph{RSBench} on the AMD+MI50 system are missing due to a compilation error in the AMD GPU backend of LLVM.
Possible speedup again varies depending the program, its input, and the deployed system, justifying the need for adaptation.

Specifically, for \emph{XSBench} (Figure~\ref{fig:cosched:xsbench}), speedup on Power9+V100 is modest, achievable only on larger inputs, with a maximum speedup of 1.1$\times$ observed at the largest input size.
By contrast, \emph{XSBench} running on the Intel+P100 consistently benefits from co-scheduling, starting with a speedup of 1.02$\times$ for the smallest input and steadily increasing on larger inputs to achieve a speedup of about 1.25$\times$ for the largest input size.
Co-scheduling on AMD+MI50 achieves marginal speedup on larger inputs.
Regarding \emph{RSBench} (Figure~\ref{fig:cosched:rsbench}), co-scheduling achieves speedup on all inputs, up to a maximum of about 1.14$\times$ speedup on Power9+V100 and 1.5$\times$ on Intel+P100 for the largest inputs.

The \emph{Adaptive-100} and data-subset folds (\emph{Adaptive-25}, \emph{Adaptive-50}, \emph{Adaptive-75}) achieve similar speedup across both programs.
Interestingly, \emph{Adaptive-25}, which trains with only 25\% of the data, achieves comparable performance.
The violin plots of figures~\ref{fig:cosched:xsbench},~\ref{fig:cosched:rsbench} show the distribution of decisions per model to provide more insight on partitioning decisions of the different models.
Adaptive models with incomplete training data make predictions in the neighborhood of the \emph{Adaptive-100} model, hence resulting speedup is comparable.

