\section{Adaptive OpenMP: Design and Implementation}
\label{sec:approach}

\begin{figure}[!t]
\centering
\includegraphics[width=.75\columnwidth]{figures/overview_v2.pdf}
\caption{Machine Learning-Driven Adaptive OpenMP}
\label{fig:overview}
\end{figure}

Figure~\ref{fig:overview} shows an overview of our proposed approach for extending OpenMP to enable machine learning-driven adaptation.
Our extensions include a new OpenMP construct, \code{adaptation}, for defining adaptive regions, and interfaces for embedding adaptation on other OpenMP constructs.
The compiler generates all necessary code for implementing adaptive execution, including code variants, the execution pipeline for profiling, model building, and model-guided adaptation, supported through a runtime library.

A generated, adaptive executable file goes through a lifecycle of adaptation.
Running it collects profiling data for annotated adaptive regions to assess variants of execution.
Once sufficient data are collected, directed by the user or autonomously, the executable automatically builds a predictive machine learning model for each adaptive code region to select per-region optimizing variants.

Users define adaptive regions customized to their application and execution target platforms, by providing a set of features that capture input-/data-dependent possibilities for adaptation and possible variants.
Also, they are in control of determining what kind of machine model to train and how many profiling data to collect for training.
Executing an adaptive program transparently collects profiling data, while performing its normal computation, to be used for training adaptive machine learning models and those data accumulate in persistent storage across runs to capture the application's performance.
Once sufficient, by user direction, training data are collected, adaptive OpenMP trains an adaptive model for the region to guide variant selection.
It is possible to collect sufficient training data within a single run of a program, typically for iterative computations, to train and use an adaptive model within this single run, which is made available for later runs too.
Otherwise, adaptive OpenMP will use accumulated data across runs to train a model.

\subsection{OpenMP Adaptation Extensions}
We propose a new OpenMP construct, \code{adaptation} to express semantics for automatic, machine learning-driven adaptation at runtime.
The adaptation directive is a stand-alone directive specified either with a begin/end pair or a single directive over a structured block of code.
It demarcates user code that constitutes the adaptive region of execution to be profiled at runtime for data collection.

\textbf{Syntax and semantics}.
The \code{adaptation} directive has the following syntax:
\begin{minted}[fontsize=\small,escapeinside=||,bgcolor=gray!15]{text}
|\textbf{#pragma omp begin adaptation [clause[[,]clause]...] newline}|
stmt(s)
|\textbf{#pragma omp end adaptation [clause] newline}|
|\textit{or}|
|\textbf{#pragma omp adaptation [clause[[,]clause]...] newline}|
structured-block
\end{minted}

The associated clauses to specify the adaptation model are:
\begin{itemize}
    \item \code{model_name(unique-id)} (required, unique)
    \item \code{features(features-list)} (optional, unique)
    \item \code{variants(variants-list)} (required, unique)
    \item \code{min_train_data(int-expr)} (optional, unique)
    \item \code{model_type(kind-and-params)} (optional, unique)
    \item \code{model_output(identifier)} (optional, unique)
\end{itemize}

In detail, the clause \code{model_name} assigns a unique, user-defined identifier to refer to the adaptation model.

The clause \code{features} takes as argument a list of base language identifiers, corresponding to program variables in scope, as input features to the adaptation model.
The region itself is an implicit feature since models are built per region.

The clause \code{variants} takes as argument a list of variants.
Variants in the list can be user-defined, categorical identifiers, or numerical expressions, including 
a range specification syntax for defining ranges of possible numerical variants.
The range specification is of the form \code{[BEGIN:END:STEP]} and generates a range of variants starting with the numerical value \code{BEGIN} until \code{END} (inclusively) in \code{STEP} increments.
Values \code{BEGIN}, \code{END}, \code{STEP} must be constants.
For example, \code{variants([10:20:2])} is equivalent to specifying \code{variants(10, 12, 14, 16, 18, 20)}.
Evaluating the adaptation model selects a single variant.

The clause \code{min_train_data} specifies the minimum number of data that must be collected for training.
If it is unspecified, the default setting is implementation defined.
In our implementation, the minimum is equal to the number of available variants.

The clause \code{model_type} takes as argument the model kind and its parameters, which specify the machine learning method to use for building the adaptation model.
If it is unspecified, the model type and parameters are implementation defined.
In our implementation, possible kinds are \code{dtree} and \code{rfc}, which  correspond to a decision tree classifier and a random forest classifier respectively.
This design is extensible to other machine learning classifiers, such as Neural Networks or Support Vector Machines (SVMs).
The kind \code{dtree} has an optional integer expression as a single parameter for the maximum tree depth, for example \code{model_type(dtree, 4)}.
If there is no argument, the maximum tree depth is implementation defined.
The kind \code{rfc} has two integer expressions as parameters, for example \code{model_type(rfc, 10, 4)}. 
The first specifies the number of decision trees in the forest while the second specifies the maximum tree depth for those trees.
Either both arguments must be defined or none; if none is specified then both parameters are implementation defined.
In our implementation the default model type is a decision tree of depth 2, found by prior work~\cite{wood2021artemis,LiaoExtending2021} to provide effective adaptive models.

The clause \code{model_output} takes as a parameter a base language identifier to store the numerical value of the variant selected at runtime.
In case variants are specified using categorical identifiers, the numeric value in the base language identifier enumerates starting from 0 those categorical in the order they were specified.
The end component of the \code{adaptation} directive has a single clause, \code{model_name}, which associates this directive to its begin counterpart, supporting nesting of adaptive regions.

\textbf{Usage of adaptation}.
The design supports using the model defined in the adaptation directive either directly in user code by implementing execution variants depending on the value store in the identifier provided in \code{model_output}, or by using the model unique identifier with \code{metadirective} directives.
We extend \code{metadirective} for a \code{user} trait set with an \code{adaptation} trait selector of the following syntax:

\begin{minted}[fontsize=\small,escapeinside=||,bgcolor=gray!15]{text}
|\textbf{when (user={adaptation(trait-property-expression)}):}| \
  [directive-variant])
\end{minted}

Similar to the \code{condition} trait selector in OpenMP for dynamic user-defined conditions, the \code{adaptation} selector expects a boolean expression, resolved at runtime, that evaluates a model, specified using the identifier in the \code{model_name} clause of an adaptation directive.
The \code{model_name} identifier in the expression serves as a proxy to the specific variant choice made for the associated adaptation model at the declaration of the adaptation directive, with the particular set of feature values of this execution.
Relational operators operators (\code{==, !=, <, <=, >=, >}) evaluate to true when the model variant choice matches the implied or explicit ordering imposed by the definition order of categorical or numerical variant identifiers.
Boolean expressions on different \code{when} clauses should be disjoint, but if not, the superset condition that evaluates to true takes precedence, following the existing OpenMP specification rules.

Conceptually,
the \code{adaptation} directive is a producer of a dynamic variant choice, dependent on the specified input features of a particular code region.
Consumers of this choice are \code{metadirective} directives, to select a directive variant at runtime, and user code that directly uses the numerical expression of this choice.
This design gives ample flexibility in implementing a variety of adaptation choices,
including alternative code versions depending on the model's output.
We describe three example use-cases next.

\begin{listing}[t]
\begin{minipage}[t]{0.47\columnwidth}
\input{figures/adaptive_example}
\end{minipage}
\begin{minipage}{0.01\columnwidth}
\hspace{0.08\columnwidth}
\end{minipage}
\begin{minipage}[t]{0.47\columnwidth}
\input{figures/adaptive_example_apollo}
\end{minipage}
\caption{Vector addition adaptation selecting optimal device execution (CPU or GPU) using adaptive OpenMP (left) vs. Apollo (right). Matching colors show regions of corresponding functionality.}
\label{fig:example_cpu_gpu}
\end{listing}



\begin{listing}[t]
\begin{minipage}[t]{0.47\columnwidth}
\input{figures/adaptive_example_threads}
\end{minipage}
\begin{minipage}{0.01\columnwidth}
\hspace{0.08\columnwidth}
\end{minipage}
\begin{minipage}[t]{0.47\columnwidth}
\input{figures/adaptive_example_threads_apollo}
\end{minipage}
\caption{Vector addition adaptation for selecting the number of GPU threads per team using adaptive OpenMP(left) versus Apollo (right). Matching colors correspond to code regions of similar functionality.}
\label{fig:example_gpu_threads}
\end{listing}


\begin{listing}[t]
\begin{minipage}[t]{0.47\columnwidth}
\input{figures/adaptive_example_cosched}
\end{minipage}
\begin{minipage}{0.01\columnwidth}
\hspace{0.08\columnwidth}
\end{minipage}
\begin{minipage}[t]{0.47\columnwidth}
\input{figures/adaptive_example_cosched_apollo}
\end{minipage}
\caption{Vector addition adaptation partitioning work for CPU-GPU co-scheduling using adaptive OpenMP (left) versus Apollo (right). Matching colors correspond to code regions of similar functionality.}
\label{fig:example_cosched}
\end{listing}

\subsection{Examples} 
Listings~\ref{fig:example_cpu_gpu},~\ref{fig:example_gpu_threads}, and ~\ref{fig:example_cosched} use as a simple example the source code of the vector addition kernel and contrast distinct adaptation scenarios using adaptive OpenMP on the left side and an equivalent  Apollo implementation on the right side.
Our design supports all of those adaptation possibilities, which attests to its flexibility, and those are the actual adaptation choices we evaluate on a set of HPC proxy/mini applications.
We highlight corresponding code lines in the listing with the same color across both implementations to
emphasize the different actions required by both approaches to perform adaptation:
{\sethlcolor{celadon} \hl{{green}}}, highlights the instantiation/declaration of a model;
{\sethlcolor{beaublue} \hl{blue}, highlights using the adaptation model decision;
{\sethlcolor{lightgray} \hl{gray}, highlights the adapted algorithm;
{\sethlcolor{bubblegum} \hl{red}, highlights the model destruction.

\subsubsection{Selection of optimal execution device}
Listing~\ref{fig:example_cpu_gpu} shows an example adaptive execution that selects whether to run a kernel on the CPU or GPU.
In the adaptive OpenMP version, Listing~\ref{fig:example_cpu_gpu} (left), the lines~\ref{lst3:adapt:begin}--\ref{lst3:adapt:end} contain the \code{begin adaptation} directive that starts an adaptive region named \code{by_len}, with the single feature \code{N} for model building, equal to the input vector size.
The user-defined variants \code{cpu} and \code{gpu} semantically correspond to CPU or GPU execution and there is no specification for the model or minimum number of training, thus those resolve to implementation defaults.
Line~\ref{lst3:adaptend} contains the \code{end adaptation} counterpart which demarcates the end of this adaptive region.
Lines~\ref{lst3:meta2:begin}--\ref{lst3:meta2:end} contain the metadirective that selects whether the computation will execute on the host CPU or the GPU device.

The proposed adaptive OpenMP avoids the verbosity of Apollo's interface.
Moreover, it removes hard-to-maintain code duplication by re-using a single implementation of the computation's for-loop. 
The composition of the model decision with the extended \code{metadirective} is a natural fit for a producer-consumer paradigm. 
The model produces a selection which the \texttt{metadirective} consumes to decide the execution variant.

\subsubsection{Selection of optimal number of GPU threads per team}
Listing~\ref{fig:example_gpu_threads} presents a use-case for dynamically selecting the number of GPU threads per team contrasting the proposed adaptive OpenMP approach with Apollo.
In adaptive OpenMP, Listing~\ref{fig:example_gpu_threads}~(left), the user defines a range of numerical variants in the adaptation directive at lines~\ref{lst4:adapt:begin}--\ref{lst4:adapt:end} and provides a program variable, \code{GPU_THREADS}, to store the output of the model's evaluation.
This variable sets the number of threads in the executable directive of the target region at lines~\ref{lst4:omp:begin}--\ref{lst4:omp:end}.
The user requests a decision tree (default tree depth) and the minimum training data equal to 10 times the number of possible variants.
The model will explore different variants for data collection, or to the variant that optimizes execution performance after training.

By contrast to adaptive OpenMP, the Apollo implementation, Listing~\ref{fig:example_gpu_threads}~(right), besides instrumentation verbosity, requires manually implementing the logic of translating Apollo policy indices back to the possible number of thread choices.
Adaptive OpenMP avoids this complexity by leveraging compiler extensions for code generation, so the user concisely defines and uses the adaptation model.

\subsubsection{Co-Scheduling GPU and CPU execution}
Listing~\ref{fig:example_cosched} presents a more complicated use-case where the user partitions computation across both the host CPU and the GPU device to concurrently utilize them.
Adaptation dynamically optimize work distribution between the devices.
Using adaptive OpenMP, shown in Listing~\ref{fig:example_cosched}~(left), the \code{begin adaptation} directive in lines~\ref{lst5:adapt:begin}--\ref{lst5:adapt:end} specifies the model, where the variants are numerical expressions that represent the fraction of computation to execute on the GPU device.
The user specifies the program variable \code{GPU_FRAC} to store the variant value chosen by the model.
The device data environment directives at lines~\ref{lst5:enter:begin}--\ref{lst5:enter:end},~\ref{lst5:exit} are parametric to the fraction of work assigned to the GPU for (un-)mapping data.
Lines~\ref{lst5:gpu:begin}--\ref{lst5:gpu:end} contain the GPU computation, adjusting the loop bounds based on the variant evaluated by the model, and similarly lines~\ref{lst5:cpu:begin}--\ref{lst5:cpu:end} contain the CPU computation.
Note that for co-scheduling, the target region is marked as \code{nowait} to execute concurrently with the parallel region on the host, while \code{taskwait} at line~\ref{lst5:taskwait} synchronizes execution to ensure the target region has finished to exit the device data environment.

Contrasting with the implementation using Apollo, shown in Listing~\ref{fig:example_cosched}~(right), the adaptive OpenMP formulation is more succinct, again avoiding instrumentation and policy translation, while 
supporting similar functionality in the higher-level abstraction of OpenMP.

The next section discusses the compiler extensions for lowering adaptive OpenMP, targeting the Apollo runtime API for the implementation.

\subsection{Compiler and Runtime Support}

We extend the mature, production-level Clang/LLVM compiler framework to implement our proposed OpenMP extensions.
Specifically, our modifications are in the Clang frontend, extending parsing and semantic analysis to ingest the new adaptation directive, its clauses, and the adaptation trait for the \code{metadirective}, to
generate AST nodes with corresponding LLVM IR code generation.
Our code generation extensions lower the AST nodes related to adaptation by emitting functions calls in LLVM IR to the C bindings of the Apollo runtime API, including global state (variables) needed for adaptive execution, and the control logic to dynamically select the variant of choice to execute.

In details, the \code{begin adaptation} declaration emits a global variable to store the region handler pointer returned by Apollo, named \code{__omp_adaptation_name_<model name>} and initialized to \code{null}.
It also emits global variables that translate the categorical variant parameters to integer values in the form of \code{__omp_adaptation_<model name>_<variant>=<integer>} setting the leftmost variant to 0 and incrementing for the rest.
Lastly, it emits a global variable named \code{__omp_adaptation_policy_<model name>} to store the variant selection returned by Apollo.
The memory footprint of the state produced by the translation, assuming an 8-byte size for both pointer and integer types, amounts to $(N+2)\times 8$ bytes per region, where $N$ is the number of possible variants.

Further, \code{begin adaptation} emits a call to \code{__apollo_region_create()} to create a region and store the returned pointer to the handler global, only when the handler is \code{null}.
Following, it emits a call to \code{__apollo_region_begin()} to begin the region's context and possibly multiple calls to \code{__apollo_region_set_feature} for providing the values of features specified in the \code{features} clause.
Lastly, it emits a call to \code{__apollo_region_get_policy()} to get the variant selection of Apollo and store it in the related global variable and any user variable defined in \code{model_output}.

The \code{end adaptation} declaration emits a call to \code{__apollo_region_end()}, so that Apollo collects internal profiling data measurements and triggers training of an optimizing variant selection model when the minimum training data requirement is met, as set by the user or by implementation defaults.
