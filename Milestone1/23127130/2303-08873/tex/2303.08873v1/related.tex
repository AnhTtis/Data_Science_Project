\section{Related Work}
\label{sec:related}

Researchers have extensively studied machine learning techniques to guide compiler optimizations for decades. A comprehensive survey was given by Wang et al.~\cite{wang2018machine}. 
Ashouri et al.~\cite{ashouri2018survey} surveyed machine learning techniques used to solve optimization selection and phase-ordering problems for compilers.  
The Milepost GCC project~\cite{fursin2011milepost} combines GCC with machine learning to predict profitable optimizations to adapt to different architectures.  
For optimal splitting ratio between CPU and GPU computation, Luk et al.~\cite{luk2009qilin} profile execution variants to build linear regression models. 

Machine learning is also a popular approach to help the selection decision of CPU vs. GPU for applications.
Grewe et al.~\cite{grewe2013portable} used decision tree models to decide if it is profitable to run OpenCL kernels on GPUs. 
Hayashi et al.~\cite{hayashi2015machine} used offline, supervised machine-learning techniques to select preferred computing resources between CPUs and  GPUs for individual Java kernels using a JIT compiler. 
To avoid manual defining model features, DeepTune~\cite{cummins2017end} directly takes raw source code as input to develop a deep neural network to guide optimal mapping for OpenCL programs. 

There is a large body of research focusing on auto-tuning techniques.
Typical examples include ATLAS~\cite{whaley1998automatically}, Active Harmony~\cite{ActiveHarmony:Tapus:SC:2002}, FFTW~\cite{frigo2005design}, POET~\cite{yi2007poet}, CHILL~\cite{chen2008chill}.
OpenTuner~\cite{ansel2014opentuner} provides a general-purpose framework for building domain-specific multi-objective program auto-tuners. It allows the use of ensembles of different search techniques. 
CLTune~\cite{nugteren2015cltune} is  a generic auto-tuner for OpenCL kernels, supporting user-defined search space of possible parameter value combinations. 
Bliss~\cite{10.1145/3453483.3454109} proposes probabilistic Bayesian optimization to tune hardware (core frequency, hyperthread) and software execution parameters (OpenMP threads, algorithmic alternatives) for the whole application, curated by the user.
ANGEL~\cite{chen2015angel} uses a hierarchical method to enable online tuning of  multiple objectives, such as balancing the trade-off between execution time and power consumption. 
Bari et al. in~\cite{ARCS:Bari:CLUSTER:2016} present ARCS framework for tuning OpenMP program targeting on optimizing power consumption.

Given the popularity of OpenMP in HPC, there is growing interest in tuning OpenMP programs for heterogeneous platforms.  One early example is the source code outlining technique developed by Liao et al.~\cite{liao2009effective} to enable autotuning of OpenMP loops extracted from large applications. 
Sreenivasan et al. ~\cite{sreenivasan2019framework} proposed a lightweight OpenMP pragma autotuner to optimize execution parameters such as scheduling policies, chunk sizes, and thread counts. 
Pennycook et al.~\cite{OMP5.0Eva:Pennycook:P3HPC:2018} used the miniMD benchmark from the Mantevo suite to study the benefits of using \code{metadirective} and \code{declare variant} introduced in OpenMP 5.0.
They confirmed that these features allowed more compact source code form to express code variants for competitive performance portability, although the features only support compile-time adaptation.


There have been several prior efforts to integrate machine-learning-based adaptation through a programming interface.
Previous work~\cite{LiaoExtending2021} attempted to enhance adaptation in OpenMP using machine learning. However, the work is preliminary since the proposed directive supports only a single adaptive code pattern associated with metadirective. The paper also only used a single kernel for evaluation. Apollo~\cite{beckingsale2017apollo,wood2021artemis} is a state-of-the-art auto-tuning library that supports user-defined tunable regions. Users can specify each region's input features and a set of code variants (called \emph{policies} in Apollo terminology) to express possible choices for tuning. Through a set of collaborative API calls, Apollo runtime provides programmers with the functionalities of profiling data collection, model building, and model-driven adaptation. However, significant manual code rewriting is needed to use Apollo given that it is designed to be a runtime library. 

Our work differs from the aforementioned studies in that we directly incorporate machine learning into a directive-based programming model to enable more versatile, transparent, and automated model-driven runtime adaptation at the level of fine-grain code regions within a program. 
Through the support of multiple adaptation code variants, our approach significantly enhances portability, performance and productivity of HPC programming targeting heterogeneous architectures. 
Our evaluation is also more comprehensive by using a wide range of benchmarks and a prototype implementation using a production quality compiler. 
