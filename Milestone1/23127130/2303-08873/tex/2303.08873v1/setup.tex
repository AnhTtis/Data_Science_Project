\section{Experimentation Setup}
\label{sec:setup}

\begin{table}[!tp]
\centering
\caption{Hardware and Software platforms}
\small
\resizebox{\columnwidth}{!}{
\input{tables/systems}
}
\label{tab:machines}
\end{table}

\begin{table}[!t]
\centering
\caption{Benchmark programs, including brief descriptions, tested inputs, features for modeling and tuning possibilities}
\resizebox{\columnwidth}{!}{
\input{tables/benchmarks}
}
\label{tab:benchmarks}
\end{table}

Table~\ref{tab:machines} shows the hardware and software platform of three heterogeneous systems used in the evaluation.
Following state-of-practice, we disable SMT and use thread and memory pinning for optimal performance and reduced variability.
Our implementation extends Clang/LLVM v14.0.0 (commit e08f3bf) with our OpenMP adaptive extensions and targets 
the Apollo library (\code{develop} branch commit 3b5d38e).
We note that the OpenMP offloading implementation in Clang/LLVM is mature for NVIDIA GPUs (Power9+V100, Intel+P100), but it is experimental for AMD GPUs (AMD+MI50).

Table~\ref{tab:benchmarks} lists the programs we used in our evaluation.
Those benchmarks are taken from the HeCBench benchmark suite~\cite{jin2021rodinia} that includes
a large collection of HPC benchmarks, kernels and proxy/mini-applications coded in CUDA, OpenMP offloading, SYCL, and HIP.
We modify the OpenMP offloading implementation to use our adaptive OpenMP extensions for three different adaptation possibilities:
\begin{inparaenum}[(1)]
\item select the device of execution, CPU or GPU,
\item select the number of threads per team on each adaptive region for GPU execution, and
\item select work partitioning when co-scheduling a computation to both CPU and GPU.
\end{inparaenum}
The table records which use-cases are beneficial on which benchmarks, since this is application dependent.
For example, benchmarks with irregular memory accesses are poor matches for CPU-GPU co-scheduling due to requiring costly memory updates across devices.

We use \emph{Speedup} over a baseline configuration as the metric of performance.
For our evaluation we build decision tree models of maximum depth 2 which has been shown~\cite{wood2021artemis,LiaoExtending2021} to achieve a good compromise on the trade-off of model building and evaluation at runtime.
Nevertheless, other model types and hyper-parameter choices are explorable through our work.
Each adaptation possibility includes different comparators to show the performance potential and overheads.
For each program, input, and comparator configuration we do 5 runs and report the mean speedup observed.

\subsection{CPU-GPU execution.}
The baseline is GPU execution.
We build an \emph{Adaptive-100} model that ingests profiling data from all the runs, including all inputs and variants for training, to evaluate the prediction accuracy and resulting application performance of a model that has a complete view (100\%) of the profiling data.
Additionally, we use an experimentation methodology similar to $K$-fold cross-validation to test models built from subsets of training data. Specifically, we split application inputs into $K$ equal-sized groups.
A fraction $f$ of groups is used for model training while the rest are used for testing, to evaluate the performance of adaptive execution.
By $K$-fold construction, each input appears at least once in the training set and in the testing set. 
In our experimentation, $K=4$ and we test different fraction scenarios $f=25\%, 50\%, 75\%$ of training data.
$K$-fold creation is repeated 10 times, each time shuffling the inputs, to train different models on the permuted data.
We evaluate the performance of those models on applicable testing inputs, excluding inputs used for training.
We refer to $f$ fraction scenarios as \emph{Adaptive-25, Adaptive-50} and \emph{Adaptive-75}.
Those models show the performance of adaptation when models built with a subset of inputs to collect profiling data while they are used to predict optimizing variants on unseen inputs.
For reference, we also show the performance of CPU execution, denoted as \emph{Static,CPU}.

\subsection{GPU threads.}
The baseline is GPU execution with 256 threads per team, which is the default configuration in the original OpenMP offloading implementation.
Similarly to CPU-GPU execution, we show results building an \emph{Adaptive-100} model trained on all possible data.
Those experiments perform computation entirely on the GPU, without crossing device boundaries, hence the computation data environment resides fully on the GPU.
Due to that, we also show the speedup of \emph{Online} training by letting adaptive OpenMP train a model in every single run using a minimum of training data that is equal to the number of different threading configurations (variants), leveraging iterative computation in programs to generate enough training data.
Contrasting \emph{Online} with \emph{Adaptive-100} shows the performance difference when building a specialized model in every run versus using a generalized model built using all the data points of configurations.
\emph{Static,Best} shows the best, static threading configuration across regions.

\subsection{Co-scheduling.}
The baseline is GPU only execution.
Again, we build an \emph{Adaptive-100} model trained on all possible inputs and include results of models trained on subsets of profiling data (\emph{Adaptive-25}, \emph{Adaptive-50}, and \emph{Adaptive-75}).

