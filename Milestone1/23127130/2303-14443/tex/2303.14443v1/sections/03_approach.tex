\section{Adversarial Papers}
\label{sec:approach}
\input{includes/figure-overview}

We proceed to introduce our approach for subverting the paper-reviewer assignments. To this end, we first define a threat model for our attack, and then examine challenges and required steps to control the matching.

\paragraph{Threat model.}
We consider a scenario where the adversary only modifies her submission---the \emph{adversarial paper}---to manipulate the assigned reviewers. We assume two representative classes of adversaries with varying degrees of knowledge.
%
First, we focus on \emph{white-box adversaries} with complete access to the assignment system, including the trained model and reviewer archives. This represents a very strong class of adversaries and allows us to generally study the strength as well as limitations of our attack against assignment systems.
%
Second, we study the more realistic scenario with a \emph{black-box adversary}. The adversary is assumed to have only a general knowledge about the assignment system (\ie, AutoBid is an open-source project~\cite{misc-autobid}). No access to the training data and learned model is given. In this setting, we envision adversaries that exploit public information about a conference, such as knowledge about the program committee.

\paragraph{Challenges.}
The adversary has to operate both in the problem space~\Dom and the feature space~\F. The former consists of the input objects (\eg{}, the \LaTeX{} source files of the paper); the latter contains the feature vectors that are used as inputs for the learning system. In contrast to domains like image recognition, the mapping from the problem space to the feature space is not bijective, \ie{}, there is no one-to-one correspondence between \Dom and \F. This poses a challenge for the adversary because a promising feature vector may not be mapped to a valid submission. A further obstacle is that some modifications in the problem space cannot be applied without side effects: If an adversary, for instance, adds a sentence  to include a particular word, she inevitably adds other words that change the feature vector. 

To deal with these challenges, we introduce a \emph{hybrid} optimization strategy that alternates between the feature-space and problem-space representations of the attack submission.
This optimization enables us to preserve the semantics and plausibility of the document, while at the same time gradually changing the assignment of reviewers. A general overview of our attack is outlined in Figure \ref{fig:overview}.
Second, we transfer problem-space restrictions to the feature space. In this way, we resolve restrictions in a generic manner without adjusting our problem-space transformations.

\paragraph{Attack goals.}
Given a submission $\inputpdf$, our goal is to find an adversarial paper~$\inputpdf'$ that leads to the adversary's targeted review assignment. In the feature space, we thus want to manipulate the set of assigned reviewers $\reviewersubset_\submission$. That is, we want to \emph{select} and \emph{reject} arbitrary reviewers to be included in respectively excluded from $\reviewersubset_\submission$. Formally, we define two sets $\requestedreviewers$ and $\rejectedreviewers$ and our goal is to find a vector $\modifications \in \F$ such that the modified word counts $\submission' \coloneqq \submission + \modifications$ fulfill
\begin{equation}
\label{eq:feature-space-attack-goal}
\begin{split}
    \reviewer \in \requestedreviewers & \Rightarrow r \in \reviewersubset_{\submission'} \text{, and} \\
    r \in \rejectedreviewers & \Rightarrow r \notin \reviewersubset_{\submission'} , \forall \reviewer \in \reviewersset .
\end{split}
\end{equation}
We require every reviewer $\reviewer \in \requestedreviewers$ is included in $\reviewersubset_{\submission'}$ and likewise every reviewer $\reviewer \in \rejectedreviewers$ excluded from~$\reviewersubset_{\submission'}$. 
In addition, we take care that the targeted solution is feasible with $ \left| \requestedreviewers \right| \leq \paperload$ and $ \left| \rejectedreviewers \right|  \leq \left| \reviewersset  \right| - \paperload$.

%
Furthermore, we restrict the modifications to $\modificationsmannorm \leq \maxmannorm$ and $\modificationsinfnorm \leq \maxinfnorm$. The $L_1$ constraint limits the amount of modifications to the submissions and makes the attack less suspicious. Similarly, the $L_\infty$ constraint restricts the maximum change to a single feature, so that a word is not included too frequently. 
%
Finally, with respect to the concrete assignment process, we assume an automatic matching that always selects the reviewers with the highest assignment scores. We note that this assumption can be relaxed, as shown by \citet{jecmen-20-mitigating}, and combined with colluding reviewers. We further discuss the impact of concurring submissions in Section \ref{sec:discussion}.

For manipulations in the problem space, we design various transformations for adapting the submission~\inputpdf. We denote a single transformation by $\transformation : \Dom \longrightarrow \Dom$,$\;$ $\inputpdf \mapsto \inputpdf'$, where multiple transformations can be chained together as $\transformations = \transformation_k \circ \dots \circ \transformation_2 \circ \transformation_1$. To avoid transformations from creating artifacts and visible clues, we introduce the following problem-space constraints:
%
First, we need to \emph{preserve the semantics} of the text, so that the paper is still a meaningful submission. Second, we add a \emph{plausibility} constraint, that is, the modifications should be as inconspicuous as possible. 
We summarize the constraints as $\Upsilon$ and write \mbox{$\transformations(\inputpdf) \models \Upsilon$} if a transformation sequence \transformations on a submission fulfills these constraints.

\paragraph{Optimization problem.}
We arrive at the following optimization problem for generating adversarial examples, integrating constraints from the problem space and the feature space:
\begin{equation}
\label{eq:attack-goal}
\begin{split}
  & r \in \requestedreviewers \Rightarrow r \in \reviewersubset_{\submission'} \text{, and} \\ 
  &  r \in \rejectedreviewers \Rightarrow r \notin \reviewersubset_{\submission'}, \forall \reviewer \in \reviewersset\\
\text{ subject to \;\; } & 
\modificationsmannorm \leq \maxmannorm \text{ and }
 \modificationsinfnorm \leq \maxinfnorm \\
& \transformations(\inputpdf) \models \Upsilon 
\end{split}
\end{equation}
with \mbox{$\submission \; = \extractor \circ \preprocessing (\inputpdf)$}, \mbox{$\submission' = \extractor \circ \preprocessing (\transformations(\inputpdf))$}, and \mbox{$\modifications = (\submission' - \submission)$}.
We proceed to realize this optimization strategy by first introducing our attack in the feature space and then in the problem space, before merging both components.

\subsection{Feature Space}
\label{sec:feature-space}

In an automatic paper-reviewer assignment system, the set of reviewers $\reviewersubset_\submission$ for a submission is determined by the computed assignment scores $\bid_{\reviewer, \submission}$ between reviewers $\reviewer$ (characterized by their archives $\archive_\reviewer$) and the submission vector $\submission$: 
\begin{equation}
    \label{eq:score}
    \bid_{\reviewer, \submission}  \coloneqq \topicextractor(\archive_\reviewer) \cdot \topicextractor(\submission)^\top
\end{equation}

To change the assignment score and thus affect the matching, we can only influence the extracted high-level features $\topicextractor(\submission)$ since $\archive_r$ is fixed for a given set of $\reviewersset$. However, even when we have full control over $\topicextractor(\submission)$, changing the relative ordering---the \emph{ranking}---between reviewers is not straightforward. For instance, suppose we have two reviewers $\reviewer_1$ and $\reviewer_2$ that share most topics (i.e., $\topicextractor(\archive_{\reviewer_1}) \approx \topicextractor(\archive_{\reviewer_2})$), adjusting $\topicextractor(\submission)$ in this case will have a similar effect on both.
In particular, if we na\"ively  try to increase the assignment score from $\reviewer_1$, we simultaneously also increase the score of $\reviewer_2$ and vice versa. Even if reviewers are not working in the same area, their topic distributions often partially overlap, as their research builds on similar principles and concepts. Hence, to modify the ranking we need to carefully maneuver the submission in the feature space.
%
This is significantly more challenging compared to attacking a classification, as we need to both attack the model's prediction while simultaneously considering effects on concurring reviewers.

Our attack is further complicated by the fact that altering the topic distribution itself is a challenging task, since we need to make changes in the latent topic space. For \ac{LDA}, this distribution $\topicextractor(\submission) = \topicdocumentdist_\submission$ is computed using a probabilistic inference procedure. Thus, typical gradient-style attacks are not applicable. Indeed, \citet{zhou-20-evalda} even show that the manipulation of this inference is \emph{NP-hard}. Moreover, \ac{LDA} typically assigns only a small weight to individual words, so an attacker is required to manipulate a comparatively large set of words for subverting the topic assignment.

To address both of these challenges, we use a stochastic beam search. For a given submission vector $\submission$, we start with an empty modification vector $\modifications$ which is extended in each iteration until we find a successful submission vector $\submission' \coloneqq \submission + \modifications$ or a maximum number of iteration $\maxitr$ is reached. During this search, we consider $B$ candidate vectors in parallel and select successors after each iteration with a probability increasing as a function of the candidates' loss. 

\paragraph{Loss.}
For our search, we define the following loss function to evaluate the quality of a submission $\submission$ in terms of the objective from Equation \ref{eq:feature-space-attack-goal} that incorporates the selection and rejection of reviewers:
%
\begin{equation}
    \loss \coloneqq \loss_{\select} + \loss_{\reject}
\end{equation}
%
For selected reviewers, the loss $\loss_{\select}$ is reduced when the assignment scores $\bid_{\hat{\reviewer}, \submission}$ increase or when the ranks of the reviewers improve (\ie{}, when reviewers ascend in the ranking):
%
\begin{equation}
\loss_{\select} \coloneqq \sum_{\hat{\reviewer} \in \requestedreviewers}{\text{rank}_\submission^{\hat{\reviewer}} \cdot (1 - \bid_{\hat{\reviewer}, \submission})}
\end{equation}
%
where $\text{rank}_\submission^{\hat{\reviewer}}$ is the rank of reviewer $\hat{\reviewer}$ for submission $\submission$.
%
Similarly, for rejected reviewers the loss $\loss_{\reject}$ is reduced when the assignment scores $\bid_{\check{\reviewer}, \submission}$ decrease:
%
\begin{equation}
\loss_{\reject} \coloneqq \sum_{\check{\reviewer} \in \rejectedreviewers}{\max(\text{rank}^{\reject}_{\submission} - \text{rank}_\submission^{\check{\reviewer}}, 0) \cdot (\bid_{\check{\reviewer}, \submission} - \bid_{\reviewer_{\reject}, \submission})}
\end{equation}
%
where $\text{rank}^{\reject}_{\submission}$ is the target rank for a rejected reviewer (\ie{}, rejected reviewer are pushed down towards this rank) and $\bid_{\reviewer_{\reject}, \submission}$ is the corresponding assignment score.
%
This loss is designed to focus on reviewers that are far off, but simultaneously allows reviewers to provide \say{room} for following reviewers, for example, when we want to move a group of reviewers upwards/downwards in the ranking.

We consider a submission vector $\submission$ successful when the objective from Equation \ref{eq:feature-space-attack-goal} is fulfilled. At this point, we are naturally just at the boundary of the underlying decision function. To make the submission vector more \emph{resilient}, we could continue to decrease the loss. However, since we already successfully ordered the reviewer (\ie{}, the ranking), we are more interested in maximizing the \emph{margin} of selected and rejected reviewers to the border of $\reviewersubset_\submission$. We denote this margin as $\margin$ and set $\loss \coloneqq -\margin$ whenever $\submission$ satisfies Equation \ref{eq:feature-space-attack-goal}. Decreasing the loss is then equivalent to maximizing $\margin$. 

\paragraph{Candidate generation.}

A key operation of the beam search is the generation of new candidate vectors. We create a successor from a given submission by adding (respectively removing) $\stepsize$ words to adjust topic distribution $\topicdocumentdist_\submission = \topicextractor(\submission)$ and ultimately the ranking of submission $\submission$.
%
To select words, we represent (broadly speaking) each reviewer by a set of \emph{predictive} words and sample words that lie in the disjunction between a target and its concurring reviewers. An example of this is shown in Figure \ref{fig:reviewer-words}.

To construct these sets, we first represent each reviewer $\reviewer$ by a  \emph{reviewer to words} distribution $\reviewerwords_\reviewer$ over vocabulary $\vocabulary$. 
Intuitively, this distribution assigns each word the probability how predictive it is for $\reviewer$. Formally, we define the probability mass function for $\reviewerwords_\reviewer$ as follows:
\begin{equation*}
\label{eq:reviewerwords}
   \reviewerwordsmass_\reviewer \colon \vocabulary \rightarrow \mathbb{R}, \quad 
   \word \mapsto 
   \frac
   {\frac{1}{\mid\topics\mid} \sum_{\topic \in \topics}{\prob(\word \mid \topic)\ \prob(\topic \mid \reviewer)}}
   {\sum_{\word\in\vocabulary}{\frac{1}{\mid\topics\mid} \sum_{\topic \in \topics}{\prob(\word \mid \topic)\ \prob(\topic \mid \reviewer)}}}
\end{equation*}

\input{includes/fiigure-reviewer-words}

Remember that each topic $\topic$ defines a distribution over $\vocabulary$ and each reviewer can be represented by $\topicextractor(\archive_\reviewer)$. $\reviewerwordsmass_\reviewer$ assigns each word the average probability over all topics $\topics$ scaled by the relevance of topic $\topic$ for reviewer $\reviewer$. 
Randomly sampling from $\reviewerwords_\reviewer$ thus yield words with a probability given as a function of their \emph{predictiveness} for $\reviewer$. In practice, $\vocabulary$ is typically large and most words are assigned with an insignificant probability. To improve performance, we therefore restrict $\reviewerwords_\reviewer$ to the $\reviewerwordsmax$ words with highest probability. We rescale the mass function to sum up to 1 so that $\reviewerwords$ forms a valid distribution.

To select $\reviewer$, we could now simply add predictive words sampled from this distribution. However, as described earlier, naively doing this will likely have unwanted side effects because of concurring reviewers.
%
To account for this, we further refine this distribution and simultaneously consider multiple reviewers. Let $\tilde{\reviewer}$ be a targeted reviewer and $\surroundingreviewers$ a set of concurring reviewers. We want to restrict $\reviewerwords_{\tilde{\reviewer}}$ to only include words that are predictive for $\tilde{\reviewer}$ but not for any reviewer in $\surroundingreviewers$. Specifically, we define $\reviewerwords^+_{\tilde{\reviewer}, \surroundingreviewers}$ with 
\[
    \reviewerwordsmass^+_{\tilde{\reviewer}, \surroundingreviewers} \colon \vocabulary \rightarrow \mathbb{R}, \quad \word \mapsto
    \begin{cases*}
        \reviewerwordsmass_{\tilde{\reviewer}}(w) &
        if \parbox[t]{5.5cm}{$\reviewerwordsmass_{\tilde{\reviewer}}(w) \neq 0 \wedge\\   
         \forall r \in \surroundingreviewers: \reviewerwordsmass_{\reviewer}(w) = 0$} \\
        0 & otherwise \\
    \end{cases*}    
\]

Subsequently, to form a valid probability mass function, we rescale $\reviewerwordsmass^+_{\tilde{\reviewer}, \surroundingreviewers}$ to sum up to 1. Note for $\surroundingreviewers = \emptyset$ it follows $\reviewerwords^+_{\tilde{\reviewer}, \surroundingreviewers} = \reviewerwords_{\tilde{\reviewer}}$. 
Sampling from $\reviewerwords^+_{\tilde{\reviewer}, \surroundingreviewers}$ only yields words that are predictive for $\tilde{\reviewer}$ but not $\surroundingreviewers$. 
%
Often we are also interested in the opposite case, i.e., words that are predictive for all reviewer in $\surroundingreviewers$ but not for $\tilde{\reviewer}$ (e.g., when we want to remove words to promote $\tilde{\reviewer}$ in the ranking). Analogous, we define  $\reviewerwords^-_{\tilde{\reviewer}, \surroundingreviewers}$ and write 
\[
    \reviewerwordsmass^-_{\tilde{\reviewer}, \surroundingreviewers} \colon \vocabulary \rightarrow \mathbb{R}, \quad \word \mapsto
    \begin{cases*}
        \frac{1}{|\surroundingreviewers|}\sum_{\reviewer \in \surroundingreviewers}{\reviewerwordsmass_{\reviewer}(w)} &
        if \parbox[t]{5.5cm}{$\reviewerwordsmass_{\tilde{\reviewer}}(w) = 0 \wedge\\   
         \forall r \in \surroundingreviewers: \reviewerwordsmass_{\reviewer}(w) \neq 0$} \\
        0 & otherwise \\
    \end{cases*}
\]

Again, we rescale $\reviewerwordsmass^-_{\tilde{\reviewer}, \surroundingreviewers}$ to sum up to 1. For $\surroundingreviewers = \emptyset$, the distribution $\reviewerwords^-_{\tilde{\reviewer}, \surroundingreviewers}$ is not well defined, as its mass function always evaluates to 0 and we thus set $\reviewerwords^-_{\tilde{\reviewer}, \surroundingreviewers} := \reviewerwords_{\tilde{\reviewer}}$. Figure \ref{fig:reviewer-words} graphically depict this construction.
%
For reviewer selection, we consider sets of concurring reviewer $\surroundingreviewers$ that are close to $\reviewer$ in the ranking. Specifically, we randomly sample $\nosuccessors$ subsets from
\begin{align*}
\surroundingreviewers \subseteq Pow(\left \{ \reviewer \mid \forall \reviewer \neq \tilde{\reviewer} \in \reviewersset: 0 \leq \text{rank}_{\tilde{\reviewer}} - \text{rank}_{\reviewer} - \revieweroffset \leq \reviewerwindow \right \})
\end{align*}
%
for a given reviewer window $\reviewerwindow$ with offset $\revieweroffset$. In other words, we exploit locality and focus on reviewer that are either before or close behind $\reviewer$ in the ranking.
%
For each subset, we create two candidates by (1) adding $\stepsize$ words from $\reviewerwords^+_{\tilde{\reviewer}, \surroundingreviewers}$ respectively (2) remove $\stepsize$ words from $\reviewerwords^-_{\tilde{\reviewer}, \surroundingreviewers}$. 
%
Reviewer rejection follows analogous with the distributions interchanged and sets sampled from
\begin{align*}
\surroundingreviewers \subseteq Pow(\left \{ \reviewer \mid \forall \reviewer \neq \tilde{\reviewer} \in \reviewersset: -\reviewerwindow \leq \text{rank}_{\tilde{\reviewer}} - \text{rank}_{\reviewer} + \revieweroffset \leq 0 \right \})
\end{align*}
%
Finally, for multiple target reviewer in $\requestedreviewers$ and $\rejectedreviewers$, we  consider the union of candidates from individual reviewer.

\subsection{Problem Space}
\label{sec:problem-space}

The result of the feature space attack is a modification vector $\modifications \in~\F$ containing the words that have to be modified in the problem space. These words must be incorporated into an actual template PDF file $\inputpdf' \in \Dom$ such that both the semantics and plausibility constraints are satisfied. 
Fortunately, the assignment system obtains a document as input and not the raw text. This provides an adversary with more capabilities and flexibility. She can carefully manipulate the text of her submission as well as exploit weak spots in the text representation or document format.

\input{includes/table-problem-space-overview-transformations}

Consequently, we divide the modifications into \emph{text-level}, \emph{encoding-level}, and \emph{format-level} transformations---sorted according to their deniability. Text-level modifications operate on the actual text, so that only targeted modifications are possible. However, the modifications are deniable if the submission raises suspicion during reviewing. Encoding-level and format-level transformations manipulate the text representation and format, respectively, and enable large modifications, but are not deniable once detected.
Table~\ref{table:problem-space-overview-transformations} lists the transformations implemented in our approach. For a detailed overview, we refer the reader to Appendix~\ref{app:problem-space-transformations}.

\paragraph{Text-level transformations.}
We begin with transformations that are based solely on changes to the visible text and applicable to any text format. As such, they cannot be readily recognized without a semantic analysis of the text.

\newcommand{\minipara}[1]{\emph{#1~}}

\minipara{(a) Reference addition.}
As the first transformation, we consider additions to the submission's bibliography. The transformation adds real references that contain the words to be added. As references, we use publications from security conferences and security-related technical reports. Our evaluation demonstrates that this transformation is very effective, while creating plausible and semantics-preserving changes to a paper. However, it introduces side effects, as not only selected words are added, but also parts of the conference names, authors, and titles. This motivates the hybrid search strategy that we outline in Section~\ref{sec:feature-problem-space}.

\minipara{(b) Synonym.}
We develop a transformation that replaces a word with a \emph{synonym}. To enhance the quality of the proposed synonyms, instead of using a general model for the English language~\mbox{\cite[\eg][]{li-19-textbugger, jin-20-bert, ren-19-generating}}, we use a security-domain specific neural embedding that we compute on a collection of 11,770~security papers. Section~\ref{sec:discussion} presents the dataset. This domain-specific model increases the quality of the synonyms, so that this transformation is also difficult to spot.

\minipara{(c) Spelling mistake.}
As a third type of text-level manipulations, we implement a spelling-mistake transformation, which is common for misleading text classifiers~\cite{gao-18-blackbox, liu-20-joint}. Here, we improve on prior work by trying to find typographical errors from a list of common misspellings~\cite{misc-mispellings} instead of introducing arbitrary mistakes.
For example, the suffix \emph{ance} is often confused with \emph{ence}, so that \say{appearance} becomes the unobtrusive misspelling \say{appearence}. If we do not find such errors, we apply a common procedure from the adversarial learning literature: We either swap two adjacent letters or delete a letter in the word~\cite{li-19-textbugger, gao-18-blackbox, liu-20-joint}. 

\minipara{(d) Language model.} Finally, we apply the large-scale unsupervised language model \mbox{OPT}~\cite{zhang-22-opt} to create text containing the words to be added. To generate security-related text, we finetune the model using the corpus of 11,770 security~papers. While the created sentences are not necessarily plausible, this transformation allows us to technically evaluate the possibility that an adversary creates new text to insert words. Given the increasing capabilities of language models, we expect the chances of creating plausible text to rise in the long run. Moreover, we assume that in practice attackers would manually polish the generated text to reduce their detection probability.

\paragraph{Encoding-level transformations.}
%
As the second class of transformations, we consider manipulations of the text encoding. These manipulations may include the substitution of characters, the application of unicode operations, or changes to the font face and color. For our implementation, we focus on \emph{homoglyph} transformation, inspired by previous works that replaces characters with visually similar counterparts~\cite{li-19-textbugger, eger-19-text}. By replacing a character with a homoglyph, we can remove selected words from the bag-of-words vector used for the topic model. 
Similarly, there are several other strategies for tampering with text encoding~\citep{boucher-22-bad}. Since these manipulations also change only the visual appearance of the text, we consider homoglyphs as a representative example of the class of encoding-level transformations.

\paragraph{Format-level transformations.}
%
As the third class of transformations, we focus on changes specific to the underlying document format, such as accessibility features, scripting support, and parsing ambiguity \citep{markwood-17-pdf}. As an example of this class of transformations, we consider \emph{hidden boxes} in the PDF format. 
Our transformation relies on accessibility support with the latex package \texttt{accsupp} to define an invisible alternative text in a hidden box associated with a word. The text extractor processes the alternate text, while PDF readers display only the original word. This discrepancy allows an attacker to add words as alternate text. Likewise, she can put an empty alternative text over a word that should be removed.

\paragraph{Improved transformations.}
In addition, we exploit the preprocessing implemented by assignment systems. 
First, we benefit from stemming, so that the transformations only need to add or delete \emph{stems} instead of words. This increases the possibilities to find suitable text manipulations. For example, an attacker can modify the words \emph{attacker} or \emph{attackable} to remove the feature~\emph{attack}, since both are reduced to the same stem.
Second, we exploit the filtering of stop words. The hidden box transformation requires sacrificing a word for defining an alternative text. As stop words are not part of the feature vector, no side effects occur if they are changed.

\subsection{Feature-Problem-Space Attack}
\label{sec:feature-problem-space}

We are now equipped with (i) a strategy to find modifications~\mbox{$\modifications \in \F$} and (ii) transformations \mbox{$\transformation \in \Dom$} to realize $\modifications$ in a paper submission.
The ultimately missing piece is an optimization strategy that brings these two components together. In general, this optimization is responsible for guiding the transformations towards the targeted assignment. 
In the following, we first present the basic principle of our applied strategy and then introduce two practical extensions.

\paragraph{Hybrid optimization strategy.}
Due to the challenges around the problem space and the feature space, we use a strategy that \emph{switches alternately} between \Dom and \F.
Figure~\ref{fig:overview} on page \pageref{fig:overview} schematically illustrates our alternating approach. For an initial submission~\inputpdf, the adversary extracts the features (step~\stepone) and performs a feature-space attack (step~\steptwo and \stepthree). As $\extractor$ is not invertible, the adversary then has to find suitable transformations in the problem space (step~\stepfour) that realize the requested modifications. This leads to a new feature vector in \F (step~\stepfive). However, this vector is shifted due to side effects and limitations of the transformations. Consequently, the adversary continues her search from this new position and repeats the process iteratively until the target is reached or the maximum number of iterations have passed.

We note that side effects are not always negative as assumed by prior work~\cite{pierazzi-20-intriguing}. In our evaluation, for example, we found that the additional words introduced by the reference transformation can further push a reviewer's rank towards the target, since the additional words may also relate to other selected reviewers, for example, due to co-authors or paper titles. However, the impact of side effects is difficult to predict in advance, so that an optimization strategy should be capable of dealing with positive as well as negative side effects.

\paragraph{Constraint mapping $\Dom \rightarrow \F$.}
Our first extension to this hybrid strategy addresses the complexity of problem-space modifications. In practice, not every requested modification from \F can be realized in~\Dom with the implemented transformations due to PDF and \LaTeX{} restrictions.
For example, in \LaTeX{}, homoglpyhs are not usable in the listing environment, while the hidden box is not applicable in captions or section titles. In general, such restrictions are difficult to predict given the large number of possible \LaTeX{} packages.
Instead of solving such shortcomings in the problem space by tediously adjusting the transformations to each special case, we resort to a more generic approach and transfer problem-space constraints back to the feature space. 
%
The transformers in \Dom first collect words that cannot be handled, which are then blocked from being sampled during candidate generation in~\F.

\paragraph{Surrogate models.}
We introduce a second extension for the black-box scenario. 
In this scenario, the adversary has no access to the victim model. Still, she can leverage public information about the program committee, collect papers from its members, and assemble a dataset similar to the original training data. This allows her to train a \emph{surrogate model} that enables preparing an adversarial paper without access to the assignment system. This strategy has been successfully used for attacks against neural networks~\cite{papernot-16-transferability}.
However, in our case, this strategy is hindered by a problem: LDA models suffer from high variance \cite{agrawal-18-what, mantyla-18-measuring}. Even if the adversary had access to the original data, she would still get different models with varying predictions. This makes it unlikely that an adversarial paper computed for one model transfers to another. 

As a remedy, we propose to use an ensemble of surrogate models to better approximate the space of possible LDA models. 
We run the attack simultaneously for multiple models until being successful against \emph{all} surrogates. To this end, we extend the feature-space attack to multiple target models: (i) we create candidates for each surrogate model independently and consider the union over all surrogates and (ii) we compute the loss as the sum of individual losses over all surrogates. 
Intuitively, this increases the robustness of an adversarial paper and, consequently, improves the success rate that the attack transfers to the unknown victim model. 
