\section{Related Work}
\label{sec:related}

Our attack touches different areas of security research. In the following, we examine related concepts and methods.

\paragraph{Adversarial learning.} A large body of work has focused on methods for creating adversarial examples that mislead learning-based systems~\cite{biggio-18-wild}. However, most of this work considers attacks in the image domain and assumes a one-to-one mapping between pixels and features. This assumption does not hold in discrete domains, leading to the notion of \emph{problem-space attacks}~\cite{pierazzi-20-intriguing, quiring-19-misleading}. Our work follows this research strand and introduces a new hybrid attack strategy for operating in both the feature space and problem space.
Furthermore, we examine weak spots in \emph{text preprocessing}, which extend the attack surface for adversarial papers. 
These findings complement prior work advocating that the security of preprocessing in machine learning needs be considered in general~\cite{quiring-20-adversarial}. 

Table~\ref{table:related-work-overview} summarizes prior work on misleading text classifiers. While we build on some insights developed in these works, text classification and paper assignment differ in substantial aspects: First, the majority of prior work focuses on untargeted attacks that aim at removing individual features. In our case, however, we have to consider a targeted attack where an adversary needs to specifically change the assignment of reviewers. Second, prior attacks often directly exploit the gradient of neural networks or compute a gradient by using word importance scores. Such gradient-style attacks are not applicable to probabilistic topic models. 

In view of these differences, our work is more related to the attack from \citet{zhou-20-evalda} which studies the manipulation of \ac{LDA}. The authors show that an evasion is \emph{NP-hard} and present an attack to promote and demote individual \ac{LDA} topics. For our manipulation, however, we need to adjust not only individual topics but the complete topic distribution as well as consider side effects with concurring reviewers. 

\input{includes/table-related-work}
\paragraph{Attacks on assignment systems.}
Finally, another strain of research has explored the robustness of paper-reviewer assignment systems. 
Most of these works are based on \emph{content-masking attacks}~\cite{markwood-17-pdf, tran-19-pdfphantom}, which use format-level transformation to exploit the discrepancy between displayed and extracted text.
More specifically, \citet{markwood-17-pdf} and \citet{tran-19-pdfphantom}, similar to our work, target the paper-reviewer assignment task.
Their attack is evaluated against Latent Semantic Indexing \cite{deerwester-90-indexing}---that is not used in real-world systems like TPMS. 
Although \citet{tran-19-pdfphantom} recognize the shortcomings of format-level transformations, they do not explore text-level transformations or the interplay between the problem space and feature space of topic models. 

Complementary to our work, a further line of research focuses on the collusion of reviewers. These works have analyzed semi-automatic paper matching systems under the assumption that malicious researchers can manipulate the paper assignment by carefully adjusting their paper biddings.
\citet{jecmen-20-mitigating} propose a probabilistic matching to decrease the probability of a malicious reviewer to be assigned to a target submission, while \citet{wu-21-making} tries to limit the disproportional influence of malicious biddings.
