\section{Discussion}
\label{sec:discussion}

Our work reveals a notable vulnerability in systems for automatic paper-reviewer assignment. 
In the following, we discuss further aspects of our findings, including limitations, defenses, and the implications and benefits of an attack.

\paragraph{Committee size.}
We simulate an automatic assignment for two large security conferences with committees composed of 120 and 165 reviewers, respectively. Considering the current trend, it is likely that these conferences will continue to grow larger. In the following, we want to understand how an increased set of concurring reviewers impacts the attack.
%
Therefore, we consider committees with 100--500 reviewers sampled from a pool of 528 reviewers (taken from USENIX and S\&P committees between 2018--2022) with a total of 11,770 papers in the underlying corpus.
%
Appendix \ref{app:committee-size} shows the number of required changes as a function of the committee size. Across the three objectives---selection, rejection, and substitution---we observe only a minor impact on the attack. The attack remains successful with a success rate between 98.00\% -- 98.92\% and the number of required modifications remains largely unaffected. For the smallest committee considered (with 100 reviewers), we observe a slightly larger uncertainty. Intuitively, in this case the assignment is more dependent on the particular choice of the committee which gets averaged out for larger committees.

\paragraph{Load balancing and conflicts.}
Our attack focuses on the manipulation of assignment scores and assumes a direct matching from PC members to submissions. For a complete end-to-end attack, an attacker would also need to take load balancing of submissions and reviewer conflicts into account. For example, the target submission might compete with another paper on the same topic and get a different assignment despite a successful manipulation of the topic model.

Unfortunately, these obstacles are hard to model, as conflicts and the other submissions are typically not known to the adversary. Instead, we can generally improve the resilience of our attack. By increasing the margin $\margin$ of the target reviewer to others, we can make a matching assignment more likely. Interestingly, in this case, conflicts can be even seen as a simplification: if a target reviewer is the top candidate among all reviewers $\reviewersset$, she is also the top candidate for only a subset of reviewers (i.e., all unconflicted reviewers).

To further understand the role of this margin, we simulate the selection of a reviewer for different values of $\margin \in \{0, 0.1, 0.2\}$ and varying numbers of concurring submissions between 200 and 1,000 (sampled from a hold-out corpus). We model the full assignment to maximize similarity subjected to load constraints as introduced in Section \ref{sec:background:assignment}. We assume $\paperload = 5$ reviews per paper and that each reviewer is assigned $\reviewerload = 10$ submissions.
%
Appendix \ref{app:load-balancing} shows the attack's success rate as a function of the number of concurring submissions. The attack remains effective but we observe a slight downward trend of its success rate. This is expected: with increasing number of submissions, there exist more similar paper that compete for a given reviewer. An attacker can account for this by (1) increasing the margin and, as the attack is undetectable (in general), an attacker could (2) further increase her chances by repeating the attack (e.g., resubmitting a rejected paper).

\paragraph{Paper corpus.}
We select \emph{accepted} papers from IEEE S\&P 2022 as basis for our evaluation. This selection leads to a potential bias, as rejected submissions are not considered. However, we do not expect any impact on our results. Papers follow a common structure, so that our transformations in \LaTeX{} are applicable in general. The feature-space algorithm works on bag-of-words vectors, which is just another representation for any paper.
In Appendix~\ref{app:generalizaton}, we test our attack with papers from the 29th USENIX Security Symposium and find no significant difference in our results.

\paragraph{Countermeasures and defenses.}
Our results show that systems based on topic models such as \ac{LDA} have relatively strong robustness towards adversarial noise. This stands in stark contrast to neural networks, where changing only a few words can already lead to a misclassification~\cite[e.g.,][]{gao-18-blackbox, li-19-textbugger}. However, our work also demonstrates that \ac{LDA}-based systems are still vulnerable to adversarial examples and there is a need for appropriate defenses. 

Unfortunately, text-level manipulations are challenging to fend off, as they can only be spotted on the semantic level. In our user study, participants often struggled to differentiate adversarial modifications from benign issues and an adversary can always \emph{manually} rewrite an adversarial paper to further reduce the detection probability.
%
Moreover, even completely machine generated text---such as done with our OPT-based transformer---is often indistinguishable from natural text \cite{brown-20-language, mink-22-deepphish}. The underlying models are evolving rapidly and current state-of-the-art models such as 
InstructGPT \cite{ouyang-22-instructgpt} 
and Galactica \cite{taylor-22-galactica} 
are now actively used for academic writing.

For encoding-level and format-level changes, however, defenses are feasible: The root cause of these manipulations is the disparity between human perception and parser-based text extraction. Thus, an effective defense needs to mimic a human reader as close as possible similar to defenses recently proposed for adversarial examples in other domains~\cite[e.g.][]{eisenhofer-21-dompteur}. 
%
To evaluate this defense, we replace the parser-based text extraction (\texttt{pdftotext}) with an optical character recognition (OCR) (\texttt{tesseract}). We observe that for the modified system the encoding-level and format-level attacks now completely fail, while the performance of the text-level attacks remains unaffected. At the same time, however, we observe a large increase in runtime. Compared to the parser-based extraction, OCR is orders of magnitude slower and needs an average time of 56\,s for a single submission compared to 0.14\,s with conventional text extraction.

Other countermeasures can be more tailored to the individual transformations: Flag usage of unusual font encodings to prevent homoglyph attacks, remove comment boxes and non-typeset pages in a preprocessing step, or automatically verify the bibliography entries using online bibliography databases.


\paragraph{Benefits and implications.}
Manipulating a submission comes with a considerable risk if the attack is detected. This can range from a desk reject over a submission ban at a specific venue to permanent damage of the authors' scientific reputation~\cite{shah-22-challenges}. 
Nevertheless, recent incidents show that academic misconduct happens. Dishonest authors, for example, leveraged synthetic texts to increase the paper output~\cite{cabanac-21-tortured}. Moreover, \emph{collusion rings} exist where authors and reviewers collaborate to accept each other's papers~\cite{littman-21-collusion}. 
Automated assignment techniques can raise the bar for dishonest collaborations considerably~\cite{leyton-22-matching}, yet our work shows that these techniques need to be implemented with care. 
Apart from collusion rings, dishonest authors can also work alone: They can try to promote an unfamiliar reviewer who might overlook paper issues and thus more likely submit a positive review. 

We believe that dishonest authors more likely risk \emph{deniable} manipulations such as a few spelling mistakes or additional references. Our evaluation shows this is sometimes already enough, for example, to promote an unfamiliar reviewer.
As the line between adversarial and benign issues in a paper is often not clear, such an attack can be hard to discover.
% 
All in all, the automatic assignment of papers enables not only manipulations that undermine the entire reviewing process, but also small-scale attacks in which assignments are tweaked by a few deniable changes. 