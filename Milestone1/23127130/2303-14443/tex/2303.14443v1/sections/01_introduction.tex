\section{Introduction}
\label{sec:introduction}

Peer review is a major pillar of academic research and the scientific publication process. Despite its well-known weaknesses, it is still an essential instrument for ensuring high-quality standards through the independent evaluation of scientific findings~\cite{soneji-22-experts, misc-nips, misc-esa}. For this evaluation, a \emph{submission} is assigned to a group of reviewers, taking into account their expertise, preferences, and potential biases. For conferences, this assignment is traditionally carried out by a program chair, while for journals, the task is performed by an editor. This mechanism has proven effective in the past, but is becoming increasingly difficult to realize as research communities grow. For example, the number of papers submitted to top-tier security conferences is increasing exponentially, reaching over 3,000 submissions in 2020. Likewise, the number of reviewers continuously grows for all major security conferences~\citep{misc-circus}.

To handle this growth, conference management tools have become indispensable in peer review. They allow reviewers to bid for submissions and support the program chair to find a good assignment based on a best-effort matching. Unfortunately, even these tools reach their limit when the number of submissions continues to grow and manual bidding becomes intractable, as for example, in the area of machine learning. Major conferences in this area regularly have over 10,000 submissions that need to be distributed among more than 7,000 reviewers~\cite{misc-neurips}. For this reason, conference management tools are increasingly extended with automatic systems for \emph{paper-reviewer assignment}~\cite{misc-autobid, charlin-13-toronto}. These systems use topic models from machine learning to assess reviewer expertise, filter submissions, and automate the assignment process. 

In this work, we show that this automation can be exploited to manipulate the assignment of reviewers. In contrast to prior work that focused on bid manipulations and reviewer collusion~\citep{jecmen-20-mitigating, wu-21-making}, our attack rests on adversarial learning. In particular, we propose an attack that adapts a given paper so that it misleads the underlying topic model. This enables us to reject and select specific reviewers from the program committee. To reach this goal, we introduce a novel optimization strategy that alternates between the feature space and problem space when adapting a paper. This optimization allows us to preserve the semantics and plausibility of the document, while carefully changing the assignment of reviewers.

Our attack consists of two alternating steps: 
%
First, we aim at misleading the topic model employed in current assignment systems~\cite{charlin-13-toronto, misc-autobid}. This model defines a latent topic space that is difficult to attack because neither gradients nor an explicit decision boundary exist. To address this problem, we develop a search algorithm for exploring the latent space and manipulating decisions in it.
%
As a counterpart, we introduce a framework for modifying papers in the problem space. This framework provides several transformations for adapting the paper's content, ranging from invisible comments to synonym replacement and generated text. These transformations enable us to preserve the paper's semantics, while gradually changing the assignment of reviewers.

To empirically evaluate the practical feasibility of our attack, we simulate the paper-reviewer assignment of the 43rd IEEE Symposium on Security and Privacy (IEEE S\&P) with the original program committee of 165 reviewers in both a \emph{black-box} and a \emph{white-box} threat scenario. As the basis for our attacks, we consider 32 original submissions that are publicly available with \LaTeX{} source code. 

Our white-box adversary achieves an alarming performance: we can successfully remove \emph{any} of the initially assigned reviewers from a submission, and even scale the attack to completely choose \emph{all} reviewers in the automated assignment process. 
In the black-box scenario, we can craft adversarial papers that transfer to an unknown target system by only using public knowledge about a conference. We achieve a success rate of up to 90\% to select a reviewer and 81\% to reject one. Furthermore, we demonstrate that the attack remains robust against variations in the training data.

Our work points to a serious problem in the current peer review process: With the application of machine learning, the process inherits vulnerabilities and becomes susceptible to new forms of manipulation.
We discuss potential defenses: (1) For the feature space, robust topic modeling may limit the attacker's capabilities and (2) for the problem space, we recommend using optical character recognition (OCR) techniques to retrieve the displayed text.
Nevertheless, these safeguards cannot completely fend off our manipulations and reviewers should be made aware of this threat.

\paragraph{Contributions.}
We make the following key contributions:
\begin{itemize}[topsep=3pt, itemsep=3pt, partopsep=3pt, parsep=3pt]
\item \emph{Attack against topic models.}  We introduce a novel attack against topic models suitable for manipulating the ranking of reviewers. The attack does not depend on the availability of gradients and explores the latent topic space through an efficient beam search.
\item \emph{Problem-space transformations.} Our attack ensures that both the semantics and plausibility of the generated adversarial papers are preserved. This goal is achieved by a variety of transformations that carefully manipulate the document format and text of a submission.
\item \emph{Adversarial papers.} 
We present a method for constructing adversarial papers in a black-box and white-box scenario, unveiling a serious problem in automatic reviewer assignment. The attack rests on a novel hybrid approach to construct adversarial examples in discrete domains
\end{itemize}
Examples of the created adversarial papers are provided at \href{https://github.com/rub-syssec/adversarial-papers}{https://github.com/rub-syssec/adversarial-papers}. We also make our code and artifacts publicly available here.
