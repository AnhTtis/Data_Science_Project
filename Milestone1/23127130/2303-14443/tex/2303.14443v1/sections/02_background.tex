\section{Technical Background}
\label{sec:background}

Let us start by reviewing the necessary background for the design of our attack, covering the process of paper-review assignment and the underlying topic modeling. 

\paragraph{Systems for paper-reviewer assignment.} 
To cope with the abundance of submissions, several concepts have been proposed to assign reviewers to submissions~\citep[e.g.,][]{liu-14-robust, li-13-automatic, stelmakh-19-peerreview, long-13-good}. In practice, the most widely used concept is \ac{TPMS} by \citet{charlin-13-toronto}. Because of its high-quality assignments and direct integration with Microsoft's conference management tool CMT~\cite{misc-cmt}, \ac{TPMS} is used by numerous conferences in different fields, including ACM CCS in 2017--2019 and NeurIPS/ICML. \ac{TPMS} can be considered the de facto standard for automatic matching of papers to reviewers. Unfortunately, the implementation of \ac{TPMS} is not publicly available and thus we focus in this work on \emph{Autobid}~\cite{misc-autobid}, an open-source realization of the \ac{TPMS} concept. 
Autobid closely follows the process described by \citet{charlin-13-toronto}. The system has been designed to work alongside HotCRP~\cite{misc-hotcrp} and was used to support reviewer assignment at the IEEE Symposium on Security and Privacy (S\&P) in 2017 and 2018.

Technically, \ac{TPMS} and Autobid implement a processing pipeline similar to most matching concepts: (a) the text from the submission document is extracted and cleansed using natural language processing, (b) the preprocessed text is then mapped to the latent space of a topic model, and finally (c) an assignment is determined by deriving a ranking of reviewers. In the following, we review these steps in detail.

\begin{figure}[b]
    \centering
    % \input{figures/tikz/background_pipeline.tex}
        \includegraphics{figures/main-figure0.pdf}
    \caption{Text preprocessing in paper-reviewer assignment.}
    \label{fig:background_pipeline}
\end{figure}

\paragraph{(a) Text preprocessing.}
When working with natural languages, multiple steps are required to bring text into a form suitable for machine learning (see Figure~\ref{fig:background_pipeline}). 
As paper submissions can be provided in different formats, the pipeline starts by extracting text from the underlying document, typically the PDF format. This original document resides in the problem space of our attack and is denoted as \inputpdf in the following. Autobid employs the tool \texttt{pdftotext} for this task, which is used in our evaluation in Section~\ref{sec:evaluation}.
%
The extracted text is then normalized using a preprocessor function \preprocessing. Typically, it is tokenized, converted to lowercase, and stemmed~\cite{lovins-68-development}. Subsequently, stop words are removed so that each submission is now represented as a sequence of filtered stems. Autobid employs the NLTK package \citep{bird-09-natural} to perform this task. 

Finally, a feature extractor $\extractor$ maps the input $\preprocessing(\inputpdf)$ to a bag-of-words vector $\bow \in \mathbb{N}^{|\vocabulary|}$ with $\vocabulary$ being the vocabulary formed over all words (stems). That is, a submission is represented by a high-dimensional vector whose individual dimensions reflect the count of words.  Although this representation is frequently applied in supervised learning, the high dimensionality is problematic for unsupervised learning and complicates determining topics in the submission.

\paragraph{(b) Topic modeling.}
The key to matching reviewers to papers is the automatic identification of topics in the text. This unsupervised learning task is denoted as \emph{topic modeling}. While there exist several algorithms for this modeling, many assignment systems, including \ac{TPMS} and Autobid, use \ac{LDA}. 
%
\ac{LDA} is a Bayesian probabilistic method for topic modeling that allows representing a document as a low-dimension mixture of latent topics. 
%
Formally, we define this representation as a function
\[ 
   \topicextractor\colon \mathbb{N}^{\left|\vocabulary\right|} \longrightarrow \topicspace, \quad 
   \bow\mapsto \topicdocumentdist_\bow
\]
mapping a bag-of-words vector $\bow$ to a low-dimensional vector space $\topicspace$, whose dimensions reflect different  topics.

Generally, \ac{LDA} is modeled as a generative probabilistic process \cite{blei-02-lda, hoffman-10-online, darling-11-theoretical}. It assumes a corpus $\corpus$ of documents and models each document as a random mixture over a set of latent topics $\topics$. A topic $\topic$ is characterized by a multinomial distribution over the vocabulary $\vocabulary$, and drawn from a Dirichlet distribution $\topicworddist_\topic \sim \textit{Dirichlet}(\topicwordprior)$ with the prior $\topicwordprior$. The Dirichlet prior is usually sparse (i.e., $\topicwordprior<1$) to model that words are not uniformly assigned to topics. Given these topics, for each document $\corpusdoc \in \corpus$, a distribution of topics $\topicdocumentdist_\corpusdoc \sim \textit{Dirichlet}(\topicdocumentprior)$ is drawn. Again, the prior $\topicdocumentprior$ is sparse to account for that documents are usually only associated with a small number of topics. Finally, for each word $\word_i \in \corpusdoc$, a topic $\topic_i \sim \textit{Multinom}(\topicdocumentdist_\corpusdoc)$ is selected and the observed word $\word_i \sim \textit{Multinom}(\topicworddist_{\topic_i})$ is drawn. 
%
This process can be summarized by the joint probability
\begin{equation}
    \prob(\mathbf{\word},\mathbf{\topic},\topicdocumentdist,\topicworddist|\topicdocumentprior, \topicwordprior) = \prob(\topicworddist|\topicwordprior)\prob(\topicdocumentdist|\topicdocumentprior)\prob(\mathbf{\topic}|\topicdocumentdist)\prob(\mathbf{\word}|\topicworddist_{\mathbf{\topic}})
\end{equation}
with $\mathbf{\word} = (\word_1, \dots, \word_{\corpusdoclen}) $ and $\mathbf{\topic} = (\topic_1, \dots, \topic_{\corpusdoclen})$.

To create a topic model in practice, we need to reverse this process and learn the posterior distribution of the latent variables $\mathbf{\topic}$, $\topicdocumentdist$, and $\topicworddist$ given the \emph{observed} documents $\corpus$. Specifically, we need to solve
%
\begin{equation}
    \prob(\topicdocumentdist, \topicworddist, \mathbf{\topic}|\mathbf{\word}, \topicdocumentprior, \topicwordprior) = \frac{\prob(\topicdocumentdist, \topicworddist, \mathbf{\topic}, \mathbf{\word}|\topicdocumentprior, \topicwordprior)}{\prob(\mathbf{\word}|\topicdocumentprior, \topicwordprior)}.
\end{equation}
%
Solving this equation is intractable as the  term $\prob(\mathbf{\word}|\topicdocumentprior, \topicwordprior)$ cannot be computed exactly~\cite{blei-02-lda}. To address this, different approximated techniques, such as variational inference \cite{blei-02-lda, hoffman-10-online} or Gibbs Sampling \cite{darling-11-theoretical}, are typically used for implementations of \ac{LDA}. 
Autobid builds on variational inference based on the implementation of GenSim~\cite{rehurek-10-gensim}.

For the feature vector $\corpusdoc$ of a new submission, the same technique---conditioned on the corpus $\corpus$---is used to compute the corresponding topic mixture $\topicdocumentdist_\corpusdoc$. Attacking this process is challenging, as no gradients or other guides for moving in the direction of particular topics are directly available. Hence, we develop a new search algorithm for subverting the topic assignment of \ac{LDA} in Section~\ref{sec:feature-space}.

\paragraph{(c) Paper-reviewer assignment.}
\label{sec:background:assignment}
Finally, the topic model is used to estimate the reviewer expertise and automate the matching of submissions to reviewers. More specifically, let $\reviewersset$ be the set of all potential reviewers and $\submissions$ a set of submissions $\submission \in \mathbb{N}^{|\vocabulary|}$. For each reviewer $\reviewer$, we collect an archive $\archive_\reviewer \in \mathbb{N}^{|\vocabulary|}$ representative of the reviewer's expertise and interests. Since researchers are naturally described best by their works, this could, for example, be a selected set of previously published papers. The corresponding archives are modeled as a union over all papers.

For each pair of reviewer $\reviewer$ and submission $\submission$, a \emph{bidding score} $\bid_{\reviewer, \submission}$ is calculated. This score reflects the similarity between the reviewer's archive $\archive_\reviewer$ and a submission $\submission$: the more similar, the higher the score. Given the topic extractor $\topicextractor(\cdot)$, a reviewer $\reviewer$ and a submission $\submission$, Autobid defines the bidding score as the following dot-product
\begin{equation}
\bid_{\reviewer,\submission} \coloneqq \topicextractor(\archive_\reviewer) \cdot \topicextractor(\submission)^\top.
\end{equation}

Subsequently, these bidding scores are used for the final assignment $\assignment \in \{0,1\}^{|\reviewersset|\times|\submissions|}$ with the goal to maximize the similarity between reviewers and submissions.
%
In this phase, additional constraints are included: the assignment is subjected to (1) the targeted number of reviewers $\paperload$ assigned to a submission and (2) the maximum number of submissions $\reviewerload$ assigned to a reviewer.
%
More formally, we can describe the assignment as the following bipartite matching problem:

\begin{equation*}
\begin{array}{ll@{}ll}
\underset{\scriptscriptstyle \assignment}{\text{maximize}}    & \displaystyle \displaystyle\sum_{\reviewer}\sum_{\submission} \bid_{\reviewer, \submission} \cdot \assignment_{\reviewer, \submission} &\\[3ex]
\text{subject to}   & \assignment_{\reviewer, \submission} \in \{0,1\} \;\forall \reviewer,\submission \\[2ex]
                    & \displaystyle \sum_{\reviewer} \assignment_{\reviewer, \submission} \leq \paperload  \;\forall \submission \\[3ex]
                    & \displaystyle \sum_{\submission} \assignment_{\reviewer, \submission} \leq \reviewerload \;\forall \reviewer \\
                    
\end{array}
\end{equation*}
%
This optimization problem can then be reformulated and efficiently solved with \emph{Linear Programming (LP)} \cite{taylor-08-optimal}.