\vspace{-0.25em}
\section{Evaluation}
\label{sec:evaluation}

In the following, we evaluate the efficacy of the proposed approach to prepare adversarial papers. To this end, we simulate the automatic paper-reviewer assignment process of a real conference with the full program committee (PC). 
%
We consider two different scenarios:
%
First, we demonstrate how a white-box attacker with full-knowledge about the target system can select and reject reviewers for a submission.
%
Second, we consider a black-box adversary with only limited knowledge and no access to the trained assignment system. We show that such an adversary can generate effective surrogate models by exploiting public knowledge about the conference.
%
Finally, we verify that the manipulated papers are plausible and preserve the semantics of the text.

\paragraph{Setup.} 
We use Autobid~\cite{misc-autobid} as an open-source realization of the TPMS concept~\cite{charlin-13-toronto}. We simulate an automatic assignment for the \emph{43rd IEEE Symposium on Security and Privacy} with the full PC and set the paper load $\paperload = 5$ (\ie, assign each submission to five reviewers). In contrast to the real conference, we assume a fully automated assignment without load balancing and conflicts (see Section~\ref{sec:discussion}).
%
As we do not have access to the original submissions, we use the accepted papers as substitutes. In total, we find 32 papers on the arXiv e-Print archive with \LaTeX{} source, which we use for our evaluation.

The PC consists of 165 persons. For each PC member, we construct an archive $\archive_\reviewer$ of papers representative for the person's expertise by crawling their \emph{Google Scholar} profile. We select 20 paper for each reviewer and compile the corpus as the union of these archives. To simulate a black-box scenario, we additionally generate \emph{surrogate corpuses} that overlap with the original data between 0\%  and 100\%. Appendix \ref{app:corpus} describes this process in detail. In all cases, we train Autobid with the default configuration on a given corpus.

For each attack, we perform a grid search on its parameters to realize a reasonable trade-off between efficacy and efficiency. We start by relaxing any constraints on $\modifications$ ($\maxmannorm = \infty$ and $\maxinfnorm = \infty$) and run the attack with at most $\switches = 8$ transitions between the feature space and problem space (see Appendix \ref{app:hyperparameters} for details).
%
All experiments are performed on a server with 256 GB RAM and two Intel Xeon Gold 5320 CPUs. % Our code is available at \href{https://github.com/rub-syssec/adversarial-papers}{https://github.com/rub-syssec/adversarial-papers}.

\paragraph{Performance measures.} We use three measures to evaluate the attack's performance. First, we consider an adversarial paper $\inputpdf'$ to be \emph{successful} if the constraints from Equation \ref{eq:attack-goal} are fulfilled.
%
Second, to quantify modifications to the submission, we use two standard measures: $L_1$ and $L_\infty$ norm. Given the modified word counts $\submission' \coloneqq \submission + \modifications $, these are computed as
	\begin{equation}
	   \modificationsmannorm = \sum_{i}{\left| \modifications_i \right|} \text{ and } \modificationsinfnorm = \max_i \left| \modifications_i \right|.
	\end{equation}
$L_1$ is the absolute number of modified words and provides a general overview on the total amount of modifications. Intuitively, we are interested in minimizing $L_1$ to make an attack less suspicious. Similarly, $L_\infty$ is the maximum change in a single dimension (i.e., a single word) and ensures that a single word is not included too frequently. 
%
Third, we assess the \emph{semantics} and \emph{plausibility} of the manipulated papers in a user study with security researchers.

\input{includes/table-feature-space-search-strategy}
\subsection{White-box Scenario}

In our first scenario, we focus on a white-box scenario and consider three attack objectives: (1) selection, (2) rejection, and (3) substitution of a reviewer. For these objectives, we focus on reviewers that are already \say{close} to a submission in the assignment system. For example, a paper on binary analysis would raise suspicion if it would get assigned to a reviewer with a cryptography background.

We use the initial assignment scores of the submission as a proxy to simulate this setting. We determine potential reviewers by computing the ranking for the unmodified submission and consider the 10 reviewers with highest scores. To study objective (1), we sample reviewers from the ranks 6--10 and attempt to get them assigned to the submission. Analogously, for objective (2), we select reviewers from the ranks 1--5 and aim at eliminating their assignment. Finally, for objective (3), we first select a reviewer for removal and then a counterpart for selection. We repeat this procedure 100 times with random combinations of papers and reviewers for each objective. Moreover, to account for the high variance of LDA, we train the topic model 8 times and average results in the following.


\paragraph{Feature-space search.} 

We start our evaluation by examining the feature-space search of our attack in detail. For this experiment, we consider format-level transformations that can realize arbitrary changes. Other transformations are evaluated later when we investigate the problem-space side of our attack.

The results of this experiment are presented in Table \ref{table:feature-space-search-strategy} and further detailed in Appendix \ref{app:boxplots}. We observe that our approach is very effective: 99.7\,\% of the attacks finish successfully with a median run-time of 7 minutes.
%
The number of performed changes shows high variance, ranging between 9 and 22,621 adapted words. Despite this broad range, however, the average manipulation involves only between 704 and 1,032 words for objectives (1) and (2), respectively. For reference, an unmodified submission contains 7,649 words on average, so that the necessary changes for preparing an adversarial paper amount to 9\% and 13\% of the words.  

Among the three objectives, we see a trend that selecting a reviewer is more efficient than rejecting one. Rejected reviewers have---per construction---a high assignment score, and hence share many topics with nearby reviewers. In contrast, for selected reviewers it is easier to determine topics with less side effects. The third scenario, where we both reject and select a reviewer, is naturally the hardest case. Generally, we observe that topic models based on \ac{LDA} are comparatively robust against adversarial noise, in relation to neural networks which can be deceived into a misclassification by changing only a few words~\cite[e.g.,][]{gao-18-blackbox, li-19-textbugger}.

\paragraph{Baseline experiments.} To put these numbers into perspective, we examine two baselines. 
%
First, we implement a hill climbing approach that directly manipulates the topic vector of a submission (cf. Equation \ref{eq:score}) by sampling words from the topic-word distributions associated with a reviewer.
For the second baseline, we consider an approach that morphs a target submission with papers that already contains the correct topic-word distribution. 
To find these papers, we compute all assignments of the training corpus and identify submissions to which the target reviewer is assigned. We then repeatedly select words from these submissions and expand our adversarial paper until we reach the target. In rare cases, we could not find papers in which the reviewer is highly rated. We exclude such cases from our experiments.

Considering all three objectives, the hill climbing approach shows a lower success rate: Only 92.2\,\% of the papers are successfully manipulated. The failed submissions either reach the maximum number of 1,000 iterations or get stuck in a local minimum. In successful cases, the attacker needs to introduce more than twice as many changes compared to our attack and the median $L_1$ norm increases from 704--2,059 to 1,652--5,526 words. For the morphing baseline, the attack is successful in only 91.1\,\% of the cases and again needs to introduce significantly more words. We find that the median $L_1$ norm increases by a factor of $4.35$ with a maximum of 29,291 modified words for a single attack.

\paragraph{Generalization of attack.} 
To investigate the generalization of our attack, we repeat this experiment for a second real conference. In particular, we simulate the assignment of the \emph{29th USENIX Security Symposium} with 120 reviewers. We consider 24 original submissions and construct targets as before. Results of this experiment are presented in Appendix \ref{app:generalizaton}.
%
We observe a similar performance across all three objectives, indicating the general applicability of our attack.

\input{includes/figure-transformations}
\paragraph{Scaling of target reviewers.}
Next, we scale the attack to larger sets of target reviewers and consider different combinations for selecting, rejecting, and substituting reviewers. We allow an attacker to select up to five target reviewers, which is equivalent to replacing \emph{all} of the initially assigned reviewers. Furthermore, we allow the rejection of up to two reviewers. We focus again on close reviewers and randomly select 100 sets of targets per combination.

The results are summarized in Appendix \ref{app:scaling}. The attack remains effective and we can successfully craft adversarial papers in most of the cases. 
%
We observe a clear trend that with increasing numbers of target reviewers, we need to perform more changes to the submission. For example, to select all five reviewers, in the median we need to modify 5,968 words. This is expected: we have to move the submission in the topic space from the initially-assigned reviewers to the targeted ones. By adding more reviewers, we include more constraints which results in a significant amount of modifications.

\paragraph{All transformations.}
So far, we have focused on format-level transformations to realize manipulations. These transformations exploit intrinsics of the submission format, which effectively allows us to make arbitrary changes to a PDF file. An attacker likely has access to similar transformations in any practical setting. In fact, robust parsing of PDF files has been shown to be a hard problem \citep[e.g.,][]{carmony-16-extract}. However, we believe it is important for an attacker to minimize any traces and consider different classes of transformations as introduced in Section \ref{sec:problem-space}.

\minipara{(a) Attack budget.}
For this experiment, we introduce an attack budget to describe the maximum amount of allowed modifications for a given transformation. This budget trades off the ability of a transformation to introduce changes with their conspicuousness, since too many (visible) modifications will likely lead to a rejected submission. In particular, we assume a maximum of 25 real and 5 adaptive added \BibTeX{} entries, at most 25 replacements of words with synonyms, no more than 20 spelling mistakes, and up to 10 requested words on average through a text from a \mbox{language model}. In Section~\ref{sec:user-study}, we validate these parameters and assess if the resulting adversarial papers are unobtrusive to human observers.

As a result of the attack budget, we cannot realize arbitrary modifications, since their total amount is restricted. To study this in more detail, we consider the success rate as a function of the attack budget scaled with a factor $\attackbudgetscale$ between 0.25 and 4. During the attack, we split the budget equally across 8 feature-problem-space transitions. We require that targets are feasible with this budget and randomly select 100 targets from the three attack objectives that require $\leq 1,000$ changes in $\F$. Finally, we consider three different configurations: (1) text-level transformations, (2) text-level and encoding-level transformations, and (3) text-level, encoding-level, and format-level transformations combined. We do not restrict the budget for format-level transformations as these transformations are generally not visible.

The results are shown on the left side of Figure \ref{fig:transformer}. For text-level transformations and text-level \& encoding-level transformations, we see an increase in the success rate when the attack budget grows. For the base budget ($\sigma=1$), 40.75\% of the adversarial papers can be prepared with text-level transformations only. That is, no changes in the format and encoding are necessary for manipulating the reviewer assignment.
This can be further improved by increasing the budget, for instance, 67.13\% of the papers become adversarial by scaling it to 4. For smaller budgets, however, we observe that there is often not enough capacity to realize the required modifications. Still, using format-level transformations improves the success rate to 100\% in almost all cases. In rare case, we observe that the attack gets stuck in a local minima. Interestingly, this is more likely with larger budgets. In these cases, the attack makes bigger steps per iteration which introduces more side effects. From the perspective of an attacker, this can be resolved by either increasing the number of switches or reducing the budget.

\minipara{(b) Problem-feature-space transitions.}
To better understand the influence of the alternating search on the success rate of our attack, we conduct an additional experiment. In particular, we simulate our attack for different numbers of transitions $\switches \in \{1, 2, 4, 8, 16\}$ between the problem space and the feature space. We consider the same targets as before and set the attack budget to $\attackbudgetscale = 1$.

The results of this experiment are depicted on the right side of Figure \ref{fig:transformer}. Increasing the number of transitions has a significant effect on the success rate. For all configurations, we see a steady improvement when the number of problem-feature-space transitions increases.
%
Notably, even the format-level transformations \emph{require} multiple transitions in some cases. The success rate increases from 77.13\%---with no transitions---to 100\% when increasing $\switches$. By alternating between $\F$ and $\Dom$ we share constraints between problem and feature space to find modifications that can be realized in the problem space. This further underlines that it is beneficial and in fact necessary to consider both spaces together.

\input{includes/figure-surrogates}
\subsection{Black-box Scenario} 

In practice, an attacker typically does not have unrestricted access to the target system. In the following, we therefore assume a black-box scenario and consider an adversary with only limited knowledge. In particular, this adversary cannot access the assignment system and its training data. Instead, we demonstrate that she could leverage her knowledge about the program committee and construct a surrogate dataset to train her own models for preparing adversarial papers.

The assignment systems AutoBid and TPMS do not specify how the corpus for training a topic model is constructed. They only require that the selected publications are representative of the reviewers. Hence, even if we do not know the exact composition of the training data, we can still collect a surrogate corpus of representative data with public information, such as recent papers of the PC members, and transfer our attack between models. In practice, the success of this transfer depends on two factors: (a) the stability of the surrogate models and (b) the overlap of publications between the original training data and the surrogate corpus.

\paragraph{Stability of surrogate models.}
The training of LDA introduces high variance \cite{agrawal-18-what, mantyla-18-measuring}, so that adversarial papers na\"\i vely computed against one model will likely not transfer to another. To account for this instability, we approximate the model space and consider an \emph{ensemble} of surrogate models.
%
That is, we run our attack simultaneously against multiple surrogate models trained on the same data. We focus on format-level transformations and repeat the attacks for all three objectives. We vary the number of models in the ensemble from 1 to 8 and consider an overlap of 70\% between the underlying surrogate corpus and the original training data.

Figure~\ref{fig:surrogates} show the results of this experiment. %
Across all objectives, we see an improvement of the success rate when increasing the number of surrogate models. This is intuitive: the adversarial papers are optimized against all models and thus more likely to transfer to other models.
%
This robustness, however, comes at a cost and the number of modifications increases as well. The median $L_1$ norm increases from 1,990 to 7,556 when considering 8 instead of a single surrogate model (see Appendix \ref{app:surrogate-boxplots}). 

As a result, an adversary in the black-box scenario must find a trade-off: If she needs a successful attack with high probability, she must sacrifice detectability and modify a large number of words. If, on the other end, she only wants to increase her chances for a specific assignment, she can operate without an ensemble and adapt only a few words.

\input{includes/figure-transferability}

To further study the transferability of our attack, we sample 100 target reviewer from the median ranking computed over 8 assignment systems and simulate the attack with an ensemble of 8 surrogates. Figure \ref{fig:transferability} visualizes how the resulting adversarial papers transfer among the target systems.
%
96\% of the papers are successful against four or more target systems and 34\,\% are successful considering all 8 systems.

\paragraph{Overlap of surrogate corpus.}
%
To understand the role of the surrogate corpus, we finally repeat the previous experiment with varying levels of overlap.
%
Surprisingly, the attack remains robust against variations in training data. The success rate only fluctuates slightly: 78.0\% (100\% overlap), 80.0\% (70\% overlap), 79.6\% (30\% overlap), and 82.8\% (0\% overlap).
%
To explain this, we compute the cross-entropy of the reviewer-to-words distributions $\reviewerwords_\reviewer$ for models trained on training data with different overlaps. We observe that the cross-entropy between models trained on the same dataset (i.e., 100\% overlap) is in the same range compared to models trained on different data (cf. Appendix \ref{app:cp-overlap} for details).
%
As LDA models trained on the same corpus already vary significantly, our attack seeks a robust solution that transfers well if the surrogate models have less overlap with the original training data.

\subsection{Plausibility and Semantics}
\label{sec:user-study}
Finally, we empirically verify if the adversarial modifications are (a) plausible and (b) preserve the semantics of the text. 

\paragraph{Study design.}
As dataset, we use the combined set of original and adversarial papers from our evaluation.
In total, we select seven original papers and their adversarial counterparts, ensuring varying topics and transformations. The attack budget is $\attackbudgetscale = 1.00$. 
Due to a limited number of participants, we focus on visible transformations (i.e.\ encoding-level and text-level) that a reviewer could detect.
Each participant selects (``bids on'') one paper. This selection cannot be changed afterwards and participants are secretly assigned either to the adversarial or to the unmodified version. Each participant will only check one paper to avoid potential bias and fatigue effects.


We design the review process along two phases. Our methodology here is inspired by the work from Bajwa et al. \cite{bajwa-2019-might} and Sullivan et al. \cite{sullivan-10-reviewer}. In the first phase, we request participants to write a mini-review (as a proxy task) for a given paper. In the second phase, we ask if they think the paper has been manipulated. Importantly, the answers of phase~1 cannot be changed. This two-phase separation allows us to observe two factors: First, we can analyze how suspicious adversarial papers are to an unaware reader. Second, once the reader is informed, we can learn about which transformations are noticeable and make our attack detectable.
In each phase, we provide a template with questions on a numerical scale from 1--5, together with free text fields for justifying the rating.
Participants are debriefed finally.
We obtained approval from our institution's Institutional Review Board (IRB) and our study protocol was deemed to comply with all  regulations.

\paragraph{Results.} We recruited 21 security researchers (15$\times$ PhD students, 4$\times$ postdocs, 1$\times$ faculty, 1$\times$ other). All participants are familiar with the academic review process but have different review experience (7$\times$ have not written a review before, $4\times$ between 1-2 reviews, 6$\times$ between 3-10, and 4$\times$ at least 10 reviews). 
%
The participants reviewed a total of 12 adversarial and 9 original submissions.

Figure~\ref{fig:user-study-v2} summarizes the results. Benign and adversarial submissions are rated similar across all review questions. No participant was certain that a paper was manipulated (\ie, gave it a ranking of 5) and only a single of the 12 manipulated submissions was flagged as suspicious with a rating of~4. This was justified with missing references and redundancy in the text---neither of which were introduced by our attack. Interestingly, this reviewer did notice the spelling mistake and language model transformer (when asked about the writing quality), but did not attribute this as a sign for manipulation. This is opposed to two false positive ratings of benign papers, which results in a overall detection precision of 33\% with a recall of only 8\%. This highlights the difficulty to detect any introduced modifications.

\begin{figure}[t]
    \centering
    % \input{figures/tikz/user-study-results.tex}
     \includegraphics{figures/main-figure1.pdf}
    \vspace{-0.75em}
    \caption{\textbf{Ratings of benign and adversarial papers.} For each question, the upper boxplot shows the ratings from the benign papers, the lower boxplot from the adversarial papers.}
    \label{fig:user-study-v2}
    % \vspace{-1.5em}
\end{figure}

Finally, we check that the semantics of the papers are not changed. With the limited attack budget, only small, bounded changes are made to a paper. 
This is further supported by the organization and comprehensibility ratings in Figure \ref{fig:user-study-v2}, which are similar between manipulated and benign submissions.