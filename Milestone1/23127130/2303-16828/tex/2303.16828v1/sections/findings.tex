In this section, we discuss our findings from the collaborative model with context experts, which we have described in the previous section. While we drew most of these observations from our process in Myanmar, they demonstrate similar issues in other low-resource language settings where machine learning may be applied. An appreciation of these findings is crucial for designing efficient machine learning systems. We have identified three central issues: (i) exploring avenues to boost ground truth data, (ii) incentivizing data work for machine learning, and (iii) engendering open data-sharing practices.

\subsection{Exploring avenues to boost ground truth data}
A major challenge with hate speech detection tasks in low-resource language settings is limited training data in target languages. This limitation is often due to the rarity of hate speech in proportion to the amount of available social media data and the infeasibility of labeling the entire dataset on a given platform~\cite{madukwe2020data}. Much of the progress in the field of machine learning has been driven by the availability of benchmark datasets such as ImageNet \cite{deng2009imagenet} for computer vision, and GLUE \cite{wang2018glue} for natural language processing. While such datasets have sprung up for hate speech detection in the English language \cite{waseem2016hateful, mathew2020hatexplain}, researchers often filter out data not in English, leaving more work to do for very low-resource languages. This lack of training data can hamper machine learning research within these contexts. The ML4D community can adopt the process described in this work to bootstrap the development of hate speech training datasets in diverse contexts.

A caveat to note is that engaging in elaborate data collection and labeling efforts do not always guarantee significant outcomes. We found that only less than 4\% of the entire Facebook posts in our dataset were labeled by annotators as hate speech. To boost training data, practitioners have relied on curated hate speech lexicons in the target languages. One example is the PeaceTech Lab Lexicons, a series of hate speech terms explaining inflammatory social media keywords and offering counter-speech suggestions to combat the spread of hate speech~\cite{peacetechlab}. The PeaceTech Lab has curated hate lexicons for languages in conflict-affected countries such as the Democratic Republic of the Congo, Sudan, and Lebanon. Though keyword approaches are ineffective when used alone, they can help researchers select a subset of data to work with. Organizing civil society workshops or other avenues for crowdsourcing hate terms can be beneficial for supporting hate speech detection work.

Our findings reveal how quickly new hate terms emerge on social media. This dynamism is primarily due to changes in political and social concerns. For instance, online actors may derive new hate terms as social media discourse changes from talking about a local election to focusing on a pandemic~\cite{ziems2020racism}. Context experts can help identify when social media topics drift or when users find creative ways to guise hate speech. 

\subsection{Incentivizing data work for machine learning}
Data work is ``any human activity related to creating, collecting, managing, curating, analyzing, interpreting, and communicating data''~\cite{bossen2019data}. This fundamental component of machine learning is often undervalued in research and practice~\cite{wagstaff2012machine} and can lead to negative outcomes because data is critical for effective machine learning systems~\cite{polyzotis2018data}. Sambasivan et al.~\cite{sambasivan2021everyone} discuss the challenges with data cascades in high-stakes AI, which they define as compounding events over time that result from the undervaluing of data quality by researchers and practitioners. These data cascades often have significant effects in low-resource language settings due to the lack of existing ground data and supporting infrastructure for data collection and processing. We identified some of the cascade factors identified by Sambasivan et al.~\cite{sambasivan2021everyone} in our work, namely, incentives in AI and data education. First, we observed that while stakeholders and partners might identify with and value the role of AI in addressing the problem of low-resource hate speech, they often did not place similar consideration on the invisible and challenging data work. Second, context experts and partners lack experience in creating the kinds of complex and quality datasets that hate speech research requires. Stakeholders ought to see the value in data work to appreciate the time spent on training context experts.

At the onset of communications with sponsors and partners, it is crucial to emphasize the importance of data work for machine learning and the vital role that context experts will play. This step may imply a more costly engagement with context experts, both in time and financial compensation. We learned that a comprehensive data training plan is helpful at the start of the project to address the data education challenge. 

\subsection{Supporting open data sharing practices}
Social media researchers in low-resource contexts do not often have the freedom to choose what platforms to work on. This decision is mainly driven by which platform is widely used and most likely to have relevant data within a given context. With over 21 million active users in 2019~\cite{kemp19}, our work in Myanmar primarily focused on Facebook. Like any other platform, Facebook offers unique affordances and constraints for data access and sharing. For example, the company grants privileged data access to researchers via its CrowdTangle API service. Yet, its terms of use do not allow researchers to share any data with people outside CrowdTangle. Such practices make it challenging to freely collaborate with other teams who may also have access to data but are bound by the platform's terms.

These limitations posed by platforms motivate community-based data-sharing workflows led by context experts and built on trust, ethics, and shared values. Data sharing has been discussed as an effective means for scientific progress, especially within developing countries \cite{chawinga2019global, sebake2012assessing}. However, there are lots of structural, organizational, cultural, and ethical complexities that undermine data sharing~\cite{anane2018you}. Exploring data-sharing practices that actively address power imbalances, understands potential benefits and risks, and comply with local norms and cultures~\cite{abebe2021narratives} will help produce a critical mass of relevant data that can be useful for tackling low-resource hate speech.

Furthermore, significant aspects of a country's history are often undocumented digitally, only existing as paper documents within locked-out cabinets or as informal knowledge passed on from one generation to another. In some cases, only a fraction of the context experts know the historical and cultural issues that underlie hateful speech posted online. Our annotation model addresses this knowledge imbalance with the peer feedback model discussed as part of our process. 