Most automatic hate speech detection systems rely on machine learning algorithms trained on existing ground truth data. However, the resources needed to facilitate this task are limited to a small set of languages and organizations. Similarly, publicly-available pretrained large language models only work on a handful of languages. This limitation implies that progress in automatic hate speech detection tends to follow a top-down approach where only privileged languages gain engineering attention. To balance this disproportion, we aim to design a collaborative process with context experts in low-resource language settings to scope the problem of hate speech detection and develop machine learning models to address them.

To concretize this collaboration, we focus on the task of developing machine learning models in the Burmese language to automatically detect hate speech posted on social media within the context of the Myanmar general election. A process approach helps us temporally order the crucial aspects of this work to support replication, reuse, and revalidation. By developing this process, we hope to provide a recipe for practitioners and researchers interested in collaborative work that addresses hate speech in low-resource language settings. Our approach consists of the following four key steps:
\begin{enumerate}
    \item establish partnerships to co-design project scope, identify and recruit paid volunteers.
    \item contextualize hate speech definitions and annotation guidelines for local relevance with legal experts.
    \item generate quality data and train machine learning models.
    \item validate trained machine learning models, and iterate step (3).
\end{enumerate}

To facilitate this process, context experts take on two roles. First, they serve as technologists, actively contributing to shaping the overall direction of our research, and subsequently as model validators, identifying opportunities for improving the machine learning system.

\subsection{Context Experts as Technologists}
Attygale~\cite{attygalle2017context} discusses the idea of engaging with context experts as an intentional process of co-creating solutions in partnership with people who know the opportunities for and barriers to impact through their own experiences. According to Attygale~\cite{attygalle2017context}, context experts offer perspectives that add depth and breadth to the technical expertise of ``content experts'' (in our case, machine learning researchers). In this work, we adopt context experts to refer to Burmese natives working in civil society and with an expressed interest in their nation's political system. \textbf{We argue that sustainably addressing hate speech in low-resource language settings involves empowering context experts with the technology-driven tools needed to tackle the problem locally}. The goal is to center the voices of the context experts from the onset of the project and ensure they have a sense of ownership. This type of collaborative model is often broadly described as community consultation, deliberation, engagement, or participation~\cite{ranasinghe2018engaged}. It differs from simply relying on them for feedback or buy-in, as in a contextual inquiry.

\subsubsection{Recruitment and Training} 
Our partners in Myanmar advertised recruitment calls on public social and political Facebook pages. These pages were selected due to the number of accounts following the page and its focus on social and political issues. The call sought people who were active on Facebook and interested in the Myanmar's political system. The earliest 12 respondents were interviewed on a roll-in basis. The interviews were conducted by our resident project manager and focused on the interviewees' knowledge of relevant political issues and their ability to use a computer. After reviews and interviews, eight people were recruited. Five of them identified as female and three identified as male. All the context experts were native Burmese speakers and resident in Myanmar. The context experts attended an initial virtual session to connect and discuss the motivation for the project concerning the forthcoming Myanmar national elections. A subsequent training session to prepare the context experts for the data curation process necessary for the task. We provided labeling guidelines in the English and Burmese languages. The training was facilitated by an experienced project manager, an expert in Myanmar politics. The facilitator conducted the training session in the Burmese language through a video conference call that lasted for 90 minutes. We then created a shared workspace to facilitate communication between researchers and context experts. The context experts were monetarily compensated for their work throughout the project.

\subsubsection{Hate Speech Definition}
There is no universally agreed-on definition for hate speech, and what counts as hate speech in one context may not be considered hate speech in another. This presents a challenge because accurate data annotation requires a standardized framework for consistency and reliability. We aimed to adopt a definition broad enough to potentially cover all instances of hate speech relevant to Myanmar, but was specific enough to avoid ambiguous interpretations by the annotators. To do this, we explored definitions provided by the UN, social media platform companies, and those widely used in hate speech research. 

According to the United Nations Strategy and Plan of Action on Hate Speech~\cite{guterres2019united}, hate speech is \textit{``any communication in speech, writing, or behavior, that attacks or uses pejorative or discriminatory language with reference to a person or a group on the basis of who they are, in other words, based on their religion, ethnicity, nationality, race, color, descent, gender or other identity factor"}. Researchers have developed definitions such as \textit{``language used to express hatred towards a targeted group or is intended to be derogatory, to humiliate, or to insult the members of the group."}~\cite{davidson2017automated} or \textit{``a deliberate attack directed towards a specific group of people motivated by aspects of the group’s identity"}~\cite{de2018hate}. 

Platforms often provide specific categories within which a person or group may be a target of hateful speech. For example, Facebook defines hate speech as \textit{``a direct attack against people — rather than concepts or institutions— on the basis of what we call protected characteristics: race, ethnicity, national origin, disability, religious affiliation, caste, sexual orientation, sex, gender identity and serious disease"}~\cite{facebook21} and Twitter defines it as \textit{``promote violence against or directly attack or threaten other people on the basis of race, ethnicity, national origin, caste, sexual orientation, gender, gender identity, religious affiliation, age, disability, or serious disease."}~\cite{twitter21}. Since Myanmar had not adopted a national definition for hate speech, we adapted a description for our annotation guideline composed of the UN's hate speech definition and included protected characteristics mentioned by Facebook. 
.
\subsubsection{Annotation Guidelines} 
Together with the context experts, we developed an annotation procedure designed to help standardize the annotation process. The annotation guideline for the task was made available in the English and Burmese languages. It contained our adapted definition of hate speech. We emphasized that hate speech must constitute dehumanizing or demeaning sentiment and be expressed because of who the target is based on some protected characteristics. We further added that hate speech may be directed towards an individual or a group \cite{elsherief2018hate} and could be explicit or implicit—the implicit case requiring an understanding of the context. We added a link to the original posts from the annotation files to enable this context inquiry. We highlighted instances when a speech is not regarded as hate speech, such as defamatory speech that does not invoke a protected characteristic or benign attacks against government policy. For example, a post saying \textit{``The Burmese military is corrupt!"} might be an attack on the military's integrity but does not constitute hate speech since the military is not a protected group. Annotators were provided with examples of posts that were hate speech and not hate speech, according to the guidelines. To validate our annotation guideline, we shared a copy with a team of legal experts based in Yangon, Myanmar, for feedback and edits. The experts provided three main feedback to improve the guideline.
\begin{enumerate}[i]
    \item The guideline should use an exhaustive list of protected characteristics instead of sampling of few examples that can leave too much room for annotator subjectivity.
    \item The guideline should clarify what action to take on hate speech regarding political holders. While political holders are protected from aforementioned attacks to protected characteristics, speech that attacks political decisions or ideology of the office holder is not hate speech.
    \item The annotator training should acknowledge the inherent challenges with defining an "attack" or a "demeaning post", especially for cases where hate speech is implicit. Since a post can be considered an attack or demeaning based on the perception of the recipient, there could be potential uncertainty in the labelled data.
\end{enumerate}
We edited our guidelines to address the concerns raised in (i) and (ii). For (iii), we relied on the competence of the context experts to reduce uncertainty. A final copy of our annotation guideline is provided in our Github repository.\footnote{\url{https://github.com/TID-Lab/myanmarhsc}}

\subsubsection{Data Collection}
Using Hatebase \cite{HateBase20}, an online repository of multilingual hate speech terms, we retrieved 128 crowdsourced Burmese hate terms. Three context experts manually inspected and removed 56 of the keywords considered overly context-sensitive and could potentially result in many benign posts. This inspection reduced the number of Burmese hate terms from Hatebase to 72. During the data collection phase, Phandeeyar~\cite{phandeeyar2021}, a technology organization based in Myanmar, released a lexicon of 88 hate terms resulting from their work tracking COVID-19 related hate speech on social media. We checked for possible appearances of the same words within the Hatebase and Phandeeyar lexicon sets. We found only two occurrences of an exact match. Another five were cases where a hate term in one set is the subset of another term in the other set. We combined both lexicons for a total of 158 hate terms for the next step. The list is also provided in our Github repository. \footnote{\url{https://github.com/TID-Lab/myanmarhsc}}

Next, we use CrowdTangle \cite{team2020crowdtangle}, a public insights service provided by Facebook which enables access to public groups and pages, to retrieve posts containing any of these select hate terms. Together with the context experts, we curated a dashboard of social and political pages in Myanmar using a combination of direct Facebook searches and local knowledge of popular social and political groups and pages. To identify new groups and pages, we snowball from already known pages to new pages that interact with them via shares or mentions. We downloaded historical Facebook posts from the CrowdTangle dashboard between October 2018 and June 2020 that contain any words from our hate lexicon. The downloaded data contained 43,996 posts. 

As a preprocessing step, we removed posts containing only URLs or blank shares of other posts. Next, we removed duplicate posts. This was a majority of the posts because CrowdTangle periodically provided updated interaction metrics for the same post leading to duplicates. We retained only the most recent versions of each post. We then randomized the posts in the dataset to remove any consecutive posts from the same account, group, or page. This process would help decrease anchoring bias in the annotations, which emerges when an annotator reads different posts from the same source and may become likely to label them similarly \cite{tversky1974judgment}. Finally, we removed posts with less than three syllables using the Myanmar word segmenter available as part of the Myanmar language tools package~\footnote{https://github.com/MyanmarOnlineAdvertising/myanmar\_language\_tools}. These posts will be too short and lack sufficient context for accurate labeling and feature engineering. After these preprocessing and filtering steps, 5,646 posts remained, which we proceeded to label.

\subsubsection{Labelling Plan}
Our recruited context experts understood Myanmar's socio-political terrain but were not necessarily hate speech experts. We were concerned that one remote training session might be insufficient to prepare them adequately, so we adopted a pairing strategy to boost annotator agreement. In this strategy, each annotator initially receives the same set of posts to label as one other annotator. Each annotator is asked to label the posts assigned to them independently. After labeling their initial batch of posts, they are then asked to schedule a call with their partner and the training facilitator to discuss their experience labeling the posts and address areas in which they disagreed. Each annotator was provided with a laptop and an internet connection to facilitate these conversations. There were eight annotators in our setup, resulting in four pairs. Each annotator received 100 posts per day over four days. After the fourth day, the remainder of the posts were divided equally amongst the annotators for individual labeling. Table~\ref{fig:pc} shows the number of posts labeled as Yes or No for each annotator, grouped with their paired annotator.

\begin{figure}[!ht]
    \center
    \includegraphics[width=\linewidth,scale = 0.5, trim=8 8 8 8,clip]{images/protected.png}
    \caption{\label{schema}Protected characteristics invoked in posts labeled as hate speech}
\end{figure}

\subsubsection{Labelling Result}
After labeling, the final dataset contained 225 unique instances of posts labeled as hate speech. Figure \ref{fig:pc} shows the distribution of protected characteristics identified by the annotators as invoked in the post. We find that this is consistent with our expectations given Myanmar's socio-political scene. To measure the reliability of annotations, researchers rely on a measure of inter-annotator agreement to quantify the level of overlap among annotators on the labels they have chosen for each sentence. However, these scores such as Fleiss'~\cite{fleiss1971measuring} and Cohen's~\cite{cohen1960coefficient} kappas have been known to be affected by annotator bias and class imbalance. There is also no widely accepted kappa level for determining sufficient  reliability~\cite{ross2017measuring}. As a result, most hate speech research report very low agreement scores or do not report this value. For example, a batch of 100 posts given to one pair of annotators may include only one hate speech and 99 non-hate speech posts. If one annotator returns all 100 as not hate speech while the other accurately returns 99 not hate speech and one hate speech, their Cohen's kappa is 0.0 even though they agree 99\% of the time. Though percent agreement has been critiqued on the basis that it does not account for random agreements or guesses due to inadequate annotator training~\cite{mchugh2012interrater}, we adopt it as our measure of annotator improvement for our use case. We limit annotator guessing by training annotators extensively and setting up iterative peer feedback sessions at the end of each cycle.

We incorporated this peer learning model as part of the annotator training process to facilitate knowledge sharing where some annotators have some historical or current affairs context of some posts and build annotators' confidence from learning how much they are in sync with a peer. Figure~\ref{fig:percag} shows how percentage agreement among pairs of annotators increased after the first peer feedback session. Annotators were especially encouraged to discuss areas of disagreement. To avoid a situation where a pair is consistently wrong, we asked each pair to meet remotely with the training facilitator to discuss what they learned from that batch and confirm their results. By the fourth iteration, annotator agreement had increased from an average of 68.75\% to 93\%.

\begin{table*}[ht]
\centering
\caption{Number of posts labeled as Yes or No  by each annotator.}
\label{tab:percscore}
\begin{tabular}{|*{17}{c|}}
\hline
\multirow{3}*{\textbf{Step}} &
\multicolumn{4}{|c}{\textbf{Pair 1}} & \multicolumn{4}{|c}{\textbf{Pair 2}} & \multicolumn{4}{|c}{\textbf{Pair 3}} & \multicolumn{4}{|c|}{\textbf{Pair 4}} \\ \cline{2-17} &
\multicolumn{2}{|c}{\textbf{A1}} & \multicolumn{2}{|c}{\textbf{A2}} & \multicolumn{2}{|c}{\textbf{A3}} & \multicolumn{2}{|c|}{\textbf{A4}} & \multicolumn{2}{|c}{\textbf{A5}} & \multicolumn{2}{|c}{\textbf{A6}} & \multicolumn{2}{|c}{\textbf{A7}} & \multicolumn{2}{|c|}{\textbf{A8}} \\ \cline{2-17} &
Yes & No & Yes & No & Yes & No & Yes & No & Yes & No & Yes & No & Yes & No & Yes & No \\ \hline
1 & 60 & 40 & 76 & 24 & 59 & 41 & 13 & 87 & 10 & 90 & 17 & 83 & 17 & 83 & 34 & 66 \\ \hline
2 & 19 & 81 & 4 & 96 & 3 & 97 & 2 & 98 & 5 & 95 & 2 & 98 & 0 & 100 & 13 & 87 \\ \hline
3 & 2 & 98 & 4 & 96 & 1 & 99 & 1 & 99 & 4 & 96 & 1 & 99 & 5 & 95 & 1 & 99 \\ \hline
4 & 12 & 88 & 4 & 96 & 4 & 96 & 6 & 96 & 9 & 91 & 6 & 94 & 2 & 98 & 0 & 100 \\ \hline
\end{tabular}
\end{table*}

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth,scale = 0.5, trim=8 8 8 8,clip]{images/irscore.png}
    \caption{Percentage agreement within pairs of annotators for each training stage.}
    \label{fig:percag}
\end{figure} 

\subsubsection{Text classification} 
We designed a four-stage data preprocessing pipeline: first, we converted the Zawgyi character to Unicode. We used an open-source Myanmar language tools library~\cite{khine2020tools} for this step. Next, we removed posts predominantly in languages that are not Burmese. We removed emojis. We retained instances of code-switching between English and Burmese. In addition, since the Burmese language does not consistently use white spaces to mark word boundaries, we used the Myanmar language tools~\cite{khine2020tools} library to perform word-level segmentation on the text splitting each post into tokens. Finally, we used an openly available Burmese stop words list~\cite{maung2008rule} to remove stop words from each remaining post in our dataset.


\subsubsection{Classification Models}
We employed classical machine learning classification algorithms (Support Vector Machines~\cite{cortes1995support}, Balanced Random Forests~\cite{chen2004using}, and FastText~\cite{bojanowski2017enriching}) to conform to similar constraints in contexts where models such as pre-trained Transformers are unavailable. We ran several feature combinations of n-grams, term-frequency weighting, and hyper-parameter searches to select the best combination. We show precision, recall, and F-1 scores for the models in Table \ref{tab:modelperf}. From Table \ref{tab:modelperf}, we observe that the FastText model (with oversampling) performed best across all three metrics.

\begin{table}[h]
  \caption{Model performance for hate speech classification}
  \label{tab:modelperf}
  \begin{tabular}{lrrr}
    \toprule
    Model & Precision & Recall & F1-score\\
    \midrule
    SVM & 0.48 & 0.50 & 0.49\\
    SVM (with oversampling) & 0.88 & 0.87 & 0.87\\
    BRF  & 0.53 & 0.70 & 0.45\\
    BRF (with oversampling)  & 0.88 & 0.87 & 0.87\\
    FastText & 0.84 & 0.63 & 0.69\\
    FastText (with oversampling) & 0.93 & 0.92 & 0.92\\
    %MBert & 0 & 0 & 0\\
    %XLM-Roberta & 0 & 0 & 0\\
  \bottomrule
\end{tabular}
\end{table}



\subsection{Context Experts as Model Validators}
Beyond the cross-validation techniques discussed in the previous section, we sought to validate our model qualitatively to understand which cases led to model error and why. There are limited tools for performing these kinds of model validation~\cite{gao2019ai}. For our analysis, we collected live data from Facebook different from those in our training and tests set and passed these data for inference on our best-performing model. We provided a sample of the data and asked our context experts to label them. We then show the model decisions to the context experts and discuss areas where they disagreed with the model. 

We identified two major error types from the model from this step. First, false-positive cases where benign posts containing terms in the hate lexicons or words typically used in hate speech posts and highly represented in the training data were wrongly flagged as hate speech by the model. Second, false-negative cases were hateful posts that did not contain the archetypal hate terms and were not classified as hate speech by the model.