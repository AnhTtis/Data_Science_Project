We now outline some promising directions for future research work in this area addressing issues of limited ground truth data, non-text-based techniques, multimodality, and non-expert collaboration:
 
\subsection{Data augmentation for low-resource hate speech detection} 
Data augmentation involves strategies for increasing training examples for machine learning tasks without explicitly collecting new data. Data augmentation has received recent attention in natural language processing research due to increased work in new domains with limited data and the need for substantial amounts of training data for large neural networks~\cite{feng2021survey}. As our findings have shown, running entire data labeling schemes may not necessarily lead to more data, especially for tasks where true labels are scarce. Low-resource hate speech detection work can benefit from data augmentation strategies that generate new data to augment the sparsity in new contexts. This technique can take the form of generating entirely new corpora from validated ground truths in high-resource settings or creating new variations of existing small data in the target language.
 
\subsection{Exploring network-based modeling approaches} 
Current hate speech detection tools in low-resource language settings mostly rely on text-based user-generated data and less on other contextual information such as linked news sources, parallel comments from local news websites, platform metadata, and other social network data. Incorporating these additional contexts into existing models could improve single-instance hate speech detection and coordinated hate attacks that can be difficult to detect in real time. One idea is to focus on understanding problematic actors within these contexts and identifying relationships that increase the likelihood of spreading hateful content. Some recent works have explored graph-based models for this task~\cite{del2019you, mishra2018author, beatty2020graph} but little is known about how these methods translate to low-resource contexts. A practical solution could help address the challenges posed by limited training data and other vulnerabilities of text-based methods such as topic drift and adversarial attacks.

\subsection{Multimodal hate speech detection} 
A vast amount of hate speech posts in low-resource language settings are multimodal, often containing a combination of images, audio content, videos, live streams, external links, etc. Models trained with data on a single modality might be insufficient to address the problem effectively. Presently, content moderators have to watch hours of videos in their native language to identify hateful rhetoric embedded in the videos before recommending take-downs. This process is challenging to scale, takes lots of moderation hours, and videos tend to spread faster than platform action. Image-text datasets such as MMHS150K~\cite{gomez2020exploring} and the Hateful Memes~\cite{kiela2020hateful} have been released in English for this task. Further work is needed for hate speech detection for multimodal content for low-resource contexts.

\subsection{Active learning and uncertainty-aware predictions}
Given established concerns over the scarcity of true labels and the prevalence of noisy data in the domain of hate speech detection, an active learning strategy might help boost model performance in practice. The idea is that a machine learning model can perform better with fewer labeled training instances if it can choose a reasonable sample to learn from~\cite{settles.tr09}. This is especially useful for systems that are deployed in the wild where the machine learning model can provide strategic queries to human annotators and retrain on a valuable set of new training data. An uncertainty-aware sampling strategy can help ensure that the uncertainty resulting from the data imbalance is well-calibrated~\cite{lewis1994sequential}.

\subsection{Working with non-experts on machine learning deployment projects}  
Machine learning work with non-experts requires a degree of care to avoid the trap of participation-washing where their involvement is either outrightly performative or only aimed at extracting free labor and consultation~\cite{sloan2020participation}. We are only beginning to scratch the surface of what forms effective AI collaboration with context experts may take. In this work, we have shown how we defined and scoped the problem with context experts, collected and labeled the data, and validated the model. Nonetheless, the technical gaps in the context experts' knowledge of machine learning implied that some parts of the modeling process were inadvertently black-boxed from them. This gap may present challenges for machine learning project sustainability, and further work is needed to understand ways to identify and mitigate the critical (people, process, and cultural~\cite{kumar2006impact}) failure factors that might hinder the long-term success of AI deployments in low-resource contexts.