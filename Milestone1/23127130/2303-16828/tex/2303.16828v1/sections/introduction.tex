The rapid adoption of social media in Myanmar has been accompanied by a surge in the dissemination of problematic content such as hate speech, disinformation, and misinformation~\cite{kyaw2019facebooking}. While platforms like Facebook offer opportunities for people to connect online,  do business, and participate in online activism~\cite{best2016mobile}, they have also seen use as a medium for inciting violence against minority groups from online actors~\cite{mclaughlin2018facebook, muller2021fanning}. In 2018, a United Nations independent fact-finding report highlighted the role social media, specifically Facebook, played in spreading hate speech and disinformation that led to a genocide of the Rohingya ethnic minority group in Myanmar. Lee~\cite{lee2019extreme} discussed the role of citizen-generated posts and state media-led publication outlets in spreading anti-minority rhetoric that influenced violent narratives about the Rohingya on social media. Violence towards the Rohingya community spurred by the proliferation of hate speech and disinformation on Facebook~\cite{fink2018dangerous} led to the murder of thousands of civilians, creating almost a million refugees~\cite{un2018report, warofka2018independent}. Myanmar presents a chilling yet increasingly familiar account of the weaponization of Facebook in a nation with a history of armed conflict, authoritarianism, and censorship.

Outside Myanmar, similar challenges with hate speech have been witnessed in countries like Ethiopia~\cite{gagliardone2016mechachal} and Sri Lanka~\cite{wijeratne2018control}. There is a dearth of work investigating effective strategies for real-world hate speech detection in low-resource languages. Current strategies for tackling hate speech in low-resource contexts primarily entail two main steps: exploring the use of sophisticated machine learning tools for detecting hate speech and contracting human content moderators to flag, demote, and ultimately remove problematic content. Both approaches present notable limitations. Machine learning systems require ground truth data and data processing capabilities that are not readily available with low-resource languages. In addition, the vast amount of social media content produced daily makes it infeasible to engage human trackers to detect every instance of hate speech except the most prominent. Even if this was possible for a single context, it is not trivial to scale to new contexts, languages, and countries. 

We seek to address this need by exploring a community-driven approach to tackle hate speech in low-resource language settings. This approach involves working with \textit{context experts} on the entire machine learning project pipeline: scoping the project with a local partner focused on issues related to digital threats to democracy, assessing hate speech definitions and guidelines in tandem with legal experts, and working with paid volunteers to generate quality data, train, and validate machine learning models. We use the term \textit{context experts} to highlight their role not merely as language translators but as experts with deep and personal knowledge of the context resulting from their lived experience. We interchangeably use the terms annotator and context experts in some parts of this paper. We only refer to the context experts as annotators when maintaining standard language for data labeling tasks. 

This paper offers a report on our remote study of hate speech in Myanmar. In the months leading up to the 2020 Myanmar national elections, we worked remotely, due to the COVID-19 pandemic, with context experts to curate a dataset of 226 Burmese hate speech posts from Facebook through its CrowdTangle API service~\cite{team2020crowdtangle}. Our work contributes to research in machine learning for development (ML4D) seeking to understand ways to tackle hate speech on social media. We develop a process for coordinating machine learning work within low-resource language settings and show that working with context experts offers a key solution to the problem of hate speech in these settings. We also provide an early look into downstream classification tasks for the Burmese language using classical and neural network-based machine learning models.