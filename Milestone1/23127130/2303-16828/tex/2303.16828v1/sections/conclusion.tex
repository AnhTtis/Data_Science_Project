There is an urgent need to identify ways to tackle hate speech on social media in low-resource language settings. We have argued that addressing this problem will require community-based approaches that combine a deep understanding of social and political contexts with automated tools to process the vast amount of content produced online. In this paper, we presented findings from our remote study on the automatic detection of hate speech on Facebook in Myanmar. We have developed a systematic process for collaborating with context experts covering critical stages of data collection, annotation, and model validation strategies. Our work offers insights for researchers and practitioners in machine learning for development (ML4D) and highlights challenges stemming from small and imbalanced datasets, the need to balance non-glamorous data work and stakeholder priorities, and closed data sharing practices. These findings motivate further research exploring strategies for data augmentation, non-text-based detection models, multimodal hate speech detection, and best practices for working with non-machine learning experts on machine learning-powered projects. 

\textbf{Ethics statement:} 
We hope this work can support efforts to protect the integrity of social media platforms and defend democracies against the threats of hate speech, disinformation, and misinformation. We understand that automated tools such as those proposed in this work could be misused to target and oppress dissenting voices, especially within authoritarian regimes. We unequivocally state that such use will be contrary to our core goal of offering ideas for combating harmful content online.