\section{Theoretical Understanding}
\label{sec:theory}

In this section, we will show some primary efforts on understanding the effect of neural networks on LPN problems.
Because the general understanding of the optimization power and generalization effect of the neural network is very limited, the results in this 
section can't fully explain all the features of our algorithms and our hyperparameter choices. 
However, this theoretical analysis improves our understanding of our empirical findings and hence is recorded here.

This section is organized according to the decomposition in~\Cref{eq:decompose}. 
\Cref{sec:theory_representation} shows that the representation gap of the model architecture MLP over all loss is optimal over the whole function class $f:\R^d \to [0,1]$.
\Cref{sec:theory_optimization} shows that despite the inevitable exponential dependency on the dimension, the time complexity of~\Cref{alg:abundant_theory} scaled optimally with respect to noise.
\Cref{sec:theory_generalization} introduces some prior results that partially explain why weight decay is a powerful regularization as discovered by our ablation study in~\Cref{sec:restricted}.  \Cref{sec:theory_hardness} provides a detailed discussion on the hardness of LPN problem using gradient-based methods.


\subsection{Representation Power}
\label{sec:theory_representation}









The main result of this section is the following theorem, which shows that an MLP with a width equivalent to the input dimension is sufficient to represent the best prediction of the LPN inputs.


\begin{theorem}\label{thm:rep}
    For any continuous loss function $l: [0,1] \times \{0,1\} \to \R^+$, dimension $n$ and error rate $\tau$, there exists a weight $\weight$ for a depth 1 MLP $\model$ with width $n$, $\sigma_1 = ReLU$, $\sigma_2 = \sigmoid$, such that the representation gap $\populoss(f)$ 
    of the specified function $f = \model[\weight]$ is approximately minimized over the function class $\functionclass = \{ f':\R^d \to [0,1] \}$. Quantitatively, for any $\epsilon > 0$, there exists $\weight_{\epsilon}$, such that 
    \begin{align*}
        \populoss(\model(\weight_\epsilon)) - \min_{f \in \functionclass} \populoss(f) \le \epsilon.
    \end{align*}
\end{theorem}

\begin{proof}\label{proof:rep}
    For LPN problems with dimension $n$, secret $s$ and error rate $\tau$, it holds that 
    \begin{align*}
        \populoss(f) &= \E_{x \sim U(\Z_2^n)}[\tau \ell(f(x), s^t x \mod 2) + (1 - \tau) \ell(f(x), (s^t x + 1) \mod 2)] \\
        &\ge \E_{x \sim U(\Z_2^n)}[\min_{p \in [0,1]} \tau \ell(p, s^t x \mod 2) + (1 - \tau) \ell(p, (s^t x + 1) \mod 2) ]. \\
    \end{align*}

    We will now show that in fact this lower bound can be reached approximately when $f$ is specified by a weight $\weight$ for a depth 1 MLP $\model$ with width $n$, $\sigma_1 = \relu$, $\sigma_2 = \sigmoid$.

    By~\Cref{lem:rep}, there exists a weight $\weight'$  for a depth 1 MLP $\model'$ with width $n$, $\sigma_1 = \relu$, $\sigma_2 = \identity$ such that
    \begin{align*}
        \forall x \in \{0,1\}^n, \model'[\weight'](x) = s^tx \mod 2.
    \end{align*}

    Given $l$ is continuous, for $b \in \{0,1\}$, we can find $a_{b,\epsilon} \in (0,1)$, such that
    \begin{align*}
        \tau \ell(a_{b,\epsilon}, b) + (1 - \tau) \ell(a_{b,\epsilon}, 1 - b) \le \min_{p \in [0,1]} \ell(p, b) + (1 - \tau) \ell(p, 1 - b)  + \epsilon.
    \end{align*}

    Further define $\gamma_{b,\epsilon}$ satisfies $\sigmoid(\gamma_{b,\epsilon}) = a_{b, \epsilon}$.
    For $\weight$ satisfying
    \begin{align*}
        \weight_1 &= \weight'_1, \\
        b_1 &= b'_1, \\
        \weight_2 &= (\gamma_{1,\epsilon} - \gamma_{0,\epsilon})\weight'_2, \\
        b_2 &= \gamma_{0,\epsilon},
    \end{align*}
    it holds that $\model[\weight](x) = a_{s^tx \mod 2, \epsilon}$. This implies $\populoss(\model[\weight]) \le \epsilon + \min_{f \in \functionclass} \populoss(f)$. The proof is complete.
\end{proof}

The above proof relies on~\Cref{lem:rep}, whose proof is as follows.

\begin{lemma}\label{lem:rep}
    For any secret $s \in \{0,1\}^n$, there exists a weight $\weight$ for a depth 1 MLP $\model$ with width $n$, $\sigma_1 = \relu$, $\sigma_2 = \identity$, such that
    \begin{align*}
        \forall x \in \{0,1\}^n, \model[\weight](x) = s^tx \mod 2  
    \end{align*}
\end{lemma}

\begin{proof}
    Recall in~\Cref{def:mlp}, the weight $\weight$ is a list of two matrices tuples specifying the affine transformation on each layer. 
    
    We will choose $\weight_1 = es^t$ with $e$ as the all $1$ $n-$dimensional vector, $b_1 = [0, -1 ,...,-n + 1]^t$. Then we would have 
    \begin{align*}
        \sigma_1(T[\weight_1, b_1](x)) = \sigma_1((s^tx) e - b_1) = [\max\{(s^tx) - i + 1, 0\}]^t_{i \in [1:n]}. 
    \end{align*}

    We will further define $b_2 = 0$ and recursively $\weight_2 \in \R^n$ as 
    \begin{align*}
        &\weight_{2,1} = 1. \\
        &\weight_{2,i} = i \mod 2 - \sum_{j = 1}^{i - 1} \weight_{2,j} (i - j + 1). \quad d -1 \ge i \ge 2
    \end{align*}

    It is then easy to check
    \begin{align*}
        T[\weight_2, b_2]\left( \sigma_1(T[\weight_1, b_1](x)) \right) = \sum_{i = 1}^n \weight_{2,i} \max\{(s^tx) - i + 1, 0\} = s^tx \mod 2.
    \end{align*}
    The proof is then complete.
\end{proof}

\subsection{Optimization Power}
\label{sec:theory_optimization}
While~\Cref{sec:theory_representation} shows that an MLP with width $n$ is sufficient to minimize the representation gap, it does not 
show how may we find such representation. As mentioned in~\Cref{sec:prelim}, it is common practice to use the gradient method to optimize 
neural networks. Pitifully,~\cite{abbe2020poly} has shown that it requires at least $n^{\Omega(\text{Hamming weight of secret})}$ time complexity to 
use any gradient method to solve LPN instance.~\cite{barak2022hidden}  shows through experimental and theoretical analysis 
that the time complexity to solve LPN instance with noise rate $0$ (known as the \emph{parity problem}) with neural networks seems to match this lower 
bound closely. In this section, we show that despite this exponential reliance on the problem dimension, the training of neural networks is robust to 
the noise rate $\tau$ in~\Cref{setting:abundant} by the following theorem.

\begin{theorem}
\label{thm:optimization}
    If there exists weight initialization $\weight_0$,  learning rate $\eta$, weight decay parameter $\lambda < 1/\eta$, and step threshold $T$ such that~\Cref{alg:abundant_theory} can return a model with 
    accuracy at least $\gamma > \frac{1}{2}$ on LPN problem with dimension $n$, secret $s$ and noise rate $0$, for any batch size $B$ greater than threshold $B_{th}$ with constant probability $p > 0$, then for any $\gamma' < \gamma, p' < p$,
    there exists another threshold $B_{th, \tau} = \max \left(O(\frac{1}{(1 - 2\tau)^2}) , B_{th} \right)$, such that~\Cref{alg:abundant_theory} can return a model with 
    accuracy at least $\gamma' \tau + (1 - \gamma')(1 - \tau)$ on LPN problem with dimension $n$, secret $s$ and noise rate $\tau$ when batch size $B \ge B_{th, \tau}$ and learning rate $\eta' = \frac{\eta}{1 - 2\tau}$ with probability $p'$ with all other hyperparameters fixed. 
\end{theorem}


As the time complexity of~\Cref{alg:abundant_theory} is $O(BT)$,~\Cref{thm:optimization} shows that with neural network can solve the LPN problem with noise rate $\tau$ in time complexity $O(\frac{1}{(1 - 2\tau)^2})$ under~\Cref{setting:abundant}, 
given the underlying parity problem can be solved by the same neural network with constant probability. This rate coincides with the sample complexity of hypothesis testing on whether a boolean vector is $s$ and is in this sense \emph{optimal}.

One would naturally ask whether corresponding results exist under the two other settings. We observe empirically that as apposed to the high converging accuracy in~\Cref{setting:abundant}, under~\Cref{setting:moderate}, the final converging model would have low train accuracy (though greater than $50\%$) despite the training time.
In~\Cref{setting:abundant}, however, the training accuracy can tend to $1$ in a short period of time while the testing accuracy increases slowly, in many cases after the convergence of training accuracy. 
These interesting phenomenons show that the optimization dynamics may be highly different under the three settings and it remains an open question to fully characterize 
how different sample complexity shapes the optimization landscape of neural networks in the LPN problem.


\begin{proof}[Proof of~\Cref{thm:optimization}]


    We will denote the weight sequence generating by applying~\Cref{alg:abundant_theory} with batch size $B$ and learning rate $\eta$ on LPN instance with dimension $n$ and noise rate $\tau$ as $W_t$.
    Assuming the corresponding batch is $\dataset_t = \{(x_{t,i}, y_{t,i})\}$ for $t \le T$. 
    We will further define $\dataset_{t,\tau} = \{(x_{t,i}, y_{t,i} + f_{t,i} \mod 2)\}$ where $f_{t,i}$ are independent boolean variables which equals to $1$ with probability $\tau$.
    Finally, we will define the coupled weight sequence $\weight_{t, \tau}$ as  weight sequence generating by applying~\Cref{alg:abundant_theory} with learning rate $\eta/(1 - 2\tau)$ and batches $\dataset_{t,\tau}$.

    Consider the following sequence of weights,
    \begin{align*}
        \tw_0 &= W_0 \\
        \tw_t &= \tw_{t-1} - \eta\lambda \tw_{t-1} - \eta \nabla_{\weight} \E_{ {x \sim U(\Z_2^n)}} l(\model[\tw_{t-1}](x), s^t x \mod 2 ), 1 \le t \le T, t\in \Z.
    \end{align*}
    

    By standard approximation theorem, when the batch size $B$ tends to infinity, we would have $W_t \to \tw_t$ in probability. This implies $\tw_t$ corresponds to a function with accuracy at least $\gamma$ on LPN data without noise.
    We can choose $r$ small enough such that for any weight $\weight$ in $r$ neighbors of $\tw_T$ all correspond to a function with accuracy at least $\gamma'$ on LPN data without noise.
    Suppose now when $B \ge B_r > B_{th}$, we would have with probability $1 - \frac{p - p'}{2}$, $\|\weight_{t} - \tw_t \| \le \frac{r}{2}$.
    Here $\tilde W_l$ and $\tilde b_l$ are the corresponding weight and bias of $\tilde W$.
    We would now choose a compact convex set $\mathcal{C}$ large enough such that it contains $\{\tw_t\}_{t \in [0:T]}$ and their $r-$neighbor. By our assumption, we would have for any fixed $x \in \{0,1\}^n, y \in \{0,1\}^n$, and $w \in \{W_1, W_2, b_1, b_2\}$,
    $\frac{\partial l(M[\weight](x), y)}{\partial w}$ is first-order differentiable functions, hence we can assume that there exists constant $C_1$ and $C_2$, such that,
    \begin{align*}
        &C_1 \ge \max_{\weight = ((W_1,b_1),(W_2,b_2)) \in \mathcal{C}} \max_{w \in \{W_1, W_2, b_1, b_2\}} \max_{x \in \{0,1\}^n, y \in \{0,1\}} \left\| \frac{\partial \model[\weight](x_i)}{\partial w} \right \|_{\infty} \\
        &C_2 \|W_a - W_b \|_2 \ge \max_{x \in \{0,1\}^n, y \in \{0,1\}} \left\| \frac{\partial l(\model[\weight](x),y)}{\partial \weight} \mid_{\weight = \weight_a} - \frac{\partial l(\model[\weight](x),y)}{\partial \weight} \mid_{\weight = \weight_b} \right\|
    \end{align*}

    We will first fix $\epsilon$ be a small constant such that $e^{C_2T} \epsilon \le \frac{r}{2}$ and then choose $B_{th,\tau} = \max\{\frac{2 \ln T + \ln nd - \ln(p - p')}{(1 - 2\tau)^{2}} \frac{32ndC_1^2}{\epsilon^2}, B_{r} \}$.
    When $B \ge B_{th,\tau}$, we will inductively prove that for step $t \le T$, event $E_t: \|\weight_{t,\tau} - \weight_{t}\|_2 \le e^{\eta C_2 t} t \epsilon $ happens with probability $1 - \frac{(p - p')t}{2T} - \frac{(p - p')}{2}$.
    
    We would first suppose $\|W_t - \tw_t\| \le \frac{r}{2}$ by the definition of $B_{r}$.
    Suppose $E_t$ happens, as $e^{\eta C_2 t} \epsilon \le \frac{r}{2}$, we would have the $\weight_{t,\tau} \in C$.
    By~\Cref{lem:estimate_gradient},  it holds that with probability $1 - (p - p')/2T$, for any parameter $w$ in $\weight_{t,\tau}$, it holds
    that
    \begin{align*}
        \left \| \frac{1}{1 - 2 \tau}\frac{\partial \frac{1}{B} \sum_{i} l\left(\model[\weight](x_{t,i}), f_{t,i} + y_{t,i} \mod 2 \right)}{ \partial w} - \frac{\partial \frac{1}{B} \sum_{i} l\left(\model[\weight](x_{t,i}), y_{t,i} \right)}{ \partial w} \right \|_2 \le \frac{\epsilon}{4}.
    \end{align*}
    Considering the update rule of the SGD optimizer,
    \begin{align*}
        \left \| \weight_{t+1, \tau} - \weight_{t,\tau} - \eta \lambda \weight_{t,\tau} - \eta \frac{\partial \frac{1}{B} \sum_{i}l\left(\model[\weight](x_{t,i}), y_{t,i} \right)}{ \partial \weight} \mid_{\weight = \weight_{t,\tau}} \right\|_2 \le \epsilon.
    \end{align*}

    This then implies
    \begin{align*}
        \| \weight_{t + 1, \tau} - \weight_{t + 1} \|_2 &\le 
        \epsilon + \| \weight_{t, \tau} - \weight_t \|_2 + \eta C_2 \| W_{t,\tau} - W_t\|_2 \\
        &\le \epsilon + \exp(\eta C_2) \exp(\eta C_2 t) t \epsilon \\
        &\le e^{\eta C_2 (t + 1)} (t + 1) \epsilon.
    \end{align*}
    The induction is then complete.

    Considering the induction conclusion when $t = T$ and combining the definition of $r$, the proof is complete.
\end{proof}







\begin{lemma}
\label{lem:estimate_gradient}
    With loss function $l$ being the MAE loss and $\model$ being a one-layer MLP with weight $d$ and smooth activation function, suppose $(x_i,y_i)_{i \in [1:B]}$ are i.i.d sample from LPN problem with dimension $n$ and noise rate $0$. Suppose further $f_i$ are i.i.d
    random variables following distribution $p(f_1 = 0) = 1 - p(f_1 = 1) = \tau$, for any weight $\weight = ((W_1,b_1),(W_2,b_2))$ for $\model$, it holds that for $w \in \{W_1, W_2, b_1, b_2\}$, with probability $1 - 8nd\exp \left( \frac{- \epsilon^2 (1 - 2\tau)^2B}{2ndC^2}\right)$,
    \begin{align*}
        \left \| \frac{1}{1 - 2 \tau}\frac{\partial \frac{1}{B} \sum_{i = 1}^B l\left(\model[\weight](x_i), f_i + y_i \mod 2 \right)}{ \partial w} - \frac{\partial \frac{1}{B} \sum_{i = 1}^B l\left(\model[\weight](x_i), y_i \right)}{ \partial w} \right \|_2 \le \epsilon.
    \end{align*}
    Here $C = \max_{w \in \{W_1, W_2, b_1, b_2\}}\max_{x \in \{0,1\}^n, y \in \{0,1\}} \| \frac{\partial \model[\weight](x_i)}{\partial w} \|_{\infty}$.
\end{lemma}
\begin{proof}
    Denote $n_i = 2f_i - 1  \in \{-1,1\}$.
    The key observation of this proof is that for the MAE loss, it holds that 
    \begin{align*}
        \frac{\partial l\left(\model[\weight](x_i), f_i + y_i \mod 2 \right)}{\partial w} = n_i\frac{\partial l\left(\model[\weight](x_i), f_i  \right)}{\partial w}.
    \end{align*}


    Now consider any index $k$ of $w$ ($k$ can be two dimensional for a matrix), $n_i\frac{\partial l\left(\model[\weight](x_i), f_i  \right)}{\partial w_k}$ are bounded random variable in $[-C,C]$, then by Hoeffding's bound,  it holds for any $t > 0$
    \begin{align}\label{eq:hoeffding}
        &\pr \left( \left| \sum_{i = 1}^B n_i\frac{\partial l\left(\model[\weight](x_i), f_i  \right)}{\partial w_k} - \E\left(\sum_{i = 1}^B n_i\frac{\partial l\left(\model[\weight](x_i), f_i  \right)}{\partial w_k}\right) \right| \ge t\right)  \notag
        \\&\le 2\exp\left( -\frac{t^2}{2BC^2}\right).
    \end{align}

    Now we have
    \begin{align*}
        \sum_{i = 1}^B n_i\frac{\partial l\left(\model[\weight](x_i), f_i  \right)}{\partial w_k} &= \frac{\partial \frac{1}{B} \sum_{i = 1}^B l\left(\model[\weight](x_i), f_i + y_i \mod 2 \right)}{\partial w_k}. \\
        \E\left[\sum_{i = 1}^B n_i\frac{\partial l\left(\model[\weight](x_i), f_i  \right)}{\partial w_k}\right] &= (1 - 2\tau) \frac{\partial \frac{1}{B} \sum_{i = 1}^B l\left(\model[\weight](x_i), y_i  \right)}{\partial w_k}.
    \end{align*}

    By~\Cref{eq:hoeffding}, by choosing $t = (1 - 2\tau)\frac{B\epsilon}{\sqrt{nd}}$, we would have with probability $1 - 8nd\exp \left( \frac{- \epsilon^2 (1 - 2\tau)^2B}{2ndC^2}\right) $, it holds that 
    \begin{align*}
        \left \| \frac{1}{1 - 2 \tau}\frac{\partial \frac{1}{B} \sum_{i = 1}^B l\left(\model[\weight](x_i), f_i + y_i \mod 2 \right)}{ \partial w} - \frac{\partial \frac{1}{B} \sum_{i = 1}^B l\left(\model[\weight](x_i), y_i \right)}{ \partial w} \right\|_2 \le \epsilon.
    \end{align*}
    The proof is then complete.
\end{proof}


\subsection{Generalization Effect}
\label{sec:theory_generalization}

In modern deep learning theory, explaining the generalization effect of neural network is a long standing open problem. 
Under~\Cref{setting:abundant}, the generalization gap is naturally zero as the data distribution coincides with the population distribution.
However, under~\Cref{setting:moderate} and~\Cref{setting:restricted}, it would be necessary for us to consider generalization effect.
As we observed in~\Cref{sec:experiment}, under~\Cref{setting:moderate}, the generalization gap is still small and the key difficulty lies in the optimization 
of the neural network. Under~\Cref{setting:restricted}, however, the gap between training accuracy and test accuracy is typically large.
This result is expected because as the sample complexity decreases, the optimization landscape over population distribution and data distribution tends 
to differ. Our experiment shows that applying L2 regularization, or equivalently, weight decay, helps to mitigate this problem and reduce the learning rate.

Although the optimization dynamics of neural networks over the general LPN problem is hard to track, existing literature contains results showing the provable benefit of weight decay on sample complexity under a special setting. We include this result in this subsection for completeness. It remains an open problem to extend this result to secret $s$ with hamming weight proportional to the problem dimension.

\begin{theorem}[Informal Version of Theorem 1.1 in~\cite{wei2019regularization}]
\label{thm:wd}
     If a gradient-based optimization algorithm uses SGD optimizer, logistic loss, MLP with depth $1$ initialized by Kaiming initialization, weight decay $0$, and a small enough learning rate, the sample complexity required to learn the secret of a parity problem with dimension $n$ and Hamming weight $2$ secret is $\Omega(n^2)$.
    However, with proper weight decay constant $\lambda > 0$, the sample complexity of the same problem could be reduced to $\tilde O(n)$.
\end{theorem}


\subsection{Discussion on the Hardness of Using Neural Networks to Solve LPN}
\label{sec:theory_hardness}

As mentioned in~\Cref{sec:theory_optimization}, it is proved in~\cite{abbe2020poly,shalev2017failures} that it requires $n^{\Omega(k)}$ sample complexity to solve the parity problem without noise using the full batch gradient descent method, which is also the regime our~\Cref{thm:optimization} falls into. 

However, this hardness constraint fails to hold for the stochastic gradient descent method. In fact~\cite{abbe2020poly} show that if a family of distribution is polynomial-time learnable, then there exists a polynomial-size neural network architecture, with a polynomial-time computable initialization that only depends on the family of the distribution, such that when stochastic gradient descent with batch size $1$ is performed on the network, it can learn the distribution in polynomial time complexity. We would like to remark some implications of these results.
\begin{itemize}
    \item These results suggest that whether there exists an architecture and initialization scheme such that~\Cref{alg:abundant_theory} can solve the LPN problem in polynomial time remains open and is inherently equivalent to whether LPN is in P.
    \item The construction of the neural network that simulates the polynomial time learning algorithm given by~\cite{abbe2020poly} relies heavily on deterministic initialization and it conjectures that SGD on random initialization will still require super polynomial time or sample complexity. 
    \item Although the full-batch gradient method will require $n^{\Omega(k)}$ time and sample complexity to solve the parity problem, this does not exclude the gradient method from being used to solve the LPN problem. First, it is still possible to use the gradient method as a building block of a larger algorithm that can effectively solve LPN. Second, given the best exponent component is not known, it is still possible that neural networks will show supreme performance over other classical algorithms, especially in the medium dimension with high noise regime, which our~\Cref{alg:moderate} tries to address.
\end{itemize}
