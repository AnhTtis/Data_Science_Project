

\section{Preliminary}\label{sec:prelim}

\paragraph{Notations and terminology.} 
In cryptography, the security parameter is a variable that is used to parameterize the computational complexity of the cryptographic algorithm or protocol, and the adversary's probability of breaking security. 

Let $\R, \Z, \N$ be the set of real numbers, integers, and positive integers. 
For $q\in\N_{\geq 2}$, denote $\Z/q\Z$ by $\Z_q$. 
For $n\in\N$, $[n] := \set{1, ..., n}$. A vector in $\R^n$ (represented in column form by default) is written as a bold lower-case letter, e.g. $\ary{v}$. For a vector $\ary{v}$, the $i^{th}$ component of $\ary{v}$ will be denoted by $v_i$. %
A matrix is written as a bold capital letter, e.g. $\mat{A}$. The $i^{th}$ column vector of $\mat{A}$ is denoted $\ary{a}_i$. 
The length of a vector is the $\ell_p$-norm $\|\ary{v}\|_p := (\sum v_i^p)^{1/p}$, or the infinity norm given by its largest entry $\|\ary v\|_{\infty} := \max_i\{|v_i|\}$. 
The length of a matrix is the norm of its longest column: $\|\mat{A}\|_p := \max_i \|\ary{a}_i\|_p$. 
By default, we use $\ell_2$-norm unless explicitly mentioned. 
For a binary vector $\ary{v}$, let $\HW(\ary{v})$ denote the Hamming weight of $\ary{v}$.
Let $B^n_p$ denote the open unit ball in $\R^n$ in the $\ell_p$ norm.
We will write $x\ e\ y$ as short hands for $x \times 10^y$.

When a variable $v$ is drawn uniformly random from the set $S$ we denote it as $v\la U(S)$. 
When a function $f$ is applied on a set $S$, it means $f(S) := \sum_{x\in S}f(x)$. %




\subsection{Learning Parity with Noise}\label{sec:prelim_coding}


The learning parity with noise problem (LPN) is defined as follows

\begin{definition}[LPN~\cite{DBLP:conf/crypto/BlumFKL93,DBLP:journals/jacm/BlumKW03}] \label{def:lpn}
Let $n \in\N$ be the dimension, $m\in\N$ be the number of samples, $\tau\in(0, 1/2)$ be the error rate. 
Let $\eta_\tau$ be the error distribution that output 1 with probability $\tau$, 0 with probability $1-\tau$. 
A set of $m$ LPN samples is obtained from sampling $\ary{s}\la U(\Z_2^n)$, $\mat{A}\la U(\Z_2^{n\times m})$, $\ary{e}\la\eta_\tau^m$, and outputting $(\mat{A}, \ary{y}^t := \ary{s}^t\mat{A}+\ary{e}^t \mod 2)$.

We say that an algorithm solves $\LPN_{n, m, \tau}$ if it outputs $\ary{s}$ given $\mat{A}$ and $\ary y$ with non-negligible probability. 
\end{definition}


An algorithm solves the decisional version of LPN if it distinguishes the LPN sample $\LPN_{n, m, \tau}$ from random samples over $\Z_2^{n\times m}\times \Z_2^m$ with probability greater than $1/2 + 1/poly(n)$.
The decisional LPN problem is as hard as the search version of LPN~\cite{DBLP:conf/crypto/BlumFKL93}.

The LPN problem reduces to a variant of LPN where the secret is sampled from the error distribution~\cite{DBLP:conf/crypto/ApplebaumCPS09}. The reduction is simple and important for our application so we sketch the theorem statement and the proof here. 

\begin{lemma}
\label{lem:s_sparse}
If $\LPN_{n, m, \tau}$ is hard, then so is the following variant of LPN: we sample each coordinate of the secret $\ary{s}\in\Z_q^n$ from the same distribution as the error distribution, i.e., $\eta_\tau$, and then output $m-n$ LPN samples. 
\end{lemma}
\begin{proof}
Given $m$ standard LPN samples, denoted as $(\mat{A}, \ary{y}^t := \ary{s}^t\mat{A}+\ary{e}^t \mod 2)$. 
Write $\mat{A} = [\mat{A}_1 \mid \mat{A}_2]$ where $\mat{A}_1\in\Z_2^{n\times n}$. Without a loss of generality, assume $\mat{A}_1$ is invertible (if not, pick another block of $n$ full-rank columns from $\mat{A}$ as $\mat{A}_1$). 
Write $\ary{y}^t = [\ary{y}^t_1 \mid \ary{y}^t_2]$ where $\ary{y}_1\in\Z_2^n$.  
Let $\bar{\mat{A}}:= -\mat{A}_1^{-1}\cdot \mat{A}_2$. 
Let $\bar{\ary{y}}^t := \ary{y}^t_1\cdot \bar{\mat{A}} + \ary{y}^t_2$. Then 
$\bar{\ary{y}}^t = (\ary{s}^t\mat{A}_1+\ary{e}^t_1)\cdot (-\mat{A}_1^{-1}\cdot \mat{A}_2) + (\ary{s}^t\mat{A}_2+\ary{e}^t_2) = \ary{e}^t_1\cdot \bar{\mat{A}} + \ary{e}^t_2$, meaning that
$\bar{\mat{A}}, \bar{\ary{y}}^t$ is composed of $m-n$ LPN samples where the secret is sampled from the error distribution.
\end{proof}


\subsection{Machine Learning}

\paragraph{Supervised Learning}The goal of supervised learning is to learn a function that maps inputs to labels. The input $x \in \mathcal{X}$ and the label $y  \in \mathcal{Y}$ are usually assumed to obey a fixed distribution $\distribution$ over $\mathcal{X} \times \mathcal{Y}$. Usually, $\distribution$ is not directly accessible to the learner, instead, another distribution $\datadistribution$, known as empirical distribution, is provided to the learner. This distribution is usually a uniform distribution over a finite set of inputs and labels $\trainingset \triangleq \{ (x_i, y_i)\}_{i \in [1:N]}$. This set $\trainingset$ is usually named \textit{training set} and $(x_i,y_i)$ is assumed to obey $\distribution$ independently.



The goal of \textit{learning} is to choose from a function class $\functionclass$ a function $f: X \to Y$  given $\datadistribution$. To measure the quality of $f$, \textit{loss function} $\ell: Y \times Y \to \nonneg$ is often considered. We now provide some examples of loss functions that will be used in our paper.

\begin{definition}[Zero-one Loss]
    $\zerooneloss(y_1, y_2) = \one{y_1 \neq y_2}$.
\end{definition}

\begin{definition}[Logistic Loss]
\label{def:logistic}
    $\logisticloss(y_1, y_2) = -y_2 \log( 1 - y_1) - (1 - y_2) \log y_1, y_2 \in \{0,1\},y_1 \in [0,1]$.
\end{definition}


\begin{definition}[Mean Absolute Error Loss]
    $\maeloss (y_1, y_2) = |y_1 - y_2|$.
\end{definition}

\begin{definition}[Mean Square Error Loss]
    $\mseloss (y_1, y_2) = |y_1 - y_2|^2$.
\end{definition}

\begin{definition}
    Given a loss function $\ell: Y \times Y \to \nonneg$, the \emph{population loss} $\populoss: \functionclass \to \nonneg$  is defined as
    \begin{align*}
        \populoss(f) = \E_{(x,y) \sim \distribution}[\ell(f(x), y)].
    \end{align*}
\end{definition}

For zero-one loss, $1$ minus the expected loss is also called \textit{accuracy}. We usually abuse this notation when $f$'s co-domain is $[0,1]$ by calling the accuracy of the rounding of $f$ as the accuracy of $f$. The \emph{training accuracy} is defined as accuracy with the underlying population as the uniform distribution over the training set.
The goal of learning can then be rephrased to find $f \in \functionclass$ with low population loss. Two questions then naturally arise, (1) How to evaluate population loss? and (2) How to effectively minimize population loss?

To evaluate the loss, the \textit{test set} $\testset \triangleq \{(x_i, y_i) \}_{i \in [N+1, N+M]}$ is usually considered. The element in $\testset$ is also assumed to obey $\distribution$ independently and is also independent of the elements in $\trainingset$. 
\begin{definition}
    Given a loss function $\ell: Y \times Y \to \nonneg$, the \emph{test loss} $\testloss: \functionclass \to \nonneg$  is defined as
    \begin{align*}
        \testloss(f) = \frac{1}{M}\sum_{i = N+1}^{N+M} \ell(f(x_i), y_i).
    \end{align*}
    The \emph{test accuracy} is defined as accuracy with the underlying population as the uniform distribution over the test set.
\end{definition}
When $f$ is chosen by the learning algorithm given the training data, $\testloss(f)$ can then serve as an unbiased estimator of $\populoss(f)$. In the traditional machine learning community, $\testloss$ is usually only measured once after training, and another set called \textit{validation set} is used to track the performance of the algorithm through the course of the training. However, this boundary is blurred in modern literature and we ignore this subtlety here because our final objective is to utilize machine learning to solve LPN secrets instead of fitting the data. 

To effectively minimize the loss, the learner would use a learning algorithm $\algo$ that maps training distribution to a function $f \in H$ (usually with randomness). As the learner only has access to the data distribution, $\algo$ is usually designed to minimize \emph{training loss}.
\begin{definition}
    Given a loss function $\ell: Y \times Y \to \nonneg$, the \emph{training loss} $\trainloss: \functionclass \to \nonneg$  is defined as
    \begin{align*}
        \trainloss(f) = \frac{1}{M}\sum_{i = 1}^{M} \ell(f(x_i), y_i).
    \end{align*}
\end{definition}


When trying to characterize the gap between the learned function and the best available function in the function class, the following decomposition is common in machine learning literature.
\begin{align}
&\populoss(\algo(\datadistribution)) - \underbrace{\min_{f \in \functionclass} \populoss(f)}_{\mathrm{Representation\ Gap}} \notag\\
=&\underbrace{\populoss(\algo(\datadistribution)) - \trainloss(\algo(\datadistribution))}_{\mathrm{Generalization\ Gap}} + \underbrace{\trainloss(\algo(\datadistribution)) - \trainloss(f^*) }_{\mathrm{Optimization\ Gap}} \notag \\&+ \underbrace{ \trainloss(f^*)  - \populoss(f^*)}_{\mathrm{Stochastic\ Error}}, \quad f^* = \argmin_{f \in \functionclass} \populoss(f). \label{eq:decompose}
\end{align}

The three gaps in the above equation characterize different aspects of machine learning. 
\begin{enumerate}
    \item The function class needs to be chosen to be general enough to minimize the representation gap.
    \item The learning algorithm needs to be chosen to find the best trade-off between the generalization gap and the optimization gap given the function class.
\end{enumerate}
In the recent revolution brought by neural networks, it is shown that choosing the function class as $\{ f \mid f \text{ can be represented by a fixed neural architecture}\}$ and learning algorithm as the gradient-based optimization method can have surprising effects over various domains. We will now briefly introduce neural networks and gradient-based optimization algorithms.

\paragraph{Neural Networks}

Neural networks are defined by \emph{architecture}, which maps differentiable weights to a function from $\mathcal{X}$ to $\mathcal{Y}$. This function is called the \emph{neural network} and the weights are called the \emph{parameterization} of the network. The most simple architecture is \emph{Multi-Layer Perceptron (MLP)}.
\begin{definition}[MLP]
\label{def:mlp}
    Multi-Layer Perceptron is defined as a mapping $\model$ from $\R^{d_1}$ to $\R^{d_{L+1}}$, with
    \begin{align*}
        \model[\theta_1, ..., \theta_L](x) = (\sigma_L \circ T[\theta_L] \circ  ... \circ \sigma_1 \circ T[\theta_1])(x),
    \end{align*}
    where $\theta_i = (W_i, b_i), W_i \in \R^{d_{i+1} \times d_{i}}, b_i \in \R^{d_{i+1}}$ and $T[\theta_i]$ is an affine function with $T[\theta_i](x) = W_ix + b_i$. $\sigma_i: \R^d \to \R^d $ is a function that is applied \emph{coordinate-wise} and is called \emph{activation function}. $L$ and $L - 1$ are called the number of \emph{layers} and \emph{depth} of the MLP, and $\{ d_i \}_{i = 2,..,L}$ is  called the \emph{widths} of the MLP.
\end{definition}

We now provide some examples of activation functions that will be used in our work.

\begin{definition}[ReLU]
$\relu: \R^d \to \R^d$ is defined as $(\relu(x))_i = x_i \one{x_i \ge 0} $.
\end{definition}


\begin{definition}[Sigmoid]
$\sigmoid: \R^d \to \R^d$ is defined as $(\sigmoid(x))_i = \frac{1}{1 + e^{-x_i}} $.  
\end{definition}

\begin{definition}[Cosine]
$\cosine: \R^d \to \R^d$ is defined as $(\cosine(x))_i = \cos(x_i) $.  
\end{definition}

The base model we used in this work is defined as followed.
\begin{definition}[Base Model]
\label{def:basemodel}
    Our base model is defined as MLP with depth $1$ with activation $\sigma_1 = \relu$ and $\sigma_2 = \sigmoid$. We will denote this model as $\basemodel_{d}$ with $d$ specifying $d_2$. We will write $\basemodel$ with activation $\sigma$ to indicate replacing $\sigma_1$ by $\sigma$. 
\end{definition}


\paragraph{Gradient-based Optimization}

It is common to use gradient-based optimization methods to optimize the neural network. A standard template is shown in~\Cref{alg:optim_example}. The unspecified parameters such as $\model, \weight$, and $\ell$ in the algorithm are often called \emph{hyperparameters}. 

\begin{algorithm}
\caption{Gradient-based Optimization}\label{alg:optim_example}
\begin{algorithmic}
\Require $\text{A neural network architecture }\model$
\Require $\text{An initialization parameter for the model }\weight_0$
\Require $\text{A differentiable loss function } \ell$
\Require $\text{A Stop Criterion } \stopcriterion$ 
\Require $\text{A Data Sampler } \sampler$ \Comment{See~\Cref{def:sampler}}
\Require $\text{An Optimizer } \optimizer$ \Comment{See~\Cref{def:optimizer}}
\Require $\text{A Regularization Function } \regularization$  \Comment{See~\Cref{def:regularization}}
\State $step \gets 0$
\While{$\stopcriterion$ \text{ is not reached}} \Comment{See~\Cref{def:stop_criterion}}
\State $f \gets \model[\weight_{step}]$
\State $\batch = \{(x_i, y_i)\}_{i = 1,...,B} \gets \sampler.GetData()$.
\State $\batchloss \gets \frac{1}{B} \sum_{i = 1}^B l(f(x_i), y_i) + \regularization(\weight_{step})$.  \Comment{Calculate regularized loss}
\State  $g_W \gets -\frac{\partial \batchloss}{\partial W} \mid_{W = W_{step}}$. \Comment{Calculate gradient w.r.t the model parameter}
\State $W_{step + 1} \gets \optimizer.GetUpdate(W_{step}, g_W)$.
\State $step \gets step + 1$.
\EndWhile
\State \Return $\model[\weight_{step}]$
\end{algorithmic}
\end{algorithm}

\begin{definition}[Sampler]
\label{def:sampler} 
A \emph{sampler} is a finite-state machine, on each call of method $GetData$, it will return a set of $B$ samples $\{(x_i, y_i)\}$ satisfying $x_i,y_i \sim \datadistribution$. The number $B$ is called \emph{batch size} and the $B$ samples are called a \emph{batch}.
\end{definition}

We hereby provide two examples of samplers that will be used in our papers. \Cref{alg:fix_sample} is the sampler used in Settings~\ref{setting:restricted} and~\ref{setting:moderate} and \Cref{alg:inf_sample} is the sampler used in \Cref{setting:abundant}.

\begin{algorithm}
\caption{Batch Sampler}\label{alg:fix_sample}
\begin{algorithmic}
\Params
 \State $\trainset = \{(x_i, y_i)\}_{i = 1,..,N}$, training set
\State $B \le N$, batch size
\EndParams
\Procedure{GetData}{$\ $}
\State Sample i.i.d from $\{1,2,..,N\}$ $B$ index to form index set $I_{\text{Batch}}$
\State $\batch = \{ (x_i, y_i) \mid i \in  I_{\text{Batch}}\}$.
\State \Return $\batch$.
 \EndProcedure
\end{algorithmic}
\end{algorithm}



\begin{algorithm}
\caption{Oracle Sampler}\label{alg:inf_sample}
\begin{algorithmic}
\Params
 \State $\distribution$, the underlying distribution
\State $B \le N$, batch size
\EndParams
\Procedure{GetData}{$\ $}
\State $\batch = \{ (x_{k}, y_{k}) \}_{k = 1...B}$ with $(x_k, y_k)$ i.i.d sampled from $\distribution$.
\State \Return $\batch$.
 \EndProcedure
\end{algorithmic}
\end{algorithm}


\begin{definition}[Optimizer]
\label{def:optimizer}
An \emph{optimizer} is an automaton, on each call of method $GetUpdate$, it will update states given the current parameter and gradient, and return an updated parameter.
\end{definition}

We now provide some examples of optimizers. The \emph{stochastic gradient descent (SGD) Optimizer} is shown in \Cref{alg:sgd}. When the batch size equals the size of the training set, this algorithm is often called \emph{gradient descent} directly. In our paper, we use a more complicated optimizer named \emph{Adam}, as shown in \Cref{alg:adam}. This optimizer, although poorly understood theoretically, has been widely applied across domains by the current machine-learning community.


\begin{algorithm}[h]
\caption{SGD Optimizer}\label{alg:sgd}
\begin{algorithmic}
\Params
 \State $\eta$, learning rate
 \State $\lambda$, weight decay
\EndParams
\Procedure{GetUpdate}{$W, g_W$}
\State \Return $W - \eta(\lambda W +  g_W)$.
 \EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
\caption{Adam Optimizer}\label{alg:adam}
\begin{algorithmic}
\Variables
\State $m$, first moment, initialized to be $0$.
\State $v$, second moment, initialized to be $0$.
\State $\eps$, a small positive constant, by default $1e-8$
\EndVariables
\Params
 \State $\eta$, learning rate
 \State $\lambda$, weight decay
 \State $\beta_1$, $\beta_2$, moving average factor for moments, by default $0.9, 0.999$.
\EndParams
\Procedure{GetUpdate}{$W, g_W$}
\State  $dW \gets \lambda W + g_W$.
\State  $m \gets \beta_1 m + (1 - \beta_1)dW$.
\State  $v \gets \beta_2 m + (1 - \beta_2)dW^2$.
\State  $\hat m \gets m / (1 - \beta_1)$.
\State  $\hat v \gets v / (1 - \beta_2)$.
\State  \Return $W - \eta \hat m / (\sqrt{\hat v} + \eps)$
 \EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{definition}[Regularization]
\label{def:regularization}
A \emph{regularization function} is defined as a mapping from the parameter space to $\R$. 
\end{definition}

Theoretically and empirically, a proper choice of a regularization function can improve the generalization of the learned model in previous literature.
We hereby provide two examples of regularization functions that will be used in our paper. 
One can easily notice that L2 regularization (\Cref{def:l2}) applied in the gradient-based optimization method is simply another form of weight decay.
L1 regularization (\Cref{def:l1}) applied with the linear model is known as \emph{LASSO} and can induce sparsity in the model parameters (meaning the model parameters contain more zeroes).

\begin{definition}[L2 Regularization]
\label{def:l2}
    L2 Regularization $R_2: (\R^{d_1},... ,\R^{d_k}) \to \R$ with penalty factor $\lambda$ is defined as $R_2(w_1, ..., w_d) = \frac{\lambda}{2} \sum_i \|w_i\|_2^2$.
\end{definition}

\begin{definition}[L1 Regularization]
\label{def:l1}
    L1 Regularization $R_1: (\R^{d_1},... ,\R^{d_k}) \to \R$ with penalty factor $\lambda$ is defined as $R_1(w_1, ..., w_d) = \lambda \sum_i \|w_i\|_1$.
\end{definition}

\begin{definition}[Stop Criterion]
\label{def:stop_criterion}
A \emph{stop criterion} is defined as a function that returns True or False determining whether the procedure should terminate.
\end{definition}
There are typically three kinds of stop criteria, which we list as below.

\begin{definition}[Stop-by-time]
\label{def:stop_time}
A stop-by-time criterion $\timestopcriterion(\rtime)$ returns true if physical running time exceeds threshold $t$.
\end{definition}

\begin{definition}[Stop-by-step]
\label{def:stop_step}
A stop-by-step criterion $\stepstopcriterion(\step)$ returns true if the weight update step exceeds threshold $T$.
\end{definition}

\begin{definition}[Stop-by-accuracy]
\label{def:stop_acc}
A stop-by-accuracy criterion $\accstopcriterion(\dataset, \gamma)$ returns true if the accuracy of the learned function on $\dataset$ exceeds threshold $\gamma$.
\end{definition}