\section{Experiment}
\label{sec:experiment}





In this section, we perform experiments on our methods and classical methods like BKW and Gauss as well. Without otherwise mention, the experiment results we report are conducted on 8 NVIDIA 3090 GPUs, which cost approximately 10000 dollars. For other experiments using CPUs, we use a 128 CPU cores server with 496GB memory. Specifically, the server contains two AMD EPYC 7742 64-Core Processors, which cost approximately 10000 dollars as well.

This section is organized as follows. The results for three settings are shown respectively in~\Cref{sec:abundant,sec:restricted,sec:moderate}. In each subsection, we will first show our method's overall performance with a comparison to classical algorithms. Then we will present a case study section, in which pilot experiments are demonstrated to show the experiment observation we have under the setting. It will be followed by a hyperparameter selection section, in which we showcase how our meta hyperparameter selection algorithm is performed and also our conclusion on the importance of different hyperparameters.

\subsection{Abundant Sample Setting}\label{sec:abundant}


In this section, we study the empirical performance of Algorithm \ref{alg:abundant}. The performance of Algorithm \ref{alg:abundant} is summarized in~\Cref{tab:abundant}. As a comparison, we list the performance of Gaussian Elimination in~\Cref{tab:gauss}. The hyperparameters may not be optimal for all $n$ and $\tau$. Nevertheless, it loosely reflects how the performance of Algorithm \ref{alg:abundant} varies with $n$ and $\tau$. A key observation is that the performance does not deteriorate much when we raise the noise. For instance, the runtime only triples when raising $\tau$ from 0.45 to 0.49. However, the running time rises quickly when we raise the dimension. In contrast, the running time of Gaussian Elimination rockets with both dimension and noise rate. These features makes~\Cref{alg:abundant} competitive in medium dimension with super high noise cases (note that LPN instances with medium dimension and super high noise often appear at the final stage of the BKW reduction or other dimension-reduction algorithms). In~\Cref{sec:abundant_case}, we will first illustrate typical phenomena that arise when running the algorithm. In~\Cref{sec:abundant_hyper}, we show how to tune the algorithm to its best performance.

\begin{table}[t]
\centering
    \begin{subtable}[t]{0.45\textwidth}
    \centering
    \begin{tabular}{cccccc}
        \toprule
        \diagbox{$n$}{$\tau$} & 0.4 & 0.45 & 0.49 & 0.495 & 0.498 \\
        \midrule
        20 & 39   & 70      & 197     & 323 & 730\\
        30 & 139  & 374     & 1576 &  & \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Neural Network}}
    \label{tab:abundant}
    \end{subtable}
    \begin{subtable}[t]{0.45\textwidth}
    \centering
    \begin{tabular}{cccccc}
        \toprule
        \diagbox{$n$}{$\tau$} & 0.4 & 0.45 & 0.49 & 0.495 & 0.498\\
        \midrule
        20 & 0.40 & 5.76 & 22.0 & 312 & 6407\\
        30 & 26.4 & 682 &  & \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Gaussian Elimination}}
    \label{tab:gauss}
    \end{subtable}
    \caption{\textbf{Time Complexity w.r.t. Dimension and Noise Rate}. Each entry represents the running time (in seconds) for the corresponding algorithm to solve the corresponding LPN instance with constant probability. For a neural network, the criterion for solving the LPN is that the accuracy of the network reaches $80\%$ on clean data. For Gaussian Elimination, the criterion for success is to get at least 7 correct secrets out of 10 attempts. The experiments presented in~\Cref{tab:abundant} are performed on a single GPU and in~\Cref{tab:gauss} on a single 64 cores Processor. The running time here is averaged over all runs that recover the correct secret in the time limit (3 hours). For the empty cell, no runs are successful within 3 hours. }
    \label{tab:perf}
\end{table}


\subsubsection{Case Study} 
\label{sec:abundant_case}
We run Algorithm \ref{alg:abundant} on $\LPN_{44,\infty,0.2}$. The training accuracy and the test accuracy are shown in Figure \ref{fig:abundant_case}. 

\begin{figure}
    \centering
    \begin{subfigure}{0.45\textwidth}
    \centering
        \includegraphics[scale = 0.35]{figures/fresh_44_0.2_train_acc.pdf}
        \caption{Training Accuracy}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
    \centering
        \includegraphics[scale = 0.35]{figures/fresh_44_0.2_eval_acc.pdf}
        \caption{Test Accuracy}
    \end{subfigure}
    \caption{\textbf{Experiments on $\text{LPN}_{44,\infty,0.2}$}. Figures (a) and (b) show the training accuracy and the evaluation accuracy respectively. The horizontal axis represents the training iteration. One unit on this axis represents 100 iterations. The vertical axis represents the accuracy of the model on the corresponding dataset. The training accuracy here is calculated on the batch used for training the network before the gradient is used to update the weight. There are 8 curves in every graph, each corresponding to a random initialization. The same type of graphs will appear frequently in this paper.}
    \label{fig:abundant_case}
\end{figure}

\begin{Empirical}
    Under~\Cref{setting:abundant}, we find that (i) The randomness in initialization has little impact on the running time or the final converged accuracy of the model. (ii) The training and test accuracy tie closely and test accuracy usually reaches $100\%$ eventually on clean data after it departs from $50\%$.
\end{Empirical}

\paragraph{Initialization} We use Kaiming initializations. As depicted in Figure \ref{fig:abundant_case}, it is observed that whether a model can learn the task is independent of the randomness in the Kaiming initialization. We also note that the running time needed to learn the LPN instance does not differ much among different initializations. Considering this phenomenon, we do not try multiple initializations in designing~\Cref{alg:abundant}.


\paragraph{Matching Training and Test Accuracy.} One can observe an almost identical value between the training and test accuracy in~\Cref{fig:abundant_case}.  This is expected because, on each train iteration, fresh data is sampled so the training accuracy, similar to the test accuracy, reflects the population accuracy. 
It is further observed that after the test accuracy starts to depart from 50\%, it always reaches 100\% accuracy eventually (here 100\% refers to the test accuracy on noiseless data). This phenomenon suggests that most local minima of $\populoss(f)$ found by algorithm \ref{alg:optim_example} are global minima of $\populoss(f)$.

\paragraph{Direct Inference of the Secret} We experiment with models with accuracy $80\%$ on the post processing step of~\Cref{alg:abundant} and find out that the post processing step returns secret with high probability with typical time complexity less than $1$s.


\subsubsection{Hyperparameter Selection}
\label{sec:abundant_hyper}

In this section, we fix an LPN problem setup with $n = 20$ and $\tau = 0.498$ and showcase the application of our~\Cref{alg:abundant_meta}. Recalling our goal is to find hyperparameters that minimize the time complexity, we report the minimal running time in seconds for the model to reach accuracy $80\%$ on noiseless data for one out of the four datasets we experimented on. For each experiment, we fix all except the investigated hyperparameters the same as in the corresponding column of~\Cref{alg:abundant} in~\Cref{tab:hyperparameter,tab:shared_hyperparameter}.


\begin{Empirical}
Under~\Cref{setting:abundant},  we find that (i) Larger batch size has tolerance for larger learning rate, however, all sufficiently large batch size yields similar running time. (ii) The depth of the network should be fixed to 1 and the width should be carefully tuned.
\end{Empirical}

\paragraph{Learning Rate and Batch Size} The performance of algorithm \Cref{alg:abundant} under different learning rates and batch sizes are listed in Table \ref{tab:ablation_1_lr_bz}. For a fixed batch size, if the learning rate is too small, the model will fit slowly to prolong the learning time. If on contrary, the learning rate is too large, the gradient steps would be so large that the learner cannot locate the local minima of the loss. The upper threshold of the learning rate increases with respect to the batch size because, for a fixed learning rate, a small batch size would result in an inaccurate gradient estimate and render the learning to fail. On the contrary, a large batch size can make the running time unnecessarily long. The best running times for all batch sizes are similar and for all learning rates are similar as well. As a result, we would recommend one first finds a batch size and learning rate combination that can fit the data and then tune one of the factors while fixing the other for higher performance.




\begin{table}[h]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        \diagbox{Learning Rate}{$\log_2$Batch Size} & 17 & 18 & 19 & 20 \\
        \midrule
        $6\times10^{-5}$ & 1807    & 2405    & 3569    & 4960\\
        $2\times10^{-4}$ & 806     & 1140    & 1389    & 1650\\
        $6\times10^{-4}$ & $>4000$ & $>4000$ & 692     & 970\\
        $2\times10^{-3}$ & $>4000$ & $>4000$ & $>4000$ & 750\\
        $6\times10^{-3}$ & $>4000$ & $>4000$ & $>4000$ & 723\\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Time Complexity w.r.t. Learning Rate and Batch Size}. Each entry represents the running time (in seconds) for~\Cref{alg:abundant} with corresponding hyperparameters to solve $\text{LPN}_{20,\infty,0.498}$ with probability approximately $1/4$.}
    \label{tab:ablation_1_lr_bz}
\end{table}


\paragraph{L2 Regularization} L2 regularization does not help to learn at all and increases running time over all settings we experimented on. As mentioned in~\Cref{sec:abundant_case}, generalization is not an issue under this setting hence L2 regularization is unnecessary.



\paragraph{Width and Depth of the Model}
We experiment with different architectures including MLP models with different widths in $\{500, 1000, 2000\}$ and depths in $\{1,2,3\}$. The base model we apply outperforms other architectures significantly and returns the correct secret at least $5$ times faster. This experiment, alongside the following architecture experiment in~\Cref{sec:restricted_hyper}, shows that one should tend to use a shallow neural network with depth $1$ and carefully tune the width of the model.




\subsection{Restricted Sample Setting}\label{sec:restricted}


In this section, we study empirically the minimal sample complexity required by neural networks to learn LPN problems. The investigation here is mostly limited to the case where the noise is low, as a complement to the cases in Settings~\ref{setting:abundant} and~\ref{setting:moderate}. As mentioned in~\Cref{sec:method_restricted}, we utilize~\Cref{alg:restricted}, which aims to solve the decision version of the problem. The performance of~\Cref{alg:restricted} is shown in~\Cref{tab:restricted_dimension_error}. These sample complexities are typically comparable with the classical algorithm, such as BKW. In~\Cref{sec:restricted_case}, we will show some important observations that guide us in designing~\Cref{alg:restricted}. In~\Cref{sec:restricted_hyper}, we will show how to use~\Cref{alg:restricted_meta} to find hyperparameters of the~\cref{alg:restricted}.
\begin{table}[h]
    \centering
    \begin{tabular}{cccc}
        \toprule
        \diagbox{Dimension}{Error Rate} & 0.1  & 0.2 & 0.3\\
        \midrule
        25 & 7    & 10.5 & 13  \\
        30 & 9.5  & 12.5 & 16  \\
        35 & 9.5  & 15 & 17.5  \\
        40 & 10.5 & 16 & 20.5  \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Sample Complexity w.r.t. Dimension and Noise Rate}. Each entry represents the logarithm of the minimal number of samples with base $2$ for~\Cref{alg:restricted} to return the correct guess of the last bit of the secret with a probability of approximately $2/3$.}\label{tab:restricted_dimension_error}
\end{table}


\subsubsection{Case Study}
\label{sec:restricted_case}
\begin{figure}[t]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale = 0.23]{figures/44_0.2_0_eval_acc.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale = 0.23]{figures/44_0.2_1_eval_acc.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale = 0.23]{figures/44_0.2_2_eval_acc.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale = 0.23]{figures/30_0.2_0_eval_acc.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale = 0.23]{figures/30_0.2_1_eval_acc.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale = 0.23]{figures/30_0.2_2_eval_acc.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale = 0.23]{figures/30_0.5_0_eval_acc.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale = 0.23]{figures/30_0.5_1_eval_acc.pdf}
        \caption{}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[scale = 0.23]{figures/30_0.5_2_eval_acc.pdf}
        \caption{}
    \end{subfigure}
    \caption{\textbf{Performance of the Models} These nine graphs show the performance of models during training under three different setups specified in~\Cref{tab:restricted_setup}. The first, second, and third row corresponds to Setup 1, 2, and 3 respectively. Different pictures in the same row correspond to different datasets. Different colors in the same picture correspond to different initializations of the networks.} 
    \label{fig:naive}
\end{figure}

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Setup & $n$  & $\tau$ & $m$ & Sparsity\\
        \hline
        1 & 44   & $0.2$ & $2^{17}$  & $0.2$ \\
        \hline
        2 & 29  &  $0.2$ & $2^{17}$  & $0.2$\\
        \hline
        3 & 30  & $0.2$ & $2^{17}$  & $0.5$ \\
        \hline
    \end{tabular}
    \caption{\textbf{Three Different Setups Shown in~\Cref{fig:naive}}. The sparsity column means the hamming weight of the secret over $n$.}\label{tab:restricted_setup}
\end{table}



\begin{figure}[h]
    \centering
    \begin{subfigure}{0.45\textwidth}
    \centering
        \includegraphics[scale = 0.35]{figures/44_0.2_implicit_train_acc.pdf}
        \caption{Training Accuracy}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
    \centering
        \includegraphics[scale = 0.35]{figures/44_0.2_implicit_eval_acc.pdf}
        \caption{Test Accuracy}
    \end{subfigure}
    \caption{\textbf{Experiments on $\text{LPN}_{44,185362,0.2}$} The legend in this figure is the same as in~\Cref{fig:abundant_case}. We can observe a large deviation between the training and test accuracy curve. In fact, for the initialization that the accuracy boosts the highest, the accuracy increases after all the training data are correctly predicted, or in other words, memorized.}. 
    \label{fig:restricted_case}
\end{figure}




We first illustrate the common phenomena observed under~\Cref{setting:restricted} that guide us in designing~\Cref{alg:restricted}.

We conducted experiments with neural networks using~\Cref{alg:optim_example} under three setups specified in~\Cref{tab:restricted_setup}. The hyperparameters follows~\Cref{tab:hyperparameter,tab:shared_hyperparameter}, and the step threshold $\step$ is set to $300k$. To test the effectiveness of using sparse secrets (cf.~\Cref{lem:s_sparse}), we vary the sparsity of the secret (Hamming weight / dimension) in the third setup. We plot the test accuracy of the model with respect to training iterations in~\Cref{alg:abundant}. Notice here we test on LPN data directly hence the highest possible population accuracy is $80\%$. 


\begin{Empirical}
    Under~\Cref{setting:restricted}, we find that (i) The sparse secret makes training easier. (ii) Initialization significantly affects the performance of the converged trained model. (iii) The reduction to the decision problem enables us to learn LPN instances with larger $n$. (iv) Typically there is a significant gap between training and test accuracy under this setting, which calls for methods to improve generalization.
\end{Empirical}



\paragraph{Sparsity of Secret} Comparing the first and the last row of~\Cref{fig:naive}, Algorithm \ref{alg:restricted} succeeded in solving $\LPN_{44,2^{17},0.2}$ with secrets of sparsity $0.2$ for two out of three datasets, while completely fail on solving $\LPN_{30,2^{17},0.2}$ with secrets of sparsity $0.5$. Hence, using sparse secret greatly reduce the hardness of learning. This implies we should always apply~\Cref{lem:s_sparse} to reduce the secret to a sparse secret when the noise level is low.

\paragraph{Initialization Matters} Graph (a),(b), and (c) in~\Cref{fig:naive} show that initialization can affect whether a neural network can distinguish the correct and the wrong guesses. This shows the necessity to try different initialization on the same dataset (possibly in parallel).



\paragraph{Necessity to Transfer to Decision Problem} Previously we have established that Algorithm \ref{alg:optim_example} can solve the decision version of $\LPN_{44,m,0.2}$. Graphs (d), (e), (f) show that even if we reduce $n$ to 29, direct learning cannot yield a model that perfectly simulates the LPN oracle without noise, in which case the test performance should get close to 0.8. However, all the trained models only yield an accuracy lower than $0.7$. Hence with the reduction, we can solve an LPN instance with larger $n$ with the same $m$ and $\tau$.

\paragraph{Divergence of Training and Test Accuracy} In~\Cref{fig:restricted_case}, we plot a run of~\Cref{alg:optim_example} on LPN data when the sample complexity is greatly limited. Because the size of the training set is very limited, the model easily reaches $100\%$ accuracy on the training set. This phenomenon is known as \emph{overfitting} in the machine learning community. However, one can observe that the test accuracy of one of the model in (b) boost after the model overfits the training set. Though strange at first sight, this phenomenon, known as \emph{grokking} where validation accuracy suddenly increases after the training accuracy plateau is common in deep learning experiments and repetitively appears in our experiments on LPN data. The exact implication of this phenomenon is still opaque. At the very least, we can infer from the figures that \emph{generalization}, instead of optimization is the key obstacle in~\Cref{setting:restricted}.

\subsubsection{Hyperparameter Selection}
\label{sec:restricted_hyper}

\begin{table}[t]
\centering
\begin{subtable}[t]{0.2 \textwidth}
\centering
    \begin{tabular}{ccc}
        \toprule
        1 & 2 & 3\\
        \midrule
        17 & 19 & $>$19\\
       \bottomrule
    \end{tabular}
    \caption{Depth}
    \label{tab:ablation_3_depth}  
\end{subtable}
\begin{subtable}[t]{0.4 \textwidth}
\centering
    \begin{tabular}{cccccc}
        \toprule
        500 & 800 & 1000 & 1200 & 1500 & 2000\\
        \midrule
        $>$19 & 17.5 & 17 & 17.5 & 18.5 & 17.5\\
        \bottomrule
    \end{tabular}
    \caption{Width}
    \label{tab:ablation_3_width}
\end{subtable}
\begin{subtable}[t]{0.30 \textwidth}
    \centering
    \begin{tabular}{ccc}
        \toprule
        ReLU & Sigmoid & COS\\
        \midrule
        17 & 18.5 & 18 \\
        \bottomrule
    \end{tabular}
    \caption{Activation}
    \label{tab:ablation_3_activation}
\end{subtable}

\begin{subtable}[t]{0.15 \textwidth}
    \centering
    \begin{tabular}{cc}
        \toprule
        MSE & Logistic \\
        \midrule
        18 & 17 \\
        \bottomrule
    \end{tabular}
    \caption{Loss}
    \label{tab:ablation_3_loss}
\end{subtable}
\begin{subtable}[t]{0.46 \textwidth}
    \centering
    \begin{tabular}{cccccc}
        \toprule
        0 & $5e-4$ & $1e-3$ & $2e-3$ & $3e-3$ & $6e-3$ \\
        \midrule
        17 & 17 & 16.5 & \textbf{16} & 16.5 & $>19$ \\
        \bottomrule
    \end{tabular}
    \caption{L2 regularization}
    \label{tab:ablation_3_l2}
\end{subtable}
\begin{subtable}[t]{0.25 \textwidth}
    \centering
    \begin{tabular}{ccc}
        \toprule
        Full & Full/2 & Full/4 \\
        \midrule
        17 & 17 & 17 \\
        \bottomrule
    \end{tabular}
    \caption{Batch Size}
    \label{tab:ablation_3_batch}
\end{subtable}


\begin{subtable}[t]{0.8 \textwidth}
    \centering
    \begin{tabular}{ccccccccc}
        \toprule
        $1e-5$ & $2e-5$ & $1e-4$ & $2e-4$ & $1e-3$ & $2e-3$ & $1e-2$ & $2e-2$ & $1e-1$\\
        \midrule
        18.5 & 17 & 17 & 18 & 18 & 18.5 & 18 & 18.5 & 19\\
        \bottomrule
    \end{tabular}
    \caption{Learning Rate}
    \label{tab:ablation_3_lr}
\end{subtable}
\caption{\textbf{Time Complexity w.r.t. Multiple Hyperparameters.} Each entry represents the logarithm of sample complexity with base $2$ for~\Cref{alg:restricted} with corresponding hyperparameters to solve $\text{LPN}_{44,m,0.2}$ with probability approximately $2/3$. The hyperparameter profile we considered in our experiments always shares all except the investigated hyperparameter as~\Cref{tab:shared_hyperparameter,tab:base_hyperparameter}.}
\label{tab:ablation_3}
\end{table}

In this subsection, we fix an LPN problem setup with $n = 44$ and $\tau = 0.2$ and illustrate the conclusion of our hyperparameter selection process based on~\Cref{alg:restricted_meta}. Recalling our goal is to find hyperparameters that minimize the sample complexity, we report $\log_2 \text{\# Sample}$ for our reduction algorithm to successfully return the secret with probability approximately equal to $2/3$ following the convention of~\cite{bogos2016solving}. The hyperparameter profiles we considered in our experiments are identical except for the investigated hyperparameter as~\Cref{tab:shared_hyperparameter,tab:base_hyperparameter}. The aggregated results are shown in~\Cref{tab:ablation_3}. 

\begin{Empirical}
Under~\Cref{setting:restricted}, the architecture of the model, learning rate, and L2 regularization are the most important components in determining the sample complexity. In the meantime, tuning batch size or applying other regularization methods are in general not helpful.
\end{Empirical}


\begin{table}[]
    \centering
    \begin{tabular}{|c|c|c|c|c|} 
    \hline  Sampler & Learning Rate & Weight Decay &  Batch Size &Step Threshold $\step$ \\
    \hline
      Batch & $2e-5$ & 0 & Size of Train Set & $300k$ \\
    \hline
    \end{tabular}
    \caption{Base Hyperparameters}
    \label{tab:base_hyperparameter}
\end{table}


\paragraph{Depth of Network}  In contrast to the trend of using \emph{deep} neural network, the depth of the network on LPN problem should not be large. Based on~\Cref{tab:ablation_3_depth}, we find depth $1$ neural network performs significantly better than any larger depth network.




\paragraph{Width of Network} Similar to the case in~\Cref{sec:abundant}, width of the network is critical, as shown in~\Cref{tab:ablation_3_width}. However, the blessing here is that the width is not very versatile in this setting. The width must exceed a lower bound for better performance while all the widths larger than that show similar performance. However, the optimal width is still a finite number, in this case, near $1000$.

\paragraph{Activation Function} We experiment with replacing the activation function of the first layer and find that while all the activation function can make~\Cref{alg:restricted} work given enough sample, the ReLU activation performs better than other candidates. The results are shown in~\Cref{tab:ablation_3_activation}.


\paragraph{Loss} We experiment with two types of loss, the MSE loss and the logistic loss in~\Cref{tab:ablation_3_loss}. Training using these two loss functions are usually referred to as \emph{regression} and \emph{classification} in machine learning literature. We find out that logistic loss performs slightly better than MSE loss.


\paragraph{Batch Size} One may naturally wonders why in~\Cref{setting:moderate} and~\Cref{setting:abundant}, we both use fix batch size, while in the~\Cref{setting:restricted}, we recommend using full batch training. One important issue to notice is that the batch size in the two other settings is typically larger than the total dataset itself in the current setting. In our experiments with smaller batch sizes,  the results are almost identical to those using the full training set size. 


\paragraph{Learning Rate} We observe a similar phenomenon as in~\Cref{sec:abundant_hyper} for the learning rate, i.e., it should be neither too large nor too small to achieve the best performance. However, it is worth noticing that the best learning rate in this regime is typically a magnitude smaller than the best learning rate for~\Cref{alg:abundant}. We have two hypotheses for the reason behind this. First, loosely based on our theory prediction in~\Cref{thm:optimization}, a higher noise rate calls for a higher learning rate. Second, the dependency between the model weight and the sample we used for each step calls for a smaller learning rate to reduce the caveat of overfitting.


\paragraph{Weight Decay} As mentioned in~\Cref{sec:restricted_case}, generalization is the key obstacle for neural networks to learn to distinguish LPN data from random data in~\Cref{setting:restricted}. We experiment with several different regularization functions and find out that weight decay, or L2 regularization, is of significance in reducing the sample size. This coincides with previous theoretical prediction~\Cref{thm:wd} in a simpler setting. The best weight decay parameter we find for our settings is $2e-3$. We observe empirically that when increasing this parameter to $6e-3$, the weight of the model will quickly collapse to all zeroes. In this sense, the weight decay parameter needs to be large, but not to the extent that interferes with optimization.





\paragraph{Sparsity Regularization} We also try regularization methods like L1 regularization and architectures with hard-coded sparsity (by fixing some of the parameters to $0$ or using a convolutional neural network), the results are not shown in~\Cref{tab:ablation_3} because all these methods increase the required sample complexity by a large amount. Although in our~\Cref{thm:rep}, the ground truth model we constructed is sparse when the secret is sparse, the ways we utilize sparsity hurt the optimization performance of~\Cref{alg:optim_example}. It remains open whether one can design better architectures or find other regularization functions that better suit the LPN problems.


\subsection{Moderate Sample Setting}\label{sec:moderate}

In~\Cref{setting:moderate}, we aim at finding ways to use neural networks as a part of the classical \emph{reduction-decoding} scheme of LPN solving. We have observed in~\Cref{sec:abundant} that neural networks show resilience to noise when the dimension is small, which is further validated by~\Cref{thm:optimization}. It would then be tempting to use neural networks in the decoding phase where typically the dimension is low and the noise rate is high. However, to make neural networks feasible for solving the problem after reduction, we can no longer assume the number of training samples is infinite. Instead, we should utilize all the training samples generated after the reduction phases and try reducing the time complexity based on the samples we are given.

In this section, we use~\Cref{alg:moderate} to solve for the last $26$ bits of the secret for LPN instance  $\text{LPN}_{125,1.2e10,0.25}$ in \textbf{106 minutes}. As a comparison, in~\cite{DBLP:conf/crypto/EsserKM17}, more than 3 days are taken to recover the $26$ bits of the secret. We believe this result shed light on the potential of utilizing neural networks and GPUs as components of faster solving algorithms of LPN in practice.

In~\Cref{sec:moderate_case}, we show the exact time component for solving the last $26$ bits as well as important phenomenons we observe under~\Cref{setting:moderate}. In~\Cref{sec:moderate_hyper}, we mainly discuss the hyperparameter tuning required for the post-processing part of~\Cref{alg:moderate}. We will show that the hypothesis testing threshold $\tau'$ is not very versatile for the algorithm performance and hence is easy to tune.


\subsubsection{Case Study}
\label{sec:moderate_case}

We will first introduce how we solve for the last $26$ bits of the LPN instance $\text{LPN}_{125,1.2e10,0.25}$. On our server with 128 cores, we can reduce this problem to $\text{LPN}_{26, 1.1e8, 0.498}$ in 40 minutes. We will then enumerate the last $6$ bits of the secret to further reduce the problem to $\text{LPN}_{20, 1.1e8, 0.498}$ and apply our~\Cref{alg:moderate} on it. We specified the time constraint $\rtime$ in~\Cref{tab:hyperparameter} for~\Cref{alg:moderate} as $20$ minutes and hypothesis testing threshold $\tau'$ as $0.483$ under this setting. We further restricted the running time of the rebalance and post-processing step by $2$ minutes. By using $8$ 3090 GPUs to enumerate secret in parallel, we try the correct postfix in the third round of enumeration and the post-processing step return the right secret in the pool of secrets. The applications of enumeration and~\Cref{alg:moderate} take less than $66$ minutes. In total, we solve for the last $26$ bits in less than $106$ minutes. 

In designing~\Cref{alg:moderate}, two primary factors affect the performance. The first factor is the probability of~\Cref{alg:optim_example} procedure returns model with high accuracy on test data, which we dub as \emph{success rate}. The second factor is the accuracy the model can reach given the time limit under such a scenario, which we dub as \emph{mean accuracy}.  The repeat number for starting with different initializations $repeat$ mainly depends on the success rate and the mean accuracy affects the time complexity of our post-processing step. We will now introduce some important phenomenons we observe in applying~\Cref{alg:optim_example} on LPN instance with dimension $20$ and error rate $0.498$ that guides us in selecting the hyperparameters of~\Cref{alg:moderate}.


\begin{Empirical}
    Under~\Cref{setting:moderate}, we find that (i) Both the success rate and the mean accuracy increases with the size of the training set. (ii) An imbalance in the output distribution exists in all models we trained in this setting (predicting one outcome with a significantly higher probability than the other over random inputs) (iii) Different than the case in~\Cref{setting:restricted}, the training accuracy typically plateau at a low level.
\end{Empirical}

\paragraph{The Size of Training Set}
We first show sample complexity affects these two factors mutually.
As most of the samples consumed are used to perform~\Cref{alg:optim_example}, we experiment with varying the sample in~\Cref{tab:ablation_3_m}. It is observed that both the success rate and the mean accuracy increase with the sample complexity. Under the scenario where $1e8$ samples are provided, the success rate reaches $87.5\%$, allowing us to set $repeat$ as 1 to get a final algorithm that succeeds with high probability. Given the randomness here is over the initialization of the model instead of the dataset itself, our results also show that we can compensate for the lack of samples by trying more initializations. 

\begin{table}[h]
    \centering
    \begin{tabular}{ccc}
        \toprule
        $m$ & \quad Success Rate & \quad Mean Accuracy \\
        \midrule 
        $4e7$ & $25.0\%$ & $51.1\%$\\
        $6e7$ & $62.5\%$& $51.7\%$ \\
        $8e7$ & $62.5\%$& $51.8\%$\\
        $1e8$& $87.5\%$ & $52.2\%$ \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Success Rate and Mean Accuracy w.r.t. Training Set Size $m$ on $\text{LPN}_{20,m,0.498}$}.
    We define a successful run as a model with random initialization that converges to a model with an accuracy greater than $51.5\%$ on clean data in the time limit. The success rate means the probability of a run being successful over the randomness of initialization. The mean accuracy is only calculated on successful runs.
    We clearly observe that with an increasing number of samples, both the success rate and the mean accuracy increase significantly.}
    \label{tab:ablation_3_m}
    \vspace{-0.3in}
\end{table}

\paragraph{Discussion on Output Distribution} 
Reader may find the rebalance step in~\Cref{alg:moderate} seemingly unnecessary. However, we observe in experiments that the trained neural network typically has a bias towards $1$ or $0$. The proportion of two different results given random inputs can be as large as $1.5$. This fact makes the testing of Gaussian Elimination steps hard and leans toward providing a wrong secret. We mitigate this effect by performing the rebalance step and believe that if this phenomenon can be addressed through better architecture designs or training methods, the performance of neural network based decoding method can be further boosted.


\paragraph{Optimization Obstacle} We plot the training and test accuracy curve for applying~\Cref{alg:optim_example} over $\text{LPN}_{20,1e8,0.498}$ in~\Cref{fig:moderate_case}. Here the test is performed on clean data to provide better visualization. Different from~\Cref{setting:restricted}, where the training set is greatly limited and the network can overfit the training set, here we observe that the network can hardly fit the training data.
This fact has two sides. Firstly, it shows that overfitting ceases given enough samples and generalization won't be a severe issue. However, as we observe in~\Cref{fig:restricted_case}, the generalization performance may boost when overfitting happens, hence the low training accuracy may also be a reason behind the low final converged accuracy.


\begin{figure}[h]
    \centering
    \begin{subfigure}{0.49\textwidth}
    \centering
        \includegraphics[scale = 0.35]{figures/moderate_20_0.498_train_acc.pdf}
        \caption{Training Accuracy}
    \end{subfigure}
    \begin{subfigure}{0.49\textwidth}
    \centering
        \includegraphics[scale = 0.35]{figures/moderate_20_0.498_eval_acc.pdf}
        \caption{Test Accuracy}
    \end{subfigure}    \caption{\textbf{Experiments on $\text{LPN}_{20,1e8,0.498}$} We observe that the training accuracy is low, showing a vast contract with~\Cref{setting:restricted}. One should notice here the test accuracy is over clean data while the training accuracy is over the noisy training set.}. 
    \label{fig:moderate_case}
\end{figure}













\subsubsection{Hyperparameter Selection}
\label{sec:moderate_hyper}

For the ease of hyperparameter tuning, we use hyperparameters for~\Cref{alg:optim_example} we found in~\Cref{sec:abundant_hyper} and our experiments show that this choice can already let us get models with enough accuracy to perform boosting. Hence here we will focus on discussing the hyperparameters for the post processing the Gaussian Elimination step.

\begin{Empirical}
Under~\Cref{setting:moderate}, we find that (i) Hyperparameters for the network training can follow the hyperparameters under~\Cref{setting:abundant}. 
(ii) The hypothesis testing threshold $\tau$ in~\Cref{alg:moderate} has large tolerance and has little impact on time complexity even when it's $1\%$ larger than the ground-truth error rate in the test set for the post processing step.
\end{Empirical}


\paragraph{Hypothesis Testing Threshold} In the post processing step, the samples are labeled by the neural networks and are in this sense \emph{free}. Under our experiments, we choose the size of boosting set $m' = 231072$, with $131072$ samples as the sample pool where Gaussian elimination input are sampled and $100000$ samples for hypothesis testing, which are more than sufficient for the estimated error rate $\approx 0.48$.

The remaining question for hyperparameter tuning may be that how should we set the hypothesis testing threshold given that it may be hard to estimate on the fly. We propose to use a meta run to estimate the converged accuracy. As one can observe that from~\Cref{fig:moderate_case}, the final converged accuracy of successful run (with final accuracy greater than $51.5\%$) center closely. 

We further show through experiments that the hypothesis testing threshold $\tau'$ in the Gaussian step are not critical for the performance of~\Cref{alg:moderate} in~\Cref{tab:ablation_3_tau}. The estimation of hypothesis threshold can be off by almost $1\%$ while still returning the correct secret in $20$ repetitive pooled Gaussian run. However, the results in this table also shows that the final testing on noisy real data is necessary as all the pools of secrets contain wrong secrets. The reader should notice that the final testing step takes almost negligible time, as for each model, at most $20$ secrets need to be tested.


\begin{table}[h]
    \centering
    \begin{tabular}{cccc}
        \toprule
         0.480 & 0.483  & 0.486 & 0.489 \\
        \midrule 
        6 & 6 & 1 & 2  \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Number of Occurrences of the Correct Secret in 20 runs w.r.t. Estimate Error Rate $\tau'$} A subtlety in hyperparameter tuning for~\Cref{alg:moderate} is that in general $\tau'$ is hard to know precisely. We propose to use a meta run to first estimate $\tau'$ of the converged model. As one can infer from~\Cref{fig:moderate_case}, the test accuracy of the converged model in successful runs tends to be stable. With the ground truth error rate in the test set being about $0.479$, our experiments shows that in fact the post-processing step has high tolerance for the hypothesis testing threshold, ranging from $0.480$ to $0.489$.} 
    \label{tab:ablation_3_tau}
\end{table}
















