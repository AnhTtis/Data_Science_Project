\section{Introduction}
\label{sec:intro}
Neural networks are magical, capable of learning how to play various board games \cite{alphago,CAMPBELL200257,Tesauro1995TDGammonAS} and video games \cite{Mnih2015,Vinyals2019,DBLP:journals/corr/abs-1912-06680}, how to control fusion reactors \cite{Degrave2022}, how to predict spatial structures of proteins \cite{Jumper2021}, etc. In recent years, the rise of neural networks not only revolutionizes the field of artificial intelligence but also greatly impacts other fields in computer science. It is natural to ask whether neural networks can help us with cryptography. 

One of the signature cryptographic hard problems is the Learning Parity with Noise problem (LPN), also known as decoding binary random linear codes, a canonical problem in coding theory.
Let $n$ be the dimension, $\tau\in(0, 0.5)$ be the error rate. Let $\ary{s}$ be a secret vector in $\Z_2^n$. The LPN problem asks to find the secret vector $\ary{s}$ given an oracle which, on its $i^{th}$ query, outputs a random vector $\ary{a}_i\in \Z_2^n$, and a bit $y_i := \ipd{\ary{s}}{\ary{a}_i}+e_i \pmod 2$, where $e_i$ is drawn from the error distribution that outputs 1 with probability $\tau$, 0 with probability $1-\tau$. 
When $\tau$ is very small, say $\tau\in (0,1/n)$, then as long as we obtain $\Omega(n)$ LPN samples, we can efficiently find out which $n$ of the samples are error-free and use Gaussian elimination to find the secret. However, for large $\tau\in (1/n^c, 0.5)$ where $0<c<1$, no classical or quantum algorithm is known for solving LPN in polynomial time in $n$. 
\textbf{\textbf{}}
The LPN problem was proposed by machine learning experts as a conjectured hard problem for good cryptographic use~\cite{DBLP:conf/crypto/BlumFKL93}.
Since its proposal, researchers have found numerous interesting cryptographic applications from LPN, including authentication protocols~\cite{DBLP:conf/asiacrypt/HopperB01,DBLP:conf/eurocrypt/KiltzPCJV11}, public-key encryptions~\cite{DBLP:conf/focs/Alekhnovich03,DBLP:conf/crypto/YuZ16}, identity-based encryptions~\cite{DBLP:conf/eurocrypt/BrakerskiLSV18,DBLP:conf/pkc/DottlingGHM18}, and efficient building blocks for secure multiparty computation~\cite{boyle2019efficient}. The LPN problem also inspires the formulation of the learning with errors problem  (LWE)~\cite{DBLP:journals/jacm/Regev09}, which is more powerful in building cryptographic tools.




For LPN with constant noise rates, the asymptotically fastest algorithm, due to Blum, Kalai, and Wasserman~\cite{DBLP:journals/jacm/BlumKW03}, takes $2^{O\left(\frac{n}{\log n}\right)}$ time and requires $2^{O\left(\frac{n}{\log n}\right)}$ samples.
However, for cryptosystems based on LPN, the number of samples is typically a small polynomial in $n$. In this setting, Lyubashevsky gives a $2^{O\left(\frac{n}{\log\log n}\right)}$ time algorithm~\cite{DBLP:conf/approx/Lyubashevsky05} by first amplifying the number of samples and then running the BKW algorithm. 
To improve the concrete running time for solving LPN, researchers develop more sophisticated hybrid algorithms, see for example~\cite{DBLP:conf/scn/LevieilF06,DBLP:conf/asiacrypt/GuoJL14,bogos2016solving,zhang2016faster,DBLP:conf/asiacrypt/BogosV16,DBLP:conf/crypto/EsserKM17,DBLP:conf/crypto/EsserHK0S18}. The hybrid algorithms combine the BKW reduction with other tools like the ``Guess-then-Gaussian-elimination'' (henceforth Gauss) algorithm and decoding algorithms in coding theory~\cite{DBLP:conf/asiacrypt/MayMT11,DBLP:conf/eurocrypt/BeckerJMM12}.  
Using hybrid algorithms, Esser, K{\"{u}}bler, and May~\cite{DBLP:conf/crypto/EsserKM17} show that middle-size LPN instances are within the reach of the current computation power. For example, they show that LPN with dimension $n = 135$, noise rate $\tau = 0.25$ can be solved within 5.69 days using 64 CPU cores. The practical running time for solving a larger instance of $n = 150$,  $\tau = 0.25$ is recently reported as less than 5 hours by Wiggers and Samardjiska~\cite{wiggers2021practically} by
using 80 CPU cores and more carefully designed reduction chains. %




\paragraph{Machine learning and LPN.}

The most elementary setting of machine learning is the \emph{supervised learning} paradigm. In this setting, we are given a set of labeled data that consists of some input data (e.g. pictures) and corresponding labels (e.g. whether the picture contains animals, not necessarily correct). The input data is presumed to be drawn from a fixed distribution and the given dataset often does not contain most of the possible inputs. The goal is to find a function that predicts the labels from inputs. The predictions are required to be accurate on both seen and unseen inputs. The size of the dataset required to find a good enough function is called the \emph{sample complexity} and the function is often called \emph{model}. The function is always chosen from the \emph{function class}, a pre-determined set of functions. If the function class is simple, it would be easy to find the best function, but none of them would be sophisticated enough to perform well on complex tasks. In contrast, a complex function class holds greater expressive power but may be hard to optimize and has higher sample complexity. We often call the optimization procedure \emph{learning}. Neural networks provide us with huge and adjustable expressiveness as well as an efficient learning algorithm. The recent success of neural networks largely hinges on their expressive power, as well as recent advances in big data and computing resources to make them useful.

The LPN problem perfectly fits into the supervised learning paradigm. Specifically, let the queries be inputs and the parity with noise be labels. If we can find a model that simulates the LPN oracle without noise, we can use this model to sample some data and use gaussian elimination to recover the secret. Thus it is tempting to try using the power of neural networks to break LPN. 
However, perhaps surprisingly, there are very few instances where neural networks outperform the conventional algorithms in the cryptanalysis literature. In fact, most of the successful examples are in the attack of blockciphers, see e.g.~\cite{alani2012neuro,DBLP:conf/crypto/Gohr19,benamira2021deeper}.
The only documented attempt of using neural networks in solving problems related to LPN is actually aiming at solving LWE, conducted by Wenger et al.~\cite{DBLP:journals/iacr/WengerCCL22}. But their neural network algorithm is not yet competitive with the traditional algorithms. 
Besides, the limited theoretical understanding of neural networks we have is in stark contrast to its splendid empirical achievements. Hence this paper mainly focuses on empirically demonstrating neural networks' usefulness in breaking the LPN problem with the most rudimentary networks. We hope this paper can serve as a starting point for breaking LPN and other applications in cryptography with neural networks. We also hope this paper can motivate further theoretical works on relevant problems.



\subsection{Our contributions}

Our main contribution is designing families of neural networks that practically outperform classical algorithms in high-noise, low-dimension regimes. \footnote{We release our code in \url{https://github.com/WhenWen/Solving-LPN-using-Neural-Networks.git}.} 
We consider three settings. In the \emph{abundant sample setting}, the time complexity is considered the prime. In the \emph{restricted sample setting}, efforts are made to reduce the sample complexity required by neural networks. In the \emph{moderate sample setting}, we consider optimizing time complexity given sample complexity typically given by the reduction phase of the hybrid algorithm.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline 
     Model $\model$ & Initialization& Loss & Optimizer\\
    \hline
         $\basemodel_{1000}$ (\Cref{def:basemodel})& Kaiming \cite{he2015delving}& Logistic & Adam
    \\
    \hline
    \end{tabular}
    \caption{Shared Features in All Three Settings}
    \label{tab:intro_shared_hyperparameter}
\end{table}


Our main experimental results suggest that two-layer neural networks
with Adam optimizer and logistic loss function work the fastest in all three settings. Some shared features of our algorithms are presented in~\Cref{tab:intro_shared_hyperparameter}. Our neural network architectures are quite different from the ones used by Wenger et al.~\cite{DBLP:journals/iacr/WengerCCL22} where they use the transformer model to attack LWE. Let us also remark that previous attempts of using neural networks for solving decoding problems use more complicated network structures with more than five layers~\cite{nachmani2018deep,bennatan2018deep}, in contrast to our design that only uses two layers. 

Although the power of neural networks is usually hard to explain, for some settings we are able to explain the rationale of the design of our machine learning models in theory, in terms of their representation capability, optimization power, and the generalization effect. We present some first-step analysis in~\Cref{sec:theory}.

Comparing with the previous experiments of Esser, K{\"{u}}bler, and May~\cite{DBLP:conf/crypto/EsserKM17}, for dimension $n=26$, noise rate $\tau = 0.498$, the ``Guess-then-Gaussian-elimination'' algorithm takes 3.12 days on 64 CPU cores, whereas our algorithm takes $66$ minutes on 8 GPUs. For large instances like $n=125$, $\tau = 0.25$, our algorithm can also be plugged into hybrid algorithms to reduce the running time from $4.22$ days\footnote{The experiment reported in \cite[Page~28]{DBLP:conf/crypto/EsserKM17} solved an LPN instance of $n=135$, $\tau = 0.25$ in $5.69$ days. They first spent $1.47$ days to enumerate 10 bits of secrets. We didn't repeat the enumeration step and directly start from $n=125, \tau = 0.25$.} to $3.5$ hours. To reach this performance, we first use BKW to reduce $1.2e10$ LPN samples of dimension $125$, noise rate $0.25$ to $1.1e8$ LPN samples of dimension $26$, noise rate $0.498$ in $40$ minutes with $128$ CPUs. Then we enumerate the last $6$ bits and apply our neural network algorithm (\Cref{alg:moderate}) with 8 GPUs on LPN with $n=20, \tau = 0.498$ to find out $26$ bits of the secret in $66$ minutes. The whole process (BKW+neural network) is then repeated to solve another $26$ bits in another $106$ minutes. The final reduced problem has dimension $73$ and noise rate $0.25$ and can be solved by the MMT algorithm in $1$ minute. In total, the running time is $3.55$ hours.

Let us remark that we haven't compared our neural network algorithm with the more sophisticated decoding algorithms such as Walsh-Hadamard or MMT. %
But our experiments have already shown that neural networks can achieve competitive performance for solving LPN in the high-noise, low-dimension regime compared to the Gauss decoding, when running on hardware with similar costs, and there is a huge potential for improvement for our neural network algorithm.
There is also clear room for improvement in our reduction algorithm. For example, the time complexity of the second round can be significantly reduced given that we already know $26$ bits of the secret. However, our algorithm mainly focuses on accelerating the decoding phase of the hybrid algorithm. In this sense, our contributions are orthogonal to~\cite{DBLP:conf/asiacrypt/BogosV16,wiggers2021practically}, where the improvement is made possible by constructing a better reduction chain.



\paragraph{More details in the three settings.}

As mentioned, we consider three settings, named \emph{abundant}, \emph{restricted}, and \emph{moderate} sample setting. Let us now explain those three settings.

\begin{setting}[Abundant Sample] \label{setting:abundant}
In the ``Abundant Sample'' setting, we assume an unlimited amount of fresh LPN samples are given, and we look for the algorithm that solves LPN as fast as possible. This setting has a clear definition and will serve as the starting point for our algorithms.
\end{setting}

Under~\Cref{setting:abundant}, we first present the naive algorithm that directly considers the LPN problem as a supervised learning algorithm. This algorithm ignores the special structures of the LPN problem. However, our experiments in~\Cref{sec:abundant} show that given enough samples, neural networks can learn the LPN encoding perfectly. One example run on an LPN problem with dimension $20$ and noise rate $0.498$ is shown in~\Cref{fig:intro}. We also compare the time complexity of our algorithm with the Gauss algorithm in \Cref{tab:perf_intro}, showing the supremacy of our algorithm in the heavy noise regime. 

\begin{figure}
    \centering
    \includegraphics[scale = 0.4]
    {figures/intro.pdf}
    \caption{\textbf{Experiments on 
    $\text{LPN}_{20,\infty,0.498}$}. The horizontal axis represents the training iteration. One unit on this axis represents 10 iterations. The vertical axis represents the accuracy of the model on the clean test set without noise. There are 8 curves in this graph, each corresponding to a random initialization.}
    \label{fig:intro}
\end{figure}

\begin{table}[t]
\centering
    \begin{subtable}[t]{0.45\textwidth}
    \centering
    \begin{tabular}{cccccc}
        \toprule
        \diagbox{$n$}{$\tau$} & 0.4 & 0.45 & 0.49 & 0.495 & 0.498 \\
        \midrule
        20 & 39   & 70      & 197     & 323 & 730\\
        30 & 139  & 374     & 1576 &  & \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Neural Network}}
    \label{tab:intro_abundant}
    \end{subtable}
    \begin{subtable}[t]{0.45\textwidth}
    \centering
    \begin{tabular}{cccccc}
        \toprule
        \diagbox{$n$}{$\tau$} & 0.4 & 0.45 & 0.49 & 0.495 & 0.498\\
        \midrule
        20 & 0.40 & 5.76 & 22.0 & 312 & 6407\\
        30 & 26.4 & 682 &  & \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Gaussian Elimination}}
    \label{tab:intro_gauss}
    \end{subtable}
    \caption{\textbf{Time Complexity w.r.t Dimension and Noise Rate}. Each entry represents the running time (in seconds). The experiments for neural networks are performed on a single GPU. The experiments for Gaussian elimination are performed on a single 64 cores processor. For a neural network, the criterion for solving the LPN is that the accuracy of the network reaches $80\%$ on clean data. For Gaussian Elimination, the criterion for success is to get at least 7 correct secrets out of 10 attempts. The running time here is averaged over all runs that recover the correct secret in the time limit (3 hours). For the empty cell, no runs are successful within 3 hours.   }
    \label{tab:perf_intro}
\end{table}

\begin{setting}[Restricted Sample] \label{setting:restricted}
In the ``Restricted Sample'' setting, sample complexity is considered the prime. For the same LPN task, an algorithm that solves the task with fewer LPN oracle queries is more desirable. 
\end{setting}


In~\Cref{setting:restricted}, as the sample complexity is bounded, we would apply the \emph{search-decision} reduction first to simplify our goal. However, even distinguishing LPN data from random data is hard in this case due to the phenomenon of~\emph{overfitting}. This means when training on a small amount of data, the dataset might render the neural network to memorize all the data, resulting in poor performance on unseen inputs. We show that the commonly used regularization method named L2 regularization, or weight decay, is significantly helpful under this setting in~\Cref{sec:restricted}.
Empirical evaluations show that our algorithms achieve comparable sample complexity with state-of-the-art algorithms, the results are shown in~\Cref{tab:restricted_dimension_error}.
Together with the positive results in~\Cref{setting:abundant}, we show that neural-network based algorithms have the potential to improve the breaking algorithm of cryptography primitives.


\begin{setting}[Moderate Sample] \label{setting:moderate}
In the ``Moderate Sample'' setting, we constrain the sample complexity and seek the smallest running time. This setting is typically used as part of hybrid algorithms, where our algorithm is used to solve LPN instances reduced from BKW or other algorithms.  
\end{setting}

To further validate our points, we consider the more refined~\cref{setting:moderate}, where we consider minimizing time complexity given the sample complexity. We show with experiments that with the number of samples provided by reduction algorithms like BKW, neural networks can already learn a model with moderate accuracy, even with noise as high as $0.498$. This resolves the impracticability of the algorithm we design for~\Cref{setting:abundant}, which requires too many samples. With a combination with the traditional algorithm Gauss, we can leverage neural networks to solve low dimension, high noise LPN instances faster than previously reported~\cite{DBLP:conf/crypto/EsserKM17}.


Concluding the three settings, we show that neural networks have huge potential in solving LPN in a practical sense, for metrics spanning from pure time complexity to pure sample complexity. We also show that we can already include neural networks as a building block of the \emph{reduction-decoding} scheme to accelerate the breaking of large instances of LPN problem, which to our knowledge is the first time for neural-network based algorithm to achieve comparable, or even better, performance for LPN problem.















\paragraph{Future directions.}
At the end of the introduction we would like to mention a few interesting open problems:
\begin{enumerate}
    \item The neural network structure used by our solver is quite simple -- we only use two-layer, fully-connected networks. Are there any neural networks with more dedicated structures that help to solve LPN? 
    \item Is it possible to use our technique to solve the LWE problem? Compared to LPN, LWE uses a large modulus and uses $\ell_2$ norm to measure the length of the noise. Are those differences crucial for the competitiveness of neural networks?
    \item In addition to the previous works that use neural networks in designing decoding algorithms in coding theory~\cite{nachmani2018deep,bennatan2018deep}, our work shows the neural network has a plausible advantage in decoding binary random linear codes in the high noise regimes. 
    Can we develop practically fast neural network decoding algorithms for other codes?
\end{enumerate}

\paragraph{Organization.}
The paper is organized as follows. In~\Cref{sec:prelim} we fix some notations throughout the paper. This section also contains a brief tutorial on machine learning and neural networks. In~\Cref{sec:method} we explain our techniques and algorithms used for breaking LPN. In~\Cref{sec:experiment} we conduct experiments to demonstrate the usefulness of techniques used and compare the performance of our algorithms to SOTA prior algorithms. We also provide a guideline for tuning hyperparameters for our algorithm. In~\Cref{sec:theory} we provide theories that explain the rationales of our network architectures. 





