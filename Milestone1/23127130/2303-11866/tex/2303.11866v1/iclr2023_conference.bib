@inproceedings{clip,
  author    = {Alec Radford and
               Jong Wook Kim and
               Chris Hallacy and
               Aditya Ramesh and
               Gabriel Goh and
               Sandhini Agarwal and
               Girish Sastry and
               Amanda Askell and
               Pamela Mishkin and
               Jack Clark and
               Gretchen Krueger and
               Ilya Sutskever},
  editor    = {Marina Meila and
               Tong Zhang},
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning,
               {ICML} 2021, 18-24 July 2021, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pages     = {8748--8763},
  publisher = {{PMLR}},
  year      = {2021},
  url       = {http://proceedings.mlr.press/v139/radford21a.html},
  timestamp = {Wed, 25 Aug 2021 17:11:17 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/RadfordKHRGASAM21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ufo,
  title={UFO: A UniFied TransfOrmer for Vision-Language Representation Learning},
  author={Jianfeng Wang and Xiaowei Hu and Zhe Gan and Zhengyuan Yang and Xiyang Dai and Zicheng Liu and Yumao Lu and Lijuan Wang},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.10023}
}

@inproceedings{bert,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  editor    = {Jill Burstein and
               Christy Doran and
               Thamar Solorio},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
               and Short Papers)},
  pages     = {4171--4186},
  publisher = {Association for Computational Linguistics},
  year      = {2019},
  url       = {https://doi.org/10.18653/v1/n19-1423},
  doi       = {10.18653/v1/n19-1423},
  timestamp = {Wed, 16 Mar 2022 23:55:36 +0100},
  biburl    = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{albef,
 author = {Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {9694--9705},
 publisher = {Curran Associates, Inc.},
 title = {Align before Fuse: Vision and Language Representation Learning with Momentum Distillation},
 url = {https://proceedings.neurips.cc/paper/2021/file/505259756244493872b7709a8a01b536-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{filip,
  title={FILIP: Fine-grained Interactive Language-Image Pre-Training},
  author={Lewei Yao and Runhui Huang and Lu Hou and Guansong Lu and Minzhe Niu and Hang Xu and Xiaodan Liang and Zhenguo Li and Xin Jiang and Chunjing Xu},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.07783}
}

@article{simla,
  title={Single-Stream Multi-Level Alignment for Vision-Language Pretraining},
  author={Zaid Khan and B Vijaykumar and Xiang Yu and Samuel Schulter and Manmohan Chandraker and Yun Raymond Fu},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.14395}
}

@article{declip,
  title={Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm},
  author={Yangguang Li and Feng Liang and Lichen Zhao and Yufeng Cui and Wanli Ouyang and Jing Shao and Fengwei Yu and Junjie Yan},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.05208}
}

@article{triple_contrastive_learning,
  title={Vision-Language Pre-Training with Triple Contrastive Learning},
  author={Jinyu Yang and Jiali Duan and S. Tran and Yi Xu and Sampath Chanda and Liqun Chen and Belinda Zeng and Trishul M. Chilimbi and Junzhou Huang},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.10401}
}

@article{lit,
  title={LiT: Zero-Shot Transfer with Locked-image Text Tuning},
  author={Xiaohua Zhai and Xiao Wang and Basil Mustafa and Andreas Steiner and Daniel Keysers and Alexander Kolesnikov and Lucas Beyer},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.07991}
}

@inproceedings{attention_is_all_you_need,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{vit,
  author    = {Alexey Dosovitskiy and
               Lucas Beyer and
               Alexander Kolesnikov and
               Dirk Weissenborn and
               Xiaohua Zhai and
               Thomas Unterthiner and
               Mostafa Dehghani and
               Matthias Minderer and
               Georg Heigold and
               Sylvain Gelly and
               Jakob Uszkoreit and
               Neil Houlsby},
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition
               at Scale},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=YicbFdNTTy},
  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/DosovitskiyB0WZ21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{simcse,
   title={{SimCSE}: Simple Contrastive Learning of Sentence Embeddings},
   author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
   booktitle={Empirical Methods in Natural Language Processing (EMNLP)},
   year={2021}
}

@inproceedings{dino,
  title={Emerging Properties in Self-Supervised Vision Transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J\'egou, Herv\'e  and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of the International Conference on Computer Vision (ICCV)},
  year={2021}
}

@article{beit,
  title={BEiT: BERT Pre-Training of Image Transformers},
  author={Hangbo Bao and Li Dong and Furu Wei},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.08254}
}

@article{ibot_mim,
  title={iBOT: Image BERT Pre-Training with Online Tokenizer},
  author={Jinghao Zhou and Chen Wei and Huiyu Wang and Wei Shen and Cihang Xie and Alan Loddon Yuille and Tao Kong},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.07832}
}

@article{masked_autoencoder,
  title={Masked Autoencoders Are Scalable Vision Learners},
  author={Kaiming He and Xinlei Chen and Saining Xie and Yanghao Li and Piotr Doll'ar and Ross B. Girshick},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.06377}
}

@article{infonce,
  title={Representation Learning with Contrastive Predictive Coding},
  author={A{\"a}ron van den Oord and Yazhe Li and Oriol Vinyals},
  journal={ArXiv},
  year={2018},
  volume={abs/1807.03748}
}

@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@article{coco,
  author    = {Tsung{-}Yi Lin and
               Michael Maire and
               Serge J. Belongie and
               Lubomir D. Bourdev and
               Ross B. Girshick and
               James Hays and
               Pietro Perona and
               Deva Ramanan and
               Piotr Doll{\'{a}}r and
               C. Lawrence Zitnick},
  title     = {Microsoft {COCO:} Common Objects in Context},
  journal   = {CoRR},
  volume    = {abs/1405.0312},
  year      = {2014},
  url       = {http://arxiv.org/abs/1405.0312},
  eprinttype = {arXiv},
  eprint    = {1405.0312},
  timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LinMBHPRDZ14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{karpathy_split,
  author    = {Andrej Karpathy and
               Li Fei{-}Fei},
  title     = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
  journal   = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
  volume    = {39},
  number    = {4},
  pages     = {664--676},
  year      = {2017},
  url       = {https://doi.org/10.1109/TPAMI.2016.2598339},
  doi       = {10.1109/TPAMI.2016.2598339},
  timestamp = {Mon, 22 Jul 2019 14:55:32 +0200},
  biburl    = {https://dblp.org/rec/journals/pami/KarpathyF17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{imagenetv2,
  title={Do ImageNet Classifiers Generalize to ImageNet?},
  author={Benjamin Recht and Rebecca Roelofs and Ludwig Schmidt and Vaishaal Shankar},
  journal={ArXiv},
  year={2019},
  volume={abs/1902.10811}
}

@INPROCEEDINGS{flickr30k,
  author={Plummer, Bryan A. and Wang, Liwei and Cervantes, Chris M. and Caicedo, Juan C. and Hockenmaier, Julia and Lazebnik, Svetlana},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models}, 
  year={2015},
  volume={},
  number={},
  pages={2641-2649},
  doi={10.1109/ICCV.2015.303}}
  
  
  @INPROCEEDINGS{imagenet,  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},   title={ImageNet: A large-scale hierarchical image database},   year={2009},  volume={},  number={},  pages={248-255},  doi={10.1109/CVPR.2009.5206848}}
  
  @article{resnet,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={770-778}
}

@inproceedings{deit,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Herv'e J'egou},
  booktitle={ICML},
  year={2021}
}

@article{Turc2019WellReadSL,
  title={Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation},
  author={Iulia Turc and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={ArXiv},
  year={2019},
  volume={abs/1908.08962}
}


@article{adamw,
  title={Fixing Weight Decay Regularization in Adam},
  author={Ilya Loshchilov and Frank Hutter},
  journal={ArXiv},
  year={2017},
  volume={abs/1711.05101}
}

@inproceedings{randaugment,
  author    = {Ekin D. Cubuk and
               Barret Zoph and
               Jonathon Shlens and
               Quoc V. Le},
  title     = {Randaugment: Practical automated data augmentation with a reduced
               search space},
  booktitle = {2020 {IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
               {CVPR} Workshops 2020, Seattle, WA, USA, June 14-19, 2020},
  pages     = {3008--3017},
  publisher = {Computer Vision Foundation / {IEEE}},
  year      = {2020},
  url       = {https://openaccess.thecvf.com/content\_CVPRW\_2020/html/w40/Cubuk\_Randaugment\_Practical\_Automated\_Data\_Augmentation\_With\_a\_Reduced\_Search\_Space\_CVPRW\_2020\_paper.html},
  doi       = {10.1109/CVPRW50498.2020.00359},
  timestamp = {Tue, 31 Aug 2021 14:00:09 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/CubukZSL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{virtex,
    title={{VirTex: Learning Visual Representations from Textual Annotations}},
    author={Karan Desai and Justin Johnson},
    booktitle={CVPR},
    year={2021}
}

@inproceedings{align,
  title={Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision},
  author={Chao Jia and Yinfei Yang and Ye Xia and Yi-Ting Chen and Zarana Parekh and Hieu Pham and Quoc V. Le and Yun-Hsuan Sung and Zhen Li and Tom Duerig},
  booktitle={ICML},
  year={2021}
}


@article{evaluating_clip,
  title={Evaluating CLIP: Towards Characterization of Broader Capabilities and Downstream Implications},
  author={Sandhini Agarwal and Gretchen Krueger and Jack Clark and Alec Radford and Jong Wook Kim and Miles Brundage},
  journal={ArXiv},
  year={2021},
  volume={abs/2108.02818}
}

@article{Ba2016LayerN,
  title={Layer Normalization},
  author={Jimmy Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
  journal={ArXiv},
  year={2016},
  volume={abs/1607.06450}
}

@inproceedings{basic,
  title={Combined Scaling for Open-Vocabulary Image Classification},
  author={Hieu Pham and Zihang Dai and Golnaz Ghiasi and Kenji Kawaguchi and Hanxiao Liu and Adams Wei Yu and Jiahui Yu and Yi-Ting Chen and Minh-Thang Luong and Yonghui Wu and Mingxing Tan and Quoc V. Le},
  year={2021}
}

@article{defilip,
  title={Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision},
  author={Yufeng Cui and Lichen Zhao and Feng Liang and Yangguang Li and Jing Shao},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.05796}
}

@article{blip,
  title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  author={Junnan Li and Dongxu Li and Caiming Xiong and Steven C. H. Hoi},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.12086}
}

@inproceedings{NEURIPS2021_01b7575c,
 author = {Tsimpoukelli, Maria and Menick, Jacob L and Cabi, Serkan and Eslami, S. M. Ali and Vinyals, Oriol and Hill, Felix},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {200--212},
 publisher = {Curran Associates, Inc.},
 title = {Multimodal Few-Shot Learning with Frozen Language Models},
 url = {https://proceedings.neurips.cc/paper/2021/file/01b7575c38dac42f3cfb7d500438b875-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{lu2021fpt,
  title={Pretrained Transformers as Universal Computation Engines},
  author={Kevin Lu and Aditya Grover and Pieter Abbeel and Igor Mordatch},
  journal={arXiv preprint arXiv:2103.05247},
  year={2021}
}

@article{Rothermel2021DontSY,
  title={Don't Sweep your Learning Rate under the Rug: A Closer Look at Cross-modal Transfer of Pretrained Transformers},
  author={Dan Rothermel and Margaret Li and Tim Rocktaschel and Jakob N. Foerster},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.12460}
}

@article{Li2021PrefixTuningOC,
  title={Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author={Xiang Lisa Li and Percy Liang},
  journal={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  year={2021},
  volume={abs/2101.00190}
}

@article{Lester2021ThePO,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Brian Lester and Rami Al-Rfou and Noah Constant},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.08691}
}

@inproceedings{Kovaleva2021BERTBO,
  title={BERT Busters: Outlier Dimensions that Disrupt Transformers},
  author={Olga Kovaleva and Saurabh Kulshreshtha and Anna Rogers and Anna Rumshisky},
  booktitle={FINDINGS},
  year={2021}
}

@inproceedings{Bachlechner2021ReZeroIA,
  title={ReZero is All You Need: Fast Convergence at Large Depth},
  author={Thomas C. Bachlechner and Bodhisattwa Prasad Majumder and Huanru Henry Mao and G. Cottrell and Julian McAuley},
  booktitle={UAI},
  year={2021}
}

@article{Xiong2020OnLN,
  title={On Layer Normalization in the Transformer Architecture},
  author={Ruibin Xiong and Yunchang Yang and Di He and Kai Zheng and Shuxin Zheng and Chen Xing and Huishuai Zhang and Yanyan Lan and Liwei Wang and Tie-Yan Liu},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.04745}
}

@inproceedings{Huang2020ImprovingTO,
  title={Improving Transformer Optimization Through Better Initialization},
  author={Xiaoshan Huang and Felipe P{\'e}rez and Jimmy Ba and Maksims Volkovs},
  booktitle={ICML},
  year={2020}
}

@inproceedings{DBLP:conf/nips/ZhangH20,
  author    = {Minjia Zhang and
               Yuxiong He},
  editor    = {Hugo Larochelle and
               Marc'Aurelio Ranzato and
               Raia Hadsell and
               Maria{-}Florina Balcan and
               Hsuan{-}Tien Lin},
  title     = {Accelerating Training of Transformer-Based Language Models with Progressive
               Layer Dropping},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html},
  timestamp = {Tue, 19 Jan 2021 15:57:41 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/ZhangH20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Gu2021OnTT,
  title={On the Transformer Growth for Progressive BERT Training},
  author={Xiaotao Gu and Liyuan Liu and Hongkun Yu and Jing Li and Chen Chen and Jiawei Han},
  booktitle={NAACL},
  year={2021}
}

@TECHREPORT{cifar,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {},
    year = {2009}
}

@inproceedings{svhn,
  title={Reading Digits in Natural Images with Unsupervised Feature Learning},
  author={Yuval Netzer and Tao Wang and Adam Coates and A. Bissacco and Bo Wu and A. Ng},
  year={2011}
}

@article{natural_adversarial_examples,
  title={Natural Adversarial Examples},
  author={Dan Hendrycks and Kevin Zhao and Steven Basart and Jacob Steinhardt and Dawn Song},
  journal={CVPR},
  year={2021}
}

@article{mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@article{vgg,
  title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author={Karen Simonyan and Andrew Zisserman},
  journal={CoRR},
  year={2015},
  volume={abs/1409.1556}
}

@inproceedings{bitfit,
    title = "{B}it{F}it: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models",
    author = "Ben Zaken, Elad  and
      Goldberg, Yoav  and
      Ravfogel, Shauli",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.1",
    doi = "10.18653/v1/2022.acl-short.1",
    pages = "1--9",
    abstract = "We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods.Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.",
}

@inproceedings{Houlsby2019ParameterEfficientTL,
  title={Parameter-Efficient Transfer Learning for NLP},
  author={Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
  booktitle={ICML},
  year={2019}
}

@article{gelu,
  title={Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units},
  author={Dan Hendrycks and Kevin Gimpel},
  journal={ArXiv},
  year={2016},
  volume={abs/1606.08415}
}

@misc{aggarwal2020zeroshot,
      title={Towards Zero-shot Cross-lingual Image Retrieval},
      author={Pranav Aggarwal and Ajinkya Kale},
      year={2020},
      eprint={2012.05107},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Raghu2021DoVT,
  title={Do Vision Transformers See Like Convolutional Neural Networks?},
  author={Maithra Raghu and Thomas Unterthiner and Simon Kornblith and Chiyuan Zhang and Alexey Dosovitskiy},
  booktitle={NeurIPS},
  year={2021}
}

@article{Tay2021ScaleEI,
  title={Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers},
  author={Yi Tay and Mostafa Dehghani and Jinfeng Rao and William Fedus and Samira Abnar and Hyung Won Chung and Sharan Narang and Dani Yogatama and Ashish Vaswani and Donald Metzler},
  journal={ArXiv},
  year={2021},
  volume={abs/2109.10686}
}

@article{Sung2021VLAdapterPT,
  title={VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks},
  author={Yi-Lin Sung and Jaemin Cho and Mohit Bansal},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.06825}
}

@article{He2021TowardsAU,
  title={Towards a Unified View of Parameter-Efficient Transfer Learning},
  author={Junxian He and Chunting Zhou and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.04366}
}

@inproceedings{adapters,
  title={Parameter-Efficient Transfer Learning for NLP},
  author={Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
  booktitle={ICML},
  year={2019}
}

@inproceedings{hyperformer,
  title={Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks},
  author={Karimi Mahabadi, Rabeeh and Ruder, Sebastian and Dehghani, Mostafa and Henderson, James},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2021}
}

@inproceedings{compacter,
  title={Compacter: Efficient Low-Rank Hypercomplex Adapter Layers},
  author={Rabeeh Karimi Mahabadi and James Henderson and Sebastian Ruder},
  booktitle={NeurIPS},
  year={2021}
}

@article{Liu2021PTuningVP,
  title={P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks},
  author={Xiao Liu and Kaixuan Ji and Yicheng Fu and Zhengxiao Du and Zhilin Yang and Jie Tang},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.07602}
}

@inproceedings{Eichenberg2021MAGMAM,
  title={MAGMA - Multimodal Augmentation of Generative Models through Adapter-based Finetuning},
  author={Constantin Eichenberg and Sid Black and Samuel Weinbach and Letitia Parcalabescu and Anette Frank},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2021}
}

@misc{LinearlyMappingImagePavlick2022,
  title = {Linearly {{Mapping}} from {{Image}} to {{Text Space}}},
  author = {Merullo, Jack and Castricato, Louis and Eickhoff, Carsten and Pavlick, Ellie},
  year = {2022},
  month = sep,
  number = {arXiv:2209.15162},
  eprint = {2209.15162},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.15162},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
}

@article{Li2023BLIP2BL,
  title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.12597}
}

@misc{GroundingLanguageModelsFried2023,
  title = {Grounding {{Language Models}} to {{Images}} for {{Multimodal Generation}}},
  author = {Koh, Jing Yu and Salakhutdinov, Ruslan and Fried, Daniel},
  year = {2023},
  month = jan,
  number = {arXiv:2301.13823},
  eprint = {2301.13823},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.13823},
}

@article{Khan2022SingleStreamMA,
  title={Single-Stream Multi-Level Alignment for Vision-Language Pretraining},
  author={Zaid Khan and B Vijaykumar and Xiang Yu and Samuel Schulter and Manmohan Chandraker and Yun Raymond Fu},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.14395}
}

@inproceedings{Wang2022UnifyingAT,
  title={Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework},
  author={Peng Wang and An Yang and Rui Men and Junyang Lin and Shuai Bai and Zhikang Li and Jianxin Ma and Chang Zhou and Jingren Zhou and Hongxia Yang},
  booktitle={International Conference on Machine Learning},
  year={2022}
}

@article{Zhu2021UniPerceiverPU,
  title={Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks},
  author={Xizhou Zhu and Jinguo Zhu and Hao Li and Xiaoshi Wu and Xiaogang Wang and Hongsheng Li and Xiaohua Wang and Jifeng Dai},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={16783-16794}
}

@article{Lu2022UnifiedIOAU,
  title={Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks},
  author={Jiasen Lu and Christopher Clark and Rowan Zellers and Roozbeh Mottaghi and Aniruddha Kembhavi},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.08916}
}

@article{Alayrac2022FlamingoAV,
  title={Flamingo: a Visual Language Model for Few-Shot Learning},
  author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andy Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.14198}
}
