We show that the performance of full model training for contrastive vision language alignment can be matched by updating a small number of parameters in existing vision models and language models, followed by an insertion of trainable modules.
This suggests that the current paradigm of full-model training for contrastive vision language alignment involves significant unnecessary computation, and can be replaced by parameter-efficient transfer learning when the downstream use cases are natural-language classification or image-text retrieval.
Current alignment strategies align representations from the top of each encoder stack. 
We find that in the text encoder, alignment changes the normalization parameters in the shallowest layers the most, while it is the opposite for the image encoder. 
Investigating and exploiting the asymmetry between vision and language could yield further benefits for multimodal understanding or more efficient training strategies.
For future work, it would be interesting to analyze whether CLIP-like models created through parameter-efficient transfer learning are similar to CLIP in ways other than performance --- for example, are they more or less biased? Or more or less robust to distribution shift? 
Another useful line of investigation would be probing vision-language models further to understand how alignment training effects the ability of the model to understand language. 
In summary, we believe that existing training methods are not fully exploiting the knowledge that exists in their initializations.
Our approach presents one simple but effective way to use that knowledge.
% Finally, we show that updating only a small number of parameters can be \textit{better} than full-model training for zero-shot generalization, which suggests that the contrastive alignment paradigm may be distorting the unimodal representation space.