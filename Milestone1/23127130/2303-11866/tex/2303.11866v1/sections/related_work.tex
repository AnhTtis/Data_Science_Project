\textbf{Vision-Language Pretraining} The dual-encoder CLIP \citep{clip} (400m pairs) and ALIGN \citep{align} (1b+ pairs) architectures were the first attempts at large-scale contrastive image-language alignment using the InfoNCE \citep{infonce} loss to maximize the mutual information between matched image and text pairs.
Subsequent work \citep{basic,declip,filip,defilip,triple_contrastive_learning,simla,albef} has improved on the training tasks, dataset, and architecture of CLIP.
While systems utilizing a multimodal encoder and cross attention \cite{blip,simla,Wang2022UnifyingAT,Lu2022UnifiedIOAU,Zhu2021UniPerceiverPU} perform better on benchmarks, their multimodal encoder makes them unsuitable for latency-sensitive search application, because rather than learning separate but aligned image and text embeddings, they learn a single multimodal embedding for an image-text pair.
Thus, neural search remains the domain of contrastive vision-language models.

\textbf{Frozen Language Models}
\citet{NEURIPS2021_01b7575c} demonstrated that pretrained large language models are capable of quickly adapting to image understanding.
They use an autoregressive transformer-based language model, which is frozen.
A trainable ResNet \citep{resnet} is then trained to transform images into input the frozen transformer can understand, by backpropagating the loss through the frozen transformer.
MAGMA \cite{Eichenberg2021MAGMAM}, FROMAGE \cite{GroundingLanguageModelsFried2023} and FLAMINGO \cite{Alayrac2022FlamingoAV} scaled the conceptual approach of \citet{NEURIPS2021_01b7575c} to billions of parameters, and recently, \citet{LinearlyMappingImagePavlick2022} have shown that a simple linear mapping is enough to allow a frozen large language model to (roughly) understand visual input, as long as the visual encoder has been trained to represent visual concepts aligned to language (e.g. CLIP).
However, emerging approaches such as BLIP-2 \cite{Li2023BLIP2BL} show that by combining soft prompting with a frozen LLM and a trainable visual encoder, a LLM can achieve state-of-the-art accuracy on visuolinguistic understanding tasks such as visual question answering.
\citet{lu2021fpt} propose the idea that transformers trained on language are capable of a form of universal computation, and can adapt to new tasks even if they are frozen, and do so better than fine-tuned models. 
However, \citet{Rothermel2021DontSY} find the findings may be reversed under certain hyperparameter settings.
Interestingly, both note that the normalization layers seem to play an important role in this adaptation. 

\textbf{Parameter-Efficient Finetuning}
Many forms of adapters \citep{adapters,hyperformer,compacter} have been explored in natural language processing.
VL-Adapter \citep{Sung2021VLAdapterPT} investigate adapters in vision-language, but assume aligned visual representations.
\citet{Lester2021ThePO} find that for very large language models, parameter-efficient adaptation approaches such as soft prompting are equivalent to finetuning the large language model. 
\citet{Liu2021PTuningVP} extend this finding, showing that combining soft prompting with adapters can often \textit{exceed} finetuning on a given downstream task.
Both prefix \citep{Li2021PrefixTuningOC} and prompt \citep{Lester2021ThePO} tuning can also be understood as exploiting the knowledge in frozen transformers, as their optimization loops involve freezing the language model, effectively turning it into a part of the loss.
\citet{DBLP:conf/nips/ZhangH20} develop a training scheme that progressively unfreezes / freezes layers of a transformer language model, and see significant improvements in training speed. 
Progressive growth approaches \citep{Gu2021OnTT} slowly increase the depth of a transformer as training proceeds.





\textbf{Layer Normalization in Transformers} \citet{Kovaleva2021BERTBO} find that the representations of transformers contain outlier dimensions that disrupt the quality of the learned embedding, and point to high-magnitude parameters in the layer normalization layers.
A variety of techniques targeting layer normalization in transformers have been proposed, with various benefits.
\citet{Xiong2020OnLN} prove that the placement of layer normalization layers relative to the residual connection in the transformer block contributes to learning instability under large learning rates, and propose an alternate placement. 
In contrast, FixUp \citep{Huang2020ImprovingTO} develops a novel initialization scheme for transformers that enables removing the normalization layers entirely. 
ReZero \citep{Bachlechner2021ReZeroIA} adds a learnable gate parameter to each residual connection before layer normalization, and demonstrate training extremely deep transformers quickly. 