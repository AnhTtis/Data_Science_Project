Advances in transfer learning within the field of natural language processing \citep{adapters,bitfit} have shown that when adapting to a novel task, updates to a small percentage of neurons ($<1$\%) in large, pretrained transformer-based language models can achieve nearly equivalent results to finetuning the entire model.
\cite{Sung2021VLAdapterPT} showed that given the existence of already-aligned visual representations (e.g. CLIP's visual encoder) only a small number ($4$\%) of parameters in a pretrained language model need to be updated for the language model to complete tasks such as visual question answering using the already-aligned visual representations.
However, \textit{the creation of aligned vision and language representations typically involves updating all the parameters} of a language model and a vision model, often randomly initialized \citep{clip}.
\cite{lit} find that if the weights of a pretrained vision model are used as an initialization, only the neurons of the language model need to be updated to align the visual and language representations and match or exceed the performance of full-model training, resulting in a $50$\% reduction in trainable parameters.
We take this line of investigation to its natural conclusion, asking --- \textit{given that strong, pretrained vision and language models both exist, can we minimally update both of their parameters to align their representations?}

Answering this question is valuable for two reasons.
From a practical perspective, contrastive vision-language alignment constitutes a form of large-scale pretraining and hence a heavy energy expenditure. 
Methods for parameter-efficient transfer learning result in significantly reduced GPU memory requirements, and can therefore lower energy costs.
Second, collecting millions of images with textual annotations is prohibitively expensive when millions of image-text pairs cannot be scraped from the internet, such as in the case of low resource languages or images from domains that require expert descriptions. 
In these cases, transfer learning by maximally preserving knowledge from strong, unimodal pretraining becomes compelling. 
Our contributions can be summarized as follows.
\begin{itemize}
    \item We show contrastive vision-language models can be created by updates to a relatively small ($<$7\%) set of parameters in pretrained vision and language models, which we dub \textbf{LilT} (Locked image-language tuning) for brevity.
    \item We conduct an detailed empirical study of combinations and interactions of various methods for parameter-efficient transfer learning.
    \item We show that contrastive vision-language models created with parameter-efficient transfer learning conserve useful existing knowledge from their initializations better than full model finetuning, and this has benefits in realistic scenarios.
\end{itemize}

\textbf{Limitations} Similar to \cite{virtex}, we conduct most of our experiments on the COCO dataset, and conduct additional scaling experiments with a larger dataset of $1.5$M pairs. 
There is a possibility that our conclusions may not hold beyond this range.
Second, we choose to focus on zero-shot classification and information retrieval tasks. 
Our conclusions may not hold for other uses of image-text embeddings, such as using them as input for downstream vision-language tasks.
Finally, we explicitly limit the scope of the study to transformer-based contrastive vision-language models.
Thus, our conclusions may not apply to those based on other architectures.
Despite these limitations, we believe our conclusions are useful because there are realistic situations in which there are much fewer than $1.5$M image-text pairs (e.g. low resource languages) available.

\textbf{Outline} First, we cover background material (\S\ref{sec:background}), then introduce our approach of parameter-efficient transfer learning for contrastive vision-language alignment (\S\ref{sec:methods}).
We then describe experiments and a discussion of experimental results (\S\ref{sec:experiments}), followed by related work (\S\ref{sec:related}).


% Note: I describe some of our research questions below.
% can parameter efficient transfer learning be used for creating contrastive vision language models
% how do the different methods of parameter efficient transfer learning interact with each other 
% are there differences between models created with different parameter efficient transfer learning methods

% We show that the layer normalization parameters \cite{Ba2016LayerN} in transformers \cite{attention_is_all_you_need} have an outsized effect on alignment (Table \ref{tab:ablations}, Fig. \ref{fig:layernorm-analysis}) despite being $<0.5\%$ of the total parameter count.
% We propose a generic two-part recipe for parameter efficient contrastive alignment: LilT (\textbf{L}ocked-image locked-text \textbf{T}uning) that consists of selectively unlocking specific parameters (\S\ref{sec:unlocked-layernorms}), followed by inserting adapters (\S\ref{sec:growing-adapter}).
% We show that our recipe is compatible with multiple adapter types and parameter unlocking strategies (Table \ref{tab:ablations}), and scales with model size (Fig. \ref{fig:lilt-scaling}) and dataset size (Table \ref{tab:scaling-1500k}).
% We conduct a detailed experimental evaluation (Table \ref{tab:performance}) and show LilT can obtain absolute improvements of $5.3\%$ over CLIP in a cross-lingual zero-shot retrieval setting (Table \ref{tab:xlingual}).

% We exploit this finding to devise a two-part (\S \ref{sec:growing-adapter}, \S \ref{sec:unlocked-layernorms}) strategy, LilT (\textbf{L}ocked-image locked-text \textbf{T}uning) that can align two nearly frozen transformer encoders, reducing the number of trainable parameters by $90-97\%$ while achieving highly competitive performance with large CLIP and LiT models (Table \ref{tab:performance}).