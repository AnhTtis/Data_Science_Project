\subsection{Additional Datasets}
\FloatBarrier
We conduct zero-shot classification experiments on three further datasets (Table \ref{tab:additional-datasets}): CIFAR-100 \cite{cifar}, SVHN\cite{svhn}, and ImageNet-A\cite{natural_adversarial_examples}.
As CIFAR-100 and SVHN are both standard datasets, we only briefly describe them here. 
The CIFAR-100 dataset consists of 60k 32x32 colour images divided into 100 classes containing 600 images per class. 
Each class has 500 training and 100 test images, for a total of 50k training and 10k test images.
We use the CIFAR-100 test set for the evaluations. 
SVHN is a harder version of MNIST \cite{mnist}, consisting of natural images of digits cropped from street-level pictures.
We use the 26k test images for evaluation.
ImageNet-A consists of \textit{natural adversarial examples} from the ImageNet1k distribution, which are natural, correctly labeled images that classifiers incorrectly classify with high confidence. 
We use the 7k test images. 
\input{figures/additional_datasets}
\subsection{Natural Adversarial Examples}
Vision language models display impressive performance on ImageNet-A.
ImageNet-A can be considered a "hard slice" of the ImageNet distribution, containing samples which are problematic for supervised classifiers.
Suprisingly, the zero-shot classification performance of self-supervised vision-language models on ImageNet-A matches and is sometimes greater than the performance of supervised classifiers (ResNet-50 \cite{resnet} and VGG-19 \cite{vgg}).
This may be partially due to the parameter count --- there are more total parameters in most of the vision-language models compared to the supervised CNNs.
However, considering that the vision-language models are facing a harder problem (performing zero-shot classification), their performance relative to supervised CNNs is surprising.
\subsection{Where do the models fail?}
On the SVHN dataset, performance is poor. 
The large models perform worse than random chance ($<10\%$), and the smaller the model, the better it performs. 
One explanation could be that there is no way for the models to learn a correspondence between images of digits and the name of each digit, as nothing similar appears in the COCO training distribution, which only contains common objects.
\subsection{Does pretraining matter?}
\FloatBarrier
\subsubsection{Pretraining vs. Random Initialization}
\begin{figure}[htbp]
    \centering
    \includegraphics{figures/pretraining-vs-random.pdf}
    \caption{The effect of pretraining on model performance.}
    \label{fig:pretraining-vs-random}
\end{figure}
We follow the standard training procedure  (\S \ref{sec:implementation-details}) and train a CLIP-base model where both of the encoders are initialized randomly, instead of using weights initialized from unimodally pretrained models (DeIT \cite{deit} and SimCSE \cite{simcse}).
We train three models, one for each dataset size.
The results can be seen in Fig \ref{fig:pretraining-vs-random}.
Compared to the randomly initialized model, the pretrained model is substantially better across all three datasets and all 3 model sizes. 
However, it is likely that the benefit of unimodal pretraining will be diminished as the number of training pairs available for multimodal vision-language pretraining increases, although we do not explore this.
\subsubsection{Does the kind of unimodal pretraining matter?}
\FloatBarrier
\begin{figure}[htbp]
    \centering
    \includegraphics{figures/type-of-pretraining.pdf}
    \caption{A comparison of different kinds of pretraining on LilT performance. Each model is trained on 591k pairs.}
    \label{fig:pretraining-type}
\end{figure}
We train LilT-base models with encoders initialized from different kinds of pretraining methods.
For the text encoder, we choose between \texttt{bert-base-uncased} \cite{bert} and SimCSE \cite{simcse}.
For the image encoder, we choose between DeiT\cite{deit} and DINO \cite{dino}.
We train all models on 591k pairs following \S\ref{sec:implementation-details}.
The unimodal pretraining methods chosen do have an effect on the performance on the vision-language model.
The combination of SimCSE and DeiT appears to be consistently better than other combinations, although on ImageNetV2, BERT-DeiT performs better.
\subsection{Zero-shot Prompts}
Although CLIP\cite{clip} uses a prompt ensemble, we use only a single prompt for all datasets except SVHN: \texttt{a photo of \{ \}}.
For SVHN, we use the prompt \texttt{a photo of the number \{ \}}.
\subsection{Encoder Symmetry}
\label{sec:encoder-symmetry}
Which encoder matters more? 
We train three configurations of CLIP on 5k, 50k, 591k pairs (Fig. \ref{fig:encoder-symmetry}).
One is the symmetric CLIP-base, while the two asymmetric configurations have their text encoder and image encoder respectively replaced with the "tiny" version.
Across all three dataset scales, the model with the smaller text encoder performs worse.
\citet{lit} find that on large scale data ($10$m+ pairs), the opposite holds true --- a larger image encoder is better than a larger language model. 
\begin{figure}
  \centering
  \includegraphics{figures/text-vs-image-importance.pdf}
  \caption{CLIP appears to be more sensitive to the size of the text encoder than the size of the image encoder.}
  \label{fig:encoder-symmetry}
\end{figure}
\subsection{Does LilT work with smaller models and less data?}
We test LilT and full-model training on smaller versions of transformers, corresponding to `bert-base`, `bert-small`, `bert-tiny`, and with decreasing amounts of image-text pairs (5k, 50k). 
The results are depicted in Figure \ref{fig:lilt-scaling} and Figure \ref{fig:clip-scaling} for LilT$_DA$.
There are no idiosyncratic results --- as model size is decreased, performance decreases for both full model training and parameter efficient transfer learning.
Similarly, as the amount of data decreases, performance also decreases.
This holds true for all tested combinations of dataset size and model size.
\begin{figure}
  \centering
  \includegraphics{figures/lilt_scaling.pdf}
  \caption{LilT's performance scales with increasing model size and dataset size --- it is not limited to a specific model size or dataset size. LilT$_{DA}$ is pictured.}
  \label{fig:lilt-scaling}
\end{figure}
\begin{figure}
  \centering
  \includegraphics{figures/clip_scaling.pdf}
  \caption{The performance of full-model training on smaller models and with less data.}
  \label{fig:clip-scaling}
\end{figure}