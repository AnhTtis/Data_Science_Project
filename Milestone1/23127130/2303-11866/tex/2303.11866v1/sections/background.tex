In this section, we briefly cover the mechanics of contrastive language image alignment as used by \citep{clip}, as well as the common "two-tower" \citep{lit}, dual transformer encoder architectures employed by CLIP-style models.
Contrastive language image alignment pulls representations of matched image-text pairs together, while pushing those of unmatched pairs apart. 
The goal is to learn an image encoder $f_{\theta}$ and a text encoder $g_\phi$ such that given an image-text pair $(\mathbf{x}^I, \mathbf{x}^T)$, the encoded representations $f_{\theta}\left(\boldsymbol{x}^{I}\right)$ and $g_{\phi}\left(\boldsymbol{x}^{T}\right)$ are close under a distance metric if they are semantically similar and far apart if not.
Let $\left\{x_{k}^{I}, x_{k}^{T}\right\}_{k=1}^{b}$ be a batch of $b$ image-text pairs. 
For each image $\boldsymbol{x}_{k}^{I}$ in an image-text pair $\left\{x_{k}^{I}, x_{k}^{T}\right\}$, the matched text $\boldsymbol{x}_{k}^{T}$ is the positive, while all other texts within the batch are used as negatives.
The image-to-text contrastive loss $\mathcal{L}_{k}^{I}$ for $\boldsymbol{x}_{k}^{I}$ is then 
$$
\mathcal{L}_{k}^{I}\left(\boldsymbol{x}_{k}^{I},\left\{\boldsymbol{x}_{j}^{T}\right\}_{j=1}^{b}\right)=-\frac{1}{b} \log \frac{\exp \left(s_{k, k}^{I}\right)}{\sum_{j} \exp \left(s_{k, j}^{I}\right)},
$$
where $s_{k, j}^{I}$ is the similarity of the $k$-th image to the $j$-th text.
The similarity function is usually taken to be the cosine similarity, which can be easily computed as $f_{\theta}\left(\boldsymbol{x}^{I}\right)\cdot g_{\phi}\left(\boldsymbol{x}^{T}\right)$ if the representations are normalized to unit length.
Conversely, the text-to-image contrastive loss for $\boldsymbol{x}_{k}^{T}$ is 
$$
\mathcal{L}_{k}^{T}\left(\boldsymbol{x}_{k}^{T},\left\{\boldsymbol{x}_{j}^{I}\right\}_{j=1}^{b}\right)=-\frac{1}{b} \log \frac{\exp \left(s_{k, k}^{T}\right)}{\sum_{j} \exp \left(s_{j, k}^{T}\right)}.
$$
The complete training loss then becomes
\begin{equation}
\mathcal{L}=\frac{1}{2} \sum_{k=1}^{b}\left(\mathcal{L}_{k}^{I}+\mathcal{L}_{k}^{T}\right).
\end{equation}

Architectures for contrastive language image alignment must encode both texts and images to vector representations.
This is usually implemented using separate text encoder and image encoders. 
A variety of choices are possible for these encoders, but we restrict ourselves to the popular \citep{clip,albef,declip,filip,simla,lit,triple_contrastive_learning,ufo} choice of transformer \citep{attention_is_all_you_need} architectures, specifically, the BERT \citep{bert} family of language models for the text encoder, and the ViT \citep{vit} family for the image encoder. 
Let $t(\cdot)$ denote an arbitrary architecture from one of the above families.
After consuming an input $\mathbf{x}$, the transformer $t(\cdot)$ produces a sequence of vectors $t(\mathbf{x})=\left\{\mathbf{z}_{\mathrm{cls}}, \mathbf{z}_{1}, \ldots, \mathbf{z}_{N}\right\}$, where $\mathbf{z}_{\text {cls }}$ is the embedding of the \texttt{[CLS]} token, which is taken to be the representation of the input $\mathbf{x}$ following dimensionality reduction by a trainable linear projection. 