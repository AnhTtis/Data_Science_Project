We conduct experiments on zero-shot multimodal classification, image-text retrieval, and multilingual image text retrieval to investigate the following research questions.
\begin{enumerate}
    \item Can contrastive vision language models be created through parameter-efficient transfer learning?
    \item How do different methods for parameter efficient transfer learning interact with each other?
    \item Do contrastive vision language models created through parameter-efficient transfer learning conserve useful knowledge from their initializations better than full-model finetuning?
    \item Does parameter-efficient transfer learning scale with respect to model size and dataset size?
\end{enumerate}

We evaluate all models on five tasks: zero-shot natural-language guided image classification \citep{clip}, image-to-text retrieval (TR), text-to-image retrieval (IR), and 0-shot TR/IR.
For zero-shot classification, we use the ImageNetV2 \citep{imagenetv2} test set.
For IR/TR, we use the COCO2014 test split of \citet{karpathy_split}, containing 5k images and 25k captions.
For zero-shot IR/TR, we use the test set of Flickr30k\citep{flickr30k}, containing 1k images and 5k captions.
% \begin{figure}
%   \centering
%   \includegraphics{figures/clip_scaling.pdf}
%     \vspace{-8mm}
%   \caption{CLIP scaling.}
%   \label{fig:clip-scaling}
% \end{figure}

% We first conduct experiments on the scaling on CLIP models as model size changes and dataset size changes (Fig. \ref{fig:clip-scaling}).
% CLIP's scaling behavior with respect to the number of training pairs is similar across all model sizes and all five evaluation tasks. 
% Alignment is clearly possible even with less than a million image-text pairs and relatively small models (CLIP-tiny has less than half the parameters of a ResNet-50\cite{resnet}).
% Large, higher-capacity models outperform smaller models, even for small amounts of data when they may be expected to overfit.
% LilT leaves CLIP's scaling behavior largely unaffected (Fig. \ref{fig:lilt-scaling}), improving as both model size and the number of training pairs available increases.




\subsection{Ablation Study}
\label{sec:ablations}
\input{figures/ablations2}
The results of the study are displayed in Table \ref{tab:ablations}.
After updating only $0.24$\% of parameters, parameter unlocking methods achieve equivalent zero-shot classification performance to full-model training: compare (d) \& (e) to (p).
However, parameter unlocking alone is insufficient to achieve the image-text retrieval abilities of full-model training, but adapter-based methods (f-k) can match full-model training (p) in both zero-shot classification and image-text retrieval. 
BitFit and layer normalization unlocking are interchangeable as parameter unlocking strategies ($<0.2$\% difference between (f/j) and (h/i)).
LilT$_{LwA}$ (h), with the layerwise adapters, is substantially better ($\approx7\%$) at image text retrieval than LilT$_{DA}$ (f), and only slightly worse at classification.
LilT and LiT are complimentary (m/n), and it is possible to only align only \textit{one} of the encoders in a parameter-efficient manner.
While LiT (k) excels at image classification, it suffers from a similar problem as parameter unlocking strategies: it is relatively poor at image text retrieval.
% \begin{wrapfigure}{R}{0.5\textwidth}
% 	\begin{minipage}{0.5\textwidth}
% 			\vspace{-3em}
% 			\centering
% 			\includegraphics{figures/adapter_improves_lit.pdf}
% 			\caption{LiT and LilT are complementary.}
% 			\label{fig: adapter-improves-lit}
% 	\end{minipage}
% \end{wrapfigure}

\textbf{Discussion} First, it is clear that creating contrastive vision-language models through parameter-efficient transfer learning is feasible, and there are clear differences between model capabilities induced by different parameter-efficient transfer learning methods.
Layerwise adapters stand out as the parameter-efficient transfer learning strategy capable of matching or exceeding full-model training.
However, in cases where the language distribution is sufficiently simple (e.g. a list of singular words), parameter unlocking is sufficient, and easier to implement.
Deep adapters stand out for their ability to achieve \textit{better} performance than full-model training when combined with LiT (m).


\subsection{Conservation of knowledge from initialization}
We hypothesize that parameter efficient transfer learning preserves more knowledge from initialization than full model finetuning, and this is beneficial in some realistic scenarios. 
Low-resource languages likely do not have large-scale image-text pairs available to train a multimodal CLIP-like model for that language. 
However, \textit{unimodal}, multilingual language models that have been trained on a dataset containing sentences from a given low-resource language often exist. 
A possible solution in this situation is to train a CLIP-like model on available image-text pairs from a high-resource language, while using a multilingual language model as the text encoder.
The resulting model may be able to generalize to image-text retrieval tasks in a language unseen during vision-language alignment due to the multilinguality of the pretrained text encoder.
We simulate this setting by aligning a pretrained multilingual \texttt{BERT-base} model with an ImageNet-pretrained ViT-B/16 on English-only image-text pairs, and evaluate it on image-text pairs in six different languages that the model was never provided paired images for. 
If parameter-efficient training preserves more knowledge from initialization, and that knowledge is useful, we expect that the retrieval model created through parameter efficient transfer learning should retain more of its multilingual language ability, and hence display greater accuracy on non-English languages.

\input{figures/x-lingual.tex}
We reuse the English training data from \S\ref{sec:implementation-details}, and evaluate each model on the test set of \citet{aggarwal2020zeroshot}, which contains 1400 image-text pairs, split equally between Russian, Polish, Turkish, Chinese, Korean, Italian, and Spanish.
We summarize results in Table \ref{tab:xlingual}.
LilT$_{LwA}$ outperforms CLIP on $12/14$ tasks (5.3\% absolute improvement), while LilT$_{DA}$ achieves better performance than CLIP on 11/14 tasks (1.4\% absolute improvement).
This suggests that parameter-efficient transfer learning conserves more information from initialization, and that information is  useful for multimodal tasks.

% \subsection{Encoder Symmetry}
% \label{sec:encoder-symmetry}
% Which encoder matters more? 
% We train three configurations of CLIP on 5k, 50k, 591k pairs (Fig. \ref{fig:encoder-symmetry}).
% One is the symmetric CLIP-base, while the two asymmetric configurations have their text encoder and image encoder respectively replaced with the "tiny" version.
% Across all three dataset scales, the model with the smaller text encoder performs worse.
% \citet{lit} find that on large scale data ($10$m+ pairs), the opposite holds true --- a larger image encoder is better than a larger language model. 

% \begin{figure}
%   \centering
%   \includegraphics{figures/text-vs-image-importance.pdf}
%   \caption{CLIP appears to be more sensitive to the size of the text encoder than the size of the image encoder.}
%   \vspace{-3mm}
%   \label{fig:encoder-symmetry}
% \end{figure}
\subsection{Scaling with respect to data and model size}
Can parameter-efficient transfer learning take advantage of larger models and larger amounts of data? 
We test the the performance of parameter-efficient transfer learning as the amount of image-text pairs is increased to 1500k from 591k (Table \ref{tab:scaling-1500k}) and as model size is increased (Table \ref{tab:performance}) from \texttt{base} ($\approx 200$M params) to \texttt{large} ($\approx 700$M params).
When the amount of training pairs available triples, parameter-efficient transfer learning continues to match the performance of full-model training: (b) vs (d) in Table \ref{tab:scaling-1500k}.
Similarly, the performance of parameter-efficient transfer learning improves as model size increases: (a) vs (b) \& (c) vs (d) in Table \ref{tab:performance}.
\input{figures/performance}
\input{figures/scaling-1500k}


\subsection{What happens during alignment?}
\begin{figure}
  \centering
  \includegraphics{figures/layernorm-analysis-linreg.pdf}
  \caption{The depth of the layer normalization layers affects how much they are changed by alignment training, and the pattern is reversed between the image and text encoders. $\rho$ is the Pearson correlation coefficient, and the translucent blue/yellow shading indicates 95\% confidence intervals. }
  \label{fig:layernorm-analysis}
\end{figure}
We attempt to understand how alignment changes the language and vision model by studying the layer normalization layers of each model.
Let $f_{\theta}$ be an image encoder $g_\phi$ be a text encoder.
We initialize $f_{\theta}$ with weights from DEiT\cite{deit}, and $g_\phi$ with weights from SimCSE \cite{simcse}.
We then lock all parameters except the layer normalization layers (configuration (c) in Tab. \ref{tab:ablations}), and train the model following the standard CLIP training procedure, resulting in a pair of aligned encoders ($\bar{f}_{\theta}$, $\bar{g}_\phi$).
In total, we have four different models: the unaligned and aligned image encoders ($f_{\theta},\bar{f}_{\theta}$) and the unaligned and aligned text encoders ($g_\phi, \bar{g}_\phi$).
Without loss of generality, we describe our procedure for the text encoder pair ($g_\phi, \bar{g}_\phi$).
Let $\mathrm{LN}^1_i(\gamma, \beta)$ and $\mathrm{LN}^2_i(\gamma, \beta)$, denote the two normalization sublayers of the $i$-th layer in the transformer encoder stack.
For layer $i \in {1,2, \ldots N}$, we plot the $L^1$ norm of the difference between the trainable layer normalization parameters $\gamma, \beta$ of the aligned and unaligned encoders. 
We plot the results in Fig \ref{fig:layernorm-analysis}.
Surprisingly, the text and image encoders display clearly opposite patterns (negative Pearson's \textit{r}). 
In the text encoder, the difference between the aligned and unaligned layer normalization parameters decreases with depth --- layer normalization parameters in the deeper layers of the text encoder change less as a result of alignment training. 
This is the \textit{opposite} of the image encoder. 
In the image encoder, the layer normalization parameters which shift the most as a result of training are the deepest.
We conduct another experiment with 50k pairs (Fig \ref{fig:layernorm-consequences}) to test the consequences of this pattern.
% Based on (Fig \ref{fig:layernorm-analysis}), we should expect that locking the layer normalization parameters in the deepest layers of the language model and the shallowest layers of the vision model (Pattern A) should have a smaller effect on performance than the opposite pattern (Pattern B), because we are locking the layer normalization parameters which change the least. This is exactly what we find.
% \begin{figure}
%   \centering
%       \vspace{-4mm}
%   \includegraphics{figures/lilt_scaling_larger.pdf}
%   \vspace{-8mm}
%   \caption{LilT's performance scales with increasing model size and dataset size --- it is not limited to a specific model size or dataset size. LilT$_{DA}$ is pictured.}
%   \label{fig:lilt-scaling}
% \end{figure}

\textbf{Discussion} The patterns in the layer normalization layers may indicate that during alignment, the language and image modalities undergo changes at different semantic levels. The shallowest three layer normalization layers of the ViT-B/16 experience a $\approx 70\%$ lower magnitude shift than the deepest three layers. The shallow layers of a vision transformer attend more to local information \citep{Raghu2021DoVT}, while the deeper layers attend more to global context. Intuitively, this makes sense -- we should expect an asymmetry between the amount of information in a short image caption compared to a dense image. Simple natural language concepts are often visually complex. Interestingly, this has already been exploited by certain vision-language models --- \citep{simla,albef}  align \textit{the lower half} of their text encoder to the visual encoder, while using the top half for a different purpose. This makes sense, given that the lower layers of the text encoder seem to change the most during alignment.

\begin{figure}
  \centering
  \includegraphics{figures/layernorm-consequences.pdf}
  \caption{We freeze all parameters except for the LN parameters, then progressively lock LN parameters by layer. Fig \ref{fig:layernorm-analysis} suggests that freezing the LN parameters in the deepest layers of the language model and the shallowest layers of the vision model (Pattern A) should have a smaller effect on performance than the opposite pattern (Pattern B), relative to the baseline (LNs in every layer unlocked) which we observe.}
  \label{fig:layernorm-consequences}
\end{figure}
