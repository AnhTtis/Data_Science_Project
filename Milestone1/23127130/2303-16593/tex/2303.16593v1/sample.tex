%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
%%%% Small single column format, used for CIE, CSUR, DTRAP, JACM, JDIQ, JEA, JERIC, JETC, PACMCGIT, TAAS, TACCESS, TACO, TALG, TALLIP (formerly TALIP), TCPS, TDSCI, TEAC, TECS, TELO, THRI, TIIS, TIOT, TISSEC, TIST, TKDD, TMIS, TOCE, TOCHI, TOCL, TOCS, TOCT, TODAES, TODS, TOIS, TOIT, TOMACS, TOMM (formerly TOMCCAP), TOMPECS, TOMS, TOPC, TOPLAS, TOPS, TOS, TOSEM, TOSN, TQC, TRETS, TSAS, TSC, TSLP, TWEB.
% \documentclass[acmsmall]{acmart}

%%%% Large single column format, used for IMWUT, JOCCH, PACMPL, POMACS, TAP, PACMHCI
% \documentclass[acmlarge,screen]{acmart}

%%%% Large double column format, used for TOG
% \documentclass[acmtog, authorversion]{acmart}

%%%% Generic manuscript mode, required for submission
%%%% and peer review
%\documentclass[manuscript,screen,review]{acmart}
%\documentclass[manuscript,review,anonymous]{acmart}
\documentclass[manuscript]{acmart}
%documentclass[manuscript,review]{acmart}
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}
    


%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2022}
\acmYear{2022}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[CHI '23 Workshop]{Make}{Apr. 2023}{Anon.}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
% The ACM International Conference on Mobile Human-Computer Interaction
\acmBooktitle{The ACM Conference on Human Factors in Computing Systems (CHI) 2023 Workshop: Generative AI and HCI} 
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%--------package added by Yongquan Hu--------------
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subfigure} 
\usepackage{amsmath}
%\usepackage{amssymb}
\usepackage{makecell}

%-------------------------------------------------------



%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.

%Yeo: I think change to Discreet is better, easier to write, and Aaron knows alot about Discreet so he can write many things~
\title[Exploring AIGC for AR]{Exploring the Design Space of Employing AI-Generated Content for Augmented Reality Display}
% AJQ - work on title 
%MicroSense? MicroSurface? SenSurface? MicroSurf? MicroSurfSense? MicroAware? MicroAI? MicroFlush? 
%Microinteractions?? Discreet input using Micro sensing?

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Yongquan Hu}
\email{yongquan.hu@unsw.edu.au}
\affiliation{%
  \institution{University of New South Wales}
  \city{Sydney}
  \state{NSW}
  \country{Australia}
}

\author{Mingyue Yuan}
\email{mingyue.yuan@unsw.edu.au}
\affiliation{%
  \institution{University of New South Wales}
  \city{Sydney}
  \state{NSW}
  \country{Australia}
}

\author{Kaiqi Xian}
\email{kaiqi.xian@student.unsw.edu.au}
\affiliation{%
  \institution{University of New South Wales}
  \city{Sydney}
  \state{NSW}
  \country{Australia}
}

\author{Don Samitha Elvitigala}
\email{s.elvitigala@unsw.edu.au}
\affiliation{%
  \institution{University of New South Wales}
  \city{Sydney}
  \state{NSW}
  \country{Australia}
}


\author{Aaron Quigley}
\email{aaron.quigley@data61.csiro.au}
\affiliation{%
  \institution{Data61, CSIRO}
  \city{Sydney}
  \state{NSW}
  \country{Australia}
}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Anonymous and Anonymous, et al.}



\begin{abstract} % or MicroSense?



%%ABSTRACT START%%
As the two most common display content of Augmented Reality (AR), the creation process of image and text often requires a human to execute. However, due to the rapid advances in Artificial Intelligence (AI), today the media content can be automatically generated by software. The ever-improving quality of AI-generated content (AIGC) has opened up new scenarios employing such content, which is expected to be applied in AR. In this paper, we attempt to explore the design space for projecting AI-generated image and text into an AR display. Specifically, we perform an exploratory study and suggest a ``user-function-environment'' design thinking by building a preliminary prototype and conducting focus groups based on it. With the early insights presented, we point out the design space and potential applications for combining AIGC and AR.


\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% ACM Classfication
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003120.10003121</concept_id>
<concept_desc>Human-centered computing~Human computer interaction (HCI)</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003120.10003121.10003125.10011752</concept_id>
<concept_desc>Human-centered computing~Haptic devices</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10003120.10003121.10003122.10003334</concept_id>
<concept_desc>Human-centered computing~User studies</concept_desc>
<concept_significance>100</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~Human computer interaction (HCI)}
\ccsdesc[300]{Human-centered computing~augmented reality—empirical study \% interactive system}
%\ccsdesc[100]{Human-centered computing~User studies}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{augmented reality display; generative AI; media generation.}


%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans thed
%% page.
% \begin{teaserfigure}
%   \centering
%   \includegraphics[width=\textwidth]{figures/teaser.png}
%   \caption{Our prototype system and the display of it: (a) The user's speech into the microphone is converted into text, which is then fed into an AI model for generating artistic images and more text; (b) Generated content in Spatial Augmented Reality (SAR): an example of Samsung Freestyle project; (c) Generated content in Head-Mounted Display (HMD): an example of Microsoft HoloLens 2; (d) Generated content in Hand-Held Display (HHD): an example of OnePlus 10 Pro.}
%   \Description{teaser image.}
%   \label{fig:prototype}
% \end{teaserfigure}


%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
% Kaiqi, when you are free, try to check and solve the warning here
\maketitle

\section{Introduction \& Related Work}

Augmented Reality (AR) serves as a means to connect the physical and digital worlds, supplementing or extending the former in contrast to Virtual Reality (VR) \cite{jung2018augmented, nebeling2018trouble}. Three main methods of AR display exist: Spatial Augmented Reality (SAR), Head-Mounted Display (HMD), and Hand-Held Display (HHD) \cite{wang2021ar, giunta2018review}. Each method presents its own advantages and disadvantages for various tasks and scenarios, as examined extensively in previous studies. For instance, SAR has been identified to merge the real and virtual worlds by directly projecting light onto physical surfaces, but it poses privacy concerns as the displayed content is publicly visible \cite{roesner2014security, kotsios2015privacy}. To address the limitations of individual AR display methods, researchers have explored the integration of multiple display technologies. For instance, Hartmann et al. proposed Augmented Augmented Reality (AAR) by combining wearable AR displays with wearable spatial augmented reality projectors to mitigate the isolated experience of using an individual AR device \cite{hartmann2020aar}. In short, AR displays offer enhanced contextual information and immersion while retaining focus on the physical world for users.

% Augmented Reality (AR) bridges the physical and digital worlds. Compared with Virtual Reality (VR), it could be seen as a supplement or extension of the  physical world \cite{jung2018augmented, nebeling2018trouble}. Broadly speaking, there are typically three methods for AR display: Spatial Augmented Reality (SAR), Head-Mounted Display (HMD) and Hand-Held Display (HHD) \cite{wang2021ar, giunta2018review}. These displays each have their own advantages and disadvantages for different scenarios and diverse tasks, which have been extensively explored by many studies. For instance, SAR has been pointed out to achieve the effect of merging the real and virtual world by projecting light directly onto physical surfaces \cite{roesner2014security, kotsios2015privacy}, but it exhibits clear privacy protection problems as the displayed content is public. 
% Existing research has identified limitations and tried to combine multiple display methods to complete the AR display system. For example, Hartmann et al. proposed Augmented Augmented Reality (AAR) by combining a wearable AR display with a wearable spatial augmented reality projector, in order to solve the problem of isolated experience of an individual wearable AR device \cite{hartmann2020aar}. In short, AR displays can provide more contextual information, increase immersion but maintain focus on the physical world for users.

AR content primarily consists of images and text \cite{bach2017drawing, weerasinghe2022arigato, chiu2018interactive, jing2019snapchart}, both of which currently require human involvement in their creation, such as 3D modeling using Unity or animation story scripting \cite{bassyouni2021augmented, vaataja2013exploring}. However, recent advancements in Artificial Intelligence (AI) technology have improved the capabilities of AI-generated content (AIGC) to the extent that the boundary between human and machine-generated content has become impressively indistinguishable. For instance, OpenAI's Generative Pre-trained Transformer 3 (GPT-3) model, the largest Natural Language Processing (NLP) model with 175 billion parameters, can automatically generate text, translate languages, and answer text-based questions \cite{dale2021gpt, floridi2020gpt}. Additionally, the Stable Diffusion \cite{rombach2022high} has gained popularity due to its faster generation speed compared to earlier models like Disco Diffusion \cite{rafael2021diffusion} and DALL·E 2 \cite{marcus2022very} for image generation, greatly reducing generation time to an average of dozens of seconds. Consequently, AI-based generative models are anticipated to become more prevalent in various applications, including automatic code generation and digital art creation, owing to their greater usability and flexibility.

% As the media content displayed by AR, there are mainly two ways of image and text \cite{bach2017drawing, weerasinghe2022arigato, chiu2018interactive, jing2019snapchart}.
% Currently, the creation of them still requires human participation, such as the modeling of 3D images in Unity, or the writing of animation story script \cite{bassyouni2021augmented, vaataja2013exploring}.
% Meanwhile, with the rapid development of Artificial Intelligence (AI) technology in recent years, the performance of AIGC (AI-Generated Content) is getting much improved, and some of those results even make the boundary between human creation and machine generation impressively indistinguishable for people. For example, for text generating, the GPT-3 is the third edition of generative pre-trained transformer model developed by OpenAI \cite{dale2021gpt, floridi2020gpt}. It contains 175 billion parameters and is by far the largest Natural Language Processing (NLP) model. There are many language task applications based on this model such as automatic text generation, automatic translation and text-based question answering \cite{floridi2020gpt}. In terms of image generation, the Stable Diffusion \cite{rombach2022high} model quickly became popular due to its better performance especially much faster generation speed compared to previous models, such as Disco Diffusion \cite{rafael2021diffusion} and DALL·E 2 \cite{marcus2022very}, especially greatly shortened generation time (average around dozens of seconds). Therefore, AI-based generative model could be expected to be widely used in various scenarios (e.g., automatic code generation, digital art creation) in the future due to the higher usability and the more flexibility.

As stated above, we see the opportunities that may open up by combining generative AI and AR (AIGC+AR), but there are few work on it. Although there have been some broad discussions on generative AI \cite{danry2022ai}, specific consideration for AR domain is still lacking. Alternatively, some work applied generative AI model to AR scenarios with more attention on technical solutions or engineering tasks, but little discussion on the design guidance and holistic system analysis of generative AI in AR \cite{xu2023generative}. Hence, to the best of our knowledge, there is no work addressing the general discussion of design space where AIGC is employed into AR display. To sum up, in this paper, we deployed generative AI models in the AR display system, and then conducted an empirical study based on this prototype, for summarizing and exploring the design space and potential applications. In particular, our contributions are as follows: 
\begin{itemize}
\item Concept and a preliminary prototype of combining two latest generative AI models with three AR display methods;
\item Focus groups and discussion summary on a ``user-function-environment'' design thinking;
\item Potential application scenarios for the combination of AIGC and AR;

\end{itemize}

%We envision GenerativeAIR as a comprehensive and discreet instance of exploring generative AR display, which the concept and summary of multimodal AI models and AR display combinations involved in could provide some insights and references for future follow-up research.



% \section{Related Work}
% Since no similar work was found, we discuss the two parts (AR display and text-input Generative AI) of the whole system respectively and introduce the related work of them separately.
% % Due to the lack of work related to our whole system, we present the related work in these subsections separately: AR for displaying and generative AI for media creation.


% \subsection{Augmented Reality Display}
% AR is usually expected to see the virtual content projected into reality through different display devices. There are three typical categories (SAR, HMD and HHD) for AR display.

% \subsubsection{Spatial Augmented Reality (SAR)}
% SAR is a traditional AR display that doesn't require a wearable device for interacting with virtual objects. However, it can be affected by occlusion and ambient light and lacks privacy when multiple users are in the same space \cite{benko2014dyadic,kotsios2015privacy,roesner2014security}. Butz et al. created a privacy model using privacy and publicity lamps to separate users, but this requires a significant amount of space and resources \cite{butz1998vampire}. Hartmann et al. proposed AAR, which combines a wearable AR display with a spatial augmented reality projector to partially address the privacy issue when sharing with multiple users \cite{hartmann2020aar}.


% \subsubsection{Head-Mounted Display (HMD)}
% HMD is a popular AR display known for creating an immersive and private experience for the user, making virtual objects harder to distinguish from reality \cite{itoh2021towards}. While the user can selectively share content, the high cost of HMD equipment hinders multi-user interaction and information sharing \cite{hartmann2020aar}. Chen et al.'s work explored multi-person collaboration by allowing other devices to join the HMD space for interaction, making low-cost sharing interactions possible \cite{chen20153d}.



% \subsubsection{Hand-Held Display (HHD)}
% % need to imprve and combine it to one paragraph!
% HHD is a handheld AR system for interacting with virtual objects in reality \cite{rekimoto1996transvision}. While the early system Transvision was influential, HHD can sometimes require continuous body rotation to get a larger view, leading to an offset from the real object, and one hand is always occupied. Researchers have addressed these limitations by converting them into input interaction, such as Anders.H et al.'s application that uses a mobile device with a camera and push button interface to provide six degrees of freedom input for interacting with 3D content \cite{henrysson2005mobile}.


% %They also tried to apply traditional 3D object manipulation techniques to manipulating objects projected by AR, which enhances the practicality of HHD \cite{henrysson2005virtual, chen20153d}.

% \subsection{Text-Input Generative Artificial Intelligence}
% %---kaiqi, dont change it--------------------
% Multimodal generative AI can automatically output corresponding results based on contextual input from the user, with text input being the most common method. Here, we only consider text and image processing methods due to the significant advancements in DL (Deep Learning) for these tasks.

% % ---------------kaiqi dont change----------------- got it
% \subsubsection{Text-Text Generative Model}
% Text generation is a field in Natural Language Processing (NLP), and text-text generation is a specific application that can be used for various purposes such as chat, writing assistance, and story extension. Early approaches to NLP used distributed representation of input data in deep generative models, with Word2Vec being a famous example proposed by Mikolov et al. in 2013 \cite{mikolov2013efficient, mikolov2013distributed}. Recurrent Neural Networks (RNN) is now a powerful algorithm for modeling sequential data in NLP \cite{sherstinsky2020fundamentals}. Generative Adversarial Networks (GAN)  \cite{ian2014generative} and Variational Auto-Encoder (VAE) \cite {kingma2013auto} have also had a revolutionary impact on text processing. GPT-3 is the latest NLP model, with 175 billion parameters and generating text of such high quality that it can be difficult to distinguish from human writing \cite{floridi2020gpt, dale2021gpt}.



% %-------------------kaiqi check this part and add citation-------------------
% \subsubsection{Text-Image Generative Model}
% Text-image generation involves generating an image based on input text. Previously, GAN \cite{ian2014generative} and VAE \cite {kingma2013auto} were commonly used for this task, but they had weaknesses such as unstable training \cite{kim2021unrealistic, gonzalez2021dynamics}. New models like Disco Diffusion \cite{rafael2021diffusion} and Stable Diffusion \cite{rombach2022high} have improved AI painting and creation, and Borji et a. found that Stable Diffusion generates more realistic faces. It's also fast and produces original art. The latest version, Stable Diffusion 2, is even better \cite{borji2022generated}. Because of its superior performance and speed, we choose Stable Diffusion as our text-image generative model \cite{vermillion2022iterating}.


%  Text-image generation is a cross-domain task, which refers to generating an image that conforms to the corresponding semantic information based on the input text. Before the diffusion model, the mainstream methods for generating images were GAN \cite{ian2014generative} and VAE \cite {kingma2013auto}, but each had its own weaknesses, such as unstable training and difficulty in guaranteeing the quality of generated images by GAN \cite{kim2021unrealistic, gonzalez2021dynamics}.
% Afterwards, the models such as and Disco Diffusion \cite{rafael2021diffusion}
% and Stable Diffusion \cite{rombach2022high}
% has highly improved the performance of AI painting and creation, and it is beginning to be difficult for people to distinguish the cartoon pictures drawn by human painter and AI. The stable diffusion model has some unique strengths. For example, Borji et al. found that the stable-diffusion model generates more realistic and better fitting faces for a given description \cite{borji2022generated}. 
% High speed, imaginative and original generation are uniquely featured by this model in the art field as well \cite{vermillion2022iterating}. Besides, the latest second version (Stable Diffusion 2) is even better. Overall, stable diffusion model is extremely superior in terms of computing time and performance, which is why we choose it as our text-image generative model. 




%=======================================================================

\section{Prototype Implementation}\label{prototype}
In order to enhance the more intuitive experience of subsequent interviewees and obtain more design references, we have developed a preliminary prototype system called ``GenerativeAIR'' (Generative AI plus AR). It could be seen as an instance of AIGC+AR to be explored. The system comprises of software (two multimodal generative AI models) and hardware (three AR display devices). We have chosen speech as the interactive input of the system, as it is a low-cost and natural way of interaction for users. The system generates media content based on text descriptions and outputs text and image displayed on different AR devices. The three types of AR displays used in our system are Samsung Freestyle projector \cite{samsungfreestle}  for SAR, HoloLens 2 \cite{hololens2} for HMD, and Oneplus 10 Pro \cite{oneplus} for HHD.

% We built a preliminary prototype for subsequent interviews. Our prototype system consists of software (two multimodal generative AI models) and hardware (three AR display devices). In account of the expression brevity, the name abbreviation ''GenerativeAIR'' (Generative AI plus AR) is used to refer to our proposed system.
% For the software part of GenerativeAIR, the modality of input and output needs to be paid attention. It should be emphasized that current generative AI models are usually based on text input (generating media content based on text description). Whereas speech can be converted into text and has a one-to-one correspondence. What's more, speaking is a low-cost and natural way of interaction for users. Therefore we consider speech as the interactive input of the system. As output, text and image are the two most common forms of media content that will be generated by the corresponding model and displayed on different AR devices. 
% For the three types of AR displays, the respective hardware devices are: Samsung Freestyle projector \cite{samsungfreestle} for SAR, AR glasses - HoloLens 2 \cite{hololens2} for HMD and Oneplus 10 Pro \cite{oneplus} for HHD. 

Specifically, we first use the built-in microphone in the mobile phone to convert the voice of the user into text due to the popularity of smartphone. Next, the application programming interface (api) provided by Google is leveraged for converting speech to text \cite{speech2text}. As for AI generation part, the interfaces of two text-input models is applied, i.e. the ChatGPT (GPT-3) \cite{chatgpt} is for text-text generation, and the Stable Diffusion 2 \cite{stablediffusion2} is for text-image generation. Furthermore, it is worth noting that these models are mounted on the cloud rather than deployed locally on the phone. The prompts for content generation and the generated results (image and text) are both transmitted through the wireless network, which inevitably leads to a certain but tolerable delay (the actual test delay in our network environment is at the millisecond  level).
 As shown in the Figure~\ref{fig:prototype}, at the beginning, the user speaks through a microphone, and then the transcribed text is used as input into two AI models to generate corresponding images and text (as shown in Figure~\ref{fig:prototype} (a)). Finally, the generated media content is transmitted to different AR devices through the network and displayed (as shown from Figure~\ref{fig:prototype} (b) to (d)).


\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{teaser_compressed.png}
  \caption{The workflow and display effect of our GenerativeAIR prototype: (a) The user's speech into the microphone is converted into text, which is then fed into an AI model for generating artistic images and more text; (b) Generated content in Spatial Augmented Reality (SAR): an example of Samsung Freestyle project; (c) Generated content in Head-Mounted Display (HMD): an example of Microsoft HoloLens 2; (d) Generated content in Hand-Held Display (HHD): an example of OnePlus 10 Pro.}
  ~\label{fig:prototype}
\end{figure}


%===========================================================================
\section{Methodology}\label{methodology}
In order to collect design factors for AIGC+AR in an open-ended fashion, we chose to use focus group interviews,  which are particularly suitable for early exploration in identifying new problems and assessing users' needs \cite{morgan1997focus}. Moreover, we let each participant freely experience the our GenerativeAIR during conducting interviews.

\subsection{Participants}
We recruited a diverse group of ten participants, 7 male and 3 female, for our focus group interviews. Participants were identified by the indices P1 to P10, and their backgrounds varied: 6 participants were academic researchers from different disciplines (computer science (4), mechanical engineering (1), and design (1)); and the remaining 4 were working professionals from various industries ( UI/UX design (2), telecommunications (1), and IT (1)). The mean age of the participants was 28.7 years (SD=6.90), and all of them had at least two years of experience studying the technology or design of AI or AR.

\subsection{Procedure}
We conducted three focus groups with a total of ten participants (G1=3; G2=3; G3=4), each lasting approximately 80 minutes and consisting of five steps. Firstly, the moderator introduced the research purpose (\textasciitilde5 mins). Secondly, participants were asked to freely experience the GenerativeAIR system and respond to any questions raised during the process (\textasciitilde15 minutes). Thirdly, participants provided self-introductions and shared their initial impressions of the GenerativeAIR system (\textasciitilde10 mins). Fourthly, In the main discussion participants freely discussed two topics: RQ1) What are the characteristics need to be considered when comparing AIGC and AR with other related technologies; RQ2) what features should be envisioned when developing the AIGC and AR technology itself (\textasciitilde40 mins). Finally, a summary and debriefing of the discussion was provided (\textasciitilde10 minutes). It is worth mentioning that in the fourth step, for the first question RQ1, we sent a questionnaire to the participants, asking them to score and compare AIGC and AR with their related technologies. More details are in the following section \ref{RQ2}.

\subsection{Analysis}
The discussions in each focus group were recorded and later transcribed and coded using Grounded Theory \cite{strauss1998basics} by our two authors. To ensure the validity of the motivation categorization, efforts were made to minimize the influence of less logical statements that are common in focus groups. Specifically, the moderator encouraged participants to reflect on and verbalize the underlying logical meaning behind their statements. During the coding phase, less logical statements without support from other statements were excluded as evidence. Furthermore, a visualization analysis is also involved for the first question RQ1, in order to better clarify the consideration factors in system design.



\section{Discussions \& Findings}\label{discussion}
In this section, we present and analyze the results of focus group discussions on the design factors that need to be paid attention to when integrating AIGC and AR by answering the following two research questions: the first question is to investigate the importance of the overall system by presenting external characteristics; the second question is for studying the performance of the system itself through clarifying internal differences.

\subsection{RQ1: What are the characteristics need to be considered when comparing AIGC and AR with other related technologies?}\label{RQ1}


In order to identify the advantages and disadvantages of AIGC+AR in comparison to related technologies and discover more suitable application scenarios in the future, a visualization method to qualitatively compare the characteristics of distinct dimension was implemented \cite{feger2022electronicsar}. Given the lack of related work that is similar to the overall system, we separately compared and analyzed its two components, namely, AR display and generative AI. The comparison was based on five different dimensions for both display performance (Figure \ref{fig:comparison} (a)) and content generation performance (Figure \ref{fig:comparison} (b)). The three gray pentagons of different sizes in the figure represent low, medium, and high levels, respectively. It should be noted that these ratings are based on extensive discussions with 10 participants, but they were not exhibited in any systematically reviewed literature. Therefore, these results cannot be considered as rigorous or unique findings, nor are they conclusive regarding the significance of related technologies. Rather, they are intended to address the external characteristics for design considerations.


\begin{figure}
\centering
\includegraphics[width=0.65\textwidth]{comparison_compressed.png}
  \caption{The comparisons of AR display+generative AI and their related techniques: (a) display performance comparison of AR, VR and normal monitor; (b) content-generation performance comparison of generative AI (machine), AI assist (machine+human) and human.}
  ~\label{fig:comparison}
\end{figure}

With respect to display performance, most participants generally believed the AR technology is superior to traditional monitors in terms of functionality, interactivity, and immersion. However, it comes at a higher cost and with lower fidelity. Here, we define that the dimension ``fidelity'' refers to the degree of similarity between the displayed virtual objects and the physical world. For example, participant P2 expressed that using AR displays can obtain more interesting and rich experiences, but the displayed virtual objects are still very different from the physical objects in the real world by saying: \textit{``I was very obsessed with Pokémon GO, a mobile AR game. Its novel operating experience and interesting settings brought me a lot of fun. Yet the display effect of Pokemon in the game is not satisfactory. For example, sometimes Pikachu will appear in the air on the edge of my table or the light and shadow of the displayed trees look strange. It is easy for people to realize that these virtual objects are fake.''}. Considering the current gap between virtual simulation and the real world, some participants thought that a higher proportion of virtual components in the display might reduce the user's real experience of the physical world.  For example, the participant P6 worried that too many 3D virtual objects could aggravate her dizziness and cause discomfort by saying: \textit{``I have 3D motion sickness, so I prefer AR to VR because it's less virtual and I feel better. I'm looking forward to having a custom function for the displayed virtual part, so that I can easily decide its position, size and proportion of the screen.''}. Certainly, there may be more complicated factors in practical cases that need to be considered in-depth in future work.
For content generation performance, all participants agreed that generative AI has an unparalleled advantage over human generation in terms of speed and complexity, whether compared to human generation alone or human-machine collaborative generation. For example, the participant P1 greatly appreciated this convenience for his life by saying \textit{``I am a painting enthusiast. Imagining that you only need to say a few words to the machine to generate a Van Gogh-style Opera House of Sydney. Generative AI is really amazing for me!''}. Participant P2 also believed that this high efficiency improved her work productivity by saying \textit{``Chatgpt can help me program ! I tried to assign some simple code tasks to it, and it can be completed very well, which greatly improved my work efficiency.''}. In addition, we noticed that there was some controversy among the participants regarding the "accuracy" rating in Figure \ref{fig:comparison} (b), as accuracy refers to the gap between the generated content and the expected results of human generation, which is a relatively subjective indicator. Some participants (e.g., P1) felt that AIGC content (such as automatically generated art drawings) was better than self-made ones by saying: \textit{``AI-generated paintings are better than mine''}, while others  (e.g., P5) thought otherwise by saying: \textit{``The layout and storytelling of AI paintings are far from meeting my expectations''}. Hence, we finally hypothesize that, regardless of complicating factors such as time cost or individual ability difference, the most satisfactory results are achieved when people are involved in the generation process. Although the results generated by state-of-the-art AI models are already close to human expectations in some specific cases, we consider the ``medium'' rating to be cautious.


\subsection{RQ2: What features should be envisioned when developing the AIGC and AR technology itself?}\label{RQ2}

The comparison of AR and AIGC and related technologies stimulates ideas for application scenarios. Further exploration of their technical features and details can improve interactive experience and system performance.

Regarding display form, there are 7 participants who all involved remarks with similar meanings: the portability of AR glasses and mobile phones distinguishes them from stationary display methods like SAR. As almost everyone now owns a smartphone, it is expected to be the most common way for following related work on generative AR display. For example, the participant P3  mentioned that mobile phone AR (HHD) has unparalleled advantages in mobility and flexibility compared to the other two AR display methods by saying: \textit{``I'm not always comfortable wearing AR glasses, and the projector works better at night. Compared with these time and space constraints, the mobile phone is more flexible and convenient because I can carry it with me anywhere and take it out to take a look at it at any time.''}. Another participant, P7, chimed in that head-mounted AR displays (HMDs) are generally expensive and have limited uses. He expressed his opinion by saying: \textit{``Although I was very impressed when I first tried Hololens 2, that feeling quickly faded away. I couldn't help but wonder if there was a good reason for me to spend over three thousand dollars on a new gadget that doesn't have much practical value. For me, the answer seems to be no.''}.
On the other hand, 4 participants who pointed out the deficiencies of HHD, i.e., the display scope of a projector is much larger than a mobile phone screen, which can hinder the recognition of generated text in AR apps. For example, the participant P8 claimed that the display content on mobile phone was difficult to view while moving by saying: \textit{``I realized that it was difficult for me to stay focused on what was displayed on my phone when I was moving around, especially small text content.''}. Therefore, tasks or scenes that rely heavily on text generation are not suitable for HHD devices.
8 participants highlighted that privacy and accessibility are important considerations, especially in multi-user collaboration and sharing scenarios. For instance, the participant P5 proposed that AIGC should be able to make some adjustments according to different situations by saying: \textit{``Content privacy issues need to be taken seriously. For different scenarios or different display modes, the displayed media content can be displayed in layers according to different permissions. In the past, this matter was usually participated by humans, but now it may be handed over to AI for automatic processing.''}. Potential solutions would be adopted for our AIGC+AR project include combining multiple display methods to surpass their limitations and providing hierarchical permissions for different users based on AI identification and authentication.

Additionally,as the main carrier of information, we noted that there is a significant difference in user experience between 2D and 3D content generation in consequence of the interview conversations. For image generation, many participants recognized that 3D virtual images can greatly increase user immersion and improve system usability. For example, the participant P9 expressed interest in trying on 3D virtual clothes by saying: \textit{``The idea of AR virtual trying on clothes is not that new, it is very interesting but also a bit troublesome because the clothes to try on always need to be manually configured by humans. Now generative AI provides new possibilities, and it may be very interesting to change clothes by speaking.''}. Nevertheless, a part of participants also showed rejection of 3D content, such as the P6 above-mentioned who has 3D motion vertigo by saying: \textit{``I don't think 3D objects in AR are necessary for me until I find a solution to my vertigo, 2D and 3D objects look fake anyway''}.
Text generation, meanwhile, is more sensitive to display size and requires sufficient space or dynamic display methods like scrolling or refresh. One of participants, P7, said such statement: \textit{``After trying your GenerativeAIR, I found that when the text generated by AI becomes too much, it is very difficult for me to read. Firstly, because of the limited size of some display devices such as head-mounted display or the small mobile screen. Secondly, since the generated content is not designed into a good layout and interaction, I have no way to adjust them.''}. Alternatively, converting text interaction to audio interaction is a practicable solution. For example, applying ChatGPT for dialog generation and display on smaller mobile phone screens may not be user-friendly.  For example, the P7 also supplemented: \textit{``Of course, we don't have to use our eyes to see. For text, it is also feasible to communicate only by voice for me.''}.






%=======================================================================
\section{Design Space}\label{design}
Via clustering and merging, we have condensed the multiple factors gathered from the focus group interviews into three overarching categories, namely ``user'', ``function'', and ``environment''. These categories are widely recognized as fundamental considerations in the design of interactive systems, as noted by previous studies \cite{villegas2018characterizing, wang2022constructing, henricksen2006developing}. Our objective is to explore the design space of AIGC+AR by investigating the relationships among these categories. Starting from user-centered thinking, our design space is structured into three aspects: what functions do the user need from the system (user-function), how the environment provides feedback to the user (user-environment) and what are the differences in the needs of different users (user-user).

\subsection{User-Function Design}
The AIGC+AR system is envisioned to offer diverse functions for multiple purposes, with particular attention paid to the AI-related software in this design phase as the AI algorithms are crucial to realizing these functions. In addition to the core generative AI, other AI algorithms can be integrated into the system to supplement and enhance its functionality, such as automated text prompts for the generated model.
\textit{``Context-aware visualization is one such function that supports user daily activities. By integrating sensors, the system can perceive and predict user behavior and provide potential visual support for tasks such as recommending applications or adapting the interface. (P10)''}.
Another function is lifelogging based on data mining and video/image abstraction. \textit{``By capturing and analyzing personal data, such as images and text on social media accounts, AI can create personalized lifelog visualizations in the form of AR displays. (P5)''}. However, issues concerning personal privacy may arise, which we will address in more detail in later sections \cite{korayem2016enhancing}.
%For example, specifically, a user named Tom is used to watching news every time he brushes his teeth. Tom's toothbrush behavior would be detected through a hybrid sensor (such as the IMU in his mobile phone plus some external additional motion sensors), then projector installed on the top of the bathroom will be turned on automatically and played daily news.


\subsection{User-Environment Design}
For such a system, the feedback supplied by the environment to the user is mainly dependent on the presentation of the AR display. As mentioned above, the three AR display methods (SAR, HMD and HHD) have separate display performances, particularly in terms of functionality, portability and privacy. \textit{``The potential application scenarios of HHD based on mobile phones are vast and varied. (P3)''}. Alternatively, in different scenarios (e.g., indoors, outdoors, working and entertainment), environmental factors (e.g., light and sound) should be included in the design as well, which is beneficial for ameliorating user experience such as immersion and interactivity. \textit{``It would be fun if a machine could somehow know my current mood and adapt the content and style of the generated image accordingly. (P4)''}.



\subsection{User-User Design}
We consider the coordination of AI and AR to meet the differences in needs among multiple users. One feasible way to classify users is: for AR display, the needs of ``presentation user'' (who conducts the AR device for presenting purpose) and ``observation user'' (who observes the AR display as a viewer) to engage with media content is asymmetric. Similar issues have been discussed in other studies. For instance, the HMD user, in Hartmann's work \cite{hartmann2020aar}, refers to the ``presentation user'', and external user there means the ``observation user'' in our paper. For ``presentation user'', apart from having a wider range of operational permissions, they care more about portability, privacy, portability. For ``observation users'', their main needs are immersion, low cognitive cost and communication with other users. Therefore, both of AI part and AR part need to be coordinated for distinctive needs for multiple users, such as hierarchical user accessibility based on AI recognition and authentication, or content sharing and switching based on different AR display methods. \textit{``If the generated content involves my privacy, such as my photo albums and life vlog, I would like AI to understand which content can be shown to my friends. If it generates or shows what I don't want others to see content, it would be too embarrassing. (P2)''}.




% \begin{table}[H]
%   \centering
%   \begin{tabular}{| l | r | r | r | }

%      %\toprule
%     % & \cmidrule(r){3-4}
%     {\small\textit{Performance}}
%     & {\small \textit{SAR}}
%       & {\small \textit{HMD}}
%         & {\small \textit{HHD}}\\
%     \midrule
    
%     \small \textit{Generated Image} & dark & fair & fair \\
%     \hline
%    \small \textit{Generated Text} & dark and fair & small & small \\
%     \hline
%     %\bottomrule
%   \end{tabular}
%   \caption{Performance of GenerativeAIR.}~\label{tab:resNet-50 and mobileNet}
% \end{table}

% \begin{table}[H]
%    \centering
%     \begin{tabular}
%     {|c|r|r|}
%         \hline
%         \multirow{2}*{a} &
%         \multicolumn{2}{c|}{b}\\
%         \cline{2-3} 
%         & c & d\\ \hline
%         e & 95 & 99 \\ \hline
%     \end{tabular}
%     \caption{Performance of GenerativeAIR.}~\label{tab:resNet-50 and mobileNet}
% \end{table}



\section{Potential Applications}\label{application}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{application_compressed.png}
  \caption{Potential Applications of GenerativeAIR: (a) Boosting real-time creative media generation; (b) Optimizing lifelog; (c) Facilitating multi-user collaboration. }
  ~\label{fig:application}
\end{figure}


In this section, we summarize and highlight three potential applications enabled by Stable Diffusion 2. Firstly, generative AI models can enhance real-time creative media generation, as illustrated in Figure \ref{fig:application} (a), where a boy wearing AR glasses interacts with a virtual teddy bear. Secondly, AR displays can optimize lifelog by supplementing captured objects with relevant information, as shown in Figure \ref{fig:application} (b) where an ice cream photo is analyzed and its calorie content is displayed. Furthermore, GenerativeAIR can address privacy and privilege classification issues in multi-user scenarios by assigning hierarchical display content based on user permissions and privacy levels through id-authentication, as depicted in Figure \ref{fig:application} (c).

To interpret these application scenarios, we employ a user-centered design approach as discussed in section \ref{design}. Specifically, (a) and (b) are intended for single users, while (c) is designed for multiple users. GenerativeAIR offers various functions to meet diverse requirements, and the interaction between user and environment remains a persistent theme.





\section{Limitations and Future Work}\label{future_work}

This work aims to explore the potential design space of generative AI for using a simple prototype GenerativeAIR. The limitations of our work primarily lie in the technical aspects that require further improvement. For example, our current prototype only generates 2D images using the Stable Diffusion model, and we acknowledge the potential benefits of generating 3D content in AR. Additionally, our prototype is currently offline and lacks real-time interaction, hindering its practical application. Future work will focus on implementing real-time functionality and integrating additional software and hardware to enrich the system's functions. Moreover, we have not addressed the hierarchical difficulty of privacy and permissions in multi-user scenarios, which is a critical issue for collaborative and sharing settings.



% \subsection{Future Work}
% Future research should be undertaken to explore the tree topics we will discuss below.

% \subsubsection{Hardware Portability}
% There are many areas that need to be optimized to make generative AIR become portable. This includes by improving the computing performance of handheld devices to prevent it becomes hot to touch or minimize the volume of head-mounted device to make it wearable but senseless.

% \subsubsection{Multi-type device interaction}
% Our findings provide the insights for the future social and multi users interaction. However, there is no enough existing research on the multi-type device AR interaction. Interaction between multiple devices can be further explored and studied, which could significantly improve the share ability of generated AR content and increase the value of potential application.  

% \subsubsection{3D conversion}
% Future research should focus on developing algorithms and techniques for accurately and effectively converting or pasting 2D generated content into 3D model. This could include investigating the use of depth information and depth maps, as well as exploring methods for generating and refining 3D models from 2D images. Considerably more work will need to be done to improving the realism and quality of the generated 3D models, as well as investigating ways to optimize the conversion process for increased speed and efficiency. 

\section{Conclusion}
This paper introduces the concept of AIGC+AR and explores its design space. We first construct a prototype by integrating two text-input generative AI models with three common AR displays in section \ref{prototype}. More details about our focus group could be seen in section \ref{methodology}. Next, in section \ref{discussion}, we provide a qualitative comparison of the advantages and disadvantages of our work compared to related studies. Also we discuss factors that need to be considered in technology development itself. Furthermore, a ``user-fucntion-environment'' design thinking is proposed and discussed section \ref{design}. Last, in section \ref{application}, we present and analyze potential application scenarios.





%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample}





\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
