\section{Conclusion and Discussion}
\paragraph{Conclusion} We evaluated transformer models and human behaviors on an unknown Chinese naming task. This task is difficult for both humans and transformer models, as the average accuracy is lower than 50\%. Humans have higher accuracy than \MODA and lower accuracy than \MODB, and the models and the humans have very similar performances. First, saliency effects were found in both human data and the models' results, suggesting that both models and humans utilize the statistical distribution of the phonetic radical to infer the character's pinyin. Further, although humans' answers are more similar to each other, our models also achieved a substantial overlap with humans' answers. Additionally, the production probability of each answer type is highly correlated between models and humans (except for \textit{semantic} type), suggesting that both models and humans are able to apply all regularity patterns in producing answers. Finally, models with radical's pinyins in the input are more similar to humans and achieved higher accuracy. 

\paragraph{Capturing quasi-regularity}
Our work is also related to the long-standing criticism that the neural networks may only learn the most \textit{frequent} class and can not extend other minority classes, thus would fail to learn the quasi-regularity in languages \cite{marcus1995german}. Previous studies on morphological inflections have shown that the neural models overgeneralized the most frequent inflections on nonce words and had almost no correlation with human's production probability on the less frequent inflections (e.g., $\rho$ = 0.05 for the /-er/ suffix in German plural \cite{mccurdy2020inflecting}, and $r$ = 0.17 for irregular English verbs \cite{corkery2019we}). However, our results showed that the transformer models could learn the quasi-regularity in Chinese character naming, that the models produce all answer types, and the production probability of each type is highly correlated with human data. 

However, our results do not contradict the previous studies. Chinese character naming and morphological inflection both exhibit quasi-regularity, but the two domains are very different: the patterns in Chinese character naming are less rule-governed. This paper's contribution to the debate of quasi-regularity in language processing is not to provide a `yes' or `no' answer; instead, we used a novel task and showed that the neural models have the potential to model human behaviors in learning quasi-regularity. We hope our study could inspire future work in this field to apply diverse tasks and conduct more detailed examinations of neural models' ability in learning quasi-regularity. 

\paragraph{Modeling Chinese reading with neural network} Our study also contributed to the current debate of whether reading skill is acquired by domain-general statistical learning mechanism \cite{plaut2005connectionist}, or language-specific knowledge such as DRC model \cite{coltheart2001drc}. Our results demonstrated that a general statistical learning mechanism (implemented as the transformer model) could learn the Chinese grapheme-phoneme mapping. We not only successfully simulated the general saliency effects in human's unknown character naming behavior, but also showed in details that the answers produced by models and humans are highly similar. 
Another contribution to modeling Chinese reading is that we are the first study that incorporated the radicals' pinyin in the model. Models with pinyin as input not only had better accuracy, but also are more similar to human behavior. Our results echoed the recent literature on the pinyin effect. For modern Chinese speakers who type characters through pinyin more often than hand-writing characters, pinyin can be an important mediator for the grapheme-phoneme mapping process.



%Previous studies on morphological inflection generation found low correlation between the models' results and human's production probability on minority classes, e.g. the English irregular verbs has a correlation of $\rho$ = 0.17 between models' results and human's production probability \cite{corkery2019we}, and the minority German plural /-er/ has a correlation of  $\rho$ = 0.05 between models and human \cite{mccurdy2020inflecting}. 
%However, our study found high correlations between humans and the aggregated models in different types of answers, even for minority types like \textit{alliterating} ($\rho$ = 0.51) and \textit{radical} ($\rho$ = 0.67). 
%The only type that is dissimilar in transformer and human is the semantic type, which is affected 


%and morphological inflection both exhibit quasi-regularity, the two domains are very different in that the patterns in character naming are more implicit and opaque. Our findings are not contradicting the results on morphological inflections, as the results can only demonstrate that the transformer successfully generalized minor classes similar to human behaviors in rare Chinese character naming tasks. However, our results show that neural networks have potentials to be applied in some quasi-regularity tasks, and well correlated with human results. 

% Previous studies on morphological inflection generation found that the models' results do not correlate well with human data's production probability. For example, for English past tense verbs, the production probability correlation between humans and the models is 0.45 for the regular verbs and 0.17 for the irregular verbs; for German plurals, correlation ranges from 0.33 - 0.05 for the 5 German plural types \cite{mccurdy2020inflecting}. These results correspond to the criticism of the neural models as cognitive models: the neural models may learn to extend the \textit{most frequent} class and struggle to generalize the minority class \cite{marcus1995german}. 
% However, our study found high correlation between of different types of answers, even for minority types like alliterating and radical. The only type that is dissimilar in transformer and human is the semantic type, which is affected by the identification of the phonetic radical. The transformer models were not given the rich linguistic input that humans have thus failed to capture human's behavior in identifying the phonetic radical. 
% In addition, although transformer models and humans are very similar in our study, our results cannot confirm that neural networks can model human behavior and are not necessarily contradicting the results on morphological inflections. Although written-spoken form mapping and morphological inflection both exhibit quasi-regularity, the two domains are very different. Therefore, our results can only show that in rare Chinese character naming tasks, the transformer is similar to human and successfully generalized minor classes similar to human behaviors. 

