\section{Comparison Between Models' Results and Human Behaviors}
\label{sec:4.3}
In this section, we compared transformer models' results in Experiments 1 (\MODA) and 2 
(\MODB) with human performance. Since human participants are different, i.e., they have different vocabularies, and they may use different strategies to identify the phonetic radical, we used all 80 models in each experiment to represent the human variety. Following \citet{corkery2019we}, each random initialization was also treated as an individual participant. Therefore, the sample size for the human participants is 55, and the sample size for the models in each experiment is 400 (80 models $\times$ 5 initialization). We focused on three types of similarities: 1) accuracy, i.e., do humans and models show similar accuracy on each character? 2) overlap, i.e., do humans and models predict the same pinyin for each character? 3) variability, i.e., do humans and models have similar answer regularity patterns?

\paragraph{Accuracy}
We calculated each character's accuracy for \MODA and \MODB. First, both models showed saliency effect: the model's character accuracy is positively correlated with saliency score (Pearson $r$ = 0.48 for \MODA and $r$ = 0.57 for \MODB), which is not significantly different from human's saliency correlation ($r$ = 0.62). In addition, there's a strong correlation between human character accuracy and both models' character accuracy (\MODA $r$ = 0.79, \MODB $r $= 0.88), suggesting that the humans and models are in high agreement. In conclusion, the transformer models' answers are very similar to the human answers in terms of character accuracy. 



\paragraph{Overlap}
\label{sec:overlap}
The overlap rate was computed to measure to what extent different human speakers (and models) predict the same answers for each character. For example, if participant 1 and 2 have 30 same answers, then the overlap rate = 50\% (30/60). Among 1,485 answer pairs of 55 human speakers, the average overlap rate of human-human is 50.2\%, with a range of 25.0\% - 73.3\%. For \MODA, among 400 models and 55 speakers, the average overlap rate for 22,000 answer pairs is 39.6\%, with a range of 12\% - 66.7\%. For \MODB, the average overlap rate of 22,000 answer pairs is 45.2\%, with a range of 16.7\% - 71.7\%. Both models' overlap rates are significantly lower than the human-human overlap rate, and \MODB's overlap rate is significantly higher than \MODA.  In addition, we computed the human-model overlap rate for different models, with 275 answer pairs for each model (5 random seeds $\times$ 55 human speakers).\footnote{See the detailed overlap results for \MODA and \MODB in Table~\ref{app_tab: overlap}, Appendix B.}
The best model for \MODA is \dall with \labmr with tone and without shuffling, with an overlap rate of 45.6\%. 
The best model for \MODB is \dallf with \labs with tone and without shuffling, with an overlap rate of 50.1\%, which is not significantly different from human-human overlap rate. The overlap results are summarized in Table \ref{tab:overlap}. The density plot of the overlap rate for human-human, human-all models, and human-best model is shown in Figure \ref{fig:overlap}. In general, the humans' answers are more similar to each other than to the models' answers. \MODB's answers are more similar to human answers than \MODA. 

\begin{table}[!ht]
\small
\centering
\begin{tabular}{lll}
\toprule
 & Overlap rate& Range \\
 \midrule
Human - Human & 50.2\std{7.0} & 25.0-73.3 \\
\midrule
\multicolumn{3}{l}{Transformer models - Human}\\
\midrule
All \MODA & 39.6*\std{7.6} & 11.7-66.7\\
Best \MODA & 45.6*\std{5.8} & 28.3-61.7\\
All \MODB & 45.3*\std{7.0} & 16.7-71.7 \\
Best \MODB & 50.1\std{6.1} & 31.7-66.7 \\
\bottomrule
\multicolumn{3}{l}{\scriptsize*indicates significantly smaller than 50.2}
\end{tabular}
\caption{\small The average overlap rate (\%) and its range for human-human and transformer-human comparison.}
\label{tab:overlap}
\end{table}
\begin{figure}[t!]
    \centering
    \includegraphics[width = 0.5\textwidth]{graph/overlap3.png}
    \caption{\small Density plot of the overlap rate.}
    \label{fig:overlap}
\end{figure}

\paragraph{Variability}
\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.95\textwidth]{graph/variety1.png}
    \caption{\small The production probability of 5 answer types produced by humans (top), \MODA (middle), and \MODB (bottom).}
    \label{fig:variety}
\end{figure*}
Like human speakers, transformer models also produce different answers for each character. We categorized these answers based on their regularity type and calculated the models' averaged production probability ($P_p$) for each answer type, as listed in Table~\ref{tab:Ttype}. We further calculated Spearman correlation ($\rho$) and Pearson correlation ($r$) between the production probability of each type in human answers and the models' answers on each character (N = 60). All the regularity types are highly correlated except for the \textit{semantic} type. The models did not produce as many \textit{semantic} type answers as humans, suggesting the models are better at identifying the phonetic radical than humans. In addition, we also calculated the cross-entropy between the humans and the models on the production probability of 5 regularity types. The cross-entropy for \MODA is $\mathrm{H}$(human, \MODA) = 1.79 and for \MODB is $\mathrm{H}$(human, \MODB) = 1.74, suggesting that \MODB is slightly more similar to the human than the \MODA. 

\begin{table}[!ht]
\small
\centering
\begin{tabular}{lllll}
\toprule
 & \makecell{\textsc{model}\\\scriptsize\textsc{[-pinyin]}} & Cor. & \makecell{\textsc{model}\\\scriptsize\textsc{[+pinyin]}} & Cor. \\
 \midrule
\multirow{2}{*}{\textit{Reg.}} & \multirow{2}{*}{39.4*\std{32.6}} & $\rho$ 0.72 & \multirow{2}{*}{52.9\std{30.3}} & $\rho$ 0.70 \\
 &  & $r$ 0.71 &  & $r$ 0.72 \\
 \hline
\multirow{2}{*}{\textit{Alli.}} & \multirow{2}{*}{10.9\std{21.4}} & $\rho$ 0.59 & \multirow{2}{*}{10.1\std{19.7}} & $\rho$ 0.51 \\
 &  & $r$ 0.95 &  & $r$ 0.85 \\
\hline
\multirow{2}{*}{\textit{Rhym.}} & \multirow{2}{*}{22.6*\std{28.4}} & $\rho$ 0.64 & \multirow{2}{*}{20.6\std{25.1}} & $\rho$ 0.70 \\
 &  & $r$ 0.58 &  & $r$ 0.62 \\
 \hline
\multirow{2}{*}{\textit{Irr.}}& \multirow{2}{*}{26.6\std{28.1}} & $\rho$ 0.55 & \multirow{2}{*}{16.1\std{20.6}} & $\rho$ 0.67 \\
 &  & $r$ 0.50 &  & $r$ 0.58 \\
 \hline
\multirow{2}{*}{\textit{Sem.}}& \multirow{2}{*}{0.5*\std{1.7}} & $\rho$ 0.39 & \multirow{2}{*}{0.2*\std{0.9}} & $\rho$ NA$\dagger$ \\
 &  & $r$ 0.30 &  & $r$ 0.09 \\
\bottomrule
\multicolumn{5}{l}{\scriptsize {\begin{tabular}[c]{@{}l@{}}* indicates significantly different from human's  ($P_p$).\\NA$\dagger$ is due to too many zeros in the data that the correlation\\ cannot be calculated\end{tabular}}}
\end{tabular}
\caption{\small The average production probability ($P_p$) of each answer type and their correlation ($\rho$ and $r$) with humans for \MODA and \MODB.}
\label{tab:Ttype}
\end{table}

The production probability of different regularity types for each character is shown in Figure \ref{fig:variety}. The answer type patterns are very similar for humans and models except for the \textit{semantic} type. Humans produced \textit{semantic} type answers for 15 characters, while both our models produced \textit{semantic} type for fewer characters with a much smaller production probability. This implied that phonetic radicals are identified differently by humans and transformer models. Humans are affected by a wide range of linguistic knowledge in identifying the phonetic radical, including the semantic meaning of the radical, vocabulary size, and reading comprehension \cite{anderson2013learning,yeh2017lexical}. The models did not receive these extra inputs, and thus did not closely capture human behavior on \textit{semantic} answer type.

