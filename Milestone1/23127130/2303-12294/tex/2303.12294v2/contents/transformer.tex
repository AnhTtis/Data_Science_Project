\section{Transformer Model}
To model the joint probability of the syllable onset and final, we used seq-to-seq transformers \cite{vaswani2017attention} to generate the pinyin of Chinese characters trained from scratch.\footnote{We did not use a classification model because there are certain rules in pinyin formation (e.g., /ü/ can not follow /b/, /p/, /m/, /f/), which requires the model to learn the syllable onsets and finals jointly.}

\subsection{Experiment Setups}
Both encoder and decoder of all our models had 2 layers, 4 attention heads, 128 expected features in the input, and 256 as the dimension of the feed-forward network model. 
For training, we split the dataset into train/dev splits of 90-10, and replace those tokens that appear once in training data by \textlangle unk\textrangle. We also set dropout to 0.1, batch size to 16, and used Adam optimizer \cite{DBLP:journals/corr/KingmaB14} with varied learning rates in the training process computed according to \citet{vaswani2017attention}. We used 5 different random seeds, and trained 40 epochs with early stopping %
for all of our experiments. For inference, we set beam size to 3. 

\subsection{Experiment 1}
\label{sec:test_acc}

We trained a set of models to simulate the grapheme-phoneme mapping process in Chinese speakers. Our \base model used the phonetic radical's orthographic forms to generate syllable onset and final (without tone) of the target character.
We further examined whether identifying the phonetic radical before generating the syllable onset and final would improve the model's performance.  %
We labeled the phonetic radical's position (left or right) with two methods: \labm and \labs. \labm used the true position of the phonetic radical as the ground truth label. 
Besides, since human speakers do not always identify the phonetic radical's position correctly,
\labs labeled the position of the phonetic radical based on the phonetic similarity. We calculated the phonetic similarity between the character's pinyin and the two radicals' pinyins using the Chinese Phonetic Similarity Estimator \cite{li2018dimsim}. The radical with higher phonetic similarity was labeled as the phonetic radical.\footnote{For example, the character `烙' $<$luo4$>$ (`flatiron') consists of the semantic radical `火' $<$huo3$>$ (`fire') and the phonetic radical `各' $<$ge4$>$ (`each'). The distance between $<$luo4$>$ and $<$huo3$>$ is 7.5, and the distance between $<$luo4$>$ and $<$ge4$>$ is 35.6. For \labs, the output radical should be `left', although the left radical `火' is the semantic radical.} We further labeled the regularity type of the characters based on \labm and \labs, hence yielding \labmr and \labsr. Examples of input and gold output in the training data are shown in Table~\ref{input_example}. All the models were trained on \dall, \dmid, and \dhigh datasets as described in section~\ref{sec:train}.

Since previous studies suggested that the regularity and consistency effects are more prominent for the characters with low frequency than high frequency  \cite[e.g.,][]{ziegler2000phonology,chen2009homophone}, the frequency of the known characters might also influence how participants predict the unknown characters. We further added the frequency label as an input feature in the full training data as the \dallf model. The characters  were categorized into four categories based on their frequency: `rare' (frequency = 1), `low' (1 < frequency $\leq$ 50\% percentile), `mid' (50\% percentile < frequency $\leq$ 75\% percentile) and `high' (frequency > 75\% percentile).%
The distribution of regularity types is similar for the characters with different frequencies. The summary of the number of characters and each regularity type can be found in Appendix B, Table~\ref{app_tab:3_freq}. 

 In addition, we added two conditions for output in training all models: Shuffling and Adding tones. We shuffle the position of the syllable onset and final in model output to explore the impact of the generated order since we don't know if the human speakers identify the syllable onset or syllable final first in character naming. We also add tones before the `End' token in the generation to see whether it improves the model performance. Examples of input and output of the conditions are shown in Table \ref{input_example}. In total, there are 80 types of models with different settings. 

\begin{table}[t!]
\small
\centering
\begin{tabular}{p{0.18\linewidth}p{0.68\linewidth}}\toprule
Input & Begin, 火, 各, End \\
\midrule
Model & Output \\
\midrule
\base & Begin, l, uo, End \\
\labm & Begin, right, l, uo, End \\
\labs & Begin, left, l, uo, End \\
\labmr & Begin, right, irregular, l, uo, End \\
\labsr & Begin, left, rhyming, l, uo, End \\
\midrule\midrule
Condition & Input \\\midrule
\dallf & Begin, 火, 各, high, End \\
\midrule
Condition & Output (\base model as an example) \\
\midrule
{[+}Shuffle{]} & Begin, uo, l, End \\
{[+}Tone{]} & Begin, l, uo, 4, End \\
\bottomrule
\end{tabular}
\caption{\small \label{input_example}Input and gold output in the training data of our models and conditions for character `烙'$<$luo4$>$, tokens are separated by comma.}
\end{table}







\paragraph{Accuracy Results}
We calculated the test accuracy the same way as for the human data: we only counted the accuracy of the syllable onset and final. For polyphone characters, as long as the model predicted one correct pinyin, it is counted as correct. The average accuracy of all 400 models (80 types x 5 random seeds) is 42.1\%, which is significantly lower than the human's accuracy (45.3\%, t = 3.15, p<0.01). The average accuracy of each type of model is listed in Table~\ref{tab:3_dfreq}. The best performing model is \dallf with \labm without tone and with shuffling, which achieved an accuracy of 50.3\%. Compared to the \base model, adding the label of phonetic position label and the character's regularity label usually could improve the model's accuracy. Adding tone would generally hurt the model's accuracy. Shuffling the syllable onset and final and adding the frequency label in the input would not change the model's accuracy.  


\begin{table}[!ht]
\small
\centering
\begin{tabular}{llllll}
\toprule
data & label & -T-S & -T+S & +T-S & +T+S \\
\midrule
\multirow{5}{*}{\dall} & \base & 49.3 & 49.3 & 42.3 & 46.0 \\
 & \labm & 48.0 & 49.7 & 45.3 & 47.7 \\
 & \labs & 46.0 & 45.3 & 42.3 & 48.7 \\
 & \labmr & 47.0 & 48.7 & 48.7 & 49.7\\
 & \labsr & 44.0 & 47.3 & 45.0 & 48.3 \\
 \midrule

\multirow{5}{*}{\dmid} & \base & 41.7 & 41.3 & 38.7 & 41.7 \\
 & \labm & 44.3 & 43.0 & 44.0 & 42.3 \\
 & \labs & 41.3 & 43.3 & 41.3 & 42.3 \\
 & \labmr & 42.0 & 40.3 & 39.7 & 44.3 \\
 & \labsr & 37.7 & 42.7 & 39.0 & 42.0 \\
 \midrule

\multirow{5}{*}{\dhigh} & \base & 28.7 & 32.3 & 29.3 & 32.3 \\
 & \labm & 36.3 & 34.7 & 30.7 & 35.3 \\
 & \labs & 32.7 & 36.0 & 30.0 & 34.0 \\
 & \labmr & 31.3 & 31.7 & 31.3 & 32.0 \\
 & \labsr & 32.3 & 32.0 & 31.0 & 33.7 \\
 \midrule

\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}\textsc{all+}\\ \textsc{freq}\end{tabular}} & \base & 46.7 & 47.0 & 47.7 & 46.7 \\
 & \labm & 49.7 & 50.3 & 47.3 & 47.0 \\
 & \labs & 45.3 & 47.3 & 47.0 & 48.3 \\
 & \labmr & 46.3 & 49.3 & 47.0 & 48.0 \\
 & \labsr & 47.7 & 44.7 & 44.0 & 47.7 \\
 \bottomrule
\end{tabular}
\caption{\label{tab:3_dfreq}The average accuracy (over 5 seeds) on test set for models trained on \dhigh, \dmid, or adding frequency label as input features on \dall. {+}T, {-}T, {+}S, {-}S refers to adding tone, no tone, shuffling, and no shuffling, respectively.}
\end{table}

\subsection{Experiment 2}
In Experiment 1, the input of our models only used the orthographic form of the radicals, which is how the previous literature described the Chinese grapheme-phoneme mapping process. However, the models might not have enough data to learn the full mapping from radicals to pinyin because many radicals only appeared once or twice in the training data since we only included compound characters with the left-right structure. For example, the phonetic radical `乘' $<$cheng$>$ only occurred once in the character `剩' $<$sheng$>$ in the training data.\footnote{We choose the first pinyin from the pinyin package for polyphone radicals.} The models would not be able to accurately learn the pinyins of these radicals. However, human speakers know the pinyin of most radicals, since many radicals are also commonly used as stand-alone characters, e.g., `乘' is a stand-alone character meaning `to multiply'. In order to better model the human speakers, it is necessary to inject pinyin of the radicals as external information to the model. The model would also benefit from the added radicals' pinyin to generate the character's pinyin. 

In addition, pinyin also plays an important role in modern Chinese speakers' reading and spelling experience. Pinyin is a Romanized phonetic coding system created in 1958 to promote literacy \cite{zhou1958}. In the information age, pinyin has become indispensable in Chinese speakers' lives because it's the dominant typing system for computers, smart phones, and electronic devices. The prevalent experience of typing characters through pinyin has challenged the traditional view that Chinese characters are processed purely through orthographic forms \cite{tan2013china}. Many recent studies have found that pinyin mediates the character recognition process\cite{chen2017effect,lyu2021comparison,yuan2022role}. To better capture modern Chinese speakers' character naming process, it is necessary to incorporate the radical's orthographic form as well as its pinyin in our models. 

Therefore, in Experiment 2, we added the radical's pinyin (syllable onset, syllable final, and tone) in the input, as shown in Table~\ref{tab:EXP2}. We used the same model variations as in Experiment 1\footnote{For the output, we added \labm, \labs, \labmr, \labsr as well as adding tone and shuffling. For the input, we added frequency label to create \dallf.} and trained 80 different types of models (5 random seeds for each type) with the new input. The training settings are the same as Experiment 1. 
\begin{table}[!ht]
    \centering
    \small
    \begin{tabular}{ll}
    \toprule
       Input  & Begin, 火, h, uo, 3, End, 各, g, e, 4, End \\
    \bottomrule
    \end{tabular}
    \caption{Input in the training data for Experiment 2 using `烙' $<$luo4$>$ as an example.}
    \label{tab:EXP2}
\end{table}

\paragraph{Accuracy Results}
Adding pinyin to the input has increased the model's accuracy.\footnote{We can not fully rule out the possibility that the increased accuracy is due to the model having longer inputs with pinyin instead of the model making use of the phonetic information. However, the input length might not have a significant impact on the models because our models with frequency labels (\dall vs \dallf) also vary in input lengths but the accuracies didn't change much.} The average accuracy of 400 models in Experiment 2 is 47.4\%, which is significantly higher than the human's accuracy (t = -2.7, p <0.01). The accuracy for each type of model is listed in Table~\ref{exp2_tab:3_dfreq} in Appendix B. The best performing model is \dallf with \labmr without tone and with shuffling, which achieved an accuracy of 55\%. The effects of different labels, adding tone, and shuffling are similar to the models in Experiment 1. 