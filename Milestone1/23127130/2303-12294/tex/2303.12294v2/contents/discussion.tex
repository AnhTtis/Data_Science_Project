\section{Conclusion and Discussion}
\paragraph{Conclusion} We evaluated transformer models and human behaviors on an unknown Chinese naming task. This task is difficult for both humans and transformer models, as the average accuracy is lower than 50\%. Humans have higher accuracy than \MODA and lower accuracy than \MODB, and the models and the humans have very similar performances. First, saliency effects were found in both human data and the models' results, suggesting that both models and humans utilize the statistical distribution of the phonetic radical to infer the character's pinyin. Further, although humans' answers are more similar to each other, our models also achieved a substantial overlap with humans' answers. Additionally, the production probability of each answer type is highly correlated between models and humans (except for \textit{semantic} type), suggesting that both models and humans are able to apply all regularity patterns in producing answers. Finally, models with radical's pinyins in the input are more similar to humans and achieved higher accuracy. 

\paragraph{Capturing quasi-regularity}
Our work is also related to the long-standing criticism that the neural networks may only learn the most \textit{frequent} class and can not extend other minority classes, thus would fail to learn the quasi-regularity in languages \cite{marcus1995german}. Previous studies on morphological inflections have shown that the neural models overgeneralized the most frequent inflections on nonce words and had almost no correlation with human's production probability on the less frequent inflections (e.g., $\rho$ = 0.05 for the /-er/ suffix in German plural \cite{mccurdy2020inflecting}, and $r$ = 0.17 for irregular English verbs \cite{corkery2019we}). However, our results showed that the transformer models could learn the quasi-regularity in Chinese character naming, that the models produce all answer types, and the production probability of each type is highly correlated with human data. 

However, our results do not contradict the previous studies. Chinese character naming and morphological inflection both exhibit quasi-regularity, but the two domains are very different: the patterns in Chinese character naming are less rule-governed. This paper's contribution to the debate of quasi-regularity in language processing is not to provide a `yes' or `no' answer; instead, we used a novel task and showed that the neural models have the potential to model human behaviors in learning quasi-regularity. We hope our study could inspire future work in this field to apply diverse tasks and conduct more detailed examinations of neural models' ability in learning quasi-regularity. 

\paragraph{Modeling Chinese reading with neural network} Our study also contributed to the current debate of whether reading skill is acquired by domain-general statistical learning mechanism \cite{plaut2005connectionist}, or language-specific knowledge such as DRC model \cite{coltheart2001drc}. Our results demonstrated that a general statistical learning mechanism (implemented as the transformer model) could learn the Chinese grapheme-phoneme mapping. We not only successfully simulated the general saliency effects in human's unknown character naming behavior, but also showed in details that the answers produced by models and humans are highly similar. 
Another contribution to modeling Chinese reading is that we are the first study that incorporated the radicals' pinyin in the model. Models with pinyin as input not only had better accuracy, but also are more similar to human behavior. Our results echoed the recent literature on the pinyin effect. For modern Chinese speakers who type characters through pinyin more often than hand-writing characters, pinyin can be an important mediator for the grapheme-phoneme mapping process.







