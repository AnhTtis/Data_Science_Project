\section{Introduction}

Time series data in the real world is high dimensional, unstructured, and complex with unique properties, leading to challenges for data modeling~\citep{yang200610}. In addition, without human recognizable patterns, it is much harder to label time series data than images and languages in real-world applications. These labeling limitations hinder deep learning methods, which typically require a huge amount of labeled data for training, being applied on time series data~\citep{eldele2021time}. Representation learning learns a fixed-dimension embedding from the original time series that keeps its inherent features. Compared to the raw time series data,  these representations are with better transferability and generalization capacity. To deal with labeling limitations, contrastive learning methods have been widely adopted in various domains for their soaring performance on representation learning, including vision, language, and graph-structured data~\citep{chen2020simple,xie2019unsupervised,you2020graph}. In a nutshell, contrastive learning methods typically train an encoder to map instances to an embedding space where dissimilar (negative) instances are easily distinguishable from similar (positive) ones and model predictions to be invariant to small noise applied to either input examples or hidden states. 

\begin{figure*}
% \vspace{-0.65cm}
    \centering
    \includegraphics[width=5in]{figures/framework.pdf}
        % \vspace{-0.2cm}
    \caption{InfoTS is composed of three parts: (1) candidate transformation that generates different augmentations of the original inputs, (2) a meta-learner network that selects the optimal augmentations, (3) an encoder that learns representations of time series instances. The meta-learner is learned in tandem with contrastive encoder learning. }\vspace{-0.6cm}
    \label{fig:framework}
\end{figure*}

Despite being effective and prevalent, contrastive learning has been less explored in the time series domain~\citep{eldele2021time,franceschi2019unsupervised,fan2020self,tonekaboni2021unsupervised}. Existing contrastive learning approaches often involve a specific data augmentation strategy that creates novel and realistic-looking training data without changing its label to construct positive alternatives for any input sample. Their success relies on carefully designed rules of thumb guided by domain expertise. Routinely used data augmentations for contrastive learning are mainly designed for image and language data, such as color distortion, flip, word replacement, and back-translation~\citep{chen2020simple,luo2021unsupervised}. These augmentation techniques generally do not apply to time series data. Recently, some researchers propose augmentations for time series to enhance the size and quality of the training data~\citep{TSAugmentationSurvey}. For example, TS-TCC~\citep{eldele2021time} and Self-Time~\citep{fan2020self} adopt jittering, scaling, and permutation strategies to generate augmented instances. Franceschi et.al. propose to extract subsequences for data augmentation~\citep{franceschi2019unsupervised}. In spite of the current progress, existing methods have two main limitations. First, unlike images with human recognizable features, time series data are often associated with inexplicable underlying patterns. Strong 
augmentation such as permutation may ruin such patterns and consequently, the model will mistake the negative handcrafts for positive ones. While weak augmentation methods such as jittering may generate augmented instances that are too similar to the raw inputs to be informative enough for contrastive learning. On the other hand, time series datasets from different domains may have diverse natures. Adapting a universal data augmentation method, such as subsequence~\citep{xie2019unsupervised}, for all datasets and tasks leads to sub-optimal performances. Other works follow empirical rules to select suitable augmentations from expensive trial-and-error. Akin to hand-crafting features, hand-picking choices of data augmentations are undesirable from the learning perspective. The diversity and heterogeneity of real-life time series data further hinder these methods away from wide applicability.

To address the challenges, we first introduce the criteria for selecting good data augmentations in contrastive learning. Data augmentation benefits generalizable, transferable, and robust representation learning by correctly extrapolating the input training space to a larger region~\citep{wilk2018learning}. The positive instances enclose a discriminative zone in which all the data points should be similar to the original instance. %Specifically, data augmentations generate modified inputs that are class-preserving. %By constraining the model to make similar predictions, we generalize the learned knowledge from limited training instances to a variety of new inputs. Such that the model can make more correct predictions in the input space. 
The desired data augmentations for contrastive representation learning should have both \textit{high fidelity} and \textit{high variety}. High fidelity encourages the augmented data to maintain the semantic identity that is invariant to transformations~\citep{wilk2018learning}. For example, if the downstream task is classification, then the generated augmentations of inputs should be class-preserving.
Meanwhile, generating augmented samples with high variety benefits representation learning by increasing the generalization capacity~\citep{chen2020simple}. From the motivation, we theoretically analyze the information flows in data augmentations based on information theory and derive the criteria for selecting desired time series augmentations. Due to the inexplicability in practical time series data, we assume that the semantic identity is presented by the target in the downstream task. Thus, high fidelity can be achieved by maximizing the mutual information between the downstream label and the augmented data. A one-hot pseudo label is assigned to each instance in the unsupervised setting when downstream labels are unavailable. These pseudo-labels encourage augmentations of different instances to be distinguishable from each other. We show that data augmentations preserving these pseudo labels can add new information without decreasing the fidelity. %from an information theory perspective.
% Concurrently, we minimize the mutual information between the augmented instances and the original instances to increase the variety of data augmentations.
Concurrently, we maximize the entropy of augmented data conditional on the original instances to increase the variety of data augmentations.


Based on the derived criteria, we propose an adaptive data augmentation method, InfoTS (as shown in Figure~\ref{fig:framework}), to avoid ad-hoc choices or painstaking trial-and-error tuning. Specifically, we utilize another neural network, denoted by meta-learner, to learn the augmentation prior in tandem with contrastive learning. The meta-learner 
automatically selects optimal augmentations from candidate augmentations to generate feasible positive samples. Along with randomly sampled negative instances, augmented instances are then fed into a time series encoder to learn representations in a contrastive manner. With a reparameterization trick, the meta-learner can be efficiently optimized with back-propagation based on the proposed criteria. Therefore, the meta-learner can automatically select data augmentations in a per dataset and per learning task manner without resorting to expert knowledge or tedious downstream validation. Our main contributions include:

\begin{itemize}
    \item We propose criteria to guide the selection of data augmentations for contrastive time series representation learning without prefabricated knowledge.
    \item We propose an approach to automatically select feasible data augmentations for different time series datasets, which can be efficiently optimized with back-propagation.
    \item We empirically verify the effectiveness of the proposed criteria to find optimal data augmentations. Extensive experiments demonstrate that InfoTS can achieve highly competitive performance with up to 12.0\% reduction in MSE on forecasting tasks and up to
    3.7\% relative improvement in accuracy on classification tasks over the leading baselines. 
\end{itemize}