\begin{document}
We appreciate the valuable feedback from all the reviewers and will include the following discussion into our work.

Review 1
Q1. The reason why InfoMin may fail to generate reasonable augmentations for time series data should be discussed clearly.
A: In the Section "Methodology", we have a subsection "Relation to InfoMin", which describes why InfoMin may not work well for time series data. "InfoMin assumes that augmented views are functions of the input, which heavily constrains the variance of data augmentations." We will add more explanation in the experimental section.

Q2. The novelty of meta-learner Network is insignificant and should be presented more clearly.
A. Thanks for the reviewer's suggestion and we will revise it accordingly. 

Q3. The related work "Time-Series Representation Learning via Temporal and Contextual Contrasting" should be taken as a baseline.
A. We did include the mentioned paper (TS-TCC) as a baseline for time series classification (Page 6 and appendix). Since Time Series Forecasting is not analyzed in the mentioned paper, we didn't use this model as a baseline in the Forecasting task. We will check whether we can extend the original method for the forecasting task in the next version.

Q4. It seems to lack experimental results to support the conclusion about the evaluation of the criteria.
A. Besides the section "Evaluation of The Criteria" which provides empirical verification regarding our claims, more extensive experiments can be found in the Appendix "Evaluation of The Criteria with Time series Classification".

Q5. Some typos should be corrected, e.g., "In Section," after "Evaluation of The Criteria.".
Thanks for pointing that out. We will fix them.


Review 2
W1. No code is provided for reproducing the experiment results.
A. We will release the code upon acceptance. 

W2. Only a linear model/SVM is used for forecasting/classification.
A. Thanks for pointing that out. We will consider more sophisticated models for evaluations. 



Review 3
Q1. Is it possible to theoretically prove Eq. (4)? Why replace with expâ¡(sim(z,z))? What is the difference compared to other MI upper bounds? Is the performance of other MI upper bounds similar to this one?
A. Yes. As shown in Poole et al. 2019, the leave-one-out upper (L1Out) bound is proved as an upper bound of mutual information.  Since this part is not our main contribution, we just select one of the most representative methods, although our framework is flexible to the selection of other MI upper bounds. We will provide a comparison between the performances of L1Out and others in the experimental section later. Thanks for the suggestion.

Q2. P4, in Relation to Information Bottleneck, how to get MI(e;x)=H(e)?
A. We briefly explain the reason in our current version. From the definition, we have MI(e;x)= H(e)-H(e|x). 
"(Information Bottleneck) the compressed representation e is a deterministic function of input x". So H(e|x)=0. We will revise it to make it more clear.

Q3. Figure 3, what is the meaning of the abscissa?
A. The x-axis is the measurement of "our proposed criteria" 


Q4. Table 3, are the experimental results for the univariate or multivariate case? And why are the results of InfoTS not the same as both univariate and multivariate experiments?
A. Thanks for pointing this out. The results are for the univariate case. Slight differences are caused by different random seeds. We will add more details to this part to make it more clear.

Q5. Appendix-Rationality of Approximation of Bernoulli Distribution with Binary Concrete Distribution, how to get P(epsilon>1-p_i )=p_i?
A. From Eq. (10), we know that epsilon~Uniform(0,1). The range between [1-pi,1] is pi. So P(epsilon>1-p_i )=p_i. We will clearly state the definition of epsilon in the appendix to make it more clear.

Q6. Appendix-Case Study on Signal Detection, the explanations are not clear enough, so it is recommended to explain the meaning of "informative signals", and the difference between Label1 and Label2 in Figure 7.
A. Thanks for the suggestion. We will add more details in our next version.

Q7. Appendix-Figure 6, Parameter a or b Sensitivity Studies, what is the value of b or a?
"a" and "b" are two hyperparameters to achieve the trade-off. Both range from 0.1 to 10.

Q8. P3, y_s may be more suitable to represent as the pseudo label of a time series instance x
Thanks for the suggestions. 





\end{document}