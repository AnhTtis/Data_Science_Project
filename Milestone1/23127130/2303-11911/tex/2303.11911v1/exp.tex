% \vspace{-0.2cm}
\section{Experiments}
% \vspace{-0.2cm}
\label{sec:exp}
In this section, we compare InfoTS with SOTA baselines on time series forecasting and classification tasks. We also conduct case studies to show insights into the proposed criteria and meta-learner network. Detailed experimental setups are shown in Appendix~\ref{sec:app:setup}. Full experimental results and extra experiments, such as parameter sensitivity studies, are presented in  Appendix~\ref{sec:app:exp}.

\begin{table*}[h!]
% \vspace{-0.5cm}
  \centering
    \caption{Univariate time series forecasting results.}
  \label{tab:forecast-univar}
  \scalebox{0.8}{
  \begin{tabular}{lccccccccccccccc}
  \toprule
         &  & \multicolumn{2}{c}{InfoTS} &
                  \multicolumn{2}{c}{TS2Vec} &
         \multicolumn{2}{c}{Informer} &
         \multicolumn{2}{c}{LogTrans} &
         \multicolumn{2}{c}{N-BEATS} &
         \multicolumn{2}{c}{TCN} &
         \multicolumn{2}{c}{LSTnet} \\
    \cmidrule(r){3-4} \cmidrule(r){5-6} \cmidrule(r){7-8} \cmidrule(r){9-10} \cmidrule(r){11-12} \cmidrule(r){13-14}  \cmidrule(r){15-16}
    Dataset & $L_y$ & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\
    \midrule
    \multirow{5}*{ETTh$_1$}
    & 24 & \textbf{0.039} & \textbf{0.149} & \textbf{0.039} & 0.152 & 0.098 & 0.247 & 0.103 & 0.259 & 0.094 & 0.238 & 0.075 & 0.210 & 0.108 & 0.284 \\
    & 48 &\textbf{0.056} & \textbf{0.179} & 0.062 & 0.191 & 0.158 & 0.319 & 0.167 & 0.328 & 0.210 & 0.367 & 0.227 & 0.402 & 0.175 & 0.424 \\
    & 168 &\textbf{0.100} & \textbf{0.239} & 0.134 & 0.282 & 0.183 & 0.346 & 0.207 & 0.375 & 0.232 & 0.391 & 0.316 & 0.493 & 0.396 & 0.504 \\
    & 336 &\textbf{0.117} & \textbf{0.264} & 0.154 & 0.310 & 0.222 & 0.387 & 0.230 & 0.398 & 0.232 & 0.388 & 0.306 & 0.495 & 0.468 & 0.593 \\
    & 720 & \textbf{0.141} & \textbf{0.302} & 0.163 & 0.327 & 0.269 & 0.435 & 0.273 & 0.463 & 0.322 & 0.490 & 0.390 & 0.557 & 0.659 & 0.766 \\
    \midrule
   
   
    \multirow{5}*{ETTh$_2$}
    & 24  &\textbf{0.081} &\textbf{0.215} & 0.090 & 0.229 & 0.093 & 0.240 & 0.102 & 0.255 & 0.198 & 0.345 & 0.103 & 0.249 & 3.554 & 0.445 \\
    & 48 & \textbf{0.115} & \textbf{0.261} & 0.124 & 0.273 & 0.155 & 0.314 & 0.169 & 0.348 & 0.234 & 0.386 & 0.142 & 0.290 & 3.190 & 0.474 \\
    & 168  &\textbf{0.171}& \textbf{0.327} & 0.208 & 0.360 & 0.232 & 0.389 & 0.246 & 0.422 & 0.331 & 0.453 & 0.227 & 0.376 & 2.800 & 0.595 \\
    & 336 & \textbf{0.183}& \textbf{0.341} & 0.213 & 0.369 & 0.263 & 0.417 & 0.267 & 0.437 & 0.431 & 0.508 & 0.296 & 0.430 & 2.753 & 0.738 \\
    & 720  &\textbf{0.194}& \textbf{0.357} & 0.214 & 0.374 & 0.277 & 0.431 & 0.303 & 0.493 & 0.437 & 0.517 & 0.325 & 0.463 & 2.878 & 1.044 \\
    
    \midrule
 

    
    \multirow{5}*{ETTm$_1$}
    & 24 & \textbf{0.014} &\textbf{0.087} & 0.015 &  0.092 & 0.030 & 0.137 & 0.065 & 0.202 & 0.054 & 0.184 & 0.041 & 0.157 & 0.090 & 0.206   \\
    & 48 & \textbf{0.025} & \textbf{0.117}  &  0.027 &  0.126 & 0.069 & 0.203 & 0.078 & 0.220 & 0.190 & 0.361 & 0.101 & 0.257 & 0.179 & 0.306 \\
    & 96 & \textbf{0.036} &\textbf{0.142}  &  0.044 &  0.161 & 0.194 & 0.372 & 0.199 & 0.386 & 0.183 & 0.353 & 0.142 & 0.311 & 0.272 & 0.399 \\
    & 288 &\textbf{0.071}& \textbf{0.200} &  0.103 &  0.246 & 0.401 & 0.554 & 0.411 & 0.572 & 0.186 & 0.362 & 0.318 & 0.472 & 0.462 & 0.558 \\
    & 672 & \textbf{0.102}& \textbf{0.240}  & 0.156 &  0.307 & 0.512 & 0.644 & 0.598 & 0.702 & 0.197 & 0.368 & 0.397 & 0.547 & 0.639 & 0.697 \\

    \midrule
  
    \multirow{5}*{Electricity}
    & 24 & \textbf{0.245} & \textbf{0.269} & 0.260 & 0.288 & 0.251 & 0.275 & 0.528 & 0.447 & 0.427 & 0.330 & 0.263 & 0.279 & 0.281 & 0.287 \\
    & 48 & \textbf{0.294} & \textbf{0.301} & 0.319 & 0.324 & 0.346 & 0.339 & 0.409 & 0.414 & 0.551 & 0.392 & 0.373 & 0.344 & 0.381 & 0.366 \\
    & 168 & \textbf{0.402} & \textbf{0.367} & 0.427 & 0.394 & 0.544 & 0.424 & 0.959 & 0.612 & 0.893 & 0.538 & 0.609 & 0.462 & 0.599 & 0.500 \\
    & 336 & \textbf{0.533} & \textbf{0.453} & 0.565 & 0.474 & 0.713 & 0.512 & 1.079 & 0.639 & 1.035 & 0.669 & 0.855 & 0.606 & 0.823 & 0.624 \\
    % & 720 & \textbf{0.876} & \textbf{0.643} & 1.182 & 0.806 & 1.001 & 0.714 & 1.548 & 0.881 & 1.263 & 0.858 & 1.278 & 0.906 \\

    \midrule
 
    \multicolumn{2}{l}{Avg.} & \textbf{0.154} & \textbf{0.253}  & 0.175 &0.278 & 0.263 & 0.367 & 0.336 & 0.419 & 0.338 & 0.402 & 0.289 & 0.359 & 1.090 & 0.516 \\
    \bottomrule
  \end{tabular}
  }
%   \vspace{-0.3cm}
\end{table*}



\begin{table*}[h]
  \centering
    \caption{Multivarite time series classification on 30 UEA datasets.}\label{tab:full-uea-unsup-sum}
    \scalebox{0.8}{
  \begin{tabular}{lccccccccccc}
  \toprule
    Method  & $\text{InfoTS}_s$ & InfoTS & TS2Vec & T-Loss & TNC & TS-TCC & TST & DTW \\
    \midrule
    Avg. ACC & \textbf{0.730}& 0.714& 0.704& 0.658 & 0.670 & 0.668 & 0.617 & 0.629 \\
    Avg. Rank & \textbf{1.967}& 2.633& 3.067& 3.833 & 4.367 & 4.167 & 5.0 & 4.366 \\
    \bottomrule
  \end{tabular}
 } \vspace{-0.5cm}
\end{table*}

\subsection{Time Series Forecasting}
% \vspace{-0.1cm}
 Time series forecasting aims to predict the future $L_y$ time stamps, with the last $L_x$ observations. We follow~\citep{yue2021learning} to train a linear model regularized with the L2 norm penalty to make predictions. The output has dimension $L_y$ in the univariate case and $L_y\times F$ for the multivariate case, where $F$ is the feature dimension. 
 
 
 


 \stitle{Datasets and Baselines.} Four benchmark datasets for time series forecasting are adopted, including ETTh1, ETTh2, ETTm1~\citep{zhou2021informer}, and the Electricity dataset~\citep{Dua2019}. These datasets are used in both univariate and multivariate settings.  We compare unsupervised InfoTS to the SOTA baselines, including TS2Vec~\citep{yue2021learning}, Informer~\citep{zhou2021informer}, StemGNN~\citep{cao2021spectral},  TCN~\citep{bai2018empirical}, LogTrans~\citep{li2019enhancing}, LSTnet~\citep{lai2018modeling}, and N-BEATS~\citep{oreshkin2019n}. Among these methods, N-BEATS is merely designed for the univariate and StemGNN is for multivariate only. We refer to~\citep{yue2021learning} to set up baselines for a fair comparison. Standard metrics for a regression problem, Mean Squared Error (MSE), and Mean Absolute Error (MAE) are utilized for evaluation. Evaluation results of univariate time series forecasting are shown in Table \ref{tab:forecast-univar}, while multivariate forecasting results are reported in the Appendix~\ref{sec:app:full} due to the space limitation. 

\stitle{Performance.} As shown in Tabel~\ref{tab:forecast-univar} and Tabel~\ref{tab:forecast-multivar}, comparison in both univariate and multivariate settings indicates that InfoTS consistently matches or outperforms the leading baselines. Some results of StemGNN are unavailable due to the out-of-memory issue~\citep{yue2021learning}. Specifically, we have the following observations. TS2Vec, another contrastive learning method with data augmentations, achieves the second-best performance in most cases. The consistent improvement of TS2Vec over other baselines indicates the effectiveness of contrastive learning for time series representations learning.  However, such universal data augmentations may not be the most informative ones to generate positive pairs. Comparing to TS2Vec, InfoTS decreases the average MSE by 12.0\%, and the average MAE by 9.0\% in the univariate setting. In the multivariate setting, the MSE and MAE decrease by 5.5\% and 2.3\%, respectively. The reason is that InfoTS can adaptively select the most suitable augmentations in a data-driven manner with high variety and high fidelity. Encoders trained with such informative augmentations learn representations with higher quality. 

\normalsize
\begin{figure}[h!]
    \centering
    \subfigure[MSE]{\includegraphics[width=1.3in]{figures/exp/criteria/mse.pdf}\label{fig:app:criteria:mse}}\quad\quad\quad\quad
    \subfigure[MAE]{\includegraphics[width=1.3in]{figures/exp/criteria/mae.pdf}\label{fig:app:criteria:mae}}\vspace{-0.2cm}
    \caption{Evaluation of the criteria on forecasting.}
    \label{fig:exp:criteria}
    \vspace{-0.5cm}
\end{figure}



% \vspace{-0.2cm}
\subsection{Time Series Classification}
% \vspace{-0.2cm}
\label{sec:exp:classifcation}
Following the previous setting, we evaluate the quality of representations on time series classification in a standard supervised manner~\citep{franceschi2019unsupervised,yue2021learning}. We train an SVM classifier with a radial basis function kernel on top of representations in the training split and then compare the prediction in the test set.

\stitle{Datasets and Baselines.} Two types of benchmark datasets are used for evaluations. The UCR archive~\citep{dau2019ucr} is a collection of 128 univariate time series datasets and the UEA archive~\citep{bredin2017tristounet} consists of 30 multivariate datasets. We compare InfoTS with baselines including TS2Vec~\citep{yue2021learning}, T-Loss~\citep{franceschi2019unsupervised}, TS-TCC~\citep{eldele2021time}, TST~\citep{zerveas2021transformer}, and DTW~\citep{franceschi2019unsupervised}. For our methods, $\text{InfoTS}_s$, training labels are only used to train the meta-learner network to select suitable augmentations, and InfoTS is with a purely unsupervised setting for representation learning.


\stitle{Performance.} The results on the UEA datasets are summarized in Table~\ref{tab:full-uea-unsup-sum}. Full results are provided in Appendix~\ref{sec:app:full}.  With the ground-truth label guiding the meta-learner network, InfoTS$_s$ substantially outperforms other baselines. On average, it improves the classification accuracy by 3.7\% over the best baseline, TS2Vec, with an average rank value 1.967 on all 30 UEA datasets.  Under the purely unsupervised setting, InfoTS preserves fidelity by adopting one-hot encoding as the pseudo labels. InfoTS achieves the second best average performance in Table~\ref{tab:full-uea-unsup-sum}, with an average rank value 2.633. Performances on the 128 UCR datasets are shown in Table~\ref{tab:app:full-ucr} in Appendix~\ref{sec:app:full}. These datasets are univariate with easily recognized patterns, where data augmentations have marginal or even negative effects~\citep{yue2021learning}. However, with adaptively selected augmentations for each dataset based on our criteria, InfoTS$_s$ and InfoTS still outperform the state-of-the-arts.





\subsection{Ablation Studies}
To present deep insights into the proposed method, we conduct multiple ablation studies on the Electricity dataset to empirically verify the effectiveness of the proposed information aware criteria and the framework to adaptively select suitable augmentations.  MSE is utilized for evaluation. 

\begin{table}[h]
  \centering
    \caption{Ablation studies on Electricity with MSE as the evaluation.}
  \label{tab:exp:ablation}
  \scalebox{0.8}{
  \begin{tabular}{lccccc}
  \toprule
        %   &\multicolumn{2}{c}{\multirow{2}*{InfoTS}} &
         &\multirow{2}*{InfoTS} &
         \multicolumn{2}{c}{Data Augmentation} &
         \multicolumn{2}{c}{Meta Objective}\\
        \cmidrule(r){3-4} \cmidrule(r){5-6}
         & &
         Random &
         All  &
         w/o Fidelity&
        w/o Variety\\
     \midrule
    24  & \textbf{0.245} & 0.252 & 0.249  &0.254  &0.251 \\
    48  & \textbf{0.295} & 0.303 & 0.303  &0.306  &0.297 \\
    168 & \textbf{0.400} & 0.414 & 0.414  &0.414  &0.409 \\
    336 & \textbf{0.541} & 0.565 & 0.563  &0.562  &0.542 \\
     \midrule
    Avg. &\textbf{0.370} &0.384  &0.382	  &0.384  &0.375 \\
    \bottomrule
  \end{tabular}}
  \vspace{-0.4cm}
\end{table}



\stitle{Evaluation of The Criteria.}
In Section~\ref{sec:criteria}, we propose information-aware criteria of data augmentations for time series that good augmentations should have high variety and fidelity. With L1Out and cross-entropy as approximations, we get the criteria in Eq.~(\ref{eq:criteria}).  To empirically verify the effectiveness of the proposed criteria, we adopt two groups of augmentations, subsequence augmentations with different lengths and jitter augmentations with different standard deviations. Subsequence augmentations work on the temporal dimension, and jitter augmentations work on the feature dimension. For the subsequence augmentations, we range the ratio of subsequences $r$ in the range $[0.01,0.99]$. The subsequence augmentation with ratio $r$ is denoted by Sub$_r$, such as Sub$_{0.01}$. For the jitter augmentations, the standard deviations are chosen from the range $[0.01,3.0]$. The jitter augmentation with standard deviation $std$  is denoted by Jitter$_{std}$, such as Jitter$_{0.01}$.

Intuitively, with $r$ increasing, Sub$_r$ generates augmented instances with lower variety and higher fidelity.  For example, with $r=0.01$, Sub$_r$ generates subsequences that only keep $1\%$ time stamps from the original input, leading to  high  variety but extremely low fidelity. Similarly, for jitter augmentations, with $std$ increasing, Jitter$_{std}$ generates augmented instances with higher variety but lower fidelity. 






In Figure~\ref{fig:exp:criteria}, we show the relationship between forecasting performance and our proposed criteria. In general, performance is positively related to the proposed criteria in both MAE and MSE settings, verifying the correctness of using the criteria as the objective in the meta-learner network training.

\stitle{Evaluation of The Meta-Learner Network.}
In this part, we empirically show the advantage of the developed meta-learner network on learning optimal augmentations. Results are shown in Table~\ref{tab:exp:ablation}. 
% To demonstrate the advantage of adaptive selection of augmentations, 
We compare InfoTS with variants ``Random'' and ``All''.  ``Random'' randomly selects an augmentation from candidate transformation functions each time and ``All'' sequentially applies transformations to generate augmented instances. Their performances are heavily affected by the low-quality candidate augmentations, verifying the key role of adaptive selection in our method. 2) To show the effects of variety and fidelity objectives in meta-learner network training, we include two variants, ``w/o Fidelity'' and ``w/o Variety'', which dismiss the fidelity or variety objective, respectively. The comparison between InfoTS and the variants empirically confirms both variety and fidelity are important for data augmentation in contrastive learning. 












