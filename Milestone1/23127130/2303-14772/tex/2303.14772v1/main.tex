\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage[table,xcdraw]{xcolor}
\usepackage{pifont}
\usepackage{array}
\usepackage[ruled]{algorithm2e}
\usepackage{diagbox}
\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}
\newcommand{\cmark}{\faCheck}%
\newcommand{\cmarkcircle}{\faCheckCircle}%
\newcommand{\xmark}{\faTimes}
\newcommand{\xmarkcircle}{\faTimesCircle}
\definecolor{Gray}{gray}{0.95}
\definecolor{highlight}{rgb}{0.824,0.976,0.824}
\definecolor{darkgray}{rgb}{0.902,0.898,0.902} 
\definecolor{dim}{rgb}{0.95,0.95,0.95}
\definecolor{uniform}{rgb}{0.702,0.702,0.702}
\definecolor{delta-color}{rgb}{0.824,0.824,0.976}
\definecolor{baseline}{gray}{0.9}
\usepackage{colortbl}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{fontawesome}
\usepackage{siunitx}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\usepackage[capitalize]{cleveref}

\iccvfinalcopy %
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\newcommand{\cls}[1]{{\small\texttt{#1}}}
\newcommand{\smethodnames}{$\Delta$-modules\xspace}
\newcommand{\smethodname}{$\Delta$-module\xspace}
\newcommand{\settingname}{$\Delta$-Patching\xspace}
\newcommand{\methodname}{$\Delta$-Networks\xspace}

\newcommand{\ve}[1]{\mathbf{#1}} %
\newcommand{\ma}[1]{\mathrm{#1}} %
\newcommand{\tr}{{^{\top}}}
\newcommand{\fr}{_\mathrm{F}}

\def\iccvPaperID{11987} %
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{\vspace{-1.0cm}$\Delta$-Networks for Efficient Model Patching}

\author{Chaitanya Devaguptapu\textsuperscript{1}\thanks{Major part of the work was done when Chaitanya was a Visiting Graduate Researcher at University of Toronto and a Masters student at IIT-Hyderabad. Corresponding author: email@chaitanya.one} \hspace{0.5cm} Samarth Sinha\textsuperscript{2,4} \hspace{0.5cm}  K J Joseph\textsuperscript{6}\\
Vineeth N Balasubramanian\textsuperscript{3} \hspace{1cm} Animesh Garg\textsuperscript{2,4,5} \\\\
\textsuperscript{1}Fujitsu Research India \hspace{0.25cm}\textsuperscript{2}University of Toronto  \hspace{0.25cm} \textsuperscript{3}Indian Institute of Technology, Hyderabad\hspace{0.25cm} \\\textsuperscript{4}Vector Institute\hspace{0.25cm} \textsuperscript{5}NVIDIA\hspace{0.25cm} \textsuperscript{6}Adobe Research\\
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi


\begin{abstract}
Models pre-trained on large-scale datasets are often finetuned to support newer tasks and datasets that arrive over time. This process necessitates storing copies of the model over time for each task that the pre-trained model is finetuned to. Building on top of recent model patching work, we propose $\Delta$-Patching for finetuning neural network models in an efficient manner, without the need to store model copies. We propose a simple and lightweight method called $\Delta$-Networks to achieve this objective. Our comprehensive experiments across setting and architecture variants show that $\Delta$-Networks outperform earlier model patching work while only requiring a fraction of parameters to be trained. We also show that this approach can be used for other problem settings such as transfer learning and zero-shot domain adaptation, as well as other tasks such as detection and segmentation.
\end{abstract}

\vspace{-10pt}
\section{Introduction}
\label{sec_intro}
\vspace{-4pt}
The number of pre-trained models available for common vision tasks has increased rapidly in recent years. As of writing this, the task of image classification alone has around 2179 pre-trained models publicly available \cite{hugging-face-2022}. These models are often trained on large-scale datasets and the features learned thereby are expected to be beneficial for a wide range of tasks. Over the years, a significant use of such pre-trained models has been in a finetuning or transfer learning context, where the model weights are partially or fully modified while maximizing performance on a target task, thereby losing performance on the primary task for which these models were originally trained. More recently, there have been efforts to interpolate such pre-trained models to improve performance on a given task \cite{modelsoups2022} or ``\textit{patch}" pre-trained models  towards improved performance on different target tasks without losing performance on the original task \cite{modelpatching2022}. Retaining the performance and features learned on the primary task helps in leveraging the model and its representations for a wide-range of tasks, without having to store multiple model copies. 

\textit{Model patching} is inspired by similar procedures and terminology in software development, and mirrors recent efforts in treating machine learning model development similar to open-source software \cite{ml-like-software-2021, modelpatching2022}. Patching large-scale models can be of significant use in many settings of practical value. For example, an apparel e-commerce company that has access to a large-scale model pre-trained on a set of apparel classes may need an updated model for newer apparel categories that arrive over time, or update the model on seasonal categories for a brief period of time -- all without losing performance on the original categories. Patching the pre-trained model on the newer task without losing performance on the original task becomes quintessential in such settings. Besides, this allows the model to continue to be re-used and finetuned for newer tasks arriving over time. 

\begin{figure}
  \includegraphics[width=0.96\linewidth]{Final.pdf}
  \caption{Comparison of \settingname with relevant learning with limited data paradigms. \settingname ensures there is \textbf{\textit{zero-drop in performance}} on the base task and \methodname (proposed approach to perform \settingname) uses \textbf{\textit{significantly less trainable parameters}} as compared to existing approaches} 
  \label{fig:teaser}
  \vspace{-0.55cm}
\end{figure}

\begin{table*}[t]
\centering
\footnotesize
\begin{tabular}{l|p{3cm}|p{2.5cm}|p{2.5cm}|p{2cm}} 
\toprule
\rowcolor{dim}\textbf{\textsc{~~Characteristics~$\rightarrow$}}&\textbf{Adapt off-the-shelf pre-trained models}&\textbf{Maintain performance on base task}&\textbf{Adapt to multiple tasks over time}&\textbf{End-to-end learning}\\
\rowcolor{dim}\textbf{\textsc{Settings~$\downarrow$}}&&&&\\
\hline
\textcolor{uniform}{Meta-Learning \cite{hospedales2021metalearning-survey}}& \textcolor{uniform}{\xmark} & \textcolor{uniform}{\xmark}& \textcolor{uniform}{\cmarkcircle} & \textcolor{uniform}{\cmark}\\ 
\textcolor{uniform}{Zero/Few-shot Learning\cite{song2022comprehensive-fslsurvery, bendre2020learning-zsl-survey}}& \textcolor{uniform}{\xmark}& \textcolor{uniform}{\cmark}& \textcolor{uniform}{\xmark}& \textcolor{uniform}{\cmark}  \\
\textcolor{uniform}{Continual Learning \cite{shaheen2022continual-survey, qu2021recent-clsurvey} }& \textcolor{uniform}{\xmark}& \textcolor{uniform}{\cmarkcircle}& \textcolor{uniform}{\cmark}& \textcolor{uniform}{\cmark}  \\ 
\hline
Transfer Learning  \cite{tl-survey}& \cmark& \xmark & \cmarkcircle& \cmark\\
Model Editing/Debugging \cite{sinitsin2020editable, ribeiro2022adaptive} & \cmark & \cmark & \xmark & \cmark          \\
Model Patching \cite{modelpatching2022}        & \cmark & \cmarkcircle & \cmark& \xmark \\
\rowcolor{highlight} \settingname  (proposed setting) & \cmark  & \cmark & \cmark & \cmark\\
\bottomrule
\end{tabular}
\vspace{-4pt}
\caption{Comparison of proposed \settingname with other related settings. \cmark: Yes, \xmark: No, \cmarkcircle: Yes in certain cases. %
Similar to \cite{modelpatching2022}, we focus on adapting off-the-shelf pre-trained models in this work, unlike zero/few-shot learning or continual learning.}
\label{tab:comparison}
\vspace{-0.3cm}
\end{table*}

In this work, we propose \settingname whose primary objective is to efficiently leverage models trained on large-scale datasets to support addition of new tasks \textit{without} degrading accuracy on tasks where performance is already adequate. While this high-level objective is shared with recent work in this area \cite{modelpatching2022}, the proposed work is different in many ways: (i) While \cite{modelpatching2022} focuses on patching open-vocabulary models like CLIP \cite{pmlr-v139-radford21a} to support multiple tasks, especially zero-shot adaptation, we focus on general-purpose Convolutional Neural Networks (CNNs) that continue to be widely used across application domains, and require frequent finetuning to new tasks. (ii) Secondly, in \cite{modelpatching2022}, given a pre-trained model $\mathcal{M}_0$, %
a copy of the model $\mathcal{M}_1$ is created and finetuned on a target task. Once the fine-tuning is complete,  $\mathcal{M}_0$ and $\mathcal{M}_1$ are linearly interpolated $\mathcal{M}_{\text{patch}} = (1 - \alpha) \mathcal{M}_{0} + \alpha \mathcal{M}_{1} $, where the value of $\alpha$ is determined empirically from a pre-defined set of values. This requires two copies of the model. In the proposed method, we achieve this objective without any model copies, using only the original model. In particular, our model, \methodname, leverages input-conditioned skip connections to perform patching in an input-adaptive and efficient manner, all while not modifying a single weight of the original model. (iii) Thirdly, the $\Delta$ in $\Delta$-patching and $\Delta$-networks specifies a small number of input-conditioned skip connections added to the model, which can be varied in discrete quanta, thus allowing to patch models in a memory-sensitive manner at different levels depending on memory and compute constraints. Also importantly, \methodname is fully-differentiable and does not require model interpolations or empirically chosen weight co-efficients. Figure \ref{fig:teaser} illustrates the proposed setting and its difference from earlier work. Table \ref{tab:comparison} presents a feature-wise comparison of the setting when compared to similar settings in related work. %








 




\noindent Our key contributions can be summarized as follows:
\vspace{-5pt}
\begin{itemize}[leftmargin=*]
\setlength\itemsep{-0.2em}
    \item We propose a new strategy for model patching called \settingname, which can allow quick adaptation of pre-trained models to new tasks, with zero loss of performance on the base task the model was originally trained on. We develop \methodname to perform \settingname.
    \item The proposed method is end-to-end differentiable and allows \textit{patching} at a preferred level based on memory/compute constraints.
    \item We test our proposed methodology on seven different datasets across different computer vision tasks including image classification, object detection and semantic segmentation, as well as other problem settings such as transfer learning and zero-shot domain adaptation. \methodname outperforms earlier work on model patching by significant amounts, while using only a fraction of trainable parameters. Our method also achieves a new state-of-the-art on the task of transfer learning.  
\end{itemize}

\section{Related Work}
\label{sec:related-work}
\vspace{-5pt}
We discuss related work below from two broad perspectives: a setting perspective and a method perspective.

\vspace{3pt}
\noindent \textbf{Related Settings.} 
We briefly discuss relevant problem settings (as in Table \ref{tab:comparison} and Fig \ref{fig:teaser}), in order to present the context of the proposed work.

\vspace{3pt}
\noindent \underline{\textit{Meta-Learning:}} These methods attempt to learn a meta-model over multiple learning episodes that can easily be finetuned to target tasks and thus generalize across tasks \cite{finn2017maml}. Please see \cite{hospedales2021metalearning-survey} for a detailed overview of various meta-learning approaches. %
\settingname has a different objective, and focuses on adapting off-the-shelf pre-trained models to new tasks without affecting the performance on the primary tasks the models are trained on.

\noindent \textit{\underline{Zero-shot/Few-shot learning (ZSL/FSL):}} These methods primarily focus on training a model to a new task with access to zero or few samples, respectively, for the task/categories of interest. For a detailed survey of such methods, please see \cite{song2022comprehensive-fslsurvery, bendre2020learning-zsl-survey}. 
\settingname has a fundamentally different objective of being able to finetune a base model with multiple tasks over time (which ZSL/FSL methods don't do), without losing the original model itself. %

\noindent \underline{\textit{Continual Learning (CL):}} These methods focus on adapting a model to new tasks while alleviating the problem of catastrophic forgetting through explicit regularization schemes, parameter isolation/masking methods, leveraging exemplar samples (either stored previously or generated) or 
introducing dynamic growing architectures. Please see \cite{shaheen2022continual-survey,qu2021recent-clsurvey} for a detailed survey of CL methods. On a first look, this seems close to our objective, but %
as shown in Fig \ref{fig:teaser} and Table \ref{tab:comparison}, a fundamental difference is that while CL methods attempt to learn a model that works together for all tasks so far (including learning the base model itself), \settingname operates in a different context, wherein our objective is to \textit{patch} a pre-trained off-the-shelf model without losing the model's original performance itself.



\noindent \underline{\textit{Transfer Learning (TL):}} These methods focus on performing well on a new task, most often by finetuning weights pre-trained on a source dataset. Please see~\cite{tl-survey,plested2022deep} for a detailed overview of TL methods. A key difference between \settingname and TL is that the latter do not attempt to maintain performance on the base task explicitly, while this is a key objective in \settingname. This allows \settingname to support multiple tasks with a single copy of the model while TL would require multiple model copies to generalize to multiple tasks. 
We nevertheless show how our method for \settingname can be used for TL in our experiments.

\noindent \underline{\textit{Model Editing/Debugging:}} In a different line of work that is however relevant for completeness of this discussion, ~\cite{sinitsin2020editable,de2021editing,mitchell2021fast,shibani2021editing,ribeiro2020beyond,ribeiro2022adaptive} have proposed methods to update a model on certain inputs without altering the performance on other inputs. This helps correct model predictions for certain inputs or to update the model to reflect changes in the real-world. 
While model editing operates by modifying model behavior at a sample level, \settingname operates at a task or dataset level.

\vspace{3pt}
\noindent \textbf{Related Methods.} We now discuss specific methods that are closely related to our approach in this work. %

\vspace{3pt}
\noindent \underline{\textit{Model Patching:}} Patching an existing model towards a different objective has emerged as a problem setting in recent years, primarily by Goel \etal in~\cite{goel2021model} and Ilharco \etal's PAINT in ~\cite{modelpatching2022}. The widespread availability of pre-trained models and the need to quickly adapt them to specific objectives in different application domains has motivated the need for such approaches.
\cite{goel2021model} focuses on making an existing model robust and invariant to subgroup differences using a two-stage approach. \cite{modelpatching2022} instead focuses on leveraging large-scale open vocabulary models to improve accuracy on specific tasks without degrading accuracy on tasks where performance is already adequate. Our work is more aligned with \cite{modelpatching2022}, although our approach is different as detailed in Sec \ref{sec_intro}. We however compare our performance with \cite{modelpatching2022} as a baseline in our experiments.

\noindent \underline{\textit{Use of Skip Connections:}}  The proposed \methodname are based on input-adaptive skip connections that help patch a given model to new tasks. We hence discuss other related methods that have studied the use of skip connections for different objectives. Since the introduction of skip connections \cite{resnets2015}, several works have proposed to use skip connections selectively. 
Rather than using skip connections only for specific layers or blocks, \cite{densenets2016} introduced DenseNets that uses all possible feedforward skip connections and replaced the fully-differentiable addition operation in ResNets with concatenation. 
\cite{sparsenet} introduced SparseNet, a sparse variant of DenseNets which drops skip connections from middle layers preserving only the farthest and nearest connections. 
\cite{wscisr} proposed weighted skip connections for super-resolution, where a separate self-attention mechanism is used to compute the weights. This was computationally feasible in their work, since the maximum number of skip connections in their  architecture was 15. None of these efforts however leverage skip connections to learn different tasks, which the proposed approach focuses on. %

\noindent \underline{\textit{Input-conditioned architectures:}}  A few methods \cite{SkipNet,batch-shaping,blockdrop,networks-with-stochastic-depth,pathnet} have been proposed in the recent past to reduce the cost of inference by choosing layers in an input-conditioned manner. Another work, SpotTune \cite{spottune}, determines which layers to fine-tune in an input-conditioned manner. 
While our method also adapts to each input, 
unlike the abovementioned methods which adapt at a layer or block level, we only focus on skip connections in this work. Besides, our problem setting of model patching is different from the objectives of all the above efforts.

The proposed \methodname leverage hypernetworks \cite{hypernetwork2016}, an approach of using one network to generate weights of another network, to generate weights for skip connections in an input-adaptive manner. We show in Sec \ref{sec_method} that such an approach also allows modulating the modifications to the base model based on available memory/compute resources (and hence the name \methodname). The work closest to ours is PAINT \cite{modelpatching2022} which patches large-scale open vocabulary models to support learning of new tasks. However, there are significant differences in our approach as well as our overall objective, as discussed in Sec \ref{sec_intro}. While PAINT focuses on using model interpolation, our approach of using input-adaptive skip connections without disturbing the base model has another advantage -- it allows us to easily unlearn earlier tasks by simply resetting the weights on the skip connections or overwriting them with the values of a new task. We now describe the proposed methodology.



 
\vspace{-6pt}
\section{\methodname for Efficient Model Patching}
\label{sec_method}
\vspace{-3pt}

Consider a model $\mathcal{M_S}$ trained for a base task $\mathcal{T_S}$ using samples from a dataset $\mathcal{D_S}$. Our primary goal is to \textit{patch} $\mathcal{M_S}$ for performing well on a new task $\mathcal{T_P}$ with samples from  $\mathcal{D_P}$ without any performance degradation on $\mathcal{T_S}$. No samples can be accessed from $\mathcal{D_S}$ nor any samples can be generated using $\mathcal{M_S}$. Let the features learned using $\mathcal{D_S}$ for the base/supporting task $\mathcal{T_S}$ be denoted as $\mathcal{F_S}$.  $\mathcal{F_S}^k$ then denotes a layer (or a block of layers) (for e.g. with convolutional, max-pooling operations and activation functions in a traditional CNN). $\mathcal{F_S}$, the final representation learned with samples from $\mathcal{D_S}$ can be viewed as a composition of features learned at each layer, and can be written as below for a given input samples $\mathbf{x}_s$ from the dataset $\mathcal{D_S}$.
\vspace{-4pt}
\begin{equation}
    \mathcal{F_S} = \mathcal{F_S}^k(\mathcal{F_S}^{k-1} (\dots (\mathcal{F_S}^1(\mathbf{x_s})))) 
    \label{eq:1}
    \vspace{-2pt}
\end{equation}
\noindent The final model $\mathcal{M_S}$ is formed by passing the features $\mathcal{F_S}$ into a classification layer/network, denoted using $\phi_{S}$ which typically is a fully-connected layer with neurons equal to the number of classes in $\mathcal{D_S}$:
\vspace{-4pt}
\begin{equation}
    \mathcal{M_S} = \mathcal{F_S} \circ \phi_{S}  
    \label{eq:2}
    \vspace{-4pt}
\end{equation}
\noindent When a new patching task $\mathcal{T_P}$ arrives with a corresponding dataset $\mathcal{D_P}$, the number of classes in the patching task is generally different from that of the original task. So, adding a new classification layer, $\phi_{P}$ is most often inevitable. Directly modifying $\mathcal{F_S}$ to perform better on $\mathcal{T_P}$ affects the performance on $\mathcal{T_S}$ which defeats our objective. We hence introduce an architectural modification to $\mathcal{M_S}$ in such a way that it helps address the new task while not affecting the performance on the original task. 

As mentioned previously, $\mathcal{F}^k$ denotes a sub-block of layers inside $\mathcal{M_S}$. Most modern-day architectures stack sub-blocks to build blocks which are again stacked together to build larger architectures. In a general-purpose CNN architecture such as ResNet \cite{resnets2015}, $\mathcal{F}^k$ is a residual block with one skip connection (we refer to this as a sub-block). %
The output representation for any $\mathcal{F}^j$ which represents a sub-block of layers with one skip connection can be denoted as follows:
\vspace{-3pt}
\begin{equation}
    \mathcal{F}^j = \mathcal{F}^j(\mathcal{F}^{j-1} (\dots (\mathcal{F}^1(\mathbf{x}))); \mathbf{W}_j) + \underbrace{{\textcolor{red}{\mathcal{F}^{j-1}(\mathbf{x}; {S_w})}}}_{\text{Skip connection}} 
    \label{eq:3}
    \vspace{-4pt}
\end{equation}
\noindent where $\mathbf{W_j}$ denotes the weights corresponding to the $j^{th}$ sub-block and the highlighted term in Eqn \ref{eq:3} refers to the representation learned by the previous sub-block, mapped using a weight matrix ${S_w}$. In a standard residual network, this highlighted component is obtained by setting ${S_w} = I$, where $I$ is the identity matrix. Generalizing the notion of skip connections using ${S_w}$ is a key component of the proposed \methodname methodology. 

While most architectures as well as earlier transfer learning/patching literature focus on updating $\mathbf{W}_j$, we instead turn our focus to the above generalization of skip connections to leverage the features learned on the base/original task. Firstly, we consider all possible skip connections between between each sub-block in a block as possible routes for a given input. (Note that we do not introduce skip connections inside layers of a sub-block to keep the overhead minimal.) Figure \ref{fig:method-figure} shows an illustration.
One could view this as similar to a sparse variant of DenseNets, however with a difference -- each connection has a learnable weight matrix ${S_w}$ in our case, as described later in this section. The output of a block in Eqn \ref{eq:3} can hence be written as: 
\vspace{-6pt}
\begin{equation}
    \mathcal{F}^j = \mathcal{F}^j(\mathcal{F}^{j-1} (\dots (\mathcal{F}^1(\mathbf{x}))); \mathbf{W}_j) + {\textcolor{red}{\sum_{i=1}^{j-1}\mathcal{F}^{i}(\mathbf{x}; {S_w})}} 
    \label{eq:4}
    \vspace{-4pt}
\end{equation}
\noindent where ${S_w}$ is learnable and specifies how two sub-blocks interact. Introducing this architectural modification helps in addressing the objectives of \settingname stated in Table \ref{tab:comparison}. Since this modification does not involve any changes to existing model weights, off-the-shelf pre-trained models can be easily leveraged. Besides, not making any changes to the base model ensures that the performance on the base task is retained without any drop (by simply dropping the additional skip connections). Through this generalization, our objective can be viewed as developing a mechanism that allows us to control interactions among pre-trained features at various granularities (for e.g, early vs late layers) so that it enables effective patching for new tasks.
\begin{figure}
 \centering \includegraphics[width=0.7\linewidth]{iccv2023AuthorKit/Method_Diag.pdf}
  \vspace{-8pt}
  \caption{Illustration of a block in \methodname} 
  \label{fig:method-figure}
  \vspace{-0.5cm}
\end{figure}

\vspace{3pt}
\noindent \textbf{The \smethodname.}
When learning ${S_w}$s for skip connections on top of a pre-trained model for a target task, the same weights that are optimal for one input might be sub-optimal for another input, even from the same dataset or class label. Keeping this in mind, we propose the use of a \smethodname which is a separate, small and lightweight network parameterized by weights $\theta$ that learns the weights of the additional skip connections independent of the base model.
Thus, for a given input $\mathbf{x}$, the added skip connections are weighted in an input-conditioned manner using a \smethodname. Following Eqn \ref{eq:4}, the output of a sub-block $\mathcal{F}_j$ now changed as follows when a \smethodname (denoted using $\Delta$ below) is introduced: 
\vspace{-4pt}
\begin{equation*}
     \textcolor{red}{\Lambda_{i} = \Delta(\mathbf{x}; \theta)}
    \vspace{-9pt}
 \end{equation*}
 \begin{equation*}
    \mathcal{F}^j (\textbf{x})= \mathcal{F}^j(\mathcal{F}^{j-1} (\dots (\mathcal{F}^1(\mathbf{x}))); \mathbf{W}_j) + {\sum_{i=1}^{j-1}\mathcal{F}^{i}(\mathbf{x}; \textcolor{red}{\Lambda_i})} 
 \vspace{-3pt}
 \end{equation*}

\noindent Intuitively, we first pass the input to the \smethodname to obtain weighting coefficients $\Lambda_{{1, \dots (j-1)}}$ for each of the new skip connections in our $\Delta$-network. 
Since the \smethodname is a parametrized neural network and therefore fully differentiable, it can be trained directly on the new task loss via gradient descent.
Unlike standard skip connections with $\Lambda_i = I$, adaptive skip connections have input-conditioned weights on them instead. 
Since the skip connections are introduced after training, our model can be directly used with existing models that are pre-trained for a different task. (For models that already have skip connections -- for e.g, DenseNets, the $\Lambda_i$s for those pre-existing skip connections are defaulted to the identity matrix when addressing the base task.) 
An end-to-end summary of our approach is provided in Algorithm \ref{alg:end-to-end}.

Conditioning skip connections using an external fully-differentiable network satisfies the end-to-end learning requirement of \settingname (as stated in Table \ref{tab:comparison}), and also helps finetune the model easily on a new patching task. When new tasks arrive in a sequence, we introduce a new \smethodname and a classification layer for each task. These \smethodnames are intended to be lightweight (2-layer feedforward networks in our implementation), and hence can be stored easily for each task arriving over time. %
As each task has its own classifier and \methodname, there is no loss of performance on any of the preceding tasks and the model can support as many tasks as required. %

\vspace{-5pt}
\begin{algorithm}[]
\footnotesize
\SetAlgoLined
\textbf{Inputs and Initialization:} \\ 
\begin{itemize}[noitemsep,leftmargin=*]
    \item Model trained on supporting task $\mathcal{M_S} = \mathcal{F_S} \circ \phi_{S}$, $\mathcal{F_S} \rightarrow \mathcal{F_S}^{1 \dots k}$ with $k$ blocks; \item Randomly initialized classification head $\phi_{P}$, and \methodname, $\Delta^{1 \dots k}$
    \item Samples from patching task $\mathbf{x_d}^{m=1 \dots n} \in \mathcal{D_P}$ in $n$ batches, Loss function used for the task $\mathcal{L}$ 
    \item Add $\phi_T$ to $\mathcal{F_S}$; Introduce all possible skip connections ($p$) in each sub-block of $\mathcal{F_S}^{1 \dots k}$
\end{itemize}
\textbf{Training:} \\ 
\For{$num\_epochs$}{
    \For{$\mathbf{x_d}^{m=1 \dots n}$}{
        \For{$j = 1 \dots k$}{
            $\mathbf{\lambda}_{1 \dots p} = \Delta^j(\mathbf{x_d^m}; \theta_j)$ \\
            ${\displaystyle \mathcal{F_S}^j = \mathcal{F_S}^j(\mathbf{x_d^m}; \mathbf{W_j}) + \sum_{i=1}^{j-1} \mathcal{F_S}^i(\mathbf{x_d^m}; \lambda_i) }$ \\
        }
        $\mathcal{M_P} =  (\mathcal{F_S}, \Delta^{1 \dots k}) \circ \phi_{T}$ \\ 
        Calculate $\mathcal{L}$  \\ 
        Update $\Delta^{k}$, $\phi_T$  \\
    }
}
\textbf{Output:} $ \mathcal{F_S} \circ \phi_{T} \rightarrow$ model patched for task $\mathcal{T_P}$  \\
\caption{\methodname for Model Patching}
\label{alg:end-to-end}
\end{algorithm}
\vspace{-15pt}

\vspace{2pt}
\noindent \textbf{Adapting to Memory/Compute Requirements.} When using Algorithm \ref{alg:end-to-end},  the memory and compute requirements can be controlled using %
the number of \smethodnames used for a task under consideration. When there are memory/compute constraints, one can choose to use a small \smethodname for learning the weights of skip connections of a few blocks alone. $\Delta$ hence becomes a unit quantum of additional overhead, lending the name \methodname to this approach. With more memory, a set of \smethodnames can be used to learn different skip connections on the base model. The proposed algorithm thus allows customizing \settingname in a memory-sensitive manner. Increase in number of \smethodnames increases the number of trainable parameters, and hence results in performance gain in general. We provide some design choices and thumb rules of connecting \smethodname to subsets of skip connections towards the end of this section. 




\vspace{2pt}
\noindent \textbf{Addressing Variance Introduced by Skip Connections.} 
Weight initialization methods like He's initialization \cite{heinit} are quite commonly used in all CNN architectures to maintain the variance between input and output of every block or layer. 
However, when skip connections are present in an architecture, such initialization methods \cite{heinit} result in increased variance as shown in \cite{fixupinit}.
To understand this problem better, we look at  the output of a given $j$th block, $\mathcal{F}^j$, with one skip connection.
The output representation is then given by: $\mathcal{F}^j = \mathcal{F}^j(\mathcal{F}^{j-1}(\mathbf{x})) + \mathcal{F}^{j-1}(\mathbf{x})$. The variance of the output representation can hence be written as: 
\vspace{-6pt}
\begin{equation}
\begin{aligned}
    \operatorname{Var}\Big[\mathcal{F}^j\Big] & = \operatorname{Var}\Big[\mathcal{F}^j(\mathcal{F}^{j-1}(\mathbf{x}) + \mathcal{F}^{j-1}(\mathbf{x})\Big] \\ 
    &= \operatorname{Var}\Big[\mathcal{F}^j(\mathcal{F}^{j-1}(\mathbf{x}))\Big] +  \operatorname{Var}\Big[\mathcal{F}^{j-1}(\mathbf{x}))\Big] \\ & \quad \quad + \operatorname{Cov}\Big[\mathcal{F}^j(\mathcal{F}^{j-1}(\mathbf{x})), \mathcal{F}^{j-1}(\mathbf{x}))\Big] \\
    \label{eq:6}
\end{aligned}
\end{equation}

\vspace{-0.7cm}
\noindent where the first two terms refer to variance of input and output blocks respectively, and the third term denotes the covariance between  input and output. While initialization methods maintain the variance between input and output of a block \cite{heinit},
when skip connections are present, the overall variance of the block output is approximately two times the variance of the input (we formally show in the Appendix): 
\vspace{-6pt}
\begin{equation}\label{eq:7}
    \operatorname{Var}\Big[\mathcal{F}^j\Big] \approx 2\operatorname{Var} \Big[\mathcal{F}^{j-1}(\mathbf{x})\Big] 
    \vspace{-5pt}
\end{equation}

\noindent The variance thus doubles at each block and exponentially increases with increasing number of blocks. 
A general solution to address this problem is to introduce batch normalization (BN) layers inside the block \cite{batch-norm}. %
When all possible skip connections are introduced in \methodname, the variance can thus increase rapidly since we add features from all preceding blocks. %
We address this issue using BN layers in each \smethodname in our implementation. %

\begin{table*}[]
\centering
\captionsetup{font=footnotesize}
\footnotesize
\resizebox{0.9\linewidth}{!}{\begin{tabular}{>{\kern-\tabcolsep}l|c|c|c|c|c|c|c|c<{\kern-\tabcolsep}}
 \toprule
\textbf{Architecture}           & \textbf{Method} & \multicolumn{1}{l|}{\textbf{\# Parameters}} & \multicolumn{1}{l|}{\textbf{RESISC-45}} & \multicolumn{1}{l|}{\textbf{STL-10}} & \multicolumn{1}{l|}{\textbf{GTSRB}} & \multicolumn{1}{l|}{\textbf{Aircraft}} & \multicolumn{1}{l|}{\textbf{Flowers}} & \multicolumn{1}{l}{\textbf{Cars}} \\ \midrule
\multirow{2}{*}{ResNet-18}      & PAINT           & 11.2M                                       & 24.38                                   & 55.41                                & 18.39                               & 3.51                                   & 12.70                                 & 2.01                              \\ \cmidrule(l){2-9} 
                               & \cellcolor{highlight}\methodname (1)    &\cellcolor{highlight} 0.4M                                        & \cellcolor{highlight}\textbf{74.48}                          &\cellcolor{highlight} \textbf{79.55}                       & \cellcolor{highlight}\textbf{70.22}                      & \cellcolor{highlight}\textbf{46.56}                         & \cellcolor{highlight}\textbf{75.65}                        & \cellcolor{highlight}\textbf{46.78}                    \\ \midrule
\multirow{2}{*}{ResNet-34}      & PAINT           & 21.8M                                       & 21.91                                   & 62.64                                & 19.87                               & 2.94                                   & 16.16                                 & 4.39                              \\ \cmidrule(l){2-9} 
                                &\cellcolor{highlight}\methodname (1)    &\cellcolor{highlight} 0.4M                                        & \cellcolor{highlight}\textbf{78.02}                          & \cellcolor{highlight}\textbf{83.41}                       & \cellcolor{highlight}\textbf{70.83}                      & \cellcolor{highlight}\textbf{45.74}                         & \cellcolor{highlight}\textbf{78.31}                        & \cellcolor{highlight}\textbf{47.31}                    \\ \midrule
\multirow{2}{*}{ResNet-50}      & PAINT           & 23.5M                                       & 48.04                                   & 86.30                                & 28.11                               & 7.33                                   & 56.97                                 & 12.54                             \\ \cmidrule(l){2-9} 
                                &\cellcolor{highlight}\methodname (1)    & \cellcolor{highlight}7.1M                                        & \cellcolor{highlight}\textbf{84.02}                          &\cellcolor{highlight} \textbf{87.53}                       & \cellcolor{highlight}\textbf{77.79}                      & \cellcolor{highlight}\textbf{55.81}                         & \cellcolor{highlight}\textbf{82.64}                        &\cellcolor{highlight} \textbf{59.31}                    \\ \midrule
\multirow{2}{*}{ResNet-101}     & PAINT           & 42.5M                                       & 41.25                                   & 64.51                                & 18.14                               & 4.89                                   & 58.13                                 & 10.74                             \\ \cmidrule(l){2-9} 
                                &\cellcolor{highlight}\methodname (1)    & \cellcolor{highlight}7.1M                                        &\cellcolor{highlight} \textbf{83.47}                          &\cellcolor{highlight} \textbf{88.80}                       &\cellcolor{highlight} \textbf{77.38}                      & \cellcolor{highlight}\textbf{53.00}                         &\cellcolor{highlight} \textbf{82.79}                        &\cellcolor{highlight} \textbf{56.48}                    \\ \midrule
\multirow{2}{*}{ConvNeXt-Tiny}  & PAINT           & 27.8M                                       & 34.81                                   & 84.57                                & 12.36                               & 6.18                                   & 69.06                                 & 6.87                              \\ \cmidrule(l){2-9} 
                                &\cellcolor{highlight} \methodname (1)    &\cellcolor{highlight} 4.0M                                        & \cellcolor{highlight}\textbf{86.54}                          & \cellcolor{highlight}\textbf{89.69}                       & \cellcolor{highlight}\textbf{82.47}                      & \cellcolor{highlight}\textbf{69.59}                         &\cellcolor{highlight} \textbf{87.49}                        & \cellcolor{highlight}\textbf{73.00}                    \\ \midrule
\multirow{2}{*}{ConvNeXt-Small} & PAINT           & 49.5M                                       & 39.57                                   & 81.47                                & 25.33                               & 7.99                                   & 69.12                                 & 8.59                              \\ \cmidrule(l){2-9} 
                                & \cellcolor{highlight}\methodname (1)    &\cellcolor{highlight} 4.0M                                        & \cellcolor{highlight}\textbf{86.11}                          & \cellcolor{highlight}\textbf{90.27}                       &\cellcolor{highlight} \textbf{81.81}                      & \cellcolor{highlight}\textbf{65.81}                         &\cellcolor{highlight} \textbf{87.25}                        &\cellcolor{highlight} \textbf{69.93}                    \\ \midrule
\multirow{2}{*}{ConvNeXt-Base}  & PAINT           & 87.6M                                       & 40.34                                   & 79.61                                & 19.85                               & 6.52                                   & 79.00                                 & 12.67                             \\ \cmidrule(l){2-9} 
                                & \cellcolor{highlight}\methodname (1)    & \cellcolor{highlight}7.1M                                        & \cellcolor{highlight}\textbf{86.71}                          &\cellcolor{highlight} \textbf{90.96}                       &\cellcolor{highlight} \textbf{82.30}                      &\cellcolor{highlight} \textbf{65.21}                         &\cellcolor{highlight} \textbf{87.01}                        & \cellcolor{highlight}\textbf{71.75}                    \\ \midrule
\multirow{2}{*}{ConvNeXt-Large} & PAINT           & 196.2M                                      & 36.06                                   & 80.72                                & 17.71                               & 14.19                                  & 77.27                                 & 10.80                             \\ \cmidrule(l){2-9} 
                                & \cellcolor{highlight}\methodname (1)    & \cellcolor{highlight}15.9M                                       & \cellcolor{highlight}\textbf{87.31}                          & \cellcolor{highlight}\textbf{91.18}                       & \cellcolor{highlight}\textbf{81.88}                      & \cellcolor{highlight}\textbf{65.14}                         &\cellcolor{highlight} \textbf{87.08}                        & \cellcolor{highlight}\textbf{71.93}                    \\ \bottomrule 
\end{tabular}}
\vspace{-4pt}
\caption{Harmonic mean of top-1 accuracies on ImageNet and the patched task obtained using the final patched model. \methodname (1) denotes the use of one single \smethodname. Parameters exclude the parameters included in FC layer since this is a common denominator for both methods. \textbf{\methodname significantly outperform PAINT with lesser number of trainable parameters.}}
\vspace{-11pt}
\label{tab:patching-one-task}
\end{table*}



\vspace{2pt}
\noindent \textbf{Implementation and Design Details.}
Any neural network model developed by stacking sub-blocks of layers can be segregated into a set of blocks, denoted as $\mathcal{F}^j$ earlier in this section, with each block containing varying number of sub-blocks. For e.g., ResNet architectures are often divided into four blocks.
If a block contains $k$ sub-blocks, we introduce a total $k(k+1)/2$ skip connections in the block, which is controlled by one \smethodname. %
One can use additional \smethodnames to weight skip connections in other blocks. We denote \methodname $(n)$ to denote a $\Delta$-network with $n$ \smethodnames.
Considering that we only need to learn the weights of the skip connections (and not all parameters of the network), each of our \smethodname consists of a simple two-layer CNN with ReLU activations, normalization layers, and a fully-connected layer to determine the weights for skip connections. The output of a \smethodname is constrained between $[0,1]$ using a sigmoid activation. %



\vspace{-6pt}
\section{Experiments and Results}
\vspace{-6pt}
We empirically study the effectiveness of the proposed methodology in this section. We begin with compare our approach \settingname with PAINT\cite{modelpatching2022} on the task of image-classification on different patching settings. 
We then present results on other vision tasks including object detection and segmentation, as well as other problem settings including transfer learning and zero-shot domain adaptation.

\vspace{-2pt}
\subsection{\settingname for Image Classification} \label{sec:patching}
\vspace{-4pt}
Following \cite{modelpatching2022}, we present results on three different patching settings: (1) \textit{Patching on a single task}; (2) \textit{Joint Patching} -- Patching on a pre-defined set of tasks; (3) \textit{Sequential Patching} -- Patching on sequentially arriving tasks. Since PAINT \cite{modelpatching2022} focused only on open-vocabulary models in their work, We use their official code repository and run their method on all the models we consider in this work. For \textit{Single-task patching} and \textit{Joint Patching}, we use the harmonic mean of top-1 accuracies on the pre-trained task and on the new finetuned task, since we are keen on strong performance on both these tasks. (We also present the individual top-1 accuracies in the appendix for completeness.) %
We use the ImageNet pre-trained CNN models provided by the official PyTorch \cite{paszke2019pytorch} and PyTorch Image Models \cite{rw2019timm} libraries. In all of these experiments, for both \methodname and PAINT, we add a fully connected (FC) layer at the end for the new task, as in \cite{modelpatching2022}. As PAINT requires fine-tuning the entire set of pre-trained weights, for all the experiments where we compare with PAINT, we finetune the pre-trained model weights for 50 epochs using the same procedure and data splits as used in ~\cite{modelpatching2022} for fairness of comparison.  

\vspace{2pt}
\noindent \textbf{Patching on One Task:}
Table \ref{tab:patching-one-task} shows the results of our experiments on this setting. %
We consider six datasets and seven architectural variations for this experiment. For rows pertaining to PAINT, we pick the model interpolation coefficient that gives the best harmonic mean metric mentioned earlier. %
As shown in the table, \methodname significantly outperform PAINT by large margins and use relatively significantly lower number of trainable parameters too. 

When using \methodname in this setting, %
one \smethodname is introduced in the last block of the base architecture with corresponding skip connections therein. As described in Sec \ref{sec_method}, the output of the preceding block is passed to the \smethodname which determines the weights of all the skip connections introduced in the current block. The output from the preceding block is then routed based on the skip connection weights. %
As we only finetune the fully-connected layer and the small \smethodname, the number of trainable parameters are very less compared to full finetuning approaches like PAINT. Increasing the number of \smethodnames by introducing them in the first, second and third blocks along with the last block increases the performance even more (please see Appendix for these results). 

\vspace{2pt}
\noindent \textbf{Joint Patching:}
In this setting, all the datasets for tasks that need to learned are available together. Following \cite{modelpatching2022}, we concatenate the training and test sets of all these tasks respectively before we start the finetuning. The introduced fully connected layer contains neurons equal to the sum of all classes present across all tasks. A single \smethodname is still learned for all tasks combined. Table \ref{tab:joint-patching} shows the results. Our method with just a single \smethodname outperforms PAINT in all the task and architecture variants considered. 

\begin{table}[]
\captionsetup{font=footnotesize}
\footnotesize
\resizebox{\linewidth}{!}{\begin{tabular}{>{\kern-\tabcolsep}l|l|c|c|c<{\kern-\tabcolsep}}
\toprule
\textbf{Architecture} & \textbf{Method}    & \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}} Aircraft\\Flowers\\STL10\end{tabular}}} & \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}} GTSRB\\Cars\\Flowers\end{tabular}}} & \multicolumn{1}{l}{\textbf{\begin{tabular}[c]{@{}l@{}} STL10\\Cars\\GTSRB\end{tabular}}} \\ \midrule
ResNet-18             & PAINT              & 22.33                                                                                                        & 4.35                                                                                                     & 0.66                                                                                                  \\
                      & \cellcolor{highlight}\methodname (1) & \cellcolor{highlight}\textbf{68.46}                                                                                                        & \cellcolor{highlight}\textbf{61.41}                                                                                                    & \cellcolor{highlight}\textbf{66.91}                                                                                                 \\ \midrule
ResNet-34             & PAINT              & 13.75                                                                                                        & 6.75                                                                                                     & 15.20                                                                                                 \\
                      & \cellcolor{highlight}\methodname (1) & \cellcolor{highlight}\textbf{71.25}                                                                                                        & \cellcolor{highlight}\textbf{64.51}                                                                                                    &\cellcolor{highlight} \textbf{70.31}                                                                                                 \\ \midrule
ResNet-50             & PAINT              & 55.65                                                                                                        & 4.70                                                                                                     & 35.16                                                                                                 \\
                      & \cellcolor{highlight}\methodname (1) &\cellcolor{highlight} \textbf{79.80}                                                                                                        &\cellcolor{highlight} \textbf{71.96}                                                                                                    & \cellcolor{highlight}\textbf{76.56}                                                                                                 \\ \midrule
ConvNeXt-TINY         & PAINT              & 34.72                                                                                                        & 1.61                                                                                                     & 6.70                                                                                                  \\
                      &\cellcolor{highlight}\methodname (1) & \cellcolor{highlight}\textbf{81.12}                                                                                                        & \cellcolor{highlight}\textbf{77.45}                                                                                                    & \cellcolor{highlight}\textbf{81.02}                                                                                                 \\ \bottomrule
\end{tabular}}
\vspace{-4pt}
\caption{Joint Patching: Harmonic Mean of Top-1 accuracies on ImageNet and the jointly patched tasks. \smethodname denotes the use of one single \smethodname, as before.} 
\vspace{-11pt}
\label{tab:joint-patching}
\end{table}



\vspace{2pt}
\noindent \textbf{Sequential Patching:}
In this setting, the tasks that need to be patched arrive in a sequential manner. In case of PAINT, following \cite{modelpatching2022}, we start with the pre-trained model, introduce a new FC layer and perform patching on the first task by finetuning the backbone along with the newly introduced FC layer. For finding the best interpolation coefficient, we use the harmonic mean of top-1 accuracies on the pre-trained and patched tasks. The interpolated weights are then used for the subsequent task, and we again determine the best interpolation coefficient using the harmonic mean of accuracies on all previous tasks. %
During evaluation, we use the task id to determine which FC layer to use. 
For \methodname, we introduce one \smethodname for each new task along with the new FC layer. The \smethodnames are trained along with the FC layer for each task. Table \ref{tab:seq-patching} shows the average accuracy of the final patched model on all the previous tasks (note that we use average accuracy instead of the harmonic mean here, since there are multiple tasks to simultaneously evaluate at the end of this setting). 
Since PAINT does not retain the base model and the interpolated models are used for later tasks, its performance can deteriorate on earlier tasks, while \methodname has a zero drop in performance on  previously learned tasks by design. We present a more detailed analysis of these results in the Appendix.

\begin{table}[]
\captionsetup{font=footnotesize}
\footnotesize
\resizebox{\linewidth}{!}{\begin{tabular}{>{\kern-\tabcolsep}l|l|c|c|c<{\kern-\tabcolsep}}
\toprule
\textbf{Architecture} & \textbf{Method}    & \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}} Aircraft\\Flowers\\STL10\end{tabular}}} & \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}} GTSRB\\Cars\\Flowers\end{tabular}}} & \multicolumn{1}{l}{\textbf{\begin{tabular}[c]{@{}l@{}} STL10\\Cars\\GTSRB\end{tabular}}} \\ \midrule
ResNet-18             & PAINT              & 20.02                                                                                                      &14.28                                                                                                  & 26.85                                                                                                 \\
                      &\cellcolor{highlight}\methodname (1) & \cellcolor{highlight}\textbf{70.52}                                                                                                       & \cellcolor{highlight}\textbf{64.95}                                                                                                  & \cellcolor{highlight}\textbf{67.51}                                                                                            \\ \midrule
ConvNeXt-TINY         & PAINT              & 19.95                                                                                                        & 28.68                                                                                                  & 27.01                                                                                                 \\
                      & \cellcolor{highlight}\methodname (1) &\cellcolor{highlight} \textbf{83.52}                                                                                                        &\cellcolor{highlight} \textbf{64.72}                                                                                                 &\cellcolor{highlight} \textbf{65.74}                                                                                             \\ \bottomrule
\end{tabular}}
\vspace{-4pt}
\caption{Sequential Patching: Mean of Top-1 accuracy on ImageNet and the Top1 accuracies on all the three patched tasks. \methodname (1) denotes the use of one single \smethodname, as before.} 
\vspace{-11pt}
\label{tab:seq-patching}
\end{table}


\vspace{-3pt}
\subsection{What makes \methodname work? Analysis and Ablation Studies}
\vspace{-3pt}
We herein study the importance of the components of \methodname -- in particular, the use of skip connections, both learnable and input-conditioned, as well as the relevance of batch normalization layers. %
We consider four variants of ResNets, including additional architectures beyond what we studied so far (for analysis purposes), and train them in multiple setting on two datasets namely, FGVC-Aircraft \cite{fgvc-aircraft} and Stanford-Cars \cite{stanford-cars}. %
\begin{table}[]
\captionsetup{font=footnotesize}
\resizebox{\linewidth}{!}{\begin{tabular}{>{\kern-\tabcolsep}l|cc<{\kern-\tabcolsep}}
\toprule
\textbf{Architecture}                       & \textbf{FGVC Aircraft}  & \textbf{Stanford Cars}  \\ \midrule
\rowcolor{baseline}ResNet-18 + FC (R-18)      & 59.89          & 67.63          \\
+ Learnable Skip connections       & 59.26          & 51.71          \\ \cmidrule(r){1-1}
R-18 + All Skips (Learnable)       & 63.70           & 59.85          \\
+ BN                               & 70.69          & \textbf{77.63} \\ \cmidrule(r){1-1}
R-18 + All Skips                   & 55.18          & 61.43          \\
+ BN                               & \underline{71.12}          & 75.43          \\
+ Random                           & 21.33          & 26.38          \\
\rowcolor{highlight}+ \methodname (4)                       & \textbf{72.91} & \underline{77.21}          \\ \midrule
\rowcolor{baseline}ResNet-34 + FC (R-34)       & 59.83          & 68.9           \\
+ Learnable Skip connections       & 61.48          & 65.26          \\ \cmidrule(r){1-1}
R-34 + All Skips (Learnable)       & 41.52          & 27.91          \\
+ BN                               & 64.63          & \underline{79.31}          \\ \cmidrule(r){1-1}
R34 + All Skips                    & 47.34          & 44.45          \\
+ BN                               & \underline{67.78}          & 66.19          \\
+ Random                           & 9.31           & 7.16           \\
\rowcolor{highlight}+ \methodname (4)                      & \textbf{75.19} & \textbf{81.51} \\ \midrule
\rowcolor{baseline}ResNeXt-50 + FC (RX-50)     & 61.06          & 69.39          \\
 + Learnable Skip connections        & 58.87          & 69.23          \\ \cmidrule(r){1-1}
RX-50 + All Skips (Learnable)        & 47.53          & 72.49          \\
+ BN                               & 50.85          & \underline{77.04}          \\ \cmidrule(r){1-1}
RX-50 + All Skips                   & 52.72          & 50.14          \\
+ BN                               & \underline{72.82}          & 75.56          \\
+ Random                           & 20.94          & 21.81          \\
\rowcolor{highlight}+\methodname (4)                       & \textbf{78.04} & \textbf{85.25} \\ \midrule
\rowcolor{baseline} WideResNet-50 + FC (WRN-50)  & 55.96          & 64.89          \\
+ Learnable Skip connections     & 56.55          & 64.45          \\ \cmidrule(r){1-1}
WRN-50 + All Skips (Learnable)     & 30.92          & 60.47          \\
+ BN                               & 53.51          & \underline{76.40}           \\ \cmidrule(r){1-1}
WRN-50 + All Skips               & 51.28          & 48.97          \\
+ BN                               & \underline{72.52}          & 75.15          \\
+ Random                           & 14.37          & 12.05          \\
\rowcolor{highlight}+ \methodname (4)                       & \textbf{76.18} & \textbf{83.08} \\ \bottomrule
\end{tabular}}
\vspace{-4pt}
\caption{Quantitative ablation analysis of marginal contribution of each component in \methodname. \methodname (4) denotes the use of four \smethodnames, as before.}
\vspace{-17pt}
\label{tab:ablation}
\end{table}

\noindent \textbf{Role of Skip Connections and Input-Conditioning.} In order to study this, we introduce four kinds of skip connections in the standard ResNets. (1) \textit{Learnable Skip Connections:} We make the already existing skip connections in residual networks soft by introducing a weight parameter, i.e. when adding an input to the output block, instead of using a simple identity mapping, we use a scaled identity mapping (multiply the input using this learnable weight parameter). (2) \textit{All Skips:} We introduce all possible skip connections inside each ResNet module. (3) \textit{All Skips (Learnable):}  All Skip connections with learnable weights \ie after all possible skip connections are introduced, we  assign a separate weight to all of them and learn the weight using standard backpropagation. (4) \textit{Skip Connections in \methodname:} When a \smethodname is used, we introduce all possible skip connections and weight those skip connections in an input-conditioned manner \ie the weight of the skip connection is determined by the \smethodname. In (1) and (3), the skip connections weights are not input-conditioned; once learned on the training set, they stay fixed during inference irrespective of the input. 
Table \ref{tab:ablation} shows the results of this study on patching. Evidently, \methodname outperform other strategies in the results. %
Introducing all possible skip connections without input-conditioning does not help much. While learnable skip connections seems to help marginally, the improvement is low compared to \methodname-based skip connections which are input-conditioned. 

\noindent \textbf{Role of Batch Normalization (BN).} We explicitly study the role of BN layers since introducing all possible skip connections can result in an increased variance. We introduce a BN layer in our approach to curtail this.
Table \ref{tab:ablation} presents related results; as seen in the table, adding BN layers improves performance in general even for other approaches when skip connections are added, demonstrating their usefulness in  approaches that modify or adapt skip connections. %


\vspace{-5pt}
\section{\settingname for Other Tasks and Settings}
\vspace{-4pt}
We also study \settingname on other settings such as zero-shot domain adaptation and transfer learning, as well as other vision tasks such as object detection and segmentation. While earlier patching efforts did not study such settings, our objective is to show that \methodname can also be used to adapt pre-trained models on various tasks and settings. We hence do not seek state-of-the-art performance in these results, but show how adding \smethodnames to an existing pre-trained model helps finetune the model without losing performance on the base task.






\noindent \underline{\textit{Zero-shot Domain Adaptation (ZSDA):}} 
In ZSDA, the models are trained on a given source dataset and evaluated on their ability to transfer to a different target dataset that has the same classes but is from a different data distribution.
We follow the same experimental setting as \cite{uniformprior}, and evaluate \methodname on a ResNet-18 architecture. We compare against the Uniform Prior approach \cite{uniformprior} as well as and Adversarial Discriminative Domain Adaptation \cite{adda}, as well as study a combination of our approach with theirs since they can co-exist. Table \ref{tab:zsda} shows the results.
We continue to observe that by adding \methodname, we can significantly improve the network's performance on the target dataset. Importantly, the results also show that \methodname can be a general strategy to use along with existing techniques.


\begin{table}[]
\captionsetup{font=footnotesize}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{>{\kern-\tabcolsep}l|ccc<{\kern-\tabcolsep}}
\toprule
\textbf{ResNet-18}                        & \textbf{MNIST $\rightarrow$USPS} & \textbf{USPS $\rightarrow$MNIST} & \textbf{SVHN$\rightarrow$MNIST} \\ \midrule
\rowcolor{baseline} Vanilla                       & 49.0                   & 42.81                   & 69.7                  \\
 + \methodname (4)                & 56.5                 & 47.0                   & 75.3                  \\
 + Uniform prior \cite{uniformprior}       & 67.2                  & 56.2                   & 71.3                  \\

\rowcolor{highlight} + Uniform prior + \methodname (4) & \textbf{71.3}           & \textbf{61.0 }           & \textbf{77.9}          \\ \midrule
\rowcolor{baseline} ADDA \cite{adda}                         & 88.2                  & 89.0                    & 73.4                  \\
  + \methodname(4)                   & 90.1                    & 90.7                   & 81.1                  \\
 + Uniform Prior          & 91.6                    & 92.7                   & 79.4                \\
\rowcolor{highlight} + Uniform Prior + \methodname(4)    & \textbf{92.4 }           & \textbf{94.1}           & \textbf{83.6 }          \\ \midrule
Target only                   & 98.1                    & 99.8                    & 99.8                 \\ \bottomrule
\end{tabular}
}
\vspace{-2pt}
\caption{\textbf{\methodname improves performance of existing ZSDA methods.} Comparison of introducing \methodname for Zero-Shot domain adaptation using a base ResNet-18 network. \methodname (4) denotes the use of four \smethodnames, as before.}
\vspace{-9pt}
\label{tab:zsda}
\end{table}


\noindent \underline{\textit{Transfer Learning (TL):}} We study \methodname in a traditional TL setting and compare our methodology with six different transfer learning methods, including state-of-the-art approaches: LWF~\cite{LWF}, BSS~\cite{BSS}, DELTA~\cite{li2018delta}, StochNorm~\cite{stochnorm}, Co-Tuning~\cite{CoTuning}, and Bi-Tuning~\cite{bituning}. We also compare with a vanilla baseline, where the backbone architecture is finetuned end-to-end on the target dataset, and a combination of our method with~\cite{bituning}. We use the standard TL benchmark and training protocols provided in ~\cite{dalib}, comprising FGVC-Aircraft~\cite{fgvc-aircraft} and Stanford-Cars~\cite{stanford-cars} datasets, with ResNet-50 as the backbone, following ~\cite{stochnorm,CoTuning,bituning}. %
To test the effectiveness of our approach in the low-data regime, we train the model at different levels, i.e. on 15\%, 30\%, 50\% and 100\% of the training data, for the above experiments. Table ~\ref{tab:compare-tl-approaches} presents our results.%
\methodname outperforms existing approaches on almost all experiments consistently, even with varying amounts of training data in the target domain. Bi-Tuning ~\cite{bituning} uses contrastive learning and leverages both supervised and unsupervised pre-trained representations, which perhaps makes it stronger compared to other baselines. \methodname instead uses a simple strategy to outperform all baselines including Bi-Tuning on most settings.

\begin{table}
\captionsetup{font=footnotesize}
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{>{\kern-\tabcolsep}l|ccccr|ccccr<{\kern-\tabcolsep}}
\toprule
\textbf{Method}             & \multicolumn{5}{c|}{\textbf{FGVC Aircraft}}                                                       & \multicolumn{5}{c}{\textbf{Stanford Cars}}                                                       \\ \midrule
                   & 15\%          & 30\%          & 50\%          & 100\%         & \multicolumn{1}{l|}{Avg} & 15\%          & 30\%          & 50\%          & 100\%         & \multicolumn{1}{l}{Avg} \\ \midrule
\rowcolor{baseline} Vanilla Baseline          & 41.6          & 57.8          & 68.7          & 80.2          & 62.1                     & 41.1          & 65.9          & 78.4          & 87.8          & 68.3                    \\
+ LWF              & 44.1          & 60.6          & 68.7          & 82.4          & 64.0                     & 44.9          & 67.0          & 77.6          & 87.5          & 69.3                    \\
+ BSS              & 43.6          & 59.5          & 69.6          & 81.2          & 63.5                     & 43.3          & 67.6          & 79.6          & 88.0          & 69.6                    \\
+ DELTA            & 44.4          & 61.9          & 71.4          & 82.7          & 65.1                     & 45.0          & 68.4          & 79.6          & 88.4          & 70.4                    \\
+ Co-Tuning        & 45.9          & 61.2          & 71.3          & 82.2          & 65.2                     & 49.0          & 70.6          & 81.9          & 89.1          & 72.7                    \\
+ StochNorm        & 44.3          & 60.6          & 70.1          & 81.5          & 64.1                     & 44.4          & 68.1          & 79.1          & 87.9          & 69.9                    \\
+ Bi-Tuning        & \underline{47.2}          & 64.3          & 73.7          & 84.3          & 67.4                     & 48.3          & 72.8          & \underline{83.3}          & 90.2          & 73.7                    \\ \midrule
+ \methodname (4)             & 46.9          & \underline{64.8}          & \underline{74.3}          & \underline{85.3}          & \underline{67.8}                     & \underline{48.5}          & \underline{73.8}          & 82.5          & \underline{90.4}          & \underline{73.8}                    \\
\rowcolor{highlight} + Bi-Tuning + \methodname (4) & \textbf{47.7} & \textbf{65.1} & \textbf{76.5} & \textbf{87.1} & \textbf{69.1}            & \textbf{49.2} & \textbf{73.9} & \textbf{84.3} & \textbf{91.1} & \textbf{74.6}           \\ \bottomrule
\end{tabular}}
\vspace{-5pt}
\caption{\footnotesize Comparison of performance of \methodname with existing methods on standard TL benchmark ~\cite{dalib}, including in low-data regimes with limited target data. (E.g, 50\% denotes availability of 50\% of target dataset for finetuning). \textbf{Note that \methodname not only outperforms existing TL approaches, but also improves performance of existing TL approaches (see row for Bi-Tuning + \methodname)}. \methodname (4) denotes the use of four \smethodnames, as before.}
\label{tab:compare-tl-approaches}
\vspace{-12pt}
\end{table}



\noindent \underline{\textit{Object Detection and Segmentation:}} We present our studies on other vision tasks such as object detection and segmentation in the Appendix.


 
\vspace{-6pt}
\section{Conclusions and Future Work}
\vspace{-4pt}
In this work, we propose a new setting \settingname, to extend the definition of model patching \cite{modelpatching2022} to general purpose CNNs, and also ensuring the maintenance of performance on a base task. We propose \methodname as a simple and lightweight architectural modification to efficiently implement our model patching variant in this work. \settingname significantly outperform existing patching methods while only using a fraction of parameters for training. We also show that the proposed approach can be used for other problem settings such as transfer learning and zero-shot domain adaptation, as well as other vision tasks such as object detection and segmentation. Future directions include extensions to large-scale vision-language models and simultaneous adaptation to multiple tasks while only training a fraction of parameters. 

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\newpage
\clearpage

\appendix

\renewcommand{\thesection}{A\arabic{section}}
\renewcommand{\thetable}{A\arabic{table}}
\renewcommand{\thefigure}{A\arabic{figure}}

\section*{\centering Appendix}


\noindent In this appendix, we provide additional details which we could not include in
the main paper due to space constraints, including some quantitative and qualitative
results, as well as corresponding discussion and analysis that provide more insights
into the proposed method. In particular, We discuss the following:




 
\section{ Experiments on Scaling \methodname} \label{sec:scaling}
\vspace{-3pt}
For all the patching experiments in Section \ref{sec:patching} and Tables \ref{tab:patching-one-task}, \ref{tab:joint-patching}, \ref{tab:seq-patching} of the main paper, we used one \smethodname in the \methodname. In this section, we present the experimental results with more \smethodnames (1, 2, 4) introduced in \methodname. Each \smethodname increases the trainable parameters by a quantum, and hence, one could view such a study as scaling \methodname with increasing parameter sizes. As mentioned in the main paper, when only one \smethodname is introduced, we introduce it in the last block (of the four blocks) of the network. When 2\texttt{x}\smethodnames are introduced, we introduce them in the last two blocks of the network and in case where 4\texttt{x}\smethodnames are introduced, we have one \smethodname for each block (assuming 4 blocks in ResNet architectures).

Tables \ref{tab:single-task-scale-appendix}, \ref{tab:joint-patching-scale-appendix}, \ref{tab:seq-patching-scale-appendix} show the results of Single-Task Patching, Joint Patching, Sequential Patching with varying number of \smethodnames. While there is a general trend of increase in performance with increase in number of \smethodnames, there are some tasks where this doesn't strongly hold. Automatically choosing when to introduce a new \smethodname by studying the capacity of the learned network would be an interesting extension to this work. 


\begin{table}[h]
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{@{}l|l|c|c|c|c|c|c|c@{}}
\toprule
\textbf{Architecture}          & \multicolumn{1}{c|}{\textbf{Method}} & \multicolumn{1}{l|}{\textbf{\# Parameters}} & \textbf{Aircraft} & \textbf{Flowers} & \textbf{STL10} & \textbf{RESISC45} & \textbf{Cars}  & \textbf{GTSRB} \\ \midrule
\multirow{4}{*}{ResNet-18}     & PAINT                                & 11.2M                                       & 3.51              & 12.70            & 55.41          & 24.38             & 2.01           & 18.39          \\
                               & \methodname (1)                        & 0.4M                                        & 46.56             & 75.65            & 79.55          & 74.48             & 46.78          & 70.22          \\
                               & \methodname (2)                        & 0.6M                                        & 50.48             & \textbf{75.91}   & \textbf{79.66} & 76.08             & 51.29          & 76.20          \\
                               & \methodname (4)                      & 0.6M                                        & \textbf{52.67}    & 75.65            & 79.57          & \textbf{76.42}    & \textbf{76.23} & \textbf{78.94} \\ \midrule
\multirow{4}{*}{ResNet-34}     & PAINT                                & 21.8M                                       & 2.94              & 16.16            & 62.64          & 21.91             & 4.39           & 19.87          \\
                               & \methodname (1)                         & 0.4M                                        & 45.74             & 78.31            & 83.41          & 78.02             & 47.31          & 70.83          \\
                               & \methodname (2)                        & 0.6M                                        & 51.39             & \textbf{79.04}   & 83.59          & 80.18             & 55.24          & 78.11          \\
                               & \methodname (4)                      & 0.6M                                        & \textbf{54.82}    & 78.82            & \textbf{83.60} & \textbf{80.70}    & \textbf{57.33} & \textbf{82.36} \\ \midrule
\multirow{4}{*}{ResNet-50}     & PAINT                                & 23.5M                                       & 7.33              & 56.97            & 86.30          & 48.04             & 12.54          & 28.11          \\
                               & \methodname (1)                         & 7.1M                                        & 55.81             & 82.64            & \textbf{87.53} & 84.02             & 59.31          & 77.79          \\
                               & \methodname (2)                        & 8.9M                                        & \textbf{62.14}    & \textbf{82.74}   & 87.41          & 84.93             & 65.96          & 84.09          \\
                               & \methodname (4)                      & 9.3M                                        & 61.48             & 80.34            & 87.40          & \textbf{85.03}    & \textbf{67.11} & \textbf{85.67} \\ \midrule
\multirow{4}{*}{ConvNeXT-Tiny} & PAINT                                & 27.8M                                       & 6.18              & 69.06            & 84.57          & 34.81             & 6.87           & 12.36          \\
                               & \methodname (1)                         & 4.0M                                        & 69.59             & \textbf{87.49}   & \textbf{89.69} & 86.54             & 73.00          & 82.47          \\
                               & \methodname (2)                        & 5.0M                                        & \textbf{71.74}    & 84.89            & 89.32          & 86.97             & 75.44          & 88.01          \\
                               & \methodname (4)                      & 5.3M                                        & 70.81             & 82.35            & 89.35          & \textbf{87.24}    & \textbf{76.23} & \textbf{88.96} \\ \bottomrule
\end{tabular}}
\caption{Harmonic Mean of Top-1 Accuracies -- Scaling \methodname  -- Single Task Patching. \textbf{Increasing the number of \smethodnames results in a slight increase in the number of parameters and in general translates to an improvement in performance in most cases when patching for a single-task}}
\label{tab:single-task-scale-appendix}
\end{table}









 
\section{ Theoretical Analysis of Variance Issues} \label{sec:theory}
\vspace{-3pt}
\noindent As shown in Eqn \ref{eq:6} in the main paper, with the use of residual connections, we have:
\begin{equation}
\begin{aligned}
    \operatorname{Var}(\mathcal{F}^j) & = \operatorname{Var}(\mathcal{F}^j(\mathcal{F}^{j-1}(\mathbf{x})) + \mathcal{F}^{j-1}(\mathbf{x})) \\ 
    &= \operatorname{Var}(\mathcal{F}^j(\mathcal{F}^{j-1}(\mathbf{x}))) +  \operatorname{Var}(\mathcal{F}^{j-1}(\mathbf{x}))) \\ & \quad \quad + \operatorname{Cov}(\mathcal{F}^j(\mathcal{F}^{j-1}(\mathbf{x})), \mathcal{F}^{j-1}(\mathbf{x})))
    \label{eq:a1}
\vspace{-15pt}
\end{aligned}
\end{equation}
To deduce Eqn \ref{eq:7} of the main paper from the above equation, we follow two observations: (1) When Kaiming initialization is used, the output variance of a residual block (without skip connection) is (approximately) equal to the input variance i.e $\operatorname{Var}(\mathcal{F}^{j-1}) (\mathbf{x}) \approx \operatorname{Var}(\mathbf{x})$; and (2) The input and output of a residual block are weakly correlated (as also suggested in \cite{NEURIPS2020_9b861925}).
We now describe the above two observations, and thus derive Eqn \ref{eq:7}. %


\noindent \textbf{(1) Output Variance $\approx$ Input Variance:} With Kaiming initialization, $\operatorname{Var}(\mathcal{F}^{j-1}(\mathbf{x})) \approx \operatorname{Var}(\mathbf{x})$ \cite{heinit}. Consider the variance at a particular convolutional layer of a neural network, whose output is given by: %
\begin{equation}\label{eq:forward}
  \ve{y}_l=\ma{W}_l\ve{x}_l + \ve{b}_l
\end{equation}
\noindent where $\ve{x}$ is a $k^2c \times 1$ vector that represents $k \times k$ pixels in $c$ input channels, and $k$ is the spatial filter size. With $n=k^2c$ denoting the number of connections of a response, 
$\ma{W}$ is a $d$-by-$n$ matrix, where $d$ is the number of filters and each row of $\ma{W}$ represents the weights of a filter. $\ve{b}$ is a vector of biases, $\ve{y}$ is the response at a pixel of the output map, and $l$ denotes the index of a layer.
$\ve{x}_{l}=f(\ve{y}_{l-1})$
where $f$ is the activation and $c_l = d_{l-1}$. 
As in \cite{heinit,GlorotAISTATS2010}, we assume that initialized elements in $\ma{W}_{l}$ are mutually independent and share the same distribution, as well as that $\ve{x}_l$ are mutually independent and share the same distribution. $\ve{x}_l$ and $\ma{W}_{l}$ are independent of each other. Then: 

\begin{equation}
\operatorname{Var}[y_{l}]=n_l\operatorname{Var}[w_{l}x_l],
\end{equation}
where $y_{l}$, $x_{l}$, and $w_{l}$ represent the random variables of each element in $\ve{y}_l$, $\ma{W}_l$, and $\ve{x}_l$ respectively. Since Kaiming initialization forces $w_{l}$ to have a zero mean, the variance of the product of independent variables gives us:

\begin{equation}\label{eq:y1}
\operatorname{Var}[y_{l}]=n_l\operatorname{Var}[w_{l}]E[x^2_{l}].
\end{equation}
Please note that $E[x^2_{l}]\neq \operatorname{Var}[x_l]$ unless $x_l$ has zero mean. For the ReLU activation (which is used in modern-day architectures), $x_{l}=max(0, y_{l-1})$ and thus may not have zero mean. 
If we let $w_{l-1}$ also have a symmetric distribution around zero and $b_{l-1}=0$, then $y_{l-1}$ has zero mean and has a symmetric distribution around zero. This leads to
$E[x^2_{l}]=\frac{1}{2}\operatorname{Var}[y_{l-1}]$ when $f$ is ReLU.
Combining this with Eqn (\ref{eq:y1}), we obtain:
\begin{equation}\label{eq:y2}
\operatorname{Var}[y_{l}]=\frac{1}{2}n_l\operatorname{Var}[w_{l}]\operatorname{Var}[y_{l-1}].
\end{equation}
With $L$ layers put together, we have:
\begin{equation}\label{eq:prod_fw}
\operatorname{Var}[y_{L}]=\operatorname{Var}[y_{1}]\left(\prod_{l=2}^{L}\frac{1}{2}n_l\operatorname{Var}[w_{l}]\right).
\end{equation}
The above equation is key for Kaiming initialization. A good initialization method should avoid reducing or magnifying the magnitudes of input signals significantly. Hence, for the product in Eqn \ref{eq:prod_fw} to only grow linearly (say by a scalar factor 1), %
a sufficient condition is:
\begin{equation}\label{eq:init_fw}
\frac{1}{2}n_l\operatorname{Var}[w_{l}]=1, \quad \forall l.
\end{equation}
This leads to a zero-mean Gaussian distribution whose standard deviation (std) is $\sqrt{2/{n_l}}$. This is used in Kaiming initialization along with initializing $\ve{b}=0$. 

Because of this careful initialization, the output variance of a residual block without skip connection, $\operatorname{Var}(\mathcal{F}^{j-1} (\mathbf{x}))$ is approximately equal to the input variance with Kaiming initialization, i.e. 
\begin{equation}
\operatorname{Var}(\mathcal{F}^{j-1}(\mathbf{x})) \approx \operatorname{Var}(\mathbf{x})
\end{equation}

\noindent \textbf{(2) The input and output of the residual block are weakly correlated:} Following \cite{NEURIPS2020_9b861925}, to evaluate the covariance term $\operatorname{Cov}(\mathcal{F}^j(\mathcal{F}^{j-1}(\mathbf{x})), \mathcal{F}^{j-1}(\mathbf{x})))$ in Eqn \ref{eq:a1}, we assume that any two coordinates in $\mathbf{x}$ are uncorrelated. With this assumption, one can show that the covariance term is about $O(1/\sqrt{d})$ small. To this end, we consider a single layer: $\mathcal{F(\mathbf{x})} = \ve{W}x$, where elements of $\ve{W}$ are sampled from a Gaussian as: $w_{i j} \sim \mathcal{N}(0,1 / d)$ to match the input/output variance.

\begin{equation}\label{eq:final}
\begin{aligned}
\operatorname{Cov}\left(\left[\mathbf{x}\right]_{i},\left[\mathcal{F}^{j}\left(\mathbf{x}\right)\right]_{i}\right) &= \operatorname{Cov}\left(\left[\mathbf{x}\right]_{i}, \sum_{j=1}^{d} w_{i j}\left[\mathbf{x}\right]_{j}\right) \\ 
&= \sum_{j=1}^{d} w_{i j} \operatorname{Cov}\left(\left[\mathbf{x}\right]_{i},\left[\mathbf{x}\right]_{j}\right) \\ &= w_{i i} \operatorname{Var}\left(\left[\mathbf{x}\right]_{i}\right)
\end{aligned}
\end{equation}

The above equation holds since we assume any two coordinates in $\mathbf{x}$ are uncorrelated. From Eqn \ref{eq:final}, $\operatorname{Var}(\left[ x_{k+1}\right]_i) \approx (2 + w_{ii})\operatorname{Var}(\left[x_k\right]_i)$. Since $w_{i j} \sim \mathcal{N}(0,1 / d)$, with probability at least $1 - \exp(-d/4)$, $2+w_{ii} > 2 - 1/\sqrt{2}$. In a common ResNet, with $d \geq 32$, the output variance of the residual block increases exponentially with very high probability. If there are multiple layers in the residual block $\mathcal{F}$, ReLU activations would further decrease (at least not increase) the correlation between $\mathbf{x}$ and $\mathcal{F}(\mathbf{x})$. So, the correlations between $\mathbf{x}$ and $\mathcal{F}(\mathbf{x})$ are $O(\sqrt{d})$ small. We request the interested reader to refer to \cite{NEURIPS2020_9b861925} for more details on this analysis.

The above analysis theoretically shows the problem of increase in variance when skip connections are introduced. This forms the primary motivation for introducing additional batch-normalization layers in \methodname (as discussed towards the end of Section \ref{sec_method}).


\begin{table}[]
\resizebox{\linewidth}{!}{\begin{tabular}{@{}l|c|c|c|c@{}}
\toprule
\textbf{Architecture} & \textbf{Method}    & \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}} Aircraft\\Flowers\\STL10\end{tabular}}} & \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}} GTSRB\\Cars\\Flowers\end{tabular}}} & \multicolumn{1}{l}{\textbf{\begin{tabular}[c]{@{}l@{}} STL10\\Cars\\GTSRB\end{tabular}}} \\ \midrule

ResNet-18     & PAINT              & 22.33                           & 4.35                        & 0.66                      \\
              & \methodname (1) & 66.91                           & 61.41                       & 66.91                     \\ 
             & \methodname (2) & 69.04                           & 64.71                       & 69.45                     \\
              & \methodname (4) & \textbf{71.14}                           & \textbf{66.64}                       & \textbf{71.14}                     \\ 
             \midrule
ResNet-34     & PAINT              & 13.75                           & 6.75                        & 15.20                     \\
              & \methodname (1) & 70.31                           & 64.51                       & 70.31                     \\ 
              & \methodname (2) & \textbf{82.48}                           & 67.81                       & 72.93                     \\
              & \methodname (4) & 75.39                           & \textbf{71.32}                       & \textbf{75.39}                     \\
              \midrule
ResNet-50     & PAINT              & 55.65                           & 4.70                        & 35.16                     \\
              & \methodname (1) & 76.56                           & 71.96                       & 76.56                     \\ 
              & \methodname (2) & 80.61                           & 76.94                       & 80.02                     \\
              & \methodname (4) & \textbf{81.79}                           & \textbf{78.75}                       & \textbf{81.79}                     \\
              \midrule

ConvNeXt-TINY & PAINT              & 34.72                           & 1.61     & 6.70                      \\
              & \methodname (1) & 81.12                           & 77.45                       & 81.02                     \\    
              & \methodname (2) & 82.69                           & 80.91                       & 83.29                     \\
              & \methodname (4) & \textbf{82.91}                           & \textbf{82.32}                       & \textbf{84.33}                     \\ 
              \bottomrule
\end{tabular}}
\caption{Harmonic Mean of Top-1 Accuracies -- Scaling \methodname -- Joint Patching. \textbf{Increasing the number of \smethodnames improves the performance in majority of the cases when performing joint patching.}}
\label{tab:joint-patching-scale-appendix}
\end{table}

\begin{table}[]
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{@{}l|l|c|c|c@{}}
\toprule
\textbf{Architecture}      & \textbf{Method} & \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}}Aircraft\\ Flowers\\ STL10\end{tabular}}} & \multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}}GTSRB\\ Cars\\ Flowers\end{tabular}}} & \multicolumn{1}{l}{\textbf{\begin{tabular}[c]{@{}l@{}}STL10\\ Cars\\ GTSRB\end{tabular}}} \\ \midrule
\multirow{4}{*}{ResNet-18} & PAINT           & 20.02                                                                                            & 14.28                                                                                        & 26.85                                                                                     \\
                           & \methodname (1)        & 70.52                                                                                            & 64.95                                                                                        & 67.51                                                                                     \\
                           & \methodname (2)        & 71.93                                                                                            & 69.89                                                                                        & 72.37                                                                                     \\
                           & \methodname (4)        & \textbf{72.41}                                                                                   & \textbf{79.03}                                                                               & \textbf{81.60}                                                                            \\ \midrule
\multirow{4}{*}{ConvNeXT-Tiny} & PAINT           & 19.95                                                                                            & 28.68                                                                                        & 27.01                                                                                     \\
                           & \methodname (1)        & 83.52                                                                                            & 64.72                                                                                        & 65.74                                                                                     \\
                           & \methodname (2)        & \textbf{82.71}                                                                                   & \textbf{83.45}                                                                               & 85.94                                                                                     \\
                           & \methodname (4)        & 81.06                                                                                            & 83.04                                                                                        & \textbf{86.84}                                                                            \\ \bottomrule
\end{tabular}}
\caption{Mean of Top-1 accuracies. Scaling \methodname -- Sequential Patching. \textbf{Increasing the number of \smethodnames in majority cases translates to an improvement in performance in the case of sequential patching}}
\label{tab:seq-patching-scale-appendix}
\end{table}

\begin{table*}[]
\resizebox{\linewidth}{!}{\begin{tabular}{@{}l|c|c|cc|cc|cc|cc|cc|cc@{}}
\toprule
\textbf{Architecture}           & \textbf{Method}       & \multicolumn{1}{l|}{\textbf{\# Parameters}} & \multicolumn{2}{c|}{\textbf{RESISC-45}}           & \multicolumn{2}{c|}{\textbf{STL-10}}                 & \multicolumn{2}{c|}{\textbf{GTSRB}}                  & \multicolumn{2}{c|}{\textbf{Aircraft}}               & \multicolumn{2}{c|}{\textbf{Flowers}}                & \multicolumn{2}{c}{\textbf{Cars}}                 \\ \midrule
                                & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{}                       & \multicolumn{1}{c|}{ImageNet}    & RESISC-45      & \multicolumn{1}{c|}{ImageNet}       & STL-10         & \multicolumn{1}{c|}{ImageNet}       & GTSRB          & \multicolumn{1}{c|}{ImageNet}       & Aircraft       & \multicolumn{1}{c|}{ImageNet}       & Flowers        & \multicolumn{1}{c|}{ImageNet}    & Cars           \\ \midrule
\multirow{2}{*}{ResNet-18}      & PAINT                 & 11.2M                                       & \multicolumn{1}{c|}{{\underline{68.87}}} & 14.81          & \multicolumn{1}{c|}{{\underline{68.87}}}    & 48.35          & \multicolumn{1}{c|}{{\underline{68.87}}}    & 10.61          & \multicolumn{1}{c|}{{\underline{68.87}}}    & 1.80           & \multicolumn{1}{c|}{{\underline{68.87}}}    & 6.99           & \multicolumn{1}{c|}{{\underline{68.87}}} & 1.02           \\ \cmidrule(l){2-15} 
                                & \methodname (1)          & 0.4M                                        & \multicolumn{1}{c|}{{\underline{68.87}}} & \textbf{81.10} & \multicolumn{1}{c|}{{\underline{68.87}}}    & \textbf{94.14} & \multicolumn{1}{c|}{{\underline{68.87}}}    & \textbf{71.62} & \multicolumn{1}{c|}{{\underline{68.87}}}    & \textbf{35.16} & \multicolumn{1}{c|}{{\underline{68.87}}}    & \textbf{83.90} & \multicolumn{1}{c|}{{\underline{68.87}}} & \textbf{35.42} \\ \midrule
\multirow{2}{*}{ResNet-34}      & PAINT                 & 21.8M                                       & \multicolumn{1}{c|}{{\underline{74.66}}} & 12.84          & \multicolumn{1}{c|}{{\underline{74.66}}}    & 53.95          & \multicolumn{1}{c|}{{\underline{74.66}}}    & 11.46          & \multicolumn{1}{c|}{{\underline{74.66}}}    & 1.50           & \multicolumn{1}{c|}{{\underline{74.66}}}    & 9.06           & \multicolumn{1}{c|}{{\underline{74.66}}} & 2.26           \\ \cmidrule(l){2-15} 
                                & \methodname (1)          & 0.4M                                        & \multicolumn{1}{c|}{{\underline{74.66}}} & \textbf{81.70} & \multicolumn{1}{c|}{{\underline{74.66}}}    & \textbf{94.49} & \multicolumn{1}{c|}{{\underline{74.66}}}    & \textbf{67.38} & \multicolumn{1}{c|}{{\underline{74.66}}}    & \textbf{32.97} & \multicolumn{1}{c|}{{\underline{74.66}}}    & \textbf{82.34} & \multicolumn{1}{c|}{{\underline{74.66}}} & \textbf{34.62} \\ \midrule
\multirow{2}{*}{ResNet-50}      & PAINT                 & 23.5M                                       & \multicolumn{1}{c|}{{\underline{80.15}}} & 34.30          & \multicolumn{1}{c|}{{\underline{80.15}}}    & 93.46          & \multicolumn{1}{c|}{{\underline{80.15}}}    & 17.05          & \multicolumn{1}{c|}{{\underline{80.15}}}    & 3.84           & \multicolumn{1}{c|}{52.91}          & 61.70          & \multicolumn{1}{c|}{{\underline{80.15}}} & 6.80           \\ \cmidrule(l){2-15} 
                                & \methodname (1)          & 7.1M                                        & \multicolumn{1}{c|}{{\underline{80.15}}} & \textbf{88.29} & \multicolumn{1}{c|}{{\underline{80.15}}}    & \textbf{96.40} & \multicolumn{1}{c|}{{\underline{80.15}}}    & \textbf{75.57} & \multicolumn{1}{c|}{{\underline{80.15}}}    & \textbf{42.81} & \multicolumn{1}{c|}{\textbf{80.15}} & \textbf{85.28} & \multicolumn{1}{c|}{{\underline{80.15}}} & \textbf{47.07} \\ \midrule
\multirow{2}{*}{ResNet-101}     & PAINT                 & 42.5M                                       & \multicolumn{1}{c|}{{\underline{81.87}}} & 27.57          & \multicolumn{1}{c|}{74.66}          & 56.79          & \multicolumn{1}{c|}{78.27}          & 10.26          & \multicolumn{1}{c|}{{\underline{81.87}}}    & 2.54           & \multicolumn{1}{c|}{64.50}          & 52.90          & \multicolumn{1}{c|}{{\underline{81.87}}} & 5.75           \\ \cmidrule(l){2-15} 
                                & \methodname (1)          & 7.1M                                        & \multicolumn{1}{c|}{{\underline{81.87}}} & \textbf{85.13} & \multicolumn{1}{c|}{\textbf{81.87}} & \textbf{97.01} & \multicolumn{1}{c|}{\textbf{81.87}} & \textbf{73.35} & \multicolumn{1}{c|}{{\underline{81.87}}}    & \textbf{39.18} & \multicolumn{1}{c|}{\textbf{81.87}} & \textbf{83.72} & \multicolumn{1}{c|}{{\underline{81.87}}} & \textbf{43.10} \\ \midrule
\multirow{2}{*}{ConvNeXt-Tiny}  & PAINT                 & 27.8M                                       & \multicolumn{1}{c|}{{\underline{82.47}}} & 22.06          & \multicolumn{1}{c|}{\textbf{86.78}} & 82.47          & \multicolumn{1}{c|}{{\underline{82.47}}}    & 6.68           & \multicolumn{1}{c|}{{\underline{82.47}}}    & 3.21           & \multicolumn{1}{c|}{76.69}          & 62.81          & \multicolumn{1}{c|}{{\underline{82.47}}} & 3.58           \\ \cmidrule(l){2-15} 
                                & \methodname (1)          & 4.0M                                        & \multicolumn{1}{c|}{{\underline{82.47}}} & \textbf{91.03} & \multicolumn{1}{c|}{82.47}          & \textbf{98.29} & \multicolumn{1}{c|}{{\underline{82.47}}}    & \textbf{82.48} & \multicolumn{1}{c|}{{\underline{82.47}}}    & \textbf{60.19} & \multicolumn{1}{c|}{\textbf{82.47}} & \textbf{93.15} & \multicolumn{1}{c|}{{\underline{82.47}}} & \textbf{65.49} \\ \midrule
\multirow{2}{*}{ConvNeXt-Small} & PAINT                 & 49.5M                                       & \multicolumn{1}{c|}{{\underline{83.30}}} & 25.95          & \multicolumn{1}{c|}{{\underline{83.30}}}    & 79.71          & \multicolumn{1}{c|}{{\underline{83.30}}}    & 14.93          & \multicolumn{1}{c|}{{\underline{83.30}}}    & 4.20           & \multicolumn{1}{c|}{76.22}          & 63.23          & \multicolumn{1}{c|}{{\underline{83.30}}} & 4.53           \\ \cmidrule(l){2-15} 
                                & \methodname (1)          & 4.0M                                        & \multicolumn{1}{c|}{{\underline{83.30}}} & \textbf{89.11} & \multicolumn{1}{c|}{{\underline{83.30}}}    & \textbf{98.53} & \multicolumn{1}{c|}{{\underline{83.30}}}    & \textbf{80.38} & \multicolumn{1}{c|}{{\underline{83.30}}}    & \textbf{54.40} & \multicolumn{1}{c|}{\textbf{83.30}} & \textbf{91.59} & \multicolumn{1}{c|}{{\underline{83.30}}} & \textbf{60.27} \\ \midrule
\multirow{2}{*}{ConvNeXt-Base}  & PAINT                 & 87.6M                                       & \multicolumn{1}{c|}{{\underline{84.15}}} & 26.52          & \multicolumn{1}{c|}{{\underline{84.15}}}    & 75.54          & \multicolumn{1}{c|}{{\underline{84.15}}}    & 11.25          & \multicolumn{1}{c|}{{\underline{84.15}}}    & 3.39           & \multicolumn{1}{c|}{77.52}          & 80.53          & \multicolumn{1}{c|}{{\underline{84.15}}} & 6.85           \\ \cmidrule(l){2-15} 
                                & \methodname (1)          & 7.1M                                        & \multicolumn{1}{c|}{{\underline{84.15}}} & \textbf{89.43} & \multicolumn{1}{c|}{{\underline{84.15}}}    & \textbf{98.98} & \multicolumn{1}{c|}{{\underline{84.15}}}    & \textbf{80.52} & \multicolumn{1}{c|}{{\underline{84.15}}}    & \textbf{53.23} & \multicolumn{1}{c|}{\textbf{84.15}} & \textbf{90.06} & \multicolumn{1}{c|}{{\underline{84.15}}} & \textbf{62.53} \\ \midrule
\multirow{2}{*}{ConvNeXt-Large} & PAINT                 & 196.2M                                      & \multicolumn{1}{c|}{{\underline{84.52}}} & 22.92          & \multicolumn{1}{c|}{{\underline{84.52}}}    & 77.25          & \multicolumn{1}{c|}{{\underline{84.52}}}    & 9.89           & \multicolumn{1}{c|}{8.02}           & 61.33          & \multicolumn{1}{c|}{80.99}          & 73.87          & \multicolumn{1}{c|}{{\underline{84.52}}} & 5.77           \\ \cmidrule(l){2-15} 
                                & \methodname (1)          & 15.9M                                       & \multicolumn{1}{c|}{{\underline{84.52}}} & \textbf{90.29} & \multicolumn{1}{c|}{{\underline{84.52}}}    & \textbf{98.99} & \multicolumn{1}{c|}{{\underline{84.52}}}    & \textbf{79.40} & \multicolumn{1}{c|}{\textbf{84.52}} & \textbf{52.99} & \multicolumn{1}{c|}{\textbf{84.52}} & \textbf{89.80} & \multicolumn{1}{c|}{{\underline{84.52}}} & \textbf{62.60} \\ \bottomrule
\end{tabular}}
\caption{Top-1 accuracies of the final patched model on the base task (ImageNet) and the patching task. Harmonic mean of these values are presented in Table 2 of main paper. \underline{\quad} denotes the experiments where the accuracy of our proposed method and the baseline (PAINT) are same. \textbf{\methodname significantly outperform PAINT on the patching task, while maintaining the accuracy on the base task.} }
\label{tab:one-task-patching-appendix}
\end{table*}

\section{ Ablation Studies for Patching} \label{sec:patching-abl}
\vspace{-3pt}
Owing to space constraints, we could not provide a detailed analysis of the single-task patching and sequential patching experiments in the main paper. We present these results in this section.


\subsection{ Single-Task Patching}
\vspace{-3pt}
\noindent Table \ref{tab:patching-one-task} in the main paper presents the harmonic mean of the top-1 accuracies of the final patched model on the patching task and the Base Task. While harmonic mean as discussed in the main paper gives equal importance to performance on all the previously learned tasks and the patching task, top-1 accuracies on the base and patched tasks provide a holistic view. Table \ref{tab:one-task-patching-appendix}  presents top-1 accuracies of the patched model on the base task (ImageNet) and the patching task (dataset for which the model is leveraged for).  In most cases, PAINT with $\alpha=0$ has the highest harmonic mean, where $\alpha=0$ corresponds to the model containing only the weights pertaining to the base task i.e ImageNet pre-trained weights in this setting. As a result, the ImageNet accuracy is good but the accuracy on the patched task is very low as evident in Table \ref{tab:one-task-patching-appendix}. \methodname perform significantly better than PAINT on the downstream task while also maintaining the performance on the base task. 

\subsection{  Sequential Patching: Understanding the performance drop in PAINT}
\vspace{-3pt}
In Table \ref{tab:seq-patching} of the main paper, we presented results for sequential patching where we report the mean of top-1 accuracies of the final model on all patching tasks. As PAINT requires choosing an $\alpha$ value empirically, we choose the $\alpha$ value that gives best performance on all the previous tasks (mean based aggregation). To provide better insights into how PAINT performs on all previous tasks with various interpolation coefficient values, we present the top-1 accuracies for all values of $\alpha$ in Table \ref{tab:sequential-patching-all}. As PAINT does not retain the base model weights or weights learned for each task, there is significant drop in performance on all previously learned tasks.

\begin{table*}[]
\captionsetup{font=footnotesize}
\footnotesize
\resizebox{\linewidth}{!}{\begin{tabular}{l|c|cccc|cccc|cccc<{\kern-\tabcolsep}}
\toprule
\textbf{Method (Base Model)}                                                       & \textbf{\begin{tabular}[c]{@{}c@{}}Alpha \\ (interpolation\\ coefficient)\end{tabular}} & \multicolumn{4}{c|}{\textbf{Aircraft-Flowers-STL}}                        & \multicolumn{4}{c|}{\textbf{GTSRB-Cars-Flowers}}                         & \multicolumn{4}{c}{\textbf{STL-Cars-GTSRB}}                            \\ \midrule
\multirow{12}{*}{\begin{tabular}[c]{@{}l@{}}PAINT \\ (ConvNeXT-Tiny)\end{tabular}} & \textbf{}                                                                               & \textbf{ImageNet} & \textbf{Aircraft} & \textbf{Flowers} & \textbf{STL}   & \textbf{ImageNet} & \textbf{GTSRB}  & \textbf{Cars}   & \textbf{Flowers} & \textbf{ImageNet} & \textbf{STL10} & \textbf{Cars}   & \textbf{GTSRB}  \\
                                                                                   & 0.0                                                                                     & 0.09              & 1.26              & 28.59            & 18.31          & 0.11              & 1.91            & \textbf{71.74}           & 13.03            & 0.09              & 11.18          & \textbf{72.96}           & 7.63            \\
                                                                                   & 0.1                                                                                     & 0.10              & 0.99              & 0.33             & 10.00          & 0.10              & 1.98            & 71.62           & 15.53            & 0.07              & 11.34          & 23.06           & 52.37           \\
                                                                                   & 0.2                                                                                     & 0.10              & 0.99              & 0.42             & 10.00          & 0.10              & 1.95            & 71.41           & 18.38            & 0.09              & 10.60          & 2.49            & 89.12           \\
                                                                                   & 0.3                                                                                     & 0.10              & 0.99              & 0.42             & 10.00          & 0.10              & 1.91            & 71.09           & 21.74            & 0.11              & 10.54          & 1.09            & 96.05           \\
                                                                                   & 0.4                                                                                     & 0.10              & 0.99              & 0.42             & 10.00          & 0.09              & 1.77            & 70.40           & 26.26            & 0.13              & 10.53          & 0.80            & 97.34           \\
                                                                                   & 0.5                                                                                     & 0.10              & 0.99              & 0.42             & 10.00          & 0.08              & 1.80            & 69.95           & 30.61            & 0.13              & 10.74          & 0.63            & 97.77           \\
                                                                                   & 0.6                                                                                     & 0.07              & 0.99              & 0.65             & 10.09          & 0.09              & 1.84            & 68.95           & 35.71            & 0.12              & 10.76          & 0.53            & \textbf{97.85}           \\
                                                                                   & 0.7                                                                                     & 0.07              & 1.02              & 1.09             & 12.51          & 0.09              & 1.94            & 67.77           & 40.32            & 0.13              & 10.51          & 0.53            & \textbf{97.85}           \\
                                                                                   & 0.8                                                                                     & 0.08              & 0.54              & 1.68             & 33.81          & 0.11              & 1.92            & 66.57           & 44.30            & 0.12              & 10.36          & 0.53            & 97.71           \\
                                                                                   & 0.9                                                                                     & 0.12              & 1.05              & 2.26             & 65.40          & 0.11              & 2.01            & 65.30           & 47.78            & 0.12              & 10.20          & 0.52            & 97.43           \\
                                                                                   & 1.0                                                                                     & 0.12              & 1.17              & 3.55             & 74.95          & 0.12              & 1.94            & 63.15           & 49.52            & 0.12              & 10.14          & 0.51            & 97.26           \\ \midrule
\rowcolor{highlight}\begin{tabular}[c]{@{}c@{}}ConvNeXT-Tiny \\ +\\\methodname(1)\end{tabular}    & \textbf{-}                                                                              & \textbf{82.47}   & \textbf{60.19}    & \textbf{93.15}   & \textbf{98.29} & \textbf{82.47}   & \textbf{82.48} & 65.49 & \textbf{93.15}   & \textbf{82.47}   & \textbf{98.29} & 65.49 & 82.48 \\ \bottomrule

\multirow{12}{*}{\begin{tabular}[c]{@{}l@{}}PAINT \\ (ResNet-18)\end{tabular}} & \textbf{}                                                                               & \multicolumn{1}{c}{\textbf{}}      & \multicolumn{1}{c}{\textbf{}}      & \multicolumn{1}{c}{\textbf{}}     & \multicolumn{1}{c|}{\textbf{}}      & \textbf{}      & \textbf{}      & \textbf{}      & \textbf{}     & \multicolumn{1}{c}{\textbf{}}      & \multicolumn{1}{c}{\textbf{}}      & \multicolumn{1}{c}{\textbf{}}      & \multicolumn{1}{c}{\textbf{}} \\
                                                                               & 0.0                                                                                     & 0.07                               & 1.38                               & 13.73                             & 37.08                               & 0.06           & 5.70           & 2.54           & 0.55          & 0.08                               & 35.25                              & 1.14                               & 6.13                          \\
                                                                               & 0.1                                                                                     & 0.10                               & 0.99                               & 0.47                              & 10.00                               & 0.09           & 5.69           & 4.24           & 0.55          & 0.10                               & 10.00                              & 0.45                               & 5.46                          \\
                                                                               & 0.2                                                                                     & 0.10                               & 1.00                               & 0.50                              & 10.00                               & 0.09           & 5.73           & 7.72           & 0.55          & 0.10                               & 10.00                              & 0.53                               & 5.46                          \\
                                                                               & 0.3                                                                                     & 0.10                               & 1.00                               & 0.50                              & 10.00                               & 0.07           & 5.66           & 13.98          & 0.57          & 0.10                               & 10.00                              & 0.53                               & 5.46                          \\
                                                                               & 0.4                                                                                     & 0.10                               & 1.00                               & 0.50                              & 10.00                               & 0.07           & 6.32           & 23.70          & 0.85          & 0.10                               & 10.00                              & 0.44                               & 5.46                          \\
                                                                               & 0.5                                                                                     & 0.10                               & 1.00                               & 0.50                              & 10.00                               & 0.08           & 4.62           & 32.48          & 1.72          & 0.10                               & 10.01                              & 0.44                               & 5.46                          \\
                                                                               & 0.6                                                                                     & 0.11                               & 1.00                               & 0.60                              & 10.03                               & 0.07           & 3.20           & 39.25          & 3.43          & 0.10                               & 10.00                              & 0.44                               & 2.85                          \\
                                                                               & 0.7                                                                                     & 0.09                               & 1.01                               & 0.82                              & 11.07                               & 0.09           & 2.15           & \textbf{41.67} & 8.51          & 0.10                               & 10.00                              & 0.44                               & 4.99                          \\
                                                                               & 0.8                                                                                     & 0.10                               & 0.79                               & 1.06                              & 25.95                               & 0.09           & 2.28           & 38.74          & 18.26         & 0.14                               & 12.68                              & 0.55                               & 17.81                         \\
                                                                               & 0.9                                                                                     & 0.12                               & 0.97                               & 1.24                              & 64.39                               & 0.07           & 2.88           & 29.39          & 31.11         & 0.10                               & 10.46                              & 0.36                               & 94.14                         \\
                                                                               & 1.0                                                                                     & 0.12                               & 1.02                               & 1.73                              & 77.20                               & 0.06           & 4.55           & 13.80          & 38.69         & 0.10                               & 8.44                               & 0.47                               & \textbf{98.38}                \\ \midrule
\rowcolor{highlight}\begin{tabular}[c]{@{}c@{}}ResNet-18\\ +\\\methodname(1)\end{tabular}    & \textbf{-}                                                                        & \multicolumn{1}{c}{\textbf{68.87}} & \multicolumn{1}{c}{\textbf{35.16}} & \multicolumn{1}{c}{\textbf{83.9}} & \multicolumn{1}{c|}{\textbf{94.14}} & \textbf{68.87} & \textbf{71.62} & 35.42          & \textbf{83.9} & \multicolumn{1}{c}{\textbf{68.87}} & \multicolumn{1}{c}{\textbf{94.14}} & \multicolumn{1}{c}{\textbf{35.42}} & \multicolumn{1}{c}{71.62}     \\ \bottomrule

\end{tabular}}
\caption{Sequential Patching on ConvNeXT-Tiny and ResNet-18: Top-1 accuracies of the final model for each value of interpolation coefficient ($\alpha$ used by PAINT) on all previously learned tasks, including the base task. \textbf{While PAINT has a significant drop in performance on the previous tasks, \methodname have zero-drop in performance on the previously learned tasks.}}
\label{tab:sequential-patching-all}
\end{table*}







\section{ Experiments on More Vision Tasks and Settings} \label{sec:other-exps}
\vspace{-3pt}
\subsection{ Detection and Segmentation}
\vspace{-3pt}
\noindent Pre-trained ImageNet models are crucial for object detection and semantic segmentation since they are used as network backbones for both tasks. To study the usefulness of \methodname as a component of the pre-trained backbone networks of existing detection/segmentation models, we add \methodname in commonly used architectures that are used for these tasks. We study the Faster-RCNN model \cite{renNIPS15fasterrcnn} (implementation provided by \cite{jjfaster2rcnn}) with ResNet-18 and ResNet-34 backbones for object detection. For semantic segmentation, we use DeepLab-V3 \cite{deeplabv3} with a ResNet-50 backbone. Our results for these experiments on the PASCAL-VOC dataset \cite{pascal-voc-2007} are shown in Table \ref{tab:detect-segm-results}. When \methodname are introduced, we train only the \smethodnames with the backbone frozen. As evident, using \methodname provides improvements in performance. (We note that our objective was not to obtain state-of-the-art performance on these tasks, but to show that adding \methodname improves performance on architectures used for different tasks.) %

\begin{table}
\captionsetup{font=footnotesize}
\footnotesize
\centering
\begin{tabular}{>{\kern-\tabcolsep}c|l|c<{\kern-\tabcolsep}}
\toprule
Task                                                                              & Backbone        & mAP/IoU        \\ \midrule
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Object \\ Detection\end{tabular}}                                                 & ResNet-18       & 59.72         \\
                                                                                  & \cellcolor{highlight} + \methodname (4) &\cellcolor{highlight} \textbf{61.33} \\ \cmidrule(l){2-3} 
                                                                                  & ResNet-34       & 64.40         \\
                                                                                  & \cellcolor{highlight} + \methodname (4) &\cellcolor{highlight} \textbf{67.80} \\ \midrule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Semantic\\ Segementation\end{tabular}} & ResNet-50       & 70.02          \\
                                                                                  & \cellcolor{highlight}+ \methodname (4) &\cellcolor{highlight} \textbf{70.80 } \\ \bottomrule
\end{tabular}
\vspace{-4pt}
\caption{Detection and Segmentation results on Faster-RCNN and DeepLab-v3 using various backbones. \methodname (4) denotes the use of four \smethodnames.\textbf{Adding \methodname improves performance even on more complex tasks like detection and segmentation }.}
\vspace{-4pt}
\label{tab:detect-segm-results}
\end{table}

\subsection{ Training \methodname from Scratch}
\vspace{-3pt}
\noindent All experiments shown so far deal with pre-trained models. It is also important to understand how \methodname behaves when training the model from scratch. To this end, we train and compare standard models and models with \methodname without any form of pre-training. Results for this experiment are shown in Table \ref{tab:scratch-train}.  
\methodname shows consistent improvement even when there is no pre-training involved. Since the fine-grained datasets considered are relatively smaller in size, larger models tend to overfit and perform poorly on test data. \methodname helps in avoiding this overfitting problem when training from scratch with limited data.
\noindent To test the effectiveness of \methodname on commonly used image classification benchmarks, we run experiments on CIFAR-10, CIFAR-100 and ImageNet datasets. 
Results are shown in Table \ref{tab:scratch-acad}. \methodname once again shows improvement on these benchmarks with no pre-training (without significant hyperparameter tuning). (For the Imagenet dataset, we show results only with ResNet-18 architecture due to the significant time and computation requirements otherwise.)


\begin{table}
\centering
\begin{tabular}{l|cc<{\kern-\tabcolsep}}
\toprule
\multicolumn{1}{l|}{\textbf{Architecture}} & \textbf{FGVC-Aircraft}                & \textbf{Stanford-Cars}                \\ \midrule
ResNet-18                                  & 53.40          & 53.47           \\
\rowcolor{highlight}+ \methodname (4)                            & \textbf{54.12 } & \textbf{55.92 } \\ \midrule
ResNet-34                                  & 48.59           & 43.39           \\
 \rowcolor{highlight} + \methodname (4)                           & \textbf{50.93} & \textbf{54.54} \\ \bottomrule
\end{tabular}
\caption{Comparison of different architectures when trained from scratch without any pre-training.\textbf{\methodname can help when training models from scratch on fine-grained datasets}.}

\label{tab:scratch-train}
\end{table}



\begin{table}
\resizebox{\linewidth}{!}{\begin{tabular}{l|ccc<{\kern-\tabcolsep}}
\toprule
\textbf{Architecture} & \textbf{CIFAR-10} & \textbf{CIFAR-100} & \textbf{ImageNet (Top-5)} \\ \midrule
ResNet-18             & 85.21             & 50.43              & 85.87                     \\
\rowcolor{highlight} + \methodname (4)      & \textbf{85.47}             & \textbf{52.17}              & \textbf{86.16}                     \\ \midrule
ResNet-34             & 81.25             & 48.05              & -                         \\
\rowcolor{highlight} + \methodname (4)      & \textbf{84.76}             & \textbf{51.77}              & -                         \\ \bottomrule
\end{tabular}}
\caption{Comparison of different architectures when trained from scratch without any pre-training on standard image classification datasets.\textbf{\methodname can help improve performance on common image classification benchmark datasets without pre-training}.}

\label{tab:scratch-acad}
\end{table}



\section{ Visualizations} \label{sec:viz}
\vspace{-3pt}
\subsection{ Visualizing Learned $\Lambda$ Values}
\vspace{-3pt}
To see if \methodname uses all skip connections, we visualize the weights of these skip connections in Fig \ref{fig:vis_lambda}.
We take the average value of weights for all test samples in FGVC-aircraft and Stanford-Dogs datasets.  Even without imposing any specific sparsity schemes such as L1-penalty, some of the skip connection weights are zeros, which shows how the \methodname learns to combine features with few skip connections, which is better than finetuning entire architectures for feature reuse. 


\begin{figure}[h]
\captionsetup{font=footnotesize}
  \centering
  \includegraphics[width=0.95\linewidth]{Lambda_Vis_big.pdf}
  \caption{\textbf{Despite not imposing a specific sparsity scheme, \methodname learns to select input-conditioned skip connections in a task-specific manner and zeros out some skip connections (especially in Blocks 2 and 3)}. Qualitative visualization of the weights (average) learned by the \methodname for all test samples of FGVC-Aircraft and Stanford Cars datasets, we use ResNeXt-50 as the base network and introduce four \smethodnames in our \methodname, one for each block}
  \label{fig:vis_lambda}
\end{figure} 


\subsection{ Visualization of Learned Design Principles}
\vspace{-6pt}
We visualize the learned values of $\Lambda$ (weights assigned for the skip connection by the \smethodname) for ResNet-18 on samples from three of the datasets we considered in our experiments.These visualizations are shown in Figures \ref{figvis1}, \ref{figvis2}, \ref{figvis3}. These visualizations provide interesting insights that could be useful for architecture design in general. 
In particular, we note that: (i) In the initial modules, skip connections from the beginning of the block to the end of the block are sufficient, skip connections across blocks have a very less weight; and (ii) In the last module, all skip connections are required with each of them having a weight of 1.
One common trend seen across all datasets is that weights for most skip connections in the last module of the network are always 1 consistently. This can also be observed in Figure \ref{fig:vis_lambda}. 

\begin{figure*}[h]
  \centering
  \includegraphics[page=1, width=0.9\linewidth]{iccv2023AuthorKit/PolicVis.pdf} 
  \caption{Visualization of $\Lambda$ values on two samples of FGVC-Aircraft dataset \cite{fgvc-aircraft} for ResNet-18 network}
  \label{figvis1}
\end{figure*} 

\begin{figure*}[h]
  \centering
  \includegraphics[page=2, width=0.9\linewidth]{iccv2023AuthorKit/PolicVis.pdf} 
    \caption{Visualization of $\Lambda$ values on two samples of Flowers-102 dataset \cite{flowers-102} for ResNet-18 network}
    \label{figvis2}
\end{figure*} 

\begin{figure*}[h]
  \centering
  \includegraphics[page=3, width=0.9\linewidth]{iccv2023AuthorKit/PolicVis.pdf} 
    \caption{Visualization of $\Lambda$ values on two samples of Stanford-Cars dataset \cite{stanford-cars} for ResNet-18 network}
    \label{figvis3}
\end{figure*} 

\newpage
\end{document}
