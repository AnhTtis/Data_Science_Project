\begin{center}
    \Large \textbf{Supplementary Material}
\end{center}

\section{Effect of Hyper-parameter $\alpha$, $\beta$}
\noindent Tab.~\ref{tab:abaltion_alpha_beta} presents the ablation study on the use of different hyper-parameter $\alpha, \beta$ ratios in Eq. (2) during the pretraining of \ours. The results demonstrate that the setting of $\alpha : \beta = 1: 2$ yields the best performance, which is consistent with the recommended setting in the CoCa~\cite{yu2022coca} paper.

\input{Tables/supp-1-ablation-alpha-beta}

\section{Effect of Margin Hyper-parameter $m$}
\noindent Tab.~\ref{tab:abaltion_margin} presents an ablation study for using different margin hyper-parameter $m$ in Eq. (7) when finetuning \ours-R. The results show that a margin of $m=0.1$ achieves the best performance, and we adopt this value as the default for all other experiments.

\input{Tables/supp-2-ablation-margin}

\section{Effect of Random Sampling Comments}
\noindent During training, we create image-comment pairs by randomly selecting one comment from the available list of comments for an image, if there are multiple comments associated with the same image. Tab.~\ref{tab:abaltion_sampling_comment} shows the effect of such random sampling during aesthetic pretraining. When a fixed comment is used for each image, the AVA ZSL performance drops from 0.663 PLCC to 0.596. Random sampling is an effective approach since different comments may cover different aesthetic aspects of the same image, allowing the model to fully expose itself to diverse and rich aesthetic information in the noisy dataset. This strategy enables the mining of open-set aesthetic concepts automatically.
\input{Tables/supp-3-sampling-comment}

\section{Per-class Evaluation on AVA-Style}
\noindent We show the per-class evaluation on AVA-Style in Tab.~\ref{tab:ava_style_prediction_per_class}, comparing to the same baselines as in our main paper. 
\input{Tables/supp-6-style-prediction}

\section{Details on ZSL for AVA-Style Classification}

\input{Tables/supp-5-style-prompts}
\noindent\textbf{Single prompt.}
In this approach, we use the 14 photographic style names as the language prompts: \{``complementary colors'', ``duo tones'', ``hdr'', ``image grain'', ``light on white'', ``long exposure'', ``macro'', ``motion blur'', ``negative image'', ``rule of thirds'', ``shallow dof'', ``silhouettes'', ``soft focus'', ``vanishing point''\}. The cosine similarity between the prompt text embedding and the image embedding is used as the  prediction score.

\vspace{+3mm}
\noindent\textbf{Ensemble of prompts.} In this approach, we manually curate five sentences/phrases that are frequently mentioned in the AVA-Caption user comments, for each of the styles. These prompts either use synonyms (\emph{e.g.} ``color'' and ``colors'') of the styles or add more text contexts (\emph{e.g.}, ``i like the lines and fading or vanishing''). Tab.~\ref{tab:style_prompts} shows these prompts.

\section{Details on ZSL for IAA}
\noindent To effectively perform zero-shot learning for IAA, we use a pair of prompts with opposite meanings (``good" v.s. ``bad").

\vspace{+3mm}
\noindent\textbf{Single prompt.} In this approach, we use \{``good image", ``bad image"\} as input prompts. Let $\vect{p}_g$ and $\vect{p}_b$ be the normalized unimodal text embedding for the ``good" and ``bad" prompts respectively, $\vect{v}$ be the normalized image contrastive embedding. We compute the cosine similarity and use the softmax normalized score for ``good image" as the final score $r$ for IAA.
\begin{align*}
r = \frac{e^{\vect{v}^\top\vect{p}_g}}{e^{\vect{v}^\top\vect{p}_g} + e^{\vect{v}^\top\vect{p}_b}}
\end{align*}

\noindent\textbf{Ensemble of prompts.} In this approach, we similarly construct six pairs of ``good" v.s. ``bad" prompts for \{``image", ``lighting", ``content", ``background", ``foreground", ``composition"). The second group in Tab.~\ref{tab:iaa_prompts} shows these pairs of prompts. For each pair, we can obtain a score $r_i, i = 1,..., 6$. Then we use the average ensemble of the scores to get the final score $r$ for IAA.
\input{Tables/supp-4-iaa-prompts}



\section{Results on KonIQ-10k}
\input{Tables/supp-7-koniq10k}

\noindent Table~\ref{tab:koniq-results} presents additional results on the image quality dataset KonIQ-10k~\cite{koniq10k}. We adopt the same data split as~\cite{koniq10k} and and employ a batch size of 32 to finetune the rank-based adapter for 30k steps, with a learning rate of 5e-4 and linear decay to zero, and 0.04 weight decay. Our proposed VILA-R outperforms CLIP-IQA$^+$ \cite{wang2022exploring} which trains a prompt tuning module on top of CLIP features. While CLIP features only use general pretraining, VILA-R benefits from the aesthetic pretraining which learns rich perceptual quality information, highlighting the importance of the proposed aesthtic pretraining. Remarkably, with only 0.1\% tunable parameters, VILA-R's performance is competitive with KonCept512 \cite{koniq10k} and MUSIQ \cite{Ke_2021_ICCV}, which rely on much larger resolutions. It is worth noting that KonIQ-10k~\cite{koniq10k} is not solely focused on aesthetics quality, and it includes images with technical quality problems such as compression and blur. There is limited user comments mentioning such aspects on the AVA-Captions dataset. Despite the gap, our model demonstrates competitive performance on KonIQ-10k, showcasing its robustness in capturing the visual appeal of the image across different datasets.


\section{More Qualitative Examples}

\noindent Fig.~\ref{fig:ava_styles_all} displays additional style retrieval results (top-5) on KonIQ-10k~\cite{koniq10k} using AVA-style names as the query. In order to provide clear attribution to the image sources, we have opted to showcase images from the KonIQ-10k dataset instead of the AVA dataset. Attribution to the images are provided in  Table~\ref{tab:koniq-attribution}. Overall, the retrieved results align with our aesthetic perspective. Notably, \ours{} accuratly captures the lighting or color related information. For example, images retrieved for ``Silhouettes" and ``Complementary colors'' accurately depict the corresponding concepts. Additionally, \ours{} recognizes concepts aesthetic concepts like ``Motion blur'' with high accuracy. However, there are also some failure cases where improvements are possible. For example, among the images retrieved using the query ``Rule of thirds", the last three images are centered rather than following the rule of thirds, which may be attributed to the random cropping augmentation during training. Augmentation improvement may help mitigate this issue.  For ``Duo tones", the top retrieved images have a yellowish tone, possibly due to training data bias in the AVA-Captions dataset. Thus, using a more diverse aesthetic pretraining dataset may further enhance the model's performance.


\begin{figure}[!t]
    \centering
    \includegraphics[width=8cm]{Figures/ava-styles-supp.png}
    \caption{More examples for the top-5 images retrieved using style name query on KonIQ-10k~\cite{koniq10k}. The source of the displayed images are provided in Table~\ref{tab:koniq-attribution}.}
    \label{fig:ava_styles_all}
    \vspace{-2mm}
\end{figure}

\section{KonIQ-10k Images Attribution}
\noindent In this paper, we display several images from KonIQ-10k~\cite{koniq10k}. The Flickr links and the license information for these images can be found in Table~\ref{tab:koniq-attribution}. We extend our gratitude to the original photographers for sharing their images.

\input{Tables/supp-8-image-attribution}