\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{Figures/teaser.png}\vspace{-0.5mm}
    \caption{We present \ours, a vision-language aesthetics learning framework based on image and user comment pairs. By pretraining on a contrastive and generative target, it shows superior performance on aesthetic captioning as well as zero-shot aesthetic tasks, \eg, IAA, and style classification. With a lightweight rank-based adapter, we can efficiently adapt the pretrained model to IAA.  }\vspace{-2.5mm}
    \label{fig:teaser} 
\end{figure}

\noindent Image Aesthetic Assessment (IAA) aims to quantify the human perceived aesthetics of an image. It has many important applications, including photo recommendation, selection, and editing. IAA is challenging because it is inherently subjective, and depends on various factors including image composition, color usage, photographic style, and subject matter. In recent years, various learning-based IAA methods have been proposed by leveraging deep models such as convolutional neural networks (CNN)~\cite{talebi2018nima, Chen_2020_CVPR, he_2022_ijcai, Hosu_2019_CVPR} and transformers~\cite{Ke_2021_ICCV}. These approaches learn from human-labeled IAA datasets where images are paired with aesthetic ratings, and models are trained to regress towards the mean opinion scores (MOS).

Directly learning IAA models on human-labeled aesthetic ratings, such as MOS, can be suboptimal as it lacks context regarding why an image is aesthetically pleasing or not. To provide richer supervision, various methods have attempted to integrate external knowledge such as theme~\cite{he_2022_ijcai,niu2022comment}, human eye fixation~\cite{ghosal2019geometry}, and aesthetic attributes~\cite{dhar2011high,kong2016photo}, to enhance IAA performance. These approaches typically rely on multitask training or cascade score prediction with a frozen attribute network. However, obtaining additional labeled data or off-the-shelf models for such methods can be costly.

Compared to the aforementioned methods that require additional annotations, our approach utilizes the abundance of image-comment pairs available on aesthetic websites and photographic forums.  These pairs can be easily obtained from the Internet and contain extensive aesthetic information (\eg objects, themes, styles, and user emotions), since humans are better at expressing aesthetic preferences  through natural language than through abstract scores.
On image sharing platforms like Flickr and DPChallenge\footnote{\url{https://www.dpchallenge.com/}}, user comments offer valuable insights into how they evaluate an image's aesthetics. For instance, as shown in Fig.~\ref{fig:teaser} (top), comments such as ``very cool patterns and curls" and ``little bit on the blurry side" reflects users' positive and negative aesthetic opinions respectively. We aim to learn the diverse aesthetic semantics present in these image-comment pairs to establish a solid foundation for downstream IAA tasks. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{Figures/main.png}\vspace{-0.5mm}
    \caption{Our proposed vision-language aesthetic (\ours) framework contains two parts: (1) \ours-P: pretraining a vision-language model using images and user comments on aesthetics, and (2) \ours-R:  a rank-based adapter that efficiently adapts the frozen pretrained model to score-based IAA with a small amount of tunable parameters (purple block). }\vspace{-2.5mm}
    \label{fig:overview}
\end{figure*}

Using image-comment pairs for aesthetics learning remains largely unexplored. While previous works have leveraged user comments to improve IAA, their approaches differ significantly from ours.
For example, ~\cite{zhou2016joint,hii2017multigap,zhang2021mscan} proposed to aggregate visual and comment features, yet they require both the image and comment as inputs during inference. This requirement makes it difficult to use such methods in real-world settings where images may not always be accompanied by comments. To mitigate this, Niu \etal \cite{niu2022comment} proposed to use the LDA topics~\cite{blei2003latent} from the comments as pseudo labels to guide image representation learning. However, the simplification of comments into topics may result in a loss of valuable contextual information. Therefore, we are motivated to explore other strategies for utilizing raw comments to extract richer aesthetic textual information. 

In this paper, we present a novel two-stage \textbf{VI}sion-\textbf{L}anguage \textbf{A}esthetics (\textbf{VILA}) learning framework incorporating image-text pretraining. Our goal is to develop a model that can effectively generalize to multiple downstream aesthetic tasks (Fig.~\ref{fig:teaser}). In the first \textbf{P}retraining stage, we learn an image-text model (\textbf{\ours-P}) by employing contrastive and text sequence generation objectives, enbaling us to fully leverage fine-grained knowledge from aesthetic image-comment pairs.
Our approach is motivated by recent advancements in vision-language models, such as CLIP~\cite{radford2021learning}, ALIGN~\cite{jia2021scaling}, and CoCa~\cite{yu2022coca}, which exhibit impressive performance and generalization ability across multiple tasks. These models align vision and language feature spaces to capture the rich semantic information.
However, these models are typically pretrained on general image-text pairs from the web, which can result in under-representation of aesthetic-related information. Our experimental results indicate that such generally pretrained vision-language models underperform on aesthetic tasks (Sec.~\ref{sec:results:mos}). As a solution, we propose the adoption of vision-language pretraining on aesthetic image-comment pairs from photograph sharing websites. To the best of our knowledge, our work is the first to explore the use of image-comment pairs in vision-language pretraining for aesthetics learning.

After pretraining \ours-P on image-comment pairs, we finetune it for downstream score-based IAA tasks using a lightweight \textbf{R}ank-based adapter (\textbf{\ours-R}). This adapter involves adding feature residuals to the frozen image embeddings to move images with high aesthetic quality closer to the anchor text ``good image," and images with low aesthetic quality away from it. This method can effectively rank images based on human rated preferences. With 0.1\% tunable parameters, our model outperforms previous works on IAA correlation metrics over the AVA dataset~\cite{murray2012ava}.

Our proposed \ours{} is capable of tackling multiple aesthetic-related tasks beyond score-based IAA (Fig.~\ref{fig:teaser}). Not only can it generate high-quality aesthetic comments, but it also exhibits impressive zero-shot learning (ZSL) capabilities for aesthetic style classification and quality analysis. Using text queries such as ``good image" and ``bad image" to compare images, our ZSL model outperforms supervised learning models like NIMA~\cite{talebi2018nima} which requires labor-intensive ratings as ground truth. This highlights the potential of learning rich image aesthetic concepts without relying on human-labeled data, thereby significantly reducing data collection costs.

We summarize the contributions of our work as follows:

\begin{itemize}[nolistsep,noitemsep]
\setlength
    \item We propose a vision-language aesthetic learning framework (\ours) for learning rich image aesthetic features using image-comment pairs. 
    \item We design a novel rank-based module to adapt the model to downstream IAA tasks without perturbing the pretrained weights, effectively learning the aesthetic quality concepts with minimal additional parameters.
    \item Our pretrained aesthetic model outperforms prior works for aesthetic captioning on the AVA-Captions~\cite{ghosal2019aesthetic} dataset. Even without any supervised labels, our zero-shot model achieves 69\% mAP on the AVA-Style~\cite{murray2012ava} dataset and 0.657 SRCC on the AVA dataset~\cite{murray2012ava}, outperforming many supervised approaches. With the proposed adapter and a small number of tunable parameters, our method further achieves state-of-the-art performance on AVA.
\end{itemize}
