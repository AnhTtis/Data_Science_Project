\section{Image Aesthetics Pretraining using CoCa}
\label{sec:method:image_text}
\noindent In this section, we present our approach to pretrain the image aesthetic model \ours-P. Our goal in the pretraining stage is to learn powerful multimodal representations for image aesthetics in a self-supervised manner, using both images and their associated user comments. 

Without loss of generality, we adopt the CoCa \cite{yu2022coca} architecture, which combines contrastive learning and image-to-caption generation in a single framework.  Our approach is generally applicable to broader vision-language pretraining models. Fig.~\ref{fig:overview} (1) provides an overview of our pretraining architecture for \ours-P.

\subsection{Preliminary of CoCa}
\noindent CoCa contains an image encoder, a unimodal text decoder, and a multimodal text decoder. The image encoder produces an image representation, while the unimodal text decoder generates a text representation with an appended \texttt{[CLS]} token. These two representations are aligned using a contrastive objective.  The multimodal text decoder generates captions by cross-attending to the image features.

\noindent \textbf{Encoding Image:} The image encoder is in the form of a Vision Transformer~\cite{dosovitskiy2020image}, which splits an image into patches and treats them as tokens. The patches are then projected to $D$-dimensional features and fed to the transformer blocks to generate a sequence of visual embeddings $\vect{V} = \{\vect{v}_1, ..., \vect{v}_K\}$, where $K$ is the number of visual tokens.

\noindent \textbf{Encoding Text: } The text is first tokenized into a sequence of tokens, with each token  mapped to a $D$-dimensional word embedding vector. A \texttt{[CLS]} token is appended to the sequence, and the sequence is passed through transformer layers to generate the unimodal text representation $\vect{W} = \{\vect{w}_1, ..., \vect{w}_L, \vect{w}_{cls} \}$, where $\vect{w}_{cls}$ is output of the \texttt{[CLS]} token, and $L$ is the number of text tokens. The transformer text decoder layers are trained with causally-masked self-attention for the captioning objective, which prevents tokens from attending to future tokens. The learnable token $\vect{w}_{cls}$ is used as the contrastive text embedding.

\noindent \textbf{Contrastive Learning Objective: } The two unimodal encoding modules are jointly optimized by a contrastive target which tries to align the image-text pairs:
\vspace{-1mm}
\begin{align}
    \label{eq:contrastive_loss}
    \begin{split}
        \mathcal{L}^{i2t}_{\text{Con}} &= -\frac{1}{N} \Big(\sum_i^N \log \frac{\exp (\vect{x}_i^\top\vect{y}_i)/\tau}{\sum_{j=1}^N\exp(\vect{x}_i^\top\vect{y}_j/\tau)}\Big) \\
        \mathcal{L}^{t2i}_{\text{Con}} &= -\frac{1}{N} \Big(\sum_i^N \log \frac{\exp (\vect{y}_i^\top\vect{x}_i)/\tau}{\sum_{j=1}^N\exp(\vect{y}_i^\top\vect{x}_j/\tau)} \Big) \\
        \mathcal{L}_{\text{Con}} &= \mathcal{L}^{i2t}_{\text{Con}} + \mathcal{L}^{t2i}_{\text{Con}}
    \end{split}
\end{align}
\vspace{-1mm}

\noindent $\vect{x}_i$ and $\vect{y}_i$ are the normalized contrastive embeddings of the $i$-th image and text in the batch. $L^{i2t}_{\text{Con}}$ is the image-to-text contrastive loss and $ L^{t2i}_{\text{Con}}$ is the text-to-image counterpart, $\tau$ is the learnable temperature, $N$ is the batch size.


\noindent \textbf{Generative Learning Objective: } For captioning, the multimodal text decoder learns to maximize the likelihood of generating the paired text conditioned on visual features in an autoregressive manner:
\vspace{-1mm}
$$
\mathcal{L}_{\text{Gen}} = -\sum_{t=1} ^ L \log P (\vect{w}_t | \vect{w}_{< t}, \vect{V}).
$$
\vspace{-2mm}


\noindent\textbf{Cotraining Contrastive and Generative Objective: } \noindent To cotrain the two targets, two task-specific attentional pooling layers \cite{lee2019set} are added on top of the image encoder to generate a contrastive image representation and a generative image representation. The pretraining objective is a weighted sum of the contrastive loss and the generative loss, using hyper-parameters $\alpha$ and $\beta$:
\vspace{-1mm}
\begin{align}
\mathcal{L} = \alpha \mathcal{L}_{\text{Con}} + \beta \mathcal{L}_{\text{Gen}}.
\end{align}
\vspace{-4mm}


\subsection{Vision-Language Pretraining for Aesthetics}
\noindent Vision-language pretraining methods require large-scale data to learn the complex dynamics between visual and textual information. Many of these methods are trained on large proprietary datasets~\cite{radford2021learning, jia2021scaling} with image-text pairs crawled from the web. While this  general pretraining strategy has proven useful for tasks such as image classification and retrieval, it is limited in its ability to represent aesthetic-related information due to the under-representation of such information on the web. Consequently, the aesthetic information gets diluted in the vast amount of pretraining data. To address this limitation, we propose a two-stage pretraining approach that involves initializing the model with a generally pretrained image-text model and then further pretraining it on aesthetic image-comment pairs. For general pretraining, we use a 650M filtered subset of the openly available LAION-5B-English~\cite{schuhmann2022laion} dataset. For aesthetic pretraining, we use the AVA-Captions dataset~\cite{ghosal2019aesthetic} which is currently the largest available dataset for aesthetic comments. Each image in AVA-Captions is associated with one or more user comments that provide informative insights into different aesthetic aspects of the image. We randomly sample one comment for each image to construct image-comment pairs during training.

In contrast to traditional supervised learning with pre-defined labels or categories, vision-language pretraining enables learning of open-set aesthetic concepts through noisy image-comment pairs. This results in visual and textual representations that encompass a wider range of aesthetic concepts, enhancing transferability to downstream tasks.

\section{Adapting Vision-Language Model for IAA}
\label{sec:method:regression_prompts}
\noindent The pretrained model \ours{}-P contains extensive multimodal aesthetic information, enabling it to perform zero-shot aesthetic tasks and to even outperform supervised models (Sec~\ref{sec:results:mos} and Sec~\ref{sec:results:ava_comments}). In this section, we aim to further enhance the model's performance for IAA tasks using the mean-opinion-score (MOS) labels. Finetuning the entire model is computationally expensive and can harm the pretrained model's zero-shot and captioning capability. Therefore, we propose a lightweight rank-based adapter module that adapts the pretrained vision-language model to downstream IAA tasks while keeping the image and text backbone frozen with only a few tunable parameters. The adapter module allows the model to retain the benefits of the pretrained backbone, while leveraging the rich aesthetic textual information for IAA tasks. Fig.~\ref{fig:overview} (2) depicts the overview of the adapter module, and we refer to the resulting model as \ours-R.

\subsection{Image Aesthetic Assessment Formulation}
\noindent The goal of IAA is to predict the aesthetic score for a given image. We focus on the case where the image is represented by the frozen image embedding extracted by the image encoder in \ours-P. Formally, 
\vspace{-1mm}
\begin{align}
\vect{v} &= E(\vect{I}, \vect{\theta}_{frozen}), \\
r &= F(\vect{v}, \vect{\gamma}),
\end{align}
\noindent where $\vect{I}$ is the input image,  $\vect{v}$ is the image features extracted using image encoder $E$ with its frozen pretrained weights $\vect{\theta}_{frozen}$.  $F$ is the IAA scoring model with parameters $\vect{\gamma}$, and $r$ is the predicted aesthetic score.


During training, given two images represented by $\vect{v}_i$ and $\vect{v}_j$, and their corresponding MOS labels $l_i$ and $l_j$, the IAA model output $r_i$ and $r_j$ are trained to respect the order of $l_i$ and $l_j$. The performance of the proposed model $F$ is evaluated by the correlation between $r$ and $l$.

To obtain an effective $F$ with few parameters, we draw inspiration from the ZSL setting where no parameter tuning is required. Since the cosine similarity between paired image-text is maximized by the contrastive pretraining objective (Eq.~\ref{eq:contrastive_loss}), we can use the cosine similarity between the contrastive image embedding $\vect{v}$ and the text embedding $\vect{w}$ as a measure of how much the image aligns with the textual concept. By using text as ``prompts", we can effectively score images for the textual concept (\emph{e.g.}, whether they are ``good image''). Our preliminary study shows that using text prompts for IAA scoring results in a correlation of over $0.6$, suggesting  that the text decoder in \ours-P contains useful information about what constitutes a visually pleasing image.  We aim to utilize this information as an anchor to further enhance the model's IAA ranking capability by designing a lightweight rank-based adapter module.


\subsection{Rank-based Adapter Module}
 
\noindent The pretraining process, which includes contrastive and generative objectives, captures rich textual concepts related to aesthetically pleasing images in the text decoder, and embeds them in the same latent space as the image. Therefore, we can make slight adjustments to the image embedding to improve its alignment with these textual concepts. Concretely, we propose using the frozen text embedding of ``good image" as an anchor to score images, and optimize the relative ranking between two images according to their MOS labels by adjusting their image representations. This is illustrated in Fig.~\ref{fig:overview} (2).

Let  $\vect{v}$ represent the unnormalized contrastive image embedding from the frozen \ours-P image encoder. To obtain the rank-adjusted image embedding $\Tilde{\vect{v}}$, we add a learnable residual represented by $\vect{H} \in \mathbb{R}^{D\times D}$ and normalize the output as follows:
\vspace{-1mm}
\begin{align}
\Tilde{\vect{v}} = \mathrm{normalize}(\vect{v}^\top \vect{H} + \vect{v}),
\end{align}

Next, we use ``good image" as the prompt, and extract its normalized frozen text embedding $\vect{w}_p$ from the \texttt{[CLS]} position of the unimodal text decoder. The cosine similarity between the rank-adjusted image embedding $\Tilde{\vect{v}}$ and the anchor $\vect{w}_p$ is used as the predicted IAA score for ranking:
\vspace{-1mm}
\begin{align}
r = \Tilde{\vect{v}}^\top \vect{w}_p
\end{align}

To optimize the relative ranking between two images, we use $\vect{w}_p$ as the anchor and optimize the triplet ranking loss $\mathcal{L}_{\text{RA}}$ for a pair of input images:

\vspace{-1mm}
\begin{align}
\mathcal{L}_{\text{RA}} = \frac{1}{P} \sum_{i, j, i\neq j, l_i > l_j } \max \Big(0, m -  \Tilde{\vect{v}}_i^\top \vect{w}_p + \Tilde{\vect{v}}_j^\top \vect{w}_p\Big)
\end{align}

\noindent $m$  is the margin hyper-parameter with default value $0.1$. The positive sample $\Tilde{\vect{v}}_i$ corresponds to the image with a higher MOS label $l_i$, and the negative sample $\Tilde{\vect{v}}_j$ corresponds to the image with a lower MOS label $l_j$. The ranking loss ensures that the  similarity between the positive sample  and the ``good image" anchor is greater than that of the negative sample, effectively ranking the images according to its aesthetic ratings. The only tunable parameter is $\vect{H}$ with $D^2$ parameters, about 0.1\% of the total parameters in \ours-P. 

It is worth noting that the frozen text embedding $\vect{w}_p$ can be exported for training and inference without the text backbone. Therefore, the final IAA model has the same computational and storage as a single image-encoder-only model, and it only needs the image as input for IAA inference.
