\section{Related Work}
\label{sec:related}

\noindent\textbf{Image Aesthetic Assessment} has a wide range of applications such as search, ranking, and recommendation. Unlike the technical quality assessment~\cite{koniq10k,Ying_2020_CVPR,Fang_2020_CVPR} which focuses on image distortion, cropping, or noise, IAA aims to measure the aesthetic quality.
During the deep learning era, works such as \cite{murray2012ava,karayev2013recognizing,tang2013content,Ren_2017_ICCV,lee2018photographic,Yang_2022_CVPR,he_2022_ijcai} focused on data-driven methods and collected large-scale datasets containing images and human ratings.
Based on these datasets, \cite{kong2016photo} built a ranking-based model, while \cite{murray2017deep,talebi2018nima,zeng2019unified} proposed to approximate the groundtruth score distributions. 
Different from these works, our model benefits from the image-text pretraining framework that has rarely been explored in IAA.

Additional supervision in IAA has been explored in works such as \cite{zhou2016joint,wang2019neural}, where natural language annotations were introduced in their curated datasets. However, these methods either treat IAA as one of multiple parallel tasks~\cite{wang2019neural,niu2022comment}, do not generate quality related outputs~\cite{zhou2016joint,wang2019neural}, or require both image and comment at inference time~\cite{zhou2016joint, hii2017multigap, zhang2021mscan}. In contrast, our model leverages user comments to learn meaningful aesthetic representations using contrastive and generative targets, and the learned image model can be used independently without text input.


Moreover, various studies have focused on network design to preserve high-resolution aesthetic information for IAA, such as CNN-based methods \cite{Mai_2016_CVPR,Hosu_2019_CVPR,Chen_2020_CVPR} that reduce the negative effects of cropping and resizing, and transformer architectures \cite{Ke_2021_ICCV,ghosal2022image} that treat input image as visual tokens and support variable-length sequences, preserving image resolution and aspect ratios. Our method achieves state-of-the-art results with a fixed $224\times224$ input without considering original resolution and aspect ratios, and we believe that these related methods could further enhance our model and be incorporated in future work.


\noindent\textbf{Image-Text Pretraining} utilizes the fact that paired image and text are correlated. Initially, contrastive learning was used to draw image representation and aligned text representation closer \cite{frome2013devise,kiros2014unifying,faghri2018vse++}.
Later, self-supervised learning objectives were explored, such as masked region reconstruction, masked object prediction, word region alignment \cite{chen2019uniter,lu2019vilbert,li2019visualbert,su2019vlbert,tan-bansal-2019-lxmert}. These early models used off-the-shelf visual detectors, which limited their generalization to large-scale pretraining. The introduction of ViT~\cite{dosovitskiy2020image} enabled end-to-end multimodal transformer-based methods \cite{kim2021vilt,wang2021vlmo} for large-scale vision-language pretraining. Recently, several methods such as  CLIP~\cite{radford2021learning}, ALIGN~\cite{jia2021scaling}, and CoCa~\cite{yu2022coca} have proposed image-text foundation models trained on large-scale image-text corpus~\cite{jia2021scaling,Zhai_2022_CVPR}. These methods adopted general pretraining using billions of image-text pairs from the web, and showed impressive results on various tasks such as retrieval, classification, and captioning. Concurrent works \cite{hentschel2022clip, wang2022exploring} have shown the benefit of using such generally pretrained CLIP features for aesthetics learning. However, due to the sparsity of aesthetics-related image-text pairs on the web, aesthetic information gets diluted in such general pretraining process. To address this, we propose the aesthetics pretraining on image-comment pairs to further enhance aesthetics information. Our model is based on the CoCa~\cite{yu2022coca} architecture, with a novel rank-based adapter module designed for IAA to learn relative aesthetic quality with minimal tunable parameters. The rank-based adapter optimizes only a small set of learnable parameters, avoiding catastrophic forgetting~\cite{french1999catastrophic,kirkpatrick2017overcoming} while retaining the rich knowledge from the pretrained model.
