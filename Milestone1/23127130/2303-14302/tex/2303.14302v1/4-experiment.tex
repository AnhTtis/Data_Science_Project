\section{Experiments}
\label{sec:results}
\subsection{Datasets}
\label{sec:results:datasets}

\noindent \textbf{LAION-5B-English-Filtered} is a 650M subset from the English split in LAION-5B~\cite{schuhmann2022laion}, which is currently the largest publicly available dataset with 5B CLIP-filtered image-text pairs. The filtered subset is obtained by removing non-informative or bad data, such as poorly formatted text, bad image size or aspect ratio, and poor image content. We use this subset for general image-text pretraining.

\noindent \textbf{AVA Dataset}~\cite{murray2012ava} is a widely-used IAA benchmark originating from the DPChallenge website. It consists of over 250,000 images with user voting scores ranging from 1 to 10. We evaluate the IAA performance of our model on the available 19,928 AVA test images, reporting Spearman rank order correlation coefficient (SRCC) and Pearson linear correlation coefficient (PLCC) metrics.

\noindent \textbf{AVA-Captions}~\cite{ghosal2019aesthetic} dataset is a collection of user comments for the AVA images, crawled from the DPChallenge website, with basic text filtering applied. It contains 230k images and 1.5M captions, with an average of 5 comments per image. To avoid potential data leakage, we strictly follow the official data split of both AVA and AVA-Captions, excluding both test sets from training, resulting in a training dataset with 212,585 images paired with 1.2M captions. We evaluate the aesthetic comment generation quality of our model on 9,361 AVA-Captions test images, reporting BLEU~\cite{papineni2002bleu}, ROUGE~\cite{rouge2004package}, and CIDEr~\cite{vedantam2015cider} scores.

\noindent \textbf{AVA-Style}~\cite{murray2012ava} contains images with 14 photographic style labels. We use the 2,809 testing images to assess the zero-shot aesthetic style classification capability of our pretrained model.
% (11,270 for training and 2,809 for testing) 
% complementary colors, duo tones, hdr, image grain, light on white, long exposure, macro, motion blur, negative image, rule of thirds, shallow dof, silhouettes, soft focus, and vanishing point

\subsection{Implementation Details}
\label{sec:results:details}

% \junjiek{Confirm the parameters number on 224x224 input. Should we add FLOPS?}

\noindent We use CoCa-Base, the smallest variant of CoCa~\cite{yu2022coca}. It contains a ViT-B/16~\cite{dosovitskiy2020image} image encoder with 12 transformer~\cite{vaswani2017attention} layers, hidden dimension $D=768$, and MLP size $3072$. The image resolution is set to $224\times224$ with a patch size of  $16\times 16$, resulting in $K=196$ image tokens. Data augmentation during training includes random horizontal flipping and random cropping from $272\times272$. The unimodal text decoder consists of 6 transformer layers with the same hidden dimension and MLP size, while the multimodal text decoder consists of another 6 transformer layers. The maximum text length is set to $64$ during training.  For LAION pretraining, we train with 4096 batch size for 500k steps, using 5e-4 learning rate with linear decay to zero, and $0.01$ weight decay. For image aesthetic pretraining on AVA-Captions, we train with 128 batch size for 500k steps, using 1e-5 learning rate with linear decay to zero, and $0.04$ weight decay. We set contrastive loss weight $\alpha=1$ and generative loss weight $\beta=2$. A trainable temperature $\tau$ with an initial value of $0.07$ is used for the contrastive loss, following \cite{yu2022coca, jia2021scaling}. To finetune the rank-based adapter on AVA, we train with 128 batch size for 30k steps using 1e-5 learning rate with linear decay to zero, and $0.01$ weight decay. All experiments use the Adafactor~\cite{shazeer2018adafactor} optimizer with $\beta_1=0.9, \beta_2=0.999$, and are conducted on TPUv3.

\subsection{AVA Image Aesthetic Assessment}
\label{sec:results:mos}

\input{Tables/1-mos-prediction}

\noindent \textbf{Comparing to SOTA.} Tab.~\ref{tab:mos_prediction} shows our results on the AVA dataset. The first group shows the baselines including the ranking method \cite{kong2016photo}, distribution matching based approaches \cite{murray2017deep,talebi2018nima,zeng2019unified}, customized neural networks \cite{Hosu_2019_CVPR,Chen_2020_CVPR,Ke_2021_ICCV,ghosal2022image,tu2022maxvit}, and semantic-aware methods \cite{he_2022_ijcai,niu2022comment,hentschel2022clip}.
Our approach \ours{}-R achieves the best performance overall and outperforms the current SOTA GAT$_{\times3}$-GATP~\cite{ghosal2022image} by 1.6\% and 1.3\% in terms of SRCC (0.774 vs 0.762) and PLCC (0.774 vs 0.764), respectively. Moreover, our method uses a lower resolution of 224$\times$224 while other methods may benefit from the larger inputs. For example, MUSIQ~\cite{Ke_2021_ICCV} uses the full-size image and two additional resolutions, yet it  underperforms our model.  Hentschel \etal \cite{hentschel2022clip} utilize frozen CLIP features for learning image aesthetics, and \ours{}-R outperforms their approach, which shows the additional benefit of the proposed aesthetic pretraining.

\noindent \textbf{Zero-shot Learning (ZSL) for IAA.} The second group in Tab.~\ref{tab:mos_prediction} shows the results of using our image-text pretrained model \ours-P (Sec.~\ref{sec:method:image_text}) for zero-shot IAA. We utilize the cosine similarity between the contrastive image and text embeddings for these experiments. In the single prompt setting, we compute the cosine  similarity between the image and a single pair of prompts (``good image", ``bad image"), and use the softmax normalized output for ``good image" as the ZSL score for IAA. For ensemble prompts, we use an average ensemble of six pairs of prompts, each consisting of ``good" or ``bad" plus ``image", ``lighting", ``composition", ``foreground", ``background", and ``content" (see supplementary material). Notably, without any human label supervision, our ZSL model (SRCC 0.657, PLCC 0.663) has already outperformed several supervised baselines such as Kong \etal~\cite{kong2016photo}, NIMA~\cite{talebi2018nima}, and AFDC + SPP~\cite{Chen_2020_CVPR}. These observations demonstrate the potential of leveraging unlabelled user comments for IAA, significantly reducing human labeling costs.

\input{Tables/2-ablation-pretraining}
\input{Tables/3-ablation-adapter}

\noindent \textbf{Effects of image-text pretraining.} Tab.~\ref{tab:ava_captions_pretraining} presents an ablation study to validate the effectiveness of the proposed image-text pretraining. We conduct the general pretraining and aesthetic pretraining on the LAION~\cite{schuhmann2022laion} subset and AVA-Captions~\cite{ghosal2019aesthetic}, respectively. With only the general pretraining, the model has suboptimal performance on the IAA task, verifying the assumption that image aesthetic information gets diluted by the vast amount of unrelated data from the web. Adding aesthetic pretraining greatly improves model performance in both zero-shot and finetuned settings. Both general and aesthetic pretraining have a significant positive impact on the final IAA task predictions. Regardless of the pretraining schema, the proposed rank-based adapter enhances the model's IAA performance with minimally tuned parameters. 


\noindent \textbf{Effectiveness of the proposed rank-based adapter.} Tab.~\ref{tab:abaltion_adapter} shows an ablation study for the proposed rank-based adapter (Sec.~\ref{sec:method:regression_prompts}). We compare  different options for adapting the frozen \ours-P to downstream score-based IAA. The first group shows regression baselines that predict either the single MOS score using a L2 loss or the distribution of MOS scores using EMD loss~\cite{talebi2018nima}. \ours-R outperforms both of them, showing the effectiveness of a rank-based target. In the second group, we ablate the components in the proposed adapter. ``w/o Text Anchor" denotes using a learnable projection to replace the frozen text prompt embedding $\vect{w}_p$. \ours-R performs better, showing the benefit of using the rich text embedding as a ranking anchor. For  ``w/o Residual", we use a simple learnable projection without the residual,~\ie, $\Tilde{\vect{v}} = \mathrm{normalize}(\vect{v}^\top \vect{H)}$. Its sub-par performance confirms the intuition that we only need to slightly adjust the image embedding, thus learning the residual is easier. The final line shows that \ours-R can be further improved with finetuning the image encoder. However, its gain in performance comes at the cost of disturbing the generic pretrained weights, \eg its ZSL performance on AVA-Style drops from 69.0\% to 26.3\% mAP. \ours-R enables effective IAA adaptation while inheriting the pretrained weights.  

\subsection{AVA-Captions Image-Text Pretraining}
\label{sec:results:ava_comments}

\noindent In this section we aim to verify \ours-P model learns meaningful representations that are   generalizable to other tasks. We evaluate its performance on zero-shot style classification and the quality of its generated aesthetic comments.


\noindent \textbf{Zero-shot Style Classification.}
To demonstrate that \ours{}-P captures diverse aesthetic aspects such as composition, color, and style, we evaluate its ZSL performance on the AVA-Style test set. We manually curate text prompts based on the 14 class names, and use the cosine similarities to approximate the probability that an image involves specific styles (see supplementary material). Tab.~\ref{tab:style_prediction} shows the results. The first group contains supervised methods trained on 11k images with style annotations. Without such supervision, \ours-P achieves 69.0\% ZSL mAP,  outperforming many supervised methods such as MNet~\cite{sun2017convolution} (65.5\%) and Lu \etal \cite{lu2015deep} (64.1\%). This demonstrates the ability of the proposed framework to learn open-set aesthetic information without human labelling. Tab.~\ref{tab:style_prediction} also shows that the performance of the model trained only with general pretraining is much lower than that with aesthetic pretraining.  This again verifies that the proposed aesthetic pretraining is necessary for capturing rich aesthetic information.


\input{Tables/4-style-prediction}

\noindent \textbf{AVA Comments Generation.} We evaluate the captioning performance of \ours-P on AVA-Captions test set, and the results are shown in Tab.~\ref{tab:comment_generation}. Our method outperforms CWS~\cite{ghosal2019aesthetic} and Yeo \etal~\cite{yeo2021generating} in terms of BLEU-2, BLEU-3, BLEU-4, ROUGE and CIDEr. Although our method has a slightly lower BLEU-1 than CWS, it is important to note that BLEU-1 only measures precision of unigram, while and higher order BLEU scores (BLEU-2, BLEU-3, BLEU-4) place more emphasis on the fluency of generated sentences. Moreover, our method's superior ROUGE and CIDEr scores indicates that our model generates more semantically  similar sentences to the real user comments.

\input{Tables/5-captioning}

\noindent \textbf{Qualitative Examples.}
To properly credit our image sources, we choose to display images from the KonIQ-10k~\cite{koniq10k} dataset instead of the AVA dataset for illustration in this section. The image sources are provided in supplementary material. Fig.~\ref{fig:bad_vs_good} depicts the top-5 images retrieved by text queries ``Bad photo'' and ``Good photo'' on KonIQ-10k. For ``Bad photo'', the retrieved results exhibit poor  lighting, bad composition and meaningless content. In contrast, the ``Good photo'' group has noticeably better aesthetic quality. These examples provide qualitative evidence of the aesthetic knowledge captured by the pretrained model.

\begin{figure}[t]
    \centering
    \vspace{-2mm}
    \includegraphics[width=1\linewidth]{Figures/good-and-bad.png}
    \caption{Top 5 images retrieved with ``bad photo'', ``good photo'' on KonIQ-10k~\cite{koniq10k}. See supplementary material for image sources.}
    \label{fig:bad_vs_good}
    \vspace{-4mm}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{Figures/ava-styles.png}
    \caption{Top 5 images retrieved using AVA-Style class names on KonIQ-10k~\cite{koniq10k}. To give proper attribution to image sources, we choose to showcase images from the KonIQ-10k dataset instead of the AVA dataset. See supplementary material for image sources.}
    \label{fig:style_qualitative_examples} 
    \vspace{-3mm}
\end{figure}

Fig.~\ref{fig:style_qualitative_examples} illustrates the AVA-Style predictions of \ours{} by visualizing the top-5 images retrieved using style class names on KonIQ-10k. This provides a qualitative demonstration of the aesthetic information captured by \ours{}. Results show that the aesthetic pretraining on image-comment pairs has helped the model to understand low-level aesthetic attributes quite well. For example, the learned model understands that ``Macro'' is a visual concept that captures finer details, regardless of the semantic objects, such as strawberry or insects. Another example is ``HDR'', for which all retrieved photos have high dynamic range while portraying  different semantic objects such as buildings and cars.


Fig.~\ref{fig:caption_qualitative_examples} shows aesthetic comments generated by \ours{}. The model is capable of generating diverse captions conditioned on the images, mentioning attributes such as ``color'', ``saturation'' and ``persepective''. In addition, it even includes critiques about the cropping of the image, which aligns with our aesthetic perspective.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{Figures/captioning.png}
    \vspace{-3mm}
    \caption{Aesthetic comments generated by \ours.}
    \label{fig:caption_qualitative_examples} 
    \vspace{-4mm}
\end{figure}


