\section{Conclusion}
\label{sec:conclusion}
\noindent We propose a general framework for learning image aesthetics (\ours). By pretraining vision-language models on image-comment pairs from image sharing websites, we enable the model to learn rich aesthetic semantics in a self-supervised manner without the need for expensive labeled data. The resulting pretrained model, \ours-P, exhibits state-of-the-art performance on the AVA-Captions dataset and enables various interesting tasks, including zero-shot learning for IAA, style classification, and retrieval. Our experiments demonstrate that \ours{}-P surpasses many supervised baselines on these tasks with ZSL. To efficiently adapt the pretrained model for  IAA without impairing its powerful zero-shot abilities or damaging the rich representation, we introduce a lightweight rank-based adapter module. By employing the text embedding as an anchor and explicitly modeling the ranking concept, we achieve state-of-the-art IAA performance on the AVA dataset with only a small amount of injected parameters. Although we design the rank-based adapter module for IAA, our method is generally applicable for adapting large-scale visual-language models to other ranking based tasks.