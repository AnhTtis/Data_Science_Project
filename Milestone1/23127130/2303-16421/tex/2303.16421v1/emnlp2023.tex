% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{EMNLP2022}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{array}
\usepackage{booktabs}
\usepackage{engord}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{pifont}
\usepackage{amsthm,amsmath,amssymb}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\usepackage{color}
% \usepackage{nidanfloat}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Ning Bian$^{\rm 1,2}$, Xianpei Han$^{\rm 2}$, Le Sun$^{\rm 2}$, Hongyu Lin$^{\rm 2}$, Yaojie Lu$^{\rm 2}$, Ben He$^{\rm 1,2}$\\
  $^{\rm 1}$School of Computer Science and Technology, \\University of Chinese Academy of Sciences, Beijing, China \\
  $^{\rm 2}$Institute of Software, Chinese Academy of Sciences, Beijing, China \\
  \tt bianning21@mails.ucas.ac.cn\\ {\tt \{xianpei,sunle,linhongyu,luyaojie\}@iscas.ac.cn} \\ \tt benhe@ucas.ac.cn\\}

\begin{document}
\maketitle
\begin{abstract}
Large language models (LLMs) such as ChatGPT and GPT-4 have made significant progress in NLP. However, their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point for LLMs. It remains unclear that: 
(1) Can GPTs effectively answer commonsense questions?
(2) Are GPTs knowledgeable in commonsense?
(3) Are GPTs aware of the underlying commonsense knowledge for answering a specific question?
(4) Can GPTs effectively leverage commonsense for answering questions?
To evaluate the above commonsense problems, we conduct a series of experiments to evaluate ChatGPT's commonsense abilities, and the experimental results show that:
(1) GPTs can achieve good QA accuracy in commonsense tasks, while they still struggle with certain types of knowledge.
(2) ChatGPT is knowledgeable, and can accurately generate most of the commonsense knowledge using knowledge prompts.
(3)	Despite its knowledge, ChatGPT is an inexperienced commonsense problem solver, which cannot precisely identify the needed commonsense knowledge for answering a specific question, i.e., ChatGPT does not precisely know what commonsense knowledge is required to answer a question.
The above findings raise the need to investigate better mechanisms for utilizing commonsense knowledge in LLMs, such as instruction following, better commonsense guidance, etc. 
\end{abstract}

\section{Introduction}

Commonsense knowledge is a fundamental aspect of human cognition that includes our intuitive understanding of the world and our ability to reason about it. It includes knowledge about the spatial, physical, social, temporal, and psychological aspects of typical every-day life, as well as an understanding of social norms, beliefs, and values, and the ability to predict and interpret human behavior \cite{liu2004conceptnet}. Commonsense knowledge is crucial for building NLP systems that can understand and generate human-like language. Despite its importance for many tasks, obtaining and representing commonsense knowledge in machines has been a long-standing challenge in NLP \cite{li2021language, zhang2022alleviating}, as commonsense is often implicit and context-dependent  \cite{gordon2013reporting,shwartz2020neural}. In recent years, there has been a growing interest in addressing the commonsense problem of NLP models and enabling more human-like language generation and understanding \cite{bauer2018commonsense, wang2020connecting, jiang2021m, liu2021kg, liu2022generated}. 

\begin{figure}[!t]
  \centering
  \includegraphics[width=\columnwidth]{0.pdf}
  \caption{This paper answers several key questions on the commonsense abilities of LLMs.}
  \label{figure-f0}
\end{figure}

Recently, large language models (LLMs) such as GPT-3 \cite{brown2020language}, ChatGPT, and GPT-4 have achieved remarkable success in a wide range of NLP capabilities, including inference, contextual understanding, and chain-of-thought reasoning \cite{wei2022chain}. These abilities suggest that large language models possess a certain degree of commonsense knowledge \cite{west2022symbolic}. However, the commonsense problem is often regarded as a major limitation of large language models \cite{zhou2020evaluating,bhargava2022commonsense}. As these models become increasingly powerful, it is still unclear to what extent they are capable of understanding and reasoning about commonsense knowledge. This raises several key questions:
(1) \textit{Can GPTs effectively answer commonsense questions?}
(2) \textit{Are GPTs knowledgeable in commonsense?}
(3) \textit{Are GPTs aware of the underlying commonsense knowledge for answering a specific question?}
(4) \textit{Can GPTs effectively leverage commonsense for answering questions?}
Answering these questions is crucial for understanding the capabilities and limitations of LLMs and for developing better methods to evaluate and improve their performance on commonsense tasks.

In this paper, to evaluate models' ability in answering commonsense questions, we use 11 commonsense QA datasets that cover 8 diverse commonsense domains including physical, social, temporal, and numerical reasoning, etc. Firstly, We ask models to answer these questions and evaluate the accuracy of their responses. To evaluate whether large language models have an understanding of the necessary commonsense knowledge for answering these questions, we ask the model to describe the necessary knowledge and evaluate whether the descriptions are accurate. To assess whether large language models can recall and describe the necessary knowledge to answer the question, we ask models whether they know the necessary knowledge and evaluate whether the responses are correct and relevant. Finally, to assess whether the model can leverage commonsense knowledge for reasoning, we use the knowledge generated in the previous experiments as context and ask the model to answer the questions again. We compare their performances to using golden knowledge and evaluate whether their reasoning process can effectively leverage the identified knowledge. 

Our experiments provide insights into the commonsense problem of large language models:
(1) GPTs can achieve good QA accuracy in commonsense tasks, while they still struggle with certain types of knowledge.
(2) ChatGPT is knowledgeable, and can accurately generate most of the commonsense knowledge using knowledge prompts.
(3)	ChatGPT is an inexperienced commonsense problem solver, which cannot precisely identify the needed commonsense knowledge for solving a specific question. Furthermore, ChatGPT cannot leverage commonsense knowledge in context effectively for answering a specific question.
The main contributions of this paper are:

\begin{itemize}[leftmargin=*]
\parskip -0.4ex
\item We investigate the commonsense ability of large language models in detail by conducting experiments to answer 4 key questions.

\item We design a series of experiments to evaluate ChatGPT's ability to memorize, represent and leverage commonsense knowledge, including answering commonsense questions, identifying and generating necessary knowledge, and leveraging commonsense knowledge in its reasoning process.

\item By identifying the strengths and weaknesses of ChatGPT's ability in commonsense knowledge and reasoning, we provide insights into the development of more advanced language models that can effectively leverage and reason about commonsense knowledge.
\end{itemize}


\section{What is Commonsense}

\begin{table*}[t]
\centering
\small
\begin{tabular}{ll|m{320pt}}
\hline
Dataset &
  Domain &
  \multicolumn{1}{c}{Example (Bold texts are the answers)} \\ \hline
CommonsenseQA &
  General &
  Choose your answer to the question: Where are you likely to find a hamburger? \textbf{A. fast food restaurant}, B. pizza, C. ground up dead cows, D. mouth, E. cow circus 
   \\ \hline
OpenBookQA &
  General &
  Choose your answer to the question: If a person walks in the opposite direction of a compass arrow they are walking A. west, B. north, C. east, \textbf{D. south}
   \\ \hline
WSC &
  General &
  Choose subsentence A or B that completes the sentence: The trophy doesn't fit into the brown suitcase because A. the trophy is too small. \textbf{B. the suitcase is too small.} 
   \\ \hline
PIQA &
  Physical &
  Choose one that is correct: \textbf{A. ice box will turn into a cooler if you add water to it.} B. ice box will turn into a cooler if you add soda to it. 
   \\ \hline
Social IQA &
  Social &
  Taylor taught math in the schools after studying to be a teacher for four years. Choose the most suitable answer for the question: What does Taylor need to do before this? \textbf{A. get a certificate}, B. teach small children, C. work in a school 
   \\ \hline
ARC &
  Science &
  Choose your answer to the question: Which technology was developed most recently? \textbf{A. cellular telephone}, B. television, C. refrigerator, D. airplane 
   \\ \hline
QASC &
  Science &
  Choose your answer to the question: What is described in terms of temperature and water in the air? A. storms; \textbf{B. climate}; C. mass; D. seasonal; E. winter; F. density; G. length; H. fluid 
   \\ \hline
HellaSWAG &
  Event &
  Choose your answer to the question: We see a chair with a pillow on it. A. a man holding a cat does curling. B. a man holding a cat starts hitting objects on an item. C. a man holding a cat is wrapping a box. \textbf{D. a man holding a cat sits down on the chair.}
   \\ \hline
NumerSense &
  Numerical &
  a square is a shape with \textless mask\textgreater\ equally lengthed sides. (\textbf{four})
   \\ \hline
ProtoQA &
  Prototypical &
  Use simple words separated by commas to name something in your life that could cause you to lose weight. 
  (\textbf{Eating less, exercising more, stress.}) \\ \hline
MC-TACO &
  Temporal &
  Select all feasible answers for the question: Carl Laemmle, head of Universal Studios, gave Einstein a tour of his studio and introduced him to Chaplin. At what time did Einstein return home? \textbf{A. 8:00 PM}; B. a second later; \textbf{C. a hour later}  \\ \hline
\end{tabular}
\caption{\label{t1}
Domains and examples of the commonsense QA datasets used in this paper.
}
\end{table*}



Commonsense knowledge is ``a huge portion of human experience, encompassing knowledge about the spatial, physical, social, temporal, and psychological aspects of typical everyday life. ''\cite{liu2004conceptnet,brachman2022new}. This type of knowledge is often taken for granted and is typically acquired through years of experience and socialization in a particular culture. To help the reader better understand this concept, we summarize several main categories of commonsense:

\paragraph{General commonsense:} This type of commonsense refers to knowledge that is widely shared and assumed to be true by most people, such as the fact that the sun rises in the east and sets in the west.

\paragraph{Physical commonsense:} This type of commonsense involves knowledge about the physical world, such as the fact that objects fall to the ground when dropped, and that water flows downhill.

\paragraph{Social commonsense:} This type of commonsense involves knowledge about social norms, customs, and practices, such as the idea that it is polite to say "please" and "thank you" when making requests.

\paragraph{Science commonsense:} This type of commonsense involves knowledge about basic scientific concepts and principles, such as the fact that gravity pulls all objects on Earth to Earth's center.

\paragraph{Event commonsense:} This type of commonsense involves knowledge about the sequence of events and the causal relationships between them, such as the fact that if a glass is knocked over, the liquid inside will spill.

\paragraph{Numerical commonsense:} This type of commonsense involves knowledge about numbers, such as the fact that human has two hands and ten fingers.

\paragraph{Prototypical commonsense:} This type of commonsense involves knowledge about typical or prototypical examples of concepts, such as the fact that a swallow is a kind of bird, and a bird has wings.

\paragraph{Temporal commonsense:} This type of commonsense involves knowledge about time, such as the fact that taking an oversea trip requires a longer time than taking a walk.


\section{Can GPTs Effectively Answer Commonsense Questions?}\label{s3}

In this section, we evaluate the performances of LLMs to answer commonsense questions. Specifically, we use 11 commonsense QA datasets covering 8 commonsense domains including general, physical, social, science, event, numerical, prototypical, and temporal. The 11 datasets are CommonsenseQA \cite{talmor2019commonsenseqa}, OpenBookQA \cite{mihaylov2018can}, WSC \cite{levesque2012winograd}, PIQA \cite{bisk2020piqa}, Social IQA \cite{sap2019social}, ARC \cite{clark2018think}, QASC \cite{khot2020qasc}, HellaSWAG \cite{zellers2019hellaswag}, NumerSense \cite{lin2020birds}, ProtoQA \cite{boratko2020protoqa}, and MC-TACO \cite{zhou2019going}. The datasets and their domains, with an example for each dataset, are shown in Table \ref{t1}. 

We sample 100 questions from the development set of each commonsense QA dataset for evaluation except ProtoQA, which has only 52 questions in its development set. We use GPT-3 (davinci), GPT-3.5 (text-davinci-003), and ChatGPT as the large language models for evaluation. For GPT-3, we use 4-shot in-context learning. For GPT-3.5 and ChatGPT, we use zero-shot inference and design prompt templates for different datasets.


\begin{table*}[!t]
\centering
\small
\begin{tabular}{llccc}
\hline
Dataset       & Domain       & GPT-3 & GPT-3.5     & ChatGPT       \\ \hline
CommonsenseQA & General      & 38    & \textbf{81} & 74            \\
OpenBookQA    & General      & 22    & 65          & \textbf{73}   \\
WSC           & General      & 46    & \textbf{78} & \textbf{78}   \\
PIQA          & Physical     & 48    & 77          & \textbf{78}   \\
Social IQA    & Social       & 36    & \textbf{71} & 62            \\
ARC           & Science      & 27    & 88          & \textbf{94}   \\
QASC          & Science      & 25    & \textbf{75} & 74            \\
HellaSWAG     & Event        & 19    & 61          & \textbf{67}   \\
NumerSense    & Numerical    & 45    & 63          & \textbf{79}   \\
ProtoQA       & Prototypical & 67.3  & 84.6        & \textbf{94.2} \\
MC-TACO       & Temporal     & 20    & \textbf{53} & 52            \\ \hline 
\end{tabular}
\caption{\label{t2}
Evaluation results (accuracy) of large language models on commonsense QA datasets.
}
\end{table*}

The results are shown in Table \ref{t2}. From the results, we can see that:

\textbf{GPTs can accurately answer commonsense questions.} We evaluate the performances of different LLMs on 11 commonsense QA datasets. The results in Table \ref{t2} show that both GPT-3.5 and ChatGPT can achieve good performances on most datasets. The best-performed dataset is ARC and ProtoQA, on which ChatGPT achieves 94\% and 94.2\% accuracy. These results demonstrate that large language models like GPT-3.5 and ChatGPT are effective problem solvers for commonsense questions that can provide accurate answers across different types of commonsense questions. 

\textbf{The ability of GPTs to leverage commonsense knowledge can be improved through instruction tuning and human alignment.} We compare the performances of three large language models, GPT-3, GPT-3.5, and ChatGPT in Table \ref{t2}. From the results, we can see that GPT-3.5 and ChatGPT achieve significant improvements compared with GPT-3. These results show that pre-training alone is insufficient for learning to leverage knowledge. By incorporating instruction and alignment tuning techniques \cite{ouyang2022training}, models can better leverage and reason about commonsense knowledge. 

Overall, ChatGPT achieves higher accuracy than GPT-3.5 in most domains, demonstrating the effectiveness of the RLHF technique in enhancing knowledge-leveraging ability. However, GPT-3.5 slightly outperforms ChatGPT on certain datasets including CommonsenseQA and Social IQA. This is because ChatGPT tends to be cautious and refuses to give answers when there is insufficient information, resulting in outputs such as ``\textit{Based on the information given, it is not possible to determine ...}'' This highlights the need for further research on how to balance the caution and accuracy in models where there is inadequate information. Achieving this requires the model to understand the necessary knowledge to answer the question and be aware of whether the model possesses that knowledge.

\textbf{Although GPTs perform well on most commonsense knowledge domains, they still struggle with certain types of knowledge.} The experiments in Table \ref{t2} show that large language models lag behind in social, event, and temporal commonsense QA (Social IQA, HellaSWAG, and MC-TACO datasets). The ChatGPT's performances of these datasets are below 70\%. This shows that LLMs still have drawbacks on the social, event, and temporal commonsense knowledge. We believe this is because these types of commonsense knowledge require a deeper understanding of human behavior and social interactions, and they appear infrequently in text corpora. This suggests that current LLMs need to be improved on these domains of commonsense, which requires models to go beyond superficial semantic understanding and learn about human behaviors.


\section{Are GPTs Aware of the Commonsense Knowledge for Answering a Question?}\label{s4}
In Section \ref{s3}, we found that GPTs perform well on commonsense QA datasets. This intrigues us to explore whether GPTs are experienced experts that are aware of what knowledge is needed and can leverage the knowledge for question answering, or if they are inexperienced problem solvers that rely on memorizing a large amount of information that covers the questions. 

To answer this question, we sample 20 questions from each commonsense QA dataset and ask ChatGPT ``What knowledge is necessary for answering this question?''. For datasets that have $\geq$10 wrong answered questions, we sample 10 correct and 10 wrong answered questions, otherwise, we take all wrong answered questions and sample more correct answered questions to fill up the 20 questions. 

We manually evaluate the precision and recall of each generated response and use the F1 score as the measurement. For example, Table \ref{t3} shows a response of ChatGPT that describes the necessary knowledge for answering the question. From the ChatGPT response, we label knowledge 1 and 3 as useful for answering the question, while knowledge 2 is overgeneralized and knowledge 4 and 5 are not necessary. Then we judge whether the question is answerable based on these 2 pieces of knowledge, and further write 2 additional pieces of necessary knowledge in human evaluation, knowledge a and b. Thus, the precision for this response is $2/5$ and the recall is $2/4$, and the F1 score is $44.44\%$.

\begin{table}[!t]
\centering
\small
% \resizebox{0.49\textwidth}{!}{
\begin{tabular}{|m{200pt}|}
\hline
% \specialrule{0em}{0pt}{3pt}
\multicolumn{1}{|l|}{
    \tabincell{m{200pt}}{ 
    \specialrule{0em}{0pt}{3pt}
    \textbf{Question}: How to make outdoor pillow? \\
 A. Blow into \emph{tin can} and tie with rubber band. \ding{55} \\
 B. Blow into \emph{trash bag} and tie with rubber band. \ding{51}
    }} \\
\multicolumn{1}{|l|}{
    \tabincell{m{200pt}}{ 
    \specialrule{0em}{0pt}{3pt}
    \textbf{ChatGPT Response}: \\
			1. Understanding of what an outdoor pillow is. \ding{51}\\
			2. Knowledge of how to make an outdoor pillow.\\
			3. Familiarity with the materials required to make an outdoor pillow. \ding{51}\\
			4. Understanding of how to inflate a container such as a tin can or trash bag.\\
			5. Knowledge of how to tie a rubber band.\\
    }} \\
\multicolumn{1}{|l|}{
    \tabincell{m{200pt}}{ 
    \specialrule{0em}{0pt}{3pt}
    \textbf{Human Evaluation}: \\
			1. Understanding of what an outdoor pillow is. \\
			3. Familiarity with the materials required to make an outdoor pillow. \\
			a. Knowing that tin can is hard and trash bag is soft.\\
			b. Knowing that human prefers soft pillows.
    }} \\
\hline
\end{tabular}
\caption{\label{t3}
An example of necessary knowledge generated by ChatGPT and human evaluation. The question is from the PIQA dataset.
}
\end{table}

 
\begin{table*}[!t]
\centering
\small
\begin{tabular}{llccc}
\hline
Dataset       & Domain       & Correct              & Wrong                 & Overall\\ \hline
CommonsenseQA & General      & 65.83 / 94.17 / 75.86    & 50.00 / 72.50 / 57.79     & 57.92 / 83.33 / 66.82\\
OpenBookQA    & General      & 80.50 / 100.00 / 87.94   & 35.83 / 55.83 / 42.81     & 58.17 / 77.92 / 65.37\\
WSC           & General      & 80.00 / 87.50 / 83.21    & 57.50 / 83.33 / 65.90     & 68.75 / 85.12 / 74.56\\
PIQA          & Physical     & 60.00 / 80.00 / 64.90    & 53.36 / 88.33 / 63.25     & 56.78 / 84.17 / 64.08\\ 
Social IQA    & Social       & 53.00 / 90.00 / 63.43    & 28.17 / 40.00 / 32.05     & 40.58 / 65.00 / 47.74\\
ARC           & Science      & 73.57 / 100.00 / 82.80   & 45.00 / 83.33 / 55.36     & 65.00 / 95.00 / 74.57\\
QASC          & Science      & 67.17 / 100.00 / 78.79   & 68.33 / 88.33 / 73.48     & 67.75 / 94.17 / 76.13\\
HellaSWAG     & Event        & 64.00 / 95.00 / 74.10    & 47.55 / 73.00 / 57.31     & 55.77 / 84.00 / 65.70\\
NumerSense    & Numerical    & 44.00 / 95.00 / 58.29    & 44.00 / 89.17 / 58.21     & 44.00 / 92.08 / 58.25\\
ProtoQA       & Prototypical & 65.88 / 98.04 / 76.96    & 48.33 / 88.89 / 58.73     & 63.25 / 96.67 / 74.23\\
MC-TACO       & Temporal     & 47.50 / 80.00 / 58.00    & 26.17 / 61.67 / 35.57     & 36.83 / 70.83 / 46.79\\ \hline
\end{tabular}
\caption{\label{t4}
Precision / Recall / F1 scores of ChatGPT generated necessary knowledge for correct- and wrong-answered questions.
}
\end{table*}


The results are shown in Table \ref{t4}. From the results, we can see that:

\textbf{ChatGPT is an inexperienced problem solver, which struggles to precisely identify the necessary commonsense knowledge to answer a specific question.} We manually evaluate the necessary knowledge generated by ChatGPT and the results are shown in Table \ref{t4}. The results show that the overall F1 scores range from 40\% to 75\% on most of the commonsense QA datasets. These results demonstrate that ChatGPT is an inexperienced problem solver and cannot accurately identify the necessary knowledge to answer a specific commonsense question. 

Specifically, the model performs relatively well in the science domain, achieving 74.57\% F1 on ARC and 76.13\% on QASC. However, the model exhibits the lowest performances on social and temporal domains, i.e., Social IQA and MC-TACO. This discrepancy in F1 scores is likely because scientific commonsense knowledge is more prevalent in the text corpus than social and temporal knowledge. For instance, textbooks frequently discuss scientific concepts such as ``\textit{climate is described by temperature and humidity}'', but rarely mention social norms like ``\textit{students don't like taking big exams}'' or temporal knowledge such as ``\textit{it doesn't take months to eat a single meal}''. This suggests that large language models like ChatGPT still have limitations in social and temporal domains. As a result, it is important to develop more effective training strategies to inject these domains of commonsense knowledge.

\textbf{GPTs cannot effectively distinguish between relevant and irrelevant commonsense knowledge for answering a specific question, which usually generate knowledge with a high noise rate.} We evaluate the precision and recall of the generated necessary knowledge in Table \ref{t4}. The average recall of all datasets is 84.42\% and the average precision is 55.88\%. This shows that while the model can identify most of the commonsense knowledge for questions, it struggles with accurately identifying which pieces of knowledge are essential for answering specific commonsense questions. The model usually generates knowledge that is irrelevant or overgeneralized. For example, knowledge 2 in Table \ref{t3} is overgeneralized because the question itself is ``how to make outdoor pillow'', and knowledge 4 and 5 are irrelevant given the difference between the two choices. We believe this is because the model relies on keyword and topic matching, rather than a full understanding of the logical relationships within the question. This further demonstrates that GPTs are still inexperienced problem solvers. Therefore, LLMs need to increase their self-awareness of the task at hand and distinguish crucial information from irrelevant background information. For example, they need to be able to determine which parts of a question are necessary for decision-making, such as ``blow into tin can/trash bag'' in Table \ref{t3}, and which parts are extraneous.


\begin{figure}[!t]
  \centering
  \includegraphics[width=\columnwidth]{1.pdf}
  \caption{Correlation between overall necessary knowledge F1 and the answer accuracy of ChatGPT.}
  \label{figure-f1}
\end{figure}

\textbf{The commonsense ability of GPTs can be significantly improved by enhancing their knowledge-awareness, i.e., guiding the models to identify the necessary knowledge for a specific question.} In Figure \ref{figure-f1} we plot and analyze the correlations between the overall F1 scores of generated necessary knowledge and the answer accuracies of ChatGPT. The results demonstrate a strong correlation between accuracy and knowledge F1, with a Pearson coefficient of 0.77. Furthermore, Table \ref{t4} shows that the knowledge F1 scores for correctly answered questions are significantly higher than those for incorrectly answered questions. These findings suggest that accurately identifying necessary knowledge is crucial for correctly answering commonsense questions. Consequently, enhancing the model's self-awareness of necessary knowledge has the potential to improve its performance on downstream tasks including commonsense QA.




\section{Are GPTs Knowledgeable in Commonsense?}

This section answers the question: To what extent do large language models possess commonsense knowledge? To answer this question, similar to \citet{shwartz2020unsupervised}, we manually construct knowledge-querying prompts based on the generated necessary knowledge in Section \ref{s3}. For example, as shown in Table \ref{t5}, based on knowledge 1 in Table \ref{t3}, we write the question as ``Do you have an understanding of what an outdoor pillow is?''. We input the prompts into ChatGPT and manually label each generated knowledge description by ChatGPT as correct or incorrect. 

Table \ref{t5} shows an example of a knowledge-querying question and the generated knowledge description. The first description says ``blowing into a trash bag and tying it with a rubber band may create a cushion-like surface, but it is unlikely to be durable or comfortable for prolonged use as an outdoor pillow'', but in reality, it is a typical practice. So, this description is labeled as incorrect.

The results are shown in Table \ref{t6}. From the results, we can see that:

\textbf{GPTs are knowledgeable and contain most of the commonsense knowledge for accurately answering questions.} By asking knowledge-querying questions and manually evaluating the response of ChatGPT, we evaluate the accuracy of necessary knowledge in ChatGPT. The results in Table \ref{t6} shows that the generated knowledge descriptions of ChatGPT can achieve over 70\% accuracy on most commonsense QA datasets, achieving an average accuracy of 82.66\%. This shows that ChatGPT can generate accurate commonsense knowledge descriptions given knowledge-querying questions. This suggests that LLMs can serve as commonsense knowledge bases and provide commonsense for downstream tasks. However, the accuracy is low in the social domain, at 54.92\%. We believe this is because social commonsense, such as ``The person who receives help, rather than gives it, should say thank you'', is not commonly described in texts. This highlights the importance of developing specific guidance that can instruct ChatGPT in acquiring social commonsense knowledge.

\textbf{GPTs contain misleading and overgeneralized commonsense knowledge.} To assess the relevance and informativeness of the knowledge descriptions on the CommonsenseQA dataset, we conducted a manual evaluation. The results show that 26.25\% of the descriptions include irrelevant and misleading information, and 15.00\% of the descriptions are overgeneralized and fail to provide the specific knowledge necessary to answer the question. For example, the description in Table \ref{t5} mentions ``exploring other methods or materials that are specifically designed for making outdoor pillows'', which is unhelpful and misleading for answering the question. We believe this is because there is noisy and redundant information in the training data of large language models, which impairs the ability to accurately judge the relevance of information. These findings emphasize the need for ChatGPT to generate knowledge descriptions that are relevant and informative to ensure that the generated results have practical value for answering the question.

\textbf{There is a gap between knowing and leveraging commonsense knowledge in LLMs.} To explore the relationship between the accuracy of generated knowledge descriptions and answer accuracy, we conducted a correlation analysis and plot the result in Figure \ref{figure-f2}. Our result reveals a weak correlation between the two accuracies, with a Pearson coefficient of 0.40. Notably, while the answer accuracies are low on both the Social IQA and the MC-TACO datasets, there was a significant difference in the accuracy of knowledge descriptions between them: it was low for Social IQA (54.92\%) but high for MC-TACO (86.25\%). Table \ref{t6} further shows that the difference in description accuracy between correctly and incorrectly answered questions is relatively small compared to the results in Section \ref{s4}. This shows that a good knowledge description does not necessarily translate to a correct answer. We believe this is because answering commonsense questions not only requires knowledge but also other abilities like reasoning and making inferences under conditions of insufficient information. 

\begin{table}[!t]
\centering
\small
% \resizebox{0.49\textwidth}{!}{
\begin{tabular}{|m{200pt}|}
\hline
% \specialrule{0em}{0pt}{3pt}

\multicolumn{1}{|l|}{
    \tabincell{m{200pt}}{ 
    \specialrule{0em}{0pt}{3pt}
    \textbf{Question 1}: Do you have an understanding of what an outdoor pillow is? \\
	\textbf{ChatGPT Response}: \\
\ \ As an AI language model, I have an understanding that an outdoor pillow is a type of pillow that is designed to be used outside, such as on a patio or deck, and is typically made of weather-resistant materials to withstand exposure to the elements.

\ \ Regarding the two options you provided, neither option appears to be a conventional method for making an outdoor pillow. In option A, blowing into a tin can and tying it with a rubber band does not create a cushion-like surface suitable for use as a pillow. In option B, blowing into a trash bag and tying it with a rubber band may create a cushion-like surface, but it is unlikely to be durable or comfortable for prolonged use as an outdoor pillow.

\ \ Therefore, I would suggest exploring other methods or materials that are specifically designed for making outdoor pillows, such as using weather-resistant fabrics and stuffing materials.
    }} \\

\multicolumn{1}{|l|}{
    \tabincell{m{200pt}}{ 
    \specialrule{0em}{0pt}{3pt}
    ......
    }} \\

%\multicolumn{1}{|l|}{
%    \tabincell{m{200pt}}{ 
%    \specialrule{0em}{0pt}{3pt}
%    \textbf{Question 3}: Are you familiar with the materials required to make an outdoor pillow? \\
%	\textbf{ChatGPT Response}: \\
%\ \ Yes, as an AI language model, I am familiar with the materials that can be used to make outdoor pillows. Since outdoor pillows are designed to withstand exposure to the elements, it's important to choose materials that are weather-resistant and durable. Some common materials used for making outdoor pillows include:
%
%\ \ Outdoor Fabric: This is a fabric that is specially designed for outdoor use. It is usually made of synthetic materials like acrylic or polyester and is treated to be resistant to water, mildew, and fading.
%
%\ \ Pillow Inserts: For the filling of the pillow, you can use outdoor-rated foam or fiberfill that is resistant to mold and mildew. These types of inserts are specifically designed to withstand exposure to moisture and other outdoor elements......
%    }} \\
%
%\multicolumn{1}{|l|}{
%    \tabincell{m{200pt}}{ 
%    \specialrule{0em}{0pt}{3pt}
%    ......
%    }} \\
\hline
\end{tabular}
\caption{\label{t5}
An example of generated knowledge by ChatGPT. The question is in Table 3.
}
\end{table}

\begin{table}[!t]
\centering
\small
\begin{tabular}{lccc}
\hline
Dataset       & Correct & Wrong     & Overall\\ \hline
CommonsenseQA & 100.00   & 83.83     & 91.92\\
OpenBookQA    & 84.83   & 100.00     & 92.42\\
WSC           & 90.00   & 74.17     & 82.08\\
PIQA          & 85.00   & 62.14     & 73.57\\
Social IQA    & 58.33   & 51.50     & 54.92\\
ARC           & 91.67   & 97.62     & 95.83\\
QASC          & 88.33   & 89.17     & 88.75\\
HellaSWAG     & 80.00   & 70.83     & 75.42\\
NumerSense    & 85.17   & 84.50     & 84.83\\
ProtoQA       & 80.29   & 100.00     & 83.25\\
MC-TACO       & 95.00   & 77.50     & 86.25\\ \hline
\end{tabular}
\caption{\label{t6}
Accuracies of ChatGPT generated knowledge descriptions for correct- and wrong-answered questions.
}
\end{table}

\begin{figure}[t!]
  \centering
  \includegraphics[width=\columnwidth]{2.pdf}
  \caption{Correlation between overall generated knowledge accuracy and the answer accuracy of ChatGPT.}
  \label{figure-f2}
\end{figure}


\section{Can GPTs Effectively Leverage Commonsense in Context for Reasoning?}

This section answers the question: Can large language models leverage commonsense knowledge in context for reasoning and answering questions? To answer this question, after answering the knowledge-querying questions in Section \ref{s4}, we ask the model to answer the commonsense questions again, and evaluate whether the answers change after describing the necessary knowledge. Table \ref{t7} shows an example that a previously incorrect answer remains unchanged after generating knowledge descriptions.

\begin{table}[!t]
\centering
\small
% \resizebox{0.49\textwidth}{!}{
\begin{tabular}{|m{200pt}|}
\hline
% \specialrule{0em}{0pt}{3pt}
\multicolumn{1}{|l|}{
    \tabincell{m{200pt}}{ 
    \specialrule{0em}{0pt}{3pt}
    \textbf{Question}: How to make outdoor pillow? \\
 A. Blow into \emph{tin can} and tie with rubber band. \ding{55} \\
 B. Blow into \emph{trash bag} and tie with rubber band. \ding{51}
    }} \\
\multicolumn{1}{|l|}{
    \tabincell{m{200pt}}{ 
    \specialrule{0em}{0pt}{3pt}
	\textbf{ChatGPT Answer before Knowledge Description}: \\
Neither option is a good way to make an outdoor pillow. Blowing into a tin can or a trash bag and tying it with a rubber band will not provide adequate support or comfort for an outdoor pillow. Over time, the air will leak out and the pillow will become flat and uncomfortable. A better option for making an outdoor pillow would be to use durable and weather-resistant materials such as outdoor fabric and stuffing or foam designed for outdoor use. Sewing or using fabric glue to attach the fabric together and stuffing it with the appropriate material will result in a more functional and comfortable outdoor pillow.
    }} \\
\multicolumn{1}{|l|}{
    \tabincell{m{200pt}}{ 
    \specialrule{0em}{0pt}{3pt}
	\textbf{ChatGPT Answer after Knowledge Description}: \\
Neither of these options is a suitable way to make an outdoor pillow. Inflating a tin can or a trash bag and tying it with a rubber band would not provide the necessary level of firmness or support for a pillow, and the materials used may not be durable enough to withstand exposure to outdoor elements.

A better option for making an outdoor pillow would be to use weather-resistant fabric and outdoor-rated stuffing or foam inserts, as I mentioned earlier. This will ensure that the pillow is comfortable, durable, and able to withstand exposure to the elements.
    }} \\


\hline
\end{tabular}
\caption{\label{t7}
An example of the answer before and after knowledge descriptions by ChatGPT. The question is from the PIQA dataset.
}
\end{table}

The results are shown in Table 8. From the results, we can see that:

\textbf{ChatGPT cannot effectively leverage the generated commonsense knowledge if we only add them to the question context.} Our analysis of answer changes before and after generating knowledge descriptions shows that in most datasets there are no obvious and consistent accuracy improvements after using the generated commonsense descriptions in contexts. There are both wrong-to-correct changes and correct-to-wrong changes, as well as a significant proportion of unchanged answers, for the example shown in Table \ref{t7}. In the case of the Social IQA dataset, the accuracy of knowledge generation was low, leading to more correct answers being modified to become wrong. This shows that ChatGPT cannot effectively exploit its own generated knowledge descriptions to answer questions, and the accuracy of the knowledge generation has a huge impact on the answering results. We believe this is because the model already possesses the generated knowledge, thus adding redundant knowledge is not useful.

\textbf{ChatGPT's performance improvement in commonsense QA is not significant even using golden knowledge.} We use two human-annotated commonsense explanation datasets for the CommonsenseQA dataset, CoS-E \cite{rajani2019explain} and ECQA \cite{aggarwal2021explanations}, as the golden knowledge as context and ask the ChatGPT to generate the answers. We discover that there are only 4 wrong $\to$ correct answers given CoS-E explanations, and 8 wrong $\to$ correct answers given ECQA explanations while with one correct $\to$ wrong answer. This shows that ChatGPT cannot answer all questions correctly even given the golden knowledge explanations. We believe this is because large language models lack the ability to use knowledge for complex commonsense reasoning, such as negation. For example, here is a question that requires reasoning of negation: ``\textit{What would not be true about a basketball if it had a hole in it but it did not lose its general shape? A. punctured, B. popular in America, \textbf{C. full of air}, D. gone, E. round}''. The CoS-E explanation for this question is ``\textit{Air cannot stay in any object that has a hole in it.}'', but ChatGPT still predicts the wrong answer A and explains ``\textit{If a basketball has a hole in it, it is punctured and air can escape from it.}''. These results suggest that large language models require further guidance and improvement to better leverage and reason about commonsense knowledge in context.

\begin{table}[!t]
\centering
\small
\begin{tabular}{lcc}
\hline
Dataset       & C $\to$ W &W $\to$ C  \\ \hline
CommonsenseQA & 1/10   & 2/10     \\
OpenBookQA    & 0/10   & 5/10     \\
WSC           & 2/10   & 1/10     \\
PIQA          & 1/10   & 1/10     \\
Social IQA    & 5/10   & 1/10     \\
ARC           & 0/14   & 2/6     \\
QASC          & 1/10   & 4/10     \\
HellaSWAG     & 3/10   & 4/10     \\
NumerSense    & 4/10   & 4/10     \\
ProtoQA       & 3/17   & 2/3    \\
MC-TACO       & 4/10   & 2/10     \\ \hline
\end{tabular}
\caption{\label{t8}
Changes of answers after the generation of knowledge descriptions. C $\to$ W means a correct answer changes to a wrong answer, and W $\to$ C means a wrong answer changes to a correct answer.
}
\end{table}

\section{Related Work}

Recent studies have shown that large language models such as GPT-3, ChatGPT, and GPT-4 have made significant progress in natural language processing (NLP) tasks, including question answering, text generation, and language translation \cite{brown2020language}. However, there is a growing concern about their ability to understand and reason about commonsense knowledge \cite{tamborrino2020pre,cui2021commonsense,bhargava2022commonsense}. Several recent studies have focused on evaluating the ability of large language models to understand commonsense knowledge \cite{davison2019commonsense,liu2020commonsense,niu2021semantic,ma2021exploring,klein2021towards}. For example, \citet{zhou2020evaluating} evaluates several LLMs on a set of commonsense reasoning tasks and found that these models have a certain degree of commonsense knowledge, but there is still a gap between models and humans. \citet{wang2021language} studies the generalizability of language models for commonsense inference and found that the ability relies heavily on whether the objects to predict are seen during training.
In this paper, we evaluate the commonsense abilities of LLMs including answering commonsense questions, identifying and generating necessary knowledge, and leveraging commonsense knowledge for reasoning.


\section{Conclusions and Discussions}

In this paper, we investigate the commonsense abilities of large language models and found that ChatGPT is a knowledgeable but inexperienced problem solver: (1) While GPTs can achieve good accuracy in commonsense QA, they still struggle with certain types of knowledge, including social and temporal commonsense. (2) ChatGPT is knowledgeable in commonsense, which can accurately generate most of the commonsense knowledge using knowledge prompts. (3) ChatGPT is an inexperienced commonsense problem solver. It struggles to precisely identify the underlying commonsense knowledge for a given question, and often generates knowledge with a high noise rate. Furthermore, ChatGPT cannot effectively leverage commonsense knowledge in contexts to answer commonsense questions.

The above findings raise several promising directions for future work:

(1) Although current GPTs are knowledgeable, they are still not experienced problem solvers. Therefore, it is critical to investigate better mechanisms for utilizing commonsense knowledge in LLMs, such as instruction tuning, better commonsense-guided reasoning, etc.

(2) There are still several types of commonsense knowledge missing in LLMs, such as social and temporal commonsense. Therefore it is critical to design knowledge injection approaches for these knowledge types. Furthermore, it is important to design lightweight commonsense updating methods to keep the knowledge up-to-date.

(3) Because ChatGPT and GPT-4 do not release their full details, such as training data, hyper-parameters, and checkpoints, and it is very different to evaluate an ``artificial general intelligence'' model, it is crucial to design and construct benchmarks with wider coverage, and design evaluation methods that provide a more comprehensive and unbiased assessment of large language models.


% Entries for the entire Anthology, followed by custom entries
\bibliography{custom}
\bibliographystyle{acl_natbib}


\end{document}
