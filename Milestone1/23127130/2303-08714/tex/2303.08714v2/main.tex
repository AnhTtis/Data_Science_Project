\documentclass[10pt,twocolumn,letterpaper]{article}
\pdfoutput=1
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{stfloats}
\usepackage{multirow}
\usepackage{setspace}

\usepackage{caption}
\captionsetup[table]{skip=5pt}
\captionsetup{skip=-7pt}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{11200} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE


\title{ResDiff: Combining CNN and Diffusion Model for Image Super-Resolution} 

\renewcommand{\thefootnote}{\fnsymbol{counter variable}}

\author{
% Shuyao Shang\\
% Shandong University\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
Shuyao Shang\\
Shandong University\\
{\tt\small 202000800098@mail.sdu.edu.cn}
\and
Zhengyang Shan\\
Shandong University\\
{\tt\small 202000800128@mail.sdu.edu.cn}\\
\and
Guangxing Liu\\
Shandong University\\
{\tt\small 202000800141@mail.sdu.edu.cn}\\
\and
Jinglin Zhang\footnote{Corresponding author.}\\
Shandong University\\
{\tt\small jinglin.zhang@sdu.edu.cn}\\
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT


\begin{abstract}
Adapting the Diffusion Probabilistic Model (DPM) for direct image super-resolution is wasteful, given that a simple Convolutional Neural Network (CNN) can recover the main low-frequency content. Therefore, we present \textbf{ResDiff}, a novel \textbf{Diff}usion Probabilistic Model based on \textbf{Res}idual structure for Single Image Super-Resolution (SISR). ResDiff utilizes a combination of a CNN, which restores primary low-frequency components, and a DPM, which predicts the residual between the ground-truth image and the CNN-predicted image. In contrast to the common diffusion-based methods that directly use LR images to guide the noise towards HR space, ResDiff utilizes the CNN's initial prediction to direct the noise towards the residual space between HR space and CNN-predicted space, which not only accelerates the generation process but also acquires superior sample quality. Additionally, a frequency-domain-based loss function for CNN is introduced to facilitate its restoration, and a frequency-domain guided diffusion is designed for DPM on behalf of predicting high-frequency details. The extensive experiments on multiple benchmark datasets demonstrate that ResDiff outperforms previous diffusion-based methods in terms of shorter model convergence time, superior generation quality, and more diverse samples.
\end{abstract}

%%%%%%%%% BODY TEXT



\section{Introduction}

Single Image Super-Resolution (SISR) is a difficult task in computer vision, which aims to recover high-resolution (HR) images from their low-resolution (LR) counterparts. During image degradation, the high-frequency components are lost, and multiple HR images could produce the same LR image, making this task ill-posed. After Generative Adversarial Networks(GAN) \cite{gan} was proposed, the main generative-model-based SISR methods are GAN-driven. However, GAN-based methods are hard to train and prone to fall into pattern collapse, causing a lack of diversity. Therefore, a superior generative model is required in the SISR task.

\begin{figure}[t]
\begin{center}
\includegraphics[width=8 cm]{fig/Compare.pdf}
\end{center}
\caption{\textbf{Subplot a:} Generated high-resolution samples by our ResDiff ($512 \times 512 \to 2048 \times 2048$). \textbf{Subplot b:} Face super-resolution test results ($4\times$). Compared to SR3, ResDiff generates superior texture details, such as the baby's eyelids in the first row and the woman's eyemazing in the third row.}
\label{fig:compare}
\end{figure}

Diffusion Probabilistic Model (DPM) has already demonstrated impressive capabilities in image synthesis \cite{imagen, dpm_gen, latent-diff, unclip} and image restoration \cite{ILVR,DDRM,DDNM}. It has also shown promising prospects in SISR tasks \cite{sr3,srdiff}. However, current Diffusion-based methods for SISR, such as SR3\cite{sr3}, generate HR images directly from random noise, and LR images are only used as conditional input to the diffusion process (Fig.\ref{fig:intro_compare} 
(a)). Consequently, the diffusion model needs to recover both the high and low-frequency contents of the image, which not only prolongs the convergence time but also inhibits the model from focusing on the fine-grained information, potentially missing texture details. Li et al.\cite{srdiff} had taken this into account but employed only a bilinear interpolation for the initial prediction, which, compared to CNN, failed to restore sufficiently low-frequency contents and was incapable of generating any high-frequency components in the initial prediction (Fig.\ref{fig:intro_compare} (b)). Similarly, whang et al.\cite{deblur} designed a random-sampler and a deterministic-predictor to tackle this problem. However, there is no information interaction between the random-sampler and the deterministic-predictor, resulting in the latter not functioning to its full potential (Fig.\ref{fig:intro_compare} (c)).

\begin{figure}[t]
\begin{center}
\includegraphics[width=7.5 cm]{fig/overall_structure.pdf}
\end{center}
\caption{Overall struture of proposed ResDiff.}
\label{fig:overall}
\end{figure}

Inspired by the above \cite{srdiff,deblur}, we propose ResDiff, a residual-structure-based diffusion model. Unlike \cite{srdiff}, ResDiff utilizes a CNN for initial prediction. And in contrast to \cite{deblur}, the CNN in ResDiff is pre-trained, thus capable of restoring the major low-frequency components and partial high-frequency components. The initial prediction of the CNN is adopted to guide the random noise towards the Res Space (i.e., the residual space between the Ground Truth image and the CNN predicted image). Compared to the methods that only use LR space as guidance, ResDiff can leverage additional information and generate richer high-frequency details. (Fig.\ref{fig:intro_compare} (d)). Fig.\ref{fig:overall} presents the structure of ResDiff. The CNN used in ResDiff contains a limited number of parameters. Thus, two more loss functions are introduced to strengthen its recovery capabilities. To further enhance the generation quality, we design a Frequency Domain-guided Diffusion (FD-guided Diffusion) as shown in  Fig.\ref{fig:intro_compare} (d) where the high-frequency space also guides the generation process. FD-guided Diffusion consists of two novel modules. The first is a Frequency-Domain Information Splitter (FD Info Splitter) that separates high-frequency and low-frequency contents and performs adaptive denoising on the noisy image. The second is a high-frequency guided cross-attention module (HF-guided CA) that helps the diffusion model predict high-frequency details.

Experiments on two face datasets (FFHQ and CelebA) and two general datasets (Div2k and Urban100) demonstrate that ResDiff not only accelerates the model's convergence speed but also generates more fine-grained images, as shown in Fig.\ref{fig:compare}. To verify the generalization of our method, more experiments on different types of datasets are given in the supplementary material.

Our contributions can be summarized as follows:

• \textbf{Shorter Convergence Time}: We have designed ResDiff, a residual structure-based diffusion model for the SISR task that leads to an apparent improvement in convergence speed compared to other diffusion-based methods.

• \textbf{Superior Generation Quality}: We have introduced FD-guided Diffusion to enhance the diffusion model's concentration on high-frequency details, resulting in superior generation quality.

• \textbf{More Diverse Output}: Experiments have demonstrated that ResDiff holds a lower perceptual-based evaluation value, indicating our method is capable of producing diverse samples.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Works}

Generative-model-based methods have created great success in SISR, which can be classified into GAN-based\cite{srgan,esrgan,DPSRGAN,SFT-GAN,ranksrgan}, flow-based\cite{srflow,Conditional-Flow}, and fiffusion-based\cite{sr3,srdiff} methods. 

\vspace{-1em}

\paragraph{GAN-based methods} Ledig et al.\cite{srgan} proposed SRGAN, which employs a perceptual loss function to generate high-quality images. Similarly, Kim et al.\cite{esrgan} introduced ESRGAN, which adopted an enhanced super-resolution GAN and a superior loss function to improve the perceptual quality. GAN-based methods combine content losses with adversarial losses, allowing them to generate sharp edges and richer textures. However, they are prone to mode-collapse, which decreases diversity in the generated SR samples. Moreover, training GANs is challenging and may lead to unexpected artifacts in the generated image.

\vspace{-1em}

\paragraph{Flow-based methods} Lugmayr et al.\cite{srflow} proposed SRFlow, which is a flow-based method that learns the conditional distribution of high-resolution images given their low-resolution counterparts, enabling high-quality image super-resolution with natural and diverse outputs. Flow-based methods map HR images to flow-space latents using an invertible encoder and connect the encoder and decoder with an invertible flow module, which avoids training instability but requires higher training costs and provides lower perceptual quality. 

\vspace{-1em}

\paragraph{Diffusion-based methods} Li et al.\cite{srdiff} introduced SrDiff, the first diffusion-based model for SISR, demonstrating that using the diffusion model for SISR tasks is feasible and promising. Saharia et al. proposed Sr3 \cite{sr3}, which adapts Denoising Diffusion Probabilistic Models (DDPM) to perform SISR tasks, yielding a competitive perceptual-based evaluation value. Diffusion-based methods utilize a diffusion process that simulates noise reduction, resulting in sharper and more detailed images. However, a high computational cost is needed due to multiple forward and backward passes through the entire network during the training process. Our proposed ResDiff, though without improving the training speed of a single iteration, accelerates convergence, which can alleviate this issue from another perspective.

\begin{figure*}[t]
\begin{center}
\includegraphics[width=16 cm]{fig/intro_compare.pdf}
\end{center}
\caption{Comparison of different generation processes. In contrast to (a) \cite{sr3}, (b) \cite{srdiff}, (c) \cite{deblur} where only LR Space is used to guide the generation, our ResDiff (d) makes full utilization of CNN Prediction Space and High-Frequency Space to guide a faster and better generation.}
\label{fig:intro_compare}
\end{figure*}

% \begin{figure*}[t]
% \begin{center}
% \includesvg[width=16 cm]{fig/intro_compare.svg}
% \end{center}
% \caption{The process of the diffusion model involves adding noise (forward) and denoising (reverse). In the forward process, Gaussian noise is gradually added to the original image $x_0$, resulting in the standard Gaussian distribution $x_T$. Conversely, in the reverse process, the standard Gaussian distribution $x_T$ is gradually denoised, resulting in the original image $x_0$.}
% \label{fig:intro_compare}
% \end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Denosing Diffusion Probabilistic Models \label{DDPM}}

The diffusion model, first proposed by Sohl-Dickstein et al.\cite{dm2015} and developed by Ho et al.\cite{ddpm}, is a generative model that leverages the diffusion process to model the data distribution. A denoising diffusion probabilistic model (DDPM) employs two Markov chains. One is a forward chain that adds gaussian noise to data:

\vspace{-0.5em}

\begin{equation}
    q(x_t|x_{t-1})= \mathcal{N}(x_t;\sqrt{1-\beta _t}x_{t-1},\beta _tI)
\end{equation}

 where $ \beta _t \in (0, 1)$ for all $t = 1, . . . , T$ and is a pre-chosen hyperparameter.

Using the reparameterization trick, we can obtain $x_t$ directly from $x_0$:

\vspace{-0.5em}

\begin{equation}
    q(x_{t}|x_0)= \mathcal{N}(x_t;\sqrt{\bar{\alpha _t}}x_0,(1-\bar{\alpha _t})I)
\end{equation}

where $\alpha_t=1-\beta _t$ and $\bar{\alpha_t} =\prod_{i=0}^{t}\alpha _i$.

The other Markov chain is a reverse chain that converts noise back into data distribution:

\vspace{-1em}

\begin{equation}
    p_{\theta }(x_{t-1}|x_t)= \mathcal{N}(x_{t-1};\mu _\theta (x_t,t),\Sigma _{\theta}(x_t,t))
    \label{p_process}
\end{equation}

where $\theta$ denotes model parameters, and the mean or variance is parameterized by this model.

The forward chain aims to convert any data distribution into a standard Gaussian distribution. In contrast, the reverse Markov chain employs a neural network, such as a U-net\cite{unet}, to learn transition kernels that reverse the effects of the forward chain. To generate new data, a random variable $x_T$ is initially sampled from the standard Gaussian distribution, followed by ancestral sampling via the reverse Markov chain.
% Figure \ref{fig:diff} illustrates the forward and reverse processes.

% minimize the Kullback-Leibler (KL) divergence between the joint distribution of the reverse process and the forward process:

% \begin{equation}
% \begin{aligned}
% &KL(q(x_{0:T})||p_{\theta }(x_{0:T})) \\
% &=\mathbb{E}_{q}[-logp(x_T)-\sum_{t=1}^Tlog \frac{p_\theta (x_{t-1}|x_t)}{q(x_t|x_{t-1})}]+C \\
% &\geqslant \mathbb{E}[-logp_{\theta}(x_0)]+C
% \end{aligned}
% \end{equation}

% Therefore, we need to 

To optimize the model, we choose to maximize the variational lower bound (VLB). According to \cite{ddpm,sr3,deblur}, the loss function can be written as:

\vspace{-0.5em}

\begin{equation}
    \underset{\theta }{min}L(\theta)=\mathbb{E}[\left\| \epsilon -\epsilon _{\theta}(x_t,t) \right\|_1]
\end{equation}

where $\epsilon$ is a randomly generated gaussian noise, $\epsilon _{\theta}$ is our noise predictor network, and $x_t=\sqrt{\bar{\alpha _t}}x_0+(1-\bar{\alpha _t})\epsilon$.

After training the noise prediction network $\epsilon _{\theta}$, we can generate data directly from Gaussian noise: Initially, $x_T$ is sampled from $\mathcal{N}(0,I)$. The mean and variance of the distribution $p(x_{t-1}|x_t)$ are  subsequently computed according to Eq.\ref{p_process}:

\vspace{-1em}

\begin{equation} \label{eq:mu}
    \mu _\theta (x_t,t)=\frac{1}{\sqrt{\alpha _t}} (x_t-\frac{\beta _t}{\sqrt{1-\bar{\alpha _t}}}\epsilon _\theta (x_t,t))
\end{equation}

\begin{equation} \label{eq:sigma}
    \Sigma _\theta (x_t,t)=\frac{1-\bar{\alpha }_{t-1}}{1-\bar{\alpha _{t}}}\beta _t
\end{equation}

Therefore, $x_{T-1}$ can be sampled from $p_{\theta }(x_{t-1}|x_t)= \mathcal{N}(x_{t-1};\mu _\theta (x_t,t),\Sigma _{\theta}(x_t,t))$ , and so on, until the predicted original image $x_0$ is obtained.

For a Conditional Diffusion Model(CDM) as the one in this paper, we only need to add the condition $y$ to the noise prediction model $\epsilon _{\theta}$, which becomes $\epsilon _{\theta}(x_t,t,y)$. The pseudo-code for sampling with ResDiff is as Alg.\ref{alg:2}.


\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{algorithm}[t]
 \begin{spacing}{1} 
  \caption{ResDiff Inference}
  \label{alg:2}
  \begin{algorithmic}[1]
    \Require
      low-resolution image $x_{LR}$; pre-trained CNN; $\mu_\theta$ and $\Sigma_\theta$ in Eq.\ref{p_process};
    \Ensure
      High-resolution image generated by ResDiff;
    \State $x_{cnn}$ = CNN($x_{LR}$)
    \State $x_T \sim \mathcal{N}(0,I)$   
    \State $\textbf{for} \ t=T:1 \ \textbf{do}$
    \State $ \quad \epsilon \sim \mathcal{N}(0,I) \ $if$ \ t>1, \ $else$ \ \epsilon=0 $
    \State $\quad x_{t-1}= \mu_\theta(x_t,t,x_{cnn}) + \sqrt{\Sigma_\theta(x_t,t,x_{cnn})} \ \epsilon  $
    % \State $\quad x_{t-1}=\frac{1}{\sqrt{\alpha _t}} (x_t-\frac{\beta _t}{\sqrt{1-\bar{\alpha _t}}}\epsilon _\theta (x_t,CNN(x_{LR}),t))$
    % \Statex $ \quad \quad \quad \quad  +\sqrt{\frac{1-\bar{\alpha }_{t-1}}{1-\bar{\alpha _{t}}}\beta _t }\cdot  \epsilon$
    \State $\textbf{end} \ \textbf{for}$
    \State $\textbf{return} \ x_0+x_{cnn}$
  \end{algorithmic}
\end{spacing}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Proposed ResDiff}

\subsection{Pre-trained CNN}

To reduce additional training costs, we utilize a CNN with a reduced number of parameters to generate an initial prediction. This CNN aims to recover primary low-frequency components and partial high-frequency components, consequently facilitating the diffusion model's restoration of the more intricate high-frequency details. To ensure its generating capability, we are enlightened by \cite{dwt_loss, dwt_loss_2} and introduce two more loss functions (Fig.\ref{fig:cnn_loss}), namely $\mathcal{L}_{FFT}$ based on the Fast Fourier Transform (FFT) \cite{fft} and $\mathcal{L}_{DWT}$ based on the Discrete Wavelet Transform (DWT) \cite{dwt}, in addition to the original loss function.

\begin{figure}[t]
\begin{center}
\includegraphics[width=8.5 cm]{fig/CNN_loss.pdf}
\end{center}
\caption{Depiction of the three loss functions utilized in CNN pre-training. A spatial domain loss (GT Loss) and two frequency domain losses (FFT Loss and DWT Loss) are computed.}
\label{fig:cnn_loss}
\end{figure}

The $\mathcal{L}_{FFT}$ can be defined as the mean square error(MSE) between the magnitudes of the FFT coefficients of the two images:

\vspace{-0.9em}

\begin{equation}
    \mathcal{L}_{FFT} = \mathbb{E}[\left\| M - \hat{M} \right\|^2]
\end{equation}

where $M$ and $\hat{M}$ denote the frequency domain images obtained by performing FFT on the ground-truth image and the predicted image.

In a bid to enable the CNN to further recover partial high-frequency contents on top of recovering the primary low-frequency contents, we designed $\mathcal{L}_{DWT}$. Performing DWT on an image will decompose it into four sub-bands: low-low (LL), low-high (LH), high-low (HL), and high-high (HH). LL sub-band contains the low-frequency content of the image, while the remaining three contain the high-frequency components of the image from horizontal, vertical, and diagonal directions, respectively. The LL sub-band can perform further similar decomposition to obtain multi-layer high-frequency components. As for $\mathcal{L}_{DWT}$, we extract the wavelet coefficients of the high-frequency bands $H$, $V$, and $D$, which refer to the high-frequency components in the horizontal, vertical, and diagonal directions, respectively. For both the ground-truth image and predicted image, $\mathcal{L}_{DWT}$ compute the MSE between each high-frequency sub-band:

\vspace{-0.9em}

\begin{equation}
    \mathcal{L}_{DWT} = \sum_{i=1}^{L}\mathbb{E} [ \left\|\hat{H_i}-H_i\right\|^2 + \left\|\hat{V_i}-V_i\right\|^2 +\left\|\hat{G_i}-G_i\right\|^2  ]
\end{equation}

where $H_i$,$V_i$,$D_i$ are the sub-bands of the ground-truth image in the $i$-th downsampling, and $\hat{H_i}$,$\hat{V_i}$,$\hat{D_i}$ are the sub-bands of the predicted image in the $i$-th downsampling, $L$ is the total level of downsampling.

We also add the spatial domain loss named $\mathcal{L}_{GT}$: let the ground-truth image be $Y$, the predicted image be $\hat{Y}$, and $\mathcal{L}_{GT}$ is the MSE between them:

\vspace{-0.5em}

\begin{equation}
    \mathcal{L}_{GT} = \mathbb{E}[\left\| Y - \hat{Y} \right\|^2]
\end{equation}

The total loss function of pre-trained CNN thus is:

\vspace{-0.5em}

\begin{equation}
    \mathcal{L}_{CNN} = \mathcal{L}_{GT} + \alpha \mathcal{L}_{FFT} +  \beta \mathcal{L}_{DWT}
\end{equation}

where $\alpha$ and $\beta$ are adjustable hyperparameters.

Furthermore, we design a simple CNN using residual-connection \cite{resnet} and pixel-shuffle \cite{pixel-shuffle}, named \textbf{SimpleSR}, for initial prediction (the specific structure is given in the supplementary material). Ablation studies on the proposed loss function and SimpleSR are given in the supplementary material.
% Experiments in Table \ref{table:compare_pram} indicate that our SimpleSR outperformed SRCNN \cite{srcnn}.


\subsection{FD-guided Diffusion}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=18 cm]{fig/Unet.pdf}
\end{center}
\caption{An overview of the model architecture in proposed FD-guided diffusion. The pre-trained CNN prediction and the noisy image $x_t$ from step $t$ are fed into the FD-info-Splitter, and its output is then passed on to a U-net, which is equipped with HF-guided cross-attention.}
\label{fig:unet}
\end{figure*}

After obtaining the image $I$ predicted by the pre-trained CNN, we adapt a diffusion model to predict the residuals between $I$ and the ground truth, i.e., the high-frequency components of the ground-truth image. To this end, we propose a Frequency-Domain guided diffusion (FD-guided diffusion), as shown in Fig.\ref{fig:unet}. In contrast to SR3 \cite{sr3}, which simply concatenates the bilinear interpolated image with the noisy image $x_t$ at step $t$, we propose a Frequency-Domain Information Splitter module (FD-Info-Splitter): $I$ and $x_t$ is first fed into the FD-Info-Splitter, whose output is then fed into the U-net \cite{unet}. We follow the Imagen \cite{imagen}, where the self-attention layer is added. In addition, a Frequency-Domain guided Cross-Attention mechanism (FD-guild CA) is designed, which utilizes the high-frequency features obtained from DWT at each layer to generate more fine-grained detail features.

\subsection{FD Info Splitter}

For CNN's initial prediction, low-frequency components are mixed with high-frequency contents. As the diffusion model only needs to recover high-frequency details, the input low and high-frequency features have different statuses: the former mainly assist the generation of high-frequency components globally, while the latter is required to provide guidance for fine-grained details in each region. Therefore, we introduce Frequency-Domain Information Splitter (FD Info Splitter), which explicitly separates high-frequency and low-frequency information for better restoration. Additionally, it effectively mitigates noise for noisy images with large time steps, resulting in better noise prediction (The detailed structure of FD Info Splitter is shown in Fig.\ref{fig:unet}).

For the CNN predicted images $x_{cnn} \in \mathbb{R}^{H \times W \times C}$, we first perform 2D FFT along the spatial dimensions to obtain the frequency domain feature map $M$:

\vspace{-0.5em}

\begin{equation}
    M= FFT(x_{cnn}) \in \mathbb{C}^{H \times W \times C}
\end{equation}

where $FFT(\cdot )$ denotes the 2D FFT. We adapt the methods proposed by \cite{senet,resnet} and merged them into the ResSE module (Residual Squeeze-and-Excitation module), the details of which are shown in the supplementary material.

To implement adaptive high-pass filtering, a Gaussian high-pass filter is utilized whose Standard deviation is obtained from $M$ as follows:

\vspace{-0.7em}

\begin{equation}
    \sigma = min( |ResSE(M)| + \frac{l}{2} ,l )
\end{equation}

where $l = min(H,W)$. The operation for the acquired $ResSE(M)$ is for numerical stability. After obtaining $\sigma$, adaptive gaussian high-pass filter can be given directly as:

\begin{equation}
    H(u, v) = 1 - e^{{-D^2(u, v)}/{ (2 \sigma^ 2) }}
\end{equation}

where $D(u, v)$ is the distance from the point $(u, v)$ in the frequency domain to the center point. The gaussian high-pass filter are then preformed element-wise multiplication with $M$ to obtain the adaptive high-pass filtered feature map $M^{'}$:

\vspace{-0.7em}

\begin{equation}
    M^{'} = A_{hp} \otimes   M
\end{equation}

Finally, we reverse $M^{'}$ back to the spatial domain by adopting inverse FFT to obtain an feature map $x_{HF}$ rich in high-frequency components:

\vspace{-0.5em}

\begin{equation}
    x_{HF} = FFT^{-1}(M^{'})  \in \mathbb{R}^{H \times W \times C}
\end{equation}

where $FFT^{-1}(\cdot )$ denotes the Inverse 2D FFT. Meanwhile, we feed $M^{'}$ into a ResSE module to acquire the attention weights learned in the frequency domain and then perform element-wise multiplication with $x_{cnn}$ to obtain a feature map $x_{LF}$ containing abundant low-frequency information:

\vspace{-0.5em}

\begin{equation}
    x_{LF} = ResSE(M) \otimes  x_{cnn} 
\end{equation}

These two feature maps, dominated by high-frequency and low-frequency components, are concatenated in the channel dimension. By explicitly separating the input's mixed high-frequency and low-frequency components, the network can utilize both differently and more efficiently. 

For a noisy image $x_t$ at a large time step $t$, the noise components can be so large that it hinders network inference. Hence, an adaptive denoising is utilized on $x_t$ to obtain the partially denoised noisy image $x_t^{'}$:

\vspace{-0.3em}

\begin{equation}
    x_t^{'} = ResSE(T) \otimes  x_t 
\end{equation}

The three feature maps $x_{HF}$, $x_{LF}$, $x_t^{'}$, along with $x_{cnn}$ and $x_t$, are all concatenated in the channel dimension and fed into the U-net.

% \begin{figure*}[htbp]
% \begin{center}
% \includesvg[width=12 cm]{fig/FD_Info_Spliter.svg}
% \end{center}
% \caption{Detailed structure of FD Info Spliter.}
% \label{fig:fd-mix}
% \end{figure*}

\subsection{HF-guided CA}

% \begin{figure}[t]
% \begin{center}
% \includesvg[width=6.5 cm]{fig/HF-guided_CS.svg}
% \end{center}
% \caption{The flow of HF-guided CA.}
% \label{fig:hf-ca}
% \end{figure}

In the original U-net architecture, the encoder features are directly concatenated with the features obtained by the decoder \cite{unet}. This fusion facilitates the network to integrate the higher and lower-layer features effectively but lacks the ability to extract high-frequency features. To tackle this issue, we introduce a High-Frequency feature guided Cross-Attention mechanism (HF-guided CA) to recover fine-grained high-frequency details. The flow of the HF-guided CA is illustrated in Fig.\ref{fig:unet}.

We utilize the pre-trained CNN prediction by extracting the $\hat{H_i}$, $\hat{V_i}$, and $\hat{D_i}$ coefficients at the $i$-th level of the DWT. By adding these extracted coefficients with a linear projection, we obtain the feature map $Q$ with aggregated high-frequency information:

\begin{equation}
    Q=Conv_{1\times 1}(\hat{H_i}+\hat{V_i}+\hat{D_i})
\end{equation}

Then, different linear projections of the input feature map $M$ are constructed to obtain $K$ and $V$ in the cross-attention mechanism \cite{cross_attention} :

\vspace{-0.5em}

\begin{equation}
    K = Conv_{1\times 1}(M)
\end{equation}

\vspace{-1.5em}

\begin{equation}
    V = Conv_{1\times 1}(M)
\end{equation}

The output feature map $M^{'}$ can then be obtained from the formula:

\begin{equation}
    M^{'}=Softmax(\frac{QK^T}{\sqrt{d_k}})V
\end{equation}

where $d_k$ is the number of columns of matrix $Q$.


% \subsection{Training and Inference}

% \begin{algorithm}[h]
%   \caption{Training}
%   \label{alg:1}
%   \begin{algorithmic}[1]
%     \State \textbf{repeat}
%     \State $(x_{LR},x_{HR}) \sim p $
%     \State $x_0=x_{HR}-CNN(x_{LR})$
%     \State $\epsilon \sim \mathcal{N}(0,I), \ t \sim U[1,T]$
%     \State Take gradient step on 
%     \Statex  $ \quad \quad \nabla _{\theta } \left\| \epsilon - \epsilon _{\theta }(x_t,CNN(x_{LR}),t)\right\|_1 $
%     \Statex $ \quad \quad (x_t=\sqrt{\bar{\alpha _t}}x_0+\sqrt{1-\bar{\alpha _t}}\epsilon ) $
%     \State \textbf{until} converged
%   \end{algorithmic}
% \end{algorithm}

% During the training phase of ResDiff, LR images will be fed into the pre-trained CNN first to get the initial predicted images, and then the residual image will be computed with HR images to get the generation target $x_0$ of the diffusion model. Following Delur, L1-loss is used instead of L2-loss. The algorithm for ResDiff training is shown in pseudo code in Alg.\ref{alg:1}, with the symbols as defined in Section\ref{DDPM}.

% In the inference stage of ResDiff, $x_T$ is first sampled from a standard normal distribution, and then fed into the diffusion model with LR images to gradually obtain $x_{T-1:0}$. The obtained $x_0$ is summed with the pre-trained CNN output to acquire the predicted SR image. The algorithm for ResDiff inference is shown in pseudo code in Alg.\ref{alg:2}, with the symbols as defined in Section\ref{DDPM}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=16 cm]{fig/res_compare.pdf}
\end{center}
\caption{DIV2k 4$\times$ results. Note that ResDiff provides richer details and more natural textures than other diffusion-based methods for the recovery of small objects (e.g., the clock in the first column) and difficult scenes (e.g., the bridge structure in the second column, the building in the fourth column).}
\label{fig:res_compare}
\end{figure*}

% \subsection{Datasets}

% We have trained and evaluated our ResDiff model on four datasets: two face-SR datasets (FFHQ \cite{ffhq} and CelebA \cite{celeba}) and two general datasets (DIV2K \cite{div2k} and Urban100 \cite{urban100}). To ensure a fair comparison with other methods, our ResDiff was trained solely on the provided training data. The supplementary material contains detailed information about the training process, hyperparameters, and other relevant details.

% \textbf{FFHQ} The FFHQ (Flickr-Faces-HQ) dataset is a high-quality image dataset of human faces. This dataset consists of 70,000 high-resolution images of faces, each with a resolution of 1024 x 1024 pixels. The images are diverse in terms of age, gender, and ethnicity, and have been curated from Flickr using a combination of automated and manual methods to ensure high visual quality and diversity. In this paper, we conducted experiments at $32 \times 32 \to 128 \times 128$ and $256 \times 256 \to 1024 \times 1024$ on FFHQ.

% \textbf{CelebA} CelebA (Celebrities Attributes) is a large-scale face attribute dataset consisting of over 200,000 celebrity images. The images cover large pose variations, facial expressions, and background clutter. In this paper, we conducted experiments at $20 \times 20 \to 160 \times 160$ and $64 \times 64 \to 256 \times 256$ on CelebA.

\subsection{Performance}

To evaluate the performance of our ResDiff model, we compared it with previous diffusion-based and GAN-based methods using four datasets: two face datasets (FFHQ \cite{ffhq}, and CelebA \cite{celeba}) and two general datasets (Div2k \cite{div2k}, and Urban100 \cite{urban100}). The selected evaluation metrics include two distortion-based metrics (PSNR and SSIM \cite{ssim}), as well as a perceptual-based metric (FID \cite{fid}). Our ResDiff is trained solely on the provided training data to guarantee a fair comparison. The supplementary material contains detailed information about the training process, hyperparameters, and other relevant details. Since several methods did not state their performance on some datasets we use, their values are marked as "-" in the table. More experiments with different types of datasets are presented in the supplementary material.

\paragraph{FFHQ and CelebA Results}

\begin{table}[ht]
\small
\setlength{\tabcolsep}{1pt}
\centering
\vspace{-0.5em}
\begin{tabular}{lcccccc}
\toprule

& \multicolumn{3}{c}{$32 \to 128 $}
& \multicolumn{3}{c}{$256 \to 1024 $} \\

\cmidrule(lr){2-4} \cmidrule(lr){5-7}

& PSNR$\uparrow$
& SSIM$\uparrow$
& FID$\downarrow$
& PSNR$\uparrow$
& SSIM$\uparrow$
& FID$\downarrow$ \\

\midrule

Ground Truth
& $\infty$   & 1.000  & 0.00    
& $\infty$   & 1.000  & 0.00     \\

\cmidrule{1-7}

% Bicubic
% & 25.57    & 0.766     & 135.51 
% &  -   &   -  &  - \\

SRGAN\cite{srgan}
& 17.57    & 0.688     & 156.07     
& 21.49    & 0.515     & 60.67  \\

ESRGAN \cite{esrgan}
& 15.43    & 0.267     & 166.36  
& 19.84    & 0.353     & 72.73  \\
	
BRGM\cite{brgm}
& 24.16    & 0.70     &  -
&   -       &   --       &   -  \\

PULSE\cite{pulse}
& 15.74    & 0.37     &   -
&    -      &     -     & - \\

SRDiff\cite{srdiff}
& {26.07}    & {0.794}     
& {72.36}
& {23.01}    & {0.656}   
& {56.17}\\

SR3\cite{sr3}
& {25.37}    & {0.778}     
& {75.29}
& {22.78}    & {0.647}   
& {60.12}\\

\cmidrule{1-7}

ResDiff
& \textbf{26.73}  & \textbf{0.818}  
& \textbf{70.54} 
& \textbf{23.15}   & \textbf{0.668}
& \textbf{53.23} \\

\bottomrule

\end{tabular}

\caption{Quantitative comparison on the FFHQ \cite{ffhq} dataset, where the bolded values represent the best value in each evaluation metric.}
\label{table:ffhq_res}

\setlength{\belowcaptionskip}{10pt}

\end{table}

\begin{table}[ht]
\small
\setlength{\tabcolsep}{1pt}
\centering
\vspace{-0.5em}
\begin{tabular}{lcccccc}
\toprule

& \multicolumn{3}{c}{$20 \to 160 $}
& \multicolumn{3}{c}{$64 \to 256 $} \\

\cmidrule(lr){2-4} \cmidrule(lr){5-7}

& PSNR$\uparrow$
& SSIM$\uparrow$
& FID$\downarrow$
& PSNR$\uparrow$
& SSIM$\uparrow$
& FID$\downarrow$ \\

\midrule

Ground Truth
& $\infty$   & 1.000  & 0.00       
& $\infty$   & 1.000  & 0.00     \\

\cmidrule{1-7}

% Bicubic
% & 23.38    & 0.65     &  -
% & \textbf{27.27}    &0.782     &103.3\\

ESRGAN\cite{esrgan}
& 23.24    & 0.66      &     -
&   -   &  -   & -  \\

PULSE\cite{pulse}
&     -          &  -         &   -
& 22.74         &  0.623        & 40.33 \\


SRFlow\cite{srflow}
& 25.28    & 0.72     &   -
&   -       &   -       &  - \\

SRDiff\cite{srdiff}
& 25.32    & 0.73     &   80.98
& 26.84      &   0.792       & 39.16 \\

SR3\cite{sr3}
&24.89     & 0.728    &  83.11
&26.04     &  0.779   &   43.27\\

\cmidrule{1-7}

ResDiff
& \textbf{25.37}   & \textbf{0.734}   &  \textbf{78.52}
& \textbf{27.16}   & \textbf{0.797}   & \textbf{38.47} \\

\bottomrule

\end{tabular}

\caption{Quantitative comparison on the CelebA \cite{celeba} dataset, where the bolded values represent the best value in each evaluation metric.}

\label{table:celeba_res}

\setlength{\belowcaptionskip}{10pt}

\end{table}

\vspace{-1em}

The quantitative results at $32 \times 32 \to 128 \times 128$ ($4\times$) ,$256 \times 256 \to 1024 \times 1024$ ($4\times$) on FFHQ \cite{ffhq} and $20 \times 20 \to 160 \times 160$ ($8\times$), $64 \times 64 \to 256 \times 256$ ($4\times$) on CelebA \cite{celeba} are shown in table \ref{table:ffhq_res},\ref{table:celeba_res}. Our ResDiff demonstrates superior performance compared to all diffusion-based methods, as evidenced by the metrics presented in the table, and has about 50\% reduction in Perceptual metrics (FID) than the GAN-based model.

\vspace{-1em}

\paragraph{DIV2K and Urban100 Results}

\begin{table}[ht]
\small
\setlength{\tabcolsep}{1pt}
\centering
\vspace{-0.5em}
\begin{tabular}{lcccccc}
\toprule

& \multicolumn{3}{c}{ DIV2K $4 \times$}
& \multicolumn{3}{c}{ Urban100 $4 \times$} \\

\cmidrule(lr){2-4} \cmidrule(lr){5-7}

& PSNR$\uparrow$
& SSIM$\uparrow$
& FID$\downarrow$
& PSNR$\uparrow$
& SSIM$\uparrow$
& FID$\downarrow$ \\

\midrule

Ground Truth
& $\infty$   & 1.000  & 0.00      
& $\infty$   & 1.000  & 0.00     \\

\cmidrule{1-7}

SRDiff\cite{srdiff}
& 26.87    & 0.69     &   110.32
&    26.49      &   0.79      &  51.37 \\

SR3\cite{sr3}
& 26.17    & 0.65    &  111.45
&  25.18     &  0.62   &   61.14 \\

\cmidrule{1-7}

ResDiff
&\textbf{27.94}   & \textbf{0.72}   &  \textbf{106.71}
& \textbf{27.43}      & \textbf{0.82}   & \textbf{42.35} \\

\bottomrule

\end{tabular}

\caption{Quantitative comparison on the DIV2K \cite{div2k} and Urban100 \cite{urban100} dataset, where the bolded values represent the best value in each evaluation metric.}

\label{table:general_res}

\setlength{\belowcaptionskip}{10pt}

\end{table}

The quantitative results at $40 \times 40 \to 160 \times 160$ ($4\times$) on DIV2K \cite{div2k} and $40 \times 40 \to 160 \times 160$ ($4\times$) on Urban100 \cite{urban100} are shown in table \ref{table:general_res}. Note that ResDiff's distortion-based metric values can significantly outperform other diffusion-based methods on these general datasets whose restoration is more difficult. Fig.\ref{fig:res_compare} presents partial results of ResDiff and other diffusion-based methods.


\vspace{-1.5em}

\paragraph{Model Convergence Speed}

\begin{figure}[t]
\begin{center}
\includegraphics[width=8.5 cm]{fig/converge_compare.pdf}
\end{center}
\caption{Curves of the PSNR of the validation set when training our ResDiff compared to SR3 \cite{sr3} on FFHQ $4\times$SR (subplot a) and CelebA $8\times$SR (subplot b). ResDiff achieves convergence in almost half the time of SR3, significantly alleviating the training burden.}
\label{fig:converge_compare}
\end{figure}

The training curves of SR3 and ResDiff on FFHQ ($4\times$) and CelebA ($8\times$) are presented in Fig. \ref{fig:converge_compare}. In subplot a, ResDiff can converge in about 250k iterations, while SR3 needs to converge in about 400k iterations. In subplot b, ResDiff also converged in about 250k iterations, while SR3 converged in about 500k. This reduction in training time and associated overhead is a major advantage of ResDiff over other diffusion-based methods.

\subsection{Ablation Study}

In this section, we perform an ablation study on FFHQ ($4\times$) to investigate the effectiveness of each component in ResDiff, including the influence of different CNNs, and the usefulness of the proposed FD Info Splitter/HF-guided CA. The results are shown in Table \ref{table:compare_cnn}. Note that utilizing the residual structure, even with a simple bilinear interpolation for the initial prediction, can significantly improve the performance. In terms of CNN selection, our proposed SimpleSR also outperforms SRCNN \cite{srcnn}. Moreover, the addition of FD Info Splitter and HF-guided CA both have an improvement in the results. More detailed ablation studies are given in the supplementary material.

\begin{table}[ht]
\small
\setlength{\tabcolsep}{1pt}
\centering
\tabcolsep=0.08cm
\vspace{-0.5em}
\begin{tabular}{lcp{1.8cm}<{\centering}cccc}
\toprule

& \multicolumn{3}{c}{Model Components}
& \multicolumn{3}{c}{Metrics} \\

\cmidrule(lr){2-4} \cmidrule(lr){5-7}

& \multirow{2}{*}{CNN}
& FD Info 
& HF-guided 
& \multirow{2}{*}{PSNR$\uparrow$}
& \multirow{2}{*}{SSIM$\uparrow$}
& \multirow{2}{*}{FID$\downarrow$} \\

& ~
& Splitter 
& CA
& ~
& ~
& ~ \\

\midrule

&  SimpleSR  & $\checkmark$      &   $\checkmark$
& 26.73     & 0.818     & 70.54 \\

\cmidrule{1-7}

& N/A  & $\checkmark$  & $\checkmark$     
& 25.49    & 0.781  & 74.18 \\

& Bilinear     & $\checkmark$      &   $\checkmark$ 
& 25.99    &  0.792   & 74.29   \\

& SRCNN\cite{srcnn}        & $\checkmark$      &   $\checkmark$ 
&  26.14   &  0.809   &  72.17  \\

& SimpleSR  & \multirow{2}{*}{$\checkmark$}      &\multirow{2}{*}{$\checkmark$} 
&  \multirow{2}{*}{26.47}   &  \multirow{2}{*}{0.812}    &  \multirow{2}{*}{71.58}  \\

& (only $\mathcal{L}_{GT}$)   & ~      &   ~
&  ~   & ~    &  ~  \\

\cmidrule{1-7}

& SimpleSR       &        &    
& 25.41    & 0.788    & 77.21  \\

& SimpleSR   &      &   $\checkmark$ 
& 26.09    &  0.796   &  72.42     \\

& SimpleSR   & $\checkmark$      &   
&  25.97   &  0.793   &  73.17  \\


\bottomrule

\end{tabular}

\caption{Ablation study over different model components on the ffhq \cite{ffhq} test sets (The model components we use are placed in the first row). N/A denotes no residual structure used.}

\label{table:compare_cnn}

\setlength{\belowcaptionskip}{10pt}

\end{table}


% \begin{table}[ht]

% \setlength{\tabcolsep}{1pt}
% \centering
% \tabcolsep=0.1cm
% \vspace{-0.5em}
% \begin{tabular}{lcp{1.8cm}<{\centering}cccc}
% \toprule

% & \multicolumn{3}{c}{Hyperparameters}
% & \multicolumn{3}{c}{Metrics} \\

% \cmidrule(lr){2-4} \cmidrule(lr){5-7}

% & Total
% & Channel
% & \multirow{2}{*}{Loss}
% & \multirow{2}{*}{PSNR$\uparrow$}
% & \multirow{2}{*}{SSIM$\uparrow$}
% & \multirow{2}{*}{FID$\downarrow$} \\

% & Time Step 
% & Size 
% & ~
% & ~
% & ~
% & ~ \\

% \midrule

% & 1000   &  64      &   $l_1$ 
% & 26.73     & 0.818     & 70.54 \\

% \cmidrule{1-7}

% & 50   &  64      &   $l_1$ 
% & 25.54     & 0.772     & 76.89 \\

% & 200   &  64      &   $l_1$ 
% & 25.93     & 0.795     & 74.54 \\

% & 2000   &  64      &   $l_1$ 
% & 26.84     & 0.818     & 68.70 \\

% & 50$^\dagger$  &  64      & $l_1$ 
% & 26.28     & 0.812    & 72.38 \\

% \cmidrule{1-7}

% & 1000   &  32      &   $l_1$ 
% & 26.41     & 0.809     & 74.26 \\

% & 1000   &  128      &   $l_1$ 
% & 26.78     & 0.821     & 71.06\\

% \cmidrule{1-7}

% & 1000   &  64      &   $l_2$ 
% & 26.36     & 0.804     & 74.73 \\


% \bottomrule

% \end{tabular}

% \caption{Ablation study over different hyperparameters on the ffhq \cite{ffhq} test sets (The hyperparameters we use are placed in the first row). Where the $\dagger$ denotes using DDIM \cite{ddim} sampling.}

% \label{table:compare_pram}

% \setlength{\belowcaptionskip}{10pt}

% \end{table}

% Meanwhile, we explore the effects of hyperparameters, like different total time-step $T$, channel size $c$ in U-net and choose $L_1$ loss or $L_2$ loss. These ablation experiments are also performed on the FFHQ dataset ($32 \times 32 \to 128 \times 128$) and the results are shown in Table \ref{table:compare_pram}. It can be seen that the longer total time step, the better model performance, but the correspondingly lower the sampling speed. In addition, the number of channels in the U-net is underfitting at a low number (32) and overfitting at a high number (128). Finally, it can be found that $l_2$ loss is inferior to $l_1$ loss , which is consistent with \cite{deblur,sr3}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \paragraph{Visualization Analysis}

% \paragraph{Limitation}

% Although our proposed ResDiff can accelerate the model convergence speed, it still has a high training cost, which is a shortcoming of the diffusion model itself. In addition, our proposed series of modules focus on restoring high frequency details of images, but seem to lack the emphasis on color space. Fig.x in the supplementary material shows that the images predicted with insufficient training times have fine restoration of high frequency content, but there is a significant gap in the color domain with Ground Truth images.In addition, ResDiff does not get State-of-the-art results on large benchmark datasets, partly because of the insufficient parameter of the U-net in diffusion model. However, we cannot use larger models for training anymore due to the equipment limitation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Conclusion and Future Work}

In this paper, we propose ResDiff, a residual structure-based diffusion model. In contrast to the previous works, which only adapt LR images to generate HR images, ResDiff utilizes the feature-richer CNN prediction for guidance. Meanwhile, we introduce a frequency-domain-based loss function to the CNN and design a frequency-domain guided diffusion to facilitate the diffusion model in generating low-frequency information. Comprehensive experiments on different datasets demonstrate that the proposed ResDiff accelerates the training convergence speed and provides superior image generation quality.

Our ResDiff can also be adapted for other image restoration tasks, such as image blind super-resolution, deblurring, and inpainting. Although ResDiff can accelerate convergence, operations such as DWT are still time-consuming and call for optimization in future work. In addition, it can be seen from the supplementary material that the color will appear a large discrepancy when the model is under-trained, which may be caused by a lack of color features in the guided high-frequency information. Utilizing a global color feature may well address this issue in future work. Moreover, our ResDiff does not outperform current State-Of-The-Art(SOTA) SISR methods \cite{sota1,sota2}. This is attributed to the disparity between model parameters. Due to equipment limitations, adopting a larger U-net model in ResDiff is left to future work. In addition, if a pre-trained SOTA model is applied to replace the CNN in ResDiff, it may be possible to establish a new SOTA. Finally, ResDiff may consider incorporating more DPM techniques \cite{latent-diff,guilded-diff,free-guilded-diff} and superior network architectures \cite{better-network,better-network2} in the future.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\begin{center}
{\Large \bf Supplementary Material }
\end{center}


\section{Model Details}

\paragraph{SimpleSR}

We designed a simple network with a few parameters named SimpleSR, which uses residual connections with the PixelShuffle module, as shown in Fig.\ref{fig:model} (a). This network with a lesser parameter count can reduce the pre-training burden of the CNN while restoring sufficient low-frequency content.

\paragraph{ResSE}

Extracting attention weights requires global information, so simple single-layer convolution is no longer applicable while using a self-attention mechanism is too computationally expensive. Therefore, we introduce the ResSE module (Fig.\ref{fig:model} (b)) by combining \cite{resnet} and \cite{senet}. ResSE can aggregate global information with a few parameters to obtain the attention weights of feature aggregation.


\section{Training Details}

\subsection{Datasets}

For the FFHQ \cite{ffhq} dataset and CelebA \cite{celeba} dataset, we directly downsample the HR images with a bicubic kernel. Following SrDiff \cite{srdiff}, we select 5000 images as the test set and all other images as the training set.

For the DIV2K \cite{div2k} dataset and Urban100 \cite{urban100} dataset, we crop the HR images into $160 \times 160$ patches and downsample with the bicubic kernel sequentially in the training phase, just like SrDiff. In the testing phase, we downsample the original HR images and get the SR prediction images based on the LR images. The DIV2K test set uses its given test set of 100 images, and for Urban100, we select 20 images as the test set (such a smaller test set may affect the calculation of FID metrics).


\subsection{Model and Training Configuration}

We set the initial number of Channels in U-net to 64, with channel multipliers ${1, 2, 4, 8, 8}$. The self-attention blocks 
 \cite{imagen} are only added to the bottom and penultimate layers, and two residual convolution blocks \cite{resnet} are used in each Conv block, with the dropout set to 0.2.

For the training of the diffusion model, we used the Adam \cite{adam} optimizer with a learning rate set to $1 \times 10^{-4}$ and a batch size set to 4. The total time step $T$ was set to 1000, using a linear increase in $\beta_{1:T}$ from $1 \times 10^{-6}$ to $1 \times 10^{-2}$. We trained on 1 NVIDIA Tesla A100 with 100k steps.

For CNN pre-training, we performed only 10k iterations and set the batch size to 128 due to the small size of the network. The rest of the hyperparameters were kept the same as above.

\begin{figure}[t]
\begin{center}
\includegraphics[width=8 cm]{fig/Supply.pdf}
\end{center}
\caption{Network architecture of SimpleSR and ResSE.}
\label{fig:model}
\end{figure}

\section{Additional Experimental Results}

\subsection{Additional Dataset Results}

\begin{table}[ht]
\small
\setlength{\tabcolsep}{1pt}
\centering
\vspace{-0.5em}
\begin{tabular}{lcccc}
\toprule

& \multicolumn{2}{c}{ LSCIDMR $4 \times$}
& \multicolumn{2}{c}{ Manga109 $4 \times$} \\

\cmidrule(lr){2-3} \cmidrule(lr){4-5}

& PSNR$\uparrow$
& SSIM$\uparrow$
& PSNR$\uparrow$
& SSIM$\uparrow$ \\

\midrule

Ground Truth
& $\infty $   & 1.0 
& $\infty $   & 1.0 \\

\cmidrule{1-5}

SRDiff\cite{srdiff}
& 27.54    & 0.807
& 27.04    & 0.813    \\

SR3\cite{sr3}
& 26.13    & 0.782
& 26.88    &   0.805  \\

\cmidrule{1-5}

ResDiff
&\textbf{27.79}   & \textbf{0.812}
&\textbf{27.76}   & \textbf{0.832}\\

\bottomrule

\end{tabular}

\caption{Quantitative comparison on the LSCIDMR \cite{LSCIDMR} and Manga109 \cite{manga109} dataset, where the bolded values represent the best value in each evaluation metric.}

\label{table:otherdataset}

\setlength{\belowcaptionskip}{10pt}

\end{table}

\begin{table}[ht]
\small
\setlength{\tabcolsep}{1pt}
\centering
\vspace{-0.5em}
\begin{tabular}{lcccc}
\toprule

& \multicolumn{2}{c}{ Set5 $4 \times$}
& \multicolumn{2}{c}{ Set14 $4 \times$}\\

\cmidrule(lr){2-3} \cmidrule(lr){4-5}

& PSNR$\uparrow$
& SSIM$\uparrow$
& PSNR$\uparrow$
& SSIM$\uparrow$ \\

\midrule

Ground Truth
& $\infty $   & 1.0 
& $\infty $  & 1.0 \\

\cmidrule{1-5}

SRDiff\cite{srdiff}
& 28.72    &  0.843
&  25.63    &  0.702   \\

SR3\cite{sr3}
& 27.31    & 0.767
& 25.29    &  0.684    \\

\cmidrule{1-5}

ResDiff
&\textbf{29.32}   & \textbf{0.854}
&\textbf{26.19}   & \textbf{0.718}\\

\bottomrule

\end{tabular}

\caption{Quantitative comparison on the Set5 \cite{set5} and Set14 \cite{set14} dataset, where the bolded values represent the best value in each evaluation metric.}

\label{table:setdataset}

\setlength{\belowcaptionskip}{10pt}

\end{table}

To verify the generalizability of the model, we performed the same comparison of each diffusion-based method on LSCIDMR (a satellite cloud image dataset) \cite{LSCIDMR} and Manga109 (comic image dataset) \cite{manga109}. Fig.\ref{fig:LSCIDMR_compare} presents the results of ResDiff on the LSCIDMR dataset ($4\times$). Furthermore, we use the weights trained on the DIV2k \cite{div2k} dataset to test on the set5 \cite{set5} and set14 \cite{set14} dataset. The results are shown in Table \ref{table:otherdataset}, \ref{table:setdataset}. Note that the proposed ResDiff can still outperform other diffusion-based methods. Fig.\ref{fig:set_compare} presents the results of different diffusion-based methods on the set14 dataset ($4\times$).


\subsection{Additional Ablation Study}

\begin{table}[ht]

\setlength{\tabcolsep}{1pt}
\centering
\tabcolsep=0.1cm
\vspace{-0.5em}
\begin{tabular}{lcp{1.8cm}<{\centering}cccc}
\toprule

& \multicolumn{3}{c}{Hyperparameters}
& \multicolumn{3}{c}{Metrics} \\

\cmidrule(lr){2-4} \cmidrule(lr){5-7}

& Total
& Channel
& \multirow{2}{*}{Loss}
& \multirow{2}{*}{PSNR$\uparrow$}
& \multirow{2}{*}{SSIM$\uparrow$}
& \multirow{2}{*}{FID$\downarrow$} \\

& Time Step 
& Size 
& ~
& ~
& ~
& ~ \\

\midrule

& 1000   &  64      &   $l_1$ 
& 26.73     & 0.818     & 70.54 \\

\cmidrule{1-7}

& 50   &  64      &   $l_1$ 
& 25.54     & 0.772     & 76.89 \\

& 200   &  64      &   $l_1$ 
& 25.93     & 0.795     & 74.54 \\

& 2000   &  64      &   $l_1$ 
& 26.84     & 0.818     & 68.70 \\

& 50$^\dagger$  &  64      & $l_1$ 
& 26.28     & 0.812    & 72.38 \\

\cmidrule{1-7}

& 1000   &  32      &   $l_1$ 
& 26.41     & 0.809     & 74.26 \\

& 1000   &  128      &   $l_1$ 
& 26.78     & 0.821     & 71.06\\

\cmidrule{1-7}

& 1000   &  64      &   $l_2$ 
& 26.36     & 0.804     & 74.73 \\


\bottomrule

\end{tabular}

\caption{Ablation study over different hyperparameters on the ffhq \cite{ffhq} test sets (The hyperparameters we use are placed in the first row). Where the $\dagger$ denotes using DDIM \cite{ddim} sampling.}

\label{table:compare_pram}

\setlength{\belowcaptionskip}{10pt}

\end{table}

We explore the effects of hyperparameters as different total time-step $T$, channel size $c$ in U-net and choose $L_1$ loss or $L_2$ loss. These ablation experiments are also performed on the FFHQ dataset ($32 \times 32 \to 128 \times 128$), and the results are shown in Table \ref{table:compare_pram}. It can be seen that the longer the total time step, the better the model performance, but the correspondingly lower the sampling speed. In addition, the number of channels in the U-net is underfitting at a low number (32) and overfitting at a high number (128). Finally, it can be found that $l_2$ loss is inferior to $l_1$ loss, consistent with \cite{deblur,sr3}.


\subsection{Additional Sampling Results}

Fig.\ref{fig:un_trained} illustrates the image colour deviation of ResDiff when the convergence is not completed, a problem caused by not aggregating the colour information effectively. Fig.\ref{fig:diff_process} presents the diffusion iterations on the DIV2K dataset, and Fig.\ref{fig:div_res} demonstrates more high-resolution images of ResDiff sampled on the FFHQ ($4 \times$) and DIV2K ($4 \times$) datasets.

\begin{figure*}[t]
\begin{center}
\includegraphics[width=14 cm]{fig/LSCIDMR_res.pdf}
\end{center}
\caption{LSCIDMR $4\times$ results. The image super-resolution of remote-sensing dataset demands the restoration of more texture details, and ResDiff accomplishes this well.}
\label{fig:LSCIDMR_compare}
\end{figure*}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=16 cm]{fig/set_res.pdf}
\end{center}
\caption{Set14 $4\times$ results. Compared to other diffusion-based methods, ResDiff generates images with richer textures and sharper edges.}
\label{fig:set_compare}
\end{figure*}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=16 cm]{fig/un_trained.pdf}
\end{center}
\caption{Demonstration of image colour deviation problems on CelebA \cite{celeba} ($4 \times$). The leftmost image is the LR image, the rightmost image is the HR image, and the middle is the predicted image when the model has not converged. Note that although the model has been able to learn satisfactory margin information, there is a significant colour difference.}
\label{fig:un_trained}
\end{figure*}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=16 cm]{fig/diff_process.pdf}
\end{center}
\caption{The diffusion process of ResDiff on the DIV2K dataset. The leftmost image shows the initial prediction of the CNN, and the following four images show the denoising process of the image in turn.}
\label{fig:diff_process}
\end{figure*}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=16 cm]{fig/div_res.pdf}
\end{center}
\caption{Partially sampled high-resolution images from ResDiff on the DIV2K dataset ($4\times$).}
\label{fig:div_res}
\end{figure*}



\end{document}