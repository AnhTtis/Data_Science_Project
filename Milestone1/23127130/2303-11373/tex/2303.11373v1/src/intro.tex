\input{src/figs/two_level_hierarchy}
\section{Introduction}
The power of an abstraction depends on its usefulness for solving new problems.
Object rearrangement~\citep{batra2020rearrangement} offers an intuitive setting for studying the problem of learning reusable abstractions.
Solving novel rearrangement problems requires an agent to not only infer object representations without supervision, but also recognize that the same action for moving an object between two locations can be reused for different objects in different contexts.

We study the simplest setting in simulation with pick-and-move action primitives that move one object at a time.
Even such a simple setting is challenging because the space of object configurations is combinatorially large, resulting in long-horizon combinatorial task spaces.
We formulate rearrangement as an offline goal-conditioned reinforcement learning (RL) problem, where the agent is pretrained on a experience buffer of sensorimotor interactions and is evaluated on producing actions for rearranging objects specified in the input image to satisfy constraints depicted in a goal image.

Offline RL methods~\citep{levine2020offline} that do not infer factorized representations of entities struggle to generalize to problems with more objects.
But planning with object-centric methods that do infer entities~\citep{veerapaneni2020entity} is also not easy because the difficulties of long-horizon planning with learned parametric models~\citep{janner2019trust} are exacerbated in combinatorial spaces.

Instead of planning with parametric models, our work takes inspiration from non-parametric planning methods that have shown success in combining neural networks with graph search to generate long-horizon plans.
These methods~\citep{yang2020plan2vec,zhang2018composable,lippi2020latent,emmons2020sparse} explicitly construct a transition graph from the experience buffer and plan by searching through the actions recorded in the graph with a learned distance metric.
The advantage of such approaches is the ability to stitch different path segments from offline data to solve new problems.
The disadvantage is that the non-parametric nature of such methods requires transitions that will be used for solving new problems to have already been recorded in the buffer, making conventional methods, which store entire observations monolithically, ill-suited for combinatorial generalization.
Fig.~\ref{fig:problem}b shows that the same state transition can manifest for different objects and in different contexts, but monolithic non-parametric methods are not constrained to recognize that all scenarios exhibit the same state transition at an abstract level.
This induces an blowup in the number of nodes in the graph.
To overcome this problem, we devise a method that exploits the similarity among state transitions in different contexts.

Our method, \textbf{Neural Constraint Satisfaction} (\methodname)\footnote{See Appdx.~\ref{appdx:name} for an explanation behind this name.},
marries the strengths of non-parametric planning with those of object-centric representations.
Our main contribution is to show that factorizing the traditionally monolithic entity representation into action-invariant features (its \textbf{type}) and action-dependent features (its \textbf{state}) makes it possible during planning and control to reuse action representations for different objects in different contexts, thereby addressing the core combinatorial challenge in object rearrangement.
To implement this factorization,~\methodname constructs a two-level hierarchy (Fig.~\ref{fig:two_level_hierarchy}) to abstract the experience buffer into a graph over state transitions of individual entities, separated from other contextual entities (Fig.~\ref{fig:model}).
To solve new rearrangement problems,~\methodname infers what state transitions can be taken given the current and goal image observations, re-composes sequences of state transitions from the graph, and translates these transitions into actions.

In \S\ref{sec:problem} we introduce a problem formulation that exposes the combinatorial structure of object rearrangement tasks by explicitly modeling the independence, symmetry, and factorization of latent entities.
This reveals two challenges in object rearrangement which we call the \textbf{correspondence problem} and \textbf{combinatorial problem}.
In \S\ref{sec:method} we present~\methodname, a method for controlling an agent that plans over and acts with emergent learned entity representations, as a unified method for tackling both challenges.
We show in \S\ref{sec:experiments} that~\methodname outperforms both state-of-the-art offline RL methods and object-centric shooting-based planning methods in simulated rearrangement problems.