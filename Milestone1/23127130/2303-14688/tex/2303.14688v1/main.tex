\def\isarxiv{1}

\ifdefined\isarxiv
\documentclass[11pt]{article}

\usepackage{amsthm, amsmath, amssymb, graphicx, url}
\usepackage[margin=1in]{geometry}

\else
% \documentclass[anon,12pt]{colt2023} % Anonymized submission
\documentclass[final]{colt2023} % avoid compile error
%\documentclass[final,12pt]{colt2023} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e
\fi

\usepackage{latexsym, amscd, amsfonts, mathrsfs, stmaryrd, tikz-cd, mathrsfs, bbm, esint, listings, moreverb, hyperref, pifont, algorithm, algpseudocode, enumitem}

\def\bE{\mathbb{E}}
\def\bP{\mathbb{P}}
\def\bR{\mathbb{R}}
\def\bZ{\mathbb{Z}}

\def\cA{\mathcal{A}}
\def\cO{\mathcal{O}}
\def\cP{\mathcal{P}}
\def\cX{\mathcal{X}}
\def\cY{\mathcal{Y}}
\def\cZ{\mathcal{Z}}

\def\simiid{\stackrel{\text{iid}}{\sim}}
\def\wt{\widetilde}
\def\wh{\widehat}

\newcommand{\circnum}[1]{%
  \text{\ding{\the\numexpr #1+191}}%
}

\DeclareMathOperator\atanh{\mathrm{atanh}}
\DeclareMathOperator\Aut{\mathrm{Aut}}
\DeclareMathOperator\BEC{\mathrm{BEC}}
\DeclareMathOperator\BSC{\mathrm{BSC}}
\DeclareMathOperator\BP{\mathrm{BP}}
\DeclareMathOperator\FSC{\mathrm{FSC}}
\DeclareMathOperator\diag{\mathrm{diag}}
\DeclareMathOperator\Po{\mathrm{Po}}
\DeclareMathOperator\poly{\mathrm{poly}}
\DeclareMathOperator\Res{\mathrm{Res}}
\DeclareMathOperator\sgn{\mathrm{sgn}}
\DeclareMathOperator\SKL{\mathrm{SKL}}
\DeclareMathOperator\Unif{\mathrm{Unif}}
\DeclareMathOperator\Var{\mathrm{Var}}
\DeclareMathOperator\Id{\mathrm{Id}}
\DeclareMathOperator\EC{\mathrm{EC}}
\DeclareMathOperator\rk{\mathrm{rk}}
\DeclareMathOperator{\argmax}{arg\,max}
\DeclareMathOperator{\argmin}{arg\,min}
\DeclareMathOperator\pa{\mathrm{pa}}


\ifdefined\isarxiv
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}

\theoremstyle{definition}
% \newtheorem{conv}[theorem]{Convention}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem*{remark}{Remark}
\fi

\usepackage{times}
% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}

\newcommand{\yuzhou}[1]{{\color{blue}[Yuzhou: #1]}}
\newcommand{\yury}[1]{{\color{red}[Yury: #1]}}

\begin{document}

\ifdefined\isarxiv
\title{Uniqueness of BP fixed point for the Potts model and applications to community detection}
\date{}
\author{
Yuzhou Gu\thanks{\texttt{yuzhougu@mit.edu}. MIT.}
\and
Yury Polyanskiy\thanks{\texttt{yp@mit.edu}. MIT.}
}
\else
\title[Uniqueness of BP fixed point for the Potts model]{Uniqueness of BP fixed point for the Potts model and applications to community detection}
\coltauthor{%
 \Name{Yuzhou Gu} \Email{yuzhougu@mit.edu}\\
 \addr Masaschusetts Institute of Technology
 \AND
 \Name{Yury Polyanskiy} \Email{yp@mit.edu}\\
 \addr Masaschusetts Institute of Technology%
}
\fi

\maketitle

\begin{abstract}%
  In the study of sparse stochastic block model (SBM) one needs to analyze a distributional recursion, known as  belief propagation (BP) on a tree. Uniqueness of the fixed point of this recursion implies several results about the SBM, including optimal recovery algorithms for SBM (\cite{mossel2016belief}) and SBM with side information (\cite{mossel2016local}), and a formula for SBM mutual information (\cite{abbe2021stochastic}). The 2-community case corresponds to an Ising model, for which~\cite{yu2022ising} established uniqueness for all cases.

  Here, we analyze broadcasting of $q$-ary spins on a Galton-Watson tree with expected offspring degree $d$ and Potts channels with second-largest eigenvalue $\lambda$. We allow for the intermediate vertices to be observed through noisy channels (side information) We prove BP uniqueness holds with and without side information when $d\lambda^2 \ge 1 + C \max\{\lambda, q^{-1}\}\log q$ for some absolute constant $C>0$ independent of $q,d,\lambda$. For large $q$ and $\lambda = o(1/\log q)$, this is asymptotically achieving the Kesten-Stigum threshold $d\lambda^2=1$.
  %For the low SNR regime, we prove that these properties hold whenever $d\lambda^2 < q^{-2}$.
  These results imply mutual information formula and optimal recovery algorithms for the $q$-community SBM in the corresponding ranges.

  % In this paper, we study uniqueness of BP fixed point and boundary irrelevance for the $q$-ary Potts model. We prove that these properties hold for a wide range of parameters. In particular, for ferromagnetic Potts models, these properties hold whenever signal-to-noise-ratio (SNR) is outside the range $[f(q), g(q)]$, where $f(q) = 1/q^2$, $g(q) = \Theta(\log q)$ as $q\to \infty$. This improves over the previous best result \cite{chin2020optimal} which proved Uniqueness of BP Fixed Point for SNR larger than $\poly(q)$.

  For $q\ge 4$,  \cite{sly2011reconstruction,mossel2022exact} shows that there exist choices of $q,d,\lambda$ below Kesten-Stigum (i.e. $d\lambda^2 < 1$) but reconstruction is possible.
  Somewhat surprisingly, we show that in such regimes BP uniqueness \textit{does not hold} at least in the presence of weak side information.

  Our technical tool is a theory of $q$-ary symmetric channels, that we initiate here, generalizing the classical and widely-utilized information-theoretic characterization of BMS (binary memoryless symmetric) channels.
%
\end{abstract}

\ifdefined\isarxiv
\else
\begin{keywords}%
  stochastic block model, Potts model, broadcasting on trees, cavity equation, belief propagation, boundary irrelevance, $q$-ary symmetric channels
\end{keywords}
\fi

% \tableofcontents


\section{Introduction}
\paragraph{Stochastic block model.}
The stochastic block model (SBM) is a random graph model with community structures. It has a rich set of results and phenomena, investigated in the last decade~ \cite{abbe2017community}.
In this paper, we focus on the sparse symmetric multi-community case. The model has four parameters: $n\in \bZ_{\ge 1}$, the number of vertices; $q\in \bZ_{\ge 2}$, the number of communities; $a,b\in \bR_{\ge 0}$, edge probabilities within community and outside community.
The model is defined as follows. First, we assign a random label (community) $X_i \sim \Unif([q])$ i.i.d~for $i\in V=[n]$. Then a graph $G=(V, E)$ is constructed, where $(i,j)\in E$ with probability $\frac an$ if $X_i=X_j$, and with probability $\frac bn$ if $X_i \ne X_j$, independently for all $(i,j)\in \binom V2$.
When $a>b$, we say the model is assortative. When $a<b$, we say the model is disassortative.

For sparse SBM, an important problem is weak recovery. We say the model admits weak recovery if there exists an estimator $\wh X(G) \in [q]^V$ such that
\begin{align}
  &\lim_{n\to \infty} \frac 1n \bE d_H(X, \wh X(G)) < 1-\frac 1q, \\
  \text{where}\quad &d_H(X,Y) := \min_{\tau \in \Aut([q])} \sum_{i\in [n]} \mathbbm{1}\{X_i \ne \tau(Y_i)\}.
\end{align}
\cite{decelle2011asymptotic} conjectured that the (algorithmic) weak recovery threshold is at the Kesten-Stigum threshold, and there is an information-computation gap for $q\ge 5$. The positive (algorithmic) part of their conjecture has been established by a series of works \cite{massoulie2014community,mossel2018proof,abbe2016achieving,abbe2015community,abbe2015detection,abbe2018proof,bordenave2015non,stephan2019robustness}, in a very general sense allowing asymmetric communities.
\cite{abbe2015community,abbe2015detection,abbe2018proof} gave inefficient reconstruction algorithms below the Kesten-Stigum threshold, for disassortative models with $q\ge 4$ and assortative models with $q\ge 5$, giving evidence for the information-computation gap.
The informational weak recovery threshold has been established in special cases by \cite{mossel2015reconstruction,mossel2018proof} ($q=2$), \cite{mossel2022exact} ($q=3,4$ assuming large enough degree), \cite{coja2017information} (general $q$, disassortative).
For the assortative case, the informational weak recovery threshold for $q=3,4$ low degree or $q\ge 5$ is still open, despite some partial progress \cite{banks2016information,gu2020non}.
The information-computation gap is also wide open.

When weak recovery is possible, an interesting problem is what is the optimal recovery algorithm, i.e., one that minimizes the probability of error
\begin{align}
  \lim_{n\to \infty} \inf_{\wh X} \frac 1n \bE d_H(X, \wh X(G)).
\end{align}
\cite{decelle2011asymptotic} conjectured that the belief propagation algorithm is optimal.
The conjecture is not proved yet but significant progress has been made: a series of works \cite{mossel2016belief,abbe2021stochastic,yu2022ising} established optimal recovery algorithms for $q=2$; \cite{chin2020optimal,chin2021optimal} gave optimal recovery algorithms for the general case (general $q$, not necessarily symmetric) under the condition that SNR is large enough.

A fundamental quantity of the stochastic block model is its (normalized) mutual information
\begin{align}
  \lim_{n\to \infty} \frac 1n I(X; G).
\end{align}
\cite{coja2017information} proved a mutual information formula for the disassortative model (general $q$).
\cite{abbe2021stochastic,yu2022ising} proved a mutual information formula for the $q=2$ case.
\cite{dominguez2022mutual} conjectured another formula for the $q=2$ case and proved a matching lower bound. It is not known whether their conjectured formula is equivalent to the one proved by \cite{abbe2021stochastic,yu2022ising}.

\paragraph{Broadcasting on trees.}
Stochastic block model has a close relationship with the broadcasting on trees (BOT) model. The reason is that in SBM, the $r$-neighborhood (for any constant $r$) of a random vertex converges (in the sense of local weak convergence) to a Galton-Watson tree with Poisson offspring distribution. Therefore, properties of BOT can often imply corresponding results on SBM.
For sparse symmetric $q$-SBM, the corresponding model is the Potts model (i.e., BOT model with Potts channels). This model has three parameters: $q\in \bZ_{\ge 2}$, the number of colors; $\lambda \in [-\frac 1{q-1}, 1]$, edge correlation strength; $d\in \bZ_{\ge 0}$, expected offspring.
The Potts model is defined as follows.
Let $T$ be a regular tree of offspring $d$ or a Galton-Watson tree with offspring distribution $\Po(d)$ (Poisson distribution with expectation $d$).
Let $\rho$ be the root of $T$.
We assign to every vertex $v$ a color $\sigma_v \in [q]$ according to the following process:
(1) $\sigma_\rho \sim \Unif([q])$;
(2) given $\sigma_u$, colors of children of $u$ are iid  $\sim P_\lambda(\cdot | \sigma_u)$,
where $P_\lambda$ is the Potts channel
$P_\lambda(j|i) = \lambda \mathbbm{1}\{i=j\} + \frac{1-\lambda}q.$

An important problem on BOT is the reconstruction problem, asking whether we can gain any non-trivial information about the root given observation of far away vertices.
That is, whether the limit
\begin{align}
  \lim_{k\to \infty} I(\sigma_\rho ; \sigma_{L_k} | T_k)
\end{align}
is non-zero, where $L_k$ stands for the set of vertices at distance $k$ to the root $\rho$.
It is known that non-reconstruction results for the Potts model imply impossibility of weak recovery for the corresponding SBM, although the other side does not hold: in the case $a=0$, there is a gap of factor $2$ (as $q\to \infty$) between the BOT reconstruction threshold and the SBM weak recovery threshold.

The reconstruction problem on trees has been studied a lot under many different settings \cite{bleher1995purity,evans2000broadcasting,mossel2001reconstruction,mossel2003information,mezard2006reconstruction,borgs2006kesten,bhatnagar2010reconstruction,sly2009reconstruction,kulske2009symmetric,liu2019large,gu2020non,mossel2022exact}. For the Potts model, it is known (\cite{sly2011reconstruction,mossel2022exact}) that the Kesten-Stigum threshold \cite{kesten1966additional} $d\lambda^2=1$ is tight (i.e., equal to the reconstruction threshold) for $q=3,4$ when $d$ is large enough, and is not tight when $q\ge 5$ or when $q=4$, $\lambda<0$ and $d$ is small enough.

\paragraph{Belief propagation.}
Belief propagation is a powerful tool for studying the BOT model. It is usually described as an algorithm for computing posterior distribution of vertex colors given observation. Here we take an information-theoretic point of view and describe BP in terms of constructing communication channels.

We view the BOT model as an information channel from the root color to the observation.
Let $M_k$ denote the channel $\sigma_\rho \mapsto \sigma_{L_k}$.%, where $L_k$ denotes the set of vertices at distance $k$ to the root $\rho$.
Then $(M_k)_{k\in \bZ_{\ge 0}}$ satisfies the following recursion, which we call belief propagation recursion:
\begin{align}
  M_{k+1} = \bE_b (M_k \circ P_\lambda)^{\star b}
\end{align}
where $b$ following the branching number distribution (constant in the regular tree case, $\Po(d)$ in the Poisson tree case), and $(\cdot)^{\star b}$ denotes $\star$-convolution power. % (see Section~\ref{sec:fms} for more information).
Let $\BP$ be the operator
\begin{align} \label{eqn:bp-operator}
  \BP(M) := \bE_b (M \circ P_\lambda)^{\star b}
\end{align}
defined on the space of information channels with input alphabet $[q]$. Due to symmetry in colors, we can regard $\BP$ as an operator on the space of FMS channels (see Section~\ref{sec:fms}).
In terms of the $\BP$ operator, the reconstruction problem can be rephrased as whether the limit channel $\BP^\infty(\Id) := \lim_{n\to \infty} \BP^n(\Id)$ (where $\Id$ stands for the identity channel $\Id(y|x) = \mathbbm\{x=y\}$) is trivial or not.
The problem of optimal recovery for SBM can be reduced to the following problem on trees: whether the limit
\begin{align}
  \lim_{n\to \infty} I(\sigma_\rho; \omega_{L_k} | T_k)
\end{align}
where $\omega$ is the observation of $\sigma$ through a non-trivial channel $W$, stays the same for any non-trivial FMS $W$. Therefore, it is important to study the non-trivial fixed points of the $\BP$ operator (the trivial channel is always a fixed point).

\cite{mossel2016belief} proved uniqueness of BP fixed point for $q=2$ and large enough SNR.
\cite{abbe2021stochastic} improved to $q=2$ and SNR $\not \in [1,3.513]$.
\cite{yu2022ising} proved uniqueness of BP fixed point for $q=2$ and any parameter $d,\lambda$, closing the question for binary models.
For $q\ge 3$, \cite{chin2020optimal} proved that when the $U$ is close enough to $\Id$, and $d\lambda^2 > C_q$, where $C_q$ is a constant depending on $q$, then $\BP^\infty(U) = \BP^\infty(\Id)$. They did not give asymptotics for $C_q$, but it seems like it is at least polynomial in $q$.
\cite{chin2021optimal} generalized \cite{chin2020optimal} to asymmetric models.

% \yuzhou{move this paragraph to somewhere}
% The phase transition is summarized as follows:
% \begin{itemize}
%   \item Below the reconstruction threshold (i.e., in the non-reconstruction regime), $\BP$ operator has only one fixed point, the trivial fixed point.
%   \item Above the reconstruction threshold and below the Kesten-Stigum threshold ($d\lambda^2<1$), $\BP$ operator has one trivial fixed point and at least one non-trivial fixed point (which is $\BP^\infty(\Id)$). We do not know whether there is a unique non-trivial fixed point. However, we know that for weak enough $W$, the limit $\BP^\infty(W)$ is trivial.
%   \item Above the Kesten-Stigum threshold ($d\lambda^2 > 1$), $\BP$ operator has one trivial fixed point and at least one non-trivial fixed point. We do not know whether there is a unique non-trivial fixed point. However, we know that if there is a unique non-trivial fixed point, then $\BP^\infty(W)$ is the same for all non-trivial FMS $W$.
% \end{itemize}

\paragraph{Boundary irrelevance.}
\cite{abbe2021stochastic} reduces the SBM mutual information problem to the boundary irrelevance problem, on a tree model called the broadcasting on trees with survey (BOTS) model. In the BOTS model, we observe color of every vertex through a noisy FMS channel $W$ (called the survey).
When the limit
\begin{align}
  \lim_{n\to \infty} I(\sigma_\rho ; \sigma_{L_k} | T_k, \omega_{T_k})
\end{align}
is zero for any non-trivial $W$ (where $T_k$ is the set of all vertices within distance at most $k$ to the root, and $\omega$ is the observation of $\sigma$ through $W$), we say the model satisfies boundary irrelevance.
It is not hard to show that, boundary irrelevance is equivalent to the condition that the operator
\begin{align} \label{eqn:bp-prime-operator}
  \BP'(M) := \left(\bE_b (M\circ P_\lambda)^{\star B}\right) \star W
\end{align}
has a unique fixed point in the space of FMS channels.
Because $\BP$ and $\BP'$ have very similar forms, the boundary irrelevance problem has a close relationship with the problem of uniqueness of BP fixed point. Indeed, these two problems can be solved using the same method.

\paragraph{Symmetric channels}
One technical tool we develop in this paper is a theory of $q$-ary input symmetric channels (Section~\ref{sec:fms}).
The topic of symmetric channels is classic in information theory (see e.g.~\cite[Section 19.4]{polyanskiy2023information}). The binary-input case has been studied extensively under the name of BMS channels (see e.g.,~\cite[Chapter 4]{richardson2008modern}).
However, the theory of comparison between $q$-ary symmetric channels is much less developed before the current work, with only example known to us being~\cite{makur2018comparison}.

\paragraph{Our results.}
Our first main result is uniqueness of BP fixed point and boundary irrelevance for a wide range of parameters. (For a more precise statement, see Theorem~\ref{thm:bi-low-snr} and Theorem~\ref{thm:bi-high-snr}.)
\begin{theorem}[Uniqueness of BP fixed point and boundary irrelevance] \label{thm:bi-imprecise}
  There exists an absolute constant $C>0$ such that the following statement holds.
  Consider the $q$-spin Potts model with broadcasting channel $P_\lambda$ on a regular tree or a Poisson tree with expected offspring $d$.
  If either $d\lambda^2 < q^{-2}$ or $d\lambda^2 > 1 + C \max\{\lambda, q^{-1}\} \log q$,
  then boundary irrelevance holds.
  That is, there exists $\epsilon > 0$ such that for any FMS survey channel $W$ with $C_{\chi^2}(W) \le \epsilon$, we have
  $
    \lim_{k\to \infty} I(\sigma_\rho; \sigma_{L_k} | T_k, \omega_{T_k}) > 0.
  $
  Furthermore, under the same conditions, the $\BP$ operator has a unique non-trivial FMS fixed point.
\end{theorem}

Our second main result is that boundary irrelevance does not hold between the reconstruction threshold and the Kesten-Stigum threshold.
\begin{theorem}[Boundary irrelevance does not always hold] \label{thm:non-bi}
  Consider the $q$-spin Potts model with broadcasting channel $P_\lambda$ on a regular tree or a Poisson tree with expected offspring $d$.
  If $d\lambda^2 < 1$ and reconstruction is possible, then boundary irrelevance does not hold for weak enough survey channel.
  That is, there exists $\epsilon > 0$ such that for any FMS survey channel $W$ with $C_{\chi^2}(W) \le \epsilon$, we have
  $
    \lim_{k\to \infty} I(\sigma_\rho; \sigma_{L_k} | T_k, \omega_{T_k}) > 0.
  $
\end{theorem}

\paragraph{Applications.}
Two main applications of boundary irrelevance and uniqueness of BP fixed point are mutual information formula and optimal recovery algorithm. We state the main results here.
\begin{theorem}[Mutual information formula] \label{thm:sbm-mutual-info}
  Let $(X,G)\sim \mathrm{SBM}(n,q,\frac an,\frac bn)$.
  Let $d = \frac {a+(q-1)b}q$ and $\lambda = \frac{a-b}{a+(q-1)b}$.
  Let $(T,\sigma)$ be the Potts model with broadcasting channel $P_\lambda$ on a Poisson tree with expected offspring $d$. Let $\rho$ be the root of $T$, $L_k$ be the set of vertices at distance $k$ to $\rho$, $T_k$ be the set of vertices at distance $\le k$ to $\rho$.
  If $(q,d,\lambda)$ satisfies the condition in Theorem~\ref{thm:bi-imprecise}, then we have
  \begin{align}
    \lim_{n\to \infty} \frac 1n I(X;G) = \int_0^1 \lim_{k\to \infty} I(\sigma_\rho; \omega^\epsilon_{T_k\backslash \rho} | T_k) d\epsilon,
  \end{align}
  where $\omega^\epsilon$ denotes observation through survey channel $\EC_\epsilon$, the erasure channel with erasing probability $\epsilon$.
\end{theorem}

For SBM with side information, boundary irrelevance immediately implies optimal recovery algorithm.
\begin{theorem}[Optimal recovery for SBM with side information] \label{thm:optimal-recovery-side-information}
  Work under the same setting as Theorem~\ref{thm:sbm-mutual-info}.
  Suppose that in addition to $G$, we observe side information $Y_v \sim W(\cdot | X_v)$ for all $v\in V$, where $W$ is some non-trivial FMS channel.
  If $(q,d,\lambda,W)$ satisfy the condition in Theorem~\ref{thm:bi-imprecise}, then belief propagation (Algorithm~\ref{alg:optimal-recovery-side-information}) achieves optimal recovery accuracy, with probability of error per vertex
  $
    \lim_{k\to \infty} P_e(\sigma_\rho | T_k, \omega_{T_k}).
  $
\end{theorem}

For vanilla SBM, uniqueness of BP fixed point implications optimal recovery, given an initial recovery algorithm with nice accuracy guarantees.
\begin{theorem}[Optimal recovery for SBM] \label{thm:optimal-recovery-vanilla}
  Work under the same setting as Theorem~\ref{thm:sbm-mutual-info}.
  Suppose $d\lambda^2>1$ and $(q,d,\lambda)$ satisfy the condition in Theorem~\ref{thm:bi-imprecise}.
  Suppose there is an algorithm $\cA$ and a constant $\epsilon>0$ (not depending on $n$)  such that with probability $1-o(1)$, the empirical transition matrix $F\in \bR^{q\times q}$ defined as
  \begin{align}
    F_{i,j} := \frac{\#\{v\in V: X_v=i, \wh X_v = j\}}{\#\{v\in V: X_v = i\}}, \qquad \wh X := \cA(G)
  \end{align}
  satisfies
  \begin{enumerate}[label=(\arabic*)]
    \item $\|F^\top \mathbbm{1} - \mathbbm{1}\|_{\infty} = o(1)$; \label{item:thm-optimal-recovery-vanilla-req-1}
    \item $\sigma_{\min}(F) > \epsilon$, where $\sigma_{\min}$ is the smallest singular value; \label{item:thm-optimal-recovery-vanilla-req-2}
    \item there exists a permutation $\tau\in \Aut([q])$ such that $F_{\tau(i),i} > F_{\tau(i),j}+\epsilon$ for all $i\ne j\in [q]$. \label{item:thm-optimal-recovery-vanilla-req-3}
  \end{enumerate}
  (Note that we do not asssume $F$ stays the same for different calls to $\cA$.)

  Then there is an algorithm (Algorithm~\ref{alg:optimal-recovery-vanilla}) achieving optimal recovery accuracy, with probability of error per vertex
  $
    \lim_{k\to \infty} P_e(\sigma_\rho | T_k, \sigma_{L_k}).
  $
\end{theorem}
The theoretical guarantees for the initial recovery algorithm provided by previous works (that achieve the Kesten-Stigum threshold) \cite{abbe2018proof,stephan2019robustness} seem not enough to use for belief propagation recursion. There are several different  ways to formulate the initial point requirement.
The initial point requirement we state here is weaker than the one in \cite{chin2020optimal} (which requires the initial point to be close enough to $\Id$, which seems unlikely to hold when $d\lambda^2$ is slightly above $1$), and a generalization of the requirement in the $q=2$ case used by \cite{mossel2016belief}.
Our initial point requirement seems more likely to hold near the Kesten-Stigum threshold. For example, it is plausible that a balanced algorithm would achieve the empirical transition matrix $A$ to be close to $P_\lambda$ for some $|\lambda|=\Omega(1)$.

\paragraph{Our technique.}
For the positive result (Theorem~\ref{thm:bi-imprecise}), we generalize the degradation method of \cite{abbe2021stochastic} to $q$-ary symmetric channels.
In this method, we find suitable potential functions $\Phi$ on the space of FMS channels, such that for two channels $M,\wt M$ are related by degradation ($\wt M\le_{\deg} M$), we have (1) $\Phi(M)-\Phi(\wt M)$ contracts to $0$ under iterations of $\BP$ (2) if $\Phi(M) = \Phi(\wt M)$, then $M=\wt M$.
This shows that the limit channels $\BP^\infty(M)$ and $\BP^\infty(\wt M)$ are equal.

To carry out this method, we develop a theory of $q$-ary symmetric channels, generalizing the classical theory of binary memoryless symmetric (BMS) channels. We show that $q$-ary symmetric channels are equivalent to symmetric distributions on the probability simplex $\cP([q])$, and degradation relationship has a coupling characterization under the distribution interpretation.

For the negative result (Theorem~\ref{thm:non-bi}), we show that when the survey channel is weak enough, the limit channel $\BP^{\prime \infty}$ can be arbitrarily weak. We use $\chi^2$-capacity to characterize the strength of a channel.
One difficulty of $\chi^2$-capacity is that they are not subadditive under $\star$-convolution (unlike KL capacity or SKL capacity).
To deal with this problem, we prove a local subadditivity result (Lemma~\ref{lemma:subadd}), i.e., $\chi^2$-capacity is almost subadditive for weak enough channels.

Mutual information formula (Theorem~\ref{thm:sbm-mutual-info}) and optimal recovery for SBM with side information (Theorem~\ref{thm:optimal-recovery-side-information}) are direct consequences of boundary irrelevance (Theorem~\ref{thm:bi-imprecise}), generalizing \cite{abbe2021stochastic} and \cite{mossel2016local} respectively.
For optimal recovery for the vanilla SBM (Theorem~\ref{thm:optimal-recovery-vanilla}), we need to handle asymmetric initial points (Section~\ref{sec:asymm}), and suitably generalize the algorithm in \cite{mossel2016belief}.

\paragraph{Structure of the paper.}
In Section~\ref{sec:fms}, we establish a theory of FMS channels.
In Section~\ref{sec:bi}, we prove Theorem~\ref{thm:bi-imprecise}, boundary irrelevance and uniqueness of BP fixed point for a wide range of parameters.
In Section~\ref{sec:non-bi}, we prove Theorem~\ref{thm:non-bi}, that boundary irrelevance does not hold between the reconstruction threshold and the Kesten-Stigum threshold.

In Section~\ref{sec:proof-fms}, we give missing proofs in Section~\ref{sec:fms}.
In Section~\ref{sec:proof-bi}, we give missing proofs in Section~\ref{sec:bi}.
In Section~\ref{sec:proof-non-bi}, we give missing proofs in Section~\ref{sec:non-bi}, including the local subadditivity of $\chi^2$-information (Lemma~\ref{lemma:subadd}).
In Section~\ref{sec:asymm}, we discuss asymmetric fixed points of the $\BP$ operator.
In Section~\ref{sec:mutual-info}, we prove Theorem~\ref{thm:sbm-mutual-info}, SBM mutual information formula.
In Section~\ref{sec:optimal-recovery}, we prove Theorem~\ref{thm:optimal-recovery-side-information} and~\ref{thm:optimal-recovery-vanilla}, two optimal recovery results.

\section{FMS Channels} \label{sec:fms}
In this section we introduce $q$-ary fully memoryless symmetric ($q$-FMS) channels.\footnote{Here ``fully'' modifies ``symmetric'', and indicates that the symmetry group of the channel is the full symmetric group $\Aut(\cX)$ as opposed to a subgroup.} They are a generalization of binary memoryless symmetric (BMS) channels to $q$-ary input alphabet.
For background on BMS channels, see e.g., \cite{richardson2008modern}.
\begin{definition} \label{defn:fms-channel}
	A $q$-FMS channel (or FMS channel when $q$ is obvious from context) is a channel $P: \cX \to \cY$ with input alphabet $\cX = [q]$ such that
	there exists a group homomorphism $\iota: \Aut(\cX) \to \Aut(\cY)$ such that
	for any measurable $E\subseteq \cY$, we have
	$
		P(E | x) = P(\iota(\tau) E | \tau(x))
	$
	for all $x\in \cX$, $\tau \in \Aut(\cX)$.
\end{definition}
By definition, $2$-FMS channels are just BMS channels.

As binary symmetric channels (BSCs) are basic building blocks for BMS channels, we define FSC channels, which are basic building blocks for FMS channels.
\begin{definition} \label{defn:fsc-channel}
	Let $\cX = [q]$, $\cY = \Aut(\cX)$. For $\pi \in \cP(\cX)/\Aut(\cX)$, define channel $\FSC_\pi: \cX \to \cY$ as
	\begin{align}
		\FSC_\pi(\tau | i) = \frac 1{(q-1)!}\pi_{\tau^{-1}(i)} \qquad \forall i\in \cX, \tau\in \Aut(\cX),
	\end{align}
	where $\Aut(\cX)$ acts on $\cY$ by left multiplication.
\end{definition}
We verify that FSCs are examples of FMSs by checking the definition.
% We can verify that
% \begin{align}
%   \FSC_\pi(\eta \tau | \eta(i)) = \frac{1}{(q-1)!} \pi_{(\eta\tau)^{-1}(\eta(i))} = \frac 1{(q-1)!} \pi_{\tau^{-1}(i)} = \FSC_\pi(\tau | i)
% \end{align}
% for $i\in \cX$, $\eta,\tau\in \Aut(\cX)$. So FSCs are examples of FMSs.

One of the most basic relationships between two channels is degradation.
\begin{definition} \label{defn:fsc-degradation}
  Let $P: \cX \to \cY$ ad $Q: \cX \to \cZ$ be two channels with the same input alphabet.
  We say $P$ is a degradation of $Q$ (denoted $P \le_{\deg} Q$) if there exists a channel $R: \cZ \to \cY$ respecting $\Aut(\cX)$ action such that $P = R\circ Q$.
  We say $P$ and $Q$ are equivalent if $P \le_{\deg} Q$ and $Q \le_{\deg} P$.
\end{definition}
In other words, $P$ is a degradation of $Q$ if we can simulate $P$ by postprocessing the output of $Q$.

In the binary case, we know that all BMSs are equivalent to mixtures of BSCs. This is also true for FMS channels.
\begin{proposition} \label{prop:fms-mixture}
  Every FMS is equivalent to a mixture of FSCs, i.e., every FMS $P:\cX\to \cY$ is equivalent to a channel
	$X\to (\pi, Z)$ where $\pi \sim P_\pi\in \cP(\cP(\cX)/\Aut(\cX))$ is independent of $X$,
	and $Z \sim \FSC_\pi(\cdot | X)$ conditioned on $\pi$ and $X$.
	Furthermore, $P_\pi$ is uniquely deteremined by $P$.
\end{proposition}
Proof is deferred to Section~\ref{sec:proof-fms}.
For BMS channels we usually denote the $\pi$-component as a single number $\Delta\in [0, \frac 12]$, and call it the $\Delta$-component.

Degradation has a nice characterization in terms of the $\pi$-components.
\begin{proposition} \label{prop:fms-degradation-coupling}
	Let $P, Q$ be two FMSs, where $\pi_P$ and $\pi_Q$ are their $\pi$-components.
	Then $P\le_{\deg} Q$ if and only if there exists coupling between $\pi_P$ and $\pi_Q$ such that
	\begin{align}
		\pi \le_m \bE[\pi_Q | \pi_P = \pi] \qquad \forall \pi\in \cP(\cX)/\Aut(\cX),
	\end{align}
	where $\le_m$ denotes majorization.
  We use the convention that elements $\pi\in\cP(\cX)/\Aut(\cX)$ satisfy $\pi_1\ge \cdots\ge \pi_q$, making the expectation well-defined.
\end{proposition}
Proof is deferred to Section~\ref{sec:proof-fms}.

There are different ways to construct new FMS channels from given FMS channels. In this paper we will study two ways: composition with Potts channels, and $\star$-convolution.

Fix $q\ge 2$. For $\lambda \in [-\frac{1}{q-1}, 1]$, define Potts channel $P_\lambda: [q] \to [q]$ as
$
  P_\lambda(y | x) = \lambda \mathbbm{1}\{x=y\} + \frac{1-\lambda}q
$
for $x,y\in [q]$.
Then given any $q$-FMS channel $P$, $P\circ P_\lambda$ is also a FMS channel.
Furthermore, if the $\pi$-component of $P$ has distribution $P_\pi$, then the $\pi$-component of $P\circ P_\lambda$ has distribution $f_*(P_\pi)$ where $f(\pi) = \lambda \pi + \frac {1-\lambda} q$, and $f_*$ is the induced pushforward map.

Given two channels $P: \cX\to \cY$, $Q: \cY\to \cZ$, their $\star$-convolution $P\star Q: \cX \to \cY \times \cZ$ is defined as $(P \star Q)(E \times F | x) = P(E | x) Q(F | y)$, i.e., applying $P$ and $Q$ to the input independently. When $P$ and $Q$ are $q$-FMSs, $P\star Q$ has a natural $q$-FMS structure.
If the $\pi$-component of $P$ (resp.~$Q$) has distribution $P_\pi$ (resp.~$Q_\pi$), then the $\pi$-component of $P\star Q$ has distribution
\begin{align}
  &\bE_{\substack{\pi \sim P_\pi \\ \pi' \sim Q_\pi}}\left[ \sum_{\tau \in \Aut([q])} \left(\frac 1{(q-1)!}\sum_{i\in [q]} \pi_i \pi'_{\tau(i)}\right) \mathbbm{1}_{\pi \star_\tau \pi'}\right]
  \label{eqn:star-formula}\\
  \text{where}\quad &\pi \star_\tau \pi' := \left(\frac {\pi_i \pi'_{\tau(i)}}{\sum_{j\in [q]} \pi_j \pi'_{\tau(j)}}\right)_{i\in [q]} \in \cP([q])/\Aut([q])
\end{align}
and $\mathbbm{1}_\theta \in \cP(\cP([q])/\Aut([q]))$ denotes the point distribution at $\theta\in \cP([q])/\Aut([q])$.
We use $M^{\star b}$ to denote the $b$-th $\star$-power: $M^{\star 0} := \Id$ and $M^{\star b} := M^{\star (b-1)} \star M$.

Given any $q$-FMS $P$, we can restrict the input alphabet to get a $q'$-FMS for $q'\le q$. (Because of symmetry, the restricted channel is unique up to channel equivalence no matter what size-$q'$ subset we choose.)
In this paper we only use the case $q'=2$, i.e., restrict to a BMS channel.
We use $P^R$ to denote the restricted BMS channel.\footnote{Here ``$R$'' stands for ``restriction''.}

In this paper we study the behavior of information measures under belief propagation. Below are a few useful information measures.
\begin{definition} \label{defn:fms-info-measure}
  Let $P$ be a $q$-FMS channel and $\pi$ be its $\pi$-component. We define the following quantities.
  \begin{align*}
    P_e(P) &= \bE \min\{1-\pi_i : i\in [q]\}  \tag{probability of error} \\
    C(P) &= \log q - \bE \sum_{i\in [q]} \pi_i \log \frac 1{\pi_i} \tag{capacity} \\
    C_{\chi^2}(P) &= \bE \left[q \sum_{i\in [q]} \pi_i^2 -1\right] \tag{$\chi^2$-capacity} \\
    C_{\SKL}(P) &= \bE \sum_{i\in [q]} (\pi_i - \frac 1q) \log(\pi_i) \tag{$\SKL$ capacity}
  \end{align*}
  % \begin{align*}
  %   P_e(P) &= \bE  \min\{\pi_i : i\in [q]\}  \tag{probability of error} \\
  %   C(P) &= \bE D(\pi \| \Unif([q])) \tag{capacity} \\
  %   &= \log q - \bE \sum_{i\in [q]} \pi_i \log \frac 1{\pi_i} \\
  %   C_{\chi^2}(P) &= \bE \chi^2 (\pi \| \Unif([q])) \tag{$\chi^2$-capacity} \\
  %   &= \bE \left[q \sum_{i\in [q]} \pi_i^2 -1\right] \\
  %   C_{\SKL}(P) &= \bE D_{\SKL}(\pi \| \Unif(q))  \tag{$\SKL$ capacity}\\
  %   &= \bE \left(D(\pi \| \Unif(q)) + D(\Unif(q) \| \pi)\right) \\
  %   &= \bE \sum_{i\in [q]} (\pi_i - \frac 1q) \log(\pi_i).
  % \end{align*}
  For BMS channels we further define the Bhattacharyya coefficient
  \begin{align}
    Z(P) &= \bE \left[2\sqrt{\Delta(1-\Delta)}\right]
    \tag{Bhattacharyya coefficient}
  \end{align}
  where $\Delta$ is the $\Delta$-component of $P$.
\end{definition}

These information measures respect degradation, as summarized in the next lemma.
\begin{lemma} \label{lemma:fms-info-measure-monotone}
  Let $P$ and $Q$ be two $q$-FMSs with $P\le_{\deg} Q$. Then
  $
    P_e(P) \ge P_e(Q), C(P) \le C(Q), C_{\chi^2}(P) \le C_{\chi^2}(Q), C_{\SKL}(P) \le C_{\SKL}(Q).
  $
  Furthermore for BMS channels (i.e.~$q=2$) we have
  $
    Z(P) \ge Z(Q).
  $
\end{lemma}
\begin{proof}
  By definition of degradation, and data processing inequality for $f$-divergences.
\end{proof}

\section{Uniqueness and boundary irrelevance results} \label{sec:bi}
In this section we prove uniqueness of BP fixed point and boundary irrelevance results for the Potts model for a wide range of parameters.
We consider the $q$-spin Potts model with broadcasting channel $P_\lambda$ on a regular tree or a Poisson tree with expected offspring $d$.

We state two results, one for the low SNR regime and one for the high SNR regime. We define the following constants used in the results.
\begin{definition}
  For $q\in \bZ_{\ge 2}$, $\lambda\in [-\frac 1{q-1}, 1]$, $d\ge 0$, we define
  \begin{align}
    C^L(\lambda,q) &:= \sup_{\substack{\pi \in \cP([q]) \\ v \in \mathbbm{1}^\perp \subseteq \bR^q}} \frac{f^L(\lambda \pi + \frac {1-\lambda} q, v)}{f^L(\pi, v)}, \label{eqn:thm:bi-low-snr-defn-C}\\
    \text{where}~f^L(\pi, v) &:= \left\langle \pi^{-1} + q \pi^{-2}, v^2\right\rangle, \label{eqn:thm:bi-low-snr-defn-f}\\
  % \end{align}
  % \begin{align}
    C^H(\lambda,q) &:= \sup_{\substack{\pi \in \cP([q]) \\ v \in \mathbbm{1}^\perp \subseteq \bR^q}} \frac{f^H(\lambda \pi + \frac {1-\lambda} q, v)}{f^H(\pi, v)}, \label{eqn:thm:bi-high-snr-defn-C}\\
    \text{where}~f^H(\pi, v) &:= \|\pi^{1/4}\|_2^2 \|\pi^{-3/4} v\|_2^2 - \left\langle \pi^{1/3}, \pi^{-3/4}v \right\rangle^2, \label{eqn:thm:bi-high-snr-defn-f}\\
    c^H(q,d,\lambda) &:= \left(\frac 2q + \frac{q-2}q \cdot \frac{d\lambda^2-1}{d\lambda-1}\right)^{-1}.\label{eqn:thm:bi-high-snr-defn-c}
  \end{align}
\end{definition}
We have the following bounds on these constants: $C^L(\lambda, q) \le q^2$ (Prop.~\ref{prop:bi-low-snr-C-bound}), $C^H(\lambda, q) \le q^{5/2}$ (Prop.~\ref{prop:bi-high-snr-C-bound}), $c^H(q,d,\lambda) \ge 1$ (obvious).

\begin{theorem}[Low SNR] \label{thm:bi-low-snr}
  If \begin{align}
    d\lambda^2 C^L(\lambda,q) < 1, \label{eqn:thm:bi-low-snr-cond}
  \end{align}
  where $C^L$ is defined in~\eqref{eqn:thm:bi-low-snr-defn-C}, then boundary irrelevance and uniqueness of BP fixed point hold.
\end{theorem}

\begin{theorem}[High SNR] \label{thm:bi-high-snr}
  If $d\lambda^2>1$ and
  \begin{align}
    d\lambda^2 \exp\left(-c^H(q,d,\lambda)\cdot \frac{d\lambda^2-1}2\right) C^H(\lambda,q) < 1, \label{eqn:thm:bi-high-snr-cond}
  \end{align}
  where $c^H$ is defined in~\eqref{eqn:thm:bi-high-snr-defn-c}, $C^H$ is defined in~\eqref{eqn:thm:bi-high-snr-defn-C},
  then boundary irrelevance and uniqueness of BP fixed point hold.
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{thm:bi-imprecise} given Theorem~\ref{thm:bi-low-snr} and~\ref{thm:bi-high-snr}]
  We prove the low SNR case and the high SNR case separately.

  \textbf{Low SNR:}
  By Prop.~\ref{prop:bi-low-snr-C-bound}, $C^L(\lambda,q) \le q^2$. If $d \lambda^2 < q^{-2}$, then~\eqref{eqn:thm:bi-low-snr-cond} holds and Theorem~\ref{thm:bi-low-snr} applies.

  \textbf{High SNR:}
  We prove that~\eqref{eqn:thm:bi-high-snr-cond} holds whenever $d\lambda^2 > 1 + 56 \max\{\lambda, q^{-1}\}\log q$.

  By Prop.~\ref{prop:bi-high-snr-C-bound}, $C^H(\lambda,q) \le q^{5/2}$.
  For $d\lambda^2>1$, we have
  \begin{align}
    c^H(q,d,\lambda)\ge \left(\frac 2q + \frac {q-2}q \cdot \max\{\lambda,0\}\right)^{-1} \ge \left(\frac 2q + \max\{\lambda,0\}\right)^{-1} \ge \frac 14 \max\{\lambda,q^{-1}\}^{-1}.
  \end{align}
  Therefore we have
  \begin{align}
    d\lambda^2 \exp(-c^H(q,d,\lambda) \cdot \frac{d\lambda^2-1}2) C^H(\lambda,q)
    \le d\lambda^2 \exp\left(-\frac{d\lambda^2-1}{8 \max\{\lambda,q^{-1}\}}\right) q^{5/2} =: g_{q,\lambda}(d).
  \end{align}
  Computing $g'_{q,\lambda}(d)$, we see that $g_{q,\lambda}(d)$ is monotone decreasing in $d$ when $d\lambda^2 > 8\max\{\lambda,q^{-1}\}$.
  Therefore it suffices to prove $g_{q,\lambda}(d_0)<1$ where $d_0\lambda^2 = 1+56\max\{\lambda,q^{-1}\}\log q$.
  We have
  \begin{align}
    g_{q,\lambda}(d_0) = (1+56 \max\{\lambda,q^{-1}\}\log q) \exp(-7 \log q) q^{5/2} \le (1+56 \log q) q^{-9/2}.
  \end{align}
  The last expression is $<1$ for all $q\ge 3$. This finishes the proof.
\end{proof}

\subsection{The degradation method}
Let $M_k$ denote the FMS channel $\sigma_\rho \to (\sigma_{L_k}, \omega_{T_k})$, and $\wt M_k$ denote the FMS channel $\sigma_\rho \to \omega_{T_k}$ (both with the obvious symmetries). Then $\wt M_k \le_{\deg} M_k$ by forgetting $\sigma_{L_k}$.
Both sequences $\{M_k\}_{k\ge 0}$ and $\{\wt M_k\}_{k\ge 0}$ satisfy the Belief Propagation recursion, i.e.,
\begin{gather}
  M_{k+1} = \BP(M_k), \qquad \wt M_{k+1} = \BP(\wt M_k),\\
  \BP(M) := \bE_b [(M_k \circ P_\lambda)^{\star b}\star W],
\end{gather}
where $b$ follows the branching number distribution (constant if working with regular trees), and $W$ is the survey FMS channel. They start with different initial values $M_0, \wt M_0$ (both FMSs) where $\wt M_0 \le_{\deg} M_0$.
We will make one of two assumptions: (1) survey FMS channel is non-trivial, or (2) $\wt M_0$ is non-trivial. The first assumption corresponds to boundary irrelevance, and the second assumption corresponds to uniqueness of BP fixed point.

Let $\phi: \cP([q]) \to \bR$ be a strongly convex function invariant under $\Aut([q])$ action.
Extend it to a function $\Phi: \{\text{FMSs}\} \to \bR$ as $\Phi(P) = \bE \phi(\pi_P)$.
By degradation, we have
\begin{align}
	\Phi(M_k) \ge \Phi(M_{k+1}) \ge \Phi(\wt M_{k+1}) \ge \Phi(\wt M_k).
\end{align}
The following proposition shows that it suffices to prove contraction of potential function $\Phi$.
\begin{proposition} \label{prop:phi-contraction-imply-l2}
  Assume that $\phi: \cP([q]) \to \bR$ is $\alpha$-strongly convex for some $\alpha>0$, and that
  $
    \lim_{k\to \infty} (\Phi(M_k)-\Phi(\wt M_k))=0.
  $
  Then under the canonical coupling, we have
  $
    \lim_{k\to \infty} \bE \|\pi_k-\wt \pi_k\|_2^2 =0,
  $
  where $\pi_k$ (resp.~$\wt \pi_k$) is the $\pi$-component of $M_k$ (resp.~$\wt M_k$).
  In particular,
  $
    \lim_{k\to \infty} M_k = \lim_{k\to \infty} \wt M_k.
  $
\end{proposition}
\begin{proof}
  \begin{align}
    \Phi(M_k) - \Phi(\wt M_k) &= \bE_{\wt \pi_k} \bE[ \phi(\pi_k) - \phi(\wt \pi_k) | \wt \pi_k] \\
    &\ge \bE_{\wt \pi_k} \bE[\langle \nabla \phi(\wt \pi_k), \pi_k - \wt \pi_k\rangle + \frac \alpha 2 \|\pi_k - \wt \pi_k\|_2^2 | \wt \pi_k] \nonumber \\
    &\ge \bE_{\wt \pi_k} \langle \nabla \phi(\wt \pi_k), \bE[\pi_k | \wt \pi_k]- \wt \pi_k\rangle + \frac \alpha 2 \bE \|\pi_k - \wt \pi_k\|_2^2 \nonumber \\
    & \ge \frac \alpha 2 \bE \|\pi_k - \wt \pi_k\|_2^2. \nonumber
  \end{align}
  The second step is by $\alpha$-strongly convexity.
  The third step is because $\wt \pi_k\le_m \bE[\pi_k | \wt \pi_k]$ and $\phi$ is convex (thus Schur-convex).
  Taking the limit $k\to \infty$ we get the desired result.
\end{proof}
\begin{proposition} \label{prop:contraction-imply-bi}
  Assume that $\phi: \cP([q]) \to \bR$ is $\alpha$-strongly convex for some $\alpha>0$, and that
  $
    \lim_{k\to \infty} (\Phi(M_k)-\Phi(\wt M_k))=0.
  $
  whenever $\wt M_0 \le_{\deg} M_0$ and (1) survey FMS channel $W$ is non-trivial, or (2) $\wt M_0$ is non-trivial.
  Then boundary irrelevance holds, and there is a unique non-trivial FMS BP fixed point.
\end{proposition}
\begin{proof}
  \textbf{Boundary irrelevance:}
  Let $\wt M_0 = \FSC_{\Unif([q])}$, $M_0 = \FSC_{(1,0,\ldots,0)}$.
  By condition and Prop.~\ref{prop:phi-contraction-imply-l2}, we have
  $
    \lim_{k\to \infty} M_k = \lim_{k\to \infty} \wt M_k.
  $
  In particular,
  $
    \lim_{k\to \infty} C(M_k) = \lim_{k\to \infty} C(\wt M_k)
  $
  where $C$ denote capacity (Definition~\ref{defn:fms-info-measure}).
  Note that
  \begin{align}
    \lim_{k\to \infty} C(M_k) &= \lim_{k\to\infty} I(\sigma_\rho; \sigma_{L_k}, \omega_{T_k} |T_k),\\
    \lim_{k\to \infty} C(\wt M_k) &= \lim_{k\to\infty} I(\sigma_\rho; \omega_{T_k} |T_k).
  \end{align}
  So this proves boundary irrelevance.

  \textbf{Uniqueness of BP fixed point:}
  Suppose there is a non-trivial fixed point FMS channel $U$.
  Let $\wt M_0 = U$, $M_0 = \FSC_{(1,0,\ldots,0)}$.
  By condition and Prop.~\ref{prop:phi-contraction-imply-l2}, we have
  $
    \lim_{k\to \infty} M_k = \lim_{k\to \infty} \wt M_k.
  $
  Because $U$ is a fixed point, LHS is equal to $U$.
  On the other hand, RHS does not depend on $U$.
  Therefore there is a unique non-trivial FMS fixed point.
\end{proof}

% By strictly convexity, if
% \begin{align}
% 	\lim_{k\to \infty} \Phi(M_k) = \lim_{k\to \infty} \Phi(\wt M_k),
% \end{align}
% then $\{M_k\}_{k\ge 0}$ and $\{\wt M_k\}_{k\ge 0}$ converge in distribution to the same FMS channel.
% Therefore it suffices to prove contraction of $\Phi(M_k) - \Phi(\wt M_k)$ under BP recursion.


\subsection{Low SNR} \label{sec:low-snr}
% \yuzhou{Check original uniqueness proof}
For the low SNR case, we use $\SKL$-capacity as the potential function. We define
\begin{align}
	\phi^L(\pi) = C_{\SKL}(\FSC_\pi) = \sum_{i\in [q]} (\pi_i - \frac 1q) \log \pi_i.
\end{align}

We state a few properties of the function $\phi^L$.
% First we verify that $\phi^L$ is strongly convex.
\begin{proposition} \label{prop:phi-L-convex}
	$\phi^L$ is $\frac 1q$-strictly convex on $\cP([q])$.
\end{proposition}
% Proof is deferred to Section~\ref{sec:proof-bi:potential}.

% The following lemma shows that $\Phi^L$ behaves nicely under $\star$-convolution.
\begin{lemma} \label{lemma:phi-L-additive-under-star}
	$\Phi^L(\cdot) = C_{\SKL}(\cdot)$ is additive under $\star$-convolution.
\end{lemma}
Proofs of Prop.~\ref{prop:phi-L-convex} and Lemma~\ref{lemma:phi-L-additive-under-star} are deferred to Section~\ref{sec:proof-bi:potential}.

Condition~\eqref{eqn:thm:bi-low-snr-cond} implies the desired contraction.
\begin{proposition} \label{prop:phi-L-contraction}
	If \eqref{eqn:thm:bi-low-snr-cond} holds, then
	$
		\lim_{k\to \infty} \left(\Phi^L(M_{k}) - \Phi^L(\wt M_{k})\right) = 0.
	$
\end{proposition}
Proof is deferred to Section~\ref{sec:proof-bi:contraction}.

\begin{proof}[Proof of Theorem~\ref{thm:bi-low-snr}]
  By combining Prop.~\ref{prop:phi-L-contraction}, Prop.~\ref{prop:phi-L-convex}, and Prop.~\ref{prop:contraction-imply-bi}.
\end{proof}

\subsection{High SNR} \label{sec:high-snr}
For the high SNR case, we use Bhattacharyya coefficient as the potential function.
We define
\begin{align}
	\phi^H(\pi) = Z(\FSC_\pi^R) = \frac 1{q-1} \left(\left(\sum_{i\in [q]} \sqrt {\pi_i}\right)^2 -1\right).
\end{align}

We state a few properties of the function $\phi^H$.
% We verify that $\phi^H$ is strongly concave.
\begin{proposition} \label{prop:phi-H-concave}
	$\phi^H$ is $\alpha$-strongly concave on $\cP([q])$ for some $\alpha>0$.
\end{proposition}
% Proof is deferred to Section~\ref{sec:proof-bi:potential}.

% We show that $\Phi^H$ behaves nicely under $\star$-convolution.
\begin{lemma} \label{lemma:phi-H-mult-under-star}
	$\Phi^H(\cdot) = Z(\cdot)$ is multiplicative under $\star$-convolution.
\end{lemma}
Proofs of Prop.~\ref{prop:phi-H-concave} and Lemma~\ref{lemma:phi-H-mult-under-star} are deferred to Section~\ref{sec:proof-bi:potential}.

Condition~\eqref{eqn:thm:bi-high-snr-cond} implies the desired contraction.
\begin{proposition} \label{prop:phi-H-contraction}
  If \eqref{eqn:thm:bi-high-snr-cond} holds, then
	$
		\lim_{k\to \infty} \left(\Phi^H(M_{k}) - \Phi^H(\wt M_{k})\right) = 0.
	$
\end{proposition}
Proof is deferred to Section~\ref{sec:proof-bi:contraction}.

% \begin{lemma}\label{lemma:h2-hess-ratio}
%   Let $\pi \in \cP([q])$, and $v\in \mathbbm{1}^\perp \subseteq \bR^q$ be non-zero.
%   Then we have
%   \begin{align}
%     \frac{f(\lambda \pi + \frac {1-\lambda} q, v)}{f(\pi, v)} \le q^{5/2}
%   \end{align}
%   where $f(\pi, v) = -v^\top \nabla^2 \phi^H(\pi) v$ is defined in \eqref{eqn:h2-hess-form}.
% \end{lemma}

\begin{proof}[Proof of Theorem~\ref{thm:bi-high-snr}]
  Combine Prop.~\ref{prop:phi-H-contraction},~\ref{prop:phi-H-concave}, and~\ref{prop:contraction-imply-bi}.
\end{proof}


\section{Boundary irrelevance does not always hold} \label{sec:non-bi}
In this section we prove that boundary irrelevance does not hold for the Potts model between the reconstruction threshold and the Kesten-Stigum threshold.
\begin{proposition} \label{prop:no-robust-recon}
  In the setting of Theorem~\ref{thm:non-bi}, for all $\delta>0$, there exists $\epsilon>0$ such that for any FMS survey channel $W$ with $C_{\chi^2}(W) \le \epsilon$, we have
  \begin{align}
    \lim_{k\to \infty} I_{\chi^2}(\sigma_\rho; \omega_{T_k} | T_k) \le \delta.
  \end{align}
\end{proposition}

\begin{proof}[Proof of Theorem~\ref{thm:non-bi} given Prop.~\ref{prop:no-robust-recon}]
  Because we are in the reconstruction regime,
  \begin{align}
    \lim_{k\to \infty} I(\sigma_\rho; \sigma_{L_k}, \omega_{T_k} | T_k) \ge \lim_{k\to \infty} I(\sigma_\rho; \sigma_{L_k} | T_k) > 0.
  \end{align}

  Take $\delta>0$ such that $\delta \log 2 < \lim_{k\to \infty} I(\sigma_\rho; \sigma_{L_k} | T_k)$.
  Because $I\le I_{\chi^2} \log 2$, and by Prop.~\ref{prop:no-robust-recon}, for weak enough survey channel $W$ we have
  \begin{align}
    \lim_{k\to \infty} I(\sigma_\rho; \omega_{T_k} | T_k) \le \lim_{k\to \infty} I_{\chi^2}(\sigma_\rho; \omega_{T_k} | T_k) \log 2 \le \delta \log 2 < \lim_{k\to \infty} I(\sigma_\rho; \sigma_{L_k}, \omega_{T_k} | T_k).
  \end{align}
  Therefore
  \begin{align}
    \lim_{k\to \infty} I(\sigma_\rho; \sigma_{L_k} | T_k, \omega_{T_k})
    &= \lim_{k\to \infty} (I(\sigma_\rho; \sigma_{L_k}, \omega_{T_k} | T_k) - I(\sigma_\rho; \omega_{T_k} | T_k)) \\
    &= \lim_{k\to \infty} I(\sigma_\rho; \sigma_{L_k}, \omega_{T_k} | T_k) - \lim_{k\to \infty} I(\sigma_\rho; \omega_{T_k} | T_k)
    > 0. \nonumber
  \end{align}
\end{proof}

Proof of Prop.~\ref{prop:no-robust-recon} is deferred to Section~\ref{sec:proof-non-bi:no-robust-recon}.
As we mentioned in the introduction, the proof uses contraction and local subadditivity properties of the information measure $C_{\chi^2}$.

\begin{lemma}[Contraction] \label{lemma:contraction}
  For any FMS channel $P$, we have
  \begin{align}
    C_{\chi^2}(P\circ P_\lambda) \le \lambda^2 C_{\chi^2}(P).
  \end{align}
\end{lemma}
\begin{proof}
  % By reversibility and $\chi^2$-contraction coefficient: $\eta_{\chi^2}(M, \pi) = \lambda^2$.
  By reversibility and $\chi^2$-contraction coefficient: $\eta_{\chi^2}(P_\lambda, \Unif([q])) = \lambda^2$.
\end{proof}

\begin{lemma}[Local subadditivity] \label{lemma:subadd}
  There exists a constant $c=c(q)>0$ such that for any $\delta>0$ and $0<\epsilon \le c \delta^6$, the following holds: if $P$ and $Q$ are two FMS channels with $C_{\chi^2}(P) \le \epsilon$, $C_{\chi^2}(Q) \le \epsilon$, then
  \begin{align}
    C_{\chi^2}(P\star Q) \le (1+\delta) (C_{\chi^2}(P) + C_{\chi^2}(Q)).
  \end{align}
\end{lemma}
Proof is deferred to Section \ref{sec:proof-non-bi:subadd}.

% \begin{proof}[Proof of Theorem~\ref{thm:no-robust-recon}]
%   Choose $\delta > 0$ so that $(1+\delta)^b b \lambda^2 < 1$.
%   Let $\epsilon > 0$ be a constant such that Lemma~\ref{lemma:subadd} holds with parameters $(\delta=\delta, \epsilon=(1+\delta)^b \epsilon)$.

%   We prove by induction on $k$ that $I_{\chi^2}(\sigma_\rho; \nu_{L_k}) \le \epsilon$.
%   The base case $k=0$ is by
%   \begin{align}
%     I_{\chi^2}(\sigma_\rho; \nu_{L_0}) = C_{\chi^2}(U) \le \epsilon.
%   \end{align}

%   Induction step:
%   Let $M_k$ be the channel $\sigma_\rho \mapsto \nu_{L_k}$.
%   Then $M_{k+1} = (M_k\circ P_\lambda)^{\star b}$.

%   By Lemma~\ref{lemma:contraction},
%   \begin{align}
%     C_{\chi^2}(M_k\circ P_\lambda) \le \lambda^2 C_{\chi^2}(M_k).
%   \end{align}

%   Let us prove by induction that for $d\le b$,
%   \begin{align}
%     C_{\chi^2}((M_k\circ P_\lambda)^{\star d}) \le (1+\delta)^d d \lambda^2\epsilon.
%   \end{align}
%   Base case $d=0$ is trivial.
%   Induction step:
%   By induction hypothesis,
%   \begin{align}
%     C_{\chi^2}((M_k\circ P_\lambda)^{\star d}) \le (1+\delta)^d d \lambda^2 \epsilon \le (1+\delta)^b b \lambda^2 \epsilon \le \epsilon.
%   \end{align}
%   For $d+1\le b$, by Lemma~\ref{lemma:subadd},
%   \begin{align}
%     C_{\chi^2}((M_k\circ P_\lambda)^{\star (d+1)}) &\le (1+\delta) ((1+\delta)^d d \lambda^2 \epsilon + \lambda^2 \epsilon)\\
%     &\le (1+\delta)^{d+1} (d+1)\lambda^2 \epsilon.\nonumber
%   \end{align}
%   Therefore
%   \begin{align}
%     C_{\chi^2}(M_{k+1}) \le (1+\delta)^b b \lambda^2 \wt \epsilon \le \epsilon.
%   \end{align}
%   This completes the induction.

%   So for all small enough $\epsilon > 0$ we have
%   \begin{align}
%     \lim_{k\to \infty} I_{\chi^2}(\sigma_\rho; \nu_{L_k}) \le \epsilon.
%   \end{align}
%   Therefore
%   \begin{align}
%     \lim_{k\to \infty} I_{\chi^2}(\sigma_\rho; \nu_{L_k}) = 0.
%   \end{align}
% \end{proof}

% Acknowledgments---Will not appear in anonymized version
\ifdefined\isarxiv
\section*{Acknowledgments}
We thank Emmanuel Abbe, Elisabetta Cornacchia, Jingbo Liu, Youngtak Sohn for helpful discussions.

Research was sponsored by the United States Air Force Research Laboratory and the United States Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19-2-1000. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the United States Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.
\else
\acks{We thank a bunch of people and funding agency.\yuzhou{todo}}
\fi

\ifdefined\isarxiv
\bibliographystyle{alpha}
\fi
\bibliography{ref}

\appendix

\section{Proofs in Section~\ref{sec:fms}} \label{sec:proof-fms}
\begin{proof}[Proof of Prop.~\ref{prop:fms-mixture}]
  \textbf{Existence:}
  \textbf{Step 1.} We first prove that we can replace $P$ with an equivalent FMS channel whose $\Aut(\cX)$ action is free.
  Define channel $\wt P: \cX \to \cY \times \wt \cY$,
  where $\wt \cY = \Aut(\cX)$, sending $X$ to $(Y, \wt Y)$ where $Y\sim P(\cdot | X)$ and $\wt Y \sim \Unif(\Aut(\cX))$ is independent of $X$.
  We give $\wt P$ an FMS structure where $\Aut(\cX)$ acts on $\wt \cY$ by left multiplication.
  It is easy to see that $P$ is equivalent to $\wt P$.
  Therefore we can replace $P$ with $\wt P$ and wlog assume that $\Aut(\cX)$ action is free.

  \textbf{Step 2.}
  Let $\cO = \cY / \Aut(\cX)$ be the space of orbits of the $\Aut(\cX)$ action on $\cY$.
  For an orbit $o\in \cO$, for any two elements $y_1,y_2\in o$, the posterior distributions $\pi_1 = P_{X|Y=y_1}$ and $\pi_2 = P_{X|Y=y_2}$ differ by a permutation, by the assumption that $P$ is FMS.
  In particular, $\pi_1$ and $\pi_2$ map to the same element in $\cP(\cX)/\Aut(\cX)$.
  Therefore we can uniquely assign an element $\pi_o\in \cP(\cX)/\Aut(\cX)$ for any $o\in \cO$.

  Note that by symmetry, the distribution of $o$ does not depend on the input distribution. Let $P_o \in \cP(\cO)$ be this distribution.
  Then $P$ is equivalent to the channel $X\to (o, Z)$ where $o\in P_o$ is independent of $X$, and $Z\sim \FSC_{\pi_o}(\cdot | X)$.
  (Because $\Aut(\cX)$ action on $\cY$ is free, this equivalence is in fact just renaming the output space.)

  \textbf{Step 3.}
  Finally we prove that the FMS channel $X\to (o, Z)$ is equivalent to $X\to (\pi_o,Z)$.
  One side is easy: given $(o,Z)$, we can generate $(\pi_o,Z)$.
  For the other side, given $(\pi,Z)$, we can generate $o' \sim P_{o | \pi_o = \pi}$.
  Then $(o',Z)$ has the same distribution as $(o,Z)$, conditioned on any input distribution.
  This finishes the existence proof.

  \textbf{Uniqueness:}
  For any FMS channel $X\xrightarrow{Q}Y$, we can associate it with a distribution $Q_\pi$ on $\cP(\cX)/\Aut(\cX)$, defined as the distribution of the posterior distribution $Q_{X|Y}$, where $Y\sim Q_{Y|X} \circ \Unif(\cX)$ is generated with uniform prior distribution. (By definition $Q_\pi$ is a distribution on $\cP(\cX)$. However, by symmetry property of FMS, $Q_\pi$ is invariant under $\Aut(\cX)$ action.)
  It is easy to see that $Q_\pi$ distribution is preserved under equivalence between FMS channels.
  Furthermore, for a FMS channel of form $X\to (\pi,Z)$ as described in the proposition statement, this distribution of posterior distribution is equal to $P_\pi$.
  Therefore $P_\pi$ is uniquely deteremined by $P$.
\end{proof}

\begin{proof}[Proof of Prop.~\ref{prop:fms-degradation-coupling}]
  \textbf{Degradation $\Rightarrow$ Coupling:}
  Say $P$ maps $X$ to $Y$, and $Q$ maps $X$ to $Z$.
  Let $\pi'_P \in \cP(\cX)$ be the posterior distribution of input $X$ given output $Y$, where $Y\sim P_{Y|X} \circ \Unif(\cX)$ is generated with uniform prior distribution.
  Similarly define $\pi'_Q$.
  Then $\pi_P$ (resp.~$\pi_Q$) is the orbit of $\pi'_P$ (resp.~$\pi'_Q$) under permutation.

  Degradation relationship $P = R\circ Q$ induces a coupling on the posterior distributions $\pi'_P$ and $\pi'_Q$. One can check that this coupling is invariant under $\Aut(\cX)$ action and satisfying
  \begin{align}
    \pi' = \bE[\pi'_Q | \pi'_P = \pi'] \qquad \forall \pi' \in \cP(\cX).
  \end{align}
  For any $\pi' \in \cP(\cX)$, let $p(\pi')\in \cP(\cX)/\Aut(\cX)$ denotes its projection.
  Then we have
  \begin{align}
    p(\pi') \le_m \bE[p(\pi'_Q) | \pi'_P = \pi'].
  \end{align}
  Taking expectation over the orbit, we get
  \begin{align}
    \pi \le_m \bE[\pi_Q | \pi_P = \pi] \qquad \forall \pi \in \cP(\cX)/\Aut(\cX).
  \end{align}

  \textbf{Coupling $\Rightarrow$ Degradation:}
  \textbf{Step 1.} We prove that for $\pi,\pi'\in \cP(\cX)/\Aut(\cX)$,
  if $\pi \le_m \pi'$, then $\FSC_\pi \le_{\deg} \FSC_{\pi'}$.
  Because $\pi\le_m \pi'$, there exists $a\in \cP(\Aut(\cX))$ such that
  \begin{align}
    \pi_i = \sum_{\sigma \in \Aut(\cX)} a_\sigma \pi'_{\sigma^{-1}(i)} \qquad \forall i\in \cX.
  \end{align}
  For $\rho\in \Aut(\cX)$, we have
  \begin{align}
    \FSC_\pi(\rho | i) = \frac 1{(q-1)!} \pi_{\rho^{-1}(i)}
    = \sum_{\sigma\in \Aut(\cX)} a_\sigma \frac 1{(q-1)!} \pi'_{\sigma^{-1}\rho^{-1}(i)}
    = \sum_{\sigma\in \Aut(\cX)} a_\sigma \FSC_{\pi'}(\rho \sigma | i).
  \end{align}
  Therefore we can let $R$ map $\rho\sigma$ to $\rho$ with probability $a_\sigma$, for all $\sigma\in \Aut(\cX)$.
  This gives the desired degradation map $R$.

  \textbf{Step 2.}
  We use the FSC mixture representation (Prop.~\ref{prop:fms-mixture}).
  Suppose $P$ maps $X$ to $(\pi_P, Z_P)$, and $Q$ maps $X$ to $(\pi_Q, Z_Q)$.
  If
  \begin{align}
    \pi = \bE[\pi_Q | \pi_P = \pi] \qquad \forall \pi \in \cP(\cX)/\Aut(\cX),
  \end{align}
  then we can construct $R$ by mapping $\pi_Q$ to coupled $\pi_P$ (randomly), and keeping the $Z$ component.

  Now define a FMS channel $\wt P$ whose $\pi$-component is $f(\pi_P)$, where
  \begin{align}
    f(\pi) := \bE[\pi_Q | \pi_P = \pi].
  \end{align}
  Then by Step 1, $P\le_{\deg} \wt P$. By Step 2, $\wt P\le_{\deg} Q$.
  Therefore $P\le_{\deg} Q$.
\end{proof}

\section{Proofs in Section~\ref{sec:bi}} \label{sec:proof-bi}

\subsection{Properties of potential functions} \label{sec:proof-bi:potential}
\begin{proof}[Proof of Prop.~\ref{prop:phi-L-convex}]
	\begin{align}
		\nabla^2 \phi^L(\pi) = \diag\left(\pi^{-1} + \frac 1q \pi^{-2}\right) \succeq \frac 1q I.
	\end{align}
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lemma:phi-L-additive-under-star}]
  Follows from additivity of $\SKL$-divergence under $\star$-convolution \cite{kulske2009symmetric}.
  For completeness, we give a direct proof using~\eqref{eqn:star-formula} here.

  By FSC mixture decomposition (Prop.~\ref{prop:fms-mixture}), it suffices to prove that
  \begin{align}
    \Phi^L(\FSC_\pi \star \FSC_{\pi'}) = \Phi^L(\FSC_\pi) + \Phi^L(\FSC_{\pi'}).
  \end{align}
  We have
  \begin{align*}
    &~\Phi^L(\FSC_\pi \star \FSC_{\pi'}) \\
    =&~ \sum_{\tau \in \Aut([q])} \left(\frac 1{(q-1)!}\sum_{i\in [q]} \pi_i \pi'_{\tau(i)}\right) \phi^L(\pi \star_\tau \pi') \\
    =&~ \sum_{\tau \in \Aut([q])} \left(\frac 1{(q-1)!}\sum_{i\in [q]} \pi_i \pi'_{\tau(i)}\right)
    \sum_{j\in [q]} \left((\pi \star_\tau \pi')_j - \frac 1q\right)\log (\pi \star_\tau \pi')_j\\
    =&~ \sum_{\tau \in \Aut([q])} \left(\frac 1{(q-1)!}\sum_{i\in [q]} \pi_i \pi'_{\tau(i)}\right)
    \sum_{j\in [q]} \left(\frac {\pi_j \pi'_{\tau(j)}}{\sum_{k\in [q]} \pi_k \pi'_{\tau(k)}} - \frac 1q\right)\log \frac {\pi_j \pi'_{\tau(j)}}{\sum_{k\in [q]} \pi_k \pi'_{\tau(k)}}\\
    =&~ \sum_{\tau \in \Aut([q])} \frac 1{(q-1)!}
    \sum_{j\in [q]} \left(\pi_j \pi'_{\tau(j)} - \frac 1q \sum_{i\in [q]} \pi_i \pi'_{\tau(i)} \right)\log \frac {\pi_j \pi'_{\tau(j)}}{\sum_{k\in [q]} \pi_k \pi'_{\tau(k)}} \\
    =&~ \sum_{\tau \in \Aut([q])} \frac 1{(q-1)!}
    \sum_{j\in [q]} \left(\pi_j \pi'_{\tau(j)} - \frac 1q \sum_{i\in [q]} \pi_i \pi'_{\tau(i)} \right)\log (\pi_j \pi'_{\tau(j)}) \\
    =&~ \sum_{j\in [q]} (\pi_j-\frac 1q)\log \pi_j + \sum_{j\in [q]} (\pi'_j-\frac 1q)\log \pi'_j \\
    =&~ \Phi^L(\FSC_\pi) + \Phi^L(\FSC_{\pi'}).
  \end{align*}
\end{proof}

\begin{proof}[Proof of Prop.~\ref{prop:phi-H-concave}]
	For any $\pi \in \cP([q])$ we have
	\begin{align}
		\nabla^2 \phi^H(\pi) = \frac 1{q-1} \left(\frac 12 \left(\pi^{-1/2}\right) \left(\pi^{-1/2}\right)^\top - \frac 12 \left(\sum_{i\in [q]} \pi^{1/2}_i\right)\diag\left(\pi^{-3/2}\right)\right).
	\end{align}
	So for any $v\in \mathbbm{1}^\perp \subseteq \bR^q$,
  \begin{align}
		v^\top \nabla^2 \phi^H(\pi) v &= \frac 1{2(q-1)} \left(\left\langle \pi^{-1/2}, v\right\rangle^2 - \left(\sum_{i\in [q]} \pi^{1/2}_i\right) \left\langle \pi^{-3/2}, v^2\right\rangle\right).
  \end{align}

	Let us prove that
	\begin{align}
		\frac{\left\langle \pi^{-1/2}, v\right\rangle^2}{\left(\sum_{i\in [q]} \pi^{1/2}_i\right) \left\langle \pi^{-3/2}, v^2\right\rangle} \le 1-\frac{1}{\sqrt q}. \label{eqn:h2-hess-ratio-step-1}
	\end{align}
  Performing change of variable $u = \pi^{-3/4} v$, LHS of \eqref{eqn:h2-hess-ratio-step-1} becomes
	\begin{align}
		\frac{\left\langle \pi^{1/4}, u\right\rangle^2}{\left(\sum_{i\in [q]} \pi^{1/2}_i\right) \|u\|_2^2}.
		\label{eqn:h2-hess-ratio-step-1-cov}
	\end{align}
	We would like to maximize this expression over the hyperplane $\left\langle \pi^{3/4}, u\right\rangle=0$.
	By geometric interpretation, maximum value is achieved at projection of $\pi^{1/4}$ onto the hyperplane, i.e.,
	\begin{align}
		u = \pi^{1/4} - \frac{\left\langle \pi^{1/4}, \pi^{3/4}\right\rangle}{\|\pi^{3/4}\|_2^2} \pi^{3/4} = \pi^{1/4} - \frac{\pi^{3/4}}{\|\pi^{3/4}\|_2^2},
	\end{align}
	at which \eqref{eqn:h2-hess-ratio-step-1-cov} achieves value
	\begin{align}
		& \frac{\left( \sum_{i\in [q]} \pi_i^{1/2} - \frac 1{\sum_{i\in [q]} \pi_i^{3/2}}\right)^2}{\left(\sum_{i\in [q]} \pi_i^{1/2}\right) \left( \sum_{i\in [q]} \pi_i^{1/2} - \frac 1{\sum_{i\in [q]} \pi_i^{3/2}}\right)} \\
		& = \frac{\sum_{i\in [q]} \pi_i^{1/2} - \frac 1{\sum_{i\in [q]} \pi_i^{3/2}}}{\sum_{i\in [q]} \pi_i^{1/2}} \nonumber \\
		& = 1 - \frac 1{\left(\sum_{i\in [q]} \pi_i^{1/2}\right) \left(\sum_{i\in [q]} \pi_i^{3/2}\right)} \nonumber \\
		& \le 1-\frac 1{\sqrt q} \nonumber
	\end{align}
	where the last step is because
	\begin{align}
		\sum_{i\in [q]} \pi_i^{1/2} \le \sqrt q, \qquad \sum_{i\in [q]} \pi_i^{3/2} \le 1.
	\end{align}
  This finishes the proof of~\eqref{eqn:h2-hess-ratio-step-1}.

  Therefore
  \begin{align}
    v^\top \nabla^2 \phi^H(\pi) v &= \frac 1{2(q-1)} \left(\left\langle \pi^{-1/2}, v\right\rangle^2 - \left(\sum_{i\in [q]} \pi^{1/2}_i\right) \left\langle \pi^{-3/2}, v^2\right\rangle\right) \\
		&\le  -\frac 1{2(q-1)\sqrt q} \left(\sum_{i\in [q]} \pi^{1/2}_i\right) \left\langle \pi^{-3/2}, v^2\right\rangle\nonumber \\
		&\le -\frac 1{2(q-1)\sqrt q} \|v\|_2^2 \nonumber
	\end{align}
	where the second step is by~\eqref{eqn:h2-hess-ratio-step-1}, and the third step is because $\sum_{i\in [q]} \pi_i^{1/2} \ge 1$ and $\pi^{-3/2} \ge 1$.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lemma:phi-H-mult-under-star}]
  Follows from tensorization property of Hellinger distance.
  For completeness, we give a direct proof using~\eqref{eqn:star-formula} here.

  By FSC mixture decomposition (Prop.~\ref{prop:fms-mixture}), it suffices to prove that
  \begin{align}
    \Phi^H(\FSC_\pi \star \FSC_{\pi'}) = \Phi^H(\FSC_\pi) + \Phi^H(\FSC_{\pi'}).
  \end{align}
  We have
  \begin{align*}
    &~\Phi^H(\FSC_\pi \star \FSC_{\pi'}) \\
    =&~ \sum_{\tau \in \Aut([q])} \left(\frac 1{(q-1)!}\sum_{i\in [q]} \pi_i \pi'_{\tau(i)}\right) \phi^H(\pi \star_\tau \pi') \\
    =&~ \sum_{\tau \in \Aut([q])} \left(\frac 1{(q-1)!}\sum_{i\in [q]} \pi_i \pi'_{\tau(i)}\right)
    \frac 1{q-1} \left(\left(\sum_{j\in [q]} \sqrt{(\pi \star_\tau \pi')_j}\right)^2-1\right)\\
    =&~ \sum_{\tau \in \Aut([q])} \left(\frac 1{(q-1)!}\sum_{i\in [q]} \pi_i \pi'_{\tau(i)}\right)
    \frac 1{q-1} \left(\left(\sum_{j\in [q]} \sqrt{\frac {\pi_j \pi'_{\tau(j)}}{\sum_{k\in [q]} \pi_k \pi'_{\tau(k)}}}\right)^2-1\right)\\
    =&~ \sum_{\tau \in \Aut([q])} \frac 1{(q-1)!}
    \cdot \frac 1{q-1} \left(\left(\sum_{j\in [q]} \sqrt{\pi_j \pi'_{\tau(j)}}\right)^2-\sum_{i\in [q]} \pi_i \pi'_{\tau(i)}\right)\\
    =&~ \sum_{\tau \in \Aut([q])} \frac 1{(q-1)!}
    \cdot \frac 1{q-1} \sum_{j\ne k\in [q]} \sqrt{\pi_j \pi'_{\tau(j)} \pi_k \pi'_{\tau(k)}}\\
    =&~ \frac 1{(q-1)^2} \left(\sum_{j\ne k\in [q]} \sqrt{\pi_j \pi_k}\right)\left(\sum_{j'\ne k'\in [q]}\sqrt{\pi'_{\tau(j')} \pi'_{\tau(k')}}\right)\\
    =&~ \Phi^H(\FSC_\pi) \Phi^H(\FSC_{\pi'}).
  \end{align*}
\end{proof}

\begin{lemma}\label{lemma:h2-chi2-bound}
	For any BMS channel $P$, we have
	\begin{align}
		Z(P) \le \sqrt{1-C_{\chi^2}(P)}.
	\end{align}
\end{lemma}
\begin{proof}
	Let $\Delta$ be the $\Delta$-component of $P$. Then
	\begin{align}
		Z(P) = \bE [2\sqrt{\Delta(1-\Delta)}] \le \sqrt{1-\bE[(1-2\Delta)^2]} = \sqrt{1-C_{\chi^2}(P)}.
	\end{align}
	The inequality step is by concavity of $\sqrt{\cdot}$.
\end{proof}

\subsection{Contraction of potential functions} \label{sec:proof-bi:contraction}
\begin{proof}[Proof of Prop.~\ref{prop:phi-L-contraction}]
  Using BP equation and Lemma~\ref{lemma:phi-L-additive-under-star}, we get
	\begin{align}
		\Phi^L(M_{k+1}) = \bE_b \left[b \Phi^L(M_k\circ P_\lambda) + \Phi^L(W)\right] = d \Phi^L(M_k\circ P_\lambda) + \Phi^L(W),
	\end{align}
	and the same holds with $M$ replaced with $\wt M$.

	To prove that
	\begin{align}
		\Phi^L(M_{k+1}) - \Phi^L(\wt M_{k+1}) \le c \left(\Phi^L(M_{k}) - \Phi^L(\wt M_{k})\right),
	\end{align}
	for some $c<1$, it suffices to prove that
	\begin{align}
		d \Phi^L(\FSC_\pi \circ P_\lambda) - c \Phi^L(\FSC_\pi)
		= d \phi^L(\lambda \pi + \frac{1-\lambda}q) - c \phi^L(\pi)
	\end{align}
	is concave in $\pi$.

  Let $c = d \lambda^2 C^L(\lambda, q)$.
  Then for all $v\in \mathbbm{1}^\perp \in \bR^q$, we have
	\begin{align}
		v^\top \nabla^2 \left(d \phi^L(\lambda \pi + \frac{1-\lambda}q) - c \phi^L(\pi)\right) v
    = d \lambda^2 f^L(\lambda \pi + \frac{1-\lambda}q, v) - c f^L(\pi, v)
    \le 0
	\end{align}
  where the first step is because $v^\top \nabla^2 \phi^L(\pi) v = f^L(\pi, v)$ and
  the second step is by definition of $C^L$.
	Therefore contraction holds.
\end{proof}

\begin{proof}[Proof of Prop.~\ref{prop:phi-H-contraction}]
	We treat the regular tree case and the Poisson tree case (almost) uniformly.
	\begin{align}
		\Phi^H(M_{k+1}) = \bE_b\left[ \left(\Phi^H(M_k \circ P_\lambda)\right)^b \Phi^H(W)\right]
	\end{align}
	and the same holds with $M$ replaced with $\wt M$.

	For $i\ge 0$, define
	\begin{align}
		\Phi_i = \bE_b\left[ \prod_{j\in [b]} \left(\left(\Phi^H(\wt M_k \circ P_\lambda)\right)^{\mathbbm{1}\{j\le i\}} \left(\Phi^H(M_k \circ P_\lambda)\right)^{\mathbbm{1}\{j > i\}} \right) \Phi^H(W)\right].
	\end{align}

	Fix $i\ge 1$. Let us prove that
	\begin{align}
		\Phi_i - \Phi_{i-1} \le c_i \left( \Phi(\wt M_k) - \Phi(M_k)\right)
	\end{align}
	for some constant $c_i$ to be determined later.

	Note that
	\begin{align}
		\Phi_i - \Phi_{i-1}
		& = \left(\Phi^H(\wt M_k \circ P_\lambda) - \Phi^H(M_k \circ P_\lambda) \right)
		\\ & \cdot \bE_b \left[\mathbbm{1}\{b\ge i\}\left(\Phi^H(\wt M_k \circ P_\lambda)\right)^{i-1} \left(\Phi^H(M_k \circ P_\lambda)\right)^{b-i}\right] \Phi^H(W). \nonumber
	\end{align}

	% Define
	% \begin{align}
	% 	f(\pi, v) = -v^\top \nabla^2 \phi^H(\pi) v \label{eqn:h2-hess-form}
	% \end{align}
  % (which is non-negative by Prop.~\ref{prop:phi-H-concave})
	% and
	% \begin{align}
	% 	c = \sup_{\substack{\pi \in \cP([q]) \\ v \in \mathbbm{1}^\perp \subseteq \bR^q}} \frac{f(\lambda \pi + \frac {1-\lambda} q, v)}{f(\pi, v)}. \label{eqn:h2-hess-ratio-const}
	% \end{align}

  % By Lemma~\ref{lemma:h2-hess-ratio}, we have
  % \begin{align}
  %   c \le q^{5/2}.
  % \end{align}

  Note that $f^H(\pi, v) = -v^\top \nabla^2 \phi^H(\pi) v$.
  Therefore by definition of $C^H(\lambda, q)$, we have
	\begin{align}
		\nabla^2 \left(\Phi^H(\FSC_\pi \circ P_\lambda) - \lambda^2 C^H(\lambda, q) \Phi^H (\FSC_\pi)\right) \succeq 0.
	\end{align}
	So by degradation,
	\begin{align}
		\Phi^H(\wt M_k \circ P_\lambda) - \Phi^H(M_k \circ P_\lambda) \le \lambda^2 C^H(\lambda, q) \left(\Phi^H(\wt M_k) - \Phi^H(M_k)\right).
	\end{align}

	Let
	\begin{align}
		c_i = \lambda^2 \Phi^H(W) C^H(\lambda, q) \cdot \left\{\begin{array}{ll} \bE_b \left[\mathbbm{1}\{b\ge i\} \left(1-\frac{d\lambda^2-1}{d-1}\right)^{\frac{b-1}2} \right] & \text{Regular tree case,} \\ \bE_b \left[\mathbbm{1}\{b\ge i\} \left(1-\frac{d\lambda^2-1}{d}\right)^{\frac{b-1}2} \right] & \text{Poisson tree case.}\end{array}\right.
	\end{align}

  By Prop.~\ref{prop:bot-majority} and Lemma \ref{lemma:h2-chi2-bound},
	\begin{align}
		\Phi_i - \Phi_{i-1} \le c_i \left(\Phi^H(\wt M_k) - \Phi^H(M_k)\right).
	\end{align}

	Then
	\begin{align}
		\sum_{i\ge 1} c_i &= d \lambda^2 \Phi^H(W) C^H(\lambda, q) \left\{\begin{array}{ll}
			\left(1-\frac{d\lambda^2-1}{d-1}\right)^{\frac{d-1}2} & \text{Regular tree case,} \\
			\exp\left(-d\left(1-\sqrt{1-\frac{d\lambda^2-1}{d}}\right)\right) & \text{Poisson tree case.}
		\end{array}\right. \\
		% &\le d \lambda^2 \Phi^H(W) q^{5/2} \exp\left(-\frac{d \lambda^2-1}{2}\right)
	\end{align}
  % where the last step is by Lemma~\ref{lemma:h2-hess-ratio}.

	Then
	\begin{align}
		&\Phi^H(\wt M_{k+1}) - \Phi^H(M_{k+1}) \\
		&\le \left(\sum_{i\ge 1} c_i\right) \left(\Phi^H(\wt M_k) - \Phi^H(M_k)\right) \nonumber\\
		&\le d \lambda^2 \Phi^H(W) C^H(\lambda, q) \exp\left(-\frac{d \lambda^2-1}{2}\right)  \left(\Phi^H(\wt M_k) - \Phi^H(M_k)\right) \nonumber
	\end{align}
  and the desired contraction holds.
\end{proof}

\subsection{Majority decider} \label{sec:proof-bi:majority}
\begin{proposition}\label{prop:bot-majority}
	Consider the Potts model with leaf observations through a non-trivial FMS channel $U$.
	Let $M_k^U$ denote the channel $\sigma_\rho \to \nu_{L_k}$ where $\nu_v \sim P_\eta(\sigma_v)$.
  Assume that $d\lambda^2>1$.
	Then
	\begin{align}
		\lim_{k\to \infty} C_{\chi^2}\left((M_k^U \circ P_\lambda)^R\right) \ge \left\{\begin{array}{ll} c(q,d,\lambda)\cdot \frac{d\lambda^2-1}{d-1} & \text{Regular tree case,} \\ c(q,d,\lambda) \cdot \frac{d\lambda^2-1}{d} & \text{Poisson tree case.}\end{array}\right.
	\end{align}
  where
  \begin{align}
    c(q,d,\lambda) := \left(\frac 2q + \frac{q-2}q \cdot \frac{d\lambda^2-1}{d\lambda-1}\right)^{-1}.
  \end{align}

  Furthermore, $c(q,d,\lambda) \ge 1$ for all $\lambda \in [-\frac 1{q-1},1]$ and $d\lambda^2>1$.
\end{proposition}

\begin{proof}
  Let $U^*$ be the reverse channel of $U$.
  Then the composition $U^*\circ U$ is a non-trivial Potts channel.
  So there exists $\eta\ne 0$ such that $P_\eta \le_{\deg} U$.
  By replacing $U$ with $P_\eta$ (and using Lemma~\ref{lemma:fms-info-measure-monotone}), we can wlog assume that $U=P_\eta$ for some $\eta\ne 0$.

	Fix any embedding $\{\pm\} \subseteq [q]$.
	Let $e\in \bR^q$ denote the vector with $e_+ = 1, e_-=-1, e_i=0$ for $i\not \in \{\pm\}$.
	Let
	\begin{align}
		S_k = \sum_{v\in L_k} e_{\nu_v}.
	\end{align}
	We view $S_k$ as a channel $[q] \to \bZ$.
	By variational characterization of $\chi^2$-divergence, we have
	\begin{align}
		C_{\chi^2}((M_k^U \circ P_\lambda)^R) \ge \frac{\left(\bE^+[S_k \circ P_\lambda]\right)^2}{\bE^+[S_k^2 \circ P_\lambda]}
	\end{align}
	where $\bE^+$ denotes expectation conditioned on root color being $+$.
	Similarly, we use $\bE^-$ to denote expectation conditioned on root color being $-$, and use $\bE^0$ to denote expectation conditioned on root coloring being any color not $\pm$.
	Same for $\Var^+$, $\Var^-$, $\Var^0$.

	For simplicity, in this proof only, let us use the following notation.
	Let $\mathbbm{1}_R$ be $1$ if we are working with regular trees, and $0$ otherwise.
	Let $\mathbbm{1}_P$ be $1$ if we are working with Poisson trees, and $0$ otherwise.
  Clearly $\mathbbm{1}_P + \mathbbm{1}_R = 1$.

	It is easy to see that
	\begin{align}
		\bE^i S_k = e_i \eta (d\lambda)^k.
	\end{align}

	Using variance decomposition formula, we have
	\begin{align}
		\Var^i(S_{k+1}) & = \Var^i(\bE[S_{k+1} | b]) + \bE_b \Var^i(\bE[S_{k+1} | b, \sigma_1, \ldots, \sigma_b])\\
		& + \bE \Var^i( S_{k+1} | b, \sigma_1,\ldots,\sigma_b) \nonumber
	\end{align}
	where $\sigma_1,\ldots, \sigma_b$ are colors of children.

	Let us compute each summand.
	\begin{align}
		\Var^i(\bE[S_{k+1} | b]) = \Var^i(b \lambda e_i \eta (d\lambda)^k)
		= e_i^2 d \lambda^2 \eta^2 (d\lambda)^{2k} \mathbbm{1}_P.
	\end{align}
	\begin{align}
		\bE_b \Var^i (\bE[S_{k+1} | b, \sigma_1, \ldots, \sigma_b])
		&= d \eta^2 (d\lambda)^{2k} \Var_{j\sim M(\cdot | i)}(e_j).
	\end{align}
	\begin{align}
		\bE \Var^i( S_{k+1} | b, \sigma_1,\ldots,\sigma_b)
		& = d \bE_{j\sim M(\cdot | i)}[\Var^j(S_k)].
	\end{align}

	We have $\Var^-(S_k) = \Var^+(S_k)$ and
	\begin{align}\label{eqn:maj-var-plus-recursion}
		\Var^+(S_{k+1}) &= d \eta^2 (d\lambda)^{2k} \left((\lambda + \frac{1-\lambda}q\cdot 2) - \lambda^2 \mathbbm{1}_R\right)\\
		&+ d \left(\left(\lambda + \frac{1-\lambda}q\cdot 2\right) \Var^+(S_k) + \frac{1-\lambda}q\cdot (q-2) \Var^0(S_k)\right). \nonumber
	\end{align}
	\begin{align}\label{eqn:maj-var-0-recursion}
		\Var^0(S_{k+1}) &= d \eta^2 (d\lambda)^{2k} \left(\frac{1-\lambda}q\cdot 2\right)\\
		&+ d \left(\frac{1-\lambda}q\cdot 2 \Var^+(S_k) + \left(\lambda + \frac{1-\lambda}q\cdot (q-2)\right) \Var^0(S_k)\right). \nonumber
	\end{align}
	By computing linear combinations of \eqref{eqn:maj-var-plus-recursion}\eqref{eqn:maj-var-0-recursion}, we get
	\begin{align}
		\Var^+(S_{k+1}) - \Var^0(S_{k+1}) &= d \lambda \left(\Var^+(S_k) - \Var^0(S_k) \right)\\
		& + d \eta^2 (d\lambda)^{2k} (\lambda - \lambda^2 \mathbbm{1}_R). \nonumber
	\end{align}
	\begin{align}
		\Var^+(S_{k+1}) + \frac{q-2}2 \Var^0(S_{k+1})
		&= d \left(\Var^+(S_k) + \frac{q-2}2 \Var^0(S_{k+1})\right) \\
		&+ d \eta^2 (d\lambda)^{2k} (1-\lambda^2 \mathbbm{1}_R). \nonumber
	\end{align}

	Then
	\begin{align}
		& \Var^+(S_{k}) - \Var^0(S_{k})\\
		&= \left(\Var^+(S_0) - \Var^0(S_0)\right) (d\lambda)^k
		+ \sum_{1\le i\le k} d \eta^2(d\lambda)^{2i-2}(d\lambda)^{k-i} (\lambda - \lambda^2 \mathbbm{1}_R) \nonumber\\
		& = O_k\left((d\lambda)^k\right) + d \eta^2 (d\lambda)^{k-1} \frac{(d\lambda)^k-1}{d\lambda-1} (\lambda - \lambda^2 \mathbbm{1}_R) \nonumber\\
		& = (1+o_k(1)) \frac{1-\lambda \mathbbm{1}_R}{d\lambda-1} \eta^2 (d\lambda)^{2k}.\nonumber
	\end{align}
	\begin{align}
		&\Var^+(S_k) + \frac{q-2}2 \Var^0(S_k) \\
		&= \left(\Var^+(S_0) + \frac{q-2}2 \Var^0(S_0)\right) d^k
		+ \sum_{1\le i\le k} d \eta^2 (d\lambda)^{2i-2} d^{k-i} (1-\lambda^2 \mathbbm{1}_R) \nonumber\\
		&= O_k\left(d^k\right) + \eta^2 d^k \frac{(d\lambda^2)^k-1}{d\lambda^2-1} (1-\lambda^2 \mathbbm{1}_R) \nonumber\\
		& = (1+o_k(1)) \frac{1-\lambda^2 \mathbbm{1}_R}{d\lambda^2-1} \eta^2 (d\lambda)^{2k}. \nonumber
	\end{align}
	\begin{align}
		\Var^+(S_k) = (1+o_k(1)) \frac 2q \left(\frac{1-\lambda^2 \mathbbm{1}_R}{d\lambda^2-1} + \frac{1-\lambda \mathbbm{1}_R}{d\lambda-1} \cdot \frac{q-2}2\right) \eta^2 (d\lambda)^{2k}.
	\end{align}

	Now let us compute moments of $S_k \circ P_\lambda$.
	\begin{align}
		\bE^+[S_k \circ P_\lambda] = \lambda \eta (d\lambda)^k.
	\end{align}
	\begin{align}
		\bE^+[S_k^2 \circ P_\lambda] &= \left(\lambda + \frac{1-\lambda}q \cdot 2\right) \bE^+[S_k^2] + \frac{1-\lambda}q \cdot (q-2) \bE^0[S_k^2]\\
		&= \left(\lambda + \frac{1-\lambda}q \cdot 2\right) \left(\Var^+(S_k^2) + (\bE^+ S_k)^2\right) + \frac{1-\lambda}q \cdot (q-2) \Var^0(S_k^2) \nonumber\\
		&= \lambda (1+o_k(1)) \frac 2q \left(\frac{1-\lambda^2\mathbbm{1}_R}{d\lambda^2-1} + \frac{1-\lambda\mathbbm{1}_R}{d\lambda-1} \cdot \frac{q-2}2\right) \eta^2 (d\lambda)^{2k} \nonumber\\
		&+ \left(\lambda + \frac{1-\lambda}q \cdot 2\right) \eta^2 (d\lambda)^{2k}
		+ \frac{1-\lambda}q \cdot 2 \cdot (1+o_k(1)) \frac{1-\lambda^2 \mathbbm{1}_R}{d\lambda^2-1} \eta^2 (d\lambda)^{2k} \nonumber \\
		% & \le (1+o_k(1)) \left(\lambda + \frac{1-\lambda}q \cdot 2\right) \frac{d\lambda^2-\lambda^2\mathbbm{1}_R}{d\lambda^2-1} \eta^2 (d\lambda)^{2k} \nonumber.
    & = (1+o_k(1))\left(\frac 2q \cdot \frac{d\lambda^2-\lambda^2\mathbbm{1}_R}{d\lambda^2-1} + \lambda \cdot \frac{q-2}q \cdot \frac{d\lambda-\lambda \mathbbm{1}_R}{d\lambda-1}\right) \eta^2 (d\lambda)^{2k} \nonumber \\
    % & = (1+o_k(1))\left(\frac 2q \cdot \frac{1}{d\lambda^2-1} + \lambda \cdot \frac{q-2}q \cdot \frac 1{d\lambda^2-\lambda}\right) (d-\mathbbm{1}_R) \lambda^2 \eta^2 (d\lambda)^{2k}. \nonumber
    & = (1+o_k(1))c(q,d,\lambda)^{-1} \frac{d-\mathbbm{1}_R}{d\lambda^2-1} \lambda^2 \eta^2 (d\lambda)^{2k}. \nonumber
	\end{align}
	% (The inequality step uses $\frac{1-\lambda\mathbbm{1}_R}{d\lambda-1} \le \frac{1-\lambda^2\mathbbm{1}_R}{d\lambda^2-1}$.)
	Finally,
	\begin{align}
		C_{\chi^2}((M_k^U \circ P_\lambda)^R)
    \ge \frac{\left(\bE^+[S_k \circ P_\lambda]\right)^2}{\bE^+[S_k^2 \circ P_\lambda]}
		=  (1+o_k(1)) c(q,d,\lambda) \cdot \frac{d\lambda^2-1}{d-\mathbbm{1}_R}.
		% & \ge \frac{(1+o_k(1)) (d\lambda^2-1)}{d-\mathbbm{1}_R}. \nonumber
	\end{align}
\end{proof}

\subsection{Bounds on key constants} \label{sec:proof-bi:const}
In this section we prove bounds on key constants used in Theorem~\ref{thm:bi-low-snr} and~\ref{thm:bi-high-snr}.

\begin{proposition} \label{prop:bi-low-snr-C-bound}
  For $q\in \bZ_{\ge 2}$, $\lambda \in [-\frac 1{q-1},1]$, we have $C^L(\lambda, q) \le q^2$,
  where $C^L(\lambda, q)$ is defined in~\eqref{eqn:thm:bi-low-snr-defn-C}.
\end{proposition}
\begin{proof}
We have
\begin{align}
  & \frac{\left\langle \left(\lambda\pi+\frac{1-\lambda}q\right)^{-1}+q \left(\lambda\pi+\frac{1-\lambda}q\right)^{-2}, v^2\right\rangle}{\left\langle \pi^{-1} + q\pi^{-2}, v^2 \right\rangle} \\
  & \le \max\left\{\frac{\left\langle \left(\lambda\pi+\frac{1-\lambda}q\right)^{-1},v^2\right\rangle}{\left\langle \pi^{-1}, v^2 \right\rangle}, \frac{\left\langle\left(\lambda\pi+\frac{1-\lambda}q\right)^{-2}, v^2\right\rangle}{\left\langle\pi^{-2},v^2\right\rangle}\right\} \nonumber \\
  & \le \max\{q,q^2\}=q^2. \nonumber
\end{align}
where the second step is by Lemma~\ref{lemma:quad-form-ratio}.
\end{proof}

\begin{proposition} \label{prop:bi-high-snr-C-bound}
  For $q\in \bZ_{\ge 2}$, $\lambda \in [-\frac 1{q-1},1]$, we have
  $C^H(\lambda, q) \le q^{5/2}$ for all $\lambda \in [-\frac1{q-1},1]$,
  where $C^H(\lambda, q)$ is defined in~\eqref{eqn:thm:bi-high-snr-defn-C}.
\end{proposition}\begin{proof}
	By \eqref{eqn:h2-hess-ratio-step-1},
	\begin{align}
		f(\pi, v) \ge \frac 1{\sqrt q} \left(\sum_{i\in [q]} \pi^{1/2}_i\right) \left\langle \pi^{-3/2}, v^2\right\rangle \ge \frac 1{\sqrt q} \left\langle \pi^{-3/2}, v^2\right\rangle.
	\end{align}
	On the other hand,
	\begin{align}
		f(\lambda \pi + \frac {1-\lambda}{q}, v)
		& \le \left(\sum_{i\in [q]} \left(\lambda \pi_i + \frac {1-\lambda}{q}\right)^{1/2}\right) \left\langle \left(\lambda \pi + \frac {1-\lambda}{q}\right)^{-3/2}, v^2\right\rangle \\
    & \le \sqrt q \cdot \left\langle \left(\lambda \pi + \frac {1-\lambda}{q}\right)^{-3/2}, v^2\right\rangle. \nonumber
		% & \le \sqrt q  \cdot \left\langle \left(\frac{\pi}q\right)^{-3/2}, v^2\right\rangle \nonumber\\
		% & = q^2 \left\langle \pi^{-3/2}, v^2\right\rangle. \nonumber
	\end{align}
	Combining the above two inequalities we get
	\begin{align}
		% c = \sup_{\substack{\pi \in \cP([q]) \\ v \in \mathbbm{1}^\perp \subseteq \bR^q}}
    \frac{f(\lambda \pi + \frac {1-\lambda} q, v)}{f(\pi, v)} \le \frac{\left\langle \left(\lambda \pi + \frac {1-\lambda}{q}\right)^{-3/2}, v^2\right\rangle}{\left\langle \pi^{-3/2}, v^2\right\rangle}\le q^{5/2}
	\end{align}
  where the last step is by Lemma~\ref{lemma:quad-form-ratio}.
\end{proof}



The following lemma is the crucial step in the proof of Prop.~\ref{prop:bi-low-snr-C-bound} and~\ref{prop:bi-high-snr-C-bound}.
\begin{lemma} \label{lemma:quad-form-ratio}
  For $q\in \bZ_{\ge 2}$, $\alpha \in \bR_{\ge 1}$, $\lambda \in [-\frac 1{q-1},1]$, we have
  \begin{align}
    \sup_{\substack{\pi \in \cP([q]) \\ v \in \mathbbm{1}^\perp \subseteq \bR^q}} \frac{\left\langle\left(\lambda \pi + \frac {1-\lambda} q\right)^{-\alpha}, v^2\right\rangle}{\left\langle \pi^{-\alpha}, v^2\right\rangle} \le q^\alpha.
  \end{align}
\end{lemma}

\begin{proof}
  We prove the ferromagnetic case $(\lambda \in [0,1])$ and anti-ferromagnetic case $(\lambda \in [-\frac 1{q-1},0])$ separately.

  \textbf{Ferromagnetic case $(\lambda\in [0,1])$.}
  In this case, we have
  \begin{align}
    \lambda x + \frac {1-\lambda} q \ge \frac xq
  \end{align}
  for all $x\in [0, 1]$.
  Therefore
  \begin{align}
    \left\langle\left(\lambda \pi + \frac {1-\lambda} q\right)^{-\alpha}, v^2\right\rangle \le \left\langle\left(\frac \pi q\right)^{-\alpha}, v^2\right\rangle = q^\alpha \left\langle \pi^{-\alpha}, v^2\right\rangle.
  \end{align}
  Note that we did not use the assumption that $v\in \mathbbm{1}^\perp$.

  \textbf{Anti-ferromagnetic case $(\lambda\in [-\frac 1{q-1},0])$.}
  We would like to prove that
  \begin{align} \label{eqn:lemma-quad-form-ratio-diff}
    q^\alpha \left\langle \pi^{-\alpha}, v^2\right\rangle - \left\langle\left(\lambda \pi + \frac {1-\lambda} q\right)^{-\alpha}, v^2\right\rangle =: \left\langle b, v^2\right\rangle
  \end{align}
  is non-negative for all $\pi \in \cP([q])$, $v\in \mathbbm{1}^\perp$,
  where
  \begin{align}
    b := \left(\frac \pi q\right)^{-\alpha} - \left(\lambda \pi + \frac {1-\lambda} q\right)^{-\alpha}.
  \end{align}

  \textbf{Step 1.}
  We fix $\pi \in \cP([q])$ and determine the optimal $v\in \mathbbm{1}^\perp$ to plug in~\eqref{eqn:lemma-quad-form-ratio-diff}, reducing the statement to one involving $\pi$ only.

  If $\lambda x + \frac {1-\lambda}q \le \frac xq$ for some $x\in [0,1]$, then
  $x\ge \frac{1-\lambda}{1-\lambda q} \ge \frac{q}{2q-1} > \frac 12$.
  So there exists at most one $i$ such that
  $\lambda \pi_i + \frac {1-\lambda}q \le \frac {\pi_i} q$ (equivalently, $b_i \le 0$).
  We can wlog assume that $\pi_1 \ge \pi_2 \ge \cdots \ge \pi_q$.
  Then we know $\lambda \pi_i + \frac {1-\lambda}q > \frac {\pi_i} q$ (equivalently, $b_i > 0$) for all $2\le i\le q$.
  If $b_1\ge 0$, then~\eqref{eqn:lemma-quad-form-ratio-diff} is non-negative for all $v$ and we are done.
  Therefore, it remains to consider the case $b_1 < 0$.

  If $v_1=0$, then~\eqref{eqn:lemma-quad-form-ratio-diff} is non-negative.
  Therefore we can assume $v_1\ne 0$. By rescaling, we can assume that $v_1=1$.
  So $v_2+\cdots+v_q = -1$.
  Because $b_2,\ldots, b_q$ are all positive, to minimize $\sum_{2\le i\le q} b_i v_i^2$ under linear constraint $v_2+\cdots+v_q=-1$, the optimal choice is
  $v_i = -b_i^{-1} Z^{-1}$ for $2\le i\le q$ where $Z := \sum_{2\le i\le q} b_i^{-1}$.
  For this choice of $v$, we have
  \begin{align}
    \eqref{eqn:lemma-quad-form-ratio-diff} = b_1 + \sum_{2\le i\le q} b_i \cdot (-b_i^{-1} Z^{-1})^2
    = b_1 + Z^{-1}.
  \end{align}
  Therefore, it remains to prove
  \begin{align}
    Z \le (-b_1)^{-1} \label{eqn:lemma-quad-form-ratio-after-step-1}
  \end{align}
  where $\pi_1\ge \cdots \ge \pi_q$, $b_1<0$, and $b_2,\ldots,b_q>0$.

  \textbf{Step 2.}
  We reduce to the case where $\pi_3=\cdots=\pi_q=0$.
  Note that
  \begin{align}
    Z = \sum_{2\le i\le q} b_i^{-1} =
    \sum_{2\le i\le q} \left(\left(\frac {\pi_i} q\right)^{-\alpha} - \left(\lambda \pi_i + \frac {1-\lambda} q\right)^{-\alpha}\right)^{-1}.
  \end{align}
  By Lemma~\ref{lemma:quad-form-ratio-helper1}, for fixed $\pi_1$, the optimal choice (for maximizing $Z$) of
  $\pi_2,\ldots, \pi_q$ is $\pi_3 = \cdots = \pi_q=0$.

  Write $\pi_1 = 1-x$, $\pi_2 = x$ ($x\in [0, \frac {1-\lambda}{1-\lambda q}]$).
  Then $b_1 = \left(\frac {1-x} q\right)^{-\alpha} - \left(\lambda (1-x) + \frac {1-\lambda} q\right)^{-\alpha}$
  and $Z = \left(\left(\frac x q\right)^{-\alpha} - \left(\lambda x + \frac {1-\lambda} q\right)^{-\alpha}\right)^{-1}$.
  By rearranging terms in~\eqref{eqn:lemma-quad-form-ratio-after-step-1}, we reduce to proving
  \begin{align}
    \left(\frac x q\right)^{-\alpha} + \left(\frac {1-x} q\right)^{-\alpha} \ge \left(\lambda x + \frac {1-\lambda} q\right)^{-\alpha} + \left(\lambda (1-x) + \frac {1-\lambda} q\right)^{-\alpha}
    \label{eqn:lemma-quad-form-ratio-after-step-2}
  \end{align}
  for $q\in \bZ_{\ge 2}$, $\alpha \in \bR_{\ge 1}$, $\lambda \in [-\frac 1{q-1},0]$, $x\in [0, \frac{1-\lambda}{1-\lambda q}]$.

  \textbf{Step 3.}
  Let $g_{q,\alpha,x}(\lambda) := \left(\lambda x + \frac {1-\lambda} q\right)^{-\alpha} + \left(\lambda (1-x) + \frac {1-\lambda} q\right)^{-\alpha}$ be the RHS of~\eqref{eqn:lemma-quad-form-ratio-after-step-2}.
  Then
  \begin{align*}
    g_{q,\alpha,x}''(\lambda) &= \alpha(\alpha+1) \left(x-\frac 1q\right)^2 \left(\lambda x + \frac {1-\lambda} q\right)^{-\alpha-2}\\
    &+ \alpha(\alpha+1) \left(1-x-\frac 1q\right)^2 \left(\lambda (1-x) + \frac {1-\lambda} q\right)^{-\alpha-2} \\
    &>0.
  \end{align*}
  So $g_{q,\alpha,x}$ is convex in $\lambda$.
  Therefore it suffices to verify~\eqref{eqn:lemma-quad-form-ratio-after-step-2} for $\lambda=0$ and $\lambda=-\frac 1{q-1}$.
  When $\lambda=0$, we have
  \begin{align}
    g_{q,\alpha,x}(\lambda) = \left(\frac 1q\right)^{-\alpha} + \left(\frac 1q\right)^{-\alpha} \le \left(\frac x q\right)^{-\alpha} + \left(\frac {1-x} q\right)^{-\alpha}.
  \end{align}
  When $\lambda = -\frac 1{q-1}$, we have
  \begin{align}
    g_{q,\alpha,x}(\lambda) = \left(\frac {1-x}{q-1}\right)^{-\alpha} + \left(\frac x{q-1}\right)^{-\alpha} \le \left(\frac x q\right)^{-\alpha} + \left(\frac {1-x} q\right)^{-\alpha}.
  \end{align}
  This finishes the proof.
\end{proof}

\begin{lemma} \label{lemma:quad-form-ratio-helper1}
  For $q\in \bZ_{\ge 2}$, $\alpha \in \bR_{\ge 1}$, $\lambda \in [-\frac 1{q-1},0]$, the function
  \begin{align}
    f(x) := \left(\left(\frac x q\right)^{-\alpha} - \left(\lambda x + \frac {1-\lambda} q\right)^{-\alpha}\right)^{-1}.
  \end{align}
  is convex in $x\in [0, \frac{1-\lambda}{1-\lambda q}]$.
\end{lemma}
\begin{proof}
  Let $g(x) = \frac 1{f(x)}$.
  Then
  \begin{align}
    f''(x) = \frac{2g'(x)^2-g(x)g''(x)}{g(x)^3}.
  \end{align}
  It suffices to prove that
  \begin{align}
    2g'(x)^2-g(x)g''(x)\ge 0.
  \end{align}
  We have
  \begin{align*}
    &~2g'(x)^2-g(x)g''(x) \\
    =&~2\alpha^2 \left(q^{-1}\left(\frac x q\right)^{-\alpha-1}-\lambda  \left(\lambda x + \frac {1-\lambda} q\right)^{-\alpha-1}\right)^2\\
    &-\left(\left(\frac x q\right)^{-\alpha} - \left(\lambda x + \frac {1-\lambda} q\right)^{-\alpha}\right) \\
    &\cdot
    \alpha(\alpha+1)\left(q^{-2}\left(\frac x q\right)^{-\alpha-2}-\lambda^2 \left(\lambda x + \frac {1-\lambda} q\right)^{-\alpha-2}\right).
  \end{align*}
  Write $u = \frac xq$, $v = \lambda x + \frac {1-\lambda}q$, $c = q\lambda$.
  Then we have $0\le u\le v\le 1$, and $-\frac{q}{q-1}\le c\le 0$.
  It suffices to prove
  \begin{align}
    2\alpha^2 (u^{-\alpha-1} - c v^{-\alpha-1})^2 - \alpha (\alpha+1)(u^{-\alpha} - v^{-\alpha})(u^{-\alpha-2}-c^2 v^{-\alpha-2}) \ge 0.
  \end{align}
  We have
  \begin{align*}
    &~2\alpha^2 (u^{-\alpha-1} - c v^{-\alpha-1})^2 - \alpha (\alpha+1)(u^{-\alpha} - v^{-\alpha})(u^{-\alpha-2}-c^2 v^{-\alpha-2})\\
    \ge&~\alpha (\alpha+1) ((u^{-\alpha-1} - c v^{-\alpha-1})^2 - (u^{-\alpha} - v^{-\alpha})(u^{-\alpha-2}-c^2 v^{-\alpha-2})) \\
    =&~\alpha(\alpha+1)u^{-\alpha-1} v^{-\alpha-1}(u^{-1}-cv^{-1})^2\\
    \ge &~0.
  \end{align*}
  This finishes the proof.
\end{proof}

\section{Proofs in Section~\ref{sec:non-bi}} \label{sec:proof-non-bi}
\subsection{Local subadditivity} \label{sec:proof-non-bi:subadd}
In this section we prove Lemma~\ref{lemma:subadd}.
We first prove the following local subadditivity for FSC.
\begin{lemma}\label{lemma:subadd-fsc}
  There exists a constant $c=c(q)>0$ such that for any $\delta > 0$, for $0<\epsilon\le c \delta^2$,
  if $\pi,\pi'\in \cP([q])/\Aut([q])$ and $C_{\chi^2}(\FSC_{\pi'}) \le \epsilon$, then
  \begin{align}
    C_{\chi^2} (\FSC_\pi \star \FSC_{\pi'}) \le (1+\delta) (C_{\chi^2}(\FSC_\pi) + C_{\chi^2}(\FSC_{\pi'})).
  \end{align}
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lemma:subadd} given Lemma~\ref{lemma:subadd-fsc}]
  Note that statement of Lemma~\ref{lemma:subadd} is monotone in $\epsilon$.
  So we can wlog assume that $\epsilon = C_{\chi^2}(P) \ge C_{\chi^2}(Q)$.

  Let $\pi_P$ (resp.~$\pi_Q$) be the $\pi$-component of $P$ (resp.~$Q$).

  Fix $\delta$. Choose $0 < \wt \delta < \delta$.
  Choose $\epsilon > 0$ such that $(1+\wt \delta) \epsilon + (q-1)\epsilon^{\frac 43} \le (1+\delta) \epsilon$
  and Lemma~\ref{lemma:subadd-fsc} holds with parameters $(\delta = \wt \delta, \epsilon = \epsilon^{\frac 13})$.
  Define $\wt \epsilon = \epsilon^{\frac 13}$.

  Then
  \begin{align}
    C_{\chi^2}(P\star Q) &= \bE_{\pi_P,\pi_Q} C_{\chi^2}(\FSC_{\pi_P} \star \FSC_{\pi_Q}) \\
    & = \bE_{\pi_P,\pi_Q} [C_{\chi^2}(\FSC_{\pi_P} \star \FSC_{\pi_Q}) \mathbbm{1} \{\min\{C_{\chi^2}(\pi_P), C_{\chi^2}(\pi_Q)\} \le \wt \epsilon\}] \label{eqn:lemma-subadd-1}\\
    & + \bE_{\pi_P,\pi_Q} [C_{\chi^2}(\FSC_{\pi_P} \star \FSC_{\pi_Q}) \mathbbm{1} \{\min\{C_{\chi^2}(\pi_P), C_{\chi^2}(\pi_Q)\} > \wt \epsilon\}] \label{eqn:lemma-subadd-2}.
  \end{align}

  By Lemma~\ref{lemma:subadd-fsc},
  \begin{align}
    \eqref{eqn:lemma-subadd-1} &\le \bE_{\pi_P,\pi_Q} [(1+\wt \delta)(C_{\chi^2}(\FSC_{\pi_P})+C_{\chi^2}(\FSC_{\pi_Q})) \\
    &\cdot \mathbbm{1} \{\min\{C_{\chi^2}(\pi_P), C_{\chi^2}(\pi_Q)\} \le \wt \epsilon\}] \nonumber \\
    & \le \bE_{\pi_P,\pi_Q} [(1+\wt \delta)(C_{\chi^2}(\FSC_{\pi_P})+C_{\chi^2}(\FSC_{\pi_Q}))] \nonumber \\
    & = (1+\wt \delta) (C_{\chi^2}(P) + C_{\chi^2}(Q)). \nonumber
  \end{align}

  \begin{align}
    \eqref{eqn:lemma-subadd-2} &\le (q-1) \bP(C_{\chi^2}(\pi_P) > \wt \epsilon) \bP(C_{\chi^2}(\pi_Q) > \wt \epsilon) \\
    & \le (q-1) \frac{C_{\chi^2}(P)}{\wt \epsilon} \frac{C_{\chi^2}(Q)}{\wt \epsilon} \nonumber \\
    & \le (q-1) \frac{\epsilon^2}{\wt \epsilon^2} = (q-1) \epsilon^{\frac 43}. \nonumber
  \end{align}

  So
  \begin{align}
    C_{\chi^2}(P\star Q) &\le \eqref{eqn:lemma-subadd-1} + \eqref{eqn:lemma-subadd-2} \\
    & \le (1+\wt \delta) (\epsilon + C_{\chi^2}(Q)) + (q-1) \epsilon^{\frac 43} \nonumber\\
    & \le (1+\delta) (C_{\chi^2}(P) + C_{\chi^2}(Q)). \nonumber
  \end{align}

  If we take $\wt \delta = \delta/2$, then it suffices to take $\epsilon = c \delta^6$ for some constant $c=c(q)>0$.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lemma:subadd-fsc}]
  Note that statement of Lemma~\ref{lemma:subadd-fsc} is monotone in $\epsilon$. So we can wlog assign that $C_{\chi^2}(\FSC_{\pi'}) = \epsilon$.

  Throught this proof, statements like ``$f\le Cg$'' should be understood as ``there exists a constant $C_q$ depending only on $q$ such that $f\le C_q g$''.

  Let $\pi'_i = \frac{1+\epsilon_i} q$.
  Then $\epsilon = C_{\chi^2}(\FSC_{\pi'}) = \frac 1q \sum_i \epsilon_i^2$ and $\sum_i \epsilon_i = 0$.
  \begin{align} \label{eqn:lemma-subadd-fsc-1}
    C_{\chi^2}(\FSC_{\pi} \star \FSC_{\pi'}) &= q \sum_{\tau \in S_q} \frac 1{(q-1)!} \frac{\sum_i \pi_i^2 \pi_{\tau(i)}^{\prime 2}}{\sum_i \pi_i \pi'_{\tau(i)}} -1\\
    &= q \sum_{\tau \in S_q} \frac 1{q!} \frac{\sum_i \pi_i^2 (1+\epsilon_{\tau(i)})^2}{1+\sum_i \pi_i \epsilon_{\tau(i)}} -1. \nonumber
  \end{align}

  We have
  \begin{align}
    \frac 1{1+x} = 1-x+x^2 - \frac{x^3}{1+x}. \label{eqn:lemma-subadd-fsc-basic}
  \end{align}
  Apply \eqref{eqn:lemma-subadd-fsc-basic} with $x=\sum_i \pi_i \epsilon_{\tau(i)}$.
  Note that $|x| \le C \sqrt \epsilon$. So
  \begin{align}
    \left|\frac{x^3}{1+x}\right|  \le C \epsilon^{\frac 32}.
  \end{align}

  Then
  \begin{align} \label{eqn:lemma-subadd-fsc-2}
    & \frac{\sum_i \pi_i^2 (1+\epsilon_{\tau(i)})^2}{1+\sum_i \pi_i \epsilon_{\tau(i)}}\\
    &= \left(\sum_i \pi_i^2 (1+\epsilon_{\tau(i)})^2\right)\left(1-x+x^2 - \frac{x^3}{1+x}\right) \nonumber \\
    &\le \left(\sum_i \pi_i^2 (1+\epsilon_{\tau(i)})^2\right)\left(1-\sum_i \pi_i \epsilon_{\tau(i)}+\left(\sum_i \pi_i \epsilon_{\tau(i)}\right)^2 + C \epsilon^{\frac 32}\right) \nonumber \\
    &\le \left(\sum_i \pi_i^2 (1+\epsilon_{\tau(i)})^2\right)\left(1-\sum_i \pi_i \epsilon_{\tau(i)}+\left(\sum_i \pi_i \epsilon_{\tau(i)}\right)^2\right) + C \epsilon^{\frac 32}. \label{eqn:lemma-subadd-fsc-3}
  \end{align}
  The last step is by
  \begin{align}
    \sum_i \pi_i^2 (1+\epsilon_{\tau(i)})^2\le C.
  \end{align}

  Let us expand the first summand in \eqref{eqn:lemma-subadd-fsc-3}.
  \begin{align} \label{eqn:lemma-subadd-fsc-4}
    &\left(\sum_i \pi_i^2 (1+\epsilon_{\tau(i)})^2\right)\left(1-\sum_i \pi_i \epsilon_{\tau(i)}+\left(\sum_i \pi_i \epsilon_{\tau(i)}\right)^2\right)\\
    &= \left(\sum_i \pi_i^2 + 2 \sum_i \pi_i^2 \epsilon_{\tau(i)} + \sum_i \pi_i^2 \epsilon_{\tau(i)}^2\right)\left(1-\sum_i \pi_i \epsilon_{\tau(i)}+\left(\sum_i \pi_i \epsilon_{\tau(i)}\right)^2\right)\nonumber \\
    &=: (\circnum{1} + \circnum{2} + \circnum{3})(1 - \circnum{4} + \circnum{5}) \nonumber\\
    &= \circnum{1}-\circnum{1}\circnum{4}+\circnum{1}\circnum{5}
    +\circnum{2}-\circnum{2}\circnum{4}+\circnum{2}\circnum{5}
    +\circnum{3}-\circnum{3}\circnum{4}+\circnum{3}\circnum{5}. \nonumber
    % &= \left(\sum_i \pi_i^2\right)
    % + \left(2 \sum_i \pi_i^2 \epsilon_{\tau(i)}\right)
    % -\left(\sum_i \pi_i^2\right)\left(\sum_i \pi_i \epsilon_{\tau(i)}\right) \nonumber \\
    % &+\left(\sum_i \pi_i^2 \epsilon_{\tau(i)}^2\right)
    % -\left(2 \sum_i \pi_i^2 \epsilon_{\tau(i)}\right)\left(\sum_i \pi_i \epsilon_{\tau(i)}\right)
    % +\left(\sum_i \pi_i^2\right)\left(\sum_i \pi_i \epsilon_{\tau(i)}\right)^2 \nonumber \\
    % &+ C \epsilon^{\frac 32}. \nonumber
  \end{align}

  Note that we have the following loose bounds:
  \begin{align}
    \circnum{1} \le C, \quad |\circnum{2}| \le C \sqrt \epsilon, \quad \circnum{3} \le C \epsilon, \quad |\circnum{4}| \le C\sqrt \epsilon, \quad \circnum{5} \le C \epsilon. \label{eqn:lemma-subadd-fsc-loose}
  \end{align}

  Let us study every term under $\sum_{\tau\in S_q} \frac 1{q!}$.
  For simplicity, write
  \begin{align}
    A = \sum_i \pi_i^2 = \frac 1q\left(1+C_{\chi^2}(\FSC_{\pi})\right).
  \end{align}

  \circnum{1}:
  \begin{align} \label{eqn:lemma-subadd-fsc-c1}
    \sum_{\tau\in S_q} \frac 1{q!} \circnum{1} = \sum_{\tau\in S_q} \frac 1{q!} \sum_i \pi_i^2 = A.
  \end{align}

  \circnum{1}\circnum{4}:
  \begin{align} \label{eqn:lemma-subadd-fsc-c1c4}
    \sum_{\tau\in S_q} \frac 1{q!} \circnum{1}\circnum{4} = \sum_{\tau\in S_q} \frac 1{q!} \left(\sum_i \pi_i^2\right) \left(\sum_i \pi_i \epsilon_{\tau(i)}\right)= 0.
  \end{align}

  \circnum{1}\circnum{5}:
  \begin{align} \label{eqn:lemma-subadd-fsc-c1c5}
    \sum_{\tau\in S_q} \frac 1{q!} \circnum{1}\circnum{5}
    &=\sum_{\tau\in S_q} \frac 1{q!} \left(\sum_i \pi_i^2\right)\left(\sum_i \pi_i \epsilon_{\tau(i)}\right)^2 \\
    &=A \sum_{i,j} \sum_{\tau\in S_q} \frac 1{q!} \pi_i \pi_j \epsilon_{\tau(i)} \epsilon_{\tau(j)} \nonumber \\
    &=A \sum_{i,j} \epsilon_i \epsilon_j \sum_{\tau\in S_q} \frac 1{q!} \pi_{\tau(i)} \pi_{\tau(j)} \nonumber \\
    &=A \sum_{i,j} \epsilon_i \epsilon_j \left\{
      \begin{array}[]{ll}
        \frac 1q \sum_k \pi_k^2 & i=j,\\
        \frac 1{q(q-1)} \left(1-\sum_k \pi_k^2\right) & i\ne j,
      \end{array}
    \right. \nonumber \\
    &= A \left(\sum_i \epsilon_i^2 \frac 1q \sum_k \pi_k^2 + \sum_i \epsilon_i (-\epsilon_i)\frac 1{q(q-1)} \left(1-\sum_k \pi_k^2\right) \right) \nonumber \\
    & = A \frac 1q \sum_i \epsilon_i^2 \left(\sum_k \pi_k^2 - \frac 1{q-1} \left(1-\sum_k \pi_k^2\right) \right) \nonumber \\
    &= \epsilon A \frac{q A-1}{q-1}. \nonumber
  \end{align}

  \circnum{2}:
  \begin{align} \label{eqn:lemma-subadd-fsc-c2}
    \sum_{\tau\in S_q} \frac 1{q!} \circnum{2} = \sum_{\tau\in S_q} \frac 1{q!} 2 \sum_i \pi_i^2 \epsilon_{\tau(i)} = 0.
  \end{align}

  \circnum{2}\circnum{4}:
  \begin{align} \label{eqn:lemma-subadd-fsc-c2c4}
    \sum_{\tau\in S_q} \frac 1{q!} \circnum{2}\circnum{4}
    &=\sum_{\tau\in S_q} \frac 1{q!} \left(2 \sum_i \pi_i^2 \epsilon_{\tau(i)}\right)\left(\sum_i \pi_i \epsilon_{\tau(i)}\right) \\
    &= \sum_{i,j} \sum_{\tau\in S_q} \frac 1{q!} 2 \pi_i^2 \pi_j \epsilon_{\tau(i)} \epsilon_{\tau(j)} \nonumber \\
    &=\sum_{i,j} 2 \epsilon_i \epsilon_j \sum_{\tau\in S_q} \frac 1{q!} \pi_{\tau(i)}^2 \pi_{\tau(j)} \nonumber \\
    &=\sum_{i,j} 2 \epsilon_i \epsilon_j \left\{
      \begin{array}{ll}
        \frac 1q \sum_k \pi_k^3 & i=j,\\
        \frac 1{q(q-1)} \sum_k \pi_k^2(1-\pi_k) & i\ne j,
      \end{array}\right. \nonumber \\
    &=2\sum_i \epsilon_i^2 \frac 1q \sum_k \pi_k^3 + 2\sum_i \epsilon_i(-\epsilon_i) \frac 1{q(q-1)} \sum_k \pi_k^2(1-\pi_k) \nonumber \\
    &=2 \frac 1q \sum_i \epsilon_i^2 \left(\sum_k \pi_k^3 - \frac 1{q-1} \sum_k \pi_k^2(1-\pi_k)\right) \nonumber \\
    &=2 \epsilon  \frac 1{q-1}\left(q\sum_k \pi_k^3 - \sum_k \pi_k^2\right) \ge 0. \nonumber
  \end{align}
  The last step is by
  \begin{align}
    \left(\sum_k \pi_k^3\right)\left(\sum_k \pi_k\right) \ge \left(\sum_k \pi_k^2\right)^2 \ge \frac 1q \left(\sum_k \pi_k^2\right).
  \end{align}

  \circnum{2}\circnum{5}:
  By \eqref{eqn:lemma-subadd-fsc-loose},
  \begin{align} \label{eqn:lemma-subadd-fsc-c2c5}
    \left|\sum_{\tau\in S_q} \frac 1{q!} \circnum{2}\circnum{5} \right| \le C \epsilon^{\frac 32}.
  \end{align}

  \circnum{3}:
  \begin{align} \label{eqn:lemma-subadd-fsc-c3}
    \sum_{\tau\in S_q} \frac 1{q!} \circnum{2} = \sum_{\tau\in S_q} \frac 1{q!} \left(\sum_i \pi_i^2 \epsilon_{\tau(i)}^2\right) = \sum_i \pi_i^2 \frac 1q \sum_j \epsilon_j^2 = \epsilon A.
  \end{align}

  \circnum{3}\circnum{4}:
  By \eqref{eqn:lemma-subadd-fsc-loose},
  \begin{align} \label{eqn:lemma-subadd-fsc-c3c4}
    \left|\sum_{\tau\in S_q} \frac 1{q!} \circnum{3}\circnum{4}\right| \le C \epsilon^{\frac 32}.
  \end{align}

  \circnum{3}\circnum{5}:
  By \eqref{eqn:lemma-subadd-fsc-loose},
  \begin{align} \label{eqn:lemma-subadd-fsc-c3c5}
    \left|\sum_{\tau\in S_q} \frac 1{q!} \circnum{3}\circnum{5}\right| \le C \epsilon^2.
  \end{align}

  Plugging \eqref{eqn:lemma-subadd-fsc-c1} - \eqref{eqn:lemma-subadd-fsc-c3c5} into \eqref{eqn:lemma-subadd-fsc-4}\eqref{eqn:lemma-subadd-fsc-2}\eqref{eqn:lemma-subadd-fsc-1}, we get
  \begin{align} \label{eqn:lemma-subadd-fsc-final}
    C_{\chi^2}(\FSC_{\pi} \star \FSC_{\pi'})
    &\le q(A+\epsilon A \frac{qA-1}{q-1} + \epsilon A + C \epsilon^{\frac 32})-1\\
    &=(qA-1)(1+\frac{q\epsilon A}{q-1} + \epsilon) + \epsilon + C \epsilon^{\frac 32}. \nonumber
  \end{align}
  Because $A \le 1$, for $\epsilon$ small enough (it suffices to take $\epsilon = c \delta^2$ for a small enough constant $c=c(q)>0$), we finish the proof.
\end{proof}


\subsection{Proof of Prop.~\ref{prop:no-robust-recon}} \label{sec:proof-non-bi:no-robust-recon}
\begin{proof}[Proof of Prop.~\ref{prop:no-robust-recon}]
  \textbf{Regular tree.}
  Fix $\delta > 0$. Choose $\wt \delta > 0$ so that $(1+\wt \delta)^d d \lambda^2 < 1$.
  Let $\wt \epsilon > 0$ be a constant such that $\wt \epsilon < \delta$ and Lemma~\ref{lemma:subadd} holds with parameters $(\delta=\wt \delta, \epsilon=(1+\wt \delta)^d \wt \epsilon)$.
  Let $\epsilon > 0$ be a constant such that $(1+\wt \delta)^d (d \lambda^2 \wt \epsilon + \epsilon) \le \wt \epsilon$.

  We prove by induction on $k$ that $I_{\chi^2}(\sigma_\rho; \omega_{T_k} | T_k) \le \wt \epsilon$.
  The base case $k=0$ is by
  \begin{align}
    I_{\chi^2}(\sigma_\rho; \omega_{T_0} | T_0) = C_{\chi^2}(W) \le \epsilon\le \wt \epsilon.
  \end{align}

  Induction step:
  Let $M_k$ be the channel $\sigma_\rho \mapsto \omega_{T_k}$.
  Then $M_{k+1} = (M_k\circ P_\lambda)^{\star b} \star W$.

  By Lemma~\ref{lemma:contraction},
  \begin{align}
    C_{\chi^2}(M_k\circ P_\lambda) \le \lambda^2 C_{\chi^2}(M_k).
  \end{align}

  % Let us prove by induction that for $d\le b$,
  % \begin{align}
  %   C_{\chi^2}((M_k\circ P_\lambda)^{\star d} \star W) \le (1+\wt \delta)^d (d \lambda^2 \wt \epsilon + \epsilon).
  % \end{align}
  % Base case $d=0$ is by assumption $C_{\chi^2}(W) \le \epsilon$.
  % Induction step:
  % By induction hypothesis,
  % \begin{align}
  %   C_{\chi^2}((M_k\circ P_\lambda)^{\star d} \star W) \le (1+\wt \delta)^d (d \lambda^2 \wt \epsilon + \epsilon) \le (1+\wt \delta)^b (b \lambda^2 \wt \epsilon + \epsilon) \le \wt \epsilon.
  % \end{align}
  % For $d+1\le b$, by Lemma~\ref{lemma:subadd},
  % \begin{align}
  %   C_{\chi^2}((M_k\circ P_\lambda)^{\star (d+1)} \star W) &\le (1+\wt \delta) ((1+\wt \delta)^d (d \lambda^2 \wt \epsilon + \epsilon) + \lambda^2 \wt \epsilon)\\
  %   &\le (1+\wt \delta)^{d+1} ((d+1)\lambda^2 \wt \epsilon + \epsilon).\nonumber
  % \end{align}
  % Therefore
  Repeatedly applying Lemma~\ref{lemma:subadd}, we get
  \begin{align}
    C_{\chi^2}(M_{k+1}) \le (1+\wt \delta)^d (d \lambda^2 \wt \epsilon + \epsilon) \le \wt \epsilon.
  \end{align}
  This completes the induction.

  \textbf{Poisson tree.}
  Fix $\delta > 0$. Let $b_0$ be a large enough constant to be chosen later.
  Note that there exists $c>0$ such that for $b_0$ large enough,
  \begin{align}
    \bP[\Po(d) > b_0] \le \exp(-c b_0).\label{eqn:poisson-tail}
  \end{align}

  Choose $b_0$ large enough and $\wt \delta = b_0^{-2}$ so that $(1+\wt \delta)^{b_0} d \lambda^2 < 1 - b_0^{-1}$.
  Let $\wt \epsilon = b_0^{-14}$. Then for $b_0$ large enough we have (1) $\wt \epsilon < \delta$ (2) Lemma~\ref{lemma:subadd} holds with parameters $(\delta = \wt \delta, \epsilon = (1+\wt \delta)^{b_0} b_0 \lambda^2 \wt \epsilon)$.
  Choose $\epsilon>0$ such that
  \begin{align}
    (1+\wt \delta)^{b_0} (d \lambda^2 \wt \epsilon + \epsilon) + (q-1) \exp(-c b_0) < \wt \epsilon.
  \end{align}
  Such $\epsilon$ exists because $\exp(-c b_0) < b_0^{-20}$ for large enough $b_0$.

  We have
  \begin{align} \label{eqn:prop-no-robust-recon-poisson}
    C_{\chi^2}(M_{k+1}) &= \bE_{b\sim \Po(d)} C_{\chi^2}((M_k\circ P_\lambda)^b \star W) \\
    &\le \bE_{b\sim \Po(d)} [C_{\chi^2}((M_k\circ P_\lambda)^b \star W) \mathbbm{1}\{b \le b_0\}] + \bP[\Po(d) > b_0] (q-1) \nonumber \\
    & =: \circnum{1} + \circnum{2}. \nonumber
  \end{align}
  Repeatedly applying Lemma~\ref{lemma:subadd}, we get
  \begin{align} \label{eqn:prop-no-robust-recon-term-1}
    \circnum{1} &\le \bE_{b\sim \Po(d)} [(1+\wt \delta)^b (b \lambda^2 \wt \epsilon + \epsilon) \mathbbm{1}\{b \le b_0\}] \\
    &\le (1+\wt \delta)^{b_0} \bE_{b\sim \Po(d)} [(b \lambda^2 \wt \epsilon + \epsilon) \mathbbm{1}\{b \le b_0\}] \nonumber \\
    &\le (1+\wt \delta)^{b_0} (d \lambda^2 \wt \epsilon + \epsilon). \nonumber
  \end{align}
  By \eqref{eqn:poisson-tail},
  \begin{align} \label{eqn:prop-no-robust-recon-term-2}
    \circnum{2} &\le (q-1) \exp(-c b_0).
  \end{align}
  Plugging \eqref{eqn:prop-no-robust-recon-term-1}\eqref{eqn:prop-no-robust-recon-term-2} into \eqref{eqn:prop-no-robust-recon-poisson}, we get
  \begin{align}
    C_{\chi^2}(M_{k+1}) \le \wt \epsilon.
  \end{align}
  This completes the induction.
\end{proof}

\section{Asymmetric fixed points} \label{sec:asymm}
In this paper we have focused on symmetric fixed points of the $\BP$ operator.
If we view the $\BP$ operator as an operator from the space of $q$-ary input (possibly asymmetric) channels, then a natural question is what are the (possibly asymmetric) fixed points.
In the case $q=2$, \cite{yu2022ising} showed that there is only one non-trivial fixed point, and the fixed point is symmetric.
For $q\ge 3$, it is no longer the case.
\begin{proposition} \label{prop:bp-fixed-point-asymm}
  Work under the setting of Theorem~\ref{thm:bi-imprecise}.
  If $q\ge 3$ and $d\lambda^2>1$, then the $\BP$ operator (Eq.~\eqref{eqn:bp-operator}) at least one non-trivial asymmetric fixed point.
\end{proposition}
\begin{proof}
  Consider the channel $U: [q]\to \{\pm\}$, which maps $1$ to $+$ and $2,\ldots,q$ to $-$.
  Because $\BP(U)\le_{\deg} U$, the sequence $\{\BP^k(U)\}_{k\ge 0}$ is non-increasing in degradation preorder.
  This means that for any convex potential function $\phi$ on the probability simplex $\cP([q])$, the sequence $\{\Phi(\BP^k(U))\}_{k\ge 0}$ is non-increasing (where $\Phi$ is the induced function on the space of $q$-ary input channels).
  By monotone convergence theorem, the limit
  \begin{align}
    \lim_{k\to \infty} \Phi(\BP^k(U))
  \end{align}
  exists for all convex potential functions $\phi$.
  These limit potentials are compatible with each other and uniquely determines the limit channel $\BP^\infty(U)$.

  We would like to show that $\BP^\infty(U)$ is a non-FMS non-trivial fixed point.
  When $d\lambda^2>1$, count-reconstruction is possible (see e.g.~\cite{mossel2001reconstruction}).
  So it is possible to gain non-trivial information about whether the input is $1$ by counting the number of $+$.
  So $\BP^\infty(U)$ is non-trivial.

  On the other hand, $\BP^\infty(U) (\cdot | i)$ are the same for $i=2,\ldots, q$.
  This cannot happen for any non-trivial FMS channel.
  Therefore $\BP^\infty(U)$ is not an FMS.
\end{proof}

Nevertheless, when the condition in Theorem~\ref{thm:bi-imprecise} holds, for an open set of initial channels, it will converge to the unique FMS fixed point under BP iterations.
We define full rank $q$-ary input channels.
\begin{definition}
  Let $U: \cX \to \cY$ be a channel where $\cX = [q]$.
  We say $U$ has full rank if there exists a partition of
  $\cY$ into $q$ measurable subsets $\cY = E_1 \cup \cdots \cup E_q$ such that the $q\times q$ matrix
  \begin{align}
    (U(E_j | i))_{i\in [q], j\in [q]}
  \end{align}
  is invertible.
\end{definition}
\begin{proposition} \label{prop:bp-fixed-point-full-rank}
  Work under the setting of Theorem~\ref{thm:bi-imprecise}.
  If $(q,d,\lambda)$ satisfies the condition in Theorem~\ref{thm:bi-imprecise}, then for any $q$-ary input (possibly asymmetric) channel $U$ of full rank, we have
  \begin{align}
    \BP^\infty(U) = \BP^\infty(\Id).
  \end{align}
\end{proposition}
\begin{proof}
  We prove that under the condition $\rk U = q$, there exists channel $R$ such that $R\circ U$ is a non-trivial Potts channel.

  Because $U$ is of full rank, there exists a partition $\cY = E_1 \cup \cdots \cup E_q$ such that
  \begin{align}
    (U(E_j | i))_{i\in [n], j\in [m]}
  \end{align}
  is invertible.
  Define $Q: \cY \to [q]$ by mapping $y\in E_i$ to $i$ for all $i\in [q]$.
  Then we can replace $U$ with $Q \circ U$ and wlog assume that $\cY=[q]$.

  Let $J\in \bR^{q\times q}$ be the all ones matrix.
  Because $U$ is a stochastic matrix, we have $U^{-1} J=J$.
  By continuity, there exists $\lambda \ne 0$ such that $U^{-1} P_\lambda$ has non-negative entries.
  Note that $U^{-1} P_\lambda \mathbbm{1} = U^{-1} \mathbbm{1} = \mathbbm{1}$.
  So $U^{-1} P_\lambda$ is a stochastic matrix.
  Let $R$ be the channel corresponding to $U^{-1} P_\lambda$.
  Then $R\circ U = UR = P_\lambda$ is a non-trivial Potts channel.

  Therefore $P_\lambda \le_{\deg} U \le_{\deg} \Id$.
  Degradation of $q$-ary input (possibly asymmetric) channels is preserved under $\BP$ operator.
  So by iterating the $\BP$ operator, we get
  \begin{align}
    \BP^\infty(P_\lambda) \le_{\deg} \BP^\infty(U) \le_{\deg} \BP^\infty(\Id).
  \end{align}
  The first and third channels are equal by Theorem~\ref{thm:bi-imprecise}.
  Therefore $\BP^\infty(U) = \BP^\infty(\Id)$.
\end{proof}

For the boundary irrelevance operator $\BP'$ (Eq.~\eqref{eqn:bp-prime-operator}), the situation is simpler: when the survey FMS channel $W$ is non-trivial, there is no asymmetric fixed point.
\begin{proposition} \label{prop:bp-prime-fixed-point-asymm}
  Work under the setting of Theorem~\ref{thm:bi-imprecise}.
  If $(q,d,\lambda)$ satisfies the condition in Theorem~\ref{thm:bi-imprecise}, then $\BP'$ has only one fixed point.
\end{proposition}
\begin{proof}
  Suppose $U$ is a fixed point of $\BP'$. We have
  \begin{align}
    \FSC_{\Unif([q])} \le_{\deg} U \le_{\deg} \Id.
  \end{align}
  Degradation for $q$-ary input (possibly asymmetric) channels is preserved under $\BP'$ operator.
  So by iterating the $\BP'$ operator, we get
  \begin{align}
    \BP^{\prime \infty} (\FSC_{\Unif([q])}) \le_{\deg} \BP^{\prime \infty}(U) \le_{\deg} \BP^{\prime \infty}(\Id).
  \end{align}
  The first and third channels are equal by Theorem~\ref{thm:bi-imprecise}.
  Therefore $U = \BP^{\prime\infty}(U) = \BP^{\prime \infty}(\Id)$ is equal to the unique FMS fixed point.
\end{proof}

\section{SBM mutual information} \label{sec:mutual-info}
In this section we prove Theorem~\ref{thm:sbm-mutual-info}.
The proof is a direct generalization of \cite[Theorem 1]{abbe2021stochastic}.
\begin{proof}[Proof of Theorem~\ref{thm:sbm-mutual-info}]
  Let $Y_v^\epsilon \sim \EC_\epsilon(\cdot | X_v)$ for $v\in V$ and $\epsilon\in [0,1]$.
  Let $\rho\in v$ be any vertex.
  Define $f(\epsilon) := \frac 1n I(X; G, Y^\epsilon)$.
  Then $f(0) = \frac 1n I(X; G)$ and $f(1) = H(X_\rho)$.
  Furthermore, calculation shows that
  \begin{align}
    f'(\epsilon) = H(X_\rho | G, Y^\epsilon_{V\backslash \rho}).
  \end{align}
  Let $k\in \bZ_{\ge 1}$ be a constant, $B(\rho,k)$ be the set of vertices with distance $\le k$ to $\rho$, and
  $\partial B(\rho,k)$ be the set of vertices at distance $k$ to $\rho$.
  Then by data processing inequality, we have
  \begin{align}
    I(X_\rho; Y^\epsilon_{B(\rho,k) \backslash \rho}) \le I(X_\rho; Y^\epsilon_{V\backslash \rho}) \le I(X_\rho; Y^\epsilon_{B(\rho,k)\backslash \rho}, X_{\partial B(\rho,k)}).
  \end{align}
  By Lemma~\ref{lemma:sbm-bot-coupling}, we have
  \begin{align}
    I(\sigma_\rho; \omega^\epsilon_{T_k \backslash \rho}) - o(1) \le I(X_\rho; Y^\epsilon_{V\backslash \rho}) \le I(\sigma_\rho; \omega^\epsilon_{T_k \backslash \rho}, \sigma_{L_k}) + o(1).
  \end{align}
  Taking limit $n\to \infty$, then taking limit $k\to \infty$, we get
  \begin{align}
    \lim_{k\to \infty} I(\sigma_\rho; \omega^\epsilon_{T_k \backslash \rho} | T_k) \le \lim_{n\to \infty} I(X_\rho; Y^\epsilon_{V\backslash \rho}) \le \lim_{k\to \infty} I(\sigma_\rho; \omega^\epsilon_{T_k \backslash \rho}, \sigma_{L_k} | T_k).
  \end{align}
  The first and third terms are equal by boundary irrelevance (Theorem~\ref{thm:bi-imprecise}).
  Therefore
  \begin{align}
    \lim_{n\to\infty} I(X_\rho; Y^\epsilon_{V\backslash \rho}) = \lim_{k\to \infty} I(\sigma_\rho; \omega^\epsilon_{T_k \backslash \rho} | T_k)
  \end{align}
  Therefore
  \begin{align}
    \lim_{n\to \infty} \frac 1n I(X; G) &= H(X_\rho) - \int_0^1 \lim_{n\to\infty} H(X_\rho | Y^\epsilon_{V\backslash \rho}) d\epsilon \\
    &=\int_0^1 \lim_{n\to \infty} I(X_\rho; Y^\epsilon_{V\backslash \rho}) d\epsilon  \nonumber \\
    &=\int_0^1 \lim_{k\to \infty} I(\sigma_\rho; \omega^\epsilon_{T_k \backslash \rho} | T_k) d\epsilon. \nonumber
  \end{align}
\end{proof}

\begin{lemma} \label{lemma:sbm-bot-coupling}
  Let $(X,G)\sim \mathrm{SBM}(n,q,\frac an,\frac bn)$.
  Let $v\in V$ and $k=o(\log n)$. Let $B(v,k)$ be the set of vertices with distance $\le k$ to $v$, and
  $\partial B(v,k)$ be the set of vertices at distance $k$ to $v$.
  Let $d = \frac {a+(q-1)b}q$ and $\lambda = \frac{a-b}{a+(q-1)b}$.
  Let $(T,\sigma)$ be the Potts model with broadcasting channel $P_\lambda$ on a Poisson tree with expected offspring $d$. Let $\rho$ be the root of $T$, $L_k$ be the set of vertices at distance $k$ to $\rho$, $T_k$ be the set of vertices at distance $\le k$ to $\rho$.

  Then $(G|_{B(v,k)}, X_{B(v,k)})$ can be coupled (with $o(1)$ TV distance) to $(T_k, \sigma_{T_k})$.
\end{lemma}
\begin{proof}
  This is a well-known result and has appeared in many places. For a proof, see e.g.,~\cite[Lemma 6.2]{mossel2022exact}.
\end{proof}

\section{Optimal recovery algorithm} \label{sec:optimal-recovery}
In this section we prove Theorem~\ref{thm:optimal-recovery-side-information} and~\ref{thm:optimal-recovery-vanilla}.
\begin{algorithm}[!ht]\caption{Belief propagation algorithm for SBM with side information}\label{alg:optimal-recovery-side-information}
  \begin{algorithmic}[1]
    \State \textbf{Input: } SBM graph $G=(V,E)$, side information $Y\in \cY^V$
    \State \textbf{Output: } $\wh X\in [q]^V$
    \State $m_{u\to v}^{(0)} \gets \left(\frac{W(Y_u | i)}{\sum_{j\in [q]} W(Y_u | j)}\right)_{i\in [q]} \quad \forall (u,v)\in E$
    \State $r \gets \lfloor \log^{0.9} n\rfloor $
    \For{$t=0\to r-1$}
    \For{$(u,v)\in E$}
    \State $$Z_{u\to v}^{(t+1)} \gets \sum_{j\in [q]} W(Y_u | j) \prod_{(u,w)\in E, w\ne v} \sum_{k\in [q]} m_{w\to u}^{(t)}(k) P_\lambda(k | j)$$
    \State $$m_{u\to v}^{(t+1)} \gets \left( \left(Z_{u\to v}^{(t+1)}\right)^{-1} \cdot W(Y_u | i) \prod_{(u,w)\in E, w\ne v} \sum_{k\in [q]} m_{w\to u}^{(t)}(k) P_\lambda(k | i)\right)_{i\in [q]}$$
    \EndFor
    \EndFor
    \For{$u\in V}$
    \State $$Z_u \gets \sum_{j\in [q]} W(Y_u | j) \prod_{(u,w)\in E} \sum_{k\in [q]} m_{w\to u}^{(r)}(k) P_\lambda(k | j)$$
    \State $$m_u \gets \left(Z_u^{-1} \cdot W(Y_u | i) \prod_{(u,w)\in E} \sum_{k\in [q]} m_{w\to u}^{(r)}(k) P_\lambda(k | i)\right)_{i\in [q]}$$
    \State $\wh X_u \gets \argmax_{i\in [q]} m_u(i)$
    \EndFor
    \State \Return $\wh X$
  \end{algorithmic}
\end{algorithm}

\begin{proof}[Proof of Theorem~\ref{thm:optimal-recovery-side-information}]
  We run Algorithm~\ref{alg:optimal-recovery-side-information}.
  Let $\rho \in V$. For $k\in \bZ_{\ge 1}$, define $B(\rho,k)$, $\partial B(\rho,k)$ as in Lemma~\ref{lemma:sbm-bot-coupling}.
  By Lemma~\ref{lemma:sbm-bot-coupling} and induction on $t$, we see that $m_{u\to v}^{(t)}$ has the same distribution (up to $o(1)$ TV distance) as the posterior distribution of $\sigma_\rho$ conditioned $\omega_{T_t}$.
  Therefore $m_{u\to v}^{(r)}$ has the same distribution (up to $o(1)$ TV distance) as the posterior distribution of $\sigma_\rho$ conditioned $\omega_{T_r}$.
  So as $n\to \infty$, Algorithm~\ref{alg:optimal-recovery-side-information} achieves probability of error
  \begin{align}
    \lim_{k\to \infty} P_e(\sigma_\rho | T_k, \omega_{T_k}).
  \end{align}

  On the other hand, by data processing inequality and Lemma~\ref{lemma:sbm-bot-coupling}, we have
  \begin{align}
    P_e(X_\rho | G, Y) \ge P_e(X_\rho | G, Y_{B(\rho,k)})
    = P_e(X_\rho | G|_{B(\rho,k)}, Y_{B(\rho,k)})
    \ge P_e(\sigma_\rho | T_k, \omega_{T_k}) - o(1).
  \end{align}
  Taking limit $n\to \infty$, then $k\to \infty$, we see that
  \begin{align}
    P_e(X_\rho | G, Y) \ge \lim_{k\to \infty} P_e(\sigma_\rho | T_k, \omega_{T_k}).
  \end{align}
  This shows that Algorithm~\ref{alg:optimal-recovery-side-information} is optimal.
\end{proof}

\begin{algorithm}[!ht]\caption{Belief propagation algorithm for SBM}\label{alg:optimal-recovery-vanilla}
  \begin{algorithmic}[1]
    \State \textbf{Input: } SBM graph $G=(V,E)$, initial recovery algorithm $\cA$
    \State \textbf{Output: } $\wh X\in [q]^V$
    \State $s\gets 1$ if model is assortative; $s\gets -1$ if model is disassortative
    \State $r \gets \lfloor \log^{0.9} n\rfloor $
    \State $U \gets $ random subset of $V$ of size $\lfloor \sqrt n\rfloor$
    \State $Y \gets \cA(G\backslash U)$
    \State For $i\in [q]$, $u\in U$, compute $N_Y(u,i) \gets \#\{Y_v=i : v\in V, (u,v)\in E\}$
    \State For $i\in [q]$, choose $u_i\in U$ such that  \label{line:alg-optimal-recovery-choose-u}
    \begin{enumerate}[label=(\alph*)]
      \item $u_i$ has at least $\sqrt{\log n}$ neighbors in $V\backslash U$, and \label{item:alg-optimal-recovery-choose-u-req-a}
      \item $s N_Y(u_i,i) > s N_Y(u_i,j)$ for $j\in [q]\backslash i$. \label{item:alg-optimal-recovery-choose-u-req-b}
    \end{enumerate}
    \For{$v\in V\backslash U$}
      \State $Y^v \gets \cA(G\backslash B(v,r-1) \backslash U)$
      \State Relabel $Y^v$ by performing a permutation $\tau \in \Aut([q])$, so that $s N_{Y^v}(u_i,i) > s N_{Y^v}(u_i,j)$ for $i\in [q]$, $j\in [q]\backslash i$. Report failure if this cannot be achieved.
      \State $H^v_{i,j} \gets \frac{N_{Y^v}(u_i,j)}{\sum_{j\in [q]}N_{Y^v}(u_i,j)}$
      \State Run belief propagation on $B(v,r-1)$ with boundary condition $Y^v_{\partial B(v,r)}$, assuming the channel from $\partial B(v,r-1)$ to $\partial B(v,r)$ is $H^v$ \label{line:alg-optimal-recovery-bp}
      \State $\wh X_v \gets $ maximum likelihood label according to belief propagation \label{line:alg-optimal-recovery-bp-result}
    \EndFor
    \State $\wh X_v \gets 1$ for all $v\in U$
    \State \Return $\wh X$
  \end{algorithmic}
\end{algorithm}

\begin{proof}[Proof of Theorem~\ref{thm:optimal-recovery-vanilla}]
  We run Algorithm~\ref{alg:optimal-recovery-vanilla}.
  The proof is a variation of the proof in \cite{mossel2016belief}.

  Choice of $u_i$: For every $i\in [q]$, the set $\{u\in U: X_u=i\}$ has size $\Omega(\sqrt n)$.
  Therefore with high probability, there exists $u\in U$ with $X_u=i$ that satisfies~\ref{item:alg-optimal-recovery-choose-u-req-a}.
  Furthermore, because $Y$ is independent of $U$, we can equivalently first generate the graph $G\backslash U$, then compute $Y$, then generate the edges adjacent to $U$.
  In this way, we see that with high probability, for all $u\in U$ satisfying~\ref{item:alg-optimal-recovery-choose-u-req-a}, the empirical distribution of $\{Y_v: v\in V, (u,v)\in E\}$ has $o(1)$ total variation distance to $P_\lambda F$.
  By assumption~\ref{item:thm-optimal-recovery-vanilla-req-1}\ref{item:thm-optimal-recovery-vanilla-req-3} in Theorem~\ref{thm:optimal-recovery-vanilla}, we have $s (P_\lambda F)_{i,\tau(i)} > s (P_\lambda F)_{i,\tau(j)} + |\lambda| \epsilon$ for $i\in [q]$, $j\in [q]\backslash i$.
  Therefore with high probability, for all $u\in U$ satisfying~\ref{item:alg-optimal-recovery-choose-u-req-a}, we can identify $X_u$ up to a permutation $\tau\in \Aut([q])$ by computing $\argmax_{j\in [q]} s N_Y(u,j)$.
  Therefore with high probability we are able to choose the $u_i$s in Line~\ref{line:alg-optimal-recovery-choose-u}.

  Alignment of $Y$ with $Y^v$: The above discussion still holds with $Y$ replaced by $Y^v$. One thing to note is that by Lemma~\ref{lemma:sbm-bot-coupling}, $|B(v,r-1)| = n^{o(1)}$ with high probability. So removing $B(v,r-1)$ from $G$ has negligible influence to the the empirical distribution of labels of neighbors of $u_i$s.
  Therefore, with high probability, we are able to permute the labels $Y^v$ so that the empirical distributions align with that of $Y$.
  (Note that we do not assume the empirical distributions for $Y$ and $Y^v$ are the same; we only use that they both satisfy condition~\ref{item:thm-optimal-recovery-vanilla-req-3}.)
  Furthermore, we can compute the transition matrix $H^v = P_\lambda F^v + o(1)$.

  Boundary condition of BP:
  Because $Y^v$ is independent of edges between $\partial B(v,r-1)$ and $\partial B(v,r)$, we can equivalently first generate the graph $G\backslash B(v,r-1)\backslash U$, then compute $Y^v$, then generate $E(\partial B(v,r-1), \partial B(v,r))$.
  In this way, it is clear that $Y^v_w$ for one $w\in \partial B(v,r)$ is equivalent to one observation of $X_{\pa(w)}$ through channel $H^v$, where $\pa(w)$ is the parent of $w$ in $\partial B(v,r-1)$.

  Property of $H^v$: By condition~\ref{item:thm-optimal-recovery-vanilla-req-2}, $\sigma_{\min}(H^v) \ge |\lambda| \sigma_{\min}(F^v) - o(1) = |\lambda| \epsilon - o(1)$.
  By proof of Prop.~\ref{prop:bp-fixed-point-full-rank}, we see that $P_{\lambda'} \le_{\deg} H^v$ for some constant $\lambda'>0$ not depending on $n$.

  Convergence of BP recursion:
  Because $\lambda'>0$ is a constant, the limit in Prop.~\ref{prop:bot-majority} converges to desired accuracy in a constant number of iterations. (We only need constant additive error in the limit, when it is used in Prop.~\ref{prop:phi-H-contraction}.)
  Because $r = \omega(1)$, belief propagation in Line~\ref{line:alg-optimal-recovery-bp} converges to $o(1)$ in TV distance to the fixed point.
  Therefore we achieve desired probability of error in Line~\ref{line:alg-optimal-recovery-bp-result}.
\end{proof}

\end{document}
