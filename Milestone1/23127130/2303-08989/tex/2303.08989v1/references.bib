
@article{villalonga_establishing_2020,
	title = {Establishing the quantum supremacy frontier with a 281 {Pflop}/s simulation},
	volume = {5},
	issn = {2058-9565},
	url = {https://doi.org/10.1088/2058-9565/ab7eeb},
	doi = {10.1088/2058-9565/ab7eeb},
	abstract = {Noisy intermediate-scale quantum (NISQ) computers are entering an era in which they can perform computational tasks beyond the capabilities of the most powerful classical computers, thereby achieving ‘quantum supremacy’, a major milestone in quantum computing. NISQ supremacy requires comparison with a state-of-the-art classical simulator. We report HPC simulations of hard random quantum circuits (RQC), which have been recently used as a benchmark for the first experimental demonstration of quantum supremacy, sustaining an average performance of 281 Pflop/s (true single precision) on Summit, currently the fastest supercomputer in the world. These simulations were carried out using qFlex, a tensor-network-based classical high-performance simulator of RQCs. Our results show an advantage of many orders of magnitude in energy consumption of NISQ devices over classical supercomputers. In addition, we propose a standard benchmark for NISQ computers based on qFlex.},
	language = {en},
	number = {3},
	urldate = {2021-05-25},
	journal = {Quantum Science and Technology},
	author = {Villalonga, Benjamin and Lyakh, Dmitry and Boixo, Sergio and Neven, Hartmut and Humble, Travis S. and Biswas, Rupak and Rieffel, Eleanor G. and Ho, Alan and Mandrà, Salvatore},
	month = apr,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {034003},
}

@inproceedings{lu_supporting_2010,
	address = {New York, NY, USA},
	series = {{DaMoN} '10},
	title = {Supporting extended precision on graphics processors},
	isbn = {978-1-4503-0189-3},
	url = {https://doi.org/10.1145/1869389.1869392},
	doi = {10.1145/1869389.1869392},
	abstract = {Scientific computing applications often require support for non-traditional data types, for example, numbers with a precision higher than 64-bit floats. As graphics processors, or GPUs, have emerged as a powerful accelerator for scientific computing, we design and implement a GPU-based extended precision library to enable applications with high precision requirement to run on the GPU. Our library contains arithmetic operators, mathematical functions, and data-parallel primitives, each of which can operate at either multi-term or multi-digit precision. The multi-term precision maintains an accuracy of up to 212 bits of signifcand whereas the multi-digit precision allows an accuracy of an arbitrary number of bits. Additionally, we have integrated the extended precision algorithms to a GPU-based query processing engine to support efficient query processing with extended precision on GPUs. To demonstrate the usage of our library, we have implemented three applications: parallel summation in climate modeling, Newton's method used in nonlinear physics, and high precision numerical integration in experimental mathematics. The GPU-based implementation is up to an order of magnitude faster, and achieves the same accuracy as their optimized, quadcore CPU-based counterparts.},
	urldate = {2023-03-11},
	booktitle = {Proceedings of the {Sixth} {International} {Workshop} on {Data} {Management} on {New} {Hardware}},
	publisher = {Association for Computing Machinery},
	author = {Lu, Mian and He, Bingsheng and Luo, Qiong},
	month = jun,
	year = {2010},
	pages = {19--26},
}

@misc{nakata_mplapack_2022,
	title = {{MPLAPACK} version 2.0.1 user manual},
	url = {http://arxiv.org/abs/2109.13406},
	doi = {10.48550/arXiv.2109.13406},
	abstract = {The MPLAPACK (formerly MPACK) is a multiple-precision version of LAPACK (https://www.netlib.org/lapack/). MPLAPACK version 2.0.1 is based on LAPACK version 3.9.1 and translated from Fortran 90 to C++ using FABLE, a Fortran to C++ source-to-source conversion tool (https://github.com/cctbx/cctbx\_project/tree/master/fable/). MPLAPACK version 2.0.1 provides the real and complex version of MPBLAS, and the real and complex versions of MPLAPACK support all LAPACK features: solvers for systems of simultaneous linear equations, least-squares solutions of linear systems of equations, eigenvalue problems, and singular value problems, and related matrix factorizations except for mixed-precision routines. The MPLAPACK defines an API for numerical linear algebra, similar to LAPACK. It is easy to port legacy C/C++ numerical codes using MPLAPACK. MPLAPACK supports binary64, binary128, FP80 (extended double), MPFR, GMP, and QD libraries (double-double and quad-double). Users can choose MPFR or GMP for arbitrary accurate calculations, double-double or quad-double for fast 32 or 64-decimal calculations. We can consider the binary64 version as the C++ version of LAPACK. Moreover, it comes with an OpenMP accelerated version of MPBLAS for some routines and CUDA (A100 and V100 support) for double-double versions of Rgemm and Rsyrk. The peak performances of the OpenMP version are almost proportional to the number of cores, and the performances of the CUDA version are impressive, and approximately 400-600 GFlops. MPLAPACK is available at GitHub (https://github.com/nakatamaho/mplapack/) under the 2-clause BSD license.},
	urldate = {2023-03-12},
	publisher = {arXiv},
	author = {Nakata, Maho},
	month = sep,
	year = {2022},
	note = {arXiv:2109.13406 [cs]},
	keywords = {Computer Science - Mathematical Software},
}

@inproceedings{mukunoki_implementation_2012,
	title = {Implementation and {Evaluation} of {Triple} {Precision} {BLAS} {Subroutines} on {GPUs}},
	doi = {10.1109/IPDPSW.2012.175},
	abstract = {We implemented and evaluated the triple precision Basic Linear Algebra Subprograms (BLAS) subroutines, AXPY, GEMV and GEMM on a Tesla C2050. In this paper, we present a Double Single (D+S) type triple precision floating-point value format and operations. They are based on techniques similar to Double-Double (DD) type quadruple precision operations. On the GPU, the D+S-type operations are more costly than the DD-type operations in theory and in practice. Therefore, the triple precision GEMM, which is a compute-bound operation, is slower than the quadruple precision GEMM. However, the triple precision AXPY and GEMV are memory-bound operations on the GPU, thus their execution time of these triple precision subroutines is close to 3/4 of the quadruple precision subroutines. Therefore, we conclude that the triple precision value format is useful for memory-bound operations, in cases where the quadruple precision is not required, but double precision is not sufficient.},
	booktitle = {2012 {IEEE} 26th {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} \& {PhD} {Forum}},
	author = {Mukunoki, Daichi and Takahashi, Daisuke},
	month = may,
	year = {2012},
	keywords = {Algorithms, Arrays, BLAS, GPU, Graphics processing unit, Instruction sets, Kernel, Layout, Libraries, triple precision},
	pages = {1378--1386},
}

@article{li_design_2002,
	title = {Design, implementation and testing of extended and mixed precision {BLAS}},
	volume = {28},
	issn = {0098-3500},
	url = {https://doi.org/10.1145/567806.567808},
	doi = {10.1145/567806.567808},
	abstract = {This article describes the design rationale, a C implementation, and conformance testing of a subset of the new Standard for the BLAS (Basic Linear Algebra Subroutines): Extended and Mixed Precision BLAS. Permitting higher internal precision and mixed input/output types and precisions allows us to implement some algorithms that are simpler, more accurate, and sometimes faster than possible without these features. The new BLAS are challenging to implement and test because there are many more subroutines than in the existing Standard, and because we must be able to assess whether a higher precision is used for internal computations than is used for either input or output variables. We have therefore developed an automated process of generating and systematically testing these routines. Our methodology is applicable to languages besides C. In particular, our algorithms used in the testing code will be valuable to all other BLAS implementors. Our extra precision routines achieve excellent performance---close to half of the machine peak Megaflop rate even for the Level 2 BLAS, when the data access is stride one.},
	number = {2},
	urldate = {2023-03-12},
	journal = {ACM Transactions on Mathematical Software},
	author = {Li, Xiaoye S. and Demmel, James W. and Bailey, David H. and Henry, Greg and Hida, Yozo and Iskandar, Jimmy and Kahan, William and Kang, Suh Y. and Kapur, Anil and Martin, Michael C. and Thompson, Brandon J. and Tung, Teresa and Yoo, Daniel J.},
	month = jun,
	year = {2002},
	keywords = {BLAS, double-double arithmetic, extended and mixed precision},
	pages = {152--205},
}

@article{nakasato_fast_2011,
	title = {A fast {GEMM} implementation on the cypress {GPU}},
	volume = {38},
	issn = {0163-5999},
	url = {https://doi.org/10.1145/1964218.1964227},
	doi = {10.1145/1964218.1964227},
	abstract = {We present benchmark results of optimized dense matrix multiplication kernels for Cypress GPU. We write general matrix multiply (GEMM) kernels for single (SP), double (DP) and double-double (DDP) precision. Our SGEMM and DGEMM kernels show {\textasciitilde} 2 Top/s and {\textasciitilde} 470 Glop/s, respectively. These results for SP and DP correspond to 73\% and 87\% of the theoretical performance of the GPU, respectively. Currently, our SGEMM and DGEMM kernels are fastest with one GPU chip to our knowledge. Furthermore, the performance of our matrix multiply kernel in DDP is 31 Gop/s. This performance in DDP is more than 200 times faster than the performance results in DDP on single core of a recent CPU (with mpack version 0.6.5). We describe our GEMM kernels with main focus on the SGEMM implementation since all GEMM kernels share common programming and optimization techniques. While a conventional wisdom of GPU programming recommends us to heavily use shared memory on GPUs, we show that texture cache is very effective on the Cypress architecture.},
	number = {4},
	urldate = {2023-03-12},
	journal = {ACM SIGMETRICS Performance Evaluation Review},
	author = {Nakasato, Naohito},
	month = mar,
	year = {2011},
	pages = {50--55},
}

@inproceedings{haidar_harnessing_2018,
	title = {Harnessing {GPU} {Tensor} {Cores} for {Fast} {FP16} {Arithmetic} to {Speed} up {Mixed}-{Precision} {Iterative} {Refinement} {Solvers}},
	doi = {10.1109/SC.2018.00050},
	abstract = {Low-precision floating-point arithmetic is a powerful tool for accelerating scientific computing applications, especially those in artificial intelligence. Here, we present an investigation showing that other high-performance computing (HPC) applications can also harness this power. Specifically, we use the general HPC problem, Ax b, where A is a large dense matrix, and a double precision (FP64) solution is needed for accuracy. Our approach is based on mixed-precision (FP16-FP64) iterative refinement, and we generalize and extend prior advances into a framework, for which we develop architecture-specific algorithms and highly tuned implementations. These new methods show how using half-precision Tensor Cores (FP16-TC) for the arithmetic can provide up to 4× speedup. This is due to the performance boost that the FP16-TC provide as well as to the improved accuracy over the classical FP16 arithmetic that is obtained because the GEMM accumulation occurs in FP32 arithmetic.},
	booktitle = {{SC18}: {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Haidar, Azzam and Tomov, Stanimire and Dongarra, Jack and Higham, Nicholas J.},
	month = nov,
	year = {2018},
	keywords = {Acceleration, FP16 Arithmetic, GPU Computing, Graphics processing units, Half Precision, Iterative Refinement Computation, Iterative algorithms, Linear Algebra, Matrices, Mixed Precision Solvers},
	pages = {603--613},
}

@inproceedings{collange_parallel_2016,
	title = {Parallel floating-point expansions for extended-precision {GPU} computations},
	doi = {10.1109/ASAP.2016.7760783},
	abstract = {GPUs are an important hardware development platform for problems where massive parallel computations are needed. Many of these problems require a higher precision than the standard double floating-point (FP) available. One common way of extending the precision is the multiple-component approach, in which real numbers are represented as the unevaluated sum of several standard machine precision FP numbers. This representation is called a FP expansion and it offers the simplicity of using directly available and highly optimized FP operations. In this article we present new data-parallel algorithms for adding and multiplying FP expansions specially designed for extended precision computations on GPUs. These are generalized algorithms that can manipulate FP expansions of different sizes (from double-double up to a few tens of doubles) and ensure a certain worst case error bound on the results.},
	booktitle = {2016 {IEEE} 27th {International} {Conference} on {Application}-specific {Systems}, {Architectures} and {Processors} ({ASAP})},
	author = {Collange, Caroline and Joldes, Mioara and Muller, Jean-Michel and Popescu, Valentina},
	month = jul,
	year = {2016},
	note = {ISSN: 2160-052X},
	keywords = {Algorithm design and analysis, Approximation algorithms, Graphics processing units, Heuristic algorithms, Instruction sets, Parallel processing, Standards, floating-point arithmetic, floating-point expansions, graphics processing unit, high precision arithmetic, multiple-precision arithmetic, parallel computations},
	pages = {139--146},
}

@inproceedings{mukunoki_accurate_2021,
	address = {New York, NY, USA},
	series = {{ICPP} '21},
	title = {Accurate {Matrix} {Multiplication} on {Binary128} {Format} {Accelerated} by {Ozaki} {Scheme}},
	isbn = {978-1-4503-9068-2},
	url = {https://doi.org/10.1145/3472456.3472493},
	doi = {10.1145/3472456.3472493},
	abstract = {Although IEEE 754-2008 binary128 (with a 15-bit exponent and 113-bit significand, i.e., quadruple-precision) is not currently implemented on x86 in hardware, software emulation is available on some compilers. However, the performance is significantly slower compared to the binary64 operation, which is supported natively in hardware. This study proposes a fast implementation of matrix multiplication on matrices stored in the binary128 format on x86 CPUs. The proposed implementation utilizes the Ozaki scheme, which is an accurate matrix multiplication algorithm proposed by Ozaki et al. in 2012. This scheme enables one to perform most computations using the binary64 matrix multiplication (the DGEMM routine in Basic Linear Algebra Subprograms (BLAS)); it can exploit the high-performance of highly-optimized vendor BLAS. Although the achievable performance depends on the input matrices (the inner-product dimension, the absolute range, and the significand bit length), the proposed implementation can achieve better performance and accuracy compared to naive matrix multiplication performed using the GCC’s binary128 emulation in many cases. In addition, we discuss GPU acceleration, performance on reduced precision inputs, an implementation based on binary32 matrix multiplication (SGEMM), application to memory-intensive operations, and the possibility of a distributed parallel implementation.},
	urldate = {2023-03-11},
	booktitle = {Proceedings of the 50th {International} {Conference} on {Parallel} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Mukunoki, Daichi and Ozaki, Katsuhisa and Ogita, Takeshi and Imamura, Toshiyuki},
	month = oct,
	year = {2021},
	keywords = {Accurate, BLAS, Binary128, FP128, Linear algebra, Matrix multiplication, Quadruple precision},
	pages = {1--11},
}

@article{abdelfattah_survey_2021,
	title = {A survey of numerical linear algebra methods utilizing mixed-precision arithmetic},
	volume = {35},
	issn = {1094-3420},
	url = {https://doi.org/10.1177/10943420211003313},
	doi = {10.1177/10943420211003313},
	abstract = {The efficient utilization of mixed-precision numerical linear algebra algorithms can offer attractive acceleration to scientific computing applications. Especially with the hardware integration of low-precision special-function units designed for machine learning applications, the traditional numerical algorithms community urgently needs to reconsider the floating point formats used in the distinct operations to efficiently leverage the available compute power. In this work, we provide a comprehensive survey of mixed-precision numerical linear algebra routines, including the underlying concepts, theoretical background, and experimental results for both dense and sparse linear algebra problems.},
	language = {en},
	number = {4},
	urldate = {2023-03-12},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Abdelfattah, Ahmad and Anzt, Hartwig and Boman, Erik G and Carson, Erin and Cojean, Terry and Dongarra, Jack and Fox, Alyson and Gates, Mark and Higham, Nicholas J and Li, Xiaoye S and Loe, Jennifer and Luszczek, Piotr and Pranesh, Srikara and Rajamanickam, Siva and Ribizel, Tobias and Smith, Barry F and Swirydowicz, Kasia and Thomas, Stephen and Tomov, Stanimire and Tsai, Yaohung M and Yang, Ulrike Meier},
	month = jul,
	year = {2021},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {Computer Science - Mathematical Software, G.1.3, G.4, Mathematics - Numerical Analysis},
	pages = {344--369},
}

@inproceedings{mukunoki_implementation_2012-1,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Implementation and {Evaluation} of {Quadruple} {Precision} {BLAS} {Functions} on {GPUs}},
	isbn = {978-3-642-28151-8},
	doi = {10.1007/978-3-642-28151-8_25},
	abstract = {We implemented the quadruple precision Basic Linear Algebra Subprograms (BLAS) functions, AXPY, GEMV and GEMM, on graphics processing units (GPUs), and evaluated their performance. We used DD-type quadruple precision operations, which combine two double precision values to represent a quadruple precision value. On an NVIDIA Tesla C1060, our BLAS functions are up to approximately 30 times faster than the existing quadruple precision BLAS on an Intel Core i7 920. Additionally, the execution time of quadruple precision AXPY takes only approximately 2.7 times longer than that of double precision AXPY on the Tesla C1060. We have shown that quadruple precision BLAS operations are suitable for GPUs.},
	language = {en},
	booktitle = {Applied {Parallel} and {Scientific} {Computing}},
	publisher = {Springer},
	author = {Mukunoki, Daichi and Takahashi, Daisuke},
	editor = {Jónasson, Kristján},
	year = {2012},
	keywords = {GPU, double-double precision, quadruple precision BLAS},
	pages = {249--259},
}

@misc{parikh_cascading_2023,
	title = {Cascading {GEMM}: {High} {Precision} from {Low} {Precision}},
	shorttitle = {Cascading {GEMM}},
	url = {http://arxiv.org/abs/2303.04353},
	doi = {10.48550/arXiv.2303.04353},
	abstract = {This paper lays out insights and opportunities for implementing higher-precision matrix-matrix multiplication (GEMM) from (in terms of) lower-precision high-performance GEMM. The driving case study approximates double-double precision (FP64x2) GEMM in terms of double precision (FP64) GEMM, leveraging how the BLAS-like Library Instantiation Software (BLIS) framework refactors the Goto Algorithm. With this, it is shown how approximate FP64x2 GEMM accuracy can be cast in terms of ten ``cascading'' FP64 GEMMs. Promising results from preliminary performance and accuracy experiments are reported. The demonstrated techniques open up new research directions for more general cascading of higher-precision computation in terms of lower-precision computation for GEMM-like functionality.},
	urldate = {2023-03-12},
	publisher = {arXiv},
	author = {Parikh, Devangi N. and van de Geijn, Robert A. and Henry, Greg M.},
	month = mar,
	year = {2023},
	note = {arXiv:2303.04353 [cs]},
	keywords = {Computer Science - Mathematical Software, G.4},
}

@article{chen_64-qubit_2018,
	title = {64-qubit quantum circuit simulation},
	volume = {63},
	issn = {2095-9273},
	url = {https://www.sciencedirect.com/science/article/pii/S2095927318302809},
	doi = {10.1016/j.scib.2018.06.007},
	abstract = {Classical simulations of quantum circuits are limited in both space and time when the qubit count is above 50, the realm where quantum supremacy reigns. However, recently, for the low depth circuit with more than 50 qubits, there are several methods of simulation proposed by teams at Google and IBM. Here, we present a scheme of simulation which can extract a large amount of measurement outcomes within a short time, achieving a 64-qubit simulation of a universal random circuit of depth 22 using a 128-node cluster, and 56- and 42-qubit circuits on a single PC. We also estimate that a 72-qubit circuit of depth 23 can be simulated in about 16 h on a supercomputer identical to that used by the IBM team. Moreover, the simulation processes are exceedingly separable, hence parallelizable, involving just a few inter-process communications. Our work enables simulating more qubits with less hardware burden and provides a new perspective for classical simulations.},
	language = {en},
	number = {15},
	urldate = {2023-03-10},
	journal = {Science Bulletin},
	author = {Chen, Zhao-Yun and Zhou, Qi and Xue, Cheng and Yang, Xia and Guo, Guang-Can and Guo, Guo-Ping},
	month = aug,
	year = {2018},
	keywords = {Parallel computing, Partitioning, Quantum supremacy, Simulation of quantum circuits, Universal random circuit},
	pages = {964--971},
}

@inproceedings{ootomo_reducing_2023,
	address = {New York, NY, USA},
	series = {{HPC} {Asia} '23},
	title = {Reducing shared memory footprint to leverage high throughput on {Tensor} {Cores} and its flexible {API} extension library},
	isbn = {978-1-4503-9805-3},
	url = {https://doi.org/10.1145/3578178.3578238},
	doi = {10.1145/3578178.3578238},
	abstract = {Matrix-matrix multiplication is used for various linear algebra algorithms such as matrix decomposition and tensor contraction. NVIDIA Tensor Core is a mixed-precision matrix-matrix multiplication and addition computing unit, where the theoretical peak performance is more than 300 TFlop/s on NVIDIA A100 GPU. NVIDIA provides WMMA API for using Tensor Cores in custom kernel functions. The most common way to use Tensor Core is to supply the input matrices from shared memory, which has higher bandwidth than global memory. However, the Bytes-per-Flops (B/F) ratio of the shared memory and Tensor Cores is small since the performance of Tensor Cores is high. Thus, it is important to reduce the shared memory footprint for efficient Tensor Cores usage. In this paper, we analyze the simple matrix-matrix multiplication on Tensor Cores by the roofline model and figure out that the bandwidth of shared memory might be a limitation of the performance when using WMMA API. To alleviate this issue, we provide a WMMA API extension library to boost the throughput of the computation, which has two components. The first one allows for manipulating the array of registers input to Tensor Cores flexibly. We evaluate the performance improvement of this library. The outcome of our evaluation shows that our library reduces the shared memory footprint and speeds up the computation using Tensor Cores. The second one is an API for the SGEMM emulation on Tensor Cores without additional shared memory usage. We have demonstrated that the single-precision emulating batch SGEMM implementation on Tensor Cores using this library achieves 54.2 TFlop/s on A100 GPU, which outperforms the theoretical peak performance of FP32 SIMT Cores while achieving the same level of accuracy as cuBLAS. The achieved throughput can not be achieved without reducing the shared memory footprint done by our library with the same amount of register usage.},
	urldate = {2023-03-03},
	booktitle = {Proceedings of the {International} {Conference} on {High} {Performance} {Computing} in {Asia}-{Pacific} {Region}},
	publisher = {Association for Computing Machinery},
	author = {Ootomo, Hiroyuki and Yokota, Rio},
	month = feb,
	year = {2023},
	keywords = {GPU, Tensor Cores, WMMA API},
	pages = {1--8},
}

@article{schlag_high-quality_2022,
	title = {High-{Quality} {Hypergraph} {Partitioning}},
	issn = {1084-6654},
	url = {https://doi.org/10.1145/3529090},
	doi = {10.1145/3529090},
	abstract = {Hypergraphs are a generalization of graphs where edges (aka nets) are allowed to connect more than two vertices. They have a similarly wide range of applications as graphs. This paper considers the fundamental and intensively studied problem of balanced hypergraph partitioning, which asks for partitioning the vertices into k disjoint blocks of bounded size while minimizing an objective function over the hyperedges. Here, we consider the two most commonly used objectives: the cut-net metric and the connectivity metric. We describe our open source hypergraph partitioner KaHyPar which is based on the successful multi-level approach – driving it to the extreme of using one level for (almost) every vertex. Using carefully designed data structures and dynamic update techniques, this approach turns out to have a very good time–quality tradeoff. We present two preprocessing techniques – pin sparsification using locality sensitive hashing and community detection based on the Louvain algorithm. The community structure is used to guide the coarsening process that incrementally contracts vertices. Portfolio-based partitioning of the contracted hypergraph then already achieves a good initial solution. While reversing the contraction process, a combination of several refinement techniques achieves a good final partitioning. In particular, we support a highly-localized local search that can directly produce a k-way partitioning and complement this with flow-based techniques that take a more global view. Optionally, a memetic algorithm evolves a pool of solution candidates to an overall good solution. We evaluate KaHyPar for a large set of instances from a wide range of application domains. With respect to quality, KaHyPar outperforms all previously considered systems that can handle large hypergraphs such as hMETIS, PaToH, Mondriaan, or Zoltan. Somewhat surprisingly, to some extend, this even extends to graph partitioners such as KaHIP when considering the special case of graphs. KaHyPar is also faster than most of these systems except for PaToH which represents a different speed–quality tradeoff.},
	urldate = {2023-02-10},
	journal = {ACM Journal of Experimental Algorithmics},
	author = {Schlag, Sebastian and Heuer, Tobias and Gottesbüren, Lars and Akhremtsev, Yaroslav and Schulz, Christian and Sanders, Peter},
	month = apr,
	year = {2022},
	note = {Just Accepted},
	keywords = {community detection, maximum flows, memetic algorithm, multilevel algorithm, partitioning, portfolio},
}

@article{smith_opt_einsum_2018,
	title = {opt{\textbackslash}\_einsum - {A} {Python} package for optimizing contraction order for einsum-like expressions},
	volume = {3},
	issn = {2475-9066},
	url = {https://joss.theoj.org/papers/10.21105/joss.00753},
	doi = {10.21105/joss.00753},
	abstract = {Smith et al., (2018). opt\_einsum - A Python package for optimizing contraction order for einsum-like expressions
. Journal of Open Source Software, 3(26), 753, https://doi.org/10.21105/joss.00753},
	language = {en},
	number = {26},
	urldate = {2023-01-25},
	journal = {Journal of Open Source Software},
	author = {Smith, Daniel G. a and Gray, Johnnie},
	month = jun,
	year = {2018},
	pages = {753},
}

@article{schlag_high-quality_2022-1,
	title = {High-{Quality} {Hypergraph} {Partitioning}},
	issn = {1084-6654},
	url = {https://doi.org/10.1145/3529090},
	doi = {10.1145/3529090},
	abstract = {Hypergraphs are a generalization of graphs where edges (aka nets) are allowed to connect more than two vertices. They have a similarly wide range of applications as graphs. This paper considers the fundamental and intensively studied problem of balanced hypergraph partitioning, which asks for partitioning the vertices into k disjoint blocks of bounded size while minimizing an objective function over the hyperedges. Here, we consider the two most commonly used objectives: the cut-net metric and the connectivity metric. We describe our open source hypergraph partitioner KaHyPar which is based on the successful multi-level approach – driving it to the extreme of using one level for (almost) every vertex. Using carefully designed data structures and dynamic update techniques, this approach turns out to have a very good time–quality tradeoff. We present two preprocessing techniques – pin sparsification using locality sensitive hashing and community detection based on the Louvain algorithm. The community structure is used to guide the coarsening process that incrementally contracts vertices. Portfolio-based partitioning of the contracted hypergraph then already achieves a good initial solution. While reversing the contraction process, a combination of several refinement techniques achieves a good final partitioning. In particular, we support a highly-localized local search that can directly produce a k-way partitioning and complement this with flow-based techniques that take a more global view. Optionally, a memetic algorithm evolves a pool of solution candidates to an overall good solution. We evaluate KaHyPar for a large set of instances from a wide range of application domains. With respect to quality, KaHyPar outperforms all previously considered systems that can handle large hypergraphs such as hMETIS, PaToH, Mondriaan, or Zoltan. Somewhat surprisingly, to some extend, this even extends to graph partitioners such as KaHIP when considering the special case of graphs. KaHyPar is also faster than most of these systems except for PaToH which represents a different speed–quality tradeoff.},
	urldate = {2023-01-25},
	journal = {ACM Journal of Experimental Algorithmics},
	author = {Schlag, Sebastian and Heuer, Tobias and Gottesbüren, Lars and Akhremtsev, Yaroslav and Schulz, Christian and Sanders, Peter},
	month = apr,
	year = {2022},
	note = {Just Accepted},
	keywords = {community detection, maximum flows, memetic algorithm, multilevel algorithm, partitioning, portfolio},
}

@misc{roberts_tensornetwork_2019,
	title = {{TensorNetwork}: {A} {Library} for {Physics} and {Machine} {Learning}},
	shorttitle = {{TensorNetwork}},
	url = {http://arxiv.org/abs/1905.01330},
	doi = {10.48550/arXiv.1905.01330},
	abstract = {TensorNetwork is an open source library for implementing tensor network algorithms. Tensor networks are sparse data structures originally designed for simulating quantum many-body physics, but are currently also applied in a number of other research areas, including machine learning. We demonstrate the use of the API with applications both physics and machine learning, with details appearing in companion papers.},
	urldate = {2023-01-25},
	publisher = {arXiv},
	author = {Roberts, Chase and Milsted, Ashley and Ganahl, Martin and Zalcman, Adam and Fontaine, Bruce and Zou, Yijian and Hidary, Jack and Vidal, Guifre and Leichenauer, Stefan},
	month = may,
	year = {2019},
	note = {arXiv:1905.01330 [cond-mat, physics:hep-th, physics:physics, stat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Strongly Correlated Electrons, High Energy Physics - Theory, Physics - Computational Physics, Statistics - Machine Learning},
}

@article{pan_simulation_2022,
	title = {Simulation of {Quantum} {Circuits} {Using} the {Big}-{Batch} {Tensor} {Network} {Method}},
	volume = {128},
	issn = {0031-9007, 1079-7114},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.128.030501},
	doi = {10.1103/PhysRevLett.128.030501},
	language = {en},
	number = {3},
	urldate = {2023-01-25},
	journal = {Physical Review Letters},
	author = {Pan, Feng and Zhang, Pan},
	month = jan,
	year = {2022},
	pages = {030501},
}

@article{huang_efficient_2021,
	title = {Efficient parallelization of tensor network contraction for simulating quantum computation},
	volume = {1},
	copyright = {2021 The Author(s)},
	issn = {2662-8457},
	url = {https://www.nature.com/articles/s43588-021-00119-7},
	doi = {10.1038/s43588-021-00119-7},
	abstract = {We develop an algorithmic framework for contracting tensor networks and demonstrate its power by classically simulating quantum computation of sizes previously deemed out of reach. Our main contribution, index slicing, is a method that efficiently parallelizes the contraction by breaking it down into much smaller and identically structured subtasks, which can then be executed in parallel without dependencies. We benchmark our algorithm on a class of random quantum circuits, achieving greater than 105 times acceleration over the original estimate of the simulation cost. We then demonstrate applications of the simulation framework for aiding the development of quantum algorithms and quantum error correction. As tensor networks are widely used in computational science, our simulation framework may find further applications.},
	language = {en},
	number = {9},
	urldate = {2023-01-25},
	journal = {Nature Computational Science},
	author = {Huang, Cupjin and Zhang, Fang and Newman, Michael and Ni, Xiaotong and Ding, Dawei and Cai, Junjie and Gao, Xun and Wang, Tenghui and Wu, Feng and Zhang, Gengyan and Ku, Hsiang-Sheng and Tian, Zhengxiong and Wu, Junyin and Xu, Haihong and Yu, Huanjun and Yuan, Bo and Szegedy, Mario and Shi, Yaoyun and Zhao, Hui-Hai and Deng, Chunqing and Chen, Jianxin},
	month = sep,
	year = {2021},
	note = {Number: 9
Publisher: Nature Publishing Group},
	keywords = {Computational science, Quantum information},
	pages = {578--587},
}

@inproceedings{muirhead_aspects_1982,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Aspects of {Multivariate} {Statistical} {Theory}},
	isbn = {978-0-470-31655-9 978-0-471-09442-5},
	url = {http://doi.wiley.com/10.1002/9780470316559},
	doi = {10.1002/9780470316559},
	abstract = {Tables. Commonly Used Notation. 1. The Multivariate Normal and Related Distributions. 2. Jacobians, Exterior Products, Kronecker Products, and Related Topics. 3. Samples from a Multivariate Normal Distribution, and the Wishart and Multivariate BETA Distributions. 4. Some Results Concerning Decision-Theoretic Estimation of the Parameters of a Multivariate Normal Distribution. 5. Correlation Coefficients. 6. Invariant Tests and Some Applications. 7. Zonal Polynomials and Some Functions of Matrix Argument. 8. Some Standard Tests on Covariance Matrices and Mean Vectors. 9. Principal Components and Related Topics. 10. The Multivariate Linear Model. 11. Testing Independence Between k Sets of Variables and Canonical Correlation Analysis. Appendix: Some Matrix Theory. Bibliography. Index.},
	language = {en},
	urldate = {2022-12-11},
	publisher = {John Wiley \& Sons, Inc.},
	editor = {Muirhead, Robb J.},
	month = mar,
	year = {1982},
	doi = {10.1002/9780470316559},
}

@article{singh_secdh_2022,
	title = {{SecDH}: {Security} of {COVID}-19 images based on data hiding with {PCA}},
	volume = {191},
	issn = {0140-3664},
	shorttitle = {{SecDH}},
	url = {https://www.sciencedirect.com/science/article/pii/S0140366422001621},
	doi = {10.1016/j.comcom.2022.05.010},
	abstract = {Nowadays, image security and copyright protection become challenging, especially after the COVID-19 pandemic. In the paper, we develop SecDH as a medical data hiding scheme, which can guarantee the security and copyright protection of the COVID-19 images. Firstly, the cover image is normalized, which offers high resistance against the geometric attacks. Secondly, the normalized principal component as embedding factor is computed, which are calculated based on principal component analysis (PCA) between cover and mark image. Thirdly, the medical image is invisibly marked with secret mark based on normalized component, redundant discrete wavelet transform (RDWT) and randomized singular value decomposition (RSVD) is introduced. Finally, Arnold cat map scheme employed to ensure the security of the watermarking system. Under the experimental evaluation, our SecDH tool is not only imperceptible, but also has a satisfactory advantage in robustness and security compared with the traditional watermarking schemes.},
	language = {en},
	urldate = {2022-12-08},
	journal = {Computer Communications},
	author = {Singh, O. P. and Singh, Amit Kumar and Agrawal, Amrit Kumar and Zhou, Huiyu},
	month = jul,
	year = {2022},
	keywords = {Cyber physical system, Data hiding, Healthcare, PCA, Security},
	pages = {368--377},
}

@inproceedings{feng_faster_2018,
	title = {Faster {Matrix} {Completion} {Using} {Randomized} {SVD}},
	doi = {10.1109/ICTAI.2018.00098},
	abstract = {Matrix completion is a widely used technique for image inpainting and personalized recommender system, etc. In this work, we focus on accelerating the matrix completion using faster randomized singular value decomposition (rSVD). Firstly, two fast randomized algorithms (rSVD-PI and rSVDBKI) are proposed for handling sparse matrix. They make use of an eigSVD procedure and several accelerating skills. Then, with the rSVD-BKI algorithm and a new subspace recycling technique, we accelerate the singular value thresholding (SVT) method in [1] to realize faster matrix completion. Experiments show that the proposed rSVD algorithms can be 6× faster than the basic rSVD algorithm [2] while keeping same accuracy. For image inpainting and movie-rating estimation problems (including up to 2 × 107 ratings), the proposed accelerated SVT algorithm consumes 15× and 8× less CPU time than the methods using svds and lansvd respectively, without loss of accuracy.},
	booktitle = {2018 {IEEE} 30th {International} {Conference} on {Tools} with {Artificial} {Intelligence} ({ICTAI})},
	author = {Feng, Xu and Yu, Wenjian and Li, Yaohang},
	month = jan,
	year = {2018},
	note = {ISSN: 2375-0197},
	keywords = {Acceleration, Approximation algorithms, Computer science, Matlab, Matrix decomposition, Principal component analysis, Sparse matrices, image inpainting, matrix completion, randomized SVD, recommender system},
	pages = {608--615},
}

@article{intawichai_missing_2022,
	title = {A {Missing} {Data} {Reconstruction} {Method} {Using} an {Accelerated} {Least}-{Squares} {Approximation} with {Randomized} {SVD}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1999-4893},
	url = {https://www.mdpi.com/1999-4893/15/6/190},
	doi = {10.3390/a15060190},
	abstract = {An accelerated least-squares approach is introduced in this work by incorporating a greedy point selection method with randomized singular value decomposition (rSVD) to reduce the computational complexity of missing data reconstruction. The rSVD is used to speed up the computation of a low-dimensional basis that is required for the least-squares projection by employing randomness to generate a small matrix instead of a large matrix from high-dimensional data. A greedy point selection algorithm, based on the discrete empirical interpolation method, is then used to speed up the reconstruction process in the least-squares approximation. The accuracy and computational time reduction of the proposed method are demonstrated through three numerical experiments. The first two experiments consider standard testing images with missing pixels uniformly distributed on them, and the last numerical experiment considers a sequence of many incomplete two-dimensional miscible flow images. The proposed method is shown to accelerate the reconstruction process while maintaining roughly the same order of accuracy when compared to the standard least-squares approach.},
	language = {en},
	number = {6},
	urldate = {2022-12-08},
	journal = {Algorithms},
	author = {Intawichai, Siriwan and Chaturantabut, Saifon},
	month = jun,
	year = {2022},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {discrete empirical interpolation method, least-squares method, low-rank approximation, missing data approximation, randomized algorithm, singular value decomposition},
	pages = {190},
}

@inproceedings{wang_large-scale_2022,
	title = {Large-{Scale} {System} {Identification} {Using} a {Randomized} {SVD}},
	doi = {10.23919/ACC53348.2022.9867836},
	abstract = {Learning a dynamical system from input/output data is a fundamental task in the control design pipeline. In the partially observed setting there are two components to identification: parameter estimation to learn the Markov parameters, and system realization to obtain a state space model. In both sub-problems it is implicitly assumed that standard numerical algorithms such as the singular value decomposition (SVD) can be easily and reliably computed. When trying to fit a high-dimensional model to data, even computing an SVD may be intractable. In this work we show that an approximate matrix factorization obtained using randomized methods can replace the standard SVD in the realization algorithm while maintaining the finite-sample performance and robustness guarantees of classical methods.},
	booktitle = {2022 {American} {Control} {Conference} ({ACC})},
	author = {Wang, Han and Anderson, James},
	month = jun,
	year = {2022},
	note = {ISSN: 2378-5861},
	keywords = {Approximation algorithms, Computational modeling, Numerical models, Parameter estimation, Pipelines, Robustness, System realization},
	pages = {2178--2185},
}

@article{zhang_projection-based_nodate,
	title = {Projection-based techniques for high-dimensional optimal transport problems},
	volume = {n/a},
	issn = {1939-0068},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.1587},
	doi = {10.1002/wics.1587},
	abstract = {Optimal transport (OT) methods seek a transformation map (or plan) between two probability measures, such that the transformation has the minimum transportation cost. Such a minimum transport cost, with a certain power transform, is called the Wasserstein distance. Recently, OT methods have drawn great attention in statistics, machine learning, and computer science, especially in deep generative neural networks. Despite its broad applications, the estimation of high-dimensional Wasserstein distances is a well-known challenging problem owing to the curse-of-dimensionality. There are some cutting-edge projection-based techniques that tackle high-dimensional OT problems. Three major approaches of such techniques are introduced, respectively, the slicing approach, the iterative projection approach, and the projection robust OT approach. Open challenges are discussed at the end of the review. This article is categorized under: Statistical and Graphical Methods of Data Analysis {\textgreater} Dimension Reduction Statistical Learning and Exploratory Methods of the Data Sciences {\textgreater} Manifold Learning},
	language = {en},
	number = {n/a},
	urldate = {2022-12-08},
	journal = {WIREs Computational Statistics},
	author = {Zhang, Jingyi and Ma, Ping and Zhong, Wenxuan and Meng, Cheng},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1587},
	keywords = {Wasserstein distance, curse of dimensionality, dimension reduction, optimal transport},
	pages = {e1587},
}

@inproceedings{chiu_self-supervised_2022,
	title = {Self-supervised learning with random-projection quantizer for speech recognition},
	url = {https://proceedings.mlr.press/v162/chiu22a.html},
	abstract = {We present a simple and effective self-supervised learning approach for speech recognition. The approach learns a model to predict the masked speech signals, in the form of discrete labels generated with a random-projection quantizer. In particular the quantizer projects speech inputs with a randomly initialized matrix, and does a nearest-neighbor lookup in a randomly-initialized codebook. Neither the matrix nor the codebook are updated during self-supervised learning. Since the random-projection quantizer is not trained and is separated from the speech recognition model, the design makes the approach flexible and is compatible with universal speech recognition architecture. On LibriSpeech our approach achieves similar word-error-rates as previous work using self-supervised learning with non-streaming models, and provides lower word-error-rates than previous work with streaming models. On multilingual tasks the approach also provides significant improvement over wav2vec 2.0 and w2v-BERT.},
	language = {en},
	urldate = {2022-12-08},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chiu, Chung-Cheng and Qin, James and Zhang, Yu and Yu, Jiahui and Wu, Yonghui},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {3915--3924},
}

@inproceedings{hegde_random_2007,
	title = {Random {Projections} for {Manifold} {Learning}},
	volume = {20},
	url = {https://proceedings.neurips.cc/paper/2007/hash/1e6e0a04d20f50967c64dac2d639a577-Abstract.html},
	urldate = {2022-12-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Hegde, Chinmay and Wakin, Michael and Baraniuk, Richard},
	year = {2007},
}

@article{chen_efficient_2018,
	title = {Efficient extreme learning machine via very sparse random projection},
	volume = {22},
	issn = {1433-7479},
	url = {https://doi.org/10.1007/s00500-018-3128-7},
	doi = {10.1007/s00500-018-3128-7},
	abstract = {Extreme learning machine (ELM) is a kind of random projection-based neural networks, whose advantages are fast training speed and high generalization. However, three issues can be improved in ELM: (1) the calculation of output weights takes \$\$O{\textbackslash}left( \{L{\textasciicircum}\{2\}N\} {\textbackslash}right) \$\$time (with N training samples and L hidden nodes), which is relatively slow to train a model for large N and L; (2) the manual tuning of L is tedious, exhaustive and time-consuming; (3) the redundant or irrelevant information in the hidden layer may cause overfitting and may hinder high generalization. Inspired from compressive sensing theory, we propose an efficient ELM via very sparse random projection (VSRP) called VSRP-ELM for training with large N and L. The proposed VSRP-ELM adds a novel compression layer between the hidden layer and output layer, which compresses the dimension of the hidden layer from \$\$N{\textbackslash}times L\$\$to \$\$N{\textbackslash}times k {\textbackslash},({\textbackslash}hbox \{where \} k{\textless}L)\$\$under projection with random sparse-Bernoulli matrix. The advantages of VSRP-ELM are (1) faster training time \$\$O{\textbackslash}left( \{k{\textasciicircum}\{2\}N\} {\textbackslash}right) , k{\textless}L,\$\$is obtained for large L; (2) the tuning time of L can be significantly reduced by initializing a large L, and then shrunk to k using just a few trials, while maintaining a comparable result of the original model accuracy; (3) higher generalization may be benefited from the cleaning of redundant or irrelevant information through VSRP. From the experimental results, the proposed VSRP-ELM can speed ELM up to 7 times, while the accuracy can be improved up to 6\%.},
	language = {en},
	number = {11},
	urldate = {2022-12-08},
	journal = {Soft Computing},
	author = {Chen, Chuangquan and Vong, Chi-Man and Wong, Chi-Man and Wang, Weiru and Wong, Pak-Kin},
	month = jun,
	year = {2018},
	keywords = {Compression layer, Dimension reduction, Extreme learning machine (ELM), Sparse-Bernoulli matrix, Very sparse random projection},
	pages = {3563--3574},
}

@inproceedings{fern_random_2003,
	address = {Washington, DC, USA},
	series = {{ICML}'03},
	title = {Random projection for high dimensional data clustering: a cluster ensemble approach},
	isbn = {978-1-57735-189-4},
	shorttitle = {Random projection for high dimensional data clustering},
	abstract = {We investigate how random projection can best be used for clustering high dimensional data. Random projection has been shown to have promising theoretical properties. In practice, however, we find that it results in highly unstable clustering performance. Our solution is to use random projection in a cluster ensemble approach. Empirical results show that the proposed approach achieves better and more robust clustering performance compared to not only single runs of random projection/clustering but also clustering with PCA, a traditional data reduction method for high dimensional data. To gain insights into the performance improvement obtained by our ensemble method, we analyze and identify the influence of the quality and the diversity of the individual clustering solutions on the final ensemble performance.},
	urldate = {2022-12-08},
	booktitle = {Proceedings of the {Twentieth} {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {AAAI Press},
	author = {Fern, Xiaoli Zhang and Brodley, Carla E.},
	year = {2003},
	pages = {186--193},
}

@inproceedings{vinh_training_2016,
	title = {Training robust models using {Random} {Projection}},
	doi = {10.1109/ICPR.2016.7899688},
	abstract = {Regularization plays an important role in machine learning systems. We propose a novel methodology for model regularization using random projection. We demonstrate the technique on neural networks, since such models usually comprise a very large number of parameters, calling for strong regularizers. It has been shown recently that neural networks are sensitive to two kinds of samples: (i) adversarial samples, which are generated by imperceptible perturbations of previously correctly-classified samples—yet the network will misclassify them; and (ii) fooling samples, which are completely unrecognizable, yet the network will classify them with extremely high confidence. In this paper, we show how robust neural networks can be trained using random projection. We show that while random projection acts as a strong regularizer, boosting model accuracy similar to other regularizers, such as weight decay and dropout, it is far more robust to adversarial noise and fooling samples. We further show that random projection also helps to improve the robustness of traditional classifiers, such as Random Forrest and Gradient Boosting Machines.},
	booktitle = {2016 23rd {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Vinh, Nguyen Xuan and Erfani, Sarah and Paisitkriangkrai, Sakrapee and Bailey, James and Leckie, Christopher and Ramamohanarao, Kotagiri},
	month = feb,
	year = {2016},
	keywords = {Artificial neural networks, Data models, Learning systems, Robustness, Training, Training data},
	pages = {531--536},
}

@misc{nakatsukasa_fast_2020,
	title = {Fast and stable randomized low-rank matrix approximation},
	url = {http://arxiv.org/abs/2009.11392},
	doi = {10.48550/arXiv.2009.11392},
	abstract = {Randomized SVD has become an extremely successful approach for efficiently computing a low-rank approximation of matrices. In particular the paper by Halko, Martinsson, and Tropp (SIREV 2011) contains extensive analysis, and has made it a very popular method. The typical complexity for a rank-\$r\$ approximation of \$m{\textbackslash}times n\$ matrices is \$O(mn{\textbackslash}log n+(m+n)r{\textasciicircum}2)\$ for dense matrices. The classical Nystr\{{\textbackslash}"o\}m method is much faster, but applicable only to positive semidefinite matrices. This work studies a generalization of Nystr\{{\textbackslash}"o\}m method applicable to general matrices, and shows that (i) it has near-optimal approximation quality comparable to competing methods, (ii) the computational cost is the near-optimal \$O(mn{\textbackslash}log n+r{\textasciicircum}3)\$ for dense matrices, with small hidden constants, and (iii) crucially, it can be implemented in a numerically stable fashion despite the presence of an ill-conditioned pseudoinverse. Numerical experiments illustrate that generalized Nystr\{{\textbackslash}"o\}m can significantly outperform state-of-the-art methods, especially when \$r{\textbackslash}gg 1\$, achieving up to a 10-fold speedup. The method is also well suited to updating and downdating the matrix.},
	urldate = {2022-12-06},
	publisher = {arXiv},
	author = {Nakatsukasa, Yuji},
	month = sep,
	year = {2020},
	note = {Number: arXiv:2009.11392
arXiv:2009.11392 [cs, math]},
	keywords = {65F55, 68W20, Mathematics - Numerical Analysis},
}

@article{gu_efficient_2012,
	title = {Efficient {Algorithms} for {Computing} a {Strong} {Rank}-{Revealing} {QR} {Factorization}},
	copyright = {Copyright © 1996 Society for Industrial and Applied Mathematics},
	url = {https://epubs.siam.org/doi/10.1137/0917055},
	doi = {10.1137/0917055},
	abstract = {Given an \$m {\textbackslash}times n\$ matrix M with \$m {\textbackslash}geqslant n\$, it is shown that there exists a permutation \${\textbackslash}Pi \$ and an integer k such that the QR factorization {\textbackslash}[ M{\textbackslash}Pi = Q{\textbackslash}left( \{{\textbackslash}begin\{array\}\{*\{20\}c\} \{A\_k \} \& \{B\_k \} {\textbackslash}{\textbackslash} \{\} \& \{C\_k \} {\textbackslash}{\textbackslash} {\textbackslash}end\{array\} \} {\textbackslash}right) {\textbackslash}] reveals the numerical rank of M: the \$k {\textbackslash}times k\$ upper-triangular matrix \$A\_k \$ is well conditioned, \${\textbackslash}{\textbar}C\_k {\textbackslash}{\textbar}\_2 \$ is small, and \$B\_k \$is linearly dependent on \$A\_k \$ with coefficients bounded by a low-degree polynomial in n. Existing rank-revealing QR (RRQR) algorithms are related to such factorizations and two algorithms are presented for computing them. The new algorithms are nearly as efficient as QR with column pivoting for most problems and take \$O(mn{\textasciicircum}2 )\$ floating-point operations in the worst case.},
	language = {en},
	urldate = {2022-11-26},
	journal = {SIAM Journal on Scientific Computing},
	author = {Gu, Ming and Eisenstat, Stanley C.},
	month = feb,
	year = {2012},
	note = {Publisher: Society for Industrial and Applied Mathematics},
}

@article{zhang_randomized_2012,
	title = {Randomized {SVD} methods in hyperspectral imaging},
	volume = {2012},
	issn = {2090-0147},
	url = {https://doi.org/10.1155/2012/409357},
	doi = {10.1155/2012/409357},
	abstract = {We present a randomized singular value decomposition (rSVD) method for the purposes of lossless compression, reconstruction, classification, and target detection with hyperspectral (HSI) data. Recent work in low-rank matrix approximations obtained from random projections suggests that these approximations are well suited for randomized dimensionality reduction. Approximation errors for the rSVD are evaluated on HSI, and comparisons aremade to deterministic techniques and as well as to other randomized low-rank matrix approximation methods involving compressive principal component analysis. Numerical tests on real HSI data suggest that the method is promising and is particularly effective for HSI data interrogation.},
	urldate = {2022-10-30},
	journal = {Journal of Electrical and Computer Engineering},
	author = {Zhang, Jiani and Erway, Jennifer and Hu, Xiaofei and Zhang, Qiang and Plemmons, Robert},
	year = {2012},
	pages = {3:3},
}

@article{li_large-scale_2015,
	title = {Large-{Scale} {Nyström} {Kernel} {Matrix} {Approximation} {Using} {Randomized} {SVD}},
	volume = {26},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2014.2359798},
	abstract = {The Nyström method is an efficient technique for the eigenvalue decomposition of large kernel matrices. However, to ensure an accurate approximation, a sufficient number of columns have to be sampled. On very large data sets, the singular value decomposition (SVD) step on the resultant data submatrix can quickly dominate the computations and become prohibitive. In this paper, we propose an accurate and scalable Nyström scheme that first samples a large column subset from the input matrix, but then only performs an approximate SVD on the inner submatrix using the recent randomized low-rank matrix approximation algorithms. Theoretical analysis shows that the proposed algorithm is as accurate as the standard Nyström method that directly performs a large SVD on the inner submatrix. On the other hand, its time complexity is only as low as performing a small SVD. Encouraging results are obtained on a number of large-scale data sets for low-rank approximation. Moreover, as the most computational expensive steps can be easily distributed and there is minimal data transfer among the processors, significant speedup can be further obtained with the use of multiprocessor and multi-GPU systems.},
	number = {1},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Li, Mu and Bi, Wei and Kwok, James T. and Lu, Bao-Liang},
	month = jan,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Algorithm design and analysis, Approximation algorithms, Approximation methods, Distributed computing, Kernel, Matrix decomposition, Nyström method, Standards, Time complexity, graphics processor, large-scale learning, low-rank matrix approximation, randomized SVD, randomized SVD.},
	pages = {152--164},
}

@article{yang_low_2018,
	title = {Low rank approximation methods for {MR} fingerprinting with large scale dictionaries},
	volume = {79},
	issn = {1522-2594},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.26867},
	doi = {10.1002/mrm.26867},
	abstract = {Purpose This work proposes new low rank approximation approaches with significant memory savings for large scale MR fingerprinting (MRF) problems. Theory and Methods We introduce a compressed MRF with randomized singular value decomposition method to significantly reduce the memory requirement for calculating a low rank approximation of large sized MRF dictionaries. We further relax this requirement by exploiting the structures of MRF dictionaries in the randomized singular value decomposition space and fitting them to low-degree polynomials to generate high resolution MRF parameter maps. In vivo 1.5T and 3T brain scan data are used to validate the approaches. Results T1, T2, and off-resonance maps are in good agreement with that of the standard MRF approach. Moreover, the memory savings is up to 1000 times for the MRF-fast imaging with steady-state precession sequence and more than 15 times for the MRF-balanced, steady-state free precession sequence. Conclusion The proposed compressed MRF with randomized singular value decomposition and dictionary fitting methods are memory efficient low rank approximation methods, which can benefit the usage of MRF in clinical settings. They also have great potentials in large scale MRF problems, such as problems considering multi-component MRF parameters or high resolution in the parameter space. Magn Reson Med 79:2392–2400, 2018. © 2017 International Society for Magnetic Resonance in Medicine.},
	language = {en},
	number = {4},
	urldate = {2022-10-30},
	journal = {Magnetic Resonance in Medicine},
	author = {Yang, Mingrui and Ma, Dan and Jiang, Yun and Hamilton, Jesse and Seiberlich, Nicole and Griswold, Mark A. and McGivney, Debra},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/mrm.26867},
	keywords = {MR fingerprinting, SVD, low rank approximation, polynomial fitting, quantitative imaging, randomized SVD},
	pages = {2392--2400},
}

@inproceedings{lyra-leite_improved_2012,
	title = {Improved {MRI} reconstruction and denoising using {SVD}-based low-rank approximation},
	doi = {10.1109/WEA.2012.6220082},
	abstract = {The reconstruction of multi-dimensional magnetic resonsance imaging (MRI) data can be a computationally demanding task. Signal-to-noise ratio is also a concern, specially in high-resolution imaging. Data compression may be useful not only for reducing reconstruction complexity and memory requirements, but also for reducing noise, as it is capable of eliminating spurious components. This work proposes the use of SVD-based low-rank approximation for the reconstruction and denoising of MRI data. The Akaike information criterion is used to estimate the appropriate model order. The model order is used to remove noisy components and to reduce the amount of data to be stored and processed. The proposed method is evaluated using in vivo MRI data. We present images reconstructed using less than 20\% visual inspection. A quantitative evaluation is also presented.},
	booktitle = {2012 {Workshop} on {Engineering} {Applications}},
	author = {Lyra-Leite, Davi Marco and da Costa, João Paulo Carvalho Lustosa and de Carvalho, João Luiz Azevedo},
	month = may,
	year = {2012},
	keywords = {Approximation methods, Image reconstruction, Inspection, Magnetic resonance, Magnetic resonance imaging, Nonhomogeneous media, Visualization},
	pages = {1--6},
}

@article{anand_cloud_2021,
	title = {Cloud based secure watermarking using {IWT}-{Schur}-{RSVD} with fuzzy inference system for smart healthcare applications},
	volume = {75},
	issn = {2210-6707},
	url = {https://www.sciencedirect.com/science/article/pii/S2210670721006715},
	doi = {10.1016/j.scs.2021.103398},
	abstract = {The healthcare records include personal and sensitive information about the patient. These records are easily distributed via different platform for clinical diagnosis purpose. Presently, large volumes of medical data are stored on cloud platform. However, outsourcing medical data to these popular platforms may introduce security issues. Thus, a secure watermarking algorithm based on integer wavelet transform (IWT)-Schur-Randomized Singular Value Decomposition (RSVD) is put forward for medical records sharing in the cloud environment. To ensure high level of authentication, discrete wavelet transform (DWT) based embedding is conducted to achieve dual watermarking by concealing system MAC address in the logo image. Prior to this, the system address is encoded via turbo code to reduce /eliminate the channel noise, if any. Further, both invisibility and robustness are achieved by fuzzy inference system (FIS), and the marked image is encrypted using chaotic encryption scheme to further improve the security. Furthermore, experimental results is reported and compared with the existing works which demonstrated promising outcomes, indicating its potential towards realization of a value-added tool for smart healthcare applications.},
	language = {en},
	urldate = {2022-10-30},
	journal = {Sustainable Cities and Society},
	author = {Anand, Ashima and Singh, Amit Kumar},
	month = dec,
	year = {2021},
	keywords = {Cloud computing, Encryption, Fuzzy inference system, IoMT, Smart city, Turbo code, Watermarking},
	pages = {103398},
}

@inproceedings{erichson_compressed_2017,
	title = {Compressed {Singular} {Value} {Decomposition} for {Image} and {Video} {Processing}},
	doi = {10.1109/ICCVW.2017.222},
	abstract = {We demonstrate a heuristic algorithm to compute the approximate low-rank singular value decomposition. The algorithm is inspired by ideas from compressed sensing and, in particular, is suitable for image and video processing applications. Specifically, our compressed singular value decomposition (cSVD) algorithm employs aggressive random test matrices to efficiently sketch the row space of the input matrix. The resulting compressed representation of the data enables the computation of an accurate approximation of the dominant high-dimensional left and right singular vectors. We benchmark cSVD against the current state-of-the-art randomized SVD and show a performance boost while attaining near similar relative errors. The cSVD is simple to implement as well as embarrassingly parallel, i.e, ideally suited for GPU computations and mobile platforms.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} {Workshops} ({ICCVW})},
	author = {Erichson, N. Benjamin and Brunton, Steven L. and Kutz, J. Nathan},
	month = oct,
	year = {2017},
	note = {ISSN: 2473-9944},
	keywords = {Approximation algorithms, Compressed sensing, Eigenvalues and eigenfunctions, Image coding, Matrix decomposition, Singular value decomposition, Sparse matrices},
	pages = {1880--1888},
}

@article{rokhlin_fast_2008,
	title = {A fast randomized algorithm for overdetermined linear least-squares regression},
	volume = {105},
	url = {https://www.pnas.org/doi/10.1073/pnas.0804869105},
	doi = {10.1073/pnas.0804869105},
	abstract = {We introduce a randomized algorithm for overdetermined linear least-squares regression. Given an arbitrary full-rank m × n matrix A with m ≥ n, any m × 1 vector b, and any positive real number ε, the procedure computes an n × 1 vector x such that x minimizes the Euclidean norm ‖Ax − b‖ to relative precision ε. The algorithm typically requires 𝒪((log(n)+log(1/ε))mn+n3) floating-point operations. This cost is less than the 𝒪(mn2) required by the classical schemes based on QR-decompositions or bidiagonalization. We present several numerical examples illustrating the performance of the algorithm.},
	number = {36},
	urldate = {2022-10-30},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Rokhlin, Vladimir and Tygert, Mark},
	month = sep,
	year = {2008},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {13212--13217},
}

@article{shabat_randomized_2018,
	title = {Randomized {LU} decomposition},
	volume = {44},
	issn = {1063-5203},
	url = {https://www.sciencedirect.com/science/article/pii/S1063520316300069},
	doi = {10.1016/j.acha.2016.04.006},
	abstract = {Randomized algorithms play a central role in low rank approximations of large matrices. In this paper, the scheme of the randomized SVD is extended to a randomized LU algorithm. Several error bounds are introduced, that are based on recent results from random matrix theory related to subgaussian matrices. The bounds also improve the existing bounds of already known randomized SVD algorithm. The algorithm is fully parallelized and thus can utilize efficiently GPUs without any CPU–GPU data transfer. Numerical examples, which illustrate the performance of the algorithm and compare it to other decomposition methods, are presented.},
	language = {en},
	number = {2},
	urldate = {2022-10-30},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Shabat, Gil and Shmueli, Yaniv and Aizenbud, Yariv and Averbuch, Amir},
	month = mar,
	year = {2018},
	keywords = {LU decomposition, Matrix factorizations, Random matrices, Randomized algorithms},
	pages = {246--272},
}

@article{lopez-sanchez_tuning_2022,
	title = {Tuning {Database}-{Friendly} {Random} {Projection} {Matrices} for {Improved} {Distance} {Preservation} on {Specific} {Data}},
	volume = {52},
	issn = {1573-7497},
	url = {https://doi.org/10.1007/s10489-021-02626-6},
	doi = {10.1007/s10489-021-02626-6},
	abstract = {Random Projection is one of the most popular and successful dimensionality reduction algorithms for large volumes of data. However, given its stochastic nature, different initializations of the projection matrix can lead to very different levels of performance. This paper presents a guided random search algorithm to mitigate this problem. The proposed method uses a small number of training data samples to iteratively adjust a projection matrix, improving its performance on similarly distributed data. Experimental results show that projection matrices generated with the proposed method result in a better preservation of distances between data samples. Conveniently, this is achieved while preserving the database-friendliness of the projection matrix, as it remains sparse and comprised exclusively of integers after being tuned with our algorithm. Moreover, running the proposed algorithm on a consumer-grade CPU requires only a few seconds.},
	language = {en},
	number = {5},
	urldate = {2022-10-30},
	journal = {Applied Intelligence},
	author = {López-Sánchez, Daniel and de Bodt, Cyril and Lee, John A. and Arrieta, Angélica González and Corchado, Juan M.},
	month = mar,
	year = {2022},
	keywords = {Dimensionality reduction, Nearest neighbor search, Neighborhood preservation, Random projection, Randomized algorithms},
	pages = {4927--4939},
}

@inproceedings{kim_bilinear_2015,
	title = {Bilinear {Random} {Projections} for {Locality}-{Sensitive} {Binary} {Codes}},
	url = {https://openaccess.thecvf.com/content_cvpr_2015/html/Kim_Bilinear_Random_Projections_2015_CVPR_paper.html},
	urldate = {2022-10-30},
	author = {Kim, Saehoon and Choi, Seungjin},
	year = {2015},
	pages = {1338--1346},
}

@misc{xie_survey_2018,
	title = {A survey of dimensionality reduction techniques based on random projection},
	url = {http://arxiv.org/abs/1706.04371},
	doi = {10.48550/arXiv.1706.04371},
	abstract = {Dimensionality reduction techniques play important roles in the analysis of big data. Traditional dimensionality reduction approaches, such as principal component analysis (PCA) and linear discriminant analysis (LDA), have been studied extensively in the past few decades. However, as the dimensionality of data increases, the computational cost of traditional dimensionality reduction methods grows exponentially, and the computation becomes prohibitively intractable. These drawbacks have triggered the development of random projection (RP) techniques, which map high-dimensional data onto a low-dimensional subspace with extremely reduced time cost. However, the RP transformation matrix is generated without considering the intrinsic structure of the original data and usually leads to relatively high distortion. Therefore, in recent years, methods based on RP have been proposed to address this problem. In this paper, we summarize the methods used in different situations to help practitioners to employ the proper techniques for their specific applications. Meanwhile, we enumerate the benefits and limitations of the various methods and provide further references for researchers to develop novel RP-based approaches.},
	urldate = {2022-10-30},
	publisher = {arXiv},
	author = {Xie, Haozhe and Li, Jie and Xue, Hanqing},
	month = may,
	year = {2018},
	note = {Number: arXiv:1706.04371
arXiv:1706.04371 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{tsagkatakis_random_2009,
	title = {Random {Projections} for face detection under resource constraints},
	doi = {10.1109/ICIP.2009.5413651},
	abstract = {Face detection is a key component in numerous computer vision applications. Most face detection algorithms achieve real-time performance by some form of dimensionality reduction of the input data, such as Principal Component Analysis. In this paper, we are exploring the emerging method of Random Projections (RP), a data independent linear projection method, for dimensionality reduction in the context of face detection. The benefits of using random projections include computational efficiency that can be obtained by implementing matrix multiplications with a small number of integer additions or subtractions. The computational savings are of great significance in resource constrained environments, such as wireless video sensor networks. Experimental results suggest that RP can achieve performance that is comparable to that obtained with traditional dimensionality reduction techniques for face detection using support vector machines.},
	booktitle = {2009 16th {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Tsagkatakis, Grigorios and Savakis, Andreas},
	month = jan,
	year = {2009},
	note = {ISSN: 2381-8549},
	keywords = {Computer vision, Dimensionality reduction, Discrete cosine transforms, Face detection, Face recognition, Image recognition, Object detection, Principal component analysis, Random projections, Support vector machine classification, Support vector machines, Wireless sensor networks},
	pages = {1233--1236},
}

@inproceedings{mccane_random_2014,
	address = {New York, NY, USA},
	series = {{IVCNZ} '14},
	title = {Random projections for feature matching},
	isbn = {978-1-4503-3184-5},
	url = {https://doi.org/10.1145/2683405.2683432},
	doi = {10.1145/2683405.2683432},
	abstract = {Several methods of random projection for feature matching are investigated. These include: random projections of established feature descriptors for dimensionality reduction; random projections of raw image data as feature descriptors; random projections of raw image data as keypoint detectors; and random projections as stereo keypoint detectors and descriptors. Results indicate that random projections as stereo keypoint detectors produce accurate correspondences, but are computationally costly; and that random projections of established descriptors are both efficient and accurate.},
	urldate = {2022-10-29},
	booktitle = {Proceedings of the 29th {International} {Conference} on {Image} and {Vision} {Computing} {New} {Zealand}},
	publisher = {Association for Computing Machinery},
	author = {McCane, Brendan},
	year = {2014},
	pages = {78--83},
}

@inproceedings{fradkin_experiments_2003,
	address = {New York, NY, USA},
	series = {{KDD} '03},
	title = {Experiments with random projections for machine learning},
	isbn = {978-1-58113-737-8},
	url = {https://doi.org/10.1145/956750.956812},
	doi = {10.1145/956750.956812},
	abstract = {Dimensionality reduction via Random Projections has attracted considerable attention in recent years. The approach has interesting theoretical underpinnings and offers computational advantages. In this paper we report a number of experiments to evaluate Random Projections in the context of inductive supervised learning. In particular, we compare Random Projections and PCA on a number of different datasets and using different machine learning methods. While we find that the random projection approach predictively underperforms PCA, its computational advantages may make it attractive for certain applications.},
	urldate = {2022-10-29},
	booktitle = {Proceedings of the ninth {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Fradkin, Dmitriy and Madigan, David},
	year = {2003},
	keywords = {dimensionality reduction, random projection},
	pages = {517--522},
}

@article{menon_fast_2016,
	title = {Fast {SVD} {With} {Random} {Hadamard} {Projection} for {Hyperspectral} {Dimensionality} {Reduction}},
	volume = {13},
	issn = {1558-0571},
	doi = {10.1109/LGRS.2016.2581172},
	abstract = {While data-dependent dimensionality reduction has dominated in many applications of hyperspectral imagery, there is increasing interest in data-independent strategies - such as random projections - due to their promise for reduced computational complexity as well as their demonstrated ability to preserve application-important information. Such random-projection-based dimensionality reduction is investigated in the specific context of supervised hyperspectral classification. Both Hadamard- and Gaussian-based random projections are considered, applied alone as well as incorporated into a fast approximate singular value decomposition (SVD). Experimental results reveal that the proposed Hadamard-based random projection with the fast SVD (FSVD) offers a computationally attractive alternative to not only traditional SVD but also Gaussian-based FSVD for dimensionality reduction in hyperspectral classification.},
	number = {9},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author = {Menon, Vineetha and Du, Qian and Fowler, James E.},
	month = sep,
	year = {2016},
	note = {Conference Name: IEEE Geoscience and Remote Sensing Letters},
	keywords = {Computational efficiency, Dimensionality reduction, Hyperspectral imaging, Prognostics and health management, Support vector machines, Testing, Training data, hyperspectral imagery, random projection, supervised classification},
	pages = {1275--1279},
}

@misc{nabil_random_2017,
	title = {Random {Projection} and {Its} {Applications}},
	url = {http://arxiv.org/abs/1710.03163},
	doi = {10.48550/arXiv.1710.03163},
	abstract = {Random Projection is a foundational research topic that connects a bunch of machine learning algorithms under a similar mathematical basis. It is used to reduce the dimensionality of the dataset by projecting the data points efficiently to a smaller dimensions while preserving the original relative distance between the data points. In this paper, we are intended to explain random projection method, by explaining its mathematical background and foundation, the applications that are currently adopting it, and an overview on its current research perspective.},
	urldate = {2022-10-30},
	publisher = {arXiv},
	author = {Nabil, Mahmoud},
	month = oct,
	year = {2017},
	note = {Number: arXiv:1710.03163
arXiv:1710.03163 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{yamazaki_mixed-precision_2015,
	title = {Mixed-{Precision} {Cholesky} {QR} {Factorization} and {Its} {Case} {Studies} on {Multicore} {CPU} with {Multiple} {GPUs}},
	volume = {37},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/10.1137/14M0973773},
	doi = {10.1137/14M0973773},
	abstract = {To orthonormalize the columns of a dense matrix, the Cholesky QR (CholQR) requires only one global reduction between the parallel processing units and performs most of its computation using BLAS-3 kernels. As a result, compared to other orthogonalization algorithms, CholQR obtains superior performance on many of the current computer architectures, where the communication is becoming increasingly expensive compared to the arithmetic operations. This is especially true when the input matrix is tall-skinny. Unfortunately, the orthogonality error of CholQR depends quadratically on the condition number of the input matrix, and it is numerically unstable when the matrix is ill-conditioned. To enhance the stability of CholQR, we recently used mixed-precision arithmetic; the input and output matrices are in the working precision, but some of its intermediate results are accumulated in the doubled precision. In this paper, we analyze the numerical properties of this mixed-precision CholQR. Our analysis shows that by selectively using the doubled precision, the orthogonality error of the mixed-precision CholQR only depends linearly on the condition number of the input matrix. We provide numerical results to demonstrate the improved numerical stability of the mixed-precision CholQR in practice. We then study its performance. When the target hardware does not support the desired higher precision, software emulation is needed. For example, using software-emulated double-double precision for the working 64-bit double precision, the mixed-precision CholQR requires about 
8.5×
8.5×
 more floating-point instructions than that required by the standard CholQR. On the other hand, the increase in the communication cost using the double-double precision is less significant, and our performance results on multicore CPU with a different graphics processing unit (GPU) demonstrate that the overhead of using the double-double arithmetic is decreasing on a newer architecture, where the computation is becoming less expensive compared to the communication. As a result, with a latest NVIDIA GPU, the mixed-precision CholQR was only 
1.4×
1.4×
 slower than the standard CholQR. Finally, we present case studies of using the mixed-precision CholQR within communication-avoiding variants of Krylov subspace projection methods for solving a nonsymmetric linear system of equations and for solving a symmetric eigenvalue problem, on a multicore CPU with multiple GPUs. These case studies demonstrate that by using the higher precision for this small but critical segment of the Krylov methods, we can improve not only the overall numerical stability of the solvers but also, in some cases, their performance.},
	number = {3},
	urldate = {2022-10-13},
	journal = {SIAM Journal on Scientific Computing},
	author = {Yamazaki, Ichitaro and Tomov, Stanimire and Dongarra, Jack},
	month = jan,
	year = {2015},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {65F25, GPU computation, mixed-precision, orthogonalization},
	pages = {C307--C330},
}

@misc{developers_cirq_2022,
	title = {Cirq},
	url = {https://doi.org/10.5281/zenodo.6599601},
	publisher = {Zenodo},
	author = {Developers, Cirq},
	month = apr,
	year = {2022},
	doi = {10.5281/zenodo.6599601},
}

@misc{treinish_qiskitqiskit_2022,
	title = {Qiskit/qiskit: {Qiskit} 0.38.0},
	copyright = {Open Access},
	shorttitle = {Qiskit/qiskit},
	url = {https://zenodo.org/record/2573505},
	abstract = {Changed {\textless}code{\textgreater}qiskit-aer{\textless}/code{\textgreater} updated to its latest release, 0.11.0.},
	urldate = {2022-09-24},
	publisher = {Zenodo},
	author = {Treinish, Matthew and Gambetta, Jay and Nation, Paul and Qiskit-Bot and Kassebaum, Paul and Rodríguez, Diego M. and De La Puente González, Salvador and Shaohan Hu and Krsulich, Kevin and Lishman, Jake and Garrison, Jim and Zdanski, Laura and Yu, Jessie and Bello, Luciano and Marques, Manoel and Gacon, Julien and McKay, David and Gomez, Juan and Capelluto, Lauren and Travis-S-IBM and Panigrahi, Ashish and Lerongil and Rafey Iqbal Rahman and Wood, Steve and Toshinari Itoko and Wood, Christopher J. and Divyanshu Singh and Drew and Arbel, Eli and Glen},
	month = sep,
	year = {2022},
	doi = {10.5281/ZENODO.2573505},
}

@article{suzuki_qulacs_2021,
	title = {Qulacs: a fast and versatile quantum circuit simulator for research purpose},
	volume = {5},
	shorttitle = {Qulacs},
	url = {https://quantum-journal.org/papers/q-2021-10-06-559/},
	doi = {10.22331/q-2021-10-06-559},
	abstract = {Yasunari Suzuki, Yoshiaki Kawase, Yuya Masumura, Yuria Hiraga, Masahiro Nakadai, Jiabao Chen, Ken M. Nakanishi, Kosuke Mitarai, Ryosuke Imai, Shiro Tamiya, Takahiro Yamamoto, Tennin Yan, Toru Kawakubo, Yuya O. Nakagawa, Yohei Ibe, Youyuan Zhang, Hirotsugu Yamashita, Hikaru Yoshimura, Akihiro Hayashi, and Keisuke Fujii,
Quantum 5, 559 (2021).
To explore the possibilities of a near-term intermediate-scale quantum algorithm and long-term fault-tolerant quantum computing, a fast and versatile quantum circuit simulator is needed. Her…},
	language = {en-GB},
	urldate = {2022-09-24},
	journal = {Quantum},
	author = {Suzuki, Yasunari and Kawase, Yoshiaki and Masumura, Yuya and Hiraga, Yuria and Nakadai, Masahiro and Chen, Jiabao and Nakanishi, Ken M. and Mitarai, Kosuke and Imai, Ryosuke and Tamiya, Shiro and Yamamoto, Takahiro and Yan, Tennin and Kawakubo, Toru and Nakagawa, Yuya O. and Ibe, Yohei and Zhang, Youyuan and Yamashita, Hirotsugu and Yoshimura, Hikaru and Hayashi, Akihiro and Fujii, Keisuke},
	month = oct,
	year = {2021},
	note = {Publisher: Verein zur Förderung des Open Access Publizierens in den Quantenwissenschaften},
	pages = {559},
}

@article{jones_quest_2019,
	title = {{QuEST} and {High} {Performance} {Simulation} of {Quantum} {Computers}},
	volume = {9},
	copyright = {2019 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-47174-9},
	doi = {10.1038/s41598-019-47174-9},
	abstract = {We introduce QuEST, the Quantum Exact Simulation Toolkit, and compare it to ProjectQ, qHipster and a recent distributed implementation of Quantum++. QuEST is the first open source, hybrid multithreaded and distributed, GPU accelerated simulator of universal quantum circuits. Embodied as a C library, it is designed so that a user’s code can be deployed seamlessly to any platform from a laptop to a supercomputer. QuEST is capable of simulating generic quantum circuits of general one and two-qubit gates and multi-qubit controlled gates, on pure and mixed states, represented as state-vectors and density matrices, and under the presence of decoherence. Using the ARCUS and ARCHER supercomputers, we benchmark QuEST’s simulation of random circuits of up to 38 qubits, distributed over up to 2048 compute nodes, each with up to 24 cores. We directly compare QuEST’s performance to ProjectQ’s on single machines, and discuss the differences in distribution strategies of QuEST, qHipster and Quantum++. QuEST shows excellent scaling, both strong and weak, on multicore and distributed architectures.},
	language = {en},
	number = {1},
	urldate = {2022-09-24},
	journal = {Scientific Reports},
	author = {Jones, Tyson and Brown, Anna and Bush, Ian and Benjamin, Simon C.},
	month = jul,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Quantum simulation, Software},
	pages = {10736},
}

@article{arute_quantum_2019,
	title = {Quantum supremacy using a programmable superconducting processor},
	volume = {574},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1666-5},
	doi = {10.1038/s41586-019-1666-5},
	abstract = {The promise of quantum computers is that certain computational tasks might be executed exponentially faster on a quantum processor than on a classical processor1. A fundamental challenge is to build a high-fidelity processor capable of running quantum algorithms in an exponentially large computational space. Here we report the use of a processor with programmable superconducting qubits2–7 to create quantum states on 53 qubits, corresponding to a computational state-space of dimension 253 (about 1016). Measurements from repeated experiments sample the resulting probability distribution, which we verify using classical simulations. Our Sycamore processor takes about 200 seconds to sample one instance of a quantum circuit a million times—our benchmarks currently indicate that the equivalent task for a state-of-the-art classical supercomputer would take approximately 10,000 years. This dramatic increase in speed compared to all known classical algorithms is an experimental realization of quantum supremacy8–14 for this specific computational task, heralding a much-anticipated computing paradigm.},
	language = {en},
	number = {7779},
	urldate = {2022-09-24},
	journal = {Nature},
	author = {Arute, Frank and Arya, Kunal and Babbush, Ryan and Bacon, Dave and Bardin, Joseph C. and Barends, Rami and Biswas, Rupak and Boixo, Sergio and Brandao, Fernando G. S. L. and Buell, David A. and Burkett, Brian and Chen, Yu and Chen, Zijun and Chiaro, Ben and Collins, Roberto and Courtney, William and Dunsworth, Andrew and Farhi, Edward and Foxen, Brooks and Fowler, Austin and Gidney, Craig and Giustina, Marissa and Graff, Rob and Guerin, Keith and Habegger, Steve and Harrigan, Matthew P. and Hartmann, Michael J. and Ho, Alan and Hoffmann, Markus and Huang, Trent and Humble, Travis S. and Isakov, Sergei V. and Jeffrey, Evan and Jiang, Zhang and Kafri, Dvir and Kechedzhi, Kostyantyn and Kelly, Julian and Klimov, Paul V. and Knysh, Sergey and Korotkov, Alexander and Kostritsa, Fedor and Landhuis, David and Lindmark, Mike and Lucero, Erik and Lyakh, Dmitry and Mandrà, Salvatore and McClean, Jarrod R. and McEwen, Matthew and Megrant, Anthony and Mi, Xiao and Michielsen, Kristel and Mohseni, Masoud and Mutus, Josh and Naaman, Ofer and Neeley, Matthew and Neill, Charles and Niu, Murphy Yuezhen and Ostby, Eric and Petukhov, Andre and Platt, John C. and Quintana, Chris and Rieffel, Eleanor G. and Roushan, Pedram and Rubin, Nicholas C. and Sank, Daniel and Satzinger, Kevin J. and Smelyanskiy, Vadim and Sung, Kevin J. and Trevithick, Matthew D. and Vainsencher, Amit and Villalonga, Benjamin and White, Theodore and Yao, Z. Jamie and Yeh, Ping and Zalcman, Adam and Neven, Hartmut and Martinis, John M.},
	month = oct,
	year = {2019},
	note = {Number: 7779
Publisher: Nature Publishing Group},
	keywords = {Quantum information, Quantum physics},
	pages = {505--510},
}

@article{gray_quimb_2018,
	title = {quimb: {A} python package for quantum information and many-body calculations},
	volume = {3},
	issn = {2475-9066},
	shorttitle = {quimb},
	url = {https://joss.theoj.org/papers/10.21105/joss.00819},
	doi = {10.21105/joss.00819},
	abstract = {Gray, (2018). quimb: A python package for quantum information and many-body calculations. Journal of Open Source Software, 3(29), 819, https://doi.org/10.21105/joss.00819},
	language = {en},
	number = {29},
	urldate = {2022-09-19},
	journal = {Journal of Open Source Software},
	author = {Gray, Johnnie},
	month = sep,
	year = {2018},
	pages = {819},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	volume = {32},
	shorttitle = {{PyTorch}},
	url = {https://papers.nips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.
In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.
We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
	urldate = {2022-09-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
}

@inproceedings{okuta_cupy_2017,
	title = {{CuPy}: {A} {NumPy}-{Compatible} {Library} for {NVIDIA} {GPU} {Calculations}},
	author = {Okuta, Ryosuke and Unno, Yuya and Nishino, Daisuke and Hido, Shohei and Loomis, Crissman},
	year = {2017},
}

@article{strassen_gaussian_1969,
	title = {Gaussian elimination is not optimal},
	volume = {13},
	issn = {0945-3245},
	url = {https://doi.org/10.1007/BF02165411},
	doi = {10.1007/BF02165411},
	language = {en},
	number = {4},
	urldate = {2022-09-17},
	journal = {Numerische Mathematik},
	author = {Strassen, Volker},
	month = aug,
	year = {1969},
	keywords = {Gaussian Elimination, Mathematical Method},
	pages = {354--356},
}

@article{finkelstein_quantum_2022,
	title = {Quantum {Perturbation} {Theory} {Using} {Tensor} {Cores} and a {Deep} {Neural} {Network}},
	volume = {18},
	issn = {1549-9618},
	url = {https://doi.org/10.1021/acs.jctc.2c00274},
	doi = {10.1021/acs.jctc.2c00274},
	abstract = {Time-independent quantum response calculations are performed using Tensor cores. This is achieved by mapping density matrix perturbation theory onto the computational structure of a deep neural network. The main computational cost of each deep layer is dominated by tensor contractions, i.e., dense matrix–matrix multiplications, in mixed-precision arithmetics, which achieves close to peak performance. Quantum response calculations are demonstrated and analyzed using self-consistent charge density-functional tight-binding theory as well as coupled-perturbed Hartree–Fock theory. For linear response calculations, a novel parameter-free convergence criterion is presented that is well-suited for numerically noisy low-precision floating point operations and we demonstrate a peak performance of almost 200 Tflops using the Tensor cores of two Nvidia A100 GPUs.},
	number = {7},
	urldate = {2022-09-15},
	journal = {Journal of Chemical Theory and Computation},
	author = {Finkelstein, Joshua and Rubensson, Emanuel H. and Mniszewski, Susan M. and Negre, Christian F. A. and Niklasson, Anders M. N.},
	month = jul,
	year = {2022},
	note = {Publisher: American Chemical Society},
	pages = {4255--4268},
}

@article{lyakh_efficient_2015,
	title = {An efficient tensor transpose algorithm for multicore {CPU}, {Intel} {Xeon} {Phi}, and {NVidia} {Tesla} {GPU}},
	volume = {189},
	issn = {0010-4655},
	url = {https://www.sciencedirect.com/science/article/pii/S0010465514004330},
	doi = {10.1016/j.cpc.2014.12.013},
	abstract = {An efficient parallel tensor transpose algorithm is suggested for shared-memory computing units, namely, multicore CPU, Intel Xeon Phi, and NVidia GPU. The algorithm operates on dense tensors (multidimensional arrays) and is based on the optimization of cache utilization on x86 CPU and the use of shared memory on NVidia GPU. From the applied side, the ultimate goal is to minimize the overhead encountered in the transformation of tensor contractions into matrix multiplications in computer implementations of advanced methods of quantum many-body theory (e.g., in electronic structure theory and nuclear physics). A particular accent is made on higher-dimensional tensors that typically appear in the so-called multireference correlated methods of electronic structure theory. Depending on tensor dimensionality, the presented optimized algorithms can achieve an order of magnitude speedup on x86 CPUs and 2–3 times speedup on NVidia Tesla K20X GPU with respect to the naïve scattering algorithm (no memory access optimization). The tensor transpose routines developed in this work have been incorporated into a general-purpose tensor algebra library (TAL-SH).},
	language = {en},
	urldate = {2022-08-19},
	journal = {Computer Physics Communications},
	author = {Lyakh, Dmitry I.},
	month = apr,
	year = {2015},
	keywords = {Array reordering, Electronic structure, Intel Xeon Phi, Many-body theory, Multireference, NVidia GPU, Tensor contraction, Tensor transpose},
	pages = {84--91},
}

@inproceedings{mutlu_hpc_2018,
	title = {{HPC} {Software} {Verification} in {Action}: {A} {Case} {Study} with {Tensor} {Transposition}},
	shorttitle = {{HPC} {Software} {Verification} in {Action}},
	doi = {10.1109/Correctness.2018.00006},
	abstract = {As HPC platforms get increasingly complex, the complexity of software optimized for these platforms has also increased. There is a pressing need to ensure correctness of scientific applications to enhance our confidence in the results they produce. In this paper, we focus on checking the functional equivalence of libraries providing a small but important functionality—tensor transposition—used in computational chemistry applications. While several correctness tools have been developed and deployed, there are several practical challenges in using them to check correctness of production HPC software. We present our experiences using two tools—CIVL and CodeThorn—in checking the functional equivalence of two index permutation libraries. We observe that, with some effort, the tools we evaluated can handle kernels from production codes. We present observations that will aid library writers to write code that can be checked with these tools.},
	booktitle = {2018 {IEEE}/{ACM} 2nd {International} {Workshop} on {Software} {Correctness} for {HPC} {Applications} ({Correctness})},
	author = {Mutlu, Erdal and Panyala, Ajay and Krishnamoorthy, Sriram},
	month = nov,
	year = {2018},
	keywords = {CIVL, CodeThorn, Indexes, Kernel, Libraries, Production, Tools, equivalence-checking, index-transposition, model-checking},
	pages = {9--16},
}

@inproceedings{vedurada_ttlg_2018,
	title = {{TTLG} - {An} {Efficient} {Tensor} {Transposition} {Library} for {GPUs}},
	doi = {10.1109/IPDPS.2018.00067},
	abstract = {This paper presents a Tensor Transposition Library for GPUs (TTLG). A distinguishing feature of TTLG is that it also includes a performance prediction model, which can be used by higher level optimizers that use tensor transposition. For example, tensor contractions are often implemented by using the TTGT (Transpose-Transpose-GEMM-Transpose) approach - transpose input tensors to a suitable layout and then use high-performance matrix multiplication followed by transposition of the result. The performance model is also used internally by TTLG for choosing among alternative kernels and/or slicing/blocking parameters for the transposition. TTLG is compared with current state-of-the-art alternatives for GPUs. Comparable or better transposition times for the "repeated-use" scenario and considerably better "single-use" performance are observed.},
	booktitle = {2018 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Vedurada, Jyothi and Suresh, Arjun and Rajam, Aravind Sukumaran and Kim, Jinsung and Hong, Changwan and Panyala, Ajay and Krishnamoorthy, Sriram and Nandivada, V. Krishna and Srivastava, Rohit Kumar and Sadayappan, P.},
	month = may,
	year = {2018},
	note = {ISSN: 1530-2075},
	keywords = {GPU, Graphics processing units, High performance, Indexes, Instruction sets, Libraries, Taxonomy, Tensile stress, Tensor Transpose, Two dimensional displays},
	pages = {578--588},
}

@article{stewart_efficient_1980,
	title = {The {Efficient} {Generation} of {Random} {Orthogonal} {Matrices} with an {Application} to {Condition} {Estimators}},
	copyright = {Copyright © 1980 Society for Industrial and Applied Mathematics},
	url = {https://epubs.siam.org/doi/10.1137/0717034},
	doi = {10.1137/0717034},
	abstract = {This paper presents a method for generating pseudo-random orthogonal matrices from the Haar distribution for the group of orthogonal matrices. The random matrices are expressed as products of \$n - 1\$ Householder transformations, which can be computed in \$O(n{\textasciicircum}2 )\$ time. The technique is used in an empirical study of two methods for estimating the condition number of a matrix.},
	language = {en},
	urldate = {2022-08-17},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Stewart, G. W.},
	month = jul,
	year = {1980},
	note = {Publisher: Society for Industrial and Applied Mathematics},
}

@misc{simhadri_results_2022,
	title = {Results of the {NeurIPS}'21 {Challenge} on {Billion}-{Scale} {Approximate} {Nearest} {Neighbor} {Search}},
	url = {http://arxiv.org/abs/2205.03763},
	doi = {10.48550/arXiv.2205.03763},
	abstract = {Despite the broad range of algorithms for Approximate Nearest Neighbor Search, most empirical evaluations of algorithms have focused on smaller datasets, typically of 1 million points{\textasciitilde}{\textbackslash}citep\{Benchmark\}. However, deploying recent advances in embedding based techniques for search, recommendation and ranking at scale require ANNS indices at billion, trillion or larger scale. Barring a few recent papers, there is limited consensus on which algorithms are effective at this scale vis-{\textbackslash}`a-vis their hardware cost. This competition compares ANNS algorithms at billion-scale by hardware cost, accuracy and performance. We set up an open source evaluation framework and leaderboards for both standardized and specialized hardware. The competition involves three tracks. The standard hardware track T1 evaluates algorithms on an Azure VM with limited DRAM, often the bottleneck in serving billion-scale indices, where the embedding data can be hundreds of GigaBytes in size. It uses FAISS{\textasciitilde}{\textbackslash}citep\{Faiss17\} as the baseline. The standard hardware track T2 additional allows inexpensive SSDs in addition to the limited DRAM and uses DiskANN{\textasciitilde}{\textbackslash}citep\{DiskANN19\} as the baseline. The specialized hardware track T3 allows any hardware configuration, and again uses FAISS as the baseline. We compiled six diverse billion-scale datasets, four newly released for this competition, that span a variety of modalities, data types, dimensions, deep learning models, distance functions and sources. The outcome of the competition was ranked leaderboards of algorithms in each track based on recall at a query throughput threshold. Additionally, for track T3, separate leaderboards were created based on recall as well as cost-normalized and power-normalized query throughput.},
	urldate = {2022-08-08},
	publisher = {arXiv},
	author = {Simhadri, Harsha Vardhan and Williams, George and Aumüller, Martin and Douze, Matthijs and Babenko, Artem and Baranchuk, Dmitry and Chen, Qi and Hosseini, Lucas and Krishnaswamy, Ravishankar and Srinivasa, Gopal and Subramanya, Suhas Jayaram and Wang, Jingdong},
	month = may,
	year = {2022},
	note = {Number: arXiv:2205.03763
arXiv:2205.03763 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Databases, Computer Science - Machine Learning, Computer Science - Performance},
}

@article{jegou_product_2011,
	title = {Product {Quantization} for {Nearest} {Neighbor} {Search}},
	volume = {33},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2010.57},
	abstract = {This paper introduces a product quantization-based approach for approximate nearest neighbor search. The idea is to decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for SIFT and GIST image descriptors show excellent search accuracy, outperforming three state-of-the-art approaches. The scalability of our approach is validated on a data set of two billion vectors.},
	number = {1},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Jégou, Herve and Douze, Matthijs and Schmid, Cordelia},
	month = jan,
	year = {2011},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Electronic mail, Euclidean distance, File systems, High-dimensional indexing, Image databases, Indexing, Nearest neighbor searches, Neural networks, Permission, Quantization, Scalability, approximate search., image indexing, very large databases},
	pages = {117--128},
}

@misc{connolly_randomized_2022,
	type = {{MIMS} {Preprint}},
	title = {Randomized {Low} {Rank} {Matrix} {Approximation}: {Rounding} {Error} {Analysis} and a {Mixed} {Precision} {Algorithm}},
	shorttitle = {Randomized {Low} {Rank} {Matrix} {Approximation}},
	url = {http://eprints.maths.manchester.ac.uk/2863/},
	abstract = {The available error bounds for randomized algorithms for computing a low rank approximation to a matrix assume exact arithmetic. Rounding errors potentially dominate the approximation error, though, especially when the algorithms are run in low precision arithmetic. We give a rounding error analysis of the method that computes a randomized rangefinder and then computes an approximate singular value decomposition approximation. Our analysis covers the basic method and the power iteration for the fixed-rank problem, as well as the power iteration for the fixed-precision problem. We give both worst-case and probabilistic rounding error bounds as functions of the problem dimensions and the rank. The worst-case bounds are pessimistic, but the probabilistic bounds are reasonably tight and still reliably bound the error in practice. We also propose a mixed precision version of the algorithm that offers potential speedups by gradually decreasing the precision during the execution of the algorithm.},
	language = {en},
	urldate = {2022-07-21},
	author = {Connolly, Michael P. and Higham, Nicholas J. and Pranesh, Srikara},
	month = jul,
	year = {2022},
	note = {Issue: 2022.10
Number: 2022.10},
}

@misc{corporation_nvidia_2020,
	title = {{NVIDIA} {A100} {TENSOR} {CORE} {GPU}},
	url = {https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-nvidia-us-2188504-web.pdf},
	abstract = {The fastest data center platform for AI and HPC.},
	language = {en-us},
	urldate = {2021-06-06},
	journal = {NVIDIA},
	author = {Corporation, NVIDIA},
	year = {2020},
}

@misc{corporation_nvidia_2022,
	title = {{NVIDIA} {H100} {TENSOR} {CORE} {GPU}},
	url = {https://resources.nvidia.com/en-us-tensor-core/nvidia-h100-datasheet},
	author = {Corporation, NVIDIA},
	year = {2022},
}

@unpublished{amestoy_five-precision_2021,
	title = {Five-{Precision} {GMRES}-based iterative refinement},
	url = {https://hal.archives-ouvertes.fr/hal-03190686},
	abstract = {GMRES-based iterative refinement in three precisions (GMRES-IR3) uses a low precision LU factorization to accelerate the solution of a linear system without compromising numerical stability or robustness. GMRES-IR3 solves the update equation using GMRES preconditioned by the LU factors, where all operations within GMRES are carried out in the working precision u, except for the matrix-vector products and the application of the preconditioner, which require the use of extra precision u 2. The use of extra precision can be expensive, and is especially unattractive if it is not available in hardware; for this reason, existing implementations have not used extra precision, despite the absence of an error analysis for this approach. We relax the requirements on the precisions used within GMRES, allowing the use of arbitrary precisions up (for applying the preconditioner) and ug (for the rest of the operations). We obtain the five-precision GMRES-based iterative refinement (GMRES-IR5) algorithm. We carry out a rounding error analysis that generalizes that of GMRES-IR3, obtaining conditions under which the forward and backward errors converge to their limiting values. Our analysis makes use of a new result on the backward stability of MGS-GMRES in two precisions. On hardware where up to five arithmetics are available, the number of possible combinations of precisions in GMRES-IR5 is extremely large, but our analysis identifies a small subset of relevant combinations. By choosing from within this subset one can achieve different levels of tradeoff between cost and robustness, which allows for a finer choice of precisions depending on the problem difficulty and the available hardware. Our numerical experiments on both random dense matrices and real-life sparse matrices from a wide range of applications show that the practical behavior of GMRES-IR5 is in good agreement with our theoretical analysis. GMRES-IR5 therefore has the potential to solve relatively badly conditioned problems in less time and memory than GMRES-IR3, thanks to the use of lower precision arithmetic in the GMRES iterations.},
	urldate = {2022-07-04},
	author = {Amestoy, Patrick and Buttari, Alfredo and Higham, Nicholas and L'Excellent, Jean-yves and Mary, Théo and Vieuble, Bastien},
	month = apr,
	year = {2021},
	keywords = {GMRES, backward error, floating-point arithmetic, forward error, iterative refinement, linear system, mixed precision, multiple precision, preconditioning, rounding error analysis},
}

@article{carson_three-precision_2020,
	title = {Three-{Precision} {GMRES}-{Based} {Iterative} {Refinement} for {Least} {Squares} {Problems}},
	volume = {42},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/20M1316822},
	doi = {10.1137/20M1316822},
	abstract = {The standard iterative refinement procedure for improving an approximate solution to the least squares problem \${\textbackslash}min\_x{\textbackslash}{\textbar}b - Ax{\textbackslash}{\textbar}\_2\$, where \$A{\textbackslash}in{\textbackslash}mathbb\{R\}{\textasciicircum}\{m{\textbackslash}times n\}\$ with \$m {\textbackslash}ge n\$ has full rank, is based on solving the \$(m+n){\textbackslash}times (m+n)\$ augmented system with the aid of a QR factorization. In order to exploit multiprecision arithmetic, iterative refinement can be formulated to use three precisions, but the resulting algorithm converges only for a limited range of problems. We build an iterative refinement algorithm called GMRES-LSIR, analogous to the GMRES-IR algorithm developed for linear systems [E. Carson and N. J. Higham, SIAM J. Sci. Comput., 40 (2018), pp. A817--A847], that solves the augmented system using GMRES preconditioned by a matrix based on the computed QR factors. We explore two left preconditioners; the first has full off-diagonal blocks, and the second is block diagonal and can be applied in either left-sided or split form. We prove that for a wide range of problems the first preconditioner yields backward and forward errors for the augmented system of order the working precision under suitable assumptions on the precisions and the problem conditioning. Our proof does not extend to the block diagonal preconditioner, but our numerical experiments show that with this preconditioner the algorithm performs about as well in practice. The experiments also show that if we use MINRES in place of GMRES then the convergence is similar for sufficiently well conditioned problems but worse for the most ill conditioned ones.},
	number = {6},
	urldate = {2022-07-04},
	journal = {SIAM Journal on Scientific Computing},
	author = {Carson, Erin and Higham, Nicholas J. and Pranesh, Srikara},
	month = jan,
	year = {2020},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {65F05, 65F08, 65F20, GMRES, MINRES, half precision arithmetic, iterative refinement, least squares, mixed precision, preconditioning},
	pages = {A4063--A4083},
}

@misc{oktay_mixed_2022,
	title = {Mixed {Precision} {GMRES}-based {Iterative} {Refinement} with {Recycling}},
	url = {http://arxiv.org/abs/2201.09827},
	doi = {10.48550/arXiv.2201.09827},
	abstract = {With the emergence of mixed precision capabilities in hardware, iterative refinement schemes for solving linear systems \$Ax=b\$ have recently been revisited and reanalyzed in the context of three or more precisions. These new analyses show that under certain constraints on condition number, the LU factorization of the matrix can be computed in low precision without affecting the final accuracy. Another promising technique is GMRES-based iterative refinement, which, in contrast to the standard approach, use GMRES preconditioned by the low-precision triangular factors to solve for the approximate solution update in each refinement step. This more accurate solution method extends the range of problems which can be solved with a given combination of precisions. However, in certain settings, GMRES may require too many iterations per refinement step, making it potentially more expensive than simply recomputing the LU factors in a higher precision. Krylov subspace recycling is a well-known technique for reusing information across sequential invocations of a Krylov subspace method on systems with the same or a slowly changing coefficient matrix. In this work, we incorporate the idea of Krylov subspace recycling into a mixed precision GMRES-based iterative refinement solver. The insight is that in each refinement step, we call preconditioned GMRES on a linear system with the same coefficient matrix \$A\$, with only the right-hand side changing. In this way, the GMRES solves in subsequent refinement steps can be accelerated by recycling information obtained from the first step. We perform extensive numerical experiments on various random dense problems, Toeplitz problems (prolate matrices), and problems from real applications, which confirm the benefits of the recycling approach.},
	urldate = {2022-07-04},
	publisher = {arXiv},
	author = {Oktay, Eda and Carson, Erin},
	month = feb,
	year = {2022},
	note = {Number: arXiv:2201.09827
arXiv:2201.09827 [cs, math]},
	keywords = {65F08, 65F10, 65G50, 65Y10, G.1.3, G.4, Mathematics - Numerical Analysis},
}

@article{oktay_multistage_2022,
	title = {Multistage mixed precision iterative refinement},
	volume = {n/a},
	issn = {1099-1506},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nla.2434},
	doi = {10.1002/nla.2434},
	abstract = {Low precision arithmetic, in particular half precision (16-bit) floating point arithmetic, is now available in commercial hardware. Using lower precision can offer significant savings in computation and communication costs with proportional savings in energy. Motivated by this, there has been a renewed interest in mixed precision iterative refinement schemes for solving linear systems Ax=b, and new variants of GMRES-based iterative refinement have been developed. Each particular variant with a given combination of precisions leads to different condition number-based constraints for convergence of the backward and forward errors, and each has different performance costs. The constraints for convergence given in the literature are, as an artifact of the analyses, often overly strict in practice, and thus could lead a user to select a more expensive variant when a less expensive one would have sufficed. In this work, we develop a multistage mixed precision iterative refinement solver which aims to combine existing mixed precision approaches to balance performance and accuracy and improve usability. For a user-specified initial combination of precisions, the algorithm begins with the least expensive approach and convergence is monitored via inexpensive computations with quantities produced during the iteration. If slow convergence or divergence is detected using particular stopping criteria, the algorithm switches to use a more expensive, but more reliable variant. A novel aspect of our approach is that, unlike existing implementations, our algorithm first attempts to use “stronger” GMRES-based solvers for the solution update before resorting to increasing the precision(s). In some scenarios, this can avoid the need to refactorize the matrix in higher precision. We perform extensive numerical experiments on a variety of random dense problems and problems from real applications which confirm the benefits of the multistage approach.},
	language = {en},
	number = {n/a},
	urldate = {2022-07-04},
	journal = {Numerical Linear Algebra with Applications},
	author = {Oktay, Eda and Carson, Erin},
	month = feb,
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nla.2434},
	keywords = {GMRES, adaptive algorithms, iterative refinement, mixed precision},
	pages = {e2434},
}

@article{sandamirskaya_neuromorphic_2022,
	title = {Neuromorphic computing hardware and neural architectures for robotics},
	volume = {7},
	url = {https://www.science.org/doi/10.1126/scirobotics.abl8419},
	doi = {10.1126/scirobotics.abl8419},
	number = {67},
	urldate = {2022-07-04},
	journal = {Science Robotics},
	author = {Sandamirskaya, Yulia and Kaboli, Mohsen and Conradt, Jorg and Celikel, Tansu},
	month = jun,
	year = {2022},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eabl8419},
}

@article{abdelaziz_rethinking_2021,
	title = {Rethinking {Floating} {Point} {Overheads} for {Mixed} {Precision} {DNN} {Accelerators}},
	volume = {3},
	url = {https://proceedings.mlsys.org/paper/2021/hash/5f93f983524def3dca464469d2cf9f3e-Abstract.html},
	language = {en},
	urldate = {2022-07-04},
	journal = {Proceedings of Machine Learning and Systems},
	author = {Abdelaziz, Hamzah and Shafiee, Ali and Shin, Jong Hoon and Pedram, Ardavan and Hassoun, Joseph},
	month = mar,
	year = {2021},
	pages = {223--239},
}

@inproceedings{kotipalli_ampt-ga_2019,
	address = {New York, NY, USA},
	series = {{ICS} '19},
	title = {{AMPT}-{GA}: automatic mixed precision floating point tuning for {GPU} applications},
	isbn = {978-1-4503-6079-1},
	shorttitle = {{AMPT}-{GA}},
	url = {https://doi.org/10.1145/3330345.3330360},
	doi = {10.1145/3330345.3330360},
	abstract = {Mixed precision computations improve high performance computing throughput for applications that can tolerate decreased mathematical precision in their computations. Native mixed precision computation is commonplace in today's GPGPU accelerators where it is applied to applications with well-known tolerances for reduced mathematical precision. Applications with stricter accuracy needs lack support for selecting precisions that both improve performance and satisfy these accuracy requirements. Prior works have focused primarily on accuracy, leaving performance concerns such as the overhead of casting unanswered in GPGPU contexts. In this paper, we present a system called AMPT-GA that selects application-level data precisions to maximize performance while satisfying accuracy constraints. We combine static analysis for casting-aware performance modeling with dynamic analysis for modeling and enforcing precision constraints. We further improve our optimizations with application-aware mutations in our genetic algorithm-based search function. AMPT-GA improves the performance efficiency of our target applications more than the prior state-of-the-art approach called Precimonious. AMPT-GA outperforms Precimonious in efficiency by 14--63\%.},
	urldate = {2022-07-03},
	booktitle = {Proceedings of the {ACM} {International} {Conference} on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Kotipalli, Pradeep V and Singh, Ranvijay and Wood, Paul and Laguna, Ignacio and Bagchi, Saurabh},
	year = {2019},
	pages = {160--170},
}

@article{schneck_impact_2021,
	title = {Impact of mixed precision and storage layout on additive {Schwarz} smoothers},
	volume = {28},
	issn = {1099-1506},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nla.2366},
	doi = {10.1002/nla.2366},
	abstract = {The growing discrepancy between CPU computing power and memory bandwidth drives more and more numerical algorithms into a bandwidth-bound regime. One example is the overlapping Schwarz smoother, a highly effective building block for iterative multigrid solution of elliptic equations with higher order finite elements. Two options of reducing the required memory bandwidth are sparsity exploiting storage layouts and representing matrix entries with reduced precision in floating point or fixed point format. We investigate the impact of several options on storage demand and contraction rate, both analytically in the context of subspace correction methods and numerically at an example of solid mechanics. Both perspectives agree on the favourite scheme: fixed point representation of Cholesky factors in nested dissection storage.},
	language = {en},
	number = {4},
	urldate = {2022-07-04},
	journal = {Numerical Linear Algebra with Applications},
	author = {Schneck, Jakob and Weiser, Martin and Wende, Florian},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nla.2366},
	keywords = {higher-order finite elements, mixed precision, overlapping Schwarz smoother},
	pages = {e2366},
}

@article{le_gallo_mixed-precision_2018,
	title = {Mixed-precision in-memory computing},
	volume = {1},
	copyright = {2018 The Author(s)},
	issn = {2520-1131},
	url = {https://www.nature.com/articles/s41928-018-0054-8},
	doi = {10.1038/s41928-018-0054-8},
	abstract = {As complementary metal–oxide–semiconductor (CMOS) scaling reaches its technological limits, a radical departure from traditional von Neumann systems, which involve separate processing and memory units, is needed in order to extend the performance of today’s computers substantially. In-memory computing is a promising approach in which nanoscale resistive memory devices, organized in a computational memory unit, are used for both processing and memory. However, to reach the numerical accuracy typically required for data analytics and scientific computing, limitations arising from device variability and non-ideal device characteristics need to be addressed. Here we introduce the concept of mixed-precision in-memory computing, which combines a von Neumann machine with a computational memory unit. In this hybrid system, the computational memory unit performs the bulk of a computational task, while the von Neumann machine implements a backward method to iteratively improve the accuracy of the solution. The system therefore benefits from both the high precision of digital computing and the energy/areal efficiency of in-memory computing. We experimentally demonstrate the efficacy of the approach by accurately solving systems of linear equations, in particular, a system of 5,000 equations using 998,752 phase-change memory devices.},
	language = {en},
	number = {4},
	urldate = {2022-07-04},
	journal = {Nature Electronics},
	author = {Le Gallo, Manuel and Sebastian, Abu and Mathis, Roland and Manica, Matteo and Giefers, Heiner and Tuma, Tomas and Bekas, Costas and Curioni, Alessandro and Eleftheriou, Evangelos},
	month = apr,
	year = {2018},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {Computational science, Electronic devices},
	pages = {246--253},
}

@inproceedings{martel_floating-point_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Floating-{Point} {Format} {Inference} in {Mixed}-{Precision}},
	isbn = {978-3-319-57288-8},
	doi = {10.1007/978-3-319-57288-8_16},
	abstract = {We address the problem of determining the minimal precision on the inputs and on the intermediary results of a program containing floating-point computations in order to ensure a desired accuracy on the outputs. The first originality of our approach is to combine forward and backward static analyses, done by abstract interpretation. The backward analysis computes the minimal precision needed for the inputs and intermediary values in order to have a desired accuracy on the results, specified by the user. The second originality is to express our analysis as a set of constraints made of first order predicates and affine integer relations only, even if the analyzed programs contain non-linear computations. These constraints can be easily checked by an SMT Solver. The information collected by our analysis may help to optimize the formats used to represent the values stored in the floating-point variables of programs. Experimental results are presented.},
	language = {en},
	booktitle = {{NASA} {Formal} {Methods}},
	publisher = {Springer International Publishing},
	author = {Martel, Matthieu},
	editor = {Barrett, Clark and Davies, Misty and Kahsai, Temesghen},
	year = {2017},
	keywords = {Abstract Domain, Arithmetic Expression, Binary Search, Double Precision, IEEE754 Standard},
	pages = {230--246},
}

@misc{maynard_precision_2018,
	title = {Precision of the {ENDGame}: {Mixed}-precision arithmetic in the iterative solver of the {Unified} {Model}},
	shorttitle = {Precision of the {ENDGame}},
	url = {http://arxiv.org/abs/1811.03852},
	doi = {10.48550/arXiv.1811.03852},
	abstract = {The Met Office's weather and climate simulation code the Unified Model is used for both operational Numerical Weather Prediction and Climate modelling. The computational performance of the model running on parallel supercomputers is a key consideration. A Krylov sub-space solver is employed to solve the equations of the dynamical core of the model, known as ENDGame. These describe the evolution of the Earth's atmosphere. Typically, 64-bit precision is used throughout weather and climate applications. This work presents a mixed-precision implementation of the solver, the beneficial effect on run-time and the impact on solver convergence. The complex interplay of errors arising from accumulated round-off in floating-point arithmetic and other numerical effects is discussed. A careful analysis is required, however, the mixed-precision solver is now employed in the operational forecast to satisfy run-time constraints without compromising the accuracy of the solution.},
	urldate = {2022-07-04},
	publisher = {arXiv},
	author = {Maynard, Christopher M. and Walters, David N.},
	month = nov,
	year = {2018},
	note = {Number: arXiv:1811.03852
arXiv:1811.03852 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Mathematics - Numerical Analysis},
}

@misc{zhang_high_2019,
	title = {High {Accuracy} {Low} {Precision} {QR} {Factorization} and {Least} {Square} {Solver} on {GPU} with {TensorCore}},
	url = {http://arxiv.org/abs/1912.05508},
	doi = {10.48550/arXiv.1912.05508},
	abstract = {Driven by the insatiable needs to process ever larger amount of data with more complex models, modern computer processors and accelerators are beginning to offer half precision floating point arithmetic support, and extremely optimized special units such as NVIDIA TensorCore on GPU and Google Tensor Processing Unit (TPU) that does half precision matrix-matrix multiplication exceptionally efficiently. In this paper we present a large scale mixed precision linear least square solver that achieves high accuracy using the low precision TensorCore GPU. The mixed precision system consists of both innovative algorithms and implementations, and is shown to be up to 14x faster than single precision cuSOLVER at QR matrix factorization at large scale with slightly lower accuracy, and up to 10x faster than double precision direct QR least square solver with comparable accuracy.},
	urldate = {2022-07-04},
	publisher = {arXiv},
	author = {Zhang, Shaoshuai and Wu, Panruo},
	month = dec,
	year = {2019},
	note = {Number: arXiv:1912.05508
arXiv:1912.05508 [cs]},
	keywords = {Computer Science - Mathematical Software},
}

@inproceedings{chiang_rigorous_2017,
	address = {New York, NY, USA},
	series = {{POPL} '17},
	title = {Rigorous floating-point mixed-precision tuning},
	isbn = {978-1-4503-4660-3},
	url = {https://doi.org/10.1145/3009837.3009846},
	doi = {10.1145/3009837.3009846},
	abstract = {Virtually all real-valued computations are carried out using floating-point data types and operations. The precision of these data types must be set with the goals of reducing the overall round-off error, but also emphasizing performance improvements. Often, a mixed-precision allocation achieves this optimum; unfortunately, there are no techniques available to compute such allocations and conservatively meet a given error target across all program inputs. In this work, we present a rigorous approach to precision allocation based on formal analysis via Symbolic Taylor Expansions, and error analysis based on interval functions. This approach is implemented in an automated tool called FPTuner that generates and solves a quadratically constrained quadratic program to obtain a precision-annotated version of the given expression. FPTuner automatically introduces all the requisite precision up and down casting operations. It also allows users to flexibly control precision allocation using constraints to cap the number of high precision operators as well as group operators to allocate the same precision to facilitate vectorization. We evaluate FPTuner by tuning several benchmarks and measuring the proportion of lower precision operators allocated as we increase the error threshold. We also measure the reduction in energy consumption resulting from executing mixed-precision tuned code on a real hardware platform. We observe significant energy savings in response to mixed-precision tuning, but also observe situations where unexpected compiler behaviors thwart intended optimizations.},
	urldate = {2022-07-03},
	booktitle = {Proceedings of the 44th {ACM} {SIGPLAN} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {Association for Computing Machinery},
	author = {Chiang, Wei-Fan and Baranowski, Mark and Briggs, Ian and Solovyev, Alexey and Gopalakrishnan, Ganesh and Rakamarić, Zvonimir},
	year = {2017},
	keywords = {Energy-efficient computing, Floating-point arithmetic, Precision allocation, Program optimization, Rigorous compilation},
	pages = {300--315},
}

@inproceedings{eleyat_mixed-precision_2010,
	title = {Mixed-{Precision} {Parallel} {Linear} {Programming} {Solver}},
	doi = {10.1109/SBAC-PAD.2010.14},
	abstract = {We use mixed-precision technique, which is used to exploit the high single precision performance of modern processors, to build the first sparse mixed-precision linear programming solver on the Cell BE processor. The technique is used to enhance the performance of an LP IPM-based solver by implementing mixed-precision sparse Cholesky factorization, the most time consuming part of LP solvers. Moreover, we implemented sparse matrix multiplication of the form required by the solver as it is also very time consuming for some LP problems. Implemented on the Cell BE processor (Playstation 3) and tested using Netlib data sets, our LP solver achieved a maximum speedup of 2.9 just by using the mixed-precision technique. Moreover, we found that some problems, especially in final iterations, result in ill-conditioned matrices where mixed-precision can not be used. As a result, the solver needs to switch to double-precision if a more accurate solution of an LP problem is required.},
	booktitle = {2010 22nd {International} {Symposium} on {Computer} {Architecture} and {High} {Performance} {Computing}},
	author = {Eleyat, Mujahed and Natvig, Lasse},
	month = oct,
	year = {2010},
	note = {ISSN: 1550-6533},
	keywords = {Acceleration, Accuracy, Cell BE processor, Cholesky factorization, Computer architecture, Indexes, Linear Programming, Linear programming, Mixed-precsision, Program processors, Sparse matrices},
	pages = {41--46},
}

@inproceedings{doucet_mixed-precision_2019,
	title = {Mixed-{Precision} {Tomographic} {Reconstructor} {Computations} on {Hardware} {Accelerators}},
	doi = {10.1109/IA349570.2019.00011},
	abstract = {The computation of tomographic reconstructors (ToR) is at the core of a simulation framework to design the next generation of adaptive optics (AO) systems to be installed on future Extremely Large Telescopes (ELT). In fact, it is also a critical component for their operation on sky. The goals of these instruments range from the detection of the light from the most distant galaxies to the analysis of the composition of exoplanets terrestrial atmospheres. Based on advanced AO techniques, the instrument MOSAIC relies on a computational framework to filter out the Earth atmospheric turbulence and eventually enhance the images quality out of ground-based telescopes. The ToR calculation is a compute-bound operation based on the Cholesky factorization. Due to its cubical algorithmic complexity, the ToR may represent a major bottleneck for the E-ELT when scaling up the large number of wavefront sensors used in the baseline MOSAIC design. To mitigate this increasing dimensionality overhead, this paper presents the implementation of a novel mixed-precision Cholesky-based dense matrix solver on hardware accelerators. The new algorithm takes into account the data-sparse structure of the covariance matrix operator and uses the tensor cores of NVIDIA V100 GPUs to leverage performance at an unprecedented scale. To our knowledge, this is the first computational astronomy application that exploits V100's tensor cores outside of the traditional arena of artificial intelligence. Experimental results demonstrate the accuracy robustness and the high performance of the mixed-precision ToR on synthetic datasets, which paves the way for future instrument deployments on the E-ELT.},
	booktitle = {2019 {IEEE}/{ACM} 9th {Workshop} on {Irregular} {Applications}: {Architectures} and {Algorithms} ({IA3})},
	author = {Doucet, Nicolas and Ltaief, Hatem and Gratadour, Damien and Keyes, David},
	month = nov,
	year = {2019},
	keywords = {Atmospheric measurements, Covariance matrices, European Extremely Large Telescope, GPUs and Tensor Cores, Hardware, High Performance Computing, Mixed-Precision Algorithms, Telescopes, Tensors, Tomographic Reconstructor, Tomography},
	pages = {31--38},
}

@article{lee_energy-efficient_2018,
	title = {Energy-{Efficient} {Iterative} {Refinement} {Using} {Dynamic} {Precision}},
	volume = {8},
	issn = {2156-3365},
	doi = {10.1109/JETCAS.2018.2850665},
	abstract = {Mixed precision is a promising approach to save energy in iterative refinement algorithms since it obtains speed-up without necessitating additional cores and parallelization. However, conventional mixed precision methods utilize statically defined precision in a loop, thus hindering further speed-up and energy savings. We overcome this problem by proposing novel methods which allow iterative refinement to utilize variable precision arithmetic dynamically in a loop (i.e., a trans-precision approach). Our methods restructure a numeric algorithm dynamically according to runtime numeric behavior and remove unnecessary accuracy checks. We implemented our methods by extending one conventional mixed precision iterative refinement algorithm on an Intel Xeon E5-2650 2GHz core with MKL 2017 and XBLAS 1.0. Our dynamic precision approach demonstrates 2.0-2.6× speed-up and 1.8-2.4× energy savings compared with mixed precision iterative refinement when double precision solution accuracy is required for forward error and with matrix dimensions ranging from 4K to 32K.},
	number = {4},
	journal = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
	author = {Lee, JunKyu and Vandierendonck, Hans and Arif, Mahwish and Peterson, Gregory D. and Nikolopoulos, Dimitrios S.},
	month = dec,
	year = {2018},
	note = {Conference Name: IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
	keywords = {Approximation algorithms, Circuits and systems, Energy management, Heuristic algorithms, Iterative algorithms, Runtime, Transprecision, dynamic algorithm, dynamic precision, energy savings, iterative refinement},
	pages = {722--735},
}

@inproceedings{cao_improving_2009,
	title = {Improving {Dense} {Linear} {Equation} {Solver} on {Hybrid} {CPU}-{GPU} {System}},
	doi = {10.1109/I-SPAN.2009.154},
	abstract = {In recent years, GPU (Graphic Processor Unit) has become an import accelerator for conventional applications. User has to program in GPU-based environments, such as CUDA, and it usually requires detailed tuning for good performances. Also since GPU has high Single Precision (SP) performance while its Double Precision (DP) performance falls short, it has limited application in scientific computing. In this paper, our algorithm aims at accelerating the solving of dense linear equation on hybrid CPU-GPU system. We adopt iterative refinement to utilize the high SP capability of GPUs while achieving DP precision requirements. Specifically, we implement algorithm with utilize both GPU and CPU for computation-intensive parts by overlapping computations. Its performance reaches up to 236 GFLOP/s, which is by far better than the result achieved by DP-only algorithms.},
	booktitle = {2009 10th {International} {Symposium} on {Pervasive} {Systems}, {Algorithms}, and {Networks}},
	author = {Cao, Zhichao and Xu, Shiming and Xue, Wei and Chen, Wenguang},
	month = dec,
	year = {2009},
	note = {ISSN: 2375-527X},
	keywords = {Acceleration, Algorithm design and analysis, Equations, Graphics, Iterative algorithms, Linear systems, Numerical stability, Parallel processing, Performance analysis, Scientific computing},
	pages = {556--562},
}

@article{wu_efficient_2022,
	title = {Efficient {Reconfigurable} {Mixed} {Precision} ℓ1 {Solver} for {Compressive} {Depth} {Reconstruction}},
	issn = {1939-8018},
	doi = {10.1007/s11265-022-01766-3},
	abstract = {Rapid reconstruction of depth images from sparsely sampled data is important for many applications in machine perception, including robot or vehicle assistance or autonomy. Approximate computing techniques have been widely adopted to reduce resource consumption and increase efficiency in energy and resource constrained systems, especially targeted at FPGA and solid state implementation. Whereas previous work has focused on approximate, but static, representation of data in LiDAR systems, in this paper we show how the flexibility of an arbitrary precision accelerator with fine-grain tuning allows a better trade-off between algorithmic performance and implementation efficiency. A mixed precision framework of ℓ1 solvers is presented, with compact ADMM and PGD, for the lasso problem, enabling compressive depth reconstruction by varying the precision scaling in single bit granularity during the iterative optimization process. Implementing mixed precision ℓ1 solvers on an FPGA with a pipelined architecture for depth image reconstruction across various sensing scenarios, over 74\% savings in hardware resources and 60\% in power are achieved with only minor reductions in reconstructed depth image quality when compared to single float precision, while over 10\% saving in hardware resources and power is achieved compared to relative consistently reduced precision solutions.},
	journal = {Journal of Signal Processing Systems},
	author = {Wu, Yun and Wallace, Andrew Michael and Mota, João F. C. and Aßmann, Andreas and Stewart, Brian},
	month = may,
	year = {2022},
}

@article{khazali_mixed_2014,
	title = {Mixed precision parallel preconditioner and linear solver for compositional reservoir simulation},
	volume = {18},
	issn = {1573-1499},
	url = {https://doi.org/10.1007/s10596-014-9421-3},
	doi = {10.1007/s10596-014-9421-3},
	abstract = {Reservoir simulation role in value creation and strategic management decisions cannot be over emphasized. Simulation of complex challenging reservoirs with millions of grid blocks especially in compositional mode is very time-consuming even with fast modern computers. On the other hand, high price of cluster supercomputers prevents them for being commonly used for fast simulation of such reservoirs. In recent years, the development of many-core processors like cell processors, DSPs, and graphical processing units (GPUs) has provided a very cost-effective hardware platform for fast computational operations. However, programming for such processors is much more difficult than conventional CPUs, and new parallel algorithm design and special parallel implementation methods are needed. Using the computational power of CPUs, GPUs, and/or any other processing unit, Open Computing Language (OpenCL) provides a framework for programming for heterogeneous platforms. In this paper, OpenCL is used to employ the computational power of a GPU to build a preconditioner and solve the linear system arising from compositional formulation of multiphase flow in porous media. The proposed parallel preconditioner is proved to be quite effective, even in heterogeneous porous media. Using data-parallel modules on GPU, the preconditioner/solver runtime reduced at least 1 order of magnitude compared to their serial implementation on CPU.},
	language = {en},
	number = {5},
	urldate = {2022-07-04},
	journal = {Computational Geosciences},
	author = {Khaz’ali, Ali Reza and Rasaei, Mohammad Reza and Moghadasi, Jamshid},
	month = oct,
	year = {2014},
	keywords = {Compositional modeling, GPGPU, Iterative linear solver, Many-core processor, Minimal residual method, Partial sorting, Simultaneous solution},
	pages = {729--746},
}

@misc{amestoy_combining_2022,
	type = {{MIMS} {Preprint}},
	title = {Combining sparse approximate factorizations with mixed precision iterative refinement},
	url = {http://eprints.maths.manchester.ac.uk/2845/},
	language = {en},
	urldate = {2022-07-04},
	author = {Amestoy, Patrick and Buttari, Alfredo and Higham, Nicholas J. and L'Excellent, Jean-Yves and Mary, Theo and Vieublé, Bastien},
	month = jan,
	year = {2022},
	note = {Issue: 2022.2
Number: 2022.2},
}

@article{tamstorf_discretization-error-accurate_2021,
	title = {Discretization-{Error}-{Accurate} {Mixed}-{Precision} {Multigrid} {Solvers}},
	copyright = {© 2021, Society for Industrial and Applied Mathematics},
	url = {https://epubs.siam.org/doi/10.1137/20M1349230},
	doi = {10.1137/20M1349230},
	abstract = {This paper builds on the algebraic theory in the companion paper [S. F. McCormick, J. Benzaken, and R. Tamstorf, SIAM J. Sci. Comput., (2021), pp. S392--S419] to obtain discretization-error-accurate solutions for linear elliptic partial differential equations (PDEs) by mixed-precision multigrid solvers. It is often assumed that the achievable accuracy is limited by discretization or algebraic errors. On the contrary, we show that the quantization error incurred by simply storing the matrix in any fixed precision quickly begins to dominate the total error as the discretization is refined. We extend the existing theory to account for these quantization errors and use the resulting bounds to guide the choice of four different precision levels in order to balance quantization, algebraic, and discretization errors in the progressive-precision scheme proposed in the companion paper. A remarkable result is that while iterative refinement is susceptible to quantization errors during the residual and update computations, the V-cycle used to compute the correction in each iteration is much more resilient and continues to work if the system matrices in the hierarchy become indefinite due to quantization. As a result, the V-cycle only requires relatively few bits of precision per level. Based on our findings, we outline a simple way to implement a progressive-precision full multigrid (FMG) solver with minimal overhead and demonstrate as an example that the one-dimensional biharmonic equation can be solved reliably to any desired accuracy using just a few V-cycles when the underlying smoother works well. Additionally, we show that the progressive-precision scheme leads to memory savings of up to 50\% compared to fixed precision.},
	language = {en},
	urldate = {2022-07-04},
	journal = {SIAM Journal on Scientific Computing},
	author = {Tamstorf, Rasmus and Benzaken, Joseph and McCormick, Stephen F.},
	month = jun,
	year = {2021},
	note = {Publisher: Society for Industrial and Applied Mathematics},
}

@misc{tamstorf_discretization-error-accurate_2020,
	title = {Discretization-error-accurate mixed-precision multigrid solvers},
	url = {http://arxiv.org/abs/2007.06628},
	doi = {10.48550/arXiv.2007.06628},
	abstract = {This paper builds on the algebraic theory in the companion paper [Algebraic Error Analysis for Mixed-Precision Multigrid Solvers] to obtain discretization-error-accurate solutions for linear elliptic partial differential equations (PDEs) by mixed-precision multigrid solvers. It is often assumed that the achievable accuracy is limited by discretization or algebraic errors. On the contrary, we show that the quantization error incurred by simply storing the matrix in any fixed precision quickly begins to dominate the total error as the discretization is refined. We extend the existing theory to account for these quantization errors and use the resulting bounds to guide the choice of four different precision levels in order to balance quantization, algebraic, and discretization errors in the progressive-precision scheme proposed in the companion paper. A remarkable result is that while iterative refinement is susceptible to quantization errors during the residual and update computation, the V-cycle used to compute the correction in each iteration is much more resilient, and continues to work if the system matrices in the hierarchy become indefinite due to quantization. As a result, the V-cycle only requires relatively few bits of precision per level. Based on our findings, we outline a simple way to implement a progressive precision FMG solver with minimal overhead, and demonstrate as an example that the one dimensional biharmonic equation can be solved reliably to any desired accuracy using just a few V-cycles when the underlying smoother works well. In the process we also confirm many of the theoretical results numerically.},
	urldate = {2022-07-04},
	publisher = {arXiv},
	author = {Tamstorf, Rasmus and Benzaken, Joseph and McCormick, Stephen F.},
	month = jul,
	year = {2020},
	note = {Number: arXiv:2007.06628
arXiv:2007.06628 [cs, math]},
	keywords = {65F10, 65G50, 65M55, Mathematics - Numerical Analysis},
}

@article{benner_gpu-aware_2019,
	title = {A {GPU}-aware mixed-precision solver for low-rank algebraic {Riccati} equations},
	volume = {31},
	issn = {1532-0634},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.4462},
	doi = {10.1002/cpe.4462},
	abstract = {We investigate different alternatives for the solution of algebraic Riccati equations on hybrid hardware platforms (ie, CPUs+GPUs). We evaluate a mixed-precision approach that uses single-precision arithmetic to obtain an approximation to the solution and later improve it to the desired precision, applying some steps of an economic iterative refinement. This method exploits the higher performance of the hardware to accelerate the solver when single-precision arithmetic is employed and simultaneously obtains a high-accuracy solution with the iterative refinement. We extend this approach to exploit the low-rank property of the equation, when possible, to further improve its efficiency. The experimental evaluation shows that the mixed-precision approach reports time and energy savings and provides similar or even more accurate solutions than well-known methods like the sign function iteration or the structure-preserving doubling algorithm.},
	language = {en},
	number = {6},
	urldate = {2022-07-04},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Benner, Peter and Dufrechou, Ernesto and Ezzatti, Pablo and Remón, Alfredo and Saak, Jens},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.4462},
	keywords = {algebraic Riccati equations, graphics processing units, low-rank, matrix equations, mixed precision},
	pages = {e4462},
}

@inproceedings{walden_mixed_2019,
	title = {A {Mixed} {Precision} {Multicolor} {Point}-{Implicit} {Solver} for {Unstructured} {Grids} on {GPUs}},
	doi = {10.1109/IA349570.2019.00010},
	abstract = {This paper presents a new mixed-precision implementation of a linear-solver kernel used in practical large-scale CFD simulations to improve GPU performance. The new implementation reduces memory traffic by using the half-precision format for some critical computations while maintaining double-precision solution accuracy. As the linear-solver kernel is memory bound on GPUs, a reduction in memory traffic directly translates to improved performance. The performance of the new implementation is assessed for a benchmark steady flow simulation and a large-scale unsteady turbulent flow application. Both studies were conducted using NVIDIAÆ Tesla V100 GPUs on the Summit system at the Oak Ridge Leadership Computing Facility.},
	booktitle = {2019 {IEEE}/{ACM} 9th {Workshop} on {Irregular} {Applications}: {Architectures} and {Algorithms} ({IA3})},
	author = {Walden, Aaron and Nielsen, Eric and Diskin, Boris and Zubair, Mohammad},
	month = nov,
	year = {2019},
	keywords = {Bandwidth, GPU, Graphics processing units, Image color analysis, Jacobian matrices, Kernel, Mathematical model, Sparse matrices, high performance computing, mixed precision, optimization},
	pages = {23--30},
}

@inproceedings{wu_mixed_2021,
	title = {Mixed {Precision} ℓ1 {Solver} for {Compressive} {Depth} {Reconstruction}: {An} {ADMM} {Case} {Study}},
	shorttitle = {Mixed {Precision} ℓ1 {Solver} for {Compressive} {Depth} {Reconstruction}},
	doi = {10.1109/SiPS52927.2021.00021},
	abstract = {Rapid reconstruction of depth images from sparsely sampled data is important for many machine learning applications, including robot or vehicle assistance or autonomy, which require low power LiDAR sensing for eye safety, and resource reduction for FPGA or solid state implementation, especially with constrained energy budgets. A new compressive depth reconstruction design approach is proposed using a compact ADMM solver for the lasso problem, which varies the precision scaling in an iterative optimization process. Implementations on an FPGA architecture show over 55\% savings in hardware resources and 78\% in power with only minor reduction in reconstructed depth image quality compared to single float precision.},
	booktitle = {2021 {IEEE} {Workshop} on {Signal} {Processing} {Systems} ({SiPS})},
	author = {Wu, Yun and Wallace, Andrew Michael and Aßmann, Andreas and Stewart, Brian},
	month = oct,
	year = {2021},
	note = {ISSN: 2374-7390},
	keywords = {Alternating Direction Method of Multipliers, Compressive Sensing, Depth Reconstruction, Field-Programmable Gate Array, Image coding, Machine learning, Mixed Precision, Robot sensing systems, Safety, Schedules, Sensors, Table lookup},
	pages = {70--75},
}

@techreport{abdelfattah_advances_2021,
	title = {Advances in {Mixed} {Precision} {Algorithms}: 2021 {Edition}.},
	shorttitle = {Advances in {Mixed} {Precision} {Algorithms}},
	url = {https://www.osti.gov/biblio/1814447},
	abstract = {Abstract not provided.},
	language = {English},
	number = {SAND2021-10227R},
	urldate = {2022-07-04},
	institution = {Sandia National Lab. (SNL-NM), Albuquerque, NM (United States)},
	author = {Abdelfattah, Ahmad and Anzt, Hartwig and Ayala, Alan and Boman, Erik and Carson, Erin and Cayrols, Sebastien and Cojean, Terry and Dongarra, Jack and Falgout, Rob and Gates, Mark and Higham, Nicholas J. and Kruger, Scott E. and Li, Sherry and Lindquist, Neil and Liu, Yang and Loe, Jennifer and Nayak, Pratik and Osei-Kuffuor, Daniel and Pranesh, Sri and Rajamanickam, Sivasankaran and Ribizel, Tobias and Smith, Bryce and Swirydowicz, Kasia and Thomas, Stephen and Tomov, Stanimire and Yamazaki, Ichitaro and Yang, Urike Meier},
	month = aug,
	year = {2021},
	doi = {10.2172/1814447},
}

@article{carson_mixed_2022,
	title = {Mixed precision s-step {Lanczos} and conjugate gradient algorithms},
	volume = {29},
	issn = {1099-1506},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nla.2425},
	doi = {10.1002/nla.2425},
	abstract = {Compared to the classical Lanczos algorithm, the s-step Lanczos variant has the potential to improve performance by asymptotically decreasing the synchronization cost per iteration. However, this comes at a price; despite being mathematically equivalent, the s-step variant may behave quite differently in finite precision, potentially exhibiting greater loss of accuracy and slower convergence relative to the classical algorithm. It has previously been shown that the errors in the s-step version follow the same structure as the errors in the classical algorithm, but are amplified by a factor depending on the square of the condition number of the O(s)-dimensional Krylov bases computed in each outer loop. As the condition number of these s-step bases grows (in some cases very quickly) with s, this limits the s values that can be chosen and thus can limit the attainable performance. In this work, we show that if a select few computations in s-step Lanczos are performed in double the working precision, the error terms then depend only linearly on the conditioning of the s-step bases. This has the potential for drastically improving the numerical behavior of the algorithm with little impact on per-iteration performance. Our numerical experiments demonstrate the improved numerical behavior possible with the mixed precision approach, and also show that this improved behavior extends to mixed precision s-step CG. We present preliminary performance results on NVIDIA V100 GPUs that show that the overhead of extra precision is minimal if one uses precisions implemented in hardware.},
	language = {en},
	number = {3},
	urldate = {2022-07-04},
	journal = {Numerical Linear Algebra with Applications},
	author = {Carson, Erin and Gergelits, Tomáš and Yamazaki, Ichitaro},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nla.2425},
	keywords = {Krylov subspace methods, Lanczos, avoiding communication, error analysis, finite precision, mixed precision},
	pages = {e2425},
}

@misc{loe_study_2021,
	title = {A {Study} of {Mixed} {Precision} {Strategies} for {GMRES} on {GPUs}},
	url = {http://arxiv.org/abs/2109.01232},
	doi = {10.48550/arXiv.2109.01232},
	abstract = {Support for lower precision computation is becoming more common in accelerator hardware due to lower power usage, reduced data movement and increased computational performance. However, computational science and engineering (CSE) problems require double precision accuracy in several domains. This conflict between hardware trends and application needs has resulted in a need for mixed precision strategies at the linear algebra algorithms level if we want to exploit the hardware to its full potential while meeting the accuracy requirements. In this paper, we focus on preconditioned sparse iterative linear solvers, a key kernel in several CSE applications. We present a study of mixed precision strategies for accelerating this kernel on an NVIDIA V\$100\$ GPU with a Power 9 CPU. We seek the best methods for incorporating multiple precisions into the GMRES linear solver; these include iterative refinement and parallelizable preconditioners. Our work presents strategies to determine when mixed precision GMRES will be effective and to choose parameters for a mixed precision iterative refinement solver to achieve better performance. We use an implementation that is based on the Trilinos library and employs Kokkos Kernels for performance portability of linear algebra kernels. Performance results demonstrate the promise of mixed precision approaches and demonstrate even further improvements are possible by optimizing low-level kernels.},
	urldate = {2022-07-04},
	publisher = {arXiv},
	author = {Loe, Jennifer A. and Glusa, Christian A. and Yamazaki, Ichitaro and Boman, Erik G. and Rajamanickam, Sivasankaran},
	month = sep,
	year = {2021},
	note = {Number: arXiv:2109.01232
arXiv:2109.01232 [cs, math]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Mathematical Software, Mathematics - Numerical Analysis},
}

@misc{amestoy_mixed_2021,
	title = {Mixed {Precision} {Low} {Rank} {Approximations} and their {Application} to {Block} {Low} {Rank} {LU} {Factorization}},
	url = {https://hal.archives-ouvertes.fr/hal-03251738},
	abstract = {We introduce a novel approach to exploit mixed precision arithmetic for low-rank approximations. Our approach is based on the observation that singular vectors associated with small singular values can be stored in lower precisions while preserving high accuracy overall. We provide an explicit criterion to determine which level of precision is needed for each singular vector. We apply this approach to block low-rank (BLR) matrices, most of whose off-diagonal blocks have low rank. We propose a new BLR LU factorization algorithm that exploits the mixed precision representation of the blocks. We carry out the rounding error analysis of this algorithm and prove that the use of mixed precision arithmetic does not compromise the numerical stability of BLR LU factorization. Moreover our analysis determines which level of precision is needed for each floating-point operation (flop), and therefore guides us towards an implementation that is both robust and efficient. We evaluate the potential of this new algorithm on a range of matrices coming from real-life problems in industrial and academic applications. We show that a large fraction of the entries in the LU factors and flops to perform the BLR LU factorization can be safely switched to lower precisions, leading to significant reductions of the storage and flop costs, of up to a factor three using fp64, fp32, and bfloat16 arithmetics.},
	language = {en},
	urldate = {2022-07-04},
	author = {Amestoy, Patrick and Boiteau, Olivier and Buttari, Alfredo and Gerest, Matthieu and Jézéquel, Fabienne and L'Excellent, Jean-yves and Mary, Théo},
	month = jun,
	year = {2021},
}

@article{abdelfattah_investigating_2020,
	title = {Investigating the {Benefit} of {FP16}-{Enabled} {Mixed}-{Precision} {Solvers} for {Symmetric} {Positive} {Definite} {Matrices} {Using} {GPU}},
	volume = {12138},
	url = {https://europepmc.org/articles/PMC7302814},
	abstract = {Half-precision computation refers to performing floating-point operations in a 16-bit format. While half-precision has been driven largely by machine learning applications, recent algorithmic advances in numerical linear algebra have discovered beneficial use cases for half precision in accelerating the solution of linear systems of equations at higher precisions. In this paper, we present a high-performance, mixed-precision linear solver (},
	language = {eng},
	urldate = {2022-07-04},
	journal = {Computational Science – ICCS 202020th International Conference, Amsterdam, The Netherlands, June 3–5, 2020, Proceedings, Part II},
	author = {Abdelfattah, Ahmad and Tomov, Stan and Dongarra, Jack},
	month = jun,
	year = {2020},
	pmcid = {PMC7302814},
	keywords = {Gpu Computing, Half-precision, Mixed-precision Solvers},
	pages = {237--250},
}

@misc{lopez_mixed_2020,
	type = {{MIMS} {Preprint}},
	title = {Mixed {Precision} {LU} {Factorization} on {GPU} {Tensor} {Cores}: {Reducing} {Data} {Movement} and {Memory} {Footprint}},
	shorttitle = {Mixed {Precision} {LU} {Factorization} on {GPU} {Tensor} {Cores}},
	url = {http://eprints.maths.manchester.ac.uk/2782/},
	language = {en},
	urldate = {2022-07-04},
	author = {Lopez, Florent and Mary, Theo},
	month = sep,
	year = {2020},
	note = {Issue: 2020.20
Number: 2020.20},
}

@article{mukunoki_performance_2020,
	title = {Performance and energy consumption of accurate and mixed-precision linear algebra kernels on {GPUs}},
	volume = {372},
	issn = {0377-0427},
	url = {https://www.sciencedirect.com/science/article/pii/S037704271930706X},
	doi = {10.1016/j.cam.2019.112701},
	abstract = {This paper presents the implementation, performance, and energy consumption of accurate and mixed-precision linear algebra kernels, including inner-product (DOT), dense matrix–vector multiplication (GEMV), dense matrix multiplication (GEMM), and sparse matrix–vector multiplication (SpMV) for the compressed sparse row (CSR) format (CSRMV), on graphics processing units (GPUs). We employ a mixed-precision design in our implementation, which makes it possible to perform internal floating-point operations with at least 2-fold the precision of the input and output data precision: for binary32 data, the computation is performed on binary64, and for binary64 data, the computation is performed on 2-fold the precision with an accurate inner product algorithm referred to as Dot2. We developed highly optimized implementations which can achieve performance close to the upper bound performance. From our evaluation on Titan V, a Volta architecture GPU, we made the following observations: as the Dot2 operation consumes 11 times binary64 instructions, GEMM requires the corresponding overheads (in terms of both execution time and energy consumption), compared to the standard binary64 implementation. On the other hand, the accuracy of DOT, GEMV, and CSRMV is improved with a very small overhead to the execution time and up to roughly 30\% overhead to the energy requirement.},
	language = {en},
	urldate = {2022-07-04},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Mukunoki, Daichi and Ogita, Takeshi},
	month = jul,
	year = {2020},
	keywords = {Accuracy, BLAS, Energy consumption, GPU, Mixed-precision, SpMV},
	pages = {112701},
}

@misc{jang_multi_2011,
	title = {Multi {GPU} {Performance} of {Conjugate} {Gradient} {Solver} with {Staggered} {Fermions} in {Mixed} {Precision}},
	url = {http://arxiv.org/abs/1111.0125},
	doi = {10.48550/arXiv.1111.0125},
	abstract = {GPU has a significantly higher performance in single-precision computing than that of double precision. Hence, it is important to take a maximal advantage of the single precision in the CG inverter, using the mixed precision method. We have implemented mixed precision algorithm to our multi GPU conjugate gradient solver. The single precision calculation use half of the memory that is used by the double precision calculation, which allows twice faster data transfer in memory I/O. In addition, the speed of floating point calculations is 8 times faster in single precision than in double precision. The overall performance of our CUDA code for CG is 145 giga flops per GPU (GTX480), which does not include the infiniband network communication. If we include the infiniband communication, the overall performance is 36 giga flops per GPU (GTX480).},
	urldate = {2022-07-04},
	publisher = {arXiv},
	author = {Jang, Yong-Chull and Kim, Hyung-Jin and Lee, Weonjong},
	month = nov,
	year = {2011},
	note = {Number: arXiv:1111.0125
arXiv:1111.0125 [hep-lat, physics:physics]},
	keywords = {High Energy Physics - Lattice, Physics - Computational Physics},
}

@techreport{moon_mixed-precision_2021,
	title = {Mixed-{Precision} {Schemes} for {Linear} {Algebra} {Kernels} on {GPUs}.},
	url = {https://www.osti.gov/biblio/1854428-mixed-precision-schemes-linear-algebra-kernels-gpus},
	abstract = {Abstract not provided.},
	language = {English},
	number = {SAND2021-2511C},
	urldate = {2022-07-04},
	institution = {Sandia National Lab. (SNL-NM), Albuquerque, NM (United States)},
	author = {Moon, Gordon and Rajamanickam, Sivasankaran},
	month = mar,
	year = {2021},
	doi = {10.2172/1854428},
}

@inproceedings{loe_experimental_2021,
	title = {Experimental {Evaluation} of {Multiprecision} {Strategies} for {GMRES} on {GPUs}},
	isbn = {978-1-66543-577-2},
	url = {https://www.computer.org/csdl/proceedings-article/ipdpsw/2021/357700a469/1uHgKEr11bG},
	doi = {10.1109/IPDPSW52791.2021.00078},
	abstract = {Support for lower precision computation is becoming more common in accelerator hardware due to lower power usage, reduced data movement and increased computational performance. However, computational science and engineering (CSE) problems require double precision accuracy in several domains. This conflict between hardware trends and application needs has resulted in a need for multiprecision strategies at the linear algebra algorithms level if we want to exploit the hardware to its full potential while meeting the accuracy requirements. In this paper, we focus on preconditioned sparse iterative linear solvers, a key kernel in several CSE applications. We present a study of multiprecision strategies for accelerating this kernel on GPUs. We seek the best methods for incorporating multiple precisions into the GMRES linear solver; these include iterative refinement and parallelizable preconditioners. Our work presents strategies to determine when multiprecision GMRES will be effective and to choose parameters for a multiprecision iterative refinement solver to achieve better performance. We use an implementation that is based on the Trilinos library and employs Kokkos Kernels for performance portability of linear algebra kernels. Performance results demonstrate the promise of multiprecision approaches and demonstrate even further improvements are possible by optimizing low-level kernels.},
	language = {English},
	urldate = {2022-07-04},
	publisher = {IEEE Computer Society},
	author = {Loe, Jennifer A. and Glusa, Christian A. and Yamazaki, Ichitaro and Boman, Erik G. and Rajamanickam, Sivasankaran},
	month = jun,
	year = {2021},
	pages = {469--478},
}

@inproceedings{abdelfattah_investigating_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Investigating the {Benefit} of {FP16}-{Enabled} {Mixed}-{Precision} {Solvers} for {Symmetric} {Positive} {Definite} {Matrices} {Using} {GPUs}},
	isbn = {978-3-030-50417-5},
	doi = {10.1007/978-3-030-50417-5_18},
	abstract = {Half-precision computation refers to performing floating-point operations in a 16-bit format. While half-precision has been driven largely by machine learning applications, recent algorithmic advances in numerical linear algebra have discovered beneficial use cases for half precision in accelerating the solution of linear systems of equations at higher precisions. In this paper, we present a high-performance, mixed-precision linear solver (\$\$Ax=b\$\$) for symmetric positive definite systems in double-precision using graphics processing units (GPUs). The solver is based on a mixed-precision Cholesky factorization that utilizes the high-performance tensor core units in CUDA-enabled GPUs. Since the Cholesky factors are affected by the low precision, an iterative refinement (IR) solver is required to recover the solution back to double-precision accuracy. Two different types of IR solvers are discussed on a wide range of test matrices. A preprocessing step is also developed, which scales and shifts the matrix, if necessary, in order to preserve its positive-definiteness in lower precisions. Our experiments on the V100 GPU show that performance speedups are up to 4.7\$\${\textbackslash}times \$\$ against a direct double-precision solver. However, matrix properties such as the condition number and the eigenvalue distribution can affect the convergence rate, which would consequently affect the overall performance.},
	language = {en},
	booktitle = {Computational {Science} – {ICCS} 2020},
	publisher = {Springer International Publishing},
	author = {Abdelfattah, Ahmad and Tomov, Stan and Dongarra, Jack},
	editor = {Krzhizhanovskaya, Valeria V. and Závodszky, Gábor and Lees, Michael H. and Dongarra, Jack J. and Sloot, Peter M. A. and Brissos, Sérgio and Teixeira, João},
	year = {2020},
	keywords = {GPU computing, Half-precision, Mixed-precision solvers},
	pages = {237--250},
}

@article{buttari_exploiting_2008,
	title = {Exploiting {Mixed} {Precision} {Floating} {Point} {Hardware} in {Scientific} {Computations}},
	url = {https://ebooks.iospress.nl/volumearticle/26280},
	urldate = {2022-07-04},
	journal = {High Performance Computing and Grids in Action},
	author = {Buttari, Alfredo and Dongarra, Jack and Kurzak, Jakub and Langou, Julie and Langou, Julien and Luszczek, Piotr and Tomov, Stanimire},
	year = {2008},
	note = {Publisher: IOS Press},
	pages = {19--36},
}

@article{haidar_mixed-precision_2020,
	title = {Mixed-precision iterative refinement using tensor cores on {GPUs} to accelerate solution of linear systems},
	volume = {476},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2020.0110},
	doi = {10.1098/rspa.2020.0110},
	abstract = {Double-precision floating-point arithmetic (FP64) has been the de facto standard for engineering and scientific simulations for several decades. Problem complexity and the sheer volume of data coming from various instruments and sensors motivate researchers to mix and match various approaches to optimize compute resources, including different levels of floating-point precision. In recent years, machine learning has motivated hardware support for half-precision floating-point arithmetic. A primary challenge in high-performance computing is to leverage reduced-precision and mixed-precision hardware. We show how the FP16/FP32 Tensor Cores on NVIDIA GPUs can be exploited to accelerate the solution of linear systems of equations Ax = b without sacrificing numerical stability. The techniques we employ include multiprecision LU factorization, the preconditioned generalized minimal residual algorithm (GMRES), and scaling and auto-adaptive rounding to avoid overflow. We also show how to efficiently handle systems with multiple right-hand sides. On the NVIDIA Quadro GV100 (Volta) GPU, we achieve a 
4×−5×
4×−5×
 performance increase and 5× better energy efficiency versus the standard FP64 implementation while maintaining an FP64 level of numerical stability.},
	number = {2243},
	urldate = {2022-07-04},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Haidar, Azzam and Bayraktar, Harun and Tomov, Stanimire and Dongarra, Jack and Higham, Nicholas J.},
	month = nov,
	year = {2020},
	note = {Publisher: Royal Society},
	keywords = {GMRES, GPU computing, LU factorization, half precision arithmetic, iterative refinement, mixed precision solvers},
	pages = {20200110},
}

@article{d_fast_2010,
	title = {A fast and robust mixed-precision solver for the solution of sparse symmetric linear systems},
	url = {https://dl.acm.org/doi/abs/10.1145/1731022.1731027},
	doi = {10.1145/1731022.1731027},
	abstract = {On many current and emerging computing architectures, single-precision calculations
are at least twice as fast as double-precision calculations. In addition, the use
of single precision may reduce pressure on memory bandwidth. The penalty for using
...},
	language = {EN},
	urldate = {2022-07-04},
	journal = {ACM Transactions on Mathematical Software (TOMS)},
	author = {D, HoggJ and A, ScottJ},
	month = apr,
	year = {2010},
	note = {Publisher: ACM
		PUB27
		New York, NY, USA},
}

@article{baboulin_accelerating_2009,
	series = {40 {YEARS} {OF} {CPC}: {A} celebratory issue focused on quality software for high performance, grid and novel computing architectures},
	title = {Accelerating scientific computations with mixed precision algorithms},
	volume = {180},
	issn = {0010-4655},
	url = {https://www.sciencedirect.com/science/article/pii/S0010465508003846},
	doi = {10.1016/j.cpc.2008.11.005},
	abstract = {On modern architectures, the performance of 32-bit operations is often at least twice as fast as the performance of 64-bit operations. By using a combination of 32-bit and 64-bit floating point arithmetic, the performance of many dense and sparse linear algebra algorithms can be significantly enhanced while maintaining the 64-bit accuracy of the resulting solution. The approach presented here can apply not only to conventional processors but also to other technologies such as Field Programmable Gate Arrays (FPGA), Graphical Processing Units (GPU), and the STI Cell BE processor. Results on modern processor architectures and the STI Cell BE are presented.
Program summary
Program title: ITER-REF Catalogue identifier: AECO\_v1\_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AECO\_v1\_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 7211 No. of bytes in distributed program, including test data, etc.: 41 862 Distribution format: tar.gz Programming language: FORTRAN 77 Computer: desktop, server Operating system: Unix/Linux RAM: 512 Mbytes Classification: 4.8 External routines: BLAS (optional) Nature of problem: On modern architectures, the performance of 32-bit operations is often at least twice as fast as the performance of 64-bit operations. By using a combination of 32-bit and 64-bit floating point arithmetic, the performance of many dense and sparse linear algebra algorithms can be significantly enhanced while maintaining the 64-bit accuracy of the resulting solution. Solution method: Mixed precision algorithms stem from the observation that, in many cases, a single precision solution of a problem can be refined to the point where double precision accuracy is achieved. A common approach to the solution of linear systems, either dense or sparse, is to perform the LU factorization of the coefficient matrix using Gaussian elimination. First, the coefficient matrix A is factored into the product of a lower triangular matrix L and an upper triangular matrix U. Partial row pivoting is in general used to improve numerical stability resulting in a factorization PA=LU, where P is a permutation matrix. The solution for the system is achieved by first solving Ly=Pb (forward substitution) and then solving Ux=y (backward substitution). Due to round-off errors, the computed solution, x, carries a numerical error magnified by the condition number of the coefficient matrix A. In order to improve the computed solution, an iterative process can be applied, which produces a correction to the computed solution at each iteration, which then yields the method that is commonly known as the iterative refinement algorithm. Provided that the system is not too ill-conditioned, the algorithm produces a solution correct to the working precision. Running time: seconds/minutes},
	language = {en},
	number = {12},
	urldate = {2022-07-04},
	journal = {Computer Physics Communications},
	author = {Baboulin, Marc and Buttari, Alfredo and Dongarra, Jack and Kurzak, Jakub and Langou, Julie and Langou, Julien and Luszczek, Piotr and Tomov, Stanimire},
	month = dec,
	year = {2009},
	keywords = {Iterative refinement, Mixed precision, Numerical linear algebra},
	pages = {2526--2533},
}

@article{zounon_performance_2022,
	title = {Performance impact of precision reduction in sparse linear systems solvers},
	volume = {8},
	issn = {2376-5992},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8771784/},
	doi = {10.7717/peerj-cs.778},
	abstract = {It is well established that reduced precision arithmetic can be exploited to accelerate the solution of dense linear systems. Typical examples are mixed precision algorithms that reduce the execution time and the energy consumption of parallel solvers for dense linear systems by factorizing a matrix at a precision lower than the working precision. Much less is known about the efficiency of reduced precision in parallel solvers for sparse linear systems, and existing work focuses on single core experiments. We evaluate the benefits of using single precision arithmetic in solving a double precision sparse linear system using multiple cores. We consider both direct methods and iterative methods and we focus on using single precision for the key components of LU factorization and matrix–vector products. Our results show that the anticipated speedup of 2 over a double precision LU factorization is obtained only for the very largest of our test problems. We point out two key factors underlying the poor speedup. First, we find that single precision sparse LU factorization is prone to a severe loss of performance due to the intrusion of subnormal numbers. We identify a mechanism that allows cascading fill-ins to generate subnormal numbers and show that automatically flushing subnormals to zero avoids the performance penalties. The second factor is the lack of parallelism in the analysis and reordering phases of the solvers and the absence of floating-point arithmetic in these phases. For iterative solvers, we find that for the majority of the matrices computing or applying incomplete factorization preconditioners in single precision provides at best modest performance benefits compared with the use of double precision. We also find that using single precision for the matrix–vector product kernels provides an average speedup of 1.5 over double precision kernels. In both cases some form of refinement is needed to raise the single precision results to double precision accuracy, which will reduce performance gains.},
	urldate = {2022-07-04},
	journal = {PeerJ Computer Science},
	author = {Zounon, Mawussi and Higham, Nicholas J. and Lucas, Craig and Tisseur, Françoise},
	month = jan,
	year = {2022},
	pmid = {35111903},
	pmcid = {PMC8771784},
	pages = {e778},
}

@inproceedings{anzt_mixed_2012,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Mixed {Precision} {Iterative} {Refinement} {Methods} for {Linear} {Systems}: {Convergence} {Analysis} {Based} on {Krylov} {Subspace} {Methods}},
	isbn = {978-3-642-28145-7},
	shorttitle = {Mixed {Precision} {Iterative} {Refinement} {Methods} for {Linear} {Systems}},
	doi = {10.1007/978-3-642-28145-7_24},
	abstract = {The convergence analysis of Krylov subspace solvers usually provides an estimation for the computational cost. Exact knowledge about the convergence theory of error correction methods using different floating point precision formats would enable to determine a priori whether the implementation of a mixed precision iterative refinement solver using a certain Krylov subspace method as error correction solver outperforms the plain solver in high precision. This paper reveals characteristics of mixed precision iterative refinement methods using Krylov subspace methods as inner solver.},
	language = {en},
	booktitle = {Applied {Parallel} and {Scientific} {Computing}},
	publisher = {Springer},
	author = {Anzt, Hartwig and Heuveline, Vincent and Rocker, Björn},
	editor = {Jónasson, Kristján},
	year = {2012},
	keywords = {Convergence Analysis, GPGPU, Krylov Subspace Methods, Linear Solvers, Mixed Precision Iterative Refinement},
	pages = {237--247},
}

@article{abdelfattah_survey_2021,
	title = {A survey of numerical linear algebra methods utilizing mixed-precision arithmetic},
	volume = {35},
	issn = {1094-3420},
	url = {https://doi.org/10.1177/10943420211003313},
	doi = {10.1177/10943420211003313},
	abstract = {The efficient utilization of mixed-precision numerical linear algebra algorithms can offer attractive acceleration to scientific computing applications. Especially with the hardware integration of low-precision special-function units designed for machine learning applications, the traditional numerical algorithms community urgently needs to reconsider the floating point formats used in the distinct operations to efficiently leverage the available compute power. In this work, we provide a comprehensive survey of mixed-precision numerical linear algebra routines, including the underlying concepts, theoretical background, and experimental results for both dense and sparse linear algebra problems.},
	number = {4},
	urldate = {2022-07-04},
	journal = {International Journal of High Performance Computing Applications},
	author = {Abdelfattah, Ahmad and Anzt, Hartwig and Boman, Erik G and Carson, Erin and Cojean, Terry and Dongarra, Jack and Fox, Alyson and Gates, Mark and Higham, Nicholas J and Li, Xiaoye S and Loe, Jennifer and Luszczek, Piotr and Pranesh, Srikara and Rajamanickam, Siva and Ribizel, Tobias and Smith, Barry F and Swirydowicz, Kasia and Thomas, Stephen and Tomov, Stanimire and Tsai, Yaohung M and Yang, Ulrike Meier},
	year = {2021},
	keywords = {Computer Science - Mathematical Software, G.1.3, G.4, GPUs, Mathematics - Numerical Analysis, Mixed-precision arithmetic, high-performance computing, linear algebra, numerical mathematics},
	pages = {344--369},
}

@misc{higham_mixed_2021,
	type = {{MIMS} {Preprint}},
	title = {Mixed {Precision} {Algorithms} in {Numerical} {Linear} {Algebra}},
	url = {http://eprints.maths.manchester.ac.uk/2841/},
	abstract = {Today's floating-point arithmetic landscape is broader than ever. While scientific computing has traditionally used single precision and double precision floating-point arithmetics, half precision is increasingly available in hardware and quadruple precision is supported in software. Lower precision arithmetic brings increased speed and reduced communication and energy costs, but it produces results of correspondingly low accuracy. Higher precisions are more expensive but can potentially provide great benefits, even if used sparingly. A variety of mixed precision algorithms have been developed that combine the superior performance of lower precisions with the better accuracy of higher precisions. Some of these algorithms aim to provide results of the same quality as algorithms running in a fixed precision but at a much lower cost; others use a little higher precision to improve the accuracy of an algorithm. This survey treats a broad range of mixed precision algorithms in numerical linear algebra, both direct and iterative, for problems including matrix multiplication, matrix factorization, linear systems, least squares, eigenvalue decomposition, and singular value decomposition. We identify key algorithmic ideas, such as iterative refinement, adapting the precision to the data, and exploiting mixed precision block fused multiply--add operations. We also describe the possible performance benefits and explain what is known about the numerical stability of the algorithms. This survey should be useful to a wide community of researchers and practitioners who wish to develop or benefit from mixed precision numerical linear algebra algorithms.},
	language = {en},
	urldate = {2022-07-04},
	author = {Higham, Nicholas J. and Mary, Theo},
	month = dec,
	year = {2021},
	note = {Issue: 2021.20
Number: 2021.20},
}

@misc{amd_amd_2021,
	title = {{AMD} {CDNA}™ 2 {ARCHITECTURE}},
	author = {AMD},
	year = {2021},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2022-06-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2022-06-27},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {Number: arXiv:2010.11929
arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{ootomo_recovering_2022,
	title = {Recovering single precision accuracy from {Tensor} {Cores} while surpassing the {FP32} theoretical peak performance:},
	copyright = {© The Author(s) 2022},
	shorttitle = {Recovering single precision accuracy from {Tensor} {Cores} while surpassing the {FP32} theoretical peak performance},
	url = {https://journals.sagepub.com/eprint/C6XUQKVBS3PU5SXTTFIJ/full},
	doi = {10.1177/10943420221090256},
	abstract = {Tensor Core is a mixed-precision matrix–matrix multiplication unit on NVIDIA GPUs with a theoretical peak performance of more than 300 TFlop/s on Ampere archite...},
	language = {en},
	urldate = {2022-06-06},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Ootomo, Hiroyuki and Yokota, Rio},
	month = jun,
	year = {2022},
	note = {Publisher: SAGE PublicationsSage UK: London, England},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	issn = {1533-7928},
	shorttitle = {Scikit-learn},
	url = {http://jmlr.org/papers/v12/pedregosa11a.html},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	number = {85},
	urldate = {2022-05-21},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
	year = {2011},
	pages = {2825--2830},
}

@article{fasi_numerical_2021,
	title = {Numerical behavior of {NVIDIA} tensor cores},
	volume = {7},
	issn = {2376-5992},
	doi = {10.7717/peerj-cs.330},
	abstract = {We explore the floating-point arithmetic implemented in the NVIDIA tensor cores, which are hardware accelerators for mixed-precision matrix multiplication available on the Volta, Turing, and Ampere microarchitectures. Using Volta V100, Turing T4, and Ampere A100 graphics cards, we determine what precision is used for the intermediate results, whether subnormal numbers are supported, what rounding mode is used, in which order the operations underlying the matrix multiplication are performed, and whether partial sums are normalized. These aspects are not documented by NVIDIA, and we gain insight by running carefully designed numerical experiments on these hardware units. Knowing the answers to these questions is important if one wishes to: (1) accurately simulate NVIDIA tensor cores on conventional hardware; (2) understand the differences between results produced by code that utilizes tensor cores and code that uses only IEEE 754-compliant arithmetic operations; and (3) build custom hardware whose behavior matches that of NVIDIA tensor cores. As part of this work we provide a test suite that can be easily adapted to test newer versions of the NVIDIA tensor cores as well as similar accelerators from other vendors, as they become available. Moreover, we identify a non-monotonicity issue affecting floating point multi-operand adders if the intermediate results are not normalized after each step.},
	language = {eng},
	journal = {PeerJ. Computer Science},
	author = {Fasi, Massimiliano and Higham, Nicholas J. and Mikaitis, Mantas and Pranesh, Srikara},
	year = {2021},
	pmid = {33816984},
	pmcid = {PMC7959640},
	keywords = {Binary16, Dot product, Floating-point arithmetic, Half precision, IEEE 754 arithmetic, Matrix multiply-accumulate, NVIDIA A100 GPU, NVIDIA T4 GPU, NVIDIA V100 GPU, Tensor core},
	pages = {e330},
}

@techreport{voronin_rsvdpack_2016,
	title = {{RSVDPACK}: {An} implementation of randomized algorithms for computing the singular value, interpolative, and {CUR} decompositions of matrices on multi-core and {GPU} architectures},
	shorttitle = {{RSVDPACK}},
	url = {http://arxiv.org/abs/1502.05366},
	abstract = {RSVDPACK is a library of functions for computing low rank approximations of matrices. The library includes functions for computing standard (partial) factorizations such as the Singular Value Decomposition (SVD), and also so called "structure preserving" factorizations such as the Interpolative Decomposition (ID) and the CUR decomposition. The ID and CUR factorizations pick subsets of the rows/columns of a matrix to use as bases for its row/column space. Such factorizations preserve properties of the matrix such as sparsity or non-negativity, are helpful in data interpretation, and require in certain contexts less memory than a partial SVD. The package implements highly efficient computational algorithms based on randomized sampling, as described and analyzed in [N. Halko, P.G. Martinsson, J. Tropp, "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions," SIAM Review, 53(2), 2011], and subsequent papers. This manuscript presents some modifications to the basic algorithms that improve performance and ease of use. The library is written in C and supports both multi-core CPU and GPU architectures.},
	number = {arXiv:1502.05366},
	urldate = {2022-05-18},
	institution = {arXiv},
	author = {Voronin, Sergey and Martinsson, Per-Gunnar},
	month = aug,
	year = {2016},
	doi = {10.48550/arXiv.1502.05366},
	note = {arXiv:1502.05366 [cs, math]
type: article},
	keywords = {Computer Science - Mathematical Software, Mathematics - Numerical Analysis},
}

@inproceedings{velten_memory_2022,
	address = {New York, NY, USA},
	series = {{ICPE} '22},
	title = {Memory {Performance} of {AMD} {EPYC} {Rome} and {Intel} {Cascade} {Lake} {SP} {Server} {Processors}},
	isbn = {978-1-4503-9143-6},
	url = {https://doi.org/10.1145/3489525.3511689},
	doi = {10.1145/3489525.3511689},
	abstract = {Modern processors, in particular within the server segment, integrate more cores with each generation. This increases their complexity in general, and that of the memory hierarchy in particular. Software executed on such processors can suffer from performance degradation when data is distributed disadvantageously over the available resources. To optimize data placement and access patterns, an in-depth analysis of the processor design and its implications for performance is necessary. This paper describes and experimentally evaluates the memory hierarchy of AMD EPYC Rome and Intel Xeon Cascade Lake SP server processors in detail. Their distinct microarchitectures cause different performance patterns for memory latencies, in particular for remote cache accesses. Our findings illustrate the complex NUMA properties and how data placement and cache coherence states impact access latencies to local and remote locations. This paper also compares theoretical and effective bandwidths for accessing data at the different memory levels and main memory bandwidth saturation at reduced core counts. The presented insight is a foundation for modeling performance of the given microarchitectures, which enables practical performance engineering of complex applications. Moreover, security research on side-channel attacks can also leverage the presented findings.},
	urldate = {2022-04-18},
	booktitle = {Proceedings of the 2022 {ACM}/{SPEC} on {International} {Conference} on {Performance} {Engineering}},
	publisher = {Association for Computing Machinery},
	author = {Velten, Markus and Schöne, Robert and Ilsche, Thomas and Hackenberg, Daniel},
	year = {2022},
	keywords = {amd epyc rome, amd zen 2, cache coherence, intel xeon cascade lake, intel xeon skylake, memory hierarchy},
	pages = {165--175},
}

@article{aumuller_ann-benchmarks_2020,
	title = {{ANN}-{Benchmarks}: {A} benchmarking tool for approximate nearest neighbor algorithms},
	volume = {87},
	issn = {0306-4379},
	shorttitle = {{ANN}-{Benchmarks}},
	url = {https://www.sciencedirect.com/science/article/pii/S0306437918303685},
	doi = {10.1016/j.is.2019.02.006},
	abstract = {This paper describes ANN-Benchmarks, a tool for evaluating the performance of in-memory approximate nearest neighbor algorithms. It provides a standard interface for measuring the performance and quality achieved by nearest neighbor algorithms on different standard data sets. It supports several different ways of integrating k-NN algorithms, and its configuration system automatically tests a range of parameter settings for each algorithm. Algorithms are compared with respect to many different (approximate) quality measures, and adding more is easy and fast; the included plotting front-ends can visualize these as images, LaTeX plots, and websites with interactive plots. ANN-Benchmarks aims to provide a constantly updated overview of the current state of the art of k-NN algorithms. In the short term, this overview allows users to choose the correct k-NN algorithm and parameters for their similarity search task; in the longer term, algorithm designers will be able to use this overview to test and refine automatic parameter tuning. The paper gives an overview of the system, evaluates the results of the benchmark, and points out directions for future work. Interestingly, very different approaches to k-NN search yield comparable quality-performance trade-offs. The system is available at http://ann-benchmarks.com.},
	language = {en},
	urldate = {2022-04-15},
	journal = {Information Systems},
	author = {Aumüller, Martin and Bernhardsson, Erik and Faithfull, Alexander},
	month = jan,
	year = {2020},
	keywords = {Benchmarking, Evaluation, Nearest neighbor search},
	pages = {101374},
}

@inproceedings{zhang_efficient_2018,
	title = {Efficient {Large}-{Scale} {Approximate} {Nearest} {Neighbor} {Search} on {OpenCL} {FPGA}},
	doi = {10.1109/CVPR.2018.00517},
	abstract = {We present a new method for Product Quantization (PQ) based approximated nearest neighbor search (ANN) in high dimensional spaces. Specifically, we first propose a quantization scheme for the codebook of coarse quantizer, product quantizer, and rotation matrix, to reduce the cost of accessing these codebooks. Our approach also combines a highly parallel k-selection method, which can be fused with the distance calculation to reduce the memory overhead. We implement the proposed method on Intel HARPv2 platform using OpenCL-FPGA. The proposed method significantly outperforms state-of-the-art methods on CPU and GPU for high dimensional nearest neighbor queries on billion-scale datasets in terms of query time and accuracy regardless of the batch size. To our best knowledge, this is the first work to demonstrate FPGA performance superior to CPU and GPU on high-dimensional, large-scale ANN datasets.},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Zhang, Jialiang and Li, Jing and Khoram, Soroosh},
	month = jun,
	year = {2018},
	note = {ISSN: 2575-7075},
	keywords = {Databases, Field programmable gate arrays, Graphics processing units, Micromechanical devices, Quantization (signal), System-on-chip, Table lookup},
	pages = {4924--4932},
}

@incollection{andoni_approximate_2018,
	title = {Approximate nearest neighbor search in high dimensions},
	isbn = {978-981-327-287-3},
	url = {https://www.worldscientific.com/doi/10.1142/9789813272880_0182},
	urldate = {2022-04-15},
	booktitle = {Proceedings of the {International} {Congress} of {Mathematicians} ({ICM} 2018)},
	publisher = {WORLD SCIENTIFIC},
	author = {Andoni, Alexandr and Indyk, Piotr and Razenshteyn, Ilya},
	month = jun,
	year = {2018},
	doi = {10.1142/9789813272880_0182},
	pages = {3287--3318},
}

@article{dashti_efficient_2013,
	title = {Efficient {Computation} of k-{Nearest} {Neighbour} {Graphs} for {Large} {High}-{Dimensional} {Data} {Sets} on {GPU} {Clusters}},
	volume = {8},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0074113},
	doi = {10.1371/journal.pone.0074113},
	abstract = {This paper presents an implementation of the brute-force exact k-Nearest Neighbor Graph (k-NNG) construction for ultra-large high-dimensional data cloud. The proposed method uses Graphics Processing Units (GPUs) and is scalable with multi-levels of parallelism (between nodes of a cluster, between different GPUs on a single node, and within a GPU). The method is applicable to homogeneous computing clusters with a varying number of nodes and GPUs per node. We achieve a 6-fold speedup in data processing as compared with an optimized method running on a cluster of CPUs and bring a hitherto impossible -NNG generation for a dataset of twenty million images with 15 k dimensionality into the realm of practical possibility.},
	language = {en},
	number = {9},
	urldate = {2022-04-15},
	journal = {PLOS ONE},
	author = {Dashti, Ali and Komarov, Ivan and D’Souza, Roshan M.},
	month = sep,
	year = {2013},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Computing methods, Data processing, Diffraction, Graphs, Machine learning, Machine learning algorithms, Vector spaces},
	pages = {e74113},
}

@article{arya_optimal_1998,
	title = {An optimal algorithm for approximate nearest neighbor searching fixed dimensions},
	volume = {45},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/293347.293348},
	doi = {10.1145/293347.293348},
	abstract = {Consider a set of S of n data points in real d-dimensional space, Rd, where distances are measured using any Minkowski metric. In nearest neighbor searching, we preprocess S into a data structure, so that given any query point q∈ Rd, is the closest point of S to q can be reported quickly. Given any positive real ϵ, data point p is a (1 +ϵ)-approximate nearest neighbor of q if its distance from q is within a factor of (1 + ϵ) of the distance to the true nearest neighbor. We show that it is possible to preprocess a set of n points in Rd in O(dn log n) time and O(dn) space, so that given a query point q ∈ Rd, and ϵ {\textgreater} 0, a (1 + ϵ)-approximate nearest neighbor of q can be computed in O(cd, ϵ log n) time, where cd,ϵ≤d ⌈1 + 6d/ϵ⌉d is a factor depending only on dimension and ϵ. In general, we show that given an integer k ≥ 1, (1 + ϵ)-approximations to the k nearest neighbors of q can be computed in additional O(kd log n) time.},
	number = {6},
	urldate = {2022-04-15},
	journal = {Journal of the ACM},
	author = {Arya, Sunil and Mount, David M. and Netanyahu, Nathan S. and Silverman, Ruth and Wu, Angela Y.},
	year = {1998},
	keywords = {approximation algorithms, box-decomposition trees, closet-point queries, nearest neighbor searching, post-office problem, priority search},
	pages = {891--923},
}

@inproceedings{benkaouz_nearest_2016,
	title = {Nearest {Neighbors} {Graph} {Construction}: {Peer} {Sampling} to the {Rescue}},
	volume = {9944},
	shorttitle = {Nearest {Neighbors} {Graph} {Construction}},
	url = {https://hal.inria.fr/hal-01407514},
	doi = {10.1007/978-3-319-46140-3_4},
	abstract = {In this paper, we propose an efficient KNN service, called KPS (KNN-Peer-Sampling). The KPS service can be used in various contexts e.g. recommendation systems, information retrieval and data mining. KPS borrows concepts from P2P gossip-based clustering protocols to provide a localized and efficient KNN computation in large-scale systems. KPS is a sampling-based iterative approach, combining ran-domness, to provide serendipity and avoid local minimum, and clustering , to ensure fast convergence. We compare KPS against the state of the art KNN centralized computation algorithm NNDescent, on multiple datasets. The experiments confirm the efficiency of KPS over NNDescent: KPS improves significantly on the computational cost while converging quickly to a close to optimal KNN graph. For instance, the cost, expressed in number of pairwise similarity computations, is reduced by ≈ 23\% and ≈ 49\% to construct high quality KNN graphs for Jester and MovieLens datasets, respectively. In addition, the randomized nature of KPS ensures eventual convergence, not always achieved with NNDescent.},
	language = {en},
	urldate = {2022-04-15},
	publisher = {Springer},
	author = {Benkaouz, Yahya and Erradi, Mohammed and Kermarrec, Anne-Marie},
	month = may,
	year = {2016},
	pages = {48},
}

@article{jones_randomized_2011,
	title = {Randomized approximate nearest neighbors algorithm},
	volume = {108},
	url = {https://www.pnas.org/doi/10.1073/pnas.1107769108},
	doi = {10.1073/pnas.1107769108},
	number = {38},
	urldate = {2022-04-15},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Jones, Peter Wilcox and Osipov, Andrei and Rokhlin, Vladimir},
	month = sep,
	year = {2011},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {15679--15686},
}

@inproceedings{tagami_annexml_2017,
	address = {New York, NY, USA},
	series = {{KDD} '17},
	title = {{AnnexML}: {Approximate} {Nearest} {Neighbor} {Search} for {Extreme} {Multi}-label {Classification}},
	isbn = {978-1-4503-4887-4},
	shorttitle = {{AnnexML}},
	url = {https://doi.org/10.1145/3097983.3097987},
	doi = {10.1145/3097983.3097987},
	abstract = {Extreme multi-label classification methods have been widely used in Web-scale classification tasks such as Web page tagging and product recommendation. In this paper, we present a novel graph embedding method called "AnnexML". At the training step, AnnexML constructs a k-nearest neighbor graph of label vectors and attempts to reproduce the graph structure in the embedding space. The prediction is efficiently performed by using an approximate nearest neighbor search method that efficiently explores the learned k-nearest neighbor graph in the embedding space. We conducted evaluations on several large-scale real-world data sets and compared our method with recent state-of-the-art methods. Experimental results show that our AnnexML can significantly improve prediction accuracy, especially on data sets that have larger a label space. In addition, AnnexML improves the trade-off between prediction time and accuracy. At the same level of accuracy, the prediction time of AnnexML was up to 58 times faster than that of SLEEC, which is a state-of-the-art embedding-based method.},
	urldate = {2022-04-14},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Tagami, Yukihiro},
	year = {2017},
	keywords = {approximate nearest neighbor search, extreme multi-label classification, k-nearest neighbor graph, learning-to-rank},
	pages = {455--464},
}

@inproceedings{he_difficulty_2012,
	address = {Madison, WI, USA},
	series = {{ICML}'12},
	title = {On the difficulty of nearest neighbor search},
	isbn = {978-1-4503-1285-1},
	abstract = {Fast approximate nearest neighbor(NN) search in large databases is becoming popular. Several powerful learning-based formulations have been proposed recently. However, not much attention has been paid to a more fundamental question: how difficult is (approximate) nearest neighbor search in a given data set? And which data properties affect the difficulty of nearest neighbor search and how? This paper introduces the first concrete measure called Relative Contrast that can be used to evaluate the influence of several crucial data characteristics such as dimensionality, sparsity, and database size simultaneously in arbitrary normed metric spaces. Moreover, we present a theoretical analysis to prove how the difficulty measure (relative contrast) determines/affects the complexity of Local Sensitive Hashing, a popular approximate NN search method. Relative contrast also provides an explanation for a family of heuristic hashing algorithms with good practical performance based on PCA. Finally, we show that most of the previous works in measuring NN search meaningfulness/difficulty can be derived as special asymptotic cases for dense vectors of the proposed measure.},
	urldate = {2022-04-14},
	booktitle = {Proceedings of the 29th {International} {Coference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {He, Junfeng and Kumar, Sanjiv and Chang, Shih-Fu},
	year = {2012},
	keywords = {Computer Science - Databases, Computer Science - Information Retrieval, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {41--48},
}

@inproceedings{aumuller_algorithm_2020,
	address = {Dagstuhl, Germany},
	series = {Leibniz {International} {Proceedings} in {Informatics} ({LIPIcs})},
	title = {Algorithm {Engineering} for {High}-{Dimensional} {Similarity} {Search} {Problems} ({Invited} {Talk})},
	volume = {160},
	isbn = {978-3-95977-148-1},
	url = {https://drops.dagstuhl.de/opus/volltexte/2020/12075},
	doi = {10.4230/LIPIcs.SEA.2020.1},
	urldate = {2022-04-15},
	booktitle = {18th {International} {Symposium} on {Experimental} {Algorithms} ({SEA} 2020)},
	publisher = {Schloss Dagstuhl–Leibniz-Zentrum für Informatik},
	author = {Aumüller, Martin},
	editor = {Faro, Simone and Cantone, Domenico},
	year = {2020},
	note = {ISSN: 1868-8969},
	keywords = {Benchmarking, Nearest neighbor search},
	pages = {1:1--1:3},
}

@article{iliopoulos_all-near-neighbor_2020,
	title = {All-{Near}-{Neighbor} {Search} {Among} {High}-{Dimensional} {Data} via {Hierarchical} {Sparse} {Graph} {Code} {Filtering}},
	url = {https://dukespace.lib.duke.edu/dspace/handle/10161/21017},
	abstract = {{\textless}p{\textgreater}This thesis addresses the problem of all-to-all near-neighbor (all-NN) search among a large dataset of discrete points in a high-dimensional feature space endowed with a distance metric.  Near-neighbor search is fundamental to numerous computational tasks arising in modern data analysis, modeling, and knowledge discovery.  The accuracy and efficiency of near-neighbor search depends on the underlying structure of the dataset.  In emerging data-driven analysis applications, the dataset is not necessarily stationary, the structure is not necessarily built once for all queries, and search is not necessarily limited to a few query points.  To facilitate accurate and efficient near-neighbor search in a stationary or dynamic dataset, we make a systematic investigation of all-NN search at multiple resolution levels, with attention to several standing or previously overlooked issues associated with high-dimensional feature spaces.{\textless}/p{\textgreater}{\textless}p{\textgreater}The key contributions are outlined as follows.  (i) We proclaim the inherent sparse distribution of a finite discrete dataset in a high-dimensional space.  We demonstrate that exploiting this sparsity is pivotal to dispelling the so-called curse of dimensionality in theoretical analysis and practical algorithm design.  (ii) We introduce the adjacency graph hierarchy (AGH) as an analysis apparatus as well as base infrastructure for efficient all-NN search algorithm design.  (iii) We present an efficient AGH construction algorithm by directly and deterministically pinpointing adjacent nodes at each resolution level via sparse adjacency code filtering, in lieu of exhaustive search within local node neighborhoods.  We provide at the same time a statistical estimate of the significant gap in algorithm complexity between direct location of adjacent nodes and local search.  (iv) We introduce an adjacency-sensitive, graph-based but decentralized approach for hierarchical partition of a discrete dataset, with potential advantage for accurate and efficient all-NN search in comparison to existing non-interacting partition structures.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2022-04-15},
	author = {Iliopoulos, Alexandros Stavros},
	year = {2020},
	note = {Accepted: 2020-06-09T17:59:58Z},
}

@techreport{cheng_manifold_2021,
	title = {Manifold {Learning} with {Approximate} {Nearest} {Neighbors}},
	url = {https://ideas.repec.org/p/msh/ebswps/2021-3.html},
	abstract = {Manifold learning algorithms are valuable tools for the analysis of high-dimensional data, many of which include a step where nearest neighbors of all observations are found. This can present a computational bottleneck when the number of observations is large or when the observations lie in more general metric spaces, such as statistical manifolds, which require all pairwise distances between observations to be computed. We resolve this problem by using a broad range of approximate nearest neighbor algorithms within manifold learing algorithms and evaluating their impact on embedding accuracy. We use approximate nearest neighbors for statistical maifolds by exploiting the connection between Hellinger/Total variation distance for discrete distributions and the L2/L1 norm. Via a thorough empirical investigation based on the benchmark MNIST dataset, it is shown that approximate nearest neighbors lead to substantial improvements in computational time with little to no loss in the accuracy of the embedding produced by a manifold learning algorithm. This result is robust to the use of different manifold learning algorithms, to the use of different approximate nearest neighbor algorithms, and to the use of different measures of embedding accuracy. The proposed method is applied to learning statistical manifolds data on distributions of electricity usage. This application demonstrates how the proposed methods can be used to visualize and identify anomalies and uncover underlying structure within high-dimensional data in a way that is scalable to large datasets.},
	language = {en},
	number = {3/21},
	urldate = {2022-04-15},
	institution = {Monash University, Department of Econometrics and Business Statistics},
	author = {Cheng, Fan and Hyndman, Rob J. and Panagiotelis, Anastasios},
	year = {2021},
	note = {Publication Title: Monash Econometrics and Business Statistics Working Papers},
	keywords = {Hellinger distance, anomaly detection, dimension reduction, k-d trees, smart meter data, statistical manifold},
}

@inproceedings{zhou_large_2013,
	title = {Large {Scale} {Nearest} {Neighbors} {Search} {Based} on {Neighborhood} {Graph}},
	doi = {10.1109/CBD.2013.20},
	abstract = {Large scale approximate k-nearest neighbors search is an important and very useful technique for many multimedia retrieval applications. Most of existing search algorithms used the centralized indexing approaches and thus cannot meet the needs to search upon large scale datasets. This paper proposes an efficient and distributed approximate k-nearest neighbors search algorithm over a billion high-dimensional visual descriptors. We propose a randomized partitioning strategy and then design a two-layer distributed indexing scheme based on a neighborhood graph for large scale k-nearest neighbors search. The experimental results show that our method achieves excellent performance and scalability.},
	booktitle = {2013 {International} {Conference} on {Advanced} {Cloud} and {Big} {Data}},
	author = {Zhou, Wenhui and Yuan, Chunfeng and Gu, Rong and Huang, Yihua},
	month = dec,
	year = {2013},
	keywords = {Algorithm design and analysis, Indexing, Multimedia communication, Nearest neighbor searches, Partitioning algorithms, Scalability, Upper bound, approximate k-nearest neighbors, distributed indexing, large scale search, neighborhood graph},
	pages = {181--186},
}

@inproceedings{zhang_grasp_2022,
	address = {New York, NY, USA},
	series = {{WSDM} '22},
	title = {{GraSP}: {Optimizing} {Graph}-based {Nearest} {Neighbor} {Search} with {Subgraph} {Sampling} and {Pruning}},
	isbn = {978-1-4503-9132-0},
	shorttitle = {{GraSP}},
	url = {https://doi.org/10.1145/3488560.3498425},
	doi = {10.1145/3488560.3498425},
	abstract = {Nearest Neighbor Search (NNS) has recently drawn a rapid growth of interest because of its core role in high-dimensional vector data management in data science and AI applications. The interest is fueled by the success of neural embedding, where deep learning models transform unstructured data into semantically correlated feature vectors for data analysis, e.g., recommending popular items. Among several categories of methods for fast NNS, graph-based approximate nearest neighbor search algorithms have led to the best-in-class search performance on a wide range of real-world datasets. While prior works improve graph-based NNS search efficiency mainly through exploiting the structure of the graph with sophisticated heuristic rules, in this work, we show that the frequency distributions of edge visits for graph-based NNS can be highly skewed. This finding leads to the study of pruning unnecessary edges to avoid redundant computation during graph traversal by utilizing the query distribution, an important yet under-explored aspect of graph-based NNS. In particular, we formulate graph pruning as a discrete optimization problem, and introduce a graph optimization algorithm GraSP that improves the search efficiency of similarity graphs by learning to prune redundant edges. GraSP enhances an existing similarity graph with a probabilistic model. It then performs a novel subgraph sampling and iterative refinement optimization to explicitly maximize search efficiency when removing a subset of edges in expectation over a graph for a large set of training queries. The evaluation shows that GraSP consistently improves the search efficiency on real-world datasets, providing up to 2.24X faster search speed than state-of-the-art methods without losing accuracy.},
	urldate = {2022-04-14},
	booktitle = {Proceedings of the {Fifteenth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Minjia and Wang, Wenhan and He, Yuxiong},
	year = {2022},
	keywords = {graph sampling, search efficiency, vector management and search},
	pages = {1395--1405},
}

@article{xiao_fast_2018,
	title = {Fast {Approximate} {Nearest} {Neighbor} {Search} via k-{Diverse} {Nearest} {Neighbor} {Graph}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/12138},
	abstract = {Approximate nearest neighbor search is a fundamental problem and has been studied for a few decades. Recently graph-based indexing methods have demonstrated their great efficiency, whose main idea is to construct neighborhood graph offline and perform a greedy search starting from some sampled points of the graph online. Most existing graph-based methods focus on either the precise k-nearest neighbor (k-NN) graph which has good exploitation ability, or the diverse graph which has good exploration ability. In this paper, we propose the k-diverse nearest neighbor (k-DNN) graph, which balances the precision and diversity of the graph, leading to good exploitation and exploration abilities simultaneously. We introduce an efficient indexing algorithm for the construction of the k-DNN graph inspired by a well-known diverse ranking algorithm in information retrieval (IR). Experimental results show that our method can outperform both state-of-the-art precise graph and diverse graph methods.},
	language = {en},
	number = {1},
	urldate = {2022-04-15},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Xiao, Yan and Guo, Jiafeng and Lan, Yanyan and Xu, Jun and Cheng, Xueqi},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {nearest neighbor search},
}

@inproceedings{arya_approximate_1993,
	address = {USA},
	series = {{SODA} '93},
	title = {Approximate nearest neighbor queries in fixed dimensions},
	isbn = {978-0-89871-313-8},
	urldate = {2022-04-14},
	booktitle = {Proceedings of the fourth annual {ACM}-{SIAM} symposium on {Discrete} algorithms},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Arya, Sunil and Mount, David M.},
	year = {1993},
	pages = {271--280},
}

@article{li_approximate_2020,
	title = {Approximate {Nearest} {Neighbor} {Search} on {High} {Dimensional} {Data} — {Experiments}, {Analyses}, and {Improvement}},
	volume = {32},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2019.2909204},
	abstract = {Nearest neighbor search is a fundamental and essential operation in applications from many domains, such as databases, machine learning, multimedia, and computer vision. Because exact searching results are not efficient for a high-dimensional space, a lot of efforts have turned to approximate nearest neighbor search. Although many algorithms have been continuously proposed in the literature each year, there is no comprehensive evaluation and analysis of their performance. In this paper, we conduct a comprehensive experimental evaluation of many state-of-the-art methods for approximate nearest neighbor search. Our study (1) is cross-disciplinary (i.e., including 19 algorithms in different domains, and from practitioners) and (2) has evaluated a diverse range of settings, including 20 datasets, several evaluation metrics, and different query workloads. The experimental results are carefully reported and analyzed to understand the performance results. Furthermore, we propose a new method that achieves both high query efficiency and high recall empirically on majority of the datasets under a wide range of settings.},
	number = {8},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Li, Wen and Zhang, Ying and Sun, Yifang and Wang, Wei and Li, Mingjie and Zhang, Wenjie and Lin, Xuemin},
	month = aug,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Data models, Machine learning, Nearest neighbor methods, Performance evaluation, Similarity search, approximate nearest neighbor search, dense vector, high-dimensional space, metric space},
	pages = {1475--1488},
}

@inproceedings{anastasiu_tutorial_2019,
	address = {New York, NY, USA},
	series = {{KDD} '19},
	title = {Tutorial: {Are} {You} {My} {Neighbor}? {Bringing} {Order} to {Neighbor} {Computing} {Problems}.},
	isbn = {978-1-4503-6201-6},
	shorttitle = {Tutorial},
	url = {https://doi.org/10.1145/3292500.3332292},
	doi = {10.1145/3292500.3332292},
	abstract = {Finding nearest neighbors is an important topic that has attracted much attention over the years and has applications in many fields, such as market basket analysis, plagiarism and anomaly detection, community detection, ligand-based virtual screening, etc. As data are easier and easier to collect, finding neighbors has become a potential bottleneck in analysis pipelines. Performing pairwise comparisons given the massive datasets of today is no longer feasible. The high computational complexity of the task has led researchers to develop approximate methods, which find many but not all of the nearest neighbors. Yet, for some types of data, efficient exact solutions have been found by carefully partitioning or filtering the search space in a way that avoids most unnecessary comparisons. In recent years, there have been several fundamental advances in our ability to efficiently identify appropriate neighbors, especially in non-traditional data, such as graphs or document collections. In this tutorial, we provide an in-depth overview of recent methods for finding (nearest) neighbors, focusing on the intuition behind choices made in the design of those algorithms and on the utility of the methods in real-world applications. Our tutorial aims to provide a unifying view of "neighbor computing" problems, spanning from numerical data to graph data, from categorical data to sequential data, and related application scenarios. For each type of data, we will review the current state-of-the-art approaches used to identify neighbors and discuss how neighbor search methods are used to solve important problems.},
	urldate = {2022-04-14},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Anastasiu, David C. and Rangwala, Huzefa and Tagarelli, Andrea},
	year = {2019},
	keywords = {graph construction, nearest neighbor search, sparse data},
	pages = {3241--3242},
}

@article{plaku_distributed_2007,
	title = {Distributed {Computation} of the knn {Graph} for {Large} {High}-{Dimensional} {Point} {Sets}},
	volume = {67},
	issn = {0743-7315},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2764297/},
	doi = {10.1016/j.jpdc.2006.10.004},
	abstract = {High-dimensional problems arising from robot motion planning, biology, data mining, and geographic information systems often require the computation of k nearest neighbor (knn) graphs. The knn graph of a data set is obtained by connecting each point to its k closest points. As the research in the above-mentioned fields progressively addresses problems of unprecedented complexity, the demand for computing knn graphs based on arbitrary distance metrics and large high-dimensional data sets increases, exceeding resources available to a single machine. In this work we efficiently distribute the computation of knn graphs for clusters of processors with message passing. Extensions to our distributed framework include the computation of graphs based on other proximity queries, such as approximate knn or range queries. Our experiments show nearly linear speedup with over one hundred processors and indicate that similar speedup can be obtained with several hundred processors.},
	number = {3},
	urldate = {2022-04-15},
	journal = {Journal of parallel and distributed computing},
	author = {Plaku, Erion and Kavraki, Lydia E.},
	month = mar,
	year = {2007},
	pmid = {19847318},
	pmcid = {PMC2764297},
	pages = {346--359},
}

@inproceedings{li_improving_2020,
	address = {New York, NY, USA},
	series = {{SIGMOD} '20},
	title = {Improving {Approximate} {Nearest} {Neighbor} {Search} through {Learned} {Adaptive} {Early} {Termination}},
	isbn = {978-1-4503-6735-6},
	url = {https://doi.org/10.1145/3318464.3380600},
	doi = {10.1145/3318464.3380600},
	abstract = {In applications ranging from image search to recommendation systems, the problem of identifying a set of "similar" real-valued vectors to a query vector plays a critical role. However, retrieving these vectors and computing the corresponding similarity scores from a large database is computationally challenging. Approximate nearest neighbor (ANN) search relaxes the guarantee of exactness for efficiency by vector compression and/or by only searching a subset of database vectors for each query. Searching a larger subset increases both accuracy and latency. State-of-the-art ANN approaches use fixed configurations that apply the same termination condition (the size of subset to search) for all queries, which leads to undesirably high latency when trying to achieve the last few percents of accuracy. We find that due to the index structures and the vector distributions, the number of database vectors that must be searched to find the ground-truth nearest neighbor varies widely among queries. Critically, we further identify that the intermediate search result after a certain amount of search is an important runtime feature that indicates how much more search should be performed. To achieve a better tradeoff between latency and accuracy, we propose a novel approach that adaptively determines search termination conditions for individual queries. To do so, we build and train gradient boosting decision tree models to learn and predict when to stop searching for a certain query. These models enable us to achieve the same accuracy with less total amount of search compared to the fixed configurations. We apply the learned adaptive early termination to state-of-the-art ANN approaches, and evaluate the end-to-end performance on three million to billion-scale datasets. Compared with fixed configurations, our approach consistently improves the average end-to-end latency by up to 7.1 times faster under the same high accuracy targets. Our approach is open source at github.com/efficient/faiss-learned-termination.},
	urldate = {2022-04-14},
	booktitle = {Proceedings of the 2020 {ACM} {SIGMOD} {International} {Conference} on {Management} of {Data}},
	publisher = {Association for Computing Machinery},
	author = {Li, Conglong and Zhang, Minjia and Andersen, David G. and He, Yuxiong},
	year = {2020},
	keywords = {approximate nearest neighbor search, information retrieval},
	pages = {2539--2554},
}

@inproceedings{chen_spann_2021,
	title = {{SPANN}: {Highly}-efficient {Billion}-scale {Approximate} {Nearest} {Neighborhood} {Search}},
	volume = {34},
	shorttitle = {{SPANN}},
	url = {https://papers.nips.cc/paper/2021/hash/299dc35e747eb77177d9cea10a802da2-Abstract.html},
	abstract = {The in-memory algorithms for approximate nearest neighbor search (ANNS) have achieved great success for fast high-recall search, but are extremely expensive when handling very large scale database. Thus, there is an increasing request for the hybrid ANNS solutions with small memory and inexpensive solid-state drive (SSD). In this paper, we present a simple but efficient memory-disk hybrid indexing and search system, named SPANN, that follows the inverted index methodology. It stores the centroid points of the posting lists in the memory and the large posting lists in the disk. We guarantee both disk-access efficiency (low  latency) and high recall by effectively reducing the disk-access number and retrieving high-quality posting lists. In the index-building stage, we adopt a hierarchical balanced clustering algorithm to balance the length of posting lists and augment the posting list by adding the points in the closure of the corresponding clusters. In the search stage, we use a query-aware scheme to dynamically prune the access of unnecessary posting lists.  Experiment results demonstrate that SPANN is 2X faster than the state-of-the-art ANNS solution DiskANN to reach the same recall quality 90\% with same memory cost in three billion-scale datasets. It can reach 90\% recall@1 and recall@10 in just around one millisecond with only 32GB memory cost.  Code is available at: https://github.com/microsoft/SPTAG.},
	urldate = {2022-04-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Qi and Zhao, Bing and Wang, Haidong and Li, Mingqin and Liu, Chuanjie and Li, Zengzhong and Yang, Mao and Wang, Jingdong},
	year = {2021},
	pages = {5199--5212},
}

@article{vargas_munoz_hierarchical_2019,
	title = {Hierarchical {Clustering}-{Based} {Graphs} for {Large} {Scale} {Approximate} {Nearest} {Neighbor} {Search}},
	volume = {96},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320319302730},
	doi = {10.1016/j.patcog.2019.106970},
	abstract = {This paper presents a novel approach to perform fast approximate nearest neighbors search in high dimensional data, using a nearest neighbor graph created over large collections. This graph is created based on the fusion of multiple hierarchical clustering results, where a minimum-spanning-tree structure is used to connect all elements in a cluster. We propose a novel search technique to guide the navigation on the graph without computing exhaustively the distances to all neighbors in each step of the search, just to those in the direction of the query. The objective is to determine the nearest point to the query with a few number of distance calculations. We experimented in three datasets of 1 million SIFT, GIST, and GloVe features. Results show better speedups than another graph-based technique, and competitive speedups at high recall values when compared to classic and recent state-of-the-art techniques.},
	language = {en},
	urldate = {2022-04-15},
	journal = {Pattern Recognition},
	author = {Vargas Muñoz, Javier and Gonçalves, Marcos A. and Dias, Zanoni and da S. Torres, Ricardo},
	month = dec,
	year = {2019},
	keywords = {Approximate nearest neighbors search, Graph-based search, Guided search, Hierarchical clustering},
	pages = {106970},
}

@inproceedings{zhao_song_2020,
	title = {{SONG}: {Approximate} {Nearest} {Neighbor} {Search} on {GPU}},
	shorttitle = {{SONG}},
	doi = {10.1109/ICDE48307.2020.00094},
	abstract = {Approximate nearest neighbor (ANN) searching is a fundamental problem in computer science with numerous applications in (e.g.,) machine learning and data mining. Recent studies show that graph-based ANN methods often outperform other types of ANN algorithms. For typical graph-based methods, the searching algorithm is executed iteratively and the execution dependency prohibits GPU adaptations. In this paper, we present a novel framework that decouples the searching on graph algorithm into 3 stages, in order to parallel the performance-crucial distance computation. Furthermore, to obtain better parallelism on GPU, we propose novel ANN-specific optimization methods that eliminate dynamic GPU memory allocations and trade computations for less GPU memory consumption. The proposed system is empirically compared against HNSW–the state-of-the-art ANN method on CPU–and Faiss–the popular GPU-accelerated ANN platform–on 6 datasets. The results confirm the effectiveness: SONG has around 50-180x speedup compared with single-thread HNSW, while it substantially outperforms Faiss.},
	booktitle = {2020 {IEEE} 36th {International} {Conference} on {Data} {Engineering} ({ICDE})},
	author = {Zhao, Weijie and Tan, Shulong and Li, Ping},
	month = apr,
	year = {2020},
	note = {ISSN: 2375-026X},
	keywords = {Approximation algorithms, Data structures, Graphics processing units, Indexes, Instruction sets, Memory management},
	pages = {1033--1044},
}

@incollection{subramanya_diskann_2019,
	address = {Red Hook, NY, USA},
	title = {{DiskANN}: fast accurate billion-point nearest neighbor search on a single node},
	shorttitle = {{DiskANN}},
	abstract = {Current state-of-the-art approximate nearest neighbor search (ANNS) algorithms generate indices that must be stored in main memory for fast high-recall search. This makes them expensive and limits the size of the dataset. We present a new graph-based ...},
	number = {1233},
	urldate = {2022-04-14},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Subramanya, Suhas Jayaram and Devvrit and Kadekodi, Rohan and Krishaswamy, Ravishankar and Simhadri, Harsha Vardhan},
	year = {2019},
	pages = {13766--13776},
}

@article{malkov_approximate_2014,
	title = {Approximate nearest neighbor algorithm based on navigable small world graphs},
	volume = {45},
	issn = {0306-4379},
	url = {https://www.sciencedirect.com/science/article/pii/S0306437913001300},
	doi = {10.1016/j.is.2013.10.006},
	abstract = {We propose a novel approach to solving the approximate k-nearest neighbor search problem in metric spaces. The search structure is based on a navigable small world graph with vertices corresponding to the stored elements, edges to links between them, and a variation of greedy algorithm for searching. The navigable small world is created simply by keeping old Delaunay graph approximation links produced at the start of construction. The approach is very universal, defined in terms of arbitrary metric spaces and at the same time it is very simple. The algorithm handles insertions in the same way as queries: by finding approximate neighbors for the inserted element and connecting it to them. Both search and insertion can be done in parallel requiring only local information from the structure. The structure can be made distributed. The accuracy of the probabilistic k-nearest neighbor queries can be adjusted without rebuilding the structure. The performed simulation for data in the Euclidean spaces shows that the structure built using the proposed algorithm has small world navigation properties with log2(n) insertion and search complexity at fixed accuracy, and performs well at high dimensionality. Simulation on a CoPHiR dataset revealed its high efficiency in case of large datasets (more than an order of magnitude less metric computations at fixed recall) compared to permutation indexes. Only 0.03\% of the 10 million 208-dimensional vector dataset is needed to be evaluated to achieve 0.999 recall (virtually exact search). For recall 0.93 processing speed 2800queries/s can be achieved on a dual Intel X5675 Xenon server node with Java implementation.},
	language = {en},
	urldate = {2022-04-15},
	journal = {Information Systems},
	author = {Malkov, Yury and Ponomarenko, Alexander and Logvinov, Andrey and Krylov, Vladimir},
	month = sep,
	year = {2014},
	keywords = {Approximate nearest neighbor, Distributed data structure, Navigable small world, Similarity search, k-Nearest neighbor},
	pages = {61--68},
}

@inproceedings{ponomarenko_approximate_2011,
	title = {Approximate {Nearest} {Neighbor} {Search} {Small} {World} {Approach}},
	doi = {10.13140/2.1.2152.8964},
	abstract = {In this paper we propose a novel approach to solving the nearest neighbor search problem. We propose to build a data structure where the greedy search algorithm can be applied which is known to have logarithmic complexity in structures with navigable small world properties. The distinctive feature of our approach is that we build a non-hierarchical structure with possibility of local minimums which are circumvented by performing a series of searches starting from arbitrary elements of the structure. The performed simulation shows that the structure built using the proposed algorithms has navigable small world properties with logarithmic search complexity which is retained even for high-dimensional data.},
	author = {Ponomarenko, Alexander and Malkov, Yu and {Kov} and Logvinov, Andrey and Krylov, Vladimir},
	month = jan,
	year = {2011},
}

@article{malkov_efficient_2018,
	title = {Efficient and robust approximate nearest neighbor search using {Hierarchical} {Navigable} {Small} {World} graphs},
	url = {http://arxiv.org/abs/1603.09320},
	abstract = {We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW). The proposed solution is fully graph-based, without any need for additional search structures, which are typically used at the coarse search stage of the most proximity graph techniques. Hierarchical NSW incrementally builds a multi-layer structure consisting from hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation.},
	urldate = {2022-04-15},
	journal = {arXiv:1603.09320 [cs]},
	author = {Malkov, Yu A. and Yashunin, D. A.},
	month = aug,
	year = {2018},
	note = {arXiv: 1603.09320},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Data Structures and Algorithms, Computer Science - Information Retrieval, Computer Science - Social and Information Networks},
}

@article{fan_tree-based_2022,
	title = {Tree-based {Search} {Graph} for {Approximate} {Nearest} {Neighbor} {Search}},
	url = {http://arxiv.org/abs/2201.03237},
	abstract = {Nearest neighbor search supports important applications in many domains, such as database, machine learning, computer vision. Since the computational cost for accurate search is too high, the community turned to the research of approximate nearest neighbor search (ANNS). Among them, graph-based algorithm is one of the most important branches. Research by Fu et al. shows that the algorithms based on Monotonic Search Network (MSNET), such as NSG and NSSG, have achieved the state-of-the-art search performance in efficiency. The MSNET is dedicated to achieving monotonic search with minimal out-degree of nodes to pursue high efficiency. However, the current MSNET designs did not optimize the probability of the monotonic search, and the lower bound of the probability is only 50\%. If they fail in monotonic search stage, they have to suffer tremendous backtracking cost to achieve the required accuracy. This will cause performance problems in search efficiency. To address this problem, we propose (r,p)-MSNET, which achieves guaranteed probability on monotonic search. Due to the high building complexity of a strict (r,p)-MSNET, we propose TBSG, which is an approximation with low complexity. Experiment conducted on four million-scaled datasets show that TBSG outperforms existing state-of-the-art graph-based algorithms in search efficiency. Our code has been released on Github.},
	urldate = {2022-04-15},
	journal = {arXiv:2201.03237 [cs]},
	author = {Fan, Xiaobin and Wang, Xiaoping and Lu, Kai and Xue, Lei and Zhao, Jinjing},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.03237},
	keywords = {Computer Science - Information Retrieval},
}

@inproceedings{harwood_fanng_2016,
	title = {{FANNG}: {Fast} {Approximate} {Nearest} {Neighbour} {Graphs}},
	shorttitle = {{FANNG}},
	doi = {10.1109/CVPR.2016.616},
	abstract = {We present a new method for approximate nearest neighbour search on large datasets of high dimensional feature vectors, such as SIFT or GIST descriptors. Our approach constructs a directed graph that can be efficiently explored for nearest neighbour queries. Each vertex in this graph represents a feature vector from the dataset being searched. The directed edges are computed by exploiting the fact that, for these datasets, the intrinsic dimensionality of the local manifold-like structure formed by the elements of the dataset is significantly lower than the embedding space. We also provide an efficient search algorithm that uses this graph to rapidly find the nearest neighbour to a query with high probability. We show how the method can be adapted to give a strong guarantee of 100\% recall where the query is within a threshold distance of its nearest neighbour. We demonstrate that our method is significantly more efficient than existing state of the art methods. In particular, our GPU implementation can deliver 90\% recall for queries on a data set of 1 million SIFT descriptors at a rate of over 1.2 million queries per second on a Titan X. Finally we also demonstrate how our method scales to datasets of 5M and 20M entries.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Harwood, Ben and Drummond, Tom},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {Approximation algorithms, Buildings, Computer vision, Hardware, Quantization (signal), Search problems, Space exploration},
	pages = {5713--5722},
}

@article{doostparast_torshizi_graph-based_2017,
	title = {Graph-based semi-supervised learning with genomic data integration using condition-responsive genes applied to phenotype classification},
	volume = {25},
	issn = {1067-5027},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7647127/},
	doi = {10.1093/jamia/ocx032},
	abstract = {Objective
Data integration methods that combine data from different molecular levels such as genome, epigenome, transcriptome, etc., have received a great deal of interest in the past few years. It has been demonstrated that the synergistic effects of different biological data types can boost learning capabilities and lead to a better understanding of the underlying interactions among molecular levels.

Methods
In this paper we present a graph-based semi-supervised classification algorithm that incorporates latent biological knowledge in the form of biological pathways with gene expression and DNA methylation data. The process of graph construction from biological pathways is based on detecting condition-responsive genes, where 3 sets of genes are finally extracted: all condition responsive genes, high-frequency condition-responsive genes, and P-value–filtered genes.

Results
The proposed approach is applied to ovarian cancer data downloaded from the Human Genome Atlas. Extensive numerical experiments demonstrate superior performance of the proposed approach compared to other state-of-the-art algorithms, including the latest graph-based classification techniques.

Conclusions
Simulation results demonstrate that integrating various data types enhances classification performance and leads to a better understanding of interrelations between diverse omics data types. The proposed approach outperforms many of the state-of-the-art data integration algorithms.},
	number = {1},
	urldate = {2022-04-15},
	journal = {Journal of the American Medical Informatics Association : JAMIA},
	author = {Doostparast Torshizi, Abolfazl and Petzold, Linda R},
	month = may,
	year = {2017},
	pmid = {28505320},
	pmcid = {PMC7647127},
	pages = {99--108},
}

@inproceedings{ren_hm-ann_2020,
	title = {{HM}-{ANN}: {Efficient} {Billion}-{Point} {Nearest} {Neighbor} {Search} on {Heterogeneous} {Memory}},
	volume = {33},
	shorttitle = {{HM}-{ANN}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/788d986905533aba051261497ecffcbb-Abstract.html},
	abstract = {The state-of-the-art approximate nearest neighbor search (ANNS) algorithms face a fundamental tradeoff between query latency and accuracy, because of small main memory capacity: To store indices in main memory for short query latency, the ANNS algorithms have to limit dataset size or use a quantization scheme which hurts search accuracy. The emergence of heterogeneous memory (HM) brings a solution to significantly increase memory capacity and break the above tradeoff: Using HM, billions of data points can be placed in the main memory on a single machine without using any data compression. However, HM consists of both fast (but small) memory and slow (but large) memory, and using HM inappropriately slows down query significantly. 
In this work, we present a novel graph-based similarity search algorithm called HM-ANN, which takes both memory and data heterogeneity into consideration and enables billion-scale similarity search on a single node without using compression. On two billion-sized datasets BIGANN and DEEP1B, HM-ANN outperforms state-of-the-art compression-based solutions such as L\&C and IMI+OPQ in recall-vs-latency by a large margin, obtaining 46\% higher recall under the same search latency. We also extend existing graph-based methods such as HNSW and NSG with two strong baseline implementations on HM. At billion-point scale, HM-ANN is 2X and 5.8X faster than our HNSWand NSG baselines respectively to reach the same accuracy.},
	urldate = {2022-04-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ren, Jie and Zhang, Minjia and Li, Dong},
	year = {2020},
	pages = {10672--10684},
}

@article{groh_ggnn_2022,
	title = {{GGNN}: {Graph}-based {GPU} {Nearest} {Neighbor} {Search}},
	issn = {2332-7790},
	shorttitle = {{GGNN}},
	doi = {10.1109/TBDATA.2022.3161156},
	abstract = {Approximate nearest neighbor (ANN) search in high dimensions is an integral part of several computer vision systems and gains importance in deep learning with explicit memory representations. Since PQT, FAISS, and SONG started to leverage the massive parallelism offered by GPUs, GPU-based implementations are a crucial resource for today's state-of-the-art ANN methods. While most of these methods allow for faster queries, less emphasis is devoted to accelerating the construction of the underlying index structures. In this paper, we propose a novel GPU-friendly search structure based on nearest neighbor graphs and information propagation on graphs. Our method is designed to take advantage of GPU architectures to accelerate the hierarchical construction of the index structure and for performing the query. Empirical evaluation shows that GGNN significantly surpasses the state-of-the-art CPU- and GPU-based systems in terms of build-time, accuracy and search speed.},
	journal = {IEEE Transactions on Big Data},
	author = {Groh, Fabian and Ruppert, Lukas and Wieschollek, Patrick and Lensch, Hendrik},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Big Data},
	keywords = {Approximate search, Big Data, Big data, Graph and tree search strategies, Graphics processing units, Indexes, Information retrieval, Nearest neighbor methods, Nearest neighbor searches, Parallel processing, Quantization (signal), Search problems, Similarity search},
	pages = {1--1},
}

@article{fu_fast_2019,
	title = {Fast approximate nearest neighbor search with the navigating spreading-out graph},
	volume = {12},
	issn = {2150-8097},
	url = {https://doi.org/10.14778/3303753.3303754},
	doi = {10.14778/3303753.3303754},
	abstract = {Approximate nearest neighbor search (ANNS) is a fundamental problem in databases and data mining. A scalable ANNS algorithm should be both memory-efficient and fast. Some early graph-based approaches have shown attractive theoretical guarantees on search time complexity, but they all suffer from the problem of high indexing time complexity. Recently, some graph-based methods have been proposed to reduce indexing complexity by approximating the traditional graphs; these methods have achieved revolutionary performance on million-scale datasets. Yet, they still can not scale to billion-node databases. In this paper, to further improve the search-efficiency and scalability of graph-based methods, we start by introducing four aspects: (1) ensuring the connectivity of the graph; (2) lowering the average out-degree of the graph for fast traversal; (3) shortening the search path; and (4) reducing the index size. Then, we propose a novel graph structure called Monotonic Relative Neighborhood Graph (MRNG) which guarantees very low search complexity (close to logarithmic time). To further lower the indexing complexity and make it practical for billion-node ANNS problems, we propose a novel graph structure named Navigating Spreading-out Graph (NSG) by approximating the MRNG. The NSG takes the four aspects into account simultaneously. Extensive experiments show that NSG outperforms all the existing algorithms significantly. In addition, NSG shows superior performance in the E-commercial scenario of Taobao (Alibaba Group) and has been integrated into their billion-scale search engine.},
	number = {5},
	urldate = {2022-04-15},
	journal = {Proceedings of the VLDB Endowment},
	author = {Fu, Cong and Xiang, Chao and Wang, Changxu and Cai, Deng},
	year = {2019},
	pages = {461--474},
}

@inproceedings{prokhorenkova_graph-based_2020,
	title = {Graph-based {Nearest} {Neighbor} {Search}: {From} {Practice} to {Theory}},
	shorttitle = {Graph-based {Nearest} {Neighbor} {Search}},
	url = {https://proceedings.mlr.press/v119/prokhorenkova20a.html},
	abstract = {Graph-based approaches are empirically shown to be very successful for the nearest neighbor search (NNS). However, there has been very little research on their theoretical guarantees. We fill this gap and rigorously analyze the performance of graph-based NNS algorithms, specifically focusing on the low-dimensional (d≪lognd≪log⁡nd {\textbackslash}ll {\textbackslash}log n) regime. In addition to the basic greedy algorithm on nearest neighbor graphs, we also analyze the most successful heuristics commonly used in practice: speeding up via adding shortcut edges and improving accuracy via maintaining a dynamic list of candidates. We believe that our theoretical insights supported by experimental analysis are an important step towards understanding the limits and benefits of graph-based NNS algorithms.},
	language = {en},
	urldate = {2022-04-15},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Prokhorenkova, Liudmila and Shekhovtsov, Aleksandr},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {7803--7813},
}

@article{singh_freshdiskann_2021,
	title = {{FreshDiskANN}: {A} {Fast} and {Accurate} {Graph}-{Based} {ANN} {Index} for {Streaming} {Similarity} {Search}},
	shorttitle = {{FreshDiskANN}},
	url = {http://arxiv.org/abs/2105.09613},
	abstract = {Approximate nearest neighbor search (ANNS) is a fundamental building block in information retrieval with graph-based indices being the current state-of-the-art and widely used in the industry. Recent advances in graph-based indices have made it possible to index and search billion-point datasets with high recall and millisecond-level latency on a single commodity machine with an SSD. However, existing graph algorithms for ANNS support only static indices that cannot reflect real-time changes to the corpus required by many key real-world scenarios (e.g. index of sentences in documents, email, or a news index). To overcome this drawback, the current industry practice for manifesting updates into such indices is to periodically re-build these indices, which can be prohibitively expensive. In this paper, we present the first graph-based ANNS index that reflects corpus updates into the index in real-time without compromising on search performance. Using update rules for this index, we design FreshDiskANN, a system that can index over a billion points on a workstation with an SSD and limited memory, and support thousands of concurrent real-time inserts, deletes and searches per second each, while retaining \${\textgreater}95{\textbackslash}\%\$ 5-recall@5. This represents a 5-10x reduction in the cost of maintaining freshness in indices when compared to existing methods.},
	urldate = {2022-04-15},
	journal = {arXiv:2105.09613 [cs]},
	author = {Singh, Aditi and Subramanya, Suhas Jayaram and Krishnaswamy, Ravishankar and Simhadri, Harsha Vardhan},
	month = may,
	year = {2021},
	note = {arXiv: 2105.09613},
	keywords = {Computer Science - Information Retrieval, H.3.3},
}

@article{wang_comprehensive_2021,
	title = {A {Comprehensive} {Survey} and {Experimental} {Comparison} of {Graph}-{Based} {Approximate} {Nearest} {Neighbor} {Search}},
	url = {http://arxiv.org/abs/2101.12631},
	abstract = {Approximate nearest neighbor search (ANNS) constitutes an important operation in a multitude of applications, including recommendation systems, information retrieval, and pattern recognition. In the past decade, graph-based ANNS algorithms have been the leading paradigm in this domain, with dozens of graph-based ANNS algorithms proposed. Such algorithms aim to provide effective, efficient solutions for retrieving the nearest neighbors for a given query. Nevertheless, these efforts focus on developing and optimizing algorithms with different approaches, so there is a real need for a comprehensive survey about the approaches' relative performance, strengths, and pitfalls. Thus here we provide a thorough comparative analysis and experimental evaluation of 13 representative graph-based ANNS algorithms via a new taxonomy and fine-grained pipeline. We compared each algorithm in a uniform test environment on eight real-world datasets and 12 synthetic datasets with varying sizes and characteristics. Our study yields novel discoveries, offerings several useful principles to improve algorithms, thus designing an optimized method that outperforms the state-of-the-art algorithms. This effort also helped us pinpoint algorithms' working portions, along with rule-of-thumb recommendations about promising research directions and suitable algorithms for practitioners in different fields.},
	urldate = {2022-04-15},
	journal = {arXiv:2101.12631 [cs]},
	author = {Wang, Mengzhao and Xu, Xiaoliang and Yue, Qiang and Wang, Yuxiang},
	month = may,
	year = {2021},
	note = {arXiv: 2101.12631},
	keywords = {Computer Science - Databases, Computer Science - Information Retrieval},
}

@article{sidiropoulos_tensor_2017,
	title = {Tensor {Decomposition} for {Signal} {Processing} and {Machine} {Learning}},
	volume = {65},
	issn = {1941-0476},
	doi = {10.1109/TSP.2017.2690524},
	abstract = {Tensors or multiway arrays are functions of three or more indices (i, j, k, . . . )-similar to matrices (two-way arrays), which are functions of two indices (r, c) for (row, column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining, and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth and depth that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning.},
	number = {13},
	journal = {IEEE Transactions on Signal Processing},
	author = {Sidiropoulos, Nicholas D. and De Lathauwer, Lieven and Fu, Xiao and Huang, Kejun and Papalexakis, Evangelos E. and Faloutsos, Christos},
	month = jul,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Signal Processing},
	keywords = {Cramér–Rao bound, Gauss–Newton, Matrix decomposition, NP-hard problems, Optimization, Signal processing, Signal processing algorithms, Tensile stress, Tensor decomposition, Tucker model, Tutorials, alternating direction method of multipliers, alternating optimization, canonical polyadic decomposition (CPD), classification, collaborative filtering, communications, gradient descent, harmonic retrieval, higher-order singular value decomposition (HOSVD), mixture modeling, multilinear singular value decomposition (MLSVD), parallel factor analysis (PARAFAC), rank, source separation, speech separation, stochastic gradient, subspace learning, tensor factorization, topic modeling, uniqueness},
	pages = {3551--3582},
}

@article{ji_rank_2016,
	title = {A {Rank} {Revealing} {Randomized} {Singular} {Value} {Decomposition} ({R3SVD}) {Algorithm} for {Low}-rank {Matrix} {Approximations}},
	url = {http://arxiv.org/abs/1605.08134},
	abstract = {In this paper, we present a Rank Revealing Randomized Singular Value Decomposition (R3SVD) algorithm to incrementally construct a low-rank approximation of a potentially large matrix while adaptively estimating the appropriate rank that can capture most of the actions of the matrix. Starting from a low-rank approximation with an initial guessed rank, R3SVD adopts an orthogonal Gaussian sampling approach to obtain the dominant subspace within the leftover space, which is used to add up to the existing low-rank approximation. Orthogonal Gaussian sampling is repeated until an appropriate low-rank approximation with satisfactory accuracy, measured by the overall energy percentage of the original matrix, is obtained. While being a fast algorithm, R3SVD is also a memory-aware algorithm where the computational process can be decomposed into a series of sampling tasks that use constant amount of memory. Numerical examples in image compression and matrix completion are used to demonstrate the effectiveness of R3SVD in low-rank approximation.},
	urldate = {2022-04-01},
	journal = {arXiv:1605.08134 [cs]},
	author = {Ji, Hao and Yu, Wenjian and Li, Yaohang},
	month = may,
	year = {2016},
	note = {arXiv: 1605.08134},
	keywords = {Mathematics - Numerical Analysis},
}

@article{kolda_tensor_2009,
	title = {Tensor {Decompositions} and {Applications}},
	volume = {51},
	issn = {0036-1445},
	url = {https://epubs.siam.org/doi/10.1137/07070111X},
	doi = {10.1137/07070111X},
	abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N-way array. Decompositions of higher-order tensors (i.e., N-way arrays with \$N {\textbackslash}geq 3\$) have applications in psycho-metrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.},
	number = {3},
	urldate = {2022-04-01},
	journal = {SIAM Review},
	author = {Kolda, Tamara G. and Bader, Brett W.},
	month = aug,
	year = {2009},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {15A69, 65F99, canonical decomposition (CANDECOMP), higher-order principal components analysis (Tucker), higher-order singular value decomposition (HOSVD), multilinear algebra, multiway arrays, parallel factors (PARAFAC), tensor decompositions},
	pages = {455--500},
}

@article{panagakis_tensor_2021,
	title = {Tensor {Methods} in {Computer} {Vision} and {Deep} {Learning}},
	volume = {109},
	issn = {0018-9219, 1558-2256},
	url = {http://arxiv.org/abs/2107.03436},
	doi = {10.1109/JPROC.2021.3074329},
	abstract = {Tensors, or multidimensional arrays, are data structures that can naturally represent visual data of multiple dimensions. Inherently able to efficiently capture structured, latent semantic spaces and high-order interactions, tensors have a long history of applications in a wide span of computer vision problems. With the advent of the deep learning paradigm shift in computer vision, tensors have become even more fundamental. Indeed, essential ingredients in modern deep learning architectures, such as convolutions and attention mechanisms, can readily be considered as tensor mappings. In effect, tensor methods are increasingly finding significant applications in deep learning, including the design of memory and compute efficient network architectures, improving robustness to random noise and adversarial attacks, and aiding the theoretical understanding of deep networks. This article provides an in-depth and practical review of tensors and tensor methods in the context of representation learning and deep learning, with a particular focus on visual data analysis and computer vision applications. Concretely, besides fundamental work in tensor-based visual data analysis methods, we focus on recent developments that have brought on a gradual increase of tensor methods, especially in deep learning architectures, and their implications in computer vision applications. To further enable the newcomer to grasp such concepts quickly, we provide companion Python notebooks, covering key aspects of the paper and implementing them, step-by-step with TensorLy.},
	number = {5},
	urldate = {2022-04-01},
	journal = {Proceedings of the IEEE},
	author = {Panagakis, Yannis and Kossaifi, Jean and Chrysos, Grigorios G. and Oldfield, James and Nicolaou, Mihalis A. and Anandkumar, Anima and Zafeiriou, Stefanos},
	month = may,
	year = {2021},
	note = {arXiv: 2107.03436},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {863--890},
}

@article{zhou_decomposition_2014,
	title = {Decomposition of {Big} {Tensors} {With} {Low} {Multilinear} {Rank}},
	url = {http://arxiv.org/abs/1412.1885},
	abstract = {Tensor decompositions are promising tools for big data analytics as they bring multiple modes and aspects of data to a unified framework, which allows us to discover complex internal structures and correlations of data. Unfortunately most existing approaches are not designed to meet the major challenges posed by big data analytics. This paper attempts to improve the scalability of tensor decompositions and provides two contributions: A flexible and fast algorithm for the CP decomposition (FFCP) of tensors based on their Tucker compression; A distributed randomized Tucker decomposition approach for arbitrarily big tensors but with relatively low multilinear rank. These two algorithms can deal with huge tensors, even if they are dense. Extensive simulations provide empirical evidence of the validity and efficiency of the proposed algorithms.},
	urldate = {2022-04-01},
	journal = {arXiv:1412.1885 [cs]},
	author = {Zhou, Guoxu and Cichocki, Andrzej and Xie, Shengli},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.1885},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Mathematics - Numerical Analysis},
}

@article{lebedev_speeding-up_2015,
	title = {Speeding-up {Convolutional} {Neural} {Networks} {Using} {Fine}-tuned {CP}-{Decomposition}},
	url = {http://arxiv.org/abs/1412.6553},
	abstract = {We propose a simple two-step approach for speeding up convolution layers within large convolutional neural networks based on tensor decomposition and discriminative fine-tuning. Given a layer, we use non-linear least squares to compute a low-rank CP-decomposition of the 4D convolution kernel tensor into a sum of a small number of rank-one tensors. At the second step, this decomposition is used to replace the original convolutional layer with a sequence of four convolutional layers with small kernels. After such replacement, the entire network is fine-tuned on the training data using standard backpropagation process. We evaluate this approach on two CNNs and show that it is competitive with previous approaches, leading to higher obtained CPU speedups at the cost of lower accuracy drops for the smaller of the two networks. Thus, for the 36-class character classification CNN, our approach obtains a 8.5x CPU speedup of the whole network with only minor accuracy drop (1\% from 91\% to 90\%). For the standard ImageNet architecture (AlexNet), the approach speeds up the second convolution layer by a factor of 4x at the cost of \$1{\textbackslash}\%\$ increase of the overall top-5 classification error.},
	urldate = {2022-04-01},
	journal = {arXiv:1412.6553 [cs]},
	author = {Lebedev, Vadim and Ganin, Yaroslav and Rakhuba, Maksim and Oseledets, Ivan and Lempitsky, Victor},
	month = apr,
	year = {2015},
	note = {arXiv: 1412.6553},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{song_speeding_2020,
	address = {New York, NY, USA},
	series = {{ICMLT} 2020},
	title = {Speeding {Up} {Deep} {Convolutional} {Neural} {Networks} {Based} on {Tucker}-{CP} {Decomposition}},
	isbn = {978-1-4503-7764-5},
	url = {https://doi.org/10.1145/3409073.3409094},
	doi = {10.1145/3409073.3409094},
	abstract = {Convolutional neural networks (CNNs) have made great success in computer vision tasks. But the computational complexity of CNNs is huge, which makes CNNs run slowly especially when computational resources are limited. In this paper, we propose a scheme based on tensor decomposition to accelerate CNNs. Firstly, Tucker method is used to decompose the convolution kernel into a small core tensor with key information and two factor matrices reflecting the linear relationship in the third dimension and fourth dimension of the convolution kernel respectively. Then CP (CANDECOMP/PARAFAC) method is used to decompose the core tensor into several rank-1 tensors. This scheme can remove the linear redundancy in convolution kernels and greatly speed up CNNs while maintaining the high classification accuracy. The scheme is used to decompose all the convolutional layers in AlexNet, and the accelerated model is trained and tested on ImageNet. The results show that our scheme achieves a whole-model speedup of 4 x with merely a 1.9\% increase in top-5 error for AlexNet.},
	urldate = {2022-04-01},
	booktitle = {Proceedings of the 2020 5th {International} {Conference} on {Machine} {Learning} {Technologies}},
	publisher = {Association for Computing Machinery},
	author = {Song, Dechun and Zhang, Peiyong and Li, Feiteng},
	year = {2020},
	keywords = {CNN, CP decomposition, ImageNet, Tucker decomposition, neural network acceleration, tensor decomposition},
	pages = {56--61},
}

@article{de_lathauwer_multilinear_2000,
	title = {A {Multilinear} {Singular} {Value} {Decomposition}},
	volume = {21},
	issn = {0895-4798},
	url = {https://epubs.siam.org/doi/10.1137/S0895479896305696},
	doi = {10.1137/S0895479896305696},
	abstract = {We discuss a multilinear generalization of the singular value decomposition. There is a strong analogy between several properties of the matrix and the higher-order tensor decomposition; uniqueness, link with the matrix eigenvalue decomposition, first-order perturbation effects, etc., are analyzed. We investigate how tensor symmetries affect the decomposition and propose a multilinear generalization of the symmetric eigenvalue decomposition for pair-wise symmetric tensors.},
	number = {4},
	urldate = {2022-04-01},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {De Lathauwer, Lieven and De Moor, Bart and Vandewalle, Joos},
	month = jan,
	year = {2000},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {15A18, 15A69, higher-order tensor, multilinear algebra, singular value decomposition},
	pages = {1253--1278},
}

@article{eckart_approximation_1936,
	title = {The approximation of one matrix by another of lower rank},
	volume = {1},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/BF02288367},
	doi = {10.1007/BF02288367},
	abstract = {The mathematical problem of approximating one matrix by another of lower rank is closely related to the fundamental postulate of factor-theory. When formulated as a least-squares problem, the normal equations cannot be immediately written down, since the elements of the approximate matrix are not independent of one another. The solution of the problem is simplified by first expressing the matrices in a canonic form. It is found that the problem always has a solution which is usually unique. Several conclusions can be drawn from the form of this solution.},
	language = {en},
	number = {3},
	urldate = {2022-04-01},
	journal = {Psychometrika},
	author = {Eckart, Carl and Young, Gale},
	month = sep,
	year = {1936},
	pages = {211--218},
}

@article{kiers_towards_2000,
	title = {Towards a standardized notation and terminology in multiway analysis},
	volume = {14},
	issn = {1099-128X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/1099-128X%28200005/06%2914%3A3%3C105%3A%3AAID-CEM582%3E3.0.CO%3B2-I},
	doi = {10.1002/1099-128X(200005/06)14:3<105::AID-CEM582>3.0.CO;2-I},
	abstract = {This paper presents a standardized notation and terminology to be used for three- and multiway analyses, especially when these involve (variants of) the CANDECOMP/PARAFAC model and the Tucker model. The notation also deals with basic aspects such as symbols for different kinds of products, and terminology for three- and higher-way data. The choices for terminology and symbols to be used have to some extent been based on earlier (informal) conventions. Simplicity and reduction of the possibility of confusion have also played a role in the choices made. Copyright © 2000 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {3},
	urldate = {2022-04-01},
	journal = {Journal of Chemometrics},
	author = {Kiers, Henk A. L.},
	year = {2000},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/1099-128X\%28200005/06\%2914\%3A3\%3C105\%3A\%3AAID-CEM582\%3E3.0.CO\%3B2-I},
	keywords = {multiway data, tensors, three-way methods},
	pages = {105--122},
}

@article{tucker_mathematical_1966,
	title = {Some mathematical notes on three-mode factor analysis},
	volume = {31},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/BF02289464},
	doi = {10.1007/BF02289464},
	abstract = {The model for three-mode factor analysis is discussed in terms of newer applications of mathematical processes including a type of matrix process termed the Kronecker product and the definition of combination variables. Three methods of analysis to a type of extension of principal components analysis are discussed. Methods II and III are applicable to analysis of data collected for a large sample of individuals. An extension of the model is described in which allowance is made for unique variance for each combination variable when the data are collected for a large sample of individuals.},
	language = {en},
	number = {3},
	urldate = {2022-04-01},
	journal = {Psychometrika},
	author = {Tucker, Ledyard R.},
	month = sep,
	year = {1966},
	pages = {279--311},
}

@article{chen_tensorf_2022,
	title = {{TensoRF}: {Tensorial} {Radiance} {Fields}},
	shorttitle = {{TensoRF}},
	url = {http://arxiv.org/abs/2203.09517},
	abstract = {We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CP decomposition -- that factorizes tensors into rank-one components with compact vectors -- in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction ({\textless}30 min) with better rendering quality and even a smaller model size ({\textless}4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time ({\textless}10 min) and retaining a compact model size ({\textless}75 MB).},
	urldate = {2022-04-01},
	journal = {arXiv:2203.09517 [cs]},
	author = {Chen, Anpei and Xu, Zexiang and Geiger, Andreas and Yu, Jingyi and Su, Hao},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.09517},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{corporation_amd_nodate,
	title = {{AMD} {INSTINCT}™ {MI200} {SERIES} {ACCELERATOR}},
	url = {https://www.amd.com/system/files/documents/amd-instinct-mi200-datasheet.pdf},
	abstract = {The fastest data center platform for AI and HPC.},
	language = {en-us},
	urldate = {2021-06-06},
	journal = {NVIDIA},
	author = {Corporation, AMD},
}

@article{achlioptas_database-friendly_2003,
	series = {Special {Issue} on {PODS} 2001},
	title = {Database-friendly random projections: {Johnson}-{Lindenstrauss} with binary coins},
	volume = {66},
	issn = {0022-0000},
	shorttitle = {Database-friendly random projections},
	url = {https://www.sciencedirect.com/science/article/pii/S0022000003000254},
	doi = {10.1016/S0022-0000(03)00025-4},
	abstract = {A classic result of Johnson and Lindenstrauss asserts that any set of n points in d-dimensional Euclidean space can be embedded into k-dimensional Euclidean space—where k is logarithmic in n and independent of d—so that all pairwise distances are maintained within an arbitrarily small factor. All known constructions of such embeddings involve projecting the n points onto a spherically random k-dimensional hyperplane through the origin. We give two constructions of such embeddings with the property that all elements of the projection matrix belong in \{−1,0,+1\}. Such constructions are particularly well suited for database environments, as the computation of the embedding reduces to evaluating a single aggregate over k random partitions of the attributes.},
	language = {en},
	number = {4},
	urldate = {2022-03-01},
	journal = {Journal of Computer and System Sciences},
	author = {Achlioptas, Dimitris},
	year = {2003},
	pages = {671--687},
}

@inproceedings{ailon_fast_2008,
	address = {USA},
	series = {{SODA} '08},
	title = {Fast dimension reduction using {Rademacher} series on dual {BCH} codes},
	abstract = {The Fast Johnson-Lindenstrauss Transform (FJLT) was recently discovered by Ailon and Chazelle as a novel technique for performing fast dimension reduction with small distortion from ℓd2 to ℓd2 in time O(max\{d log d,k3\}). For k in [Ω(log d), O(d1/2)] this beats time O(dk) achieved by naive multiplication by random dense matrices, an approach followed by several authors as a variant of the seminal result by Johnson and Lindenstrauss (JL) from the mid 80's. In this work we show how to significantly improve the running time to O(d log k) for k = O(d1/2−δ), for any arbitrary small fixed δ. This beats the better of FJLT and JL. Our analysis uses a powerful measure concentration bound due to Talagrand applied to Rademacher series in Banach spaces (sums of vectors in Banach spaces with random signs). The set of vectors used is a real embedding of dual BCH code vectors over GF(2). We also discuss the number of random bits used and reduction to ℓ1 space. The connection between geometry and discrete coding theory discussed here is interesting in its own right and may be useful in other algorithmic applications as well.},
	urldate = {2022-03-01},
	booktitle = {Proceedings of the nineteenth annual {ACM}-{SIAM} symposium on {Discrete} algorithms},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Ailon, Nir and Liberty, Edo},
	year = {2008},
	pages = {1--9},
}

@inproceedings{li_very_2006,
	address = {New York, NY, USA},
	series = {{KDD} '06},
	title = {Very sparse random projections},
	isbn = {978-1-59593-339-3},
	url = {https://doi.org/10.1145/1150402.1150436},
	doi = {10.1145/1150402.1150436},
	abstract = {There has been considerable interest in random projections, an approximate algorithm for estimating distances between pairs of points in a high-dimensional vector space. Let A in Rn x D be our n points in D dimensions. The method multiplies A by a random matrix R in RD x k, reducing the D dimensions down to just k for speeding up the computation. R typically consists of entries of standard normal N(0,1). It is well known that random projections preserve pairwise distances (in the expectation). Achlioptas proposed sparse random projections by replacing the N(0,1) entries in R with entries in -1,0,1 with probabilities 1/6, 2/3, 1/6, achieving a threefold speedup in processing time.We recommend using R of entries in -1,0,1 with probabilities 1/2√D, 1-1√D, 1/2√D for achieving a significant √D-fold speedup, with little loss in accuracy.},
	urldate = {2022-03-01},
	booktitle = {Proceedings of the 12th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Li, Ping and Hastie, Trevor J. and Church, Kenneth W.},
	year = {2006},
	keywords = {random projections, rates of convergence, sampling},
	pages = {287--296},
}

@article{carson_accelerating_2018,
	title = {Accelerating the {Solution} of {Linear} {Systems} by {Iterative} {Refinement} in {Three} {Precisions}},
	volume = {40},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/10.1137/17M1140819},
	doi = {10.1137/17M1140819},
	abstract = {We propose a general algorithm for solving an \$n{\textbackslash}times n\$ nonsingular linear system \$Ax = b\$ based on iterative refinement with three precisions. The working precision is combined with possibly different precisions for solving for the correction term and for computing the residuals. Via rounding error analysis of the algorithm we derive sufficient conditions for convergence and bounds for the attainable forward error and normwise and componentwise backward errors. Our results generalize and unify many existing rounding error analyses for iterative refinement. With single precision as the working precision, we show that by using LU factorization in IEEE half precision as the solver and calculating the residuals in double precision it is possible to solve \$Ax = b\$ to full single precision accuracy for \${\textbackslash}infty\$-norm condition numbers \${\textbackslash}kappa\_\{{\textbackslash}infty\}(A) {\textbackslash}le 10{\textasciicircum}4\$, with the \$O(n{\textasciicircum}3)\$ part of the computations carried out entirely in half precision. We show further that by solving the correction equations by GMRES preconditioned by the LU factors the restriction on the condition number can be weakened to \${\textbackslash}kappa\_\{{\textbackslash}infty\}(A) {\textbackslash}le 10{\textasciicircum}8\$, although in general there is no guarantee that GMRES will converge quickly. Taking for comparison a standard \$Ax = b\$ solver that uses LU factorization in single precision, these results suggest that on architectures for which half precision is efficiently implemented it will be possible to solve certain linear systems \$Ax = b\$ up to twice as fast and to greater accuracy. Analogous results are given with double precision as the working precision.},
	number = {2},
	urldate = {2022-02-26},
	journal = {SIAM Journal on Scientific Computing},
	author = {Carson, Erin and Higham, Nicholas J.},
	month = jan,
	year = {2018},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {65F10, 65G50, GMRES, backward error, forward error, iterative refinement, linear system, mixed precision, multiple precision, preconditioning, rounding error analysis},
	pages = {A817--A847},
}

@article{higham_squeezing_2019,
	title = {Squeezing a {Matrix} into {Half} {Precision}, with an {Application} to {Solving} {Linear} {Systems}},
	volume = {41},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/10.1137/18M1229511},
	doi = {10.1137/18M1229511},
	abstract = {Motivated by the demand in machine learning, modern computer hardware is increasingly supporting reduced precision floating-point arithmetic, which provides advantages in speed, energy, and memory usage over single and double precision. Given the availability of such hardware, mixed precision algorithms that work in single or double precision but carry out part of a computation in half precision are now of great interest for general scientific computing tasks. Because of the limited range of half precision arithmetic, in which positive numbers lie between \$6{\textbackslash}times 10{\textasciicircum}\{-8\}\$ and \$7{\textbackslash}times 10{\textasciicircum}4\$, a straightforward rounding of single or double precision data into half precision can lead to overflow, underflow, or subnormal numbers being generated, all of which are undesirable. We develop an algorithm for converting a matrix from single or double precision to half precision. It first applies two-sided diagonal scaling in order to equilibrate the matrix (that is, to ensure that every row and column has \${\textbackslash}infty\$-norm \$1\$), then multiplies by a scalar to bring the largest element within a factor \${\textbackslash}theta {\textbackslash}le 1\$ of the overflow level, and finally rounds to half precision. The second step ensures that full use is made of the limited range of half precision arithmetic, and \${\textbackslash}theta\$ must be chosen to allow sufficient headroom for subsequent computations. We apply the new algorithm to GMRES-based iterative refinement (GMRES-IR), which solves a linear system \$Ax = b\$ with single or double precision data by LU factorizing \$A\$ in half precision and carrying out iterative refinement with the correction equations solved by GMRES preconditioned with the low precision LU factors. Previous implementations of this algorithm have used a crude conversion to half precision that our experiments show can cause slow convergence of GMRES-IR for badly scaled matrices or failure to converge at all. The new conversion algorithm computes \${\textbackslash}infty\$-norms of rows and columns of the matrix and its cost is negligible in the context of LU factorization. We show that it leads to faster convergence of GMRES-IR for badly scaled matrices and thereby allows a much wider class of problems to be solved.},
	number = {4},
	urldate = {2022-02-16},
	journal = {SIAM Journal on Scientific Computing},
	author = {Higham, Nicholas J. and Pranesh, Srikara and Zounon, Mawussi},
	month = jan,
	year = {2019},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {65F05, 65F08, 65F10, 65F35, GMRES, diagonal scaling, fp16, half precision arithmetic, iterative refinement, linear system, mixed precision, overflow, preconditioning, subnormal numbers, underflow},
	pages = {A2536--A2551},
}

@book{higham_accuracy_2002,
	address = {USA},
	edition = {2nd},
	title = {Accuracy and {Stability} of {Numerical} {Algorithms}},
	isbn = {978-0-89871-521-7},
	abstract = {From the Publisher: What is the most accurate way to sum floating point numbers\_\_ \_\_ What are the advantages of IEEE arithmetic\_\_ \_\_ How accurate is Gaussian elimination and what were the key breakthroughs in the development of error analysis for the method\_\_ \_\_ The answers to these and many related questions are included here. This book gives a thorough, up-to-date treatment of the behavior of numerical algorithms in finite precision arithmetic. It combines algorithmic derivations, perturbation theory, and rounding error analysis. Software practicalities are emphasized throughout, with particular reference to LAPACK and MATLAB. The best available error bounds, some of them new, are presented in a unified format with a minimum of jargon. Because of its central role in revealing problem sensitivity and providing error bounds, perturbation theory is treated in detail. Historical perspective and insight are given, with particular reference to the fundamental work of Wilkinson and Turing, and the many quotations provide further information in an accessible format. The book is unique in that algorithmic developments and motivations are given succinctly and implementation details minimized, so that attention can be concentrated on accuracy and stability results. Here, in one place and in a unified notation, is error analysis for most of the standard algorithms in matrix computations. Not since Wilkinson's Rounding Errors in Algebraic Processes (1963) and The Algebraic Eigenvalue Problem (1965) has any volume treated this subject in such depth. A number of topics are treated that are not usually covered in numerical analysis textbooks, including floating point summation, block LU factorization, condition number estimation, the Sylvester equation, powers of matrices, finite precision behavior of stationary iterative methods, Vandermonde systems, and fast matrix multiplication. Although not designed specifically as a textbook, this volume is a suitable reference for an advanced course, and could be used by instructors at all levels as a supplementary text from which to draw examples, historical perspective, statements of results, and exercises (many of which have never before appeared in textbooks). The book is designed to be a comprehensive reference and its bibliography contains more than 1100 references from the research literature. Audience Specialists in numerical analysis as well as computational scientists and engineers concerned about the accuracy of their results will benefit from this book. Much of the book can be understood with only a basic grounding in numerical analysis and linear algebra. About the Author Nicholas J. Higham is a Professor of Applied Mathematics at the University of Manchester, England. He is the author of more than 40 publications and is a member of the editorial boards of the SIAM Journal on Matrix Analysis and Applications and the IMA Journal of Numerical Analysis. His book Handbook of Writing for the Mathematical Sciences was published by SIAM in 1993.},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Higham, Nicholas J.},
	year = {2002},
}

@article{woodruff_sketching_2014,
	title = {Sketching as a {Tool} for {Numerical} {Linear} {Algebra}},
	url = {https://arxiv.org/abs/1411.4357v3},
	doi = {10.1561/0400000060},
	abstract = {This survey highlights the recent advances in algorithms for numerical linear algebra that have come from the technique of linear sketching, whereby given a matrix, one first compresses it to a much smaller matrix by multiplying it by a (usually) random matrix with certain properties. Much of the expensive computation can then be performed on the smaller matrix, thereby accelerating the solution for the original problem. In this survey we consider least squares as well as robust regression problems, low rank approximation, and graph sparsification. We also discuss a number of variants of these problems. Finally, we discuss the limitations of sketching methods.},
	language = {en},
	urldate = {2022-01-09},
	author = {Woodruff, David P.},
	month = nov,
	year = {2014},
}

@article{halko_finding_2011,
	title = {Finding {Structure} with {Randomness}: {Probabilistic} {Algorithms} for {Constructing} {Approximate} {Matrix} {Decompositions}},
	volume = {53},
	issn = {0036-1445},
	shorttitle = {Finding {Structure} with {Randomness}},
	url = {https://epubs.siam.org/doi/10.1137/090771806},
	doi = {10.1137/090771806},
	abstract = {Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed—either explicitly or implicitly—to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, robustness, and/or speed. These claims are supported by extensive numerical experiments and a detailed error analysis. The specific benefits of randomized techniques depend on the computational environment. Consider the model problem of finding the k dominant components of the singular value decomposition of an \$m {\textbackslash}times n\$ matrix. (i) For a dense input matrix, randomized algorithms require \${\textbackslash}bigO(mn {\textbackslash}log(k))\$ floating-point operations (flops) in contrast to \$ {\textbackslash}bigO(mnk)\$ for classical algorithms. (ii) For a sparse input matrix, the flop count matches classical Krylov subspace methods, but the randomized approach is more robust and can easily be reorganized to exploit multiprocessor architectures. (iii) For a matrix that is too large to fit in fast memory, the randomized techniques require only a constant number of passes over the data, as opposed to \${\textbackslash}bigO(k)\$ passes for classical algorithms. In fact, it is sometimes possible to perform matrix approximation with a single pass over the data.},
	number = {2},
	urldate = {2022-01-09},
	journal = {SIAM Review},
	author = {Halko, N. and Martinsson, P. G. and Tropp, J. A.},
	year = {2011},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {60B20, 65F30, 68W20, Johnson–Lindenstrauss lemma, dimension reduction, eigenvalue decomposition, interpolative decomposition, matrix approximation, parallel algorithm, pass-efficient algorithm, principal component analysis, random matrix, randomized algorithm, rank-revealing QR factorization, singular value decomposition, streaming algorithm},
	pages = {217--288},
}

@article{liberty_randomized_2007,
	title = {Randomized algorithms for the low-rank approximation of matrices},
	volume = {104},
	copyright = {© 2007 by The National Academy of Sciences of the USA},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/104/51/20167},
	doi = {10.1073/pnas.0709640104},
	abstract = {We describe two recently proposed randomized algorithms for the construction of low-rank approximations to matrices, and demonstrate their application (inter alia) to the evaluation of the singular value decompositions of numerically low-rank matrices. Being probabilistic, the schemes described here have a finite probability of failure; in most cases, this probability is rather negligible (10−17 is a typical value). In many situations, the new procedures are considerably more efficient and reliable than the classical (deterministic) ones; they also parallelize naturally. We present several numerical examples to illustrate the performance of the schemes.},
	language = {en},
	number = {51},
	urldate = {2022-01-09},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Liberty, Edo and Woolfe, Franco and Martinsson, Per-Gunnar and Rokhlin, Vladimir and Tygert, Mark},
	month = dec,
	year = {2007},
	pmid = {18056803},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	keywords = {PCA, SVD, matrix},
	pages = {20167--20172},
}

@article{woolfe_fast_2008,
	title = {A fast randomized algorithm for the approximation of matrices},
	volume = {25},
	issn = {1063-5203},
	url = {https://www.sciencedirect.com/science/article/pii/S1063520307001364},
	doi = {10.1016/j.acha.2007.12.002},
	abstract = {We introduce a randomized procedure that, given an m×n matrix A and a positive integer k, approximates A with a matrix Z of rank k. The algorithm relies on applying a structured l×m random matrix R to each column of A, where l is an integer near to, but greater than, k. The structure of R allows us to apply it to an arbitrary m×1 vector at a cost proportional to mlog(l); the resulting procedure can construct a rank-k approximation Z from the entries of A at a cost proportional to mnlog(k)+l2(m+n). We prove several bounds on the accuracy of the algorithm; one such bound guarantees that the spectral norm ‖A−Z‖ of the discrepancy between A and Z is of the same order as max\{m,n\} times the (k+1)st greatest singular value σk+1 of A, with small probability of large deviations. In contrast, the classical pivoted “QR” decomposition algorithms (such as Gram–Schmidt or Householder) require at least kmn floating-point operations in order to compute a similarly accurate rank-k approximation. In practice, the algorithm of this paper runs faster than the classical algorithms, even when k is quite small or large. Furthermore, the algorithm operates reliably independently of the structure of the matrix A, can access each column of A independently and at most twice, and parallelizes naturally. Thus, the algorithm provides an efficient, reliable means for computing several of the greatest singular values and corresponding singular vectors of A. The results are illustrated via several numerical examples.},
	language = {en},
	number = {3},
	urldate = {2022-01-09},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Woolfe, Franco and Liberty, Edo and Rokhlin, Vladimir and Tygert, Mark},
	year = {2008},
	keywords = {Algorithm, Fast, Lanczos, Low rank, Matrix, QR, Randomized, SVD},
	pages = {335--366},
}

@article{shabat_randomized_2018,
	title = {Randomized {LU} decomposition},
	volume = {44},
	issn = {1063-5203},
	url = {https://www.sciencedirect.com/science/article/pii/S1063520316300069},
	doi = {10.1016/j.acha.2016.04.006},
	abstract = {Randomized algorithms play a central role in low rank approximations of large matrices. In this paper, the scheme of the randomized SVD is extended to a randomized LU algorithm. Several error bounds are introduced, that are based on recent results from random matrix theory related to subgaussian matrices. The bounds also improve the existing bounds of already known randomized SVD algorithm. The algorithm is fully parallelized and thus can utilize efficiently GPUs without any CPU–GPU data transfer. Numerical examples, which illustrate the performance of the algorithm and compare it to other decomposition methods, are presented.},
	language = {en},
	number = {2},
	urldate = {2022-01-09},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Shabat, Gil and Shmueli, Yaniv and Aizenbud, Yariv and Averbuch, Amir},
	year = {2018},
	keywords = {LU decomposition, Matrix factorizations, Random matrices, Randomized algorithms},
	pages = {246--272},
}

@article{wu_note_2019,
	title = {A {Note} on {Random} {Sampling} for {Matrix} {Multiplication}},
	url = {http://arxiv.org/abs/1811.11237},
	abstract = {This paper extends the framework of randomised matrix multiplication to a coarser partition and proposes an algorithm as a complement to the classical algorithm, especially when the optimal probability distribution of the latter one is closed to uniform. The new algorithm increases the likelihood of getting a small approximation error in 2-norm and has the squared approximation error in Frobenious norm bounded by that from the classical algorithm.},
	urldate = {2022-01-09},
	journal = {arXiv:1811.11237 [cs, math]},
	author = {Wu, Yue},
	month = may,
	year = {2019},
	note = {arXiv: 1811.11237},
	keywords = {65C05, 68W20, 65C50, 62P30, Computer Science - Data Structures and Algorithms, Mathematics - Numerical Analysis},
}

@article{iyer_randomized_2019,
	title = {A randomized least squares solver for terabyte-sized dense overdetermined systems},
	volume = {36},
	issn = {1877-7503},
	url = {https://www.sciencedirect.com/science/article/pii/S1877750316301508},
	doi = {10.1016/j.jocs.2016.09.007},
	abstract = {We present a fast randomized least-squares solver for distributed-memory platforms. Our solver is based on the Blendenpik algorithm, but employs multiple random projection schemes to construct a sketch of the input matrix. These random projection sketching schemes, and in particular the use of the randomized Discrete Cosine Transform, enable our algorithm to scale the distributed memory vanilla implementation of Blendenpik to terabyte-sized matrices and provide up to ×7.5 speedup over a state-of-the-art scalable least-squares solver based on the classic QR algorithm. Experimental evaluations on terabyte scale matrices demonstrate excellent speedups on up to 16,384 cores on a Blue Gene/Q supercomputer.},
	language = {en},
	urldate = {2022-01-09},
	journal = {Journal of Computational Science},
	author = {Iyer, Chander and Avron, Haim and Kollias, Georgios and Ineichen, Yves and Carothers, Christopher and Drineas, Petros},
	year = {2019},
	keywords = {Dense least squares regression, High-performance computing, Randomized numerical linear algebra},
	pages = {100547},
}

@article{che_randomized_2021,
	title = {Randomized algorithms for the low multilinear rank approximations of tensors},
	volume = {390},
	issn = {0377-0427},
	url = {https://www.sciencedirect.com/science/article/pii/S0377042720306713},
	doi = {10.1016/j.cam.2020.113380},
	abstract = {In this paper, we focus on developing randomized algorithms for the computation of low multilinear rank approximations of tensors based on the random projection and the singular value decomposition. Following the theory of the singular values of sub-Gaussian matrices, we make a probabilistic analysis for the error bounds for the randomized algorithm. We demonstrate the effectiveness of proposed algorithms via several numerical examples.},
	language = {en},
	urldate = {2022-01-09},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Che, Maolin and Wei, Yimin and Yan, Hong},
	year = {2021},
	keywords = {Low multilinear rank approximation, Randomized algorithms, Singular value decomposition, Singular values, Sub-Gaussian matrices},
	pages = {113380},
}

@article{chen_integrating_2016,
	title = {Integrating multiple random sketches for singular value decomposition},
	url = {http://arxiv.org/abs/1608.08285},
	abstract = {The singular value decomposition (SVD) of large-scale matrices is a key tool in data analytics and scientific computing. The rapid growth in the size of matrices further increases the need for developing efficient large-scale SVD algorithms. Randomized SVD based on one-time sketching has been studied, and its potential has been demonstrated for computing a low-rank SVD. Instead of exploring different single random sketching techniques, we propose a Monte Carlo type integrated SVD algorithm based on multiple random sketches. The proposed integration algorithm takes multiple random sketches and then integrates the results obtained from the multiple sketched subspaces. So that the integrated SVD can achieve higher accuracy and lower stochastic variations. The main component of the integration is an optimization problem with a matrix Stiefel manifold constraint. The optimization problem is solved using Kolmogorov-Nagumo-type averages. Our theoretical analyses show that the singular vectors can be induced by population averaging and ensure the consistencies between the computed and true subspaces and singular vectors. Statistical analysis further proves a strong Law of Large Numbers and gives a rate of convergence by the Central Limit Theorem. Preliminary numerical results suggest that the proposed integrated SVD algorithm is promising.},
	urldate = {2022-01-09},
	journal = {arXiv:1608.08285 [math, stat]},
	author = {Chen, Ting-Li and Chang, Dawei D. and Huang, Su-Yun and Chen, Hung and Lin, Chienyao and Wang, Weichung},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.08285},
	keywords = {Mathematics - Numerical Analysis, Mathematics - Statistics Theory},
}

@inproceedings{fox_random_2016,
	title = {Random projections for scaling machine learning on {FPGAs}},
	doi = {10.1109/FPT.2016.7929193},
	abstract = {Random projections have recently emerged as a powerful technique for large scale dimensionality reduction in machine learning applications. Crucially, the projection can be obtained from sparse probability distributions, enabling hardware implementations with little overhead. In this paper, we describe a Field-Programmable Gate Array (FPGA) implementation alongside a kernel adaptive filter (KAF) that is capable of reducing computational resources by introducing a controlled error term, achieving higher modelling capacity for given hardware resources. Empirical results involving classification, regression and novelty detection show that a 40\% net increase in available resources and improvements in prediction accuracy is achievable for projections which halve the input vector length, enabling us to scale-up hardware implementations of KAF learning algorithms by at least a factor of 2. An implementation on a FPGA-based network card allows novelty detection of an 8× 24-bit input vector with latency of 404 ns, this being a 26-fold reduction compared to an Intel Core i5-2400 processor.},
	booktitle = {2016 {International} {Conference} on {Field}-{Programmable} {Technology} ({FPT})},
	author = {Fox, Sean and Tridgell, Stephen and Jin, Craig and Leong, Philip H.W.},
	month = dec,
	year = {2016},
	keywords = {Dictionaries, Field programmable gate arrays, Hardware, Kernel, Mathematical model, Prediction algorithms, Principal component analysis},
	pages = {85--92},
}

@article{ahmadi-asl_randomized_2021,
	title = {Randomized {Algorithms} for {Computation} of {Tucker} {Decomposition} and {Higher} {Order} {SVD} ({HOSVD})},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3058103},
	abstract = {Big data analysis has become a crucial part of new emerging technologies such as the internet of things, cyber-physical analysis, deep learning, anomaly detection, etc. Among many other techniques, dimensionality reduction plays a key role in such analyses and facilitates feature selection and feature extraction. Randomized algorithms are efficient tools for handling big data tensors. They accelerate decomposing large-scale data tensors by reducing the computational complexity of deterministic algorithms and the communication among different levels of memory hierarchy, which is the main bottleneck in modern computing environments and architectures. In this article, we review recent advances in randomization for computation of Tucker decomposition and Higher Order SVD (HOSVD). We discuss random projection and sampling approaches, single-pass and multi-pass randomized algorithms and how to utilize them in the computation of the Tucker decomposition and the HOSVD. Simulations on synthetic and real datasets are provided to compare the performance of some of best and most promising algorithms.},
	journal = {IEEE Access},
	author = {Ahmadi-Asl, Salman and Abukhovich, Stanislav and Asante-Mensah, Maame G. and Cichocki, Andrzej and Phan, Anh Huy and Tanaka, Tohishisa and Oseledets, Ivan},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Approximation algorithms, HOSVD, Licenses, Matrix decomposition, Memory management, Probability distribution, Randomized algorithm, Signal processing algorithms, Tensors, Tucker decomposition, random projection, sampling, tensor decomposition, unfolding},
	pages = {28684--28706},
}

@inproceedings{nguyen_fast_2016,
	title = {Fast tensor decompositions for big data processing},
	doi = {10.1109/ATC.2016.7764776},
	abstract = {Tensors, as a natural extension of matrices, and their decompositions provide important tools in many disciplines such as psychometrics, signal processing, data communication, computer vision, and machine learning. The main objective of this paper is to briefly review several recent state-of-the-art approaches for large-scale tensor data which is a crucial part of big data. Moreover, we also introduce our own contributions on this topic.},
	booktitle = {2016 {International} {Conference} on {Advanced} {Technologies} for {Communications} ({ATC})},
	author = {Nguyen, Viet-Dung and Abed-Meraim, Karim and Linh-Trung, Nguyen},
	month = oct,
	year = {2016},
	note = {ISSN: 2162-1039},
	keywords = {Adaptive decomposition, Arrays, Batch decomposition, Big data, Data analysis, Large-scale processing, Matrix decomposition, PARAFAC/CANDECOMP, Signal processing, Tensile stress, Tensor decomposition, Time-frequency analysis, Tucker},
	pages = {215--221},
}

@article{drineas_low-rank_2019,
	title = {Low-{Rank} {Matrix} {Approximations} {Do} {Not} {Need} a {Singular} {Value} {Gap}},
	volume = {40},
	issn = {0895-4798},
	url = {https://epubs.siam.org/doi/abs/10.1137/18M1163658},
	doi = {10.1137/18M1163658},
	abstract = {Low-rank approximations to a real matrix \${\textbackslash}mathbf\{A\}\$ can be computed from \${\textbackslash}mathbf\{Z\}{\textbackslash}mathbf\{Z\}{\textasciicircum}T{\textbackslash}mathbf\{A\}\$, where \${\textbackslash}mathbf\{Z\}\$ is a matrix with orthonormal columns, and the accuracy of the approximation can be estimated from some norm of \${\textbackslash}mathbf\{A\}-{\textbackslash}mathbf\{Z\}{\textbackslash}mathbf\{Z\}{\textasciicircum}T{\textbackslash}mathbf\{A\}\$. We show that computing \${\textbackslash}mathbf\{A\}-{\textbackslash}mathbf\{Z\}{\textbackslash}mathbf\{Z\}{\textasciicircum}T{\textbackslash}mathbf\{A\}\$ in the two-norm, Frobenius norms, and more generally any Schatten \$p\$-norm is a well-posed mathematical problem; and, in contrast to dominant subspace computations, it does not require a singular value gap. We also show that this problem is well-conditioned (insensitive) to additive perturbations in \${\textbackslash}mathbf\{A\}\$ and \${\textbackslash}mathbf\{Z\}\$, and to dimension-changing or multiplicative perturbations in \${\textbackslash}mathbf\{A\}\$---regardless of the accuracy of the approximation. For the special case when \${\textbackslash}mathbf\{A\}\$ does indeed have a singular values gap, connections are established between low-rank approximations and subspace angles.},
	number = {1},
	urldate = {2022-01-09},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Drineas, Petros and Ipsen, Ilse C. F.},
	year = {2019},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {15A12, 15A18, 15A42, 65F15, 65F35, additive perturbations, multiplicative perturbations, principal angles, singular value decomposition},
	pages = {299--319},
}

@inproceedings{feng_fast_2018,
	title = {Fast {Randomized} {PCA} for {Sparse} {Data}},
	url = {https://proceedings.mlr.press/v95/feng18a.html},
	abstract = {Principal component analysis (PCA) is widely used for dimension reduction and embedding of real data in social network analysis, information retrieval, and natural language processing, etc. In this work we propose a fast randomized PCA algorithm for processing large sparse data. The algorithm has similar accuracy to the basic randomized SVD (rPCA) algorithm  (Halko et al., 2011), but is largely optimized for sparse data. It also has good flexibility to trade off runtime against accuracy for practical usage. Experiments on real data show that the proposed algorithm is up to 9.1X faster than the basic rPCA algorithm without accuracy loss, and is up to 20X faster than the {\textbackslash}texttt\{svds\} in Matlab with little error. The algorithm computes the first 100 principal components of a large information retrieval data with 12,869,521 persons and 323,899 keywords in less than 400 seconds on a 24-core machine, while all conventional methods fail due to the out-of-memory issue.},
	language = {en},
	urldate = {2022-01-09},
	booktitle = {Proceedings of {The} 10th {Asian} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Feng, Xu and Xie, Yuyang and Song, Mingye and Yu, Wenjian and Tang, Jie},
	month = nov,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {710--725},
}

@inproceedings{feng_faster_2018,
	title = {Faster {Matrix} {Completion} {Using} {Randomized} {SVD}},
	doi = {10.1109/ICTAI.2018.00098},
	abstract = {Matrix completion is a widely used technique for image inpainting and personalized recommender system, etc. In this work, we focus on accelerating the matrix completion using faster randomized singular value decomposition (rSVD). Firstly, two fast randomized algorithms (rSVD-PI and rSVDBKI) are proposed for handling sparse matrix. They make use of an eigSVD procedure and several accelerating skills. Then, with the rSVD-BKI algorithm and a new subspace recycling technique, we accelerate the singular value thresholding (SVT) method in [1] to realize faster matrix completion. Experiments show that the proposed rSVD algorithms can be 6× faster than the basic rSVD algorithm [2] while keeping same accuracy. For image inpainting and movie-rating estimation problems (including up to 2 × 107 ratings), the proposed accelerated SVT algorithm consumes 15× and 8× less CPU time than the methods using svds and lansvd respectively, without loss of accuracy.},
	booktitle = {2018 {IEEE} 30th {International} {Conference} on {Tools} with {Artificial} {Intelligence} ({ICTAI})},
	author = {Feng, Xu and Yu, Wenjian and Li, Yaohang},
	month = nov,
	year = {2018},
	note = {ISSN: 2375-0197},
	keywords = {Acceleration, Approximation algorithms, Computer science, Matlab, Matrix decomposition, Principal component analysis, Sparse matrices, image inpainting, matrix completion, randomized SVD, recommender system},
	pages = {608--615},
}

@article{minster_randomized_2020,
	title = {Randomized {Algorithms} for {Low}-{Rank} {Tensor} {Decompositions} in the {Tucker} {Format}},
	volume = {2},
	url = {https://epubs.siam.org/doi/abs/10.1137/19M1261043},
	doi = {10.1137/19M1261043},
	abstract = {Many applications in data science and scientific computing involve large-scale datasets that are expensive to store and manipulate. However, these datasets possess inherent multidimensional structure that can be exploited to compress and store the dataset in an appropriate tensor format. In recent years, randomized matrix methods have been used to efficiently and accurately compute low-rank matrix decompositions. Motivated by this success, we develop randomized algorithms for tensor decompositions in the Tucker representation. Specifically, we present randomized versions of two well-known compression algorithms, namely, HOSVD and STHOSVD, and a detailed probabilistic analysis of the error in using both algorithms. We also develop variants of these algorithms that tackle specific challenges posed by large-scale datasets. The first variant adaptively finds a low-rank representation satisfying a given tolerance, and it is beneficial when the target rank is not known in advance. The second variant preserves the structure of the original tensor and is beneficial for large sparse tensors that are difficult to load in memory. We consider several different datasets for our numerical experiments: synthetic test tensors and realistic applications such as the compression of facial image samples in the Olivetti database and word counts in the Enron email dataset.},
	number = {1},
	urldate = {2022-01-09},
	journal = {SIAM Journal on Mathematics of Data Science},
	author = {Minster, Rachel and Saibaba, Arvind K. and Kilmer, Misha E.},
	year = {2020},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {15A18, 15A69, 15B52, 65F15, 65F99, 68W20, Tucker decompositions, low-rank, multilinear algebra, randomized algorithms, structure-preserving, tensors},
	pages = {189--215},
}

@article{yu_single-pass_2017,
	title = {Single-{Pass} {PCA} of {Large} {High}-{Dimensional} {Data}},
	url = {http://arxiv.org/abs/1704.07669},
	abstract = {Principal component analysis (PCA) is a fundamental dimension reduction tool in statistics and machine learning. For large and high-dimensional data, computing the PCA (i.e., the singular vectors corresponding to a number of dominant singular values of the data matrix) becomes a challenging task. In this work, a single-pass randomized algorithm is proposed to compute PCA with only one pass over the data. It is suitable for processing extremely large and high-dimensional data stored in slow memory (hard disk) or the data generated in a streaming fashion. Experiments with synthetic and real data validate the algorithm's accuracy, which has orders of magnitude smaller error than an existing single-pass algorithm. For a set of high-dimensional data stored as a 150 GB file, the proposed algorithm is able to compute the first 50 principal components in just 24 minutes on a typical 24-core computer, with less than 1 GB memory cost.},
	urldate = {2022-01-09},
	journal = {arXiv:1704.07669 [cs, math]},
	author = {Yu, Wenjian and Gu, Yu and Li, Jian and Liu, Shenghua and Li, Yaohang},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.07669},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Mathematics - Numerical Analysis},
}

@article{yu_efficient_2018,
	title = {Efficient {Randomized} {Algorithms} for the {Fixed}-{Precision} {Low}-{Rank} {Matrix} {Approximation}},
	volume = {39},
	issn = {0895-4798},
	url = {https://epubs.siam.org/doi/abs/10.1137/17M1141977},
	doi = {10.1137/17M1141977},
	abstract = {Randomized algorithms for low-rank matrix approximation are investigated, with the emphasis on the fixed-precision problem and computational efficiency for handling large matrices. The algorithms are based on the so-called QB factorization, where Q is an orthonormal matrix. First, a mechanism for calculating the approximation error in the Frobenius norm is proposed, which enables efficient adaptive rank determination for a large and/or sparse matrix. It can be combined with any QB-form factorization algorithm in which B's rows are incrementally generated. Based on the blocked randQB algorithm by Martinsson and Voronin, this results in an algorithm called randQB\_EI. Then, we further revise the algorithm to obtain a pass-efficient algorithm, randQB\_FP, which is mathematically equivalent to the existing randQB algorithms and also suitable for the fixed-precision problem. Especially, randQB\_FP can serve as a single-pass algorithm for calculating leading singular values, under a certain condition. With large and/or sparse test matrices, we have empirically validated the merits of the proposed techniques, which exhibit remarkable speedup and memory saving over the blocked randQB algorithm. We have also demonstrated that the single-pass algorithm derived by randQB\_FP is much more accurate than an existing single-pass algorithm. And with data from a scenic image and an information retrieval application, we have shown the advantages of the proposed algorithms over the adaptive range finder algorithm for solving the fixed-precision problem.},
	number = {3},
	urldate = {2022-01-09},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Yu, Wenjian and Gu, Yu and Li, Yaohang},
	year = {2018},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {15A18, 60B20, 65F15, 65F30, 68W20, adaptive rank determination, fixed-precision problem, low-rank matrix approximation, pass-efficient algorithm, randomized algorithm},
	pages = {1339--1359},
}

@article{erichson_randomized_2020,
	title = {Randomized {CP} tensor decomposition},
	volume = {1},
	issn = {2632-2153},
	url = {https://doi.org/10.1088/2632-2153/ab8240},
	doi = {10.1088/2632-2153/ab8240},
	abstract = {The CANDECOMP/PARAFAC (CP) tensor decomposition is a popular dimensionality-reduction method for multiway data. Dimensionality reduction is often sought after since many high-dimensional tensors have low intrinsic rank relative to the dimension of the ambient measurement space. However, the emergence of ‘big data’ poses significant computational challenges for computing this fundamental tensor decomposition. By leveraging modern randomized algorithms, we demonstrate that coherent structures can be learned from a smaller representation of the tensor in a fraction of the time. Thus, this simple but powerful algorithm enables one to compute the approximate CP decomposition even for massive tensors. The approximation error can thereby be controlled via oversampling and the computation of power iterations. In addition to theoretical results, several empirical results demonstrate the performance of the proposed algorithm.},
	language = {en},
	number = {2},
	urldate = {2022-01-09},
	journal = {Machine Learning: Science and Technology},
	author = {Erichson, N. Benjamin and Manohar, Krithika and Brunton, Steven L. and Kutz, J. Nathan},
	month = may,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {025012},
}

@article{erichson_randomized_2018,
	title = {Randomized nonnegative matrix factorization},
	volume = {104},
	issn = {0167-8655},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865518300138},
	doi = {10.1016/j.patrec.2018.01.007},
	abstract = {Nonnegative matrix factorization (NMF) is a powerful tool for data mining. However, the emergence of ‘big data’ has severely challenged our ability to compute this fundamental decomposition using deterministic algorithms. This paper presents a randomized hierarchical alternating least squares (HALS) algorithm to compute the NMF. By deriving a smaller matrix from the nonnegative input data, a more efficient nonnegative decomposition can be computed. Our algorithm scales to big data applications while attaining a near-optimal factorization, i.e., the algorithm scales with the target rank of the data rather than the ambient dimension of measurement space. The proposed algorithm is evaluated using synthetic and real world data and shows substantial speedups compared to deterministic HALS.},
	language = {en},
	urldate = {2022-01-09},
	journal = {Pattern Recognition Letters},
	author = {Erichson, N. Benjamin and Mendible, Ariana and Wihlborn, Sophie and Kutz, J. Nathan},
	year = {2018},
	keywords = {Dimension reduction, NMF, Randomized algorithm},
	pages = {1--7},
}

@article{drineas_lectures_2017,
	title = {Lectures on {Randomized} {Numerical} {Linear} {Algebra}},
	url = {http://arxiv.org/abs/1712.08880},
	abstract = {This chapter is based on lectures on Randomized Numerical Linear Algebra from the 2016 Park City Mathematics Institute summer school on The Mathematics of Data.},
	urldate = {2022-01-09},
	journal = {arXiv:1712.08880 [cs, stat]},
	author = {Drineas, Petros and Mahoney, Michael W.},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.08880},
	keywords = {Computer Science - Data Structures and Algorithms, Statistics - Machine Learning},
}

@article{erichson_randomized_2019,
	title = {Randomized {Dynamic} {Mode} {Decomposition}},
	volume = {18},
	url = {https://epubs.siam.org/doi/abs/10.1137/18M1215013},
	doi = {10.1137/18M1215013},
	abstract = {This paper presents a randomized algorithm for computing the near-optimal low-rank dynamic mode decomposition (DMD). Randomized algorithms are emerging techniques to compute low-rank matrix approximations at a fraction of the cost of deterministic algorithms, easing the computational challenges arising in the area of “big data.” The idea is to derive a small matrix from the high-dimensional data, which is then used to efficiently compute the dynamic modes and eigenvalues. The algorithm is presented in a modular probabilistic framework, and the approximation quality can be controlled via oversampling and power iterations. The effectiveness of the resulting randomized DMD algorithm is demonstrated on several benchmark examples of increasing complexity, providing an accurate and efficient approach to extract spatiotemporal coherent structures from big data in a framework that scales with the intrinsic rank of the data, rather than the ambient measurement dimension. For this work we assume that the dynamics of the problem under consideration is evolving on a low-dimensional subspace that is well characterized by a quickly decaying singular value spectrum.},
	number = {4},
	urldate = {2022-01-09},
	journal = {SIAM Journal on Applied Dynamical Systems},
	author = {Erichson, N. Benjamin and Mathelin, Lionel and Kutz, J. Nathan and Brunton, Steven L.},
	year = {2019},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {37M10, 37N10, 65P99, dimension reduction, dynamic mode decomposition, dynamical systems, randomized algorithm},
	pages = {1867--1891},
}

@article{drineas_randnla_2016,
	title = {{RandNLA}: randomized numerical linear algebra},
	volume = {59},
	issn = {0001-0782},
	shorttitle = {{RandNLA}},
	url = {https://doi.org/10.1145/2842602},
	doi = {10.1145/2842602},
	abstract = {Randomization offers new benefits for large-scale linear algebra computations.},
	number = {6},
	urldate = {2022-01-09},
	journal = {Communications of the ACM},
	author = {Drineas, Petros and Mahoney, Michael W.},
	year = {2016},
	pages = {80--90},
}

@article{derezinski_determinantal_2020,
	title = {Determinantal {Point} {Processes} in {Randomized} {Numerical} {Linear} {Algebra}},
	url = {http://arxiv.org/abs/2005.03185},
	abstract = {Randomized Numerical Linear Algebra (RandNLA) uses randomness to develop improved algorithms for matrix problems that arise in scientific computing, data science, machine learning, etc. Determinantal Point Processes (DPPs), a seemingly unrelated topic in pure and applied mathematics, is a class of stochastic point processes with probability distribution characterized by sub-determinants of a kernel matrix. Recent work has uncovered deep and fruitful connections between DPPs and RandNLA which lead to new guarantees and improved algorithms that are of interest to both areas. We provide an overview of this exciting new line of research, including brief introductions to RandNLA and DPPs, as well as applications of DPPs to classical linear algebra tasks such as least squares regression, low-rank approximation and the Nystr{\textbackslash}"om method. For example, random sampling with a DPP leads to new kinds of unbiased estimators for least squares, enabling more refined statistical and inferential understanding of these algorithms; a DPP is, in some sense, an optimal randomized algorithm for the Nystr{\textbackslash}"om method; and a RandNLA technique called leverage score sampling can be derived as the marginal distribution of a DPP. We also discuss recent algorithmic developments, illustrating that, while not quite as efficient as standard RandNLA techniques, DPP-based algorithms are only moderately more expensive.},
	urldate = {2022-01-09},
	journal = {arXiv:2005.03185 [cs]},
	author = {Dereziński, Michał and Mahoney, Michael W.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.03185},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning},
}

@inproceedings{salmon_parallel_2011,
	title = {Parallel random numbers: {As} easy as 1, 2, 3},
	shorttitle = {Parallel random numbers},
	doi = {10.1145/2063384.2063405},
	abstract = {Most pseudorandom number generators (PRNGs) scale poorly to massively parallel high-performance computation because they are designed as sequentially dependent state transformations. We demonstrate that independent, keyed transformations of counters produce a large alternative class of PRNGs with excellent statistical properties (long period, no discernable structure or correlation). These counter-based PRNGs are ideally suited to modern multi- core CPUs, GPUs, clusters, and special-purpose hardware because they vectorize and parallelize well, and require little or no memory for state. We introduce several counter-based PRNGs: some based on cryptographic standards (AES, Threefish) and some completely new (Philox). All our PRNGs pass rigorous statistical tests (including TestUOl's BigCrush) and produce at least 264 unique parallel streams of random numbers, each with period 2128 or more. In addition to essentially unlimited parallel scalability, our PRNGs offer excellent single-chip performance: Philox is faster than the CURAND library on a single NVIDIA GPU.},
	booktitle = {{SC} '11: {Proceedings} of 2011 {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	author = {Salmon, John K. and Moraes, Mark A. and Dror, Ron O. and Shaw, David E.},
	month = nov,
	year = {2011},
	note = {ISSN: 2167-4337},
	keywords = {Batteries, Cryptography, Generators, Hardware, Memory management, Radiation detectors, Testing},
	pages = {1--12},
}

@article{kolda_tensor_2009,
	title = {Tensor {Decompositions} and {Applications}},
	volume = {51},
	issn = {0036-1445},
	url = {https://epubs.siam.org/doi/abs/10.1137/07070111x},
	doi = {10.1137/07070111X},
	abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N-way array. Decompositions of higher-order tensors (i.e., N-way arrays with \$N {\textbackslash}geq 3\$) have applications in psycho-metrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.},
	number = {3},
	urldate = {2021-12-14},
	journal = {SIAM Review},
	author = {Kolda, Tamara G. and Bader, Brett W.},
	month = aug,
	year = {2009},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {15A69, 65F99, canonical decomposition (CANDECOMP), higher-order principal components analysis (Tucker), higher-order singular value decomposition (HOSVD), multilinear algebra, multiway arrays, parallel factors (PARAFAC), tensor decompositions},
	pages = {455--500},
}

@article{qu_hardware-enabled_2021,
	title = {Hardware-{Enabled} {Efficient} {Data} {Processing} with {Tensor}-{Train} {Decomposition}},
	issn = {1937-4151},
	doi = {10.1109/TCAD.2021.3058317},
	abstract = {In recent years, tensor computation has become a promising tool for solving big data analysis, machine learning, medical image and EDA problems. To ease the memory and computation intensity of tensor processing, decomposition techniques, especially Tensor-train Decomposition(TTD), are widely adopted to compress the extremely high-dimensional tensor data. Despite TTD’s potential to break the curse of dimensionality, researchers have not yet leveraged its full computational potential, mainly because of two reasons:(1) Executing TTD itself is time-and energy-consuming due to the singular value decomposition(SVD) operation inside each of TTD’s iteration; (2) Additional software/hardware optimizations are often required to process the obtained TT-format data in certain applications such as deep learning inference. In this paper, we address these challenges with two approaches. Firstly, we propose an algorithm-hardware co-design with customized architecture namely TTD Engine to accelerate TTD. We use MRI image compression as a demo application to illustrate the efficacy of the proposed accelerator. Secondly, we present a case study demonstrating the benefit of TT-format data processing and the efficacy of using TTD Engine. In the case study, we use TT approach to realize convolution operation, which is difficult and nontrivial for TT-format data. Experimental results show that, TTD Engine achieves, on average, 14.9×∼36.9× speedup over CPU implementations and 4.1×∼9.9× speedup compared to the GPU baseline. The energy efficiency is also improved by at least 14.4× and 5.4× over CPU and GPU, respectively. Moreover, our hardware-enabled TT-format data processing further leads to more efficient implementations of complicated operations and applications.},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Qu, Zheng and Deng, Lei and Wang, Bangyan and Chen, Hengnu and Lin, Jilan and Liang, Ling and Li, Guoqi and Zhang, Zheng and Xie, Yuan},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {Algorithm Hardware Co-design, Approximation algorithms, Engines, Hardware, Jacobian matrices, Matrix decomposition, Symmetric matrices, TT-format Data Processing., Tensor-Train Decomposition, Tensors},
	pages = {1--1},
}

@article{zniyed_tt-based_2020,
	title = {A {TT}-{Based} {Hierarchical} {Framework} for {Decomposing} {High}-{Order} {Tensors}},
	volume = {42},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/18M1229973},
	doi = {10.1137/18M1229973},
	abstract = {In the context of big data, high-order tensor decompositions have to face a new challenge in terms of storage and computational costs. The tensor train (TT) decomposition provides a very useful graph-based model reduction, whose storage cost grows linearly with the tensor order \$D\$. The computation of the TT-core tensors and TT-ranks can be done in a stable sequential (i.e., noniterative) way thanks to the popular TT-SVD algorithm. In this paper, we exploit the ideas developed for the hierarchical/tree Tucker decomposition in the context of the TT decomposition. Specifically, a new efficient estimation scheme, called TT-HSVD (Tensor-Train Hierarchical SVD), is proposed as a solution to compute the TT decomposition of a high-order tensor. The new algorithm simultaneously delivers the TT-core tensors and their TT-ranks in a hierarchical way. It is a stable (i.e., noniterative) and computationally more efficient algorithm than TT-SVD, which is very important when dealing with large-scale data. The TT-HSVD algorithm uses a new reshaping strategy and a tailored partial SVD, which allows us to deal with smaller matrices compared to those of the TT-SVD. In addition, TT-HSVD is well suited for a parallel processing architecture. An algebraic analysis of the two algorithms is carried out, showing that TT-SVD and TT-HSVD compute the same TT-ranks and TT-core tensors up to specific bases. Simulation results for different tensor orders and dimensions corroborate the effectiveness of the proposed algorithm.},
	number = {2},
	urldate = {2021-12-08},
	journal = {SIAM Journal on Scientific Computing},
	author = {Zniyed, Yassine and Boyer, Rémy and de Almeida, André L. F. and Favier, Gérard},
	month = jan,
	year = {2020},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {68W15, 68W40, dimensionality reduction, hierarchical SVD, tensor graph, tensor train},
	pages = {A822--A848},
}

@article{novikov_tensor_2020,
	title = {Tensor {Train} {Decomposition} on {TensorFlow} ({T3F})},
	volume = {21},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v21/18-008.html},
	abstract = {Tensor Train decomposition is used across many branches of machine learning. We present T3F—a library for Tensor Train decomposition based on TensorFlow. T3F supports GPU execution, batch processing, automatic differentiation, and versatile functionality for the Riemannian optimization framework, which takes into account the underlying manifold structure to construct efficient optimization methods. The library makes it easier to implement machine learning papers that rely on the Tensor Train decomposition. T3F includes documentation, examples and 94\% test coverage.},
	number = {30},
	urldate = {2021-12-07},
	journal = {Journal of Machine Learning Research},
	author = {Novikov, Alexander and Izmailov, Pavel and Khrulkov, Valentin and Figurnov, Michael and Oseledets, Ivan},
	year = {2020},
	pages = {1--7},
}

@article{daas_parallel_2021,
	title = {Parallel {Algorithms} for {Tensor} {Train} {Arithmetic}},
	url = {http://arxiv.org/abs/2011.06532},
	abstract = {We present efficient and scalable parallel algorithms for performing mathematical operations for low-rank tensors represented in the tensor train (TT) format. We consider algorithms for addition, elementwise multiplication, computing norms and inner products, orthogonalization, and rounding (rank truncation). These are the kernel operations for applications such as iterative Krylov solvers that exploit the TT structure. The parallel algorithms are designed for distributed-memory computation, and we use a data distribution and strategy that parallelizes computations for individual cores within the TT format. We analyze the computation and communication costs of the proposed algorithms to show their scalability, and we present numerical experiments that demonstrate their efficiency on both shared-memory and distributed-memory parallel systems. For example, we observe better single-core performance than the existing MATLAB TT-Toolbox in rounding a 2GB TT tensor, and our implementation achieves a \$34{\textbackslash}times\$ speedup using all 40 cores of a single node. We also show nearly linear parallel scaling on larger TT tensors up to over 10,000 cores for all mathematical operations.},
	urldate = {2021-12-07},
	journal = {arXiv:2011.06532 [cs, math]},
	author = {Daas, Hussam Al and Ballard, Grey and Benner, Peter},
	month = sep,
	year = {2021},
	note = {arXiv: 2011.06532},
	keywords = {Mathematics - Numerical Analysis},
}

@article{oseledets_tensor-train_2011,
	title = {Tensor-{Train} {Decomposition}},
	volume = {33},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/10.1137/090752286},
	doi = {10.1137/090752286},
	abstract = {A simple nonrecursive form of the tensor decomposition in d dimensions is presented. It does not inherently suffer from the curse of dimensionality, it has asymptotically the same number of parameters as the canonical decomposition, but it is stable and its computation is based on low-rank approximation of auxiliary unfolding matrices. The new form gives a clear and convenient way to implement all basic operations efficiently. A fast rounding procedure is presented, as well as basic linear algebra operations. Examples showing the benefits of the decomposition are given, and the efficiency is demonstrated by the computation of the smallest eigenvalue of a 19-dimensional operator.},
	number = {5},
	urldate = {2021-12-07},
	journal = {SIAM Journal on Scientific Computing},
	author = {Oseledets, I. V.},
	month = jan,
	year = {2011},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {15A23, 15A69, 65F99, SVD, TT-format, high-dimensional problems, tensors},
	pages = {2295--2317},
}

@article{batselier_computing_2018,
	title = {Computing {Low}-{Rank} {Approximations} of {Large}-{Scale} {Matrices} with the {Tensor} {Network} {Randomized} {SVD}},
	copyright = {Article is made available in accordance with the publisher's policy and may be subject to US copyright law. Please refer to the publisher's site for terms of use.},
	issn = {0895-4798},
	url = {https://dspace.mit.edu/handle/1721.1/121552},
	abstract = {We propose a new algorithm for the computation of a singular value decomposition (SVD) low-rank approximation of a matrix in the matrix product operator (MPO) format, also called the tensor train matrix format. Our tensor network randomized SVD (TNrSVD) algorithm is an MPO implementation of the randomized SVD algorithm that is able to compute dominant singular values and their corresponding singular vectors. In contrast to the state-of-the-art tensor-based alternating least squares SVD (ALS-SVD) and modified alternating least squares SVD (MALS-SVD) matrix approximation methods, TNrSVD can be up to 13 times faster while achieving better accuracy. In addition, our TNrSVD algorithm also produces accurate approximations in particular cases where both ALS-SVD and MALS-SVD fail to converge. We also propose a new algorithm for the fast conversion of a sparse matrix into its corresponding MPO form, which is up to 509 times faster than the standard tensor train SVD method while achieving machine precision accuracy. The efficiency and accuracy of both algorithms are demonstrated in numerical experiments. Key words: curse of dimensionality, low-rank tensor approximation, matrix factorization, matrix product operator, singular value decompositon (SVD), tensor network, tensor train (TT) decomposition, randomized algorithm},
	language = {en},
	urldate = {2021-12-07},
	journal = {SIAM},
	author = {Batselier, Kim and Yu, Wenjian and Daniel, Luca and Wong, Ngai},
	month = jan,
	year = {2018},
	note = {Accepted: 2019-07-09T19:10:20Z
Publisher: Society for Industrial \& Applied Mathematics (SIAM)},
}

@article{minster_randomized_2020,
	title = {Randomized {Algorithms} for {Low}-{Rank} {Tensor} {Decompositions} in the {Tucker} {Format}},
	volume = {2},
	url = {https://epubs.siam.org/doi/abs/10.1137/19M1261043},
	doi = {10.1137/19M1261043},
	abstract = {Many applications in data science and scientific computing involve large-scale datasets that are expensive to store and manipulate. However, these datasets possess inherent multidimensional structure that can be exploited to compress and store the dataset in an appropriate tensor format. In recent years, randomized matrix methods have been used to efficiently and accurately compute low-rank matrix decompositions. Motivated by this success, we develop randomized algorithms for tensor decompositions in the Tucker representation. Specifically, we present randomized versions of two well-known compression algorithms, namely, HOSVD and STHOSVD, and a detailed probabilistic analysis of the error in using both algorithms. We also develop variants of these algorithms that tackle specific challenges posed by large-scale datasets. The first variant adaptively finds a low-rank representation satisfying a given tolerance, and it is beneficial when the target rank is not known in advance. The second variant preserves the structure of the original tensor and is beneficial for large sparse tensors that are difficult to load in memory. We consider several different datasets for our numerical experiments: synthetic test tensors and realistic applications such as the compression of facial image samples in the Olivetti database and word counts in the Enron email dataset.},
	number = {1},
	urldate = {2021-12-07},
	journal = {SIAM Journal on Mathematics of Data Science},
	author = {Minster, Rachel and Saibaba, Arvind K. and Kilmer, Misha E.},
	month = jan,
	year = {2020},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {15A18, 15A69, 15B52, 65F15, 65F99, 68W20, Tucker decompositions, low-rank, multilinear algebra, randomized algorithms, structure-preserving, tensors},
	pages = {189--215},
}

@inproceedings{liu_closing_2021,
	address = {New York, NY, USA},
	series = {{SC} '21},
	title = {Closing the "quantum supremacy" gap: achieving real-time simulation of a random quantum circuit using a new {Sunway} supercomputer},
	isbn = {978-1-4503-8442-1},
	shorttitle = {Closing the "quantum supremacy" gap},
	url = {https://doi.org/10.1145/3458817.3487399},
	doi = {10.1145/3458817.3487399},
	abstract = {We develop a high-performance tensor-based simulator for random quantum circuits(RQCs) on the new Sunway supercomputer. Our major innovations include: (1) a near-optimal slicing scheme, and a path-optimization strategy that considers both complexity and compute density; (2) a three-level parallelization scheme that scales to about 42 million cores; (3) a fused permutation and multiplication design that improves the compute efficiency for a wide range of tensor contraction scenarios; and (4) a mixed-precision scheme to further improve the performance. Our simulator effectively expands the scope of simulatable RQCs to include the 10X10(qubits)X(1+40+1)(depth) circuit, with a sustained performance of 1.2 Eflops (single-precision), or 4.4 Eflops (mixed-precision)as a new milestone for classical simulation of quantum circuits; and reduces the simulation sampling time of Google Sycamore to 304 seconds, from the previously claimed 10,000 years.},
	urldate = {2021-11-21},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Yong (Alexander) and Liu, Xin (Lucy) and Li, Fang (Nancy) and Fu, Haohuan and Yang, Yuling and Song, Jiawei and Zhao, Pengpeng and Wang, Zhen and Peng, Dajia and Chen, Huarong and Guo, Chu and Huang, Heliang and Wu, Wenzhao and Chen, Dexun},
	month = nov,
	year = {2021},
	pages = {1--12},
}

@article{higham_new_2019,
	title = {A {New} {Approach} to {Probabilistic} {Rounding} {Error} {Analysis}},
	volume = {41},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/18M1226312},
	doi = {10.1137/18M1226312},
	abstract = {Traditional rounding error analysis in numerical linear algebra leads to backward error bounds involving the constant \${\textbackslash}gamma{\textasciicircum}\{\}\_n = nu/(1-nu)\$, for a problem size \$n\$ and unit roundoff \$u\$. In light of large-scale and possibly low-precision computations, such bounds can struggle to provide any useful information. We develop a new probabilistic rounding error analysis that can be applied to a wide range of algorithms. By using a concentration inequality and making probabilistic assumptions about the rounding errors, we show that in several core linear algebra computations \${\textbackslash}gamma{\textasciicircum}\{\}\_n\$ can be replaced by a relaxed constant \${\textbackslash}widetilde\{{\textbackslash}gamma\}{\textasciicircum}\{\}\_n\$ proportional to \${\textbackslash}sqrt\{n{\textbackslash}log n\}{\textbackslash},u\$ with a probability bounded below by a quantity independent of \$n\$. The new constant \${\textbackslash}widetilde\{{\textbackslash}gamma\}{\textasciicircum}\{\}\_n\$ grows much more slowly with \$n\$ than \${\textbackslash}gamma\_n\$. Our results have three key features: they are backward error bounds; they are exact, not first order; and they are valid for any \$n\$, unlike results obtained by applying the central limit theorem, which apply only as \$n{\textbackslash}to{\textbackslash}infty\$. We provide numerical experiments that show that for both random and real-life matrices the bounds can be much smaller than the standard deterministic bounds and can have the correct asymptotic growth with \$n\$. We also identify two special situations in which the assumptions underlying the analysis are not valid and the bounds do not hold. Our analysis provides, for the first time, a rigorous foundation for the rule of thumb that “one can take the square root of an error constant because of statistical effects in rounding error propagation.”},
	number = {5},
	urldate = {2021-11-22},
	journal = {SIAM Journal on Scientific Computing},
	author = {Higham, Nicholas J. and Mary, Theo},
	month = jan,
	year = {2019},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {65F05, 65G50, floating-point arithmetic, numerical linear algebra, rounding error analysis},
	pages = {A2815--A2835},
}

@article{yang_rounding_2021,
	title = {Rounding {Error} {Analysis} of {Mixed} {Precision} {Block} {Householder} {QR} {Algorithms}},
	volume = {43},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/19M1296367},
	doi = {10.1137/19M1296367},
	abstract = {Although mixed precision arithmetic has recently garnered interest for training dense neural networks, many other applications could benefit from the speedups and lower storage cost if applied appropriately. The growing interest in employing mixed precision computations motivates the need for rounding error analysis that properly handles behavior from mixed precision arithmetic. We develop mixed precision variants of existing Householder QR algorithms and show error analyses supported by numerical experiments.},
	number = {3},
	urldate = {2021-11-20},
	journal = {SIAM Journal on Scientific Computing},
	author = {Yang, L. Minah and Fox, Alyson and Sanders, Geoffrey},
	month = jan,
	year = {2021},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {65F08, 65F10, 65F25, 65G50, QR factorization, block QR, fp16, fp32, mixed precision, rounding error},
	pages = {A1723--A1753},
}

@article{boixo_characterizing_2018,
	title = {Characterizing quantum supremacy in near-term devices},
	volume = {14},
	copyright = {2018 The Author(s)},
	issn = {1745-2481},
	url = {https://www.nature.com/articles/s41567-018-0124-x},
	doi = {10.1038/s41567-018-0124-x},
	abstract = {A critical question for quantum computing in the near future is whether quantum devices without error correction can perform a well-defined computational task beyond the capabilities of supercomputers. Such a demonstration of what is referred to as quantum supremacy requires a reliable evaluation of the resources required to solve tasks with classical approaches. Here, we propose the task of sampling from the output distribution of random quantum circuits as a demonstration of quantum supremacy. We extend previous results in computational complexity to argue that this sampling task must take exponential time in a classical computer. We introduce cross-entropy benchmarking to obtain the experimental fidelity of complex multiqubit dynamics. This can be estimated and extrapolated to give a success metric for a quantum supremacy demonstration. We study the computational cost of relevant classical algorithms and conclude that quantum supremacy can be achieved with circuits in a two-dimensional lattice of 7 × 7 qubits and around 40 clock cycles. This requires an error rate of around 0.5\% for two-qubit gates (0.05\% for one-qubit gates), and it would demonstrate the basic building blocks for a fault-tolerant quantum computer.},
	language = {en},
	number = {6},
	urldate = {2021-10-16},
	journal = {Nature Physics},
	author = {Boixo, Sergio and Isakov, Sergei V. and Smelyanskiy, Vadim N. and Babbush, Ryan and Ding, Nan and Jiang, Zhang and Bremner, Michael J. and Martinis, John M. and Neven, Hartmut},
	month = jun,
	year = {2018},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 6
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Quantum information;Quantum simulation
Subject\_term\_id: quantum-information;quantum-simulation},
	pages = {595--600},
}

@article{knorr_ndzip-gpu_2021,
	title = {ndzip-gpu: {Efficient} {Lossless} {Compression} of {Scientific} {Floating}-{Point} {Data} on {GPUs}},
	shorttitle = {ndzip-gpu},
	url = {https://hgpu.org/?p=25401},
	abstract = {Lossless data compression is a promising software approach for reducing the bandwidth requirements of scientific applications on accelerator clusters without introducing approximation errors. Suita…},
	language = {en-US},
	urldate = {2021-10-09},
	author = {Knorr, Fabian and Thoman, Peter and Fahringer, Thomas},
	month = aug,
	year = {2021},
}

@incollection{makino_traditional_2021,
	address = {Cham},
	title = {Traditional {Approaches} and {Their} {Limitations}},
	isbn = {978-3-030-76871-3},
	url = {https://doi.org/10.1007/978-3-030-76871-3_2},
	abstract = {In this section the past “evolution” of HPC systems is overviewed. Starting from CDC 6600, which had many of features of modern processors, vector parallel computers are discussed first, and then single-chip microprocessors. The two evolution paths are largely similar, and microprocessors are now at the point when vector parallel computers were replaced by microprocessors. The relationship between these two evolutionary paths and the “quantitative” approach is also discussed. Finally, it is argued that a quantitative measure of efficiency, such as the efficiency of thermal engines and the parasite drag of airplanes, is necessary to make the computer architecture really a quantitative science.},
	language = {en},
	urldate = {2021-09-29},
	booktitle = {Principles of {High}-{Performance} {Processor} {Design}: {For} {High} {Performance} {Computing}, {Deep} {Neural} {Networks} and {Data} {Science}},
	publisher = {Springer International Publishing},
	author = {Makino, Junichiro},
	editor = {Makino, Junichiro},
	year = {2021},
	doi = {10.1007/978-3-030-76871-3_2},
	pages = {7--35},
}

@incollection{makino_present_2021,
	address = {Cham},
	title = {Present, {Past} and {Future}},
	isbn = {978-3-030-76871-3},
	url = {https://doi.org/10.1007/978-3-030-76871-3_7},
	abstract = {The goal of the book is restated and put into historical perspective. In the history of scientific computing, the transition of processor architecture took place only when the existing software could be used in the new architecture. On the other hand, the cumulative changes in the microprocessor architectures made both their transistor efficiency and the application efficiency low. Thus, though the transition is necessary, it is not likely to happen. One hope is the use of frameworks, as in the case of the field of deep learning.},
	language = {en},
	urldate = {2021-09-29},
	booktitle = {Principles of {High}-{Performance} {Processor} {Design}: {For} {High} {Performance} {Computing}, {Deep} {Neural} {Networks} and {Data} {Science}},
	publisher = {Springer International Publishing},
	author = {Makino, Junichiro},
	editor = {Makino, Junichiro},
	year = {2021},
	doi = {10.1007/978-3-030-76871-3_7},
	pages = {147--155},
}

@incollection{makino_software_2021,
	address = {Cham},
	title = {Software},
	isbn = {978-3-030-76871-3},
	url = {https://doi.org/10.1007/978-3-030-76871-3_6},
	abstract = {The software for chip-scale SIMD processors is discussed as the reference design introduced in Chap. 5 as an example target. Although it is possible to extend existing frameworks like OpenCL to take care of the memory hierarchy of the reference design, it is by far more preferable to provide frameworks or DSLs specialized to application areas such as structured and unstructured mesh calculations, particle-based simulations and deep learning. The important advantage of the framework approach is that the application programs need not reimplement complicated codes for parallelization and yet develop highly optimized applications for multiple processor architectures.},
	language = {en},
	urldate = {2021-09-29},
	booktitle = {Principles of {High}-{Performance} {Processor} {Design}: {For} {High} {Performance} {Computing}, {Deep} {Neural} {Networks} and {Data} {Science}},
	publisher = {Springer International Publishing},
	author = {Makino, Junichiro},
	editor = {Makino, Junichiro},
	year = {2021},
	doi = {10.1007/978-3-030-76871-3_6},
	pages = {135--145},
}

@incollection{makino_lower_2021,
	address = {Cham},
	title = {The {Lower} {Limit} of {Energy} {Consumption}},
	isbn = {978-3-030-76871-3},
	url = {https://doi.org/10.1007/978-3-030-76871-3_3},
	abstract = {First, target applications of exascale projects of Japan, US and EU are analyzed and classified into five categories. Then the definition of the efficiency is introduced. The efficiency of an architecture for an application is defined as the ratio between the theoretical minimum amount of energy to perform the calculations in a given application and the actual energy consumption, assuming the same semiconductor technology and same supply voltage are used. Whether or not this definition is practically meaningful or not is also addressed.},
	language = {en},
	urldate = {2021-09-29},
	booktitle = {Principles of {High}-{Performance} {Processor} {Design}: {For} {High} {Performance} {Computing}, {Deep} {Neural} {Networks} and {Data} {Science}},
	publisher = {Springer International Publishing},
	author = {Makino, Junichiro},
	editor = {Makino, Junichiro},
	year = {2021},
	doi = {10.1007/978-3-030-76871-3_3},
	pages = {37--63},
}

@incollection{makino_analysis_2021,
	address = {Cham},
	title = {Analysis of {Past} and {Present} {Processors}},
	isbn = {978-3-030-76871-3},
	url = {https://doi.org/10.1007/978-3-030-76871-3_4},
	abstract = {Past and present representative processors for HPC are analyzed in terms of the transistor efficiency, which is closely related to the power efficiency. The highest efficiency is around 30\% of TMC CM-2, while the lowest is 0.08\% of Intel Core2. Modern CPUs and GPUs are in the range of 0.8\%–2.4\%. Apparently large-scale (chip-level now) SIMD processor can be efficient. However, in early 1990s they disappeared because the problem of the external memory bandwidth could not be solved. Modern GPGPUs have addressed this limitation by making the unit of SIMD execution small, but at the cost of rather low transistor efficiency.},
	language = {en},
	urldate = {2021-09-29},
	booktitle = {Principles of {High}-{Performance} {Processor} {Design}: {For} {High} {Performance} {Computing}, {Deep} {Neural} {Networks} and {Data} {Science}},
	publisher = {Springer International Publishing},
	author = {Makino, Junichiro},
	editor = {Makino, Junichiro},
	year = {2021},
	doi = {10.1007/978-3-030-76871-3_4},
	pages = {65--94},
}

@incollection{makino_near-optimal_2021,
	address = {Cham},
	title = {“{Near}-{Optimal}” {Designs}},
	isbn = {978-3-030-76871-3},
	url = {https://doi.org/10.1007/978-3-030-76871-3_5},
	abstract = {The designs not far from the optimal point are discussed. First, special-purpose GRAPE processors are overviewed. With these processors, the efficiency close to the theoretical limit was actually achieved, albeit for just one application area. Then GRAPE-DR SIMD manycore processor and its applications are discussed in details. The MN-Core processor, which can be regarded as the successor of GRAPE-DR, is also discussed. Finally, how one can use chip-scale SIMD processors for the four major application categories and what functionalities should be added are described, and a reference design is introduced.},
	language = {en},
	urldate = {2021-09-29},
	booktitle = {Principles of {High}-{Performance} {Processor} {Design}: {For} {High} {Performance} {Computing}, {Deep} {Neural} {Networks} and {Data} {Science}},
	publisher = {Springer International Publishing},
	author = {Makino, Junichiro},
	editor = {Makino, Junichiro},
	year = {2021},
	doi = {10.1007/978-3-030-76871-3_5},
	pages = {95--134},
}

@incollection{makino_introduction_2021,
	address = {Cham},
	title = {Introduction},
	isbn = {978-3-030-76871-3},
	url = {https://doi.org/10.1007/978-3-030-76871-3_1},
	abstract = {The goal of the book is described. The goal is to define efficiency of a computer architecture in a way similar to those in other area in science and engineering, and construct a theory of computer architecture based on that efficiency.},
	language = {en},
	urldate = {2021-09-29},
	booktitle = {Principles of {High}-{Performance} {Processor} {Design}: {For} {High} {Performance} {Computing}, {Deep} {Neural} {Networks} and {Data} {Science}},
	publisher = {Springer International Publishing},
	author = {Makino, Junichiro},
	editor = {Makino, Junichiro},
	year = {2021},
	doi = {10.1007/978-3-030-76871-3_1},
	pages = {1--5},
}

@article{ballard_improving_2016,
	title = {Improving the numerical stability of fast matrix multiplication},
	url = {http://arxiv.org/abs/1507.00687},
	abstract = {Fast algorithms for matrix multiplication, namely those that perform asymptotically fewer scalar operations than the classical algorithm, have been considered primarily of theoretical interest. Apart from Strassen's original algorithm, few fast algorithms have been efficiently implemented or used in practical applications. However, there exist many practical alternatives to Strassen's algorithm with varying performance and numerical properties. Fast algorithms are known to be numerically stable, but because their error bounds are slightly weaker than the classical algorithm, they are not used even in cases where they provide a performance benefit. We argue in this paper that the numerical sacrifice of fast algorithms, particularly for the typical use cases of practical algorithms, is not prohibitive, and we explore ways to improve the accuracy both theoretically and empirically. The numerical accuracy of fast matrix multiplication depends on properties of the algorithm and of the input matrices, and we consider both contributions independently. We generalize and tighten previous error analyses of fast algorithms and compare their properties. We discuss algorithmic techniques for improving the error guarantees from two perspectives: manipulating the algorithms, and reducing input anomalies by various forms of diagonal scaling. Finally, we benchmark performance and demonstrate our improved numerical accuracy.},
	urldate = {2021-09-24},
	journal = {arXiv:1507.00687 [cs]},
	author = {Ballard, Grey and Benson, Austin R. and Druinsky, Alex and Lipshitz, Benjamin and Schwartz, Oded},
	month = jul,
	year = {2016},
	note = {arXiv: 1507.00687},
	keywords = {Mathematics - Numerical Analysis},
}

@misc{corporation_nvidia_nodate,
	title = {{NVIDIA} {AMPERE} {GA102} {GPU} {Architecture} {V2}},
	abstract = {Peek under the hood of the new NVIDIA Ampere GA10x architecture, which powers GeForce RTX 30 Series graphics cards.},
	language = {en-us},
	urldate = {2021-06-06},
	journal = {NVIDIA AMPERE GA102 GPU ARCHITECTURE},
	author = {Corporation, NVIDIA},
}

@misc{corporation_nvidia_nodate-1,
	title = {{NVIDIA} {AMPERE} {GA102} {GPU} {Architecture} {V1}},
	abstract = {Peek under the hood of the new NVIDIA Ampere GA10x architecture, which powers GeForce RTX 30 Series graphics cards.},
	language = {en-us},
	urldate = {2021-06-06},
	journal = {NVIDIA AMPERE GA102 GPU ARCHITECTURE},
	author = {Corporation, NVIDIA},
}

@misc{noauthor_ibm_2020,
	title = {{IBM} {Power} {Systems} {Announces} {POWER10} {Processor}},
	abstract = {The IBM POWER10 processor underscores IBM’s belief in hybrid cloud. With hardware co-optimized for Red Hat software, IBM POWER10-based servers will deliver:},
	language = {en-US},
	urldate = {2021-06-06},
	journal = {Servers \& Storage},
	author = {, IBM Corporation},
	month = aug,
	year = {2020},
}

@article{ghashami_frequent_2015,
	title = {Frequent {Directions} : {Simple} and {Deterministic} {Matrix} {Sketching}},
	shorttitle = {Frequent {Directions}},
	url = {http://arxiv.org/abs/1501.01711},
	abstract = {We describe a new algorithm called Frequent Directions for deterministic matrix sketching in the row-updates model. The algorithm is presented an arbitrary input matrix \$A {\textbackslash}in R{\textasciicircum}\{n {\textbackslash}times d\}\$ one row at a time. It performed \$O(d {\textbackslash}times {\textbackslash}ell)\$ operations per row and maintains a sketch matrix \$B {\textbackslash}in R{\textasciicircum}\{{\textbackslash}ell {\textbackslash}times d\}\$ such that for any \$k {\textless} {\textbackslash}ell\$ \${\textbackslash}{\textbar}A{\textasciicircum}TA - B{\textasciicircum}TB {\textbackslash}{\textbar}\_2 {\textbackslash}leq {\textbackslash}{\textbar}A - A\_k{\textbackslash}{\textbar}\_F{\textasciicircum}2 / ({\textbackslash}ell-k)\$ and \${\textbackslash}{\textbar}A - {\textbackslash}pi\_\{B\_k\}(A){\textbackslash}{\textbar}\_F{\textasciicircum}2 {\textbackslash}leq {\textbackslash}big(1 + {\textbackslash}frac\{k\}\{{\textbackslash}ell-k\}{\textbackslash}big) {\textbackslash}{\textbar}A-A\_k{\textbackslash}{\textbar}\_F{\textasciicircum}2 \$ . Here, \$A\_k\$ stands for the minimizer of \${\textbackslash}{\textbar}A - A\_k{\textbackslash}{\textbar}\_F\$ over all rank \$k\$ matrices (similarly \$B\_k\$) and \${\textbackslash}pi\_\{B\_k\}(A)\$ is the rank \$k\$ matrix resulting from projecting \$A\$ on the row span of \$B\_k\$. We show both of these bounds are the best possible for the space allowed. The summary is mergeable, and hence trivially parallelizable. Moreover, Frequent Directions outperforms exemplar implementations of existing streaming algorithms in the space-error tradeoff.},
	urldate = {2021-09-07},
	journal = {arXiv:1501.01711 [cs]},
	author = {Ghashami, Mina and Liberty, Edo and Phillips, Jeff M. and Woodruff, David P.},
	month = apr,
	year = {2015},
	note = {arXiv: 1501.01711},
	keywords = {68W40 (Primary), Computer Science - Data Structures and Algorithms},
}

@article{blalock_multiplying_2021,
	title = {Multiplying {Matrices} {Without} {Multiplying}},
	url = {http://arxiv.org/abs/2106.10860},
	abstract = {Multiplying matrices is among the most fundamental and compute-intensive operations in machine learning. Consequently, there has been significant work on efficiently approximating matrix multiplies. We introduce a learning-based algorithm for this task that greatly outperforms existing methods. Experiments using hundreds of matrices from diverse domains show that it often runs \$100{\textbackslash}times\$ faster than exact matrix products and \$10{\textbackslash}times\$ faster than current approximate methods. In the common case that one matrix is known ahead of time, our method also has the interesting property that it requires zero multiply-adds. These results suggest that a mixture of hashing, averaging, and byte shuffling\$-\$the core operations of our method\$-\$could be a more promising building block for machine learning than the sparsified, factorized, and/or scalar quantized matrix products that have recently been the focus of substantial research and hardware investment.},
	urldate = {2021-09-07},
	journal = {arXiv:2106.10860 [cs, stat]},
	author = {Blalock, Davis and Guttag, John},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.10860},
	keywords = {Computer Science - Hardware Architecture, Computer Science - Machine Learning, Computer Science - Performance, Statistics - Machine Learning},
}

@article{markidis_nvidia_2018,
	title = {{NVIDIA} {Tensor} {Core} {Programmability}, {Performance} \& {Precision}},
	url = {http://arxiv.org/abs/1803.04014},
	doi = {10.1109/IPDPSW.2018.00091},
	abstract = {The NVIDIA Volta GPU microarchitecture introduces a specialized unit, called "Tensor Core" that performs one matrix-multiply-and-accumulate on 4x4 matrices per clock cycle. The NVIDIA Tesla V100 accelerator, featuring the Volta microarchitecture, provides 640 Tensor Cores with a theoretical peak performance of 125 Tflops/s in mixed precision. In this paper, we investigate current approaches to program NVIDIA Tensor Cores, their performances and the precision loss due to computation in mixed precision. Currently, NVIDIA provides three different ways of programming matrix-multiply-and-accumulate on Tensor Cores: the CUDA Warp Matrix Multiply Accumulate (WMMA) API, CUTLASS, a templated library based on WMMA, and cuBLAS GEMM. After experimenting with different approaches, we found that NVIDIA Tensor Cores can deliver up to 83 Tflops/s in mixed precision on a Tesla V100 GPU, seven and three times the performance in single and half precision respectively. A WMMA implementation of batched GEMM reaches a performance of 4 Tflops/s. While precision loss due to matrix multiplication with half precision input might be critical in many HPC applications, it can be considerably reduced at the cost of increased computation. Our results indicate that HPC applications using matrix multiplications can strongly benefit from using of NVIDIA Tensor Cores.},
	urldate = {2021-09-06},
	journal = {2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
	author = {Markidis, Stefano and Der Chien, Steven Wei and Laure, Erwin and Peng, Ivy Bo and Vetter, Jeffrey S.},
	month = may,
	year = {2018},
	note = {arXiv: 1803.04014},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance},
	pages = {522--531},
}

@article{lecuyer_testu01_2007,
	title = {{TestU01}: {A} {C} library for empirical testing of random number generators},
	volume = {33},
	shorttitle = {{TestU01}},
	abstract = {a collection of utilities for the empirical statistical testing of uniform random number generators (RNGs). It provides general implementations of the classical statistical tests for RNGs, as well as several others tests proposed in the literature, and some original ones. Predefined tests suites for sequences of uniform random numbers over the interval (0,1) and for bit sequences are available. Tools are also oered to perform systematic studies of the interaction between a specific test and the structure of the point sets produced by a given family of RNGs. That is, for a given kind of test and a given class of RNGs, to determine how large should be the sample size of the test, as a function of the generator's period length, before the generator starts to fail the test systematically. Finally, the library provides various types of generators implemented in generic form, as well as many specific generators proposed in the literature or found in widely-used software. The tests can be applied to instances of the generators predefined in the library, or to user-defined generators, or to streams of random numbers produced by any kind of device or stored in files. Besides introducing TestU01, the paper provides a survey and a classification of statistical tests for RNGs. It also applies batteries of tests to a long list of widely used RNGs.},
	journal = {ACM Trans. Math. Softw.},
	author = {L’Ecuyer, Pierre and Simard, Richard},
	month = jan,
	year = {2007},
}

@article{francois_fast_2014,
	title = {A {Fast} {Chaos}-{Based} {Pseudo}-{Random} {Bit} {Generator} {Using} {Binary64} {Floating}-{Point} {Arithmetic}},
	volume = {38},
	url = {https://hal.archives-ouvertes.fr/hal-01024689},
	abstract = {Chaos-based cryptography is widely investigated in recent years, especially in the field of random number generators. The paper describes a novel pseudo-random bit generator (PRBG) based on chaotic logistic maps. Three logistic maps are combined in the algorithmic process, and a block of 32 random bits is produced at each iteration. The binary64 double precision format is used according to the IEEE 754-2008 standard for floating-point arithmetic. This generator provides a considerable improvement of an existing generator in the literature. Rigorous statistical analyses are carefully conducted to evaluate the quality and the robustness of the PRBG. The obtained results showed the relevance of the proposed generator, which is suitable even for real-time applications.},
	number = {2},
	urldate = {2021-09-06},
	journal = {Informatica},
	author = {François, Michael and Defour, David and Negre, Christophe},
	month = jul,
	year = {2014},
	note = {Publisher: Slovene Society Informatika, Ljubljana},
	keywords = {IEEE-754, PRBG, cryptography, logistic map, pseudo-random},
	pages = {115--124},
}

@inproceedings{miyazaki_pipeline_2018,
	title = {A {Pipeline} {Implementation} for {Dynamic} {Programming} on {GPU}},
	doi = {10.1109/CANDARW.2018.00063},
	abstract = {In this paper, we show the effectiveness of a pipeline implementation of Dynamic Programming (DP) on GPU. As an example, we parallelize a typical DP program where each element of its solution table is calculated in order by semigroup computations among some already computed elements in the table. We implement the DP program on GPU in a pipeline fashion, i.e., we use GPU cores for supporting pipeline-stages so that many elements of the solution table are partially computed in parallel at one time. Our implementation can determine one output value per one computational step, which is faster than the standard parallel implementation whose strategy is to speed up each semi-group computations. We evaluate the performance of our implementation and verify its speedup.},
	booktitle = {2018 {Sixth} {International} {Symposium} on {Computing} and {Networking} {Workshops} ({CANDARW})},
	author = {Miyazaki, Makoto and Matsumae, Susumu},
	month = nov,
	year = {2018},
	keywords = {Dynamic Programming, Dynamic programming, GPGPU, Graphics processing units, Indexes, Information science, Instruction sets, Parallel Algorithms, Pipelines, Pipelining, Standards},
	pages = {305--309},
}

@article{diwan_parallelization_2019,
	title = {A {Parallelization} of {Non}-{Serial} {Polyadic} {Dynamic} {Programming} on {GPU}},
	volume = {27},
	copyright = {Copyright (c) 2019 CIT. Journal of Computing and Information Technology},
	issn = {1846-3908},
	url = {http://cit.fer.hr/index.php/CIT/article/view/4579},
	abstract = {Parallelization of Non-Serial Polyadic Dynamic Programming (NPDP) on high-throughput manycore architectures, such as NVIDIA GPUs, suffers from load imbalance, i.e. non-optimal mapping between the sub-problems of NPDP and the processing elements of the GPU. NPDP exhibits non-uniformity in the number of subproblems as well as computational complexity across the phases. In NPDP parallelization, phases are computed sequentially whereas subproblems of each phase are computed concurrently. Therefore, it is essential to effectively map the subproblems of each phase to the processing elements while implementing thread level parallelism. We propose an adaptive Generalized Mapping Method (GMM) for NPDP parallelization that utilizes the GPU for efficient mapping of subproblems onto processing threads in each phase. Input-size and targeted GPU decide the computing power and the best mapping for each phase in NPDP parallelization. The performance of GMM is compared with different conventional parallelization approaches. For sufficiently large inputs, our technique outperforms the state-of-the-art conventional parallelization approach and achieves a significant speedup of a factor 30. We also summarize the general heuristics for achieving better gain in the NPDP parallelization.},
	language = {en},
	number = {2},
	urldate = {2021-09-05},
	journal = {CIT. Journal of Computing and Information Technology},
	author = {Diwan, Tausif and Tembhurne, Jitendra},
	month = nov,
	year = {2019},
	note = {Number: 2},
	keywords = {CUDA, GPU, NPDP, dynamic programming, parallel computing},
	pages = {55--66},
}

@misc{wani_accelerated_2013,
	title = {Accelerated {Dynamic} {Programming} on {GPU}: {A} {Study} of {Speed} {Up} and {Programming} {Approach}},
	shorttitle = {Accelerated {Dynamic} {Programming} on {GPU}},
	abstract = {GPUs (Graphics processing units) can be used for general purpose parallel computation. Developers can develop parallel programs running on GPUs using different computing architectures like CUDA or OpenCL. The Optimal Matrix Chain Multiplication problem is an optimization problem to find the optimal order for multiplying a chain of matrices. The optimal order of multiplication depends only on the dimensions of the matrices. It is known that this problem can be solved by dynamic programming technique using O(n 3)-time complexity algorithm and a work space of size O(n 2). The main contribution of this paper is to present a parallel implementation of this O(n 3)-time algorithm on a GPU and to assess the amount of programming effort required to develop this parallel implementation when compared to a similar sequential implementation.},
	author = {Wani, Mohsin Altaf and Quadri, S. M. K.},
	year = {2013},
}

@article{matsumae_solving_2018,
	title = {Solving {Dynamic} {Programming} {Problem} by {Pipeline} {Implementation} on {GPU}},
	volume = {9},
	issn = {21565570, 2158107X},
	url = {http://arxiv.org/abs/2008.01938},
	doi = {10.14569/IJACSA.2018.091272},
	abstract = {In this paper, we show the effectiveness of a pipeline implementation of Dynamic Programming (DP) on GPU. As an example, we explain how to solve a matrix-chain multiplication (MCM) problem by DP on GPU. This problem can be sequentially solved in \$O(n{\textasciicircum}3)\$ steps by DP where \$n\$ is the number of matrices, because its solution table is of size \$n {\textbackslash}times n\$ and each element of the table can be computed in \$O(n)\$ steps. A typical speedup strategy for this is to parallelize the \$O(n)\$ step computation of each element, which can be easily achieved by parallel prefix computation, i.e., an \$O({\textbackslash}log n)\$ step computation with \$n\$ threads in a tournament fashion. By such a standard parallelizing method, we can solve the MCM problem in \$O(n{\textasciicircum}2 {\textbackslash}log n)\$ steps with \$n\$ threads. In our approach, we solve the MCM problem on GPU in a pipeline fashion, i.e., we use GPU cores for supporting pipeline-stages so that many elements of the solution table are partially computed in parallel at one time. Our implementation determines one output value per one computational step with \$n\$ threads in a pipeline fashion and constructs the solution table totally in \$O(n{\textasciicircum}2)\$ steps with \$n\$ threads.},
	number = {12},
	urldate = {2021-09-05},
	journal = {International Journal of Advanced Computer Science and Applications},
	author = {Matsumae, Susumu and Miyazaki, Makoto},
	year = {2018},
	note = {arXiv: 2008.01938},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
}

@inproceedings{nishida_accelerating_2011,
	title = {Accelerating the {Dynamic} {Programming} for the {Matrix} {Chain} {Product} on the {GPU}},
	doi = {10.1109/ICNC.2011.62},
	abstract = {Modern GPUs (Graphics Processing Units) can be used for general purpose parallel computation. Users can develop parallel programs running on GPUs using programming architecture called CUDA (Compute Unified Device Architecture). The Matrix Chain Product Problem is an optimization problem for finding parentheses of the matrix chain that gives the minimum total number of multiplications necessary to compute the product of the matrix chain. It is well known that this problem can be solved using the dynamic programming technique in O(n3) time using tables of size O(n2). The main contribution of this paper is to present an efficient parallel implementation of this O(n3)-time algorithm on the GPU. In our implementation, we have considered the architecture and programming issues of the GPU system. The experimental results show that, for a chain of 16384 matrices generated at random, our implementation in the Nvidia GeForce GTX 480 achieves a speedup factor of 40 over a conventional CPU implementation.},
	booktitle = {2011 {Second} {International} {Conference} on {Networking} and {Computing}},
	author = {Nishida, Kazufumi and Ito, Yasuaki and Nakano, Koji},
	month = nov,
	year = {2011},
	keywords = {Arrays, CUDA, Dynamic Programming, Dynamic programming, GPGPU, Graphics processing unit, Heuristic algorithms, Instruction sets, Kernel, Matrix Chain Product, Parallel Processing},
	pages = {320--326},
}

@inproceedings{shyamala_design_2017,
	title = {Design and implementation of {GPU}-based matrix chain multiplication using {C}++{AMP}},
	doi = {10.1109/ICECCT.2017.8117870},
	abstract = {Matrix Chain Multiplication is one of the optimization problems widely used in graph algorithms, signal processing and network industry. The Matrix Chain multiplication is the process of multiplying number of nxn matrices cumulatively, where n is the size of the matrix. In this paper, the focus of the work is to accelerate the computation time for matrix chain multiplication with the help of high performance C++ Accelerated Massive Parallelism (AMP) code for Graphic Processing Units (GPUs). The proposed work is implemented in C++ AMP and tested with matrix sizes varying from 100 to 1000 for chain multiplication of three and four matrices. The experimental results show that, the speed up achieved through GPU implementation is 97.27\% when compared with Sequential implementation of the matrix chain multiplication.},
	booktitle = {2017 {Second} {International} {Conference} on {Electrical}, {Computer} and {Communication} {Technologies} ({ICECCT})},
	author = {Shyamala, K. and Kiran, K. Raj and Rajeshwari, D.},
	month = feb,
	year = {2017},
	keywords = {Acceleration, Algorithm design and analysis, C++ AMP, C++ languages, GPU Architecture, Graphics Processing Unit, Graphics processing units, Matrix Chain Multiplication, Parallel processing, SIMD, Signal processing algorithms},
	pages = {1--6},
}

@phdthesis{huang_practical_2018,
	type = {Thesis},
	title = {Practical fast matrix multiplication algorithms},
	url = {https://repositories.lib.utexas.edu/handle/2152/69013},
	abstract = {Matrix multiplication is a core building block for numerous scientific computing and, more recently, machine learning applications. Strassen's algorithm, the original Fast Matrix Multiplication (FMM) algorithm, has long fascinated computer scientists due to its startling property of reducing the number of computations required for multiplying n x n matrices from O(n³) to O(n [superscript 2.807]). Over the last half century, this has fueled many theoretical improvements such as other variations of Strassen-like FMM algorithms. Previous implementations of these FMM algorithms led to the "street wisdom" that they are only practical for large, relatively square matrices, that they require considerable workspace, and that they are difficult to achieve thread-level parallelism. The thesis of this work dispels these notions by demonstrating significant benefits for small and non-square matrices, requiring no workspace beyond what is already incorporated in high-performance implementations of matrix multiplication, and achieving performance benefits on multi-core, many-core, and distributed memory architectures.},
	language = {en},
	urldate = {2021-09-05},
	author = {Huang, Jianyu},
	month = oct,
	year = {2018},
	doi = {10.15781/T2V11W511},
	note = {Accepted: 2018-10-18T15:14:07Z},
}

@article{huang_strassens_2020,
	title = {Strassen's {Algorithm} {Reloaded} on {GPUs}},
	volume = {46},
	issn = {0098-3500},
	url = {https://doi.org/10.1145/3372419},
	doi = {10.1145/3372419},
	abstract = {Conventional Graphics Processing Unit (GPU) implementations of Strassen’s algorithm (Strassen) rely on the existing high-performance matrix multiplication (gemm), trading space for time. As a result, such approaches can only achieve practical speedup for relatively large, “squarish” matrices due to the extra memory overhead, and their usages are limited due to the considerable workspace. We present novel Strassen primitives for GPUs that can be composed to generate a family of Strassen algorithms. Our algorithms utilize both the memory and thread hierarchies on GPUs, reusing shared memory and register files inherited from gemm, fusing additional operations, and avoiding extra workspace. We further exploit intra- and inter-kernel parallelism by batching, streaming, and employing atomic operations. We develop a performance model for NVIDIA Volta GPUs to select the appropriate blocking parameters and predict the performance for gemm and Strassen. Overall, our 1-level Strassen can achieve up to 1.11× speedup with a crossover point as small as 1,536 compared to cublasSgemm on a NVIDIA Tesla V100 GPU. With additional workspace, our 2-level Strassen can achieve 1.19× speedup with a crossover point at 7,680.},
	number = {1},
	urldate = {2021-09-05},
	journal = {ACM Transactions on Mathematical Software},
	author = {Huang, Jianyu and Yu, Chenhan D. and Geijn, Robert A. van de},
	month = mar,
	year = {2020},
	keywords = {GEMM, GPU, Strassen, Volta, high-performance computing, linear algebra, matrix multiplication, performance optimization},
	pages = {1:1--1:22},
}

@article{huang_implementing_2018,
	title = {Implementing {Strassen}'s {Algorithm} with {CUTLASS} on {NVIDIA} {Volta} {GPUs}},
	url = {http://arxiv.org/abs/1808.07984},
	abstract = {Conventional GPU implementations of Strassen's algorithm (Strassen) typically rely on the existing high-performance matrix multiplication (GEMM), trading space for time. As a result, such approaches can only achieve practical speedup for relatively large, "squarish" matrices due to the extra memory overhead, and their usages are limited due to the considerable workspace. We present novel Strassen primitives for GPUs that can be composed to generate a family of Strassen algorithms. Our algorithms utilize both the memory and thread hierarchies on GPUs, reusing shared memory and register files inherited from GEMM, fusing additional operations, and avoiding extra workspace. We further exploit intra- and inter-kernel parallelism by batching, streaming, and employing atomic operations. We also develop a performance model for NVIDIA Volta GPUs to select the appropriate blocking parameters and predict the performance for GEMM and Strassen. Overall, our 1-level Strassen can achieve up to 1.11x speedup with a crossover point as small as 1,536 compared to cublasSgemm on a NVIDIA Tesla V100 GPU. With additional workspace, our 2-level Strassen can achieve 1.19x speedup with a crossover point at 7,680.},
	urldate = {2021-09-05},
	journal = {arXiv:1808.07984 [cs]},
	author = {Huang, Jianyu and Yu, Chenhan D. and van de Geijn, Robert A.},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.07984},
	keywords = {Computer Science - Mathematical Software},
}

@inproceedings{springer_hptt_2017,
	address = {New York, NY, USA},
	series = {{ARRAY} 2017},
	title = {{HPTT}: a high-performance tensor transposition {C}++ library},
	isbn = {978-1-4503-5069-3},
	shorttitle = {{HPTT}},
	url = {https://doi.org/10.1145/3091966.3091968},
	doi = {10.1145/3091966.3091968},
	abstract = {Recently we presented TTC, a domain-specific compiler for tensor transpositions. Despite the fact that the performance of the generated code is nearly optimal, due to its offline nature, TTC cannot be utilized in all the application codes in which the tensor sizes and the necessary tensor permutations are determined at runtime. To overcome this limitation, we introduce the open-source C++ library High-Performance Tensor Transposition (HPTT). Similar to TTC, HPTT incorporates optimizations such as blocking, multi-threading, and explicit vectorization; furthermore it decomposes any transposition into multiple loops around a so called micro-kernel. This modular design-inspired by BLIS-makes HPTT easy to port to different architectures, by only replacing the hand-vectorized micro-kernel (e.g.,a 4 x 4 transpose). HPTT also offers an optional autotuning framework-guided by performance heuristics-that explores a vast search space of implementations at runtime (similar to FFTW). Across a wide range of different tensor transpositions and architectures (e.g., Intel Ivy Bridge, ARMv7, IBM Power7), HPTT attains a bandwidth comparable to that of SAXPY, and yields remarkable speedups over Eigen's tensor transposition implementation. Most importantly, the integration of HPTT into the Cyclops Tensor Framework (CTF) improves the overall performance of tensor contractions by up to 3.1x.},
	urldate = {2021-09-05},
	booktitle = {Proceedings of the 4th {ACM} {SIGPLAN} {International} {Workshop} on {Libraries}, {Languages}, and {Compilers} for {Array} {Programming}},
	publisher = {Association for Computing Machinery},
	author = {Springer, Paul and Su, Tong and Bientinesi, Paolo},
	month = jun,
	year = {2017},
	keywords = {High-Performance Computing, autotuning, multidimensional transposition, tensor transposition, tensors, vectorization},
	pages = {56--62},
}

@article{verdoolaege_polyhedral_2013,
	title = {Polyhedral parallel code generation for {CUDA}},
	volume = {9},
	issn = {1544-3566},
	url = {https://doi.org/10.1145/2400682.2400713},
	doi = {10.1145/2400682.2400713},
	abstract = {This article addresses the compilation of a sequential program for parallel execution on a modern GPU. To this end, we present a novel source-to-source compiler called PPCG. PPCG singles out for its ability to accelerate computations from any static control loop nest, generating multiple CUDA kernels when necessary. We introduce a multilevel tiling strategy and a code generation scheme for the parallelization and locality optimization of imperfectly nested loops, managing memory and exposing concurrency according to the constraints of modern GPUs. We evaluate our algorithms and tool on the entire PolyBench suite.},
	number = {4},
	urldate = {2021-09-05},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Verdoolaege, Sven and Carlos Juega, Juan and Cohen, Albert and Ignacio Gómez, José and Tenllado, Christian and Catthoor, Francky},
	month = jan,
	year = {2013},
	keywords = {C-to-CUDA, CUDA, GPU, PPCG., Par4All, Polyhedral model, code generation, compilers, loop transformations},
	pages = {54:1--54:23},
}

@article{vasilache_tensor_2018,
	title = {Tensor {Comprehensions}: {Framework}-{Agnostic} {High}-{Performance} {Machine} {Learning} {Abstractions}},
	shorttitle = {Tensor {Comprehensions}},
	url = {http://arxiv.org/abs/1802.04730},
	abstract = {Deep learning models with convolutional and recurrent networks are now ubiquitous and analyze massive amounts of audio, image, video, text and graph data, with applications in automatic translation, speech-to-text, scene understanding, ranking user preferences, ad placement, etc. Competing frameworks for building these networks such as TensorFlow, Chainer, CNTK, Torch/PyTorch, Caffe1/2, MXNet and Theano, explore different tradeoffs between usability and expressiveness, research or production orientation and supported hardware. They operate on a DAG of computational operators, wrapping high-performance libraries such as CUDNN (for NVIDIA GPUs) or NNPACK (for various CPUs), and automate memory allocation, synchronization, distribution. Custom operators are needed where the computation does not fit existing high-performance library calls, usually at a high engineering cost. This is frequently required when new operators are invented by researchers: such operators suffer a severe performance penalty, which limits the pace of innovation. Furthermore, even if there is an existing runtime call these frameworks can use, it often doesn't offer optimal performance for a user's particular network architecture and dataset, missing optimizations between operators as well as optimizations that can be done knowing the size and shape of data. Our contributions include (1) a language close to the mathematics of deep learning called Tensor Comprehensions, (2) a polyhedral Just-In-Time compiler to convert a mathematical description of a deep learning DAG into a CUDA kernel with delegated memory management and synchronization, also providing optimizations such as operator fusion and specialization for specific sizes, (3) a compilation cache populated by an autotuner. [Abstract cutoff]},
	urldate = {2021-09-05},
	journal = {arXiv:1802.04730 [cs]},
	author = {Vasilache, Nicolas and Zinenko, Oleksandr and Theodoridis, Theodoros and Goyal, Priya and DeVito, Zachary and Moses, William S. and Verdoolaege, Sven and Adams, Andrew and Cohen, Albert},
	month = jun,
	year = {2018},
	note = {arXiv: 1802.04730},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@inproceedings{kim_code_2019,
	title = {A {Code} {Generator} for {High}-{Performance} {Tensor} {Contractions} on {GPUs}},
	doi = {10.1109/CGO.2019.8661182},
	abstract = {Tensor contractions are higher dimensional generalizations of matrix-matrix multiplication. They form the compute-intensive core of many applications in computational science and data science. In this paper, we describe a high-performance GPU code generator for arbitrary tensor contractions. It exploits domain-specific properties about data reuse in tensor contractions to devise an effective code generation schema, coupled with an effective model-driven search, to determine parameters for mapping of computation to threads and staging of data through the GPU memory hierarchy. Experimental evaluation using a set of tensor contraction benchmarks demonstrates performance improvement and/or significantly reduced code generation time over other state-of-the-art tensor contraction libraries and code generators.},
	booktitle = {2019 {IEEE}/{ACM} {International} {Symposium} on {Code} {Generation} and {Optimization} ({CGO})},
	author = {Kim, Jinsung and Sukumaran-Rajam, Aravind and Thumma, Vineeth and Krishnamoorthy, Sriram and Panyala, Ajay and Pouchet, Louis-Noël and Rountev, Atanas and Sadayappan, P.},
	month = feb,
	year = {2019},
	keywords = {Code Generation, GPU Computing, Generators, Graphics processing units, Indexes, Instruction sets, Libraries, Registers, Tensor Contractions},
	pages = {85--95},
}

@misc{intel_corporation_ponte_2021,
	title = {Ponte {Vecchio} - {Architecture} {Day}},
	url = {https://download.intel.com/newsroom/2021/client-computing/intel-architecture-day-2021-presentation.pdf},
	language = {en-us},
	urldate = {2021-06-06},
	journal = {Architecture Day},
	author = {Intel Corporation},
	year = {2021},
}

@article{liang_fast_2021,
	title = {Fast {Search} of the {Optimal} {Contraction} {Sequence} in {Tensor} {Networks}},
	volume = {15},
	issn = {1941-0484},
	doi = {10.1109/JSTSP.2021.3051231},
	abstract = {Tensor network and tensor computation are widely applied in scientific and engineering domains like quantum physics, electronic design automation, and machine learning. As one of the most fundamental operations for tensor networks, a tensor contraction eliminates the sharing orders among tensors and produces a compact sub-network. Different contraction sequence usually yields distinct storage and compute costs, and searching the optimal sequence is known as a hard problem. Prior work have designed heuristic and fast algorithms to solve this problem, however, several issues still remain unsolved. For example, the data format and data structure are not efficient, the constraints during modeling are impractical, the search of the optimal solution might fail, and the search cost is very high. In this paper, we first introduce a logk order representation and design an adjacency matrix-based data structure to efficiently accelerate the search of the optimal contraction sequence. Then, we propose an outer product pruning method with acceptable overhead to reduce the search space. Finally, we use a multithread optimization in our implementation to further improve the execution performance. We also present in-depth analysis of factors that influence the search time. This work provides a full-stack solution for optimal contraction sequence search from both high-level data structure and search algorithm to low-level execution parallelism, and it will benefit a broad range of tensor-related applications.},
	number = {3},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Liang, Ling and Xu, Jianyu and Deng, Lei and Yan, Mingyu and Hu, Xing and Zhang, Zheng and Li, Guoqi and Xie, Yuan},
	month = apr,
	year = {2021},
	note = {Conference Name: IEEE Journal of Selected Topics in Signal Processing},
	keywords = {Acceleration, BFS algorithm, Data structures, Heuristic algorithms, Optimization, Physics, Signal processing algorithms, Tensor contraction, Tensors, adjacency matrix, multithread optimization, search space reduction},
	pages = {574--586},
}

@article{markov_quantum_2018,
	title = {Quantum {Supremacy} {Is} {Both} {Closer} and {Farther} than {It} {Appears}},
	url = {http://arxiv.org/abs/1807.10749},
	abstract = {As quantum computers improve in the number of qubits and fidelity, the question of when they surpass state-of-the-art classical computation for a well-defined computational task is attracting much attention. The leading candidate task for this milestone entails sampling from the output distribution defined by a random quantum circuit. We develop a massively-parallel simulation tool Rollright that does not require inter-process communication (IPC) or proprietary hardware. We also develop two ways to trade circuit fidelity for computational speedups, so as to match the fidelity of a given quantum computer --- a task previously thought impossible. We report massive speedups for the sampling task over prior software from Microsoft, IBM, Alibaba and Google, as well as supercomputer and GPU-based simulations. By using publicly available Google Cloud Computing, we price such simulations and enable comparisons by total cost across hardware platforms. We simulate approximate sampling from the output of a circuit with 7x8 qubits and depth 1+40+1 by producing one million bitstring probabilities with fidelity 0.5\%, at an estimated cost of \$35184. The simulation costs scale linearly with fidelity, and using this scaling we estimate that extending circuit depth to 1+48+1 increases costs to one million dollars. Scaling the simulation to 10M bitstring probabilities needed for sampling 1M bitstrings helps comparing simulation to quantum computers. We describe refinements in benchmarks that slow down leading simulators, halving the circuit depth that can be simulated within the same time.},
	urldate = {2021-08-29},
	journal = {arXiv:1807.10749 [quant-ph]},
	author = {Markov, Igor L. and Fatima, Aneeqa and Isakov, Sergei V. and Boixo, Sergio},
	month = sep,
	year = {2018},
	note = {arXiv: 1807.10749},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Emerging Technologies, Quantum Physics},
}

@article{huang_strassens_2018,
	title = {Strassen's {Algorithm} for {Tensor} {Contraction}},
	volume = {40},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/17M1135578},
	doi = {10.1137/17M1135578},
	abstract = {Tensor contraction (TC) is an important computational kernel widely used in numerous applications. It is a multidimensional generalization of matrix multiplication (GEMM). While Strassen's algorithm for GEMM is well studied in theory and practice, extending it to accelerate TC has not been previously pursued. Thus, we believe this to be the first paper to demonstrate how one can in practice speed up TC with Strassen's algorithm. By adopting a block-scatter-matrix format, a novel matrix-centric tensor layout, we can conceptually view TC as GEMM for a general stride storage, with an implicit tensor-to-matrix transformation. This insight enables us to tailor a recent state-of-the-art implementation of Strassen's algorithm to a recent state-of-the-art TC, avoiding explicit transpositions (permutations) and extra workspace, and reducing the overhead of memory movement that is incurred. Performance benefits are demonstrated with a performance model as well as in practice on modern single core, multicore, and distributed memory parallel architectures, achieving up to \$1.3 {\textbackslash}times\$ speedup. The resulting implementations can serve as a drop-in replacement for various applications with significant speedup.},
	number = {3},
	urldate = {2021-08-22},
	journal = {SIAM Journal on Scientific Computing},
	author = {Huang, Jianyu and Matthews, Devin A. and van de Geijn, Robert A.},
	month = jan,
	year = {2018},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {65F99, Strassen's algorithm, matrix multiplication, multilinear algebra, tensor contraction},
	pages = {C305--C326},
}

@article{dudek_parallel_2021,
	title = {Parallel {Weighted} {Model} {Counting} with {Tensor} {Networks}},
	url = {http://arxiv.org/abs/2006.15512},
	abstract = {A promising new algebraic approach to weighted model counting makes use of tensor networks, following a reduction from weighted model counting to tensor-network contraction. Prior work has focused on analyzing the single-core performance of this approach, and demonstrated that it is an effective addition to the current portfolio of weighted-model-counting algorithms. In this work, we explore the impact of multi-core and GPU use on tensor-network contraction for weighted model counting. To leverage multiple cores, we implement a parallel portfolio of tree-decomposition solvers to find an order to contract tensors. To leverage a GPU, we use TensorFlow to perform the contractions. We compare the resulting weighted model counter on 1914 standard weighted model counting benchmarks and show that it significantly improves the virtual best solver.},
	urldate = {2021-08-17},
	journal = {arXiv:2006.15512 [cs]},
	author = {Dudek, Jeffrey M. and Vardi, Moshe Y.},
	month = jun,
	year = {2021},
	note = {arXiv: 2006.15512},
	keywords = {Computer Science - Data Structures and Algorithms},
}

@misc{son_graph_2019,
	title = {‪{Graph} neural networks with efficient tensor operations in cuda/gpu and graphflow deep learning framework in c++ for quantum chemistry‬},
	url = {https://scholar.google.com/citations?view_op=view_citation&hl=iw&user=xEXxSN4AAAAJ&citation_for_view=xEXxSN4AAAAJ:RYcK_YlVTxYC},
	urldate = {2021-08-17},
	author = {Son, Truong and Jones, Chris},
	year = {2019},
}

@book{ran_tensor_2020,
	series = {Lecture {Notes} in {Physics}},
	title = {Tensor {Network} {Contractions}: {Methods} and {Applications} to {Quantum} {Many}-{Body} {Systems}},
	isbn = {978-3-030-34488-7},
	shorttitle = {Tensor {Network} {Contractions}},
	url = {https://www.springer.com/gp/book/9783030344887},
	abstract = {Tensor network is a fundamental mathematical tool with a huge range of applications in physics, such as condensed matter physics, statistic physics, high energy physics, and quantum information sciences. This open access book aims to explain the tensor network contraction approaches in a systematic way, from the basic definitions to the important applications. This book is also useful to those who apply tensor networks in areas beyond physics, such as machine learning and the big-data analysis. Tensor network originates from the numerical renormalization group approach proposed by K. G. Wilson in 1975. Through a rapid development in the last two decades, tensor network has become a powerful numerical tool that can efficiently simulate a wide range of scientific problems, with particular success in quantum many-body physics. Varieties of tensor network algorithms have been proposed for different problems. However, the connections among different algorithms are not well discussed or reviewed. To fill this gap, this book explains the fundamental concepts and basic ideas that connect and/or unify different strategies of the tensor network contraction algorithms. In addition, some of the recent progresses in dealing with tensor decomposition techniques and quantum simulations are also represented in this book to help the readers to better understand tensor network. This open access book is intended for graduated students, but can also be used as a professional book for researchers in the related fields. To understand most of the contents in the book, only basic knowledge of quantum mechanics and linear algebra is required. In order to fully understand some advanced parts, the reader will need to be familiar with notion of condensed matter physics and quantum information, that however are not necessary to understand the main parts of the book. This book is a good source for non-specialists on quantum physics to understand tensor network algorithms and the related mathematics.},
	language = {en},
	urldate = {2021-08-17},
	publisher = {Springer International Publishing},
	author = {Ran, Shi-Ju and Tirrito, Emanuele and Peng, Cheng and Chen, Xi and Tagliacozzo, Luca and Su, Gang and Lewenstein, Maciej},
	year = {2020},
	doi = {10.1007/978-3-030-34489-4},
}

@article{gray_hyper-optimized_2021,
	title = {Hyper-optimized tensor network contraction},
	volume = {5},
	issn = {2521-327X},
	url = {http://arxiv.org/abs/2002.01935},
	doi = {10.22331/q-2021-03-15-410},
	abstract = {Tensor networks represent the state-of-the-art in computational methods across many disciplines, including the classical simulation of quantum many-body systems and quantum circuits. Several applications of current interest give rise to tensor networks with irregular geometries. Finding the best possible contraction path for such networks is a central problem, with an exponential effect on computation time and memory footprint. In this work, we implement new randomized protocols that find very high quality contraction paths for arbitrary and large tensor networks. We test our methods on a variety of benchmarks, including the random quantum circuit instances recently implemented on Google quantum chips. We find that the paths obtained can be very close to optimal, and often many orders or magnitude better than the most established approaches. As different underlying geometries suit different methods, we also introduce a hyper-optimization approach, where both the method applied and its algorithmic parameters are tuned during the path finding. The increase in quality of contraction schemes found has significant practical implications for the simulation of quantum many-body systems and particularly for the benchmarking of new quantum chips. Concretely, we estimate a speed-up of over 10,000\${\textbackslash}times\$ compared to the original expectation for the classical simulation of the Sycamore `supremacy' circuits.},
	urldate = {2021-08-17},
	journal = {Quantum},
	author = {Gray, Johnnie and Kourtis, Stefanos},
	month = mar,
	year = {2021},
	note = {arXiv: 2002.01935},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Physics - Computational Physics, Quantum Physics},
	pages = {410},
}

@article{matthews_high-performance_2018,
	title = {High-{Performance} {Tensor} {Contraction} without {Transposition}},
	volume = {40},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/16M108968X},
	doi = {10.1137/16M108968X},
	abstract = {Tensor computations---in particular tensor contraction (TC)---are important kernels in many scientific computing applications. Due to the fundamental similarity of TC to matrix multiplication and to the availability of optimized implementations such as the BLAS, tensor operations have traditionally been implemented in terms of BLAS operations, incurring both a performance and a storage overhead. Instead, we implement TC using the flexible BLAS-like Instantiation Software (BLIS) framework, which allows for transposition (reshaping) of the tensor to be fused with internal partitioning and packing operations, requiring no explicit transposition operations or additional workspace. This implementation, TBLIS, achieves performance approaching that of matrix multiplication, and in some cases considerably higher than that of traditional TC. Our implementation supports multithreading using an approach identical to that used for matrix multiplication in BLIS, with similar performance characteristics. The complexity of managing tensor-to-matrix transformations is also handled automatically in our approach, greatly simplifying its use in scientific applications.},
	number = {1},
	urldate = {2021-08-17},
	journal = {SIAM Journal on Scientific Computing},
	author = {Matthews, Devin A.},
	month = jan,
	year = {2018},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {15A69, high-performance computing, matrix multiplication, multilinear algebra, tensor contraction},
	pages = {C1--C24},
}

@article{barratt_parallel_2021,
	title = {Parallel quantum simulation of large systems on small {NISQ} computers},
	volume = {7},
	copyright = {2021 The Author(s)},
	issn = {2056-6387},
	url = {https://www.nature.com/articles/s41534-021-00420-3},
	doi = {10.1038/s41534-021-00420-3},
	abstract = {Tensor networks permit computational and entanglement resources to be concentrated in interesting regions of Hilbert space. Implemented on NISQ machines they allow simulation of quantum systems that are much larger than the computational machine itself. This is achieved by parallelising the quantum simulation. Here, we demonstrate this in the simplest case; an infinite, translationally invariant quantum spin chain. We provide Cirq and Qiskit code that translates infinite, translationally invariant matrix product state (iMPS) algorithms to finite-depth quantum circuit machines, allowing the representation, optimisation and evolution of arbitrary one-dimensional systems. The illustrative simulated output of these codes for achievable circuit sizes is given.},
	language = {en},
	number = {1},
	urldate = {2021-08-17},
	journal = {npj Quantum Information},
	author = {Barratt, F. and Dborin, James and Bal, Matthias and Stojevic, Vid and Pollmann, Frank and Green, A. G.},
	month = may,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Information theory and computation;Quantum simulation
Subject\_term\_id: information-theory-and-computation;quantum-simulation},
	pages = {1--7},
}

@inproceedings{herault_distributed-memory_2021,
	title = {Distributed-memory multi-{GPU} block-sparse tensor contraction for electronic structure},
	doi = {10.1109/IPDPS49936.2021.00062},
	abstract = {Many domains of scientific simulation (chemistry, condensed matter physics, data science) increasingly eschew dense tensors for block-sparse tensors, sometimes with additional structure (recursive hierarchy, rank sparsity, etc.). Distributed-memory parallel computation with block-sparse tensorial data is paramount to minimize the time-to-solution (e.g., to study dynamical problems or for real-time analysis) and to accommodate problems of realistic size that are too large to fit into the host/device memory of a single node equipped with accelerators. Unfortunately, computation with such irregular data structures is a poor match to the dominant imperative, bulk-synchronous parallel programming model. In this paper, we focus on the critical element of block-sparse tensor algebra, namely binary tensor contraction, and report on an efficient and scalable implementation using the task-focused PaRSEC runtime. High performance of the block-sparse tensor contraction on the Summit supercomputer is demonstrated for synthetic data as well as for real data involved in electronic structure simulations of unprecedented size.},
	booktitle = {2021 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Herault, Thomas and Robert, Yves and Bosilca, George and Harrison, Robert J. and Lewis, Cannada A. and Valeev, Edward F. and Dongarra, Jack J.},
	month = may,
	year = {2021},
	note = {ISSN: 1530-2075},
	keywords = {Computational modeling, Data models, PaRSEC, Real-time systems, Runtime, Supercomputers, Tensors, Tools, block-sparse matrix multiplication, distributed memory, electronic structure, multi-GPU nodes, tensor contraction},
	pages = {537--546},
}

@article{gray_hyper-optimized_2021-1,
	title = {Hyper-optimized tensor network contraction},
	volume = {5},
	url = {https://quantum-journal.org/papers/q-2021-03-15-410/},
	doi = {10.22331/q-2021-03-15-410},
	abstract = {Johnnie Gray and Stefanos Kourtis,
Quantum 5, 410 (2021).
Tensor networks represent the state-of-the-art in computational methods across many disciplines, including the classical simulation of quantum many-body systems and quantum circuits. Several…},
	language = {en-GB},
	urldate = {2021-08-17},
	journal = {Quantum},
	author = {Gray, Johnnie and Kourtis, Stefanos},
	month = mar,
	year = {2021},
	note = {Publisher: Verein zur Förderung des Open Access Publizierens in den Quantenwissenschaften},
	pages = {410},
}

@inproceedings{liu_sparta_2021,
	address = {New York, NY, USA},
	series = {{PPoPP} '21},
	title = {Sparta: high-performance, element-wise sparse tensor contraction on heterogeneous memory},
	isbn = {978-1-4503-8294-6},
	shorttitle = {Sparta},
	url = {https://doi.org/10.1145/3437801.3441581},
	doi = {10.1145/3437801.3441581},
	abstract = {Sparse tensor contractions appear commonly in many applications. Efficiently computing a two sparse tensor product is challenging: It not only inherits the challenges from common sparse matrix-matrix multiplication (SpGEMM), i.e., indirect memory access and unknown output size before computation, but also raises new challenges because of high dimensionality of tensors, expensive multi-dimensional index search, and massive intermediate and output data. To address the above challenges, we introduce three optimization techniques by using multi-dimensional, efficient hashtable representation for the accumulator and larger input tensor, and all-stage parallelization. Evaluating with 15 datasets, we show that Sparta brings 28 -- 576× speedup over the traditional sparse tensor contraction with sparse accumulator. With our proposed algorithm- and memory heterogeneity-aware data management, Sparta brings extra performance improvement on the heterogeneous memory with DRAM and Intel Optane DC Persistent Memory Module (PMM) over a state-of-the-art software-based data management solution, a hardware-based data management solution, and PMM-only by 30.7\% (up to 98.5\%), 10.7\% (up to 28.3\%) and 17\% (up to 65.1\%) respectively.},
	urldate = {2021-08-17},
	booktitle = {Proceedings of the 26th {ACM} {SIGPLAN} {Symposium} on {Principles} and {Practice} of {Parallel} {Programming}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Jiawen and Ren, Jie and Gioiosa, Roberto and Li, Dong and Li, Jiajia},
	month = feb,
	year = {2021},
	keywords = {heterogeneous memory, multicore CPU, non-volatile memory, sparse tensor contraction, tensor product},
	pages = {318--333},
}

@inproceedings{gao_performance_2005,
	address = {New York, NY, USA},
	series = {{PPoPP} '05},
	title = {Performance modeling and optimization of parallel out-of-core tensor contractions},
	isbn = {978-1-59593-080-4},
	url = {https://doi.org/10.1145/1065944.1065980},
	doi = {10.1145/1065944.1065980},
	abstract = {The Tensor Contraction Engine (TCE) is a domain-specific compiler for implementing complex tensor contraction expressions arising in quantum chemistry applications modeling electronic structure. This paper develops a performance model for tensor contractions, considering both disk I/O as well as inter-processor communication costs, to facilitate performance-model driven loop optimization for this domain. Experimental results are provided that demonstrate the accuracy and effectiveness of the model.},
	urldate = {2021-08-17},
	booktitle = {Proceedings of the tenth {ACM} {SIGPLAN} symposium on {Principles} and practice of parallel programming},
	publisher = {Association for Computing Machinery},
	author = {Gao, Xiaoyang and Sahoo, Swarup Kumar and Lam, Chi-Chung and Ramanujam, J. and Lu, Qingda and Baumgartner, Gerald and Sadayappan, P.},
	month = jun,
	year = {2005},
	keywords = {compiler optimization, out-of-core algorithms, parallel algorithms, performance modeling},
	pages = {266--276},
}

@misc{noauthor_parallel_2020,
	title = {A {Parallel} {Tensor} {Network} {Contraction} {Algorithm} and {Its} {Applications} in {Quantum} {Computation}},
	url = {https://deepblue.lib.umich.edu/handle/2027.42/163098},
	urldate = {2021-08-17},
	year = {2020},
}

@article{schutski_simple_2020,
	title = {Simple heuristics for efficient parallel tensor contraction and quantum circuit simulation},
	volume = {102},
	issn = {2469-9926, 2469-9934},
	url = {http://arxiv.org/abs/2004.10892},
	doi = {10.1103/PhysRevA.102.062614},
	abstract = {Tensor networks are the main building blocks in a wide variety of computational sciences, ranging from many-body theory and quantum computing to probability and machine learning. Here we propose a parallel algorithm for the contraction of tensor networks using probabilistic graphical models. Our approach is based on the heuristic solution of the \${\textbackslash}mu\$-treewidth deletion problem in graph theory. We apply the resulting algorithm to the simulation of random quantum circuits and discuss the extensions for general tensor network contractions.},
	number = {6},
	urldate = {2021-08-17},
	journal = {Physical Review A},
	author = {Schutski, Roman and Kolmakov, Dmitry and Khakhulin, Taras and Oseledets, Ivan},
	month = dec,
	year = {2020},
	note = {arXiv: 2004.10892},
	keywords = {Computer Science - Discrete Mathematics, Quantum Physics},
	pages = {062614},
}

@article{solomonik_massively_2014,
	series = {Domain-{Specific} {Languages} and {High}-{Level} {Frameworks} for {High}-{Performance} {Computing}},
	title = {A massively parallel tensor contraction framework for coupled-cluster computations},
	volume = {74},
	issn = {0743-7315},
	url = {https://www.sciencedirect.com/science/article/pii/S074373151400104X},
	doi = {10.1016/j.jpdc.2014.06.002},
	abstract = {Precise calculation of molecular electronic wavefunctions by methods such as coupled-cluster requires the computation of tensor contractions, the cost of which has polynomial computational scaling with respect to the system and basis set sizes. Each contraction may be executed via matrix multiplication on a properly ordered and structured tensor. However, data transpositions are often needed to reorder the tensors for each contraction. Writing and optimizing distributed-memory kernels for each transposition and contraction is tedious since the number of contractions scales combinatorially with the number of tensor indices. We present a distributed-memory numerical library (Cyclops Tensor Framework (CTF)) that automatically manages tensor blocking and redistribution to perform any user-specified contractions. CTF serves as the distributed-memory contraction engine in Aquarius, a new program designed for high-accuracy and massively-parallel quantum chemical computations. Aquarius implements a range of coupled-cluster and related methods such as CCSD and CCSDT by writing the equations on top of a C++ templated domain-specific language. This DSL calls CTF directly to manage the data and perform the contractions. Our CCSD and CCSDT implementations achieve high parallel scalability on the BlueGene/Q and Cray XC30 supercomputer architectures showing that accurate electronic structure calculations can be effectively carried out on top of general distributed-memory tensor primitives.},
	language = {en},
	number = {12},
	urldate = {2021-08-17},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Solomonik, Edgar and Matthews, Devin and Hammond, Jeff R. and Stanton, John F. and Demmel, James},
	month = dec,
	year = {2014},
	keywords = {Communication-avoiding algorithms, Coupled-cluster, Matrix multiplication, Tensor contractions, Topology-aware mapping},
	pages = {3176--3190},
}

@article{poya_high_2017,
	title = {A high performance data parallel tensor contraction framework: {Application} to coupled electro-mechanics},
	volume = {216},
	issn = {0010-4655},
	shorttitle = {A high performance data parallel tensor contraction framework},
	url = {https://www.sciencedirect.com/science/article/pii/S0010465517300681},
	doi = {10.1016/j.cpc.2017.02.016},
	abstract = {The paper presents aspects of implementation of a new high performance tensor contraction framework for the numerical analysis of coupled and multi-physics problems on streaming architectures. In addition to explicit SIMD instructions and smart expression templates, the framework introduces domain specific constructs for the tensor cross product and its associated algebra recently rediscovered by Bonet et al. (2015, 2016) in the context of solid mechanics. The two key ingredients of the presented expression template engine are as follows. First, the capability to mathematically transform complex chains of operations to simpler equivalent expressions, while potentially avoiding routes with higher levels of computational complexity and, second, to perform a compile time depth-first or breadth-first search to find the optimal contraction indices of a large tensor network in order to minimise the number of floating point operations. For optimisations of tensor contraction such as loop transformation, loop fusion and data locality optimisations, the framework relies heavily on compile time technologies rather than source-to-source translation or JIT techniques. Every aspect of the framework is examined through relevant performance benchmarks, including the impact of data parallelism on the performance of isomorphic and nonisomorphic tensor products, the FLOP and memory I/O optimality in the evaluation of tensor networks, the compilation cost and memory footprint of the framework and the performance of tensor cross product kernels. The framework is then applied to finite element analysis of coupled electro-mechanical problems to assess the speed-ups achieved in kernel-based numerical integration of complex electroelastic energy functionals. In this context, domain-aware expression templates combined with SIMD instructions are shown to provide a significant speed-up over the classical low-level style programming techniques.},
	language = {en},
	urldate = {2021-08-17},
	journal = {Computer Physics Communications},
	author = {Poya, Roman and Gil, Antonio J. and Ortigosa, Rogelio},
	month = jul,
	year = {2017},
	keywords = {Data parallelism, Domain-aware expression templates, Nonlinear coupled electro-mechanics, Tensor contraction},
	pages = {35--52},
}

@inproceedings{springer_ttc_2016,
	address = {New York, NY, USA},
	series = {{ARRAY} 2016},
	title = {{TTC}: a tensor transposition compiler for multiple architectures},
	isbn = {978-1-4503-4384-8},
	shorttitle = {{TTC}},
	url = {https://doi.org/10.1145/2935323.2935328},
	doi = {10.1145/2935323.2935328},
	abstract = {We consider the problem of transposing tensors of arbitrary dimension and describe TTC, an open source domain-specific parallel compiler. TTC generates optimized parallel C++/CUDA C code that achieves a significant fraction of the system's peak memory bandwidth. TTC exhibits high performance across multiple architectures, including modern AVX-based systems (e.g., Intel Haswell, AMD Steamroller), Intel's Knights Corner as well as different CUDA-based GPUs such as NVIDIA's Kepler and Maxwell architectures. We report speedups of TTC over a meaningful baseline implementation generated by external C++ compilers; the results suggest that a domain-specific compiler can outperform its general purpose counterpart significantly: For instance, comparing with Intel's latest C++ compiler on the Haswell and Knights Corner architecture, TTC yields speedups of up to 8x and 32x, respectively. We also showcase TTC's support for multiple leading dimensions, making it a suitable candidate for the generation of performance-critical packing functions that are at the core of the ubiquitous BLAS 3 routines.},
	urldate = {2021-08-17},
	booktitle = {Proceedings of the 3rd {ACM} {SIGPLAN} {International} {Workshop} on {Libraries}, {Languages}, and {Compilers} for {Array} {Programming}},
	publisher = {Association for Computing Machinery},
	author = {Springer, Paul and Sankaran, Aravind and Bientinesi, Paolo},
	month = jun,
	year = {2016},
	keywords = {SIMD, high-performance computing, multidimensional transpositions, omain-specific compiler, tensors},
	pages = {41--46},
}

@inproceedings{beams_high-order_2020,
	title = {High-{Order} {Finite} {Element} {Method} using {Standard} and {Device}-{Level} {Batch} {GEMM} on {GPUs}},
	doi = {10.1109/ScalA51936.2020.00012},
	abstract = {The following topics are dealt with: graphics processing units; mathematics computing; iterative methods; parallel machines; parallel architectures; matrix multiplication; floating point arithmetic; parallel processing; partial differential equations; multiprocessing systems.},
	booktitle = {2020 {IEEE}/{ACM} 11th {Workshop} on {Latest} {Advances} in {Scalable} {Algorithms} for {Large}-{Scale} {Systems} ({ScalA})},
	author = {Beams, Natalie and Abdelfattah, Ahmad and Tomov, Stan and Dongarra, Jack and Kolev, Tzanio and Dudouit, Yohann},
	month = nov,
	year = {2020},
	keywords = {Finite element analysis, GPU, Graphics processing units, Instruction sets, Kernel, Libraries, Performance evaluation, Tensor contractions, Tensors, batched linear algebra, finite elements, high-order methods, matrix-free FEM},
	pages = {53--60},
}

@inproceedings{shi_tensor_2016,
	title = {Tensor {Contractions} with {Extended} {BLAS} {Kernels} on {CPU} and {GPU}},
	doi = {10.1109/HiPC.2016.031},
	abstract = {Tensor contractions constitute a key computational ingredient of numerical multi-linear algebra. However, as the order and dimension of tensors grow, the time and space complexities of tensor-based computations grow quickly. In this paper, we propose and evaluate new BLAS-like primitives that are capable of performing a wide range of tensor contractions on CPU and GPU efficiently. We begin by focusing on single-index contractions involving all the possible configurations of second-order and third-order tensors. Then, we discuss extensions to more general cases. Existing approaches for tensor contractions spend large amounts of time restructuring the data which typically involves explicit copy and transpose operations. In this work, we summarize existing approaches and present library-based approaches that avoid memory movement. Through systematic benchmarking, we demonstrate that our approach can achieve 10x speedup on a K40c GPU and 2x speedup on dual-socket Haswell-EP CPUs, using MKL and CUBLAS respectively, for small and moderate tensor sizes. This is relevant in many machine learning applications such as deep learning, where tensor sizes tend to be small, but require numerous tensor contraction operations to be performed successively. Concretely, we implement a Tucker decomposition and show that using our kernels yields atleast an order of magnitude speedup as compared to state-of-the-art libraries.},
	booktitle = {2016 {IEEE} 23rd {International} {Conference} on {High} {Performance} {Computing} ({HiPC})},
	author = {Shi, Yang and Niranjan, U. N. and Anandkumar, Animashree and Cecka, Cris},
	month = dec,
	year = {2016},
	keywords = {Algebra, BLAS, Benchmark testing, GPU, Graphics processing units, Indexes, Kernel, Libraries, Parallelism, Tensile stress, Tensor},
	pages = {193--202},
}

@article{lyakh_efficient_2015,
	title = {An efficient tensor transpose algorithm for multicore {CPU}, {Intel} {Xeon} {Phi}, and {NVidia} {Tesla} {GPU}},
	volume = {189},
	issn = {0010-4655},
	url = {https://www.osti.gov/pages/biblio/1185465},
	doi = {10.1016/j.cpc.2014.12.013},
	abstract = {An efficient parallel tensor transpose algorithm is suggested for shared-memory computing units, namely, multicore CPU, Intel Xeon Phi, and NVidia GPU. The algorithm operates on dense tensors (multidimensional arrays) and is based on the optimization of cache utilization on x86 CPU and the use of shared memory on NVidia GPU. From the applied side, the ultimate goal is to minimize the overhead encountered in the transformation of tensor contractions into matrix multiplications in computer implementations of advanced methods of quantum many-body theory (e.g., in electronic structure theory and nuclear physics). A particular accent is made on higher-dimensional tensors that typically appear in the so-called multireference correlated methods of electronic structure theory. Depending on tensor dimensionality, the presented optimized algorithms can achieve an order of magnitude speedup on x86 CPUs and 2-3 times speedup on NVidia Tesla K20X GPU with respect to the na ve scattering algorithm (no memory access optimization). Furthermore, the tensor transpose routines developed in this work have been incorporated into a general-purpose tensor algebra library (TAL-SH).},
	language = {English},
	urldate = {2021-08-17},
	journal = {Computer Physics Communications},
	author = {Lyakh, Dmitry I.},
	month = jan,
	year = {2015},
	note = {Institution: Oak Ridge National Lab. (ORNL), Oak Ridge, TN (United States). Oak Ridge Leadership Computing Facility (OLCF)
Publisher: Elsevier},
}

@article{kaliman_new_2017,
	title = {New algorithm for tensor contractions on multi-core {CPUs}, {GPUs}, and accelerators enables {CCSD} and {EOM}-{CCSD} calculations with over 1000 basis functions on a single compute node},
	volume = {38},
	issn = {1096-987X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.24713},
	doi = {10.1002/jcc.24713},
	abstract = {A new hardware-agnostic contraction algorithm for tensors of arbitrary symmetry and sparsity is presented. The algorithm is implemented as a stand-alone open-source code libxm. This code is also integrated with general tensor library libtensor and with the Q-Chem quantum-chemistry package. An overview of the algorithm, its implementation, and benchmarks are presented. Similarly to other tensor software, the algorithm exploits efficient matrix multiplication libraries and assumes that tensors are stored in a block-tensor form. The distinguishing features of the algorithm are: (i) efficient repackaging of the individual blocks into large matrices and back, which affords efficient graphics processing unit (GPU)-enabled calculations without modifications of higher-level codes; (ii) fully asynchronous data transfer between disk storage and fast memory. The algorithm enables canonical all-electron coupled-cluster and equation-of-motion coupled-cluster calculations with single and double substitutions (CCSD and EOM-CCSD) with over 1000 basis functions on a single quad-GPU machine. We show that the algorithm exhibits predicted theoretical scaling for canonical CCSD calculations, O(N6), irrespective of the data size on disk. © 2017 Wiley Periodicals, Inc.},
	language = {en},
	number = {11},
	urldate = {2021-08-17},
	journal = {Journal of Computational Chemistry},
	author = {Kaliman, Ilya A. and Krylov, Anna I.},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jcc.24713},
	keywords = {GPGPU, coupled-cluster, electronic structure, equation-of-motion coupled-cluster, many-body theories, tensor computations},
	pages = {842--853},
}

@inproceedings{deng_tensor_2018,
	title = {Tensor {Sensing} for {Rf} {Tomographic} {Imaging}},
	isbn = {978-1-5386-1737-3},
	url = {https://www.computer.org/csdl/proceedings-article/icme/2018/08486609/14jQfQyVNaU},
	doi = {10.1109/ICME.2018.8486609},
	abstract = {Radio-frequency (RF) tomographic imaging is a promising technique for inferring multi-dimensional physical space by processing RF signals traversed across a region of interest. However, conventional RF tomography schemes are generally based on vector compressed sensing, which ignores the geometric structures of the target spaces and leads to low recovery precision. The recently proposed transform-based tensor model is more appropriate for sensory data processing, as it helps exploit the geometric structures of the three-dimensional target and improve the recovery precision. In this paper, we propose a novel tensor sensing approach that achieves highly accurate estimation for real-world three-dimensional spaces. First, we use the transform-based tensor model to formulate a tensor sensing problem, and propose a fast alternating minimization algorithm called Alt-Min. Secondly, we drive an algorithm which is optimized to reduce memory and computation requirements. Finally, we present evaluation of our Alt-Min approach using IKEA 3D data and demonstrate significant improvement in recovery error and convergence speed compared to prior tensor-based compressed sensing.},
	language = {English},
	urldate = {2021-08-17},
	publisher = {IEEE Computer Society},
	author = {Deng, Tao and Qian, Feng and Liu, Xiao-Yang and Zhang, Manyuan and Walid, Anwar},
	month = jul,
	year = {2018},
	pages = {1--6},
}

@article{swirydowicz_acceleration_2019,
	title = {Acceleration of tensor-product operations for high-order finite element methods},
	volume = {33},
	issn = {1094-3420},
	url = {https://doi.org/10.1177/1094342018816368},
	doi = {10.1177/1094342018816368},
	abstract = {This article is devoted to graphics processing unit (GPU) kernel optimization and performance analysis of three tensor-product operations arising in finite element methods. We provide a mathematical background to these operations and implementation details. Achieving close to peak performance for these operators requires extensive optimization because of the operators’ properties: low arithmetic intensity, tiered structure, and the need to store intermediate results during the kernel execution. We give a guided overview of optimization strategies and we present a performance model that allows us to compare the efficacy of these optimizations against an empirically calibrated roofline.},
	language = {en},
	number = {4},
	urldate = {2021-08-17},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Świrydowicz, Kasia and Chalmers, Noel and Karakus, Ali and Warburton, Tim},
	month = jul,
	year = {2019},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {Finite element method, GPU tensor operations, NVIDIA Tesla P100, elliptic problem, hexahedral elements, matrix–vector product},
	pages = {735--757},
}

@article{ma_optimizing_2019,
	title = {Optimizing sparse tensor times matrix on {GPUs}},
	volume = {129},
	issn = {0743-7315},
	url = {https://www.sciencedirect.com/science/article/pii/S0743731518305161},
	doi = {10.1016/j.jpdc.2018.07.018},
	abstract = {This work optimizes tensor-times-dense matrix multiply (Ttm) for general sparse and semi-sparse tensors on CPU and NVIDIA GPU platforms. Ttm is a computational kernel in tensor methods-based data analytics and data mining applications, such as the popular Tucker decomposition. We first design an in-place sequential SpTtm to avoid explicit data reorganizing between a tensor and a matrix in its conventional approach. We further optimize SpTtm on NVIDIA GPU platforms. Five approaches including employing fine thread granularity, arranging coalesced memory access, rank blocking, and using fast GPU shared memory are developed for GPU-SpTtm. We also optimize semi-sparse tensor-times-dense matrix multiply (SspTtm) to take advantage of the inside dense sub-structures. The optimized SpTtm and SspTtm are applied to Tucker decomposition to improve its overall performance. Our sequential SpTtm is 3–120× faster than the SpTtm from Tensor Toolbox library. GPU-SpTtm obtains 6–19× speedup on NVIDIA K40c and 23–67× speedup on NVIDIA P100 over CPU-SpTtm respectively. Our GPU-SpTtm is 3.9× faster than the state-of-the-art GPU implementation. Our SspTtm implementations outperform SpTtms by up to 4.5×, which handles the input semi-sparse tensor in a general way. Tucker decomposition achieves up to 3.2× speedup after applying the optimized Ttms. The code will be publicly released in ParTI! library: https://github.com/hpcgarage/ParTI.},
	language = {en},
	urldate = {2021-08-17},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Ma, Yuchen and Li, Jiajia and Wu, Xiaolong and Yan, Chenggang and Sun, Jimeng and Vuduc, Richard},
	month = jul,
	year = {2019},
	keywords = {GPU, Irregular algorithms, Sparse tensors, Tensor decomposition},
	pages = {99--109},
}

@inproceedings{ma_acceleration_2010,
	title = {Acceleration of {Streamed} {Tensor} {Contraction} {Expressions} on {GPGPU}-{Based} {Clusters}},
	doi = {10.1109/CLUSTER.2010.26},
	abstract = {Tensor contractions are generalized multidimensional matrix multiplication operations that widely occur in quantum chemistry. Efficient execution of tensor contractions on GPUs requires tackling several challenges to be addressed, including index permutation and small dimension-sizes reducing thread block utilization. In this paper, we present our approach to automatically generate CUDA code to execute tensor contractions on GPUs, including management of data movement between CPU and GPU. GPU-enabled code is generated for the most expensive contractions in CCSD(T), a key coupled cluster method, and incorporated into NW Chem, a popular computational chemistry suite. We demonstrate speedup over a factor of 8.4 using one core per node and over 2.6 when utilizing the entire system using hybrid CPU+GPU solution with 2 GPUs and 5 cores. Finally, we analyze the implementation behavior on future GPU systems.},
	booktitle = {2010 {IEEE} {International} {Conference} on {Cluster} {Computing}},
	author = {Ma, Wenjing and Krishnamoorthy, Sriram and Villay, Oreste and Kowalski, Karol},
	month = sep,
	year = {2010},
	note = {ISSN: 2168-9253},
	keywords = {GPGPU clusters, Generators, Graphics processing unit, Indexes, Instruction sets, Kernel, Optimization, Tensile stress, hybrid execution, tensor contractions},
	pages = {207--216},
}

@phdthesis{kim_optimizing_2019,
	title = {Optimizing {Tensor} {Contractions} on {GPUs}},
	url = {https://etd.ohiolink.edu/apexprod/rws_olink/r/1501/10?clear=10&p10_accession_num=osu1563237825735994},
	language = {en},
	urldate = {2021-08-17},
	school = {The Ohio State University},
	author = {Kim, Jinsung},
	year = {2019},
}

@article{ma_optimizing_2013,
	title = {Optimizing tensor contraction expressions for hybrid {CPU}-{GPU} execution},
	volume = {16},
	issn = {1573-7543},
	url = {https://doi.org/10.1007/s10586-011-0179-2},
	doi = {10.1007/s10586-011-0179-2},
	abstract = {Tensor contractions are generalized multidimensional matrix multiplication operations that widely occur in quantum chemistry. Efficient execution of tensor contractions on Graphics Processing Units (GPUs) requires several challenges to be addressed, including index permutation and small dimension-sizes reducing thread block utilization. Moreover, to apply the same optimizations to various expressions, we need a code generation tool. In this paper, we present our approach to automatically generate CUDA code to execute tensor contractions on GPUs, including management of data movement between CPU and GPU. To evaluate our tool, GPU-enabled code is generated for the most expensive contractions in CCSD(T), a key coupled cluster method, and incorporated into NWChem, a popular computational chemistry suite. For this method, we demonstrate speedup over a factor of 8.4 using one GPU as compared to one CPU core and over 2.6 when utilizing the entire system using hybrid CPU+GPU solution with 2 GPUs and 5 cores (instead of 7 cores per node). We further investigate tensor contraction code on a new series of GPUs, the Fermi GPUs, and provide several effective optimization algorithms. For the same computation of CCSD(T), on a cluster with Fermi GPUs, we achieve a speedup of 3.4 over a cluster with T10 GPUs. With a single Fermi GPU on each node, we achieve a speedup of 43 over the sequential CPU version.},
	language = {en},
	number = {1},
	urldate = {2021-08-17},
	journal = {Cluster Computing},
	author = {Ma, Wenjing and Krishnamoorthy, Sriram and Villa, Oreste and Kowalski, Karol and Agrawal, Gagan},
	month = mar,
	year = {2013},
	pages = {131--155},
}

@article{abdelfattah_high-performance_2016,
	series = {International {Conference} on {Computational} {Science} 2016, {ICCS} 2016, 6-8 {June} 2016, {San} {Diego}, {California}, {USA}},
	title = {High-performance {Tensor} {Contractions} for {GPUs}},
	volume = {80},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050916306536},
	doi = {10.1016/j.procs.2016.05.302},
	abstract = {We present a computational framework for high-performance tensor contractions on GPUs. High-performance is difficult to obtain using existing libraries, especially for many independent contractions where each contraction is very small, e.g., sub-vector/warp in size. However, using our framework to batch contractions plus application-specifics, we demonstrate close to peak performance results. In particular, to accelerate large scale tensor-formulated high-order finite element method (FEM) simulations, which is the main focus and motivation for this work, we represent contractions as tensor index reordering plus matrix-matrix multiplications (GEMMs). This is a key factor to achieve algorithmically many-fold acceleration (vs. not using it) due to possible reuse of data loaded in fast memory. In addition to using this context knowledge, we design tensor data-structures, tensor algebra interfaces, and new tensor contraction algorithms and implementations to achieve 90+\% of a theoretically derived peak on GPUs. On a K40c GPU for contractions resulting in GEMMs on square matrices of size 8 for example, we are 2.8× faster than CUBLAS, and 8.5× faster than MKL on 16 cores of Intel Xeon E5-2670 (Sandy Bridge) 2.60GHz CPUs. Finally, we apply autotuning and code generation techniques to simplify tuning and provide an architecture-aware, user-friendly interface.},
	language = {en},
	urldate = {2021-08-17},
	journal = {Procedia Computer Science},
	author = {Abdelfattah, A. and Baboulin, M. and Dobrev, V. and Dongarra, J. and Earl, C. and Falcou, J. and Haidar, A. and Karlin, I. and Kolev, Tz. and Masliah, I. and Tomov, S.},
	month = jan,
	year = {2016},
	keywords = {Applications, Batched linear algebra, FEM, GPU, Tensor HPC, Tensor contractions},
	pages = {108--118},
}

@misc{rivera_autotuning_2014,
	title = {Autotuning {Tensor} {Contraction} {Computations} on {GPUs}},
	url = {https://www.semanticscholar.org/paper/Autotuning-Tensor-Contraction-Computations-on-GPUs-Rivera-Hall/9393fe38cf4e5b04ccbb4428e4bf35d441761230},
	abstract = {We describe a framework for generating optimized GPU code for computing tensor contractions, a multidimensional generalization of matrix-matrix multiplication that arises frequently in computational science applications. Typical performance optimization strategies for such computations transform the tensors into sequences of matrix-matrix multiplications to take advantage of an optimized BLAS library, but this approach is not appropriate for small tensors. We instead develop an autotuning strategy that generates CUDA variants from a sequential implementation and identifies the best-performing variant. We compare our generated code with that of OpenACC when offloading the same computation to the GPU. The straightforward OpenACC implementation is as much as 23X slower than our automatically generated code for benchmarks representative of two large-scale tensor contraction computations, Nek5000 and NWChem. However, we show how changes in GPU thread-block decomposition and register placement of data in the OpenACC annotations can achieve comparable performance to our automatically generated code. This result highlights limitations of the OpenACC compiler in targeting GPUs for computations such as tensor contractions with small trip counts and large dimensionality. It also suggests additional optimizations that can overcome these limitations. Keywords-tensor contraction, spectral element method, code generation, autotuning},
	language = {en},
	urldate = {2021-08-17},
	author = {Rivera, Axel and Hall, M. and Hovland, P.},
	year = {2014},
}

@inproceedings{nelson_generating_2015,
	title = {Generating {Efficient} {Tensor} {Contractions} for {GPUs}},
	doi = {10.1109/ICPP.2015.106},
	abstract = {Many scientific and numerical applications, including quantum chemistry modeling and fluid dynamics simulation, require tensor product and tensor contraction evaluation. Tensor computations are characterized by arrays with numerous dimensions, inherent parallelism, moderate data reuse and many degrees of freedom in the order in which to perform the computation. The best-performing implementation is heavily dependent on the tensor dimensionality and the target architecture. In this paper, we map tensor computations to GPUs, starting with a high-level tensor input language and producing efficient CUDA code as output. Our approach is to combine tensor-specific mathematical transformations with a GPU decision algorithm, machine learning and auto tuning of a large parameter space. Generated code shows significant performance gains over sequential and Open MP parallel code, and a comparison with Open ACC shows the importance of auto tuning and other optimizations in our framework for achieving efficient results.},
	booktitle = {2015 44th {International} {Conference} on {Parallel} {Processing}},
	author = {Nelson, Thomas and Rivera, Axel and Balaprakash, Prasanna and Hall, Mary and Hovland, Paul D. and Jessup, Elizabeth and Norris, Boyana},
	month = sep,
	year = {2015},
	note = {ISSN: 0190-3918},
	keywords = {Computer architecture, DSL, GPUs, Graphics processing units, Indexes, Optimization, Parallel processing, Tensile stress, autotuning, tensor contraction},
	pages = {969--978},
}

@article{springer_design_2017,
	title = {Design of a high-performance {GEMM}-like {Tensor}-{Tensor} {Multiplication}},
	url = {http://arxiv.org/abs/1607.00145},
	abstract = {We present "GEMM-like Tensor-Tensor multiplication" (GETT), a novel approach to tensor contractions that mirrors the design of a high-performance general matrix-matrix multiplication (GEMM). The critical insight behind GETT is the identification of three index sets, involved in the tensor contraction, which enable us to systematically reduce an arbitrary tensor contraction to loops around a highly tuned "macro-kernel". This macro-kernel operates on suitably prepared ("packed") sub-tensors that reside in a specified level of the cache hierarchy. In contrast to previous approaches to tensor contractions, GETT exhibits desirable features such as unit-stride memory accesses, cache-awareness, as well as full vectorization, without requiring auxiliary memory. To compare our technique with other modern tensor contractions, we integrate GETT alongside the so called Transpose-Transpose-GEMM-Transpose and Loops-over-GEMM approaches into an open source "Tensor Contraction Code Generator" (TCCG). The performance results for a wide range of tensor contractions suggest that GETT has the potential of becoming the method of choice: While GETT exhibits excellent performance across the board, its effectiveness for bandwidth-bound tensor contractions is especially impressive, outperforming existing approaches by up to \$12.4{\textbackslash}times\$. More precisely, GETT achieves speedups of up to \$1.41{\textbackslash}times\$ over an equivalent-sized GEMM for bandwidth-bound tensor contractions while attaining up to \$91.3{\textbackslash}\%\$ of peak floating-point performance for compute-bound tensor contractions.},
	urldate = {2021-08-17},
	journal = {arXiv:1607.00145 [cs]},
	author = {Springer, Paul and Bientinesi, Paolo},
	month = nov,
	year = {2017},
	note = {arXiv: 1607.00145},
	keywords = {Computer Science - Mathematical Software, Computer Science - Performance, D.3.4, G.4, I.1.2, I.1.3},
}

@inproceedings{vedurada_ttlg_2018,
	title = {{TTLG} - {An} {Efficient} {Tensor} {Transposition} {Library} for {GPUs}},
	doi = {10.1109/IPDPS.2018.00067},
	abstract = {This paper presents a Tensor Transposition Library for GPUs (TTLG). A distinguishing feature of TTLG is that it also includes a performance prediction model, which can be used by higher level optimizers that use tensor transposition. For example, tensor contractions are often implemented by using the TTGT (Transpose-Transpose-GEMM-Transpose) approach - transpose input tensors to a suitable layout and then use high-performance matrix multiplication followed by transposition of the result. The performance model is also used internally by TTLG for choosing among alternative kernels and/or slicing/blocking parameters for the transposition. TTLG is compared with current state-of-the-art alternatives for GPUs. Comparable or better transposition times for the "repeated-use" scenario and considerably better "single-use" performance are observed.},
	booktitle = {2018 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Vedurada, Jyothi and Suresh, Arjun and Rajam, Aravind Sukumaran and Kim, Jinsung and Hong, Changwan and Panyala, Ajay and Krishnamoorthy, Sriram and Nandivada, V. Krishna and Srivastava, Rohit Kumar and Sadayappan, P.},
	month = may,
	year = {2018},
	note = {ISSN: 1530-2075},
	keywords = {GPU, Graphics processing units, High performance, Indexes, Instruction sets, Libraries, Taxonomy, Tensile stress, Tensor Transpose, Two dimensional displays},
	pages = {578--588},
}

@article{hynninen_cutt_2017,
	title = {{cuTT}: {A} {High}-{Performance} {Tensor} {Transpose} {Library} for {CUDA} {Compatible} {GPUs}},
	shorttitle = {{cuTT}},
	url = {http://arxiv.org/abs/1705.01598},
	abstract = {We introduce the CUDA Tensor Transpose (cuTT) library that implements high-performance tensor transposes for NVIDIA GPUs with Kepler and above architectures. cuTT achieves high performance by (a) utilizing two GPU-optimized transpose algorithms that both use a shared memory buffer in order to reduce global memory access scatter, and by (b) computing memory positions of tensor elements using a thread-parallel algorithm. We evaluate the performance of cuTT on a variety of benchmarks with tensor ranks ranging from 2 to 12 and show that cuTT performance is independent of the tensor rank and that it performs no worse than an approach based on code generation. We develop a heuristic scheme for choosing the optimal parameters for tensor transpose algorithms by implementing an analytical GPU performance model that can be used at runtime without need for performance measurements or profiling. Finally, by integrating cuTT into the tensor algebra library TAL-SH, we significantly reduce the tensor transpose overhead in tensor contractions, achieving as low as just one percent overhead for arithmetically intensive tensor contractions.},
	urldate = {2021-08-17},
	journal = {arXiv:1705.01598 [cs]},
	author = {Hynninen, Antti-Pekka and Lyakh, Dmitry I.},
	month = may,
	year = {2017},
	note = {arXiv: 1705.01598},
	keywords = {Computer Science - Mathematical Software},
}

@article{hynninen_cutt_2017-1,
	title = {{cuTT}: {A} {High}-{Performance} {Tensor} {Transpose} {Library} for {CUDA} {Compatible} {GPUs}},
	shorttitle = {{cuTT}},
	url = {http://arxiv.org/abs/1705.01598},
	abstract = {We introduce the CUDA Tensor Transpose (cuTT) library that implements high-performance tensor transposes for NVIDIA GPUs with Kepler and above architectures. cuTT achieves high performance by (a) utilizing two GPU-optimized transpose algorithms that both use a shared memory buffer in order to reduce global memory access scatter, and by (b) computing memory positions of tensor elements using a thread-parallel algorithm. We evaluate the performance of cuTT on a variety of benchmarks with tensor ranks ranging from 2 to 12 and show that cuTT performance is independent of the tensor rank and that it performs no worse than an approach based on code generation. We develop a heuristic scheme for choosing the optimal parameters for tensor transpose algorithms by implementing an analytical GPU performance model that can be used at runtime without need for performance measurements or profiling. Finally, by integrating cuTT into the tensor algebra library TAL-SH, we significantly reduce the tensor transpose overhead in tensor contractions, achieving as low as just one percent overhead for arithmetically intensive tensor contractions.},
	urldate = {2021-08-16},
	journal = {arXiv:1705.01598 [cs]},
	author = {Hynninen, Antti-Pekka and Lyakh, Dmitry I.},
	month = may,
	year = {2017},
	note = {arXiv: 1705.01598},
	keywords = {Computer Science - Mathematical Software},
}

@inproceedings{ootomo_randomized_2020,
	series = {{ISC}'2020},
	title = {Randomized {SVD} on {Tensor} {Cores}},
	language = {en-US},
	urldate = {2021-07-18},
	author = {Ootomo, Hiroyuki and Yokota, Rio},
	month = jun,
	year = {2020},
}

@inproceedings{chen_versatile_2019,
	address = {New York, NY, USA},
	series = {{SC} '19},
	title = {A versatile software systolic execution model for {GPU} memory-bound kernels},
	isbn = {978-1-4503-6229-0},
	url = {https://doi.org/10.1145/3295500.3356162},
	doi = {10.1145/3295500.3356162},
	abstract = {This paper proposes a versatile high-performance execution model, inspired by systolic arrays, for memory-bound regular kernels running on CUDA-enabled GPUs. We formulate a systolic model that shifts partial sums by CUDA warp primitives for the computation. We also employ register files as a cache resource in order to operate the entire model efficiently. We demonstrate the effectiveness and versatility of the proposed model for a wide variety of stencil kernels that appear commonly in HPC, and also convolution kernels (increasingly important in deep learning workloads). Our algorithm outperforms the top reported state-of-the-art stencil implementations, including implementations with sophisticated temporal and spatial blocking techniques, on the two latest Nvidia architectures: Tesla V100 and P100. For 2D convolution of general filter sizes and shapes, our algorithm is on average 2.5× faster than Nvidia's NPP on V100 and P100 GPUs.},
	urldate = {2021-08-12},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {Association for Computing Machinery},
	author = {Chen, Peng and Wahib, Mohamed and Takizawa, Shinichiro and Takano, Ryousei and Matsuoka, Satoshi},
	month = nov,
	year = {2019},
	keywords = {CUDA, GPU, convolution, stencil, systolic array},
	pages = {1--81},
}

@inproceedings{zhang_high_2020,
	address = {New York, NY, USA},
	series = {{HPDC} '20},
	title = {High {Accuracy} {Matrix} {Computations} on {Neural} {Engines}: {A} {Study} of {QR} {Factorization} and its {Applications}},
	isbn = {978-1-4503-7052-3},
	shorttitle = {High {Accuracy} {Matrix} {Computations} on {Neural} {Engines}},
	url = {https://doi.org/10.1145/3369583.3392685},
	doi = {10.1145/3369583.3392685},
	abstract = {Fueled by the surge of ever expanding successful applications of deep neural networks and the great computational power demanded, modern computer processors and accelerators are beginning to offer half precision floating point arithmetic support, and special units (neural engines) such as NVIDIA TensorCore on GPU and Google Tensor Processing Unit (TPU) to accelerate the training and prediction of deep neural networks. It remains unclear how neural engines can be profitably used in applications other than neural networks. In this paper we present an endeavor of accelerating and stabilizing a fundamental matrix factorization on neural engines---the QR factorization---which may open doors to much wider relevance to scientific, engineering, and data science. We show that traditional Householder QR algorithms and implementations do not have the necessary data locality, parallelism, accuracy, and robustness on neural engines which are characterized by extreme speed and low precision/range. We demonstrate that neural engines can be effectively used to accelerate matrix computations (QR 3.0x-14.6x speedup compared to cuSOLVER, reaching up to 36.6TFLOPS); however different algorithms (recursive Gram-Schmidt) are needed to expose more locality and parallelism, even at the cost of increased computations. Moreover, scaling, iterative refinement, and other safeguarding procedures are also needed to regain accuracy and avoid overflowing. Our experience seems to suggest that presently with neural engines the matrix factorizations (QR, LU, Cholesky) are best to be co-designed with its applications (linear solver, least square, orthogonalization, SVD, etc) to achieve high performance and adequate accuracy and reliability.},
	urldate = {2021-08-12},
	booktitle = {Proceedings of the 29th {International} {Symposium} on {High}-{Performance} {Parallel} and {Distributed} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Shaoshuai and Baharlouei, Elaheh and Wu, Panruo},
	month = jun,
	year = {2020},
	keywords = {GPU, Gram-Schmidt, QR factorization, half precision, householder QR, least square problem, matrix computations, mixed precision, neural engine, numerical linear algebra, recursive QR, tensorcore, v100},
	pages = {17--28},
}

@inproceedings{ootomo_tsqr_2019,
	title = {{TSQR} on {TensorCores}},
	url = {https://sc19.supercomputing.org/proceedings/tech_poster/tech_poster_pages/rpost147.html},
	urldate = {2021-08-12},
	author = {Ootomo, Hiroyuki and Yokota, Rio},
	year = {2019},
}

@misc{nvidia_corporation_nvidia_2017,
	title = {{NVIDIA} {TESLA} {V100} {GPU} {ARCHITECTURE}},
	url = {https://images.nvidia.com/data-center/a100/a100-datasheet.pdf},
	abstract = {The fastest data center platform for AI and HPC.},
	language = {en-us},
	urldate = {2021-06-06},
	journal = {NVIDIA},
	author = {NVIDIA Corporation},
	year = {2017},
}

@article{williams_roofline_2009,
	title = {Roofline: an insightful visual performance model for multicore architectures},
	volume = {52},
	issn = {0001-0782},
	shorttitle = {Roofline},
	url = {https://doi.org/10.1145/1498765.1498785},
	doi = {10.1145/1498765.1498785},
	abstract = {The Roofline model offers insight on how to improve the performance of software and hardware.},
	number = {4},
	urldate = {2021-07-25},
	journal = {Communications of the ACM},
	author = {Williams, Samuel and Waterman, Andrew and Patterson, David},
	month = apr,
	year = {2009},
	pages = {65--76},
}

@article{finkelstein_quantum-based_2021,
	title = {Quantum-based {Molecular} {Dynamics} {Simulations} using {Tensor} {Cores}},
	url = {http://arxiv.org/abs/2107.02737},
	abstract = {Tensor cores, along with tensor processing units, represent a new form of hardware acceleration specifically designed for deep neural network calculations in artificial intelligence applications. Tensor cores provide extraordinary computational speed and energy efficiency, but with the caveat that they were designed for tensor contractions (matrix-matrix multiplications) using only low-precision floating point operations. In spite of this, we demonstrate how tensor cores can be applied with high efficiency to the challenging and numerically sensitive problem of quantum-based Born-Oppenheimer molecular dynamics, which requires highly accurate electronic structure optimizations and conservative force evaluations. The interatomic forces are calculated on-the-fly from an electronic structure that is obtained from a generalized deep neural network, where the computational structure naturally takes advantage of the exceptional processing power of the tensor cores and allows for high performance in excess of 100 Tflops on the tensor cores of a single Nvidia A100 GPU. Stable molecular dynamics trajectories are generated using the framework of extended Lagrangian Born-Oppenheimer molecular dynamics, which combines computational efficiency with long-term stability, even when using approximate charge relaxations and force evaluations that are limited in accuracy by the numerically noisy conditions caused by the low precision tensor core floating-point operations. A canonical ensemble simulation scheme is also presented, where the additional numerical noise in the calculated forces is absorbed into a Langevin-like dynamics.},
	urldate = {2021-07-18},
	journal = {arXiv:2107.02737 [physics, physics:quant-ph]},
	author = {Finkelstein, Joshua and Smith, Justin S. and Mniszewski, Susan M. and Barros, Kipton and Negre, Christian F. A. and Rubensson, Emanuel H. and Niklasson, Anders M. N.},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.02737
version: 1},
	keywords = {Physics - Computational Physics, Quantum Physics},
}

@article{feng_apnn-tc_2021,
	title = {{APNN}-{TC}: {Accelerating} {Arbitrary} {Precision} {Neural} {Networks} on {Ampere} {GPU} {Tensor} {Cores}},
	shorttitle = {{APNN}-{TC}},
	url = {http://arxiv.org/abs/2106.12169},
	abstract = {Over the years, accelerating neural networks with quantization has been widely studied. Unfortunately, prior efforts with diverse precisions (e.g., 1-bit weights and 2-bit activations) are usually restricted by limited precision support on GPUs (e.g., int1 and int4). To break such restrictions, we introduce the first Arbitrary Precision Neural Network framework (APNN-TC) to fully exploit quantization benefits on Ampere GPU Tensor Cores. Specifically, APNN-TC first incorporates a novel emulation algorithm to support arbitrary short bit-width computation with int1 compute primitives and XOR/AND Boolean operations. Second, APNN-TC integrates arbitrary precision layer designs to efficiently map our emulation algorithm to Tensor Cores with novel batching strategies and specialized memory organization. Third, APNN-TC embodies a novel arbitrary precision NN design to minimize memory access across layers and further improve performance. Extensive evaluations show that APNN-TC can achieve significant speedup over CUTLASS kernels and various NN models, such as ResNet and VGG.},
	urldate = {2021-07-14},
	journal = {arXiv:2106.12169 [cs]},
	author = {Feng, Boyuan and Wang, Yuke and Geng, Tong and Li, Ang and Ding, Yufei},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.12169},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture},
}

@article{li_accelerating_2020,
	title = {Accelerating {Binarized} {Neural} {Networks} via {Bit}-{Tensor}-{Cores} in {Turing} {GPUs}},
	url = {http://arxiv.org/abs/2006.16578},
	abstract = {Despite foreseeing tremendous speedups over conventional deep neural networks, the performance advantage of binarized neural networks (BNNs) has merely been showcased on general-purpose processors such as CPUs and GPUs. In fact, due to being unable to leverage bit-level-parallelism with a word-based architecture, GPUs have been criticized for extremely low utilization (1\%) when executing BNNs. Consequently, the latest tensorcores in NVIDIA Turing GPUs start to experimentally support bit computation. In this work, we look into this brand new bit computation capability and characterize its unique features. We show that the stride of memory access can significantly affect performance delivery and a data-format co-design is highly desired to support the tensorcores for achieving superior performance than existing software solutions without tensorcores. We realize the tensorcore-accelerated BNN design, particularly the major functions for fully-connect and convolution layers -- bit matrix multiplication and bit convolution. Evaluations on two NVIDIA Turing GPUs show that, with ResNet-18, our BTC-BNN design can process ImageNet at a rate of 5.6K images per second, 77\% faster than state-of-the-art. Our BNN approach is released on https://github.com/pnnl/TCBNN.},
	urldate = {2021-07-14},
	journal = {arXiv:2006.16578 [cs]},
	author = {Li, Ang and Su, Simon},
	month = dec,
	year = {2020},
	note = {arXiv: 2006.16578},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@article{boixo_simulation_2018,
	title = {Simulation of low-depth quantum circuits as complex undirected graphical models},
	url = {http://arxiv.org/abs/1712.05384},
	abstract = {Near term quantum computers with a high quantity (around 50) and quality (around 0.995 fidelity for two-qubit gates) of qubits will approximately sample from certain probability distributions beyond the capabilities of known classical algorithms on state-of-the-art computers, achieving the first milestone of so-called quantum supremacy. This has stimulated recent progress in classical algorithms to simulate quantum circuits. Classical simulations are also necessary to approximate the fidelity of multiqubit quantum computers using cross entropy benchmarking. Here we present numerical results of a classical simulation algorithm to sample universal random circuits, on a single workstation, with more qubits and depth than previously reported. For example, circuits with \$5 {\textbackslash}times 9\$ qubits of depth 37, \$7 {\textbackslash}times 8\$ qubits of depth 27, and \$10 {\textbackslash}times ({\textbackslash}kappa {\textgreater} 10)\$) qubits of depth 19 are all easy to sample. We also show up to what depth the sampling, or estimation of observables, is trivially parallelizable. The algorithm is related to the "Feynmann path" method to simulate quantum circuits. For low-depth circuits, the algorithm scales exponentially in the depth times the smaller lateral dimension, or the treewidth, as explained in Boixo et. al., and therefore confirms the bounds in that paper. In particular, circuits with \$7 {\textbackslash}times 7\$ qubits and depth 40 remain currently out of reach. Follow up work on a supercomputer environment will tighten this bound.},
	urldate = {2021-07-07},
	journal = {arXiv:1712.05384 [quant-ph]},
	author = {Boixo, Sergio and Isakov, Sergei V. and Smelyanskiy, Vadim N. and Neven, Hartmut},
	month = jan,
	year = {2018},
	note = {arXiv: 1712.05384},
	keywords = {Quantum Physics},
}

@article{chen_classical_2018,
	title = {Classical {Simulation} of {Intermediate}-{Size} {Quantum} {Circuits}},
	url = {http://arxiv.org/abs/1805.01450},
	abstract = {We introduce a distributed classical simulation algorithm for general quantum circuits, and present numerical results for calculating the output probabilities of universal random circuits. We find that we can simulate more qubits to greater depth than previously reported using the cluster supported by the Data Infrastructure and Search Technology Division of the Alibaba Group. For example, computing a single amplitude of an \$8{\textbackslash}times 8\$ qubit circuit with depth \$40\$ was previously beyond the reach of supercomputers. Our algorithm can compute this within \$2\$ minutes using a small portion (\${\textbackslash}approx\$ 14\% of the nodes) of the cluster. Furthermore, by successfully simulating quantum supremacy circuits of size \$9{\textbackslash}times 9{\textbackslash}times 40\$, \$10{\textbackslash}times 10{\textbackslash}times 35 \$, \$11{\textbackslash}times 11{\textbackslash}times 31\$, and \$12{\textbackslash}times 12{\textbackslash}times 27 \$, we give evidence that noisy random circuits with realistic physical parameters may be simulated classically. This suggests that either harder circuits or error-correction may be vital for achieving quantum supremacy from random circuit sampling.},
	urldate = {2021-07-06},
	journal = {arXiv:1805.01450 [quant-ph]},
	author = {Chen, Jianxin and Zhang, Fang and Huang, Cupjin and Newman, Michael and Shi, Yaoyun},
	month = may,
	year = {2018},
	note = {arXiv: 1805.01450},
	keywords = {Quantum Physics},
}

@article{guo_general-purpose_2019,
	title = {General-{Purpose} {Quantum} {Circuit} {Simulator} with {Projected} {Entangled}-{Pair} {States} and the {Quantum} {Supremacy} {Frontier}},
	volume = {123},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.123.190501},
	doi = {10.1103/PhysRevLett.123.190501},
	abstract = {Recent advances on quantum computing hardware have pushed quantum computing to the verge of quantum supremacy. Here, we bring together many-body quantum physics and quantum computing by using a method for strongly interacting two-dimensional systems, the projected entangled-pair states, to realize an effective general-purpose simulator of quantum algorithms. The classical computing complexity of this simulator is directly related to the entanglement generation of the underlying quantum circuit rather than the number of qubits or gate operations. We apply our method to study random quantum circuits, which allows us to quantify precisely the memory usage and the time requirements of random quantum circuits. We demonstrate our method by computing one amplitude for a 7×7 lattice of qubits with depth (1+40+1) on the Tianhe-2 supercomputer.},
	number = {19},
	urldate = {2021-07-06},
	journal = {Physical Review Letters},
	author = {Guo, Chu and Liu, Yong and Xiong, Min and Xue, Shichuan and Fu, Xiang and Huang, Anqi and Qiang, Xiaogang and Xu, Ping and Liu, Junhua and Zheng, Shenggen and Huang, He-Liang and Deng, Mingtang and Poletti, Dario and Bao, Wan-Su and Wu, Junjie},
	month = nov,
	year = {2019},
	note = {Publisher: American Physical Society},
	pages = {190501},
}

@article{markov_simulating_2008,
	title = {Simulating {Quantum} {Computation} by {Contracting} {Tensor} {Networks}},
	volume = {38},
	issn = {0097-5397},
	url = {https://epubs.siam.org/doi/10.1137/050644756},
	doi = {10.1137/050644756},
	abstract = {The treewidth of a graph is a useful combinatorial measure of how close the graph is to a tree. We prove that a quantum circuit with T gates whose underlying graph has a treewidth d can be simulated deterministically in \$T{\textasciicircum}\{O(1)\}{\textbackslash}exp[O(d)]\$ time, which, in particular, is polynomial in T if \$d=O({\textbackslash}log T)\$. Among many implications, we show efficient simulations for log-depth circuits whose gates apply to nearby qubits only, a natural constraint satisfied by most physical implementations. We also show that one-way quantum computation of Raussendorf and Briegel (Phys. Rev. Lett., 86 (2001), pp. 5188–5191), a universal quantum computation scheme with promising physical implementations, can be efficiently simulated by a randomized algorithm if its quantum resource is derived from a small-treewidth graph with a constant maximum degree. (The requirement on the maximum degree was removed in [I. L. Markov and Y. Shi, preprint:quant-ph/0511069].)},
	number = {3},
	urldate = {2021-07-06},
	journal = {SIAM Journal on Computing},
	author = {Markov, Igor L. and Shi, Yaoyun},
	month = jan,
	year = {2008},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {963--981},
}

@article{villalonga_flexible_2019,
	title = {A flexible high-performance simulator for verifying and benchmarking quantum circuits implemented on real hardware},
	volume = {5},
	copyright = {2019 This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply},
	issn = {2056-6387},
	url = {https://www.nature.com/articles/s41534-019-0196-1},
	doi = {10.1038/s41534-019-0196-1},
	abstract = {Here we present qFlex, a flexible tensor network-based quantum circuit simulator. qFlex can compute both the exact amplitudes, essential for the verification of the quantum hardware, as well as low-fidelity amplitudes, to mimic sampling from Noisy Intermediate-Scale Quantum (NISQ) devices. In this work, we focus on random quantum circuits (RQCs) in the range of sizes expected for supremacy experiments. Fidelity f simulations are performed at a cost that is 1/f lower than perfect fidelity ones. We also present a technique to eliminate the overhead introduced by rejection sampling in most tensor network approaches. We benchmark the simulation of square lattices and Google’s Bristlecone QPU. Our analysis is supported by extensive simulations on NASA HPC clusters Pleiades and Electra. For our most computationally demanding simulation, the two clusters combined reached a peak of 20 Peta Floating Point Operations per Second (PFLOPS) (single precision), i.e., 64\% of their maximum achievable performance, which represents the largest numerical computation in terms of sustained FLOPs and the number of nodes utilized ever run on NASA HPC clusters. Finally, we introduce a novel multithreaded, cache-efficient tensor index permutation algorithm of general application.},
	language = {en},
	number = {1},
	urldate = {2021-07-06},
	journal = {npj Quantum Information},
	author = {Villalonga, Benjamin and Boixo, Sergio and Nelson, Bron and Henze, Christopher and Rieffel, Eleanor and Biswas, Rupak and Mandrà, Salvatore},
	month = oct,
	year = {2019},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Quantum information;Quantum simulation
Subject\_term\_id: quantum-information;quantum-simulation},
	pages = {1--16},
}

@article{huang_classical_2020,
	title = {Classical {Simulation} of {Quantum} {Supremacy} {Circuits}},
	url = {http://arxiv.org/abs/2005.06787},
	abstract = {It is believed that random quantum circuits are difficult to simulate classically. These have been used to demonstrate quantum supremacy: the execution of a computational task on a quantum computer that is infeasible for any classical computer. The task underlying the assertion of quantum supremacy by Arute et al. (Nature, 574, 505--510 (2019)) was initially estimated to require Summit, the world's most powerful supercomputer today, approximately 10,000 years. The same task was performed on the Sycamore quantum processor in only 200 seconds. In this work, we present a tensor network-based classical simulation algorithm. Using a Summit-comparable cluster, we estimate that our simulator can perform this task in less than 20 days. On moderately-sized instances, we reduce the runtime from years to minutes, running several times faster than Sycamore itself. These estimates are based on explicit simulations of parallel subtasks, and leave no room for hidden costs. The simulator's key ingredient is identifying and optimizing the "stem" of the computation: a sequence of pairwise tensor contractions that dominates the computational cost. This orders-of-magnitude reduction in classical simulation time, together with proposals for further significant improvements, indicates that achieving quantum supremacy may require a period of continuing quantum hardware developments without an unequivocal first demonstration.},
	urldate = {2021-07-04},
	journal = {arXiv:2005.06787 [quant-ph]},
	author = {Huang, Cupjin and Zhang, Fang and Newman, Michael and Cai, Junjie and Gao, Xun and Tian, Zhengxiong and Wu, Junyin and Xu, Haihong and Yu, Huanjun and Yuan, Bo and Szegedy, Mario and Shi, Yaoyun and Chen, Jianxin},
	month = may,
	year = {2020},
	note = {arXiv: 2005.06787},
	keywords = {Quantum Physics},
}

@article{kusumoto_experimental_2021,
	title = {Experimental quantum kernel trick with nuclear spins in a solid},
	volume = {7},
	copyright = {2021 The Author(s)},
	issn = {2056-6387},
	url = {https://www.nature.com/articles/s41534-021-00423-0},
	doi = {10.1038/s41534-021-00423-0},
	abstract = {The kernel trick allows us to employ high-dimensional feature space for a machine learning task without explicitly storing features. Recently, the idea of utilizing quantum systems for computing kernel functions using interference has been demonstrated experimentally. However, the dimension of feature spaces in those experiments have been smaller than the number of data, which makes them lose their computational advantage over explicit method. Here we show the first experimental demonstration of a quantum kernel machine that achieves a scheme where the dimension of feature space greatly exceeds the number of data using 1H nuclear spins in solid. The use of NMR allows us to obtain the kernel values with single-shot experiment. We employ engineered dynamics correlating 25 spins which is equivalent to using a feature space with a dimension over 1015. This work presents a quantum machine learning using one of the largest quantum systems to date.},
	language = {en},
	number = {1},
	urldate = {2021-06-11},
	journal = {npj Quantum Information},
	author = {Kusumoto, Takeru and Mitarai, Kosuke and Fujii, Keisuke and Kitagawa, Masahiro and Negoro, Makoto},
	month = jun,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {1--7},
}

@article{britt_quantum_2017,
	title = {Quantum {Accelerators} for {High}-{Performance} {Computing} {Systems}},
	url = {http://arxiv.org/abs/1712.01423},
	doi = {10.1109/ICRC.2017.8123664},
	abstract = {We define some of the programming and system-level challenges facing the application of quantum processing to high-performance computing. Alongside barriers to physical integration, prominent differences in the execution of quantum and conventional programs challenges the intersection of these computational models. Following a brief overview of the state of the art, we discuss recent advances in programming and execution models for hybrid quantum-classical computing. We discuss a novel quantum-accelerator framework that uses specialized kernels to offload select workloads while integrating with existing computing infrastructure. We elaborate on the role of the host operating system to manage these unique accelerator resources, the prospects for deploying quantum modules, and the requirements placed on the language hierarchy connecting these different system components. We draw on recent advances in the modeling and simulation of quantum computing systems with the development of architectures for hybrid high-performance computing systems and the realization of software stacks for controlling quantum devices. Finally, we present simulation results that describe the expected system-level behavior of high-performance computing systems composed from compute nodes with quantum processing units. We describe performance for these hybrid systems in terms of time-to-solution, accuracy, and energy consumption, and we use simple application examples to estimate the performance advantage of quantum acceleration.},
	urldate = {2021-06-09},
	journal = {2017 IEEE International Conference on Rebooting Computing (ICRC)},
	author = {Britt, Keith A. and Mohiyaddin, Fahd A. and Humble, Travis S.},
	month = nov,
	year = {2017},
	note = {arXiv: 1712.01423},
	keywords = {Computer Science - Emerging Technologies, Quantum Physics},
	pages = {1--7},
}

@article{goz_performance_2020,
	title = {Performance and {Energy} {Footprint} {Assessment} of {FPGAs} and {GPUs} on {HPC} {Systems} {Using} {Astrophysics} {Application}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2079-3197/8/2/34},
	doi = {10.3390/computation8020034},
	abstract = {New challenges in Astronomy and Astrophysics (AA) are urging the need for many exceptionally computationally intensive simulations. \&ldquo;Exascale\&rdquo; (and beyond) computational facilities are mandatory to address the size of theoretical problems and data coming from the new generation of observational facilities in AA. Currently, the High-Performance Computing (HPC) sector is undergoing a profound phase of innovation, in which the primary challenge to the achievement of the \&ldquo;Exascale\&rdquo; is the power consumption. The goal of this work is to give some insights about performance and energy footprint of contemporary architectures for a real astrophysical application in an HPC context. We use a state-of-the-art N-body application that we re-engineered and optimized to exploit the heterogeneous underlying hardware fully. We quantitatively evaluate the impact of computation on energy consumption when running on four different platforms. Two of them represent the current HPC systems (Intel-based and equipped with NVIDIA GPUs), one is a micro-cluster based on ARM-MPSoC, and one is a \&ldquo;prototype towards Exascale\&rdquo; equipped with ARM-MPSoCs tightly coupled with FPGAs. We investigate the behavior of the different devices where the high-end GPUs excel in terms of time-to-solution while MPSoC-FPGA systems outperform GPUs in power consumption. Our experience reveals that considering FPGAs for computationally intensive application seems very promising, as their performance is improving to meet the requirements of scientific applications. This work can be a reference for future platform development for astrophysics applications where computationally intensive calculations are required.},
	language = {en},
	number = {2},
	urldate = {2021-06-09},
	journal = {Computation},
	author = {Goz, David and Ieronymakis, Georgios and Papaefstathiou, Vassilis and Dimou, Nikolaos and Bertocco, Sara and Simula, Francesco and Ragagnin, Antonio and Tornatore, Luca and Coretti, Igor and Taffoni, Giuliano},
	month = jun,
	year = {2020},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {ARM-MPSoC, Energy Delay Product, Exascale, FPGAs, GPUs, HPC, N-body, acceleration architectures, astrophysics, hardware acceleration},
	pages = {34},
}

@inproceedings{arafa_verified_2020,
	address = {New York, NY, USA},
	series = {{CF} '20},
	title = {Verified instruction-level energy consumption measurement for {NVIDIA} {GPUs}},
	isbn = {978-1-4503-7956-4},
	url = {https://doi.org/10.1145/3387902.3392613},
	doi = {10.1145/3387902.3392613},
	abstract = {GPUs are prevalent in modern computing systems at all scales. They consume a significant fraction of the energy in these systems. However, vendors do not publish the actual cost of the power/energy overhead of their internal microarchitecture. In this paper, we accurately measure the energy consumption of various PTX instructions found in modern NVIDIA GPUs. We provide an exhaustive comparison of more than 40 instructions for four high-end NVIDIA GPUs from four different generations (Maxwell, Pascal, Volta, and Turing). Furthermore, we show the effect of the CUDA compiler optimizations on the energy consumption of each instruction. We use three different software techniques to read the GPU on-chip power sensors, which use NVIDIA's NVML API and provide an in-depth comparison between these techniques. Additionally, we verified the software measurement techniques against a custom-designed hardware power measurement. The results show that Volta GPUs have the best energy efficiency of all the other generations for the different categories of the instructions. This work should aid in understanding NVIDIA GPUs' microarchitecture. It should also make energy measurements of any GPU kernel both efficient and accurate.},
	urldate = {2021-06-09},
	booktitle = {Proceedings of the 17th {ACM} {International} {Conference} on {Computing} {Frontiers}},
	publisher = {Association for Computing Machinery},
	author = {Arafa, Yehia and ElWazir, Ammar and ElKanishy, Abdelrahman and Aly, Youssef and Elsayed, Ayatelrahman and Badawy, Abdel-Hameed and Chennupati, Gopinath and Eidenbenz, Stephan and Santhi, Nandakishore},
	month = may,
	year = {2020},
	keywords = {GPU power usage, NVML, PAPI, PTX, external power meters, internal power sensors},
	pages = {60--70},
}

@inproceedings{jouppi_-datacenter_2017,
	address = {New York, NY, USA},
	series = {{ISCA} '17},
	title = {In-{Datacenter} {Performance} {Analysis} of a {Tensor} {Processing} {Unit}},
	isbn = {978-1-4503-4892-8},
	url = {https://doi.org/10.1145/3079856.3080246},
	doi = {10.1145/3079856.3080246},
	abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
	urldate = {2021-06-05},
	booktitle = {Proceedings of the 44th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {Association for Computing Machinery},
	author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and et al.},
	month = jun,
	year = {2017},
	keywords = {CNN, DNN, GPU, LSTM, MLP, RNN, TPU, TensorFlow, accelerator, deep learning, domain-specific architecture, neural network},
	pages = {1--12},
}

@article{costanzo_comparison_2021,
	title = {Comparison of {HPC} {Architectures} for {Computing} {All}-{Pairs} {Shortest} {Paths}. {Intel} {Xeon} {Phi} {KNL} vs {NVIDIA} {Pascal}},
	volume = {1409},
	url = {http://arxiv.org/abs/2105.07298},
	doi = {10.1007/978-3-030-75836-3_3},
	abstract = {Today, one of the main challenges for high-performance computing systems is to improve their performance by keeping energy consumption at acceptable levels. In this context, a consolidated strategy consists of using accelerators such as GPUs or many-core Intel Xeon Phi processors. In this work, devices of the NVIDIA Pascal and Intel Xeon Phi Knights Landing architectures are described and compared. Selecting the Floyd-Warshall algorithm as a representative case of graph and memory-bound applications, optimized implementations were developed to analyze and compare performance and energy efficiency on both devices. As it was expected, Xeon Phi showed superior when considering double-precision data. However, contrary to what was considered in our preliminary analysis, it was found that the performance and energy efficiency of both devices were comparable using single-precision datatype.},
	urldate = {2021-06-09},
	journal = {arXiv:2105.07298 [cs]},
	author = {Costanzo, Manuel and Rucci, Enzo and Costi, Ulises and Chichizola, Franco and Naiouf, Marcelo},
	year = {2021},
	note = {arXiv: 2105.07298},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	pages = {37--49},
}

@book{costanzo_comparison_2021,
	title = {Comparison of {HPC} {Architectures} for {Computing} {All}-{Pairs} {Shortest} {Paths}. {Intel} {Xeon} {Phi} {KNL} vs {NVIDIA} {Pascal}},
	abstract = {Today, one of the main challenges for high-performance computing systems is to improve their performance by keeping energy consumption at acceptable levels. In this context, a consolidated strategy consists of using accelerators such as GPUs or many-core Intel Xeon Phi processors. In this work, devices of the NVIDIA Pascal and Intel Xeon Phi Knights Landing architectures are described and compared. Selecting the Floyd-Warshall algorithm as a representative case of graph and memory-bound applications, optimized implementations were developed to analyze and compare performance and energy efficiency on both devices. As it was expected, Xeon Phi showed superior when considering double-precision data. However, contrary to what was considered in our preliminary analysis, it was found that the performance and energy efficiency of both devices were comparable using single-precision datatype.},
	author = {Costanzo, Manuel and Rucci, Enzo and Costi, Ulises and Chichizola, Franco and Naiouf, Marcelo},
	month = may,
	year = {2021},
}

@misc{preferred_networks_inc_mn-core_nodate,
	title = {{MN}-{Core} - {Accelerator} for {Deep} {Learning}},
	url = {https://projects.preferred.jp/mn-core/en/},
	abstract = {Preferred Networks (PFN) has developed the Chainer™ open-source deep learning framework and has been working to build large-scale clusters that support its research and development activities with the aim of applying deep learning technology in the real world.},
	language = {en},
	urldate = {2021-06-06},
	journal = {MN-Core},
	author = {Preferred Networks, Inc.},
}

@misc{arm_ltd_developments_nodate,
	title = {Developments in the {Arm} {A}-{Profile} {Architecture}: {Armv8}.6-{A}},
	url = {https://community.arm.com/developer/ip-products/processors/b/processors-ip-blog/posts/arm-architecture-developments-armv8-6-a},
	abstract = {The Arm Architecture is continually evolving to meet the needs of our ecosystem partners. This blog gives a high-level overview of some of the changes being introduced in Armv8.6-A.
The enhancements to the architecture provide more efficient processi...},
	language = {en},
	urldate = {2021-06-06},
	author = {Arm Ltd.},
}

@article{heng_exploiting_2020,
	title = {Exploiting {GPU}-based {Parallelism} for {Quantum} {Computer} {Simulation}: {A} {Survey}},
	volume = {9},
	shorttitle = {Exploiting {GPU}-based {Parallelism} for {Quantum} {Computer} {Simulation}},
	doi = {10.5573/IEIESPC.2020.9.6.468},
	abstract = {As the advent of quantum computers comes closer, research on quantum algorithms is actively being conducted in various fields. Quantum computer simulation is mainly employed for the study of quantum algorithms due to the imperfections of quantum computers. Quantum computer simulation using classical computers has encountered significant challenges in emulating quantum algorithms, where computational time and memory usage increase exponentially due to superposition and entanglement. To resolve these problems, there have been many studies to exploit GPU-based parallelism in quantum computer simulation. In this review paper, we present various studies on implementing quantum computer simulations using single or multiple GPUs. We also analyze the performance of these studies in terms of speedup and summarize the simulation methods.},
	journal = {IEIE Transactions on Smart Processing and Computing},
	author = {Heng, Sengthai and Kim, Taekyung and Han, Youngsun},
	month = dec,
	year = {2020},
	pages = {468--476},
}

@article{zhao_simulation_2020,
	title = {Simulation of {Quantum} {Computing} on {Classical} {Supercomputers}},
	url = {http://arxiv.org/abs/2010.14962},
	abstract = {Simulation of quantum computing on supercomputers is a significant research topic, which plays a vital role in quantum algorithm verification, error-tolerant verification and other applications. Tensor network contraction based on density matrix is an important single-amplitude simulation strategy, but it is hard to execute on the distributed computing systems currently. In this paper, we dive deep into this problem, and propose a scheme based on cutting edges of undirected graphs. This scheme cuts edges of undirected graphs with large tree width to obtain many undirected subgraphs with small tree width, and these subgraphs contracted on different computing cores. The contraction results of slave cores are summarized in the master node, which is consistent with the original tensor network contraction. Thus, we can simulate the larger scale quantum circuit than single core. Moreover, it's an NP-hard problem to find the global optimum cutting edges, and we propose a search strategy based on a heuristic algorithm to approach it. In order to verify the effectiveness of our scheme, we conduct tests based on QAOA algorithm, and it can simulate 120-qubit 3-regular QAOA algorithm on 4096-core supercomputer, which greatly exceeds the simulation scale on a single core of 100-qubit.},
	urldate = {2021-05-25},
	journal = {arXiv:2010.14962 [quant-ph]},
	author = {Zhao, Ya-Qian and Li, Ren-Gang and Jiang, Jin-Zhe and Li, Chen and Li, Hong-Zhen and Wang, En-Dong and Gong, Wei-Feng and Zhang, Xin and Wei, Zhi-Qiang},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.14962},
	keywords = {Quantum Physics},
}

@article{hong_tensor_2020,
	title = {A {Tensor} {Network} based {Decision} {Diagram} for {Representation} of {Quantum} {Circuits}},
	url = {http://arxiv.org/abs/2009.02618},
	abstract = {Tensor networks have been successfully applied in simulation of quantum physical systems for decades. Recently, they have also been employed in classical simulation of quantum computing, in particular, random quantum circuits. This paper proposes a decision-diagram style data structure, called TDD (Tensor Decision Diagram), for more principled and convenient applications of tensor networks. This new data structure provides a compact and canonical representation for quantum circuits. By exploiting circuit partition, the TDD of a quantum circuit can be computed efficiently. Furthermore, we show that the operations of tensor networks essential in their applications (e.g., addition and contraction), can also be implemented efficiently in TDDs. A proof-of-concept implementation of TDDs is presented and its efficiency is evaluated on a set of benchmark quantum circuits. It is expected that TDDs will play an important role in various design automation tasks related to quantum circuits, including but not limited to equivalence checking, error detection, synthesis, simulation, and verification.},
	urldate = {2021-05-25},
	journal = {arXiv:2009.02618 [quant-ph]},
	author = {Hong, Xin and Zhou, Xiangzhen and Li, Sanjiang and Feng, Yuan and Ying, Mingsheng},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.02618},
	keywords = {Computer Science - Data Structures and Algorithms, Quantum Physics},
}

@article{schutski_adaptive_2020,
	title = {Adaptive algorithm for quantum circuit simulation},
	volume = {101},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.101.042335},
	doi = {10.1103/PhysRevA.101.042335},
	abstract = {Efficient simulation of quantum computers is essential for the development and validation of near-term quantum devices and the research on quantum algorithms. Up to date, two main approaches to simulation have been use, based on either full-state or single-amplitude evaluation. We propose an algorithm that efficiently interpolates between these two possibilities. Our approach elucidates the connection between quantum circuit simulation and partial evaluation of expressions in tensor algebra.},
	number = {4},
	urldate = {2021-05-25},
	journal = {Physical Review A},
	author = {Schutski, Roman and Lykov, Danil and Oseledets, Ivan},
	month = apr,
	year = {2020},
	note = {Publisher: American Physical Society},
	pages = {042335},
}

@article{willsch_gpu-accelerated_2021,
	title = {{GPU}-accelerated simulations of quantum annealing and the quantum approximate optimization algorithm},
	url = {http://arxiv.org/abs/2104.03293},
	abstract = {We use a GPU-accelerated version of the massively parallel J{\textbackslash}"ulich universal quantum computer simulator (JUQCS--G) to benchmark JUWELS Booster, a GPU cluster with 3744 NVIDIA A100 Tensor Core GPUs. Using JUQCS--G, we study the relation between quantum annealing (QA) and the quantum approximate optimization algorithm (QAOA). We find that a very coarsely discretized version of QA, termed approximate QA, performs surprisingly well in comparison to the QAOA. It can either be used to initialize the QAOA, or to avoid the costly optimization procedure altogether. Furthermore, we study the scaling of approximate QA for exact cover problems from 30 to 40 qubits. We find that the case with largest discretization error performs most favorably, surpassing the best result obtained from the QAOA.},
	urldate = {2021-05-25},
	journal = {arXiv:2104.03293 [physics, physics:quant-ph]},
	author = {Willsch, Dennis and Willsch, Madita and Jin, Fengping and Michielsen, Kristel and De Raedt, Hans},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.03293},
	keywords = {Physics - Computational Physics, Quantum Physics},
}

@article{zlokapa_boundaries_2020,
	title = {Boundaries of quantum supremacy via random circuit sampling},
	url = {http://arxiv.org/abs/2005.02464},
	abstract = {Google's recent quantum supremacy experiment heralded a transition point where quantum computing performed a computational task, random circuit sampling, that is beyond the practical reach of modern supercomputers. We examine the constraints of the observed quantum runtime advantage in an extrapolation to circuits with a larger number of qubits and gates. Due to the exponential decrease of the experimental fidelity with the number of qubits and gates, we demonstrate for current fidelities a theoretical classical runtime advantage for circuits deeper than a few hundred gates, while quantum runtimes for cross-entropy benchmarking limit the region of a quantum advantage to a few hundred qubits. However, the quantum runtime advantage boundary in circuit width and depth grows exponentially with respect to reduced error rates, and our work highlights the importance of continued progress along this line. Extrapolations of measured error rates suggest that the limiting circuit size for which a computationally feasible quantum runtime advantage in cross-entropy benchmarking can be achieved approximately coincides with expectations for early implementations of the surface code and other quantum error correction methods. Thus the boundaries of quantum supremacy via random circuit sampling may fortuitously coincide with the advent of scalable, error corrected quantum computing in the near term.},
	urldate = {2021-05-25},
	journal = {arXiv:2005.02464 [quant-ph]},
	author = {Zlokapa, Alexander and Boixo, Sergio and Lidar, Daniel},
	month = oct,
	year = {2020},
	note = {arXiv: 2005.02464},
	keywords = {Quantum Physics},
}

@article{efthymiou_qibo_2020,
	title = {Qibo: a framework for quantum simulation with hardware acceleration},
	shorttitle = {Qibo},
	url = {http://arxiv.org/abs/2009.01845},
	abstract = {We present Qibo, a new open-source software for fast evaluation of quantum circuits and adiabatic evolution which takes full advantage of hardware accelerators. The growing interest in quantum computing and the recent developments of quantum hardware devices motivates the development of new advanced computational tools focused on performance and usage simplicity. In this work we introduce a new quantum simulation framework that enables developers to delegate all complicated aspects of hardware or platform implementation to the library so they can focus on the problem and quantum algorithms at hand. This software is designed from scratch with simulation performance, code simplicity and user friendly interface as target goals. It takes advantage of hardware acceleration such as multi-threading CPU, single GPU and multi-GPU devices.},
	urldate = {2021-05-25},
	journal = {arXiv:2009.01845 [quant-ph]},
	author = {Efthymiou, Stavros and Ramos-Calderer, Sergi and Bravo-Prieto, Carlos and Pérez-Salinas, Adrián and García-Martín, Diego and Garcia-Saez, Artur and Latorre, José Ignacio and Carrazza, Stefano},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.01845},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Quantum Physics},
}

@article{leymann_bitter_2020,
	title = {The bitter truth about gate-based quantum algorithms in the {NISQ} era},
	volume = {5},
	issn = {2058-9565},
	url = {https://doi.org/10.1088/2058-9565/abae7d},
	doi = {10.1088/2058-9565/abae7d},
	abstract = {Implementing a gate-based quantum algorithm on an noisy intermediate scale quantum (NISQ) device has several challenges that arise from the fact that such devices are noisy and have limited quantum resources. Thus, various factors contributing to the depth and width as well as to the noise of an implementation of a gate-based algorithm must be understood in order to assess whether an implementation will execute successfully on a given NISQ device. In this contribution, we discuss these factors and their impact on algorithm implementations. Especially, we will cover state preparation, oracle expansion, connectivity, circuit rewriting, and readout: these factors are very often ignored when presenting a gate-based algorithm but they are crucial when implementing such an algorithm on near-term quantum computers. Our contribution will help developers in charge of realizing gate-based algorithms on such machines in (i) achieving an executable implementation, and (ii) assessing the success of their implementation on a given machine.},
	language = {en},
	number = {4},
	urldate = {2021-05-25},
	journal = {Quantum Science and Technology},
	author = {Leymann, Frank and Barzen, Johanna},
	month = sep,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {044007},
}

@article{guerreschi_intel_2020,
	title = {Intel {Quantum} {Simulator}: a cloud-ready high-performance simulator of quantum circuits},
	volume = {5},
	issn = {2058-9565},
	shorttitle = {Intel {Quantum} {Simulator}},
	url = {https://doi.org/10.1088/2058-9565/ab8505},
	doi = {10.1088/2058-9565/ab8505},
	abstract = {Classical simulation of quantum computers will continue to play an essential role in the progress of quantum information science, both for numerical studies of quantum algorithms and for modeling noise and errors. Here we introduce the latest release of Intel Quantum Simulator (IQS), formerly known as qHiPSTER. The high-performance computing (HPC) capability of the software allows users to leverage the available hardware resources provided by supercomputers, as well as available public cloud computing infrastructure. To take advantage of the latter platform, together with the distributed simulation of each separate quantum state, IQS allows to subdivide the computational resources to simulate a pool of related circuits in parallel. We highlight the technical implementation of the distributed algorithm and details about the new pool functionality. We also include some basic benchmarks (up to 42 qubits) and performance results obtained using HPC infrastructure. Finally, we use IQS to emulate a scenario in which many quantum devices are running in parallel to implement the quantum approximate optimization algorithm, using particle swarm optimization as the classical subroutine. The results demonstrate that the hyperparameters of this classical optimization algorithm depends on the total number of quantum circuit simulations one has the bandwidth to perform. Intel Quantum Simulator has been released open-source with permissive licensing and is designed to simulate a large number of qubits, to emulate multiple quantum devices running in parallel, and/or to study the effects of decoherence and other hardware errors on calculation results.},
	language = {en},
	number = {3},
	urldate = {2021-05-25},
	journal = {Quantum Science and Technology},
	author = {Guerreschi, Gian Giacomo and Hogaboam, Justin and Baruffa, Fabio and Sawaya, Nicolas P. D.},
	month = may,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {034007},
}

@article{zhang_high-performance_2021,
	title = {High-performance parallel classical scheme for simulating shallow quantum circuits},
	url = {http://arxiv.org/abs/2103.00693},
	abstract = {Recently, constant-depth quantum circuits are proved more powerful than their classical counterparts at solving certain problems, e.g., the two-dimensional (2D) hidden linear function (HLF) problem regarding a symmetric binary matrix. To further investigate the boundary between classical and quantum computing models, in this work we propose a high-performance two-stage classical scheme to solve a full-sampling variant of the 2D HLF problem, which combines traditional classical parallel algorithms and a gate-based classical circuit model together for exactly simulating the target shallow quantum circuits. Under reasonable parameter assumptions, a theoretical analysis reveals our classical simulator consumes less runtime than that of near-term quantum processors for most problem instances. Furthermore, we demonstrate the typical all-connected 2D grid instances by moderate FPGA circuits, and show our designed parallel scheme is a practically scalable, high-efficient and operationally convenient tool for simulating and verifying graph-state circuits performed by current quantum hardware.},
	urldate = {2021-05-25},
	journal = {arXiv:2103.00693 [quant-ph]},
	author = {Zhang, Shihao and Bao, Jiacheng and Sun, Yifan and Li, Lvzhou and Sun, Houjun and Zhang, Xiangdong},
	month = feb,
	year = {2021},
	note = {arXiv: 2103.00693},
	keywords = {Quantum Physics},
}

@article{ma_low_2021,
	title = {Low {Rank} {Approximation} in {Simulations} of {Quantum} {Algorithms}},
	url = {http://arxiv.org/abs/2104.11396},
	abstract = {Simulating quantum algorithms on classical computers is challenging when the system size, i.e., the number of qubits used in the quantum algorithm, is moderately large. However, some quantum algorithms and the corresponding quantum circuits can be simulated efficiently on a classical computer if the input quantum state is a low-rank tensor and all intermediate states of the quantum algorithm can be represented or approximated by low-rank tensors. In this paper, we examine the possibility of simulating a few quantum algorithms by using low-rank canonical polyadic (CP) decomposition to represent the input and all intermediate states of these algorithms. Two rank reduction algorithms are used to enable efficient simulation. We show that some of the algorithms preserve the low-rank structure of the input state and can thus be efficiently simulated on a classical computer. However, the rank of the intermediate states in other quantum algorithms can increase rapidly, making efficient simulation more difficult. To some extent, such difficulty reflects the advantage or superiority of a quantum computer over a classical computer. As a result, understanding the low-rank structure of a quantum algorithm allows us to identify algorithms that can benefit significantly from quantum computers.},
	urldate = {2021-05-25},
	journal = {arXiv:2104.11396 [cs, math]},
	author = {Ma, Linjian and Yang, Chao},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.11396},
	keywords = {Computer Science - Computational Engineering, Finance, and Science, Mathematics - Numerical Analysis},
}

@article{wang_quantum_2021,
	title = {A quantum circuit simulator and its applications on {Sunway} {TaihuLight} supercomputer},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-020-79777-y},
	doi = {10.1038/s41598-020-79777-y},
	abstract = {Classical simulation of quantum computation is vital for verifying quantum devices and assessing quantum algorithms. We present a new quantum circuit simulator developed on the Sunway TaihuLight supercomputer. Compared with other simulators, the present one is distinguished in two aspects. First, our simulator is more versatile. The simulator consists of three mutually independent parts to compute the full, partial and single amplitudes of a quantum state with different methods. It has the function of emulating the effect of noise and support more kinds of quantum operations. Second, our simulator is of high efficiency. The simulator is designed in a two-level parallel structure to be implemented efficiently on the distributed many-core Sunway TaihuLight supercomputer. Random quantum circuits can be simulated with 40, 75 and 200 qubits on the full, partial and single amplitude, respectively. As illustrative applications of the simulator, we present a quantum fast Poisson solver and an algorithm for quantum arithmetic of evaluating transcendental functions. Our simulator is expected to have broader applications in developing quantum algorithms in various fields.},
	language = {en},
	number = {1},
	urldate = {2021-05-25},
	journal = {Scientific Reports},
	author = {Wang, Zhimin and Chen, Zhaoyun and Wang, Shengbin and Li, Wendong and Gu, Yongjian and Guo, Guoping and Wei, Zhiqiang},
	month = jan,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {355},
}

@article{guo_verifying_2021,
	title = {Verifying {Random} {Quantum} {Circuits} with {Arbitrary} {Geometry} {Using} {Tensor} {Network} {States} {Algorithm}},
	volume = {126},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.126.070502},
	doi = {10.1103/PhysRevLett.126.070502},
	abstract = {The ability to efficiently simulate random quantum circuits using a classical computer is increasingly important for developing noisy intermediate-scale quantum devices. Here, we present a tensor network states based algorithm specifically designed to compute amplitudes for random quantum circuits with arbitrary geometry. Singular value decomposition based compression together with a two-sided circuit evolution algorithm are used to further compress the resulting tensor network. To further accelerate the simulation, we also propose a heuristic algorithm to compute the optimal tensor contraction path. We demonstrate that our algorithm is up to 2 orders of magnitudes faster than the Schrödinger-Feynman algorithm for verifying random quantum circuits on the 53-qubit Sycamore processor, with circuit depths below 12. We also simulate larger random quantum circuits with up to 104 qubits, showing that this algorithm is an ideal tool to verify relatively shallow quantum circuits on near-term quantum computers.},
	number = {7},
	urldate = {2021-05-25},
	journal = {Physical Review Letters},
	author = {Guo, Chu and Zhao, Youwei and Huang, He-Liang},
	month = feb,
	year = {2021},
	note = {Publisher: American Physical Society},
	pages = {070502},
}

@article{pan_simulating_2021,
	title = {Simulating the {Sycamore} quantum supremacy circuits},
	url = {http://arxiv.org/abs/2103.03074},
	abstract = {We propose a general tensor network method for simulating quantum circuits. The method is massively more efficient in computing a large number of correlated bitstring amplitudes and probabilities than existing methods. As an application, we study the sampling problem of Google's Sycamore circuits, which are believed to be beyond the reach of classical supercomputers and have been used to demonstrate quantum supremacy. Using our method, employing a small computational cluster containing 60 graphical processing units (GPUs), we have generated one million correlated bitstrings with some entries fixed, from the Sycamore circuit with 53 qubits and 20 cycles, with linear cross-entropy benchmark (XEB) fidelity equals 0.739, which is much higher than those in Google's quantum supremacy experiments.},
	urldate = {2021-05-25},
	journal = {arXiv:2103.03074 [physics, physics:quant-ph]},
	author = {Pan, Feng and Zhang, Pan},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.03074},
	keywords = {Physics - Computational Physics, Quantum Physics},
}

@article{perlin_quantum_2021,
	title = {Quantum circuit cutting with maximum-likelihood tomography},
	volume = {7},
	copyright = {2021 The Author(s)},
	issn = {2056-6387},
	url = {https://www.nature.com/articles/s41534-021-00390-6},
	doi = {10.1038/s41534-021-00390-6},
	abstract = {We introduce maximum-likelihood fragment tomography (MLFT) as an improved circuit cutting technique for running clustered quantum circuits on quantum devices with a limited number of qubits. In addition to minimizing the classical computing overhead of circuit cutting methods, MLFT finds the most likely probability distribution for the output of a quantum circuit, given the measurement data obtained from the circuit’s fragments. We demonstrate the benefits of MLFT for accurately estimating the output of a fragmented quantum circuit with numerical experiments on random unitary circuits. Finally, we show that circuit cutting can estimate the output of a clustered circuit with higher fidelity than full circuit execution, thereby motivating the use of circuit cutting as a standard tool for running clustered circuits on quantum hardware.},
	language = {en},
	number = {1},
	urldate = {2021-05-25},
	journal = {npj Quantum Information},
	author = {Perlin, Michael A. and Saleem, Zain H. and Suchara, Martin and Osborn, James C.},
	month = apr,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {1--8},
}

@inproceedings{tang_cutqc_2021,
	address = {New York, NY, USA},
	series = {{ASPLOS} 2021},
	title = {{CutQC}: using small {Quantum} computers for large {Quantum} circuit evaluations},
	isbn = {978-1-4503-8317-2},
	shorttitle = {{CutQC}},
	url = {https://doi.org/10.1145/3445814.3446758},
	doi = {10.1145/3445814.3446758},
	abstract = {Quantum computing (QC) is a new paradigm offering the potential of exponential speedups over classical computing for certain computational problems. Each additional qubit doubles the size of the computational state space available to a QC algorithm. This exponential scaling underlies QC’s power, but today’s Noisy Intermediate-Scale Quantum (NISQ) devices face significant engineering challenges in scalability. The set of quantum circuits that can be reliably run on NISQ devices is limited by their noisy operations and low qubit counts. This paper introduces CutQC, a scalable hybrid computing approach that combines classical computers and quantum computers to enable evaluation of quantum circuits that cannot be run on classical or quantum computers alone. CutQC cuts large quantum circuits into smaller subcircuits, allowing them to be executed on smaller quantum devices. Classical postprocessing can then reconstruct the output of the original circuit. This approach offers significant runtime speedup compared with the only viable current alternative—purely classical simulations—and demonstrates evaluation of quantum circuits that are larger than the limit of QC or classical simulation. Furthermore, in real-system runs, CutQC achieves much higher quantum circuit evaluation fidelity using small prototype quantum computers than the state-of-the-art large NISQ devices achieve. Overall, this hybrid approach allows users to leverage classical and quantum computing resources to evaluate quantum programs far beyond the reach of either one alone.},
	urldate = {2021-05-25},
	booktitle = {Proceedings of the 26th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Tang, Wei and Tomesh, Teague and Suchara, Martin and Larson, Jeffrey and Martonosi, Margaret},
	month = apr,
	year = {2021},
	keywords = {Hybrid Computing, Quantum Circuit Cutting, Quantum Computing (QC)},
	pages = {473--486},
}

@article{nguyen_tensor_2021,
	title = {Tensor {Network} {Quantum} {Virtual} {Machine} for {Simulating} {Quantum} {Circuits} at {Exascale}},
	url = {http://arxiv.org/abs/2104.10523},
	abstract = {The numerical simulation of quantum circuits is an indispensable tool for development, verification and validation of hybrid quantum-classical algorithms on near-term quantum co-processors. The emergence of exascale high-performance computing (HPC) platforms presents new opportunities for pushing the boundaries of quantum circuit simulation. We present a modernized version of the Tensor Network Quantum Virtual Machine (TNQVM) which serves as a quantum circuit simulation backend in the eXtreme-scale ACCelerator (XACC) framework. The new version is based on the general purpose, scalable tensor network processing library, ExaTN, and provides multiple configurable quantum circuit simulators enabling either exact quantum circuit simulation via the full tensor network contraction, or approximate quantum state representations via suitable tensor factorizations. Upon necessity, stochastic noise modeling from real quantum processors is incorporated into the simulations by modeling quantum channels with Kraus tensors. By combining the portable XACC quantum programming frontend and the scalable ExaTN numerical backend we introduce an end-to-end virtual quantum development environment which can scale from laptops to future exascale platforms. We report initial benchmarks of our framework which include a demonstration of the distributed execution, incorporation of quantum decoherence models, and simulation of the random quantum circuits used for the certification of quantum supremacy on the Google Sycamore superconducting architecture.},
	urldate = {2021-05-25},
	journal = {arXiv:2104.10523 [quant-ph]},
	author = {Nguyen, Thien and Lyakh, Dmitry and Dumitrescu, Eugene and Clark, David and Larkin, Jeff and McCaskey, Alexander},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.10523},
	keywords = {Quantum Physics},
}

@article{xu_utilizing_2021,
	title = {Utilizing {GPGPUs} for simulating quantum circuit},
	volume = {1748},
	issn = {1742-6596},
	url = {https://doi.org/10.1088/1742-6596/1748/5/052063},
	doi = {10.1088/1742-6596/1748/5/052063},
	abstract = {State vector describing the state of an n-qubit quantum system requires 2 n complex numbers, which means the memory usage of simulating a quantum computer grows exponentially with respect to the number of qubits. For the memory of a single hardware is limited, distributed computation is necessary for simulating quantum systems with large number of qubits. In our proposal, we developed a multi-GPU solution. State vector is divided into several chunks and allocated to different GPU device. Making it possible to simulate quantum circuits of any size on appropriate GPU cluster. We analyse the theoretical computation resource and come to the conclusion that memory copy bandwidth between GPUs is the bottleneck of time consumption. Total execution time is O(logDN/DN) if peer-to-peer memory copy is supported, otherwise time elapsed is O(logDN) where DN is number of GPU devices. Memory consumption on one GPU is O(1/DN).},
	language = {en},
	urldate = {2021-05-25},
	journal = {Journal of Physics: Conference Series},
	author = {Xu, Yuanfei and Zhang, Lixing},
	month = jan,
	year = {2021},
	note = {Publisher: IOP Publishing},
	pages = {052063},
}

@inproceedings{schmidt_survey_2020,
	title = {A {Survey} of {Singular} {Value} {Decomposition} {Methods} for {Distributed} {Tall}/{Skinny} {Data}},
	isbn = {978-1-66542-270-3},
	url = {https://www.computer.org/csdl/proceedings-article/scala/2020/105000a027/1q0FWN86r1C},
	doi = {10.1109/ScalA51936.2020.00009},
	abstract = {The Singular Value Decomposition (SVD) is one of the most important matrix factorizations, enjoying a wide variety of applications across numerous application domains. In statistics and data analysis, the common applications of SVD inclue Principal Components Analysis (PCA) and regression. Usually these applications arise on data that has far more rows than columns, so-called "tall/skinny" matrices. In the big data analytics context, this may take the form of hundreds of millions to billions of rows with only a few hundred columns. There is a need, therefore, for fast, accurate, and scalable tall/skinny SVD implementations which can fully utilize modern computing resources. To that end, we present a survey of three different algorithms for computing the SVD for these kinds of tall/skinny data layouts using MPI for communication. We contextualize these with common big data analytics techniques. Finally, we present both CPU and GPU timing results from the Summit supercomputer, and discuss possible alternative approaches.},
	language = {English},
	urldate = {2021-05-09},
	publisher = {IEEE Computer Society},
	author = {Schmidt, Drew},
	month = nov,
	year = {2020},
	pages = {27--34},
}

@inproceedings{menon_adapt_2018,
	address = {Dallas, Texas},
	series = {{SC} '18},
	title = {{ADAPT}: algorithmic differentiation applied to floating-point precision tuning},
	shorttitle = {{ADAPT}},
	url = {https://doi.org/10.1109/SC.2018.00051},
	doi = {10.1109/SC.2018.00051},
	abstract = {HPC applications use floating point arithmetic operations extensively to solve computational problems. Mixed-precision computing seeks to use the lowest precision data type that is sufficient to achieve a desired accuracy, improving performance and reducing power consumption. Manually optimizing a program to use mixed precision is challenging as it not only requires extensive knowledge about the numerical behavior of the algorithm but also estimates of the rounding errors. In this work, we present ADAPT, a scalable approach for mixed-precision analysis on HPC workloads using algorithmic differentiation to provide accurate estimates about the final output error. ADAPT provides a floating-point precision sensitivity profile while incurring an overhead of only a constant multiple of the original computation irrespective of the number of variables analyzed. The sensitivity profile can be used to make algorithmic choices and to develop mixed-precision configurations of a program. We evaluate ADAPT on six benchmarks and a proxy application (LULESH) and show that we are able to achieve a speedup of 1.2x on the proxy application.},
	urldate = {2021-04-30},
	booktitle = {Proceedings of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage}, and {Analysis}},
	publisher = {IEEE Press},
	author = {Menon, Harshitha and Lam, Michael O. and Osei-Kuffuor, Daniel and Schordan, Markus and Lloyd, Scott and Mohror, Kathryn and Hittinger, Jeffrey},
	month = nov,
	year = {2018},
	pages = {1--13},
}

@article{henry_leveraging_2019,
	title = {Leveraging the bfloat16 {Artificial} {Intelligence} {Datatype} {For} {Higher}-{Precision} {Computations}},
	url = {http://arxiv.org/abs/1904.06376},
	abstract = {In recent years fused-multiply-add (FMA) units with lower-precision multiplications and higher-precision accumulation have proven useful in machine learning/artificial intelligence applications, most notably in training deep neural networks due to their extreme computational intensity. Compared to classical IEEE-754 32 bit (FP32) and 64 bit (FP64) arithmetic, these reduced precision arithmetic can naturally be sped up disproportional to their shortened width. The common strategy of all major hardware vendors is to aggressively further enhance their performance disproportionately. One particular FMA operation that multiplies two BF16 numbers while accumulating in FP32 has been found useful in deep learning, where BF16 is the 16-bit floating point datatype with IEEE FP32 numerical range but 8 significant bits of precision. In this paper, we examine the use this FMA unit to implement higher-precision matrix routines in terms of potential performance gain and implications on accuracy. We demonstrate how a decomposition into multiple smaller datatypes can be used to assemble a high-precision result, leveraging the higher precision accumulation of the FMA unit. We first demonstrate that computations of vector inner products and by natural extension, matrix-matrix products can be achieved by decomposing FP32 numbers in several BF16 numbers followed by appropriate computations that can accommodate the dynamic range and preserve accuracy compared to standard FP32 computations, while projecting up to 5.2x speed-up. Furthermore, we examine solution of linear equations formulated in the residual form that allows for iterative refinement. We demonstrate that the solution obtained to be comparable to those offered by FP64 under a large range of linear system condition numbers.},
	urldate = {2021-04-30},
	journal = {arXiv:1904.06376 [cs]},
	author = {Henry, Greg and Tang, Ping Tak Peter and Heinecke, Alexander},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.06376},
	keywords = {Computer Science - Mathematical Software, Mathematics - Numerical Analysis},
}

@misc{noauthor_adapt_nodate,
	title = {{ADAPT} {\textbar} {Proceedings} of the {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage}, and {Analysis}},
	url = {https://dl.acm.org/doi/10.1109/SC.2018.00051},
	urldate = {2021-04-30},
}

@article{mittal_survey_2016,
	title = {A {Survey} of {Techniques} for {Approximate} {Computing}},
	volume = {48},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/2893356},
	doi = {10.1145/2893356},
	abstract = {Approximate computing trades off computation quality with effort expended, and as rising performance demands confront plateauing resource budgets, approximate computing has become not merely attractive, but even imperative. In this article, we present a survey of techniques for approximate computing (AC). We discuss strategies for finding approximable program portions and monitoring output quality, techniques for using AC in different processing units (e.g., CPU, GPU, and FPGA), processor components, memory technologies, and so forth, as well as programming frameworks for AC. We classify these techniques based on several key characteristics to emphasize their similarities and differences. The aim of this article is to provide insights to researchers into working of AC techniques and inspire more efforts in this area to make AC the mainstream computing approach in future systems.},
	number = {4},
	urldate = {2021-04-26},
	journal = {ACM Computing Surveys},
	author = {Mittal, Sparsh},
	month = mar,
	year = {2016},
	keywords = {CPU, FPGA, GPU, Review, approximate computing technique (ACT), approximate storage, classification, neural networks, quality configurability},
	pages = {62:1--62:33},
}

@inproceedings{sakamoto_effectiveness_2020,
	title = {The {Effectiveness} of {Low}-{Precision} {Floating} {Arithmetic} on {Numerical} {Codes}: {A} {Case} {Study} on {Power} {Consumption}},
	isbn = {978-1-4503-7236-7},
	shorttitle = {The {Effectiveness} of {Low}-{Precision} {Floating} {Arithmetic} on {Numerical} {Codes}},
	doi = {10.1145/3368474.3368492},
	abstract = {The low-precision floating point arithmetic that performs computation by reducing numerical accuracy with narrow bit-width is attracting since it can improve the performance of the numerical programs. Small memory footprint, faster computing speed, and energy saving are expected by performing calculation with low precision data. However, there have not been many studies on how low-precision arithmetics affects power and energy consumption of numerical codes. In this study, we investigate the power efficiency improvement by aggressively using low-precision arithmetics for HPC applications. In our evaluations, we analyze power characteristics of the Poisson's equation and the ground motion simulation programs with double precision and single precision floating point arithmetics. We confirm that energy efficiency improves by using low-precision arithmetics but it is heavily influenced by parameters such as data division and the number of OpenMP threads.},
	author = {Sakamoto, Ryuichi and Kondo, Masaaki and Fujita, Kohei and Ichimura, Tsuyoshi and Nakajima, Kengo},
	month = jan,
	year = {2020},
	pages = {199--206},
}

@misc{noauthor_effectiveness_nodate,
	title = {The {Effectiveness} of {Low}-{Precision} {Floating} {Arithmetic} on {Numerical} {Codes} {\textbar} {Proceedings} of the {International} {Conference} on {High} {Performance} {Computing} in {Asia}-{Pacific} {Region}},
	url = {https://dl.acm.org/doi/10.1145/3368474.3368492},
	urldate = {2021-04-26},
}

@article{li_tcfft_2021,
	title = {{tcFFT}: {Accelerating} {Half}-{Precision} {FFT} through {Tensor} {Cores}},
	shorttitle = {{tcFFT}},
	url = {http://arxiv.org/abs/2104.11471},
	abstract = {Fast Fourier Transform (FFT) is an essential tool in scientific and engineering computation. The increasing demand for mixed-precision FFT has made it possible to utilize half-precision floating-point (FP16) arithmetic for faster speed and energy saving. Specializing in lower precision, NVIDIA Tensor Cores can deliver extremely high computation performance. However, the fixed computation pattern makes it hard to utilize the computing power of Tensor Cores in FFT. Therefore, we developed tcFFT to accelerate FFT with Tensor Cores. Our tcFFT supports batched 1D and 2D FFT of various sizes and it exploits a set of optimizations to achieve high performance: 1) single-element manipulation on Tensor Core fragments to support special operations needed by FFT; 2) fine-grained data arrangement design to coordinate with the GPU memory access pattern. We evaluated our tcFFT and the NVIDIA cuFFT in various sizes and dimensions on NVIDIA V100 and A100 GPUs. The results show that our tcFFT can outperform cuFFT 1.29x-3.24x and 1.10x-3.03x on the two GPUs, respectively. Our tcFFT has a great potential for mixed-precision scientific applications.},
	urldate = {2021-04-26},
	journal = {arXiv:2104.11471 [cs]},
	author = {Li, Binrui and Cheng, Shenggan and Lin, James},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.11471
version: 1},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
}

@article{clark_solving_2010,
	title = {Solving {Lattice} {QCD} systems of equations using mixed precision solvers on {GPUs}},
	volume = {181},
	issn = {00104655},
	url = {http://arxiv.org/abs/0911.3191},
	doi = {10.1016/j.cpc.2010.05.002},
	abstract = {Modern graphics hardware is designed for highly parallel numerical tasks and promises significant cost and performance benefits for many scientific applications. One such application is lattice quantum chromodyamics (lattice QCD), where the main computational challenge is to efficiently solve the discretized Dirac equation in the presence of an SU(3) gauge field. Using NVIDIA's CUDA platform we have implemented a Wilson-Dirac sparse matrix-vector product that performs at up to 40 Gflops, 135 Gflops and 212 Gflops for double, single and half precision respectively on NVIDIA's GeForce GTX 280 GPU. We have developed a new mixed precision approach for Krylov solvers using reliable updates which allows for full double precision accuracy while using only single or half precision arithmetic for the bulk of the computation. The resulting BiCGstab and CG solvers run in excess of 100 Gflops and, in terms of iterations until convergence, perform better than the usual defect-correction approach for mixed precision.},
	number = {9},
	urldate = {2021-04-26},
	journal = {Computer Physics Communications},
	author = {Clark, M. A. and Babich, R. and Barros, K. and Brower, R. C. and Rebbi, C.},
	month = sep,
	year = {2010},
	note = {arXiv: 0911.3191},
	keywords = {High Energy Physics - Lattice},
	pages = {1517--1528},
}

@article{domke_double-precision_2019,
	title = {Double-precision {FPUs} in {High}-{Performance} {Computing}: an {Embarrassment} of {Riches}?},
	shorttitle = {Double-precision {FPUs} in {High}-{Performance} {Computing}},
	url = {http://arxiv.org/abs/1810.09330},
	abstract = {Among the (uncontended) common wisdom in High-Performance Computing (HPC) is the applications' need for large amount of double-precision support in hardware. Hardware manufacturers, the TOP500 list, and (rarely revisited) legacy software have without doubt followed and contributed to this view. In this paper, we challenge that wisdom, and we do so by exhaustively comparing a large number of HPC proxy application on two processors: Intel's Knights Landing (KNL) and Knights Mill (KNM). Although similar, the KNM and KNL architecturally deviate at one important point: the silicon area devoted to double-precision arithmetic's. This fortunate discrepancy allows us to empirically quantify the performance impact in reducing the amount of hardware double-precision arithmetic. Our analysis shows that this common wisdom might not always be right. We find that the investigated HPC proxy applications do allow for a (significant) reduction in double-precision with little-to-no performance implications. With the advent of a failing of Moore's law, our results partially reinforce the view taken by modern industry (e.g. upcoming Fujitsu ARM64FX) to integrate hybrid-precision hardware units.},
	urldate = {2021-04-26},
	journal = {arXiv:1810.09330 [cs]},
	author = {Domke, Jens and Matsumura, Kazuaki and Wahib, Mohamed and Zhang, Haoyu and Yashima, Keita and Tsuchikawa, Toshiki and Tsuji, Yohei and Podobas, Artur and Matsuoka, Satoshi},
	month = mar,
	year = {2019},
	note = {arXiv: 1810.09330},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
}

@article{ozaki_error-free_2012,
	title = {Error-free transformations of matrix multiplication by using fast routines of matrix multiplication and its applications},
	volume = {59},
	issn = {1572-9265},
	url = {https://doi.org/10.1007/s11075-011-9478-1},
	doi = {10.1007/s11075-011-9478-1},
	abstract = {This paper is concerned with accurate matrix multiplication in floating-point arithmetic. Recently, an accurate summation algorithm was developed by Rump et al. (SIAM J Sci Comput 31(1):189–224, 2008). The key technique of their method is a fast error-free splitting of floating-point numbers. Using this technique, we first develop an error-free transformation of a product of two floating-point matrices into a sum of floating-point matrices. Next, we partially apply this error-free transformation and develop an algorithm which aims to output an accurate approximation of the matrix product. In addition, an a priori error estimate is given. It is a characteristic of the proposed method that in terms of computation as well as in terms of memory consumption, the dominant part of our algorithm is constituted by ordinary floating-point matrix multiplications. The routine for matrix multiplication is highly optimized using BLAS, so that our algorithms show a good computational performance. Although our algorithms require a significant amount of working memory, they are significantly faster than ‘gemmx’ in XBLAS when all sizes of matrices are large enough to realize nearly peak performance of ‘gemm’. Numerical examples illustrate the efficiency of the proposed method.},
	language = {en},
	number = {1},
	urldate = {2021-04-23},
	journal = {Numerical Algorithms},
	author = {Ozaki, Katsuhisa and Ogita, Takeshi and Oishi, Shin’ichi and Rump, Siegfried M.},
	month = jan,
	year = {2012},
	pages = {95--118},
}

@article{bhaskaracharya_automatic_2020,
	title = {Automatic {Kernel} {Generation} for {Volta} {Tensor} {Cores}},
	url = {http://arxiv.org/abs/2006.12645},
	abstract = {A commonly occurring computation idiom in neural networks is to perform some pointwise operations on the result of a matrix multiplication. Such a sequence of operations is typically represented as a computation graph in deep learning compilers. When compiling to a GPU target, these computations can be individually mapped to manually tuned implementations provided by libraries such as cuBLAS and cuDNN. These libraries also provide off-the-shelf support for targeting tensor cores in NVIDIA GPUs, which can lead to huge performance boosts through their specialized support for mixed-precision matrix math. Alternatively, tensor cores can be programmed directly using CUDA APIs or inline assembly instructions, which opens up the possibility of generating efficient CUDA kernels automatically for such computations. Automatic kernel generation is particularly crucial when it is beneficial to generate efficient code for an entire computation graph by fusing several operations into a single device function instead of invoking a separate kernel for each of them. Polyhedral compilation techniques provide a systematic approach for the analysis and transformation of a sequence of affine loop-nests. In this paper, we describe a polyhedral approach to generate efficient CUDA kernels for matrix multiplication using inline assembly instructions for programming tensor cores on NVIDIA Volta GPUs. Furthermore, we build on this approach to generate fused kernels for computation sequences involving matrix multiplication and pointwise operations such as bias addition, ReLU activation etc. Experimental evaluation of these techniques show that automatically generated kernels can provide significantly better performance than manually tuned library implementations, with speedups ranging up to 2.55X.},
	urldate = {2021-04-21},
	journal = {arXiv:2006.12645 [cs]},
	author = {Bhaskaracharya, Somashekaracharya G. and Demouth, Julien and Grover, Vinod},
	month = aug,
	year = {2020},
	note = {arXiv: 2006.12645},
	keywords = {Computer Science - Programming Languages},
}

@article{jia_dissecting_2018,
	title = {Dissecting the {NVIDIA} {Volta} {GPU} {Architecture} via {Microbenchmarking}},
	url = {http://arxiv.org/abs/1804.06826},
	abstract = {Every year, novel NVIDIA GPU designs are introduced. This rapid architectural and technological progression, coupled with a reluctance by manufacturers to disclose low-level details, makes it difficult for even the most proficient GPU software designers to remain up-to-date with the technological advances at a microarchitectural level. To address this dearth of public, microarchitectural-level information on the novel NVIDIA GPUs, independent researchers have resorted to microbenchmarks-based dissection and discovery. This has led to a prolific line of publications that shed light on instruction encoding, and memory hierarchy's geometry and features at each level. Namely, research that describes the performance and behavior of the Kepler, Maxwell and Pascal architectures. In this technical report, we continue this line of research by presenting the microarchitectural details of the NVIDIA Volta architecture, discovered through microbenchmarks and instruction set disassembly. Additionally, we compare quantitatively our Volta findings against its predecessors, Kepler, Maxwell and Pascal.},
	urldate = {2021-04-16},
	journal = {arXiv:1804.06826 [cs]},
	author = {Jia, Zhe and Maggioni, Marco and Staiger, Benjamin and Scarpazza, Daniele P.},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.06826},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance},
}

@misc{noauthor_latex_nodate,
	title = {In {Latex}, how do {I} create citations to references with a hyperlink?},
	url = {https://www.researchgate.net/post/In-Latex-how-do-I-create-citations-to-references-with-a-hyperlink},
	abstract = {Read 21 answers by scientists to the question asked by Eranga Ukwatta on Jul 9, 2013},
	language = {en},
	urldate = {2021-04-16},
	journal = {ResearchGate},
}

@article{daghaghi_accelerating_2021,
	title = {Accelerating {SLIDE} {Deep} {Learning} on {Modern} {CPUs}: {Vectorization}, {Quantizations}, {Memory} {Optimizations}, and {More}},
	volume = {3},
	shorttitle = {Accelerating {SLIDE} {Deep} {Learning} on {Modern} {CPUs}},
	url = {https://proceedings.mlsys.org/paper/2021/hash/3636638817772e42b59d74cff571fbb3-Abstract.html},
	language = {en},
	urldate = {2021-04-09},
	journal = {Proceedings of Machine Learning and Systems},
	author = {Daghaghi, Shabnam and Meisburger, Nicholas and Zhao, Mengnan and Shrivastava, Anshumali},
	month = mar,
	year = {2021},
}

@article{palem_ten_2013,
	title = {Ten {Years} of {Building} {Broken} {Chips}: {The} {Physics} and {Engineering} of {Inexact} {Computing}},
	volume = {12},
	issn = {1539-9087},
	shorttitle = {Ten {Years} of {Building} {Broken} {Chips}},
	url = {https://doi.org/10.1145/2465787.2465789},
	doi = {10.1145/2465787.2465789},
	abstract = {Well over a decade ago, many believed that an engine of growth driving the semiconductor and computing industries---captured nicely by Gordon Moore’s remarkable prophecy (Moore’s law)---was speeding towards a dangerous cliff-edge. Ranging from expressions of concern to doomsday scenarios, the exact time when serious hurdles would beset us varied quite a bit---some of the more optimistic warnings giving Moore’s law until. Needless to say, a lot of people have spent time and effort with great success to find ways for substantially extending the time when we would encounter the dreaded cliff-edge, if not avoiding it altogether. Faced with this issue, we started approaching this in a decidedly different manner---one which suggested falling off the metaphorical cliff as a design choice, but in a controlled way. This resulted in devices that could switch and produce bits that are correct, namely of having the intended value, only with a probabilistic guarantee. As a result, the results could in fact be incorrect. Such devices and associated circuits and computing structures are now broadly referred to as inexact designs, circuits, and architectures. In this article, we will crystallize the essence of inexactness dating back to 2002 through two key principles that we developed: (i) that of admitting error in a design in return for resource savings, and subsequently (ii) making resource investments in the elements of a hardware platform proportional to the value of information they compute. We will also give a broad overview of a range of inexact designs and hardware concepts that our group and other groups around the world have been developing since, based on these two principles. Despite not being deterministically precise, inexact designs can be significantly more efficient in the energy they consume, their speed of execution, and their area needs, which makes them attractive in application contexts that are resilient to error. Significantly, our development of inexactness will be contrasted against the rich backdrop of traditional approaches aimed at realizing reliable computing from unreliable elements, starting with von Neumann’s influential lectures and further developed by Shannon-Weaver and others.},
	number = {2s},
	urldate = {2021-04-07},
	journal = {ACM Transactions on Embedded Computing Systems},
	author = {Palem, Krishna and Lingamneni, Avinash},
	month = may,
	year = {2013},
	keywords = {Co-design, EDA, Moore’s law, VLSI design, energy-accuracy trade-off, inexact circuit design, low power/energy, probabilistic CMOS},
	pages = {87:1--87:23},
}

@article{higham_simulating_2019,
	title = {Simulating {Low} {Precision} {Floating}-{Point} {Arithmetic}},
	volume = {41},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/19M1251308},
	doi = {10.1137/19M1251308},
	abstract = {The half-precision (fp16) floating-point format, defined in the 2008 revision of the IEEE standard for floating-point arithmetic, and a more recently proposed half-precision format bfloat16, are increasingly available in GPUs and other accelerators. While the support for low precision arithmetic is mainly motivated by machine learning applications, general purpose numerical algorithms can benefit from it, too, gaining in speed, energy usage, and reduced communication costs. Since the appropriate hardware is not always available, and one may wish to experiment with new arithmetics not yet implemented in hardware, software simulations of low precision arithmetic are needed. We discuss how to simulate low precision arithmetic using arithmetic of higher precision. We examine the correctness of such simulations and explain via rounding error analysis why a natural method of simulation can provide results that are more accurate than actual computations at low precision. We provide a MATLAB function, chop, that can be used to efficiently simulate fp16, bfloat16, and other low precision arithmetics, with or without the representation of subnormal numbers and with the options of round to nearest, directed rounding, stochastic rounding, and random bit flips in the significand. We demonstrate the advantages of this approach over defining a new MATLAB class and overloading operators.},
	number = {5},
	urldate = {2021-04-07},
	journal = {SIAM Journal on Scientific Computing},
	author = {Higham, Nicholas J. and Pranesh, Srikara},
	month = jan,
	year = {2019},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {C585--C602},
}

@article{yamazaki_real-time_2019,
	title = {Real-time simulation of a cat-scale artificial cerebellum on {PEZY}-{SC} processors},
	volume = {33},
	issn = {1094-3420},
	url = {https://doi.org/10.1177/1094342017710705},
	doi = {10.1177/1094342017710705},
	abstract = {The cerebellum is a part of the brain that plays essential roles in real-time motor learning and control and even cognitive functions. Thanks to the large amount of anatomical and physiological data, we implemented a spiking network model of the cerebellum on Shoubu, an energy-efficient supercomputer with 1280 PEZY-SC processors at RIKEN. Our artificial cerebellum consists of more than one billion neurons, which is comparable to the whole cerebellum of a cat. Using 1008 of 1280 processors on Shoubu, we achieved real-time simulation, that is, a computer simulation of the model for 1 s completes within 1 s of the real world with temporal resolution of 1 ms. Effective performance was estimated as 68 teraflops in single-precision floating points, suggesting 2.6\% of peak performance. We expect that the artificial cerebellum will be applicable to various engineering applications, such as robotics and brain-style artificial intelligence.},
	language = {en},
	number = {1},
	urldate = {2021-04-05},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Yamazaki, Tadashi and Igarashi, Jun and Makino, Junichiro and Ebisuzaki, Toshikazu},
	month = jan,
	year = {2019},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {Brain, PEZY-SC, accelerator, application, cerebellum, real time, simulation, spiking neuron},
	pages = {155--168},
}

@article{yamazaki_real-time_2017,
	title = {Real-time simulation of a cat-scale artificial cerebellum on {PEZY}-{SC} processors},
	volume = {33},
	doi = {10.1177/1094342017710705},
	abstract = {The cerebellum is a part of the brain that plays essential roles in real-time motor learning and control and even cognitive functions. Thanks to the large amount of anatomical and physiological data, we implemented a spiking network model of the cerebellum on Shoubu, an energy-efficient supercomputer with 1280 PEZY-SC processors at RIKEN. Our artificial cerebellum consists of more than one billion neurons, which is comparable to the whole cerebellum of a cat. Using 1008 of 1280 processors on Shoubu, we achieved real-time simulation, that is, a computer simulation of the model for 1 s completes within 1 s of the real world with temporal resolution of 1 ms. Effective performance was estimated as 68 teraflops in single-precision floating points, suggesting 2.6\% of peak performance. We expect that the artificial cerebellum will be applicable to various engineering applications, such as robotics and brain-style artificial intelligence.},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Yamazaki, Tadashi and Igarashi, Jun and Makino, Jun and Ebisuzaki, Toshikazu},
	month = jun,
	year = {2017},
	pages = {109434201771070},
}

@article{oseledets_tensor-train_2011,
	title = {Tensor-{Train} {Decomposition}},
	volume = {33},
	doi = {10.1137/090752286},
	abstract = {A simple nonrecursive form of the tensor decomposition in \$d\$ dimensions is presented. It does not inherently suffer from the curse of dimensionality, it has asymptotically the same number of parameters as the canonical decomposition, but it is stable and its computation is based on low-rank approximation of auxiliary unfolding matrices. The new form gives a clear and convenient way to implement all basic operations efficiently. A fast rounding procedure is presented, as well as basic linear algebra operations. Examples showing the benefits of the decomposition are given, and the efficiency is demonstrated by the computation of the smallest eigenvalue of a 19-dimensional operator.},
	journal = {SIAM J. Scientific Computing},
	author = {Oseledets, Ivan},
	month = jan,
	year = {2011},
	pages = {2295--2317},
}

@article{huber_randomized_2017,
	title = {A {Randomized} {Tensor} {Train} {Singular} {Value} {Decomposition}},
	url = {http://arxiv.org/abs/1710.08513},
	abstract = {The hierarchical SVD provides a quasi-best low rank approximation of high dimensional data in the hierarchical Tucker framework. Similar to the SVD for matrices, it provides a fundamental but expensive tool for tensor computations. In the present work we examine generalizations of randomized matrix decomposition methods to higher order tensors in the framework of the hierarchical tensors representation. In particular we present and analyze a randomized algorithm for the calculation of the hierarchical SVD (HSVD) for the tensor train (TT) format.},
	urldate = {2021-04-05},
	journal = {arXiv:1710.08513 [math]},
	author = {Huber, Benjamin and Schneider, Reinhold and Wolf, Sebastian},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.08513},
	keywords = {15A69, Mathematics - Numerical Analysis},
}

@inproceedings{carmichael_deep_2019,
	title = {Deep {Positron}: {A} {Deep} {Neural} {Network} {Using} the {Posit} {Number} {System}},
	shorttitle = {Deep {Positron}},
	doi = {10.23919/DATE.2019.8715262},
	abstract = {The recent surge of interest in Deep Neural Networks (DNNs) has led to increasingly complex networks that tax computational and memory resources. Many DNNs presently use 16-bit or 32-bit floating point operations. Significant performance and power gains can be obtained when DNN accelerators support low-precision numerical formats. Despite considerable research, there is still a knowledge gap on how low-precision operations can be realized for both DNN training and inference. In this work, we propose a DNN architecture, Deep Positron, with posit numerical format operating successfully at ≤8 bits for inference. We propose a precision-adaptable FPGA soft core for exact multiply-and-accumulate for uniform comparison across three numerical formats, fixed, floating-point and posit. Preliminary results demonstrate that 8-bit posit has better accuracy than 8-bit fixed or floating-point for three different low-dimensional datasets. Moreover, the accuracy is comparable to 32-bit floating-point on a Xilinx Virtex-7 FPGA device. The trade-offs between DNN performance and hardware resources, i.e. latency, power, and resource utilization, show that posit outperforms in accuracy and latency at 8-bit and below.},
	booktitle = {2019 {Design}, {Automation} {Test} in {Europe} {Conference} {Exhibition} ({DATE})},
	author = {Carmichael, Z. and Langroudi, H. F. and Khazanov, C. and Lillie, J. and Gustafson, J. L. and Kudithipudi, D.},
	month = mar,
	year = {2019},
	note = {ISSN: 1558-1101},
	keywords = {Biological neural networks, Computer architecture, DNN accelerators, Dynamic range, Field programmable gate arrays, Neurons, Positrons, Training, deep neural networks, floating point, low-precision, machine learning, posits, tapered precision},
	pages = {1421--1426},
}

@article{rohrig-zollner_performance_2021,
	title = {Performance of low-rank approximations in tensor train format ({TT}-{SVD}) for large dense tensors},
	url = {http://arxiv.org/abs/2102.00104},
	abstract = {There are several factorizations of multi-dimensional tensors into lower-dimensional components, known as `tensor networks'. We consider the popular `tensor-train' (TT) format and ask, how efficiently can we compute a low-rank approximation from a full tensor on current multi-core CPUs. Compared to sparse and dense linear algebra, there are much fewer and less extensive well-optimized kernel libraries for multi-linear algebra. Linear algebra libraries like BLAS and LAPACK may provide the required operations in principle, but often at the cost of additional data movements for rearranging memory layouts. Furthermore, these libraries are typically optimized for the compute-bound case (e.g.{\textbackslash} square matrix operations) whereas low-rank tensor decompositions lead to memory bandwidth limited operations. We propose a `tensor-train singular value decomposition' (TT-SVD) algorithm based on two building blocks: a `Q-less tall-skinny QR' factorization, and a fused tall-skinny matrix-matrix multiplication and reshape operation. We analyze the performance of the resulting TT-SVD algorithm using the Roofline performance model. In addition, we present performance results for different algorithmic variants for shared-memory as well as distributed-memory architectures. Our experiments show that commonly used TT-SVD implementations suffer severe performance penalties. We conclude that a dedicated library for tensor factorization kernels would benefit the community: Computing a low-rank approximation can be as cheap as reading the data twice from main memory. As a consequence, an implementation that achieves realistic performance will move the limit at which one has to resort to randomized methods that only process part of the data.},
	urldate = {2021-04-04},
	journal = {arXiv:2102.00104 [cs, math]},
	author = {Röhrig-Zöllner, Melven and Thies, Jonas and Basermann, Achim},
	month = jan,
	year = {2021},
	note = {arXiv: 2102.00104},
	keywords = {15A23, 15A69, 65F99, 65Y05, 65Y20, Computer Science - Mathematical Software, G.1.3, G.4, Mathematics - Numerical Analysis},
}

@techreport{ducas_advanced_2021,
	title = {Advanced {Lattice} {Sieving} on {GPUs}, with {Tensor} {Cores}},
	url = {https://eprint.iacr.org/2021/141},
	abstract = {In this work, we study GPU implementations of various state-of-the-art sieving algorithms for lattices (Becker-Gama-Joux 2015, Becker-Ducas-Gama-Laarhoven 2016, Herold-Kirshanova 2017) inside the General Sieve Kernel (G6K, Albrecht et al. 2019). In particular, we extensively exploit the recently introduced *Tensor Cores* -- originally designed for raytracing and machine learning -- and demonstrate their fitness for the cryptanalytic task at hand. We also propose a new *dual-hash* technique for efficient detection of `lift-worthy' pairs to accelerate a key ingredient of G6K: finding short lifted vectors.},
	number = {141},
	urldate = {2021-04-04},
	author = {Ducas, Léo and Stevens, Marc and Woerden, Wessel van},
	year = {2021},
	keywords = {Challenges., Cryptanalysis, G6K, Lattice Sieving, Shortest Vector, public-key cryptography},
}

@inproceedings{chowdhury_computational_2020,
	address = {New York, NY, USA},
	series = {{SPAA} '20},
	title = {A {Computational} {Model} for {Tensor} {Core} {Units}},
	isbn = {978-1-4503-6935-0},
	url = {https://doi.org/10.1145/3350755.3400252},
	doi = {10.1145/3350755.3400252},
	abstract = {To respond to the need for efficient training and inference of deep neural networks, a plethora of domain-specific architectures have been introduced, such as Google Tensor Processing Units and NVIDIA Tensor Cores. A common feature of these architectures is the design for efficiently computing a dense matrix product of a given small size. In order to broaden the class of algorithms that exploit these systems, we propose a computational model, named the TCU model, that captures the ability to natively multiply small matrices. We then use the TCU model for designing fast algorithms for several problems, including dense and sparse matrix multiplication and the Discrete Fourier Transform. We finally highlight a relation between the TCU model and the external memory model.},
	urldate = {2021-04-04},
	booktitle = {Proceedings of the 32nd {ACM} {Symposium} on {Parallelism} in {Algorithms} and {Architectures}},
	publisher = {Association for Computing Machinery},
	author = {Chowdhury, Rezaul and Silvestri, Francesco and Vella, Flavio},
	month = jul,
	year = {2020},
	keywords = {computational model, efficient algorithms, graph problems, hardware accelerators, linear algebra, tensor core},
	pages = {519--521},
}

@article{choquette_volta_2018,
	title = {Volta: {Performance} and {Programmability}},
	volume = {38},
	issn = {1937-4143},
	shorttitle = {Volta},
	doi = {10.1109/MM.2018.022071134},
	abstract = {GV100 is NVIDIAs latest flagship GPU. It has been designed with many features to improve performance and programmability. It features enhancements to NVLink, a redesigned streaming microprocessor (SM), and independent thread scheduling enhancements to the single instruction, multiple threads (SIMT) model. GV100 also adds new tensor cores for an order of magnitude throughput improvement for deep-learning kernels.},
	number = {2},
	journal = {IEEE Micro},
	author = {Choquette, J. and Giroux, O. and Foley, D.},
	month = mar,
	year = {2018},
	note = {Conference Name: IEEE Micro},
	keywords = {Bandwidth, CUDA, Central Processing Unit, GPU, GV100, Graphics processing units, HPC, Instruction sets, NVLink, Processor scheduling, Tensile stress, Tesla, Volta, deep learning, tensor core},
	pages = {42--52},
}

@article{raihan_modeling_2019,
	title = {Modeling {Deep} {Learning} {Accelerator} {Enabled} {GPUs}},
	url = {http://arxiv.org/abs/1811.08309},
	abstract = {The efficacy of deep learning has resulted in its use in a growing number of applications. The Volta graphics processor unit (GPU) architecture from NVIDIA introduced a specialized functional unit, the "tensor core", that helps meet the growing demand for higher performance for deep learning. In this paper we study the design of the tensor cores in NVIDIA's Volta and Turing architectures. We further propose an architectural model for the tensor cores in Volta. When implemented a GPU simulator, GPGPU-Sim, our tensor core model achieves 99.6{\textbackslash}\% correlation versus an NVIDIA Titan{\textasciitilde}V GPU in terms of average instructions per cycle when running tensor core enabled GEMM workloads. We also describe support added to enable GPGPU-Sim to run CUTLASS, an open-source CUDA C++ template library providing customizable GEMM templates that utilize tensor cores.},
	urldate = {2021-04-04},
	journal = {arXiv:1811.08309 [cs]},
	author = {Raihan, Md Aamir and Goli, Negar and Aamodt, Tor},
	month = feb,
	year = {2019},
	note = {arXiv: 1811.08309},
	keywords = {Computer Science - Hardware Architecture, Computer Science - Mathematical Software},
}

@article{navarro_gpu_2020,
	title = {{GPU} {Tensor} {Cores} for fast {Arithmetic} {Reductions}},
	url = {http://arxiv.org/abs/2001.05585},
	abstract = {This work proposes a GPU tensor core approach that encodes the arithmetic reduction of \$n\$ numbers as a set of chained \$m {\textbackslash}times m\$ matrix multiply accumulate (MMA) operations executed in parallel by GPU tensor cores. The asymptotic running time of the proposed chained tensor core approach is \$T(n)=5 log\_\{m{\textasciicircum}2\}\{n\}\$ and its speedup is \$S={\textbackslash}dfrac\{4\}\{5\} log\_\{2\}\{m{\textasciicircum}2\}\$ over the classic \$O(n {\textbackslash}log n)\$ parallel reduction algorithm. Experimental performance results show that the proposed reduction method is \${\textbackslash}sim 3.2 {\textbackslash}times\$ faster than a conventional GPU reduction implementation, and preserves the numerical precision because the sub-results of each chain of \$R\$ MMAs is kept as a 32-bit floating point value, before being all reduced into as a final 32-bit result. The chained MMA design allows a flexible configuration of thread-blocks; small thread-blocks of 32 or 128 threads can still achieve maximum performance using a chain of \$R=4,5\$ MMAs per block, while large thread-blocks work best with \$R=1\$. The results obtained in this work show that tensor cores can indeed provide a significant performance improvement to non-Machine Learning applications such as the arithmetic reduction, which is an integration tool for studying many scientific phenomena.},
	urldate = {2021-04-02},
	journal = {arXiv:2001.05585 [cs]},
	author = {Navarro, Cristóbal A. and Carrasco, Roberto and Barrientos, Ricardo J. and Riquelme, Javier A. and Vega, Raimundo},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.05585},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
}

@article{bailey_high-precision_2005,
	title = {High-precision floating-point arithmetic in scientific computation},
	volume = {7},
	issn = {1558-366X},
	doi = {10.1109/MCSE.2005.52},
	abstract = {IEEE 64-bit floating-point arithmetic is sufficient for most scientific applications, but a rapidly growing body of scientific computing applications requires a higher level of numeric precision. Software packages have yielded interesting scientific results that suggest numeric precision in scientific computations could be as important to program design as algorithms and data structures.},
	number = {3},
	journal = {Computing in Science Engineering},
	author = {Bailey, D. H.},
	month = may,
	year = {2005},
	note = {Conference Name: Computing in Science Engineering},
	keywords = {Application software, Floating-point arithmetic, High level languages, High performance computing, Mathematics, Packaging, Personal communication networks, Software algorithms, Software packages, Software performance, climate modeling, computational chemistry, computational physics, experimental mathematics, high-precision arithmetic, quantum theory},
	pages = {54--61},
}

@inproceedings{durrani_fft_2021,
	address = {New York, NY, USA},
	series = {{PPoPP} '21},
	title = {{FFT} blitz: the tensor cores strike back},
	isbn = {978-1-4503-8294-6},
	shorttitle = {{FFT} blitz},
	url = {https://doi.org/10.1145/3437801.3441623},
	doi = {10.1145/3437801.3441623},
	abstract = {The fast Fourier Transform (FFT), a reduced-complexity formulation of the Discrete Fourier Transform (DFT), is an important tool in many areas of science and engineering. FFTW is a well-known package that follows this approach and is currently one of the fastest available implementations of the FFT. NVIDIA introduced its version of FFTW called cuFFT that achieves high performance on the GPUs. In this work we present a novel way to map the FFT algorithm on the newly introduced Tensor Cores by adapting the the Cooley-Tukey recursive FFT algorithm. We present four major types of optimizations that enhance the performance of our approach for varying FFT sizes and show that the approach consistently outperforms cuFFT with a speedup of about 15\% to 250\% on average.},
	urldate = {2021-04-02},
	booktitle = {Proceedings of the 26th {ACM} {SIGPLAN} {Symposium} on {Principles} and {Practice} of {Parallel} {Programming}},
	publisher = {Association for Computing Machinery},
	author = {Durrani, Sultan and Chughtai, Muhammad Saad and Dakkak, Abdul and Hwu, Wen-mei and Rauchwerger, Lawrence},
	month = feb,
	year = {2021},
	keywords = {DFT, FFT, GPU, tensor cores},
	pages = {488--489},
}

@inproceedings{gelado_throughput-oriented_2019,
	address = {New York, NY, USA},
	series = {{PPoPP} '19},
	title = {Throughput-oriented {GPU} memory allocation},
	isbn = {978-1-4503-6225-2},
	url = {https://doi.org/10.1145/3293883.3295727},
	doi = {10.1145/3293883.3295727},
	abstract = {Throughput-oriented architectures, such as GPUs, can sustain three orders of magnitude more concurrent threads than multicore architectures. This level of concurrency pushes typical synchronization primitives (e.g., mutexes) over their scalability limits, creating significant performance bottlenecks in modules, such as memory allocators, that use them. In this paper, we develop concurrent programming techniques and synchronization primitives, in support of a dynamic memory allocator, that are efficient for use with very high levels of concurrency. We formulate resource allocation as a two-stage process, that decouples accounting for the number of available resources from the tracking of the available resources themselves. To facilitate the accounting stage, we introduce a novel bulk semaphore abstraction that extends traditional semaphore semantics by optimizing for the case where threads operate on the semaphore simultaneously. We also similarly design new collective synchronization primitives that enable groups of cooperating threads to enter critical sections together. Finally, we show that delegation of deferred reclamation to threads already blocked greatly improves efficiency. Using all these techniques, our throughput-oriented memory allocator delivers both high allocation rates and low memory fragmentation on modern GPUs. Our experiments demonstrate that it achieves allocation rates that are on average 16.56 times higher than the counterpart implementation in the CUDA 9 toolkit.},
	urldate = {2021-04-02},
	booktitle = {Proceedings of the 24th {Symposium} on {Principles} and {Practice} of {Parallel} {Programming}},
	publisher = {Association for Computing Machinery},
	author = {Gelado, Isaac and Garland, Michael},
	month = feb,
	year = {2019},
	keywords = {GPU programming, concurrency, memory allocation},
	pages = {27--37},
}

@article{ahle_similarity_2020,
	title = {Similarity {Search} with {Tensor} {Core} {Units}},
	url = {http://arxiv.org/abs/2006.12608},
	abstract = {Tensor Core Units (TCUs) are hardware accelerators developed for deep neural networks, which efficiently support the multiplication of two dense \${\textbackslash}sqrt\{m\}{\textbackslash}times {\textbackslash}sqrt\{m\}\$ matrices, where \$m\$ is a given hardware parameter. In this paper, we show that TCUs can speed up similarity search problems as well. We propose algorithms for the Johnson-Lindenstrauss dimensionality reduction and for similarity join that, by leveraging TCUs, achieve a \${\textbackslash}sqrt\{m\}\$ speedup up with respect to traditional approaches.},
	urldate = {2021-04-02},
	journal = {arXiv:2006.12608 [cs]},
	author = {Ahle, Thomas D. and Silvestri, Francesco},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.12608},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Information Retrieval},
}

@article{johnson_billion-scale_2019,
	title = {Billion-scale similarity search with {GPUs}},
	issn = {2332-7790},
	doi = {10.1109/TBDATA.2019.2921572},
	abstract = {Similarity search finds application in database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data parallel tasks such as distance computation, prior approaches in this domain are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy. We propose a novel design for k-selection. We apply it in different similarity search scenarios, by optimizing brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation operates at up to 55\% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art. It enables the construction of a high accuracy k-NN graph on 95 million images from the YFCC100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.},
	journal = {IEEE Transactions on Big Data},
	author = {Johnson, J. and Douze, M. and Jégou, H.},
	year = {2019},
	note = {Conference Name: IEEE Transactions on Big Data},
	keywords = {Big Data, Graphics processing units, Indexing, Quantization (signal), Random access memory, Task analysis},
	pages = {1--1},
}

@article{johnson_billion-scale_2019-1,
	title = {Billion-scale similarity search with {GPUs}},
	issn = {2332-7790},
	doi = {10.1109/TBDATA.2019.2921572},
	abstract = {Similarity search finds application in database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data parallel tasks such as distance computation, prior approaches in this domain are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy. We propose a novel design for k-selection. We apply it in different similarity search scenarios, by optimizing brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation operates at up to 55\% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art. It enables the construction of a high accuracy k-NN graph on 95 million images from the YFCC100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.},
	journal = {IEEE Transactions on Big Data},
	author = {Johnson, J. and Douze, M. and Jégou, H.},
	year = {2019},
	note = {Conference Name: IEEE Transactions on Big Data},
	keywords = {Big Data, Graphics processing units, Indexing, Quantization (signal), Random access memory, Task analysis},
	pages = {1--1},
}

@inproceedings{kepner_mathematical_2016,
	title = {Mathematical foundations of the {GraphBLAS}},
	doi = {10.1109/HPEC.2016.7761646},
	abstract = {The GraphBLAS standard (GraphBlas.org) is being developed to bring the potential of matrix-based graph algorithms to the broadest possible audience. Mathematically, the GraphBLAS defines a core set of matrix-based graph operations that can be used to implement a wide class of graph algorithms in a wide range of programming environments. This paper provides an introduction to the mathematics of the GraphBLAS. Graphs represent connections between vertices with edges. Matrices can represent a wide range of graphs using adjacency matrices or incidence matrices. Adjacency matrices are often easier to analyze while incidence matrices are often better for representing data. Fortunately, the two are easily connected by matrix multiplication. A key feature of matrix mathematics is that a very small number of matrix operations can be used to manipulate a very wide range of graphs. This composability of a small number of operations is the foundation of the GraphBLAS. A standard such as the GraphBLAS can only be effective if it has low performance overhead. Performance measurements of prototype GraphBLAS implementations indicate that the overhead is low.},
	booktitle = {2016 {IEEE} {High} {Performance} {Extreme} {Computing} {Conference} ({HPEC})},
	author = {Kepner, J. and Aaltonen, P. and Bader, D. and Buluç, A. and Franchetti, F. and Gilbert, J. and Hutchison, D. and Kumar, M. and Lumsdaine, A. and Meyerhenke, H. and McMillan, S. and Yang, C. and Owens, J. D. and Zalewski, M. and Mattson, T. and Moreira, J.},
	month = sep,
	year = {2016},
	keywords = {Additives, Finite element analysis, Matrices, Sparse matrices, Standards},
	pages = {1--9},
}

@book{kepner_graph_2011,
	address = {USA},
	title = {Graph {Algorithms} in the {Language} of {Linear} {Algebra}},
	isbn = {978-0-89871-990-1},
	abstract = {Graphs are among the most important abstract data types in computer science, and the algorithms that operate on them are critical to modern life. Graphs have been shown to be powerful tools for modeling complex problems because of their simplicity and generality. Graph algorithms are one of the pillars of mathematics, informing research in such diverse areas as combinatorial optimization, complexity theory, and topology. Algorithms on graphs are applied in many ways in today s world - from Web rankings to metabolic networks, from finite element meshes to semantic graphs. The current exponential growth in graph data has forced a shift to parallel computing for executing graph algorithms. Implementing parallel graph algorithms and achieving good parallel performance have proven difficult. This book addresses these challenges by exploiting the well-known duality between a canonical representation of graphs as abstract collections of vertices and edges and a sparse adjacency matrix representation. This linear algebraic approach is widely accessible to scientists and engineers who may not be formally trained in computer science. The authors show how to leverage existing parallel matrix computation techniques and the large amount of software infrastructure that exists for these computations to implement efficient and scalable parallel graph algorithms. The benefits of this approach are reduced algorithmic complexity, ease of implementation, and improved performance. Graph Algorithms in the Language of Linear Algebra is the first book to cover graph algorithms accessible to engineers and scientists not trained in computer science but having a strong linear algebra background, enabling them to quickly understand and apply graph algorithms. It also covers array-based graph algorithms, showing readers how to express canonical graph algorithms using a highly elegant and efficient array notation and how to tap into the large range of tools and techniques that have been built for matrices and tensors; parallel array-based algorithms, demonstrating with examples how to easily implement parallel graph algorithms using array-based approaches, which enables readers to address much larger graph problems; and array-based theory for analyzing graphs, providing a template for using array-based constructs to develop new theoretical approaches for graph analysis. Audience: This book is suitable as the primary text for a class on linear algebraic graph algorithms and as either the primary or supplemental text for a class on graph algorithms for engineers and scientists without training in computer science. Contents: List of Figures; List of Tables; List of Algorithms; Preface; Acknowledgments; Part I: Algorithms: Chapter 1: Graphs and Matrices; Chapter 2: Linear Algebraic Notation and Definitions; Chapter 3: Connected Components and Minimum Paths; Chapter 4: Some Graph Algorithms in an Array-Based Language; Chapter 5: Fundamental Graph Algorithms; Chapter 6: Complex Graph Algorithms; Chapter 7: Multilinear Algebra for Analyzing Data with Multiple Linkages; Chapter 8: Subgraph Detection; Part II: Data: Chapter 9: Kronecker Graphs; Chapter 10: The Kronecker Theory of Power Law Graphs; Chapter 11: Visualizing Large Kronecker Graphs; Part III: Computation: Chapter 12: Large-Scale Network Analysis; Chapter 13: Implementing Sparse Matrices for Graph Algorithms; Chapter 14: New Ideas in Sparse Matrix-Matrix Multiplication; Chapter 15: Parallel Mapping of Sparse Computations; Chapter 16: Fundamental Questions in the Analysis of Large Graphs; Index.},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Kepner, Jeremy and Gilbert, John},
	year = {2011},
}

@article{rump_accurate_2008,
	title = {Accurate {Floating}-{Point} {Summation} {Part} {II}: {Sign}, \${K}\$-{Fold} {Faithful} and {Rounding} to {Nearest}},
	volume = {31},
	issn = {1064-8275},
	shorttitle = {Accurate {Floating}-{Point} {Summation} {Part} {II}},
	url = {https://doi.org/10.1137/07068816X},
	doi = {10.1137/07068816X},
	abstract = {In Part II of this paper we first refine the analysis of error-free vector transformations presented in Part I. Based on that we present an algorithm for calculating the rounded-to-nearest result of \$s:={\textbackslash}sum p\_i\$ for a given vector of floating-point numbers \$p\_i\$, as well as algorithms for directed rounding. A special algorithm for computing the sign of \$s\$ is given, also working for huge dimensions. Assume a floating-point working precision with relative rounding error unit eps . We define and investigate a \$K\$-fold faithful rounding of a real number \$r\$. Basically the result is stored in a vector \${\textbackslash}mathtt\{Res\}\_\{{\textbackslash}nu\}\$ of \$K\$ nonoverlapping floating-point numbers such that \${\textbackslash}sum{\textbackslash}mathtt\{Res\}\_\{{\textbackslash}nu\}\$ approximates \$r\$ with relative accuracy \${\textbackslash}mathtt\{eps\}{\textasciicircum}K\$, and replacing \${\textbackslash}mathtt\{Res\}\_K\$ by its floating-point neighbors in \${\textbackslash}sum{\textbackslash}mathtt\{Res\}\_\{{\textbackslash}nu\}\$ forms a lower and upper bound for \$r\$. For a given vector of floating-point numbers with exact sum \$s\$, we present an algorithm for calculating a \$K\$-fold faithful rounding of \$s\$ using solely the working precision. Furthermore, an algorithm for calculating a faithfully rounded result of the sum of a vector of huge dimension is presented. Our algorithms are fast in terms of measured computing time because they allow good instruction-level parallelism, they neither require special operations such as access to mantissa or exponent, they contain no branch in the inner loop, nor do they require some extra precision. The only operations used are standard floating-point addition, subtraction, and multiplication in one working precision, for example, double precision. Certain constants used in the algorithms are proved to be optimal.},
	number = {2},
	urldate = {2021-03-31},
	journal = {SIAM Journal on Scientific Computing},
	author = {Rump, Siegfried M. and Ogita, Takeshi and Oishi, Shin'ichi},
	month = dec,
	year = {2008},
	keywords = {\$K\$-fold accuracy, XBLAS, directed rounding, distillation, error analysis, error-free transformations, faithful rounding, high accuracy, maximally accurate summation, rounding to nearest, sign},
	pages = {1269--1302},
}

@article{marsaglia_xorshift_2003,
	title = {Xorshift {RNGs}},
	volume = {08},
	doi = {10.18637/jss.v008.i14},
	abstract = {Description of a class of simple, extremely fast random number generators (RNGs) with periods 2k - 1 for k = 32, 64, 96, 128, 160,'2. These RNGs seem to pass tests of randomness very well.},
	journal = {Journal of Statistical Software},
	author = {Marsaglia, George},
	month = jan,
	year = {2003},
}

@inproceedings{nagasaka_high-performance_2017,
	title = {High-{Performance} and {Memory}-{Saving} {Sparse} {General} {Matrix}-{Matrix} {Multiplication} for {NVIDIA} {Pascal} {GPU}},
	doi = {10.1109/ICPP.2017.19},
	abstract = {Sparse general matrix-matrix multiplication (SpGEMM) is one of the key kernels of preconditioners such as algebraic multigrid method or graph algorithms. However, the performance of SpGEMM is quite low on modern processors due to random memory access to both input and output matrices. As well as the number and the pattern of non-zero elements in the output matrix, important for achieving locality, are unknown before the execution. Moreover, the state-of-the-art GPU implementations of SpGEMM requires large amounts of memory for temporary results, limiting the matrix size computable on fast GPU device memory. We propose a new fast SpGEMM algorithm requiring small amount of memory and achieving high performance. Calculation of the pattern and value in output matrix is optimized by using GPU's on-chip shared memory and a hash table. Additionally, our algorithm launches multiple kernels running concurrently to improve the utilization of GPU resources. The kernels for the calculation of each row of output matrix are chosen based on the number of non-zero elements. Performance evaluation using matrices from the Sparse Matrix Collection of University Florida on NVIDIA's Pascal generation GPU shows that our approach achieves speedups of up to x4.3 in single precision and x4.4 in double precision compared to existing SpGEMM libraries. Furthermore, the memory usage is reduced by 14.7\% in single precision and 10.9\% in double precision on average, allowing larger matrices to be computed.},
	booktitle = {2017 46th {International} {Conference} on {Parallel} {Processing} ({ICPP})},
	author = {Nagasaka, Y. and Nukada, A. and Matsuoka, S.},
	month = aug,
	year = {2017},
	note = {ISSN: 2332-5690},
	keywords = {Acceleration, GPU, Graphics processing units, Instruction sets, Kernel, Memory management, Parallel processing, SpGEMM, Sparse matrices, Sparse matrix},
	pages = {101--110},
}

@article{buluc_combinatorial_2011,
	title = {The combinatorial {BLAS}: {Design}, implementation, and applications},
	volume = {25},
	shorttitle = {The combinatorial {BLAS}},
	doi = {10.1177/1094342011403516},
	abstract = {This paper presents a scalable high-performance software library to be used for graph analysis and data mining. Large combinatorial graphs appear in many applications of high-performance computing, including computational biology, informatics, analytics, web search, dynamical systems, and sparse matrix methods. Graph computations are difficult to parallelize using traditional approaches due to their irregular nature and low operational intensity. Many graph computations, however, contain sufficient coarse-grained parallelism for thousands of processors, which can be uncovered by using the right primitives. We describe the parallel Combinatorial BLAS, which consists of a small but powerful set of linear algebra primitives specifically targeting graph and data mining applications. We provide an extensible library interface and some guiding principles for future development. The library is evaluated using two important graph algorithms, in terms of both performance and ease-of-use. The scalability and raw performance of the example applications, using the Combinatorial BLAS, are unprecedented on distributed memory clusters.},
	journal = {IJHPCA},
	author = {Buluç, Aydin and Gilbert, John},
	month = dec,
	year = {2011},
	pages = {496--509},
}

@article{challacombe_fast_2010,
	title = {Fast {Multiplication} of {Matrices} with {Decay}},
	url = {http://arxiv.org/abs/1011.3534},
	abstract = {A fast algorithm for the approximate multiplication of matrices with decay is introduced; the Sparse Approximate Matrix Multiply (SpAMM) reduces complexity in the product space, a different approach from current methods that economize within the matrix space through truncation or rank reduction. Matrix truncation (element dropping) is compared to SpAMM for quantum chemical matrices with approximate exponential and algebraic decay. For matched errors in the electronic total energy, SpAMM is found to require fewer to far fewer floating point operations relative to dropping. The challenges and opportunities afforded by this new approach are discussed, including the potential for high performance implementations.},
	urldate = {2021-03-30},
	journal = {arXiv:1011.3534 [cond-mat]},
	author = {Challacombe, Matt and Bock, Nicolas},
	month = nov,
	year = {2010},
	note = {arXiv: 1011.3534},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Mathematical Software, Condensed Matter - Materials Science, Mathematics - Numerical Analysis},
}

@inproceedings{lee_characterizing_2014,
	title = {Characterizing the latency hiding ability of {GPUs}},
	doi = {10.1109/ISPASS.2014.6844477},
	abstract = {This paper demonstrates a latency profiling approach to characterize and evaluate for the latency-hiding capability of modern GPU architectures. We find that the fast context-switching and massive multi-threading architecture can effectively hide much of the latency by swapping in and out warps. However, for certain GPGPU applications, such as bfs, the performance is limited by other factors. In future work, we plan to use the latency profiling approach to further investigate the limits of GPUs and seek for performance improvement opportunities.},
	booktitle = {2014 {IEEE} {International} {Symposium} on {Performance} {Analysis} of {Systems} and {Software} ({ISPASS})},
	author = {Lee, S. and Wu, C.},
	month = mar,
	year = {2014},
	keywords = {Computer architecture, Delays, Graphics processing units, Hazards, Instruction sets, Pipelines, Synchronization},
	pages = {145--146},
}

@article{dekker_floating-point_1971,
	title = {A floating-point technique for extending the available precision},
	volume = {18},
	issn = {0945-3245},
	url = {https://doi.org/10.1007/BF01397083},
	doi = {10.1007/BF01397083},
	abstract = {A technique is described for expressing multilength floating-point arithmetic in terms of singlelength floating point arithmetic, i.e. the arithmetic for an available (say: single or double precision) floating-point number system. The basic algorithms are exact addition and multiplication of two singlelength floating-point numbers, delivering the result as a doublelength floating-point number. A straight-forward application of the technique yields a set of algorithms for doublelength arithmetic which are given as ALGOL 60 procedures.},
	language = {en},
	number = {3},
	urldate = {2021-03-30},
	journal = {Numerische Mathematik},
	author = {Dekker, T. J.},
	month = jun,
	year = {1971},
	pages = {224--242},
}

@article{richard_shewchuk_adaptive_1997,
	title = {Adaptive {Precision} {Floating}-{Point} {Arithmetic} and {Fast} {Robust} {Geometric} {Predicates}},
	volume = {18},
	issn = {1432-0444},
	url = {https://doi.org/10.1007/PL00009321},
	doi = {10.1007/PL00009321},
	abstract = {Exact computer arithmetic has a variety of uses, including the robust implementation of geometric algorithms. This article has three purposes. The first is to offer fast software-level algorithms for exact addition and multiplication of arbitrary precision floating-point values. The second is to propose a technique for adaptive precision arithmetic that can often speed these algorithms when they are used to perform multiprecision calculations that do not always require exact arithmetic, but must satisfy some error bound. The third is to use these techniques to develop implementations of several common geometric calculations whose required degree of accuracy depends on their inputs. These robust geometric predicates are adaptive; their running time depends on the degree of uncertainty of the result, and is usually small.},
	language = {en},
	number = {3},
	urldate = {2021-03-30},
	journal = {Discrete \& Computational Geometry},
	author = {Richard Shewchuk, Jonathan},
	month = oct,
	year = {1997},
	pages = {305--363},
}

@article{nourazar_accelerating_2021,
	title = {Accelerating iterative {CT} reconstruction algorithms using {Tensor} {Cores}},
	issn = {1861-8219},
	url = {https://doi.org/10.1007/s11554-020-01069-5},
	doi = {10.1007/s11554-020-01069-5},
	abstract = {Tensor Cores are specialized hardware units added to recent NVIDIA GPUs to speed up matrix multiplication-related tasks, such as convolutions and densely connected layers in neural networks. Due to their specific hardware implementation and programming model, Tensor Cores cannot be straightforwardly applied to other applications outside machine learning. In this paper, we demonstrate the feasibility of using NVIDIA Tensor Cores for the acceleration of a non-machine learning application: iterative Computed Tomography (CT) reconstruction. For large CT images and real-time CT scanning, the reconstruction time for many existing iterative reconstruction methods is relatively high, ranging from seconds to minutes, depending on the size of the image. Therefore, CT reconstruction is an application area that could potentially benefit from Tensor Core hardware acceleration. We first studied the reconstruction algorithm’s performance as a function of the hardware related parameters and proposed an approach to accelerate reconstruction on Tensor Cores. The results show that the proposed method provides about 5 \$\${\textbackslash}times \$\$increase in speed and energy saving using the NVIDIA RTX 2080 Ti GPU for the parallel projection of 32 images of size \$\$512{\textbackslash}times 512\$\$. The relative reconstruction error due to the mixed-precision computations was almost equal to the error of single-precision (32-bit) floating-point computations. We then presented an approach for real-time and memory-limited applications by exploiting the symmetry of the system (i.e., the acquisition geometry). As the proposed approach is based on the conjugate gradient method, it can be generalized to extend its application to many research and industrial fields.},
	language = {en},
	urldate = {2021-03-30},
	journal = {Journal of Real-Time Image Processing},
	author = {Nourazar, Mohsen and Goossens, Bart},
	month = jan,
	year = {2021},
}

@article{bensayadia_efficient_2020,
	title = {Efficient graphical-processor-unit parallelization algorithm for computing {Eigen} values},
	volume = {29},
	issn = {1017-9909, 1560-229X},
	url = {https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging/volume-29/issue-6/063008/Efficient-graphical-processor-unit-parallelization-algorithm-for-computing-Eigen-values/10.1117/1.JEI.29.6.063008.short},
	doi = {10.1117/1.JEI.29.6.063008},
	abstract = {Several leading-edge applications such as pathology detection, biometric identification, and face recognition are based mainly on blob and line detection. To address this problem, Eigen value computing has been commonly employed due to its accuracy and robustness. However, Eigen value computing requires a raised computational processing, intensive memory data access, and data overlapping, which involve higher execution times. To overcome these limitations, we propose in this paper a new parallel strategy to implement Eigen value computing using a graphics processing unit (GPU). Our contributions are (1) to optimize instruction scheduling to reduce the computation time, (2) to efficiently partition processing into blocks to increase the occupancy of streaming multiprocessors, (3) to provide efficient input data splitting on shared memory to benefit from its lower access time, and (4) to propose new data management of shared memory to avoid access memory conflict and reduce memory bank accesses. Experimental results show that our proposed GPU parallel strategy for Eigen value computing achieves speedups of 27 compared with a multithreaded implementation, of 16 compared with a predefined function in the OpenCV library, and of eight compared with a predefined function in the Cublas library, all of which are performed into a quad core multi-central-processing unit platform. Next, our parallel strategy is evaluated through an Eigen value-based method for retinal thick vessel segmentation, which is essential for detecting ocular pathologies. Eigen value computing is executed in 0.017 s when using Structured Analysis of the Retina database images. Accordingly, we achieved real-time thick retinal vessel segmentation with an average execution time of about 0.039 s.},
	number = {6},
	urldate = {2021-03-30},
	journal = {Journal of Electronic Imaging},
	author = {Bensayadia, Sofien and Elloumi, Yaroub and Akil, Mohamed and Bedoui, Mohamed Hédi},
	month = dec,
	year = {2020},
	note = {Publisher: International Society for Optics and Photonics},
	pages = {063008},
}

@article{klower_number_2020,
	title = {Number {Formats}, {Error} {Mitigation}, and {Scope} for 16-{Bit} {Arithmetics} in {Weather} and {Climate} {Modeling} {Analyzed} {With} a {Shallow} {Water} {Model}},
	volume = {12},
	copyright = {©2020. The Authors.},
	issn = {1942-2466},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002246},
	doi = {https://doi.org/10.1029/2020MS002246},
	abstract = {The need for high-precision calculations with 64-bit or 32-bit floating-point arithmetic for weather and climate models is questioned. Lower-precision numbers can accelerate simulations and are increasingly supported by modern computing hardware. This paper investigates the potential of 16-bit arithmetic when applied within a shallow water model that serves as a medium complexity weather or climate application. There are several 16-bit number formats that can potentially be used (IEEE half precision, BFloat16, posits, integer, and fixed-point). It is evident that a simple change to 16-bit arithmetic will not be possible for complex weather and climate applications as it will degrade model results by intolerable rounding errors that cause a stalling of model dynamics or model instabilities. However, if the posit number format is used as an alternative to the standard floating-point numbers, the model degradation can be significantly reduced. Furthermore, mitigation methods, such as rescaling, reordering, and mixed precision, are available to make model simulations resilient against a precision reduction. If mitigation methods are applied, 16-bit floating-point arithmetic can be used successfully within the shallow water model. The results show the potential of 16-bit formats for at least parts of complex weather and climate models where rounding errors would be entirely masked by initial condition, model, or discretization error.},
	language = {en},
	number = {10},
	urldate = {2021-03-30},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Klöwer, M. and Düben, P. D. and Palmer, T. N.},
	year = {2020},
	note = {\_eprint: https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020MS002246},
	keywords = {16-bit arithmetic, Reduced precision, climate models, floating-point numbers, posit numbers, rounding error},
	pages = {e2020MS002246},
}

@article{liu_accelerating_2021,
	title = {Accelerating {Sparse} {Approximate} {Matrix} {Multiplication} on {GPUs}},
	url = {http://arxiv.org/abs/2103.13042},
	abstract = {Although the matrix multiplication plays a vital role in computational linear algebra, there are few efficient solutions for matrix multiplication of the near-sparse matrices. The Sparse Approximate Matrix Multiply (SpAMM) is one of the algorithms to fill the performance gap neglected by traditional optimizations for dense/sparse matrix multiplication. However, existing SpAMM algorithms fail to exploit the performance potential of GPUs for acceleration. In this paper, we present cuSpAMM, the first parallel SpAMM algorithm optimized for multiple GPUs. Several performance optimizations have been proposed, including algorithm re-design to adapt to the thread parallelism, blocking strategies for memory access optimization, and the acceleration with the tensor core. In addition, we scale cuSpAMM to run on multiple GPUs with an effective load balance scheme. We evaluate cuSpAMM on both synthesized and real-world datasets on multiple GPUs. The experiment results show that cuSpAMM achieves significant performance speedup compared to vendor optimized cuBLAS and cuSPARSE libraries.},
	urldate = {2021-03-30},
	journal = {arXiv:2103.13042 [cs]},
	author = {Liu, Xiaoyan and Liu, Yi and Dun, Ming and Yin, Bohong and Yang, Hailong and Luan, Zhongzhi and Qian, Depei},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.13042},
	keywords = {Computer Science - Performance},
}

@inproceedings{zhang_basic_2020,
	title = {Basic {Linear} {Algebra} {Operations} on {TensorCore} {GPU}},
	doi = {10.1109/ScalA51936.2020.00011},
	abstract = {Encouraged by the requirement of high speed matrix computations and training deep neural networks, TensorCore was introduced in NVIDIA GPU to further accelerate matrix-matrix multiplication. It supports very fast half precision general matrix matrix multiplications (GEMMs), which is around 8x faster than single precision CUDA core GEMMs. So far the use of TensorCore GPU for matrix operations other than matrix-matrix multiplications is under developed. In this paper, we propose some efficient BLAS3 operations that exploits TensorCore. The experimental results show that the proposed algorithms outperform cublas corresponding routines and the naive TensorCore implementation with up to 4.7× speedup.},
	booktitle = {2020 {IEEE}/{ACM} 11th {Workshop} on {Latest} {Advances} in {Scalable} {Algorithms} for {Large}-{Scale} {Systems} ({ScalA})},
	author = {Zhang, S. and Karihaloo, V. and Wu, P.},
	month = nov,
	year = {2020},
	keywords = {Acceleration, Graphics processing units, Matrix decomposition, Neural networks, Performance evaluation, Symmetric matrices, TensorCore, BLAS, GPU, Dense Linear Algebra, TRSM, SYRK, TRMM, LATER, Tensors},
	pages = {44--52},
}

@inproceedings{feng_egemm-tc_2021,
	address = {New York, NY, USA},
	series = {{PPoPP} '21},
	title = {{EGEMM}-{TC}: accelerating scientific computing on tensor cores with extended precision},
	isbn = {978-1-4503-8294-6},
	shorttitle = {{EGEMM}-{TC}},
	url = {https://doi.org/10.1145/3437801.3441599},
	doi = {10.1145/3437801.3441599},
	abstract = {Nvidia Tensor Cores achieve high performance with half-precision matrix inputs tailored towards deep learning workloads. However, this limits the application of Tensor Cores especially in the area of scientific computing with high precision requirements. In this paper, we build Emulated GEMM on Tensor Cores (EGEMM-TC) to extend the usage of Tensor Cores to accelerate scientific computing applications without compromising the precision requirements. First, EGEMM-TC employs an extendable workflow of hardware profiling and operation design to generate a lightweight emulation algorithm on Tensor Cores with extended-precision. Second, EGEMM-TC exploits a set of Tensor Core kernel optimizations to achieve high performance, including the highly-efficient tensorization to exploit the Tensor Core memory architecture and the instruction-level optimizations to coordinate the emulation computation and memory access. Third, EGEMM-TC incorporates a hardware-aware analytic model to offer large flexibility for automatic performance tuning across various scientific computing workloads and input datasets. Extensive evaluations show that EGEMM-TC can achieve on average 3.13× and 11.18× speedup over the cuBLAS kernels and the CUDA-SDK kernels on CUDA Cores, respectively. Our case study on several scientific computing applications further confirms that EGEMM-TC can generalize the usage of Tensor Cores and achieve about 1.8× speedup compared to the hand-tuned, highly-optimized implementations running on CUDA Cores.},
	urldate = {2021-03-29},
	booktitle = {Proceedings of the 26th {ACM} {SIGPLAN} {Symposium} on {Principles} and {Practice} of {Parallel} {Programming}},
	publisher = {Association for Computing Machinery},
	author = {Feng, Boyuan and Wang, Yuke and Chen, Guoyang and Zhang, Weifeng and Xie, Yuan and Ding, Yufei},
	month = feb,
	year = {2021},
	keywords = {GEMM, emulation, tensor core},
	pages = {278--291},
}

@inproceedings{firoz_feasibility_2020,
	title = {On the {Feasibility} of {Using} {Reduced}-{Precision} {Tensor} {Core} {Operations} for {Graph} {Analytics}},
	doi = {10.1109/HPEC43674.2020.9286152},
	abstract = {Today's data-driven analytics and machine learning workload have been largely driven by the General-Purpose Graphics Processing Units (GPGPUs). To accelerate dense matrix multiplications on the GPUs, Tensor Core Units (TCUs) have been introduced in recent years. In this paper, we study linear-algebra-based and vertex-centric algorithms for various graph kernels on the GPUs with an objective of applying this new hardware feature to graph applications. We identify the potential stages in these graph kernels that can be executed on the Tensor Core Units. In particular, we leverage the reformulation of the reduction and scan operations in terms of matrix multiplication [1] on the TCUs. We demonstrate that executing these operations on the TCUs, available inside different graph kernels, can assist in establishing an end-to-end pipeline on the GPGPUs without depending on hand-tuned external libraries and still can deliver comparable performance for various graph analytics.},
	booktitle = {2020 {IEEE} {High} {Performance} {Extreme} {Computing} {Conference} ({HPEC})},
	author = {Firoz, J. S. and Li, A. and Li, J. and Barker, K.},
	month = sep,
	year = {2020},
	note = {ISSN: 2643-1971},
	keywords = {Hardware, Kernel, Libraries, Machine learning, Machine learning algorithms, Pipelines, Tensors},
	pages = {1--7},
}

@article{domke_matrix_2020,
	title = {Matrix {Engines} for {High} {Performance} {Computing}:{A} {Paragon} of {Performance} or {Grasping} at {Straws}?},
	shorttitle = {Matrix {Engines} for {High} {Performance} {Computing}},
	url = {https://arxiv.org/abs/2010.14373v2},
	abstract = {Matrix engines or units, in different forms and affinities, are becoming a
reality in modern processors; CPUs and otherwise. The current and dominant
algorithmic approach to Deep Learning merits the commercial investments in
these units, and deduced from the No.1 benchmark in supercomputing, namely High
Performance Linpack, one would expect an awakened enthusiasm by the HPC
community, too.
  Hence, our goal is to identify the practical added benefits for HPC and
machine learning applications by having access to matrix engines. For this
purpose, we perform an in-depth survey of software stacks, proxy applications
and benchmarks, and historical batch job records. We provide a cost-benefit
analysis of matrix engines, both asymptotically and in conjunction with
state-of-the-art processors. While our empirical data will temper the
enthusiasm, we also outline opportunities to misuse these dense
matrix-multiplication engines if they come for free.},
	language = {en},
	urldate = {2021-03-30},
	author = {Domke, Jens and Vatai, Emil and Drozd, Aleksandr and Chen, Peng and Oyama, Yosuke and Zhang, Lingqi and Salaria, Shweta and Mukunoki, Daichi and Podobas, Artur and Wahib, Mohamed and Matsuoka, Satoshi},
	month = oct,
	year = {2020},
}

@inproceedings{carrasco_analyzing_2018,
	title = {Analyzing {GPU} {Tensor} {Core} {Potential} for {Fast} {Reductions}},
	doi = {10.1109/SCCC.2018.8705253},
	abstract = {The Nvidia GPU architecture has introduced new computing elements such as the tensor cores, which are special processing units dedicated to perform fast matrix-multiplyaccumulate (MMA) operations and accelerate Deep Learning applications. In this work we present the idea of using tensor cores for a different purpose such as the parallel arithmetic reduction problem, and propose a new GPU tensor-core based algorithm as well as analyze its potential performance benefits in comparison to a traditional GPU-based one. The proposed method, encodes the reduction of n numbers as a set of m × m MMA tensor-core operations (for Nvidia's Volta architecture m = 16) and takes advantage from the fact that each MMA operation takes just one GPU cycle. When analyzing the cost under a simplified GPU computing model, the result is that the new algorithm manages to reduce a problem of n numbers in T(n) = 5 log(m2) (n) steps with a speedup of S = 4/5 log2(m2).},
	booktitle = {2018 37th {International} {Conference} of the {Chilean} {Computer} {Science} {Society} ({SCCC})},
	author = {Carrasco, R. and Vega, R. and Navarro, C. A.},
	month = nov,
	year = {2018},
	note = {ISSN: 1522-4902},
	keywords = {Acceleration, Computational modeling, GPU Com puting, Graphics processing units, Kernel, NVIDIA Tensor Cores, Programming, Reduction, matrix-multiply-accumulate},
	pages = {1--6},
}

@article{ahmad_data-driven_2019,
	title = {Data-driven {Mixed} {Precision} {Sparse} {Matrix} {Vector} {Multiplication} for {GPUs}},
	volume = {16},
	issn = {1544-3566},
	url = {https://doi.org/10.1145/3371275},
	doi = {10.1145/3371275},
	abstract = {We optimize Sparse Matrix Vector multiplication (SpMV) using a mixed precision strategy (MpSpMV) for Nvidia V100 GPUs. The approach has three benefits: (1) It reduces computation time, (2) it reduces the size of the input matrix and therefore reduces data movement, and (3) it provides an opportunity for increased parallelism. MpSpMV’s decision to lower to single precision is data driven, based on individual nonzero values of the sparse matrix. On all real-valued matrices from the Sparse Matrix Collection, we obtain a maximum speedup of 2.61× and average speedup of 1.06× over double precision, while maintaining higher accuracy compared to single precision.},
	number = {4},
	urldate = {2021-03-30},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Ahmad, Khalid and Sundar, Hari and Hall, Mary},
	month = dec,
	year = {2019},
	keywords = {Mixed precision, SpMV, correctness, sparse matrices},
	pages = {51:1--51:24},
}

@inproceedings{mukunoki_dgemm_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{DGEMM} {Using} {Tensor} {Cores}, and {Its} {Accurate} and {Reproducible} {Versions}},
	isbn = {978-3-030-50743-5},
	doi = {10.1007/978-3-030-50743-5_12},
	abstract = {This paper proposes a method for implementing dense matrix multiplication on FP64 (DGEMM) and FP32 (SGEMM) using Tensor Cores on NVIDIA’s graphics processing units (GPUs). Tensor Cores are special processing units that perform 4×44×44{\textbackslash}times 4 matrix multiplications on FP16 inputs with FP32 precision, and return the result on FP32. The proposed method adopts the Ozaki scheme, an accurate matrix multiplication algorithm based on error-free transformation for matrix multiplication. The proposed method has three prominent advantages: first, it can be built upon the cublasGemmEx routine using Tensor Core operations; second, it can achieve higher accuracy than standard DGEMM, including the correctly-rounded result; third, it ensures bit-level reproducibility even for different numbers of cores and threads. The achievable performance of the method depends on the absolute-value range of each element of the input matrices. For example, when the matrices were initialized with random numbers over a dynamic range of 1E+9, our DGEMM-equivalent implementation achieved up to approximately 980 GFlops of FP64 operation on the Titan RTX GPU (with 130 TFlops on Tensor Cores), although cublasDgemm can achieve only 539 GFlops on FP64 floating-point units. Our results reveal the possibility of utilizing hardware with limited FP32/FP64 resources and fast low-precision processing units (such as AI-oriented processors) for general-purpose workloads.},
	language = {en},
	booktitle = {High {Performance} {Computing}},
	publisher = {Springer International Publishing},
	author = {Mukunoki, Daichi and Ozaki, Katsuhisa and Ogita, Takeshi and Imamura, Toshiyuki},
	editor = {Sadayappan, Ponnuswamy and Chamberlain, Bradford L. and Juckeland, Guido and Ltaief, Hatem},
	year = {2020},
	keywords = {Accuracy, FP16, GEMM, Half-precision, Linear algebra, Low-precision, Matrix multiplication, Reproducibility, Tensor cores},
	pages = {230--248},
}

@article{zachariadis_accelerating_2020,
	title = {Accelerating sparse matrix–matrix multiplication with {GPU} {Tensor} {Cores}},
	volume = {88},
	issn = {0045-7906},
	url = {https://www.sciencedirect.com/science/article/pii/S0045790620307011},
	doi = {10.1016/j.compeleceng.2020.106848},
	abstract = {Sparse general matrix–matrix multiplication (spGEMM) is an essential component in many scientific and data analytics applications. However, the sparsity pattern of the input matrices and the interaction of their patterns make spGEMM challenging. Modern GPUs include Tensor Core Units (TCUs), which specialize in dense matrix multiplication. Our aim is to re-purpose TCUs for sparse matrices. The key idea of our spGEMM algorithm, tSparse, is to multiply sparse rectangular blocks using the mixed precision mode of TCUs. tSparse partitions the input matrices into tiles and operates only on tiles which contain one or more elements. It creates a task list of the tiles, and performs matrix multiplication of these tiles using TCUs. To the best of our knowledge, this is the first time that TCUs are used in the context of spGEMM. We show that spGEMM, with our tiling approach, benefits from TCUs. Our approach significantly improves the performance of spGEMM in comparison to cuSPARSE, CUSP, RMerge2, Nsparse, AC-SpGEMM and spECK.},
	language = {en},
	urldate = {2021-03-30},
	journal = {Computers \& Electrical Engineering},
	author = {Zachariadis, Orestis and Satpute, Nitin and Gómez-Luna, Juan and Olivares, Joaquín},
	month = dec,
	year = {2020},
	keywords = {GPU, Parallel computing, SpGEMM, Sparse matrix multiplication, Tensor Cores},
	pages = {106848},
}

@inproceedings{zaruba_floating_2019,
	title = {The {Floating} {Point} {Trinity}: {A} {Multi}-modal {Approach} to {Extreme} {Energy}-{Efficiency} and {Performance}},
	shorttitle = {The {Floating} {Point} {Trinity}},
	doi = {10.1109/ICECS46596.2019.8964820},
	abstract = {The demand for floating-point compute power is ever growing. The domains of big-data, machine learning, and scientific computing require a wide precision range and high operational intensity. The sheer number of operations paired with increased power density implied by technology scaling makes it more important than ever to achieve maximum energy-efficiency for floating point operations. In this work, we present Kosmodrom, our novel silicon solution in Globalfoundries 22 nm Fully-Depleted Silicon on Insulator (FD-SOI) which offers a multi-dimensional approach to trade-off performance, energy-efficiency and power consumption. A variable-precision, dual-core RISC-V system together with a specialized floating point accelerator form the architectural basis. Different implementation strategies and standard cell flavors provide optimal solutions for different operating conditions while supply voltage and forward body bias (FBB) enable for a dynamic trade-off during operation. In this work, we provide a unique insight into the impact of a multitude of tuning parameters to achieve the optimal operating point on the power-performance surface. Kosmodrom achieves a peak energy-efficiency of 260Gflop/s/W and up to 28Gflop/s peak performance within a 6.2-400mW power envelope.},
	booktitle = {2019 26th {IEEE} {International} {Conference} on {Electronics}, {Circuits} and {Systems} ({ICECS})},
	author = {Zaruba, F. and Schuiki, F. and Mach, S. and Benini, L.},
	month = nov,
	year = {2019},
	pages = {767--770},
}

@inproceedings{chien_posit_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Posit {NPB}: {Assessing} the {Precision} {Improvement} in {HPC} {Scientific} {Applications}},
	isbn = {978-3-030-43229-4},
	shorttitle = {Posit {NPB}},
	doi = {10.1007/978-3-030-43229-4_26},
	abstract = {Floating-point operations can significantly impact the accuracy and performance of scientific applications on large-scale parallel systems. Recently, an emerging floating-point format called Posit has attracted attention as an alternative to the standard IEEE floating-point formats because it could enable higher precision than IEEE formats using the same number of bits. In this work, we first explored the feasibility of Posit encoding in representative HPC applications by providing a 32-bit Posit NAS Parallel Benchmark (NPB) suite. Then, we evaluate the accuracy improvement in different HPC kernels compared to the IEEE 754 format. Our results indicate that using Posit encoding achieves optimized precision, ranging from 0.6 to 1.4 decimal digit, for all tested kernels and proxy-applications. Also, we quantified the overhead of the current software implementation of Posit encoding as 4××{\textbackslash}times –19××{\textbackslash}times that of IEEE 754 hardware implementation. Our study highlights the potential of hardware implementations of Posit to benefit a broad range of HPC applications.},
	language = {en},
	booktitle = {Parallel {Processing} and {Applied} {Mathematics}},
	publisher = {Springer International Publishing},
	author = {Chien, Steven W. D. and Peng, Ivy B. and Markidis, Stefano},
	editor = {Wyrzykowski, Roman and Deelman, Ewa and Dongarra, Jack and Karczewski, Konrad},
	year = {2020},
	keywords = {Floating point precision, HPC, NPB, Posit},
	pages = {301--310},
}

@inproceedings{yan_demystifying_2020,
	title = {Demystifying {Tensor} {Cores} to {Optimize} {Half}-{Precision} {Matrix} {Multiply}},
	doi = {10.1109/IPDPS47924.2020.00071},
	abstract = {Half-precision matrix multiply has played a key role in the training of deep learning models. The newly designed Nvidia Tensor Cores offer the native instructions for half-precision small matrix multiply, based on which Half-precision General Matrix Multiply (HGEMM) routines are developed and can be accessed through high-level APIs. In this paper, we, for the first time, demystify how Tensor Cores on NVIDIA Turing architecture work in great details, including the instructions used, the registers and data layout required, as well as the throughput and latency of Tensor Core operations. We further benchmark the memory system of Turing GPUs and conduct quantitative analysis of the performance. Our analysis shows that the bandwidth of DRAM, L2 cache and shared memory is the new bottleneck for HGEMM, whose performance is previously believed to be bound by computation. Based on our newly discovered features of Tensor Cores, we apply a series of optimization techniques on the Tensor Core-based HGEMM, including blocking size optimization, data layout redesign, data prefetching, and instruction scheduling. Extensive evaluation results show that our optimized HGEMM routine achieves an average of 1.73× and 1.46× speedup over the native implementation of cuBLAS 10.1 on NVIDIA Turing RTX2070 and T4 GPUs, respectively. The code of our implementation is written in native hardware assembly (SASS).},
	booktitle = {2020 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Yan, D. and Wang, W. and Chu, X.},
	month = may,
	year = {2020},
	note = {ISSN: 1530-2075},
	keywords = {Benchmark testing, C++ languages, GEMM, GPU, Graphics processing units, Half-precision, Hidden Markov models, Layout, Registers, Tensile stress, Tensor Core},
	pages = {634--643},
}

@inproceedings{abts_think_2020,
	title = {Think {Fast}: {A} {Tensor} {Streaming} {Processor} ({TSP}) for {Accelerating} {Deep} {Learning} {Workloads}},
	shorttitle = {Think {Fast}},
	doi = {10.1109/ISCA45697.2020.00023},
	abstract = {In this paper, we introduce the Tensor Streaming Processor (TSP) architecture, a functionally-sliced microarchitecture with memory units interleaved with vector and matrix deep learning functional units in order to take advantage of dataflow locality of deep learning operations. The TSP is built based on two key observations: (1) machine learning workloads exhibit abundant data parallelism, which can be readily mapped to tensors in hardware, and (2) a simple and deterministic processor with producer-consumer stream programming model enables precise reasoning and control of hardware components, achieving good performance and power efficiency. The TSP is designed to exploit parallelism inherent in machine-learning workloads including instruction-level, memory concurrency, data and model parallelism, while guaranteeing determinism by eliminating all reactive elements in the hardware (e.g. arbiters, and caches). Early ResNet50 image classification results demonstrate 20.4K processed images per second (IPS) with a batch-size of one- a 4× improvement compared to other modern GPUs and accelerators [44]. Our first ASIC implementation of the TSP architecture yields a computational density of more than 1 TeraOp/s per square mm of silicon for its 25×29 mm 14nm chip operating at a nominal clock frequency of 900 MHz. The TSP demonstrates a novel hardware-software approach to achieve fast, yet predictable, performance on machine-learning workloads within a desired power envelope.},
	booktitle = {2020 {ACM}/{IEEE} 47th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	author = {Abts, D. and Ross, J. and Sparling, J. and Wong-VanHaren, M. and Baker, M. and Hawkins, T. and Bell, A. and Thompson, J. and Kahsai, T. and Kimmell, G. and Hwang, J. and Leslie-Hurd, R. and Bye, M. and Creswick, E. R. and Boyd, M. and Venigalla, M. and Laforge, E. and Purdy, J. and Kamath, P. and Maheshwari, D. and Beidler, M. and Rosseel, G. and Ahmad, O. and Gagarin, G. and Czekalski, R. and Rane, A. and Parmar, S. and Werner, J. and Sproch, J. and Macias, A. and Kurtz, B.},
	month = may,
	year = {2020},
	pages = {145--158},
}

@article{blanchard_mixed_2020,
	title = {Mixed {Precision} {Block} {Fused} {Multiply}-{Add}: {Error} {Analysis} and {Application} to {GPU} {Tensor} {Cores}},
	volume = {42},
	issn = {1064-8275},
	shorttitle = {Mixed {Precision} {Block} {Fused} {Multiply}-{Add}},
	url = {https://epubs.siam.org/doi/abs/10.1137/19M1289546},
	doi = {10.1137/19M1289546},
	abstract = {Computing units that carry out a fused multiply-add (FMA) operation with matrix arguments, referred to as tensor units by some vendors, have great potential for use in scientific computing. However, these units are inherently mixed precision, and existing rounding error analyses do not support them. We consider a mixed precision block FMA that generalizes both the usual scalar FMA and existing tensor units. We describe how to exploit such a block FMA in the numerical linear algebra kernels of matrix multiplication and LU factorization and give detailed rounding error analyses of both kernels. An important application is to GMRES-based iterative refinement with block FMAs, about which our analysis provides new insight. Our framework is applicable to the tensor core units in the NVIDIA Volta and Turing GPUs. For these we compare matrix multiplication and LU factorization with TC16 and TC32 forms of FMA, which differ in the precision used for the output of the tensor cores. Our experiments on an NVDIA V100 GPU confirm the predictions of the analysis that the TC32 variant is much more accurate than the TC16 one, and they show that the accuracy boost is obtained with almost no performance loss.},
	number = {3},
	urldate = {2021-03-30},
	journal = {SIAM Journal on Scientific Computing},
	author = {Blanchard, Pierre and Higham, Nicholas J. and Lopez, Florent and Mary, Theo and Pranesh, Srikara},
	month = jan,
	year = {2020},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {C124--C141},
}

@inproceedings{sorna_optimizing_2018,
	title = {Optimizing the {Fast} {Fourier} {Transform} {Using} {Mixed} {Precision} on {Tensor} {Core} {Hardware}},
	doi = {10.1109/HiPCW.2018.8634417},
	abstract = {The Fast Fourier Transform is a fundamental tool in scientific and technical computation. The highly parallelizable nature of the algorithm makes it a suitable candidate for GPU acceleration. This paper focuses on exploiting the speedup due to using the half precision multiplication capability of the latest GPUs' tensor core hardware without significantly degrading the precision of the Fourier Transform result. We develop an algorithm that dynamically splits the input single precision dataset into two half precision sets at the lowest level, uses half precision multiplication, and recombines the result at a later step. This work paves the way for using tensor cores for high precision inputs.},
	booktitle = {2018 {IEEE} 25th {International} {Conference} on {High} {Performance} {Computing} {Workshops} ({HiPCW})},
	author = {Sorna, A. and Cheng, X. and D'Azevedo, E. and Won, K. and Tomov, S.},
	month = dec,
	year = {2018},
	keywords = {Discrete Fourier transforms, Fast Fourier Transform, GPU Tensorcores, Graphics processing units, Hardware, Heuristic algorithms, Libraries, Matrix converters, Mixed Precision, eUDA},
	pages = {3--7},
}

@article{jorda_performance_2019,
	title = {Performance {Evaluation} of {cuDNN} {Convolution} {Algorithms} on {NVIDIA} {Volta} {GPUs}},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2918851},
	abstract = {Convolutional neural networks (CNNs) have recently attracted considerable attention due to their outstanding accuracy in applications, such as image recognition and natural language processing. While one advantage of the CNNs over other types of neural networks is their reduced computational cost, faster execution is still desired for both training and inference. Since convolution operations pose most of the execution time, multiple algorithms were and are being developed with the aim of accelerating this type of operations. However, due to the wide range of convolution parameter configurations used in the CNNs and the possible data type representations, it is not straightforward to assess in advance which of the available algorithms will be the best performing in each particular case. In this paper, we present a performance evaluation of the convolution algorithms provided by the cuDNN, the library used by most deep learning frameworks for their GPU operations. In our analysis, we leverage the convolution parameter configurations from widely used the CNNs and discuss which algorithms are better suited depending on the convolution parameters for both 32 and 16-bit floating-point (FP) data representations. Our results show that the filter size and the number of inputs are the most significant parameters when selecting a GPU convolution algorithm for 32-bit FP data. For 16-bit FP, leveraging specialized arithmetic units (NVIDIA Tensor Cores) is key to obtain the best performance.},
	journal = {IEEE Access},
	author = {Jordà, M. and Valero-Lara, P. and Peña, A. J.},
	year = {2019},
	note = {Conference Name: IEEE Access},
	keywords = {Convolution, GPU, Graphics processing units, Neural network, Neural networks, Performance evaluation, Three-dimensional displays, Training, Two dimensional displays, convolution, cuDNN, deep learning, volta},
	pages = {70461--70473},
}

@inproceedings{dakkak_accelerating_2019,
	address = {New York, NY, USA},
	series = {{ICS} '19},
	title = {Accelerating reduction and scan using tensor core units},
	isbn = {978-1-4503-6079-1},
	url = {https://doi.org/10.1145/3330345.3331057},
	doi = {10.1145/3330345.3331057},
	abstract = {Driven by deep learning, there has been a surge of specialized processors for matrix multiplication, referred to as Tensor Core Units (TCUs). These TCUs are capable of performing matrix multiplications on small matrices (usually 4 × 4 or 16 × 16) to accelerate HPC and deep learning workloads. Although TCUs are prevalent and promise increase in performance and/or energy efficiency, they suffer from over specialization as only matrix multiplication on small matrices is supported. In this paper we express both reduction and scan in terms of matrix multiplication operations and map them onto TCUs. To our knowledge, this paper is the first to try to broaden the class of algorithms expressible as TCU operations and is the first to show benefits of this mapping in terms of: program simplicity, efficiency, and performance. We implemented the reduction and scan algorithms using NVIDIA's V100 TCUs and achieved 89\% -- 98\% of peak memory copy bandwidth. Our results are orders of magnitude faster (up to 100 × for reduction and 3 × for scan) than state-of-the-art methods for small segment sizes (common in HPC and deep learning applications). Our implementation achieves this speedup while decreasing the power consumption by up to 22\% for reduction and 16\% for scan.},
	urldate = {2021-03-29},
	booktitle = {Proceedings of the {ACM} {International} {Conference} on {Supercomputing}},
	publisher = {Association for Computing Machinery},
	author = {Dakkak, Abdul and Li, Cheng and Xiong, Jinjun and Gelado, Isaac and Hwu, Wen-mei},
	month = jun,
	year = {2019},
	pages = {46--57},
}
