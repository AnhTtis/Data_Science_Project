\documentclass{article}

% \usepackage{neurips_2022}
% \usepackage[final]{neurips_2022}

% [preprint] option:

\usepackage[preprint]{neurips_2022}
\usepackage{amsmath}
\usepackage[svgnames]{xcolor}
\usepackage{framed}
\usepackage{soul}
\usepackage{tipa}
\definecolor{shadecolor}{named}{LightGray}
\definecolor{lightredshade}{HTML}{dea9a9}
\definecolor{lightgreenshade}{HTML}{bce3bd}
\definecolor{lightblueshade}{HTML}{cacbe8}
\usepackage{graphicx}
\graphicspath{ {./media/} }
\DeclareMathOperator*{\argmin}{argmin}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{natbib}

\title{Reflexion: an autonomous agent with dynamic memory and self-reflection}


\author{
  Noah Shinn \\
  Northeastern University\\
  Boston, MA \\
  \texttt{shinn.n@northeastern.edu} \\
  \And
  Beck Labash \\
  Northeastern University \\
  Boston, MA \\
  \texttt{labash.b@northeastern.edu} \\
  \AND
  Ashwin Gopinath \\
  Massachusetts Institute of Technology \\
  Cambridge, MA \\
  \texttt{agopi@mit.edu} \\
}


\begin{document}


\maketitle


\begin{abstract}
  Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective heuristic that enables the agent to pinpoint hallucination instances, avoid repetition in action sequences, and, in some environments, construct an internal memory map of the given environment. To assess our approach, we evaluate the agent's ability to complete decision-making tasks in AlfWorld environments and knowledge-intensive, search-based question-and-answer tasks in HotPotQA environments. We observe success rates of 97\% and 51\%, respectively, and provide a discussion on the emergent property of self-reflection.
\end{abstract}

\section{Introduction}
Mastering decision-making and knowledge-intensive search tasks in novel environments is a crucial skill set for large-scale natural language agents. LLMs such as OpenAI's GPT-3 \citep{brown2020language}, Google's PaLM \citep{chowdhery2022palm}, and others have achieved impressive results on various benchmarks \citep{kaplan2020scaling, rae2021scaling, nakano2021webgpt, kojima2022large, ouyang2022training, chung2022scaling}. These models exhibit human-like abilities to understand tasks in given environments, marking significant progress in the field of natural language processing. Grounding complex tasks in natural language allows agents to overcome high syntactic barriers that may result in false-negative errors. However, learning optimal policies for natural language RL agents is challenging due to vast and mostly unbound state spaces. 

Several decision-making approaches have been proposed to enable natural language agents to select their next action without a learned policy in text-based environments. Chain-of-thought (CoT) reasoning leverages emergent properties such as reasoning and commonsense to solve tasks in a single action but reasoned through several steps \citep{huang2022language, wei2022chain}. However, the accuracy of these approaches decrease as the number of required subtasks increase as the model is more prone to hallucinate over longer sequences. ReAct \citep{yao2023react} is an approach that utilizes emergent properties in LLMs, such as verbal reasoning traces, to solve problems by allowing the agent to reason and act, proving substantial performance in various text-based benchmarks. In addition, several recent works have aimed to allow natural language agents to exhibit reflective-like qualities to infer more intuitive future actions. The Describe, Explain, Plan, and Select (DEPS) approach uses multi-step reasoning and sub-task error correction to solve long-range tasks \citep{wang2023describe}. DEPS demonstrates impressive performance due to its ability to explain mistakes in sub-tasks \textit{within} trials, but relies on immediate failure detection for subtasks and cannot explain mistakes that may have developed over a long range of actions and subtasks. \cite{huang2022inner} use \textit{inner monologue} to further process next decisions within closed-loop feedback environments. \cite{huang2022inner} use a success detection approach in which the agent will explicitly know if an executed action has led to a successful state. \citep{huang2022language, haluptzok2022language} use self-generated solutions to fine-tune an LLM to improve performance without access to a labeled dataset. Although these approaches have achieved remarkable accuracy across various decision-making tasks or knowledge-intensive tasks, they lack the ability to utilize success detection cues to improve their behavior over long trajectories. In addition, they often succumb to common mistakes, such as repetitive action choice, cyclic hallucination, or random action choice. In other words, while these methods achieve state-of-the-art results, a small subset of tasks remain unsolved due to the agent's inability to learn from its own mistakes over long trajectories to correct future action sequence planning and execution.

To address common failure points, human-in-the-loop (HITL) approaches have been commonly used to improve performance \cite{fan2022nano, wu2022survey} \cite{yao2023react} briefly explore a human-in-the-loop (HITL) approach to redirect the agent's reasoning trace after erroneous actions. While this approach achieves improved performance with minimal human intervention, it is not fully autonomous by its reliance on human trainers to monitor trajectories at each time step. Large-scale LLMs have shown to exhibit advanced human-like qualities that enable natural language agents to solve tasks in more intuitive ways \citep{wei2022emergent}. We hypothesize that LLMs possess an emergent property of self-reflection and could effectively utilize self-optimization grounded in natural language if given the opportunity to autonomously close the trial loop. 

To test our hypothesis, we equip an LLM-based agent with a self-reflective LLM and a simple heuristic for detecting hallucination and inefficient action execution in an approach named Reflexion. We then challenge the agent to learn from its own mistakes on the AlfWorld text-based benchmark \citep{ALFWorld20} and the HotPotQA question-answering benchmark \citep{yang2018hotpotqa}. This results in improved performance in decision-making and knowledge-intensive tasks. When combined with the ReAct problem-solving technique \citep{yao2023react}, self-reflection guides the Reflexion agent to achieve a 97\% success discovery rate on the AlfWorld benchmark in just 12 autonomous trials, outperforming the base ReAct agent with an accuracy of 75\%. We also evaluated a Reflexion-based ReAct agent on 100 questions from HotPotQA. The agent achieved a 51\% success discovery rate by iteratively refining its content search and content extraction by receiving advice from its memory, outperforming a base ReAct agent by 17\%. It is essential to emphasize that Reflexion is not designed to achieve near-perfect accuracy scores; instead, its goal is to demonstrate learning through trial and error to enable discovery in tasks and environments previously considered nearly impossible to solve.
 
\section{Architecture}

\begin{figure}
    \centering
    \includegraphics[scale=0.25]{media/architecture.png}
    \caption{Reflexion can be added to any decision-making approach. We enable ReAct agents to use self-reflection to improve their own performance.}
\end{figure}
The abstract architecture of our Reflexion agent is depicted in Figure 1. In this study, Reflexion leverages ReAct \citep{yao2023react}, but any decision-making approach can be used in future implementations. In the first trial, the agent is given a task from the environment which composes the initial query. Then, the agent executes a series of actions generated by an LLM and receives observations and rewards from the environment. For environments that provide descriptive or continuous rewards, we constrain the output to a simple binary success status to ensure applicability; reward constraining is explained in further detail later. After every action $a_t$, the agent computes a heuristic $h$, which may suggest self-reflection. If self-reflection is recommended, the agent queries an LLM to reflect on its current task, trajectory history, and last reward (which is simply the fact that the agent had failed in the given environment under the binary reward constraint). Then, the agent resets the environment retry in a subsequent trial. If no self-reflection is advised, the agent adds the $a_t$ and $o_t$ to its trajectory history and queries the LLM for the next action. In practice, we set a hyperparameter limit of three maximum reflections to be stored in the agent's memory to avoiding queries beyond the limit of the LLM. If the agent exceeds the maximum number of trials, fails to improve performance between two consecutive trials, or completes the task, the run is terminated.

\subsection{Setup}
In a typical RL scenario, an agent is tasked to solve a problem by executing actions to learn from observations within an environment. At time step $t$ in state $s_t$, the agent receives an observation $o_t$ from the environment and executes an action $a_t$ based on its current policy $\pi (a_t \vert c_t)$. In text-based environments, $s_t$ considers $c_t$, which is the context given to the agent based on its current state and trajectory history, consisting of past observations and actions. In this study, we do not learn a policy over a state space. In addition, we aim to constrain specific reward information given to the agent to (1) prove the effectiveness of Reflexion in alignment with other works and (2) preserve the ability to generalize the approach to a broad range of problems. Additionally, we equip the agent with a heuristic function to detect common modes of failure.

\subsection{Heuristics}
A heuristic $\operatorname{h}(s_t, a_t, \Omega, \varepsilon, \left[a_o, o_0, \hdots, a_{t-1}, o_{t-1} \right])$ is defined to tell the agent when to reflect, where $t$ is the time step, $s_t$ is the current state, $\Omega$ and $\varepsilon$ are hyperparameters for the maximum number of repetitive action cycles and the maximum number of total actions allowed, and $\left[a_o, o_0 \hdots, a_{t-1}, o_{t-1} \right])$ is the trajectory history.
\[ 
 \operatorname{h}(s_t, a_t, \Omega, \varepsilon, \left[a_0, o_0 \hdots, a_{t-1}, o_{t-1} \right]) =\begin{cases} 
  1 & repeat(\left[a_o, o_0 \hdots, a_{t-1}, o_{t-1} \right]) > \Omega \\
  1 & t > \varepsilon  \\
  0 & o' wise
 \end{cases}
\]
$repeat$ is a simple function that determines the number of repetitive action cycles that yield the same observation(s). $\Omega$ is the maximum number of identical cycles, which is set to detect hallucination of repeated consecutive actions. $\varepsilon$ enforces efficient planning by restricting the maximum number of actions allowed in an environment per trial. $h$ is designed to replace the role of a human-in-the-loop in which a human trainer monitors the running trajectory to detect signs of hallucination or inefficient planning.

\subsection{Reflexion}
If the heuristic $h$ suggests reflection at $t$, the agent initiates a self-reflective process on its current state $s_t$, last reward $r_t$, previous actions and observations $\left[a_0, o_0, \hdots, a_t, o_t \right]$, and the agent's existing working memory, $mem$. The reflection loop aims to help the agent correct common cases of hallucination and inefficiency through trial and error. The model used for self-reflection is an LLM prompted with two-shot learning examples of domain-specific failed trajectory and ideal reflection pairs. Few-shot examples for AlfWorld and HotPotQA reflections can be found in \ref{appendix:sec.0}. To prevent the agent from memorizing correct AlfWorld trajectories or HotPotQA answers, we do not grant access to domain-specific solutions for the given problems. This approach encourages the agent to devise creative and novel techniques for future attempts.

Self-reflection is modeled in the following equation:
\[reflection = LLM(s_t, r_t, \left[a_0, o_0, \hdots, a_t, o_t \right], mem)\]
Finally, we add the reflection to the agent's memory, reset the environment, and start the next trial.

\subsection{Reward model}
Typically, designing or training an effective yet broadly-applicable reward model can be challenging. In this work, we limit the agent to a binary reward model. A binary reward model is a type of reward function that assigns a value of 0 or 1 to an action taken by the agent in the current state. 1 indicates a successful outcome and 0 indicates an unsuccessful outcome. A binary reward function was chosen to tightly constrain the agent's knowledge to the observations from the world and the status of success or failure in a given environment, as opposed to a multi-value or continuous output from a more descriptive reward model with which the agent may use to evaluate its current performance. By limiting the agent's knowledge to a binary success status within an environment, we force the agent to make meaningful inferences about its performance without external input in order to effectively improve its future decisions. Lastly, binary reward models are largely applicable to a variety of language problems, such as code generation and code debugging. We propose that Reflexion agents may be able to learn faster than random action sampling in arbitrary environments in which accurate reward functions may be hard to design or compute. In AlfWorld environments, we query the AlfWorld engine at every time step to check if the current state is a successful state. In the HotPotQA benchmark, we grade the response from the agent using exact match (EM) after an answer is given.

\subsection{Action space}
Defining a natural language action space for large language models is not straightforward, as the number of possible actions is vast. However, constraining the action space to a discrete set of actions may lead to unintended action choices. There is a trade-off between minimizing false positives in which the agent blindly chooses the most similar action, which could be largely semantically different from its initial intent, and minimizing false negatives in which the agent chooses an action that is semantically similar to the closest action but uses a different syntactic representation. We follow the work of \citep{yao2023react} in which an action space $\hat{\mathcal{A}} = \mathcal{A} \cup \mathcal{L}$ is chosen, where $\mathcal{L}$ is the space of language. We handle false negatives by responding with \textit{"Invalid Action"} observations. In the AlfWorld and HotPotQA benchmarks, we demonstrate permissible action syntax by few-shot prompting the agent with two successful trajectories.

\section{Evaluation}

\subsection{AlfWorld}

AlfWorld is a suite of text-based environments that challenge an agent to solve multi-step tasks in a variety of interactive environments based on TextWorld \citep{cote2019textworld}. In every environment, the agent can choose to execute an action $a_t$ from a list of admissible actions $\mathcal{A_{s, t}}$ and receive an observation $o_t$ and reward $\mathcal{R}(s_t, a_t)$ from the environment which determines its next state. As stated previously, we filter the reward to a binary reward by discarding the default reward output and simply query the AlfWorld engine to determine if the task has been successfully completed.

AlfWorld provides six different tasks and over 3000 environments, which challenge the agent to understand the goal task, develop a sequential plan of subtasks, and execute the actions in the given environment. Within trials, we implement the ReAct problem solving strategy as Yao et al., 2022 has shown great performance across a variety of sequential decision-making tasks. ReAct is an approach that allows the agent to reason and act by explicitly stating current thoughts and executing actions based on those thoughts. At every time step, the agent can execute a \textit{think: \textless{}thought\textgreater} action to verbalize its internal thought trace, or \textit{\textless{}action\textgreater} to elicit a response from the environment. The action space in each state is not explicitly defined, which allows the agent to have full expression in its next moves. To avoid syntactic errors, we provide two domain-specific few-shot trajectories to the agent.

\noindent \textbf{Setup}\hspace{0.5cm}Following \citep{yao2023react}, we ran the agent in 134 AlfWorld environments across six different tasks, including finding hidden objects (e.g., finding a spatula in a drawer), moving objects (e.g., moving a knife to the cutting board), and manipulating objects with other objects (e.g., chilling a tomato in the fridge). To closely replicate baseline results, we used the same hand-selected few-shot trajectory examples with GPT-3.0 for the LLM. At every time step, we evaluated $h$ to determine if the agent should self-reflect. In the baseline run, if reflection was suggested, we skipped the reflection process, reset the environment, and started a new trial. In the Reflexion run, we allowed the agent to use self-reflection to find its mistake, reset the environment, and start a new trial. Intuitively, we wanted to distinguish learning curves from self-improvement to simple probabilistic success over time. AlfWorld tasks, ReAct few-shot prompts, and Reflexion examples are included in \ref{appendix:sec.0}.

\noindent \textbf{Results}\hspace{0.5cm}We ran the agent without reflection for a single trial to establish a starting point, which achieved 63\% accuracy (Fig. 2), which is in-line with the results from \citep{yao2023react} Then, we performed the runs with and without Reflexion starting from the initial trial until the agent's ability to improve was saturated, which we defined as having a 0\% improvement between two consecutive trials.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.75]{media/alfworld_training_curves.png}
    \caption{AlfWorld performance across 134 tasks showing cumulative proportions of solved tasks and error trajectories.}
\end{figure}

Given the ability to reflect, the agent used ReAct to solve 97\% of the given environments in 12 trials, failing to solve only 4 out of 134 tasks. It is important to note that the comparison between Reflexion + ReAct and ReAct is not one of accuracy for state-of-the-art performance, but rather a demonstration of improved accuracy by self-improved learning rather than success by retry.

\subsection{HotPotQA}
HotPotQA is a Wikipedia-based dataset with 113k question-and-answer pairs that challenge agents to parse content and reason over several supporting documents. Equipped with a Wikipedia search engine, the agent must perform relevant searches to extract crucial information to answer questions that may require information presented across multiple documents. Then, the agent must provide EM answers given the context. In this study, we use EM to quickly automate the grading process between trials. We experimented with methods to relax syntactical requirements by performing semantic matching between inferred answers and correct answers, but observed several cases of false positives and negatives that required further grading by a human, which rendered the retry process inefficient and subjective.

\noindent \textbf{Setup}\hspace{0.5cm}To replicate results from the ReAct study \citep{yao2023react}, we prompted the agent with six few-shot examples of successful trajectories. We also equipped the agent with a Wikipedia API to perform one of three actions at each time step: \textbf{Search}[\textit{entity}], which returns the first five sentences of the wiki page or a list of the five most similar wiki pages according to the entity; \textbf{Lookup}[\textit{string}], which returns the sentence following the query within a wiki page; and \textbf{Finish}[\textit{answer}], which attempts to complete the task with the provided answer. Then, as stated before, EM was used as the binary reward model. The baseline and reflexion runs followed the same constraints as the AlfWorld benchmark. In the self-reflective process, the agent was given two few-shot example trajectories with reflections. Reflexion few-shot prompts are shown in \ref{appendix:sec.0}. However, in this experiment, the heuristic was not necessarily needed as we observed that the resulting trajectory lengths were highly dependent on the trajectory lengths provided in the few-shot examples.

In the baseline run, if the agent provided an incorrect answer, the environment was reset and the agent simply retried. In the reflexion run, if the agent provided an incorrect answer, the agent was given a chance to self-reflect on its past searches, lookups, observations, working memory, and final answer, reset the environment, and try again with a new working memory.

\noindent \textbf{Results}\hspace{0.5cm}We tested the base agent and Reflexion agent on a 100-question dataset. Our initial plan was to run the agents until they failed to improve their accuracy between consecutive trials. However, the base agent did not improve in accuracy between trials 1 and 2. Therefore, we ran the Reflexion agent until no improvement between two trials was observed, which occurred between trials 6 and 7 (Fig. 3), then ran the base agent for the equivalent number of trials, allowing retries on previously incorrect answers. While the base ReAct agent achieved 34\% accuracy and the Reflexion agent achieved 32\% accuracy in the first trials (Fig. 3), the Reflexion agent was able to outperform the base ReAct agent over the course of 7 trials.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.75]{media/hotpotqa_training_curves.png}
    \caption{HotPotQA performance across 100 question and answer pairs showing cumulative proportions of correct EM answers.}
\end{figure}

Reflexion enabled the agent to successfully answer 54\% of the questions from the dataset, outperforming the base ReAct agent by 20\% (Fig. 3) and its first trial attempt by 22\%. In fact, by simply retrying, the base ReAct agent was not able to successfully answer any additional questions. On the other hand, the Reflexion agent was able to improve on its performance not only between trials 1 and 2 but between every consecutive trial, demonstrating that the growing, dynamic working memory promoted learning through trial.

\section{Discussion}

\subsection{Reflexion promotes learning through discovery}

Fig. 4 shows two consecutive trials performed in the same environment. In the first trial (Fig. 4), the agent demonstrates unfamiliarity with the given environment as it searches for relevant objects in every location. Eventually, the agent finds the desk lamp but its decision-making process is faulty as it becomes distracted by the presence of the mug rather than focusing on the desk lamp. When the agent reflects on its past behavior (Fig. 4), it recognizes a flaw in its initial sequential plan. Additionally, the agent chooses to remember that the desk lamp was found on desk 1. In the next trial (Fig. 4), the agent is more efficient as it recalls that the desk lamp was on desk 1. Then, it devises a more accurate plan to find the mug after obtaining the desk lamp. The agent succeeds after 2 trials, or 1 reflection. In the ReAct-only approach, the agent solves the task on the 6th trial.

\subsection{Hallucination vs. Inefficient Planning}

Next, we explored the reasons for failure in AlfWorld runs with and without Reflexion. Hallucination is the most common reason for failure (Fig. 2). In the AlfWorld benchmark, we defined hallucination as the occurrence of two or more consecutive identical actions in which the environment responded with the same observation and inefficient planning as the occurrence of a trajectory in which the agent executed more than 30 actions without reaching a successful state. We defined inefficient planning as a mode of failure to encourage the agent to solve tasks by demonstrating strong, concise decision-making ability—not merely attempts to execute every admissible action. An example of hallucination is shown in Fig. 4 in which the agent hallucinates due to its belief that it has found the desklamp on desk 2. Therefore, it executes the same action twice, believing that the next best action is to use the desklamp.

\begin{figure}[htbp]
    \begin{minipage}[c]{\linewidth}
        \sethlcolor{lightblueshade}
        \hl{\textbf{Environment: }You are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a laundryhamper 1, a safe 1, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, and a shelf 1.\\
        \textbf{Task: }examine the mug with the desklamp.}
    \end{minipage}
    \begin{minipage}[c]{\linewidth}
        \textbf{Trial \#1}\\
        $>$ think: To solve the task, I need to find and take a mug, then find and use a desklamp.\\
        OK.\\
        $>$ think: First I need to find a mug. A mug is more likely to appear in drawer (1-6), desk (1-2), shelf (1-6), garbagecan (1), laundryhamper (1). I can check one by one, starting with drawer 1.\\
        OK.\\
        $>$ go to drawer 1\\
        The drawer 1 is closed.\\
        \dots\\
        $>$ open drawer 6\\
        You open the drawer 6. The drawer 6 is open. In it, you see a keychain 2.\\
        \sethlcolor{lightgreenshade}
        \hl{$>$ go to desk 1\\
        On the desk 1, you see a creditcard 3, a desklamp 1, a laptop 2, a mug 1, a pen 1, and a pencil 1.\\
        $>$ think: Now I find a mug (1). Next, I need to take it.\\
        OK.\\
        $>$ take mug 1 from desk 1\\
        You pick up the mug 1 from the desk 1.}\\
        $>$ think: Now I take a mug (1). Next, I need to find a desklamp. A desklamp is more likely to appear in desk (1-2), sidetable (1-2), shelf (1-6), bed (1), drawer (1-6). I can check one by one, starting with desk 1.\\
        OK.\\
        $>$ go to desk 1\\
        Nothing happens.\\
        $>$ go to desk 2\\
        On the desk 2, you see a alarmclock 1, a bowl 1, a mug 3, a pencil 3, and a pencil 2.\\
        $>$ think: Now I find a desklamp (1). Next, I need to use it.\\
        OK.\\
        \sethlcolor{lightredshade}
        \hl{$>$ use desklamp 1\\
        Nothing happens.\\
        $>$ use desklamp 1\\
        Nothing happens.\\
        Status: Fail}
    \end{minipage}
    \begin{minipage}[c]{\linewidth}
        \begin{shaded}
            \textbf{Reflection:} In this environment, my plan was to find a mug then find and use a desklamp. However, the task says to examine the mug with the desklamp. I should have looked for the desklamp first, then looked for the mug. I noticed that the desklamp was found on desk 1. In the next trial, I will go to desk 1, find the lamp, then look for the mug and examine it with the desklamp.
        \end{shaded}
    \end{minipage}
    \begin{minipage}[c]{\linewidth}
        \textbf{Trial \#2}\\
        \sethlcolor{lightgreenshade}
        \hl{$>$ go to desk 1\\
        On the desk 1, you see a creditcard 3, a desklamp 1, a laptop 2, a mug 1, a pen 1, and a pencil 1.\\
        $>$ think: To solve the task, I need to find and take a mug, then find and use a desklamp.\\
        OK.\\
        $>$ take mug 1 from desk 1\\
        You pick up the mug 1 from the desk 1.}\\
        $>$ think: To solve the task, I need to find and take a mug, then find and use a desklamp.\\
        OK.\\
        \sethlcolor{lightgreenshade}
        \hl{$>$ use desklamp 1\\
        You turn on the desklamp 1.\\
        Status: Success}
    \end{minipage}
    \caption{[Top] An AlfWorld trajectory in which the agent failed due to inefficient planning. In the reflection, the agent recognizes that it should have looked for the desklamp then the mug, not the mug then the desklamp. [Bottom] The agent is able to correct its reasoning trace and execute a sequence of actions in a concise manner.}
\end{figure}

Inefficient planning errors occur in trajectories in which the agent reaches action 30 without showing any signs of improvement. While the agent may be constructing a mental map of the environment as it discovers and observes new items and locations, we penalize this behavior as it does not demonstrate strong decision-making ability. Fig. 2 shows that although the agent can solve additional tasks through trial, it still converges to the same rough 3:1 hallucination to inefficient planning ratio as in Trial 1. However, with reflection Fig. 2, the agent can correct all of its mistakes related to inefficient planning in all but four mistakes related to hallucination.

\subsection{Reflexion Enables More Intuitive Search Queries}
Given a knowledge-intensive task and a search engine, a key component to an agent's success is its ability to form intuitive search queries. Fig. 5 shows an improvement between trials in which the agent learns from its past mistakes.

\begin{figure}[htbp]
    \begin{minipage}[t]{0.48 \linewidth}
        \sethlcolor{lightblueshade}
        \hl{\textbf{Trial \#1:} Question: Grown-Ups starred the actor who was best known for which role on  "'Allo 'Allo!"?}\\
        Thought 1: I need to search Grown-Ups and "'Allo 'Allo!", find the actor who starred in Grown-Ups, then find the role he was best known for in "'Allo 'Allo!".\\
        Action 1: Search[Grown-Ups]\\
        Observation 1: Grown-Ups is a 1980 British BBC television film devised and directed by Mike Leigh. It stars Lesley Manville, Philip Davis,  Brenda Blethyn, Janine Duvitski, Lindsay Duncan and Sam Kelly. It was  edited by Robin Sales and produced by Louis Marks for the BBC, and originally shown on BBC 2 on 28 November 1980.\\
        Thought 2: Grown-Ups starred Lesley Manville, Philip Davis, Brenda Blethyn, Janine Duvitski, Lindsay Duncan and Sam Kelly. I need to search "'Allo 'Allo!" and find which actor was best known for which role.\\
        \sethlcolor{lightredshade}
        \hl{Action 2: Search["'Allo 'Allo!"]\\}
        Observation 2: Could not find ["'Allo 'Allo!"]. Similar: ["'Allo 'Allo!", "List of 'Allo 'Allo! characters", 'Gorden Kaye', "'Allo 'Allo! (series 2)", 'Allô, allô', 'Francesca Gonshaw', "'Allo 'Allo! (series 4)", 'Kim Hartman', "'Allo 'Allo! (series 6)", 'Jeremy Lloyd']\\
        Thought 3: To find the actor who was best known for which role on "'Allo 'Allo!", I can search Gorden Kaye.\\
        Action 3: Search[Gorden Kaye]\\
        Observation 3: Gordon Irving Kaye (7 April 1941 – 23 January 2017), known professionally as Gorden Kaye, was an English actor, best known for playing womanising café owner René Artois in the television comedy series 'Allo 'Allo!.\\
        Thought 4: Gorden Kaye was best known for playing womanising café owner René Artois in the television comedy series "'Allo 'Allo!". So the answer is René Artois.\\
        \sethlcolor{lightredshade}
        \hl{Action 4: Finish$\text{[René Artois]}$\\
        Observation 4: Answer is INCORRECT}
    \end{minipage} \hfill
    \begin{minipage}[t]{0.48 \linewidth}
        \sethlcolor{lightblueshade}
        \hl{\textbf{Trial \#2:} Question: Grown-Ups starred the actor who was best known for which role on  "'Allo 'Allo!"?}\\
        Thought 1: I need to search the actor who was best known for a role on "'Allo 'Allo!" and find out what role they were best known for.\\
        Action 1: Search[Grown-Ups]\\
        Observation 1: Grown-Ups is a 1980 British BBC television film devised and directed by Mike Leigh. It stars Lesley Manville, Philip Davis,  Brenda Blethyn, Janine Duvitski, Lindsay Duncan and Sam Kelly. It was  edited by Robin Sales and produced by Louis Marks for the BBC, and originally shown on BBC 2 on 28 November 1980.\\
        \sethlcolor{lightgreenshade}
        \hl{Thought 2: The paragraph does not mention the actor who was best known for a role on "'Allo 'Allo!". I need to search the actor's name instead.\\}
        Action 2: Search[Sam Kelly]\\
        Observation 2: Roger Michael Kelly (19 December 1943 – 14 June 2014), known by the stage name Sam Kelly, was an English actor who appeared in film, television, radio and theatre. He is best known for his roles as Captain Hans Geering in 'Allo 'Allo!, Warren in Porridge, Sam in On the Up, and Ted Liversidge in Barbara.\\
        Thought 3: Sam Kelly is best known for his role as Captain Hans Geering in "'Allo 'Allo!", so the answer is Captain Hans Geering.\\
        \sethlcolor{lightgreenshade}
        \hl{Action 3: Finish[Captain Hans Geering]\\
        Observation 3: Answer is CORRECT}
    \end{minipage}
    \begin{minipage}[b]{\linewidth}
        \begin{shaded}
            \textbf{Reflection:} I searched the wrong title for the show, "'Allo 'Allo!", which resulted in no results. I should have searched the show's main character, Gorden Kaye, to find the role he was best known for in the show.
        \end{shaded}
    \end{minipage}
    \caption{2 HotPotQA trials within the same environment and task.}
\end{figure}

In Trial 1, the agent searches for "\textit{Grown-Ups}" and "\textit{'Allo 'Allo!}" independently and is unable to produce a correct answer. In Trial 2, the agent refines its second search to target one of the actors listed in the first search. Following, the agent successfully produces the correct answer. In the baseline run, the agent performs the same two searches and receives the same responses from the environment. Without Reflexion, the agent was unable to learn that its chosen method was unproductive.

Reflexion describes a highly applicable approach that may be used to equip agents to solve a variety of complex tasks. A Reflexion agent must have access to a heuristic for termination and a binary reward model. In the AlfWorld experiment, we used a simple heuristic to demonstrate the applicability of the approach. However, in other experiments, a more specific heuristic could be applied to capture a larger set of erroneous cases. For example, the heuristic can be designed to match actions that are associated with high probabilities of failure. In a household environment, the heuristic could signal termination if the agent enters a room that would not contribute to a successful plan.

The second requirement for a Reflexion agent is a binary reward model. In the AlfWorld experiment, we filtered the reward from the AlfWorld environment at every step to determine if the agent had solved the task. Likewise, in other experiments, it is straightforward to design a binary reward model. For code-writing tasks, the binary reward model can be a compilation or type-check attempt. In a capture-the-flag task, the possession status of the flag could be the binary reward model. For other cases, an efficient integration test could be written to assess the success status of the agent.

At its core, Reflexion is an approach designed to promote discovery for problem-solving LLM-agents that may require several trials to solve given tasks, similar to the self-reflective learning methods of humans when given new tasks. In the AlfWorld experiment, there were several environments in which the agent was tasked to find an object, manipulate it with another object, and then place it in another location. In the baseline test, there were several trials that exceeded 50 actions and observations. Reflexion served as a redirection mechanism in some cases and as a summarization tool in others. In subsequent trials, the agent did not need to search every location to find an object that it had previously found. This behavior may be applicable in other experiments in which the observations received from the environment may be significantly long. In those cases, Reflexion is needed to ensure that the query length does not exceed the maximum number of tokens while still discovering important information.

\subsection{Limitations of Reflexion}

Reflexion relies on the emergent property of self-reflection that is present in several large language models. In this study, we used GPT-3.0 and GPT 3.5 to power a ReAct agent \citep{yao2023react} to learn from its past mistakes. While Reflexion enabled the agent to discover new problem-solving techniques in AlfWorld decision-making tasks and HotPotQA knowledge-intensive tasks, we observed a shortcoming in its ability to improve on its baseline performance in a third benchmark, WebShop \citep{yao2022webshop}.

WebShop is a text-based problem-solving benchmark that tests natural language agents to navigate an e-commerce website to find and purchase products given requests from a client. We tested the agent in 100 environments, giving the agent two few-shot examples of successful WebShop trajectories using the ReAct problem-solving technique. However, after only 4 trials, we terminated the baseline and Reflexion runs as the agent did not show improvement in accuracy (Fig. 6) and was not generating helpful, intuitive self-reflections. The agent achieved a $33\% \rightarrow 34\%$ accuracy improvement in the baseline run and a mere $33\% \rightarrow 35\%$ accuracy improvement in the Reflexion run, which suggests that the agent only successfully completed 1 additional task relative to the baseline agent’s performance.

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.75]{media/webshop_training_curves.png}
    \caption{Reflexion vs React performance on WebShop across 100 customer shopping requests.}
\end{figure}

However, after analyzing the failed trajectories, we noted that the chance of a successful item purchase for an agent in a WebShop environment was not necessarily dependent on the agent's ability to plan and execute a correct sequence of actions, but rather on the quality of the WebShop search engine's results. This observation may not be a direct limitation of the Reflexion approach, but it highlights the ability of a Reflexion agent to optimize reasoning trace and action execution but not complete awareness of the quality of the tools that it may be using.

\section{Conclusion}

We proposed an approach that allows natural language agents to learn from past mistakes and redirect future decisions in planning sequences which removes the human trainer in a human-in-the-middle approach. We demonstrated learning curves on the AlfWorld and HotPotQA benchmarks that significantly outperform base ReAct agents. In addition, we include an inconclusive attempt to improve performance on the WebShop benchmark and provide a discussion that highlights a few limitations of this approach. Reflexion is a highly applicable method to improve performance between trials on decision-making and knowledge-intensive tasks due to its sole dependence on a binary reward model. In the AlfWorld and HotPotQA experiments, we constrained the reward model to imitate environments in which informative reward models may be difficult to design or compute. We encourage others to apply Reflexion to more complex tasks in which the agent must learn to develop new ideas, explore larger unseen state spaces, and form more accurate plans of action through its experiences in past environments.

\section{Reproducibility}

Code and log files for these experiments are available at \url{https://github.com/noahshinn024/reflexion}.


\begin{ack}
A.G. acknowledge funding from National Science Foundation (MCB2027165) for partially supporting this effort.
\end{ack}


\newpage
\bibliographystyle{apalike}
\bibliography{main}

\newpage

\appendix

\section{Few-shot examples (Reflexion)}

    \subsection{AlfWorld}
    
    \label{appendix:sec.0}
        You will be given the history of a past experience in which you were placed in an environment and given a task to complete. You were unsuccessful in completing the task. Do not summarize your environment, but rather think about the strategy and path you took to attempt to complete the task. Devise a concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken. For example, if you tried A and B but forgot C, then devise a plan to achieve C with environment-specific actions. You will need this later when you are solving the same task. Give your plan after "Plan". Here are two examples:\\
        \sethlcolor{lightblueshade}
        \hl{You are in the middle of a room. Looking quickly around you, you see a cabinet 10, a cabinet 9, a cabinet 8, a cabinet 7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a countertop 2, a countertop 1, a diningtable 2, a diningtable 1, a drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\\
        Your task is to: heat some mug and put it in coffeemachine.}\\
        $>$ think: To solve the task, I need to find and take a mug, then heat it with stoveburner, then put it in coffeemachine.\\
        Ok.\\
        $>$ look\\
        You are in the middle of a room. Looking quickly around you, you see nothing.\\
        $>$ think: First I need to find a mug. A mug is more likely to appear in countertop (1-2), diningtable (1-2), drawer (1-3), fridge (1), sinkbasin (1), stoveburner (1-4), toaster (1). I can check one by one, starting with countertop 1.\\
        Ok.\\
        $>$ look\\
        You are in the middle of a room. Looking quickly around you, you see nothing.\\
        $>$ go to countertop 1\\
        On the countertop 1, you see a butterknife 1, a cup 1, a fork 2, and a mug 1.\\
        $>$ think: Now I find a mug (1). Next, I need to take it.\\
        Ok.\\
        $>$ take mug 1 from countertop 1\\
        You pick up the mug 1 from the countertop 1.\\
        $>$ think: Now I take a mug (1). Next, I need to heat it with stoveburner (1-4).\\
        Ok.\\
        $>$ go to stoveburner 1\\
        On the stoveburner 1, you see a pan 2.\\
        $>$ examine stoveburner 1\\
        On the stoveburner 1, you see a pan 2.\\
        $>$ examine stoveburner 1\\
        On the stoveburner 1, you see a pan 2.\\
        $>$ examine stoveburner 1\\
        On the stoveburner 1, you see a pan 2.\\
        $>$ examine stoveburner 1\\
        On the stoveburner 1, you see a pan 2.\\
        STATUS: FAIL\\
        \sethlcolor{lightgreenshade}
        \hl{New plan: I was stuck in a loop in which I continually examined stoveburner 1 instead of heating mug 1 with stoveburner 1. I should have taken mug 1 from countertop 1, then heated it with stoveburner 1, then put it in coffeemachine 1. It did not help to execute two identical actions in a row. I will try to execute a different action if I am stuck in a loop again.}\\
        \\
        \sethlcolor{lightblueshade}
        \hl{You are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a laundryhamper 1, a safe 1, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, and a shelf 1.\\
        Your task is to: look at bowl under the desklamp.}\\
        $>$ think: To solve the task, I need to find and take a bowl, then find and use a desklamp.\\
        OK.\\
        $>$ think: First I need to find a bowl. A bowl is more likely to appear in drawer (1-6), desk (1-2), shelf (1-6), garbagecan (1), laundryhamper (1). I can check one by one, starting with drawer 1.\\
        OK.\\
        $>$ go to drawer 1\\
        The drawer 1 is closed.\\
        $>$ open drawer 1\\
        You open the drawer 1. The drawer 1 is open. In it, you see a cellphone 2.\\
        $>$ go to drawer 2\\
        On the drawer 2, you see a keychain 1.\\
        $>$ go to drawer 3\\
        Nothing happens.\\
        $>$ go to drawer 4\\
        The drawer 4 is closed.\\
        $>$ open drawer 4\\
        You open the drawer 4. The drawer 4 is open. In it, you see nothing.\\
        $>$ go to drawer 5\\
        On the drawer 5, you see nothing.\\
        $>$ go to drawer 6\\
        The drawer 6 is closed.\\
        $>$ open drawer 6\\
        You open the drawer 6. The drawer 6 is open. In it, you see a book 2, and a pen 2.\\
        $>$ go to desk 1\\
        On the desk 1, you see a alarmclock 1, a desklamp 1, a mug 1, and a pencil 1.\\
        $>$ think: Now I find a desklamp (1). Next, I need to use it.\\
        OK.\\
        $>$ use desklamp 1\\
        You turn on the desklamp 1.\\
        $>$\\
        Nothing happens.\\
        $>$ go to desk 2\\
        On the desk 2, you see a bowl 2, a bowl 1, a cd 2, a cd 1, and a creditcard 3.\\
        $>$ think: Now I find a bowl (2). Next, I need to take it.\\
        OK.\\
        $>$ take bowl 2 from desk 2\\
        You pick up the bowl 2 from the desk 2.\\
        $>$\\
        Nothing happens.\\
        $>$\\
        Nothing happens.\\
        STATUS: FAIL\\
        \sethlcolor{lightgreenshade}
        \hl{New plan: In this environment, my plan was to find a bowl then find and use a desklamp. However, the task says to look at bowl under the desklamp. I should have looked for the desklamp then looked for the bowl. I noticed that the desklamp was found on desk 1. In the next trial, I will go to desk 1, find the lamp, then look for the bowl under the desklamp.}
    
    \subsection{HotPotQA}
    
    \label{appendix:sec.1}
        \sethlcolor{lightblueshade}
        \hl{\textbf{Question:} The Rome Protocols were signed by three Prime Ministers one of which was assassinated as part of what?}\\
        \textbf{Thought 1:} I need to search Rome Protocols, find the three Prime Ministers, then find what they were assassinated as part of.\\
        \textbf{Action 1:} Search[Rome Protocols]\\
        \textbf{Observation 1:} The Rome Protocols were a series of three international agreements concluded in Rome on 17 March 1934 between the governments of Austria, Hungary and Italy. They were signed by Italian Prime Minister Benito Mussolini, Austrian Chancellor Engelbert Dollfuss and Hungarian Prime Minister Gyula G\"omb\"os. All of the protocols went into effect on 12 July 1934 and were registered in League of Nations Treaty Series on 12 December 1934.\\
        \textbf{Thought 2:} The three Prime Ministers were Benito Mussolini, Engelbert Dollfuss, and Gyula G\"omb\"os. I need to search Benito Mussolini and find what he was assassinated as part of.\\
        \textbf{Action 2:} Search[Benito Mussolini]\\
        \textbf{Observation 2:} Benito Amilcare Andrea Mussolini (UK:  MU(U)SS-\textbackslash textschwa-LEE-nee, US:  MOOSS-, Italian: [be\textbackslash textasciitilde ni\textbackslash textasciitilde to a\textbackslash textasciitilde milkare an\textbackslash textasciitilde dr\textbackslash textasciitilde ea musso\textbackslash textasciitilde li\textbackslash textasciitilde ni]; 29 July 1883 -- 28 April 1945) was an Italian politician and journalist who founded and led the National Fascist Party (PNF). He was Prime Minister of Italy from the March on Rome in 1922 until his deposition in 1943, as well as \"Duce\" of Italian fascism from the establishment of the Italian Fasces of Combat in 1919 until his summary execution in 1945 by Italian partisans\ldots\\
        \textbf{Thought 3:} Benito Mussolini was assassinated as part of the collapse of the Axis powers during World War II. So the Rome Protocols were signed by three Prime Ministers one of which was assassinated as part of World War II.\\
        \textbf{Action 3:} Finish[World War II]\\
        \sethlcolor{lightgreenshade}
        \hl{\textbf{Reflection:} I searched one of the prime ministers involved in the signing, then attempted to answer right away. I should have searched each of the prime ministers, then looked up 'death' on each of their pages in order to get more information before answering.}\\\\
        \sethlcolor{lightblueshade}
        \hl{\textbf{Question:} Kam Heskin plays Paige Morgan in a 2004 film directed by who?}\\
        \textbf{Thought 1:} I need to search Kam Heskin and Paige Morgan, find the 2004 film they are in, then find the director of the film.\\
        \textbf{Action 1:} Search[Kam Heskin]\\
        \textbf{Observation 1:} Kam Erika Heskin (born May 8, 1973) is an American actress. She began her career playing Caitlin Deschanel on the NBC daytime soap opera Sunset Beach (1998--1999), before appearing in films Planet of the Apes (2001) and Catch Me If You Can (2002). Heskin went on in 2003 to play Elizabeth Bennet in an independent film Pride \& Prejudice: A Latter-Day Comedy, Robin in Charmed (2004),  and Paige Morgan in The Prince and Me film franchise (2006--2010).\\
        \textbf{Thought 2:} Kam Heskin plays Paige Morgan in The Prince and Me film franchise. I need to search The Prince and Me film franchise and find the director of the 2004 film.\\
        \textbf{Action 2:} Search[The Prince and Me film franchise]\\
        \textbf{Observation 2:} Could not find [The Prince and Me film franchise]. Similar: ['Kam Heskin', 'Prince of Persia', 'Harry Potter and the Half-Blood Prince (film)', 'Prince of Persia: The Sands of Time (film)', 'The Chronicles of Narnia (film series)', 'Majestic Prince (franchise)', 'Kingsman (franchise)', 'The Little Prince (2015 film)', 'Shrek (franchise)', 'Harry Potter (film series)']\\
        \textbf{Thought 3:} I need to search The Prince and Me (2004 film) instead.\\
        \textbf{Action 3:} Search[The Prince and Me (2004 film)]\\
        \textbf{Observation 3:} Could not find [The Prince and Me (2004 film)]. Similar: ['The Prince \& Me', 'The Prince \& Me 2: The Royal Wedding', 'Kam Heskin', 'Prince of Darkness (film)', 'Prince of Persia: The Sands of Time (film)', 'Rob Knox', 'Alexander (2004 film)', 'Prince of Persia', 'Brooklynn Prince', 'Prince (musician)']\\
        \textbf{Thought 4:} I need to search The Prince \& Me (2004 film).\\
        \textbf{Action 4:} Search[The Prince \& Me (2004 film)]\\
        \textbf{Observation 4:} Could not find [The Prince \& Me (2004 film)]. Similar: ['The Prince \& Me', 'The Prince \& Me 2: The Royal Wedding', 'Prince of Darkness (film)', 'Prince of Persia: The Sands of Time (film)', 'Rob Knox', 'Alexander (2004 film)', 'Prince (musician)', 'Prince of Persia', 'Kam Heskin', 'Brooklynn Prince']\\
        \textbf{Thought 5:} I need to search The Prince \& Me (2004 film) instead.\\
        \textbf{Action 5:} Search[The Prince \& Me (2004 film)]\\
        \textbf{Observation 5:} Could not find [The Prince \& Me (2004 film)]. Similar: ['The Prince \& Me', 'The Prince \& Me 2: The Royal Wedding', 'Prince of Darkness (film)', 'Prince of Persia: The Sands of Time (film)', 'Rob Knox', 'Alexander (2004 film)', 'Prince (musician)', 'Prince of Persia', 'Kam Heskin', 'Brooklynn Prince']\\
        \textbf{Thought 6:} I need to search The Prince \& Me (2004 film).\\
        \textbf{Action 6:} Search[The Prince \& Me (2004 film)]\\
        \textbf{Observation 6:} Could not find [The Prince \& Me (2004 film)]. Similar: ['The Prince \& Me', 'The Prince \& Me 2: The Royal Wedding', 'Prince of Darkness (film)', 'Prince of Persia: The Sands of Time (film)', 'Rob Knox', 'Alexander (2004 film)', 'Prince (musician)', 'Prince of Persia', 'Kam Heskin', 'Brooklynn Prince']\\
        \sethlcolor{lightgreenshade}
        \hl{\textbf{Reflection:} I got stuck in a loop where I kept trying to search 'The Prince \& Me (2004 film)' but the page could not be found. Instead I should have tried to search the similar results that had a similar name to see and they were made in 2004.}
    
    \subsection{WebShop}
    
    \label{appendix:sec.2}
    \sethlcolor{lightblueshade}
        \hl{\textbf{Instruction:}}\\
        \hl{i am looking for dairy free and apple variety pack of chips, and price lower than 30.00 dollars}\\
        \textbf{\text{[Search]}}\\
        \\
        \textbf{Action:} search\text{[dairy free and apple variety pack of chips]}\\
        \textbf{Observation:}\\
        \text{[Back to Search]}\\
        Page 1 (Total results: 50)\\
        \text{[Next $>$]}\\
        \text{[B07HRFJWP8]}\\
        Enjoy Life Foods Soft Baked Ovals, Breakfast Bars, Nut Free Bars, Soy Free, Dairy Free, Non GMO, Gluten Free, Vegan, Variety Pack, 4 Boxes (20 Bars Total)\\
        \$100.0\\
        \text{[B01KMHY5PG]}\\
        Enjoy Life Soft Baked Chewy Bars, Variety Pack, Nut Free Bars, Soy Free, Dairy Free, Gluten Free, 6 Boxes (30 Total Bars)\\
        \$21.49\\
        \text{[B008D2X8C4]}\\
        Enjoy Life Lentil Chips Variety Pack, Dairy Free Chips, Soy Free, Nut Free, Non GMO, Vegan, Gluten Free, 24 Bags (0.8 oz)\\
        \$100.0\\
        \textbf{Action:} think\text{[The first two items are not what I want. The third item is the dairy free and apple variety pack of chips I want.]}\\
        \textbf{Observation:} OK.\\
        \textbf{Action:} click\text{[B008D2X8C4]}\\
        \textbf{Observation:}\\
        \text{[Back to Search]}\\
        \text{[$<$ Prev]}\\
        flavor name \text{[dill and sour cream][garlic \& parmesan][light sea salt][margherita pizza][thai chili lime][variety pack]}\\
        size \text{[0.8 ounce (pack of 24)][4 ounce (pack of 12)]}\\
        Enjoy Life Lentil Chips Variety Pack, Dairy Free Chips, Soy Free, Nut Free, Non GMO, Vegan, Gluten Free, 24 Bags (0.8 oz)\\
        Price: \$100.0\\
        Rating: N.A.\\
        \text{[Description]}\\
        \text{[Features]}\\
        \text{[Reviews]}\\
        \text{[Buy Now]}\\
        \textbf{Action:} think\text{[The item has options 'variety pack' and '0.8 ounce (pack of 24)' and seems good to buy.]}\\
        \textbf{Observation:} OK.\\
        \textbf{Action:} click\text{[variety pack]}\\
        \textbf{Observation:} You have clicked variety pack.\\
        \textbf{Action:} click\text{[0.8 ounce (pack of 24)]}\\
        \textbf{Observation:} You have clicked 0.8 ounce (pack of 24).\\
        \textbf{Action:} click\text{[Buy Now]}\\
        STATUS: FAIL\\
        \sethlcolor{lightgreenshade}
        {\hl{\textbf{Next plan:}} In this attempt, I was unsuccessful. I accidentally bought a product that was \$100, which is more than my budget of \$30. Either way, the initial search results were not good. Next time, I will do \textbf{search\text{["variety pack of chips"]}} and then check if the results meet the dairy free and the \$30 budget constraints. I will continue to refine my searches so that I can find more products.}

\end{document}
