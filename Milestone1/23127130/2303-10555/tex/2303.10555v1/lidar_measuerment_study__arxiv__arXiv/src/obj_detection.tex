\nsection{Robustness Evaluation on Object Detectors} \label{sec:object_detection}


In this section, we evaluate the attack impact of LiDAR spoofing attacks based on the newly-obtained LiDAR spoofing attack capabilities obtained in~\S\ref{sec:spoofing}. For injection attacks, we evaluate 3 major types of object detectors trained on 5 different datasets. For removal attacks, we evaluate the end-to-end safety consequences with a closed-loop simulation.

\nsubsection{Vulnerability to Object Injection Attacks}\label{sec:vul_to_injection}

The CPI attack capability is not feasible on next-gen LiDARs, but the attacker can still inject points while they are not well controlled as listed in Table~\ref{tbl:inject_attack}.
\newpart{To systematically evaluate the attack impact, we formulate the attack capability $\mathcal{P}_A$ against those LiDARs as follows:
\vspace{-0.03in}
\begin{align}
     \mathcal{P}_A := \{x + \delta \cdot g(x) |  x \in \mathcal{C}_{n} \subset \mathcal{C}, \delta \sim \mathcal{N}(0, \sigma )  \}
     \label{eq:attack_cap}
\end{align}\vspace{-0.03in}
, where $\mathcal{C}$ is a point cloud (i.e., chosen pattern) that the attacker originally wants to inject (e.g., point cloud of vehicle); 
$\mathcal{C}_n$ is a point cloud randomly downsampled to $n$ points from $\mathcal{C}$ to model the impact from pulse fingerprinting (\S\ref{sec:sec_enchance_feats})};
$x \in \mathbb{R}^{3}$ is a point injected by the attack; $g: \mathbb{R}^{3} \rightarrow \mathbb{R}^{3}$ is a function to obtain a movable unit direction of point $x$. Due to LiDAR physics, each point can only move along with the laser direction. As the LiDAR typically locates at the origin of the point cloud, $g(x)$ can be written as $\frac{x}{||x||_{2}}$ in this case. To simulate the spoofing inaccuracy and the effect of the timing randomization, we add a noise 
following a normal distribution $\mathcal{N}(0, \sigma)$ 
We utilize this downsampling-based attack capability to simulate the attack impact on next-gen LiDARs.


\nsubsubsection{Experiment Setup} \label{sec:inj_setup}  ~\\
\vspace{-0.1in}

\textbf{Target Object Detectors:} We select 5 popular DNN-based 3D object detectors covering the 3 major methods in AD as discussed in~\S\ref{sec:3d_obj}. For the voxel-based method, we select PointPillars~\cite{lang2019pointpillars}, SECOND~\cite{yan2018second}, and Part-A$^2$~\cite{shi2020points}. For the point-based method, we select 3DSSD~\cite{yang20203dssd}. For the point voxel-based method, we select PV-RCNN~\cite{shi2020pv}. We obtain pretrained object detection models in MMdetection3D~\cite{mmdet3d2020}, OpenPCDet~\cite{openpcdet2020}, and Apollo~\cite{apollo}.
For PointPillars~\cite{lang2019pointpillars}, we prepare 5 different models trained on 5 different datasets (KITTI~\cite{Geiger2012CVPR}, Waymo~\cite{Sun_2020_CVPR}, nuScenes~\cite{nuscenes}, Lyft~\cite{lyft} datasets), and the dataset used in Apollo 6.0~\cite{apollo}. 



\textbf{Evaluation Scenario:}
We synthesized an evaluation scenario based on a scenario in the KITTI dataset.
Fig.~\ref{fig:synth_scenario} depicts the generated scenario. 
We generate 15 scenarios varying the distance between the victim to the vehicle object from 0 m to 14 m (0 m means the victim's nose touches the vehicle object's tail). 
Fig.~\ref{fig:n_points} shows the number of points on the injected vehicle object.
We downsample the points of the injected vehicle to 10, 50, 100, and 200 points, i.e., $n = 10, 50, 100,$ and $200$ in Eq.~\ref{eq:attack_cap}. For each $n$, we generate 100 randomly downsampled cases.

\textbf{Evaluation Metric:}
We judge that the injected object is detected if there exists a detected object overlapping with the ground truth area of the injected object, i.e., the IoU between the ground truth and detected object is more than zero. 
Note that this IoU threshold is not the threshold to judge whether an object is detected or not.
We follow the default thresholds in MMdetection3D~\cite{mmdet3d2020} and OpenPCDet~\cite{openpcdet2020}.


\nsubsubsection{Impact of Model Architecture} \label{sec:impact_model}
Fig.~\ref{fig:obj_kitti} shows the detection rates of different model architectures trained on the KITTI dataset.
To focus on the impact of model architecture, we compare them on the KITTI dataset and do not consider the spoofing errors, i.e., $\sigma=0$.
The results show that PointPillars is the most vulnerable to a small number of injected points (e.g., $n=10$) as the detection rates reach more than 86\% at 5 m. The voxel-based approach likely thinks there is an object as long as there is a certain number of points because this approach cannot precisely capture the shape of points.
On the other hand, PV-RCNN shows the highest robustness as the detection rates are inconsistent with the number of spoofed points, even in the 50-point case. It should be due to the PointNet~\cite{qi2017pointnet++} backbone, which can be aware of the shape of points more precisely.
Nevertheless, the downsampling-based attacks are generally successful on all architectures when $n\geq100$. 


\nsubsubsection{Impact of Training Data} \label{sec:impact_data}
Fig.~\ref{fig:obj_dataset} shows the detection rates of the injected object with PointPillars trained on different datasets, including the PointPillars model in Baidu Apollo 6.0~\cite{apollo}, which is an industry-grade AD system. We select PointPillars considering it is widely used in AD systems~\cite {apollo, autoware}. We use $\sigma=0$ to focus on the difference between datasets.
The Apollo model has different security properties from the other models. It does not get fooled in most cases when $n = 10$ and the distance is further than 2 m. On the other hand, other models are typically fooled when $n=10$, especially the models trained on the nuScenes~\cite{nuscenes} and Lyft~\cite{lyft}.
There is a considerable gap between the industry-grade object detector trained on large-scale private datasets and the models trained with public datasets. This suggests that when we conduct the security analysis on object detectors in AD, we should target industry-grade models designed to be usable in AD systems.


\begin{observation}{RQ3}
All the 3 major types of 3D object detectors can be vulnerable to 100 points simply downsampled a point cloud of a vehicle under the accurate CPI assumption. Particularly, models trained on nuScenes~\cite{nuscenes} and Lyft~\cite{lyft} are so vulnerable that even attacks with 10 spoofed points can be highly successful.
\end{observation}

\nsubsubsection{Impact of Spoofing Accuracy} \label{sec:impact_noise}

We evaluate the 4 different $\sigma$ in Eq.~\ref{eq:attack_cap}: $\sigma$ = 0.35 m corresponds to the error of our spoofer (\S\ref{sec:inj_vlp16});  $\sigma$ = 0.9 m and $\sigma$ = 15 m corresponds to the minimum and second minimum errors of timing randomization (\S\ref{sec:sec_enchance_feats}). 
Fig.~\ref{fig:obj_noise_data} shows the detection rates under the attack with $n=100$ against PointPillars~\cite{lang2019pointpillars} trained on different datasets. 
Similar to the findings in~\S\ref{sec:impact_data}, the models trained on nuScenes~\cite{nuscenes} and Lyft~\cite{lyft} are particularly vulnerable even when $\sigma = 15 m$.
For the Apollo model, we find that the spoofing error $\sigma$ = 0.35 m (the error of our spoofer (\S\ref{sec:inj_vlp16})) does not have a significant impact on the attack success rate.
The noise levels corresponding to the timing randomization ($\sigma$ = 0.9 m and 15 m) show high defense effectiveness when $\sigma$ = 15 m. Meanwhile, the attack success rates are still high ($>$35\% in general) 
when $\sigma$ = 0.9 m (NextG\circled{3}, \S\ref{sec:sec_enchance_feats}).  
We also evaluate the impact of the model architecture under the 4 different $\sigma$. As shown in Fig.~\ref{fig:obj_noise_arch}, the difference in architecture does not have a significant impact compared to the difference in the training dataset. These results are consistent with the results in~\S\ref{sec:impact_model}.


\begin{observation}{RQ3}
Even for LiDARs with timing randomization feature, the object detectors may still be exploitable in object injection attacks if the randomization is not large enough.
Our results show that timing randomization with $\geq$15 m noise level can be considered as an effective defense against object injection attacks.
\end{observation}


\begin{figure}[t!]
\centering
\vspace{-0.12in}
\includegraphics[width=\linewidth]{imgs/obj_detector/obj_100pts_under_noise_data.pdf}
\vspace{-0.25in}
\caption{Detection rates of the PointPillars~\cite{lang2019pointpillars} 3D object detection model trained on 5 different datasets under different modelled spoofing accuracy levels ($\sigma$).}
\label{fig:obj_noise_data}
\end{figure}



\nsubsubsection{Discussion and Summary} \label{sec:injection_summary}

We find that: (1) 100 points are generally enough to fool major object detectors; (2) models trained on publicly-available datasets have higher vulnerability than the Apollo model trained with private dataset from industry; 
(3) the timing randomization can be an effective defense against the downsampling-based attack with 100 points if the randomization level is sufficiently high; and (4) the noise level in the CPI spoofing capability does not have a major effect on its attack capability on the object detector side. Based on these results, we thus consider that the first-gen LiDARs, such as VLP-16 and VLP-32c, are still vulnerable to injection attacks due to the lack of effective security-related features.


For the next-gen LiDARs, the NextG\circled{2}-level pulse fingerprinting is not effective enough to defend against object injection because, as discussed in~\S\ref{sec:sec_enchance_feats}, the attacker can still inject 113 points while they cannot select which points can bypass the fingerprinting.
Meanwhile, our results show that timing randomization can be an effective defense when the number of injected points is limited.
Thus, the combination of timing randomization and pulse fingerprinting can effectively defend against object injection attacks.


\vspace{0.1in}
\nsubsection{Vulnerability to Object Removal Attacks} \label{sec:vul_to_removal}
\vspace{0.1in}

In~\S\ref{sec:vul_to_injection}, we find that object detectors can be vulnerable to 100 spoofed points randomly downsampled from a vehicle's point cloud. 
This result indicates that removal attacks are more challenging than injection attacks because the attacker needs to remove almost all points of the target object to make the target object undetected.
Moreover, removal attacks need to be effective for a certain number of frames; otherwise, for example in AD scenarios, the victim AD can still detect the object in a few frames and can take defensive action.
As shown in Table~\ref{tbl:hfa}, our HFR attack can remove 78.1\% of points on VLP-16. 
In the scenario used in \S\ref{sec:inj_setup}, the HFR attack remains $\geq$165 points (22\% of 751 points at 14 m), which is not enough to make the object undetected.


However, we notice that almost all points in the center area are successfully removed, i.e., the remaining points are actually mostly located on the side areas. 
To more rigorously model the attack capability, we break down the aggregated attack success rates into each azimuth angle. Fig.~\ref{fig:hfa_asr} shows the attack success rates at each azimuth angle of LiDAR applicable for AD. As shown, PRA~\cite{cao2023you} can remove almost all points within $\pm40^{\circ}$ on VLP-16 as we can hit all lasers with the synchronization. Meanwhile, our HFR attack is also effective within $\pm40^{\circ}$, which is enough to cover a width of a vehicle (usually $\leq$15$^{\circ}$ for a front-near vehicle~\cite{cao2019adversarial, jiachen2020towards}). The attack success rates of NextG\circled{2} are less than 30\% since the coincidental bypass does not happen so frequently. For NextG\circled{3}, we calculate the attack success rates only within the vertical range of 33$^{\circ}$ (Appendix \ref{sec:case_nextg3}), which is enough to cover a front-near vehicle. As a result, NextG\circled{3} shows a similar vulnerability to VLP-16 and VLP-32c, but its timing randomization makes the attack success rate slightly lower even in the center areas. Based on  these results, we evaluate the end-to-end impact of the removal attacks with a closed-loop simulation.

\nsubsubsection{Experiment Setup}

We use a common setup widely used in previous work~\cite{cao2019adversarial, jiachen2020towards, hallyburton2022security, cao2023you}. For the AD system, we use Baidu Apollo 7.0~\cite{apollo}. For the driving simulator, we use LGSVL~\cite{lgsvl}. Both systems are known as industry-grade software.
Fig.~\ref{fig:overview_sim} in Appendix shows the overview of the evaluating scenario. 
We place a sedan vehicle as a target object 200 m away from the victim AD and make the victim AD drive toward the target. We add up to 1 m random perturbation both laterally and longitudinally to (1) the starting point of the victim and (2) the target object, in order to evaluate diverse scenarios. Before reaching the target object, AD reaches and keeps 40 km/h. In the benign case, AD can detect the sedan correctly and can stop before it.
To simulate the attack effect of the removal attacks, we remove points %
before feeding them to Apollo's object detector based on the attack success rates in Fig.~\ref{fig:hfa_asr}, i.e., we generate a random number for each LiDAR point at each frame and determine the points to be removed or remained based on the attack success rate. We assume that the attack laser is fired at the top of the sedan. 
To understand at which moment the attacker needs to start launching the removal attack, we set an attack start distance $\mathcal{D}$ and only start to apply the simulated removal attack effect when the distance from the victim AD vehicle to the front vehicle is $\leq\mathcal{D}$.

\begin{figure}[t!]
\centering
\vspace{-0.1in}
\includegraphics[width=\linewidth]{imgs/simulator/angle_asr.pdf}
\vspace{-0.3in}
\caption{Attack success rates of PRA~\cite{cao2023you} and our HFR attack for each azimuth angle.}
\label{fig:hfa_asr}
\end{figure}


\nsubsubsection{Experiment Results}
Fig.~\ref{fig:sim_attack} shows an example of an attack trial under our HFR attack with $\mathcal{D}$ = 20 m on VLP-16. The target sedan can be momentarily detected such as at 2 sec before the collision, but the detection disappears soon and the sedan becomes undetected until and also after the collision. Table~\ref{tbl:sim_hfa} shows the collision rate over 10 trials for different LiDARs and attack start distance $\mathcal{D}$. As shown, although both PRA and our HFR attack are effective in causing end-to-end collisions for VLP-16, HFR can achieve so without requiring synchronization with the LiDAR scanning pattern and thus can be applied to a more general set of LiDARs, especially the next-gen ones such as NextG\circled{3}.
Specifically, HFR attack is successful for both VLP-16 and VLP-32c when $\mathcal{D}$ $\geq$ 18 m. For NextG\circled{3}, it shows higher robustness than VLP-16 and VLP-32c since its timing randomization makes points occasionally detectable even in the center azimuth angles. Overall, the attacker needs to start at least 17 m away and ideally at 20 m away to more reliably cause a collision. In almost all trials, HFR attack is not successful on NextG\circled{2}. This is because we cannot bypass the pulse fingerprinting very often (\S\ref{sec:sec_enchance_feats}), and thus a sufficient number of points remains and the points are detected as an object in enough frames (except for 2 trials). This suggests that pulse fingerprinting can be quite effective in defending against the removal attack, since it is virtually impossible to bypass the fingerprinting verification on the majority points of the sedan.
The videos of the simulator experiments are available at our website: \textcolor{blue}{\textbf{\url{https://sites.google.com/view/lidar-study/}}}.

\begin{observation}{RQ2}
Our HFR attack can achieve almost similar end-to-end attack effect in critical AD scenarios as PRA~\cite{cao2023you}, while HFR does not require synchronization and thus can be applied to a much more general and recent set of LiDARs, especially the next-gen ones with timing randomization. Meanwhile, pulse fingerprinting can be quite effective in defending against the removal attacks.
\end{observation}


\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{imgs/simulator/sim_attack_case.pdf}
\caption{An example HFR attack trial with $\mathcal{D}$ = 20 m on VLP-16~\cite{VLP16}. The remaining points are occasionally detected as an object but are not sufficient to avoid a collision.}
\label{fig:sim_attack}
\end{figure}


\begin{table}[t!]
\centering
\caption{Collision rate with the target sedan out of 10 trials for each attack start distance $\mathcal{D}$ and each LiDAR. \textbf{Bold} and \underline{underline} highlight %
100\% and 0\% collision respectively.}
\label{tbl:sim_hfa}
\setlength{\tabcolsep}{2.42pt}
\renewcommand{\arraystretch}{0.75}
\begin{tabular}{ccccccccc}
\toprule
             & Benign & 10 m & 15 m & 16 m & 17 m  & 18 m  & 19 m  & 20 m  \\ \hline
VLP-16 (PRA) & \underline{0/10}   & \underline{0/10} & 2/10 & 7/10 & \textbf{10/10} & \textbf{10/10} & \textbf{10/10} & \textbf{10/10} \\
VLP-16 (HFR) & \underline{0/10}   & 3/10 & 5/10 & 7/10 & 8/10  & \textbf{10/10} & \textbf{10/10} & \textbf{10/10} \\
VLP-32c (HFR)      & \underline{0/10}   & 1/10 & 1/10 & 6/10 & \textbf{10/10} & \textbf{10/10} & \textbf{10/10} & \textbf{10/10} \\
NextG\circled{2} (HFR)         & \underline{0/10}   & \underline{0/10} & \underline{0/10} & \underline{0/10} & 1/10  & \underline{0/10}  & \underline{0/10}  & 1/10  \\ 
NextG\circled{3} (HFR) & 	\underline{0/10} & 	\underline{0/10} & 	\underline{0/10} & 	2/10 & 	4/10 & 	7/10 & 	8/10 & 	\textbf{10/10}\\
\toprule
\end{tabular}
\end{table}
