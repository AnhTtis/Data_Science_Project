


\section{Experiments and Results}
% \ty{The entire section is very draft}
\label{sec: result}

% \begin{figure*}[ht]
% \centerline{\includegraphics[width=1.0\linewidth]{figures/Figure_results.png}}
% \caption{\small{Snapshots of our approach in action on the Aliengo in simulation.}}
% \label{fig: indoor}
% \vspace{-10pt}
% \end{figure*}


We now present our experimental analysis of our Auto-Regressive Motion Planner, ARMP. Our experiments are designed to answer the following questions:
\begin{enumerate}
    \item Can ARMP generate trajectories in various scenarios?
    \item Are ARMP-generated trajectories physically feasible?
    \item Can ARMP be employed as a low-level controller for indoor navigation?
\end{enumerate}
For the rest of the section, we will first provide information about our experimental setting and then discuss each research question in the corresponding subsections.
% The following subsections are constructed as follows: first, we introduce the basic information about our learning and experiment setting. Then we test if ARMP is able to generate diverse motions to answer Question (1). For Question (2), we conducted an experiment to validate the physical feasibility of the generated motion. Lastly, we apply our approach AWMP for indoor navigation tasks to answer Question (3).


% \ty{Three questions: (1) are generated motions expressive enough to handle different terrains? (2) are generated motion physically feasible? (3) can ARMP be applied to indoor navigation task?}




\subsection{Experiment Setting}
% \ty{Here we show what simulator we are using. Robot? Training details, NN structure, training time, GPU.... }
We implement our framework using PyTorch~\cite{paszke2019pytorch}. 
Our motion planner is constructed using the mixture-of-expert architecture with 4 experts as suggested in the work of Zhang et al.~\cite{zhang2018mode}. Each expert is a multi-layer perceptron with 2 hidden layers of size [512, 512] with ELU\cite{clevert2015fast} activations. For the motion library, we construct the majority of the motion library via trajectory optimization, where we generate 56 trajectories with durations ranging from 20 to 65 seconds. The trajectories are optimized by changing target position, orientation, time horizon, and gaiting frequencies, resulting in total of 43 minutes of data with about 158000 state transitions when discretized into 30 frames per second. We generate a jumping motion by retargeting the captured dog's jumping motion in Zheng et al.~\cite{zhang2018mode}. The single jumping trajectory of 9 seconds length is duplicated 11 times and then added to the motion library. The motion library is then used for training our module via Google Colab\cite{bisong2019google}. We use AlienGo~\cite{unitree} from Unitree as our robotic platform and use PyBullet~\cite{coumans2016pybullet} only for visualization.


\subsection{Generating Trajectories in Various Scenarios} %first two questions

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.25\textwidth}
         \centerline{\includegraphics[width=1.1\linewidth]{figures/Path_Follow_1.pdf}}
         \caption{Path Following}
         \label{fig:follow1}
     \end{subfigure}%
     \begin{subfigure}[b]{0.25\textwidth}
         % \vspace{-10pt}
         \centerline{\includegraphics[width=1.1\linewidth]{figures/Path_Follow_2.pdf}}
         \caption{Velocity Tracking}
         \label{fig:follow2}
     \end{subfigure}\hfill%
\caption{\small{Tracking results of the given COM path (\textbf{Left}) and the velocity profile (\textbf{Right}).}}
\label{fig:follow}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.5\textwidth}
        \centerline{\includegraphics[width=1.1\linewidth]{figures/Jump_1.pdf}}
         % \vspace{-5pt}
         \caption{Jump Trajectory in Motion Library}
         \label{fig:jump1}
     \end{subfigure}\hfill
     \begin{subfigure}[b]{0.5\textwidth}
         % \vspace{-5pt}
         \centerline{\includegraphics[width=1.1\linewidth]{figures/Jump_2.pdf}}
         % \vspace{-5pt}
         \caption{Generated Jump Trajectories}
         \label{fig:jump2}
     \end{subfigure}\hfill%
\caption{\small{Top plot represents the jumping trajectory used in our motion library. Bottom plot shows trajectories generated by our framework with obstacles placed in different positions. Smooth transition between trotting and  the jumping motion can be observed.}}
\label{fig:jump}
\end{figure}

% Figure 2. COM and Velocity tracking
To demonstrate that our framework can generate diverse trajectories, we first check the effectiveness of our framework on following the given desired center of mass (COM) path. 
First, we generate a motion trajectory to follow a path to navigate in cluttered environment, which is generated using RRT*~\cite{karaman2011sampling}. Please note that path generation or viaual navigation is not the main focus of the paper. Rather, we aim to evaluate the capability for generating motion plans with respect to the given high-level task.
We can see in Fig. \ref{fig:follow1} that our generated trajectories can smoothly follow the desired COM path with the maximum deviation of $15.1$~cm.

We also evaluate the velocity following performance of ARMP by commanding a manually-designed velocity profile (Fig. \ref{fig:follow2}), which shows the maximum velocity error of $0.065$~m/s.
We observe the smoothed tracking of velocity due to target trajectory blending discussed in \ref{sec:trajectory blending}. Also note that the given discrete velocity profile is impossible for quadrupedal robots that needs more time for acceleration.

% Figure 3. Jumping trajectories.
We also show how the robot jumps over the obstacle at different distances by plotting the COM trajectories in Fig.~\ref{fig:jump}. It shows how ARMP adjusts and interpolate the existing plans in the motion library to jump over obstacles at different locations.
Since the motion library consists of a single jumping motion as shown in Fig.~\ref{fig:jump1}, we see that the planner retrieves the same motion from the library at the based on terrain condition.
Although the motion library did not contain any motion trajectory that transitions between jumping and trotting, our framework smoothly blends the two based on current state and terrain condition. 

% We want to explore how diverse motions can be generated by ARMP. Here, we define diversity in three aspects: velocity perspective, height perspective, and terrain perspective. 

% The velocity diversity is the range of velocity the motion planner can generate. A larger range makes robots more capable of achieving high performance. In our experiment, we show that ARMP is able to generate realistic motions with a forward velocity ranging from xx to xx and maximum angular velocity up to XX rad/s. We also found that ARMP is able to learn different gait in order to adapt to different desired velocities.

% The height diversity is defined as the maximum root height the robot can reach. A higher root height incentivize jumping motion. We show that ARMP can just jump up to XX height with XXX m forward movement. The jumping motion allows the robot to find new navigation solution when it encounters gaps or obstacles instead just walk around them. We will discuss this in Section \ref{sec: indoor navigation}.

% Terrain diversity is defined as different terrains that ARMP generated trajectories can be used for. Here we test our result in two terrain settings: slope and stairs. These two terrain can be commonly found in indoor environments, the ability to generate motions for these two setting is critical for applying ARMP to indoor navigation task. For the slope terrain, our result shows that ARMP can generate motion for slope at [5,15,30] degrees. For the stair setting, we shows that ARMP can generate stair climbing motion for stairs height at [xx and xx] cm. The generated motion can be best seen from Figure \ref{}.


\subsection{Physical Feasibility of the Generated Plans}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.5\textwidth}
         \centerline{\includegraphics[width=1.1\linewidth]{figures/Track_turn_1.pdf}}
         \caption{Base position (m)}
         \label{fig:track turn 0}
     \end{subfigure}\hfill%
     \begin{subfigure}[b]{0.5\textwidth}
         \centerline{\includegraphics[width=1.1\linewidth]{figures/Track_turn_2.pdf}}
         \caption{Base orientation (°)}
         \label{fig:three sin x}
     \end{subfigure}\hfill%
     \begin{subfigure}[b]{0.5\textwidth}
         \centerline{\includegraphics[width=1.1\linewidth]{figures/Track_turn_3.pdf}}
         \caption{Front right toe position (m) }
         \label{fig:three sin x}
     \end{subfigure}\hfill%
     % \begin{subfigure}[b]{0.25\textwidth}
     %     \centering
     %     \includegraphics[width=\textwidth]{figures/Track_turn_3.pdf}
     %     \caption{Front Left Toe position}
     % \end{subfigure}
\caption{\small{Validation in a full-scale simulation by training a tracking policy. Our planned trajectory is shown in dashed line and the tracked trajectory is in bold line.}}
\label{fig:tracking}
\end{figure}

% Motivation: why is it hard to evaluate the feasibility using optimization?
While ARMP can generate various motions by interpolating the existing ones, the physical feasibility of the generated motions must be validated to be used ARMP as a motion planner.
% However, it is not straightforward to validate the physical feasibility of the given motion plan.
Intuitively, we may want to evaluate the plan by evaluating the cost and constraint functions of the trajectory optimization described in Section~\ref{sec:data_generation}, but the comparison is not straightforward because ARMP and the trajectory optimization use different state representations: a list of vectors and Hermite polynomials. We also do not directly compare the ARMP and trajectory optimization results for the same task because two planners can generate feasible but different motion plans, particularly when it comes to long-horizon tasks.

% Figure 4. DeepMimic analysis
Instead, we check the physical feasibility by leveraging a physics-based simulation. Particularly, we utilize a neural motion imitation policy to track the given motion plan. If it can closely follow the motion plan, it indicates that it is feasible in a high-frequency, full-scale physics-based simulation. We use a deep reinforcement learning algorithm, Proximal Policy Optimization~\cite{schulman2017proximal}, to train such a tracking policy using the motion imitation framework described in Peng et al.~\cite{peng2018deepmimic}. For more details, please refer to the original paper. Fig.~\ref{fig:tracking} shows the ARMP motion plan and the trajectory in tracked in physics-based simulation~\cite{raisim} for a $360^\circ$ circular turning task, which is unseen during training. The tracking policy was able to closely follow the given COM and end-effector trajectories, which indicates the physical feasibility of the planned motion.


\subsection{Application: Indoor Navigation} % last question
\label{sec: indoor navigation}


\begin{figure*}
    \centering
    \setlength{\tabcolsep}{1pt}
    \renewcommand{\arraystretch}{0.7}
    \begin{tabular}{c c c c c}
    \includegraphics[width=0.185\textwidth]{figures/path/path_000005.png} &
    \includegraphics[width=0.185\textwidth]{figures/path/path_000008.png} &
    \includegraphics[width=0.185\textwidth]{figures/path/path_000011.png} &
    \includegraphics[width=0.185\textwidth]{figures/path/path_000014.png} &
    \includegraphics[width=0.185\textwidth]{figures/path/path_000017.png} \\
    \includegraphics[width=0.185\textwidth]{figures/jump/jump_000004.png} &
    \includegraphics[width=0.185\textwidth]{figures/jump/jump_000009.png} &
    \includegraphics[width=0.185\textwidth]{figures/jump/jump_000011.png} &
    \includegraphics[width=0.185\textwidth]{figures/jump/jump_000012.png} &
    \includegraphics[width=0.185\textwidth]{figures/jump/jump_000014.png} \\ 
    \includegraphics[width=0.185\textwidth]{figures/stair/stair_000013.png} &
    \includegraphics[width=0.185\textwidth]{figures/stair/stair_000017.png} &
    \includegraphics[width=0.185\textwidth]{figures/stair/stair_000019.png} &
    \includegraphics[width=0.185\textwidth]{figures/stair/stair_000025.png} &
    \includegraphics[width=0.185\textwidth]{figures/stair/stair_000029.png}   
    \end{tabular}
    
    \caption{Generated motions for indoor navigation.
    \textbf{Top:} a robot follows the path between obstacles.
    \textbf{Middle:} a robot jump over the obstacle.
    \textbf{Bottom:} a robot walks up the stair to get to the room on the second floor.
    }
    \label{fig: indoor}
\end{figure*}

% Motivation
% This section aims to answer the question of whether ARMP can be used as a low-level controller for indoor navigation. 
Many works on indoor navigation frameworks~\cite{habitat19iccv, szot2021habitat} only address the problem of reaching the desired goal position without climbing stairs or encountering negotiable obstacles due to the nature of wheel-based robots. On the other hand, our results show that ARMP can utilize the full potential of the legged robot, as ARMP can generate versatile motions for different terrains. This makes ARMP an effective low-level motion generator for indoor navigation with complex terrain settings. For all the experiments, high-level trajectories are generated manually or using the off-the-shelf motion planner such as RRT* because learning a navigation policy is not the main focus of the paper, although it can be approached by other visual navigation techniques~\cite{kareer2022vinl}.

In this section, we investigate ARMP in the following three scenarios. All the motions can be seen in Fig.~\ref{fig: indoor} and the supplemental video. 

\subsubsection{Simple Navigation}
One of the biggest challenges of indoor navigation is that it consists of various unnavigable areas like walls and obstacles. The goal is to move in a clustered environment to a goal located (19m, 10m) relative to robot’s start position. In this task, we use RRT* to generate a  collision free path of the robot’s base. ARMP uses this path as a target trajectory to be blended, and plans a full body trajectory that reaches the goal in 60 seconds.

\subsubsection{Obstacle Navigation}
In the second scenario, we consider a scene with an obstacle with the dimension of $0.2$~m height and $0.4$~m width. With this obstacle located in the path of robot’s target trajectory, ARMP generates a compelling full-body trajectory that includes a motion to jump over the obstacle without collision. Please note that typical navigation formulation for wheeled robots cannot follow such path.

\subsubsection{Stair Navigation}
Finally, we use ARMP to generate a motion plan which starts on the first floor and ends on the second floor. The stair in the scene is 21~cm height for one step, and there are 8 steps in total. We demonstrate that ARMP can plan a total 14.38~m path that leads the robot to the upper floor, which includes a reasonable stair-climbing motion, where the height difference is compensated using inverse kinematics. Once again, we would like to stress that two-story navigation is not thoroughly investigated in the navigation community due to the assumption of simple wheeled robots. We hope that the proposed ARMP can be used for studying the full navigation capability of legged robots, including multi-story buildings and cluttered scenes.
