{
    "arxiv_id": "2303.13997",
    "paper_title": "PowerPruning: Selecting Weights and Activations for Power-Efficient Neural Network Acceleration",
    "authors": [
        "Richard Petri",
        "Grace Li Zhang",
        "Yiran Chen",
        "Ulf Schlichtmann",
        "Bing Li"
    ],
    "submission_date": "2023-03-24",
    "revised_dates": [
        "2023-03-27"
    ],
    "latest_version": 1,
    "categories": [
        "cs.NE",
        "cs.AI"
    ],
    "abstract": "Deep neural networks (DNNs) have been successfully applied in various fields. A major challenge of deploying DNNs, especially on edge devices, is power consumption, due to the large number of multiply-and-accumulate (MAC) operations. To address this challenge, we propose PowerPruning, a novel method to reduce power consumption in digital neural network accelerators by selecting weights that lead to less power consumption in MAC operations. In addition, the timing characteristics of the selected weights together with all activation transitions are evaluated. The weights and activations that lead to small delays are further selected. Consequently, the maximum delay of the sensitized circuit paths in the MAC units is reduced even without modifying MAC units, which thus allows a flexible scaling of supply voltage to reduce power consumption further. Together with retraining, the proposed method can reduce power consumption of DNNs on hardware by up to 78.3% with only a slight accuracy loss.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.13997v1"
    ],
    "publication_venue": null
}