\section{Conclusion}
\label{sec:conclusion}

In this paper, we have proposed PowerPruning, a novel method to reduce power
consumption in digital neural network accelerators by selecting weights that
lead to less power consumption in MAC operations. The timing characteristics of
the selected weights together with activation transitions are also
evaluated.  We then selected weights and activations that lead to small delays,
so that either the clock frequency of the MAC units can be improved or voltage
scaling can be applied to reduce power consumption further.  Together with
retraining, the proposed method can reduce power consumption of DNNs on
hardware by up to 78.3\% with only a slight accuracy loss.  The proposed
method does not modify MAC units and can be combined
seamlessly with existing hardware architectures for power-efficient neural
network acceleration.  




