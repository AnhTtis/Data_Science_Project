\section{Introduction}
\label{sec:intro} 

Deep neural networks (DNNs) have been successfully applied in various fields,
e.g., image/speech recognition.  In DNNs, a huge number of 
multiply-and-accumulate (MAC) operations with weights need to be executed, which
correspondingly causes a high power consumption in hardware. This high power
consumption poses challenges in applying DNNs on power-constrained computing
scenarios, e.g., plant disease detection in agriculture \cite{chen2020ricetalk}
and medical diagnosis devices \cite{hassantabar2022MHDeep}.  

To overcome the challenge above, various methods on software and hardware
levels have been explored.  On the software level, pruning has been proposed to
reduce the number of weights in DNNs and thus power consumption. For example,
\cite{han2016deepcompression} proposes to prune weights with small absolute
values to reduce the computation cost while maintaining inference accuracy.  In
addition, structure pruning \cite{structurepruning} 
%which prunes filters\cite{??}/channels\cite{??}/kernels\cite{??} 
is further developed to facilitate the mapping of DNNs onto hardware.
%while reducing power consumption. 
Besides pruning, quantization \cite{jacob2018quantization} is another major
category of methods to reduce the computation cost of DNNs. With quantization,
MAC units are implemented to process only integer instead of floating-point
arithmetic, thus leading to a significant power reduction
\cite{DBLP:journals/corr/abs-2103-13630}.

On the hardware level, various architectures have been proposed to explore how
MAC units are organized and how data flow through the accelerators to reduce
power consumption.  The systolic array from Google \cite{TPU2017,googleedgetpu}
adopts a weight-stationary data flow, where weights are stationary and
activations and partial sums are moved across the array to maximize data
reuse.  Accordingly, the amount of memory access and thus power consumption
can be reduced.  In addition, the Eyeriss structure \cite{Eyeriss2017} uses a
row-stationary data flow where the multiplication of rows of filters and
activations is computed in a MAC array to reduce data movement and thus power
consumption.

The hardware architectures above have also been extended to reduce power
consumption further.  For example, a clock-gating scheme is proposed in
\cite{effort2020} to disable the operations of unused MAC units to reduce
dynamic power consumption.  In \cite{uptpu}, power-gating unused processing
elements is proposed to reduce leakage power in idle hardware units.  In addition, an
earlyÂ­stop technique in hardware has been proposed in \cite{earlystop} to skip
unnecessary MAC operations, though a complex control logic is needed to
implement this technique.  Furthermore, GreenTPU in \cite{greentpu} scales the
supply voltage of the computing logic down to near-threshold levels while
keeping a high compute performance.  But this method requires complex control
logic to detect timing errors on-the-fly and to track activation sequences that
cause timing errors.  Similarly, Minerva \cite{Minerva2016} proposes a voltage
scaling of memory units storing weights while exploiting the flexibility of
neural networks to tolerate weight errors. 
%However, this method requires Razor double sampling method for fault detection.

Different from the previous methods, most of which require special hardware
architecture or control logic, we propose PowerPruning, a novel method
exploiting the power and timing characteristics of weights and activations to
reduce power consumption without modifying MAC units.  PowerPruning is
the first technique to evaluate the power and timing properties of each
individual weight value and adjust neural networks accordingly.  This technique
is compatible with the previous methods for power reduction of executing neural
networks and can be integrated with them seamlessly.  The key contributions are
summarized as follows:

\begin{itemize}[topsep=3pt,itemsep=2pt,leftmargin=10pt]

 \item The power consumption of weight values is evaluated with respect to activations when the MAC
   operations are executed on hardware. Afterwards, weight values that lead to
    less power consumption in MAC operations are preferred for training
    neural networks to enhance the power efficiency. 

%Timing and power analysis of the processing elements are done in a time-efficient manner.
%To get a representative power profile, typical operation activity is considered during power analysis.

 \item We consider the actual delays of the MAC operations in hardware with
   respect to weight values and activations.  In training neural networks, the
    weight values and activations that sensitize paths with small delays are
    selected.  Correspondingly, the circuit can run faster without modifying MAC units. We then scale the supply voltage to reduce the power
    consumption while maintaining the original computational performance.

%The timing characteristics of the selected weights 
%together with various activation transitions are also evaluated. 
%According to the analysis result, 
%we further select weights and activations that lead to small delays, 
%so that sensitized path delays of the circuits implementing MAC operations are reduced, 
%which thus allows a scaling of supply voltage to reduce power consumption further.

%Sensitized path delays of 
%the circuits implementing MAC operations are reduced 
%by selecting weight/activation. % according to timing analysis result. 
%The delay reduction 
%allows a scaling of supply voltage to reduce power consumption further. 
%Based on the results from timing and power analysis, weights and activations are selected which minimize both average power consumption and sensitized worst-case delays of the processing elements. The latter enables supply voltage scaling of the processing logic without degrading compute performance.

  \item Neural networks are retrained by restricting weights and activations to
    the selected values while maximizing the inference accuracy. With the
    selected weights and activations, power consumption of DNNs can be reduced
    by up to 78.3\% with only a slight accuracy loss.

\end{itemize}

The rest of the paper is structured as follows.
Section~\ref{sec:preliminaries} explains the motivation of this work.
Section~\ref{sec:methods} elaborates the details of the proposed technique.
Experimental results are presented in Section~\ref{sec:results} and conclusions
are drawn in Section~\ref{sec:conclusion}.
