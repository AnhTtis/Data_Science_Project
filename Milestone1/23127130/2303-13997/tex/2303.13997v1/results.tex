%Main Table


%\input{table}

\section{Experimental Results}
\label{sec:results}

To verify the proposed method, we tested four different neural network and
dataset combinations, as shown in the first column of Table~\ref{tab:results}.
The weights and activations were quantized to eight bits.  The neural networks
were trained using Tensorflow while considering quantization
\cite{jacob2018quantization}. In Tensorflow, the number of 8-bit weights is 255 instead of 256 to maintain the weight distribution symmetrical while the number of 8-bit activations is 256. After training, small weights were pruned to compress the
neural network. We then applied the proposed method to reduce the
power consumption.  For LeNet-5, ResNet-20 and ResNet-50, Nvidia Quadro
RTX 6000 GPU 24 GB was used for training, and for EfficientNet-B0-Lite Nvidia
A100 80 GB GPU was used. The number of times to execute the selection of weight and activation with small delays in Section~\ref{sec:timing} is set to 20 in the experiments. 

%and then prune weights whose absolute values are
%small since pruning can reduce computation cost and thus power consumption.

%In the proposed framework, we first evaluated the power and timing
%characteristics of 8-bit integer weights and 8-bit integer activations.
%Afterwards, we selected weights and activations that lead to small power
%consumption and worst-case delay. 

%To train the neural networks with selected weights and activations while
%guaranteeing accuracy in 

%After pruning, we forced the remaining weights and activations to the selected
%weight and activations values and then retrain the neural networks to improve
%their accuracy.  The neural networks and dataset used in the experimental
%results are shown in first column of Table~\ref{tab:results}.  

To demonstrate the effectiveness of the proposed method on different types of
accelerators, two different hardware implementations of systolic array were
evaluated.  In the optimized hardware architecture (Optimized HW), clock gating
of a MAC unit in case of a zero weight to reduce dynamic power
consumption and power gating of whole unutilized columns in the systolic array
to reduce both the dynamic and static power consumption are
applied.  In the standard architecture (Standard HW), none of these
power-saving features were applied.

The power consumption during inference was estimated with Power Compiler by simulating the systolic
array executing the neural networks using Modelsim. 
%with five randomly selected pictures of the respective datasets.  The reported
%power values reflect only the combinational circuits, excluding all sequential
%elements. 
The simulation was conducted using a netlist description of the systolic array synthesized with
the NanGate 15\,nm cell libraries \cite{Nangate} and the clock frequency around
5\,GHz.  Since cycle-accurate simulations are extremely time-consuming, for
ResNet-20, ResNet-50 and EfficientNet-B0-Lite only the convolutional layers
with the largest number of MAC operations were simulated and compared.

\begin{figure}
    \centering
         \includegraphics[width=0.95\linewidth]{Fig/mainResults.pdf}
         \caption{Comparison with conventional pruning, evaluated on Optimized HW.}
         \label{fig:overallTradeoff}
\end{figure}

Table~\ref{tab:results} summarizes the experimental results of the proposed
method. The original accuracy and the accuracy with our method are shown in
the second and third columns.  According to these two columns, the accuracy
degradation is relatively small, except for EfficientNet-B0-Lite with a drop of
4\%.  With a slight accuracy loss, a significant reduction in power consumption
up to 78.3\% can be achieved, as shown in the sixth and ninth columns,
demonstrating the effectiveness of the proposed method in enhancing power
efficiency of digital accelerators for neural networks. This is especially
useful for edge devices where power consumption is a major issue. 

When executing the neural networks on Standard HW, the total power including
dynamic and leakage power was reduced by up to 60.2\% (sixth column).  On
Optimized HW, the power saving was even greater with a power reduction up to
78.3\% (ninth column). The relatively smaller power reduction on
Standard HW was caused by the leakage power consumption from the MAC units that
were not gated even when they are not used.  In all the cases, the power
consumption of EfficientNet-B0-Lite was not reduced significantly. This was due
to their depth-wise 2D convolutions, which had a very low utilization rate of
the systolic array and thus high execution time, so that the dynamic power
consumption was much lower than the leakage power.  

%\textcolor{red}{ please explain the power reduction results from weight
%selection according to power and supply voltage scaling.  The power reduction
%percentage from voltage scaling with standard hardware and optimized hardware
%is shown in the last two columns.} 

%Since the proposed method cannot reduce the leakage power, the total power
%reduction is not considerable. 

%leading to high execution times and thus a relatively low dynamic power
%consumption compared to leakge power.

To reduce the maximum delay of a MAC unit, which was 180\,ps after synthesis,
we only selected a subset of weight values and activations that lead to small
delays of the MAC operations.  The number of selected weight values and
selected activation values are shown in the tenth column (Wei.) and the
eleventh column (Act.).  According to the tenth column, the number of selected
weight values is reduced significantly, e.g., from 255 to 35 in LeNet5 and
ResNet-20. On the contrary, most activation values still remain to maintain a
good inference accuracy.  The delay reduction due to the weight and activation
selection is shown in the twelfth column (Max Delay Red.). In identifying delay
reduction, our search granularity was 10\,ps. This can be lowered if necessary,
but at the expense of more runtime.

%For exmple, the voltage can be reduced from 0.8V to 0.71V in case of LeNet5.
%summarizes the proposed framework. The accuracy degradation is relatively
%small, only for the compact EfficientNet-B0-Lite it goes up to 4\%. In
%exchange we get a significant reduction in power consumption. When running the
%models on the standard HW, total power is reduced up to xx.x\%. The power
%savings are even greater on the optimized HW, achieving a maximum power
%reduction of xx.x\%. This power reduction is in part accomplished by
%restricting the weight space to a small subset of low-power weight values. To
%reduce the maximum delay, only few activation values need to be pruned. This
%timing budget is leveraged to run the systolic array on a lower supply
%voltage, contributing to a further reduction of power consumption.

To reduce power consumption further, the supply voltage is lowered by the ratio
shown in thirteenth column (Voltage Scaling Factor). For example, for
LeNet-5-CIFAR-10, the supply voltage was reduced from 0.8\,V to 0.71\,V while
still maintaining the original clock frequency.  The relation between supply
voltage scaling and circuit delay was evaluated according to the simulation
results in \cite{power2014}. The last two columns show the percentage of power
reduction contributed by voltage scaling. For Standard HW (column V\_SHW) and
Optimized HW (column V\_OHW), voltage scaling can reduce power consumption by
up to 13.2\% and 11.5\%, respectively. 

To demonstrate the advantage of the proposed method over conventional pruning,
we show the comparison of the power consumption and the inference accuracy of
conventional pruning and the proposed method in
Figure~\ref{fig:overallTradeoff}.  According to this comparison, the proposed
method can significantly reduce the power consumption of a pruned neural
network further with only a slight accuracy loss.
%e.g., the first three comparisons in Figure~\ref{fig:overallTradeoff}.  When
%the leakage power is smaller compared with dynamic power, conventional pruning
%alone already yields a considerable reduction of power consumption.  
The proposed method achieves better power savings when the dynamic power
consumption dominates the overall power consumption,  e.g., the first three
comparisons in Figure~\ref{fig:overallTradeoff}, because it focuses on reducing
the signal switching activities in the circuits by selecting weights. 

%In case the leakage power is dominating the power consumption, conventional
%pruning cannot produce meaningful power savings, as shown with
%EfficientNet-B0-Lite.  However, in all cases, the proposed framework can reduce
%uthe total power consumption significantly with a slight accuracy loss. 

\begin{figure}
         \centering
         \includegraphics[width=0.95\linewidth]{Fig/weight_restriction.pdf}
         \caption{Tradeoff between accuracy and the number of selected weight
	 values, evaluated on Optimized HW.}
         \label{fig:spaceRestriction}
\end{figure}


\begin{figure}
         \centering
         \includegraphics[width=0.95\linewidth]{Fig/delay_acc_tradeoff_v2.pdf}
         \caption{Tradeoff between accuracy and the number of selected activation values.}
         \label{fig:activationRestriction}
\end{figure}

To demonstrate the tradeoff between the number of selected weight values and
the inference accuracy, we used different thresholds to select weight values
according to their power consumption and evaluated the accuracy by restricting
the neural networks to these weight values.  Figure~\ref{fig:spaceRestriction}
illustrates the results.  As expected, a lower power threshold leads to a lower
inference accuracy.  However, there is still a good potential for power
reduction before significant accuracy degradation appears.  For example, for
ResNet-50-CIFAR-100 the power threshold can be lowered down to 800\,$\mu$W,
which corresponds to 55 weight values,
leading to total power savings of 19.6\% with only a negligible accuracy loss.
Note that the drop at 850\,$\mu$W for ResNet-50-CIFAR-100 is due to the
stochastic nature of the training process. For EfficientNet-B0-Lite the power
reduction is relatively small due to the large contribution of leakage power.



Figure~\ref{fig:activationRestriction} shows the tradeoff between accuracy and
the number of activation values. The results are obtained by restricting neural
networks with different number of activation values based on a weight selection
threshold 800\,$\mu$W.  The different numbers of activation values reflect
different maximum delays on the MAC unit.  In this figure, the left most point
corresponds to the full activation space with 256 activation values.  As the
number of activation values and thus the maximum delay decreases, the inference
accuracy is first well-maintained and then drops.  Before the turning point,
there is optimization potential we took advantage of  to enhance computational
performance or reduce power consumption by voltage scaling.



%\begin{figure}
%         \centering
%         \includegraphics[scale=0.27]{Fig/weight_distribution.png}
%         \caption{The distribution with selected weights with proposed method.\textcolor{red}{do we need this figure??}}
%         \label{fig:activationRestriction}
%\end{figure}
