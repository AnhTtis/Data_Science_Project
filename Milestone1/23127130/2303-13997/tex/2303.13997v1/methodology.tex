\section{Weight and Activation Selection for Power-Efficient Neural Network Acceleration}
\label{sec:methods}

In this section, we introduce the proposed PowerPruning method to reduce power
consumption in digital neural network accelerators.  The weight selection
according to the average power consumption is first explained in
Section~\ref{sec:power}.  Afterwards, the selection of weights and activations
with respect to their timing characteristics is explained in Section~\ref{sec:timing}.  The retraining of
neural networks by restricting weights and activations to the selected values to reduce power consumption  
is described in Section~\ref{sec:training}. 

\subsection{Weight selection according to power consumption}\label{sec:power}

As shown in Figure~\ref{fig:powerResult}, different weights in a MAC unit lead
to different average power consumption.  To take advantage of this
characteristic to reduce power consumption of DNN accelerators, the average
power consumption of all the 8-bit integer weight values in a MAC unit should
be evaluated. To do this, the input of the MAC unit corresponding to the weight
is fixed to a given value, as shown in \figname~\ref{fig:mac}. The various
combinations of activation transitions and partial sum transitions are fed into
the other inputs of the MAC unit to obtain the switching activities of the MAC
unit.  Based on these switching activities the power consumption for the fixed
weight value can be evaluated using  Power Compiler from Synopsys. 

Two challenges in evaluating the average power consumption of a weight should
be addressed. First, the number of combined transitions of activations and
partial sums is huge, e.g., $2^{(8+22)\times 2}=2^{60}\approx10^{18}$, when the
activations and the partial sums are quantized to 8 and 22
bits, respectively, for a $64Â \times 64$ systolic array.  $\times 2$ is due to
the fact that the power consumption is caused by the transitions from a
combination of activation and partial sum to another combination, instead of
the static values of the activation and partial sum.  Accordingly, simulating
all these transitions to identify the power consumption is very time-consuming.
Second, just sampling all possible combined transitions of activations and
partial sums does not reflect the probabilities of such transitions when
executing neural networks in a systolic array.  For example, a combined
transition may appear more frequently than other transitions, so that it should
contribute more to the result of power evaluation than others.

To deal with these challenges, 
we first identify the transition distributions for activations 
and partial sums with real data executing on the systolic array, described as
follows. In addition, we partition the value range of the partial sum into a small number of bins to reduce the partial sum transition space and then evaluate the transition probability from one bin to another bin.

%we partition the value range of the partial sum into a small number of bins to
%reduce the partial sum transition space and  then evaluate the transition
%probability from one bin to another bin.

\subsubsection{Evaluation of activation transition distribution}
\label{sec:evac}

For the 8-bit activation as an input to a MAC unit, the total number of possible
transitions is $2^{8\times 2}=2^{16}$.  To obtain the activation transition
distribution, we simulate the activities of a systolic array 
%with the size $64 \times 64$ 
and count the frequency of each individual transition.  For example, for
LeNet-5 on CIFAR10, we randomly select 100 pictures and execute the neural
network on the systolic array. In total we  counted approximately $10^{17}$
activation transitions.  Since this number is larger than the  number of
possible transitions $2^{16}$, the result will well exhibit the distribution of
the activation transitions. 

\begin{figure}
     \centering
     \hfil
     \begin{subfigure}[b]{0.45\linewidth}
         \centering
         \includegraphics[width=0.72\linewidth]{Fig/activationDistribution.pdf}
         \caption{}
         \label{fig:actDis}
     \end{subfigure}
     \begin{subfigure}[b]{0.45\linewidth}
         \centering
         \includegraphics[width=0.72\linewidth]{Fig/partialSumDistribution.pdf}
         \caption{}
         \label{fig:PartSumDis}
     \end{subfigure}
     \hfil
        \caption{Transition distributions of activations and partial sums of a MAC unit.
  (a) Activation transition distribution. (b) Partial sum transition distribution.}
        \label{fig:trans}
\end{figure}

Figure~\ref{fig:trans}(a) shows the resulting activation transition distribution, where 
darker colors represent a lower probability and brighter colors a higher probability. 
%In this figure, the activation values are converted to decimal numbers. 
In this figure, the bright diagonal line clearly indicates that most
transitions appear between activations with similar values, while activation
transitions from very high to very low values and vice versa are very unlikely
to happen.

\subsubsection{Evaluation of partial sum transition distribution and transition
space reduction}

A partial sum has 22 bits in a systolic array with the size of $64 \times 64$,
which results in $2^{22\times 2}=2^{44}\approx1.8\times 10^{13}$ possible
transitions. If we would simulate 100 pictures on the systolic array,  we can
obtain approximately $2.2\times 10^{8}$ partial sum transitions, which is much
smaller than the number of possible transitions and cannot produce a
trustworthy distribution.  Increasing the number of pictures in simulation is
not a viable solution due to runtime.  To solve this problem, we partition the
value range of the partial sum into a small number of bins.  Accordingly,
instead of evaluating the transition probability of individual partial sum
values, we evaluate the transition probability from one bin to another bin.

%To partition the partial sums into bins, we %require that the partial sums in the same bin %share the same most significant bits (MSBs). %Accordingly, the difference between the %activations in the same bin is small, so that %they trigger similar changes from a previous %activation in the result of the multiplier %compared with the activations with difference %in their MSBs. Correspondingly, the number of %switching activities in the circuit is also %small. 
%guarantee that in each bin the
%switching activities of partial sums are %similar. In other words, partial sums
%are grouped according to the potential power %consumption triggered by those switching events.
%To achieve this goal, we partition the partial %sums according to
%the maximum number of consecutive most %significant bits having the same value.
%Table~\ref{tab:binningExample} lists the lower %and upper bounds of
%some selected bins. \textcolor{red}{We %determine the number of fixed bits in the %activations in the same bin by ...} 
%In total, we end up with 43 bins.  
%This partition of the
%partial sums allows us to capture how many most significant bits either stay
%constant if during a transition the sign does not change, or how many most
%significant bits change if the sign changes during a transition.
 
To partition all partial sums into a small number of bins, 
we should guarantee that in each bin 
the switching activities of partial sums should be kept as similar as possible. 
%To achieve this goal, 
We partition the partial sums by keeping the number of consecutive most significant bits with the same value as large as possible.  
Table~\ref{tab:binningExample} lists the lower and upper bounds of 
some selected bins. All values which are in the same bin have the same number of consecutive most significant bits with the same value. 
In total, we end up with 43 bins. 
This partition of the partial sums allows us to capture
how many most 
significant bits either stay constant
if 
during a transition the sign does not change, 
or how many most significant bits change if the sign changes during a transition.
This binning approach is only a heuristic solution and future work is needed to capture the bit-switching characteristics of the partial sums more accurately.



 
\begin{table}[]

\fontsize{6.5pt}{8}\selectfont
\centering
\caption{Bounds for selected bins for partial sum partition. }
\begin{tabular}{@{}ccc@{}}
\toprule
Bin index & Lower bound    & Upper Bound  \\
\midrule
0         & 1000000000000000000000 & 1011111111111111111111 \\
8         & 1111111110000000000000 & 1111111110111111111111 \\
9         & 1111111111000000000000 & 1111111111011111111111   \\
34        & 0000000001000000000000 & 0000000001111111111111   \\
35        & 0000000010000000000000 & 0000000011111111111111  \\
42        & 0100000000000000000000 & 0111111111111111111111  \\
\bottomrule
\end{tabular}
\label{tab:binningExample}
\end{table}

After the partition of partial sums into bins, we simulated 100 pictures and
assigned the real transitions into these bins. Afterwards, the probabilities of
the transitions between bins can be identified, similar to the evaluation of the
activation transition distribution in Section~\ref{sec:evac}.
Figure~\ref{fig:trans}(b) shows the partial sum transition distribution of
the bins.  It can be observed that the full value range of the partial sums are
rarely used.  The bright diagonal line from the upper left to the lower right
corner indicates that there are many transitions between partial sums with
similar values.  However, there is also a slightly weaker diagonal line from
the upper right to the lower left corner, showing that a portion of partial
sums changes their signs during transitions and causes relatively large switching
activities. 

\subsubsection{Weight selection}

With the distributions identified above, we sample 10,000 transitions of both
activations and partial sums according to their probabilities. The combined
transitions are used to simulate the activities of the MAC unit with the weight
input fixed to specific values.  The resulting switching activities are then
used to calculate the average power consumption of the MAC unit for this
weight. This simulation is repeated for each individual weight value and the
result is shown in \figname~\ref{fig:powerResult}, where the power consumption
of each weight varies greatly.  In this result, there is also a trend that
weights close to zero have especially low power consumption, with weight zero
having by far the lowest.

Based on the result of power analysis we first conduct conventional pruning to
maximize the number of weights with zero value to reduce power consumption.  Afterwards, we
select weight values that lead to small power consumption by setting a power
threshold, e.g., 900\,$\mu$W in \figname~\ref{fig:powerResult}.  By setting the
threshold lower, we can achieve potentially more power savings by excluding
more high-power weight values.  However, the accuracy of the DNN may degrade.
Therefore, a tradeoff between power saving and inference accuracy should be
made.




\subsection{Weight and activation selection according to timing profiles}\label{sec:timing}



\input{timing_selection}




\subsection{Neural network training for power reduction}\label{sec:training}

\input{training}
