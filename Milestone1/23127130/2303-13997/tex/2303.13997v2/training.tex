%\subsection{Neural network training}

%%After the power and timing characteristics of weights and activations are evaluated, 
%we can then select weights and activations that lead to small power and delays. 
%Beforing training neural networks 
%with the selected weights and activations, 

\input{table}
To reduce power consumption of DNNs on hardware, we first apply conventional
pruning to remove weights whose absolute values are close to zero.
% while maintaining inference accuracy.   
Afterwards, we select weights that lead to small power
consumption by setting a power threshold. The initial power threshold is
900\,$\mu$W and it is iteratively reduced to select weights.  In each
iteration, the neural networks are retrained with the selected weights to
verify the inference accuracy.  During retraining, we force the
weights to take the restricted values in the forward propagation. In the backward
propagation, the straight through estimator \cite{Bengio2013EstimatingOP} is
adopted to skip the restriction operation. The iterations end when the
inference accuracy starts to drop noticeably.

After the power threshold is determined, we then select weight values and
activations that lead to small delays by setting a delay threshold.  The
initial delay threshold is 170\,$ps$ and the delay
threshold is iteratively reduced by 10\,$ps$ to select weight values and
activations. In each iteration, the neural networks are retrained and verified.
%\textcolor{red}{To maintain a good inference accuracy, in each iteration, we
%execute the selection of weight values and activations described in
%Section~\ref{sec:timing_selection} several times.} 
When the inference accuracy drops by around 5\% of the original inference
accuracy of the neural networks, the best training result is returned.

When executing the neural networks, if the original clock frequency should be
maintained, we can lower the supply voltage to reduce power reduction. We  
use the results in \cite{power2014} to determine the relation between
supply voltage and the delay of the circuit. The scaling of dynamic power
consumption and leakage is conducted according to \cite{7747444}.



%The second stage uses the results from power analysis to optimize the remaining weights which have not been pruned in the first stage. Power optimization in this stage is controlled by a threshold which separates the low-power from the high-power weight values. To obtain a good tradeoff between power reduction and accuracy, we start with a high threshold and iteratively decrease it by a fixed step size. For each threshold, we retrain the DNN while restricting its weights to the corresponding set of low-power weight values. We finally choose the model which conforms to the lowest power threshold while satisfying a given accuracy performance. After the second optimization step all weights of the DNN have been restricted to low-power weight values.

%The final optimization stage utilizes the results from timing analysis to reduce the effective worst-case delay of the MAC unit, enabling further power savings by voltage scaling. We start by reducing the delay of the MAC unit by a small amount and then incrementally increase the delay reduction while monitoring the accuracy. For each iteration, we obtain the weight and activation set which will guarantee the targeted delay reduction and retrain the DNN to these weight and activation sets. Note that all high-power weight values, which have been already excluded in the second optimization stage, are not considered anymore for the delay reduction. The result of the third optimization stage will be a DNN optimized for both power consumption and reduced worst-case delay of the MAC unit.

%To let the DNN adapt itself to the pruned weight and activation sets, we include the restriction of weight and activations into the forward pass of training.

