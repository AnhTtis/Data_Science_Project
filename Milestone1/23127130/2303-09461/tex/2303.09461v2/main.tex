\documentclass{article}

\usepackage[letterpaper, left=1in, right=1in, top=1in, bottom=1in]{geometry} %

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{graphicx} %
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{amsmath}

\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}


\input{sebastian_latex}

\title{ChatGPT Participates in a Computer Science Exam}

\author{
  Sebastian Bordt \\
  Department of Computer Science\\
  University of Tübingen \\
  \texttt{sebastian.bordt@uni-tuebingen.de} \\
  \and
  Ulrike von Luxburg \\
Department of Computer Science and Tübingen AI Center\\
  University of Tübingen \\
  \texttt{ulrike.luxburg@uni-tuebingen.de} \\
}

\begin{document}

\maketitle

\begin{abstract} 
We asked ChatGPT to participate in an undergraduate computer science exam on ''Algorithms and Data Structures''. The program was evaluated on the entire exam as posed to the students. We hand-copied its answers onto an exam sheet, which was subsequently graded in a blind setup alongside those of 200 participating students. We find that ChatGPT narrowly passed the exam, obtaining 20.5 out of 40 points. This impressive performance indicates that ChatGPT can indeed succeed in challenging tasks like university exams. At the same time, the questions in our exam are structurally similar to those of other exams, solved homework problems, and teaching materials that can be found online and might have been part of ChatGPT's training data. Therefore, it would be inadequate to conclude from this experiment that ChatGPT has any understanding of computer science. We also assess the improvements brought by GPT-4. We find that GPT-4 would have obtained about 17\% more exam points than GPT-3.5, reaching the performance of the average student. The transcripts of our conversations with ChatGPT are available at \url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}, and the entire graded exam is in the appendix of this paper. 
\end{abstract}
    
\section{Introduction}

Many have noted that the capabilities of ChatGPT\footnote{\url{chat.openai.com}}, a novel chat-bot model by OpenAI, are so impressive that the model might even be able to succeed in challenging real-world tasks like university exams \citep{meta23}. Indeed, there is already existing evidence to suggest that this might be the case \citep{bommarito2022gpt,choi2023chatgpt,kung2023performance,frieder2023mathematical}. However, apart from one study by legal scholars \citep{choi2023chatgpt}, existing evaluations on university exams probe the model only on a subset of the task for which it might be particularly suited (for example, excluding all questions that contain images). In addition, evaluation of the model's responses is often not blind, which can be problematic because ChatGPT is known to produce strange answers that are subject to interpretation. As such, despite much discussion about the topic, there is to this point little systematic evidence regarding the capabilities of ChatGPT on university exams \citep{mitchell2023really}. 

{\bf We present the results of a simple but rigorous experiment} that evaluates the capabilities of ChatGPT on an undergraduate computer science exam about algorithms and data structures. We conducted this experiment alongside the regular university exam, which allowed us to evaluate the model's responses in a blind setup jointly with those of the students. We posed the different exam questions in a simple standardized format that allowed ChatGPT to give clear answers to all exam questions. We then wrote the answers from ChatGPT by hand onto an exam paper and put it into the stack of student papers. Teaching assistants then graded the answers given by ChatGPT without knowing of this experiment. 


{\bf The result of the experiment is that ChatGPT would have narrowly passed the exam, obtaining 20.5 out of 40 points.} This is impressive but also highlights the limitations of the current version of the model. In particular, the performance of the model was worse than the performance of the average student who participated in the exam (the average student obtained about 24 points, compare Table~\ref{tab:points}). The mixed performance of ChatGPT is interesting insofar as the exam is relatively standardized. Similar exams are being written all around the world, and lots of information about the topics covered in the exam is available on the internet. 

{\bf We also assess the improvements brought by GPT-4.} We find that ChatGPT with the GPT-4 base model would have obtained about 17\% more points on the exam than ChatGPT with the GPT-3.5 base model, reaching the performance of the average student. The observed improvement of 17\% puts our computer science exam roughly in the 50th percentile of the different exams considered in the GPT-4 technical report (compare Table 1 in \citet{openai2023gpt4}).



\section{Related Work}
\label{sec:related_work}

\begin{table}
\centering
\Large
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c}
ChatGPT-3.5 & \multicolumn{11}{c}{\includegraphics[width=0.73\textwidth]{images/score3.png}}\vspace{-0.25em} \\
\hline\vspace{-0.75em}\\
ChatGPT-4 & \multicolumn{11}{c}{\hspace{-0.75em}\includegraphics[width=0.715\textwidth]{images/score-gpt4-3.png}} \\[1pt]
\hline
Student Mean & $\,$ 2.3 & 2.9  &  3.1 & 2.2 & 2.9 & 2.9 & 2.6 & 2.9 & 1.7 & 1.1 & 23.9 \\
\end{tabular}
\caption{The number of points obtained by ChatGPT on the 10 different exercises in our exam, in comparison with the average number of points obtained by the 200 students who participated in our exam. The first row depicts the points obtained by ChatGPT with the GPT-3.5 base model. This is the result of the main experiment discussed in this paper, where model responses were blindly graded alongside student responses. %
ChatGPT-3.5 obtained 20.5 out of a maximum of 40 points. The second row depicts the points obtained by ChatGPT with the GPT-4 base model. Here, the model responses were graded according to the same grading scheme used in the main experiment, but grading was not blind. We gauge that ChatGPT-4 would have obtained about 24 points, reaching the performance of the average student. The third row depicts the average number of points obtained by the 200 students who participated in our exam.}
\label{tab:points}
\end{table}

Large language models, now often called foundation models, have shown remarkable success across a variety of tasks \citep{brown2020language,ouyang2022training}. In particular, they have shown diverse problem-solving capabilities, for example, on university-level math problems \citep{drori2022neural,frieder2023mathematical}. At the same time, even very large language models suffer from certain well-documented limitations that also plagued earlier iterations \citep{marcus2020gpt,bender2021dangers,borji2023categorical}. A large variety of different works now use foundation models to solve specific tasks, such as coding \citep{noever2020chess,chen2021evaluating,madaan2023learning}. Similarly, an emergent literature aims to enhance the  capabilities of language models, for example by giving them access to a numerical calculator \citep{schick2023toolformer}. Because of the dramatic increase in the capabilities of language models in recent years, there is currently a need to develop novel benchmarks to assess these models \citep{srivastava2022beyond,binz2023using,shiffrin2023probing}. For a more comprehensive review of the literature, we refer the reader to \citep{kalyan2021ammus,srivastava2022beyond,meta23}.

\section{Experimental Design}
\label{sec:experimental_design}

\subsection{The Exam}

We consider an exam of an introductory class on algorithms and data structures that Bachelor of Science students in Computer Science typically take in their second year (the third semester in the German university system). The exam covers topics such as sorting algorithms, graph traversal, and dynamic programming. Overall, the topics covered in the exam are being taught in a similar way all across the world. The exam is contained in a single latex text file, and even the figures in the exam are produced by latex code that consists of text only. This is important for our study, since it allows us to pass the figures as text to ChatGTP and evaluate the language model on the entire exam. The exam contains different types of questions, including multiple-choice questions, writing small proofs, writing pseudo-code, and drawing graphs. The entire exam, complete with the answers of ChatGPT and grading remarks, can be found in Appendix \ref{apx:exam}. The idea to conduct this experiment came only after designing the exam, and we did not bias the exercises in the exam towards the capabilities of ChatGPT. 


\subsection{Posing the Exam Questions to ChatGPT}
\label{sec:posing_questions}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/proof_1.png}
         \caption{Our conversation with ChatGPT.}
     \end{subfigure}
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/proof_2.png}
         \caption{The corresponding part of the written exam.}
     \end{subfigure}
        \caption{We obtained the responses of ChatGPT in different conversations with the model. We then hand-copied the responses on an exam sheet, so that they could be blindly graded with the student exams. In the depicted example, the model provides a proof of the fact that $\sqrt{n}\in o(n^{2/3})$.}
        \label{fig:example}
\end{figure}


{\center\includegraphics[width=0.9\textwidth]{images/q1.png}$\,$\\}

We posed the exam questions in 19 different conversations with the model, relying on the latex source file of the exam. All questions were posed to the default model on 16.2.2023. We were able to elicit clear answers to all exam questions within a couple of hours. Because it is known that the prompt can significantly impact the output of ChatGTP (''prompt engineering''), we decided to pose the exam as simply as possible. We did not perform any chain of thought reasoning \citep{wei2022chain}, since similar techniques might also enhance student performance. We informed the model that we were asking questions from a computer science exam about algorithms and asked it to provide short, precise answers. During the entire process, we did not attempt to engineer the prompts in order to steer the model toward better or worse answers. Our only objective was that the model  would give clear answers to all questions (for multiple-choice questions, this would mean that the model clearly stated which option it wanted to choose). The transcript of our conversations with the model is available at \url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}.

Some of the exam questions involved maths, pseudo-code, or graphs. In this case, we simply prompted the model with the latex source code from the exam, as in the following example:

{$\quad$\includegraphics[width=0.45\textwidth]{images/question.png}$\,$\\}

Remarkably, the model gave clear and well-defined answers to all of our questions.\footnote{In response to exercise 4b), the model drew some relatively strange figures in order to indicate different graphs. We simply copied these figures onto the exam paper so that a teaching assistant could decide whether they were worth any points (they weren't).}  When we asked the model to write a small proof, for example, it responded with latex equations. This is depicted in Figure~\ref{fig:example}. Similarly, when we asked the model to complete pseudo-code, it completed the given pseudo-code in a valid way. In two questions, the model had to draw a graph. Both times, the model responded with a valid {\tt tkiz}-graph that could be rendered in latex (this is depicted in Figure \ref{fig:graph_out}).

After conducting the conversations with the model, we copied the responses by hand onto an exam paper sheet (the students wrote the exam on these exact paper sheets, too). In doing so, we did of course ''render'' all latex output of the model onto the paper sheet. A concrete example where the model responded with a {\tt tkiz}-graph is depicted in depicted Figure \ref{fig:graph_out}.

\subsection{Grading}

The exam was taken by about 200 students, who all wrote their answers in hand on the exam sheets. We simply slipped the sheet with ChatGPT's responses among all the other student's sheets. The exam was jointly graded on one day by 10 teaching assistants, who all worked in the same room at the same time. Each of the 10 teaching assistants graded one of the 10 exercises on the exam. The study was blind, meaning that the teaching assistants did not know that one of the solutions had been produced by ChatGTP. An experimenter was present when the teaching assistants graded the exams, and there was no evidence that anyone noticed the exam by ChatGPT. Indeed, while the responses of ChatGPT were sometimes strange, some selected student responses were (unfortunately) not much more meaningful either. 
The graded exam can be found in Appendix \ref{apx:exam}. 


\section{Results}
\label{sec:results}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.35\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/graph_chat_gpt_out.png}
         \caption{Our conversation with ChatGPT.}
         \label{fig:graph_chatgpt}
     \end{subfigure}
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/graph_exam.png}
         \caption{The corresponding part of the written exam.}
         \label{fig:graph_exam}
     \end{subfigure}
        \caption{ChatGPT successfully produced structured output, including pseudo-code and {\tt tkiz}-graphs. In the depicted example, ChatGPT responds with a {\tt tkiz}-graph that is valid in the sense that it corresponds to a valid latex command that can be rendered into a graph. We then copied the graph by hand onto the exam sheet. The response by ChatGPT is not the correct answer to the exam question.}
        \label{fig:graph_out}
\end{figure}

In this section, we discuss the results of the main experiment with GPT-3.5. The improvements brought by GPT-4 are discussed in the next section. The main result is that ChatGPT obtained 20.5 out of 40 points and passed the exam. Since a minimum of 20 points was required in order to pass the exam, ChatGPT passed only very narrowly\footnote{In exercise 3a), the teaching assistant did not recognize a partial mistake made by ChatGPT. Had the mistake been recognized, ChatGPT would have gotten 20 points.} 

On some parts of the exam, the performance of ChatGPT was rather impressive. One example is the small proof depicted in Figure \ref{fig:example}. Here, the model shows that $\sqrt{n}\in o(n^{2/3})$. While this is not very difficult to show, the model's response is flawless and even gives the impression that the model ''understands'' the statement and how to prove it. In addition to this example, ChatGPT showed strong performance on all exercises where the solution involved pseudo-code (exercises 3d, 8f, and 9). In particular, the model provided a very good answer to exercise number 9 on dynamic programming. This is in contrast to our students, who tended to struggle with this exercise. For a comparison of the points obtained by ChatGPT versus the average number of points obtained by the students who participated in the exam, see  Table \ref{tab:points}. 

On other parts of the exam, ChatGPT produced wrong and sometimes strange answers (consider the  gibberish character mix in exercise 4b in the Appendix). In particular, the model struggled on all exercises where that involved structured output that was not pseudo-code (exercises 4a, 5a, 6b, 7a, and 7b). These were often exercises that asked the students to explicate the workings of standard algorithms. For the students, these were among the simplest exercises in our exam. ChatGPT, however, struggled with them. One example of this is depicted in Figure \ref{fig:graph_out}. In this exercise, the solution, by definition, is known to be a directed acyclic graph. ChatGPT, however, produces a graph with lots of loops. In general, it stands out that the model hardly managed to solve any of the questions that involved drawing graphs, even if it always produced valid \texttt{tikz}-pictures.

The performance of the model across the different multiple-choice exercises in our exam was comparable to its overall performance (after correcting for random guessing).

	
\section{GPT-4}

In this section, in addition to the main experiment considered in this paper, we assess the improvements brought by GPT-4. The GPT-4 technical report compares GPT-4 and GPT-3.5 on a number of different exams and reports large performance gains \citep{openai2023gpt4}. However, because the data sets used in the report are not available, it is hard to replicate and assess these results. It has, however, been noted that there is evidence for testing on the training data \citep{snakeoilexams2023}.

We posed the exam questions to ChatGPT with the GPT-4 base model ('ChatGPT-4') in the same way as we posed them in the main experiment (described in Section \ref{sec:posing_questions}). We again copied the answers on an exam sheet, available with the transcripts at \url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}. The only difference with the main experiment is that grading was not blind - we might interpret the result as an estimate of the points that would have been obtained under blind grading. 

In total, ChatGPT-4 obtained 24 out of 40 points. This is an improvement of 3.5 points, or 17\%, over ChatGPT with the GPT-3.5 base model. Interestingly, this means that ChatGPT-4 is on par with the performance of the average student in our exam. While the improvement might seem small, it does in fact mean that ChatGPT-4 was able to answer some of the more challenging multiple-choice questions that the previous version of the model struggled with. ChatGPT-4 was also able to correctly derive the shortest paths on a weighted graph given to it as a {\tt tkiz}-graph (exercise 7b). As such, our simple experiment indicates that GPT-4 does indeed constitute a notable improvement over GPT-3.5.

\section{Discussion}
\label{sec:discussion}

At this point, there is already a large debate about the capabilities and risks of large language models \citep{webson2021prompt,bommasani2021opportunities}. One part of this debate is about the properties of current systems \citep{acoup2023}. The other part is about the question of whether some of the demonstrably existing limitations of the current systems will or will not be overcome in future iterations \citep{marcus2020gpt,bender2021dangers}. With our simple experiment, we intend to provide a single data point of relatively high quality for the study of the currently existing systems.

The results of our experiment are in line with existing research that documents both the impressive capabilities of large language models, as well as their severe limitations \citep{binz2023using,borji2023categorical}. We would like to highlight that the fact that ChatGPT was able to pass our exam does not imply that it has any understanding of computer science, in the way in which we might expect it from a human who was able to pass the exam \citep{mitchell2023really}. Many of the topics covered in the exam are extensively documented on the internet. For example, they all have relatively high-quality Wikipedia entries, and many teaching materials including examples of solved homework problems and assignments can be found online. As such, it is reasonable to assume that ChatGPT has seen many exercises and solutions that are similar to those in our exam during training. In general, much more research is required in order to understand the abilities and limitations of models like ChatGPT \citep{binz2023using,shiffrin2023probing}. 


\section*{Acknowledgements}

This work has been supported by the German Research Foundation through the Cluster of Excellence “Machine Learning – New Perspectives for Science" (EXC 2064/1 number 390727645), the BMBF Tübingen AI Center (FKZ: 01IS18039A) and the International Max Planck Research School for Intelligent Systems (IMPRS-IS).

\bibliography{literature}

\clearpage
\appendix

 




\section{The Exam}
\label{apx:exam}

\begin{figure}
\includepdf[pages=1,pagecommand={},width=1.15\textwidth]{ExamScan.pdf}
\end{figure}

\newpage
\includepdf[pages=2-14]{ExamScan.pdf}

\end{document}
