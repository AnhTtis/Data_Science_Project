\begin{abstract}
   Conditioning image generation on specific features of the desired output is a key ingredient of modern generative models. However, existing approaches lack a general and unified way of representing structural and semantic conditioning at diverse granularity levels. This paper explores a novel method to condition image generation, based on object-centric relational representations. In particular, we propose a methodology to condition the generation of objects in an image on the attributed graph representing their structure and the associated semantic information. 
   We show that such architectural biases entail properties that facilitate the manipulation and conditioning of the generative process and allow for regularizing the training procedure.
   The proposed conditioning framework is implemented by means of a neural network that learns to generate a 2D, multi-channel, layout mask of the objects, which can be used as a soft inductive bias in the downstream generative task. To do so, we leverage both 2D and graph convolutional operators. We also propose a novel benchmark for image generation consisting of a synthetic dataset of images paired with their relational representation. Empirical results show that the proposed approach compares favorably against relevant baselines.
\end{abstract}

\section{Introduction} \label{sec:Introduction}
Graphs are an abstraction that allows for representing objects as collections of entities and binary relationships.
Previous research on graph-based image generation has been limited to the high-level conditioning of the image content by means of \textit{scene graphs}, i.e., graphs where nodes represent objects, and edges denote subject-predicate-object relationships~\citep{johnson2018image, ivgi2021scene}.
Conversely, conditioning on desired fine-grained properties, e.g., spatial location, arrangement or visual attributes, is usually performed without considering a relational structure.
In fact, most of the literature dealing with pose-constrained image generation, e.g.,~\citep{ma2017pose, siarohin2018deformable, neverova2018dense, qian2018pose, jiang2022text2human}, represents key points and semantic attributes with 2D masks or feature vectors; hence not exploiting known relationships among the components of the object being generated. Indeed such approaches are usually limited to conditioning the generation on a fixed template structure and lack flexibility. This paper aims at taking full advantage of graph representations in this context; our approach fills the above research gap and addresses the shortcomings of existing methods by encoding relational inductive biases into the processing. In our method, graphs are used both as effective and compact, fine-grained object-centric representations of the image content as well as an architectural bias for the neural architecture. 
We call such representations \textit{attributed pose graphs}, i.e., graphs whose nodes can represent particular landmarks in an object's structure and have both positional~(location) and, possibly, semantic attributes (e.g., color, class).
Edges, instead, account for relationships among nodes, capturing the object's morphology. 
Our formulation allows for encoding all the desired properties of the generated image in such a graph, without relying on additional inputs, such as reference images or similar, allowing for high flexibility in specifying the conditioning. The generation of the scene can then be manipulated, without any change to the model architecture, by editing the graph, e.g., by modifying the nodes' attributes and/or the number of landmarks used to represent each object. 
This is thanks to the exploitation of \textit{neural message passing}~\citep{gilmer2017neural} and \textit{graph neural networks} (GNNs)~\citep{bacciu2020gentle, bronstein2021geometric} used for constraining the processing.

Our model, named \textit{GraPhOSE}, relies on learning a multi-channel layout mask from the structured representation of the scene. It is trained jointly with a downstream model~(image decoder) that contextually exploits such mask to constrain the generative process. To overcome the lack of pre-annotated object masks for specific use cases, we also propose the usage of \textit{surrogate masks} corresponding to procedurally generated synthetic pose graphs, as targets for a supervised pre-training of GraPhOSE.
After the pertaining, the full architecture, i.e., GraPhOSE and the downstream model, can be trained end-to-end on the task at hand.
Additionally, since pre-training relies on randomly generated graphs potentially (partially) outside the target distribution, the proposed method can act as a regularization, preventing overfitting the most common poses in the target dataset.

To the best of our knowledge, this is the first work to use object-centric graph-based relational representations to condition a neural generative model. Furthermore, we complement the methodological contributions by introducing a benchmark for pose-conditioned image generation: we propose a synthetic dataset named \textit{Pose-Representable Objects} (PRO), which aims at assessing the performance of generative models in conditioning image generations on fine-grained structural and semantic information of the objects in the scene. PRO consists of images containing stylized 2D objects that can be rendered starting from a relational representation, i.e., a graph, encoding their structure and style.
Our contribution can be then summarized as follows:
\begin{itemize}
    \item We provide a novel and general methodology to solve the task of generating images conditioned on an attributed graph that represents the structure and semantics of objects in the scene.
    \item We provide a specific implementation of such methodology, together with a learning procedure based on a task-independent pre-training to enable transfer to different problems.
    \item We propose a novel benchmark for image generation conditioned on relational representations of the objects in the scene.
\end{itemize}
Experimental results show that our approach compares favorably against non-relational baselines and that the proposed method is flexible and can be applied to different settings.

\section{Preliminaries} \label{sec:Preliminaries}

A pose graph is a couple $\gG = \left(\gV, \gE\right)$, $\gV$ is the set of vertices (or nodes) and $\gE$ is the set of edges that connect such nodes. We define node attributes $\vv_i = \left(\vp_i, \vx_i\right)$ where $\vp_i \in \mathbb{R}^2$, $\vx_i \in \mathbb{R}^d$ represents the 2D position of the $i$-th node in $\gV$ and its $d$-dimensional feature vector, respectively. We denote with $\mV$ the node attribute matrix $\mV = \left(\mP \| \mX\right)$ stacking all the node attribute vectors.
The edge connecting the $i$-th to the $j$-th node is indicated as $e_{ij} = \langle i, j\rangle$. We assume the graph to be undirected and indicate its~(symmetric) adjacency matrix as $\mA$; nonetheless, our approach can be seamlessly extended to directed graphs.
The described graph can be processed by stacking GNN layers; in particular, we rely on the \textit{message passing} framework~\citep{gilmer2017neural}, which provides a template for building GNNs by specifying update and aggregation functions to process a node's representation and those of its neighbors.

We indicate as \textit{deep generative model} a neural network that, optionally given a conditioning input, can be trained to match a desired data distribution.
The sampling is often obtained by feeding random noise from a fixed distribution to the model, while conditioning allows to control the generative process, e.g., by specifying a class label. In our case, we consider generative processes conditioned on the objects in the scene, represented as attributed pose graphs.

\section{Graph-based Object-Centric Generation} \label{sec:Method}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.8\linewidth]{pipeline.png}
\end{center}
\caption{Our pipeline, with GraPhOSE in grey. $\mu_{\theta}$ gets pre-trained on surrogate masks. The downstream model, in yellow, can be any trainable generative model that accepts a $3$-d tensor as conditioning input. The whole pipeline can be trained end-to-end.}
\label{fig:overview}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.8\linewidth]{surrogate.png}
\end{center}
\caption{Surrogate mask for a random graph (left) and for a graph representing a person (right). Node positions in graph space are normalized between $0$ and $1$.}
\label{fig:surrogate}
\end{figure}

In this Section, we first provide a high-level description of our method, consequently propose an architecture implementing the framework and finally present our surrogate pre-training objective. 

\subsection{Overview}

Figure~\ref{fig:overview} provides a high-level view of our approach. Given an input pose graph $\gG$, we wish to output a layout mask $\tL \in \left[0,1\right]^{C \times H \times W}$, which will then be used to condition the downstream model on a target generative task. The mask generation process is designed to obtain the final mask by aggregating masks localized w.r.t.\ each node. In particular, the generation of $\tL$ is decomposed into the generation of a mask $\mM_i \in \left[0,1\right]^{H \times W}$ and a feature vector $\vf_i \in \left[0,1\right]^{C}$ for each node $\vv_i$, such that:
\begin{equation}
    \label{eq:multichannel-mask}
    \tL = \frac{1}{|\gV|}\sum_{i \in \gV}\vf_i \otimes \mM_i,
\end{equation}
where $\otimes$ indicates the \textit{tensor product}.
Notably, each node mask, $\mM_i$, is dependant on pose attributes, $\mP$, only, while features $\vf_i$ depend on both pose and attributes $\left(\mP\|\mX\right)$, so that the former encodes only structural information, while the latter will also eventually account for semantics.
In particular, both $\vf_i$ and $\mM_i$ are computed by means of two learnable mappings, such that the core processing of GraPhOSE can be summarized as:
\begin{equation}
    \label{eq:generative_model}
    \begin{split}
        \mF &= \Phi_{\phi}\left(\mV, \mA\right)\\
        \tM &= \mu_{\theta}\left(\mP, \mA\right)\\
        \tL &= \frac{1}{|\gV|}\sum_{i \in \gV}\vf_i \otimes \mM_i,
    \end{split}
\end{equation}
where $\mA$ is the graph's adjacency matrix, $\tM$ and $\mF$ indicate respectively the tensor encompassing all the $\mM_i$ masks and the matrix having as rows the node level representations $f_i$. The $\Phi_\phi$ function can be learned end-to-end with the downstream model, as shown in Section~\ref{sec:Experiments}.
Differently, $\mu_\theta$ is pre-trained on a surrogate task designed to foster the learning of masks coherent with the structure of the object being generated. After pre-training, $\mu_\theta$ can be fine-tuned by being trained end-to-end with the downstream model on the target task.
This approach is suitable to flexibly condition the generation of different objects based on their structure. Notably, we implement our method with two neural networks that operate in parallel: the \textit{encoder}~($\Phi_\phi$) and the \textit{mask generator}~($\mu_\theta$). 
The former implements function $\Phi_\phi$ from Equation~\eqref{eq:generative_model}, while the latter implements function $\mu_\theta$.
The outputs of the two networks are combined as in Equation~\eqref{eq:generative_model} to obtain the layout mask.

\subsection{Implementing the encoder \texorpdfstring{$\Phi_\phi$}{Phi phi}} 

The \textit{encoder}~($\Phi_\phi$) network is based on what we call a \textit{pose convolution} layer, which consists of the following message-passing operator:
\begin{equation}
    \label{eq:poseconv}
    \vh_{i}' = g_g\bigl(g_s\left(\vh_i, \vp_i\right) + \sum_{j \in \mathcal{N}\left(i\right)} g_l\left(\vh_j, \vp_j - \vp_i\right), \vh_i\bigr)
\end{equation}
where $\mathcal{N}\left(i\right)$ is the set of neighbours of the $i$-th node, while $g_s$, $g_l$ and $g_g$ can be any learnable function~(e.g., MLPs).
Consistently with the previous naming convention, $\vh_i$ and $\vp_i$ indicate node features and position respectively.
The layer is inspired by PointNet~\citep{qi2017pointnet}, but uses two distinct operators for processing the representation of the central node and computing messages coming from neighbors. In particular, $g_s$ can be seen as implementing a parametrized skip connection to mitigate over-smoothing node representations~\citep{li2019deepgcns}.
The final node encodings are obtained by stacking blocks of the form
\begin{equation}
    \label{eq:pose_conv_layer}
    \gG_{\mH'\|\mP} = \textsc{BN}\left(\textsc{ReLU}\left(\textsc{PConv}\left( \gG_{\mH\|\mP}\right)\right)\right),
\end{equation}
which is a common way of chaining processing layers in deep neural networks. In particular, \textsc{BN} denotes \textit{batch normalization}~\citep{ioffe2015batch}, \textsc{ReLU} is the activation function, \textsc{PConv} is our pose convolution layer and $\gG_{\mH\|\mP}$ a pose graph with features $\mH$ and node coordinates $\mP$.

\subsection{Implementing the mask generator \texorpdfstring{$\mu_\theta$}{Mu theta}} 
The \textit{mask generator}~($\mu_\theta$) network consists of a first block of stacked pose convolutional layers analogous to the ones in Equation~\eqref{eq:pose_conv_layer}. The outputs of these layers are then reshaped into bi-dimensional matrices used as input for the second stack of layers consisting of a combination of shared 2D convolution blocks, interleaved with relational \textit{pose convolution 2D} layers. Such layers implement the following message-passing function:
\begin{equation}
    \label{eq:poseconv2d}
    \begin{split}
        \mO_i &= \sum_{j \in \mathcal{N}\left(i\right)} g_o\left(\mH_i, \mH_j\right)\\
        \mH_{i}' &= g_s\left(\mH_i\right) + \sigma\left(\mO_i\right) \odot g_g\left(\mO_i\right)
    \end{split}
\end{equation}
where, $\odot$ denotes the Hadamard product, $\mathcal{N}\left(i\right)$ is the set of neighbors of node $i$ and $g_s$, $g_l$ and $g_g$ can be any learnable function that has two-dimensional inputs and outputs.

In our case,  $g_g$ is a 2D convolutional layer while $g_o$ is a convolutional block with upsampling and skip-connection and $g_s$ is a linear upsampling operation followed by a 2D convolution, where the upsampling is needed to match the dimensions of the output of $g_o$.
The rationale behind the design of Equation~\eqref{eq:poseconv2d} is promoting heterogeneity between the masks generated by different nodes. Notably, $g_s$ can act as a skip connection preserving the node representations while the gating operation allows for selectively aggregating information coming from neighbors.
Indeed, over-smoothing the node features would be particularly detrimental as it would compromise the locality of the masks learned w.r.t.\ each node.
Said property is desirable as, per Equation~\eqref{eq:multichannel-mask} this would in turn make the learned node features, $\vf_i$, localized w.r.t.\ their spatial neighborhood. Note that these soft constraints, i.e., architectural biases, can be seen as a form of regularization aligned with object-centric generation tasks.

\subsection{Surrogate Task} \label{subsec:Surrogate Task}
As mentioned previously, we want to pre-train GraPhOSE, in particular $\mu_\theta$, on the generic surrogate task of mapping a pose graph to a 2D mask.
For this purpose, we define the surrogate mask associated with a pose graph $\gG$, as shown in Figure~\ref{fig:surrogate}, which, intuitively, is a grayscale image that depicts the structure of the graph.

The mask for the whole graph is obtained by composing partial masks relative to each node and edge.
In particular, given pixel coordinates $c$, we define the value of each $i$-th partial surrogate mask at that pixel as
\begin{equation}
    \label{eq:surrogate_mask_node}
    \textsc{S}_{\mN,i}\left(\vc\right) = \exp\left(-\frac{\left\|\vp_i - \vc\right\|_2^2}{2\sigma^2}\right),
\end{equation}
and, analogously, the value of the partial surrogate mask associated with each $e_{ij}$ edge as
\begin{equation}
    \label{eq:surrogate_mask_edge}
    \textsc{S}_{\mE,ij}\left(\vc\right) = \sqrt{\frac{\exp\left(-d_{ij}(\vc)^T\cdot \mT_{ij}^{-1}\cdot d_{ij}(\vc)\right)}{\left(2\pi\right)^2\cdot\det\left(\mT_{ij}\right)}},
\end{equation}
where $\mT_{ij}$ denotes a $2\times 2$ rotation and scaling matrix dependent on the length and orientation of the segment connecting the $i$-th and $j$-th nodes~(see the supplementary material for the details). Conversely, $d_{ij}(\vc)$ is defined as
\begin{equation}
    d_{ij}(\vc) = \vc - \left(\frac{\vp_i + \vp_j}{2}\right)
\end{equation}
and denotes the distance between $\vc$ and the midpoint between the coordinates of nodes $i$ and $j$.

The final surrogate mask is obtained, for each pixel $c$, as
\begin{equation} \label{eq:surrogate_final}
    \textsc{S}_{\gG}\left(\vc\right) = \sum_{i \in \gG} \textsc{S}_{\mN, i}\left(\vc\right) + \sum_{\langle i, j \rangle \in \gG} \textsc{S}_{\mE, i, j}\left(\vc\right)
\end{equation}
which is the pixel-wise sum of the masks associated with each node and edge. All the values are then clipped between $0$ and $1$.
More details about the computation of the mask can be found in Appendix~\ref{subapp:surrogate_mask}.
Intuitively, the mask corresponding to a node is obtained by considering an isotropic bi-variate gaussian centered into the node's coordinates and with standard deviation $\sigma$. 
The mask corresponding to an edge, instead, is obtained by considering a bi-variate gaussian centered w.r.t.\ the midpoint between the edge's vertices, and a covariance matrix dependent on the distance between the two points and the orientation of the line joining them.
The surrogate mask obtained in such a way is agnostic w.r.t.\ the object represented by the graph and hence general. Note that differently from, e.g., segmentation masks, the mask we are referring to depends entirely on the structure of the objects in the image being generated and not on their class.

As a final remark, the surrogate mask has a lattice structure, which may not properly resemble the desired mask for all target tasks; indeed, depending on the specific object, some loops should be filled and the silhouette of each specific part refined.
Nonetheless, such surrogate masks are helpful in providing supervision for the pre-training routine and act as a regularization for the model.
Details on the fine-tuning procedure on downstream tasks are discussed in Section~\ref{subsec:mask_adaptation}.
The pre-training routine is carried out by minimizing a surrogate loss $\mathcal{L}_{re}$ based on a reconstruction error, e.g., mean squared error or binary cross entropy.

\section{Related Works} \label{sec:Related Work}

Image generation is a popular application of deep learning models from \textit{generative adversarial networks} (GANs)~\citep{goodfellow2014generative,gui2021review}, to \textit{variational autoencoders} (VAEs)~\citep{kingma2013auto,vahdat2020nvae,park2020swapping} and, more recently, \textit{diffusion models}~\citep{sohl2015deep, song2019generative, ho2020denoising}. 
Concurrently, many researchers have explored ways of conditioning the generation of such images, from simple class labels~\citep{mirza2014conditional, brock2018large}, to fully articulated sentences~\citep{ramesh2022hierarchical, rombach2022high} or even other images~\citep{isola2017image, liu2017unsupervised, CycleGAN2017}.
Although no previous work directly exploits pose graphs to guide image generation, several approaches focused on \textit{scene-graph-conditioned image generation} and \textit{pose-conditioned image generation}.
Even though these tasks present some similarities with our work, they are fundamentally different, as discussed in the remainder of this section.
\paragraph{Scene-Graph-Conditioned Image Generation} Scene graphs are a way of representing a visual scene context as a graph linking object nodes through predicate edges. 
These graphs are obtained by parsing natural language sentences (e.g., the phrase \textit{A man sits on a bench} turns into a graph with two nodes with attributes \textit{man} and \textit{bench}, respectively, linked by an edge with attribute \textit{to sit}), making these approaches similar to text-based image generation.
The first method for conditioning image generation on scene graphs was introduced by~\citet{johnson2018image}, which propose a GNN mapping the scene graph into a scene layout used to bias a generator network. 
\citet{ivgi2021scene} improved such an approach by designing a GNN that directly operates on 2D feature maps.
Differently from the above methods, however, our formulation represents objects as graphs of related parts, with both structural and semantic information; scene graphs contain only semantics and represent one object per node, as they are derived from natural language parsing.
This makes such methods unsuitable for our problem setting.
\paragraph{Pose-Conditioned Image Generation} Several prior works condition image generation on the pose of the objects being generated. 
In particular, several methods deal with images from human poses, with \citet{ma2017pose} introducing the first deep learning approach based on conditioning a reference image on a target pose.
The method introduced by \citet{ma2017pose} has been thereafter extended by implementing deformable convolutions~\citep{siarohin2018deformable}, adopting dense poses~\citep{neverova2018dense} and global style attributes~\citep{men2020controllable} as conditioning. Moreover, it has been used to address the problem of person re-identification~\citep{qian2018pose}.
\citet{horiuchi2021differentiable} exploits a graph-based representation for the pose paired with a reference image encoding semantics.
To the best of our knowledge, none of the previous works jointly encode pose and semantics information in a graph and then use such graph as an architectural bias for the neural processing. Exploiting this previously unused relational information allows our method to compare favorably against the state of the art w.r.t. flexibility, generality, and effectiveness.
Moreover, note that this family of techniques solves an image-to-image task, conditioned on the pose (i.e., given a person's image as input, the model outputs an image with the same context, where the person matches the target pose).
This is a very different problem w.r.t. to the graph-to-image task we are tackling. 
While a direct comparison would not be meaningful, classical pose-conditioned approaches can be better suited to tasks where the conditioning is a template image that needs to be modified or augmented.


\section{Experiments} \label{sec:Experiments}
We start the empirical validation of the proposed method by focusing on the pre-training of the mask generator $\mu_\theta$ with surrogate masks created from procedurally generated graphs.
Then, we introduce relevant baselines for our model and compare them on image generation tasks w.r.t. both synthetic and real-world data.
We carried out our experiments with a GAN, even though the same principles can be extended to work with other methodologies.
For this, noise is sampled at the input graph level, such that, $\mV = \left(\mP \| \mX \| \mZ\right)$ with $\mZ \in \mathbb{R}^{N\times Z} \sim~\mathcal{N}\left(0,1\right)$.
The code used to run the experiments is provided in the supplementary materials.

\subsection{Surrogate Pre-Training} \label{subsec:pretrain}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{mask_sample.png}
\end{center}
\caption{Sample of generated masks. For each one, the large figure is the aggregated mask, while the small ones are those associated with each node. The blue dots highlight the position of the accounted node. (a) is a random graph like those used for pre-training; (b) and (c) are simple handcrafted ones.}
\label{fig:sample_masks}
\end{figure}

For surrogate pre-training, we generate random graphs using the \textit{Barbasi-Albert}~\citep{albert2002statistical} or a \textit{Erdos-Renyi}~\citep{erdds1959random, gilbert1959random} model, with a number of nodes in the interval $\left[5, 30\right]$.
The position of each node is determined by using \textit{Kamada-Kawai}'s force-directed graph drawing algorithm~\citep{Kamada1989}.
These choices aim at generating (almost planar) graphs that can easily be drawn on the plane, i.e., with only a few intersections among edges.
More detailed information on the setup of the generators are provided in the supplementary materials.
Target masks are computed as in Equation~\eqref{eq:surrogate_final} and the model is fitted to minimize the \textit{binary cross entropy} (BCE) loss.
Figure~\ref{fig:sample_masks} shows sample results of the mask generation process. 
Each one of the larger images corresponds to the full mask generated from the input graph~(shown in blue and red), while the smaller images show masks relative to each node (in blue).
The model produces masks that match the topology of the graph, even for complex structures.
More interestingly, results show that the proposed architecture is indeed capable of generating localized node-level masks that capture the structure of the neighborhood of each node.
By looking at samples (b) and (d), which contain handcrafted graphs, we see that this property is preserved for graphs with a small diameter; even though, nodes with high connectivity tend to produce richer masks that may span it completely.
Indeed, locality is preserved against over-smoothing which is typical of isotropic graph convolutional filters; conversely, the learned node-level masks are diverse and properly localized.
\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{rand_norand.png}
\end{center}
\caption{Masks generated by pre-training on random graphs (a) and on the Humans task's ones (b). For each group, the first two columns are samples from the Humans task, last two are random. In (b), performance clearly degrades out of distribution.}
\label{fig:rand_norand_mask}
\end{figure}
Figure~\ref{fig:rand_norand_mask} additionally shows how pre-training on randomly generated graphs can act as regularization by yielding a model able to perform properly for graphs outside the distribution of the downstream task.

\subsection{Baselines} \label{subsec:baselines}
Comparing existing models to ours is not straightforward. As detailed in Section~\ref{sec:Related Work}, these methods lack flexibility concerning the objects they can generate and the representation of structural and semantic constraints. 
Moreover, the tasks they address fundamentally differ from ours, as they rely on different inputs, and thus they cannot be directly compared to our approach. 
As described in Section~\ref{sec:Method}, GraPhOSE learns to map the graph into a layout mask by learning both node representations and node masks. We thus compare it to two baselines: both use a fixed~(non-learnable) mapping to generate the node masks; the first (GNN Conditioner) uses a GNN-based encoder to learn node representations, while the second (FNN Conditioner) uses a feedforward neural network~(FNN) based one (i.e., it does not use relational information).
Considering Equation~\eqref{eq:generative_model}, we can contextually schematize the baselines as computing mask $\tM$ as
\begin{equation} \label{eq:fixed_masks}
    \tM = \|_{i \in \gV}\bigl(\textsc{S}_{\mN, i}\left(\mC\right) + \sum_{j \in \gN(i)}\textsc{S}_{\mE,ij}\left(\mC\right)\bigr)
\end{equation} \label{eq:non_relational_encoder}
and, for the FNN-based one only, as computing $\mF$ as
\begin{equation}
    \mF = \Psi_{\psi}\left(\mV\right)
\end{equation}
where $\|_{i \in \gV}({}\cdot{})$ indicates concatenation along a new axis, while, for ease of notation, $\textsc{S}_{\mN,i}\left(\mC\right)$ and $\textsc{S}_{\mE,ij}\left(\mC\right)$ denote operations applied over all the pixel coordinates w.r.t.\ nodes and edges, respectively.
Note that the FNN Conditioner has still access to structural information through the fixed masks which makes the comparison against GraPhOSE meaningful. 
For all the baselines~(including GraPhOSE), we use the same downstream model based on a simple stack of layers with 2D Convolutions, batch normalization~\citep{ioffe2015batch} and ReLU activation and residual connections~\citep{he2016deep}.
Moreover, similarly to~\citet{johnson2018image}, each layer may take as input both the previous layers's output and the downsampled layout mask to condition the generative process at the different processing steps. 
The discriminator is based on similar building blocks and receives the graph as input, together with the real/fake image.
See Appendix~\ref{subapp:architectures} for more details on the implementation for each baseline.
For GraPhOSE we pre-train the mask generator as described in Section~\ref{subsec:pretrain}, and then train the whole model on the downstream task.
Note that, during the end-to-end training, we drop the reconstruction loss originally used during the pre-training. This is to allow for adapting the masking mechanism to the downstream task, removing any bias coming from the surrogate loss.
All the baselines were trained under the same settings; more details can be found in Appendix~\ref{subapp:parameters}.

\subsection{Datasets} \label{subsec:datasets}
As previously mentioned we consider $2$ benchmarks based on synthetic and real-world data.
We explicitly design the synthetic dataset, named PRO, to highlight the benefits of object-centric relational representations. PRO consists of images containing simple stylized objects that can be rendered starting from a pose representation in which each keypoint has class and style attributes, i.e.,  from a relational description of the image. Different objects are represented at different degrees of abstraction w.r.t.\ their appearance to make the task of generating objects, with a coherent style, from the representation more challenging. 
The style and visual appearance of the different rendered components vary smoothly with respect to the structure of each object (as it is often the case in the real world), making the adoption of appropriate inductive biases particularly appealing. For our experiments, we generated $100000$ samples.
For what concerns the second task, referred to as \textit{Humans}, it consists of the problem of generating images of humans from key-points with class attributes (e.g., ankle, shoulder). To build a dataset of examples, we leveraged the existing MPII Human Pose~\citep{andriluka14cvpr}, Market 1501~\citep{zheng2015scalable} and DeepFashion~\citep{liuLQWTcvpr16DeepFashion} datasets.
For the DeepFashion dataset, we use just the Fashion Landmark Detection and the In-Shop Retrieval subsets.
The keypoint annotations for DeepFashion In-Shop Retrieval and Market $1501$ are based on~\citet{zhu2019progressive}, and were generated by using the open-source software OpenPose~\citep{openpose}.
For DeepFashion Fashion Landmark we instead rely on MediaPipe's BlazePose~\citep{mediapipe}, while MPII Human Pose already contains pose features.
All annotations are reduced to the COCO keypoint standard~\citep{coco} and used to generate the corresponding graphs with node coordinates normalized between $0$ and $1$, resulting in a dataset of roughly $300000$ samples.
Images from the real-world datasets have been downscaled~(or padded) to a $64 \times 64$ shape.
Images in PRO are generated with size $128 \times 128$ to ensure that objects are not too small. 
Further details on the datasets are provided in Appendix~\ref{subapp:PRO} and \ref{subapp:Humans}.

\section{Results} \label{sec:Results}

\begin{figure}[t]
\begin{subfigure}{.475\linewidth}
\includegraphics[width=\linewidth]{objects_model_comparison.png}
\caption{PRO}
\label{fig:objects_comparison}
\end{subfigure}\hfill
\begin{subfigure}{.475\linewidth}
\includegraphics[width=\linewidth]{human_model_comparison.png}
\caption{Humans}
\label{fig:human_comparison}
\end{subfigure}
\caption{Sample results for PRO (left) and Humans (right) tasks. Row (a) is the input graph. Generated samples come from: the PRO exact renderer (b - left), the unconditioned downstream model (b - right), the FNN-based baseline (c), the GNN-based one (d), GraPhOSE without pre-training (e), GraPhOSE (f).}
\label{fig:comparison}
\end{figure}

In the following section, we discuss the results obtained on the synthetic and real-world datasets. On the former, we used a small downstream model with around $28K$ parameters, while, on the latter, a more complex one, with roughly $5M$ parameters, was employed; details are provided in the appendix. Moreover, for the synthetic dataset, models that leverage relational representations receive inputs with $10\%$ of the semantic node attributes randomly masked in $50\%$ of the samples; we do so to show that structures can be used to generate coherent images from partial conditioning.
Table~\ref{tab:metrics} shows the Frechet Inception Distance (FID)~\citep{fid2017} and Inception Score (IS)~\citep{salimans2016improved} for our model and the baselines w.r.t.\ both datasets, together with the Structural Similarity Index Measure (SSIM)~\citep{wang2004image} for the PRO dataset. GraPhOSE achieves a significantly better FID and SSIM score compared to the baselines. Indeed, in the PRO dataset, GraPhOSE outperforms the baseline both with and without pre-training for the FID measure. 
Note that SSIM measures the accuracy of image reconstruction, making it a meaningful metric only for the PRO dataset, where the input graph fully describes the target image.
For the human pose dataset, pre-training acts as regularization allowing for reducing the variance among runs. As expected, the IS, which evaluates the distinctiveness and diversity of the generated samples, does not highlight significant differences among the models, as these properties are not highly influenced by their architectural differences.
We can then assess compliance w.r.t.\ the conditioning visually, in a qualitative manner. 

\input{table_metrics}

In this regard, Figures~\labelcref{fig:objects_comparison,fig:human_comparison}, show results generated, by the compared models, for the two tasks.
We can see that, in PRO, GraPhOSE, row (f), is clearly superior to the other baselines, in particular when the surrogate mask further deviates from the object's visual appearance. The generation quality is consistent across different numbers of objects and scales, showing the approach is suitable to handle a wide range of possible scenes.
Moreover, the FNN Conditioner is not capable of handling missing attributes or nodes, which hinders its flexibility, in particular for real-world settings.
For Humans, we again see that images generated by GraPhOSE have better visual appeal and compliance with the input pose, even when not all the key-points are present in the conditioning graph.
Furthermore, notice how images from the FNN Conditioner, row (c), are visually worse than those from models with a higher~(worse) FID score.
In general, relational representation plays a significant role in guiding the generation toward a coherent result, as structure alone, i.e., even without semantics, can be leveraged to characterize silhouettes, and neighboring information can help reconstruct missing features. Examples of this behavior are shown in Appendix~\ref{subapp:missing}.
Note that the generative models used in this work are purposely simple, as our experiments focus on the conditioning capabilities rather than image quality~(i.e., detail, realism).
The resulting image quality is comparable to that of architectures that use similar generators~\citep{johnson2018image,yan2016attribute2image}. Images of better quality can be obtained at the expense of increased computational and sample complexity by adopting more complex generators trained at higher resolutions.

\subsection{Task specific mask adaptation} \label{subsec:mask_adaptation}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{objects_mask_adaptation.png}
\end{center}
\caption{Masks generated by GraPhOSE after pre-training (b) and after subsequent training on the downstream task (c). (d), shows those generated by skipping pre-training, while (a) is the input graph}
\label{fig:objects_mask_adapt}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{submasks_condensed.png}
\end{center}
\caption{Samples of individual node masks generated from GraPhOSE. The first column is the cumulative mask for the whole input graph (superimposed with nodes in blue and edges in red). Individual node masks have the reference node highlighted in blue. Hand masks (a) are only a sub-sample for ease of visualization.}
\label{fig:submasks_condensed}
\end{figure}

The purpose of pre-training on surrogate masks of random pose graphs is to learn a general mapping between the graph and the image representation of the structure it entails.
However, this learned mapping might not produce the exact visual properties desirable for a specific downstream task.
We wish to adapt to these properties while learning the target task, during the end-to-end training.
Figure~\ref{fig:objects_mask_adapt} shows how this indeed happens for the masks being generated before~(b) and after~(c) training on a target task.
This adaptability is particularly relevant, as it shows that after pre-training the mask generator towards the correct direction, we can have it learn the specifics of the target downstream task.
Differently, the images generated by training GraPhOSE without pre-training on surrogate masks~(d) do not visually resemble the input pose.
These results suggest the lack of structure in the mask leads to degradation of the performance, in particular when the target data distribution is not heavily biased towards a few poses~(e.g., Humans contains mostly poses of people standing).
Note that, even without pre-training, the model still leverages the relational inductive biases encoded by the input graphs and accounted for by our model architecture. 
This shows the usefulness of pre-training as a soft regularization, which localizes node features w.r.t.\ corresponding portions of the image. Further examples of this property are shown in the appendix.
Figure~\ref{fig:submasks_condensed} shows how individual node masks preserve a property of locality after training on the downstream task, even though no particular constraint, aside from architectural biases, enforces this. 
These results are really meaningful, as localized masks allow, by GraPhOSE's design, for better local conditioning on node level semantics.
Moreover, it entails the learning of a distributed representation, where each node describes a specific part, rather than a collapsed representation in which all nodes contain a representation of the whole object. Further examples of this node-mask locality can be found in Appendix~\ref{subapp:mask_adaptation}.

\subsection{Structure and style sensitivity analysis} \label{subsec:sensitivity_analysis}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{move_pose.png}
\end{center}
\caption{Results (b) of changing input nodes' position (a) while maintaining other inputs constant.}
\label{fig:move_pose}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.8\linewidth]{change_semantics.png}
\end{center}
\caption{Results of changing the input nodes' semantic attributes (i.e. color), for samples of the PRO dataset. The input graph, with according colored nodes, is superimposed onto the generated image. Nodes belonging to the same finger change color in group as otherwise the input graph would be out of distribution w.r.t. training data.
}
\label{fig:change_semantics}
\end{figure}

To assess whether the model is able to disentangle structure and style, we experiment with providing it a series of human graphs with slightly different poses while fixing all the remaining attributes of the conditioning.
As shown in Figure~\ref{fig:move_pose}, generated figures are rearranged according to the changes in the input pose while the style is mostly unchanged.
Note that, however, in case of more significant changes in the pose, the resulting images can be visually different. This emergent property is nonetheless interesting and provides ground for future research. 
Similarly, we assess the sensitivity of the method to incremental perturbations of the semantic node attributes for samples of the PRO dataset.
Example results are shown in Figure~\ref{fig:change_semantics}.
We can see that the generated object's structure is not affected by the semantic changes.
Only the colors change according to the changes in node attributes.
This further highlights the disentanglement between structural and style control over the generated images.

\section{Conclusions and Future Work} \label{sec:Conclusions and Future Work}
We propose GraPhOSE, a method to exploit attributed graphs as object-centric relational inductive biases to flexibly condition image generation.
Our approach has several advantages compared to standard approaches which result in a scalable and flexible framework. Notably, we shone a light on the properties of relational representation in this context, by showing how they can be used to regularize and manipulate the generative model.
We also provided a novel synthetic dataset that pairs images with their object-centric relational representation, introducing a benchmark to support further studies.
Future research might target approaches to generate meaningful and coherent masks without relying on pre-training, or investigate methods to disentangle the different elements that contribute to the final generated image.
We argue that our study has the potential of sparking new interest in object-centric relational representations for generative modeling and that the results presented here constitute the first building block for future research in this direction.

\subsubsection*{Acknowledgments}
The authors wish to thank \textit{Daniele Zambon} for the helpful suggestions provided during the development of this work.