\section{Implementation Details} \label{app:Details}

In the following we clarify some details on the settings used to train the models, on the specifics of some models' architectures, as well as further details on surrogate masks and the hardware and software we used to run the experiments.

\subsection{Hardware and Software} \label{subapp:hardware_software}

The code to reproduce the experiments and generate the PRO dataset is available online\footnote{\url{https://github.com/LucaButera/graphose_ocrrig}}. 
It is all written in \textit{Python 3.9}, leveraging \textit{Pytorch}~\citep{paszke2019pytorch} and \textit{Pytorch Lightning}~\citep{falcon2019lightning} to define models and data, while \textit{Hydra}~\citep{Yadan2019Hydra} is used to manage the experiment configuration.
\textit{Weights \& Biases}~\citep{wandb} is used to log and compare experiment results.
We run all the experiments on an \textit{NVIDIA RTX A5000} GPU equipped with 24GBs of VRAM.

\subsection{Training settings} \label{subapp:parameters}

Each model is trained under the same settings and metrics are computed over three different runs each.
In particular, we use the \textit{Adam}~\citep{kingma2014adam} optimizer, with \textit{cosine annealing}~\citep{loshchilov2017sgdr} learning rate schedule with period $300$ for the PRO dataset and $150$ for the Humans one, starting learning rate $0.002$ and final learning rate $0.00002$.
Models are trained up to $300$ (PRO) or $150$ (Humans) epochs each, with a batch size of $64$ and early stopping with patience $50$ on the FID.
Notice that the learning rate for the parameters of the pre-trained mask generator used in GraPhOSE was reduced to $\frac{1}{2}$ (PRO) or $\frac{1}{100}$ (Humans) with respect to the learning rate of the other parameters, in order to avoid catastrophic foregetting at the beginning of training.
This procedure could be enhanced by slowly raising the learning rate of the pre-trained parameters back to the value of other parameters, as training epochs go by, with the aim of favoring mask adaptation after the training has stabilized during the first epochs. The reduction was much stronger for Humans in order to counter balance the heavy bias the dataset had towards poses of people standing up with arms straight down. However, this reduction factor is indeed an hyper-parameter that can be effectively used to allow for more or less deviation from the surrogate masks, depending on the downstream task characteristics.

\subsection{Surrogate Mask} \label{subapp:surrogate_mask}
Referring to Equation~\eqref{eq:surrogate_mask_edge}, we explicitly define $\mT_{ij}$ as

\begin{equation} \label{eq:rotoscale}
    \begin{split}
        d_{ij} &= \frac{\| \vp_i - \vp_j \|^2_2}{4}\\
        \alpha_{ij} &= \arctan2\left(\vp_j - \vp_i\right)\\
        \evt_{a,ij} &= d_{ij}\cdot \cos\left(\alpha_{ij}\right)^2 + \frac{d_{ij}}{a^2}\cdot \sin\left(\alpha_{ij}\right)^2\\
        \evt_{b,ij} &= d_{ij}\cdot \sin\left(\alpha_{ij}\right)^2 + \frac{d_{ij}}{a^2}\cdot \cos\left(\alpha_{ij}\right)^2\\
        \evt_{c,ij} &= \left(d_{ij} - \frac{d_{ij}}{a^2}\right) \cdot \sin\left(\alpha_{ij}\right) \cdot \cos\left(\alpha_{ij}\right)\\
        \mT_{ij} &= 
        \begin{bmatrix}
            \evt_{a,ij}&\evt_{c,ij}\\
            \evt_{c,ij}&\evt_{b,ij}
        \end{bmatrix},
    \end{split}
\end{equation}
where vectors $\vp$ denote node coordinates, $\arctan2$ is the element-wise 2-argument arctangent and $a$ denotes a parameter that regulates the scaling ratio of the second dimension w.r.t.\ the first one.
In our experiments $a$ is set to $10$.

\subsection{Random Graphs for Pre-Training}
Referring to Section~\ref{subsec:pretrain}, we set the \textit{Barbasi-Albert} model's number of edges for new nodes parameter to $1$ in $90\%$ of the cases and to $2$ in the remaining $10\%$, if the number of nodes is less than $10$. For graphs with more than $10$ nodes, it is always set to $1$. \textit{Erdos-Renyi} model's edge probability is set to $\exp\left(\frac{1}{n}\right) - 0.95$, where $n$ is the number of nodes.

\subsection{PRO Dataset} \label{subapp:PRO}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.8\linewidth]{objects_sample.png}
\end{center}
\caption{Examples of objects present in the PRO dataset. From left to right: Pie, Scissors, Hand, Robotic Arm, Hollow Polygon, Filled Polygon, Lattice}
\label{fig:objects}
\end{figure}

Our synthetic \textit{Pose-Representable Objects (PRO)} dataset consists of square images (i.e., with same height and width) rendered from random graphs that represent certain object's parts and their color. A graph representing an object has a fixed connectivity (i.e., adjacency matrix) and each node has color (from a pool of possible ones) and class attributes together with a coordinate in 2D space. Coordinates and colors, for each node in a sample, are generated randomly within reasonable boundaries (e.g. to stay within the image or to comply to a specific structure, like that of a regular polygon).
Figure~\ref{fig:objects} shows some examples of each object type in our dataset; the graph that describes the object has the edges superimposed in white, while the nodes are colored accordingly to their attributes.
Images from the \textit{PRO} dataset can contain multiple objects, however we experimented with up to four objects for image, in order to keep their size reasonably big.
Regarding the objects' structural constraints, we highlight the following properties: \textit{Pie} has an inner angle of up to $135^{\circ}$, as the two pie tips have the same class and it would not be possible to discriminate which part of the pie has to be filled otherwise. Pie nodes have always the same color.
\textit{Scissors} have the structural constraint that the angle between the blades and the handles must be the same and be contained between $30^{\circ}$ and $90^{\circ}$. Also blades and handles must respectively have the same color.
For the \textit{Hand} object we have constraints on the maximum relative angle between each phalanx, in order to avoid impossible orientations; moreover, fingers must have a consistent color. Hands can uniformly be either "right" or "left" hands.
The \textit{Robotic Arm}, similar to the Hand, has constraints on the relative angles between its parts, to similarly avoid collapsed configurations. Additionally, the number of segments in the arm can vary between $3$ and $7$ and their color must not change. Similarly, also the prongs must have a consistent coloring.
\textit{Hollow Polygon} and \textit{Filled Polygon} must always have all nodes of the same color and the presence of a "center" node determines if the polygon is filled or not. We experiment with polygons up to $8$ vertices.
The \textit{Lattice} is the only structure with randomized connectivity, as nodes, uniformly distributed between $3$ and $9$, are sampled from a Poisson Disk distribution~\citep{cook1986stochastic} and are connected if their distance is less than $20\%$ of the whole image width. The lattice is also the only object in which each node has a random color and the segment connecting two nodes is colored with a linear gradient going from one node's color to the other's.

\begin{table}[t]
\caption{Node classes for each object.}
\label{tab:object_node_classes}
\begin{center}
\begin{tabular}{|c|c|}
\hline
{\sc Object} & {\sc Classes}\\
\hline\hline
Pie & center, tip\\
Scissors & pivot, blade, handle\\
Hand & wrist, finger\\
Robotic Arm & base, arm, prong\\
Hollow Polygon & vertex\\
Filled Polygon & vertex, p\_center\\
Lattice & l\_vertex\\
\hline
\end{tabular}
\end{center}
\end{table}

Table~\ref{tab:object_node_classes} contains a summary of the classes for each object's node.
We would like to underline that the presence of structure in the assignment of colors and classes to object nodes is not just to make them visually consistent, but is fundamental in order to make it possible to reconstruct missing attributes by using relational information and acquired knowledge on how said structure behaves.

Regarding dataset size, in our experiments we generated $100000$ images. 
We employed $1000$ of these as validation set, and another $1000$ as test set.
Images were sampled so that the dataset contains a uniform distribution of different objects.
Moreover, also the number of objects that appear in an image at once, between $1$ and $4$, is uniformly distributed across the dataset.
Train, validation and test splits, while random, preserved these uniform distributions.

\subsection{Humans Dataset} \label{subapp:Humans}

The Humans dataset consists of roughly $300000$ images, coming from MPII Human Pose~\citep{andriluka14cvpr}, Deep Fashion~\citep{liuLQWTcvpr16DeepFashion} and Market $1501$~\citep{zheng2015scalable}, which are all widely adopted benchmarks for pose estimation and other human body related tasks.
Out of these, around $3000$ images, were used for validation and an analogous size for testing. 
Train, validation, and test sets were partitioned at random but the proportion of samples from the $3$ original benchmarks was maintained. 
Within the Humans task, the only object type is \textit{Person}, while there are $14$ node classes (left and right shoulders, elbows, wrists, ankles, knees, hips plus base of the neck and top of the head).
Note that, differently from the PRO dataset, these real-world images contain different backgrounds and lighting conditions.
We do not address the explicit conditioning over these properties in our model, however these could be incorporated via a vector representation that acts as a global graph attribute.

\subsection{Architectures} \label{subapp:architectures}

\begin{table}[t]
\caption{The Encoder~$\Phi_{\phi}$.}
\label{tab:encoder}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
{\sc Layer} & {\sc H-params} & {\sc Input} & {\sc Output}\\
\hline\hline
MultiEmbedding & $8,cat$ & $\mX$ & $\mO$\\
PoseConv & $8,8,8$ & $\mO\|\mZ,\mP,\mA$ & $\mO$\\
BatchNorm & & $\mO$ & $\mO$\\
ReLU & & $\mO$ & $\mO$\\
PoseConv & $8,16,16$ & $\mO,\mP,\mA$ & $\mO$\\
BatchNorm & & $\mO$ & $\mO$\\
ReLU & & $\mO$ & $\mO$\\
PoseConv & $16,32,32$ & $\mO,\mP,\mA$ & $\mO$\\
Sigmoid & & $\mO$ & $\mO$\\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\caption{The FNN-based Encoder~$\Psi_{\psi}$.}
\label{tab:non_relational_encoder}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
{\sc Layer} & {\sc H-params} & {\sc Input} & {\sc Output}\\
\hline\hline
MultiEmbedding & $8,cat$ & $\mX$ & $\mO$\\
FNNPoseConv & $8,8$ & $\mO\|\mZ,\mP,\mA$ & $\mO$\\
BatchNorm & & $\mO$ & $\mO$\\
ReLU & & $\mO$ & $\mO$\\
FNNPoseConv & $8,16$ & $\mO,\mP,\mA$ & $\mO$\\
BatchNorm & & $\mO$ & $\mO$\\
ReLU & & $\mO$ & $\mO$\\
FNNPoseConv & $16,32$ & $\mO,\mP,\mA$ & $\mO$\\
Sigmoid & & $\mO$ & $\mO$\\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\caption{The Mask Generator~$\mu_{\theta}$.}
\label{tab:mask_generator}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
{\sc Layer} & {\sc H-params} & {\sc Input} & {\sc Output}\\
\hline\hline
PoseConv & $3,8,8$ & $\mP\|\mZ,\mP,\mA$ & $\mO$\\
BatchNorm & & $\mO$ & $\mO$\\
ReLU & & $\mO$ & $\mO$\\
PoseConv & $8,32,32$ & $\mO,\mP,\mA$ & $\mO$\\
BatchNorm & & $\mO$ & $\mO$\\
ReLU & & $\mO$ & $\mO$\\
PoseConv & $32,128,128$ & $\mO,\mP,\mA$ & $\mO$\\
Reshape & $128,[8, 4, 4]$ & $\mO$ & $\tO$\\
PoseConv2D & $8,16$ & $\tO,\mA$ & $\tO$\\
ConvBlock2D & $16,32,2$ & $\tO$ & $\tO$\\
PoseConv2D & $32,16$ & $\tO,\mA$ & $\tO$\\
PoseConv2D & $16,8$ & $\tO,\mA$ & $\tO$\\
BatchNorm2D & & $\tO$ & $\tO$\\
ReLU & & $\tO$ & $\tO$\\
Conv2D & $8,1$ & $\tO$ & $\mO$\\
Sigmoid & & $\mO$ & $\mO$\\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\caption{The Downstream Generator.}
\label{tab:downstream_generator}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
{\sc Layer} & {\sc H-params} & {\sc Input} & {\sc Output}\\
\hline\hline
ConvBlock2D & $32,32,1$ & $\tL$ & $\tO$\\
ConvBlock2D & $32,32,1$ & $\tO$ & $\tO$\\
ConvBlock2D & $32,16,1$ & $\tO$ & $\tO$\\
ConvBlock2D & $16,16,1$ & $\tO$ & $\tO$\\
ConvBlock2D & $16,8,1$ & $\tO$ & $\tO$\\
BatchNorm2D & & $\tO$ & $\tO$\\
ReLU & & $\tO$ & $\tO$\\
Conv2D & $8,3$ & $\tO$ & $\mO$\\
Tanh & & $\mO$ & $\mO$\\
\hline
\end{tabular}
\end{center}
\end{table}


\begin{table}[t]
\caption{The Downstream Discriminator.}
\label{tab:downstream_discriminator}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
{\sc Layer} & {\sc H-params} & {\sc Input} & {\sc Output}\\
\hline\hline
MultiEmbedding & $8,cat$ & $\mX$ & $\mO_1$\\
PoseConv & $8,8,8$ & $\mO_1,\mP,\mA$ & $\mO_1$\\
BatchNorm & & $\mO_1$ & $\mO_1$\\
ReLU & & $\mO_1$ & $\mO_1$\\
PoseConv & $8,16,16$ & $\mO_1,\mP,\mA$ & $\mO_1$\\
BatchNorm & & $\mO_1$ & $\mO_1$\\
ReLU & & $\mO_1$ & $\mO_1$\\
PoseConv & $16,32,32$ & $\mO_1,\mP,\mA$ & $\mO_1$\\
DConvBlock2D & $3,8,2$ & $\tI$ & $\tO_2$\\
DConvBlock2D & $8,16,2$ & $\tO_2$ & $\tO_2$\\
DConvBlock2D & $16,16,2$ & $\tO_2$ & $\tO_2$\\
DConvBlock2D & $16,32,2$ & $\tO_2$ & $\tO_2$\\
DConvBlock2D & $32,32,2$ & $\tO_2$ & $\tO_2$\\
ReLU & & $\tO_2$ & $\tO_2$\\
Sum & $H,W$ & $\tO_2$ & $\mO_2$\\
Linear & $64,32$ & $\mO_1\|\mO_2$ & $\mO_3$\\
ReLU & & $\mO_3$ & $\mO_3$\\
Linear & $32,1$ & $\mO_3$ & $\mO_3$\\
\hline
\end{tabular}
\end{center}
\end{table}


\begin{table}[t]
\caption{The ConvBlock2D~$(in, out, up)$.}
\label{tab:conv_block}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
{\sc Layer} & {\sc H-params} & {\sc Input} & {\sc Output}\\
\hline\hline
BatchNorm & & $\tH$ & $\tO_1$\\
ReLU & & $\tO_1$ & $\tO_1$\\
Upsample & $up$ & $\tO_1$ & $\tO_1$\\
Upsample & $up$ & $\tH$ & $\tO_2$\\
Conv2D & $in, out$ & $\tO_1$ & $\tO_1$\\
Conv2D & $in, out$ & $\tO_2$ & $\tO_2$\\
Sum & & $\tO_1, \tO_2$ & $\tO_1 + \tO_2$\\
\hline
\end{tabular}
\end{center}
\end{table}


\begin{table}[t]
\caption{The DConvBlock2D~$(in, out, down)$.}
\label{tab:dconv_block}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
{\sc Layer} & {\sc H-params} & {\sc Input} & {\sc Output}\\
\hline\hline
ReLU & & $\tH$ & $\tO_1$\\
Conv2D & $in, out$ & $\tO_1$ & $\tO_1$\\
Downsample & $down$ & $\tO_1$ & $\tO_1$\\
Conv2D & $in, out$ & $\tH$ & $\tO_2$\\
Downsample & $down$ & $\tO_2$ & $\tO_2$\\
Sum & & $\mO_1, \tO_2$ & $\tO_1 + \tO_2$\\
\hline
\end{tabular}
\end{center}
\end{table}

In the following we give a more precise specification for the architecture we used in our experiments, while specifying that these represent only a possible way of implementing our approach and deeper or more complex architectures might be better suited for different tasks.
Coherently with previous notation, Tables~\labelcref{tab:encoder,tab:non_relational_encoder,tab:mask_generator,tab:downstream_generator,tab:downstream_discriminator} describe the architectures of the different components in our framework; while Tables~\labelcref{tab:conv_block,tab:dconv_block}, for clarity, describe recurring building blocks (i.e., used multiple times) inspired by~\citet{brock2018large}.
In particular, we clarify the meaning of some layers: \textit{MultiEmbedding} simply denotes multiple embedding layers~\citep{bengio2000neural} with the same output size, used to obtain a continuous representation for different discrete node attributes. The first hyperparameter refers to the embedding size, while the second describes the way in which embeddings of different features are combined into a single embedding. In particular, \textit{cat} refers to concatenation along the feature dimension.
The usage of embedding layers can be exchanged for multiple linear layers, if the input features are already continuous.
\textit{PoseConv} is a layer that implements the function described in Equation~\eqref{eq:poseconv}; by respectively naming its three hyperparameters $i$, $h$ and $o$, we can further specify that, in this implementation, $g_s$ and $g_l$ are linear layers with input size $i+2$ (2 is the size of $p_i$) and output size $h$, while $g_g$ is a linear layer with input size $h$ and output size $o$.
Analogously, \textit{PoseConv2D} refers to a layer implementing Equation~\eqref{eq:poseconv2d}, where $g_o$ is a \textit{ConvBlock2D} with $in=2\cdot i$, $out=o$ and $up=2$, followed by $f\left(\tH\right)=\sigma\left(\tH\right)\odot\tH$. $g_s$ is a times two upsampling followed by a $1\times 1$ 2D convolution with $0$ padding, input size $i$ and output size $o$ and, finally, $g_g$ is also a $1\times 1$ 2D convolution with $0$ padding and both input and output size set to $o$. In this case $i$ is the first hyperparameter of PoseConv2D, while $o$ is the second.
\textit{FNNPoseConv} is the FNN equivalent of PoseConv, computing
\begin{equation}
    \vh_{i}' = g_s\left(\vh_i\right) + g_{nr}\left(\vh_i \| \vp_i\right),
\end{equation}
where, naming the first hyperparameter $i$ and the second $o$, $g_{nr}$ and $g_s$ are both linear layers, the former with input size $i + 2$ and output size $o$ and the latter with input size $i$ and output size $o$.
Note that, when using the FNN Conditioner, the PoseConv layers of the discriminator are swapped for equivalent FNNPoseConv ones.
Furthermore, note that, unless differently stated, all Conv2D operations use a kernel of size $3\times 3$ with stride and padding set to $1$.
All layer weights are normalized through spectral normalization~\citep{miyato2018spectral}, \textit{Upsample} operations are implemented through bilinear interpolation, while \textit{Downsample} ones use 2D average pooling.
For completeness, we clarify that the \textit{Sum(H,W)} operation in Table~\ref{tab:downstream_discriminator}, refers to summing the tensor $\tO_2 \in \mathbb{R}^{B \times C \times H \times W}$ over the last two dimensions, thus obtaining the matrix $\mO_2 \in \mathbb{R}^{B \times C}$.

\begin{table}[t]
\caption{The Downstream Generator used for the human dataset.}
\label{tab:human_downstream_generator}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
{\sc Layer} & {\sc H-params} & {\sc Input} & {\sc Output}\\
\hline\hline
ConvBlock2D$^*$ & $1,512,2$ & $\tZ,\tL^*$ & $\tO$\\
ConvBlock2D$^*$ & $512,256,2$ & $\tO,\tL^*$ & $\tO$\\
ConvBlock2D$^*$ & $256,128,2$ & $\tO,\tL^*$ & $\tO$\\
ConvBlock2D$^*$ & $128,64,2$ & $\tO,\tL^*$ & $\tO$\\
Attention & $64$ & $\tO$ & $\tO$\\
ConvBlock2D$^*$ & $64,32,2$ & $\tO,\tL^*$ & $\tO$\\
BatchNorm2D & & $\tO$ & $\tO$\\
ReLU & & $\tO$ & $\tO$\\
Conv2D & $32,3$ & $\tO$ & $\mO$\\
Tanh & & $\mO$ & $\mO$\\
\hline
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\caption{The Downstream Discriminator used for the human dataset.}
\label{tab:human_downstream_discriminator}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
{\sc Layer} & {\sc H-params} & {\sc Input} & {\sc Output}\\
\hline\hline
MultiEmbedding & $8,cat$ & $\mX$ & $\mO_1$\\
PoseConv & $8,16,16$ & $\mO_1,\mP,\mA$ & $\mO_1$\\
BatchNorm & & $\mO_1$ & $\mO_1$\\
ReLU & & $\mO_1$ & $\mO_1$\\
PoseConv & $16,32,32$ & $\mO_1,\mP,\mA$ & $\mO_1$\\
BatchNorm & & $\mO_1$ & $\mO_1$\\
ReLU & & $\mO_1$ & $\mO_1$\\
PoseConv & $32,64,64$ & $\mO_1,\mP,\mA$ & $\mO_1$\\
DConvBlock2D$^*$ & $3,32,2$ & $\tI$ & $\tO_2$\\
Attention & $32$ & $\tO_2$ & $\tO_2$\\
DConvBlock2D$^*$ & $32,64,2$ & $\tO_2$ & $\tO_2$\\
DConvBlock2D$^*$ & $64,128,2$ & $\tO_2$ & $\tO_2$\\
DConvBlock2D$^*$ & $128,256,2$ & $\tO_2$ & $\tO_2$\\
DConvBlock2D$^*$ & $256,512,2$ & $\tO_2$ & $\tO_2$\\
ReLU & & $\tO_2$ & $\tO_2$\\
Sum & $H,W$ & $\tO_2$ & $\mO_2$\\
Linear & $576,512$ & $\mO_1\|\mO_2$ & $\mO_3$\\
ReLU & & $\mO_3$ & $\mO_3$\\
Linear & $512,1$ & $\mO_3$ & $\mO_3$\\
\hline
\end{tabular}
\end{center}
\end{table}

Tables~\labelcref{tab:human_downstream_generator,tab:human_downstream_discriminator} show, respectively, the architectures of the downstream generator and discriminator, used for the human dataset. In such case the GraPhOSE architecture remained the same, although the number of hidden features was increased by a factor of two and $\mu_{\theta}$ dropped the ConvBlock2D, as the target images had shape $64\times 64$, hence no further upsampling was required. The main differences are present at the discriminator and generator level, as for this more complex task we borrowed the building blocks of BigGAN~\citep{brock2018large}. In particular, DConvBlock2D$^*$ has a slightly different architecture in which the first Conv2D layer outputs $2\cdot out$ features and is followed by a ReLU and another Conv2D layer that outputs $out$ features. ConvBlock2D$^*$, instead, takes as additional input the downsampled layout mask $\tL^*$, which goes through a Conv2D layer that outputs $out$ features; these are concatenated to the output of the Conv2D on the standard input and go through a second Conv2D($2\cdot out$, $out$) layer, before the skip connection. This serves the purpose of conditioning the generative process at different scales, as, in this case, the downstream generator starts from a $1 \times 4 \times 4$ feature map, $\tZ$, which is upsampled at each step, instead of directly using the layout mask and always keeping the same feature map size. For completeness, $\tZ$ represents additional latent conditioning, required to account for the background, and is obtained by reshaping the output of a linear layer with output size $16$, which takes as input a vector of normally distributed noise of size $8$.

\begin{table}[t]
\caption{GraPhOSE and baselines approximate parameter count, for the two tasks, divided between conditioning, downstream generator and discriminator.}
\label{tab:nparams}
\setlength{\tabcolsep}{4pt}
\begin{center}
\begin{tabular}{l|c c c | c c c}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{\sc Models}}& \multicolumn{3}{c}{PRO} & \multicolumn{3}{c}{Humans} \\
\cmidrule{2-7}
& {Conditioning} & {Generator} & {Discriminator} & {Conditioning} & {Generator} & {Discriminator}\\
\midrule
{FNN Conditioning} & {2K} & {27K} & {23K} & {6K} & {8.6M} & {5.2M}\\
{GNN Conditioning} & {5K} & {27K} & {26K} & {17K} & {8.6M} & {5.2M}\\
{\textbf{GraPhOSE}} & {63K} & {27K} & {26K} & {2.4M} & {8.6M} & {5.2M}\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Table~\ref{tab:nparams} summarizes the parameter counts for GraPhOSE and the baselines, differentiating between the smaller versions used for the PRO dataset and the larger ones used for Humans.

\subsection{Metrics} \label{subapp:metrics}
We chose Frechet Inception Distance~\citep{fid2017} and Inception Score~\citep{salimans2016improved} as metrics for our generative models as both are commonly used to asses generation quality. However, only FID is effectively useful in assessing how much the generated samples resemble those coming from the real distribution, as this metric compares the similarity between the distribution of images coming from the generator and those coming from the dataset. Inception Score is limited to evaluation of some properties of the generated images distribution, hence it is less useful as a metric of comparison between models because it can only tell us if the model is capable of generating images that are sufficiently distinct and recognizable, even though they might actually be completely different from those of the real distribution. In other words, IS is only influenced by the capability of the generative model of producing samples that can be distinctively allocated to a class and that, at the same time, are diverse within each class. In our setting, the differences between GraPhose and each baseline are not expected to impact such properties, which are mostly tied to the downstream model, which is always the same for each task. This makes the resulting Inception Score quite uninformative as a way of comparing the various models; nonetheless, it highlights that learning the object's relational masks does not indeed harm the appeal of the generated samples.
For the PRO dataset we also reported the Structural Similarity Index Score (SSIM)~\citep{wang2004image}. This metric is used as a measure of image reconstruction accuracy, hence it is suitable to evaluate generation quality only in situations where the input to the model uniquely and completely describes the image to be generated. That is the case for the PRO task, as there is a deterministic mapping between the graph and the resulting image. However, that is not the case for the Humans dataset, in which the input graph just describes the person's pose but carries no information regarding the clothing or the background, hence the reason for not employing such metric in that scenario.

\section{Additional results} \label{app:experiments}

The following is a collection of additional results, comprising further examples of what already discussed in the main paper, and also images generated by training models with slight architectural changes.

\subsection{Missing attributes} \label{subapp:missing}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{missing.png}
\end{center}
\caption{Sample images generated from GraPhOSE in presence of missing semantic attributes: from left to right, Hand, Robotic Arm, Filled Polygon and Scissors. White nodes have their semantic attributes masked, while others are colored according to their color attribute.}
\label{fig:missing}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{missing_increase.png}
\end{center}
\caption{Example of how GraPhOSE behaves, with a graph with $n$ nodes, in presence of missing attributes, as the chain of subsequent ones increases in length from $0$ (left) to $n-1$ (right). White nodes represent missing attributes.}
\label{fig:missing_increase}
\end{figure}

As discussed, our formulation allows for sub-conditioning the pose graph, i.e., having nodes without any semantic attribute assigned, in order to let the generative process decide how to treat them. Figure~\ref{fig:missing} shows some examples of this behavior, in which white nodes represent nodes without any semantic attribute. We can see that the generation is mostly consistent when the attributes can be recovered from neighbouring nodes. However, in cases in which the information needs to be reconstructed through multiple hops (e.g., the scissors handle) the model uses the nearest color instead. This might be the result of biases in the data.
Mind that we use the term \textit{reconstruction} loosely here, as the model is trained with a generative objective, hence we might expect different results if we train it on a strict reconstruction task.
Figure~\ref{fig:missing_increase}, instead, shows what happens when we have a growing chain of missing attributes. We can see that semantics are coherently filled in, by GraPhOSE, up to a certain number of subsequent masked nodes. This is, in part, due to architectural constraints, as the number of hops information can traverse depends on the number of message passing steps. On the other hand, the usage of graph convolution operators that try to prevent over-smoothing might also play a role in this. However, notice we did not train our model to specifically address high chances of partial conditioning, i.e., missing attributes; different results can be expected if we purposefully train it on datasets with such characteristics. 
Even though, in such experiments, the color relationships between object components are rather simple, we think these results, tied with the architectural constraints involved, can be a promising basis for further, and more specifically aimed, experimentation.

\subsection{Mask adaptation} \label{subapp:mask_adaptation}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{before_after_mask.png}
\end{center}
\caption{Random sample of humans masks generated by our model after pre-training (b) and after subsequent training on the downstream task (c). The top row (a) shows the corresponding input graph.}
\label{fig:before_after}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.7\linewidth]{no_pretrain_mask.png}
\end{center}
\caption{Samples of humans masks generated by GraPhOSE's mask generator after end-to-end training without surrogate mask pre-training. The input graph is superimposed in blue (nodes) and red (edges).}
\label{fig:no_pretrain_masks}
\end{figure}

Figure~\ref{fig:before_after} shows additional examples of how the pre-learned masks consistently adapt to the downstream task during the end-to-end training, while figure~\ref{fig:no_pretrain_masks} shows how directly training on the downstream task leads to masks that are not visually meaningful. This is particularly severe for the case of human poses, where the dataset is heavily biased towards a few particularly common ones.

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{submasks.png}
\end{center}
\caption{Samples of individual node masks generated from GraPhOSE. The first column is the cumulative mask for the whole input graph (superimposed with nodes in blue and edges in red). Individual node masks have the reference node highlighted in blue. Hand masks (a) are only a sub-sample for ease of visualization.}
\label{fig:submasks}
\end{figure}

Figure~\ref{fig:submasks} shows further examples of how node-mask locality is preserved after downstream training, and adapted to the specifics of the objects in the task. We can notice that the Pie~(e) is the only object for which this locality seems to be lost, however it is not surprising as its graph representation diameter is only $2$, with just $3$ nodes, hence it is harder to avoid over-smoothing. For graphs with larger diameter and/or number of nodes, we see that locality is retained. 
