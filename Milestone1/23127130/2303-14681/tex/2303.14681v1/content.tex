\begin{abstract}
   Conditioning image generation on specific features of the desired output is a key ingredient of modern generative models. Most existing approaches focus on conditioning the generation based on free-form text, while some niche studies use scene graphs to describe the content of the image to be generated. This paper explores novel methods to condition image generation that are based on object-centric relational representations. In particular, we propose a methodology to condition the generation of a particular object in an image on the attributed graph representing its structure and associated style. 
   We show that such architectural biases entail  properties that facilitate the manipulation and conditioning of the generative process and allow for regularizing the training procedure.
   The proposed framework is implemented by means of a neural network architecture combining convolutional operators that operate on both the underlying graph and the 2D grid that becomes the output image. The resulting model learns to generate multi-channel masks of the object that can be used as a soft inductive bias in the downstream generative task. Empirical results show that the proposed approach compares favorably against relevant baselines on image generation conditioned on human poses.
\end{abstract}

\section{Introduction} \label{sec:Introduction}
Graphs are an abstraction that allows for representing objects as collections of entities and binary relationships.
Previous research on graph-based image generation has been limited to the high-level conditioning of the image content by means of \textit{scene graphs}, i.e., graphs where nodes represent objects, and edges denote subject-predicate-object relationships~\cite{johnson_image_2018, ivgi_scene_2021}.
Conversely, conditioning on desired fine-grained properties, e.g., spatial location, arrangement or visual attributes, is usually performed without considering a relational structure.
In fact, most of the literature dealing with pose-constrained image generation, e.g.,~\cite{ma2017pose, siarohin2018deformable, neverova2018dense, qian2018pose, jiang2022text2human}, represents key points and semantic attributes with 2D masks or feature vectors; hence not exploiting known relationships among the components of the object being generated. 
This paper aims at bridging this gap, showing how a graph, encoding these unexploited relational inductive biases, can be used as an effective and compact, fine-grained object-centric representation for the content of an image.
In fact, a graph can jointly describe an object, its composing parts, its attributes, and their location in space together with the relationships among them.
We propose an approach to condition the generation of an object in an image by means of what we call a \textit{pose graph}: an attributed graph whose nodes have both positional and style attributes.
Each node represents a particular landmark in the object's structure and may carry further attributes that characterize it, e.g., color or class.
In our formulation, all the desired properties of the generated image are encoded in the graph, without relying on additional inputs, such as reference images or similar. Furthermore, in our method, the relational structure of the graph provides both the conditioning for generation and inductive bias on the processing by exploiting neural message passing~\cite{gilmer2017neural} and Graph Neural Networks (GNNs)~\cite{bacciu2020gentle, bronstein2021geometric}. The resulting generative model, then, extracts information from a structured representation of the desired conditioning on the image content which is contextually exploited to constrain the generation process in the neural architecture.

The framework allows for conditioning the generation flexibly by easily changing the complexity of the scene or the number of landmarks we use to represent an object, all without any architectural change.
Inspired by~\cite{johnson_image_2018}, our method learns to generate a multi-channel non-binary mask, that can be used as a bias for generative models on a downstream task.
To overcome the lack of pre-annotated masks for a specific use case, we also propose the use of \textit{surrogate masks} of pose graphs, as a pre-training step performed ahead of training on the target task.
Additionally, by pre-training on random graphs, outside the target distribution, the proposed method can acts as a regularization, to prevent overfitting the most common poses.
To the best of our knowledge, this is the first work to use object-centric graph-based relational inductive biases for the conditioning of a neural generative model.
We summarize our contribution as follows:
\begin{itemize}
    \item We provide a novel and general methodology to solve the task of generating images conditioned on an attributed graph that represents the structure of an object.
    \item We provide a specific implementation of such methodology, together with a training procedure based on a task-independent surrogate objective to enable transfer to different problems.
\end{itemize}
Note that the proposed approach can be nicely extended to more complex scenes that contain multiple objects, each with its arrangement and attributes.
In fact, the appeal of our method resides in its flexibility as we will detail throughout the paper.
We name our model \textit{GraPhOSE}, to highlight that it is designed to work on attributed pose graphs.

\section{Preliminaries} \label{sec:Preliminaries}

A pose graph is a couple $\gG = \left(\gV, \gE\right)$, $\gV$ is the set of vertices (or nodes) and $\gE$ is the set of edges that connect such nodes. We define node attributes $\vv_i = \left(\vp_i, \vx_i\right)$ where $\vp_i \in \mathbb{R}^2$, $\vx_i \in \mathbb{R}^d$ represents the 2D position of the $i$-th node in $\gV$ and its $d$-dimensional feature vector, respectively. We denote with $\mV$ the node attribute matrix $\mV = \left(\mP \| \mX\right)$ stacking all the node attribute vectors.
The edge connecting the $i$-th to the $j$-th node is indicated as $e_{ij} = \langle i, j\rangle$. We assume the graph to be undirected and indicate its~(symmetric) adjacency  matrix as $\mA$; nonetheless, our approach can be seamlessly extended to directed graphs.
The described graph can be processed by stacking GNN layers; in particular, we rely on the \textit{message passing} framework~\cite{gilmer2017neural}, which provides a template for building GNNs by specifying update and aggregation functions to process a node's representation and those of its neighbors.

We indicate as \textit{deep generative model} a neural network that, optionally given a conditioning input, can be trained to match a desired data distribution.
The sampling is often obtained by feeding random noise from a fixed distribution to the model, while conditioning allows for controlling the generative process, e.g., by specifying a class label. In our case, we consider generative processes conditioned on the structure of the objects being generated, represented as pose graphs.

\section{Graph-based Object-Centric Generation} \label{sec:Method}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.9\linewidth]{pipeline}
\end{center}
   \caption{The proposed pipeline. In grey, we have the network that generates the conditioning input. The mask-generating network is the only section that gets pre-trained on surrogate masks. The downstream model, in yellow, can be any trainable generative model able to accept a 3-d tensor as conditioning input. The whole pipeline can be trained end-to-end.}
\label{fig:overview}
\end{figure*}

\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.9\linewidth]{surrogate}
\end{center}
   \caption{Examples of surrogate masks for a random graph (top) and for a graph representing a person (bottom). The position of a node in graph space is normalized between $0$ and $1$.}
\label{fig:surrogate}
\end{figure}

In this Section, we first provide a high-level description of our method, consequently propose an architecture implementing the framework and finally present our surrogate pre-training objective.
Section~\ref{sec:Results} then empirically validates the proposed design.

\subsection{Overview}

Figure~\ref{fig:overview} provides an high-level view of our approach. Given an input pose graph $\gG$, we wish to output a multi-channel mask $\tL \in \left[0,1\right]^{C \times H \times W}$, which will then be used to condition the downstream model on a target generative task.
We design the mask generation process so that the final mask is obtained by aggregating masks localized w.r.t.\ each node. In particular, we decompose the generation of $\tL$ into the generation of a mask $\mM_i \in \left[0,1\right]^{H \times W}$ and a feature vector $\vf_i \in \left[0,1\right]^{C}$ for each node $\vv_i$, such that:
\begin{equation}
    \label{eq:multichannel-mask}
    \tL = \frac{1}{|\gV|}\sum_{i \in \gV}\vf_i \otimes \mM_i,
\end{equation}
where $\otimes$ indicates the \textit{tensor product}.
Notably, we make each $\mM_i$ dependant on pose attributes $\mP$ only, while features $\vf$ depend on both pose and attributes $\left(\mP\|\mX\right)$, so that the former encodes only structural information, while the latter will also eventually account for style.
In particular, we compute both $\vf_i$ and $\mM_i$ by means of two learnable functions, such that our generative model can be summarized as:
\begin{equation}
    \label{eq:generative_model}
    \begin{split}
        \mF &= \Phi_{\phi}\left(\mV, \mA\right)\\
        \tM &= \mu_{\theta}\left(\mP, \mA\right)\\
        \tL &= \frac{1}{|\gV|}\sum_{i \in \gV}\vf_i \otimes \mM_i
    \end{split}
\end{equation}
Where $\tM$ indicates the tensor encompassing all the $\mM_i$ masks. The $\Phi_\phi$ function can be learned end-to-end with the downstream model, as shown in Section~\ref{subsec:application}.
Differently, we pre-train $\mu_\theta$ on a surrogate task designed to foster the learning of masks coherent with the structure of the object being generated. After pre-training, $\mu_\theta$ can be fine-tuned end-to-end with the downstream model on the target task.
This approach is suitable for flexibly conditioning the generation on different tasks based on the structure of the objects. Notably, we implement our method with two neural networks that work in parallel: the \textit{Encoder}~($\Phi_\phi$) and the \textit{Mask Generator}~($\mu_\theta$). 
The former implements function $\Phi_\phi$ from Equation~\eqref{eq:generative_model}, while the latter implements function $\mu_\theta$.
The outputs of the two networks are combined to obtain pose conditioning feature maps.

\subsection{Implementing the encoder $\Phi_\phi$} 

The \textit{Encoder}~($\Phi_\phi$) network is based on what we call a \textit{PoseConvolution} layer, which consists of the following message-passing operator:
\begin{equation}
    \label{eq:poseconv}
    \vh_{i}' = g_g\left(g_s\left(\vh_i, \vp_i\right) + \sum_{j \in \mathcal{N}\left(i\right)} g_l\left(\vh_j, \vp_j - \vp_i\right), \vh_i\right)
\end{equation}
where $\mathcal{N}\left(i\right)$ is the set of neighbours of the $i$-th node, while $g_s$, $g_l$ and $g_g$ can be any learnable function~(e.g., MLPs).
Consistently with the previous naming convention, $\vh_i$ and $\vp_i$ indicate node features and position respectively.
The layer is inspired by PointNet~\cite{qi2017pointnet} but uses two distinct functions for processing the representation of the central node and computing messages coming from neighbors. In particular, $g_s$ can be seen as implementing a parametrized skip connection to mitigate over-smoothing node representations~\cite{li2019deepgcns}.
The final node encodings are obtained by stacking blocks of the form
\begin{equation}
    \label{eq:pose_conv_layer}
    \gG_{\mH'\|\mP} = \textsc{BN}\left(\textsc{ReLU}\left(\textsc{PConv}\left( \gG_{\mH\|\mP}\right)\right)\right),
\end{equation}
where \textsc{BN} denotes Batch Normalization~\cite{ioffe2015batch}, \textsc{ReLU} is the activation function,\textsc{PConv} is our PoseConvolution layer and $\gG_{\mH\|\mP}$ a pose graph with features $\mH$ and node coordinates $\mP$.

\subsection{Implementing the mask generator $\mu_\theta$} 
The \textit{Mask Generator}~($\mu_\theta$) network consists of a first block of stacked Pose Convolutional layers analogous to the ones in Equation~\eqref{eq:pose_conv_layer}.
The outputs of these layers are then reshaped into bi-dimensional matrices used as input for the second stack of layers consisting of a combination of convolution blocks~(like those used in BigGan's generator network~\cite{brock2018large}), interleaved with \textit{PoseConvolution2D} layers. Such layers implement the following message-passing layer:
\begin{equation}
    \label{eq:poseconv2d}
    \begin{split}
        \mO_i &= \sum_{j \in \mathcal{N}\left(i\right)} g_o\left(\mH_i, \mH_j\right)\\
        \mH_{i}' &= g_s\left(\mH_i\right) + \sigma\left(\mO_i\right) \odot g_g\left(\mO_i\right)
    \end{split}
\end{equation}
where, $\odot$ denotes the Hadamard product, $\mathcal{N}\left(i\right)$ is the set of neighbors of node $i$ and $g_s$, $g_l$ and $g_g$ can be any learnable function that has two-dimensional inputs and outputs.

In our case,  $g_g$ is a 2D convolutional layer while $g_o$ is the previously mentioned BigGan's generator block and $g_s$ is a linear upsampling operation followed by a 2D convolution, where the upsampling is needed to match the dimensions of the output of the $g_o$.
The rationale behind the design of Equation~\eqref{eq:poseconv2d} is promoting heterogeneity between the masks generated by different nodes by exploiting gating and skip connections. Notably, $g_s$ can act as skip connection preserving the node representation while the gating operation allows for selectively aggregating information coming for the neighbors.
Indeed, over-smoothing the node features would be particularly detrimental in this case as it would compromise the locality of the masks learned w.r.t.\ each node.
Said property is desirable as, per Equation~\eqref{eq:multichannel-mask} this would in turn make the learned node embeddings localized w.r.t.\ their neighborhood. Note that these soft constraints, i.e., architectural biases, can be seen as a form of regularization  aligned with object-centric generation tasks.

\subsection{Surrogate Task} \label{subsec:Surrogate Task}
As mentioned previously, we want to pre-train $\mu_\theta$ on the generic surrogate task of mapping a pose graph to a 2D mask.
For this purpose, we define a surrogate mask associated with a graph $\gG$, as shown in Figure~\ref{fig:surrogate}, which, intuitively, is a grayscale image that depicts the structure of the graph.

The mask for the whole graph is obtained by composing partial masks relative to each node and edge.
In particular, given pixel coordinates $c$, we define the value of the partial surrogate mask associated with the $i$-th node, at that pixel, as
\begin{equation}
    \label{eq:surrogate_mask_node}
        \textsc{S}_{\mN,i}\left(\vc\right) = \exp\left(-\frac{\left\|\vp_i - \vc\right\|_2^2}{2\sigma^2}\right)
\end{equation}
and, analogously, the value of the partial surrogate mask associated with edge $e_{ij}$ as
\begin{equation}
    \label{eq:surrogate_mask_edge}
        \textsc{S}_{\mE,ij}\left(\vc\right) = \sqrt{\frac{\exp\left(-d_{ij}(\vc)^T\cdot \mT_{ij}^{-1}\cdot d_{ij}(\vc)\right)}{\left(2\pi\right)^2\cdot\det\left(\mT_{ij}\right)}},
\end{equation}
where $\mT_{ij}$ denotes a $2\times 2$ rotation and scaling matrix dependent on the length and angle of the segment connecting nodes $i$ and $j$~(see the supplementary material for the details); conversely, $d_{ij}(\vc)$ is defined as
\begin{equation}
    d_{ij}(\vc) = \vc - \left(\frac{\vp_i + \vp_j}{2}\right)
\end{equation}
and denotes the distance between $\vc$ and the midpoint between the coordinates of nodes $i$ and $j$.

The value associated with pixel $\vc$ of the final surrogate mask for $\gG$ is then obtained as
\begin{equation} \label{eq:surrogate_final}
    \textsc{S}_{\gG}\left(\vc\right) = \sum_{i \in \gG} \textsc{S}_{\mN, i}\left(\vc\right) + \sum_{\langle i, j \rangle \in \gG} \textsc{S}_{\mE, i, j}\left(\vc\right)
\end{equation}
which is the pixel-wise sum of the masks associated with each node and edge. All the values are then clipped between $0$ and $1$.
More details about the computation of the mask can be found in Appendix~\ref{subapp:surrogate_mask}.
Intuitively, the mask corresponding to a node is obtained by considering an isotropic bi-variate Gaussian with mean equal to the node coordinate and standard deviation $\sigma$, over a discrete grid with the same size as the image we wish to generate.
The mask corresponding to an edge, instead, is obtained by considering a bi-variate Gaussian with mean equal to the middle point between the two nodes connected by the edge, and covariance matrix dependent on the distance between the two points and the orientation of the line joining them.
The surrogate mask obtained in such a way is agnostic w.r.t.\ the object represented by the graph and hence general. Note that differently from, e.g., segmentation masks, the mask we are referring to depends entirely on the structure of the objects in the image being generated, which can, in principle, vary in terms of style, structure, and class.

As a final remark, the surrogate mask has a lattice structure, which may not properly resemble the desired mask for all target tasks; indeed, depending on the specific object, some loops should be filled, or some parts should have peculiarly shaped features (e.g., four nodes forming a box or the nodes corresponding to hands in a human skeleton).
Nonetheless, such surrogate masks are helpful in providing supervision for the pre-training routine and act as a regularization for the model.
Details on the fine-tuning procedure on downstream tasks are discussed in Section~\ref{subsec:mask_adaptation}.
The pre-training routine is carried out by minimizing a surrogate loss $\mathcal{L}_{re}$ based on a reconstruction error, e.g., mean squared error or binary cross entropy.

\section{Related Works} \label{sec:Related Work}

Image generation is a popular application of deep learning models from Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative}, to Variational Autoencoders (VAEs)~\cite{kingma2013auto} and, more recently, Diffusion Models~\cite{sohl2015deep, song2019generative, ho2020denoising}. 
Concurrently, many researchers have explored ways of conditioning the generation of such images, from simple class labels~\cite{mirza2014conditional, brock2018large}, to fully articulated sentences~\cite{ramesh2022hierarchical, rombach2022high} or even other images~\cite{isola2017image, liu2017unsupervised, CycleGAN2017}.
Although no previous work directly exploits pose graphs to guide image generation, several approaches focused on Scene-Graph-Conditioned Image Generation and Pose-Conditioned Image Generation.
\paragraph{Scene-Graph-Conditioned Image Generation} Scene graphs are a way of representing a visual scene context as a graph linking object nodes through predicate edges. 
The first work that proposes image generation conditioned on a particular scene graph was written by~\citet{johnson_image_2018}. 
In their work they propose a GNN that maps the scene graph into a scene layout, that is then used to bias a generator network. 
\citet{ivgi_scene_2021}, improved the first approach by using a GNN that directly works on 2D feature maps.
Differently from the above, however, our work focuses on representing a specific object and its structure in space through a graph, rather than multiple abstract objects and their relationship without any geometrical constraint.
\paragraph{Pose-Conditioned Image Generation} Other prior works that we deem related to ours are concerned with pose-guided image generation.
In particular, this niche deals with human image generation.
The first deep learning approach to generate a person image conditioned on a particular pose and reference image was proposed by~\citet{ma2017pose}.
Others built on this approach using deformable convolutions~\cite{siarohin2018deformable}, dense poses~\cite{neverova2018dense} or explicit attributes~\cite{men2020controllable}. 
A typical problem solved by these techniques is person re-identification~\cite{qian2018pose}.
The work by~\citet{horiuchi2021differentiable}, instead, implicitly uses a graph-based representation for the pose and a reference image for semantics.
The main difference with our approach is that we jointly encode pose and semantics in a graph.
This makes the conditioning more flexible and unties it from a pre-existing person image composed of a specific set of properties that are difficult to modify indiependently.

\section{Experiments} \label{sec:Results}
This section reports the results obtained by applying our method. 
In particular, we first show our results in pre-training the mask generator $\mu_\theta$ with surrogate masks created from random graphs.
Finally, we present an application to image generation conditioned on human poses and compare our approach against relevant baselines.
The code used to run the experiments is provided in the supplementary materials.

\subsection{Surrogate Pre-Training} \label{subsec:pretrain}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.99\linewidth]{mask_sample_v2}
\end{center}
\caption{Examples of masks generated from different graphs. For each sample, the larger figure shows the aggregated mask, while the smaller ones those of each specific node. The blue dots highlight the position of the node being considered. Samples (b) and (d) are simple handcrafted graphs. Sample (a) is an example of a random graph like those in the pre-training routine. Sample (c) is a human pose graph.}
\label{fig:sample_masks}
\end{figure*}

For surrogate pre-training, we generate random graphs using a \textit{Barbasi-Albert model}~\cite{albert2002statistical}, with number of nodes uniformly sampled in the interval $\left[5, 30\right]$; number of edges for new nodes is set to $1$ in $90\%$ of the cases and to $2$ in the remaining $10\%$.
These parameters are chosen as to generate graphs simple to be drawn on the plane, i.e., with only a few intersections among edges.
The position of each node is determined by using \textit{Kamada-Kawai}'s force-directed graph drawing algorithm~\cite{Kamada1989}.
While alternatives exist, and even random placing is a possible choice, the Kamada-Kawai's algorithm creates structures that mostly avoid edge overlaps for a wide range of node counts.
Objective masks are computed as in Equation~\eqref{eq:surrogate_final} and the model is fitted to minimize the Binary Cross Entropy (BCE) loss.
Figure~\ref{fig:sample_masks} shows sample results for the mask generation.
Each of the bigger images corresponds to the full mask generated from the input graph superimposed in blue (nodes) and red (edges), while the smaller images are the masks relative to each node (superimposed in blue).
The model is able to produce masks that match the topology of the graph, even for complex structures.
More interestingly, results show that the proposed architecture is indeed capable of generating node-level masks that capture the structure of the neighbourhood of each node.
 By looking at samples (b) and (d), which contain handcrafted graphs, we see that this property is preserved for graphs with a small diameter; even though, nodes with high connectivity tend to produce richer masks that may span it completely.
Indeed, locality is preserved against over-smoothing which is typical of isotropic graph convolutional filters; conversely, the learned node-level masks are diverse and properly localized.

\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.99\linewidth]{rand_norand}
\end{center}
   \caption{Examples of masks generated by pre-training on random graphs (a) and on graphs from the downstream task (b). First two columns are graphs from the downstream task, while the last two are random graphs. In (b), performance clearly degrades out of distribution.}
\label{fig:rand_norand_mask}
\end{figure}

In Figure~\ref{fig:rand_norand_mask}, we additionally show how pre-training on randomly generated graphs act as regularization by yielding a model able to perform properly for graphs outside the distribution of the downstream task,

\subsection{Application} \label{subsec:application}

\begin{table}[t]
{\small
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
{\small Model} &  {\small FID {$\downarrow$}} & {\small \# Train Params} & {\small \# Inference Params} \\
\hline\hline
BigGAN & 91.71 & 9.77 M & 4.89 M\\
Baseline & 57.71 & 13.82 M & 8.57 M\\
GraPhOSE & \textbf{46.97} & 14.15 M & 8.87 M\\
No Pretrain & 74.63 & 14.15 M & 8.87 M\\
\hline
\end{tabular}
\end{center}}
\caption{\label{tab:fid}Frechet Inception Distance (FID) score (lower is better)  and parameters counts. Note that at inference time the discriminator model is not needed.}
\end{table}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.99\linewidth]{model_comparison_v3}
\end{center}
   \caption{Random sample of images generated by GraPhOSE (e), GraPhOSE without surrogate mask pre-training (d), baseline without explicit relational structure (c), BigGAN without conditioning (b). The first row (a) shows the pose graph used for conditioning.}
\label{fig:comparison}
\end{figure*}

As an example of downstream task for which our methodology can be used, we choose that of person image generation conditioned on a specific pose and keypoint IDs as attributes.
We use BigGan~\cite{brock2018large} as downstream model, slightly modified to receive as input our conditioning as a multi-channel mask.
Moreover, similarly to~\cite{johnson_image_2018}, each stage of the generator takes as input both the previous stage's output and a properly downsampled version of the multi-channel conditioning mask to guide generation at different scales. 
The original discriminator is also modified to receive the graph as input, together with the real/fake image; further details are provided in Appendix~\ref{subapp:downstream}.
Note that, during the end-to-end training, we drop the reconstruction loss originally used to pre-train the mask generator.
This is to allow for adapting to the downstream task, without the bias coming from the surrogate loss.
We pre-train the mask generator as described in Section~\ref{subsec:pretrain}, and the whole pipeline is trained on data coming from three different datasets: MPII Human Pose Dataset~\cite{andriluka14cvpr}, Market 1501~\cite{zheng2015scalable} and DeepFashion~\cite{liuLQWTcvpr16DeepFashion}.
For the DeepFashion dataset we use just the Fashion Landmark Detection and the In-Shop Retrieval subsets.
The keypoint annotations for DeepFashion In-Shop Retreival and Market 1501 are based on~\cite{zhu2019progressive}, and were generated by using the open-source software OpenPose~\cite{openpose}.
For DeepFashion Fashion Landmark we instead rely on MediaPipe's BlazePose~\cite{mediapipe}, while MPII Human Pose already contains pose features.
All annotations are reduced to the COCO Keypoint standard~\cite{coco} and used to generate the corresponding graphs with node coordinates normalized between $0$ and $1$.
Images are resized, and zero-padded when needed, to a $64 \times 64$ shape.

We compare the results obtained by our model against two different baselines.
The first one is a standard BigGan, analogous to the one modified to use as a downstream model for our pipeline; this baseline is to assess the quality achievable by a model without any conditioning.
The other baseline is, instead, a non-relational model: if we consider Equation~\eqref{eq:generative_model}, we can describe it in an analogous way as
\begin{equation} \label{eq:baseline}
    \begin{split}
        \mF &= \Psi{\psi}\left(\mV\right)\\
        \tM &= \veccat_{i \in \gV}\left(\textsc{S}_{\mN, i}\left(\mC\right) + \sum_{j \in \gN(i)}\textsc{S}_{\mE,ij}\left(\mC\right)\right)\\
        \tL &= \frac{1}{|\gV|}\sum_{i \in \gV}\vf_i \otimes \mM_i,
    \end{split}
\end{equation}
where $\veccat_{i \in \gV}$ indicates concatenation along a new axis, while, for ease of notation, $\textsc{S}_{\mN,i}\left(\mC\right)$ and $\textsc{S}_{\mE,ij}\left(\mC\right)$ denote the respective operations applied over all the pixel coordinates.
In particular, in Equation~\eqref{eq:baseline}, $\tM$ is computed by means of a fixed (non-trainable) function, leveraging the procedure used for creating surrogate masks.
$\Psi_\psi$ instead denotes a learnable function that is shared across all nodes and does not consider relational information (i.e., the adjacency matrix); it is implemented by means of a network analogous to the encoder used in the relational model, minus the message passing.
The objective, in this case, is to compare against a model that does not account for the graph structure within the processing architecture, while still maintaining a similar design with conditioning on a hand-crafted mask.
See Appendix~\ref{subapp:baseline} for a detailed description of the baselines.
All models were trained under the same settings; more details can be found in Appendix~\ref{subapp:parameters}.

Table~\ref{tab:fid} shows the Frechet Inception Distance (FID) score~\cite{fid2017} for our model and the baselines we wish to compare it to.
GraPhOSE achieves a better score when compared to the non-relational baseline, and results suggest that the surrogate mask pre-training plays a relevant role in achieving these results given an equivalent training setting.
In fact, the model without pre-training performs worse than the one with pre-training and even worse than the vanilla BigGan baseline.
Finally, we comment that the downstream model on its own has worse performance than that of models with a conditioning on the generative process.
However, it is worth noting that FID accounts for the overall similarity between the distribution of images in the dataset and the one generated by the models; indeed, FID does not account for the position of the objects in the generated images.
We assess this property visually, in a qualitative manner. 

In this regard, Figure~\ref{fig:comparison}, shows results generated by BigGAN, the non-relational baseline and GraPhOSE, for a random sample from our validation set.
For images generated by BigGAN, row (b), the pose graph bears no meaning, as the model is not conditioned on it, but they give a visual cue on how the images generated from our model have similar quality w.r.t.\ the ones generated by the downstream model.
For GraPhOSE, row (e), the overall pose and scale of the person in the generated image comply with the input graph, even though specific details might be missing, e.g., for complex poses.
However, this is also likely to be a bias coming from the training data, in which simple standing poses are more frequent.
Moreover, training at a higher resolution might improve the generation of fine-grained details, e.g., hands, that in $64\times 64$ images occupy a very limited amount of pixels.

The images generated by training GraPhOSE without pre-training on surrogate masks, row (d), are still consistent with the input pose, even though, as shown in Figure~\ref{fig:no_pretrain_masks}, the intermediate masks $\mM_i$, generated in such setting, do not visually resemble the persons' pose.
The lack of structure in the mask leads to degradation of the performance, in particular for uncommon poses, as results suggest.
Note that, even without pre-training, the model still leverages the relational biases encoded in the graph, although not in the way our design intended, which benefits from localized masks.
This shows the usefulness of pre-training as a soft regularization, which localizes node features w.r.t.\ corresponding portions of the image.
Finally, by looking at the images generated by the non-relational baseline, row (c), it is possible to notice a generally worse visual appeal, with skin tones placed inconsistently across the figure; even though the FID score was comparable to that of the other models. 
The pose is somewhat respected, but the relational representation plays nonetheless a significant role in guiding the generation toward a coherent result.

\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.9\linewidth]{no_pretrain_mask_v2}
\end{center}
   \caption{Samples of masks generated by GraPhOSE's mask generator after end-to-end training without surrogate mask pre-training. The input graph is superimposed in blue (nodes) and red (edges).}
\label{fig:no_pretrain_masks}
\end{figure}

\subsection{Task specific mask adaptation} \label{subsec:mask_adaptation}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.99\linewidth]{before_after_mask_v2}
\end{center}
   \caption{Random sample of masks generated by our model before the end-to-end training on the downstream task (b) and after (c). The top row (a) shows the corresponding input graph. Row (b) shows masks after pre-training (no fine-tuning).}
\label{fig:before_after}
\end{figure*}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.99\linewidth]{move_pose}
\end{center}
   \caption{Row (b) shows the results obtained by fixing all inputs to our model and just changing the attributes describing the position of some nodes. The top row (a) is the corresponding input graph pose.}
\label{fig:move_pose}
\end{figure*}

The purpose of pre-training on surrogate masks of random pose graphs is to learn a general mapping between the graph and the image representation of the structure it entails.
However, this learned mapping might not produce the exact visual properties desirable for a specific downstream task.
We wish to adapt to these properties while learning the target task, during the end-to-end training.
Figure~\ref{fig:before_after} shows changes in the masks being generated before and after training on the target task, given the same input.
We can notice how the model learned that the torso section needs to be filled, that the legs should be thinner, and that the head is composed of a slim section and a round part.
This result is particularly relevant, as it shows that after a pre-training that pushes the mask generator in the correct direction, it is then possible to have it learn the specifics on the target downstream task.

\subsection{Pose Manipulation}

To assess whether the model is able to disentangle  pose attributes and style, we experiment with providing it with a series of input graphs with slightly different limb positions, while maintaining all other inputs fixed.
As shown in Figure~\ref{fig:move_pose}, generated figure moves are rearranged according to the changes in the input pose while the style is mostly unchanged.
Note that, however, in case of more significant changes in the pose, the resulting images were visually different.
This emergent property is nonetheless interesting and provides ground for future research.

\section{Conclusions and Future Work} \label{sec:Conclusions and Future Work}
We propose GraPhOSE, a method to exploit attributed graphs as object-centric relational inductive biases to flexibly condition image generation.
Our approach has several advantages compared to standard approaches which result in a scalable and flexible framework. Notably, we shone a light on the properties of relational representation in this context, by showing how they can be used to regularize and manipulate the generative model.
Future research could investigate different approaches to generate meaningful and coherent masks without pre-training and investigate methods to further disentangle the different elements that contribute to the final generated image.
We argue that our study has the potential of sparking new interest in object-centric relational inductive biases for generative modeling and that the results presented here constitute the first building block for future research in this direction.