\appendix
\section{Implementation Details} \label{app:Details}

In the following we clarify some details on the settings used to train the models, on the specifics of some models' architectures, as well as further details on surrogate masks and the hardware and software we used to run the experiments.

\subsection{Hardware and Software} \label{subapp:hardware_software}

The code to run the experiments is all written in \textit{Python 3.9}, leveraging \textit{Pytorch}~\cite{paszke2019pytorch} and \textit{Pytorch Lightning}~\cite{falcon2019lightning} to define models and data, while \textit{Hydra}~\cite{Yadan2019Hydra} is used to manage the experiment configuration.
\textit{Weights \& Biases}~\cite{wandb} is used to log and compare experiment results.
We run all the experiments on an \textit{NVIDIA RTX A5000} GPU equipped with 24GBs of VRAM.
All code will be made open-source through GitHub upon publication.

\subsection{Training settings} \label{subapp:parameters}

Each model is trained under the same settings, chosen based on those used originally to train BigGAN~\cite{brock2018large}.
In particular, we use the \textit{Adam}~\cite{kingma2014adam} optimizer, with \textit{Cosine Annealing}~\cite{loshchilov2016sgdr} learning rate schedule with period $150$, starting learning rate $0.002$ and final learning rate $0.00002$.
Models are trained for $150$ epochs each, with a batch size of $64$.
Notice that the learning rate for the parameters of the pre-trained mask generator used in GraPhOSE was 100 times smaller with respect to the learning rate of the other parameters, in order to avoid catastrophic foregetting at the beginning of training.
This procedure could be enhanced by slowly raising the learning rate of the pre-trained parameters back to the value of other parameters, as training epochs go by, with the aim of favoring mask adaptation after the training has stabilized during the first epochs.

\subsection{Surrogate Mask} \label{subapp:surrogate_mask}
Referring to Equation~\eqref{eq:surrogate_mask_edge}, we explicitly define $\mT_{ij}$ as

\begin{equation} \label{eq:rotoscale}
    \begin{split}
        d_{ij} &= \frac{\| \vp_i - \vp_j \|^2_2}{4}\\
        \alpha_{ij} &= \arctan2\left(\vp_j - \vp_i\right)\\
        \evt_{a,ij} &= d_{ij}\cdot \cos\left(\alpha_{ij}\right)^2 + \frac{d_{ij}}{a^2}\cdot \sin\left(\alpha_{ij}\right)^2\\
        \evt_{b,ij} &= d_{ij}\cdot \sin\left(\alpha_{ij}\right)^2 + \frac{d_{ij}}{a^2}\cdot \cos\left(\alpha_{ij}\right)^2\\
        \evt_{c,ij} &= \left(d_{ij} - \frac{d_{ij}}{a^2}\right) \cdot \sin\left(\alpha_{ij}\right) \cdot \cos\left(\alpha_{ij}\right)\\
        \mT_{ij} &= 
        \begin{bmatrix}
            \evt_{a,ij}&\evt_{c,ij}\\
            \evt_{c,ij}&\evt_{b,ij}
        \end{bmatrix},
    \end{split}
\end{equation}
where vectors $\vp$ denote node coordinates, $\arctan2$ is the element-wise 2-argument arctangent and $a$ denotes a parameter that regulates the scaling ratio of the second dimension w.r.t.\ the first one.
In our experiments $a$ is set to $10$.

\subsection{Changes to the downstream model} \label{subapp:downstream}
As mentioned in Section~\ref{subsec:application}, we modify BigGAN's generator and discriminator for our purposes; the former to accept our conditioning, the latter to take both an image and a graph as input.
Regarding the generator, we simply concatenate our multi-channel conditioning mask to the optional 2D noise matrix that is originally the generator's input.
Furthermore, we modify its original convolution block to have the following form
\begin{equation}
    \begin{split}
        \tH' &= \sigma\left(\textsc{Conv2D}\left(\textsc{Upsample}\left(\sigma\left(\tH\right)\right)\right)\right)\\
        \tK &= \textsc{Conv2D}\left(\textsc{AvgPool2D}\left(\tL\right)\right)\\
        \tH'' &= \textsc{Conv2D}\left(\tH\right) + \textsc{Conv2D}\left(\tH'\|\tK\right),
    \end{split}
\end{equation}
where $\tH$ represents the input feature maps, while $\tL$ is the conditioning multi-channel mask. 
\textsc{Conv2D} is a two-dimensional convolution layer, \textsc{Upsample} denotes a channel-wise times-two linear interpolation upsample operation and $\sigma$ is batch-normalization followed by a rectified linear unit.
Lastly, \textsc{AvgPool2D} is a two-dimensional average pooling operation that matches the size of the last two dimensions of $\tL$ to those of $\tH'$.
$\tH''$ is the output of the convolution block.

Regarding the discriminator, instead, we add a graph encoder with the same network architecture used to implement $\Phi_\phi$ in Equation~\eqref{eq:generative_model}.
The output of this encoder is then concatenated to the feature vector extracted from the input image by the discriminator's original image encoder.
The resulting vector goes through a linear layer, a ReLU and another linear layer, as in the original BigGAN's discriminator.
This final output is then used to compute the adversarial loss.
Note that our additional graph encoder does not change the discriminator's original design.

\subsection{Non-relational Baseline architecture} \label{subapp:baseline}
The non-relational baseline model architecture follows the same structure of our GraPhOSE model, with the main difference that, instead of graph convolutions, it uses only shared linear layers to compute $\Psi_\psi$, in Equation~\eqref{eq:baseline}.
In particular, we keep the same architecture used for GraPhOSE, made of layers of the form shown in Equation~\eqref{eq:pose_conv_layer}, where, instead of $\textsc{PConv}$ layers, we have the following operator
\begin{equation}
    \vh_{i}' = g\left(\vh_i, \vp_i\right),
\end{equation}
where $\vh_i$ are node features and $\vp_i$ denotes node positions.
In our case $g$ is a linear layer.
Node masks $\mM_i$ are computed as shown in Equation~\eqref{eq:baseline}.
The rest of the generator is the same as the one used by our model.
The discriminator for the baseline extracts features from the input graph following the same principle described in Appendix~\ref{subapp:downstream}, with the only difference that the features are extracted using $\Psi_\psi$ instead of $\Phi_\phi$.
Note that this model has no access to explicit relational information regarding how the key points are connected together, even though this information is implicitly provided through the non-learnable masks, as it is still necessary to have a meaningful comparison with our model. 

\section{Other experiments} \label{app:experiments}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.99\linewidth]{complex_mse_all}
\end{center}
   \caption{Examples of images (d) and masks (c) generated by training GraPhOSE with a mask generator pre-trained on the MSE loss function. Row (a) shows the input graph, while row (b) shows the masks generated by the pre-trained mask generator, before end-to-end training.}
\label{fig:mse_all}
\end{figure*}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.99\linewidth]{no_imgraph_discriminator}
\end{center}
   \caption{Examples of images (c) and masks (b) generated by training GraPhOSE with a discriminator that does not take the graph as input. Row (a) shows the input graph.}
\label{fig:no_graph_disc}
\end{figure*}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.99\linewidth]{low_mid_high_lr}
\end{center}
   \caption{Examples of images and masks generated by training GraPhOSE with different learning rates for the pre-trained parameters. Row (a) shows the input graph. Rows (b) and (c) show, respectively, the masks and images obtained with the lowest learning rate used. Equivalently, rows (d) and (e), and rows (f) and (g), show those obtained by using a middle ground learning rate and an higher one respectively.}
\label{fig:lowmidhigh_lr}
\end{figure*}

Here is a collection of result images generated by training models with slight architectural changes.
We deemed these changes to not affect the results sufficiently or clearly enough to be part of an interesting discussion, but we still provide them for completeness and for interested readers.

\subsection{MSE pre-trained Mask Generator}

We experiment with using the Mean Square Error (MSE) as loss function for the mask reconstruction during pre-training.
Figure~\ref{fig:mse_all} shows a sample of the results we achieved.
The most evident characteristic of the masks produced with these settings is that they appear softer than those produced by using a BCE loss.
This is likely due to the structure of the losses themselves; nonetheless we can see that the softer conditioning is still able to produce good results on average, while possibly being more prone to forgetting during end-to-end training.


\subsection{Image-only Discriminator} 

Figure~\ref{fig:no_graph_disc} shows a sample of the results obtained by training GraPhOSE with the standard BigGAN's discriminator, which does not take the graph as input, but only the real/fake image. 
It is noticeable how the input pose is mostly respected, however, the model is more susceptible to failure when receiving uncommon poses. 
The generated masks, on row (b), are more detached from the input graph's pose; this is most likely due to the fact that, as the discriminator cannot leverage information about the conditioning, it cannot effectively recognize as fake, images with good visuals but incorrect poses.
This, in turn, provides a less useful adversarial training cycle, and we could expect that, with an higher learning rate on the mask generator, or more training epochs, the pre-learned function could be completely forgotten.

\subsection{Varying learning rate reduction factor}

As already mentioned in Section~\ref{subapp:parameters}, we applied a reduction factor to the learning rate of the pre-trained parameters, in order to avoid catastrophic forgetting during the early training steps.
Here we show the results obtained by using a smaller reduction factor, hence an higher learning rate, for such parameters.

\begin{table}[t]
{\small
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
{\small Model} & LR Reduction Factor & {\small FID {$\downarrow$}} \\
\hline\hline
Low LR & 0.01 & \textbf{50.69}\\
Mid LR & 0.05 & 57.13\\
High LR & 0.1 & 54.86\\
\hline
\end{tabular}
\end{center}}
\caption{\label{tab:lowmidhigh_fid}Frechet Inception Distance (FID) score (lower is better) for different learning rate reduction factors on the pre-trained parameters.}
\end{table}

The FID scores, reported in Table~\ref{tab:lowmidhigh_fid}, show that, given otherwise equal training conditions, the highest reduction, hence the lower learning rate, provides the best results. 
However, the difference is rather small, which points towards the fact that some more advanced learning rate scheduling might be used to provide stability during the first epochs and more adaptability during the later ones.

By looking at the results shown in Figure~\ref{fig:lowmidhigh_lr}, we can see how, on average, we reach good results for all the three settings of the pre-trained parameters learning rate.
Noticeably, the characteristics of the generated masks vary strongly as the learning rate is increased, which, as mentioned, is a thing we might wish for the later stages of training, in order to better conform to the specifics of the downstream task.
Another thing worth noting is that, for the highest learning rate setting, the masks start to contain patches outside of the object's outline. 
This suggest the model is trying to condition the background, which in principle could be useful.
Further exploration of this behavior may suggest a design in which an additional entity is added to the relational structure to account for the general conditioning of the background and a more complex learning rate scheduling is used.
.