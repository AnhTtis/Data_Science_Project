\section{Experimental Details}

\label{app:experiments}

\subsection{PINN training setup}




    \paragraph{Networks.} We considered both 2-hidden-layer and 4-hidden-layer fully-connect networks with 25 and 50 nodes per layer, respectively. We used hyperbolic tangents as activation functions in order to have a proper comparison with \cite{Krishnapriyan2021}.

    \paragraph{Data.} The training and test data are obtained by randomly sampling  $(x,t)$ points on the domain $\Omega=[0,2 \pi)$ and $T \in [0,1]$ using a grid of side  $n_x =256$  and $n_t=100$. We used $N_u = 100$ (initial conditions), $N_f =2000$ (bulk) and $N_b=80$ (periodic boundary conditions). Additionally, for the ADAM optimiser, we uniformly divide the total dataset into mini-batches of size $\mathcal{O}(400)$. For all cases, we split the data into training (80\%) and test (20\%) sets.

    \paragraph{Hyperparameters.} The learning rates and wave speeds ($\beta$) used for training are given in \cref{tab:lin_adv_scan}. For the specialised hyperparameters for ADAM ($\beta_1 = 0.9, \beta_2 = 0.999, \eps=10^{-8}$, weight-decay = 0), GD  (no mini-batches and no momentum) and LBFGS (max-iter = 20, tolerance-grad = $10^{-7}$, tolerance-change = $10^{-9}$, history-size = 100), we used the Pytorch default values.\footnote{\url{https://pytorch.org/docs/stable/optim.html}}  For BBI, we used $\Delta V =0$ (objective function shift), $\delta E =2$ (extra initial energy), $N_b =4$ (number of bounces), $T_0 =500$ (fixed timesteps for bounces), and $T_1=100$ (progress-dependent timesteps for bounces). We trained the models for 1000-5000 epochs depending on the convergence. Training dynamics (train and test losses) is shown in \cref{app:losses}.


\subsection{Learning Rate search for the Linear Advection}

Let us start by discussing the criteria  used to compare the optimisation algorithms.   Naturally, ADAM, BBI, GD and LBFGS  have different  hyperparameters. BBI, in particular, has specialised hyperparameters to boost chaotic mixing via random bounces (see \cref{app:bbi}). We note empirically that for the linear advection equation, the learning rate is the hyperparameter that impacts the most in the performances.
In Tab.\,\ref{tab:lin_adv_scan} 
 we show the results for the lowest average test losses using 5 trials with random initialised networks varying the learning rates in the range $[10^{-4}, 1].$ We trained the models for the first 300--1000 epochs depending on the convergence.  We note that  BBI and GD  are numerically unstable for learning rates above $\sim 0.1$.  By performing this search,  we  estimate the best scenario for each optimiser with respect to the learning rate. 
We acknowledge, however,  that significant changes in the other hyperparameters may impact the performances. 
 

\begin{table}[tbhp]
    \captionsetup{position=top}
    \caption{\textbf{Learning rates for the lowest Test Loss.\label{tab:lin_adv_scan}}}
    \centering
    \subfloat[$\begin{bmatrix}2& 25& 25& 1\end{bmatrix}$]{\label{tab:network_1}
    \begin{tabular}{lrrrr}
        \toprule
        $\beta$ & BBI  & LBFGS  & GD  & Adam\\
        \midrule
        1  & 0.1    & 0.1      &0.01    & 0.001\\
        5  & 0.01    & 0.1      &0.01    & 0.001\\
        15 & 0.01   & 0.01      &0.0001  & 0.01\\
        30 & 0.01   & 0.01     &0.001   & 0.01\\
        \bottomrule
    \end{tabular}}
    ~
    \subfloat[$\begin{bmatrix}2& 50& 50& 50& 50 &1\end{bmatrix}$]{\label{tab:network_2}
    \begin{tabular}{lrrrr}
        \toprule
        $\beta$ & BBI  & LBFGS   & GD  & Adam\\
        \midrule
        1   &    0.01   & 0.1    &   0.01 & 0.0001\\
        5   &    0.01     & 0.1    &   0.01 & 0.001\\
        15  &    0.01     & 1    &   0.01  & 0.001\\
        30  &    0.01     & 0.001    &   0.001  & 0.001\\
        \bottomrule
    \end{tabular}}
    \captionsetup{position=bottom}
\end{table}


