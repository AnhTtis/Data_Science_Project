\section{Further details on the Curvature}\label{app:curvature_discretised}

\subsection{Curvature discretisation}
    To derive a discretised version of the curvature with respect to the training steps, consider the first-order approximation of \cref{eq:V} given by
    \begin{equation}
           \omegadot_{k + 1} \approx \omegavec_{k + 1} - \omegavec_k = \V_k.
    \end{equation}
    To obtain an approximation for the curvature in \cref{eq:kappa}, the following steps can be taken
    \begin{subequations}
        \begin{align}
            \omegaddot_{k + 1} &\approx \V_k - \V_{k-1}, \\
            |\omegadot_{k + 1}|  &\approx \sqrt{\langle\V_k, \V_k\rangle}, \\
             \frac{d}{dt}|\omegadot_{k + 1}| &\approx \frac{\langle\V_k, \V_k\rangle - \langle\V_k,\V_{k-1}\rangle}{\sqrt{\langle\V_{k},\V_k\rangle}}, \\
            \kappa_{t,k+1}
            & \approx \frac{1}{\sqrt{\langle\V_k,\V_k\rangle}} \left[\langle \V_{k-1},\V_{k-1}\rangle - \frac{\langle\V_{k-1},\V_k\rangle^2}{\langle\V_k,\V_k\rangle}\right]^{1/2}.
         \end{align}
    \end{subequations}

\subsection{Local curvature and the loss function}

Here we comment on the relation between $\kappa_\omega$, i.e. the time-independent local curvature of the training trajectory in the parameter space, and the loss function. Following a physics intuition, given an objective function, $\mathcal{L}(\boldsymbol{\omega})$, we tend to see it as a potential and assume that our trajectory lies on the hypersurface described by $\mathcal{L}(\boldsymbol{\omega})$. This is not true, as different optimisers follow different theories that modify the loss landscape and the regime (e.g. energy-conserving and friction-dominated motion). Nevertheless, given the form of \cref{eq:V}, we can reinterpret the optimiser as if it were a friction-dominated classical algorithm, such as GD. This gives us a hint about how large a gradient should be to induce that step in the GD algorithm, and it tells us how curved the distorted loss landscape, seen by the optimiser, is.

\cref{fig:loss_trajectory} shows a pictorial view of the relation between the loss function, or its distorted version seen by the optimiser, and the trajectory in the parameter space. The path followed on the loss hypersurface is depicted with a red curve, while its projection on the parameter space is in orange. The orange curve is precisely the focus of this study. Indeed, comparing trajectories living on different loss---or distorted loss---hypersurfaces is an ill-defined problem. On the other hand, monitoring the projected trajectories and their local curvature, $\kappa_\omega$, tells us how curved is the fictitious loss landscape seen by each optimiser. 

\begin{figure}[tbhp]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/distorted_loss.png}
    \caption{\label{fig:loss_trajectory}Pictorial view of the relation between the loss function (or its distorted version seen by the optimiser). The orange curve on the plane $\omega_1-\omega_2$ represents the trajectory followed by the optimiser on the parameter space.}
\end{figure}