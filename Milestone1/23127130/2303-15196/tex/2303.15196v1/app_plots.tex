\section{Additional Plots}
\label{app:plots}
\subsection{Training and Test Losses}
\label{app:losses}
Based on the performances in Fig.\,\ref{fig:error_beta} in the main text, we can divide the optimisers into two  groups:
\begin{itemize}
    \item \emph{LBFGS and ADAM are the optimisers that obtain the best results}. Although LBFGS converges faster, ADAM has the smallest error for the highest $\beta$ among all optimisers. Comparing train and test losses (\cref{fig:adam_loss,fig:lbfgd_loss}), we note that in both cases the generalisation errors (difference between test and train errors) become larger as $\beta$ increases.  Also, for large $\beta$, their losses are dominated by the periodic boundary contribution (last term  in Eq.\,\ref{eq:loss}), and test losses tend to overfit.
    \item \emph{The optimiser that performs worst is GD, followed by BBI}. In particular,  there is no learning for $\beta = 15$ and 30. In these cases,  the losses are dominated by the initial conditions (first term in Eq.\,\ref{eq:loss}), see Figs.\,\ref{fig:bbi_loss} and \ref{fig:gd_loss}.  The GD's poor performance is expected:  it has no momentum to be able to overcome saddle points and no mini-batches as a source of stochasticity that helps in escaping from local minima. On the other hand, explaining BBI's low performance requires further investigation as BBI's specialised hyperparameters offer a broad range of tuning possibilities \citep{de2022born}. In particular, training BBI for a longer time and exploring different strategies to trigger the bounces are possibilities worth exploring.
\end{itemize}


\begin{figure}[tbhp]
    \centering
    \subfloat[$\beta=1$, Network 1]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/adam_loss_nn0_b1.tex}}}
    \subfloat[$\beta=1$, Network 2]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/adam_loss_nn1_b1.tex}}}
    \\
    \subfloat[$\beta=5$, Network 1]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/adam_loss_nn0_b5.tex}}}
    \subfloat[$\beta=5$, Network 2]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/adam_loss_nn1_b5.tex}}}
    \\
    \subfloat[$\beta=15$, Network 1]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/adam_loss_nn0_b15.tex}}}
    \subfloat[$\beta=15$, Network 2]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/adam_loss_nn1_b15.tex}}}
    \\
    \subfloat[$\beta=30$, Network 1]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/adam_loss_nn0_b30.tex}}}
    \subfloat[$\beta=30$, Network 2]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/adam_loss_nn1_b30.tex}}}
    \caption{\label{fig:adam_loss}Median loss versus epoch over 10 samples when using ADAM. \emph{Solid} line is the total, and the other lines show the separate contributions. \emph{dashed} for initial condition, \emph{dotted} for the bulk, and \emph{dash-dotted} for the periodic boundary.}
\end{figure}

\begin{figure}[tbhp]
    \centering
    \subfloat[$\beta=1$, Network 1]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/bbi_loss_nn0_b1.tex}}}
    \subfloat[$\beta=1$, Network 1]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/bbi_loss_nn1_b1.tex}}}
    \\
    \subfloat[$\beta=5$, Network 1]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/bbi_loss_nn0_b5.tex}}}
    \subfloat[$\beta=5$, Network 1]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/bbi_loss_nn1_b5.tex}}}
    \\
    \subfloat[$\beta=15$, Network 2]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/bbi_loss_nn0_b15.tex}}}
    \subfloat[$\beta=15$, Network 2]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/bbi_loss_nn1_b15.tex}}}
    \\
    \subfloat[$\beta=30$, Network 2]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/bbi_loss_nn0_b30.tex}}}
    \subfloat[$\beta=30$, Network 2]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/bbi_loss_nn1_b30.tex}}}
    \caption{\label{fig:bbi_loss} Median loss versus epoch over 10 samples when using BBI. \emph{Solid} line is the total, and the other lines show the separate contributions. \emph{dashed} for initial condition, \emph{dotted} for the bulk, and \emph{dash-dotted} for the periodic boundary.}
\end{figure}

\begin{figure}[tbhp]
    \centering
    \subfloat[$\beta=1$, Network 1]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/lbfgs_loss_nn0_b1.tex}}}
    \subfloat[$\beta=1$, Network 2]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/lbfgs_loss_nn1_b1.tex}}}
    \\
    \subfloat[$\beta=5$, Network 1]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/lbfgs_loss_nn0_b5.tex}}}
    \subfloat[$\beta=5$, Network 2]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/lbfgs_loss_nn1_b5.tex}}}
    \\
    \subfloat[$\beta=15$, Network 1]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/lbfgs_loss_nn0_b15.tex}}}
    \subfloat[$\beta=15$, Network 2]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/lbfgs_loss_nn1_b15.tex}}}
    \\
    \subfloat[$\beta=30$, Network 1]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/lbfgs_loss_nn0_b30.tex}}}
    \subfloat[$\beta=30$, Network 2]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/lbfgs_loss_nn1_b30.tex}}}
    \caption{\label{fig:lbfgd_loss} Median loss versus epoch over 10 samples when using LBFGS. \emph{Solid} line is the total, and the other lines show the separate contributions. \emph{dashed} for initial condition, \emph{dotted} for the bulk, and \emph{dash-dotted} for the periodic boundary. Note the changing number of epochs for convergence.}
\end{figure}

\begin{figure}[tbhp]
    \centering
    \subfloat[$\beta=1$, Network 1]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/gd_loss_nn0_b1.tex}}}
    \subfloat[$\beta=1$, Network 2]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/gd_loss_nn1_b1.tex}}}
    \\
    \subfloat[$\beta=5$, Network 1]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/gd_loss_nn0_b5.tex}}}
    \subfloat[$\beta=5$, Network 2]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/gd_loss_nn1_b5.tex}}}
    \\
    \subfloat[$\beta=15$, Network 1]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/gd_loss_nn0_b15.tex}}}
    \subfloat[$\beta=15$, Network 2]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/gd_loss_nn1_b15.tex}}}
    \\
    \subfloat[$\beta=30$, Network 1]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/gd_loss_nn0_b30.tex}}}
    \subfloat[$\beta=30$, Network 2]{\adjustbox{width=0.42\linewidth,valign=b}{\input{figs/tikz/gd_loss_nn1_b30.tex}}}
    \caption{\label{fig:gd_loss} Median loss versus epoch over 10 when using GD. \emph{Solid} line is the total, and the other lines show the separate contributions. \emph{dashed} for initial condition, \emph{dotted} for the bulk, and \emph{dash-dotted} for the periodic boundary.}
\end{figure}

\clearpage

\subsection{Interpreting and visualizing training dynamics}
\label{app:training_dynamics}

This appendix aims to facilitate the understanding of the results shown in the main text, also providing a visualization when possible. Specifically, to better understand the behaviour of the trajectories, we calculate the Spearman correlation, $\rho$, between $\kappa$ and the MSE evaluated on the grid points. Spearman's correlation allows us to evaluate whether these quantities move together without fixing a functional form between the two. Furthermore, to understand the directions taken by the trajectory, we calculate the cosine similarity between two consecutive speeds:
\begin{equation}
    \cos (\theta_k) = \frac{\boldsymbol{\dot{\omega}}_k\cdot\boldsymbol{\dot{\omega}}_{k-1}}{\|\boldsymbol{\dot{\omega}}_k\| \|\boldsymbol{\dot{\omega}}_{k-1}\|} \qquad \mathrm{for} \qquad k=1,\dots,n_{\text{epochs}}\,.
\end{equation}

The graphs in \cref{figtab:all} show that the correlation between MSE and $\rho$ remains strongly negative (mainly $<-0.5$) in cases where the training reaches convergence (see, for example, the results where MSE $<0.01$). The third column in \cref{figtab:all} shows an example of the relation between $\kappa_\omega$ and MSE for $\beta = 1$ on the small (S) network. The colour of the dots varies from pale to intense following the number of evolution epochs. These examples show that the correlation between the curvature $\kappa_\omega$ and convergence remains negative even during the first stages of training. Therefore, the value of $\rho$ cannot be attributed only to some final spiralling in the last minimum of convergence. 
The rightmost column of \cref{figtab:all} shows an example (the same as the third column) of the relationship between the  ratio in magnitude and the cosine similarity of consecutive speed vectors. We see that while some optimisers show a preferred behaviour ($\theta_{\text{SGD}}$ tends to $\pi$ in the last stage of training, $\theta_{\text{Adam}}$ is nearly 0 before random dynamics kicks in, $\theta_{\text{BBI}}$ is always nearly 0), LBFGS does not show a preferred alignment with the gradient direction.


Finally, in \cref{fig:finalICMSE}, we show the relationship between the final values of curvature and MSE for each optimiser and $\beta$ value. In general, especially for gradient-based methods, it is possible to see an increase in the final intrinsic curvature value as the convergence increases (as MSE decreases).

\begin{table}[tbhp]
    \centering
    \caption{\label{figtab:all}Relation between convergence (MSE) and $\rho(\kappa_\omega, \text{MSE})$ or $\rho(\kappa_t, \text{MSE})$. Representative trajectory in $\kappa_\omega/\kappa_t$  and MSE space. Cosine similarity between consecutive speed vectors vs increase in magnitude in consecutive speeds.  The colour of the dots varies from pale to intense following the number of evolution epochs.}
    \begin{tabular}{M{1cm}M{3.1cm}M{3.1cm}M{3.1cm}M{3.1cm}N}
        \toprule
        & $\kappa_\omega$ & $\kappa_t$ & $\kappa$ vs. MSE & Trajectory \\\midrule
        GD & \includegraphics[width=3cm, height=3cm]{figures/Corr_IC_MSE_SGD.png} &    \includegraphics[width=3cm, height=3cm]{figures/Corr_Curv_MSE_SGD.png} & \includegraphics[width=3cm, height=3cm]{figures/MSE_C_IC_plot_NN0_SGD.png} & \includegraphics[width=3cm, height=3cm]{figures/turning_plot_NN0_SGD.png} \\
        BBI & \includegraphics[width=3cm, height=3cm]{figures/Corr_IC_MSE_BBI.png} & \includegraphics[width=3cm, height=3cm]{figures/Corr_Curv_MSE_BBI.png} & \includegraphics[width=3cm, height=3cm]{figures/MSE_C_IC_plot_NN0_BBI.png} & \includegraphics[width=3cm, height=3cm]{figures/turning_plot_NN0_BBI.png}\\
        Adam & \includegraphics[width=3cm, height=3cm]{figures/Corr_IC_MSE_Adam.png} & \includegraphics[width=3cm, height=3cm]{figures/Corr_Curv_MSE_Adam.png} & \includegraphics[width=3cm, height=3cm]{figures/MSE_C_IC_plot_NN0_Adam.png} & \includegraphics[width=3cm, height=3cm]{figures/turning_plot_NN0_Adam.png} \\
        LBFGS & \includegraphics[width=3cm, height=3cm]{figures/Corr_IC_MSE_LBFGS.png} & \includegraphics[width=3cm, height=3cm]{figures/Corr_Curv_MSE_LBFGS.png} & \includegraphics[width=3cm, height=3cm]{figures/MSE_C_IC_plot_NN0_LBFGS.png} & \includegraphics[width=3cm, height=3cm]{figures/turning_plot_NN0_LBFGS.png} \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[tbhp]
    \centering
    \caption{\label{fig:finalICMSE}Effect of optimiser on the relation between final values of MSE and curvatures $\kappa_\omega$ and $\kappa_t$}
    \begin{tabular}{M{1cm}M{3.1cm}M{3.1cm}M{3.1cm}M{3.1cm}}
        \toprule
        & GD & BBI & Adam & LBFGS\\\midrule
        $\kappa_\omega$ & \includegraphics[width=3cm, height=3cm]{figures/IC_MSE_SGD.png} & \includegraphics[width=3cm, height=3cm]{figures/IC_MSE_BBI.png} & \includegraphics[width=3cm, height=3cm]{figures/IC_MSE_Adam.png} & \includegraphics[width=3cm, height=3cm]{figures/IC_MSE_LBFGS.png} \\
        $\kappa_t$  &  \includegraphics[width=3cm, height=3cm]{figures/C_MSE_SGD.png} & \includegraphics[width=3cm, height=3cm]{figures/C_MSE_BBI.png} & \includegraphics[width=3cm, height=3cm]{figures/C_MSE_Adam.png} & \includegraphics[width=3cm, height=3cm]{figures/C_MSE_LBFGS.png}\\\bottomrule
    \end{tabular}
\end{table}
