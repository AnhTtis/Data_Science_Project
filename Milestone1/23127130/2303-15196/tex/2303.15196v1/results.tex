\section{Results and Discussion}\label{sec:results}
\cref{fig:error_beta} shows the median over 10 samples of the mean squared error (MSE) evaluated on the entire domain between the $\hat{u}$ predicted by the PINN and the analytical solution. Here the samples were formed by using 10 different random network initialisation. The corresponding training and test losses are shown in \cref{app:losses}. During testing, it was observed that the learning rate  significantly impacts the final performances; therefore, a grid search was performed over a range of learning rates to estimate the best configuration for each optimiser. See \cref{app:experiments} for the details. 
\vspace{-0.1cm}

Two multi-layer perception architectures were considered, with layer structures $S = [2, 25, 25, 1]$ and $L = [2, 50, 50, 50, 50, 1]$ resulting in \num{751} and \num{7851} trainable parameters respectively\footnote{These values keep the perturbative behaviour of the NN constant, i.e. depth/width constant~\citep{roberts_yaida_hanin_2022}.}. The number of training points was around 2000, as described in \cref{app:experiments}; therefore, the larger network is in the over-parameterised regime, i.e. it has more parameters than training data. \cref{fig:error_beta} shows that the large network generally has a lower error, in agreement with common knowledge in ML\footnote{From classical statistical learning, one expects that over-parameterised models over-fit. However, there is overwhelming evidence that large models generalise well in several cases of interest, e.g. \cite{Szegedy2015, huang2019gpipe}.}. This effect is most noticeable for BBI \footnote{The total phase space volume for BBI  can be analytically estimated and is parametrically large as the objective function is close to zero with an exponential dependence on the number of dimensions (see Appendix A.3 in \cite{de2022born}).}. A significant observation is that for all configurations of NN and optimiser, PINNs failed to produce good approximations for systems with large values of $\beta$.
\vspace{-0.1cm}

In \cref{fig:error_kappa}, we show the inverse relation  between the final values of MSE and the curvature $\kappa_\omega$. Here, only $\beta = 1,5$ were considered as they converged to reasonable errors for most of the optimisers, see \cref{app:training_dynamics}. Significantly, LBFGS achieves the highest values of $\kappa_\omega$ and the lowest error. Moreover, LBFGS has significant memory overhead but it typically converges in  fewer epochs compared to the other optimisers. Note that such a negative correlation in \cref{fig:error_kappa} links generalisation (measured by the MSE on the entire domain) with the curvature $\kappa_\omega$. Deriving their exact relation is non-trivial and we leave further investigations for future work. Nevertheless, we would like to stress that the high curvature values is not solely associated with orbiting the final local or global minima. In fact, this is due to $\kappa_\omega$ and MSE being negatively correlated throughout the whole trajectory, including its initial stages. We show examples of this behavior in \cref{app:training_dynamics}. 

\begin{figure}
    \centering
    \subfloat[][]{\label{fig:error_beta}\adjustbox{width=0.42\linewidth, valign=b}{\input{./figs/tikz/beta_error_comp.tex}}}
    ~
    \subfloat[][]{\label{fig:error_kappa}\adjustbox{width=0.52\linewidth, valign=b}{\includegraphics{./figures/Final_curvaturesv2.png}}}
    \caption{(a) Median MSE over 10 samples for various optimisers and $\beta$ values. \emph{Solid}/\emph{dashed} lines refer to small/large (S/L) network architectures. (b) Relation between final values of convergence (MSE)  and  curvature ($\kappa_\omega$) for $\beta=1$ (left) and $5$ (right). Dots and crosses refer to small (S) and large (L) networks, respectively. \label{fig:error}}
\vspace{-0.3cm}
\end{figure}
 



%\begin{table}[ht]
%  \centering 
%  \begin{tabular}{M{5.1cm}M{5.1cm}}
%     $\qquad\qquad\qquad\qquad\beta = 1$     &       $\quad\beta = 5$ \\
%     \multicolumn{2}{r}{\includegraphics[width=10cm]{figures/Final_curvatures.png} }  
%     \vspace{-0.5cm}
%  \end{tabular}
%  \caption{\label{tab:MSEvskappa} Relation between final values of convergence (MSE of $u(x,t)$)  and intrinsic curvature ($\kappa_\omega$). S/L refer to small/large networks.}
%\end{table}
%}

% \vspace{-0.5cm}

\noindent
\textsc{Outlook.} In contrast to traditional ML tasks such as image recognition and NLP, ML methods applied to science require more accurate models which, in general, are trained with high-quality data. This suggests that the training process on data from physical phenomena can be fundamentally different from common ML applications. A promising future research direction would be exploring the connection between model generalisation and the flatness of minima for problems requiring high accuracy. For example, the work of \cite{dinh2017sharp, huang2020understanding} discuss this in the context of traditional ML tasks. Finally, in the future, it would be insightful to compare Hessian-based methods such as that of \cite{Michaud2023}, with our approach linking accuracy and local trajectory curvature.









 






