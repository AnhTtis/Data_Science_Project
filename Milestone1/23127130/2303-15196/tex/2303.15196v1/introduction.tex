\section{Introduction}\label{sec:intro}
\vspace{-0.1cm}  
%\hnf{An attempt to reduce the literature review. All the original text is below btw begin(comments) ... end(comments)}

The idea of solving PDE problems using neural networks (NNs) was put forward by~\cite{Lagaris:1997ap, Lagaris_1998, 870037} in the second half of the '90s and then revised in 2017 by \cite{raissi2017physics_1,raissi2017physics_2} who named the methodology Physics-Informed Neural Networks (PINNs).  Relying on the universal approximation theorem of~\cite{cybenko1989approximation, HORNIK1991251, pinkus_1999}, PINNs aim to deliver a universal regressor that can represent any bounded continuous function and solve any PDE/ODE problems, having input and output shape as the only limitation. Although the goal of PINNs was to produce a  unifying method of solving PDE/ODEs, satisfactory results could not be achieved in a multitude of cases. The recent works of \cite{karniadakis2021physics,hao2022physics} provide an overview of the state-of-the-art; furthermore, \cite{cuomo2022scientific} focus on algorithms and applications and \cite{beck2020overview} on theoretical results.
\vspace{-0.1cm}

Several studies have analysed the effects of choosing different architectures, loss function formulations, and treatment of domain and collocation points. However, the effect of the optimiser choice on PINN performance needs more attention. Recently, new PINN-specific optimisation methods were developed or applied to improve poor convergence performance. For example, \cite{de2022born} propose a \emph{relativistic} optimisation algorithm that introduces chaotic jumps and \cite{davi2022pso} use a metaheuristic optimisation method, namely, particle swarm optimisation, to eliminate gradient-related problems. 

%Most of the aforementioned methods focus on a specific class of PDEs, and we are far from providing a unique PINN framework.

\begin{comment}
\hvg{TO MODIFY AND CUT}
The idea of solving PDE problems using neural networks (NNs) was put forward by~\cite{Lagaris:1997ap, Lagaris_1998, 870037} in the second half of the '90s and then revised in 2017 by \cite{raissi2017physics_1,raissi2017physics_2} who named the methodology Physics-Informed Neural Networks (PINNs).  Relying on the universal approximation theorem of~\cite{cybenko1989approximation, HORNIK1991251, pinkus_1999}, PINNs aim to deliver a universal regressor that can represent any bounded continuous function and solve any PDE/ODE problems, having their input and output shape as the only limitation. Although the goal of introducing PINNs was to unify the solution method of ODE/PDEs, it was soon clear that their na\"ive implementation did not lead to satisfactory results in a multitude of cases. Several recent reviews, such as ~\cite{karniadakis2021physics,hao2022physics}, allow us to have an overall picture of the state of the art of PINNs; ~\cite{cuomo2022scientific} focus on algorithms and applications, ~\cite{beck2020overview} on theoretical results.


Poor convergence properties (see, e.g., \cite{kharazmi2019variational,wang2021understanding,saadat2022neural,yu2022gradient,wang2022and}) caused studies on analyzing and modifying parts of the PINNs training pipeline to multiply. For example, using the Neural Tangent Kernel (NTK) approach of \cite{jacot2018neural}, \cite{wang2022and,saadat2022neural} show that PINNs are affected by a spectral bias in that they struggle to learn high-frequency components.


%Some works focused on rephrasing PINNs optimization using classical Physics formulations, such as Lagrangian or Hamiltonian equations \cite{cranmer2020lagrangian,lutter2019deep,greydanus2019hamiltonian,toth2019hamiltonian}.
%Others extend the applicability of PINNs to irregular domains \cite{sun2020surrogate,gao2022physaadat2022neuralsics, liu2022unified,leake2020deep,schiassi2021extreme}. %To reduce the computational time or the number of collocation points needed to evaluate higher-order derivatives, some methods replace analytical with numerical differentiation \cite{sirignano2018dgm, chiu2022can} or integration \cite{patel2022thermodynamically}.

In addition, to resolve conflicts shown by \cite{wang2021understanding} among the components of the loss function during its minimization, \cite{wang20222,wang2021understanding, wang2022and} aimed at reweighting the loss terms, \cite{zhang2021survey, wang2022respecting,maddu2022inverse} defined learning rate annealing methods to balance optimization, and \cite{wu2023comprehensive,tang2021deep,nabian2021efficient,das2022state,daw2022rethinking,subramanian2022adaptive} introduced new schedules to (re-)sample collocation points and boost convergence.

Many other studies involved modifying the optimization goal of PINNs. \cite{kharazmi2019variational, yu2018deep, zang2020weak, chiu2022can} modified the loss function, while \cite{yu2022gradient, son2021sobolev, maddu2022inverse} introduced additional regularization terms to aid convergence. Other approaches, such as \cite{yu2018deep, khodayi2020varnet, kharazmi2019variational,zang2020weak,bao2020numerical,kharazmi2021hp}, transform the PINN method into a variational optimization problem for the residuals.

The multi-scale behavior of certain physical systems (turbulence, chaoticity, etc..) cannot be captured by simple MLP architectures, e.g., \cite{steger2022how}. 
\cite{zhang2020physics, ren2022phycrnet,geneva2020modeling,koryagin2019pydens, sirignano2018dgm} tackle this problem by using specific LSTMs/RNNs architecture to detect sharp turns in temporal data. \cite{tompson2017accelerating,Geneva_2020,Thuerey_2020,ren2022phycrnet, gao2021phygeonet, liu2022predicting} use CNNs on regular grid data points, while \cite{gao2021phygeonet} aim to extend the use of CNN PINNs to irregular domains. Other works borrow techniques from recent deep learning developments such as transfer- (\cite{desai2021one, chakraborty2021transfer}) and meta-learning (\cite{psaros2022meta, liu2022novel}).

Some works aim to help convergence in multi-scale and large-scale systems by decomposing the optimization problem.
 In particular, \cite{jagtap2020conservative,jagtap2020extended,jagtap2021extended, moseley2021finite,patel2022thermodynamically,kharazmi2021hp} use domain decomposition, \cite{wang2021eigenvector,tancik2020fourier,mildenhall2020nerf} exploit feature preprocessing techniques, and \cite{haghighat2021physics,lin2021seamless,mojgani2022lagrangian} partition tasks using multiple MLPs not sharing weights. Again, to make PINNs output more flexible, \cite{sitzmann2020implicit,wong2021learning,saadat2022neural} tried to use non-conventional activation functions, such as sines, while \cite{jagtap2020adaptive,jagtap2020locally} aimed at tailoring activation functions to deal with multi-scale physical systems and the gradient vanishing problem.



Although several studies focused on the effects of choosing different architectures,  loss function formulations, and treatment of domain and collocation points, more attention is needed on comparing PINNs' performance using different optimizers. Indeed, most studies using more optimizers merely present empirical results: \cite{markidis2021old,liu2022physics,shin2020convergence,penwarden2021physics}. Recently, some new PINN optimization methods were developed or applied to solve poor convergence performance. \cite{de2022born} proposes a "relativistic" optimization algorithm that solves stochastic optimizers' diffusive properties and introduces chaotic jumps. \cite{davi2022pso} uses a metaheuristic optimization method, namely particle swarm optimization, to eliminate gradient-related problems. 

%Most of the aforementioned methods focus on a specific class of PDEs, and we are far from providing a unique PINN framework.
\end{comment}
\vspace{-0.1cm}

This work aimed to improve the understanding of how PINNs performance is affected by  optimiser choice. Specifically, we considered the following algorithms spanning different optimiser categories: gradient descent (GD) without momentum, LBFGS \citep{Dennis1973ACO, Goodfellow-et-al-2016} (a second order quasi-Newton method), ADAM \citep{kingma2014adam} (an adaptive stochastic GD algorithm), and bouncing Born--Infeld (BBI) \citep{de2022born}. The first three optimisers are widely used in ML optimisation problems, whereas BBI is a recent realisation of a frictionless energy-conserving optimiser. See \cref{app:bbi} for details. 

\vspace{-0.1cm}
Moreover, we introduce a new method to study optimisation in neural networks, which provides training trajectory curvature data at low computational cost. Using this approach, we studied the evolution of different optimisers through the network parameter space during training. Further discussion is given in \cref{sec:curvature}. 

\vspace{-0.1cm}

%Furthermore, we changed the complexity of the loss function by varying the stiffness of the hyperbolic PDE.

%Due to the non-trivial  structure of the loss function bulk term, a simple PDE with hierarchical coefficients can exhibit a highly non-convex loss landscape.


Specifically, we apply PINNs to solve the linear advection equation  as described in \cref{sec:linear_advection}. Linear advection is a simple PDE with a single parameter, i.e., the wave speed $\beta$, whose variation can tune the complexity of the PINN landscape~\citep{Krishnapriyan2021}. Moreover, to understand how a network adjusts to different depths and widths, we considered two configurations of multi-layer perceptron architectures with different numbers of hidden layers and nodes per layer.

\vspace{-0.1cm}
The main contributions of this work are:
\begin{itemize}[leftmargin=.75cm]
    \item We found that the choice of optimisation algorithm significantly impacts the convergence of PINNs models. Specifically, second-order optimisers that do not directly follow gradient directions, such as LBFGS, performed better, as shown in \cref{fig:error_beta}.
    \item We introduce a new low-cost method for studying the dynamics of optimisers.
    This relies on defining a local reference frame and evaluating the local curvature of the training trajectory in the NN parameter space.   
    %This allowed us to reveal a non-trivial relation between the training trajectory local curvature in the parameter space and generalisation---measured by the convergence error on the entire domain.
    \item We found a negative correlation between PINNs convergence error and the newly introduced training trajectory curvature which is shown in \cref{fig:error_kappa}. This implies that good PINNs solutions lie in highly curved regions of the optimiser reference frame. Therefore, making PINNs converge is hard for those techniques designed to explore shallow landscapes that generalise well under perturbation. 
\vspace{-0.3cm}
\end{itemize}

