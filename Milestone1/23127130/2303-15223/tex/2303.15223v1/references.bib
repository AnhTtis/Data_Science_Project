
@INPROCEEDINGS{Moore99,
  AUTHOR =       "R. Moore and J. Lopes",
  TITLE =        "Paper templates",
  BOOKTITLE =    "TEMPLATE'06, 1st International Conference on Template Production",
  YEAR =         "1999",
  publisher =    "SCITEPRESS",
  file = F
}

@BOOK{Smith98,
  AUTHOR =       "J. Smith",
  TITLE =        "The Book",
  PUBLISHER =    "The publishing company",
  YEAR =         "1998",
  address =      "London",
  edition =      "2nd",
  file = F
}

@book{communicationTheory,
  title={Communication Theory},
  author={Mortensen, C.D.},
  isbn={9781351527521},
  url={https://books.google.com.ua/books?id=pNwzDwAAQBAJ},
  year={2017},
  publisher={Taylor \& Francis}
}

@inproceedings{GoodfellawGAN,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Nets},
 volume = {27},
 year = {2014}
}

@inproceedings{StyleGAN2,
  title     = {Analyzing and Improving the Image Quality of {StyleGAN}},
  author    = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  booktitle = {Proc. CVPR},
  year      = {2020}
}

@article{stargan,
  doi = {10.48550/ARXIV.1711.09020},
  url = {https://arxiv.org/abs/1711.09020},
  author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation},
  year = {2017}
}

@article{affecnet,
	doi = {10.1109/taffc.2017.2740923},
	url = {https://doi.org/10.1109\%2Ftaffc.2017.2740923},
	year = 2019,
	month = {jan},
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume = {10},
	number = {1},
	pages = {18--31},
	author = {Ali Mollahosseini and Behzad Hasani and Mohammad H. Mahoor},
	title = {{AffectNet}: A Database for Facial Expression, Valence, and Arousal Computing in the Wild},
	journal = {{IEEE} Transactions on Affective Computing}
}

@misc{affectnet_hq1,
  doi = {10.48550/ARXIV.2109.07270},
  url = {https://arxiv.org/abs/2109.07270},
  author = {Wen, Zhengyao and Lin, Wenzhong and Wang, Tao and Xu, Ge},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Distract Your Attention: Multi-head Cross Attention Network for Facial Expression Recognition},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{affectnet_hq2,
  doi = {10.48550/ARXIV.2105.06070},
  url = {https://arxiv.org/abs/2105.06070},
  author = {Yang, Tao and Ren, Peiran and Xie, Xuansong and Zhang, Lei},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {GAN Prior Embedded Network for Blind Face Restoration in the Wild},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{jaffe1,
  author       = {Michael J Lyons and
                  Miyuki Kamachi and
                  Jiro Gyoba},
  title        = {{Coding Facial Expressions with Gabor Wavelets (IVC 
                   Special Issue)}},
  month        = sep,
  year         = 2020,
  note         = {{This manuscript is a modified version of a 
                   conference article, that was invited for
                   publication in a special issue of Image and Vision
                   Computing dedicated to a selection of articles
                   from the IEEE Face \&amp; Gesture 1998 conference.
                   The special issue never materialized.}},
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.4029680},
  url          = {https://doi.org/10.5281/zenodo.4029680}
}

@unpublished{jaffe2,
  author       = {Lyons, Michael J.},
  title        = {{"Excavating AI" Re-excavated: Debunking a 
                   Fallacious Account of the JAFFE Dataset}},
  note         = {{n.b. All JAFFE images in this article are subject 
                   to specific terms of use and may not be reused
                   without permission, regardless of the license
                   applied to the document as a whole.}},
  month        = jul,
  year         = 2021,
  doi          = {10.5281/zenodo.5147170},
  url          = {https://doi.org/10.5281/zenodo.5147170}
}

@INPROCEEDINGS{MMI,  author={Pantic, M. and Valstar, M. and Rademaker, R. and Maat, L.},  booktitle={2005 ICME},   title={Web-based database for facial expression analysis},   year={2005},  volume={},  number={},  pages={5 pp.-},  doi={10.1109/ICME.2005.1521424}}

@article{RaFD,
author = { Oliver   Langner  and  Ron   Dotsch  and  Gijsbert   Bijlstra  and  Daniel H. J.   Wigboldus  and  Skyler T.   Hawk  and  Ad   van Knippenberg },
title = {Presentation and validation of the Radboud Faces Database},
journal = {Cognition and Emotion},
volume = {24},
number = {8},
pages = {1377-1388},
year  = {2010},
publisher = {Routledge},
doi = {10.1080/02699930903485076},

URL = { 
        https://doi.org/10.1080/02699930903485076
    
},
eprint = { 
        https://doi.org/10.1080/02699930903485076
    
}
}

@misc{Stirling,
  title = {stirling/esrc 3d face database},
  howpublished = {\url{http://pics.stir.ac.uk/ESRC/}},
  note = {Database hosted by the Psychological Image Collection at Stirling (PICS)}
}

@ARTICLE{Ekman,
  title    = "Constants across cultures in the face and emotion",
  author   = "Ekman, P and Friesen, W V",
  journal  = "J Pers Soc Psychol",
  volume   =  17,
  number   =  2,
  pages    = "124--129",
  month    =  feb,
  year     =  1971,
  address  = "United States",
  language = "en"
}

@article{FER2,
author = {Flávio Altinier Maximiano da Silva and Helio Pedrini},
title = {{Effects of cultural characteristics on building an emotion classifier through facial expression analysis}},
volume = {24},
journal = {Journal of Electronic Imaging},
number = {2},
publisher = {SPIE},
pages = {1 -- 9},
year = {2015},
doi = {10.1117/1.JEI.24.2.023015},
URL = {https://doi.org/10.1117/1.JEI.24.2.023015}
}

@ARTICLE{FER3,  author={Li, Shan and Deng, Weihong},  journal={IEEE Transactions on Affective Computing},   title={Deep Facial Expression Recognition: A Survey},   year={2020},  volume={},  number={},  pages={1-1},  doi={10.1109/TAFFC.2020.2981446}}

@article{FER4,
title = {Facial expression recognition with Convolutional Neural Networks: Coping with few data and the training sample order},
journal = {Pattern Recognition},
volume = {61},
pages = {610-628},
year = {2017},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2016.07.026},
url = {https://www.sciencedirect.com/science/article/pii/S0031320316301753},
author = {André Teixeira Lopes and Edilson {de Aguiar} and Alberto F. {De Souza} and Thiago Oliveira-Santos},
keywords = {Facial expression recognition, Convolutional Neural Networks, Computer vision, Machine learning, Expression specific features}
}

@ARTICLE{FER7,
  title    = "A survey on Image Data Augmentation for Deep Learning",
  author   = "Shorten, Connor and Khoshgoftaar, Taghi M",
  journal  = "Journal of Big Data",
  volume   =  6,
  number   =  1,
  pages    = "60",
  month    =  jul,
  year     =  2019
}

@INPROCEEDINGS{FER8,  author={Simard, P.Y. and Steinkraus, D. and Platt, J.C.},  booktitle={Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings.},   title={Best practices for convolutional neural networks applied to visual document analysis},   year={2003},  volume={},  number={},  pages={958-963},  doi={10.1109/ICDAR.2003.1227801}}

@INPROCEEDINGS{FER9,  author={Lin, Feng and Hong, Richang and Zhou, Wengang and Li, Houqiang},  booktitle={2018 25th IEEE International Conference on Image Processing (ICIP)},   title={Facial Expression Recognition with Data Augmentation and Compact Feature Learning},   year={2018},  volume={},  number={},  pages={1957-1961},  doi={10.1109/ICIP.2018.8451039}}

@INPROCEEDINGS{FER10,  author={Yi, Wei and Sun, Yaoran and He, Sailing},  booktitle={2018 PIERS-Toyama},   title={Data Augmentation Using Conditional GANs for Facial Emotion Recognition},   year={2018},  volume={},  number={},  pages={710-714},  doi={10.23919/PIERS.2018.8598226}}

@misc{FER11,
  doi = {10.48550/ARXIV.1711.00648},
  url = {https://arxiv.org/abs/1711.00648},
  author = {Zhu, Xinyue and Liu, Yifan and Qin, Zengchang and Li, Jiahong},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Data Augmentation in Emotion Classification Using Generative Adversarial Networks},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{FER13,
title = {Enhancing CNN with Preprocessing Stage in Automatic Emotion Recognition},
journal = {Procedia Computer Science},
volume = {116},
pages = {523-529},
year = {2017},
note = {Discovery and innovation of computer science technology in artificial intelligence era: The 2nd International Conference on Computer Science and Computational Intelligence (ICCSCI 2017)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.10.038},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917320860},
author = {Diah Anggraeni Pitaloka and Ajeng Wulandari and T. Basaruddin and Dewi Yanti Liliana},
keywords = {Emotion recognition, Facial expression, Convolutional neural network, Machine learning, Computer vision, Normalization}
}

@article{FER14,
title = {Boosted NNE collections for multicultural facial expression recognition},
journal = {Pattern Recognition},
volume = {55},
pages = {14-27},
year = {2016},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2016.01.032},
url = {https://www.sciencedirect.com/science/article/pii/S0031320316000534},
author = {Ghulam Ali and Muhammad Amjad Iqbal and Tae-Sun Choi},
keywords = {Ensemble classifier, Neural network ensemble collections, Facial expression recognition, Binary neural network}
}

@article{FER15,
title = {Facial expression recognition with local prominent directional pattern},
journal = {Signal Processing: Image Communication},
volume = {74},
pages = {1-12},
year = {2019},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2019.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0923596518306556},
author = {Farkhod Makhmudkhujaev and M. Abdullah-Al-Wadud and Md Tauhid Bin Iqbal and Byungyong Ryu and Oksam Chae},
keywords = {LDPD, Histogram of directional variations, Prominent directions, Shape pattern, Facial expression recognition}
}

@article{FER16,
title = {Facial expression recognition based on Local Binary Patterns: A comprehensive study},
journal = {Image and Vision Computing},
volume = {27},
number = {6},
pages = {803-816},
year = {2009},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2008.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0262885608001844},
author = {Caifeng Shan and Shaogang Gong and Peter W. McOwan},
keywords = {Facial expression recognition, Local Binary Patterns, Support vector machine, Adaboost, Linear discriminant analysis, Linear programming}
}

@article{FER17,
title = {Facial decomposition for expression recognition using texture/shape descriptors and SVM classifier},
journal = {Signal Processing: Image Communication},
volume = {58},
pages = {300-312},
year = {2017},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2017.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0923596517301406},
author = {Khadija Lekdioui and Rochdi Messoussi and Yassine Ruichek and Youness Chaabi and Raja Touahni},
keywords = {Facial components, Texture and shape descriptors, Facial expression recognition, SVM classifier, ROI extraction}
}

@article{FER18,
title = {Facial expression recognition using radial encoding of local Gabor features and classifier synthesis},
journal = {Pattern Recognition},
volume = {45},
number = {1},
pages = {80-91},
year = {2012},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2011.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0031320311002056},
author = {Wenfei Gu and Cheng Xiang and Y.V. Venkatesh and Dong Huang and Hai Lin},
keywords = {Facial expression recognition, Radial grid encoding, Fisher linear discriminant, Classifier synthesis, Gabor filter, Human visual cortex}
}

@article{FER19,
title = {Linear subspaces for facial expression recognition},
journal = {Signal Processing: Image Communication},
volume = {29},
number = {1},
pages = {177-188},
year = {2014},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2013.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0923596513001641},
author = {Niki Aifanti and Anastasios Delopoulos},
keywords = {Face analysis, Expression recognition, Subspaces}
}

@INPROCEEDINGS{FER20,  author={Hasani, Behzad and Mahoor, Mohammad H.},  booktitle={12th IEEE FG 2017},   title={Spatio-Temporal Facial Expression Recognition Using Convolutional Neural Networks and Conditional Random Fields},   year={2017},  volume={},  number={},  pages={790-795},  doi={10.1109/FG.2017.99}}

@INPROCEEDINGS{FER21,  author={Mollahosseini, Ali and Chan, David and Mahoor, Mohammad H.},  booktitle={2016 IEEE WACV},   title={Going deeper in facial expression recognition using deep neural networks},   year={2016},  volume={},  number={},  pages={1-10},  doi={10.1109/WACV.2016.7477450}}

@INPROCEEDINGS{FER22,  author={Zavarez, Marcus Vinicius and Berriel, Rodrigo F. and Oliveira-Santos, Thiago},  booktitle={2017 30th SIBGRAPI},   title={Cross-Database Facial Expression Recognition Based on Fine-Tuned Deep Convolutional Network},   year={2017},  volume={},  number={},  pages={405-412},  doi={10.1109/SIBGRAPI.2017.60}}

@article{FER24,
title = {Feature augmentation for imbalanced classification with conditional mixture WGANs},
journal = {Signal Processing: Image Communication},
volume = {75},
pages = {89-99},
year = {2019},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2019.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0923596518308336},
author = {Yinghui Zhang and Bo Sun and Yongkang Xiao and Rong Xiao and YunGang Wei},
keywords = {Imbalanced classification, Feature augmentation, Generative adversarial nets, Wasserstein distance}
}

@INPROCEEDINGS{FER25,  author={Chu, Qian and Hu, Min and Wang, Xiaohua and Gu, Yu and Chen, Tian},  booktitle={2019 IEEE 6th International CCIS},   title={Facial Expression Recognition Based on Contextual Generative Adversarial Network},   year={2019},  volume={},  number={},  pages={120-125},  doi={10.1109/CCIS48116.2019.9073699}}

@article{DAGAN,
author = {Porcu, Simone and Floris, Alessandro and Atzori, Luigi},
year = {2020},
month = {11},
pages = {},
title = {Evaluation of Data Augmentation Techniques for Facial Expression Recognition Systems},
volume = {9},
journal = {Electronics},
doi = {10.3390/electronics9111892}
}

@misc{AWS,
  title = {Amazon Rekognition},
  howpublished = {\url{https://aws.amazon.com/rekognition/?nc1=h_ls}}
}

@misc{Affectiva,
  title = {Affectiva},
  howpublished = {\url{https://www.affectiva.com/}}
}

@misc{google,
  title = {Google Visual AI},
  howpublished = {\url{https://cloud.google.com/vision/}}
}

@misc{microsoft,
  title = {Microsoft Azure},
  howpublished = {\url{https://docs.microsoft.com/en-us/azure/cognitive-services/face/overview}}
}

@misc{vgg,
  title = {Visual Geometry Group Oxford University (VGG Face)},
  howpublished = {\url{https://www.robots.ox.ac.uk/~vgg/software/vgg_face/}}
}

@INPROCEEDINGS{ck,  author={Lucey, Patrick and Cohn, Jeffrey F. and Kanade, Takeo and Saragih, Jason and Ambadar, Zara and Matthews, Iain},  booktitle={2010 IEEE CVPRW},   title={The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression},   year={2010},  volume={},  number={},  pages={94-101},  doi={10.1109/CVPRW.2010.5543262}}

@INPROCEEDINGS{expw,
 author = {Zhanpeng Zhang, Ping Luo, Chen Change Loy and Xiaoou Tang},
 title = {From Facial Expression Recognition to Interpersonal Relation Prediction},
 month = September,
 year = {2016}
}

@INPROCEEDINGS{MUG,  author={Aifanti, Niki and Papachristou, Christos and Delopoulos, Anastasios},  booktitle={11th International Workshop on Image Analysis for Multimedia Interactive Services WIAMIS 10},   title={The MUG facial expression database},   year={2010},  volume={},  number={},  pages={1-4},  doi={}}

@article{oulu,
title = {Facial expression recognition from near-infrared videos},
journal = {Image and Vision Computing},
volume = {29},
number = {9},
pages = {607-619},
year = {2011},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2011.07.002},
author = {Guoying Zhao and Xiaohua Huang and Matti Taini and Stan Z. Li and Matti Pietikäinen}
}

@ARTICLE{NCBI,
  title    = "A Brief Review of Facial Emotion Recognition Based on Visual
              Information",
  author   = "Ko, Byoung Chul",
  abstract = "Facial emotion recognition (FER) is an important topic in the
              fields of computer vision and artificial intelligence owing to
              its significant academic and commercial potential. Although FER
              can be conducted using multiple sensors, this review focuses on
              studies that exclusively use facial images, because visual
              expressions are one of the main information channels in
              interpersonal communication. This paper provides a brief review
              of researches in the field of FER conducted over the past
              decades. First, conventional FER approaches are described along
              with a summary of the representative categories of FER systems
              and their main algorithms. Deep-learning-based FER approaches
              using deep networks enabling ``end-to-end'' learning are then
              presented. This review also focuses on an up-to-date hybrid
              deep-learning approach combining a convolutional neural network
              (CNN) for the spatial features of an individual frame and long
              short-term memory (LSTM) for temporal features of consecutive
              frames. In the later part of this paper, a brief review of
              publicly available evaluation metrics is given, and a comparison
              with benchmark results, which are a standard for a quantitative
              comparison of FER researches, is described. This review can serve
              as a brief guidebook to newcomers in the field of FER, providing
              basic knowledge and a general understanding of the latest
              state-of-the-art studies, as well as to experienced researchers
              looking for productive directions for future work.",
  journal  = "Sensors (Basel)",
  volume   =  18,
  number   =  2,
  month    =  jan,
  year     =  2018,
  keywords = "conventional FER; convolutional neural networks; deep
              learning-based FER; facial action coding system; facial action
              unit; facial emotion recognition; long short term memory",
  language = "en"
}

@book{TFEID,
  author = {Chen, L.F. and Yen, Y.S.},
  date = {2007},
  title = {Taiwanese Facial Expression Image Database},
  publisher = {Brain Mapping Laboratory, Institute of Brain Science, National Yang-Ming University},
  language = {en},
  address = {Taipei}
}

@INPROCEEDINGS{BU-3DFE,  author={Lijun Yin and Xiaozhou Wei and Yi Sun and Jun Wang and Rosato, M.J.},  booktitle={7th International FGR 2006},   title={A 3D facial expression database for facial behavior research},   year={2006},  volume={},  number={},  pages={211-216},  doi={10.1109/FGR.2006.6}}

@ARTICLE{ISED,  author={Happy, S L and Patnaik, Priyadarshi and Routray, Aurobinda and Guha, Rajlakshmi},  journal={IEEE Transactions on Affective Computing},   title={The Indian Spontaneous Expression Database for Emotion Recognition},   year={2017},  volume={8},  number={1},  pages={131-142},  doi={10.1109/TAFFC.2015.2498174}}

@ARTICLE{GEMEP,
  title    = "Introducing the Geneva Multimodal expression corpus for
              experimental research on emotion perception",
  author   = "B{\"a}nziger, Tanja and Mortillaro, Marcello and Scherer, Klaus R",
  abstract = "Research on the perception of emotional expressions in faces and
              voices is exploding in psychology, the neurosciences, and
              affective computing. This article provides an overview of some of
              the major emotion expression (EE) corpora currently available for
              empirical research and introduces a new, dynamic, multimodal
              corpus of emotion expressions, the Geneva Multimodal Emotion
              Portrayals Core Set (GEMEP-CS). The design features of the corpus
              are outlined and justified, and detailed validation data for the
              core set selection are presented and discussed. Finally, an
              associated database with microcoded facial, vocal, and body
              action elements, as well as observer ratings, is introduced.",
  journal  = "Emotion",
  volume   =  12,
  number   =  5,
  pages    = "1161--1179",
  month    =  nov,
  year     =  2011,
  address  = "United States",
  language = "en"
}

@article{faces,
  author = {Ebner, N.C. and Riediger, M. and Lindenberger, U.},
  date = {2010},
  title = {FACES - A database of facial expressions in young, middle-aged, and older women and men: Development and validation},
  volume = {42},
  pages = {351–362},
  doi = {10.3758/BRM.42.1.351.},
  language = {en},
  journal = {Behavior Research Methods},
  number = {1}
}

@misc{KDEF,
  author = {Lundqvist, D. and Flykt, A. and Öhman, A.},
  year = {1998},
  title = {The Karolinska Directed Emotional Faces—KDEF},
  publisher = {"Karolinska Institute, Department of Clinical Neuroscience, Psychology Section"},
  language = {en},
  type = {(CD ROM).},
  address = {Stockholm}
}

@InProceedings{bosphorus,
author="Savran, Arman
and Aly{\"u}z, Ne{\c{s}}e
and Dibeklio{\u{g}}lu, Hamdi
and {\c{C}}eliktutan, Oya
and G{\"o}kberk, Berk
and Sankur, B{\"u}lent
and Akarun, Lale",
editor="Schouten, Ben
and Juul, Niels Christian
and Drygajlo, Andrzej
and Tistarelli, Massimo",
title="Bosphorus Database for 3D Face Analysis",
booktitle="Biometrics and Identity Management",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="47--56",
abstract="A new 3D face database that includes a rich set of expressions, systematic variation of poses and different types of occlusions is presented in this paper. This database is unique from three aspects: i) the facial expressions are composed of judiciously selected subset of Action Units as well as the six basic emotions, and many actors/actresses are incorporated to obtain more realistic expression data; ii) a rich set of head pose variations are available; and iii) different types of face occlusions are included. Hence, this new database can be a very valuable resource for development and evaluation of algorithms on face recognition under adverse conditions and facial expression analysis as well as for facial expression synthesis.",
isbn="978-3-540-89991-4"
}

@ARTICLE{fera,
  title    = "{FERA} Addressing Head Pose in the Third Facial Expression
              Recognition and Analysis Challenge",
  author   = "Valstar, Michel F and S{\'a}nchez-Lozano, Enrique and Cohn,
              Jeffrey F and Jeni, L{\'a}szl{\'o} A and Girard, Jeffrey M and
              Zhang, Zheng and Yin, Lijun and Pantic, Maja",
  journal  = "Proc Int Conf Autom Face Gesture Recognit",
  volume   =  2017,
  pages    = "839--847",
  month    =  jun,
  year     =  2017,
  language = "en"
}

@INPROCEEDINGS{multipie,  author={Gross, Ralph and Matthews, Iain and Cohn, Jeffrey and Kanade, Takeo and Baker, Simon},  booktitle={8th IEEE FG 2008},   title={Multi-PIE},   year={2008},  volume={},  number={},  pages={1-8},  doi={10.1109/AFGR.2008.4813399}}

@article{disfa,
author = {Mavadati, Seyedmohammad and Mahoor, Mohammad and Bartlett, Kevin and Trinh, Philip and Cohn, Jeffrey},
year = {2013},
month = {04},
pages = {151-160},
title = {DISFA: A spontaneous facial action intensity database},
volume = {4},
journal = {Affective Computing, IEEE Transactions on},
doi = {10.1109/T-AFFC.2013.4}
}

@INPROCEEDINGS{sfew,  author={Dhall, Abhinav and Goecke, Roland and Lucey, Simon and Gedeon, Tom},  booktitle={2011 IEEE ICCV Workshops},   title={Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark},   year={2011},  volume={},  number={},  pages={2106-2112},  doi={10.1109/ICCVW.2011.6130508}}

@misc{fer2013,
  doi = {10.48550/ARXIV.1307.0414},
  url = {https://arxiv.org/abs/1307.0414},
  author = {Goodfellow, Ian J. and Erhan, Dumitru and Carrier, Pierre Luc and Courville, Aaron and Mirza, Mehdi and Hamner, Ben and Cukierski, Will and Tang, Yichuan and Thaler, David and Lee, Dong-Hyun and Zhou, Yingbo and Ramaiah, Chetan and Feng, Fangxiang and Li, Ruifan and Wang, Xiaojie and Athanasakis, Dimitris and Shawe-Taylor, John and Milakov, Maxim and Park, John and Ionescu, Radu and Popescu, Marius and Grozea, Cristian and Bergstra, James and Xie, Jingjing and Romaszko, Lukasz and Xu, Bing and Chuang, Zhang and Bengio, Yoshua},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Challenges in Representation Learning: A report on three machine learning contests},
  publisher = {arXiv},
  year = {2013},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{arface,
author = {Martinez, A. and Benavente, Robert},
year = {1998},
month = {01},
pages = {},
title = {The AR face database},
journal = {Tech. Rep. 24 CVC Technical Report}
}

@inproceedings{SVHN,
title	= {Reading Digits in Natural Images with Unsupervised Feature Learning},
author	= {Yuval Netzer and Tao Wang and Adam Coates and Alessandro Bissacco and Bo Wu and Andrew Y. Ng},
year	= {2011},
URL	= {http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf},
booktitle	= {NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011}
}

@inproceedings{amazon,
    title = "Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects",
    author = "Ni, Jianmo  and
      Li, Jiacheng  and
      McAuley, Julian",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1018",
    doi = "10.18653/v1/D19-1018",
    pages = "188--197",
    abstract = "Several recent works have considered the problem of generating reviews (or {`}tips{'}) as a form of explanation as to why a recommendation might match a customer{'}s interests. While promising, we demonstrate that existing approaches struggle (in terms of both quality and content) to generate justifications that are relevant to users{'} decision-making process. We seek to introduce new datasets and methods to address the recommendation justification task. In terms of data, we first propose an {`}extractive{'} approach to identify review segments which justify users{'} intentions; this approach is then used to distantly label massive review corpora and construct large-scale personalized recommendation justification datasets. In terms of generation, we are able to design two personalized generation models with this data: (1) a reference-based Seq2Seq model with aspect-planning which can generate justifications covering different aspects, and (2) an aspect-conditional masked language model which can generate diverse justifications based on templates extracted from justification histories. We conduct experiments on two real-world datasets which show that our model is capable of generating convincing and diverse justifications.",
}

@misc{cyclegan,
  doi = {10.48550/ARXIV.1703.10593},
  url = {https://arxiv.org/abs/1703.10593},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{cGAN,
  doi = {10.48550/ARXIV.1411.1784},
  url = {https://arxiv.org/abs/1411.1784},
  author = {Mirza, Mehdi and Osindero, Simon},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Conditional Generative Adversarial Nets},
  publisher = {arXiv},
  year = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{pixelRNN,
  doi = {10.48550/ARXIV.1601.06759},
  url = {https://arxiv.org/abs/1601.06759},
  author = {Oord, Aaron van den and Kalchbrenner, Nal and Kavukcuoglu, Koray},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Pixel Recurrent Neural Networks},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{text2image,
  doi = {10.48550/ARXIV.1605.05396},
  url = {https://arxiv.org/abs/1605.05396},
  author = {Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
  keywords = {Neural and Evolutionary Computing (cs.NE), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Generative Adversarial Text to Image Synthesis},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ganimation,
  doi = {10.48550/ARXIV.1807.09251},
  url = {https://arxiv.org/abs/1807.09251},
  author = {Pumarola, Albert and Agudo, Antonio and Martinez, Aleix M. and Sanfeliu, Alberto and Moreno-Noguer, Francesc},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {GANimation: Anatomically-aware Facial Animation from a Single Image},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{BEGAN,
  doi = {10.48550/ARXIV.1703.10717},
  url = {https://arxiv.org/abs/1703.10717},
  author = {Berthelot, David and Schumm, Thomas and Metz, Luke},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {BEGAN: Boundary Equilibrium Generative Adversarial Networks},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{lsgan,
  doi = {10.48550/ARXIV.1611.04076},
  url = {https://arxiv.org/abs/1611.04076},
  author = {Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond Y. K. and Wang, Zhen and Smolley, Stephen Paul},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Least Squares Generative Adversarial Networks},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{DiscoGAN,
  doi = {10.48550/ARXIV.1703.05192},
  url = {https://arxiv.org/abs/1703.05192},
  author = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jung Kwon and Kim, Jiwon},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Learning to Discover Cross-Domain Relations with Generative Adversarial Networks},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{DCGAN,
  doi = {10.48550/ARXIV.1511.06434},
  url = {https://arxiv.org/abs/1511.06434},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{stylegan1,
  doi = {10.48550/ARXIV.1812.04948},
  url = {https://arxiv.org/abs/1812.04948},
  author = {Karras, Tero and Laine, Samuli and Aila, Timo},
  keywords = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ffhq,
  doi = {10.48550/ARXIV.1812.04948},
  url = {https://arxiv.org/abs/1812.04948},
  author = {Karras, Tero and Laine, Samuli and Aila, Timo},
  keywords = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{LSUN,
  doi = {10.48550/ARXIV.1506.03365},
  url = {https://arxiv.org/abs/1506.03365},
  author = {Yu, Fisher and Seff, Ari and Zhang, Yinda and Song, Shuran and Funkhouser, Thomas and Xiao, Jianxiong},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{celebahq,
  doi = {10.48550/ARXIV.1710.10196},
  url = {https://arxiv.org/abs/1710.10196},
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  keywords = {Neural and Evolutionary Computing (cs.NE), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@INPROCEEDINGS{resnet,  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Deep Residual Learning for Image Recognition},   year={2016},  volume={},  number={},  pages={770-778},  doi={10.1109/CVPR.2016.90}}

@misc{pixelcnn,
  doi = {10.48550/ARXIV.1606.05328},
  url = {https://arxiv.org/abs/1606.05328},
  author = {Oord, Aaron van den and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Conditional Image Generation with PixelCNN Decoders},
  publisher = {arXiv},
  year = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{deepface,
  title        = {HyperExtended LightFace: A Facial Attribute Analysis Framework},
  author       = {Serengil, Sefik Ilkin and Ozpinar, Alper},
  booktitle    = {2021 International Conference on Engineering and Emerging Technologies (ICEET)},
  pages        = {1-4},
  year         = {2021},
  doi          = {10.1109/ICEET53442.2021.9659697},
  url          = {https://doi.org/10.1109/ICEET53442.2021.9659697},
  organization = {IEEE}
}

@INPROCEEDINGS{dl-recognition,  author={Rakesh, R K and Namita, G R and Kulkarni, Rohit},  booktitle={2022 First ICEEICT},   title={Image Recognition, Classification and Analysis Using Convolutional Neural Networks},   year={2022},  volume={},  number={},  pages={1-4},  doi={10.1109/ICEEICT53079.2022.9768474}}

@article{dl-detection,
  title={Automatic detection method of tunnel lining multi-defects via an enhanced You Only Look Once network},
  author={Zhou, Zhong and Zhang, Junjie and Gong, Chenjie},
  journal={Computer-Aided Civil and Infrastructure Engineering},
  volume={37},
  number={6},
  pages={762--780},
  year={2022},
  publisher={Wiley Online Library}
}

@article{dl-localization,
  title={A survey of sound source localization with deep learning methods},
  author={Grumiaux, Pierre-Amaury and Kiti{\'c}, Sr{\dj}an and Girin, Laurent and Gu{\'e}rin, Alexandre},
  journal={ASA journal},
  volume={152},
  number={1},
  pages={107--151},
  year={2022},
  publisher={Acoustical Society of America}
}

@ARTICLE{ref1,  author={Drozdowski, Pawel and Rathgeb, Christian and Dantcheva, Antitza and Damer, Naser and Busch, Christoph},  journal={IEEE SSIT},   title={Demographic Bias in Biometrics: A Survey on an Emerging Challenge},   year={2020},  volume={1},  number={2},  pages={89-103},  doi={10.1109/TTS.2020.2992344}}

@online{gricad,
    title = {infrastructure supported by Grenoble research communities.},  
    author = {Gricad},  
    url = {https://gricad.univ-grenoble-alpes.fr}
}