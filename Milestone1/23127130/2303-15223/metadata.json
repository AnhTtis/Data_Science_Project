{
    "arxiv_id": "2303.15223",
    "paper_title": "How far generated data can impact Neural Networks performance?",
    "authors": [
        "Sayeh Gholipour Picha",
        "Dawood AL Chanti",
        "Alice Caplier"
    ],
    "submission_date": "2023-03-27",
    "revised_dates": [
        "2023-03-28"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV"
    ],
    "abstract": "The success of deep learning models depends on the size and quality of the dataset to solve certain tasks. Here, we explore how far generated data can aid real data in improving the performance of Neural Networks. In this work, we consider facial expression recognition since it requires challenging local data generation at the level of local regions such as mouth, eyebrows, etc, rather than simple augmentation. Generative Adversarial Networks (GANs) provide an alternative method for generating such local deformations but they need further validation. To answer our question, we consider noncomplex Convolutional Neural Networks (CNNs) based classifiers for recognizing Ekman emotions. For the data generation process, we consider generating facial expressions (FEs) by relying on two GANs. The first generates a random identity while the second imposes facial deformations on top of it. We consider training the CNN classifier using FEs from: real-faces, GANs-generated, and finally using a combination of real and GAN-generated faces. We determine an upper bound regarding the data generation quantity to be mixed with the real one which contributes the most to enhancing FER accuracy. In our experiments, we find out that 5-times more synthetic data to the real FEs dataset increases accuracy by 16%.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.15223v1"
    ],
    "publication_venue": "Conference Publication in Proceedings of the 18th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications - Volume 5: VISAPP, 10 pages",
    "doi": "10.5220/0011629000003417"
}