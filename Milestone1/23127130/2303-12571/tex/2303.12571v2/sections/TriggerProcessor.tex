\section{Trigger Processor}
\label{sec:trigproc}

The Trigger Processor for each sector is located in the radiation-protected room, USA15.
On every bunch crossing, each NSW Trigger Processor sends to the Sector Logic up to eight unique track segments that point to the \gls{Big Wheel}.
The segments must point within $\pm$15\,mrad of the infinite momentum track from the interaction point.
If there is a NSW track segment that matches a Big Wheel track segment, the Sector Logic sends the \pT information of the segment from the Big Wheel with a ``NSW'' flag to the Muon-to-Central Trigger Processor Interface (MUCTPI).
If there is no NSW track segment with a Big Wheel track segment, the Sector Logic sends the \pT information of the segment without the flag.
The MUCTPI/CTP can make a decision, according to the Level-1 trigger menu, based on the \pT and the flags.
Due to the mismatch in Big Wheel and New Small Wheel sector coverage, the Trigger Processor sends its segments to up to seven Sector Logic modules.
A context diagram for the Trigger Processor is shown in Figure\,\ref{fig:TrigProcContext}.

\begin{figure}[htb]
\centering
\includegraphics[width=0.95\textwidth]{figures/LL_TrigProcContext_V09}
\caption{Context diagram for the Trigger Processor }
\label{fig:TrigProcContext}
\end{figure}

The Trigger Processors find track segments from the trigger primitives that they receive as described in Section\,\ref{sec:trigpath}.
The \MM ART ASICs collect hits from the \MM Front-end boards and serialize them onto a fibre link to the Trigger Processor.
For bands of strips, the sTGC strip-TDS ASICs send their strip charge information to the sTGC Trigger Processor via the Routers.
In both cases, 32~fibres at 4.8\,Gb/s connect to the Trigger Processor per sector.
The sTGC Trigger Processor also receives input from the Pad Trigger board of its sector.

%\FloatBarrier

\subsection{Hardware}
The NSW Trigger Processor platform is built according to the Advanced Telecommunications Computing Architecture (\gls{ATCA}) standard\,\cite{ATCA}.
A Trigger Processor ``blade'' consists of a carrier card, two mezzanine cards and a Rear Transition Module (RTM).
See Figure\,\ref{fig:TCP_NSWTP-overview}.
The sTGC and \MM use the same hardware for their trigger logic, but with different firmware.
A single mezzanine card with two Xilinx FPGAs\footnote{XC7VX690T}, one for \MM data and one for sTGC data, handles one sector.
The two FPGAs can communicate with each other via 68 fast (640\,Mb/s) low-latency \gls{LVDS} signals.
The ATCA blade supports two such mezzanines. The backplane signal connections are so far not used.
More information on the Trigger Processor hardware can be found in\,\cite{NswTpHW}.

\begin{figure}[t]
\centering
\includegraphics[width=0.99\textwidth]{figures/TCP_NSWTP-overview_V17.pdf}
\caption{Block diagram of the Trigger Processor ATCA card showing the Carrier, two mezzanines, the Rear Transition Module (RTM)
            and an Intelligent Platform Management Controller (\gls{IPMC}) card.
            Each mezzanine hosts an sTGC FPGA and a \MM FPGA for a sector.}
\label{fig:TCP_NSWTP-overview}
\vspace{-10pt}
\end{figure}

\para{Carrier}
The Carrier conforms to the \gls{ATCA} standard.
Two Xilinx Ultrascale FPGAs\footnote{XCKU060-1FFVA1156I}, one for each sector, hold the FELIX interface.
Should it be required, buffering and preparing the Level-1 accepted data could be moved here from the mezzanines.
A Xilinx Zynq System-on-Chip FPGA\footnote{XC7Z7015\,\cite{zynq}}, running Linux CentOS\,7 on the Zynq's 32-bit ARM CPU, provides some board management functions.
An on-board Ethernet switch connects the Carrier's Sector FPGAs, the Zynq processor, the Zynq FPGA fabric and the CERN IPMC card\,\cite{CERN-IPMC} to the external network.
Board management functions include
configuring the Carrier and Mezzanine FPGAs, via the Xilinx Virtual Cable (XVC)\,\cite{XVC} over the Ethernet network,
configuring jitter cleaners, clocks and the Ethernet switch and reading the board ID.
XVC can also be used as a debug interface for the firmware of all the other FPGAs.
Other board management functions are provided by the IPMC card.
In addition, there is a dedicated high-speed serial link between the Zynq and each of the two Carrier FPGAs which may be used for debugging or monitoring purposes.

\para{Mezzanines\footnote{The custom mezzanines do not conform to the ATCA mezzanine standard.}}
In addition to the two FPGAs mentioned above, the mezzanine connects to 72 fibres via three 12-channel 10\,Gb/s microPOD\,\cite{microPOD} optical receivers, three 12-channel 10\,Gb/s microPOD optical transmitters per FPGA
and jitter cleaners for the required design clocks and FPGA transceiver reference clocks.
A Module Management Controller (\gls{MMC}) from Samway Electronic SRL\,\cite{Samway,MMC} on each mezzanine communicates with the IPMC management card on the Carrier.

\para{Rear Transition Module (RTM)}
The RTM provides several \gls{SFP} cages for serial transceivers: one connected to the on-board switch for an Ethernet transceiver, another to the Zynq,
and several connected to the Sector FPGAs for the fibre transceivers to FELIX.
The RJ-45 connectors provide four LVDS lines to each of the Sector FPGAs.
The RTM provides also a clock input which may alternatively supply a clock to the entire Carrier system.
For self-triggering of the NSW, an LVDS trigger output signal is available from one of the RJ-45 connectors.
There is also an MMC on the RTM.

%\FloatBarrier

\subsection{FPGA firmware}
The Trigger Processor segment finding pipeline in the mezzanine FPGAs is shown in Figure\,\ref{fig:TrigProcPipeline}.
A brief overview is given below and further details can be found in\,\cite{TrigProcFWspec}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.99\textwidth]{figures/LL_TriggerProcessorPipeline_v02.pdf}
\caption{The Trigger Processor segment finding pipelines implemented in the mezzanine FPGAs.}
\label{fig:TrigProcPipeline}
\end{figure}

\subsubsection{\MM specific trigger processing}

The main blocks of the \MM Trigger Processor (MMTP) algorithm are shown in Figure\,\ref{fig:GI_MMAlg}.  Its functionality requires sixteen copies of the algorithm operating in parallel. Each copy is sharing information with its neighbour to avoid boundary issues.  The algorithm generates its 320\,MHz clock from the main BC clock.

\para{Input capture and alignment}
The first stage of the firmware receives data from 32~GBT links from the 16 ADDC boards in one sector.  The deserializer uses the 320\,MHz clock derived from the main bunch crossing clock.  The data from the GBTx ASICs on the ADDC boards are captured by fixed latency receiver firmware\,\cite{GBTfixedLatency}.  In order though to account for each fibre length and provide fixed latency to a single BC clock, the data is registered twice using a clock that is phase aligned to the recovered BC clock. The first register is used to account for any individual fibre length differences. The second register provides a configurable phase adjustment but the phase is a single constant set for all fibres. This register aligns the data from all fibres and provides a fixed latency.



\begin{figure}[ht]
\centering
\includegraphics[width=0.99\textwidth]{figures/GI_MMAlg.pdf}
\caption{The \MM Trigger Processor block diagram.}
\label{fig:GI_MMAlg}
\end{figure}



\para{Decoder}
Incoming strip hit addresses are decoded into global slope values. % \red{multiplied with} a constant.
A strip's stored slope value is defined as the orthogonal distance between a given strip and the beam-line divided by the $z$ location of the relevant detection plane. It is pre-computed taking into account a strip offset and a $z$ position stored for each of the 8 planes and 16 radial segments of each wedge. The slope range is divided into approximately 1000 slope-roads which are used to form a track candidate.


\para{Finder}
The track finding algorithm is slope-road based with the wedge divided from bottom to top into approximately 1000 roads which corresponds to about eight detector strips per road. Hit data from the decoder
is routed to the corresponding road and that road is marked as being hit. Each slope-road can hold a single hit for each plane and this hit
expires after the hit configurable integration time of up to eight BC clocks. The slope-roads will overlap each other to accommodate tracks on the  boundaries.
Each slope-road is checked once per bunch crossing to determine if a coincidence threshold has been met.
Coincidence requires a minimum number of planes to be hit and the oldest hit of the track to be expiring.
Coincidence identification is accomplished using combinatorial logic and a priority encoder. The strip
number and slope for each hit are calculated and passed to the track fitting algorithm.

The Finder algorithm separates the X-plane (perpendicular to the $\eta$ coordinate) and the UV-plane coincidences ($\pm1.5^{\mathrm{o}}$ with respect to the X-plane respectively). The fitter will first identify a coincidence trigger on the X planes
and then scan from left to right all of the UV-roads that overlap the triggered X-road. The overlapping
area of UV-roads are the so-called ``diamonds''. Figure\,\ref{fig:GI_XUVTrigger} shows such a trigger coincidence.  For each X trigger there can be up to 57 UV-roads that are searched. The number of diamonds is configured for each algorithm region to match the length of
the Micromegas detector strips. Every bunch crossing, each of the 16 algorithm regions can process two
independent X-road coincidences. For every X-road coincidence, the Finder can process three UV-road
coincidences.
The amount of resources used by the Finder is proportional to the product of the number of roads and the number of planes.
The Finder implementation focuses on minimizing the resources used in each slope-road.
In the total design, it uses the most resources.

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{figures/GI_XUVTrigger.pdf}
\caption{Illustration of the \MM XUV trigger candidate. The X-plane candidate is merged with the UV-candidate to form a coincidence on the so-called diamond road which corresponds to the geometrical coincidence of the strips.}
\label{fig:GI_XUVTrigger}
\end{figure}

\para{Fitter}
In the fit, individual hit slopes in a slope-road are used to calculate global slopes associated
with each plane type.
From these  slopes, the  expressions for the fit quantities $\theta$ (the zenith),  and $\phi$ (the azimuth) can be derived. The $\Delta\theta$ (the difference in $\theta$ between the direction of the segment extrapolated back to the interaction point
and its direction when entering the detector region) can be derived.

The expected tolerance for installation of the Micromegas chambers is roughly 2\,mm, although it is possible
to have larger deviations. In order to retain the optimal performance of the trigger, alignment corrections
need to be performed. The corrections need to be performed rapidly to minimize latency, and also to have
a small number of constants to avoid large look-up tables.

\para{Candidate selection and output}
The process starts by collecting the segments from the
16 algorithm regions. Each region can generate eight segments every bunch crossing. A priority encoder
is used to select eight total segments each bunch crossing from all regions. The selected segments
are transferred to the sTGC FPGA using sixteen  640\,Mb/s LVDS signals. The receiver in the sTGC FPGA, uses
independently delayed versions of the same signal to set the sampling point to the center of the data ``eye''
and provide segment data to the sTGC logic that will be aligned to the sTGC clock with no additional
clock domain crossing being necessary.



\subsubsection{sTGC specific trigger processing}

\para{Input capture and deskew}
The FPGA multi-gigabit serializer-deserializer pairs do not have fixed latency.
After every power cycle or link reset, the data are written to the deserializer's parallel output bus with an uncertain shift of $\pm$1\,deserializer output bus clock.
The capture block waits for a configurable worst-case delay, including differing cable delays, to ensure all the data has arrived.
This delay has been determined by observation, and all inputs are continuously monitored to confirm that they never exceed the configured worst case.
The strip-TDS band data are transmitted in four 30-bit scrambled packets in one BC.
The packet format is shown in\,\cite{HU2022167504}.)
The Xilinx FPGAs in the Router and Trigger Processors work with 20-bit packets.
The capture block unscrambles the data and rebuilds the 104-bit payload.

\para{Band-builder} The Pad Trigger may find up to four coincidence towers.
For each band of strips passing through a tower, each of eight strip-TDS ASICs, one (or two for split bands) per layer,
will transmit the band's strip charge information via the Router handling its layer to the Trigger Processor.
Each Router has four fibres to be able to transmit up to four bands.
In the case of multiple bands, the bands' strip charge information arrives at the Trigger Processor on one of the four fibres from each Router, but not necessarily with the same fibre id for all of the eight layers.
For each band, the Band-builder routes the data for all eight layers of that band to one of the four instances of the trigger algorithm.
Latency is reduced by using the Band-ids and $\phi$-ids provided by the Pad Trigger a few BC's earlier than the Router data arrive
to address all the Look-up Tables needed to process that data for that Band-id and $\phi$-id.
The 128 channels of the strip-TDS are not necessarily aligned with the boundaries of the bands.
Consequently, about 7\% of the bands in a layer span over two strip-TDS ASICs.
The Band-builder concatenates the two charge vectors received from the two input fibres.
There are 86 bands for which all eight layers are available for use in the trigger.
% no room to mention that only 14 of the 17 strips in a band can be transmitted in one BC...

\para{sTGC algorithm}
There are four identical instances of the algorithm that take the strip charge information of the bands in eight layers and calculates a track segment.
A track segment consists of a radial R-index, an azimuthal $\phi$-id and a $\Delta\theta$ as described in Section\,\ref{sec:sTGCtrigger}.
They are calculated from the centroids of the charge distributions received for each layer.
Each stage of the algorithm is described below:

\vspace{3pt}\noindent\textit{Cluster selection:}
If the charge distribution in a layer of a band is inconsistent with a minimum ionizing particle, then that layer of the band is rejected.
Valid charge distributions, ``clusters'', are defined by a look-up table whose address is a vector with ``ones'' for strips above a threshold
and whose value gives the location of the beginning of the cluster (or -1 for an invalid cluster).
The look-up table requires cluster widths between two and five and with at most one extra isolated noise strip. This rejects bands with ionization from neutrons and $\delta$-rays.

\vspace{3pt}\noindent\textit{Layer centroid:}
The centroid of each cluster is calculated and projected onto the plane of the wires.

\vspace{3pt}\noindent\noindent\textit{Quadruplet centroid:}
The required three or four valid centroids of a quadruplet are projected onto the central plane of the quadruplet,
assuming the track originates from the interaction point, and the average radial position is calculated.
This accounts for the different $z$\,positions of the valid planes.

\vspace{3pt}\noindent\textit{Segment calculation:}
To calculate  $\Delta\theta$, a piecewise linear approximation to $\tan(\theta_{\textrm{IP}} - \theta_{\textrm{local}}$) is made.
Each linear segment corresponds to the region of a Band-id.
The extrapolation of $R$ is done to the sTGC wire plane furthest from the Interaction Point and assigned to an R-index.
The $\phi$-id is provided by the Pad Trigger data.

% 1. Cluster selection:
% The aim is to reject events with a charge distribution inconsistent with MIP. (2 to 5 strip cluster with an extra noise strip elsewhere)
% Implementation is done with a LUT -  the address is which charges are on. The value is the location of the beginning of the cluster
% Current values in the LUT, accept events with up to 1 noise strip + up to 5 strips in a cluster
%
% 2. Layer centroid:
% Basically what was done before - but now it also accounts for H/L bit, and also for reverse direction of the TDS. An important item to remember is that the plane of the charge is the wire plane (not the strips)
%
% 3. Quadruplet centroid:
% All centroids of a quadruplet are projected into the middle plane of the quad. And then average is calculated
%
% 4. Segment calculation:
% To calculate deltaTheta we do a piecewise linear approximation to
% tan(theta_IP- theta_local). Each linear segment corresponds to the region of a band-ID.
% The extrapolation of R is done to the last sTGC plane
%

\subsubsection{Segment merging and duplicate removal}
The \MM Trigger Processor finishes finding segments before the sTGC does.
It transfers up to eight track segments found to the sTGC Trigger Processor.% therefore,  this operation does not contribute to the overall latency.
They are transferred via sixteen 640\,Mb/s LVDS signals within one BC clock.
They are held until the sTGC Trigger algorithms complete.
From the up to four sTGC segments and up to eight \MM segments, a maximum of eight segments can be sent to the Sector Logic.
The Merge block removes duplicates and drops segments beyond the eight allowed.
Priority is currently given to sTGC segments.
There are options to ignore one or the other of \MM or sTGC segments and for ``duplicates'', to take the $\phi$-id from \MM and the other variables from the sTGC segment.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Ancillary functions}

\para{Connection to FELIX:}
The Trigger Processor interfaces to FELIX with the Xilinx\,\gls{GTH} serializer/deserializer and CERN's GBT-FPGA firmware\,\cite{GBT-FPGA, GBT_FPGA2}.  Together they emulate the GBTx ASIC.
This firmware runs on the Carrier Sector FPGAs. Each \MM and sTGC sector have a FELIX link.
The GBT-FPGA recovers the BC clock with fixed latency and decodes/encodes a received/transmitted 120-bit GBT packet every BC clock.
The received and transmitted 120-bit words are mirrored between Carrier and mezzanine FPGAs every BC via 8b/10b encoded serial transmission at 6.4\,Gb/s.
The mezzanines connect to the appropriate fields in the 120-bit word that correspond to specific E-links.
This mirror connection does not have fixed latency, and so is suitable only for the readout, configuration and monitoring paths, but not for the eight TTC signals.
These are transferred to the mezzanine via eight LVDS lines along with the BC clock.
Zero-delay PLL jitter cleaners recondition the BC clock and ensure fixed latency of the FELIX link interface.
A soft processor implemented in the Carrier Sector FPGA manages the PLL and GTH transceiver during the clock acquisition sequence and runtime operation.

\para{Readout of Level-1 Accept data:}
All input data, output segments and some intermediate data are captured every bunch crossing in FIFOs.
When a Level-1 Trigger arrives at the Trigger Processor, data from a configurable window of bunch crossings is formatted and queued in a derandomizer for output to FELIX via E-links.
To increase the total bandwidth, several 320\,Mb/s E-links can be used by sending groups of input channels and segment output via different E-links.
Furthermore, since Level-1 Triggers on consecutive bunch crossings are allowed (Phase-2), the readout block allows the data for a given bunch crossing to be transmitted in more than one output event packet.

\para{Configuration of operating parameters and reporting status:}
The SCAx package\,\cite{SCAxIEEE, SCXug} provides read/write access to operating parameters, configurable look-up tables and status words.
It emulates the \ItwoC channel of the SCA ASIC.
This enables the configuration and status reporting  to be done using the same software based on OPC\,UA,\cite{OPC-UA} as used for the Front-ends.
Unexpected conditions such as \gls{BCID} mismatch, can be reported by sending an exception code and context information via an exception E-link. These exceptions are further handled by software.

\para{Monitoring ``interesting'' non-Level-1 Accepted bunch crossings:}
A separate readout E-link exists for monitoring bunch crossings of interest, e.g.\ receiving a 3-out-of-4 Pad Trigger coincidence or corner cases and anomalies.
Figure\,\ref{fig:NF_monitoring} shows the architecture of the monitoring logic.
Intermediate pipeline data such as band-builder output, segments, and merge candidates are buffered in the Latency FIFOs.
Various points along the processing pipeline may trigger the current bunch crossing as one to be monitored.
The data from triggered BC's are transferred to the Derandomizer FIFOs.
For bunch crossings that have been triggered, the Monitoring Readout Controller builds the data  for that bunch crossing from all Derandomizer FIFOs, into a monitoring packet that is sent out on an E-link dedicated to monitoring.
In this way, the FPGA firmware can be debugged and verified.
 %\red{NF, G.Ch: The diagram shows a trigger for each data stream. Can you confirm that all streams are sent out if there is a trigger for at least one?}

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\textwidth]{figures/NF_monitoring.png}
\caption{Monitoring in the Trigger Processor}
\label{fig:NF_monitoring}
\end{figure}


\para{Level-1 readout of all ATLAS for monitoring ``interesting'' NSW bunch crossings}
If data only from bunch crossings that are accepted by the Level-1 trigger are recorded,
we cannot know if NSW segments did not result in a trigger due to improper functioning of the NSW trigger.
To address this, the Trigger Processor can flag in the  packet sent to the Sector Logic that this bunch crossing is ``interesting'', e.g.\ a 3-out-of-4 coincidence.
The Sector Logic forwards the flag via the MUCTPI to the CTP,
which generates a Level-1 Accept with trigger type ``NSWMON'', unbiased by any other detector or trigger processing.
%All of ATLAS is read out.
This allows a monitoring process to correlate NSW trigger data with the more detailed NSW front-end data, the Sector Logic and the MUCTPI data for that bunch crossing.
The NSWMON trigger type is broadcast to all swRODs via TTC; it allows other detectors and NSW sectors that did not generate that NSWMON event to exclude these events from their monitoring sample.
The CTP ensures that the rate of these events does not exceed a few 10's of Hertz.

%The idea is to get data from other ATLAS detectors, unbiased by Level-1 Accept, but enriched by the pre-trigger generated by the NSW Pad Trigger, especially from the NSW itself and the Sector Logic.
%The NSW-TP sees only data from the NSW trigger path, not from the NSW readout path which has much more detail.
%And of course it does not see data from the Sector Logic which includes whether the NSW segments were matched by Big Wheel hits, or not.
%The NSW-TP of course knows which BCID is such a trigger, but HLT and physics need to ignore it.


\subsection{Bunch crossing synchronization of the trigger paths}
\label{sec:BCsync}

Data from the sTGC Pad Trigger, Routers and the \MM Trigger Processor for a given orbit and bunch crossing must arrive at a point in the sTGC Trigger Processor simultaneously.
This alignment is done by defining the first bunch crossing in the first orbit in a run by
broadcasting an Orbit Count Reset Request via the TTC path to the Pad Trigger and \MM Trigger Processor just before the start of the run.
A flag in the data transferred to sTGC from \MM and the Pad Trigger indicates if the data are before the first bunch crossing in the first orbit in the run.
The sTGC Trigger Processor then discards such data but saves unflagged data in a FIFO that holds it until needed in the sTGC pipeline.
For the sTGC path, the Pad Trigger sends the non-existent band-id, 0xFE, on the first bunch crossing of the first orbit in the run.
The reception of a data packet with this band-id by the sTGC Trigger Processor begins the tagging of bunch crossings in its pipeline as being in the run.

%\noindent\red{work needed here to shrink}
%
%For a given bunch crossing, the output segments of the \MM trigger algorithm arrive at the sTGC Trigger Processor merge block before those from the sTGC algorithm.
%Likewise, for a given bunch crossing, the band-id's from the Pad Trigger arrive before they are needed by the sTGC Band Builder.
%Rather than use configurable delays that must be calibrated, the \MM and Pad Trigger mark their data sent from the first BCID in the first orbit of a Run.
%This is done as follows:
%\begin{itemize}[topsep=2pt, itemsep=0pt, parsep=1pt]   %? \itemsep-5pt
%\item Between runs, the Pad Trigger and the Trigger Processors are in an ``Idle state'' where they mark their output segments as ``out-of-run''.
%\item At the begriming of every ATLAS run, an Orbit Counter Reset request (OCR) is transmitted through the TTC system to all NSW components.
%
%\item On receipt of OCR, the local orbit counters are reset by the \emph{next rollover to zero} of the BCID counter.
%\item From the first bunch crossing of the first orbit, the Pad Trigger and \MM output are marked as ``in-run''.
%\item \red{sTGC and band-id\,=\,0xFE}
%\item At the end of the run, the Trigger Processors and Pad Trigger are returned to the ``Idle state''.
%\end{itemize}
