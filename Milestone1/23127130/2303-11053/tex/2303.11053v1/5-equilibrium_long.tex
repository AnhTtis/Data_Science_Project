\section{Strategy-proofness of the online algorithm}
We give the details of the Pure Nash Equilibrium here.
% 1. Offline optimal algorithm is prone to strategic manipulation where agents can benefit by hiding availability. 
% 2. There is a pure Nash equilibrium in the offline setting such that the utility of the Nash equilibrium has the same value as that of the online algorithm. 
% 3. If there are multiple pure Nash equilibria, it is not clear which one will people land up into. To avoid landing up into bad Nash equilibrium, the online algorithm is useful.
% 4. The online algorithm gives a $1+\delta$-approximation to optimal solution, hence the online solution is a $1+ \delta$-approximation to any Nash equilibrium.

% -----------------------


%minimize the number of days she reports as available while still being able to receive the resource. This corresponds to the reluctance of the agent to lose out on outside options that the agent could have exercised by not turning up to wait in line for the vaccine.
%Agents can be strategic by under-reporting the categories from which they qualify for the resource. Further, agents also have the option of marking only a subset of their actual available days as available.
\subsection{Pure Nash equilibrium}\label{sec:Nash-eq}
The offline algorithm might choose any arbitrary matching that maximizes the utility. We present a deterministic tie-breaking rule similar to the one used in \cite{Aziz21} to force the algorithm to pick a unique matching. For this, we fix an ordering \(\pi\) on agents. We show the existence of a pure Nash equilibrium under the deterministic tie-breaking. We cast our problem as a linear program as given in Fig~\ref{fig:lp1}.

\begin{figure}
\begin{alignat*}{2}
  & \text{maximize: } & & \smashoperator[l]{\sum_{\substack{i\in A, j\in C,\\ k\in D}}} u_{ik}.x_{ijk} \\
    & \text{subject to: }& \quad & \smashoperator[l]{\sum_{i\in A,j\in C}}
                    \begin{aligned}[t]
                        x_{ijk} & \le s_k,& \forall k & \in D\\[3ex]
                    \end{aligned}\\
    &             & \quad & \smashoperator[l]{\sum_{i\in A\ \ \ \ }}
                    \begin{aligned}[t]
                        x_{ijk} & \le q_{jk},& \forall (j,k) & \in C\times D\\[3ex]
                    \end{aligned}\\
    &             & \quad & \smashoperator[l]{\sum_{j\in C,k\in D}}
                    \begin{aligned}[t]
                        x_{ijk} & \in [0,1],& \forall i & \in A\\[3ex]
                    \end{aligned}\\
    &             & \quad & \quad\ \ \ \ \ \ \ \
                    \begin{aligned}[t]
                        x_{ijk} & \in [0,1],& \forall (i,j,k) & \in A\times C \times D\\[3ex]
                    \end{aligned}\\
\end{alignat*}
\caption{Here $u_{ik}$ is the utility value of agent $i$ on day $k$, and $ s_k \& q_{jk}$ are the daily supply and daily quotas respectively.}
\label{fig:lp1}
\end{figure}

 It can be seen that this LP models the network flow formulation of our problem stated in Section~\ref{sec:offline-optimal-algorithm}. It is known (\cite{lawler2001combinatorial}) that the polytope arising from the network flow problem is integral. To impose the deterministic tie breaking, we modify the objective function as follows. 
 
 \begin{align*}
  \text{maximize\ }& \smashoperator[l]{\sum_{\substack{i\in A, j\in C,\\ k\in D}}} u_{ik}.x_{ijk} + \lambda \times REG, \text{where}\\
  REG &= \sum_{i \in A} \frac{\sum_{k \in D, j \in C} x_{ijk}}{2^{\pi(i)}}
\end{align*}

For a sufficiently small \(\lambda\) \((\lambda < \delta^{|D|+1})\), the difference between utilities of any two allocations is greater than \(REG\). Therefore, the linear program in Figure~\ref{fig:lp1} maximizes the objective function in Fig~\ref{fig:lp1}, but breaks ties to maximize \(REG\).

Let \(A_{d_i}\) be defined as the set of agents matched on a day \(d_i\in D\) and \(A_\infty\) be the set of unmatched agents at the end of a run of the Algorithm~\ref{alg:online-greedy-m1}. Let agent \(a_p\) be matched on \(d_i\) (WLOG, assume all unmatched agents are matched on day \(\infty\). Now, we present a proof of Theorem~\ref{thm:nash}. 

\begin{proof}(of Theorem~\ref{thm:nash})
%In the profile described above, agent \(a_p\) is matched on \(d_i\).
Suppose the agent \(a_p\) is matched on day $d_i$, and deviates to reporting a subset of the actual available days. %Let \({D_{p_i}} \subseteq D_p\) be the days prior to \(d_i\).

If agent \(a_p\) gets matched on a day \(d_j\), $j<i$, because of misreporting her available days, then some agent \(a_q\) on day \(d_j\) will remain unmatched. This follows, since on any given day, the matching  computed by algorithm \ref{alg:online-greedy-m1} is of maximum size and all agents other than \(a_p\) turn up on at most one day. The rest of the matching will remain unchanged.
But, agent \(a_q\) is prioritized by \(\pi\) over agent \(a_p\). Otherwise, algorithm \ref{alg:online-greedy-m1} would have matched \(a_p\) and not \(a_q\) on day \(d_j\). Hence, agent \(a_p\) cannot replace agent \(a_q\) on day \(d_j\) even after misreporting her availability.

Therefore agent \(a_p\) has no advantage in deviating from the strategy. Hence, the above matching is a pure Nash equilibrium.
\end{proof}

% It is reasonable to believe agents are not computationally strong entities with good beliefs about the availability of other agents. Hence, coordinating on a pure Nash equilibrium that maximizes utility will be hard. Agents might play an equilibrium with a worse performance. But, they will not be able to play an equilibrium that is substantially better than the one described above (the utility of the above described equilibrium is exactly equal to the utility of the \(1+\delta\)--approximation algorithm \ref{alg:online-greedy-m1}).

% The online approximation reduces the scope for strategic behavior from agents. Agents, at any point must decide on signing up availability one day at a time. But, deciding this requires far less information than the one needed to decide its strategy while participating in the offline optimal.
% Thus, it might be desirable to run the online algorithm with strategic agents, despite the offline optimal possibly returning a better utility.