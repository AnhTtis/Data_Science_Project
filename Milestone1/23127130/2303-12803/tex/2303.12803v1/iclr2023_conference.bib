
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.



@inproceedings{cully2019autonomous,
  title={Autonomous skill discovery with quality-diversity and unsupervised descriptors},
  author={Cully, Antoine},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={81--89},
  year={2019}
}
@article{burda2018exploration,
  title={Exploration by random network distillation},
  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  journal={arXiv preprint arXiv:1810.12894},
  year={2018}
}
@inproceedings{peters2007reinforcement,
  title={Reinforcement learning by reward-weighted regression for operational space control},
  author={Peters, Jan and Schaal, Stefan},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={745--750},
  year={2007}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{gae,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1506.02438},
  year={2015}
}

@article{shi2020efficient,
  title={Efficient Novelty Search Through Deep Reinforcement Learning},
  author={Shi, Longxiang and Li, Shijian and Zheng, Qian and Yao, Min and Pan, Gang},
  journal={IEEE Access},
  volume={8},
  pages={128809--128818},
  year={2020},
  publisher={IEEE}
}

@inproceedings{khadka2019collaborative,
  title={Collaborative evolutionary reinforcement learning},
  author={Khadka, Shauharda and Majumdar, Somdeb and Nassar, Tarek and Dwiel, Zach and Tumer, Evren and Miret, Santiago and Liu, Yinyin and Tumer, Kagan},
  booktitle={Proceedings of the International Conference on Machine Learning},
  pages={3341--3350},
  year={2019},
}

@inproceedings{khadka2018evolutionaryNIPS,
  title={Evolution-Guided Policy Gradient in Reinforcement Learning},
  author={Khadka, Shauharda and Tumer, Kagan},
  booktitle={Neural Information Processing Systems},
  year={2018},
  volume={31},
}

@article{stanton2016curiosity,
  title={Curiosity search: producing generalists by encouraging individuals to continually explore and acquire skills throughout their lifetime},
  author={Stanton, Christopher and Clune, Jeff},
  journal={PloS one},
  volume={11},
  number={9},
  pages={e0162235},
  year={2016},
  publisher={Public Library of Science San Francisco, CA USA}
}


@inproceedings{doncieux2019novelty,
  title={Novelty search: a theoretical perspective},
  author={Doncieux, Stephane and Laflaqui{\`e}re, Alban and Coninx, Alexandre},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={99--106},
  year={2019}
}

@article{lehman2011abandoning,
  title={Abandoning objectives: Evolution through the search for novelty alone},
  author={Lehman, Joel and Stanley, Kenneth O},
  journal={Evolutionary computation},
  volume={19},
  number={2},
  pages={189--223},
  year={2011},
  publisher={MIT Press}
}

@article{cully2015robots,
  title={Robots that can adapt like animals},
  author={Cully, Antoine and Clune, Jeff and Tarapore, Danesh and Mouret, Jean-Baptiste},
  journal={Nature},
  volume={521},
  number={7553},
  pages={503--507},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{koos2012transferability,
  title={The transferability approach: Crossing the reality gap in evolutionary robotics},
  author={Koos, Sylvain and Mouret, Jean-Baptiste and Doncieux, St{\'e}phane},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={17},
  number={1},
  pages={122--145},
  year={2012},
  publisher={IEEE}
}

@article{mouret2015illuminating,
  title={Illuminating search spaces by mapping elites},
  author={Mouret, Jean-Baptiste and Clune, Jeff},
  journal={arXiv preprint arXiv:1504.04909},
  year={2015}
}

@article{cully2017quality,
  title={Quality and diversity optimization: A unifying modular framework},
  author={Cully, Antoine and Demiris, Yiannis},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={22},
  number={2},
  pages={245--259},
  year={2017},
  publisher={IEEE}
}

@inproceedings{fujimoto2018addressing,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={Proceedings of the International Conference on Machine Learning},
  pages={1587--1596},
  year={2018},
}


@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@inproceedings{silver2014deterministic,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={Proceedings of the 30th International Conference in Machine Learning},
  year={2014}
}

@inproceedings{colas2020scaling,
  title={Scaling map-elites to deep neuroevolution},
  author={Colas, C{\'e}dric and Madhavan, Vashisht and Huizinga, Joost and Clune, Jeff},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={67--75},
  year={2020}
}

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@article{salimans2017evolution,
  title={Evolution strategies as a scalable alternative to reinforcement learning},
  author={Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1703.03864},
  year={2017}
}

@article{stanley2002evolving,
  title={Evolving neural networks through augmenting topologies},
  author={Stanley, Kenneth O and Miikkulainen, Risto},
  journal={Evolutionary computation},
  volume={10},
  number={2},
  pages={99--127},
  year={2002},
  publisher={MIT Press}
}

@inproceedings{conti2018improving,
  title={Improving exploration in evolution strategies for deep reinforcement learning via a population of novelty-seeking agents},
  author={Conti, Edoardo and Madhavan, Vashisht and Such, Felipe Petroski and Lehman, Joel and Stanley, Kenneth and Clune, Jeff},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5027--5038},
  year={2018}
}

@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K},
  booktitle={Aaai},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA}
}

@article{gaier2018data,
  title={Data-efficient design exploration through surrogate-assisted illumination},
  author={Gaier, Adam and Asteroth, Alexander and Mouret, Jean-Baptiste},
  journal={Evolutionary computation},
  volume={26},
  number={3},
  pages={381--410},
  year={2018},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{haarnoja2018soft,
  title={Soft actor-critic algorithms and applications},
  author={Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and others},
  journal={arXiv preprint arXiv:1812.05905},
  year={2018}
}

@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@article{frans2017meta,
  title={Meta learning shared hierarchies},
  author={Frans, Kevin and Ho, Jonathan and Chen, Xi and Abbeel, Pieter and Schulman, John},
  journal={Proc. of ICLR},
  year={2018}
}

@article{paolo2019unsupervised,
  title={Unsupervised Learning and Exploration of Reachable Outcome Space},
  author={Paolo, Giuseppe and Laflaquiere, Alban and Coninx, Alexandre and Doncieux, Stephane},
  journal={algorithms},
  volume={24},
  pages={25},
  year={2019}
}
@article{pere2018unsupervised,
  title={Unsupervised learning of goal spaces for intrinsically motivated goal exploration},
  author={P{\'e}r{\'e}, Alexandre and Forestier, S{\'e}bastien and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  journal={arXiv preprint arXiv:1803.00781},
  year={2018}
}

@article{pong2019skew,
  title={Skew-fit: State-covering self-supervised reinforcement learning},
  author={Pong, Vitchyr H and Dalal, Murtaza and Lin, Steven and Nair, Ashvin and Bahl, Shikhar and Levine, Sergey},
  journal={arXiv preprint arXiv:1903.03698},
  year={2019}
}
@article{lee2019efficient,
  title={Efficient exploration via state marginal matching},
  author={Lee, Lisa and Eysenbach, Benjamin and Parisotto, Emilio and Xing, Eric and Levine, Sergey and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1906.05274},
  year={2019}
}
@article{islam2019marginalized,
  title={Marginalized State Distribution Entropy Regularization in Policy Optimization},
  author={Islam, Riashat and Ahmed, Zafarali and Precup, Doina},
  journal={arXiv preprint arXiv:1912.05128},
  year={2019}
}

@article{badia2020agent57,
  title={Agent57: Outperforming the Atari Human Benchmark},
  author={Badia, Adri{\`a} Puigdom{\`e}nech and Piot, Bilal and Kapturowski, Steven and Sprechmann, Pablo and Vitvitskyi, Alex and Guo, Daniel and Blundell, Charles},
  journal={arXiv preprint arXiv:2003.13350},
  year={2020}
}
@ARTICLE{deboer05tutorial,
  author = {De Boer, P-T. and Kroese, D.P and Mannor, S. and Rubinstein, R.Y.},
  title = {A Tutorial on the Cross-Entropy Method},
  journal = {Annals of Operations Research},
  year = {2005},
  volume = {134},
  number = {1},
  pages = {19--67},
  file = {:/home/stulp/docs/bibliography/robotics/adaptive-exploration/Boer, Kroese, Mannor, Rubinstein - A Tutorial on the Cross Entropy Method.pdf:PDF},
  keywords = {tutorial, exploration},
}
@article{silver2017mastering,
  title={Mastering the game of Go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Research}
}
@article{parascandolo2020divide,
  title={Divide-and-Conquer Monte Carlo Tree Search For Goal-Directed Planning},
  author={Parascandolo, Giambattista and Buesing, Lars and Merel, Josh and Hasenclever, Leonard and Aslanides, John and Hamrick, Jessica B and Heess, Nicolas and Neitz, Alexander and Weber, Theophane},
  journal={arXiv preprint arXiv:2004.11410},
  year={2020}
}
@article{andersen2018learning,
  title={Learning High-level Representations from Demonstrations},
  author={Andersen, Garrett and Vrancx, Peter and Bou-Ammar, Haitham},
  journal={arXiv preprint arXiv:1802.06604},
  year={2018}
}

@article{vassiliades2017using,
  title={Using centroidal voronoi tessellations to scale up the multidimensional archive of phenotypic elites algorithm},
  author={Vassiliades, Vassilis and Chatzilygeroudis, Konstantinos and Mouret, Jean-Baptiste},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={22},
  number={4},
  pages={623--630},
  year={2017},
  publisher={IEEE}
}

@inproceedings{vassiliades2018discovering,
  title={Discovering the elite hypervolume by leveraging interspecies correlation},
  author={Vassiliades, Vassiiis and Mouret, Jean-Baptiste},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={149--156},
  year={2018}
}


@inproceedings{zhang2021importance,
  title={On the importance of hyperparameter optimization for model-based reinforcement learning},
  author={Zhang, Baohe and Rajan, Raghu and Pineda, Luis and Lambert, Nathan and Biedenkapp, Andr{\'e} and Chua, Kurtland and Hutter, Frank and Calandra, Roberto},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4015--4023},
  year={2021},
}



@article{eysenbach2018diversity,
  title={Diversity is All You Need: Learning Skills without a Reward Function},
  author={Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
  journal={arXiv preprint arXiv:1802.06070},
  year={2018}
}
@article{jaderberg2017population,
  title={Population-based training of neural networks},
  author={Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and others},
  journal={arXiv preprint arXiv:1711.09846},
  year={2017}
}
@article{matheron2020pbcs,
  title={{PBCS}: Efficient Exploration and Exploitation Using a Synergy between Reinforcement Learning and Motion Planning},
  author={Matheron, Guillaume and Perrin, Nicolas and Sigaud, Olivier},
  journal={arXiv preprint arXiv:2004.11667},
  year={2020}
}

@article{kume2017map,
  title={Map-based multi-policy reinforcement learning: enhancing adaptability of robots by deep reinforcement learning},
  author={Kume, Ayaka and Matsumoto, Eiichi and Takahashi, Kuniyuki and Ko, Wilson and Tan, Jethro},
  journal={arXiv preprint arXiv:1710.06117},
  year={2017}
}

@inproceedings{tobin2018domain,
  title={Domain randomization and generative models for robotic grasping},
  author={Tobin, Josh and Biewald, Lukas and Duan, Rocky and Andrychowicz, Marcin and Handa, Ankur and Kumar, Vikash and McGrew, Bob and Ray, Alex and Schneider, Jonas and Welinder, Peter and others},
  booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={3482--3489},
  year={2018},
  organization={IEEE}
}
@article{akkaya2019solving,
  title={Solving Rubik's Cube with a Robot Hand},
  author={Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and others},
  journal={arXiv preprint arXiv:1910.07113},
  year={2019}
}


@article{andrychowicz2017hindsight,
  title={Hindsight {E}xperience {R}eplay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1707.01495},
  year={2017}
}

@inproceedings{schaul2015universal,
  title={Universal Value Function Approximators},
  author={Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
  booktitle={International Conference on Machine Learning},
  pages={1312--1320},
  year={2015}
}

@article{sigaud2019policy,
  title={Policy Search in Continuous Action Domains: an Overview},
  author={Sigaud, Olivier and Stulp, Freek},
  journal={Neural Networks},
  pages = {28-40},
  volume = {113},
  year={2019}
}

@article{forestier2017intrinsically,
  title={Intrinsically motivated goal exploration processes with automatic curriculum learning},
  author={Forestier, S{\'e}bastien and Mollard, Yoan and Oudeyer, Pierre-Yves},
  journal={arXiv preprint arXiv:1708.02190},
  year={2017}
}

# explo

@article{tang2016exploration,
  title={\# Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning},
  author={Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1611.04717},
  year={2016}
}
@inproceedings{bellemare2016unifying,
  title={Unifying count-based exploration and intrinsic motivation},
  author={Bellemare, Marc and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1471--1479},
  year={2016}
}
@article{fortunato2017noisy,
  title={Noisy Networks for Exploration},
  author={Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and Menick, Jacob and Osband, Ian and Graves, Alex and Mnih, Vlad and Munos, Remi and Hassabis, Demis and Pietquin, Olivier and others},
  journal={arXiv preprint arXiv:1706.10295},
  year={2017}
}
@article{plappert2017parameter,
  title={Parameter Space Noise for Exploration},
  author={Plappert, Matthias and Houthooft, Rein and Dhariwal, Prafulla and Sidor, Szymon and Chen, Richard Y. and Chen, Xi and Asfour, Tamim and Abbeel, Pieter and Andrychowicz, Marcin},
  journal={arXiv preprint arXiv:1706.01905},
  year={2017}
}

# diversity and RL

@article{jaderberg2019human,
  title={Human-level performance in 3D multiplayer games with population-based reinforcement learning},
  author={Jaderberg, Max and Czarnecki, Wojciech M and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C and Morcos, Ari S and Ruderman, Avraham and others},
  journal={Science},
  volume={364},
  number={6443},
  pages={859--865},
  year={2019},
  publisher={American Association for the Advancement of Science}
}

@article{doan2019attraction,
  title={Attraction-Repulsion Actor-Critic for Continuous Control Reinforcement Learning},
  author={Doan, Thang and Mazoure, Bogdan and Durand, Audrey and Pineau, Joelle and Hjelm, R Devon},
  journal={arXiv preprint arXiv:1909.07543},
  year={2019}
}

@inproceedings{jung2020population,
  title={Population-Guided Parallel Policy Search for Reinforcement Learning},
  author={Jung, Whiyoung and Park, Giseung and Sung, Youngchul},
  booktitle={Proceedings of the International Conference on Learning Representations},
  year={2020}
}

@inproceedings{pourchot2018cem,
  author    = {Alo{\"{\i}}s Pourchot and
               Olivier Sigaud},
  title     = {{CEM-RL:} Combining evolutionary and gradient-based methods for policy
               search},
  booktitle = {Proceedings of the International Conference on Learning Representations},
  year      = {2019},
}

@inproceedings{colas2018gep,
  author    = {C{\'{e}}dric Colas and
               Olivier Sigaud and
               Pierre{-}Yves Oudeyer},
  title     = {{GEP-PG:} Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms},
  booktitle = {Proceedings of the International Conference on Machine Learning},
  volume    = {80},
  pages     = {1038--1047},
  year      = {2018},
}


@inproceedings{parker2020effective,
  title={Effective Diversity in Population-Based Reinforcement Learning},
  author={Parker-Holder, Jack and Pacchiano, Aldo and Choromanski, Krzysztof and Roberts, Stephen},
  booktitle={Neural Information Processing Systems},
  year={2020},
  volume={33},
  pages={18050--18062},
}

# quality-diversity, novelty

@article{pugh2016quality,
  title={Quality diversity: A new frontier for evolutionary computation},
  author={Pugh, Justin K and Soros, Lisa B and Stanley, Kenneth O.},
  journal={Frontiers in Robotics and AI},
  volume={3},
  pages={40},
  year={2016},
  publisher={Frontiers}
}

@article{such2017deep,
  title={Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning},
  author={Petroski Such, Felipe and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  journal={arXiv preprint arXiv:1712.06567},
  year={2017}
}

@article{ecoffet2019go,
  title={Go-explore: a new approach for hard-exploration problems},
  author={Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  journal={arXiv preprint arXiv:1901.10995},
  year={2019}
}

@inproceedings{kingmaadam,
  title={ADAM: AMETHOD FOR STOCHASTIC OPTIMIZATION},
  author={Kingma, Diederik P and Ba, Jimmy Lei},
  booktitle={Proc. of ICLR},
  year={2015}
}


@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015},
  publisher={Nature Publishing Group}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  publisher={MIT Press},
  year={2018}
}

@article{van2018deep,
  title={Deep reinforcement learning and the deadly triad},
  author={Van Hasselt, Hado and Doron, Yotam and Strub, Florian and Hessel, Matteo and Sonnerat, Nicolas and Modayil, Joseph},
  journal={arXiv},
  year={2018}
}

@inproceedings{lehman2011evolving,
  title={Evolving a diversity of virtual creatures through novelty search and local competition},
  author={Lehman, Joel and Stanley, Kenneth O},
  booktitle={Proceedings of the 13th annual conference on Genetic and evolutionary computation},
  pages={211--218},
  year={2011}
}

@inproceedings{cully2013behavioral,
  title={Behavioral repertoire learning in robotics},
  author={Cully, Antoine and Mouret, Jean-Baptiste},
  booktitle={Proceedings of the 15th annual conference on Genetic and evolutionary computation},
  pages={175--182},
  year={2013}
}

@article{cazenille2019exploring,
title={Exploring Self-Assembling Behaviors in a Swarm of Bio-micro-robots using Surrogate-Assisted MAP-Elites},
author={Cazenille, Leo and Bredeche, Nicolas and Aubert-Kato, Nathanael},
journal={arXiv preprint arXiv:1910.00230},
year={2019}
}

@inproceedings{alvarez2019empowering,
  title={Empowering quality diversity in dungeon design with interactive constrained MAP-Elites},
  author={Alvarez, Alberto and Dahlskog, Steve and Font, Jose and Togelius, Julian},
  booktitle={2019 IEEE Conference on Games (CoG)},
  pages={1--8},
  year={2019},
  organization={IEEE}
}
@inproceedings{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation.},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay and others},
  booktitle={NIPs},
  volume={99},
  pages={1057--1063},
  year={1999},
  organization={Citeseer}
}

@inproceedings{vemula2019contrasting,
  title={Contrasting exploration in parameter and action space: A zeroth-order optimization perspective},
  author={Vemula, Anirudh and Sun, Wen and Bagnell, J},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={2926--2935},
  year={2019},
  organization={PMLR}
}

@article{nasiriany2019planning,
  title={Planning with goal-conditioned policies},
  author={Nasiriany, Soroush and Pong, Vitchyr H and Lin, Steven and Levine, Sergey},
  journal={arXiv preprint arXiv:1911.08453},
  year={2019}
}

@inproceedings{nilsson2021policy,
  title={Policy gradient assisted map-elites},
  author={Nilsson, Olle and Cully, Antoine},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={866--875},
  year={2021}
}


@article{Vassiliades2018,
   abstract = {Evolution has produced an astonishing diversity of species, each filling a different niche. Algorithms like MAP-Elites mimic this divergent evolutionary process to find a set of behaviorally diverse but high-performing solutions, called the elites. Our key insight is that species in nature often share a surprisingly large part of their genome, in spite of occupying very different niches; similarly, the elites are likely to be concentrated in a specific "elite hypervolume" whose shape is defined by their common features. In this paper, we first introduce the elite hypervolume concept and propose two metrics to characterize it: the genotypic spread and the genotypic similarity. We then introduce a new variation operator, called "directional variation", that exploits interspecies (or inter-elites) correlations to accelerate the MAP-Elites algorithm. We demonstrate the effectiveness of this operator in three problems (a toy function, a redundant robotic arm, and a hexapod robot).},
   author = {Vassilis Vassiliades and Jean-Baptiste Mouret},
   doi = {10.1145/3205455.3205602},
   journal = {GECCO 2018 - Proceedings of the 2018 Genetic and Evolutionary Computation Conference},
   keywords = {Illumination algorithms,MAP-Elites,Quality diversity},
   month = {4},
   pages = {149-156},
   publisher = {Association for Computing Machinery, Inc},
   title = {Discovering the Elite Hypervolume by Leveraging Interspecies Correlation},
   url = {http://arxiv.org/abs/1804.03906 http://dx.doi.org/10.1145/3205455.3205602},
   year = {2018},
}

@inproceedings{Fontaine2021,
  title={Differentiable quality diversity},
  author={Fontaine, Matthew and Nikolaidis, Stefanos},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={10040--10052},
  year={2021}
}

@inproceedings{fontaine2020covariance,
  title={Covariance matrix adaptation for the rapid illumination of behavior space},
  author={Fontaine, Matthew C and Togelius, Julian and Nikolaidis, Stefanos and Hoover, Amy K},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={94--102},
  year={2020}
}

@inproceedings{flajolet2022fast,
  title={Fast Population-Based Reinforcement Learning on a Single Machine},
  author={Flajolet, Arthur and Monroc, Claire Bizon and Beguir, Karim and Pierrot, Thomas},
  booktitle={Proceedings of the International Conference on Machine Learning},
  pages={6533--6547},
  year={2022},
}

@article{AGAC2021,
   abstract = {Despite definite success in deep reinforcement learning problems, actor-critic algorithms are still confronted with sample inefficiency in complex environments, particularly in tasks where efficient exploration is a bottleneck. These methods consider a policy (the actor) and a value function (the critic) whose respective losses are built using different motivations and approaches. This paper introduces a third protagonist: the adversary. While the adversary mimics the actor by minimizing the KL-divergence between their respective action distributions, the actor, in addition to learning to solve the task, tries to differentiate itself from the adversary predictions. This novel objective stimulates the actor to follow strategies that could not have been correctly predicted from previous trajectories, making its behavior innovative in tasks where the reward is extremely rare. Our experimental analysis shows that the resulting Adversarially Guided Actor-Critic (AGAC) algorithm leads to more exhaustive exploration. Notably, AGAC outperforms current state-of-the-art methods on a set of various hard-exploration and procedurally-generated tasks.},
   author = {Yannis Flet-Berliac and Johan Ferret and Olivier Pietquin and Philippe Preux},
   title = {ADVERSARIALLY GUIDED ACTOR-CRITIC},
}

@article{EvoRL,
   abstract = {Deep Reinforcement Learning (DRL) algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically suffer from three core difficulties: temporal credit assignment with sparse rewards, lack of effective exploration, and brittle convergence properties that are extremely sensitive to hyperparameters. Collectively, these challenges severely limit the applicability of these approaches to real-world problems. Evolutionary Algorithms (EAs), a class of black box optimization techniques inspired by natural evolution, are well suited to address each of these three challenges. However, EAs typically suffer from high sample complexity and struggle to solve problems that require optimization of a large number of parameters. In this paper, we introduce Evolutionary Reinforcement Learning (ERL), a hybrid algorithm that leverages the population of an EA to provide diversified data to train an RL agent, and reinserts the RL agent into the EA population periodically to inject gradient information into the EA. ERL inherits EA's ability of temporal credit assignment with a fitness metric, effective exploration with a diverse set of policies, and stability of a population-based approach and complements it with off-policy DRL's ability to leverage gradients for higher sample efficiency and faster learning. Experiments in a range of challenging continuous control benchmarks demonstrate that ERL significantly outperforms prior DRL and EA methods.},
   author = {Shauharda Khadka and Kagan Tumer},
   title = {Evolution-Guided Policy Gradient in Reinforcement Learning},
}



@article{DBLP:journals/corr/VassiliadesCM16,
  author    = {Vassilis Vassiliades and
               Konstantinos I. Chatzilygeroudis and
               Jean{-}Baptiste Mouret},
  title     = {Scaling Up MAP-Elites Using Centroidal Voronoi Tessellations},
  journal   = {CoRR},
  volume    = {abs/1610.05729},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.05729},
  eprinttype = {arXiv},
  eprint    = {1610.05729},
  timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/VassiliadesCM16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{CERL,
   abstract = {Deep reinforcement learning algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically struggle with achieving effective exploration and are extremely sensitive to the choice of hyper-parameters. One reason is that most approaches use a noisy version of their operating policy to explore-thereby limiting the range of exploration. In this paper, we introduce Collaborative Evolutionary Reinforcement Learning (CERL), a scal-able framework that comprises a portfolio of policies that simultaneously explore and exploit diverse regions of the solution space. A collection of learners-typically proven algorithms like TD3-optimize over varying time-horizons leading to this diverse portfolio. All learners contribute to and use a shared replay buffer to achieve greater sample efficiency. Computational resources are dynamically distributed to favor the best learners as a form of online algorithm selection. Neuroevo-lution binds this entire process to generate a single emergent learner that exceeds the capabilities of any individual learner. Experiments in a range of continuous control benchmarks demonstrate that the emergent learner significantly outperforms its composite learners while remaining overall more sample-efficient-notably solving the Mujoco Hu-manoid benchmark where all of its composite learners (TD3) fail entirely in isolation.},
   author = {Shauharda Khadka and Somdeb Majumdar and Tarek Nassar and Zach Dwiel and Evren Tumer and Santiago Miret and Yinyin Liu and Kagan Tumer},
   title = {Collaborative Evolutionary Reinforcement Learning},
}

@inproceedings{pierrot2022qdpg,
  author    = {Thomas Pierrot and
               Valentin Mac{\'{e}} and
               F{\'{e}}lix Chalumeau and
               Arthur Flajolet and
               Geoffrey Cideron and
               Karim Beguir and
               Antoine Cully and
               Olivier Sigaud and
               Nicolas Perrin{-}Gilbert},
  title     = {Diversity policy gradient for sample efficient quality-diversity optimization},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
  pages     = {1075--1083},
  year      = {2022},
}

@inproceedings{tjanaka2022approxdqd,
  title={Approximating gradients for differentiable quality diversity in reinforcement learning},
  author={Tjanaka, Bryon and Fontaine, Matthew C and Togelius, Julian and Nikolaidis, Stefanos},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={1102--1111},
  year={2022}
}


@inproceedings{Alvarez_2019,
	doi = {10.1109/cig.2019.8848022},
  
	url = {https://doi.org/10.1109%2Fcig.2019.8848022},
  
	year = 2019,
	month = {aug},
  
	publisher = {{IEEE}
},
  
	author = {Alberto Alvarez and Steve Dahlskog and Jose Font and Julian Togelius},
  
	title = {Empowering Quality Diversity in Dungeon Design with Interactive Constrained {MAP}-Elites},
  
	booktitle = {2019 {IEEE} Conference on Games ({CoG})}
}

@inproceedings{10.1145/3319619.3321897,
author = {Gaier, Adam and Asteroth, Alexander and Mouret, Jean-Baptiste},
title = {Are Quality Diversity Algorithms Better at Generating Stepping Stones than Objective-Based Search?},
year = {2019},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {115–116},
}

@inproceedings{rakicevicCK21,
  title={Policy manifold search: Exploring the manifold hypothesis for diversity-based neuroevolution},
  author={Rakicevic, Nemanja and Cully, Antoine and Kormushev, Petar},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={901--909},
  year={2021}
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.2.5},
  year = {2018},
}


@article{freeman2021brax,
  title={Brax--A Differentiable Physics Engine for Large Scale Rigid Body Simulation},
  author={Freeman, C Daniel and Frey, Erik and Raichuk, Anton and Girgin, Sertan and Mordatch, Igor and Bachem, Olivier},
  journal={arXiv preprint arXiv:2106.13281},
  year={2021}
}

@misc{amin2021explorl,
  doi = {10.48550/ARXIV.2109.00157},
  url = {https://arxiv.org/abs/2109.00157},
  author = {Amin, Susan and Gomrokchi, Maziar and Satija, Harsh and van Hoof, Herke and Precup, Doina},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {A Survey of Exploration Methods in Reinforcement Learning},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{qdax,
  title={Accelerated Quality-Diversity for Robotics through Massive Parallelism},
  author={Lim, Bryan and Allard, Maxime and Grillotti, Luca and Cully, Antoine},
  journal={arXiv preprint arXiv:2202.01258},
  year={2022}
}

@inproceedings{evojax2022,
  title={Evojax: Hardware-accelerated neuroevolution},
  author={Tang, Yujin and Tian, Yingtao and Ha, David},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  pages={308--311},
  year={2022}
}

@inproceedings{sarkar2021generating,
  title={Generating and blending game levels via quality-diversity in the latent space of a variational autoencoder},
  author={Sarkar, Anurag and Cooper, Seth},
  booktitle={Proceedings of the International Conference on the Foundations of Digital Games},
  pages={1--11},
  year={2021}
}

@inproceedings{gravina2019procedural,
  title={Procedural content generation through quality diversity},
  author={Gravina, Daniele and Khalifa, Ahmed and Liapis, Antonios and Togelius, Julian and Yannakakis, Georgios N},
  booktitle={2019 IEEE Conference on Games (CoG)},
  pages={1--8},
  year={2019},
}

@inproceedings{cully2018hierarchical,
  title={Hierarchical behavioral repertoires with unsupervised descriptors},
  author={Cully, Antoine and Demiris, Yiannis},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={69--76},
  year={2018}
}

@article{lehman2020surprising,
  title={The surprising creativity of digital evolution: A collection of anecdotes from the evolutionary computation and artificial life research communities},
  author={Lehman, Joel and Clune, Jeff and Misevic, Dusan and Adami, Christoph and Altenberg, Lee and Beaulieu, Julie and Bentley, Peter J and Bernard, Samuel and Beslon, Guillaume and Bryson, David M and others},
  journal={Artificial life},
  volume={26},
  number={2},
  pages={274--306},
  year={2020},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{gaier2019quality,
  title={Are quality diversity algorithms better at generating stepping stones than objective-based search?},
  author={Gaier, Adam and Asteroth, Alexander and Mouret, Jean-Baptiste},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  pages={115--116},
  year={2019}
}
