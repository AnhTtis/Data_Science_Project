\makeatletter
\makeatother


\begin{algorithm}[H]
    \small
    \SetAlgoLined
    \DontPrintSemicolon
    \SetKwInput{KwInput}{Given}
    \KwInput{ 
    \begin{itemize}
        \item $\mathbb{M}$: \me repertoire
        \item $N \in \mathbb{N}^*$: maximum number of environment steps 
        \item $M \in \mathbb{N}^*$: number of isoline-variation offsprings per iteration
        \item $P \in \mathbb{N}^*$: size of the population of RL agents
        \item $S \in \mathbb{N}^*$: number of training steps per iteration per agent
        \item $p, k, n \in \left] 0, 1 \right[ $: \pbt proportions 
        \item an RL agent template
        \item $F(\cdot)$: fitness function
        \item $\Phi(\cdot)$: behavior descriptor function
    \end{itemize}
    }
    \texttt{\\}

    \tcp{Initialization}
    % Use \cvt with $K$ initial points to initialize a repertoire of $M$ cells over the descriptors space\;
    Randomly initialize $P + M$ agents following the chosen RL template $((\pi_{\theta_i}, \phi_i, \mathbf{h}_i))_{1 \leq i \leq P+M}$. \;
    Run one episode in the environment using each of  $(\pi_{\theta_i})_{1 \leq i \leq P+M}$ to evaluate $(F(\pi_{\theta_i}))_{1 \leq i \leq P+M}$ and $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq P+M}$.\;
    Insert $((\pi_{\theta_i}, \phi_i, \mathbf{h}_i))_{1 \leq i \leq P+M}$ in $\mathbb{M}$ based on $(F(\pi_{\theta_i}))_{1 \leq i \leq P+M}$ and $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq P+M}$.\;

    Initialize $P$ replay buffers $(\mathbb{B}_i)_{1 \leq i \leq P}$ using the data collected respectively by each agent during the initial evaluations (if replay buffers are used by the RL agent).\;
    \texttt{\\}
    
    \tcp{Main loop}
    Initialize $n_{\text{steps}}$, the total number of environment interactions carried out so far, to $0$.\;
    \While{$n_{\text{steps}} \leq N$}{
    \texttt{\\}
    
        \tcp{Population Update}
        Re-order the agents $i = 1, \cdots, P$ in increasing order of their fitnesses $(F(\pi_{\theta_i}))_{1 \leq i \leq P}$.\;
        Update agents $i=1, \cdots, p P$ by copying randomly-sampled agents from $i=(1-n) P, \cdots, P$ and copy the replay buffers accordingly (if replay buffers are used by the RL agent).\;
        Sample new hyperparameters for agents $i=1, \cdots, p P$.\;
        Sample $k P$ indices $(i_j)_{1 \leq j \leq k P}$ uniformly without replacement from $\{pP +1, \cdots, (1-n)P - 1 \}$.\;
        Replace agents $i = i_j, 1 \leq j \leq k P$ by agents randomly(-uniformly) sampled from $\mathbb{M}$.\;
        Train agents $i=1, \cdots, P$ independently for $S$ steps using the RL agent template, sampling data from the replay buffers if they are used by the RL agent.\;
        \texttt{\\}
        
        \tcp{Repertoire Update}
        Run one episode in the environment using each of  $(\pi_{\theta_i})_{1 \leq i \leq P}$ to evaluate $(F(\pi_{\theta_i}))_{1 \leq i \leq P}$ and $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq P}$.\;
    Insert $((\pi_{\theta_i}, \phi_i, \mathbf{h}_i))_{1 \leq i \leq P}$ in $\mathbb{M}$ based on $(F(\pi_{\theta_i}))_{1 \leq i \leq P}$ and $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq P}$.\;

        Sample uniformly $2M$ agents from $\mathbb{M}$.\;
        Copy them and apply isoline variation to obtain $M$ offsprings $((\pi_{\theta_i}, \phi_i, \mathbf{h}_i))_{P < i \leq P + M}$.\;
        Run one episode in the environment using each of  $(\pi_{\theta_i})_{P < i \leq P + M}$ to evaluate $(F(\pi_{\theta_i}))_{P < i \leq P + M}$ and $(\Phi(\pi_{\theta_i}))_{P < i \leq P + M}$.\;
    Insert $((\pi_{\theta_i}, \phi_i, \mathbf{h}_i))_{P < i \leq P + M}$ in $\mathbb{M}$ based on $(F(\pi_{\theta_i}))_{P < i \leq P + M}$ and $(\Phi(\pi_{\theta_i}))_{P < i \leq P + M}$.\;
        
        Update $n_{\text{steps}}$.
        \texttt{\\}
    }
    
    \caption{\pbtme}
    \label{alg:main}
\end{algorithm}


\newpage

\begin{algorithm}[H]
    \small
    \SetAlgoLined
    \DontPrintSemicolon
    \SetKwInput{KwInput}{Given}
    \KwInput{
    \begin{itemize}
        \item $\mathbb{M}$: \me repertoire
        \item $N \in \mathbb{N}^*$: maximum number of environment steps 
        \item $M \in \mathbb{N}^*$: number of offsprings per iteration
        \item $F(\cdot)$: fitness function
        \item $\Phi(\cdot)$: behavior descriptor function
    \end{itemize}
    }
    \texttt{\\}

    \tcp{Initialization}
    Randomly initialize $M$ policies $(\pi_{\theta_i})_{1 \leq i \leq M}$.\;
    Run one episode in the environment using each of  $(\pi_{\theta_i})_{1 \leq i \leq M}$ to evaluate $(F(\pi_{\theta_i}))_{1 \leq i \leq M}$ and $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq M}$.\;
    Insert $(\pi_{\theta_i})_{1 \leq i \leq M}$ in $\mathbb{M}$ based on $(F(\pi_{\theta_i}))_{1 \leq i \leq M}$ and $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq M}$.\;
    \texttt{\\}

    \tcp{Main loop}
    Initialize $n_{\text{steps}}$, the total number of environment interactions carried out so far, to $0$.\;
    \While{$n_{\text{steps}} \leq N$}{
    \texttt{\\}

        Randomly sample $2 M$ policies from $\mathbb{M}$.\;
        Copy them and apply isoline variations to obtain $M$ new policies $(\pi_{\theta_i})_{1 \leq i \leq M}$.\;
        Run one episode in the environment using each of  $(\pi_{\theta_i})_{1 \leq i \leq M}$ to evaluate $(F(\pi_{\theta_i}))_{1 \leq i \leq M}$ and $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq M}$.\;
        Insert $(\pi_{\theta_i})_{1 \leq i \leq M}$ in $\mathbb{M}$ based on $(F(\pi_{\theta_i}))_{1 \leq i \leq M}$ and $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq M}$.\;
        Update $n_{\text{steps}}$.
    }

    \caption{\me}
    \label{alg:ME}
\end{algorithm}



% \newpage


\begin{algorithm}[H]
    \small
    \SetAlgoLined
    \DontPrintSemicolon
    \SetKwInput{KwInput}{Given}
    \KwInput{
    \begin{itemize}
        \item $\mathbb{M}$: \me repertoire 
        \item $N \in \mathbb{N}^*$: maximum number of environment steps 
        \item $M \in \mathbb{N}^*$: number of offsprings per iteration
        \item $S_c \in \mathbb{N}^*$: number of TD3 training steps used to update the shared critic per iteration
        \item $S_p \in \mathbb{N}^*$: number of TD3 policy update steps per iteration per policy
        \item TD3 hyperparameters
        \item $F(\cdot)$: fitness function
        \item $\Phi(\cdot)$: behavior descriptor function
    \end{itemize}
    }
    \texttt{\\}

    \tcp{Initialization}
    Initialize a replay buffer $\mathbb{B}$.\;
    Randomly initialize $M$ policies $(\pi_{\theta_i})_{1 \leq i \leq M}$.\;
    Run one episode in the environment using each of  $(\pi_{\theta_i})_{1 \leq i \leq M}$ to evaluate $(F(\pi_{\theta_i}))_{1 \leq i \leq M}$ and $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq M}$.\;
    Insert $(\pi_{\theta_i})_{1 \leq i \leq M}$ in $\mathbb{M}$ based on $(F(\pi_{\theta_i}))_{1 \leq i \leq M}$ and $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq M}$.\;
    Update $\mathbb{B}$ with transition data collected during the initial evaluations.\;
    Initialize the critic $Q_\phi$, the target critic $Q_{\phi'}$, the greedy policy $\pi_{\theta}$, and the target greedy policy $\pi_{\theta'}$. \;
    \texttt{\\}

    \tcp{Main loop}
    Initialize $n_{\text{steps}}$, the total number of environment interactions carried out so far, to $0$.\;
    \While{$n_{\text{steps}} \leq N$}{
    \texttt{\\}

        \tcp{Update the shared critic alongside the greedy policy}
        Carry out $S_c$ TD3 training steps to update $Q_\phi, Q_{\phi'}, \pi_{\theta}$ and $\pi_{\theta'}$ (sampling batches of data from $\mathbb{B}$).\;
        \texttt{\\}

        \tcp{Generate new offsprings using the isoline variation operator}
        Randomly sample $M$ policies from $\mathbb{M}$.\;
        Copy them and apply isoline variations to obtain $M / 2$ new policies $(\pi_{\theta_i})_{1 \leq i \leq M/2}$.\;
        \texttt{\\}

        \tcp{Generate new offsprings using TD3 policy-gradient updates}
        Randomly sample $M / 2 - 1$ policies from $\mathbb{M}$ $(\pi_{\theta_i})_{M/2 < i \leq M - 1}$.\;
        Carry out $S_p$ TD3 policy gradient steps for each of them independently (sampling batches of data from $\mathbb{B}$).\;
        \texttt{\\}

        \tcp{Update the repertoire}
        Assign $\pi_{\theta_M} = \pi_\theta$.\;
        Run one episode in the environment using each of  $(\pi_{\theta_i})_{1 \leq i \leq M}$ to evaluate $(F(\pi_{\theta_i}))_{1 \leq i \leq M}$ and $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq M}$.\;

        Insert $(\pi_{\theta_i})_{1 \leq i \leq M}$ in $\mathbb{M}$ based on $(F(\pi_{\theta_i}))_{1 \leq i \leq M}$ and $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq M}$.\;
        Update $\mathbb{B}$ with transition data collected during the evaluations of all $M$ new policies.\;
        Update $n_{\text{steps}}$.
    }

    \caption{\pgame}
    \label{alg:PGA-ME}
\end{algorithm}




\begin{algorithm}[H]
    \small
    \SetAlgoLined
    \DontPrintSemicolon
    \SetKwInput{KwInput}{Given}
    \KwInput{
    \begin{itemize}
        \item $\mathbb{M}$: \me repertoire
        \item $N \in \mathbb{N}^*$: maximum number of environment steps 
        \item $S \in \mathbb{N}^*$: number of consecutive gradient steps for a given policy
        \item $N_{\textbf{grad}} \in \mathbb{N}^*$: number of evaluations for gradient approximations
        \item $N_{\textbf{init}} \in \mathbb{N}^*$: number of randomly-initialized policies used to initialize $\mathbb{M}$
        \item $\sigma > 0$: standard deviation of the normal distribution used to perturb parameters for gradient approximations
        \item $\eta > 0$: learning rate
        \item $\mathbb{A}$: archive of behavior descriptors
        \item $N(\cdot, \cdot)$: novelty function that takes as an input a behavior descriptor as first argument and $\mathbb{A}$ as a second argument
        \item $F(\cdot)$: fitness function
        \item $\Phi(\cdot)$: behavior descriptor function
    \end{itemize}
    }
    \texttt{\\}

    \tcp{Initialization}
    
    Randomly initialize $N_{\textbf{init}}$ policies $(\pi_{\theta_i})_{1 \leq i \leq N_{\textbf{init}}}$.\;
    Run one episode in the environment using each of  $(\pi_{\theta_i})_{1 \leq i \leq N_{\textbf{init}}}$ to evaluate $(F(\pi_{\theta_i}))_{1 \leq i \leq N_{\textbf{init}}}$ and $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq N_{\textbf{init}}}$.\;
    Insert $(\pi_{\theta_i})_{1 \leq i \leq N_{\textbf{init}}}$ in $\mathbb{M}$ based on $(F(\pi_{\theta_i}))_{1 \leq i \leq N_{\textbf{init}}}$ and $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq N_{\textbf{init}}}$.\;
    Add $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq N_{\textbf{init}}}$ to $\mathbb{A}$.\;
    \texttt{\\}

    \tcp{Main loop}
    Initialize $n_{\text{steps}}$, the total number of environment interactions carried out so far, to $0$.\;
    Initialize $n_{\text{grads}}$, the total number of gradient steps carried out so far, to $0$.\;
    $\text{use\_novelty} = \text{true}$\;
    \While{$n_{\text{steps}} \leq N$}{
    \texttt{\\}
        \If{$n_{\text{grads}} \equiv 0 \mod S$}{
            \tcp{Decide if we should optimize for novelty or fitness.}
            Set $\text{use\_novelty}$ to $\text{true}$ with probability $0.5$ and to $\text{false}$ otherwise.\;
            \texttt{\\}

            \tcp{Sample a high-performing policy from $\mathbb{M}$}
            \eIf{$\text{use\_novelty}$}{
                Sample a policy $\pi_{\theta} \in \mathbb{M}$ uniformly from the set of five policies with the highest novelty $N(B(\pi_{\theta}), \mathbb{A})$.\;
            }{
                Sample, with probability $0.5$, a policy $\pi_{\theta} \in \mathbb{M}$ from the set of two policies with the highest fitness $F(\pi_{\theta})$ or from the last five updated policies.\;
            }
        }
        \texttt{\\}

        \tcp{Update the current policy using a gradient approximation}
        Sample $(\theta_i)_{1 \leq i \leq N_{\textbf{grad}}} \sim \mathcal{N}(\theta, \sigma^2 I)$ small perturbations of the current policy's parameters.\;
        Run one episode in the environment using each of the corresponding policies $(\pi_{\theta_i})_{1 \leq i \leq N_{\textbf{grad}}}$ to evaluate $(F(\pi_{\theta_i}))_{1 \leq i \leq N_{\textbf{grad}}}$ and $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq N_{\textbf{grad}}}$.\;

        \eIf{$\text{use\_novelty}$}{
            Compute the gradient approximation $\nabla \theta = \frac{1}{N_{\textbf{grad}} \sigma} \sum_{i=1}^{N_{\textbf{grad}}} N(\Phi(\pi_{\theta_i}), \mathbb{A}) \frac{\theta_i - \theta}{\sigma}$.\;
        }{
            Compute the gradient approximation $\nabla \theta = \frac{1}{N_{\textbf{grad}} \sigma} \sum_{i=1}^{N_{\textbf{grad}}} F(\pi_{\theta_i}) \frac{\theta_i - \theta}{\sigma}$.\;
        }

        Update $\theta = \theta + \eta \cdot \nabla \theta$.\;
        Run one episode in the environment using $\pi_\theta$ to compute $\Phi(\pi_{\theta})$ and $F(\pi_{\theta})$.\;
        Insert $\pi_\theta$ in $\mathbb{M}$ based on $\Phi(\pi_{\theta})$ and $F(\pi_{\theta})$.\;
        Add $\Phi(\pi_{\theta})$ to $\mathbb{A}$.\;
        Update $n_{\text{steps}}$.\;
        $n_{\text{grads}} = n_{\text{grads}} + 1$\;
    }

    \caption{\mees}
    \label{alg:ME-ES}
\end{algorithm}



\newpage


\begin{algorithm}[H]
    \small
    \SetAlgoLined
    \DontPrintSemicolon
    \SetKwInput{KwInput}{Given}
    \KwInput{
    \begin{itemize}
        \item $\mathbb{M}$: \me repertoire 
        \item $N \in \mathbb{N}^*$: maximum number of environment steps 
        \item $P \in \mathbb{N}^*$: size of the population of RL agents
        \item $S \in \mathbb{N}^*$: number of TD3 training steps per iteration per agent
        \item TD3 hyperparameters
        \item $N(\cdot)$: novelty reward function
        \item $F(\cdot)$: fitness function
        \item $\Phi(\cdot)$: behavior descriptor function
    \end{itemize}
    }
    \texttt{\\}

    \tcp{Initialization}
    Initialize a replay buffer $\mathbb{B}$.\;
    Randomly initialize P policies $(\pi_{\theta_i})_{1 \leq i \leq P}$.\;
    Run one episode in the environment using each of  $(\pi_{\theta_i})_{1 \leq i \leq P}$ to evaluate $(F(\pi_{\theta_i}))_{1 \leq i \leq P}$ and $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq P}$.\;
    Insert $(\pi_{\theta_i})_{1 \leq i \leq P}$ in $\mathbb{M}$ based on $(F(\pi_{\theta_i}))_{1 \leq i \leq P}$ and $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq P}$.\;
    Update $\mathbb{B}$ with transition data collected during the initial evaluations.\;
    Initialize the \emph{quality} (resp. \emph{diversity}) critic $Q^Q_{\phi}$ (resp. $Q^D_{\phi}$) and the corresponding target $Q^Q_{\phi'}$ (resp. $Q^D_{\phi'}$).\;
    \texttt{\\}

    \tcp{Main loop}
    Initialize $n_{\text{steps}}$, the total number of environment interactions carried out so far, to $0$.\;
    \While{$n_{\text{steps}} \leq N$}{
    \texttt{\\}

        Sample uniformly $P$ policies $(\pi_{\theta_i})_{1 \leq i \leq P}$ from $\mathbb{M}$.\;
        \texttt{\\}

        \tcp{Update the quality critic alongside the first half of the policies}
        \For{$s=1$ \KwTo $S$}
        {   
            Sample $P/2$ batches of transitions from $\mathbb{B}$.\;
            Carry out, using one batch of transition per agent, one TD3 training step for each of the agents $((\pi_{\theta_i}, Q^Q_{\phi}, Q^Q_{\phi'}))_{1 \leq i \leq P/2}$ in parallel, averaging gradients over the agents for the shared critic parameters.\;
        }
        \texttt{\\}

        \tcp{Update the diversity critic alongside the second half of the policies}
        \For{$s=1$ \KwTo $S$}
        {   
            Sample $P/2$ batches of transitions from $\mathbb{B}$.\;
            Overwrite the rewards using the novelty reward function $N(\cdot)$.\;
            Carry out, using one batch of transition per agent, one TD3 training step for each of the agents $((\pi_{\theta_i}, Q^D_{\phi}, Q^D_{\phi'}))_{P/2 < i \leq P}$ in parallel, averaging gradients over the agents for the shared critic parameters.\;
        }
        \texttt{\\}

        \tcp{Update the repertoire}
        Run one episode in the environment using each of  $(\pi_{\theta_i})_{1 \leq i \leq P}$ to evaluate $(F(\pi_{\theta_i}))_{1 \leq i \leq P}$ and $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq P}$.\;

        Insert $(\pi_{\theta_i})_{1 \leq i \leq P}$ in $\mathbb{M}$ based on $(F(\pi_{\theta_i}))_{1 \leq i \leq P}$ and $(\Phi(\pi_{\theta_i}))_{1 \leq i \leq P}$.\;
        Update $\mathbb{B}$ with transition data collected during the evaluations of all $P$ new policies.\;
        Update $n_{\text{steps}}$.
    }

    \caption{\qdpg}
    \label{alg:QDPG}
\end{algorithm}

\newpage


\begin{algorithm}[H]
    \small
    \SetAlgoLined
    \DontPrintSemicolon
    \SetKwInput{KwInput}{Given}
    \KwInput{ 
    \begin{itemize}
        \item $N \in \mathbb{N}^*$: maximum number of environment steps 
        \item $P \in \mathbb{N}^*$: size of the population of RL agents
        \item $S \in \mathbb{N}^*$: number of training steps per iteration per agent
        \item $p, n \in \left] 0, 1 \right[ $: \pbt proportions 
        \item an RL agent template
        \item $F(\cdot)$: fitness function
    \end{itemize}
    }
    \texttt{\\}
    
    \tcp{Initialization}
    Randomly initialize $P$ agents following the chosen RL template $((\pi_{\theta_i}, \phi_i, \mathbf{h}_i))_{1 \leq i \leq P}$. \;

    Initialize $P$ replay buffers $(\mathbb{B}_i)_{1 \leq i \leq P}$ (only if replay buffers are used by the RL agent).\;
    \texttt{\\}
    
    \tcp{Main loop}
    Initialize $n_{\text{steps}}$, the total number of environment interactions carried out so far, to $0$.\;
    \While{$n_{\text{steps}} \leq N$}{
    \texttt{\\}
        Train agents $i=1, \cdots, P$ independently for $S$ steps using the RL agent template and the replay buffers (only if replay buffers are used by the RL agent), interacting with the environment as many times as dictated by the RL agent.\;
        Run one episode in the environment using each of  $(\pi_{\theta_i})_{1 \leq i \leq P}$ to evaluate $(F(\pi_{\theta_i}))_{1 \leq i \leq P}$.\;

        Re-order the agents $i = 1, \cdots, P$ in increasing order of their fitnesses $(F(\pi_{\theta_i}))_{1 \leq i \leq P}$.\;
        Update agents $i=1, \cdots, p P$ by copying randomly-sampled agents from $i=(1-n) P, \cdots, P$ and copy the replay buffers accordingly (only if replay buffers are used by the RL agent).\;
        Sample new hyperparameters for agents $i=1, \cdots, p P$.\;
        Update $n_{\text{steps}}$.\;
    }

    \caption{\pbt}
    \label{alg:pbt}
\end{algorithm}
