\section{Background}\label{sec:background}

One of the major drawbacks of describing a problem in terms of fractional operators is the lack of uniqueness in the mathematical formulation. This contrasts sharply with the classical calculus. In fact, in fractional calculus there exists a variety of different definitions of fractional operators (differentials and integrals), and they are chosen according to a given set of assumptions imposed to satisfy different needs and constraints of the particular problem. There are already in the literature a number of excellent texts that review the mathematics of fractional calculus, \eg~\cite{mainardi_fractional_2010}. Among the different definitions of fractional operators, the Caputo fractional derivative appears as the most commonly used for modeling temporal evolution phenomena, which accounts for past interactions and also non-local properties.
%takes account of interactions within the past and also problems with non-local properties.
Moreover, mathematically the initial conditions imposed for fractional differential equations with Caputo derivatives coincides with the integer-order differential equations. Such derivative is defined as follows:
%\begin{equation}
%^{C}D_0^{\alpha}f(t)=\int_0^t %\frac{f^{(n)}(\tau)}{(t-\tau)^{\alpha+1-n}}d\tau, \mbox{ %with}\quad n-1<\alpha<n.
%\label{caputo_def}
%\end{equation}
%Note that if $0<\alpha<1$, $n$ turns out to be 1.
\begin{equation}
^{C}D_t^{\alpha}f(t)=\int_0^t \frac{f^{(n)}(\tau)}{(t-\tau)^{\alpha+1-n}}d\tau,
\label{caputo_def}
\end{equation}
with $n$ an integer such that $ n = \ceil{\alpha}$. Note that if $0<\alpha<1$, $n = 1$.

The simplest dynamic process described in terms of fractional operators consists in an initial value problem for a linear fractional rate equation, which for a given variable $y(t)$ is given by
\begin{equation}
^{C}D_t^{\alpha} y(t)=-\lambda y(t), \quad y(0)=y_0.
\end{equation}
The solution of this equation can not be expressed in a closed-form, being rather an infinite series. It was first obtained by the mathematician Mittag-Leffler, and is formally written as
\begin{equation}
y(t)=y_0\, E_{\alpha}(-\lambda t^{\alpha}),
\end{equation}
where $E_{\alpha}(t)$ is the function defined as
\begin{equation}
E_{\alpha}(-\lambda\, t^{\alpha})=\sum_{k=0}^{\infty} \frac{(-\lambda\,t^{\alpha})^k}{\Gamma(k\alpha+1)},
\end{equation}
and carries his name. Here $\Gamma(x)$ denotes the Euler's Gamma function. Note that when $\alpha=1$, it reduces to the exponential function. Essentially, the Mittag-Leffler function generalizes the exponential function, and has been considered for many as the {\it Queen function} of the fractional calculus~\cite{mainardi_why_2020, gorenflo_mittag-leffler_2020, gorenflo_fractional_1997}. For complex parameters $\alpha$ and $\beta$ with $\mathbb{R}\{\alpha\} > 0$, the Mittag-Leffler function (ML function) can be further generalized, and was defined as
\begin{equation}
\label{eq:ml_def}
    E_{\alpha,\beta}(z) = \sum_{k = 0}^{\infty}{\frac{z^k}{\Gamma(k\alpha + \beta)}}, z \in \mathbb{C}.
\end{equation}
Here $E_{\alpha, \beta}(z)$ is an entire function of order $\rho = 1 / \mathbb{R}\{\alpha\}$ and type 1.  The exponential function can be recovered by setting $\alpha = \beta = 1$ since $\Gamma(k~+~1)~=~k!$ for $k \in \mathbb{N}$. When $\beta = 1$, the notation of the ML function can be simplified to $E_\alpha(z) = E_{\alpha, 1}(z)$.


A naive approach to evaluate the ML function comes directly from its definition in (\ref{eq:ml_def}). However, this is ill-advised for most applications since the convergence of the series is very slow if either the modulus $\abs{z}$ is large or the value of $\alpha$ is small. Moreover, the terms of the series may grow very large before decreasing, which can cause overflows and catastrophic numerical cancellations when using standard double-precision arithmetic (IEEE 754). For this reason, the scalar ML function is usually evaluated through alternative methods~\cite{hilfer_computation_2006, garrappa_numerical_2015, seybold_numerical_2009, gorenflo_computation_2002}, such as the Taylor series, inversion of the Laplace transformation, integral representation and other techniques.

To evaluate the ML matrix function, Garrappa~\cite{garrappa_computing_2018} recently proposed a Schur-Parlett algorithm~\cite{davies_schur-parlett_2003}, which in practice requires computing its derivatives. This method, however, is only suited for small matrices since the Schur-Parlett algorithm scales with $O(n^3)$, but it can go up to $O(n^4)$ depending on the distribution of the eigenvalues. Moreover, if the eigenvalues are highly clustered, the algorithm may require high-order derivatives which are more expensive to compute, while also being less accurate. There is also a Krylov-based method for computing the action of a Mittag-Leffler function over a vector~\cite{moret_convergence_2011}, however, the code is not publicly available.

As an alternative to deterministic methods, Monte Carlo algorithms for approximating matrix functions have already been proposed in the past~\cite{forsythe_matrix_1950, dimov_monte_2008, dimov_parallel_2001, ji_convergence_2013, benzi_analysis_2017}, primarily for solving linear systems. In essence, these methods generate random walks sampled from a discrete Markov chain governed by the matrix $\mat{A}$, approximating each power of the Neumann series \cite{higham_functions_2008}. The convergence of this method was rigorously established in~\cite{benzi_analysis_2017, ji_convergence_2013, dimov_new_2015}. It is broadly accepted that these stochastic methods offer interesting features from a computational point of view, such as being easily parallelizable, fault-tolerant, and more suited to heterogeneous architectures (which are extremely important attributes in view of the current high performance computers). However, the truth is that they also exhibit some significant weaknesses, namely a very slow convergence rate to the solution. This can cause the underlying algorithms to be highly demanding computationally, especially when high accuracy solutions are required.

Only recently that Monte Carlo methods have been extended to compute other matrix functions, namely the matrix exponential \cite{acebron_monte_2019}. The method requires first to decompose the input matrix in two matrices, one of them a diagonal matrix, and the other a Laplacian matrix. The computation of the matrix exponential is then approximated using the Strang splitting. The algorithm consists on generating continuous-time random walks using the Laplacian matrix as the generator, with a holding time governed by an exponential distribution. Later, in ~\cite{acebron_highly_2020} it was applied a multilevel technique to improve the performance of this method. However, the Strang splitting cannot be applied to the ML matrix function, and thus, we have to generate the random walks directly using the input matrix.

For the specific case of the one-dimensional fractional diffusion equation,~\cite{nichols_subdiffusive_2018} introduced a Monte Carlo method for simulating discrete time random walks which are used for obtaining numerical solutions of the fractional equation, but not for computing the ML matrix function.


