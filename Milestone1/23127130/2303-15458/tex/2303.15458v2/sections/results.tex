\section{Numerical Results}\label{sec:results}

%% I use the Multilevel paper as a reference, double check if they are not too similar. Maybe added more information about fractional equations...
In this section, we evaluate the performance of our Monte Carlo method by solving time-fractional partial differential equations (fPDEs) through the method of lines. The method of lines~\cite{schiesser_numerical_1991, guo_fractional_2015} consists in discretizing the spatial variables of the initial problem to obtain a system of coupled fractional ordinary differential equations with time as the independent variable. The system can then be solved by computing the action of the Mittag-Leffler function over the discretized initial values. Using this method, we solve the Dirichlet boundary-value problem for both a 2D time-fractional diffusion equation and a 3D time-fractional convection-diffusion equation.

\subsection{Setup}

All simulations regarding the shared-memory architecture were executed on a commodity workstation with an AMD Ryzen 5800X 8C @4.7GHz and 32GB of RAM, running Arch Linux. The \texttt{MCML\_Full} (Algorithm \ref{code:ml_ctrw_full}) was implemented in C++ with OpenMP and uses \texttt{PCG64DXSM} \cite{oneill_pcg_2014} as its random number generator. \texttt{PCG64DXSM} is an improved version of the default generator of the popular NumPy \cite{numpy} module for Python. The code was compiled with the Clang/LLVM v14.0.0 with the \texttt{-O3} and \texttt{-march=znver3} flags. We will refer to the Monte Carlo method that uses dense and sparse matrices as {\tt mc\_dense} and {\tt mc\_sparse}, respectively.

To the best of our knowledge, the only freely available code capable of computing the Mittag-Leffler function for matrices was proposed by Garrappa and Popolizio~\cite{garrappa_computing_2018} and it is written in MATLAB 2021a. This code is based on the Schur-Parlett algorithm~\cite{davies_schur-parlett_2003, higham_functions_2008} that was implemented in MATLAB as the {\tt funm} command. The Schur-Parlett algorithm consists in decomposing the matrix $\mat{A}$ as $\mat{V}\mat{T}\mat{V}^{*}$ (Schur decomposition~\cite{golub_matrix_2013}) and then computing the Mittag-Leffler function on the upper triangular~$\mat{T}$ matrix using the Taylor series and the Parlett recurrence. The derivatives for the Taylor series are calculated through a combination of series expansion, numerical inversion of the Laplace transform and summation formula. We will refer to Garrappa and Popolizio's code as \texttt{matlab}.

Note that the MATLAB backend is written in C/C++ and calls the Intel Math Kernel Library~\cite{intel_mkl} for many matrix and vector operations, which exploits very efficiently the hardware resources of modern CPUs, including multi-threading and SIMD units. It is worth remarking that we only compare our code against a MATLAB implementation due to the lack of any parallel code freely available in C/C++.

Nevertheless, there are some noticeable differences between the algorithms. First of all, \texttt{matlab} only supports dense matrices and has a fixed precision (IEEE 754 double-precision standard), while ours supports both dense and sparse matrices as well as an user-defined precision. Moreover, a Monte Carlo method is not only fully parallelizable but also allows the computation of single entries of the vector solution, which is not possible with the classical deterministic algorithm. Due to the random nature of the Monte Carlo algorithms, all results reported in this section correspond to the mean values obtained after several runs. With the exception of the variance analysis, the program calculates the solution for all points in domain in all simulations.

\subsection{Time-fractional diffusion equation} \label{sec:poisson}

The first example we consider consists in solving the 2D time-fractional diffusion equation,  
\begin{equation}
\label{eq:heat}
D^\alpha_t u(\vec{x}, t) = \nabla^2 u(\vec{x}, t),
\end{equation}
in a domain $\Omega = [-\mu, \mu]^2$, with time $t > 0$,  a space variable $\vec{x}~=~(x, y)~\in~\mathbb{R}^2$, an initial condition $u(\vec{x}, 0)=u_0(\vec{x})$ and a Dirichlet boundary condition $u(\vec{x}, t)\lvert_{\partial\Omega}\ = 0$. Here $D^\alpha_t$ is the Caputo's fractional derivative of order $\alpha$ with $0<\alpha \leq 1$. Considering a discrete mesh with $m$ cells in each dimension, such that $\Delta x = \Delta y = 2\mu/m$, and the standard 5-point stencil finite difference approximation, the approximated solution $\hat{u}(\vec{x}, t)$ for (\ref{eq:heat}) can be written as
\begin{equation}
\label{eq:heat_solution}
\hat{u}(\vec{x}, t) = E_\alpha \left (\frac{m ^ 2}{4\mu^2} \, \hat{\mat{L}} \, t^\alpha \right) \, \hat{u}_0(\vec{x}) = E_\alpha(\mat{A} \, t^\alpha) \, \hat{u}_0(\vec{x})
\end{equation}
where $\hat{\mat{L}}$ denotes the discrete Laplacian operator. Here, we consider that the off-diagonal entries of $\hat{\mat{L}}$ are positive and the diagonal negative.

% If $\mat{I}$ is the $m \times m$ identity matrix and $\mat{C}$ is an $m \times m$ matrix defined as follows, then matrix $\hat{\mat{L}}$ is given as
%
% \begin{equation*}
%     \hat{\mat{L}} = \begin{bmatrix}
%           ~\mat{C} & ~\mat{I} & ~0 & ~0 & \cdots & ~0 \\
%           ~\mat{I} & ~\mat{C} & ~\mat{I}  & ~0 & \cdots & ~0 \\
%           ~0 & ~\mat{I} & ~\mat{C} & ~\mat{I} & \cdots & ~0 \\
%           \vdots & \ddots  & \ddots & \ddots & \ddots & \vdots \\
%           ~0 & \cdots & ~0 & ~\mat{I} & ~\mat{C} & ~\mat{I} \\
%           ~0 & \cdots & \cdots & ~0 & ~\mat{I} & ~\mat{C}
%         \end{bmatrix}
%     \qquad
%     \mat{C} = \begin{bmatrix}
%           -4 & ~1 & ~0 & ~0 & \cdots & ~0 \\
%           ~1 & -4 & ~1 & ~0 & \cdots & ~0 \\
%           ~0 & ~1 & -4 & ~1 & \cdots & ~0 \\
%           \vdots & \ddots & \ddots & \ddots & \ddots & \vdots \\
%           ~0 & \cdots & ~0 & ~1 & -4 & ~1 \\
%           ~0 & \cdots & \cdots & ~0 & ~1 & -4
%         \end{bmatrix}
% \end{equation*}

% \begin{figure}[t]
% 	\centering	\includegraphics[width=\textwidth]{figures/poisson-solution}
% 	\caption{The solution of a 2D time-fractional diffusion for different values of $\alpha$ considering a discrete mesh with $m = 160$ cells and a time $t = 0.1$.}
% 	\label{fig:poisson_solution}
% \end{figure}

This example is particularly suited for analyzing the numerical errors since the eigenvalues of the matrix $\mat{A}$ can be calculated analytically~\cite{strang_computational_2007} and, consequently, the corresponding solution of the equation. In fact, for a point $\vec{x} = (x, y)$ in the discrete domain, the corresponding eigenvalue $\lambda(\vec{x})$ and eigenvector $\vec{v}(\vec{x})$ are known to be equal to
\begin{align*}
    \lambda(\vec{x}) &= \frac{m^2}{4\mu^2} \left [ 2 \cos \left (\frac{x \pi}{m + 1} \right ) + 2 \cos \left (\frac{y \pi}{m + 1} \right ) - 4 \right ] \\
   \vec{v}(\vec{x}) &= \left \{ v_{(i, j)} = \sin \left ( \frac{ix \pi}{m + 1}  \right ) \sin \left ( \frac{jy \pi}{m + 1} \right ) \text{ for } i, j = 1, 2, \ldots, m \right \}
\end{align*}

After organizing the eigenvalues and eigenvectors of $\mat{A}$ into the matrices $\mat{D}$ and $\mat{V}$, respectively, the solution for the 2D diffusion problem can be calculated as

\begin{equation}
    \hat{u}(\vec{x}, t) = \mat{V} \, E_\alpha (\mat{D} \, t^\alpha) \, \mat{V}^\intercal \, \hat{u}_0(\vec{x})
    \label{eq:poisson_analytic_solution}
\end{equation}

Moreover, we can determine the stiffness ratio $r = \lambda_{max} / \lambda_{min}$~\cite{lambert_computational_1973} for this problem, which is given asymptotically by $r \sim 4m^2 / \pi^2$ for large values of $m$. In particular, the diffusion problem can be considered stiff for the range of values of $m$ used in this section and, thus, is a suitable example for testing the performance of our method in solving this class of problems.

For all the results shown in this section, we chose the domain to be $\Omega = [-1, 1]^2$ and the initial condition $\hat{u}_0(\vec{x}) = c \, m^2 \, \delta (\vec{x}-\vec{x}_c)$, which consists on a discrete impulse located at the center of the computational grid, $\vec{x}_c~=~(m / 2, m / 2)$, and strength $c=1/4096$. Since the {\tt matlab} code only works with dense matrices, it requires around $25$GB of memory when calculating the solution for a discrete mesh with $m = 160$. In comparison, {\tt mc\_dense} uses around $10$GB of memory for the same mesh, while {\tt mc\_sparse}, only a couple of megabytes. For this reason, the maximum number of cells $m$ in any experiment will be equal to $160$. Regarding the time variable $t$, we choose $t = 0.1$ where it is possible to clearly distinguish the diffusion process for different values of $\alpha$.


\begin{figure}[t]
\centering
\begin{minipage}[t]{0.475 \textwidth}
	\centering	\includegraphics[width=\textwidth]{figures/poisson-err}
	\caption{Maximum absolute error as a function of the number of random paths when solving the 2D diffusion equation for $t = 0.1$ and $m = 80$.}
	\label{fig:poisson_err}
\end{minipage} %
\quad
\begin{minipage}[t]{0.475 \textwidth}
	\centering	\includegraphics[width=\textwidth]{figures/poisson-error-dist}
	\caption{A histogram of the maximum absolute error for $8000$ runs, each one starting with a different random seed. In each run, we solved the 2D diffusion equation with $t = 0.1$, $m = 80$ and $N_p = 2.5 \times 10^{5}$.}
	\label{fig:poisson_err_dist}
\end{minipage}
\end{figure}

Fig.~\ref{fig:poisson_err} shows the maximum absolute error as function of the number of random paths $N_p$. The error has been computed using the analytical solution in (\ref{eq:poisson_analytic_solution}). Recall from Section~\ref{sec:implementation} that the numerical error of the Monte Carlo method approximates a Gaussian random variable with standard deviation determined by $\sigma \, N_p^{-1/2}$. This relation is confirmed numerically by the trend line in the graph, which has a slope of approximately $-1/2$ in the logarithmic scale. Repeating the Monte Carlo simulation several times with different initial seeds, we observe in Fig.~\ref{fig:poisson_err_dist} that the numerical errors are indeed distributed according to a normal distribution.

An estimation of the variance can be obtained for this simple initial condition, and in the limiting case when $m\gg 1$. Note that for this case there are only two possible outcomes for the random variable $\omega$ in (\ref{eq:single_solution}), $0$ or $c\, m^2$, with probabilities $1-\xi$ and $\xi$, respectively. The outcome $c\, m^2$ is obtained when the ending state of the random path coincides with the node of the computational mesh corresponding to the point $\vec{x}_c$, being $0$ otherwise. For simplicity, let us assume that we choose the point where the solution is computed to be the same $\vec{x}_c$. Then, the outcome will be different from $0$ only when,
after a random number of jumps $n$ in a random path, the final state is the same as the initial one. Estimating the probability of $n$ is straightforward, since this problem is equivalent to the problem of computing the probability of returning to the initial point after $n$ steps for a simple symmetric 2D random walk \cite{ibe_elements_2013}, and is given by
\begin{equation}
h_n=\left[\binom{n}{n/2}\frac{1}{2n}\right]^2, \quad n=0,2,4,\ldots.
\end{equation}
\\ \\
Asymptotically when $n\gg 1$, $h_n\sim 2/\pi (n+1)$. Concerning the number of transitions, this is a random variable governed by a probability distribution $P_n$, and depends on the number of events occurring during the time interval $[0,t]$. Moreover, since the sojourn time in every state is Mittag-Leffler distributed, then the probability can be estimated, and it turns out to be the so-called fractional Poisson distribution~\cite{laskin_fractional_2003}, where the mean number of events $\bar{n}$ is
\begin{equation}
\bar{n}=\frac{m^2 t^{\alpha}}{\Gamma{(\alpha+1)}},
\label{eq:mean_num_states}
\end{equation}
and the probability of no events occurring during this time interval is \linebreak
$P_0 = E_\alpha (m^2 \, t^\alpha)$. When $m\to \infty$, a simple piecewise constant function can be used as an approximation, and yields
\begin{equation}
 P_n \approx 
    \begin{cases}   P_0,   & n\le \frac{1}{P_0} \\
                    0,     & n> \frac{1}{P_0}. 
    \end{cases}
\end{equation}
Then, it follows that the probability $\xi$ is given by
\begin{equation}
\xi=\sum_{n=0}^\infty h_{2n} P_{2n}\approx P_0+P_0\sum_{n=1}^{1/P_0} h_{2n}.
\end{equation}
Using the known difference equation $\sum_{k=0}^{n-1}(k+x)^{-1}=\psi(x+n)-\psi(x)$, where $\psi(x)$ is the digamma function, we obtain
\begin{equation}
\xi\approx P_0+\frac{P_0}{\pi}\left[\psi \left(\frac{3}{2}+\frac{1}{P_0} \right)-\psi \left(\frac{5}{2} \right)\right].
\end{equation}
When $m \to \infty$, $P_0\sim [m^2 \, t^{\alpha} \, \Gamma(1-\alpha)]^{-1}$, and then using the asymptotic expansion properties of the digamma function it holds that $\xi\sim [m^{2} \, t^{\alpha} \, \Gamma(1~-~\alpha)]^{-1}$. Thus, asymptotically an estimation of the variance $\sigma^2$ is given by
\begin{equation}
\sigma^2\sim (c\, m^2)^2\xi=c^2\,m^2 \frac{t^{-\alpha}}{\Gamma(1-\alpha)}=c^2\,N \frac{t^{-\alpha}}{\Gamma(1-\alpha)}.
\label{eq:variance_poisson}
\end{equation}

\begin{figure}[t]
	\centering	\includegraphics[width=\textwidth]{figures/poisson-scale-var}
	\caption{Variance $\sigma^2$ of the Monte Carlo algorithm as a function of the matrix size $N = m^2$ and $\alpha$ when solving the 2D diffusion equation at a single point located at the centre of the mesh for $t = 0.1$. The number of random paths was kept fixed to $10^{10}$.}
	\label{fig:poisson_scale_var}
\end{figure}

This relation can be seen in Fig. \ref{fig:poisson_scale_var}. For a fixed $\alpha$ and $t$, the variance $\sigma^2$ scales linearly with the matrix size $N = m^2$. Likewise, for a fixed $N$, $t$, the variance $\sigma ^2$ is proportional to $[t^{\alpha} \, \Gamma(1~-~\alpha)]^{-1}$. Note that the points in the graph do not perfectly match the curve from (\ref{eq:variance_poisson}) due to the low number of cells $m$ used in the simulation. Still, it provides the best approximation for the variance $\sigma^2$ under these conditions.

\begin{figure}[t]
\centering
\begin{minipage}[t]{0.475 \textwidth}
	\centering	\includegraphics[width=\textwidth]{figures/poisson-time-scale}
	\caption{Elapsed time for solving the 2D diffusion equation as a function of the time $t$ and the parameter $\alpha$ for $m = 80$. The number of random paths was kept fixed at $10^5$.}
	\label{fig:poisson_time_scale}
\end{minipage} %
\quad
\begin{minipage}[t]{0.475 \textwidth}
	\centering	\includegraphics[width=\textwidth]{figures/poisson-scale-time}
	\caption{Elapsed time for solving the 2D diffusion equation as a function of the matrix size $N = m^2$ and the parameter $\alpha$ for $t~=~0.1$. The number of random paths was kept fixed at $10^6$.}
	\label{fig:poisson_scale_samples}
\end{minipage}
\end{figure}

Concerning the computational cost of the Monte Carlo algorithm, this depends on the number of random paths and the time for generating each path. Even though this time is random, we can readily assume to be proportional to the mean time of events in (\ref{eq:mean_num_states}). Therefore, the computational cost of the algorithm is of the order of $O(N_p \, N \, t^\alpha)$, which is in good agreement with the results shown in Fig.~\ref{fig:poisson_time_scale} and~\ref{fig:poisson_scale_samples}. For a fixed number of random paths~$N_p$ and matrix size $N$, the execution time is proportional to $t^{\alpha}$ as shown in Fig.~\ref{fig:poisson_time_scale}. Note that the scale is $log-log$ and the slope of the curves coincides with $\alpha$. Similarly, when we kept  fixed both the time $t$ and number of random paths $N_p$, the execution time grows linearly with the matrix size $N$ as shown in Fig.~\ref{fig:poisson_scale_samples}. It is worth mentioning that exponential random numbers are much cheaper to produce than ML random numbers, resulting in significantly lower execution times when $\alpha = 1$.

\afterpage{
\begin{figure}[H]
	\centering	\includegraphics[width=\textwidth]{figures/poisson-scale}
	\caption{Serial (left) and parallel with 8 threads (right) execution times for solving the 2D diffusion equation as a function of the matrix size $N = m^2$ for $t = 0.1$. For the Monte Carlo algorithms, the accuracy $\epsilon$ was kept fixed at $3 \times 10^{-4}$.}
	\label{fig:poisson_scale}
\end{figure}
}

To compare the performance and scaling of our methods against a classical approach, we measure the serial and parallel execution times for different matrix sizes $N = m^2$ as shown in Fig.~\ref{fig:poisson_scale}. In terms of execution time, the Monte Carlo algorithm is several times faster than the {\tt matlab} code, especially when fully exploiting the sparse nature of the problem, while maintaining a reasonable precision (\ie, the maximum error in any simulation is below $3 \times 10^{-4}$).  When using 8 threads and considering a discrete mesh with $m = 128$, both \texttt{matlab} and \texttt{mc\_dense} shows a speedup of $6.5$ (or an efficiency of $81.25\%$), while \texttt{mc\_sparse} presents a speedup of $7.5$ ($95\%$ efficiency). 

Since matrix $\mat{A}$ is real and symmetric, its Schur decomposition can be written as $\mat{A} = \mat{V}\mat{D}\mat{V}^\intercal$, where $\mat{D}$ is a diagonal matrix containing the eigenvalues of $\mat{A}$, and the columns of the matrix $\mat{V}$ the corresponding eigenvectors. In this case, the matrix function $f(\mat{A})$ can be simply evaluated as $f(\mat{A}) = \mat{V}f(\mat{D})\mat{V}^\intercal$~\cite{davies_schur-parlett_2003}.  MATLAB uses a QR algorithm to compute the Schur decomposition, directly calling the corresponding routine from the Intel Math Kernel Library (\eg, \texttt{dsteqr} for real symmetric matrices). The QR algorithm has a well-known computational complexity of order $O(N^3)$. 

In comparison, the Monte Carlo algorithm requires an order of $O(N^2)$ operations to solve the diffusion equation, as shown in Fig.~\ref{fig:poisson_scale}. Considering that the variance $\sigma^2$ and, consequently, the statistical error grow with the matrix size $N$, the number of random paths must increase proportionally to the matrix size in order to keep the same level of accuracy for the numerical solution. 

\subsection{Time-fractional convection-diffusion equation} \label{sec:fem}

\begin{figure}
\centering
\begin{minipage}[c]{0.475 \textwidth}
	\centering \includegraphics[width=\textwidth]{figures/fem-mesh}
	\caption{Computational mesh of the domain where the fractional convection-diffusion equation is solved.}
	\label{fig:fem_mesh}
\end{minipage}%
\quad
\begin{minipage}[c]{0.475 \textwidth}
	\centering	\includegraphics[width=\textwidth]{figures/fem-eigen-hist}
	\caption{Histogram of the eigenvalues of $\mat{A} = \mat{B}^{-1}\mat{K}$ from (\ref{eq:fem_solution}). Here  the scale parameter was set to $s = 30$, which results in a matrix of size $N~=~453$.}
	\label{fig:fem_eigen_hist}
\end{minipage}
\end{figure}

The second example we discuss in this paper consists in solving a time-fractional convection-diffusion equation:
\begin{equation}
\label{eq:convec_diff}
D^\alpha_t u(\vec{x}, t) = c\nabla^2 u(\vec{x}, t) + \nu\nabla u(\vec{x}, t)
\end{equation}
with boundary and initial conditions, respectively
\begin{equation}
\label{eq:diff_initial}
u(\vec{x}, t)\lvert_{\partial\Omega}\ = g(\vec{x}, t) \qquad u(\vec{x}, 0) = f(\vec{x})
\end{equation}
where $c$ is the diffusion coefficient and $\nu$ the velocity field. After applying the standard Galerkin finite element method~\cite{zienkiewicz_finite_2013} to the discretized nodes $x_i$ for $i = 0, 1, \dots, n$, we obtain the following system of equations:
\begin{equation}
\label{eq:fem}
\mat{B} D^\alpha_t \vec{u} = \mat{K} \vec{u} + \vec{h}, \qquad \vec{u}(0) = \vec{u}_0
\end{equation}
where $\vec{u} = \{u(x_1, t), u(x_2, t), \dots, u(x_n, t)\}$, $\mat{B}$ is the assembled mass matrix, $\mat{K}$ is the corresponding assembled stiffness matrix and $\vec{h}$ is the load vector. The diffusion coefficient $c$, the velocity field $\nu$, and the boundary conditions are already included in these matrices and vector. To simplify the computation, the mass matrix was lumped \cite{zienkiewicz_finite_2013}, transforming the mass matrix into a diagonal matrix. 

The solution for the inhomogeneous systems of fPDEs in (\ref{eq:fem}) can be written in terms of the Mittag-Leffler function as follows 

\begin{equation}
\label{eq:fem_solution}
\vec{u}(\vec{x}, t) = E_\alpha(\mat{A} \, t^\alpha) \ \vec{u}_0(\vec{x}) + \int_0^t ds (t - s)^{\alpha - 1} E_\alpha(\mat{A} \, (t - s)^\alpha) \vec{v} 
\end{equation}
with $\mat{A} = \mat{B}^{-1}\mat{K}$ and $\vec{v} = \mat{B}^{-1}\vec{h}$. Note that the second term in the right side depends on the boundary data, and in particular for numerical purpose it will require to approximate a definite integral. This will entail another source of error along with the aforementioned statistical error. Since the goal of this manuscript is to focus solely on computing numerically the ML function, and analyzing the associated statistical error, here we set a null value for the Dirichlet boundary conditions $f(\vec{x}) = 0$, and hence $\vec{h} = 0$. Clearly the general case deserves further investigation, a detailed analysis is left for a future manuscript. 

For the domain, we consider here a block of aluminium of $2s \times 2s \times 20s$ ($W \times H \times L$, in cm) with a scale parameter $s$, an isotropic diffusion coefficient $c~=~9.4 \times 10^{-5} m^2 / s$~\cite{parker_flash_1961}, a field velocity $\nu = 0$ and a Dirichlet boundary condition $u = 0$ on the surface of the block. The finite-element mesh in Fig.~\ref{fig:fem_mesh} was generated using the scientific application tool \textit{COMSOL}~\cite{comsol} considering a maximum and minimum element size equal to $20$ cm and $0.1$ cm, respectively. Based on this mesh, we generated the corresponding finite-element method (FEM) matrices and vectors. For the initial conditions we use again a discrete impulse located at the position $(s, s, 10s)$. It is worth mentioning that the resulting matrix is asymmetric, having entries entirely arbitrary in both sign and magnitude, and in this sense this can be considered as a much more complex example than the case analysed in the previous section.

\begin{table}[t]
\centering
\caption{The execution time of the \texttt{matlab} code for computing the solution of the 3D convection-diffusion equation for different values of $t$ and $\alpha$ considering a scale parameter $s = 30$ (the total number of nodes in the computational mesh was $453$). The parameters of \texttt{funm} were kept at their default values. The number in parenthesis indicates the order of the derivative of the largest block.}
    \begin{tabular}[c]{ccccc}\toprule
    & $\alpha = 1$ & $\alpha = 0.9$ & $\alpha = 0.7$ & $\alpha = 0.5$\\\midrule
    $t = 20$ & $6.43\ (19)$ & FAIL & FAIL & $919\ (21)$\\\midrule
    $t = 40$ & $5.81\ (22)$ & FAIL & FAIL & $965\ (24)$\\\midrule
    $t = 60$ & $5.55\ (24)$ & FAIL & FAIL & $967\ (24)$\\\midrule
    $t = 80$ & $6.15\ (27)$ & FAIL & FAIL & $971\ (25)$\\\midrule
    $t = 100$ & $6.78\ (30)$ & FAIL & FAIL & $982\ (27)$\\\bottomrule
    \end{tabular}
\label{table:fem_matlab}
\end{table}

\begin{figure}[t]
        \centering	\includegraphics[width=\textwidth]{figures/fem-time-scale} \label{fig:fem_time_scale}
        \caption{Serial execution time of the probabilistic algorithm for computing the solution of the 3D convection-diffusion equation for different values of $t$ and $\alpha$, considering a scale parameter $s = 30$ (the total number of nodes in the computational mesh was $453$). The number of random paths was kept fixed at $10^8$, resulting in a precision $\epsilon \approx  10^{-4}$.}
\end{figure}

Similarly to the previous example, we examine the eigenvalues of matrix $\mat{A}$ to gain some insight about its numerical properties. In this case, the eigenvalues were calculated through the command \texttt{eigs} of MATLAB, which corresponds in essence to the Arnoldi method. Fig.~\ref{fig:fem_eigen_hist} depicts the histogram for the eigenvalues of the matrix $\mat{A}$. The eigenvalues are not only small but  are also highly clustered between $-0.02$ and $-0.05$, which is a problem for the \texttt{matlab} code of \cite{garrappa_computing_2018}. On one hand, this leads to very large blocks during the partitioning of the Schur form of $\mat{A}$ in the Schur-Parlett algorithm~\cite{davies_schur-parlett_2003, garrappa_computing_2018}. The evaluation of the Taylor series is particularly expensive for large blocks since it requires order $O(m^4)$ operations, where $m$ is the size of the block. On the other hand, the numerical evaluation of high-order derivatives of the Mittag-Leffler function often entails very large numbers even for small input values due to the rapidly increasing factorial $(x)_k = x(x - 1) \dots (x - k + 1)$:
\begin{equation}
\label{eq:ml_derivative}
   \frac{d^k}{dz^k} E_{\alpha}(z) = \sum_{j = k}^{\infty}{\frac{(j)_k}{\Gamma(\alpha j + 1)}z^{j - k}}, \quad k \in \mathbb{N}
\end{equation}

Indeed, MATLAB reports an infinite derivative when trying to compute the ML function for $\alpha = 0.7$ and the algorithm fails to converge to a solution for $\alpha = 0.9$ even after computing $250$ terms of the Taylor series. This is shown in Table~\ref{table:fem_matlab}. The \texttt{matlab} algorithm was able to reach a solution for $\alpha = 0.5$ and $\alpha = 1$. 

In the Schur-Parlett algorithm, the Schur form of $\mat{A}$ are divided into blocks according to its eigenvalues. In particular, if the absolute difference between two eigenvalues is less than a tolerance $\delta$, they will be assigned to the same block. With the default tolerance $\delta = 0.1$, almost all eigenvalues of $\mat{A}$ will be grouped into a single, large block. Even after decreasing the tolerance to $\delta = 0.01$, most eigenvalues are still concentrated into a single block. Note that the value of $\delta$ cannot be too small, otherwise the two distinct blocks may contain close eigenvalues, causing the Parlett's recurrence to break down due to numerical cancellations~\cite{davies_schur-parlett_2003, higham_accuracy_2002}. It is worth mentioning that the derivatives of the ML function are calculated serially and, thus, the \texttt{matlab} code shows very little speedup when using multiple threads.

\begin{figure}[t]
    \centering
    \begin{minipage}[t]{0.475 \textwidth}
    	\centering	\includegraphics[width=\textwidth]{figures/fem-err}
    	\caption{Maximum absolute error of the \texttt{mc\_sparse} algorithm as a function of the number of random paths when solving the 3D convection-diffusion equation for time $t = 40$ and scale parameter $s = 100$ (the total number of nodes in the computational mesh was $25 120$). }
    	\label{fig:fem_err}
    \end{minipage}%
    \quad
    \begin{minipage}[t]{0.475 \textwidth}
    	\centering	\includegraphics[width=\textwidth]{figures/fem-scale-err}
    	\caption{Maximum absolute error of the \texttt{mc\_sparse} algorithm as a function of the number of nodes $N$ in the computational mesh when solving the 3D convection-diffusion equation for $t = 40$. The number of random paths was kept fixed at $10^9$.}
    	\label{fig:fem_scale_err}
    \end{minipage}
\end{figure}

Concerning the computational cost of the Monte Carlo method for solving this problem, recall that this depends on the number of random paths and the mean number of events occurred during the prescribed time interval. The former is chosen according to the desired level of accuracy for the numerical solution. As it was already explained for the previous example, the numerical error depends not only on the number of random paths but also on the variance, which depends on the specific entries of the matrix $\mat{A}$ and the input vector $\vec{u}$. The more heterogeneous they are, the larger the variance will be, and consequently more random paths will be required for attaining the prescribed accuracy. Regarding the mean number of events, this depends on the specific values of the diagonal entries of $\mat{A}$, being larger for larger entries. 

In contrast to the \texttt{matlab} code which depends strongly on the distribution of eigenvalues, the Monte Carlo method do not show explicit dependency on them. For this reason, the stochastic method was able to compute the solution regardless of the values of $t$ or $\alpha$. The execution time of {\tt mc\_dense} and {\tt mc\_sparse} for different values of $t$ and $\alpha$ are shown in Fig. \ref{fig:fem_time_scale}. Regarding the parallel performance, {\tt mc\_dense} and {\tt mc\_sparse} usually achieve a speedup between $5$ and $6$ when using $8$ threads.

Figures~\ref{fig:fem_err} and~\ref{fig:fem_scale_err} show the maximum absolute error as a function of the number of random paths and the matrix size $N$, respectively. Note that the numerical error of the method $\epsilon$ decreases with the square root of the number of random paths as theoretically expected.

\begin{figure}[t]
    \centering	\includegraphics[width=\textwidth]{figures/fem-scale}
    \caption{Parallel execution time of the Monte Carlo method as a function of the number of nodes $N$ in the computational mesh when computing the solution of the 3D convection-diffusion equation for $t = 40$. The number of random paths was kept fixed at $10^9$, resulting in an error $\epsilon \approx 2 \times 10^{-5}$.}
    \label{fig:fem_scale}
\end{figure}

In the previous example, the domain was fixed to $\Omega~=~[-1, 1]^2$, and the matrix size was enlarged increasing the number of grid points inside the domain. As a result the magnitude of the diagonal entries of the matrix increases accordingly, and so do the mean number of events and the variance.  We now consider a case where the distance between two nodes in the finite-element mesh was kept constant when the domain is conveniently scaled in order to increase the matrix size. This leads to a matrix with diagonal entries independent of the matrix size, and therefore, the execution time and error of the Monte Carlo algorithm only depend on the number of random paths. This is shown in Fig.~\ref{fig:fem_scale_err} and~\ref{fig:fem_scale}. Note however that the algorithm needs to perform a binary search for selecting the next state of the random path, which causes the execution time to slightly grow as the matrix size increases. This has a negligible effect on the sparse implementation due to a low number of nonzero entries per matrix row.

\subsection{Distributed-Memory Performance} \label{sec:perf_dist}

\begin{figure}[t]
\centering
\begin{minipage}[t]{0.475 \textwidth}
    \centering	\includegraphics[width=\textwidth]{figures/poisson-mpi}
    \caption{Weak scaling of \texttt{mc\_sparse} in the Karolina supercomputer. This test solves a 2D diffusion equation for $t = 0.1$, $\mu = 1$ and $m = 1024$ ($N = m^2 = 1,048,576$). The number of random paths was chosen based on the number of cores, beginning with $2 \times 10^6$.}
    \label{fig:poisson_mpi}
\end{minipage} %
\quad
\begin{minipage}[t]{0.475 \textwidth}
    \centering	\includegraphics[width=\textwidth]{figures/fem-mpi}
    \caption{Weak scaling of \texttt{mc\_sparse} in the Karolina supercomputer. This test solves a 3D convection-diffusion equation for $t = 40$ considering a scalar parameter $s = 300$ (the total number of nodes in the computational mesh was $753,675$). The number of random paths was chosen based on the number of cores, beginning with $10^{11}$.}
    \label{fig:fem_mpi}
\end{minipage}
\end{figure}

The distributed-memory tests were carried out at the Karolina Supercomputer located in IT4Innovations National Supercomputing Centre. Each computational node in the cluster contains two AMD 7H12 64C~@2.6GHz CPUs and 256GB of RAM, running CentOS 7. For all tests, two MPI processes were launched on each node (one per processor) with as many threads as the number of physical cores (64). Since there is no freely available code capable of computing the action of a Mittag-Leffler function over a vector that is suitable for distributed-memory systems, this section only contains the results for the Monte Carlo method (\texttt{mc\_sparse}). The C++ code was compiled with GCC 12.1.0 and OpenMPI 4.1.4 with the {\tt -O3} and {\tt -march=native} flags. 

Figures~\ref{fig:poisson_mpi} and~\ref{fig:fem_mpi} show the weak scaling of \texttt{mc\_sparse} between 1 and 128 nodes ($16,384$ cores), considering a single computational node as the baseline. While the scaling of the domain size is typically used to increase the workload, for the specific FEM problem in (\ref{eq:fem}), as it was shown in the previous section, this does not affect the computational time, and therefore does not provide any useful insight about the scalability of the algorithm. Instead, we increased the workload by scaling the number of random paths according with the number of nodes used. In doing so the statistical error reduces, improving the accuracy of the computed solution.

Since all random paths in the Monte Carlo method are independent, the task of generating them can be done in parallel and the parallel code only needs to communicate at the end of the simulation to combine the results. This is the reason that the \texttt{mc\_sparse} shows perfect scalability in both examples.
