\section{Background}\label{sec:background}

One of the major drawbacks of describing a problem in terms of fractional operators is the lack of uniqueness in the mathematical formulation. This contrasts sharply with the classical calculus. In fact, in fractional calculus there exists a variety of different definitions of fractional operators (differentials and integrals), and they are chosen according to a given set of assumptions imposed to satisfy different needs and constraints of the particular problem. There are already in the literature a number of excellent texts that review the mathematics of fractional calculus, \eg~\cite{mainardi_fractional_2010}. Among the different definitions of fractional operators, the Caputo fractional derivative appears as the most commonly used for modeling temporal evolution phenomena, which accounts for past interactions and also non-local properties.
%takes account of interactions within the past and also problems with non-local properties.
Moreover, mathematically the initial conditions imposed for fractional differential equations with Caputo derivatives coincides with the integer-order differential equations. Such derivative is defined as follows:
%\begin{equation}
%^{C}D_0^{\alpha}f(t)=\int_0^t %\frac{f^{(n)}(\tau)}{(t-\tau)^{\alpha+1-n}}d\tau, \mbox{ %with}\quad n-1<\alpha<n.
%\label{caputo_def}
%\end{equation}
%Note that if $0<\alpha<1$, $n$ turns out to be 1.
\begin{equation}
^{C}D_t^{\alpha}f(t)=\int_0^t \frac{f^{(n)}(\tau)}{(t-\tau)^{\alpha+1-n}}d\tau
\label{caputo_def}
\end{equation}
with $n$ an integer such that $ n = \ceil{\alpha}$. Note that if $0<\alpha<1$, $n = 1$.

The simplest dynamical process described in terms of fractional operators consists in an initial value problem for a linear fractional rate equation, which for a given variable $y(t)$ is given by
\begin{equation}
^{C}D_t^{\alpha} y(t)=-\lambda y(t), \quad y(0)=y_0.
\end{equation}
The solution of this equation can not be expressed in a closed-form, being rather an infinite series. It was first obtained by the mathematician Mittag-Leffler, and is formally written as
\begin{equation}
y(t)=y_0\, E_{\alpha}(-\lambda t^{\alpha}),
\end{equation}
where $E_{\alpha}(t)$ is the function defined as
\begin{equation}
E_{\alpha}(-\lambda\, t^{\alpha})=\sum_{k=0}^{\infty} \frac{(-\lambda\,t^{\alpha})^k}{\Gamma(k\alpha+1)},
\end{equation}
and carries his name. Here $\Gamma(x)$ denotes the Euclidean Gamma function. Note that when $\alpha=1$, it reduces to the exponential function. Essentially, the Mittag-Leffler function generalizes the exponential function, and has been considered for many as the {\it Queen function} of the fractional calculus~\cite{mainardi_why_2020, gorenflo_mittag-leffler_2020, gorenflo_fractional_1997}. For complex parameters $\alpha$ and $\beta$ with $\mathbb{R}\{\alpha\} > 0$, the Mittag-Leffler function (ML function) can be further generalized, and was defined as
\begin{equation}
\label{eq:ml_def}
    E_{\alpha,\beta}(z) = \sum_{k = 0}^{\infty}{\frac{z^k}{\Gamma(k\alpha + \beta)}}, z \in \mathbb{C}.
\end{equation}
Here $E_{\alpha, \beta}(z)$ is an entire function of order $\rho = 1 / \mathbb{R}\{\alpha\}$ and type 1.  The exponential function can be recovered by setting $\alpha = \beta = 1$ since $\Gamma(k~+~1)~=~k!$ for $k \in \mathbb{N}$. When $\beta = 1$, the notation of the ML function can be simplified to $E_\alpha(z) = E_{\alpha, 1}(z)$.

In this paper we consider the $n$-dimensional case of a system described by a set time-fractional linear differential equations, whose solution is given by~(\ref{eq:n-dim-sol}). We therefore need to evaluate the ML function over a potentially very large system's matrix.

A naive approach to evaluate the ML function comes directly from its definition in (\ref{eq:ml_def}). However, this is ill-advised for most applications since the convergence of the series is very slow if either the modulus $\abs{z}$ is large or the value of $\alpha$ is small. Moreover, the terms of the series may grow very large before decreasing, which can cause overflows and catastrophic numerical cancellations when using standard double-precision arithmetic (IEEE 754). For this reason, the scalar ML function is usually evaluated through alternative methods~\cite{hilfer_computation_2006, garrappa_numerical_2015, seybold_numerical_2009, gorenflo_computation_2002}, such as the Taylor series, inversion of the Laplace transformation, integral representation and other techniques.

To evaluate the ML matrix function, Garrappa~\cite{garrappa_computing_2018} recently proposed a Schur-Parlett algorithm~\cite{davies_schur-parlett_2003}, which in practice requires computing its derivatives. This method, however, is only suited for small matrices since the Schur-Parlett algorithm scales with $O(n^3)$, but it can go up to $O(n^4)$ depending on the distribution of the eigenvalues. Moreover, if the eigenvalues are highly clustered, the algorithm may require high-order derivatives which are more expensive to compute, while also being less accurate. There is also a Krylov-based method for computing the action of a Mittag-Leffler function over a vector~\cite{moret_convergence_2011}, however, the code is not publicly available.

As an alternative to deterministic methods, Monte Carlo algorithms for approximating matrix functions have already been proposed in the past~\cite{forsythe_matrix_1950, dimov_monte_2008, dimov_parallel_2001, ji_convergence_2013, benzi_analysis_2017}, primarily for solving linear systems.  The main ideia of these algorithms consists in generating random paths corresponding to the realizations of a discrete Markov chain. Those random paths evolves through the different indices of the matrix $\mat{A}$, which is used as the generator. Essentially, the method corresponds to a Monte Carlo sampling of the Neumann series~\cite{higham_functions_2008} of the matrix. The convergence of this method was rigorously established in~\cite{benzi_analysis_2017, ji_convergence_2013, dimov_new_2015}. It is broadly accepted that these stochastic methods offer interesting features from a computational point of view, such as being easily parallelizable, fault-tolerant, and more suited to heterogeneous architectures (which are extremely important attributes in view of the current high performance computers). However, the truth is that they also exhibit some significant weaknesses, namely a very slow convergence rate to the solution. This can cause the underlying algorithms to be highly demanding computationally, especially when high accuracy solutions are required.

The extension of the Monte Carlo method for other matrix functions, such as the matrix exponential, was only accomplished recently in~\cite{acebron_monte_2019}. The method consists in generating continuous-time random walks over the matrix $\mat{A}$, whose holding time follows an exponential distribution. Later,~\cite{acebron_highly_2020} applied a multilevel technique to improve the performance of this method.

For the specific case of the one-dimensional fractional diffusion equation,~\cite{nichols_subdiffusive_2018} introduced a Monte Carlo method for simulating discrete time random walks which are used for obtaining numerical solutions of the fractional equation, but not for computing the ML matrix function.

% In this paper, we propose an efficient, highly-scalable algorithm for the computation of the ML function that is able to handle large-scale problems.
