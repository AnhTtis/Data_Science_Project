\section{Hardware-based detection Assessment}
\label{sec:hw-based_performance_efficiency}

The following sections analyze the performance and efficiency of the state-of-the-art in \gls{HMD} and explore \gls{ml} techniques to enhance detector performance.

%Sub-section \ref{subsec:evaluation_metrics} reviewed the metrics used for evaluating the performance and efficiency of malware detectors. Sub-section \ref{subsec:hardware-based_detection_framework} discussed its implementation in the light of a generic framework. With these basements, this sub-section presents some considerations about the performance and efficiency of the hardware-based detection approach.

\subsection{Performance}
\label{subsubsec:performance}

Tables \ref{tab:performance_analysis} and \ref{tab:main_studies} provide a comprehensive overview of the literature contributions in the field, aiming to facilitate fair comparisons by presenting the best-case results in Table \ref{tab:performance_analysis}. Metrics were directly sourced from the paper's text whenever feasible, with manual extraction from reported \gls{ROC} curves employed only when necessary. The "Classification" column denotes the classification algorithm associated with the best result, with the Weka implementation serving as a reference. Conversely, Table \ref{tab:main_studies} outlines, for each contribution in Table \ref{tab:performance_analysis}, the range of considered scenarios in terms of malware, classifiers, and system characteristics. The values in Table \ref{tab:performance_analysis} underscore the efficacy of \gls{HMD} in supporting malware detection and highlight the overall high quality of the findings.
 
\begin{table*}
\begin{threeparttable}[b]
\caption{Summary of best-case performance from main studies in the hardware-based malware detection approach. \# \glspl{HPC} column refers to the number of hardware events the classifiers consider. Classification algorithm labels are based on Weka implementations used in the referenced studies. Evaluation metrics as defined in Section \ref{subsec:evaluation_metrics}: A is Accuracy, P is Precision, S is Specificity, and F1 is the F1-Score.}
\label{tab:performance_analysis}
\centering
%\resizebox{\textwidth}{!}{%
\begin{tabularx}{\linewidth}
{
>{\hsize=.1\hsize\linewidth=\hsize}c
>{\hsize=.1\hsize\linewidth=\hsize}c
>{\hsize=.4\hsize\linewidth=\hsize}X
>{\hsize=.1\hsize\linewidth=\hsize}c
>{\hsize=.2\hsize\linewidth=\hsize}X
>{\hsize=.1\hsize\linewidth=\hsize}X
>{\hsize=.1\hsize\linewidth=\hsize}X
>{\hsize=.1\hsize\linewidth=\hsize}c
>{\hsize=.1\hsize\linewidth=\hsize}c
>{\hsize=.1\hsize\linewidth=\hsize}c
>{\hsize=.1\hsize\linewidth=\hsize}c
>{\hsize=.1\hsize\linewidth=\hsize}c
>{\hsize=.1\hsize\linewidth=\hsize}c
}
%{ccXcXXXcccccc}
\toprule\
\textbf{Year} & \textbf{Ref.} & \textbf{Target} & \textbf{\# \glspl{HPC}} & \textbf{Classification} & \textbf{Learning} & \textbf{Latency} & \multicolumn{6}{c}{Evaluation Metrics} \\
 &  &  &  &  &  &  & \textbf{A} & \textbf{P} & \textbf{\gls{TPR}} & \textbf{S} & \textbf{F1} & \textbf{AUC}\\
\midrule\\
\rowcolor{shadecolor}
2013 & \cite{Demme_2013} & Android malware & 6 & Decision Tree & Offline
& NA & - & - & - & - & - & 0.83\\
\rowcolor{shadecolor}
& & Linux rootkits & 4 & KNN & Offline
& NA & - & - & 0.70\tnote{1} & - & - & -\\
\midrule
2014 & \cite{Tang_2014} & Internet Explorer exploitation & 4 & \gls{SVM} & Offline
& NA & - & - & - & - & - & 1.00\\
& & Adobe PDF Reader exploitation & 4 & \gls{SVM} & Offline
& NA & - & - & - & - & - & 1.00 \\
\midrule
\rowcolor{shadecolor}
2015 & \cite{Khasawneh_2015} & Ransomware & 5 & Logistic regression & Offline
& NA & - & - & - & - & - & 0.94 \\
\rowcolor{shadecolor}
& & Ransomware & 5  & Logistic regression (with Specialization) & Offline 
& NA & 0.87 & - & 0.81 & 0.96 & - & -\\
\midrule
2015 & \cite{Ozsoy_2015} & Viruses, worms, trojan horses, spyware, adware, and botnets & 5 & \gls{ann} & Offline
& NA & - & - & 1.00\tnote{1} & - & - & -\\
\midrule
\rowcolor{shadecolor}
2017 & \cite{Patel_2017} & Malware from various categories, sourced from VirusTotal~\cite{VirusTotal} dataset & 4 & BayesNet & Offline
& 0.624ms (SW) / 140ns (HW) & 0.85 & - & - & - & - & -\\
\midrule
2017 & \cite{Singh_2017} & Rootkits & 16 & \gls{SVM} &
& NA & 1.00 & 1.00 & 1.00 & - & 1.00 & - \\
\rowcolor{shadecolor}
\midrule
2018 & \cite{Sayadi_2018} & Malware from various categories, sourced from VirusTotal~\cite{VirusTotal} dataset & 4 & J48 (with Ensemble Learning) & Offline
& NA & 0.83 & - & - & - & - & 0.94 \\
\midrule
2018 & \cite{Zhou_2018} & Malware from various categories, sourced from VirusTotal~\cite{VirusTotal} dataset & 6 & Random Forest & Offline
& NA & - & 0.86 & 0.83 & - & 0.85 & 0.92\\
\midrule
\rowcolor{shadecolor}
2019 & \cite{Das_2019} & Malware from various categories, sourced from VX Heaven~\cite{Qiao:2016aa} dataset & 5 & J48 & Offline
& NA & - & 0.82 & 0.82 & - & 0.82 & 0.93 \\
\midrule
2019 & \cite{Sayadi_2019} & Backdoor & 4 & OneR & Offline
& NA & - & - & - & - & 0.94 & -\\
&  & Rookit & 4 & \gls{MLP} & Offline
& NA & - & - & - & - & 0.94 & -\\
&  & Virus & 4 & J48	 and ensemble learning (AdaBoostM1) & Offline
& NA & - & - & - & - & 0.96 & - \\
&  & Trojan & 4 & \gls{MLP} & Offline
& NA & - & - & - & - & 0.99 & -\\
\midrule
\rowcolor{shadecolor}
2021 & \cite{Gao_2021} & Trojan & 4 & JRIP  & Offline
& 20ns & - & 0.93 & - & - & - & -\\
\midrule
2021 & \cite{Sayadi_2021} & Stealthy rootkits & 4 & \gls{dnn} & Offline
& NA & 0.93 & 0.95 & 0.90 & - & 0.93 & 0.98 \\
\midrule
\rowcolor{shadecolor}
2022 & \cite{Torres_Liu_2022} & Data-only exploits~\cite{Hu_2015} & 50\tnote{2} & Two Classes-\gls{SVM} & Offline 
& NA & 0.99 & - & - & - & - & -\\
\rowcolor{shadecolor}
& & Data-only exploits~\cite{Hu_2015} & 6 & LZ78 & Offline
& NA & 0.84 & - & - & - & - & - \\
\midrule
2022 & \cite{Konstantinou:2022aa} & Stealthy attack on power grid & 6 & \gls{SVM} & Offline 
& 120s & 0.94 & - & - & - & - & - \\
\bottomrule\\
\end{tabularx}
\begin{tablenotes} [flushleft]
\item[1] Values extracted from ROC curves considering a false positive rate of 10\%.
\item[2] 50 is the whole set of features. This is why the authors also investigated a reduced set (in the following line).
\end{tablenotes}
%}
\end{threeparttable}
\end{table*}

Among all contributions reported in Table~\ref{tab:performance_analysis}, authors in \cite{Konstantinou:2022aa} showcase the effectiveness of \gls{HMD} on real scenarios: DARPA \gls{RADICS}, Intel \gls{TDT}, and Microsoft Defender. This is a tangible exploitation of \gls{HMD} into actual products. Still, using a single type of classifier (i.e., \gls{SVM}) leaves room for research and improvements. 

\begin{table*}
\begin{threeparttable}[b]
\caption{Reference studies including details on the full list of targets and classifications approaches tested and details on the reference systems.}
\label{tab:main_studies}
\centering
%\resizebox{\textwidth}{!}{%
\begin{tabularx}{\linewidth}{ccXXXX}
\toprule\
\textbf{Year} & \textbf{Ref.} & \textbf{Targets} & \textbf{Classification} & \textbf{Devices} & \textbf{OS}\\
\midrule\\
\rowcolor{shadecolor}
2013 & \cite{Demme_2013} & Android malware, Linux rootkits & Decision trees, \gls{ann}, \gls{KNN}, Random Forest & Arm Cortex-A9 OMAP4460, Intel Xeon X5550 & Android 4.1.1-1 (kernel 3.2), Linux kernel 2.6.32\\
\midrule
2014 & \cite{Tang_2014} & Exploitations on Internet Explorer and Adobe PDF Reader  & \gls{SVM} & Intel IvyBridge Core i7 & Windows XP\\
\midrule
\rowcolor{shadecolor}
2015 & \cite{Khasawneh_2015} & Ransomware, password stealers, trojan horses, backdoor, worms & Logistic regression	(w/o specialization) & Not specified & Windows 7\\
\midrule
2015 & \cite{Ozsoy_2015} & Viruses, worms, trojan horses, spyware, adware, and botnets & \gls{ann} &
Not specified, Altera EP4CE115 & Windows 7\\
\midrule
\rowcolor{shadecolor}
2017 & \cite{Patel_2017} & Malware from various categories, sourced from VirusTotal~\cite{VirusTotal} dataset & Logistic, SimpleLogistic, BayesNet, NaiveBayes, J48, PART, JRIP, OneR, MultiLayerPerceptron, SMO, SGD & Intel Haswell Core i5-4590, Xilinx Virtex 7 & Ubuntu 14.04 (kernel 4.4)\\
\midrule
2017 & \cite{Singh_2017} & Rootkits & \gls{SVM}, Decision tree, OC-SVM, Naive Bayes & Intel IvyBridge and Broadwell & Windows 7\\
\midrule
\rowcolor{shadecolor}
2018 & \cite{Sayadi_2018} & Malware from various categories, sourced from VirusTotal~\cite{VirusTotal} dataset & BayesNet, J48, REPTree, JRIP, OneR, MultiLayerPerceptron, SMO, SGD (w/o ensemble learning based on AdaBoostM1, Bagging) & Intel Xeon X5550, Xilinx Virtex 7 & Ubuntu 14.04 (kernel 4.4)\\
\midrule
2018 & \cite{Zhou_2018} & Malware from various categories, sourced from VirusTotal~\cite{VirusTotal} dataset & Decision trees, Naive Bayes, ANN, KNN, Random Forest (w/o ensemble learning AdaBoost) & AMD Bulldozer & Windows 7\\
\midrule
\rowcolor{shadecolor}
2019 & \cite{Das_2019} & Malware from various categories, sourced from VX Heaven~\cite{Qiao:2016aa} dataset & J48, IBk, SMO & Intel Sandy Bridge, Haswell, and Skylake & Ubuntu 16.04\\
\midrule
2019 & \cite{Sayadi_2019} & Backdoor, rootkits, viruses, trojan horses & J48, JRIP, OneR, \gls{MLP} (w/o AdaBoostM1) & Intel Xeon X5550, Xilinx Virtex 7 & Ubuntu 14.04 (kernel 4.4)\\
\midrule
\rowcolor{shadecolor}
2021 & \cite{Gao_2021} & Worms, rootkits, viruses, trojan horses & REPTree, JRIP, OneR, \gls{MLP}, SGD & Intel Xeon X5550, Xilinx Virtex 7 & Ubuntu 14.04 (kernel 4.4)\\
\midrule
2021 & \cite{Sayadi_2021} & Stealthy backdoor, rootkits, and trojan horses & \gls{dnn} &Intel Xeon X5550 &
Ubuntu 14.04 (kernel 4.4)\\
\midrule
\rowcolor{shadecolor}
2022 & \cite{Torres_Liu_2022} & Data-only exploitation & TC-SVM, OC-SVM, LZ78 & Intel Nehalem Core i7-920 & Ubuntu 16.04 (kernel 4.13)\\
\midrule
2022 & \cite{Konstantinou:2022aa} & Stealthy attack on power grid &\gls{SVM} & OpenPLC controller with Raspberry PI & 8-bus power grid in a PowerWorld simulator \\
\bottomrule\\
\end{tabularx}
%}
\end{threeparttable}
\end{table*}

As most of the current works on \gls{HMD} rely on \gls{ml} classifiers, the analysis conducted by Patel et al. \cite{Patel_2017}, summarized in Table \ref{tab:classifiers_analysis}, is particularly interesting. The authors thoroughly analyze eleven \gls{ml} classification algorithms (based on Weka\cite{Frank:2005aa} implementations). The goal was to understand the trade-offs between the design parameters offered by the algorithms. The chosen metric to evaluate performance was accuracy. The dataset used for training and testing the algorithms was extracted using the PERF tool in intervals of 10 ms executed in an Intel Haswell Core i5-4590 processor running Ubuntu 14.04 with Linux kernel 4.4. The baseline of benign application comprises the Mibench benchmark suite \cite{Guthaus_2001}, Linux system programs, browsers, text editors, and word processors. The malware came from the VirusTotal dataset. Since the \glspl{HPC} available in an Intel architecture are considerable, the accuracy of \gls{ml} algorithms covers different numbers (i.e., 32, 8, 4, 2, and 1) of hardware events. Table \ref{tab:classifiers_analysis} reports the accuracy for 4 hardware events, a reasonable quantity for concurrent monitoring in most modern processors, even in embedded scenarios~\cite{Dutto:2021aa}. JRIP (rule-based) presented the top accuracy, followed by four classifiers with the same top-two accuracy: J48 (decision-tree), OneR and PART (rule-based), and SGD. In this case, most classifiers have accuracy above 80\%. Another interesting observation is that reducing the hardware events below four significantly impacts the performance of most classifiers.

Similar findings are reported in Torres and Liu \cite{Torres_Liu_2022}. While the authors concentrated on a particular malware subclass (data-only exploits from \cite{Hu_2015}), they implemented two different experiments on different classifiers, distinguishing between using the complete set of 50 features or a smaller set of 6 features. The findings report a very high accuracy on the complete set of features (as seen on the first of the two rows dedicated to the paper in Table \ref{tab:performance_analysis}) and a degradation when only a subset is used. 


\begin{table}
\begin{threeparttable}[b]
%\renewcommand*{\arraystretch}{1}
%\scriptsize
\caption{Performance and efficiency of classifiers based on Weka implementations. Extracted from \cite{Patel_2017}.}
\label{tab:classifiers_analysis}
\centering
\begin{tabularx}{\linewidth}{
>{\hsize=.5\hsize\linewidth=\hsize}l
>{\hsize=.1\hsize\linewidth=\hsize}c
>{\hsize=.1\hsize\linewidth=\hsize}c
>{\hsize=.1\hsize\linewidth=\hsize}c
>{\hsize=.1\hsize\linewidth=\hsize}c
>{\hsize=.1\hsize\linewidth=\hsize}c
}
\toprule\
\multirow{3}{*}{Classifier} & \multirow{3}{*}{Accuracy} & SW  & \multicolumn{3}{c}{HW}                \\ %\cline{3-6} 
                            &                           & Latency  & Latency  & Power  & Area\tnote{1} \\ %\cline{3-6} 
                            &                           &    (ms)          &    (ns)                & (W) &       \\
 \midrule\\
\rowcolor{shadecolor}
BayesNet & 81.13 & 0.624 & 140 & 0.44 & 6794 \\
\midrule
J48 & 82.07 & 0.663 & 60 & 0.44 & 1801 \\
\midrule
\rowcolor{shadecolor}
JRIP & 83.96 & 0.653 & 90 & 0.44 & 1504 \\
\midrule
Logistic & 79.24 & 0.844 & 340 & 0.63 & 13041 \\
\midrule
\rowcolor{shadecolor}
\gls{MLP} & 81.13 & 0.870 & 40 & 1.03 & 36252 \\
\midrule
NaiveBayes & 78.30 & 0.802 & 10 & 1.34 & 58177 \\
\midrule
\rowcolor{shadecolor}
OneR & 82.07 & 0.653 & 220 & 0.32 & 1258 \\
\midrule
PART & 82.07 & 0.642 & 680 & 0.44 & 2131 \\
\midrule
\rowcolor{shadecolor}
SGD & 82.07 & 0.652 & 340 & 0.44 & 2556 \\
\midrule
SimpleLogistic & 79.24 & 0.648 & 3020 & 0.45 & 4721 \\
\midrule
\rowcolor{shadecolor}
SMO & 73.58 & 0.652 & 2330 & 0.44 & 2556 \\
\bottomrule\\
\end{tabularx}
\begin{tablenotes} [flushleft]
\item[1] The area is a function of total lookup tables, flip-flops, and DSP blocks.
\end{tablenotes}
\end{threeparttable}
\end{table}

\subsection{Efficiency}
\label{subsec:efficiency}

Alongside the detection quality, the \gls{HMD} aims to reduce the detectors cost in terms of resources. As the data required for the classification come from the hardware layer of the system stack, most studies evaluate FPGA-based implementations of \gls{ml} classifiers, providing measures for the power consumption and the area as the goal is to understand the trade-offs between the design parameters offered by the algorithms. When the classifier is software-based, the evaluation usually includes the latency, avoiding further monitoring of other resources. Unfortunately, as seen in Table~\ref{tab:performance_analysis}, not all works report the latency of the detection or, more in general, the costs of it. Generally, whenever the detection is performed at the software level, the latency is less than 1 ms. At the same time, more optimized hardware implementations can scale down to tens or hundreds of ns.

As reported in the previous section, the work from Patel et al. \cite{Patel_2017} covered a thorough analysis and, for this reason, is undoubtedly an excellent candidate to show the efficiency of the methodology. For hardware implementation, authors used the Xilinx Virtex 7 FPGA, implemented Weka models in C code, and used the Xilinx \gls{HLS} compiler to generate the final bitstream. The latency was evaluated both in software and hardware implementations. Authors implemented the classification algorithms in software at the \gls{OS} kernel level, which includes the time to read the \gls{HPC} and execute the classifiers. Eventually, the Intel Turbo Boost technology was disabled, as it might introduce errors in the time measurement, and the CPU governor was operating at a constant frequency of 800 MHz. The IP cores with the algorithms were synthesized in Vivado to estimate the power consumption, considering a 100 MHz clock. Power estimation contains both static power and dynamic power consumption of digital logic. %Their results are shown in the fifth column in Table \ref{tab:classifiers_analysis}.

Values in Table \ref{tab:classifiers_analysis} show the considerable difference between the latencies in software and hardware implementations. Software implementations have latencies almost in the order of milliseconds (ranging from 0.624ms to 0.870ms, best and worst cases). In contrast, hardware implementations are in the order of nanoseconds (ranging, in this case, from 10ns to 3020ns). The authors underlined that these slow profiles displayed by classifiers in the kernel space are three orders bigger than several malware executions (ranging in microseconds). Other findings related to latency are crucial to highlight. In software implementations, the latency for reading the \gls{HPC} is negligible when monitoring a single core but may increase significantly when monitoring multiple cores.
Moreover, the more \glspl{HPC} to read, the longer it takes. Concerning the classification algorithms, BayesNet (Bayesian network), PART (rule-based), and SimpleLogistic (logistic regression) showed the lowest latency values when implemented in software. Conversely, none of these three are on the list of the top three low latencies in hardware. NaiveBayes (Bayesian network), \gls{MLP} (\gls{ann}), and J48 (decision tree) are the three best hardware implementations. This paradox demonstrates the uncorrelation between the algorithms' latencies when comparing implementations at the kernel space and hardware.

%Table \ref{tab:classifiers_analysis} reports all latencies, showing how BayesNet (Bayesian network), PART (rule-based), and SimpleLogistic (logistic regression) has the lowest latency values when implemented in software. Some findings are crucial to highlight. At the kernel space, the latency for reading the \gls{HPC} is negligible when monitoring a single core but may increase significantly when monitoring multiple cores.  Moreover, while the more \glspl{HPC} to read, the longer it takes, authors underlined that the displayed latencies (milliseconds) were three orders of magnitude bigger than several malware (ranging in microseconds). The same table reports the findings of the hardware implementation. Latency is stated in several cycles, and each cycle is ten ns. NaiveBayes (Bayesian network), \gls{MLP} (\gls{ann}), and J48 (decision tree) are the three best implementations concerning the latency. Interestingly, none of the top three hardware latencies is in the list of a maximum of three software latencies.

%OneR (rule-based) presented the lowest power consumption, followed by J48 (decision tree), and JRIP and PART (rule-based) had the second lowest consumption. Eventually, the hardware area was evaluated, including the total number of lookup tables, flip-flops, and DSP blocks. According to Table \ref{tab:classifiers_analysis}, OneR, JRIP, and J48 algorithms present the top three lower hardware costs. %They made a comparison considering accuracy, latency, and power consumption. They calculated the Power Delay Product (PDP) using the latency and power consumption to account for power and latency together. PDP was later compared with accuracy in a functional analysis to find algorithms more suitable for battery-powered embedded devices. ML classifiers with lower PDP and higher accuracy are preferred. Classifiers OneR, JRIP, PART, and J48 perform better than the other classifiers. OneR, JRIP, and PART are rule-based classifiers and generate rules for the features that involve comparisons rather than computation. Therefore, they can run faster. This is also the case for the tree-based classifier, J48. Classifiers such as BayesNet and Logistic regression involve computation like probabilities and sigmoid functions, resulting in higher execution latency.
%Eventually, the hardware area was evaluated, including the total number of lookup tables, flip-flops, and DSP blocks. According to Table \ref{tab:classifiers_analysis}, OneR, JRIP, and J48 algorithms present the top three lower hardware costs. %They compared accuracy and area through the accuracy ratio over the area. Again, rule-based and tree-based classifiers performed significantly better in this analysis than highly accurate but complex Bayesnet, MultiLayerPerceptron, and logistic classifiers.

%The following sub-section (\ref{subsec:machine_learning_techniques}) continues the discussion of efficiency in the hardware-based detection approach but focuses on the efficiency overhead imposed by special machine-learning techniques to improve the performance.

\subsection{Machine learning techniques considerations}
\label{subsec:machine_learning_techniques}

Recent studies have explored various \gls{ml} methods to enhance the performance of \gls{HMD} detection approaches, especially in the last five years. These techniques aim to overcome the challenge of limited application characterization due to the concurrent capacity of \glspl{PMU} to monitor hardware events. While these methods show performance improvements, they often introduce increased complexity in classifiers, resulting in reduced efficiency, i.e., higher power consumption and increased area requirements. This section discusses ensemble learning, specialization, adaptive detection, and time series \gls{ml} techniques in \gls{HMD}.

In ensemble learning, multiple \gls{ml} algorithms are trained separately to create a classifier, combining their results to improve decision accuracy \cite{Dietterich_2000}. In \gls{HMD}, ensemble classifiers leverage the characteristics of individual algorithms to detect various types of malware while minimizing hardware events for runtime detection \cite{Khasawneh_2015, Sayadi_2018, Sayadi_2019}. However, the performance gains come with increased complexity and efficiency overhead \cite{Sayadi_2018, Gao_2021}.

Sayadi et al. \cite{Sayadi_2018} assessed the efficiency impact of ensemble learning in a malware detector on Xilinx Virtex 7 FPGA. Significant latency increases were observed when comparing a general classifier with 8 \glspl{HPC} to a Boosted classifier~\cite{Freund_Schapire_1997} with 4 \glspl{HPC}. When Boosted, the general \gls{MLP} algorithm passed from a latency of 3020ns to a latency of 5910ns. OneR increased from 10ns to 700ns, and J48  increased from 90ns to 670ns. In terms of hardware cost, the largest area increases were observed in OneR (from 2.1\% to 5.1\%), JRIP (from 2.5\% to 5.3\%), and BayesNet (from 11.5\% to 13.6\%). Conversely, J48, REPTree, and \gls{MLP} showed smaller area increases. The findings highlight substantial overhead in both latency and hardware costs. 

Another interesting \gls{ml} technique is the specialization. Instead of training a single multi-class classifier able to recognize several malware categories, different classifiers are trained, each specialized in detecting a specific malware. Authors in \cite{Khasawneh_2015} discuss and explore specialized detectors in \gls{HMD}. They used a logistic regression-based classifier for each malware class. As a result, the proposed detectors reduced the false positive rate by more than half compared to a single detector while increasing the detection rate. The authors proposed a two-level detector in the same paper, mixing a first level based on the hardware detection approach and a second level based on the software detection approach. The hardware detector was based on specialized ensemble techniques. The latency of this scheme was compared with malware detection purely based on software methods. As a result, they reported average latency reduced to 1/6.6 when the fraction of malware is low and latency reduced to 1/3.1 when 20\% of the programs are malware.

In 2019, Sayadi et al. \cite{Sayadi_2019} introduced a specialized two-stage malware detector, leveraging ensemble learning techniques, significantly improving accuracy. The first stage classifies applications into benign or malware categories (virus, rootkit, backdoor, and trojan horse). The second stage deploys an \gls{ml} classification algorithm that works best for each category of malware. Their 2021 work \cite{Sayadi_2021} continued using specialization for an accurate and run-time stealthy malware detector. They also evaluated the efficiency overhead of their specialized and ensemble learning malware detector, implemented on Xilinx Virtex 7 FPGA. A comparison of a general classifier with 4 \glspl{HPC} to a Boosted classifier with 4 \glspl{HPC} revealed notable latency increases for \gls{MLP} (from 1.020 to 5.910 ms), OneR (from 10 to 700 ns), J48 (from 30 to 670 ns), and JRIP (from 20 to 560 ns). \gls{MLP} (from 43.2\% to 61.7\%), JRIP (from 0.26\% to 5.3\%), OneR (from 0.49\% to 5.1\%) and J48 (from 0.93\% to 4.3\%) exhibited considerable increases regarding hardware cost. The findings emphasize substantial latency and hardware cost overhead.

Adaptive detection was proposed by Gao et al. \cite{Gao_2021} to optimize the performance versus cost. It targets higher or similar performance as ensemble learning, with a reduced cost. The technique leverages the concept that the \gls{ml} algorithm employed in the detector strongly correlates both the nature of the scrutinized malware and the overall performance metric. Adaptive detection involves a dynamic framework that assesses all underlying \gls{ml} algorithms in real time, opting for the optimal classifier to identify malicious patterns effectively. The implementation encompasses two primary online stages: (i) algorithm selection and (ii) malware detection. Consequently, only the most efficient ML-based detector is employed to differentiate malware from the benign class, eliminating the need to acquire results from individual base detectors and enhancing overall efficiency.

In the adaptive detector proposed by Gao et al. \cite{Gao_2021}, the algorithm selection step is done by a lightweight tree-based decision-making algorithm that accurately selects the most efficient model for inference. As a result, the scheme showed up to a 94\% detection rate while improving the cost-efficiency by more than 5X compared to existing ensemble-based malware detection methods.

%Time series are every day and everywhere. Both human activities and nature produce time series, like weather readings, ﬁnancial recordings, physiological signals, and industrial observations. Earning representations and classifying time series are still attracting much attention. A time series classification problem does not treat each time point as a separate feature, nor ignore information contained in the time order of the data. In other words, the prediction would change if the feature order were scrambled in time series classification. There are several approaches for building time series classifiers: distance-based, shapelet-based, ensembled-based, dictionary-based, interval-based, and deep-learning-based. Particular focus has been given to deep neural networks, especially Fully Convolutional Neural Network (FCN) and Long Short-Term Memory (LSTM) \cite{Wang_2017,Karim_2018}. Alternative approaches have also been proposed, looking for less costly solutions. Sch\"{a}fer \cite{Schafer_2016}, and Li and Lin \cite{Li_Lin_2017} proposed a series of scalable time series classification approaches that are significantly faster than FCN and LSTM.

Eventually, time series classification is fundamental to understanding the key concept behind hardware-based malware detection. The intuition driving this technique stems from the program's phase behavior, transforming malware detection into a time series classification problem. In addressing this challenge, Sayadi et al., as outlined in \cite{Sayadi_2020} and \cite{Sayadi_2021}, introduced a time series \gls{ml} technique designed to identify stealthy malware in real time. In scenarios where attackers embed malicious files within benign programs on target hosts, executing both applications as a single thread, traditional signature-based antivirus tools falter. Embedded malware remains elusive even when the exact malware signature is in the detector database. The authors proposed a classifier based on a \glspl{FCNN} and exclusively utilized branch instructions as a low-level feature in their solution. The results demonstrated the efficacy of their technique, achieving a remarkable average detection performance of 94\% with only one \gls{HPC} feature, surpassing state-of-the-art detection methods. This enhanced performance, however, comes at a higher computational cost associated with employing a deep-learning-based solution.

While not explicitly implementing a time series technique, also \cite{Konstantinou:2022aa} reports similar results on the Intel \gls{TDT} use case. Although no specific numbers are provided, the paper compares the \gls{FFT} counting traces of the branch instructions and branch misprediction events for the WannaCry ransomware, underlining the significant difference with or without the ransomware. 

%Table \ref{tab:main_studies} summarizes the primary studies in the hardware-based malware detection approach. The columns "Devices" and "OS" (Operational Systems) refer to processors, FPGAs, and software in which the hardware-based detection framework was performed. The selection of studies was based on the following criteria: papers with the most citations, cited by these, and most recent publications.



%\begin{table*}
%\renewcommand*{\arraystretch}{1}
%\scriptsize
%\caption{Summary of main studies in the hardware-based malware detection approach. Compilation elaborated by the author.}
%\label{tab:main_studies}
%\centering
%\begin{tabularx}{\linewidth}{
%>{\hsize=.1\hsize\linewidth=\hsize}l
%>{\hsize=.1\hsize\linewidth=\hsize}l
%>{\hsize=.2\hsize\linewidth=\hsize}X
%>{\hsize=.4\hsize\linewidth=\hsize}X
%>{\hsize=.1\hsize\linewidth=\hsize}X
%>{\hsize=.1\hsize\linewidth=\hsize}X}
%\toprule
%\textbf{Year} & \textbf{Work} & \textbf{Topic} & \textbf{Focus} & \textbf{Devices} & \textbf{OS}\\
%\midrule
%\rowcolor{shadecolor}
%2002 &
%Sprunt \cite{Sprunt_2002} &
%Hardware monitoring units &
%The work is from the beginning of the hardware monitoring units. It discussed their basics and how they can be used for performance improvements &
%- &
%-\\
%
%2011 &
%Malone et al. \cite{Malone_2011} &
%Feasibility of hardware malware detection approach &
%As the first seminal work in the field, they introduced the approach and performed experiments to demonstrate the feasibility of detecting code modifications in programs based on the deviation of hardware events &
%Intel Yorkfield Q9400 &
%Ubuntu (kernel 2.6.38)\\
%
%\rowcolor{shadecolor}
%2013 &
%Demme et al. \cite{Demme_2013} &
%Feasibility of hardware malware detection approach &
%As the second seminal work in the field, they introduced the approach and performed two experiments to prove the feasibility, one detecting Android malware in ARM processors and the other detecting kernel rootkits in Intel processors &
%Arm Cortex-A9 OMAP4460, Intel Xeon X5550 &
%Android 4.1.1-1 (kernel 3.2), Linux kernel 2.6.32\\
%
%2013 &
%Wang and Karri \cite{Wang_Karri_2013} &
%Detection of kernel rootkits &
%The work proposed and tested an HPCs-based framework to run-time detection of kernel rootkits in a virtual machine &
%AMD K10 1356 &
%Redhat 7.3 (kernel 2.4.18), Fedora (kernel 2.6.11)\\
%
%\rowcolor{shadecolor}
%2014 &
%Tang et al. \cite{Tang_2014} &
%Feasibility of hardware malware detection approach and unsupervised learning &
%The work explored the feasibility of a hardware malware detection approach using unsupervised learning; the idea came from the anomaly-based detection approach &
%Intel IvyBridge Core i7 &
%Windows XP\\
%
%2015 &
%Garcia-Serrano \cite{Garcia-serrano_2015} &
%Feasibility of hardware malware detection approach and unsupervised learning &
%Based on Tang et al. \cite{Tang_2014}, the work investigated the use of unsupervised learning (anomaly-based detection approach) without the necessity of any other statistical construction like the rank-preserving power transform &
%- &
%-\\
%
%\rowcolor{shadecolor}
%2015 &
%Khasawneh et al. \cite{Khasawneh_2015} &
%ML classifiers, performance, and run-time detection &
%The work explored specialized detectors and ensemble learning techniques to improve the performance of hardware malware detectors &
%Not specified &
%Windows 7\\
%
%2015 &
%Ozsoy et al. \cite{Ozsoy_2015} &
%Feature selection, ML classifiers, performance, and run-time detection &
%The work was the first to address run-time in the hardware detection approach. They discussed malware detectors based on mixed approaches (hardware and software). They also implemented and tested a hardware-based malware detector in FPGA &
%Not specified, Altera EP4CE115 &
%Windows 7\\
%
%\rowcolor{shadecolor}
%2017 &
%Patel et al. \cite{Patel_2017} &
%ML classifiers, performance, efficiency, and run-time detection &
%The work analyzed various robust ML algorithms to help guide architectural decisions for better performance and efficiency. They analyzed the classifiers implemented in the software OS kernel and FPGA &
%Intel Haswell Core i5-4590, Xilinx Virtex 7 &
%Ubuntu 14.04 (kernel 4.4)\\
%
%2017 &
%Singh et al. \cite{Singh_2017} &
%Detection of kernel rootkits &
%The work discussed the difficulty in detecting rootkits because they execute in the context of other processes that access kernel information. They experiment with the detection of rootkits using the hardware detection approach &
%Intel IvyBridge and Broadwell &
%Windows 7\\
%
%\rowcolor{shadecolor}
%2018 &
%Sayadi et al. \cite{Sayadi_2018} &
%ML classifiers, performance, and run-time detection &
%The work proposed and tested ensemble learning techniques for the classifier to solve the small number of HPCs in run-time detection. They also developed and analyzed an FPGA implementation of ensemble classifiers &
%Intel Xeon X5550, Xilinx Virtex 7 &
%Ubuntu 14.04 (kernel 4.4)\\
%
%2018 &
%Zhou et al. \cite{Zhou_2018} &
%Feasibility of hardware malware detection approach &
%The work argued that detecting malware using HPCs is impossible. They reproduced preview works with good detailing and verified low detection rates, supporting their argument &
%AMD Bulldozer &
%Windows 7\\
%
%\rowcolor{shadecolor}
%2019 &
%Das et al. \cite{Das_2019} &
%Feature extraction and HPCs accuracy &
%The work presented the reasons for the inaccurate measurement of HPC and some suggestions/techniques to improve the process. They performed experiments related to the non-determinism of HPCs and the proposed techniques &
%Intel Sandy Bridge, Haswell, and Skylake &
%Ubuntu 16.04\\
%
%2019 &
%Sayadi et al. \cite{Sayadi_2019} &
%ML classifiers, performance, and run-time detection &
%The work explored specialized detectors and ensemble learning techniques for the problem of few performance counters in run-time. They implemented the proposed detector in FPGA &
%Intel Xeon X5550, Xilinx Virtex 7 &
%Ubuntu 14.04 (kernel 4.4)\\
%
%\rowcolor{shadecolor}
%2020 &
%Sayadi et al. \cite{Sayadi_2020} &
%ML classifiers, run-time detection, and embedded malware &
%The work explored a specialized time series machine learning approach to detect stealthy malware (embedded malware) at run-time accurately &
%Intel Xeon X5550 &
%Ubuntu 14.04 (kernel 4.4)\\
%
%2021 &
%Gao et al. \cite{Gao_2021} &
%ML classifiers, performance, efficiency, and run-time detection &
%The work is argued to be the first that addressed the challenge of performance vs. efficiency. They explored an adaptive detector without using ensemble learning techniques composed of a model selector and a set of ML classifiers. They also implemented and tested the scheme in an FPGA &
%Intel Xeon X5550, Xilinx Virtex 7 &
%Ubuntu 14.04 (kernel 4.4)\\
%
%\rowcolor{shadecolor}
%2021 &
%Sayadi et al. \cite{Sayadi_2021} &
%ML classifiers, run-time detection, and embedded malware &
%The work explored a specialized time series machine learning approach to accurately detect stealthy malware (embedded malware) at run-time. They deeply discussed embedded malware detection and time series classification &
%Intel Xeon X5550 &
%Ubuntu 14.04 (kernel 4.4)\\
%
%2022 &
%Sayadi et al. \cite{Sayadi_2022} &
%Overview of hardware malware detection approach and side-channel attacks &
%The work analyzed recent trends in AI-enabled hardware-based security and their challenges and opportunities, with a special section about side-channel attacks. They performed experiments with machine learning side-channel attacks in an AES-128 implemented in an MCU and an FPGA &
%STMicroelectronics STM32F415, Xilinx XC7A100T &
%-\\
%
%\rowcolor{shadecolor}
%2022 &
%Torres and Liu \cite{Torres_Liu_2022} &
%Detection of data-only attacks &
%The work discussed the problem of data-only attack detection and performed an experiment showing the high detection accuracy using the hardware-based detection approach. It also discusses the impact of feature selection algorithms &
%Intel Nehalem Core i7-920 &
%Ubuntu 16.04 (kernel 4.13)\\
%
%2022 &
%Botacin and Gr{\'e}gio \cite{Botacin_Gregio_2022} &
%Feasibility of hardware malware detection approach &
%The work argued that the hardware detection approach is only effective against some malware categories, specifically with attacks that exploit architectural side-eﬀects. They claim the need for a theory of maliciousness to better state malware threats and evaluate proposed defenses &
%Not specified &
%Not specified\\
%\bottomrule
%\end{tabularx}
%\end{table*}