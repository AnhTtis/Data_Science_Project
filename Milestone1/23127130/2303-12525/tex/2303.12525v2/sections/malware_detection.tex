\section{Overview of Malware detection}
\label{sec:malware_detection}

Malware detection involves determining whether a given program exhibits malicious intent. Figure \ref{fig:overview_malware_detection_approaches} offers an overview of contemporary solutions for malware detection, categorized into two main groups: software-based and hardware-based approaches. This division is rooted in differing observation points within the system stack and different detection methodologies. Recent advancements, as underscored by \cite{Deldar:2023aa} and \cite{He:2021aa}, increasingly rely on \gls{ml} or \gls{ai} techniques to facilitate detection.

%Historically, software-based detection has been the traditional solution. It relies on the analysis of the software and its behavior to determine if there is a malicious code behavior. A common example is the popular antivirus software, which deals against several malware classes of Section \ref{sec:malware_basics}, despite the name. Nevertheless, 

\Figure[htb]()[width=0.98\columnwidth]{Figures/Overview_malware_detection_approaches}
   {Overview of the contemporary solutions for malware detection. Elaborated by the authors based on \cite{Aslan_Samet_2020}.\label{fig:overview_malware_detection_approaches}}

This section presents an overview of software-based and hardware-based malware detection (sub-sections \ref{subsec:software_malware_detection} and \ref{subsec:hardware_malware_detection}), starting by reviewing the metrics used for evaluating the performance and efficiency of the detectors (sub-section \ref{subsec:evaluation_metrics}). %Eventually, Sub-section \ref{subsec:hardware_strengths_weaknesses} discusses the strengths and weaknesses of the hardware-based approach.

\subsection{Evaluation metrics}
\label{subsec:evaluation_metrics}

Before delving into specific malware detection techniques, readers need to consider the evaluation metrics used to assess their effectiveness. These metrics serve as quality indicators, pivotal in determining the adoption of a technique on a commercial scale. Since malware detection is a classification problem, the quality evaluation of the detectors is based on the standard classification metrics. They can be grouped as performance metrics and efficiency metrics. Performance is the degree to which a system or component accomplishes its designated functions within given constraints, i.e., correctly detects the malware. Efficiency is the degree to which a system or component performs its specified functions with minimum consumption of resources \cite{ISO_IEC_IEEE_Vocabulary_2017}. 

The primary evaluation tool for performance is the confusion matrix. This matrix is fundamental in \gls{ml} and classification tasks, summarizing results in a tabular form. It comprises four elements (see Table \ref{tab:confusion_matrix}): \glspl{TP} represent instances where the model correctly predicts malware presence, \glspl{TN} indicate correct predictions of malware absence. In contrast, \glspl{FP} and \glspl{FN} denote incorrect predictions of malware presence or absence, respectively.

\begin{table}[htb]
\centering
\caption{Confusion matrix for malware detection.}
\label{tab:confusion_matrix}
\begin{tabular}{ccc}
\toprule
 & \textbf{Predicted Negative} & \textbf{Predicted Positive} \\
\midrule\\
\rowcolor{shadecolor}
\textbf{Actual Negative} & \glspl{TN} & \glspl{FP} \\
\midrule
\textbf{Actual Positive} & \glspl{FN} & \glspl{TP} \\
\bottomrule
\end{tabular}
\end{table}

Such a matrix allows for the definition of more descriptive metrics, and Table \ref{tab:usefull_matrix} summarizes the most common ones \cite{Ye:2017aa}. The \textit{accuracy} summarizes the overall correctness of the classification model by expressing the number of correct predictions, making it one of the most widely used metrics. In scenarios where it is crucial to avoid incorrect malware predictions, \textit{precision} provides an accurate measure of the \glspl{TP} among all positive predictions. Shifting the evaluation focus to ensure no malware passes unnoticed, the \textit{\gls{TPR}} (also known as \textit{Recall} or \textit{Sensitivity}) weighs \glspl{TP} against all positive samples. It has two counterparts: (i) the \textit{\gls{FPR}}, representing the probability of a \gls{TP} being missed, and (ii) the \textit{specificity}, also known as \textit{\gls{TNR}}, indicating the probability of an actual negative (\gls{TN}) being correctly classified. Balancing Precision and Recall is often essential, and the evaluation can be accomplished using the \textit{F1-score}, which represents their harmonic mean.

Eventually, the \textit{\gls{ROC} curve} offers a visual perspective to performance evaluation. It plots the \gls{TPR} against the \gls{FPR} on a 2D graph, enabling a visual comparison of different models and capturing multiple classification aspects by inspecting the \textit{\gls{AUC}}. In simple terms, the larger the \gls{AUC}, the better the model. \gls{AUC} is closely related to the robustness of the classifier, indicating how effectively the classifier distinguishes between malware and benign applications.

\begin{table}[htb]
\centering
\caption{Most common metrics for performance evaluation of classification.}
\label{tab:usefull_matrix}
\begin{tabular}{cc}
\toprule
 \textbf{Matrix} & \textbf{Expression} \\
\midrule\\
\rowcolor{shadecolor}
\textbf{Accuracy (A)} & $A=\frac{TP+TN}{TP+TN+FP+FN}$ \\
\midrule
\textbf{Precision (P)} & $P=\frac{TP}{TP+FP}$ \\
\midrule
\rowcolor{shadecolor}
\textbf{\gls{TPR}} & $TPR=\frac{TP}{TP+FN}$ \\
\midrule
\textbf{\gls{FPR}} & $FPR=\frac{FN}{FN+TP}$ \\
\midrule
\rowcolor{shadecolor}
\textbf{Specificity (S)} & $S = \frac{TN}{TN+FP}$ \\
\midrule
\textbf{F1-Score (F1)} & $F1 = \frac{2 \times (P \times R)}{(P + R)}$ \\
\midrule
\rowcolor{shadecolor}
\textbf{ROC} & $ROC = 1 - S = \frac{FP}{FP+TN}$\\
\midrule
\textbf{AUC} & $AUC = \int_{0}^{1} R (FPR) \,dFPR$\\
\bottomrule
\end{tabular}
\end{table}

According to \cite{ISO_IEC_IEEE_Vocabulary_2017}, efficiency is related to the resources used for malware detection. Many metrics can be used to evaluate the efficiency \cite{Sze_2020}, but in the malware detection field, latency, power consumption, and hardware cost are the main interest:

    \begin{itemize}
    
    \item \textbf{Latency} is the time between collecting all features analyzed by the malware detector and concluding its detection. A low latency is vital for run-time detection of malware that acts in a short interval of time;
   
    \item \textbf{Power consumption} indicates the energy the detector consumes per unit of time. Two factors primarily impact the power consumption of the detector: the hardware that implements or where the classifier runs and the detection algorithm (those with higher computing processing tend to consume more);

    \item \textbf{Hardware cost} indicates the monetary cost of building the detection system. This is important from both an industry and a research perspective to dictate whether a system is ﬁnancially viable. The main parameter to evaluate the hardware cost is the chip area (usually reported in square millimeters) in conjunction with the process technology (for example, 45 nm). Sometimes, the amount of memory is also used to evaluate the hardware cost. 
    \end{itemize}

%Such costs are often referred to as \textit{overhead}, a deviation from the average operational costs.

%In the hardware-based detection approach experiments, efficiency is often addressed when there is control over the resources. This is the case when classifiers are directly implemented in software (without machine learning tools, like Scikit-learn, Weka, and Matlab) or when the classifiers are directly implemented in hardware (like an FPGA).

\subsection{Software-based malware detection}
\label{subsec:software_malware_detection}

Software-based protection relies on specific software running in the system and analyzing the potential malware presence using different approaches. Authors in \cite{Aslan_Samet_2020} and \cite{Deldar:2023aa} proposed a very comprehensive selection of them:

\begin{itemize}

    \item \textbf{Signature-based}: the signature is a unique malware feature extracted from structural properties (e.g., code sequences) or run-time properties \cite{Idika_Mathur_2007}. The detection works as follows: features extracted from the executable generate a signature stored in a signature database. When the system is required to classify a potential threat, the detector extracts the related features and computes the signature, comparing it with signatures on the database. The potential threat is marked as malware if a hit occurs during the comparison. This approach is widely used within commercial antivirus and does not allow zero-day detection~\cite{Deldar:2023aa};

    \item \textbf{Software behavior analysis}: this approach is based on dynamic characteristics from run-time executions of programs \cite{Aslan_Samet_2020}. Dynamic characteristics might include processor and memory information, kernel usage (system calls), file system activities, and network communications. They are extracted with monitoring tools, a dataset is created, and a \gls{ml} detector distinguishes malicious and harmless applications. Software behavior analysis can detect malware variants often missed by the signature-based approach;

    %\item \textbf{Software Behavior Analysis}: This approach assesses program behaviors using monitoring tools to distinguish between malicious and harmless software. The process involves automatic analysis through sandboxing, monitoring system calls, tracking file changes, comparing registry snapshots, observing network activities, and monitoring processes. A dataset is created using these procedures to detect threats, specific features are extracted, and machine learning algorithms are employed for classification.

 
    \item \textbf{Heuristic-based detection}: this method relies on experiences and techniques, including rules and \gls{ml}. The process involves two phases: first, the detector system is trained with normal and abnormal data to identify relevant characteristics. In the second phase, known as monitoring or detection, the trained detector intelligently assesses new samples to make decisions \cite{Alzarooni_2012};
    
    \item \textbf{Deep Learning}: this falls under the umbrella of \gls{ml} algorithms, enabling computational models with multiple layers to extract more advanced features from raw input~\cite{Deldar:2023aa}. The \gls{FE} aspect combines elements from previous approaches, making it a novel method. Additionally, it proves highly effective for zero-day detection, as the \gls{FE}, employing multiple techniques, facilitates context adaptation and model updates, as highlighted in \cite{Deldar:2023aa}.
    \end{itemize}

Regarding software-based detection, it is also crucial to distinguish among the types of analysis carried out to extract the required information. According to \cite{Idika_Mathur_2007}, three ways are possible: (i) via \textit{static} analysis, using syntax or structural properties of the program/process (e.g., code sequences), (ii) via \textit{dynamic} analysis, extracting the necessary data during or after program execution, leveraging run-time information, and (iii) via \textit{hybrid} analysis, combining the two previous. Selecting one of those also affects the expected latency of the detection. While a static analysis aims to detect the threat even before executing the malicious program, the other two might require an entire execution before detection.

\subsection{Hardware-based malware detection}
\label{subsec:hardware_malware_detection}

Hardware-based detection, or \gls{HMD}, addresses the performance and computational overhead challenges of traditional malware detection techniques by utilizing low-level micro-architectural features of running applications on the target system \cite{He:2021aa}. The concept that malware can be identified through micro-architecture hardware events stems from the observation that programs exhibit phase behaviors \cite{Sherwood_2003, Isci_2006}. Program phases, which vary significantly between programs, manifest as patterns in architectural and micro-architectural events. This variation enables the discrimination of programs based on their time-behavioral hardware event patterns, facilitating the differentiation between malicious and benign applications. In 2011, Malone et al. \cite{Malone_2011} demonstrated the feasibility of detecting program code modifications based on the deviation of hardware events. In 2013, Demme et al. \cite{Demme_2013} showed the feasibility of detecting Android malware and Linux rootkits using hardware events values analyzed by a \gls{ml} classifier. 

The idea of \gls{HMD} is to perform dynamic analysis leveraging micro-architecture hardware events monitored by most modern microprocessors using \glspl{HPC} \cite{Alonso:2023aa}. Various \gls{ml} techniques can be applied to the \glspl{HPC} collected data \cite{He:2021aa}. 
One of the primary advantages of \gls{HMD} is that the analysis relies on real-time hardware collected data, enabling fast \gls{ml} classification; a few milliseconds suffice to identify threats. This translates to low latency, enabling runtime detection \cite{Sayadi_2018, Sayadi_2019, Patel_2017}. Unlike static technique analysis employed by most software-based antivirus solutions, which can be easily subverted by stealthy malware using concealment techniques, dynamic analysis via hardware-based approaches facilitates the detection of code variants and unknown malware \cite{Demme_2013}. Moreover, while software-based detection tools are software-based and susceptible to bugs or oversights in the underlying system software, hardware-based detection with secure hardware significantly reduces the possibility of malware subverting protection mechanisms \cite{Demme_2013, Tang_2014}.

On the performance front, the dynamic analysis conducted by software-based detection necessitates sophisticated computation, often at the expense of significant performance overhead. The increasing software size further complicates dynamic software analysis \cite{Demme_2013}. Conversely, in the hardware-based approach, understanding software behavior provided by micro-architectural events simplifies the analysis, reducing computational processing efforts and the cost of hardware-based detection \cite{Demme_2013, Sayadi_2022}.

%The problem of the computational overhead imposed by the software-based detection approach was probably already observed by many computer users with antivirus software installed on the operating system. When the antivirus scans, the computational overhead frequently causes the user to experience a slowdown in the applications, sometimes even making usability unfeasible.

deHowever, while the \glspl{HPC} demonstrate their ability to track behavioral deviations~\cite{Dutto:2021aa, Torres_Liu_2022, Kasap:2023aa}, their effectiveness remains open to discussion. On the positive side, \cite{Demme_2013, Tang_2014} demonstrated detector performance using this approach, reporting accuracy consistently exceeding 80\%, deeming it effective. Conversely, \cite{Zhou_2018} and \cite{Zhou_2021} conducted experiments challenging the effectiveness of hardware-based detection. They argued that reported detection capabilities often stem from tiny sample sizes and experimental setups favoring the detection mechanism unrealistically. Even if accurate, an 80\% accuracy is insufficient in scenarios with thousands of executables, risking many benign applications being misclassified as malware. They also questioned the causal link between low-level micro-architectural events and high-level software behavior. Lastly, they illustrated the hardware-based detector inability to distinguish ransomware embedded in a benign application like Notepad++. In a recent contribution, \cite{Botacin_Gregio_2022} acknowledged the absence of a perfect malware detector and argued that hardware-based detection is only effective for specific malware types. In particular, \cite{Botacin_Gregio_2022} proposes its effectiveness in identifying attacks exploiting architectural side-effects, citing examples such as RowHammer \cite{Kim_2014, Mutlu_2020} (detectable through excessive cache flushes \cite{Li_Gaudiot_2019}), \gls{ROP} attacks \cite{Prandini_Ramilli_2012} (identified by an abundance of instruction misses \cite{Wang_Backer_2016}), and DirtyCoW \cite{NIST_CVE-2016-5195} (detectable through heightened paging activity). The authors also emphasized the necessity for a maliciousness theory to enhance the understanding of malware threats and assess proposed defenses.

While \glspl{HPC} have been used in the past for safety and security, performance analysis, and optimization \cite{Weaver_McKee_2008,Carelli:2018aa,Carelli:2019aa}, it is well-known that they may suffer from inconsistency in implementation, leading to non-determinism and overcounting \cite{Weaver_Terpstra_Moore_2013}. Das et al. highlighted some of these \gls{HPC} challenges in security \cite{Das_2019}. Recent studies address \gls{HPC} discrepancies, propose methodologies, analyze resilience, and compare HPCs in various machines \cite{Barrera_2020,Kadiyala_2020,Ritter_2022,Sasongko_2023}. Given that \glspl{HPC} are hardware-based protections, detectors may be designed for specific devices with characteristics defined by the architecture and manufacturer. For instance, processors may track different numbers of events simultaneously, and discrepancies in instruction counting methods are possible \cite{Weaver_McKee_2008}. These factors underscore the need for malware detection applications to abstract software from the hardware level.

Among the inconsistencies and limitations of \glspl{HPC}, some countermeasures can be deployed to stabilize the generated data~\cite{Weaver_McKee_2008, Das_2019}. They include per-process filtering of events (applied by saving and restoring the counter values at context switches), proper interrupt handling, and minimizing the impact of non-deterministic events. In general, all works acknowledge that the evolution and improvement of the processors hardware monitoring units also tend to reduce this issue. 
Eventually, the classification task built on top of the \gls{HPC} data is commonly a \gls{ml} one. This frequently leads to techniques that increase the complexity of such algorithms, like ensemble learning and time series or even \glspl{dnn}~\cite{He:2021aa}.
%The problem of HPC consistency and accuracy of HPCs is relevant and deserves a little more discussion here. Based on systematization proposed by Das et al. \cite{Das_2019}, the sources for the inconsistency and inaccuracy are:
%
%    \begin{itemize}
%    
%    \item \textbf{Non-determinism}: refers to identical runs returning different values. Hardware interrupts and page faults were observed as the causes of non-determinism in X86 processors. For example, hardware interrupts are non-deterministic, and when they occur, they cause an extra time increment in the event, making it also non-deterministic. Another example: the first time a page of memory is accessed, it causes a page fault that triggers an interruption and consequently affects the event determinism \cite{Weaver_Terpstra_Moore_2013};
%  
%    \item \textbf{Overcounting}: means some instructions counting multiple times. There are various cases of overcounting in X86 processors: if the X87 top-of-stack pointer overflows, when the floating point unit is used for the first time due to missing terms in the instruction classifying hardware on the monitoring unit, and when an event measures microcoded events rather than retired events \cite{Weaver_Terpstra_Moore_2013};
%    
%    \item \textbf{External sources}: refers to variations in the run-time environment. For example, operating system activity, scheduling of programs in multitasking environments, memory layout, pressure, and multi-processor interactions may change between different runs \cite{Das_2019};
%    
%    \item \textbf{Variations in tool implementations}: the several tools to help obtain performance counter measurements often yield different results for the same application, even in a strictly controlled environment. The variation of sizes may result from the techniques involved in acquiring them (the point at which they start the counters), the reading technique (polling or sampling), the measurement level (thread, process, core, multiple cores), and the noise-ﬁltering approach used \cite{Das_2019}.
%    
%    \end{itemize}


