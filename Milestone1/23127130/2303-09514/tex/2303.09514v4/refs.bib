@InProceedings{maskrcnn,
  author={He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
  booktitle={ICCV}, 
  title={Mask R-CNN}, 
  year={2017},
  volume={},
  number={},
  pages={2980-2988},
  doi={10.1109/ICCV.2017.322}
 }

@inproceedings{attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@misc{swin,
  doi = {10.48550/ARXIV.2103.14030},
  url = {https://arxiv.org/abs/2103.14030},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@InProceedings{detr,
author={Carion, Nicolas
and Massa, Francisco
and Synnaeve, Gabriel
and Usunier, Nicolas
and Kirillov, Alexander
and Zagoruyko, Sergey},
title={End-to-End Object Detection with Transformers},
booktitle={ECCV},
year={2020},
pages={213--229},
abstract={We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
isbn={978-3-030-58452-8}
}

@inproceedings{def_detr,
title={Deformable DETR: Deformable Transformers for End-to-End Object Detection},
author={Xizhou Zhu and Weijie Su and Lewei Lu and Bin Li and Xiaogang Wang and Jifeng Dai},
booktitle={ICLR},
year={2021},
url={https://openreview.net/forum?id=gZ9hCDWe6ke}
}

@inproceedings{maskformer,
 author = {Cheng, Bowen and Schwing, Alex and Kirillov, Alexander},
 booktitle = {NeurIPS},
 pages = {17864--17875},
 title = {Per-Pixel Classification is Not All You Need for Semantic Segmentation},
 url = {https://proceedings.neurips.cc/paper/2021/file/950a4152c2b4aa3ad78bdd6b366cc179-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{mask2former,
  author    = {Bowen Cheng and
               Ishan Misra and
               Alexander G. Schwing and
               Alexander Kirillov and
               Rohit Girdhar},
  title     = {Masked-attention Mask Transformer for Universal Image Segmentation},
  journal   = {CoRR},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.01527},
  eprinttype = {arXiv},
  eprint    = {2112.01527},
  timestamp = {Tue, 07 Dec 2021 12:15:54 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-01527.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{mvit,
    author    = {Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and Li, Yanghao and Yan, Zhicheng and Malik, Jitendra and Feichtenhofer, Christoph},
    title     = {Multiscale Vision Transformers},
    booktitle = {ICCV},
    month     = {October},
    year      = {2021},
    pages     = {6824-6835}
}

@misc{trasetr,
  doi = {10.48550/ARXIV.2202.08453},
  url = {https://arxiv.org/abs/2202.08453},
  author = {Zhao, Zixu and Jin, Yueming and Heng, Pheng-Ann},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {TraSeTR: Track-to-Segment Transformer with Contrastive Query for Instance-level Instrument Segmentation in Robotic Surgery},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{isinet,
  doi = {10.48550/ARXIV.2007.05533},
  url = {https://arxiv.org/abs/2007.05533},
  author = {González, Cristina and Bravo-Sánchez, Laura and Arbelaez, Pablo},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {ISINet: An Instance-Based Approach for Surgical Instrument Segmentation},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
 
 @inproceedings{ternausnet,
	doi = {10.1109/icmla.2018.00100},
	url = {https://doi.org/10.1109\%2Ficmla.2018.00100},
	year = 2018,
	publisher = {{IEEE}},
	author = {Alexey A. Shvets and Alexander Rakhlin and Alexandr A. Kalinin and Vladimir I. Iglovikov},
	title = {Automatic Instrument Segmentation in Robot-Assisted Surgery using Deep Learning},
	booktitle = {{ICMLA}}
}

@misc{mftapnet,
  doi = {10.48550/ARXIV.1907.07899},
  url = {https://arxiv.org/abs/1907.07899},
  author = {Jin, Yueming and Cheng, Keyun and Dou, Qi and Heng, Pheng-Ann},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Incorporating Temporal Prior from Motion Flow for Instrument Segmentation in Minimally Invasive Surgery Video},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{dualmf,
  doi = {10.48550/ARXIV.2007.02501},
  url = {https://arxiv.org/abs/2007.02501},
  author = {Zhao, Zixu and Jin, Yueming and Gao, Xiaojie and Dou, Qi and Heng, Pheng-Ann},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Learning Motion Flows for Semi-supervised Instrument Segmentation from Robotic Surgical Video},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@Article{MaskSwinCnn,
author={Sun, Xinan
and Zou, Yuelin
and Wang, Shuxin
and Su, He
and Guan, Bo},
title={A parallel network utilizing local features and global representations for segmentation of surgical instruments},
journal={International Journal of Computer Assisted Radiology and Surgery},
year={2022},
month={Oct},
day={01},
volume={17},
number={10},
pages={1903-1913},
abstract={Automatic image segmentation of surgical instruments is a fundamental task in robot-assisted minimally invasive surgery, which greatly improves the context awareness of surgeons during the operation. A novel method based on Mask R-CNN is proposed in this paper to realize accurate instance segmentation of surgical instruments.},
issn={1861-6429},
doi={10.1007/s11548-022-02687-z},
url={https://doi.org/10.1007/s11548-022-02687-z}
}

@Article{offsets,
author={Kurmann, Thomas
and M{\'a}rquez-Neila, Pablo
and Allan, Max
and Wolf, Sebastian
and Sznitman, Raphael},
title={Mask then classify: multi-instance segmentation for surgical instruments},
journal={International Journal of Computer Assisted Radiology and Surgery},
year={2021},
month={Jul},
day={01},
volume={16},
number={7},
pages={1227-1236},
abstract={The detection and segmentation of surgical instruments has been a vital step for many applications in minimally invasive surgical robotics. Previously, the problem was tackled from a semantic segmentation perspective, yet these methods fail to provide good segmentation maps of instrument types and do not contain any information on the instance affiliation of each pixel. We propose to overcome this limitation by using a novel instance segmentation method which first masks instruments and then classifies them into their respective type.},
issn={1861-6429},
doi={10.1007/s11548-021-02404-2},
url={https://doi.org/10.1007/s11548-021-02404-2}
}

@InProceedings{sanchez,
author={Sanchez-Matilla, Ricardo
and Robu, Maria
and Luengo, Imanol
and Stoyanov, Danail},
title={Scalable Joint Detection and Segmentation of Surgical Instruments with Weak Supervision},
booktitle={MICCAI},
year={2021},
abstract={Computer vision based models, such as object segmentation, detection and tracking, have the potential to assist surgeons intra-operatively and improve the quality and outcomes of minimally invasive surgery. Different work streams towards instrument detection include segmentation, bounding box localisation and classification. While segmentation models offer much more granular results, bounding box annotations are easier to annotate at scale. To leverage the granularity of segmentation approaches with the scalability of bounding box-based models, a multi-task model for joint bounding box detection and segmentation of surgical instruments is proposed. The model consists of a shared backbone and three independent heads for the tasks of classification, bounding box regression, and segmentation. Using adaptive losses together with simple yet effective weakly-supervised label inference, the proposed model use weak labels to learn to segment surgical instruments with a fraction of the dataset requiring segmentation masks. Results suggest that instrument detection and segmentation tasks share intrinsic challenges and jointly learning from both reduces the burden of annotating masks at scale. Experimental validation shows that the proposed model obtain comparable results to that of single-task state-of-the-art detector and segmentation models, while only requiring a fraction of the dataset to be annotated with masks. Specifically, the proposed model obtained 0.81 weighted average precision (wAP) and 0.73 mean intersection-over-union (IOU) in the Endovis2018 dataset with 1{\%} annotated masks, while performing joint detection and segmentation at more than 20 frames per second.},
isbn={978-3-030-87196-3}
}

@InProceedings{tapir,
author={Valderrama, Natalia
and Ruiz Puentes, Paola
and Hern{\'a}ndez, Isabela
and Ayobi, Nicol{\'a}s
and Verlyck, Mathilde
and Santander, Jessica
and Caicedo, Juan
and Fern{\'a}ndez, Nicol{\'a}s
and Arbel{\'a}ez, Pablo},
title={Towards Holistic Surgical Scene Understanding},
booktitle={MICCAI},
year={2022},
abstract={Most benchmarks for studying surgical interventions focus on a specific challenge instead of leveraging the intrinsic complementarity among different tasks. In this work, we present a new experimental framework towards holistic surgical scene understanding. First, we introduce the Phase, Step, Instrument, and Atomic Visual Action recognition (PSI-AVA) Dataset. PSI-AVA includes annotations for both long-term (Phase and Step recognition) and short-term reasoning (Instrument detection and novel Atomic Action recognition) in robot-assisted radical prostatectomy videos. Second, we present Transformers for Action, Phase, Instrument, and steps Recognition (TAPIR) as a strong baseline for surgical scene understanding. TAPIR leverages our dataset's multi-level annotations as it benefits from the learned representation on the instrument detection task to improve its classification capacity. Our experimental results in both PSI-AVA and other publicly available databases demonstrate the adequacy of our framework to spur future research on holistic surgical scene understanding.},
isbn={978-3-031-16449-1}
}

@misc{endovis2017,
  doi = {10.48550/ARXIV.1902.06426},
  url = {https://arxiv.org/abs/1902.06426},
  author = {Max Allan et. al},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {2017 Robotic Instrument Segmentation Challenge},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{endovis2018,
  doi = {10.48550/ARXIV.2001.11190},
  url = {https://arxiv.org/abs/2001.11190},
  author = {Max Allan and Satoshi Kondo et. al.},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {2018 Robotic Scene Segmentation Challenge},
  publisher = {arXiv},
  year = {2020},
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@INPROCEEDINGS{unetplus,
  author={Kamrul Hasan, S. M. and Linte, Cristian A.},
  booktitle={EMBC}, 
  title={U-NetPlus: A Modified Encoder-Decoder U-Net Architecture for Semantic and Instance Segmentation of Surgical Instruments from Laparoscopic Images}, 
  year={2019},
  volume={},
  number={},
  pages={7205-7211},
  doi={10.1109/EMBC.2019.8856791}}
  
 @INPROCEEDINGS{cnn-rnn,
  author={Attia, Mohamed and Hossny, Mohammed and Nahavandi, Saeid and Asadi, Hamed},
  booktitle={2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={Surgical tool segmentation using a hybrid deep CNN-RNN auto encoder-decoder}, 
  year={2017},
  volume={},
  number={},
  pages={3373-3378},
  doi={10.1109/SMC.2017.8123151}}
  
@inproceedings{stereo,
author = {Ahmed Mohammed and Sule  Yildirim and Ivar  Farup and Marius Pedersen and {\O}istein  Hovde},
title = {{StreoScenNet: surgical stereo robotic scene segmentation}},
booktitle = {Medical Imaging 2019: Image-Guided Procedures, Robotic Interventions, and Modeling},
keywords = {StreoScenNet, Robotic scene segmentation, Semantic segmentation , Deep learning, Y-Net, Convolutional neural network, da Vinci  surgery, Instrument segmentation },
year = {2019},
doi = {10.1117/12.2512518},
URL = {https://doi.org/10.1117/12.2512518}
}

@misc{onetomany,
  doi = {10.48550/ARXIV.2103.12988},
  url = {https://arxiv.org/abs/2103.12988},
  author = {Zhao, Zixu and Jin, Yueming and Lu, Bo and Ng, Chi-Fai and Dou, Qi and Liu, Yun-Hui and Heng, Pheng-Ann},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {One to Many: Adaptive Instrument Segmentation via Meta Learning and Dynamic Online Adaptation in Robotic Surgical Video},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{saliency,
  doi = {10.48550/ARXIV.1907.00214},
  url = {https://arxiv.org/abs/1907.00214},
  author = {Islam, Mobarakol and Li, Yueyuan and Ren, Hongliang},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Learning Where to Look While Tracking Instruments in Robot-assisted Surgery},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@INPROCEEDINGS{rasnet,
  author={Ni, Zhen-Liang and Bian, Gui-Bin and Xie, Xiao-Liang and Hou, Zeng-Guang and Zhou, Xiao-Hu and Zhou, Yan-Jie},
  booktitle={2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}, 
  title={RASNet: Segmentation for Tracking Surgical Instruments in Surgical Videos Using Refined Attention Segmentation Network}, 
  year={2019},
  number={},
  pages={5735-5738},
  doi={10.1109/EMBC.2019.8856495}}

@inproceedings{toolnet,
	doi = {10.1109/iros.2017.8206462},
	url = {https://doi.org/10.1109\%2Firos.2017.8206462},
	year = 2017,
	month = {sep},
	publisher = {{IEEE}},
	author = {Luis C. Garcia-Peraza-Herrera and Wenqi Li and Lucas Fidon and Caspar Gruijthuijsen and Alain Devreker and George Attilakos and Jan Deprest and Emmanuel Vander Poorten and Danail Stoyanov and Tom Vercauteren and Sebastien Ourselin},
	title = {{ToolNet}: Holistically-nested real-time segmentation of robotic surgical tools},
	booktitle = {IROS}
}

@misc{endovis2015,
  doi = {10.48550/ARXIV.1805.02475},
  url = {https://arxiv.org/abs/1805.02475},
  author = {Sebastian Bodenstedt et. al.},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Comparative evaluation of instrument segmentation and tracking methods in minimally invasive surgery},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@ARTICLE{stswincl,
  author={Jin, Yueming and Yu, Yang and Chen, Cheng and Zhao, Zixu and Heng, Pheng-Ann and Stoyanov, Danail},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Exploring Intra- and Inter-Video Relation for Surgical Semantic Scene Segmentation}, 
  year={2022},
  number={},
  doi={10.1109/TMI.2022.3177077}
}

@misc{vit,
  doi = {10.48550/ARXIV.2010.11929},
  url = {https://arxiv.org/abs/2010.11929},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{ruurda2002robot,
  title={Robot-assisted surgical systems: a new era in laparoscopic surgery.},
  author={Ruurda, JP and Van Vroonhoven, Th JMV and Broeders, IAMJ},
  journal={Annals of the Royal College of Surgeons of England},
  volume={84},
  number={4},
  pages={223},
  year={2002},
  publisher={Royal College of Surgeons of England}
}

@article{luongo2021deep,
  title={Deep learning-based computer vision to recognize and classify suturing gestures in robot-assisted surgery},
  author={Luongo, Francisco and Hakim, Ryan and Nguyen, Jessica H and Anandkumar, Animashree and Hung, Andrew J},
  journal={Surgery},
  volume={169},
  number={5},
  pages={1240--1244},
  year={2021},
  publisher={Elsevier}
}

@article{du2016combined,
  title={Combined 2D and 3D tracking of surgical instruments for minimally invasive and robotic-assisted surgery},
  author={Du, Xiaofei and Allan, Maximilian and Dore, Alessio and Ourselin, Sebastien and Hawkes, David and Kelly, John D and Stoyanov, Danail},
  journal={International journal of computer assisted radiology and surgery},
  volume={11},
  number={6},
  pages={1109--1119},
  year={2016},
  publisher={Springer}
}

@inproceedings{chua2021toward,
  title={Toward force estimation in robot-assisted surgery using deep learning with vision and robot state},
  author={Chua, Zonghe and Jarc, Anthony M and Okamura, Allison M},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={12335--12341},
  year={2021},
  organization={IEEE}
}

@inproceedings{zhou2019needle,
  title={Needle localization for robot-assisted subretinal injection based on deep learning},
  author={Zhou, Mingchuan and Wang, Xijia and Weiss, Jakob and Eslami, Abouzar and Huang, Kai and Maier, Mathias and Lohmann, Chris P and Navab, Nassir and Knoll, Alois and Nasseri, M Ali},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={8727--8732},
  year={2019},
  organization={IEEE}
}


@article{joskowicz2017computer,
  title={Computer-aided surgery meets predictive, preventive, and personalized medicine},
  author={Joskowicz, Leo},
  journal={EPMA Journal},
  volume={8},
  number={1},
  pages={1--4},
  year={2017},
  publisher={Springer}
}

@article{chmarra2007systems,
  title={Systems for tracking minimally invasive surgical instruments},
  author={Chmarra, Magdalena K and Grimbergen, CA and Dankelman, J},
  journal={Minimally Invasive Therapy \& Allied Technologies},
  volume={16},
  number={6},
  pages={328--340},
  year={2007},
  publisher={Taylor \& Francis}
}

@inproceedings{laina2017concurrent,
  title={Concurrent segmentation and localization for tracking of surgical instruments},
  author={Laina, Iro and Rieke, Nicola and Rupprecht, Christian and Vizca{\'\i}no, Josu{\'e} Page and Eslami, Abouzar and Tombari, Federico and Navab, Nassir},
  booktitle={MICCAI},
  pages={664--672},
  year={2017},
  organization={Springer}
}

@inproceedings{FCN,
  title={Fully convolutional networks for semantic segmentation},
  author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle={CVPR},
  pages={3431--3440},
  year={2015}
}

@inproceedings{kurmann2017simultaneous,
  title={Simultaneous recognition and pose estimation of instruments in minimally invasive surgery},
  author={Kurmann, Thomas and Marquez Neila, Pablo and Du, Xiaofei and Fua, Pascal and Stoyanov, Danail and Wolf, Sebastian and Sznitman, Raphael},
  booktitle={MICCAI},
  pages={505--513},
  year={2017},
  organization={Springer}
}
@article{du2018articulated,
  title={Articulated multi-instrument 2-D pose estimation using fully convolutional networks},
  author={Du, Xiaofei and Kurmann, Thomas and Chang, Ping-Lin and Allan, Maximilian and Ourselin, Sebastien and Sznitman, Raphael and Kelly, John D and Stoyanov, Danail},
  journal={IEEE transactions on medical imaging},
  volume={37},
  number={5},
  pages={1276--1287},
  year={2018},
  publisher={IEEE}
}

@article{sanchez2022data,
  title={Data-centric multi-task surgical phase estimation with sparse scene segmentation},
  author={Sanchez-Matilla, Ricardo and Robu, Maria and Grammatikopoulou, Maria and Luengo, Imanol and Stoyanov, Danail},
  journal={International Journal of Computer Assisted Radiology and Surgery},
  volume={17},
  number={5},
  pages={953--960},
  year={2022},
  publisher={Springer}
}

@InProceedings{coco,
author={Lin, Tsung-Yi
and Maire, Michael
and Belongie, Serge
and Hays, James
and Perona, Pietro
and Ramanan, Deva
and Doll{\'a}r, Piotr
and Zitnick, C. Lawrence},
title={Microsoft COCO: Common Objects in Context},
booktitle={ECCV},
year={2014},
pages={740--755},
}

@misc{surgical_2019, title={Da Vinci Surgical System}, url={https://www.intuitive.com/en-us/products-and-services/da-vinci}, journal={Intuitive Surgical}, publisher={Intuitive Surgical}, author={Surgical, I}, year={2019} }

@misc{kinetics,
  doi = {10.48550/ARXIV.1705.06950},
  url = {https://arxiv.org/abs/1705.06950},
  author = {Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and Suleyman, Mustafa and Zisserman, Andrew},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {The Kinetics Human Action Video Dataset},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{vivit,
  doi = {10.48550/ARXIV.2103.15691},
  url = {https://arxiv.org/abs/2103.15691},
  author = {Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lučić, Mario and Schmid, Cordelia},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {ViViT: A Video Vision Transformer},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{videoswin,
  title={Video Swin Transformer},
  author={Ze Liu and Jia Ning and Yue Cao and Yixuan Wei and Zheng Zhang and Stephen Lin and Han Hu},
  journal={CVPR},
  year={2022},
  pages={3192-3201}
}

@misc{vidtr,
  doi = {vidtr},
  url = {https://arxiv.org/abs/2104.11746},
  author = {Zhang, Yanyi and Li, Xinyu and Liu, Chunhui and Shuai, Bing and Zhu, Yi and Brattoli, Biagio and Chen, Hao and Marsic, Ivan and Tighe, Joseph},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {VidTr: Video Transformer Without Convolutions},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{timesformer,
  doi = {10.48550/ARXIV.2102.05095},
  
  url = {https://arxiv.org/abs/2102.05095},
  
  author = {Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Is Space-Time Attention All You Need for Video Understanding?},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{mask2former_video,
  author    = {Bowen Cheng and
               Anwesa Choudhuri and
               Ishan Misra and
               Alexander Kirillov and
               Rohit Girdhar and
               Alexander G. Schwing},
  title     = {Mask2Former for Video Instance Segmentation},
  journal   = {CoRR},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.10764},
  eprinttype = {arXiv},
  eprint    = {2112.10764},
  timestamp = {Tue, 04 Jan 2022 15:59:27 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-10764.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2102-13644,
  author    = {Aneeq Zia and
               Kiran Bhattacharyya and
               Xi Liu and
               Ziheng Wang and
               Satoshi Kondo and
               Emanuele Colleoni and
               Beatrice van Amsterdam and
               Razeen Hussain and
               Raabid Hussain and
               Lena Maier{-}Hein and
               Danail Stoyanov and
               Stefanie Speidel and
               Anthony M. Jarc},
  title     = {Surgical Visual Domain Adaptation: Results from the {MICCAI} 2020
               SurgVisDom Challenge},
  journal   = {CoRR},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.13644},
  eprinttype = {arXiv},
  eprint    = {2102.13644},
  timestamp = {Sun, 02 Oct 2022 15:32:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-13644.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{MARZULLO2021105834,
title = {Towards realistic laparoscopic image generation using image-domain translation},
journal = {Computer Methods and Programs in Biomedicine},
volume = {200},
pages = {105834},
year = {2021},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2020.105834},
url = {https://www.sciencedirect.com/science/article/pii/S0169260720316679},
author = {Aldo Marzullo and Sara Moccia and Michele Catellani and Francesco Calimeri and Elena De Momi},
keywords = {Generative Adversarial Networks, Minimally Invasive Surgery, Image translation, Data Augmentation},
}

@ARTICLE{image_to_image,
  author={Colleoni, Emanuele and Stoyanov, Danail},
  journal={IEEE Robotics and Automation Letters}, 
  title={Robotic Instrument Segmentation With Image-to-Image Translation}, 
  year={2021},
  volume={6},
  number={2},
  pages={935-942},
  doi={10.1109/LRA.2021.3056354}}

@article{sintetic,
  author    = {Emanuele Colleoni and
               Philip J. Edwards and
               Danail Stoyanov},
  title     = {Synthetic and Real Inputs for Tool Segmentation in Robotic Surgery},
  journal   = {CoRR},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.09107},
  eprinttype = {arXiv},
  eprint    = {2007.09107},
  timestamp = {Tue, 28 Jul 2020 14:46:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-09107.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kinematic_padoy,
  author    = {Cristian da Costa Rocha and
               Nicolas Padoy and
               Benoit Rosa},
  title     = {Self-Supervised Surgical Tool Segmentation using Kinematic Information},
  journal   = {CoRR},
  volume    = {abs/1902.04810},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.04810},
  eprinttype = {arXiv},
  eprint    = {1902.04810},
  timestamp = {Sat, 23 Jan 2021 01:19:46 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-04810.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}