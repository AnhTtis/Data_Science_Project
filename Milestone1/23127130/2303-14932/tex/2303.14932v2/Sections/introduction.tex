\section{Introduction}\label{sec:introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
%\IEEEPARstart{T}{his} demo file is intended to serve as a ``starter file''
%for IEEE Communications Society journal papers produced under \LaTeX\ using
%IEEEtran.cls version 1.8b and later.
%% You must have at least 2 lines in the paragraph with the drop letter
%% (should never be an issue)
%I wish you the best of success.
%
%\hfill mds
%
%\hfill August 26, 2015

%\subsection{Subsection Heading Here}
%Subsection text here.

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

%\subsubsection{Subsubsection Heading Here}
%Subsubsection text here.


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an -eps-converted-to.pdf filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.

%%%%%%%%%%%%%%%%%START HERE%%%%%%%%%%%%%
%-----UNCONSTRAINED MDPs and POMDPs
\IEEEPARstart{S}{ingle-Agent} 
Markov Decision Processes (SA-MDPs) \cite{bellman57} and Single-Agent Partially Observable Markov Decision Processes (SA-POMDPs) \cite{astrom65} have long served as the basic building-blocks in the study of sequential decision-making. An SA-MDP is an abstraction in which an agent sequentially interacts with a fully-observable Markovian environment to solve a multi-period optimization problem; in contrast, in SA-POMDP, the agent only gets to observe a noisy or incomplete version of the environment. %Approaches to solve these two problems have also come a long way. 
In 1957, Bellman proposed dynamic-programming as an approach to solve SA-MDPs \cite{bellman57,howard:dp}. This combined with the characterization of SA-POMDP into an equivalent SA-MDP \cite{smallwood1973optimal, sondik1978optimal, kaelbing199899} (in which the agent maintains a belief about the environment's true state) made it possible to extend dynamic-programming results to SA-POMDPs. 
% However, dynamic-programming was considered to be of little practical use in its onset -- due to the computational capacity required to solve these models.\footnote{It has been shown that SA-MDP and SA-POMDP are respectively P-complete and PSPACE-complete \cite{papadimitriou87}.} Nevertheless, with the ever increasing success in information technology, in particular the ability to process large quantities of data, dynamic-programming based approaches have not only been used to (approximately) solve a given known 
% model of the two kinds but have also led to the development of algorithmic frameworks (\cite{watkins1992, rummery1994, schulman2015, schulman2017, subramanian19, subramanian22} to name a few) that attempt to solve the harder problem of \emph{sequential decision-making under no (or little) knowledge of the environment}, an area of research broadly known as \emph{reinforcement learning} \cite{sutton98}.% or simply \emph{learning}.  
\emph{Reinforcement learning}~\cite{sutton98} based algorithmic frameworks % (\cite{watkins1992, rummery1994, schulman2015, schulman2017, subramanian19, subramanian22} to name a few) 
use data-driven dynamic-programming approaches to solve such single-agent sequential decision-making problems when the environment is unknown.

% With the advent of cyber-physical systems and internet-of-things, now there has been a growing interest in team-based sequential decision-making problems where there are multiple agents that work together under a prespecified set of operational constraints, in order to achieve a common goal. Besides the operational constraints, an important aspect of such problems is that communication between agents is costly. Consider, for instance, a fleet of roaming robots that pick, pack or stack objects in a large warehouse. Communication between the robots may take time that could otherwise be spent performing their assigned physical actions.  %Thus, it may be sub-optimal for the robots to communicate frequently.
% Thus, from both the planning and learning perspectives, the agents' designer is faced with the difficult task of deciding what each robot should do in between communications, when it has access to only its own sensory information. 
In many engineering systems, there are multiple decision-makers that collectively solve a sequential decision-making problem but with safety constraints: e.g., a team of robots performing a joint task, a fleet of automated cars navigating a city, multiple traffic-light controllers in a city, etc. Bandwidth constrained communications and communication delays in such systems lead to a decentralized team problem with information asymmetry. In this work, we study a fairly general abstraction of such systems, 
namely 
that of a cooperative multi-agent constrained POMDP, hereon referred to as \macpomdp. The special cases of \macpomdp 
when there are no constraints, when there is only one agent, or when the environment is fully observable to each agent, are referred to as MA-POMDP, SA-C-POMDP, and MA-C-MDP, respectively. The relationships among such models are shown in Figure \ref{fig:model_relationships}.


\begin{rem} 
MA-C-POMDP, is an extension of the decentralized POMDP (Dec-POMDP) to the setting of constrained decision-making, i.e., Decentralized Constrained POMDP (Dec-C-POMDP). Importantly, in this paper, MA-POMDP is equivalent to Dec-POMDP and MA-C-MDP to Dec-C-MDP. In Dec-POMDP or Dec-C-POMDP, agents are assumed to act based on their individual information without any communication with each other. 

For a good introduction to Dec-POMDPs, please see \cite{oliehoek16}. We inform beforehand that \cite{oliehoek16} considers MA-POMDP as a special case of Dec-POMDP wherein agents communicate all their information with each other. We have decided to deviate from this categorization because the term multi-agent itself does not specify whether agents engage in communication and/or the degree to which they do so.\footnote{Settings that involve communication can be incorporated in our MA-C-POMDP formulation through actions and observations of the agents (see~\cite{oliehoek16}).}
\end{rem}

\begin{comment}
% \textbf{Remarks}:
% \begin{itemize}[leftmargin=0pt, 
% itemindent=10pt,
% labelwidth=0pt, 
% labelsep=5pt, 
% % listparindent=0.7cm,
% % align=left
% ]
% \item 
We inform the reader that our chosen framework, MA-C-POMDP, is an extension of the decentralized POMDP (Dec-POMDP) to the setting of constrained decision-making, i.e., Decentralized Constrained POMDP (Dec-C-POMDP). Importantly, in this paper, 
% we note that 
MA-POMDP is equivalent to Dec-POMDP and MA-C-MDP to Dec-C-MDP. 

In Dec-POMDP or Dec-C-POMDP, agents are assumed to act based on their individual information without communicating with each other at all. 
% \item 
For a good introduction to Dec-POMDPs, please see \cite{oliehoek16}. We inform beforehand that \cite{oliehoek16} considers MA-POMDP as a special case of Dec-POMDP by allowing it as a framework in which agents communicate all their information with each other. We have decided to deviate from this categorization because the term multi-agent itself does not specify whether agents engage in communication and/or the degree to which they do so.\footnote{Settings that involve communication can be incorporated in our MA-C-POMDP formulation through actions and observations of the agents (see~\cite{oliehoek16}).}

% \item By assuming no communication between agents, settings that involve communication can still be modelled by incorporating them through actions and observations of the agents.

% \item 
% Settings that involve communication can be incorporated in multi-agent/decentralized POMDPs through actions and observations of the agents (see~\cite{oliehoek16}).
% \end{itemize}
\end{comment}



\subsection{Related Work}
\subsubsection{Single-Agent Settings}
Prior work on planning and learning under constraints has primarily focused on single-agent constrained MDP (SA-C-MDP) where unlike in SA-MDPs, the agent solves a constrained optimization problem. For this setup, a number of fundamental results from the planning perspective have been derived -- for instance, \cite{altman94, altman96, feinberg94, feinberg95, feinberg96, feinberg2000, feinberg2020}; see \cite{altman-constrainedMDP} for details of the convex-analytic approach for SA-C-MDPs. 
% A well-expounded presentation on SA-C-MDPs can be found in the excellent book by Altman \cite{altman-constrainedMDP}. These aforementioned results have led to the development of a number of nice algorithms in the learning setting (such as \cite{borkar2005AnAA,bhatnagar2010AnAA,bhatnagar2012AnOA,Wei2022APM,wei22a-pmlr-v151,bura2021,vaswani2022}). 
These aforementioned results have led to the development of many algorithms in the learning setting: see \cite{borkar2005AnAA, bhatnagar2010AnAA, bhatnagar2012AnOA, Wei2022APM, wei22a-pmlr-v151, bura2021, vaswani2022}. Unlike SA-C-MDPs, rigorous results for SA-C-POMDPs are limited; few works include \cite{dongho2011, jongmin18, undurti2010, jamgochian2022}.

\subsubsection{Multi-Agent Settings}
% \subsubsection{\nk{Decentralized} Settings}
%----MA-POMDPs and MA-C-POMDPs
% Unlike the single-agent setting, there has been little (or no) work done on MA-POMDPs and MA-C-POMDPs. The primary reason for the lack of such work is that it is quite challenging -- due, in particular, to \emph{partial observability} of the environment combined with the \emph{information-asymmetry}\footnote{Here, information-asymmetry refers to the mismatch in the information each agent has to choose their action.} inherent in the multi-agent setup. These two issues together create difficult analytical and practical challenges to find jointly-optimal (decentralized) policy-profiles in a computationally-feasible time. In fact, it has been shown that solving a finite-horizon MA-POMDP with more than two agents is NEXP-complete \cite{bernstein00}, thus implying a doubly-exponential complexity growth in the worst case. Nevertheless, theoretical solutions have been proposed to solve finite-horizon MA-POMDPs. Most notable among them (in the authors' opinion) is the \emph{common information (CI) approach} introduced in \cite{nayyar13, nayyar14}. In this approach, the common information of all agents is used to instantiate a fictitious/virtual entity, known as the \emph{coordinator}, that takes an action based only on this common information, while its action is an enforcing prescription to each agent on how to act given a specific realization of their private history. 
Challenges arising from the combination of \emph{partial observability} of the environment and \emph{information-asymmetry}\footnote{%Here, information-asymmetry refers to the 
Mismatch in the information of the agents.} have led to difficulties in developing general solutions to MA-POMDPs: e.g., solving a finite-horizon MA-POMDP with more than two agents is known to be NEXP-complete~\cite{bernstein00}. Nevertheless, conceptual approaches exist to establish solution methodologies and structural properties in (finite-horizon) MA-POMDPs namely: i) the person-by-person approach~\cite{witsenhausen1979structure}; ii) the designer's approach~\cite{witsenhausen1973standard}; and iii) the \emph{common-information (CI) approach}~\cite{nayyar13,nayyar14}.
% In the CI approach, the common information of all agents is used to instantiate a fictitious/virtual entity, known as the \emph{coordinator}, that takes an action based only on this common information, while its action is an enforcing prescription to each agent on how to act given a specific realization of their private history. Akin to a SA-POMDP, this transformation leads to the formulation of a dynamic program that in principle can be used to solve the (finite-horizon) MA-POMDP.
Using a fictitious \emph{coordinator} that only uses the common information to take actions, the CI approach allows for the transformation of the problem to a SA-POMDP which can be used to solve for an optimal control. 
The CI approach has also led to the development of a multi-agent reinforcement learning (MARL) framework \cite{hsu22} where agents learn good compressions of common and private information that can suffice for approximate optimality. On the empirical front, worth-mentioning works include 
% \cite{gupta17,nolan2020,rashid2020,rashid2020-2}. 
\cite{gupta17,rashid2020-2}. 
Finally, as far as we know, work on 
MA-C-POMDPs 
%Dec-C-POMDPs 
is non-existent.





% MA-C-POMDPs (not much to mention, little or no prior work)
\subsection{Contribution}
For MA-C-POMDPs, the technical challenges increase even more from those of MA-POMDPs because restriction of the search space to deterministic policy-profiles is no longer an option\footnote{Restricting to deterministic policies can be sub-optimal in SA-C-MDPs and SA-C-POMDPs: see \cite{altman-constrainedMDP} and \cite{dongho2011}.}. Therefore, the coordinator in the equivalent SA-C-POMDP has an uncountable prescription space, which leads to an uncountable state-space in its equivalent SA-C-MDP. This is an issue because most fundamental results in the theory of SA-C-MDPs (largely based on occupation-measures) rely heavily on the state-space being at most countably-infinite; see \cite{altman-constrainedMDP}. Due to these reasons, the study of MA-C-POMDPs 
calls for a new methodology---one which avoids this transformation and directly studies the decentralized problem. Our work takes the first steps in this direction and presents a rigorous approach for MA-C-POMDPs 
which is based on structural characterization of the set of behavioral policies and their performance measures, and using measure theoretic results. The main result in this paper, namely Theorem \ref{thm:strongduality}, establishes strong duality and existence of a saddle-point for MA-C-POMDPs, thus providing a firm theoretical basis for (future) development of primal-dual type planning and learning algorithms.


\subsection{Organization}
The rest of the paper is organized as follows. Mathematical model of (cooperative) MA-C-POMDP is introduced in Section \ref{sec:problem}. The optimization problem is formulated in Section \ref{sec:optimization_problem}. Results on strong duality and existence of a saddle point are then derived in Section \ref{sec:strongduality}. 
% with details deferred to Appendices \ref{seC:appendix:intermediary_results}, 
% % \ref{sec:appendix:helpful_facts}, and \ref{sec:appendix:minimax}. 
% and \ref{sec:appendix:helpful_facts}.
Finally, concluding remarks are given in Section \ref{sec:conclusion}.

\subsection{Notation}
Before we present the model, we highlight the key notations in this paper.
\begin{itemize}[leftmargin=0pt, 
itemindent=10pt,
labelwidth=0pt, 
labelsep=5pt, 
% listparindent=0.7cm,
% align=left
]
\item The sets of integers and positive integers are respectively denoted by $\mbb{Z}$ and $\mbb{N}$. For integers $a$ and $b$, $[a,b]_{\mbb{Z}}$ represents the set $\{a, a+1, \dots, b\}$ if $a\le b$ and $\emptyset$ otherwise. The notations [a] and $[a,\infty]_{\mbb{Z}}$ are used as shorthand for $[1, a]_{\mbb{Z}}$ and $\{a, a+1, \dots \}$, respectively.
\item For integers $a \le b$ and $c \le d$, and a quantity of interest $q$, $\un{q}{a:b}$ is a shorthand for the vector $\l( \un{q}{a}, \un{q}{a+1}, \dots, \un{q}{b} \r)$ while $\ut{q}{c:d}$ is a shorthand for the vector $\l( \ut{q}{c}, \ut{q}{c+1}, \dots, \ut{q}{d} \r)$. The combined notation $\utn{q}{a:b}{c:d}$ is a shorthand for the vector $(\utn{q}{i}{j}: i \in [a,b]_{\mbb{Z}}, j \in [c, d]_{\mbb{Z}})$. The infinite tuples $\l( \un{q}{a}, \un{q}{a+1}, \dots, \r)$ and $\l( \ut{q}{c}, \ut{q}{c+1}, \dots, \r)$ are respectively denoted by $ \un{q}{a:\infty}$ and $\ut{q}{c:\infty}$.
\item For two real-valued vectors $v_1$ and $v_2$, the inequalities $v_1 \le v_2$ and $v_1 < v_2$ are meant to be element-wise inequalities.
\item Probability and expectation operators are denoted by $\pr$ and $\mbb{E}$, respectively. Random variables are denoted by upper-case letters and their realizations by the corresponding lower-case letters.  At times, we also use the shorthand  $\mbb{E}\l[ \cdot | x \r] \defeq \mbb{E}\l[ \cdot | X = x \r]$ and $\pr\l( y | x \r) \defeq \pr\l( Y = y | X = x \r)$ for conditional quantities.
\item Topological spaces are denoted by upper-case calligraphic letters. For a topological-space $\mcl{W}$, $\borel{\mcl{W}}$ denotes the Borel $\sigma$-algebra, measurability is determined with respect to $\borel{\mcl{W}}$, and $\m{\mcl{W}}$ denotes the set of all probability measures on $\borel{\mcl{W}}$ endowed with the topology of weak convergence. Also, unless stated otherwise, ``measure'' means a non-negative measure.
\item Unless otherwise stated, if a set $\mcl{W}$ is countable, as a topological space it will be assumed to have the discrete topology. Therefore, the corresponding Borel $\sigma$-algebra $\borel{\mcl{W}}$ will be the power-set $2^{\mcl{W}}$.
\item Unless stated otherwise, the product of a collection of topological spaces will be assumed to have the product topology.
\item The notation in Appendices \ref{sec:appendix:helpful_facts} and \ref{sec:appendix:minimax} is exclusive and should be read independent of the rest of the manuscript.
\end{itemize}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/model_relationships.pdf}
    \caption{Relationships between Models of Cooperative Sequential Decision-Making under Constraints.}
    \label{fig:model_relationships}
\end{figure}