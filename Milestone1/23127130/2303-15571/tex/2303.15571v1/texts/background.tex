\section{Background} \label{sec: background}
This section presents relevant background on adversarial attacks, protection, and EM side-channel.

\subsection{Adversarial Attacks on DNNs}\label{sec:adversarial}
DNN is an artificial neural network with multiple layers to represent a function, $F:X\rightarrow Y$, 
with parameters $\omega_F$ such as weights, kernels, and biases, where $X$ denotes the input space and $Y$ the output space.
%The connections between layers are weighted by the model weights $\omega_F$.
In the training phase, a DNN is trained with a dataset of input-output pairs to arrive at optimal values of $\omega_F$, 
to minimize the loss function $J_F$, which is a distance measurement between the model predicted result $F(x)$ and the ground truth $y*$.
%Then the model training phase become an optimization problem to minimize the loss $J_F$.
The widely-used optimizers include stochastic gradient descent (SGD)~\cite{sgd} and Adam~\cite{adam}.
%Under our settings, the victim model is pre-trained with fixed weights and biases.
%The victim model we protect is a pre-trained model with fixed $\omega_F$, which is used for image inference to assign a label to an unknown input  $x_t$.
Taking image classification as an example, the DNN model runs inference on the unknown input, $x_t$, and predicts a class, out of $m$ classes, with the largest probability.
%We protect the deployed victim model, used for image inference to assign a label to an unknown input, $x_t$.
%For an $m$-class classification model, the output will be the class with the largest probability.
\begin{equation}
    y = F(x_t) = \text{softmax}(Z(x_t))
    \label{eq: m-class classifier}
\end{equation}
where the vector $Z(x_t)$ is known as logits.
Our detection method assumes that the defender can only query the model and know the output $y$ while the logits $Z(x)$ are unavailable.

DNN model is vulnerable to adversarial attacks.  
An adversarial sample ($x'$) is a carefully crafted sample, which has a human-imperceivable difference from the original benign sample ($x$), 
but causes the DNN to misclassify it to a different class $F(x') \neq y$.
%We define an input $x$ to $F(\cdot)$ is benign when it is benignly create(like samples in the training and testing set), where $F(x) = y$
%Given the same $F(\cdot)$ and $x'$ close to $x$ so that $F(x') \neq y$ we say $x'$ is an (untargeted) adversarial sample.
If $F(x')$ is an arbitrary class except for $y$, $x'$ is an untargeted adversarial sample.
A more restrictive and harmful case is the targeted adversarial sample, where $F(x')=l \neq y$, a specific target class. 
%the attacker picks a specific class $l\neq y$ and generates $x'$ close to $x$ with $F(x')=l$, where $x'$ is a targeted adversarial example.
%Intuitively, an adversarial sample is a special input that is very similar to the benign inputs,
%but the machine model will assign different labels for these inputs.
%We focus the targeted adversarial attack in this paper except for Deepfool attack.
In this paper, we consider both untargeted and targeted attacks.
The difference between the adversarial example and benign example can be measured by $L_p$, defined as $\Delta_p = \sum_{i=0}^n(|x_i-x'_i|^p)^{\frac{1}{p}}$.
Common choices of $L_p$ include: $L_0$, the number of pixels changed; $L_1$, the Manhattan norm; $L_2$, the Euclidean distance norm; $L_{\inf}$ the largest absolute change of any pixels.
%By control these distance distrotion, the images can looks very similar visually.
The adversarial attack can be viewed as an optimization problem:
\begin{equation}
    \textbf{minimize} \quad \Delta_p(x, x') \quad\textbf{s.t.} \quad F(x') \neq y
    \label{eq: adversarial attack}
\end{equation}
%Therefore, such attacks by minimization are also called gradient-based attacks.

GoodFellow et al.~\cite{goodfellow2014explaining} first proposed the concept of the adversarial sample and introduced Fast Gradient Sign Method (FGSM) to generate adversarial samples.
% The Basic Iterative Method (BIM)~\cite{kurakin2016adversarial} extends FGSM by iterating it with a smaller step size for multiple times, where each step the intermediate results are clipped
% so as to ensure smaller perturbation. %to ensure they are in an $\epsilon$-neighbourhood of the original input.
Madry et al. introduced the Projected Gradient Descent (PGD)~\cite{madry2017towards} attack to improve the attack efficiency of the Basic Iterative Method (BIM)~\cite{kurakin2016adversarial}. %Instead of starting from a fixed point, PGD is re-started from many points randomly. 
Other learning-based attacks include DeepFool~\cite{moosavi2016deepfool} and Carlini and Wagner Attack (CW)~\cite{carlini2017towards}, 
where CW attack is proven to be one of the strongest attacks~\cite{carlini2017towards, carlini2017adversarial} at the cost of generation speed.
% DeepFool attack \cite{moosavi2016deepfool} makes an assumption that the boundaries of a class are line planes and searches the adversarial sample within a specific region with $L_2$ distance. It can only generate untargeted adversarial examples. %It can only be used in a untargeted case, where the attacker can only ensure that the model classifies the adversarial example in a class different from the original.
% Recently Carlini and Wagner Attack (CW) was introduced~\cite{carlini2017towards}, which formulates finding adversarial examples as an optimization problem by defining a success function $f(x+\delta)\leq 0$ if and only if the model misclassifies. %It's sum is minimized with a trade off constant '$c$' chosen by modified binary search.
% This formulation enables the usage of modern optimizers such as the ADAM solver~\cite{adam}.
% CW attack is proven to be powerful and effective \cite{carlini2017towards, carlini2017adversarial} at the cost of generation speed.
% In this paper, we focus on three representative attacks, PGD, DeepFool and CW.

\subsection{Existing Software Detection Methods}
The existing adversarial detection methods, all software-based, can be classified into three categories.

\textit{Distributional Detection:}
These detectors perform some statistical analysis on the inputs or intermediate values of model execution (e.g., activation values) to find adversarial samples. 
Grosse \cite{grosse2017statistical} used Maximum Mean Discrepancy test (MMD) to determine whether the benign and adversarial inputs have the same underlying distribution or not.
Feinman \cite{feinman2017detecting} use Kernel Density Estimation to measure the distance of distributions. %and identify adversarial examples falling into a different distribution.
However, these detection methods are ineffective on more complex datasets \cite{carlini2017adversarial}.
Ma \cite{ma2018characterizing} introduced Local Intrinsic Dimensionality (LID) to characterize adversarial regions of the model. However, LID is proven to perform poorly on a number of attacks~\cite{ma2019nic}.
%, especially transferred attacks.

\textit{Latent Space Detection:}
The second type of detector employs a pre-processing step to reduce variation. 
Grosse \cite{grosse2017statistical} found that adversarial examples tend to place a higher weight on larger principal components, narrowing down the targets for detection. %By decomposing the input image, one can detect adversarial images.
%Bhagoji \cite{bhagoji2017dimensionality} proposed a defense method that trains the classifier with only a number of principal components,
%which limits the attacker to only manipulate the first several principal components.\yf{Is this defense or detection?}
Some approaches train denoisers to reconstruct the inputs by removing the adversarial noise added by the attacker, such as
%and reform the input samples where it also remove the adversarial noise added by the attacker.
auto-encoders used in MagNet \cite{meng2017magnet}  and the mean blur method used in \cite{li2017adversarial}.
% proposed a mean blur method % that applies an $3\times 3$
%with an average filter to blur the input images before inputting to the classifier.
%They also applied PCA to the intermediate outputs from the convolutional layers of the DNN, and used SVM classifiers to detect the adversarial samples.
%Specifically, they cascade all classifiers from each layer's output and make a joint decision on whether reject the input or not.
%They also extract intermediate outputs of the convolutional layers of the DNN and apply Principal Component Analysis (PCA) and SVM classifier for adversarial detection.
%These methods all require a well-selected training dataset to train the denoiser or find the principal components.
Most of them work for simple attack methods such as FGSM, but cannot resist the state-of-the-art CW attack.

\textit{Inconsistency Detection:}
This approach focuses on the model misbehavior during inference of adversarial examples.
%Specifically, the DNN will show some inconsistency in some model features such as the neuron activation values.
Feinman et al.~\cite{feinman2017detecting} proposed Bayesian neural network uncertainty to measure the uncertainty of a DNN under a given input.
By introducing some randomness (e.g., Dropout \cite{srivastava2014dropout}) during the inference, the DNN model tends to give the same outputs for benign inputs but different outputs for adversarial ones. % because of the randomness.
The Feature Squeezing approach \cite{xu2017feature}
% achieves very high detection rate. The author finds that DNN tends to have an unnecessarily large feature spaces. They propose a Squeezing techniques by reducing
reduces the color depth %from $8$-bit to small ones %to limits the degree of freedom to an adversary,
and observe that
%After squeezing the feature space, the
adversarial samples are likely to induce different classification results while benign inputs are not.
Tao et al.~\cite{tao2018attacks} introduced the Attacks meet Interpretability structure (AmI),
which measures the inconsistency of the victim DNN with another neural network enhanced with human perceptible attributes under adversarial examples.
%A state-of-the-art detection framework, called 
In the Network Invariant Checking (NIC) work proposed by Ma \cite{ma2019nic}, 
the key idea is during model execution, there are class-dependent
%The author claims that the adversarial samples attack the model via
provenance channels (the distribution of activated neurons in the network) and activation value channels (value distributions of activated neurons). 
%They propose a  framework to detect the adversarial sample via the provenance invariant(PI) and activation value invariant(VI).
It employs a one-class SVM to determine outliers.
NIC shows promising results against a broader range of attacks, including the CW attack.
%is utilized to determine the invariant of any inputs on PI and VI.
%All methods above using model inconsistency checking achieve a good detection performance and most of them is unsupervised which does not require the involvements of adversarial samples to train.
%However, one limitation of these work is they all require knowledge of the victim model execution, including the structure, layer (neuron) outputs, and logits. These methods are not viable for mobile or edge devices where the privacy of victim model becomes a big issue. 
%However, it falls into the conventional category of white-box software method that requires knowledge of the model internals and the model output logits. 
Our approach generally falls into the type of inconsistency detection, %is closely related to the NIC work, but treats the victim system as a third-party black-box and uses the EM leakage instead for adversarial detection. 
in a black-box victim system scenario. 
%but overcome the limitation by using EM leakage instead of direct observations for internal model execution. 

\subsection{Class Activation Map in Adversarial Detection} \label{sec: CAM}
\ry{In this paragraph, I want to propose adversarial example detection using CAM, which leverages detection using semantic and discriminative information.}
Class Activation Map (CAM)~\cite{selvaraju2016grad, jiang2021layercam} is commonly used to explain the behavior of deep neural networks, 
\ry{semantic information is the CAM from benign samples, deterministic information is the real CAM used by adversarial sample leading to wrong results.}
showing how the network progressively (with more layers) identifies the important region of the input (features) that leads to the class prediction. \yf{the previous line needs rephrasing. Verify if my edits are correct}
For benign samples, CAMs can represent the images' semantic information~\cite{zhou2021removing, vinogradova2020towards}. 
However, the adversarial perturbations can impact the focus of neural networks, which leads to wrong predictions. \yf{complete the previous line.} 
For example, in Fig.~\ref{fig: adv-gradcam}, we present an example originally comes from Class ``Sandal'' and is classified as ``Trouser'' via CW attack, 
together with samples from source and target class, followed by their class activation maps of the first two convolutional layers, respectively. 
We conclude that:
\begin{itemize}[leftmargin=*]
    \item For benign samples, the class activation maps (semantic information) visually show features that lead to the classification result. \yf{where are discriminative features? never mentioned. how did you arrive at the first conclusion.}
    \item The CAMs of adversarial samples do not resemble those of benign examples that represent the target class. 
    And because of adversarial noise, their CAMs diverge from the source class to some anomalies gradually by the model depth.
    \item CAMs of different layers vary, and the impact of adversarial perturbations will be amplified by the network depth. 
\end{itemize}
% Class activation map inspires us to look into the contradictions during DNN model inference such as activation patterns, important features and relations between layers, prompting us exploiting EM emanations during DPU computation in the following sections.
Therefore, one way for adversarial detection is to train an out-of-distribution detector to figure out the mismatch between CAMs from benign samples and ones from adversarial samples.

% \begin{figure}
%     \setlength\tabcolsep{1pt}
%     \centering
%     \begin{tabularx}{\textwidth}{ccc|ccc}
%     \includegraphics[width=0.16\linewidth]{imgs/Orginal_norm.pdf} &
%     \includegraphics[width=0.16\linewidth]{imgs/GradCAM_norm_layer1.pdf}&
%     \includegraphics[width=0.16\linewidth]{imgs/GradCAM_norm_layer2.pdf}&
%     \includegraphics[width=0.16\linewidth]{imgs/Origin_PGDLinf.pdf}&
%     \includegraphics[width=0.16\linewidth]{imgs/GradCAM_adv_layer1_PGDLinf.pdf}&
%     \includegraphics[width=0.16\linewidth]{imgs/GradCAM_adv_layer2_PGDLinf.pdf}\\
%     \multicolumn{3}{l}{\textcolor{black}{Benign:"Trouser"}} & \multicolumn{3}{l}{\textcolor{black}{PGD ($L_{inf}$):"Sandal"->"Trouser"}}\\
%     \includegraphics[width=0.16\linewidth]{imgs/Origin_PGDL1.pdf}&
%     \includegraphics[width=0.16\linewidth]{imgs/GradCAM_adv_layer1_PGDL1.pdf}&
%     \includegraphics[width=0.16\linewidth]{imgs/GradCAM_adv_layer2_PGDL1.pdf}&
%     \includegraphics[width=0.16\linewidth]{imgs/Origin_CW.pdf}&
%     \includegraphics[width=0.16\linewidth]{imgs/GradCAM_adv_layer1_CW.pdf}&
%     \includegraphics[width=0.16\linewidth]{imgs/GradCAM_adv_layer2_CW.pdf}\\
%     \multicolumn{3}{l}{\textcolor{black}{PGD ($L_{1}$):"Shirt"->"Trouser"}} & \multicolumn{3}{l}{\textcolor{black}{CW ($L_{2}$):"Coat"->"Trouser"}}\\
%     \end{tabularx}
%     \Description{GradCAM of Adversarial sample}
%     \caption{GradCAM results of benign image and adversarial images when the victim outputs are `Trouser' (attack target)}
%     \label{fig: adv-gradcam}
% \end{figure}

\begin{figure}[t]
    % \setlength{\extrarowheight}{1em}
    \centering
    \small
    \begin{tabularx}{\linewidth}{XXXX}
    \begin{tabular}[x]{@{}X@{}} \textbf{Benign:}\\\textbf{``Trouser''}\end{tabular} &
    \includegraphics[width=\linewidth]{imgs/Orginal_norm.pdf} &
    \includegraphics[width=\linewidth]{imgs/GradCAM_norm_layer1.pdf}&
    \includegraphics[width=\linewidth]{imgs/GradCAM_norm_layer2.pdf}\\
    \begin{tabular}[x]{@{}X@{}} \textbf{Benign:}\\\textbf{``Sandal''}\end{tabular}&
    \includegraphics[width=\linewidth]{imgs/Orginal_norm_2.pdf}&
    \includegraphics[width=\linewidth]{imgs/GradCAM_norm_conv2d_8.pdf}&
    \includegraphics[width=\linewidth]{imgs/GradCAM_norm_conv2d_1_8.pdf}\\
    \begin{tabular}[x]{@{}X@{}} \textbf{Adversarial:}\\\textbf{``Sandal''}->\textbf{``Trouser''}\end{tabular} &
    \includegraphics[width=\linewidth]{imgs/Orginal_adv_2.pdf}&
    \includegraphics[width=\linewidth]{imgs/GradCAM_adv_layer1_PGDLinf.pdf}&
    \includegraphics[width=\linewidth]{imgs/GradCAM_adv_layer2_PGDLinf.pdf}\\
    % \multicolumn{3}{l}{} & \multicolumn{3}{l}{}\\
    % \includegraphics[width=0.16\linewidth]{imgs/Origin_PGDL1.pdf}&
    % \includegraphics[width=0.16\linewidth]{imgs/GradCAM_adv_layer1_PGDL1.pdf}&
    % \includegraphics[width=0.16\linewidth]{imgs/GradCAM_adv_layer2_PGDL1.pdf}&
    % \includegraphics[width=0.16\linewidth]{imgs/Origin_CW.pdf}&
    % \includegraphics[width=0.16\linewidth]{imgs/GradCAM_adv_layer1_CW.pdf}&
    % \includegraphics[width=0.16\linewidth]{imgs/GradCAM_adv_layer2_CW.pdf}\\
    % \multicolumn{3}{l}{\textcolor{black}{PGD ($L_{1}$):"Shirt"->"Trouser"}} & \multicolumn{3}{l}{\textcolor{black}{CW ($L_{2}$):"Coat"->"Trouser"}}\\
    \end{tabularx}
    \Description{GradCAM of Adversarial sample}
    \caption{GradCAM illustration of adversarial attacks. The malicious noise in adversarial sample (the third row) increases with the model depth, which finally causes it's misclassification to the target.}
    \label{fig: adv-gradcam}
    \vspace{-5mm}
\end{figure}


\subsection{Electromagnetic and Power Side-Channel} \label{sec: EM and power side channel attack}
\ry{Background about EM traces}
Both EM emanations and power consumption of a computer system depend on the circuit operations and data~\cite{agrawal2002side}.
% arise as a consequence of current flows within the control, I/O, data processing or other parts of a device. Similar as the power consumption of CMOS devices, it can be showed that the EM emanations is data-dependent. An attacker is typically interested in such data-dependent emanations during data processing.  In CMOS device, current only flows when the logic state of a device is changed which results in compromising emanations in unintended manner. Different active components of the device produce various type of emanations, which provide different view of current events in one clock cycle. Traditionally, researchers focus on the information leakage from EM emanations, known as EM side-channel attacks.
%For example, Kuhn \cite{kuhn2013compromising} utilizes EM radiation leakage to extract confidential data from Liquid Crystal Displays(LCDs).
Such side-channels have been extensively analyzed to retrieve the secret key of cryptographic algorithms ~\cite{kocher1999differential, chari2002template, das2019x}, and recently have been utilized to infer deep neural network model information. % studies have demonstarte that severe vulnerabilities exist in hardware implementations of neural network.
Yu \cite{yu2020deepem} proposed a SEMA to retrieve the topology of the victim model.
Batina \cite{batina2019csi} applied differential EM analysis to recover simple MLP model parameters of microcontroller implementations.
Zhang \cite{zhang2021stealing} successfully extracted the structure of a network running on an FPGA via power side-channel.
Chmielewski \cite{chmielewski2021reverse} targeted GPU DNN implementations and recovered the model structure with EM side-channel information.
All the prior work focuses on reverse engineering partial \textit{model} information, while our work associates the EM emanation patterns with input sample classes.
%but few of them take the data dependency between EM emanations and input samples into consideration. In section \ref{sec: EM traces}, we will describe our observation on EM leakage about the classiication labels and expand the usage of EM leakage.



\ry{Different methods to analyze EM power traces.}
There are multiple strategies to analyze EM/power side channel information, such as traditional statistical way and modern learning-based methods.
Statistical analysis requires alignment of the EM/power measurements with the computation processes and relies on certain power models, such as Hamming Weight and Hamming Distance model, or mutual information between distributions to discern the secret. 
When leveraging EM side-channel leakage of DNN model execution for classification and adversarial detection, the model structure is complex, the execution is computation-intensive, and the hardware platform supports highly parallel operations, and therefore learning-based methods are more suitable for coping with the misalignment, noise, and feature extraction, etc. 


\subsection{Target Platform} \label{sec: DPU}
\ry{Are we going to put the target platform part here or in the experiment part.}
We choose XilinxÂ® Deep Learning Processing Unit (DPU) as our platform.  DPU is a popular configurable hardware neural network accelerator on FPGA
and achieves the best throughput for DNN inference~\cite{9069951}.
%It is one of the fastest DNN accelerators.The primary optimization target of DPU is convolutional neural networks.
%DPU leverages parallel processing elements to accelerate heavy computations, and 
%The parallelism of DPU is configurable according to the hardware resources of the hardware platforms.
DPU supports common CNNs such as VGG\cite{simonyan2015deep}, ResNet\cite{he2015deep}, GoogLeNet\cite{szegedy2014going}, YOLO\cite{redmon2016look}, and MobileNet\cite{howard2017mobilenets}.
%Xilinx System-on-Chip (SoC), such as Zynq-7000 SoC and Zynq UltraScale+ MPSoC, contains both ARM processors and FPGAs to host DPU.
%can host DPU. These SoCs have an ARM processor and an FPGAs directly connected. The ARM processor provides interrupt services and coordinates data transfers. The DPU IP is on the FPGAs and accelerates the computations.
%DPU currently only supports 8-bit quantized neural networks.
Xilinx provides Vitis AI~\cite{BibEntry2022Jan}, a development stack to compile neural networks software trained with generic DNN platforms such as ~\cite{BibEntry2021Dec},  onto a DPU accelerator.
%Vitis AI can quantize and optimize a neural network model trained by a generic DNN software platform like TensorFlow~\cite{BibEntry2021Dec} and compile the model to the byte code that the DPU recognizes. 