\section{Experiments and Evaluations} \label{sec: experiments}
In this section, we present the experimental setup and evaluation results.

\subsection{Experiment Setup}\label{sec: setup}
\noindent\textbf{EM Trace Collection:}
The device under test (DUT) is a Xilinx DPU~\cite{ZynqDPUv92:online} running on an Ultra96-V2
board~\cite{Ultra96-V2}, a multi-processor System-on-Chip with ARM cores and Xilinx Zynq UltraScale+ FPGA.
The board runs the official PYNQ image v2.5 from the vendor AVNET
% We did not change the DPU configuration that came with the image.
\footnote{B1600, which supports up to 1600 multiplications and additions per clock cycle.}~\cite{PYNQ}.
The EM probe is PBS set 2 with a pre-amplifier~\cite{probe}.
We use a Lecroy  WR640Zi oscilloscope~\cite{LeCroy2022Jan} to collect EM traces. %, with the maximum sampling frequency as $20$GHz.
Fig.~\ref{fig: device}(a) shows a picture of our trace collection setup.
%We removed the heatsink to place the EM probe directly above SoC.
The control and monitoring workstation first sends a command via SSH to the DUT with the pre-trained DNN model (bitstream) deployed, and the DUT loads an input image and starts executing inference.
Meanwhile, the oscilloscope is triggered to capture the EM trace until the DUT finishes execution.
%To collect an EM trace of inference, we first send a command via SSH to the DUT.
%Then, the DUT loads an image and starts inferencing this image repeatedly.
%The change of the EM signal will trigger the oscilloscope to capture a trace.
Then the trace is streamed to the workstation for storage and processing.
%we retrieve the trace from the oscilloscope and save it.
%We made the trace much longer than the inference so that there is at least one complete inference in the trace.
The collected dataset of EM traces is used to train the EM classifiers and the anomaly detectors.
The training is performed on a server, with an AMD Ryzen 9 3900X 12-Core processor, 32 GB of RAM, and one Nvidia GTX TITAN GPU card.

\noindent\textbf{Datasets and Victim Models:}
We start from a LeNet-5 convolutional neural network on Fashion MNIST to evaluate our \name framework. % from Section~\ref{sec: exp: evaluate traces} to Section~\ref{sec: exp: comparison}.
%In Section~\ref{sec: exp: robust},
We also experiment with a robust LeNet-5 retrained with adversarial examples.
Furthermore, we evaluate our framework on a larger VGG model over the colored CIFAR-10 dataset. % with  in Section~\ref{sec: exp: cifar10}.
The Fashion MNIST dataset is representative of computer vision tasks suitable for edge devices such as FPGA accelerators and mobile systems.
It consists of a training set of $60,000$ examples and a test set of $10,000$ samples, which are $28\times 28$ grayscale images, labeled into $10$ classes.
The LeNet-5 CNN achieves a $91.2\%$ prediction accuracy on the dataset~\cite{CNNfmnist}.%, close to the benchmark CNN models~\cite{CNNfmnist}. % reaches an accuracy of $92\%$.
The confusion matrix of the LeNet-5 on Fashion MNIST is given in Fig.~\ref{fig: device}(b).
Note that among the ten classes, the model is more likely to misclassify Class \textit{Shirt} (the $6^{th}$ row in the confusion matrix) to other three classes, \textit{T-shirt}, \textit{Pullover}, and \textit{Coat}, due to similar features.
This lower classification rate for these classes will affect the performance of our adversarial detector accordingly, analyzed in detail in Section~\ref{sec: exp: detection performance}.
CIFAR-10 is more complex, consisting of $60,000$ $32\times 32$ color images in $10$ classes.
We adopt a larger VGG-like model and obtain a $90.5\%$ testing accuracy.
We randomly divide these datasets into a training subset ($60\%$), a validation subset ($20\%$), and a testing subset ($20\%$).
% To train the VAE anomaly detectors, we use the EM classifier outputs of the testing subset as the dataset, and split it further into training, validation, and testing subsets with the ratio of $60\%:20\%:20\%$.

% \begin{figure}[htbp]
%     \centering
%     \begin{minipage}[t]{0.43\linewidth}
%         \includegraphics[width=\linewidth]{imgs/photo.png}
%         \caption{Trace Collection Setup}
%         \label{fig: device}
%     \end{minipage}%
%         \hfill%
%     \begin{minipage}[t]{0.5\linewidth}
%         \includegraphics[width=\linewidth]{imgs/victim_confusion.pdf}
%         \caption{Confusion Matrix of LeNet-5 on Fashion MNIST}
%         \label{fig: victim-confusion}
%     \end{minipage}
%     \vspace{-0.3cm}
%     \Description{Trace collection setup and confusion matrix for FMNIST}
% \end{figure}

\begin{figure}[t]%%%F8
    \subcaptionbox{Trace Collection Setup}[.5\linewidth][c]{%
    \includegraphics[width=\linewidth]{imgs/photo.png}}\quad
    \subcaptionbox{Confusion Matrix}[.46\linewidth][c]{%
    \includegraphics[width=\linewidth]{imgs/victim_confusion-1.pdf}}
    \Description{Trace collection setup and confusion matrix of Fashion MNIST}
    \caption{Collection Setup and Victim's Confusion Matrix}
    \label{fig: device}
    \vspace{-3mm}
\end{figure}


\noindent\textbf{Adversarial Attacks:}
%We test our  detection framework for detecting the gradient-based adversarial attack during inference of the Fashion MNIST Dataset.
Our adversarial detector can detect a wide range of adversarial examples with EM emanations.
We employ three state-of-the-art adversarial attack methods discussed before in Section~\ref{sec:adversarial}: CW (targeted), PGD (targeted), and DeepFool (untargeted).
For PGD attacks, we test different distance measurements: $L_1$, $L_2$, and $L_{inf}$ to evaluate the model robustness against various distance losses.
For CW and DeepFool attacks, we test the commonly used $L_2$ measurements.
All the attack implementations are from the Foolbox library with commonly-used parameters~\cite{rauber2017foolbox}. % to generate the adversarial samples.
For targeted attacks,
%Unlike the next class and the least-like targeted attack settings~\cite{ma2019nic},
we consider a general attack model where the targeted label  (misclassification) can be any of the incorrect classes.
When evaluating the adversarial detector performance, % on different classes,
we sample the examples to different adversarial classes for both targeted and untargeted attacks. % to evaluate the detector performance on various classes.
%By default, we select the same attack parameters as~\cite{rauber2017foolbox}.
For each class, we select $9,000$ adversarial samples equally distributed among the rest $9$ source classes.
Fig.~\ref{fig: adversarial-examples} shows one image from the source class of \textit{shirt} and corresponding adversarial images generated by various attack methods to a target class of \textit{trouser}. % to fool the victim LeNet-5 CNN into classifying as \textit{trouser}.
%In Section~\ref{sec: exp: cifar10},
When evaluating CIFAR-10, we present the results of PGD L1 attacks due to the page limit.
\yf{Did you use L1 or L0?  be consistent throughout}
\ry{I use L1 attack}

\begin{figure}[t]%%%F9%
    \centering
    \begin{minipage}{\linewidth}
        \subcaptionbox*{Origin}[.16\linewidth][c]{%
        \includegraphics[width=\linewidth]{imgs/org.pdf}}
        \subcaptionbox*{PGD $L_1$}[.16\linewidth][c]{%
        \includegraphics[width=\linewidth]{imgs/targetPGDL1.pdf}}
        \subcaptionbox*{PGD $L_2$}[.16\linewidth][c]{%
        \includegraphics[width=\linewidth]{imgs/targetPGD.pdf}}
        \subcaptionbox*{PGD $L_{inf}$}[.16\linewidth][c]{%
        \includegraphics[width=\linewidth]{imgs/targetPGDLinf.pdf}}
        \subcaptionbox*{CW $L_2$}[.16\linewidth][c]{%
        \includegraphics[width=\linewidth]{imgs/targetcw.pdf}}
        \subcaptionbox*{DF $L_2$}[.16\linewidth][c]{%
        \includegraphics[width=\linewidth]{imgs/DeepFool.pdf}}
        \Description{Adversarial images of different attacks on Fashion MNIST.(\textit{T-shirt} to \textit{trouser})}
        \caption{Adversarial images (\textit{T-shirt} to \textit{trouser})}
        \label{fig: adversarial-examples}
    \end{minipage}
    \vspace{-3mm}
\end{figure}
\begin{figure}[t]%%%F10%
    \centering
    \begin{minipage}{\linewidth}
        \includegraphics[width=\linewidth]{imgs/fm-trace.pdf}
        \Description{The raw traces and filtered traces of EM signal}
        \caption{EM trace (blue) and BPF-filtered trace (yellow)}
        \label{fig: raw-trace}
    \end{minipage}
    \vspace{-3mm}
\end{figure}

\noindent\textbf{Detection Evaluation Metrics:}
% We use the testing accuracy to evaluate the EM classifiers and F-score to the entire adversarial detector.
We evaluate two main components of the detector, EM classifiers and anomaly detectors, with two metrics: testing accuracy and F1-score.
%Two DNN-based classifiers are involved in the entire adversarial detector: EM classifiers and anomaly detectors.
Note that our training is only on benign examples while the detection (inference) is on unknown benign or adversarial samples.
%The former is a multiple-class classification neural network, and the latter is an unsupervised learning model which is evaluated with both benign and adversarial samples.
In practice, the number of adversarial samples is far less than the benign ones.
Due to this imbalance, we use F1-score \cite{FScoreDe2:online}, $F_{em}$, to measure the classification performance.
\begin{equation}
    F_{em} = \frac{TP}{TP + \frac{1}{2}(FP+FN)}
    \label{eq: F1-score}
\end{equation}
where $TP$, $FP$, $FN$ are the true positive (adversarial detection rate), false positive and false negative ratio of the prediction results, respectively.
%For adversarial detection task, $TP$ is the same as adversarial detection rate. 
We plot Precision-Recall Curve (PR curve) to show the trade-off between detection precision and recall.
Just like ROC AUC~\cite{ROCAUC}, the Precision-Recall Area Under Curve (PR AUC) Score can be used for comparison between different detection settings.

\noindent\textbf{Baseline Comparison:}
\ry{In this section, we discuss about the comparison of \name with four baseline methods}
To the best of our knowledge, \name is the first hardware-based adversarial detector.  
It captures the contradiction between semantic EM signals and the victim output under a `black-box' setting.
We compare our method with four prior software-based detection methods, Kernel Density Estimation (KDE)~\cite{feinman2017detecting}, Network Invariant Checking (NIC)~\cite{ma2019nic}, Feature Squeezing (FS)~\cite{xu2017feature} and MagNet~\cite{meng2017magnet},  against PGD and CW $L_2$ adversarial samples.\yf{You shouldn't repeat explaining each of the prior work.  They should have been summarized in Section 2.2.  How is this section connected with 2.2? Are all the prior work inconsistency detection based}
\ry{I remove the detail description here, I want to emphasize the input }
% \begin{itemize}[leftmargin=*]
%     \item Kernel Density Estimation (KDE)~\cite{feinman2017detecting}. This method requires the model's intermediate outputs - activation patterns, and computes how a testing sample's probability fits the probability densities of each class.\yf{this summary stays at high level.  Verify if I rephrased correctly.} 
%     %measures the KDE requires the model's intermediate outputs.   It takes model activation patterns and fits probability densities for layers' output.
%     %By computing the probability of a sample under test, one can tell it is adversarial or not~\cite{feinman2017detecting}.
%     \item Network Invariant Checking(NIC)~\cite{ma2019nic}. NIC explores provenance channels and activation channels of DNN execution for anomaly detection. %    NIC requires the model intermediate outputs from every layer. This technique modelizes victim's execution with the provenance channel and activation pattern channel. Then the detector will train One Class SVM to distinguish adversarial samples which show different behavior in these channels~\cite{ma2019nic}.
%     \item Feature Squeezing (FS)~\cite{xu2017feature}. This method requires model inputs, and compares the model outputs for squeezed samples with the original outputs. %  FS requires the model inputs.   Then it will compare the outputs of the squeezed sample with the victim's original outputs~\cite{xu2017feature}.
%     \item MagNet~\cite{meng2017magnet}.
%     MagNet also requires model inputs. It introduces a reconstruct and reform procedure to remove the adversarial perturbation from the inputs~\cite{meng2017magnet}.
% \end{itemize}
However, these baseline methods aren't all `black-box': KDE and NIC requires the model's intermediate outputs; FS and MagNet requires testing inputs.
The metric we use for comparison is Detection Rate (DR) when the FP rate is controlled at $10\%$. 
%In Section~\ref{sec: exp: comparison}, we will compare Detection Rate (DR) of our method with that of these four state-of-the-art methods when all False Positive Rates(FPR) are controlled at $10\%$. \ad{The FAR rate later is just FPR? Would be better to use consistent terms: all change to True Positive Rate (TPR) = DR, and FPR.}



\subsection{EM Trace Processor}\label{sec: exp: evaluate traces}
\ry{In this part, I want to give a general analysis of the EM traces.}
%EM side-channel leakage of model execution has been leveraged to reverse engineer coarse-grained model information, such as the model structure (e.g., the layer type) and hyper-parameters (e.g., the layer size and kernel size), which are data independent.
% Figure~\ref{fig: raw-trace} shows an example EM trace for one benign image inference,
% where the blue curve is the original trace with sample points as $x$-axis and EM leakage intensity as the $y$-axis.
% The trace is noisy.  Once applied bandpass filtering (BPF) with the center frequency 160MHz, the yellow trace is much clear, where
% \yf{You need to add a little explanation here about the BPT.  How is the 160MHz selected? Why do you need such BPF instead of relying on the future spectrogram processing to filter noise.}
% we can see multiple leakage segments, with different patterns, and therefore can infer the structure such as the layer type: two convolutional layers and three dense layers.  This is similar to the traditional SEMA. The BPF trace just aims partitioning the raw trace to multiple segments before we apply
% STFT onto them.
\subsubsection{EM traces and segmentation}
Fig.~\ref{fig: raw-trace} shows an example EM trace for one benign image inference, where the blue curve is the original trace with sample point as the $x$-axis and EM leakage intensity as the $y$-axis.
We apply a bandpass filter (BPF) with a center frequency 150MHz to reduce the noise and obtain a clearer signal (yellow trace).
After BPF, the high-intensity segments will be clean enough and can be easily partitioned.
Among the segments in Fig.~\ref{fig: raw-trace}, the first $6$ are long segments (more than $30,000$ sample points)  and the following ones are shorter (less than $10,000$ sample points).
We infer that the longer segments come from the first two convolutional layers, which utilize more Processing Element (PE) for parallel computation.
The rest shorter segments come from the dense layers, which run faster and turn out to be less informative.
In real applications, the detector has a black-box view of the model and has no information about which layer the segment comes from, but can automatically process the trace with BPF and partitioning.
%We empirically select longer segments (say, more than $10,000$ sample points) for the following EM classifiers and anomaly detectors.

% However, our work aims to associate the EM side-channel with the input images, i.e., when the model is processing different classes of images, the EM traces are distinctively different due to the execution process difference.
% EM leakage has also been utilized by differential EM analysis (DEMA) to retrieve the secret key of ciphers or the parameters (weights and biases) of DNN models~\cite{de2005electromagnetic}, where certain time points are selected to run correlation analysis.
% Our classification problem is different from all the prior SEMA and DEMA processes, aiming to retrieve the input class by analyzing the entire trace, extracting features dispersed along the time horizon, such as the envelope of the EM trace or the power leakage points corresponding to the neuron activation pattern.


\begin{figure}[t]%%%%F11
    \subcaptionbox{Time-domain}[.45\linewidth][c]{%
    \includegraphics[width=\linewidth]{imgs/Ttest-raw-1.pdf}}\quad
    \subcaptionbox{Frequency-domain}[.45\linewidth][c]{%
    \includegraphics[width=\linewidth]{imgs/Ttest-fft-1.pdf}}\quad\\
    \subcaptionbox{Spectrogram T-scores}[\linewidth][c]{%
    \includegraphics[width=0.95\linewidth]{imgs/Ttest-stft-1.pdf}}\quad
    % \Description{The T-test result of Time domain traces and the specturm on the band around the operating frequency}
    \caption{(a) T-statistics of time-domain traces; (b) T-statistics of frequency-domain spectrums, the red line marks the operating frequency $150$MHz;
    (c)T-statistics of spectrograms, the intensity of each point represents the T value.}
    \label{fig: Ttest}
    \Description{T-test results of the trace}
    \vspace{-3mm}
\end{figure}

\ry{In this part, I want to give some insight about class information of EM traces}
\subsubsection{EM Spectrograms} \label{sec: exp: spectrogram}
Class-related features/signals in the EM traces have to be preserved to build a highly accurate EM classifier.
We show that the spectrograms generated by our STFT data processing method outperform both the time-domain traces and the simple frequency-domain spectrum (after applying the fast-fourier-transformation (FFT) on the entire time-domain EM trace).
To localize and detect class-related signals, we run the victim model on two classes of input images and collect EM traces. We applied the student T-test across the two class-datasets, on three kinds of data representations of EM traces: spectrograms, the original time-domain traces, and the frequency-domain spectrums.
%, with a Student-T test~\cite{}.\yf{is this the first place mentioning T-test? if so, citation is needed followed by a succinct summary.}
The T-test statistically tests the average difference between two groups of data.
A large absolute value of T-statistics on a point indicates that the traces of two different classes differ significantly here, thus this location contains a strong class-specific signal.
%Intuitively, the absolute value of T-statistics for two traces of two different classes should be as large as possible, indicating that their features are significantly different.



Fig.~\ref{fig: Ttest} presents the T-statistics results for Class 0 and Class 1.
%data preprocessing methods on samples from class 0 and class 1.
The T value is shown on the y-axis, while the X axis shows the time for the time-domain trace in Fig.~\ref{fig: Ttest} (a) and frequency for the frequency-domain spectrum in Fig.~\ref{fig: Ttest} (b).
For 2-D spectrogram T scores shown in Fig.~\ref{fig: Ttest} (c), the X axis is the time, Y axis is the frequency while the T value is represented by the intensity on the heatmap.
Intuitively the spectrogram depicts the time-varying spectrum, while the frequency-domain spectrum just presents average frequency components.
When comparing Fig.~\ref{fig: Ttest} (a) and (c), we can view each row of Fig.~\ref{fig: Ttest} (c) as a constituent  component of Fig.~\ref{fig: Ttest} (a).
By filtering the bottom rows and only keeping the rows with high intensity (near the top), we are filtering irrelevant noise with low T-values.
When comparing Fig.~\ref{fig: Ttest} (b) and (c), we can view each column of Fig.~\ref{fig: Ttest} (c) as a spectrum for a short time window, and the spectrum is varying along the time.
The energy (high intensity) focuses on the frequency band near the top of Fig.~\ref{fig: Ttest} (c) (i.e., the beginning frequencies of  Fig.~\ref{fig: Ttest} (b)), which is around the operating frequency of the DUT.
%We notice that both the highlighted part for the spectrogram T-statistics heatmap (rows in Figure~\ref{fig: Ttest}(c))  and the highest part of the Time-domain T-statistics curve(Figure~\ref{fig: Ttest}(a)) happens at the same period.
%And the highlighted part of the spectrogram T-statistics heatmap(columns in Figure~\ref{fig: Ttest}(c)) and the peak of spectrum T-statistics curve(Figure~\ref{fig: Ttest}(b)) are at the same frequency around the operating frequency.
% It confirms our conjecture in Section~\ref{sec: EM traces} that more information leakage comes from the computation part of the device.



Fig.~\ref{fig: Ttest} also shows the peak absolute value of T-statistics of the spectrogram is $173.14$ compared to $162.79$ and $151.64$ for the time-domain traces and frequency-domain spectrums, respectively.
We can conclude that the spectrogram contains more signals for classification than the other two forms of data.
%Also, spectrogram is more convenient for further filtering.
In our experiment, we only select $15$ frequency bands of the spectrogram around the device operating frequency and discard other bands. %, as the other frequency bands have relatively lower signals.
This bandpass filtering is effective de-noising. %We will further discuss the frequency bands selection in~\ref{sec: exp: EM classifiers}.
%Such an idea is similar to a bandpass filter for denoising. Spectrograms are more reasonable to classification.
As spectrogram preserves both the frequency and time information, its 2-D form resembles an image and suits CNN classification naturally.
%As the computation of DNN is time-variant on a specific frequency band, the spectrograms preserve both frequency and time information.
%By adopting a convolutional neural network, the spectrograms are treated as 2D inputs for the model to capture the class dependency on both the time and frequency domain.



\subsection{Evaluation of EM Classifiers} \label{sec: exp: EM classifiers}
%In Section~\ref{sec: EM traces}, we discuss that
%As the EM emanations leak the model computation and activation patterns, together indicating the class of the input image.
%Intuitively, the better we can trace EM emanations, the more efficiently the anomaly detector will detect the attacks.
%In the first part of our detector, we develop EM classifiers to extract segments of inference flows from the EM leakage of each segment of the victim model computation and activation patterns.
%We train the EM classifiers to categorize a segment of the EM trace (its processed spectrogram) to the corresponding victim model output class label (for benign examples only).
% The EM classifier is like a shadow classifier to the victim model, except for the input difference.

% To see how well the victim model inference flow can be observed by our EM classifiers, we show their classification performance in
% Table~\ref{tab: EM-classifier-results}.
Table~\ref{tab: EM-classifier-results} gives the performance of EM classifiers (we use VGG-11 models) %: F1-score and accuracy for each segment of 
for LeNet-5 on MNIST. %, and randomly split the dataset of spectrograms for training, validation, and testing with the ratio of $60\%:20\%:20\%$.
For the original raw trace in Fig.~\ref{fig: raw-trace}, we analyze the first $18$ segments corresponding to computations for the two convolutional layers and the first dense layer of the victim model,
while the last two layers leak less information (with lower intensity and shorter time).
%This is owing to the more intensive computation in the convolutional layers and a large number of neurons in the first dense layers. % while the last two dense layers are tapering off in the number of neurons.
We process each of the segments separately with STFT and build a classifier, locating the most prominent class-specific information.


%%%%\hlinewd{2pt}
\begin{table*}[t]
    \centering
    \caption{The EM classifiers' performance for each segment}
    \label{tab: EM-classifier-results}
    \begin{tabularx}{\textwidth}{cXXXX|XX|XXXXXXXX}
         \toprule
        Layer Type &  \multicolumn{6}{c}{Convolutional Layers} & \multicolumn{8}{c}{Fully-connected Layers} \\
        \hline
        Index &  \multicolumn{4}{c|}{$1^{\text{st}}$  Layer} & \multicolumn{2}{c|}{$2^{\text{nd}}$  Layer} &\multicolumn{8}{c}{$3^{\text{rd}}$  Layer}\\
        \hline
        Segment  & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12-16 & 17\\
        \hline
        Accuracy & 0.68 & 0.70 & 0.64 & 0.56 & 0.68 & 0.68 & 0.23 & 0.38 & 0.37 & 0.10 & 0.33 & 0.33 & 0.10 & 0.59 \\
        F1-score & 0.65 & 0.67 & 0.61 & 0.52 & 0.66 & 0.66 & 0.18 & 0.35 & 0.32 & 0.02 & 0.26 & 0.26 & 0.02 & 0.57\\
        \hline
    \end{tabularx}
 \end{table*}

%Among its columns, we compare the EM classification results of different segments.
%Our victim model is a 5-layer CNN with two convolutional and three fully-connected layers.
%As shown in Figure~\ref{fig: raw-trace}, the model leaks weak signals for the latter two dense layers with low leakage intensity and short periods, which we are not using as the input for EM classifiers.
%We only compute and report the EM classifiers' results of the first three layers, including two convolutional layers and one fully-connected layer.
%One possible reason is that the primary EM leakage comes from the computation of kernels for convolution layers, the high number of neurons for the first dense layer.
%In contrast, the number of neurons in the last two layers significantly decreases, leading to a drop in EM emanations.
For most of the segments, the EM classifier does extract some model execution information of that segment, where the class prediction accuracies based only on EM traces range from $23\%$ to $70\%$.
Some segments (the 12th-16th) from dense layers reveal little information, resulting in only $10\%$ accuracy (the same as a random guess among $10$ classes).
We choose to only use the strong signals from convolutional layers for adversarial detection in the next part without much degradation.
% These segments as well as segments corresponding to the last two layers are not utilized for the adversarial detection in the next part, because inclusion of them does not improve detection performance by our experiments.



For different classes, the amount of information carried in each segment also varies.
As an example, we separate the first three-segment EM classification performances by the output class labels and report them in Table~\ref{tab: resultofB0}.
%The results show that different segments emphasize different class features.
For instance, for the $8^\text{th}$ class, Segment 0 contains most information;
for Class 5, Segment 1 contains most information;
for Class 1, Segment 2.
Different segments of DNN execution focus on different semantic features.
Since the most informative feature varies from one output class to another, the most informative EM segment also varies.
%Also, for different segments corresponding to the same layer of DNN, they likely corresponds different parts of features within that layer.
%Due to constrained resources, the DPU does not perform one layer operations all at once in parallel.  Instead, it processes them in a sequential manner, i.e., enumerating parts of the input image/feature map progressively.
%For different output classes, the victim DNN activates different parts of features within a layer so that the corresponding EM segments are more informative about certain output class.
%computes the matrix multiplication and activation in a divide-and-conquer manner, which means that it will deal with different image parts one after another.
%Therefore, the EM classifiers for different segments focus on the leaked features from various image parts, leading to certain classes. %even those segments come from the same layer.
%Another reason is DNNs' features discussed in~\ref{sec: detect-adv}, which deals with different macro or micro-features layer by layer.\yf{rephrase this line}
%Therefore, the overall performance of the first layer and second layer will be different.

%%%%%\hlinewd{2pt}
\begin{table*}[t]
    \centering
       \caption{The EM classifiers' classification report for Segment 0, 1, 2}
    \label{tab: resultofB0}
    \begin{tabularx}{\textwidth}{c|XXXXXXXXXXX}
        \toprule
        \multicolumn{2}{c}{Class}  & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
        \hline
        \multirow{ 2}{*}{Segment 0}
        & Accuracy & 0.58 & 0.92 & 0.55 & 0.55 & 0.49 & 0.67 & 0.35 & 0.64 & \textbf{0.89} & 0.83 \\
        & F1-score & 0.56 & 0.81 & 0.38 & 0.65 & 0.50 & 0.63 & 0.37 & 0.72 & \textbf{0.91} & 0.77 \\
        \hline
        \multirow{ 2}{*}{Segment 1}
        & Accuracy & 0.70 & 0.81 & 0.71 & 0.69 & 0.40 & \textbf{0.85} & 0.37 & 0.91 & 0.81 & 0.85 \\
        & F1-score & 0.72 & 0.86 & 0.53 & 0.71 & 0.42 & \textbf{0.90} & 0.04 & 0.82 & 0.84 & 0.88 \\
        \hline
        \multirow{ 2}{*}{Segment 2}
        & Accuracy & 0.59 & \textbf{0.95} & 0.47 & 0.65 & 0.35 & 0.82 & 0.47 & 0.65 & 0.62 & 0.80 \\
        & F1-score & 0.68 & \textbf{0.92} & 0.53 & 0.68 & 0.40 & 0.70 & 0.08 & 0.74 & 0.61 & 0.82 \\
        \hline
    \end{tabularx}
    \vspace{-3mm}
 \end{table*}


To visualize the utilization of varying parts of EM segments, we use the GradCAM~\cite{selvaraju2016grad} on our EM classifiers.
The results are shown in Appendix~\ref{sec: gradCAM}.
For different classes, the benign inputs generally activate different neurons within a segment, and our EM classifiers capture the patterns.
When an adversarial example does not activate these neurons, it results in different output logits for the EM classifier.
%Next, for effective adversarial detection, our anomaly detector ensembles logits for EM classifiers corresponding to all segments on the trace and tries to detect deviations from the benign inference flow.


\yf{at the end of Section 4.3, you need to add a line to discuss your experiments when changing the window and stride size of STFT.  how many you have tried? What is the impact? NO figure is fine, but need to be stated}

%will generalize all segments' information in an ensemble manner to utilize all leakage .


\begin{figure}[t]%%F12
    \subcaptionbox{Target 1 VAE loss}[.40\linewidth][c]{%
    \includegraphics[width=\linewidth]{imgs/pgd1-1.pdf}}\quad
    \subcaptionbox{PR curves for classes}[.48\linewidth][c]{%
    \includegraphics[width=\linewidth]{imgs/PRcurve-1.pdf}}
    \Description{Class1 anomaly detection and PR curve}
    \caption{VAE Loss of Target 1 and PRcurves for ten targets}\yf{Class 1 means the source class or target class. be accurate!}
    \label{fig: loss and pr}
    \vspace{-3mm}
\end{figure}

\subsection{Evaluation of Anomaly Detector} \label{sec: exp: detection performance}
% \begin{table*}[t]
%     \centering
%         \caption{The Detection Rate (DR) and False Alarm Rate (FAR) of the VAE Detector}\yf{should the DR be TPR?}\ad{Ruyi: check the notation consitency: Is DR just TPR and is FAR just FPR?}
%             \vspace{-0.3cm}
%     \begin{tabularx}{\textwidth}{cc|XXXXXXXXXXX}
%         \hlinewd{2pt}
%         Attack & Metric  & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
%         \hline
%         \multirow{ 2}{*}{PGD($L_2$)}
%         & DR & 0.771 & 1.00  & 1.00 & 0.952 & 0.950 & 1.00 & 0.732 & 1.00 & 0.999 & 1.00 \\
%         & $F_1$-score ($F_a$) & 0.820 & 0.999  & 0.957 & 0.957 & 0.899 & 0.999 & 0.758 & 0.999 & 0.968 & 0.999 \\
%         \hline
%         \multirow{ 2}{*}{PGD($L_{inf}$)}
%         & DR & 0.997 & 1.00  & 0.998 & 0.998 & 0.987 & 1.00 & 0.951 & 1.00 & 0.99 & 1.00 \\
%         & $F_1$-score ($F_a$) & 0.946 & 0.999  & 0.956 & 0.981 & 0.918 & 0.999 & 0.884 & 0.999 & 0.963 & 0.999 \\
%         \hline
%         \multirow{ 2}{*}{PGD($L_1$)}
%         & DR & 1.00 & 1.00  & 1.00& 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\
%         & $F_1$-score ($F_a$) & 0.948 & 0.999  & 0.957 & 0.982 & 0.924 & 0.999 & 0.910 & 0.999 & 0.968 & 0.999\\
%         \hline
%         \multirow{ 2}{*}{CW($L_2$)}
%         & DR & 0.735 & 1.00  & 1.00 & 1.00 & 1.00 & 1.00 & 0.730 & 1.00 & 0.982 & 1.00 \\
%         & $F_1$-score ($F_a$) & 0.797 & 0.999  & 0.957  & 0.982 & 0.924 & 0.999 &0.756& 0.999& 0.958 & 0.999 \\
%         \hline
%         \multirow{ 2}{*}{DeepFool($L_2$)}
%         & DR & 0.942 & 1.00  & 0.743 & 0.963 & 0.782 & 1.00 & 0.729 & 1.00 & 1.00 & 1.00 \\
%         & $F_1$-score ($F_a$) & 0.918 & 0.999  & 0.811 & 0.963 & 0.804& 0.999& 0.756 & 0.999 & 0.968& 0.999\\
%         \hline
%         &FAR     & 0.110 & 0.001 & 0.090 & 0.038 & 0.164 & 0.001 & 0.200 & 0.001 & 0.066 & 0.001\\
%         \hline
%     \end{tabularx}
%     \label{tab: detection rate of vae}
% \end{table*}




We concatenate the logits from EM classifiers for all segments to get a logits vector reflecting the execution flow of the victim model.
The follow-on VAE extracts compressed latent features from the benign logits vectors so that the benign vectors can be reconstructed from the compressed latent features with small loss.
The adversarial examples cause different execution flows and their logits vectors can not be well reconstructed from compressed latent features.
%We concatenate the logits from EM classifiers for different segments and fit the VAE.
%We train the VAE with $80\%$ training dataset of only the benign concatenated logits traces and apply it onto testing dataset.
%So how well the VAE fits the concatenated logits can be used for anomaly detection.
%First of all,
% we fit the VAE with $80\%$ training benign logits.
Fig.~\ref{fig: loss and pr}(a) presents the testing reconstruction loss of the pre-trained VAE for both benign and adversarial samples, where the blue bars are for benign samples and the yellow bars are for adversarial examples from PGD L2 attack.  The red dash vertical line is an empirically selected threshold to determine whether the input is benign or adversarial.
It shows that two distributions are disjoint and adversarial examples can be easily distinguished from benign examples.
Fig.~\ref{fig: loss and pr}(b) shows the precision-recall curves of the VAE for all 10 classes.
Similar to the receiver operating characteristic curve, the PR curve shows the model performance trade-off between precisions and recalls.
The classification algorithm is desired to have both high precision and high recall.
Therefore, a larger Area Under Curve (AUC) indicates a better classifier.
Three classes, Class 0, 4, 6, have relatively worse performance, due to the original classification inaccuracy of the victim model among these classes.
%In conclusion, Figure~\ref{fig: Reconstruction-loss} (a) shows that our model can detect the targeted PGD L2 attack on Class 1 with a high detection rate.




In Fig.~\ref{fig: embedding}, we visualize the features of a selected adversarial sample (generated by PGD $L_2$ attack) and two benign samples (one of the source class and the other of the target class) with a 3-D embedding of their logits vectors.
% and the targeted adversarial samples.
The embedding demonstrates that the logits of the adversarial sample are different from both those of the source class and the target class, while relatively closer to the former (more different from the target).
Combined with the victim model output (misclassified to a target class), the anomaly detector finds that essentially the adversarial example bears more similarity to another class (the source class) than the predicted one, presenting a conflicted result and therefore capturing the discrepancy.

%%F13 %% F14
\aptLtoX[graphics=no, type=html]{\begin{figure}[t]
    \centering
    \begin{minipage}{.21\textwidth}
        \includegraphics[width=\linewidth]{imgs/3D-1.pdf}
        \caption{3D Embeddings}
        \label{fig: embedding}
        \Description{3D embeddings}
    \end{minipage}
\end{figure}}{}

\aptLtoX[graphics=no, type=html]{\begin{figure}[t]
    \begin{minipage}{.25\textwidth}
        \includegraphics[width=\linewidth]{imgs/PRcurve-latent-1.pdf}
        \caption{Latent PRcurves}
        \label{fig: pr latent}
        \Description{The Precision-Recall Curve of latent space}
    \end{minipage}
    \vspace{-3mm}
\end{figure}}{}

%%F13 %% F14
\aptLtoX[graphics=no, type=html]{}{\begin{figure}[t]
    \centering
    \begin{minipage}{.21\textwidth}
        \includegraphics[width=\linewidth]{imgs/3D-1.pdf}
        \caption{3D Embeddings}
        \label{fig: embedding}
        \Description{3D embeddings}
    \end{minipage}
    \begin{minipage}{.25\textwidth}
        \includegraphics[width=\linewidth]{imgs/PRcurve-latent-1.pdf}
        \caption{Latent PRcurves}
        \label{fig: pr latent}
        \Description{The Precision-Recall Curve of latent space}
    \end{minipage}
    \vspace{-3mm}
\end{figure}}

\subsection{Overhead and Delay}\label{sec: exp: overhead}
We measure the overhead and delay of the EMShepherd detection framework.
As the detector is outside of the victim model and the system, it will not affect the victim model execution at all.
It detects an adversarial example within $169$ milliseconds after the victim model finishes execution on the experimental platform.
The delay is composed of the average processing time of EM traces ($10$ ms), EM classifiers inference ($128$ ms) and anomaly detector execution ($31$ ms).
The processing time can be reduced by running the EM classifiers for different trace segments in parallel.  It can be further reduced by running the detection along with the measurements in a pipelined fashion - starting processing a segment as soon as it is measured while the victim model is still executing the next segment. %executing future steps speculatively when the detector is checking the malfunction of DNN models.


\subsection{Impact of the Detector Parameters} \label{sec: exp: ablation}
In this section, we evaluate the impact of different experimental settings on the performance of our adversarial detector. 
For instance, the trace sampling frequency and sliding window size of STFT will impact the classification accuracy of EM classifiers,
and the structure of the anomaly detector (such as the latent space size) may affect the generalizability of VAE.
Different attack methods or the distance measures used in attacks will also lead to different detection results.
%In this part, we will further study the factors related to the detection results.



\noindent\textbf{EM Trace Sampling Frequency:}
\ry{Add results about different original trace sampling rate}
The sampling frequency of EM traces has an effect on the information density for the classifiers.
We adjust the oscillator's sampling frequency from $500$MHz to $20$GHz and report the EM classification accuracy of the first segment in Table~\ref{tab: sampling frequency}.
Note that according to the Nyquist-Shannon sampling theorem, the minimum sampling frequency should be larger than $300$ MHz (two times of operating frequency).
The results show that a sampling frequency above 2GHz is sufficient to achieve good classification accuracy.\yf{I remember there is some question about the high sampling frequency.  Do you have some segment that doesn't require this high sampling frequency to get good classification accuracy?}
%According to the results, a high oscillator resolution benifits the performance of EM classifiers. However, we can always use a lower sampling frequency to reduce the complexity without too much performance degradation.

%%%%\hlinewd{2pt}
\begin{table}[ht]
    \centering
    \caption{Classification accuracy versus sampling frequency }
    \begin{tabularx}{\linewidth}{cXXXXXX}
        \toprule
        Frequency (GHz) & 0.5 & 1 & 2 & 4 & 10 & 20 \\
        \hline
       Accuracy & 0.32 & 0.32 & 0.65 & 0.67 & 0.67 & \textbf{0.68}\\
        \hline
    \end{tabularx}
    \label{tab: sampling frequency}
    \vspace{-3mm}
\end{table}


\noindent\textbf{Sliding Window Size:}
We further compare the classifiers' accuracy with different STFT window sizes.
When the Hanning window size changes, the number of bands with most signals also changes.
Table~\ref{tab: stft-config} presents the results for different STFT configurations.
As the window size changes from 64 to 1024, the classifiers' average accuracy goes up first and then goes down, with the maximum accuracy of
67\% at the window size of 128 with the top $15$ frequency bands (above the red dash line in Fig.~\ref{fig: gradCAM} (d)) in the Appendix are kept for the follow-on classifier and anomaly detector).
%With a smaller hanning window (128) and top $30$ frequency bands, the average accuracy of the classifiers go down to $0.564$.
%A larger window with a size of 512 and top $8$ bands also reduce the accuracy to $0.593$.
%More comparison between STFT configurations can be find in
%For the target system, $256$ is the best window size and the top $15$ frequency bands (above the red dash line in Fig.~\ref{fig: gradCAM} (d)) in the Appendix are kept for the follow-on classifier and anomaly detector.


%%%%\hlinewd{2pt}
\begin{table}[ht]
    \centering
      \caption{Classification accuracy vs. the sliding window size }
    \label{tab: stft-config}
    \begin{tabularx}{\linewidth}{cXXXXX}
        \toprule
        Window size & 64 & 128 & \textbf{256} & 512 & 1024\\
        \hline
        Band number & 60 & 30 & \textbf{15} & 8 & 4\\
        \hline
        Accuracy & 0.53 & 0.56 & \textbf{0.67} & 0.59 & 0.34\\
        \hline
    \end{tabularx}
    %   \vspace{-0.3cm}
\end{table}

\noindent\textbf{Latent Space Size:}
The latent space of VAE is a variable that may affect the performance of anomaly detection.
We vary the size of the latent space between 2 and 9, which reflects the model's capability to express the input data features.
The results are presented in Fig.~\ref{fig: pr latent}.
Overall the latent space size has no significant effect on anomaly detection. The space size of 6 is slightly better than others.  %By comparing the area under the PR curve, we select the latent space size of 6, which is slightly better than others.

% Next we discuss the detector robustness across various target classes, different attack methods, and different distance metric for the attack. % with F-scores and PR curves.



\noindent\textbf{Attack Methods:}
Our anomaly detector can detect adversarial samples for a wide range of existing attacks, PGD, CW, and DeepFool, with different attack distance metrics.
%In Table~\ref{tab: detection rate of vae}, we test the $L_2$ based attack on different the state of the art attack methods, such as PGD, CW, and DeepFool.
We tested five attack methods, $PGD(L_1)$, $PGD(L_2)$, $PGD(L_{inf})$, $CW(L_2)$, and $DeepFool(L_2)$, 
%Also, we compare the influence of PGD attack distance measurements.
%We test $L_1$, $L_2$, and $L_inf$ attack.
%We report the detection rate(DR) and $F_1$-score when the anomaly detector is the same for various attack methods.
and the adversarial detector all has a reasonable detection performance, shown in Table~\ref{tab: detection rate of vae}. 



Comparing the different distances used in attacks, we observe that using an EM-based detector has better performance for the $L_1$ attack, then $L_{inf}$, and the worst is $L_2$ for PGD attacks.
% One explanation is the $L_2$ and $L_{inf}$ attacks impact a large number of pixels of the benign inputs.
% The adversarial samples will have more variation in the latent space.
% Therefore, when testing adversarial attack on the neighbor classes (such as class 0, 4, 6), the performance will be affected.
% In contrast, for $L_1$ attack, there can be extreme value, which results in big changes in the inference flow at the earliest EM segments that can be detected easily.
%Thus, most victim models' activation patterns and computation features won't change much.\yf{this is not explaining well.  When the number of pixels modified are limited, the changes have to be big enough to generate adversarial examples, and therefore activate the neurons differently?  According to your logic, less number of pixel changes then less activation changes. how does that make the adversarial detection better?}
For $L_1$ attack, only a few pixels are modified to make an adversarial example from the original source-class sample, as shown in Fig.~\ref{fig: adversarial-examples} (b). Although the victim model is misled to predict it to be the target class, the EM trace of the inference bears more similarity to the original class sample, distinctly different from the EM traces of the target class samples.  Therefore, it is easier to detect the adversarial.
While for $L_{inf}$ attack, many pixels are changed, randomly distributed, easily visualized in Fig.~\ref{fig: adversarial-examples} (d).
The EM trace will differ both from that of the source class and that of the target class, still caught easily by the adversarial detector. % leaks a strong EM emanation.
%In these cases, the adversarial EM traces are always different from the target class so that the detector can identify them.
%However, the $L_2$ attack acts as a hybrid of $L_1$ and $L_{inf}$.
For $L_2$ attack, there are more pixels changed than the $L_1$ attack, but around the object in the image rather than randomly distributed like in $L_{inf}$, making the EM trace somewhat between those of the source class and target class and causing
%Its samples tend to embed between benign and target classes, which causes
the anomaly detector low confidence in making the prediction.


%%%% \hlinewd{2pt}
\begin{table*}[t]
    \centering
        \caption{F1-scores of the VAE Detector}\yf{should the DR be TPR?}\ad{Ruyi: check the notation consitency: Is DR just TPR and is FAR just FPR?}
    \begin{tabularx}{\textwidth}{X|X|X|X|X|X|X|X|X|X|X}
        \toprule
         Attack & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
        \hline
        PGD($L_2$) & 0.820 & 0.999  & 0.957 & 0.957 & 0.899 & 0.999 & 0.758 & 0.999 & 0.968 & 0.999 \\
        \hline
        PGD($L_{inf}$) & 0.946 & 0.999  & 0.956 & 0.981 & 0.918 & 0.999 & 0.884 & 0.999 & 0.963 & 0.999 \\
        \hline
        PGD($L_1$) & 0.948 & 0.999  & 0.957 & 0.982 & 0.924 & 0.999 & 0.910 & 0.999 & 0.968 & 0.999\\
        \hline
        CW($L_2$) & 0.797 & 0.999  & 0.957  & 0.982 & 0.924 & 0.999 &0.756& 0.999& 0.958 & 0.999 \\
        \hline
        DF($L_2$) & 0.918 & 0.999  & 0.811 & 0.963 & 0.804& 0.999& 0.756 & 0.999 & 0.968& 0.999\\
        \hline
    \end{tabularx}
    \label{tab: detection rate of vae}
\end{table*}

\noindent\textbf{Target Classes:}
%In Section~\ref{sec: setup}, on the Fashion MNIST dataset,  we showed that the victim CNN model
%does not perform classification well among three classes, 0, 4, and 6, due to their class similarity and the limited model complexity.
%we talk about the CNN performance on Fashion Mnist dataset, especially on class 0, 4, 6 are 'T-shirt/top', 'Coat', 'Shirt'.
%The original model does not perform well as the limitation of model complexity and the class similarity.
Fig.~\ref{fig: loss and pr}(b) shows that the prediction F1-scores of the victim model on the three classes, 0, 4, and 6, are $0.84$, $0.83$, and $0.70$, respectively, lower than other classes with scores above $0.9$. \ad{Did the Figure changed during revision? Fig.~\ref{fig: device}(b) shows the confusion matrix with no F1-scores. }
Such inaccuracy affects the performance of our adversarial detectors.
The samples from these classes, therefore, include similar computation along a large part of the victim model execution flow, making the detection hard from the EM emanations of the execution.
%The similarity of original input classes may cause the inputs to activate neurons in a similar way, which leak similar EM emanations.
%In columns of
Table~\ref{tab: detection rate of vae} also presents the detector performance on various adversarial target classes (columns) by different attack methods (rows) using F1-scores.
Particularly Class 0(T-shirt) and 6(Shirt) do not perform as well as other classes.
%Class 1, 2, 3, 5, 7, 8, 9 has relatively better performance among different categories, while class 0, 4, and 6 do not perform so well.
For other classes, our detection framework can detect close to $94\%$ of adversarial samples with less than a $10\%$ false positive rate.
\yf{\textcolor{red}{Why didn't you present individual FAR for each attack method?  Also the highest FAR excluding classes 0, 4, 6 is 9\%, not what you reported as 3.75\%?}}\ad{I assume that FAR is FPR, so same for all attack method. However, if overall FPR is 10\%, then average FAR across the 10 classes should be 10\%, but that is not the case in Table 3. Why?}



%\noindent\textbf{On Different Distance Metrics:}
\if false 
\noindent\textbf{Distance Measurements:}
Comparing the different distances used in attacks, we observe that using an EM-based detector has better performance for the $L_1$ attack, then $L_{inf}$, and the worst is $L_2$ for PGD attacks.
% One explanation is the $L_2$ and $L_{inf}$ attacks impact a large number of pixels of the benign inputs.
% The adversarial samples will have more variation in the latent space.
% Therefore, when testing adversarial attack on the neighbor classes (such as class 0, 4, 6), the performance will be affected.
% In contrast, for $L_1$ attack, there can be extreme value, which results in big changes in the inference flow at the earliest EM segments that can be detected easily.
%Thus, most victim models' activation patterns and computation features won't change much.\yf{this is not explaining well.  When the number of pixels modified are limited, the changes have to be big enough to generate adversarial examples, and therefore activate the neurons differently?  According to your logic, less number of pixel changes then less activation changes. how does that make the adversarial detection better?}
For $L_1$ attack, only a few pixels are modified to make an adversarial example from the original source-class sample, as shown in Fig.~\ref{fig: adversarial-examples} (b). Although the victim model is misled to predict it to be the target class, the EM trace of the inference bears more similarity to the original class sample, distinctly different from the EM traces of the target class samples.  Therefore, it is easier to detect the adversarial.
While for $L_{inf}$ attack, many pixels are changed, randomly distributed, easily visualized in Fig.~\ref{fig: adversarial-examples} (d).
The EM trace will differ both from that of the source class and that of the target class, still caught easily by the adversarial detector. % leaks a strong EM emanation.
%In these cases, the adversarial EM traces are always different from the target class so that the detector can identify them.
%However, the $L_2$ attack acts as a hybrid of $L_1$ and $L_{inf}$.
For $L_2$ attack, there are more pixels changed than the $L_1$ attack, but around the object in the image rather than randomly distributed like in $L_{inf}$, making the EM trace somewhat between those of the source class and target class and causing
%Its samples tend to embed between benign and target classes, which causes
the anomaly detector low confidence in making the prediction.


\ad{the above description does not seem to fit the visualization in the figure. I wrote a paragraph below.}
Figure~\ref{fig: embedding} shows the 3-D embedding of the logits traces of PGD $L_2$ attack versus benign logits traces of two classes. We can observe that the benign logits traces of two classes are well separated, indicating that the victim model inference flows are distinct for these two classes. The adversarial samples result in inference flows different from either of the benign class inference flows, and the resulting logits traces would be detected as anomaly.
\fi

\subsection{Comparison with Other Methods} \label{sec: exp: comparison}
\ry{This section focuses on Table 6, compare \name with other methods in terms of detection performance. The comparison conclusion includes 1. the reason we get a higher accuracy than others(contradiction between discriminative features and semantic features. 2. Difference between PGD and CW. 3. Finds about class.)}
Table~\ref{tab: comparison} shows our comparison between the hardware-based EMShepherd with state-of-the-art software detection methods. Note that our detector is under a stricter `black-box' scenario where only the EM traces of model execution along with the model prediction output are available. 
We control the False Positive (FP) rate under $10\%$ and evaluate the detection rates under targeted PGD $L_2$ attacks and CW $L_2$ attacks on all the  $10$ classes.
We draw three major conclusions.
\begin{itemize}[leftmargin=*]
    \item \name outperforms all baseline methods in the detection of PGD and CW attacks,
    with a $94\%$ detection rate on average.
    This demonstrates that our \name successfully captures the different computations of the model inference for benign and adversarial samples.
    \item The detection performance varies in different target classes.
    PGD attacks on Class 0, 4, and 6 cannot be easily detected, due to the relatively lower EM classification accuracies for these three classes (See Table~\ref{tab: resultofB0}). %However, the MagNet~\cite{} performs well on these three classes.\yf{add some explanation why MagNet performs well.  Add citations.}
    \item Our detector performs consistently across the two different attacks, while the performance of other methods varies significantly for the two attacks. %The detection performance on PGD adversarial samples and CW adversarial samples are different.
    PGD attacks can be effectively detected by MagNet, which utilizes only testing inputs (semantic information) from the victim inputs.
    On the other hand, NIC, which focuses on the execution flow, has a better performance on CW adversarial samples.
Our method is more general as it obtains both of such information from EM traces.\yf{rephrase the last line? be precise about semantic information? Semantic information of what? the input image??}
    %so it will be able to detect some unknown adversarial samples much easier in the future.
\end{itemize}

%%%%%  \hlinewd{2pt}
\begin{table*}[t]
    \centering
    \small
    \caption{Detection Rate(\%) when $FPR=10\%$}
    \label{tab: comparison}
    \begin{tabularx}{\linewidth}{c?llllllllll?llllllllll}
        \toprule
        \multirow{ 2}{*}{Method}
        & \multicolumn{10}{c}{PGD $L_2$ Targeted Class}
        & \multicolumn{10}{c}{CW $L_2$ Targeted Class}\\
        \cline{2-21}
        & 0 & 1 & 2 & 3 &4 & 5 & 6 & 7 & 8 & 9 & 0 & 1 & 2 & 3 &4 & 5 & 6 & 7 & 8 & 9\\
        \hline
        EM &  74.9 & \textbf{100} & \textbf{100} & \textbf{99.5} & 67.4 & \textbf{100} & 68.0 & \textbf{100} & \textbf{99.9} & \textbf{100} &
          72.4 & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & \textbf{100} & 65.6 & \textbf{100} & \textbf{100} & \textbf{100}\\
        \hline
        KDE & 57.5 & 53.8 & 51.9 & 42.4 & 50.6 & 57.3 & 50.7 & 56.3 & 52.8 & 65.2 &
        49.1 & 59.8 & 48.0 & 45.9 & 54.5 & 69.6 & 44.7 & 68.8 & 60.7 & 74.6 \\
        \hline
        NIC & 55.0 & 55.8 & 52.8 & 43.0 & 41.4 & 57.2 & 50.0 & 83.0 & 47.0 & 56.9 &
        \textbf{82.0} & 81.7 & 77.3 & 86.5 & 79.6 & 81.0 & \textbf{74.0} & 82.5 & 87.8 & 85.6\\
        \hline
        FS & 64.4 & 51.2 & 72.1 & 60.8 & 69.9 & 40.1 & 66.8 & 39.6 & 65.2 & 50.1 &
         69.5 & 62.0 & 68.0 & 62.8 & 79.0 & 66.3 & 66.8 & 73.9 & 67.4 & 69.5 \\
        \hline
        MagNet & \textbf{88.0} & 87.2& 81.2 & 88.2 & \textbf{82.0} & 84.5 & \textbf{87.4} & 87.2 & 88.1 & 85.9 &
        67.2 & 62.3 & 63.2 & 62.7 & 65.3 & 80.1 & 65.2 & 74.3 & 68.4 & 68.8\\
        \hline
    \end{tabularx}

\end{table*}


% \subsection{Comparison} \label{sec: exp: comparison}
% \ry{This section focuses on Table 6, compare \name with other methods in terms of detection performance. The comparison conclusion includes 1. the reason we get a higher accuracy than others(contradiction between discriminative features and semantic features. 2. Difference between PGD and CW. 3. Finds about class.)}
% Table~\ref{tab: comparison} shows our comparison between the hardware-based EMShepherd with state-of-the-art detection methods using software methods.
% Note that our detector is under a stricter `black-box' scenario that the detector can only touch the inference EM signals along with victim outputs.
% We control the False Postive Rate (FPR) under $10\%$ and evaluate detection rates on targeted PGD $L_2$ attack and CW $L_2$ attack on all $10$ classes.
% We can draw three major conclusions.
% \begin{itemize}[leftmargin=*]
%     \item \name outperforms all baseline methods in the detection of PGD and CW attacks,
%     with a $94\%$ detection rate on average.
%     This indicates that our \name successfully captures the computation procedure of model inference on both benign and adversarial samples.
%     \item The detection performance varies in different classes.
%     PGD attacks can't be easily detected on Class 0(T-shirt), 4(Coat), 6(Shirt),
%     which have lower EM classification accuracies (See Table~\ref{tab: resultofB0}).
%     \item The detection performance on PGD adversarial samples and CW adversarial samples are different.
%     PGD attacks can be easier detected by MagNet, which utilizes only semantic information from the victim inputs.
%     On the other hand, NIC, which focuses on the execution flow, has a better performance on CW adversarial samples.
%     Our method is more general as it obtains both of such information from EM traces,
%     so it will be able to detect some unknown adversarial samples much easier in the future.
% \end{itemize}



\yf{at the end of Section 4.4, you need to add a line to discuss your experiments when changing the latent feature space of VAE. similar to the STFT,  how many you have tried? What is the impact? NO figure is fine, but need to be stated}

%find that EM-based methods have a similar performance as the software-based methods.
%Our method beats NIC with OCSVM and is close to NIC with VAE.
%And the improvement of OCSVM to VAE does improve the final detection performance in both EM and NIC cases.



\yf{\textcolor{red}{If the (a) and (b) figures are not related, better go with different figures using the minipage setup e.g.,  Fig. 14 and Fig. 15. The colors for FSGM and Original have to be distinctly different}}

\subsection{Adversarial Detection for Robust Models} \label{sec: exp: robust}
The EMShepherd framework is model-agnostic and should also work for robust models enhanced with adversarial defense mechanisms, such as adversarial training.  
The robust model is only resilient to adversarial examples similar to the ones used in retraining, and may be circumvented by other unknown adversarial examples or
%adversarial trained model shows a strong robustness against the adversarial samples for training as well as some similar unknown attacks. However, it is still less effectiveness against some strong unknown attacks or
maliciously-designed adaptive attacks (stronger adversarial examples).  We evaluate the effectiveness of our detector on a robust model under a different adversarial attack. 
%To address this issue, we combine our detector with robust models to further detect the adversarial samples but correctly classify those invalid ones.
We train a robust LeNet-5 CNN model with benign samples and adversarial examples (with the correct labels) generated by the FGSM method.
Such a robust model is weak against the CW attack, while EMShepherd succeeds in detecting the CW adversarial examples.
We evaluate the detection performance on a testing dataset with benign, FGSM (regarded as noisy benign), and targeted CW examples, and the VAE loss distributions are presented in Fig.~\ref{fig: robust vae loss}.
Our findings are as follows: 
\begin{itemize}[leftmargin=*]
    \item \name can detect the stronger adversarial samples (yellow bars in Fig.~\ref{fig: robust vae loss}) and most of the benign examples correctly (blue and magenta bars).
    Using the threshold of 0.7, the DR of the targeted CW attack is $100\%$ when the FPR on unattacked samples (benign and FGSM) is $2.6\%$.
    \item It is noticeable that the FPR of FGSM samples is $5.1\%$.
    Note that although \name for the robust model is trained with only benign samples, it still correctly classifies FGSM samples, consistent with the robust model.
\end{itemize}

%In addition, this observation also indicates the noise resilience of EMShepherd.  Because of the small false alarm rate on FGSM nosie samples for a robust model, the system can still differentiate between adversarial examples and correctly labeled noisy samples.


\begin{figure}[t]%%F15
    \begin{minipage}{.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/robust-em-classifier-red-1.pdf}
        \caption{Robust VAE Loss}
        \label{fig: robust vae loss}
        \Description{The Anomaly detector loss of Robust Model}
    \end{minipage}
    \vspace{-3mm}
\end{figure}


\yf{\textcolor{red}{elaborate the last two lines.}} \ad{I am confused by last item. Do you mean that FGSM samples mostly give correct class labels by the robust victim model, and EMShepherd correctly identify them as benign even though only trained on traces of benign not FGSM samples? Do not see why you can FGSM samples as noise?}

\subsection{Adversarial Detection for VGG Model} \label{sec: exp: cifar10}
To show the scalability of the \name framework, we further apply it to a VGG-like model on CIFAR-10 dataset and evaluate the adversarial detection performance.
The results show that our framework can cope with large victim model execution on more complex datasets.

\begin{figure*}[t]%%F16
    \centering
    \subcaptionbox{An Example of CIFAR-10 EM traces}[.76\linewidth][c]{%
    \includegraphics[width=\linewidth]{imgs/cifar-trace-1.pdf}}\quad
    \subcaptionbox{Multiplications}[.22\linewidth][c]{%
    \includegraphics[width=\linewidth]{imgs/cifar-scatter-1.pdf}}
    \Description{An example CIFAR-10 EM trace}
    \caption{CIFAR-10 EM trace and layer operations}
    \label{fig: cifar10-trace}
    \vspace{-3mm}
\end{figure*}
\yf{\textcolor{red}{Fig.~\ref{fig: cifar10-trace}(b) figure can be represented better by a sort of correlation plot: X axis is the \# of multiplications and the Y-axis is the execution time, and you have five points in the plot with each labeled by ``layer i"}}


\noindent\textbf{EM Traces of VGG Model Execution on CIFAR-10:}
Compared with the grayscale Fashion MNIST, the CIFAR-10 dataset includes colored images used for objection detection.
The victim model and our EM trace collector both have to change accordingly.
%Below, we list the differences between the setting of CIFAR-10 and Fashion MNIST experiments.
\begin{itemize}[leftmargin=*]
    \item \textbf{Larger victim model:} The size of CIFAR-10 images is $32\times 32\times 3$, requiring more sophisticated models. Due to the limited resources on DPU, we choose a VGG-like model for implementation.
    The model includes $7$ layers: $5$ consecutive convolutional layers followed by $2$ dense layers, which achieves testing accuracy $90.5\%$ on CIFAR-10 ($93.6\%$ by the benchmark VGG-16~\cite{vgg}).
    It uses Tensorflow2 building of Ultra96 with a working frequency of $150$MHz.
    \item \textbf{Lower EM sampling frequency:} Due to the increase of execution time, the length of CIFAR-10 EM traces are longer than the Fashion MNIST one.
    Fig.~\ref{fig: cifar10-trace}(a) shows an example CIFAR-10 EM trace under a sampling frequency $1$ GHz.
    The blue part is the raw EM signal and the orange part stands for the signals after a bandpass filter at the DPU operating frequency.
    \item \textbf{Layer-wise separation:} As annotated on Fig.~\ref{fig: cifar10-trace}(a),
    the EM trace can be partitioned into $5$ convolutional layer segments (C1-C5) and $2$ dense layer segments (D1 and D2).
    We use C1-C5 for building the EM classifiers as they are for computations and easily distinguished by the data transmission segments between two computation ones.
    %the has stronger computational and long enough EM leakages, which can be splited by data transmission gaps (gaps between two segments) more clearly
    We find that the length of each EM segment (layer execution) is directly proportional to the number of multiplications in that layer, and the results are presented in
   Fig.~\ref{fig: cifar10-trace}(b), where C1-C5 are marked accordingly.
%     The red part is the hypothical multiplications numbers, and the blue ones are the EM trace lengths.
   % Due to the resolution, the CIFAR-10 can only be split layer by layer.  We show that we can also build the detector on top of the layer-wise separation.
\end{itemize}


\noindent\textbf{CIFAR-10 Layer-wise EM Classifiers:}
Table~\ref{tab: cifar-em-accuracy} shows the classification performance of layer-wise EM classifiers on CIFAR-10.
%According to the accuracy and top 2 accuracy, we have the following findings:
%\begin{itemize}[leftmargin=*]
 It shows that around half of EM traces can be correctly classified.  %    Note that the input of anomaly detector will be the entire logits, thus the second major component will also help detection.
 The reason for CIFAR-10 EM classifiers with lower prediction accuracies than Fashion MNIST ones is because of inherent characteristics of the datasets.
    CIFAR-10 image is 3-channel RGB while Fashion MNIST image is 1-channel Grayscale.  The VGG model used for CIFAR-10 is deeper and larger than LeNet-5.
    % To achieve a higher prediction accuracy, the model used for CIFAR-10 will be deeper and larger.
    The resolution of the EM traces is much lower due to the lower sampling frequency and limited storage/processing capabilities.
 %   So its traces are more coarse-grained as which is collected under a lower sampling frequency to control the length of input data.
 Comparing the different layer segments, we find that  $C_5$, the last convolutional layer has the lowest prediction accuracy.
    Because $C_5$ has a larger receptive field and a large number of kernels running in parallel, many neurons and activation are concurrent and time points on the EM traces bear low signal-to-noise ratios. %the  harder for an EM classifier to capture the leakage from any specific class.
%\end{itemize}
Note although the classifiers achieve lower accuracy than the previous Fashion MNIST cases, these classifiers
 are sufficient for the follow-on anomaly detectors to catch adversarial examples, as we have analyzed it is the deviation of classifiers' logits that is the characteristics of adversarial examples, i.e., a relative value instead of absolute accuracy.

%%%%\hlinewd{2pt}
\begin{table}[ht]
    \centering
      \caption{CIFAR-10 EM Classifiers Performance}
    \label{tab: cifar-em-accuracy}
    \begin{tabularx}{\linewidth}{cXXXXX}
         \toprule
         Convolutional layer & C1 & C2 & C3 & C4 & C5\\
        \hline
        Accuracy & 0.42 & 0.46 & 0.43 & 0.49 & 0.27\\
        \hline
    \end{tabularx}
    \vspace{-0.3cm}
\end{table}


\noindent\textbf{CIFAR-10 Anomaly Detector:}
We evaluate the performance of our anomaly detector on CIFAR-10 EM classifiers.
The experimental results show that our detection framework still achieves fairly good performance on the colored CIFAR-10 dataset.
The logits from all $5$ segments (convolutional layers) are utilized as inputs for the VAE anomaly detector, against targeted PGD attacks on CIFAR-10.
%Note that we only train the anomaly detectors with benign logits.
Fig.~\ref{fig: cifar10 vae loss} shows the VAE loss of the vectors of logits for both benign samples and adversarial examples.
With an optimal threshold selected, the detection accuracy for the adversarial examples is close to 100\% with some false positives on the benign examples. % (the loss of some benign examples is higher than the threshold - declared as adversarial by our detector).
%    \item[$\rhd$] The losses of the vectors of logits for majority of benign examples are smaller than those of the adversarial examples.  Thus, we can find an optimal threshold to distinguish benign and adversarial samples.
 Fig.~\ref{fig: cifar10 PR Curve} shows the precision-recall curves for different classes:
 %   \item[$\rhd$] Different from the result of Fashion MNIST in Figure~\ref{fig: loss and pr}(b), the targeted PGD attack on CIFAR-10 has similar performance on different targets. The EM classifiers has similar prediction among all classes of CIFAR-10, while it performs not well on class 6 of Fashion MNIST dataset.
 %   \item[$\rhd$] The anomaly detector can still differentiate adversarial samples with acceptable F1-scores.
    the best detection result is from target class 4 (deer) with the F1-score of $0.906$ and the worst one is class 7 (horse) with the F1-score of $0.821$.
%\end{itemize}

%%%%F17 %%F18
\aptLtoX[graphics=no, type=html]{\begin{figure}[t]
    \centering
    \begin{minipage}{.22\textwidth}
        \includegraphics[width=\linewidth]{imgs/cifar-vae-loss-1.pdf}
        \caption{CIFAR-10 VAE Loss}
        \label{fig: cifar10 vae loss}
        \Description{The Anomaly detector loss of CIFAR10 Dataset}
    \end{minipage}
\end{figure}}{}

\aptLtoX[graphics=no, type=html]{\begin{figure}[t]
    \begin{minipage}{.23\textwidth}
        \includegraphics[width=\linewidth]{imgs/cifar-prcurve-1.pdf}
        \caption{CIFAR PRcurves}
        \label{fig: cifar10 PR Curve}
        \Description{The Precision-Recall Curve of CIFAR10 Dataset}
    \end{minipage}
    \vspace{-3mm}
\end{figure}}{}


%%%%F17 %%F18
\aptLtoX[graphics=no, type=html]{}{\begin{figure}[t]
    \centering
    \begin{minipage}{.22\textwidth}
        \includegraphics[width=\linewidth]{imgs/cifar-vae-loss-1.pdf}
        \caption{CIFAR-10 VAE Loss}
        \label{fig: cifar10 vae loss}
        \Description{The Anomaly detector loss of CIFAR10 Dataset}
    \end{minipage}
    \begin{minipage}{.23\textwidth}
        \includegraphics[width=\linewidth]{imgs/cifar-prcurve-1.pdf}
        \caption{CIFAR PRcurves}
        \label{fig: cifar10 PR Curve}
        \Description{The Precision-Recall Curve of CIFAR10 Dataset}
    \end{minipage}
    \vspace{-3mm}
\end{figure}}
