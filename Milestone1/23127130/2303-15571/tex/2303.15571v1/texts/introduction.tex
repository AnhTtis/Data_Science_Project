\section{Introduction} \label{sec: introduction}
\ry{In this paragraph, I want to introduce the background, emphasize the harm of adversarial on machine learning models, especially on the medical applications.}
Recent advances in deep learning have revolutionized many application domains, including computer vision \cite{forsyth2011computer} and natural language processing \cite{manning1999foundations}.
% Human's daily life has benefited significantly from technological developments in autonomous driving~ \cite{bojarski2016end}, face recognition~\cite{parkhi2015deep}, machine translation~\cite{bahdanau2014neural}, medical diagnosis~\cite{foster2014machine, richens2020improving} etc.
Society has benefited significantly from technological developments in AI-empowered authentication and access control~\cite{parkhi2015deep}, medical diagnosis~\cite{foster2014machine, richens2020improving}, autonomous driving~\cite{bojarski2016end}, etc.
%Many are excited that these developments continue to expand in scope and have been implemented in scores of consumer-facing products.
%One particular progress is that the deep learning methods have been introduced to clinical medicine~\cite{foster2014machine, richens2020improving}.
%For example, in 2018, the U.S. Food and Drug Administration announced the approval of the first computer vision algorithm that can be utilized for medical diagnosis without the input of a human clinician~\cite{FDA}.
% Moreover, the worldwide epidemic caused by Covid-19 urged healthcare authorities to use non-contact wireless sensing technology with machine learning algorithms to diagnose infection remotely~\cite{saeed2022machine}.
However, deep-learning models in critical applications face serious security threats, including the most common adversarial attacks~\cite{goodfellow2014explaining}.
An adversary can manipulate the input to DNN models for inference with carefully selected perturbation to result in misclassification or misdetection.
The disastrous consequences of adversarial examples include access right escalation (e.g., to critical industrial control systems or nuclear plants),  fraudulent medical claims, and driving accidents.  
%In clinical medicine, the attacker can mislead the remote diagnostic system with infected CT photos to escape the quarantine policy or make fraudulent medical reimbursement. 


\ry{talk about existing adversarial detection and defense methods}
A large body of work has been developed for both defense and detection against adversarial attacks~\cite{li2017adversarial, bhagoji2017dimensionality, srivastava2014dropout, tao2018attacks, feinman2017detecting}. %in defending or detecting this vulnerability.
Defense techniques harden the DNN models through adversarial training~\cite{ganin2016domain} or stochastic methods~\cite{liu2018towards, wang2018defensive, wang2019protecting}. % to improve the resilience of DNN models against adversarial examples. 
However, the adversarial examples are provided by certain adversarial generation methods, and the model retrained may not be resilient to other unknown adversarial attacks, potentially stronger with different feature characteristics. 
%For instance, adversarial training expands the model training dataset with adversarial examples, which rely on certain adversarial generation methods, and the model retrained may not be resilient to other unknown adversarial attacks. 
Model retraining also has privacy implications as it requires to be iterative to keep up with new attacks~\cite{wang2019beyond}.
Furthermore, these protection methods have been circumvented recently by the most sophisticated attack~\cite{carlini2017towards}.
Another line of work is to detect the adversarial examples during model execution, which can be external to the model, therefore, more agile, general, and robust.
Existing detection methods rely on observing intermediate execution features or model behavior~\cite{xu2017feature,ma2019nic} or input statistics~\cite{ma2018characterizing, grosse2017statistical}, %or behavior~\cite{ma2019nic,xu2017feature}, 
and leveraging them for adversarial detection, a `white-box' scenario with the model internal and run-time execution details known.\yf{what is the difference between execution features or behavior? I merged}

\ry{This paragraphy is for motivation: what is a `black-box' machine learning model, key points: privacy, parameters invisible and can't be often updated.}
However, there are plenty of cases where the model users have limited access to the model intermediate parameters or testing images, which we also called it a `black-box' system.
For example, machine learning models in the healthcare system may be kept confidential due to their values and privacy concerns, 
where the model suppliers tend to offer model users only limited interfaces so as to prevent reverse engineering or membership inference attacks~\cite{shokri2017membership}.
%Besides, due to the existing limitations of the real application, it is not surprising that sometimes the weights of the model cannot be easily update,  increasing the difficulties when applying adversarial training and other defense techniques that require changing the model parameters.
\ry{Further clarify the `black-box' setting, pointing out the urgent requirements of a new detection method. Why current methods are not suitable on `black-box' setting}
Current detection strategies do not suit such ``black-box" systems, and they require direct access to the model structure and parameters, execution details such as activations, testing images, or model intermediate outputs, including model logits. %, or even updates to the model parameters . Unfortunately, most of current defense or detection strategies are not appropriate in the `black-boxed' system. Majority of model defense methods improve model robustness by retraining the model parameter when they will look into detailed model structure, gradient or existing adversarial samples.
%Relatively, model defense methods protect the privacy better when they usually do not require changing the victim models but some of them still need model intermediate outputs such as model logits.
%The adversarial detectors based on the semantic features of inputs turn out to be the most private strategies but they still look into the testing images, which hurts the data privacy under our `black-box' setting.
With privacy concerns, we target building an adversarial detector without access to both testing inputs and model execution details (i.e., feature maps). 
%Hence, we are trying to build an adversarial detector without any modification, intermediate outputs or testing inputs.


\ry{Introduce the EM analysis briefly}
Inspired by the simple Electromagnetic analysis (SEMA), which associates the EM emanation patterns of a computer platform with the operations it is running and data processing~\cite{emsca}, we leverage side-channel EM leakage for detecting adversarial examples. The intuition is that different classes of images will activate the network model differently and yield different patterns in the EM traces. 
For adversarial examples that impose perturbations on a source class and fool the victim model into misclassifying it as a target class, 
the semantic information leaked from the EM trace may present a discrepancy from that of the target class (learned with prior training), i.e., revealing an anomaly.

\ry{Detection Flow of EM side channel attack, given an example of melanoma detection.}
In this paper, we propose EMShepherd, a framework of adversarial sample detection via EM side-channel leakage, 
which treats the victim model as a `black-box' without probing it for execution details.
The victim model is deployed on a physical device that the user can access, e.g., an IoT edge device or a local inspection station.  %, such as diabetic retinopathy detection systems. 
Note remote access to DNN models in the cloud, a so-called Machine-learning-as-a-Service (MLaaS) scenario,  is out of scope for our work. 
The DNN implementation on the device can be software running on a CPU or GPU or a hardware accelerator running on FPGA, NPU, or TPU.
Fig.~\ref{fig: task explanation} shows an example setup of EMShepherd, 
where the deep learning inference system is attacked by samples with malicious perturbations, and the additional air-gapped \name fends off the adversarial sample at run-time with correct detection. 
For running the adversarial detector, neither the victim model (both static model internals and dynamic execution details) nor the inputs are needed. 
% invisible to the detection system.
Note our work uses EM side-channel as an example, while the framework is generally applicable to power side-channel as well, collected either by equipment like an oscilloscope or through on-chip power sensors~\cite{intelrapl,Lipp2021Platypus} where the measuring resolution has to be commensurate with the detection goal.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/icon.pdf}
    \caption{EMShepherd detection framework: the medical machine learning model misdiagnoses an attacked CT lung image with Covid, captured by \name}
    \Description{An example of detecting adversarial examples for AI chips with EM emanations.}
    \label{fig: task explanation}
    \vspace{-5mm}
\end{figure}

This work makes several contributions as follows:
\begin{itemize}[leftmargin=*]
    \item We leverage side-channel EM leakage, for the first time, for detecting adversarial attacks under a ``black-box" scenario. 
    We propose \name adversarial detector, which requires no prior knowledge of the victim models, adversarial attacks, intermediate execution details, and the model inputs.
   \item Our novel \name framework consists of scripts for EM measurements, a novel data processing method to tame the EM traces for follow-on class-specific feature extraction and learning, and an unsupervised anomaly detector. 
    \item We evaluate the framework on $5$ different adversarial attacks on the Fashion MNIST dataset running on a common FPGA deep learning accelerator. 
 %   The victim system is a commonly used FPGA accelerator for deep learning.
   Our results show that the EM-based detector can effectively detect all attacks %with different distance measurements
    with over $90\%$ detection accuracy and an acceptable false positive rate (less than $10\%$).
    \name is also applied on a large VGG neural network accelerator with the CIFAR-10 dataset. %, and the results show that our \name handles complex datasets with longer EM leakage as well.
    \item We further evaluate its performance on a robust retrained model with adversarial examples, and \name demonstrates high accuracy and low false positive rate as well.
    \item We compare our method with the state-of-the-art white-box software-based detection methods.
    The results show that the performance of our adversarial detector is comparable to the prior methods. % while not requiring the model's internal and execution details to be known.
\end{itemize}

