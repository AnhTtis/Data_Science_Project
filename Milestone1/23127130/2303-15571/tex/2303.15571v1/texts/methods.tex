% \begin{figure*}[t]
%     \centering
%     \subcaptionbox{benign example from class A}[.32\linewidth][c]{%
%     \includegraphics[width=\linewidth]{imgs/benign-demo.pdf}}\quad
%     \subcaptionbox{benign example from class B}[.32\linewidth][c]{%
%     \includegraphics[width=\linewidth]{imgs/benign-demo2.pdf}}\quad
%     \subcaptionbox{adversarial example from class A to B}[.32\linewidth][c]{%
%     \includegraphics[width=\linewidth]{imgs/adversarial-demo.pdf}}
%     \Description{A comparison between benign and adversarial power traces.}
%     \caption{Inference of benign examples vs. an adversarial example}
%     \label{fig: em-demo}
%     \vspace{-0.5cm}
% \end{figure*}

\section{\name Design} \label{sec: methods}

We next present the design rationales for the \name framework, the composition of the adversarial detector and salient functions.

\subsection{EM Emanations of DNN}\label{sec: EM-inspiration}
\ry{In this section, I want to mention the findings of EM signals about class, and there is semantic information in it.}
As mentioned in Section~\ref{sec: CAM}, semantic information can be used in adversarial detection, but how can we get it under a `black-box' setting?
We leverage EM side-channel leakage to characterize the semantic computational information for benign inputs.
Model inference is a highly computation-intensive task, involving multiple stages of parallel computation, making the EM signals complex and their dependency on computations hard to model. 
Learning-based methods can tackle these noisy EM signals well to extract class-specific features.
We build convolutional neural network (CNN) classifiers based on an EM dataset collected from a target system running on Fashion MNIST (with 10 classes). 
\yf{Here needs more context: What CNN? - classification? verify if the line I added is correct}
Figure~\ref{fig: em-features}(a) presents the feature space embedding of a CNN classification model on the testing EM dataset using a commonly-used visualization tool, T-distributed stochastic neighbor embedding (TSNE)~\cite{van2008visualizing}.\yf{what is the function of TSNE? when you use a tool you have to explain very high-level of the tool.} 
Figure~\ref{fig: em-features}(b) shows the confusion matrix of the CNN model prediction results.
We notice that,
\begin{itemize}[leftmargin=*]
    \item The CNN classifier can extract class-related features from the EM signals. 
    Some classes' features are distinct from others, while some overlap with others.
    \item The embeddings of the classes with similar semantic information are located near each other, 
    such as the cluster of (Sandal, Sneaker, Ankle Boot) (all shoes) and the cluster of (Shirt, Coat, Pullover) (all tops).
\end{itemize}
Based on these findings, we will discuss our design of \name, which leverages this semantic information in the EM signals with the model outputs to detect adversarial samples.
The design will also overcome the low prediction accuracy for some classes by further exploiting the feature space with anomaly detectors.

% \if false 
% \subsection{Threat Model} \label{sec: threat model}
% \ry{This section is threat model: attack is `white-box', detector is `black-box'}
% The victim is a DNN classifier, which is pre-trained with a public dataset and its testing dataset kept private.
% We assume a strong `white-box' attack model where the attacker has full knowledge of victim model and training dataset in order to generate adversarial samples with minimum perturbations. 
% On the contrary, the detection system is under an entire `black-box' view of the victim, indicating that it has no access to the victim's inputs, parameters, 
% intermediate outputs or even the testing samples.  
% The detector tries to distinguish adversarial samples with EM side-channel measurements and the victim's prediction outputs.

% \ry{In this part, we discuss more settings of the detector especially the data used in two phases.}
% In general, the detecting process can be summed up into two phases, training phase and detecting phase.
% To begin with, we train an Out-of-Distribution(OOD) detector on a public benign dataset of the same classification task, which should be distinct from the victim's training dataset.
% For each query, the detector will obtain the classification result and an EM trace along with the model execution to fit its EM classifiers and anomaly detectors.  
% During the detection phase, the victim model is in operation and under attack when the pre-trained detector decides whether the current input is adversarial or not, only based on the victim model output and its EM trace.
% \fi 


\begin{figure}[t]%% F3
    \centering
    \subcaptionbox{CNN Feature Space}[.53\linewidth][c]{%
    \includegraphics[width=\linewidth]{imgs/feature-space-representation-1.pdf}}\quad
    \subcaptionbox{Confusion Matrix}[.4\linewidth][c]{%
    \includegraphics[width=\linewidth]{imgs/confusion-matrix-heatmap-1.pdf}}\quad
    \Description{Confusion matrix on EM classifiers}
    \caption{Feature Space Representation and Confusion Matrix of EM signals}
    \label{fig: em-features}
\end{figure}


\subsection{Overview of the Detection System}\label{sec: design-overview}

% % This is the old flow-chart
% \begin{figure*}[t]
%     \centering
%     \subcaptionbox{Adversarial Detector Training Flow}[.9\linewidth][c]{%
%     \includegraphics[width=0.95\linewidth]{imgs/flow-train-new.pdf}}\quad
%     \subcaptionbox{Adversarial Detector Detection Flow}[.9\linewidth][c]{%
%     \includegraphics[width=0.95\linewidth]{imgs/flow-test-new.pdf}}
%     \Description{The Flow chart for training and detecting the framework}
%     \caption{Overview of the \name training and detection flows}
%     \label{fig: flow-chart}
%     \vspace{-0.5cm}
% \end{figure*}



\ry{In this paragraph, we will explain the main framework of the detection system as Fig.4.}
Figure~\ref{fig: flow-chart} shows an overview of the \name detection framework. 
The victim model is an image classifier running on a Xilinx DPU, and an EM trace is collected for each model execution. 
During the training phase, the model is queried with a benign training dataset and corresponding a training EM trace dataset is collected. 
These traces will be used to train EM classifiers whose outputs are utilized to fit a set of class-specific anomaly detectors.
After this phase, all the trainable components are fitted and the parameters are fixed, an \name detector is generated. 
In the detection phase, the pre-trained \name takes in the EM trace collected during an image inference, processes it, and feeds it to the follow-on EM classifiers and anomaly detector, to accurately detect adversarial examples guided by the output label from the victim classifier. 
%is applied on the EM trace collected at run-time during an image inference.  tries to distinguish a normal cat image and a dog image with adversarial perturbations as shown in Figure~\ref{fig: flow-chart}. For every sample to be tested, EM signal is collected, denoised and transformed with the atrace processor.  Then the pretrained samples to be tested will be fed to pretrained EM classifiers, which yields the class-related features. 
%These features are sent to our anomaly OOD detectors, which is selected by the victim's outputs, for final decision scores.

% After fitting all DNN models in the adversarial detector, it can be used to infer any testing examples.
% While the victim model is executing with an unknown image (maybe benign or adversarial) for prediction, the EM trace is collected simultaneously. 
% After preprocessing and feature extraction from EM traces, 
% the \name will utilize victim model's outputs (prediction labels) to select an appropriate anomaly detector for OOD detection.
% If it is an adversary, the victim model prediction result will be discarded, and further actions, e.g., more advanced and complex classification or human intervention, will be taken.

\begin{figure}%%F4
    \centering
    \includegraphics[width=0.95\linewidth]{imgs/system-overview.pdf}
    \Description{The Flow chart for training and detecting the framework}
    \caption{Overview of the \name detection flow}
    \label{fig: flow-chart}
    \vspace{-3mm}
\end{figure}

\if false
The adversarial detector is based on two DNN models: the EM classifier and the Adversarial Detector.
Shown in Figure \ref{fig: flow-chart} (a), the framework takes benign examples as a training dataset to train these DNN models.
Firstly, the EM leakage are collected and processed via the trace processor.
Then the preprocessed EM traces will train a number of EM classifiers.
Finally, the classification results will train a class-specific adversarial detector for the future detection.
When it comes to inferring an adversarial example (an adversarial image from the class Cat targeted to Dog),
the original model is fooled and predicts the image as Dog.
However, the well-trained EM classifier and the adversarial detector
will notice the attack and claim that the output should not be Dog but adversarial.

In details, our method uses the EM leakage during model execution.
It is passive side-channel information which won't impact the original model so that there will be no performance reduction for the normal execution.
Moreover, there is no direct information exchange between the original model and the detector such as weights, activation values, logits and model structures.
We can monitor the adversarial behaviours independently with normal inference.
As we only use the benign training dataset as inputs to train our technique, our method is not specific to a certain attack.
Ideally, our protection method is suitable for those packed edge devices with limited interfaces.
\fi

\subsection{Notation and Definition}
We next define notations used along with our system design. 
The victim model $\mathcal{M}_v$ is a pre-trained $N$-class classification model running on a device, facing adversarial attacks.
One input image to the victim model is denoted $Im_i \in \mathcal{I}_v$, and the corresponding output label is $y_i$.  
The corresponding EM trace for the execution collected is $\mathbf{T}_i \in \mathcal{T}_v$, each with $P$ number of points.
Every $\mathbf{T}_i$ can be partitioned into multiple computation segments $\mathbf{T}_i = concat(\{ \mathbf{B}_1, \mathbf{B}_2,..., \mathbf{B}_M\}_i)$, 
where the number of segments, $M$, depends on both the structure of $\mathcal{M}_v$ and its implementation on the victim device. Note that $M$ may be larger than the number of layers, as off-chip communications can happen within a layer due to the on-chip resource constraints.  
% Note that $M$ can be larger than the number of layers, indicating that one heavy layer computation may be executed in several sequential segments due to resource constraints and communication patterns.



EM segments are 1-D time series, and are preprocessed by Short-Time-Fourier-Transform (STFT) to generate 2-D EM spectrograms (details will be given in Section~\ref{sec: preprocessor}), 
$\mathbf{B}_m \rightarrow \mathbf{S}_m(t, f)$ in both time and frequency dimensions, where $m=1, 2, \dots, M$ denotes the segment index. 
Correspondingly one EM classifier is built on one EM segment, denoted $\mathcal{C}_m$.  
Given an input image $Im_i$, the logits of EM classifier $m$ is denoted as $\{\mathbf{L}_m\}_i$.
Then logits of all $M$ EM classifiers are concatenated into one vector $\boldsymbol{l}_i= concat(\{\mathbf{L}_m\}_i)$, 
providing a holistic view of the victim model's internal processing for input image $Im_i$.

The class-specific anomaly detectors are built for all $N$ classes, denoted $\{\mathcal{D}_n\}$, where $n=1, 2,...,N$, one for each class.
$\mathcal{D}_n$ is trained with $\boldsymbol{l}_i$ of the benign training samples with $y_i=n$, and a threshold is selected for each class.
For a testing input $Im_j$ with the prediction label $n$, the loss of the anomaly detector, denoted as  $\ell(Im_i)$, is compared with the corresponding class threshold, $L_T$, to detect whether the input is adversarial. %which is selected out of the benign training examples with $y=n$. 
More details will be illustrated in Section~\ref{sec: anomaly detection}.

\begin{figure}[t]%%%F5
    \centering
    \includegraphics[width=0.7\linewidth]{imgs/STFT-new.pdf}
    \Description{Illustration of STFT.}
    \caption{Short-Time Fourier Transform}
    \label{fig: stft}
    \vspace{-5mm}
\end{figure}

\subsection{Data Collection and Preprocessing} \label{sec: preprocessor}
Fig.~\ref{fig: flow-chart} shows that the raw EM traces, $\mathbf{T}_i \in \mathcal{T}_v$, will first go through a trace processor for denoising and transformation.
The trace processor performs two tasks:  extracting high-energy segments from raw traces and converting each EM segment into a spectrogram.
We analyze the trace profile and find that different input images will result in different amplitudes on the EM traces, but they all have the same number of segments due to the hardware accelerator structure.
We pick local maximum leakage points and split each trace into multiple segments.
Note that we do not need to align the segments in the time domain as we will use Short-Time Fourier Transformation to do time-frequency domain conversion.
Fig. \ref{fig: stft} depicts the processing method - Short-Time Fourier Transform.
A sliding Hanning window (e.g., $256$ points) with a stride of half of the window is used to transform the raw signals progressively.
Between two windows, the overlapping time points make sure that no information is lost by preserving the signals on the windows' boundaries.
In each window, we apply FFT to convert the signal from the time domain to the frequency domain to generate a spectrum.

\begin{equation}
    \mathbf{S}_m(t, f)=\int^{(t+1)w}_{(t-1)w}\mathbf{B}_m(\tau)e^{-j2\pi f\tau} d\tau
    \label{eq: stft}
\end{equation}
where $m=1, 2, \dots, M$ denotes the segment index, $w$ denotes half of the STFT window size, $t=1,2, \cdots, P_m/w$ denotes the index of time windows in a segment, and $f$ is the frequency selected.

\noindent There are three benefits of using STFT. 
\begin{itemize}[leftmargin=*]
    \item Noise-Filtering. As shown in Fig.~\ref{fig: stft} where the Y-axis of the spectrogram is the frequency and the brighter the color the higher the amplitude, the main energy (the brighter part) is focused on the operating frequency of the victim model, which is $150MHz$ in our case.  
    The rest frequencies have relatively lower energy.
    Thus, we can select $15$ bands around the operating frequency out of $256$ bands and filter out other lower-energy components, which increases the signal-noise ratio (SNR) of the remaining EM frequencies.
    \item Dimension-Reduction. The number of points in the spectrogram is reduced by $w$ times in the time domain compared to the raw EM segment, which makes the follow-on model learning capture the temporal patterns easier. 
    \item 2-D image. Compared to 1-D raw EM traces, the spectrogram naturally fits CNN classifiers and kernels, which not only provides time and frequency information but also the change of the spectrums along the time.  
    More details will be presented in Section~\ref{sec: exp: spectrogram} that the EM classifiers indeed exploit both time and frequency information in the spectrograms for classification.    
\end{itemize}


%
% \begin{figure}[ht]
%     \centering
%     \subcaptionbox{Trace Processor}[.33\linewidth][c]{%
%     \includegraphics[width=\linewidth]{imgs/trace-processor.pdf}}\quad
%     \subcaptionbox{Short-Time Fourier Transform}[.5\linewidth][c]{%
%     \includegraphics[width=\linewidth]{imgs/STFT.pdf}}
%     \Description{Illustration of STFT.}
%     \caption{Trace processor and Short-Time Fourier Transform}
%     \label{fig: stft}
%     \vspace{-0.5cm}
% \end{figure}



\subsection{EM Classifiers} \label{sec: EM classifiers}
\ry{First paragraph: generally introduce the structure of EM classifiers}
As aforementioned, the EM trace can leak class-specific computation and activation information during the inference process for an input sample $Im_i$.
We train a DNN classifier on each segment of EM spectrogram, with all benign samples in the EM training dataset, as depicted in Fig.~\ref{fig: EM classifiers}.

\ry{In the second paragraph, we further explain some features of EM classifier, different segments shows different features, so we will concatenate the logits from all segments.}
After that, we will concatenate all the EM classifiers' outputs into one logit vector for all the benign samples that belong to one class. This is based on the observation that for one input image, although all the constituent EM classifiers give out the same class prediction, their logit vectors may differ significantly, which may carry finer-grained feature/semantic information.
We use the experimental results on the Fashion MNIST dataset as an example. 
As shown in Fig.~\ref{fig: EM classifiers}, for an input image \textit{Sneaker}, the classifier on the first spectrogram gives out the correct prediction of \textit{Sneaker}, with a $0.88$ confidence score. 
However, Classifier M on the $M^{th}$ spectrogram, although also gives out the correct prediction, has much lower confidence as $0.49$. 
One explanation is that the victim model processes the semantic information layer by layer to get a final discriminative output,
where various EM segments correspond to the computation on different layers and different features.
Further, according to our experimental results, it is not necessary for every EM classifier to correctly identify the image class to be useful for the later anomaly detector.
One segment of the model's internal operations can focus on some features that are not informative enough to identify the image class, and the corresponding EM classifier may not result in a high score/logit for the class.
However, such information (this particular segment is not informative enough to distinguish image classes) can still be helpful when adversarial samples cause a different pattern, such as resulting in a high confidence score for a class instead of among the outputs of this segment EM classifier.  
It is the \textit{deviation} of the logits from benign ones that contribute to anomaly detection.
The concatenated logits vector, $\boldsymbol{l}= concat(\mathbf{L}_1, ...,\mathbf{L}_M)$, 
provides a pattern of how segments of internal model operations correlate to the class identification, 
regardless of high or low, which resembles an entire inference process across layers of the victim DNN model.

\begin{figure}[t]%%F6
    \centering
    \includegraphics[width=0.95\linewidth]{imgs/EM-classifier-new.pdf}
    \caption{EM-based classifiers and logits output}
    \Description{EM classifiers}
    \label{fig: EM classifiers}\yf{the fonts for many classes are hard to see. use the same color for the text font}
    \vspace{-5mm}
\end{figure}

\if false
We take the vector of logits of each classifier, and concatenate them into a longer vector.
%As shown in \ref{fig: EM classifiers} \circled{C} and \circled{D}, we train $M$ CNNs which takes the spectrograms from the previous steps as inputs and output a logits vector.
%The logits vector is a concatenation with the logits output from different EM classifiers.
Since each classifier  $\mathcal{C}_m$ only takes one spectrogram as input,  it tries to  predict the output class based on part of computational and activation features (e.g., of one layer).  \yf{what is the reason not to train one classifier with all batch spectrogram stitched together as the input?  For any choice you take, you need to justify it.  Also, why not do averaging across the logits instead of concatenating them together?  Concatenating logits seems weird}
Different classifiers for different segments may lead to a big difference in the logit vector even though their output classes are the same.
For instance, as Figure \ref{fig: EM classifiers} shows, for Classifier 1 the Cat class has a $0.9$ confidence score, but for Classifier M it only has $0.5$, even though both are the maximum score among all the classes.
We use the ensemble method by connecting the logits from all segments into a long vector $\boldsymbol{l}_n= concat(\mathbf{L}_1, ...,\mathbf{L}_M)$,
where $n$ denotes the original output class of the testing sample,
 and the length of $\boldsymbol{l}_n$ is $M\times N$.\yf{check notations here. you will have many $\boldsymbol{l}_n$ so it should be a set instead of a single one?}
\fi

\subsection{Anomaly Detection Models} \label{sec: anomaly detection}
\ry{In this section, we talk about VAE, the main idea we want to show is that VAE can reconstruct logits value and distinguish OOD samples based on the total loss.}
The concatenated logits vector $\boldsymbol{l}$ of benign inputs from the same class are likely to be similar.
Therefore, we can build OOD detector to distinguish adversarial samples which very likely do not fall into the target class (claimed by the victim model). We select a  reconstruct-based detector using Variational AutoEncoder (VAE).
The structure of VAE is shown in Fig.~\ref{fig: vae},
%train a VAE-based adversarial detector with the distribution of logits as the output.
with an encoder followed by a decoder, where the middle layer is the latent-space representation. 
%For each class $n$, we build one VAE which encodes the input vector of logits $\boldsymbol{l}$  to latent-space features and then rebuild it with the decoder.
The encoder and decoder of our VAE each contain four fully-connected layers.
The loss of the VAE includes a latent-space regularizer loss $\ell_{KL}$ in addition to the encoder-decoder's reconstruction loss $\ell_{recon}$.
% Different from a regular autoencoder, VAE considers a regularizer loss $\ell_{KL}$ in addition to the common reconstruction loss $\ell_{recon}$.
%The error of the decoder is noted as the reconstruction loss $\ell_{recon}^n$.
The $\ell_{KL}$ measures the Kullback-Leibler divergence of the latent space when fitted to some distribution assumption, which is a Gaussian distribution in our case.
The $\ell_{recon}$ measures the difference between the output and the input of the autoencoder. 
% The regularizer loss is defined as the Kullback-Leibler divergence  $\ell_{KL}$ loss of the latent space when fitted to a gaussian distribution.
% During the training of VAE, both the two losses % $\ell_{recon}$ and the regularizer error $\ell_{KL}$
% are taken together as the total loss to minimize based on gradient descent.
When fitting the VAE, we utilize ADAM optimizer to minimize the total loss $\ell_{total} = \ell_{recon} + \lambda \ell_{KL}$, where $\lambda$ is a constant.


The VAE total loss $ell_{total}$ during inference can be used to detect OOD samples.
When inferring benign samples, the EM classifiers' logits will match the prediction output, which fits the pre-trained VAE model with a lower loss.  
However, when the victim model is attacked by an adversarial sample that misleads the prediction to be a different target class, the EM signals will be Out-of-Distribution.
Therefore, the EM classifiers' logits will also be Out-of-Distribution leading to a higher VAE loss.
By selecting a VAE loss threshold based on the validation of benign samples, one can detect adversarial samples with a controllable false positive rate.


\begin{figure}[t]%%F7
    \includegraphics[width=0.85\linewidth]{imgs/VAE-new.pdf}
    \Description{Illustration of Variational AutoEncoder}
    \caption{The structure of anomaly detector and VAE}
    \label{fig: vae}
    \vspace{-3mm}
\end{figure}


% \subsection{Insight of EMShepherd} \label{sec: detect-adv}

% One inspiration for our \name comes from a property of deep learning models, i.e., different layers focus on different features of the image.
% For example, when predicting a class among \textit{Cat}, \textit{Dog}, or \textit{Bird}, one layer of the model may focus on the low-level features such as tail, feather and claws, 
% while other layers may extract more abstract features such as the number of legs and the shape of head.
% Based on this notion, the state-of-the-art white-box adversarial detection method~\cite{ma2019nic} leverages ``digital" provenance/activation channels for adversarial detection.

% With resource-constrained devices, neurons in each layer are not necessarily all executed concurrently but executed in batches, i.e., one-layer execution may yield multiple segments in the EM traces and the neurons' computation and activations are preserved in certain time points on the traces. 
% At the high level,  these EM segments also leak class-specific computational patterns which focus on different features. 
% Such patterns differ for benign samples and adversarial samples, which are learned by EM classifiers and the anomaly detector.
% For instance, one EM segment may correspond to the operations dealing with the feature of `tail', and another EM segment for the feature `head shape'.
% A benign \textit{Cat} image may result in the first segment EM classifier predicting $\{\textit{Cat:} 0.45, \textit{Dog:} 0.45, \textit{Bird:} 0.1\}$ because both \textit{Cat} and \textit{Dog} have tails; 
% and the second segment EM classifier predicts $\{\textit{Cat:}0.6, \textit{Dog:} 0.1, \textit{Bird:} 0.3\}$ from the shape of head.
% An adversarial \textit{Cat} (from the source class \textit{Bird}) sample may change some pixels on the head. 
% This perturbation results in the first segment EM classifier still predicting \textit{Bird} at $\{\textit{Cat:} 0.2, \textit{Dog:} 0.2, \textit{Bird:} 0.6\}$ due to no change in the tail feature, 
% but causes the second segment EM classifier to predict $\{\textit{Cat:}0.6, \textit{Dog:} 0.2, \textit{Bird:} 0.2\}$ from the modified `head shape'.
% This adversarial \textit{Cat} example is detected as the inference flow clearly differs from a benign \textit{Cat} example in the segment for `tail feature.' %(the tail feature rarely predicts a Bird for benign Cat examples but strongly suggest a Bird for this adversarial example).
% Therefore, the main idea of our adversarial detector is to inspect if the inference flow inside the DNN model execution is similar to the flow of most benign examples through the EM side-channel. 

