{
    "arxiv_id": "2303.10049",
    "paper_title": "Uncertainty-informed Mutual Learning for Joint Medical Image Classification and Segmentation",
    "authors": [
        "Kai Ren",
        "Ke Zou",
        "Xianjie Liu",
        "Yidi Chen",
        "Xuedong Yuan",
        "Xiaojing Shen",
        "Meng Wang",
        "Huazhu Fu"
    ],
    "submission_date": "2023-03-17",
    "revised_dates": [
        "2023-03-31"
    ],
    "latest_version": 3,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Classification and segmentation are crucial in medical image analysis as they enable accurate diagnosis and disease monitoring. However, current methods often prioritize the mutual learning features and shared model parameters, while neglecting the reliability of features and performances. In this paper, we propose a novel Uncertainty-informed Mutual Learning (UML) framework for reliable and interpretable medical image analysis. Our UML introduces reliability to joint classification and segmentation tasks, leveraging mutual learning with uncertainty to improve performance. To achieve this, we first use evidential deep learning to provide image-level and pixel-wise confidences. Then, an Uncertainty Navigator Decoder is constructed for better using mutual features and generating segmentation results. Besides, an Uncertainty Instructor is proposed to screen reliable masks for classification. Overall, UML could produce confidence estimation in features and performance for each link (classification and segmentation). The experiments on the public datasets demonstrate that our UML outperforms existing methods in terms of both accuracy and robustness. Our UML has the potential to explore the development of more reliable and explainable medical image analysis models. We will release the codes for reproduction after acceptance.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.10049v1",
        "http://arxiv.org/pdf/2303.10049v2",
        "http://arxiv.org/pdf/2303.10049v3"
    ],
    "publication_venue": "10 pages, 3 figures, 3 tables"
}