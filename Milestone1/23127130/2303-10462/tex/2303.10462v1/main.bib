@string{mwr = "Mon. Wea. Rev."}
@string{qjrms = "Q. J. R. Meteorol. Soc."}
@string{jgr = "J. Geophys. Res."}
@string{npg = "Nonlin. Processes Geophys."}
@string{npgd = "Nonlin. Processes Geophys. Discuss."}
@string{ijep = "Int. J. Environ. Pollut."}
@string{ip = "Inverse Problems"}
@string{jmsj = "J. Meteor. Soc. Japan"}
@string{acp = "Atmos. Chem. Phys."}
@string{acpd = "Atmos. Chem. Phys. Discuss."}
@string{atmoenv = "Atmos. Env."}
@string{jcg = "J. Comp. Geosci."}
@string{jcp = "J. Comp. Phys."}
@string{grl = "Geophys. Res. Lett."}
@string{jas = "J. Atmos. Sci."}
@string{est = "Environ. Sci. Technol."}
@string{jer = "J. Environ. Radioactivity"}
@string{gmd = "Geosci. Model Dev."}
@string{gmdd = "Geosci. Model Dev. Discuss."}
@string{pnas = "PNAS"}
@string{siamjopt = "SIAM J. Optim."}
@string{siamjsc = "SIAM J. Sci. Comput."}
@string{siamjuq = "SIAM/ASA J. Uncertainty Quantification"}
@string{siamjmaa = "SIAM J. Matrix Anal. \& Appl"}
@string{prl = "Phys. Rev. Lett."}
@string{pre = "Phys. Rev. E"}
@string{prx = "Phys. Rev. X"}
@string{ep = "Environ. Pollution"}
@string{jnrs = "J. Nuclear Radiochemical Sci."}
@string{blm = "Boundary-Layer Meteor."}
@string{ta = "Tellus A"}
@string{tb = "Tellus B"}
@string{bams = "Bull. Amer. Meteor. Soc."}
@string{jcs = "J. Comput. Sci."}
@string{fods = "Foundations of Data Science"}
@string{james = "J. Adv. Model. Earth Syst."} % Journal of Advances in Modeling Earth Systems
@string{fams = "Front. Appl. Math. Stat."}
@string{ptrsa = "Phil. Trans. R. Soc. A"}
@string{pag = "Pure Appl. Geophys"}



@article{tandeo2020review,
  title={A review of innovation-based methods to jointly estimate model and observation error covariance matrices in ensemble data assimilation},
  author={Tandeo, Pierre and Ailliot, Pierre and Bocquet, Marc and Carrassi, Alberto and Miyoshi, Takemasa and Pulido, Manuel and Zhen, Yicun},
  journal={Monthly Weather Review},
  volume={148},
  number={10},
  pages={3973--3994},
  year={2020}
}

@inproceedings{masci2011stacked,
  title={Stacked convolutional auto-encoders for hierarchical feature extraction},
  author={Masci, Jonathan and Meier, Ueli and Cire{\c{s}}an, Dan and Schmidhuber, J{\"u}rgen},
  booktitle={International conference on artificial neural networks},
  pages={52--59},
  year={2011},
  organization={Springer}
}


@article{tang2020deep,
  title={A deep-learning-based surrogate model for data assimilation in dynamic subsurface flow problems},
  author={Tang, Meng and Liu, Yimin and Durlofsky, Louis J},
  journal={Journal of Computational Physics},
  volume={413},
  pages={109456},
  year={2020},
  publisher={Elsevier}
}

@InProceedings{10.1007/978-3-031-18988-3_13,
author="Ouala, Said
and Tandeo, Pierre
and Chapron, Bertrand
and Collard, Fabrice
and Fablet, Ronan",
editor="Chapron, Bertrand
and Crisan, Dan
and Holm, Darryl
and M{\'e}min, Etienne
and Radomska, Anna",
title="End-to-End Kalman Filter in a High Dimensional Linear Embedding of the Observations",
booktitle="Stochastic Transport in Upper Ocean Dynamics",
year="2023",
publisher="Springer International Publishing",
address="Cham",
pages="211--221",
abstract="Data assimilation techniques are the state-of-the-art approaches in the reconstruction of a spatio-temporal geophysical state such as the atmosphere or the ocean. These methods rely on a numerical model that fills the spatial and temporal gaps in the observational network. Unfortunately, limitations regarding the uncertainty of the state estimate may arise when considering the restriction of the data assimilation problems to a small subset of observations, as encountered for instance in ocean surface reconstruction. These limitations motivated the exploration of reconstruction techniques that do not rely on numerical models. In this context, the increasing availability of geophysical observations and model simulations motivates the exploitation of machine learning tools to tackle the reconstruction of ocean surface variables. In this work, we formulate sea surface spatio-temporal reconstruction problems as state space Bayesian smoothing problems with unknown augmented linear dynamics. The solution of the smoothing problem, given by the Kalman smoother, is written in a differentiable framework which allows, given some training data, to optimize the parameters of the state space model.",
isbn="978-3-031-18988-3"
}


@article{casas2020,
  title={A Reduced Order Deep Data Assimilation model},
  author={Casas, C{\'e}sar Quilodr{\'a}n and Arcucci, Rossella and Wu, Pin and Pain, Christopher and Guo, Yi-Ke},
  journal={Physica D: Nonlinear Phenomena},
  volume={412},
  pages={132615},
  year={2020},
  publisher={Elsevier}
}


@article{peyron2021latent,
  title={Latent space data assimilation by using deep learning},
  author={Peyron, Mathis and Fillion, Anthony and G{\"u}rol, Selime and Marchais, Victor and Gratton, Serge and Boudier, Pierre and Goret, Gael},
  journal={Quarterly Journal of the Royal Meteorological Society},
  volume={147},
  number={740},
  pages={3759--3777},
  year={2021},
  publisher={Wiley Online Library}
}

@article{mohd2022deep,
  title={Deep Learning for Latent Space Data Assimilation in Subsurface Flow Systems},
  author={Mohd Razak, Syamil and Jahandideh, Atefeh and Djuraev, Ulugbek and Jafarpour, Behnam},
  journal={SPE Journal},
  pages={1--21},
  year={2022}
}

@article{liang2023machine,
  title={A machine learning approach to the observation operator for satellite radiance data assimilation},
  author={Liang, Jianyu and Terasaki, Koji and Miyoshi, Takemasa},
  journal={},
  year={2023},
  publisher={公益社団法人 日本気象学会}
}

@article{wang2022deep,
  title={Deep Learning Augmented Data Assimilation: Reconstructing Missing Information With Convolutional Autoencoders},
  author={Wang, Yueya and Shi, Xiaoming and Lei, Lili and Fung, Jimmy Chi-Hung},
  journal={Monthly Weather Review},
  year={2022}
}
@article{cheng2022data,
  title={Data-driven surrogate model with latent data assimilation: Application to wildfire forecasting},
  author={Cheng, Sibo and Prentice, I Colin and Huang, Yuhan and Jin, Yufang and Guo, Yi-Ke and Arcucci, Rossella},
  journal={Journal of Computational Physics},
  pages={111302},
  year={2022},
  publisher={Elsevier}
}

@article{maulik2021efficient,
  title={Efficient high-dimensional variational data assimilation with machine-learned reduced-order models},
  author={Maulik, Romit and Rao, Vishwas and Wang, Jiali and Mengaldo, Gianmarco and Constantinescu, Emil and Lusch, Bethany and Balaprakash, Prasanna and Foster, Ian and Kotamarthi, Rao},
  journal={arXiv preprint arXiv:2112.07856},
  year={2021}
}

@article{pawar2022equation,
  title={Equation-free surrogate modeling of geophysical flows at the intersection of machine learning and data assimilation},
  author={Pawar, Suraj and San, Omer},
  journal={arXiv preprint arXiv:2205.13410},
  year={2022}
}



@article{chen2022autodifferentiable,
  title={Autodifferentiable ensemble Kalman filters},
  author={Chen, Yuming and Sanz-Alonso, Daniel and Willett, Rebecca},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={4},
  number={2},
  pages={801--833},
  year={2022},
  publisher={SIAM}
}

@InProceedings{pmlr-v139-frerix21a,
  title = 	 {Variational Data Assimilation with a Learned Inverse Observation Operator},
  author =       {Frerix, Thomas and Kochkov, Dmitrii and Smith, Jamie and Cremers, Daniel and Brenner, Michael and Hoyer, Stephan},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {3449--3458},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month-dummy = 	 {18--24 Jul},
  publisher =    {PMLR},
  abstract = 	 {Variational data assimilation optimizes for an initial state of a dynamical system such that its evolution fits observational data. The physical model can subsequently be evolved into the future to make predictions. This principle is a cornerstone of large scale forecasting applications such as numerical weather prediction. As such, it is implemented in current operational systems of weather forecasting agencies across the globe. However, finding a good initial state poses a difficult optimization problem in part due to the non-invertible relationship between physical states and their corresponding observations. We learn a mapping from observational data to physical states and show how it can be used to improve optimizability. We employ this mapping in two ways: to better initialize the non-convex optimization problem, and to reformulate the objective function in better behaved physics space instead of observation space. Our experimental results for the Lorenz96 model and a two-dimensional turbulent fluid flow demonstrate that this procedure significantly improves forecast quality for chaotic systems.}
}


@article{li2022joint,
  title={Joint estimation of parameter and state with hybrid data assimilation and machine learning},
  author={Li, Xiao and Xiao, Cong and Cheng, Aijie and Lin, Haixiang},
  year={2022},
  journal={Preprint}
}

@inproceedings{liu2018deep,
  title={Deep inference for covariance estimation: Learning gaussian noise models for state estimation},
  author={Liu, Katherine and Ok, Kyel and Vega-Brown, William and Roy, Nicholas},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1436--1443},
  year={2018},
  organization={IEEE}
}

@inproceedings{nadler2019data,
  title={Data assimilation for parameter estimation in economic modelling},
  author={Nadler, Philip and Arcucci, Rossella and Guo, Yi-Ke},
  booktitle={2019 15th International Conference on Signal-Image Technology \& Internet-Based Systems (SITIS)},
  pages={649--656},
  year={2019},
  organization={IEEE}
}

@inproceedings{nadler2020neural,
  title={A neural sir model for global forecasting},
  author={Nadler, Philip and Arcucci, Rossella and Guo, Yike},
  booktitle={Machine Learning for Health},
  pages={254--266},
  year={2020},
  organization={PMLR}
}

@article{vlachas2018data,
  title={Data-driven forecasting of high-dimensional chaotic systems with long short-term memory networks},
  author={Vlachas, Pantelis R and Byeon, Wonmin and Wan, Zhong Y and Sapsis, Themistoklis P and Koumoutsakos, Petros},
  journal={Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume={474},
  number={2213},
  pages={20170844},
  year={2018},
  publisher={The Royal Society Publishing}
}

@inproceedings{vega2013cello,
  title={CELLO: A fast algorithm for covariance estimation},
  author={Vega-Brown, William and Bachrach, Abraham and Bry, Adam and Kelly, Jonathan and Roy, Nicholas},
  booktitle={2013 IEEE International Conference on Robotics and Automation},
  pages={3160--3167},
  year={2013},
  organization={IEEE}
}


@article{lguensat2017analog,
  title={The analog data assimilation},
  author={Lguensat, Redouane and Tandeo, Pierre and Ailliot, Pierre and Pulido, Manuel and Fablet, Ronan},
  journal={Monthly Weather Review},
  volume={145},
  number={10},
  pages={4093--4107},
  year={2017}
}

@article{zhen2020adaptive,
  title={An adaptive optimal interpolation based on analog forecasting: application to SSH in the Gulf of Mexico},
  author={Zhen, Yicun and Tandeo, Pierre and Leroux, St{\'e}phanie and Metref, Sammy and Penduff, Thierry and Le Sommer, Julien},
  journal={Journal of Atmospheric and Oceanic Technology},
  volume={37},
  number={9},
  pages={1697--1711},
  year={2020},
  publisher={American Meteorological Society}
}



@article{pulido2016estimation,
  title={Estimation of the functional form of subgrid-scale parametrizations using ensemble-based data assimilation: a simple model experiment},
  author={Pulido, Manuel and Scheffler, Guillermo and Ruiz, Juan Jos{\'e} and Lucini, Mar{\'\i}a Magdalena and Tandeo, Pierre},
  journal={Quarterly Journal of the Royal Meteorological Society},
  volume={142},
  number={701},
  pages={2974--2984},
  year={2016},
  publisher={Wiley Online Library}
}

@article{pulido2018stochastic,
  title={Stochastic parameterization identification using ensemble Kalman filtering combined with maximum likelihood methods},
  author={Pulido, Manuel and Tandeo, Pierre and Bocquet, Marc and Carrassi, Alberto and Lucini, Magdalena},
  journal={Tellus A: Dynamic Meteorology and Oceanography},
  volume={70},
  number={1},
  pages={1--17},
  year={2018},
  publisher={Taylor \& Francis}
}

@article{chau2022comparison,
  title={Comparison of simulation-based algorithms for parameter estimation and state reconstruction in nonlinear state-space models},
  author={Chau, Thi Tuyet Trang and Ailliot, Pierre and Monbet, Val{\'e}rie and Tandeo, Pierre},
  journal={Discrete and Continuous Dynamical Systems-Series S},
  pages={1--24},
  year={2022}
}



@article{sacco2022evaluation,
  title={Evaluation of machine learning techniques for forecast uncertainty quantification},
  author={Sacco, Maximiliano A and Ruiz, Juan J and Pulido, Manuel and Tandeo, Pierre},
  journal={Quarterly Journal of the Royal Meteorological Society},
  volume={148},
  number={749},
  pages={3470--3490},
  year={2022},
  publisher={Wiley Online Library}
}

@article{wu2021latent,
  title={A latent factor analysis-based approach to online sparse streaming feature selection},
  author={Wu, Di and He, Yi and Luo, Xin and Zhou, MengChu},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume={52},
  number={11},
  pages={6744--6758},
  year={2021},
  publisher={IEEE}
}

@Article{egusphere-2022-1316,
AUTHOR = {Tandeo, P. and Ailliot, P. and S\'evellec, F.},
TITLE = {Data-driven Reconstruction of Partially Observed Dynamical Systems},
JOURNAL = {EGUsphere},
VOLUME = {2022},
YEAR = {2022},
PAGES = {1--11},
URL = {https://egusphere.copernicus.org/preprints/egusphere-2022-1316/},
DOI = {10.5194/egusphere-2022-1316}
}

@article{cocucci2021model,
  title={Model error covariance estimation in particle and ensemble Kalman filters using an online expectation--maximization algorithm},
  author={Cocucci, Tadeo J and Pulido, Manuel and Lucini, Magdalena and Tandeo, Pierre},
  journal={Quarterly Journal of the Royal Meteorological Society},
  volume={147},
  number={734},
  pages={526--543},
  year={2021},
  publisher={Wiley Online Library}
}


@article{frangos2010surrogate,
  title={Surrogate and reduced-order modeling: a comparison of approaches for large-scale statistical inverse problems},
  author={Frangos, Michalis and Marzouk, Youssef and Willcox, Karen and van Bloemen Waanders, Bart},
  journal={Large-Scale Inverse Problems and Quantification of Uncertainty},
  pages={123--149},
  year={2010},
  publisher={Wiley Online Library}
}

@article{gong2022efficient,
  title={An efficient digital twin based on machine learning SVD autoencoder and generalised latent assimilation for nuclear reactor physics},
  author={Gong, Helin and Cheng, Sibo and Chen, Zhang and Li, Qing and Quilodr{\'a}n-Casas, C{\'e}sar and Xiao, Dunhui and Arcucci, Rossella},
  journal={Annals of Nuclear Energy},
  volume={179},
  pages={109431},
  year={2022},
  publisher={Elsevier}
}

@article{gong2022data,
  title={Data-enabled physics-informed machine learning for reduced-order modeling digital twin: application to nuclear reactor physics},
  author={Gong, Helin and Cheng, Sibo and Chen, Zhang and Li, Qing},
  journal={Nuclear Science and Engineering},
  volume={196},
  number={6},
  pages={668--693},
  year={2022},
  publisher={Taylor \& Francis}
}

@article{bai2021non,
  title={Non-intrusive nonlinear model reduction via machine learning approximations to low-dimensional operators},
  author={Bai, Zhe and Peng, Liqian},
  journal={Advanced Modeling and Simulation in Engineering Sciences},
  volume={8},
  number={1},
  pages={1--24},
  year={2021},
  publisher={Springer}
}

@phdthesis{taddei2017model,
  title={Model order reduction methods for data assimilation: state estimation and structural health monitoring},
  author={Taddei, Tommaso},
  year={2017},
  school={Massachusetts Institute of Technology}
}

@article{arcucci2021deep,
  title={Deep data assimilation: integrating deep learning with data assimilation},
  author={Arcucci, Rossella and Zhu, Jiangcheng and Hu, Shuang and Guo, Yi-Ke},
  journal={Applied Sciences},
  volume={11},
  number={3},
  pages={1114},
  year={2021},
  publisher={MDPI}
}

@article{wanders2014benefits,
  title={The benefits of using remotely sensed soil moisture in parameter identification of large-scale hydrological models},
  author={Wanders, Niko and Bierkens, Marc FP and de Jong, Steven M and de Roo, Ad and Karssenberg, Derek},
  journal={Water resources research},
  volume={50},
  number={8},
  pages={6874--6891},
  year={2014},
  publisher={Wiley Online Library}
}

@inproceedings{he2011use,
  title={Use of reduced-order models for improved data assimilation within an EnKF context},
  author={He, Jincong and Sarma, Pallav and Durlofsky, Louis J},
  booktitle={SPE Reservoir Simulation Symposium},
  year={2011},
  organization={OnePetro}
}

@article{smith2009variational,
  title={Variational data assimilation for parameter estimation: application to a simple morphodynamic model},
  author={Smith, Polly J and Dance, Sarah L and Baines, Michael J and Nichols, Nancy K and Scott, Tania R},
  journal={Ocean Dynamics},
  volume={59},
  number={5},
  pages={697--708},
  year={2009},
  publisher={Springer}
}

@article{alessandri2021parameter,
  title={Parameter estimation of fire propagation models using level set methods},
  author={Alessandri, Angelo and Bagnerini, Patrizia and Gaggero, Mauro and Mantelli, Luca},
  journal={Applied Mathematical Modelling},
  volume={92},
  pages={731--747},
  year={2021},
  publisher={Elsevier}
}

@article{lautenberger2013wildland,
  title={Wildland fire modeling with an Eulerian level set method and automated calibration},
  author={Lautenberger, Chris},
  journal={Fire Safety Journal},
  volume={62},
  pages={289--298},
  year={2013},
  publisher={Elsevier}
}

@book{albini1976estimating,
  title={Estimating wildfire behavior and effects},
  author={Albini, Frank A},
  volume={30},
  year={1976},
  publisher={Department of Agriculture, Forest Service, Intermountain Forest and Range~…}
}

@article{brown2013ecological,
  title={Ecological forecasting in Chesapeake Bay: using a mechanistic--empirical modeling approach},
  author={Brown, Christopher W and Hood, Raleigh R and Long, Wen and Jacobs, J and Ramers, DL and Wazniak, C and Wiggert, JD and Wood, R and Xu, J},
  journal={Journal of Marine Systems},
  volume={125},
  pages={113--125},
  year={2013},
  publisher={Elsevier}
}

@article{modares2010parameter,
  title={Parameter identification of chaotic dynamic systems through an improved particle swarm optimization},
  author={Modares, Hamidreza and Alfi, Alireza and Fateh, Mohammad-Mehdi},
  journal={Expert Systems with Applications},
  volume={37},
  number={5},
  pages={3714--3720},
  year={2010},
  publisher={Elsevier}
}

@article{cheng2022parameter,
  title={Parameter flexible wildfire prediction using machine learning techniques: Forward and inverse modelling},
  author={Cheng, Sibo and Jin, Yufang and Harrison, Sandy P and Quilodr{\'a}n-Casas, C{\'e}sar and Prentice, Iain Colin and Guo, Yi-Ke and Arcucci, Rossella},
  journal={Remote Sensing},
  volume={14},
  number={13},
  pages={3228},
  year={2022},
  publisher={MDPI}
}

@article{cheng2022observation,
  title={Observation error covariance specification in dynamical systems for data assimilation using recurrent neural networks},
  author={Cheng, Sibo and Qiu, Mingming},
  journal={Neural Computing and Applications},
  volume={34},
  number={16},
  pages={13149--13167},
  year={2022},
  publisher={Springer}
}

@article{janjic2018representation,
  title={On the representation error in data assimilation},
  author={Janji{\'c}, Tijana and Bormann, Niels and Bocquet, Marc and Carton, JA and Cohn, SE and Dance, Sarah L and Losa, SN and Nichols, Nancy K and Potthast, R and Waller, Joanne A and others},
  journal={Quarterly Journal of the Royal Meteorological Society},
  volume={144},
  number={713},
  pages={1257--1278},
  year={2018},
  publisher={Wiley Online Library}
}

@article{menard2016error,
  title={Error covariance estimation methods based on analysis residuals: Theoretical foundation and convergence properties derived from simplified observation networks},
  author={M{\'e}nard, Richard},
  journal={Quarterly Journal of the Royal Meteorological Society},
  volume={142},
  number={694},
  pages={257--273},
  year={2016},
  publisher={Wiley Online Library}
}

@Article{Desroziers01,
  Title                    = {Diagnosis and adaptive tuning of observation-error parameters in a variational assimilation},
  Author                   = {Desroziers, Gerald and Ivanov, Serguei},
  Journal                  = {Quarterly Journal of the Royal Meteorological Society},
  Year                     = {2001},
  Pages                    = {1433 - 1452},
  Volume                   = {127},
    Number                   = {574},
  Booktitle                = {Quarterly Journal of the Royal Meteorological Society},
  File                     = {:home/d30576/Bureau/bibliographie/Articles/Desroziers-01(2).pdf:PDF}
}


@article{zhuang2022ensemble,
  title={Ensemble latent assimilation with deep learning surrogate model: application to drop interaction in a microfluidics device},
  author={Zhuang, Yilin and Cheng, Sibo and Kovalchuk, Nina and Simmons, Mark and Matar, Omar K and Guo, Yi-Ke and Arcucci, Rossella},
  journal={Lab on a Chip},
  volume={22},
  number={17},
  pages={3187--3202},
  year={2022},
  publisher={Royal Society of Chemistry}
}

@article{liu2022enkf,
  title={EnKF data-driven reduced order assimilation system},
  author={Liu, C and Fu, R and Xiao, D and Stefanescu, R and Sharma, P and Zhu, C and Sun, S and Wang, C},
  journal={Engineering Analysis with Boundary Elements},
  volume={139},
  pages={46--55},
  year={2022},
  publisher={Elsevier}
}

@article{mack2020attention,
  title={Attention-based convolutional autoencoders for 3d-variational data assimilation},
  author={Mack, Julian and Arcucci, Rossella and Molina-Solana, Miguel and Guo, Yi-Ke},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={372},
  pages={113291},
  year={2020},
  publisher={Elsevier}
}

@misc{amendola2020,
      title={Data Assimilation in the Latent Space of a Neural Network}, 
      author={Maddalena Amendola and Rossella Arcucci and Laetitia Mottet and Cesar Quilodran Casas and Shiwei Fan and Christopher Pain and Paul Linden and Yi-Ke Guo},
      year={2020},
      eprint={2012.12056},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{storto2021neural,
  title={A Neural Network--Based Observation Operator for Coupled Ocean--Acoustic Variational Data Assimilation},
  author={Storto, Andrea and De Magistris, Giovanni and Falchetti, Silvia and Oddo, Paolo},
  journal={Monthly Weather Review},
  volume={149},
  number={6},
  pages={1967--1985},
  year={2021}
}



@article{Cheng2022JSC,
  title={Generalised latent assimilation in heterogeneous reduced spaces with machine learning surrogate models},
  author={Cheng, Sibo and Chen, Jianhua and Anastasiou, Charitos and Angeli, Panagiota and Matar, Omar K and Guo, Yi-Ke and Pain, Christopher C and Arcucci, Rossella},
  journal={Journal of Scientific Computing},
  volume={94},
  number={1},
  pages={1--37},
  year={2023},
  publisher={Springer}
}

@article{rakhimov2020latent,
  title={Latent video transformer},
  author={Rakhimov, Ruslan and Volkhonskiy, Denis and Artemov, Alexey and Zorin, Denis and Burnaev, Evgeny},
  journal={arXiv preprint arXiv:2006.10704},
  year={2020}
}

@article{tong2022probabilistic,
  title={Probabilistic Decomposition Transformer for Time Series Forecasting},
  author={Tong, Junlong and Xie, Liping and Yang, Wankou and Zhang, Kanjian},
  journal={arXiv preprint arXiv:2210.17393},
  year={2022}
}

@article{brunton2016discovering,
  title={Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
  author={Brunton, Steven L and Proctor, Joshua L and Kutz, J Nathan},
  journal={Proceedings of the national academy of sciences},
  volume={113},
  number={15},
  pages={3932--3937},
  year={2016},
  publisher={National Acad Sciences}
}

@article{kaiser2018sparse,
  title={Sparse identification of nonlinear dynamics for model predictive control in the low-data limit},
  author={Kaiser, Eurika and Kutz, J Nathan and Brunton, Steven L},
  journal={Proceedings of the Royal Society A},
  volume={474},
  number={2219},
  pages={20180335},
  year={2018},
  publisher={The Royal Society Publishing}
}

@InProceedings{paglia2022,
author="Paglia, Chiara
and Stiehl, Annika
and Uhl, Christian",
editor="Brito Palma, Lu{\'i}s
and Neves-Silva, Rui
and Gomes, Lu{\'i}s",
title="Identification of Low-Dimensional Nonlinear Dynamics from High-Dimensional Simulated and Real-World Data",
booktitle="CONTROLO 2022",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="205--213",
isbn="978-3-031-10047-5"
}

@article{cai2022online,
  title={An Online Data-Driven Method to Locate Forced Oscillation Sources from Power Plants Based on Sparse Identification of Nonlinear Dynamics (SINDy)},
  author={Cai, Yaojie and Wang, Xiaozhe and Joos, Geza and Kamwa, Innocent},
  journal={IEEE Transactions on Power Systems},
  year={2022},
  publisher={IEEE}
}

@article{champion2019data,
  title={Data-driven discovery of coordinates and governing equations},
  author={Champion, Kathleen and Lusch, Bethany and Kutz, J Nathan and Brunton, Steven L},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={45},
  pages={22445--22451},
  year={2019},
  publisher={National Acad Sciences}
}

@article{hawthorne2022general,
  title={General-purpose, long-context autoregressive modeling with Perceiver AR},
  author={Hawthorne, Curtis and Jaegle, Andrew and Cangea, C{\u{a}}t{\u{a}}lina and Borgeaud, Sebastian and Nash, Charlie and Malinowski, Mateusz and Dieleman, Sander and Vinyals, Oriol and Botvinick, Matthew and Simon, Ian and others},
  journal={arXiv preprint arXiv:2202.07765},
  year={2022}
}

@article{Kaptanoglu2022,doi = {10.21105/joss.03994},  publisher = {The Open Journal}, volume = {7}, number = {69}, pages = {3994}, author = {Alan A. Kaptanoglu and Brian M. de Silva and Urban Fasel and Kadierdan Kaheman and Andy J. Goldschmidt and Jared Callaham and Charles B. Delahunt and Zachary G. Nicolaou and Kathleen Champion and Jean-Christophe Loiseau and J. Nathan Kutz and Steven L. Brunton}, title = {PySINDy: A comprehensive Python package for robust sparse system identification}, journal = {Journal of Open Source Software},year={2021} }

@article{jaegle2021perceiver,
  title={Perceiver io: A general architecture for structured inputs \& outputs},
  author={Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and others},
  journal={arXiv preprint arXiv:2107.14795},
  year={2021}
}

@article{desroziers2005diagnosis,
  title={Diagnosis of observation, background and analysis-error statistics in observation space},
  author={Desroziers, G{\'e}rald and Berre, Loic and Chapnik, Bernard and Poli, Paul},
  journal={Quarterly Journal of the Royal Meteorological Society: A journal of the atmospheric sciences, applied meteorology and physical oceanography},
  volume={131},
  number={613},
  pages={3385--3396},
  year={2005},
  publisher={Wiley Online Library}
}

@article{berry2013adaptive,
  title={Adaptive ensemble Kalman filtering of non-linear systems},
  author={Berry, Tyrus and Sauer, Timothy},
  journal={Tellus A: Dynamic Meteorology and Oceanography},
  volume={65},
  number={1},
  pages={20331},
  year={2013},
  publisher={Taylor \& Francis}
}

@article{dreano2017estimating,
  title={Estimating model-error covariances in nonlinear state-space models using Kalman smoothing and the expectation--maximization algorithm},
  author={Dreano, Denis and Tandeo, Pierre and Pulido, Manuel and Ait-El-Fquih, Boujemaa and Chonavel, Thierry and Hoteit, Ibrahim},
  journal={Quarterly Journal of the Royal Meteorological Society},
  volume={143},
  number={705},
  pages={1877--1885},
  year={2017},
  publisher={Wiley Online Library}
}

@article{mehra1970identification,
  title={On the identification of variances and adaptive Kalman filtering},
  author={Mehra, Raman},
  journal={IEEE Transactions on automatic control},
  volume={15},
  number={2},
  pages={175--184},
  year={1970},
  publisher={IEEE}
}

@article{dee1995line,
  title={On-line estimation of error covariance parameters for atmospheric data assimilation},
  author={Dee, Dick P},
  journal={Monthly weather review},
  volume={123},
  number={4},
  pages={1128--1145},
  year={1995},
  publisher={American Meteorological Society}
}

@article{stroud2018bayesian,
  title={A Bayesian adaptive ensemble Kalman filter for sequential state and parameter estimation},
  author={Stroud, Jonathan R and Katzfuss, Matthias and Wikle, Christopher K},
  journal={Monthly weather review},
  volume={146},
  number={1},
  pages={373--386},
  year={2018}
}

@article{shumway1982approach,
  title={An approach to time series smoothing and forecasting using the EM algorithm},
  author={Shumway, Robert H and Stoffer, David S},
  journal={Journal of time series analysis},
  volume={3},
  number={4},
  pages={253--264},
  year={1982},
  publisher={Wiley Online Library}
}
@article{bannister2008review,
  title={A review of forecast error covariance statistics in atmospheric variational data assimilation. II: Modelling the forecast error covariance statistics},
  author={Bannister, Ross N},
  journal={Quarterly Journal of the Royal Meteorological Society: A journal of the atmospheric sciences, applied meteorology and physical oceanography},
  volume={134},
  number={637},
  pages={1971--1996},
  year={2008},
  publisher={Wiley Online Library}
}

@article{dong2019towards,
  title={Towards a deeper understanding of adversarial losses},
  author={Dong, Hao-Wen and Yang, Yi-Hsuan},
  journal={arXiv preprint arXiv:1901.08753},
  year={2019}
}

@article{dozat2016incorporating,
  title={{Incorporating Nesterov momentum into adam}},
  author={Dozat, Timothy},
  year={2016}
}

@article{buizza2022data,
  title={Data learning: Integrating data assimilation and machine learning},
  author={Buizza, Caterina and Casas, C{\'e}sar Quilodr{\'a}n and Nadler, Philip and Mack, Julian and Marrone, Stefano and Titus, Zainab and Le Cornec, Cl{\'e}mence and Heylen, Evelyn and Dur, Tolga and Ruiz, Luis Baca and others},
  journal={Journal of Computational Science},
  volume={58},
  pages={101525},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16000--16009},
  year={2022}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@misc{lever2017points,
  title={{Points of significance: Principal component analysis}},
  author={Lever, Jake and Krzywinski, Martin and Altman, Naomi},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{makhzani2015adversarial,
  title={Adversarial autoencoders},
  author={Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  journal={arXiv preprint arXiv:1511.05644},
  year={2015}
}

@article{quilodran2021adversarially,
  title={{Adversarially trained LSTMs on reduced order models of urban air pollution simulations}},
  author={Quilodr{\'a}n-Casas, C{\'e}sar and Arcucci, Rossella and Pain, Christopher and Guo, Yike},
  journal={arXiv preprint arXiv:2101.01568},
  year={2021}
}

@inproceedings{reddy2019reduced,
  title={Reduced Order Model for Unsteady Fluid Flows via Recurrent Neural Networks},
  author={Reddy, Sandeep B and Magee, Allan Ross and Jaiman, Rajeev K and Liu, J and Xu, W and Choudhary, A and Hussain, AA},
  booktitle={International Conference on Offshore Mechanics and Arctic Engineering},
  volume={58776},
  pages={V002T08A007},
  year={2019},
  organization={American Society of Mechanical Engineers}
}

@article{reddy2020deep,
  title={{Deep Convolutional Recurrent Autoencoders for Flow Field Prediction}},
  author={Reddy Bukka, Sandeep and Magee, Allan Ross and Jaiman, Rajeev Kumar},
  journal={arXiv},
  pages={arXiv--2003},
  year={2020}
}

@article{wang2019improving,
  title={Improving neural language modeling via adversarial training},
  author={Wang, Dilin and Gong, Chengyue and Liu, Qiang},
  journal={arXiv preprint arXiv:1906.03805},
  year={2019}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}


@article{reddy2020deep,
  title={Deep Convolutional Recurrent Autoencoders for Flow Field Prediction},
  author={Reddy Bukka, Sandeep and Magee, Allan Ross and Jaiman, Rajeev Kumar},
  journal={arXiv},
  pages={arXiv--2003},
  year={2020}
}

@article{song2018natural,
  title={Natural ventilation in cities: the implications of fluid mechanics},
  author={Song, Jiyun and Fan, S and Lin, William and Mottet, L and Woodward, H and Davies Wykes, M and Arcucci, R and Xiao, D and Debay, J-E and ApSimon, H and others},
  journal={Building Research \& Information},
  volume={46},
  number={8},
  pages={809--828},
  year={2018},
  publisher={Taylor \& Francis}
}

@book{asch2016data,
  title={Data assimilation: methods, algorithms, and applications},
  author={Asch, Mark and Bocquet, Marc and Nodet, Ma{\"e}lle},
  volume={11},
  year={2016},
  publisher={SIAM}
}

@Article{Engl1996,
	author  = {H. K. Engl and M. Hanke and A. Neubauer},
	title   = {{Regularization of Inverse Problems}},
	journal = {Kluwer},
	year    = {1996}
}

@Book{Nichols2010,
	author    = {N. Nichols},
	title     = {{Data Assimilation - Chapter Mathematical concepts in data assimilation}},
	publisher = {Springer},
	pages     = {13-39},
	year      = {2010}
}

@Book{Hansen1998,
	author  = {P.C. Hansen},
	title   = {{Rank Deficient and Discrete Ill-Posed Problems}},
    publisher = {Society for Industrial and Applied Mathematics},
	year    = {1998}
}

@article{hannachi2007empirical,
    title   = {{Empirical orthogonal functions and related techniques in atmospheric science: A review}},
    author  = {A. Hannachi and I.T. Jolliffe and D.B. Stephenson},
    journal = {International Journal of Climatology: A Journal of the Royal Meteorological Society},
    volume  = {27},
    pages   = {1119-1152},
    year    = {2007}
}

@misc{lorenz1956empirical,
    title     = {{Empirical orthogonal functions and statistical weather prediction.}},
    author    = {E.N. Lorenz},
    year      = {1956},
    publisher = {Massachusetts Institute of Technology, Department of Meteorology Cambridge}
}

@article{arcucci2019optimal,
  title={Optimal reduced space for Variational Data Assimilation},
  author={Arcucci, Rossella and Mottet, Laetitia and Pain, Christopher and Guo, Yi-Ke},
  journal={Journal of Computational Physics},
  volume={379},
  pages={51--69},
  year={2019},
  publisher={Elsevier}
}

@article{arcucci2019deep,
  title={{Deep Data Assimilation (DDA): Integrating deep learning with data assimilation}},
  author={Arcucci, Rossella and Jiangcheng Zhu and Shuang Hu and Guo, Yi-Ke},
  journal={Journal of Computational Physics},
  volume={379},
  pages={51--69},
  year={2019},
  publisher={Elsevier}
}

@article{greff2016lstm,
  title={LSTM: A search space odyssey},
  author={Greff, Klaus and Srivastava, Rupesh K and Koutn{\'\i}k, Jan and Steunebrink, Bas R and Schmidhuber, J{\"u}rgen},
  journal={IEEE transactions on neural networks and learning systems},
  volume={28},
  number={10},
  pages={2222--2232},
  year={2016},
  publisher={IEEE}
}

@inproceedings{xingjian2015convolutional,
  title={Convolutional LSTM network: A machine learning approach for precipitation nowcasting},
  author={Xingjian, SHI and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-Kin and Woo, Wang-chun},
  booktitle={Advances in neural information processing systems},
  pages={802--810},
  year={2015}
}

@article{gers1999learning,
  title={Learning to forget: Continual prediction with LSTM},
  author={Gers, Felix A and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  year={1999},
  publisher={IET}
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}

@article{dawson2016eofs,
  title={eofs: A library for eof analysis of meteorological, oceanographic, and climate data},
  author={Dawson, Andrew},
  journal={Journal of Open Research Software},
  volume={4},
  number={1},
  year={2016},
  publisher={Ubiquity Press}
}

@article{leeds2014emulator,
  title={Emulator-assisted reduced-rank ecological data assimilation for nonlinear multivariate dynamical spatio-temporal processes},
  author={Leeds, William B and Wikle, Christopher K and Fiechter, Jerome},
  journal={Statistical Methodology},
  volume={17},
  pages={126--138},
  year={2014},
  publisher={Elsevier}
}

@article{margvelashvili2016emulator,
  title={Emulator-assisted data assimilation in complex models},
  author={Margvelashvili, Nugzar Yu and Herzfeld, Mike and Rizwi, Farhan and Mongin, Mathieu and Baird, Mark E and Jones, Emlyn and Schaffelke, Britta and King, Edward and Schroeder, Thomas},
  journal={Ocean Dynamics},
  volume={66},
  number={9},
  pages={1109--1124},
  year={2016},
  publisher={Springer}
}

@article{lins2013prediction,
  title={Prediction of sea surface temperature in the tropical Atlantic by support vector machines},
  author={Lins, Isis Didier and Araujo, Moacyr and das Chagas Moura, M{\'a}rcio and Silva, Marcus Andr{\'e} and Droguett, Enrique L{\'o}pez},
  journal={Computational Statistics \& Data Analysis},
  volume={61},
  pages={187--198},
  year={2013},
  publisher={Elsevier}
}

@article{mcdonnell2004poisson,
  title={A Poisson regression model of tropical cyclogenesis for the Australian--southwest Pacific Ocean region},
  author={McDonnell, Katrina A and Holbrook, Neil J},
  journal={Weather and Forecasting},
  volume={19},
  number={2},
  pages={440--455},
  year={2004}
}

@article{bakun2003environmental,
  title={Environmental ‘loopholes’ and fish population dynamics: comparative pattern recognition with focus on El Nino effects in the Pacific},
  author={Bakun, Andrew and Broad, Kenneth},
  journal={Fisheries Oceanography},
  volume={12},
  number={4-5},
  pages={458--473},
  year={2003},
  publisher={Wiley Online Library}
}

@article{du2016ensemble,
  title={Ensemble data assimilation applied to an adaptive mesh ocean model},
  author={Du, Juan and Zhu, Jiang and Fang, Fangxin and Pain, CC and Navon, IM},
  journal={International Journal for Numerical Methods in Fluids},
  volume={82},
  number={12},
  pages={997--1009},
  year={2016},
  publisher={Wiley Online Library}
}

@article{hotelling1933analysis,
  title={Analysis of a complex of statistical variables into principal components.},
  author={Hotelling, Harold},
  journal={Journal of educational psychology},
  volume={24},
  number={6},
  pages={417},
  year={1933},
  publisher={Warwick \& York}
}


@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={807--814},
  year={2010}
}

@phdthesis{quilodran2018fast,
  title={{Fast ocean data assimilation and forecasting using a neural-network reduced-space regional ocean model of the north Brazil current}},
  author={Quilodr\'an Casas, César},
  year={2018},
  school={Imperial College London}
}


@article{davies2011fluidity,
  title={Fluidity: A fully unstructured anisotropic adaptive mesh computational modeling framework for geodynamics},
  author={Davies, D Rhodri and Wilson, Cian R and Kramer, Stephan C},
  journal={Geochemistry, Geophysics, Geosystems},
  volume={12},
  number={6},
  year={2011},
  publisher={Wiley Online Library}
}

@article{zhu1997algorithm,
  title={Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization},
  author={Zhu, Ciyou and Byrd, Richard H and Lu, Peihuang and Nocedal, Jorge},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={23},
  number={4},
  pages={550--560},
  year={1997},
  publisher={ACM}
}

@book{cacuci2016computational,
  title={Computational methods for data evaluation and assimilation},
  author={Cacuci, Dan Gabriel and Navon, Ionel Michael and Ionescu-Bujor, Mihaela},
  year={2016},
  publisher={Chapman and Hall/CRC}
}

@book{kalnay2003atmospheric,
  title={Atmospheric modeling, data assimilation and predictability},
  author={Kalnay, Eugenia},
  year={2003},
  publisher={Cambridge university press}
}




@ARTICLE{Kalman1960,
	author = {Kalman, R.E.},
	title = {A new approach to linear filtering and prediction problems.},
	journal = {Trans. ASME J. Basic Eng.},
	year = {1960},
	volume = {82(Series D)},
	pages = {35-45}
}



@BOOK{Smith:2012qr,
	title = {{B}ook {T}itle},
	publisher = {Publisher},
	author = {Smith, J.~M. and Jones, A.~B.},
	year = {2012},
	edition = {7th},
}

@BOOK{NEMOVAR,
	title = {The NEMOVAR ocean data assimilation system as implemented in the ECMWF ocean analysis for System 4},
	publisher = {CERFACS, Toulouse},
	author = {Kristian Mogensen, Magdalena Alonso Balmaseda, Anthony Weaver},
	year = {2012},
}

@ARTICLE{Freitas2002,
	author = {Christopher J.Freitas},
	title = {The issue of numerical uncertainty},
	journal = {Applied Mathematical Modelling},
	year = {2002},
	volume = {26},
	pages = {237-248}
}


@ARTICLE{Ford2004,
	author = {Ford, R. and Pain, C. C. and Goddard, A. J. H. and De Oliveira, C. R. E. and Umpleby, A. P.},
	title = {A non-hydrostatic finite-element model for three-dimensional stratified oceanic flows. Part {I}: Model formulation},
	journal = {Monthly Weather Review},
	year = {2004},
	volume = {132},
	pages = {2816-2831}
}

@ARTICLE{Aristodemou2009,
	author = {Aristodemou, E. and Bentham, T. and Pain, C. and Robins, A.},
	title = {A comparison of mesh-adaptive LES with wind tunnel data for flow past buildings: mean flows and velocity fluctuations.},
	journal = {Atmospheric Environment journal},
	year = {2009},
	volume = {43},
	pages = {6238-6253}
}

@article{AMCG2015,
author = {Imperial College London AMCG},
title = {Fluidity manual v4.1.12},
year = {2015},
month = {4},
url = {https://figshare.com/articles/Fluidity_Manual/1387713}
}

@ARTICLE{Smith:2013jd,
	author = {Jones, A.~B. and Smith, J.~M.},
	title = {{A}rticle {T}itle},
	journal = {{J}ournal {T}itle},
	year = {2013},
	volume = {13},
	pages = {123-456},
	number = {52},
	month = {March},
	publisher = {Publisher}
}

    @article{pain2001tetrahedral,
	Author = {Pain, CC and Umpleby, AP and De Oliveira, CRE and Goddard, AJH},
	Journal = {Computer Methods in Applied Mechanics and Engineering},
	Number = {29},
	Pages = {3771--3796},
	Publisher = {Elsevier},
	Title = {Tetrahedral mesh optimisation and adaptivity for steady-state and transient finite element calculations},
	Volume = {190},
	Year = {2001}}

@ARTICLE{MooreROMS,
	author = {Moore, A. M.; and Arango, H. G.; and Broquet, G.; and Powell, B. S.; and Weaver, A. T.; and Zavala-Garay},
	title = {The Regional Ocean Modeling System (ROMS) 4-dimensional variational data assimilation systems: Part I – System overview and formulation},
	journal = {J. Progress in Oceanography},
	year = {2011},
	volume = {91},
	pages = {34 - 49},
	number = {1}
}

@ARTICLE{Li2001,
	author = {	Dong-Hui Li	and Masao Fukushima	},
	title = {A modified BFGS method and its global convergence in nonconvex minimization},
	journal = {Journal of Computational and Applied Mathematics},
	year = {2001},
	volume = {129},
	pages = {15-35}
}


@ARTICLE{LBFGS10,
	author = {J. Nocedal and R.H. Byrd, and P. Lu and C. Zhu},
	title = {L-BFGS-B: Fortran Subroutines for Large-Scale Bound-Constrained Optimization},
	journal = {ACM Transactions on Mathematical Software},
	year = {1997},
	volume = {23},
	pages = {550-560},
	number = {4}
}


@ARTICLE{Nichols,
	author = {JN. Nichols},
	title = {Mathematical Concepts in Data Assimilation},
	journal = {W. Lahoz et al. (eds.), Data Assimilation},
	year = {2010},
	publisher = {Springer}
}


@ARTICLE{Liu1989,
	author = {Liu, D.C., and Nocedal, J.},
	title = {On the limited memory BFGS method for large scale optimization},
	journal = {Mathematical Programming},
	year = {1989},
	volume = {45},
	pages = {503-528}
}


@ARTICLE{Lorenc1997,
	author = {Lorenc, A.C.},
	title = {Development of an operational variational assimilation scheme},
	journal = {Journal of the Meteorological Society of Japan},
	year = {1997},
	volume = {75},
	pages = {339-346}
}

@ARTICLE{Lorenz,
	author = {E. N.  Lorenz},
	title = {Empirical orthogonal functions and statistical weather prediction.},
	journal = {Statistical Forecasting Project, M.I.T., Sci. Rep. No. 1},
	year = {1956},
	number = {1},
	publisher = {S Cambridge}
}


@BOOK{Kalnay,
	title = {Atmospheric modeling, data assimilation and predictability},
	publisher = {Cambridge},
	author = {E. Kalnay},
	year = {2003}
}

@ARTICLE{Hansen98,
	author = {C.Hansen},
	title = {Rank-Deficient and Discrete Ill-Posed Problems, numerical aspects of linear inversion},
	journal = {SIAM},
	year = {1998}
}


@ARTICLE{Hansen06,
	author = {P. C. Hansen, and J. G. Nagy, and D. P. O'Leary},
	title = {Deblurring Images: Matrices, Spectra, and Filtering},
	journal = {SIAM},
	year = {2006}
}


@ARTICLE{Haben,
	author = {JS. A. Haben},
	title = {Conditioning and Preconditioning of the Minimisation Problem in Variational Data Assimilation},
	journal = {PhD. Thesis},
	year = {2011},
	publisher = {University of Reading}
}


@ARTICLE{Engl,
	author = {H. K. Engl, and M. Hanke, and A. Neubauer},
	title = {Regularization of Inverse Problems},
	journal = {Kluwer},
	year = {1996}
}


@ARTICLE{oceanvar08,
	author = {S. Dobricic, and N. Pinardi},
	title = {An oceanographic three-dimensional variational data assimilation scheme},
	journal = {Ocean Modelling},
	year = {2008},
	volume = {22},
	pages = {89-105}
}

@ARTICLE{Christie05,
	author = {Michael A. Christie, and James Glimm, and John W. Grove, and David M. Higdon, and David H. Sharp, and Merri M. Wood-Schultz},
	title = {Error Analysis and Simulations of Complex Phenomena},
	journal = {Los Alamos Science},
	year = {2005},
	number = {29}
}


@ARTICLE{Coutierb,
	author = {JP. Courtier},
	title = {A strategy for operational implementation of 4D-VAR, using an incremental approach.},
	journal = {Q J R Meteorol Soc},
	year = {1994},
	volume = {120},
	pages = {1367-1387},
	number = {519}
}



@ARTICLE{Cacuci13,
	author = {D.G. Cacuci, and I. M. Navon, and M. Ionescu-Bujor,},
	title = {Computational Methods for Data Evaluation and Assimilation.},
	journal = {CRC Press},
	year = {2013}
}



@ARTICLE{ECMWF,
	author = {E. Andersson, and J. Haseler, and P. Und\'en, and P. Courtier, and G. Kelly, and D. Vasiljevic, and C. Brancovic, and C. Cardinali, and C. Gaffard, and A.Hollingsworth, and C. Jakob, and P. Janssen, and E. Klinker, and A. Lanzinger, and M. Miller, and F. Rabier, and A. Simmons, and B. Strauss, and J-N.Thepaut and P. Viterbo},
	title = {The ECMWF implementation of three dimensional variational assimilation (3DVar). Part III: Experimental results},
	journal = {Quarterly Journal Royal Met. Society.},
	year = {1998},
	volume = {124},
	pages = {1831-1860},
	number = {550}
}

@ARTICLE{NCARBaker,
	author = {D. M. Baker, and W. Huang, and Y.R. Guo, and J. Bourgeois, and Q.N. Xiao},
	title = {Three-Dimensional Variational Data Assimilation System for MM5: Implementation and Initial Results},
	journal = {Mon. Wea. Rev.},
	year = {2004},
	volume = {132},
	pages = {897-914}
}

@ARTICLE{Arcucci2017,
	author = {R. Arcucci, and L. D’Amore, and J. Pistoia, and R. Toumi and A. Murli},
	title = {On the variational data assimilation problem solving and sensitivity analysis},
	journal = {Journal of Computational Physics},
	year = {2017},
	volume = {335},
	pages = {311-326},
	publisher = {Elsevier}
}


@ARTICLE{Arcucci2017par,
	author = {R. Arcucci, and L. D’Amore, and L. Carracciuolo, and G. Scotti and G. Laccetti},
	title = {A Decomposition of the Tikhonov Regularization Functional Oriented to Exploit Hybrid Multilevel Parallelism},
	journal = {International Journal of Parallel Programming},
	year = {2017},
	volume = {45(5)},
	pages = {1214-1235}
}

@ARTICLE{Arcucci2017Caspian,
	author = {R. Arcucci, and L. Carracciuolo, and  R. Toumi},
	title = {Toward a preconditioned scalable 3DVAR for assimilating Sea Surface Temperature collected into the Caspian Sea},
	journal = {Journal of Numerical Analysis, Industrial and Applied Mathematics},
	year = {2018},
	volume = { 12(1-2)},
	pages = {9-28}
}

@article{babovic2002data,
  title={Data assimilation of local model error forecasts in a deterministic model},
  author={Babovic, Vladan and Fuhrman, David R},
  journal={International journal for numerical methods in fluids},
  volume={39},
  number={10},
  pages={887--918},
  year={2002},
  publisher={Wiley Online Library}
}

@article{babovic2001neural,
  title={Neural networks as routine for error updating of numerical models},
  author={Babovic, Vladan and Ca{\v{n}}izares, Rafael and Jensen, H Ren{\'e} and Klinting, Anders},
  journal={Journal of Hydraulic Engineering},
  volume={127},
  number={3},
  pages={181--193},
  year={2001},
  publisher={American Society of Civil Engineers}
}

@inproceedings{babovic2000global,
  title={From global to local modelling: a case study in error correction of deterministic models},
  author={Babovic, V and Keijzer, M and Bundzel, M},
  booktitle={Proceedings of fourth international conference on hydroinformatics},
  year={2000}
}

@ARTICLE{Arcucci2018,
	author = {R. Arcucci, and C. Pain, and  Y. Guo},
	title = {Effective variational data assimilation in air-pollution prediction},
	journal = { Big Data Mining and Analytics},
	year = {2018},
	volume = { 1(4)},
	pages = {297 - 307}
}

@ARTICLE{Hannachi2004,
	author = {A. Hannachi},
	title = {A Primer for EOF Analysis of Climate Data},
	journal = {Department of Meteorology, University of Reading, UK},
	year = {2004}
}

@ARTICLE{Alekseev2009,
	author = { A. K.Alekseev, and I. M. Navon and J. Steward },
	title = {Comparison of advanced large-scale minimization algorithms for the solution of inverse ill-posed problems},
	journal = {Optimization Methods and Software},
	year = {2009},
    volume = { 24(1)},
	pages = {63-87}
}


@ARTICLE{Daescu2003,
	author = {D. Daescu, and I. M. Navon},
	title = {An Analysis of a Hybrid Optimization method for variational data assimilation},
	journal = {International Journal of Computational Fluid Dynamics},
	year = {2003},
    volume = { 17(4)},
	pages = {299-306}
}

@ARTICLE{Navon1987,
	author = {I. M. Navon, and David M Legler},
	title = {Conjugate-Gradient Methods for Large-Scale Minimization in Meteorology},
	journal = {Monthly Weather Review},
	year = {1987},
    volume = { 115(4)},
	pages = {1479-1502}
}


@ARTICLE{Zou1993,
	author = {X.Zou, and I. M. Navon, and M. Berger, and K.H. Phua, and T.Schlick, and F.X. Le Dimet},
	title = {Numerical Experience with Limited Memory Quasi-Newton and Truncated Newton Methods},
	journal = {SIAM Journal on Optimization},
	year = {1993},
    volume = { 3(3)},
	pages = {582-608}
}

@ARTICLE{Cacuci2005,
	author = {D.G. Cacuci, and  M. Ionescu-Bujor, and I. M. Navon},
	title = {Sensitivity and Uncertainty
Analysis. II. Applications to Large Scale Systems},
	journal = {hapman and Hall: Boca
Raton, Florida},
	year = {2005}
}


@ARTICLE{Grattona2009,
	author = {S. Grattona, and J. Tshimanga},
	title = {An observation-space formulation of variational assimilation
using a restricted preconditioned conjugate gradient
algorithm},
	journal = {Q. J. R. Meteorol. Soc},
	year = {2009},
    volume = { 135},
	pages = {1573-1585}
}

@ARTICLE{Golub1996,
	author = {Gene H. Golub, and Charles F. Van Loan},
	title = {Matrix Computations},
	journal = {JHU Press},
	year = {1996}
}

@article{zhu2019model,
  title={Model error correction in data assimilation by integrating neural networks},
  author={Zhu, Jiangcheng and Hu, Shuang and Arcucci, Rossella and Xu, Chao and Zhu, Jihong and Guo, Yi-ke},
  journal={Big Data Mining and Analytics},
  volume={2},
  number={2},
  pages={83--91},
  year={2019},
  publisher={TUP}
}


@ARTICLE{Gurol2014,
	author = {S. Gurol, and A. T. Weaver,and  A. M. Moore, and A. Piacentini, and H. G. Arangod, and S. Gratton
},
	title = {B-preconditioned minimization algorithms for variational data
assimilation with the dual formulation},
	journal = {Q. J. R. Meteorol. Soc.},
	year = {2014},
    volume = { 140},
	pages = {539-556}
}

@inproceedings{baldi2012autoencoders,
  title={Autoencoders, unsupervised learning, and deep architectures},
  author={Baldi, Pierre},
  booktitle={Proceedings of ICML workshop on unsupervised and transfer learning},
  pages={37--49},
  year={2012}
}

@article{watson2019applying,
  title={Applying machine learning to improve simulations of a chaotic dynamical system using empirical error correction},
  author={Watson, Peter AG},
  journal={Journal of advances in modeling earth systems},
  volume={11},
  number={5},
  pages={1402--1417},
  year={2019},
  publisher={Wiley Online Library}
}

@article{mccreanor2007respiratory,
  title={Respiratory effects of exposure to diesel traffic in persons with asthma},
  author={McCreanor, James and Cullinan, Paul and Nieuwenhuijsen, Mark J and Stewart-Evans, James and Malliarou, Eleni and Jarup, Lars and Harrington, Robert and Svartengren, Magnus and Han, In-Kyu and Ohman-Strickland, Pamela and others},
  journal={New England Journal of Medicine},
  volume={357},
  number={23},
  pages={2348--2358},
  year={2007},
  publisher={Mass Medical Soc}
}

@article{sinharay2018respiratory,
  title={Respiratory and cardiovascular responses to walking down a traffic-polluted road compared with walking in a traffic-free area in participants aged 60 years and older with chronic lung or heart disease and age-matched healthy controls: a randomised, crossover study},
  author={Sinharay, Rudy and Gong, Jicheng and Barratt, Benjamin and Ohman-Strickland, Pamela and Ernst, Sabine and Kelly, Frank J and Zhang, Junfeng Jim and Collins, Peter and Cullinan, Paul and Chung, Kian Fan},
  journal={The Lancet},
  volume={391},
  number={10118},
  pages={339--349},
  year={2018},
  publisher={Elsevier}
}

@article{burnett2018global,
  title={Global estimates of mortality associated with long-term exposure to outdoor fine particulate matter},
  author={Burnett, Richard and Chen, Hong and Szyszkowicz, Mieczys{\l}aw and Fann, Neal and Hubbell, Bryan and Pope, C Arden and Apte, Joshua S and Brauer, Michael and Cohen, Aaron and Weichenthal, Scott and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={38},
  pages={9592--9597},
  year={2018},
  publisher={National Acad Sciences}
}

@article{casas2020reduced,
  title={{A Reduced Order Deep Data Assimilation model}},
  author={Quilodr{\'a}n Casas, C{\'e}sar and Arcucci, Rossella and Wu, Pin and Pain, Christopher and Guo, Yi-Ke},
  journal={Physica D: Nonlinear Phenomena},
  volume={412},
  pages={132615},
  year={2020},
  publisher={Elsevier}
}

@article{reddy2020deep,
  title={{Deep Convolutional Recurrent Autoencoders for Flow Field Prediction}},
  author={Reddy Bukka, Sandeep and Magee, Allan Ross and Jaiman, Rajeev Kumar},
  journal={arXiv},
  pages={arXiv--2003},
  year={2020}
}

@inproceedings{shafahi2019adversarial,
  title={{Adversarial training for free!}},
  author={Shafahi, Ali and Najibi, Mahyar and Ghiasi, Mohammad Amin and Xu, Zheng and Dickerson, John and Studer, Christoph and Davis, Larry S and Taylor, Gavin and Goldstein, Tom},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3358--3369},
  year={2019}
}

@inproceedings{meng2017magnet,
  title={Magnet: a two-pronged defense against adversarial examples},
  author={Meng, Dongyu and Chen, Hao},
  booktitle={Proceedings of the 2017 ACM SIGSAC conference on computer and communications security},
  pages={135--147},
  year={2017}
}

@inproceedings{arcucci2019domain,
  title={{A Domain Decomposition Reduced Order Model with Data Assimilation (DD-RODA)}},
  author={Arcucci, Rossella and Quilodr{\'a}n Casas, C{\'e}sar and Xiao, Dunhui and Mottet, Laetitia and Fang, Fangxin and Wu, Pin and Pain, Christopher C and Guo, Yi-Ke},
  booktitle={PARCO},
  pages={189--198},
  year={2019}
}

@article{wu2020data,
  title={Data-driven reduced order model with temporal convolutional neural network},
  author={Wu, Pin and Sun, Junwu and Chang, Xuting and Zhang, Wenjie and Arcucci, Rossella and Guo, Yike and Pain, Christopher C},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={360},
  pages={112766},
  year={2020},
  publisher={Elsevier}
}

@article{cheng2020data,
  title={Data-driven modelling of nonlinear spatio-temporal fluid flows using a deep convolutional generative adversarial network},
  author={Cheng, M and Fang, Fangxin and Pain, Christopher C and Navon, IM},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={365},
  pages={113000},
  year={2020},
  publisher={Elsevier}
}

@article{xie2018tempogan,
  title={{Tempogan: A temporally coherent, volumetric gan for super-resolution fluid flow}},
  author={Xie, You and Franz, Erik and Chu, Mengyu and Thuerey, Nils},
  journal={ACM Transactions on Graphics (TOG)},
  volume={37},
  number={4},
  pages={1--15},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@inproceedings{casas2020urban,
  title={{Urban air pollution forecasts generated from latent space representation}},
  author={Quilodr\'an Casas, C\'esar and Arcucci, Rossella and Guo, Yike},
  booktitle={ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations},
  pages = {26},
  year={2020}
}



@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{quilodran2021adversarial,
  title={Adversarial autoencoders and adversarial LSTM for improved forecasts of urban air pollution simulations},
  author={Quilodr{\'a}n-Casas, C{\'e}sar and Arcucci, Rossella and Mottet, Laetitia and Guo, Yike and Pain, Christopher},
  journal={arXiv preprint arXiv:2104.06297},
  year={2021}
}

@article{pfaff2020learning,
  title={Learning mesh-based simulation with graph networks},
  author={Pfaff, Tobias and Fortunato, Meire and Sanchez-Gonzalez, Alvaro and Battaglia, Peter W},
  journal={arXiv preprint arXiv:2010.03409},
  year={2020}
}

@article{takeishi2017learning,
  title={Learning Koopman invariant subspaces for dynamic mode decomposition},
  author={Takeishi, Naoya and Kawahara, Yoshinobu and Yairi, Takehisa},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{otto2019linearly,
  title={Linearly recurrent autoencoder networks for learning dynamics},
  author={Otto, Samuel E and Rowley, Clarence W},
  journal={SIAM Journal on Applied Dynamical Systems},
  volume={18},
  number={1},
  pages={558--593},
  year={2019},
  publisher={SIAM}
}

@article{carlberg2019recovering,
  title={Recovering missing CFD data for high-order discretizations using deep neural networks and dynamics learning},
  author={Carlberg, Kevin T and Jameson, Antony and Kochenderfer, Mykel J and Morton, Jeremy and Peng, Liqian and Witherden, Freddie D},
  journal={Journal of Computational Physics},
  volume={395},
  pages={105--124},
  year={2019},
  publisher={Elsevier}
}



@inproceedings{mococxr,
  title={Moco pretraining improves representation and transferability of chest x-ray models},
  author={Sowrirajan, Hari and Yang, Jingbo and Ng, Andrew Y and Rajpurkar, Pranav},
  booktitle={Medical Imaging with Deep Learning},
  pages={728--744},
  year={2021},
  organization={PMLR}
}

@inproceedings{holden2019subspace,
  title={Subspace neural physics: Fast data-driven interactive simulation},
  author={Holden, Daniel and Duong, Bang Chi and Datta, Sayantan and Nowrouzezahrai, Derek},
  booktitle={Proceedings of the 18th annual ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
  pages={1--12},
  year={2019}
}

@inproceedings{sanchez2020learning,
  title={Learning to simulate complex physics with graph networks},
  author={Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter},
  booktitle={International Conference on Machine Learning},
  pages={8459--8468},
  year={2020},
  organization={PMLR}
}

@inproceedings{sanchez2021learning,
    title={Learning general-purpose cnn-based simulators for astrophysical turbulence},
    author={Sanchez-Gonzalez, Alvaro and Stachenfeld, Kimberly},
    booktitle={SimDL workshop at ICLR2021},
    year={2021}
}

@inproceedings{kim2019deep,
  title={Deep fluids: A generative network for parameterized fluid simulations},
  author={Kim, Byungsoo and Azevedo, Vinicius C and Thuerey, Nils and Kim, Theodore and Gross, Markus and Solenthaler, Barbara},
  booktitle={Computer Graphics Forum},
  pages={59--70},
  year={2019},
  organization={Wiley Online Library}
}

@article{wu2018deep,
  title={Deep generative markov state models},
  author={Wu, Hao and Mardt, Andreas and Pasquali, Luca and Noe, Frank},
  journal={arXiv preprint arXiv:1805.07601},
  year={2018}
}

@inproceedings{wiewel2019latent,
  title={Latent space physics: Towards learning the temporal evolution of fluid flow},
  author={Wiewel, Steffen and Becher, Moritz and Thuerey, Nils},
  booktitle={Computer graphics forum},
  volume={38},
  number={2},
  pages={71--82},
  year={2019},
  organization={Wiley Online Library}
}

@article{thuerey2020deep,
  title={Deep learning methods for Reynolds-averaged Navier--Stokes simulations of airfoil flows},
  author={Thuerey, Nils and Wei{\ss}enow, Konstantin and Prantl, Lukas and Hu, Xiangyu},
  journal={AIAA Journal},
  volume={58},
  number={1},
  pages={25--36},
  year={2020},
  publisher={American Institute of Aeronautics and Astronautics}
}

@article{karna2018thetis,
  title={Thetis coastal ocean model: discontinuous Galerkin discretization for the three-dimensional hydrostatic equations},
  author={K{\"a}rn{\"a}, Tuomas and Kramer, Stephan C and Mitchell, Lawrence and Ham, David A and Piggott, Matthew D and Baptista, Ant{\'o}nio M},
  journal={Geoscientific Model Development},
  volume={11},
  number={11},
  pages={4359--4382},
  year={2018},
  publisher={Copernicus GmbH}
}

@article{sheridan1997flow,
  title={Flow past a cylinder close to a free surface},
  author={Sheridan, John and Lin, J-C and Rockwell, D},
  journal={Journal of Fluid Mechanics},
  volume={330},
  pages={1--30},
  year={1997},
  publisher={Cambridge University Press}
}

@article{li2020using,
  title={Using machine learning to detect the turbulent region in flow past a circular cylinder},
  author={Li, Binglin and Yang, Zixuan and Zhang, Xing and He, Guowei and Deng, Bing-Qing and Shen, Lian},
  journal={Journal of Fluid Mechanics},
  volume={905},
  year={2020},
  publisher={Cambridge University Press}
}

@article{schwaller2019molecular,
  title={Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction},
  author={Schwaller, Philippe and Laino, Teodoro and Gaudin, Th{\'e}ophile and Bolgar, Peter and Hunter, Christopher A and Bekas, Costas and Lee, Alpha A},
  journal={ACS central science},
  volume={5},
  number={9},
  pages={1572--1583},
  year={2019},
  publisher={ACS Publications}
}

@article{wang2016auto,
  title={Auto-encoder based dimensionality reduction},
  author={Wang, Yasi and Yao, Hongxun and Zhao, Sicheng},
  journal={Neurocomputing},
  volume={184},
  pages={232--242},
  year={2016},
  publisher={Elsevier}
}

@article{lucia2004reduced,
  title={Reduced-order modeling: new approaches for computational physics},
  author={Lucia, David J and Beran, Philip S and Silva, Walter A},
  journal={Progress in aerospace sciences},
  volume={40},
  number={1-2},
  pages={51--117},
  year={2004},
  publisher={Elsevier}
}

@article{fu2021data,
  title={A data driven reduced order model of fluid flow by auto-encoder and self-attention deep learning methods},
  author={Fu, R and Xiao, D and Navon, IM and Wang, C},
  journal={arXiv preprint arXiv:2109.02126},
  year={2021}
}

@article{lee2020model,
  title={Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders},
  author={Lee, Kookjin and Carlberg, Kevin T},
  journal={Journal of Computational Physics},
  volume={404},
  pages={108973},
  year={2020},
  publisher={Elsevier}
}

@article{klus2018data,
  title={Data-driven model reduction and transfer operator approximation},
  author={Klus, Stefan and N{\"u}ske, Feliks and Koltai, P{\'e}ter and Wu, Hao and Kevrekidis, Ioannis and Sch{\"u}tte, Christof and No{\'e}, Frank},
  journal={Journal of Nonlinear Science},
  volume={28},
  number={3},
  pages={985--1010},
  year={2018},
  publisher={Springer}
}

@article{rega2005dimension,
  title={Dimension reduction of dynamical systems: methods, models, applications},
  author={Rega, Giuseppe and Troger, Hans},
  journal={Nonlinear Dynamics},
  volume={41},
  number={1},
  pages={1--15},
  year={2005},
  publisher={Springer}
}


@article{cui2018deep,
  title={{Deep bidirectional and unidirectional LSTM recurrent neural network for network-wide traffic speed prediction}},
  author={Cui, Zhiyong and Ke, Ruimin and Pu, Ziyuan and Wang, Yinhai},
  journal={arXiv preprint arXiv:1801.02143},
  year={2018}
}

@article{cui2020stacked,
  title={Stacked bidirectional and unidirectional {LSTM} recurrent neural network for forecasting network-wide traffic state with missing values},
  author={Cui, Zhiyong and Ke, Ruimin and Pu, Ziyuan and Wang, Yinhai},
  journal={Transportation Research Part C: Emerging Technologies},
  volume={118},
  pages={102674},
  year={2020},
  publisher={Elsevier}
}

@article{schuster1997bidirectional,
  title={{Bidirectional recurrent neural networks}},
  author={Schuster, Mike and Paliwal, Kuldip K},
  journal={IEEE transactions on Signal Processing},
  volume={45},
  number={11},
  pages={2673--2681},
  year={1997},
  publisher={Ieee}
}

@article{rives2021biological,
  title={Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
  author={Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C Lawrence and Ma, Jerry and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={15},
  pages={e2016239118},
  year={2021},
  publisher={National Acad Sciences}
}


@article{lafon_uncertainty_2022,
	title = {Uncertainty quantification when learning dynamical models and solvers with variational methods},
	journal = {arXiv},
	author = {Lafon, N. and Fablet, R. and Naveau, P.},
	year = {2022},
}



@article{fablet_multimodal_2022,
	title = {Multimodal {4DVarNets} for the reconstruction of sea surface dynamics from {SST}-{SSH} synergies},
	doi = {10.48550/arXiv.2207.01372},
	journal = {arXiv:2207.01372},
	author = {Fablet, R. and Febvre, Q. and Chapron, B.},
	year = {2022},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Physics - Atmospheric and Oceanic Physics},
}

@book{cressie_statistics_2015,
	title = {Statistics for {Spatio}-{Temporal} {Data}},
	publisher = {John Wiley \& Sons},
	author = {Cressie, N. and Wikle, C.K.},
	year = {2015},
	keywords = {Mathematics / Probability \& Statistics / General, Medical / Biostatistics, Technology \& Engineering / Remote Sensing \& Geographic Information Systems},
}


@article{liu_bilevel_2019,
	title = {Bilevel {Integrative} {Optimization} for {Ill}-posed {Inverse} {Problems}},
	journal = {arXiv:1907.03083},
	author = {Liu, R. and Ma, L. and Yuan, X. and Zeng, Shangzhi and Zhang, J.},
	year = {2019},
}


@article{fablet_learning_2021,
	title = {Learning {Variational} {Data} {Assimilation} {Models} and {Solvers}},
	volume = {13},
	number = {e2021MS002572},
	journal = {JAMES},
	author = {Fablet, R. and Chapron, B. and Drumetz, L. and Memin, E. and Pannekoucke, O. and Rousseau, F.},
	year = {2021},
	keywords = {Computer Science - Machine Learning, Physics - Atmospheric and Oceanic Physics, Physics - Computational Physics},
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{lin2022survey,
  title={A survey of transformers},
  author={Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
  journal={AI Open},
  year={2022},
  publisher={Elsevier}
}

@article{mohan2018deep,
  title={A deep learning based approach to reduced order modeling for turbulent flow control using LSTM neural networks},
  author={Mohan, Arvind T and Gaitonde, Datta V},
  journal={arXiv preprint arXiv:1804.09269},
  year={2018}
}

@article{nakamura2021convolutional,
  title={Convolutional neural network and long short-term memory based reduced order surrogate for minimal turbulent channel flow},
  author={Nakamura, Taichi and Fukami, Kai and Hasegawa, Kazuto and Nabae, Yusuke and Fukagata, Koji},
  journal={Physics of Fluids},
  volume={33},
  number={2},
  pages={025116},
  year={2021},
  publisher={AIP Publishing LLC}
}

@article{hospedales_meta-learning_2020,
	title = {Meta-learning in neural networks: {A} survey},
	journal = {arXiv:2004.05439},
	author = {Hospedales, T. and Antoniou, A. and Micaelli, P. and Storkey, A.},
	year = {2020},
}

@article{ouala_neural-network-based_2018,
	title = {Neural-{Network}-based {Kalman} {Filters} for the {Spatio}-{Temporal} {Interpolation} of {Satellite}-derived {Sea} {Surface} {Temperature}},
	journal = {Remote Sensing},
	author = {Ouala, S. and Fablet, R. and Herzet, C. and Chapron, B. and Pascual, A. and Collard, F. and Gaultier, L.},
	year = {2018},
	keywords = {Data assimilation, Data-driven, Dynamical model, Kalman filter, Neural networks},
}

@article{raissi_deep_2018,
	title = {Deep {Hidden} {Physics} {Models}: {Deep} {Learning} of {Nonlinear} {Partial} {Diﬀerential} {Equations}},
	volume = {19},
	journal = {JMLR},
	author = {Raissi, M.},
	year = {2018},
	pages = {1--24},
}

@inproceedings{chen_neural_2018,
	title = {Neural {Ordinary} {Differential} {Equations}},
	booktitle = {Proc. {NIPS}'2018},
	author = {Chen, T.Q. and Rubanova, Y. and Bettencourt, J. and Duvenaud, D.K.},
	year = {2018},
	pages = {6571--6583},
}

@article{revach_kalmannet_2022,
	title = {{KalmanNet}: {Neural} {Network} {Aided} {Kalman} {Filtering} for {Partially} {Known} {Dynamics}},
	volume = {70},
	doi = {10.1109/TSP.2022.3158588},
	journal = {IEEE Trans. on Sig. Proc.},
	author = {Revach, G. and Shlezinger, N. and Ni, X. and Escoriza, A.L. and van Sloun, R.J.C. and Eldar, Y.C.},
	year = {2022},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning},
	pages = {1532--1547},
}

@article{boudier_dan_2020,
	title = {{DAN} -- {An} optimal {Data} {Assimilation} framework based on machine learning {Recurrent} {Networks}},
	journal = {arXiv:2010.09694},
	author = {Boudier, P. and Fillion, A. and Gratton, S. and Gürol, S.},
	month = oct,
	year = {2020},
	keywords = {93B07, 93E11, 60G35, 68T07, 94A17, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
}

@article{manucharyan_deep_2021,
	title = {A {Deep} {Learning} {Approach} to {Spatiotemporal} {Sea} {Surface} {Height} {Interpolation} and {Estimation} of {Deep} {Currents} in {Geostrophic} {Ocean} {Turbulence}},
	volume = {13},
	doi = {10.1029/2019MS001965},
	number = {1},
	journal = {JAMES},
	author = {Manucharyan, G. E. and Siegelman, L. and Klein, P.},
	year = {2021},
	keywords = {Deep Learning, baroclinic instability, deep ocean flows, mesoscale eddies, sea surface height interpolation, state estimation},
	pages = {e2019MS001965},
}

@article{barth_dincae_2020,
	title = {{DINCAE} 1.0: a convolutional neural network with error estimates to reconstruct sea surface temperature satellite observations},
	volume = {13},
	doi = {10.5194/gmd-13-1609-2020},
	number = {3},
	journal = {Geosci. Mod. Dev.},
	author = {Barth, A. and Alvera-Azcárate, A. and Licer, M. and Beckers, J.-M.},
	month = mar,
	year = {2020},
	pages = {1609--1622},
}



@article{andrychowicz_learning_2016,
  title={Learning to learn by gradient descent by gradient descent},
  author={Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and De Freitas, Nando},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}



@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {20},
	issn = {0028-0836},
	doi = {10.1038/nature14539},
	language = {english},
	number = {7553},
	journal = {Nature},
	author = {LeCun, Y. and Bengio, Y. and Hinton, G.},
	year = {2015},
	keywords = {Computer science, Mathematics and computing},
	pages = {436--444},
}


@article{long_pde-net_2019,
	title = {{PDE}-{Net} 2.0: {Learning} {PDEs} from data with a numeric-symbolic hybrid deep network},
	volume = {399},
	doi = {10.1016/j.jcp.2019.108925},
	language = {en},
	journal = {Journal of Computational Physics},
	author = {Long, Z. and Lu, Y. and Dong, B.},
	year = {2019},
	keywords = {Convolutional neural network, Dynamic system, Partial differential equations, Symbolic neural network},
	pages = {108925},
}



@inproceedings{cicek_3d_2016,
	title = {{3D} {U}-{Net}: learning dense volumetric segmentation from sparse annotation},
	booktitle = {Proc. {MICCAI}},
	author = {Cicek, O. and Abdulkadir, A. and Lienkamp, S.S. and Brox, T. and Ronneberger, O.},
	year = {2016},
	pages = {424--432},
}

@article{annan_etal_2005,
	author = {Annan, JD and Hargreaves, JC and Edwards, NR and Marsh, R},
	journal = {Ocean Modelling},
	pages = {135--154},
	publisher = {Elsevier},
	title = {{Parameter estimation in an intermediate complexity Earth system model using an ensemble Kalman filter}},
	volume = {8},
	year = {2005}
}

@book{Jaz,
    author = "A.~H. Jazwinski",
    title  = "Stochastic Processes and Filtering Theory",
    publisher= "Academic Press",
    pages  = "376",
    year   = "1970" }



@article{kivman_2003,
  title={Sequential parameter estimation for stochastic systems},
  author={Kivman, GA},
  journal={Nonlinear Processes in Geophysics},
  volume={10},
  number={3},
  pages={253--259},
  year={2003},
  publisher={Copernicus Publications G{\"o}ttingen, Germany}
}

@ARTICLE{legler_janjic_2022,
  author={Legler, S. and T. Janjic},
  journal={Q J R Meteorol Soc}, 
  title={Combining data assimilation and machine learning to estimate parameters of a convective-scale model}, 
  volume = {148},
  year={2022},
  pages={860-874},
  doi={https://doi.org/10.1002/qj.4235}
 }




@article{posselt_bishop_2012,
  title={Nonlinear parameter estimation: Comparison of an ensemble Kalman smoother with a Markov chain Monte Carlo algorithm},
  author={Posselt, Derek J and Bishop, Craig H},
  journal={Monthly weather review},
  volume={140},
  number={6},
  pages={1957--1974},
  year={2012},
  publisher={American Meteorological Society}
}



@article{posselt_etal_2014,
  title={Errors in ensemble Kalman smoother estimates of cloud microphysical parameters},
  author={Posselt, Derek J and Hodyss, Daniel and Bishop, Craig H},
  journal={Monthly Weather Review},
  volume={142},
  number={4},
  pages={1631--1654},
  year={2014},
  publisher={American Meteorological Society}
}

@article{hodyss2011ensemble,
  title={Ensemble state estimation for nonlinear systems using polynomial expansions in the innovation},
  author={Hodyss, Daniel},
  journal={Monthly Weather Review},
  volume={139},
  number={11},
  pages={3571--3588},
  year={2011}
}

@article{cai2021surrogate,
  title={Surrogate models based on machine learning methods for parameter estimation of left ventricular myocardium},
  author={Cai, Li and Ren, Lei and Wang, Yongheng and Xie, Wenxian and Zhu, Guangyu and Gao, Hao},
  journal={Royal Society open science},
  volume={8},
  number={1},
  pages={201121},
  year={2021},
  publisher={The Royal Society}
}

@article{emerick2012history,
  title={History matching time-lapse seismic data using the ensemble Kalman filter with multiple data assimilations},
  author={Emerick, Alexandre A and Reynolds, Albert C},
  journal={Computational Geosciences},
  volume={16},
  number={3},
  pages={639--659},
  year={2012},
  publisher={Springer}
}

@article{RuckstuhlJanjic18,
  title={Parameter and state estimation with ensemble Kalman filter based algorithms for convective-scale applications},
  author={Ruckstuhl, Yvonne M and Janji{\'c}, Tijana},
  journal={Quarterly Journal of the Royal Meteorological Society},
  volume={144},
  number={712},
  pages={826--841},
  year={2018},
  publisher={Wiley Online Library}
}



@article{lucor2004generalized,
  title={Generalized polynomial chaos and random oscillators},
  author={Lucor, Didier and Su, C-H and Karniadakis, George Em},
  journal={International Journal for Numerical Methods in Engineering},
  volume={60},
  number={3},
  pages={571--596},
  year={2004},
  publisher={Wiley Online Library}
}

@article{borovikov2005multivariate,
  title={Multivariate error covariance estimates by Monte Carlo simulation for assimilation studies in the Pacific Ocean},
  author={Borovikov, Anna and Rienecker, Michele M and Keppenne, Christian L and Johnson, Gregory C},
  journal={Monthly weather review},
  volume={133},
  number={8},
  pages={2310--2334},
  year={2005}
}

@article{schwab2019deep,
  title={Deep learning in high dimension: Neural network expression rates for generalized polynomial chaos expansions in UQ},
  author={Schwab, Christoph and Zech, Jakob},
  journal={Analysis and Applications},
  volume={17},
  number={01},
  pages={19--55},
  year={2019},
  publisher={World Scientific}
}

@article{bousserez2015improved,
  title={Improved analysis-error covariance matrix for high-dimensional variational inversions: Application to source estimation using a 3D atmospheric transport model},
  author={Bousserez, N and Henze, DK and Perkins, A and Bowman, KW and Lee, M and Liu, J and Deng, F and Jones, DBA},
  journal={Quarterly Journal of the Royal Meteorological Society},
  volume={141},
  number={690},
  pages={1906--1921},
  year={2015},
  publisher={Wiley Online Library}
}

@article{li2009generalized,
  title={A generalized polynomial chaos based ensemble Kalman filter with high accuracy},
  author={Li, Jia and Xiu, Dongbin},
  journal={Journal of computational physics},
  volume={228},
  number={15},
  pages={5454--5469},
  year={2009},
  publisher={Elsevier}
}

@article{ghanem1999propagation,
  title={Propagation of probabilistic uncertainty in complex physical systems using a stochastic finite element approach},
  author={Ghanem, Roger and Red-Horse, John},
  journal={Physica D: Nonlinear Phenomena},
  volume={133},
  number={1-4},
  pages={137--144},
  year={1999},
  publisher={Elsevier}
}




@article{RuckstuhlJanjic20,
  title={Combined state-parameter estimation with the LETKF for convective-scale weather forecasting},
  author={Ruckstuhl, Yvonne and Janji{\'c}, Tijana},
  journal={Monthly Weather Review},
  volume={148},
  number={4},
  pages={1607--1628},
  year={2020}
}


@techreport{Geer2020,
	title = {Learning earth system models from observations: machine learning or data assimilation?},
	author = {Alan J. Geer},
	year = 2020,
	month = {05},
	number = 863,
	doi = {10.21957/7fyj2811r},
	institution = {ECMWF}
}

@article{bonavita2014data,
  title={Data Assimilation and Machine Learning Science at ECMWF},
  author={Bonavita, Massimo and Massart, Sebastien and Laloyaux, Patrick and Chrust, Marcin},
  year={2014}
}

@article{Raissi2016,
	title = {Inferring solutions of differential equations using noisy multi-fidelity data},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George},
	year = 2016,
	month = {07},
	journal = {Journal of Computational Physics},
	volume = 335,
	doi = {10.1016/j.jcp.2017.01.060}
}



@article{Perdikaris2017,
	title = {Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling},
	author = {Perdikaris, Paris and Raissi, Maziar and Damianou, Andreas and Lawrence, N. and Karniadakis, George},
	year = 2017,
	month = {02},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Science},
	volume = 473,
	pages = 20160751,
	doi = {10.1098/rspa.2016.0751}
}

@inproceedings{Lin2019,
	title = {Air Quality Forecast through Integrated Data Assimilation and Machine Learning},
	author = {Lin, Hai and Jin, Jianbing and Herik, H.},
	year = 2019,
	month = {02},
	booktitle = {International Conference on Agents and Artificial Intelligence},
	pages = {787--793},
	doi = {10.5220/0007555207870793}
}

@article{Williams2014,
	title = {Data Fusion via Intrinsic Dynamic Variables: An Application of Data-Driven Koopman Spectral Analysis},
	author = {Williams, Matthew and Rowley, Clarence and Mezic, Igor and Kevrekidis, Ioannis},
	year = 2014,
	month = 11,
	journal = {EPL (Europhysics Letters)},
	volume = 109,
	doi = {10.1209/0295-5075/109/40007}
}


@article{rasp-2018,
  title = {Deep learning to represent subgrid processes in climate models},
  author = {Rasp, Stephan and Pritchard, Michael S. and Gentine, Pierre},
  year = {2018},
  month-dummy = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {39},
  pages = {9684--9689},
  doi = {10.1073/pnas.1810286115},
  keywords = {forecast,machine learning,model error}
}

@article{bolton-2019,
  title = {Applications of Deep Learning to Ocean Data Inference and Subgrid Parameterization},
  author = {Bolton, Thomas and Zanna, Laure},
  year = {2019},
  month-dummy = jan,
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {11},
  number = {1},
  pages = {376--399},
  doi = {10.1029/2018MS001472},
  keywords = {forecast,machine learning,model error}
}

@inproceedings{jia-2019,
  title = {Physics Guided RNNs for Modeling Dynamical Systems: A Case Study in Simulating Lake Temperature Profiles},
  booktitle = {Proceedings of the 2019 SIAM International Conference on Data Mining},
  author = {Jia, Xiaowei and Willard, Jared and Karpatne, Anuj and Read, Jordan and Zwart, Jacob and Steinbach, Michael and Kumar, Vipin},
  year = {2019},
  month-dummy = may,
  pages = {558--566},
  publisher = {Society for Industrial and Applied Mathematics},
  address = {Philadelphia, PA},
  doi = {10.1137/1.9781611975673},
  isbn = {978-1-61197-567-3},
  keywords = {forecast,machine learning}
}

@article{watson-2019,
  title = {Applying Machine Learning to Improve Simulations of a Chaotic Dynamical System Using Empirical Error Correction},
  author = {Watson, Peter A. G.},
  year = {2019},
  month-dummy = may,
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {11},
  number = {5},
  pages = {1402--1417},
  doi = {10.1029/2018MS001597},
  keywords = {forecast,machine learning,model error}
}

@article{bonavita-2020,
  title = {Machine Learning for Model Error Inference and Correction},
  author = {Bonavita, Massimo and Laloyaux, Patrick},
  year = {2020},
  month-dummy = dec,
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {12},
  number = {12},
  doi = {10.1029/2020MS002232},
  keywords = {data assimilation,machine learning,model error}
}

@article{gagne-2020,
  title = {Machine Learning for Stochastic Parameterization: Generative Adversarial Networks in the Lorenz '96 Model},
  shorttitle = {Machine Learning for Stochastic Parameterization},
  author = {Gagne, David John and Christensen, Hannah M. and Subramanian, Aneesh C. and Monahan, Adam H.},
  year = {2020},
  month-dummy = mar,
  journal = {Journal of Advances in Modeling Earth Systems},
  volume = {12},
  number = {3},
  doi = {10.1029/2019MS001896},
  keywords = {forecast,GAN,machine learning}
}



@article{farchi-2021a,
  title = {A comparison of combined data assimilation and machine learning methods for offline and online model error correction},
  author = {Farchi, Alban and Bocquet, Marc and Laloyaux, Patrick and Bonavita, Massimo and Malartic, Quentin},
  year = {2021},
  month-dummy = oct,
  journal = {Journal of Computational Science},
  volume = {55},
  pages = {101468},
  doi = {10.1016/j.jocs.2021.101468},
  copyright = {All rights reserved},
  keywords = {data assimilation,machine learning,model error}
}

@article{barthelemy-2022,
  title = {Super-resolution data assimilation},
  author = {Barth{\'e}l{\'e}my, S{\'e}bastien and Brajard, Julien and Bertino, Laurent and Counillon, Fran{\c c}ois},
  year = {2022},
  month-dummy = aug,
  journal = {Ocean Dynamics},
  volume = {72},
  number = {8},
  pages = {661--678},
  doi = {10.1007/s10236-022-01523-x},
  keywords = {data assimilation,EnKF,machine learning,super-resolution}
}

@article{bocquet-2021,
  title = {Online learning of both state and dynamics using ensemble Kalman filters},
  author = {Bocquet, Marc and Farchi, Alban and Malartic, Quentin},
  year = {2021},
  journal = {Foundations of Data Science},
  volume = {3},
  number = {3},
  pages = {305--330},
  doi = {10.3934/fods.2020015},
  keywords = {data assimilation,EnKF,machine learning}
}

@article{malartic-2022,
  title = {State, global, and local parameter estimation using local ensemble Kalman filters: Applications to online machine learning of chaotic dynamics},
  shorttitle = {State, global, and local parameter estimation using local ensemble Kalman filters},
  author = {Malartic, Q. and Farchi, A. and Bocquet, M.},
  year = {2022},
  month-dummy = jun,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  pages = {qj.4297},
  doi = {10.1002/qj.4297},
  copyright = {All rights reserved},
  keywords = {data assimilation,EnKF,machine learning,parameter estimation}
}

@misc{farchi-2022a,
  type = {preprint},
  title = {Online model error correction with neural networks in the incremental 4D-Var framework},
  author = {Farchi, Alban and Chrust, Marcin and Bocquet, Marc and Laloyaux, Patrick and Bonavita, Massimo},
  year = {2022},
  month-dummy = oct,
  publisher = {Information and Computing Sciences},
  doi = {10.1002/essoar.10512719.1},
  keywords = {data assimilation,machine learning,todo: update}
}

@article{carrassi-2011,
  title = {Treatment of the error due to unresolved scales in sequential data assimilation},
  author = {Carrassi, Alberto and Vannitsem, St{\'e}phane},
  year = {2011},
  month-dummy = dec,
  journal = {International Journal of Bifurcation and Chaos},
  volume = {21},
  number = {12},
  pages = {3619--3626},
  doi = {10.1142/S0218127411030775},
  keywords = {forecast,model error,todo: update}
}

@article{dee-2005,
  title = {Bias and data assimilation},
  author = {Dee, Dick P.},
  year = {2005},
  month-dummy = oct,
  journal = {Quarterly Journal of the Royal Meteorological Society},
  volume = {131},
  number = {613},
  pages = {3323--3343},
  doi = {10.1256/qj.05.137},
  keywords = {data assimilation,model error}
}

@article{bocquet2020bayesian,
  title={Bayesian inference of chaotic dynamics by merging data assimilation, machine learning and expectation-maximization},
  author={Bocquet, Marc and Brajard, Julien and Carrassi, Alberto and Bertino, Laurent},
  journal={Foundations of Data Science},
  volume={2},
  number={1},
  pages={55},
  year={2020},
  publisher={American Institute of Mathematical Sciences}
}

@article{brajard2020combining,
  title={Combining data assimilation and machine learning to emulate a dynamical model from sparse and noisy observations: A case study with the Lorenz 96 model},
  author={Brajard, Julien and Carrassi, Alberto and Bocquet, Marc and Bertino, Laurent},
  journal={Journal of Computational Science},
  volume={44},
  pages={101171},
  year={2020},
  publisher={Elsevier}
}

@article{brajard2021combining,
  title={Combining data assimilation and machine learning to infer unresolved scale parametrization},
  author={Brajard, Julien and Carrassi, Alberto and Bocquet, Marc and Bertino, Laurent},
  journal={Philosophical Transactions of the Royal Society A},
  volume={379},
  number={2194},
  pages={20200086},
  year={2021},
  publisher={The Royal Society Publishing}
}




@article{espeholt2022deep,
  title={Deep learning for twelve hour precipitation forecasts},
  author={Espeholt, Lasse and Agrawal, Shreya and S{\o}nderby, Casper and Kumar, Manoj and Heek, Jonathan and Bromberg, Carla and Gazen, Cenk and Carver, Rob and Andrychowicz, Marcin and Hickey, Jason and others},
  journal={Nature communications},
  volume={13},
  number={1},
  pages={1--10},
  year={2022},
  publisher={Nature Publishing Group}
}

@article{farchi2021using,
  title={Using machine learning to correct model error in data assimilation and forecast applications},
  author={Farchi, Alban and Laloyaux, Patrick and Bonavita, Massimo and Bocquet, Marc},
  journal={Quarterly Journal of the Royal Meteorological Society},
  volume={147},
  number={739},
  pages={3067--3084},
  year={2021},
  publisher={Wiley Online Library}
}

@article{irrgang2021towards,
  title={Towards neural Earth system modelling by integrating artificial intelligence in Earth system science},
  author={Irrgang, Christopher and Boers, Niklas and Sonnewald, Maike and Barnes, Elizabeth A and Kadow, Christopher and Staneva, Joanna and Saynisch-Wagner, Jan},
  journal={Nature Machine Intelligence},
  volume={3},
  number={8},
  pages={667--674},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{mcgovern2019making,
  title={Making the black box more transparent: Understanding the physical implications of machine learning},
  author={McGovern, Amy and Lagerquist, Ryan and Gagne, David John and Jergensen, G Eli and Elmore, Kimberly L and Homeyer, Cameron R and Smith, Travis},
  journal={Bulletin of the American Meteorological Society},
  volume={100},
  number={11},
  pages={2175--2199},
  year={2019},
  publisher={American Meteorological Society}
}



@article{toms2020physically,
  title={Physically interpretable neural networks for the geosciences: Applications to earth system variability},
  author={Toms, Benjamin A and Barnes, Elizabeth A and Ebert-Uphoff, Imme},
  journal={Journal of Advances in Modeling Earth Systems},
  volume={12},
  number={9},
  pages={e2019MS002002},
  year={2020},
  publisher={Wiley Online Library}
}

@article{wang2000data,
  title={Data assimilation and its applications},
  author={Wang, Bin and Zou, Xiaolei and Zhu, Jiang},
  journal={Proceedings of the National Academy of Sciences},
  volume={97},
  number={21},
  pages={11143--11144},
  year={2000},
  publisher={National Acad Sciences}
}

@article{kurth2022fourcastnet,
  title={FourCastNet: Accelerating Global High-Resolution Weather Forecasting using Adaptive Fourier Neural Operators},
  author={Kurth, Thorsten and Subramanian, Shashank and Harrington, Peter and Pathak, Jaideep and Mardani, Morteza and Hall, David and Miele, Andrea and Kashinath, Karthik and Anandkumar, Animashree},
  journal={arXiv preprint arXiv:2208.05419},
  year={2022}
}

@article{ham2019deep,
  title={Deep learning for multi-year ENSO forecasts},
  author={Ham, Yoo-Geun and Kim, Jeong-Hwan and Luo, Jing-Jia},
  journal={Nature},
  volume={573},
  number={7775},
  pages={568--572},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{lorenc1986,
author = "Lorenc, A. C.",
title = "Analysis methods for numerical weather prediction",
journal = qjrms,
volume = "112",
pages = "1177--1194",
year = "1986",
doi = "10.1002/qj.49711247414"
}

@article{tremolet2006,
author = "Tr\'emolet, Y.",
title =	 "Accounting for an imperfect model in {4D-V}ar",
journal = qjrms,
volume = "132",
pages =	 "2483--2504",
year =	 "2006",
doi = "10.1256/qj.05.224"
}

@article{ledimet1986,
author = "Le Dimet, F.-X. and Talagrand, O.",
title = "Variational algorithms for analysis and assimilation of meteorological observations: 
{T}heoretical aspects",
journal = ta,
volume = "38",
pages = "97--110",
year = "1986",
doi = "10.3402/tellusa.v38i2.11706"
}

@article{talagrand1987,
author = "Talagrand, O. and Courtier, P.",
title = "Variational Assimilation of Meteorological Observation with the Adjoint Vorticity Equation. {I}: {T}heory",
journal = qjrms,
volume = "113",
pages = "1311--1328",
year = "1987",
doi = "10.1002/qj.49711347812"
}

@article{rabier2000,
author = "Rabier, F. and J{\"a}rvinen, H. and Klinker, E. and Mahfouf, J.-F. and Simmons, A.",
title =	 "The {ECMWF} operational implementation of four-dimensional variational assimilation. {I}: Experimental results with simplified physics",
journal = qjrms,
volume = "126",
pages =	 "1143--1170",
year =	"2000",
doi = "10.1002/qj.49712656415"
}

@article{rasp2020weatherbench,
  title={WeatherBench: a benchmark data set for data-driven weather forecasting},
  author={Rasp, Stephan and Dueben, Peter D and Scher, Sebastian and Weyn, Jonathan A and Mouatadid, Soukayna and Thuerey, Nils},
  journal={Journal of Advances in Modeling Earth Systems},
  volume={12},
  number={11},
  pages={e2020MS002203},
  year={2020},
  publisher={Wiley Online Library}
}

@book{daley1991,
author = "Daley, R.",
title = "Atmospheric Data Analysis",
publisher = "Cambridge University Press, New-York",
pages = "472",
year = "1991"
}

@book{asch2016,
author = "Asch, M. and Bocquet, M. and Nodet, M.",
title = "Data {A}ssimilation: {M}ethods, {A}lgorithms, and {A}pplications",
publisher = "SIAM, Philadelphia",
series = "Fundamentals of Algorithms",
pages = "324",
ISBN = "978-1-611974-53-9", 
year = "2016",
doi = "10.1137/1.9781611974546"
}

@inproceedings{boots2011online,
  title={An online spectral learning algorithm for partially observable nonlinear dynamical systems},
  author={Boots, Byron and Gordon, Geoffrey},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={25},
  number={1},
  pages={293--300},
  year={2011}
}

@article{ayed2019learning,
  title={Learning dynamical systems from partial observations},
  author={Ayed, Ibrahim and de B{\'e}zenac, Emmanuel and Pajot, Arthur and Brajard, Julien and Gallinari, Patrick},
  journal={arXiv preprint arXiv:1902.11136},
  year={2019}
}

@article{gottwald2021combining,
  title={Combining machine learning and data assimilation to forecast dynamical systems from noisy partial observations},
  author={Gottwald, Georg A and Reich, Sebastian},
  journal={Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume={31},
  number={10},
  pages={101103},
  year={2021},
  publisher={AIP Publishing LLC}
}

@article{sangiorgio2021forecasting,
  title={Forecasting of noisy chaotic systems with deep neural networks},
  author={Sangiorgio, Matteo and Dercole, Fabio and Guariso, Giorgio},
  journal={Chaos, Solitons \& Fractals},
  volume={153},
  pages={111570},
  year={2021},
  publisher={Elsevier}
}

@article{weaver2001,
author = "Weaver, A. and Courtier, P.",
title = "Correlation Modelling on the Sphere Using a Generalized Diffusion Equation",
journal = qjrms,
volume = "127",
pages = "1815–-1846",
year = "2001",
doi = "10.1002/qj.49712757518"
}

@inproceedings{hascoet2014,
author = "Hasco{\"e}t, L.",
title = "Adjoints by Automatic Differentiation",
booktitle = "Advanced data assimilation for geosciences",
editor = "Blayo, {\'E}. and Bocquet, M. and Cosme, E. and Cugliandolo, L. F.",
publisher = "Oxford University Press",
address = "Les {H}ouches school of physics",
pages = "349--369",
year = "2014",
doi = "10.1093/acprof:oso/9780198723844.001.0001"
}

@article{wikner2021using,
  title={Using data assimilation to train a hybrid forecast system that combines machine-learning and knowledge-based components},
  author={Wikner, Alexander and Pathak, Jaideep and Hunt, Brian R and Szunyogh, Istvan and Girvan, Michelle and Ott, Edward},
  journal={Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume={31},
  number={5},
  pages={053114},
  year={2021},
  publisher={AIP Publishing LLC}
}

@book{jazwinski2007stochastic,
  title={Stochastic processes and filtering theory},
  author={Jazwinski, Andrew H},
  year={2007},
  publisher={Courier Corporation}
}

@article{miller1994advanced,
  title={Advanced data assimilation in strongly nonlinear dynamical systems},
  author={Miller, Robert N and Ghil, Michael and Gauthiez, Francois},
  journal={Journal of Atmospheric Sciences},
  volume={51},
  number={8},
  pages={1037--1056},
  year={1994}
}

@incollection{ghil1991data,
  title={Data assimilation in meteorology and oceanography},
  author={Ghil, Michael and Malanotte-Rizzoli, Paola},
  booktitle={Advances in geophysics},
  volume={33},
  pages={141--266},
  year={1991},
  publisher={Elsevier}
}

@article{de2014initialisation,
  title={Initialisation of land surface variables for numerical weather prediction},
  author={de Rosnay, Patricia and Balsamo, Gianpaolo and Albergel, Cl{\'e}ment and Mu{\~n}oz-Sabater, Joaqu{\'\i}n and Isaksen, Lars},
  journal={Surveys in Geophysics},
  volume={35},
  number={3},
  pages={607--621},
  year={2014},
  publisher={Springer}
}

@article{evensen1992using,
  title={Using the extended Kalman filter with a multilayer quasi-geostrophic ocean model},
  author={Evensen, Geir},
  journal={Journal of Geophysical Research: Oceans},
  volume={97},
  number={C11},
  pages={17905--17924},
  year={1992},
  publisher={Wiley Online Library}
}

@book{evensen2009data,
  title={Data assimilation: the ensemble Kalman filter},
  author={Evensen, Geir and others},
  volume={2},
  year={2009},
  publisher={Springer}
}

@article{van2019particle,
  title={Particle filters for high-dimensional geoscience applications: A review},
  author={Van Leeuwen, Peter Jan and K{\"u}nsch, Hans R and Nerger, Lars and Potthast, Roland and Reich, Sebastian},
  journal={Quarterly Journal of the Royal Meteorological Society},
  volume={145},
  number={723},
  pages={2335--2365},
  year={2019},
  publisher={Wiley Online Library}
}

@article{calvello2022ensemble,
  title={Ensemble Kalman Methods: A Mean Field Perspective},
  author={Calvello, Edoardo and Reich, Sebastian and Stuart, Andrew M},
  journal={arXiv preprint arXiv:2209.11371},
  year={2022}
}

@incollection{Carrassi2022,
  author    = {Carrassi, Alberto
               and Bocquet, Marc
               and Demaeyer, Jonathan
               and Grudzien, Colin
               and Raanes, Patrick
               and Vannitsem, St{\'e}phane},
  editor    = {Park, Seon Ki
               and Xu, Liang},
  title     = {Data Assimilation for Chaotic Dynamics},
  booktitle = {Data Assimilation for Atmospheric, Oceanic and Hydrologic Applications (Vol. IV)},
  year      = {2022},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {1--42},
}

@misc{evensen2022data,
  title={Data Assimilation Fundamentals: A Unified Formulation of the State and Parameter Estimation Problem},
  author={Evensen, Geir and Vossepoel, Femke C and van Leeuwen, Peter Jan},
  year={2022},
  publisher={Springer Nature}
}



@article{song_next_gen,
 ISSN = {00030007, 15200477},
 URL = {http://www.jstor.org/stable/26218628},
 author = {Song-You Hong and Jimy Dudhia},
 journal = {Bulletin of the American Meteorological Society},
 number = {1},
 pages = {ES6--ES9},
 publisher = {American Meteorological Society},
 title = {NEXT-GENERATION NUMERICAL WEATHER PREDICTION: Bridging Parameterization, Explicit Clouds, and Large Eddies},
 volume = {93},
 year = {2012}
}

@article{gauthier2021next,
  title={Next generation reservoir computing},
  author={Gauthier, Daniel J and Bollt, Erik and Griffith, Aaron and Barbosa, Wendson AS},
  journal={Nature communications},
  volume={12},
  number={1},
  pages={1--8},
  year={2021},
  publisher={Nature Publishing Group}
}
@Inbook{Abarbanel1996Modeling_chaos,
author="Abarbanel, Henry D. I.",
title="Modeling Chaos",
bookTitle="Analysis of Observed Chaotic Data",
year="1996",
publisher="Springer New York",
address="New York, NY",
pages="95--114"
}



@article{tandeo2015offline,
  title={Offline parameter estimation using EnKF and maximum likelihood error covariance estimates: Application to a subgrid-scale orography parametrization},
  author={Tandeo, Pierre and Pulido, Manuel and Lott, Fran{\c{c}}ois},
  journal={Quarterly Journal of the Royal Meteorological Society},
  volume={141},
  number={687},
  pages={383--395},
  year={2015},
  publisher={Wiley Online Library}
}
@phdthesis{lguensat:tel-01784196,
  TITLE = {{Learning from ocean remote sensing data}},
  AUTHOR = {Lguensat, Redouane},
  NUMBER = {2017IMTA0050},
  SCHOOL = {{Ecole nationale sup{\'e}rieure Mines-T{\'e}l{\'e}com Atlantique}},
  YEAR = {2017},
  month-dummy = Nov,
  KEYWORDS = {Analog forecasting ; Data assimilation ; Analog data assimilation ; Sea surface temperature ; Sea level anomaly ; Deep learning ; Ocean remote sensing ; Assimilation de donn{\'e}es ; Pr{\'e}diction par analogues ; Assimilation de donn{\'e}es par analogues ; T{\'e}l{\'e}d{\'e}tection de l'oc{\'e}an ; Temp{\'e}rature de la surface de l'oc{\'e}an ; El{\'e}vation du niveau de la mer ; Apprentissage profond},
  TYPE = {Theses},
  HAL_ID = {tel-01784196},
  HAL_VERSION = {v1},
}


@article{termonia2018aladin,
  title={The ALADIN System and its canonical model configurations AROME CY41T1 and ALARO CY40T1},
  author={Termonia, Piet and Fischer, Claude and Bazile, Eric and Bouyssel, Fran{\c{c}}ois and Bro{\v{z}}kov{\'a}, Radmila and B{\'e}nard, Pierre and Bochenek, Bogdan and Degrauwe, Daan and Derkov{\'a}, Mari{\'a} and El Khatib, Ryad and others},
  journal={Geoscientific Model Development},
  volume={11},
  pages={257--281},
  year={2018}
}
@article{ruiz2013estimating,
  title={Estimating model parameters with ensemble-based data assimilation: A review},
  author={Ruiz, Juan Jose and Pulido, Manuel and Miyoshi, Takemasa},
  journal={Journal of the Meteorological Society of Japan. Ser. II},
  volume={91},
  number={2},
  pages={79--99},
  year={2013},
  publisher={Meteorological Society of Japan}
}

@manual{NEMO_man,
title="NEMO ocean engine",
author="NEMO System Team",
series="Scientific Notes of Climate Modelling Center",
number="27",
institution="Institut Pierre-Simon Laplace (IPSL)",
publisher="Zenodo",
doi="10.5281/zenodo.1464"
}

@article{ShootingPerDednam_2014,
   title={Optimized shooting method for finding periodic orbits of nonlinear dynamical systems},
   volume={31},
   ISSN={1435-5663},
   url={http://dx.doi.org/10.1007/s00366-014-0386-6},
   DOI={10.1007/s00366-014-0386-6},
   number={4},
   journal={Engineering with Computers},
   publisher={Springer Science and Business Media LLC},
   author={Dednam, W. and Botha, A. E.},
   year={2014},
   month={Nov},
   pages={749–762}
}
@article{lalley2006denoising,
  title={Denoising deterministic time series},
  author={Lalley, Steven P and Nobel, Andrew B},
  journal={arXiv preprint nlin/0604052},
  year={2006}
}

@inproceedings{lorenz1996predictability,
  title={Predictability: A problem partly solved},
  author={Lorenz, Edward N},
  booktitle={Proc. Seminar on predictability},
  volume={1},
  number={1},
  year={1996}
}

@article{sirovich1987turbulence,
  title={Turbulence and the dynamics of coherent structures. I. Coherent structures},
  author={Sirovich, Lawrence},
  journal={Quarterly of applied mathematics},
  volume={45},
  number={3},
  pages={561--571},
  year={1987}
}
@article{guo2019data,
  title={Data-driven reduced order modeling for time-dependent problems},
  author={Guo, Mengwu and Hesthaven, Jan S},
  journal={Computer methods in applied mechanics and engineering},
  volume={345},
  pages={75--99},
  year={2019},
  publisher={Elsevier}
}

@book{bitsadze1988some,
  title={Some classes of partial differential equations},
  author={Bitsadze A. V., Bicadze A.V. and Bitsadze A. V.},
  volume={4},
  year={1988},
  publisher={CRC Press}
}

@article{ypma1995historical,
  title={Historical development of the Newton--Raphson method},
  author={Ypma, Tjalling J},
  journal={SIAM review},
  volume={37},
  number={4},
  pages={531--551},
  year={1995},
  publisher={SIAM}
}
@article{girshick2015region,
  title={Region-based convolutional networks for accurate object detection and segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={38},
  number={1},
  pages={142--158},
  year={2015},
  publisher={IEEE}
}
@article{bury2021deep,
  title={Deep learning for early warning signals of tipping points},
  author={Bury, Thomas M and Sujith, RI and Pavithran, Induja and Scheffer, Marten and Lenton, Timothy M and Anand, Madhur and Bauch, Chris T},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={39},
  pages={e2106140118},
  year={2021},
  publisher={National Acad Sciences}
}
@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}
@article{ouyang2022hybrid,
  title={A hybrid mesoscale closure combining CFD and deep learning for coarse-grid prediction of gas-particle flow dynamics},
  author={Ouyang, Bo and Zhu, Li-Tao and Su, Yuan-Hai and Luo, Zheng-Hong},
  journal={Chemical Engineering Science},
  volume={248},
  pages={117268},
  year={2022},
  publisher={Elsevier}
}
@article{ALAMDARI2021107631,
title = "Improving deep speech denoising by Noisy2Noisy signal mapping",
journal = "Applied Acoustics",
volume = "172",
pages = "107631",
year = "2021",
issn = "0003-682X",
doi = "https://doi.org/10.1016/j.apacoust.2020.107631",
url = "http://www.sciencedirect.com/science/article/pii/S0003682X20307350",
author = "N. Alamdari and A. Azarang and N. Kehtarnavaz",
keywords = "Clean speech-free deep speech denoising, Self-supervised speech enhancement, Fully convolutional neural network",
abstract = "Existing deep learning-based speech denoising approaches require clean speech signals to be available for training. This paper presents a deep learning-based approach to improve speech denoising in real-world audio environments by not requiring the availability of clean speech signals as reference in training mode. A fully convolutional neural network is trained by using two noisy realizations of the same speech signal, one used as the input and the other as the target of the network. Two noisy realizations of the same speech signal are generated by using a mid-side stereo microphone. Extensive experimentations are conducted to show the superiority of the developed deep speech denoising approach over the conventional supervised deep speech denoising approach based on four commonly used performance metrics as well as a subjective testing."
}
@article{vinuesa2022enhancing,
  title={Enhancing computational fluid dynamics with machine learning},
  author={Vinuesa, Ricardo and Brunton, Steven L},
  journal={Nature Computational Science},
  volume={2},
  number={6},
  pages={358--366},
  year={2022},
  publisher={Nature Publishing Group}
}
@inproceedings{feng2014speech,
  title={Speech feature denoising and dereverberation via deep autoencoders for noisy reverberant speech recognition},
  author={Feng, Xue and Zhang, Yaodong and Glass, James},
  booktitle={2014 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={1759--1763},
  year={2014},
  organization={IEEE}
}
@article{frezat2022posteriori,
  title={A posteriori learning for quasi-geostrophic turbulence parametrization},
  author={Frezat, Hugo and Sommer, Julien Le and Fablet, Ronan and Balarac, Guillaume and Lguensat, Redouane},
  journal={arXiv preprint arXiv:2204.03911},
  year={2022}
}
@article{champion2019discovery,
  title={Discovery of nonlinear multiscale systems: Sampling strategies and embeddings},
  author={Champion, Kathleen P and Brunton, Steven L and Kutz, J Nathan},
  journal={SIAM Journal on Applied Dynamical Systems},
  volume={18},
  number={1},
  pages={312--333},
  year={2019},
  publisher={SIAM}
}
@article{frezat2021physical,
  title={Physical invariance in neural networks for subgrid-scale scalar flux modeling},
  author={Frezat, Hugo and Balarac, Guillaume and Le Sommer, Julien and Fablet, Ronan and Lguensat, Redouane},
  journal={Physical Review Fluids},
  volume={6},
  number={2},
  pages={024607},
  year={2021},
  publisher={APS}
}
@article{piomelli1999large,
  title={Large-eddy simulation: achievements and challenges},
  author={Piomelli, Ugo},
  journal={Progress in aerospace sciences},
  volume={35},
  number={4},
  pages={335--362},
  year={1999},
  publisher={Elsevier}
}
@article{xu2014regression,
  title={A regression approach to speech enhancement based on deep neural networks},
  author={Xu, Yong and Du, Jun and Dai, Li-Rong and Lee, Chin-Hui},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={23},
  number={1},
  pages={7--19},
  year={2014},
  publisher={IEEE}
}
@article{yang2019deep,
  title={Deep learning for single image super-resolution: A brief review},
  author={Yang, Wenming and Zhang, Xuechen and Tian, Yapeng and Wang, Wei and Xue, Jing-Hao and Liao, Qingmin},
  journal={IEEE Transactions on Multimedia},
  volume={21},
  number={12},
  pages={3106--3121},
  year={2019},
  publisher={IEEE}
}
@misc{he2015delving,
      title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1502.01852},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{memin2014fluid,
  title={Fluid flow dynamics under location uncertainty},
  author={M{\'e}min, Etienne},
  journal={Geophysical \& Astrophysical Fluid Dynamics},
  volume={108},
  number={2},
  pages={119--146},
  year={2014},
  publisher={Taylor \& Francis}
}
@article{d2007variational,
  title={Variational assimilation of POD low-order dynamical systems},
  author={D'adamo, Juan and Papadakis, Nicolas and Memin, Etienne and Artana, Guillermo},
  journal={Journal of Turbulence},
  number={8},
  pages={N9},
  year={2007},
  publisher={Taylor \& Francis}
}
@article{taormina_neural_2015,
	title = {Neural network river forecasting through baseflow separation and binary-coded swarm optimization},
	volume = {529},
	issn = {0022-1694},
	url = {http://www.sciencedirect.com/science/article/pii/S0022169415005673},
	doi = {10.1016/j.jhydrol.2015.08.008},
	abstract = {Summary
The inclusion of expert knowledge in data-driven streamflow modeling is expected to yield more accurate estimates of river quantities. Modular models (MMs) designed to work on different parts of the hydrograph are preferred ways to implement such approach. Previous studies have suggested that better predictions of total streamflow could be obtained via modular Artificial Neural Networks (ANNs) trained to perform an implicit baseflow separation. These MMs fit separately the baseflow and excess flow components as produced by a digital filter, and reconstruct the total flow by adding these two signals at the output. The optimization of the filter parameters and ANN architectures is carried out through global search techniques. Despite the favorable premises, the real effectiveness of such MMs has been tested only on a few case studies, and the quality of the baseflow separation they perform has never been thoroughly assessed. In this work, we compare the performance of MM against global models (GMs) for nine different gaging stations in the northern United States. Binary-coded swarm optimization is employed for the identification of filter parameters and model structure, while Extreme Learning Machines, instead of ANN, are used to drastically reduce the large computational times required to perform the experiments. The results show that there is no evidence that MM outperform global GM for predicting the total flow. In addition, the baseflow produced by the MM largely underestimates the actual baseflow component expected for most of the considered gages. This occurs because the values of the filter parameters maximizing overall accuracy do not reflect the geological characteristics of the river basins. The results indeed show that setting the filter parameters according to expert knowledge results in accurate baseflow separation but lower accuracy of total flow predictions, suggesting that these two objectives are intrinsically conflicting rather than compatible.},
	urldate = {2018-11-09},
	journal = {Journal of Hydrology},
	author = {Taormina, Riccardo and Chau, Kwok-Wing and Sivakumar, Bellie},
	month = oct,
	year = {2015},
	keywords = {Baseflow separation, Extreme Learning Machine, Modular neural network, Multi-objective optimization, Particle swarm optimization, Rainfall–runoff},
	pages = {1788--1797}
}


@article{carrassi2018data,
  title={Data assimilation in the geosciences: An overview of methods, issues, and perspectives},
  author={Carrassi, Alberto and Bocquet, Marc and Bertino, Laurent and Evensen, Geir},
  journal={Wiley Interdisciplinary Reviews: Climate Change},
  volume={9},
  number={5},
  pages={e535},
  year={2018},
  publisher={Wiley Online Library}
}

@inproceedings{chapron2001wave,
  title={Wave and wind retrieval from SAR images of the ocean},
  author={Chapron, Bertrand and Johnsen, Harald and Garello, Ren{\'e}},
  booktitle={Annales des telecommunications},
  volume={56},
  number={11-12},
  pages={682--699},
  year={2001},
  organization={Springer}
}

@article{li2020scalable,
  title={Scalable gradients for stochastic differential equations},
  author={Li, Xuechen and Wong, Ting-Kam Leonard and Chen, Ricky TQ and Duvenaud, David},
  journal={arXiv preprint arXiv:2001.01328},
  year={2020}
}

@Inbook{BookOPSteadStateParker1989,
author="Parker, Thomas S.
and Chua, Leon O.",
title="Steady-State Solutions and Limit Sets",
bookTitle="Practical Numerical Algorithms for Chaotic Systems",
year="1989",
publisher="Springer New York",
address="New York, NY",
pages="1--29",
abstract="In this section, we define dynamical systems and present some useful facts from the theory of differential equations.",
isbn="978-1-4612-3486-9",
doi="10.1007/978-1-4612-3486-9_1",
url="https://doi.org/10.1007/978-1-4612-3486-9_1"
}
@Inbook{BookOPLocatingLSParker1989,
author="Parker, Thomas S.
and Chua, Leon O.",
title="Locating Limit Sets",
bookTitle="Practical Numerical Algorithms for Chaotic Systems",
year="1989",
publisher="Springer New York",
address="New York, NY",
pages="115--138",
abstract="The first step in analyzing a dynamical system is to determine the location and stability type of its limit sets. In this chapter, we present algorithms that locate equilibrium points, fixed points, closed orbits, periodic solutions, and two-periodic solutions.",
isbn="978-1-4612-3486-9",
doi="10.1007/978-1-4612-3486-9_5",
url="https://doi.org/10.1007/978-1-4612-3486-9_5"
}



@misc{LearningtoLearn,
      title={Learning to learn by gradient descent by gradient descent}, 
      author={Marcin Andrychowicz and Misha Denil and Sergio Gomez and Matthew W. Hoffman and David Pfau and Tom Schaul and Brendan Shillingford and Nando de Freitas},
      year={2016},
      eprint={1606.04474},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@article{Implicit_RK,
title = "Some schemes for the implementation of implicit Runge-Kutta methods",
journal = "Journal of Computational and Applied Mathematics",
volume = "45",
number = "1",
pages = "213 - 225",
year = "1993",
issn = "0377-0427",
doi = "https://doi.org/10.1016/0377-0427(93)90276-H",
url = "http://www.sciencedirect.com/science/article/pii/037704279390276H",
author = "G.J. Cooper and R. Vignesvaran",
keywords = "Implementation, implicit methods, Runge-Kutta",
abstract = "The nonlinear equations, arising in the implementation of implicit Runge-Kutta methods, may be solved by a modified Newton iteration, but alternative iteration schemes have been suggested to reduce the linear algebra costs. A linear iteration scheme is examined in this article. When applied to an s-stage Runge-Kutta method, each step of the iteration requires s function evaluations and the solution of s sets of linear equations. For the scalar differential equation x′ = qx, the convergence rate of the scheme depends on the spectral radius π[M(z)] of the iteration matrix M, a function of z = hq where h is the steplength. A lower bound for π[M(z)] is established and new schemes are obtained for the two-stage Gauss method by minimizing the supremum of this lower bound over regions of the complex plane. In one scheme the supremum on the negative real axis is minimized. The iteration scheme is generalized in order to obtain improved convergence rates. When applied to an s-stage Runge-Kutta method, each step of this new scheme still requires just s function evaluations. However r sets of linear equations, r > s, have to be solved in each step. Some results are obtained for the Gauss methods and some numerical experiments reported."
}

@Inbook{RK_solvers,
author="Ixaru, Liviu Gr.
and Vanden Berghe, Guido",
title="Runge-Kutta Solvers for Ordinary Differential Equations",
bookTitle="Exponential Fitting",
year="2004",
publisher="Springer Netherlands",
address="Dordrecht",
pages="223--304",
abstract="Since the original papers of Runge [24] and Kutta [17] a great number of papers and books have been devoted to the properties of Runge-Kutta methods. Reviews of this material can be found in [4], [5], [12], [18]. Kutta [17] formulated the general scheme of what is now called a Runge-Kutta method.",
isbn="978-1-4020-2100-8",
doi="10.1007/978-1-4020-2100-8_6",
url="https://doi.org/10.1007/978-1-4020-2100-8_6"
}

@Inbook{B_stability_Book_chap,
author="Hairer, Ernst
and Wanner, Gerhard",
title="B-Stability and Contractivity",
bookTitle="Solving Ordinary Differential Equations II: Stiff and Differential-Algebraic Problems",
year="1996",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="180--200",
abstract="Here we enter a new era, the study of stability and convergence for general nonlinear systems. All the ``crimes'' and diverse omissions of which we have been guilty in earlier sections, especially in Sect. IV.2, shall now be repaired.",
isbn="978-3-642-05221-7",
doi="10.1007/978-3-642-05221-7_12",
url="https://doi.org/10.1007/978-3-642-05221-7_12"
}

@Inbook{Integrating_traj_book_chapter,
author="Parker, Thomas S.
and Chua, Leon O.",
title="Integration of Trajectories",
bookTitle="Practical Numerical Algorithms for Chaotic Systems",
year="1989",
publisher="Springer New York",
address="New York, NY",
pages="83--114",
abstract="The most important numerical task in the simulation of continuous-time dynamical systems is the calculation of trajectories.",
isbn="978-1-4612-3486-9",
doi="10.1007/978-1-4612-3486-9_4",
url="https://doi.org/10.1007/978-1-4612-3486-9_4"
}

@article{yuan2019data,
  title={Data driven discovery of cyber physical systems},
  author={Yuan, Ye and Tang, Xiuchuan and Zhou, Wei and Pan, Wei and Li, Xiuting and Zhang, Hai-Tao and Ding, Han and Goncalves, Jorge},
  journal={Nature communications},
  volume={10},
  number={1},
  pages={1--9},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{PhysRevLett.106.154101,
  title = {Predicting Catastrophes in Nonlinear Dynamical Systems by Compressive Sensing},
  author = {Wang, Wen-Xu and Yang, Rui and Lai, Ying-Cheng and Kovanis, Vassilios and Grebogi, Celso},
  journal = {Phys. Rev. Lett.},
  volume = {106},
  issue = {15},
  pages = {154101},
  numpages = {4},
  year = {2011},
  month-dummy = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.106.154101},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.106.154101}
}

@article {Schmidt81,
	author = {Schmidt, Michael and Lipson, Hod},
	title = {Distilling Free-Form Natural Laws from Experimental Data},
	volume = {324},
	number = {5923},
	pages = {81--85},
	year = {2009},
	doi = {10.1126/science.1165893},
	publisher = {American Association for the Advancement of Science},
	abstract = {For centuries, scientists have attempted to identify and document analytical laws that underlie physical phenomena in nature. Despite the prevalence of computing power, the process of finding natural laws and their corresponding equations has resisted automation. A key challenge to finding analytic relations automatically is defining algorithmically what makes a correlation in observed data important and insightful. We propose a principle for the identification of nontriviality. We demonstrated this approach by automatically searching motion-tracking data captured from various physical systems, ranging from simple harmonic oscillators to chaotic double-pendula. Without any prior knowledge about physics, kinematics, or geometry, the algorithm discovered Hamiltonians, Lagrangians, and other laws of geometric and momentum conservation. The discovery rate accelerated as laws found for simpler systems were used to bootstrap explanations for more complex systems, gradually uncovering the {\textquotedblleft}alphabet{\textquotedblright} used to describe those systems.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/324/5923/81},
	eprint = {https://science.sciencemag.org/content/324/5923/81.full.pdf},
	journal = {Science}
}
@article{pathak2017using,
  title={Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data},
  author={Pathak, Jaideep and Lu, Zhixin and Hunt, Brian R and Girvan, Michelle and Ott, Edward},
  journal={Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume={27},
  number={12},
  pages={121102},
  year={2017},
  publisher={AIP Publishing LLC}
}
@article{maass2002real,
  title={Real-time computing without stable states: A new framework for neural computation based on perturbations},
  author={Maass, Wolfgang and Natschl{\"a}ger, Thomas and Markram, Henry},
  journal={Neural computation},
  volume={14},
  number={11},
  pages={2531--2560},
  year={2002},
  publisher={MIT Press}
}

@article{jaeger2004harnessing,
  title={Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication},
  author={Jaeger, Herbert and Haas, Harald},
  journal={science},
  volume={304},
  number={5667},
  pages={78--80},
  year={2004},
  publisher={American Association for the Advancement of Science}
}

@article{patak_PhysRevLett.120.024102,
  title={Model-free prediction of large spatiotemporally chaotic systems from data: A reservoir computing approach},
  author={Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Lu, Zhixin and Ott, Edward},
  journal={Physical review letters},
  volume={120},
  number={2},
  pages={024102},
  year={2018},
  publisher={APS}
}



@article{shen_all_seq_seq_pred,
author = {Shen,Guorui  and Kurths,Jürgen  and Yuan,Ye },
title = {Sequence-to-sequence prediction of spatiotemporal systems},
journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
volume = {30},
number = {2},
pages = {023102},
year = {2020},
doi = {10.1063/1.5133405},

URL = { 
        https://doi.org/10.1063/1.5133405
    
},
eprint = { 
        https://doi.org/10.1063/1.5133405
    
}
,
    abstract = { We propose a novel type of neural networks known as “attention-based sequence-to-sequence architecture” for a model-free prediction of spatiotemporal systems. This architecture is composed of an encoder and a decoder in which the encoder acts upon a given input sequence and then the decoder yields another output sequence to make a multistep prediction at a time. In order to demonstrate the potential of this approach, we train the neural network using data numerically sampled from the Korteweg–de Vries equation—which describes the interaction between solitary waves—and then predict its future evolution. Furthermore, we validate the applicability of the approach on datasets sampled from the chaotic Lorenz system and three other partial differential equations. The results show that the proposed method can achieve good performance in predicting the evolutionary behavior of studied spatiotemporal dynamics. To the best of our knowledge, this work is the first attempt at applying attention-based sequence-to-sequence architecture to the prediction task of solitary waves. }
}

@misc{gilpin2020deep,
    title={Deep reconstruction of strange attractors from time series},
    author={William Gilpin},
    year={2020},
    eprint={2002.05909},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}



@book{sohrab2003basic,
  title={Basic real analysis},
  author={Sohrab, Houshang H},
  volume={231},
  year={2003},
  publisher={Springer}
}

@inproceedings{mirowski2009dynamic,
  title={Dynamic factor graphs for time series modeling},
  author={Mirowski, Piotr and LeCun, Yann},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={128--143},
  year={2009},
  organization={Springer}
}

@article{yablonsky_limitation_2009,
	title = {Limitation of {One}-{Dimensional} {Ocean} {Models} for {Coupled} {Hurricane}–{Ocean} {Model} {Forecasts}},
	volume = {137},
	issn = {0027-0644},
	url = {https://journals.ametsoc.org/doi/abs/10.1175/2009MWR2863.1},
	doi = {10.1175/2009MWR2863.1},
	abstract = {Wind stress imposed on the upper ocean by a hurricane can limit the hurricane’s intensity primarily through shear-induced mixing of the upper ocean and subsequent cooling of the sea surface. Since shear-induced mixing is a one-dimensional process, some recent studies suggest that coupling a one-dimensional ocean model to a hurricane model may be sufficient for capturing the storm-induced sea surface temperature cooling in the region providing heat energy to the hurricane. Using both a one-dimensional and a three-dimensional version of the same ocean model, it is shown here that the neglect of upwelling, which can only be captured by a three-dimensional ocean model, underestimates the storm-core sea surface cooling for hurricanes translating at {\textless}∼5 m s−1. For hurricanes translating at {\textless}2 m s−1, more than half of the storm-core sea surface cooling is neglected by the one-dimensional ocean model. Since the majority of hurricanes in the western tropical North Atlantic Ocean translate at {\textless}5 m s−1, the idealized experiments presented here suggest that one-dimensional ocean models may be inadequate for coupled hurricane–ocean model forecasting.},
	number = {12},
	urldate = {2018-11-09},
	journal = {Monthly Weather Review},
	author = {Yablonsky, Richard M. and Ginis, Isaac},
	month = dec,
	year = {2009},
	pages = {4410--4419}
}

@article{wang2020incorporating,
  title={Incorporating symmetry into deep dynamics models for improved generalization},
  author={Wang, Rui and Walters, Robin and Yu, Rose},
  journal={arXiv preprint arXiv:2002.03061},
  year={2020}
}
@inproceedings{wang2006gaussian,
  title={Gaussian process dynamical models},
  author={Wang, Jack and Hertzmann, Aaron and Fleet, David J},
  booktitle={Advances in neural information processing systems},
  pages={1441--1448},
  year={2006}
}

@article{ubelmann2015dynamic,
  title={Dynamic interpolation of sea surface height and potential applications for future high-resolution altimetry mapping},
  author={Ubelmann, Clement and Klein, Patrice and Fu, Lee-Lueng},
  journal={Journal of Atmospheric and Oceanic Technology},
  volume={32},
  number={1},
  pages={177--184},
  year={2015}
}

@article{barth2020dincae,
  title={DINCAE 1.0: a convolutional neural network with error estimates to reconstruct sea surface temperature satellite observations},
  author={Barth, Alexander and Alvera Azcarate, Aida and Licer, Matjaz and Beckers, Jean-Marie},
  journal={Geoscientific Model Development},
  volume={13},
  number={3},
  pages={1609--1622},
  year={2020},
  publisher={Copernicus Gesellschaften}
}

@article{frezat2020physical,
  title={Physical invariance in neural networks for subgrid-scale scalar flux modeling},
  author={Frezat, Hugo and Balarac, Guillaume and Sommer, Julien Le and Fablet, Ronan and Lguensat, Redouane},
  journal={arXiv preprint arXiv:2010.04663},
  year={2020}
}

@inproceedings{lguensat2018eddynet,
  title={EddyNet: A deep neural network for pixel-wise classification of oceanic eddies},
  author={Lguensat, Redouane and Sun, Miao and Fablet, Ronan and Tandeo, Pierre and Mason, Evan and Chen, Ge},
  booktitle={IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium},
  pages={1764--1767},
  year={2018},
  organization={IEEE}
}

@article{tandeo2014sst,
  title={SST spatial anisotropic covariances from METOP-AVHRR data},
  author={Tandeo, Pierre and Autret, Emmanuelle and Chapron, Bertrand and Fablet, Ronan and Garello, Ren{\'e}},
  journal={Remote Sensing of Environment},
  volume={141},
  pages={144--148},
  year={2014},
  publisher={Elsevier}
}

@article{donlon2002toward,
  title={Toward improved validation of satellite sea surface skin temperature measurements for climate research},
  author={Donlon, CJ and Minnett, PJ and Gentemann, Chelle and Nightingale, TJ and Barton, IJ and Ward, B and Murray, MJ},
  journal={Journal of Climate},
  volume={15},
  number={4},
  pages={353--369},
  year={2002}
}

@article{pascual2006improved,
  title={Improved description of the ocean mesoscale variability by combining four satellite altimeters},
  author={Pascual, Ananda and Faug{\`e}re, Yannice and Larnicol, Gilles and Le Traon, Pierre-Yves},
  journal={Geophysical Research Letters},
  volume={33},
  number={2},
  year={2006},
  publisher={Wiley Online Library}
}

@article{andrychowicz2016learning,
  title={Learning to learn by gradient descent by gradient descent},
  author={Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and De Freitas, Nando},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={3981--3989},
  year={2016}
}

@misc{fablet2020endtoend2joint,
      title={Joint learning of variational representations and solvers for inverse problems with partially-observed data}, 
      author={Ronan Fablet and Lucas Drumetz and Francois Rousseau},
      year={2020},
      eprint={2006.03653},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{mccann2017convolutional,
  title={Convolutional neural networks for inverse problems in imaging: A review},
  author={McCann, Michael T and Jin, Kyong Hwan and Unser, Michael},
  journal={IEEE Signal Processing Magazine},
  volume={34},
  number={6},
  pages={85--95},
  year={2017},
  publisher={IEEE}
}
@article{lucas2018using,
  title={Using deep neural networks for inverse problems in imaging: beyond analytical methods},
  author={Lucas, Alice and Iliadis, Michael and Molina, Rafael and Katsaggelos, Aggelos K},
  journal={IEEE Signal Processing Magazine},
  volume={35},
  number={1},
  pages={20--36},
  year={2018},
  publisher={IEEE}
}

@misc{fablet2019endtoend1,
      title={End-to-end learning of energy-based representations for irregularly-sampled signals and images}, 
      author={Ronan Fablet and Lucas Drumetz and François Rousseau},
      year={2019},
      eprint={1910.00556},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{ruiz2019effects,
  title={Effects of oceanic mesoscale and submesoscale frontal processes on the vertical transport of phytoplankton},
  author={Ruiz, Sim{\'o}n and Claret, Mariona and Pascual, Ananda and Olita, Antonio and Troupin, Charles and Capet, Arthur and Tovar-S{\'a}nchez, Antonio and Allen, John and Poulain, Pierre-Marie and Tintor{\'e}, Joaqu{\'i}n and others},
  journal={Journal of Geophysical Research: Oceans},
  volume={124},
  number={8},
  pages={5999--6014},
  year={2019},
  publisher={Wiley Online Library}
}

@article{escudier2013improvement,
  title={Improvement of coastal and mesoscale observation from space: Application to the northwestern Mediterranean Sea},
  author={Escudier, Romain and Bouffard, J{\'e}r{\^o}me and Pascual, Ananda and Poulain, Pierre-Marie and Pujol, Marie-Isabelle},
  journal={Geophysical Research Letters},
  volume={40},
  number={10},
  pages={2148--2153},
  year={2013},
  publisher={Wiley Online Library}
}

@Article{Duacs_reso,
AUTHOR = {Ballarotta, M. and Ubelmann, C. and Pujol, M.-I. and Taburet, G. and Fournier, F. and Legeais, J.-F. and Faug\`ere, Y. and Delepoulle, A. and Chelton, D. and Dibarboure, G. and Picot, N.},
TITLE = {On the resolutions of ocean altimetry maps},
JOURNAL = {Ocean Science},
VOLUME = {15},
YEAR = {2019},
NUMBER = {4},
PAGES = {1091--1109},
URL = {https://os.copernicus.org/articles/15/1091/2019/},
DOI = {10.5194/os-15-1091-2019}
}

@article{ruiz2019effects,
  title={Effects of oceanic mesoscale and submesoscale frontal processes on the vertical transport of phytoplankton},
  author={Ruiz, Sim{\'o}n and Claret, Mariona and Pascual, Ananda and Olita, Antonio and Troupin, Charles and Capet, Arthur and Tovar-S{\'a}nchez, Antonio and Allen, John and Poulain, Pierre-Marie and Tintor{\'e}, Joaqu{\'\i}n and others},
  journal={Journal of Geophysical Research: Oceans},
  volume={124},
  number={8},
  pages={5999--6014},
  year={2019},
  publisher={Wiley Online Library}
}
@misc{ma2020combined,
      title={A Combined Data-driven and Physics-driven Method for Steady Heat Conduction Prediction using Deep Convolutional Neural Networks}, 
      author={Hao Ma and Xiangyu Hu and Yuxuan Zhang and Nils Thuerey and Oskar J. Haidn},
      year={2020},
      eprint={2005.08119},
      archivePrefix={arXiv},
      primaryClass={physics.comp-ph}
}
@article{dussurget2011fine,
  title={Fine resolution altimetry data for a regional application in the Bay of Biscay},
  author={Dussurget, Renaud and Birol, Florence and Morrow, Rosemary and Mey, Pierre De},
  journal={Marine Geodesy},
  volume={34},
  number={3-4},
  pages={447--476},
  year={2011},
  publisher={Taylor \& Francis}
}
@ARTICLE{Pascual_OP_paper,
  
AUTHOR={Pascual, Ananda and Ruiz, Simon and Olita, Antonio and Troupin, Charles and Claret, Mariona and Casas, Benjamin and Mourre, Baptiste and Poulain, Pierre-Marie and Tovar-Sanchez, Antonio and Capet, Arthur and Mason, Evan and Allen, John T. and Mahadevan, Amala and Tintoré, Joaquín},   
	 
TITLE={A Multiplatform Experiment to Unravel Meso- and Submesoscale Processes in an Intense Front (AlborEx)},      
	
JOURNAL={Frontiers in Marine Science},      
	
VOLUME={4},      

PAGES={39},     
	
YEAR={2017},      
	  
URL={https://www.frontiersin.org/article/10.3389/fmars.2017.00039},       
	
DOI={10.3389/fmars.2017.00039},      
	
ISSN={2296-7745},   
   
ABSTRACT={The challenges associated with meso- and submesoscale variability (between 1 and 100 km) require high-resolution observations and integrated approaches. Here we describe a major oceanographic experiment designed to capture the intense but transient vertical motions in an area characterized by strong fronts. Finescale processes were studied in the eastern Alboran Sea (Western Mediterranean) about 400 km east of the Strait of Gibraltar, a relatively sparsely sampled area. In-situ systems were coordinated with satellite data and numerical simulations to provide a full description of the physical and biogeochemical variability. Hydrographic data confirmed the presence of an intense salinity front formed by the confluence of Atlantic Waters, entering from Gibraltar, with the local Mediterranean waters. The drifters coherently followed the northeastern limb of an anticyclonic gyre. Near real time data from acoustic current meter data profiler showed consistent patterns with currents of up to 1 m/s in the southern part of the sampled domain. High-resolution glider data revealed submesoscale structures with tongues of chlorophyll-a and oxygen associated with the frontal zone. Numerical results show large vertical excursions of tracers that could explain the subducted tongues and filaments captured by ocean gliders. A unique aspect of AlborEx is the combination of high-resolution synoptic measurements of vessel-based measurements, autonomous sampling, remote sensing and modeling, enabling the evaluation of the underlying mechanisms responsible for the observed distributions and biogeochemical patchiness. The main findings point to the importance of fine-scale processes enhancing the vertical exchanges between the upper ocean and the ocean interior.}
}
@inproceedings{chapron2008ocean,
  title={Ocean remote sensing: Challenges for the future},
  author={Chapron, Bertrand and Garello, Rene and Weissman, David E},
  booktitle={OCEANS 2008},
  pages={1--7},
  year={2008},
  organization={IEEE}
}
@article{le2015use,
  title={Use of satellite observations for operational oceanography: recent achievements and future prospects},
  author={Le Traon, P-Y and Antoine, David and Bentamy, Abderrahim and Bonekamp, H and Breivik, LA and Chapron, Bertrand and Corlett, G and Dibarboure, G and DiGiacomo, P and Donlon, C and others},
  journal={Journal of Operational Oceanography},
  volume={8},
  number={sup1},
  pages={s12--s27},
  year={2015},
  publisher={Taylor \& Francis}
}
@article{LIN2008913,
title = "An overview on SAR measurements of sea surface wind",
journal = "Progress in Natural Science",
volume = "18",
number = "8",
pages = "913 - 919",
year = "2008",
issn = "1002-0071",
doi = "https://doi.org/10.1016/j.pnsc.2008.03.008",
url = "http://www.sciencedirect.com/science/article/pii/S1002007108001755",
author = "Hui Lin and Qing Xu and Quanan Zheng",
keywords = "SAR, Sea surface wind, Wind retrieval algorithms",
abstract = "Studies show that synthetic aperture radar (SAR) has the capability of providing high-resolution (sub-kilometer) sea surface wind fields. This is very useful for applications where knowledge of the sea surface wind at fine scales is crucial. This paper aims to review the latest work on sea surface wind field retrieval using SAR images. As shown, many different approaches have been developed for retrieving wind speed and wind direction. However, much more work will be required to fully exploit the SAR data for improving the retrieval accuracy of high-resolution winds and for producing wind products in an operational sense."
}
@article{yueh2006polarimetric,
  title={Polarimetric microwave wind radiometer model function and retrieval testing for WindSat},
  author={Yueh, Simon H and Wilson, William J and Dinardo, Steve J and Hsiao, S Vincent},
  journal={IEEE transactions on geoscience and remote sensing},
  volume={44},
  number={3},
  pages={584--596},
  year={2006},
  publisher={IEEE}
}
@inproceedings{le2007operational,
  title={Operational SST retrieval from MetOp/AVHRR},
  author={Le Borgne, P and Legendre, G and Marsouin, A},
  booktitle={Proc. 2007 EUMETSAT Conf., Amsterdam, the Netherlands},
  year={2007},
  organization={Citeseer}
}

@article{tandeo2018joint,
  title={Joint estimation of model and observation error covariance matrices in data assimilation: a review},
  author={Tandeo, Pierre and Ailliot, Pierre and Bocquet, Marc and Carrassi, Alberto and Miyoshi, Takemasa and Pulido, Manuel and Zhen, Yicun},
  journal={arXiv preprint arXiv:1807.11221},
  year={2018}
}
@article{marsouin2015six,
  title={Six years of OSI-SAF METOP-A AVHRR sea surface temperature},
  author={Marsouin, Anne and Le Borgne, Pierre and Legendre, G{\'e}rard and P{\'e}r{\'e}, Sonia and Roquet, Herv{\'e}},
  journal={Remote Sensing of Environment},
  volume={159},
  pages={288--306},
  year={2015},
  publisher={Elsevier}
}

@book{soton9866,
           title = {Measuring the oceans from space: the principles and methods of satellite oceanography},
          author = {I.S. Robinson},
       publisher = {Springer/Praxis Publishing},
            year = {2004},
             url = {https://eprints.soton.ac.uk/9866/}
}


@article{lee2018satellite,
  title={Satellite sst and sss observations and their roles to constrain ocean models},
  author={Lee, Tong and Gentemann, Chelle},
  journal={New Frontiers in Operational Oceanography},
  pages={271--288},
  year={2018}
}

@article{verron1999extended,
  title={An extended Kalman filter to assimilate satellite altimeter data into a nonlinear numerical model of the tropical Pacific Ocean: Method and validation},
  author={Verron, J and Gourdeau, Lionel and Pham, DT and Murtugudde, R and Busalacchi, AJ},
  journal={Journal of Geophysical Research: Oceans},
  volume={104},
  number={C3},
  pages={5441--5458},
  year={1999},
  publisher={Wiley Online Library}
}
@article{morrow2012recent,
  title={Recent advances in observing mesoscale ocean dynamics with satellite altimetry},
  author={Morrow, Rosemary and Le Traon, Pierre-Yves},
  journal={Advances in Space Research},
  volume={50},
  number={8},
  pages={1062--1076},
  year={2012},
  publisher={Elsevier}
}

@book{koblinsky1992future,
  title={The Future of Spaceborne Altimetry: Oceans and Climate Change: a Long-term Strategy, a Report},
  author={Koblinsky, Chester John and Gaspar, P and Lagerloef, Gary SE},
  year={1992},
  publisher={Joint Oceanographic Institutions Incorporated}
}

@inproceedings{ghahramani1999learning,
  title={Learning nonlinear dynamical systems using an EM algorithm},
  author={Ghahramani, Zoubin and Roweis, Sam T},
  booktitle={Advances in neural information processing systems},
  pages={431--437},
  year={1999}
}
@Article{os-10-281-2014,
AUTHOR = {Malanotte-Rizzoli, P. and Artale, V. and Borzelli-Eusebi, G. L. and Brenner, S. and Crise, A. and Gacic, M. and Kress, N. and Marullo, S. and Ribera d'Alcal\`a, M. and Sofianos, S. and Tanhua, T. and Theocharis, A. and Alvarez, M. and Ashkenazy, Y. and Bergamasco, A. and Cardin, V. and Carniel, S. and Civitarese, G. and D'Ortenzio, F. and Font, J. and Garcia-Ladona, E. and Garcia-Lafuente, J. M. and Gogou, A. and Gregoire, M. and Hainbucher, D. and Kontoyannis, H. and Kovacevic, V. and Kraskapoulou, E. and Kroskos, G. and Incarbona, A. and Mazzocchi, M. G. and Orlic, M. and Ozsoy, E. and Pascual, A. and Poulain, P.-M. and Roether, W. and Rubino, A. and Schroeder, K. and Siokou-Frangou, J. and Souvermezoglou, E. and Sprovieri, M. and Tintor\'e, J. and Triantafyllou, G.},
TITLE = {Physical forcing and physical/biochemical variability of the Mediterranean Sea: a review of unresolved issues and directions for future research},
JOURNAL = {Ocean Science},
VOLUME = {10},
YEAR = {2014},
NUMBER = {3},
PAGES = {281--322},
URL = {https://os.copernicus.org/articles/10/281/2014/},
DOI = {10.5194/os-10-281-2014}
}


@article{nguyen2020variational,
  title={Variational Deep Learning for the Identification and Reconstruction of Chaotic and Stochastic Dynamical Systems from Noisy and Partial Observations},
  author={Nguyen, Duong and Ouala, Said and Drumetz, Lucas and Fablet, Ronan},
  journal={arXiv preprint arXiv:2009.02296},
  year={2020}
}
@article{bocquet2019data,
  title={Data assimilation as a learning tool to infer ordinary differential equation representations of dynamical models},
  author={Bocquet, Marc and Brajard, Julien and Carrassi, Alberto and Bertino, Laurent},
  journal={Nonlinear Processes in Geophysics},
  volume={26},
  number={3},
  pages={143--162},
  year={2019},
  publisher={Copernicus GmbH}
}

@inproceedings{wang2020towards,
  title={Towards physics-informed deep learning for turbulent flow prediction},
  author={Wang, Rui and Kashinath, Karthik and Mustafa, Mustafa and Albert, Adrian and Yu, Rose},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={1457--1466},
  year={2020}
}
@inbook{holmes_lumley_berkooz_rowley_2012, place={Cambridge}, edition={2}, series={Cambridge Monographs on Mechanics}, title={Galerkin projection}, DOI={10.1017/CBO9780511919701.006}, booktitle={Turbulence, Coherent Structures, Dynamical Systems and Symmetry}, publisher={Cambridge University Press}, author={Holmes, Philip and Lumley, John L. and Berkooz, Gahl and Rowley, Clarence W.}, year={2012}, pages={106–129}, collection={Cambridge Monographs on Mechanics}}

@article{fabius2014variational,
  title={Variational recurrent auto-encoders},
  author={Fabius, Otto and van Amersfoort, Joost R},
  journal={arXiv preprint arXiv:1412.6581},
  year={2014}
}


@book{coddington1955theory,
  title={Theory of ordinary differential equations},
  author={Coddington, Earl A and Levinson, Norman},
  year={1955},
  publisher={Tata McGraw-Hill Education}
}

@article{zou1992incomplete,
  title={Incomplete observations and control of gravity waves in variational data assimilation},
  author={Zou, X and Navon, IM and Le Dimet, FX},
  journal={Tellus A: Dynamic meteorology and oceanography},
  volume={44},
  number={4},
  pages={273--296},
  year={1992},
  publisher={Taylor \& Francis}
}

@article{reichle2008data,
  title={Data assimilation methods in the Earth sciences},
  author={Reichle, Rolf H},
  journal={Advances in water resources},
  volume={31},
  number={11},
  pages={1411--1418},
  year={2008},
  publisher={Elsevier}
}

@article{dupont2019augmented,
  title={Augmented neural odes},
  author={Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1904.01681},
  year={2019}
}

@article{zhang2019approximation,
  title={Approximation Capabilities of Neural Ordinary Differential Equations},
  author={Zhang, Han and Gao, Xi and Unterman, Jacob and Arodz, Tom},
  journal={arXiv preprint arXiv:1907.12998},
  year={2019}
}

@INPROCEEDINGS{fablet_blin_ieee,
author={R. {Fablet} and S. {Ouala} and C. {Herzet}},
booktitle={2018 26th European Signal Processing Conference (EUSIPCO)},
title={Bilinear Residual Neural Network for the Identification and Forecasting of Geophysical Dynamics},
year={2018},
volume={},
number={},
pages={1477-1481},
keywords={differential equations;geophysics computing;neural nets;Runge-Kutta methods;forecasting performance;model identification;bilinear residual neural network;geophysical dynamics;large-scale observations;model-driven models;ordinary differential equations;physically-sound data-driven representations;graphical models;residual NN architecture;bilinear layers;computation representations;Runge-Kutta methods;Artificial neural networks;Forecasting;Mathematical model;Predictive models;Computational modeling;Computer architecture;Dynamical systems;neural networks;Bilinear layer;Forecasting;ODE;Runge-Kutta methods},
doi={10.23919/EUSIPCO.2018.8553492},
ISSN={2076-1465},
month={Sep.},}

@unpublished{ouala:hal-02005399,
  TITLE = {{RESIDUAL INTEGRATION NEURAL NETWORK}},
  AUTHOR = {Ouala, Said and Pascual, Ananda and Fablet, Ronan},
  URL = {https://hal-imt-atlantique.archives-ouvertes.fr/hal-02005399},
  NOTE = {Preprint},
  YEAR = {2019},
  month-dummy = Feb,
  KEYWORDS = {Dynamical systems ; Data-driven models ; Neural networks ; Forecasting},
  PDF = {https://hal-imt-atlantique.archives-ouvertes.fr/hal-02005399/file/Integration_Residual_Net%281%29.pdf},
  HAL_ID = {hal-02005399},
  HAL_VERSION = {v1},
}

@article{raissi2018multistep,
  title={Multistep neural networks for data-driven discovery of nonlinear dynamical systems},
  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
  journal={arXiv preprint arXiv:1801.01236},
  year={2018}
}


@misc{wiewel2018latentspace,
    title={Latent-space Physics: Towards Learning the Temporal Evolution of Fluid Flow},
    author={Steffen Wiewel and Moritz Becher and Nils Thuerey},
    year={2018},
    eprint={1802.10123},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Sauer1991,
  title={Embedology},
  author={Sauer, Tim and Yorke, James A and Casdagli, Martin},
  journal={Journal of statistical Physics},
  volume={65},
  number={3},
  pages={579--616},
  year={1991},
  publisher={Springer}
}



@article{chapron2018large,
  title={Large-scale flows under location uncertainty: a consistent stochastic framework},
  author={Chapron, Bertrand and D{\'e}rian, Pierre and M{\'e}min, Etienne and Resseguier, Valentin},
  journal={Quarterly Journal of the Royal Meteorological Society},
  volume={144},
  number={710},
  pages={251--260},
  year={2018},
  publisher={Wiley Online Library}
}




@article{rice2020analyzing,
  title={Analyzing Koopman approaches to physics-informed machine learning for long-term sea-surface temperature forecasting},
  author={Rice, Julian and Xu, Wenwei and August, Andrew},
  journal={arXiv preprint arXiv:2010.00399},
  year={2020}
}

@article{chen2012variants,
  title={Variants of dynamic mode decomposition: boundary condition, Koopman, and Fourier analyses},
  author={Chen, Kevin K and Tu, Jonathan H and Rowley, Clarence W},
  journal={Journal of nonlinear science},
  volume={22},
  number={6},
  pages={887--915},
  year={2012},
  publisher={Springer}
}

@article{hemati2017biasing,
  title={De-biasing the dynamic mode decomposition for applied Koopman spectral analysis of noisy datasets},
  author={Hemati, Maziar S and Rowley, Clarence W and Deem, Eric A and Cattafesta, Louis N},
  journal={Theoretical and Computational Fluid Dynamics},
  volume={31},
  number={4},
  pages={349--368},
  year={2017},
  publisher={Springer}
}

@article{brunton2016koopman,
  title={Koopman invariant subspaces and finite linear representations of nonlinear dynamical systems for control},
  author={Brunton, Steven L and Brunton, Bingni W and Proctor, Joshua L and Kutz, J Nathan},
  journal={PloS one},
  volume={11},
  number={2},
  pages={e0150171},
  year={2016},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{kamb2020timeDobsKoopman,
  title={Time-delay observables for koopman: Theory and applications},
  author={Kamb, Mason and Kaiser, Eurika and Brunton, Steven L and Kutz, J Nathan},
  journal={SIAM Journal on Applied Dynamical Systems},
  volume={19},
  number={2},
  pages={886--917},
  year={2020},
  publisher={SIAM}
}

@article{lange2020fourierForecast,
  title={From Fourier to Koopman: Spectral Methods for Long-term Time Series Prediction},
  author={Lange, Henning and Brunton, Steven L and Kutz, Nathan},
  journal={arXiv preprint arXiv:2004.00574},
  year={2020}
}

@article{lusch2017data,
  title={Data-driven discovery of Koopman eigenfunctions using deep learning},
  author={Lusch, Bethany and Brunton, Steven L and Kutz, J Nathan},
  journal={APS},
  pages={M1--006},
  year={2017}
}

@article{arbabi2017ergodic,
  title={Ergodic theory, dynamic mode decomposition, and computation of spectral properties of the Koopman operator},
  author={Arbabi, Hassan and Mezic, Igor},
  journal={SIAM Journal on Applied Dynamical Systems},
  volume={16},
  number={4},
  pages={2096--2126},
  year={2017},
  publisher={SIAM}
}

@article{proctor2018generalizing,
  title={Generalizing Koopman theory to allow for inputs and control},
  author={Proctor, Joshua L and Brunton, Steven L and Kutz, J Nathan},
  journal={SIAM Journal on Applied Dynamical Systems},
  volume={17},
  number={1},
  pages={909--930},
  year={2018},
  publisher={SIAM}
}

@article{brunton2017chaos,
  title={Chaos as an intermittently forced linear system},
  author={Brunton, Steven L and Brunton, Bingni W and Proctor, Joshua L and Kaiser, Eurika and Kutz, J Nathan},
  journal={Nature communications},
  volume={8},
  number={1},
  pages={1--9},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{li2017extended,
  title={Extended dynamic mode decomposition with dictionary learning: A data-driven adaptive spectral decomposition of the Koopman operator},
  author={Li, Qianxiao and Dietrich, Felix and Bollt, Erik M and Kevrekidis, Ioannis G},
  journal={Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume={27},
  number={10},
  pages={103111},
  year={2017},
  publisher={AIP Publishing LLC}
}

@article{chen2012variants,
  title={Variants of dynamic mode decomposition: boundary condition, Koopman, and Fourier analyses},
  author={Chen, Kevin K and Tu, Jonathan H and Rowley, Clarence W},
  journal={Journal of nonlinear science},
  volume={22},
  number={6},
  pages={887--915},
  year={2012},
  publisher={Springer}
}

@article{schmid2010dynamic,
  title={Dynamic mode decomposition of numerical and experimental data},
  author={Schmid, Peter J},
  journal={Journal of fluid mechanics},
  volume={656},
  pages={5--28},
  year={2010},
  publisher={Cambridge University Press}
}

@article{koopman1931hamiltonian,
  title={Hamiltonian systems and transformation in Hilbert space},
  author={Koopman, Bernard O},
  journal={Proceedings of the national academy of sciences of the united states of america},
  volume={17},
  number={5},
  pages={315},
  year={1931},
  publisher={National Academy of Sciences}
}

@article{spec_analysismezic,
  title={Spectral analysis of nonlinear flows},
  author={Rowley, Clarence W and MEZI?, IGOR and Bagheri, Shervin and Schlatter, Philipp and Henningson, Dans and others},
  journal={Journal of fluid mechanics},
  volume={641},
  number={1},
  pages={115--127},
  year={2009},
  publisher={Citeseer}
}

@article{brunton2019notes,
  title={Notes on Koopman Operator Theory},
  author={Brunton, Steven L.},
  year={2019}
}

@article{HORNIK1989359,
title = "Multilayer feedforward networks are universal approximators",
journal = "Neural Networks",
volume = "2",
number = "5",
pages = "359 - 366",
year = "1989",
issn = "0893-6080",
doi = "https://doi.org/10.1016/0893-6080(89)90020-8",
url = "http://www.sciencedirect.com/science/article/pii/0893608089900208",
author = "Kurt Hornik and Maxwell Stinchcombe and Halbert White",
keywords = "Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks",
abstract = "This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators."
}
@article{jin2021nsfnets,
  title={NSFnets (Navier-Stokes flow nets): Physics-informed neural networks for the incompressible Navier-Stokes equations},
  author={Jin, Xiaowei and Cai, Shengze and Li, Hui and Karniadakis, George Em},
  journal={Journal of Computational Physics},
  volume={426},
  pages={109951},
  year={2021},
  publisher={Elsevier}
}
@book{cartwright2008using,
  title={Using artificial intelligence in chemistry and biology: a practical guide},
  author={Cartwright, Hugh},
  year={2008},
  publisher={CRC Press}
}

@book{hirsch1974differential,
  title={Differential equations, dynamical systems, and linear algebra},
  author={Hirsch, Morris W and Smale, Stephen and Devaney, Robert L},
  volume={60},
  year={1974},
  publisher={Academic press}
}

@article{lyapunov1992general,
  title={The general problem of the stability of motion},
  author={Lyapunov, Aleksandr Mikhailovich},
  journal={International journal of control},
  volume={55},
  number={3},
  pages={531--534},
  year={1992},
  publisher={Taylor \& Francis}
}

@book{trippi1995artificial,
  title={Artificial intelligence in finance and investing: state-of-the-art technologies for securities selection and portfolio management},
  author={Trippi, Robert R and Preface By-Lee, Jae K},
  year={1995},
  publisher={McGraw-Hill, Inc.}
}



@article{Takens-params-2,
  title={Choosing the dimension of reconstructed phase space},
  author={Abarbanel, Henry DI and Abarbanel, Henry DI},
  journal={Analysis of observed chaotic data},
  pages={39--67},
  year={1996},
  publisher={Springer}
}

@inproceedings{chen2018neural,
  title={Neural ordinary differential equations},
  author={Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6571--6583},
  year={2018}
}
@misc{taylor2019episodic,
      title={Episodic Learning with Control Lyapunov Functions for Uncertain Robotic Systems}, 
      author={Andrew J. Taylor and Victor D. Dorobantu and Hoang M. Le and Yisong Yue and Aaron D. Ames},
      year={2019},
      eprint={1903.01577},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}
@misc{manek2020learning,
      title={Learning Stable Deep Dynamics Models}, 
      author={Gaurav Manek and J. Zico Kolter},
      year={2020},
      eprint={2001.06116},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@Article{Calmant2008,
author="Calmant, St{\'e}phane
and Seyler, Fr{\'e}d{\'e}rique
and Cretaux, Jean Fran{\c{c}}ois",
title="Monitoring Continental Surface Waters by Satellite Altimetry",
journal="Surveys in Geophysics",
year="2008",
month="Oct",
day="01",
volume="29",
number="4",
pages="247--269",
abstract="The monitoring of continental water stages is a requirement for meeting human needs and assessing ongoing climatic changes. However, regular gauging networks fail to provide the information needed for spatial coverage and timely delivery. Although the space missions discussed here were not primarily dedicated to hydrology, 18 years of satellite altimetry have furnished complementary data that can be used to create hydrological products, such as time series of stages, estimated discharges of rivers or volume change of lakes, river altitude profiles or leveling of in situ stations. Raw data still suffer uncertainties of one to several decimeters. These require specific reprocessing such as waveform retracking or geophysical correction editing; much work still remains to be done. Besides, measuring the flow velocity appears feasible owing to SAR interferometer techniques. Inundated surfaces, and the time variations of their extent, are currently almost routinely computed using satellite imagery. Thus, the compilation of the continuous efforts of the scientific community in these various investigative directions, such as recording from space the discharges of rivers or the change in water volume stored in lakes, can be foreseen in the near future.",
issn="1573-0956",
doi="10.1007/s10712-008-9051-1",
url="https://doi.org/10.1007/s10712-008-9051-1"
}


@article{WMOP_soc,
author = {M. Juza and B. Mourre and L. Renault and S. Gómara and K. Sebastián and S. Lora and J. P. Beltran and B. Frontera and B. Garau and C. Troupin and M. Torner and E. Heslop and B. Casas and R. Escudier and G. Vizoso and J. Tintoré},
title = {SOCIB operational ocean forecasting system and multi-platform validation in the Western Mediterranean Sea},
journal = {Journal of Operational Oceanography},
volume = {9},
number = {sup1},
pages = {s155-s166},
year  = {2016},
publisher = {Taylor & Francis},
doi = {10.1080/1755876X.2015.1117764},

URL = { 
        https://doi.org/10.1080/1755876X.2015.1117764
    
},
eprint = { 
        https://doi.org/10.1080/1755876X.2015.1117764
    
}}

@INPROCEEDINGS{Res_INN,
author={S. {Ouala} and A. {Pascual} and R. {Fablet}},
booktitle={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Residual Integration Neural Network},
year={2019},
volume={},
number={},
pages={3622-3626},
abstract={In this work, we investigate residual neural network representations for the identification and forecasting of dynamical systems. We propose a novel architecture that jointly learns the dynamical model and the associated Runge-Kutta integration scheme. We demonstrate the relevance of the proposed architecture with respect to learning-based state-of-the-art approaches in the identification and forecasting of chaotic dynamics when provided with training data with low temporal sampling rates.},
keywords={Dynamical systems;Data-driven models;Neural networks;Forecasting;Runge-Kutta methods},
doi={10.1109/ICASSP.2019.8683447},
ISSN={2379-190X},
month={May},}

@Inbook{takens-params-1,
author="Abarbanel, Henry D. I.",
title="Choosing Time Delays",
bookTitle="Analysis of Observed Chaotic Data",
year="1996",
publisher="Springer New York",
address="New York, NY",
pages="25--37",
isbn="978-1-4612-0763-4"
}

@article{brunton2016koopman,
  title={Koopman invariant subspaces and finite linear representations of nonlinear dynamical systems for control},
  author={Brunton, Steven L and Brunton, Bingni W and Proctor, Joshua L and Kutz, J Nathan},
  journal={PloS one},
  volume={11},
  number={2},
  pages={e0150171},
  year={2016},
  publisher={Public Library of Science}
}

@book{Sprott_chaos,
 author = {Sprott, Julien Clinton},
 title = {Chaos and Time-Series Analysis},
 year = {2003},
 isbn = {0198508409},
 publisher = {Oxford University Press, Inc.},
 address = {New York, NY, USA},
} 

@article{KoopmanGeneral,
 ISSN = {00278424},
 URL = {http://www.jstor.org/stable/86114},
 author = {B. O. Koopman},
 journal = {Proceedings of the National Academy of Sciences of the United States of America},
 number = {5},
 pages = {315--318},
 publisher = {National Academy of Sciences},
 title = {Hamiltonian Systems and Transformations in Hilbert Space},
 volume = {17},
 year = {1931}
}



@misc{large_flow_stoch,
type = "Article",
year = "2018",
title = "Large-scale flows under location uncertainty: a consistent stochastic framework",
journal = "Quarterly Journal Of The Royal Meteorological Society",
editor = "Wiley",
volume = "144",
number = "710",
pages = "251-260",
author = "Chapron Bertrand, Derian P., Memin E., Resseguier Valentin",
url = "",
organization = "",
address = "FRANCE",
doi = "https://doi.org/10.1002/qj.3198",
abstract = "<p>Using a classical example, the Lorenz-63 model, an original stochastic framework is applied to represent large-scale geophysical flow dynamics. Rigorously derived from a reformulated material derivative, the proposed framework encompasses several meaningful mechanisms to model geophysical flows. The slightly compressible set-up, as treated in the Boussinesq approximation, yields a stochastic transport equation for the density and other related thermodynamical variables. Coupled to the momentum equation through a forcing term, the resulting stochastic Lorenz-63 model is derived consistently. Based on such a reformulated model, the pertinence of this large-scale stochastic approach is demonstrated over classical eddy-viscosity based large-scale representations.</p>",
key = ""
}

@Inbook{Lynch2010,
author="Lynch, Peter
and Huang, Xiang-Yu",
editor="Lahoz, William
and Khattatov, Boris
and Menard, Richard",
title="Initialization",
bookTitle="Data Assimilation: Making Sense of Observations",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="241--260",
abstract="The spectrum of atmospheric motions is vast, encompassing phenomena having periods ranging from seconds to millennia. The motions of interest to the forecaster typically have timescales of a day or longer, but the mathematical models used for numerical prediction describe a broader span of dynamical features than those of direct concern. For many purposes these higher frequency components can be regarded as noise contaminating the motions of meteorological interest. The elimination of this noise is achieved by adjustment of the initial fields, a process called initialization.",
isbn="978-3-540-74703-1",
doi="10.1007/978-3-540-74703-1_9",
url="https://doi.org/10.1007/978-3-540-74703-1_9"
}

@inproceedings{ouala_hal_02285700,
  TITLE = {{Learning Constrained Dynamical Embeddings for Geophysical Dynamics}},
  AUTHOR = {Ouala, Said and Brunton, Steven L and Nguyen, Duong and Drumetz, Lucas and Fablet, Ronan},
  URL = {https://hal-imt-atlantique.archives-ouvertes.fr/hal-02285700},
  BOOKTITLE = {{CI 2019 : 9th International Workshop on Climate Informatics}},
  ADDRESS = {Paris, France},
  YEAR = {2019},
  PDF = {https://hal-imt-atlantique.archives-ouvertes.fr/hal-02285700/file/CI2019_paper_46.pdf},
  HAL_ID = {hal-02285700},
  HAL_VERSION = {v1},
}






@inproceedings{NN_takens,
 author = {Frank, Jordan and Mannor, Shie and Precup, Doina},
 title = {Activity and Gait Recognition with Time-delay Embeddings},
 booktitle = {Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence},
 series = {AAAI'10},
 year = {2010},
 location = {Atlanta, Georgia},
 pages = {1581--1586},
 numpages = {6},
 acmid = {2898859},
 publisher = {AAAI Press},
} 


@Inbook{Abarbanel_model_chaos,
author="Abarbanel, Henry D. I.",
title="Modeling Chaos",
bookTitle="Analysis of Observed Chaotic Data",
year="1996",
publisher="Springer New York",
address="New York, NY",
pages="95--114",
abstract="We next discuss making models for prediction or control of the source of the observed chaotic signal. In a sense this is both the easiest and the hardest task we have discussed. It is the easiest because it is quite simple to make models of the dynamics which very accurately allow one to predict forward in time from any new initial condition close to or on the attractor within the limits of the intrinsic instabilities embodied in the positive Lyapunov exponents. It is also the hardest because there is no guideline as to which of many functional forms to use for the models and what interpretation to place on the parameters in the models from a physical point of view. In this section we make models on the attractor and evaluate them by how well they do in prediction or possibly prediction of Lyapunov exponents.",
isbn="978-1-4612-0763-4",
doi="10.1007/978-1-4612-0763-4_6",
url="https://doi.org/10.1007/978-1-4612-0763-4_6"
}
@article{ouala2020learning,
  title={Learning latent dynamics for partially observed chaotic systems},
  author={Ouala, S and Nguyen, D and Drumetz, L and Chapron, B and Pascual, A and Collard, F and Gaultier, L and Fablet, R},
  journal={Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume={30},
  number={10},
  pages={103121},
  year={2020},
  publisher={AIP Publishing LLC}
}
@article{berry2016forecasting,
  title={Forecasting turbulent modes with nonparametric diffusion models: Learning from noisy data},
  author={Berry, Tyrus and Harlim, John},
  journal={Physica D: Nonlinear Phenomena},
  volume={320},
  pages={57--76},
  year={2016},
  publisher={Elsevier}
}

@article{PhysRevE.104.015206,
  title={Physics-constrained, low-dimensional models for magnetohydrodynamics: First-principles and data-driven approaches},
  author={Kaptanoglu, Alan A and Morgan, Kyle D and Hansen, Chris J and Brunton, Steven L},
  journal={Physical Review E},
  volume={104},
  number={1},
  pages={015206},
  year={2021},
  publisher={APS}
}



@article{guan2021sparse,
  title={Sparse nonlinear models of chaotic electroconvection},
  author={Guan, Yifei and Brunton, Steven L and Novosselov, Igor},
  journal={Royal Society Open Science},
  volume={8},
  number={8},
  pages={202367},
  year={2021},
  publisher={The Royal Society}
}
@article{loiseau2018sparse,
  title={Sparse reduced-order modelling: sensor-based dynamics to full-state estimation},
  author={Loiseau, Jean-Christophe and Noack, Bernd R and Brunton, Steven L},
  journal={Journal of Fluid Mechanics},
  volume={844},
  pages={459--490},
  year={2018},
  publisher={Cambridge University Press}
}

@article{huke2006embedding,
  title={Embedding nonlinear dynamical systems: A guide to Takens' theorem},
  author={Huke, JP},
  year={2006},
  publisher={Manchester Institute for Mathematical Sciences, University of Manchester}
}
@article{levine2022framework,
  title={A framework for machine learning of model error in dynamical systems},
  author={Levine, Matthew and Stuart, Andrew},
  journal={Communications of the American Mathematical Society},
  volume={2},
  number={07},
  pages={283--344},
  year={2022}
}
@article{SVM_takens,
title = "Support vector regression with chaos-based firefly algorithm for stock market price forecasting",
journal = "Applied Soft Computing",
volume = "13",
number = "2",
pages = "947 - 958",
year = "2013",
issn = "1568-4946",
doi = "https://doi.org/10.1016/j.asoc.2012.09.024",
author = "Ahmad Kazem and Ebrahim Sharifi and Farookh Khadeer Hussain and Morteza Saberi and Omar Khadeer Hussain"
}

@article{DMD_koopman,
  title={ON DYNAMIC MODE DECOMPOSITION: THEORY AND APPLICATIONS},
  author={Tu, Jonathan H and Rowley, Clarence W and Luchtenburg, Dirk M and Brunton, Steven L and Kutz, J Nathan},
  journal={Journal of Computational Dynamics},
  volume={1},
  number={2},
  pages={391--421},
  year={2014}
}

@InProceedings{takens_theorem,
author="Takens, Floris",
editor="Rand, David
and Young, Lai-Sang",
title="Detecting strange attractors in turbulence",
booktitle="Dynamical Systems and Turbulence, Warwick 1980",
year="1981",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="366--381",
isbn="978-3-540-38945-3"
}



@article{iden_economics,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/1905689},
 abstract = {Statistical inference, from observations to economic behavior parameters, can be made in two steps:inference from the observations to the parameters of the assumed joint distribution of the observations, and inference from that distribution to the parameters of the structural equations describing economic behavior. The latter problem of inference, described by the term "identification problem," is discussed in this article in an expository manner, drawing on other more original work for concepts and theorems, using a number of examples drawn partly from econometric literature.},
 author = {Tjalling C. Koopmans},
 journal = {Econometrica},
 number = {2},
 pages = {125--144},
 publisher = {[Wiley, Econometric Society]},
 title = {Identification Problems in Economic Model Construction},
 volume = {17},
 year = {1949}
}

@article{cai2021physics,
  title={Physics-Informed Neural Networks ({PINNs}) for Heat Transfer Problems},
  author={Cai, Shengze and Wang, Zhicheng and Wang, Sifan and Perdikaris, Paris and Karniadakis, George},
  journal={Journal of Heat Transfer},
  year={2021}
}


@article{lucor2022JCP,
	author = {Didier Lucor and Atul Agrawal and Anne Sergent},
	journal = {Journal of Computational Physics},
	pages = {111022},
	title = {Simple computational strategies for more effective physics-informed neural networks modeling of turbulent natural convection},
	volume = {456},
	year = {2022}}

@article{rochoux2014towards,
  title={Towards predictive data-driven simulations of wildfire spread--Part I: Reduced-cost Ensemble Kalman Filter based on a Polynomial Chaos surrogate model for parameter estimation},
  author={Rochoux, M{\'e}lanie C and Ricci, Sophie and Lucor, Didier and Cuenot, B{\'e}n{\'e}dicte and Trouv{\'e}, Arnaud},
  journal={Natural Hazards and Earth System Sciences},
  volume={14},
  number={11},
  pages={2951--2973},
  year={2014},
  publisher={Copernicus Publications G{\"o}ttingen, Germany}
}

@article{jeong_hussain_1995, title={On the identification of a vortex}, volume={285}, DOI={10.1017/S0022112095000462}, journal={Journal of Fluid Mechanics}, publisher={Cambridge University Press}, author={Jeong, Jinhee and Hussain, Fazle}, year={1995}, pages={69–94}}

@Article{Abarbanel1996,
author="Abarbanel, Henry D. I.
and Lall, Upmanu",
title="Nonlinear dynamics of the Great Salt Lake: system identification and prediction",
journal="Climate Dynamics",
year="1996",
month="Mar",
day="01",
volume="12",
number="4",
pages="287--297",
abstract="A case study of the application of recent methods of nonlinear time series analysis is presented. The 1848--1992 biweekly time series of the Great Salt Lake (GSL) volume is analyzed for evidence of low dimensional dynamics and predictability. The spectrum of Lyapunov exponents indicates that the average predictability of the GSL is a few hundred days. Use of the false nearest neighbor statistic shows that the dynamics of the GSL can be described in time delay coordinates by four dimensional vectors with components lagged by about half a year. Local linear maps are used in this embedding of the data and their skill in forecasting is tested in split sample mode for a variety of GSL conditions: lake average volume, near the beginning of a drought, near the end of a drought, prior to a period of rapid lake rise. Implications for modeling low frequency components of the hydro-climate system are discussed.",
issn="1432-0894",
doi="10.1007/BF00219502",
url="https://doi.org/10.1007/BF00219502"
}


@article{ident_control,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2240506},
 abstract = {Strong consistency and asymptotic normality of least squares estimates in stochastic regression models are established under certain weak assumptions on the stochastic regressors and errors. We discuss applications of these results to interval estimation of the regression parameters and to recursive on-line identification and control schemes for linear dynamic systems.},
 author = {Tze Leung Lai and Ching Zong Wei},
 journal = {The Annals of Statistics},
 number = {1},
 pages = {154--166},
 publisher = {Institute of Mathematical Statistics},
 title = {Least Squares Estimates in Stochastic Regression Models with Applications to Identification and Control of Dynamic Systems},
 volume = {10},
 year = {1982}
}




@book{Fried_num_sol,
 author = {Fried, Isaac},
 title = {Numerical Solution of Differential Equations},
 year = {1979},
 isbn = {0122677803},
 publisher = {Academic Press, Inc.},
 address = {Orlando, FL, USA},
} 



@INPROCEEDINGS{osti_145724, 
author={B. A. Allan and S. Lefantzi and J. Ray}, 
booktitle={Ninth International Workshop on High-Level Parallel Programming Models and Supportive Environments, 2004. Proceedings.}, 
title={ODEPACK++: refactoring the LSODE Fortran library for use in the CCA high performance component software architecture}, 
year={2004}, 
volume={}, 
number={}, 
pages={109-119}, 
keywords={software libraries;software architecture;FORTRAN;application program interfaces;differential equations;algebra;mathematics computing;object-oriented programming;ODEPACK++;LSODE;Fortran library;CCA;high-performance component software architecture;numerical library;component-software implementation;scientific software;component-oriented software;CVODE;Ccaffeine framework;common component architecture;interface specifications;linear equations;differential equations;differential algebraic equations;parallel programming;interface design;software decomposition;object-oriented programming;Software libraries;Software architecture;Application software;Software maintenance;Differential equations;Parallel programming;Component architectures;Differential algebraic equations;Software design;Object oriented modeling}, 
doi={10.1109/HIPS.2004.1299196}, 
ISSN={}, 
month={April},}

@article{odepack,
    author = {Hindmarsh, A. C.},
    citeulike-article-id = {2732738},
    journal = {IMACS Transactions on Scientific Computation},
    keywords = {file-import-thesis},
    pages = {55--64},
    posted-at = {2008-04-29 09:29:26},
    priority = {0},
    title = {{ODEPACK}, A Systematized Collection of {ODE} Solvers},
    volume = {1},
    year = {1983}
}


@article{butcher_1963, title={Coefficients for the study of Runge-Kutta integration processes}, volume={3}, DOI={10.1017/S1446788700027932}, number={2}, journal={Journal of the Australian Mathematical Society}, publisher={Cambridge University Press}, author={Butcher, J. C.}, year={1963}, pages={185–201}}

@ARTICLE{raissi_multistep,
   author = {{Raissi}, M. and {Perdikaris}, P. and {Karniadakis}, G.~E.},
    title = "{Multistep Neural Networks for Data-driven Discovery of Nonlinear Dynamical Systems}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1801.01236},
 primaryClass = "math.DS",
 keywords = {Mathematics - Dynamical Systems, Mathematics - Numerical Analysis, Nonlinear Sciences - Chaotic Dynamics, Physics - Computational Physics, Statistics - Machine Learning},
     year = 2018,
    month-dummy = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180101236R},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{debezenac_deep_physical,
  author    = {Emmanuel de Bezenac and
               Arthur Pajot and
               Patrick Gallinari},
  title     = {Deep Learning for Physical Processes: Incorporating Prior Scientific
               Knowledge},
  journal   = {CoRR},
  volume    = {abs/1711.07970},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.07970},
  archivePrefix = {arXiv},
  eprint    = {1711.07970},
  timestamp = {Mon, 13 Aug 2018 16:47:44 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1711-07970},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{fablet_improving_2018,
	title = {Improving {Mesoscale} {Altimetric} {Data} {From} a {Multitracer} {Convolutional} {Processing} of {Standard} {Satellite}-{Derived} {Products}},
	volume = {56},
	issn = {0196-2892},
	doi = {10.1109/TGRS.2017.2750491},
	abstract = {Multisatellite measurements of altimeter-derived sea surface height (SSH) have provided a wealth of information on the ocean. Yet, horizontal scales below 100 km remain scarcely resolved. Especially, in the Mediterranean Sea, an important fraction of the mesoscale range, characterized by a small Rossby radius of deformation of 15-20 km, is not properly retrieved by altimeter-derived gridded products. Here, we investigate a novel processing of AVISO products with a view to resolving the horizontal scales sensed by current along-track altimeter data. The key feature of our framework is the use of linear convolutional operators to model the fine-scale SSH detail as a function of different sea surface fields, especially optimally interpolated SSH and sea surface temperature (SST). The proposed model embeds the surface quasi-geostrophic SST-SSH synergy as a special case. Using an observing system simulation experiment with simulated SSH data from model outputs in the Western Mediterranean Sea, we show that the proposed approach has the potential for improving current optimal interpolations of gridded altimeter-derived SSH fields by more than 20\% in terms of relative SSH and kinetic energy mean square error, as well as in terms of spectral signatures for horizontal scales ranging from 30 to 100 km. Our results also suggest that SST-SSH relationship may only play a secondary role compared with the interscale SSH cascade. We further discuss the relevance of the proposed approach in the context of future altimetric satellite missions.},
	number = {5},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Fablet, R. and Verron, J. and Mourre, B. and Chapron, B. and Pascual, A.},
	month = {may},
	year = {2018},
	keywords = {AVISO products, Atmospheric modeling, Convolutional models, Data models, Image reconstruction, Ocean temperature, Rossby radius, SSH fields, Sea surface, Spatial resolution, Western Mediterranean Sea, along-track altimeter data, fine-scale SSH detail, future altimetric satellite missions, gridded altimeter, gridded products, horizontal scales, interscale SSH cascade, kinetic energy mean square error, linear convolutional operators, mesoscale altimetric data, mesoscale range, model outputs, multisatellite measurements, multitracer convolutional processing, observing system simulation experiment (OSSE), ocean temperature, oceanographic regions, optimal interpolations, optimally interpolated SSH, remote sensing, sea level, sea surface fields, sea surface height, sea surface height (SSH), sea surface temperature, sea surface temperature (SST), standard satellite-derived products, superresolution, surface quasigeostrophic SST-SSH synergy, system simulation experiment},
	pages = {2518--2525}
}

@article{pascual_multi-sensor_2009,
	title = {A multi-sensor approach towards coastal ocean processes monitoring},
	volume = {9},
	journal = {Proceedings of OceanObs},
	author = {Pascual, A. and Bouffard, J. and Ruiz, S. and Escudier, R. and Garau, B. and Martínez-Ledesma, M. and Vidal-Vijande, E. and Faugere, Y. and Larnicol, G. and Vizoso, G.},
	year = {2009}
}

@article{gomis_diagnostic_2001,
	title = {Diagnostic analysis of the 3D ageostrophic circulation from a multivariate spatial interpolation of {CTD} and {ADCP} data},
	volume = {48},
	issn = {0967-0637},
	url = {http://www.sciencedirect.com/science/article/pii/S0967063700000601},
	doi = {10.1016/S0967-0637(00)00060-1},
	abstract = {A multivariate (MV) optimal statistical interpolation method is applied to conductivity–temperature–depth (CTD) and ship-mounted acoustic doppler current profiler (ADCP) data from quasi-synoptic oceanographic surveys. MV analysis aims to improve the spatial interpolation of any particular variable (e.g., dynamic height) by including in the analysis observations of other physically related variables (e.g., current). The version used in this work also provides estimates of the non-divergent and irrotational components of the flow. The method is tested in a sharp frontal region to the north of the Western Alborán gyre. After deriving the optimal analysis parameters, we first show that MV statistical dynamic height analysis errors are significantly smaller than those derived from univariate (UV) analysis. In our region, this translates in a more realistic shape for the geostrophic relative vorticity and the vertical velocity field. The latter peaks at about 45 m/day (as given by the quasi-geostrophic omega equation), with a tendency for light water to be upwelled upstream of the gyre while denser water is downwelled downstream of the gyre. For the horizontal velocity we show the existence of large (up to 40 cm/s) ageostrophic velocities. These are mainly non-divergent and can be explained by the cyclostrophic acceleration induced by the anticyclonic gyre. The irrotational velocity component is of the order of 10 cm/s towards the dense side of the front. The robustness of the method is checked by means of several tests that evaluate the sensitivity of results with respect to the synopticity of the data, the analysis parameters, the reference level and the presence of tidal or inertial currents.},
	number = {1},
	urldate = {2018-10-05TZ},
	journal = {Deep Sea Research Part I: Oceanographic Research Papers},
	author = {Gomis, D and Ruiz, S and Pedder, M. A},
	month = {jan},
	year = {2001},
	keywords = {ADCP data, Ageostrophic circulation, Spatial objective analysis},
	pages = {269--295}
}

@article{pascual_improved_2006,
	title = {Improved description of the ocean mesoscale variability by combining four satellite altimeters},
	volume = {33},
	number = {2},
	journal = {Geophysical Research Letters},
	author = {Pascual, Ananda and Faugère, Yannice and Larnicol, Gilles and Le Traon, Pierre-Yves},
	year = {2006}
}

@incollection{lecun_object_1999,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Object {Recognition} with {Gradient}-{Based} {Learning}},
	isbn = {9783540468059},
	url = {https://doi.org/10.1007/3-540-46805-6_19},
	abstract = {Finding an appropriate set of features is an essential problem in the design of shape recognition systems. This paper attempts to show that for recognizing simple objects with high shape variability such as handwritten characters, it is possible, and even advantageous, to feed the system directly with minimally processed images and to rely on learning to extract the right set of features. Convolutional Neural Networks are shown to be particularly well suited to this task. We also show that these networks can be used to recognize multiple objects without requiring explicit segmentation of the objects from their surrounding. The second part of the paper presents the Graph Transformer Network model which extends the applicability of gradient-based learning to systems that use graphs to represents features, objects, and their combinations.},
	language = {en},
	urldate = {2018-10-02TZ},
	booktitle = {Shape, {Contour} and {Grouping} in {Computer} {Vision}},
	publisher = {Springer Berlin Heidelberg},
	author = {LeCun, Yann and Haffner, Patrick and Bottou, Léon and Bengio, Yoshua},
	editor = {Forsyth, David A. and Mundy, Joseph L. and di Gesú, Vito and Cipolla, Roberto},
	year = {1999},
	doi = {10.1007/3-540-46805-6_19},
	keywords = {Convolutional Neural Network , Loss Function , Neural Information Processing System , Object Recognition , Radial Basis Function },
	pages = {319--345}
}

@article{beckers_eof_2003,
	title = {{EOF} {Calculations} and {Data} {Filling} from {Incomplete} {Oceanographic} {Datasets}},
	volume = {20},
	issn = {0739-0572},
	url = {https://journals.ametsoc.org/doi/10.1175/1520-0426%282003%29020%3C1839%3AECADFF%3E2.0.CO%3B2},
	doi = {10.1175/1520-0426(2003)020<1839:ECADFF>2.0.CO;2},
	abstract = {The paper presents a new self-consistent method to infer missing data from oceanographic data series and to extract the relevant empirical orthogonal functions. As a by-product, the new method allows for the detection of the number of statistically significant EOFs by a cross-validation procedure for a complete or incomplete dataset, as well as the noise level and interpolation error. Since the proposed filling and analysis method does not need a priori information about the error covariance structure, the method is self-consistent and parameter free.},
	number = {12},
	urldate = {2018-09-30TZ},
	journal = {Journal of Atmospheric and Oceanic Technology},
	author = {Beckers, J. M. and Rixen, M.},
	month = {dec},
	year = {2003},
	pages = {1839--1856}
}

@article{mairal_learning_2008,
	title = {Learning multiscale sparse representations for image and video restoration},
	volume = {7},
	number = {1},
	journal = {Multiscale Modeling \& Simulation},
	author = {Mairal, Julien and Sapiro, Guillermo and Elad, Michael},
	year = {2008},
	pages = {214--241}
}

@article{le_traon_improved_1998,
	title = {An improved mapping method of multisatellite altimeter data},
	volume = {15},
	number = {2},
	journal = {Journal of atmospheric and oceanic technology},
	author = {Le Traon, P. Y. and Nadal, F. and Ducet, N.},
	year = {1998},
	pages = {522--534}
}

@article{ducet_global_2000,
	title = {Global high-resolution mapping of ocean circulation from {TOPEX}/{Poseidon} and {ERS}-1 and-2},
	volume = {105},
	number = {C8},
	journal = {Journal of Geophysical Research: Oceans},
	author = {Ducet, N. and Le Traon, Pierre-Yves and Reverdin, Gilles},
	year = {2000},
	pages = {19477--19498}
}

@article{olmedo_improving_2018,
	title = {Improving {SMOS} {Sea} {Surface} {Salinity} in the {Western} {Mediterranean} {Sea} through {Multivariate} and {Multifractal} {Analysis}},
	volume = {10},
	number = {3},
	journal = {Remote Sensing},
	author = {Olmedo, Estrella and Taupier-Letage, Isabelle and Turiel, Antonio and Alvera-Azcárate, Aida},
	year = {2018},
	pages = {485}
}

@article{alvera-azcarate_analysis_2016,
	title = {Analysis of {SMOS} sea surface salinity data using {DINEOF}},
	volume = {180},
	journal = {Remote sensing of environment},
	author = {Alvera-Azcárate, Aida and Barth, Alexander and Parard, Gaëlle and Beckers, Jean-Marie},
	year = {2016},
	pages = {137--145}
}

@article{beckers_approximate_2014,
	title = {Approximate and efficient methods to assess error fields in spatial gridding with data interpolating variational analysis ({DIVA})},
	volume = {31},
	number = {2},
	journal = {Journal of Atmospheric and Oceanic Technology},
	author = {Beckers, Jean-Marie and Barth, Alexander and Troupin, Charles and Alvera-Azcárate, Aida},
	year = {2014},
	pages = {515--530}
}

@article{von_schuckmann_copernicus_2016,
	title = {The copernicus marine environment monitoring service ocean state report},
	volume = {9},
	number = {sup2},
	journal = {Journal of Operational Oceanography},
	author = {Von Schuckmann, Karina and Le Traon, Pierre-Yves and Alvarez-Fanjul, Enrique and Axell, Lars and Balmaseda, Magdalena and Breivik, Lars-Anders and Brewin, Robert JW and Bricaud, Clement and Drevillon, Marie and Drillet, Yann},
	year = {2016},
	pages = {s235--s320}
}

@article{droghei_new_2018,
	title = {A {New} {Global} {Sea} {Surface} {Salinity} and {Density} {Dataset} {From} {Multivariate} {Observations} (1993–2016)},
	volume = {5},
	journal = {Frontiers in Marine Science},
	author = {Droghei, Riccardo and Buongiorno Nardelli, Bruno and Santoleri, Rosalia},
	year = {2018},
	pages = {84}
}

@article{nardelli_evaluation_2015,
	title = {Evaluation of different covariance models for the operational interpolation of high resolution satellite {Sea} {Surface} {Temperature} data over the {Mediterranean} {Sea}},
	volume = {164},
	journal = {Remote Sensing of Environment},
	author = {Nardelli, B. Buongiorno and Pisano, A. and Tronconi, C. and Santoleri, R.},
	year = {2015},
	pages = {334--343}
}

@article{le_traon_improved_1998-1,
	title = {An improved mapping method of multisatellite altimeter data},
	volume = {15},
	number = {2},
	journal = {Journal of atmospheric and oceanic technology},
	author = {Le Traon, P. Y. and Nadal, F. and Ducet, N.},
	year = {1998},
	pages = {522--534}
}

@incollection{le_traon_satellites_2011,
	title = {Satellites and operational oceanography},
	booktitle = {Operational {Oceanography} in the 21st {Century}},
	publisher = {Springer},
	author = {Le Traon, Pierre-Yves},
	year = {2011},
	pages = {29--54}
}

@misc{noauthor_lcl_nodate,
	title = {{LCL} - {Le} {Crédit} {Lyonnais}},
	url = {https://particuliers.secure.lcl.fr/outil/UXPA/Accueil/accesPartenaire?urlPartenaire=/outil/UWWM/GenerationJeton/accueil?FCTEL=CONSULT&xtatc=INT-417},
	urldate = {2018-08-23TZ}
}

@misc{storch_statistical_1999,
	title = {Statistical {Analysis} in {Climate} {Research} by {Hans} von {Storch}},
	url = {/core/books/statistical-analysis-in-climate-research/9125090F106D2845485E4BACD79B9695},
	abstract = {Cambridge Core - Statistics for Environmental Sciences - Statistical Analysis in Climate Research -  by Hans von Storch},
	language = {en},
	urldate = {2018-07-23TZ},
	journal = {Cambridge Core},
	author = {Storch, Hans von and Zwiers, Francis W.},
	month = {jul},
	year = {1999},
	doi = {10.1017/CBO9780511612336}
}

@article{bertino_sequential_2007,
	title = {Sequential {Data} {Assimilation} {Techniques} in {Oceanography}},
	volume = {71},
	issn = {1751-5823},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2003.tb00194.x},
	doi = {10.1111/j.1751-5823.2003.tb00194.x},
	language = {en, fr},
	number = {2},
	urldate = {2018-07-23TZ},
	journal = {International Statistical Review},
	author = {Bertino, Laurent and Evensen, Geir and Wackernagel, Hans},
	month = {january},
	year = {2007},
	pages = {223--241}
}

@article{ping_improved_2016,
	title = {An {Improved} {DINEOF} {Algorithm} for {Filling} {Missing} {Values} in {Spatio}-{Temporal} {Sea} {Surface} {Temperature} {Data}},
	volume = {11},
	issn = {1932-6203},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0155928},
	doi = {10.1371/journal.pone.0155928},
	abstract = {In this study, an improved Data INterpolating Empirical Orthogonal Functions (DINEOF) algorithm for determination of missing values in a spatio-temporal dataset is presented. Compared with the ordinary DINEOF algorithm, the iterative reconstruction procedure until convergence based on every fixed EOF to determine the optimal EOF mode is not necessary and the convergence criterion is only reached once in the improved DINEOF algorithm. Moreover, in the ordinary DINEOF algorithm, after optimal EOF mode determination, the initial matrix with missing data will be iteratively reconstructed based on the optimal EOF mode until the reconstruction is convergent. However, the optimal EOF mode may be not the best EOF for some reconstructed matrices generated in the intermediate steps. Hence, instead of using asingle EOF to fill in the missing data, in the improved algorithm, the optimal EOFs for reconstruction are variable (because the optimal EOFs are variable, the improved algorithm is called VE-DINEOF algorithm in this study). To validate the accuracy of the VE-DINEOF algorithm, a sea surface temperature (SST) data set is reconstructed by using the DINEOF, I-DINEOF (proposed in 2015) and VE-DINEOF algorithms. Four parameters (Pearson correlation coefficient, signal-to-noise ratio, root-mean-square error, and mean absolute difference) are used as a measure of reconstructed accuracy. Compared with the DINEOF and I-DINEOF algorithms, the VE-DINEOF algorithm can significantly enhance the accuracy of reconstruction and shorten the computational time.},
	language = {en},
	number = {5},
	urldate = {2018-07-20TZ},
	journal = {PLOS ONE},
	author = {Ping, Bo and Su, Fenzhen and Meng, Yunshan},
	month = {may},
	year = {2016},
	keywords = {Algorithms, Geographic areas, Interpolation, Ocean temperature, Oceanography, Research errors, Signal to noise ratio, Surface temperature},
	pages = {e0155928}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2018-06-25TZ},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = {december},
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}



@article{ouala_sea_2018,
	title = {Sea surface temperature prediction and reconstruction using patch-level neural network representations},
	url = {http://arxiv.org/abs/1806.00144},
	abstract = {The forecasting and reconstruction of ocean and atmosphere dynamics from satellite observation time series are key challenges. While model-driven representations remain the classic approaches, data-driven representations become more and more appealing to benefit from available large-scale observation and simulation datasets. In this work we investigate the relevance of recently introduced bilinear residual neural network representations, which mimic numerical integration schemes such as Runge-Kutta, for the forecasting and assimilation of geophysical fields from satellite-derived remote sensing data. As a case-study, we consider satellite-derived Sea Surface Temperature time series off South Africa, which involves intense and complex upper ocean dynamics. Our numerical experiments demonstrate that the proposed patch-level neural-network-based representations outperform other data-driven models, including analog schemes, both in terms of forecasting and missing data interpolation performance with a relative gain up to 50{\textbackslash}\% for highly dynamic areas.},
	urldate = {2018-06-25TZ},
	journal = {arXiv:1806.00144 [cs, stat]},
	author = {Ouala, Said and Herzet, Cedric and Fablet, Ronan},
	month = {may},
	year = {2018},
	note = {arXiv: 1806.00144},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
}
@article{kashinath2021physics,
  title={Physics-informed machine learning: case studies for weather and climate modelling},
  author={Kashinath, K and Mustafa, M and Albert, A and Wu, JL and Jiang, C and Esmaeilzadeh, S and Azizzadenesheli, K and Wang, R and Chattopadhyay, A and Singh, A and others},
  journal={Philosophical Transactions of the Royal Society A},
  volume={379},
  number={2194},
  pages={20200093},
  year={2021},
  publisher={The Royal Society Publishing}
}
@article{donlon2002toward,
  title={Toward improved validation of satellite sea surface skin temperature measurements for climate research},
  author={Donlon, CJ and Minnett, PJ and Gentemann, Chelle and Nightingale, TJ and Barton, IJ and Ward, B and Murray, MJ},
  journal={Journal of Climate},
  volume={15},
  number={4},
  pages={353--369},
  year={2002}
}

@book{hyndman2018forecasting,
  title={Forecasting: principles and practice},
  author={Hyndman, Rob J and Athanasopoulos, George},
  year={2018},
  publisher={OTexts}
}

@article{pannekoucke_modelling_2013,
	title = {Modelling of local length-scale dynamics and isotropizing deformations},
	volume = {140},
	issn = {1477-870X},
	url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.2204},
	doi = {10.1002/qj.2204},
	abstract = {The correlation length-scale which characterizes the shape of the correlation function is often used to parametrize correlation models. This article describes how the length-scale dynamics can be employed to estimate a spatial deformation (coordinate transformation). Of particular interest is the isotropizing deformation, which transforms anisotropic correlation functions into quasi-isotropic ones. The evolution of the length-scale field under a simple advection dynamics is described in terms of the local metric tensor. This description leads to a quadratic constraint satisfied by the isotropizing deformation and from which a system of Poisson-like partial differential equations is deduced. The isotropizing deformation is obtained as the solution of a coupled system of Poisson-like partial differential equations. This system is then solved with a pseudo-diffusion scheme, where the isotropizing deformation is the steady-state solution. The isotropization process is illustrated within a simulated 2D setting. The method is shown to provide an accurate estimation of the original deformation used to build the anisotropic correlations in this idealized framework. Applications in data assimilation are discussed. First, the isotropization procedure can be useful for background-error covariance modelling. Secondly, the length-scale dynamics provides a way to simulate the dynamics of covariances for the transport of passive scalars, as encountered in chemical data assimilation.},
	language = {en},
	number = {681},
	urldate = {2018-05-25TZ},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Pannekoucke, O. and Emili, E. and Thual, O.},
	year = {2013},
	keywords = {covariance modelling, deformation, isotropization, length-scale dynamics},
	pages = {1387--1398}
}

@article{gaspari_construction_1999,
	title = {Construction of correlation functions in two and three dimensions},
	volume = {125},
	issn = {1477-870X},
	url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.49712555417},
	doi = {10.1002/qj.49712555417},
	abstract = {This article focuses on the construction, directly in physical space, of simply parametrized covariance functions for data-assimilation applications. A self-contained, rigorous mathematical summary of relevant topics from correlation theory is provided as a foundation for this construction. Covariance and correlation functions are defined, and common notions of homogeneity and isotropy are clarified. Classical results are stated, and proven where instructive. Included are smoothness properties relevant to multivariate statistical-analysis algorithms where wind/wind and wind/mass correlation models are obtained by differentiating the correlation model of a mass variable. the Convolution Theorem is introduced as the primary tool used to construct classes of covariance and cross-covariance functions on three-dimensional Euclidean space R3. Among these are classes of compactly supported functions that restrict to covariance and cross-covariance functions on the unit sphere S2, and that vanish identically on subsets of positive measure on S2. It is shown that these covariance and cross-covariance functions on S2, referred to as being space-limited, cannot be obtained using truncated spectral expansions. Compactly supported and space-limited covariance functions determine sparse covariance matrices when evaluated on a grid, thereby easing computational burdens in atmospheric data-analysis algorithms. Convolution integrals leading to practical examples of compactly supported covariance and cross-covariance functions on R3 are reduced and evaluated. More specifically, suppose that gi and gj are radially symmetric functions defined on R3 such that gi(x) = 0 for {\textbar}x{\textbar} {\textgreater} di and gj(x) = 0 for {\textbar}xv {\textgreater} dj, O {\textless} di,dj ≦, where {\textbar}. {\textbar} denotes Euclidean distance in R3. the parameters di and dj are ‘cut-off’ distances. Closed-form expressions are determined for classes of convolution cross-covariance functions Cij(x,y) := (gi * gj)(x-y), i ≠ j, and convolution covariance functions Cii(x,y) := (gi * gi)(x-y), vanishing for {\textbar}x - y{\textbar} {\textgreater} di + dj and {\textbar}x - y{\textbar} {\textgreater} 2di, respectively, Additional covariance functions on R3 are constructed using convolutions over the real numbers R, rather than R3. Families of compactly supported approximants to standard second- and third-order autoregressive functions are constructed as illustrative examples. Compactly supported covariance functions of the form C(x,y) := Co({\textbar}x - y{\textbar}), x,y ∈ R3, where the functions Co(r) for r ∈ R are 5th-order piecewise rational functions, are also constructed. These functions are used to develop space-limited product covariance functions B(x, y) C(x, y), x, y ∈ S2, approximating given covariance functions B(x, y) supported on all of S2 × S2.},
	language = {en},
	number = {554},
	urldate = {2018-05-25TZ},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Gaspari, Gregory and Cohn, Stephen E.},
	year = {1999},
	keywords = {Compactly supported, Convolution, Correlation functions, Data assimilation, Space-limited},
	pages = {723--757}
}

@article{bocquet_localization_2016,
	title = {Localization and the iterative ensemble {Kalman} smoother},
	volume = {142},
	issn = {1477-870X},
	url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.2711},
	doi = {10.1002/qj.2711},
	abstract = {The iterative ensemble Kalman smoother (IEnKS) is a data assimilation method meant for tracking the state of nonlinear geophysical models efficiently. It combines an ensemble of model states to estimate the errors similarly to the ensemble square-root Kalman filter, with a four-dimensional variational analysis performed within the ensemble space. As such, it belongs to the class of four-dimensional ensemble variational methods. It could require the use of localization of the analysis when the state-space dimension is high. However, its localization needs to be defined across time and to be as consistent as possible with the dynamical flow within the data assimilation window where the four-dimensional variational analysis is performed. We show that a Liouville equation governs the time evolution of the localization operator, which is linked to the evolution of the error correlations. It is argued that the time evolution of the localization operator depends strongly on the forecast dynamics. Using either covariance localization or domain localization, we propose and test several localization strategies meant to address the issue: (i) a static and uniform localization, (ii) propagation through the window of a restricted set of dominant modes of the error covariance matrix and (iii) the approximate propagation of the localization operator using covariant local domains that are moved in accordance with the dynamical flow. These schemes are illustrated with the one-dimensional Lorenz 40 variable model and with a two-dimensional barotropic vorticity model. In both cases, local analysis based on the covariant local domains leads to a systematic improvement of the data assimilation performance.},
	language = {en},
	number = {695},
	urldate = {2018-05-25TZ},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Bocquet, M.},
	year = {2016},
	keywords = {4DEnVar, ensemble Kalman filter, ensemble Kalman smoother, ensemble variational method, iterative ensemble Kalman smoother, localization},
	pages = {1075--1089}
}

@article{bertino_sequential_2003,
	title = {Sequential {Data} {Assimilation} {Techniques} in {Oceanography}},
	volume = {71},
	issn = {1751-5823},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2003.tb00194.x},
	doi = {10.1111/j.1751-5823.2003.tb00194.x},
	abstract = {We review recent developments of sequential data assimilation techniques used in oceanography to integrate spatio-temporal observations into numerical models describing physical and ecological dynamics. Theoretical aspects from the simple case of linear dynamics to the general case of nonlinear dynamics are described from a geostatistical point-of-view. Current methods derived from the Kalman filter are presented from the least complex to the most general and perspectives for nonlinear estimation by sequential importance resampling filters are discussed. Furthermore an extension of the ensemble Kalman filter to transformed Gaussian variables is presented and illustrated using a simplified ecological model. The described methods are designed for predicting over geographical regions using a high spatial resolution under the practical constraint of keeping computing time sufficiently low to obtain the prediction before the fact. Therefore the paper focuses on widely used and computationally efficient methods.},
	language = {en},
	number = {2},
	urldate = {2018-05-25TZ},
	journal = {International Statistical Review},
	author = {Bertino, Laurent and Evensen, Geir and Wackernagel, Hans},
	year = {2003},
	keywords = {Data assimilation, Ecological model, Geostatistics, Kalman filter, Non-linear dynamical systems, State-space models},
	pages = {223--241}
}

@inproceedings{ouala_sea_2018-1,
	address = {Italy},
	title = {Sea surface temperature prediction and reconstruction using patch-level neural network representations},
	abstract = {The forecasting and reconstruction of ocean and atmosphere dynamics from satellite observation time series are key challenges. 
While model-driven representations remain the classic approaches, data-driven representations become more and more appealing to benefit from available large-scale observation and simulation datasets.  
As a case-study, we consider satellite-derived Sea Surface Temperature time series off South Africa, which involves intense and complex upper ocean dynamics. Our numerical experiments demonstrate that the proposed patch-level neural-network-based representations outperform other data-driven models, including analog schemes, both in terms of forecasting and missing data interpolation performance with a relative gain up to 50{\textbackslash}\% for highly dynamic areas.},
	booktitle = {{IGARSS}},
	publisher = {IEEE},
	author = {OUALA, Said and Herzet, Cédric and Fablet, Ronan},
	year = {2018}
}

@misc{noauthor_sea_nodate,
	title = {Sea surface temperature prediction and reconstruction using patch-level neural network representations},
	url = {https://www.researchgate.net/publication/325367038_Sea_surface_temperature_prediction_and_reconstruction_using_patch-level_neural_network_representations},
	abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
	language = {en},
	urldate = {2018-05-25TZ}
}

@article{fraccaro_sequential_2016,
	title = {Sequential {Neural} {Models} with {Stochastic} {Layers}},
	url = {http://arxiv.org/abs/1605.07571},
	abstract = {How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.},
	urldate = {2018-05-25TZ},
	journal = {arXiv:1605.07571 [cs, stat]},
	author = {Fraccaro, Marco and Sønderby, Søren Kaae and Paquet, Ulrich and Winther, Ole},
	month = {may},
	year = {2016},
	note = {arXiv: 1605.07571},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
}

@article{chung_recurrent_2015,
	title = {A {Recurrent} {Latent} {Variable} {Model} for {Sequential} {Data}},
	url = {http://arxiv.org/abs/1506.02216},
	abstract = {In this paper, we explore the inclusion of latent random variables into the dynamic hidden state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamic hidden state.},
	urldate = {2018-05-25TZ},
	journal = {arXiv:1506.02216 [cs]},
	author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
	month = {jun},
	year = {2015},
	note = {arXiv: 1506.02216},
	keywords = {Computer Science - Learning}
}

@article{pannekoucke_parametric_2016,
	title = {Parametric {Kalman} filter for chemical transport models},
	volume = {68},
	issn = {null},
	url = {https://doi.org/10.3402/tellusa.v68.31547},
	doi = {10.3402/tellusa.v68.31547},
	abstract = {A computational simplification of the Kalman filter (KF) is introduced – the parametric Kalman filter (PKF). The full covariance matrix dynamics of the KF, which describes the evolution along the analysis and forecast cycle, is replaced by the dynamics of the error variance and the diffusion tensor, which is related to the correlation length-scales. The PKF developed here has been applied to the simplified framework of advection–diffusion of a passive tracer, for its use in chemical transport model assimilation. The PKF is easy to compute and computationally cost-effective than an ensemble Kalman filter (EnKF) in this context. The validation of the method is presented for a simplified 1-D advection–diffusion dynamics.},
	number = {1},
	urldate = {2018-05-25TZ},
	journal = {Tellus A: Dynamic Meteorology and Oceanography},
	author = {Pannekoucke, Olivier and Ricci, Sophie and Barthelemy, Sebastien and Ménard, Richard and Thual, Olivier},
	month = {dec},
	year = {2016},
	keywords = {Kalman filter, covariance dynamics, data assimilation, parameterisation of analysis},
	pages = {31547}
}

@article{loiseau2018constrained,
  title={Constrained sparse Galerkin regression},
  author={Loiseau, Jean-Christophe and Brunton, Steven L},
  journal={Journal of Fluid Mechanics},
  volume={838},
  pages={42--67},
  year={2018},
  publisher={Cambridge University Press}
}

@ARTICLE{DDControl,
  author={D. {Meng} and Y. {Jia} and J. {Du} and F. {Yu}},
  journal={IEEE Transactions on Neural Networks}, 
  title={Data-Driven Control for Relative Degree Systems via Iterative Learning}, 
  year={2011},
  volume={22},
  number={12},
  pages={2213-2225},
  doi={10.1109/TNN.2011.2174378}}
@article{schlegel_noack_2015, title={On long-term boundedness of Galerkin models}, volume={765}, DOI={10.1017/jfm.2014.736}, journal={Journal of Fluid Mechanics}, publisher={Cambridge University Press}, author={Schlegel, Michael and Noack, Bernd R.}, year={2015}, pages={325–352}}
@Inbook{Parker1989,
author="Parker, Thomas S.
and Chua, Leon O.",
title="Stability of Limit Sets",
bookTitle="Practical Numerical Algorithms for Chaotic Systems",
year="1989",
publisher="Springer New York",
address="New York, NY",
pages="57--82",
abstract="Stable limit sets are of supreme importance in experimental and numerical settings because they are the only kind of limit set that can be observed naturally, that is, by simply letting the system run. In this chapter, we examine the conditions for a limit set to be stable.",
isbn="978-1-4612-3486-9",
doi="10.1007/978-1-4612-3486-9_3",
url="https://doi.org/10.1007/978-1-4612-3486-9_3"
}


@article{sparsePlusControl,
  title={Sparse identification of nonlinear dynamics with control (SINDYc)},
  author={Brunton, Steven L and Proctor, Joshua L and Kutz, J Nathan},
  journal={IFAC-PapersOnLine},
  volume={49},
  number={18},
  pages={710--715},
  year={2016},
  publisher={Elsevier}
}
@article{raissi2019physics,
  title={Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},
  journal={Journal of Computational Physics},
  volume={378},
  pages={686--707},
  year={2019},
  publisher={Elsevier}
}
@book{wanner1996solving,
  title={Solving ordinary differential equations II},
  author={Wanner, Gerhard and Hairer, Ernst},
  year={1996},
  publisher={Springer Berlin Heidelberg}
}



@article{Ouala_2018, title={Neural Network Based Kalman Filters for the Spatio-Temporal Interpolation of Satellite-Derived Sea Surface Temperature}, volume={10}, ISSN={2072-4292}, url={http://dx.doi.org/10.3390/rs10121864}, DOI={10.3390/rs10121864}, number={12}, journal={Remote Sens.}, publisher={MDPI AG}, author={Ouala, Said and Fablet, Ronan and Herzet, Cédric and Chapron, Bertrand and Pascual, Ananda and Collard, Fabrice and Gaultier, Lucile}, year={2018}, month={Nov}, pages={1864} }
@article{cohn_dynamics_1993,
	title = {Dynamics of {Short}-{Term} {Univariate} {Forecast} {Error} {Covariances}},
	volume = {121},
	issn = {0027-0644},
	url = {https://journals.ametsoc.org/doi/abs/10.1175/1520-0493(1993)121%3C3123%3ADOSTUF%3E2.0.CO%3B2},
	doi = {10.1175/1520-0493(1993)121<3123:DOSTUF>2.0.CO;2},
	abstract = {The covariance equation based on second-order closure for dynamics governed by a general scalar nonlinear partial differential equation (PDE) is studied. If the governing dynamics involve n space dimensions, then the covariance equation is a PDE in 2n space dimensions. Solving this equation for n = 3 is therefore computationally infeasible. This is a hindrance to stochastic-dynamic prediction as well as to novel methods of data assimilation based on the Kalman filter. It is shown that the covariance equation can be solved approximately, to any desired accuracy, by solving instead an auxiliary system of PDEs in just n dimensions. The first of these is a dynamical equation for the variance field. Successive equations describe, to increasingly high order, the dynamics of the shape of either the covariance function or the correlation function for points separated by small distances. The second-order equation, for instance, describes the evolution of the correlation length (turbulent microscale) field. Each auxiliary equation is coupled only to the preceding, lower-order equations if the governing dynamics are hyperbolic, but is weakly coupled to the following equation in the presence of diffusion. Analysis of these equations reveals some of the qualitative behavior of their solutions. It is shown that the variance equation, through nonlinear coupling with the mean equation, describes the nonlinear effect of saturation of variance as well as the internal and external growth of variance. Further, it is shown that, in the presence of model error, the initial correlation field is transient, being damped as the influence of the model error correlation grows, while in the absence of model error the initial correlation is simply advected. There is also a critical correlation length, depending on the internal dynamics and on the model error, toward which the forecast error correlation length generally tends.},
	number = {11},
	urldate = {2018-05-25TZ},
	journal = {Monthly Weather Review},
	author = {Cohn, Stephen E.},
	month = {nov},
	year = {1993},
	pages = {3123--3149}
}

@article{cohn_dynamics_1993-1,
	title = {Dynamics of {Short}-{Term} {Univariate} {Forecast} {Error} {Covariances}},
	volume = {121},
	issn = {0027-0644},
	url = {https://journals.ametsoc.org/doi/abs/10.1175/1520-0493(1993)121%3C3123%3ADOSTUF%3E2.0.CO%3B2},
	doi = {10.1175/1520-0493(1993)121<3123:DOSTUF>2.0.CO;2},
	abstract = {The covariance equation based on second-order closure for dynamics governed by a general scalar nonlinear partial differential equation (PDE) is studied. If the governing dynamics involve n space dimensions, then the covariance equation is a PDE in 2n space dimensions. Solving this equation for n = 3 is therefore computationally infeasible. This is a hindrance to stochastic-dynamic prediction as well as to novel methods of data assimilation based on the Kalman filter. It is shown that the covariance equation can be solved approximately, to any desired accuracy, by solving instead an auxiliary system of PDEs in just n dimensions. The first of these is a dynamical equation for the variance field. Successive equations describe, to increasingly high order, the dynamics of the shape of either the covariance function or the correlation function for points separated by small distances. The second-order equation, for instance, describes the evolution of the correlation length (turbulent microscale) field. Each auxiliary equation is coupled only to the preceding, lower-order equations if the governing dynamics are hyperbolic, but is weakly coupled to the following equation in the presence of diffusion. Analysis of these equations reveals some of the qualitative behavior of their solutions. It is shown that the variance equation, through nonlinear coupling with the mean equation, describes the nonlinear effect of saturation of variance as well as the internal and external growth of variance. Further, it is shown that, in the presence of model error, the initial correlation field is transient, being damped as the influence of the model error correlation grows, while in the absence of model error the initial correlation is simply advected. There is also a critical correlation length, depending on the internal dynamics and on the model error, toward which the forecast error correlation length generally tends.},
	number = {11},
	urldate = {2018-05-25TZ},
	journal = {Monthly Weather Review},
	author = {Cohn, Stephen E.},
	month = {nov},
	year = {1993},
	pages = {3123--3149}
}

@article{pannekoucke_parametric_2016-1,
	title = {Parametric {Kalman} filter for chemical transport models},
	volume = {68},
	issn = {null},
	url = {https://doi.org/10.3402/tellusa.v68.31547},
	doi = {10.3402/tellusa.v68.31547},
	abstract = {A computational simplification of the Kalman filter (KF) is introduced – the parametric Kalman filter (PKF). The full covariance matrix dynamics of the KF, which describes the evolution along the analysis and forecast cycle, is replaced by the dynamics of the error variance and the diffusion tensor, which is related to the correlation length-scales. The PKF developed here has been applied to the simplified framework of advection–diffusion of a passive tracer, for its use in chemical transport model assimilation. The PKF is easy to compute and computationally cost-effective than an ensemble Kalman filter (EnKF) in this context. The validation of the method is presented for a simplified 1-D advection–diffusion dynamics.},
	number = {1},
	urldate = {2018-05-25TZ},
	journal = {Tellus A: Dynamic Meteorology and Oceanography},
	author = {Pannekoucke, Olivier and Ricci, Sophie and Barthelemy, Sebastien and Ménard, Richard and Thual, Olivier},
	month = {dec},
	year = {2016},
	keywords = {Kalman filter, covariance dynamics, data assimilation, parameterisation of analysis},
	pages = {31547}
}

@article{anderson_monte_1999,
	title = {A {Monte} {Carlo} {Implementation} of the {Nonlinear} {Filtering} {Problem} to {Produce} {Ensemble} {Assimilations} and {Forecasts}},
	volume = {127},
	issn = {0027-0644},
	url = {https://journals.ametsoc.org/doi/abs/10.1175/1520-0493%281999%29127%3C2741%3AAMCIOT%3E2.0.CO%3B2},
	doi = {10.1175/1520-0493(1999)127<2741:AMCIOT>2.0.CO;2},
	abstract = {Knowledge of the probability distribution of initial conditions is central to almost all practical studies of predictability and to improvements in stochastic prediction of the atmosphere. Traditionally, data assimilation for atmospheric predictability or prediction experiments has attempted to find a single “best” estimate of the initial state. Additional information about the initial condition probability distribution is then obtained primarily through heuristic techniques that attempt to generate representative perturbations around the best estimate. However, a classical theory for generating an estimate of the complete probability distribution of an initial state given a set of observations exists. This nonlinear filtering theory can be applied to unify the data assimilation and ensemble generation problem and to produce superior estimates of the probability distribution of the initial state of the atmosphere (or ocean) on regional or global scales. A Monte Carlo implementation of the fully nonlinear filter has been developed and applied to several low-order models. The method is able to produce assimilations with small ensemble mean errors while also providing random samples of the initial condition probability distribution. The Monte Carlo method can be applied in models that traditionally require the application of initialization techniques without any explicit initialization. Initial application to larger models is promising, but a number of challenges remain before the method can be extended to large realistic forecast models.},
	number = {12},
	urldate = {2018-05-25TZ},
	journal = {Monthly Weather Review},
	author = {Anderson, Jeffrey L. and Anderson, Stephen L.},
	month = {dec},
	year = {1999},
	pages = {2741--2758}
}

@article{houtekamer_sequential_2001,
	title = {A {Sequential} {Ensemble} {Kalman} {Filter} for {Atmospheric} {Data} {Assimilation}},
	volume = {129},
	issn = {0027-0644},
	url = {https://journals.ametsoc.org/doi/10.1175/1520-0493%282001%29129%3C0123%3AASEKFF%3E2.0.CO%3B2},
	doi = {10.1175/1520-0493(2001)129<0123:ASEKFF>2.0.CO;2},
	abstract = {An ensemble Kalman filter may be considered for the 4D assimilation of atmospheric data. In this paper, an efficient implementation of the analysis step of the filter is proposed. It employs a Schur (elementwise) product of the covariances of the background error calculated from the ensemble and a correlation function having local support to filter the small (and noisy) background-error covariances associated with remote observations. To solve the Kalman filter equations, the observations are organized into batches that are assimilated sequentially. For each batch, a Cholesky decomposition method is used to solve the system of linear equations. The ensemble of background fields is updated at each step of the sequential algorithm and, as more and more batches of observations are assimilated, evolves to eventually become the ensemble of analysis fields. A prototype sequential filter has been developed. Experiments are performed with a simulated observational network consisting of 542 radiosonde and 615 satellite-thickness profiles. Experimental results indicate that the quality of the analysis is almost independent of the number of batches (except when the ensemble is very small). This supports the use of a sequential algorithm. A parallel version of the algorithm is described and used to assimilate over 100 000 observations into a pair of 50-member ensembles. Its operation count is proportional to the number of observations, the number of analysis grid points, and the number of ensemble members. In view of the flexibility of the sequential filter and its encouraging performance on a NEC SX-4 computer, an application with a primitive equations model can now be envisioned.},
	number = {1},
	urldate = {2018-05-25TZ},
	journal = {Monthly Weather Review},
	author = {Houtekamer, P. L. and Mitchell, Herschel L.},
	month = {jan},
	year = {2001},
	pages = {123--137}
}

@article{houtekamer_data_1998,
	title = {Data {Assimilation} {Using} an {Ensemble} {Kalman} {Filter} {Technique}},
	volume = {126},
	issn = {0027-0644},
	url = {https://journals.ametsoc.org/doi/abs/10.1175/1520-0493%281998%29126%3C0796%3ADAUAEK%3E2.0.CO%3B2},
	doi = {10.1175/1520-0493(1998)126<0796:DAUAEK>2.0.CO;2},
	abstract = {The possibility of performing data assimilation using the flow-dependent statistics calculated from an ensemble of short-range forecasts (a technique referred to as ensemble Kalman filtering) is examined in an idealized environment. Using a three-level, quasigeostrophic, T21 model and simulated observations, experiments are performed in a perfect-model context. By using forward interpolation operators from the model state to the observations, the ensemble Kalman filter is able to utilize nonconventional observations. In order to maintain a representative spread between the ensemble members and avoid a problem of inbreeding, a pair of ensemble Kalman filters is configured so that the assimilation of data using one ensemble of short-range forecasts as background fields employs the weights calculated from the other ensemble of short-range forecasts. This configuration is found to work well: the spread between the ensemble members resembles the difference between the ensemble mean and the true state, except in the case of the smallest ensembles. A series of 30-day data assimilation cycles is performed using ensembles of different sizes. The results indicate that (i) as the size of the ensembles increases, correlations are estimated more accurately and the root-mean-square analysis error decreases, as expected, and (ii) ensembles having on the order of 100 members are sufficient to accurately describe local anisotropic, baroclinic correlation structures. Due to the difficulty of accurately estimating the small correlations associated with remote observations, a cutoff radius beyond which observations are not used, is implemented. It is found that (a) for a given ensemble size there is an optimal value of this cutoff radius, and (b) the optimal cutoff radius increases as the ensemble size increases.},
	number = {3},
	urldate = {2018-05-25TZ},
	journal = {Monthly Weather Review},
	author = {Houtekamer, P. L. and Mitchell, Herschel L.},
	month = {mar},
	year = {1998},
	pages = {796--811}
}

@article{mahmoudabadbozchelou2021data,
  title={Data-driven physics-informed constitutive metamodeling of complex fluids: A multifidelity neural network (MFNN) framework},
  author={Mahmoudabadbozchelou, Mohammadamin and Caggioni, Marco and Shahsavari, Setareh and Hartt, William H and Em Karniadakis, George and Jamali, Safa},
  journal={Journal of Rheology},
  volume={65},
  number={2},
  pages={179--198},
  year={2021},
  publisher={The Society of Rheology}
}

@article{houtekamer_review_2016,
	title = {Review of the {Ensemble} {Kalman} {Filter} for {Atmospheric} {Data} {Assimilation}},
	volume = {144},
	issn = {0027-0644},
	url = {https://journals.ametsoc.org/doi/abs/10.1175/MWR-D-15-0440.1},
	doi = {10.1175/MWR-D-15-0440.1},
	abstract = {This paper reviews the development of the ensemble Kalman filter (EnKF) for atmospheric data assimilation. Particular attention is devoted to recent advances and current challenges. The distinguishing properties of three well-established variations of the EnKF algorithm are first discussed. Given the limited size of the ensemble and the unavoidable existence of errors whose origin is unknown (i.e., system error), various approaches to localizing the impact of observations and to accounting for these errors have been proposed. However, challenges remain; for example, with regard to localization of multiscale phenomena (both in time and space). For the EnKF in general, but higher-resolution applications in particular, it is desirable to use a short assimilation window. This motivates a focus on approaches for maintaining balance during the EnKF update. Also discussed are limited-area EnKF systems, in particular with regard to the assimilation of radar data and applications to tracking severe storms and tropical cyclones. It seems that relatively less attention has been paid to optimizing EnKF assimilation of satellite radiance observations, the growing volume of which has been instrumental in improving global weather predictions. There is also a tendency at various centers to investigate and implement hybrid systems that take advantage of both the ensemble and the variational data assimilation approaches; this poses additional challenges and it is not clear how it will evolve. It is concluded that, despite more than 10 years of operational experience, there are still many unresolved issues that could benefit from further research.Contents Introduction...4490Popular flavors of the EnKF algorithm...4491 General description...4491Stochastic and deterministic filters...4492 The stochastic filter...4492The deterministic filter...4492Sequential or local filters...4493 Sequential ensemble Kalman filters...4493The local ensemble transform Kalman filter...4494Extended state vector...4494Issues for the development of algorithms...4495Use of small ensembles...4495 Monte Carlo methods...4495Validation of reliability...4497Use of group filters with no inbreeding...4498Sampling error due to limited ensemble size: The rank problem...4498Covariance localization...4499 Localization in the sequential filter...4499Localization in the LETKF...4499Issues with localization...4500Summary...4501Methods to increase ensemble spread...4501 Covariance inflation...4501 Additive inflation...4501Multiplicative inflation...4502Relaxation to prior ensemble information...4502Issues with inflation...4503Diffusion and truncation...4503Error in physical parameterizations...4504 Physical tendency perturbations...4504Multimodel, multiphysics, and multiparameter approaches...4505Future directions...4505Realism of error sources...4506Balance and length of the assimilation window...4506 The need for balancing methods...4506Time-filtering methods...4506Toward shorter assimilation windows...4507Reduction of sources of imbalance...4507Regional data assimilation...4508 Boundary conditions and consistency across multiple domains...4509Initialization of the starting ensemble...4510Preprocessing steps for radar observations...4510Use of radar observations for convective-scale analyses...4511Use of radar observations for tropical cyclone analyses...4511Other issues with respect to LAM data assimilation...4511The assimilation of satellite observations...4512 Covariance localization...4512Data density...4513Bias-correction procedures...4513Impact of covariance cycling...4514Assumptions regarding observational error...4514Recommendations regarding satellite observations...4515Computational aspects...4515 Parameters with an impact on quality...4515Overview of current parallel algorithms...4516Evolution of computer architecture...4516Practical issues...4517Approaching the gray zone...4518Summary...4518Hybrids with variational and EnKF components...4519 Hybrid background error covariances...4519E4DVar with the α control variable...4519Not using linearized models with 4DEnVar...4520The hybrid gain algorithm...4521Open issues and recommendations...4521Summary and discussion...4521 Stochastic or deterministic filters...4522The nature of system error...4522Going beyond the synoptic scales...4522Satellite observations...4523Hybrid systems...4523Future of the EnKF...4523APPENDIX A...4524Types of Filter Divergence...4524 Classical filter divergence...4524Catastrophic filter divergence...4524APPENDIX B...4524Systems Available for Download...4524References...4525},
	number = {12},
	urldate = {2018-05-25TZ},
	journal = {Monthly Weather Review},
	author = {Houtekamer, P. L. and Zhang, Fuqing},
	month = {jun},
	year = {2016},
	pages = {4489--4532}
}

@article{bocquet_beyond_2010,
	title = {Beyond {Gaussian} {Statistical} {Modeling} in {Geophysical} {Data} {Assimilation}},
	volume = {138},
	issn = {0027-0644},
	url = {https://journals.ametsoc.org/doi/10.1175/2010MWR3164.1},
	doi = {10.1175/2010MWR3164.1},
	abstract = {This review discusses recent advances in geophysical data assimilation beyond Gaussian statistical modeling, in the fields of meteorology, oceanography, as well as atmospheric chemistry. The non-Gaussian features are stressed rather than the nonlinearity of the dynamical models, although both aspects are entangled. Ideas recently proposed to deal with these non-Gaussian issues, in order to improve the state or parameter estimation, are emphasized. The general Bayesian solution to the estimation problem and the techniques to solve it are first presented, as well as the obstacles that hinder their use in high-dimensional and complex systems. Approximations to the Bayesian solution relying on Gaussian, or on second-order moment closure, have been wholly adopted in geophysical data assimilation (e.g., Kalman filters and quadratic variational solutions). Yet, nonlinear and non-Gaussian effects remain. They essentially originate in the nonlinear models and in the non-Gaussian priors. How these effects are handled within algorithms based on Gaussian assumptions is then described. Statistical tools that can diagnose them and measure deviations from Gaussianity are recalled. The following advanced techniques that seek to handle the estimation problem beyond Gaussianity are reviewed: maximum entropy filter, Gaussian anamorphosis, non-Gaussian priors, particle filter with an ensemble Kalman filter as a proposal distribution, maximum entropy on the mean, or strictly Bayesian inferences for large linear models, etc. Several ideas are illustrated with recent or original examples that possess some features of high-dimensional systems. Many of the new approaches are well understood only in special cases and have difficulties that remain to be circumvented. Some of the suggested approaches are quite promising, and sometimes already successful for moderately large though specific geophysical applications. Hints are given as to where progress might come from.},
	number = {8},
	urldate = {2018-05-25TZ},
	journal = {Monthly Weather Review},
	author = {Bocquet, Marc and Pires, Carlos A. and Wu, Lin},
	month = {apr},
	year = {2010},
	pages = {2997--3023}
}

@article{egmont-petersen_image_2002,
	title = {Image processing with neural networks—a review},
	volume = {35},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320301001789},
	doi = {10.1016/S0031-3203(01)00178-9},
	abstract = {We review more than 200 applications of neural networks in image processing and discuss the present and possible future role of neural networks, especially feed-forward neural networks, Kohonen feature maps and Hopfield neural networks. The various applications are categorised into a novel two-dimensional taxonomy for image processing algorithms. One dimension specifies the type of task performed by the algorithm: preprocessing, data reduction/feature extraction, segmentation, object recognition, image understanding and optimisation. The other dimension captures the abstraction level of the input data processed by the algorithm: pixel-level, local feature-level, structure-level, object-level, object-set-level and scene characterisation. Each of the six types of tasks poses specific constraints to a neural-based approach. These specific conditions are discussed in detail. A synthesis is made of unresolved problems related to the application of pattern recognition techniques in image processing and specifically to the application of neural networks. Finally, we present an outlook into the future application of neural networks and relate them to novel developments.},
	number = {10},
	urldate = {2018-05-25TZ},
	journal = {Pattern Recognition},
	author = {Egmont-Petersen, M. and de Ridder, D. and Handels, H.},
	month = {oct},
	year = {2002},
	keywords = {Digital image processing, Feature extraction, Image compression, Image understanding, Invariant pattern recognition, Neural networks, Object recognition, Optimization, Preprocessing, Segmentation},
	pages = {2279--2301}
}

@article{egmont-petersen_image_2002-1,
	title = {Image processing with neural networks—a review},
	volume = {35},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320301001789},
	doi = {10.1016/S0031-3203(01)00178-9},
	abstract = {We review more than 200 applications of neural networks in image processing and discuss the present and possible future role of neural networks, especially feed-forward neural networks, Kohonen feature maps and Hopfield neural networks. The various applications are categorised into a novel two-dimensional taxonomy for image processing algorithms. One dimension specifies the type of task performed by the algorithm: preprocessing, data reduction/feature extraction, segmentation, object recognition, image understanding and optimisation. The other dimension captures the abstraction level of the input data processed by the algorithm: pixel-level, local feature-level, structure-level, object-level, object-set-level and scene characterisation. Each of the six types of tasks poses specific constraints to a neural-based approach. These specific conditions are discussed in detail. A synthesis is made of unresolved problems related to the application of pattern recognition techniques in image processing and specifically to the application of neural networks. Finally, we present an outlook into the future application of neural networks and relate them to novel developments.},
	number = {10},
	urldate = {2018-05-25TZ},
	journal = {Pattern Recognition},
	author = {Egmont-Petersen, M. and de Ridder, D. and Handels, H.},
	month = {oct},
	year = {2002},
	keywords = {Digital image processing, Feature extraction, Image compression, Image understanding, Invariant pattern recognition, Neural networks, Object recognition, Optimization, Preprocessing, Segmentation},
	pages = {2279--2301}
}

@article{lorenc_met._nodate,
	title = {The {Met}. {Office} global three-dimensional variational data assimilation scheme},
	volume = {126},
	copyright = {Copyright © 2000 Royal Meteorological Society},
	issn = {1477-870X},
	url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.49712657002},
	doi = {10.1002/qj.49712657002},
	abstract = {The Met. Office has developed a variational assimilation for its Unified Model forecast system, which contains a grid-point mode) that is run operationally in global, mesoscale, and stratospheric configuration. Key characteristics of the design are: • a development path from three-dimensional to four-dimensional variational assimilation; • global and limited-area configurations; • variational analysis of perturbations; • and a carefully designed, well conditioned background term. The background term is implemented using a sequence of variable transforms to independent balanced and unbalanced variables, to vertical modes, and to spectral coefficients. The coefficients used are based on statistics from differences of one- and two-day forecasts valid at the same time. The covariance model represents many of the features seen in the covariances of forecast differences. The three-dimensional variational data assimilation (3D-Var) system was implemented in the operational global forecast system on 29 March 1999. In parallel trials, the 3D-Var system gave a 2.7\% improvement in a composite skill score (verified against observations and weighted according to the importance of each field).},
	language = {en},
	number = {570},
	urldate = {2018-05-25TZ},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Lorenc, A. C. and Ballard, S. P. and Bell, R. S. and Ingleby, N. B. and Andrews, P. L. F. and Barker, D. M. and Bray, J. R. and Clayton, A. M. and Dalby, T. and Li, D. and Payne, T. J. and Saunders, F. W.},
	keywords = {Numerical weather prediction, Variational data assimilation},
	pages = {2991--3012}
}

@article{hardman-mountford_relating_2003,
	series = {{ENVIFISH}: {Investigating} environmental causes of pelagic fisheries variability in the {SE} {Atlantic}},
	title = {Relating sardine recruitment in the {Northern} {Benguela} to satellite-derived sea surface height using a neural network pattern recognition approach},
	volume = {59},
	issn = {0079-6611},
	url = {http://www.sciencedirect.com/science/article/pii/S0079661103001721},
	doi = {10.1016/j.pocean.2003.07.005},
	abstract = {Processes of enrichment, concentration and retention are thought to be important for the successful recruitment of small pelagic fish in upwelling areas, but are difficult to measure. In this study, a novel approach is used to examine the role of spatio-temporal oceanographic variability on recruitment success of the Northern Benguela sardine Sardinops sagax. This approach applies a neural network pattern recognition technique, called a self-organising map (SOM), to a seven-year time series of satellite-derived sea level data. The Northern Benguela is characterised by quasi-perennial upwelling of cold, nutrient-rich water and is influenced by intrusions of warm, nutrient-poor Angola Current water from the north. In this paper, these processes are categorised in terms of their influence on recruitment success through the key ocean triad mechanisms of enrichment, concentration and retention. Moderate upwelling is seen as favourable for recruitment, whereas strong upwelling, weak upwelling and Angola Current intrusion appear detrimental to recruitment success. The SOM was used to identify characteristic patterns from sea level difference data and these were interpreted with the aid of sea surface temperature data. We found that the major oceanographic processes of upwelling and Angola Current intrusion dominated these patterns, allowing them to be partitioned into those representing recruitment favourable conditions and those representing adverse conditions for recruitment. A marginally significant relationship was found between the index of sardine recruitment and the frequency of recruitment favourable conditions (r2 = 0.61, p = 0.068, n = 6). Because larvae are vulnerable to environmental influences for a period of at least 50 days after spawning, the SOM was then used to identify windows of persistent favourable conditions lasting longer than 50 days, termed recruitment favourable periods (RFPs). The occurrence of RFPs was compared with back-calculated spawning dates for each cohort. Finally, a comparison of RFPs with the time of spawning and the index of recruitment showed that in years where there were 50 or more days of favourable conditions following spawning, good recruitment followed (Mann-Whitney U-test: p = 0.064, n = 6). These results show the value of the SOM technique for describing spatio-temporal variability in oceanographic processes. Variability in these processes appears to be an important factor influencing recruitment in the Northern Benguela sardine, although the available data time series is currently too short to be conclusive. Nonetheless, the analysis of satellite data, using a neural network pattern-recognition approach, provides a useful framework for investigating fisheries recruitment problems.},
	number = {2},
	urldate = {2018-05-25TZ},
	journal = {Progress in Oceanography},
	author = {Hardman-Mountford, N. J. and Richardson, A. J. and Boyer, D. C. and Kreiner, A. and Boyer, H. J.},
	month = {oct},
	year = {2003},
	keywords = {16°S 9°E to 26°S 16°E, PSW, Namibia, PSW, South Atlantic, Benguela upwelling, Pelagic fisheries, Sardinops sagax, Satellite altimetry, Self Organizing Maps, Surface currents, Surface temperature},
	pages = {241--255}
}

@article{hardman-mountford_relating_2003-1,
	series = {{ENVIFISH}: {Investigating} environmental causes of pelagic fisheries variability in the {SE} {Atlantic}},
	title = {Relating sardine recruitment in the {Northern} {Benguela} to satellite-derived sea surface height using a neural network pattern recognition approach},
	volume = {59},
	issn = {0079-6611},
	url = {http://www.sciencedirect.com/science/article/pii/S0079661103001721},
	doi = {10.1016/j.pocean.2003.07.005},
	abstract = {Processes of enrichment, concentration and retention are thought to be important for the successful recruitment of small pelagic fish in upwelling areas, but are difficult to measure. In this study, a novel approach is used to examine the role of spatio-temporal oceanographic variability on recruitment success of the Northern Benguela sardine Sardinops sagax. This approach applies a neural network pattern recognition technique, called a self-organising map (SOM), to a seven-year time series of satellite-derived sea level data. The Northern Benguela is characterised by quasi-perennial upwelling of cold, nutrient-rich water and is influenced by intrusions of warm, nutrient-poor Angola Current water from the north. In this paper, these processes are categorised in terms of their influence on recruitment success through the key ocean triad mechanisms of enrichment, concentration and retention. Moderate upwelling is seen as favourable for recruitment, whereas strong upwelling, weak upwelling and Angola Current intrusion appear detrimental to recruitment success. The SOM was used to identify characteristic patterns from sea level difference data and these were interpreted with the aid of sea surface temperature data. We found that the major oceanographic processes of upwelling and Angola Current intrusion dominated these patterns, allowing them to be partitioned into those representing recruitment favourable conditions and those representing adverse conditions for recruitment. A marginally significant relationship was found between the index of sardine recruitment and the frequency of recruitment favourable conditions (r2 = 0.61, p = 0.068, n = 6). Because larvae are vulnerable to environmental influences for a period of at least 50 days after spawning, the SOM was then used to identify windows of persistent favourable conditions lasting longer than 50 days, termed recruitment favourable periods (RFPs). The occurrence of RFPs was compared with back-calculated spawning dates for each cohort. Finally, a comparison of RFPs with the time of spawning and the index of recruitment showed that in years where there were 50 or more days of favourable conditions following spawning, good recruitment followed (Mann-Whitney U-test: p = 0.064, n = 6). These results show the value of the SOM technique for describing spatio-temporal variability in oceanographic processes. Variability in these processes appears to be an important factor influencing recruitment in the Northern Benguela sardine, although the available data time series is currently too short to be conclusive. Nonetheless, the analysis of satellite data, using a neural network pattern-recognition approach, provides a useful framework for investigating fisheries recruitment problems.},
	number = {2},
	urldate = {2018-05-25TZ},
	journal = {Progress in Oceanography},
	author = {Hardman-Mountford, N. J. and Richardson, A. J. and Boyer, D. C. and Kreiner, A. and Boyer, H. J.},
	month = {oct},
	year = {2003},
	keywords = {16°S 9°E to 26°S 16°E, PSW, Namibia, PSW, South Atlantic, Benguela upwelling, Pelagic fisheries, Sardinops sagax, Satellite altimetry, Self Organizing Maps, Surface currents, Surface temperature},
	pages = {241--255}
}

@misc{noauthor_20_nodate,
	title = {(20) {The} {Kalman} {Filter} and {Related} {Algorithms}: {A} {Literature} {Review}},
	shorttitle = {(20) {The} {Kalman} {Filter} and {Related} {Algorithms}},
	url = {https://www.researchgate.net/publication/236897001_The_Kalman_Filter_and_Related_Algorithms_A_Literature_Review},
	abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
	language = {en},
	urldate = {2018-05-22TZ}
}

@article{dramsch202070,
  title={70 years of machine learning in geoscience in review},
  author={Dramsch, Jesper S{\"o}ren},
  journal={Advances in geophysics},
  volume={61},
  pages={1--55},
  year={2020},
  publisher={Elsevier}
}

@book{brunton2022data,
  title={Data-driven science and engineering: Machine learning, dynamical systems, and control},
  author={Brunton, Steven L and Kutz, J Nathan},
  year={2022},
  publisher={Cambridge University Press}
}

@article{shi2015convolutional,
  title={Convolutional LSTM network: A machine learning approach for precipitation nowcasting},
  author={Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-Kin and Woo, Wang-chun},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@article{ravuri2021skilful,
  title={Skilful precipitation nowcasting using deep generative models of radar},
  author={Ravuri, Suman and Lenc, Karel and Willson, Matthew and Kangin, Dmitry and Lam, Remi and Mirowski, Piotr and Fitzsimons, Megan and Athanassiadou, Maria and Kashem, Sheleem and Madge, Sam and others},
  journal={Nature},
  volume={597},
  number={7878},
  pages={672--677},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{karniadakis2021physics,
  title={Physics-informed machine learning},
  author={Karniadakis, George Em and Kevrekidis, Ioannis G and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  journal={Nature Reviews Physics},
  volume={3},
  number={6},
  pages={422--440},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{young2018recent,
  title={Recent trends in deep learning based natural language processing},
  author={Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
  journal={ieee Computational intelligenCe magazine},
  volume={13},
  number={3},
  pages={55--75},
  year={2018},
  publisher={IEEE}
}

@article{voulodimos2018deep,
  title={Deep learning for computer vision: A brief review},
  author={Voulodimos, Athanasios and Doulamis, Nikolaos and Doulamis, Anastasios and Protopapadakis, Eftychios},
  journal={Computational intelligence and neuroscience},
  volume={2018},
  year={2018},
  publisher={Hindawi}
}

@article{kouw2019review,
  title={A review of domain adaptation without target labels},
  author={Kouw, Wouter M and Loog, Marco},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={43},
  number={3},
  pages={766--785},
  year={2019},
  publisher={IEEE}
}

@article{zhuang2020comprehensive,
  title={A comprehensive survey on transfer learning},
  author={Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  journal={Proceedings of the IEEE},
  volume={109},
  number={1},
  pages={43--76},
  year={2020},
  publisher={IEEE}
}

@misc{noauthor_gaussian_nodate,
	title = {Gaussian propagation {Kalman} {Filter} {\textbackslash}thanks\{\vphantom{\}}{Identify} applicable funding agency here. {If} none, delete this. - {Overleaf}},
	url = {https://www.overleaf.com/16206836vytrtjqhwkph},
	abstract = {The online platform for scientific writing. Overleaf is free: start writing now with one click. No sign-up required. Great on your iPad.},
	language = {en},
	urldate = {2018-05-19TZ}
}

@misc{noauthor_imt_nodate,
	title = {imt atlantique template latex - {Recherche} {Google}},
	url = {https://www.google.fr/search?client=ubuntu&hs=zl5&channel=fs&dcr=0&ei=aprDWrqLBYzbU86pqpAO&q=imt+atlantique+template+latex&oq=imt+atlantique+template+latex&gs_l=psy-ab.3...6202.7119.0.7237.6.6.0.0.0.0.106.466.5j1.6.0....0...1c.1.64.psy-ab..0.5.405...33i160k1j33i21k1.0.7AO76oMTXiQ},
	urldate = {2018-04-03TZ}
}

@article{jing2019data,
  title={Data Assimilation with a Machine Learned Observation Operator and Application to the Assimilation of Satellite data for Sea Ice Models},
  author={Jing, Siyang},
 journal={University of North Carolina at Chapel Hill},
  year={2019}
}

@ARTICLE{8423072,
  author={A. {Karpatne} and I. {Ebert-Uphoff} and S. {Ravela} and H. A. {Babaie} and V. {Kumar}},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Machine Learning for the Geosciences: Challenges and Opportunities}, 
  year={2019},
  volume={31},
  number={8},
  pages={1544-1554},
  doi={10.1109/TKDE.2018.2861006}}

@article{van_leeuwen_p._j._nonlinear_2010,
	title = {Nonlinear data assimilation in geosciences: an extremely efficient particle filter},
	volume = {136},
	issn = {0035-9009},
	shorttitle = {Nonlinear data assimilation in geosciences},
	url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.699},
	doi = {10.1002/qj.699},
	abstract = {Abstract Almost all research fields in geosciences use numerical models and observations and combine these using data?assimilation techniques. With ever?increasing resolution and complexity, the numerical models tend to be highly nonlinear and also observations become more complicated and their relation to the models more nonlinear. Standard data?assimilation techniques like (ensemble) Kalman filters and variational methods like 4D?Var rely on linearizations and are likely to fail in one way or another. Nonlinear data?assimilation techniques are available, but are only efficient for small?dimensional problems, hampered by the so?called ?curse of dimensionality?. Here we present a fully nonlinear particle filter that can be applied to higher dimensional problems by exploiting the freedom of the proposal density inherent in particle filtering. The method is illustrated for the three?dimensional Lorenz model using three particles and the much more complex 40?dimensional Lorenz model using 20 particles. By also applying the method to the 1000?dimensional Lorenz model, again using only 20 particles, we demonstrate the strong scale?invariance of the method, leading to the optimistic conjecture that the method is applicable to realistic geophysical problems. Copyright ? 2010 Royal Meteorological Society},
	number = {653},
	urldate = {2018-03-30TZ},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {{van Leeuwen P. J.}},
	month = {dec},
	year = {2010},
	keywords = {data assimilation, particle filtering},
	pages = {1991--1999}
}

@article{van2017neural,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{van2010nonlinear,
  title={Nonlinear data assimilation in geosciences: an extremely efficient particle filter},
  author={Van Leeuwen, Peter Jan},
  journal={Quarterly Journal of the Royal Meteorological Society},
  volume={136},
  number={653},
  pages={1991--1999},
  year={2010},
  publisher={Wiley Online Library}
}

@article{song2019learning,
  title={Learning normal patterns via adversarial attention-based autoencoder for abnormal event detection in videos},
  author={Song, Hao and Sun, Che and Wu, Xinxiao and Chen, Mei and Jia, Yunde},
  journal={IEEE Transactions on Multimedia},
  volume={22},
  number={8},
  pages={2138--2148},
  year={2019},
  publisher={IEEE}
}

@inproceedings{polyak2019attention,
  title={Attention-based wavenet autoencoder for universal voice conversion},
  author={Polyak, Adam and Wolf, Lior},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6800--6804},
  year={2019},
  organization={IEEE}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{lguensat_eddynet:_2017,
	title = {{EddyNet}: {A} {Deep} {Neural} {Network} {For} {Pixel}-{Wise} {Classification} of {Oceanic} {Eddies}},
	shorttitle = {{EddyNet}},
	url = {http://arxiv.org/abs/1711.03954},
	abstract = {This work presents EddyNet, a deep learning based architecture for automated eddy detection and classification from Sea Surface Height (SSH) maps provided by the Copernicus Marine and Environment Monitoring Service (CMEMS). EddyNet is a U-Net like network that consists of a convolutional encoder-decoder followed by a pixel-wise classification layer. The output is a map with the same size of the input where pixels have the following labels {\textbackslash}\{'0': Non eddy, '1': anticyclonic eddy, '2': cyclonic eddy{\textbackslash}\}. We investigate the use of SELU activation function instead of the classical ReLU+BN and we use an overlap based loss function instead of the cross entropy loss. Keras Python code, the training datasets and EddyNet weights files are open-source and freely available on https://github.com/redouanelg/EddyNet.},
	urldate = {2018-03-28TZ},
	journal = {arXiv:1711.03954 [physics]},
	author = {Lguensat, Redouane and Sun, Miao and Fablet, Ronan and Mason, Evan and Tandeo, Pierre and Chen, Ge},
	month = {nov},
	year = {2017},
	note = {arXiv: 1711.03954},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Physics - Atmospheric and Oceanic Physics}
}

@article{herrera2020neural,
  title={Neural jump ordinary differential equations: Consistent continuous-time prediction and filtering},
  author={Herrera, Calypso and Krach, Florian and Teichmann, Josef},
  journal={arXiv preprint arXiv:2006.04727},
  year={2020}
}

@article{kharazmi2021hp,
  title={hp-VPINNs: Variational physics-informed neural networks with domain decomposition},
  author={Kharazmi, Ehsan and Zhang, Zhongqiang and Karniadakis, George Em},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={374},
  pages={113547},
  year={2021},
  publisher={Elsevier}
}


@inproceedings{park_bilinear_1994,
	title = {Bilinear recurrent neural network},
	volume = {3},
	doi = {10.1109/ICNN.1994.374501},
	abstract = {A recurrent neural network and its training algorithm are proposed in this paper. Since the proposed algorithm is based on the bilinear polynomial, it can model many nonlinear systems with much more parsimony than the higher order neural networks based on Volterra series. The proposed bilinear recurrent neural network (BLRNN) is compared with multilayer perceptron neural networks (MLPNN) for time series prediction problems. The results show that the BLRNN is robust and outperforms the MLPNN in terms of prediction accuracy},
	booktitle = {, 1994 {IEEE} {International} {Conference} on {Neural} {Networks}, 1994. {IEEE} {World} {Congress} on {Computational} {Intelligence}},
	author = {Park, D. C. and Zhu, Yan},
	month = {jun},
	year = {1994},
	keywords = {Artificial neural networks, Computer networks, Electronic mail, Feedforward neural networks, Intelligent networks, Neural networks, Neurons, Nonlinear systems, Polynomials, Recurrent neural networks, bilinear polynomial, bilinear recurrent neural network, learning (artificial intelligence), multilayer perceptron neural networks, multilayer perceptrons, nonlinear systems, prediction accuracy, prediction theory, recurrent neural nets, time series, time series prediction problems, training algorithm},
	pages = {1459--1464 vol.3}
}

@article{wang_runge-kutta_1998,
	title = {Runge-{Kutta} neural network for identification of dynamical systems in high accuracy},
	volume = {9},
	issn = {1045-9227},
	doi = {10.1109/72.661124},
	abstract = {This paper proposes Runge-Kutta neural networks (RKNNs) for identification of unknown dynamical systems described by ordinary differential equations (i.e., ordinary differential equation or ODE systems) with high accuracy. These networks are constructed according to the Runge-Kutta approximation method. The main attraction of the RKNNs is that they precisely estimate the changing rates of system states (i.e., the right-hand side of the ODE x˙=f(x)) directly in their subnetworks based on the space-domain interpolation within one sampling interval such that they can do long-term prediction of system state trajectories. We show theoretically the superior generalization and long-term prediction capability of the RKNNs over the normal neural networks. Two types of learning algorithms are investigated for the RKNNs, gradient-and nonlinear recursive least-squares-based algorithms. Convergence analysis of the learning algorithms is done theoretically. Computer simulations demonstrate the proved properties of the RKNNs},
	number = {2},
	journal = {IEEE Transactions on Neural Networks},
	author = {Wang, Yi-Jen and Lin, Chin-Teng},
	month = {mar},
	year = {1998},
	keywords = {Algorithm design and analysis, Approximation methods, Computer simulation, Convergence, Differential equations, Interpolation, Neural networks, Runge-Kutta methods, Runge-Kutta neural network, Sampling methods, State estimation, Trajectory, convergence analysis, convergence of numerical methods, differential equations, feedforward neural nets, generalisation (artificial intelligence), gradient least-squares-based algorithms, identification, interpolation, learning (artificial intelligence), learning algorithms, least squares approximations, long-term prediction, nonlinear recursive least-squares-based algorithms, ordinary differential equations, space-domain interpolation, system state trajectories, uncertain systems, unknown dynamical systems},
	pages = {294--307}
}

@article{pham_stochastic_2001,
	title = {Stochastic {Methods} for {Sequential} {Data} {Assimilation} in {Strongly} {Nonlinear} {Systems}},
	volume = {129},
	issn = {0027-0644},
	url = {https://journals.ametsoc.org/doi/abs/10.1175/1520-0493(2001)129%3C1194%3ASMFSDA%3E2.0.CO%3B2},
	doi = {10.1175/1520-0493(2001)129<1194:SMFSDA>2.0.CO;2},
	abstract = {This paper considers several filtering methods of stochastic nature, based on Monte Carlo drawing, for the sequential data assimilation in nonlinear models. They include some known methods such as the particle filter and the ensemble Kalman filter and some others introduced by the author: the second-order ensemble Kalman filter and the singular extended interpolated filter. The aim is to study their behavior in the simple nonlinear chaotic Lorenz system, in the hope of getting some insight into more complex models. It is seen that these filters perform satisfactory, but the new filters introduced have the advantage of being less costly. This is achieved through the concept of second-order-exact drawing and the selective error correction, parallel to the tangent space of the attractor of the system (which is of low dimension). Also introduced is the use of a forgetting factor, which could enhance significantly the filter stability in this nonlinear context.},
	number = {5},
	urldate = {2018-02-05TZ},
	journal = {Monthly Weather Review},
	author = {Pham, Dinh Tuan},
	month = {may},
	year = {2001},
	pages = {1194--1207}
}

@article{lorenz_atmospheric_1982,
	title = {Atmospheric predictability experiments with a large numerical model},
	volume = {34},
	issn = {2153-3490},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.2153-3490.1982.tb01839.x/abstract},
	doi = {10.1111/j.2153-3490.1982.tb01839.x},
	abstract = {The instability of the atmosphere places an upper bound on the predictability of instantaneous weather patterns. The skill with which current operational forecasting procedures are observed to perform determines a lower bound. Estimates of both bounds are obtained by comparing the ECMWF operational forecast for each day of a 100-day sequence at one range with the operational forecast for the same day at another range, and with the analysis for that day. The estimated bounds are reasonably close together. Predictions at least ten days ahead as skilful as predictions now made seven days ahead appear to be possible. Additional improvements at extended range may be realized if the one-day forecast is capable of being improved significantly.},
	language = {en},
	number = {6},
	urldate = {2018-01-08TZ},
	journal = {Tellus},
	author = {Lorenz, E. N.},
	month = {dec},
	year = {1982},
	pages = {505--513}
}

@article{gordon_simulation_2000,
	title = {The simulation of {SST}, sea ice extents and ocean heat transports in a version of the {Hadley} {Centre} coupled model without flux adjustments},
	volume = {16},
	issn = {0930-7575, 1432-0894},
	url = {https://link.springer.com/article/10.1007/s003820050010},
	doi = {10.1007/s003820050010},
	abstract = {Results are presented from a new version of the Hadley Centre coupled model (HadCM3) that does not require flux adjustments to prevent large climate drifts in the simulation. The model has both an improved atmosphere and ocean component. In particular, the ocean has a 1.25° × 1.25° degree horizontal resolution and leads to a considerably improved simulation of ocean heat transports compared to earlier versions with a coarser resolution ocean component. The model does not have any spin up procedure prior to coupling and the simulation has been run for over 400 years starting from observed initial conditions. The sea surface temperature (SST) and sea ice simulation are shown to be stable and realistic. The trend in global mean SST is less than 0.009 °C per century. In part, the improved simulation is a consequence of a greater compatibility of the atmosphere and ocean model heat budgets. The atmospheric model surface heat and momentum budget are evaluated by comparing with climatological ship-based estimates. Similarly the ocean model simulation of poleward heat transports is compared with direct ship-based observations for a number of sections across the globe. Despite the limitations of the observed datasets, it is shown that the coupled model is able to reproduce many aspects of the observed heat budget.},
	language = {en},
	number = {2-3},
	urldate = {2018-01-08TZ},
	journal = {Climate Dynamics},
	author = {Gordon, C. and Cooper, C. and Senior, C. A. and Banks, H. and Gregory, J. M. and Johns, T. C. and Mitchell, J. F. B. and Wood, R. A.},
	month = {feb},
	year = {2000},
	pages = {147--168}
}

@article{klemas_remote_2011,
	title = {Remote {Sensing} of {Sea} {Surface} {Salinity}: {An} {Overview} with {Case} {Studies}},
	issn = {0749-0208},
	shorttitle = {Remote {Sensing} of {Sea} {Surface} {Salinity}},
	url = {http://www.jcronline.org/doi/abs/10.2112/JCOASTRES-D-11-00060.1},
	doi = {10.2112/JCOASTRES-D-11-00060.1},
	abstract = {Sea surface salinity (SSS) is critical for studying biological and physical processes in the ocean, such as the global water balance, ocean currents, and evaporation rates. The water and heat fluxes associated with precipitation and evaporation over global oceans are fundamental in regulating climate and weather. Yet measurements of global SSSs are sparse and do not show the required temporal and spatial variability of SSS distributions. Airborne microwave radiometers, such as the Scanning Low-Frequency Microwave Radiometer (SLFMR) and the Salinity, Temperature, and Roughness Remote Scanner (STARRS), have been used successfully to map SSS and its variability, but only in estuaries and coastal waters. Since 2009, SSS has been measured from satellite orbit by the European Soil Moisture and Ocean Salinity satellite, which is designed to provide synthesized SSS maps with a high accuracy. Other salinity-related satellites are being developed, such as Aquarius, which will provide the global view of salinity variability needed for climate studies. The objectives of this paper are to review remote sensing techniques for mapping SSS and to demonstrate the application of microwave radiometry by presenting two case studies.},
	urldate = {2018-01-08TZ},
	journal = {Journal of Coastal Research},
	author = {Klemas, Victor},
	month = {jul},
	year = {2011},
	pages = {830--838}
}

@incollection{chelton_chapter_2001,
	series = {Satellite {Altimetry} and {Earth} {Sciences}},
	title = {Chapter 1 {Satellite} {Altimetry}},
	volume = {69},
	url = {http://www.sciencedirect.com/science/article/pii/S0074614201801467},
	abstract = {The basic concept of satellite altimetry is to measure the range from the satellite to the sea surface. The altimeter transmits a short pulse of microwave radiation with known power toward the sea surface. The pulse interacts with the rough sea surface and a part of the incident radiation reflects back to the altimeter. The chapter emphasizes on the correction algorithms applied to the dual-frequency altimeter onboard the TOPEX/POSEIDON (T/P) satellite. This state-of-the-art altimeter sets the standard for future altimeter missions as it is significantly more accurate than any of the other altimeters that have been launched to date. To provide assurance that the performance requirements for altimeter measurement accuracy are met or exceeded, extensive calibration and validation (cal/val) are important elements of altimeter missions. Cal/val embraces a wide variety of activities, ranging from the interpretation of information from internal-calibration modes of the sensors to the validation of the fully corrected sea-level estimates using in situ data. The chapter concludes with a summary of the T/P mission design and an assessment of the performance of the T/P dual-frequency altimeter in addition, as well as an overview of future altimeter missions.},
	urldate = {2018-01-08TZ},
	booktitle = {International {Geophysics}},
	publisher = {Academic Press},
	author = {Chelton, Dudley B. and Ries, John C. and Haines, Bruce J. and Fu, Lee-Lueng and Callahan, Philip S.},
	editor = {Fu, Lee-Lueng and Cazenave, Anny},
	month = jan,
	year = {2001},
	doi = {10.1016/S0074-6142(01)80146-7},
	pages = {1--ii}
}

@incollection{chelton_chapter_2001-1,
	series = {Satellite {Altimetry} and {Earth} {Sciences}},
	title = {Chapter 1 {Satellite} {Altimetry}},
	volume = {69},
	url = {http://www.sciencedirect.com/science/article/pii/S0074614201801467},
	abstract = {The basic concept of satellite altimetry is to measure the range from the satellite to the sea surface. The altimeter transmits a short pulse of microwave radiation with known power toward the sea surface. The pulse interacts with the rough sea surface and a part of the incident radiation reflects back to the altimeter. The chapter emphasizes on the correction algorithms applied to the dual-frequency altimeter onboard the TOPEX/POSEIDON (T/P) satellite. This state-of-the-art altimeter sets the standard for future altimeter missions as it is significantly more accurate than any of the other altimeters that have been launched to date. To provide assurance that the performance requirements for altimeter measurement accuracy are met or exceeded, extensive calibration and validation (cal/val) are important elements of altimeter missions. Cal/val embraces a wide variety of activities, ranging from the interpretation of information from internal-calibration modes of the sensors to the validation of the fully corrected sea-level estimates using in situ data. The chapter concludes with a summary of the T/P mission design and an assessment of the performance of the T/P dual-frequency altimeter in addition, as well as an overview of future altimeter missions.},
	urldate = {2018-01-08TZ},
	booktitle = {International {Geophysics}},
	publisher = {Academic Press},
	author = {Chelton, Dudley B. and Ries, John C. and Haines, Bruce J. and Fu, Lee-Lueng and Callahan, Philip S.},
	editor = {Fu, Lee-Lueng and Cazenave, Anny},
	month = jan,
	year = {2001},
	doi = {10.1016/S0074-6142(01)80146-7},
	pages = {1--ii}
}
@INPROCEEDINGS{9553130,
  author={Ouala, Said and Fablet, Ronan and Drumetz, Lucas and Chapron, Bertrand and Pascual, Ananda and Collard, Fabrice and Gaultier, Lucile},
  booktitle={2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS}, 
  title={End-to-End Kalman Filter for the Reconstruction of Sea Surface Dynamics from Satellite Data}, 
  year={2021},
  volume={},
  number={},
  pages={7414-7417},
  doi={10.1109/IGARSS47720.2021.9553130}}
@article{chelton_global_2005,
	title = {Global {Microwave} {Satellite} {Observations} of {Sea} {Surface} {Temperature} for {Numerical} {Weather} {Prediction} and {Climate} {Research}},
	volume = {86},
	issn = {0003-0007},
	url = {http://journals.ametsoc.org/doi/abs/10.1175/BAMS-86-8-1097},
	doi = {10.1175/BAMS-86-8-1097},
	abstract = {Obtaining global sea surface temperature (SST) fields for the ocean boundary condition in numerical weather prediction (NWP) models and for climate research has long been problematic. Historically, such fields have been constructed by a blending of in situ observations from ships and buoys and satellite infrared observations from the Advanced Very High Resolution Radiometer (AVHRR) that has been operational on NOAA satellites since November 1981. The resolution of these global SST fields is limited by the sparse coverage of in situ observations in many areas of the World Ocean and cloud contamination of AVHRR observations, which can exceed 75\% over the subpolar oceans. As clouds and aerosols are essentially transparent to microwave radiation, satellite microwave observations can greatly improve the sampling and resolution of global SST fields. The Advanced Microwave Scanning Radiometer on the NASA Earth Observing System (EOS) Aqua satellite (AMSR-E) is providing the first highly accurate and global satellite microwave observations of SST. The potential for AMSR-E observations to improve the sampling, resolution, and accuracy of SST fields for NWP and climate research is demonstrated from example SST fields and from an investigation of the sensitivity of NWP models to specification of the SST boundary condition.},
	number = {8},
	urldate = {2018-01-08TZ},
	journal = {Bulletin of the American Meteorological Society},
	author = {Chelton, Dudley B. and Wentz, Frank J.},
	month = aug,
	year = {2005},
	pages = {1097--1115}
}

@article{escudier_improvement_2013,
	title = {Improvement of coastal and mesoscale observation from space: {Application} to the northwestern {Mediterranean} {Sea}},
	volume = {40},
	issn = {1944-8007},
	shorttitle = {Improvement of coastal and mesoscale observation from space},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/grl.50324/abstract},
	doi = {10.1002/grl.50324},
	abstract = {We present an innovative approach to the generation of remotely sensed high-resolution sea surface topography that improves coastal and mesoscale dynamic characterization. This new method is applied for the period 2002–2010 in the northwestern Mediterranean Sea, an area marked by a small Rossby radius. The spectral content of the new mapped data is closer to that of the along-track signal and displays higher levels of energy in the mesoscale bandwidth with the probability distribution of the new velocity fields 30\% closer to drifter estimations. The fields yield levels of eddy kinetic energy 25\% higher than standard altimetry products, especially over regions regularly impacted by mesoscale instabilities. Moreover, qualitative and quantitative comparisons with drifters, glider, and satellite sea surface temperature observations further confirm that the new altimetry product provides, in many cases, a better representation of mesoscale features (more than 25\% improvement in correlation with glider data during an experiment).},
	language = {en},
	number = {10},
	urldate = {2017-12-20TZ},
	journal = {Geophysical Research Letters},
	author = {Escudier, Romain and Bouffard, Jérôme and Pascual, Ananda and Poulain, Pierre-Marie and Pujol, Marie-Isabelle},
	month = {may},
	year = {2013},
	keywords = {4243 Marginal and semi-enclosed seas, 4262 Ocean observing systems, 4512 Currents, 4520 Eddies and mesoscale processes, 4594 Instruments and techniques, Mediterranean Sea, mesoscale variability, satellite altimetry},
	pages = {2148--2153}
}

@article{escudier_improvement_2013-1,
	title = {Improvement of coastal and mesoscale observation from space: {Application} to the northwestern {Mediterranean} {Sea}},
	volume = {40},
	issn = {1944-8007},
	shorttitle = {Improvement of coastal and mesoscale observation from space},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/grl.50324/abstract},
	doi = {10.1002/grl.50324},
	abstract = {We present an innovative approach to the generation of remotely sensed high-resolution sea surface topography that improves coastal and mesoscale dynamic characterization. This new method is applied for the period 2002–2010 in the northwestern Mediterranean Sea, an area marked by a small Rossby radius. The spectral content of the new mapped data is closer to that of the along-track signal and displays higher levels of energy in the mesoscale bandwidth with the probability distribution of the new velocity fields 30\% closer to drifter estimations. The fields yield levels of eddy kinetic energy 25\% higher than standard altimetry products, especially over regions regularly impacted by mesoscale instabilities. Moreover, qualitative and quantitative comparisons with drifters, glider, and satellite sea surface temperature observations further confirm that the new altimetry product provides, in many cases, a better representation of mesoscale features (more than 25\% improvement in correlation with glider data during an experiment).},
	language = {en},
	number = {10},
	urldate = {2017-12-20TZ},
	journal = {Geophysical Research Letters},
	author = {Escudier, Romain and Bouffard, Jérôme and Pascual, Ananda and Poulain, Pierre-Marie and Pujol, Marie-Isabelle},
	month = may,
	year = {2013},
	keywords = {4243 Marginal and semi-enclosed seas, 4262 Ocean observing systems, 4512 Currents, 4520 Eddies and mesoscale processes, 4594 Instruments and techniques, Mediterranean Sea, mesoscale variability, satellite altimetry},
	pages = {2148--2153}
}

@misc{noauthor_image_nodate,
	title = {image denoising with patch based pca - {Google} {Scholar}},
	url = {https://scholar.google.fr/scholar?q=image+denoising+with+patch+based+pca&hl=fr&as_sdt=0&as_vis=1&oi=scholart&sa=X&ved=0ahUKEwjTwrSO8JjYAhXLPhQKHfPsBv0QgQMIKTAA},
	urldate = {2017-12-20TZ}
}

@misc{noauthor_improvement_nodate,
	title = {Improvement of coastal and mesoscale observation from space: {Application} to the northwestern {Mediterranean} {Sea} - {Escudier} - 2013 - {Geophysical} {Research} {Letters} - {Wiley} {Online} {Library}},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/grl.50324/pdf},
	urldate = {2017-12-20TZ}
}

@article{donlon_operational_2012,
	series = {Advanced {Along} {Track} {Scanning} {Radiometer}({AATSR}) {Special} {Issue}},
	title = {The {Operational} {Sea} {Surface} {Temperature} and {Sea} {Ice} {Analysis} ({OSTIA}) system},
	volume = {116},
	issn = {0034-4257},
	url = {http://www.sciencedirect.com/science/article/pii/S0034425711002197},
	doi = {10.1016/j.rse.2010.10.017},
	abstract = {This paper describes a new Sea surface temperature (SST) analysis that is produced with global coverage on a daily basis at the Met Office called the Operational SST and Sea Ice Analysis (OSTIA) system. OSTIA uses satellite SST data provided by international agencies via the Group for High Resolution SST (GHRSST) Regional/Global Task Sharing (R/GTS) framework. GHRSST products include data from microwave and infrared satellite instruments with accompanying uncertainty estimates. The system also uses in situ SST data available over the Global Telecommunications System (GTS) and a sea-ice concentration product from the EUMETSAT Ocean and Sea Ice Satellite Applications Facility (OSI-SAF). The SST analysis is a multi-scale optimal interpolation that is designed for applications in numerical weather prediction and ocean forecasting systems. The background error covariance matrix is specified using ocean model data and the analysis uses correlation length scales of 10km and 100km. The OSTIA system produces a foundation SST estimate (SSTfnd, which is the SST free of diurnal variability) at an output grid resolution of 1/20° ({\textasciitilde}6km) although the smallest analysis feature resolution is based on the correlation length scale of 10km. All satellite SST data are adjusted for bias errors based on a combination of ENVISAT Advanced Along Track Scanning Radiometer (AATSR) SST data and in situ SST measurements from drifting buoys. Data are filtered (based on surface wind speed data) to remove diurnal variability and AATSR data are adjusted to represent the SST at the same depth as drifting buoy measurements (0.2–1m) before bias adjustments are made. Global coverage outputs are provided each day in GHRSST L4 netCDF format. A variety of secondary products are also provided including weekly and monthly mean data sets. OSTIA products are continuously monitored and validation/verification activities demonstrate that SST products have zero mean bias and an accuracy of {\textasciitilde}0.57K compared to in situ measurements. OSTIA is now used operationally as a boundary condition for all weather forecast models at the Met Office and at European Centre for Medium-range Weather Forecasting (ECMWF). OSTIA is produced by the Met Office as part of the European Union Global Monitoring for Environment and Security (GMES) MyOcean project.},
	number = {Supplement C},
	urldate = {2017-12-20TZ},
	journal = {Remote Sensing of Environment},
	author = {Donlon, Craig J. and Martin, Matthew and Stark, John and Roberts-Jones, Jonah and Fiedler, Emma and Wimmer, Werenfrid},
	month = {jan},
	year = {2012},
	keywords = {SST, Sea ice, Sea surface temperature},
	pages = {140--158}
}

@article{braakmann-folgmann_sea_2017,
	title = {Sea {Level} {Anomaly} {Prediction} using {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1710.07099},
	abstract = {Sea level change, one of the most dire impacts of anthropogenic global warming, will affect a large amount of the world's population. However, sea level change is not uniform in time and space, and the skill of conventional prediction methods is limited due to the ocean's internal variabi-lity on timescales from weeks to decades. Here we study the potential of neural network methods which have been used successfully in other applications, but rarely been applied for this task. We develop a combination of a convolutional neural network (CNN) and a recurrent neural network (RNN) to ana-lyse both the spatial and the temporal evolution of sea level and to suggest an independent, accurate method to predict interannual sea level anomalies (SLA). We test our method for the northern and equatorial Pacific Ocean, using gridded altimeter-derived SLA data. We show that the used network designs outperform a simple regression and that adding a CNN improves the skill significantly. The predictions are stable over several years.},
	urldate = {2017-12-20TZ},
	journal = {arXiv:1710.07099 [cs]},
	author = {Braakmann-Folgmann, Anne and Roscher, Ribana and Wenzel, Susanne and Uebbing, Bernd and Kusche, Jürgen},
	month = {oct},
	year = {2017},
	note = {arXiv: 1710.07099},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}
@article{noyes1974oscillatory,
  title={Oscillatory chemical reactions},
  author={Noyes, Richard M and Field, Richard J},
  journal={Annual Review of Physical Chemistry},
  volume={25},
  number={1},
  pages={95--119},
  year={1974},
  publisher={Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA}
}
@article{kingma_adam:_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2017-12-20TZ},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Learning}
}

@article{fablet_bilinear_2017,
	title = {Bilinear residual {Neural} {Network} for the identification and forecasting of dynamical systems},
	url = {https://scirate.com/arxiv/1712.07003},
	abstract = {Due to the increasing availability of large-scale observation and simulation datasets, data-driven representations arise as efficient and relevant computation representations of dynamical systems for a wide range of applications, where model-driven models based on ordinary differential equation remain the state-of-the-art approaches. In this work, we investigate neural networks (NN) as physically-sound data-driven representations of such systems. Reinterpreting Runge-Kutta methods as graphical models, we consider a residual NN architecture and introduce bilinear layers to embed non-linearities which are intrinsic features of dynamical systems. From numerical experiments for classic dynamical systems, we demonstrate the relevance of the proposed NN-based architecture both in terms of forecasting performance and model identification.},
	urldate = {2017-12-20TZ},
	journal = {SciRate},
	author = {Fablet, Ronan and Ouala, Said and Herzet, Cedric},
	month = {dec},
	year = {2017}
}

@unpublished{lguensat_data-driven_2017,
	title = {Data-driven {Interpolation} of {Sea} {Level} {Anomalies} using {Analog} {Data} {Assimilation}},
	url = {https://hal.archives-ouvertes.fr/hal-01609851},
	abstract = {Despite the well-known limitations of Optimal Interpolation (OI), it remains the conventional method to interpolate Sea Level Anomalies (SLA) from altimeter-derived along-track data. In consideration of the recent developments of data-driven methods as a means to better exploit large-scale observation , simulation and reanalysis datasets for solving inverse problems, this study addresses the improvement of the reconstruction of higher-resolution SLA fields using analog strategies. The reconstruction is stated as an ana-log data assimilation issue, where the analog models rely on patch-based and EOF-based representations to circumvent the curse of dimensionality. We implement an Observation System Simulation Experiment in the South China sea. The reported results show the relevance of the proposed framework with a significant gain in terms of root mean square error for scales below 100km. We further discuss the usefulness of the proposed analog model as a means to exploit high-resolution model simulations for the processing and analysis of current and future satellite-derived altimetric data.},
	urldate = {2017-12-20TZ},
	author = {Lguensat, Redouane and Huynh Viet, Phi and Sun, Miao and Chen, Ge and Fenglin, Tian and Chapron, Bertrand and FABLET, Ronan},
	month = {oct},
	year = {2017},
	keywords = {Analog Data Assimilation, Data-driven methods, Interpolation, Sea Level Anomaly, Sea Surface Height}
}

@article{fablet_data-driven_2017,
	title = {Data-{Driven} {Models} for the {Spatio}-{Temporal} {Interpolation} of {Satellite}-{Derived} {SST} {Fields}},
	volume = {3},
	doi = {10.1109/TCI.2017.2749184},
	abstract = {Satellite-derived products are of key importance for the high-resolution monitoring of the ocean surface on a global scale. Due to the sensitivity of spaceborne sensors to the atmospheric conditions as well as the associated spatio-temporal sampling, ocean remote sensing data may be subject to high-missing data rates. The spatio-temporal interpolation of these data remains a key challenge to deliver L4 gridded products to end-users. Whereas operational products mostly rely on model-driven approaches, especially optimal interpolation based on Gaussian process priors, the availability of large-scale observation and simulation datasets calls for the development of novel data-driven models. This study investigates such models. We extend the recently introduced analog data assimilation to high-dimensional spatio-temporal fields using a multiscale patch-based decomposition. Using an observing system simulation experiment for sea surface temperature, we demonstrate the relevance of the proposed data-driven scheme for the real missing data patterns of the high-resolution infrared METOP sensor. It has resulted in a significant improvement w.r.t. state-of-the-art techniques in terms of interpolation error (about 50\% of relative gain) and spectral characteristics for horizontal scales smaller than 100 km. We further discuss the key features and parameterizations of the proposed data-driven approach as well as its relevance with respect to classical interpolation techniques.},
	number = {4},
	journal = {IEEE Transactions on Computational Imaging},
	author = {Fablet, R. and Viet, P. H. and Lguensat, R.},
	month = {dec},
	year = {2017},
	keywords = {Analog and exemplar-based models, Computational modeling, Data assimilation, Data models, Gaussian process priors, Gaussian processes, Interpolation, L4 gridded products, Principal component analysis, SST fields, Sea surface, analog data assimilation, associated spatio-temporal sampling, atmospheric techniques, classical interpolation techniques, data assimilation, data patterns, data-driven approach, global scale, high-dimensional spatio-temporal fields, high-missing data rates, high-resolution infrared METOP sensor, high-resolution monitoring, horizontal scales, interpolation, interpolation error, large-scale observation, model-driven approaches, multi-scale decomposition, novel data-driven models, observing system simulation experiment, ocean remote sensing data, ocean remtote sensing data, ocean surface, ocean temperature, operational products, optimal interpolation, patch-based representation, satellite-derived products, sea surface temperature, simulation datasets, spaceborne sensors, spatio-temporal interpolation, spatiotemporal phenomena, spectral characteristics},
	pages = {647--657}
}

@misc{noauthor_data-driven_nodate,
	title = {Data-{Driven} {Models} for the {Spatio}-{Temporal} {Interpolation} of {Satellite}-{Derived} {SST} {Fields} - {IEEE} {Journals} \& {Magazine}},
	url = {http://ieeexplore.ieee.org/document/8025578/},
	urldate = {2017-12-20TZ}
}
@article{sharp1990stochastic,
  title={Stochastic differential equations in finance},
  author={Sharp, Keith P},
  journal={Applied mathematics and Computation},
  volume={37},
  number={2},
  pages={131--148},
  year={1990},
  publisher={Elsevier}
}
@article{davis_predictability_1976,
	title = {Predictability of {Sea} {Surface} {Temperature} and {Sea} {Level} {Pressure} {Anomalies} over the {North} {Pacific} {Ocean}},
	volume = {6},
	issn = {0022-3670},
	url = {http://journals.ametsoc.org/doi/abs/10.1175/1520-0485(1976)006%3C0249:POSSTA%3E2.0.CO;2},
	doi = {10.1175/1520-0485(1976)006<0249:POSSTA>2.0.CO;2},
	abstract = {Nonseasonal variability of sea level pressure (SLP) and sea surface temperature (SST) in the mid-latitude North Pacific Ocean is examined. The objective is examination of the basic scales of the variability and determination of possible causal connections which might allow prediction of short-term climatic (time scales between a month and a year) variability. Using empirical orthogonal function descriptions of the spatial structure, it is found that SLP variability is concentrated in a few large-scale modes but has a nearly white frequency spectrum. SST variability is spatially complex (being spread over many spatial modes, some of which have small-scale changes) but is dominated by low-frequency changes. The use of linear statistical estimators to examine predictability is discussed and the importance of limiting the number of candidate data used in a correlation starch is underscored. Using linear statistical predictors, it is found that (A) SST anomalies can be predicted from SST observations several months in advance with measurable skill, (B) the anomalous SLP variability can be specified from simultaneous SST data with significant skill, thus showing the fields are related, and (C) future SLP anomalous variability cannot be predicted from SST data although previous SLP can be specified. The fact that previous SLP variability is better specified by SST data than is simultaneous SLP variability, coupled with a complete inability to predict future SLP anomalies, suggests that, in the region studied and on the time scales of a month to a year, the observed connection between SST and SLP variabilities is the result of the atmosphere driving the ocean.},
	number = {3},
	urldate = {2017-12-20TZ},
	journal = {Journal of Physical Oceanography},
	author = {Davis, Russ E.},
	month = may,
	year = {1976},
	pages = {249--266}
}

@article{nicholls2010sea,
  title={Sea-level rise and its impact on coastal zones},
  author={Nicholls, Robert J and Cazenave, Anny},
  journal={science},
  volume={328},
  number={5985},
  pages={1517--1520},
  year={2010},
  publisher={American Association for the Advancement of Science}
}

@article{nobre_variations_1996,
	title = {Variations of {Sea} {Surface} {Temperature}, {Wind} {Stress}, and {Rainfall} over the {Tropical} {Atlantic} and {South} {America}},
	volume = {9},
	issn = {0894-8755},
	url = {http://journals.ametsoc.org/doi/abs/10.1175/1520-0442(1996)009%3C2464:VOSSTW%3E2.0.CO;2},
	doi = {10.1175/1520-0442(1996)009<2464:VOSSTW>2.0.CO;2},
	abstract = {Empirical orthogonal functions (E0Fs) and composite analyses are used to investigate the development of sea surface temperature (SST) anomaly patterns over the tropical Atlantic. The evolution of large-scale rainfall anomaly patterns over the equatorial Atlantic and South America are also investigated. The EOF analyses revealed that a pattern of anomalous SST and wind stress asymmetric relative to the equator is the dominant mode of interannual and longer variability over the tropical Atlantic. The most important findings of this study are as follows. Atmospheric circulation anomalies precede the development of basinwide anomalous SST patterns over the tropical Atlantic. Anomalous SST originate off the African coast simultaneously with atmospheric circulation anomalies and expand westward afterward. The time lag between wind stress relaxation (strengthening) and maximum SST warming (cooling) is about two months. Anomalous atmospheric circulation patterns over northern tropical Atlantic are phase locked to the seasonal cycle. Composite fields of SLP and wind stress over northern tropical Atlantic can be distinguished from random only within a few months preceding the March–May (MAM) season. Observational evidence is presented to show that the El Niño–Southern Oscillation phenomenon in the Pacific influences atmospheric circulation and SST anomalies over northern tropical Atlantic through atmospheric teleconnection patterns into higher latitudes of the Northern Hemisphere. The well-known droughts over northeastern Brazil (Nordeste) are a local manifestation of a much larger-scale rainfall anomaly pattern encompassing the whole equatorial Atlantic and Amazon region. Negative rainfall anomalies to the south of the equator during MAM, which is the rainy season for the Nordeste region, are related to an early withdrawal of the intertropical convergence zone toward the warm SST anomalies over the northern tropical Atlantic. Also, it is shown that precipitation anomalies over southern and northern parts of the Nordeste are out of phase: drought years over the northern Nordeste are commonly preceded by wetter years over the southern Nordeste, and vice versa.},
	number = {10},
	urldate = {2017-12-20TZ},
	journal = {Journal of Climate},
	author = {Nobre, Paulo and Shukla, J.},
	month = oct,
	year = {1996},
	pages = {2464--2479}
}

@article{holm2015variational,
  title={Variational principles for stochastic fluid dynamics},
  author={Holm, Darryl D},
  journal={Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume={471},
  number={2176},
  pages={20140963},
  year={2015},
  publisher={The Royal Society Publishing}
}

@article{Sparse_multiscale,
author = {Champion, Kathleen P. and Brunton, Steven L. and Kutz, J. Nathan},
title = {Discovery of Nonlinear Multiscale Systems: Sampling Strategies and Embeddings},
journal = {SIAM Journal on Applied Dynamical Systems},
volume = {18},
number = {1},
pages = {312-333},
year = {2019},
doi = {10.1137/18M1188227},

URL = { 
        https://doi.org/10.1137/18M1188227
    
},
eprint = { 
        https://doi.org/10.1137/18M1188227
    
}

}
@misc{Acc_rate_grad_NODE2,
      title={ANODEV2: A Coupled Neural ODE Evolution Framework}, 
      author={Tianjun Zhang and Zhewei Yao and Amir Gholami and Kurt Keutzer and Joseph Gonzalez and George Biros and Michael Mahoney},
      year={2019},
      eprint={1906.04596},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{Acc_rate_grad_NODE1,
      title={ANODE: Unconditionally Accurate Memory-Efficient Gradients for Neural ODEs}, 
      author={Amir Gholami and Kurt Keutzer and George Biros},
      year={2019},
      eprint={1902.10298},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article {Bongard9943,
	author = {Bongard, Josh and Lipson, Hod},
	title = {Automated reverse engineering of nonlinear dynamical systems},
	volume = {104},
	number = {24},
	pages = {9943--9948},
	year = {2007},
	doi = {10.1073/pnas.0609476104},
	publisher = {National Academy of Sciences},
	abstract = {Complex nonlinear dynamics arise in many fields of science and engineering, but uncovering the underlying differential equations directly from observations poses a challenging task. The ability to symbolically model complex networked systems is key to understanding them, an open problem in many disciplines. Here we introduce for the first time a method that can automatically generate symbolic equations for a nonlinear coupled dynamical system directly from time series data. This method is applicable to any system that can be described using sets of ordinary nonlinear differential equations, and assumes that the (possibly noisy) time series of all variables are observable. Previous automated symbolic modeling approaches of coupled physical systems produced linear models or required a nonlinear model to be provided manually. The advance presented here is made possible by allowing the method to model each (possibly coupled) variable separately, intelligently perturbing and destabilizing the system to extract its less observable characteristics, and automatically simplifying the equations during modeling. We demonstrate this method on four simulated and two real systems spanning mechanics, ecology, and systems biology. Unlike numerical models, symbolic models have explanatory value, suggesting that automated {\textquotedblleft}reverse engineering{\textquotedblright} approaches for model-free symbolic nonlinear system identification may play an increasing role in our ability to understand progressively more complex systems in the future.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/104/24/9943},
	eprint = {https://www.pnas.org/content/104/24/9943.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article {Free_form_genetic_intrac,
	author = {Schmidt, Michael and Lipson, Hod},
	title = {Distilling Free-Form Natural Laws from Experimental Data},
	volume = {324},
	number = {5923},
	pages = {81--85},
	year = {2009},
	doi = {10.1126/science.1165893},
	publisher = {American Association for the Advancement of Science},
	abstract = {For centuries, scientists have attempted to identify and document analytical laws that underlie physical phenomena in nature. Despite the prevalence of computing power, the process of finding natural laws and their corresponding equations has resisted automation. A key challenge to finding analytic relations automatically is defining algorithmically what makes a correlation in observed data important and insightful. We propose a principle for the identification of nontriviality. We demonstrated this approach by automatically searching motion-tracking data captured from various physical systems, ranging from simple harmonic oscillators to chaotic double-pendula. Without any prior knowledge about physics, kinematics, or geometry, the algorithm discovered Hamiltonians, Lagrangians, and other laws of geometric and momentum conservation. The discovery rate accelerated as laws found for simpler systems were used to bootstrap explanations for more complex systems, gradually uncovering the {\textquotedblleft}alphabet{\textquotedblright} used to describe those systems.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/324/5923/81},
	eprint = {https://science.sciencemag.org/content/324/5923/81.full.pdf},
	journal = {Science}
}


@article{frankignoul_sea_1985,
	title = {Sea surface temperature anomalies, planetary waves, and air-sea feedback in the middle latitudes},
	volume = {23},
	issn = {1944-9208},
	url = {http://onlinelibrary.wiley.com/doi/10.1029/RG023i004p00357/abstract},
	doi = {10.1029/RG023i004p00357},
	abstract = {The mechanisms that contribute to the generation and damping of large-scale mid-latitude sea surface temperature (SST) anomalies are discussed. The SST anomalies reflect primarily the response of the upper ocean to the changes in air-sea fluxes that are associated with daily weather fluctuations. Heat flux forcing is dominant in the lower middle latitudes, while wind-driven entrainment may be most effective in the high latitudes; advection by anomalous Ekman current is generally less important, and Ekman pumping is negligible. The SST anomalies decay in part because of entrainment effects associated with mixed-layer deepening and oceanic mixing and in part because of heat exchanges with the atmosphere. The three approaches commonly used to model the evolution of SST anomalies are reviewed: case studies based on monthly or seasonal anomaly maps of the large-scale SST and atmospheric anomalies, numerical simulations with one-dimensional mixed-layer models, and stochastic forcing models. We stress the similarities in the different approaches and discuss their main advantages and limitations. The response of the atmosphere to mid-latitude SST anomalies is considered. First, we discuss the poorly known relationship between SST anomalies and diabatic heating. Using a crude assumption for the air-sea coupling, we consider a two-layer quasi-geostrophic channel model and discuss the stationary wave response to SST anomaly forcing and the resulting air-sea feedback. It is found that the back interaction of the SST anomalies onto the atmosphere causes a weak SST anomaly damping at large scales and a strong one at small scales; the air-sea coupling should also act as an eastward propagator for the SST anomalies. The response of more realistic linear wave models to prescribed diabatic heating is then reviewed, and it is suggested that realistic mid-latitude SST anomalies have a weak influence on the atmospheric circulation, corresponding to changes in the geopotential height of 10–30 m at most. This order of magnitude is consistent with the results of general circulation model experiments and with the limited climate predictability associated with mid-latitude SST anomalies.},
	language = {en},
	number = {4},
	urldate = {2017-12-20TZ},
	journal = {Reviews of Geophysics},
	author = {Frankignoul, Claude},
	month = nov,
	year = {1985},
	keywords = {3339 Ocean/atmosphere interactions, 3384 Acoustic-gravity waves, 4215 Climate and interannual variability, 4504 Air/sea interactions},
	pages = {357--390}
}

@article{rayner_global_2003,
	title = {Global analyses of sea surface temperature, sea ice, and night marine air temperature since the late nineteenth century},
	volume = {108},
	issn = {2156-2202},
	url = {http://onlinelibrary.wiley.com/doi/10.1029/2002JD002670/abstract},
	doi = {10.1029/2002JD002670},
	abstract = {We present the Met Office Hadley Centre's sea ice and sea surface temperature (SST) data set, HadISST1, and the nighttime marine air temperature (NMAT) data set, HadMAT1. HadISST1 replaces the global sea ice and sea surface temperature (GISST) data sets and is a unique combination of monthly globally complete fields of SST and sea ice concentration on a 1° latitude-longitude grid from 1871. The companion HadMAT1 runs monthly from 1856 on a 5° latitude-longitude grid and incorporates new corrections for the effect on NMAT of increasing deck (and hence measurement) heights. HadISST1 and HadMAT1 temperatures are reconstructed using a two-stage reduced-space optimal interpolation procedure, followed by superposition of quality-improved gridded observations onto the reconstructions to restore local detail. The sea ice fields are made more homogeneous by compensating satellite microwave-based sea ice concentrations for the impact of surface melt effects on retrievals in the Arctic and for algorithm deficiencies in the Antarctic and by making the historical in situ concentrations consistent with the satellite data. SSTs near sea ice are estimated using statistical relationships between SST and sea ice concentration. HadISST1 compares well with other published analyses, capturing trends in global, hemispheric, and regional SST well, containing SST fields with more uniform variance through time and better month-to-month persistence than those in GISST. HadMAT1 is more consistent with SST and with collocated land surface air temperatures than previous NMAT data sets.},
	language = {en},
	number = {D14},
	urldate = {2017-12-20TZ},
	journal = {Journal of Geophysical Research: Atmospheres},
	author = {Rayner, N. A. and Parker, D. E. and Horton, E. B. and Folland, C. K. and Alexander, L. V. and Rowell, D. P. and Kent, E. C. and Kaplan, A.},
	month = jul,
	year = {2003},
	keywords = {1610 Atmosphere, 1635 Oceans, 1827 Glaciology, 3339 Meteorology and Atmospheric Dynamics: Ocean/atmosphere interactions, bias correction, climate change, climate data reconstruction, night marine air temperature, sea ice, sea surface temperature},
	pages = {4407}
}

@article{lagaris_artificial_1998,
	title = {Artificial {Neural} {Networks} for {Solving} {Ordinary} and {Partial} {Differential} {Equations}},
	volume = {9},
	issn = {10459227},
	url = {http://arxiv.org/abs/physics/9705023},
	doi = {10.1109/72.712178},
	abstract = {We present a method to solve initial and boundary value problems using artificial neural networks. A trial solution of the differential equation is written as a sum of two parts. The first part satisfies the boundary (or initial) conditions and contains no adjustable parameters. The second part is constructed so as not to affect the boundary conditions. This part involves a feedforward neural network, containing adjustable parameters (the weights). Hence by construction the boundary conditions are satisfied and the network is trained to satisfy the differential equation. The applicability of this approach ranges from single ODE's, to systems of coupled ODE's and also to PDE's. In this article we illustrate the method by solving a variety of model problems and present comparisons with finite elements for several cases of partial differential equations.},
	number = {5},
	urldate = {2017-09-19TZ},
	journal = {IEEE Transactions on Neural Networks},
	author = {Lagaris, I. E. and Likas, A. and Fotiadis, D. I.},
	month = sep,
	year = {1998},
	note = {arXiv: physics/9705023},
	keywords = {Nonlinear Sciences - Cellular Automata and Lattice Gases, Physics - Computational Physics, Quantum Physics},
	pages = {987--1000}
}

@inproceedings{chairez_neural_2009,
	title = {Neural network identification of uncertain 2D partial differential equations},
	doi = {10.1109/ICEEE.2009.5393456},
	abstract = {There are many examples in science and engineering which are reduced to a set of partial differential equations (PDE's) through a process of mathematical modeling. Nevertheless there exist many sources of uncertainties around the aforementioned mathematical representation. It is well known that neural networks can approximate a large set of continuous functions defined on a compact set to an arbitrary accuracy. In this paper a strategy based on DNN for the non parametric identification of a mathematical model described by a class of two dimensional (2D) partial differential equations is proposed. The adaptive laws for weights ensure the "practical stability" of the DNN trajectories to the parabolic 2D-PDE states. To verify the qualitative behavior of the suggested methodology, here a non parametric modeling problem for a distributed parameter plant is analyzed.},
	booktitle = {2009 6th {International} {Conference} on {Electrical} {Engineering}, {Computing} {Science} and {Automatic} {Control} ({CCE})},
	author = {Chairez, I. and Fuentes, R. and Poznyak, A. and Poznyak, T. and Escudero, M. and Viana, L.},
	month = jan,
	year = {2009},
	keywords = {Adaptive Identification, Distributed Parameter Systems, High definition video, Neural Networks, Neural networks, Partial Differential Equations and Practical Stability, Partial differential equations, adaptive laws, continuous functions, distributed parameter systems, identification, mathematical modeling, mathematical representation, mathematics computing, neural nets, neural network identification, nonparametric identification, parabolic 2D-PDE states, parabolic equations, partial differential equations, practical stability, stability, uncertain 2D partial differential equations},
	pages = {1--6}
}

@article{u._p._singh_predictability_2015,
	title = {Predictability {Study} of {Forced} {Lorenz} {Model}: {An} {Artificial} {Neural} {Network} {Approach}},
	shorttitle = {Predictability {Study} of {Forced} {Lorenz} {Model}},
	abstract = {The Lorenz, 1963 model is a simple model that exhibits features such as nonlinear chaotic behavior and the existence of regimes similar to the actual climate system. It can be used for studying the predictability of climate. The Lorenz model has been},
	journal = {Discovery},
	author = {{U. P. Singh} and {A. K. Mittal} and {S. Dwivedi} and A. Tiwari},
	year = {2015}
}

@inproceedings{pasini_can_2005,
	title = {Can we estimate atmospheric predictability by performance of neural network forecasting? the toy case studies of unforced and forced lorenz models},
	shorttitle = {Can we estimate atmospheric predictability by performance of neural network forecasting?},
	doi = {10.1109/CIMSA.2005.1522829},
	abstract = {Not Available},
	booktitle = {{CIMSA}. 2005 {IEEE} {International} {Conference} on {Computational} {Intelligence} for {Measurement} {Systems} and {Applications}, 2005.},
	author = {Pasini, A. and Pelino, V.},
	month = jul,
	year = {2005},
	keywords = {Air pollution, Atmosphere, Atmospheric modeling, Chaos, Computer aided software engineering, Electronic mail, Frequency, Neural networks, Predictive models, Remuneration},
	pages = {69--74}
}

@article{cintra_data_2014,
	title = {Data {Assimilation} by {Artificial} {Neural} {Networks} for an {Atmospheric} {General} {Circulation} {Model}: {Conventional} {Observation}},
	shorttitle = {Data {Assimilation} by {Artificial} {Neural} {Networks} for an {Atmospheric} {General} {Circulation} {Model}},
	url = {http://arxiv.org/abs/1407.4360},
	abstract = {This paper presents an approach for employing artificial neural networks (NN) to emulate an ensemble Kalman filter (EnKF) as a method of data assimilation. The assimilation methods are tested in the Simplified Parameterizations PrimitivE-Equation Dynamics (SPEEDY) model, an atmospheric general circulation model (AGCM), using synthetic observational data simulating localization of balloon soundings. For the data assimilation scheme, the supervised NN, the multilayer perceptrons (MLP-NN), is applied. The MLP-NN are able to emulate the analysis from the local ensemble transform Kalman filter (LETKF). After the training process, the method using the MLP-NN is seen as a function of data assimilation. The NN were trained with data from first three months of 1982, 1983, and 1984. A hind-casting experiment for the 1985 data assimilation cycle using MLP-NN were performed with synthetic observations for January 1985. The numerical results demonstrate the effectiveness of the NN technique for atmospheric data assimilation. The results of the NN analyses are very close to the results from the LETKF analyses, the differences of the monthly average of absolute temperature analyses is of order 0.02. The simulations show that the major advantage of using the MLP-NN is better computational performance, since the analyses have similar quality. The CPU-time cycle assimilation with MLP-NN is 90 times faster than cycle assimilation with LETKF for the numerical experiment.},
	urldate = {2017-09-19TZ},
	journal = {arXiv:1407.4360 [physics]},
	author = {Cintra, Rosangela S. and Velho, Haroldo F. de Campos},
	month = jul,
	year = {2014},
	note = {arXiv: 1407.4360},
	keywords = {Computer Science - Artificial Intelligence, Physics - Atmospheric and Oceanic Physics}
}

@book{volterra_theory_2005,
	title = {Theory of {Functionals} and of {Integral} and {Integro}-differential {Equations}},
	isbn = {9780486442846},
	abstract = {A general theory of the functions depending on a continuous set of values of another function, this volume is based on the author\&\#39;s fundamental notion of the transition from a finite number of variables to a continually infinite number. Deals primarily with integral equations, and also addresses the calculus of variations. 1930 edition.},
	language = {en},
	publisher = {Courier Corporation},
	author = {Volterra, Vito},
	year = {2005},
	note = {Google-Books-ID: HhAoddpkcWgC},
	keywords = {Mathematics / Differential Equations / General}
}

@inproceedings{pasini_can_2005-1,
	title = {Can we estimate atmospheric predictability by performance of neural network forecasting? the toy case studies of unforced and forced lorenz models},
	shorttitle = {Can we estimate atmospheric predictability by performance of neural network forecasting?},
	doi = {10.1109/CIMSA.2005.1522829},
	abstract = {Not Available},
	booktitle = {{CIMSA}. 2005 {IEEE} {International} {Conference} on {Computational} {Intelligence} for {Measurement} {Systems} and {Applications}, 2005.},
	author = {Pasini, A. and Pelino, V.},
	month = jul,
	year = {2005},
	keywords = {Air pollution, Atmosphere, Atmospheric modeling, Chaos, Computer aided software engineering, Electronic mail, Frequency, Neural networks, Predictive models, Remuneration},
	pages = {69--74}
}

@article{svensson_nonlinear_2015,
	title = {Nonlinear {State} {Space} {Model} {Identification} {Using} a {Regularized} {Basis} {Function} {Expansion}},
	url = {http://arxiv.org/abs/1510.00563},
	doi = {10.1109/CAMSAP.2015.7383841},
	abstract = {This paper is concerned with black-box identification of nonlinear state space models. By using a basis function expansion within the state space model, we obtain a flexible structure. The model is identified using an expectation maximization approach, where the states and the parameters are updated iteratively in such a way that a maximum likelihood estimate is obtained. We use recent particle methods with sound theoretical properties to infer the states, whereas the model parameters can be updated using closed-form expressions by exploiting the fact that our model is linear in the parameters. Not to over-fit the flexible model to the data, we also propose a regularization scheme without increasing the computational burden. Importantly, this opens up for systematic use of regularization in nonlinear state space models. We conclude by evaluating our proposed approach on one simulation example and two real-data problems.},
	urldate = {2017-09-18TZ},
	journal = {arXiv:1510.00563 [cs, stat]},
	author = {Svensson, Andreas and Schön, Thomas B. and Solin, Arno and Särkkä, Simo},
	month = dec,
	year = {2015},
	note = {arXiv: 1510.00563},
	keywords = {Computer Science - Systems and Control, Statistics - Computation},
	pages = {481--484}
}

@book{kerschen_nonlinear_2016,
	title = {Nonlinear {Dynamics}, {Volume} 1: {Proceedings} of the 34th {IMAC}, {A} {Conference} and {Exposition} on {Structural} {Dynamics} 2016},
	isbn = {9783319297392},
	shorttitle = {Nonlinear {Dynamics}, {Volume} 1},
	abstract = {Nonlinear Dynamics, Volume 1. Proceedings of the 34th IMAC, A Conference and Exposition on Dynamics of Multiphysical Systems: From Active Materials to Vibroacoustics, 2016, the fi rst volume of ten from the Conference, brings together contributions to this important area of research and engineering. Th e collection presents early fi ndings and case studies on fundamental and applied aspects of Structural Dynamics, including papers on:• Nonlinear Oscillations• Nonlinear Modal Analysis• Nonlinear System Identifi cation• Nonlinear Modeling \& Simulation• Nonlinearity in Practice• Nonlinearity in Multi-Physics Systems• Nonlinear Modes and Modal Interactions},
	language = {en},
	publisher = {Springer},
	author = {Kerschen, Gaetan},
	month = apr,
	year = {2016},
	note = {Google-Books-ID: E0sWDAAAQBAJ},
	keywords = {Science / Mechanics / Dynamics, Science / Mechanics / General, Technology \& Engineering / Manufacturing, Technology \& Engineering / Materials Science / General, Technology \& Engineering / Mechanical}
}

@article{svensson_nonlinear_2015-1,
	title = {Nonlinear {State} {Space} {Model} {Identification} {Using} a {Regularized} {Basis} {Function} {Expansion}},
	url = {http://arxiv.org/abs/1510.00563},
	doi = {10.1109/CAMSAP.2015.7383841},
	abstract = {This paper is concerned with black-box identification of nonlinear state space models. By using a basis function expansion within the state space model, we obtain a flexible structure. The model is identified using an expectation maximization approach, where the states and the parameters are updated iteratively in such a way that a maximum likelihood estimate is obtained. We use recent particle methods with sound theoretical properties to infer the states, whereas the model parameters can be updated using closed-form expressions by exploiting the fact that our model is linear in the parameters. Not to over-fit the flexible model to the data, we also propose a regularization scheme without increasing the computational burden. Importantly, this opens up for systematic use of regularization in nonlinear state space models. We conclude by evaluating our proposed approach on one simulation example and two real-data problems.},
	urldate = {2017-09-18TZ},
	journal = {arXiv:1510.00563 [cs, stat]},
	author = {Svensson, Andreas and Schön, Thomas B. and Solin, Arno and Särkkä, Simo},
	month = dec,
	year = {2015},
	note = {arXiv: 1510.00563},
	keywords = {Computer Science - Systems and Control, Statistics - Computation},
	pages = {481--484}
}

@article{paduart_identification_2010,
  title={Identification of nonlinear systems using polynomial nonlinear state space models},
  author={Paduart, Johan and Lauwers, Lieve and Swevers, Jan and Smolders, Kris and Schoukens, Johan and Pintelon, Rik},
  journal={Automatica},
  volume={46},
  number={4},
  pages={647--656},
  year={2010},
  publisher={Elsevier}
}



@incollection{d._solomatine_data-driven_2009,
	series = {Water {Science} and {Technology} {Library}},
	title = {Data-{Driven} {Modelling}: {Concepts}, {Approaches} and {Experiences}},
	isbn = {9783540798804 9783540798811},
	shorttitle = {Data-{Driven} {Modelling}},
	url = {https://link.springer.com/chapter/10.1007/978-3-540-79881-1_2},
	abstract = {Data-driven modelling is the area of hydroinformatics undergoing fast development. This chapter reviews the main concepts and approaches of data-driven modelling, which is based on computational intelligence and machine-learning methods. A brief overview of the main methods – neural networks, fuzzy rule-based systems and genetic algorithms, and their combination via committee approaches – is provided along with hydrological examples and references to the rest of the book.},
	language = {en},
	urldate = {2017-09-18TZ},
	booktitle = {Practical {Hydroinformatics}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {{D. Solomatine} and {L. M. See} and {R. J. Abrahart}},
	year = {2009},
	doi = {10.1007/978-3-540-79881-1_2},
	pages = {17--30}
}

@incollection{ernst_hairer_gerhard_wanner_syvert_p._norsett_classical_2008,
	series = {Springer {Series} in {Computational} {Mathematics}},
	title = {Classical {Mathematical} {Theory}},
	isbn = {9783540566700 9783540788621},
	url = {https://link.springer.com/chapter/10.1007/978-3-540-78862-1_1},
	abstract = {This first chapter contains the classical theory of differential equations, which we judge useful and important for a profound understanding of numerical processes and phenomena. It will also be the occasion of presenting interesting examples of differential equations and their properties.},
	language = {en},
	urldate = {2017-09-08TZ},
	booktitle = {Solving {Ordinary} {Differential} {Equations} {I}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Ernst Hairer, Gerhard Wanner, Syvert P. Nørsett},
	year = {2008},
	doi = {10.1007/978-3-540-78862-1_1},
	pages = {1--128}
}

@article{field_oscillations_1974,
	title = {Oscillations in chemical systems. {IV}. {Limit} cycle behavior in a model of a real chemical reaction},
	volume = {60},
	issn = {0021-9606},
	url = {http://aip.scitation.org/doi/abs/10.1063/1.1681288},
	doi = {10.1063/1.1681288},
	number = {5},
	urldate = {2017-09-08TZ},
	journal = {The Journal of Chemical Physics},
	author = {Field, Richard J. and Noyes, Richard M.},
	month = mar,
	year = {1974},
	pages = {1877--1884}
}
@article{beauchamp2020intercomparison,
  title={Intercomparison of data-driven and learning-based interpolations of along-track Nadir and wide-swath Swot altimetry observations},
  author={Beauchamp, Maxime and Fablet, Ronan and Ubelmann, Cl{\'e}ment and Ballarotta, Maxime and Chapron, Bertrand},
  journal={Remote Sensing},
  volume={12},
  number={22},
  pages={3806},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@inproceedings{tandeo_analog_2015,
  title={The analog data assimilation: application to 20 years of altimetric data},
  author={Tandeo, Pierre and Ailliot, Pierre and Chapron, Bertrand and Lguensat, Redouane and Fablet, Ronan},
  booktitle={CI 2015: 5th International Workshop on Climate Informatics},
  pages={1--2},
  year={2015}
}



@book{iserles_first_2009,
	title = {A {First} {Course} in the {Numerical} {Analysis} of {Differential} {Equations}},
	isbn = {9780521734905},
	abstract = {Numerical analysis presents different faces to the world. For mathematicians it is a bona fide mathematical theory with an applicable flavour. For scientists and engineers it is a practical, applied subject, part of the standard repertoire of modelling techniques. For computer scientists it is a theory on the interplay of computer architecture and algorithms for real-number calculations. The tension between these standpoints is the driving force of this book, which presents a rigorous account of the fundamentals of numerical analysis of both ordinary and partial differential equations. The exposition maintains a balance between theoretical, algorithmic and applied aspects. This second edition has been extensively updated, and includes new chapters on emerging subject areas: geometric numerical integration, spectral methods and conjugate gradients. Other topics covered include multistep and Runge-Kutta methods; finite difference and finite elements techniques for the Poisson equation; and a variety of algorithms to solve large, sparse algebraic systems.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Iserles, A.},
	year = {2009},
	note = {Google-Books-ID: M0tkw4oUucoC},
	keywords = {Mathematics / Mathematical Analysis, Mathematics / Numerical Analysis}
}

@article{lorenc_met._2000,
	title = {The {Met}. {Office} global three-dimensional variational data assimilation scheme},
	volume = {126},
	issn = {1477-870X},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/qj.49712657002/abstract},
	doi = {10.1002/qj.49712657002},
	abstract = {The Met. Office has developed a variational assimilation for its Unified Model forecast system, which contains a grid-point mode) that is run operationally in global, mesoscale, and stratospheric configuration. Key characteristics of the design are:

* •a development path from three-dimensional to four-dimensional variational assimilation;
* •global and limited-area configurations;
* •variational analysis of perturbations;
* •and a carefully designed, well conditioned background term. The background term is implemented using a sequence of variable transforms to independent balanced and unbalanced variables, to vertical modes, and to spectral coefficients. The coefficients used are based on statistics from differences of one- and two-day forecasts valid at the same time. The covariance model represents many of the features seen in the covariances of forecast differences. The three-dimensional variational data assimilation (3D-Var) system was implemented in the operational global forecast system on 29 March 1999. In parallel trials, the 3D-Var system gave a 2.7\% improvement in a composite skill score (verified against observations and weighted according to the importance of each field).},
	language = {en},
	number = {570},
	urldate = {2017-09-04TZ},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Lorenc, A. C. and Ballard, S. P. and Bell, R. S. and Ingleby, N. B. and Andrews, P. L. F. and Barker, D. M. and Bray, J. R. and Clayton, A. M. and Dalby, T. and Li, D. and Payne, T. J. and Saunders, F. W.},
	month = {oct},
	year = {2000},
	keywords = {Numerical weather prediction, Variational data assimilation},
	pages = {2991--3012}
}

@article{lorenc_met._2000-1,
	title = {The {Met}. {Office} global three-dimensional variational data assimilation scheme},
	volume = {126},
	issn = {1477-870X},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/qj.49712657002/abstract},
	doi = {10.1002/qj.49712657002},
	abstract = {The Met. Office has developed a variational assimilation for its Unified Model forecast system, which contains a grid-point mode) that is run operationally in global, mesoscale, and stratospheric configuration. Key characteristics of the design are:

* •a development path from three-dimensional to four-dimensional variational assimilation;
* •global and limited-area configurations;
* •variational analysis of perturbations;
* •and a carefully designed, well conditioned background term. The background term is implemented using a sequence of variable transforms to independent balanced and unbalanced variables, to vertical modes, and to spectral coefficients. The coefficients used are based on statistics from differences of one- and two-day forecasts valid at the same time. The covariance model represents many of the features seen in the covariances of forecast differences. The three-dimensional variational data assimilation (3D-Var) system was implemented in the operational global forecast system on 29 March 1999. In parallel trials, the 3D-Var system gave a 2.7\% improvement in a composite skill score (verified against observations and weighted according to the importance of each field).},
	language = {en},
	number = {570},
	urldate = {2017-09-04TZ},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Lorenc, A. C. and Ballard, S. P. and Bell, R. S. and Ingleby, N. B. and Andrews, P. L. F. and Barker, D. M. and Bray, J. R. and Clayton, A. M. and Dalby, T. and Li, D. and Payne, T. J. and Saunders, F. W.},
	month = oct,
	year = {2000},
	keywords = {Numerical weather prediction, Variational data assimilation},
	pages = {2991--3012}
}
@book{abarbanel2012analysis,
  title={Analysis of observed chaotic data},
  author={Abarbanel, Henry},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{kalman1963mathematical,
  title={Mathematical description of linear dynamical systems},
  author={Kalman, Rudolf Emil},
  journal={Journal of the Society for Industrial and Applied Mathematics, Series A: Control},
  volume={1},
  number={2},
  pages={152--192},
  year={1963},
  publisher={SIAM}
}

@book{parker2012practical,
  title={Practical numerical algorithms for chaotic systems},
  author={Parker, Thomas S and Chua, Leon},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@book{baker1996chaotic,
  title={Chaotic dynamics: an introduction},
  author={Baker, Gregory L and Baker, Gregory L and Gollub, Jerry P},
  year={1996},
  publisher={Cambridge university press}
}
@article{kalman_new_1960,
	title = {A {New} {Approach} to {Linear} {Filtering} and {Prediction} {Problems}},
	volume = {82},
	issn = {0098-2202},
	url = {http://dx.doi.org/10.1115/1.3662552},
	doi = {10.1115/1.3662552},
	abstract = {The classical filtering and prediction problem is re-examined using the Bode-Shannon representation of random processes and the “state-transition” method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinite-memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the co-efficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.},
	number = {1},
	urldate = {2017-09-04TZ},
	journal = {Journal of Basic Engineering},
	author = {Kalman, R. E.},
	month = mar,
	year = {1960},
	pages = {35--45}
}

@misc{noauthor_new_nodate,
	title = {A {New} {Approach} to {Linear} {Filtering} and {Prediction} {Problems} {\textbar} {Journal} of {Fluids} {Engineering} {\textbar} {ASME} {DC}},
	url = {http://fluidsengineering.asmedigitalcollection.asme.org/article.aspx?articleid=1430402},
	urldate = {2017-09-04TZ}
}

@article{lorenz_deterministic_1963,
	title = {Deterministic {Nonperiodic} {Flow}},
	volume = {20},
	issn = {0022-4928},
	url = {http://journals.ametsoc.org/doi/abs/10.1175/1520-0469(1963)020%3C0130:DNF%3E2.0.CO;2},
	doi = {10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2},
	abstract = {Finite systems of deterministic ordinary nonlinear differential equations may be designed to represent forced dissipative hydrodynamic flow. Solutions of these equations can be identified with trajectories in phase space. For those systems with bounded solutions, it is found that nonperiodic solutions are ordinarily unstable with respect to small modifications, so that slightly differing initial states can evolve into considerably different states. Systems with bounded solutions are shown to possess bounded numerical solutions. A simple system representing cellular convection is solved numerically. All of the solutions are found to be unstable, and almost all of them are nonperiodic. The feasibility of very-long-range weather prediction is examined in the light of these results.},
	number = {2},
	urldate = {2017-09-04TZ},
	journal = {Journal of the Atmospheric Sciences},
	author = {Lorenz, Edward N.},
	month = mar,
	year = {1963},
	pages = {130--141}
}

@article{butcher_1963, title={Coefficients for the study of Runge-Kutta integration processes}, volume={3}, DOI={10.1017/S1446788700027932}, number={2}, journal={Journal of the Australian Mathematical Society}, publisher={Cambridge University Press}, author={Butcher, J. C.}, year={1963}, pages={185–201}}

@book{evensen_data_2009,
	address = {Berlin, Heidelberg},
	title = {Data {Assimilation}},
	isbn = {9783642037108 9783642037115},
	url = {http://link.springer.com/10.1007/978-3-642-03711-5},
	language = {en},
	urldate = {2017-09-04TZ},
	publisher = {Springer Berlin Heidelberg},
	author = {Evensen, Geir},
	year = {2009},
	doi = {10.1007/978-3-642-03711-5}
}

@article{lguensat_analog_2017,
	title = {The {Analog} {Data} {Assimilation}},
	issn = {0027-0644, 1520-0493},
	url = {http://journals.ametsoc.org/doi/10.1175/MWR-D-16-0441.1},
	doi = {10.1175/MWR-D-16-0441.1},
	language = {en},
	urldate = {2017-09-04TZ},
	journal = {Monthly Weather Review},
	author = {Lguensat, Redouane and Tandeo, Pierre and Ailliot, Pierre and Pulido, Manuel and Fablet, Ronan},
	month = {aug},
	year = {2017}
}

@misc{noauthor_deep_nodate,
	title = {Deep {Learning} and {DATA} assimilation - {Overleaf}},
	url = {https://www.overleaf.com/9622132nrzrfrxkycxr},
	abstract = {The online platform for scientific writing. Overleaf is free: start writing now with one click. No sign-up required. Great on your iPad.},
	urldate = {2017-07-21TZ}
}

@article{rezende_stochastic_2014,
	title = {Stochastic {Backpropagation} and {Approximate} {Inference} in {Deep} {Generative} {Models}},
	url = {http://arxiv.org/abs/1401.4082},
	abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
	urldate = {2017-07-11TZ},
	journal = {arXiv:1401.4082 [cs, stat]},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	month = jan,
	year = {2014},
	note = {arXiv: 1401.4082},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology}
}

@article{kingma_auto-encoding_2013,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2017-07-11TZ},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
}




@book{wazwaz_linear_2011,
	title = {Linear and nonlinear integral equations},
	volume = {639},
	url = {http://link.springer.com/content/pdf/10.1007/978-3-642-21449-3.pdf},
	urldate = {2017-07-11TZ},
	publisher = {Springer},
	author = {Wazwaz, Abdul-Majid},
	year = {2011}
}


@article{krishnan_deep_2015,
	title = {Deep {Kalman} {Filters}},
	url = {http://arxiv.org/abs/1511.05121},
	abstract = {Kalman Filters are one of the most influential models of time-varying phenomena. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption in a variety of disciplines. Motivated by recent variational methods for learning deep generative models, we introduce a unified algorithm to efficiently learn a broad spectrum of Kalman filters. Of particular interest is the use of temporal generative models for counterfactual inference. We investigate the efficacy of such models for counterfactual inference, and to that end we introduce the "Healing MNIST" dataset where long-term structure, noise and actions are applied to sequences of digits. We show the efficacy of our method for modeling this dataset. We further show how our model can be used for counterfactual inference for patients, based on electronic health record data of 8,000 patients over 4.5 years.},
	urldate = {2017-07-11TZ},
	journal = {arXiv:1511.05121 [cs, stat]},
	author = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.05121},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
}
@article{rudy2017data,
  title={Data-driven discovery of partial differential equations},
  author={Rudy, Samuel H and Brunton, Steven L and Proctor, Joshua L and Kutz, J Nathan},
  journal={Science advances},
  volume={3},
  number={4},
  pages={e1602614},
  year={2017},
  publisher={American Association for the Advancement of Science}
}
@article{ouala2022bounded,
  title={Bounded nonlinear forecasts of partially observed geophysical systems with physics-constrained deep learning},
  author={Ouala, Said and Brunton, Steven L and Pascual, Ananda and Chapron, Bertrand and Collard, Fabrice and Gaultier, Lucile and Fablet, Ronan},
  journal={arXiv preprint arXiv:2202.05750},
  year={2022}
}
@misc{noauthor_theory_nodate,
	title = {Theory of {Functionals} and of {Integral} and {Integro}-differential {Equations} - {Vito} {Volterra} - {Google} {Livres}},
	url = {https://books.google.fr/books?id=HhAoddpkcWgC&printsec=frontcover&hl=fr&source=gbs_ge_summary_r&cad=0#v=onepage&q&f=false},
	urldate = {2017-07-10TZ}
}
@article{ouala2021learning,
  title={Learning runge-kutta integration schemes for ode simulation and identification},
  author={Ouala, Said and Debreu, Laurent and Pascual, Ananda and Chapron, Bertrand and Collard, Fabrice and Gaultier, Lucile and Fablet, Ronan},
  journal={arXiv preprint arXiv:2105.04999},
  year={2021}
}
@article{park_complex-bilinear_2002,
	title = {Complex-bilinear recurrent neural network for equalization of a digital satellite channel},
	volume = {13},
	url = {http://ieeexplore.ieee.org/abstract/document/1000135/},
	number = {3},
	urldate = {2017-07-10TZ},
	journal = {IEEE Transactions on Neural Networks},
	author = {Park, Dong-Chul and Jeong, Tae-Kyun Jung},
	year = {2002},
	pages = {711--725}
}
@misc{center_for_history_and_new_media_guide_nodate,
	title = {Guide rapide pour débuter},
	url = {http://zotero.org/support/quick_start_guide},
	author = {{Center for History and New Media}}
}




@article{gao_efficient_2021,
	title = {Efficient {Bayesian} network structure learning via local {Markov} boundary search},
	url = {http://arxiv.org/abs/2110.06082},
	abstract = {We analyze the complexity of learning directed acyclic graphical models from observational data in general settings without specific distributional assumptions. Our approach is information-theoretic and uses a local Markov boundary search procedure in order to recursively construct ancestral sets in the underlying graphical model. Perhaps surprisingly, we show that for certain graph ensembles, a simple forward greedy search algorithm (i.e. without a backward pruning phase) suffices to learn the Markov boundary of each node. This substantially improves the sample complexity, which we show is at most polynomial in the number of nodes. This is then applied to learn the entire graph under a novel identifiability condition that generalizes existing conditions from the literature. As a matter of independent interest, we establish finite-sample guarantees for the problem of recovering Markov boundaries from data. Moreover, we apply our results to the special case of polytrees, for which the assumptions simplify, and provide explicit conditions under which polytrees are identifiable and learnable in polynomial time. We further illustrate the performance of the algorithm, which is easy to implement, in a simulation study. Our approach is general, works for discrete or continuous distributions without distributional assumptions, and as such sheds light on the minimal assumptions required to efficiently learn the structure of directed graphical models from data.},
	urldate = {2021-10-17},
	journal = {arXiv:2110.06082 [cs, math, stat]},
	author = {Gao, Ming and Aragam, Bryon},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.06082},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Statistics Theory, NeurIPS, Statistics - Machine Learning},
	annote = {Comment: 30 pages, 3 figures, to appear in NeurIPS 2021},
	file = {Gao_Aragam_2021_Efficient Bayesian network structure learning via local Markov boundary search.pdf:/Users/lucor/Zotero/storage/X5GSTVW9/Gao_Aragam_2021_Efficient Bayesian network structure learning via local Markov boundary search.pdf:application/pdf},
}

@article{zhang_temperature_2021,
	title = {Temperature as {Uncertainty} in {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2110.04403},
	abstract = {Contrastive learning has demonstrated great capability to learn representations without annotations, even outperforming supervised baselines. However, it still lacks important properties useful for real-world application, one of which is uncertainty. In this paper, we propose a simple way to generate uncertainty scores for many contrastive methods by re-purposing temperature, a mysterious hyperparameter used for scaling. By observing that temperature controls how sensitive the objective is to specific embedding locations, we aim to learn temperature as an input-dependent variable, treating it as a measure of embedding confidence. We call this approach "Temperature as Uncertainty", or TaU. Through experiments, we demonstrate that TaU is useful for out-of-distribution detection, while remaining competitive with benchmarks on linear evaluation. Moreover, we show that TaU can be learned on top of pretrained models, enabling uncertainty scores to be generated post-hoc with popular off-the-shelf models. In summary, TaU is a simple yet versatile method for generating uncertainties for contrastive learning. Open source code can be found at: https://github.com/mhw32/temperature-as-uncertainty-public.},
	urldate = {2021-10-17},
	journal = {arXiv:2110.04403 [cs]},
	author = {Zhang, Oliver and Wu, Mike and Bayrooti, Jasmine and Goodman, Noah},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.04403},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 4 pages content; 1 page supplement},
	file = {Zhang et al_2021_Temperature as Uncertainty in Contrastive Learning.pdf:/Users/lucor/Zotero/storage/Y3KKGKU7/Zhang et al_2021_Temperature as Uncertainty in Contrastive Learning.pdf:application/pdf},
}

@article{kendall_what_2017,
	title = {What {Uncertainties} {Do} {We} {Need} in {Bayesian} {Deep} {Learning} for {Computer} {Vision}?},
	abstract = {There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model – uncertainty which can be explained away given enough data. Traditionally it has been difﬁcult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the beneﬁts of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.},
	language = {en},
	author = {Kendall, Alex and Gal, Yarin},
	year = {2017},
	pages = {11},
	file = {Kendall_Gal_2017_What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision.pdf:/Users/lucor/Zotero/storage/8YTR5QYV/Kendall_Gal_2017_What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision.pdf:application/pdf},
}

@phdthesis{gal_uncertainty_2016,
	address = {England},
	type = {Phd {Thesis}},
	title = {Uncertainty in {Deep} {Learning}},
	abstract = {Deep learning has attracted tremendous attention from researchers in various fields of information engineering such as AI, computer vision, and language processing [Kalch- brenner and Blunsom, 2013; Krizhevsky et al., 2012; Mnih et al., 2013], but also from more traditional sciences such as physics, biology, and manufacturing [Anjos et al., 2015; Baldi et al., 2014; Bergmann et al., 2014]. Neural networks, image processing tools such as convolutional neural networks, sequence processing models such as recurrent neural networks, and regularisation tools such as dropout, are used extensively. However, fields such as physics, biology, and manufacturing are ones in which representing model uncertainty is of crucial importance [Ghahramani, 2015; Krzywinski and Altman, 2013]. With the recent shift in many of these fields towards the use of Bayesian uncertainty [Herzog and Ostwald, 2013; Nuzzo, 2014; Trafimow and Marks, 2015], new needs arise from deep learning.
In this work we develop tools to obtain practical uncertainty estimates in deep learning, casting recent deep learning tools as Bayesian models without changing either the models or the optimisation. In the first part of this thesis we develop the theory for such tools, providing applications and illustrative examples. We tie approximate inference in Bayesian models to dropout and other stochastic regularisation techniques, and assess the approximations empirically. We give example applications arising from this connection between modern deep learning and Bayesian modelling such as active learning of image data and data-efficient deep reinforcement learning. We further demonstrate the tools’ practicality through a survey of recent applications making use of the suggested techniques in language applications, medical diagnostics, bioinformatics, image processing, and autonomous driving. In the second part of the thesis we explore the insights stemming from the link between Bayesian modelling and deep learning, and its theoretical implications. We discuss what determines model uncertainty properties, analyse the approximate inference analytically in the linear case, and theoretically examine various priors such as spike and slab priors.},
	language = {en},
	school = {University of Cambridge},
	author = {Gal, Yarin},
	year = {2016},
	file = {Gal_2016_Uncertainty in Deep Learning.pdf:/Users/lucor/Zotero/storage/XB7GMHUX/Gal_2016_Uncertainty in Deep Learning.pdf:application/pdf},
}

@mastersthesis{nguyen_uncertainty_2020,
	address = {Espoo},
	title = {Uncertainty in {Recurrent} {Neural} {Network} with {Dropout}},
	abstract = {Recurrent Neural Network is a powerful tool for processing temporal data. However, assessing prediction uncertainty from recurrent models has proven challenging. This thesis attempts to evaluate the validity of uncertainty from recurrent models using dropout.
Traditional neural network focuses on optimising data likelihood; in order to obtain model and predictive uncertainty, we need to, instead, optimise model posterior. Model posterior is usually intractable, thus we employ various dropout based approach, in the form of variational Bayesian Monte Carlo, to estimate the learning objective. This technique is applied to existing recurrent neural network benchmarks MIMIC-III [9]. The thesis shows that Monte Carlo dropout [6] applied to recurrent neural network [4] can give comparable performance to the current state of the art methods, and meaningful uncertainty of predictions.},
	language = {en},
	school = {Aalto University},
	author = {Nguyen, Kha L. H.},
	year = {2020},
	file = {Nguyen_2020_Uncertainty in Recurrent Neural Network with Dropout.pdf:/Users/lucor/Zotero/storage/HWIX83XI/Nguyen_2020_Uncertainty in Recurrent Neural Network with Dropout.pdf:application/pdf},
}

@article{hullermeier_aleatoric_2020,
	title = {Aleatoric and {Epistemic} {Uncertainty} in {Machine} {Learning}: {An} {Introduction} to {Concepts} and {Methods}},
	shorttitle = {Aleatoric and {Epistemic} {Uncertainty} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1910.09457},
	abstract = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.},
	urldate = {2021-05-31},
	journal = {arXiv:1910.09457},
	author = {Hüllermeier, Eyke and Waegeman, Willem},
	month = sep,
	year = {2020},
	note = {arXiv: 1910.09457},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Hüllermeier_Waegeman_2020_Aleatoric and Epistemic Uncertainty in Machine Learning.pdf:/Users/lucor/Zotero/storage/N2JEWDBS/Hüllermeier_Waegeman_2020_Aleatoric and Epistemic Uncertainty in Machine Learning.pdf:application/pdf},
}

@article{shaker_aleatoric_2020,
	title = {Aleatoric and {Epistemic} {Uncertainty} with {Random} {Forests}},
	url = {http://arxiv.org/abs/2001.00893},
	abstract = {Due to the steadily increasing relevance of machine learning for practical applications, many of which are coming with safety requirements, the notion of uncertainty has received increasing attention in machine learning research in the last couple of years. In particular, the idea of distinguishing between two important types of uncertainty, often refereed to as aleatoric and epistemic, has recently been studied in the setting of supervised learning. In this paper, we propose to quantify these uncertainties with random forests. More specifically, we show how two general approaches for measuring the learner's aleatoric and epistemic uncertainty in a prediction can be instantiated with decision trees and random forests as learning algorithms in a classification setting. In this regard, we also compare random forests with deep neural networks, which have been used for a similar purpose.},
	urldate = {2021-10-15},
	journal = {arXiv:2001.00893 [cs, stat]},
	author = {Shaker, Mohammad Hossein and Hüllermeier, Eyke},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.00893},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 10 pages, 4 figures},
	file = {Shaker_Hüllermeier_2020_Aleatoric and Epistemic Uncertainty with Random Forests.pdf:/Users/lucor/Zotero/storage/RBDSRE4J/Shaker_Hüllermeier_2020_Aleatoric and Epistemic Uncertainty with Random Forests.pdf:application/pdf},
}

@article{shaker_ensemble-based_2021,
	title = {Ensemble-based {Uncertainty} {Quantification}: {Bayesian} versus {Credal} {Inference}},
	shorttitle = {Ensemble-based {Uncertainty} {Quantification}},
	url = {http://arxiv.org/abs/2107.10384},
	abstract = {The idea to distinguish and quantify two important types of uncertainty, often referred to as aleatoric and epistemic, has received increasing attention in machine learning research in the last couple of years. In this paper, we consider ensemble-based approaches to uncertainty quantification. Distinguishing between different types of uncertainty-aware learning algorithms, we specifically focus on Bayesian methods and approaches based on so-called credal sets, which naturally suggest themselves from an ensemble learning point of view. For both approaches, we address the question of how to quantify aleatoric and epistemic uncertainty. The effectiveness of corresponding measures is evaluated and compared in an empirical study on classification with a reject option.},
	urldate = {2021-10-15},
	journal = {arXiv:2107.10384 [cs]},
	author = {Shaker, Mohammad Hossein and Hüllermeier, Eyke},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.10384},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: arXiv admin note: text overlap with arXiv:2001.00893},
	file = {Shaker_Hüllermeier_2021_Ensemble-based Uncertainty Quantification.pdf:/Users/lucor/Zotero/storage/JX67X26H/Shaker_Hüllermeier_2021_Ensemble-based Uncertainty Quantification.pdf:application/pdf},
}

@article{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2021-09-27},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {Kingma_Ba_2017_Adam.pdf:/Users/lucor/Zotero/storage/CLI6TBXN/Kingma_Ba_2017_Adam.pdf:application/pdf},
}

@article{mooij_joint_2020,
	title = {Joint {Causal} {Inference} from {Multiple} {Contexts}},
	url = {http://arxiv.org/abs/1611.10351},
	abstract = {The gold standard for discovering causal relations is by means of experimentation. Over the last decades, alternative methods have been proposed that can infer causal relations between variables from certain statistical patterns in purely observational data. We introduce Joint Causal Inference (JCI), a novel approach to causal discovery from multiple data sets from different contexts that elegantly unifies both approaches. JCI is a causal modeling framework rather than a specific algorithm, and it can be implemented using any causal discovery algorithm that can take into account certain background knowledge. JCI can deal with different types of interventions (e.g., perfect, imperfect, stochastic, etc.) in a unified fashion, and does not require knowledge of intervention targets or types in case of interventional data. We explain how several well-known causal discovery algorithms can be seen as addressing special cases of the JCI framework, and we also propose novel implementations that extend existing causal discovery methods for purely observational data to the JCI setting. We evaluate different JCI implementations on synthetic data and on flow cytometry protein expression data and conclude that JCI implementations can considerably outperform state-of-the-art causal discovery algorithms.},
	urldate = {2021-07-29},
	journal = {arXiv:1611.10351 [cs, stat]},
	author = {Mooij, Joris M. and Magliacane, Sara and Claassen, Tom},
	month = aug,
	year = {2020},
	note = {arXiv: 1611.10351},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	annote = {Comment: Final version, as published by JMLR},
	file = {Mooij et al_2020_Joint Causal Inference from Multiple Contexts.pdf:/Users/lucor/Zotero/storage/KF9HTHR6/Mooij et al_2020_Joint Causal Inference from Multiple Contexts.pdf:application/pdf},
}

@article{maddox_simple_2019,
	title = {A {Simple} {Baseline} for {Bayesian} {Uncertainty} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1902.02476},
	abstract = {We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, SGLD, and temperature scaling.},
	urldate = {2021-09-21},
	journal = {arXiv:1902.02476 [cs, stat]},
	author = {Maddox, Wesley and Garipov, Timur and Izmailov, Pavel and Vetrov, Dmitry and Wilson, Andrew Gordon},
	month = dec,
	year = {2019},
	note = {arXiv: 1902.02476},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Maddox et al_2019_A Simple Baseline for Bayesian Uncertainty in Deep Learning.pdf:/Users/lucor/Zotero/storage/BNILGSMN/Maddox et al_2019_A Simple Baseline for Bayesian Uncertainty in Deep Learning.pdf:application/pdf},
}

@article{xia_hatchensemble_2021,
	title = {{HatchEnsemble}: an efficient and practical uncertainty quantification method for deep neural networks},
	issn = {2198-6053},
	shorttitle = {{HatchEnsemble}},
	url = {https://doi.org/10.1007/s40747-021-00463-1},
	doi = {10.1007/s40747-021-00463-1},
	abstract = {Quantifying predictive uncertainty in deep neural networks is a challenging and yet unsolved problem. Existing quantification approaches can be categorized into two lines. Bayesian methods provide a complete uncertainty quantification theory but are often not scalable to large-scale models. Along another line, non-Bayesian methods have good scalability and can quantify uncertainty with high quality. The most remarkable idea in this line is Deep Ensemble, but it is limited in practice due to its expensive computational cost. Thus, we propose HatchEnsemble to improve the efficiency and practicality of Deep Ensemble. The main idea is to use function-preserving transformations, ensuring HatchNets to inherit the knowledge learned by a single model called SeedNet. This process is called hatching, and HatchNet can be obtained by continuously widening the SeedNet. Based on our method, two different hatches are proposed, respectively, for ensembling the same and different architecture networks. To ensure the diversity of models, we also add random noises to parameters during hatching. Experiments on both clean and corrupted datasets show that HatchEnsemble can give a competitive prediction performance and better-calibrated uncertainty quantification in a shorter time compared with baselines.},
	language = {en},
	urldate = {2021-09-17},
	journal = {Complex Intell. Syst.},
	author = {Xia, Yufeng and Zhang, Jun and Jiang, Tingsong and Gong, Zhiqiang and Yao, Wen and Feng, Ling},
	month = jul,
	year = {2021},
	file = {Xia et al_2021_HatchEnsemble.pdf:/Users/lucor/Zotero/storage/NRCIGKVW/Xia et al_2021_HatchEnsemble.pdf:application/pdf},
}

@article{nakkiran_deep_2019,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	shorttitle = {Deep {Double} {Descent}},
	url = {http://arxiv.org/abs/1912.02292},
	abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
	urldate = {2021-09-16},
	journal = {arXiv:1912.02292 [cs, stat]},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.02292},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {Nakkiran et al_2019_Deep Double Descent.pdf:/Users/lucor/Zotero/storage/CIXLHI6U/Nakkiran et al_2019_Deep Double Descent.pdf:application/pdf},
}

@article{zhang_cyclical_2020,
	title = {Cyclical {Stochastic} {Gradient} {MCMC} for {Bayesian} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1902.03932},
	abstract = {The posteriors over neural network weights are high dimensional and multimodal. Each mode typically characterizes a meaningfully different representation of the data. We develop Cyclical Stochastic Gradient MCMC (SG-MCMC) to automatically explore such distributions. In particular, we propose a cyclical stepsize schedule, where larger steps discover new modes, and smaller steps characterize each mode. We also prove non-asymptotic convergence of our proposed algorithm. Moreover, we provide extensive experimental results, including ImageNet, to demonstrate the scalability and effectiveness of cyclical SG-MCMC in learning complex multimodal distributions, especially for fully Bayesian inference with modern deep neural networks.},
	urldate = {2021-09-06},
	journal = {arXiv:1902.03932 [cs, stat]},
	author = {Zhang, Ruqi and Li, Chunyuan and Zhang, Jianyi and Chen, Changyou and Wilson, Andrew Gordon},
	month = may,
	year = {2020},
	note = {arXiv: 1902.03932},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Methodology},
	file = {Zhang et al_2020_Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning.pdf:/Users/lucor/Zotero/storage/BEDVG8RA/Zhang et al_2020_Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning.pdf:application/pdf},
}

@book{neal_bayesian_1996,
	address = {New York, NY},
	series = {Lecture {Notes} in {Statistics}},
	title = {Bayesian {Learning} for {Neural} {Networks}},
	volume = {118},
	isbn = {978-0-387-94724-2 978-1-4612-0745-0},
	url = {http://link.springer.com/10.1007/978-1-4612-0745-0},
	urldate = {2021-09-04},
	publisher = {Springer New York},
	author = {Neal, Radford M.},
	editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S.},
	year = {1996},
	doi = {10.1007/978-1-4612-0745-0},
	file = {Neal_1996_Bayesian Learning for Neural Networks.pdf:/Users/lucor/Zotero/storage/ZIFXWYTI/Neal_1996_Bayesian Learning for Neural Networks.pdf:application/pdf},
}

@article{srivastava_dropout_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	issn = {1533-7928},
	shorttitle = {Dropout},
	url = {http://jmlr.org/papers/v15/srivastava14a.html},
	number = {56},
	urldate = {2021-09-04},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958},
	file = {Srivastava et al_2014_Dropout.pdf:/Users/lucor/Zotero/storage/BRJBMLUH/Srivastava et al_2014_Dropout.pdf:application/pdf},
}

@article{fort_deep_2020,
	title = {Deep {Ensembles}: {A} {Loss} {Landscape} {Perspective}},
	shorttitle = {Deep {Ensembles}},
	abstract = {Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis.},
	journal = {arXiv:1912.02757},
	author = {Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
	month = jun,
	year = {2020},
	note = {arXiv: 1912.02757},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Fort et al_2020_Deep Ensembles.pdf:/Users/lucor/Zotero/storage/HA78CX7T/Fort et al_2020_Deep Ensembles.pdf:application/pdf},
}

@misc{noauthor_abstract_nodate,
	title = {Abstract: {Keeping} {Neural} {Networks} {Simple} by {Minimizing} the {Description} {Length} of the {Weights}},
	url = {https://www.cs.toronto.edu/~hinton/absps/colt93.html},
	urldate = {2021-08-27},
}

@inproceedings{stahl_evaluation_2020,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Evaluation of {Uncertainty} {Quantification} in {Deep} {Learning}},
	isbn = {978-3-030-50146-4},
	doi = {10.1007/978-3-030-50146-4_41},
	abstract = {Artificial intelligence (AI) is nowadays included into an increasing number of critical systems. Inclusion of AI in such systems may, however, pose a risk, since it is, still, infeasible to build AI systems that know how to function well in situations that differ greatly from what the AI has seen before. Therefore, it is crucial that future AI systems have the ability to not only function well in known domains, but also understand and show when they are uncertain when facing something unknown. In this paper, we evaluate four different methods that have been proposed to correctly quantifying uncertainty when the AI model is faced with new samples. We investigate the behaviour of these models when they are applied to samples far from what these models have seen before, and if they correctly attribute those samples with high uncertainty. We also examine if incorrectly classified samples are attributed with an higher uncertainty than correctly classified samples. The major finding from this simple experiment is, surprisingly, that the evaluated methods capture the uncertainty differently and the correlation between the quantified uncertainty of the models is low. This inconsistency is something that needs to be further understood and solved before AI can be used in critical applications in a trustworthy and safe manner.},
	language = {en},
	booktitle = {Information {Processing} and {Management} of {Uncertainty} in {Knowledge}-{Based} {Systems}},
	publisher = {Springer International Publishing},
	author = {Ståhl, Niclas and Falkman, Göran and Karlsson, Alexander and Mathiason, Gunnar},
	editor = {Lesot, Marie-Jeanne and Vieira, Susana and Reformat, Marek Z. and Carvalho, João Paulo and Wilbik, Anna and Bouchon-Meunier, Bernadette and Yager, Ronald R.},
	year = {2020},
	pages = {556--568},
	file = {Ståhl et al_2020_Evaluation of Uncertainty Quantification in Deep Learning.pdf:/Users/lucor/Zotero/storage/AQJMLHTV/Ståhl et al_2020_Evaluation of Uncertainty Quantification in Deep Learning.pdf:application/pdf},
}

@article{ghosh_ensemble_2021,
	title = {Ensemble learning-iterative training machine learning for uncertainty quantification and automated experiment in atom-resolved microscopy},
	volume = {7},
	copyright = {2021 This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply},
	issn = {2057-3960},
	url = {https://www.nature.com/articles/s41524-021-00569-7},
	doi = {10.1038/s41524-021-00569-7},
	abstract = {Deep learning has emerged as a technique of choice for rapid feature extraction across imaging disciplines, allowing rapid conversion of the data streams to spatial or spatiotemporal arrays of features of interest. However, applications of deep learning in experimental domains are often limited by the out-of-distribution drift between the experiments, where the network trained for one set of imaging conditions becomes sub-optimal for different ones. This limitation is particularly stringent in the quest to have an automated experiment setting, where retraining or transfer learning becomes impractical due to the need for human intervention and associated latencies. Here we explore the reproducibility of deep learning for feature extraction in atom-resolved electron microscopy and introduce workflows based on ensemble learning and iterative training to greatly improve feature detection. This approach allows incorporating uncertainty quantification into the deep learning analysis and also enables rapid automated experimental workflows where retraining of the network to compensate for out-of-distribution drift due to subtle change in imaging conditions is substituted for human operator or programmatic selection of networks from the ensemble. This methodology can be further applied to machine learning workflows in other imaging areas including optical and chemical imaging.},
	language = {en},
	number = {1},
	urldate = {2021-08-17},
	journal = {npj Comput Mater},
	author = {Ghosh, Ayana and Sumpter, Bobby G. and Dyck, Ondrej and Kalinin, Sergei V. and Ziatdinov, Maxim},
	month = jul,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Characterization and analytical techniques;Computational methods
Subject\_term\_id: characterization-and-analytical-techniques;computational-methods},
	pages = {1--8},
	file = {Ghosh et al_2021_Ensemble learning-iterative training machine learning for uncertainty.pdf:/Users/lucor/Zotero/storage/D9R5HSPQ/Ghosh et al_2021_Ensemble learning-iterative training machine learning for uncertainty.pdf:application/pdf},
}

@article{dsouza_tale_2021,
	title = {A {Tale} {Of} {Two} {Long} {Tails}},
	url = {http://arxiv.org/abs/2107.13098},
	abstract = {As machine learning models are increasingly employed to assist human decision-makers, it becomes critical to communicate the uncertainty associated with these model predictions. However, the majority of work on uncertainty has focused on traditional probabilistic or ranking approaches - where the model assigns low probabilities or scores to uncertain examples. While this captures what examples are challenging for the model, it does not capture the underlying source of the uncertainty. In this work, we seek to identify examples the model is uncertain about and characterize the source of said uncertainty. We explore the benefits of designing a targeted intervention - targeted data augmentation of the examples where the model is uncertain over the course of training. We investigate whether the rate of learning in the presence of additional information differs between atypical and noisy examples? Our results show that this is indeed the case, suggesting that well-designed interventions over the course of training can be an effective way to characterize and distinguish between different sources of uncertainty.},
	urldate = {2021-07-29},
	journal = {arXiv:2107.13098 [cs]},
	author = {D'souza, Daniel and Nussbaum, Zach and Agarwal, Chirag and Hooker, Sara},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.13098},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Preliminary results accepted to Workshop on Uncertainty and Robustness in Deep Learning (UDL), ICML, 2021},
	file = {D'souza et al_2021_A Tale Of Two Long Tails.pdf:/Users/lucor/Zotero/storage/BF9QG88U/D'souza et al_2021_A Tale Of Two Long Tails.pdf:application/pdf},
}

@article{zhang_incorporating_2021,
	title = {Incorporating {Label} {Uncertainty} in {Understanding} {Adversarial} {Robustness}},
	url = {http://arxiv.org/abs/2107.03250},
	abstract = {A fundamental question in adversarial machine learning is whether a robust classifier exists for a given task. A line of research has made progress towards this goal by studying concentration of measure, but without considering data labels. We argue that the standard concentration fails to fully characterize the intrinsic robustness of a classification problem, since it ignores data labels which are essential to any classification task. Building on a novel definition of label uncertainty, we empirically demonstrate that error regions induced by state-of-the-art models tend to have much higher label uncertainty compared with randomly-selected subsets. This observation motivates us to adapt a concentration estimation algorithm to account for label uncertainty, resulting in more accurate intrinsic robustness measures for benchmark image classification problems. We further provide empirical evidence showing that adding an abstain option for classifiers based on label uncertainty can help improve both the clean and robust accuracies of models.},
	urldate = {2021-07-29},
	journal = {arXiv:2107.03250 [cs]},
	author = {Zhang, Xiao and Evans, David},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.03250},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: 20 pages, 6 figures, 1 table},
	file = {arXiv.org Snapshot:/Users/lucor/Zotero/storage/A53HKM2N/2107.html:text/html;Zhang_Evans_2021_Incorporating Label Uncertainty in Understanding Adversarial Robustness.pdf:/Users/lucor/Zotero/storage/5ISDM5CU/Zhang_Evans_2021_Incorporating Label Uncertainty in Understanding Adversarial Robustness.pdf:application/pdf},
}

@article{feinberg_chaospy_2015,
	title = {Chaospy: {An} open source tool for designing methods of uncertainty quantification},
	volume = {11},
	issn = {1877-7503},
	shorttitle = {Chaospy},
	url = {https://www.sciencedirect.com/science/article/pii/S1877750315300119},
	doi = {10.1016/j.jocs.2015.08.008},
	abstract = {The paper describes the philosophy, design, functionality, and usage of the Python software toolbox Chaospy for performing uncertainty quantification via polynomial chaos expansions and Monte Carlo simulation. The paper compares Chaospy to similar packages and demonstrates a stronger focus on defining reusable software building blocks that can easily be assembled to construct new, tailored algorithms for uncertainty quantification. For example, a Chaospy user can in a few lines of high-level computer code define custom distributions, polynomials, integration rules, sampling schemes, and statistical metrics for uncertainty analysis. In addition, the software introduces some novel methodological advances, like a framework for computing Rosenblatt transformations and a new approach for creating polynomial chaos expansions with dependent stochastic variables.},
	language = {en},
	urldate = {2021-07-28},
	journal = {Journal of Computational Science},
	author = {Feinberg, Jonathan and Langtangen, Hans Petter},
	month = nov,
	year = {2015},
	keywords = {Monte Carlo simulation, Polynomial chaos expansions, Python package, Rosenblatt transformations, Uncertainty quantification},
	pages = {46--57},
	file = {Feinberg_Langtangen_2015_Chaospy.pdf:/Users/lucor/Zotero/storage/JXRW8IXW/Feinberg_Langtangen_2015_Chaospy.pdf:application/pdf},
}

@article{saha_uncertainty_2021,
	title = {Uncertainty {Quantification} of {Differential} {Algebraic} {Equations} {Using} {Polynomial} {Chaos}},
	issn = {1555-1415},
	url = {https://doi.org/10.1115/1.4051821},
	doi = {10.1115/1.4051821},
	abstract = {The focus of this paper is on the use of Polynomial Chaos for developing surrogate models for Differential Algebraic Equations with time-invariant uncertainties. Intrusive and non-intrusive approaches to synthesize Polynomial Chaos surrogate models are presented including the use of Lagrange interpolation polynomials as basis functions. Unlike ordinary differential equations, if the algebraic constraints are a function of the stochastic variable, some initial conditions of the differential algebraic equations are also random.  A benchmark RLC circuit which is used as a benchmark for linear models is used to illustrate the development of a Polynomial Chaos based surrogate model. A nonlinear example of a simple pendulum also serves as a benchmark to illustrate the potential of the proposed approach. Statistics of the results of the Polynomial Chaos models are validated using Monte Carlo simulations in addition to estimating the evolving PDFs of the states of the pendulum.},
	urldate = {2021-07-22},
	journal = {Journal of Computational and Nonlinear Dynamics},
	author = {Saha, Premjit and Singh, Tarunraj and Dargush, Gary F.},
	month = jul,
	year = {2021},
	file = {Saha et al_2021_Uncertainty Quantification of Differential Algebraic Equations Using Polynomial.pdf:/Users/lucor/Zotero/storage/ZXH4RD7F/Saha et al_2021_Uncertainty Quantification of Differential Algebraic Equations Using Polynomial.pdf:application/pdf},
}

@inproceedings{izmailov_what_2021,
	title = {What {Are} {Bayesian} {Neural} {Network} {Posteriors} {Really} {Like}?},
	url = {http://proceedings.mlr.press/v139/izmailov21a.html},
	language = {en},
	urldate = {2021-07-20},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Izmailov, Pavel and Vikram, Sharad and Hoffman, Matthew D. and Wilson, Andrew Gordon Gordon},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {4629--4640},
	file = {Izmailov et al_2021_What Are Bayesian Neural Network Posteriors Really Like.pdf:/Users/lucor/Zotero/storage/7PQM2KD3/Izmailov et al_2021_What Are Bayesian Neural Network Posteriors Really Like.pdf:application/pdf;Supplementary PDF:/Users/lucor/Zotero/storage/PYA6FVD2/Izmailov et al. - 2021 - What Are Bayesian Neural Network Posteriors Really.pdf:application/pdf},
}

@article{bhatt_uncertainty_2021,
	title = {Uncertainty as a {Form} of {Transparency}: {Measuring}, {Communicating}, and {Using} {Uncertainty}},
	shorttitle = {Uncertainty as a {Form} of {Transparency}},
	url = {http://arxiv.org/abs/2011.07586},
	abstract = {Algorithmic transparency entails exposing system properties to various stakeholders for purposes that include understanding, improving, and contesting predictions. Until now, most research into algorithmic transparency has predominantly focused on explainability. Explainability attempts to provide reasons for a machine learning model’s behavior to stakeholders. However, understanding a model’s specific behavior alone might not be enough for stakeholders to gauge whether the model is wrong or lacks sufficient knowledge to solve the task at hand. In this paper, we argue for considering a complementary form of transparency by estimating and communicating the uncertainty associated with model predictions. First, we discuss methods for assessing uncertainty. Then, we characterize how uncertainty can be used to mitigate model unfairness, augment decision-making, and build trustworthy systems. Finally, we outline methods for displaying uncertainty to stakeholders and recommend how to collect information required for incorporating uncertainty into existing ML pipelines. This work constitutes an interdisciplinary review drawn from literature spanning machine learning, visualization/HCI, design, decision-making, and fairness. We aim to encourage researchers and practitioners to measure, communicate, and use uncertainty as a form of transparency.},
	language = {en},
	urldate = {2021-07-15},
	journal = {arXiv:2011.07586 [cs]},
	author = {Bhatt, Umang and Antorán, Javier and Zhang, Yunfeng and Liao, Q. Vera and Sattigeri, Prasanna and Fogliato, Riccardo and Melançon, Gabrielle Gauthier and Krishnan, Ranganath and Stanley, Jason and Tickoo, Omesh and Nachman, Lama and Chunara, Rumi and Srikumar, Madhulika and Weller, Adrian and Xiang, Alice},
	month = may,
	year = {2021},
	note = {arXiv: 2011.07586},
	keywords = {Computer Science - Machine Learning, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
	annote = {Comment: AAAI/ACM Conference on Artificial Intelligence, Ethics, and Society (AIES) 2021},
	file = {Bhatt et al. - 2021 - Uncertainty as a Form of Transparency Measuring, .pdf:/Users/lucor/Zotero/storage/73DYR4YA/Bhatt et al. - 2021 - Uncertainty as a Form of Transparency Measuring, .pdf:application/pdf},
}

@article{dixler_uncertainty_nodate,
	title = {Uncertainty {Quantification} by {Optimal} {Spline} {Dimensional} {Decomposition}},
	volume = {n/a},
	issn = {1097-0207},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nme.6778},
	doi = {10.1002/nme.6778},
	abstract = {An optimal version of spline dimensional decomposition (SDD) is unveiled for general high-dimensional uncertainty quantification analysis (UQ) of complex systems subject to independent but otherwise arbitrary probability measures of input random variables. The resulting method involves optimally derived knot vectors of basis splines (B-splines) in some or all coordinate directions, whitening transformation producing measure-consistent ortho normalized B-splines equipped with optimal knots, and Fourier-spline expansion of a general high-dimensional output function of interest. In contrast to standard SDD, there is no need to select the knot vectors uniformly or intuitively. The generation of optimal knot vectors can be viewed as an inexpensive pre-processing step toward creating the optimal SDD. Analytical formulae are proposed to calculate the second-moment properties by the optimal SDD method for a general output random variable in terms of the expansion coefficients involved. The computational complexity of the optimal SDD method is polynomial, as opposed to exponential, thus mitigating the curse of dimensionality to a discernible magnitude. Numerical results affirm that the optimal SDD method developed is more precise than polynomial chaos expansion, sparse-grid quadrature, and the standard SDD method in calculating not only the second-moment statistics, but also the cumulative distribution function (CDF) of an output random variable. More importantly, the optimal SDD outperforms standard SDD by sustaining nearly identical computational cost.},
	language = {en},
	number = {n/a},
	urldate = {2021-07-15},
	journal = {International Journal for Numerical Methods in Engineering},
	author = {Dixler, Steven and Jahanbin, Ramin and Rahman, Sharif},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nme.6778},
	keywords = {B-splines, Polynomial Chaos Expansion, Sparse Grids, Spline Chaos Expansion, Spline Dimensional Decomposition, Stochastic analysis},
	file = {Snapshot:/Users/lucor/Zotero/storage/HUFMRVWX/nme.html:text/html},
}

@article{tsirtsis_counterfactual_2021,
	title = {Counterfactual {Explanations} in {Sequential} {Decision} {Making} {Under} {Uncertainty}},
	url = {http://arxiv.org/abs/2107.02776},
	abstract = {Methods to find counterfactual explanations have predominantly focused on one step decision making processes. In this work, we initiate the development of methods to find counterfactual explanations for decision making processes in which multiple, dependent actions are taken sequentially over time. We start by formally characterizing a sequence of actions and states using finite horizon Markov decision processes and the Gumbel-Max structural causal model. Building upon this characterization, we formally state the problem of finding counterfactual explanations for sequential decision making processes. In our problem formulation, the counterfactual explanation specifies an alternative sequence of actions differing in at most k actions from the observed sequence that could have led the observed process realization to a better outcome. Then, we introduce a polynomial time algorithm based on dynamic programming to build a counterfactual policy that is guaranteed to always provide the optimal counterfactual explanation on every possible realization of the counterfactual environment dynamics. We validate our algorithm using both synthetic and real data from cognitive behavioral therapy and show that the counterfactual explanations our algorithm finds can provide valuable insights to enhance sequential decision making under uncertainty.},
	urldate = {2021-07-15},
	journal = {arXiv:2107.02776 [cs, stat]},
	author = {Tsirtsis, Stratis and De, Abir and Gomez-Rodriguez, Manuel},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.02776},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computers and Society},
	annote = {Comment: To appear at the ICML 2021 workshop on Interpretable Machine Learning in Healthcare},
	file = {Tsirtsis et al_2021_Counterfactual Explanations in Sequential Decision Making Under Uncertainty.pdf:/Users/lucor/Zotero/storage/6T8Y5SZ9/Tsirtsis et al_2021_Counterfactual Explanations in Sequential Decision Making Under Uncertainty.pdf:application/pdf},
}

@article{rojano_uncertainty_2021,
	title = {Uncertainty quantification of a thrombosis model considering the clotting assay {PFA}-100},
	url = {http://arxiv.org/abs/2107.00578},
	abstract = {Mathematical models of thrombosis are currently used to study clinical scenarios of pathological thrombus formation. Most of these models involve inherent uncertainties that must be assessed to increase the confidence in model predictions and identify avenues of improvement for both thrombosis modeling and anti-platelet therapies. In this work, an uncertainty quantification analysis of a multi-constituent thrombosis model is performed considering a common assay for platelet function (PFA-100). The analysis is performed using a polynomial chaos expansion as a parametric surrogate for the thrombosis model. The polynomial approximation is validated and used to perform a global sensitivity analysis via computation of Sobol' coefficients. Six out of fifteen parameters were found to be influential in the simulation variability considering only individual effects. Nonetheless, parameter interactions are highlighted when considering the total Sobol' indices. In addition to the sensitivity analysis, the surrogate model was used to compute the PFA-100 closure times of 300,000 virtual cases that align well with clinical data. The current methodology could be used including common anti-platelet therapies to identify scenarios that preserve the hematological balance.},
	urldate = {2021-07-06},
	journal = {arXiv:2107.00578 [q-bio]},
	author = {Rojano, Rodrigo Méndez and Zhussupbekov, Mansur and Antaki, James F. and Lucor, Didier},
	month = jun,
	year = {2021},
	note = {arXiv: 2107.00578},
	keywords = {Quantitative Biology - Quantitative Methods},
	annote = {Comment: 17 pages, 10 figures, 3 tables, original research article},
	file = {Rojano et al_2021_Uncertainty quantification of a thrombosis model considering the clotting assay.pdf:/Users/lucor/Zotero/storage/YIDFSGKE/Rojano et al_2021_Uncertainty quantification of a thrombosis model considering the clotting assay.pdf:application/pdf},
}

@article{wu_quantifying_2021,
	title = {Quantifying {Uncertainty} in {Deep} {Spatiotemporal} {Forecasting}},
	url = {http://arxiv.org/abs/2105.11982},
	abstract = {Deep learning is gaining increasing popularity for spatiotemporal forecasting. However, prior works have mostly focused on point estimates without quantifying the uncertainty of the predictions. In high stakes domains, being able to generate probabilistic forecasts with confidence intervals is critical to risk assessment and decision making. Hence, a systematic study of uncertainty quantification (UQ) methods for spatiotemporal forecasting is missing in the community. In this paper, we describe two types of spatiotemporal forecasting problems: regular grid-based and graph-based. Then we analyze UQ methods from both the Bayesian and the frequentist point of view, casting in a unified framework via statistical decision theory. Through extensive experiments on real-world road network traffic, epidemics, and air quality forecasting tasks, we reveal the statistical and computational trade-offs for different UQ methods: Bayesian methods are typically more robust in mean prediction, while confidence levels obtained from frequentist methods provide more extensive coverage over data variations. Computationally, quantile regression type methods are cheaper for a single confidence interval but require re-training for different intervals. Sampling based methods generate samples that can form multiple confidence intervals, albeit at a higher computational cost.},
	urldate = {2021-07-06},
	journal = {arXiv:2105.11982 [cs, stat]},
	author = {Wu, Dongxia and Gao, Liyao and Xiong, Xinyue and Chinazzi, Matteo and Vespignani, Alessandro and Ma, Yi-An and Yu, Rose},
	month = jun,
	year = {2021},
	note = {arXiv: 2105.11982},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Statistics - Applications},
	annote = {Comment: arXiv admin note: text overlap with arXiv:2102.06684},
	file = {Wu et al_2021_Quantifying Uncertainty in Deep Spatiotemporal Forecasting.pdf:/Users/lucor/Zotero/storage/PYNWXR8I/Wu et al_2021_Quantifying Uncertainty in Deep Spatiotemporal Forecasting.pdf:application/pdf},
}

@article{daw_pid-gan_2021,
	title = {{PID}-{GAN}: {A} {GAN} {Framework} based on a {Physics}-informed {Discriminator} for {Uncertainty} {Quantification} with {Physics}},
	shorttitle = {{PID}-{GAN}},
	url = {http://arxiv.org/abs/2106.02993},
	doi = {10.1145/3447548.3467449},
	abstract = {As applications of deep learning (DL) continue to seep into critical scientific use-cases, the importance of performing uncertainty quantification (UQ) with DL has become more pressing than ever before. In scientific applications, it is also important to inform the learning of DL models with knowledge of physics of the problem to produce physically consistent and generalized solutions. This is referred to as the emerging field of physics-informed deep learning (PIDL). We consider the problem of developing PIDL formulations that can also perform UQ. To this end, we propose a novel physics-informed GAN architecture, termed PID-GAN, where the knowledge of physics is used to inform the learning of both the generator and discriminator models, making ample use of unlabeled data instances. We show that our proposed PID-GAN framework does not suffer from imbalance of generator gradients from multiple loss terms as compared to state-of-the-art. We also empirically demonstrate the efficacy of our proposed framework on a variety of case studies involving benchmark physics-based PDEs as well as imperfect physics. All the code and datasets used in this study have been made available on this link : https://github.com/arkadaw9/PID-GAN.},
	urldate = {2021-07-06},
	journal = {arXiv:2106.02993 [cs, stat]},
	author = {Daw, Arka and Maruf, M. and Karpatne, Anuj},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.02993},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 11 pages, 11 figures, 2 tables, Published at KDD 2021},
	file = {Daw et al_2021_PID-GAN.pdf:/Users/lucor/Zotero/storage/XENTYUX7/Daw et al_2021_PID-GAN.pdf:application/pdf},
}

@article{schulam_can_nodate,
	title = {Can {You} {Trust} {This} {Prediction}? {Auditing} {Pointwise} {Reliability} {After} {Learning}},
	abstract = {To use machine learning in high stakes applications (e.g. medicine), we need tools for building conﬁdence in the system and evaluating whether it is reliable. Methods to improve model reliability often require new learning algorithms (e.g. using Bayesian inference to obtain uncertainty estimates). An alternative is to audit a model after it is trained. In this paper, we describe resampling uncertainty estimation (RUE), an algorithm to audit the pointwise reliability of predictions. Intuitively, RUE estimates the amount that a prediction would change if the model had been ﬁt on diﬀerent training data. The algorithm uses the gradient and Hessian of the model’s loss function to create an ensemble of predictions. Experimentally, we show that RUE more effectively detects inaccurate predictions than existing tools for auditing reliability subsequent to training. We also show that RUE can create predictive distributions that are competitive with state-of-the-art methods like Monte Carlo dropout, probabilistic backpropagation, and deep ensembles, but does not depend on speciﬁc algorithms at train-time like these methods do.},
	language = {en},
	author = {Schulam, Peter and Saria, Suchi},
	pages = {10},
	file = {Schulam_Saria_Can You Trust This Prediction.pdf:/Users/lucor/Zotero/storage/QDPEYDX3/Schulam_Saria_Can You Trust This Prediction.pdf:application/pdf},
}

@article{lines_disentangling_nodate,
	title = {Disentangling {Sources} of {Uncertainty} for {Active} {Exploration}},
	abstract = {only allows PILCO to make decisions based on the total uncertainty on the cost. I use variance as a metric for uncertainty and employ the law of total variance to decompose the total cost uncertainty into its aleatoric and epistemic components. I introduce a gold-standard Monte-Carlo scheme to separately estimate each quantity by propagating trajectories through an approximation to PILCO’s dynamics model. Finally, I show that when PILCO is learning efficiently it is selecting policies associated with a high ratio of epistemic cost uncertainty to total cost uncertainty.},
	language = {en},
	author = {Lines, David},
	pages = {72},
	file = {Lines_Disentangling Sources of Uncertainty for Active Exploration.pdf:/Users/lucor/Zotero/storage/JCSDG4FJ/Lines_Disentangling Sources of Uncertainty for Active Exploration.pdf:application/pdf},
}

@unpublished{depeweg_modeling_2019,
	type = {{PhD} {Thesis}},
	title = {Modeling {Epistemic} and {Aleatoric} {Uncertainty}  with {Bayesian} {Neural} {Networks} and {Latent} {Variables}},
	abstract = {AbstractSupervised learning methods, especially neural networks, are part of many data-drivensolutions  to  real-world  problems.   While  these  methods  can  provide  high  predictionquality, simply making predictions is often not enough.  For decision making, it is alsonecessary  to  estimate  the  confidence,  or  uncertainty,  in  the  prediction  of  the  model.Here,  we can distinguish between epistemic and aleatoric predictive uncertainty.  Theepistemic part originates from lack of knowledge about the true data-generating functionand will reduce the more data we collect.  The aleatoric part is irreducible; it is the resultof intrinsic randomness, or partial observability, of the data-generating process.  ModernBayesian methods, such as Bayesian neural networks, are scalable black-box tools thatcombine the universality of neural networks with the principled probabilistic reasoning ofBayesian inference.  These methods, however, only model the epistemic part of predictiveuncertainty.In this thesis, we develop a novel probabilistic model, called Bayesian neural networkswith latent variables (BNN+LV). This model class can describe complex stochastic pat-terns in the data via a distribution over latent input variables (aleatoric uncertainty),while, at the same time, account for model uncertainty via a distribution over weights(epistemic uncertainty).  We show how these two forms of uncertainty can be extractedfrom the predictive distribution of the model and develop novel model inspection tech-niques to explain the observed uncertainty based on the input features.Empirically,  we  show  that  BNN+LV  provide  high  uncertainty  quality  over  a  widerange  of  regression  tasks.   In  a  series  of  model  inspection  studies,  we  show  that  thedecomposition  of  uncertainty  provides  meaningful  results  and  models  the  data  gener-ating process accurately.  For decision making, we study active learning scenarios andshow that utilizing a decomposition of uncertainty in BNN+LV leads to a more efficientdata  acquisition  strategy.   In  model-based  reinforcement  learning  (RL)  we  study  howBNN+LV can be used as approximate models for stochastic dynamic systems.  Here, weshow that BNN+LV serve as powerful models,  that give rise to policies that producelower  costs  than  baseline  methods.   Again,  utilizing  the  decomposition  of  uncertaintywe develop a novel risk-sensitive criterion for risk-sensitive RL, that enables a practi-tioner to specify its preference for policies that minimize costs, avoid stochasticity and minimize the risk for model-bias.},
	language = {en},
	author = {Depeweg, Stefan},
	year = {2019},
	file = {Depeweg_Modeling Epistemic and Aleatoric Uncertainty with Bayesian Neural Networks and.pdf:/Users/lucor/Zotero/storage/TXC4PA4H/Depeweg_Modeling Epistemic and Aleatoric Uncertainty with Bayesian Neural Networks and.pdf:application/pdf},
}

@article{dorjsembe_sparsity_2021,
	title = {Sparsity {Increases} {Uncertainty} {Estimation} in {Deep} {Ensemble}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2073-431X/10/4/54},
	doi = {10.3390/computers10040054},
	abstract = {Deep neural networks have achieved almost human-level results in various tasks and have become popular in the broad artificial intelligence domains. Uncertainty estimation is an on-demand task caused by the black-box point estimation behavior of deep learning. The deep ensemble provides increased accuracy and estimated uncertainty; however, linearly increasing the size makes the deep ensemble unfeasible for memory-intensive tasks. To address this problem, we used model pruning and quantization with a deep ensemble and analyzed the effect in the context of uncertainty metrics. We empirically showed that the ensemble members’ disagreement increases with pruning, making models sparser by zeroing irrelevant parameters. Increased disagreement im-plies increased uncertainty, which helps in making more robust predictions. Accordingly, an energy-efficient compressed deep ensemble is appropriate for memory-intensive and uncertainty-aware tasks.},
	language = {en},
	number = {4},
	urldate = {2021-06-29},
	journal = {Computers},
	author = {Dorjsembe, Uyanga and Lee, Ju Hong and Choi, Bumghi and Song, Jae Won},
	month = apr,
	year = {2021},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep ensemble, deep learning, model compression, uncertainty estimation},
	pages = {54},
	file = {Dorjsembe et al_2021_Sparsity Increases Uncertainty Estimation in Deep Ensemble.pdf:/Users/lucor/Zotero/storage/WVH5YPZ9/Dorjsembe et al_2021_Sparsity Increases Uncertainty Estimation in Deep Ensemble.pdf:application/pdf},
}

@article{kronheim_tensorbnn_2020,
	title = {{TensorBNN}: {Bayesian} {Inference} for {Neural} {Networks} using {Tensorflow}},
	shorttitle = {{TensorBNN}},
	url = {http://arxiv.org/abs/2009.14393},
	abstract = {TensorBNN is a new package based on TensorFlow that implements Bayesian inference for modern neural network models. The posterior density of neural network model parameters is represented as a point cloud sampled using Hamiltonian Monte Carlo. The TensorBNN package leverages TensorFlow's architecture and training features as well as its ability to use modern graphics processing units (GPU) in both the training and prediction stages.},
	urldate = {2021-06-28},
	journal = {arXiv:2009.14393 [physics]},
	author = {Kronheim, Braden and Kuchera, Michelle and Prosper, Harrison},
	month = oct,
	year = {2020},
	note = {arXiv: 2009.14393},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
	file = {Kronheim et al_2020_TensorBNN.pdf:/Users/lucor/Zotero/storage/JRHVPXWR/Kronheim et al_2020_TensorBNN.pdf:application/pdf},
}


@article{jose_information-theoretic_2021,
	title = {Information-{Theoretic} {Analysis} of {Epistemic} {Uncertainty} in {Bayesian} {Meta}-learning},
	url = {http://arxiv.org/abs/2106.00252},
	abstract = {The overall predictive uncertainty of a trained predictor can be decomposed into separate contributions due to epistemic and aleatoric uncertainty. Under a Bayesian formulation, assuming a well-specified model, the two contributions can be exactly expressed (for the log-loss) or bounded (for more general losses) in terms of information-theoretic quantities (Xu and Raginsky, 2020). This paper addresses the study of epistemic uncertainty within an information-theoretic framework in the broader setting of Bayesian meta-learning. A general hierarchical Bayesian model is assumed in which hyperparameters determine the per-task priors of the model parameters. Exact characterizations (for the log-loss) and bounds (for more general losses) are derived for the epistemic uncertainty - quantified by the minimum excess meta-risk (MEMR)- of optimal meta-learning rules. This characterization is leveraged to bring insights into the dependence of the epistemic uncertainty on the number of tasks and on the amount of per-task training data. Experiments are presented that compare the proposed information-theoretic bounds, evaluated via neural mutual information estimators, with the performance of a novel approximate fully Bayesian meta-learning strategy termed Langevin-Stein Bayesian Meta-Learning (LS-BML).},
	urldate = {2021-06-22},
	journal = {arXiv:2106.00252 [cs, eess, math]},
	author = {Jose, Sharu Theresa and Park, Sangwoo and Simeone, Osvaldo},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.00252},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Theory, Electrical Engineering and Systems Science - Signal Processing},
	annote = {Comment: Under review, 21 pages},
	file = {Jose et al_2021_Information-Theoretic Analysis of Epistemic Uncertainty in Bayesian.pdf:/Users/lucor/Zotero/storage/2T8MTH9H/Jose et al_2021_Information-Theoretic Analysis of Epistemic Uncertainty in Bayesian.pdf:application/pdf},
}

@article{zimmermann_nested_2021,
	title = {Nested {Variational} {Inference}},
	url = {http://arxiv.org/abs/2106.11302},
	abstract = {We develop nested variational inference (NVI), a family of methods that learn proposals for nested importance samplers by minimizing an forward or reverse KL divergence at each level of nesting. NVI is applicable to many commonly-used importance sampling strategies and provides a mechanism for learning intermediate densities, which can serve as heuristics to guide the sampler. Our experiments apply NVI to (a) sample from a multimodal distribution using a learned annealing path (b) learn heuristics that approximate the likelihood of future observations in a hidden Markov model and (c) to perform amortized inference in hierarchical deep generative models. We observe that optimizing nested objectives leads to improved sample quality in terms of log average weight and effective sample size.},
	urldate = {2021-06-22},
	journal = {arXiv:2106.11302 [cs, stat]},
	author = {Zimmermann, Heiko and Wu, Hao and Esmaeili, Babak and van de Meent, Jan-Willem},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.11302},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Zimmermann et al_2021_Nested Variational Inference.pdf:/Users/lucor/Zotero/storage/VX9P6MJR/Zimmermann et al_2021_Nested Variational Inference.pdf:application/pdf},
}

@article{cicirello_machine_2021,
	title = {Machine {Learning} based optimization for interval uncertainty propagation with application to vibro-acoustic models},
	url = {http://arxiv.org/abs/2106.11215},
	abstract = {Two non-intrusive uncertainty propagation approaches are proposed for the performance analysis of engineering systems described by expensive-to-evaluate deterministic computer models with parameters defined as interval variables. These approaches employ a machine learning based optimization strategy, the so-called Bayesian optimization, for evaluating the upper and lower bounds of a generic response variable over the set of possible responses obtained when each interval variable varies independently over its range. The lack of knowledge caused by not evaluating the response function for all the possible combinations of the interval variables is accounted for by developing a probabilistic description of the response variable itself by using a Gaussian Process regression model. An iterative procedure is developed for selecting a small number of simulations to be evaluated for updating this statistical model by using well-established acquisition functions and to assess the response bounds. In both approaches, an initial training dataset is defined. While one approach builds iteratively two distinct training datasets for evaluating separately the upper and lower bounds of the response variable, the other builds iteratively a single training dataset. Consequently, the two approaches will produce different bound estimates at each iteration. The upper and lower bound responses are expressed as point estimates obtained from the mean function of the posterior distribution. Moreover, a confidence interval on each estimate is provided for effectively communicating to engineers when these estimates are obtained for a combination of the interval variables for which no deterministic simulation has been run. Finally, two metrics are proposed to define conditions for assessing if the predicted bound estimates can be considered satisfactory.},
	urldate = {2021-06-22},
	journal = {arXiv:2106.11215 [cs, eess, math]},
	author = {Cicirello, Alice and Giunta, Filippo},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.11215},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Mathematics - Optimization and Control},
	annote = {Comment: Preprint submitted to Mechanical Systems and Signal Processing},
	file = {Cicirello_Giunta_2021_Machine Learning based optimization for interval uncertainty propagation with.pdf:/Users/lucor/Zotero/storage/Z9JBIQ6K/Cicirello_Giunta_2021_Machine Learning based optimization for interval uncertainty propagation with.pdf:application/pdf},
}

@article{mukhoti_deterministic_2021,
	title = {Deterministic {Neural} {Networks} with {Inductive} {Biases} {Capture} {Epistemic} and {Aleatoric} {Uncertainty}},
	url = {http://arxiv.org/abs/2102.11582},
	abstract = {We show that a single softmax neural net with minimal changes can beat the uncertainty predictions of Deep Ensembles and other more complex single-forward-pass uncertainty approaches. Standard softmax neural nets suffer from feature collapse and extrapolate arbitrarily for OoD points. This results in arbitrary softmax entropies for OoD points which can have high entropy, low, or anything in between, thus cannot capture epistemic uncertainty reliably. We prove that this failure lies at the core of "why" Deep Ensemble Uncertainty works well. Instead of using softmax entropy, we show that with appropriate inductive biases softmax neural nets trained with maximum likelihood reliably capture epistemic uncertainty through their feature-space density. This density is obtained using simple Gaussian Discriminant Analysis, but it cannot represent aleatoric uncertainty reliably. We show that it is necessary to combine feature-space density with softmax entropy to disentangle uncertainties well. We evaluate the epistemic uncertainty quality on active learning and OoD detection, achieving SOTA {\textasciitilde}98 AUROC on CIFAR-10 vs SVHN without fine-tuning on OoD data.},
	urldate = {2021-06-16},
	journal = {arXiv:2102.11582 [cs, stat]},
	author = {Mukhoti, Jishnu and Kirsch, Andreas and van Amersfoort, Joost and Torr, Philip H. S. and Gal, Yarin},
	month = jun,
	year = {2021},
	note = {arXiv: 2102.11582},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Mukhoti et al_2021_Deterministic Neural Networks with Inductive Biases Capture Epistemic and.pdf:/Users/lucor/Zotero/storage/WLAN4XVL/Mukhoti et al_2021_Deterministic Neural Networks with Inductive Biases Capture Epistemic and.pdf:application/pdf},
}

@article{liu_simple_nodate,
	title = {Simple and {Principled} {Uncertainty} {Estimation} with {Deterministic} {Deep} {Learning} via {Distance} {Awareness}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/543e83748234f7cbab21aa0ade66565f-Abstract.html},
	abstract = {Bayesian neural networks (BNN) and deep ensembles are principled approaches to estimate the predictive uncertainty of a deep learning model. However their practicality in real-time, industrial-scale applications are limited due to their heavy memory and inference cost. This motivates us to study principled approaches to high-quality uncertainty estimation that require only a single deep neural network (DNN). By formalizing the uncertainty quantification as a minimax learning problem, we first identify input distance awareness, i.e., the model’s ability to quantify the distance of a testing example from the training data in the input space, as a necessary condition for a DNN to achieve high-quality (i.e., minimax optimal) uncertainty estimation. We then propose Spectral-normalized Neural Gaussian Process (SNGP), a simple method that improves the distance-awareness ability of modern DNNs, by adding a weight normalization step during training and replacing the output layer. On a suite of vision nd language understanding tasks and on modern architectures (Wide-ResNet and BERT), SNGP is competitive with deep ensembles in prediction, calibration and out-of-domain detection, and outperforms the other single-model approaches.},
	language = {en},
	author = {Liu, Jeremiah Zhe and Lin, Zi and Padhy, Shreyas and Tran, Dustin and Bedrax-Weiss, Tania and Lakshminarayanan, Balaji},
	pages = {15},
	file = {Liu et al_Simple and Principled Uncertainty Estimation with Deterministic Deep Learning.pdf:/Users/lucor/Zotero/storage/6B86KP93/Liu et al_Simple and Principled Uncertainty Estimation with Deterministic Deep Learning.pdf:application/pdf},
}

@article{yang_b-pinns_2021,
	title = {B-{PINNs}: {Bayesian} {Physics}-{Informed} {Neural} {Networks} for {Forward} and {Inverse} {PDE} {Problems} with {Noisy} {Data}},
	volume = {425},
	issn = {00219991},
	shorttitle = {B-{PINNs}},
	url = {http://arxiv.org/abs/2003.06097},
	doi = {10.1016/j.jcp.2020.109913},
	abstract = {We propose a Bayesian physics-informed neural network (B-PINN) to solve both forward and inverse nonlinear problems described by partial differential equations (PDEs) and noisy data. In this Bayesian framework, the Bayesian neural network (BNN) combined with a PINN for PDEs serves as the prior while the Hamiltonian Monte Carlo (HMC) or the variational inference (VI) could serve as an estimator of the posterior. B-PINNs make use of both physical laws and scattered noisy measurements to provide predictions and quantify the aleatoric uncertainty arising from the noisy data in the Bayesian framework. Compared with PINNs, in addition to uncertainty quantification, B-PINNs obtain more accurate predictions in scenarios with large noise due to their capability of avoiding overfitting. We conduct a systematic comparison between the two different approaches for the B-PINN posterior estimation (i.e., HMC or VI), along with dropout used for quantifying uncertainty in deep neural networks. Our experiments show that HMC is more suitable than VI for the B-PINNs posterior estimation, while dropout employed in PINNs can hardly provide accurate predictions with reasonable uncertainty. Finally, we replace the BNN in the prior with a truncated Karhunen-Lo{\textbackslash}`eve (KL) expansion combined with HMC or a deep normalizing flow (DNF) model as posterior estimators. The KL is as accurate as BNN and much faster but this framework cannot be easily extended to high-dimensional problems unlike the BNN based framework.},
	urldate = {2021-06-10},
	journal = {Journal of Computational Physics},
	author = {Yang, Liu and Meng, Xuhui and Karniadakis, George Em},
	month = jan,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {109913},
	file = {Yang et al_2021_B-PINNs.pdf:/Users/lucor/Zotero/storage/BD894F28/Yang et al_2021_B-PINNs.pdf:application/pdf},
}

@article{yao_quality_2019,
	title = {Quality of {Uncertainty} {Quantification} for {Bayesian} {Neural} {Network} {Inference}},
	url = {http://arxiv.org/abs/1906.09686},
	abstract = {Bayesian Neural Networks (BNNs) place priors over the parameters in a neural network. Inference in BNNs, however, is difficult; all inference methods for BNNs are approximate. In this work, we empirically compare the quality of predictive uncertainty estimates for 10 common inference methods on both regression and classification tasks. Our experiments demonstrate that commonly used metrics (e.g. test log-likelihood) can be misleading. Our experiments also indicate that inference innovations designed to capture structure in the posterior do not necessarily produce high quality posterior approximations.},
	urldate = {2021-06-08},
	journal = {arXiv:1906.09686 [cs, stat]},
	author = {Yao, Jiayu and Pan, Weiwei and Ghosh, Soumya and Doshi-Velez, Finale},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.09686},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Yao et al_2019_Quality of Uncertainty Quantification for Bayesian Neural Network Inference.pdf:/Users/lucor/Zotero/storage/7MP8ZZ6F/Yao et al_2019_Quality of Uncertainty Quantification for Bayesian Neural Network Inference.pdf:application/pdf},
}

@article{dusenberry_efficient_2020,
	title = {Efficient and {Scalable} {Bayesian} {Neural} {Nets} with {Rank}-1 {Factors}},
	url = {http://arxiv.org/abs/2005.07186},
	abstract = {Bayesian neural networks (BNNs) demonstrate promising success in improving the robustness and uncertainty quantification of modern deep learning. However, they generally struggle with underfitting at scale and parameter efficiency. On the other hand, deep ensembles have emerged as alternatives for uncertainty quantification that, while outperforming BNNs on certain problems, also suffer from efficiency issues. It remains unclear how to combine the strengths of these two approaches and remediate their common issues. To tackle this challenge, we propose a rank-1 parameterization of BNNs, where each weight matrix involves only a distribution on a rank-1 subspace. We also revisit the use of mixture approximate posteriors to capture multiple modes, where unlike typical mixtures, this approach admits a significantly smaller memory increase (e.g., only a 0.4\% increase for a ResNet-50 mixture of size 10). We perform a systematic empirical study on the choices of prior, variational posterior, and methods to improve training. For ResNet-50 on ImageNet, Wide ResNet 28-10 on CIFAR-10/100, and an RNN on MIMIC-III, rank-1 BNNs achieve state-of-the-art performance across log-likelihood, accuracy, and calibration on the test sets and out-of-distribution variants.},
	urldate = {2021-06-02},
	journal = {arXiv:2005.07186 [cs, stat]},
	author = {Dusenberry, Michael W. and Jerfel, Ghassen and Wen, Yeming and Ma, Yi-An and Snoek, Jasper and Heller, Katherine and Lakshminarayanan, Balaji and Tran, Dustin},
	month = aug,
	year = {2020},
	note = {arXiv: 2005.07186},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Dusenberry et al_2020_Efficient and Scalable Bayesian Neural Nets with Rank-1 Factors.pdf:/Users/lucor/Zotero/storage/XCSZUH62/Dusenberry et al_2020_Efficient and Scalable Bayesian Neural Nets with Rank-1 Factors.pdf:application/pdf},
}

@article{maddison_concrete_2017,
	title = {The {Concrete} {Distribution}: {A} {Continuous} {Relaxation} of {Discrete} {Random} {Variables}},
	shorttitle = {The {Concrete} {Distribution}},
	url = {http://arxiv.org/abs/1611.00712},
	abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
	urldate = {2021-06-01},
	journal = {arXiv:1611.00712 [cs, stat]},
	author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
	month = mar,
	year = {2017},
	note = {arXiv: 1611.00712},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Maddison et al_2017_The Concrete Distribution.pdf:/Users/lucor/Zotero/storage/LZTPPYHF/Maddison et al_2017_The Concrete Distribution.pdf:application/pdf},
}

@article{hobbhahn_fast_2021,
	title = {Fast {Predictive} {Uncertainty} for {Classification} with {Bayesian} {Deep} {Networks}},
	url = {http://arxiv.org/abs/2003.01227},
	abstract = {In Bayesian Deep Learning, distributions over the output of classification neural networks are approximated by first constructing a Gaussian distribution over the weights, then sampling from it to receive a distribution over the categorical output distribution. This is costly. We reconsider old work to construct a Dirichlet approximation of this output distribution, which yields an analytic map between Gaussian distributions in logit space and Dirichlet distributions (the conjugate prior to the categorical) in the output space. We argue that the resulting Dirichlet distribution has theoretical and practical advantages, in particular more efficient computation of the uncertainty estimate, scaling to large datasets and networks like ImageNet and DenseNet. We demonstrate the use of this Dirichlet approximation by using it to construct a lightweight uncertainty-aware output ranking for the ImageNet setup.},
	urldate = {2021-05-31},
	journal = {arXiv:2003.01227 [cs, stat]},
	author = {Hobbhahn, Marius and Kristiadi, Agustinus and Hennig, Philipp},
	month = feb,
	year = {2021},
	note = {arXiv: 2003.01227},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Hobbhahn et al_2021_Fast Predictive Uncertainty for Classification with Bayesian Deep Networks.pdf:/Users/lucor/Zotero/storage/S8RALNYH/Hobbhahn et al_2021_Fast Predictive Uncertainty for Classification with Bayesian Deep Networks.pdf:application/pdf},
}

@article{kristiadi_predictive_2019,
	title = {Predictive {Uncertainty} {Quantification} with {Compound} {Density} {Networks}},
	url = {http://arxiv.org/abs/1902.01080},
	abstract = {Despite the huge success of deep neural networks (NNs), finding good mechanisms for quantifying their prediction uncertainty is still an open problem. Bayesian neural networks are one of the most popular approaches to uncertainty quantification. On the other hand, it was recently shown that ensembles of NNs, which belong to the class of mixture models, can be used to quantify prediction uncertainty. In this paper, we build upon these two approaches. First, we increase the mixture model's flexibility by replacing the fixed mixing weights by an adaptive, input-dependent distribution (specifying the probability of each component) represented by NNs, and by considering uncountably many mixture components. The resulting class of models can be seen as the continuous counterpart to mixture density networks and is therefore referred to as compound density networks (CDNs). We employ both maximum likelihood and variational Bayesian inference to train CDNs, and empirically show that they yield better uncertainty estimates on out-of-distribution data and are more robust to adversarial examples than the previous approaches.},
	urldate = {2021-05-31},
	journal = {arXiv:1902.01080 [cs, stat]},
	author = {Kristiadi, Agustinus and Däubener, Sina and Fischer, Asja},
	month = dec,
	year = {2019},
	note = {arXiv: 1902.01080},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	annote = {Comment: Bayesian deep learning workshop, NeuRIPS 2019},
	file = {Kristiadi et al_2019_Predictive Uncertainty Quantification with Compound Density Networks.pdf:/Users/lucor/Zotero/storage/7RNQR2NS/Kristiadi et al_2019_Predictive Uncertainty Quantification with Compound Density Networks.pdf:application/pdf},
}

@article{fortuin_priors_2021,
	title = {Priors in {Bayesian} {Deep} {Learning}: {A} {Review}},
	shorttitle = {Priors in {Bayesian} {Deep} {Learning}},
	url = {http://arxiv.org/abs/2105.06868},
	abstract = {While the choice of prior is one of the most critical parts of the Bayesian inference workflow, recent Bayesian deep learning models have often fallen back on uninformative priors, such as standard Gaussians. In this review, we highlight the importance of prior choices for Bayesian deep learning and present an overview of different priors that have been proposed for (deep) Gaussian processes, variational autoencoders, and Bayesian neural networks. We also outline different methods of learning priors for these models from data. We hope to motivate practitioners in Bayesian deep learning to think more carefully about the prior specification for their models and to provide them with some inspiration in this regard.},
	urldate = {2021-05-24},
	journal = {arXiv:2105.06868 [cs, stat]},
	author = {Fortuin, Vincent},
	month = may,
	year = {2021},
	note = {arXiv: 2105.06868},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Fortuin_2021_Priors in Bayesian Deep Learning.pdf:/Users/lucor/Zotero/storage/MFJH6ML4/Fortuin_2021_Priors in Bayesian Deep Learning.pdf:application/pdf},
}

@article{seoh_qualitative_2020,
	title = {Qualitative {Analysis} of {Monte} {Carlo} {Dropout}},
	url = {http://arxiv.org/abs/2007.01720},
	abstract = {In this report, we present qualitative analysis of Monte Carlo (MC) dropout method for measuring model uncertainty in neural network (NN) models. We first consider the sources of uncertainty in NNs, and briefly review Bayesian Neural Networks (BNN), the group of Bayesian approaches to tackle uncertainties in NNs. After presenting mathematical formulation of MC dropout, we proceed to suggesting potential benefits and associated costs for using MC dropout in typical NN models, with the results from our experiments.},
	urldate = {2021-05-21},
	journal = {arXiv:2007.01720 [cs, stat]},
	author = {Seoh, Ronald},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.01720},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Seoh_2020_Qualitative Analysis of Monte Carlo Dropout.pdf:/Users/lucor/Zotero/storage/LNWDFYVC/Seoh_2020_Qualitative Analysis of Monte Carlo Dropout.pdf:application/pdf},
}

@inproceedings{gal_dropout_2016,
	title = {Dropout as a {Bayesian} {Approximation}:  {Representing} {Model} {Uncertainty} in {Deep} {Learning}},
	abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classiﬁcation do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs –extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacriﬁcing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classiﬁcation, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and ﬁnish by using dropout’s uncertainty in deep reinforcement learning.},
	language = {en},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	year = {2016},
	pages = {1050--1059},
	file = {Gal_Ghahramani_Dropout as a Bayesian Approximation.pdf:/Users/lucor/Zotero/storage/QZH5EEBV/Gal_Ghahramani_Dropout as a Bayesian Approximation.pdf:application/pdf},
}

@article{teye_bayesian_nodate,
	title = {Bayesian {Uncertainty} {Estimation} for {Batch} {Normalized} {Deep} {Networks}},
	abstract = {We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models. We further demonstrate that this ﬁnding allows us to make meaningful estimates of the model uncertainty using conventional architectures, without modiﬁcations to the network or the training procedure. Our approach is thoroughly validated by measuring the quality of uncertainty in a series of empirical experiments on different tasks. It outperforms baselines with strong statistical signiﬁcance, and displays competitive performance with recent Bayesian approaches.},
	language = {en},
	author = {Teye, Mattias and Azizpour, Hossein and Smith, Kevin},
	pages = {10},
	file = {Teye et al. - Bayesian Uncertainty Estimation for Batch Normaliz.pdf:/Users/lucor/Zotero/storage/7E7IKB2H/Teye et al. - Bayesian Uncertainty Estimation for Batch Normaliz.pdf:application/pdf},
}

@article{izmailov_subspace_2019,
	title = {Subspace {Inference} for {Bayesian} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1907.07504},
	abstract = {Bayesian inference was once a gold standard for learning with neural networks, providing accurate full predictive distributions and well calibrated uncertainty. However, scaling Bayesian inference techniques to deep neural networks is challenging due to the high dimensionality of the parameter space. In this paper, we construct low-dimensional subspaces of parameter space, such as the first principal components of the stochastic gradient descent (SGD) trajectory, which contain diverse sets of high performing models. In these subspaces, we are able to apply elliptical slice sampling and variational inference, which struggle in the full parameter space. We show that Bayesian model averaging over the induced posterior in these subspaces produces accurate predictions and well calibrated predictive uncertainty for both regression and image classification.},
	urldate = {2021-05-18},
	journal = {arXiv:1907.07504 [cs, stat]},
	author = {Izmailov, Pavel and Maddox, Wesley J. and Kirichenko, Polina and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.07504},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published at UAI 2019},
	file = {Izmailov et al_2019_Subspace Inference for Bayesian Deep Learning.pdf:/Users/lucor/Zotero/storage/8BA7ZBVD/Izmailov et al_2019_Subspace Inference for Bayesian Deep Learning.pdf:application/pdf},
}

@mastersthesis{bachstein_uncertainty_2019,
	address = {Germany},
	title = {Uncertainty {Quantification} in {Deep} {Learning}},
	url = {https://sbachstein.de/master_thesis.pdf},
	abstract = {Evaluation of recently published methods for Uncertainty Quantification in Deep Learning. Improving on existing algorithms and developing new approaches. Comparing deep learning methods to other, non neural network related, concepts like Gaussian Processes},
	language = {English},
	school = {Ulm University},
	author = {Bachstein, Simon},
	month = jan,
	year = {2019},
	file = {Bachstein_2019_Uncertainty Quantification in Deep Learning.pdf:/Users/lucor/Zotero/storage/J4GK7NPR/Bachstein_2019_Uncertainty Quantification in Deep Learning.pdf:application/pdf},
}

@article{amini_deep_2020,
	title = {Deep {Evidential} {Regression}},
	url = {http://arxiv.org/abs/1910.02600},
	abstract = {Deterministic neural networks (NNs) are increasingly being deployed in safety critical domains, where calibrated, robust, and efficient measures of uncertainty are crucial. In this paper, we propose a novel method for training non-Bayesian NNs to estimate a continuous target as well as its associated evidence in order to learn both aleatoric and epistemic uncertainty. We accomplish this by placing evidential priors over the original Gaussian likelihood function and training the NN to infer the hyperparameters of the evidential distribution. We additionally impose priors during training such that the model is regularized when its predicted evidence is not aligned with the correct output. Our method does not rely on sampling during inference or on out-of-distribution (OOD) examples for training, thus enabling efficient and scalable uncertainty learning. We demonstrate learning well-calibrated measures of uncertainty on various benchmarks, scaling to complex computer vision tasks, as well as robustness to adversarial and OOD test samples.},
	urldate = {2021-05-17},
	journal = {arXiv:1910.02600 [cs, stat]},
	author = {Amini, Alexander and Schwarting, Wilko and Soleimany, Ava and Rus, Daniela},
	month = nov,
	year = {2020},
	note = {arXiv: 1910.02600},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {code: https://github.com/aamini/evidential-deep-learning},
	file = {Amini et al. - 2020 - Deep Evidential Regression.pdf:/Users/lucor/Zotero/storage/QLIWTLYY/Amini et al. - 2020 - Deep Evidential Regression.pdf:application/pdf},
}

@inproceedings{pearce_uncertainty_2020,
	title = {Uncertainty in {Neural} {Networks}: {Approximately} {Bayesian} {Ensembling}},
	shorttitle = {Uncertainty in {Neural} {Networks}},
	url = {http://proceedings.mlr.press/v108/pearce20a.html},
	abstract = {Understanding the uncertainty of a neural network’s (NN) predictions is essential for many purposes. The Bayesian framework provides a principled approach to this, however applying it to NNs is cha...},
	language = {en},
	urldate = {2021-05-07},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Pearce, Tim and Leibfried, Felix and Brintrup, Alexandra},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {234--244},
	file = {Pearce et al_2020_Uncertainty in Neural Networks.pdf:/Users/lucor/Zotero/storage/GAG778UJ/Pearce et al_2020_Uncertainty in Neural Networks.pdf:application/pdf},
}

@article{caldeira_deeply_2020,
	title = {Deeply uncertain: comparing methods of uncertainty quantification in deep learning algorithms},
	volume = {2},
	issn = {2632-2153},
	shorttitle = {Deeply uncertain},
	url = {https://doi.org/10.1088/2632-2153/aba6f3},
	doi = {10.1088/2632-2153/aba6f3},
	abstract = {We present a comparison of methods for uncertainty quantification (UQ) in deep learning algorithms in the context of a simple physical system. Three of the most common uncertainty quantification methods—Bayesian neural networks (BNNs), concrete dropout (CD), and deep ensembles (DEs) — are compared to the standard analytic error propagation. We discuss this comparison in terms endemic to both machine learning (‘epistemic’ and ‘aleatoric’) and the physical sciences (‘statistical’ and ‘systematic’). The comparisons are presented in terms of simulated experimental measurements of a single pendulum—a prototypical physical system for studying measurement and analysis techniques. Our results highlight some pitfalls that may occur when using these UQ methods. For example, when the variation of noise in the training set is small, all methods predicted the same relative uncertainty independently of the inputs. This issue is particularly hard to avoid in BNN. On the other hand, when the test set contains samples far from the training distribution, we found that no methods sufficiently increased the uncertainties associated to their predictions. This problem was particularly clear for CD. In light of these results, we make some recommendations for usage and interpretation of UQ methods.},
	language = {en},
	number = {1},
	urldate = {2021-05-07},
	journal = {Mach. Learn.: Sci. Technol.},
	author = {Caldeira, João and Nord, Brian},
	month = dec,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {015002},
	annote = {Le code est accessible à cette adresse: https://github.com/deepskies/DeeplyUncertain-Public },
	file = {Caldeira_Nord_2020_Deeply uncertain.pdf:/Users/lucor/Zotero/storage/KFUL2QZ6/Caldeira_Nord_2020_Deeply uncertain.pdf:application/pdf},
}

@article{verdoja_notes_2020,
	title = {Notes on the {Behavior} of {MC} {Dropout}},
	url = {http://arxiv.org/abs/2008.02627},
	abstract = {Among the various options to estimate uncertainty in deep neural networks, Monte-Carlo dropout is widely popular for its simplicity and effectiveness. However the quality of the uncertainty estimated through this method varies and choices in architecture design and in training procedures have to be carefully considered and tested to obtain satisfactory results. In this paper we present a study offering a different point of view on the behavior of Monte-Carlo dropout, which enables us to observe a few interesting properties of the technique to keep in mind when considering its use for uncertainty estimation.},
	urldate = {2021-05-05},
	journal = {arXiv:2008.02627 [cs, stat]},
	author = {Verdoja, Francesco and Kyrki, Ville},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.02627},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Verdoja_Kyrki_2020_Notes on the Behavior of MC Dropout.pdf:/Users/lucor/Zotero/storage/KT9AGVNY/Verdoja_Kyrki_2020_Notes on the Behavior of MC Dropout.pdf:application/pdf},
}

@article{gal_concrete_2017,
	title = {Concrete {Dropout}},
	url = {http://arxiv.org/abs/1705.07832},
	abstract = {Dropout is used as a practical tool to obtain uncertainty estimates in large vision models and reinforcement learning (RL) tasks. But to obtain well-calibrated uncertainty estimates, a grid-search over the dropout probabilities is necessary - a prohibitive operation with large models, and an impossible one with RL. We propose a new dropout variant which gives improved performance and better calibrated uncertainties. Relying on recent developments in Bayesian deep learning, we use a continuous relaxation of dropout's discrete masks. Together with a principled optimisation objective, this allows for automatic tuning of the dropout probability in large models, and as a result faster experimentation cycles. In RL this allows the agent to adapt its uncertainty dynamically as more data is observed. We analyse the proposed variant extensively on a range of tasks, and give insights into common practice in the field where larger dropout probabilities are often used in deeper model layers.},
	urldate = {2021-05-05},
	journal = {arXiv:1705.07832 [stat]},
	author = {Gal, Yarin and Hron, Jiri and Kendall, Alex},
	month = may,
	year = {2017},
	note = {arXiv: 1705.07832},
	keywords = {Statistics - Machine Learning},
	annote = {code: https://github.com/yaringal/ConcreteDropout},
	file = {Gal et al. - 2017 - Concrete Dropout.pdf:/Users/lucor/Zotero/storage/FPQT9K9Q/Gal et al. - 2017 - Concrete Dropout.pdf:application/pdf},
}

@article{liu_stein_2019,
	title = {Stein {Variational} {Gradient} {Descent}: {A} {General} {Purpose} {Bayesian} {Inference} {Algorithm}},
	shorttitle = {Stein {Variational} {Gradient} {Descent}},
	url = {http://arxiv.org/abs/1608.04471},
	abstract = {We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.},
	urldate = {2021-05-03},
	journal = {arXiv:1608.04471 [cs, stat]},
	author = {Liu, Qiang and Wang, Dilin},
	month = sep,
	year = {2019},
	note = {arXiv: 1608.04471},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Liu_Wang_2019_Stein Variational Gradient Descent.pdf:/Users/lucor/Zotero/storage/5Y3BMVK5/Liu_Wang_2019_Stein Variational Gradient Descent.pdf:application/pdf},
}

@article{hernandez-lobato_probabilistic_2015,
	title = {Probabilistic {Backpropagation} for {Scalable} {Learning} of {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1502.05336},
	abstract = {Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.},
	urldate = {2021-04-29},
	journal = {arXiv:1502.05336 [stat]},
	author = {Hernández-Lobato, José Miguel and Adams, Ryan P.},
	month = jul,
	year = {2015},
	note = {arXiv: 1502.05336},
	keywords = {Statistics - Machine Learning},
	file = {Hernández-Lobato and Adams - 2015 - Probabilistic Backpropagation for Scalable Learnin.pdf:/Users/lucor/Zotero/storage/2UTQZWMF/Hernández-Lobato and Adams - 2015 - Probabilistic Backpropagation for Scalable Learnin.pdf:application/pdf},
}



@inproceedings{blundell_weight_2015,
	title = {Weight {Uncertainty} in {Neural} {Network}},
	url = {http://proceedings.mlr.press/v37/blundell15.html},
	abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularis...},
	language = {en},
	urldate = {2021-04-26},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	month = jun,
	year = {2015},
	pages = {1613--1622},
	file = {Blundell et al. - 2015 - Weight Uncertainty in Neural Network.pdf:/Users/lucor/Zotero/storage/QEE2XMMC/Blundell et al. - 2015 - Weight Uncertainty in Neural Network.pdf:application/pdf},
}

@article{papamarkou_challenges_2021,
	title = {Challenges in {Markov} chain {Monte} {Carlo} for {Bayesian} neural networks},
	url = {http://arxiv.org/abs/1910.06539},
	abstract = {Markov chain Monte Carlo (MCMC) methods have not been broadly adopted in Bayesian neural networks (BNNs). This paper initially reviews the main challenges in sampling from the parameter posterior of a neural network via MCMC. Such challenges culminate to lack of convergence to the parameter posterior. Nevertheless, this paper shows that a non-converged Markov chain, generated via MCMC sampling from the parameter space of a neural network, can yield via Bayesian marginalization a valuable predictive posterior of the output of the neural network. Classification examples based on multilayer perceptrons showcase highly accurate predictive posteriors. The postulate of limited scope for MCMC developments in BNNs is partially valid; an asymptotically exact parameter posterior seems less plausible, yet an accurate predictive posterior is a tenable research avenue.},
	urldate = {2021-04-23},
	journal = {arXiv:1910.06539 [cs, stat]},
	author = {Papamarkou, Theodore and Hinkle, Jacob and Young, M. Todd and Womble, David},
	month = feb,
	year = {2021},
	note = {arXiv: 1910.06539},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology, Statistics - Computation},
	file = {Papamarkou et al. - 2021 - Challenges in Markov chain Monte Carlo for Bayesia.pdf:/Users/lucor/Zotero/storage/FPMQNFQ7/Papamarkou et al. - 2021 - Challenges in Markov chain Monte Carlo for Bayesia.pdf:application/pdf},
}

@article{goan_bayesian_2020,
	title = {Bayesian {Neural} {Networks}: {An} {Introduction} and {Survey}},
	volume = {2259},
	shorttitle = {Bayesian {Neural} {Networks}},
	url = {http://arxiv.org/abs/2006.12024},
	doi = {10.1007/978-3-030-42553-1_3},
	abstract = {Neural Networks (NNs) have provided state-of-the-art results for many challenging machine learning tasks such as detection, regression and classification across the domains of computer vision, speech recognition and natural language processing. Despite their success, they are often implemented in a frequentist scheme, meaning they are unable to reason about uncertainty in their predictions. This article introduces Bayesian Neural Networks (BNNs) and the seminal research regarding their implementation. Different approximate inference methods are compared, and used to highlight where future research can improve on current methods.},
	urldate = {2021-04-21},
	journal = {arXiv:2006.12024 [cs, stat]},
	author = {Goan, Ethan and Fookes, Clinton},
	year = {2020},
	note = {arXiv: 2006.12024},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {45--87},
	file = {Goan and Fookes - 2020 - Bayesian Neural Networks An Introduction and Surv.pdf:/Users/lucor/Zotero/storage/59VB3IBZ/Goan and Fookes - 2020 - Bayesian Neural Networks An Introduction and Surv.pdf:application/pdf},
}

@article{jospin_hands-bayesian_2020,
	title = {Hands-on {Bayesian} {Neural} {Networks} -- a {Tutorial} for {Deep} {Learning} {Users}},
	url = {http://arxiv.org/abs/2007.06823},
	abstract = {Modern deep learning methods have equipped researchers and engineers with incredibly powerful tools to tackle problems that previously seemed impossible. However, since deep learning methods operate as black boxes, the uncertainty associated with their predictions is often challenging to quantify. Bayesian statistics offer a formalism to understand and quantify the uncertainty associated with deep neural networks predictions. This paper provides a tutorial for researchers and scientists who are using machine learning, especially deep learning, with an overview of the relevant literature and a complete toolset to design, implement, train, use and evaluate Bayesian neural networks.},
	urldate = {2021-04-13},
	journal = {arXiv:2007.06823 [cs, stat]},
	author = {Jospin, Laurent Valentin and Buntine, Wray and Boussaid, Farid and Laga, Hamid and Bennamoun, Mohammed},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.06823},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, 62-02 (Primary), G.3, Hands-on, I.2.6, Tutorial},
	file = {Jospin et al_2020_Hands-on Bayesian Neural Networks -- a Tutorial for Deep Learning Users.pdf:/Users/lucor/Zotero/storage/GRBHRQ7D/Jospin et al_2020_Hands-on Bayesian Neural Networks -- a Tutorial for Deep Learning Users.pdf:application/pdf},
}

@article{wu_deterministic_2019,
	title = {Deterministic {Variational} {Inference} for {Robust} {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1810.03958},
	abstract = {Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data. Among approaches to realize probabilistic inference in deep neural networks, variational Bayes (VB) is theoretically grounded, generally applicable, and computationally efficient. With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for BNNs in real applications? We argue that variational inference in neural networks is fragile: successful implementations require careful initialization and tuning of prior variances, as well as controlling the variance of Monte Carlo gradient estimates. We provide two innovations that aim to turn VB into a robust inference tool for Bayesian neural networks: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel Empirical Bayes procedure for automatically selecting prior variances. Combining these two innovations, the resulting method is highly efficient and robust. On the application of heteroscedastic regression we demonstrate good predictive performance over alternative approaches.},
	urldate = {2021-04-13},
	journal = {arXiv:1810.03958 [cs, stat]},
	author = {Wu, Anqi and Nowozin, Sebastian and Meeds, Edward and Turner, Richard E. and Hernández-Lobato, José Miguel and Gaunt, Alexander L.},
	month = mar,
	year = {2019},
	note = {arXiv: 1810.03958},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Wu et al_2019_Deterministic Variational Inference for Robust Bayesian Neural Networks.pdf:/Users/lucor/Zotero/storage/6X3HC36Q/Wu et al_2019_Deterministic Variational Inference for Robust Bayesian Neural Networks.pdf:application/pdf},
}

@inproceedings{wenzel_how_2020,
	title = {How {Good} is the {Bayes} {Posterior} in {Deep} {Neural} {Networks} {Really}?},
	url = {http://proceedings.mlr.press/v119/wenzel20a.html},
	abstract = {During the past five years the Bayesian deep learning community has developed increasingly accurate and efficient approximate inference procedures that allow for Bayesian inference in deep neural n...},
	language = {en},
	urldate = {2021-04-13},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wenzel, Florian and Roth, Kevin and Veeling, Bastiaan and Swiatkowski, Jakub and Tran, Linh and Mandt, Stephan and Snoek, Jasper and Salimans, Tim and Jenatton, Rodolphe and Nowozin, Sebastian},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	keywords = {Review of UQ Methods},
	pages = {10248--10259},
	file = {Wenzel et al_2020_How Good is the Bayes Posterior in Deep Neural Networks Really.pdf:/Users/lucor/Zotero/storage/PKBD55R6/Wenzel et al_2020_How Good is the Bayes Posterior in Deep Neural Networks Really.pdf:application/pdf},
}

@article{nomura_calibrated_2021,
	title = {Calibrated uncertainty estimation for interpretable proton computed tomography image correction using {Bayesian} deep learning},
	volume = {66},
	issn = {0031-9155},
	url = {https://iopscience.iop.org/article/10.1088/1361-6560/abe956/meta},
	doi = {10.1088/1361-6560/abe956},
	language = {en},
	number = {6},
	urldate = {2021-04-13},
	journal = {Phys. Med. Biol.},
	author = {Nomura, Yusuke and Tanaka, Sodai and Wang, Jeff and Shirato, Hiroki and Shimizu, Shinichi and Xing, Lei},
	month = mar,
	year = {2021},
	note = {Publisher: IOP Publishing},
	pages = {065029},
	file = {Nomura et al_2021_Calibrated uncertainty estimation for interpretable proton computed tomography.pdf:/Users/lucor/Zotero/storage/EZ6LWTXY/Nomura et al_2021_Calibrated uncertainty estimation for interpretable proton computed tomography.pdf:application/pdf},
}

@inproceedings{schut_generating_2021,
	title = {Generating {Interpretable} {Counterfactual} {Explanations} {By} {Implicit} {Minimisation} of {Epistemic} and {Aleatoric} {Uncertainties}},
	abstract = {Counterfactual explanations (CEs) are a practical tool for demonstrating why machine learning classifiers make particular decisions. For CEs to be useful, it is important that they are easy for us...},
	language = {en},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Schut, Lisa and Key, Oscar and Grath, Rory Mc and Costabello, Luca and Sacaleanu, Bogdan and Corcoran, Medb and Gal, Yarin},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {AISTATS 2021, Code Available, Counterfactuals},
	pages = {1756--1764},
	file = {Schut et al_2021_Generating Interpretable Counterfactual Explanations By Implicit Minimisation.pdf:/Users/lucor/Zotero/storage/VUX6DRBQ/Schut et al_2021_Generating Interpretable Counterfactual Explanations By Implicit Minimisation.pdf:application/pdf;schut21a-supp.pdf:/Users/lucor/Zotero/storage/5VLMNCLB/schut21a-supp.pdf:application/pdf},
}

@article{jain_deup_2021,
	title = {{DEUP}: {Direct} {Epistemic} {Uncertainty} {Prediction}},
	shorttitle = {{DEUP}},
	url = {http://arxiv.org/abs/2102.08501},
	abstract = {Epistemic uncertainty is the part of out-of-sample prediction error due to the lack of knowledge of the learner. Whereas previous work was focusing on model variance, we propose a principled approach for directly estimating epistemic uncertainty by learning to predict generalization error and subtracting an estimate of aleatoric uncertainty, i.e., intrinsic unpredictability. This estimator of epistemic uncertainty includes the effect of model bias and can be applied in non-stationary learning environments arising in active learning or reinforcement learning. In addition to demonstrating these properties of Direct Epistemic Uncertainty Prediction (DEUP), we illustrate its advantage against existing methods for uncertainty estimation on downstream tasks including sequential model optimization and reinforcement learning. We also evaluate the quality of uncertainty estimates from DEUP for probabilistic classification of images and for estimating uncertainty about synergistic drug combinations.},
	urldate = {2021-04-12},
	journal = {arXiv:2102.08501 [cs, stat]},
	author = {Jain, Moksh and Lahlou, Salem and Nekoei, Hadi and Butoi, Victor and Bertin, Paul and Rector-Brooks, Jarrid and Korablyov, Maksym and Bengio, Yoshua},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.08501},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Jain et al_2021_DEUP.pdf:/Users/lucor/Zotero/storage/9C8WKA32/Jain et al_2021_DEUP.pdf:application/pdf},
}

@inproceedings{wang_beyond_2021,
	title = {Beyond {Marginal} {Uncertainty}: {How} {Accurately} can {Bayesian} {Regression} {Models} {Estimate} {Posterior} {Predictive} {Correlations}?},
	shorttitle = {Beyond {Marginal} {Uncertainty}},
	url = {http://proceedings.mlr.press/v130/wang21g.html},
	abstract = {While uncertainty estimation is a well-studied topic in deep learning, most such work focuses on marginal uncertainty estimates, i.e. the predictive mean and variance at individual input locations. But it is often more useful to estimate predictive correlations between the function values at different input locations. In this paper, we consider the problem of benchmarking how accurately Bayesian models can estimate predictive correlations. We first consider a downstream task which depends on posterior predictive correlations: transductive active learning (TAL). We find that TAL makes better use of models’ uncertainty estimates than ordinary active learning, and recommend this as a benchmark for evaluating Bayesian models. Since TAL is too expensive and indirect to guide development of algorithms, we introduce two metrics which more directly evaluate the predictive correlations and which can be computed efficiently: meta-correlations (i.e. the correlations between the models correlation estimates and the true values), and cross-normalized likelihoods (XLL). We validate these metrics by demonstrating their consistency with TAL performance and obtain insights about the relative performance of current Bayesian neural net and Gaussian process models.},
	language = {en},
	urldate = {2021-04-12},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Wang, Chaoqi and Sun, Shengyang and Grosse, Roger},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {AISTATS 2021, Code Available, AISTATS},
	pages = {2476--2484},
	file = {Wang et al_2021_Beyond Marginal Uncertainty.pdf:/Users/lucor/Zotero/storage/6H2Y86PB/Wang et al_2021_Beyond Marginal Uncertainty.pdf:application/pdf},
}

@article{zhang_quantification_2020,
	title = {On the quantification and efficient propagation of imprecise probabilities with copula dependence},
	volume = {122},
	issn = {0888-613X},
	url = {https://www.sciencedirect.com/science/article/pii/S0888613X20301511},
	doi = {10.1016/j.ijar.2020.04.002},
	abstract = {This paper addresses the problem of quantification and propagation of uncertainties associated with dependence modeling when data for characterizing probability models are limited. Practically, the system inputs are often assumed to be mutually independent or correlated by a multivariate Gaussian distribution. However, this subjective assumption may introduce bias in the response estimate if the real dependence structure deviates from this assumption. In this work, we overcome this limitation by introducing a flexible copula dependence model to capture complex dependencies. A hierarchical Bayesian multimodel approach is proposed to quantify uncertainty in dependence model-form and model parameters that result from small data sets. This approach begins by identifying, through Bayesian multimodel inference, a set of candidate marginal models and their corresponding model probabilities, and then estimating the uncertainty in the copula-based dependence structure, which is conditional on the marginals and their parameters. The overall uncertainties integrating marginals and copulas are probabilistically represented by an ensemble of multivariate candidate densities. A novel importance sampling reweighting approach is proposed to efficiently propagate the overall uncertainties through a computational model. Through an example studying the influence of constituent properties on the out-of-plane properties of transversely isotropic E-glass fiber composites, we show that the composite property with copula-based dependence model converges to the true estimate as data set size increases, while an independence or arbitrary Gaussian correlation assumption leads to a biased estimate.},
	language = {en},
	urldate = {2021-04-12},
	journal = {International Journal of Approximate Reasoning},
	author = {Zhang, Jiaxin and Shields, Michael},
	month = jul,
	year = {2020},
	keywords = {Uncertainty quantification, Bayesian inference, Copula, Imprecise probability, Multimodel inference, Small data},
	pages = {24--46},
	file = {Zhang_Shields_2020_On the quantification and efficient propagation of imprecise probabilities with.pdf:/Users/lucor/Zotero/storage/NYZNTKHX/Zhang_Shields_2020_On the quantification and efficient propagation of imprecise probabilities with.pdf:application/pdf},
}

@article{mobiny_dropconnect_2021,
	title = {{DropConnect} is effective in modeling uncertainty of {Bayesian} deep networks},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-84854-x},
	doi = {10.1038/s41598-021-84854-x},
	abstract = {Deep neural networks (DNNs) have achieved state-of-the-art performance in many important domains, including medical diagnosis, security, and autonomous driving. In domains where safety is highly critical, an erroneous decision can result in serious consequences. While a perfect prediction accuracy is not always achievable, recent work on Bayesian deep networks shows that it is possible to know when DNNs are more likely to make mistakes. Knowing what DNNs do not know is desirable to increase the safety of deep learning technology in sensitive applications; Bayesian neural networks attempt to address this challenge. Traditional approaches are computationally intractable and do not scale well to large, complex neural network architectures. In this paper, we develop a theoretical framework to approximate Bayesian inference for DNNs by imposing a Bernoulli distribution on the model weights. This method called Monte Carlo DropConnect (MC-DropConnect) gives us a tool to represent the model uncertainty with little change in the overall model structure or computational cost. We extensively validate the proposed algorithm on multiple network architectures and datasets for classification and semantic segmentation tasks. We also propose new metrics to quantify uncertainty estimates. This enables an objective comparison between MC-DropConnect and prior approaches. Our empirical results demonstrate that the proposed framework yields significant improvement in both prediction accuracy and uncertainty estimation quality compared to the state of the art.},
	language = {en},
	number = {1},
	urldate = {2021-11-03},
	journal = {Sci Rep},
	author = {Mobiny, Aryan and Yuan, Pengyu and Moulik, Supratik K. and Garg, Naveen and Wu, Carol C. and Van Nguyen, Hien},
	month = mar,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Biomedical engineering;Computational science;Computer science
Subject\_term\_id: biomedical-engineering;computational-science;computer-science},
	keywords = {Biomedical engineering, Computational science, Computer science},
	pages = {5458},
	file = {Mobiny et al_2021_DropConnect is effective in modeling uncertainty of Bayesian deep networks.pdf:/Users/lucor/Zotero/storage/8H7XLJY5/Mobiny et al_2021_DropConnect is effective in modeling uncertainty of Bayesian deep networks.pdf:application/pdf},
}

@article{ghoshal_estimating_2021,
	title = {Estimating uncertainty in deep learning for reporting confidence to clinicians in medical image segmentation and diseases detection},
	volume = {37},
	issn = {1467-8640},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/coin.12411},
	doi = {10.1111/coin.12411},
	abstract = {Deep learning (DL), which involves powerful black box predictors, has achieved a remarkable performance in medical image analysis, such as segmentation and classification for diagnosis. However, in spite of these successes, these methods focus exclusively on improving the accuracy of point predictions without assessing the quality of their outputs. Knowing how much confidence there is in a prediction is essential for gaining clinicians' trust in the technology. In this article, we propose an uncertainty estimation framework, called MC-DropWeights, to approximate Bayesian inference in DL by imposing a Bernoulli distribution on the incoming or outgoing weights of the model, including neurones. We demonstrate that by decomposing predictive probabilities into two main types of uncertainty, aleatoric and epistemic, using the Bayesian Residual U-Net (BRUNet) in image segmentation. Approximation methods in Bayesian DL suffer from the “mode collapse” phenomenon in variational inference. To address this problem, we propose a model which Ensembles of Monte-Carlo DropWeights by varying the DropWeights rate. In segmentation, we introduce a predictive uncertainty estimator, which takes the mean of the standard deviations of the class probabilities associated with every class. However, in classification, we need an alternative approach since the predictive probabilities from a forward pass through the model does not capture uncertainty. The entropy of the predictive distribution is a measure of uncertainty, but its exponential depends on sample size. The plug-in estimate in mutual information is subject to sampling bias. We propose Jackknife resampling, to correct for sample bias, which improves estimating uncertainty quality in image classification. We demonstrate that our deep ensemble MC-DropWeights method, using the bias-corrected estimator produces an equally good or better result in both quantified uncertainty estimation and quality of uncertainty estimates than approximate Bayesian neural networks in practice.},
	language = {en},
	number = {2},
	urldate = {2021-11-03},
	journal = {Computational Intelligence},
	author = {Ghoshal, Biraja and Tucker, Allan and Sanghera, Bal and Lup Wong, Wai},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/coin.12411},
	keywords = {deep learning, bias-corrected uncertainty estimation, classification, dropweights, ensembles, medical image segmentation},
	pages = {701--734},
	file = {Ghoshal et al_2021_Estimating uncertainty in deep learning for reporting confidence to clinicians.pdf:/Users/lucor/Zotero/storage/S9K8GJWX/Ghoshal et al_2021_Estimating uncertainty in deep learning for reporting confidence to clinicians.pdf:application/pdf},
}

@article{singh_quantifying_2021,
	title = {Quantifying {Model} {Predictive} {Uncertainty} with {Perturbation} {Theory}},
	url = {http://arxiv.org/abs/2109.10888},
	abstract = {We propose a framework for predictive uncertainty quantification of a neural network that replaces the conventional Bayesian notion of weight probability density function (PDF) with a physics based potential field representation of the model weights in a Gaussian reproducing kernel Hilbert space (RKHS) embedding. This allows us to use perturbation theory from quantum physics to formulate a moment decomposition problem over the model weight-output relationship. The extracted moments reveal successive degrees of regularization of the weight potential field around the local neighborhood of the model output. Such localized moments represent well the PDF tails and provide significantly greater accuracy of the model's predictive uncertainty than the central moments characterized by Bayesian and ensemble methods or their variants. We show that this consequently leads to a better ability to detect false model predictions of test data that has undergone a covariate shift away from the training PDF learned by the model. We evaluate our approach against baseline uncertainty quantification methods on several benchmark datasets that are corrupted using common distortion techniques. Our approach provides fast model predictive uncertainty estimates with much greater precision and calibration.},
	urldate = {2021-11-03},
	journal = {arXiv:2109.10888 [cs, math]},
	author = {Singh, Rishabh and Principe, Jose C.},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.10888},
	keywords = {Computer Science - Machine Learning, Computer Science - Information Theory},
	annote = {Comment: 16 pages, 12 figures, 4 tables. arXiv admin note: text overlap with arXiv:2103.01374},
	file = {Singh_Principe_2021_Quantifying Model Predictive Uncertainty with Perturbation Theory.pdf:/Users/lucor/Zotero/storage/2JTCGUNU/Singh_Principe_2021_Quantifying Model Predictive Uncertainty with Perturbation Theory.pdf:application/pdf},
}

@article{galil_disrupting_2021,
	title = {Disrupting {Deep} {Uncertainty} {Estimation} {Without} {Harming} {Accuracy}},
	url = {http://arxiv.org/abs/2110.13741},
	abstract = {Deep neural networks (DNNs) have proven to be powerful predictors and are widely used for various tasks. Credible uncertainty estimation of their predictions, however, is crucial for their deployment in many risk-sensitive applications. In this paper we present a novel and simple attack, which unlike adversarial attacks, does not cause incorrect predictions but instead cripples the network's capacity for uncertainty estimation. The result is that after the attack, the DNN is more confident of its incorrect predictions than about its correct ones without having its accuracy reduced. We present two versions of the attack. The first scenario focuses on a black-box regime (where the attacker has no knowledge of the target network) and the second scenario attacks a white-box setting. The proposed attack is only required to be of minuscule magnitude for its perturbations to cause severe uncertainty estimation damage, with larger magnitudes resulting in completely unusable uncertainty estimations. We demonstrate successful attacks on three of the most popular uncertainty estimation methods: the vanilla softmax score, Deep Ensembles and MC-Dropout. Additionally, we show an attack on SelectiveNet, the selective classification architecture. We test the proposed attack on several contemporary architectures such as MobileNetV2 and EfficientNetB0, all trained to classify ImageNet.},
	urldate = {2021-11-03},
	journal = {arXiv:2110.13741 [cs, stat]},
	author = {Galil, Ido and El-Yaniv, Ran},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.13741},
	keywords = {Computer Science - Machine Learning, NeurIPS, Statistics - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: To be published in NeurIPS 2021},
	file = {Galil_El-Yaniv_2021_Disrupting Deep Uncertainty Estimation Without Harming Accuracy.pdf:/Users/lucor/Zotero/storage/XZIBUVMK/Galil_El-Yaniv_2021_Disrupting Deep Uncertainty Estimation Without Harming Accuracy.pdf:application/pdf},
}

@article{van_molle_leveraging_2021,
	title = {Leveraging the {Bhattacharyya} coefficient for uncertainty quantification in deep neural networks},
	volume = {33},
	issn = {1433-3058},
	url = {https://doi.org/10.1007/s00521-021-05789-y},
	doi = {10.1007/s00521-021-05789-y},
	abstract = {Modern deep learning models achieve state-of-the-art results for many tasks in computer vision, such as image classification and segmentation. However, its adoption into high-risk applications, e.g. automated medical diagnosis systems, happens at a slow pace. One of the main reasons for this is that regular neural networks do not capture uncertainty. To assess uncertainty in classification, several techniques have been proposed casting neural network approaches in a Bayesian setting. Amongst these techniques, Monte Carlo dropout is by far the most popular. This particular technique estimates the moments of the output distribution through sampling with different dropout masks. The output uncertainty of a neural network is then approximated as the sample variance. In this paper, we highlight the limitations of such a variance-based uncertainty metric and propose an novel approach. Our approach is based on the overlap between output distributions of different classes. We show that our technique leads to a better approximation of the inter-class output confusion. We illustrate the advantages of our method using benchmark datasets. In addition, we apply our metric to skin lesion classification—a real-world use case—and show that this yields promising results.},
	language = {en},
	number = {16},
	urldate = {2021-11-03},
	journal = {Neural Comput \& Applic},
	author = {Van Molle, Pieter and Verbelen, Tim and Vankeirsbilck, Bert and De Vylder, Jonas and Diricx, Bart and Kimpe, Tom and Simoens, Pieter and Dhoedt, Bart},
	month = aug,
	year = {2021},
	pages = {10259--10275},
	file = {Van Molle et al_2021_Leveraging the Bhattacharyya coefficient for uncertainty quantification in deep.pdf:/Users/lucor/Zotero/storage/BS6PADRY/Van Molle et al_2021_Leveraging the Bhattacharyya coefficient for uncertainty quantification in deep.pdf:application/pdf},
}

@inproceedings{tomani_post-hoc_2021,
	address = {Nashville, TN, USA},
	title = {Post-hoc {Uncertainty} {Calibration} for {Domain} {Drift} {Scenarios}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9577789/},
	doi = {10.1109/CVPR46437.2021.00999},
	abstract = {We address the problem of uncertainty calibration. While standard deep neural networks typically yield uncalibrated predictions, calibrated conﬁdence scores that are representative of the true likelihood of a prediction can be achieved using post-hoc calibration methods. However, to date, the focus of these approaches has been on in-domain calibration. Our contribution is two-fold. First, we show that existing post-hoc calibration methods yield highly overconﬁdent predictions under domain shift. Second, we introduce a simple strategy where perturbations are applied to samples in the validation set before performing the post-hoc calibration step. In extensive experiments, we demonstrate that this perturbation step results in substantially better calibration under domain shift on a wide range of architectures and modelling tasks.},
	language = {en},
	urldate = {2021-11-21},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Tomani, Christian and Gruber, Sebastian and Erdem, Muhammed Ebrar and Cremers, Daniel and Buettner, Florian},
	month = jun,
	year = {2021},
	pages = {10119--10127},
	file = {Tomani et al_2021_Post-hoc Uncertainty Calibration for Domain Drift Scenarios.pdf:/Users/lucor/Zotero/storage/9AFFSGY9/Tomani et al_2021_Post-hoc Uncertainty Calibration for Domain Drift Scenarios.pdf:application/pdf},
}

@article{egele_autodeuq_2021,
	title = {{AutoDEUQ}: {Automated} {Deep} {Ensemble} with {Uncertainty} {Quantification}},
	shorttitle = {{AutoDEUQ}},
	url = {http://arxiv.org/abs/2110.13511},
	abstract = {Deep neural networks are powerful predictors for a variety of tasks. However, they do not capture uncertainty directly. Using neural network ensembles to quantify uncertainty is competitive with approaches based on Bayesian neural networks while benefiting from better computational scalability. However, building ensembles of neural networks is a challenging task because, in addition to choosing the right neural architecture or hyperparameters for each member of the ensemble, there is an added cost of training each model. We propose AutoDEUQ, an automated approach for generating an ensemble of deep neural networks. Our approach leverages joint neural architecture and hyperparameter search to generate ensembles. We use the law of total variance to decompose the predictive variance of deep ensembles into aleatoric (data) and epistemic (model) uncertainties. We show that AutoDEUQ outperforms probabilistic backpropagation, Monte Carlo dropout, deep ensemble, distribution-free ensembles, and hyper ensemble methods on a number of regression benchmarks.},
	urldate = {2021-11-21},
	journal = {arXiv:2110.13511 [cs]},
	author = {Egele, Romain and Maulik, Romit and Raghavan, Krishnan and Balaprakash, Prasanna and Lusch, Bethany},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.13511},
	keywords = {Computer Science - Machine Learning},
	file = {Egele et al_2021_AutoDEUQ.pdf:/Users/lucor/Zotero/storage/UZWGG5TG/Egele et al_2021_AutoDEUQ.pdf:application/pdf},
}

@article{ricotti_neural_1999,
	title = {Neural network approach to sensitivity and uncertainty analysis},
	volume = {64},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S095183209800057X},
	doi = {10.1016/S0951-8320(98)00057-X},
	abstract = {Computer simulation of the dynamic evolution of complex systems has become a fundamental tool for many modern engineering activities. In particular, risk-informed design projects and safety analyses require that the system behavior be analyzed under several diverse conditions in the presence of substantial model and parameter uncertainty which must be accounted for. In this paper we investigate the capabilities of artificial neural networks of providing both a first-order sensitivity measure of the importance of the various parameters of a model and a fast, efficient tool for dynamic simulation, to be used in uncertainty analyses. The dynamic simulation of a steam generator is considered as a test-bed to show the potentialities of these tools and to point out the difficulties and crucial issues which typically arise when attempting to establish an efficient neural network structure for sensitivity and uncertainty analyses.},
	language = {en},
	number = {1},
	urldate = {2021-11-17},
	journal = {Reliability Engineering \& System Safety},
	author = {Ricotti, M. E. and Zio, E.},
	month = apr,
	year = {1999},
	keywords = {Error backpropagation, Multi-layered, feedforward neural networks, Sensitivity and uncertainty analysis, Steam generator transient simulation, Superposition},
	pages = {59--71},
	file = {ScienceDirect Snapshot:/Users/lucor/Zotero/storage/KBGBB8QF/S095183209800057X.html:text/html},
}

@article{bartl_sensitivity_2021,
	title = {Sensitivity analysis of {Wasserstein} distributionally robust optimization problems},
	url = {http://arxiv.org/abs/2006.12022},
	abstract = {We consider sensitivity of a generic stochastic optimization problem to model uncertainty. We take a non-parametric approach and capture model uncertainty using Wasserstein balls around the postulated model. We provide explicit formulae for the first order correction to both the value function and the optimizer and further extend our results to optimization under linear constraints. We present applications to statistics, machine learning, mathematical finance and uncertainty quantification. In particular, we provide explicit first-order approximation for square-root LASSO regression coefficients and deduce coefficient shrinkage compared to the ordinary least squares regression. We consider robustness of call option pricing and deduce a new Black-Scholes sensitivity, a non-parametric version of the so-called Vega. We also compute sensitivities of optimized certainty equivalents in finance and propose measures to quantify robustness of neural networks to adversarial examples.},
	urldate = {2021-11-17},
	journal = {arXiv:2006.12022 [math, q-fin, stat]},
	author = {Bartl, Daniel and Drapeau, Samuel and Obloj, Jan and Wiesel, Johannes},
	month = nov,
	year = {2021},
	note = {arXiv: 2006.12022},
	keywords = {Mathematics - Statistics Theory, Mathematics - Optimization and Control, Mathematics - Probability, Quantitative Finance - Mathematical Finance},
	annote = {Comment: Forthcoming in "Proceedings of the Royal Society A"},
	file = {Bartl et al_2021_Sensitivity analysis of Wasserstein distributionally robust optimization.pdf:/Users/lucor/Zotero/storage/SJUVDCAQ/Bartl et al_2021_Sensitivity analysis of Wasserstein distributionally robust optimization.pdf:application/pdf},
}

@article{kamarthi_when_2021,
	title = {When in {Doubt}: {Neural} {Non}-{Parametric} {Uncertainty} {Quantification} for {Epidemic} {Forecasting}},
	shorttitle = {When in {Doubt}},
	url = {http://arxiv.org/abs/2106.03904},
	abstract = {Accurate and trustworthy epidemic forecasting is an important problem that has impact on public health planning and disease mitigation. Most existing epidemic forecasting models disregard uncertainty quantification, resulting in mis-calibrated predictions. Recent works in deep neural models for uncertainty-aware time-series forecasting also have several limitations; e.g. it is difficult to specify meaningful priors in Bayesian NNs, while methods like deep ensembling are computationally expensive in practice. In this paper, we fill this important gap. We model the forecasting task as a probabilistic generative process and propose a functional neural process model called EPIFNP, which directly models the probability density of the forecast value. EPIFNP leverages a dynamic stochastic correlation graph to model the correlations between sequences in a non-parametric way, and designs different stochastic latent variables to capture functional uncertainty from different perspectives. Our extensive experiments in a real-time flu forecasting setting show that EPIFNP significantly outperforms previous state-of-the-art models in both accuracy and calibration metrics, up to 2.5x in accuracy and 2.4x in calibration. Additionally, due to properties of its generative process,EPIFNP learns the relations between the current season and similar patterns of historical seasons,enabling interpretable forecasts. Beyond epidemic forecasting, the EPIFNP can be of independent interest for advancing principled uncertainty quantification in deep sequential models for predictive analytics},
	urldate = {2021-11-17},
	journal = {arXiv:2106.03904},
	author = {Kamarthi, Harshavardhan and Kong, Lingkai and Rodríguez, Alexander and Zhang, Chao and Prakash, B. Aditya},
	month = nov,
	year = {2021},
	note = {arXiv: 2106.03904},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted at NeurIPS 2021},
	file = {Kamarthi et al_2021_When in Doubt.pdf:/Users/lucor/Zotero/storage/SMA8L7VJ/Kamarthi et al_2021_When in Doubt.pdf:application/pdf},
}

@inproceedings{kumar_verified_2019,
	title = {Verified {Uncertainty} {Calibration}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/f8c0c968632845cd133308b1a494967f-Abstract.html},
	urldate = {2021-11-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kumar, Ananya and Liang, Percy S and Ma, Tengyu},
	year = {2019},
	file = {Kumar et al_2019_Verified Uncertainty Calibration.pdf:/Users/lucor/Zotero/storage/WI76APCW/Kumar et al_2019_Verified Uncertainty Calibration.pdf:application/pdf},
}

@article{tennoe_uncertainpy_2018,
	title = {Uncertainpy: {A} {Python} {Toolbox} for {Uncertainty} {Quantification} and {Sensitivity} {Analysis} in {Computational} {Neuroscience}},
	volume = {12},
	issn = {1662-5196},
	shorttitle = {Uncertainpy},
	url = {https://www.frontiersin.org/article/10.3389/fninf.2018.00049},
	doi = {10.3389/fninf.2018.00049},
	abstract = {Computational models in neuroscience typically contain many parameters that are poorly constrained by experimental data. Uncertainty quantification and sensitivity analysis provide rigorous procedures to quantify how the model output depends on this parameter uncertainty. Unfortunately, the application of such methods is not yet standard within the field of neuroscience. Here we present Uncertainpy, an open-source Python toolbox, tailored to perform uncertainty quantification and sensitivity analysis of neuroscience models. Uncertainpy aims to make it quick and easy to get started with uncertainty analysis, without any need for detailed prior knowledge. The toolbox allows uncertainty quantification and sensitivity analysis to be performed on already existing models without needing to modify the model equations or model implementation. Uncertainpy bases its analysis on polynomial chaos expansions, which are more efficient than the more standard Monte-Carlo based approaches. Uncertainpy is tailored for neuroscience applications by its built-in capability for calculating characteristic features in the model output. The toolbox does not merely perform a point-to-point comparison of the “raw” model output (e.g., membrane voltage traces), but can also calculate the uncertainty and sensitivity of salient model response features such as spike timing, action potential width, average interspike interval, and other features relevant for various neural and neural network models. Uncertainpy comes with several common models and features built in, and including custom models and new features is easy. The aim of the current paper is to present Uncertainpy to the neuroscience community in a user-oriented manner. To demonstrate its broad applicability, we perform an uncertainty quantification and sensitivity analysis of three case studies relevant for neuroscience: the original Hodgkin-Huxley point-neuron model for action potential generation, a multi-compartmental model of a thalamic interneuron implemented in the NEURON simulator, and a sparsely connected recurrent network model implemented in the NEST simulator.},
	urldate = {2021-11-14},
	journal = {Frontiers in Neuroinformatics},
	author = {Tennøe, Simen and Halnes, Geir and Einevoll, Gaute T.},
	year = {2018},
	pages = {49},
	file = {Tennøe et al_2018_Uncertainpy.pdf:/Users/lucor/Zotero/storage/JQCGJAI8/Tennøe et al_2018_Uncertainpy.pdf:application/pdf},
}

@inproceedings{morales-alvarez_activation-level_2020,
	title = {Activation-level uncertainty in deep neural networks},
	url = {https://openreview.net/forum?id=UvBPbpvHRj-},
	abstract = {Current approaches for uncertainty estimation in deep learning often produce too confident results. Bayesian Neural Networks (BNNs) model uncertainty in the space of weights, which is usually...},
	language = {en},
	author = {Morales-Alvarez, Pablo and Hernández-Lobato, Daniel and Molina, Rafael and Hernández-Lobato, José Miguel},
	month = sep,
	year = {2020},
	file = {Morales-Alvarez et al_2020_Activation-level uncertainty in deep neural networks.pdf:/Users/lucor/Zotero/storage/U4DKQ2Z3/Morales-Alvarez et al_2020_Activation-level uncertainty in deep neural networks.pdf:application/pdf},
}

@article{shanmugam_better_2021,
	title = {Better {Aggregation} in {Test}-{Time} {Augmentation}},
	abstract = {Test-time augmentation -- the aggregation of predictions across transformed versions of a test input -- is a common practice in image classification. Traditionally, predictions are combined using a simple average. In this paper, we present 1) experimental analyses that shed light on cases in which the simple average is suboptimal and 2) a method to address these shortcomings. A key finding is that even when test-time augmentation produces a net improvement in accuracy, it can change many correct predictions into incorrect predictions. We delve into when and why test-time augmentation changes a prediction from being correct to incorrect and vice versa. Building on these insights, we present a learning-based method for aggregating test-time augmentations. Experiments across a diverse set of models, datasets, and augmentations show that our method delivers consistent improvements over existing approaches.},
	journal = {arXiv:2011.11156},
	author = {Shanmugam, Divya and Blalock, Davis and Balakrishnan, Guha and Guttag, John},
	month = oct,
	year = {2021},
	note = {arXiv: 2011.11156},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Shanmugam et al_2021_Better Aggregation in Test-Time Augmentation.pdf:/Users/lucor/Zotero/storage/B9RL4WET/Shanmugam et al_2021_Better Aggregation in Test-Time Augmentation.pdf:application/pdf},
}

@inproceedings{ashukha_pitfalls_2020,
	title = {Pitfalls of {In}-{Domain} {Uncertainty} {Estimation} and {Ensembling} in {Deep} {Learning}},
	abstract = {Uncertainty estimation and ensembling methods go hand-in-hand. Uncertainty estimation is one of the main benchmarks for assessment of ensembling performance. At the same time, deep learning ensembles have provided state-of-the-art results in uncertainty estimation. In this work, we focus on in-domain uncertainty for image classiﬁcation. We explore the standards for its quantiﬁcation and point out pitfalls of existing metrics. Avoiding these pitfalls, we perform a broad study of different ensembling techniques. To provide more insight in this study, we introduce the deep ensemble equivalent score (DEE) and show that many sophisticated ensembling techniques are equivalent to an ensemble of only few independently trained networks in terms of test performance.},
	language = {en},
	booktitle = {Eighth {International} {Conference} on {Learning} {Representations}},
	author = {Ashukha, Arsenii and Molchanov, Dmitry and Lyzhov, Alexander and Vetrov, Dmitry},
	year = {2020},
	pages = {30},
	file = {Ashukha et al_2020_Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning.pdf:/Users/lucor/Zotero/storage/PWMPGAJY/Ashukha et al_2020_Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning.pdf:application/pdf},
}

@article{gawlikowski_survey_2021,
  title={A survey of uncertainty in deep neural networks},
  author={Gawlikowski, Jakob and Tassi, Cedrique Rovile Njieutcheu and Ali, Mohsin and Lee, Jongseok and Humt, Matthias and Feng, Jianxiang and Kruspe, Anna and Triebel, Rudolph and Jung, Peter and Roscher, Ribana and others},
  journal={arXiv preprint arXiv:2107.03342},
  year={2021}
}



@article{ghosh_uncertainty_2021,
	title = {Uncertainty {Quantification} 360: {A} {Holistic} {Toolkit} for {Quantifying} and {Communicating} the {Uncertainty} of {AI}},
	shorttitle = {Uncertainty {Quantification} 360},
	url = {http://arxiv.org/abs/2106.01410},
	abstract = {In this paper, we describe an open source Python toolkit named Uncertainty Quantification 360 (UQ360) for the uncertainty quantification of AI models. The goal of this toolkit is twofold: first, to provide a broad range of capabilities to streamline as well as foster the common practices of quantifying, evaluating, improving, and communicating uncertainty in the AI application development lifecycle; second, to encourage further exploration of UQ's connections to other pillars of trustworthy AI such as fairness and transparency through the dissemination of latest research and education materials. Beyond the Python package ({\textbackslash}url\{https://github.com/IBM/UQ360\}), we have developed an interactive experience ({\textbackslash}url\{http://uq360.mybluemix.net\}) and guidance materials as educational tools to aid researchers and developers in producing and communicating high-quality uncertainties in an effective manner.},
	urldate = {2022-03-03},
	journal = {arXiv:2106.01410 [cs]},
	author = {Ghosh, Soumya and Liao, Q. Vera and Ramamurthy, Karthikeyan Natesan and Navratil, Jiri and Sattigeri, Prasanna and Varshney, Kush R. and Zhang, Yunfeng},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.01410},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Added references},
	file = {Ghosh et al_2021_Uncertainty Quantification 360.pdf:/Users/lucor/Zotero/storage/B6K5XATV/Ghosh et al_2021_Uncertainty Quantification 360.pdf:application/pdf},
}

@article{amri_sampling_2021,
	title = {A sampling criterion for constrained {Bayesian} optimization with uncertainties},
	url = {http://arxiv.org/abs/2103.05706},
	abstract = {We consider the problem of chance constrained optimization where it is sought to optimize a function and satisfy constraints, both of which are affected by uncertainties. The real world declinations of this problem are particularly challenging because of their inherent computational cost. To tackle such problems, we propose a new Bayesian optimization method. It applies to the situation where the uncertainty comes from some of the inputs, so that it becomes possible to define an acquisition criterion in the joint controlled-uncontrolled input space. The main contribution of this work is an acquisition criterion that accounts for both the average improvement in objective function and the constraint reliability. The criterion is derived following the Stepwise Uncertainty Reduction logic and its maximization provides both optimal controlled and uncontrolled parameters. Analytical expressions are given to efficiently calculate the criterion. Numerical studies on test functions are presented. It is found through experimental comparisons with alternative sampling criteria that the adequation between the sampling criterion and the problem contributes to the efficiency of the overall optimization. As a side result, an expression for the variance of the improvement is given.},
	urldate = {2022-02-20},
	journal = {arXiv:2103.05706 [cs, math, stat]},
	author = {Amri, Reda El and Riche, Rodolphe Le and Helbert, Céline and Blanchet-Scalliet, Christophette and Da Veiga, Sébastien},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.05706},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {Amri et al_2021_A sampling criterion for constrained Bayesian optimization with uncertainties.pdf:/Users/lucor/Zotero/storage/2RC68HXN/Amri et al_2021_A sampling criterion for constrained Bayesian optimization with uncertainties.pdf:application/pdf},
}

@inproceedings{shah_autoai-ts_2021,
	title = {{AutoAI}-{TS}: {AutoAI} for {Time} {Series} {Forecasting}},
	shorttitle = {{AutoAI}-{TS}},
	url = {http://arxiv.org/abs/2102.12347},
	abstract = {A large number of time series forecasting models including traditional statistical models, machine learning models and more recently deep learning have been proposed in the literature. However, choosing the right model along with good parameter values that performs well on a given data is still challenging. Automatically providing a good set of models to users for a given dataset saves both time and effort from using trial-and-error approaches with a wide variety of available models along with parameter optimization. We present AutoAI for Time Series Forecasting (AutoAI-TS) that provides users with a zero configuration (zero-conf ) system to efficiently train, optimize and choose best forecasting model among various classes of models for the given dataset. With its flexible zero-conf design, AutoAI-TS automatically performs all the data preparation, model creation, parameter optimization, training and model selection for users and provides a trained model that is ready to use. For given data, AutoAI-TS utilizes a wide variety of models including classical statistical models, Machine Learning (ML) models, statistical-ML hybrid models and deep learning models along with various transformations to create forecasting pipelines. It then evaluates and ranks pipelines using the proposed T-Daub mechanism to choose the best pipeline. The paper describe in detail all the technical aspects of AutoAI-TS along with extensive benchmarking on a variety of real world data sets for various use-cases. Benchmark results show that AutoAI-TS, with no manual configuration from the user, automatically trains and selects pipelines that on average outperform existing state-of-the-art time series forecasting toolkits.},
	booktitle = {{ACM} {SIGMOD}},
	author = {Shah, Syed Yousaf and Patel, Dhaval and Vu, Long and Dang, Xuan-Hong and Chen, Bei and Kirchner, Peter and Samulowitz, Horst and Wood, David and Bramble, Gregory and Gifford, Wesley M. and Ganapavarapu, Giridhar and Vaculin, Roman and Zerfos, Petros},
	month = mar,
	year = {2021},
	note = {arXiv: 2102.12347},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted for publication at ACM SIGMOD 2021 Industry Track},
	file = {Shah et al_2021_AutoAI-TS.pdf:/Users/lucor/Zotero/storage/LNNESVER/Shah et al_2021_AutoAI-TS.pdf:application/pdf},
}

@article{wang_physics_2020,
	title = {Physics {Informed} {Deep} {Kernel} {Learning}},
	url = {https://openreview.net/forum?id=vNw0Gzw8oki},
	abstract = {Deep kernel learning is a promising combination of deep neural networks and nonparametric function estimation. However, as a data-driven approach, the performance of deep kernel learning can still...},
	language = {en},
	urldate = {2022-02-07},
	author = {Wang, Zheng and Xing, Wei and Kirby, Robert and Zhe, Shandian},
	month = sep,
	year = {2020},
	file = {Wang et al_2020_Physics Informed Deep Kernel Learning.pdf:/Users/lucor/Zotero/storage/77TVGUHE/Wang et al_2020_Physics Informed Deep Kernel Learning.pdf:application/pdf},
}

@article{tuo_uncertainty_2020,
	title = {Uncertainty {Quantification} for {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/2002.01569},
	abstract = {Bayesian optimization is a class of global optimization techniques. It regards the underlying objective function as a realization of a Gaussian process. Although the outputs of Bayesian optimization are random according to the Gaussian process assumption, quantification of this uncertainty is rarely studied in the literature. In this work, we propose a novel approach to assess the output uncertainty of Bayesian optimization algorithms, in terms of constructing confidence regions of the maximum point or value of the objective function. These regions can be computed efficiently, and their confidence levels are guaranteed by newly developed uniform error bounds for sequential Gaussian process regression. Our theory provides a unified uncertainty quantification framework for all existing sequential sampling policies and stopping criteria.},
	urldate = {2022-02-07},
	journal = {arXiv:2002.01569 [math, stat]},
	author = {Tuo, Rui and Wang, Wenjia},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.01569},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
	file = {Tuo_Wang_2020_Uncertainty Quantification for Bayesian Optimization.pdf:/Users/lucor/Zotero/storage/G46E5UX7/Tuo_Wang_2020_Uncertainty Quantification for Bayesian Optimization.pdf:application/pdf},
}

@article{blundell_weight_2015-1,
	title = {Weight {Uncertainty} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1505.05424},
	abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	urldate = {2022-02-04},
	journal = {arXiv:1505.05424 [cs, stat]},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	month = may,
	year = {2015},
	note = {arXiv: 1505.05424},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: In Proceedings of the 32nd International Conference on Machine Learning (ICML 2015)},
	file = {arXiv.org Snapshot:/Users/lucor/Zotero/storage/42DXZK3A/1505.html:text/html;Blundell et al_2015_Weight Uncertainty in Neural Networks.pdf:/Users/lucor/Zotero/storage/FJQXWZ3P/Blundell et al_2015_Weight Uncertainty in Neural Networks.pdf:application/pdf},
}

@article{schwab_deep_2019,
	title = {Deep learning in high dimension: {Neural} network expression rates for generalized polynomial chaos expansions in {UQ}},
	volume = {17},
	issn = {0219-5305},
	shorttitle = {Deep learning in high dimension},
	url = {https://www.worldscientific.com/doi/10.1142/S0219530518500203},
	doi = {10.1142/S0219530518500203},
	abstract = {We estimate the expressive power of certain deep neural networks (DNNs for short) on a class of countably-parametric, holomorphic maps 
��:��→ℝ
u:U→ℝ
 on the parameter domain 
��=
[−1,1]
ℕ
U=[−1,1]ℕ
. Dimension-independent rates of best 
��
n
-term truncations of generalized polynomial chaos (gpc for short) approximations depend only on the summability exponent of the sequence of their gpc expansion coefficients. So-called 
(��,��)
(b,��)
-holomorphic maps 
��
u
, with 
��∈
ℓ
��
b∈ℓp
 for some 
��∈(0,1)
p∈(0,1)
, are known to allow gpc expansions with coefficient sequences in 
ℓ
��
ℓp
. Such maps arise for example as response surfaces of parametric PDEs, with applications in PDE uncertainty quantification (UQ) for many mathematical models in engineering and the sciences. Up to logarithmic terms, we establish the dimension independent approximation rate 
��=1/��−1
s=1/p−1
 for these functions in terms of the total number 
��
N
 of units and weights in the DNN. It follows that certain DNN architectures can overcome the curse of dimensionality when expressing possibly countably-parametric, real-valued maps with a certain degree of sparsity in the sequences of their gpc expansion coefficients. We also obtain rates of expressive power of DNNs for countably-parametric maps 
��:��→��
u:U→V
, where 
��
V
 is the Hilbert space 
��
1
0
([0,1])
H01([0,1])
.},
	number = {01},
	urldate = {2022-01-29},
	journal = {Anal. Appl.},
	author = {Schwab, Christoph and Zech, Jakob},
	month = jan,
	year = {2019},
	note = {Publisher: World Scientific Publishing Co.},
	keywords = {deep networks, Generalized polynomial chaos, sparse grids, uncertainty quantification},
	pages = {19--55},
	file = {Schwab_Zech_2019_Deep learning in high dimension.pdf:/Users/lucor/Zotero/storage/RX6D4ZI3/Schwab_Zech_2019_Deep learning in high dimension.pdf:application/pdf},
}

@article{zheng_mini-data-driven_2021,
	title = {Mini-data-driven {Deep} {Arbitrary} {Polynomial} {Chaos} {Expansion} for {Uncertainty} {Quantification}},
	url = {http://arxiv.org/abs/2107.10428},
	abstract = {The surrogate model-based uncertainty quantification method has drawn a lot of attention in recent years. Both the polynomial chaos expansion (PCE) and the deep learning (DL) are powerful methods for building a surrogate model. However, the PCE needs to increase the expansion order to improve the accuracy of the surrogate model, which causes more labeled data to solve the expansion coefficients, and the DL also needs a lot of labeled data to train the neural network model. This paper proposes a deep arbitrary polynomial chaos expansion (Deep aPCE) method to improve the balance between surrogate model accuracy and training data cost. On the one hand, the multilayer perceptron (MLP) model is used to solve the adaptive expansion coefficients of arbitrary polynomial chaos expansion, which can improve the Deep aPCE model accuracy with lower expansion order. On the other hand, the adaptive arbitrary polynomial chaos expansion's properties are used to construct the MLP training cost function based on only a small amount of labeled data and a large scale of non-labeled data, which can significantly reduce the training data cost. Four numerical examples and an actual engineering problem are used to verify the effectiveness of the Deep aPCE method.},
	urldate = {2022-01-29},
	journal = {arXiv:2107.10428 [cs]},
	author = {Zheng, Xiaohu and Zhang, Jun and Wang, Ning and Tang, Guijian and Yao, Wen},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.10428},
	keywords = {Computer Science - Machine Learning},
	file = {Zheng et al_2021_Mini-data-driven Deep Arbitrary Polynomial Chaos Expansion for Uncertainty.pdf:/Users/lucor/Zotero/storage/XLUUCRRQ/Zheng et al_2021_Mini-data-driven Deep Arbitrary Polynomial Chaos Expansion for Uncertainty.pdf:application/pdf},
}

@article{markidis_old_2021,
	title = {The {Old} and the {New}: {Can} {Physics}-{Informed} {Deep}-{Learning} {Replace} {Traditional} {Linear} {Solvers}?},
	shorttitle = {The {Old} and the {New}},
	abstract = {Physics-Informed Neural Networks (PINN) are neural networks encoding the problem governing equations, such as Partial Differential Equations (PDE), as a part of the neural network. PINNs have emerged as a new essential tool to solve various challenging problems, including computing linear systems arising from PDEs, a task for which several traditional methods exist. In this work, we focus first on evaluating the potential of PINNs as linear solvers in the case of the Poisson equation, an omnipresent equation in scientific computing. We characterize PINN linear solvers in terms of accuracy and performance under different network configurations (depth, activation functions, input data set distribution). We highlight the critical role of transfer learning. Our results show that low-frequency components of the solution converge quickly as an effect of the F-principle. In contrast, an accurate solution of the high frequencies requires an exceedingly long time. To address this limitation, we propose integrating PINNs into traditional linear solvers. We show that this integration leads to the development of new solvers whose performance is on par with other high-performance solvers, such as PETSc conjugate gradient linear solvers, in terms of performance and accuracy. Overall, while the accuracy and computational performance are still a limiting factor for the direct use of PINN linear solvers, hybrid strategies combining old traditional linear solver approaches with new emerging deep-learning techniques are among the most promising methods for developing a new class of linear solvers.},
	journal = {arXiv:2103.09655},
	author = {Markidis, Stefano},
	month = jul,
	year = {2021},
	note = {arXiv: 2103.09655
version: 2},
	keywords = {Physics - Computational Physics, Computer Science - Distributed, Parallel, and Cluster Computing, Mathematics - Numerical Analysis},
	annote = {Comment: Preprint - submitted to Frontiers},
	file = {Markidis_2021_The Old and the New.pdf:/Users/lucor/Zotero/storage/VBC5755H/Markidis_2021_The Old and the New.pdf:application/pdf},
}

@article{shahane_uncertainty_2019,
	title = {Uncertainty quantification in three dimensional natural convection using polynomial chaos expansion and deep neural networks},
	volume = {139},
	issn = {0017-9310},
	url = {https://www.sciencedirect.com/science/article/pii/S0017931018363919},
	doi = {10.1016/j.ijheatmasstransfer.2019.05.014},
	abstract = {This paper analyzes the effects of input uncertainties on the outputs of a three dimensional natural convection problem in a differentially heated cubical enclosure. Two different cases are considered for parameter uncertainty propagation and global sensitivity analysis. In case A, stochastic variation is introduced in the two non-dimensional parameters (Rayleigh and Prandtl numbers) with an assumption that the boundary temperature is uniform. Being a two dimensional stochastic problem, the polynomial chaos expansion (PCE) method is used as a surrogate model. Case B deals with non-uniform stochasticity in the boundary temperature. Instead of the traditional Gaussian process model with the Karhunen-Loève expansion, a novel approach is successfully implemented to model uncertainty in the boundary condition. The boundary is divided into multiple domains and the temperature imposed on each domain is assumed to be an independent and identically distributed (i.i.d) random variable. Deep neural networks are trained with the boundary temperatures as inputs and Nusselt number, internal temperature or velocities as outputs. The number of domains which is essentially the stochastic dimension is 4, 8, 16 or 32. Rigorous training and testing process shows that the neural network is able to approximate the outputs to a reasonable accuracy. For a high stochastic dimension such as 32, it is computationally expensive to fit the PCE. This paper demonstrates a novel way of using the deep neural network as a surrogate modeling method for uncertainty quantification with the number of simulations much fewer than that required for fitting the PCE, thus, saving the computational cost.},
	language = {en},
	urldate = {2022-01-28},
	journal = {International Journal of Heat and Mass Transfer},
	author = {Shahane, Shantanu and Aluru, Narayana R. and Vanka, Surya Pratap},
	month = aug,
	year = {2019},
	keywords = {Uncertainty quantification, Deep neural networks, Natural convection, Polynomial chaos expansion},
	pages = {613--631},
	file = {Shahane et al_2019_Uncertainty quantification in three dimensional natural convection using.pdf:/Users/lucor/Zotero/storage/UT6L829R/Shahane et al_2019_Uncertainty quantification in three dimensional natural convection using.pdf:application/pdf},
}

@article{raissi_hidden_2020,
	title = {Hidden fluid mechanics: {Learning} velocity and pressure fields from flow visualizations},
	copyright = {Copyright © 2020 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works},
	shorttitle = {Hidden fluid mechanics},
	doi = {10.1126/science.aaw4741},
	abstract = {A machine learning approach exploiting the knowledge of Navier-Stokes equations can extract detailed fluid flow information.},
	language = {EN},
	journal = {Science},
	author = {Raissi, Maziar and Yazdani, Alireza and Karniadakis, George Em},
	month = feb,
	year = {2020},
	note = {Publisher: American Association for the Advancement of Science},
	file = {Raissi et al_2020_Hidden fluid mechanics.pdf:/Users/lucor/Zotero/storage/LEH4VJ78/Raissi et al_2020_Hidden fluid mechanics.pdf:application/pdf},
}

@article{shahzadi_deep_2021,
	title = {Deep {Neural} {Network} and {Polynomial} {Chaos} {Expansion}-{Based} {Surrogate} {Models} for {Sensitivity} and {Uncertainty} {Propagation}: {An} {Application} to a {Rockfill} {Dam}},
	volume = {13},
	issn = {2073-4441},
	shorttitle = {Deep {Neural} {Network} and {Polynomial} {Chaos} {Expansion}-{Based} {Surrogate} {Models} for {Sensitivity} and {Uncertainty} {Propagation}},
	doi = {doi.org/10.3390/w13131830},
	abstract = {Computational modeling plays a significant role in the design of rockfill dams. Various constitutive soil parameters are used to design such models, which often involve high uncertainties due to the complex structure of rockfill dams comprising various zones of different soil parameters. This study performs an uncertainty analysis and a global sensitivity analysis to assess the effect of constitutive soil parameters on the behavior of a rockfill dam. A Finite Element code (Plaxis) is utilized for the structure analysis. A database of the computed displacements at inclinometers installed in the dam is generated and compared to in situ measurements. Surrogate models are significant tools for approximating the relationship between input soil parameters and displacements and thereby reducing the computational costs of parametric studies. Polynomial chaos expansion and deep neural networks are used to build surrogate models to compute the Sobol indices required to identify the impact of soil parameters on dam behavior.},
	language = {en},
	number = {13},
	journal = {Water},
	author = {Shahzadi, Gullnaz and Soulaïmani, Azzeddine},
	month = jun,
	year = {2021},
	pages = {1830},
	file = {Shahzadi_Soulaïmani_2021_Deep Neural Network and Polynomial Chaos Expansion-Based Surrogate Models for.pdf:/Users/lucor/Zotero/storage/2E9CB2YS/Shahzadi_Soulaïmani_2021_Deep Neural Network and Polynomial Chaos Expansion-Based Surrogate Models for.pdf:application/pdf},
}

@article{yang_adversarial_2019,
	title = {Adversarial uncertainty quantification in physics-informed neural networks},
	volume = {394},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999119303584},
	abstract = {We present a deep learning framework for quantifying and propagating uncertainty in systems governed by non-linear differential equations using physics-informed neural networks. Specifically, we employ latent variable models to construct probabilistic representations for the system states, and put forth an adversarial inference procedure for training them on data, while constraining their predictions to satisfy given physical laws expressed by partial differential equations. Such physics-informed constraints provide a regularization mechanism for effectively training deep generative models as surrogates of physical systems in which the cost of data acquisition is high, and training data-sets are typically small. This provides a flexible framework for characterizing uncertainty in the outputs of physical systems due to randomness in their inputs or noise in their observations that entirely bypasses the need for repeatedly sampling expensive experiments or numerical simulators. We demonstrate the effectiveness of our approach through a series of examples involving uncertainty propagation in non-linear conservation laws, and the discovery of constitutive laws for flow through porous media directly from noisy data.},
	language = {en},
	journal = {Journal of Computational Physics},
	author = {Yang, Yibo and Perdikaris, Paris},
	month = oct,
	year = {2019},
	keywords = {Data-driven modeling, Generative adversarial networks, Probabilistic deep learning, Probabilistic scientific computing, Variational inference},
	pages = {136--152},
	file = {Yang_Perdikaris_2019_Adversarial uncertainty quantification in physics-informed neural networks.pdf:/Users/lucor/Zotero/storage/I4D87UTA/Yang_Perdikaris_2019_Adversarial uncertainty quantification in physics-informed neural networks.pdf:application/pdf},
}

@article{delaney_uncertainty_2021,
	title = {Uncertainty {Estimation} and {Out}-of-{Distribution} {Detection} for {Counterfactual} {Explanations}: {Pitfalls} and {Solutions}},
	shorttitle = {Uncertainty {Estimation} and {Out}-of-{Distribution} {Detection} for {Counterfactual} {Explanations}},
	url = {http://arxiv.org/abs/2107.09734},
	abstract = {Whilst an abundance of techniques have recently been proposed to generate counterfactual explanations for the predictions of opaque black-box systems, markedly less attention has been paid to exploring the uncertainty of these generated explanations. This becomes a critical issue in high-stakes scenarios, where uncertain and misleading explanations could have dire consequences (e.g., medical diagnosis and treatment planning). Moreover, it is often difficult to determine if the generated explanations are well grounded in the training data and sensitive to distributional shifts. This paper proposes several practical solutions that can be leveraged to solve these problems by establishing novel connections with other research works in explainability (e.g., trust scores) and uncertainty estimation (e.g., Monte Carlo Dropout). Two experiments demonstrate the utility of our proposed solutions.},
	urldate = {2022-01-27},
	journal = {arXiv:2107.09734},
	author = {Delaney, Eoin and Greene, Derek and Keane, Mark T.},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.09734},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Delaney et al_2021_Uncertainty Estimation and Out-of-Distribution Detection for Counterfactual.pdf:/Users/lucor/Zotero/storage/KFV6QFHM/Delaney et al_2021_Uncertainty Estimation and Out-of-Distribution Detection for Counterfactual.pdf:application/pdf},
}

@article{chen_robust_2021,
	title = {Robust {Out}-of-distribution {Detection} for {Neural} {Networks}},
	abstract = {Detecting out-of-distribution (OOD) inputs is critical for safely deploying deep learning models in the real world. Existing approaches for detecting OOD examples work well when evaluated on benign in-distribution and OOD samples. However, in this paper, we show that existing detection mechanisms can be extremely brittle when evaluating on in-distribution and OOD inputs with minimal adversarial perturbations which don't change their semantics. Formally, we extensively study the problem of Robust Out-of-Distribution Detection on common OOD detection approaches, and show that state-of-the-art OOD detectors can be easily fooled by adding small perturbations to the in-distribution and OOD inputs. To counteract these threats, we propose an effective algorithm called ALOE, which performs robust training by exposing the model to both adversarially crafted inlier and outlier examples. Our method can be flexibly combined with, and render existing methods robust. On common benchmark datasets, we show that ALOE substantially improves the robustness of state-of-the-art OOD detection, with 58.4\% AUROC improvement on CIFAR-10 and 46.59\% improvement on CIFAR-100.},
	journal = {arXiv:2003.09711},
	author = {Chen, Jiefeng and Li, Yixuan and Wu, Xi and Liang, Yingyu and Jha, Somesh},
	month = dec,
	year = {2021},
	note = {arXiv: 2003.09711},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Paper accepted by Workshop on Adversarial Machine Learning and Beyond at AAAI 2022},
	file = {Chen et al_2021_Robust Out-of-distribution Detection for Neural Networks.pdf:/Users/lucor/Zotero/storage/949TK3Z3/Chen et al_2021_Robust Out-of-distribution Detection for Neural Networks.pdf:application/pdf},
}

@article{kwon_uncertainty_2020,
	title = {Uncertainty quantification using {Bayesian} neural networks in classification: {Application} to biomedical image segmentation},
	volume = {142},
	issn = {0167-9473},
	shorttitle = {Uncertainty quantification using {Bayesian} neural networks in classification},
	url = {https://www.sciencedirect.com/science/article/pii/S016794731930163X},
	doi = {10.1016/j.csda.2019.106816},
	abstract = {Most recent research of deep neural networks in the field of computer vision has focused on improving performances of point predictions by developing network architectures or learning algorithms. Reliable uncertainty quantification accompanied by point estimation can lead to a more informed decision, and the quality of prediction can be improved. In this paper, we invoke a Bayesian neural network and propose a natural way of quantifying uncertainties in classification problems by decomposing the moment-based predictive uncertainty into two parts: aleatoric and epistemic uncertainty. The proposed method takes into account the discrete nature of the outcome, yielding the correct interpretation of each uncertainty. We demonstrate that the proposed uncertainty quantification method provides additional insights into the point prediction using two Ischemic Stroke Lesion Segmentation Challenge datasets and the Digital Retinal Images for Vessel Extraction dataset.},
	language = {en},
	urldate = {2022-01-27},
	journal = {Computational Statistics \& Data Analysis},
	author = {Kwon, Yongchan and Won, Joong-Ho and Kim, Beom Joon and Paik, Myunghee Cho},
	month = feb,
	year = {2020},
	keywords = {Uncertainty quantification, Aleatoric and epistemic uncertainty, Bayesian neural network, Ischemic stroke lesion segmentation, Retinal blood vessel segmentation},
	pages = {106816},
	file = {Kwon et al_2020_Uncertainty quantification using Bayesian neural networks in classification.pdf:/Users/lucor/Zotero/storage/FBSZQ7JJ/Kwon et al_2020_Uncertainty quantification using Bayesian neural networks in classification.pdf:application/pdf},
}

@article{oala_interval_2020,
	title = {Interval {Neural} {Networks}: {Uncertainty} {Scores}},
	shorttitle = {Interval {Neural} {Networks}},
	url = {http://arxiv.org/abs/2003.11566},
	abstract = {We propose a fast, non-Bayesian method for producing uncertainty scores in the output of pre-trained deep neural networks (DNNs) using a data-driven interval propagating network. This interval neural network (INN) has interval valued parameters and propagates its input using interval arithmetic. The INN produces sensible lower and upper bounds encompassing the ground truth. We provide theoretical justification for the validity of these bounds. Furthermore, its asymmetric uncertainty scores offer additional, directional information beyond what Gaussian-based, symmetric variance estimation can provide. We find that noise in the data is adequately captured by the intervals produced with our method. In numerical experiments on an image reconstruction task, we demonstrate the practical utility of INNs as a proxy for the prediction error in comparison to two state-of-the-art uncertainty quantification methods. In summary, INNs produce fast, theoretically justified uncertainty scores for DNNs that are easy to interpret, come with added information and pose as improved error proxies - features that may prove useful in advancing the usability of DNNs especially in sensitive applications such as health care.},
	urldate = {2022-01-25},
	journal = {arXiv:2003.11566 [cs, eess, stat]},
	author = {Oala, Luis and Heiß, Cosmas and Macdonald, Jan and März, Maximilian and Samek, Wojciech and Kutyniok, Gitta},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.11566},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, I.2.m, I.4.5, I.5.1, J.3},
	annote = {Comment: LO and CH contributed equally},
	file = {Oala et al_2020_Interval Neural Networks.pdf:/Users/lucor/Zotero/storage/KIU2DIJP/Oala et al_2020_Interval Neural Networks.pdf:application/pdf},
}

@article{ulmer_survey_2021,
	title = {A {Survey} on {Evidential} {Deep} {Learning} {For} {Single}-{Pass} {Uncertainty} {Estimation}},
	url = {http://arxiv.org/abs/2110.03051},
	abstract = {Popular approaches for quantifying predictive uncertainty in deep neural networks often involve a set of weights or models, for instance via ensembling or Monte Carlo Dropout. These techniques usually produce overhead by having to train multiple model instances or do not produce very diverse predictions. This survey aims to familiarize the reader with an alternative class of models based on the concept of Evidential Deep Learning: For unfamiliar data, they admit "what they don't know" and fall back onto a prior belief. Furthermore, they allow uncertainty estimation in a single model and forward pass by parameterizing distributions over distributions. This survey recapitulates existing works, focusing on the implementation in a classification setting. Finally, we survey the application of the same paradigm to regression problems. We also provide a reflection on the strengths and weaknesses of the mentioned approaches compared to existing ones and provide the most central theoretical results in order to inform future research.},
	journal = {arXiv:2110.03051},
	author = {Ulmer, Dennis},
	month = dec,
	year = {2021},
	note = {arXiv: 2110.03051
version: 2},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {Ulmer_2021_A Survey on Evidential Deep Learning For Single-Pass Uncertainty Estimation.pdf:/Users/lucor/Zotero/storage/5IQQGK7W/Ulmer_2021_A Survey on Evidential Deep Learning For Single-Pass Uncertainty Estimation.pdf:application/pdf},
}

@inproceedings{sensoy_evidential_2018,
	title = {Evidential {Deep} {Learning} to {Quantify} {Classification} {Uncertainty}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/a981f2b708044d6fb4a71a1463242520-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sensoy, Murat and Kaplan, Lance and Kandemir, Melih},
	year = {2018},
	file = {Sensoy et al_2018_Evidential Deep Learning to Quantify Classification Uncertainty.pdf:/Users/lucor/Zotero/storage/QB9F37JY/Sensoy et al_2018_Evidential Deep Learning to Quantify Classification Uncertainty.pdf:application/pdf;uncertainty.ipynb:/Users/lucor/Zotero/storage/5JB78I7L/uncertainty.ipynb:text/plain},
}

@article{lampinen_bayesian_2001,
	title = {Bayesian approach for neural networks—review and case studies},
	volume = {14},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608000000988},
	doi = {10.1016/S0893-6080(00)00098-8},
	abstract = {We give a short review on the Bayesian approach for neural network learning and demonstrate the advantages of the approach in three real applications. We discuss the Bayesian approach with emphasis on the role of prior knowledge in Bayesian models and in classical error minimization approaches. The generalization capability of a statistical model, classical or Bayesian, is ultimately based on the prior assumptions. The Bayesian approach permits propagation of uncertainty in quantities which are unknown to other assumptions in the model, which may be more generally valid or easier to guess in the problem. The case problem studied in this paper include a regression, a classification, and an inverse problem. In the most thoroughly analyzed regression problem, the best models were those with less restrictive priors. This emphasizes the major advantage of the Bayesian approach, that we are not forced to guess attributes that are unknown, such as the number of degrees of freedom in the model, non-linearity of the model with respect to each input variable, or the exact form for the distribution of the model residuals.},
	language = {en},
	number = {3},
	journal = {Neural Networks},
	author = {Lampinen, Jouko and Vehtari, Aki},
	month = apr,
	year = {2001},
	keywords = {Bayesian data analysis, Comparison of models, Hirarchical models, Neural networks},
	pages = {257--274},
	file = {Lampinen_Vehtari_2001_Bayesian approach for neural networks—review and case studies.pdf:/Users/lucor/Zotero/storage/LJ7JMUAP/Lampinen_Vehtari_2001_Bayesian approach for neural networks—review and case studies.pdf:application/pdf},
}

@inproceedings{malinin_predictive_2018,
	title = {Predictive {Uncertainty} {Estimation} via {Prior} {Networks}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/3ea2db50e62ceefceaf70a9d9a56a6f4-Abstract.html},
	urldate = {2022-01-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Malinin, Andrey and Gales, Mark},
	year = {2018},
	file = {Malinin_Gales_2018_Predictive Uncertainty Estimation via Prior Networks.pdf:/Users/lucor/Zotero/storage/MQ6LIV2E/Malinin_Gales_2018_Predictive Uncertainty Estimation via Prior Networks.pdf:application/pdf},
}

@inproceedings{rahaman_uncertainty_2021,
	title = {Uncertainty {Quantiﬁcation} and {Deep} {Ensembles}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/a70dc40477bc2adceef4d2c90f47eb82-Abstract.html},
	abstract = {Deep Learning methods are known to suffer from calibration issues: they typically produce over-conﬁdent estimates. These problems are exacerbated in the low data regime. Although the calibration of probabilistic models is well studied, calibrating extremely over-parametrized models in the low-data regime presents unique challenges. We show that deep-ensembles do not necessarily lead to improved calibration properties. In fact, we show that standard ensembling methods, when used in conjunction with modern techniques such as mixup regularization, can lead to less calibrated models. This text examines the interplay between three of the most simple and commonly used approaches to leverage deep learning when data is scarce: data-augmentation, ensembling, and post-processing calibration methods. Although standard ensembling techniques certainly help boost accuracy, we demonstrate that the calibration of deep ensembles relies on subtle trade-offs. We also ﬁnd that calibration methods such as temperature scaling need to be slightly tweaked when used with deep-ensembles and, crucially, need to be executed after the averaging process. Our simulations indicate that this simple strategy can halve the Expected Calibration Error (ECE) on a range of benchmark classiﬁcation problems compared to standard deep-ensembles in the low data regime.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Rahaman, Rahul},
	year = {2021},
	pages = {13},
	file = {Rahaman_2021_Uncertainty Quantiﬁcation and Deep Ensembles.pdf:/Users/lucor/Zotero/storage/2D9VXWIV/Rahaman_2021_Uncertainty Quantiﬁcation and Deep Ensembles.pdf:application/pdf},
}

@article{psaros_uncertainty_2022,
  title={Uncertainty quantification in scientific machine learning: Methods, metrics, and comparisons},
  author={Psaros, Apostolos F and Meng, Xuhui and Zou, Zongren and Guo, Ling and Karniadakis, George Em},
  journal={arXiv preprint arXiv:2201.07766},
  year={2022}
}



@inproceedings{ovadia_can_2019,
	title = {Can you trust your model' s uncertainty? {Evaluating} predictive uncertainty under dataset shift},
	volume = {32},
	shorttitle = {Can you trust your model' s uncertainty?},
	url = {https://proceedings.neurips.cc/paper/2019/hash/8558cb408c1d76621371888657d2eb1d-Abstract.html},
	urldate = {2021-12-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D. and Nowozin, Sebastian and Dillon, Joshua and Lakshminarayanan, Balaji and Snoek, Jasper},
	year = {2019},
	file = {Ovadia et al_2019_Can you trust your model' s uncertainty.pdf:/Users/lucor/Zotero/storage/YIWWAH56/Ovadia et al_2019_Can you trust your model' s uncertainty.pdf:application/pdf},
}

@phdthesis{thulasidasan_deep_2020,
	address = {Washington DC},
	type = {Thesis},
	title = {Deep {Learning} with {Abstention}: {Algorithms} for {Robust} {Training} and {Predictive} {Uncertainty}},
	copyright = {none},
	shorttitle = {Deep {Learning} with {Abstention}},
	url = {https://digital.lib.washington.edu:443/researchworks/handle/1773/45781},
	abstract = {Machine learning using deep neural networks -- also called ``Deep Learning'' --  has been at the center of practically every major advance in artificial intelligence over the last several years,  revolutionizing the fields of  computer vision, speech recognition and machine translation.  Spurred by these successes, there is now tremendous interest in using deep learning-based systems in all domains where computers can be used to make predictions after being trained on large quantities of data. In this thesis, we tackle two important practical challenges that arise when using deep neural networks (DNNs) for classification.  {\textbackslash}textit\{First, we study the problem of how to robustly train deep models when the training data itself is unreliable.\} This is a common  occurrence in real-world deep learning where large quantities of data can be easily collected, but due to the enormous size of the datasets required for deep learning, perfectly labeling them is usually infeasible; when such label noise is significant, the performance of the resulting classifier can be severely affected. To tackle this, we  devise a novel algorithmic framework for training deep models using an {\textbackslash}textit\{abstention\} based approach.  We show how such a ``deep abstaining classifier'' can improve robustness to different types of label noise, and unlike other existing approaches, also \{{\textbackslash}em learns features\} that are indicative of unreliable labels. State-of-the-art performance is achieved for noise-robust learning on a number of standard benchmarks; further gains on noise-robustness are then shown by combining abstention with techniques from classical control and semi-supervised learning. {\textbackslash}textit\{The second challenge we consider is improving the  predictive uncertainty of DNNs\}. In many applications, and especially in high-risk ones, it is critical to have a reliable measure of confidence in the model's prediction. DNNs, however, often exhibit pathological {\textbackslash}textit\{overconfidence\},  rendering standard methods of confidence-based thresholding ineffective. We first take a closer look at the roots of overconfidence in DNNs, discovering that introducing uncertainty into the training labels leads to significantly reduced overconfidence and improved probability estimates associated with predicted outcomes; using this observation, we show how threshold-based abstention can be made to work again. Then we consider the problem of open set detection -- where the classifier is presented with an instance from an unknown category -- and demonstrate how our abstention framework can be a very reliable open set detector. The contributions of this thesis are thus two-fold, and in two parts -- part one on robust training, and part two on predictive uncertainty -- but unified by a common theme: abstention. Our work demonstrates  that such an approach is highly effective both while training and deploying DNNs for classification, and hence, a useful addition to a real-world deep learning pipeline.},
	language = {en\_US},
	urldate = {2021-12-03},
	school = {University of Washington},
	author = {Thulasidasan, Sunil},
	year = {2020},
	file = {Thulasidasan_2020_Deep Learning with Abstention.pdf:/Users/lucor/Zotero/storage/L7B8P59C/Thulasidasan_2020_Deep Learning with Abstention.pdf:application/pdf},
}

@article{pedroni_computational_2022,
	title = {Computational methods for the robust optimization of the design of a dynamic aerospace system in the presence of aleatory and epistemic uncertainties},
	volume = {164},
	issn = {0888-3270},
	url = {https://www.sciencedirect.com/science/article/pii/S0888327021005811},
	doi = {10.1016/j.ymssp.2021.108206},
	abstract = {In this paper, we consider the computational model of a dynamic aerospace system and address the issues posed by the NASA Langley Uncertainty Quantification Challenge on Optimization Under Uncertainty, which comprises six tasks. Subproblem A deals with the model calibration and (aleatory and epistemic) uncertainty quantification of a subsystem by means of a limited number of observations. A simple, two-step approach based on Maximum Likelihood Estimation (MLE) is proposed to address this task. Subproblem B requires the identification and ranking of those (epistemic) parameters that are more effective in improving the predictive ability of the computational model of the subsystem. Two approaches are compared: the first is based on a sensitivity analysis within a factor prioritization setting, whereas the second employs the Energy Score (ES) as a multivariate generalization of the Continuous Rank Predictive Score (CRPS). Since the output of the subsystem is a function of time, both subproblems are addressed in the space defined by the orthonormal bases resulting from a Singular Value Decomposition (SVD) of the subsystem observations. Subproblem C requires identifying the (epistemic) reliability (resp., failure probability) bounds of a given system design. The issue is addressed by an efficient combination of: (i) Monte Carlo Simulation (MCS) to propagate the aleatory uncertainty described by probability distributions; (ii) Genetic Algorithms (GAs) to solve the optimization problems related to the propagation of epistemic uncertainty by interval analysis; and (iii) fast-running Artificial Neural Networks (ANNs) to reduce the computational time related to the repeated model evaluations. In Subproblem D, system reliability is improved by identifying a new design point within an iterative robust optimization framework. In Subproblem E both the uncertainty model and the design obtained are tuned using additional data. Finally, a risk-based design is carried out in Subproblem F by neglecting “outliers” (i.e., less likely values of some epistemic parameters) in the design optimization.},
	language = {en},
	journal = {Mechanical Systems and Signal Processing},
	author = {Pedroni, Nicola},
	month = feb,
	year = {2022},
	keywords = {Design optimization, Energy score, Model calibration, Sensitivity analysis, Singular value decomposition, Uncertainty propagation},
	pages = {108206},
	file = {Pedroni_2022_Computational methods for the robust optimization of the design of a dynamic.pdf:/Users/lucor/Zotero/storage/BI4BSL5L/Pedroni_2022_Computational methods for the robust optimization of the design of a dynamic.pdf:application/pdf},
}

@inproceedings{si_autoregressive_2022,
	title = {Autoregressive {Quantile} {Flows} for {Predictive} {Uncertainty} {Estimation}},
	url = {https://openreview.net/forum?id=z1-I6rOKv1S},
	abstract = {Numerous applications of machine learning involve representing probability distributions over high-dimensional data. We propose autoregressive quantile flows, a flexible class of normalizing flow...},
	language = {en},
	urldate = {2022-04-11},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Si, Phillip and Bishop, Allan and Kuleshov, Volodymyr},
	year = {2022},
	file = {Si et al_2022_Autoregressive Quantile Flows for Predictive Uncertainty Estimation.pdf:/Users/lucor/Zotero/storage/QXSBR2VM/Si et al_2022_Autoregressive Quantile Flows for Predictive Uncertainty Estimation.pdf:application/pdf},
}

@article{kendall_bayesian_2016,
	title = {Bayesian {SegNet}: {Model} {Uncertainty} in {Deep} {Convolutional} {Encoder}-{Decoder} {Architectures} for {Scene} {Understanding}},
	shorttitle = {Bayesian {SegNet}},
	abstract = {We present a deep learning framework for probabilistic pixel-wise semantic segmentation, which we term Bayesian SegNet. Semantic segmentation is an important tool for visual scene understanding and a meaningful measure of uncertainty is essential for decision making. Our contribution is a practical system which is able to predict pixel-wise class labels with a measure of model uncertainty. We achieve this by Monte Carlo sampling with dropout at test time to generate a posterior distribution of pixel class labels. In addition, we show that modelling uncertainty improves segmentation performance by 2-3\% across a number of state of the art architectures such as SegNet, FCN and Dilation Network, with no additional parametrisation. We also observe a significant improvement in performance for smaller datasets where modelling uncertainty is more effective. We benchmark Bayesian SegNet on the indoor SUN Scene Understanding and outdoor CamVid driving scenes datasets.},
	journal = {arXiv:1511.02680},
	author = {Kendall, Alex and Badrinarayanan, Vijay and Cipolla, Roberto},
	month = oct,
	year = {2016},
	note = {arXiv: 1511.02680},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {Kendall et al_2016_Bayesian SegNet.pdf:/Users/lucor/Zotero/storage/FEJBZH9D/Kendall et al_2016_Bayesian SegNet.pdf:application/pdf},
}

@article{bomarito_automated_2022,
	title = {Automated {Learning} of {Interpretable} {Models} with {Quantified} {Uncertainty}},
	url = {http://arxiv.org/abs/2205.01626},
	abstract = {Interpretability and uncertainty quantification in machine learning can provide justification for decisions, promote scientific discovery and lead to a better understanding of model behavior. Symbolic regression provides inherently interpretable machine learning, but relatively little work has focused on the use of symbolic regression on noisy data and the accompanying necessity to quantify uncertainty. A new Bayesian framework for genetic-programming-based symbolic regression (GPSR) is introduced that uses model evidence (i.e., marginal likelihood) to formulate replacement probability during the selection phase of evolution. Model parameter uncertainty is automatically quantified, enabling probabilistic predictions with each equation produced by the GPSR algorithm. Model evidence is also quantified in this process, and its use is shown to increase interpretability, improve robustness to noise, and reduce overfitting when compared to a conventional GPSR implementation on both numerical and physical experiments.},
	urldate = {2022-05-07},
	journal = {arXiv:2205.01626},
	author = {Bomarito, G. F. and Leser, P. E. and Strauss, N. C. M. and Garbrecht, K. M. and Hochhalter, J. D.},
	month = apr,
	year = {2022},
	note = {arXiv: 2205.01626},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Bomarito et al_2022_Automated Learning of Interpretable Models with Quantified Uncertainty.pdf:/Users/lucor/Zotero/storage/JFYAKLAL/Bomarito et al_2022_Automated Learning of Interpretable Models with Quantified Uncertainty.pdf:application/pdf},
}

@article{heiss_nomu_2021,
	title = {{NOMU}: {Neural} {Optimization}-based {Model} {Uncertainty}},
	shorttitle = {{NOMU}},
	url = {http://arxiv.org/abs/2102.13640},
	abstract = {We study methods for estimating model uncertainty for neural networks (NNs). To isolate the effect of model uncertainty, we focus on a noiseless setting with scarce training data. We introduce five important desiderata regarding model uncertainty that any method should satisfy. However, we find that established benchmarks often fail to reliably capture some of these desiderata, even those that are required by Bayesian theory. To address this, we introduce a new approach for capturing model uncertainty for NNs, which we call Neural Optimization-based Model Uncertainty (NOMU). The main idea of NOMU is to design a network architecture consisting of two connected sub-NNs, one for model prediction and one for model uncertainty, and to train it using a carefully-designed loss function. Importantly, our design enforces that NOMU satisfies our five desiderata. Due to its modular architecture, NOMU can provide model uncertainty for any given (previously trained) NN if given access to its training data. We first experimentally study noiseless regression with scarce training data to highlight the deficiencies of the established benchmarks. Finally, we study the important task of Bayesian optimization (BO) with costly evaluations, where good model uncertainty estimates are essential. Our results show that NOMU performs as well or better than state-of-the-art benchmarks.},
	urldate = {2022-05-03},
	journal = {arXiv:2102.13640 [cs, stat]},
	author = {Heiss, Jakob and Weissteiner, Jakob and Wutte, Hanna and Seuken, Sven and Teichmann, Josef},
	month = may,
	year = {2021},
	note = {arXiv: 2102.13640},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {Heiss et al_2021_NOMU.pdf:/Users/lucor/Zotero/storage/TWTEZRRB/Heiss et al_2021_NOMU.pdf:application/pdf},
}

@article{mena_survey_2021,
	title = {A {Survey} on {Uncertainty} {Estimation} in {Deep} {Learning} {Classification} {Systems} from a {Bayesian} {Perspective}},
	volume = {54},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3477140},
	doi = {10.1145/3477140},
	abstract = {Decision-making based on machine learning systems, especially when this decision-making can affect human lives, is a subject of maximum interest in the Machine Learning community. It is, therefore, necessary to equip these systems with a means of estimating uncertainty in the predictions they emit in order to help practitioners make more informed decisions. In the present work, we introduce the topic of uncertainty estimation, and we analyze the peculiarities of such estimation when applied to classification systems. We analyze different methods that have been designed to provide classification systems based on deep learning with mechanisms for measuring the uncertainty of their predictions. We will take a look at how this uncertainty can be modeled and measured using different approaches, as well as practical considerations of different applications of uncertainty. Moreover, we review some of the properties that should be borne in mind when developing such metrics. All in all, the present survey aims at providing a pragmatic overview of the estimation of uncertainty in classification systems that can be very useful for both academic research and deep learning practitioners.},
	number = {9},
	urldate = {2022-07-26},
	journal = {ACM Comput. Surv.},
	author = {Mena, José and Pujol, Oriol and Vitrià, Jordi},
	month = oct,
	year = {2021},
	keywords = {deep learning, Classification, machine learning, uncertainty},
	pages = {193:1--193:35},
	file = {Mena et al_2021_A Survey on Uncertainty Estimation in Deep Learning Classification Systems from.pdf:/Users/lucor/Zotero/storage/VA6WEMGW/Mena et al_2021_A Survey on Uncertainty Estimation in Deep Learning Classification Systems from.pdf:application/pdf},
}

@misc{staber_quantitative_2022,
	title = {Quantitative performance evaluation of {Bayesian} neural networks},
	url = {http://arxiv.org/abs/2206.06779},
	doi = {10.48550/arXiv.2206.06779},
	publisher = {arXiv},
	author = {Staber, Brian and da Veiga, Sébastien},
	year = {2022},
	note = {arXiv:2206.06779},
	keywords = {Computer Science - Machine Learning},
	file = {Staber_da Veiga_2022_Quantitative performance evaluation of Bayesian neural networks.pdf:/Users/lucor/Zotero/storage/EG69ADG6/Staber_da Veiga_2022_Quantitative performance evaluation of Bayesian neural networks.pdf:application/pdf},
}

@misc{osband_epistemic_2022,
	title = {Epistemic {Neural} {Networks}},
	url = {http://arxiv.org/abs/2107.08924},
	doi = {10.48550/arXiv.2107.08924},
	abstract = {Intelligence relies on an agent's knowledge of what it does not know. This capability can be assessed based on the quality of joint predictions of labels across multiple inputs. Conventional neural networks lack this capability and, since most research has focused on marginal predictions, this shortcoming has been largely overlooked. We introduce the epistemic neural network (ENN) as an interface for models that represent uncertainty as required to generate useful joint predictions. While prior approaches to uncertainty modeling such as Bayesian neural networks can be expressed as ENNs, this new interface facilitates comparison of joint predictions and the design of novel architectures and algorithms. In particular, we introduce the epinet: an architecture that can supplement any conventional neural network, including large pretrained models, and can be trained with modest incremental computation to estimate uncertainty. With an epinet, conventional neural networks outperform very large ensembles, consisting of hundreds or more particles, with orders of magnitude less computation. We demonstrate this efficacy across synthetic data, ImageNet, and some reinforcement learning tasks. As part of this effort we open-source experiment code.},
	urldate = {2022-07-09},
	publisher = {arxiv},
	author = {Osband, Ian and Wen, Zheng and Asghari, Seyed Mohammad and Dwaracherla, Vikranth and Ibrahimi, Morteza and Lu, Xiuyuan and Van Roy, Benjamin},
	month = jul,
	year = {2022},
	note = {arXiv:2107.08924 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {Osband et al_2022_Epistemic Neural Networks.pdf:/Users/lucor/Zotero/storage/B4UAX5ZY/Osband et al_2022_Epistemic Neural Networks.pdf:application/pdf},
}

@article{gao_gaussian_2020,
	title = {Gaussian mixture model fitting method for uncertainty quantification by conditioning to production data},
	volume = {24},
	issn = {1420-0597, 1573-1499},
	url = {http://link.springer.com/10.1007/s10596-019-9823-3},
	doi = {10.1007/s10596-019-9823-3},
	language = {en},
	number = {2},
	urldate = {2022-08-16},
	journal = {Comput Geosci},
	author = {Gao, Guohua and Jiang, Hao and Vink, Jeroen C. and Chen, Chaohui and El Khamra, Yaakoub and Ita, Joel J.},
	month = apr,
	year = {2020},
	pages = {663--681},
}

@article{abbasi_physics-informed_2022,
	title = {Physics-{Informed} {Machine} {Learning} for {Uncertainty} {Reduction} in {Time} {Response} {Reconstruction} of a {Dynamic} {System}},
	volume = {26},
	issn = {1089-7801, 1941-0131},
	url = {https://ieeexplore.ieee.org/document/9764364/},
	doi = {10.1109/MIC.2022.3170736},
	number = {4},
	urldate = {2022-08-30},
	journal = {IEEE Internet Comput.},
	author = {Abbasi, Amirhassan and Nataraj, C.},
	month = jul,
	year = {2022},
	pages = {35--44},
	file = {Abbasi_Nataraj_2022_Physics-Informed Machine Learning for Uncertainty Reduction in Time Response.pdf:/Users/lucor/Zotero/storage/URSDQQVY/Abbasi_Nataraj_2022_Physics-Informed Machine Learning for Uncertainty Reduction in Time Response.pdf:application/pdf},
}

@misc{sun_bayesian_2022,
	title = {Bayesian {Spline} {Learning} for {Equation} {Discovery} of {Nonlinear} {Dynamics} with {Quantified} {Uncertainty}},
	url = {http://arxiv.org/abs/2210.08095},
	abstract = {Nonlinear dynamics are ubiquitous in science and engineering applications, but the physics of most complex systems is far from being fully understood. Discovering interpretable governing equations from measurement data can help us understand and predict the behavior of complex dynamic systems. Although extensive work has recently been done in this field, robustly distilling explicit model forms from very sparse data with considerable noise remains intractable. Moreover, quantifying and propagating the uncertainty of the identified system from noisy data is challenging, and relevant literature is still limited. To bridge this gap, we develop a novel Bayesian spline learning framework to identify parsimonious governing equations of nonlinear (spatio)temporal dynamics from sparse, noisy data with quantified uncertainty. The proposed method utilizes spline basis to handle the data scarcity and measurement noise, upon which a group of derivatives can be accurately computed to form a library of candidate model terms. The equation residuals are used to inform the spline learning in a Bayesian manner, where approximate Bayesian uncertainty calibration techniques are employed to approximate posterior distributions of the trainable parameters. To promote the sparsity, an iterative sequential-threshold Bayesian learning approach is developed, using the alternative direction optimization strategy to systematically approximate L0 sparsity constraints. The proposed algorithm is evaluated on multiple nonlinear dynamical systems governed by canonical ordinary and partial differential equations, and the merit/superiority of the proposed method is demonstrated by comparison with state-of-the-art methods.},
	urldate = {2022-10-20},
	publisher = {arXiv},
	author = {Sun, Luning and Huang, Daniel Zhengyu and Sun, Hao and Wang, Jian-Xun},
	month = oct,
	year = {2022},
	note = {arXiv:2210.08095 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: 28 pages, 11 figures},
	file = {arXiv.org Snapshot:/Users/lucor/Zotero/storage/EGNK4B44/2210.html:text/html;Sun et al_2022_Bayesian Spline Learning for Equation Discovery of Nonlinear Dynamics with.pdf:/Users/lucor/Zotero/storage/VZ84ZHUG/Sun et al_2022_Bayesian Spline Learning for Equation Discovery of Nonlinear Dynamics with.pdf:application/pdf},
}

@misc{jung_bayesian_2022,
	title = {Bayesian deep learning framework for uncertainty quantification in high dimensions},
	url = {http://arxiv.org/abs/2210.11737},
	doi = {10.48550/arXiv.2210.11737},
	abstract = {We develop a novel deep learning method for uncertainty quantification in stochastic partial differential equations based on Bayesian neural network (BNN) and Hamiltonian Monte Carlo (HMC). A BNN efficiently learns the posterior distribution of the parameters in deep neural networks by performing Bayesian inference on the network parameters. The posterior distribution is efficiently sampled using HMC to quantify uncertainties in the system. Several numerical examples are shown for both forward and inverse problems in high dimension to demonstrate the effectiveness of the proposed method for uncertainty quantification. These also show promising results that the computational cost is almost independent of the dimension of the problem demonstrating the potential of the method for tackling the so-called curse of dimensionality.},
	urldate = {2022-10-24},
	publisher = {arXiv},
	author = {Jung, Jeahan and Choi, Minseok},
	month = oct,
	year = {2022},
	note = {arXiv:2210.11737 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {Jung_Choi_2022_Bayesian deep learning framework for uncertainty quantification in high.pdf:/Users/lucor/Zotero/storage/A9XN8LAB/Jung_Choi_2022_Bayesian deep learning framework for uncertainty quantification in high.pdf:application/pdf},
}

@article{tabarisaadi_optimized_2022,
	title = {An {Optimized} {Uncertainty}-{Aware} {Training} {Framework} for {Neural} {Networks}},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2022.3213315},
	abstract = {Uncertainty quantification (UQ) for predictions generated by neural networks (NNs) is of vital importance in safety-critical applications. An ideal model is supposed to generate low uncertainty for correct predictions and high uncertainty for incorrect predictions. The main focus of state-of-the-art training algorithms is to optimize the NN parameters to improve the accuracy-related metrics. Training based on uncertainty metrics has been fully ignored or overlooked in the literature. This article introduces a novel uncertainty-aware training algorithm for classification tasks. A novel predictive uncertainty estimate-based objective function is defined and optimized using the stochastic gradient descent method. This new multiobjective loss function covers both accuracy and uncertainty accuracy (UA) simultaneously during training. The performance of the proposed training framework is compared from different aspects with other UQ techniques for different benchmarks. The obtained results demonstrate the effectiveness of the proposed framework for developing the NN models capable of generating reliable uncertainty estimates.},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Tabarisaadi, Pegah and Khosravi, Abbas and Nahavandi, Saeid and Shafie-Khah, Miadreza and Catalão, João P. S.},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Classification, Artificial neural networks, Bayes methods, deep neural network (NN), Measurement, Prediction algorithms, Predictive models, Training, Uncertainty, uncertainty accuracy (UA), uncertainty quantification (UQ)},
	pages = {1--8},
	file = {Tabarisaadi et al_2022_An Optimized Uncertainty-Aware Training Framework for Neural Networks.pdf:/Users/lucor/Zotero/storage/M357IUNH/Tabarisaadi et al_2022_An Optimized Uncertainty-Aware Training Framework for Neural Networks.pdf:application/pdf},
}

@article{hullermeier_aleatoric_2021,
	title = {Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods},
	volume = {110},
	issn = {0885-6125, 1573-0565},
	shorttitle = {Aleatoric and epistemic uncertainty in machine learning},
	doi = {10.1007/s10994-021-05946-3},
	abstract = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.},
	language = {en},
	number = {3},
	urldate = {2022-11-17},
	journal = {Mach Learn},
	author = {Hüllermeier, Eyke and Waegeman, Willem},
	month = mar,
	year = {2021},
	pages = {457--506}
}

@article{kabir_neural_2018,
	title = {Neural {Network}-{Based} {Uncertainty} {Quantification}: {A} {Survey} of {Methodologies} and {Applications}},
	volume = {6},
	issn = {2169-3536},
	shorttitle = {Neural {Network}-{Based} {Uncertainty} {Quantification}},
	url = {https://ieeexplore.ieee.org/document/8371683/},
	doi = {10.1109/ACCESS.2018.2836917},
	abstract = {Uncertainty quantiﬁcation plays a critical role in the process of decision making and optimization in many ﬁelds of science and engineering. The ﬁeld has gained an overwhelming attention among researchers in recent years resulting in an arsenal of different methods. Probabilistic forecasting and in particular prediction intervals (PIs) are one of the techniques most widely used in the literature for uncertainty quantiﬁcation. Researchers have reported studies of uncertainty quantiﬁcation in critical applications such as medical diagnostics, bioinformatics, renewable energies, and power grids. The purpose of this survey paper is to comprehensively study neural network-based methods for construction of prediction intervals. It will cover how PIs are constructed, optimized, and applied for decision-making in presence of uncertainties. Also, different criteria for unbiased PI evaluation are investigated. The paper also provides some guidelines for further research in the ﬁeld of neural network-based uncertainty quantiﬁcation.},
	language = {en},
	urldate = {2022-11-17},
	journal = {IEEE Access},
	author = {Kabir, H. M. Dipu and Khosravi, Abbas and Hosen, Mohammad Anwar and Nahavandi, Saeid},
	year = {2018},
	pages = {36218--36234},
	file = {Kabir et al. - 2018 - Neural Network-Based Uncertainty Quantification A.pdf:/Users/lucor/Zotero/storage/NFZS7CZM/Kabir et al. - 2018 - Neural Network-Based Uncertainty Quantification A.pdf:application/pdf},
}

@incollection{brooks_mcmc_2011,
	title = {{MCMC} {Using} {Hamiltonian} {Dynamics}},
	volume = {20116022},
	isbn = {978-1-4200-7941-8 978-1-4200-7942-5},
	language = {en},
	urldate = {2022-11-17},
	booktitle = {Handbook of {Markov} {Chain} {Monte} {Carlo}},
	publisher = {Chapman and Hall/CRC},
	author = {Neal, Radford},
	editor = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
	month = may,
	year = {2011},
	note = {Series Title: Chapman \& Hall/CRC Handbooks of Modern Statistical Methods},
	file = {Neal - 2011 - MCMC Using Hamiltonian Dynamics.pdf:/Users/lucor/Zotero/storage/PE9FTPZX/Neal - 2011 - MCMC Using Hamiltonian Dynamics.pdf:application/pdf},
}

@inproceedings{cobb_scaling_nodate,
  title={Scaling Hamiltonian Monte Carlo inference for Bayesian neural networks with symmetric splitting},
  author={Cobb, Adam D and Jalaian, Brian},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={675--685},
  year={2021},
  organization={PMLR}
}



@article{nemeth_stochastic_2021,
  title={Stochastic gradient markov chain monte carlo},
  author={Nemeth, Christopher and Fearnhead, Paul},
  journal={Journal of the American Statistical Association},
  volume={116},
  number={533},
  pages={433--450},
  year={2021},
  publisher={Taylor \& Francis}
}


@inproceedings{welling_bayesian_nodate,
  title={Bayesian learning via stochastic gradient Langevin dynamics},
  author={Welling, Max and Teh, Yee W},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={681--688},
  year={2011}
}




@article{abdar2021review,
  title={A review of uncertainty quantification in deep learning: Techniques, applications and challenges},
  author={Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U Rajendra and others},
  journal={Information Fusion},
  volume={76},
  pages={243--297},
  year={2021},
  publisher={Elsevier}
}


@article{owhadi2013optimal,
  title={Optimal uncertainty quantification},
  author={Owhadi, Houman and Scovel, Clint and Sullivan, Timothy John and McKerns, Mike and Ortiz, Michael},
  journal={Siam Review},
  volume={55},
  number={2},
  pages={271--345},
  year={2013},
  publisher={SIAM}
}

@article{berkooz1993proper,
  title={The proper orthogonal decomposition in the analysis of turbulent flows},
  author={Berkooz, Gal and Holmes, Philip and Lumley, John L},
  journal={Annual review of fluid mechanics},
  volume={25},
  number={1},
  pages={539--575},
  year={1993},
  publisher={Annual Reviews 4139 El Camino Way, PO Box 10139, Palo Alto, CA 94303-0139, USA}
}

@article{hinton2006reducing,
  title={Reducing the dimensionality of data with neural networks},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  journal={science},
  volume={313},
  number={5786},
  pages={504--507},
  year={2006},
  publisher={American Association for the Advancement of Science}
}

@book{shogren2013encyclopedia,
  title={Encyclopedia of energy, natural resource, and environmental economics},
  author={Shogren, Jason},
  year={2013},
  publisher={Newnes}
}




@inproceedings{gal:16,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={International Conference on Machine Learning},
  pages={1050--1059},
  year={2016},
}

@inproceedings{blundell:15,
  title={Weight uncertainty in neural network},
  author={Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  booktitle={International Conference on Machine Learning},
  pages={1613--1622},
  year={2015},
}

@inproceedings{lakshminarayanan:17,
 author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 title = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
 volume = {30},
 year = {2017}
}

@article{hullermeier:21,
  title={Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods},
  author={H{\"u}llermeier, Eyke and Waegeman, Willem},
  journal={Machine Learning},
  volume={110},
  number={3},
  pages={457--506},
  year={2021},
}



@inproceedings{ashukha:20,
    title={Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning},
    author={Arsenii Ashukha and Alexander Lyzhov and Dmitry Molchanov and Dmitry Vetrov},
    booktitle={International Conference on Learning Representations},
    year={2020},
}

@article{fort:19,
  title={Deep ensembles: A loss landscape perspective},
  author={Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
  journal={arXiv:1912.02757},
  year={2019}
}

@article{rahaman:20,
    author={Rahaman, Rahul and Thiery, Alexandre H},
    title={Uncertainty quantification and deep ensembles},
    journal={arXiv:2007.08792},
    year={2020}
}

@inproceedings{antoran:2021,
    author      = {Javier Antoran and Umang Bhatt and Tameem Adel and Adrian Weller and Jos{\'e} Miguel Hern{\'a}ndez-Lobato},
    title       = {Getting a {CLUE}: a method for explaining uncertainty estimates},
    booktitle   = {International Conference on Learning Representations},
    year        = {2021},
}

@article{gilpin:18,
    author  = {Gilpin, LH and Bau, D and Yuan, BZ and Bajwa, A and Specter, M and Kagal, L},
    title   = {Explaining explanations: An approach to evaluating interpretability of {ML}},
    journal = {arXiv:1806.00069},
    year    = {2018}
}



@article{mezic2008uncertainty,
  title={Uncertainty propagation in dynamical systems},
  author={Mezi{\'c}, Igor and Runolfsson, Thordur},
  journal={Automatica},
  volume={44},
  number={12},
  pages={3003--3013},
  year={2008},
  publisher={Elsevier}
}





@BOOK{davgam21,
        AUTHOR             = {S. {Da~Veiga} and F. Gamboa and B. Iooss and C. Prieur},
        PUBLISHER          = {SIAM},
        TITLE              = {Basics and Trends in Sensitivity Analysis. Theory and Practice in R},
        YEAR               = {2021}
}

@BOOK{ghahig17,
        EDITOR             = {R. Ghanem and D. Higdon and H. Owhadi},
        PUBLISHER          = {Springer},
        title          = {Springer Handbook on {Uncertainty Quantification}},
        YEAR               = {2017}
}

@BOOK{saltar04,
        AUTHOR             = {A. Saltelli and S. Tarantola and F. Campolongo and M. Ratto},
        KEY                = {saltar04},
        PUBLISHER          = {Wiley},
        TITLE              = {Sensitivity analysis in practice: A guide to assessing scientific models},
        YEAR               = {2004}
}

@INPROCEEDINGS{iooken22,
        AUTHOR             = {B. Iooss and R. Kennet and P. Secchi},
        TITLE              = {Different views of interpretability},
        PUBLISHER          = {Springer},
	    EDITOR		   = {A. Lepore and B. Palumbo and J-M. Poggi},
        BOOKTITLE          = {Interpretability for Industry 4.0: Statistical and Machine Learning Approaches},
        YEAR               = {2022}
}

@BOOK{mol22,
        AUTHOR             = {C. Molnar},
        PUBLISHER          = {github},
        URL = {https://christophm.github.io/interpretable-ml-book/},
        TITLE              = {Interpretable machine learning: {A} guide for making black-box models explainable (2nd ed.)},
        YEAR               = {2022}
} 

@Article{ilibou22,
  author = 	 {M. {Il Idrissi} and N. Bousquet and F. Gamboa and B. Iooss and J-M. Loubès},
  title = 	 {Quantile-constrained Wasserstein projections for robust interpretability analyses of numerical and machine learning models},
	journal = {Preprint, arXiv:2209.11539}, 
  year={2022}
}


@article{rihan2003sensitivity,
  title={Sensitivity analysis for dynamic systems with time-lags},
  author={Rihan, Fathalla A},
  journal={Journal of Computational and Applied Mathematics},
  volume={151},
  number={2},
  pages={445--462},
  year={2003},
  publisher={Elsevier}
}

@book{cacuci2005sensitivity,
  title={Sensitivity and uncertainty analysis, volume II: applications to large-scale systems},
  author={Cacuci, Dan G and Ionescu-Bujor, Mihaela and Navon, Ionel Michael},
  year={2005},
  publisher={CRC press}
}

@article{d2013uncertainty,
  title={Uncertainty quantification for data assimilation in a steady incompressible Navier-Stokes problem},
  author={D’Elia, Marta and Veneziani, Alessandro},
  journal={ESAIM: Mathematical Modelling and Numerical Analysis},
  volume={47},
  number={4},
  pages={1037--1057},
  year={2013},
  publisher={EDP Sciences}
}


@article{binev2017data,
  title={Data assimilation in reduced modeling},
  author={Binev, Peter and Cohen, Albert and Dahmen, Wolfgang and DeVore, Ronald and Petrova, Guergana and Wojtaszczyk, Przemyslaw},
  journal={SIAM/ASA Journal on Uncertainty Quantification},
  volume={5},
  number={1},
  pages={1--29},
  year={2017},
  publisher={SIAM}
}

@article{phillips2020autoencoder,
  title={An autoencoder-based reduced-order model for eigenvalue problems with application to neutron diffusion},
  author={Phillips, Toby RF and Heaney, Claire E and Smith, Paul N and Pain, Christopher C},
  journal={International Journal for Numerical Methods in Engineering},
  volume={122},
  number={15},
  pages={3780--3811},
  year={2021},
  publisher={Wiley Online Library}
}

@inproceedings{he_deep_2015-1,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@inproceedings{krishnan_structured_2016,
  title={Structured inference networks for nonlinear state space models},
  author={Krishnan, Rahul and Shalit, Uri and Sontag, David},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

@article{nguyen2019emlike,
  title={Em-like learning chaotic dynamics from noisy and partial observations},
  author={Nguyen, Duong and Ouala, Said and Drumetz, Lucas and Fablet, Ronan},
  journal={arXiv preprint arXiv:1903.10335},
  year={2019}
}

@BOOK{smith2014,
        AUTHOR             = {R.C. Smith},
        PUBLISHER          = {SIAM},
        TITLE              = {Uncertainty quantification},
        YEAR               = {2014}
}

@BOOK{sullivan2015,
        AUTHOR             = {T.J. Sullivan},
        PUBLISHER          = {Springer},
        TITLE              = {Interoduction to uncertainty quantification},
        YEAR               = {2015}
}

@article{xiao2018parameterised,
  title={Parameterised non-intrusive reduced order methods for ensemble Kalman filter data assimilation},
  author={Xiao, D and Du, J and Fang, F and Pain, CC and Li, J},
  journal={Computers \& Fluids},
  volume={177},
  pages={69--77},
  year={2018},
  publisher={Elsevier}
}


@BOOK{hastib09,
        AUTHOR             = {T. Hastie and R. Tibshirani and J. Friedman},
        PUBLISHER          = {Springer},
				EDITION = {Second},
        TITLE              = {The elements of statistical learning},
        YEAR               = {2009}
}

@TechReport{dee20,
    title = {{Machine Learning in Certified Systems}},
    year = {2020},
    author = {{DEEL Certification Workgoup}},
    institution = {DEpendable \& Explainable Learning (DEEL), IRT Saint Exup\'ery},
    note = {Report No. {S079L03T00-005}}
}

@BOOK{shaben14,
        AUTHOR             = {S. Shalev-Shwartz and S. Ben-David},
        PUBLISHER          = {Cambridge University Press},
        TITLE              = {Understanding machine learning: {F}rom theory to algorithms},
        YEAR               = {2014}
}

@article{menhoo16,
  title={Quantifying Uncertainty in Random Forests via Confidence Intervals and Hypothesis Tests},
  author={L. Mentch ad G. Hooker},
  journal={Journal of Machine Learning Research},
  volume={17},
  pages={1-41},
  year={2016}
}

@InProceedings{legmar17,
  author =	 {L. {Le Gratiet} and S. Marelli and B. Sudret},
  title = 	 {Metamodel-Based Sensitivity Analysis: {P}olynomial Chaos Expansions and {G}aussian Processes},
  editor =	 {R. Ghanem and D. Higdon and H. Owhadi},
  publisher =    {Springer},
  booktitle = 	 {Springer Handbook on {Uncertainty Quantification}},
	pages = {1289--1325},
  year = 	 {2017}
}

@article{demioo21,
  author = 	 {C. Demay and B. Iooss and L. Le Gratiet and A. Marrel},
  title = 	 {Model selection for {G}aussian {P}rocess regression: an application with highlights on the model variance validation},
  journal = {Quality and Reliability Engineering International Journal},
  year = 	 {2021}
}

@article{mei09,
  title={Forest {G}arotte},
  author={N. Meinshausen},
  journal={Electronic Journal of Statistics},
  volume={3},
  pages={1288–1304},
  year={2009}
}

@article{shavov08,
  title={A Tutorial on Conformal Prediction},
  author={G. Shafer and V. Vovk},
  journal={Journal of Machine Learning Research},
  volume={9},
  pages={371-421},
  year={2008}
}
G. Blatman, , Université Clermont II


@BOOK{bla09,
        AUTHOR             = {G. Blatman},
        TITLE              = {Adaptive sparse polynomial chaos expansions for uncertainty propagation and sensitivity analysis},
        PUBLISHER          = {PhD Thesis of Blaise Pascal - Clermont II University},
        YEAR               = {2009}
}

@article{jospin2022hands,
  title={Hands-on Bayesian neural networks—A tutorial for deep learning users},
  author={Jospin, Laurent Valentin and Laga, Hamid and Boussaid, Farid and Buntine, Wray and Bennamoun, Mohammed},
  journal={IEEE Computational Intelligence Magazine},
  volume={17},
  number={2},
  pages={29--48},
  year={2022},
  publisher={IEEE}
}

@inproceedings{gal2016dropout,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016},
  organization={PMLR}
}

@article{eltoft2006multivariate,
  title={On the multivariate Laplace distribution},
  author={Eltoft, Torbj{\o}rn and Kim, Taesu and Lee, Te-Won},
  journal={IEEE Signal Processing Letters},
  volume={13},
  number={5},
  pages={300--303},
  year={2006},
  publisher={IEEE}
}

@article{lakshminarayanan2017simple,
  title={Simple and scalable predictive uncertainty estimation using deep ensembles},
  author={Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{fasel2022ensemble,
  title={Ensemble-SINDy: Robust sparse model discovery in the low-data, high-noise limit, with active learning and control},
  author={Fasel, Urban and Kutz, J Nathan and Brunton, Bingni W and Brunton, Steven L},
  journal={Proceedings of the Royal Society A},
  volume={478},
  number={2260},
  pages={20210904},
  year={2022},
  publisher={The Royal Society}
}

@article{brunton2016sparse,
  title={Sparse identification of nonlinear dynamics with control (SINDYc)},
  author={Brunton, Steven L and Proctor, Joshua L and Kutz, J Nathan},
  journal={IFAC-PapersOnLine},
  volume={49},
  number={18},
  pages={710--715},
  year={2016},
  publisher={Elsevier}
}

@article{carnerero2022state,
  title={State-Space Kriging: A Data-Driven Method to Forecast Nonlinear Dynamical Systems},
  author={Carnerero, A Daniel and Ramirez, Daniel R and Alamo, Teodoro},
  journal={IEEE Control Systems Letters},
  volume={6},
  pages={2258--2263},
  year={2022},
  publisher={IEEE}
}

@article{cheng2021observation,
  title={Observation data compression for variational assimilation of dynamical systems},
  author={Cheng, Sibo and Lucor, Didier and Argaud, Jean-Philippe},
  journal={Journal of Computational Science},
  volume={53},
  pages={101405},
  year={2021},
  publisher={Elsevier}
}

@article{yao2019quality,
  title={Quality of uncertainty quantification for Bayesian neural network inference},
  author={Yao, Jiayu and Pan, Weiwei and Ghosh, Soumya and Doshi-Velez, Finale},
  journal={arXiv preprint arXiv:1906.09686},
  year={2019}
}

@article{huang2019novel,
  title={A novel Kullback--Leibler divergence minimization-based adaptive student's t-filter},
  author={Huang, Yulong and Zhang, Yonggang and Chambers, Jonathon A},
  journal={IEEE Transactions on signal Processing},
  volume={67},
  number={20},
  pages={5417--5432},
  year={2019},
  publisher={IEEE}
}

@article{seuss2021bridging,
  title={Bridging the gap between explainable AI and uncertainty quantification to enhance trustability},
  author={Seu{\ss}, Dominik},
  journal={arXiv preprint arXiv:2105.11828},
  year={2021}
}


@article{agarwal2021explainable,
  title={Explainable AI for ML jet taggers using expert variables and layerwise relevance propagation},
  author={Agarwal, Garvita and Hay, Lauren and Iashvili, Ia and Mannix, Benjamin and McLean, Christine and Morris, Margaret and Rappoccio, Salvatore and Schubert, Ulrich},
  journal={Journal of High Energy Physics},
  volume={2021},
  number={5},
  pages={1--36},
  year={2021},
  publisher={Springer}
}

@article{zhang2022explainable,
  title={Explainable machine learning in image classification models: An uncertainty quantification perspective},
  author={Zhang, Xiaoge and Chan, Felix TS and Mahadevan, Sankaran},
  journal={Knowledge-Based Systems},
  volume={243},
  pages={108418},
  year={2022},
  publisher={Elsevier}
}

@article{sokol2019counterfactual,
  title={Counterfactual explanations of machine learning predictions: opportunities and challenges for AI safety},
  author={Sokol, Kacper and Flach, Peter A},
  journal={SafeAI@ AAAI},
  year={2019}
}

@article{xiao2016non,
  title={Non-intrusive reduced order modelling of fluid--structure interactions},
  author={Xiao, Dunhui and Yang, Pan and Fang, Fangxin and Xiang, Jiansheng and Pain, Chris C and Navon, Ionel M},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={303},
  pages={35--54},
  year={2016},
  publisher={Elsevier}
}

@article{ferrat2018classifying,
  title={Classifying dynamic transitions in high dimensional neural mass models: A random forest approach},
  author={Ferrat, Lauric A and Goodfellow, Marc and Terry, John R},
  journal={PLoS computational biology},
  volume={14},
  number={3},
  pages={e1006009},
  year={2018},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{luo2020highly,
  title={Highly-accurate community detection via pointwise mutual information-incorporated symmetric non-negative matrix factorization},
  author={Luo, Xin and Liu, Zhigang and Shang, Mingsheng and Lou, Jungang and Zhou, MengChu},
  journal={IEEE Transactions on Network Science and Engineering},
  volume={8},
  number={1},
  pages={463--476},
  year={2020},
  publisher={IEEE}
}

@article{cheng2021graph,
  title={A graph clustering approach to localization for adaptive covariance tuning in data assimilation based on state-observation mapping},
  author={Cheng, Sibo and Argaud, Jean-Philippe and Iooss, Bertrand and Pon{\c{c}}ot, Ang{\'e}lique and Lucor, Didier},
  journal={Mathematical Geosciences},
  volume={53},
  number={8},
  pages={1751--1780},
  year={2021},
  publisher={Springer}
}

@article{xiao2019domain,
  title={A domain decomposition method for the non-intrusive reduced order modelling of fluid flow},
  author={Xiao, Dunhui and Fang, Fangxin and Heaney, Claire E and Navon, IM and Pain, CC},
  journal={Computer Methods in Applied Mechanics and Engineering},
  volume={354},
  pages={307--330},
  year={2019},
  publisher={Elsevier}
}

@article{schneider2022esa,
  title={ESA-ECMWF Report on recent progress and research directions in machine learning for Earth System observation and prediction},
  author={Schneider, Rochelle and Bonavita, Massimo and Geer, Alan and Arcucci, Rossella and Dueben, Peter and Vitolo, Claudia and Le Saux, Bertrand and Demir, Beg{\"u}m and Mathieu, Pierre-Philippe},
  journal={npj Climate and Atmospheric Science},
  volume={5},
  number={1},
  pages={51},
  year={2022},
  publisher={Nature Publishing Group UK London}
}