\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
% \usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{csquotes}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{rotating}
\usepackage{wrapfig}

\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{url} % not crucial - just used below for the URL
\usepackage[bookmarks]{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  filecolor=magenta,
  urlcolor=cyan,
  citecolor=black
}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[reftex]{theoremref}

\DeclareMathOperator*{\plim}{plim}
\DeclareMathOperator*{\sgn}{sgn}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\arcsec}{arcsec}
\DeclareMathOperator*{\arccsc}{arccsc}
\DeclareMathOperator*{\arccot}{arccot}
\DeclareMathOperator*{\arcsinh}{arcsinh}
\DeclareMathOperator*{\arccosh}{arccosh}
\DeclareMathOperator*{\arctanh}{arctanh}
\DeclareMathOperator*{\arcsech}{arcsech}
\DeclareMathOperator*{\arccsch}{arccsch}
\DeclareMathOperator*{\arccoth}{arccoth}
\DeclareMathOperator*{\sinc}{sinc}

% Display breaks in equations in articles
\allowdisplaybreaks

% Control equation numbering
\numberwithin{equation}{section}

% Unnumbered
\theoremstyle{definition}
\newtheorem*{definition*}{Definition}
\newtheorem*{intuition*}{Intuition}
\newtheorem*{assumption*}{Assumption}
\newtheorem*{remark*}{Remark}
\newtheorem*{example*}{Example}
\newtheorem*{exercise*}{Exercise}
\newtheorem*{solution}{Solution}
\newtheorem*{justification}{Justification}
\newtheorem*{notation}{Notation}

\theoremstyle{plain}
\newtheorem*{theorem*}{Theorem}
\newtheorem*{proposition*}{Proposition}
\newtheorem*{claim*}{Claim}

% Numbered but not by Section
\theoremstyle{definition}
\newtheorem{dfn}{Definition}
\newtheorem{intuit}{Intuition}
\newtheorem{asm}{Assumption}
\newtheorem{asms}{Assumption}
\newtheorem{rmk}{Remark}
\newtheorem{exm}{Example}
\newtheorem{exc}{Exercise}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{prp}{Proposition}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}

% Numbered by Section
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{intuition}{Intuition}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]
\newtheorem{exercise}{Exercise}[section]

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conjecture}{Conjecture}[section]

% Independence in Probability Theory and Statistics.
\newcommand{\indep}{\raisebox{0.05em}{\rotatebox[origin=c]{90}{$\models$}}}

\usepackage{natbib}
\AtBeginDocument{\renewcommand{\harvardand}{and}}
\bibliographystyle{agsm}
% \usepackage{bibliographystyle}

\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-1in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.7in}%
\addtolength{\topmargin}{-1in}%

\begin{document}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{\bf BOOTSTRAP BASED ASYMPTOTIC REFINEMENTS FOR HIGH-DIMENSIONAL
NONLINEAR MODELS}
\date{March 2023}

\if0\blind
{
  \author{Joel L. Horowitz \\
    and\\
    Ahnaf Rafi\\
    Department of Economics, Northwestern University
    \footnote{We thank Jian Huang, Soumendra Lahiri, and Whitney Newey for
    helpful comments and discussions.}}
  \maketitle
} \fi

\if1\blind
{
  \author{}
  \maketitle
} \fi

\bigskip

\begin{abstract}
We consider penalized extremum estimation of a high-dimensional, possibly
nonlinear model that is sparse in the sense that most of its parameters are zero
but some are not. We use the SCAD penalty function, which provides model
selection consistent and oracle efficient estimates under suitable conditions.
However, asymptotic approximations based on the oracle model can be inaccurate
with the sample sizes found in many applications. This paper gives conditions
under which the bootstrap, based on estimates obtained through SCAD penalization
with thresholding, provides asymptotic refinements of size \(O \left( n^{- 2}
\right)\) for the error in the rejection (coverage) probability of a symmetric
hypothesis test (confidence interval) and \(O \left( n^{- 1} \right)\) for the
error in rejection (coverage) probability of a one-sided or equal
tailed test (confidence interval). The results of Monte Carlo experiments show
that the bootstrap can provide large reductions in errors in coverage
probabilities. The bootstrap is consistent, though it does not necessarily
provide asymptotic refinements, even if some parameters are close but not equal
to zero. Random-coefficients logit and probit models and nonlinear moment models
are examples of models to which the procedure applies.
\end{abstract}

\noindent%
{\it Keywords: extremum estimation, nonlinear models, high-dimensional
inference, bootstrap based confidence intervals, asymptotic refinements}
\vfill

\newpage

\spacingset{1.9} % DON'T change the spacing!

\section{Introduction}

This paper is about using the bootstrap to obtain asymptotic refinements for
inference about the sparse but possibly high-dimensional parameter
\(\theta_{0}\) that is estimated by a thresholded version of the penalized
extremum estimator
\begin{equation}
  \widetilde{\theta}_{n} = \argmin_{\theta \in \Theta_{n}} \left[ Q_{n} \left(
  \chi_{n}, \theta \right) + p_{\lambda_{n}} (\theta) \right].
  \label{eqn--penalized-extremum}
\end{equation}
In this equation, \(\Theta_{n}\) is a parameter set, \(\chi_{n}\) is a random
sample of size \(n\) from the distribution of a random vector, \(Q_{n}\) is a
known function such as minus a log-likelihood function, \(p_{\lambda_{n}}\)
is a penalty function, and is \(\lambda_{n}\) is a penalization parameter. In
contrast to most of the large literature on high-dimensional estimation, we do
not assume that \(\theta\) is the vector of parameters of a linear or
generalized linear model, the vector of coefficients of a linear combination of
covariates (linear index), or the vector of coefficients of a linear
approximation to a nonlinear function. Instead, \(Q_{n}\) (or \(- Q_{n}\)) is
the objective function of a general extremum estimator, such as a maximum
likelihood estimator; linear or nonlinear regression estimator; instrumental
variables estimator of a linear or nonlinear model; or generalized method of
moments (GMM) estimator. The random coefficients logit and probit models are
examples of widely used models that are neither generalized linear models,
linear index models, or easily approximated by a linear combination of functions
of their covariates. A non-separable, nonlinear demand model with a possibly
endogenous price variable is another example. Maximum likelihood estimation of
random coefficients logit or probit models and GMM estimation of a demand model
are among the estimators that are accommodated by the methods presented in this
paper.

If \(\theta_{0}\) has a fixed dimension and is point identified, then conditions
under which \(\sqrt{n} (\widetilde{\theta}_{n} - \theta_{0})\) is
asymptotically normally distributed without penalization are well known.
See, for example, \citet{1985amemiyaAdvancedEconometrics}, among many other
references. However, the asymptotic normal approximation can be inaccurate with
the sample sizes found in many applications. Under conditions that are
satisfied in many applications, the bootstrap provides asymptotic refinements
for confidence intervals and hypothesis tests. See, for example,
\citet{1992hallBootstrapEdgeworthExpansion} and
\citet{2001horowitzChapterBootstrap}, among other references. The resulting
reductions in the differences between true and nominal coverage and rejection
probabilities (errors in coverage and rejection probabilities or ECPs and ERPs)
can be large. This paper gives conditions under which the same asymptotic
refinements can be obtained in penalized estimation of nonlinear models. We
assume that \(\theta_{0}\) is sparse, meaning that most of its components are
zero but some are non-zero. We carry out inference about a non-zero component or
linear combination of non-zero components. We give conditions under which
bootstrap asymptotic refinements have the same order of magnitude that they
would have if they were obtained from estimation of the oracle model (the model
in which it is known a priori which components are non-zero and which are zero).
For example, the error in the coverage (rejection) probability of a symmetrical
confidence interval (hypothesis test) is \(O \left( n^{- 2} \right)\). We also
give conditions under which the bootstrap is consistent, though it does not
necessarily provide asymptotic refinements, even if some non-zero components of
\(\theta_{0}\) are close but not equal to zero. Under these conditions, there is
not a risk of inconsistency due to violation of the exact sparsity assumption.

\citet{2010chatterjeeAsymptoticPropertiesResidual,
2011chatterjeeBootstrappingLassoEstimators} give conditions under which the
bootstrap based on LASSO (\citet{1996tibshiraniRegressionShrinkageSelection})
estimators of high-dimensional linear models is
consistent. \citet{2013chatterjeeRatesConvergenceAdaptive} give conditions under
which the bootstrap provides asymptotic refinements for confidence intervals and
hypothesis tests based on adaptive LASSO (ALASSO,
\citet{2006zouAdaptiveLassoOracle}) estimators of high-dimensional linear
models. \citet{2022dasHigherOrderAccurate} give conditions under which the
bootstrap provides asymptotic refinements for symmetrical tests and confidence
intervals in penalized high dimensional linear models with a variety of penalty
functions. The asymptotic refinements provided in the present paper are of the
same or higher order than those of
\citet{2013chatterjeeRatesConvergenceAdaptive} and
\citet{2022dasHigherOrderAccurate} but are for models that may be nonlinear.

We use the SCAD penalty function
(\citet{2001antoniadisRegularizationWaveletApproximations},
\citet{2001fanVariableSelectionNonconcave}), which avoids penalization bias of
estimates of the non-zero components of
\(\theta_{0}\). \citet{2001fanVariableSelectionNonconcave} and
\citet{2004fanNonconcavePenalizedLikelihood} give conditions under which a
penalized maximum likelihood estimator with the SCAD penalty function is oracle
efficient, meaning that the centered and scaled estimates of non-zero components
of \(\theta_{0}\) have the same asymptotic distribution that they would have if
it were known a priori which components are zero. We consider the more general
estimator \eqref{eqn--penalized-extremum}.

To the best of our knowledge, this paper is the first to obtain its order of
asymptotic refinements for high dimensional models that may be nonlinear.  Not
surprisingly, achieving these refinements requires assumptions that are stronger
than those in much of the recent literature.  We assume that the number of
parameters is less than \(n\), though it may be an increasing function of \(n\),
and that the number of non-zero components of \(\theta_{0}\) is fixed as \(n\)
increases. These assumptions are motivated by applications in the social
sciences, where a model may have many parameters, but the total number of
parameters is less than the sample size and few have substantial effects on the
dependent variable. In a random coefficients logit or probit model, for example,
the parameters include the means and variances of the coefficients, but some
coefficients may be non-stochastic, in which case the corresponding variances
are zero. As in \citet{2013chatterjeeRatesConvergenceAdaptive} and
\citet{2022dasHigherOrderAccurate}, we require the non-zero parameters to be
sufficiently far from zero, though the distance of these parameters from zero
can decrease as the sample size increases. This ensures that ``large''
parameters can be distinguished from ``small'' ones with sufficiently high
probability. It is not possible to obtain asymptotic refinements of the order
obtained here without making such a distinction, though the bootstrap is
consistent even if some non-zero parameters are ``small.''  The appendix
presents precise statements of our assumptions and their justifications.

The results in this paper are most closely related to those of
\citet{2013chatterjeeRatesConvergenceAdaptive} and
\citet{2022dasHigherOrderAccurate}, who show that suitable versions of the
residual and permutation bootstraps provide asymptotic refinements for test
statistics based on a large class of penalized estimators of the coefficients of
a linear mean-regression model. See also
\citet{2019dasPerturbationBootstrapAdaptive}. These papers obtain their results
by carrying out higher-order expansions of the distributions of the relevant
statistics. This paper uses a different approach. We give conditions under which
a combination of penalized estimation with the SCAD penalty function and hard
thresholding causes differences between the penalized, thresholded parameter
estimate and the infeasible oracle estimate to converge to zero very rapidly.
Consequently, the estimate obtained from penalization and thresholding can be
treated as if it were the oracle estimate. It suffices to consider only the
properties of the bootstrap applied to the oracle model, which are well
known. It is not necessary to carry out higher-order expansions of the
distribution of the penalized estimator.

The literature on high-dimensional estimation is very large. Here, we mention
only few references that are most relevant to the present paper.
\citet{1996tibshiraniRegressionShrinkageSelection} introduces the
LASSO. \citet{2000knightAsymptoticsLassoTypeEstimators};
\citet{2007candesDantzigSelectorStatistical};
\citet{2008huangAsymptoticPropertiesBridge};
\citet{2008zhangSparsityBiasLasso};
\citet{2011belloniL1penalizedQuantileRegression}; and
\citet{2011buhlmannStatisticsHighDimensionalData}
describe properties of the LASSO and related penalized estimators of linear
mean- and quantile-regression models. \citet{2006zouAdaptiveLassoOracle}
introduces the ALASSO and describes its
properties. \citet{2001fanVariableSelectionNonconcave} and
\citet{2004fanNonconcavePenalizedLikelihood} describe properties of
SCAD-penalized least squares and maximum likelihood estimators.
\citet{2014belloniInferenceTreatmentEffects};
\citet{2014vandegeerAsymptoticallyOptimalConfidence};
\citet{2014zhangConfidenceIntervalsLow};
\citet{2018chernozhukovDoubleDebiasedMachine};
\citet{2017luConfidenceIntervalsRegions};
\citet{2020wangDebiasedInferenceTreatment}; and
\citet{2020yuConfidenceIntervalsSparse} describe methods for first order
asymptotic inference in high-dimensional models.
\citet{2015buhlmannConfidenceIntervalsTests}
and \citet{2015dezeureHighDimensionalInference} provide reviews.
\citet{2009bachModelConsistentSparse};
\citet{2010chatterjeeAsymptoticPropertiesResidual,
2011chatterjeeBootstrappingLassoEstimators,
2013chatterjeeRatesConvergenceAdaptive};
\citet{2018javanmardDebiasingLasso};
\citet{2011minnierPerturbationMethodInference};
\citet{2015camponovoValidityPairsBootstrap,
2020camponovoBootstrapInferencePenalized};
\citet{2017dezeureHighDimensionalSimultaneous};
\citet{2017zhangSimultaneousInferenceHigh};
\citet{2018wangWildResidualBootstrap};
\citet{2019dasPerturbationBootstrapAdaptive};
\citet{2020liuBootstrapLassoPartial}; and \citet{2022dasHigherOrderAccurate}
describe bootstrap methods for high-dimensional estimators.
\citet{2011chatterjeeBootstrappingLassoEstimators} apply hard thresholding to
the LASSO estimator of a linear model. They show that the residual bootstrap
consistently estimates the distribution of the \(t\)-statistic this model but do
not obtain asymptotic refinements or treat nonlinear models.
\citet{2009meinshausenLassoTypeRecovery} and
\citet{2011buhlmannStatisticsHighDimensionalData} also discuss thresholding the
LASSO.

The remainder of this paper is organized as follows. Section \ref{sec--method}
describes our method and its properties. This section also treats the case in
which some parameters are close but not equal to zero. Section \ref{sec--sims}
presents the results of a Monte Carlo investigation of the numerical behavior of
the method, and Section \ref{sec--empirical} presents an empirical example of
the application of the method. Section \ref{sec--conclusions} presents
conclusions. Regularity conditions are in the appendix. The proofs of theorems
and certain auxiliary results are in the online supplement.

\section{The Method}
\label{sec--method}

Section \ref{sec--notation} defines notation that is used in the remainder of
this paper. Section \ref{sec--method-bootstrap} presents the bootstrap method
and its properties. Section \ref{sec--method-close-not-zero} treats the case in
which some parameters are close but not equal to zero.

\subsection{Notation}
\label{sec--notation}

Let \(\chi_{n} = \left\{ X_{i} : i = 1, \dots, n \right\}\) be an independent
random sample of the random vector \(X\). To accommodate the possibility that
the dimension of the target parameter may increase as \(n\) increases, we use
the notation \(\theta_{0 n}\) for the target parameter and \(\Theta_{n}\) for
the parameter set. Define \(p_{n} = \mathrm{dim} \left( \theta_{0 n}
\right)\). Let \(A_{0}\) and \(\overline{A}_{0}\), respectively denote the sets
of indices of the non-zero and zero components of \(\theta_{0 n}\). Let
\(\theta_{0 n}\) have \(p_{0}\) non-zero components. This number is fixed as
\(n\) increases. We assume without further loss of generality that the first
\(p_{0}\) components of \(\theta_{0 n}\) are the non-zero ones. Thus, \(A_{0} =
\left\{ 1, \dots, p_{0} \right\}\), and \(\overline{A}_{0} = \left\{ p_{0} + 1,
\dots, p_{n} \right\}\). Let \(\theta_{0 n A_{0}}\) be the \(p_{0} \times 1\)
vector of non-zero components of \(\theta_{0 n}\). Then \(\theta_{0 n}^{\prime}
= \left(\theta_{0 n A_{0}}^{\prime}, 0_{p_{n} - p_{0}}^{\prime} \right)\), where
\(0_{p_{n} - p_{0}}\) denotes a \(\left( p_{n} - p_{0} \right) \times 1\) vector
of zeros. Denote a generic element of \(\Theta_{n}\) by \(\theta_{n}^{\prime} =
\left(\theta_{n A_{0}}^{\prime}, \theta_{n \overline{A}_{0}}^{\prime}
\right)\). Write \(Q_{n} \left( \chi_{n}, \theta_{n} \right)\) as \(Q_{n}
\left(\chi_{n}, \theta_{n A_{0}}, \theta_{n \overline{A}_{0}} \right)\) when it
is necessary to distinguish between components whose indices are in \(A_{0}\)
and components whose indices are in \(\overline{A}_{0}\). Let \(\theta_{n j}\)
(\(j = 1, \dots, p_{n}\)) denote the \(j\)\textsuperscript{th} component of any
vector \(\theta_{n} \in \Theta_{n}\). The penalty parameter depends on \(n\) and
therefore, is denoted by \(\lambda_{n}\). The SCAD penalty function is
\begin{equation*}
  p_{\lambda_{n}} \left( \theta_{n} \right) = \lambda_{n} \sum_{j = 1}^{p_{n}}
  \widetilde{p}_{\lambda_{n}} \left( \left| \theta_{n j} \right| \right),
\end{equation*}
where the function \(\widetilde{p}_{\lambda_{n}}\) is defined by its derivative
\begin{equation*}
  \widetilde{p}_{\lambda_{n}}^{\prime} (v) = I \left( v \leq \lambda_{n} \right)
  + \frac{\max \left\{ a \lambda_{n} - v, 0 \right\}}{(a - 1) \lambda_{n}} I
  \left( v > \lambda_{n} \right); \quad a > 2, v > 0.
\end{equation*}

Let \(\widetilde{\theta}_{n}\) denote the penalized extremum estimator defined
in \eqref{eqn--penalized-extremum} with the SCAD penalty function. Define the
thresholded estimator \(\widehat{\theta}_{n}\) as the \(p_{n} \times 1\) vector
whose \(j\)\textsuperscript{th} component is
\begin{equation}
  \widehat{\theta}_{n j} = \widetilde{\theta}_{n j} I \left(
  \widetilde{\theta}_{n j} \geq \tau_{n} \right)
  \label{eqn--thresholded-estimator}
\end{equation}
where \(\tau_{n} \ll \lambda_{n}\) is a thresholding parameter. Define the sets
\begin{align*}
  \widehat{A}_{0}
  = & \ \left\{ j = 1, \dots, p_{n} : \left| \widehat{\theta}_{n j}
      \right| > 0 \right\}, \\
  \overline{\widehat{A}}_{0}
  = & \ \left\{ j = 1, \dots, p_{n} : \left| \widehat{\theta}_{n j}
      \right| = 0 \right\}.
\end{align*}
Let \(\widehat{\theta}_{n \widehat{A}_{0}}\) and \(\widehat{\theta}_{n
\overline{\widehat{A}}_{0}}\), respectively, denote the vectors of non-zero and
zero components of \(\widehat{\theta}_{n}\). Define
\begin{equation*}
  \Theta_{n}^{O} = \left\{ \theta_{n} \in \Theta_{n} : \theta_{n
  \overline{A}_{0}} = 0_{p_{n} - p_{0}} \right\},
\end{equation*}
and
\begin{equation*}
  \Theta_{n}^{P O} = \left\{ \theta_{n} \in \Theta_{n} : \theta_{n
  \overline{\widehat{A}}_{0}} = 0_{\left| \overline{\widehat{A}}_{0} \right|}
  \right\},
\end{equation*}
where \(\left| \overline{\widehat{A}}_{0} \right|\) is the number of elements in
\(\overline{\widehat{A}}_{0}\) and \(0_{\left| \overline{\widehat{A}}_{0}
\right|}\) is a \(\left| \overline{\widehat{A}}_{0} \right| \times 1\) vector of
zeros. The infeasible oracle estimator of \(\theta_{0 n}\) is
\(\widehat{\theta}_{n}^{O} = \left( \widehat{\theta}_{n
A_{0}}^{O}, 0_{p_{n} - p_{0}} \right)\), where
\begin{equation}
  \widehat{\theta}_{n}^{O} = \argmin_{\theta_{n} \in
  \Theta_{n}^{O}} Q_{n} \left( \chi_{n}, \theta_{n} \right).
  \label{eqn--oracle-estimator}
\end{equation}
This is the estimator obtained by setting \(\theta_{n \overline{A}_{0}} =
0_{p_{n} - p_{0}}\) and choosing \(\theta_{n A_{0}}\) to minimize the
unpenalized objective function \(Q_{n}\). Define the pseudo-oracle estimator
\(\widehat{\theta}_{n}^{P O}\) by
\begin{equation}
  \widehat{\theta}_{n}^{P O} = \argmin_{\theta_{n} \in
  \Theta_{n}^{P O}} Q_{n} \left( \chi_{n}, \theta_{n} \right).
  \label{eqn--pseudo-oracle-estimator}
\end{equation}
This is the estimator obtained by setting \(\theta_{n
\overline{\widehat{A}}_{0}} = 0_{\left| \overline{\widehat{A}}_{0} \right|}\)
and choosing \(\theta_{n \widehat{A}_{0}}\) to minimize the unpenalized
objective function.

Finally, let \(T \left( \widehat{\theta}_{n \widehat{A}_{0}}^{P O} \right)\) be
a statistic based on \(\widehat{\theta}_{n \widehat{A}_{0}}^{P O}\) for testing
a hypothesis about a smooth scalar function of \(\theta_{0 n A_{0}}\). For
example, \(T\) might be a symmetrical \(t\)-statistic for testing a hypothesis
about the \(j\)\textsuperscript{th} component of \(\theta_{0 n A_{0}}\). Denote
the hypothesized value of this component by \(\theta_{0 n A_{0}, j}\).  Then
\begin{equation}
  T \left( \widehat{\theta}_{n \widehat{A}_{0}}^{P O} \right) = \frac{\left|
  \widehat{\theta}_{n \widehat{A}_{0}, j}^{P O} - \theta_{0 n A_{0}, j}
  \right|}{s^{P O}},
  \label{eqn--studentized-symmetrical-tstat}
\end{equation}
where \(s^{P O}\) is a standard error, and \(\theta_{0 n A_{0}, j}\) is the
hypothesized value. Let \(T \left( \widehat{\theta}_{n A_{0}}^{O} \right)\) be
the same statistic based on the oracle estimate \(\widehat{\theta}_{n
A_{0}}^{O}\). Then, the foregoing statistic is
\begin{equation*}
  T \left( \widehat{\theta}_{n A_{0}}^{O} \right) = \frac{\left|
  \widehat{\theta}_{n A_{0}, j}^{O} - \theta_{0 n A_{0}, j} \right|}{s^{O}},
\end{equation*}
where \(s^{O}\) is a standard error. Let \(\widehat{c}_{\alpha}^{P O}\) be the
\(\alpha\)-level critical value of \(T \left( \widehat{\theta}_{n
\widehat{A}_{0}}^{P O} \right)\) that is obtained by the bootstrap procedure
described in Section \ref{sec--method-bootstrap}. Let
\(\widehat{c}_{\alpha}^{O}\) be the \(\alpha\)-level critical value of
\(T \left( \widehat{\theta}_{n A_{0}}^{O} \right)\) that would be obtained
through the conventional bootstrap if \(A_{0}\) were known.

\subsection{Description and Properties of the Method}
\label{sec--method-bootstrap}

The method proposed in this paper consists of the following steps.
\begin{enumerate}
\item Obtain the penalized estimator \(\widetilde{\theta}_{n}\) from
  \eqref{eqn--penalized-extremum} with the SCAD penalty function.
\item Obtain the thresholded estimator \(\widehat{\theta}_{n}\) from
  \eqref{eqn--thresholded-estimator}.
\item Obtain the pseudo-oracle estimator \(\widehat{\theta}_{n
  \widehat{A}_{0}}^{P O}\) from \eqref{eqn--pseudo-oracle-estimator}.
\item Obtain bootstrap samples by sampling the data randomly with replacement
(not the residual bootstrap). Obtain the critical value
\(\widehat{c}_{\alpha}^{P O}\) by using the conventional bootstrap methods with
\(\widehat{\theta}_{n \widehat{A}_{0}}^{P O}\) treated as if it were true oracle
estimator \(\widehat{\theta}_{n A_{0}}^{O}\).
\end{enumerate}

If \(A_{0}\) were known and the values of the parameters were fixed,
an \(\alpha\)-level critical value \(\widehat{c}_{\alpha}^{O}\) could be
obtained by applying conventional bootstrap methods such as those described by
\citet{1992hallBootstrapEdgeworthExpansion} and
\citet{2001horowitzChapterBootstrap} to the oracle estimator. The null
hypothesis being tested would be rejected at the nominal level \(\alpha\) if \(T
\left( \widehat{\theta}_{n A_{0}}^{O} \right) > \widehat{c}_{\alpha}^{O}\). The
difference between the nominal and true probabilities of rejecting a correct
null hypothesis (the ERP) would be
\begin{equation*}
  \mathrm{ERP}^{O} = \left| P \left[ T \left( \widehat{\theta}_{n A_{0}}^{O}
  \right) > \widehat{c}_{\alpha}^{O} \right] - \alpha \right|.
\end{equation*}
\(\mathrm{ERP}^{O}\) is \(O \left( n^{- 2} \right)\) for a the Studentized
symmetrical statistic \(T \left( \widehat{\theta}_{n A_{0}}^{O} \right)\).  It
is typically \(O \left( n^{- c} \right)\), where \(c = 1 / 2, 1, 3 / 2, 2\),
depending on the hypothesis and the test statistic, for non-Studentized
statistics or statistics for one-sided or equal-tailed hypothesis tests. We
assume that the conventional bootstrap provides the same order of refinements if
some non-zero parameters approach zero at the rate specified in
\th\ref{asm--tps} \ref{asm--large-non-zeros} of the appendix.
This paper concentrates on the Studentized, symmetrical statistic in
\eqref{eqn--studentized-symmetrical-tstat}, but the results presented here apply
after obvious modifications to the other statistics.

The method of this paper replaces the unknown \(A_{0}\) with \(\widehat{A}_{0}\)
and applies conventional bootstrap methods to \(\widehat{\theta}_{n
\widehat{A}_{0}, j}^{P O}\) as if \(\widehat{A}_{0}\) were non-stochastic. This
works because \(P \left( \widehat{A}_{0} \neq A_{0} \right)\) approaches zero
very rapidly. The precise result is given by the following theorem.

\begin{theorem}
  \th\label{thm--main}
  Let Assumptions \ref{asm--dgp}-\ref{asm--boot} in the appendix hold. Then
  \begin{equation}
    P \left( \widehat{A}_{0} \neq A_{0} \right) = o \left( n^{- 2} \right)
    \label{eqn--model-selection-rate}
  \end{equation}
  as \(n \to \infty\). In addition
  \begin{equation}
    \left| P \left[ T \left( \widehat{\theta}_{n \widehat{A}_{0}}^{P O} \right)
    > \widehat{c}_{\alpha}^{P O} \right] - P \left[ T \left( \widehat{\theta}_{n
    A_{0}}^{O} \right) > \widehat{c}_{\alpha}^{O} \right] \right| = o \left(
    n^{- 2} \right).
    \label{eqn--erp-rate}
  \end{equation}
\end{theorem}

It follows from \th\ref{thm--main} that
\begin{equation}
  \mathrm{ERP}^{P O} = \left| P \left[ T \left( \widehat{\theta}_{n
  \widehat{A}_{0}}^{P O} \right) > \widehat{c}_{\alpha}^{P O} \right] - \alpha
  \right| = O \left( n^{- 2} \right).
  \label{eqn--erp-po}
\end{equation}
Therefore, a test based on the feasible statistic \(T \left( \widehat{\theta}_{n
\widehat{A}_{0}}^{P O} \right)\) and a feasible bootstrap critical value
\(\widehat{c}_{\alpha}^{P O}\) is equivalent up to \(O \left( n^{- 2} \right)\)
to a test based on the infeasible statistic \(T \left( \widehat{\theta}_{n
A_{0}}^{O} \right)\) and infeasible bootstrap critical value
\(\widehat{c}_{\alpha}^{O}\). Moreover, \(\widehat{\theta}_{n
\widehat{A}_{0}}^{P O}\) is an oracle efficient estimator of \(\theta_{0 n}\).
A confidence interval for a smooth scalar function of \(\theta_{0 n A_{0}}\) is
the set of values of the function that are not rejected by the hypothesis test.
Therefore, \eqref{eqn--erp-po} also applies to the error in the coverage
probability (ECP) of a confidence interval.

\subsection{Small Parameters}
\label{sec--method-close-not-zero}

In this section, we assume that some or all components of \(\theta_{0 n
\overline{A}_{0}}\) are non-zero but small in the sense that \(\left\Vert
\theta_{0 n \overline{A}_{0}} \right\Vert_{1} = o \left( \tau_{n} \right)\). We
use the following additional notation. Let \(\theta_{0 n A_{0}}\) denote the
\(p_{0} \times 1\) vector of components of that are ``large'' in the sense that
\(\left| \theta_{0 n A_{0}, j} \right| \gg \lambda_{n}\) for all \(j = 1, \dots,
p_{0}\).  Define \(\widehat{\theta}_{n A_{0}}^{P T}\) to be the parameter
estimate obtained from the unpenalized pseudo-true model
\begin{equation}
  \widehat{\theta}_{n A_{0}}^{P T} = \argmin_{\theta_{n A_{0}}} Q_{n} \left(
  \chi_{n}, \theta_{n A_{0}}, 0_{\overline{A}_{0}} \right).
\end{equation}
This is the pseudo-true estimate of the large parameters that would be obtained
if \(A_{0}\) were known and all the components of the parameters with indices in
\(\overline{A}_{0}\) were set equal to zero.

The following theorem gives conditions under which the bootstrap consistently
estimates the asymptotic distribution of
\begin{equation*}
  T \left( \widehat{\theta}_{n A_{0}}^{P T} \right) = \frac{\widehat{\theta}_{n
  A_{0}, j}^{P T} - \theta_{0 n A_{0}, j}}{s^{P T}},
\end{equation*}
where \(s^{P T}\) is a standard error. \(T \left( \widehat{\theta}_{n A_{0}}^{P
T} \right)\) is a \(t\)-statistic for testing a hypothesis about \(\theta_{0 n
A_{0}, j}\) (the true parameter value, not a pseudo-true value) when \(A_{0}\)
is known. Under the assumptions of the theorem, the bootstrap estimates the
distribution of \(T \left( \widehat{\theta}_{n A_{0}}^{P T} \right)\)
consistently when \(A_{0}\) is unknown. Equivalently, the bootstrap provides a
confidence interval for \(\theta_{0 n A_{0}, j}\) with asymptotically correct
coverage probability.

\begin{theorem}
\th\label{thm--small-param}
Let Assumptions \ref{asm--tps}\ref{asm--large-non-zeros},
\ref{asm--obj}\ref{asm--uc} and \ref{asm--small}-\ref{asm--sboot} of the
appendix hold. Let \(\widehat{\theta}_{n \widehat{A}_{0}}^{P T}\) be the
estimate of \(\theta_{0 n A_{0}}\) obtained from the unpenalized pseudo-true
model with \(\widehat{A}_{0}\) in place of \(A_{0}\):
\begin{equation*}
  \widehat{\theta}_{n \widehat{A}_{0}}^{P T} = \argmin_{\theta_{n
  \widehat{A}_{0}}} Q_{n} \left( \chi_{n}, \theta_{n \widehat{A}_{0}},
  0_{\overline{\widehat{A}}_{0}} \right),
\end{equation*}
where \(\widehat{A}_{0}\) is defined following
\eqref{eqn--thresholded-estimator}. Denote the bootstrap sample by
\(\chi_{n}^{\ast}\) and the bootstrap estimate based on the unpenalized
pseudo-true model by
\begin{equation*}
  \theta_{n \widehat{A}_{0}}^{\ast P T} = \argmin_{\theta_{n
  \widehat{A}_{0}}} Q_{n} \left( \chi_{n}^{\ast}, \theta_{n \widehat{A}_{0}},
  0_{\overline{\widehat{A}}_{0}} \right).
\end{equation*}
Define the statistic \(T\) based on \(\theta_{n \widehat{A}_{0}}^{\ast P T}\)
by
\begin{equation*}
  T \left( \theta_{n \widehat{A}_{0}}^{\ast P T} \right) = \frac{\theta_{n
  \widehat{A}_{0}, j}^{\ast P T} - \widehat{\theta}_{n \widehat{A}_{0}, j}^{P
  T}}{s^{\ast P T}}
\end{equation*}
where \(s^{\ast P T}\) is the bootstrap standard error obtained when
\(\widehat{A}_{0}\) used in place of \(A_{0}\). Then
\begin{equation*}
  \sup_{z \in \mathbb{R}} \left| P^{\ast} \left[ T \left(
  \theta_{n \widehat{A}_{0}}^{\ast P T} \right) \leq z \right] - P \left[ T
  \left( \widehat{\theta}_{n A_{0}}^{P T} \right) \leq z \right] \right|
  \overset{\mathrm{p}}{\to} 0.
\end{equation*}
\end{theorem}

\section{Monte Carlo Experiments}
\label{sec--sims}

This section reports the results of a Monte Carlo investigation of the
finite-sample performance of the penalization and thresholding method. Section
\ref{sec--computation} describes the computational algorithm. Section
\ref{sec--sims-res} describes the investigation.

\subsection{Computational Algorithm}
\label{sec--computation}

The algorithm estimates \(\theta_{0 n}\) iteratively. Let
\(\widetilde{\theta}_{n}^{0}\) denote the starting value; \(t = 1, 2, \dots\)
index iterations; and \(\widetilde{\theta}_{n}^{t}\) denote the estimate of
\(\theta_{0 n}\) at iteration \(t\). We use the local linear approximation of
the SCAD penalty function of \citet{2008zouOneStepSparseEstimates}. We obtain
\(\widetilde{\theta}_{n}^{t + 1}\) from \(\widetilde{\theta}_{n}^{t}\)
by using the coordinate descent method of
\citet{2007friedmanPathwiseCoordinateOptimization} and
\citet{2010friedmanRegularizationPathsGeneralized} to solve
\begin{equation*}
  \widetilde{\theta}_{n}^{t + 1} = \argmin_{\theta_{n} \in \Theta_{n}} Q_{n}
  \left( \theta_{n} \right) + \sum_{j = 1}^{p_{n}}
  \widetilde{p}^{\prime}_{\lambda_{n}} \left( \left| \widetilde{\theta}_{n
  j}^{t} \right| \right) \left| \theta_{n j} \right|.
\end{equation*}
To speed the computation, we iterate only over non-zero components at a given
\(t\) and implement updates to the set of non-zero components as in
\citet{2018zhaoPathwiseCoordinateOptimization}.
% and \citet{2019gePicassoSparseLearning}.

\subsection{Monte Carlo Results}
\label{sec--sims-res}

We estimate the parameter \(\theta_{0 n}\) of the binary logit model
\begin{equation}
  P (Y = 1 | X) = \frac{\exp \left( \theta_{0 n}^{\prime} X \right)}{1 + \exp
  \left( \theta_{0 n}^{\prime} X \right)}
  \label{eqn--logit}
\end{equation}
where \(Y \in \{0, 1\}\); \(X \sim \mathcal{N} \left( 0_{p_{n}}, \Sigma_{n}
\right)\); \(\Sigma_{n, j \ell} = 0.3^{|j - \ell|}\) for \(j, \ell = 1, \dots,
p_{n}\); \(p_{n} = n / 10\), \(p_{n} = n / 2\) or \(p_{n} = 3 n / 4\); \(p_{0} =
15\); \(\mathrm{dim} \left( \theta_{0 n} \right) = p_{n} \times 1\); and
\begin{equation*}
  \theta_{0 n} = (4, - 1.5, - 3, 1.9, 2.6, 4, - 1.5, - 3, 1.9, 2.6, 4, - 1.5, -
  3, 1.9, 2.6, 0, \dots, 0).
\end{equation*}
\(- Q_{n} \left( \chi_{n}, \theta_{n} \right)\) is the log-likelihood function
for estimating \(\theta_{n}\). The penalty parameter was selected to minimize
the BIC criterion
\begin{equation}
  \min_{\lambda} Q_{n} \left( \chi_{n}, \widetilde{\theta}_{n} (\lambda) \right)
  + C_{n} \left| S_{n} (\lambda) \right| \log n,
  \label{eqn--bic}
\end{equation}
where \(S_{n} (\lambda)\) is the set of indices of non-zero components of the
penalized MLE \(\widetilde{\theta}_{n} (\lambda)\) with penalty parameter
\(\lambda\) but without thresholding, \(\left| S_{n} (\lambda) \right|\) is
the cardinality of \(S_{n} (\lambda)\), and \(C_{n} = 1\) or \(\log \log
p_{n}\). The concavity parameter in the SCAD penalty function was \(a = 3.7\)
\citep{2001fanVariableSelectionNonconcave}, and the thresholding parameter was
\(\tau_{n} = n^{- 1 / 8} a \lambda_{n}\). There were 500 Monte Carlo
replications and 2000 bootstrap replications per experiment.

Tables \ref{tab--theta1-n10}-\ref{tab--theta2-3n4} show empirical coverage
probabilities of nominal one-sided and symmetrical 0.90 intervals for
\(\theta_{0 n, 1}\) and \(\theta_{0 n, 2}\) obtained the following six
ways. Section B.4 of the online supplement presents additional results.
\begin{enumerate}
\item Unpenalized MLE of the full model with first-order asymptotic critical
  values.
\item Unpenalized MLE of the full model with bootstrap critical values.
\item Unpenalized MLE of the infeasible oracle model with first-order asymptotic
  critical values.
\item Unpenalized MLE of the infeasible oracle model with bootstrap critical
  values.
\item Penalized and thresholded MLE with first-order asymptotic critical values.
\item Penalized and thresholded MLE with bootstrap critical values.
\end{enumerate}
The tables show that the empirical coverage probabilities obtained with the full
model are far from the nominal coverage probabilities with either first-order
asymptotic or bootstrap-based critical values. The empirical coverage
probabilities are especially far from the nominal probabilities when \(p_{n} = n
/ 2\) and \(p_{n} = 3 n / 4\). This is because the estimated Hessian and outer
product matrices with these values of \(p_{n}\) are nearly singular.
Consequently, random sampling errors in the inverse of the estimated Hessian (or
outer product), which is used for Studentization, are very large. The empirical
coverage probabilities obtained with the oracle model and bootstrap-based
critical values are close to the nominal probabilities, but the oracle model is
unknown and infeasible in applications. The empirical coverage probabilities
obtained with the penalized, thresholded estimator and bootstrap-based critical
values are close to the nominal probabilities and are not sensitive to the
choice of \(C_{n}\) when \(n \geq 1000\).

\section{Empirical Example}
\label{sec--empirical}

\citet{2019gentzkowMeasuringGroupDifferences} investigated the relation between
party affiliation and two-word phrases (bigrams) spoken by members of Congress.
We use a subset of their data to estimate a binary logit model of the
probability of a member’s party affiliation (Democrat or Republican) conditional
on phrases that the member has used. Our data consist of observations on 4319
members of Congress from 2001-2016. The covariates are the number of times a
member used each of 2441 phrases as well as 60 variables describing
characteristics of the member (e.g. state represented, gender). Thus, the logit
model has 2501 covariates in total. Most of the phrases are used roughly
equally often by Democrats and Republicans. These phrases are unlikely to be
useful for predicting party affiliation, thereby justifying an assumption of
sparsity or approximate sparsity.

The model is as in \eqref{eqn--logit}, where \(Y = 1\) if a member is a
Republican, \(Y = 0\) if the member is a Democrat, and \(X\) is the \(2501
\times 1\) vector of covariates. The SCAD penalty and thresholding parameters
were selected as described in Section \ref{sec--computation}. Penalized
estimation with \(C_{n} = 1\) in \eqref{eqn--bic} resulted in selection of 106
phrases out of the initial 2441. Penalized estimation with \(C_{n} = \log \log
p_{n}\) resulted in selection of 59 phrases, 56 of which are among the 106
selected with \(C_{n} = 1\).

Table \ref{tab--empirical} shows point estimates of the coefficients of several
example phrases and nominal 90\% first-order asymptotic and bootstrap-based
confidence intervals for the coefficients. The point estimates and confidence
intervals are not highly sensitive to the choice of \(C_{n}\).

\section{Conclusions}
\label{sec--conclusions}

Empirical research in economics, among other fields, often involves estimation
of the parameters of a nonlinear model in which the number of parameters may be
a large fraction of the sample size but most parameters are zero or close to
zero. In such settings, the accuracy of inference about large parameters can be
improved greatly through the use of penalized estimation methods that reduce the
number of parameters that must be estimated. However, inference is usually based
on asymptotic approximations that can be highly inaccurate in finite
samples. Under suitable conditions, the bootstrap provides asymptotic
refinements that increase the accuracy of inference, but the usual conditions,
such as those of \citet{1992hallBootstrapEdgeworthExpansion}, are not satisfied
in penalized estimation. This paper has described a method for obtaining
bootstrap asymptotic refinements in penalized estimation of nonlinear
models. The refinements are of the same order as those that would be achieved
with the oracle model if it were known. The results of Monte Carlo experiments
show that with samples of the sizes encountered in much applied research, the
method can achieve large reductions in the errors of the coverage probabilities
of confidence intervals. The bootstrap is consistent even if the sparsity
assumption needed to obtain the asymptotic refinements reported here is not
satisfied. An empirical example has illustrated the method’s practical
usefulness.

\begin{sidewaystable}
  \centering
  \caption{Logit Coverage Probabilities for \(\theta_{0 n, 1}\), Nominal level
  0.90, \(p_{n} = n / 10\)}
  \input{tables/tab_theta1_n10.tex}
  \label{tab--theta1-n10}
\end{sidewaystable}

\begin{sidewaystable}
  \centering
  \caption{Logit Coverage Probabilities for \(\theta_{0 n, 1}\), Nominal level
  0.90, \(p_{n} = n / 2\)}
  \input{tables/tab_theta1_n2.tex}
  \label{tab--theta1-n2}
\end{sidewaystable}

\begin{sidewaystable}
  \centering
  \caption{Logit Coverage Probabilities for \(\theta_{0 n, 1}\), Nominal level
  0.90, \(p_{n} = 3 n / 4\)}
  \input{tables/tab_theta1_3n4.tex}
  \label{tab--theta1-3n4}
\end{sidewaystable}

\begin{sidewaystable}
  \centering
  \caption{Logit Coverage Probabilities for \(\theta_{0 n, 2}\), Nominal level
  0.90, \(p_{n} = n / 10\)}
  \input{tables/tab_theta2_n10.tex}
  \label{tab--theta2-n10}
\end{sidewaystable}

\begin{sidewaystable}
  \centering
  \caption{Logit Coverage Probabilities for \(\theta_{0 n, 2}\), Nominal level
  0.90, \(p_{n} = n / 2\)}
  \input{tables/tab_theta2_n2.tex}
  \label{tab--theta2-n2}
\end{sidewaystable}

\begin{sidewaystable}
  \centering
  \caption{Logit Coverage Probabilities for \(\theta_{0 n, 2}\), Nominal level
  0.90, \(p_{n} = 3 n / 4\)}
  \input{tables/tab_theta2_3n4.tex}
  \label{tab--theta2-3n4}
\end{sidewaystable}

\begin{sidewaystable}
  \centering
  \caption{Coefficients and Confidence Intervals in the Empirical Example}
  \input{tables/tab_empirical.tex}
  \label{tab--empirical}
\end{sidewaystable}

\newpage

\bibliography{refs}

\appendix

\section{Regularity conditions}

\subsection{Assumptions for \th\ref{thm--main}}
\label{sec--assumptions-main}

\begin{asm}
  \th\label{asm--dgp}
  \hfill
  \begin{enumerate}[label=(\roman*)]
  \item \label{asm--iid} \(\chi_{n} = \left\{ X_{i} : i = 1, \dots, n
    \right\}\) is an independent random sample from the distribution of the
    random vector \(X\).
  \item \label{asm--compact} For each \(n\), \(\Theta_{n}\) is a
    compact subset of \(\mathbb{R}^{p_{n}}\).
  \item \label{asm--uniform-compact} There exists \(C < \infty\) such that
    \(\left\Vert \theta_{n} \right\Vert_{1} \leq C\) for all \(\theta_{n} \in
    \Theta_{n}\) and all \(n\).
  \end{enumerate}
\end{asm}
\begin{asm}
  \th\label{asm--tps}
  \hfil
  \begin{enumerate}[label=(\roman*)]
  \item \label{asm--dims} \(p_{n} = O \left( n^{b} \right)\) for some \(0 \leq b
    < 1\).
  \item \label{asm--large-non-zeros} \(\left| \theta_{0 n A_{0}, j} \right| \gg
    \lambda_{n}\) for each \(j = 1, \dots, p_{0}\).
  \item \label{asm--penalty} \(\lambda_{n} = \lambda_{0} n^{- 1 / 4 + 2
    \zeta}\), where \(0 < \zeta < 1 / 8\) and \(\lambda_{0} > 0\) is a constant.
  \end{enumerate}
\end{asm}

For the next set of assumptions, define the following quantities.

\begin{enumerate}[label=\alph*.]
\item \(\delta_{n} = n^{- d}\) and \(m_{n} = \exp \left( n \delta_{n} \right)\),
  where \(1 - 4 \zeta < d < 1 - b\).
\item \(\tau_{n} = \tau_{0} n^{- 1 / 4 + \zeta}\), where \(\tau_{0}\) is a
  constant and \(0 < \tau_{0} < a \lambda_{0}\).
\item The set \(V_{n v} = \left\{ \theta_{n} : \left\Vert \theta_{n} - \theta_{0
  n} \right\Vert_{1} \leq v \right\}\), where \(v > 0\) is a constant.
\item \(S_{n \infty} \left( \theta_{n} \right) = Q_{n \infty} \left( \theta_{n}
  \right) + p_{\lambda_{n}} \left( \theta_{n} \right)\), where \(Q_{n \infty}\)
  is the non-stochastic function defined in
  \th\ref{asm--obj} \ref{asm--uc} below and \(p_{\lambda_{n}}\)
  is the SCAD penalty function.
\end{enumerate}

\begin{asm}
  \th\label{asm--obj}
  \hfill
  \begin{enumerate}[label=(\roman*)]
  \item \label{asm--uc} There are non-stochastic functions \(Q_{n \infty} \left(
    \theta_{n} \right)\) and positive, finite constants \(A\),
    \(\varepsilon_{0}\), \(c\), \(\ell\) and \(n_{0}\) such that
    \begin{equation*}
      P \left[ \sup_{\theta_{n} \in \Theta_{n}} \left| Q_{n} \left( \chi_{n},
      \theta_{n} \right) - Q_{n \infty} \left( \theta_{n} \right) \right| >
      \varepsilon \right] \leq A m_{n} \exp \left( - c n \varepsilon^{2} \right)
    \end{equation*}
    for any \(\ell \tau_{n}^{2} \leq \varepsilon \leq \varepsilon_{0}\) and \(n
    \geq n_{0}\).
  \item \label{asm--sp-min} For each \(n\) such that \(p_{n} > p_{0}\), \(Q_{n
    \infty}\) has a (not necessarily unique) global minimum in \(\Theta_{n}\) at
    a point \(\left( \theta_{0 n A_{0}}^{\prime}, 0_{p_{n} - p_{0}}^{\prime}
    \right)^{\prime} \in \mathrm{int} \left( \Theta_{n} \right)\).
  \item \label{asm--2diff-conv} \(Q_{n \infty}\) is weakly convex and
    twice continuously differentiable in a neighborhood of \(\left( \theta_{0 n
    A_{0}}^{\prime}, 0_{p_{n} - p_{0}}^{\prime} \right)^{\prime}\).
  \item \label{asm--identification} There is a constant \(\rho > 0\) such
    that for any \(v > 0\) and any \(n\),
    \[
      \inf_{\left\Vert \theta_{n A_{0}} - \theta_{0 n A_{0}} \right\Vert_{2}
      \geq v} \left[ Q_{n \infty} \left( \theta_{n} \right) - Q_{n \infty}
      \left( \theta_{0 n} \right) \right] \geq \rho v^{2}.
    \]
  \end{enumerate}
\end{asm}

For the next assumption, define
\begin{align*}
  H_{n, 1 1} \left( \theta_{n} \right) =
  & \ \frac{\partial^{2}}{\partial \theta_{n A_{0}} \partial \theta_{n
    A_{0}}^{\prime}} Q_{n \infty} \left( \theta_{n} \right), \\
  H_{n, 1 2} \left( \theta_{n} \right) =
  & \ \frac{\partial^{2}}{\partial \theta_{n A_{0}} \partial \theta_{n
    \overline{A}_{0}}^{\prime}} Q_{n \infty} \left( \theta_{n} \right), \\
  H_{n, 2 1} \left( \theta_{n} \right) =
  & \ H_{n, 1 2} \left( \theta_{n} \right)^{\prime}, \\
  H_{n, 2 2} \left( \theta_{n} \right) =
  & \ \frac{\partial^{2}}{\partial \theta_{n \overline{A}_{0}} \partial
    \theta_{n \overline{A}_{0}}^{\prime}} Q_{n \infty} \left( \theta_{n}
    \right).
\end{align*}
Note that \(H_{n, 1 2}\) is a \(p_{0} \times \left( p_{n} - p_{0} \right)\)
matrix. Let \(\mu_{n} \left( \theta_{n} \right)\) denote the smallest eigenvalue
of \(H_{n, 1 1} \left( \theta_{n} \right)\).

\begin{asm}
  \th\label{asm--hess}
  There is a \(v > 0\) such that for all \(\theta_{n} \in V_{n v}\),
  \begin{enumerate}[label=(\roman*)]
  \item \label{asm--min-eig} \(\mu_{n} \geq \mu_{0}\) for all \(n\) and some
    \(\mu_{0} > 0\).
  \item \label{asm--irrepresentable} The components of \(H_{n, 2 1} \left(
    \theta_{n} \right) H_{n, 1 1}^{- 1} \left( \theta_{n} \right)\) are bounded
    for all \(n\).
  \end{enumerate}
\end{asm}

\begin{asm}
  \th\label{asm--boot}
  The bootstrap provides asymptotic refinements through \(O \left( n^{- 2}
  \right)\) for the quantity \(P \left[ T \left( \widehat{\theta}_{n A_{0}}^{O}
  \right) > \widehat{c}_{\alpha}^{O} \right]\). That is, \(\left| P \left[ T
  \left( \widehat{\theta}_{n A_{0}}^{O} \right) > \widehat{c}_{\alpha}^{O}
  \right] - \alpha \right| = O \left( n^{- 2} \right)\).
\end{asm}

\th\ref{asm--dgp} specifies the sampling process and parameter
set. \th\ref{asm--tps}\ref{asm--dims} restricts the rate at which \(p_{n}\) can
grow as \(n\) increases and rules out \(p_{n} >
n\). \citet{2013chatterjeeRatesConvergenceAdaptive} obtain bootstrap asymptotic
refinements through \(O_{p} \left( n^{- 1 / 2} \right)\) with \(p_{n} > n\) for
a linear model. \citet{2011fanNonconcavePenalizedLikelihood} give conditions
under which \(P \left( \widehat{\theta}_{n \overline{A}_{0}} = 0 \right) = O
\left( n^{- 1}\right)\) for a generalized linear model with \(p_{n} > n\). This
paper gives conditions under which \(P \left( \widehat{A}_{0} = A_{0} \right) =
o \left(n^{- 2} \right)\) and the bootstrap achieves refinements through \(O
\left( n^{- 2} \right)\) for a large class of nonlinear models that contains but
is not restricted to linear and generalized linear
models. \th\ref{asm--tps}\ref{asm--large-non-zeros} allows the components of
\(\theta_{0 n A_{0}}\) to be small, but they must be larger than random sampling
error. Keeping non-zero coefficients sufficiently far from zero is necessary to
obtain model selection consistency and asymptotic refinements. See, for example,
\citet{2009poetscherDistributionPenalizedMaximum};
\citet{2011buhlmannStatisticsHighDimensionalData};
\citet{2011chatterjeeBootstrappingLassoEstimators,
2013chatterjeeRatesConvergenceAdaptive};
\citet{2011fanNonconcavePenalizedLikelihood};
\citet{2019dasPerturbationBootstrapAdaptive}; and
\citet{2022dasHigherOrderAccurate}. \th\ref{asm--tps}\ref{asm--penalty}
specifies the rate of convergence to zero of the penalization
parameter. \th\ref{asm--obj}\ref{asm--uc} is a high-level restriction on the
objective function \(Q_{n}\). In typical applications, \(Q_{n \infty} = E
\left[Q_{n} \left( \chi_{n}, \theta_{n} \right) \right]\). Proposition 1 in
Section B.3 of the online supplement gives conditions under which
\th\ref{asm--obj}\ref{asm--uc} is satisfied. Section B.3 also presents examples
of models and objective functions that satisfy these conditions, including
log-likelihood functions and objective functions of GMM
estimation. \th\ref{asm--obj}\ref{asm--sp-min}-\ref{asm--identification} and
\ref{asm--hess}\ref{asm--min-eig}-\ref{asm--irrepresentable} place restrictions
on the shapes of the functions and ensure that the non-zero parameter vector is
identified. \th\ref{asm--boot} applies to the oracle
model. \citet{1992hallBootstrapEdgeworthExpansion} gives conditions under which
\th\ref{asm--boot} holds when the non-zero parameters have fixed values. We
assume that \th\ref{asm--boot} holds when the non-zero parameters satisfy
\th\ref{asm--tps}\ref{asm--large-non-zeros}.

\subsection{Assumptions for \th\ref{thm--small-param} but not
\th\ref{thm--main}}
\label{sec--assumptions-small-params}

\renewcommand{\theasms}{\arabic{asms}S}
\begin{asms}
\th\label{asm--small}
\(\left\| \theta_{0 n \overline{A}_{0}} \right\|_{1} = o \left( \tau_{n}
\right)\) as \(n \to \infty\).
\end{asms}

\begin{asms}
  \th\label{asm--shess}
  \hfill
  \begin{enumerate}
  \item \(\mu_{n} \left( \theta_{n} \right)\) is bounded away from 0 for all
    sufficiently large \(n\) uniformly over \(\theta_{n}\) in a neighborhood of
    \(\theta_{0 n}\).
  \item The components of \(H_{n, 1 2} \left( \theta_{n} \right)\) are bounded
    for all \(n\) uniformly over \(\theta_{n}\) in a neighborhood of
    \(\theta_{0 n}\).
  \end{enumerate}
\end{asms}

\begin{asms}
  \th\label{asm--sboot}
  \hfill
  \begin{enumerate}[label=(\roman*)]
  \item The bootstrap consistently estimates the asymptotic distribution of
    \(T \left( \widehat{\theta}_{n A_{0}}^{P T} \right) = \left(
    \widehat{\theta}_{n A_{0}, j}^{P T} - \theta_{0 n A_{0}, j} \right) / s^{P
    T}\). That is,
    \begin{equation*}
      \sup_{z \in \mathbb{R}} \left| P^{\ast} \left[ T \left( \theta_{n
      A_{0}}^{\ast P T} \right) \leq z \right] - P \left[ T \left(
      \widehat{\theta}_{n A_{0}}^{P T} \right) \leq z \right] \right|
      \overset{\mathrm{p}}{\to} 0.
    \end{equation*}
  \item \(P \left( \sqrt{n} \left( \widehat{\theta}_{n A_{0}}^{P T} - \theta_{0
    n A_{0}} \right) \leq z \right)\) is a continuous function of \(z\).
  \item \(\sqrt{n} \cdot s^{P T}\) converges in probability to a non-stochastic,
    finite and positive constant.
  \end{enumerate}
\end{asms}


% \renewcommand{\thesection}{B.\arabic{section}}

Section \ref{section--oa-thm-2.1} presents the proof of \th\ref{thm--main}.
Section \ref{section--oa-thm-2.2} presents the proof of
\th\ref{thm--small-param}. Section \ref{section--oa-sufficient-conditions}
presents auxiliary results. Section \ref{section--oa-logit-ci} presents
additional Monte Carlo results.

\section{Proof Of \th\ref{thm--main}}
\label{section--oa-thm-2.1}

Assumptions 1-5 from the paper hold throughout this section. Define
\begin{equation*}
  S_{n} \left( \chi_{n}, \theta_{n} \right) = Q_{n} \left( \chi_{n}, \theta_{n}
  \right) + p_{\lambda_{n}} \left( \theta_{n} \right)
\end{equation*}
and
\begin{equation*}
  S_{n \infty} \left( \chi_{n}, \theta_{n} \right) = Q_{n \infty} \left(
  \chi_{n}, \theta_{n} \right) + p_{\lambda_{n}} \left( \theta_{n} \right),
\end{equation*}
where \(p_{\lambda_{n}}\) is the SCAD penalty function.

\begin{lem}
  Let \(\left\{ \theta_{n} \right\}\) be any sequence with \(\theta_{n} \in
  \Theta_{n}\) for each \(n\). For sufficiently large \(n\), \(S_{n \infty}
  \left( \theta_{n} \right) \geq S_{n \infty} \left( \theta_{0 n} \right)\) with
  equality holding only if \(\theta_{n} = \theta_{0 n}\).
\end{lem}

\begin{proof}
Define \(\delta_{n A_{0}, j} = \theta_{n A_{0}, j} - \theta_{0 n A_{0}, j}\).
Then,
\begin{align*}
  S_{n \infty} \left( \theta_{n} \right) - S_{n \infty} \left( \theta_{0 n}
  \right)
  =
  & \ Q_{n \infty} \left( \theta_{n} \right) - Q_{n \infty} \left( \theta_{0
    n} \right) \\
  & + \lambda_{n} \sum_{j = 1}^{p_{0}} \left[ \widetilde{p}_{\lambda_{n}}
    \left( \left| \theta_{0 n A_{0}, j} + \delta_{n A_{0}, j} \right|
    \right) - \widetilde{p}_{\lambda} \left( \left| \theta_{0 n A_{0}, j}
    \right| \right) \right] \\
  & + \lambda_{n} \sum_{j = p_{0} + 1}^{p_{n}} \left[
    \widetilde{p}_{\lambda_{n}} \left( \left| \theta_{n \overline{A}_{0}, j}
    \right| \right) \right] \\
  \geq
  & \ Q_{n \infty} \left( \theta_{n} \right) - Q_{n \infty} \left( \theta_{0
    n} \right) \\
  & - \frac{1}{2} p_{0} (1 + a) \lambda_{n}^{2} \\
  & + \lambda_{n} \sum_{j = p_{0} + 1}^{p_{n}} \left[
    \widetilde{p}_{\lambda_{n}} \left( \left| \theta_{n \overline{A}_{0}, j}
    \right| \right) \right].
\end{align*}
If \(\left| \theta_{n A_{0}, j} - \theta_{0 n A_{0}, j} \right| \gg
\lambda_{n}\) for some \(j = 1, \dots, p_{0}\), then \(\left[ Q_{n \infty}
\left( \theta_{n} \right) - Q_{n \infty} \left( \theta_{0 n} \right)\right] \gg
\rho \lambda_{n}^{2}\) for all sufficiently large \(n\) by Assumption
3(iv). Therefore,
\begin{equation*}
  S_{n \infty} \left( \theta_{n} \right) - S_{n \infty} \left( \theta_{0 n}
  \right) \gg \lambda_{n}^{2} + \lambda_{n} \sum_{j = p_{0} + 1}^{p_{n}}
  \widetilde{p}_{\lambda_{n}} \left( \left| \theta_{n \overline{A}_{0}, j}
  \right| \right),
\end{equation*}
and \(S_{n \infty} \left( \theta_{n} \right) - S_{n \infty} \left( \theta_{0 n}
\right) > 0\) for all sufficiently large \(n\). If \(\left| \theta_{n A_{0}, j}
- \theta_{0 n A_{0}, j} \right| \leq c \lambda_{n}\) for some \(c > 0\) then by
Assumption 2(ii)
\begin{equation*}
  \widetilde{p}_{\lambda_{n}} \left( \left| \theta_{n A_{0}, j} \right| \right)
  - \widetilde{p}_{\lambda_{n}} \left( \left| \theta_{0 n A_{0}, j} \right|
  \right) = 0
\end{equation*}
for all \(j = 1, \dots, p_{0}\) and sufficiently large \(n\). Therefore,
\begin{align*}
  S_{n \infty} \left( \theta_{n} \right) - S_{n \infty} \left( \theta_{0 n}
  \right)
  = & \ Q_{n \infty} \left( \theta_{n} \right) - Q_{n \infty} \left( \theta_{0
      n} \right) + \lambda_{n} \sum_{j = p_{0} + 1}^{p_{n}} \left[
      \widetilde{p}_{\lambda_{n}} \left( \left| \theta_{n \overline{A}_{0}, j}
      \right| \right) \right] \\
  \geq & \ \lambda_{n} \sum_{j = p_{0} + 1}^{p_{n}} \left[
      \widetilde{p}_{\lambda_{n}} \left( \left| \theta_{n \overline{A}_{0}, j}
      \right| \right) \right].
\end{align*}
The lemma follows from the observation that \(\widetilde{p}_{\lambda_{n}} \left(
\left| \theta_{n \overline{A}_{0}, j} \right| \right) > 0\) if \(\theta_{n
\overline{A}_{0}, j} \neq \theta_{0 n \overline{A}_{0}, j}\).
\end{proof}

Define
\begin{equation*}
  \mathcal{N}_{n} = \left\{ \theta \in \Theta_{n} : \left\| \theta - \theta_{0
  n} \right\|_{1} < \tau_{n} \right\}
\end{equation*}
and
\begin{equation*}
  \varepsilon_{n} = \inf_{\theta \in \Theta_{n} \setminus \mathcal{N}_{n}} S_{n
  \infty} \left( \theta \right) - S_{n \infty} \left( \theta_{0 n} \right).
\end{equation*}

It follows from \th\ref{lem--a3} below that \(\varepsilon_{n} > 0\) for all
sufficiently large \(n\). Let \(B_{n}\) be the event
\begin{equation*}
  \sup_{\theta \in \Theta_{n}} \left| S_{n} \left( \chi_{n}, \theta \right) -
  S_{n \infty} \left( \theta \right) \right| < \varepsilon_{n} / 2.
\end{equation*}

\begin{lem}
  \th\label{lem--a2}
  \(\widehat{A}_{0} = A_{0}\) for all sufficiently large \(n\) if \(B_{n}\)
  occurs.
\end{lem}

\begin{proof}
It follows from the definition of \(B_{n}\) that
\begin{equation}
  B_{n} \implies S_{n \infty} \left( \widetilde{\theta}_{n} \right) -
  \frac{\varepsilon_{n}}{2} < S_{n} \left( \chi_{n}, \widetilde{\theta}_{n}
  \right)
  \label{eqn--lem2-ineq1}
\end{equation}
and
\begin{equation}
  B_{n} \implies S_{n} \left( \chi_{n}, \theta_{0 n} \right) -
  \frac{\varepsilon_{n}}{2} < S_{n \infty} \left( \theta_{0 n} \right).
  \label{eqn--lem2-ineq2}
\end{equation}
By the definition of \(\widetilde{\theta}_{n}\), \(S_{n} \left( \chi_{n},
\widetilde{\theta}_{n} \right) \leq S_{n} \left( \chi_{n}, \theta_{0 n}
\right)\). Therefore,
\begin{equation}
  B_{n} \implies S_{n \infty} \left( \widetilde{\theta}_{n} \right) -
  \frac{\varepsilon_{n}}{2} < S_{n} \left( \chi_{n}, \theta_{0 n} \right).
  \label{eqn--lem2-ineq3}
\end{equation}
Combining \eqref{eqn--lem2-ineq2} and \eqref{eqn--lem2-ineq3} yields
\begin{equation*}
  B_{n} \implies S_{n \infty} \left( \widetilde{\theta}_{n} \right) -
  \varepsilon_{n} < S_{n \infty} \left( \theta_{0 n} \right)
\end{equation*}
and
\begin{equation*}
  B_{n} \implies S_{n \infty} \left( \widetilde{\theta}_{n} \right) - S_{n
  \infty} \left( \theta_{0 n} \right) < \varepsilon_{n}.
\end{equation*}
Therefore,
\begin{align*}
  B_{n} \implies
  & \ \left\| \widetilde{\theta}_{n} - \theta_{0 n} \right\|_{1} <
    \tau_{n}, \\
  B_{n} \implies
  & \ \left\| \widetilde{\theta}_{n \overline{A}_{0}} \right\|_{1}
    < \tau_{n},
\end{align*}
and
\begin{equation*}
  B_{n} \implies \left| \widehat{\theta}_{n j} \right| > 2 \tau_{n}
\end{equation*}
for each \(j = 1, \dots, p_{0}\) and sufficiently large \(n\). Moreover,
\begin{equation*}
  B_{n} \implies \left| \widetilde{\theta}_{n j} \right| \leq \tau_{n} \text{
  and } \left| \widehat{\theta}_{n j} \right| = 0 \text{ for all } j = p_{0} +
  1, \dots, p_{n}.
\end{equation*}
It follows from the definition of \(\tau_{n}\) that  \(B_{n} \implies
\widehat{A}_{0} = A_{0}\).
\end{proof}

\begin{lem}
  \th\label{lem--a3}
  There is a finite constant \(C_{\varepsilon}\) such that for all sufficiently
  large \(n\),
  \begin{equation*}
    \varepsilon_{n} \geq C_{\varepsilon} \tau_{n}^{2}.
  \end{equation*}
\end{lem}

\begin{proof}
The proof follows from showing that \(\left\| \theta_{A_{0}} - \theta_{0 n
A_{0}} \right\|_{1} = \tau_{n}\) and \(\theta_{\overline{A}_{0}} = 0\) satisfy
the Karush-Kuhn-Tucker (KKT) conditions for
\begin{equation*}
  \varepsilon_{n} = \inf_{\theta \in \Theta_{n} \setminus \mathcal{N}_{n}}
  S_{n \infty} (\theta) - S_{n \infty} \left( \theta_{0 n} \right)
\end{equation*}
if \(n\) is sufficiently large.

We have
\begin{equation}
  \begin{split}
    S_{n \infty} \left( \theta \right) - S_{n \infty} \left( \theta_{0
    n}\right) =
    & \ Q_{n \infty} \left( \theta \right) - Q_{n \infty} \left( \theta_{0
      n} \right) + \lambda_{n} \sum_{j = 1}^{p_{0}} \left[
      \widetilde{p}_{\lambda_{n}} \left( \left| \theta_{j} \right| \right) -
      \widetilde{p}_{\lambda_{n}} \left( \left| \theta_{0 n, j} \right|
      \right) \right] \\
    & + \lambda_{n} \sum_{j = p_{0} + 1}^{p_{n}} \widetilde{p}_{\lambda_{n}}
      \left( \left| \theta_{j} \right| \right).
  \end{split}
  \label{eqn--oa-b14}
\end{equation}
Define
\begin{align*}
  h \left( \theta_{n} \right) =
  & \ \left\| \theta_{n A_{0}} - \theta_{0 n A_{0}} \right\|_{1} \\
  =
  & \ \sum_{j = 1}^{p_{0}} \left[ \left( \theta_{n, j} - \theta_{0 n, j}
    \right) I \left( \theta_{n, j} - \theta_{0 n, j} \geq 0 \right) - \left(
    \theta_{n, j} - \theta_{0 n, j} \right) I \left( \theta_{n, j} - \theta_{0
    n, j} < 0 \right) \right].
\end{align*}
Then
\begin{equation*}
  \frac{\partial h}{\partial \theta_{n j}} \left( \theta_{n j} \right) =
  s_{j},
\end{equation*}
where
\begin{equation*}
  s_{j} =
  \begin{cases}
    1   & \theta_{n, j} - \theta_{0 n, j} \geq 0 \text{ and } j \in A_{0} \\
    - 1 & \theta_{n, j} - \theta_{0 n, j} < 0 \text{ and } j \in A_{0} \\
    0   & j \in \overline{A}_{0}
  \end{cases}
\end{equation*}
Set \(s = \left( s_{1}, \dots, s_{p_{n}} \right)\). If \(\left\| \theta_{n
A_{0}} - \theta_{0 n A_{0}} \right\| = \tau_{n}\) and \(j \in A_{0}\), then
\(\widetilde{p}_{\lambda_{n}} \left( \theta_{n j} \right) -
\widetilde{p}_{\lambda_{n}} \left( \theta_{0 n j} \right)\). If, in addition,
\(\theta_{n \overline{A}_{0}} = 0\), the KKT conditions for
\eqref{eqn--oa-b14} are
\begin{equation}
  \frac{\partial Q_{n \infty}}{\partial \theta_{n j}} \left( \theta_{n}
  \right) + v s_{j} = 0
  \label{eqn--oa-b15}
\end{equation}
if \(j = 1, \dots, p_{0}\), where \(v\) is a Lagrangian multiplier, and
\begin{equation}
  \left| \frac{\partial Q_{n \infty}}{\partial \theta_{n j}} \left( \theta_{n}
  \right) \right| \leq \lambda_{n}
  \label{eqn--oa-b16}
\end{equation}
if \(j = p_{0} + 1, \dots, p_{n}\). By a Taylor series expansion,
\begin{equation*}
  \frac{\partial Q_{n \infty}}{\partial \theta_{n}} \left( \theta_{n} \right)
  = \left[ \frac{\partial^{2} Q_{n \infty}}{\partial \theta_{n} \partial
  \theta_{n}^{\prime}} \left( \check{\theta}_{n} \right) \right] \left(
  \theta_{n} - \theta_{0 n} \right)
\end{equation*}
where \(\check{\theta}_{n}\) is the Taylor series intermediate point and may
be different in different occurrences. Therefore, \eqref{eqn--oa-b15} and
\eqref{eqn--oa-b16} can be written as
\begin{align}
  & H_{n 11} \left( \check{\theta}_{n} \right) \left( \theta_{n A_{0}} -
  \theta_{0 n A_{0}} \right) + v s_{A_{0}} = 0,
  \label{eqn--oa-b17} \\
  & \left| \left[ H_{n 2 1} \left( \check{\theta}_{n} \right) \left( \theta_{n
  A_{0}} - \theta_{0 n A_{0}} \right) \right]_{j} \right| \leq \lambda_{n}
  \text{ for every } j = p_{0} + 1, \dots, p_{n},
  \label{eqn--oa-b18}
\end{align}
where \(s_{A_{0}}^{\prime} = \left( s_{1}, \dots, s_{p_{0}} \right)\).
By \eqref{eqn--oa-b17} and  Assumption 4(i),
\begin{equation*}
  \left( \theta_{n A_{0}} - \theta_{0 n A_{0}} \right) = - v H_{n 1 1}^{- 1}
  \left( \check{\theta}_{n} \right) s_{A_{0}}
\end{equation*}
so \(\left\| \theta_{n A_{0}} - \theta_{0 n A_{0}} \right\|_{1} = \tau_{n}\)
implies that
\begin{equation*}
  |v| \sum_{j = 1}^{p_{0}} \left| \left( H_{n 1 1}^{- 1} \left(
  \check{\theta}_{n} \right) s_{A_{0}} \right)_{j} \right| = \tau_{n}.
\end{equation*}
Define
\begin{equation*}
  C_{n A_{0}} = \sum_{j = 1}^{p_{0}} \left| \left( H_{n 1 1}^{- 1} \left(
  \check{\theta}_{n} \right) s_{A_{0}} \right)_{j} \right|.
\end{equation*}
Then \(|v| = \tau_{n} / C_{n A_{0}}\) and
\begin{equation*}
  \left| \left( \theta_{n A_{0}} - \theta_{0 n A_{0}} \right)_{j} \right| =
  \frac{\tau_{n}}{C_{n A_{0}}} \left| \left( H_{n 1 1}^{- 1} \left(
  \check{\theta}_{n} \right) s_{A_{0}} \right)_{j} \right|
\end{equation*}
for each \(j = 1, \dots, p_{0}\). Inequality \eqref{eqn--oa-b18} is
\begin{equation*}
  C_{n A_{0}}^{- 1} \left| H_{n 2 1} \left( \check{\theta}_{n} \right) H_{n 1
  1}^{- 1} \left( \check{\theta}_{n} \right) s_{A_{0}} \right| \leq
  \frac{\lambda_{n}}{\tau_{n}}.
\end{equation*}
By Assumption 4(ii), this holds for all sufficiently large \(n\) because
\(\tau_{n} \ll \lambda_{n}\) and \(C_{n A_{0}}\) is bounded away from 0 for
all \(n\). It follows that the KKT conditions are satisfied.

Now
\begin{align*}
  Q_{n \infty} \left( \theta_{n A_{0}}, 0_{p_{n} - p_{0}} \right) - Q_{n
  \infty} \left( \theta_{0 n} \right) =
  & \ \frac{1}{2} \left( \theta_{n A_{0}} - \theta_{0 n A_{0}}
    \right)^{\prime} H_{n 1 1} \left( \check{\theta}_{n} \right) \left(
    \theta_{n A_{0}} - A_{0 n A_{0}} \right) \\
  =
  & \ \frac{1}{2} \left( \frac{\tau_{n}}{C_{n A_{0}}} \right)^{2}
    s_{A_{0}}^{\prime} H_{n 1 1}^{- 1} \left( \check{\theta}_{n} \right)
    s_{A_{0}} \geq \frac{1}{2 \mu_{0}} \left( \frac{\tau_{n}}{C_{n A_{0}}}
    \right)^{2}.
\end{align*}
Therefore,
\begin{equation*}
  \varepsilon_{n} \geq \frac{1}{2 \mu_{0}} \left( \frac{\tau_{n}}{C_{n A_{0}}}
  \right)^{2}.
\end{equation*}
Set \(C_{\varepsilon} = \min_{n} \left[ 1 / \left( 2 \mu_{0} C_{n A_{0}}^{2}
\right) \right]\).
\end{proof}

\begin{proof}[Proof of Theorem 2.1]
The proof consists of proving equations \eqref{eqn--model-selection-rate}
and \eqref{eqn--erp-rate}.

Equation \eqref{eqn--model-selection-rate}: Note that
\begin{equation*}
  S_{n} \left( \chi_{n}, \widehat{\theta}_{n} \right) - S_{n \infty} \left(
  \chi_{n}, \widehat{\theta}_{n} \right) = Q_{n} \left( \chi_{n},
  \widehat{\theta}_{n} \right) - Q_{n \infty} \left( \chi_{n},
  \widehat{\theta}_{n} \right).
\end{equation*}
Equation \eqref{eqn--model-selection-rate} now follows from Assumption 3(i),
\th\ref{lem--a2} and \th\ref{lem--a3}.

Equation \eqref{eqn--erp-rate}: By the definition of \(\widehat{c}_{\alpha}^{P
O}\),
\begin{equation*}
  P \left[ T \left( \widehat{\theta}_{n \widehat{A}_{0}}^{P O} \right) >
  \widehat{c}_{\alpha}^{P O} \middle| \widehat{A}_{0} = A_{0} \right] = P
  \left[ T \left( \widehat{\theta}_{n A_{0}}^{O} \right) >
  \widehat{c}_{\alpha}^{O} \right].
\end{equation*}
By \eqref{eqn--model-selection-rate},
\begin{align*}
  P \left[ T \left( \widehat{\theta}_{n \widehat{A}_{0}}^{P O} \right) >
  \widehat{c}_{\alpha}^{P O} \right] =
  & \ P \left[ T \left( \widehat{\theta}_{n \widehat{A}_{0}}^{P O} \right) >
    \widehat{c}_{\alpha}^{P O} \middle| \widehat{A}_{0} = A_{0} \right] P
    \left( \widehat{A}_{0} = A_{0} \right) \\
  & + P \left[ T \left( \widehat{\theta}_{n \widehat{A}_{0}}^{P O} \right) >
    \widehat{c}_{\alpha}^{P O} \middle| \widehat{A}_{0} \neq A_{0} \right] P
    \left( \widehat{A}_{0} \neq A_{0} \right) \\
  =
  & \ P \left[ T \left( \widehat{\theta}_{n \widehat{A}_{0}}^{P O} \right) >
    \widehat{c}_{\alpha}^{P O} \middle| \widehat{A}_{0} = A_{0} \right] \\
  & + \left\{
    \begin{array}{c}
      P \left[ T \left( \widehat{\theta}_{n \widehat{A}_{0}}^{P O} \right) >
      \widehat{c}_{\alpha}^{P O} \middle| \widehat{A}_{0} \neq A_{0} \right]
      \\
      - P \left[ T \left( \widehat{\theta}_{n \widehat{A}_{0}}^{P O} \right) >
      \widehat{c}_{\alpha}^{P O} \middle| \widehat{A}_{0} = A_{0} \right]
    \end{array}
    \right\} \times P \left( \widehat{A}_{0} \neq A_{0} \right) \\
  =
  & \ \ P \left[ T \left( \widehat{\theta}_{n \widehat{A}_{0}}^{P O} \right) >
    \widehat{c}_{\alpha}^{P O} \middle| \widehat{A}_{0} = A_{0} \right] + o
    \left( n^{- 2} \right) \\
  =
  & \ P \left[ T \left( \widehat{\theta}_{n A_{0}}^{O} \right) >
    \widehat{c}_{\alpha}^{O} \right] + o \left( n^{- 2} \right).
\end{align*}
Therefore,
\begin{equation*}
  \left| P \left[ T \left( \widehat{\theta}_{n \widehat{A}_{0}}^{P O} \right)
  > \widehat{c}_{\alpha}^{P O} \right] - P \left[ T \left( \widehat{\theta}_{n
  A_{0}}^{O} \right) > \widehat{c}_{\alpha}^{O} \right] \right| = o \left(
  n^{- 2} \right).
  \label{eqn--erp-rate}
\end{equation*}
\end{proof}

\section{Proof of \th\ref{thm--small-param}}
\label{section--oa-thm-2.2}

\begin{lem}
\th\label{lem--a4}
Let Assumptions 2(ii), 3(i) and 1S-3S hold. Then
\begin{enumerate}[label=(\roman*)]
\item \(S_{n \infty}\) is minimized at \(\left( \check{\theta}_{0 n A_{0}},
  0_{\overline{A}_{0}} \right)\) for some \(\check{\theta}_{0 n A_{0}}\).
\item \(\left\| \check{\theta}_{0 n A_{0}} - \theta_{0 n A_{0}} \right\|_{1} =
  o \left( \lambda_{n} \right)\).
\item \(\widehat{\theta}_{n} - \check{\theta}_{0 n} \overset{\mathrm{p}}{\to}
  0\) as \(n \to \infty\) and \(\lim_{n \to \infty} P \left( \widehat{A}_{0} =
  A_{0} \right) = 1\).
\end{enumerate}
\end{lem}

\begin{proof}
Part (i): The proof consists of showing that for all sufficiently large \(n\),
the KKT conditions for minimizing \(S_{n \infty}\) are satisfied by
\(\theta_{n} = \left( \check{\theta}_{0 n A_{0}}, 0_{\overline{A}_{0}}
\right)\). By a Taylor series expansion,
\begin{equation*}
  Q_{n \infty} \left( \theta_{n} \right) = Q_{n \infty} \left( \theta_{0 n}
  \right) + \frac{1}{2} \left( \theta_{n} - \theta_{0 n} \right)^{\prime}
  H_{n} \left( \overline{\theta}_{n} \right) \left( \theta_{n} - \theta_{0 n}
  \right),
\end{equation*}
where \(H_{n} := \frac{\partial^{2} Q_{n}}{\partial \theta_{n} \partial
\theta_{n}^{\prime}}\) and \(\overline{\theta}_{n}\) is the Taylor series
intermediate point. Define \(\overline{H}_{n} = H_{n} \left(
\overline{\theta}_{n} \right)\). Then
\begin{align*}
  \left( \theta_{n} - \theta_{0 n} \right)^{\prime}
  & \overline{H}_{n} \left(
  \theta_{n} - \theta_{0 n} \right) \\
  =
  & \ \left[ \left( \theta_{n A_{0}} - \theta_{0 n A_{0}} \right)^{\prime},
    \left( \theta_{n \overline{A}_{0}} - \theta_{0 n \overline{A}_{0}}
    \right)^{\prime} \right] \left[
    \begin{array}{cc}
      \overline{H}_{n 1 1} & \overline{H}_{n 1 2} \\
      \overline{H}_{n 2 1} & \overline{H}_{n 2 2}
    \end{array}
    \right] \left[
    \begin{array}{c}
      \theta_{n A_{0}} - \theta_{0 n A_{0}} \\
      \theta_{n \overline{A}_{0}} - \theta_{0 n \overline{A}_{0}}
    \end{array}
    \right]
\end{align*}
and
\begin{align*}
  \frac{\partial}{\partial \theta_{n A_{0}}} Q_{n \infty} \left( \theta_{n}
  \right) =
  & \ \overline{H}_{n 1 1} \left( \theta_{n A_{0}} - \theta_{0 n A_{0}}
    \right) + \overline{H}_{n 1 2} \left( \theta_{n \overline{A}_{0}} -
    \theta_{0 n \overline{A}_{0}} \right), \\
  \frac{\partial}{\partial \theta_{n \overline{A}_{0}}} Q_{n \infty} \left(
  \theta_{n} \right) =
  & \ \overline{H}_{n 2 1} \left( \theta_{n A_{0}} - \theta_{0 n A_{0}}
    \right) + \overline{H}_{n 2 2} \left( \theta_{n \overline{A}_{0}} -
    \theta_{0 n \overline{A}_{0}} \right).
\end{align*}
The KKT conditions with \(\left| \theta_{n A_{0}, j} \right| \gg \lambda_{n}\)
for all \(j = 1, \dots, p_{0}\) and \(\theta_{n \overline{A}_{0}} = 0\) are
\begin{align}
  \frac{\partial}{\partial \theta_{n A_{0}}} Q_{n \infty} \left( \theta_{n}
  \right) =
  & \ \overline{H}_{n 1 1} \left( \theta_{n A_{0}} - \theta_{0 n A_{0}}
    \right) - \overline{H}_{n 1 2} \theta_{0 n \overline{A}_{0}} = 0
    \label{eqn--a-2-1} \\
  - \lambda_{n} \leq \frac{\partial}{\partial \theta_{n \overline{A}_{0}}}
  Q_{n \infty} \left( \theta_{n} \right) =
  & \ \overline{H}_{n 2 1} \left( \theta_{n A_{0}} - \theta_{0 n A_{0}}
    \right) - \overline{H}_{n 2 2} \theta_{0 n \overline{A}_{0}} \leq
    \lambda_{n}
    \label{eqn--a-2-2}
\end{align}
component-wise. If \(\theta_{n \overline{A}_{0}} = 0\), \eqref{eqn--a-2-1}
gives
\begin{equation*}
  \theta_{n A_{0}} - \theta_{0 n A_{0}} = \overline{H}_{n 1 1}^{- 1}
  \overline{H}_{n 1 2} \theta_{0 n \overline{A}_{0}}.
\end{equation*}
Therefore, by Assumption 2S,
\begin{equation}
  \left| \theta_{n A_{0}, j} - \theta_{0 n A_{0}, j} \right| \leq M o \left(
  \tau_{n} \right) = o \left( \lambda_{n} \right)
  \label{eqn--a-2-3}
\end{equation}
for some \(M < \infty\) and all \(j \in A_{0}\). Condition \eqref{eqn--a-2-2}
is
\begin{equation}
  \left| \overline{H}_{n 2 1} \left( \theta_{n A_{0}} - \theta_{0 n A_{0}}
  \right) - \overline{H}_{n 2 2} \theta_{0 n \overline{A}_{0}} \right| \leq
  \lambda_{n},
  \label{eqn--a-2-4}
\end{equation}
component-wise. This inequality is satisfied for all sufficiently large \(n\)
because both terms on its left hand side are \(o \left( \tau_{n}
\right)\). The result follows from \eqref{eqn--a-2-3} and \eqref{eqn--a-2-4}.

Part (ii): This follows from part (i) and \eqref{eqn--a-2-3}.

Part (iii): The conclusion and proof of \th\ref{lem--a2} remain unchanged
after replacing \(\theta_{0 n}\) with \(\check{\theta}_{0 n} = \left(
\check{\theta}_{0 n A_{0}}, 0_{p_{n} - p_{0}} \right)\). Therefore, it follows
from Assumption 3(i) that \(\widehat{\theta}_{n} - \check{\theta}_{0 n}
\overset{\mathrm{p}}{\to} 0\) and \(P \left( \widehat{A}_{0} = A_{0} \right)
\to 1\) as \(n \to \infty\).
\end{proof}

Part (i) of Lemma 4 shows that the penalization and thresholding procedure
drives the small non-zero parameters to zero and replaces the true values of the
large parameters with the pseudo-true values \(\check{\theta}_{0 n A_{0}}\).
Part (ii) shows that the true and pseudo-true parameter values differ by \(o
\left( \lambda_{n} \right)\). Part (iii) of Lemma 4 shows that the penalization
and thresholding procedure estimates the parameters of the pseudo-true model
consistently and discriminates correctly between large and small parameters as
\(n \to \infty\).

\begin{proof}[Proof of Theorem 2.2]
  By Assumption 3S and part (iii) of \th\ref{lem--a4}, it suffices to prove that
  \begin{equation*}
    \sup_{z} \left| P^{\ast} \left( \frac{\theta_{n A_{0}, j}^{\ast P T} -
    \widehat{\theta}_{n A_{0}, j}^{P T}}{s_{A_{0}}^{\ast P T}} \leq z \right) -
    P \left( \frac{\widehat{\theta}_{n A_{0}, j}^{P T} - \theta_{0 n A_{0},
    j}}{s^{P T}} \leq z \right) \right| \overset{p}{\to} 0
  \end{equation*}
  as \(n \to \infty\), where \(s_{A_{0}}^{\ast P T}\) is the bootstrap standard
  error obtained when \(\widehat{A}_{0}\) is replaced with \(A_{0}\). Now
  \begin{align*}
    P \left( \frac{\widehat{\theta}_{n A_{0}, j}^{P T} - \theta_{0 n A_{0},
    j}}{s^{P T}} \leq z \right) =
    & \ P \left( \frac{\widehat{\theta}_{n A_{0}, j}^{P T} - \check{\theta}_{0 n
      A_{0}, j}}{s^{P T}} + \frac{\check{\theta}_{0 n A_{0}, j} - \theta_{0 n
      A_{0}, j}}{s^{P T}} \leq z \right) \\
    = & \ P \left( \frac{\widehat{\theta}_{n A_{0}, j}^{P T} - \check{\theta}_{0 n
      A_{0}, j}}{s^{P T}} \leq z \right) + o (1)
  \end{align*}
  uniformly over \(- \infty < z < \infty\). Therefore,
  \begin{align*}
    \sup_{z}
    & \left| P^{\ast} \left( \frac{\theta_{n A_{0}, j}^{\ast P T} -
      \widehat{\theta}_{n A_{0}, j}^{P T}}{s_{A_{0}}^{\ast P T}} \leq z \right)
      - P \left( \frac{\widehat{\theta}_{n A_{0}, j}^{P T} - \theta_{0 n A_{0},
      j}}{s^{P T}} \leq z \right) \right| \\
    =
    & \ \sup_{z} \left| P^{\ast} \left( \frac{\theta_{n A_{0}, j}^{\ast P T} -
      \widehat{\theta}_{n A_{0}, j}^{P T}}{s_{A_{0}}^{\ast P T}} \leq z \right)
      - P \left( \frac{\widehat{\theta}_{n A_{0}, j}^{P T} - \check{\theta}_{0 n
      A_{0}, j}}{s^{P T}} \leq z \right) \right| + o (1)
  \end{align*}
  and the result follows from Assumption 3S.
\end{proof}

\section{Sufficient Conditions for Assumption 3(i)}
\label{section--oa-sufficient-conditions}

\begin{prp}
\th\label{prp--high-dim-unif-wlln}
For each \(\theta \in [0, 1]^{p_{n}}\) let \(g (X, \theta)\) be a measurable
(real-valued) function of the possibly possibly vector-valued random element
\(X\). Assume:
\begin{enumerate}[label=(\roman*)]
\item \(\left\{ X_{i} : i = 1, \dots, n \right\}\) is an independent random
  sample from the distribution of \(X\).
\item \(\theta \in \Theta_{n} = [0, 1]^{p_{n}}\), \(\| \theta \|_{1} \leq C\)
  for some \(C < \infty\), every \(n\) and \(p_{n} = n^{b}\) for some \(b < 1\).
\item \(E [g (X, \theta)] = 0\) and \(E \left[ g (X, \theta)^{2} \right] \leq
  \sigma_{g}^{2}\) for some constant \(\sigma_{g}^{2} < \infty\), all \(\theta
  \in \Theta_{n}\) and all \(n\).
\item There is a constant \(K_{g} < \infty\) not depending on \(n\) such that
  for each \(\ell = 3, 4, \dots\), \(E \left[ \left| g (X, \theta)
  \right|^{\ell} \right] \leq \ell! \sigma_{g}^{2} K_{g}^{\ell - 2}\) for all
  \(\theta \in \Theta_{n}\) and all \(n\).
\item For each \(n\), there is a function \(M_{n} (X)\) such that
  \begin{equation*}
    \left| g \left( X, \theta_{1} \right) - g \left( X, \theta_{2} \right)
    \right| \leq M_{n} (X) \left\| \theta_{1} - \theta_{2} \right\|_{1}
  \end{equation*}
  for all \(\theta_{1}, \theta_{2} \in \Theta_{n}\). Moreover, there are finite
  constants \(M^{\ast}\) and \(K_{M}\) not depending on \(n\) such that \(\left|
  E \left[ M_{n} (X) \right] \right| \leq M^{\ast}\) and \(E \left[ \left| M_{n}
  (X) - E \left[ M_{n} (X) \right]
  \right|^{\ell} \right] \leq \ell! \sigma^{2}_{M_{n}} K_{M}^{\ell - 2}\)
  for every \(\ell = 3, 4, \dots\) and each \(n\), where \(\sigma_{M_{n}}^{2} =
  \mathrm{Var} \left[ M_{n} (X) \right]\) and for some finite and positive
  \(m_{0}, M_{0}\), and \(m_{0} \leq \sigma_{M_{n}}^{2} \leq M_{0}\).
\end{enumerate}
Then, there are finite constants \(c > 0\), \(\varepsilon_{0} > 0\), \(n_{0} >
0\) and \(\ell > 0\) such that
\begin{equation*}
  P \left[ \sup_{\theta \in \Theta_{n}} \left| \frac{1}{n} \sum_{i = 1}^{n} g
  \left( X_{i}, \theta \right) \right| > \varepsilon \right] \leq 3 m_{n} \exp
  \left( - c n \varepsilon^{2} \right)
\end{equation*}
for all \(n > n_{0}\) if \(\ell \tau_{n}^{2} \leq \varepsilon <
\varepsilon_{0}\).
\end{prp}

\begin{proof}
Define \(m_{n}\) as in Section A.1 of the appendix. Divide \(\Theta_{n}\) into
\(m_{n}\) hypercubic cells whose edges have lengths \(d_{n} = m_{n}^{-
\frac{1}{p_{n}}}\). Denote the cells by \(\Theta_{n j}\) for \(j \in \{1, \dots,
m_{n}\}\). Let \(\theta_{n}^{(j)}\) be a point in the interior of \(\Theta_{n
j}\). Let \(\varepsilon > 0\) be given. Define
\begin{equation*}
  P_{n} = P \left[ \sup_{\theta \in \Theta_{n}} \left| \frac{1}{n} \sum_{i =
  1}^{n} g \left( X_{i}, \theta \right) \right| > \varepsilon \right].
\end{equation*}
Then
\begin{align*}
  P_{n} =
  & \ P \left[ \max_{1 \leq j \leq m_{n}} \left\{ \sup_{\theta \in \Theta_{n
  j}} \left| \frac{1}{n} \sum_{i = 1}^{n} g \left( X_{i}, \theta \right) \right|
  \right\} > \varepsilon \right] \\
  =
  & \ P \left[ \max_{1 \leq j \leq m_{n}} \left\{ \sup_{\theta \in \Theta_{n
    j}} \left| \frac{1}{n} \sum_{i = 1}^{n} g \left( X_{i}, \theta_{n}^{(j)}
    \right) + \frac{1}{n} \sum_{i = 1}^{n} \left[ g \left( X_{i}, \theta \right)
    - g \left( X_{i}, \theta_{n}^{(j)} \right) \right] \right| \right\} >
    \varepsilon \right] \\
  \leq
  & \ P \left[ \max_{1 \leq j \leq m_{n}} \left\{ \left| \frac{1}{n} \sum_{i =
    1}^{n} g \left( X_{i}, \theta_{n}^{(j)} \right) \right| + \sup_{\theta \in
    \Theta_{n j}} \left| \frac{1}{n} \sum_{i = 1}^{n} \left[ g \left( X_{i},
    \theta \right) - g \left( X_{i}, \theta_{n}^{(j)} \right) \right] \right|
    \right\} > \varepsilon \right] \\
  \leq
  & \ P \left[ \max_{1 \leq j \leq m_{n}} \left| \frac{1}{n} \sum_{i =
    1}^{n} g \left( X_{i}, \theta_{n}^{(j)} \right) \right| >
    \frac{\varepsilon}{2} \right] \\
  & \ + P \left[ \max_{1 \leq j \leq m_{n}} \sup_{\theta \in \Theta_{n j}}
    \left| \frac{1}{n} \sum_{i = 1}^{n} \left[ g \left( X_{i}, \theta \right) -
    g \left( X_{i}, \theta_{n}^{(j)} \right) \right] \right| >
    \frac{\varepsilon}{2} \right] \\
  \equiv & \ P_{n 1} + P_{n 2}.
\end{align*}
Consider \(P_{n 1}\). By Bernstein's inequality
\begin{equation*}
  P_{n 1} = P \left[ \max_{1 \leq j \leq m_{n}} \left| \frac{1}{n} \sum_{i =
  1}^{n} g \left( X_{i}, \theta_{n}^{(j)} \right) \right| >
  \frac{\varepsilon}{2} \right] \leq 2 m_{n} \exp \left( - \frac{n
  \varepsilon^{2}}{32 \sigma_{g}^{2}} \right)
\end{equation*}
if \(\varepsilon < \frac{4 \sigma_{g}^{2}}{K_{g}}\).

Now consider \(P_{n 2}\). By assumption (v)
\begin{align*}
  P_{n 2} =
  & \ P \left[ \max_{1 \leq j \leq m_{n}} \sup_{\theta \in \Theta_{n j}}
    \left| \frac{1}{n} \sum_{i = 1}^{n} \left[ g \left( X_{i}, \theta \right) -
    g \left( X_{i}, \theta_{n}^{(j)} \right) \right] \right| >
    \frac{\varepsilon}{2} \right] \\
  \leq
  & \ P \left[ \max_{1 \leq j \leq m_{n}} \sup_{\theta \in \Theta_{n j}}
    \frac{1}{n} \sum_{i = 1}^{n} \left| g \left( X_{i}, \theta \right) -
    g \left( X_{i}, \theta_{n}^{(j)} \right) \right| > \frac{\varepsilon}{2}
    \right] \\
  \leq
  & \ P \left[ \max_{1 \leq j \leq m_{n}} \sup_{\theta \in \Theta_{n j}}
    \frac{1}{n} \sum_{i = 1}^{n} M_{n} \left( X_{i} \right) \left\| \theta -
    \theta_{n}^{(j)} \right\|_{1} > \frac{\varepsilon}{2} \right] \\
  \leq
  & \ m_{n} P \left[ d_{n} p_{n} \frac{1}{n} \sum_{i = 1}^{n} M_{n} \left( X_{i}
    \right) > \frac{\varepsilon}{2} \right] \\
  =
  & \ m_{n} P \left[ \frac{1}{n} \sum_{i = 1}^{n} M_{n} \left( X_{i} \right) >
    \frac{\varepsilon}{2 d_{n} p_{n}} \right].
\end{align*}
Therefore,
\begin{align*}
  P_{n 2} \leq
  & \ m_{n} P \left[ \frac{1}{n} \sum_{i = 1}^{n} \left[ M_{n} \left( X_{i}
    \right) - E \left[ M_{n} (X) \right] \right] > \frac{\varepsilon}{2 d_{n}
    p_{n}} - E \left[ M_{n} (X) \right] \right] \\
  \leq
  & \ m_{n} P \left[ \frac{1}{n} \sum_{i = 1}^{n} \left[ M_{n} \left( X_{i}
    \right) - E \left[ M_{n} (X) \right] \right] > \frac{\varepsilon}{2 d_{n}
    p_{n}} - M^{\ast} \right].
\end{align*}
By Bernstein's inequality,
\begin{align*}
  P_{n 2} \leq
  & \ m_{n} \exp \left\{ - \frac{n \left[ \left( \varepsilon / \left( 2 d_{n}
    p_{n} \right) \right) - M^{\ast} \right]^{2}}{4 M_{0} + 2 K_{M} \left[
    \left( \varepsilon / \left( 2 d_{n} p_{n} \right) \right) - M^{\ast}
    \right]} \right\} \\
  \leq
  & \ m_{n} \exp \left\{ - \frac{n \left[ \left( \varepsilon / \left( 2 d_{n}
    p_{n} \right)
    \right) \right]^{2}}{4 M_{0} + 2 K_{M} \left[ \left( \varepsilon / \left( 2
    d_{n} p_{n} \right) \right) \right]} \right\}.
\end{align*}
if \(\varepsilon > 4 d_{n} p_{n} M^{\ast}\). Now let \(\varepsilon > 4 d_{n}
p_{n} \max \left\{ M^{\ast}, M_{0} / K_{M} \right\}\). Then
\begin{equation*}
  P_{n 2} \leq 2 m_{n} \exp \left[ - \frac{n \varepsilon^{2}}{32 K_{M} d_{n}
  p_{n}} \right].
\end{equation*}
Let \(n_{0}\) be the smallest value of \(n\) such that
\begin{equation*}
  \left[ 4 d_{n} p_{n} \max \left\{ M^{\ast}, M_{0} / K_{M} \right\}
  \right]^{\frac{1}{2}} < \ell \tau_{n}^{2} < \sigma_{g}^{2} / \left( K_{M}
  d_{n} p_{n} \right)
\end{equation*}
for some \(\ell > 0\). Then if \(n > n_{0}\) and \(\ell \tau_{n}^{2} \leq
\varepsilon < \sigma_{g}^{2} / \left( K_{M} d_{n} p_{n} \right)\),
\begin{equation*}
  P_{n 2} \leq 2 m_{n} \left( - \frac{n \varepsilon^{2}}{32 \sigma_{g}^{2}}
  \right).
\end{equation*}
Set
\begin{equation*}
  \varepsilon_{0} = \min \left\{ \frac{4 \sigma_{g}^{2}}{K_{g}},
  \frac{\sigma_{g}^{2}}{K_{M} d_{n_{0}} p_{n_{0}}} \right\}
\end{equation*}
and \(c = \frac{1}{32 \sigma_{g}^{2}}\).
\end{proof}

\subsection{Examples of Models that Satisfy Assumption 3(i)}

\begin{exm}
Penalized least squares estimation of a linear model. The model is
\begin{equation}
  Y_{i} = \sum_{j = 1}^{p_{n}} \theta_{0 n, j} X_{i j} + U_{i} = X_{i}^{\prime}
  \theta_{0 n} + U_{i}; \quad i = 1, \dots, n
  \label{eqn--eg-lm}
\end{equation}
where the \(X_{i j}\)'s are random variables, \(\left\| X_{i} \right\|_{1} \leq
M\) for some \(M < \infty\), all \(i = 1, \dots, n\) and all \(j = 1, \dots,
p_{n}\); the \(U_{i}\)'s are independent and identically distributed
sub-Gaussian random variables; \(E \left[ U_{i} | X_{i} \right] = 0\) and \(E
\left[ U_{i}^{2} \right] = \sigma^{2}\) for all \(i = 1, \dots, n\). Assume that
\(\|\theta\|_{1} \leq M\) for all \(\theta \in \Theta_{n}\) and all \(n\). Also
assume that \(p_{n} = O \left( n^{b} \right)\) for some \(0 \leq b < 1\). Let
\begin{equation*}
  Q_{n} \left( \chi_{n}, \theta \right) = \frac{1}{n} \sum_{i = 1}^{n}
  \left( Y_{i} - X_{i}^{\prime} \theta \right)^{2} = \frac{1}{n} \sum_{i =
  1}^{n} \left[ U_{i} - X_{i}^{\prime} \left( \theta - \theta_{0 n} \right)
  \right]^{2},
\end{equation*}
and
\begin{equation*}
  Q_{n \infty} \left( \theta \right) = E \left[ Q_{n} \left( \chi_{n},
  \theta \right) \right] = \sigma^{2} + \left( \theta - \theta_{0 n}
  \right)^{\prime} \Sigma_{X X} \left( \theta - \theta_{0 n} \right),
\end{equation*}
where \(\Sigma_{X X} = E \left[ X X^{\prime} \right]\). In the notation of
\th\ref{prp--high-dim-unif-wlln}, the function \(g\) here is
\begin{equation*}
  g \left( X, U, \theta \right) = \left( U^{2} - \sigma^{2} \right) - 2 U
  X^{\prime} \left( \theta - \theta_{0 n} \right) + \left( \theta -
  \theta_{0 n} \right)^{\prime} \left( X X^{\prime} - \Sigma_{X X} \right)
  \left( \theta - \theta_{0 n} \right).
\end{equation*}
We show that model \eqref{eqn--eg-lm} satisfies the conditions of
\th\ref{prp--high-dim-unif-wlln}. Conditions (i), (ii), and (iii) are satisfied
by the definition of the model and by the arguments below for condition (iv)
with \(\ell = 2\).
Condition (iv): Let \(X (j)\) denote the \(j\)\textsuperscript{th} component of
\(X\). Then,
\begin{align*}
  g \left( X, U, \theta \right) \leq
  & \left| U^{2} - \sigma^{2} \right| + 2 |U| \left| X^{\prime} \left(
    \theta - \theta_{0 n} \right) \right| \\
  & + \left| \left( \theta - \theta_{0 n} \right)^{\prime} \left( X
    X^{\prime} - \Sigma_{X X} \right) \left( \theta - \theta_{0 n} \right)
    \right|, \\
  \left| \left( \theta - \theta_{0 n} \right)^{\prime} X \right| \leq
  & \ \left| \sum_{j = 1}^{p_{n}} \left( \theta_{n, j} - \theta_{0 n, j} \right)
    X (j) \right| \\
  \leq
  & \sum_{j = 1}^{p_{n}} \left| \theta_{n, j} - \theta_{0 n, j} \right| \left| X
    (j) \right| \\
  \leq
  & \ M \left\| \theta - \theta_{0 n} \right\|_{1} \\
  \leq
  & \ 2 M^{2}, \\
  \left( \theta - \theta_{0 n} \right)^{\prime} X X^{\prime} \left(
  \theta - \theta_{0 n} \right) \leq & 4 M^{4}, \\
  \left( \theta - \theta_{0 n} \right)^{\prime} \Sigma_{X X} \left(
  \theta - \theta_{0 n} \right) \leq & 4 M^{4},
\end{align*}
so that
\begin{equation*}
  \left| g \left( X, U, \theta \right) \right| \leq \left| U^{2} -
  \sigma^{2} \right| + 4 |U| M^{2} + 8 M^{4}.
\end{equation*}
Therefore,
\begin{align*}
  E \left[ \left| g \left( X, U, \theta \right) \right|^{\ell} \right] \leq
  & E \left[ \left| \left| U^{2} - \sigma^{2} \right| + 4 |U| M^{2} + 8 M^{4}
    \right|^{\ell} \right] \\
  \leq
  & \ 2^{\ell} E \left[ \left| U^{2} - \sigma^{2} \right|^{\ell} \right] +
    2^{\ell} E \left[ \left( 4 M^{2} |U| + 8 M^{4} \right)^{\ell} \right] \\
  \leq
  & \ 2^{\ell} E \left[ \left| U^{2} - \sigma^{2} \right|^{\ell} \right] +
    2^{4 \ell} M^{2 \ell} E \left[ |U|^{\ell} \right] +
    2^{2 \ell} \left( 8 M^{4} \right)^{\ell}.
\end{align*}
The first and second terms on the right-hand side of the inequality satisfy
condition (iv) because \(U\) is sub-Gaussian and \(U^{2} - \sigma^{2}\) is
sub-exponential. The third term satisfies condition (iv) because it is a
constant.

Condition (v): \(g \left( X, U, \theta \right)\) is continuously differentiable
with respect to \(\theta\). Therefore,
\begin{equation*}
  \left| g \left( X, U, \theta_{2} \right) - g \left( X, U, \theta_{1}
  \right) \right| \leq \left| \frac{\partial}{\partial \theta} g \left( X, U,
  \widetilde{\theta} \right) \left( \theta_{2} - \theta_{1} \right) \right|.
\end{equation*}
where \(\widetilde{\theta}\) is the Taylor series intermediate point.
\begin{align*}
  \frac{\partial}{\partial \theta} g \left( X, U, \widetilde{\theta} \right) =
  & \ - 2 \left[ X^{\prime} U - \left( \widetilde{\theta} - \theta_{0
    n}\right)^{\prime} \left( X X^{\prime} - \Sigma_{X X} \right) \right] \\
  \left| \frac{\partial}{\partial \theta} g \left( X, U, \widetilde{\theta}
  \right) \left( \theta_{2} - \theta_{1} \right) \right| \leq
  & \ 2 M^{2} |U| + 8 M^{4}.
\end{align*}
Condition (v) is satisfied because \(U\) is sub-Gaussian.
\end{exm}

\begin{exm}
A binary logit model with normally distributed random coefficients. Let
\(\left\{ Y_{i}, X_{i} : i = 1, \dots, n \right\}\) be independently and
identically distributed realizations of the binary random variable \(Y\)
supported in \(\{0, 1\}\) and the \(d_{X} \times 1\) random vector \(X\). Let
\(X_{i j}\) denote the \(j\)\textsuperscript{th} component of \(X_{i}\) for each
\((i , j)\), and assume that \(\|X\|_{1} \leq M\) for some \(M <
\infty\). Define \(\theta = \mathrm{vec} (\beta, C)\) where \(C\) is a \(d_{X} \times
d_{X}\) Cholesky factorization matrix. The model is
\begin{equation}
  P \left( Y_{i} = 1 \middle| X_{i}, \theta \right) = \int \left\{ \frac{\exp
  \left[ \left( \beta + C \varepsilon \right)^{\prime} X_{i} \right]}{1 + \exp
  \left[ \left( \beta + C \varepsilon \right)^{\prime} X_{i} \right]} \right\} f
  (\varepsilon) \mathrm{d} \varepsilon,
  \label{eqn--eg-rclogit}
\end{equation}
where \(f\) is the \(\mathcal{N} \left( 0, I_{d_{X}} \right)\) probability
density function. In the notation of \th\ref{prp--high-dim-unif-wlln}, \(g (x,
\theta) = P \left( Y = 1 | X = x, \theta \right)\). Assume that \(g (x,
\theta)\) is bounded away from \(0\) and \(1\) for all \(x \in \mathrm{support}
(X)\) and all \(\theta \in \Theta_{n}\). The log-likelihood function for
estimating \(\theta\) is
\begin{equation*}
  Q_{n} \left( \chi_{n}, \theta  \right) = \frac{1}{n} \sum_{i = 1}^{n} \left\{
  Y_{i} \log g \left( X_{i}, \theta \right) + \left( 1 - Y_{i} \right) \log
  \left[ 1 - g \left( X_{i}, \theta \right) \right] \right\}.
\end{equation*}
Let
\begin{align*}
  Q_{n \infty} (\theta) =
  & \ E \left[ P (Y = 1 | X) \cdot \log g (X, \theta) + P (Y = 0 | X) \log
    \left[ 1 - g (X, \theta) \right] \right] \\
  =
  & \ E \left[ g \left( X, \theta_{0 n} \right) \cdot \log g (X, \theta) +
    \left[ 1 - g \left( X, \theta_{0 n} \right) \right] \log \left[ 1 - g (X,
    \theta) \right] \right]
\end{align*}
where \(\theta_{0 n} \in \Theta_{n}\). We show that model
\eqref{eqn--eg-rclogit} satisfies the conditions of
\th\ref{prp--high-dim-unif-wlln}.

Conditions (i) and (iii) are satisfied by the definition of the
model. Conditions (iii) and (iv) are satisfied because \(\left| Y_{i} \log g
\left( X_{i}, \theta \right) + \left( 1 - Y_{i} \right) \log \left[ 1 - g \left(
X_{i}, \theta \right) \right] \right|\) is bounded for all \(i = 1, \dots, n\).

Condition (v): This condition is satisfied if \(\left| g \left( X, \theta_{2}
\right) - g \left( X, \theta_{1} \right) \right| \leq M_{g} \left\| \theta_{2} -
\theta_{1} \right\|_{1}\), where \(M_{g} < \infty\) is a constant. To establish
this inequality, define
\begin{equation*}
  \pi (x, \varepsilon, \theta) = \frac{\exp \left[ \left( \beta + C
  \varepsilon \right)^{\prime} x \right]}{1 + \exp \left[ \left( \beta + C
  \varepsilon \right)^{\prime} x \right]}.
\end{equation*}
Then
\begin{equation*}
  \frac{\partial}{\partial \beta_{j}} \pi = X_{j} \pi (1 - \pi)
\end{equation*}
and
\begin{equation*}
  \frac{\partial}{\partial C_{j k}} \pi = \varepsilon_{j} X_{k} \pi (1 - \pi).
\end{equation*}
A Taylor series expansion gives
\begin{align*}
  \left| g \left( X, \theta_{2} \right) - g \left( X, \theta_{1} \right) \right|
  \leq
  & \int \left| \pi \left( X, \varepsilon, \theta_{2} \right) - \pi \left(
    X, \varepsilon, \theta_{1} \right) \right| f (\varepsilon) \; \mathrm{d}
    \varepsilon \\
  \leq
  & \ \int \sum_{j = 1}^{\mathrm{dim} (\theta)} \left| \frac{\partial}{\partial
    \theta_{j}} \pi \left( X, \varepsilon, \widetilde{\theta} \right) \right|
    \left| \theta_{1 j} - \theta_{2 j} \right| f (\varepsilon) \; \mathrm{d}
    \varepsilon \\
  =
  & \ \sum_{j = 1}^{\mathrm{dim} (\theta)} \left| \theta_{1 j} - \theta_{2 j}
    \right| \int \left| \frac{\partial}{\partial \theta_{j}} \pi \left( X,
    \varepsilon, \widetilde{\theta} \right) \right| f (\varepsilon) \;
    \mathrm{d} \varepsilon,
\end{align*}
where \(\widetilde{\theta}\) is the Taylor series intermediate point. Let \(\mu
= (2 / \pi)^{\frac{1}{2}}\) denote the first absolute moment of the
\(\mathcal{N} (0, 1)\) distribution. Because \(\pi (1 - \pi) \leq 0.25\) and
\(\left| X_{i j} \right| \leq M\),
\begin{align*}
  \left| g \left( X, \theta_{2} \right) - g \left( X, \theta_{1} \right) \right|
  \leq
  & \ 0.25 M \int \left( \sum_{j = 1}^{d_{X}} \left| \beta_{2 j} - \beta_{1 j}
    \right| + \sum_{j, k = 1}^{d_{X}} \left| \varepsilon_{j} \right| \left| C_{2
    j k} - C_{1 j k} \right| \right) f (\varepsilon) \mathrm{d} \varepsilon \\
  \leq
  & \ 0.25 M \left\| \beta_{2} - \beta_{1} \right\|_{1} + 0.25 M \mu \sum_{j, k
    = 1}^{d_{X}} \left| C_{2 j k} - C_{1 j k } \right| \\
  \leq
  & \ 0.25 M \left\| \beta_{2} - \beta_{1} \right\|_{1} + 0.25 M \sum_{j, k =
    1}^{d_{X}} \left| C_{2 j k} - C_{1 j k } \right|.
\end{align*}
Therefore,
\begin{equation*}
  \left| g \left( X, \theta_{2} \right) - g \left( X, \theta_{1} \right) \right|
  \leq 0.25 M \left\| \theta_{2} - \theta_{1} \right\|_{1}.
\end{equation*}
Set \(M_{g} = 0.25 M\).
\end{exm}

\begin{exm}
A Generalized Method of Moments Estimator with a Fixed Weight Matrix. Let
\begin{equation*}
  Q_{n} \left( \chi_{n}, \theta \right) = \left[ \frac{1}{n} \sum_{i = 1}^{n} g
  \left( X_{i}, \theta \right) \right]^{\prime} \Omega \left[ \frac{1}{n}
  \sum_{i = 1}^{n} g \left( X_{i}, \theta \right) \right]
\end{equation*}
and
\begin{equation*}
  Q_{n \infty} (\theta) = E [g (X, \theta)]^{\prime} \Omega E [g (X, \theta)],
\end{equation*}
where \(g\) is a \(q \times 1\) vector-valued function and \(\Omega\) is a \(q
\times q\) positive definite symmetrical matrix of finite constants. Assume each
component of \(g\) satisfies the conditions of Proposition 1. Then Assumption
3(i) is satisfied.
\end{exm}

\begin{exm}
A Generalized Method of Moments Estimator with the Continuous Updating Estimate
of the Asymptotically Optimal Weight Matrix.

Use the notation of Example 3 and assume that \(\Omega_{0} (\theta) =
\mathrm{Var} [g (X, \theta)]\) is positive definite with bounded eigenvalues for
all \(\theta \in \Theta_{n}\) and \(n\). Also assume that the components of
\(\Omega_{0} (\theta)\) satisfy \(\left| \Omega_{0, i j} (\theta) \right| \leq
M\) for some \(M < \infty\) and all \(\theta \in \Theta_{n}\), \(i\), \(j\) and
\(n\). Then
\begin{equation*}
  Q_{n} \left( \chi_{n}, \theta \right) = \left[ \frac{1}{n} \sum_{i = 1}^{n} g
  \left( X_{i}, \theta \right) \right]^{\prime} \Omega_{n}^{- 1} (\theta) \left[
  \frac{1}{n} \sum_{i = 1}^{n} g \left( X_{i}, \theta \right) \right]
\end{equation*}
where
\begin{align*}
  \Omega_{n} (\theta) =
  & \ \frac{1}{n} \sum_{i = 1}^{n} g \left( X_{i}, \theta \right) g \left(
    X_{i}, \theta \right)^{\prime} - \overline{g}_{n} (\theta) \overline{g}_{n}
    (\theta) \\
  \overline{g}_{n} (\theta) =
  & \ \frac{1}{n} \sum_{i = 1}^{n} g \left( X_{i}, \theta \right)
\end{align*}
and
\begin{equation*}
  Q_{n \infty} (\theta) = E [g (X, \theta)]^{\prime} \Omega_{0}^{- 1} (\theta) E
  [g (X, \theta)].
\end{equation*}
Let each component of \(g\) satisfy the conditions of
\th\ref{prp--high-dim-unif-wlln}. Then Assumption 3(i) holds.
\end{exm}

\section{Logit Confidence Intervals}
\label{section--oa-logit-ci}
This section presents confidence intervals for the logit model of Section 3
averaged over Monte Carlo replications. Confidence intervals for the full model
are not shown because, as Tables 1-6 show, the differences between their true
and nominal coverage probabilities are very large in most cases.

\begin{sidewaystable}
  \centering
  \caption{Logit Confidence Intervals for \(\theta_{0 n, 1}\), Nominal level
  0.90, \(p_{n} = n / 10\)}
  \input{oa_tables/tab_theta1_n10.tex}
  \label{tab--theta1-n10}
\end{sidewaystable}

\begin{sidewaystable}
  \centering
  \caption{Logit Confidence Intervals for \(\theta_{0 n, 1}\), Nominal level
  0.90, \(p_{n} = n / 2\)}
  \input{oa_tables/tab_theta1_n2.tex}
  \label{tab--theta1-n2}
\end{sidewaystable}

\begin{sidewaystable}
  \centering
  \caption{Logit Confidence Intervals for \(\theta_{0 n, 1}\), Nominal level
  0.90, \(p_{n} = 3 n / 4\)}
  \input{oa_tables/tab_theta1_3n4.tex}
  \label{tab--theta1-3n4}
\end{sidewaystable}

\begin{sidewaystable}
  \centering
  \caption{Logit Confidence Intervals for \(\theta_{0 n, 2}\), Nominal level
  0.90, \(p_{n} = n / 10\)}
  \input{oa_tables/tab_theta2_n10.tex}
  \label{tab--theta2-n10}
\end{sidewaystable}

\begin{sidewaystable}
  \centering
  \caption{Logit Confidence Intervals for \(\theta_{0 n, 2}\), Nominal level
  0.90, \(p_{n} = n / 2\)}
  \input{oa_tables/tab_theta2_n2.tex}
  \label{tab--theta2-n2}
\end{sidewaystable}

\begin{sidewaystable}
  \centering
  \caption{Logit Confidence Intervals for \(\theta_{0 n, 2}\), Nominal level
  0.90, \(p_{n} = 3 n / 4\)}
  \input{oa_tables/tab_theta2_3n4.tex}
  \label{tab--theta2-3n4}
\end{sidewaystable}

\end{document}
% LocalWords:  ECPs ERPs MLE
