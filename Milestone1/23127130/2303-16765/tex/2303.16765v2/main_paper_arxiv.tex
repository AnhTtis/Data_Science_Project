\documentclass[10pt,twocolumn,letterpaper]{article}
\pdfoutput=1

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{docmute}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{duckuments}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{comment}
\usepackage[inline]{enumitem}

\usepackage{cuted}
\usepackage{capt-of}

% Include other packages here, before hyperref.
\usepackage{multirow}

\usepackage{xcolor}
\usepackage{color, colortbl}
\usepackage{graphbox}

\usepackage[percent]{overpic}

\definecolor{A}{RGB}{255,0,0}
\definecolor{B}{RGB}{0, 0, 255}
\definecolor{star}{RGB}{255, 0, 255}

\definecolor{aabbcc}{rgb}{0.8,0.4,0.8}
\definecolor{aabbdd}{rgb}{0.2,0.2,0}
\definecolor{aabbee}{rgb}{0.2,0.6,0.8}
\definecolor{aabbff}{rgb}{0.2,0.0,0.4}
\definecolor{aabbgg}{rgb}{0.4,0.8,0}
\definecolor{aaccdd}{rgb}{0.4,0.8,1}
\definecolor{aaccee}{rgb}{1,0.6,0.8}
\definecolor{aaccff}{RGB}{255,204,51}
\definecolor{aaccgg}{RGB}{125,125,255}
\definecolor{aaddee}{RGB}{255,127,0}
\definecolor{aaddgg}{RGB}{150,150,150}
\definecolor{aaddff}{RGB}{30,30,30}

\usepackage{tkz-euclide}
\usepackage{tikz}
% \usetkzobj{all}
\usepackage{pgfplots}
\pgfplotsset{compat=1.14}
\usetikzlibrary{positioning,calc,fadings,backgrounds,fit,3d,shapes.misc,shapes.geometric}
\usetikzlibrary{matrix}

\usepackage{algorithm}
\usepackage{algpseudocode}



% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\crefname{section}{Section}{Sections}
\crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{9189} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

% \newcommand{\etal}{\textit{et al.}}

\begin{document}

%%%%%%%%% TITLE
\title{MDP: A Generalized Framework for Text-Guided Image Editing by Manipulating the Diffusion Path}

\author{Qian Wang\\
KAUST\\
{\tt\small qian.wang@kaust.edu.sa}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Biao Zhang\\
KAUST\\
{\tt\small biao.zhang@kaust.edu.sa}
\and
Michael Birsak\\
KAUST\\
{\tt\small michael.birsak@kaust.edu.sa}
\and
Peter Wonka\\
KAUST\\
{\tt\small peter.wonka@kaust.edu.sa}
}


\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

\begin{strip}
    \vspace{-20pt}
    \centering
    \scriptsize
    \resizebox{\linewidth}{!}{%
        \setlength{\tabcolsep}{-0pt}
        \begin{tabular}{c@{\hskip 2pt}cc@{\hskip 1pt}cc@{\hskip 1pt}cc@{\hskip 1pt}cc}
            &  \multicolumn{2}{c}{``Cat'' to ``Dog''} & \multicolumn{2}{c}{``Red'' to ``Purple''} & \multicolumn{2}{c}{Remove ``Stuff''} & \multicolumn{2}{c}{Mix ``Teapot'' and ``Shoes''}\\
            Local Edits & \includegraphics[align=c, scale=0.11]{figures/teaser_jpg/changing_object/002_input.jpg} & \includegraphics[align=c, scale=0.11]{figures/teaser_jpg/changing_object/002_epsilon.jpg} & \includegraphics[align=c, scale=0.11]{figures/teaser_jpg/changing_attributes/004_input.jpg} & \includegraphics[align=c, scale=0.11]{figures/teaser_jpg/changing_attributes/004_epsilon.jpg} & \includegraphics[align=c, scale=0.11]{figures/teaser_jpg/removing_object/003_input.jpg} & \includegraphics[align=c, scale=0.11]{figures/teaser_jpg/removing_object/003_epsilon.jpg} & \includegraphics[align=c, scale=0.11]{figures/teaser_jpg/mixing_objects/005_input.jpg} & \includegraphics[align=c, scale=0.11]{figures/teaser_jpg/mixing_objects/005_epsilon_1.jpg}\\ [1cm]
            & \multicolumn{2}{c}{``Table'' to ``Galaxy''} & \multicolumn{2}{c}{``Fall'' to ``Winter''} & \multicolumn{2}{c}{``Painting'' to ``Children's drawing''} & \multicolumn{2}{c}{Stylize ``Blanket''} \\
            Global Edits & \includegraphics[align=c, scale=0.11]{figures/teaser_jpg/changing_background/002_input.jpg} & \includegraphics[align=c, scale=0.11]{figures/teaser_jpg/changing_background/002_epsilon.jpg} & \includegraphics[align=c, scale=0.11]{figures/teaser_jpg/in_domain_transfer/005_input.jpg} & \includegraphics[align=c, scale=0.11]{figures/teaser_jpg/in_domain_transfer/005_epsilon.jpg} & \includegraphics[align=c, scale=0.11]{figures/teaser_jpg/out_domain_transfer/003_input.jpg} & \includegraphics[align=c, scale=0.11]{figures/teaser_jpg/out_domain_transfer/003_epsilon.jpg} & \includegraphics[align=c, scale=0.11]{figures/teaser_jpg/stylization/004_input.jpg} & \includegraphics[align=c, scale=0.11]{figures/teaser_jpg/stylization/004_epsilon.jpg}
        \end{tabular}
    }
    %\caption{Our proposed editing framework MDP can do multiple local and global edits without additional training.\label{fig:teaser}
    \vspace{-8pt}
    \captionof{figure}{Our proposed manipulation of predicted noise, which fits into our proposed editing framework \textbf{MDP}, can do multiple local and global edits without training or finetuning.
\label{fig:teaser}}
\end{strip}


%%%%%%%%% ABSTRACT
\begin{abstract}
\vspace{-5mm}
Image generation using diffusion can be controlled in multiple ways. 
% As the diffusion process consists of many individual steps, we can control and steer each timestep separately.
In this paper, we systematically analyze the equations of modern generative diffusion networks to propose a framework, called MDP, that explains the design space of suitable manipulations. We identify 5 different manipulations, including intermediate latent, conditional embedding, cross attention maps, guidance, and predicted noise. We analyze the corresponding parameters of these manipulations and the manipulation schedule. We show that some previous editing methods fit nicely into our framework. 
Particularly, we identified one specific configuration as a new type of control by manipulating the predicted noise, which can perform higher-quality edits than previous work for a variety of local and global edits. We provide the code in \href{https://github.com/QianWangX/MDP-Diffusion}{https://github.com/QianWangX/MDP-Diffusion}.
%Additionally, we propose a generalized framework for manipulating the diffusion generation path and analyze the resulting design space and parameters. 
% We investigate different types of local and global image editing operations and propose recommended parameters for these editing operations in the design space. 
\end{abstract}


%%%%%%%%% BODY TEXT
\section{Introduction}
% Image translation task, GAN works
% Diffusion models

Previously, image editing using GANs has achieved great success \cite{isola2017imagegan,zhu2017cyclegan,choi2018stargan,wang2018highres,huang2018multimodal,kim2017discogan,bau2020semantic}. As large-scale text-to-image datasets became available, text-guided image synthesis and editing has obtained increasing attention \cite{ramesh2021dalle,crowson2022vqganclip,ding2021cogview,ding2022cogview2,gafni2022make-a-scene}. 
Generative diffusion models ~\cite{ho2020ddpm,rombach2022latentdiffusion,saharia2022imagen,nichol2022glide,ramesh2022dalle2} are also a powerful tool for multiple image processing tasks, such as inpainting ~\cite{lugmayr2022repaint,choi2021ilvr,li2022sdm,xie2022smartbrush}, style transfer~\cite{kwon2022disentangled_style_content,bansal2023universal-guidance}, text-guided image editing\cite{brooks2022instructpix2pix,kawar2022imagic,hertz2022prompt2prompt,tumanyan2022plug-and-play}, map-to-image translation \cite{voynov2022sketch-diffusion,avrahami2022spatext} and segmentation \cite{burgert2022peekaboo,baranchuk2021label-efficient}.

We are interested in simple and effective image editing methods that do not require retraining, fine-tuning, or training of an auxiliary network. Specifically, for text-guided image editing, we can leverage pre-trained diffusion models to perform editing tasks on real images. An arbitrary real image can first be embedded into the diffusion latent space to obtain an initial noise tensor \cite{wallace2022edict,mokady2022null}. Then, by manipulating the diffusion generation path starting from this initial noise tensor, we can edit the input image.

Given an input image, we perform edits guided by a condition (\eg, a text prompt or a class label) while keeping the overall layout from the input image. We can see this editing task as combining the layout from the input image with new semantics from the condition.
Since diffusion models generate images progressively, the layout of an image is generated during the early timesteps of the denoising process, while the semantics, \ie the texture, color, details, are generated during later steps \cite{hoogeboom2023simple-diffusion,liew2022magicmix}. Therefore, it is intuitive that each timestep should be controlled separately.
% Therefore, it is only natural to investigate how one can utilize this property of diffusion to extract the layout from an input image and combine it with the semantics from a condition.

For each denoising step, there are several components that we can modify during the diffusion process. Intuitively, we want to denoise using information from the input image, for which we want to preserve the layout, in the early stages, and then the components of the new condition in the later stages. At this point, two questions arise: 
\begin{enumerate*}[label=(\arabic*)]
\item \textit{Which component of the generation process should be modified?}
\item \textit{For which denoising steps should the modification take place?}
\end{enumerate*}

In order to tackle the problems, we analyze the equations of modern generative diffusion networks to find out which variables could be manipulated and identified 5 different manipulations that are suitable: intermediate latent, conditional embedding, cross attention maps, guidance and predicted noise. We take the manipulation of attention maps from the seminal paper Prompt-to-Prompt (P2P) \cite{hertz2022prompt2prompt}. As result of the analysis, we introduce a generalized editing framework, \textbf{MDP}, that contains these 5 different manipulations, their corresponding parameters, and a manipulation schedule for them.

%Similar insights for image editing are shared by other works. In particular, the seminal paper Prompt-to-Prompt (P2P) \cite{hertz2022prompt2prompt} proposes to manipulate the attention maps, which result in manipulating the conditioning information, during the denoising process to achieve plausible local edits (e.g. changing objects and adding objects) as well as selected global edits (e.g. style transfer from photo to painting). 

%We particularly experimenting with intermediate latent, conditional embedding and predicted noise to modify, which we find that by manipulating the predicted noise, we can faithfully achieve many image editing applications. 

In this design space, we identified one particular configuration, MDP-$\epsilon_t$, that yields very good results for a variety of local and global image editing problems. The results from MDP-$\epsilon_t$ are more consistent in quality than previous work (P2P) and other possible configurations. We advocate for the use of MDP-$\epsilon_t$ as a new method for diffusion-based image editing.

%In order to put our novel manipulations into context, we also provide a generalized framework for manipulating the diffusion path for image editing, and set out to systematically analyze the parameters of the diffusion process. We therefore call our generalized framework \textbf{MDP}. We identified the following aspects that can be manipulated: the intermediate latent representation, the conditional embeddings, cross attention maps, guidance, and the predicted noise. The manipulations are feasible at each timestep and can be mixed and parameterized in an manipulation schedule. We also compared our manipulation of predicted noise with existing baselines and find that manipulating the predicted noise is generally more powerful than other kinds of edits.

% Our work

% we show the results

Our work makes the following contributions:
\begin{itemize}
\item We present a framework for a generalized design space to manipulate the diffusion path that includes multiple existing methods as special cases.
\item We analyze the design space to find good configurations suitable for editing applications.
\item We propose a new solution by manipulating predicted noise that can solve practical local and global editing problems better than other configurations and previous work.
% \item We highlight / find X configurations that seem to be particularly useful to solve practical editing problems
% \item We present a small taxonomy of editing tasks to compile a small test suite of edits
\end{itemize}

\begin{figure*}[htb]
    \centering
    \resizebox{\linewidth}{!}{%
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{cccccccccc}
        50 & 40 & 30 & 20 & 10 & 5 & 3 & 2 & 1 & 0\\
        \includegraphics[scale=0.15]{figures/tex/diffusion_layout/50.jpg} &
        \includegraphics[scale=0.15]{figures/tex/diffusion_layout/40.jpg} &
        \includegraphics[scale=0.15]{figures/tex/diffusion_layout/30.jpg} &
        \includegraphics[scale=0.15]{figures/tex/diffusion_layout/20.jpg} &
        \includegraphics[scale=0.15]{figures/tex/diffusion_layout/10.jpg} &
        \includegraphics[scale=0.15]{figures/tex/diffusion_layout/5.jpg} &
        \includegraphics[scale=0.15]{figures/tex/diffusion_layout/3.jpg} &
        \includegraphics[scale=0.15]{figures/tex/diffusion_layout/2.jpg} &
        \includegraphics[scale=0.15]{figures/tex/diffusion_layout/1.jpg} &
        \includegraphics[scale=0.15]{figures/tex/diffusion_layout/0.jpg} \\
    \end{tabular}
    }
    \vspace{-8pt}
    \caption{Assume the total number of sampling steps $T=50$. The number on top of each image shows the number of diffusion generation steps for which we use $\mathbf c^{(A)}$ ``\textit{Photo of a cat sitting next to a mirror}'' to invert and then denoise. For the remaining diffusion generation steps, we switch to $\mathbf c^{(B)}$ ``\textit{Photo of a tiger sitting next to a mirror}'' to denoise. The leftmost image is equivalent to generating an image using condition A. The rightmost image is equivalent to generating an image using condition B. If we change condition in the early stage (\eg, 30 and 20), we can preserve the layout of $\mathbf c^{(A)}$ while including the semantics from $\mathbf c^{(B)}$. If we switch at a later stage (\eg, 5, 3, 2, 1), the layout of $\mathbf c^{(A)}$ is hardly maintained in the result.}
    \label{fig:diffusion-layout-details}
\end{figure*}
\section{Related Work}
\paragraph{Image diffusion models.}
% Michael please write something
%DDPM [Submitted on 19 Jun 2020 (v1), last revised 16 Dec 2020 (this version, v2)]
%Glide [Submitted on 20 Dec 2021 (v1), last revised 8 Mar 2022 (this version, v3)]
%Dalle2 [Submitted on 13 Apr 2022]
%DDIM [Submitted on 6 Oct 2020 (v1), last revised 5 Oct 2022 (this version, v4)]

%Imagen [Submitted on 23 May 2022]
%Latent Diffusion / Stable Diffusion 
%Ediff-I 
%Composer 
%Karras EDM 

In recent years, the interest in deep generative models has gained strong momentum and many different methods have been proposed to create high-quality samples from a magnitude of different data domains. One kind of architecture that currently stands out are diffusion models \cite{ho2020ddpm, dhariwal2021beatgans, karras2022edm}.

% Pioneering in this context is the work by Ho \etal who proposed Denoising Diffusion Probabilistic Models (DDPMs) \cite{ho2020ddpm}. % Although being unconditional, their approach achieves better sample quality than other conditional models in the literature on datasets like CelebA-HQ $256 \times 256$.

In order to obtain better control over the denoising process and the generated content, several works proposed text-conditioned image synthesis using diffusion models and either CLIP guidance or classifier-free guidance \cite{nichol2022glide, ramesh2022dalle2, saharia2022imagen}.

Several approaches focus on different optimization strategies and either propose to generalize DDPMs via a class of non-Markovian diffusion processes to trade off computation for sample quality \cite{song2021ddim} or to do the diffusion process in the latent space of a pretrained autoencoder \cite{rombach2022latentdiffusion} instead of the RGB pixel space.

%Nichol \etal propose a method for Guided Language to Image Diffusion for Generation and Editing (GLIDE) \cite{nichol2022glide} that allows text-conditional image synthesis. They compare two different guidance strategies: CLIP guidance and classifier-free guidance and conclude that the latter is preferred by human evaluators for both photorealism and caption similarity.

% Ramesh \etal propose their DALLÂ·E 2 framework \cite{ramesh2022dalle2} and improve upon the GLIDE model by increasing the output resolution from $256 \times 256$ to $1024 \times 1024$. They present a two-stage model consisting of a prior that generates a CLIP image embedding for a given text caption, and a decoder that essentially inverts this embedding (hence the internal name unCLIP) to generate an image.

% While being able to generate high-quality samples, one critical drawback of diffusion models in general compared to GANs is that they require many iterations to produce a high-quality sample. This computational complexity in the sampling process is imposed by the many steps that are needed to map pure noise to a clean output image. To accelerate sampling, Song \etal propose Denoising Diffusion Implicit Models (DDIMs) \cite{song2021ddim}. In essence, they generalize DDPMs via a class of non-Markovian diffusion processes to trade off computation for sample quality to draw samples $10 \times$ up to $50 \times$ faster compared to DDPMs.

% Saharia \etal proposed Imagen \cite{saharia2022imagen}

% Rombach \etal propose to do the diffusion process not directly in the RGB pixel space, but to leverage the beneficial properties of the latent space of a pretrained autoencoder \cite{rombach2022latentdiffusion}. The resulting latent diffusion models (LDMs) significantly reduce the training complexity due to the smaller spatial resolution of input images, while at the same time achieving new state-of-the-art scores for image inpainting and class-conditional image synthesis.

Others proposed to use an ensemble of expert denoising networks \cite{balaji2022ediff-i} or to compose an image using both global and local representative factors \cite{huang2023composer}.

% Balaji \etal proposed eDiff-I \cite{balaji2022ediff-i}
% Huang \etal proposed Composer \cite{huang2023composer}
% Karras \etal proposed EDM \cite{karras2022edm}

\paragraph{Image editing with diffusion models.}
%Palette \cite{saharia2022palette} can do multiple image-to-image editing tasks like inpainting, colorization and JPEG restoration. 
InstructPix2Pix~\cite{brooks2022instructpix2pix} can edit images by following user instructions. Paint by Example \cite{yang2022paint_by_example} allows users to replace an object with a conditional example image. ControlNet \cite{zhang2023control_net} trains a task-specific condition network on top of a pre-trained diffusion model and supports various edits based on a specific training set. All these works usually require re-designing of the model, collecting training data and a long training time to do image translation.

UniTune \cite{kawar2022imagic} and Imagic \cite{kawar2022imagic} can do realistic text-guided image editing by either just finetuning the diffusion models or the models and text embeddings together. However, because of the finetuning process, it takes several minutes or more to generate a single image. 
%Kwon and Ye \cite{kwon2022disentangled_style_content} can do text-guided and image-guided image translation. However, they optimize several loss terms together, which requires additional effort to determine weights for each loss term.

Recently, there are many interesting works that only utilize a pretrained diffusion model to do text-guided image editing without any training or finetuning \cite{meng2021sdedit,radford2021clip,couairon2022diffedit,liew2022magicmix,hertz2022prompt2prompt,park2022shape-guided,parmar2023pix2pix_zero,tumanyan2022plug-and-play}. 
%SDEdit \cite{meng2021sdedit} firsts add noise to the input image, then using the condition to denoise it based on a prior. 
%BlendedDiffusion spatially blends the noisy images generated by CLIP \cite{radford2021clip} with the noisy input image using an input mask. However, BlendedDiffusion suffers from low quality when dealing with challenging edits and additionally requires masks as input.
DiffEdit \cite{couairon2022diffedit} automatically generates a mask by computing the differences between the noisy latent generated from an input image and a conditional text prompt. However, because of its inpainting nature, DiffEdit cannot perform global editing operations like converting a photo to an oil painting. MagicMix \cite{liew2022magicmix} interpolates the noisy input latent and the denoised latent to mix the objects that have large semantic differences. Prompt-to-Prompt \cite{hertz2022prompt2prompt} proposes to manipulate the cross-attention maps corresponding to the changes between the input and guided text prompt. However, Prompt-to-Prompt always requires a prompt together with an input image. 
%Shape-guided Diffusion \cite{park2022shape-guided} designs an inside-outside attention mechanism to edit an image based on the provided mask to distinguish the inside from the outside of an object. Although no input text prompt is required, an additional mask is needed to tell which part of the image to modify. 
Concurrent work pix2pix-zero \cite{parmar2023pix2pix_zero} computes an editing direction to edit an image based on a provided prompt, combined with a cross-attention mechanism to preserve the layout of the input image.

Compared to prior works, we propose a generalization that includes multiple previous approaches as special case.  
%as our image editing framework for diffusion models has more control over manipulations of the diffusion path. 
We also highlight a novel manipulation, that is suitable to better perform a wide range of local and global edits than previous methods. Our edits do not require masks or prompting for an input image. Our method is purely based on a pre-trained diffusion model and does not require finetuning.

\section{Method}
\subsection{Preliminaries}
% \begin{itemize}
%     \item TODO Describe the diffusion image generation at a level that enables us to describe the design space TODO
%     \item make sure we define all or almost all variables we need
%     \item (make an overview figure to explain diffusion image generation)(maybe not sure)
% \end{itemize}
% % 1. DDIM sampling steps + classifer free guidance(Biao) 
% % 2. DDIM inversion + null text inversion (Biao)
% $z_t$: the noisy data point at timestep $t$.
% $\epsilon_\theta(z_t, t, c)$: unet, denoising network.
% xxxxxxxxxxxxxxxxxxxxxxxx
\paragraph{Denoising diffusion probabilistic models.}
To train a conditional generative diffusion models, we consider the following objective,
\begin{equation}
    \min_\theta\mathbb{E}_{\mathbf{x}_{0}\sim \mathcal{D}, \boldsymbol\epsilon\sim\mathcal{N}(\mathbf{0}, \mathbf{I}),t\sim U(1, T)}\left\|\boldsymbol\epsilon-\boldsymbol\epsilon_\theta(\mathbf{x}_t, \mathbf{c}, t)\right\|^2,
\end{equation}
where $\mathcal{D}$ is an image dataset (could be raw image pixels or latents obtained from an image autoencoder), $\mathbf{x}_t$ is a noised version of the image $\mathbf{x}_0$, $\mathbf{c}$ is a conditional embedding (\eg, text, class label, image) and $\boldsymbol\epsilon_\theta(\cdot, \cdot, \cdot)$ is a neural network parameterized by $\theta$. After training, we can sample a new image given condition $\mathbf{c}$ with the commonly used DDIM sampler,
\begin{equation}\label{eq:ddim-sampler}
    \begin{aligned}
        \mathbf{x}_{t-1} =& \mathrm{DDIM}(\mathbf{x}_t, \boldsymbol\epsilon_{t}, t) \\
        =& \sqrt{\alpha_{t-1}}\cdot f_\theta(\mathbf{x}_t, \mathbf{c}, t) + \sqrt{1-\alpha_{t-1}}\cdot \boldsymbol\epsilon_\theta(\mathbf{x}_t, \mathbf{c}, t), \\
        % =& \sqrt{\alpha_{t-1}} \cdot \tilde{\mathbf{x}}_{0,t} + \tilde{\mathbf{x}}_t
    \end{aligned}
\end{equation}
where $f_\theta(\mathbf{x}_t, \mathbf{c}, t) = \frac{\mathbf{x}_t - \sqrt{1-\alpha_t}\cdot\boldsymbol\epsilon_\theta(\mathbf{x}_t, \mathbf{c}, t)}{\sqrt{\alpha_t}}$, $\boldsymbol\epsilon_t=\boldsymbol\epsilon_\theta(\mathbf{x}_t, \mathbf{c}, t)$, and $\alpha_t$ is a noise schedule factor as in DDIM. The sampling is to iteratively apply the above equation by giving an initial noise $\mathbf{x}_T\sim\mathcal{N}(\mathbf{0}, \mathbf{I})$. For brevity, we only consider DDIM as our sampler. 
%However, the framework generalizes to any deterministic sampler.
\begin{comment}
\paragraph{Diffusion inversion.}
Similar to GAN inversion, we want to edit an image using its latent representation. Diffusion inversion tries to revert the generating process, \ie, given an image $\mathbf{x}_0$ find the initial noise $\mathbf{x}_T$ (and the intermediate noised images $\mathbf{x}_t$) which generates $\mathbf{x}_0$. The first approach works by adding noise to $\mathbf{x}_0$. The noise schedule is exactly the same as the training noise schedule of DDPM. However, this process is stochastic and we cannot guarantee the obtained initial noise $\mathbf{x}_T$ can reconstruct $\mathbf{x}_0$ faithfully. The second approach is DDIM inversion based on the DDIM sampler in Eq.~\eqref{eq:ddim-sampler}. We can calculate the initial noise by applying the formula iteratively, $\mathbf{x}_{t+1}=\sqrt{\alpha_{t+1}}\cdot f_\theta(\mathbf{x}_t, \mathbf{c}, t) + \sqrt{1-\alpha_{t+1}}\cdot\boldsymbol\epsilon_\theta(\mathbf{x}_t, \mathbf{c}, t)$. But when working with classifier-free guidance, the reconstruction quality is insufficient. The third inversion method is Null-text Inversion~\cite{mokady2022null}. It is based on the DDIM inversion and optimized for high reconstruction quality. In most cases, we will use the Null-text Inversion as our diffusion inversion method.
\end{comment}


\paragraph{Image editing using pre-trained diffusion models.}
Our generalized editing framework MDP edits an image in its latent space also called noise space. Given an image $\mathbf{x}_0$ and the corresponding initial noise $\mathbf{x}_T$, we modify $\mathbf{x}_T$ or the generating process and our aim is to obtain a different $\mathbf{x}_0^{\star}$ which suits our needs. We summarize the symbols used in our later discussions here. One step of diffusion sampling can be represented as
% \begin{equation}\label{eq:diff-step}
    $\begin{cases}
        \boldsymbol\epsilon_t = \boldsymbol\epsilon_\theta(\mathbf{x}_t, \mathbf{c}, t), \\
        \mathbf{x}_t = \mathrm{DDIM}(\mathbf{x}_t, \boldsymbol\epsilon_t, t).
    \end{cases}$
% \end{equation}
We represent the intermediate results of the generating process at timestep $t$ by $\mathbf{x}_t=\mathrm{Gen}(\mathbf{x}_T, \mathbf{c}, t),$ given an initial noise $\mathbf{x}_T$ and a condition $\mathbf{c}$. We also represent all the intermediate outputs by
$\left\{\left(\mathbf{x}_t, \boldsymbol\epsilon_t\right)\right\}_{t=[T,\dots, 0]}=\mathrm{GenPath}\left(\mathbf{x}_T, \mathbf{c}, t\right).$
% where $\boldsymbol\epsilon_t$ means the predicted noise (\ie, output of denoising network $\boldsymbol\epsilon_\theta$) at timestep $t$.
\begin{comment}
\begin{figure}
    \centering
    \include{figures/tex/diffusion}
    \vspace{-25pt}
    \caption{Computation graph of the sampling process in diffusion. Square $\square$ cells are data and circular $\circ$ cells are functions. Editable parts are highlighted with colors.}
    \label{fig:diff-step}
\end{figure}
\end{comment}


\subsection{Design Space for Manipulating the Diffusion Path}
% \begin{itemize}
%     \item What are the main parameters to configure the design space? Multi path: epsilon, xt, c
%     \item The outcome should be a parametric model that enables a user to define an image generation. What are the parameters the user can control in our new framework. Which path to edit (epsilon, xt, c)? How to edit (interpolation factor)? Which timesteps to do the edits? (path schedule)
% \end{itemize}
% Based on the generation step in Eq.~\eqref{eq:diff-step}, we have three typical components that can be edited: intermediate latent, condition and predicted noise.  
Simply switching to another condition during denoising is a very straightforward way to add new semantics to the input image when doing text-guided image editing. We perform a simple experiment in \cref{fig:diffusion-layout-details} to illustrate that the early timesteps in the diffusion process contribute to the layout, while the later timesteps are adding semantics and details to the image. Our goal is to systematically analyze the design space for manipulating the diffusion path over time to perform these types of edits. As result of our analysis, we propose \textbf{MDP} as generalized framework for text-guided image editing and show how concurrent and previous methods fit our framework as special cases. A summary can be found in Table~\ref{tab:manip-design}. Each manipulating operation is formalized as an operator, including 
$$
\begin{aligned}
\mathcal{O}=
\{
&%\mathcal{O}_{\mathrm{INM}}, 
\mathcal{O}_{\mathrm{IDI}}, \mathcal{O}_{\mathrm{IDM}}, \mathcal{O}_{\mathrm{CEI}}, 
\mathcal{O}_{\mathrm{CAM}}, \\&\mathcal{O}_{\mathrm{G}}, \mathcal{O}_{\mathrm{PNI}}, \mathcal{O}_{\mathrm{PNM}}\}.
\end{aligned}
$$
%Besides the manipulation of the initial noise, 
All manipulations in our framework are time-dependent and have parameters, \eg interpolation parameters. MDP allows the user to specify different types of manipulations and different manipulation parameters at each timestep, which we call the manipulation schedule.

Assume we have an image $\mathbf{x}_0^{(A)}$ as input, along with a condition $\mathbf{c}^{(A)}$ that is used to generate $\mathbf{x}_0^{(A)}$ and a new condition $\mathbf{c}^{(B)}$. We first invert the image $\mathbf{x}_0^{(A)}$ to get an initial noise $\mathbf{x}_T$. We identify the following types of manipulations:

\begin{comment}
\paragraph{Initial noise.}
Given two initial (standard Gaussian) noise images $\mathbf{x}_T^{(A)}$ and $\mathbf{x}_T^{(B)}$, the corresponding generated images are $\mathbf{x}_0^{(A)}$ and $\mathbf{x}_0^{(B)}$, respectively.
We can start the diffusion process with an interpolated noise $\mathbf{x}_T^{(\star)}$,
\begin{equation}
    \mathbf{x}^{(\star)}_T=\mathrm{slerp}\left(
        \mathbf{x}_T^{(A)}, \mathbf{x}_T^{(B)}, \omega
    \right),
\end{equation}
where $\mathrm{slerp}$ is the spherical linear interpolation that has been used for sampling in GAN latent spaces~\cite{white2016sampling}. 
In addition, we can use a binary mask $\mathbf{M}$ to control the interpolation of the two noise images,
\begin{equation}
    \mathbf{x}^{(\star)}_0=\mathbf{M} \odot \mathbf{x}_0^{(A)} + (1-\mathbf{M})\odot \mathbf{x}_0^{(B)}
\end{equation}
The interpolated initial noise $\mathbf{x}^{(\star)}_T$ should result in an image $\mathbf{x}_0^{(\star)}$ which has characteristics of both $\mathbf{x}_0^{(A)}$ and $\mathbf{x}_0^{(B)}$. This is the most naive way of editing images.
\end{comment}

\begin{figure}[htb]
    \centering
    %\include{figures/tex/pipeline.tex}
    \includegraphics[width=\linewidth]{figures/pipeline_epsilon.jpg}
    %\vspace{-25pt}
    \caption{Predicted Noise Manipulation. The top branch is inverted from a real image given condition $\textcolor{A}{\mathbf{c}^{(A)}}$ ``\textit{Photo of a rabbit on the grass}''. The bottom branch is generated using condition $\textcolor{B}{\mathbf{c}^{(B)}}$ ``\textit{Photo of a rabbit in a library}''. We copy the predicted noise from step $t_{max}$ to $t_{min}$ of the top branch, then use $\textcolor{B}{\mathbf{c}^{(B)}}$ to denoise and generate the middle images.}
    \label{fig:pred-noise-manip}
\end{figure}
\paragraph{Intermediate denoised output.}
%Given two conditions $\mathbf{c}^{(A)}$ and $\mathbf{c}^{(B)}$, we generate two images $\mathbf{x}_0^{(A)}$ and $\mathbf{x}_0^{(B)}$ using the same initial noise $\mathbf{x}_T$. 
Starting from $\mathbf{x}_T$, we can generate image $\mathbf{x}_0^{(A)}$ using the new condition $\mathbf{x}_0^{(B)}$.
Assume we have all the intermediate latents for the generation of $\mathbf{x}_t^{(A)}$,
% \begin{equation}
% \begin{aligned}
    $\left\{\left(\mathbf{x}_t^{(A)}\right)\right\}_{t=[T,\dots, 0]}=\mathrm{GenPath}\left(\mathbf{x}_T, \mathbf{c}^{(A)}, t\right).$
% \end{aligned}
% \end{equation}
We denote the intermediate latents in the new path as $\mathbf{x}_t^{(\star)}$. We can modify the intermediate outputs $\mathbf{x}_t^{(A)}$ and $\mathbf{x}_t^{(\star)}$ with either linear interpolation or masking.
We can control the resulting image by changing the interpolation factor $w$ (or the binary mask $\mathbf{M}$),
\begin{equation}
\label{inter_denoised}
    \mathbf{x}^{(\star)}_t=\mathrm{lerp}\left(\mathbf{x}_t^{(A)}, \mathbf{x}_t^{(\star)}, \omega\right)
\end{equation}
or
\begin{equation}
    \mathbf{x}^{(\star)}_t=\mathbf{M} \odot \mathbf{x}_t^{(A)} + (1-\mathbf{M})\odot \mathbf{x}_t^{(\star)},
\end{equation}
where when we start this manipulation, $\mathbf{x}_t^{(\star)}=\mathbf{x}_t^{(B)}$. The strategy is used in MagicMix~\cite{liew2022magicmix} and DiffEdit~\cite{couairon2022diffedit}. 

\paragraph{Condition embeddings.}
Given two conditions $\mathbf{c}^{(A)}$ and $\mathbf{c}^{(B)}$, we can change $\mathbf{c}^{(A)}$ at timestep $t$,
\begin{equation}\label{eq:lerp-c}
    \mathbf{c}_t^{(\star)}=\mathrm{lerp}\left(\mathbf{c}^{(A)}, \mathbf{c}^{(B)}, \omega\right),
\end{equation}
where $\mathbf{c}_t^{(\star)}$ will be used as the new condition that embeds the information from both condition $\mathbf{c}^{(A)}$ and $\mathbf{c}^{(B)}$ .

\paragraph{Cross attention.}
Recent condition diffusion models inject condition information $\mathbf{c}$ by cross attending between $\mathbf{c}$ and image features. Prompt-to-Prompt ~\cite{hertz2022prompt2prompt} and Attend-and-Excite \cite{chefer2023attend} find that modifying the cross attention maps can give interesting image editing effects. Here, we do not dig into details of how to manipulate the cross-attention maps. Instead, we simply write the manipulation in functional form $\mathcal{M}$,
\begin{equation}\label{eq:p2p}
    \boldsymbol\epsilon_t^{(\star)}=\mathcal{M}\left(\boldsymbol\epsilon_\theta\left(\mathbf{x}^{(A)}, \cdot, t\right), \mathbf{c}^{(A)}, \mathbf{c}^{(B)}\right).
\end{equation}
% This way is similar to mixing condition embeddings in Eq.~\eqref{eq:lerp-c}.

\paragraph{Guidance.}
Classifier-free guidance~\cite{ho2022classifier} proposed 
\begin{equation}\label{eq:c.f.g.}
    \begin{aligned}
    \boldsymbol\epsilon_t^{(\star)} =& \boldsymbol\epsilon_\theta\left(\mathbf{x}_t, \mathbf{c}^{(A)}, t\right) + \\ 
    & \beta\left(\boldsymbol\epsilon_\theta\left(\mathbf{x}_t, \mathbf{c}^{(A)}, t\right) - \boldsymbol\epsilon_\theta\left(\mathbf{x}_t, \varnothing\phantom{^(}, t\right)\right),
    \end{aligned}
\end{equation}
where $\beta$ is a real number and $\varnothing$ denotes an empty condition. The $\beta$ is often called ``guidance scale''. The term $\boldsymbol\epsilon_\theta\left(\mathbf{x}_t, \varnothing, t\right)$ can be seen as unconditional output of $\boldsymbol\epsilon_\theta$. 
% Intuitively, the Eq.~\eqref{eq:c.f.g.} 
Similar ideas are also used in \cite{bansal2023universal-guidance,liu2023more-control,brack2022stable-artist} for image editing.
This can be generalized to
\begin{equation}\label{eq:guidance}
\begin{aligned}
    \boldsymbol\epsilon_t^{(\star)} = & \boldsymbol\epsilon_\theta\left(\mathbf{x}_t, \mathbf{c}^{(A)}, t\right) + \\
    & \beta\left(\boldsymbol\epsilon_\theta\left(\mathbf{x}_t, \mathbf{c}^{(A)}, t\right) - \boldsymbol\epsilon_\theta\left(\mathbf{x}_t, \mathbf{c}^{(B)}, t\right)\right).
\end{aligned}
\end{equation}
Intuitively, with Eq.~\eqref{eq:guidance}, we want the output to have more characteristics from $\mathbf{c}^{(A)}$ when generating using the new condition $\mathbf{c}^{(B)}$ to preserve the layout.
If $\beta$ is a number between $[-1, 0]$, we obtain this linear interpolation,
\begin{equation}\label{eq:guidance-lerp}
\begin{aligned}
    \boldsymbol\epsilon_t^{(\star)} = \omega\cdot\boldsymbol\epsilon_\theta\left(\mathbf{x}_t, \mathbf{c}^{(A)}, t\right) + (1-\omega)\cdot\boldsymbol\epsilon_\theta\left(\mathbf{x}_t, \mathbf{c}^{(B)}, t\right).
\end{aligned}
\end{equation}





\paragraph{Predicted noise.}
Both the Eq.~\eqref{eq:p2p} and Eq.~\eqref{eq:guidance} are modifying predicted noise $\boldsymbol\epsilon_t$. Inspired by this, we investigate another manipulation. 
% , we investigate another way of modifying $\boldsymbol\epsilon^{(\star)}$.
%Assume that all the intermediate results of generating $\mathbf{x}_0^{(A)}$ and $\mathbf{x}_0^{(B)}$ are pre-computed,
In timestep $t$, we interpolate the predicted noise $\mathbf{x}_t^{(A)}$ with the predicted noise that is using condition $\mathbf{c}^{(B)}$. Assume we have all the predicted noises for the generation of $\mathbf{x}_t^{(A)}$:
\begin{equation}
\begin{aligned}
    \left\{\left(\mathbf{x}_t^{(A)}, \boldsymbol\epsilon_t^{(A)}\right)\right\}_{t=[T,\dots, 0]}=\mathrm{GenPath}\left(\mathbf{x}_T, \mathbf{c}^{(A)}, t\right),\\
    %\left\{\left(\mathbf{x}_t^{(B)}, \boldsymbol\epsilon_t^{(B)}\right)\right\}_{t=[T,\dots, 0]}=\mathrm{GenPath}\left(\mathbf{x}_T, \mathbf{c}^{(B)}, t\right).\\
\end{aligned}
\end{equation}
we can mix the two predicted noises,
\begin{equation}\label{eq:eps-lerp}
\begin{aligned}
    \boldsymbol\epsilon_t^{(\star)} & = \omega\cdot\boldsymbol\epsilon_\theta\left(\mathbf{x}_t^{(A)}, \mathbf{c}^{(A)}, t\right) + (1-\omega)\cdot\boldsymbol\epsilon_\theta\left(\mathbf{x}_t^{(\star)}, \mathbf{c}^{(B)}, t\right)\\
    & = \mathrm{lerp}\left(
        \boldsymbol\epsilon_\theta\left(\mathbf{x}_t^{(A)}, \mathbf{c}^{(A)}, t\right), \boldsymbol\epsilon_\theta\left(\mathbf{x}_t^{(\star)}, \mathbf{c}^{(B)}, t\right), \omega
    \right),
\end{aligned}
\end{equation}
where when we start this manipulation, $\mathbf{x}_t^{(\star)}=\mathbf{x}_t^{(B)}$. We show the pipeline of this method in \cref{fig:pred-noise-manip}.
% \begin{equation}
%     \boldsymbol\epsilon_t^{(\star)}=\mathrm{lerp}\left(\boldsymbol\epsilon_t^{(A)}, \boldsymbol\epsilon_t^{(B)}, \omega\right)
% \end{equation}

\begin{table*}[!htbp]
    \centering
    \resizebox{\linewidth}{!}{%
    \def\arraystretch{1.15}\tabcolsep=0.01em
    \begin{tabular}{cccc|c|c}
        % \toprule
        % \hline[3pt]
        \specialrule{1pt}{1pt}{1pt}
        & Pre & Changes & Post &  Operator & Exist. Methods\\
        \hline\hline
        % \cellcolor{aabbcc!10}Initial Noise Interp. & 
        % \multirow{2}{*}{$
        % \begin{cases}
        % \textcolor{A}{\mathbf{x}_T^{(A)}}\sim\mathcal{N}(\mathbf{0}, \mathbf{I})\\
        % \textcolor{B}{\mathbf{x}_T^{(B)}}\sim\mathcal{N}(\mathbf{0}, \mathbf{I})
        % \end{cases}
        % $}
        % & $\textcolor{star}{\mathbf{x}^{(\star)}_0}=\mathrm{slerp}\left(\textcolor{A}{\mathbf{x}_T^{(A)}}, \textcolor{B}{\mathbf{x}_T^{(B)}}, \omega
        % \right)$ & 
        %     \multirow{2}{*}{$\begin{cases}
        %         \textcolor{star}{\boldsymbol\epsilon_T^{(\star)}} =\boldsymbol\epsilon_\theta\left(\textcolor{star}{\mathbf{x}^{(\star)}_T}, \mathbf{c}, T\right) \\
        %         \textcolor{star}{\mathbf{x}^{(\star)}_{T-1}}=\mathrm{DDIM}\left(\textcolor{star}{\textcolor{star}{\mathbf{x}^{(\star)}_T},\boldsymbol\epsilon_T^{(\star)}},T\right)\\
        %     \end{cases}$}
        % & $\textcolor{star}{\mathbf{x}^{(\star)}_{T-1}}=\mathcal{O}_{\mathrm{INI}}\left(\textcolor{A}{\mathbf{x}_T^{(A)}}, \textcolor{B}{\mathbf{x}_T^{(B)}}, \mathbf{c}, \omega, T\right)$
        % &\\ %[0.3cm]
        % \cellcolor{aabbcc!10}Initial Noise Masking & 
        % & $\textcolor{star}{\mathbf{x}^{(\star)}_0}=\mathbf{M} \odot \textcolor{A}{\mathbf{x}_0^{(A)}} + (1-\mathbf{M})\odot \textcolor{B}{\mathbf{x}_0^{(B)}}$ 
        % &  
        % & 
        % $\textcolor{star}{\mathbf{x}^{(\star)}_{T-1}}=\mathcal{O}_{\mathrm{INM}}\left(\textcolor{A}{\mathbf{x}_T^{(A)}}, \textcolor{B}{\mathbf{x}_T^{(B)}}, \mathbf{c}, \mathbf{M}, T\right)$
        % &\\ \hline\hline
        \cellcolor{aaccff!10}Inter. Denoised Interp. & 
        \multirow{2}{*}{$
        \begin{cases}
        % \mathbf{x}_T\sim\mathcal{N}(\mathbf{0}, \mathbf{I})\\
        \textcolor{A}{\mathbf{x}_t^{(A)}}=\mathrm{GenPath}\left(\mathbf{x}_T, \textcolor{A}{\mathbf{c}^{(A)}}, t\right)\\
        %\textcolor{B}{\mathbf{x}_t^{(B)}}=\mathrm{Gen}\left(\mathbf{x}_T, \textcolor{A}{\mathbf{c}^{(B)}}, t\right)
        \end{cases}
        $}
        & $\textcolor{star}{\mathbf{x}^{(\star)}_t}=\mathrm{lerp}\left(\textcolor{A}{\mathbf{x}_t^{(A)}}, \textcolor{star}{\mathbf{x}_t^{(\star)}}, \omega\right)$ & 
            \multirow{2}{*}{$\begin{cases}
                \textcolor{star}{\boldsymbol\epsilon_t^{(\star)}} =\boldsymbol\epsilon_\theta\left(\textcolor{star}{\mathbf{x}^{(\star)}_t}, \textcolor{B}{\mathbf{c}^{(B)}}, t\right) \\
                \textcolor{star}{\mathbf{x}^{(\star)}_{t-1}}=\mathrm{DDIM}\left(\textcolor{star}{\textcolor{star}{\mathbf{x}^{(\star)}_t},\boldsymbol\epsilon_t^{(\star)}},t\right)\\
            \end{cases}$}
        & 
        $\textcolor{star}{\mathbf{x}^{(\star)}_{t-1}}=\mathcal{O}_{\mathrm{IDI}}\left(\textcolor{A}{\mathbf{x}_t^{(A)}}, \textcolor{star}{\mathbf{x}_t^{(\star)}}, \textcolor{A}{\mathbf{c}_t^{(A)}}, \textcolor{B}{\mathbf{c}_t^{(B)}}, \omega, t\right)$
        & \eg, MagicMix~\cite{liew2022magicmix}\\ %[0.3cm]
        \cellcolor{aaccff!10}Inter. Denoised Masking & & $\textcolor{star}{\mathbf{x}^{(\star)}_t}=\mathbf{M} \odot \textcolor{A}{\mathbf{x}_t^{(A)}} + (1-\mathbf{M})\odot \textcolor{star}{\mathbf{x}_t^{(\star)}}$ & & 
        $\textcolor{star}{\mathbf{x}^{(\star)}_{t-1}}=\mathcal{O}_{\mathrm{IDM}}\left(\textcolor{A}{\mathbf{x}_t^{(A)}}, \textcolor{star}{\mathbf{x}_t^{(\star)}}, \textcolor{A}{\mathbf{c}_t^{(A)}}, \textcolor{B}{\mathbf{c}_t^{(B)}}, \mathbf{M}, t\right)$
        &\eg, DiffEdit~\cite{couairon2022diffedit} \\ \hline\hline
        \cellcolor{aaddee!10}Condition Emb. Interp. & 
            % \multirow{2}{*}{
            $\begin{cases}
                % \mathbf{x}_T\sim\mathcal{N}(\mathbf{0}, \mathbf{I})\\
                \textcolor{A}{\mathbf{x}_t^{(A)}} = \mathrm{Gen}(\mathbf{x}_T, \textcolor{A}{\mathbf{c}^{(A)}}, t)
            \end{cases}$
            % }
        & $\textcolor{star}{\mathbf{c}^{(\star)}}=\mathrm{lerp}\left(\textcolor{A}{\mathbf{c}^{(A)}}, \textcolor{B}{\mathbf{c}^{(B)}}, \omega\right)$ & 
        $\begin{cases}
            \textcolor{star}{\boldsymbol\epsilon_t^{(\star)}} =\boldsymbol\epsilon_\theta\left(\textcolor{A}{\mathbf{x}^{(A)}_t}, \textcolor{B}{\textcolor{star}{\mathbf{c}^{(\star)}}}, t\right) \\
            \textcolor{star}{\mathbf{x}^{(\star)}_{t-1}}=\mathrm{DDIM}\left(\textcolor{star}{\textcolor{A}{\mathbf{x}^{(A)}_t},\boldsymbol\epsilon_t^{(\star)}},t\right)\\
        \end{cases}$
        & 
        $\textcolor{star}{\mathbf{x}^{(\star)}_{t-1}}=\mathcal{O}_{\mathrm{CEI}}\left(\textcolor{A}{\mathbf{x}^{(A)}_{t}}, \textcolor{A}{\mathbf{c}_t^{(A)}}, \textcolor{B}{\mathbf{c}_t^{(B)}}, \omega,t\right)$
        &\\ \hline\hline
        % Prompt Swap & & $\mathbf{c}^{(A)}\leftarrow \mathbf{c}^{(B)}$ & \\[0.3cm]
        \cellcolor{aabbee!10}Cross Attn. Manip. & 
        $\begin{cases}
            % \mathbf{x}_T\sim\mathcal{N}(\mathbf{0}, \mathbf{I})\\
            \textcolor{A}{\mathbf{x}_t^{(A)}} = \mathrm{Gen}(\mathbf{x}_T, \textcolor{A}{\mathbf{c}^{(A)}}, t)
        \end{cases}$ 
        & $\textcolor{star}{\boldsymbol\epsilon_t^{(\star)}}=\mathcal{M}\left(\boldsymbol\epsilon_\theta(\textcolor{A}{\mathbf{x}_t^{(A)}}, \cdot, t), \textcolor{A}{\mathbf{c}^{(A)}}, \textcolor{B}{\mathbf{c}^{(B)}}, t\right)$ & 
        $\begin{cases}
            % \textcolor{star}{\boldsymbol\epsilon_t^{(\star)}} =\boldsymbol\epsilon_\theta\left(\textcolor{A}{\mathbf{x}^{(A)}_t}, \textcolor{B}{\textcolor{star}{\mathbf{c}^{(\star)}}}, t\right) \\
            \textcolor{star}{\mathbf{x}^{(\star)}_{t-1}}=\mathrm{DDIM}\left(\textcolor{star}{\textcolor{A}{\mathbf{x}^{(A)}_t},\boldsymbol\epsilon_t^{(\star)}},t\right)\\
        \end{cases}$
        & 
        $\textcolor{star}{\mathbf{x}^{(\star)}_{t-1}}=\mathcal{O}_{\mathrm{CAM}}\left(\textcolor{A}{\mathbf{x}^{(A)}_{t}}, \textcolor{A}{\mathbf{c}_t^{(A)}}, \textcolor{B}{\mathbf{c}_t^{(B)}}, t\right)$
        &\eg, P2P~\cite{hertz2022prompt2prompt}, A\&E~\cite{chefer2023attend} \\ \hline\hline
        % Est. Image Interp. & \textcolor{brown}{TODO:}& $\tilde{\mathbf{x}}_{0,t}^{(\star)}=\mathrm{lerp}\left(\tilde{\mathbf{x}}_{0,t}^{(A)}, \tilde{\mathbf{x}}_{0,t}^{(B)}, \omega\right)$ & \\ \midrule\midrule
        \cellcolor{aabbff!10}Guidance & 
        % \multirow{2}{*}{
        $\begin{cases}
        % \textcolor{A}{\boldsymbol\epsilon_t^{(A)}}=\boldsymbol\epsilon_\theta\left(\textcolor{A}{\mathbf{x}_t^{(A)}}, \textcolor{A}{\mathbf{c}^{(A)}}, t\right)
        \mathbf{x}_T\sim\mathcal{N}(\mathbf{0}, \mathbf{I})\\
        \end{cases}$
        % }
        & 
        $\begin{cases}
            \textcolor{star}{\boldsymbol\epsilon_t^{A\star}}=\boldsymbol\epsilon_\theta\left(\textcolor{star}{\mathbf{x}_t^{(\star)}}, \textcolor{A}{\mathbf{c}^{(A)}}, t\right)\\
            \textcolor{star}{\boldsymbol\epsilon_t^{B\star}}=\boldsymbol\epsilon_\theta\left(\textcolor{star}{\mathbf{x}_t^{(\star)}}, \textcolor{B}{\mathbf{c}^{(B)}}, t\right)\\
            \textcolor{star}{\boldsymbol\epsilon_t^{\star}}=(1+\beta)\cdot\textcolor{star}{\boldsymbol\epsilon_t^{A\star}} -\beta \cdot \textcolor{star}{\boldsymbol\epsilon_t^{B\star}}
        \end{cases}$
        &         
        $\begin{cases}
            % \textcolor{star}{\boldsymbol\epsilon_t^{(\star)}} =\boldsymbol\epsilon_\theta\left(\textcolor{star}{\mathbf{x}^{(\star)}_t}, \textcolor{B}{\textcolor{star}{\mathbf{c}^{(\star)}}}, t\right) \\
            \textcolor{star}{\mathbf{x}^{(\star)}_{t-1}}=\mathrm{DDIM}\left(\textcolor{star}{\textcolor{star}{\mathbf{x}^{(\star)}_t},\boldsymbol\epsilon_t^{(\star)}},t\right)\\
        \end{cases}$
        & 
        $\textcolor{star}{\mathbf{x}^{(\star)}_{t-1}}=\mathcal{O}_{\mathrm{G}}\left(\textcolor{star}{\mathbf{x}^{(\star)}_{t}}, \textcolor{A}{\mathbf{c}_t^{(A)}}, \textcolor{B}{\mathbf{c}_t^{(B)}}, t\right)$
        &\eg, C.F.G.~\cite{ho2022classifier}\\
        % Pred. Noise Masking & & & & \\
        \hline\hline
        % Pred. Noise Swap & & $\boldsymbol\epsilon_t^{(A)}\leftarrow \boldsymbol\epsilon_t^{(B)}$ & & \\[0.3cm]
        \cellcolor{aabbff!10}Pred. Noise Interp.& 
        \multirow{2}{*}{
        $\begin{cases}
        % \mathbf{x}_T\sim\mathcal{N}(\mathbf{0}, \mathbf{I})\\
        \{
        (\textcolor{A}{\mathbf{x}_t^{(A)}}, \textcolor{A}{\boldsymbol\epsilon_t^{(A)}}
        )\}_{t=[T,\dots, 0]}=\mathrm{GenPath}\left(\mathbf{x}_T, \textcolor{A}{\mathbf{c}^{(A)}}, t\right)\\
        %\{
        %(\textcolor{B}{\mathbf{x}_t^{(B)}}, \textcolor{B}{\boldsymbol\epsilon_t^{(B)}})\}_{t=[T,\dots, 0]}=\mathrm{GenPath}\left(\mathbf{x}_T, \textcolor{B}{\mathbf{c}^{(B)}}, t\right)
        \end{cases}$
        }
        & $\textcolor{star}{\boldsymbol\epsilon_t^{(\star)}}=\mathrm{lerp}\left(\textcolor{A}{\boldsymbol\epsilon_t^{(A)}}, \textcolor{star}{\boldsymbol\epsilon_t^{(\star)}}, \omega\right)$ & 
        \multirow{2}{*}{$\begin{cases}
            % \textcolor{star}{\boldsymbol\epsilon_t^{(\star)}} =\boldsymbol\epsilon_\theta\left(\textcolor{star}{\mathbf{x}^{(\star)}_t}, \textcolor{B}{\mathbf{c}^{(B)}}, t\right) \\
            \textcolor{star}{\mathbf{x}^{(\star)}_{t-1}}=\mathrm{DDIM}\left(\textcolor{A}{\mathbf{x}^{(A)}_t},\textcolor{star}{\boldsymbol\epsilon_t^{(\star)}},t\right)\\
        \end{cases}$}
        & 
        $\textcolor{star}{\mathbf{x}^{(\star)}_{t-1}}=\mathcal{O}_{\mathrm{PNI}}\left(\textcolor{A}{\mathbf{x}^{(A)}_{t}}, \textcolor{A}{\mathbf{c}_t^{(A)}}, \textcolor{B}{\mathbf{c}_t^{(B)}}, \omega, t\right)$
        &\\ %[0.3cm]
        \cellcolor{aabbff!10}Pred. Noise Masking & & $\textcolor{star}{\boldsymbol\epsilon_t^{(\star)}}=\mathbf{M} \odot \textcolor{A}{\boldsymbol\epsilon_t^{(A)}} + (1-\mathbf{M})\odot \textcolor{star}{\boldsymbol\epsilon_t^{(\star)}}$ & & 
        $\textcolor{star}{\mathbf{x}^{(\star)}_{t-1}}=\mathcal{O}_{\mathrm{PNM}}\left(\textcolor{A}{\mathbf{x}^{(A)}_{t}}, \textcolor{A}{\mathbf{c}_t^{(A)}}, \textcolor{B}{\mathbf{c}_t^{(B)}}, \mathbf{M}, t\right)$
        &\\ %[0.3cm] %\midrule\midrule
        % \bottomrule
        % \hline[3pt]
        \specialrule{1pt}{1pt}{1pt}
    \end{tabular}
    }
    \vspace{-8pt}
    \caption{We use the superscript $\textcolor{A}{^{(A)}}$ to represent outputs obtained from condition $\textcolor{A}{\mathbf{c}^{(A)}}$ and $\textcolor{B}{^{(B)}}$ for condition $\textcolor{B}{\mathbf{c}^{(B)}}$. The edited outputs are represented with superscript $\textcolor{star}{^{(\star)}}$. $\omega$ is a number between $0$ and $1$. $\beta$ is usually a positive real number.}
    \label{tab:manip-design}
\end{table*}

\begin{comment}
\subsection{Taxonomy of Image Editing Applications}
In this subsection we describe a small taxonomy for image editing applications that we use to analyze our framework and compare it to previous work. We also propose a small benchmark of edits to analyze in supplementary materials. Given either a real or synthetic image as input, we edit the image following a condition such as a text prompt or a class label. We divide all common image editing operations into two categories: local editing and global editing. We further identify sub-categories for both local and global editing.
\subsubsection{Local editing}
Typically, we want to perform edits while keeping the overall image layout, \eg background or the shape of the chosen object in the input image.
\begin{itemize}
    \item Changing object: change an object (or objects) in the image to another one. For example, if there is a basket of \textit{apples}, we may change it to a basket of \textit{oranges}. Also, we may change the image of a \textit{dog} to an image of a \textit{cat}.
    \item Adding object: add an object that does not exist in the original image. For example, given an image of a forest, we add a \textit{car} in that forest. Given a cat face, we can add a pair of \textit{sunglasses} on it.
    \item Removing object: remove an object from the image. For example, we can remove the eggs in a basket so that the basket becomes \textit{empty}. 
    \item Changing attribute: change the attribute such as color and texture of an object. Given a \textit{red} bird, we may change it to a \textit{blue} bird. We may change a human portrait from a \textit{young} person to an \textit{old} person.
    \item Mixing objects: combine an object in the input image with another object. The two objects we may want to mix can have different semantics, \eg we can mix a \textit{corgi} and a \textit{coffee machine}, or a \textit{chocolate bar} and a \textit{purse}.  
\end{itemize}
\subsubsection{Global editing}
For global editing, the overall texture and style of the image can be changed while the layout and semantics should be the same.
\begin{itemize}
    \item Changing background: change the background while keeping a foreground object untouched. For example, we change a rabbit on \textit{grass} to a rabbit on the \textit{moon}. We can also convert a black-and-white photo to be \textit{colored}. 
    \item Stylization: stylize an object or a scene using an input style image. For example, we give an \textit{futuristic} image as a style input and we want the a generated image to have the same style.
    \item In-domain transfer: edit the input image by performing in-domain changes. For example, we transfer a photo of a valley in \textit{summer} to a photo of a valley in \textit{winter}, or we transfer a city during the \textit{day} to a city at \textit{night}. 
    \item Out-of-domain transfer: edit the input image with out-of-domain changes. For example, we transfer a \textit{photo} of a valley to an \textit{oil painting} of a valley or we change a \textit{portrait} of a human to a \textit{cartoon} character. We can also change a \textit{black-and-white} photo to a \textit{colored} photo. 
\end{itemize}
\end{comment}

\section{Experiments and Analysis}
\subsection{Settings}
\paragraph{Editing tasks.}
We describe a small taxonomy for image editing applications that we use to analyze our framework and compare it to previous work. We divide all common image editing operations into two categories: local editing and global editing. Local edits include: changing object, adding object, removing object, changing attribute and mixing objects. Global edits include: changing background, in-domain transfer, out-domain transfer, and stylization. We provide the details in the Supplementary Materials.
\paragraph{Our manipulations and baselines.}
Given a real input image $\mathbf{x}_0^{(A)}$, we first use Null-text Inversion \cite{mokady2022null} together with a text-condition $\mathbf{c}^{(A)}$ to obtain an initial noise tensor $\mathbf{x}_T$. 
%$\mathbf{c}^{(A)}$ can either be empty or a prompt that describes the content of $\mathbf{x}_0^{(A)}$. 
Another given condition $\mathbf{c}^{(B)}$ that is used to guide the editing process would induce the generation of output image $\mathbf{x}_0^{(B)}$ starting from the same initial noise tensor $\mathbf{x}_T$. 
%As the early denoising steps mostly contribute to the generation of the layout, we use the condition from $\mathbf{x}_0^{(A)}$ in early steps while denoising using new condition $\mathbf{c}^{(B)}$ at later steps. 
As we do not use a mask as an input, we only consider linear operations that do not involve a mask:
\begin{itemize}
\item MDP-$x_t$: intermediate latent interpolation ($\mathcal{O}_{\mathrm{IDI}}$). We linearly interpolate the intermediate latents $\mathbf{x}_t^{(A)}$ and $\mathbf{x}_t^{(B)}$ using Eq. \eqref{inter_denoised}. This manipulation differs from MagicMix \cite{liew2022magicmix} by using Null-text Inversion to obtain the initial noise tensor $\mathbf{x}_T$ rather than directly adding noise. 
\item MDP-$c$: conditional embedding interpolation ($\mathcal{O}_{\mathrm{CEI}}$). We interpolate the condition $\mathbf{c}^{(A)}$ and $\mathbf{c}^{(B)}$ by using Eq. \eqref{eq:lerp-c}.
\item P2P (Prompt-to-Prompt) \cite{hertz2022prompt2prompt}: cross attention manipulation ($\mathcal{O}_{\mathrm{CAM}}$). P2P manipulates the cross attention maps and self attention maps according to the changes in the new text prompt compared to the input prompt.
\item MDP-$\beta$: guidance ($\mathcal{O}_{\mathrm{G}}$). We use condition $\mathbf{c}^{(A)}$ as a guidance to inject the layout while generating new semantics by using Eq. \eqref{eq:guidance-lerp}.
%a simple way to do image-to-image editing with Stable Diffusion.  SD is to first add noise to the input image with a certain strength and then use the new condition to denoise the noisy input.
\item MDP-$\epsilon_t$: predicted noise interpolation ($\mathcal{O}_{\mathrm{PNI}}$). We mix the predicted noises when generating input image $\mathbf{x}_0^{(A)}$ with the noises using condition $\mathbf{c}^{(B)}$ by following Eq. \eqref{eq:eps-lerp}. 
%Compared to manipulating $x_t$ and $c$, this manipulation is the strongest one to inject the layout from $\mathbf{x}_t^{(A)}$, as it directly reuses the early predicted noises when generating $\mathbf{x}_t^{(A)}$, while early predicted noises of the other two manipulations still contain the information under condition $\mathbf{c}^{(B)}$.
\end{itemize}
\paragraph{Manipulation schedule.} We investigate how the methods work under a simple schedule using default settings. 
%For SD, we manipulate the strength parameter in its default setting; 
%For P2P and our manipulations, we do the following: 
We start the manipulation at step $t_{max}$ and end at step $t_{min}$. The total number of timesteps of a manipulation is denoted as $T_M = t_{max} - t_{min}$. We perform edits only during these $T_M$ steps. For the remaining steps, we simply use $\mathbf{c}^{(B)}$ to denoise the noisy latent. We vary $t_{max}$ and $t_{min}$ and manually select the best result for each method. 
We fix the interpolation factors (guiding scale for MDP-$\beta$) for the following concerns: 
\begin{enumerate*}[label=(\arabic*)]
\item We observe that setting the manipulation schedule and interpolation factor can both adjust the degree to which the layout is maintained in the edited image to some extent;
\item We want to find a good default setting of interpolation factors for each manipulation;
\item When setting the interpolation factor $\beta = 0$ for MDP-$\beta$, this manipulation is equal to MDP-$c$, which we want to avoid in order to show the characteristic of each manipulation.
\end{enumerate*}
We therefore empirically fix the interpolation factor of MDP-$x_t$, MDP-$c$, P2P, MDP-$\beta$ and MDP-$\epsilon_t$ to be 0.7, 1, 1, -0.3, and 1, respectively. In general, $t_{max}$ is ranging from 0 to 5 while $T_M$ is set to be around 20 for each manipulation. Individual settings for each manipulation differ to obtain the desired editing result. We analyze how the methods work under various manipulation schedules in the Supplementary Materials.
\vspace{-3mm}
\paragraph{Implementation.} For text-guided editing, we test all the methods using the publicly available latent diffusion model Stable Diffusion \footnote{\href{https://huggingface.co/CompVis/stable-diffusion-v1-4}{https://huggingface.co/CompVis/stable-diffusion-v1-4}}. For class-guided editing, we use the conditional latent diffusion model \cite{rombach2022latentdiffusion} trained on ImageNet \cite{deng2009imagenet}. We test our manipulations on one NVIDIA A100 GPU. As no training and finetuning is required, each manipulation can be generally done within 10 seconds. As the inversion method we use is built on top of the DDIM sampler \cite{mokady2022null}, we also use DDIM sampler during the sampling process. However, our method can adopt other deterministic samplers.
\subsection{Editing results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%Class-guided
\begin{figure}
  \begin{subfigure}[t]{0.19\linewidth}
    \includegraphics[width=\linewidth]{figures/class_label_jpg/2_input.jpg}
    \caption*{Input}
\end{subfigure}
  \begin{subfigure}[t]{0.19\linewidth}
    \includegraphics[width=\linewidth]{figures/class_label_jpg/2_2.jpg}
    \caption*{\textit{Dog}}
\end{subfigure}
  \begin{subfigure}[t]{0.19\linewidth}
    \includegraphics[width=\linewidth]{figures/class_label_jpg/2_3.jpg}
    \caption*{\textit{Fox}}
\end{subfigure}
  \begin{subfigure}[t]{0.19\linewidth}
    \includegraphics[width=\linewidth]{figures/class_label_jpg/2_4.jpg}
    \caption*{\textit{Wolf}}
\end{subfigure}
  \begin{subfigure}[t]{0.19\linewidth}
    \includegraphics[width=\linewidth]{figures/class_label_jpg/2_1.jpg}
    \caption*{\textit{Bear}}
\end{subfigure}
\quad
  \begin{subfigure}[t]{0.19\linewidth}
    \includegraphics[width=\linewidth]{figures/class_label_jpg/3_input.jpg}
    \caption*{Input}
\end{subfigure}
  \begin{subfigure}[t]{0.19\linewidth}
    \includegraphics[width=\linewidth]{figures/class_label_jpg/3_1.jpg}
    \caption*{\textit{Plate}}
\end{subfigure}
  \begin{subfigure}[t]{0.19\linewidth}
    \includegraphics[width=\linewidth]{figures/class_label_jpg/3_3.jpg}
    \caption*{\textit{Clock}}
\end{subfigure}
  \begin{subfigure}[t]{0.19\linewidth}
    \includegraphics[width=\linewidth]{figures/class_label_jpg/3_7.jpg}
    \caption*{\textit{Lemon}}
\end{subfigure}
  \begin{subfigure}[t]{0.19\linewidth}
    \includegraphics[width=\linewidth]{figures/class_label_jpg/3_6.jpg}
    \caption*{\textit{Pizza}}
\end{subfigure}
\quad
  \begin{subfigure}[t]{0.19\linewidth}
    \includegraphics[width=\linewidth]{figures/class_label_jpg/1_input.jpg}
    \caption*{Input}
\end{subfigure}
  \begin{subfigure}[t]{0.19\linewidth}
    \includegraphics[width=\linewidth]{figures/class_label_jpg/1_1.jpg}
    \caption*{\textit{Agaric}}
\end{subfigure}
  \begin{subfigure}[t]{0.19\linewidth}
    \includegraphics[width=\linewidth]{figures/class_label_jpg/1_4.jpg}
    \caption*{\textit{Purse}}
\end{subfigure}
  \begin{subfigure}[t]{0.19\linewidth}
    \includegraphics[width=\linewidth]{figures/class_label_jpg/1_6.jpg}
    \caption*{\textit{Teapot}}
\end{subfigure}
  \begin{subfigure}[t]{0.19\linewidth}
    \includegraphics[width=\linewidth]{figures/class_label_jpg/1_3.jpg}
    \caption*{\textit{Printer}}
\end{subfigure}
\vspace{-8pt}
\caption{Class-guided image editing. We synthesize the input image using one class label, then we use another category from ImageNet to edit the input image. We use MDP-$\epsilon_t$ to demonstrate the ability to do the class-guided editing.}
\label{fig:class}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%global_editing_jpg_comparison
\begin{figure}
    % \centering
    % \begin{overpic}[width=0.1\textwidth,rel=20]{figures/global_editing_jpg/changing_background/001_input.jpg}
    %     \linethickness{1.5pt}
    %     % \put(25, 15){\color{red}\vector(1,0){20}}
    % \end{overpic}  
    \centering
    \begin{tikzpicture}
        \node (small) {
            \includegraphics[width=0.1\linewidth]{figures/global_editing_jpg/changing_background/001_input.jpg}
            \put(-23,28){\rotatebox[]{0}{Input}}
        };

        \node [right=0.1cm of small] (large) {
            \begin{overpic}[width=0.8\linewidth]{figures/comparison/comparison.jpg}
                \put(0,43){\color{black}\vector(1,0){100}}
                \put(0,-1){\color{black}\vector(1,0){100}}
                \put(102,33){\rotatebox[]{0}{$x_t$}}
                \put(102,20){\rotatebox[]{0}{$c$}}
                \put(102,8){\rotatebox[]{0}{$\epsilon_t$}}
                \put(10,-7){\rotatebox[]{0}{Manipulation starts from early to late}}
            \end{overpic} 
        };
    \end{tikzpicture}
    \vspace{2mm}
    \caption{We change the background of the input image from ``grass'' to ``library''. We apply MDP-$x_t$, MDP-$c$, and MDP-$\epsilon_t$. The manipulation range $T_M$ is set to be 10, 20, and 20 respectively, which are the best settings for each method. From left to right, $t_{max}$ is progressing as $50, 49, ..., 44$. Manipulation $\epsilon_t$ can do faithful edits across different manipulation ranges while the other two methods fail.}
    \label{fig:comparison}
    \vspace{-5mm}
\end{figure}

\subsubsection{Local editing}
We show examples of local edits for changing object, adding object, removing object, changing attribute, and mixing objects in \cref{fig:local-editing}. The edits by MDP-$\beta$ are somewhat reasonable, but the overall layout is not well preserved. In general, all the other manipulations can do the edits guided by the text prompt while preserving the background of the input image. As the initial diffusion generation steps contribute to the layout of the generated image, we thus recommend to start the manipulation at the early diffusion stage, usually $t_s$ can range from 50 to 45, and $T_M$ can be ranging from 15 to 25. For larger $T_M$ the editing effect is stronger. However, for MDP-$\beta$, even if we do the manipulation under $T_M = 50$, the layout is still not well-preserved. We actually observe that when setting $\beta = 0$ and $t_{max} = 0$, MDP-$\beta$ can do the desired edits; however as we discussed before, MDP-$\beta$ is equal to MDP-$c$ with interpolation factor $w=1$. This is not the case we want to showcase for MDP-$\beta$. 
For MDP-$x_t$ and MDP-$\epsilon_t$, editing can start at later steps, \ie $t_{max}$ can be chosen to be smaller, as more layout is preserved during editing. 
Additionally, we observe some interesting findings that Prompt-to-Prompt fails for edits that remove objects. 
%Either the overall layout is not well-preserved or the objects are not faithfully removed. 
For the application of mixing objects, as there is no universal standard of what the mixed object should look like, we provide more examples generated by each method in the Supplementary Materials. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!htbp]
  \centering
  \begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/local_editing_jpg/changing_object/003_input.jpg}};
    \draw (0, 1.55) node {Input};
    \node (A) at (1.4, -0.8) {};
    \node (B) at (3, -0.8) {};
    \draw[->] (A) edge (B);
    \node[align=center] (C) at (2.2, 0.2) {``Cat''\\to\\``Tiger''};
  \end{tikzpicture}
  \begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/local_editing_jpg/changing_object/003_xt.jpg}};
    \draw (0, 1.55) node {MDP-$x_t$};
  \end{tikzpicture}
  \begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/local_editing_jpg/changing_object/003_c.jpg}};
    \draw (0, 1.55) node {MDP-$c$};
  \end{tikzpicture}
  \begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/local_editing_jpg/changing_object/003_p2p.jpg}};
    \draw (0, 1.55) node {P2P};
  \end{tikzpicture}
  \begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/local_editing_jpg/changing_object/003_guidance.jpg}};
    \draw (0, 1.55) node {MDP-$\beta$};
  \end{tikzpicture}
  \begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/local_editing_jpg/changing_object/003_epsilon.jpg}};
    \draw (0, 1.55) node {\textbf{MDP-$\epsilon_t$}};
  \end{tikzpicture}
\quad
\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{tikzpicture}
  \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/local_editing_jpg/changing_object/005_input.jpg}};
    \node (A) at (1.4, -0.8) {};
    \node (B) at (3, -0.8) {};
    \draw[->] (A) edge (B);
    \node[align=center] (C) at (2.2, 0.2) {``Train''\\to\\``Coach''};
  \end{tikzpicture}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/changing_object/005_xt.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/changing_object/005_c.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/changing_object/005_p2p.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/changing_object/005_guidance.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/changing_object/005_epsilon.jpg}
\end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{tikzpicture}
  \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/local_editing_jpg/adding_object/001_input.jpg}};
    \node (A) at (1.4, -0.8) {};
    \node (B) at (3, -0.8) {};
    \draw[->] (A) edge (B);
    \node[align=center] (C) at (2.2, 0.2) {Add\\``Car''};
  \end{tikzpicture}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/adding_object/001_xt.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/adding_object/001_c.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/adding_object/001_p2p.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/adding_object/001_guidance.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/adding_object/001_epsilon.jpg}
\end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{tikzpicture}
  \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/local_editing_jpg/adding_object/003_input.jpg}};
    \node (A) at (1.4, -0.8) {};
    \node (B) at (3, -0.8) {};
    \draw[->] (A) edge (B);
    \node[align=center] (C) at (2.2, 0.2) {Add\\``Glasses''};
  \end{tikzpicture}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/adding_object/003_xt.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/adding_object/003_c.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/adding_object/003_p2p.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/adding_object/003_guidance.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/adding_object/003_epsilon.jpg}
\end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{tikzpicture}
  \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/local_editing_jpg/removing_object/004_input.jpg}};
    \node (A) at (1.4, -0.8) {};
    \node (B) at (3, -0.8) {};
    \draw[->] (A) edge (B);
    \node[align=center] (C) at (2.2, 0.2) {Remove\\``Graffiti''};
  \end{tikzpicture}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/removing_object/004_xt.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/removing_object/004_c.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/removing_object/004_p2p.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/removing_object/004_guidance.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/removing_object/004_epsilon.jpg}
\end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{tikzpicture}
  \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/local_editing_jpg/removing_object/005_input.jpg}};
    \node (A) at (1.4, -0.8) {};
    \node (B) at (3, -0.8) {};
    \draw[->] (A) edge (B);
    \node[align=center] (C) at (2.2, 0.2) {Remove\\``Cakes''};
  \end{tikzpicture}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/removing_object/005_xt.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/removing_object/005_c.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/removing_object/005_p2p.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/removing_object/005_guidance.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/removing_object/005_epsilon.jpg}
\end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{tikzpicture}
  \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/local_editing_jpg/changing_attributes/001_input.jpg}};
    \node (A) at (1.4, -0.8) {};
    \node (B) at (3, -0.8) {};
    \draw[->] (A) edge (B);
    \node[align=center] (C) at (2.2, 0.2) {``Chocolate''\\to\\``Straw\\-berry''};
  \end{tikzpicture}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/changing_attributes/001_xt.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/changing_attributes/001_c.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/changing_attributes/001_p2p.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/changing_attributes/001_guidance.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/changing_attributes/001_epsilon.jpg}
\end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{tikzpicture}
  \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/local_editing_jpg/changing_attributes/002_input.jpg}};
    \node (A) at (1.4, -0.8) {};
    \node (B) at (3, -0.8) {};
    \draw[->] (A) edge (B);
    \node[align=center] (C) at (2.2, 0.2) {``Woman''\\to\\``Kid''};
  \end{tikzpicture}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/changing_attributes/002_xt.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/changing_attributes/002_c.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/changing_attributes/002_p2p.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/changing_attributes/002_guidance.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/changing_attributes/002_epsilon.jpg}
\end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{tikzpicture}
  \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/local_editing_jpg/mixing_objects/001_input.jpg}};
    \node (A) at (1.4, -0.8) {};
    \node (B) at (3, -0.8) {};
    \draw[->] (A) edge (B);
    \node[align=center] (C) at (2.2, 0.2) {``Corgi''\\mixed with\\``Coffee \\Machine''};
  \end{tikzpicture}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/mixing_objects/001_xt.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/mixing_objects/001_c_1.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/mixing_objects/001_p2p.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/mixing_objects/001_guidance.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/local_editing_jpg/mixing_objects/001_epsilon_1.jpg}
\end{subfigure}
\caption{Text-guided local editing results.}
\label{fig:local-editing}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Global editing}
We visualize the results in \cref{global-editing}. While in local editing MDP-$x_t$ and MDP-$c$ can yield good results, in global editing they are either not able to do the edits, or the overall layout is changed too much. In contrast, we observe that MDP-$\epsilon_t$ can produce very good results most of the time even when the strong baseline method Prompt-to-Prompt (P2P) fails. Further, we compare the editing results for MDP-$x_t$ MDP-$c$ and MDP-$\epsilon_t$ under different ranges to inject layout from the input image~\cref{fig:comparison}. We observe that MDP-$\epsilon_t$ can create meaningful edits when the manipulation starts at early or later stages, while MDP-$x_t$ only works when starting the manipulation in the initial timestep and manipulation MDP-$c$ cannot faithfully preserve the layout for any range. Also, MDP-$\beta$ sometimes fails. We therefore strongly recommend using MDP-$\epsilon_t$ when doing global editing, as it is more stable and can provide a variety of meaningful results. 
%For the manipulation schedule for $\epsilon_t$, we find that the editing is generally not able to perform when we start from the initial diffusion stage, \ie $t_max = 50$. We suppose that the early predicted noises bring too much semantic information such as color and texture from the input image to the edited image, which limits the ability for the later diffusion generation steps to change the semantic globally. Therefore, we need to inject the noises from the input image path later to avoid injecting too much semantic information to the edited image. The editing can be performed starting at later diffusion generation steps such as $t_max = 3$ or later. 
Note that all of manipulations can do stylization while we failed to find a proper setting of Prompt-to-Prompt for this application. We conjecture that Prompt-to-Prompt's manipulation of the attention maps, is good for fine-grained control, while it constrains edits to be local and it also inherits some limitations of the language model. While in our manipulation, especially for MDP-$\epsilon_t$, we do not directly manipulate the attention maps, but keep the editing ability by manipulating the diffusion paths.
\begin{figure*}[!htbp]
  \centering
  \begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/global_editing_jpg/changing_background/001_input.jpg}};
    \draw (0, 1.6) node {Input};
    \node (A) at (1.4, -0.8) {};
    \node (B) at (3, -0.8) {};
    \draw[->] (A) edge (B);
    \node[align=center] (C) at (2.2, 0.2) {``Grass''\\to\\``Library''};
  \end{tikzpicture}
  \begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/global_editing_jpg/changing_background/001_xt.jpg}};
    \draw (0, 1.6) node {MDP-$x_t$};
  \end{tikzpicture}
  \begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/global_editing_jpg/changing_background/001_c.jpg}};
    \draw (0, 1.6) node {MDP-$c$};
  \end{tikzpicture}
  \begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/global_editing_jpg/changing_background/001_p2p.jpg}};
    \draw (0, 1.6) node {P2P};
  \end{tikzpicture}
  \begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/global_editing_jpg/changing_background/001_guidance.jpg}};
    \draw (0, 1.6) node {MDP-$\beta$};
  \end{tikzpicture}
  \begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/global_editing_jpg/changing_background/001_epsilon.jpg}};
    \draw (0, 1.6) node {\textbf{MDP-$\epsilon_t$}};
  \end{tikzpicture}
\quad
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/global_editing_jpg/changing_background/004_input.jpg}};
    \node (A) at (1.4, -0.8) {};
    \node (B) at (3, -0.8) {};
    \draw[->] (A) edge (B);
    \node[align=center] (C) at (2.2, 0.2) {``Sky''\\to\\``Runway''};
  \end{tikzpicture}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/changing_background/004_xt.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/changing_background/004_c.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/changing_background/004_p2p.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/changing_background/004_guidance.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/changing_background/004_epsilon.jpg}
\end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/global_editing_jpg/in_domain_transfer/006_input.jpg}};
    \node (A) at (1.4, -0.8) {};
    \node (B) at (3, -0.8) {};
    \draw[->] (A) edge (B);
    \node[align=center] (C) at (2.2, 0.2) {``Spring''\\to\\``Winter''};
  \end{tikzpicture}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/in_domain_transfer/006_xt.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/in_domain_transfer/006_c.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/in_domain_transfer/006_p2p.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/in_domain_transfer/006_guidance.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/in_domain_transfer/006_epsilon.jpg}
\end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/global_editing_jpg/in_domain_transfer/004_input.jpg}};
    \node (A) at (1.4, -0.8) {};
    \node (B) at (3, -0.8) {};
    \draw[->] (A) edge (B);
    \node[align=center] (C) at (2.2, 0.2) {``Black\\-White''\\to\\``Colored''};
  \end{tikzpicture}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/in_domain_transfer/004_xt.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/in_domain_transfer/004_c.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/in_domain_transfer/004_p2p.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/in_domain_transfer/004_guidance.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/in_domain_transfer/004_epsilon.jpg}
\end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/global_editing_jpg/out_domain_transfer/004_input.jpg}};
    \node (A) at (1.4, -0.8) {};
    \node (B) at (3, -0.8) {};
    \draw[->] (A) edge (B);
    \node[align=center] (C) at (2.2, 0.2) {``Photo''\\to\\``Oil\\Painting''};
  \end{tikzpicture}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/out_domain_transfer/004_xt.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/out_domain_transfer/004_c.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/out_domain_transfer/004_p2p.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/out_domain_transfer/004_guidance.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/out_domain_transfer/004_epsilon.jpg}
\end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/global_editing_jpg/out_domain_transfer/002_input.jpg}};
    \node (A) at (1.4, -0.8) {};
    \node (B) at (3, -0.8) {};
    \draw[->] (A) edge (B);
    \node[align=center] (C) at (2.2, 0.2) {``Photo''\\to\\``Pencil\\Sketch''};
  \end{tikzpicture}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/out_domain_transfer/002_xt.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/out_domain_transfer/002_c.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/out_domain_transfer/002_p2p.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/out_domain_transfer/002_guidance.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/out_domain_transfer/002_epsilon.jpg}
\end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/global_editing_jpg/stylization/001_input.jpg}};
    \node (A) at (1.4, -0.8) {};
    \node (B) at (3, -0.8) {};
    \draw[->] (A) edge (B);
    \node[align=center] (C) at (2.2, 0.2) {Stylize\\``City''};
  \end{tikzpicture}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/stylization/001_xt.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/stylization/001_c.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/stylization/001_p2p.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/stylization/001_guidance.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/stylization/001_epsilon.jpg}
\end{subfigure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \begin{tikzpicture}
    \draw (0, 0) node[inner sep=0] {\includegraphics[width=0.13\linewidth]{figures/global_editing_jpg/stylization/002_input.jpg}};
    \node (A) at (1.4, -0.8) {};
    \node (B) at (3, -0.8) {};
    \draw[->] (A) edge (B);
    \node[align=center] (C) at (2.2, 0.2) {Stylize\\``Chair''};
  \end{tikzpicture}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/stylization/002_xt.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/stylization/002_c.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/stylization/002_p2p.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/stylization/002_guidance.jpg}
\end{subfigure}
  \begin{subfigure}[t]{0.13\linewidth}
    \includegraphics[width=\linewidth]{figures/global_editing_jpg/stylization/002_epsilon.jpg}
\end{subfigure}
\vspace{-0pt}
\caption{Text-guided global editing results.}
\label{global-editing}
\end{figure*}
\subsubsection{Class condition models}
We provide class-guided editing results using class-condition diffusion model instead of the common text-condition model. We show the images edited by MDP-$\epsilon_t$ in \cref{fig:class}.

\section{Conclusion and Future work}
We introduce a generalized editing framework, MDP,
which contains 5 different manipulations that are suitable, their parameters, and the manipulation schedule. We highlight a new manipulation by editing the predicted noise. 
\clearpage
The results show that this manipulation can achieve high quality edits in challenging cases where previous work may fail. 
%Also, we analyze the parameters of the diffusion process to provide a generalized framework, called \emph{MDP}, for manipulating the diffusion path to achieve local and global image editing operations. 
Our work also has multiple limitations. 
%While we analyzed many design choices, 
We cannot claim that our framework is exhaustive. 
%Some new ideas will easily be integrated into our framework while others may be fundamentally different. 
We did not analyze editing operations that become possible using fine-tuning or modifications of the network architecture. Also, we rely on the reconstruction ability of inversion for real images. In some cases the inversion may fail to faithfully reconstruct the input. An interesting topic for future work is video editing which we plan to tackle next.

{\small
\bibliographystyle{ieee_fullname}

\bibliography{egbib}
}

\clearpage
\include{supplementary_arxiv.tex}

\end{document}