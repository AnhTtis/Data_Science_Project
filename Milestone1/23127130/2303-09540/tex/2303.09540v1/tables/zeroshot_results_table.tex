\newcommand*\ZeroshotCLassificationResultsTable{%

\begingroup 
\setlength{\tabcolsep}{1.pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.} % Default value: 1
\begin{table*}
\centering
\caption[add short caption]{\textbf{Zeroshot evaluation} top1 accuracy on different datasets. Training CLIP on 63\% of the data gives higher performance in 17/24 datasets. In the first column, model names are represented by the pruning method (dedup, baseline, and rand for SemDeDup, no pruning, and random pruning respectively), and the fraction of data used for training.}
\label{table:zeroshot_results_table}
% \begin{tabular}{|p{2cm}|p{2.cm}|p{2.cm}|p{2.cm}|p{2cm}|p{2.cm}|p{2.m}|p{2.cm}|p{2cm}|p{2.cm}|p{2.cm}|p{2.cm}|}
\begin{tabular}{l|r|r|r|r|r|r|r|r|r|r|r}
\toprule
                            Data/Model &  dedup20 &  dedup40 &  dedup50 &  dedup63 &  dedup72 &  dedup80 &  baseline &  rand80 &  rand60 &  rand40 &  rand20 \\
\midrule
                               Cars &    63.33 &    78.14 &    80.26 &    81.43 &    82.05 &    \textbf{82.61} &        81.42 &   80.96 &   79.26 &   77.74 &   71.57 \\
                         Country211 &    14.16 &    17.74 &    18.26 &    18.44 &    18.70 &    18.20 &   \textbf{19.03} &   18.39 &   16.75 &   15.97 &   12.88 \\
                            Fer2013 &    41.72 &    47.90 &    46.21 &    \textbf{52.51} &    48.94 &    45.03 &        51.96 &   43.37 &   43.42 &   42.27 &   42.67 \\
                      Fgvc aircraft &     4.44 &    11.49 &    12.42 &    \textbf{15.42} &    15.27 &    15.09 &        12.66 &   14.31 &   13.11 &    9.27 &    8.85 \\
                              GTSRB &    38.22 &    37.20 &    36.22 &    43.06 &    41.00 &    35.74 &        42.00 &   \textbf{43.33} &   41.88 &   25.72 &   32.28 \\
                         Imagenet1k &    60.24 &    66.90 &    68.27 &    68.66 &    \textbf{68.93} &    68.80 &        68.74 &   68.29 &   66.12 &   64.82 &   58.86 \\
                              MNIST &    44.29 &    31.87 &    22.93 &    48.55 &    42.75 &    \textbf{48.86} &        33.23 &   43.82 &   35.73 &   36.32 &   19.22 \\
                       Renderedsst2 &    51.46 &    53.65 &    52.72 &    50.80 &    52.99 &    \textbf{57.17} &        51.13 &   52.72 &   51.29 &   52.22 &   45.47 \\
                              STL10 &    96.06 &    96.85 &    97.50 &    \textbf{97.71} &    97.69 &    97.21 &        97.62 &   97.49 &   97.38 &   97.08 &   94.31 \\
                             SUN397 &    64.81 &    67.98 &    68.26 &    68.89 &    69.25 &    \textbf{69.76} &        68.79  &   69.08 &   67.96 &   65.51 &   60.76 \\
                            VOC2007 &    77.94 &    79.51 &    79.74 &    \textbf{80.37} &    79.75 &    78.61 &        80.01 &   77.97 &   79.43 &   77.96 &   74.42 \\
                    Caltech101 &    83.05 &    84.40 &    84.98 &    \textbf{85.06} &    84.35 &    84.75 &        83.42 &   83.69 &   83.93 &   83.38 &   80.62 \\
                      CIFAR100 &    72.09 &    75.71 &    75.17 &    \textbf{77.19} &    77.16 &    77.08 &        74.61 &   76.02 &   74.08 &   72.37 &   67.79 \\
                       CIFAR10 &    92.80 &    93.78 &    94.01 &    94.00 &    \textbf{94.49} &    94.13 &        93.56 &   94.25 &   93.95 &   92.68 &   89.14 \\
 Clevr dist &    15.75 &    \textbf{23.05} &    15.75 &    19.48 &    21.95 &    21.82 &        23.03 &   18.45 &   15.59 &   18.60 &   16.21 \\
               Clevr count &    25.37 &    26.36 &    30.85 &    31.87 &    \textbf{34.73} &    20.31 &        24.14 &   15.37 &   26.43 &   14.85 &   21.67 \\
                         DMLAB &    13.16 &    17.62 &    17.99 &    19.20 &    18.52 &    20.23 &        20.46 &   18.50 &   \textbf{21.05} &   17.12 &   19.36 \\
    % dsprites\_label\_orientation &     2.44 &     2.44 &     2.05 &     2.19 &     2.06 &     2.19 &         1.67 &    2.06 &    2.23 &    2.60 &    2.45 \\
     % dsprites\_label\_x\_position &     2.73 &     3.01 &     2.77 &     3.29 &     3.04 &     2.88 &         3.56 &    3.10 &    2.69 &    3.77 &    3.26 \\
                           DTD &    49.73 &    53.51 &    54.31 &    56.76 &    \textbf{58.94} &    57.66 &        57.34 &   57.02 &   53.35 &   50.96 &   41.76 \\
                       Eurosat &    44.07 &    51.28 &    51.70 &    59.46 &    57.02 &    59.72 &        55.81 &   \textbf{59.81} &   48.63 &   51.26 &   50.00 \\
                       Flowers &    45.21 &    62.21 &    67.67 &    69.78 &    \textbf{70.48} &    66.29 &        67.88 &   68.39 &   65.43 &   62.42 &   58.16 \\
Kitti dist &    20.39 &    13.36 &    14.35 &    14.77 &    19.97 &    \textbf{26.72} &        11.25 &   11.11 &   20.68 &   17.02 &   11.25 \\
                          PCAM &    49.69 &    48.83 &    47.62 &    \textbf{52.66} &    50.09 &    52.14 &        49.09 &   50.11 &   41.28 &   55.02 &   56.59 \\
                          Pets &    77.87 &    87.30 &    89.72 &    90.02 &    90.16 &    90.57  &        \textbf{90.60} &   90.49 &   89.86 &   88.72 &   82.50 \\
                      Resisc45 &    46.76 &    57.56 &    51.69 &    51.49 &    50.14 &    53.57 &        \textbf{57.93} &   54.06 &   51.65 &   49.29 &   46.72 \\
       % smallnorb\_label\_azimuth &     5.32 &     6.28 &     5.86 &     5.82 &     5.41 &     5.36 &         5.13 &    5.42 &    5.59 &    6.02 &    5.19 \\
     % smallnorb\_label\_elevation &     9.84 &    10.27 &    11.55 &    11.21 &    11.22 &    11.28 &        11.15 &   11.78 &   11.00 &    9.43 &   11.69 \\
                          SVHN &    34.80 &    33.64 &    26.24 &    \textbf{40.87} &    33.96 &    32.68 &        35.18 &   26.77 &   26.67 &   32.70 &   25.26 \\
\bottomrule
\end{tabular}
\end{table*}
\endgroup
}


%%%%%%%%%%%%%%%  Out-of-distribution Robustness Table  %%%%%%%%%%%%%%%

\newcommand*\ZeroshotOutOfDistributionResultTable{%

\begingroup 
\setlength{\tabcolsep}{2.5pt} % Default value: 6pt
\renewcommand{\arraystretch}{1.1} % Default value: 1
\begin{table*}
\centering
\caption[add short caption]{\textbf{Out-of-distribution Robustness:} for CLIP model we trained on different number of examples. The two models trained on 63\% and 72\% of LAION440M with our de-duplication method have higher average accuracy over 6 datasets. In the first column, model names are represented by the pruning method (dedup, baseline, and rand for SemDeDup, no pruning, and random pruning respectively), and the fraction of data used  for training.}
\label{table:out_of_distribution_results_table}

\begin{tabular}{llllllll}
\toprule
Model/Dataset     &  ImageNet-A &  ImageNet-O &  ImageNet-R &  ImageNet\_Sketch &  ImageNetV2 &  ObjectNet &  Average \\
\midrule
dedup20     &       31.35 &       52.25 &       72.69 &            46.98 &       52.71 &       51.0 &             51.16 \\
dedup40     &       38.73 &        49.3 &       77.08 &            51.93 &       59.21 &      54.98 &             55.21 \\
dedup50     &       39.68 &       48.55 &       77.74 &            53.54 &       60.37 &      55.36 &             55.87 \\
dedup63     &       39.07 &       48.45 &       78.24 &            53.86 &       60.56 &      56.33 &             \textbf{56.08} \\
dedup72     &       39.53 &        47.6 &       78.61 &             53.7 &       61.23 &      56.28 &             \textbf{56.16} \\
dedup80     &       39.12 &       47.95 &       78.53 &            53.82 &       60.59 &      54.72 &             55.79 \\
baseline100 &       38.79 &       48.05 &       78.77 &            53.91 &       60.77 &      55.36 &             55.94 \\
rand80      &       37.87 &        47.7 &       78.04 &            52.81 &       60.02 &       54.3 &             55.12 \\
rand60      &        34.6 &        47.5 &       75.61 &            51.18 &       57.97 &      53.22 &             53.35 \\
rand40      &       31.88 &        49.1 &       73.65 &            49.02 &       56.83 &      49.57 &             51.67 \\
rand20      &       23.43 &        49.4 &       66.74 &            43.76 &       50.67 &      43.57 &             46.26 \\
\bottomrule
\end{tabular}
\end{table*}
\endgroup
}
