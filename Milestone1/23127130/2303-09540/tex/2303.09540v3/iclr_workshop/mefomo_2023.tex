
\documentclass{article} % For LaTeX2e
\usepackage{mefomo_2023,times}
\input{tables/zeroshot_results_table.tex}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{import}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs}


\title{SemDeDup: Data-efficient learning at web-scale through semantic deduplication}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\newcommand{\todo}[1]{{\textcolor{red}{[\textbf{TODO:} #1]}}}


\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Progress in machine learning has been driven in large part by massive increases in data. However, large web-scale datasets such as LAION \citep{schuhmann2022laion} are largely uncurated beyond searches for exact duplicates, potentially leaving much redundancy. Here, we introduce SemDeDup, a method which leverages embeddings from pre-trained models to identify and remove ``semantic duplicates'': data pairs which are semantically similar, but not exactly identical. Removing semantic duplicates preserves performance and speeds up learning. Analyzing a subset of LAION, we show that SemDeDup can remove 50\% of the data with minimal performance loss, effectively halving training time. Moreover performance increases out of distribution. Also, analyzing language models trained on C4, a partially curated dataset, we show that SemDeDup improves over prior approaches. SemDeDup provides an example of how simple ways of leveraging quality embeddings can be used to make models learn faster with less data.   
\end{abstract}


\section{Introduction} \label{sec:introduction}

\begin{figure}[ht!]
% \vskip 0.2in
\begin{center}
\includegraphics[width=0.26\columnwidth]{figures/Semantic De-Dup Conceptual Figure.pdf}
\includegraphics[width=0.35\columnwidth]{figures/openclip_results/dedup50_laion440m_imagenet_zeroshot_acc_vs_iterations.pdf}
% \includegraphics{}
\caption{\textbf{Data efficiency from semantic deduplication (SemDeDup)} Left: A schematic of the SemDeDup algorithm which efficiently removes semantic duplicates from web-scale data. Right: When SemDeDup removes 50\% of the LAION-440M dataset, training on this semantically {\it nonredundant} subset achieves almost the {\it same} performance as training on the {\it entire} 440M dataset. Also, training speed is {\it twice} as fast and completes in {\it half} the time.}
\label{fig:introfig}
\end{center}
\end{figure}

\begin{figure*}[ht!]
% \vskip 0.2in
\centering
\begin{center}
\includegraphics[width=\textwidth]{figures/semantic_duplicates.png}
\caption{\textbf{Mapping cosine similarity to perceptual and semantic similarity.} We visualize pairs of images with cosine similarity $1-\epsilon$ in the CLIP image encoder embedding space. The left most image is a random seed image from LAION, while the remaining images are sorted by their dissimilarity $\epsilon$ to the seed image. As $\epsilon$ increases from left to right, we move from perceptual to semantic duplicates, while at large values of $\epsilon$ we see semantically redundant pairs. Note the red labelled ``semantic duplicate" is a view of the original left-most seed image from a slightly different perspective.}
\label{fig:visduplicates}
\end{center}
% \vskip -0.2in
\end{figure*}

A primary driver of recent success in machine learning has been the rise of self-supervised learning (SSL) scaled to ever larger models and unlabelled datasets \citep{Hestness2017-yq,Kaplan2020-ti,Henighan2020-jf,rosenfeld2020a,Gordon2021-az,Hernandez2021-ix,Zhai2021-dl,HoffmannChinchilla}. In particular, modern large datasets are often derived at global web-scale and are generally unfiltered, with the exception of NSFW filters. One such public dataset is LAION \citep{schuhmann2022laion}, a multi-modal dataset of 5 billion image/text pairs. Multi-modal models such as CLIP \citep{radford2021learning} are trained for many epochs on these large datasets achieving impressive performance, but at the cost of extremely long training durations. 

The critical role of large datasets has led to increasing interest in scaling laws which enable us to predict how model performance changes given more data and/or parameters, leading to the observation that test error generally scales as a power law with respect to data quantity \citep{Kaplan2020-ti}. Power law scaling, however, is unsustainable as diminishing marginal returns are quickly hit such that ever increasing amounts of data are required to achieve ever diminishing improvements in performance. Notably, many of these models appear never to converge, as test performance continues to increase even after 10s of passes through these massive datasets \citep{openclip, multi-modal-scaling}. This result suggests that our best models are underfitting, likely as a result of spending an increasing fraction of learning time focusing on redundant data. 

Improving data efficiency is therefore quite impactful, either by enabling models to achieve the same performance much faster or to achieve better performance given the same computational budget. This has inspired recent work which suggests that by pruning data according to an intelligent criterion, power law scaling with respect to data can be beat and, given an optimal data ranking metric, exponential scaling might in principle be achieved \citep{Sorscher2022-wo}. Recent explorations of this direction have shown promising results, with some works able to reduce data size by almost 5-fold with minimal performance loss \citep{radenovic2023filtering}.

Optimal approaches to select data might focus on one of several different classes of examples to be removed, roughly ordered by the complexity of their discovery:



\begin{enumerate}
    \item \textbf{Perceptual duplicates}: We loosely define such data pairs to be perceptually identical to a typical human observer. These may be exact duplicates or inputs with slight, human impreceptible distortions. 
    % The most straightforward definition is exact duplicates at the pixel (for images) or token level (for text) that could easily be found via exact duplicate detection in input space. However, such approaches might miss pairs of images with human imperceptible pixel level distortions.
    % Most widely-used datasets have some exact duplicate filter already applied, though perceptual duplicates with slight pixel-level differences may pass through such filters. 
    %
    \item \textbf{Semantic duplicates}: These are examples which contain largely identical information content, but remain perceptually distinct. For example, a pair of image views which are derived from the same image, but feature different margins, aspect ratios, or color distributions could be considered semantic duplicates. 
    % A pair of sentences with the same structure but some words exchanged for synonyms would also be considered a semantic duplicate. 
    Such pairs would rarely, if ever, be detected by exact duplicate filters as they are far apart in pixel/token space. 
    %
    \item \textbf{Semantically redundant data}: in contrast to semantic duplicates, semantically redundant data are not derived from the same underlying objects and would be clearly distinguishable to a human. However, the information contained in such examples may still contain substantial overlap. For example, consider the case of two different images of two different golden retrievers in two different parks. These images are neither perceptually nor semantically identical as the content of the images differ. Each additional semantically redundant data point will provide less and less new information, eventually converging to near-zero information gained from additional such data. 
    
    % Methods such as SSL Prototypes \citep{Sorscher2022-wo} and memorization \citep{Feldman2020-yv} search for semantically {\it non-redundant} data subsets to train on.  
    
    \item \textbf{Misleading data}: these are data which rather than providing zero information (as in the previous categories) provide negative or \textit{harmful} signal, such that removing these data actually improves performance rather than having a neutral effect. While such data are easy to conceive of in supervised learning (i.e. mislabeled examples), it is much less clear what such examples may be in the context of self-supervised learning.
\end{enumerate}
% \ari{I like the above section and think it's important to go into these distinctions in one of these papers, but not sure whether it fits best here...}


In this work, we focus on the category of semantic duplicates: data which are semantically highly similar but which would be difficult to discover using simple deduplication approaches. These data points are challenging to identify because distance measures in input space are unlikely to uncover semantic duplicates. To overcome this limitation, we leverage pre-trained foundation models to compare data similarity in the learned embedding space rather than in input space. Comparing every data point to every other data point, however, is intractable, especially for web-scale datasets containing billions of examples. To make this computation possible, we use the clustering approach described in \citet{Sorscher2022-wo} to segment the embedding space, allowing us to only search for duplicate pairs within a cluster. Using this approach, we make the following contributions: 

\begin{itemize}
    \item We propose SemDeDup (Fig. \ref{fig:introfig}, left), a simple, yet effective and computationally tractable way to identify semantic duplicates. Using this approach, we show that large web-scale datasets such as LAION contain large numbers of semantic duplicates, with 50\% of examples containing at least one semantic duplicate.  
    \item Large fractions of semantic duplicates can be removed with little-to-no performance impact, greatly increasing training efficiency. We reduced the size of our LAION training set by 50\% with minimal performance loss, achieving nearly the same performance 2x faster (Fig. \ref{fig:introfig}, right), and moreover \textit{improved} performance out-of-distribution.
    \item We apply SemDeDup to C4, beating prior SoTA deduplication, providing efficiency gains 10-15\% for language model training, and sometimes even improving performance.
\end{itemize}

% Overall, our results demonstrate a simple yet surprisingly effective approach to reduce the cost of training through the removal of semantic duplicates which is likely applicable to all web-derived datasets and may help to democratize the training of large-scale foundation models by improving data and compute efficiency.   


\section{SemDeDup: Defining and identifying semantic duplicates} \label{sec:methods}
 % \label{sec:methods_id_dup}
While identifying perceptual duplicates can be easily done in input space, identifying semantic duplicates is more difficult as they maybe distant in either pixel or token space. To identify these pairs, we leverage the embedding space of a large pre-trained foundation model to provide a more semantically meaningful distance metric (Fig. \ref{fig:introfig}, left). First, we embed each data point using a foundation model (OpenCLIP \citep{openclip} for images and OPT \citep{ZhangOPT} for language. We then cluster the embeddings into $\mathcal{K}$ clusters via k-means ($\mathcal{K}=50,000$).
Within each cluster, we compute all pairwise cosine similarities and set a threshold cosine similarity (1-$\epsilon$) above which data pairs are considered semantic duplicates.
Finally, from each group of semantic duplicates within a cluster, we keep the image with the lowest cosine similarity to the cluster centroid and remove the rest. For CLIP experiments, we note that our method only considers images and ignores the captions.

% \begin{algorithm} [t]
% \caption{SemDeDup}\label{alg:semdedup}
% \begin{algorithmic}
% %
% \Require{Data, $\mathcal{D}$; pre-trained model, $f(x)$; number of clusters, $k$}
% \State $z_\mathcal{D} \gets f(d)~ \forall d \in \mathcal{D}$
% \State cluster_ids, num_clusters $\gets$ k-means($z_D$, $k$)

% \end{algorithmic}
% \end{algorithm}



\section{SemDeDup on LAION} \label{sec:results}

% If we consider pairs of data points to be semantic duplicates when their cosine similarity is at least $1-\epsilon$, then $\epsilon$ can be thought of as a dissimilarity threshold, with increasing $\epsilon$ reflecting an increasingly coarser notion of semantic equality. We expect that low thresholds of $\epsilon$ will find semantic duplicates, while higher thresholds will allow semantically redundant data pairs as well. 
% To evaluate SemDeDup's ability to discover semantic redundancy in multi-modal data, we train CLIP models on the LAION dataset (Section \ref{sec:methods}). 
We first show that the LAION dataset contains extreme amounts of semantic redundancy (Section \ref{sec:res-quantification}) and provide examples of the semantic duplicates discovered by SemDeDup (Section \ref{sec:res-examples}). Most critically, we demonstrate that removing the semantic duplicates discovered by SemDeDup has minimal to no impact on converged performance and increases learning speed (Section \ref{sec:res-train-dedup}). In Appendix \ref{sec:nlp} we show that SemDeDup improves performance of language models over prior methods.
% In Appendix we show that SemDeDup gives better 

 
\subsection{Extreme semantic redundancy at web-scale} \label{sec:res-quantification}

How many semantically redundant pairs are there in LAION? Remarkably, we find that even tiny thresholds $\epsilon$ lead SemDeDup to remove large fractions of data in LAION440M (Fig. \ref{fig:eps_vs_dataset_size}a), showing that LAION-440M contains large quantities of semantic duplicates. Surprisingly, $30\%$ of images in LAION-440M have a semantic duplicate at the highly stringent distance threshold of $\epsilon=0.00095$, while $50\%$ have a duplicate at the tight threshold of $\epsilon=0.03$ (Fig. \ref{fig:eps_vs_dataset_size}c). Moreover, a histogram of pairwise cosine similarity in LAION-440M (Fig. \ref{fig:eps_vs_dataset_size}d) reveals a high density of pairs at high cosine similarity, including a large contribution at $1$, reflecting highly similar semantic duplicates. These results demonstrate that LAION-440M contains large amounts of semantic redundancy.

\subsection{What do semantic duplicates look like?} \label{sec:res-examples}

In Fig. \ref{fig:visduplicates}, we show examples of semantic duplicates found at different thresholds $\epsilon$. At extremely low values of $\epsilon$ we find perceptual duplicates, and at slightly higher values of $\epsilon$, we find semantic duplicates, which are the same image but with distortions which evade exact de-duplication approaches such as different margins, crops, aspect ratios, and color filters, or slighly different peripheral details.  Fig. \ref{fig:before_and_after_0}, \ref{fig:before_and_after_1}, and \ref{fig:before_and_after_2} show examples of clusters that are semantically deduplicated at increasing levels of $\epsilon$, clearly indicating more semantic diversity in deduplicated clusters as $\epsilon$ increases. 

\subsection{Training on semantically deduplicated data improves efficiency} \label{sec:res-train-dedup}

If SemDeDup is effective at finding semantic duplicates, we should be able to remove these duplicates with minimal performance impact. To test this, we train CLIP models on subsets of LAION-440M deduplicated at different thresholds $\epsilon$. As $\epsilon$ rises, we retain a smaller fraction of data. In Fig. \ref{fig:LAION-440_training} (a), we plot the top-1 zero-shot accuracy of our CLIP models on ImageNet-1k. Encouragingly, we found that SemDeDup can remove up to 37\% of LAION-440M with no performance drop, and 50\% with minimal performance drop ($<0.5\%$). In contrast, randomly removing data results in much larger drops. In Fig. \ref{fig:LAION-440_training} (b), we show the average zero-shot performance across 25 tasks, finding that on average, performance increased on de-duplicated data. See Table {\ref{table:zeroshot_results_table}} for detailed performance on all $25$ tasks at $6$ deduplication thresholds as well as $1$ baseline and $4$ random controls. See also Fig. \ref{fig:all_zershot_line_plots} for performance on $25$ individual tasks. 

We also evaluated out-of-distribution robustness on $6$ datasets commonly used for this task: ImageNet-A, ImageNet-O \citep{hendrycks2021nae}, Imagenet-R \citep{hendrycks2021many}, Imagenet-sketch \citep{imagenet-sketch}, ImageNetV2 \citep{imagenet-v2}, and ObjectNet \citep{objectnet}. We again found that SemDeDup {\it increased} performance when {\it removing} 37\% of the data, and matched performance when 50\% was removed as shown in Fig. \ref{fig:LAION-440_training} (c). See Table \ref{table:out_of_distribution_results_table}  for detailed performance on $6$ OOD tasks at $6$ deduplication thresholds as well as $1$ baseline and $4$ random controls.  See also Fig. \ref{fig:fig:all_OOD_zershot_line_plots} for performance on the $6$ individual tasks. We emphasize that SemDeDup achieves these results on LAION-440M, an already highly curated dataset derived from LAION-2B which was found to have similar performance despite the almost five-fold reduction in data \citep{radenovic2023filtering}. However, to ensure that this curated subset did not bias our results, we also evaluated on LAION-233M, an uncurated subset of LAION-2B, finding qualitatively similar results (Fig. \ref{fig:laion233m_result}).  


Because SemDeDup reduces the number of training points, it enables substantially faster training. In Fig. \ref{fig:LAION-440_training} (d), we plot the top-1 zero-shot accuracy on ImageNet-1k as a function of the number of iterations for different deduplication thresholds $\epsilon$. Notably, models trained on deduplicated data reach convergence in substantially fewer iterations. %To quantify the speed of learning, we fit a simple linear regression model to the accuracy between 50k iterations and \todo{XX} iterations before the end of training, a regime in which all of our learning curves are approximately linear. In Figure \todo{XX}, we plot the learning slope as a function of deduplication fraction, showing that the more de-duplicated a dataset is, the more efficiently it learns. 


% \subsection{Out-of-distribution Robustness}
%  We find that, in contrast to random pruning, our models trained on 63\% and 72\% of LAION-440M (CLIP dedup63 and CLIP dedup72 respectively) outperform the baseline model. We show the zeroshot accuracy on these datasets in table (\ref{table:out_of_distribution_results_table}).

\begin{figure}[t]
\begin{center}
\subfigure[]{\includegraphics[width = 0.245\columnwidth]{figures/openclip_results/laion400m_deduplication_imagenet_zeroshot_top1_accuracy.pdf}}
\subfigure[]{\includegraphics[width = 0.245\columnwidth]{figures/eval_results/Average_ZeroShot_Top1_Acc_(25_Datasets).pdf}}
\subfigure[]{\includegraphics[width = 0.245\columnwidth]
{figures/eval_results/Average_OOD_ZeroShot_Top1_Acc_(6_Datasets).pdf}}
\subfigure[]{\includegraphics[width = 0.245\columnwidth]{figures/openclip_results/all_laion440M_imagenet_zeroshot_acc_vs_iterations.pdf}}
\caption{\textbf{SemDeDup allows better average zero-shot accuracy across 31 tasks with less data and faster pre-training.}
\textbf{(a)}: Down to using only 50\% of LAION-440M for pre-training CLIP, we are able match the zero-shot ImageNet accuracy of the baseline model trained on 100\% of the data (black dashed line) with a small drop of 0.47\% only, while we outperform the baseline model with only 63\% of data.
\textbf{(b)}: Average zero-shot performance for CLIP measured on 25 datasets {\it improves} down to $63\%$ of the pre-training data, yielding {\it better} performance with almost $1.6\times$ {\it faster} pre-training. 
% \textbf{SemDeDup allows better average performance across 6 ImageNet OOD tasks with less data and faster pre-training.}
\textbf{(c):} Zeroshot validation accuracy averaged over 6 ImageNet-1k OOD datasets. We outperform the baseline model on OOD tasks with only 63\% of pre-training.
\textbf{(d):} We track zeroshot ImageNet-1K performance as a function of LAION-440M pre-training iterations at different deduplication thresholds. With SemDeDup, models converge in far less iterations.
}
\label{fig:LAION-440_training}
\end{center}
\end{figure}





\textbf{Discussion:} we introduced SemDeDup, a simple yet tractable and effective method which leverages pre-trained embeddings to remove semantic duplicates which are highly semantically similar but not identical. Removing semantic duplicates improves learning speed and out-of-distribution performance while providing efficiency gains of up to 50\% on the largely uncurated LAION and 15\% on the partially curated C4. SemDeDup demonstrates the importance of data quality and the potential of data curation to dramatically improve training efficiency. 

% \paragraph{\textbf{Limitations.}} While SemDeDup does an effective job of removing semantic duplicates and some semantically redundant data points, it is only one way to remove uninformative data points. In particular, this work does not capture many aspects of semantic redundancy, nor does it address removal of bad or misleading data, all of which can likely be exploited to make substantial further reductions to dataset size without sacrificing performance.  

% In LAION, we identified semantic duplicates based only on image data, but we ignored the caption information. Leveraging this information may lead to identification of further semantic duplicates. 

% Our results on C4 showcase the potential of SemDeDup for NLP, but the gains were more modest due to the partially curated nature of C4 which has fewer duplicates than LAION. We also trained small models relative to the best models. It is possible that results may change with scale, though following \citet{Sorscher2022-wo}, it is likely increasing scale would further improve the benefits of data curation. 

% Overall, the optimal data pruning policy for finding the smallest possible data subset under computational tractability and performance constraints remains, as ever, an extremely difficult open question. However, the remarkable  efficacy of SemDeDup, especially given its underlying simplicity and scalability, suggests that the removal of semantic duplicates may well be an important prerequisite for any more sophisticated data pruning algorithm, especially when working with modern, large, highly uncurated, web-scale datasets.  

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

\clearpage
\bibliography{data_pruning}
\bibliographystyle{iclr2023_conference}

\newpage
\appendix
\renewcommand{\thefigure}{A\arabic{figure}}
\setcounter{figure}{0}
\renewcommand\thetable{A\arabic{table}}
\setcounter{table}{0}
\onecolumn

\section{Additional LAION Methods} \label{sec:laion-meth}

\subsection{Datasets and Training} \label{sec:methods_data}

\paragraph{The LAION dataset.} To train large-scale multi-modal models, we used the LAION dataset \citep{laion5b}, an open multi-modal dataset containing up to 5 billion image-text pairs scraped from the web. LAION data were filtered using a pre-trained CLIP model to only retain image-text pairs with an embedding similarity greater than 0.28. Image-text pairs containing very short captions or small images were also removed. A simple de-duplication method based on the image url was also performed. 

The majority of our experiments were performed on the LAION-440M filtered subset of LAION-2B introduced by \citep{radenovic2023filtering}. This dataset was filtered according to three criteria: (1) high enough caption complexity; (2) the caption must contain an action; (3) any text present in the image cannot substantially overlap with the caption.

To ensure this CAT filtered LAION-440M subset did not impact our results, we also performed experiments on unfiltered data derived from LAION. While we attempted to collect the original LAION-400M subset \citep{laion400m}, unfortunately, much of the data were no longer available as the source data were removed from the web. We therefore used a reduced version of the LAION-400M subset containing the 233 million data points we were able to collect, which we call LAION-233M. 

% For our language modeling experiments, we train models on the C4 dataset \citep{ColinT5}. Section \ref{sec:related_work} discussed prior deduplication efforts on this dataset. We use \citep{LeeDedup} as a baseline to test how our proposed deduplication method performs in the text domain. Since pre-training large language models on the entire dataset is beyond our compute budget, we train on subsets of this data whose sizes are compute optimal given model size ~\citep{HoffmannChinchilla}. 

\paragraph{CLIP training.} For CLIP training on LAION, we use the OpenCLIP implementation \citep{openclip}. All the models we trained have Vision Transformer Base (ViT-B-16) \citep{vit} as an image encoder and Text Transformer Base (ViT-B-16) \citep{vaswani2017attention} as a text encoder.
We trained all models with global batch size of 33k image-caption pairs. We fix the number of training epochs to 32 regardless of the dataset size. This results in training for a fewer number of iterations when training on deduplicated data, thereby achieving efficiency gains. We train with AdamW \citep{adamw} and cosine learning rate schedule with warmup. The same peak learning rate of $ 5 \mathrm{x} 10^{-4}$ was used for all models.\\

% For language model training on C4, we use same configurations as OPT \citep{ZhangOPT} to train 125M and 1.3B model sizes (see Table 1 in \citet{ZhangOPT} for full specifications). We change the learning schedule depending on the size of the dataset we train on (we allow the number of warmup updates to remain the same, but the learning rate anneals to 0 faster for smaller datasets). This allows for fair comparisons of model performances across different dataset sizes. For 1.3B model size experiments, we increase the number of warmup updates to 5550 and reduce the peak learning rate to $6 \mathrm{x} 10^{-5}$ to stabilize training.


\paragraph{CLIP Evaluation} For CLIP evaluation we use zero-shot evaluation on 31 different datasets. Table (\ref{table:zeroshot_results_table}) and Table (\ref{table:out_of_distribution_results_table}) in the Appendix list all the datasets we use for evaluation. 
% in addition to linear probe evaluation on 29 datasets. 

% \kushal{this part should be in section 3.3}
% In our experiments pruning C4, we pass each document in C4 to a pre-trained OPT model (cite) and save the last layer embedding for the last token in the document. We then apply the same method described above to cluster these embeddings. We compare to random pruning and the method described in~\citet{lee2021deduplicating}. Note that the de-duplication threshold values associated with different fractions of data remaining change (as seen in figure (cite)), and that all results in Figure (cite) and Figure (cite) are run with 3 random seeds but the error bars are too small to be seen.

\section{SemDeDup on Natural Language} \label{sec:nlp}

\begin{figure}[ht]
\begin{center}
\includegraphics[width = \columnwidth]{figures/eval_results/all_datasets_zeroshot_bar_plot.pdf}
\caption{\textbf{SemDeDup improves zeroshot and OOD performance in many tasks with less pre-training.} A comparison of zeroshot evaluation performance between a baseline CLIP model (trained on 100\% of the data) and our CLIP trained on 63\% of LAION-440M after de-duplication on $31$ tasks. The green bars show when SemDeDup outperforms the baseline model. }
\label{fig:zeroshot_bar_plot}
\end{center}
\end{figure}

% This table is at tables/zeroshot_results_table.tex


\subsection{Methods}
\label{sec:langmethods}

%  We train language models on the C4 dataset \citep{ColinT5}. Section \ref{sec:related_work} discussed prior deduplication efforts on this dataset. We use \citet{LeeDedup} as a baseline to test how our proposed deduplication method performs in the text domain. Since pre-training large language models on the entire C4 corpus is beyond our compute budget, we train on subsets of this data whose sizes are compute optimal given model size as per \citet{HoffmannChinchilla}. 

% For model and training configurations, we use same configurations as OPT \citep{ZhangOPT} to train 125M and 1.3B model sizes (see Table 1 in \citet{ZhangOPT} for full specifications). We change the learning rate schedule depending on the size of the dataset we train on (we allow the number of warmup updates to remain the same, but the learning rate anneals to 0 faster for smaller datasets). This allows for fair comparisons of model performances across different dataset sizes. For 1.3B model size experiments, we increase the number of warmup updates to 5550 and reduce the peak learning rate to $6 \mathrm{x} 10^{-5}$ to stabilize training. 

 We train language models on deduplicated versions the C4 dataset \citep{ColinT5}. Since pre-training large language models on the entire C4 corpus is beyond our compute budget, we train on subsets of this data whose sizes are compute optimal given model size as per \citet{HoffmannChinchilla}. We use the OPT model and training configurations \citep{ZhangOPT} to train 125M and 1.3B parameter models (see Table 1 in \citet{ZhangOPT} for full specifications). We use the original number of warmup updates but adjust learning rate schedule such that all training runs anneal learning rate to 0 by the end of the training — this allows for fair comparisons of model performances across different dataset sizes. 
 %We change the learning rate schedule depending on the size of the dataset we train on (we allow the number of warmup updates to remain the same, but the learning rate anneals to 0 faster for smaller datasets).
 For 1.3B model size experiments, we increase the number of warmup updates to 5550 and reduce the peak learning rate to $6 \mathrm{x} 10^{-5}$ to stabilize training. 

We evaluate our trained language models on two independent validation sets: the validation text corpora used by OPT \citep{ZhangOPT}  (referred to as "opt\_valid") and a random sample of the instruction finetuning corpus used to train the OPT-IML family of models \citep{IyerOPTIML}, composed of verbalized prompts corresponding to a wide range of NLP tasks and their solutions (referred to as "prompts\_with\_answers").

To perform SemDeDup, we pass documents through the open-sourced pre-trained 125M OPT model \citep{ZhangOPT} and save the last layer embedding for the last token in the document. We then apply the same method described in Section~\ref{sec:methods_clustering} with $\mathcal{K} = 11000$ to cluster these embeddings. We compare to random pruning and the NearDup method described in~\citet{lee2021deduplicating}. Note that the deduplication threshold values associated with different fractions of data remaining change compared to LAION-440M, as seen in Fig.~\ref{fig:nlp_appendix_125M_eps_vs_frac_data}.

%Also all results in Fig.~\ref{fig:nlp_125M_matched_epoch_c4} and Fig.~\ref{fig:nlp_125M_efficiency_graphs} are shown with error bars, reflecting standard deviation in training runs across 3 random seeds, but these error bars are often too small to be seen, indicating little variability across training runs.  


\subsection{Results} \label{sec:results_c4}

In Fig.~\ref{fig:nlp_125M_matched_epoch_c4}, we show the performance of SemDeDup versus random pruning. We observe that SemDeDup significantly outperforms random pruning as measured by perplexity on prompts\_with\_answers and average opt\_valid performance. For a breakdown of performance on individual validation sets in opt\_valid, see Figure~\ref{fig:nlp_appendix_big_ood_plot_matched_epochs} where we observe that SemDeDup beat random pruning on every single validation set in opt\_valid.

%\ari{Kushal/Daniel, update below paragraph since we changed fig 8}
Training on less data for one epoch naturally causes performance to decrease. Thus, we also explore whether continuing to train on the same smaller pruned datasets for more epochs will match the performance of a baseline model trained on a larger dataset. In Fig.~\ref{fig:nlp_125M_efficiency_graphs}, we train on datasets pruned with SemDeDup, but perform the same number of total training steps as the baseline model on the larger dataset (which was trained for $1$ epoch).  This causes the model to do multiple epochs over the pruned dataset. We observe that by training for multiple epochs over significantly pruned datasets we can reach the performance of a single-epoch run on the full dataset using 10-15\% less compute.  This is similar to the finding in Section~\ref{sec:res-train-dedup}. Notably, this efficiency gain is larger at higher pruning percentages, indicating that more aggressive pruning can yield more efficiency gains. This trend generally holds across the individual validation sets in opt\_valid (see Figure~\ref{fig:nlp_appendix_big_ood_plot_efficiency}).

On the C4 validation set, we observe that SemDeDup still outperforms random pruning in Figure~\ref{fig:nlp_appendix_125m_full_plots_matched_epoch}. In Table~\ref{table:nlp_appendix_125m_c4_matched_epoch} we compare SemDeDup to the NearDup baseline from \citet{LeeDedup}. We observe that NearDup and SemDeDup have comparable performance as is expected, because with 4\% pruning there is very little change to the underlying dataset.

\subsection{What is being pruned in language data?}
\label{sec:nlp_what_is_being_pruned}

In Figure~\ref{table:nlp_appendix_examples_one} and Figure~\ref{table:nlp_appendix_examples_two} we choose specific clusters and show a random sample of documents retained in the cluster after performing SemDeDup for different values of $\epsilon$. In Figure~\ref{table:nlp_appendix_examples_one}, we observe that at low values of $\epsilon$, we find semantic duplicates in the form of templated text, where typically few words (e.g. a geographic location or a name) is changed. This successfully evades exact-string deduplication methods but contains highly redundant information as seen in Figure~\ref{table:nlp_appendix_examples_one}. In Figure~\ref{table:nlp_appendix_examples_two}, we show an example of a cluster with semantically redundant duplicates — most examples in this cluster are advertisements about athletic shoes (Nike and Adidas shoes). These examples are not necessarily templated text or have exact string matches, but are highly redundant nonetheless. We see in Figure~\ref{table:nlp_appendix_examples_two} that at more aggressive pruning (i.e. higher $\epsilon$) these semantically redundant duplicates get pruned. We note that exact string duplicates (i.e.``perceptual duplicates for text") are rare since duplicate occurrences of any three-sentence spans were removed in C4 already.


% In Figure~\ref{fig:125M_bar_c4}, we observe that we match in-distribution performance of our baselines, and do better at all out-of-distribution tasks (for a full breakdown of OOD tasks please see Figure \todo{appendix OOD plot}). Note that this is only at 96\% data kept to have a fair comparison to the de-duplication baseline in \citet{LeeDedup}. As we increase $\epsilon$ in SemDeDup we retain less data, and we see in Figure~\ref{fig:nlp_125M_matched_epoch_c4} that our method significantly outperforms random data pruning as we prune more aggressively. Notably, when we match the number of iterations across training run (e.g. we do not do one pass training over the pruned datasets), we find that our method provides an *improvement* over the baseline in OOD and prompts as seen in Figure~\ref{fig:nlp_125M_matched_iteration}. We also observe that our method performs worse in-distribution, which we hypothesize is due to the distribution shift that comes with pruning - when we prune, the distribution strays away from the original c4 distribution, Thus, if we do multiple passes over this new distribution, we will do increasingly worse on the original distribution. This also explains why one-pass in-distribution results in Figure~\ref{fig:nlp_125M_matched_epoch_c4} do not look as poorly as multiple-pass results in Figure~\ref{fig:nlp_125M_matched_epoch}.


\begin{figure}[ht]
\begin{center}
\includegraphics[width = 0.4\textwidth]{figures_nlp/matched_epochs/limited/prompts_with_answers_125M_matched_epoch_limited.pdf}
\includegraphics[width = 0.4\textwidth]{figures_nlp/matched_epochs/limited/opt_valid_125M_matched_epoch_limited.pdf}

\caption{\textbf{SemDeDup applied to C4}.  The x-axis corresponds to different percents of data kept, and the y-axis represents the perplexity on validation sets described in Section~\ref{sec:langmethods} (lower is better). Each point is a separate 125M model trained on one-pass of its respective pruned dataset (mean and standard deviation across 3 random training seeds). The green point represents a 125M model trained on a version of C4 deduplicated via the NearDup method \citep{LeeDedup}. Note that NearDup (the single green point) keeps 96.1\% of the data. SemDeDup can match this baseline performance while keeping only 90\% of the data (see Table~\ref{table:nlp_appendix_125m_c4_matched_epoch_80} for numerical comparison).}
\label{fig:nlp_125M_matched_epoch_c4}
\end{center}
\end{figure}

\begin{figure}[ht]
\begin{center}
\includegraphics[width = 0.4\textwidth]{figures_nlp/efficiency_graphs/prompts_with_answers_efficiency.pdf}
\includegraphics[width = 0.4\textwidth]{figures_nlp/efficiency_graphs/opt_valid_efficiency.pdf}

\caption{\textbf{SemDeDup allows compute efficiency gains by training on much smaller datasets for slightly longer.} We prune datasets via SemDeDup and continue training past one epoch until we reach baseline model perplexity. The x-axis is percentage of data kept, and the y-axis is the percentage of FLOPs saved. For example, training on the 80\% pruned dataset reaches baseline model perplexity on prompts\_with\_answer in ~95.0\% of training iterations as baseline training, saving ~5.0\% compute. Mean and standard deviation provided across 3 random training seeds.}
\label{fig:nlp_125M_efficiency_graphs}
\end{center}
\end{figure}



% \begin{itemize}
%     \item Figure 1: Conceptual figure + main result
%     \item Figure 2: Duplicate analysis (examples of duplicates + histograms) @ lowest epsilon, highest similarity
%     \item Figure 3: Epsilon graphs (frac data as a function of epsilon; visual depiction showing how a particular cluster gets pruned down as we increase epsilon)
%     \item Figure 4: (1) plot of performance vs fraction of data remaining (2) plot of performance vs. iterations
%     \begin{itemize}
%         \item have separate plots for 230M (2 plots)
%         \item have separate plot for 440M (2 plots)
%         \item have separate plot comparing the only two along absolute quantity of data (1 plot)
%     \end{itemize}
%     \item Figure 5: Plot of learning curve slope vs. frac data remaining for de-dup and random 
%     \item Figure 6: Figures for downstream perf (pick pruning rate; relative to 100\% just have positive and negative bars); zero-shot + linear eval
%     \begin{itemize}
%         \item Put the breakdown on task level (each task has fraction data on x-axis and perf on y-axis) in appendix
%     \end{itemize}
%     \item Figure 7: NLP graphs (do matched epochs in main text where its frac data kept vs performance for 125M / 1.3b model? maybe just one)
%     \begin{itemize}
%         \item put matched iterations in appendix; put breakdown across ood tasks in appendix
%     \end{itemize}
% \end{itemize}


% \section{LAION Statistics} 

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\subfigure[]{\includegraphics[width=0.49\columnwidth]{figures/eps_vs_dataset_size.pdf}}
\subfigure[]{\includegraphics[width = 0.48\columnwidth]{figures/percentage_of_images_with_at_least_one_duplicate.pdf}}
\subfigure[]{\includegraphics[width = 0.50\columnwidth]{figures/maximum_within_cluster_pairwise_similarity_for_each_images.pdf}}
\caption{\textbf{Extreme semantic redundancy in LAION-440M.} (a) Fraction of data remaining as a function of deduplication threshold $\epsilon$ for LAION-440M. (b) Percentage of images in LAION-440M with at least one semantic duplicate as a function of $\epsilon$. (c) Histogram of number of within-cluster image pairs in LAION-440M at a given cosine similarity.}
\label{fig:eps_vs_dataset_size}
\end{center}
\vskip -0.2in
\end{figure}



\section{Related Work} \label{sec:related_work}
Much of the work in langauge and vision on deduplication has focused on the removal of exact duplicates. For example, ~\citet{Liao2022-ft} removed duplicates between the YFCC15M dataset \citep{Thomee2016-zd} and the ImageNet validation set to prevent train-test leakage.  The C4 text corpus - used for training T5 \citep{ColinT5} - has been deduplicated by discarding repeated occurrences of any three-sentence spans.  ~\citet{LeeDedup} showed that it’s possible to further deduplicate this dataset without loss of performance by computing approximate n-gram overlap between documents using the MinHash technique \citep{BroderMinHash}. ~\citet{RaeGopher} also applied MinHash based deduplication to curate training data for the Gopher model and demonstrated that training on the deduplicated dataset can result in lower perplexity across various validation sets. ~\citet{KandpalPrivacy}  found that deduplication prevents memorization in LLMs and thus mitigates privacy concerns. More recent works use forms of model-based feature extraction to improve the robustness of the similarity metric used for deduplication. ~\citet{Silcock2022-tp} created a supervised dataset for detecting duplicate news articles and trained models to predict those labels.  In the domain of computer vision, ~\citet{Choi2022-pg} improves on SSL techniques by removing near-duplicates in some high dimensional feature space they learn.

% \citep{Kandpal2022-ir} removed exact duplicates in natural language to prevent memorization \citep{Carlini2022-gy} of training data, and thereby also prevented privacy attacks. \citep{Silcock2022-tp} evaluated noise robust exact deduplication in a language dataset of 10 milllion articles using N-gram and neural encoder overlaps. \citep{Ben_Allal2023-ej} removed exact duplicates in code data using N-gram overlaps, finding $1\%$ to $4\%$ performance improvements. \citep{Choi2022-pg} removed duplicates in negative examples for contrastive learning. Here, in contrast to prior work, we go beyond the notion of exact duplicates to define a parametrically variable notion of {\it semantic duplicates}, and demonstrate that remove of semantic duplicates can yield more data efficient learning at the scale of 100's of millions of data points in both vision and language. 

Beyond deduplication, a host of classical machine learning approaches seek to achieve data efficiency by finding {\it coresets}, defined as small subsets of the training data that can be used to train a machine learning algorithm to the same test accuracy achievable when training on the entire training data (see e.g. \citep{Guo2022-qy,Phillips2016-ob} for reviews). However, many coreset algorithms are computationally prohibitive and therefore are difficult to scale to web-scale data.  In contrast to many traditional coreset algorithms, we develop an exceedingly simple and tractable algorithm that achieves both computational and data efficiency at scale.  

Recent approaches to achieve data efficiency in deep learning have operated in a supervised setting by defining and finding ``hard'' examples not easily learned by partially or fully trained (ensembles of) models \citep{Toneva2019-hj,Paul2021-ci, Chitta2021-se,Feldman2020-yv,meding2022trivial}.  Perhaps the closest to our work is a recent effort to break beyond neural power law scaling by pruning unlabelled data, using the embedding space of a pre-trained foundation model \citep{Sorscher2022-wo}.  However, the largest dataset for which these works examined data pruning was ImageNet. In contrast we move from relatively small, highly curated ImageNet scale to highly uncurated, web-scale data. Our analysis, at this new large and uncurated scale, reveals a possibly fundamental role for semantic deduplication as an important initial step in data-pruning for self-supervised learning that was not considered in prior data-pruning works.  


\section{Clustering to reduce computation} \label{sec:methods_clustering}

The time complexity of naive de-duplication is $\mathcal{O}(n^2)$ where $n$ is the number of data points, making this approach impractical for large web-scale data. For example, the LAION-440M dataset would require $\approx 1.9 \mathrm{x} 10^{17}$ similarity computations. The $\mathcal{K}$-means clustering step in SemDeDup reduces this complexity substantially from $\mathcal{O}(n^2)$ to $\mathcal{O}(n^2/\mathcal{K})$ assuming approximately uniform cluster size. This means we only require $\approx 4.6 \mathrm{x} 10^{12}$ intra-cluster comparisons instead of $\approx 1.9 \mathrm{x} 10^{17}$ across all pairs, a 5-order of magnitude improvement. 

%\ari{expand on case were clusters are non-uniform?} \amro{uniform size needs: $\approx 3.9 \mathrm{x} 10^{12}$, actual number of operations: $\approx 4.6 \mathrm{x} 10^{12}$}

\subsection{What do semantic duplicates look like?} \label{sec:res-examples}

Many semantic duplicates are of products which may have been displayed on multiple e-commerce websites, each with a slightly different style. As a result, semantic duplicates often contain different, but highly similar captions.  While most clusters contained 20-40\% duplicates, there are several remarkable outliers in redundancy in LAION-440M (Fig. \ref{fig:cluster_size}), including one cluster containing $\approx 307,000$ copies of the European Union flag and another with $\approx 318,000$ copies of an icon of ``Image not found."

At higher levels of $\epsilon$ in Fig. \ref{fig:visduplicates}, \ref{fig:before_and_after_0}, \ref{fig:before_and_after_1}, and \ref{fig:before_and_after_2},  we find fewer semantic duplicates, which are generally derived from the same source image, and more pairs which exhibit semantic redundancy instead, in which the same concept is present, but not derived from the same image source. For example, semantically redundant pairs may contain different images of similar objects or scenes. 


\subsection{Why do models trained on uncurated data exhibit slower learning?}
We posit that successive learning iterations involving semantic duplicates yield redundant information, thereby wasting valuable computation on data points that are highly similar to those the model has already seen. By removing these semantic duplicates, we increase the fraction of data points which provide a marginal information gain to the model, thereby increasing learning speed \citep{Sorscher2022-wo}. 

\section{LAION-233M De-duplication} \label{sec:app-laion233}
To support our results on LAION-440M, we also de-duplicate a much smaller dataset of 233 millions images. We call this dataset LAION-233M. Usually, CLIP needs to be trained on more than 400 millions images as introduced in \citep{clip}, so de-duplicating LAION-233M is more challenging in this respect.
We trained a baseline model on the 233 millions images and two models on 55\% of the data, one on random subset and the other on de-duplicated subset. We trained all the models using the same hyperparameters we used for training on LAION-440M. We show ImageNet top1 zeroshot accuracy for these models in Fig. \ref{fig:laion233m_result}. The baseline model achieved 64.62\% accuracy, while the SemDeDup model achieved 63.61\% outperforming the model trained on random subset (61.3\% accuracy).

%%%%%%%%%%%%%%%%%%% LAION233m %%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht]
\begin{center}
\includegraphics[width = 0.5\columnwidth]{figures/openclip_results/laion233m_deduplication_acc_vs_iterations.pdf}

\caption{Training CLIP on 233 millions images from LAION-2B.}
\label{fig:laion233m_result}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{CLIP Zeroshot Evaluation}
In this section we show the result of zeroshot evaluation for CLIP. Models trained on  dataset de-duplicated SemDeDup outperform the baseline model in many tasks. In Table {\ref{table:zeroshot_results_table}} we list the top1 zeroshot accuracy on 25 tasks and in Table (\ref{table:out_of_distribution_results_table} we show the top1 zeroshot accuracy on 6 datasets for out-of-distribution robustness evaluation. Our complete evaluation set has 31 different datasets in total. When using only 63\% of LAION-440M, SemDeDup outperforms the baseline model in 20 out of the 31 tasks. Fig. (\ref{fig:all_zershot_line_plots}) and Fig. (\ref{fig:fig:all_OOD_zershot_line_plots}) show the performance of different models as a function on training dataset size.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centering
\subfigure[]{\includegraphics[width = 0.49\columnwidth]{figures/openclip_results/dedup80_vs_rand80_vs_baseline_laion440M_imagenet_zeroshot_acc.pdf}}
\subfigure[]{\includegraphics[width = 0.49\columnwidth]{figures/openclip_results/dedup60_vs_rand60_vs_baseline_laion440M_imagenet_zeroshot_acc.pdf}}
\subfigure[]{\includegraphics[width=0.49\columnwidth]{figures/openclip_results/dedup20_vs_rand20_vs_baseline_laion440M_imagenet_zeroshot_acc.pdf}}
\subfigure[]{\includegraphics[width=0.49\columnwidth]{figures/openclip_results/dedup40_vs_rand40_vs_baseline_laion440M_imagenet_zeroshot_acc.pdf}}

\caption{\text{SemDeDup} is always better than training on random subset from LAION-440M. The plot shows zeroshot top1 accuracy on ImageNet for CLIP models trained on different fractions of data .}
\label{fig:dedup_vs_rand}
\end{center}
\vskip -0.2in
\end{figure}

\ZeroshotCLassificationResultsTable

\ZeroshotOutOfDistributionResultTable

% All zeroshot plots
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{
 \includegraphics[width = 0.9\textwidth]{figures/eval_results/All_Datasets_Zeroshot_Top1_Accuracy.pdf}
}
\caption{Zeroshot performance on 25 datasets. The last plot shows the average performance over all datasets.}
\label{fig:all_zershot_line_plots}
\end{center}
\vskip -0.2in
\end{figure}

% All zeroshot plots
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{
 \includegraphics[width = 0.9\textwidth]{figures/eval_results/All_Datasets_OOD_Zeroshot_Top1_Accuracy.pdf}
}
\caption{Out-of-distribution zeroshot performance zeroshot performance on 6 datasets. The last plot shows the average performance over all datasets.}
\label{fig:fig:all_OOD_zershot_line_plots}
\end{center}
\vskip -0.2in
\end{figure}


% \section{How many images have duplicates?}

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}

%  \subfigure[]{\includegraphics[width = 0.4\textwidth]{figures/percentage_of_images_with_at_least_one_duplicate.pdf}}
% \subfigure[]{\includegraphics[width = 0.4\textwidth]{figures/maximum_within_cluster_pairwise_similarity_for_each_images.pdf}}
  
% \caption{(a) Percentage of images with at least one duplicate. We consider a pair of images as duplicates if they have at least 1-$\epsilon$ cosine similarity between their representation. We see that when $\epsilon$ is 0.26 (0.74 cosine similarity as threshold), 90\% of the images have duplicates. (b) histogram of maximum pairwise similarity each image has in a cluster. The bar at 1.0 shows that we can identify a highly similar image(s) for most of the images in a cluster. \todo{Ari/Surya - move this to main plot; if you have time, add another point to the x-axis for the left figure;  for future version we should fix this and just take random subsample}}
% \label{fig:images_with_at_least_one_duplicate}
% \end{center}
% % \vskip -0.2in
% \end{figure}


%%%%%%%%%% How many images can we remove from each cluster %%%%%%%
% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{
%  \includegraphics[width = 0.3\textwidth]{figures/number_of_duplicates.pdf}
% }
% \caption{How many images can we remove from each cluster?. Moving from top to down we increase $\epsilon$ value. As we increase $\epsilon$, more examples are removed from each cluster. \todo{Ari - change this to be fraction of data removed (fraction of data that is duplicates) as a function of cluster size; have different lines for different epsilons; Amro - just do it from the data in this plot}}
% \label{fig:}
% \end{center}
% \vskip -0.2in
% \end{figure}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{
 \includegraphics[width = 0.6\textwidth]{figures/Fraction_of_Data_Removed_From_Cluster.pdf}
}
\caption{How many images can we remove from each cluster?. Moving from top to down we increase $\epsilon$ value. As we increase $\epsilon$, more examples are removed from each cluster. The x axis corresponds to the cluster size. We notice that most of examples from the large clusters (the points to the right) are removed when $\epsilon$ becomes large. The points in this figure are for 2000 clusters sampled randomly from a total of 50,000 clusters.}
\label{fig:Fraction_of_Data_Removed_From_Cluster}
\end{center}
\vskip -0.2in
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






%%%%%%%%%%%%%%%%%%% cluster_size %%%%%%%%%%%%%%%
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{
 \includegraphics[width = 0.6\textwidth]{figures/cluster_size.pdf}
}

\caption{The number of images in each cluster for 50000 clusters of LAION-440M images after running K-means clustering on images embeddings. The average cluster size is 8,748, but we also see few clsuters with more than 300,000 examples. }
\label{fig:cluster_size}
\end{center}
\vskip -0.2in
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%% clusters statistics %%%%%%%%%%%%%%%
% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{
%  \subfigure[]{\includegraphics[width = 0.3\textwidth]{figures/Avegerage_cosine_similarity_to_cluster_centroid.pdf}}
%  \subfigure[]{\includegraphics[width = 0.3\textwidth]{figures/Std_of_cosine_similarity_to_cluster_centroid.pdf}}
%  \subfigure[]{\includegraphics[width = 0.3\textwidth]{figures/Std_of_pairwise_cosine_similarity_between_cluster_items.pdf}}
% }

% \caption{(a) Avegerage_cosine_similarity_to_cluster_centroid. (b) Std_of_cosine_similarity_to_cluster_centroid. (c) Std_of_pairwise_cosine_similarity_between_cluster_items}
% \label{fig:}
% \end{center}
% \vskip -0.2in
% \end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\section{Visualizing Examples Before and After De-duplication}
To visually show which images are removed by SemDeDup from LAION-440M dataset, we viualize some images from a random cluster before and after de-duplicaton. To do that, we choose a cluster randomly and sort its examples by the cosine similarity to the centroid. By doing this, we can show the similar images next to each other. Then we visualize a sequence of images before de-duplicaton. After that we run SemDeDup, remove examples and sort the remaining examples again. Finally we visualize the squence of images from the same indices we visualze them before de-duplication. Figures (\ref{fig:before_and_after_0}-\ref{fig:before_and_after_2}) show that after applying SemDeDup with different values for the de-duplication threshold $\epsilon$, we keep the unique images. For example we see in Fig. (\ref{fig:before_and_after_0}) that the cluster contains many similar images of watches and as we remove more images with higher value of $\epsilon$ we keep more unique examples that are less semantically redundant.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width = 0.8\textwidth]{figures/before_and_after/before_and_after_-1_587.pdf}}
\centerline{\includegraphics[width = 0.8\textwidth]{figures/before_and_after/before_and_after_0.03_587.pdf}}
\centerline{\includegraphics[width = 0.8\textwidth]{figures/before_and_after/before_and_after_0.205_587.pdf}}

\caption{Examples from the same cluster from LAION-440M dataset before and after de-duplication. Images are sorted by the cosine similarity to the cluster centroid. As we increase the de-duplication threshold we start to see more unique images.}
\label{fig:before_and_after_0}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width = 0.8\textwidth]{figures/before_and_after/before_and_after_-1_6756.pdf}}
\centerline{\includegraphics[width = 0.8\textwidth]{figures/before_and_after/before_and_after_0.03_6756.pdf}}
\centerline{\includegraphics[width = 0.8\textwidth]{figures/before_and_after/before_and_after_0.07_6756.pdf}}

\caption{Examples from the same cluster from LAION-440M dataset before and after de-duplication. Images are sorted by the cosine similarity to the cluster centroid. As we increase the de-dplication threshold we start to see more unique images.}
\label{fig:before_and_after_1}
\end{center}
\vskip -0.2in
\end{figure}





\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width = 0.8\textwidth]{figures/before_and_after/before_and_after_-1_20522.pdf}}
\centerline{\includegraphics[width = 0.8\textwidth]{figures/before_and_after/before_and_after_0.03_20522.pdf}}
\centerline{\includegraphics[width = 0.8\textwidth]{figures/before_and_after/before_and_after_0.07_20522.pdf}}

\caption{Examples from the same cluster from LAION-440M dataset before and after de-duplication. Images are sorted by the cosine similarity to the cluster centroid. As we increase the de-dplication threshold we start to see more unique images.}
\label{fig:before_and_after_2}
\end{center}
\vskip -0.2in
\end{figure}

% \section{Perplexity Values for SemDeDup on Language Modeling} \label{app:ppl}

\begin{figure}[ht]
\begin{center}
% \begin{tabular}{lllll}
\begin{tabular}{p{3cm}p{2.3cm}p{2.3cm}p{2.3cm}p{2.cm}}

\toprule
 &        Baseline Training (no pruning) & NearDup from \citet{LeeDedup} &          Random &        SemDeDup \\
validation set       &                 &                   &                 &                 \\
\midrule
C4                   &  38.95 +/- 0.07 &    39.46 +/- 0.14 &  39.51 +/- 0.07 &  \textbf{39.35} +/- 0.16 \\
opt\_valid                  &  50.66 +/- 0.19 &    \textbf{50.67} +/- 0.03 &  51.31 +/- 0.23 &  \textbf{50.68} +/- 0.28 \\
prompts\_with\_answers &  29.60 +/- 0.15 &    29.79 +/- 0.11 &  29.95 +/- 0.11 &  \textbf{29.69} +/- 0.19 \\
\bottomrule
\end{tabular}

\caption{Comparison of perplexity values for 125M after pruning via different methods at 96\% pruning. Note that \citet{LeeDedup} pruned 3.9 \% of examples, while above Random and SemDeDup prune 4\% of examples. Mean and standard deviation provided across 3 training seeds.}
\label{table:nlp_appendix_125m_c4_matched_epoch}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\begin{tabular}{llll}
\toprule
&        Baseline Training (no pruning) &          Random &        SemDeDup \\
validation set       &                 &                 &                 \\
\midrule
C4                   &  38.95 +/- 0.07 &  40.43 +/- 0.06 &  \textbf{39.80} +/- 0.40 \\
opt\_valid            &  50.66 +/- 0.19 &  52.33 +/- 0.29 &  \textbf{50.72} +/- 0.16 \\
prompts\_with\_answers &  29.60 +/- 0.15 &  30.56 +/- 0.04 &  \textbf{29.73} +/- 0.20 \\
\bottomrule
\end{tabular}
\caption{Comparison of perplexity values for 125M after pruning via different methods at 90\% pruning. Mean and standard deviation provided across 3 training seeds.}
\label{table:nlp_appendix_125m_c4_matched_epoch_90}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\begin{tabular}{llll}
\toprule
 &        Baseline Training (no pruning) &          Random &        SemDeDup \\
validation set       &                 &                 &                 \\
\midrule
C4                   &  38.95 +/- 0.07 &  42.16 +/- 0.03 &  \textbf{41.98} +/- 0.09 \\
opt\_valid            &  50.66 +/- 0.19 &  54.36 +/- 0.12 &  \textbf{52.63} +/- 0.16 \\
prompts\_with\_answers &  29.60 +/- 0.15 &  31.65 +/- 0.16 &  \textbf{30.98} +/- 0.13 \\
\bottomrule
\end{tabular}

\caption{Comparison of perplexity values for 125M after pruning via different methods at 80\% pruning. Mean and standard deviation provided across 3 training seeds.}
\label{table:nlp_appendix_125m_c4_matched_epoch_80}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
% \begin{tabular}{lllll}
\begin{tabular}{p{3cm}p{2.3cm}p{2.3cm}p{2.3cm}p{2.cm}}
\toprule
 &        Baseline Training (no pruning) & NearDup from \citet{LeeDedup} &          Random &        SemDeDup \\
validation set       &                 &                   &                 &                 \\
\midrule
C4                   &  46.16 &    46.85 &  \textbf{46.15} &  46.56  \\
opt\_valid            &  59.64 &    59.27 &  59.20 &  \textbf{58.89} \\
prompts\_with\_answers &  34.04 &    33.93 &  33.91 &  \textbf{33.83} \\
\bottomrule
\end{tabular}
\caption{Comparison of perplexity values for 1.3b model after pruning via different methods at 96\% pruning. Note that \citet{LeeDedup} pruned 3.9 \% of examples, while above Random and SemDeDup prune 4\% of examples. Due to compute restrictions we do not provide random seed standard deviations.}
\label{table:nlp_appendix_1.3b_c4_matched_epoch_96}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\begin{tabular}{llll}
\toprule
 &        Baseline Training (no pruning) &          Random &        SemDeDup \\
validation set       &                 &                 &                 \\
\midrule
C4                   &  46.16 &  \textbf{62.50} &  63.36 \\
opt\_valid            &  59.64 &  76.09  &  \textbf{75.57}  \\
prompts\_with\_answers &  34.04 &  43.54 &  \textbf{42.33} \\
\bottomrule
\end{tabular}
\caption{Comparison of perplexity values for 1.3b model after pruning via different methods at 50\% pruning.  Due to compute restrictions we do not provide random seed standard deviations.}
\label{table:nlp_appendix_1.3b_c4_matched_epoch_50}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\begin{tabular}{llll}
\toprule
 &        Baseline Training (no pruning) &           Random &         SemDeDup \\
validation set       &                 &                  &                  \\
\midrule
C4                   &  46.16 &  303.71 &  \textbf{108.95} \\
opt\_valid            &  59.64 &  347.77 &  \textbf{115.69} \\
prompts\_with\_answers &  34.04 &  269.96 &   \textbf{72.64} \\
\bottomrule
\end{tabular}
\caption{Comparison of perplexity values for 1.3b model after pruning via different methods at 20\% pruning. Due to compute restrictions we do not provide random seed standard deviations.}
\label{table:nlp_appendix_1.3b_c4_matched_epoch_20}
\end{center}
\end{figure}


% \section{Qualitative Examples of SemDeDup on C4} \label{app:qualitative}
\clearpage

% \section{Full Plots for SemDeDup for Language Modeling} \label{app:language}

\begin{figure}[ht]
\begin{center}
\includegraphics[width=0.49\columnwidth]{figures_nlp/eps_vs_remain_data.pdf}


\caption{\textbf{Percent Data Remaining versus $\epsilon$ for C4}.  The x-axis corresponds to different values of $\epsilon$ from Section~\ref{sec:methods}, the y-axis represents the corresponding fraction of data in our subset of C4.}
\label{fig:nlp_appendix_125M_eps_vs_frac_data}
\end{center}
\end{figure}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centering
\subfigure[]{\includegraphics[width = 0.49\columnwidth]{figures_nlp/matched_epochs/full/C4_125M_matched_epoch_full.pdf}}
\subfigure[]{\includegraphics[width = 0.49\columnwidth]{figures_nlp/matched_epochs/full/opt_valid_125M_matched_epoch_full.pdf}}
\subfigure[]{\includegraphics[width=0.49\columnwidth]{figures_nlp/matched_epochs/full/prompts_with_answers_125M_matched_epoch_full.pdf}}

\caption{\text{SemDeDup} performance at different fractions of data for the 125M OPT model. We show results for the C4 validfation set (top left), opt\_valid (top right) and prompts\_with\_answers (bottoms). These are the same graphs as Figure~\ref{fig:nlp_125M_matched_epoch_c4}, but for a wider range of percentage of data kept. We note that \text{SemDeDup} consistently outperforms random pruning at lower percentages of data kept.}
\label{fig:nlp_appendix_125m_full_plots_matched_epoch}
\end{center}
\vskip -0.2in
\end{figure}



\begin{figure}[ht]
\begin{center}
\includegraphics[width = 1.0\textwidth]{figures_nlp/matched_epochs/big_OOD_plot.pdf}

\caption{Percentage of Data Kept vs. Perplexity on individual validation sets within opt\_valid. Runs are averages across 3 training seeds, and shaded regions represents 1 standard deviation from the mean. The title of each plot represents the name of the individual validation set within opt\_valid. Note that on all tasks, SemDedup significantly random pruning, especially at low percentages of data kept.}
\label{fig:nlp_appendix_big_ood_plot_matched_epochs}
\end{center}
\end{figure}

\begin{figure}[ht]
\begin{center}
\includegraphics[width = 1.0\textwidth]{figures_nlp/efficiency_graphs/big_OOD_plot_efficiency.pdf}

\caption{Percentage of Data Kept vs. Efficiency Gain on individual validation sets within opt\_valid. Runs are averaged across training seeds where the model achieves baseline perplexity at some point in training, and shaded regions represents 1 standard deviation from the mean. The title of each plot represents the name of the individual validation set within opt\_valid.}
\label{fig:nlp_appendix_big_ood_plot_efficiency}
\end{center}
\end{figure}




\begin{figure}[ht]
\setlength{\tabcolsep}{.5pt} % Default value: 6pt
\begin{center}
\vskip 0.3in
Keeping 90\% data
\begin{tabular}{l}
\toprule
 text \\
\midrule
It appears that you already have an account on this site associated with . To connect your existing account... \\
\bottomrule
\end{tabular}

\vskip 0.3in
Keeping 100\% data (i.e. no pruning) 
\begin{tabular}{l}
\toprule
text \\
\midrule
It appears that you already have an account on this site associated with . To connect your existing account... \\
You are visiting the placeholder page for Wells Williams. This page is here because someone used our ...\\
 &  You are visiting the placeholder page for Mathew Barrett. This page is here because someone used our ... \\
You are visiting the placeholder page for Marcus Slatar. This page is here because someone used our ... \\
 You are visiting the placeholder page for Bernice Andrews. This page is here because someone used our ... \\
 You are visiting the placeholder page for Emiko Chille. This page is here because someone used our ... \\
 You are visiting the placeholder page for Landon Buckland. This page is here because someone used our ... \\
 & .... \\
You are visiting the placeholder page for Kylie Dickens. This page is here because someone used our  ... \\
\bottomrule
\end{tabular}
\caption{Example of semantic de-duplication with SemDeDup (cluster 4500). See Section \ref{sec:nlp_what_is_being_pruned} for discussion.}
\label{table:nlp_appendix_examples_one}
\end{center}
\end{figure}


\begin{figure}[ht]

\begin{center}
\vskip 0.3in
Keeping 20\% data
% \begin{tabular}{lrl}
\begin{tabular}{p{15cm}}

\toprule
text \\
\midrule
cheap jordan shoes from china free shipping,order maroon foams , jordan blue retro 12 , jordans sz 10 , all white 14s... \\
Booming business thanks to Cristiano Ronaldo! Nike Presents Cristiano Ronaldo – CR7 Winter Collection ... \\
\bottomrule
\end{tabular}

\vskip 0.3in
Keeping 90\% data

% \begin{tabular}{l}
\begin{tabular}{p{15cm}}

\toprule
 text \\
\midrule
 Purchase from us, you can get max discount and free shipping.Free shipping and returns on Nike Jordans .... \\
 Product range. Adidas collections are divided into three groups: Sport Performance, Originals, and Sport Style ... \\
cool jordans for boys , foamposite paranorman , new black and white foams , lebron 1's ,cheap jordans online ... \\
This Comfortable Nike Huarache Free Basketball And Running has 1600 x 900 pixel resolution with jpeg format. .. \\
Top Rating: “Best high performance product.” Performance efficiency. That is the motto of our textile engineers by... \\
cheap jordan shoes online free shipping order cheap jordans for sale free shipping. Air Jordan 1's new theme ... \\
... \\
Trendy Men's Nike Kyrie 1 Best Seller 'All Star' Multicolor at high discount. Buy Nike Trainers - The Kyrie 1 All ... \\
\bottomrule
\end{tabular}
\end{center}
\caption{Example of semantically redundant de-duplication with SemDeDup (cluster 4900). See Section \ref{sec:nlp_what_is_being_pruned} for discussion.}
\label{table:nlp_appendix_examples_two}
\end{figure}


\end{document}
