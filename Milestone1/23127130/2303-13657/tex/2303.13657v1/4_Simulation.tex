\section{Application to Risk-Averse LQR}\label{Sec:riskaversecontrol}
In this section, we consider a risk-averse LQR problem and leverage the closed-form expression of the random return $G^K(x)$ to obtain an optimal policy. Since the distribution of the random return $G^K(x)$ consists of an infinite number of random variables, it is computationally unwieldy. 
Instead, we employ the approximate random return $G^K_N(x)$ proposed in Section~\ref{Sec:Approxreturn}.
As a risk measure for the problem at hand, 
we select the well-known Conditional Value at Risk (CVaR) \citep{rockafellar2000optimization}. 
We then construct an approximate risk-averse objective function, as $\hat{\mathcal{C}}_N(K):={\rm{CVaR}}_{\alpha}\left[ {G}_N^K(x)\right]$.  
For a random variable $Z$ with the CDF $F$ and a risk level $\alpha \in (0,1]$, the ${\rm{CVaR}}$ value is defined as ${\rm{CVaR}}_{\alpha}[Z] = \mathbb{E}_F[Z| Z>Z^{\alpha}]$, where $Z^{\alpha}$ is the $1-\alpha$ quantile of the distribution of the random variable $Z$. 
Given this objective function, the goal is to find the optimal risk-averse controller, that is, to select the feedback gain $K$ that minimises $\hat{\mathcal{C}}_N(K)$. 

\subsection{Risk-Averse Policy Gradient Algorithm}

 %We use the Conditional Value at Risk (CVaR) as the risk measure and construct the risk-averse objective function as $\mathcal{C}(K) := {\rm{CVaR}}_{\alpha}\left[ G^K(x)\right]$. The goal is to use policy gradient method to find the optimal risk-averse controller, that is,  selecting the feedback gain $K$ that minimizes   $\mathcal{C}(K)$.  Since the random return $G^K(x)$ consists of infinite numbers of random variables, its distribution is computationally intractable. Instead, we use the approximated random return $G^K_N(x)$ with a finite numbers of random variables to construct an approximated risk-averse objective function $\hat{\mathcal{C}}_N(K):={\rm{CVaR}}_{\alpha}\left[ {G}_N^K(x)\right]$. Then,  we  consider an alternative problem:  $\min_{K} \hat{\mathcal{C}}_N(K)$. 

In what follows, we propose a policy gradient method to solve this problem. We assume that the matrices $A,B,Q,R$ are known. The first-order gradient descent step is hard to compute as it hinges on the gradient of the CVaR function. Therefore, we rely on zeroth-order optimisation to derive the policy gradient, as detailed in Algorithm~\ref{alg:algorithm_PG}. 

%Policy gradient methods can be used to solve this problem. We assume that the matrices $A,B,Q,R$ are known. However, the first-order gradient descent is still intractable due to the fact that the gradient of the CVaR is hardly computable. Therefore, we use the zeroth-order optimization to implement policy gradient, which is detailed in Algorithm~\ref{Alg:zeroopt}. 

%Random variable $G(K) := G^{\pi}(x) $, ${G}_N(K):={G}^{\pi}_N(x)$. 



\begin{algorithm}[t]
\caption{Risk-Averse Policy Gradient}
\begin{algorithmic}[1]
\REQUIRE  initial values $K_0$, $x$, step size $\eta$, smoothing parameter $\delta$, and dimension $n$
    \FOR {$episode \; t=1,\ldots,T$}
        \STATE Sample $\hat{K}_t = K_t + U_t$, where $U_t$ is drawn at random over matrices whose norm is $\delta$;
        \STATE Compute the distribution of the random variable ${G}_N^{\hat{K}_t}$;
        \STATE Compute $\hat{\mathcal{C}}_N(\hat{K}_t)$;
        \STATE $K_{t+1}= K_t - \eta g_t$, where $g_t= \frac{n}{\delta^2} \Big(\hat{\mathcal{C}}(\hat{K}_t)- \hat{\mathcal{C}}(\hat{K}_{t-1}) \Big) U_t $.
    \ENDFOR
\end{algorithmic}\label{alg:algorithm_PG}
\end{algorithm}

%


Specifically, at each episode $t$, we sample an approximate feedback gain $\hat{K}_t = K_t +U_t$, where $U_t$ is drawn uniformly at random from the set of matrices with norm $\delta$. Given $\hat{K}_t$, we compute the approximate distribution of the random return ${G}_N^{\hat{K}_t}(x)$ in \eqref{eq:approxreturn} and the value of $\hat{\mathcal{C}}_N(\hat{K}_t)$. Then, we can perform the feedback gain update as
  $K_{t+1}= K_t - \eta g_t$,
where $g_t= \frac{n}{\delta^2} \Big(\hat{\mathcal{C}}(\hat{K}_t)- \hat{\mathcal{C}}(\hat{K}_{t-1}) \Big) U_i $. Here, the zeroth-order residual feedback technique proposed in \citet{zhang2022new} is used to reduce the variance.
The theoretical analysis of this algorithm is left as our future work.




\begin{figure}[t]
    \centering
	\subfigure[The $K$ values  when $\alpha=1$.]{
	\includegraphics[scale=0.24]{figures/S2_F1}}
	\subfigure[The ${\rm{CVaR}}$ values  when $\alpha=1$.]{
	\includegraphics[scale=0.24]{figures/S2_F2}}
        \subfigure[The $K$ values  when $\alpha=0.4$.]{
	\includegraphics[scale=0.24]{figures/S2_F3}}
        \subfigure[The ${\rm{CVaR}}$ values when $\alpha=0.4$.]{
	\includegraphics[scale=0.24]{figures/S2_F4}}
	\caption{Risk-averse control  using Algorithm~\ref{alg:algorithm_PG}. The solid lines are averages over 20 runs.} \label{fig:S2}
\end{figure}

\subsection{Numerical Experiments}
Next, we consider a risk-averse LQR  problem and experimentally illustrate the performance of Algorithm~\ref{alg:algorithm_PG}. We illustrate our approach for the same  scalar system with the same cost function as in Section~\ref{Sec:NemVer}. 
The other parameters are selected as $\gamma=0.6$, $\delta=0.1$, $\eta=0.0004$, $N= 10$, respectively. The initial controller is set as $K_0=-0.2$, which is a stable one.


We first set $\alpha=1$: in this case, the risk-averse control problem is reduced to a risk-neutral control problem. Therefore, we can use traditional LQR techniques to compute the optimal feedback gain $K^{*}=-0.4684$. 
We run the proposed risk-averse policy gradient Algorithm~\ref{alg:algorithm_PG} and the simulation results are presented in Fig.~\ref{fig:S2} (a) and (b). Specifically, in Fig.~\ref{fig:S2} (a), the controller $K$ returned by Algorithm 1 converges to $K^{*}$, which verifies our proposed method for the risk-neutral case. Fig.~\ref{fig:S2} (b) illustrates the values of ${\rm{CVaR}}$ achieved by Algorithm~\ref{alg:algorithm_PG}.
%
Additionally, we select $\alpha=0.4$ to find the optimal risk-averse controller. The simulation results are presented in Fig.~\ref{fig:S2} (c) and (d). We see that $K$  converges to $-0.55$, which leads to a smaller $A+BK$ compared to $K^{*}=-0.4684$. 





% \begin{figure}[htbp]
%     \centering
% 	\subfigure[The values of $K$.]{
% 	\includegraphics[scale=0.3]{figures/S2_F3.eps}}
% 	\quad
% 	\subfigure[The values of ${\rm{CVaR}}$.]{
% 	\includegraphics[scale=0.3]{figures/S2_F4.eps}}
% 	\caption{The simulation result achieved by Algorithm~\ref{alg:algorithm_PG} when $\alpha=0.4$. The solid lines are averages over 20 runs.} \label{fig:S2-risk}
% \end{figure}



% {\color{blue} It would be better to explain why smaller $\alpha$ leaders to smaller $K$}