\vspace{-0.2cm}
\section{Problem Statement}
Consider a discrete-time linear dynamical system:
\begin{align}
    x_{t+1} = A x_t + B u_t +v_t ,
\end{align}
where $x_t \in \mathbb{R}^n$, $u_t\in \mathbb{R}^p$, $v_t \in \mathbb{R}^n$ are the system state, control input, and the exogenous disturbance, respectively. 
We assume that the exogenous disturbances $v_t$ with bounded moments, $t\in \mathbb{N}$, are i.i.d. sampled from a distribution $ \mathcal{D}$ of arbitrary form.

\vspace{-0.2cm}
\subsection{Classical LQR}
The canonical  LQR problem aims to find a control policy $\pi: \mathbb{R}^n \rightarrow \mathbb{R}^p$ to minimise the objective
%\begin{align}
 %   J(u) = \mathbb{E}\left[ \sum_{t=0}^{\infty} \gamma^t c_t(x_t,u_t) \right]
 %   = \mathbb{E}\left[ \sum_{t=0}^{\infty} \gamma^t (x_t^T Q x_t + u_t^T R u_t)  \right],
%\end{align}
\begin{align}
    J(u) %= \mathbb{E}\left[ \sum_{t=0}^{\infty} \gamma^t c_t(x_t,u_t) \right]
    = \mathbb{E}\left[ \sum_{t=0}^{\infty} \gamma^t (x_t^T Q x_t + u_t^T R u_t)  \right],
\end{align}
where $Q,R$ are positive-definite constant matrices and $\gamma \in (0,1)$ is a discount parameter. Given a control policy $\pi$, let $V^{\pi}(x) = \mathbb{E}\left[ \sum_{t=0}^{\infty} \gamma^k (x_t^T Q x_t + u_t^T R u_t)  \right]$ denote the expected return from an initial state $x_0 =x$ with $ u_t = \pi( x_t)$.
% \begin{align}
%     V^{\pi}(x) = \mathbb{E}\left[ \sum_{t=0}^{\infty} \gamma^k (x_t^T Q x_t + u_t^T R u_t)  \right],\quad u_t = \pi( x_t),
% \end{align}
% denote the expected return from an initial state $x_0 =x$.
For the static linear policy $\pi(x_t)=K x_t$, the value function $V^{\pi}(x)$ satisfies the Bellman equation
\begin{align}\label{eq:Bellman expectation}
    V^{\pi}(x) %=\mathop{\mathbb{E}}_{X' = (A+BK)x+v_0 }[ c_0 + \gamma V^{\pi}(X')]
    &= x^T (Q + K^T R K) x + \gamma \mathop{\mathbb{E}}_{X' = (A+BK)x+v_0 } [V^{\pi}(X')],  
\end{align} 
where the capital letter $X'$ denotes a random variable over which we take the expectation.

When the exogenous disturbances $v_t$ are normally distributed with zero mean, the value function is known to take the quadratic form  $V^{\pi}(x) = x^T P x +q$, where $P>0$ is the solution of the Lyapunov equation $P = Q+ K^T R K + \gamma A_K^T P A_K$ and $q$ is a scalar related to the variance of $v_t$. In particular, the optimal control feedback gain is obtained as $K^*=-\gamma(R+\gamma B^TPB)^{-1}PA$ and $P$ is the solution to the classic Riccati equation $P = \gamma A^T P A - \gamma^2 A^T P B (R+\gamma B^T P B)^{-1} B^T P A+Q$. 




%When the exogenous disturbances $v_t$ are \emph{\yulong{Gaussian with zero mean}}, the value function is shown to take the quadratic form  $V^{\pi}(x) = x^T P x +q$, where $P>0$ is a solution of the Riccati equation $P = \gamma A^T P A - \gamma^2 A^T P B (R+\gamma B^T P B)^{-1} B^T P A$ and $q$ is a scalar related to the variance of $v_t$.


%When the exogenous disturbances $v_t$ are \emph{\yulong{Gaussian with zero mean}}, the value function is shown to take the quadratic form  $V^{\pi}(x) = x^T P x +q$, where $P>0$ is a solution of the Ricatti equation $P = \gamma A^T P A - \gamma^2 A^T P B (R+\gamma B^T P B)^{-1} B^T P A$ and $q$ is a scalar related to the variance of $v_t$.


% \begin{align}
%     V^{*}(x) &= x^T P x + q,
% \end{align}
% where $P = \gamma A^T P A - \gamma^2 A^T P B (R+\gamma B^T P B)^{-1} B^T P A$ and $q = \frac{  \gamma}{1-\gamma} {\rm{Tr}}(PR_v)$, {\color{blue}where the matrix $R_v$ is the covariance of random variable $v$.  }

\subsection{Distributional LQR}
Motivated by the advantages of DRL in better understanding the effects of the randomness in the environment and in considering more general optimality criteria, in this paper we propose a distributional approach to the LQR problem.
%
Unlike classical reinforcement learning, which relies on expected returns, DRL \citep{bdr2022} relies on the distribution of random returns.
%
The return distribution characterises the probability distribution of different returns generated by a given policy and, as such, it contains much richer information on the performance of a given policy compared to the expected return.
%
In the context of LQR, we
 denote by $G^{\pi}(x)$ the random return using the static control strategy $u_t = \pi( x_t)$ from the initial state $x_0=x$, which is defined as
\begin{align}
    G^{\pi}(x) %=  \sum_{t=0}^{\infty} \gamma^t c_t(x_t,u_t) 
    = \sum_{t=0}^{\infty} \gamma^t (x_t^T Q x_t + u_t^T R u_t) , \quad  u_t = \pi( x_t),x_0 = x.
\end{align}
%
It is straightforward to see that the expectation of $G^{\pi}(x)$ is equivalent to the value function $V^{\pi}(x)$. 
The standard Bellman equation in \eqref{eq:Bellman expectation} decomposes the long-term expected return into an immediate stage cost plus the expected return of future actions starting at the next step. 
%In fact, we can do the analog for the random return $G^{\pi}(x)$ with some modifications. Specifically, 
Similarly, we can define the distributional Bellman equation for the random return as 
\begin{align}\label{eq:rv:Bellman}
    G^{\pi}(x) & \mathop{=}^{D} x^TQx+\pi(x)^TR\pi(x) + \gamma G^{\pi}(X'), \quad X' = Ax+B\pi(x)+v_0.
\end{align}
Here we use the notation $\mathop{=}\limits^{D}$ to denote that two random variables $Z_1,Z_2$ are equal in distribution, i.e., $Z_1 \mathop{=}\limits^{D} Z_2$. Note  that $X'$ denotes a random variable, as in \eqref{eq:Bellman expectation}.
Compared to the expected return in LQR, which is a scalar, here the return distribution is infinite-dimensional and can have a complex form. 
It is  challenging to estimate an infinite-dimensional function exactly with finite data and thus an approximation of the return distribution is  necessary in practice. 
%


In this paper,  we first analytically characterise the random return for the LQR problem. Then we  show how to  approximate the  distribution of the random return using finite random variables, so such that the approximated distribution is computationally tractable and the  approximation error is bounded. The proposed distributional LQR framework allows us to consider more general optimality criteria, which we demonstrate by using the proposed return distribution to develop a policy gradient algorithm for risk-averse LQR.
%


