\documentclass[12pt]{l4dc2023}

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e


\newcommand{\yulong}[1]{\textcolor{cyan}{(Yulong:  #1)}}

\title[Policy Evaluation in Distributional LQR]{Policy Evaluation in Distributional LQR}
\usepackage{times}
\usepackage{float}
\usepackage{algorithmic}
\usepackage{algorithm}
%\usepackage{algpseudocode}
%\usepackage{algorithmic}
%\usepackage{graphicx}


% \usepackage[ruled,vlined]{algorithm2e}       % algorithm
% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}

% Authors with different addresses:


\author{%
 \Name{Zifan Wang}$^1$ \Email{zifanw@kth.se} \\
 \Name{Yulong Gao}$^2$ \Email{yulong.gao@cs.ox.ac.uk} \\
 \Name{Siyi Wang}$^3$ \Email{siyi.wang@tum.de} \\
 \Name{Michael M. Zavlanos}$^4$ \Email{michael.zavlanos@duke.edu} \\
  \Name{Alessandro Abate}$^2$ \Email{alessandro.abate@cs.ox.ac.uk}\\
  \Name{Karl H. Johansson}$^1$ \Email{kallej@kth.se} 
  \\
  \addr $^1$ Division of Decision and Control Systems, KTH Royal Institute of Technology, Sweden\\
   \addr $^2$ Department of Computer Science, University of Oxford, UK\\
   \addr $^3$ Chair of Information-oriented Control, Technical University of Munich, Germany\\
    \addr $^4$ Department of Mechanical Engineering and Materials Science, Duke University, USA
}

% \author{%
%  \Name{Zifan Wang} \Email{zifanw@kth.se} \\
%  \addr Division of Decision and Control Systems, KTH Royal Institute of Technology, Sweden
% \\
%  \Name{Yulong Gao} \Email{yulong.gao@cs.ox.ac.uk} \\
%  \addr Department of Computer Science, University of Oxford
% \\
%  \Name{Siyi Wang} \Email{siyi.wang@tum.de} \\
%  \addr Chair of Information-oriented Control, Technical University of Munich, Germany
% \\
%  \Name{Michael M. Zavlanos} \Email{michael.zavlanos@duke.edu} \\
%  \addr Department of Mechanical Engineering and Materials Science, Duke University, USA
% \\
%   \Name{Alessandro Abate} \Email{alessandro.abate@cs.ox.ac.uk}\\
%   \addr Department of Computer Science, University of Oxford 
% \\
%   \Name{Karl H. Johansson} \Email{kallej@kth.se} \\
%   \addr Division of Decision and Control Systems, KTH Royal Institute of Technology, Sweden
% }




\begin{document}

\maketitle

\begin{abstract}%
Distributional reinforcement learning (DRL) enhances the understanding of the effects of the randomness  in the environment by letting agents learn the distribution of a random return, rather than its expected value as in standard RL. 
%
At the same time, a main challenge in DRL is that policy evaluation in DRL typically relies on the representation of the return distribution, which needs to be carefully designed.
%
In this paper, we address this challenge for a special class of DRL problems that rely on discounted linear quadratic regulator (LQR) for control, advocating for a new distributional approach to LQR, which we call \emph{distributional LQR}. Specifically, we provide  a closed-form expression of the distribution of the random return which, remarkably, is applicable to all exogenous disturbances on the dynamics, as long as they are independent and identically distributed (i.i.d.).
%
While the proposed exact return distribution consists of infinitely many random variables, we show that this distribution can be approximated by a finite number of random variables, and the associated approximation error can be analytically bounded under mild assumptions. 
%
Using the approximate return distribution, we propose a zeroth-order policy gradient algorithm for risk-averse LQR using the Conditional Value at Risk (CVaR) as a measure of risk. 
%
Numerical experiments are provided to illustrate our theoretical results.
\end{abstract}

\begin{keywords}%
  Distributional LQR, distributional RL, policy evaluation, risk-averse control% 
\end{keywords}



% \begin{itemize}
%   \item Limit the main text (not counting references) to 10 PMLR-formatted pages, using this template.
%   \item Include {\em in the main text} enough details, including proof details, to convince the reviewers of the contribution, novelty and significance of the submissions.
% \end{itemize}

% Acknowledgments---Will not appear in anonymized version



\input{1_Introduction.tex}

\input{2_Proble.tex}

\input{3_Main.tex}

\input{4_Simulation.tex}

\input{5_Conclusion.tex}

\acks{This work is supported in part by the Knut and Alice Wallenberg Foundation, the Swedish Strategic Research Foundation, the Swedish Research Council,  AFOSR under award \#FA9550-19-1-0169, and  NSF under award CNS-1932011.}

%\bibliography{bibfile}

\begin{thebibliography}{22}
	\providecommand{\natexlab}[1]{#1}
	\providecommand{\url}[1]{\texttt{#1}}
	\expandafter\ifx\csname urlstyle\endcsname\relax
	\providecommand{\doi}[1]{doi: #1}\else
	\providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi
	
	\bibitem[Barth-Maron et~al.(2018)Barth-Maron, Hoffman, Budden, Dabney, Horgan,
	Tb, Muldal, Heess, and Lillicrap]{barth2018distributed}
	Gabriel Barth-Maron, Matthew~W Hoffman, David Budden, Will Dabney, Dan Horgan,
	Dhruva Tb, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap.
	\newblock Distributed distributional deterministic policy gradients.
	\newblock \emph{arXiv preprint arXiv:1804.08617}, 2018.
	
	\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and
	Munos]{bellemare2017distributional}
	Marc~G Bellemare, Will Dabney, and R{\'e}mi Munos.
	\newblock A distributional perspective on reinforcement learning.
	\newblock In \emph{Proceedings of International Conference on Machine
		Learning}, pages 449--458. PMLR, 2017.
	
	\bibitem[Bellemare et~al.(2023)Bellemare, Dabney, and Rowland]{bdr2022}
	Marc~G. Bellemare, Will Dabney, and Mark Rowland.
	\newblock \emph{Distributional Reinforcement Learning}.
	\newblock MIT Press, 2023.
	\newblock \url{http://www.distributional-rl.org}.
	
	\bibitem[Chapman and Lessard(2021)]{chapman2021toward}
	Margaret~P Chapman and Laurent Lessard.
	\newblock Toward a scalable upper bound for a {CVaR-LQ} problem.
	\newblock \emph{IEEE Control Systems Letters}, 6:\penalty0 920--925, 2021.
	
	\bibitem[Dabney et~al.(2018{\natexlab{a}})Dabney, Ostrovski, Silver, and
	Munos]{dabney2018implicit}
	Will Dabney, Georg Ostrovski, David Silver, and R{\'e}mi Munos.
	\newblock Implicit quantile networks for distributional reinforcement learning.
	\newblock In \emph{Proceedings of International Conference on Machine
		Learning}, pages 1096--1105. PMLR, 2018{\natexlab{a}}.
	
	\bibitem[Dabney et~al.(2018{\natexlab{b}})Dabney, Rowland, Bellemare, and
	Munos]{dabney2018distributional}
	Will Dabney, Mark Rowland, Marc Bellemare, and R{\'e}mi Munos.
	\newblock Distributional reinforcement learning with quantile regression.
	\newblock In \emph{Proceedings of AAAI Conference on Artificial Intelligence},
	volume~32, 2018{\natexlab{b}}.
	
	\bibitem[Dean et~al.(2020)Dean, Mania, Matni, Recht, and Tu]{dean2020sample}
	Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu.
	\newblock On the sample complexity of the linear quadratic regulator.
	\newblock \emph{Foundations of Computational Mathematics}, 20\penalty0
	(4):\penalty0 633--679, 2020.
	
	\bibitem[Fazel et~al.(2018)Fazel, Ge, Kakade, and Mesbahi]{fazel2018global}
	Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi.
	\newblock Global convergence of policy gradient methods for the linear
	quadratic regulator.
	\newblock In \emph{Proceedings of International Conference on Machine
		Learning}, pages 1467--1476. PMLR, 2018.
	
	\bibitem[Kim and Yang(2021)]{kim2021distributional}
	Kihyun Kim and Insoon Yang.
	\newblock Distributional robustness in minimax linear quadratic control with
	{Wasserstein} distance.
	\newblock \emph{arXiv preprint arXiv:2102.12715}, 2021.
	
	\bibitem[Kishida and Cetinkaya(2022)]{kishida2022risk}
	Masako Kishida and Ahmet Cetinkaya.
	\newblock Risk-aware linear quadratic control using conditional value-at-risk.
	\newblock \emph{IEEE Transactions on Automatic Control}, 2022.
	
	\bibitem[Li et~al.(2021)Li, Tang, Zhang, and Li]{li2021distributed}
	Yingying Li, Yujie Tang, Runyu Zhang, and Na~Li.
	\newblock Distributed reinforcement learning for decentralized linear quadratic
	control: A derivative-free policy optimization approach.
	\newblock \emph{IEEE Transactions on Automatic Control}, 2021.
	
	\bibitem[Malik et~al.(2019)Malik, Pananjady, Bhatia, Khamaru, Bartlett, and
	Wainwright]{malik2019derivative}
	Dhruv Malik, Ashwin Pananjady, Kush Bhatia, Koulik Khamaru, Peter Bartlett, and
	Martin Wainwright.
	\newblock Derivative-free methods for policy optimization: guarantees for
	linear quadratic systems.
	\newblock In \emph{Proceedings of 22nd International Conference on Artificial
		Intelligence and Statistics}, pages 2916--2925. PMLR, 2019.
	
	\bibitem[Rockafellar et~al.(2000)Rockafellar, Uryasev,
	et~al.]{rockafellar2000optimization}
	R~Tyrrell Rockafellar, Stanislav Uryasev, et~al.
	\newblock Optimization of conditional value-at-risk.
	\newblock \emph{Journal of Risk}, 2:\penalty0 21--42, 2000.
	
	\bibitem[Singh et~al.(2020)Singh, Zhang, and Chen]{singh2020improving}
	Rahul Singh, Qinsheng Zhang, and Yongxin Chen.
	\newblock Improving robustness via risk averse distributional reinforcement
	learning.
	\newblock In \emph{Proceedings of Learning for Dynamics and Control
		Conference}, pages 958--968. PMLR, 2020.
	
	\bibitem[Singh et~al.(2022)Singh, Lee, and Chen]{singh2022sample}
	Rahul Singh, Keuntaek Lee, and Yongxin Chen.
	\newblock Sample-based distributional policy gradient.
	\newblock In \emph{Proceedings of Learning for Dynamics and Control
		Conference}, pages 676--688. PMLR, 2022.
	
	\bibitem[Tang et~al.(2019)Tang, Zhang, and Salakhutdinov]{tang2019worst}
	Yichuan~Charlie Tang, Jian Zhang, and Ruslan Salakhutdinov.
	\newblock Worst case policy gradients.
	\newblock \emph{arXiv preprint arXiv:1911.03618}, 2019.
	
	\bibitem[Tsiamis et~al.(2021)Tsiamis, Kalogerias, Ribeiro, and
	Pappas]{tsiamis2021linear}
	Anastasios Tsiamis, Dionysios~S Kalogerias, Alejandro Ribeiro, and George~J
	Pappas.
	\newblock Linear quadratic control with risk constraints.
	\newblock \emph{arXiv preprint arXiv:2112.07564}, 2021.
	
	\bibitem[Tu and Recht(2018)]{tu2018least}
	Stephen Tu and Benjamin Recht.
	\newblock Least-squares temporal difference learning for the linear quadratic
	regulator.
	\newblock In \emph{Proceedings of International Conference on Machine
		Learning}, pages 5005--5014. PMLR, 2018.
	
	\bibitem[Van~Parys et~al.(2015)Van~Parys, Kuhn, Goulart, and
	Morari]{van2015distributionally}
	Bart~PG Van~Parys, Daniel Kuhn, Paul~J Goulart, and Manfred Morari.
	\newblock Distributionally robust control of constrained stochastic systems.
	\newblock \emph{IEEE Transactions on Automatic Control}, 61\penalty0
	(2):\penalty0 430--442, 2015.
	
	\bibitem[Yaghmaie et~al.(2022)Yaghmaie, Gustafsson, and
	Ljung]{yaghmaie2022linear}
	Farnaz~Adib Yaghmaie, Fredrik Gustafsson, and Lennart Ljung.
	\newblock Linear quadratic control using model-free reinforcement learning.
	\newblock \emph{IEEE Transactions on Automatic Control}, 2022.
	
	\bibitem[Zhang et~al.(2022)Zhang, Zhou, Ji, and Zavlanos]{zhang2022new}
	Yan Zhang, Yi~Zhou, Kaiyi Ji, and Michael~M Zavlanos.
	\newblock A new one-point residual-feedback oracle for black-box learning and
	control.
	\newblock \emph{Automatica}, 136:\penalty0 110006, 2022.
	
	\bibitem[Zheng et~al.(2021)Zheng, Furieri, Kamgarpour, and Li]{zheng2021sample}
	Yang Zheng, Luca Furieri, Maryam Kamgarpour, and Na~Li.
	\newblock Sample complexity of linear quadratic {G}aussian {(LQG)} control for
	output feedback systems.
	\newblock In \emph{Proceedings of Learning for Dynamics and Control
		Conference}, pages 559--570. PMLR, 2021.
	
\end{thebibliography}


\end{document}
