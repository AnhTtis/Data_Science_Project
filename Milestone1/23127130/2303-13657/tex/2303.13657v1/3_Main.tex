\section{Main Results}
\subsection{Exact Form of the Return Distribution}
In this section, we precisely characterise the distribution of the random return  that satisfies the distributional  Bellman equation \eqref{eq:rv:Bellman}. 
Given a static linear policy $\pi(x_t)=K x_t$, we denote by $G^K(x)$ the random return $G^{\pi}(x)$  under the policy $\pi(x_t)$ from the initial state $x_0=x$ , which is defined as
\begin{align*}
    G^{K}(x) =  \sum_{t=0}^{\infty} \gamma^t x_t^T (Q+K^T R K) x_t  , \quad x_0 = x.
\end{align*}
The random return  $G^{K}(x)$ satisfies the following distributional Bellman equation  
\begin{align}\label{eq:rv:bellman}
    G^{K}(x) & \mathop{=}^{D} x^TQ_Kx + \gamma G^{K}(X'),\quad X'=A_K x+ v_0,
\end{align}
where $A_K:=A+BK$ and $Q_K := Q+ K^T R K$. 
In the following theorem, we provide an explicit expression of the random return $G^K(x)$.

\begin{theorem}
Suppose that the feedback gain  $K$ is stabilizing, i.e., $A_K=A+BK$ is stable. Let  \begin{align}\label{eq:dist_func}
    G^{K}(x) = x^T P x +  \sum_{k = 0 }^{\infty}  \gamma^{k+1}  w_k^T P w_k +  2 \sum_{k = 0 }^{\infty} \gamma^{k+1} w_k^T P A_K^{k+1}x + 
 2  \sum_{k = 1 }^{\infty} \gamma^{k+1} w_k^T P \sum_{\tau=0}^{k-1} A_K^{k-\tau}w_{\tau},
\end{align}
where  $P$ is obtained from the algebraic Riccati equation $P = Q+ K^T R K + \gamma A_K^T P A_K$, and the random variables $w_k \sim \mathcal{D} $ are independent from each other for all $k\in\mathbb{N}$. Then, the random variable $G^{K}(x)$ defined in 
    \eqref{eq:dist_func} is a fixed point solution to the distributional Bellman equation \eqref{eq:rv:bellman}.
\end{theorem}
\begin{proof}
Recall that $X' = A_K x + v_0$, where $v_0$ is a  random variable sampled from the distribution $\mathcal{D}$ and is independent from $w_k$, $k\in \mathbb{N}$, in \eqref{eq:dist_func}. 
Substituting \eqref{eq:dist_func} into the right hand side of the equation \eqref{eq:rv:bellman}, we have that
\begin{align*}
    & x^T(Q+K^T R K)x + \gamma G^{K}(X') \\
    =  & x^TQ_Kx + \gamma X'^T P X' +  \sum_{t=0}^{\infty} \gamma^{t+2} w_t^T P w_t + 2 \sum_{t=0}^{\infty} \gamma^{t+2} w_t^T  P A_K^{t+1}  X' \\
    &+ 2  \sum_{t=1}^{\infty} \gamma^{t+2} w_t^T P A_K \sum_{i=0}^{t-1} A_K^{t-1-i} w_i  \\
    = & x^TQ_Kx + \gamma (A_K x + v_0)^T P (A_K x + v_0) + \gamma^2 \sum_{t=0}^{\infty} \gamma^t w_t^T P w_t  + 2 \gamma^2 \sum_{t=1}^{\infty} \gamma^t w_t^T P \sum_{i=0}^{t-1} A_K^{t-i} w_i \\
    &+ 2\gamma^2 \sum_{t=0}^{\infty} \gamma^t w_t^T  P A_K^{t+1}  (A_K x + v_0) \\
    = & x^T(Q_K + \gamma A_K^T P A_K)x + \underbrace{\gamma v_0^T P v_0 + \gamma^2 \sum_{t=0}^{\infty} \gamma^t w_t^T P w_t }_{:=T_1} +\underbrace{ 2\gamma v_0^T PA_K x + 2\gamma^2 \sum_{t=0}^{\infty} \gamma^t w_t^T  P A_K^{t+2} x}_{:=T_2} \\
    & + \underbrace{2 \gamma^2 \sum_{t=1}^{\infty} \gamma^t w_t^T P  \sum_{i=0}^{t-1} A_K^{t-i} w_i + 2\gamma^2 \sum_{t=0}^{\infty} \gamma^t w_t^T  P  A_K^{t+1} v_0}_{:=T_3}.
\end{align*}
Define $ \xi_0 :=v_0$, $\xi_t = w_{t-1}$, $t=1,2,\ldots$. 
From the definition of the term $T_1$, we have that
\begin{align*}
    T_1 & = \gamma v_0^T P v_0 + \gamma^2 \sum_{t=0}^{\infty} \gamma^t w_t^T P w_t 
     \mathop{=}^{k=t+1}  \gamma \xi_0^T P \xi_0 + \gamma \sum_{k=1}^{\infty} \gamma^k \xi_{k}^T P \xi_{k} 
    =  \gamma \sum_{k=0}^{\infty} \gamma^k \xi_{k}^T P \xi_{k}.
\end{align*}
% For the term $T_2$, applying similar techniques as above, we have that $T_2=2\gamma  \sum_{k=0}^{\infty} \gamma^k \xi_k^T P A_K^{k+1} x$.
For the term $T_2$, we have that
\begin{align*}
    T_2 & = 2\gamma v_0^T PA_K x + 2\gamma^2 \sum_{t=0}^{\infty} \gamma^t w_t^T  P A_K^{t+2} x 
    = 2\gamma \xi_0^T P A_K x + 2\gamma^2 \sum_{t=0}^{\infty} \gamma^t \xi_{t+1}^T  P A_K^{t+2} x \\
    &\mathop{=}^{k=t+1}  2\gamma \xi_0^T P A_K x + 2\gamma \sum_{k=1}^{\infty} \gamma^k \xi_{k}^T  P A_K^{k+1} x 
    =  2\gamma  \sum_{k=0}^{\infty} \gamma^k \xi_k^T P A_K^{k+1} x.
\end{align*}
%
%
%
%
%
Using similar techniques for the term $T_3$, we obtain that
$T_3 = 2 \gamma \sum_{k=1}^{\infty} \gamma^{k} \xi_{k}^T P A_K  \sum_{i=0}^{k-1} A_K^{k-1-i} \xi_i.$ 
% \begin{align*}
%     T_3 =& 2 \gamma^2 \sum_{t=1}^{\infty} \gamma^t w_t^T P A_K \sum_{i=0}^{t-1} A_K^{t-1-i} w_i + 2\gamma^2 \sum_{t=0}^{\infty} \gamma^t w_t^T  P  A_K^{t+1} v_0 \\
%     = & 2 \gamma^2 \sum_{t=1}^{\infty} \gamma^t \xi_{t+1}^T P A_K \sum_{i=0}^{t-1} A_K^{t-1-i} \xi_{i+1} + 2\gamma^2 \sum_{t=1}^{\infty} \gamma^t \xi_{t+1}^T P  A_K^{t+1} \xi_0   +2\gamma^2 \xi_1^T P A_K \xi_0\\
%     = & 2 \gamma \sum_{t=1}^{\infty} \gamma^{t+1} \xi_{t+1}^T P A_K \left(  \sum_{i=0}^{t-1} A_K^{t-1-i} \xi_{i+1} + A_K^t \xi_0\right)  +2\gamma^2 \xi_1^T P A_K \xi_0\\
%     = & 2 \gamma \sum_{t=1}^{\infty} \gamma^{t+1} \xi_{t+1}^T P A_K  \left( \sum_{i=0}^t A_K^{t-i} \xi_i \right) +2\gamma^2 \xi_1^T P A_K \xi_0\\
%     \mathop{=}^{k=t+1} &  2 \gamma \sum_{k=2}^{\infty} \gamma^{k} \xi_{k}^T P A_K  \left( \sum_{i=0}^{k-1} A_K^{k-1-i} \xi_i \right) +2\gamma^2 \xi_1^T P A_K \xi_0\\
%     = & 2 \gamma \sum_{k=1}^{\infty} \gamma^{k} \xi_{k}^T P A_K  \sum_{i=0}^{k-1} A_K^{k-1-i} \xi_i .
% \end{align*}
Due to the fact that $P =Q+ K^T R K + \gamma A_K^T P A_K$, we have 
\begin{align}\label{eq:proof:P1:temp1}
    & x^TQ_Kx + \gamma G^{K}(X') 
    =  x^T P x + T_1 +T_2 +T_3 \nonumber \\
    = & x^T P x + \gamma \sum_{k=0}^{\infty} \gamma^k \xi_{k}^T P \xi_{k} + 2\gamma  \sum_{k=0}^{\infty} \gamma^k x^T P A_K^{k+1} \xi_k + 2 \gamma \sum_{k=1}^{\infty} \gamma^{k} \xi_{k}^T P A_K  \sum_{i=0}^{k-1} A_K^{k-1-i} \xi_i,
\end{align}
which is in the same form as in \eqref{eq:dist_func}.
Since $\{ \xi_k\}_{k=0}^{\infty}$ and $\{ w_k\}_{k=0}^{\infty}$ are i.i.d.,
we have that the two random variables \eqref{eq:dist_func} and \eqref{eq:proof:P1:temp1} have the same distribution, i.e.,
$G^{K}(x)  \mathop{=}\limits^{D}  x^TQ_Kx + \gamma G^{K}(X').$
%The proof is completed. 
\end{proof}

% \begin{proposition}
%     {\color{blue}Suppose that the exogenous disturbance $v_t$ is Gaussian and the feedback gain  $K$ is stabilizing, i.e., $A_K=A+BK$ is stable.  Then, for any $x\in \mathcal{R}^d$, }we have that $${\rm var}(G^{\pi}(x))< \infty.$$  
% \end{proposition}



\subsection{Approximation of the Return Distribution with Finite Parameters}\label{Sec:Approxreturn}

In this section, we show how to approximate the random return defined in \eqref{eq:dist_func} using a finite number of random variables. Considering only the first $N$ terms in the summations in the expression in \eqref{eq:dist_func}  and disregarding the terms for $k$ larger than $N$ yields the following:   
\begin{align}\label{eq:approxreturn}
    {G}^{K}_N(x) =  x^T P x +  \sum_{k = 0 }^{N-1}  \gamma^{k+1}  w_k^T P w_k +  2 \sum_{k = 0 }^{N-1} \gamma^{k+1} w_k^T P A_K^{k+1}x + 
 2 \sum_{k = 1 }^{N-1} \gamma^{k+1} w_k^T P \sum_{\tau=0}^{k-1} A_K^{k-\tau}w_{\tau}.
\end{align}
Let $F^{K}_x$ and ${F}^{K}_{x,N}$ denote the cumulative distribution function (CDF) of $G^{K}(x)$ and ${G}_N^{K}(x)$, respectively. The following theorem provides an upper bound on the difference between $F^{K}_x$ and ${F}^{K}_{x,N}$, and shows that  the sequence $\{{G}^{K}_N(x)\}_{N\in \mathbb{N}}$ converges pointwise in distribution to $G^{K}(x)$,  $\forall x\in\mathbb{R}^n$. 

\begin{theorem}\label{Theorem:approx}
Assume that the probability density functions of $w_k$ exist and are bounded, and satisfy $\mathbb{E}[w_k^T w_k] \leq \sigma_0^2$, $\mathbb{E}[\left\|w_k\right\|_2] \leq \mu_0 $, for $\forall k\in \mathbb{N}$.
Suppose that the feedback gain  $K$ is stabilizing such that $\left\|A_K\right\|_2 = \rho_K <1$. Then, the sup difference between the CDFs $F^K_x$ and ${F}^{K}_{x,N}$ is bounded by
\begin{align}\label{eq:approximat:bound}
     \sup_{z}|{F}^{K}_x(z)-{F}^{K}_{x,N}(z) | \leq C \gamma^N,
\end{align}
where $C$ is a constant that depends on the matrices $A,B,Q,R,K$, the initial state value $x$, and the parameters $\gamma, \rho_K, \sigma_0,\mu_0$. 
\end{theorem}

\begin{proof}
Define $Y_N:= G^K(x) - G^K_N(x)$, we have
\begin{align}\label{eq:upp_bound_temp1}
     &\sup_{z}|{F}^{K}_x(z)-{F}^{K}_{x,N}(z) | 
     =\sup_{z}|\mathbb{P}(G^K_N(x) \leq z) -\mathbb{P}(G^K(x)\leq z) | \nonumber \\
     =& \sup_{z}|\mathbb{P}(G^K_N(x)\leq z) -\mathbb{P}(G^K_N(x) +Y_N\leq z) | \nonumber \\
     =& \sup_{z}\Big|\mathbb{P}(G^K_N(x)\leq z) \int_{-\infty}^{\infty} \mathbb{P}(Y_N = t)dt -\int_{-\infty}^{\infty} \mathbb{P}(G^K_N(x) \leq z-t) \mathbb{P}(Y_N = t) dt \Big| \nonumber \\
     =& \sup_{z}\Big| \int_{-\infty}^{\infty} \mathbb{P}(Y_N = t) \big(  F^K_{x,N}(z) -F^K_{x,N}(z-t) \big) dt \Big|.
\end{align}
Since the random variables $w_t$ are i.i.d for all $t>0$ and the probability density function of $w_t$ exists, the function $F^K_{x,N}$ is continuous and differentiable. 
Applying the mean value theorem, when $t>0$ there exists a point $z'\in[z-t,z]$ such that $F^K_{x,N}(z) -F^K_{x,N}(z-t) = f^K_{x,N}(z') t$, where $f^K_{x,N}$ is the probability density function of $G^K_N(x)$. Since the probability density function of $w_t$ is bounded, it further follows  that $f^K_{x,N}$ is bounded.   Then, we have that $|F^K_{x,N}(z) -F^K_{x,N}(z-t)| = |f^K_{x,N}(z') t| \leq L_0 |t|$, where $L_0$ is an upper bound of the probability function $f^K_{x,N}$. Following a similar argument, we can show that this inequality holds when $t\leq 0$. Substituting this inequality into \eqref{eq:upp_bound_temp1}, we obtain
\begin{align}\label{eq:upp_bound_temp2}
   \sup_{z}|{F}^{K}_x(z)-{F}^{K}_{x,N}(z) | \leq \sup_{z}\Big| \int_{-\infty}^{\infty} \mathbb{P}(Y_N = t) L_0 |t| dt \Big| 
     = L_0 \mathbb{E}|Y_N|.
\end{align}
From the definition of $Y_N$, we obtain that
\begin{align*}
     Y_N =& 
     \sum_{k = N }^{\infty}  \gamma^{k+1}  w_k^T P w_k +  2 \sum_{k = N }^{\infty} \gamma^{k+1} w_k^T P A_K^{k+1}x + 2 \sum_{k = N }^{\infty} \gamma^{k+1} w_k^T P \sum_{\tau=0}^{k-1} A_K^{k-\tau}w_{\tau} \nonumber \\
     \mathop{=}^{t=k-N}&    \gamma^N \Big(  \sum_{t = 0 }^{\infty} \gamma^{t+1}  w_{t+N}^T P w_{t+N} +  2 \sum_{t = 0 }^{\infty} \gamma^{t+1} w_{t+N}^T P A_K^{t+N+1}x \nonumber \\
     &+ 2 \sum_{t = 0 }^{\infty} \gamma^{t+1} w_{t+N}^T P \sum_{\tau=0}^{t+N-1} A_K^{t+N-\tau}w_{\tau}  \Big).
\end{align*}
%\begin{align*}
%      Y_N =& 
%      \sum_{k = N }^{\infty}  \gamma^{k+1}  w_k^T P w_k +  2 \sum_{k = N }^{\infty} \gamma^{k+1} w_k^T P A_K^{k+1}x + 2 \sum_{k = N }^{\infty} \gamma^{k+1} w_k^T P \sum_{\tau=0}^{k-1} A_K^{k-\tau}w_{\tau} \nonumber \\
%      \mathop{=}^{t=k-N}&  \sum_{t = 0 }^{\infty} \gamma^{t+N+1}  w_{t+N}^T P w_{t+N} +  2 \sum_{t = 0 }^{\infty} \gamma^{t+N+1} w_{t+N}^T P A_K^{t+N+1}x \nonumber \\
%      &+ 2 \sum_{t = 0 }^{\infty} \gamma^{t+N+1} w_{t+N}^T P \sum_{\tau=0}^{t+N-1} A_K^{t+N-\tau}w_{\tau} \nonumber \\
%      =&  \gamma^N \Big(  \sum_{t = 0 }^{\infty} \gamma^{t+1}  w_{t+N}^T P w_{t+N} +  2 \sum_{t = 0 }^{\infty} \gamma^{t+1} w_{t+N}^T P A_K^{t+N+1}x \nonumber \\
%      &+ 2 \sum_{t = 0 }^{\infty} \gamma^{t+1} w_{t+N}^T P \sum_{\tau=0}^{t+N-1} A_K^{t+N-\tau}w_{\tau}  \Big).
% \end{align*}
Taking the expectation of the absolute value of $Y_N$, we have
\begin{align*}
    \mathbb{E}|Y_N| \leq &\gamma^N \Big( \sum_{t = 0 }^{\infty} \gamma^{t+1} \mathbb{E} | w_{t+N}^T P w_{t+N} |   +  2 \sum_{t = 0 }^{\infty} \gamma^{t+1} \mathbb{E}| w_{t+N}^T P A_K^{t+N+1}x|\nonumber \\
    &+ 2 \sum_{t = 0 }^{\infty} \gamma^{t+1} \mathbb{E} |w_{t+N}^T P \sum_{\tau=0}^{t+N-1} A_K^{t+N-\tau}w_{\tau} |  \Big) .
\end{align*}
We handle the terms in the above inequality one by one. For the first term, we have that
\begin{align}\label{eq:upp_bound_temp4}
    \sum_{t = 0 }^{\infty} \gamma^{t+1} \mathbb{E} | w_{t+N}^T P w_{t+N} | \leq \sum_{t = 0 }^{\infty} \gamma^{t+1}  \mathbb{E}| \lambda_{\max}(P) w_{t+N}^T w_{t+N} | \leq \lambda_{\max}(P) \sigma_0^2 \frac{\gamma}{1-\gamma}.
\end{align}
For the second term, we have that
\begin{align}\label{eq:upp_bound_temp5}
    &2 \sum_{t = 0 }^{\infty} \gamma^{t+1} \mathbb{E}| w_{t+N}^T P A_K^{t+N+1}x|\leq  2\mu  \sum_{t = 0 }^{\infty} \gamma^{t+1} \left\| P \right\|_2 \left\| A_K^{t+N+1} \right\|_2 \left\|x \right\|_2 \nonumber \\
    \leq& 2\mu  \sum_{t = 0 }^{\infty} \gamma^{t+1} \left\| P\right\|_2 \rho_K^{t+N-1} \left\|x \right\|_2 \leq 
    2\mu \left\| P\right\|_2 |x|\frac{\gamma \rho_K^{N-1}}{1-\gamma \rho_K} \leq 2\mu \left\| P\right\|_2 |x|\frac{\gamma}{1-\gamma \rho_K},
\end{align}
where the second inequality is due to the fact that $\left\|A_K^{t+N+1} \right\|_2\leq (\left\|A_K \right\|_2)^{t+N+1} \leq \rho_K^{t+N+1}$ and the last inequality follows from the fact that $N\geq 1$.
% We define $S:=\sum_{t = 0 }^{\infty} \gamma^{t+1} A_K^{t+N+1}$. Multiplying both sides by $\gamma A_K$, we obtain
% \begin{align}\label{eq:upp_bound_temp6}
%     \gamma A_K S =\sum_{t = 0 }^{\infty} \gamma^{t+2} A_K^{t+N+2}\mathop{=}^{k=t+1} \sum_{k = 1 }^{\infty} \gamma^{k+1} A_K^{k+N+1}= S - \gamma A_K^{N+1}.
% \end{align}
% Since $A_K$ is stable, we have that $I-\gamma A_K$ is full rank and invertible. Rearranging the terms in \eqref{eq:upp_bound_temp6}, we obtain $S=\gamma (I-\gamma A_K)^{-1} A_K^{N+1}$. Substituting $S$ into \eqref{eq:upp_bound_temp5}, it gives that
% \begin{align}\label{eq:upp_bound_temp7}
%     &2 \sum_{t = 0 }^{\infty} \gamma^{t+1} \mathbb{E}| w_{t+N}^T P A_K^{t+N+1}x|\leq  2\gamma \mu \boldsymbol{1}^T P (I-\gamma A_K)^{-1}  A_K^{N+1}  |x| 
% ;;[----\end{align}
For the third term, we have that
\begin{align}\label{eq:upp_bound_temp8}
    &2 \sum_{t = 0 }^{\infty} \gamma^{t+1} \mathbb{E} |w_{t+N}^T P \sum_{\tau=0}^{t+N-1} A_K^{t+N-\tau}w_{\tau} | 
    \leq   2 \sum_{t = 0 }^{\infty} \gamma^{t+1} \mathbb{E} \left[ \left\| w_{t+N}^T\right\|_2  \left\| P \right\|_2  \left\|\sum_{\tau=0}^{t+N-1} A_K^{t+N-\tau} w_{\tau}\right\|_2 \right] \nonumber \\
    \leq &  2 \mu \left\| P \right\|_2  \sum_{t = 0 }^{\infty} \gamma^{t+1}   \mathbb{E} \left[ \left\|\sum_{\tau=0}^{t+N-1} A_K^{t+N-\tau} w_{\tau}\right\|_2 \right]
    \leq  2 \mu \left\| P \right\|_2  \sum_{t = 0 }^{\infty} \gamma^{t+1}   \mathbb{E} \left[ \sum_{\tau=0}^{t+N-1} \left\|A_K^{t+N-\tau} \right\|_2\left\|w_{\tau}\right\|_2 \right] \nonumber\\
    \leq  &2 \mu^2 \left\| P \right\|_2 \sum_{t = 0 }^{\infty} \gamma^{t+1}  \sum_{\tau=0}^{t+N-1}  \rho_K^{t+N-\tau} \leq 2 \mu^2 \left\| P \right\|_2 \sum_{t = 0 }^{\infty} \gamma^{t+1} \frac{\rho_K}{1-\rho_K} \leq 2 \mu^2 \left\| P \right\|_2 \frac{\gamma \rho_K}{(1-\gamma)(1-\rho_K)},
\end{align}
where the second inequality is due to the fact that $w_{\tau}$ and $w_{t+N}$ are independent and the  second to last inequality follows from the fact that $\sum_{\tau=0}^{t+N-1}  \rho_K^{t+N-\tau} = \sum_{\tau=1}^{t+N} \rho_K^{\tau} \leq \frac{\rho_K}{1-\rho_K} $.
Combining \eqref{eq:upp_bound_temp4}, \eqref{eq:upp_bound_temp5} and \eqref{eq:upp_bound_temp8}, we have that
\begin{align}
    &\sup_{z}|{F}^{K}_x(z)-{F}^{K}_{x,N}(z) | \leq  L_0 \mathbb{E}|Y_N| \nonumber \\
    \leq & L_0 \gamma^N \Big(\lambda_{\max}(P) \sigma_0^2 \frac{\gamma}{1-\gamma} + 2\mu \left\| P\right\|_2 |x|\frac{\gamma}{1-\gamma \rho_K}
    +  2 \mu^2 \left\| P \right\|_2 \frac{\gamma \rho_K}{(1-\gamma)(1-\rho_K)} \Big) 
    :=  C \gamma^N . \nonumber 
\end{align}
The proof is complete and also yields the expression of the constant $C$. 
\end{proof}

\begin{remark}
The bound on the distribution approximation in \eqref{eq:approximat:bound} relies on the conditions of Theorem~\ref{Theorem:approx}, which ensure that the PDF of $G^K_N$ is continuous and bounded. Note that these conditions are not particularly strict, and indeed hold for many  noise distributions commonly used in linear dynamical systems, including Gaussian and uniform. Future work will investigate relaxations of these conditions. 
\end{remark}


\subsection{Numerical Experiments on Quality of the Approximation of the Return Distribution}\label{Sec:NemVer}

In the following experiment, we consider a scalar model with matrices $A=B=1$. Similarly, the weighting matrices in the LQR cost are chosen as $Q=R=1$. The exogenous disturbances are standard normal distributions with zero mean. 

% \textcolor{red}{[With some further thoughts, some reviewers might complain that we are comparing our approximation versus another approximation, assuming the latter is closer to the true return distribution. So, they may say, why do we need the truncated distributions to begin with? We should eprhaps argue more for the value of the truncated approximation: like, the MC one is not formal/analytical and so we cannot really use it any further (plus it comes only with a statistical error, whereas we have bounds!); or it takes so many samples to be a good approximation; or similar arguments (see related discussion on sample-based approximation for DRL in the introduction) .... by the way, is it not possible to obtain an analytical form of the true return distribution even in the scalar case? if so this is a good display of how unwieldy such form is in general.]}
% %
Even for this scalar system, it is impossible to simplify the expression of the exact return distribution, which still depends on an infinite number of random variables.  
Thus, as a baseline for the return distribution, we generate an empirical distribution that approximates the true distribution of the random return. More specifically, we use the Monte Carlo (MC) method to obtain 10000 samples of the random return and use the sample frequency over evenly-divided regions as an approximation of the probability density function. According to the law of large numbers, the empirical distribution approaches the real one as the  number of trials increases. 
Note that, although the MC method provides an alternative way to approximate the return distribution, it relies on using sufficiently many samples that can be time-consuming, and its (statistical) approximation error is generally difficult to analyse. Thus, the MC method is not applicable for practical policy evaluation of distributional LQR, and in this experiment, it is used only to verify our approximate return distribution.
In comparison, the approximate return distribution using finite number of random variables in this paper is analytical for policy evaluation and the corresponding approximation error can be bounded: as such, it is further usable for policy optimisation, as shown in Section~\ref{Sec:riskaversecontrol}. We denote here by $f_N$ the distribution of the approximated random return $G^K_N(x_0)$ obtained considering $N$ random variables. 
%as it takes too many samples to obtain a good estimate and no theoretical bound of the estimation error can be provided. In this experiment, the MC method with sufficient samples, which is extremely time-consuming, is used only for the verification of our approximation of the return distribution.}
% We thus consider the empirical distribution return obtained from the Monte Carlo method as the baseline. 


\begin{figure}[t]
    \centering
	\subfigure[$\gamma=0.6$, $x_0=1$.]{
	\includegraphics[scale=0.24]{figures/S1_F1}}
	\subfigure[$\gamma=0.8$, $x_0=1$.]{
	\includegraphics[scale=0.24]{figures/S1_F2}}
        \subfigure[$\gamma=0.6$, $x_0=8$.]{
	\includegraphics[scale=0.24]{figures/S1_F3}}
        \subfigure[$\gamma=0.85$, $x_0=8$.]{
	\includegraphics[scale=0.24]{figures/S1_F4}}
	\caption{Return distribution and its approximation with finite number of random variables for different $\gamma$ and $x_0$. MC denotes the distribution returned by the Monte Carlo method and $f_N$ denotes the distribution of the approximated random return $G^K_N(x_0)$.}\label{fig:S1}
 \vspace{-0.2in}
\end{figure}



We fix the feedback gain as $K= -0.4684$ and select different values of $\gamma$ and $x_0$. The results are shown in Fig.~\ref{fig:S1}. Specifically,  Fig.~\ref{fig:S1} (a) and (c) show that when $\gamma$ is small, the return distribution can be well approximated using only few random variables ($N=3$ works well). 
%
However, when $\gamma$ approaches $1$, more random variables are needed for an accurate approximation: we employ $N=15$ and $N=20$ random variables in the case of $\gamma=0.8$ and $\gamma=0.85$, respectively, as shown in Fig.~\ref{fig:S1} (b) and (d). Moreover, the value of the initial state $x_0$ has an influence on the shape of the return distribution, which can be clearly observed from the scalar case. When $x_0$ is large, the random variable $w_k^T P A_K^{k+1}x_0$ dominates and, therefore, its distribution is close to a Gaussian distribution, as shown in Fig.~\ref{fig:S1} (c) and (d). If instead $x_0$ is  small, then the random variable $w_k^T P w_k$ plays a leading role, so the overall distribution is close to the chi-square one, as shown in Fig.~\ref{fig:S1} (a) and (b). 
In conclusion,  when $N$ is large, the approximate distribution is closer to the distribution obtained from the MC method, and thus to the true distribution. 
