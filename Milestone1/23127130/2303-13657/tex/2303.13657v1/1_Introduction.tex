\section{Introduction}

In reinforcement learning, the value of implementing a policy at a given state is captured by a value function, which models the expected sum of returns following this prescribed policy. 
%
Recently, \citet{bellemare2017distributional}  proposed the notion of distributional reinforcement learning (DRL), which learns the return distribution of a policy from a given state, instead of only its expected return. 
%
Compared to the scalar expected value function, the return distribution is infinite-dimensional and contains far more information. 
%
It is, therefore, not surprising that a few DRL algorithms, including C51 \citep{bellemare2017distributional}, D4PG \citep{barth2018distributed}, QR-DQN \citep{dabney2018distributional} and SDPG \citep{singh2022sample}, dramatically improve the empirical performance in practical applications over their non-distributional counterpart. 
%





In DRL, the practical effectiveness of algorithms builds on the theory by \citet{bellemare2017distributional}, where the distributional Bellman operator is shown to be a contraction in the (maximum form of) the Wasserstein metric between probability distributions.
%
However, it is usually difficult to  characterise the exact return distribution in DRL with finite data. 
% However, it is usually intractable to characterize the accurate return distribution with finite data.
%
Approximations of the return distribution are thus necessary to make it computable in practice.
%
To address this challenge, \citet{bellemare2017distributional} propose a categorical method that partitions the return distribution into a finite number of uniformly spaced atoms in a fixed region.
%
One drawback of this method is that it relies on prior knowledge of the range of the returned values. 
%
To address this limitation, a quantile function method 
\citep{dabney2018distributional} and a sample-based method \citep{singh2022sample} have been recently proposed. 
% This issue is to some extent addressed by a quantile function method \citep{dabney2018distributional} and a sample-based method \citep{singh2022sample}.
%
However,  these works cannot provide an analytical expression for the approximation error, and computational cost needs to be decided manually to guarantee approximation accuracy.



In this paper, we characterise the return distribution of the random cost for the classical discounted linear quadratic regulator (LQR) problem, which we term \emph{distributional LQR}.  
%
To the best of our knowledge, the return distribution in LQR has not been explored in the literature. 
%
Our contributions are summarised as follows:

\begin{enumerate}
    \item 
     %In the policy evaluation in distribution LQR, we provide an analytical expression of the random return, which is proven to be a fixed-point solution of the distributional Bellman equation. Note that this analytical expression, which consists of infinitely many random variables, suits for all the kinds of exogenous disturbances (even for noises with non-Gaussian or non-zero means) as long as they are independent and identically distributed (i.i.d.). This finding provides significant theoretical support and convenience for the analysis of distributional LQR: it saves the efforts of designing and learning representation of the return distribution.
    We provide an analytical expression of the random return for distributional LQR problems and prove that this return function is a fixed-point solution of the distributional Bellman equation. 
    % It implies that we eliminate the process of designing and learning the representation of the return distribution. 
    Specifically, we show that the proposed analytical expression consists of infinitely many random variables and holds for arbitrary i.i.d. exogenous disturbances, e.g., non-Gaussian noise or noise with non-zero mean.  
    \item We develop an approximation of the distribution of the random return using a finite number of random variables. Under mild assumptions, we theoretically show that the sup of the difference between the exact  and  approximated  return distributions deceases linearly with the numbers of random variables: this is also validated by numerical experiments.
    \item The proposed analytical return distribution provides a theoretical foundation for distributional LQR, allowing for general optimality criteria for policy improvement. In this work, we employ the return distribution to analyse risk-averse LQR problems using the Conditional Value at Risk (CVaR) as the risk measure. Since the gradient of CVaR is generally difficult to compute analytically, we propose a risk-averse policy gradient algorithm that relies on the zeroth-order optimisation to seek an optimal risk-averse policy. Numerical experiments are provided to showcase this application.   
\end{enumerate}
%






%----------------
\noindent \textbf{Related Work:} 
Most closely related to the problem considered in this paper is work on reinforcement learning for LQR, which focuses on learning the expected return through interaction with the environment; see, e.g., \citet{dean2020sample,tu2018least,fazel2018global,malik2019derivative,li2021distributed,yaghmaie2022linear,zheng2021sample}.
%
For example, \citet{fazel2018global} propose a model-free policy gradient algorithm for LQR and showed its global convergence with finite polynomial computational and sample complexity.
%
Moreover, \citet{zheng2021sample} study model-based reinforcement learning for the Linear Quadratic Gaussian problems, in which a model is first learnt from data and then used to design the policy. 
%
However, all these works rely on the expected return instead of the return distribution, 
%
hence these methods cannot  be applied here.


%
Since the return distribution captures the intrinsic randomness of the long-term cost, it provides a natural framework to consider more general optimality criteria, e.g., optimal risk-averse policies. 
%
There exist recent works on risk-averse policy design for DRL, including
\citet{singh2020improving,dabney2018implicit,tang2019worst}. For example, the work in \citet{dabney2018implicit} use the quantile function to approximate the return distribution, which is then applied to design risk-sensitive policies for Atari games. On the other hand, \citet{singh2020improving} show that risk-averse DRL achieves robustness against system disturbances in continuous control tasks. 
%
All these works focus on empirical improvements in specific tasks, however, without theoretical analysis.
Related to this paper is also work on risk-sensitive LQR, which has been studied in 
\citet{van2015distributionally,tsiamis2021linear,kim2021distributional,chapman2021toward,kishida2022risk}. Similarly, these methods however do not analyse the return distribution. 