% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,onecolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbold}
\usepackage{booktabs}
% additional packages manually imported by chi xie
\usepackage{adjustbox}
% \usepackage{makecell}
\usepackage{pifont}
\usepackage{multirow}

\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\newcommand{\Note}[1]{{\color{blue} \bf \small [NOTE: #1]}}
\newcommand{\Todo}[1]{{\color{red} [TODO: #1]}}

\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.

\usepackage[dvipsnames]{xcolor}
\usepackage{xcolor}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{4308} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
% \title{Image-Adaptive Human Object Interaction Classification}
\title{Supplementary Material for \\
Category Query Learning for Human-Object Interaction Classification}

\author{
Chi~Xie$^1$ \quad
Fangao Zeng$^{2}$ \quad
Yue~Hu$^3$ \quad
Shuang~Liang$^{1}$ \quad
Yichen~Wei$^{2}$ \\
$^{1}${Tongji University} \quad
$^{2}${MEGVII Technology} \quad
$^{3}${Shanghai Jiao Tong University} \\
$^{1}${\tt\small \{chixie, shuangliang\}@tongji.edu.cn} \\ \quad
$^{2}${\tt\small zfg472988436@163.com, wei\_yi\_chen@hotmail.com} \quad
$^{3}${\tt\small 18671129361@sjtu.edu.cn} \\
}

% \maketitle


%%%%%%%%% BODY TEXT

\section{Overview}

In this supplemental file, we provide more details of our work to supply the main paper.
Specifically,
\vspace{2pt}
\begin{itemize}
    \item[$\blacktriangleright$] \textbf{Score integration technique} used in our paper are explained in \cref{sec:score-integration};
    \item[$\blacktriangleright$] \textbf{Implementation details} are summarized in \cref{sec:implementation};
    % \item[$\blacktriangleright$] \textbf{Additional ablations} are presented in \cref{sec:additional-ablation}, including the component ablation on the other 2 baselines and the score integration technique;
    \item[$\blacktriangleright$] \textbf{Additional ablations} are presented in \cref{sec:additional-ablation}, which includes the ablations on the score integration technique;
    \item[$\blacktriangleright$] \textbf{Additional qualitative results} are presented in \cref{sec:additional-qualitative}.
\end{itemize}

\section{Score Integration Technique}
\label{sec:score-integration}

We introduce the score integration step briefly in Sec. 3.2 of the main paper, which leverages the image-level classification scores to stress or suppress certain categories during instance-level interaction categories.
As the score integration step is not the major contribution of the proposed method, and brings minor improvement (as in Tab. 4 of the paper), we did not elaborate on its details in the paper.

Before applying this score integration step, based on Eq. (4) in the paper, we can compute the classification scores for the $i$-th human-object instance over $K$ interaction categories as
\begin{equation}
    s_{i} = \operatorname{sigmoid}\left(
    \left[ \frac{\left(F_{i}, \overline{Q'}_{1}\right)}{\left\| F_{i} \right\| \left\| \overline{Q'}_{1} \right\| },
    \cdots,
    \frac{\left(F_{i}, \overline{Q'}_{K}\right)}{\left\| F_{i} \right\| \left\| \overline{Q'}_{K} \right\| } \right]
    \right),
\end{equation}
where the sigmoid operation is applied on the vector element-wise.

Next, we provide the detailed design of this score integration step. It includes a hard integration and a soft one.

\subsection{Hard Score Integration}
This hard score integration is motivated by the rank-adaptive pixel classification in RankSeg\cite{he2022rankseg}.
It consists of two steps: the first is to use the image classification results to sort and select some interaction categories, and perform H-O pair classification only on the selected categories, namely, category selection; the second is to adopt a series of temperature parameters that ranks the interaction classification results of sorted and selected categories, namely, category ranking.

\noindent{\textbf{Category selection.}} 
Instead of choosing the labels for an H-O pair from all $K$ predefined categories, based on the previous multi-label image classification prediction $\{p_{k}\}$ for the image, we perform a selected-label classification.
First, the top $\kappa$ of the classification weights $\{Q'_{k}\}$ is selected according to the descending order of image classification predictions as
\begin{equation}
    \left[\overline{Q'}_{1}, \cdots, \overline{Q'}_{\kappa}\right]
    = \operatorname{Top}-\kappa\left(\left[Q'_{1}, \cdots, Q'_{K}\right], \{p_{k}\}\right),
\end{equation}
and H-O pair classification is performed as
\begin{equation}
    s_{i}^{h} = 
    \operatorname{sigmoid}\left(
    \left[ \frac{\left(F_{i}, \overline{Q'}_{1}\right)}{\left\| F_{i} \right\| \left\| \overline{Q'}_{1} \right\| }, \cdots, \frac{\left(F_{i}, \overline{Q'}_{\kappa}\right)}{\left\| F_{i} \right\| \left\| \overline{Q'}_{\kappa} \right\| } \right]
    \right),
\label{eq.category_selection}
\end{equation}
where $\left[\overline{Q'}_{1}, \cdots, \overline{Q'}_{\kappa}\right]$ denotes the top $\kappa$ selected category queries (classification weights) associated with the largest $\kappa$ image classification scores, $s_{i}^{h}$ denotes the classification scores with hard score integration, and $\kappa$ represents the number of selected category queries, chosen as a much smaller value than $K$.

\noindent{\textbf{Category ranking.}}
On top of category selection, we apply a set of learnable temperature parameters $[\tau_{1}, \tau_{2}, \cdots, \tau_{\kappa}]$ to adjust the classification scores over the selected top $\kappa$ categories, so ~\cref{eq.category_selection} is changed to
\begin{equation}
    s_{i}^{h} = 
    \operatorname{sigmoid}\left(
    \left[ \frac{\left(F_{i}, \overline{Q'}_{1}\right)}{\left\| F_{i} \right\| \left\| \overline{Q'}_{1} \right\| \tau_{i} }, \cdots, \frac{\left(F_{i}, \overline{Q'}_{\kappa}\right)}{\left\| F_{i} \right\| \left\| \overline{Q'}_{\kappa} \right\| \tau_{\kappa} } \right]
    \right).
\end{equation}

We analyze the influence of $\kappa$ choices and the benefits of such a ranking adjustment in the ablation study.
Note that this is similar to the rank-adaptive pixel classification performed in RankSeg~\cite{he2022rankseg} for image and video segmentation tasks, though their classification is a single-label problem and softmax is applied while ours are multi-label and sigmoid is used.

\subsection{Soft Score Integration}

Another way to utilize the image-level classification scores is to directly multiply the instance classification scores $s_i$ with the image classification probabilities $\{p_{k}\}$, as
\begin{equation}
    s_{i}^{s} = \left[
    \sqrt{s_{i,1} * p_{1}}, \cdots, \sqrt{s_{i,K} * p_{K}}
    \right],
\end{equation}
where $s_{i}^{s}$ denotes the interaction classification scores of the $i$-th H-O instance, with soft sore integration.

Compared with the hard score integration, no interaction class is deprecated during instance classification. They are just stressed or suppressed in a soft way. Therefore, we call this soft score integration.

Note that hard and soft score integration can be applied together, as
\begin{equation}
    s_{i}^{s,h} = \left[
    \sqrt{s_{i,1}^{h} * \overline{p}_{1}}, \cdots, \sqrt{s_{i,\kappa}^{h} * \overline{p}_{\kappa}}
    \right],
\end{equation}
where $\left[ \overline{p}_{1}, \cdots, \overline{p}_{\kappa} \right]$ is the top $\kappa$ in $\{p_{k}\}$.
Through experiments in \cref{tab:ablation-rankingtechnique}, we find both soft and hard integration bring a small improvement and the best result is achieved when both is used.

\section{Implementation Details}
\label{sec:implementation}

Most of the implementation details have been provided in the paper. Here we summarize these details.
In the proposed category query learning, transformer decoder with 2 layers is used by default. The structure of each decoder layer in the proposed decoder consists of a cross-attention module, a self-attention module and a FFN in order. The weights of the existing losses in the baselines are not changed, and an image loss with loss weight $\lambda = 1.0$ is added to the final loss. For the asymmetric loss in image classification, we adopt $\gamma+ = 0$, $\gamma- = 4$ and $m = 0.05$.
Both hard and soft score integration are used. For category selection and ranking in hard score integration, we set $\kappa$ as 70 for HICO-DET~\cite{chao2018learning}.
Hyper-parameters like learning rate, weight decay, batch size and input image size follow the baseline settings by default.

Following the baseline detectors, the feature extractor is frozen for SCG~\cite{zhang2021spatially}, and updated for QPIC~\cite{tamura2021qpic} and GEN-VLKT~\cite{liao2022gen}.
For the experiments on GEN-VLKT, we change its classification classes from 600 HOI categories to 117 interaction categories for HICO-DET and from 263 to 29 for V-COCO, following most HOI detection methods.
For the experiments on SCG, the detection boxes are from a fine-tuned detector provided by DRG~\cite{gao2020drg} for HICO-DET and a fine-tuned DETR for V-COCO~\cite{gupta2015visual}.
The experiment is conducted on 8 Tesla V100 GPUs.

\section{Additional Ablations}
\label{sec:additional-ablation}

In this part, we perform some additional studies on technical details.


\begin{table}
  \centering
  % \begin{adjustbox}{width=1.0\linewidth}
  \begin{tabular}{c c | c | c c c}
    \toprule
    \multicolumn{2}{c |}{hard integration} & soft integration & \multicolumn{3}{c}{Default} \\
    selection & ranking & & Full & Rare & Non-Rare \\
    \midrule
    - & - & - & 34.98 & 31.73 & 35.95 \\
    \cmark & - & - & 35.09 & 32.98 & 35.72 \\
    \cmark & \cmark & - & 35.24 & 32.67 & 36.01 \\
    - & - & \cmark & 35.18 & 32.23 & 36.06 \\
    \cmark & \cmark & \cmark & \textbf{35.36} & \textbf{32.97} & \textbf{36.07} \\
    \bottomrule
  \end{tabular}
  % \end{adjustbox}
  \caption{Ablation on the techniques (soft and hard score integration) that we elaborate in \cref{sec:score-integration} to utilize image-level classification scores.
  The best results are marked in \textbf{bold}.
  }
  \label{tab:ablation-rankingtechnique}
\end{table}

\begin{table}
  \centering
  % \begin{adjustbox}{width=0.9\linewidth}
  \begin{tabular}{c | c c c c c c}
    \toprule
    $\kappa$ & - & 30 & 50 & 70 & 90 & 117 \\
    \midrule
    mAP & 34.98 & 35.04 & 35.19 & \textbf{35.24} & 35.08 & 35.03 \\
    \bottomrule
  \end{tabular}
  % \end{adjustbox}
  \caption{Ablation on the number of interaction categories in the hard score integration step, i.e., $\kappa$. The metric for comparison is the full mAP under \textit{default} setting on HICO-DET dataset. ``-'' denotes the hard score integration is not used.
  The best results are marked in \textbf{bold}.
  }
  \label{tab:ablation-topkcategories}
\end{table}

\subsection{Integration of Image-level Classification Scores}
As mentioned in \cref{sec:score-integration}, the score integration process is proposed to utilize the image-classification scores in the proposed method, with two strategies: the \textbf{hard score integration}, consisting of category selection and category ranking, and the \textbf{soft score integration}, which is a score multiplication operation between instance-level and image-level classification scores.
As shown in \cref{tab:ablation-rankingtechnique}, each of them brought a marginal improvements: the model with hard score integration achieves 35.24 mAP while the one with score integration achieves 35.18 mAP. Together, a performance of 35.36 is obtained. We use this two techniques together by default.


\subsection{Different $\kappa$ in Hard Score Integration}
In this part, we study to influence of the number of the selected categories, denoted as $\kappa$, in the hard score integration step. As shown in \cref{tab:ablation-topkcategories}, the selection and ranking on interaction categories works best when $\kappa = 70$. For a smaller $\kappa$, some categories may be filtered by mistake, like when $\kappa = 30$, the performance is only 35.04 mAP, falls behind the optimal setting by 0.15 mAP. When $\kappa = 117$, none of the categories are filtered and only ranking operation is still effective. This results in a little performance drop of 0.16 mAP. We use $\kappa = 70$ by default.

\begin{figure*}
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.8\linewidth]{figs/qualitative_more.pdf}
  \caption{More qualitative comparison between the baseline and the proposed method on HICO-DET.
  From left to right, column 1: true positive (\textcolor{Green}{TP}) detection results, whose interaction score is increased by the proposed method; column 2: false positive (\textcolor{red}{FP}) detection results, whose interaction score is decreased by the proposed method; column 3: corresponding image-level GT and predictions by the proposed method. Scores on the left and right of an image are the interaction classification scores of the visualized instance in the image from \textcolor{olive}{the baseline} and \textcolor{Violet}{the proposed method}. ``-'' for score denotes a instance not discovered (thus no scores).
  Best viewed in color.
  }
  \label{fig:qualitative-more}
\end{figure*}

\section{Additional Qualitative Results}
\label{sec:additional-qualitative}

In \cref{fig:qualitative-more}, we provide more qualitative results in addition to the cases in the paper.

The first case shows the proposed method uses the feature of other instances in the image to help the recognition of a small and challenging instance. The image contains multiple instances of person directing and inspecting an airplane. The TP instance visualized is associated with a small and occluded person, which the baseline fails to discover (the score is denoted as ``-''). The proposed method successfully predict this instance with a high score of 0.46. This is consistent with the quantitative discussion on Fig. 5 in the paper.

In the second case, the proposed method discovers an interaction category ``repair'' neglected by the baseline, possibly with the help of correlations between categories (``inspect'' and ``repair''). The ``repair'' interaction is semantically abstract, but the existence of ``inspect'' may help. This may explain why removing the self-attention from our decoder with cause performance drop in Tab. 6 of the paper: it may learns the dependencies between different interaction categories. In the forth case, the learning of ``cut with'' interaction may also benefit from the recognition of ``hold''. Additionally, there is an obvious annotation mistake in the second case: interactions like ``ride'' and ``sit on'' are not labeled though they exists (in the background). The proposed method still produces relatively high image-level classification scores for these two categories. Actually, such annotation mistakes exists widely in HICO-DET dataset, and the increase on mAP may not fully show the effectiveness of the proposed method.

In the third and forth cases, our method shows its ability to distinguish whether instances belonging to an interaction category existing in the image. In the third image, though it produce a relatively high ``hold'' score of 0.36 at image level, it does not take all the H-O pairs in the image as ``hold'', which would be very wrong. It successfully discovers the TP ``hold'' instance that the baseline missed, and suppresses the FP ``hold'' from 0.31 to 0.16. This is consistent with the quantitative results in Tab.4 of the paper that shows the proposed method benefits more from the \textit{adaptive} instance classification weight rather than simply an image classification task. Notably, these two are challenging images with ``dense'' interaction instances, especially the third case, which corresponds to the discussion in Fig. 5 and Sec 5.5 of the paper.


\section{Potential Limitation and Social Impact}

The proposed method focuses on the interaction classification sub-task in HOI detection. It does not improve H-O pair detection directly. In the future, we will try to extend this idea to the classification of human and objects in HOI to improve H-O pair detection. 

The proposed algorithm has no evident negative impact to society. However, someone might use this method for malicious usage, e.g., to attack people in military usage or invasion of privacy with surveillance. Therefore, we encourage well-intended application of the proposed method.

%%%%%%%%% REFERENCES
{\small
% \bibliographystyle{ieee_fullname}
\bibliographystyle{ieee_abbrev}
\bibliography{supplementary}
}

\end{document}
