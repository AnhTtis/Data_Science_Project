{
    "arxiv_id": "2303.09100",
    "paper_title": "Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models",
    "authors": [
        "Xinyang Liu",
        "Dongsheng Wang",
        "Miaoge Li",
        "Zhibin Duan",
        "Yishi Xu",
        "Bo Chen",
        "Mingyuan Zhou"
    ],
    "submission_date": "2023-03-16",
    "revised_dates": [
        "2023-03-17"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV",
        "cs.CL",
        "cs.LG"
    ],
    "abstract": "For downstream applications of vision-language pre-trained models, there has been significant interest in constructing effective prompts. Existing works on prompt engineering, which either require laborious manual designs or optimize the prompt tuning as a point estimation problem, may fail to describe diverse characteristics of categories and limit their applications. We introduce a Bayesian probabilistic resolution to prompt learning, where the label-specific stochastic prompts are generated hierarchically by first sampling a latent vector from an underlying distribution and then employing a lightweight generative model. Importantly, we semantically regularize prompt learning with the visual knowledge and view images and the corresponding prompts as patch and token sets under optimal transport, which pushes the prompt tokens to faithfully capture the label-specific visual concepts, instead of overfitting the training categories. Moreover, the proposed model can also be straightforwardly extended to the conditional case where the instance-conditional prompts are generated to improve the generalizability. Extensive experiments on 15 datasets show promising transferability and generalization performance of our proposed model.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.09100v1"
    ],
    "publication_venue": null
}