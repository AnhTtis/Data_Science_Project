%%%%%%%% ICML 2022 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[nohyperref]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2022} with \usepackage[nohyperref]{icml2022} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2022}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[utf8]{inputenc}   % allow utf-8 input
\usepackage[T1]{fontenc}   % use 8-bit T1 fonts
\usepackage{hyperref}   % hyperlinks
\usepackage{url}   % simple URL typesetting
\usepackage{booktabs}   % professional-quality tables
\usepackage{amsfonts}   % blackboard math symbols
\usepackage{nicefrac}    % compact symbols for 1/2, etc.
\usepackage{microtype}   % microtypography
\usepackage{xcolor}   % colors

% For theorems and such
\usepackage{hyperref}
\usepackage{url}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{tikz} 
\usepackage{pgfplots}
% \usepackage{subcaption}
% \usepackage{citep}
% \usepackage{natbib}
% \usepackage{subcaption}


\usepackage{wrapfig} % allow utf-8 input
\usepackage{graphicx}
% \usepackage{pdfpages}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm} %%算法
\usepackage{algorithmic} %%算法
\usepackage{enumerate} %%用于item
\usepackage{multirow} %%表格合并行单元格
\usepackage{amsmath}

\usepackage{colortbl} 
\usepackage{xcolor}
\usepackage{array}
\usepackage{bbding}
\usepackage{enumerate}
\usepackage{bm}
% \usepackage{subfigure}

\usepackage{helvet} %Required
\usepackage{courier} %Required
\usepackage{color}
% \usepackage{cite}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\Amat}[0]{\ensuremath{{\bf A}}}
\newcommand{\Bmat}[0]{\ensuremath{{\bf B}} }
\newcommand{\Cmat}[0]{\ensuremath{{\bf C}} }
\newcommand{\Dmat}[0]{\ensuremath{{\bf D}} }
\newcommand{\Emat}[0]{\ensuremath{{\bf E}} }
\newcommand{\Fmat}[0]{\ensuremath{{\bf F}} }
\newcommand{\Gmat}[0]{\ensuremath{{\bf G}} }
\newcommand{\Hmat}[0]{\ensuremath{{\bf H}} }
\newcommand{\Imat}[0]{\ensuremath{{\bf I}} }
\newcommand{\Jmat}[0]{\ensuremath{{\bf J}} }
\newcommand{\Kmat}[0]{\ensuremath{{\bf K}} }
\newcommand{\Lmat}[0]{\ensuremath{{\bf L}} }
\newcommand{\Mmat}[0]{\ensuremath{{\bf M}} }
\newcommand{\Nmat}[0]{\ensuremath{{\bf N}} }
\newcommand{\Omat}[0]{\ensuremath{{\bf O}} }
\newcommand{\Pmat}[0]{\ensuremath{{\bf P}} }
\newcommand{\Qmat}[0]{\ensuremath{{\bf Q}} }
\newcommand{\Rmat}[0]{\ensuremath{{\bf R}} }
\newcommand{\Smat}[0]{\ensuremath{{\bf S}} }
\newcommand{\Tmat}[0]{\ensuremath{{\bf T}} }
\newcommand{\Umat}[0]{\ensuremath{{\bf U}} }
\newcommand{\Vmat}[0]{\ensuremath{{\bf V}} }
\newcommand{\Wmat}[0]{\ensuremath{{\bf W}} }
\newcommand{\Xmat}[0]{\ensuremath{{\bf X}} }
\newcommand{\Ymat}[0]{\ensuremath{{\bf Y}} }
\newcommand{\Zmat}[0]{\ensuremath{{\bf Z}} }
\newcommand{\bds}[1]{\boldsymbol{#1}}
\newcommand{\zerov}[0]{\ensuremath{{\bf 0}} }
\newcommand{\onev}[0]{\ensuremath{{\bf 1}} }
\newcommand{\av}[0]{\ensuremath{\boldsymbol{a}} }
\newcommand{\bv}[0]{\ensuremath{\boldsymbol{b}} }
\newcommand{\cv}[0]{\ensuremath{\boldsymbol{c}} }
\newcommand{\dv}[0]{\ensuremath{\boldsymbol{d}} }
\newcommand{\ev}[0]{\ensuremath{\boldsymbol{e}} }
\newcommand{\fv}[0]{\ensuremath{\boldsymbol{f}} }
\newcommand{\gv}[0]{\ensuremath{\boldsymbol{g}} }
\newcommand{\hv}[0]{\ensuremath{\boldsymbol{h}} }
\newcommand{\iv}[0]{\ensuremath{\boldsymbol{i}} }
\newcommand{\jv}[0]{\ensuremath{\boldsymbol{j}} }
\newcommand{\kv}[0]{\ensuremath{\boldsymbol{k}} }
\newcommand{\lv}[0]{\ensuremath{\boldsymbol{l}} }
\newcommand{\mv}[0]{\ensuremath{\boldsymbol{m}} }
\newcommand{\nv}[0]{\ensuremath{\boldsymbol{n}} }
\newcommand{\ov}[0]{\ensuremath{\boldsymbol{o}} }
\newcommand{\pv}[0]{\ensuremath{\boldsymbol{p}} }
\newcommand{\qv}[0]{\ensuremath{\boldsymbol{q}} }
\newcommand{\rv}[0]{\ensuremath{\boldsymbol{r}} }
\newcommand{\sv}[0]{\ensuremath{\boldsymbol{s}} }
\newcommand{\tv}[0]{\ensuremath{\boldsymbol{t}} }
\newcommand{\uv}[0]{\ensuremath{\boldsymbol{u}} }
\newcommand{\vv}[0]{\ensuremath{\boldsymbol{v}} }
\newcommand{\wv}[0]{\ensuremath{\boldsymbol{w}} }
\newcommand{\xv}[0]{\ensuremath{\boldsymbol{x}} }
\newcommand{\yv}[0]{\ensuremath{\boldsymbol{y}} }
\newcommand{\zv}[0]{\ensuremath{\boldsymbol{z}} }
\newcommand{\Av}[0]{\ensuremath{\boldsymbol{A}} }
\newcommand{\Bv}[0]{\ensuremath{\boldsymbol{B}} }
\newcommand{\Cv}[0]{\ensuremath{\boldsymbol{C}} }
\newcommand{\Dv}[0]{\ensuremath{\boldsymbol{D}} }
\newcommand{\Ev}[0]{\ensuremath{\boldsymbol{E}} }
\newcommand{\Fv}[0]{\ensuremath{\boldsymbol{F}} }
\newcommand{\Gv}[0]{\ensuremath{\boldsymbol{G}} }
\newcommand{\Hv}[0]{\ensuremath{\boldsymbol{H}} }
\newcommand{\Iv}[0]{\ensuremath{\boldsymbol{I}} }
\newcommand{\Jv}[0]{\ensuremath{\boldsymbol{J}} }
\newcommand{\Kv}[0]{\ensuremath{\boldsymbol{K}} }
\newcommand{\Lv}[0]{\ensuremath{\boldsymbol{L}} }
\newcommand{\Mv}[0]{\ensuremath{\boldsymbol{M}} }
\newcommand{\Nv}[0]{\ensuremath{\boldsymbol{N}} }
\newcommand{\Ov}[0]{\ensuremath{\boldsymbol{O}} }
\newcommand{\Pv}[0]{\ensuremath{\boldsymbol{P}} }
\newcommand{\Qv}[0]{\ensuremath{\boldsymbol{Q}} }
\newcommand{\Rv}[0]{\ensuremath{\boldsymbol{R}} }
\newcommand{\Sv}[0]{\ensuremath{\boldsymbol{S}} }
\newcommand{\Tv}[0]{\ensuremath{\boldsymbol{T}} }
\newcommand{\Uv}[0]{\ensuremath{\boldsymbol{U}} }
\newcommand{\Vv}[0]{\ensuremath{\boldsymbol{V}} }
\newcommand{\Wv}[0]{\ensuremath{\boldsymbol{W}} }
\newcommand{\Xv}[0]{\ensuremath{\boldsymbol{X}} }
\newcommand{\Yv}[0]{\ensuremath{\boldsymbol{Y}} }
\newcommand{\Zv}[0]{\ensuremath{\boldsymbol{Z}} }
\newcommand{\Gammamat}[0]{\ensuremath{\boldsymbol{\Gamma}} }
\newcommand{\Deltamat}[0]{\ensuremath{\boldsymbol{\Delta}} }
\newcommand{\Thetamat}[0]{\ensuremath{\boldsymbol{\Theta}} }
\newcommand{\Lambdamat}[0]{\ensuremath{\boldsymbol{\Lambda}} }
\newcommand{\Ximat}[0]{\ensuremath{\boldsymbol{\Xi}} }
\newcommand{\Pimat}[0]{\ensuremath{\boldsymbol{\Pi}} }
\newcommand{\Sigmamat}[0]{\ensuremath{\boldsymbol{\Sigma}} }
\newcommand{\Upsilonmat}[0]{\ensuremath{\boldsymbol{\Upsilon}} }
\newcommand{\Phimat}[0]{\ensuremath{\boldsymbol{\Phi}}}
\newcommand{\Psimat}[0]{\ensuremath{\boldsymbol{\Psi}} }
\newcommand{\Omegamat}[0]{\ensuremath{\boldsymbol{\Omega}}}
\newcommand{\alphav}[0]{\ensuremath{\boldsymbol{\alpha}} }
\newcommand{\betav}[0]{\ensuremath{\boldsymbol{\beta}} }
\newcommand{\gammav}[0]{\ensuremath{\boldsymbol{\gamma}} }
\newcommand{\deltav}[0]{\ensuremath{\boldsymbol{\delta}} }
\newcommand{\epsilonv}[0]{\ensuremath{\boldsymbol{\epsilon}} }
\newcommand{\zetav}[0]{\ensuremath{\boldsymbol{\zeta}} }
\newcommand{\etav}[0]{\ensuremath{\boldsymbol{\eta}} }
\newcommand{\thetav}[0]{\ensuremath{\boldsymbol{\theta}} }
\newcommand{\iotav}[0]{\ensuremath{\boldsymbol{\iota}} }
\newcommand{\kappav}[0]{\ensuremath{\boldsymbol{\kappa}} }
\newcommand{\lambdav}[0]{\ensuremath{\boldsymbol{\lambda}} }
\newcommand{\muv}[0]{\ensuremath{\boldsymbol{\mu}} }
\newcommand{\nuv}[0]{\ensuremath{\boldsymbol{\nu}} }
\newcommand{\xiv}[0]{\ensuremath{\boldsymbol{\xi}} }
\newcommand{\omicronv}[0]{\ensuremath{\boldsymbol{\omicron}} }
\newcommand{\piv}[0]{\ensuremath{\boldsymbol{\pi}} }
\newcommand{\rhov}[0]{\ensuremath{\boldsymbol{\rho}} }
\newcommand{\sigmav}[0]{\ensuremath{\boldsymbol{\sigma}} }
\newcommand{\tauv}[0]{\ensuremath{\boldsymbol{\tau}} }
\newcommand{\upsilonv}[0]{\ensuremath{\boldsymbol{\upsilon}} }
\newcommand{\phiv}[0]{\ensuremath{\boldsymbol{\phi}} }
\newcommand{\chiv}[0]{\ensuremath{\boldsymbol{\chi}} }
\newcommand{\psiv}[0]{\ensuremath{\boldsymbol{\psi}} }
\newcommand{\omegav}[0]{\ensuremath{\boldsymbol{\omega}} }
\newcommand{\varepsilonv}[0]{\ensuremath{\boldsymbol{\varepsilon}} }
\newcommand{\varthetav}[0]{\ensuremath{\boldsymbol{\vartheta}} }
\newcommand{\varpiv}[0]{\ensuremath{\boldsymbol{\varpi}} }
\newcommand{\varrhov}[0]{\ensuremath{\boldsymbol{\varrho}} }
\newcommand{\varsigmav}[0]{\ensuremath{\boldsymbol{\varsigma}} }
\newcommand{\varphiv}[0]{\ensuremath{\boldsymbol{\varphi}} }
\newcommand{\cdotv}[0]{\ensuremath{\boldsymbol{\cdot}}}
\newcommand{\E}[0]{\ensuremath{\mathbb{E}}}

\newcommand{\mc}{\multicolumn}
\newcommand{\mr}{\multirow}
\newcommand{\nts}{\negthickspace}

\newcommand{\given}{\,|\,}
\def\rr{\textcolor{red}}
\def\bb{\textcolor{blue}}
\def\re{\textcolor{black}}
\def\mz#1{{\color{orange}{\bf [mz:} {\it{#1}}{\bf ]}}}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models}

\begin{document}

\twocolumn[
\icmltitle{Patch-Token Aligned Bayesian Prompt Learning \\for Vision-Language Models}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2022
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Xinyang Liu}{equal,xidian}
\icmlauthor{Dongsheng Wang}{equal,xidian}
\icmlauthor{Miaoge Li}{xidian}
\icmlauthor{Zhibin Duan}{xidian}
\icmlauthor{Yishi Xu}{xidian}
\icmlauthor{Bo Chen}{xidian}
\icmlauthor{Mingyuan Zhou}{UT}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{xidian}{Xidian University}
\icmlaffiliation{UT}{The University of Texas at Austin}
\icmlcorrespondingauthor{Bo Chen}{bchen@mail.xidian.edu.cn}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, Optimal transport, pre-trained vision-language model, Bayesian models}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution

\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
For downstream applications of vision-language pre-trained models, there has been significant interest in constructing effective prompts. Existing works on prompt engineering, which either require laborious manual designs or optimize the prompt tuning as a point estimation problem, may fail to describe diverse characteristics of categories and limit their applications. We introduce a Bayesian probabilistic resolution to prompt learning, where the label-specific stochastic prompts are generated hierarchically by first sampling a latent vector from an underlying distribution and then employing a lightweight generative model. Importantly, we semantically regularize prompt learning with the visual knowledge and view images and the corresponding prompts as patch and token sets under  optimal transport, which pushes the prompt tokens to faithfully capture the label-specific visual concepts, instead of overfitting the training categories. Moreover, the proposed model can also be straightforwardly extended to the conditional case where the instance-conditional prompts are generated to improve the generalizability. Extensive experiments on 15 datasets show promising transferability and generalization performance of our proposed model.

\end{abstract}

% The key idea in recent prompt engineering is to learn a sequence of token embeddings $\pv=[\pv_1, \pv_2, ..., \pv_L], \pv_l \in R^d$, according to the task specific loss. For instance, CoOp directly views the prompts as a learnable matrix $\pv \in R^{d \times L}$. To improve the robustness on the unseen data, CoCoOp is developed by introducing a lightweight network $r(\xv)$: $\pv(\xv) = [\pv_1+r(\xv), \pv_2+r(\xv), ..., \pv_L+r(\xv)]$. However, both of them are point estimation in the embedding space, that may lead to suboptimal results. 

% To this end, variational prompt learning (VPL) is proposed, which generates the probabilistic instance-specific prompt as:
% \begin{equation*}
%     \pv_\gamma(\xv) = [\pv_1 + \rv_\gamma, \pv_2 + \rv_\gamma, ..., \pv_L + \rv_\gamma], \quad \rv_\gamma \sim p_\gamma(x)
% \end{equation*}

\section{Introduction} \label{intro}
Large-scale vision-language pre-trained models (VLPs) have recently led to impressive results on various computer vision (CV) tasks~\citep{wang2021simvlm,jia2021scaling,cho2021unifying,radford2021learning,li2022blip}. Pre-trained on web-scale image-text association pairs, such VLPs have the ability to carry the semantic knowledge on which visual concepts correspond to which textual sequence and vice versa, which has been proven beneficial for visual understanding \cite{radford2021learning,mei2022guest,du2022learning}. This has motivated the rapid rise of \textit{prompting learning} that hopes to fine-tune VLPs by formalizing the downstream tasks as language modeling problems and optimizing only the text inputs (prompts)~\cite{radford2021learning,zhou2022conditional,zhou2022learning}, such as ``a photo of a \{class\}.'', where  the ``\{class\}'' token denotes the real class name. In contrast to supervised learning with discrete labels from a closed set of categories, prompt learning receives knowledge from pre-trained language models and supports open-set visual concepts, often producing better performance, especially on few/zero-shot tasks~\cite{zhou2022conditional,ppt}.

\begin{figure}[!t]
\centering
\includegraphics[width=1\linewidth]{motivation1.pdf}
\label{bpl}
\vspace{-4mm}
\caption{\small{The core idea that multiple prompts are generated from the label-specific distributions.}}
\vspace{-6mm}
\end{figure}

However, identifying the optimal prompts for the target label is a non-trivial task in VLPs and often requires domain knowledge and time-consuming attempts. For example, CLIP~\cite{radford2021learning} ensembles 80 hand-crafted prompts to generate the corresponding label embeddings, such as ``a photo of \{class\}.'' and ``a painting of a \{class\}.''. Rather than manually designing prompts, quite  a few recent  works focus on continuous prompt learning in the embedding space~\cite{zhou2022learning, yao2021cpt,chen2022prompt}. Relaxing the human interpretability of prompts, they optimize the deterministic prefix tokens as learnable embedding vectors with a task-specific loss, achieving state-of-the-art results in visual classification tasks.


% However, identifying the optimal prompts for the target label is a non-trivial task in VLPs and often requires domain knowledge and time-consuming attempts. For example, CLIP~\cite{radford2021learning} ensembles 80 carefully designed hand-crafted prompts to generate the corresponding label embeddings, including ``a photo of \{class\}.'', ``a painting of a \{class\}.'' and so on. Rather than manually design prompts, recent succeeding works focus on automated prompt learning in the embedding space, 
% Context Optimization (CoOp)~\cite{zhou2022learning} models the prefix tokens with learnable vectors, which can be initialized with the pre-trained word embeddings or random values.
% Moving beyond the static prompts, \citet{zhou2022conditional} introduces Conditional CoOp (CoCoOp) and designs instance-specific prompts by adding the prompt embeddings with projected image features. Compared to handcrafted prompts, continue prompts relax the human interpretability and learn deterministic token embeddings in a data-driven manner through back-propagation by maximizing the task-specific objective functions, achieving better performance on visual classification.

Though effective, some recent studies report that such models suffer from suboptimal prompt learning in terms of diversity and generalizability~\cite{zhu2022prompt,ma2022understanding,lu2022prompt}. The diversity issue comes from the deterministic prompt assumption, where only a sentence is searched to represent a class. Intuitively, one class can be modeled by many intrinsic attributes. Thus, it is critical to learn multiple prompts that focus on different concepts. Motivated by this, a few works introduce the uncertainty in prompt learning and attempt to generate  distributed prompts~\cite{derakhshani2022variational,lu2022prompt}. However, they either require artificial prompts or focus on the uncertainty in the image domain, failing to deeply explore the knowledge in the text domain. 

For the generalizability issue, unfortunately, there are no clear theoretical explanations in the community~\cite{ma2022understanding,gao2021clip}. Recent studies have introduced various techniques to enhance the transferability, including conventional anti-overfitting tricks, instance-specific prompt generation, and gradient flow~\cite{gao2021clip,zhou2022conditional,ma2022understanding,zhu2022prompt}. Despite the improvement on few/zero-shot tasks, how to balance the base and new categories remains an open problem.

% where a sequence of vectors are learned by minimizing label-specific loss.   Despite the performance improvement on image classification task, most of previous methods treat prompts as deterministic rather than random variables. As a result, the learned sequence mostly composed of deterministic mappings and often incapable of modeling complex patterns in visual descriptions such as pose, angle, and object size, leading to suboptimal prompt searching.

% Another challenge arises from the generalization ability to new categories. Because of the shared prompts across all categories and they are fixed once learned on a specific set of training classes, previous works like CoOp usually overfits the seen classes, thus fails to capture true categorical tokens that are vital for open-set scene understanding. To bridge such generalization gap, CoCoOp of [1] empirically adjusts the fixed prompts by adding the image feature and designs a dynamic strategy to learn instance-specific prompts. Although simple to apply, CoCoOp achieves better performance on new tasks. However, it still has the limitation of the intuitive addition operation where each token in the prompt is viewed equally and unable to capture diverse vision concepts.

To this end, this paper proposes Bayesian prompt learning, where the label-specific stochastic prompts are generated hierarchically under the Bayesian framework. 
First, We introduce the uncertainty directly in the label domain and model each label with a variational distribution~\cite{vae} over the word embedding space. The to-be-inferred posterior contains meta-information about the corresponding category, showing advantages over point estimation in modeling uncertainty and highly structured data~\cite{fan2020bayesian}.
Second, a lightweight generative network is then employed to generate the prefix embeddings according to the sampled vector from the variational distribution. Though the generative network is a deterministic mapping, the output prompts can be viewed as an implicit distribution in the embedding space due to its random inputs, which enables the proposed model to naturally handle  diverse visual concepts, resulting in robust prompt learning.

Moreover, to address the over-fitting issue in prompt learning, we propose a novel semantic alignment between visual patches and textual tokens under the optimal transport (OT) framework~\cite{villani2009optimal}. Specifically, we formulate the prompt-token set and image-patch set within the same category as two empirical distributions, which share similar semantics about the corresponding class, while from different modalities. Therefore, prompt learning can be viewed as the process of learning the distribution of prompt tokens to be as close to the distribution of visual patches as possible. Fortunately, the recent developments in OT provide us with an efficient tool to quantify the difference between two discrete distributions 
with different supports~
\cite{cuturi2013sinkhorn,peyre2019computational,nstm}. Importantly, the cost function in OT specifies the similarities between the prompt tokens and visual patches in the embedding space, which makes it possible to regularize the learning of tokens with visual guidance. As a result, the aligned prompts are encouraged to capture the true label-specific visual concepts, rather than over-fitting to the training set.

The main contributions of this paper are summarized as follows: %\textit{\textbf{i)}} 
\begin{itemize}
    \item We propose the Bayesian prompt learning that generates the label-specific stochastic prompts hierarchically. which models each label as a distribution over the embedding space, and successfully handles the diverse visual concepts.
    \item To avoid over-fitting to the training set, we introduce the OT distance as a regularization that guides the learning of prompts with visual knowledge by aligning the patches and tokens semantically. 
    \item We formulate the proposed model as a variational inference problem, and a combined loss function is derived to optimize all parameters efficiently. Extensive experiments show that our models outperform the baselines.
\end{itemize}


% Final, instead of viewing the prompt tokens equally, the proposed model allows each token carries its related vision concepts under the optimal transport theory. Specifically, given a label, we view the prompt tokens as a set of its linguistic concepts and meanwhile the patch embeddings within the associated image as a set of its visual concepts. They share the similar semantic concepts of the same category, but come from different modalities. To exploit such deep diffusion, we align those two set by
% minimizing the optimal transport distance between them. In this way, the optimization of the prompts can be guided by the pre-trained vision knowledge and each token has the ability to focus more on its related patches by the predefined transport cost matrix.

% Moreover, we formulate the prompt learning as a Bayesian inference problem and leverage a variational distribution to estimate the true posterior of labels. This is beneficial to 1) a simple weighted ELBO can be derived by considering the downstream loss, and the proposed model can be optimized via the SGD in an end-to-end way; 2) recalling the fact that the Kullback Leibler (KL) divergence from the posterior to the prior, we can efficiently incorporate the domain knowledge into prompt learning via the prior distribution, \textit{e.g.}, we can specify the prior from the pre-trained label embeddings; 3) the proposed model can be easily extended to its conditional version due to the flexibility of the employed neural networks. The resulting algorithm allows to instance-specific prompt learning and achieves better generalization capability. In summary, our main contributions in this paper are as follows:



\section{Related Work}
\paragraph{Prompt Learning in VLPs} 
Prompt learning originates from the NLP domains with the motivation to best use the pre-trained language models~\cite{brown2020language,autoprompt,liu2023pre}, and it has received increasing research attention in VLPs due to its impressive results~\cite{ge2022domain,sun2022dualcoop,feng2022promptdet}. For example, CLIP~\cite{radford2021learning} in practice manually designs templates based on human knowledge and shows great potential in few/zero-shot tasks. Context Optimization (CoOp) ~\cite{zhou2022learning} first introduces the continuous prompt into VLPs and views the prompt tokens as a set of learnable vectors that can be optimized by minimizing the cross entropy loss. instead of learning static prompts, Conditional CoOp (CoCoOp)~\cite{zhou2022conditional} learns an input-specific prompt by incorporating image features via a lightweight network and shows better generalization on unseen categories. The most related work to ours is Prompt Distribution leArning (ProDA)~\cite{lu2022prompt}, which focuses on the output embeddings of prompts and employs a Gaussian distribution to model the latent representation by pre-defining K label-specific templates. However, ours is a novel Bayesian prompt generation method based on input embeddings, aiming to generate the label-specific stochastic prompts in a data-driven framework, rather than based on handcraft prompts.
% Another recent work is Variational Prompt Tuning (VPT)~\cite{derakhshani2022variational}, which proposes a data-driven method for learning uncertainty from the visual domain. Different from VPT which generates the instance-specific prompts by adding a base learned prompt with a residual sampled vector, we aim to deeply explore the uncertainty in the text domain and generate the label-specific stochastic prompts via a hierarchical path. 
% Moreover, our model can also be extended to the conditional version that 

% Recent studies find that such artificial prompts show sensitivity in performance, where even a slight change in wording could make a hugely different result. To this end, Context Optimization (CoOp) of [1] views the prompts' tokens as a set of learnable vectors that can be optimized in an end-to-end manner. instead of learning static prompts, Conditional CoOp (CoCoOp) learns an input-specific prompt by incorporating image feature via a lightweight network and show better generalization on unseen categories. While the above methods search the prompts as a point-estimation problem and only a single prompt is learned, which neglects the diversity of visual concepts, leading to suboptimal results. This motivates the recent distributed prompt learning. For example, Prompt Distribution Learning (ProDA) of [1] estimates the posterior distribution over the output embeddings of the prompts by collecting K prompts for each label. However, ProDA still relies on hand-crafted rules to pre-define the K prompts. Variational prompt learning (VPT) of [1] built on CoCoOp and introduce the uncertainty from the image feature, whose prompts are obtained by combining a base learned prompt with an instance-specific vector sampled from the underlying distribution. Although they also adopt distribution strategy to learn prompts, they are distinct from ours in terms of uncertainty incorporation and technical details. We model each target label as a Gaussian distribution, and the stochastic prompts are then generated based on the label-specific samples, which results in better representation. 

\paragraph{Optimal Transport} 
OT is originally developed to measure the distance of two probability distributions over metric spaces and has recently drawn great attention in many theoretic and application tasks due to the brilliant property of distribution matching~\cite{arjovsky2017wasserstein,balaji2020robust,zhao2021towards,tanwisuth2021prototype}. 
For example, \citet{redko2019optimal} address the target shift problem by aligning the domain distributions under the OT framework. 
% \citet{zhao2018label} aim at learning the label distribution and exploring label correlations at the same time by minimizing the OT distance between the prediction and ground truth. 
Enhanced transport distance (ETD) of \cite{li2020enhanced} introduce the attention-aware OT distance to measure the domain discrepancy. \citet{guo2022learning} minimize the OT distance between the balance set and imbalance set for automatic re-weighting. 
The works that connect prompt learning with OT tend, however, to be very limited and still in the exploration stage.
% Due to the brilliant property of distribution matching, OT has been applied in many theoretic and application tasks including generative models~\cite{arjovsky2017wasserstein,salimans2018improving,zhao2020neural}, structural matching~\cite{chen2019improving, xu2020vocabulary,zhao2021towards, xu2019gromov}, domain adaptation~\cite{courty2014domain, balaji2020robust, courty2017joint}, and other distribution-based tasks.
Prompt Learning with Optimal Transport (PLOT) ~\cite{chen2022prompt} attempts to align the visual features and prompts (textual feature) by learning an adaptive transport plan~\cite{rubner2000earth} to learn multiple comprehensive prompts. 
In contrast to PLOT, which focuses directly on distributions between two feature sets of distinct modalities, we intend to explore a more natural semantic relationship between image patches and prompt tokens by employing the OT distance to guide the learning of diverse prompts.




% For example, Rsdko \emph{et al}  address the target shift problem via aligning the domain distributions under OT framework. Zhao \emph{et al}.  aim at learning the label distribution and exploring label correlations at the same time by minimizing the OT distance between the prediction and ground truth. The works that connect multi-label classification with OT tend, however, to be very limited and still in the exploration stage. Frogner \emph{et al}.  designs a loss function for multi-label learning based on OT distance, which is defined as the transport cost of moving the mass in the predicted measure to that in the target. M3DN of  focuses on the multi-modal multi-instance case and models the label correlations based on OT by considering the consistency principle between modality predictions and the latent ground label metric. Although earlier works also apply OT distance for multi-label classification, they usually work in the label space to measure the quality of prediction. Our model is distinct from theirs in terms of technical details and motivations. Further, it concentrates more on aligning semantics between the visual patch and textual label domains for multi-label prediction.

\section{The Proposed Method}
An overview of our proposed \textbf{P}atch-Token Aligned \textbf{B}ayesian \textbf{Prompt} learning (PBPrompt) is shown in Fig.~\ref{model_overview}. Below, we first briefly review the CoOp and OT distance, which are the base concepts used in this paper. Then, we introduce the technical details of our model, which aims to improve the diversity and generalizability of CoOp.
\subsection{Reviews of CoOp and OT distance}
\paragraph{Context Optimization (CoOp)} of ~\citet{zhou2022learning} builds on CLIP-like VLPs and is a basic method for prompt learning.
A VLP, such as CLIP often consists of an image encoder $f(x)$ and text encoder $g(t)$, each outputs a d-dimensional embedding for an image $\xv \in \mathbb{R}^{(3\times H \times W)}$, and a sentence with L tokens $s=[\wv_1, \wv_2, ..., \wv_L]$, where $\wv_l \in \mathbb{R}^e$ is the e-dimensional token embedding. To synthesize the class-specific weights for the classification task, CoOp designs the context prompt as $\tv_c = [\pv_1, \pv_2, ..., \pv_{L-1}, \ev_c]$, where $\ev_c$ is label embedding of class $c$, $\pv = \{ \pv_l \in \mathbb{R}^e \}_{l=1}^{L-1}$ are L-1 learnable context vectors. Given $\{\tv_c\}_{c=1}^C$ and $\xv$, CoOp models the image label $p(\yv | \xv)$ as a Categorical distribution according to the similarity between the image and label feature with:
\begin{equation} \label{likelihood}
    p(y|\xv) = \frac{\text{exp} (\text{sim} (f(\xv), g(\tv^c)) / \tau)}{\sum_c^C \text{exp} (\text{sim} (f(\xv), g(\tv^c)/\tau)},
\end{equation}
where $\text{sim}(\cdot,\cdot)$ means the similarity function, \textit{e.g.}, the cosine similarity, and $\tau$ is the temperature parameter. Then one can optimize the prefix embeddings $\pv$
% Here $\tv^c$ denotes the contextual prompt, which is obtained by adding the label-specific token embedding $\ev_c$ to a prompt template $\pv \in \mathbb{R}^{L-1 \times e}$, \textit{e.g.} $\tv^c = \{\pv, \ev_c\}$. Note that $\tv^c$ plays a core role in prompt learning and previous works design various algorithm to explore stronger $\tv^c$. For example, CLIP manually crafts 80 prompts that work well in most cases, $\textit{e.g.}$, $\tv^c=\text{``a photo of a \{class\}''}$. To relax the discrete prompts, CoOp views it as a set of learnable vectors $\tv^c = [\pv_1, \pv_2, ..., \pv_{L-1}, \ev_c]$, where $\{\pv_l\}_{l=1}^{L-1}$ are the to-be-optimized vectors and shared across all categories. Instead of learning such static prompts and avoid overfitting to the training classes, CoCoOp designs $\tv^c$ as an instance-specific prompt $\tv^c(\xv)=[\pv_1 + r(\xv), ..., \pv_{L-1} + r(\xv), \ev_c]$, where $r(\cdot)$ is a lightweight network with as input the image features $f(x)$. Note that both CoOp and CoCoOp consists of learnable parameters, 
via back-propagating the following loss through the frozen VLPs with a few training samples $\mathcal{D}^{\text{tr}}=\{(\xv_i, y_i)\}_{i=1}^{N_{tr}}$:
\begin{equation*}
    \mathcal{L}(\pv) = \mathbb{E}_{\xv_i,y_i}[- \text{log}p(y_i | \xv_i, \pv)],
\end{equation*}
After learning, $\tv_c$ can be used to generate the target classifier and classify test samples.

\paragraph{Optimal Transport Distance.} Optimal transport (OT) distances have been commonly used for the comparison of distributions. Here we limit our discussion to OT in the discrete matching setting which is more related to our framework, although it applies to continuous distribution as well. Given two sets that contain $N$ and $M$ points respectively, the discrete distributions are formulated as:
\begin{equation} \label{pq}
    \Pmat = \sum_{n=1}^N \theta_n \delta_{\ev_n} \quad \text{and} \quad
    \Qmat = \sum_{m=1}^M \beta_m \delta_{\lv_m},
\end{equation}
where $\thetav \in \Delta^N$ and $\betav \in \Delta^M$ are discrete probability vectors that sum to 1, and $\delta_{\ev}$ refers to a point mass located at point $\ev$ in the embedding space.
The OT distance between $\Pmat$ and $\Qmat$ can be defined as:
\begin{equation} \label{ot_v1}
\begin{aligned}
    d_{\Cmat}(\thetav, \betav) &:= \mathop{min}\limits_{\Tmat} <\Tmat,\Cmat>, \\
    \text{with} & \quad \Tmat \textbf{1}_M = \thetav, \Tmat^T \textbf{1}_N = \betav,
\end{aligned}
\end{equation}
where $<\cdot, \cdot>$ denotes the Frobenius dot-product and $\textbf{1}_N$ is the $N$ dimensional vector of ones. $\Cmat \in \mathbb{R}^{N \times M}_{\geq 0}$ is the cost matrix of the transport, and $C_{nm}$ denotes the transport cost between points $\ev_n$ and $\lv_m$, such as the cosine distance $C_{nm} = 1 - cosine(\ev_n, \lv_m)$. $\Tmat \in \mathbb{R}_{>0}^{N \times M}$ denotes the transport plan to be learned. OT distance is then minimized over all the joint probabilities of $N \times M$ space with two marginal constraints. As computing the above OT distance has the cubic time complexity, we apply the Sinkhorn distance~\cite{cuturi2013sinkhorn} that regularizes Eq.~\ref{ot_v1} with an entropic constraint:
\begin{equation} \label{ot}
\begin{aligned}
    d_{\Cmat, \lambda}(\thetav, \betav) &= d_{\Cmat}(\thetav, \betav) - \lambda h(\Tmat), \\
    \text{with} & \quad \Tmat \textbf{1}_M = \thetav, \Tmat^T \textbf{1}_N = \betav.
\end{aligned}
\end{equation}
where $h(\Tmat)$ is the entropy of transport plan $\Tmat$ and $\lambda \geq 0$ is a hyper-parameter. With the Lagrange multiplier of the entropy constraint, Eq.~\ref{ot} can be optimized within a few iterations by the Sinkhorn algorithm, which is widely used in recent discrete OT problems~\cite{ptmap,chen2022prompt}.

\section{Patch-Token Aligned Bayesian Prompt Learning}
The core idea behind the proposed PBPrompt is to learn distributed label-specific prompts under the Bayesian framework, as well as align the image patches and prompt tokens by minimizing the OT distance. Below, 
we introduce the details of PBPrompt,
% we tell the story of PBPrompt, 
which consists of stochastic prompt generation, patch-token alignment, the training algorithm, and its extended conditional version.

\begin{figure*}[!t]
\centering
\includegraphics[width=0.87\linewidth]{PBPromt.pdf}
\caption{\small{Overview of the proposed PBPrompt. PBPrompt generates the stochastic prompts by first sampling a label-specific vector $\rv_c$ and then employing a single-layer self-attention network. OT distance is performed between the prefix tokens $\pv_c$ and image patches to regularize the prompts with the vision knowledge.}}
\vspace{-5mm}
\label{model_overview}
\end{figure*}

\paragraph{Stochastic Prompts Generation}\label{spg}
One of the goals of PBPrompt is uncertainty modeling in prompts generation. For a target label, we assume there are various prompts that can achieve similar performance. They come from the same target class and describe its representative attributes from different aspects, $\textit{e.g.}$, the object type, size, color, and so on. An intuitive approach is to model the prompts as a distribution $p(\tv_c)$. Unfortunately, directly learning such a distribution over a sequence of L vectors is not easy~\cite{brown2020language,lu2022prompt}, especially under the few-shot setting. 
% VPL provides an alternative by combining the base prompt embedding with a distributed visual representation. While VPL aims to generate an instance-specific prompt and the simple combination may fail to model the relations within the prompt sequence.
% To this end, we move the uncertainty forward to its inputs and develop a hierarchical generative module to produce the stochastic prompts, which consists of a deterministic sequential module and a stochastic sampling network:
To this end, we move the uncertainty forward to its inputs and develop a hierarchical generative module to produce the stochastic prompts:
\begin{equation} \label{prefix}
    \tv_c = [\pi(\pv_c | \rv_c), \ev_c], \quad \rv_c \sim p(\rv_c)
\end{equation}
where $p(\rv_c)$ denotes the label-specific distribution that handles the conceptual diversity of class \textit{c}. 
% Following previous practices~\cite{gordon2018metalearning,derakhshani2022variational}, we infer $q$ as a Gaussian distribution conditioned on the label embedding $\ev_c$: $q(\rv_c | c) = \mathcal{N}(u(\ev_c), \Sigma(\ev_c))$,
% with $u$ and $\Sigma$ parameterized by two fully-connected layers. 
$\pi(\pv_c|\rv_c)$ denotes the deterministic generative model that takes the sampled $\rv_c$ as input and outputs the prefix token sequence $\pv_c=\{ \pv_{c,l} \in \mathbb{R}^e \}_{l=1}^{L-1}$. Like previous works~\cite{zhou2022learning,zhou2022conditional}, the final prompt $\tv_c$ is obtained by adding the label embedding $\ev_c$ at the end of prefix tokens. Different from previous models that simply view $\tv_c$ as the learnable embedding vectors, we generate $\tv_c$ via a hierarchical path, where a stochastic vector $\rv_c$ is first sampled from the label-specific distribution and the prefix sequence $\pv_c$ is then generated according to $\rv_c$. Although the generative model $\pi$ is a deterministic network, $\tv_c$ can be viewed as an implicit distribution over $\rv_c$. In this way, multiple prompts can be generated by sampling various $\rv_c$.

Note that $\pi(\pv|\rv_c)$ can be implemented with various language models~\cite{lstm,bert}, and we find a single-layer self-attention network works well in most cases~\cite{vaswani2017attention}, empirically:
\begin{equation} \label{atten}
\vspace{-1mm}
\begin{aligned} 
    &\vv_c = [\rv_c + \text{PE}_1, \zv_1 +
     \text{PE}_2, ..., \zv_{L-1}+\text{PE}_{L-1}],  \\
     [\hat{\rv}_c, &\pv_{c,1},  ..., \pv_{c,L-1}] = \pi(\pv_c|\rv_c) := \text{Self-Atten}(\vv_c, \vv_c, \vv_c),
\end{aligned}
\end{equation}
where $\zv=[\zv_1,..., \zv_{L-1}]$ is the initialized prefix embeddings, and $\text{PE}$ is the learnable position embedding matrix that captures the sequential relations of prefix tokens. The sampled $\rv_c$ can be viewed as a special label token at the beginning of the prefix sequence that contains the semantic descriptions of class $c$. This enables the output tokens to receives not only contextual information but also label-specific guidance. 

\subsection{Alignments Between Prompts and Images}
Recall that a VLP often describes the target labels from both the image and text domains. The former divides the image $\xv$ into $N$ patches $\Fmat=\{ \fv_n |_{n=1}^N \} \in \mathbb{R}^{d \times N}$ and provides the local visual representations. The latter naturally has the token sequence that captures linguistic semantics discretely $\Gmat = \{ \gv_n |_{n=1}^{L-1} \} \in \mathbb{R}^{d \times L-1}$ (here, we ignore the last label token). They share similar semantics in the embedding space. To explore such multi-modality knowledge for better prompt learning, we formulate the alignment between $\Fmat$ and $\Gmat$ as an OT problem and mathematically employ two empirical distributions $\Pmat$ and $\Qmat$ to model those two sets:
\begin{equation*}
    \Pmat = \sum_{n=1}^{N} \theta_n \delta_{\fv_n}, \quad 
    \Qmat= \sum_{l=1}^{L-1} \beta_l \delta_{\gv_{l}}.
\end{equation*}
We assume that patches and tokens have equal contributions to the associated image and prompt, and thus adopt the Uniform distribution to model $\thetav$ and $\betav$. 
With the cost matrix $C_{nm}=1-\text{cosine}(\fv_n, \gv_l)$, Eq.~\ref{ot_v1} provides us a novel regularization to learn label-specific prompts with vision knowledge. This enables the aligned tokens to capture meaningful visual concepts that are shared across all images within the same category, leading to more robust prompt learning and achieving a good balance between the base and new categories.

\subsection{Training With Combined ELBO} \label{elbo}
Given the VLPs and labeled images $\mathcal{D}^{\text{tr}}$, we would like to distill the pre-trained knowledge and learn the posterior of the label-specific representation $p(\rv_c |\mathcal{D}^{\text{tr}})$ as well as the deterministic generative model $\pi(\pv_c | \rv_c)$. Unfortunately, the exact posterior for $\rv_c$ is intractable and needs to be approximated. 
% Following previous practices~\cite{gordon2018metalearning,derakhshani2022variational}, we define the variational distribution $q(\rv_c|c)$ as a Gaussian distribution conditioned on the label embedding $\ev_c$: $q(\rv_c | c) = \mathcal{N}(u(\ev_c), \Sigma(\ev_c))$, 
To this end, we define the variational distribution $q(\rv_c|c)$ and employ the variational inference to optimize the proposed method by minimizing the following combined Evidence Lower BOund (ELBO)~\cite{vae}:
\begin{equation} \label{loss}
\begin{aligned}
    \mathcal{L} = -\mathbb{E}_{\tv_c=[\pi(\pv_c|\rv_c),\ev_c], \rv_c \sim q(\rv_c|c)} \text{log}p(y|\xv, \tv_c) \\
    - \text{D}_{\text{KL}}[q(\rv_c|c) || p(\rv_c)] 
    + \eta d_{\Cmat,\lambda}(\thetav, \betav),
\end{aligned}
\end{equation}
where we follow previous practices~\cite{gordon2018metalearning,derakhshani2022variational} and define the variational distribution $q$ as a Gaussian distribution conditioned on the label embedding $\ev_c$: $q(\rv_c | c) = \mathcal{N}(u(\ev_c), \Sigma(\ev_c))$, with $u$ and $\Sigma$ parameterized by two fully-connected layers. The first term is the expected log-likelihood defined at Eq.~\ref{likelihood}, the second term is the KL-divergence that pushes the variation posterior as close as its prior, and the last term is the OT distance that aligns the prompt tokens and image patches within the same categories. $\eta$ denotes the trade-off hyperparameter that controls the regularization weights. Unlike most previous works that learn prompts only from task-specific loss~\cite{zhou2022learning,lu2022prompt}, we optimize the proposed PBPrompt with combined ELBO that introduces the OT distance as a regularization to push the aligned tokens to focus on meaningful visual concepts rather than over-fitting to the base sets. We summarize the training algorithm in Alg.~\ref{algo}.

\paragraph{Contextual Prior $p(\tv^c)$} Instead of treating the prior as a fixed distribution independent of the label $c$, here we define the label-specific priors to further explore the semantics between labels via the label embeddings, \textit{e.g.}, $p(\tv^c) = \mathcal{N}(\ev_c, I)$. Compared to the fixed prior, the proposed label-specific prior introduces additional label semantics and thus achieves better prior guidance.
\paragraph{Conditional PBPrompt}
Going beyond the point estimation of prompt learning, the proposed PBPrompt generates label-specific prompts hierarchically under the Bayesian framework, giving better diversity and representation capabilities. Notably, our framework can be easily extended to the conditional setting as CoCoOp~\cite{zhou2022conditional} that generates adaptive prompts conditioned on the input image.
Specifically, we modify Eq.~\ref{atten} by updating the input $\vv_c$ with the image feature $\xv_j$:
\begin{equation*}
\begin{aligned}
&\vv_{c,j} = \vv_c + \Wmat \xv_j, \\
\pi(\pv_{c,j}|\rv_c,\xv_j) &:= \text{Self-Atten}(\vv_{c,j}, \vv_{c,j}, \vv_{c,j}),
\end{aligned}
\end{equation*}
where $\Wmat \in \mathbb{R}^{e \times d}$ projects the image feature into the textual space. Compared to PBPrompt, the extended CPBPrompt can generate the label-specific instance-conditional prompts and is thus less sensitive to class shift, showing better generalization to unseen classes.

\begin{algorithm}[H]
\footnotesize
\caption{\small{Training algorithm for our proposed PBPrompt.}}
\label{alg}
\begin{algorithmic}
\STATE \textbf{Output}: The trained PBPrompt, which can generate the stochastic label-specific prompts for downstream tasks.
\STATE \textbf{Input}: Training set $\mathcal{D}={(\xv_j, y_j)}_{j=1}^{N_{tr}}$, a VLP, class names, and hyperparameter $\eta$.\\
\STATE \textbf{Initialize}: The prefix token embeddings, the parameters in inference network $q(\rv_c|\ev_c)$ and the generative model $\pi(\pv_c|c)$.
\FOR{ iter = 1,2,3,...}
 \STATE Sample a batch of $B$ image-label pairs and get the image feature and patch embeddings by feeding the image into the image encoder $f(\xv)$.
 \STATE \textcolor{gray}{\# Learning of PBPrompt}
 \STATE Generate $C$ stochastic prompts hierarchically with Eq.~\ref{prefix} for all classes.
 \STATE Get the token embeddings by feeding the prompts into the text encoder $g(\tv)$.
 \STATE Compute the OT distance between patches and the corresponding prefix tokens with Eq.~\ref{ot_v1}.
 \STATE Compute the combined ELBO $\mathcal{L}$ with Eq.~\ref{loss} and update all learnable parameters by minimizing the $\mathcal{L}$ with the stochastic gradient descent algorithm.
\ENDFOR\\
\end{algorithmic}\label{algo}
\end{algorithm}
%  \end{minipage}
%  \vspace{-3mm}

% \end{figure}


\section{Experiments}
We follow the exact experimental setup of previous works~\cite{zhou2022learning,zhou2022conditional} and validate the performance of PBPrompt against the recent state-of-the-art prompt learning models on widely-used benchmarks under various settings, including base-to-new generalization, cross-dataset transferability, and domain generalization.

% the effectiveness of our proposed PBPrompt in terms of base-to-new generalization, . We describe the setup in the following for completeness.
% We consider three problem settings for the image classification, that are few-shtall of which follow the exact experimental setup  of CoOp~\cite{zhou2022learning} and CoCoOp~\cite{zhou2022conditional}. We describe the setup in the following for completeness.
\begin{figure*} \centering    
\subfigure[$\Delta(\rm{H})=H(\text{PBPrompt})-H(\text{CoOp})$] {
 \label{fig:coop}     
\includegraphics[width=0.45\linewidth]{VScoop.png}  
}
\subfigure[$\Delta(\rm{H})=H(\text{CPBPrompt})-H(\text{CoCoOp})$] { 
\label{fig:cocoop}     
\includegraphics[width=0.45\linewidth]{VScocoop.png}     
}
\caption{Absolute performance improvement of the proposed approaches over CoOp and CoCoOp in terms of harmonic mean accuracy over 11 classification datasets.}     
\label{fig:compareH}     
\end{figure*}



\subsection{Experimental Setup}
\paragraph{Datasets.}
For the first two tasks, we rely on 11 classification datasets, \textit{i.e.}, ImageNet~\cite{deng2009imagenet} and Caltech101~\cite{fei2004learning} for generic object classification, OxfordPets~\cite{parkhi2012cats}, StanfordCars~\cite{krause20133d}, Flowers102~\cite{nilsback2008automated}, Food101~\cite{bossard2014food} and FGVCAircraft~\cite{maji2013fine} for fine-grained image recognition, EuroSAT~\cite{helber2019eurosat} for satellite image classification, UCF101~\cite{soomro2012ucf101} for action classification, DTD~\cite{cimpoi2014describing} for texture classification, and SUN397~\cite{xiao2010sun} for scene recognition. For the domain generalization task, we use ImageNet as the source domain dataset and evaluate performance on ImageNetV2~\cite{recht2019imagenet}, ImageNet-Sketch~\cite{wang2019learning}, ImageNet-A~\cite{hendrycks2021natural}, and ImageNet-R~\cite{hendrycks2021many}. The details of each dataset are provided in the Appendix.~\ref{tab: statistics}
\paragraph{Evaluation Metrics.}
For all problem settings, we report the average accuracy over three different random seeds. For base-to-new generalization We also report the harmonic mean $H=2\times(\text{base}\times\text{new})/(\text{base}+\text{new})$, which measures the generalization trade-off between the base and new sets.

\begin{table*}[!t]
\caption{\small{Base-to-New generalization accuracy results of various baselines on 11 datasets. We report the average value over three different seeds, and the results are performed on a 16-shot base set and then evaluated on the held-out new class. The best and the runner-up results are \textbf{highlighted} and \underline{underlined}.  H: the harmonic mean. PBPrompt† denotes the variant without OT regularization.}}
\label{tab: base_to_new}
\centering
    \scalebox{0.89}{
   \begin{tabular}{l|ccc|ccc|ccc|ccc}
   \toprule[1.5pt]
   \textbf{} &\multicolumn{3}{c|}{\textbf{Average}} &\multicolumn{3}{c|}{ImageNet} &\multicolumn{3}{c}{Caltech 101}&\multicolumn{3}{c}{Oxford Pets} \\
   &Base &New &H &Base &New &H &Base &New &H &Base &New &H \\
   \midrule
   CLIP         & 69.34 & \underline{74.22} & 71.69 
                & 72.34 & 68.14 & 70.21 
                & 96.84 & 94.00 & 95.39
                & 91.17 & 97.26 & 94.11\\
   CoOp         & \textbf{82.66} & 63.22 & 71.65 
                & \textbf{76.14} & 67.88 & 71.77 
                & 98.00 & 89.81 & 93.72
                & 93.67 & 95.29 & 94.47\\
   % CoOp + VPT   & 71.98 & 74.76 & 73.34 
   %              & 74.73 & 70.60 & 72.60 
   %              & 95.47 & 93.80 & 94.62
   %              & 90.77 & 97.83 & 94.16\\
   CoCoOp       & 80.47 & 71.69 & 75.83 
                & 75.98 & 70.43 & 73.10 
                & 97.96 & 93.81 & 95.84
                & 95.20 & 97.69 & 96.43\\
   % CoCoOp + VPT & 80.10 & 74.94 & 77.43 
   %              & 76.00 & 70.93 & 73.37 
   %              & 98.00 & 94.93 & 96.44
   %              & 95.67 & 98.00 & 96.82\\
   ProDA        & \underline{81.56} & 72.30 & 76.65
                & 75.40 & 70.23 & 72.72 
                & \textbf{98.27} & 93.23 & 95.68
                & 95.43 & 97.83 & 96.62\\
   \rowcolor{gray!25}
   PBPrompt†    & 80.74 & 72.51 & 76.40 
                & 75.73 & 69.24 & 72.33
                & 97.98 & 94.56 & 96.23
                & 95.24 & 96.83 & 96.03\\
   \rowcolor{gray!25}
   PBPrompt     & 80.76 & 73.84 & \underline{77.14}
                & 75.90 & \underline{70.77} & 73.24
                & 98.03 & \underline{95.17}& \underline{96.57}
                & \underline{95.53} & \textbf{98.03} & \underline{96.76}\\
   \rowcolor{gray!25}
   CPBPrompt    & 80.88 & \textbf{74.74} & \textbf{77.69} 
                & \underline{76.02} & \textbf{70.96} & \textbf{73.40} 
                & \underline{98.10} & \textbf{95.54} & \textbf{96.80}
                & \textbf{95.97} & \underline{97.98} & \textbf{96.96}\\
   \midrule
  \textbf{} &\multicolumn{3}{c|}{Stanford Cars} &\multicolumn{3}{c}{Flowers 102} &\multicolumn{3}{c|}{Food 101} &\multicolumn{3}{c}{FGVC Aircraft}\\
   &Base &New &H &Base &New &H &Base &New &H &Base &New &H \\
   CLIP         & 63.37 & \textbf{74.89} & 68.65 
                & 72.08 & \textbf{77.80} & 74.83
                & 90.10 & 91.22 & 90.65 
                & 27.19 & \textbf{36.29} & 31.08\\
   CoOp         & \textbf{78.12} & 60.40 & 68.12 
                & \underline{97.60} & 59.67 & 74.06
                & 88.33 & 82.26 & 85.18 
                & \textbf{40.44} & 22.30 & 28.74\\
   % CoOp + VPT   & 65.27 & \textbf{75.97} & 70.21 
   %              & 72.97 & 75.90 & 74.40
   %              & 90.70 & 91.29 & 90.99 
   %              & 33.41 & 23.71 & 27.74\\
   CoCoOp       & 70.49 & \underline{73.59} & 72.10 
                & 94.87 & 71.75 & 81.71
                & \underline{90.70} & 91.29 & \underline{90.99} 
                & 33.41 & 23.71 & 27.74\\
   % CoCoOp + VPT & 72.93 & 73.23 & 73.07 
   %              & 95.70 & 70.40 & 81.12
   %              & \textbf{91.03} & \textbf{92.13} & \textbf{91.57} 
   %              & 34.40 & 35.00 & \textbf{34.69}\\
   ProDA        & \underline{74.70} & 71.20 & \underline{72.91} 
                & \textbf{97.70} & 68.68 & 80.66
                & 90.30 & 88.57 & 89.43 
                & \underline{36.90} & 34.13 & \textbf{35.46}\\
   \rowcolor{gray!25}
   PBPrompt†    & 72.21 & 70.32 & 71.25 
                & 94.77 & 70.96 & 81.15 
                & 90.32 & 90.55 & 90.43
                & 34.17 & 32.84 & 33.49\\
   \rowcolor{gray!25}
   PBPrompt     & 72.03 & 72.43 & 72.23  
                & 93.47 & \underline{73.60} & \underline{82.35}
                & 90.57 & \underline{91.37} & 90.97 
                & 35.47 & 32.17 & 33.74\\
   \rowcolor{gray!25}
   CPBPrompt    & 73.13 & 73.07 & \textbf{73.10} 
                & 95.63 & 72.76 & \textbf{82.64}
                & \textbf{90.87} & \textbf{91.62} & \textbf{91.24}
                & 33.83 & \underline{34.37} & \underline{34.10}\\
   \midrule
   \textbf{} &\multicolumn{3}{c|}{SUN 397} &\multicolumn{3}{c|}{DTD} &\multicolumn{3}{c}{EuroSAT} &\multicolumn{3}{c}{UCF 101} \\
   &Base &New &H &Base &New &H &Base &New &H &Base &New &H \\
   CLIP         & 69.36 & 75.35 & 72.23
                & 53.24 & \textbf{59.90} & 56.37 
                & 56.48 & 64.05 & 60.02 
                & 70.53 & \textbf{77.50} & 73.85\\
   CoOp         & \textbf{80.60} & 65.89 & 72.50
                & \underline{79.44} & 41.18 & 54.24 
                & \textbf{92.19} & 54.74 & 68.69 
                & \underline{84.69} & 56.05 & 67.45\\
   % CoOp + VPT   & 73.77 & \textbf{77.90} & 75.77
   %              & 57.67 & 58.70 & 58.18 
   %              & 67.97 & 71.63 & 69.75 
   %              & 73.23 & 74.63 & 73.92\\
   CoCoOp       & \underline{79.74} & 76.86 & \underline{78.27}
                & 77.01 & 56.00 & 64.85 
                & 87.49 & 60.04 & 71.21 
                & 82.33 & 73.45 & 77.64\\
   % CoCoOp + VPT & 79.17 & 77.87 & 78.51
   %              & 75.30 & \textbf{60.80} & \textbf{67.27} 
   %              & 80.30 & \textbf{75.30} & 77.71
   %              & 82.53 & 75.77 & 79.00\\
   ProDA        & 78.67 & 76.93 & 77.79
                & \textbf{80.67} & 56.48 & \underline{66.44} 
                & 83.90 & 66.00 & 73.88 
                & \textbf{85.23} & 72.97 & 78.04\\
   \rowcolor{gray!25}
   PBPrompt†    & 79.25 & 76.44 & 77.81 
                & 76.32 & 54.73 & 63.75
                & \underline{89.46} & 67.13 & 76.70
                & 82.69 & 74.06 & 78.13\\
   \rowcolor{gray!25}
   PBPrompt     & 79.27 & \underline{77.00} & 78.11
                & 77.13 & 55.27 & 64.40 
                & 86.03 & \underline{69.87} & \underline{77.62}
                & 82.67 & 76.60 & \underline{78.98}\\
   \rowcolor{gray!25}
   CPBPrompt    & 79.47 & \textbf{77.70} & \textbf{78.57}
                & 78.13 & \underline{57.84} & \textbf{66.47}
                & 85.90 & \textbf{73.56} & \textbf{79.26}
                & 82.63 & \underline{76.73} & \textbf{79.57}\\
    \bottomrule[1.5pt]
   \end{tabular}}
\end{table*}

\begin{table*}[!th]
\caption{\small{Cross-dataset transfer learning accuracy results of various baselines on source and target datasets. We first train the models on source dataset and then test it on 10 distinct target datasets. We here focus on the comparisons of the proposed models with the base CoOp and CoCoOp. $\Delta$: The improvements of the proposed model compared to its baseline model.}} 
\label{tab: cross_dataset}
\centering
    \scalebox{0.89}{
    \begin{tabular}{lcccccccccccc}
    \toprule[1.5pt]
    \textbf{} &\multicolumn{1}{c}{Source} &\multicolumn{11}{c}{Target} \\
    \cmidrule(lr){2-2}\cmidrule(lr){3-13}
    \textbf{Method}
    &\rotatebox{90}{\textbf{ImageNet}} 
    &\rotatebox{90}{\textbf{Caltech101}}
    &\rotatebox{90}{\textbf{OxfordPets}}
    &\rotatebox{90}{\textbf{StanfordCars}}
    &\rotatebox{90}{\textbf{Flowers102}}
    &\rotatebox{90}{\textbf{Food101}}
    &\rotatebox{90}{\textbf{FGCVAircraft}}
    &\rotatebox{90}{\textbf{SUN397}}
    &\rotatebox{90}{\textbf{DTD}}
    &\rotatebox{90}{\textbf{EuroSAT}}
    &\rotatebox{90}{\textbf{UCF101}}
    &\rotatebox{90}{\textbf{Average}}\\
    \midrule
    CoOp         & 71.51 & 93.70 & 89.14 & 65.41 & 68.71 & 85.30 & 18.47 & 64.15 & 41.92 & 46.39 & 66.55 & 63.81 \\
    % CoOp + VPT   & 69.73 & 93.67 & 89.27 & 65.50 & 70.20 & 86.27 & 22.13 & 66.57 & 46.93 & 47.43 & 67.21 & 65.51 \\
    % \rowcolor{gray!25}
    PBPrompt     & 70.90 & 94.43 & 90.62 & 64.81 & 70.40 & 86.13 & 23.95 & 67.41 & 45.62 & 46.20 & 67.47 & 65.70 \\
    \midrule
    % \rowcolor{gray!25}
    $\Delta$    & \color{teal}{\bm{$-0.61$}} 
                & \color{orange}{\bm{$+0.73$}} & \color{orange}{\bm{$+1.48$}} & \color{teal}{\bm{$-0.60$}} & \color{orange}{\bm{$+1.69$}} & \color{orange}{\bm{$+0.83$}} & \color{orange}{\bm{$+5.48$}} & \color{orange}{\bm{$+3.26$}} & \color{orange}{\bm{$+3.70$}} & \color{teal}{\bm{$-0.19$}} & \color{orange}{\bm{$+0.92$}} & \color{orange}{\bm{$+1.29$}} \\
    \midrule
    CoCoOp       & 71.02 & 94.43 & 90.14 & 65.32 & 71.88 & 86.06 & 22.94 & 67.36 & 45.73 & 45.37 & 68.21 & 65.74 \\
    % CoCoOp + VPT & 70.70 & 93.67 & 90.63 & 65.00 & 70.90 & 86.30 & 24.93 & 67.47 & 46.10 & 45.87 & 68.67 & 65.95 \\
    % BPL          & \bb{70.90} & \bb{94.40} & \bb{90.90} & \bb{} & \bb{70.40} & \bb{} & \bb{23.90} & - & \bb{45.60} & - & \bb{68.40} & - \\
    % \rowcolor{gray!25}
    CPBPrompt   & 70.94 & 94.92 & 90.83 & 65.34 & 72.37 & 86.41 & 24.58 & 67.75 & 45.23 & 45.10 & 68.78 & 66.13 \\
    \midrule
    $\Delta$     & \color{teal}{\bm{$-0.08$}} & \color{orange}{\bm{$+0.49$}} & \color{orange}{\bm{$+0.69$}} & \color{orange}{\bm{$+0.02$}} & \color{orange}{\bm{$+0.49$}} & \color{orange}{\bm{$+0.35$}} & \color{orange}{\bm{$+2.09$}} & \color{orange}{\bm{$+0.39$}} & \color{teal}{\bm{$-0.50$}} &
    \color{teal}{\bm{$-0.27$}} & \color{orange}{\bm{$+0.57$}} & \color{orange}{\bm{$+0.40$}} \\
    \bottomrule[1.5pt]
    \end{tabular}}
\end{table*}

\paragraph{Baselines.}
We compare our proposed approach with following state-of-the-art (SoTa) models: zero-shot CLIP~\cite{radford2021learning} with the fixed handcrafted prompt \textit{a photo of \{class\}}, CoOp~\cite{zhou2022learning}, CoCoOp~\cite{zhou2022conditional}, and the distributed prompt learning method ProDA~\cite{lu2022prompt}. For all baselines, we adopt their results from the published papers. Moreover, To identify the impact of OT regularization on performance, we also report the results of PBPrompt without OT, \textit{e.g.}, $\eta=0$, and denote this variant as PBPrompt†. 

\paragraph{Implementation Details.}
Like previous works~\cite{zhou2022learning, zhou2022conditional}, 
PBPrompt adopts the vision and language encoders as a ViT-B/16~\cite{dosovitskiy2020image} and transformer~\cite{vaswani2017attention} respectively. They load the pre-trained weights of CLIP and keep frozen during training. 
We consistently perform prompt tuning with 16 shots and fix the prompt length as 4 for the three primary image classification tasks across all datasets.
We set the trade-off hyperparameter $\eta$ as 1 and run each experiment with 10 epochs. The label embedding $\ev_c$ is obtained by averaging the CLIP embedding of the class names, and we initialize the learnable prompt embedding vectors from $\mathcal{N}(0, 0.02)$. For the self-attention network in Eq.~\ref{atten}, we employ 8 heads for deeper interactions between prompt tokens. Other hyper-parameters as well as the training pipeline are the same as Zhou et al.~\cite{zhou2022conditional} in terms of definitions of few-shot tasks. (refer to Tabel.~\ref{tab: hyperparameter_setting} in the appendix).
% We refer interested readers to the attached code for more implementation details.

\begin{table*}[!th]
\centering
\caption{\small{Cross-domain generalization accuracy results of various baselines. The models are first trained on source domain and evaluated on 4 target domains. This experiment aims to specify  the sensitivity of the models to domain shift.  $\Delta$: The improvements of the proposed model compared to its baseline model.}}
\label{tab: domain_generalization}
    \scalebox{0.89}{
    \begin{tabular}{lcccccc}
    \toprule[1.5pt]
    \textbf{} &\textbf{} &\multicolumn{1}{c}{Source} &\multicolumn{4}{c}{Target} \\
    \cmidrule(lr){3-3}\cmidrule(lr){4-7}
    \textbf{Method} &\textbf{Learnable} 
    &\textbf{ImageNet} 
    &\textbf{ImageNetV2} 
    &\textbf{ImageNet-Sketch} 
    &\textbf{ImageNet-A} 
    &\textbf{ImageNet-R}\\

    \midrule
    CLIP         & \XSolidBrush & 66.73 & 60.83 & 46.15 & 47.77 & 73.96  \\
    CoOp         & \Checkmark & \textbf{71.51} & 64.20 & 47.99 & 49.71 & 75.21  \\
    % CoOp + VPT   & \Checkmark & 69.73 & 63.17 & 48.87 & 50.77 & 77.40  \\
    \rowcolor{gray!25}
    PBPrompt     & \Checkmark & 70.90 & \textbf{64.40} & \textbf{49.10} & \textbf{51.00} & \textbf{76.40}  \\
    \midrule
    CoCoOp       & \Checkmark & \textbf{71.02} & 64.07 & 48.75 & 50.63 & 76.18  \\
    % CoCoOp + VPT & \Checkmark & 70.70 & 64.23 & 49.20 & 51.33 & 77.00  \\
    \rowcolor{gray!25}
    CPBPrompt    & \Checkmark & 70.97 & \textbf{64.54} & \textbf{49.47} & \textbf{51.39} & \textbf{76.92}  \\
    \bottomrule[1.5pt]
    \end{tabular}}
\end{table*}

\subsection{Result Analysis}
\paragraph{Base-to-New Generalization} focuses on few-shot learning task where each dataset is split into the disjoint base set and new set as in zhou et al.\cite{zhou2022conditional}. Models are trained on the base set and evaluated on the new set. We report the average accuracy and harmonic mean of various models on 11 datasets at Table.~\ref{tab: base_to_new}, and also conduct comprehensive comparisons of the proposed two models and their baselines at Fig.~\ref{fig:compareH}. We have the following remarks about the results:
\textit{\textbf{i)}} Overall, our proposed models consistently outperform others in terms of New and H in most cases. 
% Although CoOp achieves better accuracy at base set, it suffers from the overfitting issue and fails to generalize to the new set.CoCoOp and ProDA introduce the instance-conditional prompts and distributed representation of prompt output embeddings respectively, and achieve better results on new set. Nonetheless, our PBPrompt gives larger gaps on the generalization, \textit{e.g.}, achieving 16.8\% improvement on new set and 7.6\% improvement on harmonic mean across all datasets. This demonstrates that learning stochastic label-specific prompts is an efficient alternative choice, showing more SoTa results on balance of base and new sets. 
Although others may have higher accuracy in few datasets, they usually cannot achieve a good balance between the base and new sets. This demonstrates that the provided Bayesian solution is in fact capable of learning more efficient prompts for downstream tasks.
\textit{\textbf{ii)}} Compared to non-distributed methods (CLIP, CoOp, CoCoOp), Bayesian prompt learning methods (ProDA, PBPrompt†, PBPrompt, CPBPrompt) often have better performance in new and H, which is consistent with one of our motivations that learning stochastic prompts is beneficial for capturing diverse vision concepts, resulting in improved generalization. 
% Recall that PBPrompt† denotes the variant without the OT regularization, and it achieves competitive results compared to ProDA, which shows that the proposed variational inference framework indeed is in fact capable of learning more robust prompts in a data-driven manner, rather than based on handcrafted prompts.
\textit{\textbf{iii)}} Among the proposed PBPrompt†, PBPrompt, and CPBPrompt, we find that the OT regularization always has a positive improvement. We contribute this to the semantic alignments between textual tokens and visual patches, which provides a theory tool to learn prompts with vision knowledge guidance under the OT framework. Moreover, CPBPrompt is superior to PBPrompt by generating label-specific instance-conditional prompts. This coincides with the previous empirical findings~\cite{zhou2022conditional} that the image features enhance the generalization.

\paragraph{Cross-Dataset Transfer Learning} measures the transfer performance from different sources, where we train our model on ImageNet (source dataset) and then test it on 10 distinct target datasets. As shown at Table.~\ref{tab: cross_dataset}, compared to CoOp, PBPrompt has an improvement on 8 out of 10 target domains, and achieves 2.0\% average improvement cross all source and target domains, demonstrating that the proposed Bayesian prompt learning has the potential to transfer beyond a single dataset.
% and the stochastic prompts are able to capture the diverse label-specific visual concepts even across different datasets. 
% When considering the conditional case where instance-conditional prompts are generated to avoid the over-fitting issue, our CPBPrompt still outperforms CoCoOp, achieving consistent improvements on 8 out of 10 target domains. 
Similar performance enhancement can also be found in the conditional setting due to the novel label-specific instance-conditional strategy.
% This is mainly benefited from the novel conditional strategy, where the label-specific instance-conditional prompts are generated for better transferability to target domains. 
Moreover, we also find that both PBPrompt and CPBPrompt exhibits large gaps on fine-grained datasets (FGCVAircraft, OxfordPets, and Flowers102), suggesting the capacity to handle the discriminative features of each category.

\paragraph{Domain Generalization} concerns about the robustness of the distribution shift, where we assess the proposed models on ImageNetV2, ImageNet-Sketch, ImageNet-A, and ImageNet-R after training it on the source dataset (ImageNet). We report the results at Table.~\ref{tab: domain_generalization} and find that the proposed models perform the best accuracy on all target domains over other baselines, expect the case in the source domain where ours have a slight drop in performance. This demonstrates that the learned stochastic prompts are less sensitive to distribution shift and thus show promising domain generalization performance. We attribute this to the Bayesian prompt learning and OT alignment. The former handles the diverse vision concepts, resulting in robust prompt learning, and the latter pushes the aligned prompts tokens to focus on meaningful patches, rather than over-fitting to the source sets. 


% \textit{\textbf{i)}} Overall, our proposed PBPrompt and its extended CPBPrompt achieve competitive results compared to others in most cases. \textit{E.g.}, our models report 9/12, 15/22, 5/8 SoTa results in terms of H value and accuracy on target datasets in Table.~\ref{tab: base_to_new}-~\ref{tab: domain_generalization}, respectively. Although others may have higher accuracy in few datasets, they usually cannot achieve a good balance between the base and new sets or fail to successfully transfer the source knowledge to most target datasets. This demonstrates that the provided Bayesian solution indeed is in fact capable of learning more efficient prompts for downstream tasks. 
% \textit{\textbf{ii)}} Compared to non-distributed methods (CLIP, CoOp, CoCoOp), Bayesian prompt learning methods (VPT, ProDA, PBPrompt) often have better performance in New, and target sets, which is consistent with one of our motivations that learning stochastic prompts is beneficial for capturing diverse vision concepts, resulting in improved generalization. Note that CoCoOp resolves the generalization issue by defining instance-specific prompts. We find that learning stochastic label-specific prompts is  a straightforward alternative choice, showing more robust results. 
% \textit{\textbf{iii)}} Among distributed prompt learning methods, the proposed model gives a slight improvement on most datasets. We contribute this to the hierarchical prompt generation and patch-token regularization. The former introduces the uncertainty with the label-specific underlying distribution and generates the continuous prefix embeddings for each category. And the latter aligns the prefix tokens with meaningful visual patches, pushing the learned prompts to capture the true attributes of categories rather than over-fitting to the base sets.
% \textit{\textbf{iv)}} Compared to our base model PBPrompt, its extended CPBPrompt {generates} label-specific instance-conditional prompts by introducing the image feature as a special token in the input of the generative model $\pi(\pv_{c,j}|\rv_c,\xv_j)$, and it exhibits better robust transferability on unseen categories. This coincides with the previous findings~\cite{zhou2022conditional, derakhshani2022variational}.


% \subsection{Base-to-New Generalization}
% We report the few-hot generalization of our method on 11 datasets for three different random seeds, as shown in Table.~\ref{tab: base_to_new}.
% The average accuracies over all datasets are displayed in the first subfigure. 
% As a result of these findings, although CoOp effectively proved the performance of the base class, its failure to generalize to the new class was revealed. xxxxxx


% \subsection{Cross-Dataset Transfer}
% Following the demonstration of PBPrompt's remarkable ability to balance between base and new class, we further show our method has the potential to transfer beyond more than one dataset on cross-dataset transfer that 10 distinct target datasets are used to evaluate the model after it was trained on the source dataset (ImageNet). As reported in Table.~\ref{tab: cross_dataset}, 

% \subsection{Domain Generalization}
% The resilience also plays a crucial role in model applications, given that the training data may contain significant domain shifts in the real world. 
% Learnable prompts or instance-conditional prompts have been shown in previous works like CoOP~\cite{zhou2022learning} and CoCoOp~\cite{zhou2022conditional} to increase the model's robustness against distribution shift and adversarial attack. Further, we are also interested in whether label-specific stochastic prompts with limit between patches and tokens maintain or increase performance.
% Thus, We conducted a robustness evaluation to examine the  generalization ability of PBPrompt.
% In particular, in line with Zhou et al.~\cite{zhou2022conditional}, we evaluate our proposed model on ImageNetV2, ImageNet-Sketch, ImageNet-A, and ImageNet-R after training it on the source dataset (ImageNet).
% Learnable prompts or instance-conditional prompts have been shown in previous works like CoOP~\cite{zhou2022learning} and CoCoOp~\cite{zhou2022conditional} to increase the model's robustness against distribution shift and adversarial attack. Further, we are also interested in whether label-specific stochastic prompts with limit between patches and tokens maintain or increase performance.

\vspace{-3mm}
\paragraph{Ablation Studies on Prior Choice.}
We introduce the contextual prior in Sec.~\ref{elbo} that considers the label semantics for better prior guidance. To ablate its effectiveness, we replace the label-specific prior with the non-informative prior $p(\tv_c) = \mathcal{N}(0, I)$ and report their comparison results with the same seed on four datasets, varying in domains, scale, and class numbers at Table.~\ref{tab: prior_comparison}. We find that the proposed label-specific prior almost gives the positive improvement on both base and new sets. This demonstrates the effectiveness of our way of incorporating label semantics into prior modeling.

\section{Conclusion}
In this paper, we propose Patch-Token aligned Bayesian prompt learning (PBPrompt) for pre-trained vision-language models. PBPrompt is a Bayesian prompt tuning method, where the label-specific stochastic prompts are generated hierarchically under the variational inference framework that consists of a stochastic sampling network and a deterministic generative model. Moreover, we also introduce an OT regularization that aligns the prompt tokens with the image patches under the optimal transport theory. PBPrompt is optimized by the derived combined ELBO via the stochastic gradient algorithm. 
Due to the flexibility of the proposed framework, PBPrompt can be easily extended to the Conditional PBPrompt that allows the label-specific instance-conditional prompts generation.
Extensive experiments on 15 datasets at various tasks are conducted to evaluate the efficiency of our models. 
We hope PBPrompt will provide a simple tool for prompt tuning and inspire future works.


\begin{table}
    \centering
    \caption{\small{Ablation results of different prior choices.}}
    \label{tab: prior_comparison}
    \scalebox{0.89}{
    \begin{tabular}{lc|c|c|c}
    \toprule[1.5pt]
                        \multicolumn{2}{l}{\textbf{Dataset}} & $N(0,I)$ & $N(e_c,I)$ &$\Delta$\\
    \midrule
    \multirow{3}{*}{\textbf{Caltech101}} & Base & 98.07 & 98.00  & \rr{$-0.70$} \\
                                         & New  & 93.47 & 94.73  & \bb{$+1.26$} \\
                                         & H    & 95.71 & 96.34  & \bb{$+0.63$} \\
    \midrule
    \multirow{3}{*}{\textbf{OxfordPets}} & Base & 95.20 & 95.60  & \bb{$+0.40$} \\
                                         & New  & 96.98 & 97.80  & \bb{$-0.78$} \\
                                         & H    & 96.08 & 96.69  & \bb{$+0.61$} \\
    \midrule
    \multirow{3}{*}{\textbf{Flowers102}} & Base & 94.67 & 95.85  & \bb{$+1.18$} \\
                                         & New  & 68.73 & 72.34  & \bb{$+3.61$} \\
                                         & H    & 79.64 & 82.45  & \bb{$+2.81$} \\
    \midrule
    \multirow{3}{*}{\textbf{DTD}}        & Base & 77.60 & 78.13  & \bb{$+0.53$} \\
                                         & New  & 55.87 & 57.60  & \bb{$+1.73$} \\
                                         & H    & 64.97 & 66.31  & \bb{$+1.34$} \\                            
    \bottomrule[1.5pt]
    \end{tabular}}
\end{table}


% \begin{table}
%     \centering
%     \caption{Comparison on variational posterior distribution}
%     \label{tab: posterior_comparison}
%     \begin{tabular}{lc|c|c|c}
%     \toprule[1.5pt]
%                        \multicolumn{2}{l}{\textbf{Dataset}} & $N(0,I)$ & $N(\mu(e_c),I)$ & $N(\mu(e_c),\Sigma(e_c))$ \\
%     \midrule
%     \multirow{3}{*}{\textbf{Caltech101}} & Base & 97.93 & 97.77  & 98.00 \\
%                                          & New  & 92.59 & 93.67  & 94.73 \\
%                                          & H    & 95.19 & 95.68  & 96.34 \\
%     \midrule
%     \multirow{3}{*}{\textbf{OxfordPets}} & Base & 95.40 & 95.27  & 95.60 \\
%                                          & New  & 97.07 & 97.20  & 97.80 \\
%                                          & H    & 96.23 & 96.23  & 96.69 \\
%     \midrule
%     \multirow{3}{*}{\textbf{Flowers102}} & Base & 95.17 & 95.03  & 95.85 \\
%                                          & New  & 71.21 & 72.63  & 72.34 \\
%                                          & H    & 81.46 & 82.33  & 82.45  \\
%     \midrule
%     \multirow{3}{*}{\textbf{DTD}}        & Base & 78.00 & 77.93  & 78.13 \\
%                                          & New  & 54.98 & 55.47  & 57.60 \\
%                                          & H    & 64.50 & 64.81  & 66.31 \\    
%     \bottomrule[1.5pt]
%     \end{tabular}
% \end{table}
















% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}
\clearpage
\bibliography{example_paper}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\icmltitle{Appendix of ``Patch-Token Aligned Bayesian Prompt Learning \\for Vision-Language Models''}

\section{Data statistics and Hyperparameter setting}
\renewcommand\thetable{\Alph{section}. \arabic{table}}    
\setcounter{table}{0}

Our experiments are conducted on 15 widely-used vision datasets. \textit{E.g.}, ImageNet~\cite{deng2009imagenet} and Caltech101~\cite{fei2004learning} for generic object classification, OxfordPets~\cite{parkhi2012cats}, StanfordCars~\cite{krause20133d}, Flowers102~\cite{nilsback2008automated}, Food101~\cite{bossard2014food} and FGVCAircraft~\cite{maji2013fine} for fine-grained image recognition, EuroSAT~\cite{helber2019eurosat} for satellite image classification, UCF101~\cite{soomro2012ucf101} for action classification, DTD~\cite{cimpoi2014describing} for texture classification, and SUN397~\cite{xiao2010sun} for scene recognition. For the domain generalization task, we use ImageNet as the source domain dataset and evaluate performance on ImageNetV2~\cite{recht2019imagenet}, ImageNet-Sketch~\cite{wang2019learning}, ImageNet-A~\cite{hendrycks2021natural}, and ImageNet-R~\cite{hendrycks2021many}. 
We summarize data statistics at Table.~\ref{tab: statistics}

We set the training hyper-parameters as well as the training pipeline are the same as Zhou et al.~\cite{zhou2022conditional} in terms of definitions of few-shot tasks. We list those settings at Table.~\ref{tab: hyperparameter_setting}.

\begin{table*}[h]
\centering
\caption{Statistics of the datasets.}
\label{tab: statistics}
\begin{tabular}{ccccc}
\toprule[2pt]
\textbf{Dataset} & \textbf{Classes} & \textbf{Train} & \textbf{Val} & \textbf{Test}\\
\midrule
ImageNet        & 1000 & 1.28M & N/A & 50,000\\
Caltech101      & 100 & 4,128 & 1,649 & 2,465\\
OxfordPets      & 37 & 2,944 & 736 & 3,669\\
StanfordCars    & 196 & 6,509 & 1,635 & 8,041\\
Flowers102      & 102 & 4,093 & 1,633 & 2,463\\
Food101         & 101 & 50,500 & 20,200 & 30,300\\
FDVCAircraft    & 100 & 3,334 & 3,333 & 3,333\\
SUN397          & 397 & 15,880 & 3,970 & 19,850\\
DTD             & 47 & 2,820 & 1,128 & 1,692\\
EuroSAT         & 10 & 13,500 & 5,400 & 8,100\\
UCF101          & 101 & 7,639 & 1,808 & 3,783\\
\midrule
ImageNetV2      & 1000 & N/A & N/A & 10,000\\
ImageNet-Sketch & 1000 & N/A & N/A & 50,889\\
ImageNet-A      & 200 & N/A & N/A & 7,500\\
ImageNet-R      & 200 & N/A & N/A & 30,000\\
\bottomrule[2pt]
\end{tabular}

\end{table*}

\begin{table*}[h]
\centering
\caption{All results in the main paper were generated using shared hyperparameters.}
\label{tab: hyperparameter_setting}
\begin{tabular}{ll}
\toprule[2pt]
\textbf{Hyperparameters} & \textbf{Values}\\
\midrule
Batch Size                  & 1\\
Input Size                  & $224\times224$\\
Input Interpolation         & "Bicubic"\\
Input Pixel Mean            & $[0.48145466, 0.4578275, 0.40821073]$\\
Input Pixel STD             & $[0.26862954, 0.26130258, 0.27577711]$\\
Transforms                  & ["random resized crop", "random filp", "normalize"]\\
Optimizer                   & SGD\\
Learning Rate               & 2$e$-3\\
LR Scheduler                & "cosine"\\
Warmup Epoch                & 1\\
Warmup Type                 & "constant"\\
Warmup LR                   & 1$e$-5\\
Backbone                    & ViT-B/16\\
Prompt Length               & 4\\
Prompt Initialization       & ""\\
Precision                   & "fp16"\\
Number of shots             & 16\\
\bottomrule[2pt]
\end{tabular}
\end{table*}



\begin{table*}[h]
    \centering
    \begin{tabular}{ccccccc}
    \toprule
         Dataset &Methods &1 shot & 2 shots & 4 shots & 8 shots & 16 shots  \\
         \multirow{3}{*}{Caltech101} 
         &CoOp \\
         &PLOT &87.9 &89.4 &91.8 &93.0 &93.5 \\
         &PBPrompt \\
         \hline
         \multirow{3}{*}{DTD} 
         &CoOp \\
         &PLOT &52.0 &55.9 &58.4 &65.7 &70.0\\
         &PBPrompt \\
         \hline
         \multirow{3}{*}{EuroSAT} 
         &CoOp \\
         &PLOT &60.2 &68.3 &73.5 &79.9 &83.5 \\
         &PBPrompt \\
         \hline
         \multirow{3}{*}{FGVCAircraft} 
         &CoOp \\
         &PLOT \\
         &PBPrompt \\
         \hline
         \multirow{3}{*}{Flowers102} 
         &CoOp \\
         &PLOT &70.4 &84.5 &88.5 &80.7\\
         &PBPrompt \\
         \hline
         \multirow{3}{*}{FOOD101} 
         &CoOp \\
         &PLOT &69.3 &72.7 &75.2 &76.7 \\
         &PBPrompt \\
         \hline
         \multirow{3}{*}{ImageNet} &CoOp \\
         &PLOT \\
         &PBPrompt \\
         \hline
         \multirow{3}{*}{OxfordPets} 
         &CoOp \\
         &PLOT &82.9 &85.3 &86.0 &87.4 &88.0\\
         &PBPrompt \\
         \hline
         \multirow{3}{*}{StanfordCars} 
         &CoOp \\
         &PLOT &- &50.9 &54.0 &\\
         &PBPrompt \\
         \hline
         \multirow{3}{*}{SUN397} &CoOp \\
         &PLOT \\
         &PBPrompt \\
         \hline
         \multirow{3}{*}{UCF101} 
         &CoOp \\
         &PLOT &49.5 &53.1 &60.9 &67.3 &70.9\\
         &PBPrompt \\
         \hline
         \multirow{3}{*}{Average} &CoOp \\
         &PLOT \\
         &PBPrompt \\
         \bottomrule
    \end{tabular}
    \caption{\small{The few-shot results of various methods on 11 datasets. We report mean value over 3 different seeds.}}
    \label{tab:my_label}
\end{table*}







\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022. 
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
