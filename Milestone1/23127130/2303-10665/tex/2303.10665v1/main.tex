%%%%%%%% EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\documentclass[hyperref]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{style} with \usepackage[nohyperref]{style} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{style}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{style}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\E}{\operatorname{\mathbb E}}
\newcommand{\innermid}{\;\middle\lvert\;}

\allowdisplaybreaks

\usepackage{tikz}
\usepackage{tikz-cd}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newenvironment{subproof}[1][\proofname]{%
  \renewcommand{\qedsymbol}{$\blacksquare$}%
  \begin{proof}[#1]%
}{%
  \end{proof}%
}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Multi-Agent Reinforcement Learning via Mean Field Control: Common Noise, Major Agents and Approximation Properties}

\begin{document}

\twocolumn[
\icmltitle{Multi-Agent Reinforcement Learning via Mean Field Control: Common Noise, Major Agents and Approximation Properties}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the style
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Kai Cui}{tud}
\icmlauthor{Christian Fabian}{tud}
\icmlauthor{Heinz Koeppl}{tud}
\end{icmlauthorlist}

\icmlaffiliation{tud}{Department of Electrical Engineering and Information Technology, Technische UniversitÃ¤t Darmstadt, Germany}

\icmlcorrespondingauthor{Kai Cui}{kai.cui@tu-darmstadt.de}
\icmlcorrespondingauthor{Heinz Koeppl}{heinz.koeppl@tu-darmstadt.de}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Reinforcement Learning, Mean Field Control, Multi-Agent System}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Recently, mean field control (MFC) has provided a tractable and theoretically founded approach to otherwise difficult cooperative multi-agent control. However, the strict assumption of many independent, homogeneous agents may be too stringent in practice. In this work, we propose a novel discrete-time generalization of Markov decision processes and MFC to both many minor agents and potentially complex major agents -- major-minor mean field control (M3FC). In contrast to deterministic MFC, M3FC allows for stochastic minor agent distributions with strong correlation between minor agents through the major agent state, which can model arbitrary problem details not bound to any agent. Theoretically, we give rigorous approximation properties with novel proofs for both M3FC and existing MFC models in the finite multi-agent problem, together with a dynamic programming principle for solving such problems. In the infinite-horizon discounted case, existence of an optimal stationary policy follows. Algorithmically, we propose the major-minor mean field proximal policy optimization algorithm (M3FPPO) as a novel multi-agent reinforcement learning algorithm and demonstrate its success in illustrative M3FC-type problems.
\end{abstract}


\section{Introduction}
Recent successes of reinforcement learning (RL) \cite{vinyals2019grandmaster, schrittwieser2020mastering, ouyang2022training} motivate the search for according techniques in the multi-agent case, aptly referred to as multi-agent reinforcement learning (MARL). Due to the complexity of many-agent control problems \cite{bernstein2002complexity, daskalakis2009complexity}, a common approach is to exploit problem structure in order to achieve principled, scalable solutions. In this work, we consider systems with many agents, interacting through aggregated information -- referred to as mean field -- of all agents. Indeed, in practice such aggregation is often found on some level, in e.g. chemical reaction networks where molecules are summarized into their aggregate mass \cite{anderson2011continuous}, related mass-action epidemics models where infection rates scale with the infected \cite{kiss2017mathematics}, or traffic where congestion depends on the number of cars on a road \cite{cabannes2021solving}.

\paragraph{Mean field games and control.}
Control in aggregated interaction models is often considered by the study of mean field games (MFG) and control (MFC), where agents interact only through their empirical distribution. Since the introduction of stochastic differential MFGs \cite{huang2006large, lasry2007mean}, the concept has been studied in various forms, ranging from partial observability \cite{saldi2019partially, sen2019mean} over learning solutions \cite{guo2019learning, perrin2020fictitious, cui2021approximately, guo2020general, perolat2021scaling, perrin2021generalization} and graphical interaction \cite{caines2019graphon, tchuendom2021critical, cui2022learning, hu2022graphon} to correlated equilibria \cite{muller2021learning, campi2022correlated, bonesini2022correlated}, see also surveys \cite{bensoussan2013mean, carmona2018probabilistic, lauriere2022learning}.

Many applications of competitive MFG have already been considered, e.g. epidemics modelling \cite{dunyak2021large}, drone swarms \cite{shiri2019massive}, self organization \cite{carmona2022synchronization}, and also many other financial and engineering applications \cite{djehiche2017mean, carmona2020applications}. However, existing approaches and applications using MFGs and mean field models often focus on games with selfish agents, which can run counter to the goal of engineering many-agent behavior, e.g. achieving cooperative instead of selfish drone behavior \cite{shiri2019massive}. In this work, we hence focus on cooperative MFC, which still remains to be further developed, both theoretically and algorithmically.

\paragraph{Mean field reinforcement learning.} 
Mean field approximations enable crucial advances to theoretically founded handling of MARL problems, which are well-known to be difficult in the presence of many agents, e.g. due to the exponential nature of state and action spaces \cite{zhang2021multi}. One well-known line of works \cite{yang2018mean, ganapathi2020multi, subramanian2020partially, subramanian2022decentralized} focuses on networked agents, i.e. agents on a graph, approximating the influence of neighbors on any agent by their average actions. Relatedly, a number of MARL algorithms based on exponential decay introduce approximations over certain neighborhoods of any agent \cite{qu2020scalable, qu2020scalable2, liu2022scalable}. In contrast, typical MFGs \cite{huang2006large, saldi2018markov, guo2019learning} and MFC \cite{pham2018bellman, gu2019dynamic, mondal2022approximation} consider dependence on the global distribution of agents instead of explicit local neighbors and actions. 

However, the stringent assumption of only "minor" agents -- i.e. homogeneous agents that are abstracted into their distribution -- limits direct applicability of MFGs and MFC to finite systems, as evidenced by the abundance of works on learning for MFGs \cite{cardaliaguet2017learning, perrin2020fictitious, perolat2021scaling, perrin2021generalization} instead of MFGs (or especially MFC) for MARL \cite{yardim2022policy}. Here, we develop an MFC-based MARL algorithm, applicable both directly and to general systems not restricted to many minor agents and common noise.


\paragraph{Stochastic mean fields.} Some existing works consider common noise in stochastic differential MFGs \cite{carmona2016mean} and more recently discrete-time MFGs \cite{perrin2020fictitious}. However, while some considerations of common noise in MFC exist for differential MFC \cite{carmona2018probabilistic}, static MFC \cite{sanjari2020optimal}, or in discrete time more generally with major states \cite{gast2011mean}, the mean field and limiting dynamics usually remained deterministic. Only very recently, a number of works \cite{carmona2019model, bauerle2021mean, motte2022mean, motte2022quantitative} consider common noise in discrete-time MFC, giving rise to stochastic mean fields. However, to the best of our knowledge, only \citet{mondal2023mean} recently consider discrete-time MFC with stochastic mean fields beyond common noise, i.e. in the presence of more general environment states. Major agents on the other hand, i.e. agents with actions that can affect the entire system directly, have only been studied in continuous time and for competitive settings (MFGs) \cite{nourian2013mm, csen2014mean, caines2016mm, sen2016mean}. 

\paragraph{Major and minor agents.}
In practice, major states and agents are of great importance, as many systems consist of more than many summarized minor agents. For instance, in modelling car traffic on road networks via MFGs, one could model the position of each car on the road network \cite{cabannes2021solving}, which will constitute the minor agents. It is then sufficient to consider only the distribution of such minor agent cars on the network. However, it still remains impossible to model elements of the environment not bound to a specific car, or random common noise, i.e. noise that affects all agents similarly and induces correlation between agents. For example, common noise could include random, instantaneous accidents during an epoch, affecting all agents on the road at once. Further, major states include semi-permanent construction sites affecting the flow of traffic, whereas a major agent could be a traffic light redirecting many minor agents at once.

Existing MFC models are unable to model the above, resulting in a gap between tractability of MFC and modelling strength of general Markov decision processes (MDP). In this work, we thus generalize MFC to include arbitrary major states and random mean fields. By further considering major agents, we obtain a generalization of both MDPs and MFC as in Figure~\ref{fig:overview}. Here, we understand MFC and MFC as frameworks for multi-agent control in the space of MMDPs, i.e. a model for multi-agent systems with many (or almost infinitely many) agents, whereas MDPs model only a single agent. Even though the limiting MFC MDP is formally an MDP, and an MDP can in theory be modelled by MFC without mean field interactions, the inclusions in Figure~\ref{fig:overview} are to be understood for the actual multi-agent systems. While (i) MDPs are capable of handling highly general single agent environments, and (ii) MFC is capable of handling many identically-modelled agents interacting via their mean field, (iii) our framework handles both tractably. 

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figs/M3FC_overview_diagramm_4}
    \caption{The proposed M3FC framework generalizes both MFC and MDPs in the space of general multi-agent MDPs (MMDPs).}
    \label{fig:overview}
\end{figure}

\paragraph{Contributions.} In order to deal with the aforementioned gap between modelling strength and tractability, we formulate a general MFC-based MARL algorithm. Our contributions may be summarized as follows: (i) We formulate for the first time discrete-time major-minor MFC (M3FC), resulting in an extensive framework that generalizes MDPs and MFC while retaining desirable tractability properties in multi-agent control; (ii) We provide novel proofs for approximation guarantees and dynamic programming principles (DPPs) of MFC and M3FC by restricting to appropriate convergence modes and working with two characterizations of weak convergence. We relax continuity assumptions, extend from finite to compact spaces, and most importantly allow general stochastic mean fields; (iii) We use our framework as the basis for a powerful and tractable MARL scheme, directly applicable to finite systems with many agents in contrast to prior work, and capable of leveraging advances in single-agent RL. The algorithmic contribution is finally verified on various novel examples, outperforming state-of-the-art MARL techniques on exemplary many-agent systems. To the best of our knowledge, a comparison to MARL has not been performed yet, and approximation guarantees have not been shown for discrete-time MFC with compact state and action spaces, or major agents. 

\textit{Notation: In the following, $\mathcal P(\mathcal X)$ shall denote the space of probability measures on compact metric spaces $\mathcal X$, endowed with the topology of weak convergence. Unless noted otherwise, we metrize $\mathcal P(\mathcal X)$ with the $1$-Wasserstein distance $W_1(\mu, \nu) \coloneqq \sup_{\lVert f \rVert_{\mathrm{Lip}} \leq 1} \left| \int f \, \mathrm d(\mu - \nu) \right|$ over real-valued $f$ with minimal Lipschitz constant $\lVert f \rVert_{\mathrm{Lip}} \leq 1$ \cite{villani2009optimal}. By compactness, we have the uniformly equivalent (but not Lipschitz equivalent) metric $d_\Sigma(\mu, \mu') \coloneqq \sum_{m=1}^\infty 2^{-m} | \int f_m \, \mathrm d(\mu - \mu') |$ for a sequence of continuous $f_m \colon \mathcal X \to [-1, 1]$ (cf. \cite{parthasarathy2005probability}, Theorem~6.6). Note also that continuity is equivalent to uniform continuity in compact spaces $\mathcal X$, including $\mathcal P(\mathcal X)$ by Prokhorov's theorem \cite{billingsley2013convergence}. Proofs are found in the Appendix.}

\section{Deterministic Mean Field Control} \label{sec:mfc}
Before we consider the most general model, for expository purposes it is instructive to first consider deterministic MFC, where we also extend existing theoretical results. Here, our proof technique generalizes existing approximation properties and dynamic programming principles beyond finite spaces and Lipschitz continuity assumptions to compact spaces and simple continuity. Our proof later allows an extension to common noise, major states and beyond.

\subsection{Finite-Agent Control}
Consider systems consisting of $N$ agents $i \in [N] \coloneqq \{1, \ldots, N\}$ with compact metric state and action spaces $\mathcal X$, $\mathcal U$ and corresponding random states and actions $x^{i,N}_t$ and $u^{i,N}_t$ at times $t \in \mathbb N$, where the initial states $x^{i,N}_0 \sim \mu_0$ are independently sampled from some initial distribution $\mu_0 \in \mathcal P(\mathcal X)$. Depending on the actions, the agent states evolve according to a Markov kernel $p$ such that $x^{i,N}_{t+1} \sim p(x^{i,N}_{t+1} \mid x^{i,N}_t, u^{i,N}_t, \mu^N_t)$, i.e. the dynamics depend (i) on the agent's states and actions, and (ii) the anonymous system state given by the $\mathcal P(\mathcal X)$-valued empirical distribution of states $\mu^N_t \coloneqq \frac 1 N \sum_{i=1}^N \delta_{x^{i,N}_t}$ -- the so-called empirical mean field. Accordingly, we consider (possibly time-variant) feedback policies $\pi$ shared across all agents, reacting to the local agent state and mean field, $u^{i,N}_t \sim \pi_t(u^{i,N}_t \mid x^{i,N}_t, \mu^N_t)$. Lastly, define the infinite-horizon discounted maximization objective $J(\pi) \coloneqq \mathbb E \left[ \sum_{t=0}^\infty \gamma^t r(\mu^N_t) \right]$ with discount factor $\gamma \in (0, 1)$ and reward function $r \colon \mathcal P(\mathcal X) \to \mathbb R$, to optimize over a class $\Pi \subseteq \mathcal P(\mathcal U)^{\mathcal X \times \mathcal P(\mathcal X) \times \mathbb N}$ of policies. Overall, for all $i \in [N]$ and $t \in \mathbb N$, we have the finite MFC system
\begin{subequations} \label{eq:mmdp}
\begin{align}
    u^{i,N}_t &\sim \pi_t(u^{i,N}_t \mid x^{i,N}_t, \mu_t^N), \\
    x^{i,N}_{t+1} &\sim p(x^{i,N}_{t+1} \mid x^{i,N}_t, u^{i,N}_t, \mu_t^N), \\
    J^N(\pi) &= \E \left[ \sum_{t=0}^{\infty} \gamma^t r(\mu^N_t) \right].
\end{align}
\end{subequations}

\begin{remark}
This model is as expressive as in many prior works \cite{mondal2022approximation, gu2019dynamic}, including (i) action or joint state-action mean fields $\nu_t \in \mathcal P(\mathcal X \times \mathcal U)$ \cite{mondal2022approximation}, by splitting time steps into two and defining the new state space $\mathcal X \cup \mathcal X \times \mathcal U$, (ii) average rewards over agents and (iii) conditionally random rewards $r_t^i$ by their expectation $r(\mu^N_t) \equiv \frac 1 N \sum_{i=1}^N \E \left[ r_t^i \innermid x^{i,N}_t, \mu^N_t \right]$. A finite horizon objective can be handled analogously, though the existence of optimal \textit{stationary} policies is no longer given. Note that while agents are modelled homogeneously, heterogeneity can be integrated into the agent state space.
\end{remark}

In the following, we obtain a large, more tractable subclass of cooperative multi-agent control problems, which may otherwise suffer from the curse of many agents (combinatorial joint state-action spaces, e.g. \cite{zhang2021multi}). 

\subsection{Mean Field Control}
For tractability, one introduces the mean field limit by formally taking $N \to \infty$, and then showing its rigorous foundation for many-agent systems. The intractable finite-agent control problem is replaced by a more tractable, higher-dimensional single-agent MDP -- the MFC MDP \cite{carmona2019model, gu2021mean}. Under a shared policy $\pi$, one abstracts the agents into their probability law, i.e. the mean field $\mu_t \equiv \mathcal L(x^{i,N}_t) \in \mathcal P(\mathcal X)$, which replaces its empirical analogue $\mu^N_t$ via a law of large numbers (LLN). Thus, by definition $\mu_t$ should evolve deterministically according to
\begin{multline}
    \mu_{t+1} = T(\mu_t, \mu_t \otimes \pi_t(\mu_t)) \\
    = \iint p(\cdot \mid x, u, \mu_t) \pi_t(\mathrm du \mid x, \mu_t) \mu_t(\mathrm dx),
\end{multline}
as verified in Theorem~\ref{thm:muconv}, with $\pi_t(\mu_t) \coloneqq \pi_t(\cdot \mid \cdot, \mu_t)$, the measures $\mu_t \otimes \pi_t(\mu_t)$ on the product space $\mathcal X \times \mathcal U$, and the MFC dynamics $T(\mu, \nu) \coloneqq \iint p(\cdot \mid x, u, \mu) \nu(\mathrm dx, \mathrm du)$. 

Therefore, the states of the deterministic MFC MDP should consist only of the mean field $\mu_t$. For compactness reasons, the policies in $\Pi$ may induce $\mathcal H(\mu)$ or closed subsets thereof, which for any $\mu \in \mathcal P(\mathcal X)$ is the compact subset $\mathcal H(\mu) \subseteq \mathcal P(\mathcal X \times \mathcal U)$ of joint state-action distributions with first marginal $\mu$ (see Appendix~\ref{app:thm:dpp}). We identify the action $\mu_t \otimes \pi_t(\mu_t) \equiv h_t \in \mathcal H(\mu_t)$, obtaining the MFC MDP
\begin{subequations} \label{eq:mfc}
\begin{align}
    h_t &\sim \hat \pi_t(h_t \mid \mu_t), \\ 
    \mu_{t+1} &= T(\mu_t, h_t), \\
    J(\hat \pi) &= \E \left[ \sum_{t=0}^{\infty} \gamma^t r(\mu_t) \right]
\end{align}
\end{subequations}
with deterministic dynamics $T$, for "upper-level" MFC policies $\hat \pi \in \hat \Pi$ mapping randomly from mean field $\mu_t$ to its desired state-action distribution $h_t$. For deterministic $\hat \pi$, we write $\pi_t = \Phi(\hat \pi_t)$ to injectively reobtain agent policies by disintegration \cite{kallenberg2017random} of $h_t = \hat \pi_t(\mu_t)$ into $\mu_t \otimes \pi'_t$ and using $\pi_t(\mu_t) \equiv \pi'_t$. Inversely, any $\pi \in \Pi$ is representable in the MFC MDP by choosing $\hat \pi = \Phi^{-1}(\pi)$ via $\hat \pi_t(\mu_t) = \mu_t \otimes \pi_t(\mu_t)$.

\subsection{Theoretical Analysis}
Preempting the following results, we may solve the hard finite-agent system \eqref{eq:mmdp} near-optimally by instead solving the MFC MDP, allowing direct application of single-agent RL to the MFC MDP with approximate optimality in large systems. Mild continuity assumptions are required.

\begin{assumption} \label{ass:pcont}
The transition kernel $p$ is continuous.
\end{assumption}
\begin{assumption} \label{ass:rcont}
The reward $r$ is continuous.
\end{assumption}
\begin{assumption} \label{ass:picont}
The considered class of policies $\Pi$ is equi-Lipschitz, i.e. there exists $L_\Pi > 0$ such that for all $t$ and $\pi \in \Pi$, $\pi_t \in \mathcal P(\mathcal U)^{\mathcal X \times \mathcal P(\mathcal X)}$ is $L_\Pi$-Lipschitz.
\end{assumption}

We note that the Lipschitz condition is inconsequential and standard \cite{pasztor2021efficient, mondal2022approximation}, as (i) we may usually parametrize policies in a Lipschitz manner; (ii) neural networks are Lipschitz continuous; and (iii) Lipschitz policies should allow approximating other policies. In particular, Assumption~\ref{ass:pcont} holds true in studied-before finite spaces, if each transition matrix entry of $P$ is continuous in the $|\mathcal X|$-dimensional mean field vector on the simplex (but not necessarily Lipschitz as in \cite{gu2021mean, mondal2022approximation}, the conditions of which we relax). Our assumptions imply continuity of the MFC dynamics $T$.

\begin{lemma} \label{lem:Tcont}
Under Assumption~\ref{ass:pcont}, we have $T(\mu_n, \nu_n) \to T(\mu, \nu)$ whenever $(\mu_n, \nu_n) \to (\mu, \nu)$, 
\end{lemma}

First, we invoke a powerful, accessible DPP \cite{hernandez2012discrete} to possibly solve for and show existence of a deterministic, time-independent optimal policy using the value function $V^*$, i.e. the fixed point of the Bellman equation $V^*(\mu) = \max_{h \in \mathcal H(\mu)} r(\mu) + \gamma V^*(T(\mu, h))$.

\begin{theorem} \label{thm:dpp}
Under Assumptions~\ref{ass:pcont} and \ref{ass:rcont}, there exists an optimal stationary, deterministic policy $\hat \pi$ for \eqref{eq:mfc}, with $\hat \pi(\mu) \in \argmax_{h \in \mathcal H(\mu)} r(\mu) + \gamma V^*(T(\mu, h))$.
\end{theorem}
% \begin{proof}
% The result follows directly from \cite{hernandez2012discrete}, Theorem~4.2.3, by noting weak continuity of the MFC MDP dynamics and semicontinuous-semicompact rewards, see Appendix~\ref{app:thm:dpp}.
% \end{proof}

Our proof constitutes an alternate approach to the one taken in \cite{carmona2019model}. We use established and well-developed MDP theory, and later generalize to the major-minor case. As a result, we may (i) solve MFC problems analytically through the DPP, or (ii) later computationally by using policy gradients with stationary policies for the MFC MDP with naturally continuous actions: Finite state-action spaces lead to continuous MFC MDP actions $\mathcal H(\mu)$, while continuous spaces yield infinite-dimensional $\mathcal H(\mu)$, motivating policy gradient methods.

Lastly, we show \textit{propagation of chaos} \cite{sznitman1991topics}, i.e. convergence of empirical distributions to the mean field, in order to obtain the approximate optimality of MFC solutions in the finite system, theoretically backing the reduction of finite-agent control to "single-agent" MFC MDPs. Here, prior results \cite{gu2021mean, mondal2022approximation} are extended to quite general compact spaces.

\begin{theorem} \label{thm:muconv}
Fix any family of equicontinuous functions $\mathcal F \subseteq \mathbb R^{\mathcal P(\mathcal X)}$. Under Assumption~\ref{ass:pcont} and \ref{ass:picont}, the empirical mean field converges weakly, uniformly over $f \in \mathcal F$, $\pi \in \Pi$, $\Phi^{-1}(\pi) \in \hat \Pi$, to the limiting mean field at all times $t \in \mathbb N$,
\begin{equation} \label{eq:muconv}
    \sup_{\pi \in \Pi} \sup_{f \in \mathcal F} \left| \E \left[ f(\mu^N_{t}) \right] - \E \left[ f(\mu_{t}) \right] \right| \to 0.
\end{equation}
\end{theorem}
% \begin{proof}
% The statement \eqref{eq:muconv} is shown by induction over $t \geq 0$,
% \begin{align*}
%     &\sup_{\pi \in \Pi} \sup_{f \in \mathcal F} \left| \E \left[ f(\mu^N_{t+1}) - f(\mu_{t+1}) \right] \right| \\
%     &\leq \sup_{\pi \in \Pi} \sup_{f \in \mathcal F} \left| \E \left[ f(\mu^N_{t+1}) - f(T(\mu^N_t, \mu^N_t \otimes \pi_t(\mu^N_t))) \right] \right| \\
%     &\quad + \sup_{\pi \in \Pi} \sup_{f \in \mathcal F} \left| \E \left[ f(T(\mu^N_t, \mu^N_t \otimes \pi_t(\mu^N_t))) - f(\mu_{t+1}) \right] \right|
% \end{align*}
% where we handle the first term by a weak LLN argument, while the second follows from the induction assumption. More details are found in Appendix~\ref{app:thm:muconv}.
% \end{proof}
Therefore, we may solve the difficult finite control problem by detouring over the corresponding MFC MDP, which we solve using powerful single-agent RL techniques in Section~\ref{sec:algo}. See also Figure~\ref{fig:detour} for the corresponding diagram.

\begin{figure}
    \centering
    \resizebox{\linewidth}{!}{
        $$
        \begin{tikzcd}[column sep=large,ampersand replacement=\&]
        \text{$N$-agent control} 
        \arrow[dashed, draw=red]{d}[anchor=west]{\text{optimize \textit{(\textbf{intractable})}}} 
        \arrow[draw=blue]{r}{N \to \infty} 
        \& \text{MFC / M3FC} 
        \arrow[draw=blue]{d}[anchor=west]{\text{optimize}} \\
        \text{Optimal $N$-agent policies} 
        \& \arrow[draw=blue]{l}{\text{approx.}} 
        \text{MFC / M3FC policy}
        \end{tikzcd}
        $$
    }
    \caption{Approximation of multi-agent control by MFC (blue path). Otherwise intractable $N$-agent control problems are first abstracted into MFC, the solution of which then constitutes an approximately optimal solution in the $N$-agent control problem.}
    \label{fig:detour}
\end{figure}

\begin{corollary} \label{coro:epsopt}
Under Assumptions~\ref{ass:pcont}, \ref{ass:rcont} and \ref{ass:picont}, an optimal deterministic policy in the MFC $\pi^* \in \argmax_{\hat \pi} J(\hat \pi)$ with $\Phi(\pi^*) \in \Pi$ results in an $\varepsilon$-optimal policy $\Phi(\pi^*)$ in the finite-agent system with $\varepsilon \to 0$ as $N \to \infty$,
\begin{equation}
    J^N(\Phi(\pi^*)) \geq \sup_{\pi \in \Pi} J^N(\pi) - \varepsilon.
\end{equation}
\end{corollary}


\section{Stochastic Mean Field Control} \label{sec:stochastic}
In this section, we extend MFC by allowing for stochastic mean fields and a major agent that is not abstractable into its distribution. Overall, we obtain a tractable class of models that generalizes both MDPs and classical MFC. We will use the same symbols as in Section~\ref{sec:mfc}, extended accordingly. 

\subsection{Common Noise and Major States}
In the classical sense \cite{perrin2020fictitious, motte2022mean}, common noise is given by random noise $\epsilon^0_t \sim p_\epsilon(\epsilon^0_t)$ sampled from a stationary distribution $p_\epsilon$ and affecting all minor agents at once, $x^{i,N}_{t+1} \sim p(x^{i,N}_{t+1} \mid x^{i,N}_t, u^{i,N}_t, \epsilon^0_t, \mu_t^N)$. This allows describing systems with stochastic mean fields and inter-agent correlation, and has added difficulty to the theoretical analysis \cite{carmona2016mean}. 

We go beyond common noise by general major states $x^{0,N}_t$ indexed by agent index $i=0$, which need not be sampled from fixed distributions but may evolve dynamically. Consider major states $x^{0,N}_t$ from a compact major state space $\mathcal X^0$ in addition to the prequel. The major state is sampled from an initial distribution $\mu^0_0 \in \mathcal P(\mathcal X^0)$ and evolves according to a transition kernel $p^0$ as $x^{0,N}_{t+1} \sim p^0(x^{0,N}_{t+1} \mid x^{0,N}_t, \mu_t^N)$. Letting the major state influence minor agents gives the novel\footnote{We note that the recent work \cite{mondal2023mean} independently proposed a related model with major states for finite spaces.} system with major states \eqref{eq:msmmdp} in Appendix~\ref{app:msmfc}, with corresponding limiting system and results analogous to the following more general model with major agents.


\subsection{Major-Minor MFC}
In a final step, we generalize both MFC and MDPs to deal with both minor and major agents. As a result, our novel model tractably describes many-agent systems with both MFC and MDP components. Note that the major agent state may include also any components of the environment unrelated to agents, e.g. cars in a routing problem affected by construction sites, controlled traffic lights and other dynamical circumstances that are not bound to a specific car. 

We consider major actions $u^{0,N}_t$ in a compact major action space $\mathcal U^0$, sampled from policies $\pi^0 \in \Pi^0$ in the space of major agent policies $\Pi^0 \subseteq \mathcal P(\mathcal U^0)^{\mathcal X^0 \times \mathcal P(\mathcal X) \times \mathbb N}$. Allowing major actions to affect dynamics and rewards, we formulate the finite M3FC system
\begin{subequations} \label{eq:m3mdp}
\begin{align}
    u^{i,N}_t &\sim \pi_t(u^{i,N}_t \mid x^{i,N}_t, x^{0,N}_t, \mu_t^N), \\
    x^{i,N}_{t+1} &\sim p(x^{i,N}_{t+1} \mid x^{i,N}_t, u^{i,N}_t, x^{0,N}_t, u^{0,N}_{t}, \mu_t^N), \\
    u^{0,N}_{t} &\sim \pi^0_t(u^{0,N}_{t} \mid x^{0,N}_t, \mu_t^N), \\
    x^{0,N}_{t+1} &\sim p^0(x^{0,N}_{t+1} \mid x^{0,N}_t, u^{0,N}_{t}, \mu_t^N), \\
    J^N(\pi, \pi^0) &= \E \left[ \sum_{t=0}^{\infty} \gamma^t r(x^{0,N}_t, u^{0,N}_t, \mu^N_t) \right],
\end{align}
\end{subequations}
and accordingly the limiting M3FC MDP 
\begin{subequations} \label{eq:m3fc}
\begin{align}
    h_t &\sim \hat \pi(h_t \mid x^0_t, \mu_t), \\ 
    \mu_{t+1} &= T(x^0_t, u^0_{t}, \mu_t, h_t), \\
    u^0_{t} &\sim \pi^0(u^0_{t} \mid x^0_t, \mu_t), \\
    x^0_{t+1} &\sim p^0(x^0_{t+1} \mid x^0_t, u^0_{t}, \mu_t), \\
    J(\hat \pi, \pi^0) &= \E \left[ \sum_{t=0}^{\infty} \gamma^t r(x^0_t, u^0_t, \mu_t) \right]
\end{align}
\end{subequations}
with $T(x^0, u^0, \mu, h) \coloneqq \iint p(\cdot \mid x, u, x^0, u^0, \mu) h(\mathrm dx, \mathrm du)$, where we identify $(h_t, u^0_t)$ as the action of the M3FC MDP, and $(x^0_t, \mu_t)$ as its state, visualized also in Figure~\ref{fig:pgm}.


\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figs/PGM_MFC.pdf}
    \caption{The dynamics model as a probabilistic graphical model, with actions in grey (inputs omitted for readability). The diamond denotes a deterministic function of its inputs. The M3FC approach hides minor agents $i \in [N]$ by a LLN, considering only their distribution, i.e. the variables in the dotted box.}
    \label{fig:pgm}
\end{figure}

As shown in Figure~\ref{fig:overview}, the M3FC generalizes both MDP and MFC in the space of multi-agent Markov decision processes (MMDP, e.g. \cite{oliehoek2016concise}). In other words, M3FC handles both highly general single agents and many minor agents at the same time. Here, we understand MFC and M3FC as frameworks for multi-agent control, and not e.g. the limiting M3FC MDP, which is -- formally -- an MDP.

\begin{remark}
Strictly speaking, in centralized MMDPs one may select jointly $(u^{0,N}_t, u^{1,N}_t, \ldots, u^{N,N}_t)$ given joint states $(x^{0,N}_t, x^{1,N}_t, \ldots, x^{N,N}_t)$. However, under mean field dynamics, by the LLN (i) information from the joint state should reduce to $(x^{0,N}_t, \mu^N_t)$, while (ii) joint actions would be replaced by LLN via sampling from $h_t$. We conjecture that it is possible to extend optimality (e.g. Corollary~\ref{coro:m3epsopt}) over larger such classes of policies, see Appendix~\ref{app:moreopt}.
\end{remark}

\subsection{Theoretical Analysis}
We now perform a theoretical analysis of the novel M3FC model. For the proof, we now assume Lipschitz continuity.

\begin{assumption} \label{ass:m3pcont}
The transition kernels $p$, $p^0$ are Lipschitz continuous with constants $L_p$, $L_{p^0}$.
\end{assumption}
\begin{assumption} \label{ass:m3rcont}
The reward $r$ is Lipschitz continuous.
\end{assumption}
\begin{assumption} \label{ass:m3picont}
The classes of policies $\Pi$, $\Pi^0$ are equi-Lipschitz, i.e. there exist $L_\Pi, L_{\Pi^0}$ as in Assumption~\ref{ass:picont}.
\end{assumption}

As in deterministic MFC, we obtain a DPP by defining again the value function via the Bellman equation $V^*(x^0, \mu) = \max_{(h, u^0) \in \mathcal H(\mu) \times \mathcal U^0} r(x^0, u^0, \mu) + \gamma \mathbb E_{y^0 \sim p^0(y^0 \mid x^0, u^0, \mu)} V^*(y^0, T(x^0, u^0, \mu, h))$.

\begin{theorem} \label{thm:m3dpp}
Under Assumptions~\ref{ass:m3pcont} and \ref{ass:m3rcont}, there exist optimal stationary, deterministic policies $\hat \pi$, $\pi^0$ for the M3FC MDP \eqref{eq:m3fc} by choosing $(\hat \pi(x^0, \mu), \pi^0(x^0, \mu))$ from the maximizers of $\argmax_{(h, u^0) \in \mathcal H(\mu) \times \mathcal U^0} r(x^0, u^0, \mu) + \gamma \mathbb E_{y^0 \sim p^0(y^0 \mid x^0, u^0, \mu)} V^*(y^0, T(x^0, u^0, \mu, h))$.
\end{theorem}

\begin{remark} \label{remark:joint}
Note that though \eqref{eq:m3fc} has independent $\hat \pi$ and $\pi^0$, we still obtain existence of optimal deterministic stationary $\hat \pi$, $\pi^0$ via existence of such M3FC policies $\tilde \pi \equiv \hat \pi \otimes \pi^0$, $(h_t, u^0_t) \sim \tilde \pi((h_t, u^0_t) \mid x^0_t, \mu_t)$ by Theorem~\ref{thm:m3dpp}.
\end{remark}

Hence again, as in the MFC case, we may solve analytically via the DPP or use policy gradients with stationary policies, motivating the M3FC model together with the following convergence results and approximate optimality of M3FC MDP solutions in the finite M3FC system.

\begin{theorem} \label{thm:m3muconv}
Fix any family of equi-Lipschitz functions $\mathcal F \subseteq \mathbb R^{\mathcal X^0 \times \mathcal U^0 \times \mathcal P(\mathcal X)}$ with shared Lipschitz constant $L_{\mathcal F}$ for all $f \in \mathcal F$. Under Assumptions~\ref{ass:m3pcont} and \ref{ass:m3picont}, the random variable $(x^{0,N}_t, u^{0,N}_{t}, \mu_t^N)$ converges weakly, uniformly over $f \in \mathcal F$, $\pi \in \Pi$, $\pi^0 \in \Pi^0$, to $(x^0_t, u^0_{t}, \mu_t)$ at all times $t \in \mathbb N$,
\begin{equation} \label{eq:m3muconv}
    \sup_{f, \pi, \pi^0} \left| \E \left[ f(x^{0,N}_t, u^{0,N}_{t}, \mu_t^N) - f(x^0_t, u^0_{t}, \mu_t) \right] \right| \to 0.
\end{equation}
\end{theorem}
The result is a more tractable framework for general systems of many agents by conceptually reducing to higher-dimensional MDPs, which will provide us a basis for MARL algorithms with the following $\varepsilon$-optimality guarantee.

\begin{corollary} \label{coro:m3epsopt}
Under Assumptions~\ref{ass:m3pcont}, \ref{ass:m3rcont} and \ref{ass:m3picont}, optimal deterministic policies $(\pi^*, \pi^{0*}) \in \argmax_{(\hat \pi, \pi^0)} J(\hat \pi, \pi^0)$ in the M3FC MDP with $\Phi(\pi^*)$ yield $\varepsilon$-optimal $(\Phi(\pi^*), \pi^{0*})$ in the finite M3FC system with $\varepsilon \to 0$ as $N \to \infty$,
\begin{equation}
    J^N(\Phi(\pi^*), \pi^{0*}) \geq \sup_{(\pi, \pi^0) \in \Pi \times \Pi^0} J^N(\pi, \pi^0) - \varepsilon.
\end{equation}
\end{corollary}




\section{Major-Minor Mean Field MARL} \label{sec:algo}
In order to solve multi-agent control, it is crucial to find tractable sample-based MARL techniques, both for (i) otherwise too complex problems and (ii) for problems where we have no access to the dynamics or reward model. On the one hand, RL has been applied to solve MFC. On the other, we could use the MFC formalism to instead give rise to novel MARL algorithms. While literature often focuses on the former \cite{carmona2019model, pasztor2021efficient, mondal2022approximation}, in our work we understand the proposed algorithm as both a solution to M3FC MDPs, and to the more interesting, actual finite system. We apply the following perspective: By Theorem~\ref{thm:m3muconv}, the MFC MDP is approximated well by the finite system. Therefore, there is no need to solve the limiting M3FC MDP, and instead it is sufficient to apply our proposed MFC solution directly to finite MFC systems. In particular, accessing an MFC MDP would already allow instantiating finite systems of any size.

\begin{algorithm}[t]
    \caption{Major-Minor Mean Field PPO (M3FPPO)}
    \label{alg:ppo}
    \begin{algorithmic}[1]
        \STATE Initialize PPO policy $\pi^\theta_{\mathrm{RL}}$ and value critic $V^\psi$.
        \FOR {iterations $n=0, 1, \ldots$}
            \STATE Initialize empty batch buffer $B = \emptyset$.
            \FOR {time step $t = 0, \ldots, B_{\mathrm{len}}-1$}
                % \STATE Compute mean field $\mu^N_t \coloneqq \frac 1 N \sum_{i=1}^N \delta_{x^{i,N}_t}$.
                \STATE Sample M3FC MDP action from PPO policy, $u_t \equiv (u^{0,N}_t, \pi'_t) \sim \pi^\theta_{\mathrm{RL}}((u^{0,N}_t, \pi'_t) \mid x^{0,N}_t, \mu^N_t)$.
                \FOR {minor agent $i = 1, \ldots, N$}
                    \STATE Sample actions $u^{i,N}_t \sim \pi'_t(u^{i,N}_t \mid x^{i,N}_t)$.
                \ENDFOR
                \STATE Execute actions $\{u^{i,N}_t\}_{i=0,1,\ldots}$ in environment. 
                \STATE Observe reward $r^N_t$, next states $\{ x^{i,N}_{t+1} \}_{i=0,1,\ldots}$, and episode termination flag $d_{t+1} \in \{0, 1\}$.
                % \STATE Compute mean field $\mu^N_{t+1} \coloneqq \frac 1 N \sum_{i=1}^N \delta_{x^{i,N}_{t+1}}$.
                \STATE Add $((x^{0,N}_t, \mu^N_t), u_t, r^N_t, d_{t+1}, (x^{0,N}_{t+1}, \mu^N_{t+1}))$ (a transition sample) to batch buffer $B$.
            \ENDFOR
            \STATE Compute GAE advantages $\hat A$ on $B$.
            \FOR {update epoch $i = 1, \ldots, N_{\mathrm{SGD}}$}
                \STATE Sample mini-batch $b$ randomly from $B$.
                \STATE Update $\theta$ via PPO loss gradient $\nabla_\theta L_\theta$ on data $b$.
                \STATE Update $\psi$ via $L_2$ loss gradient $\nabla_\psi L_\psi$ on data $b$.
            \ENDFOR
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\paragraph{MDP-based MARL.} Since we know by Theorem~\ref{thm:m3dpp} that there exists an optimal stationary policy, we solve the M3FC MDP \eqref{eq:m3fc} using standard stationary policies and single-agent RL, aptly referred to as major-minor mean field MARL (M3FMARL). Here, the M3FC MDP \eqref{eq:m3fc} is actually given by a large MARL problem \eqref{eq:m3mdp}, i.e. in a sense the natural particle filter approximation for the M3FC MDP. Specifically, we will use the proximal policy optimization (PPO) algorithm \cite{schulman2017proximal} to obtain a M3FC policy $\pi_{\mathrm{RL}}$ according to Algorithm~\ref{alg:ppo}, but other RL algorithms could also be used. Beyond improved tractability, an advantage of this approach is therefore that we can profit from any advances in single-agent RL. For this purpose, we must first provide a parametric form of the mean fields in $\mathcal P(\mathcal X)$ and the joint distribution part of M3FC MDP actions $\mathcal H(\mu^N_t)$. 

\paragraph{Parametrization of spaces.} For discrete $\mathcal X$, $\mathcal U$, this is straightforward by finite-dimensional vectors $\mu^N_t \in \mathcal P(\mathcal X)$ on the simplex and by considering as part of the M3FC policy actions the two-dimensional matrix $O \in [-1, 1]^{\mathcal X \times \mathcal U}$. The values of $O$ are then mapped to probabilities of actions in any state $\pi'_t(u \mid x) = \frac{O_{xu}+1+\epsilon}{\sum_{u' \in \mathcal U} (O_{xu'}+1+\epsilon)}$ for a small $\epsilon = 10^{-10}$ for numerical stability. The implicitly defined M3FC MDP action $h^N_t \in \mathcal H(\mu^N_t)$ is hence $h^N_t \equiv \mu^N_t \otimes \pi'_t(u \mid x)$, where $O \sim \pi_{\mathrm{RL}}(O \mid x^{0,N}_t, \mu^N_t)$ is sampled from RL policy $\pi_{\mathrm{RL}}$. For continuous $\mathcal X$, $\mathcal U$, we partition $\mathcal X$ into $M$ bins and represent $\mu^N_t$ by binned histograms. Action distributions are instead parametrized by affinely mapping values $O \in [-1, 1]^{M \times 2}$ to (clipped) diagonal Gaussian parameters, e.g. $\mu_{\mathcal X_i} \in \mathcal U$, $\sigma_{\mathcal X_i} \in [\epsilon, 0.25 + \epsilon]$ of $\mathcal N(\mu_{\mathcal X_i}, \sigma_{\mathcal X_i}) \in \mathcal P(\mathcal U)$ defined for all $M$ equisized bins $\mathcal X_i \subseteq \mathcal X$, constituting $\pi'_t$. 

\paragraph{Hierarchical structure.} The M3FC MDP state $(x^{0,N}_t, \mu^N_t)$ is fed into the policy, while the distribution of major actions $u^{0,N}_t$ is parametrized via categorical or diagonal Gaussian distributions for discrete and continuous $\mathcal U^0$ respectively. Sampling major actions and $O$ from $\pi_{\mathrm{RL}}$, and minor actions from $\pi'_t$, allows direct application of PPO to MARL \eqref{eq:m3mdp} and completes M3FPPO by observing next states $(x^{0,N}_{t+1}, \mu^N_{t+1})$ and reward $r^N_t$, see also Algorithm~\ref{alg:ppo}. Overall, Algorithm~\ref{alg:ppo} is "hierarchical" in that M3FC MDP actions specify behavior at once for all minor agents.
 
\paragraph{Centralized training, decentralized execution.} Our algorithm falls into the centralized training decentralized execution (CTDE) paradigm \cite{zhang2021multi}, as we sample a single central M3FC MDP action during training, but enable decentralized execution by sampling $\pi'_t$ instead separately on each agent (before line $7$ in Algorithm~\ref{alg:ppo}). For instance assuming a deterministic M3FC policy (of which an optimal one is guaranteed to exist by Theorem~\ref{thm:m3dpp}), the M3FC action will trivially be equal for all agents. Regardless, we also verify decentralized execution experimentally.

\paragraph{Comparison.} We compare against independent PPO (IPPO), i.e. PPO with independent learning \cite{tan1993multi} and parameter sharing \cite{gupta2017cooperative}, which often shows state-of-the-art performance in cooperative MARL \cite{de2020independent, papoudakis2021benchmarking, yu2021surprising}, though we separate major and minor agent policies for performance. To compare, we use the same observations (where minor agents additionally observe own states), policy architecture, PPO implementation and hyperparameters as in M3FPPO.

\section{Experiments}
In this section, we demonstrate the applicability of M3FPPO on illustrative problems. We use $M=49$ bins ($M=7$ in Potential) and train M3FPPO on the natural approximation of the M3FC system, which is the finite-agent system \eqref{eq:m3mdp} with $N=300$ agents (training for lower $N$ in Appendix~\ref{app:exp}). For implementation and problem details, see Appendix~\ref{app:exp}.

\subsection{Problems}
The \textbf{2G} problem is a simple major state problem, where minor agents should form a sinusoidal time-variant mixture of two Gaussians $\mu^*_t$ -- the major state -- which is noisily observed analogously to $\mu^N_t$ by binning into $M=49$ bins.

In the \textbf{Formation} problem, the added major agent should stay close to an Ornstein-Uhlenbeck target, while minor agents should form a Gaussian around the major agent.

The \textbf{Beach Bar} process is an adapted classic \cite{perrin2020fictitious}, where minor agents minimize their distance to a bar, and crowdedness. Here, we consider a discrete torus variant, with a moving bar following a randomly walking target.

In the \textbf{Foraging} problem, agents must forage from random foraging areas, e.g. consider a movement-limited package truck with minor drones picking up packages in a city. The minor agents gain encumbrance proportional to closeness to foraging area, and empty their load at the major agent.

Lastly, in the \textbf{Potential} problem, minor agents move to generate a potential landscape on a continuous one-dimensional torus (sphere), the gradient of which is followed by the major agent to keep it close to an Ornstein-Uhlenbeck target.  

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/M3FC_training_curves.pdf}
    \caption{Training curves (mean episode return) of M3FPPO (red), with standard deviation (three trials, shaded) and maximum (blue). (a) 2G; (b) Formation; (c) Beach Bar; (d) Foraging; (e) Potential.}
    \label{fig:curvem3fc}
\end{figure}


\begin{figure}[b!]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/MM_MARL_training_curves.pdf}
    \caption{Training curves of IPPO, as in Figure~\ref{fig:curvem3fc} (no maxima).}
    \label{fig:marl}
\end{figure}


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/MM_qualitative_behavior_two.pdf}
    \includegraphics[width=0.99\linewidth]{figs/MM_qualitative_behavior_formation.pdf}
    \caption{Qualitative visualization of learned MFC behavior in the simple 2G (a-d) and Formation (e-h) problems. Red: minor agent; blue triangle: major agent; green triangle: major agent target.}
    \label{fig:qualitative1}
\end{figure}

\begin{figure}[b!]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/MM_qualitative_behavior_beach.pdf}
    \includegraphics[width=0.99\linewidth]{figs/MM_qualitative_behavior_foraging.pdf}
    \includegraphics[width=0.99\linewidth]{figs/MM_qualitative_behavior_potential_1d.pdf}
    \caption{Qualitative visualization of learned MFC behavior in the Beach (a-d), Foraging (e-h) and Potential (i-l) problems. (a-d): empirical distribution of states, with major agent and target both in green; (e-h): As in Figure~\ref{fig:qualitative1}(e-h), but green for less-than-half-encumbered minor agents, and red otherwise; Foraging areas are indicated in purple; (i-l): As in Figure~\ref{fig:qualitative1}(e-h), with arrow indicating the current potential gradient (not to scale).}
    \label{fig:qualitative2}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/MM_CTDE_comparisons_all.pdf}
    \caption{The mean episode return of the final learned MFC policy in finite $N$-agent systems over (a-c) $100$, (d) $300$ or (e) $500$ trials, with 95\% confidence interval (shaded). MF: CE for $N=500$; CE: centralized execution; DE: decentralized execution.}
    \label{fig:Ncompare}
\end{figure}

\subsection{Training results}
In Figure~\ref{fig:curvem3fc} and Appendix~\ref{app:exp}, we see that M3FPPO stably obtains good results, despite the fact that a MARL problem is solved, as M3FPPO reduces otherwise hard many-agent problems to single-agent RL of higher dimension. Though RL nonetheless remains difficult and could profit from intricate hyperparameter tuning, M3FPPO trains comparatively stably by avoiding various pathologies of MARL techniques such as non-stationarity of multi-agent learning, or the exponential nature of joint state-actions \cite{zhang2021multi}. On the other hand, IPPO under the same hyperparameters leads to the sometimes unstable training seen in Figure~\ref{fig:marl}. This is intuitive, as for many agents, the reward signal for any single agent's action can sometimes become uninformative (as a cooperative "averaged" signal over all agents).


\paragraph{Qualitative behavior.}
In Figure~\ref{fig:qualitative1}, it can be seen that M3FPPO successfully learns to form mixtures of Gaussians in 2G, and a Gaussian around a moving major agent successfully tracking its target in Formation. As expected in 2G, the two Gaussians at their sinusoidal peaks $t=25$ and $t=50$ are not perfectly tracked, in order to minimize the cost in following time steps, when the other Gaussian reappears. Meanwhile, in Figure~\ref{fig:qualitative2} we observe success in Beach, Foraging and Potential: In Beach, M3FPPO learns to accumulate up to $70\%$ of agents on the bar, as more will lead to a decrease in reward. In Foraging, agents successfully deplete the foraging areas in the bottom left after moving on to other areas. Lastly, in Potential, the minor agents usually successfully push the major agent towards its current target.

\paragraph{Finite system and decentralized execution.}
In Figure~\ref{fig:Ncompare}, we transfer the trained M3FPPO policy to $N=2, \ldots, 50$, comparing against the performance in the limiting system (with $N=500$). We observe that as $N$ grows, the performance converges to that of the limiting system, supporting Theorem~\ref{thm:m3muconv} and Corollary~\ref{coro:m3epsopt}. We thus conclude transferability to varying numbers of agents. In an ablation in Appendix~\ref{app:exp}, we alternatively also find similar success in directly training M3FPPO on small $N$-agent systems instead of transferring behavior from a large limiting system.

Comparing Figures~\ref{fig:curvem3fc}, \ref{fig:marl} and \ref{fig:Ncompare}, we see that (i) by experience sharing, IPPO can be more sample efficient, partly as time steps give $N$ samples instead of just one in M3FPPO; (ii) however, except in Potential, M3FPPO outperforms or matches IPPO for many agents, even if IPPO stops early at its maximum during training (comparing best of $3$ trials at $N=20$, in 2G $-23$ vs. $-42$, Beach $-310$ vs. $-350$, Formation both $-60$, and especially Foraging $1000$ vs. $570$).

Lastly, we verify the effect of fully decentralized execution by agent-wise randomization, i.e. instead of sampling a single centralized M3FC action, we apply the upper-level policy on each agent separately. It turns out that decentralized execution has little effect and can even marginally improve performance, e.g. in Beach, see Figure~\ref{fig:Ncompare}.

\section{Discussion}
We have proposed a generalization of MDPs and MFC, enabling tractable MARL on quite general many-agent systems, with both theoretical and empirical support. For future work, one could attempt to show extended optimality results as discussed in Appendix~\ref{app:moreopt}, consider more refined approximations \cite{gast2018refined}, local \cite{qu2020scalable} and global interactions together, or apply the framework in practice. Algorithmically, parametrization of M3FC MDP actions $\mathcal H(\mu)$ could move beyond binning of $\mathcal X$ to further gain performance, e.g. via kernel methods. Hyperparameter tuning could additionally improve results. Lastly, another goal is quantifying convergence to the classical rate $\mathcal O(1/\sqrt N)$ \cite{huang2006large}, as the current proof would require difficult-to-verify $d_\Sigma$-Lipschitz assumptions.

\section*{Acknowledgements}
This work has been co-funded by the LOEWE initiative (Hesse, Germany) within the emergenCITY center, and the Hessian Ministry of Science and the Arts (HMWK) within the projects "The Third Wave of Artificial Intelligence - 3AI" and hessian.AI. The authors acknowledge the Lichtenberg high performance computing cluster of the TU Darmstadt for providing computational facilities for the calculations of this research. We thank anonymous reviewers for their helpful comments to improve the manuscript.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{main}
\bibliographystyle{style}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Proofs} \label{app:proofs}
Here, we provide lengthy proofs that were omitted in the main text.

\subsection{Proof of Lemma~\ref{lem:Tcont}} \label{app:lem:Tcont}
\begin{proof}
To show $T(\mu_n, \nu_n) \to T(\mu, \nu)$, consider any Lipschitz and bounded $f$ with Lipschitz constant $L_f$, then
\begin{align*}
    &\left| \int f \, \mathrm d(T(\mu_n, \nu_n) - T(\mu, \nu)) \right| \\
    &= \left| \iiint f(x') p(\mathrm dx' \mid x, u, \mu_n) \nu_n(\mathrm dx, \mathrm du) - \iiint f(x') p(\mathrm dx' \mid x, u, \mu) \nu(\mathrm dx, \mathrm du) \right| \\
    &\quad \leq \iint \left| \int f(x') p(\mathrm dx' \mid x, u, \mu_n) - \int f(x') p(\mathrm dx' \mid x, u, \mu) \right| \nu_n(\mathrm dx, \mathrm du) \\
    &\qquad + \left| \iiint f(x') p(\mathrm dx' \mid x, u, \mu) (\nu_n(\mathrm dx, \mathrm du) - \nu(\mathrm dx, \mathrm du)) \right| \\
    &\quad \leq \sup_{x \in \mathcal X, u \in \mathcal U} L_f W_1(p(\cdot \mid x, u, \mu_n), p(\cdot \mid x, u, \mu)) \\
    &\qquad + \left| \iiint f(x') p(\mathrm dx' \mid x, u, \mu) (\nu_n(\mathrm dx, \mathrm du) - \nu(\mathrm dx, \mathrm du)) \right| \to 0
\end{align*}
for the first term by $1$-Lipschitzness of $\frac{f}{L_f}$ and Assumption~\ref{ass:pcont} (with compactness implying the uniform continuity), and for the second by $\nu_n \to \nu$ and from continuity by the same argument of $(x,u) \mapsto \iint f(x') p(\mathrm dx' \mid x, u, \mu)$.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:dpp}} \label{app:thm:dpp}
\begin{proof}
The MFC MDP fulfills \cite{hernandez2012discrete}, Assumption~4.2.1. Here, we use \cite{hernandez2012discrete}, Condition~3.3.4(b1) instead of (b2), see also alternatively \cite{hernandez1992discrete}. 

More specifically, for \cite{hernandez2012discrete}, Assumption~4.2.1(a), the cost function $-r$ is continuous by Assumption~\ref{ass:rcont}, therefore also bounded by compactness of $\mathcal P(\mathcal X)$, and finally also inf-compact on the state-action space of the MFC MDP, since for any $\mu \in \mathcal P(\mathcal X)$ the set $\{ h \in \mathcal H(\mu) \mid -r(\mu) \leq c \}$ is trivially given by $\mathcal H(\mu)$ whenever $-r(\mu) \leq c$, or $\emptyset$ otherwise. Here, we show that $\mathcal H(\mu) \subseteq \mathcal P(\mathcal X \times \mathcal U)$ is a subset of the compact space $\mathcal P(\mathcal X \times \mathcal U)$ that is closed (and therefore also compact). Note first that two measures $\mu, \mu' \in \mathcal P(\mathcal X)$ are equal if and only if for all continuous and bounded $f$ we have $\int f \, \mathrm d\mu = \int f \, \mathrm d\mu'$, see e.g. \cite{billingsley2013convergence}, Theorem 1.3. 

Therefore, as $\mathcal H(\mu)$ is defined by its first marginal $\mu$, $\mathcal H(\mu)$ can be written as an intersection
\begin{align*}
    \mathcal H(\mu) = \bigcap_{f \in C_b(\mathcal X)} \left\{ h \in \mathcal P(\mathcal X \times \mathcal U) \innermid \int f \otimes \mathbf 1 \, \mathrm dh = \int f \, \mathrm d\mu \right\}
\end{align*}
of closed sets: Since $h \mapsto \int f \otimes \mathbf 1 \, \mathrm dh$ is continuous, its preimage of the closed set $\{ \int f \, \mathrm d\mu \}$ is closed. Here, $\otimes$ denotes the tensor product of $f$ with the function $\mathbf 1$ equal one, i.e. $f \otimes \mathbf 1$ is the map $(x, u) \mapsto f(x)$.

Similarly, for \cite{hernandez2012discrete}, Assumption~4.2.1(b), the transition dynamics $T$ are weakly continuous, as for any $(\mu_n, \nu_n) \to (\mu, \nu) \in \mathcal P(\mathcal X) \times \mathcal P(\mathcal X \times \mathcal U)$ we have $T(\mu_n, \nu_n) \to T(\mu, \nu)$ by Lemma~\ref{lem:Tcont} and therefore $\int f \, \mathrm d\delta_{T(\mu_n, \nu_n)} = f(T(\mu_n, \nu_n)) \to f(T(\mu, \nu)) = \int f \, \mathrm d\delta_{T(\mu, \nu)}$ for any continuous and bounded $f \colon \mathcal P(\mathcal X) \to \mathbb R$. 

Furthermore, the MFC MDP fulfills \cite{hernandez2012discrete}, Assumption~4.2.2 by boundedness of $r$ from Assumption~\ref{ass:rcont}. Therefore, the desired statement follows from \cite{hernandez2012discrete}, Theorem~4.2.3.
\end{proof}


\subsection{Proof of Theorem~\ref{thm:muconv}} \label{app:thm:muconv}
\begin{proof}
The statement \eqref{eq:muconv} is shown inductively over $t \geq 0$. At time $t=0$, \eqref{eq:muconv} holds by the weak LLN argument, see also the first term below. Assuming \eqref{eq:muconv} at time $t$, then for time $t+1$ we have
\begin{align}
    &\sup_{\pi \in \Pi} \sup_{f \in \mathcal F} \left| \E \left[ f(\mu^N_{t+1}) - f(\mu_{t+1}) \right] \right| \nonumber \\
    &\quad \leq \sup_{\pi \in \Pi} \sup_{f \in \mathcal F} \left| \E \left[ f(\mu^N_{t+1}) - f(T(\mu^N_t, \mu^N_t \otimes \pi_t(\mu^N_t))) \right] \right| \label{eq:first} \\
    &\qquad + \sup_{\pi \in \Pi} \sup_{f \in \mathcal F} \left| \E \left[ f(T(\mu^N_t, \mu^N_t \otimes \pi_t(\mu^N_t))) - f(\mu_{t+1}) \right] \right|. \label{eq:second}
\end{align}

For the first term \eqref{eq:first}, first note that by compactness of $\mathcal P(\mathcal X)$, $\mathcal F$ is uniformly equicontinuous, and hence admits a non-decreasing, concave (as in \cite{devore1993constructive}, Lemma~6.1) modulus of continuity $\omega_{\mathcal F} \colon [0, \infty) \to [0, \infty)$ where $\omega_{\mathcal F}(x) \to 0$ as $x \to 0$ and $|f(\mu) - f(\nu)| \leq \omega_{\mathcal F}(W_1(\mu, \nu))$ for all $f \in \mathcal F$. 

We also have uniform equicontinuity of $\mathcal F$ with respect to the space $(\mathcal P(\mathcal X), d_\Sigma)$ instead of $(\mathcal P(\mathcal X), W_1)$, as the identity map $\mathrm{id} \colon (\mathcal P(\mathcal X), d_\Sigma) \to (\mathcal P(\mathcal X), W_1)$ is uniformly continuous (as both $d_\Sigma$ and $W_1$ metrize the topology of weak convergence, and $\mathcal P(\mathcal X)$ is compact), and therefore there exists a modulus of continuity $\tilde \omega$ for the identity map such that for any $\mu, \nu \in (\mathcal P(\mathcal X), d_\Sigma)$, by the prequel
\begin{align*}
    |f(\mu) - f(\nu)| \leq \omega_{\mathcal F}(W_1(\mathrm{id} \, \mu, \mathrm{id} \, \nu)) \leq \omega_{\mathcal F}(\tilde \omega(d_\Sigma(\mu, \nu)))
\end{align*}
with $\tilde \omega_{\mathcal F} \coloneqq \omega_{\mathcal F} \circ \tilde \omega$, which can be replaced by its least concave majorant (again as in \cite{devore1993constructive}, Lemma~6.1).

Therefore, by Jensen's inequality, for \eqref{eq:first} we obtain
\begin{align*}
    &\left| \E \left[ f(\mu^N_{t+1}) - f(T(\mu^N_t, \mu^N_t \otimes \pi_t(\mu^N_t))) \right] \right| \\
    &\quad \leq \E \left[ \tilde \omega_{\mathcal F}(d_\Sigma(\mu^N_{t+1}, T(\mu^N_t, \mu^N_t \otimes \pi_t(\mu^N_t)))) \right] \\
    &\quad \leq \tilde \omega_{\mathcal F} \left( \E \left[ d_\Sigma(\mu^N_{t+1}, T(\mu^N_t, \mu^N_t \otimes \pi_t(\mu^N_t))) \right] \right)
\end{align*}
irrespective of $\pi$, $f$ via concavity of $\tilde \omega_{\mathcal F}$. Introducing for readability $x^N_t \equiv \{x^{i,N}_t\}_{i\in[N]}$, we then obtain
\begin{align*}
    &\E \left[ d_\Sigma(\mu^N_{t+1}, T(\mu^N_t, \mu^N_t \otimes \pi_t(\mu^N_t))) \right] \\
    &\quad = \sum_{m=1}^\infty 2^{-m} \E \left[ \left| \int f_m \, \mathrm d(\mu^N_{t+1} - T(\mu^N_t, \mu^N_t \otimes \pi_t(\mu^N_t))) \right| \right] \\
    &\quad \leq \sup_{m \geq 1} \E \left[ \E \left[ \left| \int f_m \, \mathrm d(\mu^N_{t+1} - T(\mu^N_t, \mu^N_t \otimes \pi_t(\mu^N_t))) \right| \innermid x^N_t \right] \right],
\end{align*}
and by the following weak LLN argument, for the squared term and any $f_m$
\begin{align*}
    &\E \left[ \left| \int f_m \, \mathrm d(\mu^N_{t+1} - T(\mu^N_t, \mu^N_t \otimes \pi_t(\mu^N_t))) \right| \innermid x^N_t \right]^2 \\
    &\quad = \E \left[ \left| \frac 1 N \sum_{i=1}^N \left( f_m(x^{i,N}_{t+1}) - \E \left[ f_m(x^{i,N}_{t+1}) \innermid x^N_t \right] \right) \right| \innermid x^N_t \right]^2 \\
    &\quad \leq \E \left[ \left| \frac 1 N \sum_{i=1}^N \left( f_m(x^{i,N}_{t+1}) - \E \left[ f_m(x^{i,N}_{t+1}) \innermid x^N_t \right] \right) \right|^2 \innermid x^N_t \right] \\
    &\quad = \frac{1}{N^2} \sum_{i=1}^N \E \left[ \left( f_m(x^{i,N}_{t+1}) - \E \left[ f_m(x^{i,N}_{t+1}) \innermid x^N_t \right] \right)^2 \innermid x^N_t \right] \leq \frac{4}{N} \to 0
\end{align*}
by bounding $|f_m| \leq 1$, as the cross-terms are zero by conditional independence of $x^{i,N}_{t+1}$ given $x^N_t$. By the prequel, the term \eqref{eq:first} hence converges to zero.

For the second term \eqref{eq:second}, we have
\begin{align*}
    &\sup_{\pi \in \Pi} \sup_{f \in \mathcal F} \left| \E \left[ f(T(\mu^N_t, \mu^N_t \otimes \pi_t(\mu^N_t))) - f(\mu_{t+1}) \right] \right| \\
    &\quad = \sup_{\pi \in \Pi} \sup_{f \in \mathcal F} \left| \E \left[ f(T(\mu^N_t, \mu^N_t \otimes \pi_t(\mu^N_t))) - f(T(\mu_t, \mu_t \otimes \pi_t(\mu_t))) \right] \right| \\
    &\quad \leq \sup_{\pi \in \Pi} \sup_{g \in \mathcal G} \left| \E \left[ g(\mu^N_t) - g(\mu_t) \right] \right| \to 0
\end{align*}
by the induction assumption, where we defined $g = f \circ \tilde T^{\pi_t}$ from the class $\mathcal G$ of equicontinuous functions with modulus of continuity $\omega_{\mathcal G} \coloneqq \omega_{\mathcal F} \circ \omega_{T}$, where $\omega_T$ denotes the uniform modulus of continuity of $\mu_t \mapsto \tilde T^{\pi_t}(\mu_t) \coloneqq T(\mu_t, \mu_t \otimes \pi_t(\mu_t)))$ over all policies $\pi$. Here, this equicontinuity of $\{ \tilde T^{\pi_t} \}_{\pi \in \Pi}$ follows from Lemma~\ref{lem:Tcont} and the equicontinuity of functions $\mu_t \mapsto \mu_t \otimes \pi_t(\mu_t)$ due to uniformly Lipschitz $\Pi$ as we show in the following, completing the proof by induction:

Consider $\mu_n \to \mu \in \mathcal P(\mathcal X)$, then we have
\begin{align*}
    &\sup_{\pi \in \Pi} W_1(\mu_n \otimes \pi_t(\mu_n), \mu \otimes \pi_t(\mu)) \\
    &\quad = \sup_{\pi \in \Pi} \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} \left| \int f' \, \mathrm d(\mu_n \otimes \pi_t(\mu_n) - \mu \otimes \pi_t(\mu)) \right| \\
    &\quad \leq \sup_{\pi \in \Pi} \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} \left| \iint f'(x, u) (\pi_t(\mathrm du \mid x, \mu_n) - \pi_t(\mathrm du \mid x, \mu)) \mu_n(\mathrm dx) \right| \\
    &\qquad + \sup_{\pi \in \Pi} \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} \left| \iint f'(x, u) \pi_t(\mathrm du \mid x, \mu) (\mu_n(\mathrm dx) - \mu(\mathrm dx)) \right|
\end{align*}
where for the first term
\begin{align*}
    &\sup_{\pi \in \Pi} \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} \left| \iint f'(x, u) (\pi_t(\mathrm du \mid x, \mu_n) - \pi_t(\mathrm du \mid x, \mu)) \mu_n(\mathrm dx) \right| \\
    &\quad \leq \sup_{\pi \in \Pi} \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} \int \left| \int f'(x, u) (\pi_t(\mathrm du \mid x, \mu_n) - \pi_t(\mathrm du \mid x, \mu)) \right| \mu_n(\mathrm dx) \\
    &\quad \leq \sup_{\pi \in \Pi} \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} \sup_{x \in \mathcal X} \left| \int f'(x, u) (\pi_t(\mathrm du \mid x, \mu_n) - \pi_t(\mathrm du \mid x, \mu)) \right| \\
    &\quad = \sup_{\pi \in \Pi} \sup_{x \in \mathcal X} W_1(\pi_t(\cdot \mid x, \mu_n), \pi_t(\cdot \mid x, \mu)) \\
    &\quad \leq L_\Pi W_1(\mu_n, \mu) \to 0
\end{align*}
by Assumption~\ref{ass:picont}, and similarly for the second by first noting $1$-Lipschitzness of $x \mapsto \int \frac {f'(x, u)} {L_\Pi + 1}  \pi_t(\mathrm du \mid x, \mu)$, as for $y \neq x$
\begin{align}
    &\left| \int \frac {f'(y, u)} {L_\Pi + 1}  \pi_t(\mathrm du \mid y, \mu) - \int \frac {f'(x, u)} {L_\Pi + 1}  \pi_t(\mathrm du \mid x, \mu) \right| \nonumber\\
    &\quad \leq \left| \int \frac {f'(y, u) - f'(x, u)} {L_\Pi + 1}  \pi_t(\mathrm du \mid y, \mu) \right| + \left| \int \frac {f'(x, u)} {L_\Pi + 1} (\pi_t(\mathrm du \mid y, \mu) - \pi_t(\mathrm du \mid x, \mu)) \right| \nonumber\\
    &\quad \leq \frac{1}{L_\Pi + 1} d(y, x) + \frac{1}{L_\Pi + 1} W_1(\pi_t(\cdot \mid y, \mu), \pi_t(\cdot \mid x, \mu)) \nonumber\\
    &\quad \leq \left( \frac{1}{L_\Pi + 1} + \frac{L_\Pi}{L_\Pi + 1} \right) d(x, y) \label{eq:lipschitz-fpi}
\end{align}
with $\frac{1}{L_\Pi + 1} + \frac{L_\Pi}{L_\Pi + 1} = 1 \leq 1$, and therefore again
\begin{align*}
    &\sup_{\pi \in \Pi} \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} \left| \iint f'(x, u) \pi_t(\mathrm du \mid x, \mu) (\mu_n(\mathrm dx) - \mu(\mathrm dx)) \right| \\
    &\quad = \sup_{\pi \in \Pi} \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} (L_\Pi + 1) \left| \iint \frac{f'(x, u)}{L_\Pi + 1}  \pi_t(\mathrm du \mid x, \mu) (\mu_n(\mathrm dx) - \mu(\mathrm dx)) \right| \\
    &\quad \leq (L_\Pi + 1) W_1(\mu_n, \mu) \to 0.
\end{align*}
This completes the proof by induction.
\end{proof}


\subsection{Proof of Corollary~\ref{coro:epsopt}} \label{app:coro:epsopt}
\begin{proof}
First, we show that from uniform convergence in Theorem~\ref{thm:muconv}, the finite-agent objectives converge uniformly to the MFC limit.

\begin{lemma} \label{lem:Jconv}
Under Assumptions~\ref{ass:pcont}, \ref{ass:rcont} and \ref{ass:picont}, the finite-agent objective converges uniformly to the MFC limit,
\begin{equation}
    \sup_{\pi \in \Pi} \left| J^N(\pi) - J(\Phi^{-1}(\pi)) \right| \to 0.
\end{equation}
\end{lemma}
\begin{subproof}
    For any $\varepsilon > 0$, choose time $T \in \mathbb N$ such that $\sum_{t=T}^{\infty} \gamma^t \E \left[ \left| r(\mu^N_t) - r(\mu_t) \right| \right] \leq \frac{\gamma^T}{1-\gamma} \max_\mu 2 |r(\mu)| < \frac \varepsilon 2$. By Theorem~\ref{thm:muconv}, $\sum_{t=0}^{T-1} \gamma^t \E \left[ \left| r(\mu^N_t) - r(\mu_t) \right| \right] < \frac \varepsilon 2$ for sufficiently large $N$. The result follows.
\end{subproof}

The approximate optimality of MFC solutions in the finite system follows immediately: By Lemma~\ref{lem:Jconv}, we have
\begin{align*}
    &J^N(\Phi(\pi^*)) - \sup_{\pi \in \Pi} J^N(\pi) = \inf_{\pi \in \Pi} (J^N(\pi^*) - J^N(\pi)) \\
    &\quad \geq \inf_{\pi \in \Pi} (J^N(\Phi(\pi^*)) - J(\pi^*))
    + \inf_{\pi \in \Pi} (J(\pi^*) - J(\Phi^{-1}(\pi))) + \inf_{\pi \in \Pi} (J(\Phi^{-1}(\pi)) - J^N(\pi)) \\
    &\quad \geq - \frac \varepsilon 2 + 0 - \frac \varepsilon 2 = - \varepsilon
\end{align*}
for sufficiently large $N$, where the second term is zero by optimality of $\pi^*$ in the MFC problem.
\end{proof}


\subsection{Proof of Theorem~\ref{thm:m3dpp}} \label{app:thm:m3dpp}
\begin{proof}
The proof is analogous to Appendix~\ref{app:thm:dpp} by first showing the continuity of $T$ (proof further below).

\begin{lemma} \label{lem:m3Tcont}
Under Assumption~\ref{ass:m3pcont}, for any sequence $(x^0_n, u^0_n, \mu_n, \nu_n) \to (x^0, u^0, \mu, \nu) \in \mathcal X^0 \times \mathcal U^0 \times \mathcal P(\mathcal X) \times \mathcal P(\mathcal X \times \mathcal U)$, we have $T(x^0_n, u^0_n, \mu_n, \nu_n) \to T(x^0, u^0, \mu, \nu)$.
\end{lemma}

For \cite{hernandez2012discrete}, Assumption~4.2.1(a), the cost function $-r$ is continuous by Assumption~\ref{ass:m3rcont}, therefore also bounded by compactness of $\mathcal X^0 \times \mathcal P(\mathcal X)$, and finally also inf-compact on the state-action space of the M3FC MDP, since for any $(x^0, \mu) \in \mathcal X^0 \times \mathcal P(\mathcal X)$ the set $\{ (h, u^0) \in \mathcal H(\mu) \times \mathcal U^0 \mid -r(x^0, u^0, \mu) \leq c \}$ is given by $\mathcal H(\mu) \times \tilde r^{-1}((-\infty, c])$, where we defined $\tilde r(u^0) \coloneqq -r(x^0, u^0, \mu)$. Note that $\mathcal H(\mu)$ is compact by the same argument as in Appendix~\ref{app:thm:dpp}, while $\tilde r$ is continuous by Assumption~\ref{ass:m3rcont} and therefore its preimage of the closed set $(-\infty, c]$ is compact.

For \cite{hernandez2012discrete}, Assumption~4.2.1(b), consider any continuous and bounded $f \colon \mathcal X^0 \times \mathcal P(\mathcal X) \to \mathbb R$. The continuity is uniform by compactness. Hence, $\sup_{x' \in \mathcal X^0} \left| f(x', \mu'_n) - f(x', \mu') \right| \to 0$ as $\mu'_n \to \mu' \in \mathcal P(\mathcal X)$. Thus, whenever $(x^0_n, u^0_n, \mu_n, \nu_n) \to (x^0, u^0, \mu, \nu) \in \mathcal X^0 \times \mathcal U^0 \times \mathcal P(\mathcal X) \times \mathcal P(\mathcal X \times \mathcal U)$, we have
\begin{align*}
    &\left| \iint f(x', \mu) \, \delta_{x^0_n, T(x^0_n, u^0_n, \mu_n, \nu_n)}(\mathrm d\mu') \, p^0(\mathrm dx' \mid x^0_n, u^0_n, \mu_n) - \iint f(x', \mu) \, \delta_{x^0, T(x^0, u^0, \mu, \nu)}(\mathrm d\mu') \, p^0(\mathrm dx' \mid x^0, u^0, \mu) \right| \\
    &\quad = \left| \int f(x', T(x^0_n, u^0_n, \mu_n, \nu_n)) \, p^0(\mathrm dx' \mid x^0_n, u^0_n, \mu_n) - \int f(x', T(x^0, u^0, \mu, \nu)) \, p^0(\mathrm dx' \mid x^0, u^0, \mu) \right| \\
    &\quad \leq \left| \int f(x', T(x^0_n, u^0_n, \mu_n, \nu_n)) \, p^0(\mathrm dx' \mid x^0_n, u^0_n, \mu_n) - \int f(x', T(x^0, u^0, \mu, \nu)) \, p^0(\mathrm dx' \mid x^0_n, u^0_n, \mu_n) \right| \\
    &\qquad + \left| \int f(x', T(x^0, u^0, \mu, \nu)) \, p^0(\mathrm dx' \mid x^0_n, u^0_n, \mu_n) - \int f(x', T(x^0, u^0, \mu, \nu)) \, p^0(\mathrm dx' \mid x^0, u^0, \mu) \right| \\
    &\quad \leq \sup_{x' \in \mathcal X^0} \left| f(x', T(x^0_n, u^0_n, \mu_n, \nu_n)) - f(x', T(x^0, u^0, \mu, \nu)) \right| \\
    &\qquad + \left| \int \tilde f(x') \, p^0(\mathrm dx' \mid x^0_n, u^0_n, \mu_n) - \int \tilde f(x') \, p^0(\mathrm dx' \mid x^0, u^0, \mu) \right| \to 0
\end{align*}
for the first term by the prequel where $T(x^0_n, u^0_n, \mu_n, \nu_n) \to T(x^0, u^0, \mu, \nu)$ by Lemma~\ref{lem:m3Tcont}, and for the second term by applying Assumption~\ref{ass:m3pcont} to $\tilde f(x') \coloneqq f(x', T(x^0, u^0, \mu, \nu))$. This shows weak continuity of the dynamics.

Furthermore, the M3FC MDP fulfills \cite{hernandez2012discrete}, Assumption~4.2.2 by boundedness of $r$ from Assumption~\ref{ass:m3rcont}. Therefore, the desired statement follows from \cite{hernandez2012discrete}, Theorem~4.2.3.
\end{proof}

\paragraph{Proof of Lemma~\ref{lem:m3Tcont}}
\begin{proof}
To show $T(x^0_n, u^0_n, \mu_n, \nu_n) \to T(x^0, u^0, \mu, \nu)$, consider any Lipschitz and bounded $f$ with Lipschitz constant $L_f$, then
\begin{align*}
    &\left| \int f \, \mathrm d(T(x^0_n, u^0_n, \mu_n, \nu_n) - T(x^0, u^0, \mu, \nu)) \right| \\
    &= \left| \iiint f(x') p(\mathrm dx' \mid x, u, x^0_n, u^0_n, \mu_n) \nu_n(\mathrm dx, \mathrm du) - \iiint f(x') p(\mathrm dx' \mid x, u, x^0, u^0, \mu) \nu(\mathrm dx, \mathrm du) \right| \\
    &\quad \leq \iint \left| \int f(x') p(\mathrm dx' \mid x, u, x^0_n, u^0_n, \mu_n) - \int f(x') p(\mathrm dx' \mid x, u, x^0, u^0, \mu) \right| \nu_n(\mathrm dx, \mathrm du) \\
    &\qquad + \left| \iiint f(x') p(\mathrm dx' \mid x, u, x^0, u^0, \mu) (\nu_n(\mathrm dx, \mathrm du) - \nu(\mathrm dx, \mathrm du)) \right| \\
    &\quad \leq \sup_{x \in \mathcal X, u \in \mathcal U} L_f W_1(p(\cdot \mid x, u, x^0_n, u^0_n, \mu_n), p(\cdot \mid x, u, x^0, u^0, \mu)) \\
    &\qquad + \left| \iiint f(x') p(\mathrm dx' \mid x, u, x^0, u^0, \mu) (\nu_n(\mathrm dx, \mathrm du) - \nu(\mathrm dx, \mathrm du)) \right| \to 0
\end{align*}
for the first term by $1$-Lipschitzness of $\frac{f}{L_f}$ and Assumption~\ref{ass:m3pcont} (with compactness implying the uniform continuity), and for the second by $\nu_n \to \nu$ and continuity of $(x,u) \mapsto \iint f(x') p(\mathrm dx' \mid x, u, x^0, u^0, \mu)$ by the same argument.
\end{proof}


\subsection{Proof of Theorem~\ref{thm:m3muconv}} \label{app:thm:m3muconv}
\begin{proof}
The statement \eqref{eq:m3muconv} is shown inductively over $t \geq 0$. At time $t=0$, \eqref{eq:m3muconv} holds by the weak LLN argument, see also the first term below. Assuming \eqref{eq:m3muconv} at time $t$, then for time $t+1$ we have
\begin{align}
    &\sup_{(\pi, \pi^0) \in \Pi \times \Pi^0} \sup_{f \in \mathcal F} \left| \E \left[ f(x^{0,N}_{t+1}, u^{0,N}_{t+1}, \mu^N_{t+1}) - f(x^0_{t+1}, u^0_{t+1}, \mu_{t+1}) \right] \right| \nonumber \\
    &\quad \leq \sup_{\pi, \pi^0} \sup_{f \in \mathcal F} \left| \E \left[ f(x^{0,N}_{t+1}, u^{0,N}_{t+1}, \mu^N_{t+1}) - f(x^{0,N}_{t+1}, u^{0,N}_{t+1}, \hat \mu^N_{t+1}) \right] \right| \label{eq:m3first} \\
    &\qquad + \sup_{\pi, \pi^0} \sup_{f \in \mathcal F} \left| \E \left[ f(x^{0,N}_{t+1}, u^{0,N}_{t+1}, \hat \mu^N_{t+1}) - f(x^0_{t+1}, u^0_{t+1}, \mu_{t+1}) \right] \right| \label{eq:m3second}
\end{align}
where for readability, we again write $\pi_t(x^0_t, \mu_t) \coloneqq \pi_t(\cdot \mid \cdot, x^0_t, \mu_t)$ and introduce the random variable
\begin{align*}
    \hat \mu^N_{t+1} \coloneqq T(x^{0,N}_{t}, u^{0,N}_{t}, \mu^N_t, \mu^N_t \otimes \pi_t(x^{0,N}_{t}, \mu^N_t)).
\end{align*}

By compactness of $\mathcal X^0 \times \mathcal U^0 \times \mathcal P(\mathcal X)$, $\mathcal F$ is uniformly equicontinuous, and hence admits a non-decreasing, concave (as in \cite{devore1993constructive}, Lemma~6.1) modulus of continuity $\omega_{\mathcal F} \colon [0, \infty) \to [0, \infty)$ where $\omega_{\mathcal F}(x) \to 0$ as $x \to 0$ and $|f(x, u, \mu) - f(x', u', \nu)| \leq \omega_{\mathcal F}(d(x, x') + d(u, u') + W_1(\mu, \nu))$ for all $f \in \mathcal F$, and analogously there exists such $\tilde \omega_{\mathcal F}$ with respect to $(\mathcal P(\mathcal X), d_\Sigma)$ instead of $(\mathcal P(\mathcal X), W_1)$ as in Appendix~\ref{app:thm:muconv}. 

For the first term \eqref{eq:m3first}, let $x^N_t \equiv \{x^{i,N}_t\}_{i\in[N]}$. Then, by the weak LLN argument,
\begin{align}
    &\sup_{\pi, \pi^0} \sup_{f \in \mathcal F} \left| \E \left[ f(x^{0,N}_{t+1}, u^{0,N}_{t+1}, \mu^N_{t+1}) - f(x^{0,N}_{t+1}, u^{0,N}_{t+1}, \hat \mu^N_{t+1}) \right] \right| \nonumber\\
    &\quad \leq \sup_{\pi, \pi^0} \E \left[ \tilde \omega_{\mathcal F}(d_\Sigma(\mu^N_{t+1}, \hat \mu^N_{t+1})) \right] \nonumber\\
    &\quad \leq \sup_{\pi, \pi^0} \tilde \omega_{\mathcal F} \left( \sum_{m=1}^\infty 2^{-m} \E \left[ \left| \mu^N_{t+1}(f_m) - \hat \mu^N_{t+1}(f_m) \right| \right] \right) \nonumber\\
    &\quad \leq \sup_{\pi, \pi^0} \tilde \omega_{\mathcal F} \left(  \sup_{m \geq 1} \E \left[ \E \left[ \left| \mu^N_{t+1}(f_m) - \hat \mu^N_{t+1}(f_m) \right| \innermid x^{0,N}_{t}, u^{0,N}_{t}, x^N_t \right] \right] \right) \nonumber\\
    &\quad = \sup_{\pi, \pi^0} \tilde \omega_{\mathcal F} \left( \sup_{m \geq 1} \E \left[ \E \left[ \left| \frac 1 N \sum_{i=1}^N \left( f_m(x^{i,N}_{t+1}) - \E \left[ f_m(x^{i,N}_{t+1}) \innermid x^{0,N}_{t}, u^{0,N}_{t}, x^N_t \right] \right) \right| \innermid x^{0,N}_{t}, u^{0,N}_{t}, x^N_t \right] \right] \right) \nonumber\\
    &\quad \leq \sup_{\pi, \pi^0} \tilde \omega_{\mathcal F} \left( \sup_{m \geq 1} \E \left[ \E \left[ \left| \frac 1 N \sum_{i=1}^N \left( f_m(x^{i,N}_{t+1}) - \E \left[ f_m(x^{i,N}_{t+1}) \innermid x^{0,N}_{t}, u^{0,N}_{t}, x^N_t \right] \right) \right|^2 \innermid x^{0,N}_{t}, u^{0,N}_{t}, x^N_t \right] \right]^{1/2} \right) \nonumber\\
    &\quad = \sup_{\pi, \pi^0} \tilde \omega_{\mathcal F} \left( \sup_{m \geq 1} \left( \frac{1}{N^2} \sum_{i=1}^N \E \left[ \E \left[ \left( f_m(x^{i,N}_{t+1}) - \E \left[ f_m(x^{i,N}_{t+1}) \innermid x^{0,N}_{t}, u^{0,N}_{t}, x^N_t \right] \right)^2 \innermid x^{0,N}_{t}, u^{0,N}_{t}, x^N_t \right] \right] \right)^{1/2} \right) \nonumber\\
    &\quad \leq \tilde \omega_{\mathcal F} \left( \frac{2}{\sqrt N} \right) \to 0 \label{eq:m3dconv}
\end{align}
by bounding $|f_m| \leq 1$, as the cross-terms disappear.

For the second term \eqref{eq:m3second}, by noting $\hat \mu^N_{t+1} = T(x^{0,N}_{t}, u^{0,N}_{t}, \mu^N_t, \mu^N_t \otimes \pi_t(x^{0,N}_{t}, \mu^N_t))$, we have
\begin{align}
    &\sup_{\pi, \pi^0} \sup_{f \in \mathcal F} \left| \E \left[ f(x^{0,N}_{t+1}, u^{0,N}_{t+1}, \hat \mu^N_{t+1}) - f(x^0_{t+1}, u^0_{t+1}, \mu_{t+1}) \right] \right| \nonumber \\
    &\quad = \sup_{\pi, \pi^0} \sup_{f \in \mathcal F} \left| \E \left[ \iint f(x', u', \hat \mu^N_{t+1}) \pi^0_t(\mathrm du' \mid x', \mu^N_{t+1}) p^0(\mathrm dx' \mid x^{0,N}_{t}, u^{0,N}_{t}, \mu^N_t) \nonumber
    \right.\right.\\&\hspace{3cm}\left.\left. 
    - \iint f(x', u', \mu_{t+1}) \pi^0_t(\mathrm du' \mid x', \mu_{t+1}) p^0(\mathrm dx' \mid x^0_{t}, u^{0}_{t}, \mu_t) \right] \right| \nonumber \\
    &\quad \leq \sup_{\pi, \pi^0} \sup_{f \in \mathcal F} \E \left[ \sup_{x'} \left| \int f(x', u', \hat \mu^N_{t+1}) (\pi^0_t(\mathrm du' \mid x', \mu^N_{t+1}) - \pi^0_t(\mathrm du' \mid x', \hat \mu^N_{t+1})) \right| \right] \label{eq:m3q2} \\
    &\qquad + \sup_{\pi, \pi^0} \sup_{g \in \mathcal G} \left| \E \left[ g(x^{0,N}_{t}, u^{0,N}_{t}, \mu^N_t) - g(x^0_{t}, u^0_{t}, \mu_t) \right] \right| \label{eq:m3q3}
\end{align}
and analyze each term separately, where we defined the function $g \colon \mathcal X^0 \times \mathcal U^0 \times \mathcal P(\mathcal X)$ as
\begin{align*}
    g(x^0, u^0, \mu) \coloneqq \iint f(x', u', T(x^0, u^0, \mu, \mu \otimes \pi_t(x^0, \mu))) \pi^0_t(\mathrm du' \mid x', T(x^0, u^0, \mu, \mu \otimes \pi_t(x^0, \mu))) p^0(\mathrm dx' \mid x^0, u^0, \mu)
\end{align*}
from the class $\mathcal G$ of such functions for any policies $\pi, \pi^0$.

For \eqref{eq:m3q2}, defining a modulus of continuity $\tilde \omega_{\Pi^0}$ for $\Pi^0$ as for $\mathcal F$, we have
\begin{align*}
    &\sup_{\pi, \pi^0} \sup_{f \in \mathcal F} \E \left[ \sup_{x'} \left| \int f(x', u', \hat \mu^N_{t+1}) (\pi^0_t(\mathrm du' \mid x', \mu^N_{t+1}) - \pi^0_t(\mathrm du' \mid x', \hat \mu^N_{t+1})) \right| \right] \\
    &\quad \leq \sup_{\pi, \pi^0} \E \left[ L_{\mathcal F} \sup_{x'} W_1(\pi^0_t(\cdot \mid x', \mu^N_{t+1}), \pi^0_t(\cdot \mid x', \hat \mu^N_{t+1})) \right] \\
    &\quad \leq \sup_{\pi, \pi^0} \E \left[ L_{\mathcal F} \tilde \omega_{\Pi^0}(d_\Sigma(\mu^N_{t+1}, \hat \mu^N_{t+1})) \right] \leq L_{\mathcal F} \tilde \omega_{\Pi^0} \left( \frac{2}{\sqrt N} \right) \to 0.
\end{align*}

Lastly, for \eqref{eq:m3q3}, we first note that the class $\mathcal G$ of functions is equi-Lipschitz. 

\begin{lemma} \label{lem:app:Tlip}
Under Assumptions~\ref{ass:m3pcont} and \ref{ass:m3picont}, the map $(x^0, u^0, \mu) \mapsto T(x^0, u^0, \mu, \mu \otimes \pi_t(x^0, \mu))$ is Lipschitz with constant $L_T \coloneqq (2L_\Pi + 1) \cdot (L_p + (L_p + 1) L_\Pi + (L_p + L_\Pi + 1))$.
\end{lemma}

\begin{lemma} \label{lem:app:glip}
Under Assumptions~\ref{ass:m3pcont} and \ref{ass:m3picont}, for any equi-Lipschitz $\mathcal F$ with constant $L_{\mathcal F}$, the function class $\mathcal G$ is equi-Lipschitz with constant $L_{\mathcal G} \coloneqq (L_{\mathcal F} L_T + L_{\mathcal F} L_{\Pi^0} L_T + L_{\mathcal F} L_{\Pi} L_{p^0})$.
\end{lemma}

Therefore, for \eqref{eq:m3q3}, we have
\begin{align*}
    \sup_{\pi, \pi^0} \sup_{g \in \mathcal G} \left| \E \left[ g(x^{0,N}_{t}, u^{0,N}_{t}, \mu^N_t) - g(x^0_{t}, u^0_{t}, \mu_t) \right] \right| \to 0
\end{align*}
by the induction assumption over the class $\mathcal G$ of equi-Lipschitz functions, completing the proof by induction. The existence of independent optimal $\pi$, $\pi^0$ follows from Remark~\ref{remark:joint}.
\end{proof}

Finally, we give the proofs of the lemmas used in the prequel.

\subsubsection{Proof of Lemma~\ref{lem:app:Tlip}}
\begin{proof}
First note Lipschitz continuity of $(x^0, \mu) \mapsto \mu \otimes \pi_t(x^0, \mu)$ as in Appendix~\ref{app:thm:muconv}, as for any $(x^0_*, \mu_*), (x^0, \mu) \in \mathcal X^0 \times \mathcal P(\mathcal X)$, then
\begin{align*}
    &\sup_{\pi \in \Pi} W_1(\mu_* \otimes \pi_t(x^0_*, \mu_*), \mu \otimes \pi_t(x^0, \mu)) \\
    &\quad = \sup_{\pi \in \Pi} \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} \left| \int f' \, \mathrm d(\mu_* \otimes \pi_t(x^0_*, \mu_*) - \mu \otimes \pi_t(x^0, \mu)) \right| \\
    &\quad \leq \sup_{\pi \in \Pi} \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} \left| \iint f'(x, u) (\pi_t(\mathrm du \mid x, x^0_*, \mu_*) - \pi_t(\mathrm du \mid x, x^0, \mu)) \mu_*(\mathrm dx) \right| \\
    &\qquad + \sup_{\pi \in \Pi} \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} \left| \iint f'(x, u) \pi_t(\mathrm du \mid x, x^0, \mu) (\mu_*(\mathrm dx) - \mu(\mathrm dx)) \right|
\end{align*}
where for the first term
\begin{align*}
    &\sup_{\pi \in \Pi} \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} \left| \iint f'(x, u) (\pi_t(\mathrm du \mid x, x^0_*, \mu_*) - \pi_t(\mathrm du \mid x, x^0, \mu)) \mu_*(\mathrm dx) \right| \\
    &\quad \leq \sup_{\pi \in \Pi} \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} \int \left| \int f'(x, u) (\pi_t(\mathrm du \mid x, x^0_*, \mu_*) - \pi_t(\mathrm du \mid x, x^0, \mu)) \right| \mu_*(\mathrm dx) \\
    &\quad \leq \sup_{\pi \in \Pi} \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} \sup_{x \in \mathcal X} \left| \int f'(x, u) (\pi_t(\mathrm du \mid x, x^0_*, \mu_*) - \pi_t(\mathrm du \mid x, x^0, \mu)) \right| \\
    &\quad = \sup_{\pi \in \Pi} \sup_{x \in \mathcal X} W_1(\pi_t(\cdot \mid x, x^0_*, \mu_*), \pi_t(\cdot \mid x, x^0, \mu)) \\
    &\quad \leq L_\Pi d((x^0_*, \mu_*), (x^0, \mu))
\end{align*}
by Assumption~\ref{ass:m3picont}, and similarly for the second by noting $1$-Lipschitzness of $x \mapsto \int \frac {f'(x, u)} {L_\Pi + 1}  \pi_t(\mathrm du \mid x, x^0, \mu)$, as before in \eqref{eq:lipschitz-fpi}, and therefore again
\begin{align*}
    &\sup_{\pi \in \Pi} \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} \left| \iint f'(x, u) \pi_t(\mathrm du \mid x, x^0, \mu) (\mu_*(\mathrm dx) - \mu(\mathrm dx)) \right| \\
    &\quad = \sup_{\pi \in \Pi} \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} (L_\Pi + 1) \left| \iint \frac{f'(x, u)}{L_\Pi + 1}  \pi_t(\mathrm du \mid x, x^0, \mu) (\mu_*(\mathrm dx) - \mu(\mathrm dx)) \right| \\
    &\quad \leq (L_\Pi + 1) W_1(\mu_*, \mu).
\end{align*}
Hence, the map $(x^0, u^0, \mu) \mapsto \mu \otimes \pi_t(x^0, \mu)$ is Lipschitz with constant $(2L_\Pi + 1)$. 

As a result, the entire map $(x^0, u^0, \mu) \mapsto T(x^0, u^0, \mu, \mu \otimes \pi_t(x^0, \mu)$ is Lipschitz, since for any 
\begin{align*}
    &W_1(T(x^0_*, u^0_*, \mu_*, \mu_* \otimes \pi_t(x^0_*, \mu_*)), T(x^0, u^0, \mu, \mu \otimes \pi_t(x^0, \mu)) \\
    &\quad = \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} \left| \iiint f'(x') p(\mathrm dx' \mid x, u, x^0_*, u^0_*, \mu_*) \pi_t(\mathrm du \mid x, x^0_*, \mu_*) \mu_*(\mathrm dx)
    \right.\nonumber\\&\hspace{2.5cm}\left.
    - \iiint f'(x') p(\mathrm dx' \mid x, u, x^0, u^0, \mu) \pi_t(\mathrm du \mid x, x^0, \mu) \mu(\mathrm dx) \right| \\
    &\quad \leq \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} \sup_{(x,u) \in \mathcal X \times \mathcal U} \left| \int f'(x') (p(\mathrm dx' \mid x, u, x^0_*, u^0_*, \mu_*) - p(\mathrm dx' \mid x, u, x^0, u^0, \mu)) \right| \\
    &\qquad + \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} \sup_{x \in \mathcal X} \left| \iint f'(x') p(\mathrm dx' \mid x, u, x^0, u^0, \mu) (\pi_t(\mathrm du \mid x, x^0_*, \mu_*) - \pi_t(\mathrm du \mid x, x^0, \mu)) \right| \\
    &\qquad + \sup_{\lVert f' \rVert_{\mathrm{Lip}} \leq 1} \left| \iiint f'(x') p(\mathrm dx' \mid x, u, x^0, u^0, \mu) \pi_t(\mathrm du \mid x, x^0, \mu) (\mu_*(\mathrm dx) - \mu(\mathrm dx)) \right| \\
    &\quad \leq \sup_{(x,u) \in \mathcal X \times \mathcal U} W_1(p(\cdot \mid x, u, x^0_*, u^0_*, \mu_*), p(\cdot \mid x, u, x^0, u^0, \mu)) \\
    &\qquad + \sup_{x \in \mathcal X} (L_p + 1) W_1(\pi_t(\cdot \mid x, x^0_*, \mu_*), \pi_t(\cdot \mid x, x^0, \mu)) \\
    &\qquad + \sup_{(x,u) \in \mathcal X \times \mathcal U} (L_p + L_\Pi + 1) W_1(\mu_*, \mu) \\
    &\quad \leq \underbrace{(L_p + (L_p + 1) L_\Pi + (L_p + L_\Pi + 1))}_{L_*} d((x^0_*, u^0_*, \mu_*), (x^0, u^0, \mu))
\end{align*}
with Lipschitz constant $L_T \coloneqq (2L_\Pi + 1) \cdot L_*$ from Assumptions~\ref{ass:m3pcont} and \ref{ass:m3picont}, using the same argument as in \eqref{eq:lipschitz-fpi}. 
\end{proof}

\subsubsection{Proof of Lemma~\ref{lem:app:glip}}
\begin{proof}
For any $g \in \mathcal G$, for any $(x^0_*, u^0_*, \mu_*), (x^0, u^0, \mu) \in \mathcal X^0 \times \mathcal U^0 \times \mathcal P(\mathcal X)$, we have
\begin{align}
    &\left| g(x^0_*, u^0_*, \mu_*) - g(x^0, u^0, \mu) \right| \nonumber\\
    &\quad = \left| \iint f(x', u', T(x^0_*, u^0_*, \mu_*, \mu_* \otimes \pi_t(x^0_*, \mu_*))) \pi^0_t(\mathrm du' \mid x', T(x^0_*, u^0_*, \mu_*, \mu_* \otimes \pi_t(x^0_*, \mu_*))) p^0(\mathrm dx' \mid x^0_*, u^0_*, \mu_*)
    \right.\nonumber\\&\hspace{1.5cm}\left.
    - \iint f(x', u', T(x^0, u^0, \mu, \mu \otimes \pi_t(x^0, \mu))) \pi^0_t(\mathrm du' \mid x', T(x^0, u^0, \mu, \mu \otimes \pi_t(x^0, \mu))) p^0(\mathrm dx' \mid x^0, u^0, \mu) \right| \nonumber\\
    &\quad \leq \sup_{x', u'} \left| f(x', u', T(x^0_*, u^0_*, \mu_*, \mu_* \otimes \pi_t(x^0_*, \mu_*))) - f(x', u', T(x^0, u^0, \mu, \mu \otimes \pi_t(x^0, \mu))) \right| \label{eq:g1}\\
    &\qquad + \sup_{x'} \left| \int f(x', u', T(x^0, u^0, \mu, \mu \otimes \pi_t(x^0, \mu))) 
    \right.\nonumber\\&\hspace{2.5cm}\left.
    (\pi^0_t(\mathrm du' \mid x', T(x^0_*, u^0_*, \mu_*, \mu_* \otimes \pi_t(x^0_*, \mu_*))) - \pi^0_t(\mathrm du' \mid x', T(x^0, u^0, \mu, \mu \otimes \pi_t(x^0, \mu)))) \right| \label{eq:g2}\\
    &\qquad + \left| \iint f(x', u', T(x^0, u^0, \mu, \mu \otimes \pi_t(x^0, \mu))) \pi^0_t(\mathrm du' \mid x', T(x^0, u^0, \mu, \mu \otimes \pi_t(x^0, \mu)))
    \right.\nonumber\\&\hspace{2.5cm}\left.
    (p^0(\mathrm dx' \mid x^0_*, u^0_*, \mu_*) - p^0(\mathrm dx' \mid x^0, u^0, \mu)) \right| \label{eq:g3}.
\end{align}

By Lemma~\ref{lem:app:Tlip}, for \eqref{eq:g1} we obtain
\begin{align*}
    &\sup_{x', u'} \left| f(x', u', T(x^0_*, u^0_*, \mu_*, \mu_* \otimes \pi_t(x^0_*, \mu_*))) - f(x', u', T(x^0, u^0, \mu, \mu \otimes \pi_t(x^0, \mu))) \right| \\
    &\quad \leq L_{\mathcal F} L_T d((x^0_*, u^0_*, \mu_*), (x^0, u^0, \mu)).
\end{align*}

Similarly for \eqref{eq:g2}, by Assumption~\ref{ass:m3picont} we analogously have 
\begin{align*}
    &\sup_{x'} \left| \int f(x', u', T(x^0, u^0, \mu, \mu \otimes \pi_t(x^0, \mu))) 
    \right.\nonumber\\&\hspace{2.5cm}\left.
    (\pi^0_t(\mathrm du' \mid x', T(x^0_*, u^0_*, \mu_*, \mu_* \otimes \pi_t(x^0_*, \mu_*))) - \pi^0_t(\mathrm du' \mid x', T(x^0, u^0, \mu, \mu \otimes \pi_t(x^0, \mu)))) \right| \\
    &\quad \leq L_{\mathcal F} W_1(\pi^0_t(\cdot \mid x', T(x^0_*, u^0_*, \mu_*, \mu_* \otimes \pi_t(x^0_*, \mu_*))), \pi^0_t(\cdot' \mid x', T(x^0, u^0, \mu, \mu \otimes \pi_t(x^0, \mu))) \\
    &\quad \leq L_{\mathcal F} L_{\Pi^0} L_T d((x^0_*, u^0_*, \mu_*), (x^0, u^0, \mu)).
\end{align*}

Lastly, for \eqref{eq:g3}, as before in \eqref{eq:lipschitz-fpi}, by Assumption~\ref{ass:m3pcont} and \ref{ass:m3picont} we have again
\begin{align*}
    &\left| \iint f(x', u', T(x^0, u^0, \mu, \mu \otimes \pi_t(x^0, \mu))) \pi^0_t(\mathrm du' \mid x', T(x^0, u^0, \mu, \mu \otimes \pi_t(x^0, \mu)))
    \right.\nonumber\\&\hspace{2.5cm}\left.
    (p^0(\mathrm dx' \mid x^0_*, u^0_*, \mu_*) - p^0(\mathrm dx' \mid x^0, u^0, \mu)) \right| \\
    &\quad \leq L_{\mathcal F} L_{\Pi} W_1(p^0(\cdot \mid x^0_*, u^0_*, \mu_*), p^0(\cdot \mid x^0, u^0, \mu)) \\
    &\quad \leq L_{\mathcal F} L_{\Pi} L_{p^0} d((x^0_*, u^0_*, \mu_*), (x^0, u^0, \mu)).
\end{align*}

Therefore, $\mathcal G$ is equi-Lipschitz with Lipschitz constant $(L_{\mathcal F} L_T + L_{\mathcal F} L_{\Pi^0} L_T + L_{\mathcal F} L_{\Pi} L_{p^0})$.
\end{proof}

\subsection{Proof of Corollary~\ref{coro:m3epsopt}} \label{app:coro:m3epsopt}
\begin{proof}
As in Lemma~\ref{lem:Jconv}, for any $\varepsilon > 0$, choose time $T \in \mathbb N$ such that 
\begin{align*}
    \sum_{t=T}^{\infty} \gamma^t \E \left[ \left| r(x^{0,N}_t, u^{0,N}_{t}, \mu_t^N) - r(x^0_t, u^0_{t}, \mu_t) \right| \right] \leq \frac{\gamma^T}{1-\gamma} \max_\mu 2 |r(\mu)| < \frac \varepsilon 2.
\end{align*}
By Theorem~\ref{thm:m3muconv}, 
\begin{align*}
    \sum_{t=0}^{T-1} \gamma^t \E \left[ \left| r(x^{0,N}_t, u^{0,N}_{t}, \mu_t^N) - r(x^0_t, u^0_{t}, \mu_t) \right| \right] < \frac \varepsilon 2
\end{align*}
for sufficiently large $N$. Therefore, $\sup_{(\pi, \pi^0) \in \Pi \times \Pi^0} \left| J^N(\pi, \pi^0) - J(\Phi^{-1}(\pi), \pi^0) \right| \to 0$. 

As a result, we have
\begin{align*}
    J^N(\Phi(\pi^*), \pi^{0*}) - \sup_{(\pi, \pi^0) \in \Pi \times \Pi^0} J^N(\pi, \pi^0)
    &= \inf_{(\pi, \pi^0) \in \Pi \times \Pi^0} (J^N(\Phi(\pi^*), \pi^{0*}) - J^N(\pi, \pi^0)) \\
    &\geq \inf_{(\pi, \pi^0) \in \Pi \times \Pi^0} (J^N(\Phi(\pi^*), \pi^{0*}) - J(\pi^*, \pi^{0*})) \\
    &\quad + \inf_{(\pi, \pi^0) \in \Pi \times \Pi^0} (J(\pi^*, \pi^{0*}) - J(\pi, \pi^0)) \\
    &\quad + \inf_{(\pi, \pi^0) \in \Pi \times \Pi^0} (J(\pi, \pi^0) - J^N(\pi, \pi^0)) \\
    &\geq - \frac \varepsilon 2 + 0 - \frac \varepsilon 2 = - \varepsilon
\end{align*}
for sufficiently large $N$, where the second term is zero by optimality of $(\pi^*, \pi^{0*})$ in the M3FC problem.
\end{proof}


\section{Results for MFC with Major States} \label{app:msmfc}
For convenience, we restate the results for MFC with major players instead for only major states. We have the finite MFC system with major states
\begin{subequations} \label{eq:msmmdp}
\begin{align}
    u^{i,N}_t &\sim \pi_t(u^{i,N}_t \mid x^{i,N}_t, x^{0,N}_t, \mu_t^N), \\
    x^{i,N}_{t+1} &\sim p(x^{i,N}_{t+1} \mid x^{i,N}_t, u^{i,N}_t, x^{0,N}_t, \mu_t^N), \\
    x^{0,N}_{t+1} &\sim p^0(x^{0,N}_{t+1} \mid x^{0,N}_t, \mu_t^N), \\
    J^N(\pi) &= \E \left[ \sum_{t=0}^{\infty} \gamma^t r(x^{0,N}_t, \mu^N_t) \right]
\end{align}
\end{subequations}
analogous to \eqref{eq:mmdp}, and the corresponding limiting MFC MDP with major states analogous to \eqref{eq:mfc},
\begin{subequations} \label{eq:msmfc}
\begin{align}
    h_t &\sim \hat \pi(h_t \mid x^0_t, \mu_t), \\ 
    \mu_{t+1} &= T(x^0_t, \mu_t, h_t), \\
    x^0_{t+1} &\sim p^0(x^0_{t+1} \mid x^0_t, \mu_t), \\
    J(\hat \pi) &= \E \left[ \sum_{t=0}^{\infty} \gamma^t r(x^0_t, \mu_t) \right]
\end{align}
\end{subequations}
with $T(x^0, \mu, h) \coloneqq \iint p(\cdot \mid x, u, x^0, \mu) h(\mathrm dx, \mathrm du)$.

\begin{assumption} \label{ass:mspcont}
The transition kernels $p$, $p^0$ are Lipschitz continuous with constants $L_p$, $L_{p^0}$.
\end{assumption}
\begin{assumption} \label{ass:msrcont}
The reward $r$ is Lipschitz continuous.
\end{assumption}
\begin{assumption} \label{ass:mspicont}
The class of policies $\Pi$ are equi-Lipschitz, i.e. there exists $L_\Pi > 0$ such that for all $t$ and $\pi \in \Pi$, $\pi_t \in \mathcal P(\mathcal U)^{\mathcal X \times \mathcal P(\mathcal X)}$ is $L_\Pi$-Lipschitz.
\end{assumption}

\begin{theorem} \label{thm:msdpp}
Under Assumptions~\ref{ass:mspcont} and \ref{ass:msrcont}, there exists an optimal stationary, deterministic policy $\hat \pi$ for the MFC MDP \eqref{eq:msmfc} by choosing $\hat \pi(x^0, \mu)$ from the maximizers of $\argmax_{h \in \mathcal H(\mu)} r(x^0, \mu) + \gamma \mathbb E_{y^0 \sim p^0(y^0 \mid x^0, \mu)} V^*(y^0, T(x^0, \mu, h))$, with $V^*$ the unique fixed point of the Bellman equation $V^*(x^0, \mu) = \max_{h \in \mathcal H(\mu)} r(x^0, \mu) + \gamma \mathbb E_{y^0 \sim p^0(y^0 \mid x^0, \mu)} V^*(y^0, T(x^0, \mu, h))$ (value function).
\end{theorem}

\begin{theorem} \label{thm:msmuconv}
Fix any family of equi-Lipschitz functions $\mathcal F \subseteq \mathbb R^{\mathcal X^0 \times \mathcal P(\mathcal X)}$ with shared Lipschitz constant $L_{\mathcal F}$ for all $f \in \mathcal F$. Under Assumptions~\ref{ass:mspcont} and \ref{ass:mspicont}, the random variable $(x^{0,N}_t, \mu_t^N)$ converges weakly, uniformly over $\mathcal F$, $\Pi$, to $(x^0_t, \mu_t)$ at all times $t \in \mathbb N$,
\begin{equation} \label{eq:msmuconv}
    \sup_{\pi \in \Pi} \sup_{f \in \mathcal F} \left| \E \left[ f(x^{0,N}_t, \mu_t^N) - f(x^0_t, \mu_t) \right] \right| \to 0.
\end{equation}
\end{theorem}

\begin{corollary} \label{coro:msepsopt}
Under Assumptions~\ref{ass:mspcont}, \ref{ass:msrcont} and \ref{ass:mspicont}, optimal deterministic MFC policies $\pi^* \in \argmax_{\pi} J(\pi)$ result in $\varepsilon$-optimal policies $\Phi(\pi^*)$ in the finite-agent problem with $\varepsilon \to 0$ as $N \to \infty$,
\begin{equation}
    J^N(\Phi(\pi^*)) \geq \sup_{\pi \in \Pi} J^N(\pi) - \varepsilon.
\end{equation}
\end{corollary}

The proofs and interpretation are directly analogous to the M3FC case in Appendix~\ref{app:proofs} by leaving out the major agent actions, or alternatively using the M3FC results with a trivial singleton major action space, $|\mathcal U^0| = 1$.


\section{Extended optimality conjectures} \label{app:moreopt}
Intuitively, in large mean field systems governed by dynamics of the form \eqref{eq:m3mdp}, almost all information of the joint state $(x^{0,N}_t, x^{1,N}_t, \ldots, x^{N,N}_t)$ is contained in $(x^{0,N}_t, \mu^N_t)$, while heterogeneous policies should by LLN be replaceable by a shared one. To fully complete the theory of MFC, it is therefore interesting to establish the optimality of the considered mean field policies over arbitrary other policies acting on the joint state $(x^{0,N}_t, x^{1,N}_t, \ldots, x^{N,N}_t)$. 

We conjecture that it is possible to extend optimality (Corollary~\ref{coro:m3epsopt}) over larger classes of policies in the finite system. In particular, at least for finite state-action spaces, (i) any joint-state policy $\pi(\mathrm du \mid x^{0,N}_t, x^{1,N}_t, \ldots, x^{N,N}_t)$ might in the limit be replaced by an averaged policy $\bar \pi(\mathrm du \mid x^0, \mu) \coloneqq \sum_{x^N \in \mathcal X^N \colon \frac 1 N \sum_i \delta_{x^{i,N}} = \mu} \pi(\mathrm du \mid x^0, x^N)$ under some exchangeability of agents; (ii) any optimal policy $\pi$ outputting joint actions for all agents might be replaced by an independent policy for each agent, as in the limit all information is contained in the joint state-action distribution, which may be approximated increasingly closely by LLN; and (iii) heterogeneous policies for each minor agent $\pi^1, \ldots, \pi^N$ might similarly be replaced by $\bar \pi_t(u \mid x) \propto \sum_i \mathbb P(x^{i,N}_t = x) \pi^i_t(u \mid x)$, averaging the action distributions in any specific state over the proportion of agent likelihoods in that state. 

Showing such results would allow us to conclude that the policy classes $\Pi$ are natural and sufficient in mean field systems, as more general or heterogeneous policies cannot perform much better in MFC-type problems. A result related to (iii) has been shown for static cases \cite{sanjari2020optimal, cui2021discrete}. However, more general rigorous proofs are difficult and remain outside of our scope.



\section{Experimental Details} \label{app:exp}
In this section, we give lengthy experimental details that were omitted in the main text.

\subsection{Implementation Details}
For the implementation, we use multilayer perceptrons with two hidden layers of $256$ nodes and $\tanh$ activations as the neural networks of the policies. The neural network policy outputs parameters of a diagonal Gaussian over the major action $u^0$ and matrices $O$ as discussed in Section~\ref{sec:algo}, i.e. sampling clipped values $O$ in $[-1, 1]$ from the Gaussian which will parametrize $\mathcal H(\mu_t)$. In the discrete Beach scenario, the neural network instead outputs a categorical distribution using a final softmax layer. The PPO implementation used in our work is the RLlib 2.0.1 implementation \cite{liang2018rllib} with default settings and the hyperparameters as in Table~\ref{tab:hyperparams}. The optimal transport costs are computed via Python Optimal Transport \cite{flamary2021pot}, our M3FC MDP implementation follows the gym interface \cite{brockman2016openai}, while the multi-agent implementation follows RLlib interfaces \cite{liang2018rllib}.

\begin{table}[ht!]
    \centering
    \caption{Shared hyperparameter configurations for M3FPPO and IPPO.}
    \label{tab:hyperparams}
    \begin{tabular}{@{}ccc@{}}
    \toprule
    Symbol     & Name          & Value     \\ \midrule
    $\gamma$ &   Discount factor &  $0.99$\\
    $\lambda$ &   GAE lambda &  $1$\\
    $\beta$ &   KL coefficient & $0.03$ \\
    $\epsilon$ &  Clip parameter & $0.2$ \\
    $l_{r}$ &   Learning rate & $0.00005$ \\
    $B_{\mathrm{len}}$ &  Training batch size &  $24000$ \\
    $b_{\mathrm{len}}$ &  Mini-batch size &  $4000$ \\
    $N_{\mathrm{SGD}}$ &  Gradient steps per training batch & $8$ \\ \bottomrule
    \end{tabular}
\end{table}

\subsection{Problem Details}
In this section, we give details to the problems considered in this work. We omit the superscript $N$ for readability.

\paragraph{2G.}
In the 2G problem, we formally let $\mathcal X = [-2, 2]^2$, $\mathcal U = [-1, 1]^2$, $\mathcal X^0 =\{0, 1, \ldots 49\}$ according to \eqref{eq:msmmdp}. We allow noisy movement of minor agents following the Gaussian law
\begin{align*}
    p(x^i_{t+1} \mid x^i_{t}, u^i_{t}) = \mathcal N \left( x^i_{t+1} \innermid x^i_{t} + v_{\mathrm{max}} \frac{u^i_{t}}{\max(1, \lVert u^i_{t} \rVert_2)}, \mathrm{diag}(\sigma^2, \sigma^2) \right)
\end{align*}
for some maximum speed $v_{\mathrm{max}} = 0.2$, noise covariance $\sigma^2 = 0.03$ and projecting back actions $u$ with norm larger than $1$, with the additional modification that agent positions are clipped back into $\mathcal X$ whenever the agents move out of bounds.

We then consider a time-variant mixture of two Gaussians
\begin{align*}
    \mu^*_t \coloneqq \frac{1 + \cos(2 \pi t / 50)}{2} \mathcal N \left( \mathbf e_1, \mathrm{diag}(\sigma_*^2, \sigma_*^2) \right) + \frac{1 - \cos(2 \pi t / 50)}{2} \mathcal N \left( -\mathbf e_1, \mathrm{diag}(\sigma_*^2, \sigma_*^2) \right)
\end{align*}
for unit vector $\mathbf e_1$ and covariance $\sigma_*^2 = 0.05$, i.e. we have a period of $50$ time steps, and let the major state follow the clock dynamics $p^0(x^0 + 1 \mod 50 \mid x^0, \mu) = 1$. 

The goal of minor agents is to minimize the Wasserstein metric $\hat W_1$ under the squared Euclidean distance,
\begin{align*}
    \hat W_1(\mu, \mu') \coloneqq \inf_{\gamma \in \Gamma(\mu, \mu')} \left\{ \int \lVert x - y \rVert_2^2 \gamma(\mathrm dx, \mathrm dy) \right\}
\end{align*}
defined over all couplings $\Gamma(\mu, \mu')$ with first and second marginals $\mu$, $\mu'$ (which is strictly speaking not a metric but an optimal transportation cost, since the squared Euclidean distance fails the triangle inequality), between their empirical distribution and the desired mixture of Gaussians
\begin{align*}
    r(x^0_t, \mu_t) = - \hat W_1(\mu_t, \mu^*_t)
\end{align*}
which is computed numerically by the empirical distance, sampling $100$ samples from $\mu^*_t$.

The initialization of minor agents is uniform, i.e. $\mu_0 = \mathrm{Unif}(\mathcal X)$, and $x^0_0 = 0$. For sake of simulation, we define the episode length $T=100$ after which a new episode starts.

\paragraph{Formation.}
The Formation problem is an extension of the 2G problem, where instead $\mathcal X^0 = \mathcal X \times \mathcal X$ and $\mathcal U^0 = \mathcal U$, the major agent follows the same dynamics as the minor agents, and movements are noise-free, i.e. $\sigma^2 = 0$. The major agent state $x^0_t = (\hat x^0_t, x^*_t)$ here contains both the major agent position $\hat x^0_t$ and its target position $x^*_t$. The desired minor agent distribution is centered around the major agent
\begin{align*}
    \mu^*_t \coloneqq \mathcal N \left( \hat x^0_t, \mathrm{diag}(\sigma_*^2, \sigma_*^2) \right)
\end{align*}
with covariance $\sigma_*^2 = 0.3$, and is also observed by agents as in 2G via binning. Additionally, the major agent should follow a random target $x^*_t$ following discretized Ornstein-Uhlenbeck dynamics
\begin{align*}
    x^*_{t+1} \sim \mathcal N \left( 0.95 x^*_t, \mathrm{diag}(\sigma_{\mathrm{targ}}^2, \sigma_{\mathrm{targ}}^2) \right)
\end{align*}
with $\sigma_{\mathrm{targ}}^2 = 0.02$. Thus, similar to 2G, the reward function becomes
\begin{align*}
    r(x^0_t, u^0_t, \mu_t) = - \lVert \hat x^0_t - x^*_t \rVert_2 - \hat W_1(\mu_t, \mu^*_t).
\end{align*}

The initialization of agents is uniform, while the target starts around zero, i.e. $\mu_0 = \mathrm{Unif}(\mathcal X)$ and $\mu^0_0 = \mathrm{Unif}(\mathcal X) \otimes \mathcal N \left( 0, \mathrm{diag}(\sigma_{\mathrm{targ}}^2, \sigma_{\mathrm{targ}}^2) \right)$. For sake of simulation, we define the episode length $T=100$ after which a new episode starts.

\paragraph{Beach Bar Process.}
In the discrete beach bar process, we consider a discrete torus $\mathcal X = \{0, 1, \ldots, 4\}^2$, $\mathcal X^0 = \mathcal X \times \mathcal X$ and actions $\mathcal U = \mathcal U^0 = \{(0,0), (-1,0), (0,-1), (1,0), (0,1)\}$ indicating movement in any of the four cardinal directions. The major agent state $x^0_t = (\hat x^0_t, x^*_t)$ here contains both the major agent position $\hat x^0_t$ and its target position $x^*_t$. In other words, the dynamics follow
\begin{align*}
    \hat x^0_{t+1} = \hat x^0_{t} + u^0_{t} \mod (5,5), \quad
    x^i_{t+1} = x^i_{t} + u^i_{t} \mod (5,5).
\end{align*}
The target position follows a simple random walk on the torus
\begin{align*}
    x^*_{t+1} \sim x^*_t + \epsilon_t \mathrm{Unif}((-1,0), (0,-1), (1,0), (0,1)) \mod (5,5)
\end{align*}
with walking probability $\epsilon_t \sim \mathrm{Bernoulli}(0.2)$, uniformly in any direction.

The costs are then given by the average toroidal distance $d$ (the $L_1$ "wrap-around" distance on the torus) between the major agent and its target, the average distance between major and minor agents, and the crowdedness of agents
\begin{align*}
    r(x^0_t, u^0_t, \mu_t) = - 0.5 d(x^0_t, x^*_t) - 2.5 \int d(x, x^0_t) \mu_t(\mathrm dx) - 6.25 \int \mu_t(x) \mu_t(\mathrm dx).
\end{align*}
The initialization of agents is uniform, while the target starts at zero, i.e. $\mu_0 = \mathrm{Unif}(\mathcal X)$ and $\mu^0_0 = \mathrm{Unif}(\mathcal X) \otimes \delta_{(0,0)}$. For sake of simulation, we define the episode length $T=200$ after which a new episode starts.

For the neural network policy, we use a one-hot encoding of major states as input, i.e. the concatenation of two $5$-dimensional one-hot vectors for the major agent position $\hat x^0_t$ and its target position $x^*_t$ respectively.

\paragraph{Foraging.}
In the Foraging problem, we formally define $\mathcal X = [-2, 2]^2 \times [0, 1]$, $\mathcal U = [-1, 1]^2 = \mathcal U^0$ and $\mathcal X^0 = ([-2, 2] \times [-2, -1]) \times \bigcup_{n=0}^5 \left( [-2, 2]^2 \times [0, 1.5] \right)^n$. The minor agent states $x^i_t = (\hat x^i_t, \tilde x^i_t)$ here contain their positions $\hat x^i_t \in [-2, 2]^2$ and encumbrance $\hat x^i_t \in [0, 1]$. Meanwhile, the major agent state $x^0_t = (\hat x^0_t, x^{\mathrm{env}}_t)$ here contains both the major agent position $\hat x^0_t$ restricted to $[-2, 2] \times [-2, -1]$, and the current environment state $x^{\mathrm{env}}_t$. Here, the minor and major agents move as in Formation, though with different maximum velocities for minor agents $v_{\mathrm{max}} = 0.3$ and major agent $v^0_{\mathrm{max}} = 0.1$ respectively. 

The environment state consists of up to $5$ spatially localized foraging areas, which are observed via another additional binned histogram. In each time step, $N_t = \mathrm{Pois}(0.2)$ new foraging areas appear, up to a maximum total number of $5$. The location $x^m_t$ of each foraging area $m=1,\ldots,5$ is sampled uniformly randomly from $\mathrm{Unif}(\mathcal X)$, while their total initial size $L^m_t$ is sampled from $\mathrm{Unif}([0.5, 1.5])$, making up the environment state $x^{\mathrm{env}}_t = (x^m_t, L^m_t)_m$. At every time step, the foraging areas $m$ are depleted by nearby agents closer than range $0.5$,
\begin{align*}
    L^m_{t+1} = L^m_t - \Delta L^m(\mu_t), \quad \text{where} \quad 
    \Delta L^m(\mu_t) \coloneqq \min(L^m_{t+1} - L^m_t, \min(0.1, \int (0.5 - \lVert x - x^m_t \rVert_2)^+ \, \mu_t(\mathrm dx))
\end{align*}
where $(\cdot)^+ \coloneqq \max(0, \cdot)$, until they are fully depleted and disappear ($L^m_{t+1} \leq 0$).

Foraging minor agents simulate encumbrance, gaining it from nearby foraging areas and depositing to a nearby major agent, by splitting the foraged amount among all nearby minor agents according to their foraged contribution, and wasting any amount going beyond maximum encumbrance $1$,
\begin{align*}
    \tilde x^i_{t+1} =
    \begin{cases}
        \min(1, \tilde x^i_t + \Delta L^m(\mu_t) \cdot \frac{(0.5 - \lVert x - x^m_t \rVert_2)^+}{\int (0.5 - \lVert x - x^m_t \rVert_2)^+ \, \mu_t(\mathrm dx)}) \quad \text{if} \quad \lVert x^i_t - x^0_t \rVert_2 \geq 0.5, \\
        0 \quad \text{else.}
    \end{cases}
\end{align*}
The reward at each time step is then given by the according total foraged and then deposited amount by the minor agents, where any clipped amount is wasted.

The initialization of agents is uniform, while the environment starts empty, i.e. $\mu_0 = \mathrm{Unif}(\mathcal X)$ and $\mu^0_0 = \mathrm{Unif}(\mathcal X) \otimes \delta_{\emptyset}$. For sake of simulation, we define the episode length $T=200$ after which a new episode starts.

\paragraph{Potential.}
Lastly, in Potential we consider minor agents on a continuous one-dimensional torus $\mathcal X = [-2, 2]$ (i.e. the points $-2$ and $2$ are identified), actions $\mathcal U = [-1, 1]$ and major state $\mathcal X^0 = \mathcal X \times \mathcal X$. The minor agents move as in Foraging (wrapping around the torus instead of clipping), while the major agent follows the gradient of the potential landscape generated by minor agents, with the goal of staying close to its current target. The major agent state $x^0_t = (\hat x^0_t, x^*_t)$ here contains both the major agent position $\hat x^0_t$ and its target position $x^*_t$. For simplicity, here we use a simple linear repulsive force decreasing from $\frac 1 N$ to $0$ over a range of $1$,
\begin{align*}
    \hat x^0_{t+1} = \hat x^0_t + 0.05 \sum_{x_{\mathrm{off}} \in \{-4, 0, 4\}} \int (1 - \lVert \hat x^0_t - x + x_{\mathrm{off}} \rVert_2)^+ \frac{\hat x^0_t - x + x_{\mathrm{off}}}{\lVert \hat x^0_t - x + x_{\mathrm{off}} \Vert_2} \mu_t(\mathrm dx) \mod [-2, 2]
\end{align*}
where we let terms $0/0=0$ and use the offset $x_{\mathrm{off}}$ to account for the wrap-around on the torus. 

The target follows the discretized Ornstein-Uhlenbeck process
\begin{align*}
    x^*_{t+1} \sim \mathcal N \left( 0.99 x^*_t, \mathrm{diag}(\sigma_{\mathrm{targ}}^2, \sigma_{\mathrm{targ}}^2) \right)
\end{align*}
with covariance $\sigma_{\mathrm{targ}}^2 = 0.005$, and gives rise to the reward function via the toroidal distance between target and major agent
\begin{align*}
    r(x^0_t, \mu_t) = - d(\hat x^0_t, x^*_t).
\end{align*}
The initialization of agents is uniform, while the target starts around zero, i.e. $\mu_0 = \mathrm{Unif}(\mathcal X)$ and $\mu^0_0 = \mathrm{Unif}(\mathcal X) \otimes \mathcal N \left( 0, \mathrm{diag}(\sigma_{\mathrm{targ}}^2, \sigma_{\mathrm{targ}}^2) \right)$. For sake of simulation, we define the episode length $T=100$ after which a new episode starts. In contrast to $M = 7^2 = 49$ in 2G, Formation and Foraging, here we use $M=7$ bins for the one-dimensional problem.


\subsection{Training M3FPPO on smaller systems}
Lastly, in Figure~\ref{fig:Ntrain} we verify the training of M3FPPO on small finite system. Comparing to Figures~\ref{fig:curvem3fc} and \ref{fig:Ncompare}, for M3FPPO we see little difference between training on a small finite-agent system versus training on a large system and applying the policy on the smaller system. For the chosen hyperparameters, the performance in the Potential problem depends on the initialization. However, M3FPPO compares especially favorably to IPPO in Beach and Foraging, even when directly training on the finite system. This shows that we can either (i) directly apply M3FPPO as a MARL algorithm to small systems, or (ii) train on a fixed system, and transfer the learned behavior to systems of arbitrary other sizes.

\begin{figure}[b!]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/M3FC_training_curves_on_N.pdf}
    \caption{Training curves (mean episode return vs. time steps) of M3FPPO in red, trained on the finite systems with $N \in \{ 5, 10, 20 \}$. (a) 2G; (b) Formation; (c) Beach Bar; (d) Foraging; (e) Potential.}
    \label{fig:Ntrain}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
