
In this section we provide a rigorous evaluation of methods and their \fwName components.
Inspired by the initial findings in Section~\ref{sec:motivatingExperiments} and our overview of recent methods in Section~\ref{sec:relatedWork}, we focus mainly on the SSL and CM components in our evaluation.
We found these components to be most influential on the methods' performance.
For completeness, we include experiments with different fusion and CM components in the supplementary.

\subsection{Setup}
\label{subsec:setup}
    \thisfloatsetup{floatrowsep=quad}
    \begin{figure*}
        \begin{floatrow}
        \capbtabbox[0.8\textwidth]{%
            \setlength{\tabcolsep}{.6mm}
            \tableFontSize\undoCaptionSep\renewcommand{\arraystretch}{0.85}
            \input{tab/benchmark/agg/merged.tex}
        }{%
          \caption{
            Aggregated evaluation results for the dataset groups. Models are sorted from lowest to highest by average Z-score for each group. Higher Z-scores indicate better clusterings. Our new instances are \underline{underlined}.
            \textbf{Abbreviations:}
            BL = Simple baseline,
            CA = Contrastive alignment,
            DDC = Deep divergence-based clustering
            MI = Mutual information,
            \( \bar Z \) = Average Z-score for group.
        }
        \label{tab:aggBenchmark}
        }
        \ffigbox[0.15\textwidth]{%
          {\scriptsize
            \flushleft
            \def\figwidth{3.7cm}
            \def\figheight{4.5cm}
            \input{fig/increasing_views/new_instances.tex}}

        }{%
          \caption{Accuracies on Caltech7 with increasing number of views.}
        \label{fig:incviews}
        }
        \end{floatrow}
    \end{figure*}

    \customparagraph{Baselines.}
        In addition to the new instances presented in Section~\ref{sec:newVariations}, we include \( 6 \) baseline models from previous work in our experiments.
        The following baseline models were selected to include a diverse set of framework components in the evaluation:
        \begin{enumerate*}[label=(\roman*)]
            \item Deep Multimodal Subspace Clustering (DMSC)~\cite{abavisaniDeepMultimodalSubspace2018};
            \item Multi-view Spectral Clustering Network (MvSCN)~\cite{huangMultiviewSpectralClustering2019};
            \item End-to-end Adversarial-attention Multimodal Clustering (EAMC)~\cite{zhouEndtoEndAdversarialAttentionNetwork2020};
            \item Simple Multi-View Clustering (SiMVC)~\cite{trostenReconsideringRepresentationAlignment2021};
            \item Contrastive Multi-View Clustering (CoMVC)~\cite{trostenReconsideringRepresentationAlignment2021};
            \item Multi-view Variational Autoencoder (Multi-VAE)~\cite{xuMultiVAELearningDisentangled2021}.
        \end{enumerate*}

        As can be seen in Table~\ref{tab:previousMethods}, this collection of models includes both reconstruction-based and alignment-based SSL, as well as traditional (\( k \)-means and spectral) and deep learning-based CMs.
        They also include several fusion strategies and encoder networks.
        Section~\ref{subsec:ablation} includes an ablation study that examines the influence of SSL components in these models.

    \customparagraph{Datasets.}
        We evaluate the baselines and new instances on \( 8 \) widely used benchmark datasets for deep MVC.
        We prioritize datasets that were also used in the original publications for the selected baselines.
        Not only does this result in a diverse collection of datasets common in deep MVC -- it also allows us to compare the performance of our implementations to what was reported by the original authors.
        The results of this comparison are given in the supplementary.

        The following datasets are used for evaluation:
        \begin{enumerate*}[label=(\roman*)]
            \item \textbf{NoisyMNIST\,/\,NoisyFashion}: A version of MNIST~\cite{lecunGradientbasedLearningApplied1998}\,/\,FashionMNIST~\cite{xiaoFashionMNISTNovelImage2017} where the first view contains the original image, and the second view contains an image sampled from the same class as the first image, with added Gaussian noise (\( \sigma = 0.2 \)).
            \item \textbf{EdgeMNIST\,/\,EdgeFashion}: Another version of MNIST\,/\,FashionMNIST where the first view contains the original image, and the second view contains an edge-detected version of the same image.
            \item \textbf{COIL-20}: The original COIL-20~\cite{neneColumbiaObjectImage1996} dataset, where we randomly group the images of each object into groups of size \( 3 \), resulting in a \( 3 \)-view dataset.
            \item \textbf{Caltech7\,/\,Caltech20}: A subset of the Caltech101~\cite{fei-feiLearningGenerativeVisual2007} dataset including \( 7 \)\,/\,\( 20 \) classes.
                We use the \( 6 \) different features extracted by Li \etal~\cite{liLargeScaleMultiViewSpectral2015}, resulting in a \( 6 \)-view dataset\footnote{The list of classes and feature types is included in the supplementary.}.
            \item \textbf{PatchedMNIST}: A subset of MNIST containing the first three digits, where views are extracted as \( 7 \times 7 \) non-overlapping patches of the original image.
                The corner patches are dropped as they often contain little information about the digit, resulting in a dataset with \( 12 \) views.
                Each patch is resized to \( 28 \times 28 \).
        \end{enumerate*}

        All views are individually normalized so that the values lie in \( [0, 1] \).
        Following recent work on deep MVC, we train and evaluate on the full datasets~\cite{zhouEndtoEndAdversarialAttentionNetwork2020,trostenReconsideringRepresentationAlignment2021,xuMultiVAELearningDisentangled2021,maoDeepMutualInformation2021}.
        More dataset details are provided in the supplementary.

    \customparagraph{Hyperparameters.}
        The baselines use the hyperparameters reported by the original authors, because
        \begin{enumerate*}[label=(\roman*)]
            \item it is not feasible for us to tune hyperparameters individually for each model on each dataset; and
            \item it is difficult to tune hyperparameters in a realistic clustering setting due to the lack of labeled validation data.
        \end{enumerate*}
        For each method, the same hyperparameter configuration is used for all datasets.
        
        New instances use the same hyperparameters as for the baselines wherever possible\footnote{Hyperparameters for all models are listed in the supplementary.}.
        Otherwise, we set hyperparameters such that loss terms have the same order of magnitude, and such that the training converges.
        We refrain from any hyperparameter tuning that includes the dataset labels to keep the evaluation fair and unsupervised.
        We include a hyperparameter sweep in the supplementary, in order to assess the new instances' sensitivity to changes in their hyperparameter.
        However, we emphasize that the results of this sweep were \emph{not} used to select hyperparameters for the new instances.
        All models use the same encoder architectures and are trained for \( 100 \) epochs with the Adam optimizer~\cite{kingmaAdamMethodStochastic2015}.

    \customparagraph{Evaluation protocol.}
        We train each model from \( 5 \) different initializations.
        Then we select the run that resulted in the lowest value of the loss and report the performance metrics from that run, following~\cite{kampffmeyerDeepDivergencebasedApproach2019,trostenReconsideringRepresentationAlignment2021}.
        This evaluation protocol is both fully unsupervised, and is not as impacted by poorly performing runs, as for instance the mean performance of all runs.
        The uncertainty of the performance metric under this model selection protocol is estimated using bootstrapping\footnote{Details on uncertainty computations are included in the supplementary.}.
        We measure clustering performance with the accuracy (ACC) and normalized mutual information (NMI).
        Both metrics are bounded in \( [0, 1] \), and higher values correspond to better performing models, with respect to the ground truth labels.

\subsection{Evaluation results}
\label{subsec:benchmarkResults}
    To emphasize the findings from our experiments, we compute the average Z-score for each model, for \( 4 \) groups of datasets\footnote{Results for all methods/datasets are included in the supplementary.}.
    Z-scores are calculated by subtracting the mean and dividing by the standard deviation of results, per dataset and per metric.
    Table~\ref{tab:aggBenchmark} shows Z-scores for the groups:
    \begin{enumerate*}[label=(\roman*)]
        \item \textbf{All datasets}.
        \item \textbf{Random pairings:} Datasets generated by randomly pairing within-class instances to synthesize multiple views (NoisyMNIST, NoisyFashion, COIL-20).
        \item \textbf{Many views:} Datasets with many views (Caltech7, Caltech20, PatchedMNIST).
        \item \textbf{Balanced vs.\ imbalanced:} Datasets with balanced classes (NoisyMNIST, NoisyFashion, EdgeMNIST, EdgeFashion, COIL-20, PatchedMNIST) vs.\ datasets with imbalanced classes (Caltech7, Caltech20).
    \end{enumerate*}
    Our main experimental findings are:

    \textbf{Dataset properties significantly impact the performance of methods.}
    We observe that the ranking of methods varies significantly based on dataset properties, such as the number of views (Table~\ref{tab:aggBenchmark}c) and class (im)balance (Table~\ref{tab:aggBenchmark}d).
    Hence, there is not a single ``state-of-the-art'' for all datasets.

    \textbf{Our new instances outperform previous methods.}
    In Table~\ref{tab:aggBenchmark}a we see that the simple baselines perform remarkably well, when compared to the other, more complex methods.
    This highlights the importance of including simple baselines like these in the evaluation.
    Table~\ref{tab:aggBenchmark}a shows that \cae overall outperforms the other methods, and on datasets with many views (Table~\ref{tab:aggBenchmark}c) we find that \mimvc and \mviic outperform the others by a large margin.

    \textbf{Maximization of mutual information outperforms contrastive alignment on datasets with many views.}
    Contrastive alignment-based methods show good overall performance, but they struggle when the number of views becomes large (Table~\ref{tab:aggBenchmark}c).
    This holds for both baseline methods (as observed in Section~\ref{sec:motivatingExperiments}), and the new instances.
    As in Section~\ref{sec:motivatingExperiments}, we hypothesize that this is due to issues with representation alignment, where the presence of less informative views is more likely when the number of views becomes large.
    Contrastive alignment attempts to align view-specific representations to this less informative view, resulting in clusters that are harder to separate in the representation space.
    This is further verified in Figures~\ref{fig:motivatingIncviews} and~\ref{fig:incviews}, illustrating a decrease in performance on Caltech7 for contrastive alignment-based models with \( 5 \) or \( 6 \) views.
    Models based on maximization of mutual information do not have the same problem.
    We hypothesize that this is because maximizing mutual information still allows the view-specific representations to be different, avoiding the above issues with alignment.
    The MI-based models also include regularization terms that maximize the entropy of view-specific representations, preventing the representations from collapsing to a single value.

    \textbf{Contrastive alignment works particularly well on datasets consisting of random pairings (Table~\ref{tab:aggBenchmark}b).}
    In these datasets, the class label is the only thing the views have in common.
    Contrastive alignment, \ie learning a shared representation for all pairs within a class, thus asymptotically amounts to learning a unique representation for each class, making it easier for the CM to separate between classes.

    \textbf{The DDC CM performs better than the other CMs on balanced datasets.}
    With the DDC CM, the models are end-to-end trainable -- jointly optimizing all components in the model.
    The view-specific representations can thus be adapted to suit the CM, potentially improving the clustering result.
    DDC also has an inherent bias towards balanced clusters~\cite{kampffmeyerDeepDivergencebasedApproach2019}, which helps produce better clusterings when the ground truth classes are balanced.

    \textbf{Reproducibility of original results.}
    During our experiments we encountered issues with reproducibility with several of the methods from previous work.
    In the supplementary we include a comparison between our results and those reported by the original authors of the methods from previous work.
    We find that most methods use different network architectures and evaluation protocols in the original publications, making it difficult to accurately compare performance between methods and their implementations.
    This illustrates the difficulty of reproducing and comparing results in deep MVC, highlighting the need for a unified framework with a consistent evaluation protocol and an open-source implementation.

\subsection{Effect of SSL components}
\label{subsec:ablation}
    \begin{table}[t]
        \centering
        \setlength{\tabcolsep}{1mm}
        \tableFontSize\undoCaptionSep\renewcommand{\arraystretch}{0.85}
        \input{tab/ablation/both_ssl.tex}
        \caption{Accuracies from ablation studies with SSL components.}
        \label{tab:ablationSSL}
    \end{table}
    Table~\ref{tab:ablationSSL} shows the results of ablation studies with the SV-SSL and MV-SSL components.
    These results show that having at least one form of SSL is beneficial for the performance of all models, with the exception being \sae/\cae, which on Caltech7 performs best without any self-supervision.
    We suspect that this particular result is due to the issues with many views and class imbalance discussed in Section~\ref{subsec:benchmarkResults}.
    Further, we observe that having both forms of SSL is not always necessary.
    For instance is there no difference with and without SV-SSL for \cae and \caekm, both of which include contrastive alignment-based MV-SSL.
    Lastly, we note that contrastive alignment-based MV-SSL decreases performance on Caltech7 for most models.
    This is consistent with our theoretical findings in Section~\ref{sec:motivatingExperiments}, as well as the results in Section~\ref{subsec:benchmarkResults} and in Figures~\ref{fig:motivatingIncviews} and~\ref{fig:incviews} -- illustrating that contrastive alignment is not suitable for datasets with a large number of views.


