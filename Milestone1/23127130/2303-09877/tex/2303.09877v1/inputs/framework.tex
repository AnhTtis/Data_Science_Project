
In this section we present the \fwName framework, its components and their purpose, and how they fit together.
This allows us to, in the next section, summarize recent work on deep MVC, and illustrate that the majority of recent methods can be regarded as instances of \fwName.

Suppose we have a multi-view dataset consisting of \( n \) instances and \( V \) views, and let \( \vec x_i^{(v)} \) be the observation of instance \( i \) through view \( v \).
The task of the \fwName framework is then to cluster the instances into \( k \) clusters, and produce cluster membership indicators \( \alpha_{ic} \in [0, 1] \), \( c = 1, \dots, k \).
The framework is illustrated in Figure~\ref{fig:framework}.
It consists of the following components.

\customparagraph{View-specific encoders.}
The framework is equipped with \( V \) deep neural network encoders \( f^{(1)}, \dots, f^{(V)} \), one for each view.
Their task is to produce the view-specific representations \( \vec z_i^{(v)} = f^{(v)}(\vec x_i^{(v)}) \) from the input data.

\customparagraph{Single-view self-supervised learning (SV-SSL).}
The SV-SSL component consists of a set of pretext tasks (auxiliary objectives) that are designed to aid the optimization of the view-specific encoders.
Specifically, the tasks should be designed to help the encoders learn representations that simplify the clustering task.
Each pretext task is specific to its designated view, and is isolated from all other views.

\customparagraph{Multi-view self-supervised learning (MV-SSL).}
MV-SSL is similar to SV-SSL -- they are both self-supervised modules whose goals are to help the encoders learn representations that are suitable for clustering.
However, MV-SSL leverages all views simultaneously in the pretext tasks, allowing the model to exploit information from all views simultaneously to learn better features.

\customparagraph{Fusion.}
This component combines view-specific representations into a shared representation for all views.
Fusion is typically done using a (weighted) average~\cite{liDeepAdversarialMultiview2019,trostenReconsideringRepresentationAlignment2021}, or by concatenation~\cite{huangMultiviewSpectralClustering2019,xinSelfSupervisedDeepCorrelational2021,xuMultiVAELearningDisentangled2021}.
More complex fusion modules using \eg attention mechanisms~\cite{zhouEndtoEndAdversarialAttentionNetwork2020}, are also possible.

\customparagraph{Clustering module (CM).}
The CM is responsible for determining cluster memberships based on view-specific or fused representations.
The CM can consist of a traditional clustering method, such as \( k \)-means~\cite{macqueenMethodsClassificationAnalysis1967} or Spectral Clustering~\cite{shiNormalizedCutsImage2000}.
Such CMs are applied to the fused representations after other components have been trained, resulting in a two-stage method that first learns fused representations, and then applies a clustering algorithm to these representations.

Alternatively, the CM can be integrated into the model~\cite{zhouEndtoEndAdversarialAttentionNetwork2020,trostenReconsideringRepresentationAlignment2021}, allowing it to be trained alongside other components, resulting in fused representations that are better suited for clustering.

\customparagraph{Loss functions and training.}
The loss functions for the models are specified by the SV-SSL, MV-SSL, and CM components.
To train the model, the terms arising from the different components can be minimized simultaneously or they can be minimized in an alternating fashion.
It is also possible with pre-training/fine-tuning setups where the model is pre-trained with one subset of the losses and fine-tuned with another subset of the losses.

We note that \fwName is a conceptual framework, and that a model is not necessarily completely described by a list of its \fwName components.
Consequently, it is possible for two models with similar \fwName components to have slightly different implementations.
This illustrates the importance of our open-source implementation of \fwName, which allows the implementation of a model to be completely transparent.
