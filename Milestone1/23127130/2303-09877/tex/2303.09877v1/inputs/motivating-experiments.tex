
As can be seen in Table~\ref{tab:previousMethods}, SSL components are crucial in recent state-of-the-art methods for deep MVC.
Recent works have focused on aligning view-specific representations~\cite{zhouEndtoEndAdversarialAttentionNetwork2020,trostenReconsideringRepresentationAlignment2021}, and in particular, contrastive alignment~\cite{trostenReconsideringRepresentationAlignment2021}.
We study a simplified setting where, for each view, all observations in a cluster are located at the same point.
This allows us to prove that aligning view-specific representations has a negative impact on the cluster separability after fusion.
This is the same starting point as in~\cite{trostenReconsideringRepresentationAlignment2021}, but we extend the analysis to investigate contrastive alignment when the number of views increases.

\input{inputs/propositions.tex}
\propositionMinSeparbleClusters
\begin{proof}
    See~\cite{trostenReconsideringRepresentationAlignment2021}.
\end{proof}

According to Proposition~\ref{prop:prop1}, when the view-specific representations are perfectly aligned, the number of separable clusters after fusion, \( \kappa \), depends on the number of separable clusters in the \emph{least informative view} -- the view with the lowest \( k_v \).
The following propositions show what happens to \( \min \{ k_v \} \) when the number of views increases\footnote{The proofs of Propositions~\ref{prop:conditionalProbMinimum} and~\ref{prop:expectationMinimum} are given in the supplementary}.

\propositionConditionalProbMinimum
\propositionExpectationMinimum

Assuming the view-specific representations are perfectly aligned, Propositions~\ref{prop:conditionalProbMinimum} and~\ref{prop:expectationMinimum} show that:
\begin{enumerate*}[label=(\roman*)]
    \item Given a number of views, adding another view will, with probability \( 1 \), not increase \( \min \{k_v \} \).
    \item Among two datasets with the same distribution for the \( k_v \), the dataset with the \emph{smallest number of views} will have the highest expected value of \( \min \{k_v\} \).
\end{enumerate*}

In summary, we have shown that contrastive alignment-based models perform worse when the number of views in a dataset increases.
These findings are supported by the experimental results in Figure~\ref{fig:motivatingIncviews} and Table~\ref{tab:motivatingMNIST} which show that, when the number of views increases, the contrastive alignment-based model is outperformed by the model without any alignment.

\customparagraph{Alignment as a pretext task.}
    In contrast to our theoretical findings in the simplified case, Figure~\ref{fig:motivatingIncviews} and Table~\ref{tab:motivatingMNIST} show that contrastive alignment can sometimes be beneficial for the performance, particularly when the number of views is small.
    This is because alignment might be a good pretext task that helps the encoders learn informative representations, by learning to represent the information that is shared across views.
    However, we emphasize that this is only true when the number of views is small ($\le 4$ in Figure~\ref{fig:motivatingIncviews}), meaning that alignment should be used with caution when the number of views increases beyond this point.

\begin{figure}
\begin{floatrow}
\ffigbox[0.35\columnwidth]{%
    \centering
    \bgroup
    \def\figwidth{3.7cm}
    \def\figheight{4.2cm}
    \scriptsize
    \input{fig/increasing_views/motivating.tex}
    \egroup
}{%
  \caption{Clustering accuracy for an increasing number of views on Caltech7.}
  \label{fig:motivatingIncviews}
}
\capbtabbox[0.5\columnwidth]{%
    \bgroup
    \tableFontSize
    \setlength{\tabcolsep}{.9mm}
    \input{tab/motivating-mnist.tex}
    \egroup
}{%
  \caption{Clustering accuracies on datasets with varying number of views.}
  \label{tab:motivatingMNIST}
}
\end{floatrow}
\end{figure}
