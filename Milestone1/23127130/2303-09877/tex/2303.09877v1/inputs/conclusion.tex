
We investigate the role of self-supervised learning (SSL) in deep MVC.
Due to its recent success, we focus particularly on contrastive alignment, and prove that it can be detrimental to the clustering performance, especially when the number of views becomes large.
To properly evaluate models and components, we develop \fwName~-- a new unified framework for deep MVC, including the majority of recent methods as instances.
By leveraging the new insight from our framework and theoretical findings, we develop \( 6 \) new \fwName instances with several promising forms of SSL, which perform remarkably well compared to previous methods.
We conduct a thorough experimental evaluation of our new instances, previous methods, and their \fwName components -- and find that SSL is a crucial component in state-of-the-art methods for deep MVC.
In line with our theoretical analysis, we observe that contrastive alignment worsens performance when the number of views becomes large.
Further, we find that performance of methods depends heavily on dataset characteristics, such as number of views, and class imbalance.
Developing methods that are robust towards changes in these properties can thus result in methods that perform well over a wide range of multi-view clustering problems.
To this end, we make the following recommendations for future work in deep MVC:

\textbf{Improving contrastive alignment or maximization of mutual information to handle both few and many views.}
    Addressing pitfalls of alignment to improve contrastive alignment-based methods on many views, is a promising direction for future research.
    Similarly, we believe that improving the methods based on maximization of mutual information on few views, will result in better models.

\textbf{Developing end-to-end trainable clustering modules that are not biased towards balanced clusters.}
    The performance of the DDC clustering module illustrates the potential of end-to-end trainable clustering modules, which are capable of adapting the representations to produce better clusterings.
    Mitigating the bias towards balanced clusters thus has the potential to produce models that perform well, both on balanced and imbalanced datasets.

\textbf{Proper evaluation and open-source implementations.}
    Finally, we emphasize the importance of evaluating new methods on a representative collection of datasets, \eg many views and few views, paired, imbalanced, \etc.
    Also, in the reproducibility study (see supplementary), we find that original results can be difficult to reproduce.
    We therefore encourage others to use the open-source implementation of \fwName, as open code and datasets, and consistent evaluation protocols, are crucial to properly evaluate models and facilitate further development of new methods and components.
