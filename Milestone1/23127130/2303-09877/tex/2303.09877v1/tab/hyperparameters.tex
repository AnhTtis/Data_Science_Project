\def\layer#1{\texttt{#1}}
\def\Conv{\layer{Conv}}
\def\BatchNormalization{\layer{BatchNorm}}
\def\RELU{\layer{ReLU}}
\def\MaxPool{\layer{MaxPool}}
\def\Dense{\layer{Dense}}
\def\ConvTranspose{\layer{TransposeConv}}
\def\UpSample{\layer{UpSample}}
\def\Sigmoid{\layer{Sigmoid}}

\def\CNNencoder{
    \begin{tabular}{l} \toprule
        \cthead{Layer} \\ \cmidrule(lr){1-1}
        \Conv \( (64 \times 3 \times 3) \) \\
        \RELU \\
        \Conv \( (64 \times 3 \times 3) \) \\
        \BatchNormalization \\
        \RELU \\
        \MaxPool \( (2 \times 2) \) \\
        \Conv \( (64 \times 3 \times 3) \) \\
        \RELU \\
        \Conv \( (64 \times 3 \times 3) \) \\
        \BatchNormalization \\
        \RELU \\
        \MaxPool \( (2 \times 2) \) \\
        \bottomrule
    \end{tabular}
}

\def\CNNdecoder{
    \begin{tabular}{l} \toprule
        \cthead{Layer} \\ \cmidrule(lr){1-1}
        \UpSample \( (2 \times 2) \) \\
        \ConvTranspose \( (64 \times 3 \times 3) \) \\
        \RELU \\
        \ConvTranspose \( (64 \times 3 \times 3) \) \\
        \BatchNormalization \\
        \RELU \\
        \UpSample \( (2 \times 2) \) \\
        \ConvTranspose \( (64 \times 3 \times 3) \) \\
        \RELU \\
        \ConvTranspose \( (1 \times 3 \times 3) \) \\
        \Sigmoid \\
        \bottomrule
    \end{tabular}
}

\def\MLPencoder{
    \begin{tabular}{l} \toprule
        \Dense \( (1024) \) \\
        \BatchNormalization \\
        \RELU \\
        \Dense \( (1024) \) \\
        \BatchNormalization \\
        \RELU \\
        \Dense \( (1024) \) \\
        \BatchNormalization \\
        \RELU \\
        \Dense \( (1024) \) \\
        \BatchNormalization \\
        \RELU \\
        \Dense \( (256) \) \\
    \end{tabular}
}

\def\MLPdecoder{
    \begin{tabular}{l} \toprule
        \Dense \( (256) \) \\
        \BatchNormalization \\
        \RELU \\
        \Dense \( (1024) \) \\
        \BatchNormalization \\
        \RELU \\
        \Dense \( (1024) \) \\
        \BatchNormalization \\
        \RELU \\
        \Dense \( (1024) \) \\
        \BatchNormalization \\
        \RELU \\
        \Dense \( (\texttt{input dim}) \) \\
        \Sigmoid \\
        \bottomrule
    \end{tabular}
}

\def\AllArch{
    \begin{tabular}{llll} \toprule
        \cthead{CNN encoder} & \cthead{CNN decoder} & \cthead{MLP encoder} & \cthead{MLP decoder} \\ \midrule
        \Conv \( (64 \times 3 \times 3) \) & \UpSample \( (2 \times 2) \) & \Dense \( (1024) \) & \Dense \( (256) \) \\
        \RELU & \ConvTranspose \( (64 \times 3 \times 3) \) & \BatchNormalization & \BatchNormalization \\
        \Conv \( (64 \times 3 \times 3) \) & \RELU & \RELU & \RELU \\
        \BatchNormalization & \ConvTranspose \( (64 \times 3 \times 3) \) & \Dense \( (1024) \) & \Dense \( (1024) \) \\
        \RELU & \BatchNormalization & \BatchNormalization & \BatchNormalization \\
        \MaxPool \( (2 \times 2) \) & \RELU & \RELU & \RELU \\
        \Conv \( (64 \times 3 \times 3) \) & \UpSample \( (2 \times 2) \) & \Dense \( (1024) \) & \Dense \( (1024) \) \\
        \RELU & \ConvTranspose \( (64 \times 3 \times 3) \) & \BatchNormalization & \BatchNormalization \\
        \Conv \( (64 \times 3 \times 3) \) & \RELU & \RELU & \RELU \\
        \BatchNormalization & \ConvTranspose \( (1 \times 3 \times 3) \) & \Dense \( (1024) \) & \Dense \( (1024) \) \\
        \RELU & \Sigmoid & \BatchNormalization & \BatchNormalization \\
        \MaxPool \( (2 \times 2) \) & & \RELU & \RELU \\
        && \Dense \( (256) \) & \Dense \( (\texttt{input dim}) \) \\
        &&& \Sigmoid \\
        \bottomrule
    \end{tabular}
}

\def\EAMCLearningRate{\( \dagger \) = EAMC~\cite{zhouEndtoEndAdversarialAttentionNetwork2020} has different learning rates for the different components, namely \( 10^{-5} \) for the encoders and clustering module, and \( 10^{-4} \) for the attention module and discriminator.}

\def\hyperparametersAll{
    \begin{tabular}{lccccccc} \toprule
        Model  & Batch size & Learning rate         & \( \SVSSLWeight \) & \( \MVSSLWeight \) & \( \CMWeight \) & Pre-train & Gradient clip \\ \cmidrule(lr){1-1} \cmidrule(lr){2-8}
        \dmsc  & \( 100 \)  & \( 10^{-3} \)         & \( 1.0 \)          & --                 & --              & \TRUE     & \( 10 \) \\
        \mvscn & \( 512 \)  & \( 10^{-4} \)         & \( 0.999 \)        & \( 0.001 \)        & --              & \FALSE    & \( 10 \) \\
        \eamc  & \( 100 \)  & \( \dagger \)         & --                 & \( 1.0 \)          & \( 1.0 \)       & \FALSE    & \( 10 \) \\
        \simvc & \( 100 \)  & \( 10^{-3} \)         & --                 & --                 & \( 1.0 \)       & \FALSE    & \( 10 \) \\
        \comvc & \( 100 \)  & \( 10^{-3} \)         & --                 & \( 0.1 \)          & \( 1.0 \)       & \FALSE    & \( 10 \) \\
        \mvae  & \( 64 \)   & \( 5 \cdot 10^{-4} \) & --                 & \( 1.0 \)          & --              & \TRUE     & \( 10 \) \\
        \sae   & \( 100 \)  & \( 10^{-3} \)         & \( 1.0 \)          & --                 & \( 1.0 \)       & \TRUE     & \( 10 \) \\
        \cae   & \( 100 \)  & \( 10^{-3} \)         & \( 1.0 \)          & \( 0.1 \)          & \( 1.0 \)       & \TRUE     & \( 10 \) \\
        \saekm & \( 100 \)  & \( 10^{-3} \)         & \( 1.0 \)          & --                 & --              & \FALSE    & \( 10 \) \\
        \caekm & \( 100 \)  & \( 10^{-3} \)         & \( 1.0 \)          & \( 0.1 \)          & --              & \FALSE    & \( 10 \) \\
        \mimvc & \( 256 \)  & \( 10^{-3} \)         & --                 & \( 0.1 \)          & \( 1.0 \)       & \FALSE    & \( 10 \) \\
        \mviic & \( 256 \)  & \( 10^{-3} \)         & --                 & \( 0.01 \)         & \( 1.0 \)       & \FALSE    & \( 10 \) \\
        \bottomrule
    \end{tabular}
}

