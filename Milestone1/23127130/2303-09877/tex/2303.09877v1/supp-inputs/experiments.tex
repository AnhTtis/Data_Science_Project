\subsection{Datasets}
    \begin{table*}
        \centering
        \caption{Dataset details. \( n \) = number of instances, \( v \) = number of views, \( k \) = number of classes/clusters, \( n_\text{small} \) = number of instances in smallest class, \( n_\text{big} \) = number of instances in largest class, Dim.~= view dimensions.}
        \label{tab:datasets}
        \undoCaptionSep
        \tableFontSize
        \input{tab/datasets.tex}
    \end{table*}

    Dataset details are listed in Table~\ref{tab:datasets}.
    The code repository includes pre-processed Caltech7 and Caltech20 datasets.
    The other datasets can be generated by following the instructions in \suppdir{README.md} (these could not be included in the archive due to limitations on space).

    \customparagraph{Caltech details.}
        We use the same features and subsets of the Caltech101~\cite{fei-feiLearningGenerativeVisual2007} dataset as~\cite{huangMultiviewSpectralClustering2019}.
        \begin{itemize}
            \item \textbf{Features:}  Gabor, Wavelet Moments, CENsus TRansform hISTogram (CENTRIST), Histogram of Oriented Gradients (HOG), GIST, and Local Binary Patterns (LBP).
            \item \textbf{Caltech7 classes:} Face, Motorbikes, Dolla-Bill, Garfield, Snoopy, Stop-Sign, Windsor-Chair.
            \item \textbf{Caltech20 classes:} Face, Leopards, Motorbikes, Binocular, Brain, Camera,Car-Side, Dolla-Bill, Ferry, Garfield, Hedgehog, Pagoda, Rhino, Snoopy, Stapler, Stop-Sign, Water-Lilly, WindsorChair, Wrench, Yin-yang.
        \end{itemize}

\subsection{Hyperparameters}
    \begin{table*}
        \centering
        \caption{Network architectures.}
        \label{tab:arch}
        \tableFontSize
        \AllArch
    \end{table*}
    \customparagraph{Network architectures.}
    The encoder and decoder architectures are listed in Table~\ref{tab:arch}.
    MLP encoders/decoders are used for Caltech7 and Caltech20 as these contain vector data.
    The other datasets contain images, so CNN encoders and decoders are used for them.

    \customparagraph{Other hyperparameters.}
        \begin{table*}
            \centering
            \caption{Hyperparameters used to train the models. \EAMCLearningRate}
            \label{tab:hyperparametersAll}
            \tableFontSize
            \hyperparametersAll
        \end{table*}
        Table~\ref{tab:hyperparametersAll} lists other hyperparameters used for the baselines and new instances.

\subsection{Computational resources}
    We run our experiments on a Kubernetes cluster, where jobs are allocated to nodes with
    Intel(R) Xeon(R) E5-2623 v4 or Intel(R) Xeon(R) Silver 4210 CPUs (\( 2 \) cores allocated per job);
    and Nvidia GeForce GTX 1080 Ti or Nvidia GeForce RTX 2080 Ti GPUs.
    Each job has \( 16 \) GB RAM available.

    With this setup, \( 5 \) training runs on NoisyMNIST, NoisyFashion, EdgeMNIST, and EdgeFashion take approximately \( 24 \) hours.
    Training times for the other datasets are approximately between \( 1 \) and \( 3 \) hours.

    The Dockerfile used to build our docker image can be found in the code repository.


\subsection{Evaluation protocol}
    \customparagraph{Metrics.}
        We measure performance using the accuracy
        \begin{align}
            \text{ACC} = \max\limits_{m \in \cl M} \frac{\sum_{i=1}^{n} \delta(m(\hat y_i) - y_i)}{n}
        \end{align}
        where \( \delta(\cdot) \) is the Kronecker-delta, \( \hat y_i \) is the predicted cluster of instance \( i \), and \( y_i \) is the ground truth label of instance \( i \).
        The maximum runs over \( \cl M \), which is the set of all bijective mappings from \( \{ 1, \dots, k \} \) to itself.

        We also compute the normalized mutual information
        \begin{align}
            \text{NMI} = \frac{MI(\hat{\vec y}, \vec y)}{ \frac{1}{2}(H(\hat{\vec y}) + H(\vec y))}
        \end{align}
        where \( \hat{\vec y} = [\hat y_1, \dots, \hat y_n] \), \( \vec y = [y_1, \dots, y_n] \), \( MI(\cdot, \cdot) \) and \( H(\cdot) \) denotes the mutual information and entropy, respectively.

    \customparagraph{Uncertainty estimation.}
        The uncertainty of our performance statistic can be estimated using bootstrapping.
        Suppose the \( R \) training runs result in the \( R \) tuples
        \begin{align}
            (L_1, M_1), \dots, (L_R, M_R)
        \end{align}
        where \( L_i \) is the final loss of run \( i \), and \( M_i \) is resulting performance metric for run \( i \).
        We then sample \( B \) bootstrap samples uniformly from the original results
        \begin{align}
            & (L^b_j, M^b_j) \sim \text{Uniform}\{ (L_1, M_1), \dots, (L_R, M_R) \},\\
            \nonumber
            & \qquad j = 1, \dots, R, \quad b = 1, \dots B.
        \end{align}
        The performance statistic for bootstrap sample \( b \) is then given by
        \begin{align}
            M_{\star}^b = M^b_{j^b_{\star}}, \quad j^b_{\star} = \arg\min\limits_{j=1, \dots, R} \{ L^b_j \}.
        \end{align}
        We then estimate the uncertainty of the performance statistic by computing the standard deviation of the bootstrap statistics \( M_\star^1, \dots M_\star^B \)
        \begin{align}
            \hat\sigma_{M_\star} = \sqrt{\frac{\sum_{b=1}^{B} (M_{\star}^b - \bar{M}_{\star})^2 }{B-1}}, \text{ where}\quad \bar{M}_{\star} = \frac{\sum_{b=1}^{B} M^\star_b }{B}.
        \end{align}

\subsection{Results}
    \customparagraph{Evaluation results.}
        The complete evaluation results are given in Table~\ref{tab:fullResults}.
        \begin{table*}
            \centering
            \caption{Clustering results. Standard deviations (obtained by bootstrapping) are shown in parentheses. \( ^\dagger \) = training ran out of memory, \( ^\ddagger \) = training resulted in NaN loss.}
            \label{tab:fullResults}
            \tableFontSize
            \setlength{\tabcolsep}{.8mm}
            \input{tab/benchmark/benchmark1.tex}
            \input{tab/benchmark/benchmark2.tex}
        \end{table*}

    \customparagraph{Ablation study -- Fusion and Clustering module.}
        \begin{table}
            \centering
            \caption{Accuracies from ablation studies with the Fusion and CM components.}
            \label{tab:ablationFusionCM}
            \setlength{\tabcolsep}{.9mm}
            \tableFontSize%
            \begin{subtable}[t]{\columnwidth}
                \centering
                \caption{Fusion}
                \input{tab/ablation/fusion.tex}
            \end{subtable}
            \begin{subtable}[t]{\columnwidth}
                \centering
                \caption{CM}
                \input{tab/ablation/cm.tex}
            \end{subtable}
        \end{table}
        Table~\ref{tab:ablationFusionCM} shows the results of ablation studies with the fusion and clustering module (CM) components.
        Since these components can not be completely removed, we instead replace more complicated components, with the simplest possible component.
        Thus, we replace weighted sum with concatenate for the fusion component, and DDC with \( k \)-means for the CM component.

        For the fusion component, we see that the weighted sum tends to improve over the concatenation.
        For the CM, we observe that the performance is better with DDC than with \( k \)-means on NoisyMNIST, but the improvement more varied on Caltech7.
        This is consistent with what we observed in the evaluation results in the main paper.

    \customparagraph{Reproducibility of original results.}
        \begin{table}
            \bgroup
            \centering
            \caption{Accuracies from our experiment vs. accuracies reported by the original authors.
            \( ^\dagger \) = method is originally evaluated on a slightly different dataset.}
            \label{tab:refResults}
            \tableFontSize%
            \setlength{\tabcolsep}{.9mm}
            \input{tab/ref_results/ref_results.tex}
            \egroup
        \end{table}
        Table~\ref{tab:refResults} compares the results of our re-implementation of the baselines, to the results reported by the original authors.
        The comparison shows large differences in performance for several methods, and the differences are particularly large for MvSCN and Multi-VAE.
        For MvSCN, we do not use the same autoencoder preprocessing of the data.
        We also had difficulties getting the Cholesky decomposition to converge during training.
        For MultiVAE, we note that NoisyMNIST and NoisyFashion are generated without noise in the original paper, possibly resulting in datasets that are simpler to cluster.
        We were however not able to determine the reason for the difference in performance on COIL-20.
    
        Additionally, all methods use different network architectures and evaluation protocols in the original publications, making it difficult to accurately compare performance between methods and their implementations.
        This illustrates the difficulty of reproducing and comparing results in deep MVC, highlighting the need for a unified framework with a consistent evaluation protocol and an open-source implementation.


    \customparagraph{Sensitivity to hyperparameters}
        Table~\ref{tab:hpar} shows the results of hyperparameter sweeps for the following hyperparameters:
        \begin{itemize}
            \item Weight of reconstruction loss (\( \SVSSLWeight \)).
            \item Weight of contrastive loss (\( \MVSSLWeight \)).
            \item Temperature in contrastive loss (\( \tau \)).
            \item Weight of entropy regularization (\( \lambda \)).
        \end{itemize}
        We emphasize that these results were \emph{not} used to tune hyperparameters for the new instances.
        Rather, they are included to investigate how robust these methods are towards changes in the hyperparameter configuration.
        The results show that the new instances are mostly insensitive to changes in their hyperparameters.
        We however observe two cases where the hyperparameter configurations can have significant impact on the model performance.
        First, \cae shows a drop in performance when the weight of the contrastive loss is set to high on Caltech7 (Table~\ref{tab:hparConweight}).
        This is consistent with our observations regarding contrastive alignment on datasets with many views.
        Second, \mimvc and \mviic performs worse when the entropy regularization weight is set too low, indicating that sufficient regularization is required for these models to perform well.


        \begin{table*}
            \centering
            \caption{Results (NMI) of hyperparameter sweeps for the new instances.}
            \label{tab:hpar}
            \tableFontSize
            \setlength{\tabcolsep}{1mm}
            \begin{subtable}{\textwidth}
                \centering
                \caption{Weight of reconstruction loss (\( \SVSSLWeight \)).}
                \label{tab:hparRec}
                \input{tab/hpar/rec}
            \end{subtable}
            \begin{subtable}{\textwidth}
                \centering
                \caption{Weight of contrastive loss (\( \MVSSLWeight \)).}
                \label{tab:hparConweight}
                \input{tab/hpar/conweight}
            \end{subtable}
            \begin{subtable}{\textwidth}
                \centering
                \caption{Temperature in the contrastive loss (\( \tau \)).}
                \label{tab:hparContau}
                \input{tab/hpar/contau}
            \end{subtable}
            \begin{subtable}{\textwidth}
                \centering
                \caption{Weight of the entropy regularization (\( \lambda \)).}
                \label{tab:hparLambda}
                \input{tab/hpar/lambda}
            \end{subtable}
        \end{table*}

