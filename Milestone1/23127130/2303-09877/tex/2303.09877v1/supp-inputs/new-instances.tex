In this section we provide additional details on loss functions, particularly the weighted sum fusion, and the DDC~\cite{kampffmeyerDeepDivergencebasedApproach2019} clustering module.
The loss functions used to train the new instances are on the form
\begin{align}
    \TOTLoss = \SVSSLWeight \SVSSLLoss + \MVSSLWeight \MVSSLLoss + \CMWeight \CMLoss
\end{align}
where \( \SVSSLLoss \), \( \MVSSLLoss \), and \( \CMLoss \) denote the losses from the SV-SSL, MV-SSL, and CM components, respectively.
Note that the losses \( \SVSSLLoss \) and \( \MVSSLLoss \) correspond to the losses in Section 5 of the main paper.
\( (\SVSSLWeight, \MVSSLWeight, \CMWeight) \) are optional weights for the respective losses, which are all set to \( 1 \) unless specified otherwise.

\customparagraph{Connection between InfoDDC and contrastive self-supervised learning}
    For two views \( u \neq v \in 1, \dots, V \), contrastive SSL can be regarded as variational maximization of the mutual information
    \begin{align}
        I(\vec z^{(v)}, \vec z^{(u)})
    \end{align}
    where \( \vec z^{(v)} \) and \( \vec z^{(u)} \) have \emph{multi-variate, continuous} distributions in \( \real^d \).

    In InfoDDC, we instead maximize mutual information between pairs of \emph{uni-variate, discrete} random variables
    \begin{align}
        I(c^{(v)}, c^{(u)})
    \end{align}
    where we assume that the distributions of \( c^{(v)} \) and \( c^{(u)} \) are given by the view-specific representations
    \begin{align}
        \Prob(c^{(w)} = i) = z^{(w)}_{[i]},\quad i = 1, \dots, d, \quad w \in \{u, v\}
    \end{align}
    where \( z^{(w)}_{[i]} \) denotes component \( i \) of the view-specific representation \( \vec z^{(w)} = f^{(w)}(\vec x^{(w)}) \).
    Hence, although InfoDDC might appear similar to CA-based methods, the maximization of mutual information is done for different pairs of random variables.

\customparagraph{Weighted sum fusion.}
    As~\cite{trostenReconsideringRepresentationAlignment2021}, we implement the weighted sum fusion as
    \begin{align}
        \vec z_i = \sums{v=1}{V} w^{(v)} \vec z^{(v)}_i,
    \end{align}
    where the weights \( w^{(1)}, \dots, w^{(V)} \) are non-negative and sum to \( 1 \).
    These constraints are implemented by keeping a vector of trainable, un-normalized weights, from which \( w^{(1)}, \dots, w^{(V)} \) can be computed by applying the softmax function.

 \customparagraph{DDC clustering module.}
    The DDC~\cite{kampffmeyerDeepDivergencebasedApproach2019} clustering module consists of two fully-connected layers.
    The first layer calculates the hidden representation \( \vec h_i \in \real^{D_{DDC}} \) from the fused representation \( \vec z_i \).
    The dimensionality of the hidden representation, \( D_{DDC} \) is a hyperparameter set to \( 100 \) for all models.
    The second layer computes the cluster membership vector \( \vec \alpha_i \in \real^k \) from the hidden representation.

    DDC's loss function consists of three terms
    \begin{align}
        \CMLoss_{\text{DDC}} = \cl L_{\text{DDC, 1}} + \cl L_{\text{DDC, 2}} + \cl L_{\text{DDC, 3}}.
    \end{align}
    The three terms encourage
    \begin{enumerate*}[label=(\roman*)]
        \item separable and compact clusters in the hidden space;
        \item orthogonal cluster membership vectors; and
        \item cluster membership vectors close to simplex corners,
    \end{enumerate*}
    respectively.

    The first term maximizes the pairwise Cauchy-Schwarz divergence~\cite{jenssenCauchySchwarzDivergence2006} between clusters (represented as probability densities) in the space of hidden representations
    \begin{align}
        &\cl L_{\text{DDC, 1}} =\\
        &\binom{k}{2}^{-1}~~\sums{a=1}{k-1}\sums{b=a}{k} \frac{
            \sums{i=1}{n}\sums{j=1}{n} \alpha_{ia} \kappa_{ij} \alpha_{jb}
        }{
            \sqrt{\sums{i=1}{n}\sums{j=1}{n} \alpha_{ia} \kappa_{ij} \alpha_{ja} \sums{i=1}{n}\sums{j=1}{n} \alpha_{ib} \kappa_{ij} \alpha_{jb} }
        }
    \end{align}
    where \( \kappa_{ij} = \exp\lrp{-\frac{||\vec h_i - \vec h_j||^2}{2 \sigma^2}} \) and \( \sigma \) is a hyperparameter.
    Following~\cite{kampffmeyerDeepDivergencebasedApproach2019}, we set \( \sigma \) to \( 15\% \) of the median pairwise difference between the hidden representations.

    The second term minimizes the pairwise inner product between cluster membership vectors
    \begin{align}
        \cl L_{\text{DDC, 2}} = \frac{2}{n(n-1)} \sums{i=1}{n-1}\sums{j=i+1}{n} \vec \alpha_i \vec \alpha_j\T.
    \end{align}

    The third term encourages cluster membership vectors to be close to the corners of the probability simplex in \( \real^k \)
    \begin{align}
        &\cl L_{\text{DDC, 3}} =\\
        &\binom{k}{2}^{-1}~~\sums{a=1}{k-1}\sums{b=a}{k} \frac{
            \sums{i=1}{n}\sums{j=1}{n} m_{ia} \kappa_{ij} m_{jb}
        }{
             \sqrt{\sums{i=1}{n}\sums{j=1}{n} m_{ia} \kappa_{ij} m_{ja} \sums{i=1}{n}\sums{j=1}{n} m_{ib} \kappa_{ij} m_{jb} }
        }
    \end{align}
    where \( m_{ia} = \exp(-||\vec \alpha_i - \vec e_a||^2) \), and \( \vec e_a \) is the \( a \)-th simplex corner.

