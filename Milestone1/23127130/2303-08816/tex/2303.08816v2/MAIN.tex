\documentclass[11pt]{article}
\pdfoutput=1
\usepackage{enumerate}
\usepackage{pdfsync}
\usepackage[OT1]{fontenc}
%\usepackage{kpfonts}

\usepackage[usenames]{color}
%\usepackage[dvips]{graphicx}
\usepackage[colorlinks,
            linkcolor=red,
            anchorcolor=blue,
            citecolor=blue
            ]{hyperref}
%\usepackage{mathrsfs}
\usepackage{fullpage}
%\usepackage{hyperref}
\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage{pbox}
\usepackage{setspace}
\usepackage{tabularx}
\usepackage{float}
\usepackage{wrapfig,lipsum}
\usepackage{enumitem}
\usepackage{natbib}
\usepackage{multirow} 

\usepackage{subcaption}
\usepackage{linegoal}


% \renewcommand{\baselinestretch}{1.04}
% %\renewcommand{\floatpagefraction}{.8}

% \makeatletter
% \newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
% \makeatother


\usepackage{uclaml_macro}

\allowdisplaybreaks

\newcommand{\alglinelabel}{%
  \addtocounter{ALC@line}{-1}% Reduce line counter by 1
  \refstepcounter{ALC@line}% Increment line counter with reference capability
  \label% Regular \label
}
\usepackage{enumitem}
\newcommand{\tao}[1]{{\color{cyan}{\bf [Tao: #1]}}}
\newcommand{\yue}[1]{{\color{red}{\bf [Yue: #1]}}}
\newcommand{\hao}[1]{{\color{blue}{\bf [Hao: #1]}}}
\newcommand{\gu}[1]{{\color{orange}{\bf [Gu: #1]}}}

\usepackage[textsize=tiny]{todonotes}

% \setlength{\marginparwidth}{0.6in}
% \newcommand{\todoq}[2][]{\todo[size=\scriptsize,color=orange!20!white,#1]{Quanquan: #2}}

%%
\newcommand{\parenv}[1]{\left(#1\right)}


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{tikz}
\usetikzlibrary{fit}

\begin{document}
\title{\huge Borda Regret Minimization for Generalized Linear Dueling Bandits}
\author
{
Yue Wu\thanks{
Department of Computer Science, 
University of California, Los Angeles, 
Los Angeles, 
CA 90095; 
e-mail: {\tt ywu@cs.ucla.edu}
} 
	~~
Tao Jin\thanks{
Department of Computer Science,
University of Virginia,
Charlottesville, 
VA 22903;
e-mail: {\tt taoj@virginia.edu}} 
	~~
Qiwei Di \thanks{
Department of Computer Science, 
University of California, Los Angeles, 
Los Angeles, 
CA 90095; 
} 
~~
Hao Lou\thanks{
Department of Electrical \& Computer Engineering,
University of Virginia,
Charlottesville, 
VA 22903;
e-mail: {\tt haolou@virginia.edu}} 
	~~
Farzad Farnoud\thanks{
Department of Electrical \& Computer Engineering,
University of Virginia,
Charlottesville, 
VA 22903;
e-mail: {\tt farzad@virginia.edu}} 
	~~
Quanquan Gu\thanks{Department of Computer Science, University of California, Los Angeles, Los Angeles, CA 90095; e-mail: {\tt qgu@cs.ucla.edu}}
}
\date{}

\maketitle
\begin{abstract}
    Dueling bandits are widely used to model preferential feedback prevalent in many applications such as recommendation systems and ranking. 
    In this paper, we study the Borda regret minimization problem for dueling bandits, which aims to identify the item with the highest Borda score while minimizing the cumulative regret.
    We propose a rich class of generalized linear dueling bandit models, which cover many existing models.
    We first prove a regret lower bound of order $\Omega(d^{2/3} T^{2/3})$ for the Borda regret minimization problem, where $d$ is the dimension of contextual vectors and $T$ is the time horizon.
    To attain this lower bound, we propose an explore-then-commit type algorithm for the stochastic setting, which has a nearly matching regret upper bound $\tilde{O}(d^{2/3} T^{2/3})$. 
    We also propose an EXP3-type algorithm for the adversarial linear setting, where the underlying model parameter can change at each round. Our algorithm achieves an $\tilde{O}(d^{2/3} T^{2/3})$ regret, which is also optimal.
    Empirical evaluations on both synthetic data and a simulated real-world environment are conducted to corroborate our theoretical analysis.
\end{abstract}

\section{Introduction}
%With a plethora of choices for decision-making systems to pick, the problem of identifying the best choice among candidates and how to explore efficiently have been studied numerous times throughout years \cite{EvenDar2002PACBF, Ramamohan2016DuelingBB, Saha2022VersatileDB}. 
%The agent's goal is usually to maximize rewards and the common strategy is keeping a proper balance between trying arms whose rewards the agent is not certain with and pulling the arm that produces the highest reward so far. 

% talk a bit about the applications

%Scalar/ordinal values associated with a choice are simple to obtain but has limited expressiveness compared to the \textit{preferential feedback}. The latter can capture complex ordering of items without an implicit global ordering narrative as most scoring system implies. 
%In addition, the preferential/comparative feedback are more natural for human to provide, while coming up a cardinal value from a human is not always easy and accurate: usually it relies on an underlying scoring system in mind that might not be consistent. Thus, we believe it is more important to study the problem with preferential feedback.

Multi-armed bandits (MAB) \citep{Lattimore2020BanditA} is an interactive game where at each round, an agent chooses an arm to pull and receives a noisy reward as feedback. In contrast to numerical feedback considered in classic MAB settings, preferential feedback is more natural in various online learning tasks including information retrieval~\cite{yue2009interactively}, recommendation systems~\cite{sui2014clinical}, ranking~\cite{Minka2018TrueSkill2A}, crowdsourcing~\cite{chen2013pairwise}, etc. Moreover, numerical feedback is also more difficult to gauge and prone to errors in many real-world applications. For example, when provided with items to shop or movies to watch, it is more natural for a customer to pick a preferred one than scoring the options. This motivates
\emph{Dueling Bandits} \citep{yue2009interactively}, where the agent repeatedly pulls two arms at a time and is provided with feedback being the binary outcome of ``duels'' between the two arms. 
%The line of work for preferential feedback in the multi-armed bandit (MAB) community is called dueling bandits \cite{Yue2012TheKD}, where the learner instead of receiving the feedback for a single item, it picks a pair of items to be compared and receives a comparative result. 


In dueling bandits problems, the outcome of duels is commonly modeled as Bernoulli random variables due to their binary nature. At each round, suppose the agent chooses to compare arm $i$ and $j$, then the binary feedback is assumed to be sampled independently from a Bernoulli distribution. For a dueling bandits instance with $K$ arms, the probabilistic model of the instance can be fully characterized by a $K \times K$ preference probability matrix with each entry being:
$
    p_{i,j} = \PP (\text{arm $i$ is chosen over arm $j$}).
$

In a broader range of applications such as ranking, ``arms'' are often referred to as ``items''. We will use these two terms interchangeably in the rest of this paper. 
One central goal of dueling bandits is to devise a strategy to identify the ``optimal'' item as quickly as possible, measured by either sample complexity or cumulative regret. However, the notion of optimality for dueling bandits is way harder to define than for multi-armed bandits. The latter can simply define the arm with the highest numerical feedback as the optimal arm, while for dueling bandits there is no obvious definition solely dependent on $\{p_{i,j} | i,j \in [K] \}$.

The first few works on dueling bandits imposed strong assumptions on $p_{i,j}$. For example, \citet{Yue2012TheKD} assumed that there exists a true ranking that is coherent among all items, and the preference probabilities must satisfy both strong stochastic transitivity (SST) and stochastic triangle inequality (STI). While relaxations like weak stochastic transitivity~\citep{falahatgar2018limits} or relaxed stochastic transitivity~\citep{Yue2011BeatTM} exist, they typically still assume the true ranking exists and the preference probabilities are consistent, i.e., $p_{i,j} > \frac12$ if and only if $i$ is ranked higher than $j$.
In reality, the existence of such coherent ranking aligned with item preferences is rarely the case. For example, $p_{i,j}$ may be interpreted as the probability of one basketball team $i$ beating another team $j$, and there can be a circle among the match advantage relations.  
% These aforementioned assumptions can also be implied by parametric models like the Bradley–Terry model. 
% We emphasize that these consistency conditions are not assumed or implicitly implied in our setting. 
% some of them is of maximum item identification. Some of them is for pure exploration and rank estimation. 
%Another line of work is to assume the existence of an optimal arm or a total ranking by imposing consistency conditions on the preference probabilities. Common consistency conditions include Strong Stochastic Transitivity (SST), Weak Stochastic Transitivity (WST), 
 % and Stochastic Triangle Inequality (STI).

In this paper, we do not assume such coherent ranking exists and solely rely on the \emph{Borda score} based on preference probabilities. The Borda score $B(i)$ of an item $i$ is the probability that it is preferred when compared with another random item, namely $B(i) := \frac{1}{K-1} \sum_{j \ne i} p_{i,j}$. The item with the highest Borda score is called the \textit{Borda winner}. The Borda winner is intuitively appealing and always well-defined for any set of preferential probabilities. The Borda score also does not require the problem instance to obey any consistency or transitivity, and it is considered one of the most general criteria.

To identify the Borda winner, estimations of the Borda scores are needed. Since estimating the Borda score for one item requires comparing it with every other items,  the sample complexity is prohibitively high when there are numerous items. On the other hand, in many real-world applications, the agent has access to side information that can assist the evaluation of $p_{i,j}$. For instance, an e-commerce item carries its category as well as many other attributes, and the user might have a preference for a certain category \citep{Wang2018BillionscaleCE}. For a movie, the genre and the plot as well as the directors and actors can also be taken into consideration when making choices \citep{Liu2017CollaborativeTR}. 

%Some work explores the item-wise relationship contribution to the final ranking by imposing assumptions such as a strong one named Strong Stochastic Transitivity (SST) in \cite{Yue2012TheKD}, which is rarely the case in real-life. 
%There are also systems that completely disregard the item-wise relationships to solely rely on a metric based on winning probabilities of item pairs such as Borda/or Copeland scores \cite{Zoghi2015CopelandDB, Saha2021AdversarialDB}.

% While in most applications, choices usually accompanies their
%However, those works consider no side information which is usually present in real-world settings. For instance, an e-commerce item carries its category as well as many other attributes, and the user might have a preference for a certain category \cite{Wang2018BillionscaleCE}. For a movie, the genre and the plot as well as directors and actors can also be taken into consideration when making choices% \cite{}
%. This is not reflected in above-mentioned algorithms. Thus, we introduce the problem of Contextual Borda Dueling Bandits which effectively achieves the goal in the middle: we do not impose strong item-wise assumption, neither do we completely discard their relationship. Instead, their relationship can be captured linearly based on the contextual vector.
% \paragraph{Contributions} 

Based on the above motivation, we consider \emph{Generalized Linear Dueling Bandits}. At each round, the agent selects two items from a finite set of items and receives a comparison result of the preferred item. The comparisons depend on known intrinsic contexts/features associated with each pair of items. The contexts can be obtained from upstream tasks, such as topic modeling \citep{Zhu2012MedLDAMM} or embedding \citep{vasile2016meta}.
Our goal is to adaptively select items and minimize the regret with respect to the optimal item (i.e., Borda winner). 
Our main contributions are summarized as follows:
\begin{itemize}[leftmargin=*,nosep]
    % \item We propose a new generalized linear dueling bandit model along with a Borda regret. Our model can cover multi-armed dueling bandits, as well as some more specific formulations of contextual dueling bandits in prior works \citep{saha2021optimal}.
    \item We show a hardness result regarding the Borda regret minimization for the (generalized) linear model. We prove a worst-case regret lower bound $\Omega(d^{2/3} T^{2/3})$ for our dueling bandit model, showing that even in the stochastic setting, minimizing the Borda regret is difficult. The construction and proof of the lower bound are new and might be of independent interest.
    \item We propose an explore-then-commit type algorithm under the stochastic setting, which can achieve a nearly matching upper bound $\tilde{O}(d^{2/3} T^{2/3})$. When the number of items $K$ is small, the algorithm can also be configured to achieve a smaller regret $\tilde{O}\big( (d \log K)^{1/3} T^{2/3}\big)$.
    \item We propose an EXP3 type algorithm for  linear dueling bandits under the adversarial setting, which can achieve a nearly matching upper bound $\tilde{O}\big( (d \log K)^{1/3} T^{2/3}\big)$. 
    \item We conduct empirical studies to verify the correctness of our theoretical claims. Under both synthetic and real-world data settings, our algorithms can  outperform all the baselines in terms of cumulative regret.
    % but also exhibits minimal performance fluctuation throughout repeated runs. 
\end{itemize}


%TODO: 1. Borda compared to cordorcet, why it is a better metric 2. context is more efficient 3. we can only do T^2/3 

\paragraph{Notation}
In this paper, we use normal letters to denote scalars, lowercase bold letters to denote vectors, and uppercase bold letters to denote matrices. For a vector $\xb$, $\|\xb\|$ denotes its $\ell_2$-norm. The weighted $\ell_2$-norm associated with a positive-definite matrix $\Ab$ is defined as $\|\xb\|_{\Ab} = \sqrt{\xb^{\top} \Ab \xb}$.
The minimum eigenvalue of a matrix $\Ab$ is written as $\lambda_{\min}(\Ab)$.
We use $\Ab \succeq \Bb$ to denote that the matrix $\Ab - \Bb$ is positive semi-definite.
We use standard asymptotic notations including $O(\cdot), \Omega(\cdot), \Theta(\cdot)$, and $\tilde O(\cdot),\tilde\Omega(\cdot), \tilde\Theta(\cdot)$ will hide logarithmic factors.
 For a positive integer $N$, $[N] := \{1,2,\dots,N\}$.
\section{Related Work}


% \subsection{Bandits}

\textbf{Multi-armed and Contextual Bandits} Multi-armed bandit is a problem of identifying the best choice in a sequential decision-making system. It has been studied in numerous ways with a wide range of applications~\citep{EvenDar2002PACBF,lai1985asymptotically,kuleshov2014algorithms}. Contextual linear bandit is a special type of bandit problem where the agent is provided with side information, i.e., contexts, and rewards are assumed to have a linear structure. Various algorithms~\citep{rusmevichientong2010linearly,filippi2010parametric,AbbasiYadkori2011ImprovedAF,li2017provably,jun2017scalable} have been proposed to utilize this contextual information. 

\textbf{Dueling Bandits and Its Performance Metrics} Dueling bandits is a variant of MAB with preferential feedback ~\citep{Yue2012TheKD,Zoghi2014RelativeUC,Zoghi2015CopelandDB}. A comprehensive survey can be found at~\citet{bengs2021preference}. As discussed previously, the probabilistic structure of a dueling bandits problem is governed by the preference probabilities, over which an optimal item needs to be defined. Optimality under the \emph{Borda score} criteria has been adopted by several previous works~\citep{jamieson2015sparse,falahatgar2017maxing,heckel2018approximate,Saha2021AdversarialDB}. 
The most relevant work to ours is \citet{Saha2021AdversarialDB}, where they studied the problem of regret minimization for adversarial dueling bandits and proved a $T$-round Borda regret upper bound $\tilde O(K^{1/3}T^{2/3})$. They also provide an $\Omega(K^{1/3}T^{2/3})$ lower bound for stationary dueling bandits using Borda regret. 
% Together their result implies that for multi-armed stochastic dueling bandit, the minimum Borda regret must be of the order $\Omega(K^{1/3}T^{2/3})$.
% In contrast, the model considered in this paper takes contextual information into account and the preference probabilities depend on a linear function of $d$-dimensional feature vectors. Proved upper and lower bounds on the Borda regret are thus determined by $d$. 

%In~\cite{Yue2012TheKD}, the proposed algorithm Interleaved Filtering (IF) works on a preferential system that satisfies Strong Stochastic Transitivity (SST) and Stochastic Triangle Inequality (STI). They defined the strong regret as $\sum_{t \in T} P(i^*, i_t) + P(i^*, j_t) - 1$. Their proved regret is of approximately $K\log(T)$ with a matching information theoretic lower bound. To improve this, Beat the Mean algorithm \cite{Yue2011BeatTM} relaxed SST to Relaxed Stochastic Transitivity (RST). And had a better instance dependent regret bound of the same order. We emphasize that conditions like the aforementioned transitivities can be implied by some structured parametric models.
%Since even RST is a strong assumption in certain cases, and STI rarely holds in real-world cases such as tournaments and voting due to inconsistencies and paradoxes. Some work has resorted to a metric based solution where the items are ranked according to scores such as Borda and Copeland.
%A generic algorithm \cite{Urvoy2013GenericEA} proposed a sampling routing to estimate sets of parameters effectively, which they were able to directly apply it to the Copeland dueling bandits problem by estimating $K(K-1)/2$ parameters in the probability matrix $P$. In case a Condorcet winner existence assumption is given, an improved exploration bound can be derived. 

Apart from the Borda score, \emph{Copeland score} is also a widely used criteria~\citep{Urvoy2013GenericEA,Zoghi2015CopelandDB,zoghi2014relative,Wu2016DoubleTS,komiyama2016copeland}. It is defined as $C(i) := \frac{1}{K-1} \sum_{j \ne i} \ind \{ p_{i,j} > 1/2 \} $. A Copeland winner is the item that beats the most number of other items. It can be viewed as a ``thresholded'' version of Borda winner. In addition to Borda and Copeland winners, optimality notions such as a von Neumann winner were also studied in~\citet{Ramamohan2016DuelingBB,Dudk2015ContextualDB,balsubramani2016instance}.
%A theoretical study of Copeland bandits appears in \cite{Zoghi2015CopelandDB}. Denote the Copeland score as $C_i := \sum_{j} \mathbbm{1} (P(i, j) > \frac12) $. Their regret is defined on two arms that are pulled at $t \in [T]$, $\text{Regret}(T) = \EE[\sum_t (2C_{i^*} - C_{i_t} - C_{j_t})]$. They introduced an algorithm of producing $O(K^2\log T)$ instance dependent regret upper bound of the Copeland bandit and with $O(K\log T)$ regret with Condorcet assumption. 

Another line of work focuses on identifying the optimal item or the total ranking, assuming the preference probabilities are consistent. Common consistency conditions include Strong Stochastic Transitivity~\citep{Yue2012TheKD,falahatgar2017maxing,falahatgar2017maximum}, Weak Stochastic Transitivity~\citep{falahatgar2018limits,ren2019sample,wu2022adaptive,lou2022active}, Relaxed Stochastic Transitivity~\citep{Yue2011BeatTM} and Stochastic Triangle Inequality. Sometimes the aforementioned transitivity can also be implied by some structured models like the Bradley–Terry model.
We emphasize that these consistency conditions are not assumed or implicitly implied in our setting.


\textbf{Contextual Dueling Bandits} In~\citet{Dudk2015ContextualDB}, contextual information is incorporated in the dueling bandits framework. Later,~\citet{saha2021optimal} studied a structured contextual dueling bandits setting where each item $i$ has its own contextual vector $\xb_i$ (sometimes called Linear Stochastic Transitivity). Each item then has an intrinsic score $v_i$ equal to the linear product of an unknown parameter vector $\btheta^*$ and its contextual vector $\xb_i$. The preference probability between two items $i$ and $j$ is assumed to be $\mu(v_i - v_j)$ where $\mu(\cdot)$ is the logistic function. These intrinsic scores of items naturally define a ranking over items. The regret is also computed as the gap between the scores of pulled items and the best item. While in this paper, we assume that the contextual vectors are associated with item pairs and define regret on the Borda score. In Section~\ref{subsec:structure}, we provide a more detailed discussion showing that the setting considered in~\citet{saha2021optimal} can be viewed as a special case of our model.
%To improve the efficiency of exploration, one may consider the side information available associated with each arm to formulate it in linear bandit settings \citep{AbbasiYadkori2011ImprovedAF}. However, the vanilla linear regression model does not catch the probabilities well, otherwise more assumption or restriction has to be given to the parameter space so that the output matches the probability values, which is usually in $[0, 1]$. Thus, it is natural to consider generalized linear models (GLM) since a logistic regression can be incorporated into the framework. For the non-dueling regret, the SupCB-GLM \cite{li2017provably} adopted the UCB algorithm into contextual linear bandit setting and achieved $\tilde{O}(\sqrt(dT))$ regret. 


%No single work that the authors known of have proposed algorithm for regret minimization problem for the dueling Borda winner under stochastic bandit setting that achieves an instance independent $\log(T)$ regret.
%However, recently one work studies it in adversarial case \cite{Saha2021AdversarialDB} sheds some light on the lower bound of such problem which disputes the possibility of a logarithmic regret in this setting. It turned out that the lower bound of the regret is of $\Omega(K^{1/3}T^{2/3})$. It also matches the upper bound for the adversarial case.
%TODO: We also show this in the appendix for completness of the literature.
%In contrast, for adversarial bandits the upper bound is $O(K^{1/2}T^{1/2})$. 

% In addition to the Borda and Copeland winner, there are other metrics of interest. Other not so popular metrics were studies in \cite{Ramamohan2016DuelingBB} such as Top cycle, Uncovered Set and Banks set. Von Neumann winner that has a game theoretical maxmin interpretation is mentioned in \cite{Dudk2015ContextualDB}.

% \subsection{Contextual Bandits}

% \subsection{Contextual Dueling Bandits}
%The dueling contextual bandit was discussed in \cite{saha2021optimal}, their structural assumption is has similar of those in to SST, a regret of $\tilde{O}(\sqrt{dT})$ is proven with matching lower bound. We discuss this further in \Cref{subsec:structure}. 
% TODO: Another work \cite{Saha2021EfficientAO} considers the preference matrix is generated by a given function class 



\section{Backgrounds and Preliminaries} \label{sec:prelim}
\subsection{Problem Setting} \label{sec:probsetup}
We first consider the stochastic preferential feedback model with $K$ items in the fixed time horizon setting. We denote the item set by $[K]$ and let $T$ be the total number of rounds. At each round $t$, the agent can pick any pair of items $(i_t, j_t)$ to compare and receive stochastic feedback about whether item $i_t$ is preferred over item $j_t$, (denoted by $i_t \succ j_t$). We denote the probability of seeing the event $i \succ j$ as $p_{i,j} \in [0,1]$. Naturally, we assume $p_{i,j} + p_{j,i} = 1$, and $p_{i,i} = 1/2$. 

In this paper, we are concerned with the generalized linear model (GLM), where there is assumed to exist an \textit{unknown} parameter $\btheta^* \in \RR^d$, and each pair of items $(i,j)$ has its own \textit{known} contextual/feature vector $\bphi_{i,j} \in \RR^d$ with $\|\bphi_{i,j}\| \le 1$. There is also a fixed known link function (sometimes called comparison function) $\mu(\cdot)$ that is monotonically increasing and satisfies $\mu(x) + \mu(-x) = 1$, e.g. a linear function or the logistic function $\mu(x) = 1/(1+e^{-x})$. The preference probability is defined as $p_{i,j} = \mu( \bphi_{i,j}^{\top} \btheta^* )$. At each round, denote $r_{t} = \ind \{i_t \succ j_t \}$, then we have

\begin{align*}
    \EE[r_{t} | i_t, j_t]
    & =
    p_{i_t,j_t}
    = 
    \mu( \bphi_{i_t,j_t}^{\top} \btheta^*  ).
\end{align*}
Then our model can also be  written as 
\begin{align*}
    r_{t} = \mu(\bphi_{i_t,j_t}^{\top} \btheta^*) + \epsilon_{t},
\end{align*}
where the noises $\{\epsilon_t\}_{t \in [T]}$ are zero-mean, $1$-sub-Gaussian and assumed independent from each other. Note that, given the constraint $p_{i,j} + p_{j,i} = 1$, it is implied that $\bphi_{i,j} = - \bphi_{j,i}$ for any $i\in[K], j\in[K]$. 


The agent's goal is to maximize the cumulative Borda score. The (slightly modified \footnote{Previous works define Borda score as $B'_i = \frac{1}{K-1} \sum_{j \ne i} p_{i,j}$, excluding the diagonal term $p_{i,i} = 1/2$. Our definition is equivalent since the difference between two items satisfies $B(i) - B_j = \frac{K-1}{K} (B'_i - B'_j)$. Therefore, the regret will be in the same order for both definitions.}) Borda score of item $i$ is defined as $B(i) = \frac{1}{K}\sum_{j =1}^{K}  p_{i,j}$, 
and the Borda winner is defined as $i^* = \argmax_{i\in[K]} B(i)$.  
The problem of merely identifying the Borda winner was deemed trivial \citep{Zoghi2014RelativeUC,BusaFekete2018PreferencebasedOL} because for a fixed item $i$, uniformly random sampling $j$ and receiving feedback $r_{i,j} = \mathrm{Bernoulli}(p_{i,j})$ yield a Bernoulli random variable with its expectation being the Borda score $B(i)$. This so-called  \textit{Borda reduction} trick makes identifying the Borda winner as easy as the best-arm identification for $K$-armed bandits. Moreover, if the regret is defined as $\mathrm{Regret}(T) = \sum_{t=1}^{T} (B(i^*) - B(i_t))$, then any optimal algorithms for multi-arm bandits can achieve $\tilde{O}(\sqrt{T})$ regret.

However, the above definition of regret does not respect the fact that a pair of items are selected at each round. When the agent chooses two items to compare, it is natural to define the regret so that both items contribute equally. A commonly used regret, e.g., in~\citet{Saha2021AdversarialDB}, has the following form:
\begin{align} \label{eqn:borda-regret}
    \mathrm{Regret}(T) = \sum_{t=1}^{T} \big( 2B(i^*) - B(i_t) - B(j_t) \big),
\end{align}
where the regret is defined as the sum of the sub-optimality of both selected arms. Sub-optimality is measured by the gap between the Borda scores of the compared items and the Borda winner. This form of regret deems any classical multi-arm bandit algorithm with Borda reduction vacuous because taking $j_t$ into consideration will invoke $\Theta(T)$ regret. 

% Later we will see that minimizing the dueling regret is far more difficult and non-trivial. The best regret any algorithm can achieve is $\Omega(T^{2/3})$, showing that the problem itself is not as easy as a standard bandit problem.

\paragraph{Adversarial Setting} {\citet{saha2021adversarial} considered an adversarial setting for the multi-armed case, where at each round $t$, the comparison follows a potentially different probability model, denoted by $\{p^t_{i,j}\}_{i, j \in [K]}$. In this paper, we consider its contextual counterpart. Formally, we assume there is an underlying parameter $\btheta^*_t$, and at round $t$, the preference probability is defined as $p^t_{i,j} = \mu( \bphi_{i,j}^{\top} \btheta_t^* )$. }

The Borda score of item $i \in [K]$ at round $t$ is defined as $B_t(i) = \frac{1}{K}\sum_{j =1}^{K}  p^{t}_{i,j}$, 
and the Borda winner at round $T$ is defined as $i^* = \argmax_{i\in[K]} \sum_{t=1}^{T} B_t(i)$.  The $T$-round regret is thus defined as
$\mathrm{Regret}(T) = \sum_{t=1}^{T} \big( 2B_t(i^*) - B_t(i_t) - B_t(j_t) \big)$.

% Each pair of items $(i,j)$ have their unique associated context/feature $\bphi_{i,j}$ which is assumed to be provided by an upstream algorithm/model. To connect it with current stochastic preferential model, we let $P(i,j) = F(\langle \bphi_{ij},\btheta \rangle )$, where $F(\cdot)$ is a link function, which can be sigmoid. 
% The weight vector $\btheta \in \RR^d$.  The parameter $\btheta$ is to be estimated. In addition, to satisfy the ``order invariance'' assumption, given how $P(i, j)$ is defined, we also need $\bphi_{i,j} = -\bphi_{j,i}$.

% Then the Borda score can be defined on $P(i,j)$. Borda score of item $i \in [K]$ is $B(i) = \sum_{j \in [K]} \frac1K P(i,j)$.

% Now, we proceed to define the notion of regret in the sense of dueling bandit. Note that for the non-dueling regret, the conventional case can be simply reduced to this problem by sampling the opponent uniformly at random. Given the horizon $T$, the algorithm is going to decide a pair $(i_t, j_t)$ to be queried at each $t \in [T]$. Then the total regret can be written as 
% $R_T = \EE\left[\sum_{t \in [T]} \biggl( 2B(i^*) - B(i_t) - B(j_t) \biggr)\right]$.

\subsection{Assumptions}
In this section, we present the assumptions required for establishing theoretical guarantees. Due to the fact that the analysis technique is largely extracted from \citet{li2017provably}, we follow them to make assumptions to enable regret minimization for generalized linear dueling bandits. 

We make a regularity assumption about the distribution of the contextual vectors:
\begin{assumption} \label{assumption:lambda0}
There exists a constant $\lambda_0 > 0$ such that $\lambda_{\min} \big(\frac{1}{K^2}\sum_{i=1}^{K} \sum_{j=1}^{K} \bphi_{i,j} \bphi_{i,j}^{\top} \big) \ge \lambda_0$. 
\end{assumption}
This assumption is only utilized to initialize the design matrix $\Vb_{\tau} = \sum_{t=1}^{\tau} \bphi_{i_t,j_t}\bphi_{i_t,j_t}^{\top}$ so that the minimum eigenvalue is large enough. We follow \citet{li2017provably} to deem $\lambda_0$ as a constant.



We also need the following assumption regarding the link function $\mu(\cdot)$:
\begin{assumption} \label{assumption:kappa} Let $\dot{\mu}$ be the first-order derivative of $\mu$. We have
$
    \kappa :=
    \inf_{\|\xb\| \le 1, \| \btheta - \btheta^* \| \le 1} \dot{\mu}(\xb^{\top} \btheta) > 0. 
$
\end{assumption}
Assuming $\kappa > 0$ is necessary to ensure the maximum log-likelihood estimator can converge to the true parameter $\btheta^*$ \citep[Section 3]{li2017provably}.
This type of assumption is commonly made in previous works for generalized linear models~\citep{filippi2010parametric, li2017provably, faury2020improved}. 

Another common assumption is regarding the continuity and smoothness of the link function.
\begin{assumption} \label{assumption:mu}
$\mu$ is twice differentiable. Its first and second-order derivatives are upper-bounded by constants $L_{\mu}$ and $M_{\mu}$ respectively.
\end{assumption}
This is a very mild assumption. For example, it is easy to verify that the logistic link function satisfies \cref{assumption:mu} with $L_{\mu} = M_{\mu} = 1/4$.

% \subsection{Existing Results for Structured Contexts}\label{subsec:structure}
% \yue{consider shrink this subsection?}
% A structural assumption made by some previous works \citep{saha2021optimal} is that $\bphi_{i,j} = \xb_i - \xb_j$, where $\xb_i$ can be seen as some feature vectors tied to the item. In this work, we do not consider minimizing the Borda regret under the structural assumption. 

% The immediate reason is that, when $p_{i,j} = \mu(\xb_{i}^{\top}\btheta^* - \xb_{j}^{\top}\btheta^*)$, with $\mu(\cdot)$ being the logistic function, the probability model $p_{i,j}$ effectively becomes (a linear version of) the well-known Bradley-Terry model. Namely, each item is tied to a value $v_i = \xb_i^{\top} \btheta^*$, and the comparison probability follows $p_{i,j} = \frac{e^{v_i}}{e^{v_i} + e^{v_j}}$.
% More importantly, this kind of model satisfies both the strong stochastic transitivity (SST) and the stochastic triangle inequality (STI), which are unlikely to satisfy in reality.

% Furthermore, when stochastic transitivity holds, there is a true ranking among the items, determined by $\xb_i^{\top} \btheta^*$. A true ranking renders concepts like the Borda winner or Copeland winner redundant because the rank-one item will always be the winner in every sense. When $\bphi_{i,j} = \xb_i - \xb_j$, \citet{saha2021optimal} proposed algorithms that can achieve nearly optimal regret $\tilde{O}(d\sqrt{T})$, with regret being defined as 
% \begin{align} \label{eqn:saha-regret}
%     \mathrm{Regret}(T) = \sum_{t=1}^{T} 2\la \xb_{i^*}, \btheta^* \ra - \la \xb_{i_t}, \btheta^* \ra - \la \xb_{j_t}, \btheta^* \ra,
% \end{align}
% where $i^* = \argmax_{i} \la \xb_{i}, \btheta^* \ra$, which also happens to be the Borda winner.
% Meanwhile, by \cref{assumption:mu},
% \begin{align*}
%     B(i^*) - B(j) 
%     & =
%     \frac{1}{K}
%     \sum_{k=1}^{K}
%     \big[\mu(\la \xb_{i^*} - \xb_{k}, \btheta^* \ra) - \mu(\la \xb_{j} - \xb_{k}, \btheta^* \ra) \big]
%     \le L_{\mu} \cdot \la \xb_{i^*} - \xb_{j} , \btheta^* \ra,
% \end{align*}
% where $L_{\mu}$ is the upper bound on the derivative of $\mu(\cdot)$. For logistic function $L_{\mu} = 1/4$. The Borda regret \eqref{eqn:borda-regret} is thus at most a constant multiple of \eqref{eqn:saha-regret}. This shows Borda regret minimization can be sufficiently solved by \citet{saha2021optimal} when structured contexts are present. 
% We consider the most general case where the only restriction is the implicit assumption that $\bphi_{i,j} = - \bphi_{j,i}$.


\section{The Hardness Result} \label{sec:hardness}


\begin{figure*}[hbt]
    \centering
    % \begin{scaletikzpicturetowidth}{\textwidth}
    \input{mat.tex}
    % \end{scaletikzpicturetowidth}
    \caption{Illustration of the hard-to-learn preference probability matrix $\{p^{\btheta}_{i,j}\}_{i \in [K], j \in [K]}$. There are $K = 2^{d+1}$ items in total. The first $2^d$ items are ``good'' items with higher Borda scores, and the last $2^d$ items are ``bad'' items. The upper right block $\{p_{i,j}\}_{i < 2^d, j \ge 2^d}$ is defined as shown in the blue bubble. The lower left block satisfies $p_{i,j} = 1 - p_{j,i}$. For any $\btheta$, there exist one and only best item $i$ such that $\textbf{bit}(i) = \textbf{sign}(\btheta)$.}
    \label{fig:hardness_mat}
\end{figure*}


This section presents \cref{thm:lower}, a worst-case regret lower bound for the stochastic linear dueling bandits. The proof of \cref{thm:lower} relies on a class of hard instances, as shown in \cref{fig:hardness_mat}. We show that any algorithm will incur a certain amount of regret when applied to this hard instance class. The constructed hard instances follow a stochastic linear model, which is a sub-class of the generalized linear model. \citet{saha2021adversarial} 
first proposed a similar construction for finite many arms with no contexts. Our construction is for a contextual setting and the proof of the lower bound takes a rather different route. 

For any $d > 0$, we construct the class of hard instances as follows. An instance is specified by a vector $\btheta \in \{- \Delta, + \Delta \}^{d} $.
The instance contains $2^{d+1}$ items (indexed from 0 to $2^{d+1}-1$).
The preference probability for an instance is defined by $p^{\btheta}_{i,j}$ as:
\begin{align*}
    p^{\btheta}_{i,j} & =
    \begin{cases}
        \frac{1}{2}, \text{ if } i < 2^d, j < 2^d   \text{ or if } i \ge 2^d, j \ge 2^d  \\
        \frac{3}{4}, \text{ if } i < 2^d, j \ge 2^d   \\
        \frac{1}{4}, \text{ if } i \ge 2^d, j < 2^d   \\
    \end{cases}
    + \la \bphi_{i,j}, \btheta \ra, 
\end{align*}
and the $d$-dimensional feature vectors $\bphi_{i,j}$ are given by 
\begin{align*}
    \bphi_{i,j} & =
    \begin{cases}
        \mathbf{0}, \text{ if } i < 2^d, j < 2^d   \text{ or if } i \ge 2^d, j \ge 2^d      \\
        \textbf{bit}(i), \text{ if } i < 2^d, j \ge 2^d   \\
        -\textbf{bit}(j), \text{ if } i \ge 2^d, j < 2^d, \\
    \end{cases}
\end{align*}
where $\textbf{bit}(\cdot)$ is the (shifted) bit representation of non-negative integers, i.e., suppose $x$ has the binary representation $x= b_0 \times 2^0 + b_1 \times 2^1 + \cdots + b_{d-1} \times 2^{d-1}$, then \[\textbf{bit}(x) =(2b_0-1,2b_1-1,\ldots, 2b_{d-1}-1)  = 2 \bb - 1.\] Note that $\textbf{bit}(\cdot) \in \{-1, +1 \}^{d}$, and that $\bphi_{i,j} = - \bphi_{j,i}$ is satisfied. The definition of $p^{\btheta}_{i,j}$ can be slightly tweaked to fit exactly the model described in \cref{sec:prelim} (see \cref{rem:hard-d+1} in Appendix).

Some calculation shows that the Borda scores of the $2^{d+1}$ items are:
\begin{align*}
    B^{\btheta}(i) & =
    \begin{cases}
        \frac{5}{8} + \frac{1}{2} \la \textbf{bit}(i), \btheta \ra , \text{ if } i < 2^d, \\
        \frac{3}{8}, \text{ if } i \ge 2^d.     
    \end{cases}
\end{align*}

Intuitively, the former half of items (those indexed from $0$ to $2^d-1$) are ``good'' items (one among them is optimal, others are nearly optimal), while the latter half of items are ``bad'' items. Under such hard instances, every time one of the two pulled items is a ``bad'' item, then a one-step regret $B^{\btheta}(i^*) - B^{\btheta}(i) \ge 1/4$ is incurred. To minimize regret, we should thus try to avoid pulling ``bad'' items. However, in order to identify the best item among all ``good'' items, comparisons between ``good'' and ``bad'' items are necessary. The reason is simply that comparisons between ``good'' items give no information about the Borda scores as the comparison probabilities are $p^{\btheta}_{i,j} = \frac{1}{2}$ for all $i,j<2^{d}$. Hence, any algorithm that can decently distinguish among the ``good'' items has to pull ``bad'' ones for a fair amount of times, and large regret is thus incurred. A similar observation is also made by \citet{Saha2021AdversarialDB}.

This specific construction emphasizes the intrinsic hardness of Borda regret minimization: to differentiate the best item from its close competitors, the algorithm must query the bad items to gain information.  

Formally, this class of hard instances leads to the following regret lower bound for both stochastic and adversarial settings:
\begin{theorem}
    \label{thm:lower}
    For any algorithm $\cA$, there exists a hard instance $\{ p^{\btheta}_{i,j}\}$ with $T > 4 d^2$, such that $\cA$ will incur expected regret at least
    $\Omega(d^{2/3} T^{2/3})$.
\end{theorem}

The construction of this hard instance for linear dueling bandits is inspired by the worst-case lower bound for the stochastic linear bandit \citep{dani2008stochastic}, which has the order $\Omega(d \sqrt{T})$, while ours is $\Omega(d^{2/3}T^{2/3})$. The difference is that for the linear or multi-armed stochastic bandit, eliminating bad arms can make further exploration less expensive. But in our case, any amount of exploration will not reduce the cost of further exploration.
This essentially means that exploration and exploitation must be separate, which is also supported by the fact that a simple explore-then-commit algorithm shown in \cref{sec:alg} can be nearly optimal.



\section{Stochastic Contextual Dueling Bandit} \label{sec:alg}
\subsection{Algorithm Description}
\begin{algorithm*}[htb]
    \caption{\textsc{BETC-GLM}}
    \label{alg:BETC-GLM}
    \begin{algorithmic}[1]
        \STATE {\bfseries Input:} time horizon $T$, number of items $K$, feature dimension $d$, feature vectors $\bphi_{i,j}$ for $i \in [K]$, $j \in [K]$, exploration rounds $\tau$, error tolerance $\epsilon$, failure probability $\delta$.
        \FOR{$t=1,2, \dots, \tau$} \label{line:pure-exploration1}
            \STATE sample $i_t \sim \mathrm{Uniform}([K])$, $j_t \sim \mathrm{Uniform}([K])$
            \STATE query pair $(i_t, j_t)$ and receive feedback $r_t$
        \ENDFOR \label{line:pure-exploration4}
        \STATE Find the G-optimal design $\pi(i,j)$ based on $\bphi_{i,j}$ for $i \in [K]$, $j \in [K]$ \label{line:optimal-design}
        \STATE Let $N(i,j) = \Big \lceil \frac{d \pi(i,j)}{\epsilon^2} \Big \rceil$ for any $(i,j) \in \text{supp}(\pi)$ , denote $N = \sum_{i=1}^{K} \sum_{j=1}^{K} N(i,j)$ \label{line:designed-exploration1}
        \FOR{$i \in [K]$, $j \in [K]$, $s \in [N(i,j)]$}\label{line:designed-exploration2}
            \STATE set $t \leftarrow t + 1$, set $(i_t, j_t) = (i, j)$\label{line:designed-exploration3}
            \STATE query pair $(i_t, j_t)$ and receive feedback $r_t$ \label{line:designed-exploration4}
        \ENDFOR \label{line:designed-exploration5}
        \STATE Calculate the empirical MLE estimator $\hat{\btheta}_{\tau+N}$ based on all $\tau + N$ samples via \eqref{eqn:MLE} \label{line:MLE}
        \STATE Estimate the Borda score for each item:
        \begin{align*}
            \hat{B}(i) = \frac{1}{K} \sum_{j=1}^{K} \mu(\bphi_{i,j}^{\top} \hat{\btheta}_{\tau+N}),
            \qquad 
            \hat{i} = \argmax_{i \in [K]} \hat{B}(i)
        \end{align*} \label{line:estimate-Borda}
        % \STATE Eliminate items with low Borda score and keep items as:
        % \begin{align*}
        %     \cI_{\text{good}} & :=
        %     \{
        %     i \in [K] | \max_{j} \hat{B}_{j \in [K] } - \hat{B}(i) \le 2 \epsilon .
        %     \}
        % \end{align*}
        \STATE Keep querying $(\hat{i}, \hat{i})$ for the rest of the time. \label{line:exploitation}
    \end{algorithmic}
\end{algorithm*}

We propose an algorithm named Borda Explore-Then-Commit for Generalized Linear Models (\textsc{BETC-GLM}), presented in Algorithm~\ref{alg:BETC-GLM}. Our algorithm is inspired by the algorithm for generalized linear models proposed by \citet{li2017provably}. 

At the high level, Algorithm~\ref{alg:BETC-GLM} can be divided into two phases: the exploration phase (Line~\ref{line:pure-exploration1}-\ref{line:designed-exploration5}) and the exploitation phase (Line~\ref{line:MLE}-\ref{line:exploitation}). The exploration phase ensures that the MLE estimator $\hat{\btheta}$ is accurate enough so that the estimated Borda score is within $\tilde{O}(\epsilon)$-range of the true Borda score (ignoring other quantities). Then the exploitation phase simply chooses the empirical Borda winner to incur small regret.

During the exploration phase, the algorithm first performs ``pure exploration'' (Line~\ref{line:pure-exploration1}-\ref{line:pure-exploration4}), which can be seen as an initialization step for the algorithm. 
The purpose of this step is to ensure the design matrix $\Vb_{\tau+N} = \sum_{t=1}^{\tau+N} \bphi_{i_t,j_t}\bphi_{i_t,j_t}^{\top}$ is positive definite. 

After that, the algorithm will perform the ``designed exploration''. Line~\ref{line:optimal-design} will find the G-optimal design, which minimizes the objective function $g(\pi) = \max_{i,j}\|\bphi_{i,j}\|^2_{\Vb(\pi)^{-1}}$, where $\Vb(\pi):= \sum_{i,j} \pi(i,j) \bphi_{i,j}\bphi_{i,j}^{\top}$. The G-optimal design $\pi^*(\cdot)$ satisfies $\|\bphi_{i,j}\|^2_{\Vb(\pi^*)^{-1}} \le d$, and can be efficiently approximated by the Frank-Wolfe algorithm (See \cref{rem:optimal-design} for a detailed discussion). Then the algorithm will follow $\pi(\cdot)$ found at Line~\ref{line:optimal-design} to determine how many samples (Line~\ref{line:designed-exploration1}) are needed. At Line~\ref{line:designed-exploration2}-\ref{line:designed-exploration5}, there are in total $N = \sum_{i=1}^{K} \sum_{j=1}^{K} N(i,j)$ samples queried, and the algorithm shall index them by $t= \tau+1, \tau+2, \dots, \tau+N$. 

At Line~\ref{line:MLE}, the algorithm collects all the $\tau+N$ samples and performs the maximum likelihood estimation (MLE).
For the generalized linear model, the MLE estimator $\hat{\btheta}_{\tau+N}$ satisfies:
\begin{align} \label{eqn:MLE}
    \sum_{t=1}^{\tau + N}  \mu(\bphi_{i_t,j_t}^{\top} \hat{\btheta}_{\tau+N})\bphi_{i_t,j_t}
    =
    \sum_{t=1}^{\tau + N}  r_{t}\bphi_{i_t,j_t}, 
    % \tag{MLE}
\end{align}
or equivalently, it can be determined by solving a strongly concave optimization problem:
\begin{align*}
    \hat{\btheta}_{\tau+N} & \in 
    \argmax_{\btheta} 
    \sum_{t=1}^{\tau + N}
    \bigg(
    r_{t}\bphi_{i_t,j_t}^{\top} \btheta - m(\bphi_{i_t,j_t}^{\top} \btheta)
    \bigg)
    ,
\end{align*}
where $\dot{m}(\cdot) = \mu(\cdot)$. For the logistic link function, $m(x) = \log(1 + e^{x})$.
As a special case of our generalized linear model, the linear model has a closed-form solution for \eqref{eqn:MLE}. For example, if $\mu(x) = \frac{1}{2} + x$, i.e. $p_{i,j} = \frac{1}{2} + \bphi_{i,j}^{\top} \btheta^*$, then \eqref{eqn:MLE} becomes:
\begin{align*}
    \hat{\btheta}_{\tau+N}
    & = 
    % \bigg(\sum_{t=1}^{\tau + N}  \bphi_{i_t,j_t} \bphi_{i_t,j_t}^{\top} \bigg)^{-1}
    \Vb_{\tau+N}^{-1}
    \sum_{t=1}^{\tau + N}  (r_{t} - 1/2) \bphi_{i_t,j_t},    
\end{align*}
where $\Vb_{\tau+N} = \sum_{t=1}^{\tau + N}  \bphi_{i_t,j_t} \bphi_{i_t,j_t}^{\top}$. 

% $\frac{\partial m(\phi \theta)}{\partial \theta} = \frac{\partial \log (1 + \exp(\phi \theta))}{\partial \theta} = \phi \frac{\exp(\phi \theta)}{1 + \exp(\phi \theta)} = \phi \mu(\theta \phi)$

After the MLE estimator is obtained, Line~\ref{line:estimate-Borda} will calculate the estimated Borda score $\hat{B}(i)$ for each item based on $\hat{\btheta}_{\tau+N}$, and pick the empirically best one. 


\subsection{A Matching Regret Upper Bound}
\cref{alg:BETC-GLM} can be configured to tightly match the worst-case lower bound. The configuration and performance are described as follows:
\begin{theorem}
\label{thm:BETC-GLM-matching}
Suppose Assumption~\ref{assumption:lambda0}-\ref{assumption:mu} hold and $T = \Omega(d^2)$. For any $\delta > 0$, if we set $\tau = C_4 \lambda_0^{-2} (d+\log(1/\delta))$ ($C_4$ is a universal constant) and $\epsilon = d^{1/6} T^{-1/3}$, then with probability at least $1-2 \delta$,
    \cref{alg:BETC-GLM} will incur regret bounded by:
    \begin{align*}
        {O}
    \Big(
    \kappa^{-1}
    d ^{2/3} T^{2/3} 
    \sqrt{\log\big( {T}/{d\delta} \big)}
    \Big).
    \end{align*}
By setting $\delta = T^{-1}$, the expected regret is bounded as $\tilde{O}(\kappa^{-1}
    d ^{2/3} T^{2/3} )$.
\end{theorem}
For linear bandit models, such as the hard-to-learn instances in \cref{sec:hardness}, $\kappa$ is a universal constant. Therefore, \cref{thm:BETC-GLM-matching} tightly matches the lower bound in \cref{thm:lower}, up to logarithmic factors.


\begin{remark}[Regret for Fewer Arms]
In typical scenarios, the number of items $K$ is not exponentially large in the dimension $d$. In this case, we can choose a different parameter set of $\tau$ and $\epsilon$ such that \cref{alg:BETC-GLM} can achieve a smaller regret bound $\tilde{O}\big(\kappa^{-1}(d \log K)^{1/3} T^{2/3}\big)$ with smaller dependence on the dimension $d$. See \cref{thm:BETC-GLM} in \cref{sec:add-result}.
\end{remark}

\begin{remark}[Regret for Infinitely Many Arms]
In most practical scenarios of dueling bandits, it is adequate to consider a finite number $K$ of items (e.g., ranking items). 
Nonetheless, BETC-GLM can be easily adapted to accommodate infinitely many arms in terms of regret. We can construct a covering over all $\bphi_{i,j}$ and perform optimal design and exploration on the covering set. The resulting regret will be the same as our upper bound, i.e., $\tilde{O}(d^{2/3}T^{2/3})$ 
 up to some error caused by the epsilon net argument.
\end{remark}

\begin{remark}[Approximate G-optimal Design] \label{rem:optimal-design}
\cref{alg:BETC-GLM} assumes an exact G-optimal design $\pi$ is obtained.
In the experiments, we use the Frank-Wolfe algorithm to solve the constraint optimization problem (See \cref{alg:fw}, \cref{sec:fw}). To find a policy $\pi$ such that $g(\pi) \le (1+\varepsilon) g(\pi^*)$, roughly $O(d/\varepsilon)$ optimization steps are needed. Such a near-optimal design will introduce a factor of $(1 + \varepsilon)^{1/3}$ into the upper bounds. 
% Each optimization step will take $O(d^2K^2)$ basic operations. 
\end{remark}

% \begin{remark}[Computational Complexity] \label{rem:computational-complexity}
% While the regret or sample complexity does not rely on the number of arms $K$, the computation complexity of any algorithm will inevitably suffer from large $K$. It is clear that $\Omega(K^2)$ operations are necessary to at least traverse over all contextual vectors $\bphi_{i,j}$. For \cref{alg:BETC-GLM}, the most computation-intensive part is the G-optimal design, where each optimization step will take $O(d^2K^2)$ basic operations. A breakdown of this cost is available in \cref{sec:fw}. To solve \eqref{eqn:MLE}, gradient descent cab be used and each step requires $O(dK^2)$ operations.
% \end{remark}




% \subsection{Regret Bound for Fewer Arms}
% \yue{Consider moving this section to Appendix}
% In typical scenarios, the number of items $K$ is not exponentially large in the dimension $d$. If this is the case, then we can choose a different parameter set of $\tau$ and $\epsilon$ such that \cref{alg:BETC-GLM} can achieve a regret bound depending on $\log K$, and reduce the dependence on $d$.
% The performance can be characterized by the following theorem:
% \begin{theorem}
%     \label{thm:BETC-GLM}
%     For any $\delta > 0$, suppose the number of total rounds $T$ satisfies, 
% \begin{align}\label{eqn:large-T}
%     T \ge
%     \frac{C_3}{\kappa^6 \lambda_0^{3/2}}
%     \max \Big\{
%     d^{5/2}, 
%     \frac{\log(K^2/\delta)}{\sqrt{d}}
%     \Big\},
% \end{align}
% where $C_3$ is some large enough universal constant. Then if we set $\tau = (d \log(K/\delta))^{1/3} T^{2/3}$ and $\epsilon = d^{1/3} T^{-1/3} \log(3K^2/\delta)^{-1/6}$,
% \cref{alg:BETC-GLM} will incur regret bounded by:
% \begin{align*}
%     O\big( \kappa^{-1} (d \log(K/\delta))^{1/3} T^{2/3}\big).
% \end{align*}
% By setting $\delta = T^{-1}$, the expected regret is bounded as $\tilde{O}\big(\kappa^{-1}
%     (d \log K)^{1/3} T^{2/3}\big)$.
% \end{theorem}



% \subsection{Overview of the Proof}
% In this section, we briefly discuss the proof of \cref{thm:BETC-GLM-matching}. The proof of \cref{thm:BETC-GLM} follows the same idea but utilizes different lemmas. For formal proof please refer to \cref{sec:proof-upper}.


% The proof of \cref{thm:BETC-GLM-matching} can be divided into three steps: 

% \textbf{Step 1:} Show that the initial $\tau$ rounds for exploration will guarantee $\lambda_{\min}(\Vb_{\tau}) \ge 1$. The algorithm is configured to first perform $\tau = \tilde{O}(d)$ rounds of purely random exploration. Due to \cref{assumption:lambda0}, we utilize the fact that plenty of randomly selected $(i_t, j_t)$ can guarantee the concentration of the matrix $\frac{1}{\tau} \Vb_{\tau} \approx \frac{1}{K^2}\sum_{i=1}^{K} \sum_{j=1}^{K} \bphi_{i,j} \bphi_{i,j}^{\top}$, which establishes the lower bound on the minimum eigenvalue. This is guaranteed by Proposition 1 in \citet{li2017provably} (See also \cref{lemma:matrix-concentration}).

% \textbf{Step 2:} Show that the MLE estimator $\hat{\btheta}_{\tau+N}$ can ensure 
% \begin{align*}
%     \bphi_{i,j}^{\top}(\hat{\btheta}_{\tau+N} - \btheta^*) = \tilde{O}(\sqrt{d} \epsilon / \kappa).
% \end{align*} 
% This is fulfilled by two partial results. First, we utilize \cref{lemma:self-normalizing} to show the MLE estimator satisfies the following with high probability:
% \begin{align*}
%     \| \hat{\btheta}_{\tau+N} - \btheta^* \|_{\Vb_{\tau+N}} = \tilde{O}(\sqrt{d} / \kappa),
% \end{align*}
% which is a rather standard result that dates back to \citet{AbbasiYadkori2011ImprovedAF}. $\kappa$ comes from removing the link function, in order to utilize the result for linear bandits.
% Then, the optimal design $\pi$ ensures that for any $\bphi_{i,j}$, we have
% $
%     \| \bphi_{i,j} \|_{\Vb(\pi)^{-1}} \le \sqrt{d},
% $
% which, along with the definition of $N(i,j) = \lceil d \pi(i,j) / \epsilon^2 \rceil$, further guarantees
% \begin{align*}
%     \| \bphi_{i,j} \|_{\Vb_{\tau+N}^{-1}} \le \epsilon.
% \end{align*}
% Utilizing the Cauchy-Schwartz inequality yields the result we want.

% \textbf{Step 3:} Balance $\epsilon$ to obtain the optimal regret. For $t \in [\tau + N]$, we loosely bound the one-step regret by $2$, since $B(i) \in [0,1]$ for any item $i$. Also, we roughly have $\tau = \tilde{O}(d)$ and $N = \tilde{O}(d/\epsilon^2)$.
% For $t>\tau+N$, the regret can be bounded by $\tilde{O}(\sqrt{d} \epsilon / \kappa)$. Ignoring $\kappa$, we have
% \begin{align*}
%     \mathrm{Regret}(T) = \tilde{O}(d / \epsilon^2 + d^{1/2} \epsilon T).
% \end{align*}
% Optimizing over $\epsilon$ yields $\mathrm{Regret}(T) = \tilde{O}(d^{2/3} T^{2/3})$.


\section{Adversarial Contextual Dueling Bandit} \label{sec:adv}
This section addresses Borda regret minimization under the adversarial setting. As we introduced in \cref{sec:probsetup}, the unknown parameter  $\btheta_t$ can vary for each round $t$, while the contextual vectors $\bphi_{i,j}$ are fixed. 

Our proposed algorithm, BEXP3, is designed for the contextual linear model. Formally, at round $t$ and given pair $(i,j)$, we have $p^t_{i,j} = \frac{1}{2} +  \la \bphi_{i,j},\btheta^*_t\ra$. 
% The main reason that EXP3-type design cannot be applied to generalized linear model is that the 

\subsection{Algorithm Description}

\begin{algorithm*}[htb]
    \caption{BEXP3}
    \label{alg:bexp3}
    \begin{algorithmic}[1]
        \STATE \textbf{Input:} time horizon $T$, number of items $K$, feature dimension $d$, feature vectors $\bphi_{i,j}$ for $i \in [K]$, $j \in [K]$, learning rate $\eta$,
        % exploration policy $\pi$,
        exploration parameter $\gamma$.
        \STATE \textbf{Initialize:} $q_1(i) = \frac{1}{K}$. \label{line:A-init}
        \FOR{$t=1,\ldots,T$}
        \STATE Sample items $i_t \sim q_t$, $j_t \sim q_t$. \label{line:A-sample}
        \STATE Query pair $(i_t, j_t)$ and receive feedback $r_t$ \label{line:A-query}
        \STATE Calculate $Q_t = \sum_{i \in [K]}\sum_{j \in [K]} q_t(i) q_t(j) \bphi_{i,j} \bphi_{i,j}^{\top}$, $\hat{\btheta}_t = Q_t^{-1} \bphi_{i_t,j_t}r_t$. \label{line:A-estimate}
        \STATE Calculate the (shifted) Borda score estimates $\hat B_t(i) = 
        \la 
        \frac{1}{K}\sum_{j \in [K]} 
        \bphi_{i,j}, \hat{\btheta}_t
        \ra$.  \label{line:A-borda}
        \STATE Update for all $i \in [K]$, set
        \begin{align*}
        \tilde{q}_{t+1}(i) & = \frac{\exp (\eta \sum_{l=1}^{t}\hat B_l(i))}{\sum_{j \in [K]}\exp (\eta \sum_{l=1}^{t}\hat B_l(j))};
            & q_{t+1}(i)  = (1-\gamma)\tilde{q}_{t+1}(i) + \frac{\gamma}{K}.
        \end{align*}  \label{line:A-update}
        \ENDFOR
    \end{algorithmic}
\end{algorithm*}

\cref{alg:bexp3} is adapted from the DEXP3 algorithm in \citet{saha2021adversarial}, which deals with the adversarial multi-armed dueling bandit. 
\cref{alg:bexp3} maintains a distribution $q_t(\cdot)$ over $[K]$, initialized as uniform distribution (Line~\ref{line:A-init}). At every round $t$, two items are chosen following $q_t$ independently. Then Line~\ref{line:A-estimate} calculates the one-sample unbiased estimate $\hat{\btheta}_t$ of the true underlying parameter $\btheta^*_t$. Line~\ref{line:A-borda} further calculates the unbiased estimate of the (shifted) Borda score. Note that the true Borda score at round $t$ satisfies $B_t(i) = \frac{1}{2} + \la 
        \frac{1}{K}\sum_{j \in [K]} 
        \bphi_{i,j}, \btheta^*_t
        \ra$.
$\hat{B}_t$ instead only estimates the second term of the Borda score. This is a choice to simplify the proof.
The cumulative estimated score $\sum_{l=1}^{t}\hat B_l(i)$ can be seen as the estimated cumulative reward of item $i$ at round $t$.  
In Line~\ref{line:A-update}, ${q}_{t+1}$ is defined by the classic exponential weight update, along with a uniform exploration policy controlled by $\gamma$.

\subsection{Upper Bounds}
\cref{alg:bexp3} can also be configured to tightly match the worst-case lower bound:
\begin{theorem}\label{Thm:adv}
    Suppose \cref{assumption:lambda0} holds.
    If we set $\eta = (\log K)^{2/3}d^{-1/3}T^{-2/3}$ and $\gamma = \sqrt{\eta d /\lambda_0} = (\log K)^{1/3}d^{1/3}T^{-1/3} \lambda_0^{-1/2}$, then the expected regret is upper-bounded by$$O \big( (d\log K)^{1/3} T^{2/3} \big).$$
\end{theorem}
Note that the lower bound construction in \cref{thm:lower} is for the linear model and has $K=O(2^d)$, thus exactly matching the upper bound.

\section{Experiments}

% \begin{wrapfigure}{r}{0.30\textwidth}
%     \centering
%     \includegraphics[width=0.23\textwidth]{figures/simulated.pdf}
%     % \caption{Generated Hard Case}
%     \caption{The regret of the proposed algorithm and the baselines.}
%     \label{fig:simulated_data}
% \end{wrapfigure}
\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/simulated.pdf}
        \caption{Generated Hard Case}
        \label{fig:simulated_data}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/events.pdf}
        \caption{EventTime}
        \label{fig:ev_data}
    \end{subfigure}
    \caption{The regret of the proposed algorithms (\textsc{BETC-GLM}, \textsc{BEXP3}) and the baseline algorithms (\textsc{UCB-Borda}, \textsc{DEXP3}, \textsc{ETC-Borda}).}
\end{wrapfigure}


This section compares the proposed algorithm \textsc{BETC-GLM} with existing ones that are capable of minimizing Borda regret. % We aim to answer: (i) How does BETC compare with its non-contextual counterparts? (ii) How does our theoretical result reflect in simulation? %To answer these questions, w
We use random responses (generated from fixed preferential matrices) to interact with all tested algorithms. Each algorithm is run for 50 times over a time horizon of $T = 10^6$. %run each algorithm 50 times with a horizon of $T = 10^6$ on fixed preferential matrices derived from datasets, and use randomly generated responses to interactive with the algorithms. 
We report both the mean and the standard deviation of the cumulative Borda regret and supply some analysis. The following list summarizes all methods we study in this section, a more complete description of the methods and parameters is available in \cref{sec:exp-list}:
\textsc{BETC-GLM(-Match)}: \cref{alg:BETC-GLM} proposed in this paper with different parameters. 
\textsc{UCB-Borda}: The UCB algorithm~\citep{Auer2002FinitetimeAO} using \textit{Borda reduction}.
\textsc{DEXP3}: Dueling-Exp3 developed by \citet{Saha2021AdversarialDB}.
\textsc{ETC-Borda}: A simple explore-then-commit algorithm that does not take any contextual information into account.
\textsc{BEXP3}: The proposed method for adversarial Borda bandits displayed in \cref{alg:bexp3}.

\paragraph{Generated Hard Case}
% To verify the validity of the algorithm we conducted experiments on simulated data first. 
% \tao{We used $p_{i,j}^{\btheta}$ in the appendix, but in main text we only use $p_{i,j}$.} \yue{We should use $p_{i,j}^{\btheta}$ in main text too}

% \tao{I think there is a way to select $\alpha$ in UCB algorithm according to T? This is for \cref{alg:UCB-Borda}}

% \tao{it seems that etc is not exploring enough}

We first test the algorithms on the hard instances constructed in \cref{sec:hardness}. %To construct this dataset, we generate a set of contextual vectors $\bphi$ following~\cref{rem:hard-d+1} in the proof of~\cref{thm:lower} in~\cref{sec:proof-lower}, which is considered to be a hard case. We set $\mu(x) = \frac12 + x$ and $r_t \in \{-1, 1\}, t \in [T]$. 
We generate $\btheta^*$ randomly from $\{-\Delta,+\Delta\}^d$ with $\Delta = \frac1{4d}$ so that the comparison probabilities $p^{\btheta^*}_{i,j}\in [0, 1]$ for all $i,j \in [K]$. We pick the dimension $d = 6$ and the number of arms is therefore $K = 2^{d+1} = 128$. Note the dual usage of $d$ in our construction and the model setup in \cref{sec:probsetup}. We refer readers to \cref{rem:hard-d+1} in \cref{sec:proof-lower} for more details.

% The complete simplified procedure named \textsc{BETC-Linear} is displayed in~\cref{alg:BETC-Linear} in~\cref{sec:BETC-Linear}.

As depicted in \cref{fig:simulated_data}, the proposed algorithms (\textsc{BETC-GLM}, \textsc{BEXP3}) outperform the baseline algorithms in terms of cumulative regret when reaching the end of time horizon $T$. For \textsc{UCB-Borda}, since it is not tailored for the dueling regret definition, it suffers from a linear regret as its second arm is always sampled uniformly at random, leading to a constant regret per round.
% \tao{I can also add a graph showing even if it's single arm regret, it will be worse than us for extended runs since it has to continuously exploring.}
\textsc{DEXP3} and \textsc{ETC-Borda} are two algorithms designed for $K$-armed dueling bandits. Both are unable to utilize contextual information and thus demand more exploration. As expected, their regrets are higher than \textsc{BETC-GLM} or \textsc{BEXP3}. 
% In addition, although \textsc{DEXP3} and \textsc{ETC-Borda} share the same theoretical upper bound $\tilde{O}(K^{1/3} T^{2/3})$, it is very likely \textsc{DEXP3} suffers from some overhead because it caters to the non-stationary environment. 
% \textsc{ETC-Borda} takes a similar number of exploration rounds compared to \textsc{BETC-GLM}. However, without contextual information, this uniform exploration strategy results in a less accurate estimate of the Borda score and less efficient sample usage overall. 
% The high variance of its cumulative regret suggests that it suffers from a higher chance of committing to the wrong item after exploration.

\paragraph{Real-world Dataset}
To showcase the performance of the algorithms in a real-world setting, we use the EventTime dataset \citep{Zhang2016CrowdsourcedTA}. In this dataset, $K = 100$ historical events are compared in a pairwise fashion by crowd-sourced workers. 
We first calculate the empirical preference probabilities $\tilde p_{i,j}$ from the collected responses, and construct a generalized linear model based on the empirical preference probabilities. The algorithms are tested under this generalized linear model.
Due to space limitations, more details are deferred to \cref{sec:exp-realworld}. 

As depicted in \cref{fig:ev_data}, the proposed algorithm \textsc{BETC-GLM} outperforms the baseline algorithms in terms of cumulative regret when reaching the end of time horizon $T$. The other proposed algorithm \textsc{BEXP3} performs equally well even when misspecified (the algorithm is designed for the linear setting, while the comparison probability follows a logistic model). 
% #TODO: ucb linear bandit



% We can find a dataset contains movies / books / news / etc, run topic modeling on it \cite{Zhu2012MedLDAMM}. In this way, $x_i$ is obtained. $\phi_(i, j)$ can be obtained in by $x_i - x_j$. Since we don't compare with other linear contextual bandit algorithms. It guarantees that we are still performing better. 

% Steps: 1. find preferential data on movies books. 2. get side information 3. run lda.

% Replace 3 with a small NN, the input is concatenated side information, \textit{in order according to item number}. And when we run the algorithm, it is also in this order. So we eliminate the problem of having $\phi_{ij} \neq  -\phi_{ji}$. Since we are always using $\phi_{ij}, i < j$. The target is an output of logistic function indicating whether it is preferred or not. When doing the actual bandit problem, we using the last layer before the logistic function as $\phi$ for each pair.
% If the optional assumption on $\bx_i$ is given, then a new algorithm other than explore then commit has to be designed to exploit the SST structure. \cite{Yue2011BeatTM} \cite{Yue2012TheKD} can be a good starting point. It is unclear how to deal with the bounds in this case. Read-world empirical study can be done.

% If the phi has some structure, then the upper/lower bound can have the sqrt(T) regret.




\section{Conclusion and Future Work}
In this paper, we introduced Borda regret into the generalized linear dueling bandits setting, along with an explore-then-commit type algorithm \textsc{BETC-GLM} and an EXP3 type algorithm \textsc{BEXP3}. The algorithms can achieve a nearly optimal regret upper bound, which we corroborate with a matching lower bound.
The theoretical performance of the algorithms is verified empirically. It demonstrates superior performance compared to other baseline methods.


%\newpage
\appendix

% \section{Proof of Theorem~\ref{thm:BETC}}
% We first introduce the lemma regarding the G-optimal design: given the action set $\cX \subseteq \RR^d$ that is compact and $\mathrm{span}(\cX) = \RR^d$. A fixed design $\pi(\cdot): \cX \rightarrow [0,1]$ satisfies $\sum_{\xb \in \cX} \pi(\xb) = 1$. Define $\Vb(\pi) := \sum_{\xb \in \cX} \pi(\xb) \xb \xb^{\top}$ and $g(\pi) := \max_{\xb \in \cX} \|\xb\|^2_{V(\pi)^{-1}}$.
% \begin{lemma}[Kiefer–Wolfowitz]\label{lemma:g-optimal}
% There exists an optimal design $\pi^*(\cdot)$ such that $|\mathrm{supp}(\pi)| \le d(d+1)/2$, and satisfies:
% \begin{enumerate}
%     \item $g(\pi^*) = d$.
%     \item $\pi^*$ is the minimizer of $g(\cdot)$.
% \end{enumerate}
% \end{lemma}
% The following lemma characterizes the behavior of the least square estimator under some fixed design of $N$ queries $\bphi_1, \bphi_2, \dots, \bphi_{N}$. We define $\hat{\btheta}_N = \Vb_N^{-1} \sum_{t=1}^{N} \bphi_t r_t$, where $\Vb_N = \sum_{t=1}^{N} \bphi_t \bphi_t^{\top}$ is invertible, $\EE[r_t | \bphi_t] = \la \btheta, \bphi_t \ra$ and $r_t$ is $1$-sub-Gaussian.
% \begin{lemma}\label{lemma:concentration}
%     For any fixed direction $\xb \in \RR^d$, with probability $1-\delta$,
%     \begin{align*}
%         \la \hat{\btheta}_N - \btheta^*, \xb \ra 
%         \le 
%         \sqrt{2 \| \xb \|^2_{\Vb_N^{-1}} \log(\delta^{-1})}.
%     \end{align*}
% \end{lemma}
% \begin{proof}[Proof of Lemma~\ref{lemma:concentration}]
    
% \end{proof}

% \begin{proof}[Proof of Theorem~\ref{thm:BETC}]
% Given the optimal design $\pi(i,j)$, the algorithm queries each arm $\bphi \in \mathrm{supp}(\pi)$ for exactly $N_{\bphi} = \lceil 2d\pi(\bphi) \log(K^2 /\delta) / \epsilon^2 \rceil$ times. Therefore, the covariance matrix $\Vb$ satisfies
% \begin{align*}
%     \Vb_{\tau+N}  & := \sum_{\bphi \in \mathrm{supp}(\pi)} N_{\bphi} \bphi \bphi^{\top} 
%     \\
%     & \succeq \sum_{\bphi \in \mathrm{supp}(\pi)} \frac{2d\pi(\bphi) \log(K^2 /\delta)}{\epsilon^2}  \bphi \bphi^{\top}
%     \\
%     & = 
%     \frac{2d \log(K^2 /\delta)}{\epsilon^2} \Vb(\pi),
% \end{align*}
% where $\Vb(\pi) := \sum_{\bphi \in \mathrm{supp}(\pi)}\pi(\bphi) \bphi \bphi^{\top}$.
% Therefore, by Lemma~\ref{lemma:concentration}, we have for any fixed $\bphi_{i,j}$, with probability $1-\delta/K^2$, that
% \begin{align*}
%     \la \hat{\btheta} - \btheta^*, \bphi_{i,j} \ra 
%     & \le 
%     \sqrt{2 \| \bphi_{i,j} \|^2_{\Vb^{-1}_{\tau+N}  } \log(K^2 / \delta)}
%     \\
%     & \le 
%     \sqrt{2 \cdot \frac{\epsilon^2}{2d \log(K^2/\delta)} \cdot\| \bphi_{i,j} \|^2_{\Vb(\pi)^{-1}} \log(K^2 / \delta)}
%     \\
%     & =
%     \epsilon \sqrt{\frac{\| \bphi_{i,j} \|^2_{\Vb(\pi)^{-1}}}{d }  }
%     \\
%     & \le \epsilon,
% \end{align*}
% where the first inequality is from Lemma~\ref{lemma:concentration} the second inequality holds because $\Vb_{\tau+N}  \succeq \frac{2d \log(K^2 /\delta)}{\epsilon^2} \Vb(\pi)$; the last inequality holds because $\pi$ is an optimal design and by Lemma~\ref{lemma:g-optimal}, $\| \bphi_{i,j} \|^2_{\Vb(\pi)^{-1}} \le d$.

% By the definition of the empirical Borda score, we know that 
% \begin{align*}
%     | \hat{B}(i) - B(i) |
%     & = 
%     \bigg| \frac{1}{K} \sum_{j=1}^{K} \la \bphi_{i,j}, \hat{\btheta} - \btheta^* \ra 
%     \bigg|
%     \\
%     & \le \epsilon.
% \end{align*}

% Meanwhile, denote $N = \sum_{i,j} N(i,j)$, we have that 
% \begin{align*}
%     N & \le d(d+1)/2 + \frac{2d \log(K^2/\delta) }{\epsilon^2}.
% \end{align*}

% Therefore, with probability $1-\delta$, the regret is bounded by:
% \begin{align*}
%     \mathrm{Regret}(T)
%     & \le 
%     d(d+1)/2 + \frac{2d \log(K^2/\delta) }{\epsilon^2}
%     +
%     \epsilon T
%     \\
%     & = 
%     O(d^2 + (d \log(K^2/\delta))^{1/3} T^{2/3} ),
% \end{align*}
% where we set $\epsilon = (d \log(K/\delta))^{1/3} T^{-1/3}$.

% By setting $\delta = T^{-1}$ and the assumption that $T = \Omega(\sqrt{d^3 \log(K)})$, we can show that the expected regret of Algorithm~\ref{alg:BETC} is bounded by
% \begin{align*}
%      O\big((d \log(KT)^{1/3} T^{2/3})
%      \big).
% \end{align*}
% Note that if there are exponentially many contextual vectors ($K \approx 2^d$), the upper bound becomes $\tilde{O}(d^{2/3}T^{2/3})$.
% \end{proof}



% \newpage 

\section{Additional Results and Discussion} 
\subsection{Existing Results for Structured Contexts}\label{subsec:structure}
A structural assumption made by some previous works \citep{saha2021optimal} is that $\bphi_{i,j} = \xb_i - \xb_j$, where $\xb_i$ can be seen as some feature vectors tied to the item. In this work, we do not consider minimizing the Borda regret under the structural assumption. 

The immediate reason is that, when $p_{i,j} = \mu(\xb_{i}^{\top}\btheta^* - \xb_{j}^{\top}\btheta^*)$, with $\mu(\cdot)$ being the logistic function, the probability model $p_{i,j}$ effectively becomes (a linear version of) the well-known Bradley-Terry model. Namely, each item is tied to a value $v_i = \xb_i^{\top} \btheta^*$, and the comparison probability follows $p_{i,j} = \frac{e^{v_i}}{e^{v_i} + e^{v_j}}$.
More importantly, this kind of model satisfies both the strong stochastic transitivity (SST) and the stochastic triangle inequality (STI), which are unlikely to satisfy in reality.

Furthermore, when stochastic transitivity holds, there is a true ranking among the items, determined by $\xb_i^{\top} \btheta^*$. A true ranking renders concepts like the Borda winner or Copeland winner redundant because the rank-one item will always be the winner in every sense. When $\bphi_{i,j} = \xb_i - \xb_j$, \citet{saha2021optimal} proposed algorithms that can achieve nearly optimal regret $\tilde{O}(d\sqrt{T})$, with regret being defined as 
\begin{align} \label{eqn:saha-regret}
    \mathrm{Regret}(T) = \sum_{t=1}^{T} 2\la \xb_{i^*}, \btheta^* \ra - \la \xb_{i_t}, \btheta^* \ra - \la \xb_{j_t}, \btheta^* \ra,
\end{align}
where $i^* = \argmax_{i} \la \xb_{i}, \btheta^* \ra$, which also happens to be the Borda winner.
Meanwhile, by \cref{assumption:mu},
\begin{align*}
    B(i^*) - B(j) 
    & =
    \frac{1}{K}
    \sum_{k=1}^{K}
    \big[\mu(\la \xb_{i^*} - \xb_{k}, \btheta^* \ra) - \mu(\la \xb_{j} - \xb_{k}, \btheta^* \ra) \big]
    \le L_{\mu} \cdot \la \xb_{i^*} - \xb_{j} , \btheta^* \ra,
\end{align*}
where $L_{\mu}$ is the upper bound on the derivative of $\mu(\cdot)$. For logistic function $L_{\mu} = 1/4$. The Borda regret \eqref{eqn:borda-regret} is thus at most a constant multiple of \eqref{eqn:saha-regret}. This shows Borda regret minimization can be sufficiently solved by \citet{saha2021optimal} when structured contexts are present. 
We consider the most general case where the only restriction is the implicit assumption that $\bphi_{i,j} = - \bphi_{j,i}$.
\subsection{Regret Bound for Fewer Arms}\label{sec:add-result}
In typical scenarios, the number of items $K$ is not exponentially large in the dimension $d$. If this is the case, then we can choose a different parameter set of $\tau$ and $\epsilon$ such that \cref{alg:BETC-GLM} can achieve a regret bound depending on $\log K$, and reduce the dependence on $d$.
The performance can be characterized by the following theorem:
\begin{theorem}
    \label{thm:BETC-GLM}
    For any $\delta > 0$, suppose the number of total rounds $T$ satisfies, 
\begin{align}\label{eqn:large-T}
    T \ge
    \frac{C_3}{\kappa^6 \lambda_0^{3/2}}
    \max \Big\{
    d^{5/2}, 
    \frac{\log(K^2/\delta)}{\sqrt{d}}
    \Big\},
\end{align}
where $C_3$ is some large enough universal constant. Then if we set $\tau = (d \log(K/\delta))^{1/3} T^{2/3}$ and $\epsilon = d^{1/3} T^{-1/3} \log(3K^2/\delta)^{-1/6}$,
\cref{alg:BETC-GLM} will incur regret bounded by:
\begin{align*}
    O\big( \kappa^{-1} (d \log(K/\delta))^{1/3} T^{2/3}\big).
\end{align*}
By setting $\delta = T^{-1}$, the expected regret is bounded as $\tilde{O}\big(\kappa^{-1}
    (d \log K)^{1/3} T^{2/3}\big)$.
\end{theorem}

\section{Omitted Proof in \cref{sec:hardness}}\label{sec:proof-lower}
The proof relies on a class of hard-to-learn instances. We first present the construction again for completeness.

    For any $d > 0$, we construct a hard instance with $2^{d+1}$ items (indexed from 0 to $2^{d+1}-1$).
    We construct the hard instance $p^{\btheta}_{i,j}$ for any $\btheta \in \{- \Delta, + \Delta \}^{d} $ as:
    \begin{align}
        p^{\btheta}_{i,j} & =
        \begin{cases}
            \frac{1}{2}, \text{ if } i < 2^d, j < 2^d     \\
            \frac{1}{2}, \text{ if } i \ge 2^d, j \ge 2^d \\
            \frac{3}{4}, \text{ if } i < 2^d, j \ge 2^d   \\
            \frac{1}{4}, \text{ if } i \ge 2^d, j < 2^d   \\
        \end{cases}
        + \la \bphi_{i,j}, \btheta \ra, \label{eqn:hard-p}
        % \begin{cases}
        %     0, \text{ if } i \le 2^d, j \le 2^d \\
        %     0, \text{ if } i > 2^d, j > 2^d \\
        %     \frac{1}{2} c, \text{ if } i \le 2^d, j > 2^d \\
        %     -\frac{1}{2} \la \bphi_{i,j}, \btheta \ra, \text{ if } i > 2^d, j \le 2^d, \\
        % \end{cases}
    \end{align}
    where the feature vectors $\bphi_{i,j}$ and the parameter $\btheta$ are of dimension $d$, and have the following forms:
    \begin{align*}
        \bphi_{i,j} & =
        \begin{cases}
            \mathbf{0}, \text{ if } i < 2^d, j < 2^d          \\
            \mathbf{0}, \text{ if } i \ge 2^d, j \ge 2^d      \\
            \textbf{bit}(i), \text{ if } i < 2^d, j \ge 2^d   \\
            -\textbf{bit}(j), \text{ if } i \ge 2^d, j < 2^d, \\
        \end{cases}
    \end{align*}
    where $\textbf{bit}(\cdot)$ is the (shifted) bit representation of non-negative integers, i.e., suppose $x = b_0 \times 2^0 + b_1 \times 2^1 + \cdots + b_{d-1} \times 2^{d-1}$, then $\textbf{bit}(x) = 2 \bb - 1$. Note that $\textbf{bit}(\cdot) \in \{-1, +1 \}^{d}$, and $\bphi_{i,j} = - \bphi_{j,i}$.

    \begin{remark}[$d+1$-dimensional instance]\label{rem:hard-d+1}
        The hard instance described above does not strictly satisfy the assumption that $p^{\btheta}_{i,j} = \la \btheta, \bphi_{i,j} \ra$, but can be easily fixed by appending an additional dimension to address the bias term defined in \eqref{eqn:hard-p}. More specifically, we can set $F(x) = \frac{1}{2} + x$ and $p^{\btheta}_{i,j} = F(\la \Tilde{\bphi}_{i,j}, \Tilde{\btheta} \ra)$, where $\Tilde{\btheta} \in \{- \Delta, + \Delta \}^{d} \times \{ \frac{1}{4}\} \subset \RR^{d+1}$ and $\Tilde{\bphi}_{i,j} = (\bphi_{i,j}, c_{i,j})$, with $c_{i,j} = \begin{cases}
                0, \text{ if } i < 2^d, j < 2^d     \\
                0, \text{ if } i \ge 2^d, j \ge 2^d \\
                1, \text{ if } i < 2^d, j \ge 2^d   \\
                -1, \text{ if } i \ge 2^d, j < 2^d. \\
            \end{cases}$
        To ensure $\| \tilde{\bphi}_{i,j} \|_2 \le 1$, we can further set $\tilde{\bphi}_{i,j} \leftarrow (d+1)^{-1/2} \tilde{\bphi}_{i,j}$ and $\tilde{\btheta} \leftarrow (d+1)^{1/2} \Tilde{\btheta}$.
    \end{remark}


    We rewrite \eqref{eqn:hard-p} as:
    \begin{align}
        p^{\btheta}_{i,j} & =
        \begin{cases}
            \frac{1}{2}, \text{ if } i < 2^d, j < 2^d     \\
            \frac{1}{2}, \text{ if } i \ge 2^d, j \ge 2^d \\
            \frac{3}{4}, \text{ if } i < 2^d, j \ge 2^d   \\
            \frac{1}{4}, \text{ if } i \ge 2^d, j < 2^d   \\
        \end{cases}
        +
        \begin{cases}
            0, \text{ if } i < 2^d, j < 2^d                                     \\
            0, \text{ if } i \ge 2^d, j \ge 2^d                                 \\
            \la \textbf{bit}(i), \btheta \ra, \text{ if } i < 2^d, j \ge 2^d    \\
            - \la \textbf{bit}(j), \btheta \ra, \text{ if } i \ge 2^d, j < 2^d, \\
        \end{cases}
    \end{align}
    and the Borda scores are:
    \begin{align*}
        B^{\btheta}(i) & =
        \begin{cases}
            \frac{5}{8} + \frac{1}{2} \la \textbf{bit}(i), \btheta \ra , \text{ if } i < 2^d, \\
            \frac{3}{8}, \text{ if } i \ge 2^d.                                               \\
        \end{cases}
    \end{align*}
    Intuitively, the former half arms indexed from $0$ to $2^d-1$ are ``good'' arms (one among them is optimal), while the latter half arms are ``bad'' arms. It is clear that choosing a ``bad'' arm $i$ will incur regret $B(i^*) - B(i) \ge 1/4$.


    Now we are ready to present the proof.

\begin{proof}[Proof of \cref{thm:lower}]
    First, we present the following lemma:
    \begin{lemma} \label{lemma:reduction}
        Under the hard instance we constructed above, for any algorithm $\cA$ that ever makes queries $i_t \ge 2^d$, there exists another algorithm $\cA'$ that only makes queries $i_t < 2^d$ for every $t>0$ and always achieves no larger regret than $\cA$.
    \end{lemma}
    \begin{proof}[Proof of Lemma \ref{lemma:reduction}]
        The proof is done by reduction. For any algorithm $\cA$, we wrap $\cA$ with such a agent $\cA'$:
        \begin{enumerate}
            \item If $\cA$ queries $(i_t,j_t)$ with $i_t < 2^d$, the agent $\cA'$ will pass the same query $(i_t,j_t)$ to the environment and send the feedback $r_t$ to $\cA$;
            \item If $\cA$ queries $(i_t,j_t)$ with $i_t \ge 2^d, j_t < 2^d$, the agent $\cA'$ will pass the query $(j_t,i_t)$ to the environment and send the feedback $1 -r_t$ to $\cA$;
            \item If $\cA$ queries $(i_t,j_t)$ with $i_t \ge 2^d, j_t \ge 2^d$, the agent $\cA'$ will uniform-randomly choose $i'_t$ from $0$ to $2^d-1$, pass the query $(i'_t, i'_t)$ to the environment and send the feedback $r_t$ to $\cA$.
        \end{enumerate}
        For each of the cases defined above, the probabilistic model of bandit feedback for $\cA$ is the same as if $\cA$ is directly interacting with the original environment.
        For Case 1, the claim is trivial. For Case 2, the claim holds because of the symmetry of our model, that is $p^{\btheta}_{i,j} = 1 - p^{\btheta}_{j,i}$. For Case 3, both will return $r_t$ following $\mathrm{Bernoulli(1/2)}$.
        Therefore, the expected regret of $\cA$ in this environment wrapped by $\cA'$ is equal to the regret of $\cA$ in the original environment.

        Meanwhile, we will show $\cA'$ will incur no larger regret than $\cA$. For the first two cases, $\cA'$ will incur the same one-step regret as $\cA$. For the third case, we know that $B^{\btheta}(i_t) = B^{\btheta}(j_t) = \frac{3}{8}$, while $\EE[B^{\btheta}(i'_t)] = \frac{5}{8} + \frac{1}{2} \la \EE_{i'_t}[ \textbf{bit}(i'_t)], \btheta \ra = \frac{5}{8} + \frac{1}{2} \la \mathbf{0}, \btheta \ra = \frac{5}{8}$, meaning that the one-step regret is smaller.
    \end{proof}


    Lemma~\ref{lemma:reduction} ensures it is safe to assume $i_t < 2^d$.
    For any $\btheta$ and $k \in [d]$, define
    \begin{align*}
        \PP_{\btheta, k}
         & :=
        \PP_{\btheta}
        \bigg(
        \sum_{t=1}^{T}
        \ind
        \{
        \textbf{bit}^{[k]}(i_t)
        \ne
        \text{sign}(\btheta^{[k]})
        \}
        \ge
        \frac{T}{2}
        \bigg),
    \end{align*}
    where the superscript $^{[k]}$ over a vector denotes taking the $k$-th entry of the vector.
    Meanwhile, we define $\btheta^{\backslash k}$ to satisfy $(\btheta^{\backslash k})^{[k]} = - \btheta^{[k]}$ and be the same as $\btheta$ at all other entries.
    We have
    \begin{align*}
        \PP_{\btheta^{\backslash k}, k}
         & :=
        \PP_{\btheta^{\backslash k}}
        \bigg(
        \sum_{t=1}^{T}
        \ind
        \big\{
        \textbf{bit}^{[k]}(i_t)
        \ne
        \text{sign} \big( (\btheta^{\backslash k})^{[k]} \big)
        \big\}
        \ge
        \frac{T}{2}
        \bigg)
        \\
         & =
        \PP_{\btheta^{\backslash k}}
        \bigg(
        \sum_{t=1}^{T}
        \ind
        \{
        \textbf{bit}^{[k]}(i_t)
        =
        \text{sign}(\btheta^{[k]})
        \}
        \ge
        \frac{T}{2}
        \bigg)
        \\
         & =
        \PP_{\btheta^{\backslash k}}
        \bigg(
        \sum_{t=1}^{T}
        \ind
        \{
        \textbf{bit}^{[k]}(i_t)
        \ne
        \text{sign}(\btheta^{[k]})
        \}
        <
        \frac{T}{2}
        \bigg).
    \end{align*}
    Denote $\PP_{\btheta, \cA}(i_1,j_1,r_1,i_2,j_2,r_2,\dots)$ as the canonical probability distribution of algorithm $\cA$ under the model $\PP_{\btheta}$. By the Bretagnolle–Huber inequality and the decomposition of the relative entropy, we have
    \begin{align*}
        \PP_{\btheta, k} + \PP_{\btheta^{\backslash k}, k}
         & \ge
        \frac{1}{2}
        \exp
        \big(
        - \mathrm{KL}(\PP_{\btheta, \cA} \| \PP_{\btheta^{\backslash k}, \cA})
        \big)
        \\
         & \ge
        \frac{1}{2}
        \exp
        \bigg(
        - \EE_{\btheta}
        \bigg[
            \sum_{t=1}^{T} \mathrm{KL}
            \Big(
            p^{\btheta}_{i,j}
            \Big \|
            p^{\btheta^{\backslash k}}_{i,j}
            \Big)
            \bigg]
        \bigg)
        \\
         & \ge
        \frac{1}{2}
        \exp
        \bigg(
        - \EE_{\btheta}
        \bigg[
            \sum_{t=1}^{T}
            10
            \la \bphi_{i_t,j_t},
            \btheta - \btheta^{\backslash k} \ra^2
            \bigg]
        \bigg)
        \\
         & =
        \frac{1}{2}
        \exp
        \bigg(
        - \EE_{\btheta}
        \bigg[
            40 \Delta^2
            \sum_{t=1}^{T}
            \ind \{ i_t < 2^d \land j_t \ge 2^d \}
            \bigg]
        \bigg),
    \end{align*}
    where the first inequality comes from the Bretagnolle–Huber inequality; the second inequality is the decomposition of the relative entropy; the third inequality holds because the Bernoulli KL divergence $KL(p \| p+x)$ is $10$-strongly convex in $x$ for any fixed $p \in [1/8, 7/8]$, and indeed $p_{i,j}^{\btheta} \in [1/8, 7/8]$ as long as $d \Delta \le 1/8$; the last equation holds because $\bphi_{i_t,j_t}$ has non-zero entries only when $(i_t,j_t)$ belongs to that specific regions.

    From now on, we denote $N(T) := \sum_{t=1}^{T}
        \ind \{ i_t < 2^d \land j_t \ge 2^d \}$.
    Further averaging over all $\btheta \in \{-\Delta, +\Delta\}^d$, we have
    \begin{align*}
        \frac{1}{2^d} \sum_{\btheta \in \{-\Delta, +\Delta\}^d} \PP_{\btheta, k}
         & \ge \frac{1}{4}
        \frac{1}{2^d} \sum_{\btheta \in \{-\Delta, +\Delta\}^d}
        \exp
        \big(
        -
        40 \Delta^2
        \EE_{\btheta}
            [
                N(T)
            ]
        \big)
        \\
         & \ge
        \frac{1}{4}
        \exp
        \bigg(
        -
        40 \Delta^2
        \frac{1}{2^d} \sum_{\btheta \in \{-\Delta, +\Delta\}^d}
        \EE_{\btheta}
            [
                N(T)
            ]
        \bigg),
    \end{align*}
    where the first inequality is from averaging over all $\btheta$; the second inequality is from Jensen's inequality.

    Utilizing the inequality above, we establish that
    \begin{align}
        \frac{1}{2^d} \sum_{\btheta \in \{-\Delta, +\Delta\}^d} \mathrm{Regret}(T; \btheta, \cA)
         & \ge
        \frac{1}{2^d} \sum_{\btheta \in \{-\Delta, +\Delta\}^d}
        \EE_{\btheta} \bigg[
            \sum_{t=1}^{T} {B^{\btheta}(i^*) - B^{\btheta}(i_t)}
            \bigg]
        \notag \\
         & =
        \frac{1}{2^d} \sum_{\btheta \in \{-\Delta, +\Delta\}^d}
        \EE_{\btheta} \bigg[
            \sum_{t=1}^{T} {\la \btheta, \textbf{sign}(\btheta) - \textbf{bit}(i_t) \ra }
            \bigg]
        \notag \\
         & =
        \frac{1}{2^d} \sum_{\btheta \in \{-\Delta, +\Delta\}^d}
        \EE_{\btheta} \bigg[
        \sum_{t=1}^{T}
        \sum_{k=1}^{d}
        2\Delta
        \ind
        \{
        \textbf{bit}^{[k]}(i_t)
        \ne
        \text{sign}(\btheta^{[k]})
        \}
        \bigg]
        \notag \\
         & =
        \frac{2\Delta}{2^d} \sum_{\btheta \in \{-\Delta, +\Delta\}^d}
        \sum_{k=1}^{d}
        \EE_{\btheta} \bigg[
        \sum_{t=1}^{T}
        \ind \{
        \textbf{bit}^{[k]}(i_t)
        \ne
        \text{sign}(\btheta^{[k]})
        \}
        \bigg]
        \notag \\
         & \ge
        \frac{2\Delta}{2^d} \sum_{\btheta \in \{-\Delta, +\Delta\}^d}
        \sum_{k=1}^{d}
        \PP_{\btheta, k} \cdot \frac{T}{2}
        \notag \\
         & \ge
        \frac{\Delta dT}{4}
        \exp
        \bigg(
        -
        40 \Delta^2
        \frac{1}{2^d} \sum_{\btheta \in \{-\Delta, +\Delta\}^d}
        \EE_{\btheta}
            [
                N(T)
            ]
        \bigg), \label{eqn:lower-bound1}
    \end{align}
    where the first inequality comes from the Borda regret; the second inequality comes from the inequality $\EE[X] \ge a \PP(X \ge a)$ for any non-negative random variable; the last inequality is from rearranging terms and invoking the results above.


    Meanwhile, we have (remember $N(T) := \sum_{t=1}^{T}
        \ind \{ i_t < 2^d \land j_t \ge 2^d \}$)
    \begin{align}
        \frac{1}{2^d} \sum_{\btheta \in \{-\Delta, +\Delta\}^d} \mathrm{Regret}(T; \btheta, \cA)
         & \ge
        \frac{1}{2^d} \sum_{\btheta \in \{-\Delta, +\Delta\}^d} \EE_{\btheta} \bigg[\frac{1}{4} \sum_{t=1}^{T} \ind \{ i_t < 2^d \land j_t \ge 2^d \} \bigg]
        \notag \\
         & =
        \frac{1}{4} \frac{1}{2^d} \sum_{\btheta \in \{-\Delta, +\Delta\}^d} \EE_{\btheta}
        [
            N(T)
        ],    \label{eqn:lower-bound2}
    \end{align}
    where the first inequality comes from that any items $i \ge 2^d$ will incur at least $1/4$ regret.

    Combining \eqref{eqn:lower-bound1} and \eqref{eqn:lower-bound2} together and denoting that $X = \frac{1}{2^d} \sum_{\btheta \in \{-\Delta, +\Delta\}^d} \EE_{\btheta}
        [
            N(T)
        ]$, we have that for any algorithm $\cA$, there exists some $\btheta$, such that (set $\Delta = \frac{d^{-1/3} T^{-1/3}}{\sqrt{40}}$)
    \begin{align*}
        \mathrm{Regret}(T; \btheta, \cA)
         & \ge
        \max \bigg\{
        \frac{\Delta dT}{4}
        \exp
        (
        -
        40 \Delta^2
        X),
        \frac{X}{4}
        \bigg \}
        \\
         & =
        \max \bigg\{
        \frac{d^{2/3} T^{2/3}}{4 \sqrt{40}}
        \exp
        (
        -
        d^{-2/3} T^{-2/3}
        X),
        \frac{X}{4}
        \bigg\}
        \\
         & \ge
        \frac{d^{2/3} T^{2/3}}{4 \sqrt{40}}
        \max \bigg\{
        \exp
        (
        -
        d^{-2/3} T^{-2/3}
        X),
        d^{-2/3} T^{-2/3}
        X
        \bigg\}
        \\
         & \ge
        \frac{d^{2/3} T^{2/3}}{8 \sqrt{40}},
    \end{align*}
    where the first inequality is the combination of \eqref{eqn:lower-bound1} and \eqref{eqn:lower-bound2}; the second inequality is a rearrangement and loosely lower bounds the constant; the last is due to $\max \{e^{-y}, y \} > 1/2$ for any $y$.
\end{proof}

\section{Omitted Proof in \cref{sec:alg}} \label{sec:proof-upper}

We first introduce the lemma about the theoretical guarantee of G-optimal design: given an action set $\cX \subseteq \RR^d$ that is compact and $\mathrm{span}(\cX) = \RR^d$. A fixed design $\pi(\cdot): \cX \rightarrow [0,1]$ satisfies $\sum_{\xb \in \cX} \pi(\xb) = 1$. Define $\Vb(\pi) := \sum_{\xb \in \cX} \pi(\xb) \xb \xb^{\top}$ and $g(\pi) := \max_{\xb \in \cX} \|\xb\|^2_{\Vb(\pi)^{-1}}$.

\begin{lemma}[The Kiefer–Wolfowitz Theorem, Section 21.1, \citet{Lattimore2020BanditA}]\label{lemma:g-optimal}
There exists an optimal design $\pi^*(\cdot)$ such that $|\mathrm{supp}(\pi)| \le d(d+1)/2$, and satisfies:
\begin{enumerate}
    \item $g(\pi^*) = d$.
    \item $\pi^*$ is the minimizer of $g(\cdot)$.
\end{enumerate}
\end{lemma}


The following lemma is also useful to show that under mild conditions, the minimum eigenvalue of the design matrix can be lower-bounded:
\begin{lemma}[Proposition 1, \citealt{li2017provably}] \label{lemma:matrix-concentration}
    Define $\Vb_{\tau} = \sum_{t=1}^{\tau} \bphi_{i_t,j_t}\bphi_{i_t,j_t}^{\top}$, where each $(i_t,j_t)$ is drawn i.i.d. from some distribution $\nu$. Suppose $\lambda_{\min} \big(\EE_{(i,j) \sim \nu}[\bphi_{i,j}^{\top}\bphi_{i,j}] \big) \ge \lambda_0$, and
    \begin{align*}
        \tau & \ge \bigg( 
        \frac{C_1 \sqrt{d} + C_2 \sqrt{\log(1/\delta)}}{\lambda_0}
        \bigg)^2
        +
        \frac{2B}{\lambda_0},
    \end{align*}    
    where $C_1$ and $C_2$ are some universal constants. Then with probability at least $1-\delta$,
    \begin{align*}
        \lambda_{\min}(\Vb_{\tau}) \ge B.
    \end{align*}
\end{lemma}

\subsection{Proof of \cref{thm:BETC-GLM-matching}}
The proof relies on the following lemma to establish an upper bound on $|\la \bphi_{i,j}, \hat{\btheta}_{\tau+N} - \btheta^* \ra|$.
\begin{lemma}[extracted from Lemma 3, \citet{li2017provably}]\label{lemma:self-normalizing}
Suppose $\lambda_{\min}(\Vb_{\tau+N}) \ge 1$.  For any $\delta > 0$, with probability at least $1-\delta$, we have
\begin{align*}
    \| \hat{\btheta}_{\tau+N} - \btheta^{*} \|_{\Vb_{\tau+N}}
    & \le 
    \frac{1}{\kappa}
    \sqrt{
    \frac{d}{2}
    \log(1+2(\tau+N)/d) + \log (1/\delta)
    }.
\end{align*}
\end{lemma}

\begin{proof}[Proof of \cref{thm:BETC-GLM-matching}]

The proof can be divided into three steps: 1. invoke Lemma~\ref{lemma:matrix-concentration} to show that the initial $\tau$ rounds for exploration will guarantee $\lambda_{\min}(\Vb_{\tau}) \ge 1$; 2. invoke Lemma~\ref{lemma:g-optimal} to obtain an optimal design $\pi$ and utilize Cauchy-Schwartz inequality to show that $|\la \hat{\btheta}_{\tau+N} - \btheta, \bphi_{i,j} \ra | \le 3\epsilon / \kappa$; 3. balance the not yet determined $\epsilon$ to obtain the regret upper bound.

Since we set $\tau$ such that
\begin{align*}
    \tau 
    & = 
    C_4 \lambda_0^{-2} (d+\log(1/\delta))
    \\
    & \ge 
    \bigg( 
    \frac{C_1 \sqrt{d} + C_2 \sqrt{\log(1/\delta)}}{\lambda_0}
    \bigg)^2
    +
    \frac{2}{ \lambda_0},
\end{align*}
with a large enough universal constant $C_4$, by \cref{lemma:matrix-concentration} to obtain that with probability at least $1-\delta$,
\begin{align} \label{eqn:lambda-min-V-tau-1}
        \lambda_{\min}(\Vb_{\tau}) \ge 
        1.
\end{align}
From now on, we assume \eqref{eqn:lambda-min-V-tau-1} always holds. 

Define $N := \sum_{i,j}N(i,j)$, $\Vb_{\tau+1:\tau+N} := \sum_{t=\tau+1}^{\tau+N} \bphi_{i_t,j_t}\bphi_{i_t,j_t}^{\top}$, $\Vb_{\tau+N}  : = \Vb_{\tau} + \Vb_{\tau+1:\tau+N}$.
Given the optimal design $\pi(i,j)$, the algorithm queries the pair $(i,j) \in \mathrm{supp}(\pi)$ for exactly $N(i,j) = \lceil d\pi(i,j) / \epsilon^2 \rceil$ times. Therefore, the design matrix $\Vb_{\tau+N} $ satisfies
\begin{align*}
    \Vb_{\tau+N} 
    & \succeq
    \Vb_{\tau+1:\tau+N} 
    \\
    & = \sum_{i,j} N(i,j) \bphi_{i,j} \bphi_{i,j}^{\top} 
    \\
    & \succeq \sum_{i,j} \frac{d\pi(i,j) }{\epsilon^2}  \bphi_{i,j} \bphi_{i,j}^{\top}
    \\
    & = 
    \frac{d }{\epsilon^2} \Vb(\pi),
\end{align*}
where $\Vb(\pi) := \sum_{i,j}\pi(i,j) \bphi_{i,j} \bphi_{i,j}^{\top}$. The first inequality holds because $\Vb_{\tau}$ is positive semi-definite, and the second inequality holds due to the choice of $N(i,j)$.

When \eqref{eqn:lambda-min-V-tau-1} holds, from \cref{lemma:self-normalizing}, we have with probability at least $1-\delta$, that for each $\bphi_{i,j}$,
\begin{align}
    |\la \hat{\btheta} - \btheta^*, \bphi_{i,j} \ra |
    & \le 
    \| \hat{\btheta}_{\tau+N} - \btheta^{*} \|_{\Vb_{\tau+N}}
    \cdot
    \|\bphi_{i,j} \|_{\Vb_{\tau+N}^{-1}} 
    \notag \\
    & \le 
    \| \hat{\btheta}_{\tau+N} - \btheta^{*} \|_{\Vb_{\tau+N}}
    \cdot
    \frac{\epsilon \|\bphi_{i,j} \|_{\Vb(\pi)^{-1}} }{\sqrt{d}}
    \notag \\
    & \le 
    \| \hat{\btheta}_{\tau+N} - \btheta^{*} \|_{\Vb_{\tau+N}}
    \cdot
    \epsilon
    \notag \\
    & \le 
    \frac{\epsilon}{\kappa}
    \cdot
    \sqrt{
    \frac{d}{2}
    \log(1+2(\tau+N)/d) + \log (1/\delta)
    } \label{eqn:inner-product-concentration-matching}
\end{align}
where 
the first inequality is due to the Cauchy-Schwartz inequality;
the second inequality holds because $\Vb_{\tau+N}  \succeq \frac{d}{\epsilon^2} \Vb(\pi)$; the third inequality holds because $\pi$ is an optimal design and by Lemma~\ref{lemma:g-optimal}, $\| \bphi_{i,j} \|^2_{\Vb(\pi)^{-1}} \le d$;
the last inequality comes from \cref{lemma:self-normalizing}.

To summarize, we have that with probability at least $1-2\delta$, for every $i \in [K]$,
\begin{align}
    | \hat{B}(i) - B(i) |
    & = 
    \bigg| \frac{1}{K} \sum_{j=1}^{K} \big(\mu(\bphi_{i,j}^{\top}\btheta^*) 
    -
    \mu(\bphi_{i,j}^{\top}\hat{\btheta}) \big)
    \bigg|
    \notag\\
    & \le 
    \frac{1}{K} \sum_{j=1}^{K} 
    \Big|\mu(\bphi_{i,j}^{\top}\btheta^*) 
    -
    \mu(\bphi_{i,j}^{\top}\hat{\btheta})  
    \Big|
    \notag\\
    & \le 
    \frac{L_{\mu}}{K} \sum_{j=1}^{K} 
    \big|
    \bphi_{i,j}^{\top}
    \big(\btheta^* - \hat{\btheta})
    \big|
    \notag\\
    & \le \frac{3L_{\mu}\epsilon}{\kappa}
    \cdot
    \sqrt{
    \frac{d}{2}
    \log(1+2(\tau+N)/d) + \log (1/\delta)
    }, \label{eqn:estimated-Borda-matching}
\end{align}
where the first equality is by the definition of the empirical/true Borda score; the first inequality is due to the triangle inequality; the second inequality is from the Lipschitz-ness of $\mu(\cdot)$ ($L_{\mu} = 1/4$ for the logistic function); the last inequality holds due to \eqref{eqn:inner-product-concentration-matching}. This further implies the gap between the empirical Borda winner and the true Borda winner is bounded by:
\begin{align*}
    B(i^*) - B(\hat{i})
    & =
    B(i^*) - \hat{B}(i^*) + \hat{B}(i^*) - B(\hat{i})
    \\
    & \le 
    B(i^*) - \hat{B}(i^*) + \hat{B}(\hat{i}) - B(\hat{i})
    \\
    & \le 
    \frac{6L_{\mu}\epsilon}{\kappa}\cdot
    \sqrt{
    \frac{d}{2}
    \log(1+2(\tau+N)/d) + \log (1/\delta)
    },
\end{align*}
where the first inequality holds due to the definition of $\hat{i}$, i.e., $\hat{B}(\hat{i}) \ge \hat{B}(i)$ for any $i$; the last inequality holds due to~\eqref{eqn:estimated-Borda-matching}.

Meanwhile, since $N := \sum_{(i,j) \in \mathrm{supp}(\pi)} N(i,j)$ and $|\mathrm{supp}(\pi)| \le d(d+1)/2$ from Lemma~\ref{lemma:g-optimal}, we have that 
\begin{align*}
    N & \le d(d+1)/2 + \frac{d}{\epsilon^2},
\end{align*}
because $\lceil x \rceil < x + 1$.


Therefore, with probability at least $1-2\delta$, the regret is bounded by:
\begin{align*}
    \mathrm{Regret}(T)
    & =
    \mathrm{Regret}_{1:\tau} + \mathrm{Regret}_{\tau+1 : \tau+N} + \mathrm{Regret}_{\tau+N+1:T}
    \\
    & \le 
    \tau +
    N
    +
    \frac{12L_{\mu}\epsilon T}{\kappa}  \cdot
    \sqrt{
    \frac{d}{2}
    \log(1+2(\tau+N)/d) + \log (1/\delta)
    }
    \\
    & \le 
    \tau +
    d(d+1)/2 + \frac{d}{\epsilon^2}
    +
    \frac{12L_{\mu}\epsilon T}{\kappa}  \cdot
    O
    \Bigg( d^{1/2} \sqrt{\log\bigg( \frac{T}{d\delta} \bigg)} \Bigg)
    \\
    & = 
    {O}
    \Bigg(
    \kappa^{-1}
    d ^{2/3} T^{2/3} 
    \sqrt{\log\bigg( \frac{T}{d\delta} \bigg)}
    \Bigg),
\end{align*}
where 
the first equation is simply dividing the regret into 3 stages: $1$ to $\tau$, $\tau+1$ to $\tau+N$, and $\tau+N+1$ to $T$; 
the second inequality is simply bounding the one-step regret from $1$ to $\tau+N$ by $1$, while for $t > \tau+N$, we have shown that the one-step regret is guaranteed to be smaller than $12L_{\mu} \epsilon \sqrt{
    d
    \log(1+2(\tau+N)/d) + \log (1/\delta)
    } / \sqrt{2} \kappa $.
The last line holds because we set $\tau = O(d + \log(1/\delta))$ and $\epsilon = d^{1/6} T^{-1/3}$. Note that to ensure $\tau + N < T$, it suffices to assume $T = \Omega(d^2)$.


By setting $\delta = T^{-1}$, we can show that the expected regret of Algorithm~\ref{alg:BETC-GLM} is bounded by
\begin{align*}
     \tilde{O} \big( \kappa^{-1} (d^{2/3} T^{2/3})
     \big).
\end{align*}
\end{proof}

\subsection{Proof of \cref{thm:BETC-GLM}}
The following lemma characterizes the non-asymptotic behavior of the MLE estimator. It is extracted from \citet{li2017provably}. 
\begin{lemma}[Theorem 1, \citealt{li2017provably}]\label{lemma:concentration}
    Define $\Vb_{s} = \sum_{t=1}^{s} \bphi_{i_t,j_t}\bphi_{i_t,j_t}^{\top}$, and $\hat{\btheta}_s$ as the MLE estimator \eqref{eqn:MLE} at round $s$.
    If $\Vb_{s}$ satisfies
    \begin{align} \label{eqn:lambda-min}
        \lambda_{\min}
        (\Vb_{s})
        & \ge 
        \frac{512 M_{\mu}^{2} (d^2 + \log(3/\delta))}{\kappa^{4}},
    \end{align}
    then for any fixed $\xb \in \RR^d$, with probability at least $1-\delta$,
    \begin{align*}
        |\la \hat{\btheta}_s - \btheta^*, \xb \ra |
        \le 
        \frac{3}{\kappa}
        \sqrt{\| \xb \|^2_{\Vb_s^{-1}} \log(3/\delta)}.
    \end{align*}
\end{lemma}

\begin{proof}[Proof of Theorem~\ref{thm:BETC-GLM}]

The proof can be essentially divided into three steps: 1. invoke Lemma~\ref{lemma:matrix-concentration} to show that the initial $\tau$ rounds for exploration will guarantee \eqref{eqn:lambda-min} be satisfied; 2. invoke Lemma~\ref{lemma:g-optimal} to obtain an optimal design $\pi$ and utilize Lemma~\ref{lemma:concentration} to show that $|\la \hat{\btheta}_{\tau+N} - \btheta, \bphi_{i,j} \ra | \le 3\epsilon / \kappa$; 3. balance the not yet determined $\epsilon$ to obtain the regret upper bound.



First, we explain why we assume
\begin{align*}
    T \ge
    \frac{C_3}{\kappa^6 \lambda_0^{3/2}}
    \max \Big\{
    d^{5/2}, 
    \frac{\log(K^2/\delta)}{\sqrt{d}}
    \Big\}.
\end{align*}
To ensure \eqref{eqn:lambda-min} in \cref{lemma:concentration} can hold, we resort to \cref{lemma:matrix-concentration}, that is 
\begin{align*}
    \tau & \ge \bigg( 
    \frac{C_1 \sqrt{d} + C_2 \sqrt{\log(1/\delta)}}{\lambda_0}
    \bigg)^2
    +
    \frac{2B}{\lambda_0},
    \\
    B & :=
    \frac{512 M_{\mu}^{2} (d^2 + \log(3/\delta))}{\kappa^{4}}.
\end{align*}  
Since we set $\tau = (d \log(K^2/\delta))^{1/3} T^{2/3}$,
this means $T$ should be large enough, so that
\begin{align*}
    (d \log(K^2/\delta))^{1/3} T^{2/3} & \ge \bigg( 
        \frac{C_1 \sqrt{d} + C_2 \sqrt{\log(1/\delta)}}{\lambda_0}
        \bigg)^2
        +
        \frac{1024 M_{\mu}^2(d^2 + \log(3K^2/\delta))}{ \kappa^4 \lambda_0}.
\end{align*}
With a large enough universal constant $C_3$, it is easy to verify that the inequality above will hold as long as
\begin{align*}
    T \ge
    \frac{C_3}{\kappa^6 \lambda_0^{3/2}}
    \max \Big\{
    d^{5/2}, 
    \frac{\log(K^2/\delta)}{\sqrt{d}}
    \Big\}.
\end{align*}



By Lemma~\ref{lemma:matrix-concentration}, we have that with probability at least $1-\delta$,
\begin{align} \label{eqn:lambda-min-V-tau}
        \lambda_{\min}(\Vb_{\tau}) \ge 
        \frac{512 M_{\mu}^{2} (d^2 + \log(3K^2/\delta))}{\kappa^{4}}.
\end{align}
From now on, we assume \eqref{eqn:lambda-min-V-tau} always holds. 

Define $N := \sum_{i,j}N(i,j)$, $\Vb_{\tau+1:\tau+N} := \sum_{t=\tau+1}^{\tau+N} \bphi_{i_t,j_t}\bphi_{i_t,j_t}^{\top}$, $\Vb_{\tau+N} : = \Vb_{\tau} + \Vb_{\tau+1:\tau+N}$.
Given the optimal design $\pi(i,j)$, the algorithm queries each pair $(i,j) \in \mathrm{supp}(\pi)$ for exactly $N(i,j) = \lceil d\pi(i,j) / \epsilon^2 \rceil$ times. Therefore, the design matrix $\Vb_{\tau+N} $ satisfies
\begin{align*}
    \Vb_{\tau+N} 
    & \succeq
    \Vb_{\tau+1:\tau+N} 
    \\
    & = \sum_{i,j} N(i,j) \bphi_{i,j} \bphi_{i,j}^{\top} 
    \\
    & \succeq \sum_{i,j} \frac{d\pi(i,j) }{\epsilon^2}  \bphi_{i,j} \bphi_{i,j}^{\top}
    \\
    & = 
    \frac{d}{\epsilon^2} \Vb(\pi),
\end{align*}
where $\Vb(\pi) := \sum_{i,j}\pi(i,j) \bphi_{i,j} \bphi_{i,j}^{\top}$. The first inequality holds because $\Vb_{\tau}$ is positive semi-definite, and the second inequality holds due to the choice of $N(i,j)$.


To invoke Lemma~\ref{lemma:concentration}, notice that $\lambda_{\min}(\Vb) \ge \lambda_{\min}(\Vb_{\tau})$. Along with \eqref{eqn:lambda-min-V-tau}, by Lemma~\ref{lemma:concentration}, we have for any fixed $\bphi_{i,j}$, with probability at least $1-\delta/K^2$, that
\begin{align}
    |\la \hat{\btheta} - \btheta^*, \bphi_{i,j} \ra |
    & \le 
    \frac{3}{\kappa} \sqrt{\| \bphi_{i,j} \|^2_{\Vb^{-1}_{\tau+N}  } \log(3K^2 / \delta)}
    \notag \\
    & \le 
    \frac{3}{\kappa}\sqrt{\frac{\epsilon^2}{d} \cdot\| \bphi_{i,j} \|^2_{\Vb(\pi)^{-1}} \log(3K^2 / \delta)}
    \notag \\
    & =
    \frac{3\epsilon}{\kappa} \sqrt{\frac{\| \bphi_{i,j} \|^2_{\Vb(\pi)^{-1}}}{d }  }
    \cdot \sqrt{\log(3K^2 / \delta)}
    \notag \\
    & \le \frac{3\epsilon}{\kappa} \cdot \sqrt{\log(3K^2 / \delta)}, \label{eqn:inner-product-concentration}
\end{align}
where the first inequality comes from Lemma~\ref{lemma:concentration}; the second inequality holds because $\Vb_{\tau+N}  \succeq \frac{d }{\epsilon^2} \Vb(\pi)$; the last inequality holds because $\pi$ is an optimal design and by Lemma~\ref{lemma:g-optimal}, $\| \bphi_{i,j} \|^2_{\Vb(\pi)^{-1}} \le d$. 

Taking union bound for each $(i,j) \in [K] \times [K]$, we have that with probability at least $1-\delta$, for every $i \in [K]$,
\begin{align}
    | \hat{B}(i) - B(i) |
    & = 
    \bigg| \frac{1}{K} \sum_{j=1}^{K} \big(\mu(\bphi_{i,j}^{\top}\btheta^*) 
    -
    \mu(\bphi_{i,j}^{\top}\hat{\btheta}) \big)
    \bigg|
    \notag\\
    & \le 
    \frac{1}{K} \sum_{j=1}^{K} 
    \Big|\mu(\bphi_{i,j}^{\top}\btheta^*) 
    -
    \mu(\bphi_{i,j}^{\top}\hat{\btheta})  
    \Big|
    \notag\\
    & \le 
    \frac{L_{\mu}}{K} \sum_{j=1}^{K} 
    \big|
    \bphi_{i,j}^{\top}
    \big(\btheta^* - \hat{\btheta})
    \big| 
    \notag\\
    & \le \frac{3L_{\mu}\epsilon}{\kappa} \cdot \sqrt{\log(3K^2 / \delta)}, \label{eqn:estimated-Borda}
\end{align}
where the first equality is by the definition of the empirical/true Borda score; the first inequality is due to the triangle inequality; the second inequality is from the Lipschitz-ness of $\mu(\cdot)$ ($L_{\mu} = 1/4$ for the logistic function); the last inequality holds due to \eqref{eqn:inner-product-concentration}. This further implies the gap between the empirical Borda winner and the true Borda winner is bounded by:
\begin{align*}
    B(i^*) - B(\hat{i})
    & =
    B(i^*) - \hat{B}(i^*) + \hat{B}(i^*) - B(\hat{i})
    \\
    & \le 
    B(i^*) - \hat{B}(i^*) + \hat{B}(\hat{i}) - B(\hat{i})
    \\
    & \le 
    \frac{6L_{\mu}\epsilon}{\kappa} \cdot \sqrt{\log(3K^2 / \delta)},
\end{align*}
where the first inequality holds due to the definition of $\hat{i}$, i.e., $\hat{B}(\hat{i}) \ge \hat{B}(i)$ for any $i$; the last inequality holds due to~\eqref{eqn:estimated-Borda}.


Meanwhile, since $N := \sum_{(i,j) \in \mathrm{supp}(\pi)} N(i,j)$ and $|\mathrm{supp}(\pi)| \le d(d+1)/2$ from Lemma~\ref{lemma:g-optimal}, we have that 
\begin{align*}
    N & \le d(d+1)/2 + \frac{d}{\epsilon^2},
\end{align*}
because $\lceil x \rceil < x + 1$.


Therefore, with probability at least $1-2\delta$, the regret is bounded by:
\begin{align*}
    \mathrm{Regret}(T)
    & =
    \mathrm{Regret}_{1:\tau} + \mathrm{Regret}_{\tau+1 : \tau+N} + \mathrm{Regret}_{\tau+N+1:T}
    \\
    & \le 
    \tau +
    N
    +
    \frac{12L_{\mu}\epsilon}{\kappa} T \cdot \sqrt{\log(3K^2 / \delta)}
    \\
    & \le 
    \tau +
    d(d+1)/2 + \frac{d}{\epsilon^2}
    +
    \frac{12L_{\mu}\epsilon}{\kappa} T \cdot \sqrt{\log(3K^2 / \delta)}
    \\
    & = 
    O
    \big(
    \kappa^{-1}
    (d \log(K/\delta))^{1/3} T^{2/3} 
    \big),
\end{align*}
where 
the first equation is simply dividing the regret into 3 stages: $1$ to $\tau$, $\tau+1$ to $\tau+N$, and $\tau+N+1$ to $T$. the second inequality is simply bounding the one-step regret from $1$ to $\tau+N$ by $1$, while for $t > \tau+N$, we have shown that the one-step regret is guaranteed to be smaller than $12L_{\mu} \epsilon \sqrt{\log(3K^2 / \delta)} / \kappa $.
The last line holds because we set $\tau = (d \log(3K^2/\delta))^{1/3} T^{2/3}$ and $\epsilon = d^{1/3} T^{-1/3} \log(3K^2/\delta)^{-1/6} $.

By setting $\delta = T^{-1}$, we can show that the expected regret of Algorithm~\ref{alg:BETC-GLM} is bounded by
\begin{align*}
     O\big( \kappa^{-1} (d \log(KT))^{1/3} T^{2/3})
     \big).
\end{align*}
Note that if there are exponentially many contextual vectors ($K \approx 2^d$), the upper bound becomes $\tilde{O}(d^{2/3}T^{2/3})$.
\end{proof}


\section{Omitted Proof in \cref{sec:adv}}
We make the following notation. Let $\cH_{t-1} := (q_1, P_1,(i_1, j_1), r_1,\ldots, q_t
, P_t)$ denotes the history up to time $t$. Here $P_t$ means the comparison probability $p_{i,j}^{t}$ at round $t$. The following lemmas are used in the proof. We first bound the estimate $\hat B_t(i)$.
\begin{lemma}\label{lemma1}
    For all $t \in [T]$, $i \in [K]$, it holds that $\hat B_t(i) \leq \lambda_0^{-1}/\gamma^2$.
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma1}]
Using our choice of $q_t \geq \gamma/K$, we have the following result for the matrix $Q_t$:
\begin{align}
Q_t = \sum_{i \in [K]}\sum_{j \in [K]} q_t(i) q_t(j) \bphi_{i,j} \bphi_{i,j}^{\top} \succeq \gamma^2 \frac{1}{K^2}\sum_{i \in [K]}\sum_{j \in [K]} \bphi_{i,j} \bphi_{i,j}^{\top}.\label{17.1}
\end{align}
Furthermore, we can use the definition of the estimate $\hat B_t(i)$ to show that
    \begin{align*}
    \hat B_t(i) &= \left\la \frac{1}{K}\sum_{j \in [K]} \bphi_{i,j}, \hat{\btheta}_t\right\ra = \left\la \frac{1}{K}\sum_{j \in [K]} \bphi_{i,j},Q_t^{-1} \bphi_{i_t,j_t}\right\ra r_t(i_t,j_t)\\
    &\leq \frac{1}{K}\sum_{j \in [K]}\|\bphi_{i,j}\|^2_{Q_t^{-1}},
    \end{align*}
where we use the fact that $|r_t| \le 1$.
Let $\bSigma = \frac{1}{K^2}\sum_{i=1}^{K} \sum_{j=1}^{K} \bphi_{i,j} \bphi_{i,j}^{\top}$. With \eqref{17.1} we have $Q_t \succeq \gamma^2 \bSigma$. Therefore, we can further bound $\hat B_t(i)$ with 
\begin{align*}
     \hat B_t(i) &\leq \frac{1}{K \gamma^2}\sum_{j \in [K]}\|\bphi_{i,j}\|^2_{\bSigma^{-1}} \\
    & \leq \frac{1}{\gamma^2} \max_{i,j}\|\bphi_{i.j}\|^2_{\bSigma^{-1}}\\
    & \leq \frac{\lambda_0^{-1}}{\gamma^2},
\end{align*}
where the first inequality holds due to \eqref{17.1} and that $\| \xb \|_{\Ab^{-1}}^2 \le \| \xb \|_{\Bb^{-1}}^2$ if $\Ab \succeq \Bb$; the third inequality holds because we assume $\lambda_0 \leq \lambda_{\min} \big(\frac{1}{K^2}\sum_{i=1}^{K} \sum_{j=1}^{K} \bphi_{i,j} \bphi_{i,j}^{\top} \big) $ and $\| \bphi_{i,j}\| \le 1$.
\end{proof}
The following lemma proves that our (shifted) estimate is unbiased.
\begin{lemma}\label{lemma2}
    For all $t \in [T]$, $i \in [K]$, the following equality holds:
    \begin{align*}    
    \EE[\hat B_t(i)] = B_t(i) - \frac{1}{2} .
    \end{align*}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma2}]
Using our definition of $\hat B_t(i)$, we have
    \begin{align*}
        \hat B_t(i) = \left\la \frac{1}{K}\sum_{j \in [K]} \bphi_{i,j}, \hat{\btheta}_t\right\ra = \left\la \frac{1}{K}\sum_{j \in [K]} \bphi_{i,j}, Q_t^{-1} \bphi_{i_t,j_t} \right\ra r_t(i_t,j_t).
    \end{align*}
Therefore, by the law of total expectation (tower rule), we have
\begin{align*}
    \EE[\hat B_t(i)] &= \EE_{\cH_{t-1}}\bigg[\EE_{(i_t,j_t,r_t)}\Big[\Big\la \frac{1}{K}\sum_{j \in [K]} \bphi_{i,j}, Q_t^{-1} \bphi_{i_t,j_t} \Big\ra r_t(i_t,j_t)|\cH_{t-1}\Big]\bigg]\\
    & = \EE_{\cH_{t-1}}\bigg[\EE_{(i_t,j_t)}\Big[\Big\la \frac{1}{K}\sum_{j \in [K]} \bphi_{i,j}, Q_t^{-1} \bphi_{i_t,j_t} \Big\ra \EE_{r_t}[r_t(i_t,j_t)|(i_t,j_t)]\Big|\cH_{t-1}\Big]\bigg]\\
    & = \EE_{\cH_{t-1}}\bigg[\EE_{(i_t,j_t)}\Big[\Big\la \frac{1}{K}\sum_{j \in [K]} \bphi_{i,j}, Q_t^{-1} \bphi_{i_t,j_t} \Big\ra p_t(i_t,j_t)\Big| \cH_{t-1}\Big]\bigg]
\end{align*}
Then we use the definition of $p_t$ and the expectation. We can further get the equality
\begin{align*}
    \EE[\hat B_t(i)] & = \EE_{\cH_{t-1}}\bigg[\EE_{(i_t,j_t)}\Big[\Big\la \frac{1}{K}\sum_{j \in [K]} \bphi_{i,j}, Q_t^{-1} \bphi_{i_t,j_t}\bphi^{\top}_{i_t,j_t}\btheta^* \Big\ra \Big| \cH_{t-1}\Big]\bigg]\\
    & = \EE_{\cH_{t-1}}\bigg[\Big\la \frac{1}{K}\sum_{j \in [K]} \bphi_{i,j}, Q_t^{-1} \Big(\sum_{i\in[K]}\sum_{j \in [K]}q_t(i)q_t(j)\bphi_{i,j}\bphi^{\top}_{i,j}\Big)\btheta^* \Big\ra \bigg|\cH_{t-1}\bigg]\\
    &= \EE_{\cH_{t-1}}\bigg[\Big\la \frac{1}{K}\sum_{j \in [K]} \bphi_{i,j},\btheta^* \Big\ra \bigg| \cH_{t-1}\bigg]\\
    &= B_t(i) - \frac{1}{2}.
\end{align*}
Therefore, we have completed the proof of Lemma \ref{lemma2}.
\end{proof}
The following lemma is similar to Lemma 5 in \citet{saha2021adversarial}.
\begin{lemma}\label{lemma3}
    $\EE_{\cH_t}[q_t^{\top}\hat B_t] = \EE_{\cH_{t-1}}\big[\EE_{x\sim q_t}[B_t(x)|\cH_{t-1}]\big] - \frac{1}{2}$, $\forall t \in [T]$.
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma3}]
Taking conditional expectation, we have
\begin{align*}
    \EE_{\cH_t}[q^{\top}_t \hat B_t]
    &= \EE_{\cH_t}\left[\sum_{i=1}^K q_t(i)\hat B_t(i)\right] \\
    &=\EE_{\cH_{t-1}}\left[\sum^K_{i=1}q_t(i)\EE_{(i_t,j_t,r_t)}\left[\hat B(i) \Big| \cH_{t-1}\right]\right]\\
    &= \EE_{\cH_{t-1}}\left[\sum^{K}_{i=1}q_t(i)\left(B_t(i) - \frac{1}{2} \right)\right] 
    \\
    &= \EE_{\cH_{t-1}}\left[\EE_{x\sim q_t}\left[B_t(x) \Big| \cH_{t-1}\right]\right]
    -
    \frac{1}{2},
\end{align*}
where we use the law of total expectation again as well as \cref{lemma2}.
\end{proof}
The last lemma bounds a summation $\sum_{i \in [K]}q_t(i)\hat B_t(i)^2$, which will be important in our proof.
\begin{lemma}\label{Lemma:4}
    At any time $t$, $\EE[\sum_{i \in [K]}q_t(i)\hat B_t(i)^2] \leq d/\gamma$.
\end{lemma}
\begin{proof}[Proof of Lemma \ref{Lemma:4}]
    Let $\hat{P}_{t}(i,j) = \la \bphi_{i,j},\hat{\btheta}_t\ra$.
    Using the definition of $\hat B_t$ and $\hat{P}_{t}(i,j)$, we have the following inequality:
    \begin{align*}
        \EE\left[\sum_{i \in [K]}q_t(i)\hat B_t(i)^2\right] &= \EE\left[\sum_{i \in [K]}q_t(i)\left(\frac{1}{K}\sum_{j \in [K]}\hat{P}_t(i,j)\right)^2\right]\\
        & \leq \EE\left[\sum_{i \in [K]}q_t(i)\frac{1}{K}\sum_{j \in [K]}\hat{P}^2_t(i,j)\right]\\
        & = \EE\left[\sum_{i \in [K]}q_t(i)\frac{1}{\gamma}\sum_{j \in [K]}\frac{\gamma}{K}\hat{P}^2_t(i,j)\right]\\
        & \leq \frac{1}{\gamma}\EE\left[\sum_{i \in [K]}\sum_{j \in [K]}q_t(i) q_t(j)\hat{P}^2_t(i,j)\right].
    \end{align*}
    The first inequality holds due to the Cauchy-Schwartz inequality; the second inequality holds because the definition of $q_t$ satisfies $q_t \geq \gamma/K$.

    Expanding the definition of $\hat{P}^2_t(i,j)$, we have
    \begin{align*}
    \hat{P}_t^2(i,j) &= 
r_t^2(i_t,j_t)\left(\bphi_{i,j}^{\top}Q_t^{-1}\bphi_{i_t,j_t}\right)^2\\
& \leq \bphi_{i_t,j_t}^{\top}Q_t^{-1}\bphi_{i,j}\bphi_{i,j}^{\top}Q_t^{-1}\bphi_{i_t,j_t}, 
 \end{align*}
 where we use $0 \leq r_t^2(i_t,j_t) \leq 1$.
 Therefore, the following inequality holds,
 \begin{align*}
     \sum_{i \in [K]}\sum_{j \in [K]}q_t(i) q_t(j)\hat{P}^2_t(i,j) &\leq \sum_{i \in [K]}\sum_{j \in [K]}q_t(i) q_t(j) \bphi_{i_t,j_t}^{\top}Q_t^{-1}\bphi_{i,j}\bphi_{i,j}^{\top}Q_t^{-1}\bphi_{i_t,j_t}\\
     &=  \bphi_{i_t,j_t}^{\top}Q_t^{-1}\left(\sum_{i \in [K]}\sum_{j \in [K]}q_t(i) q_t(j)\bphi_{i,j}\bphi_{i,j}^{\top}\right)Q_t^{-1}\bphi_{i_t,j_t}\\
     &= \bphi_{i_t,j_t}^{\top}Q_t^{-1}\bphi_{i_t,j_t}\\
      &= \text{trace} (\bphi_{i_t,j_t}\bphi_{i_t,j_t}^{\top}Q_t^{-1}).
 \end{align*}
 Using the property of trace, we have
 \begin{align*}
     \EE\left[\sum_{i \in [K]}\sum_{j \in [K]}q_t(i) q_t(j)\hat{P}^2_t(i,j)\right]\leq \text{trace} \left(\sum_{i \in [K]}\sum_{j \in [K]}q_t(i)q_t(j)\bphi_{i,j}\bphi_{i,j}^{\top}Q_t^{-1}\right)=d.
 \end{align*}
 Therefore, we finish the proof of Lemma \ref{Lemma:4}.
\end{proof}

\begin{proof}[Proof of Theorem \ref{Thm:adv}]
Our regret is defined as follows, 
\begin{align*}
\EE_{\cH_T}[R_T] &= \EE_{\cH_T}\left[\sum_{t=1}^{T}\left[2 B_t(i^*)-B_t(i_t)-B_t(j_t)\right]\right]\\
&= \max_{i \in [K]} \EE_{\cH_T} \left[\sum_{t=1}^{T} \left[2 B_t(i)-B_t(i_t)-B_t(j_t)\right]\right].
\end{align*}
The second equality holds because $B_t$ and $i^*$ are independent of the randomness of the algorithm.
Furthermore, we can write the expectation of the regret as 
\begin{align}
    \EE_{\cH_T}[R_T] &= 2 \max_{i \in [K]} \sum_{t=1}^{T}  B_t(i)-\sum_{t=1}^{T}\EE_{\cH_T} \left[B_t(i_t)+B_t(j_t)\right]
    \notag \\
    &= 2 \max_{i \in [K]} \sum_{t=1}^{T}  B_t(i)-2\sum_{t=1}^{T}\EE_{\cH_{t-1}} \left[\EE_{x\sim q_t}\left[B_t(x)|\cH_{t-1}\right]\right]
    \notag \\
    &= 2 \max_{i \in [K]} \sum_{t=1}^{T} \left( B_t(i) - \frac{1}{2} \right)  - 2\EE_{\cH_t}\left[q_t^{\top}\hat B_t\right], \label{eqn:regret-decomp}
\end{align}
where the last equality is due to Lemma \ref{lemma3}.

Then we follow the standard proof of EXP3 algorithm \citep{Lattimore2020BanditA}. Let $S_{t,k} = \sum_{s=1}^{t}\left( B_s(k) - \frac{1}{2} \right)$,  $\hat S_{t,k} = \sum_{s=1}^{t}\hat B_s(k)$, $\omega_t = \sum_{k \in [K]}\exp (-\eta \hat S _{t,k})$ and $\omega_0 = K$. We have $\forall a \in [K]$,
\begin{align}
    \exp(-\eta \hat S_{T,a}) &\leq \sum_{k\in[K]} \exp(-\eta \hat S_{T,k})= \omega_T = \omega_0 \cdot \prod_{t=1}^{T}\frac{\omega_{t}}{\omega_{t-1}}.\label{2.1}
\end{align}
For each term in the product, we have \begin{align}
    \notag\frac{\omega_{t+1}}{\omega_t} &= \sum_{k \in [K]} \frac{\exp(-\eta \hat S_{t-1,k})}{\omega_{t-1}} \cdot \exp (-\eta \hat B_t(k))\\
    &= \sum_{k \in [K]}\tilde q_t(k) \exp (-\eta \hat B_t(k)),\label{2.2}
\end{align}
where the second equality holds because of the definition of $\tilde q_t$.
For any $\eta \leq \lambda_0 \gamma^2$,  Lemma \ref{lemma1} presents $|\eta \hat{B}_t(k)| \leq 1$. Thus, using the basic inequality $\exp(x) \leq 1 + x + x^2/2$ when $x \leq 1$, and $\exp (x) \geq 1 + x$, we have
\begin{align}
    \notag\frac{\omega_{t+1}}{\omega_t} &\leq 
    \sum_{k \in [K]}\tilde q_t(k) \left(1-\eta \hat B_t(k) + \eta^2 \hat B_t^2 (k)\right)\\
    \notag&= 1 -\eta \sum_{k \in [K]} \tilde q_t(k)\hat B_t(k) + \eta^2 \sum_{k \in [K]}\tilde q_t(k)\hat B_t^2(k)\\
    & \leq \exp \left(-\eta \sum_{k \in [K]} \tilde q_t(k)\hat B_t(k) + \eta^2 \sum_{k \in [K]}\tilde q_t(k)\hat B_t^2(k)\right).\label{2.3}
\end{align}
Combining \eqref{2.1}, \eqref{2.2} and \eqref{2.3}, we have 
\begin{align*}
    \exp(-\eta \hat S_{T,a}) \leq K \exp \left(\sum_{t=1}^T \left[-\eta \sum_{k \in [K]} \tilde q_t(k)\hat B_t(k) + \eta^2 \sum_{k \in [K]}\tilde q_t(k)\hat B_t^2(k)\right]\right),
\end{align*}
and therefore 
\begin{align*}
    \sum_{t=1}^T \hat B_t(a) - \sum _{t=1}^T \tilde q_t ^{\top} \hat B_t \leq \frac{\log K}{\eta} + \eta \sum_{t=1}^T\sum_{k \in [K]}\tilde q_t(k)\hat B_t^2(k). 
\end{align*}
Since $\tilde q_t = \frac{q_t-\gamma/K}{1-\gamma}$, we have
\begin{align*}
    (1-\gamma)\sum_{t=1}^T \hat B_t(a) - \sum _{t=1}^T  q_t ^{\top} \hat B_t \leq \frac{\log K}{\eta} + \eta \sum_{t=1}^T\sum_{k \in [K]}\tilde q_t(k)\hat B_t^2(k).
\end{align*}
Choosing $a = i^*$, changing the summation index to $i$ and taking expectation on both sides, we have
\begin{align*}
    (1-\gamma)\EE_{\cH_T}\sum_{t=1}^T \hat B_t(i^*) - \sum _{t=1}^T  \EE_{\cH_T}\left[q_t ^{\top} \hat B_t \right] \leq \frac{\log K}{\eta} + \EE_{\cH_T}\left[\eta \sum_{t=1}^T\sum_{i \in [K]} q_t(i)\hat B_t^2(i)\right].
\end{align*}
Substituting the above inequality into \eqref{eqn:regret-decomp} and using Lemma \ref{lemma2}, \ref{lemma3}, we can bound the regret as
    \begin{align*}
        \EE[R_T] &\leq 2 \gamma T + \frac{2\log K}{\eta} + 2 \eta\sum_{t=1}^T\EE_{\cH_T}\left[\sum_{i \in [K]}q_t(i)s_t(i)^2\right]\\
        & \leq 2\gamma T + 2\frac{\log K}{\eta} + \frac{2\eta d T}{\gamma}\\
        & \leq 2(\log K)^{1/3}d^{1/3}T^{2/3}\sqrt{1/\lambda_0} + 2(\log K)^{1/3} d^{1/3}T^{2/3} + 2(\log K)^{1/3} d^{1/3}T^{2/3}\sqrt{\lambda_0},
    \end{align*}
    where the second inequality holds due to Lemma \ref{Lemma:4}. In the last inequality, we put in our choice of parameters $\eta = (\log K)^{2/3}d^{-1/3}T^{-2/3}$ and $\gamma = \sqrt{\eta d /\lambda_0} = (\log K)^{1/3}d^{1/3}T^{-1/3} \lambda_0^{-1/2}$.
    This finishes our proof of Theorem \ref{Thm:adv}.
\end{proof}



%\newpage

\section{Detailed Explanation of Studied Algorithms in Experiments}\label{sec:exp-list}
The following list summarizes all methods we implemented:

\textsc{BETC-GLM(-Match)}: \cref{alg:BETC-GLM} proposed in this paper. 
% When running with synthetic data, it can be equivalently written as \cref{alg:BETC-Linear} in \cref{sec:BETC-Linear}, where $\hat \btheta$ can be calculated in closed form. 
For general link function, to find $\hat \btheta$ by MLE in \eqref{eqn:MLE}, 100 rounds of gradient descent are performed. The failure probability is set to $\delta = 1/T$. Parameters $\tau$ and $\epsilon$ are set to values listed in \cref{thm:BETC-GLM}. For \textsc{BETC-GLM-Match}, we use the $\tau$ and $\epsilon$ outlined in \cref{thm:BETC-GLM-matching}. 

\textsc{UCB-Borda}: The UCB algorithm~\citep{Auer2002FinitetimeAO} using \textit{Borda reduction} technique mentioned by \citet{BusaFekete2018PreferencebasedOL}. The complete listing is displayed in \cref{alg:UCB-Borda}. 

\textsc{DEXP3}: Dueling-Exp3 is an adversarial Borda bandit algorithm developed by \citet{Saha2021AdversarialDB}, which also applies to our stationary bandit case. Relevant tuning parameters are set according to their upper-bound proof. % This is the only method in literature the authors are known of that works for dueling Borda regret considered in this paper.

\textsc{ETC-Borda}: %Since \textsc{DEXP3} is designed to work with a non-stationary environment, its adaptiveness may lead to an overhead in regret. 
We devise a simple explore-then-commit algorithm, named \textsc{ETC-Borda}. Like \textsc{DEXP3}, \textsc{ETC-Borda} does not take any contextual information into account. 
% However, it is designed to only work in a stationary environment and therefore considered a better comparison to \textsc{BETC-GLM}. 
The complete procedure of \textsc{ETC-Borda} is displayed in \cref{alg:ETC-Borda}, \cref{sec:ETC-Borda}. The failure probability $\delta$ is optimized as $1/T$.

\textsc{BEXP3}: The proposed method for adversarial Borda bandits displayed in \cref{alg:bexp3}. $\eta$ and $\gamma$ are chosen to be the value stated in \cref{Thm:adv}.

\section{Real-world Data Experiments}\label{sec:exp-realworld}
To showcase the performance of the algorithms in a real-world setting, we use EventTime dataset \citep{Zhang2016CrowdsourcedTA}. In this dataset, $K = 100$ historical events are compared in a pairwise fashion by crowd-sourced workers. 





We first calculate the empirical preference probabilities $\tilde p_{i,j}$ from the collected responses. A visualized preferential matrix consisting of $\tilde p_{i,j}$ is shown in \cref{fig:ev_data_mat} in \cref{sec:data-vis}, which demonstrates that STI and SST conditions hardly hold in reality. 
During simulation, $\tilde p_{i,j}$ is the parameter of the Bernoulli distribution that is used to generate the responses whenever a pair $(i,j)$ is queried.
The contextual vectors $\bphi_{i,j}$ are generated randomly from $\{-1, +1\}^5$.
For simplicity, we assign the item pairs that have the same probability value with the same contextual vector, i.e., if $\tilde p_{i,j} = \tilde p_{k,l}$ then $\bphi_{i,j} = \bphi_{k,l}$. 
The MLE estimator $\hat \btheta$ in \eqref{eqn:MLE} is obtained to construct the recovered preference probability $\hat p_{i,j} := \mu( \bphi_{i,j}^{\top} \hat \btheta )$ where $\mu(x) = 1/(1+e^{-x})$ is the logistic function. We ensure that the recovered preference probability $\hat p_{i,j}$ is close to $\tilde p_{i,j}$, so that $\bphi_{i,j}$ are informative enough. 
%Which is reasonable since the empirical estiamte itself does not accurately reflect the underlying probabilies of those pairs. % This step is to make sure that we can recover the theta using the randomly generated phi.
As shown in \cref{fig:ev_data-app}, our algorithm outperforms the baseline methods as expected. In particular, the gap between our algorithm and the baselines is even larger than that under the generated hard case.  In both settings, our algorithms demonstrated a stable performance with negligible variance.

\begin{figure}[H]
    \centering
        \includegraphics[width=0.45\textwidth]{figures/events.pdf}
        \caption{EventTime}
        \label{fig:ev_data-app}
    \caption{The regret of the proposed algorithm (\textsc{BETC-GLM,BEXP3}) and the baseline algorithms (\textsc{UCB-Borda}, \textsc{DEXP3}, \textsc{ETC-Borda}).}
\end{figure}

\subsection{Data Visualization}\label{sec:data-vis}

The events in EventTime dataset are ordered by the time they occurred. In \cref{fig:ev_data_mat},  the magnitude of each $\tilde p_{i,j}$ is color coded. It is apparent that there is no total/consistent ordering (i.e., $\tilde p_{i,j} > \frac12 \Leftrightarrow i \succ j$) can be inferred from this matrix due to inconsistencies in the ordering and many potential paradoxes. Hence STI and SST can hardly hold in this case. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/events_ds.png}
    \caption{Estimated preferential matrix consists of $\tilde p_{i,j}$ from the EventTime dataset.}
    \label{fig:ev_data_mat}
\end{figure}



\section{Additional Information for Experiments}

% \subsection{The \textsc{BETC-Linear} Algorithm}\label{sec:BETC-Linear}
% The \textsc{BETC-Linear} procedure, displayed in \Cref{alg:BETC-Linear}, is a simplified version of \cref{alg:BETC-GLM} with a linear link function. It can be reduced to \cref{alg:BETC-GLM} by setting $\tau = 0$ and $\mu(x) = \frac12 + x$. The MLE estimator can be calculated in closed form in this case.
% \begin{algorithm}[ht]
%     \caption{\textsc{BETC-Linear}}
%     \label{alg:BETC-Linear}
%     \begin{algorithmic}[1]
%         \STATE {\bfseries Input:} time horizon $T$, number of items $K$, feature dimension $d$, feature vectors $\bphi_{i,j}$ for $i \in [K]$, $j \in [K]$.

%         \STATE Set $\epsilon = (d \log(K/\delta))^{1/3} T^{-1/3}$
%         \STATE Find the G-optimal design $\pi(i,j)$
%         \STATE Let $N(i,j) = \big \lceil \frac{2d \pi(i,j)}{\epsilon^2} \log(K^2/\delta) \big \rceil$ for any $(i,j) \in \text{supp}(\pi)$
%         \FOR{$i \in [K]$, $j \in [K]$, $s \in [N(i,j)]$}
%             \STATE set $t \leftarrow t + 1$, set $(i_t, j_t) = (i, j)$
%             \STATE query pair $(i_t, j_t)$ and receive feedback $r_t$
%         \ENDFOR
%         \STATE Calculate the empirical least square estimator:
%         \begin{align*}
%             \hat{\btheta} = \Vb^{-1} \sum_{i,j} \sum_{t=1}^{N(i,j)} \bphi_{i,j} r_t, \Vb = \sum_{i,j} N(i,j) \bphi_{i,j} \bphi_{i,j}^{\top}
%         \end{align*}
%         \STATE Estimate the Borda score for each item:
%         \begin{align*}
%             \hat{B}(i) = \frac{1}{K} \sum_{j=1}^{K} \bphi_{i,j}^{\top} \hat{\btheta},
%             \hat{i} = \argmax_{i \in [K]} \hat{B}(i)
%         \end{align*}
%         \STATE Keep querying $(\hat{i}, \hat{i})$ for the rest of the time.
%     \end{algorithmic}
% \end{algorithm}

\subsection{The \textsc{UCB-Borda} Algorithm}\label{sec:UCB-Borda}
The \textsc{UCB-Borda} procedure, displayed in \cref{alg:UCB-Borda} is a \textsc{UCB} algorithm with Borda reduction only capable of minimization of regret in the following form:
\begin{align*}
    \mathrm{Regret}(T) = \sum_{t=1}^{T} \big( B(i^*) - B(i_t) \big).
\end{align*}
Let $\nbb_i$ be the number of times arm $i \in [K]$ has been queried. Let $\wb_i$ be the number of times arm $i$ wins the duel. $\hat B(i)$ is the estimated Borda score.
$\alpha$ is set to 0.3 in all experiments.
\begin{algorithm}[H]
    \caption{\textsc{UCB-Borda}}
    \label{alg:UCB-Borda}
    \begin{algorithmic}[1]
        \STATE {\bfseries Input:} time horizon $T$, number of items $K$, exploration parameter $\alpha$.
        \STATE {\bfseries Initialize: } $\nbb = \wb = \{0\}^K$, $\hat B(i) = \frac12, i \in [K]$
        
        \FOR{$t = 1, 2, \dots, T$}
            \STATE $i_t = \argmax_{k \in [K]}
                \big(
                \hat B_k + 
                \sqrt{
                \frac{\alpha \log(t)}{\nbb_k}
                }
                \big)$
            \STATE sample $j_t \sim \mathrm{Uniform}([K])$
            \STATE query pair $(i_t, j_t)$ and receive feedback $r_t \sim \text{Bernoulli}(p_{i_t, j_t})$
            \STATE $\nbb_{i_t} = \nbb_{i_t} + 1$, $\wb_{i_t} = \wb_{i_t} + r_t$, $\hat B(i_t) = \frac{\wb_{i_t}}{\nbb_{i_t}}$
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\subsection{The \textsc{ETC-Borda} Algorithm}\label{sec:ETC-Borda}
The \textsc{ETC-Borda} procedure, displayed in \cref{alg:ETC-Borda} is an explore-then-commit type algorithm capable of minimizing the Borda dueling regret.
% It can be reduced to the \textsc{BETC-Linear} case when $d = 1$.
It can be shown that the regret of \cref{alg:ETC-Borda} is $\Tilde{O}(K^{1/3} T^{2/3})$.
\begin{algorithm}[H]
    \caption{\textsc{ETC-Borda}}
    \label{alg:ETC-Borda}
    \begin{algorithmic}[1]
        \STATE {\bfseries Input:} time horizon $T$, number of items $K$, target failure probability $\delta$
        \STATE {\bfseries Initialize: } $\nbb = \wb = \{0\}^K$, $\hat B(i) = \frac12, i \in [K]$
        
        \STATE Set $N = \lceil K^{-2/3}T^{2/3} \log(K/\delta)^{1/3} \rceil$
        \FOR{$t = 1, 2, \dots, T$}
            \STATE Choose action \label{line:action-i}
                $i_t \leftarrow \begin{cases}
                    1 + (t-1) \text{ mod } K, \text{ if } t \le KN, \\
                    \argmax_{i \in [K]} \hat{B}(i), \text{ if } t > KN.
                \end{cases}$
            \STATE Choose action \label{line:action-j}
                $j_t = \begin{cases}
                    \mathrm{Uniform}([K]), \text{ if } t \le KN, \\
                    \argmax_{i \in [K]} \hat{B}(i), \text{ if } t > KN.
                \end{cases}$   
            \STATE query pair $(i_t, j_t)$ and receive feedback $r_t \sim \text{Bernoulli}(p_{i_t, j_t})$
            \IF{$t \le N$}
            \STATE $\nbb_{i_t} = \nbb_{i_t} + 1$, $\wb_{i_t} = \wb_{i_t} + r_t$, $\hat B(i_t) = \frac{\wb_{i_t}}{\nbb_{i_t}}$
            \ENDIF
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\subsection{\textsc{Frank-Wolfe} algorithm used to find approximate solution for G-optimal design}\label{sec:fw}
In order to find a solution for the G-optimal design problem, we resort to the Frank-Wolfe algorithm to find an approximate solution. The detailed procedure is listed in \cref{alg:fw}. In Line~\ref{line:fw-wv}, each outer product costs $d^2$ multiplications, $K^2$ such matrices are scaled and summed into a $d$-by-$d$ matrix $\Vb(\pi)$, which costs $O(K^2d^2)$ operations in total. In Line~\ref{line:fw-maxij}, one matrix inversion costs approximately $O(d^3)$. The weighted norm requires $O(d^2)$ and the maximum is taken over $K^2$ such calculated values. The scaling and update in the following lines only require $O(K^2)$. In summary, the algorithm is dominated by the calculation in Line~\ref{line:fw-maxij} which costs $O(d^2K^2)$. 

In experiments, the G-optimal design $\pi(i,j)$ is approximated by running 20 iterations of Frank-Wolfe algorithm, which is more than enough for its convergence given our particular problem instance. (See Note 21.2 in \citep{Lattimore2020BanditA}). 

\begin{algorithm}[htb]
    \caption{\textsc{G-optimal design by Frank-Wolfe}}
    \label{alg:fw}
    \begin{algorithmic}[1]
    \STATE {\bfseries Input:} number of items $K$, contextual vectors $\bphi_{i,j}, i \in [K], j \in [K]$, number of iterations $R$
    \STATE {\bfseries Initialize: } $\pi_1(i,j) = 1 / K^2$
    \FOR{$r = 1, 2, \cdots, R$}
        \STATE $ \Vb(\pi_r) = \sum_{i,j} \pi_r(i,j) \bphi_{i,j} \bphi_{i,j}^{\top} $
        \label{line:fw-wv}
        \STATE $i_{r}^*,j_{r}^* = \argmax_{(i,j) \in [K] \times [K]} ||\bphi_{i,j}||_{\Vb(\pi_r)^{-1}} $
        \label{line:fw-maxij}
        \STATE $g_r = ||\bphi_{i_r^*,j_r^*}||_{\Vb(\pi_r)^{-1}} $
        \STATE $ \gamma_r = \frac{g_r-1 / d}{g_r - 1} $
        \STATE $\pi_{r+1}(i,j) = (1-\gamma_r) \pi_r(i,j) + \gamma_r \mathbbm{1}(i_{r}^* = i)\mathbbm{1}(j_{r}^* = j)$
    \ENDFOR
    \STATE {\bfseries Output:} Approximate G-optimal design solution $\pi_{R+1}(i,j)$
    \end{algorithmic}
\end{algorithm}
% \tao{There is a sentence mentioning FW and cited the text book, shall I expand it? i.e.  it the dominant term is the summation of $K^2$ $d$-vectors' weighted norm which is $K^2d^4$.}

% \tao{Hard to describe in 2 -3 lines, please see if \cref{alg:fw} in \cref{sec:fw} works. Otherwise, I can try again to compress that into several sentences.}
% \newpage


% \begin{figure}
%     \centering
%     \input{mat.tex}
%     \caption{Matrix used to construct the hard case of the lower bound.}
%     \label{fig:hardness_mat}
% \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliography{ref}
\bibliographystyle{ims}

\end{document}