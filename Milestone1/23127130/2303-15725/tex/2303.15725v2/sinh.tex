\section{Sinh Regression}\label{sec:sinh}
 
In this section, we provide detailed analysis of $L_{\sinh}$. In Section~\ref{sec:sinh:definition} we define the loss function $L_{\sinh}$ based on $\sinh(x)$. In Section~\ref{sec:sinh:gradient} we compute the gradient of $L_{\sinh}$ by detail. In Section~\ref{sec:sinh:hessian} we compute the hessian of $L_{\sinh}$ by detail. In Section~\ref{sec:sinh:gradient_hessian}, we summarize the result of Section~\ref{sec:sinh:gradient} and Section~\ref{sec:sinh:hessian} and aquire the gradient $\nabla L_{\sinh}$ and hessian $\nabla^2 L_{\sinh}$ for $L_{\sinh}$. In Section~\ref{sec:sinh:loss_reg} we define $L_{\sinh,\reg}$ by adding the regularization term 
$L_{\reg}$ in Section~\ref{sec:preli:regularization} to $L_{\sinh}$ and compute the gradient $\nabla L_{\sinh,\reg}$ and hessian $\nabla^2 L_{\sinh,\reg}$ of $L_{\sinh,\reg}$. In Section~\ref{sec:sinh:convex} we proved that $\nabla^2 L_{\sinh,\reg} \succ 0$ and thus showed that $L_{\sinh,\reg}$ is convex. In Section~\ref{sec:sinh:lipschitz} we provide the upper bound for $\|\nabla^2 L_{\sinh,\reg}(x)-\nabla^2 L_{\sinh,\reg}(y)\|$ and thus proved $\nabla^2 L_{\sinh,\reg}$ is lipschitz.
\subsection{Definition}\label{sec:sinh:definition}
\begin{definition}\label{def:L_sinh}
Given $A \in \R^{n \times d}$ and $b \in \R^n$. For a vector $x \in \R^d$, we define loss function $L(x)$ as follows:
\begin{align*}
L(x) := 0.5 \cdot \| \sinh(Ax) - b \|_2^2
\end{align*}
\end{definition}



\subsection{Gradient}\label{sec:sinh:gradient}

\begin{lemma}[Gradient for Sinh]\label{lem:gradient_sinh}
We have
\begin{itemize}
    
    \item Part 1.
    \begin{align*}
        \frac{ \d ( \sinh(Ax) - b ) }{ \d t } = \cosh(Ax) \circ \frac{A \d x }{ \d t}
    \end{align*}
    \item Part 2. 
    \begin{align*}
        \frac{\d L_{\sinh} }{ \d t} = (\sinh(Ax) - b )^\top \cdot ( \cosh(Ax) \circ \frac{A \d x}{ \d t } )
    \end{align*}
\end{itemize}
Further, we have for each $i \in [d]$
\begin{itemize}
    
    \item Part 3.
    \begin{align*}
        \frac{ \d ( \sinh(Ax) - b ) }{ \d x_i } = \cosh(Ax) \circ A_{*,i}
    \end{align*}
    \item Part 4. 
    \begin{align*}
        \frac{\d L_{\sinh} }{ \d x_i}  
        = & ~ (\sinh(Ax) - b )^\top \cdot ( \cosh(Ax) \circ A_{*,i} )
    \end{align*}
    \item Part 5.
    \begin{align*}
        \frac{\d L_{\sinh} }{ \d x} =  A^\top \diag( \cosh(Ax) ) (\sinh(Ax) - b)
    \end{align*}
\end{itemize}
\end{lemma}

\begin{proof}


{\bf Proof of Part 1.}

For each $i \in [n]$, we have
\begin{align*}
 \frac{ \d ( \sinh(Ax) - b )_i }{ \d t } 
 = & ~ \cosh(Ax)_i \cdot \frac{\d (Ax)_i}{\d t} \\
 = & ~ \cosh(Ax)_i \cdot \frac{ (A \d x)_i}{\d t} 
\end{align*}
where the first and second step follow from the differential chain rule.

Thus, we complete the proof.

{\bf Proof of Part 2.}

We have
 
\begin{align*}
    \frac{\d L_{\sinh} }{ \d t} 
    = & ~ (\sinh(Ax) - b )^\top \cdot \frac{ \d ( \sinh(Ax) - b ) }{\d t} \\
    = & ~ (\sinh(Ax) - b )^\top \cdot ( \cosh(Ax) \circ \frac{A \d x}{ \d t } )
\end{align*}
where the first step follows from the differential chain rule, and the second step follows from $\frac{ \d ( \sinh(Ax) - b ) }{ \d t } = \cosh(Ax) \circ \frac{A \d x }{ \d t}$ in {\bf Part 1}.



{\bf Proof of Part 3.}

We have
\begin{align*}
    \frac{\d (\sinh(Ax)-b)}{\d x_i}
    = & ~ \frac{\d (\sinh(Ax))}{\d x_i} - \frac{\d b}{\d x_i}\\
    = & ~ \cosh(Ax) \circ \frac{\d Ax}{\d x_i} - 0\\
    = & ~ \cosh(Ax) \circ A_{*,i}
\end{align*}
where the first step follows from the property of the gradient, the second step follows from the differential chain rule, and the last step directly follows from Lemma~\ref{lem:Ax_gradient_hessian}.

{\bf Proof of Part 4.}

By substitute $x_i$ into $t$ of {\bf Part 3}, we get
\begin{align*}
    \frac{\d L}{\d x_i}
    = & ~ (\sinh(Ax) - b )^\top \cdot ( \cosh(Ax) \circ \frac{A \d x}{ \d x_i } )\\
    = & ~ (\sinh(Ax) - b )^\top \cdot ( \cosh(Ax) \circ A_{*,i} ) \\
    = & ~ A_{*,i}^\top \diag( \cosh(Ax) ) (\sinh(Ax) - b )
\end{align*}
where the first step follows from the result of {\bf Part 2} and the second step follows from the result of Lemma~\ref{lem:Ax_gradient_hessian}, the last step follows from Fact~\ref{fac:circ_diag}.

{\bf Proof of Part 5.}

We have
\begin{align*}
    \frac{\d L}{ \d x}
    = & ~ A^\top \diag( \cosh(Ax) ) (\sinh(Ax) - b)
\end{align*}
where this step follows from the result of {\bf Part 4} directly.
\end{proof}

\subsection{Hessian}\label{sec:sinh:hessian}

\begin{lemma}\label{lem:hessian_sinh}
\begin{itemize}

    \item Part 1.
    \begin{align*}
        \frac{ \d^2 ( \sinh(Ax) - b ) }{ \d x_i^2 }
        = & ~ A_{*,i} \circ \sinh(Ax) \circ A_{*,i}
    \end{align*}
    \item Part 2.
    \begin{align*}
        \frac{ \d^2 ( \sinh(Ax) - b ) }{ \d x_i \d x_j }
        = & ~ A_{*,j} \circ \sinh(Ax) \circ A_{*,i}
    \end{align*}
    \item Part 3.
    \begin{align*}
        \frac{\d^2 L_{\sinh} }{ \d x_i^2}
        = & ~ A_{*,i}^\top \diag( 2\sinh(Ax) \circ \sinh(Ax) - b \circ \sinh(Ax) + {\bf 1}_n )  A_{*,i}
    \end{align*}
    \item Part 4. 
     \begin{align*}
        \frac{\d^2 L_{\sinh} }{ \d x_i \d x_j} = A_{*,i}^\top \diag( 2 \sinh(Ax) \circ \sinh(Ax) - b \circ \sinh(Ax) - {\bf 1}_n ) A_{*,j}
    \end{align*}
\end{itemize}
\end{lemma}
\begin{proof}

    
{\bf Proof of Part 1.}
\begin{align*}
    \frac{ \d^2 ( \sinh(Ax) - b ) }{ \d x_i^2 }
        = & ~ \frac{\d}{\d x_i}\bigg(\frac{\d (\sinh(Ax) - b)}{\d x_i} \bigg) \\
        = & ~ \frac{\d (\cosh(Ax) \circ A_{*,i})}{\d x_i} \\
        = & ~ A_{*,i} \circ \frac{\d \cosh(Ax)}{\d x_i} \\
        = & ~ A_{*,i} \circ \sinh(Ax) \circ A_{*,i}
\end{align*}
where the first step is an expansion of the Hessian, the second step follows from the differential chain rule, the third step extracts the matrix $A_{*,i}$ with constant entries out of the derivative, and the last step also follows from the chain rule.

{\bf Proof of Part 2.}
\begin{align*}
    \frac{ \d^2 ( \sinh(Ax) - b ) }{ \d x_i \d x_j }
        = & ~ \frac{\d}{\d x_i}\bigg(\frac{\d}{\d x_j}\bigg(\sinh(Ax) - b\bigg) \bigg) \\
        = & ~ \frac{\d}{\d x_i}\bigg(\cosh(Ax) \circ A_{*,j} \bigg) \\
        = & ~ A_{*,j} \circ \sinh(Ax) \circ A_{*,i}
\end{align*}
where the first step is an expansion of the Hessian, the second and third steps follow from the differential chain rule.

{\bf Proof of Part 3.}
\begin{align*}
    \frac{\d^2 L }{ \d x_i^2}
        = & ~ \frac{\d}{\d x_i}\bigg(\frac{\d L}{\d x_i} \bigg) \\
        = & ~ \frac{\d}{\d x_i}\bigg((\sinh(Ax) - b )^\top \cdot ( \cosh(Ax) \circ A_{*,i} ) \bigg) \\
        = & ~ (\cosh(Ax) \circ A_{*,i})^\top \cdot ( \cosh(Ax) \circ A_{*,i} )+(\sinh(Ax) - b )^\top \cdot (A_{*,i} \circ \sinh(Ax) \circ A_{*,i})\\ 
        = & ~ A_{*,i}^\top \diag( \cosh^2(Ax) + \sinh^2(Ax) - b \circ \sinh(Ax) ) A_{*,i} \\
        = & ~ A_{*,i}^\top \diag( 2\sinh(Ax) \circ \sinh(Ax) - b \circ \sinh(Ax) + {\bf 1}_n )  A_{*,i}
\end{align*}
where the first step is an expansion of the Hessian,
the second step follows from {\bf Part 4} of Lemma~\ref{lem:gradient_sinh},
the third step follows from the product rule of calculus,
the fourth step follows from Fact~\ref{fac:circ_diag},
the last step follows from Fact~\ref{fac:circ_diag}.


{\bf Proof of Part 4.}
\begin{align*}
    \frac{\d^2 L }{ \d x_i \d x_j}
        = & ~ \frac{\d}{\d x_i}\bigg(\frac{\d L}{\d x_j} \bigg) \\
        = & ~ \frac{\d}{\d x_i}\bigg((\sinh(Ax) - b )^\top \cdot ( \cosh(Ax) \circ A_{*,j} ) \bigg) \\
        = & ~ (\cosh(Ax) \circ A_{*,i})^\top \cdot ( \cosh(Ax) \circ A_{*,j} )+(\sinh(Ax) - b )^\top \cdot (A_{*,j} \circ \sinh(Ax) \circ A_{*,i}) \\
        = & ~ A_{*,i}^\top \diag( \cosh^2(Ax) + \sinh^2(Ax) - b \circ \sinh(Ax) ) A_{*,j} \\
        = & ~ A_{*,i}^\top \diag( 2\sinh(Ax) \circ \sinh(Ax) - b \circ \sinh(Ax) + {\bf 1}_n )  A_{*,j}
\end{align*}
where the first step is an expansion of the Hessian,
the second step follows from {\bf Part 4} of Lemma~\ref{lem:gradient_sinh},
the third step follows from the product rule of calculus,
the fourth step follows from Fact~\ref{fac:circ_diag},
the last step follows from Fact~\ref{fac:circ_diag}.

 
\end{proof}


\subsection{Gradient and Hessian of the Loss function for Sinh Function}\label{sec:sinh:gradient_hessian}

\begin{lemma}\label{lem:gradient_hessian_sinh}
    Let $L: \R^d \to \R_{\geq 0}$ be defined in Definition~\ref{def:L_exp}. Then for any $i, j \in [d]$, we have
    \begin{itemize}
        \item Part 1. Gradient
    \begin{align*}
        \nabla L_{\sinh} = A^\top \diag(\cosh(Ax)) \diag(\sinh(Ax) - b) {\bf 1}_n
    \end{align*}
        \item Part 2. Hessian
        \begin{align*}
            \nabla^2 L_{\sinh} = A^\top \diag( 2\sinh(Ax) \circ \sinh(Ax) - b \circ \sinh(Ax) + {\bf 1}_n ) A
        \end{align*}
    \end{itemize}
\end{lemma}
 
\begin{proof}

{\bf Part 1.}
We run Lemma~\ref{lem:gradient_sinh} and Fact~\ref{fac:circ_diag} directly.

{\bf Part 2.}
It follows from Part 5 of Lemma~\ref{lem:hessian_sinh}.
\end{proof}



\subsection{Loss Function with a Regularization Term}\label{sec:sinh:loss_reg}

\begin{definition}\label{def:L_sinh_and_regularized}
Given matrix $A \in \R^{n \times d}$ and $b \in \R^n$, $w \in \R^n$. For a vector $x \in \R^d$, we define loss function $L(x)$ as follows
\begin{align*}
L_{\sinh,\reg}(x): = 0.5 \cdot \| \sinh(Ax) - b \|_2^2+ 0.5 \cdot \| W A x \|_2^2
\end{align*}
where $W = \diag(w)$.
\end{definition}

\begin{lemma}
Let $L$ be defined as Definition~\ref{def:L_sinh_and_regularized}, then we have
\begin{itemize}
    \item Part 1. Gradient
    \begin{align*}
        \frac{\d L_{\sinh,\reg}}{\d x} = A^\top \diag(\cosh(Ax)) ( \diag(\sinh(Ax) - b) ) {\bf 1}_n + A^\top W^2 A x
    \end{align*}
    \item Part 2. Hessian
    \begin{align*} 
        \frac{\d^2 L_{\sinh,\reg}}{\d x^2} = A^\top \diag( 2\sinh(Ax) \circ \sinh(Ax) - b \circ \sinh(Ax) + {\bf 1}_n ) A +  A^\top W^2 A
    \end{align*}
\end{itemize}
\end{lemma}
\begin{proof}
{\bf Proof of Part 1.}
We run Lemma~\ref{lem:gradient_hessian_exp} and Lemma~\ref{lem:regularization} directly.

{\bf Proof of Part 2.}
We run Lemma~\ref{lem:gradient_hessian_exp} and Lemma~\ref{lem:regularization} directly.
\end{proof}
 



\subsection{Hessian is Positive Definite}\label{sec:sinh:convex}
\begin{lemma}\label{lem:hessian_is_pd_sinh}
Let $l > 0$ denote a parameter. 
 If $w_{i}^2 > 0.5 b_{i}^2 + l/\sigma_{\min}(A)^2 - 1$ for all $i \in [n]$, then
    \begin{align*}
        \frac{\d^2 L}{\d x^2} \succeq l \cdot I_d
    \end{align*}
\end{lemma}

\begin{proof}



We define $D$
\begin{align*}
D = \diag( 2\sinh(Ax) \circ \sinh(Ax) - b \circ \sinh(Ax) + {\bf 1}_n ) + W^2
\end{align*}

Then we can rewrite Hessian as 
\begin{align*}
\frac{\d^2 L}{\d x^2} = A^\top D A.
\end{align*} 

We define
\begin{align*}
z_i = \sinh( ( Ax )_i )
\end{align*}
 
Then we have
\begin{align*}
D_{i,i} 
= & ~ ( 2 \sinh^2((Ax)_i) +1 - b_i \sinh((Ax)_i) ) ) + w_{i,i}^2 \\
= & ~ ( 2z_i^2 + 1  - b_i z_i ) + w_{i}^2 \\
= & ~ 2 z_i^2 - b_i z_i + w_i^2 + 1 \\
> & ~ 2 z_i^2 - b_i z_i + 0.5 b_{i}^2 + l/\sigma_{\min}(A)^2\\
= & ~ 0.5 ( 2z_i - b_i )^2 + l/\sigma_{\min}(A)^2\\
\geq & ~ l/\sigma_{\min}(A)^2
\end{align*}
where the first step follows from simple algebra, the second step follows from replacing $\sinh(Ax)$ with $z = \sinh(Ax)$ and $\cosh^2() = \sinh^2()+1$ (Fact~\ref{fac:e_cosh_sinh_exact}), the third step follows from $w_{i}^2 > 0.5b_{i}^2 + 1/\sigma_{\min}(A)^2-1$, the fourth step follows from simple algebra, the fifth step follows from $x^2\geq0, \forall x$.
 

Since we know $D_{i,i} > l$ for all $i \in [n]$ and Lemma~\ref{lem:ADA_pd}, we have 
\begin{align*}
A^\top D A \succeq (\min_{i \in [n]} D_{i,i}) \cdot \sigma_{\min}(A)^2 I_d \succeq l \cdot I_d
\end{align*}
Thus, Hessian is positive definite forever and thus the function is convex.
\end{proof}



\subsection{Hessian is Lipschitz}\label{sec:sinh:lipschitz}
\begin{lemma}[Hessian is Lipschitz]\label{lem:hessian_is_lipschitz_sinh}
If the following condition holds
 
\begin{itemize}
    \item Let $H(x) = \frac{\d^2 L_{\sinh,\reg}}{\d x^2}$
    \item Let $R > 2$
    \item $\|x \|_2 \leq R, \| y \|_2 \leq R$
    \item $\| A (x-y) \|_{\infty} < 0.01$
    \item $\| A \| \leq R$
    \item $\| b \|_2 \leq R$
\end{itemize}
Then we have
    \begin{align*}
        \| H(x) - H(y) \| \leq \sqrt{n} \exp(6R^2) \cdot \| x- y \|_2
    \end{align*}
\end{lemma}
\begin{proof}

We have
\begin{align}\label{eq:rewrite_H_diff_sinh}
& ~ \| H(x) - H(y) \| \notag \\
= & ~ \| A^\top \diag(2 \sinh(Ax) - b) \diag(\sinh(Ax)) A -  A^\top \diag(2 \sinh(Ay) - b) \diag(\sinh(Ay)) A \| \notag \\
\leq & ~ \| A \|^2 \cdot \|  (2 \sinh(Ax) - b) \circ \sinh(Ax) - (2 \sinh(Ay) - b) \circ \sinh(Ay) \|_2  \notag \\
= & ~ \| A \|^2 \cdot \| 2 (\sinh(Ax) + \sinh(Ay) )\circ ( \sinh(Ax) - \sinh(Ay) ) - b \circ ( \sinh(Ax) - \sinh(A y) ) \|_2 \notag \\
= & ~ \| A \|^2 \cdot \| ( 2 \sinh(Ax) + 2 \sinh(Ay) - b ) \circ ( \sinh(Ax) - \sinh(Ay) ) \|_2  \notag \\
\leq & ~ \| A \|^2 \cdot \| ( 2 \sinh(Ax) + 2 \sinh(Ay) - b ) \|_{\infty} \cdot \| \sinh(Ax) - \sinh(Ay) \|_2
\end{align}
where the first step follows from $H(x) = \nabla^2L$ and simple algebra, 
the second step follows from Fact~\ref{fac:matrix_norm}, the third step follows from simple algebra, the fourth step follows from simple algebra, the last step follows from Fact~\ref{fac:vector_norm}.
 

For the first term in Eq.~\eqref{eq:rewrite_H_diff_sinh}, we have
\begin{align}\label{eq:upper_bound_H_x_H_y_step_1_sinh}
\| A \|^2 \leq R^2
\end{align}

For the second term in Eq.~\eqref{eq:rewrite_H_diff_sinh}, we have
\begin{align}\label{eq:upper_bound_H_x_H_y_step_2_sinh}
\| ( 2 \sinh(Ax) + 2 \sinh(Ay) - b ) \|_{\infty} \notag
\leq & ~ \| 2 \sinh(Ax)\|_{\infty} + \|2 \sinh(Ay)\|_{\infty} + \|b\|_\infty \notag \\
\leq & ~ \| 2 \cosh(Ax)\|_{\infty} + \|2 \cosh(Ay)\|_{\infty} + \|b\|_\infty \notag \\
\leq & ~ 2\exp(\|Ax\|_2) + 2\exp(\|Ay\|_2) + \|b\|_\infty \notag \\
\leq & ~ 4 \exp(R^2) + \| b \|_{\infty} \notag \\
\leq & ~ 4 \exp(R^2) + R \notag \\
\leq & ~ 5 \exp(R^2)
\end{align}

where the first step follows from Fact~\ref{fac:vector_norm}
, the second step follows from Fact~\ref{fac:vector_norm}, the third step follows from Fact~\ref{fac:vector_norm},  the fourth step follows from $\|Ax\|_2 \leq R^2, \|Ay\|_2 \leq R^2$, the fifth step follows from $\|b\|_\infty \leq R$, the last step follows from $R \geq 2$.



For the third term in Eq.~\eqref{eq:rewrite_H_diff_sinh}, we have
\begin{align}\label{eq:upper_bound_H_x_H_y_step_3_sinh}
\| \sinh(Ax) - \sinh(Ay) \|_2 
\leq & ~ \| \cosh(Ax) \|_2 \cdot 2 \| A (y-x) \|_{\infty} \notag \\
\leq & ~ \sqrt{n}\| \cosh(Ax) \|_\infty \cdot 2 \| A (y-x) \|_{\infty} \notag \\
\leq & ~ \sqrt{n}\exp(\|Ax\|_2) \cdot 2 \| A (y-x) \|_{2} \notag \\
\leq & ~ \sqrt{n}\exp(R^2)  \cdot 2 \| A (y-x) \|_2 \notag \\
\leq & ~ \sqrt{n}\exp(R^2)  \cdot 2 \| A \| \cdot \| y - x \|_2 \notag\\
\leq & ~ 2 \sqrt{n}R \exp(R^2) \cdot \|y - x\|_2
\end{align}
where the first step follows  from $\| A (y-x) \|_{\infty} < 0.01$ and Fact~\ref{fac:vector_norm}, the second step follows from Fact~\ref{fac:vector_norm}, the third step follows from Fact~\ref{fac:vector_norm},   the fourth step follows from $\|Ax\|_2 \leq R^2$, the fifth step follows from Fact~\ref{fac:matrix_norm}, the last step follows from $\|A\| \leq R$.
 


Putting it all together, we have
\begin{align*}
 \| H(x) - H(y) \| 
 \leq & ~ R^2 \cdot 5 \exp(R^2) \cdot 2 \sqrt{n} R\exp(R^2) \| y - x \|_2 \\
= & ~ 10 \sqrt{n} R^3 \exp(2R^2) \cdot \| y - x \|_2 \\
 \leq & ~ \sqrt{n}\exp(4R^2) \cdot  \exp(2R^2) \cdot \| y - x \|_2 \\
 = & ~ \sqrt{n}\exp(6R^2) \cdot \| y - x \|_2
\end{align*}
where the first step follows from by applying Eq.~\eqref{eq:upper_bound_H_x_H_y_step_1_sinh}, Eq.~\eqref{eq:upper_bound_H_x_H_y_step_2_sinh}, and Eq.~\eqref{eq:upper_bound_H_x_H_y_step_3_sinh}, the second step follows from simple algebra, the third step follows from $R \geq 2$, the last step follows from simple algebra.

\end{proof}