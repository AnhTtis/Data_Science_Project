\section{Introduction}

 





State-of-the-art language models like Transformer \cite{vsp+17}, BERT \cite{dclt18}, GPT-3 \cite{bmr+20}, PaLM \cite{cnd+22}, and OPT \cite{zrg+22} exhibit greater proficiency in natural language processing when compared to smaller models or traditional techniques. These models have the capacity to understand and generate intricate language, proving beneficial in various applications such as language translation, sentiment analysis, and question answering. LLMs can be customized for multiple purposes without necessitating their reconstruction from scratch. An instance of this is ChatGPT, an OpenAI-developed chat software that employs GPT-3's full potential. The latest iteration,  GPT-4 \cite{openai23},  has the potential to surpass GPT-3 in its impressive capabilities, including text generation, question answering, and language translation. This development could lead to significant implications in the field of NLP, with new applications potentially emerging in areas such as virtual assistants, chatbots, and automatic content creation. However, even though deep learning has a swift incline in popularity, we hold the belief that there exist discrepancies in our comprehension of the concept of attention and the reasoning behind its effectiveness.


The primary technical foundation behind LLMs is the attention matrix \cite{vsp+17,rns+18,dclt18,bmr+20,as23,zhdk23}. An attention matrix is a matrix that features rows and columns aligning with individual words or "tokens" and their relationships within a given text. Its purpose is to measure the critical nature of each token in a sequence in relation to the intended output. The attention matrix is learned during training. These parameters are optimized to maximize the model's accuracy in predicting the desired output. Through the attention mechanism, each input token is evaluated based on its importance or relevance to the desired output. This is achieved by weighing the token score, which is based on a similarity function comparing the current output state and input states.

More formally, the attention matrix can be expressed by considering two matrices,  $Q$ and $K$, containing query and key tokens, respectively. Both $Q$ and $K$ hold values in the $n \times d$ dimensional space. The attention matrix can be denoted by the square matrix $A$ which is of size $n \times n$. This matrix establishes a relationship between the input tokens in the sequence where every entry represents the attention weight or score between a particular input token (query token $Q$) and an output token (key token $K$). It is essential to mention that diagonal entries of this matrix display self-attention scores, signifying the importance of each token with respect to itself. A majority of the methods used for effective computation of attention matrices are divided into two primary categories based on their approach. One approach involves leveraging sparsity, as seen in Reformer \cite{kkll20}, while the other involves utilizing the low-rank attributes of the attention matrices, as observed in Linformer \cite{wlk+20} and Performer \cite{cld+20}. 

During training, our primary goal is to tackle the issue of multiple attention regression by utilizing the exponential function and its corresponding equation: $\min_{X} \|D^{-1} \exp(AX) - B \|_2$, where $D^{-1}$ denotes the normalization factor.
 However, upon further investigation, it has been brought to our attention that the single regression scenario has not been thoroughly studied. As a result, this study is centered on the situation of single regression. 

In our setting, the presence of $D$ is unnecessary due to the fact that $Ax$ is a column vector (but not a matrix). Thus, we have opted to address the optimization problem with regards to $\min_x \|\exp(Ax) - b\|_2$. It is important to note that our approach is not exclusive to the exponential function, and can also be extended to other hyperbolic functions such as $\cosh$ and $\sinh$.\footnote{$\cosh(x):=\frac{1}{2}(e^x + e^{-x})$ and $\sinh(x):=\frac{1}{2}(e^x - e^{-x})$}



\subsection{Our Results}
We state our result as follows.
\begin{theorem}[Main result, Informal version of Theorem~\ref{thm:main_formal}]\label{thm:main_informal}
Given matrix $A \in \R^{n \times d}$, $b \in \R^n$, and $w \in \R^n$. 

Let $f$ be any of functions $\exp, \cosh$ and $\sinh$. Let $g$ denote the gradient of function $f$.

Let $x^*$ denote the optimal solution of 
\begin{align*}
\min_{x \in \R^d} 0.5 \| f(Ax) - b \|_2^2 + 0.5 \| \diag(w) A x \|_2^2
\end{align*}
that $g(x^*) = {\bf 0}_d$  
and $\| x^* \|_2 \leq R$.

Let $\| A \| \leq R, \| b \|_2 \leq R$. Let $w_{i}^2 > 0.5b_{i}^2 + 1 + l/\sigma_{\min}(A)^2$ for all $i \in [n]$.

% \sqrt{n} 
Let $M = \exp(6 (R^2 +\log n))$.

Let $x_0$ denote an initial point such that $M \| x_0 - x^* \|_2 \leq 0.1 l$.

For any accuracy parameter $\epsilon \in (0,0.1)$ and failure probability $\delta \in (0,0.1)$.  There is a randomized algorithm (Algorithm~\ref{alg:main}) that runs $\log(\| x_0 - x^* \|_2/ \epsilon)$ iterations and spend 
\begin{align*}
O( (\nnz(A) + d^{\omega} ) \cdot \poly(\log(n/\delta)) 
\end{align*}
time per iteration, and finally outputs a vector $\wt{x} \in \R^d$ such that
\begin{align*}
\| \wt{x} - x^* \|_2 \leq \epsilon
\end{align*}
holds with probability at least $1-\delta$.
\end{theorem}




\begin{algorithm}[!ht]\caption{}\label{alg:main}
\begin{algorithmic}[1]
\Procedure{FastAlgorithm}{$A \in \R^{n \times d}, b \in \R^n, w \in \R^{n}, \epsilon \in (0,0.1), \delta \in (0,0.1)$} \Comment{Theorem~\ref{thm:main_informal}}
    \State Pick an initialization point $x_0$
    \State $T \gets \log( \| x_0 - x^* \|_2 / \epsilon )$
    \For{$t=0 \to T$}
        \State $D \gets \diag( (2 \exp( A x_{t} ) - b) \circ \exp( A x_{t} ) - w \circ w)$
        \State $\wt{D} \gets \textsc{SubSample}(D,A,\epsilon_1 = \Theta(1), \delta_1 = \delta/T)$ \Comment{Lemma~\ref{lem:subsample}}
        \State $g \gets A^\top \diag(\exp(Ax) \circ (\exp(Ax) - b) ) {\bf 1}_n$
        \State \Comment{$H = A^\top D A$ is the exat Hessian}
        \State \Comment{$x_{t+1} \gets x_t + H^{-1} g$ is the exact update}
        \State $\wt{H} \gets A^\top \wt{D} A$ \Comment{$\wt{H}$ is an approximation of $H$}
        \State $x_{t+1} \gets x_t + \wt{H}^{-1} g$ \Comment{This is an approximate update step}
    \EndFor
    \State $\wt{x}\gets x_{T+1}$
    \State \Return $\wt{x}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Related Work}


\paragraph{Input sparsity time algorithms}



Input sparsity is a term used to describe datasets that have a majority of elements that are either zero or negligible. Utilizing algorithms optimized for sparse input data enables faster processing times than those utilized in dense data algorithms. This is because these algorithms can solely focus on non-zero elements, thereby minimizing computational and memory usage. 
Sparse algorithmsâ€™ time complexity depends only on the number of non-zero elements rather than the number of total elements. Input sparsity algorithms are highly applicable within fields such as solving John Ellipsoid \cite{syyz22}, discrepancy minimization \cite{dsw22},  low-rank approximation \cite{cw13,rsw16,swz17,swz19_soda}, subspace embeddings \cite{nn13}, column subset selection \cite{swz19_neurips1,swz19_neurips2} and  least squares regression \cite{dswy19}.

\paragraph{Algorithmic Regularization}
The standard exponential regression is non-convex. We study the regularization version of the exponential regression problem which is a convex problem.
In the context of deep learning, the non-convexity of the objective function necessitates the use of regularization techniques. Due to the possibility of the objective function generating multiple global minima that are widely scattered and vary significantly in their generalization capabilities, this becomes essential. Algorithmic regularization can be observed in various machine learning applications such as   binary classification  \cite{shn+18,ll19,cb20}, matrix factorization \cite{glss18,achl19}, convolutional neural networks \cite{glss18,jrg22}, generative adversarial networks \cite{azl21}, contrastive learning \cite{wl21} and mixture of experts \cite{cdw+22}. There are numerous factors that can induce algorithmic regularization. 
\cite {gdg+17, hhs17, kmn+16,
skyl18, lwm19} introduce how learning rate and batch size help regularize the training process. \cite{lwm19} explains that while a small initial learning rate may result in prompt training and better performance initially, a larger learning rate tends to yield improved generalization shortly after the annealing of the learning rate.  
The regularizer's role in the GAN model is expounded in \cite{azl21}. In addition, \cite{jl22} has conducted a theoretical analysis of the regularization generated by momentum. Furthermore, the employment of an adaptive step-size optimizer, such as Adam optimizer, can also function as a form of regularization during the training process \cite{kb14, nss15, d17, wrs+17,zclg21, jmgl22}. Batch normalization is analyzed in a theoretical capacity in \cite{ all18, hbgs19, is15}, which delves into its impact on regularization. A separate study conducted by \cite{shk+14, wkm20} explains how introducing dropout into the training process can prevent overfitting.
In \cite{dssw18}, they apply TensorSketch to the Kronecker product of multiple matrices efficiently without explicitly computing the tensor product and also apply the regularization to solve the Kronecker product regression.



\paragraph{Attention Theory}
 The topic of attention's expressivity has been a focus of early theoretical works. 
 In the case of self-attention blocks, \cite{vbc20} interpret self-attention as a system of self-interacting particles and theoretically explain the attention. 
 \cite{egkz21} explain it from inductive biases
and variable creation perspective. The role of attention in Transformers was studied by \cite{wcm21,dgv+18}. 
 In terms of optimization, \cite{zkv+20} examined the impact of adaptive approaches on attention models, while \cite{szks21} analyzed the dynamics of single-head attention to approximate Seq2Seq architecture's learning process.
For most LLMs,  it generally suffices to conduct attention computations in an approximate manner during the inference process, provided that there are adequate assurances of accuracy. Research conducted by various sources such as \cite{sparse_transformer19,kkll20,wlk+20,dkod20,kvpf20,cdw+21,cdl+22} has underscored this perspective. Due to that motivation,  \cite{zhdk23,as23} study the  computation of the attention matrix from the hardness perspective and purpose faster algorithms.


\paragraph{Newton Method and Hessian Computation}

Computing the Hessian or approximately computing the Hessian is a standard task in convex optimization. Many of the previous have work on this direction and use that improve several optimization problems such as linear programming \cite{cls19,lsz19,b20,blss20,sy21,jswz21,dly21,b21,gs22}, empirical risk minimization \cite{lsz19,qszz23}, cutting plane method \cite{jlsw20}, semi-definite programming \cite{jkl+20,hjs+22,gs22}, sum of squares \cite{jnw22}, training over-parameterized neural network \cite{zmg19, cgh+19,bpsw21,szz21,hswz22,z22}.


\paragraph{Roadmap.}
We organize the following paper as follows. In Section~\ref{sec:preli} we provide some tools for basic algebra and the analysis of a regularization term $L_{\reg}$. In Section~\ref{sec:exp} we provide detailed analysis of the loss function based on $\exp(x)$ (denoted as $L_{\exp}$) and the loss function with a regularization term $L_{\exp,\reg}$. In Section~\ref{sec:cosh} we provide detailed analysis of $L_{\cosh}$ and $L_{\cosh,\reg}$. In Section~\ref{sec:sinh} we provide detailed analysis of $L_{\sinh}$ and $L_{\sinh,\reg}$. In Section~\ref{sec:newton} we provide an approximate version of newton method which use for solving convex optimization problem which is more efficient under certain assumptions. 