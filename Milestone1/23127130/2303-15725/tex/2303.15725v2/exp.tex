\section{Exponential Regression}\label{sec:exp}
In this section, we provide detailed analysis of $L_{\exp}$. In Section~\ref{sec:exp:definition} we define the loss function $L_{\exp}$ based on $\exp(x)$. In Section~\ref{sec:exp:gradient} we compute the gradient of $L_{\exp}$ by detail. In Section~\ref{sec:exp:hessian} we compute the hessian of $L_{\exp}$ by detail. In Section~\ref{sec:exp:gradient_hessian}, we summarize the result of Section~\ref{sec:exp:gradient} and Section~\ref{sec:exp:hessian} and aquire the gradient $\nabla L_{\exp}$ and hessian $\nabla^2 L_{\exp}$ for $L_{\exp}$. In Section~\ref{sec:exp:loss_reg} we define $L_{\exp,\reg}$ by adding the regularization term $L_{\reg}$ in Section~\ref{sec:preli:regularization} to $L_{\exp}$ and compute the gradient $\nabla L_{\exp,\reg}$ and hessian $\nabla^2 L_{\exp,\reg}$ of $L_{\exp,\reg}$. In Section~\ref{sec:exp:convex} we proved that $\nabla^2 L_{\exp,\reg} \succ 0$ and thus showed that $L_{\exp,\reg}$ is convex. In Section~\ref{sec:exp:lipschitz} we provide the upper bound for $\|\nabla^2 L_{\exp,\reg}(x)-\nabla^2 L_{\exp,\reg}(y)\|$ and thus proved $\nabla^2 L_{\exp,\reg}$ is lipschitz.
\subsection{Definitions}\label{sec:exp:definition}

\begin{definition}[Loss function for Exp Regression]\label{def:L_exp}
Given $A \in \R^{n \times d}$ and $b \in \R^n$. 
For a vector $x \in \R^d$, we define loss function $L_{\exp}(x)$ as follows
\begin{align*}
L_{\exp}(x):= 0.5 \cdot \| \exp(Ax) - b \|_2^2
\end{align*}
\end{definition}


\subsection{Gradient}\label{sec:exp:gradient}

\begin{lemma}[Gradient for Exp]\label{lem:gradient_exp}
We have
\begin{itemize}
    
    \item Part 1.
    \begin{align*}
        \frac{ \d ( \exp(Ax) - b ) }{ \d t } = \exp(Ax) \circ \frac{A \d x }{ \d t}
    \end{align*}
    \item Part 2. 
    \begin{align*}
        \frac{\d L_{\exp} }{ \d t} = (\exp(Ax) - b )^\top \cdot ( \exp(Ax) \circ \frac{A \d x}{ \d t } )
    \end{align*}
\end{itemize}
Further, we have for each $i \in [d]$
\begin{itemize}
    
    \item Part 3.
    \begin{align*}
        \frac{ \d ( \exp(Ax) - b ) }{ \d x_i } = \exp(Ax) \circ A_{*,i}
    \end{align*}
    \item Part 4. 
    \begin{align*}
        \frac{\d L_{\exp} }{ \d x_i}  
        = & ~ (\exp(Ax) - b )^\top \cdot ( \exp(Ax) \circ A_{*,i} )
    \end{align*}
    \item Part 5.
    \begin{align*}
        \frac{\d L_{\exp} }{ \d x} =  A^\top \diag( \exp(Ax) ) (\exp(Ax) - b)
    \end{align*}
\end{itemize}
\end{lemma}

\begin{proof}


{\bf Proof of Part 1.}

For each $i \in [n]$, we have  
\begin{align*}
 \frac{ \d ( \exp(Ax) - b )_i }{ \d t } 
 = & ~ \exp(Ax)_i \cdot \frac{\d (Ax)_i}{\d t} \\
 = & ~ \exp(Ax)_i \cdot \frac{ (A \d x)_i}{\d t} 
\end{align*}
where the first and second step follow from the differential chain rule.

%Thus, we complete the proof.

{\bf Proof of Part 2.}

We have
 
\begin{align*}
    \frac{\d L_{\exp} }{ \d t} 
    = & ~ (\exp(Ax) - b )^\top \cdot \frac{ \d ( \exp(Ax) - b ) }{\d t} \\
    = & ~ (\exp(Ax) - b )^\top \cdot ( \exp(Ax) \circ \frac{A \d x}{ \d t } )
\end{align*}
where the first step follows from the differential chain rule, and the second step follows from $\frac{ \d ( \exp(Ax) - b ) }{ \d t } = \exp(Ax) \circ \frac{A \d x }{ \d t}$ in {\bf Part 1}.
 


{\bf Proof of Part 3.}

We have
\begin{align*}
    \frac{\d (\exp(Ax)-b)}{\d x_i}
    = & ~ \frac{\d (\exp(Ax))}{\d x_i} - \frac{\d b}{\d x_i}\\
    = & ~ \exp(Ax) \circ \frac{\d Ax}{\d x_i} - 0\\
    = & ~ \exp(Ax) \circ A_{*,i}
\end{align*}
where the first step follows from the property of the gradient, the second step follows from simple algebra, and the last step directly follows from Lemma~\ref{lem:Ax_gradient_hessian}.
 

{\bf Proof of Part 4.}

By substitute $x_i$ into $t$ of Part 3, we get
\begin{align*}
    \frac{\d L}{\d x_i}
    = & ~ (\exp(Ax) - b )^\top \cdot ( \exp(Ax) \circ \frac{A \d x}{ \d x_i } )\\
    = & ~ (\exp(Ax) - b )^\top \cdot ( \exp(Ax) \circ A_{*,i} ) \\
    = & ~ A_{*,i}^\top \diag( \exp(Ax) ) (\exp(Ax) - b )
\end{align*}
where the first step follows from the result of {\bf Part 2} and $\frac{\d y^2}{\d x} = 2y^\top \frac{\d y}{\d x}$,
the second step follows from the result of Lemma~\ref{lem:Ax_gradient_hessian}, 
the last step follows from Fact~\ref{fac:circ_diag}.
 

{\bf Proof of Part 5.}

We have
\begin{align*}
    \frac{\d L}{ \d x}
    = & ~ A^\top \diag( \exp(Ax) ) (\exp(Ax) - b)
\end{align*}
where this step follows from the result of {\bf Part 4} directly.
\end{proof}

\subsection{Hessian}\label{sec:exp:hessian}

\begin{lemma}\label{lem:hessian_exp}
\begin{itemize}

    \item Part 1.
    \begin{align*}
        \frac{ \d^2 ( \exp(Ax) - b ) }{ \d x_i^2 }
        = & ~ A_{*,i} \circ \exp(Ax) \circ A_{*,i}
    \end{align*}
    \item Part 2.
    \begin{align*}
        \frac{ \d^2 ( \exp(Ax) - b ) }{ \d x_i \d x_j }
        = & ~ A_{*,j} \circ \exp(Ax) \circ A_{*,i}
    \end{align*}
    \item Part 3. 
    \begin{align*}
        \frac{\d^2 L_{\exp} }{ \d x_i^2}
        = & ~ A_{*,i}^\top \diag( 2 \exp(Ax) - b ) \diag(\exp(Ax)) A_{*,i}
    \end{align*}
    \item Part 4. 
     \begin{align*}
        \frac{\d^2 L_{\exp} }{ \d x_i \d x_j} = A_{*,i}^\top \diag( 2 \exp(Ax) - b ) \diag(\exp(Ax)) A_{*,j}
    \end{align*}
\end{itemize}
\end{lemma}
\begin{proof}

    
{\bf Proof of Part 1.}
\begin{align*}
    \frac{ \d^2 ( \exp(Ax) - b ) }{ \d x_i^2 }
        = & ~ \frac{\d}{\d x_i}\bigg(\frac{\d (\exp(Ax) - b)}{\d x_i} \bigg) \\
        = & ~ \frac{\d (\exp(Ax) \circ A_{*,i})}{\d x_i} \\
        = & ~ A_{*,i} \circ \frac{\d \exp(Ax)}{\d x_i} \\
        = & ~ A_{*,i} \circ \exp(Ax) \circ A_{*,i}
\end{align*}
where the first step is an expansion of the Hessian, the second step follows from the differential chain rule, the third step extracts the matrix $A_{*,i}$ with constant entries out of the derivative, and the last step also follows from the chain rule.

{\bf Proof of Part 2.}
\begin{align*}
    \frac{ \d^2 ( \exp(Ax) - b ) }{ \d x_i \d x_j }
        = & ~ \frac{\d}{\d x_i}\bigg(\frac{\d}{\d x_j}\bigg(\exp(Ax) - b\bigg) \bigg) \\
        = & ~ \frac{\d}{\d x_i}\bigg(\exp(Ax) \circ A_{*,j} \bigg) \\
        = & ~ A_{*,j} \circ \exp(Ax) \circ A_{*,i}
\end{align*}
where the first step is an expansion of the Hessian, 
the second step follows from {\bf Part 3} of Lemma~\ref{lem:gradient_exp},
the third step follows from {\bf Part 3} of Lemma~\ref{lem:gradient_exp}.


{\bf Proof of Part 3.}
\begin{align*}
    \frac{\d^2 L }{ \d x_i^2}
        = & ~ \frac{\d}{\d x_i}\bigg(\frac{\d L}{\d x_i} \bigg) \\
        = & ~ \frac{\d}{\d x_i}\bigg((\exp(Ax) - b )^\top \cdot ( \exp(Ax) \circ A_{*,i} ) \bigg) \\
        = & ~ (\exp(Ax) \circ A_{*,i})^\top \cdot ( \exp(Ax) \circ A_{*,i} )+(\exp(Ax) - b )^\top \cdot (A_{*,i} \circ \exp(Ax) \circ A_{*,i})\\ 
        = & ~ A_{*,i}^\top \diag( 2 \exp(Ax) - b ) \diag(\exp(Ax)) A_{*,i}
\end{align*}
where the first step is an expansion of the Hessian,
the second step follows from {\bf Part 4} of Lemma~\ref{lem:gradient_exp},
the third step follows from differential chain rule and {\bf Part 1} of Lemma~\ref{lem:gradient_exp},
the last step follows from Fact~\ref{fac:circ_diag}.


{\bf Proof of Part 4.}
\begin{align*}
    \frac{\d^2 L }{ \d x_i \d x_j}
        = & ~ \frac{\d}{\d x_i}\bigg(\frac{\d L}{\d x_j} \bigg) \\
        = & ~ \frac{\d}{\d x_i}\bigg((\exp(Ax) - b )^\top \cdot ( \exp(Ax) \circ A_{*,j} ) \bigg) \\
        = & ~ (\exp(Ax) \circ A_{*,i})^\top \cdot ( \exp(Ax) \circ A_{*,j} )+(\exp(Ax) - b )^\top \cdot (A_{*,j} \circ \exp(Ax) \circ A_{*,i}) \\ 
        = & ~ A_{*,i}^\top \diag( 2 \exp(Ax) - b ) \diag(\exp(Ax)) A_{*,j}
\end{align*}
where the first step is an expansion of the Hessian, 
the second step follows from {\bf Part 4} of Lemma~\ref{lem:gradient_exp},
the third step follows from differential chain rule and {\bf Part 1} of Lemma~\ref{lem:gradient_exp}, 
the last step follows from Fact~\ref{fac:circ_diag}.  

\end{proof}

\subsection{Gradient and Hessian of the Loss function for Exp Function}\label{sec:exp:gradient_hessian}

\begin{lemma}\label{lem:gradient_hessian_exp}
    Let $L_{\exp}: \R^d \to \R_{\geq 0}$ be defined in Definition~\ref{def:L_exp}. Then for any $i, j \in [d]$, we have  
    \begin{itemize}
        \item Part 1. Gradient
    \begin{align*}
        \nabla L_{\exp} = A^\top \diag(\exp(Ax)) \diag(\exp(Ax) - b) {\bf 1}_n
    \end{align*}
        \item Part 2. Hessian
        \begin{align*}
            \nabla^2 L_{\exp} = A^\top \diag(2 \exp(Ax) - b) \diag( \exp(Ax) ) A
        \end{align*}
    \end{itemize}
\end{lemma}
 
\begin{proof}

{\bf Part 1.}
We run Lemma~\ref{lem:gradient_exp} and Fact~\ref{fac:circ_diag} directly.

{\bf Part 2.}
It follows from Part 3 and 4 of Lemma~\ref{lem:hessian_exp}. 
\end{proof}


 


\subsection{Loss Function with a Regularization Term}\label{sec:exp:loss_reg}

\begin{definition}\label{def:L_exp_and_regularized}
Given matrix $A \in \R^{n \times d}$ and $b \in \R^n$, $w \in \R^n$. For a vector $x \in \R^d$, we define loss function $L_{\exp,\reg}(x)$ as follows
\begin{align*}
L_{\exp,\reg}(x): = 0.5 \cdot \| \exp(Ax) - b \|_2^2+ 0.5 \cdot \| W A x \|_2^2
\end{align*}
where $W = \diag(w)$.
\end{definition}

\begin{lemma}
Let $L_{\exp,\reg}$ be defined as Definition~\ref{def:L_exp_and_regularized}, then we have
\begin{itemize}
    \item Part 1. Gradient
    \begin{align*}
        \frac{\d L_{\exp,\reg}}{\d x} = A^\top \diag(\exp(Ax)) \diag(\exp(Ax) - b) {\bf 1}_n + A^\top W^2 A x
    \end{align*}
    \item Part 2. Hessian
    \begin{align*} 
        \frac{\d^2 L_{\exp,\reg}}{\d x^2} = A^\top \diag(2 \exp(Ax) - b) \diag(\exp(Ax)) A +  A^\top W^2 A
    \end{align*}
    
\end{itemize}
\end{lemma}
\begin{proof}
{\bf Proof of Part 1.}
We run Lemma~\ref{lem:gradient_hessian_exp} and Lemma~\ref{lem:regularization} directly.

{\bf Proof of Part 2.}
We run Lemma~\ref{lem:gradient_hessian_exp} and Lemma~\ref{lem:regularization} directly.

  


\end{proof}

\subsection{Hessian is Positive Definite}\label{sec:exp:convex}

\begin{lemma}[Hessian is positive definite]\label{lem:hessian_is_pd_exp}
Let $l > 0 $ denote a parameter.

If the following condition hold
\begin{itemize}
    \item Let $H(x) = \frac{\d^2 L_{\exp,\reg}}{\d x^2}$
    \item $w_{i}^2 > 0.5 b_{i}^2 + l/\sigma_{\min}(A)^2 $ for all $i \in [n]$
\end{itemize}
Then, we have  
    \begin{align*}
         H(x) \succeq l \cdot I_d
    \end{align*}
   
\end{lemma}
\begin{proof}
We define $D$
\begin{align*}
D = \diag(2 \exp(Ax) - b ) \diag(\exp(Ax)) + W^2
\end{align*}

Then we can rewrite Hessian as 
\begin{align*}
\frac{\d^2 L}{\d x^2} = A^\top D A.
\end{align*} 

We define
\begin{align*}
z = \exp(Ax).
\end{align*}
 
Then we have
\begin{align*}
D_{i,i} 
= & ~ 2( \exp( (Ax)_i ) - b_i ) \exp( (Ax)_i ) + w_{i,i}^2 \\
= & ~ 2 (z_i - b_i) z_i + w_{i,i}^2 \\
> & ~ 2 (z_i - b_i) z_i + 0.5 b_{i}^2 + l/\sigma_{\min}(A)^2 \\
= & ~ 0.5 ( 2z_i - b_i )^2 + l/\sigma_{\min}(A)^2 \\
\geq & ~ l/\sigma_{\min}(A)^2
\end{align*}
where the first step follows from simple algebra, the second step follows from replacing $\exp(Ax)$ with $z = \exp(Ax)$, the third step follows from $w_{i}^2 > 0.5b_{i}^2 + l/\sigma_{\min}(A)^2$, the fourth step follows from simple algebra, the fifth step follows from $x^2\geq0, \forall x$.


Since we know $D_{i,i} > l/\sigma_{\min}(A)^2$ for all $i \in [n]$ and Lemma~\ref{lem:ADA_pd}, we have 
\begin{align*}
A^\top D A \succeq (\min_{i \in [n]} D_{i,i}) \cdot \sigma_{\min}(A)^2 I_d \succeq l \cdot I_d
\end{align*}
Thus, Hessian is positive definite forever and thus the function is convex.


\end{proof}


\subsection{Hessian is Lipschitz}\label{sec:exp:lipschitz}
\begin{lemma}[Hessian is Lipschitz]\label{lem:hessian_is_lipschitz_exp}
If the following condition holds
 
\begin{itemize}
    \item Let $H(x) = \frac{\d^2 L_{\exp,\reg}}{\d x^2}$
    \item Let $R > 2$
    \item $\|x \|_2 \leq R, \| y \|_2 \leq R$
    \item $\| A (x-y) \|_{\infty} < 0.01$
    \item $\| A \| \leq R$
    \item $\| b \|_2 \leq R$
\end{itemize}
Then we have
    \begin{align*}
        \| H(x) - H(y) \| \leq \sqrt{n} \cdot \exp(6 R^2) \cdot \| x - y \|_2
    \end{align*}
\end{lemma}
\begin{proof}

We have
\begin{align}\label{eq:rewrite_H_diff_exp}
& ~ \| H(x) - H(y) \| \notag \\
= & ~ \| A^\top \diag(2 \exp(Ax) - b) \diag(\exp(Ax)) A -  A^\top \diag(2 \exp(Ay) - b) \diag(\exp(Ay)) A \| \notag \\
\leq & ~ \| A \|^2 \cdot \|  (2 \exp(Ax) - b) \circ \exp(Ax) - (2 \exp(Ay) - b) \circ \exp(Ay) \|_2  \notag \\
= & ~ \| A \|^2 \cdot \| 2 (\exp(Ax) + \exp(Ay) )\circ ( \exp(Ax) - \exp(Ay) ) - b \circ ( \exp(Ax) - \exp(A y) ) \|_2 \notag \\
= & ~ \| A \|^2 \cdot \| ( 2 \exp(Ax) + 2 \exp(Ay) - b ) \circ ( \exp(Ax) - \exp(Ay) ) \|_2  \notag \\
\leq & ~ \| A \|^2 \cdot \| ( 2 \exp(Ax) + 2 \exp(Ay) - b ) \|_{\infty} \cdot \| \exp(Ax) - \exp(Ay) \|_2
\end{align}
where the first step follows from $H(x) = \nabla^2L$ and simple algebra, the second step follows from Fact~\ref{fac:matrix_norm}, the third step follows from simple algebra, the fourth step follows from simple algebra, the last step follows from Fact~\ref{fac:vector_norm}.
 

For the first term in Eq.~\eqref{eq:rewrite_H_diff_exp}, we have
\begin{align}\label{eq:upper_bound_H_x_H_y_step_1_exp}
\| A \|^2 \leq R^2
\end{align}

For the second term in Eq.~\eqref{eq:rewrite_H_diff_exp}, we have
\begin{align}\label{eq:upper_bound_H_x_H_y_step_2_exp}
\| ( 2 \exp(Ax) + 2 \exp(Ay) - b ) \|_{\infty} \notag
\leq & ~ \| 2 \exp(Ax)\|_{\infty} + \|2 \exp(Ay)\|_{\infty} + \|b\|_\infty \notag \\
\leq & ~ 2\exp(\|Ax\|_2) + 2\exp(\|Ay\|_2) + \|b\|_\infty \notag \\
\leq & ~ 4 \exp( R^2) + \| b \|_{\infty} \notag \\
\leq & ~ 4 \exp(R^2) + R \notag \\
\leq & ~ 5 \exp(R^2)
\end{align}

where the first step follows from Fact~\ref{fac:vector_norm}
, the second step follows from Fact~\ref{fac:vector_norm}, the third step follows from $\|Ax\|_2 \leq R^2,\|Ay\|_2 \leq R^2$, the fourth step follows from $\|b\|_\infty \leq \|b\|_2 \leq R$, the last step follows from $R \geq 2$.
 


For the third term in Eq.~\eqref{eq:rewrite_H_diff_exp}, we have
\begin{align}\label{eq:upper_bound_H_x_H_y_step_3_exp}
\| \exp(Ax) - \exp(Ay) \|_2 
\leq & ~ \| \exp(Ax) \|_2 \cdot 2 \| A (y-x) \|_{\infty} \notag \\
\leq & ~ \sqrt{n} \cdot \| \exp(Ax) \|_{\infty}  \cdot 2 \| A (y-x) \|_{\infty} \notag \\
\leq & ~ \sqrt{n} \cdot \| \exp(Ax) \|_{\infty}   \cdot 2 \| A (y-x) \|_2 \notag \\
\leq & ~ \sqrt{n} \cdot \exp(\|Ax\|_2)  \cdot 2 \| A (y-x) \|_2 \notag \\
\leq & ~ \sqrt{n} \exp( R^2)  \cdot 2 \| A \| \cdot \| y - x \|_2 \notag\\
\leq & ~ 2 \sqrt{n} R \exp(R^2) \cdot \|y - x\|_2
\end{align}
where the first step follows  from $\| A (y-x) \|_{\infty} < 0.01$ and Fact~\ref{fac:vector_norm},  
the second step follows from Fact~\ref{fac:vector_norm}, 
the third step follows from Fact~\ref{fac:vector_norm}
, the fourth step follows from Fact~\ref{fac:vector_norm},
the fifth step follows from Fact~\ref{fac:matrix_norm} and $\|Ax\|_2 \leq R^2$, the last step follows from $\|A\| \leq R$.
 

Putting it all together, we have
\begin{align*}
 \| H(x) - H(y) \| 
 \leq & ~ R^2 \cdot 5 \exp(R^2) \cdot 2 \sqrt{n} R \exp(R^2) \| y - x \|_2 \\
= & ~ 10 \sqrt{n} R^3 \exp(2R^2) \cdot \| y - x \|_2 \\
 \leq & ~ \sqrt{n} \exp(4R^2) \cdot  \exp(2R^2) \cdot \| y - x \|_2 \\
 = & ~ \sqrt{n} \exp(6R^2) \cdot \| y - x \|_2
\end{align*}
where the first step follows from by applying Eq.~\eqref{eq:upper_bound_H_x_H_y_step_1_exp}, Eq.~\eqref{eq:upper_bound_H_x_H_y_step_2_exp}, and Eq.~\eqref{eq:upper_bound_H_x_H_y_step_3_exp}, the second step follows from simple algebra, the third step follows from $R \geq 2$, the last step follows from simple algebra.

\end{proof}