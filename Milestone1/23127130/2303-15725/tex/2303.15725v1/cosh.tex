\section{Cosh Regression}\label{sec:cosh}
 
In this section, we provide detailed analysis of $L_{\cosh}$. In Section~\ref{sec:cosh:definition} we define the loss function $L_{\cosh}$ based on $\cosh(x)$. In Section~\ref{sec:cosh:gradient} we compute the gradient of $L_{\cosh}$ by detail. In Section~\ref{sec:cosh:hessian} we compute the hessian of $L_{\cosh}$ by detail. In Section~\ref{sec:cosh:gradient_hessian}, we summarize the result of Section~\ref{sec:cosh:gradient} and Section~\ref{sec:cosh:hessian} and aquire the gradient $\nabla L_{\cosh}$ and hessian $\nabla^2 L_{\cosh}$ for $L_{\cosh}$. In Section~\ref{sec:cosh:loss_and_reg} we define $L_{\cosh,\reg}$ by adding the regularization term $L_{\reg}$ in Section~\ref{sec:preli:regularization} to $L_{\cosh}$ and compute the gradient $\nabla L_{\cosh,\reg}$ and hessian $\nabla^2 L_{\cosh,\reg}$ of $L_{\cosh,\reg}$. In Section~\ref{sec:cosh:convex} we proved that $\nabla^2 L_{\cosh,\reg} \succ 0$ and thus showed that $L_{\cosh,\reg}$ is convex. In Section~\ref{sec:cosh:lipschitz} we provide the upper bound for $\|\nabla^2 L_{\cosh,\reg}(x)-\nabla^2 L_{\cosh,\reg}(y)\|$ and thus proved $\nabla^2 L_{\cosh,\reg}$ is lipschitz.
\subsection{Definition}\label{sec:cosh:definition}

\begin{definition}\label{def:L_cosh}
Given $A \in \R^{n \times d}$ and $b \in \R^n$. For a vector $x \in \R^d$, we define loss function $L_{\cosh}(x)$ as follows:
\begin{align*}
L_{\cosh}(x) := 0.5 \cdot \| \cosh(Ax) - b \|_2^2
\end{align*}
\end{definition}


\subsection{Gradient}\label{sec:cosh:gradient}

\begin{lemma}[Gradient for Cosh]\label{lem:gradient_cosh}
We have
\begin{itemize}
    
    \item Part 1.
    \begin{align*}
        \frac{ \d ( \cosh(Ax) - b ) }{ \d t } = \sinh(Ax) \circ \frac{A \d x }{ \d t}
    \end{align*}
    \item Part 2. 
    \begin{align*}
        \frac{\d L_{\cosh} }{ \d t} = (\cosh(Ax) - b )^\top \cdot ( \sinh(Ax) \circ \frac{A \d x}{ \d t } )
    \end{align*}
\end{itemize}
Further, we have for each $i \in [d]$
\begin{itemize}
    
    \item Part 3.
    \begin{align*}
        \frac{ \d ( \cosh(Ax) - b ) }{ \d x_i } = \sinh(Ax) \circ A_{*,i}
    \end{align*}
    \item Part 4. 
    \begin{align*}
        \frac{\d L_{\cosh} }{ \d x_i}  
        = & ~ (\cosh(Ax) - b )^\top \cdot ( \sinh(Ax) \circ A_{*,i} )
    \end{align*}
    \item Part 5.
    \begin{align*}
        \frac{\d L_{\cosh} }{ \d x} =  A^\top \diag( \sinh(Ax) ) (\cosh(Ax) - b)
    \end{align*}
\end{itemize}
\end{lemma}

\begin{proof}


{\bf Proof of Part 1.}

For each $i \in [n]$, we have
\begin{align*}
 \frac{ \d ( \cosh(Ax) - b )_i }{ \d t } 
 = & ~ \sinh(Ax)_i \cdot \frac{\d (Ax)_i}{\d t} \\
 = & ~ \sinh(Ax)_i \cdot \frac{ (A \d x)_i}{\d t} 
\end{align*}
where the first and second step follow from the differential chain rule.

Thus, we complete the proof.

{\bf Proof of Part 2.}

We have
 
\begin{align*}
    \frac{\d L_{\cosh} }{ \d t} 
    = & ~ (\cosh(Ax) - b )^\top \cdot \frac{ \d ( \cosh(Ax) - b ) }{\d t} \\
    = & ~ (\cosh(Ax) - b )^\top \cdot ( \sinh(Ax) \circ \frac{A \d x}{ \d t } )
\end{align*}
where the first step follows from the differential chain rule, and the second step follows from $\frac{ \d ( \cosh(Ax) - b ) }{ \d t } = \sinh(Ax) \circ \frac{A \d x }{ \d t}$ in Part 1.



{\bf Proof of Part 3.}

We have
\begin{align*}
    \frac{\d (\cosh(Ax)-b)}{\d x_i}
    = & ~ \frac{\d (\cosh(Ax))}{\d x_i} - \frac{\d b}{\d x_i}\\
    = & ~ \sinh(Ax) \circ \frac{\d Ax}{\d x_i} - 0\\
    = & ~ \sinh(Ax) \circ A_{*,i}
\end{align*}
where the first step follows from the property of the gradient, the second step follows from the differential chain rule, and the last step directly follows from Lemma~\ref{lem:Ax_gradient_hessian}.

{\bf Proof of Part 4.}

By substitute $x_i$ into $t$ of Part 3, we get
\begin{align*}
    \frac{\d L}{\d x_i}
    = & ~ (\cosh(Ax) - b )^\top \cdot ( \sinh(Ax) \circ \frac{A \d x}{ \d x_i } )\\
    = & ~ (\cosh(Ax) - b )^\top \cdot ( \sinh(Ax) \circ A_{*,i} ) \\
    = & ~ A_{*,i}^\top \diag( \sinh(Ax) ) (\cosh(Ax) - b )
\end{align*}
where the first step follows from the result of Part 2 and the second step follows from the result of Lemma~\ref{lem:Ax_gradient_hessian}, the last step follows from Fact~\ref{fac:circ_diag}.

{\bf Proof of Part 5.}

We have
\begin{align*}
    \frac{\d L}{ \d x}
    = & ~ A^\top \diag( \sinh(Ax) ) (\cosh(Ax) - b)
\end{align*}
where this step follows from the result of Part 4 directly.
\end{proof}

\subsection{Hessian}\label{sec:cosh:hessian}

\begin{lemma}\label{lem:hessian_cosh}
\begin{itemize}

    \item Part 1.
    \begin{align*}
        \frac{ \d^2 ( \cosh(Ax) - b ) }{ \d x_i^2 }
        = & ~ A_{*,i} \circ \cosh(Ax) \circ A_{*,i}
    \end{align*}
    \item Part 2.
    \begin{align*}
        \frac{ \d^2 ( \cosh(Ax) - b ) }{ \d x_i \d x_j }
        = & ~ A_{*,j} \circ \cosh(Ax) \circ A_{*,i}
    \end{align*}
    \item Part 3. 
    \begin{align*}
        \frac{\d^2 L_{\cosh} }{ \d x_i^2}
        = & ~ A_{*,i}^\top \diag( 2 \cosh(Ax) \circ \cosh(Ax) - b \circ \cosh(Ax) - {\bf 1}_n )  A_{*,i}
    \end{align*}
    \item Part 4. 
     \begin{align*}
        \frac{\d^2 L_{\cosh} }{ \d x_i \d x_j} = A_{*,i}^\top \diag( \sinh^2(Ax) + \cosh^2(Ax) - b \circ \cosh(Ax) ) A_{*,j}
    \end{align*}
\end{itemize}
\end{lemma}
\begin{proof}

    
{\bf Proof of Part 1.}
\begin{align*}
    \frac{ \d^2 ( \cosh(Ax) - b ) }{ \d x_i^2 }
        = & ~ \frac{\d}{\d x_i}\bigg(\frac{\d (\cosh(Ax) - b)}{\d x_i} \bigg) \\
        = & ~ \frac{\d (\sinh(Ax) \circ A_{*,i})}{\d x_i} \\
        = & ~ A_{*,i} \circ \frac{\d \sinh(Ax)}{\d x_i} \\
        = & ~ A_{*,i} \circ \cosh(Ax) \circ A_{*,i}
\end{align*}
where the first step is an expansion of the Hessian, the second step follows from the differential chain rule, the third step extracts the matrix $A_{*,i}$ with constant entries out of the derivative, and the last step also follows from the chain rule.

{\bf Proof of Part 2.}
\begin{align*}
    \frac{ \d^2 ( \cosh(Ax) - b ) }{ \d x_i \d x_j }
        = & ~ \frac{\d}{\d x_i}\bigg(\frac{\d}{\d x_j}\bigg(\cosh(Ax) - b\bigg) \bigg) \\
        = & ~ \frac{\d}{\d x_i}\bigg(\sinh(Ax) \circ A_{*,j} \bigg) \\
        = & ~ A_{*,j} \circ \cosh(Ax) \circ A_{*,i}
\end{align*}
where the first step is an expansion of the Hessian, the second and third steps follow from the differential chain rule.

{\bf Proof of Part 3.}

Here in the proof, for simplicity, we let use $\cosh^2(Ax) = \cosh(Ax) \circ \cosh(Ax)$.

We have
\begin{align*}
    \frac{\d^2 L }{ \d x_i^2}
        = & ~ \frac{\d}{\d x_i}\bigg(\frac{\d L}{\d x_i} \bigg) \\
        = & ~ \frac{\d}{\d x_i}\bigg((\cosh(Ax) - b )^\top \cdot ( \sinh(Ax) \circ A_{*,i} ) \bigg) \\
        = & ~ (\sinh(Ax) \circ A_{*,i})^\top \cdot ( \sinh(Ax) \circ A_{*,i} )+(\cosh(Ax) - b )^\top \cdot (A_{*,i} \circ \cosh(Ax) \circ A_{*,i}) \\ 
        = & ~ A_{*,i}^\top \diag( \sinh(Ax) \circ \sinh(Ax) + \cosh(Ax) \circ \cosh(Ax) - b \circ \cosh(Ax) ) A_{*,i} \\
        = & ~ A_{*,i}^\top \diag( 2 \cosh(Ax) \circ \cosh(Ax) - b \circ \cosh(Ax) - {\bf 1}_n ) A_{*,i}
\end{align*}
where the first step is an expansion of the Hessian, and the third step follows from the product rule of calculus, the last step follows from Fact~\ref{fac:vector_norm}.

{\bf Proof of Part 4.}
\begin{align*}
    \frac{\d^2 L }{ \d x_i \d x_j}
        = & ~ \frac{\d}{\d x_i}\bigg(\frac{\d L}{\d x_j} \bigg) \\
        = & ~ \frac{\d}{\d x_i}\bigg((\cosh(Ax) - b )^\top \cdot ( \sinh(Ax) \circ A_{*,j} ) \bigg) \\
        = & ~ (\sinh(Ax) \circ A_{*,i})^\top \cdot ( \sinh(Ax) \circ A_{*,j} )+(\cosh(Ax) - b )^\top \cdot (A_{*,j} \circ \cosh(Ax) \circ A_{*,i}) \\
        = & ~ A_{*,i}^\top \diag( \sinh(Ax) \circ \sinh(Ax) + \cosh(Ax) \circ \cosh(Ax) - b \circ \cosh(Ax) ) A_{*,j} \\
        = & ~ A_{*,i}^\top \diag( 2 \cosh(Ax) \circ \cosh(Ax) - b \circ \cosh(Ax) - {\bf 1}_n ) A_{*,j}
\end{align*}
where the first step is an expansion of the Hessian,  and the third step follows from the product rule of calculus.  
\end{proof}


\subsection{Gradient and Hessian of the Loss function for Cosh Function}\label{sec:cosh:gradient_hessian}

\begin{lemma}\label{lem:gradient_hessian_cosh}
    Let $L: \R^d \to \R_{\geq 0}$ be defined in Definition~\ref{def:L_exp}. Then for any $i, j \in [d]$, we have
    \begin{itemize}
        \item Part 1. Gradient
    \begin{align*}
        \nabla L_{\cosh} = A^\top \diag(\sinh(Ax)) \diag(\cosh(Ax) - b) {\bf 1}_n
    \end{align*}
        \item Part 2. Hessian
        \begin{align*}
            \nabla^2 L_{\cosh} = A^\top \diag( 2 \cosh(Ax) \circ \cosh(Ax) - b \circ \cosh(Ax) ) A
        \end{align*}
    \end{itemize}
\end{lemma}
 
\begin{proof}

{\bf Part 1.}
We run Lemma~\ref{lem:gradient_cosh} and Fact~\ref{fac:circ_diag} directly.

{\bf Part 2.}
It follows from Part 5 of Lemma~\ref{lem:hessian_cosh}.
\end{proof}



\subsection{Loss Function with a Regularization Term}\label{sec:cosh:loss_and_reg}

\begin{definition}\label{def:L_cosh_and_regularized}
Given matrix $A \in \R^{n \times d}$ and $b \in \R^n$, $w \in \R^n$. For a vector $x \in \R^d$, we define loss function $L(x)$ as follows
\begin{align*}
L_{\cosh,\reg}(x): = 0.5 \cdot \| \cosh(Ax) - b \|_2^2+ 0.5 \cdot \| W A x \|_2^2
\end{align*}
where $W = \diag(w)$.
\end{definition}

\begin{lemma}
Let $L$ be defined as Definition~\ref{def:L_cosh_and_regularized}, then we have
\begin{itemize}
    \item Part 1. Gradient
    \begin{align*}
        \frac{\d L_{\cosh,\reg}}{\d x} = A^\top \diag(\sinh(Ax)) ( \diag(\cosh(Ax) - b) ) {\bf 1}_n + A^\top W^2 A x
    \end{align*}
    \item Part 2. Hessian
    \begin{align*} 
        \frac{\d^2 L_{\cosh,\reg}}{\d x^2} = A^\top \diag( 2  \cosh(Ax) \circ \cosh(Ax) - b \circ \cosh(Ax) ) A +  A^\top W^2 A
    \end{align*}
    
\end{itemize}
\end{lemma}
\begin{proof}
{\bf Proof of Part 1.}
We run Lemma~\ref{lem:gradient_hessian_exp} and Lemma~\ref{lem:regularization} directly.

{\bf Proof of Part 2.}
We run Lemma~\ref{lem:gradient_hessian_exp} and Lemma~\ref{lem:regularization} directly.

 


\end{proof}


\subsection{Hessian is Positive Definite}\label{sec:cosh:convex}

\begin{lemma}\label{lem:hessian_is_pd_cosh}
 If $w_{i}^2 > 0.5 b_{i}^2 + l/\sigma_{\min}(A)^2 + 1$ for all $i \in [n]$, then 
    \begin{align*}
        \frac{\d^2 L}{\d x^2} \succeq l \cdot I_d
    \end{align*}
\end{lemma}

\begin{proof}
We define diagonal matrix $D \in \R^{n \times n}$
\begin{align*}
D = \diag( \sinh^2(Ax) + \cosh^2(Ax) - b \circ \cosh(Ax) ) + W^2
\end{align*}

Then we can rewrite Hessian as 
\begin{align*}
\frac{\d^2 L}{\d x^2} = A^\top D A.
\end{align*} 

 
Then we have
\begin{align*}
D_{i,i} 
= & ~ ( \sinh^2( (Ax)_i ) + \cosh^2((Ax)_i) - b_i \cosh^2((Ax)_i) ) ) + w_{i,i}^2 \\
= & ~ ( z_i^2 - 1 + z_i^2 - b_i z_i ) + w_{i}^2 \\
= & ~ 2 z_i^2 - b_i z_i + w_i^2 -1 \\
> & ~ 2 z_i^2 - b_i z_i + 0.5 b_{i}^2 + l/\sigma_{\min}(A)^2 \\
= & ~ 0.5 ( 2z_i - b_i )^2 + l/\sigma_{\min}(A)^2 \\
\geq & ~ l/\sigma_{\min}(A)^2
\end{align*}
where the first step follows from simple algebra, the second step follows from replacing $\cosh(Ax)$ with $z = \cosh(Ax)$ and $\sinh^2() = \cosh^2()-1$ (Fact~\ref{fac:e_cosh_sinh_exact}), the third step follows from $w_{i}^2 > 0.5b_{i}^2 + l/\sigma_{\min}(A)^2 + 1$, the fourth step follows from simple algebra, the fifth step follows from $x^2\geq0, \forall x$.


Since we know $D_{i,i} > 0$ for all $i \in [n]$ and Lemma~\ref{lem:ADA_pd}, we have 
\begin{align*}
A^\top D A \succeq (\min_{i \in [n]} D_{i,i}) \cdot \sigma_{\min}(A)^2 I_d \succeq l \cdot I_d
\end{align*}
Thus, Hessian is positive definite forever and thus the function is convex.
\end{proof}


\subsection{Hessian is Lipschitz}\label{sec:cosh:lipschitz}
\begin{lemma}[Hessian is Lipschitz]\label{lem:hessian_is_lipschitz_cosh}
If the following condition holds
 
\begin{itemize}
    \item Let $H(x) = \frac{\d^2 L_{\cosh,\reg}}{\d x^2}$
    \item Let $R > 2$
    \item $\|x \|_2 \leq R, \| y \|_2 \leq R$
    \item $\| A (x-y) \|_{\infty} < 0.01$
    \item $\| A \| \leq R$
    \item $\| b \|_2 \leq R$
\end{itemize}
Then we have
    \begin{align*}
        \| H(x) - H(y) \| \leq \exp(6R^2) \cdot \| x- y \|_2
    \end{align*}
\end{lemma}
\begin{proof}

We have
\begin{align}\label{eq:rewrite_H_diff_cosh}
& ~ \| H(x) - H(y) \| \notag \\
= & ~ \| A^\top \diag(2 \cosh(Ax) - b) \diag(\cosh(Ax)) A -  A^\top \diag(2 \cosh(Ay) - b) \diag(\cosh(Ay)) A \| \notag \\
\leq & ~ \| A \|^2 \cdot \|  (2 \cosh(Ax) - b) \circ \cosh(Ax) - (2 \cosh(Ay) - b) \circ \cosh(Ay) \|_2  \notag \\
= & ~ \| A \|^2 \cdot \| 2 (\cosh(Ax) + \cosh(Ay) )\circ ( \cosh(Ax) - \cosh(Ay) ) - b \circ ( \cosh(Ax) - \cosh(A y) ) \|_2 \notag \\
= & ~ \| A \|^2 \cdot \| ( 2 \cosh(Ax) + 2 \cosh(Ay) - b ) \circ ( \cosh(Ax) - \cosh(Ay) ) \|_2  \notag \\
\leq & ~ \| A \|^2 \cdot \| ( 2 \cosh(Ax) + 2 \cosh(Ay) - b ) \|_{\infty} \cdot \| \cosh(Ax) - \cosh(Ay) \|_2
\end{align}
where the first step follows from $H(x) = \nabla^2L$ and simple algebra, 
the second step follows from Fact~\ref{fac:matrix_norm}, the third step follows from simple algebra, the fourth step follows from simple algebra, the last step follows from Fact~\ref{fac:vector_norm}.
 

For the first term in Eq.~\eqref{eq:rewrite_H_diff_cosh}, we have
\begin{align}\label{eq:upper_bound_H_x_H_y_step_1_cosh}
\| A \|^2 \leq R^2
\end{align}

For the second term in Eq.~\eqref{eq:rewrite_H_diff_cosh}, we have
\begin{align}\label{eq:upper_bound_H_x_H_y_step_2_cosh}
\| ( 2 \cosh(Ax) + 2 \cosh(Ay) - b ) \|_{\infty} \notag
\leq & ~ \| 2 \cosh(Ax)\|_{\infty} + \|2 \cosh(Ay)\|_{\infty} + \|b\|_\infty \notag \\
\leq & ~ \| 2 \cosh(Ax)\|_2 + \|2 \cosh(Ay)\|_2 + \|b\|_\infty \notag \\
\leq & ~ 2\exp(\|Ax\|_2) + 2\exp(\|Ay\|_2) + \|b\|_\infty \notag \\
\leq & ~ 4 \exp(\| A \| R) + \| b \|_{\infty} \notag \\
\leq & ~ 4 \exp(R^2) + R \notag \\
\leq & ~ 5 \exp(R^2)
\end{align}

where the first step follows from Fact~\ref{fac:vector_norm}
, the second step follows from Fact~\ref{fac:vector_norm}, the third step follows from Fact~\ref{fac:vector_norm}, the fourth step follows from $\|x\|_2 \leq R,\|y\|_2 \leq R$ and Fact~\ref{fac:vector_norm}, the fifth step follows from $\|b\|_2 \leq R$, the last step follows from $R \geq 2$.
 


For the third term in Eq.~\eqref{eq:rewrite_H_diff_cosh}, we have
\begin{align}\label{eq:upper_bound_H_x_H_y_step_3_cosh}
\| \cosh(Ax) - \cosh(Ay) \|_2 
\leq & ~ \| \cosh(Ax) \|_2 \cdot 2 \| A (y-x) \|_{\infty} \notag \\
\leq & ~ \exp(\| A \| R)  \cdot 2 \| A (y-x) \|_{\infty} \notag \\
\leq & ~ \exp(\| A \| R)  \cdot 2 \| A (y-x) \|_2 \notag \\
\leq & ~ \exp(\| A \| R)  \cdot 2 \| A \| \cdot \| y - x \|_2 \notag\\
\leq & ~ 2 R \exp(R^2) \cdot \|y - x\|_2
\end{align}
where the first step follows  from $\| A (y-x) \|_{\infty} < 0.01$ and Fact~\ref{fac:vector_norm}, the second step follows from Fact~\ref{fac:vector_norm}, the third step follows from Fact~\ref{fac:vector_norm},  the forth step follows from Fact~\ref{fac:matrix_norm}, the fifth step follows from $\|A\| \leq R$.
 
 

Putting it all together, we have
\begin{align*}
 \| H(x) - H(y) \| 
 \leq & ~ R^2 \cdot 5 \exp(R^2) \cdot 2 \exp(R^2) \| y - x \|_2 \\
= & ~ 10 R^2 \exp(2R^2) \cdot \| y - x \|_2 \\
 \leq & ~ \exp(4R^2) \cdot  \exp(2R^2) \cdot \| y - x \|_2 \\
 = & ~ \exp(6R^2) \cdot \| y - x \|_2
\end{align*}
where the first step follows from by applying Eq.~\eqref{eq:upper_bound_H_x_H_y_step_1_cosh}, Eq.~\eqref{eq:upper_bound_H_x_H_y_step_2_cosh}, and Eq.~\eqref{eq:upper_bound_H_x_H_y_step_3_cosh}, the second step follows from simple algebra, the third step follows from $R \geq 2$, the last step follows from simple algebra.

\end{proof}