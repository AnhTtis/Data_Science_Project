\section{Preliminary}\label{sec:preli}
 
In this section, we provide preliminaries to be used in our paper. In Section~\ref{sec:preli:notations} we introduce notations we use. In Section~\ref{sec:preli:basic} we provide some facts about approximate computations and exact computations. In Section~\ref{sec:preli:gradient_hessian}, we provide some trivial facts regarding gradient and hessian. In Section~\ref{sec:preli:regularization}, we computed the gradient and hessian of a regularization term $L_{\reg}$.
\subsection{Notations}\label{sec:preli:notations}
We use $\R$ to denote real. We use $\R_{\geq 0}$ to denote non-negative real numbers.

For any vector $x \in \R^n$, we use $\exp(x) \in \R^n$ to denote a vector where $i$-th entry is $\exp(x_i)$. 

For a vector $x \in \R^n$, we use $\| x \|_2$ to denote its $\ell_2$ norm, i.e., $\| x \|_2 := ( \sum_{i=1}^n x_i^2 )^{1/2}$.

For any matrix $A \in \R^{n \times k}$, we denote the spectral norm of $A$ by $\| A \|$, i.e., 
\begin{align*}
\| A \| := \sup_{x\in\R^k} \| A x \|_2 / \| x \|_2.
\end{align*}

For a matrix $A$, we use $\sigma_{\max}(A)$ to denote the largest singular value of $A$. We use $\sigma_{\min}(A)$ to denote the smallest singular value of $A$.


For a vector $x \in \R^n$, we use $\| x \|_{\infty}$ to denote $\max_{i \in [n]} |x_i|$.

For each $x \in \R^n, y \in \R^n$, we use $x\circ y$ to denote a vector that has length $n$ and the $i$-th entry is $x_i y_i$ for all $i \in [n]$.

For a vector $x \in \R^n$ with each entry of $x_i$, we use $\diag(x)=A$ to generate a diagonal matrix $A \in \R^{n \times n}$ where each entry of $A$ on the diagonal is $A_{i,i}=x_i$ for all $i \in [n]$.

Given two column vectors $a, b \in \R^n$, we use $a \circ b$ to denote a column vector that $(a\circ b)_i$ is $a_ib_i$.

We use ${\bf 1}_n$ to denote a length-$n$ vector where all the entries are ones.

We say $A \succeq B$ if $x^\top  Ax \geq x^\top B x$ for all vector $x$.

We define $\cosh(x) = \frac{1}{2}( \exp(x) + \exp(-x))$ and $\sinh(x) = \frac{1}{2} ( \exp(x) - \exp(-x))$.

For any given matrix $A \in \R^{n \times d}$, we use $\nnz(A)$ to denote the number of non-zero entries of $A$, i.e., $\nnz(A) := | \{ (i,j) \in [n] \times [d] ~|~ A_{i,j} \neq 0 \} |$


For a diagonal matrix $D \in \R^{n \times n}$, we say $D$ is a $k$-sparse diagonal matrix, i.e., $k = |\{ i \in [n] ~|~ D_{i,i} \neq 0 \}|$.

For any function $f$, we use $\wt{O}(f)$ to denote $f \cdot \poly(\log f)$.



\subsection{Basic Algebras}\label{sec:preli:basic}

We state some facts which can give exact computation.
\begin{fact}[$\exp$, $\cosh$, $\sinh$ exact and approximate for general range]\label{fac:e_cosh_sinh_exact}
We have 
\begin{itemize}
    \item $\frac{\d \exp(x) }{ \d x} = \exp(x)$
    \item $\frac{\d \cosh(x)}{ \d x } = \sinh(x)$
    \item $\frac{\d \sinh(x)}{ \d x} = \cosh(x)$
    \item $\cosh^2(x) - \sinh^2(x) = 1$
    
    \item $|\exp(x)| \leq \exp(|x|)$
    \item $|\cosh(x)| = \cosh(|x|) \leq \exp(|x|)$  
    \item $|\sinh(x)| =  \sinh(|x|)$ 
    \item $\exp(x) = \sum_{i=0}^{\infty} \frac{1}{i!} x^i$
    \item $\cosh(x) = \sum_{i=0}^{\infty} \frac{1}{(2i)!} x^{2i} $
    \item $\sinh(x) = \sum_{i=0}^{\infty} \frac{1}{(2i+1)!} x^{2i+1} $
\end{itemize}
\end{fact}

We state some facts which can give a reasonable approximate computation.
\begin{fact}[$\exp$, $\cosh$, $\sinh$ approximate computation in small range]\label{fac:e_cosh_sinh_approx}
We have
\begin{itemize}
    \item For any $x$ satisfy that $ |x| \leq 0.1$, we have $|\exp(x) - 1| \leq 2|x|$
    \item For any $x$ satisfy that $|x| \leq 0.1$, we have $|\cosh(x) - 1| \leq x^2$
    \item For any $x$ satisfy that $|x| \leq 0.1$, we have $|\sinh(x) | \leq 2|x|$
    \item For any $x,y$ satisfy that $| x - y | \leq 0.1$, we have $| \exp(x) - \exp(y) | \leq \exp(x) \cdot 2 |x-y|$
    \item For any $x,y$ satisfy that $| x - y | \leq 0.1$, we have $| \cosh(x) - \cosh(y) | \leq \cosh(x) \cdot 2|x - y|$
    \item For any $x,y$ satisfy that $| x - y | \leq 0.1$, we have $| \sinh(x) - \sinh(y) | \leq \cosh(x) \cdot 2|x - y|$
\end{itemize}
\end{fact}


\begin{fact}\label{fac:circ_diag}
For any vectors $a \in \R^n, b \in \R^n$, we have 
\begin{itemize}
    \item $a \circ b = b \circ a = \diag(a) \cdot b = \diag(b) \cdot a $
    \item $a^\top (b \circ c)= a^\top \diag(b) c$
    \item $a^\top (b \circ c) = b^\top (a \circ c) = c^\top (a \circ b)$
    \item $a^\top \diag(b) c = b^\top \diag(a) c = a^\top \diag(c) b$
    \item $\diag(a) \cdot \diag(b) \cdot {\bf 1}_n = \diag(a) b$
    \item $\diag(a \circ b) = \diag(a) \diag(b)$
    \item $\diag(a) + \diag(b) = \diag(a + b) $
\end{itemize}
\end{fact}

\begin{fact}\label{fac:vector_norm}
For vectors $a,b \in \R^n$, we have
\begin{itemize}
    \item $\| a \circ b \|_2 \leq \| a \|_{\infty} \cdot \| b \|_2$
    \item $\| a \|_{\infty} \leq \| a \|_2$
    \item $\| \exp(a) \|_2 \leq \exp(\| a \|_2)$
    \item $\| \cosh(a) \|_2 \leq \cosh( \| a \|_2 ) \leq \exp( \| a \|_2 )$
    \item $\| \sinh(a) \|_2 \leq \sinh(\| a \|_2) \leq \cosh(\| a \|_2) \leq \exp(\| a \|_2)$
    \item $\cosh(a)\circ \cosh(a) - \sinh(a) \circ \sinh(a) = {\bf 1}_n$
    \item For any $\| a - b \|_{\infty} \leq 0.01$, we have $\| \exp(a) - \exp(b) \|_2 \leq \| \exp(a) \|_2 \cdot 2 \| a - b \|_{\infty}$
    \item For any $\| a - b \|_{\infty} \leq 0.01$, we have $\| \cosh(a) - \cosh(b) \|_2 \leq \| \cosh(a) \|_2 \cdot 2 \| a - b \|_{\infty}$
    \item For any $\| a - b \|_{\infty} \leq 0.01$, we have $\| \sinh(a) - \sinh(b) \|_2 \leq \| \cosh(a) \|_2 \cdot 2 \| a - b \|_{\infty}$
\end{itemize}
\end{fact}

\begin{fact}\label{fac:matrix_norm}
For matrices $A,B$, we have 
\begin{itemize}
    \item $\| A^\top \| = \| A \|$
    \item $\| A \| \geq \| B \| - \| A - B \|$
    \item $\| A + B \| \leq \| A \| + \| B \|$
    \item $\| A \cdot B \| \leq \| A \| \cdot \| B \|$ 
    \item If $A \preceq \alpha \cdot B$, then $\| A \| \leq \alpha \cdot \| B \|$
\end{itemize}
\end{fact}

\begin{fact}\label{fac:psd}
Let $\epsilon \in (0,0.1)$. 
If 
\begin{align*}
(1-\epsilon) A \preceq B \preceq (1+\epsilon) A
\end{align*}
Then
 
\begin{itemize}
    \item $-\epsilon A \preceq B- A \preceq  \epsilon A$
    \item $-\epsilon \preceq A^{-1/2} (B-A) A^{-1/2} \preceq \epsilon $
    \item $\| A^{-1/2} (B-A) A^{-1/2} \| \leq \epsilon$
    \item $\| A^{-1}  ( B - A ) \|\leq \epsilon$
    \item $(1+\epsilon)^{-1} B \preceq A \preceq (1-\epsilon)^{-1} A $
    \item $(1-2\epsilon) B \preceq A \preceq (1+2\epsilon) B $
    \item $\| B^{-1/2} (B-A) B^{-1/2} \| \leq 2\epsilon$
    \item $\| B^{-1} (B-A) \| \leq 2\epsilon$
\end{itemize}
\end{fact}
 

\subsection{Standard Gradient and Hessian Computation}\label{sec:preli:gradient_hessian}

\begin{lemma}[Standard Gradient and Hessian Computation]\label{lem:Ax_gradient_hessian}
We have
\begin{itemize}
\item Part 1. $ \frac{\d Ax}{\d t} = A \frac{\d x}{ \d t}$.
\item Part 2. $  \frac{\d Ax}{\d x_i} = A_{*,i} \in \R^n $
\item Part 3.  $ \frac{\d^2 Ax}{\d x_i^2}  =  0 $
\end{itemize}
\end{lemma}
\begin{proof}

{\bf Proof of Part 1.}

It trivially follows from chain rule.

{\bf Proof of Part 2.}

The equation takes derivative of the vector $x$ by each entry $x_i$ of itself and trivially gets the result of $A_{*,i}$.

{\bf Proof of Part 3.}
\begin{align*}
        \frac{\d^2 Ax}{\d x_i^2}
        = & ~ \frac{\d}{\d x_i}\bigg(\frac{\d Ax}{\d x_i} \bigg) \\
        = & ~ \frac{\d A_{*,i}}{\d x_i} \\
        = & ~ 0
    \end{align*}
    where the first step is an expansion of the Hessian, the second step follows from the differential chain rule, and the last step is due to the constant entries of the matrix $A_{*,i}$.

\end{proof}

\subsection{Regularization term}\label{sec:preli:regularization}

\begin{lemma}\label{lem:regularization}
Let $w \in \R^n$ denote a weight vector
Let $L_{\reg}:=0.5 \| W A x \|_2^2$. Let $W = \diag(w) \in \R^{n \times n}$ denote a diagonal matrix.

 Then we have
\begin{itemize}
\item Part 1. Gradient
\begin{align*}
\frac{\d L_{\reg}}{\d x} = A^\top W^2 A x
\end{align*}
\item Part 2. Hessian
\begin{align*}
\frac{ \d^2 L_{\reg} }{\d x^2} = A^\top W^2 A
\end{align*}
\end{itemize}
\end{lemma}
 


\begin{proof}
{\bf Proof of Part 1.}
\begin{align*}
    \frac{\d L_{\reg}}{\d x}
    = & ~ ( \frac{\d}{\d x} (WAx ) )^\top \cdot  (WAx) \\
    = & ~ A^\top W^\top \cdot WAx  \\
    = & ~ A^\top W^2 Ax
\end{align*}
where the first step follows from chain rule. 
 


{\bf Proof of Part 2.}
\begin{align*}
    \frac{ \d^2 L_{\reg} }{\d x^2}
    = & ~ \frac{\d}{\d x}\Big(\frac{ \d L_{\reg} }{\d x} \Big) \\
    = & ~ \frac{\d}{\d x}\Big(A^\top W^2 Ax \Big) \\
    = & ~ A^\top W^2 A
\end{align*}
where the first step follows from the expansion of Hessian, the second step follows from by applying the arguments in Part 1, the third step follows from simple algebra.
\end{proof}

\begin{lemma}\label{lem:ADA_pd}
Let $A \in \R^{n \times d}$.

Let $D \in \R^{n \times n}$ denote a diagonal matrix where all diagonal entries are positive. Then we have
\begin{align*}
A^\top D A  \succeq \sigma_{\min}(A)^2 \cdot \min_{i \in [n]} D_{i,i}  \cdot I_d
\end{align*}
\end{lemma}
\begin{proof}

For any $x \in \R^d$, we have
\begin{align*}
  x^\top A^\top D A x 
= & ~ \| \sqrt{D} A x \|_2^2 \\
= & ~ \sum_{i=1}^n D_{i,i} (Ax)_i^2 \\
\geq & ~ ( \min_{i \in [n]} D_{i,i} ) \cdot \sum_{i=1}^n (Ax)_i^2 \\
\geq & ~ \min_{i \in [n]} D_{i,i} \cdot \| A x \|_2^2 \\
\geq & ~ \min_{i \in [n]} D_{i,i} \cdot \sigma_{\min}(A)^2 \cdot \| x \|_2^2 \\
= & ~ x^\top \cdot ( \min_{i \in [n]} D_{i,i} \cdot \sigma_{\min}(A)^2 \cdot  I_d ) x
\end{align*} 
Thus, we complete the proof.
\end{proof}