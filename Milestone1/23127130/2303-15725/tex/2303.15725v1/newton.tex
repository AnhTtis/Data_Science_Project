\section{Newton Method}\label{sec:newton}
 
In this section, we provide an approximate version of the Newton method for solving convex optimization problem and provide detailed analysis of such method. In Section~\ref{sec:newton:definitions} we define some assumptions under which we can tackle the optimization problem efficiently. In Section~\ref{sec:newton:connection} we state a simple lemma which is useful in Section~\ref{sec:newton:shrink}.In Section~\ref{sec:newton:approximation} we provides a approximation variant for the update step of newton method for convex optimization. In Section~\ref{sec:newton:hessian_property} we provide the upper bound of $\|H(x_k)\|$.  In Section~\ref{sec:newton:shrink} we provide the upper bound for $\|r_{k+1}\|$ and thus showed that our approximate update step is effective in solving the optimization problem. In Section~\ref{sec:newton:induction} we provide a lemma that showed our update step is effective. In Section~\ref{sec:newton:main}, we prove our main result.


\subsection{Definition and Update Rule}\label{sec:newton:definitions}
Let us study the local convergence of the Newton method. Consider the problem
\begin{align*}
    \min_{x \in \R^d } f(x)
\end{align*}
under the following assumptions:
\begin{definition}\label{def:f_ass}
We have
\begin{itemize}
    \item {\bf $l$-local Minimum.} Let $l > 0$ denote a parameter. There is a vector $x^* \in \R^d$ such that
    \begin{itemize}
        \item $\nabla f(x^*) = {\bf 0}_d$.
        \item $\nabla^2 f(x^*) \succeq l \cdot I_d$.
    \end{itemize}
    \item {\bf Hessian is $M$-Lipschitz.} Let $M>0$ denote a parameter that  \begin{align*}
        \| \nabla^2 f(y) - \nabla^2 f(x) \| \leq M \cdot \| y - x \|_2 
    \end{align*}
    \item {\bf Good Initialization Point.} Let $r_0:=\| x_0 -x_*\|_2$ such that
    \begin{align*}
        r_0 M \leq 0.1 l
    \end{align*}    
\end{itemize}
\end{definition}

We define gradient and Hessian as follows
\begin{definition}[Gradient and Hessian]=
We define gradient function $g : \R^d \rightarrow \R^d$ as
\begin{align*}
    g(x) := \nabla f(x)
\end{align*}
We define the Hessian function $H : \R^d \rightarrow \R^{d \times d}$ ,
\begin{align*}
    H(x) := \nabla^2 f(x)
\end{align*}
\end{definition}

Using the $g: \R^d \rightarrow \R^d$ and $H : \R^d \rightarrow \R^{d \times d}$, we can rewrite the exact process as follows 
:
\begin{definition}[Exact update]\label{def:exact_update_variant}
\begin{align*}
    x_{k+1} = x_k - H(x_k)^{-1} \cdot g(x_k)
\end{align*}
\end{definition}
\subsection{Connection between Gradient and Hessian}\label{sec:newton:connection}

\begin{lemma}[folklore]\label{lem:integral_gradient_hessian}
Let $g$ denote the gradient function and let $H: \R^d \rightarrow \R^{d \times d}$ denote the hessian function, then for any $x,y$, we have
\begin{align*}
g(y) - g(x) = \int_0^1 H( x + \tau(y-x) ) \cdot (y-x) \d \tau
\end{align*}
\end{lemma}
\begin{proof}

We have
\begin{align*}
\int_0^1 H( x + \tau(y-x) ) \cdot (y-x) \d \tau
= & ~ g( x+ \tau(y-x) ) \big|_0^1 \\
= & ~ g(x + 1\cdot (y-x)) - g(x+ 0 \cdot(y-x) ) \\
= & ~ g(y) - g(x)
\end{align*}
\end{proof}

\subsection{Approximate of Hessian and Update Rule}\label{sec:newton:approximation}
In many optimization applications, computing $\nabla^2 f(x_k )$ or $(\nabla^2 f(x_k))^{-1}$ is quite expensive. Therefore, a natural motivation is to approximately formulate its Hessian or inverse of Hessian.

\begin{definition}[Approximate Hessian]\label{def:wt_H}
For any $H(x_k)$, we define $\wt{H}(x_k)$ to satisfy the following condition 
\begin{align*}
 (1-\epsilon_H) \cdot H(x_k) \preceq \wt{H}(x_k) \preceq (1+\epsilon_H) \cdot H(x_k) .
\end{align*}
\end{definition}

To efficiently compute $\wt{H}(x_k)$, we use a standard tool from the literature
\begin{lemma}[\cite{syyz22,dsw22}]\label{lem:subsample}
Let $\epsilon_H = 0.01$. 
Given a matrix $A \in \R^{n \times d}$, for any positive diagonal matrix $D \in \R^{n \times n}$, there is an algorithm that runs in time
\begin{align*}
O( (\nnz(A) + d^{\omega} ) \poly(\log(n/\delta)) )
\end{align*}
output a $O(d \log(n/\delta))$ sparse diagonal matrix $\wt{D} \in \R^{n \times n}$ such that 
\begin{align*}
(1- \epsilon_H) A^\top D A \preceq A^\top \wt{D} A \preceq (1+\epsilon_H) A^\top D A.
\end{align*}
\end{lemma}





\begin{definition}[Approximate update]\label{def:update_x_k+1}
We consider the following process
\begin{align*}
    \underbrace{ x_{k+1} }_{d \times 1} = \underbrace{ x_k }_{d \times 1} - \underbrace{ \wt{H}(x_k)^{-1} }_{d \times d} \cdot \underbrace{ g(x_k) }_{d \times 1}
\end{align*}
\end{definition}

 
\subsection{Property of Hessian}\label{sec:newton:hessian_property}
\begin{lemma}\label{lem:lower_bound_spectral_hessian}
If the following conditions hold
\begin{itemize}
    \item Let $f$ be function that Hessian is $M$-Lipschitz (see Definition~\ref{def:f_ass}) 
    \item Suppose the optimal solution $x^*$  satisfy that $\| H(x_*) \| \geq l$ (see Definition~\ref{def:f_ass})
    \item Let $r_k := \| x_k - x^* \|_2$
\end{itemize}
 
We have
\begin{align*}
\| H(x_k) \| \geq l - M \cdot r_k
\end{align*}
\end{lemma}
\begin{proof}
We can show that
\begin{align*}
\| H(x_k) \| \geq & ~ \| H(x_*) \| - \| H(x_*) - H(x_k) \| \\
\geq & ~ \| H(x_*) \| - M \cdot \| x_* - x_k \|_2 \\
\geq & ~ l - M r_k
\end{align*}
where the first step follows from Fact~\ref{fac:matrix_norm}, the second step follows from $f$ is a $M$-bounded function, the third step follows from $\|H(x_k)\|\geq l$.
\end{proof}



\subsection{One Step Shrinking Lemma}\label{sec:newton:shrink}
\begin{lemma}\label{lem:one_step_shrinking}
If the following condition hold
\begin{itemize}
    \item Function $f$ follows from Definition~\ref{def:f_ass}. 
    \item Let $H$ denote the Hessian of $f$
    \item Let $g$ denote the gradient of $f$
    \item Let $r_k:= \| x_k - x^* \|_2$
\end{itemize}
 Then we have
\begin{align*}
r_{k+1} \leq 2(\epsilon_H + \frac{M r_k}{ l - M r_k } ) \cdot r_k
\end{align*} 
\end{lemma}

\begin{proof}
We have
\begin{align}\label{eq:variant_of_r_k_plus_1}
    x_{k+1} - x^* 
    = & ~ x_k - x^* - \wt{H}(x_k)^{-1} \cdot g(x_k)  \notag \\
    = & ~ x_k - x^* - \wt{H}(x_k)^{-1} \cdot ( g(x_k) - g(x^*) ) \notag\\ 
    = & ~ x_k - x^* - \wt{H}(x_k)^{-1} \cdot \int_0^1 H ( x^* + \tau (x_k-x^*) ) (x_k - x^*) \d \tau \notag\\ 
    = & ~ \wt{H}(x_k)^{-1} ( \wt{H}(x_k) ( x_k - x^*) ) - \wt{H}(x_k)^{-1} \cdot \int_0^1 H ( x^* + \tau (x_k-x^*) ) (x_k - x^*) \d \tau \notag\\ 
    = & ~  \wt{H}(x_k)^{-1} \bigg(\wt{H}(x_k) - \int_0^1 H ( x^* + \tau (x_k-x^*) ) \d \tau \bigg) \cdot (x_k - x^*) \notag\\ 
    = & ~ \wt{H}(x_k)^{-1} \bigg(\int_0^1 \wt{H}(x_k) \d \tau - \int_0^1 H ( x^* + \tau (x_k-x^*) ) \d \tau \bigg) \cdot (x_k - x^*) \notag\\ 
    = & ~  \bigg(\wt{H}(x_k)^{-1} \int_0^1 (\wt{H}(x_k) - H ( x^* + \tau (x_k-x^*)) \d \tau \bigg) \cdot (x_k - x^*) \notag\\ 
    = & ~ G_k \cdot (x_k - x^*)
\end{align}
where the first step follows from Definition~\ref{def:update_x_k+1}, the second step follows from $g(x^*) = {\bf 0}_d$, the third step follows from  Lemma~\ref{lem:integral_gradient_hessian}, the forth step follows from $H^{-1} H = I$, the fifth step follows from simple algebra, the sixth step follows from simple algebra, 
 the last step follows from rewrite the equation using $G_k$ below
:
\begin{align*}
    G_k := \wt{H}(x_k)^{-1} \cdot \int_0^1 \wt{H}(x_k) - H(x^*+ \tau (x_k - x^*)) \d \tau
\end{align*}

 

Then we can bound $\| G_k \|$ as follows
\begin{align}\label{eq:upper_bound_G_k_divide}
    \| G_k \|
    = & ~ \Big\| \wt{H}(x_k)^{-1} \cdot  \int_0^1 ( \wt{H}(x_k) - H( x^* + \tau (x_k - x^*) ) ) \d \tau \Big\| \notag \\ 
    = & ~ \Big\| \wt{H}(x_k)^{-1} \cdot  \int_0^1 ( \wt{H}(x_k) - H(x_k) + H(x_k) - H( x^* + \tau (x_k - x^*) ) ) \d \tau \Big\| \notag \\
    = & ~ \Big\| \wt{H}(x_k)^{-1} \cdot  \Big(\int_0^1 ( \wt{H}(x_k) - H(x_k)) \d \tau + \int_0^1(H(x_k) - H( x^* + \tau (x_k - x^*) ) ) \d \tau \Big) \Big\| \notag \\
    \leq & ~\Big\| \wt{H}(x_k)^{-1} \cdot \int_0^1 ( \wt{H}(x_k) - H(x_k) ) \d \tau \Big\| + \Big\| \wt{H}(x_k)^{-1} \cdot \int_0^1 (H(x_k) - H( x^* + \tau (x_k - x^*) )) \d \tau \Big\|
\end{align}
where the first step follows from the definition of $G_k$, the second step follows from simple algebra, the third step follows from simple algebra, the last step follows from Fact~\ref{fac:matrix_norm}.
 

For the first term, we have

\begin{align}\label{eq:upper_bound_G_k_1st_step}
    \Big\| \wt{H}(x_k)^{-1} \cdot \int_0^1 ( \wt{H}(x_k) - H(x_k) ) \d \tau \Big\| \notag
    = & ~ \Big\| \wt{H}(x_k)^{-1} \cdot ( \wt{H}(x_k) - H(x_k) ) \int_0^1 \d \tau \Big\| \\ \notag
    = & ~ \| \wt{H}(x_k)^{-1}  ( \wt{H}(x_k) - H(x_k) ) \|  \\
    \leq & ~ 2\epsilon_H
\end{align}
where the first step follows from simple algebra, the second step follows from $\int_0^1 \d \tau = 1$, the third step follows from Fact~\ref{fac:psd}.

For the second term, we have
\begin{align}\label{eq:upper_bound_G_k_2nd_step}
    & ~\Big\| \wt{H}(x_k)^{-1} \cdot \int_0^1 (H(x_k) - H( x^* + \tau (x_k - x^*) ))  \d \tau \Big\|  \notag \\
    \leq & ~ \| \wt{H}(x_k)^{-1} \| \cdot \Big\| \int_0^1 (H(x_k) - H( x^* + \tau (x_k - x^*) ) ) \d \tau \Big\| \notag \\ 
    \leq & ~ (1+\epsilon_H) \cdot \| H(x_k)^{-1} \| \cdot \Big\| \int_0^1 (H(x_k) - H( x^* + \tau (x_k - x^*) ) )  \d \tau \Big\| \notag\\ 
    \leq & ~ (1+\epsilon_H) \cdot \| H(x_k)^{-1} \| \cdot \int_0^1 \Big\|  H(x_k) - H( x^* + \tau (x_k - x^*) )  \Big\| \d \tau \notag\\ 
    \leq & ~ (1+\epsilon_H) \cdot \| H(x_k)^{-1} \| \cdot \max_{\tau \in [0,1]} \Big\|  H(x_k) - H( x^* + \tau (x_k - x^*) )  \Big\| \notag\\ 
    \leq & ~ (1+\epsilon_H) \cdot \| H(x_k)^{-1} \| \cdot r_k M \notag\\ 
    \leq & ~ (1+\epsilon_H) \cdot (l - M r_k)^{-1} \cdot r_k M \notag\\
    \leq & ~ 2 \frac{M r_k}{l- M r_k}
\end{align}
where the first step follows from Fact~\ref{fac:matrix_norm}, the second step follows from $(1+\epsilon_H)H(x_k)<\wt{H}(x_k)$ and Fact~\ref{fac:matrix_norm}
, the third step follows from $\|\int \d \tau \| \leq \int \| \| \d \tau$, the forth step follows from $\int_0^1 f(\tau) \d \tau \leq \max_{\tau \in [0,1]} f(\tau)$
, the fifth step follows from Definition~\ref{def:f_ass}
, the sixth step follows from $\|H(x_k)\| \ge l-Mr_k$ (see Lemma~\ref{lem:lower_bound_spectral_hessian}), the last step follows from $\epsilon_H \in (0,1)$.


Thus, we have, 
    \begin{align*}
        r_{k+1}
        = & ~ \|G_k \cdot (x_k - x^*)\| \\
        \leq & ~ \|G_k\| \cdot \|(x_k - x^*)\| \\
        = & ~ \|G_k\| \cdot r_k \\
        \leq & ~ 2(\epsilon_H +  \frac{M r_k}{l- M r_k}) \cdot r_k
    \end{align*}

 
where the first step follows from Eq.~\eqref{eq:variant_of_r_k_plus_1} and by definition of $r_k$, the second step follows form $\|ab\| \leq \|a\|\|b\|, \forall a,b$, the third step follows from $r_k = \|x_k - x^*\|$, the last step follows from Eq.~\eqref{eq:upper_bound_G_k_divide}, Eq.~\eqref{eq:upper_bound_G_k_1st_step}, and Eq.~\eqref{eq:upper_bound_G_k_2nd_step}.

\end{proof}


\subsection{Induction}\label{sec:newton:induction}

\begin{lemma}\label{lem:newton_induction}
If the following condition hold
\begin{itemize}
    \item $\epsilon_H = 0.01$
    \item $r_{i} \leq 0.4 r_{i-1}$, for all $i \in [k]$
    \item $M \cdot r_i \leq 0.1 l$, for all $i \in [k]$
\end{itemize}
Then we have
\begin{itemize}
    \item $r_{k+1} \leq 0.4 r_k$
    \item $M \cdot r_{k+1} \leq 0.1 l$
\end{itemize}
\end{lemma}
\begin{proof}

{\bf Proof of Part 1.}

 We have
 \begin{align*}
r_{k+1} \leq &  2(\epsilon_H + \frac{M r_k}{ l - M r_k } ) \cdot r_k \\
\leq & ~ 2 ( 0.01 + \frac{M r_k}{ l - M r_k }  ) r_k \\
\leq & ~ 2 (0.01 + \frac{0.1 l}{l - 0.1l}) r_k \\
\leq & ~ 0.4 r_k
 \end{align*}
 where the first step follows from Lemma~\ref{lem:one_step_shrinking}.


{\bf Proof of Part 2.}

We have 
\begin{align*}
M \cdot r_{k+1} 
\leq & ~ M \cdot 0.4 r_{k} \\
\leq & ~ 0.4 \cdot 0.1 l \\
\leq & ~ 0.1 l
\end{align*}

\end{proof}

\subsection{Main Result}\label{sec:newton:main}

We state our main result as follows.
\begin{theorem}[Formal version of Theorem~\ref{thm:main_informal}]\label{thm:main_formal}
Given matrix $A \in \R^{n \times d}$, $b \in \R^n$, and $w \in \R^n$. 

Let $f$ be any of functions $\exp, \cosh$ and $\sinh$.

Let $x^*$ denote the optimal solution of 
\begin{align*}
\min_{x \in \R^d} 0.5 \| f(Ax) - b \|_2^2 + 0.5 \| \diag(w) A x \|_2^2
\end{align*}
that $g(x^*) = {\bf 0}_d$ and $\| x^* \|_2 \leq R$.

Let $\| A \| \leq R, \| b \|_2 \leq R$.

\begin{itemize}
    \item Let $w_{i}^2 \geq 0.5 b_i^2 + l$ for all $i \in [n]$. (If $f=\exp$, see Lemma~\ref{lem:hessian_is_pd_exp})
    \item Let $w_{i}^2 \geq 0.5 b_i^2 + l/\sigma_{\min}(A)^2 + 1$ for all $i \in [n]$. (If $f=\cosh$, see Lemma~\ref{lem:hessian_is_pd_cosh})
    \item Let $w_{i}^2 \geq 0.5 b_i^2 + l/\sigma_{\min}(A)^2 - 1$ for all $i \in [n]$. (If $f=\sinh$, see Lemma~\ref{lem:hessian_is_pd_sinh})
    
\end{itemize}


Let $M = \exp(6R^2)$.

Let $x_0$ denote an initial point such that $M \| x_0 - x^* \|_2 \leq 0.1 l$.

For any accuracy parameter $\epsilon \in (0,0.1)$ and failure probability $\delta \in (0,0.1)$.  There is a randomized algorithm (Algorithm~\ref{alg:main}) that runs $\log(\| x_0 - x^* \|_2/ \epsilon)$ iterations and spend  
\begin{align*}
O( (\nnz(A) + d^{\omega} ) \cdot \poly(\log(n/\delta)) 
\end{align*}
time per iteration, and finally outputs a vector $\wt{x} \in \R^d$ such that
 
\begin{align*}
\| \wt{x} - x^* \|_2 \leq \epsilon
\end{align*}
holds with probability at least $1-\delta$.
 
\end{theorem}
\begin{proof}
{\bf Proof of $\exp$ function.}


It follows from combining Lemma~\ref{lem:hessian_is_pd_exp}, Lemma~\ref{lem:newton_induction}, Lemma~\ref{lem:subsample}, and
Lemma~\ref{lem:one_step_shrinking}.

After $T$ iterations, we have
\begin{align*}
\| x_T - x^* \|_2 \leq 0.4^T \cdot \| x_0 - x^* \|_2
\end{align*}
By choice of $T$, we get the desired bound. The failure probability is following from union bound over $T$ iterations.

{\bf Proof of $\cosh$ function.}


It follows from combining Lemma~\ref{lem:hessian_is_pd_cosh}, Lemma~\ref{lem:newton_induction}, Lemma~\ref{lem:subsample}, and
Lemma~\ref{lem:one_step_shrinking}.

{\bf Proof of $\sinh$ function.}


It follows from combining Lemma~\ref{lem:hessian_is_pd_sinh}, Lemma~\ref{lem:newton_induction}, Lemma~\ref{lem:subsample}, and
Lemma~\ref{lem:one_step_shrinking}.

\end{proof}