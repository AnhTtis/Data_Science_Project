\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.28]{image/network.jpg}
    \caption{Overview of our proposed method. (a) Self-paced Learning (SPL). Target images with their reference images are presented in different categories, easy samples are chosen by categories to participate in training stage. (b) The backbone CNN. Target images and reference images are input to the backbone network simultaneously to obtain paired basic feature vectors. In each layer of the CNN, a normalization operation is applied on the paired feature maps. (c) Neutral expression-disentangled features (NDFs). NDFs are obtained by a subtraction operation. NDFs contain expression deviation information and few disturbance information. The cross entropy (CE) loss value of each paired sample is further used for SPL.}
    \label{fig:Network}
\end{figure*}


\section{Proposed Method}

\subsection{Overview}
Figure \ref{fig:Network} shows an overview of the proposed SPNDL.
SPL first selects a part of paired samples (target image and reference neutral image) that are easy for the initial network. Then, paired feature vectors of these chosen samples are extracted by a backbone CNN. In each layer, a normalization operation is applied on the paired feature maps to preliminarily eliminate the distribution difference. After this, a simple but very effective subtraction operation is applied on the paired feature vectors to obtain NDFs, in which disturbance information is removed and deviation information is collected. Finally, NDFs are used for classification and samples' loss values are used for the next pace of SPL.      

\subsection{Neutral Expression-Disentangled Features}
\label{sec:NDFs}
Information in facial expression image consists of expression-related information and expression-unrelated information (disturbance information). The disturbance information can confuse networks in training stage and make micro-facial movements more difficult to perceive. Let $\bm{V}_{tar}$ be the basic feature vector of a target image. We assume that there is an ideal method to decompose $\bm{V}_{tar}$ into expression feature vector $\bm{V}_{exp}$ and disturbance feature vector $\bm{V}_{dis}$:
\begin{equation}
    \bm{V}_{tar} = \bm{V}_{exp} + \bm{V}_{dis}.
\label{eq:vexp2}
\end{equation}
But in practice, such an ideal decomposition is hard to achieve. Fortunately, we find that neutral information in neutral facial expression consists of initial state of expression and similar disturbance information shared with its corresponding target image. With the ideal decomposition method, the basic feature vector of a reference neutral image $\bm{V}_{ref}$ can be decomposed into initial state feature vector $\bm{V}_{init}$ and disturbance feature vector $\bm{V}_{dis}$: 

\begin{equation}
    \bm{V}_{ref} = \bm{V}_{init} + \bm{V}_{dis}.
\label{eq:vexp1}
\end{equation}

Based on the above-mentioned observations, instead of directly decomposing $\bm{V}_{tar}$ and $\bm{V}_{ref}$, we use a subtraction operation to remove the interference of disturbance information and obtain the initial state of expression, which can help networks to extract deviation information. Thus the neutral expression-disentangled features $\bm{V}_{exp-d}$  focused on expression deviation information is given by
\begin{equation}
    \bm{V}_{exp-d} = \bm{V}_{tar}-\bm{V}_{ref}=\bm{V}_{exp}-\bm{V}_{init}.
\label{eq:vexp3}
\end{equation}

This formulation is based on early analysis of facial expression manifold, which shows the variations of face images can be represented as low dimensional manifolds in feature space and individual facial expression images can always gather around close to the neutral face in a sub-manifold \cite{2021Zhang, YaChang, 1978Facial, 2005Shan}. Therefore, it is reasonable to obtain NDFs by the proposed subtraction operation.     

%image sequences of facial expressions of an individual makes a continuous manifold. However, due to significant appearance variation across different subjects, the manifolds of different subjects vary much in the covered regions and the stretching directions. Those expressions from the same identity always Besides, the semantic-similar expressions from different identities are analogous on the expression manifold.
%DLN\cite{2021Zhang} also used such a disentangled manner, they fixed a pre-trained FaceNet's parameters to get identity feature vector and then stripped it from the output feature vector of a training FaceNet. Their method relies too much on the effect of pre-training, and fixing parameters leads to that the expression information is also included in the identity feature vector. Our method uses a shared network to extract $\bm{V}_{tar}$ from target image and $\bm{V}_{ref}$ from reference image at the same time. Target expression information only exists in $\bm{V}_{tar}$ while initial state information of expression only exists in $\bm{V}_{ref}$. And they share similar disturbance information.
\par
However, when there is a large distribution difference between the features of target image and reference image, the proposed subtraction operation will make the generated $\bm{V}_{exp-d}$ meaningless or even harmful. Thus we apply a normalization operation, which is performed based on the BN layer of ResNet-18 but plays a different role -- conducting normalization between the output features of target image and reference image in the backbone CNN's each layer. Let $\bm{F}_{tar}^{l}$ be the output features of target image in the $l^{th}$ convolutional layer and $\hat{\bm{F}}_{tar}^{l}$ be the input features. Let $\bm{F}_{ref}^{l}$ be the output features of reference image in the $l^{th}$ convolutional layer and $\hat{\bm{F}}_{ref}^{l}$ be the input features. We first calculate average value $\mu$ and variance $\sigma^{2}$  by\footnote{For simplicity, the $i^{th}$ $\bm{F}_{tar}^{l}$ (or $\bm{F}_{ref}^{l}$) is denoted by $\bm{F}_{tar}^{l}$ (or $\bm{F}_{ref}^{l}$).}
\begin{equation}
    \mu=\frac{1}{M} \sum_{i=1}^{M}(\bm{F}_{tar}^{l}+\bm{F}_{ref}^{l}),
\label{eq:normalization1}
\end{equation}

\begin{equation}
\sigma^{2} = \frac{1}{M} \sum_{i=1}^{M}\left((\bm{F}_{tar}^{l}-\mu)^{2} + (\bm{F}_{ref}^{l}-\mu)^{2}\right),
\label{eq:normalization2}
\end{equation}
where $M$ represents the number of paired samples in a batch. Then, we obtain the input features of the next layer by 

\begin{equation}
\hat{\bm{F_{.}}}^{l+1} = g\left(\gamma \frac{\bm{F_{.}}^{l}-\mu}{\sqrt{\sigma^{2}+\epsilon}} +\beta\right),
\label{eq:normalization3}
\end{equation}
where $\bm{F_{.}}^{l}$ represents $\bm{F}_{tar}^{l}$ or $\bm{F}_{ref}^{l}$, $\hat{\bm{F_{.}}}^{l}$ represents $\hat{\bm{F}}_{tar}^{l}$ or $\hat{\bm{F}}_{ref}^{l}$,  $\gamma$ and $\beta$ are reconstruction parameters learned by the networks, $\epsilon$ is a constant that prevents the denominator from being zero, and $g(\cdot)$ denotes activation function. Eq. (\ref{eq:normalization1}) to Eq. (\ref{eq:normalization3}) are all calculated separately for each channel of the feature maps. At last, the basic feature vector of target image or reference image (represented by $\bm{V_{.}}$) can be obtained by a average pooling implemented on the last layer's output features (represented by $\bm{F_{.}}^{-1}$):
\begin{equation}
\bm{V_{.}} = Average Pooling(\bm{F_{.}}^{-1})
\end{equation}

\par
One may argue that in real life it is possible that for a certain facial expression image, there is no corresponding neutral expression image to match it. We here propose a solution that can extract neutral information from the recognized images themselves. Learning from deviation learning network (DLN) \cite{2021Zhang}, we use two backbone CNN architectures, one extracts expression information and the other extracts neutral information. Figure \ref{fig:new} shows this basic feature extraction part, and the other parts are the same as our proposed method. Therefore, without a reference neutral image as input, the neutral expression-disentangled features can be obtained by
\begin{equation}
    \bm{V}_{exp-d} = f(x_{tar}) - f'(x_{tar}) = \bm{V}_{tar} - \bm{V}_{ref},
\end{equation}
where $f(\cdot)$ represents the expression-information-extracted network and $f'(\cdot)$ represents the neutral-information-extracted network. 

The neutral-information-extracted network can be trained while DLN uses a fixed CNN to obtain target images' identity information, which relies too much on pre-training. The expression-information-extracted network and the neutral-information-extracted network are both fine-tuned with the same ResNet-18 model, which is trained on the MS-Celeb-1M face recognition dataset. The effect of our subtraction operation is to weaken some parts of $V_{tar}$ (subtraction of numbers with the same sign) or enhance some parts of $V_{tar}$ (subtraction of numbers with different signs). At the beginning of training, $V_{tar}$ and $V_{ref}$ are almost equal. To make the subtraction effective, the model will learn which parts of $V_{tar}$ need to be weakened and which parts need to be enhanced. In other words, the model learn by itself to obtain expression-related information while removing expression-unrelated information.
\par

\begin{figure}[!t]
\centering
{\includegraphics[scale=0.25]{image/new.jpg}}
 \caption{Basic fearture extracting without neutral expression images. The two backbone convolutional neutral networks are both trainable. The yellow one is the expression-information-extracted network while the grey one is the neutral-information-extracted network.}
 \label{fig:new}
\end{figure}

\subsection{Self-Paced Neutral Expression-Disentangled Learning}
Although normalization operation can alleviate the distribution difference to some extent, it is still difficult to generate consistently distributed NDFs from some paired samples. Here we propose a SPL strategy to further suppress the impact of this problem.
\par

With our SPL strategy, the training process of the network is divided into multiple paces. In the early stages of training, with large learning rate, the network is in an unstable exploratory state, which means the network can be easily impacted. Therefore, rather than considering all the samples simultaneously, we give priority to samples whose NDFs are safe and easy to the networks for training at early paces. 
As the network becomes robust, those samples excluded in early paces will be gradually included. At the last pace, the network will be trained with all of the samples.    
In addition, we found that in some databases, certain classes of samples are more difficult to learn than samples in other classes, resulting in an unfair selection of samples in SPL process. For example, in the experiments we found that most excluded samples are “fear” faces and “angry” faces, which in turn lead to the network learning unbalanced sample distribution in early paces. Therefore, instead of just simply selecting easy ones from all of the samples, we independently select samples within each class to ensure that the selection in each class tends to be consistently proportional.
\par
Let $\mathcal{X} = \{(\mathbf{x}_{tar}^{(1)}, \mathbf{x}_{ref}^{(1)}, y^{(1)}) , \cdots,(\mathbf{x}_{tar}^{(N)}, \mathbf{x}_{ref}^{(N)}, y^{(N)})\}$ be the data set where $\mathbf{x}_{tar}^{(i)} \in \mathbb{R}^{W \times H}$ is the $i^{th}$ target image, $\mathbf{x}_{ref}^{(i)} \in \mathbb{R}^{W \times H}$ is the $i^{th}$ reference image, $N$ is the total number of samples, and $y^{(i)} \in\{1, 2, \cdots, K\}$ represents its label where $K$ is the number of classes ($K \geq 2$).  

Particularly, a latent weight variable ${v}_{k,i}$ is defined to indicate whether the $i^{th}$ sample of the $k^{th}$ class is selected or not. Depending on the degree of training complexity, ${v}_{k,i}$ will be optimized as $1$ (selected) or $0$ (unselected). The aim of self-paced learning is to learn model parameters $\bm{\theta}$ and the latent weight variable $\bm{v}$ simultaneously by minimizing:

\begin{equation}
\min _{\boldsymbol{\theta}, \bm{v}} \sum_{k=1 }^{K} \sum_{i\in N_{k}}{v}_{k,i} l_{k,i}-\sum_{k=1 }^{K}\lambda_{k}\sum_{i\in N_{k}}{v}_{k,i},
\label{eq:self-paced}
\end{equation}
where $N_{k}$ denotes the set of indices of samples in the $k^{th}$ class, $\lambda_{k}$ is a parameter defined by category controlling the learning pace, and $l_{k,i}$ is the CE loss of the $i^{th}$ sample in the $k^{th}$ class:  
%\begin{equation}
%\begin{split}
%l_{k,i}=L(y^{(i)}=k,g(\mathbf{x}_{tar}^{(i)}, %\mathbf{x}_{ref}^{(i)};\boldsymbol{\theta}))= -\log %\frac{e^{\boldsymbol{\theta}_{k}^{T} \bm{V}_{exp}^{(i)}}}{\sum_{l=1}^{K} %e^{\boldsymbol{\theta}_{l}^{T} \bm{V}_{exp}^{(i)}}}
%\end{split}
%\end{equation}
\begin{equation}
l_{k,i}=L(y^{(i)}=k,g(\mathbf{x}_{tar}^{(i)},
\mathbf{x}_{ref}^{(i)};\boldsymbol{\theta}))= -\log(p_{k,i}).
\end{equation}
Here, $p_{k,i}$ is the probability that the $i^{th}$ sample belongs to the $k^{th}$ class, defined as:
\begin{equation}
p_{k,i} = \frac{e^{\boldsymbol{\theta}_{k}^{T} \bm{V}_{exp-d}^{(i)}}}{\sum_{l=1}^{K} e^{\boldsymbol{\theta}_{l}^{T} \bm{V}_{exp-d}^{(i)}}},
\end{equation}
where $\boldsymbol{\theta}_{k}$ denotes the parameter vector of the $k^{th}$ class in the linear fully-connected layer, and $\bm{V}^{(i)}_{exp-d}$ defined in Section \ref{sec:NDFs} is the NDFs of the paired sample $\{ \mathbf{x}_{tar}^{(i)}, \mathbf{x}_{ref}^{(i)}\}$. 

Eq. (\ref{eq:self-paced}) can be viewed as a traditional classification problem with fixed $\bm{v}$. When $\theta$ is fixed, optimal $\bm{v}^{*}$ can be easily obtained using the following equation:
%shown using calculation by
\begin{equation}
{v}_{k,i}^{*}= 
\begin{cases}
1, & \text { if } l_{k,i}<\lambda_{k} \\ 
0, & \text { otherwise }\end{cases}.
\label{eq:solve_v}
\end{equation}
Paired samples whose loss values are less than $\lambda_{k}$ will be declared as easy samples and the corresponding $v_{k,i}$ will be set to one. These paired samples are chose to train the network, while the others are excluded. $\lambda_{k}$ iteratively increases between paces, samples in each class are dynamically and proportionally involved in the training process, starting with easy examples and ending up with all samples. 
\par
Aiming at minimizing Eq. (\ref{eq:self-paced}),  the model parameters obtained at each pace will be used for initialization at the next pace (where the networks are retrained with the new chosen samples). 
In this way, those easy samples with high qualities (which are selected at early paces) contribute more to the training process, while the negative impact of those low-quality samples (such as ambiguous samples) and samples with inconsistently distributed NDFs can be reduced.
%our model is initialized to the result of the last pace when a new pace starts, then the networks are retrained with the new chosen paired samples. As such, our model always starts learning with the result of the previous learning pace, adaptively calibrated by easy samples. In this process, not only low-quality samples (such as ambiguous samples), but also samples with inconsistently distributed NDFs are excluded.




\subsection{Optimization}    
The alternative search strategy is adopted to solve Eq. (\ref{eq:self-paced}). We iteratively optimize the two parameters $\boldsymbol{\theta}$ and $\bm{v}$, \emph{i.e.}, updating one with the other being fixed. The main procedure consists of the following two steps.
\par
\noindent
\textbf{Optimizing $\bm{v}$ with fixed $\boldsymbol{\theta}$.} $v_{k,i}$ is a binary variable that indicates the $i^{th}$ sample of the $k^{th}$ class is selected or not. %When optimizing it, $\boldsymbol{\theta}$ is fixed. 
Given $\lambda_{k}$, we can directly use the Eq.~(\ref{eq:solve_v}) to obtain the optimal $v_{k,i}$.
At the first pace of the training stage, the parameter $\lambda_{k}$ can be initialized to exclude $50\%$ hard samples in each class. Then, at each subsequent pace, $\lambda_{k}$ is progressively increased to involve $10\%$ more samples until all the samples are selected. %In this learning process, the network gradually approaches to the optimal solution, alleviating the non-convex optimization problem.
\par
\noindent
\textbf{Optimizing $\boldsymbol{\theta}$ with fixed $\bm{v}$.} With samples chosen by the fixed $\bm{v}$, the network parameters $\boldsymbol{\theta}$ are updated through gradient descent. In early stages, these chosen samples are high-quality and easy to learn, so $\boldsymbol{\theta}$ converges easily.

%\subsection{Complexity Analysis}
%In the $l^{th}$ layer of CNN, respectively, $M_{l}$, $K_{l}$ and $C_{l}$ are denoted as the feature map size, the convolution kernel size, and the channel number. We also denote $N$ as the data size, $n$ as the batch size, $M$ as the number of categories, $D$ as the number of layers of CNN, and $H$ as the dimensionality of NDFs. In the mini-batch optimization process, the complexity of the FC layer is $O(nMH)$, the complexity of computing the cross-entropy loss is $O(nM)$. Moreover, the complexity of CNN is $O(n\sum_{l=1}^{D}M_{l}^{2}K_{l}^2C_{l-1}C_{l})$. Therefore, the total complexity to our train model is $O(N/n(nMH + nM + n\sum_{l=1}^{D}M_{l}^{2}K_{l}^2C_{l-1}C_{l})) = O(N \times \mbox{\emph{a constant value}})$. In conclusion, the complexity of SPNDL is linear to the data size $N$.    









