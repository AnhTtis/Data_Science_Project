\section{Experiments}
We adopt three variations of our proposed method for experiments: baseline (which only extracts features from target images using ResNet-18), SPNDL, and SPNDL w/o neutral image (which is described in Figure \ref{fig:new}).
\subsection{Experimental Setup}
\label{section:ExperimentalSetup}

\textbf{Databases.} The experiments are carried out on three popular databases: \textbf{Extended Cohn-Kanade Database (CK+)} \cite{2010ck+} contains 593 video sequences, recording 123 subjects' facial expressions in controlled lab environments. Each video sequence starts with the neutral expression and ends with the peak expression. There are only 327 video sequences with 118 subjects that are labeled as one of seven expressions, \emph{i.e.}, anger, contempt, disgust, fear, happiness, sadness, and surprise. We choose the first neutral frame and three peak expression frames from each video sequence. The first neutral frame is paired with each peak frame as a sample, thus resulting in a total of 981 paired samples. \textbf{Oulu-CASIA Database} \cite{2011oulu} captured 480 facial expressions from 80 subjects under three different illumination conditions using two types of cameras, thus containing a total of $480 \times 6$  video sequences. In our experiments, only the sequences captured under strong illumination condition with the VIS camera are used. Each of them is labeled as one of the six expressions (\emph{i.e.}, anger, disgust, fear, happiness, sadness, and surprise). Similar to the experimental setting on the CK+ database, from each video sequence, the first neutral frame is paired with each of the three peak expression frames to construct three paired samples, resulting in a total of 1440 paired samples. \textbf{Real-world Affective Faces Database (RAF-DB)}\cite{rafdb} is a in-the-wild database, containing 30,000 images labeled with basic or compound expressions by 40 trained human coders. 12271 training images and 3068 testing images with seven expressions (\emph{i.e.}, neutral, happiness, surprise, sadness, anger, disgust, and fear) are used in our experiments. Because there is no reference neutral image for each target image, RAF-DB is only used in experiments without neutral images.
\par 
\textbf{Comparison methods.}
We compare our method with both sequence-based methods and image-based methods on the CK+ and Oulu-CASIA databases. Specifically, sequence-based methods evaluate on the whole facial expression video sequences, the comparison methods include:   
\vspace{-0.2cm}
\begin{itemize}[leftmargin=*]
    \item Learning expressionlets on spatio-temporal manifold (STM-Explet) \cite{2014Liu}    
    \item Deep temporal appearance-geometry network (DTAGN) \cite{2015Joint}
    \item Dynamic FER using longitudinal facial expression atlases (Atlases) \cite{2012Atlases}
\end{itemize}
\vspace{-0.2cm}
\par
Rather than using the whole video sequences, the following image-based methods only evaluate on the three peak frames of each sequence:
\vspace{-0.2cm}
\begin{itemize}[leftmargin=*]
     \item Deep disturbance-disentangled learning (DDL) \cite{2020Ruan}
    \item De-expression residue learning (DeRL) \cite{2018Yang}
    \item Peak-piloted deep network (PPDN) \cite{2016Peak}
    \item Feature decomposition and reconstruction learning (FDRL) \cite{2021Feature}
    \item Inconsistent pseudo annotations to latent truth (IPA2LT) \cite{2016FaceNet2ExpNet}
    \item Region attention network (RAN) \cite{RAN}
    \item Selfcure network (SCN) \cite{2020Suppressing}
    \item Facial expression recognition with grid-wise attention and visual transformer (FER-VT) \cite{FER-VT}
    \item Harmonious Representation Learning (HRL) \cite{HAN2022104}
\end{itemize}
\vspace{-0.2cm}
\par
\textbf{Evaluation metrics.} The performance of all the compared methods is evaluated by the recognition accuracy (ACC). Because both CK+ and Oulu-CASIA databases are not divided into training set and test set, following the compared methods, a 10-fold subject-independent cross-validation is performed. The averaged  results of 10-fold cross-validation are reported.
%The ACC is the average of the 10-fold experiments' results.

\subsection{Implementation Details}
On CK+ and Oulu-CASIA databases, the face region of each facial image is cropped by four boundary points of the landmark points (left boundary point, right boundary point, upper boundary point and lower boundary point). On RAF-DB, the facial images are all detected and aligned using Retinaface \cite{detected2020}. All images are resized to $224\times224$ pixels on each database. During the training process, a random horizontal flipping or Gaussian noise adding operation is applied for data augmentation purposes to increase the number of images to avoid overfitting. Notice that the target image and reference image in each paired sample are augmented by the same operation simultaneously. 
\par 
The SPNDL method is implemented with the Pytorch toolbox and the backbone is a ResNet-18 model \cite{2016resnet} pre-trained on the ImageNet database \cite{2009ImageNet}. On CK+ and Oulu-CASIA databases, we train SPNDL and SPNDL (w/o neutral image) in an end-to-end manner for 250 epochs in each run of the 10-fold experiments. On RAF-DB, because there is on reference neutral image, we only train SPNDL (w/o neutral image) for 150 epochs. The batch size is set to 128 for all databases. In the training stage, we adopt Adam as the optimization method, with learning rate $lr = 0.001$, first-order momentum $\beta_1 = 0.9$, and second-order momentum $\beta_2 = 0.999$. All experiments are run with Python 3.8 and Pytorch 1.7.1 on a Linux server equipped with 2.5GHz CPU and 16GB RAM, a single RTX3070 GPU is used to accelerate the training stage.

\begin{table}[t]
  \caption{Recognition accuracy ($\%$) on the CK+ for 7 expressions classification.}
  \centering
    \begin{tabular}{p{5.5cm}p{4.3cm}p{2.6cm}}
    \hline
    \multicolumn{1}{c}{Method} &
    \multicolumn{1}{c}{Setting} & 
    \multicolumn{1}{c}{Accuracy} \\
    \hline
    \makecell[r]{STM-E\cite{2014Liu}(2014)\hspace{0.9cm} }&  \makecell[c]{sequence-based}&
    \makecell[c]{94.19}\\
    \makecell[r]{DTAGN\cite{2015Joint}(2015)\hspace{0.9cm} } & \makecell[c]{sequence-based} & \makecell[c]{97.25}  \\
    \makecell[r]{PPDN\cite{2016Peak}(2016)\hspace{0.9cm} }&  \makecell[c]{image-based}&
    \makecell[c]{99.30}\\
    \makecell[r]{DeRL\cite{2018Yang}(2018)\hspace{0.9cm} } & \makecell[c]{image-based} & \makecell[c]{97.30}  \\
    \makecell[r]{DDL\cite{2020Ruan}(2020)\hspace{0.9cm} } & \makecell[c]{image-based} & \makecell[c]{99.16}  \\
    \makecell[r]{FDRL\cite{2021Feature}(2021)\hspace{0.9cm} } & 
    \makecell[c]{image-based} & \makecell[c]{99.54}\\
    \makecell[r]{HRL\cite{HAN2022104}(2022)\hspace{0.9cm} } & 
    \makecell[c]{image-based} & \makecell[c]{98.90}\\
     \hline
    \makecell[c]{Baseline(ResNet-18)} & \makecell[c]{image-based} & \makecell[c]{97.55} \\ 
    \makecell[c]{SPNDL(w/o neutral image)} & \makecell[c]{image-based} & \makecell[c]{99.18} \\
    \makecell[c]{\textbf{SPNDL}} & \makecell[c]{image-based} & \makecell[c]{\textbf{99.69}}  \\
    \hline
    \end{tabular}%
  \label{tab:comparision_ck+}%
\end{table}%

\begin{table}[tbp]
\caption{Recognition accuracy ($\%$) on the Oulu-CASIA for 6 expressions classification.}
  \centering
    \begin{tabular}{p{5.5cm}p{4.3cm}p{2.6cm}}
    \hline
    \multicolumn{1}{c}{Method} &
    \multicolumn{1}{c}{Setting} & 
    \multicolumn{1}{c}{Accuracy} \\
    \hline
    \makecell[r]{Atlases\cite{2012Atlases}(2012)\hspace{0.9cm} }&  \makecell[c]{sequence-based}&
    \makecell[c]{75.52}\\
    \makecell[r]{DTAGN\cite{2015Joint}(2015)\hspace{0.9cm} } & \makecell[c]{sequence-based} & \makecell[c]{81.46}  \\
    \makecell[r]{FN2EN\cite{2016FaceNet2ExpNet}(2016)\hspace{0.9cm} }&  \makecell[c]{image-based}&
    \makecell[c]{87.71}\\
    \makecell[r]{DeRL\cite{2018Yang}(2018)\hspace{0.9cm} } & \makecell[c]{image-based} & \makecell[c]{88.00}  \\
    \makecell[r]{DDL\cite{2020Ruan}(2020)\hspace{0.9cm} } & \makecell[c]{image-based} & \makecell[c]{88.26}  \\
    \makecell[r]{FDRL\cite{2021Feature}(2021)\hspace{0.9cm} } & \makecell[c]{image-based} & \makecell[c]{88.26}\\
    \hline
    \makecell[c]{Baseline(ResNet-18)} & \makecell[c]{image-based} & \makecell[c]{87.57}  \\
    \makecell[c]{SPNDL(w/o neutral image)} & \makecell[c]{image-based} & \makecell[c]{88.54}  \\
    \makecell[c]{\textbf{SPNDL}} & \makecell[c]{image-based} & \makecell[c]{\textbf{90.14}}  \\
    \hline
    \end{tabular}%
  \label{tab:comparision_oulu}%
\end{table}%


\begin{table}[tbp]
\caption{Recognition accuracy ($\%$) on the RAF database for 7 expressions classification.}
  \centering
    \begin{tabular}{p{5.5cm}p{4.3cm}p{2.6cm}}
    \hline
    \multicolumn{1}{c}{Method} &
    \multicolumn{1}{c}{Setting} & 
    \multicolumn{1}{c}{Accuracy} \\
    \hline
    \makecell[r]{IPA2LT\cite{IPA2LT}(2018)\hspace{0.9cm} }&  \makecell[c]{image-based}&
    \makecell[c]{86.77}\\
    \makecell[r]{RAN\cite{RAN}(2020)\hspace{0.9cm} } & \makecell[c]{image-based} & \makecell[c]{86.90}  \\
    \makecell[r]{DDL\cite{2020Ruan}(2020)\hspace{0.9cm} } & \makecell[c]{image-based} & \makecell[c]{87.71}  \\
    \makecell[r]{SCN\cite{2020Suppressing}(2020)\hspace{0.9cm} } & \makecell[c]{image-based} & \makecell[c]{88.14}  \\
    \makecell[r]{FER-VT\cite{FER-VT}(2021)\hspace{0.9cm} }&  \makecell[c]{image-based}&
    \makecell[c]{88.26}\\
    \makecell[r]{\textbf{FDRL}\cite{2021Feature}(2021)\hspace{0.9cm} } & \makecell[c]{image-based} & \makecell[c]{\textbf{89.47}}\\
    \makecell[r]{HRL\cite{HAN2022104}(2022)\hspace{0.9cm} } & 
    \makecell[c]{image-based} & \makecell[c]{87.77}\\
    \hline
    \makecell[c]{Baseline(ResNet-18)} & \makecell[c]{image-based} & \makecell[c]{87.13}  \\
    \makecell[c]{SPNDL(w/o neutral image)} & \makecell[c]{image-based} & \makecell[c]{89.08}  \\
    \hline
    \end{tabular}%
  \label{tab:comparision_raf}%
\end{table}%


\begin{figure}[!t]
  \centering
  \begin{subfigure}{0.32\textwidth}
  \centering
    \includegraphics[scale=0.3]{image/Confusion_Matrix_ck+.png}
    \caption{CK+}
    \label{fig:matrix_ck+}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.32\textwidth}
  \centering
    \includegraphics[scale=0.3]{image/Confusion_Matrix_oulu.png}
    \caption{Oulu-CASIA}
    \label{fig:matrix_oulu}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.32\textwidth}
  \centering
    \includegraphics[scale=0.3]{image/Confusion_Matrix_raf.png}
    \caption{RAF-DB}
    \label{fig:matrix_raf}
  \end{subfigure}
  \caption{Confusion matrix of our SPNDL method on CK+, Oulu-CASIA and RAF-DB.}
  \label{fig:matrix}
\end{figure}
\par

\subsection{Comparison with State-of-the-Art}
We compare the recognition accuracy of the proposed SPNDL with the state-of-the-art methods mentioned in Section \ref{section:ExperimentalSetup} on CK+, Oulu-CASIA and RAF-DB. The corresponding results are reported in Table \ref{tab:comparision_ck+} , Table \ref{tab:comparision_oulu}, and Table \ref{tab:comparision_raf} respectively. The corresponding confusion matrix of our method is shown in Figure \ref{fig:matrix}. 
\par
On the two in-the-lab databases, our SPNDL outperforms the baseline method. Like other image-based methods, the performance of SPNDL is much better than those sequence-based methods. In particular, SPNDL gains a slight improvement of $0.15\%$ on CK+ compared with the recent state-of-the-art method FDRL. SPNDL outperforms FDRL and DDL by $1.88\%$ on Oulu-CASIA. It may be explained by the following reason. Many of the samples in the CK+ database are high-quality, with distinctive features that distinguish them from other facial expressions. Therefore, most methods can achieve high recognition accuracies of over $99\%$. However, there are many ambiguous samples and samples with micro-facial expressions in the Oulu-CASIA database. Our proposed method will exclude these difficult samples in training's early stages, helping the network avoid poor local solutions. With the gradual enhancement of extracting effective NDFs, the network is able to correctly classify these samples. Thus, our method can achieve a significant improvement on Oulu-CASIA.




\par
When there is no reference neutral image, experiments are conducted on RAF-DB, where most subjects have no neutral face. In particular, SPNDL (w/o neutral image) achieves recognition accuracy of $89.08\%$, outperforming most of the comparison methods. The accuracy is slightly lower than FDRL while SPNDL outperforms FDRL on CK+ and Oulu-CASIA. FDRL first decomposes expression basic features into a set of facial action-aware latent features, and then reconstruct the expression feature according to their intra-feature relation weights and their inter-feature relation weights, which are all learned by a network. Their method can find shared information among similar expressions while strengthen expression-specific variations. To find expression-specific variations, our method tries to obtain deviation information through the initial state of expression existing in neutral expression image. But initial state of expression is hard to obtain in in-the-wild datasets like RAF-DB. That’s the reason why our method only outperforms FDRL on in-the-lab datasets. However, our method still performs well on RAF-DB, because we can strengthen expression-related features by removing disturbance information.

To further prove the effectiveness of SPNDL (w/o neutral image), instead of inputting paired images, we remove the reference neutral images in CK+ and Oulu-CASIA. Our method achieves recognition accuracy of $99.18\%$ on CK+, which is much higher than the baseline method. On Oulu-CASIA, without neutral images, our method can still achieve state-of-the-art result. On both CK+ and Oulu-CASIA, it can be seen that the recognition accuracy without reference neutral images is better than that of baseline but worse than that with neutral images. This is because a part of the disturbance information in target image can be extracted to generate $\bm{V}_{ref}$ by the neutral-information-extracted network. However, at the same time, some useful expression information will inevitably be included in $\bm{V}_{ref}$, which will be removed from $\bm{V}_{tar}$ in the subtraction operation. Another reason is that expression's initial state cannot be obviously introduced, which means deviation information is hard to obtain.


\begin{table}
\caption{Ablation studies for two key methods of our model
on the CK+ and Oulu-CASIA databases. The recognition accuracy ($\%$) is used for performance evaluation.}
  \centering
    \begin{tabular}{p{1.3cm}p{1.7cm}|p{1.5cm}p{1.8cm}}
    \hline
    \multicolumn{1}{c}{\multirow{2}[2]{*}{NDFs}} & \multicolumn{1}{c|}{\multirow{2}[2]{*}{SPL}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{CK+}} & \multicolumn{1}{c}{\multirow{2}[2]{*}{Oulu-CASIA}} \\
          &       &       &  \\
    \hline
    \makecell[c]{\XSolidBrush}  & \makecell[c]{\XSolidBrush} &  \makecell[c]{97.55}     & \makecell[c]{87.57}  \\
     \makecell[c]{\XSolidBrush} & \makecell[c]{\Checkmark} & 
 
    \makecell[c]{99.08}    &  \makecell[c]{88.33}\\
    \makecell[c]{\Checkmark} & \makecell[c]{\XSolidBrush} &  
    \makecell[c]{99.29}    &  \makecell[c]{89.58}\\
    \makecell[c]{\Checkmark} & \makecell[c]{\Checkmark} &   
    \makecell[c]{99.69}    &  \makecell[c]{90.14}\\
    \hline
    \end{tabular}%
 \label{tab:ablation}%
\end{table}%

\subsection{Ablation Studies}
The validity of our proposed method is mainly demonstrated by the following experiments on the CK+ and Oulu-CASIA databases. To get a reference standard result, we first feed all samples without neutral face into the backbone convolutional neutral network and directly use the output basic feature vector for classification task.
\par
\textbf{Neutral expression-disentangled features.} To validate the effectiveness of our NDFs, we feed all target images with their reference neutral images into the backbone to get paired basic feature vectors. Notice that target image and reference image are not fed into the same backbone CNN respectively, instead they are input simultaneously as members from one batch. So the normalization operation between them can be applied in each layer of the CNN. At last, a subtraction operation is implemented to obtain the final NDFs for classification.
\par
\textbf{Self-paced learning strategy.} Our SPL strategy consists of 6 paces. At the first pace, $\bm{\lambda}$ is initialized to obtain $50\%$ samples from each category to train the model. At the following paces, $\bm{\lambda}$ is progressively increased such that $10\%$ more samples were gradually involved at each pace. SPL stops when all the samples are included.


The experimental results of ablation studies are reported in Table \ref{tab:ablation}. It can be seen that separately incorporating NDFs or SPL into the backbone network can improve the performance, which shows the effectiveness of NDFs and SPL. Moreover, by employing NDFs and SPL simultaneously, we are able to achieve better recognition accuracy. This is because when SPL works alone, hard and low-quality samples are excluded. The network learns easy samples first and then gradually steps to hard ones, bad local solutions can be avoided in this process.

Specifically, when just using NDFs, the key features of facial expression are more easily to extract due to the elimination of disturbing factors. Deviation information is also included, enabling our model to capture distinguishing characteristics among similar expressions. When SPL and NDFs work together, they can be organically combined to reduce the impact of inconsistently distributed NDFs. Samples with these NDFs are excluded at early paces. At later paces, the network can gradually understand a part of these NDFs and is more likely to classify them correctly. When all the samples are included, meaningless NDFs will have limited negative impact because the network has become more robust. %cannot be optimized will have little impact because the network has become more robust.

\begin{figure}[!t]
\centering
{\includegraphics[scale=0.18]{image/compare_v2.png}}
 \caption{The recognition accuracy at each pace of SPL with and without NDFs. The results are compared with the baseline and method only using NDFs.}
 \label{fig:compare}
\end{figure}

\par
Figure \ref{fig:compare} shows the recognition accuracy at each pace of SPL with and without NDFs. The results are from a random chosen experiment from the 10-fold experiments on the Oulu-CASIA database. In the first two paces, the accuracy of SPL is less than baseline (which is mainly because the training samples are not enough), but it achieves a better result at the third pace. The similar situation happens between method just using NDFs and method using NDFs $+$ SPL. The accuracy of the latter exceeds the former at the fifth pace. In addition, the performance of method with NDFs are always much better than method without NDFs, which proves the effectiveness of the proposed NDFs.

\begin{figure}[!t]
\centering
{\includegraphics[scale=0.6]{image/loss.png}}
 \caption{Losses versus different epochs when training on RAF-DB. The blue line are training losses with SPL, and the yellow line are training losses w/o SPL.}
 \label{fig:loss}
\end{figure}

To prove the effectiveness of our SPL, we present the losses versus different epochs when training on RAF-DB with and w/o SPL in Figure \ref{fig:loss}. When training w/o SPL, the loss decreases gradually until it converges to about 0.005. When using SPL, there is a sudden increase of the loss in the beginning of each pace, this is because more training samples are added for training, but the loss still decreases gradually within a pace. At epoch 150, all samples are added for training, and the loss finally converges to about 0.003, which is lower than that of the method w/o SPL, showing our SPL can help the model escape from pool local solutions to a lower loss value.



\subsection{Visualization}
\begin{figure}[!t]
  \centering
  \begin{subfigure}{0.48\textwidth}
  \centering
    \includegraphics[scale=0.11]{image/distribution_pace2.png}
    \caption{The distribution of NDFs at pace 2.}
    \label{fig:pace2}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.48\textwidth}
  \centering
    \includegraphics[scale=0.11]{image/distribution_pace5.png}
    \caption{The distribution of NDFs at pace 5.}
    \label{fig:pace5}
  \end{subfigure}
  \caption{The distribution of NDFs from “fear” and “disgust” expressions in self-paced learning process. Pace 2 and pace 5 are randomly chosen for visualization. Three kinds of paired samples are chosen from each category to present. Circled paired samples are excluded for training.}
  \label{fig:distribution}
\end{figure}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=13cm, height=5cm]{image/unselected.png}
    \caption{Samples excluded at each pace. We randomly choose some samples excluded by our method at each pace on RAF-DB. Corresponding labels (Ne=Neutral, Ha=Happiness, Sa=Sadness, Su=Surprise, Fe=Fear, Di=Disgust, An=Anger) and loss values are presented below the images. There is no pace 6 because all samples are included for training at pace 6.}
    \label{fig:unselected}
\end{figure*}


In Figure \ref{fig:distribution}, we visualize the distribution of NDFs via t-SNE \cite{van2008visualizing} from “fear” and “disgust” expressions at pace 2 and pace 5 on Oulu-CASIA. $40\%$ samples from each category are excluded at pace 2 while $20\%$ samples from each category are excluded at pace 5. There are three kinds of paired samples: samples located in the distribution center (samples a and d in Figure \ref{fig:pace2}, samples a and d in Figure \ref{fig:pace5}); samples farthest from the center (samples c and f in Figure \ref{fig:pace2}, samples c and f in Figure \ref{fig:pace5}); randomly chosen samples that have been excluded (samples b and e in Figure \ref{fig:pace2}, samples b and e in Figure \ref{fig:pace5}). The center samples have low loss values, they are easy to learn. Loss values of the last two kinds of samples are high, which is caused by micro-facial movements or similarities across different expressions. These samples are neglected at early paces. At pace 2, the inconsistently distributed problem of NDFs is obvious. At pace 5, the problem has been significantly improved.

Figure \ref{fig:unselected} shows some randomly chosen samples that were excluded when training on RAF-DB. Because these images are collected from the Internet, some of them are low-quality (\emph{e.g.}, occlusion, uneven illumination, and low resolution) while some are annotated with ambiguous or even incorrect labels. Training with these samples at early paces may resulting in poor local solutions. At pace 1, the loss values of poor samples are generally low, this is because the network cannot distinguish them well at the beginning of the training. However, these poor samples maintain high loss values at the following paces, which means they are prevented from affecting the network by SPL. At the last pace, all samples will be included for training to avoid overfitting. 










