% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage[dvipsnames]{xcolor}

% Remove the "review" option to generate the final version.
\usepackage{ACL2023}


% Standard package includes
\usepackage{times}
\usepackage{pgfplots}
\usepackage{latexsym}
\usepackage{algorithm}
\usepackage{pgfplots}
\usepackage{algpseudocode}
\usepgfplotslibrary{external}
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{comment}
\usepackage{enumitem}
\usepackage{tikz}
\setlist{nosep}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% seems to be causing some issues where they end up on the wrong page
% \usepackage{tablefootnote}
\usepackage{multirow}
\usepackage{fontawesome}

\usepackage{arydshln}

\newcommand{\fmicro}{F1\textsubscript{micro} }
\newcommand{\fmacro}{F1\textsubscript{macro} }
\newcommand{\Fmicro}{F1\textsubscript{micro}}
\newcommand{\Fmacro}{F1\textsubscript{macro}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{Team SheffieldVeraAI at SemEval-2023 Task 3: Mono and multilingual approaches for genre, framing and persuasion technique detection}

\title{Team SheffieldVeraAI at SemEval-2023 Task 3: Mono and multilingual approaches for news genre, topic and persuasion technique classification}


% \title{Team SheffieldVeraAI at SemEval-2023 Task 3: Monolingual versus Multilingual Approaches for News Genre, Framing, and Persuasion Technique Detection}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Ben Wu$^\# $, Olesya Razuvayevskaya$^\#$, Freddy Heppell$^\# $, João A. Leite$^\#$, \\ {\bf Carolina Scarton, Kalina Bontcheva}, and {\bf Xingyi Song} \\
Department of Computer Science, The University of Sheffield, Sheffield, UK\\
\texttt{\{bpwu1, o.razuvayevskaya, frheppell1, jaleite1\}@sheffield.ac.uk}}



\begin{document}

\maketitle
\def\thefootnote{\#}\footnotetext{Equal contribution, listed randomly.}\def\thefootnote{\arabic{footnote}}
\begin{abstract}
This paper describes our approach for SemEval-2023 Task 3: Detecting the category, the framing, and the persuasion techniques in online news in a multi-lingual setup. For Subtask 1 (News Genre), we propose an ensemble of fully trained and adapter mBERT models which was ranked
%top 3 for 6 of 9 languages, including
joint-first for German, and had the highest mean rank of multi-language teams. For Subtask 2 (Framing), we achieved first place in 3 languages, and the best average rank across all the languages, by using two separate ensembles: a monolingual RoBERTa-MUPPET\textsubscript{LARGE} and an ensemble of XLM-RoBERTa\textsubscript{LARGE} with adapters and task adaptive pretraining. For Subtask 3 (Persuasion Techniques), we train a monolingual RoBERTa-Base model for English and a multilingual mBERT model for the remaining languages, which achieved top 10 for all languages, including 2nd for English. For each subtask, we compare monolingual and multilingual approaches, and consider class imbalance techniques. \footnote{Our code is available at \url{https://github.com/GateNLP/semeval2023-multilingual-news-detection}}
\end{abstract}
%The abstract should contain a few sentences summarizing the paper.

%Instruction on submission requirements can be found here: \url{https://semeval.github.io/paper-requirements.html} (important points repeated below). A suggested structure (that this template follows) and examples can be found here: \url{https://semeval.github.io/system-paper-template.html}. We here assume your paper covers only this task. Otherwise, please check the web pages carefully for necessary changes.

%This paper can be up to 5 pages excluding acknowledgments, references, and appendices. You can add an additional page for camera ready submission.

%You have to use the title as above, just replace "<Team Name>" and "<Descriptive Title>". Usual patterns are to use your team's TIRA code name as "<Team Name>" or to start "<Descriptive Title>" with "The <TIRA code name> approach [to/of/...]".

%At SemEval, papers are not anonymous when submitted for review.

%Your paper should focus on:
%\par\noindent{\bf Replicability}: present all details that will allow someone else to replicate your system. Provide links to code repositories if you made your code open source, and the docker image name if you used Docker submission. {\bf Note:} We will in our overview paper and at other opportunities point out which approaches are available open source and (even better) as Docker image to promote their widespread usage. If you re-submit your approach as Docker image in TIRA until the camera-ready deadline (and it produces the same results), please tell us so that we can include it in our overview paper.
%\par\noindent{\bf Analysis}: focus more on results and analysis and less on discussing rankings; report results on several runs of the system (even beyond the official submissions); present ablation experiments showing usefulness of different features and techniques; show comparisons with baselines.
%\par\noindent{\bf Duplication}: cite the task description paper \cite{froebe:2023a}; you can avoid repeating details of the task and data, however, briefly outlining the task and relevant aspects of the data is a good idea. (The official BibTeX citations for papers will not be released until the camera-ready submission period; the current bibtex entry is a placeholder and we will send you the correct one later.)




\section{Introduction}
With the rise of opinion-manipulating news and misinformation surrounding COVID-19, elections and wars, the task of propaganda and hyperpartisan detection has received much attention over the last five years. Since 2019, various SemEval tasks have addressed detecting hyperpartisan \cite{kiesel-etal-2019-semeval}, sarcasm \cite{abu-farha-etal-2022-semeval}, and persuasion techniques in textual and multimodal data \cite{da-san-martino-etal-2020-semeval, dimitrov-etal-2021-semeval}. This task \citep{semeval2023task3} can be seen as an extension of the latter two tasks, suggesting an expanded ontology of persuasion techniques and addressing other related aspects of persuasion, such as satire, opinionated news, and framing detection.

The three subtasks are the detection of: 1) genre: opinion, objective reporting or satire; 2) framing techniques: 14 multilabel techniques; 3) persuasion techniques: 23 multilabel techniques, which can be grouped into 6 high-level classes.

% justification, simplification, distraction, call, attack on reputation, manipulative wording.
% \begin{itemize}

% The data consists of a labelled training and development set, totalling 1,592 articles for subtask 1 and subtask 2, and 1,612 articles for subtask 3, while the unlabelled test set consists of 457 articles for subtasks 1, 2 and 3. The labelled sets are provided in English, Italian, Russian, French, German and Polish, and the test set additionally contains Spanish, Greek and Georgian articles. Each article has been scraped from a webpage, and in many cases automated fact-checkers as potentially containing mis-/disinformation, and include articles on COVID-19, climate change, abortion, migration, the Russo-Ukrainian war, and related topics. 

The data consists of labelled training and development sets in English, French, German, Italian, Polish and Russian, and unlabelled test sets in the same languages plus three zero-shot languages: Spanish, Greek and Georgian.

The main contributions of this paper are twofold: 1) evaluation of the viability of monolingual versus multilingual models for each of the subtasks; and 2) presentation of the models which ranked first in four subtask languages, top three for 16 subtask languages, and were within the top 10 for all.

%\item What is the task about and why is it important? Be sure to mention the language(s) covered and cite the task overview paper. about 1 paragraph

%\item What is the main strategy your system uses? about 1 paragraph

%\item What did you discover by participating in this task? Key quantitative and qualitative results, such as how you ranked relative to other teams and what your system struggles with. about 1 paragraph

%techniques addressed
% \item Have you released your code or Docker image? Give a URL

% \end{itemize}

%The bib file is already prepared with some papers you may want to cite. We humbly suggest to cite the following papers in case you need a citation.
%For the task of clickbait spoiling, we suggest our ACL paper \cite{hagen-etal-2022-clickbait}.
%For TIRA as the platform of the shared task \cite{froebe:2023b}.

Our approaches for the three subtasks differ, so each will be presented separately in sections 3-5 respectively. An overview of the techniques used in each subtask is given in Table \ref{tab:overview}.

% The structure of this paper is as follows: in the background section, an overview of prior work and techniques for similar data. Then, the system for each subtask will be described with separate system design, experimental setup and results. We will report supplemental findings on the viability of monolingual versus multilingual approaches. Finally, the conclusions of the three subtasks combined will be presented. An overview of the techniques used for the submission in each subtask is given in Table \ref{tab:overview}.

% As shown by \citep{reimers-gurevych-2017-reporting}, non-deterministic systems cannot be reliably compared based on a single score. Therefore, for the results comparing monolingual and multilingual models, we present mean scores and standard deviations calculated as a result of several runs of each system. For the official leaderboard results, we provide a single score based on which our team was ranked, however, some of our submissions represent ensemble models that take non-determinism into account.

%The official results for subtask 1 rank the systems based on F$_{1_{macro}}$ scores, while for sub-tasks~2 and ~3, F$_{1_{micro}}$ scores were considered for ranking.
%TODO official results: number of 1, 2, 3,4,5 places
%TODO average results per task

\section{Background}

\newcommand{\yes}{{\color{ForestGreen}{\faCheck}}}
% \newcommand{\no}{{\color{BrickRed}{\faTimes}}}
\newcommand{\no}{{\color{Gray}{-}}}
%\begin{table*}[t]
%    \centering
%    \vspace{-4mm}
%    \scalebox{0.8}{
%        \begin{tabular}{l l l l}
%            \toprule
%            Technique & Subtask 1 & Subtask 2 & Subtask 3  \\
%            \midrule
%            Text Cleaning & \yes $^\star$ & \yes & \no \\
%            External Data & \yes & \no & \no \\
%            Oversampling & \yes $^\star$ & \no & \no \\
%            % Trained with Translations & \no & $\Rightarrow$ EN & \no \\
%            \midrule
%            Class Weighting & \no & \yes & \yes \\
%            Adapters & \yes $^\dagger$ & \yes & \no \\
%            Task-adaptive Pre-training & \no & \yes & \no \\
%            \midrule
%            % Unseen languages & Zero-shot & \multicolumn{2}{l}{Translated to EN} \\
%            Unseen languages & Zero-shot & $\Rightarrow$ EN & $\Rightarrow$ EN \\
%            Ensemble & \yes & \yes & \no \\
%            \bottomrule
%        \end{tabular}
%    }
%    \vspace{-2mm}
%    \caption{An overview of approaches used. $^\star$ not used in adapter model for submission. $^\dagger$ as part of ensemble. }
%    \label{tab:overview}
%    \vspace{-4mm}
%\end{table*}



\begin{table*}[t]
    \centering
    \vspace{-4mm}
    \scalebox{0.8}{
        \begin{tabular}{l l l l l l l l l}
            \toprule
            Tasks & Text Clean & External Data & Oversampling & Class Weights & Adapters & TAPT & Unseen languages & Ensemble  \\
            \midrule
            Subtask1 & \yes $^\star$ & \yes & \yes $^\star$ & \no & \yes $^\dagger$  & \no & Zero-shot & \yes \\ 
            Subtask2 & \yes & \no & \no & \yes & \yes  & \yes & $\Rightarrow$ EN & \yes \\
            Subtask3 & \no & \no & \no & \yes & \no & \no & $\Rightarrow$ EN & \no \\
            \bottomrule
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{An overview of approaches used. TAPT: Task-adaptive Pre-training, $^\star$ not used in adapter model for submission. $^\dagger$ as part of ensemble. }
    \label{tab:overview}
    \vspace{-4mm}
\end{table*}


% \begin{itemize}
%     \item genre: opinion, objective reporting or satire annotated on the article level;
%     \item framing techniques: 14 multilabel frames annotated on the article level according to \citet{card-etal-2015-media};
%     \item persuasion techniques: 23 multilabel techniques annotated on the paragraph level. The 23 annotated persuasion techniques can be grouped into 6 high-level classes: justification, simplification, distraction, call, attack on reputation, manipulative wording.
% \end{itemize}

Fine-grained propaganda technique classification was first introduced by \citet{da-san-martino-etal-2019-fine}, who suggested a multi-granularity network, where the lower and higher granularity tasks refer to the fragment and sentence-level classification respectively. %The key idea is the use of a trainable gate that is able to direct the task of a lower granularity by means of weight projections. 
Other state-of-the-art approaches to this task used an ensemble of RoBERTa models with class weighting, where some models perform a semi-supervised task of span detection \cite{jurkiewicz-etal-2020-applicaai} and an ensemble of 5 different transformer models \cite{tian-etal-2021-mind}, namely BERT \cite{devlin-etal-2019-bert}, RoBERTa \cite{liu2019roberta}, XLNet \cite{yang2019xlnet}, DeBERTa \cite{he2020deberta} and ALBERT \cite{lan2019albert}.

Framing detection specifically has been addressed primarily for political news, with the models exploring unsupervised  probabilistic topic models combined with autoregressive distributed-lag models \cite{tsur-etal-2015-frame}, finetuning BERT \cite{liu-etal-2019-detecting} and multilingual BERT (mBERT) \cite{akyurek-etal-2020-multi}. The latter system is the closest to our task since it explores the multilabel multilingual setting and the effect of translations into target languages of monolingual models, however, it uses article headlines instead of the full texts as a classification data. %The authors explore monolingual and multilingual settings of the task, using both full translations and `code-switching' , i.e. translations of keywords into a target language of a monolingual model. 
It found that English BERT\textsubscript{BASE} uncased trained on translated data and tested on the data in the target language often outperforms the multilingual model. We perform similar comparison experiments for all three subtasks of this shared task.

Another important task addressed in fake news detection is satire detection, with the methods ranging from convolutional neural networks (CNNs) \cite{guibon2019multilingual} to adversarial training \cite{mchardy-etal-2019-adversarial} and BERT-based architectures with long-short-term memory (LSTM) \cite{pandey2022bert, liu2021research} and CNN \cite{kaliyar2021fakebert} layers on top.

Bottleneck adapters \citep{houlsby2019parameter,  bapna-firat-2019-simple} are a technique to improve parameter efficiency in finetuning by freezing the pretrained model and inserting adapter modules with a lower dimensionality than the model. \citeauthor{houlsby2019parameter} found that, despite training only 3.6\% of the parameters compared to a full model, performance only decreased by 0.4\%, and \citeauthor{bapna-firat-2019-simple} found adapters produced comparable, or in some cases better, results. Particularly relevant for our task is \citet{he-etal-2021-effectiveness}'s finding that adapter-based tuning of LLMs is particularly effective for low-resource and cross-lingual tasks. For our system, we used two different configurations of the bottleneck adapter modules: 1) the original \citeauthor{houlsby2019parameter} bottleneck configuration, which places adapter modules after the multi-head attention block and feed-forward block of each transformer layer; 2) the \citeauthor{pfeiffer-etal-2020-mad} configuration, which places adapter modules only after the feed-forward block of each layer.  

\citet{chalkidis-etal-2021-multieurlex} also found that for XLM-R on the MultiEURLEX dataset, training bottleneck adapters outperforms traditional full finetuning and improves zero-shot cross lingual capability. Like this task, the MultiEURLEX dataset is used for multilingual multilabel classification, though it is significantly larger than Task 3 (covering 23 languages and classifying hierarchically from 21 to 567 labels).

% The MultiEURLEX dataset \citep{chalkidis-etal-2021-multieurlex} is a similarly highly multilingual multilabel task, covering the 23 official EU languages and classifying hierarchically from 21 to 567 labels. The authors compare monolingual classification, cross-lingual transfer and multilingual fine-tuning, and find that a single multilingual model is competitive to monolingual models. However, the MultiEURLEX dataset is significantly larger than this subtask's, and is collected from EU law, so is likely less noisy than web scraping.


\citet{wang-banko-2021-practical} performed a series of experiments comparing monolingual and multilingual approaches for hate speech detection and sentiment analysis and found that different task-language combinations favour either monolingual and multilingual settings. The authors also conclude that data augmentation in the form of translation and task-adaptive pretraining (TAPT) \cite{gururangan-etal-2020-dont} help to further improve the results.

% Merge my Adapter discussion with Freddy's 
% Adapters are an alternative to fully fine-tuning a model's parameters. In adapter-based tuning, a small number of additional parameters are added to a pre-trained LLM. During training only these paramters are updated while the pre-trained weights remain fixed. 


% Each bottleneck adapter layer has a down-projection matrix \textit{Wdown} that projects the layer hidden states into a lower dimension \textit{d}, a non-linearity \textit{f}, an up-projection \textit{Wup} that projects back into the original hidden layer dimension and a residual connection \textit{r}. We used a reduction factor of 8, which corresponds to projecting the XLM-Roberta Large's hidden dimension of 1024 to a bottleneck dimension \textit{d} of 128. 

%End of adapters 

% TODO task-adaptive pretraining - Ben

%Move to Subtask 2? 

% TODO class weighting - Ben.


\section{System Description for Subtask 1}
\subsection{System Overview}\label{sec:st1_system}
The system consisted of an ensemble of four models, comprising 1) three mBERT models each finetuned using the organiser training set and 2/3 of the development set; 2)  one frozen mBERT model with a finetuned Houlsby adapter\footnote{Using a data split described in section~\ref{sec:t1_setup}}. The ensemble predictions were decided by majority vote, with rare tie cases handled by selecting the model with the best validation performance.
%classification performance measured by the 1/3 held-out development set for each target language.
%the most frequently predicted class per article. 
%and one frozen mBERT model with fine-tuned adapters. The ensemble predictions were drawn based on the most frequently predicted class per article.
%our modification of Boyer–Moore majority-voting algorithm \cite{boyer1991mjrty} illustrated in algorithm~\ref{alg:majority}. 
%For the rare tie cases, we output the prediction made by the transformer model that ranked highest based on \fmacro scores on the 1/3 held-out part of the organiser development set for a target language language.


%The mBERT and adapter bottleneck models used different data splits described in section~\ref{sec:t1_setup}.

\paragraph{Full Finetuning} mBERT\textsubscript{BASE} \citep{devlin-etal-2019-bert} was finetuned on a shuffled combination of all languages. Different to previous approaches \cite{wu-dredze-2020-languages, adebara-etal-2020-translating}, we chose the epoch with the best validation performance per-language, instead of overall, since the best overall epoch is not necessarily the best for a given language\footnote{See Appendix \ref{ap:4}}.
%This approach is based on our early stage experiments (appendix~\ref{ap:4}) which revealed that different languages peaked at different epochs, making the best overall \fmacro score suboptimal for maximizing performance on individual languages.
%instead of choosing the checkpoint with the best overall results based on all language combined development sets, we choose the checkpoint with the best performance for each language.
%The problem of selecting the best model for the multilingual tasks is not widely addressed, with the systems traditionally choosing the iteration with the best overall results on the development set \cite{wu-dredze-2020-languages, adebara-etal-2020-translating}. During the early stages of training multilingual models, we noticed that different languages peak at different epochs, as illustrated in appendix~\ref{ap:4}. This means that selecting the model based on the best overall \fmacro score is not the most optimal solution for maximising the performance on individual languages. We therefore use the held-out part of the development set to select the best model for each language, evaluating the checkpoint of the model after each iteration for each language in the held-out set. 
By using three identically-configured models in the ensemble, the data sacrificed for model selection can be rotated between them, so overall no data is truly unseen.
%The decision to sacrifice part of the data for the model selection purposes motivated the ensemble-based architecture consisting of three fine-tuned transformer models to make sure that the final system benefits from being trained on all the labeled data.

\paragraph{Adapter Model} A Houlsby bottleneck adapter was applied to a pretrained mBERT\textsubscript{BASE} model, with a reduction factor of 8 (i.e. $d=96$) was used, using the AdapterHub \citep{pfeiffer-etal-2020-adapterhub} framework. The mBERT model parameters were frozen, so only the adapter and classification head parameters were trained.



\paragraph{Data Preprocessing}
Since task data is obtained from webpages, it often contains unwanted content, such as hyperlinks, account handles, dates and author biographies. We applied the preprocessing described in Appendix \ref{ap:1} to remove this content.
% Initial experiments demonstrated that data cleaning improved the average scores for multilingual models compared to using the original data, and therefore, a set of preprocessing steps described in appendix~\ref{ap:1} was applied to organiser development, training and test sets.  

\paragraph{Long Article Truncation}
The organiser annotation instructions emphasize the difficulty discriminating \emph{opinion} from \emph{reporting} and \emph{satire}, even for human annotators, due to subtle differences in how the opinionated direct speech is balanced and reported. When discriminating between opinion and satire, the instructions mention that even one opinionated sentence at the end of the article should overwrite the \emph{satire} class. Given the limit in terms of the input length for BERT models, for the articles that were longer than 512 tokens, we sequentially selected sentences from the beginning and the end of the article, preserving the original sentence order, until the length of 512 tokens was reached.

\paragraph{External Satire} Due to the lack of satire data, our training set was supplemented with 203 English-language satire articles from \citet{golbeck18-fakenews}.

\paragraph{Data Oversampling} The training data is severely imbalanced, with less than 6\% of articles annotated as \emph{satire} and 18\% of articles annotated as \emph{reporting}. Although the external satire data improved the balance for English, the performance of the satire class in other languages still remains inadequate. To address this, we performed oversampling for both \emph{satire} and \emph{opinion} classes by repeating random oversampling without replacement on the original data for a given language and class until the classes were balanced. For English \emph{satire} class, we applied the same approach but oversampled the external satire data mentioned earlier rather than the original training set. We also compared the effectiveness of the oversampling approach with the class weighting approach in our experiment, and the results showed a slight advantage for the oversampling approach on average.

%Our comparison of class weighting and oversampling for the transformer models suggests a slight average advantage of the oversampling approach. We therefore performed oversampling for both \emph{satire} and \emph{opinion} classes by repeating random oversampling without replacement over the original data for a certain language and class until the classes for that language were balanced. For English \emph{satire} class, the same approach was applied, however, the oversampling was performed over the external satire data mentioned above rather than the original training set.


%TODO
% \begin{itemize}
	
% 	\item Key details about preprocessing, hyperparameter tuning, etc. that a reader would need to know to replicate your experiments. If space is limited, some of the details can go in an Appendix.
	
% 	\item External tools/libraries used, preferably with version number and URL in a footnote.
	
% 	\item Summarize the evaluation measures used in the task.
	
% 	\item You do not need to devote much —if any— space to discussing the organization of your code or file formats.
	
% \end{itemize}


\subsection{Experimental Setup}\label{sec:t1_setup}
For the final submission models, mBERT transformer models were finetuned on the organiser training set and a part of the development set for 30 epochs with the learning rate of 1e-5, AdamW optimiser ($\varepsilon$=1e-8) and ReLU activation function. The organiser development set was split it into three parts, stratified by label and language. We then finetuned three models, using $\frac{1}{3}$ of the development set as a test and merging $\frac{2}{3}$ of the development set with the training data and shuffling the dataset. The held-out part of the development set was used to identify the language-specific best checkpoints for each model. We utilised the checkpoint with the best overall \fmacro on the held-out set to make predictions on the surprise languages. As described in section~\ref{sec:st1_system}, all articles were preprocessed and the training data was oversampled for \emph{satire} and \emph{reporting} classes of each language.

%To make the predictions on the surprise languages, we used the checkpoint with the best overall \fmacro on the held-out set. The training data was oversampled for \emph{satire} and \emph{reporting} classes of each language as described in section~\ref{sec:st1_system}. 

Adapter models were trained on the combined organiser and development set (resplit 80\% train, 10\% validation, 10\% held-out test, stratified by label), for 20 epochs with learning rate 1e-4, AdamW optimiser ($\varepsilon$=1e-8) and Tanh activation function in the classifier, and selected the checkpoint with the highest overall validation F1\textsubscript{macro} score. The above preprocessing and oversampling were not used, and articles were truncated at 512 tokens.

After submission, we conducted additional experiments using the organiser training and development sets for consistency. For monolingual models, all articles in the training set and the external satire were translated with Google Translate into the language of each monolingual model in question. Due to the character length limitation, particularly long articles were translated sentence-by-sentence. 

\subsubsection{Results and Reflections}\label{sec:t1_results}
The final submission results of the ensemble are listed in Table~\ref{tab:st1_final_results}.

%\begin{table}[]
%    \centering
%    \scalebox{0.8}{
%        \begin{tabular}{l r r}
%           \toprule
%           Language  & \fmacro & Place \\ \midrule
%           English  & 61.282 & 3 \\
%           French & 68.157 & 5 \\
%           German & 81.951 & $^\star$1 \\
%           Italian & 72.040 & 3 \\
%           Polish & 76.455 & 3 \\
%           Russian & 72.871 & 2 \\ \midrule
%           Spanish & 44.293 & 4 \\
%           Greek & 68.681	& 6 \\
%           Georgian & 96.268 & 2 \\
%           \bottomrule
%        \end{tabular}
%    }
%    \vspace{-2mm}
%    \caption{Subtask 1 final leaderboard results. $^\star$ joint.}
%    \vspace{-4mm}
%    \label{tab:st1_final_results}
%\end{table}



\begin{table}[]
    \centering
    \scalebox{0.8}{
        \begin{tabular}{l r r  l r r }
           \toprule
           Language  & \fmacro & Place &  Language  & \fmacro & Place\\ \midrule
           English  & 61.282 & 3 & Italian & 72.040 & 3 \\
           French & 68.157 & 5 & Polish & 76.455 & 3 \\
           German & 81.951 & $^\star$1 & Russian & 72.871 & 2 \\ \midrule
           Spanish & 44.293 & 4 & Greek & 68.681	& 6 \\
           Georgian & 96.268 & 2 & & & \\
           \bottomrule
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{Subtask 1 final leaderboard results. $^\star$ joint.}
    \vspace{-4mm}
    \label{tab:st1_final_results}
\end{table}

The final ensemble results achieved a higher \fmacro score than in the supplementary multilingual results in all languages, except Polish. In English, the ensemble achieved 25 F1 higher than the single mBERT transformer or adapter model. It should be noted that the final models were trained on both training and development, however since the development sets are only $\approx\frac{1}{3}$ the size of the training sets.

 In the absence of gold standard labels for the test set, it is difficult to analyse why the model achieved a high score in Georgian, despite being zero-shot. However, our ensemble predictions suggest that there are likely no satire articles in the Georgian test set, which was consistently the most difficult class to detect.

 Table~\ref{tab:st1-mono-vs-multi} shows the differences between monolingual and multilingual versions of adapter bottleneck and transformer models, evaluated against the organiser development set. The multilingual transformer models always perform better than monolingual ones, while for 4 out of 6 languages, adapter bottleneck models benefit from the monolingual setup. This may be due to using a fixed reduction factor across all languages. Interestingly, the mBERT model demonstrates the best average result in English for both transformer and bottleneck adapter models. For Italian, XLM-R yields the best results for both transformer and adapter bottleneck models. It is also notable that English is by far the worst language in all models, possibly because the model is overly focused on capturing semantic meaning and not as effective in topic classification. 
 
 Even though transformer XLM-R demonstrated significantly better results than transformer mBERT for Italian and German, these differences were only marginal in our main setting where the validation set was smaller, while the marginally better results for Russian were not observed at all. Given the above observations and the fact that XLM-R yielded higher \fmacro fluctuations, sometimes reaching 10\%, we  opted for mBERT model as our main submission.
 %\emph{too} good at capturing semantic meaning, so is worse at this type of topic classification.

\begin{table*}
    \centering
    \scalebox{0.8}{
        \begin{tabular}{l  r r r r r r}
            \toprule
             \multirow{2}{*}{Language} & \multicolumn{3}{c}{Transformer} & \multicolumn{3}{c}{Adapter} \\ \cmidrule(lr){2-4} \cmidrule(lr){5-7}
             & Monolingual & mBERT & XLM-R & Monolingual & mBERT & XLM-R  \\
            \midrule
            English & 30.0 ±  5.6 & $^*$\textbf{36.2 ±  2.5} & 36.1 ±  2.1 & 20.5 ±  3.3 & $^*$21.4 ±  6.0 & 20.0 ±  3.1\\
            French  & 51.2 ±  3.3 & 62.5 ±  4.6 & $^*$65.5 ±  4.3 & $^*$\textbf{68.3 ±  0.6} & 64.2 ±  0.8 & 61.3 ±  2.7\\
            German  & 59.9 ±  4.1 & 59.9 ±  5.0 & $^*$\textbf{66.9 ±  1.0} & $^*$65.7 ±  3.6 & 57.8 ±  2.8 & 62.0 ±  3.2 \\
            Italian & 56.7 ±  6.5 & 55.1 ±  4.3 & $^*$\textbf{72.6 ±  6.4} & 51.9 ±  4.7 & 47.8 ±  2.3 & $^*$60.3 ±  3.1\\
            Polish  & 71.7 ±  6.6 & $^*$\textbf{81.9 ±  3.7} & 79.4 ±  1.1 & $^*$77.6 ±  2.9 & 72.9 ±  5.1 & 76.7 ±  2.2\\
            Russian & 52.8 ±  8.8 & 52.9 ±  1.6 & $^*$54.7 ±  9.8 & $^*$\textbf{56.9 ±  9.5} & 48.3 ±  2.1 & 48.0 ± 0.8\\
            \bottomrule
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{Mean \fmacro $\pm$ 1std (over 3 runs) on subtask 1 organiser development set for multilingual and monolingual models for transformer and adapter-only architecture. $^*$ best per model per language. \textbf{best} overall per language.}
    \vspace{-4mm}
    \label{tab:st1-mono-vs-multi}
\end{table*}

\subsection{Post-competition Findings}
Since our final submission represented a fully multilingual model with zero-shot predictions for surprise languages, a natural question we wanted to test after the competition was whether any of the finetuned transformer models would have yielded better results on the translated test set (the `translate-test' setting). To do so, for each of the 6 language-specific best checkpoints within the three finetuned models, we ran the predictions on test sets for the remaining languages, including surprise languages, by translating the test set into the language for which the checkpoint was made. 

The surprising finding suggests that the translate-test English $\rightarrow$ Russian and Italian $\rightarrow$ French each improved the performance by 1\% on English and Italian respectively, while French $\rightarrow$ Russian improved \fmacro by over 6\%. 

Two out of the three surprise languages, Spanish and Greek, also benefited from being translated into other languages and tested using the corresponding best checkpoints. The Spanish $\rightarrow$ English setting showed particularly striking increase in \fmacro across all three transformer models, from 68.7 to 81.7, which is also 21\% higher than the score of the winning team for Spanish and is 20\% above the other post-competition results. Except for German, translating the Greek test set into the other 5 main languages and testing using the corresponding checkpoints also provided significant improvements in the range of 5\%-13\%, which is over 1\% above the result of the winning system and is the current leaderboard-best.


\section{System Description for Subtask 2}
\subsection{System Overview}
Two systems were used for submission, depending on the language. For English and the three surprise languages, we used \textbf{a monolingual English ensemble} of 3 RoBERTa-MUPPET\textsubscript{Large} models. For the remaining languages (French, German, Italian, Russian, Polish), we used \textbf{a multilingual ensemble} of 3 XLM-R\textsubscript{Large} models (with adapters and task-adaptive pre-training (TAPT)).

An overview of the two models is shown in Table~\ref{tab:st2_summary}.  
A key difference between these two systems is that the monolingual MUPPET models were trained using traditional finetuning of all parameters, whereas the XLM-R models 1) underwent task-adaptive pre-training; and 2) were finetuned using Pfeiffer bottleneck adapters. 

For both systems, we trained our models jointly on articles in all languages (using English translations for our monolingual model). This meant that we produced a single monolingual or multilingual system that was able to make predictions for all languages. 
We chose this approach of joint training across all languages in order to maximise the number of examples seen for each class, since the dataset for Subtask 2 is quite small, particularly when split by language. Our early experiments showed that this approach was superior to training models on the individual articles in each language. 
%NOTE - I never provide any results to demonstrate that joint finetuning is > than single language finetuning. 
%KB: This would be important to demonstrate for the PLOS paper


% INCLUDE? Footnote highlighting the difference between St1's monolingual (which uses a native model for each language) vs st2's monolingual, which uses English 'translate-test' approach for each language.  

\begin{table} 
    \center
    \scalebox{0.8}{
        \begin{tabular}{p{4cm} p{4cm}} 
    
        \toprule 
        MUPPET ensemble & XLM-R ensemble \\ 
        \midrule 
        MUPPET\textsubscript{LARGE} & XLM-R\textsubscript{LARGE} \\
    
         % & \\
        
        Monolingual (English) & Multilingual \\
        Trained on all articles (in translation) & Trained on all articles (original) \\
    
         % & \\
    
        No TAPT & TAPT \\
    
        Full finetuning & Adapter finetuning \\
    
        
        Ensemble size 3 &  Ensemble size 3 \\ 
    
        \midrule
        Submitted for & \\

        EN, EL, KA, ES & All other languages \\
        \bottomrule
        \end{tabular}
    }
    \vspace{-2mm}
    \caption{Summary of the monolingual vs multilingual systems submitted for subtask 2}
    \label{tab:st2_summary}
    \vspace{-4mm}
\end{table}


\subsubsection{The English Monolingual System}

\paragraph{Full Finetuning}
Our monolingual system used a finetuned RoBERTa-MUPPET\textsubscript{Large} \citep{aghajanyan-etal-2021-muppet} ensemble. RoBERTA-MUPPET improves on its baseline RoBERTa counterpart by adding an additional `pre-finetuning' stage of multi-task learning. 
% We opted to use traditional finetuning for our monolingual model because our cross-validation experiments showed that adding adapters worsened performance. We display results for this in the Apppendix Table~\ref{tab:st2_full_crossvalidation_results}. 
We opted not to use adapters, because our cross validation experiments showed this worsened performance (see Appendix Table \ref{tab:st2_full_crossvalidation_results}).

\paragraph{English Translations}
Because RoBERTa-MUPPET is a monolingual model, we translated all articles into English for training, and used them for finetuning alongside the original English articles. We performed inference on non-English languages by translating the articles into English: a `translate-test' approach.

% INCLUDE? explain why no adapters / TAPT (time constraints) 
%KB: This would be good to include in the PLOS paper

\subsubsection{Multilingual System}
We used XLM-RoBERTa\textsubscript{Large} \citep{conneau-etal-2020-unsupervised} for our multilingual model. Our system uses two techniques to improve performance: TAPT and adapter layers. 

\paragraph{Task-adaptive Pre-training}
We performed task-adaptive pre-training on the entire XLM-R model, following the regime of \citep{gururangan-etal-2020-dont}. Masked-language modelling was performed, using all available articles (including the organisers' development and test sets). 
% (organiser training set, organiser development set, and organiser test set)
We trained for 60 epochs with a learning rate of 1e-4 and batch size of 128 (for full hyperparameters, see Appendix Table ~\ref{tab:st2_tapt_hyperparams}). 

% An alternative way to perform TAPT would be to freeze the base model and train the adapters using MLM. 
% It would be possible, and faster, to task-adaptively pre-train our multilingual model using adapters. However, we opted not to do this because due to \citet{kim-etal-2021-revisiting} finding that pre-training adapters in this manner can decrease performance. 
TAPT could alternatively be performed by freezing the base model and training the adapters with MLM. This would be faster, but it has been found to sometimes decrease performance \cite{kim-etal-2021-revisiting}.

\paragraph{Adapters}
Our multilingual system used the Pfeiffer bottleneck adapter configuration, with a reduction factor of 8, which for XLM-R\textsubscript{Large} corresponds to a bottleneck hidden size of 128.

Although using adapters did result in a slightly improved performance, we found that their main advantage lays in their low parameter number, which allowed for faster training and more experimentation. 

\subsection{Ensemble}
Predictions made by our ensembles were decided by a majority vote. Each ensemble consisted of 3 individual models (MUPPET models for monolingual; adapter-finetuned XLM-R + TAPT for multilingual). Within each ensemble, two models were trained with class-weighting, and one-without.

% Our final predictions for English were made by an ensemble of three MUPPET models (two with class-weighting, one without). Our final predictions for the other languages were made by an ensemble of three adapter-tuned XLM-R + TAPT models (two with class-weighting, one without). 

%TO DO: 1. resolve where I talk about class weighting/ensembles. 2. Fix averages + write cross-validation results. 3. Add table in appendix regarding post-competition findings. 3. Fix references. 

\paragraph{Class Weighting} 
Class weighting helps to account for class imbalance by balancing impact on the loss of under- and over-represented classes. It multiplies the logits by class weights that are inversely proportional to the class's frequency in the dataset.
% In order to account for class imbalance, we trained models with and without class weighting. This approach increases the impact that under-represented classes on the loss function by multiplying the logits of each class based on the ratio of negative to positive class examples. 

% Applying class weighting in a joint multilingual setting can be potentially problematic, as it can lead to the prioritisation of certain languages over others depending on the distribution of class labels within each language. 

Overall \fmicro scores were similar for models with or without class weights. Class-weighting did help to improve performance on less frequent frames (such as \textit{Cultural Identity} and \textit{Public Opinion}), but at the expense of more frequent classes (such as \textit{Political}). Additionally, class weights were problematic in the joint language setting, causing varying performance across languages while maintaining similar overall \fmicro. (A comparison for XLM-R is provided in the Appendix Table~\ref{tab:st2_full_crossvalidation_results}.) For this reason, we chose to use a mix of class-weighted and non-class-weighted models for our ensembles in order to reduce the variance of our final systems. 


% See Appendix? 


% \subsection{where to put this? - Questions people might have when reading this paper}
% Why not perform TAPT on adapters instead of base model? 

% We considered performing task-adaptive pre-training for the adapters, rather than the base XLM-R model. However, we decided not to due to \cite{kim-etal-2021-revisiting} finding that pre-training adapters decreased performance. 

% Why not use adapters for MUPPET model? 

% We experimented with training the MUPPET model with adapters, but found that they worsened performance.

% Why not TAPT for MUPPET? 
% Time constraints. 
 

% Why not MUPPET for French, since it performed better in cross-validation? 

% Oversight. + not a large difference. 


\subsection{Experimental setup}
\subsubsection{Data Preprocessing}
For both monolingual and multilingual models, we cleaned and preprocessed the article text using a set of steps described in appendix~\ref{ap:1} and truncated to the first 512 tokens. For monolingual English models, we used Google Translate to produce English translations. 

\subsubsection{Data split}
For subtask 2, we performed 3-fold cross-validation to determine the configurations that would produce the best individual models for a majority-voting ensemble. For cross-validation, we merged the training data with the organiser development set and produced folds that were stratified by language. 

Based on these cross-validation findings we selected the model configuration to use for ensembles. 
We produced final models for the ensemble by training on the entire training and organizer development set (rather than a 2/3 fold), using the same hyperparameters.
%After cross-validation, we trained the final models for our ensemble on the entire merged training + development set (rather than the 2/3 folds from cross-validation). 
Although this meant that we did not have a validation set to judge our final models, we believe this was the right approach due to the small size of the dataset.  

\subsubsection{Hyperparameters}
 We finetuned our monolingual models for a fixed 20 epochs using a learning rate of 3e-5 (warm-up ratio 0.1, linear decay), a batch size of 8, and the AdamW optimiser. 

We used the same hyperparameters for adapter finetuning, except we raised the the learning rate to 1e-4, as this was found to be a good learning rate for a similar task by \citet{chalkidis-etal-2021-multieurlex}. 

%TODO: explain sweep of hyperparameters: mono 1e-5 - 1e-4, for adapters 5e-5 - 2e-4, and red facto 4, 8, 16. Also swept epochs to determine 20 is about right.

\begin{table*}
\centering
    \scalebox{0.8}{
    \begin{tabular}{l c c c c c c c}
    \toprule
    \textbf{Monolingual English} & EN & DE & FR & IT & PO & RU & Overall \fmicro \\

    \midrule

    Roberta-Large & 68.4 ± 2.0 & 63.5 ± 2.0 & 57.9 ± 2.9 & 60.9 ± 0.2 & 65.8 ± 3.4 & 54.5 ± 2.7 & 63.6 ± 0.1\\
    MUPPET-Large & \textbf{70.4 ± 2.0} & 62.1 ± 3.7 & \textbf{59.0 ± 0.9} & 58.3 ± 1.5 & 65.7 ± 0.9 & 52.9 ± 1.7 & 63.5 ± 0.7\\
    \midrule
    \textbf{Multilingual} & & & & & & & \\
    \midrule
    XLM-R & 68.3 ± 1.4 & 64.4 ± 1.4 & 58.5 ± 0.7 & 60.6 ± 0.5 & 66.5 ± 3.3 & 54.9 ± 2.0 & 64.0 ± 1.2\\
    XLM-R + TAPT + Adapters & 68.2 ± 0.9 & \textbf{65.0 ± 1.8} & 58.5 ± 2.8 & \textbf{61.0 ± 0.6} & \textbf{66.7 ± 3.0} & \textbf{55.7 ± 3.1} & \textbf{64.2 ± 0.3}\\
    \bottomrule
    %weird bolding: suggests that 
    \end{tabular}
    }

\vspace{-2mm}
\caption{Mean \fmicro scores of class-weighted models (over 3-fold cross-validation). For complete version with ablations and other configurations, see Table \ref{tab:st2_full_crossvalidation_results}}
\label{tab:st2_crossvalidation_results}
\vspace{-4mm}

\end{table*}

\subsubsection{Cross-validation Findings and Language Selection}
Table \ref{tab:st2_crossvalidation_results} displays a condensed summary of our cross-validation results (for full version, see Appendix Table~\ref{tab:st2_full_crossvalidation_results}).\footnote{This table shows the average performance of individual models trained during cross-validation, and not performance of any ensembles.}
\footnote{The Overall \fmicro column refers to the \fmicro of the entire validation fold and not the mean \fmicro score across languages.} 

%TODO: decide whether to use overall F1 micro or Average. 

For monolingual models, MUPPET\textsubscript{Large} achieves an \fmicro of 70.4 on English, outperforming the RoBERTa baseline by 2 points. Similarly, our XLM-R + TAPT + Adapters demonstrates small but consistent improvements over the multilingual XLM-R baseline across most languages. 

When comparing across monolingual and multilingual models, we see that XLM-R models are unable to compete with the performance of monolingual MUPPET for English. (They are, however, able to match the performance of their monolingual counterpart RoBERTa). In contrast to this, the multilingual models generally demonstrated better performance on non-English languages. This is reflected by the overall \fmicro scores: MUPPET's 63.5 vs XLMR+TAPT+Adapter's 64.2. Based on these results, we decided to use MUPPET for English and XLM-R for other languages. 

For the unseen languages, we decided to use the monolingual 'translate-test' approach based on additional holdout experiments that indicated better MUPPET performance. Although this decision enabled us to achieve 1st place on the evaluation test set for Greek and Georgian, our Post-competition Findings (section \ref{section:st2_post_comp_findings}) discovered that submitting our multilingual model may have achieved even better results.  

%Point for conclusion - using translations is a very viable method to achieve close-to multilingual perfomance for classification tasks. Even for languages with different scripts. It is also very competitive with 'zero-shot' cross lingual transfer. Probably in contrast with other tasks, where language matters more (e.g. news detection?).  

%Highlight that multilingual papers traditionally compare the performance of monolingual models trained on English vs Multilingual models trained on English vs Multilingual models trained on all languages. This is not a fair comparison for monolingual models, since they should be trained on translations as well. 


 % Our final predictions for English were made by an ensemble of three MUPPET models (two with class-weighting, one without). Our final predictions for the other languages were made by an ensemble of three adapter-tuned XLM-R + TAPT models (two with class-weighting, one without). 

% \subsubsection{Unseen languages: translate-test or zero-shot cross-lingual?}
% When we experimented with holding out one language from training (to simulate the ‘surprise’ language environment), we found that XLM-R model’s zero-shot cross-lingual transfer capabilities were competitive with ‘translate-to-English-then-test’ approaches. However, ultimately we elected to use our monolingual English MUPPET ensemble for the surprise languages due to its high baseline English performance. We performed inference on the surprise languages by translating the articles into English (from Spanish, Georgian and Greek). 

 
\subsection{Results and Reflections}

\begin{table}
    \centering
    \scalebox{0.8}{
    \begin{tabular}{l r r}
       \toprule
       Language  & Test \fmicro & Place \\ \midrule
       Monolingual MUPPET  & & \\
       \midrule
       English  & 57.895 & 1 \\
       \hdashline
       Spanish$^*$ & 50.829 & 3 \\
       Greek$^*$ & 54.630	& 1 \\
       Georgian$^*$ & 65.421 & 1 \\

       \midrule
       Multilingual XLM-R & & \\
       (+ TAPT + Adpt) & & \\
       \midrule
       French & 53.425 & 3 \\
       German & 65.251 & 3 \\
       Italian & 57.079 & 7 \\
       Polish & 64.516 & 2 \\
       Russian & 44.144 & 2 \\
       \bottomrule
    \end{tabular}
    }
    \vspace{-2mm}
    \caption{Subtask 2 final leaderboard results for monolingual and multilingual systems. $^*$Translated to English}
    \label{tab:st2_final_results}
    \vspace{-4mm}

\end{table}


% \begin{table*}
% \centering

%     \begin{tabular}{l r r r r r r r r r}
%     \toprule
%     \textbf{System} & \textbf{EN} & \textbf{DE} & \textbf{FR} & \textbf{IT} & \textbf{PO} & \textbf{RU} & \textbf{EL} & \textbf{ES} & \textbf{KA}\\

%     \midrule
%     MUPPET ensemble & 0.57895 & & & & & & 0.5463 & 0.50829 & 0.65421 \\
%     (ranking) & (1) & & & & & & (1) & (3) & (1) \\
%     \midrule
    
%     XLM-R ensemble &  & 0.65251 & 0.53425 & 0.57079 & 0.64516 & 0.44144 & &  & \\
%     (ranking) & & (3) & (3) & (7) & (2) & (2) & & & \\
%     \bottomrule 
%     \end{tabular}

% \caption{F1 micro scores on evaluation set. Ranking indicates placement on the leaderboard. Monolingual MUPPET performed predictions on English translations of EL, ES and KA articles.}
% \end{table*}

The scores and positions of our model are shown in Table \ref{tab:st2_final_results}.

% Our monolingual MUPPET system placed first for English, Greek and Georgian, and third for Spanish.  
% Our multilingual XLM-R system placed second for Polish and Russian, and third for French and German. 

The strong performance of our monolingual model, which achieved first place in 3 out 4 languages submitted, suggests that this approach of training with additional translations is a competitive approach for performing multilingual classification, especially for zero-shot cross-lingual situations. 

Though our multilingual model also performed well, we believe stronger results could be achieved by applying our monolingual approach to other languages: training a native monolingual model with translations.  

\subsection{Post-competition Findings} 
\label{section:st2_post_comp_findings}
\subsubsection*{ Is a `translate-test' monolingual model really better than a multilingual model?}

After the competition ended, we wanted to compare how our alternative non-submitted system performed on the test set.  

Surprisingly, we found that in contrast to our cross-validation experiments, our multilingual system outperformed the monolingual submission for English (mono: 57.895 vs multi: 58.475). It also performed better on two of the surprise languages: Greek (54.64 vs 58.0) and Spanish (52.023 vs 50.829). In contrast to the findings of \citet{xenouleas2022realistic}, who found translation-based approaches "vastly outperform cross-lingual finetuning with adapters", this suggests that the two approaches are competitive. It is difficult, however, to draw firm conclusions from this finding due to the small size of the unseen test set as well as the impact of TAPT. For more details, see Appendix \ref{ap:2}.

\section{System Description for Subtask 3}
\subsection{System Overview} \label{sec:st3-system-overview}
% We addressed two important factors in subtask 3: multilinguality and the high imbalance of the 23 multilabel classes. Our aim was to experiment with leveraging data from multiple languages to account for the small dataset size and underrepresented nature of most of the classes. Although the primary metric for this task is \fmicro, which weighs each class's contribution to the metric by its size, we aimed to develop a model that could identify all 23 classes, instead of solely focusing on the major ones.

%Despite the metric for subtask 3 being \fmicro, we aimed to develop a system to identify all the classes instead of just the largest
In subtask 3, our focus is not only to maximize the overall \fmicro, but also to ensure a balanced model performance across all classes.
\footnote{We further discuss this in appendix \ref{ap:s3-full-leaderboard}} %which is difficult due to the 23 classes in subtask 3 being highly imbalanced.
Due to the highly imbalanced nature of the 23 classes in subtask 3, achieving a balanced model performance across all classes is challenging.
%To account for this, we experiment with cross-lingual training and apply class weighting, similarly to subtask 2. We considered an oversampling approach that repeated single-class examples, but this slowed training for no benefit over class weighting.
To address this issue, similarly to subtask 2, we explored cross-lingual training and implemented class weighting. We also explored an oversampling technique, but it did not provide any additional benefits compared to class weighting and slowed down training. 

%Unlike the organiser baseline approach which discards paragraphs with no label, we instead assign them a class vector of zeros and add them to the train set, which significantly improves the performance for all languages
In contrast to the organiser baseline, which discards paragraphs without a label, we assigned them a class vector of zeros and included them in the training set. This method lead to a significant improvement in performance across all languages.\footnote{A comparison of these approaches are in appendix \ref{ap:classless}.}.


% We found that all languages benefited from cross-lingual training except for English.

% Several factors could contribute to this. For instance, Table \ref{tab:st3-labelled-vs-unlabelled} shows that the English training set is significantly larger than the other languages.
% The three surprise languages were translated into English and labelled with a monolingual model, since zero-shot experiments on the development set resulted in sub-optimal performance.
%To assess the performance of unseen languages, we trained multilingual models with each language held out. Every language performed significantly worse when held out, so the three unseen languages were translated into English for the final predictions.
We evaluated the performance of the models on unseen languages by training multilingual models while holding out each language. However, the performance significantly decreased for every language when held out. Therefore, we translated the articles of the three unseen languages into English for the final predictions.


Our final models were a \textbf{RoBERTa\textsubscript{BASE}} for the submission of English and translated unseen languages, and an \textbf{mBERT\textsubscript{BASE}} for the remaining languages.



\subsection{Experimental Setup}

% Initially, our experiments focused on hyperparameter search for the English language. Prior to having access to the official development set, we used 30\% of the train set as development and ran ablation experiments. After settling on the hyperparameters described at section \ref{sec:st3-system-overview}, we began experimenting with techniques to improve performance on the under-represented classes.

 \textbf{RoBERTa\textsubscript{BASE}} was finetuned for 20 epochs only on English data\footnote{Unlike subtask 2, we finetuned the model only with the original English dataset without adding other translated languages.}, with a batch size of 32, truncated to 256 tokens, and AdamW optimizer with a learning rate of 0.00005, 20\% of the training steps as linear warm-up and weight decay of 0.1. A classification threshold of 0.4 was used.

\textbf{mBERT\textsubscript{BASE}} was finetuned on all languages combined and shuffled, with the same hyperparameters as the RoBERTa\textsubscript{BASE} model, except a batch size of 16. 

% Additionally to class weighting, we also experimented with oversampling techniques, although they were not simple to apply in multilabel settings since replicating an example could impact multiple classes. We tried simple approaches such as only replicating examples that belong to only one class. This showed similar improvements, although it significantly increased the training time. Thus, we opted to use class weighting. Another effort to improve performance with dataset manipulation is the addition of training examples that were not assigned a class by the annotators, as previously explained in Section \ref{sec:st3-system-overview}.

We conducted an experiment to explore cross-lingual training, 
%To experiment with cross-lingual training, 
we ran three different random seed initializations for each run and compared monolingual vs. multilingual models for each language with fixed hyperparameters. 
The monolingual models were finetuned and validated with only one language using either RoBERTa-Base for English or mBERT for the other languages, while the multilingual version was an mBERT finetuned with all the languages and validated on each language separately.
% We also attempted to use data augmentation in the form of translations with the Google Translate API. We translated all the train sets into English and trained a RoBERTa-Base model, evaluating it with the original English development set. However, since this approach didn't outperform our previous experiments, we didn't explore it any further.


For the final submissions, we merged the training and development sets and finetuned the models without validation data. The model with the lowest training loss across all epochs was selected as the best model. We used the random seed that produced the best \fmicro in the previous development set experiments while training the final model.

\subsubsection{Results} \label{sec:st3-results}
Table \ref{tab:st3_final_results} presents the final leaderboard results for subtask 3. Although the main metric for this subtask is \Fmicro, we highlight that our placings for subtask 3 improve considerably when measuring through \Fmacro, placing top 5 in all languages except the three surprise languages and first place for Italian and French, as shown in table \ref{tab:st3-final-results-micromacro} in the appendix.

Results for the monolingual and multilingual experiments on the official development set are displayed on table \ref{tab:st3-dev-f1micro}, where we report the \fmicro for each language. Scores are reported over three runs on different random seed initializations. Notice that English is the only language in which the monolingual model outperforms the multilingual version.

\begin{table}[]
\begin{center}
\scalebox{0.8}{
\begin{tabular}{lcc}
\toprule
\multirow{2}{*}{Language} & \multicolumn{2}{c}{\fmicro $\pm$ 1 std}     \\ \cmidrule(lr){2-3} 
 & Monolingual         & Multilingual         \\ \midrule
English & \textbf{36.2 ± 0.3} & 31.8 ± 0.6           \\
French  & 40.5 ± 0.4          & \textbf{43.4 ± 0.4}  \\
German  & 36.9 ± 0.5          & \textbf{40.9 ± 0.7}  \\
Italian & 43.4 ± 1.3          & \textbf{47.5 ± 0.3}  \\
Polish  & 28.9 ± 0.8          & \textbf{30.2 ± 0.8}  \\
Russian & 31.5  ± 1.3         & \textbf{37.5  ± 1.7} \\ \bottomrule
\end{tabular}
}
\vspace{-2mm}
\caption{Monolingual vs. multilingual \fmicro scores on the development set for each language on subtask 3. Best \fmicro per language are marked as \textbf{bold}.}
\label{tab:st3-dev-f1micro}
\vspace{-4mm}
\end{center}
\end{table}

Table \ref{tab:st3-zeroshot} shows the \fmicro for the zero-shot experiments, in which a multilingual model was finetuned with all languages except the one being evaluated. By comparison with table \ref{tab:st3-dev-f1micro}, we see that zero-shot drastically hinders the performance on all languages. For this reason, we decided to translate the test sets for the three surprise languages into English and perform inference using the English monolingual model. We were unable experiment with translating into different languages other than English due to time constraints.

\begin{table}[]
\begin{center}
\scalebox{0.8}{
\begin{tabular}{@{}ll@{}}
\toprule
\multirow{2}{*}{Language} & \fmicro $\pm$ 1 std \\ \cmidrule(lr){2-2}
    & Zero-Shot      \\ \midrule
English & 21.6 ± 0.4  \\
French  & 35.8 ± 0.7  \\
German  & 28.0 ± 0.6  \\
Italian & 34.3  ± 0.4 \\
Polish  & 21.7 ± 0.6  \\
Russian & 18.6 ± 1.1  \\ \bottomrule
\end{tabular}
}
\vspace{-2mm}
\caption{Subtask 3 \fmicro for the zero-shot experiments.}
\vspace{-4mm}
    \label{tab:st3-zeroshot}
\end{center}
\end{table}


% Due to space constraints, the final leaderboard results according to \fmacro, fine-grained classification results and other supplementary data is left in the appendix.

\begin{table}[]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{l r r}
       \toprule
       Language  & Test \fmicro & Place \\ \midrule
       Monolingual RoBERTa\textsubscript{BASE} & & \\
       \midrule
       English  & 36.802 & 2 \\
       \hdashline
       Spanish$^*$ & 27.497 & 9 \\
       Greek$^*$ & 17.426	& 7 \\
       Georgian$^*$ & 24.911 & 10 \\

       \midrule
       Multilingual mBERT & & \\
       \midrule
       French & 41.436 & 4 \\
       German & 44.726 & 6 \\
       Italian & 52.494 & 3 \\
       Polish & 34.7 & 7 \\
       Russian & 31.841 & 5 \\
       \bottomrule
    \end{tabular}
    }
    \vspace{-2mm}
    \caption{Subtask 3 final leaderboard results for monolingual and multilingual systems. $^*$Translated to English.}
    \vspace{-4mm}
    \label{tab:st3_final_results}
\end{table}

% \begin{table}[]
% \centering
% \scalebox{0.8}{
% \begin{tabular}{lrc}
% \toprule                                  
% Language & \multicolumn{1}{l}{Test F1\textsubscript{micro}} & \multicolumn{1}{l}{Place} \\ \midrule
% English  & 36.802                            & 2                        \\
% French   & 41.436                            & 4                        \\
% German   & 44.726                            & 6                        \\
% Italian  & 52.494                            & 3                        \\
% Polish   & 34.7                              & 7                        \\
% Russian  & 31.841                            & 5                        \\ \midrule
% Greek    & 17.426                            & 7                        \\
% Spanish  & 27.497                            & 9                        \\
% Georgian & 24.911                            & 10                       \\ \bottomrule
% \end{tabular}
% }
% \vspace{-2mm}
% \caption{Subtask 3 final leaderboard results.}
% \vspace{-4mm}
%     \label{tab:st3-f1microresults}
% \end{table}

% \section{System Overview}
% \subsection{Subtask 1}


% \begin{itemize}
% 	\item Key algorithms and modeling decisions in your system; resources used beyond the provided training data; challenging aspects of the task and how your system addresses them. This may require multiple pages and several subsections, and should allow the reader to mostly reimplement your system’s algorithms.
	
% 	\item Use equations and pseudocode if they help convey your original design decisions, as well as explaining them in English. If you are using a widely popular model/algorithm like logistic regression, an LSTM, or stochastic gradient descent, a citation will suffice—you do not need to spell out all the mathematical details.
	
% 	\item Give an example if possible to describe concretely the stages of your algorithm.
	
% 	\item If you have multiple systems/configurations, delineate them clearly.
	
% 	\item This is likely to be the longest section of your paper.
	
% \end{itemize}
% \subsection{Subtask 2}

% DESCRIBE THE APPROACH FOR THE SURPRISE LANGUAGES

% \section{Experimental Setup}
% \subsection{Data split}\label{sec:datasplit}
% Each of the systems that we explored was trained using different split techniques for the training and evelopment set. 

% For sub-task~2, we merged the organiser development set with the training data and performed 3-fold cross-validation (stratified by language). For sub-task~3 we used each provided split ordinarily, training with the train split and evaluating with the development split. Our final models for sub-task~2 and sub-task~3 were trained on all the data in the training and development datasets.


% \subsection{Model Architecture}
% \subsubsection{Subtask 1}


% \section{Results}
% In this section, we present the results of the comparison between monolingual and multilingual approaches as well as the official leaderboard results on the organiser test set per subtask. 
% As shown by \cite{reimers-gurevych-2017-reporting}, non-deterministic systems cannot be reliably compared based on a single score. Therefore, for the results comparing monolingual and multilingual models, we present mean scores and standard deviations calculated as a result of several runs of each system. For the official leaderboard results, we provide a single score based on which our team was ranked, however, some of our submissions represent ensemble models that take non-determinism into account.

% The official results for subtask 1 rank the systems based on F$_{1_{macro}}$ scores, while for sub-tasks~2 and ~3, F$_{1_{micro}}$ scores were considered for ranking.
% %TODO official results: number of 1, 2, 3,4,5 places
% %TODO average results per task
% \begin{itemize}
% 	\item Main quantitative findings: How well did your system perform at the task according to official metrics? How does it rank in the competition?
	
% 	\item Quantitative analysis: Ablations or other comparisons of different design decisions to better understand what works best. Indicate which data split is used for the analyses (e.g. in table captions). If you modify your system subsequent to the official submission, clearly indicate which results are from the modified system.
	
% 	\item Error analysis: Look at some of your system predictions to get a feel for the kinds of mistakes it makes. If appropriate to the task, consider including a confusion matrix or other analysis of error subtypes—you may need to manually tag a small sample for this.
%     %Olesya: maybe we can make error analysis for subtask 1 and 3 where we used the dev set for final submission as we do not have labels on the test set...We can select a couple of intetesting cases to perform the qualitative analysis when comparing different models that we tried: X was mislassified by the model that did not use upsampling, but was classified as Y
	
% \end{itemize}

\section{Conclusion}
We presented three systems aimed at solving three subtasks within SemEval-2023 Task 3. Our systems applied a variety of state-of-the-art techniques including adapters and TAPT, and consistently achieved a high rank across all available languages, including zero-shot low-resource languages. We additionally presented an analysis of the viability of monolingual vs multilingual approaches for each subtask, and found that the results of the comparison vary depending on the subtask. For subtask 1, multilingual transformer models demonstrate better average performance than monolingual models with translations. A similar effect was observed for subtask 2 and subtask 3 where multilingual settings achieved better performance than monolingual ones in all languages except English. We found the impact on bottleneck adapters to be unpredictable across tasks -- despite performing on average better for monolingual models in subtask~1, they were more beneficial for multilingual models in subtask~2 (and hindered monolingual performance). Finally, we presented post-competition findings, which suggest that subtask~2 would have benefited from a zero-shot prediction using multilingual models, while subtask~1 could have achieved much better results with the `translate-test' approach. Further analysis of this will be possible when test labels are released.



\section{Acknowledgments}
This work has been co-funded by the European Union under the Horizon Europe  vera.ai (grant 101070093) and Vigilant (grant 101073921) projects and the UK’s innovation agency (Innovate UK) grants 10039055 and 10039039.


\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix
\section{Language Models Used}

The models used in each subtask are shown in Table \ref{tab:model-list}.

% \begin{table*}
%     \centering
%     \begin{tabular}{l l l}
%         \toprule
%         Language & Huggingface Model Name & Publication  \\
%         \midrule
%         English & \texttt{bert-base-cased} & \citet{devlin-etal-2019-bert} \\
%         French & \texttt{camembert-base} & \citet{martin-etal-2020-camembert} \\
%         German & \texttt{deepset/gbert-base} & \citet{chan-etal-2020-germans} \\
%         Italian & \texttt{dbmdz/bert-base-italian-cased} & - \\
%         Polish & \texttt{dkleczek/bert-base-polish-cased-v1} & - \\
%         Russian & \texttt{DeepPavlov/rubert-base-cased} & \citet{kuratov-19-rubert} \\
%         \midrule
%         Multilingual & \texttt{bert-base-multilingual-cased} & \citet{devlin-etal-2019-bert} \\
%         % Multilingual & \texttt{xlm-roberta-base} & \citet{conneau-etal-2020-unsupervised} \\
%         \bottomrule
%     \end{tabular}
%     \caption{Monolingual and multilingual models used in Subtask 1 for comparison experiments. Adapted from \citet{chalkidis-etal-2021-multieurlex}.}
%     \label{tab:st1-models}
% \end{table*}

\begin{table*}[]
\centering
\scalebox{0.8}{
    \begin{tabular}{l l l p{0.2cm}p{0.2cm}p{0.2cm}}
    \toprule
    Language     & Huggingface Model Name             & Publication                & \multicolumn{3}{l}{Subtasks} \\ \midrule
    English      & \texttt{bert-base-cased}                    & \citet{devlin-etal-2019-bert}      & 1 &&         \\ 
    English & \texttt{roberta-base}                            & \citet{liu2019roberta}              & && 3*             \\
    English & \texttt{roberta-large}                            & \citet{liu2019roberta}              & & 2 &              \\
    English & \texttt{MUPPET-large}                            & \citet{aghajanyan-etal-2021-muppet}   & & 2$^\star$ &              \\
    
    French       & \texttt{camembert-base}                     & \citet{martin-etal-2020-camembert} & 1 &&             \\
    German       & \texttt{deepset/gbert-base}                 & \citet{chan-etal-2020-germans}     & 1   &&             \\
    Italian      & \texttt{dbmdz/bert-base-italian-cased}      & -                          & 1 &&               \\
    Polish       & \texttt{dkleczek/bert-base-polish-cased-v1} & -                          & 1  &&          \\
    Russian      & \texttt{DeepPavlov/rubert-base-cased}       & \citet{kuratov-19-rubert}          & 1 &&                \\
    Multilingual & \texttt{bert-base-multilingual-cased}       & \citet{devlin-etal-2019-bert}      & 1$^\star$&&3*                \\
    Multilingual & \texttt{xlm-roberta-base} & \citet{conneau-etal-2020-unsupervised} & 1 &  & 3 \\
    Multilingual & \texttt{xlm-roberta-large} & \citet{conneau-etal-2020-unsupervised} & & 2$^\star$ & \\
    \bottomrule
    \end{tabular}
}
\caption{Models used in each subtask. $^\star$ for final submission. Based on model selection of \citet{chalkidis-etal-2021-multieurlex}.}
\label{tab:model-list}
\end{table*}




% \section{Subtask 3}
% Table \ref{tab:st3-labelled-vs-unlabelled-size} displays the comparison between the train set sizes of all languages when adding examples that weren't assigned a class during labelling. Table \ref{tab:st3-labelled-vs-unlabelled-f1micro} shows the f1-micro for the models trained with and without the addition of the classless examples. With the exception of Russian and German, all other languages benefit from the addition of these examples.

% Table \ref{tab:st3-error-analysis} shows a more in-depth view of the performance of the monolingual classifier for the English language. Results were obtained for the best run of a single random seed over 20 epochs. Note that the model scored $0.0$ F1-score for 6 classes.

\section{Article Preprocessing}\label{ap:1}
Articles were preprocessed with the following steps, for all languages:
\begin{itemize}
    \item add a full stop at the end of the title.
    \item remove duplicate sentences directly following each other;
    \item remove the @ symbol from twitter handles;
    \item remove hyperlinks to websites and images;
\end{itemize}
English articles were further preprocessed:
\begin{itemize}
    \item remove lines indicating the possibility to share the article on different social media platforms;
    \item remove sentences suggesting the user take part in  online polls, comments, or advertisements;
    \item remove sentences indicating the terms of use;
    \item remove sentences indicating the licenses and containing phrases such as `reprinted with permission', `posted with permission' and `all rights reserved';
    \item remove sentences relating to the article author biographies
\end{itemize}

\section{Subtask~1}
\subsection{Language-specific performance after each epoch on development set}\label{ap:4}
\begin{figure}[!h]
    \begin{tikzpicture}
        \begin{axis}[
            no markers,
            xlabel=Epoch,
            ylabel={\fmacro},
            ymin=0,
            ymax=100,
            legend style={at={(0.5,-0.2)},anchor=north},
            legend columns=3,
            cycle list={
                blue,
                red,
                Green,
                orange,
                violet,
                black
            }
        ]
        \addplot file {figures/st1_langs/English.txt}; \addlegendentry{English}
        \addplot file {figures/st1_langs/French.txt}; \addlegendentry{French}
        \addplot file {figures/st1_langs/German.txt}; \addlegendentry{German}
        \addplot file {figures/st1_langs/Italian.txt}; \addlegendentry{Italian}
        \addplot file {figures/st1_langs/Polish.txt}; \addlegendentry{Polish}
        \addplot file {figures/st1_langs/Russian.txt}; \addlegendentry{Russian}

        \addplot[blue, opacity=0.3] coordinates {(12,0) (12,45.51)};
        \addplot[red, opacity=0.3] coordinates {(9,0) (9,63.30)};
        \addplot[Green, opacity=0.3] coordinates {(19,0) (19,64.37)};
        \addplot[orange, opacity=0.3] coordinates {(17,0) (17,68.56)};
        \addplot[violet, opacity=0.3] coordinates {(4,0) (4,88.16)};
        \addplot[black, opacity=0.3] coordinates {(20,0) (20,56.44)};
        \end{axis}
    \end{tikzpicture}
    \caption{Validation \fmacro of each language over time, with maximal epoch indicated.}
    \label{fig1:st1_train}
\end{figure}

Figure~\ref{fig1:st1_train} shows \fmacro scores on the held-out development set for one of the finetuned transformer models. As can be seen, Polish reaches its best performance quite early on, while German and Russian need more than 17 epochs to achieve the best score.

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{figures/st1_training_dev.png}
%     \caption{Evaluation of mBERT model on each language in the held-out development set at each checkpoint. We randomly chose one transformer model in the ensemble for illustration purposes.}
%     \label{fig1:st1_train}
% \end{figure}



% \pgfplotsset{width=8cm,compat=1.9}



%Here ends the 3D plot




\begin{comment}
\section{Team rankings}
\begin{table*}[]
\centering
\begin{tabular}{@{}lrr@{}}
\toprule                                                 \\
       {Team} & \multicolumn{1}{l}{Average rank} & \multicolumn{1}{l}{Number of languages} \\ \midrule
MLModeler5          &       2.00    &   1\\
Hitachi             &       3.17    &   6\\
SheffieldVeraAI\footnote{Referred to as 'vera' during the submission stage}      &       3.22    &   9\\
DSHacker            &       4.67    &   9\\
UMUTeam             &       4.67    &   9\\
MELODI              &       5.11    &   9\\
SharoffAndLepekhin  &       5.89    &   9\\
Unisa               &       6.00    &   1\\
QCRITeam            &       6.67    &   9\\
SinaaAI             &       7.56    &   9\\
QUST                &       7.78    &   9\\
HHU                 &       8.17    &   6\\
UnedMediaBiasTeam   &       10.11   &   9 \\
Riga                &       10.89   &   9\\
kb                  &       11.50   &   2\\
UM6P                &       12.00   &   1\\
SATLab              &       12.13   &   8\\
Baseline            &       12.89   &   9\\
Spoke               &       13.00   &   2\\
FramingFreaks       &       14.67   &   9\\
MaChAmp             &       15.22   &   9\\
JUSTR00             &       15.75   &   4\\
E8IJS               &       16.78   &   9\\
ssnNlp              &       20.00   &   1\\
UTBNLP              &       21.00   &   1\\
krasavchiki         &       23.00   &   1\\
IA2022Grupa1        &       23.00   &   1\\
NLUBot101           &       23.00   &   1\\
                   \\ \bottomrule
\end{tabular}
\caption{The table displays the team ranks for subtask 1 averaged across all the languages. Since not all teams submitted predictions for all the languages, the right column represents the number of languages the team addressed out of 9.}
\label{tab:rank_st1}
\end{table*}

\begin{table*}[]
\centering
\begin{tabular}{@{}lrr@{}}
\toprule                                                 \\
       {Team} & \multicolumn{1}{l}{Average rank} & \multicolumn{1}{l}{Number of languages} \\ \midrule
SheffieldVeraAI\footnote{Referred to as 'vera' during the submission stage}                                        &      2.56    &    9\\
MarsEclipse          &      2.78    &    9\\
TeamAmpa             &      4.33    &    9\\
QCRITeam             &      4.67    &    9\\   
Hitachi              &      5.00    &    6\\
UMUTeam              &      5.89    &    9\\
PolarIce             &      6.22    &    9\\
BERTastic            &      7.11    &    9\\
TheSyllogist         &      8.67    &    9\\
QUST                 &      10.11   &    9\\
Riga                 &      11.56   &    9\\
MaChAmp              &      12.44   &    9\\
ACCEPT               &      12.44   &    9\\
MLModeler5           &      13.00   &    1\\
SATLab               &      13.13   &    8\\
SharoffAndLepekhin   &      14.14   &    7\\
Baseline             &      14.33   &    9\\
FramingFreaks        &      14.56   &    9\\
JUSTR00              &      15.00   &    1\\
SinaaAI              &      17.56   &    9\\
UTBNLP               &      19.00   &    9\\
DigDemLab            &      19.00   &    6\\
IA2022Grupa1         &      20.00   &    1\\
                   \\ \bottomrule
\end{tabular}
\caption{The table displays the team ranks for subtask 2 averaged across all the languages. Since not all teams submitted predictions for all the languages, the right column represents the number of languages the team addressed out of 9.}
\label{tab:rank_st2}
\end{table*}


\begin{table*}[]
\centering
\begin{tabular}{@{}lrr@{}}
\toprule                                                 \\
       {Team} & \multicolumn{1}{l}{Average rank} & \multicolumn{1}{l}{Number of languages} \\ \midrule
KInIT                &      1.67    &   9\\
TeamAmpa             &      3.67    &   9 \\  
QCRITeam             &      4.22    &   9  \\
NAP                  &      4.33    &   9\\
APatt                &      5.83    &   6\\
SheffieldVeraAI\footnote{Referred to as 'vera' during the submission stage}                             &      5.89    &   9\\
ACS                  &      7.22    &   9\\
DSHacker             &      7.50    &   6\\
NLUBot101            &      7.56    &   9 \\
MaChAmp              &      9.78    &   9\\
SharoffAndLepekhin   &      9.86    &   7\\
NL4IA                &      11.00   &   1\\
Riga                 &      11.67   &   9\\
Unisa                &      12.00   &   1\\
kb                   &      13.00   &   9\\
ReDASPersuasion      &      13.00   &   6\\
CLAC                 &      13.67   &   9\\
SATLab               &      14.38   &   8\\
UnedMediaBiasTeam    &      15.33   &   9\\
Baseline             &      15.56   &   9\\
SinaaAI              &      17.78   &   9\\
QUST                 &      18.22   &   9\\
IA2022Grupa1         &      20.00   &   1\\
                   \\ \bottomrule
\end{tabular}
\caption{The table displays the team ranks for subtask 3 averaged across all the languages. Since not all teams submitted predictions for all the languages, the right column represents the number of languages the team addressed out of 9.}
\label{tab:rank_st3}
\end{table*}
\end{comment}

\section{Subtask 2}\label{ap:2}
Hyperparameters for Subtask 2 TAPT are shown in Table \ref{tab:st2_tapt_hyperparams}.
The full table of cross-validation results is shown in Table \ref{tab:st2_full_crossvalidation_results}.

\begin{table}[]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{l c}
       \toprule
       Epochs & 60 \\
       Effective Batch Size & 128 \\ 
       Max learning rate & 1e-4 \\
       Warmup ratio & 0.06, linear \\
       Learning rate decay & linear \\
       Optimiser & AdamW \\ 
       Adam epsilon & 1e-6 \\
       Adam beta weights & 0.9, 0.98 \\
       Weight decay & 0.01  \\
       \bottomrule
    \end{tabular}
    }
    \vspace{-2mm}
    \caption{Subtask 2: TAPT Hyperparameters}
    \vspace{-4mm}
    \label{tab:st2_tapt_hyperparams}
\end{table}

\subsection{Post-competition Findings}
Table \ref{tab:st2_post} shows the full set of post-competition results. The multilingual model outperforms our official monolingual submission for test set English, Greek, and Spanish, but is much worse for Georgian. Conversely, our monolingual model does better than our official multilingual submission for French and Italian. This suggests that neither monolingual nor multilingual models are consistently better than the other across all languages. 

Traditional zero-shot cross-lingual experiments focus on finetuning a multilingual model on a single source language and testing in a target language. In our situation, we jointly finetune on multiple languages, which encourages the model to retain multilingual representations, even for languages not in its training, thus improving its zero-shot cross-lingual capabilities. This may help to explain the improved performance of our multilingual model. However, it is important to note that these results are not representative of true zero-shot classification, since our multilingual model did perform task-adaptive pre-training on articles from the surprise languages. Unfortunately, because the organisers have not released labels for the test set, we are unable to perform error analysis. As mentioned in the main section, the small size of the test set also makes it difficult to draw firm conclusions on whether translate-test is better than multilingual zero-shot classification.  

\begin{table}[]
    \centering
    \scalebox{0.8}{
        \begin{tabular}{l l l}
       \toprule
       Language & Multilingual & Monolingual \\ 
       \midrule
       \midrule
       English  & \textbf{58.475} & 57.895 (1) \\
       French & 53.425 (3) & \textbf{54.181} \\
       German & \textbf{65.251} (3) & 62.069\\
       Italian & 57.079 (7) & \textbf{60.577}\\
       Polish & \textbf{64.516} (2) & 63.581\\
       Russian & \textbf{44.144} (2) & 40.8\\
       \midrule 
       Spanish & \textbf{52.023} & 0.50829 (3)\\
       Greek & \textbf{58.0} & 0.5463 (1)\\
       Georgian & 60.87 & \textbf{0.65421} (1)\\
       \bottomrule
       \end{tabular}
    }
    \vspace{-2mm}
    \caption{Subtask 2: \fmicro Perfomance on test set - post-competition comparison. () indicates ranking for official submissions.}
    \vspace{-4mm}
    \label{tab:st2_post}
    
\end{table}


\begin{table*}
\centering
\scalebox{0.8}{
    \begin{tabular}{l c c c c c c c}
    \toprule
    \textbf{Monolingual English} & EN & DE & FR & IT & PO & RU & Overall \fmicro \\

    \midrule

    Roberta-Large & 68.4 ± 2.0 & 63.5 ± 2.0 & 57.9 ± 2.9 & 60.9 ± 0.2 & 65.8 ± 3.4 & 54.5 ± 2.7 & 63.6 ± 0.1\\
    MUPPET-Large & 70.4 ± 2.0 & 62.1 ± 3.7 & 59.0 ± 0.9 & 58.3 ± 1.5 & 65.7 ± 0.9 & 52.9 ± 1.7 & 63.5 ± 0.7\\
    MUPPET-Large + Adapters & 68.0 ± 1.0 & 59.5 ± 1.6 & 54.5 ± 1.9 & 58.0 ± 0.7 & 61.9 ± 2.1 & 51.0 ± 3.7 & 61.1 ± 0.9 \\
    \midrule
    \textbf{Multilingual Models} & & & & & & & \\
    \midrule
    XLM-R & 68.3 ± 1.4 & 64.4 ± 1.4 & 58.5 ± 0.7 & 60.6 ± 0.5 & 66.5 ± 3.3 & 54.9 ± 2.0 & 64.0 ± 1.2\\
    XLM-R + Adapters & 69.0 ± 1.2 & 64.0 ± 1.4 & 58.4 ± 3.0 & 61.3 ± 0.9 & 67.4 ± 1.0 & 53.1 ± 2.7 & 64.3 ± 0.4\\
    XLM-R + TAPT + Adapters & 68.2 ± 0.9 & 65.0 ± 1.8 & 58.5 ± 2.8 & 61.0 ± 0.6 & 66.7 ± 3.0 & 55.7 ± 3.1 & 64.2 ± 0.3\\

    \midrule

    XLM-R (no class weights) & 68.8 ± 1.7 & 57.1 ± 2.2 & 61.6 ± 4.0 & 61.7 ± 1.2 & 67.4 ± 2.1 & 57.0 ± 2.2 & 65.1 ± 0.2\\
    \bottomrule
    
    \end{tabular}
}


\caption{Full version of Subtask 2 cross-validation results. Comparison of averaged \fmicro scores on 3-fold cross-validation (merged training and organiser-dev set). All models have class-weighting, except where indicated otherwise.}
\vspace{-4mm}



\label{tab:st2_full_crossvalidation_results}
\end{table*}

\section{Subtask 3}\label{ap:3}
\subsection{Training With vs. Without Non-Labelled Examples} \label{ap:classless}
Table \ref{tab:st3-labelled-vs-unlabelled-size} displays the sizes of the train set for each language without adding non-labelled examples vs. adding them. Table \ref{tab:st3-labelled-vs-unlabelled-f1micro} shows the \fmicro results of both approaches, with means and stds computed over three random seed initializations. Note that adding the non-labelled examples contributes to a considerable increase in performance for all languages, particularly English, which is also the language that had the biggest increase in train set size.

\begin{table}
\centering
\scalebox{0.8}{
    \begin{tabular}{l r r}
    \toprule
    \multirow{2}{*}{Language} & \multicolumn{2}{c}{Trainset Size}                                                  \\ \cmidrule(lr){2-3}
    & \multicolumn{1}{l}{Without Non-Labelled} & \multicolumn{1}{l}{With Non-Labelled} \\ \midrule
    English & 3760                                  & 9498 (+152\%)  \\
    French  & 1693                                  & 2259 (+33\%)                       \\
    German  & 1252                                  & 1555 (+24\%)                       \\
    Italian & 1745                                  & 2623 (+50\%)                       \\
    Polish  & 1232                                  & 2310 (+32\%)                       \\
    Russian & 1245                                  & 1962 (+57\%)                       \\ \bottomrule
    \end{tabular}
}
\caption{Subtask 3 train set sizes for each language without and with the addition of examples that weren't assigned a class during labelling.}
\label{tab:st3-labelled-vs-unlabelled-size}
\end{table}

\begin{table}
\centering
\scalebox{0.8}{
\begin{tabular}{@{}lcc@{}}
\toprule
\multirow{2}{*}{Language} & \multicolumn{2}{c}{\fmicro $\pm$ 1 std}                                                                       \\ \cmidrule(lr){2-3}
 & Without Classless                            & With Classless       \\ \midrule
English                      & 27.1  ±  1.0                                 & \textbf{36.2 ± 0.3}  \\
French                       & 41.3  ± 0.1                                  & \textbf{43.4 ± 0.4}  \\
German                       & 40.8  ± 0.1                         & \textbf{40.9 ± 0.8}  \\
Italian                      & 44.1  ± 0.6                                  & \textbf{47.5 ± 0.4}  \\
Polish                       & 27.8  ± 0.9                                  & \textbf{30.2 ± 0.1}  \\
Russian                      & 35.7  ±  0.9                        & \textbf{37.5  ± 2.0} \\ \bottomrule
\end{tabular}
}
\caption{Subtask 3 \fmicro for best model configurations for each language with and without the addition of classless examples. Best \fmicro per language are marked as \textbf{bold}.}
\label{tab:st3-labelled-vs-unlabelled-f1micro}
\vspace{-4mm}
\end{table}


\subsection{Development Set Fine-grained Results}
Table \ref{tab:st3-error-analysis} shows the fine-grained results for the English official development set. Results are obtained from the best random seed over three runs. Although \textit{Appeal\_to\_Time}, \textit{Appeal\_to\_Values}, \textit{Consequential\_Oversimplification} and \textit{Questioning\_the\_Reputation} classes do not have a single example in development set, there are six other classes in which we also obtain $0.0$ F1-Score, namely \textit{Appeal\_to\_Hypocrisy}, \textit{Appeal\_to\_Popularity}, \textit{Obfuscation-Vagueness-Confusion}, \textit{Red\_Herring}, \textit{Straw\_Man}
and \textit{Whataboutism}, although together they account for only 5\% of the development set. The three biggest classes, \textit{Loaded\_Language}, \textit{Name\_Calling-Labeling} and \textit{Doubt} account for 29\%, 15\% and 11\% of the development set, respectively, thus having a large impact on \fmicro. 

\begin{table}
\centering
\scalebox{0.8}{
    \begin{tabular}{lrcrc}
    \toprule
    \multicolumn{5}{c}{Final Submission}                                                                                                   \\
             & \multicolumn{1}{l}{Test \fmicro} & \multicolumn{1}{l}{Place} & \multicolumn{1}{l}{Test \fmacro} & \multicolumn{1}{l}{Place} \\ \midrule
    English  & 36.802                            & 2                        & 17.194                            & 2                        \\
    French   & 41.436                            & 4                        & 32.424                            & 1                        \\
    German   & 44.726                            & 6                        & 23.679                            & 3                        \\
    Italian  & 52.494                            & 3                        & 28.22                             & 1                        \\
    Polish   & 34.7                              & 7                        & 19.102                            & 4                        \\
    Russian  & 31.841                            & 5                        & 20.522                            & 2                        \\ \hdashline
    Greek    & 17.426                            & 7                        & 11.028                            & 8                        \\
    Spanish  & 27.497                            & 9                        & 13.042                            & 8                        \\
    Georgian & 24.911                            & 10                       & 29.553                            & 4                        \\ \bottomrule
    \end{tabular}
}
\caption{Subtask 3 final submission \fmicro and \fmacro and our placement according to both of them.}
\label{tab:st3-final-results-micromacro}
\vspace{-4mm}
\end{table}


\begin{table*}
\centering
\scalebox{0.8}{
\begin{tabular}{@{}lrrrr@{}}
\toprule
Class                             & \multicolumn{1}{l}{Precision} & \multicolumn{1}{l}{Recall} & \multicolumn{1}{l}{F1-Score} & \multicolumn{1}{l}{Samples} \\ \midrule
Appeal\_to\_Authority             & 0.11                          & 0.07                       & 0.09                         & 28                          \\
Appeal\_to\_Fear-Prejudice        & 0.39                          & 0.23                       & 0.29                         & 137                         \\
Appeal\_to\_Hypocrisy             & 0                             & 0                          & 0                            & 8                           \\
Appeal\_to\_Popularity            & 0                             & 0                          & 0                            & 34                          \\
Appeal\_to\_Time                  & 0                             & 0                          & 0                            & 0                           \\
Appeal\_to\_Values                & 0                             & 0                          & 0                            & 0                           \\
Causal\_Oversimplification        & 0.03                          & 0.04                       & 0.04                         & 24                          \\
Consequential\_Oversimplification & 0                             & 0                          & 0                            & 0                           \\
Conversation\_Killer              & 0.11                          & 0.28                       & 0.16                         & 25                          \\
Doubt                             & 0.26                          & 0.36                       & 0.3                          & 187                         \\
Exaggeration-Minimisation         & 0.21                          & 0.34                       & 0.26                         & 115                         \\
False\_Dilemma-No\_Choice         & 0.26                          & 0.16                       & 0.2                          & 63                          \\
Flag\_Waving                      & 0.34                          & 0.49                       & 0.4                          & 96                          \\
Guilt\_by\_Association            & 0.33                          & 0.25                       & 0.29                         & 4                           \\
Loaded\_Language                  & 0.39                          & 0.64                       & 0.48                         & 483                         \\
Name\_Calling-Labeling            & 0.42                          & 0.69                       & 0.52                         & 250                         \\
Obfuscation-Vagueness-Confusion   & 0                             & 0                          & 0                            & 13                          \\
Questioning\_the\_Reputation      & 0                             & 0                          & 0                            & 0                           \\
Red\_Herring                      & 0                             & 0                          & 0                            & 19                          \\
Repetition                        & 0.12                          & 0.24                       & 0.16                         & 141                         \\
Slogans                           & 0.21                          & 0.43                       & 0.29                         & 28                          \\
Straw\_Man                        & 0                             & 0                          & 0                            & 9                           \\
Whataboutism                      & 0                             & 0                          & 0                            & 2                           \\ \midrule
micro avg                         & 0.31                          & 0.44                       & 0.36                         & 1666                        \\
macro avg                         & 0.14                          & 0.18                       & 0.15                         & 1666                        \\ \bottomrule
\end{tabular}
}
\caption{Subtask~3 fine-grained results for the English development set.}
    \label{tab:st3-error-analysis}
    
\end{table*}

\subsection{Full Leaderboard Results} \label{ap:s3-full-leaderboard}
Table \ref{tab:st3-final-results-micromacro} shows our full final submission scores and placements according to both \fmicro and \fmacro. As we previously point out in section \ref{sec:st3-system-overview}, we aimed towards a model capable of identifying all the 23 classes, thus having high \fmacro, even though the main metric for the subtask is \fmicro. We believe that a realistic application of a model for this particular label scheme should not disregard under-represented classes, otherwise they should simply be removed from the label scheme. Although our placings according to \fmacro are considerably higher, we acknowledge that because the main metric for the subtask is not \fmacro, other teams' submissions are likely not focusing on maximizing it, thus making their scores lower on average.



\end{document}


