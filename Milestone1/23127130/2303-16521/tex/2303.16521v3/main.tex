%File: anonymous-submission-latex-2024.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{graphicx}
\usepackage{url}
\usepackage{multirow}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{newfloat}
\usepackage{sidecap}
\usepackage{dsfont}
\usepackage{placeins}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}

\makeatletter
\renewcommand{\Function}[2]{%
  \csname ALG@cmd@\ALG@L @Function\endcsname{#1}{#2}%
  \def\jayden@currentfunction{#1}%
}
\newcommand{\funclabel}[1]{%
  \@bsphack
  \protected@write\@auxout{}{%
    \string\newlabel{#1}{{\jayden@currentfunction}{\thepage}}%
  }%
  \@esphack
}
\makeatother

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{1} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Hard Regularization to Prevent Deep Online Clustering Collapse without Data Augmentation}
\author{
    Louis Mahon\textsuperscript{\rm 1, \rm 3},
    Thomas Lukasiewicz\textsuperscript{\rm 2, \rm 3}
}
\affiliations{
    \textsuperscript{\rm 1}School of Informatics, University of Edinburgh, UK\\
    \textsuperscript{\rm 2}Institute of Logic and Computation, Vienna University of Technology, Austria\\
    \textsuperscript{\rm 3}Department of Computer Science, University of Oxford, UK\\
    louis.mahon@inf.ed.ac.uk
}


\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi



\begin{document}
\pagenumbering{arabic}


\maketitle

\begin{abstract}
Online deep clustering refers to the joint use of a feature extraction network and a clustering model to assign cluster labels to each new data point or batch as it is processed. While faster and more versatile than offline methods, online clustering can easily reach the collapsed solution where the encoder maps all inputs to the same point and all are put into a single cluster. Successful existing models have employed various techniques to avoid this problem, most of which require data augmentation or which aim to make the average soft assignment across the dataset the same for each cluster. We propose a method that does not require data augmentation, and that, differently from existing methods, regularizes the hard assignments. Using a Bayesian framework, we derive an intuitive optimization objective that can be straightforwardly included in the training of the encoder network. Tested on four image datasets and one human-activity recognition dataset, it consistently avoids collapse more robustly than other methods and leads to more accurate clustering. We also conduct further experiments and analyses justifying our choice to regularize the hard cluster assignments. Code is available at \url{https://github.com/Lou1sM/online_hard_clustering}.
\end{abstract}

\section{Introduction}
%The initial successes of deep learning in computer vision, beginning with AlexNet in the ImageNet Challenge \cite{krizhevsky2012imagenet}, were all in the supervised domain. However, more recently, unsupervised learning is attracting increasing attention, because it avoids the need for manually labelled data, a need that becomes more costly the bigger datasets become, and because it is capable of learning previously unknown features of the data, which were not given in the annotations. Natural language processing (NLP) was greatly advanced by the adoption of unsupervised learning in the form of masked language modelling (e.g., \cite{devlin2018bert}), as this enabled models to be trained on large amounts of unlabelled text data (e.g., \cite{brown2020language}).In computer vision, unsupervised learning has not yet seen the same degree of success as in NLP, though substanstial progress is being made. 

%Unsupervised computer vision models can be broadly divided into clustering models, which aim to place each data point into a cluster, so as to maximize similarity within clusters and minimize it between clusters, and contrastive learning models, which train a model without labels by encouraging representations for certain data points (positive pairs) to be similar, and those for others (negative pairs) to be dissimilar. While some methods propose to leverage contrastive learning for clustering performance \cite{li2020prototypical,cai2021large,caron2020unsupervised}, the two are fundamentally different objectives. The former returns a partition of the dataset and the latter, generally, returns a feature vector for each data point. Contrastive learning requires some prior knowledge about the data in order to be able to select which data points should be similar or different. Often, this prior knowledge comes in the form of data augmentation (DA), where two augmented versions of the same data point serve as positive pairs, and this requires knowing the appropriate semantic symmetries used for augmentation, e.g., reflection or brightness shift. Contrastive learning can also require a large number of negative pairs, if there are a large number of semantically meaningful axes of variation in the data. Clustering, on the other hand, can be performed even on datasets with no prior knowledge, and does not require the generation of negative pairs.

Deep clustering refers to the combination of deep learning and clustering, where the data are first encoded with a deep neural network to a feature space, and then clustering is performed in the feature space. Clustering models can be classified as offline or online. Offline models process the entire dataset, and then assign all cluster labels at once. Online models, in contrast, assign a cluster label to each data point, or each batch, as it is processed. Offline methods can produce more accurate clusterings (e.g., \citet{mahon2021selective,niu2021spice} reaching close to supervised performance), as they can leverage information from later data points when assigning cluster labels to earlier data points. However, they are more expensive to train, as they must alternate between encoding the entire dataset, or training the encoder for some number of epochs, and clustering the encoded data. Online models, on the other hand, can jointly encode and cluster, so are less computationally expensive. They are also more versatile, being applicable in real-world settings where new data is constantly becoming available, as opposed to offline methods, which are limited to fixed pre-defined datasets \cite{silva2013data}.

The disadvantage of online methods is that they are more difficult to train. In particular, they run the risk of producing a degenerate solution where the large majority of data points are concentrated in a small number of clusters. In the extreme case, all points are placed into the same cluster. For example, an intuitive way to formulate a training procedure for online deep clustering is to update both the encoder network and the clustering parameters to make the encoding of each point close to its cluster centroid and far from other cluster centroids. (The clustering parameters are e.g., the centroids in K-means or the first two moments in a Gaussian mixture model.) However, this training objective is trivially minimized by the encoder network mapping all data points to the same point in feature space, where this point is equal to one of the cluster centroids. Techniques for avoiding collapse we refer to as partition support. Several partition support methods have been proposed, but they mostly require data augmentation (DA) \cite{zbontar2021barlow, cai2021large}, and those that do not are often ad hoc and lack a rigorous technical foundation \cite{caron2020unsupervised, deshmukh2021representation}. Additionally, they take the soft assignments as a measure of partition collapse and propose to regularize the soft assignments to make them more uniform. This paper focuses on deep clustering without DA, which is an advantage because relying on DA limits a method to domains in which sufficient prior knowledge to perform class-preserving augmentations is available. Additionally, we argue that soft assignments are not an accurate measure of collapse, and that we should instead focus on hard assignments.

We propose a DA-free partition support by regularizing hard assignments. Specifically, we consider the problem of how to optimally assign each data point in a batch to the most appropriate cluster. We express this problem probabilistically in a Bayesian framework, where the regularizing element is captured by a prior across clusters. In the case of equally-sized clusters, this can be uniform. We then use this expression to derive a precise optimization objective, which we also show to be equivalent, up to a small error term, to the objective of maximizing the mutual information of the cluster assignments and the data index. This objective itself is too slow to solve exactly, but we devise a greedy approximation algorithm that can be implemented straightforwardly and results in an intuitive method for fully online clustering, which we term combination assignment. This method outperforms existing online DA-free clustering methods on four popular image clustering datasets. We also analyze the underlying representations and show them to be of high quality. Finally, we analyze the role of hard vs.\ soft cluster assignments in our partition support method, and in previous methods, and make the case that regularizing hard assignments is a more effective approach. Note that, although existing methods can easily convert soft assignments to hard assignments, this is very different from regularizing the hard assignments, as we propose. While the relation between hard and soft clustering has been studied before \cite{bora2014comparative,kearns1998information}, its study in the context of regularizing online deep clustering collapse is new. 

Our main contributions are briefly summarized below.
\begin{itemize}
    \item We articulate a clear Bayesian framework of the problem of hard assignments in online deep clustering models, which avoids the unrealistic assumption of uniformity in each batch made by most existing methods.
    \item We use this framework to derive an optimization objective and prove that it is approximately equivalent to maximizing the mutual information of the cluster assignments and the data index.
    %\item We devise a greedy algorithm to approximately solve this optimization objective, and prove equivalence to maximizing mutual information.
    \item We show empirically that the resulting method significantly outperforms existing partition support methods in avoiding partition collapse, improving clustering accuracy and leading to more informative representations.
    \item We conduct further analysis of the performance of different partition support methods, which justifies our choice to focus on hard assignments.
\end{itemize}

The rest of this paper is organized as follows. Section \ref{sec:related-work} gives an overview of related work, while Section \ref{sec:method} lays the theoretical foundations of our method, describes our greedy algorithm for optimizing the resulting objective, and proves the equivalence to mutual information maximization. Section \ref{sec:experimental-eval} reports our empirical results and analysis, and finally Section \ref{sec:conclusion} summarizes our findings.

\section{Related Work} \label{sec:related-work}
%The first deep clustering models for images used autoencoders. After training the autoencoder to reconstruct the input, the decoder could be discarded, and the encoder used to extract feature vectors which were fed to a clustering algorithm \cite{yang2017towards,huang2014deep,xie2016unsupervised}. Autoencoders are trained on reconstruction loss in pixel space, and while this can be successful on simple datasets such as MNIST, where images from the same class have significant pixel overlap, it does not transfer to more realistic datasets where, e.g., two different images of a cat could have very different pixel values but we still desire their feature vectors to be similar. Therefore, autoencoder-based clustering is limited for complex images. 

%Offline clustering models, known as pseudo-label training, was proposed by \cite{caron2018deep}. The output of a randomly initialized convolutional encoder is clustered using k-means. Then, the cluster labels are used as training targets to update the encoder and the procedure iterated. Most recent deep clustering methods employ a version of pseudo-label training \cite{wu2019deep,mrabah2019deep,van2020scan,mahon2021selective,niu2021spice}. The strongest performance has been obtained by those methods that carefully select and refine the pseudo-labels they use for training, e.g. \cite{mahon2021selective,niu2021spice}. The disadvantages of pseudo-label training is that it is slow to alternate back and forth between computing pseudo-labels and using them as targets for training. Also, it is almost always required to be offline, because the pseudo-labels are computed using an offline clustering algorithm such as k-means, so cannot operate in a real-world setting in which new data is continuously becoming available. 

A key component of online deep clustering methods is how they avoid the collapsed solution, where (almost) every data point is placed in the same cluster. Methods designed for contrastive learning, and those clustering models that employ it as a part of the training procedure, are more resistant to collapse, because the negative pairs are encouraged to be represented differently. This can mean that they are to be placed in different clusters \cite{huang2021deep}, or just that the encodings should be far apart \cite{zhang2021supporting,cai2021large}. Either approach helps resist all points having the same representation. Even without negative pairs, data augmentation is essential for some methods to avoid collapse. \citet{grill2020bootstrap} perform representation learning using only positive pairs by training an ``online'' network to predict the output of a ``target'' network, which is a slow-moving average of the online network. In \cite{zbontar2021barlow}, data-augmented pairs are used to reduce redundancy by minimizing off-diagonals in the cross-correlation matrix.

Among online clustering models, several partition support methods have been proposed. The solution in \cite{kulshreshtha2018online} is simply to freeze the encoder during clustering, but this requires pretraining it on a separate task offline, so does not work in the fully online setting. A different approach is taken by \citet{gao2020deep}, who use an online autoencoder-based (AE) clustering model, where the reconstruction loss helps to avoid partition collapse, but this limits the method to simple datasets, because the AE's reconstruction loss requires that different images from the same class have significant pixel overlap. In \cite{zhan2020online}, the loss function is continually reweighted to encourage the assignment to smaller clusters. Additionally, when clusters decrease below a certain threshold, they are deleted, and the largest cluster is split in two using K-means, which also means the method does not work fully online. Data-augmented pairs are used by \citet{cai2021large}, with the same method as \cite{zbontar2021barlow}, treating soft cluster assignments as representations, and using data-augmented pairs. In \cite{zhong2020deep}, the sum of squares of the probability (i.e., soft assignment) of each cluster is minimized, marginalized over each training batch. Decomposing the expectation of the square, we see that this also involves minimizing the variance. However, the authors also use contrastive learning and DA to avoid collapse, so they do not fully rely on sum-of-squares minimization.

A similar idea, employed by \cite{li2020prototypical,hu2017learning,niu2021spice,niu2020gatcluster,van2020scan}, is to maximize the entropy of batch-wise marginal soft assignments. For an input distribution $X$ with corresponding soft assigned labels $Y$, both approximated over a batch, an extra term $-H(Y)$ is added to the loss function. As the entropy of a multinomial is maximized at the uniform distribution, this encourages equally sized clusters. Entropy maximization has the advantage of a solid formal interpretation as part of the maximization of the mutual information between data and cluster assignments: maximizing $H(Y)$ but minimizing $H(Y|X)$ (the latter is minimized explicitly in \cite{hu2017learning} and implicitly via contrastive learning in \cite{li2020prototypical}), implicitly maximizes 
\[
I(X;Y) = H(Y) - H(Y|X)\,.
\]
However, the marginal entropy term tends to only be partially successful at preventing partition collapse, often the entire dataset is still put into only a small number of clusters. As well as being confirmed in our experiments in Section \ref{sec:experimental-eval}, this empirical weakness of entropy maximization for avoiding partition collapse is reported in \cite{hu2017learning}, and better results are found by explicitly constraining the soft cluster assignments using non-linear programming. The work \cite{li2020prototypical} does not rely entirely on entropy maximization, because they employ contrastive learning, which (as explained above) also helps to avoid partition collapse. %Thus, while theoretically sound, entropy maximization is not sufficiently effective empirically.

Another approach is to directly impose a constraint on cluster assignments. Based on earlier work \cite{asano2019self}, \cite{caron2020unsupervised,deshmukh2021representation,kumar2021unsupervised} proposed to constrain the soft cluster assignments to be marginally uniform across each training batch. Though effective in preventing partition collapse, the constraint of exact uniformity across each batch is too strict, as noted in \cite{kumar2021unsupervised}. The ground-truth classes will almost certainly violate this constraint. %This excessive strictness is unsatisfying theoretically, as well as hurting performance empirically.


\section{Method} \label{sec:method}
\paragraph{Problem Formulation} \label{subsec:problem-formulation}
We want to simultaneously (a) train the encoder and (b) make hard assignments to each batch of points at a time, based on the features extracted by that encoder. If the encoder is $f_{\theta_1}: \mathbb{R}^n \rightarrow \mathbb{R}^m$, our clustering model is $g_{\theta_2}:\mathbb{R}^m \rightarrow \{1, \dots, K\}$, parametrized by $\theta_2$, and our batch size is $N$, then we seek a function of the form $\Gamma: \mathbb{R}^{N \times m}\ \rightarrow$ $\{1, \dots, K\}^{N}$. (The difference between $\Gamma$ and $g_{\theta_2}$ is that the former is used during training to assign an entire batch, while the latter is used at inference time and can assign each point individually.) %It is possible to use the clustering model itself to assign batch labels during training, though this is not advisable, as discussed below.
If we have a method for batchwise assignment, then the training objective can be formulated as minimizing some notion of distance of points from the centroids of their assigned clusters, and the encoder and clustering model can be trained as follows:
\begin{equation}
    \argmin_{\theta_1, \theta_2} \sum_{i=1}^N d(f_{\theta_1}(x_i) - \mu_{k_i})\,, \label{eq:online-cluster-obj-euclidean}
\end{equation}
where $x_i$ is the $i$th batch elemet, $d(\cdot)$ is a distance function, e.g., Euclidean, $k_i=\Gamma(f_{\theta_1}(x))_i$ is the 
%cluster label assigned to the encoding of $x_i$ under~$f$, 
$i$th assigned cluster label, and $\mu_k$ is the $k$th cluster centroid.
%Here we express $\theta_2$ as a matrix whose rows are cluster centroids, and use python-style indexing to indicate the $k$th row of $\theta_2$.

Finding a suitable assignment function $\Gamma$ is non-trivial. It should be more likely to assign points to the clusters whose centroids are closer. However, using only this rule, and assigning every vector to its closest centroid, we allow a collapsed solution where \eqref{eq:online-cluster-obj-euclidean} is minimized by $f_{\theta_1}$ mapping every point to a single centroid: $\forall i, f_{\theta_1}(x_i)=c_k$, for some $k\in \{0,\dots,K-1\}$. The heart of our method is the choice of a suitable $\Gamma$, which avoids the collapsed solution. We refer to it as combination assignment, because its prior is the combination of labels under a multinoulli distribution.

\paragraph{Combination Assignment} \label{subsec:comb-assignment}
Combination assignment is based on a Bayesian formulation of the online clustering problem. Let $\mathcal{D}$ be the data distribution, $X \sim \mathcal{D}$ be a sampled batch of data, $Z$ be the random variable defined by applying the feature extraction to $X$ (i.e., $Z$ is the output of a deep encoder network), and let $Y$ be the assigned cluster labels. Here, we consider the encoder to be fixed, and we are just interested in the best hard assignment of cluster labels, given the extracted features. That is, we want to determine the values of $Y$ with the maximum probability under the a posteriori distribution $p(Y|Z)$. We assume we have some reasonable estimate of the prior distribution over $K$ clusters: $p:\{0,\dots,K-1\} \rightarrow [0,1]$ (note, this estimate could change for each batch).
%This is a weaker and more realistic assumption than enforcing the prior distribution on each batch, as done by existing methods. 
Then, the prior probability of a batch containing exactly $n_k$ labels for each cluster~$k \in\{ 1, \dots, K\}$~is 
\begin{equation} \label{eq:prior}
    \prod_{k=1}^Kp(k)^{n_k} \frac{N!}{\prod_{k=1}^K n_k!}\,,
\end{equation}
where $N=\sum_{k=1}^K n_k$ is the batch size. It is often suitable to choose a uniform prior over $K$ cluster labels, $p(i) = 1/K$, however, we are free to choose any prior we would like. Simply having an estimate of the prior is a weaker assumption than existing methods, which assume equal numbers of each cluster by design. For example, all methods that use k-means as their backbone are implicitly assuming roughly uniform clusters \cite{satapathy2015emerging}. 

We model each cluster as a multivariate normal distribution in feature space, so that the likelihood is given by
\begin{gather}
p(Z=z_1, \dots, z_N|Y=k_1,\dots,k_N) = \\
\prod_{i=1}^N \frac{\exp(-\tfrac{1}{2}(z_i- \mu_{k_i})\Sigma_{k_i}^{-1}(z_i-\mu_{k_i}))}{\sqrt{(2 \pi)^{d}|\Sigma_{k_i}|}}\,,\label{full-likelihood}
\end{gather}
where $d$ is the dimension of the feature space, and $\mu_k$ and $\Sigma_k$ are the centroid and covariance matrix of the $k$th cluster, respectively. 
%If we further assume each cluster is spherical, with the same isotropic variance across all clusters, i.e., $\Sigma_k = \sigma I,\text{ for } k\in\{1,\dots,K\}$, and 
We then maximize the posterior corresponding to the prior in \eqref{eq:prior} and the likelihood in \eqref{full-likelihood}, giving the following optimization problem (more details in the appendix):
\begin{align} \label{eq:sum-objective}
    &\argmin_Y \sum_{i=1}^N d(z_i,\mu_i,\Sigma_i) - \log{p(k_i)} + \sum_{k=1}^K\log( n_k!), \\
    \text{where  }& d(z,\mu,\Sigma) =\tfrac{1}{2}(z- \mu)^T\Sigma^{-1}(z-\mu) + \tfrac{1}{2}\log{(2\pi)^d|\Sigma|}\,. \notag
\end{align}
In matrix notation, the objective is 
\begin{gather}
    \argmin_{Q \in \mathcal{B}^{N \times K}} \langle Q, \tilde{Q} \rangle - \mathds{1}_N^TQ\log{P} + \log(\mathds{1}_N^TQ!) \mathds{1}_K \label{eq:matrix-objective} \\
    \text{subject to } Q\mathds{1}_K = \mathds{1}_N\,, \notag
\end{gather}
where $\tilde{Q}_{i,j} = d(z_i, \mu_j, \Sigma_j)$, $\langle \cdot , \cdot \rangle$ denotes the Frobenius inner product, $\mathds{1}_a$ is an $a$-dimensional vector of all ones, $P$ is a $K$-dimensional probability vector specifying the prior, and $\log$ and factorial are applied element-wise to the $K$-dimensional vector $\mathds{1}_N^TQ$. Constraining $Q$ to be Boolean enforces hard assignments, constraining $Q\mathds{1}_K = \mathds{1}_N$ enforces each latent vector to be assigned to exactly one cluster. The encoder is then trained with gradient descent w.r.t. using only the distance from the chosen cluster. The prior and cluster sizes affect its training indirectly.

\paragraph{Solving the Optimization Problem} \label{subsec:greedy-strategy}
It is too slow to solve \eqref{eq:matrix-objective} exactly (see appendix for details), but we can motivate an approximation by considering the simpler problem of assigning the last data point in a batch, given that the rest have already been assigned. That is, we maximize the conditional probability of the $N$th assignment in a batch, conditioned on the $N-1$ previous assignments. This gives the following (details in the appendix).
\begin{gather}
%    \argmin_{k_N=1,\dots,K} ||z_N- \mu_{k_N}||^2 - \log{p(k_N)}+ 2\sigma \log( n_{k_N}+1!) - \log( n_{k_N}!) = \notag \\
    \argmin_{k_N=1,\dots,K} d(z_N,\mu_{k_N},\Sigma_{k_N}) - \log{p(k_N)}+ \log( n_{k_N}+1)\,, \label{eq:Nth-point-assignment}
\end{gather}
where $n_k$ is the size of cluster $k$ before the $n$th assignment.

To approximately solve \eqref{eq:matrix-objective}, we employ a greedy algorithm that iteratively solves \eqref{eq:Nth-point-assignment} with respect to the most confident assignment possible. That is, at each iteration, select the pair $(i,k)$ (corresponding to $(N,k_N)$ above) for which \eqref{eq:Nth-point-assignment} is smallest, with $i$ ranging over the indices of still-unassigned points, and $k$ ranging over all clusters, and assign the $i$th point to cluster $k$.

%\vspace{-1ex}
\paragraph{Intuition}
To get an intuition on our method, recall that clustering generally should assign points to the closest centroid, but that we should try to resist assigning to a cluster that already has lots of points assigned to it and also take into account the prior. Even if such a cluster has its centroid closest to the $N$th point, it may be better to instead assign to a smaller, further away cluster, or one with a higher prior probability. This suggests choosing the cluster assignment so as to minimize a combination of the distance to the centroid, the prior and some increasing function of cluster size, which is precisely what \eqref{eq:Nth-point-assignment} expresses. The first term says to pick a nearby cluster, the second term to pick a cluster with a high prior probability, and the third to penalize clusters that already have many points assigned. It would not be obvious, a priori, what the increasing function of cluster-size should be exactly, but the derivation of \eqref{eq:Nth-point-assignment} shows that $\log{(n+1)}$ is an appropriate choice. The fact that we take into account the cluster size when assigning points to clusters is an important difference between our method and existing methods, which generally assign to the cluster with the closest centroid (e.g., k-means) or the cluster which assigns the highest probability to the given data point (e.g., GMM clustering). Another important difference is that the centroids are trainable parameters, and can be updated by gradient descent, rather than being set to the empirical mean of the corresponding cluster.

\paragraph{Information-Theoretic Interpretation}
%Here, we show that, 
Under a uniform prior, the greedy algorithm that iteratively solves \eqref{eq:Nth-point-assignment} can be interpreted as iteratively making whatever assignment will maximize the mutual information between the batch index $i$ and the cluster labels. First, we show a close equivalence to maximizing the entropy of cluster labels in each batch.
%Specifically, we consider the objective of choosing the assignment that maximizes some combination of the likelihood and the marginal entropy. By the following argument, this can be seen to lead to essentially the same procedure as that described in Section \ref{subsec:greedy-strategy}.

Let $H^{(k)}$ be the marginal hard entropy of cluster labels after a new assignment to cluster $k$:
\begin{gather*}
    H^{(k)} = \frac{x_k + 1}{N+1} \log{\frac{x_k + 1}{N+1}} + \sum_{j=1, j\neq k}^K \frac{x_j}{N+1} \log \frac{x_j}{N+1}\,,
\end{gather*}
and consider the difference $ H^{(k)} - H^{(k')}$ between the entropy after making assignment $k$ vs.\ after making a different assignment $k'$. It can be shown (see appendix) that 

\begin{equation} \label{eq:entropy-assignment-objective}
    H^{(k)} - H^{(k')} \approx  \frac{1}{N+1} \left( \log(x_{k'} + 1) - \log (x_{k} + 1)\right)\,.
\end{equation}
Thus, if we were to make each assignment so as to maximize $\log(\mathcal{L}_k(x)) + \lambda H^{(k)}$,
where $\mathcal{L}(k)$ is the likelihood of the new data point under cluster $k$, and $\lambda$ is some hyperparameter, then, subject to the above approximation, for each $k,k' \in \{1, \dots, K\}$, we would prefer to assign to $k$ iff
\begin{gather}
    \log \mathcal{L}(k) + \lambda H^{(k)} > \log \mathcal{L}(k') + \lambda H^{(k')} \iff \notag \\
%    \log \mathcal{L}(k) - \log \mathcal{L}(k') >  \lambda H^{(k')} - \lambda H^{(k)} \iff \notag \\
    \log \mathcal{L}(k) - \log\mathcal{L}(k') >  \nonumber \\ > \frac{\lambda}{N+1} \left( \log(x_{k'} + 1) - \log (x_{k} + 1)\right)\,. \label{eq:entropy-assignment-objective-1}
\end{gather}
Modelling clusters as multivariate normal distributions (as above), and setting $\lambda = N+1$, \eqref{eq:entropy-assignment-objective-1} becomes equivalent to \eqref{eq:Nth-point-assignment}. (See appendix for full proof.)

Thus, our method closely approximates a maximization of the entropy of cluster labels. There is some similarity to those methods, discussed in Section \ref{sec:related-work}, that use an additional loss term to encourage greater entropy of soft assignments in each batch, but an important difference here is that we are maximizing the entropy of hard assignments. This means that the entropy of cluster labels given batch index is automatically zero. Therefore, using the decomposition $I(X;Y) = H(X) - H(X|Y)$, the mutual information of the batch index $i$ and the cluster labels equals the entropy of cluster labels, and so our method is a close approximation to maximizing this mutual information.

\begin{algorithm}[tb]
    \caption{During training, cluster labels are assigned batchwise, with partition support provided by a uniform prior across clusters. During inference, cluster labels are assigned pointwise, without any explicit partition support.} \label{alg:method}
    \begin{algorithmic}
    \State $f_{\theta_1} \gets$ encoder network; 
    \State $\theta_2 = \mu_1, \dots, \mu_K \gets$ centroids for each of the $K$ clusters; 
    \State $\sigma \gets$ isotropic variance of all clusters; 
    \Function{AssignBatch}{Z}   
        \State $counts \gets$ $K$-dimensional array, initially all $0$s; 
        \State $isAssigned \gets$ $N$-dimensional Boolean array, initially all False; 
        \State $D \gets$ $N \times K$ matrix, where $D_{ij} = ||Z_{i} - \mu_{j} ||^2$; %, $i=1, \dots, N$
        \For{r=1,\dots,N}
            \State $\tilde{D} \gets$ $N \times K$ matrix, where $\tilde{D}_{ij} = D_{ij} + 2\sigma \log(counts[j] + 1)$; 
            \State $(a,k) \gets$ argmin $\{\tilde{D}_{ij} : \neg isAssigned[i]\}$; 
            \State $counts[k] \gets counts[k] + 1$; 
            \State $isAssigned[a] \gets True$; 
            \State $loss \gets loss + D_{a,k}$
        \EndFor 
        \State \Return $loss$
    \EndFunction 
    \Function{TrainOnBatch}{X} \funclabel{func:train-method}
        \State $Z \gets f_{\theta_1}(X)$, encodings for a batch of $N$ data points; 
        \State $loss \gets AssignBatch(Z)$;
        \State take a train step on $loss$ with respect to $\theta_1, \theta_2$
    \EndFunction 
    \Function{PredictSingleDataPoint}{x} \funclabel{func:inference-method}
        \State $z \gets f_{\theta_1}(x)$ encodings for a batch of $N$ data points; 
        \State $assignment = \argmin_{j=1,\dots,K} ||z-\mu_j||^2$; 
        \State \Return assignment
    \EndFunction
    \end{algorithmic}
\end{algorithm}



\paragraph{Training Procedure}
Our model comprises an encoder network $f_{\theta_1}$ and cluster centroids $\theta_2 = \{\mu_1, \dots, \mu_K\}$. To train on an input batch, we first encode the raw data using $f_{\theta_1}$, then we employ the combination assignment method of Section \ref{subsec:greedy-strategy}, and use these assignments to minimize \eqref{eq:sum-objective} with respect to both $\theta_1$ and $\theta_2$. As the second two terms in \eqref{eq:sum-objective} have no gradient, the updates are made only with respect to the first term, so similarly to \eqref{eq:online-cluster-obj-euclidean}. At inference time, we do not perform combination assignment. Instead, we simply assign each point to the cluster with the nearest centroid, so the model can assign points individually, and is not restricted to operating batch-wise. The full training and inference methods are described by the functions %\ref{func:train-method} and \ref{func:inference-method}, 
TrainOnBatch and PredictBatch, respectively, in Algorithm~\ref{alg:method}.

%\begin{SCtable*}
\begin{table*}
\centering
\resizebox{0.79\textwidth}{!}{
\begin{tabular}{llllllll}
\toprule
         &     &       CA (ours) &           SK &           ENT &           SS &          CKM &       no reg \\
\midrule
CIFAR 10 & Acc &     \textbf{21.7 (1.50)} &  16.7 (0.40) &   18.6 (1.52) &  11.8 (1.92) &  15.2 (0.91) &  10.0 - \\
         & NMI &    \textbf{10.5 (1.07)} &  03.8 (0.76) &   08.7 (2.94) &  01.1 (1.29) &  02.8 (0.71) &  00.0 - \\
         & ARI &      \textbf{5.4 (0.59)} &  02.7 (0.68) &   05.7 (1.97) &  00.3 (0.42) &  01.4 (0.38) &  00.0 - \\
         & KL* &      \textbf{0.0 (0.01)} &  02.5 (0.33) &   01.8 (0.21) &  00.9 (1.36) &  01.1 (0.24) &  00.0 - \\
CIFAR 100 & Acc &      \textbf{6.9 (0.17)} &  02.6 (0.24) &   02.4 (0.19) &  01.2 (0.25) &  02.8 (0.34) &  01.0 - \\
         & NMI &    \textbf{14.2 (0.61)} &  05.3 (0.46) &   06.3 (0.82) &  00.6 (1.18) &  04.6 (0.89) &  00.0 - \\
         & ARI &      \textbf{1.7 (0.10)} &  00.3 (0.04) &   00.5 (0.11) &  00.0 (0.04) &  00.3 (0.12) &  00.0 - \\
         & KL* &      \textbf{0.5 (0.11)} &  03.5 (0.80) &   01.6 (0.22) &  00.2 (0.19) &  02.8 (0.49) &  00.0 - \\
FashionMNIST & Acc &     \textbf{59.3 (4.16)} &  25.1 (3.13) &   25.5 (6.16) &  10.0 (0.04) &  22.0 (3.83) &  10.0 - \\
         & NMI &     \textbf{55.3 (2.95)} &  17.7 (2.33) &  20.9 (11.32) &  00.0 (0.04) &  15.8 (5.28) &  00.0 - \\
         & ARI &     \textbf{42.5 (4.12)} &  09.2 (1.42) &   10.6 (6.01) &  00.0 - &  07.3 (2.84) &  00.0 - \\
         & KL* &      \textbf{0.0 (0.02)} &  02.6 (0.35) &   01.9 (0.24) &  00.0 (0.01) &  01.3 (0.31) &  00.0 - \\
STL & Acc &     \textbf{24.2 (2.56)} &  18.5 (1.08) &   10.9 (1.97) &  10.1 (0.22) &  14.1 (0.78) &  10.0 - \\
         & NMI &     \textbf{13.7 (1.32)} &  06.7 (1.14) &   01.3 (2.86) &  00.0 (0.09) &  02.5 (0.58) &  00.0 - \\
         & ARI &      \textbf{7.3 (1.11)} &  03.3 (0.88) &   00.2 (0.36) &  00.0 - &  01.1 (0.39) &  00.0 - \\
         & KL* &      \textbf{0.1 (0.10)} &  00.7 (0.41) &   00.1 (0.18) &  00.1 (0.18) &  01.2 (0.21) &  00.0 - \\
RealDisp & Acc &     \textbf{57.0 (3.02) }&  23.5 (3.09) &   18.8 (1.98) &  10.7 (1.88) &  17.2 (1.33) &  10.0 - \\
         & NMI &  \textbf{55.0 (17.11)} &  31.0 (2.93) &   29.7 (4.26) &  00.5 (0.50) &  25.8 (1.75) &  00.0 - \\
         & ARI &     \textbf{48.1 (4.49)} &  11.6 (3.02) &   06.6 (1.99) &  00.2 (0.26) &  06.4 (1.91) &  00.0 - \\
         & KL* &      \textbf{0.1 (0.06)} &  00.7 (0.13) &   01.7 (0.61) &  02.0 (0.91) &  01.6 (0.13) &  02.3 - \\
\bottomrule
\end{tabular}

}
\caption{Effect, on cluster size and clustering performance, of our method compared to two existing partition-support methods. %``CA'' refers to our method of combination assignment, ``SK'' refers to Sinkhorn-Knopp regularization, as proposed by \cite{caron2020unsupervised} and \cite{chen2021exploring}, ``Ent'' refers to entropy maximization, as used by  \cite{hu2017learning}, ``SS'' is the sum of squares minimization proposed in \cite{zhong2020deep} and others, and ``No-Reg'' is the model without any partition support component. Along with standard cluster metrics, we report the KL-divergence from the ground truth distribution across clusters (denoted ``KL$^*$'') to indicate the extent of partition collapse. 
        All figures are the mean of 5 runs, with std dev in parentheses. Best results in bold, std dev of zero after rounding, is written `-'.}\label{tab:main-results}
\end{table*}
%\end{SCtable*}
\FloatBarrier

\section{Experimental Evaluation} \label{sec:experimental-eval}

\paragraph{Datasets and Metrics}
We report results on CIFAR 10 (C10), CIFAR 100 (C100), FashionMNIST (FMNIST), and STL, with image sizes 32, 32, 28, and 96, respectively, and the human activity recognition (HAR) dataset RealDisp, of 17 subjects performing 33 different activities wearing accelerometers. We use the standard clustering metrics of accuracy (ACC), normalized mutual information (NMI), and adjusted Rand index (ARI), defined as, e.g., in \cite{sheng2020unsupervised}. We also report the KL-divergence from the ground truth of the model's empirical distribution over clusters, denoted KL$^*$. For a collapsed model, which assigns most points to a few clusters, this will be high.

\subsection{Clustering Accuracy and Degree of Collapse} \label{subsec:main-results}
Table \ref{tab:main-results} compares our method with four existing methods: sum of squares minimization, denoted ``SS'' \cite{zhong2020deep}, the Sinkhorn-Knopp algorithm for optimal transport, denoted ``SK'' \cite{caron2020unsupervised,kumar2021unsupervised}, and marginal entropy maximization \cite{li2020prototypical}, denoted ``Ent'' and concrete k-means, denoted ``CKM'' \cite{gao2020deep}. Each is described in Section \ref{sec:related-work}. To make further explicit the phenomenon of partition collapse, we also include a model without any partition support. Our method significantly outperforms others on all datasets and metrics. 

Observe that the unregularized model exhibits total collapse in all experiments, placing all points in the same cluster and consequently achieving a cluster performance no better than random guessing. The performance of SS is not much better. By making a slight change to the author's original method, we could actually significantly improve its results (see appendix), but it was still unreliable and less accurate than our method.  The other two existing partition support methods do a reasonable job of avoiding partition collapse. However, entropy maximization occasionally also reaches the state with all points in the same cluster (this is consistent with previous literature, e.g., \cite{hu2017learning}). The Sinkhorn-Knopp method is more reliable, but by far the most uniform cluster sizes are produced by our method. Note that we do not employ our assignment algorithm at inference time,  and instead assign each point to the cluster with the nearest centroid. This shows that our cluster centroids are well-distributed around the data manifold, each capturing a sizeable subset of the data even when the explicit support is removed. Together, these figures show that (a) partition support is necessary to learn anything meaningful, (b) our method of combination assignment is better at avoiding partition collapse than previous methods, and (c) our model has, consequently, better clustering performance.

To better isolate the effect of our proposed partition support method, we do not perform hyperparameter tuning, and use a relatively simple architecture for all datasets. This is the same for all methods being compared. The network consists of two convolutional layers with filter sizes $6$ and $16$,  and ReLU activations, followed by a fully connected layer to a latent space of dimension $128$. Training uses Adam, with learning rate 1e-3, $\beta_1=0.9$, $\beta_2 = 0.99$, and batch size 256. We follow previous works in setting $K$, the number of clusters, to the number of ground-truth classes. We set $\Sigma=\sigma I$ with $\sigma=1e2$. We observe very similar results for a wide range of values for $\Sigma$, including a setting where we continually estimate from the empirical distributions. 

Although some existing clustering methods produce higher accuracy than those of Table \ref{tab:main-results}, this is not a valid comparison, because (as discussed above) these higher-scoring methods (a) use data-augmentation and (b) even more significantly, address offline clustering, which is much easier than, and not comparable to, the online cluster we address.


 \subsection{Imbalanced Data} \label{subsec:imbalanced-data}
\begin{table}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lllll}
\toprule
 &  & imb 1 & imb 2 & imb 3 \\
\midrule
\multirow[t]{4}{*}{CIFAR 10} & acc & 22.2 (1.22) & 22.8 (1.46) & 24.6 (1.26) \\
 & nmi & 10.5 (1.14) & 11.3 (0.73) & 10.5 (0.50) \\
 & ari & 5.9 (0.92) & 6.3 (0.55) & 6.2 (3.22) \\
 & KL* & 0.1 (0.07) & 0.1 (0.04) & 0.2 (0.11) \\
\cline{1-5}
\multirow[t]{4}{*}{CIFAR 100} & acc & 6.6 (0.31) & 6.7 (0.62) & 9.0 (0.33) \\
 & nmi & 13.3 (0.62) & 13.7 (0.99) & 15.4 (0.30) \\
 & ari & 1.6 (0.19) & 1.9 (0.36) & 2.7 (0.27) \\
 & KL* & 0.8 (0.10) & 0.7 (0.10) & 0.6 (0.09) \\
\cline{1-5}
\multirow[t]{4}{*}{FMNIST} & acc & 54.6 (6.34) & 52.1 (3.39) & 53.8 (2.95) \\
 & nmi & 52.9 (4.41) & 50.6 (1.50) & 50.0 (2.35) \\
 & ari & 37.7 (5.56) & 36.7 (1.84) & 38.5 (3.08) \\
 & KL* & 0.2 (0.23) & 0.1 (0.05) & 0.2 (0.17) \\
\cline{1-5}
\multirow[t]{4}{*}{STL} & acc & 23.2 (2.07) & 24.6 (1.41) & 25.5 (3.13) \\
 & nmi & 13.3 (1.69) & 14.5 (1.15) & 12.6 (2.44) \\
 & ari & 6.8 (1.13) & 7.8 (0.81) & 6.8 (1.90) \\
 & KL* & 0.3 (0.20) & 0.2 (0.19) & 0.1 (0.03) \\
\cline{1-5}
\multirow[t]{4}{*}{RealDisp} & acc & 57.5 (1.45) & 46.7 (6.75) & 39.2 (6.20) \\
 & nmi & 75.1 (1.01) & 67.9 (5.65) & 62.9 (5.77) \\
 & ari & 49.4 (0.90) & 38.0 (6.66) & 30.0 (6.07) \\
 & KL* & 0.1 (0.07) & 0.0 (0.01) & 0.3 (0.03) \\
\cline{1-5}
\bottomrule
\end{tabular}
}
\caption{Performance on imbalanced data, imb. 1 removes $10\%$ of one class and $20\%$ of another, imb. 2 removes $0\%, 5\%, 10\%, \dots$ of each of the 10 C10 classes, imb.3 is the same but in increments of $10\%$ (scaled proportionally to class size for other datasets). Note, a random guess will score higher with increasing imbalance. } \label{tab:imbalanced-results}
\end{table}


As described in Section \ref{sec:method}, our method can use any prior distribution over clusters, not just a uniform distribution (i.e., equal cluster sizes) as most existing methods assume. Here, we conduct an empirical investigation of performance on imbalanced data. We assume that, in this case, we have an idea of the relative frequency of each class/cluster and that this determines our prior. To explore varying levels of class-imbalance, we begin with the balanced datasets from Section \ref{subsec:main-results}, and progressively remove parts of some classes, giving three increasing levels of imbalance. As shown in Table \ref{tab:imbalanced-results}, our method does not degrade in performance over these settings, showing robustness to varying class distributions.


%\begin{SCtable*}
\begin{table*}
\resizebox{0.68\textwidth}{!}{
\begin{tabular}{lllllll}
\toprule
         &      &    CA (ours) &           SK &          ENT &            SS &          CKM \\
\midrule
C10 & linear &  \textit{35.5 (0.73)} &  30.0 (0.66) &  35.4 (1.53) &   26.0 (2.39) & \textbf{ 40.2 (0.30)} \\
         &  KNN &  30.3 (0.87) &  27.1 (1.02) &  \textit{33.3 (2.29)} &   22.8 (1.74) &  \textbf{38.3 (0.61)} \\
C100 & linear &  \textbf{17.1 (0.08)} &   8.9 (1.74) &   6.1 (0.77) &    7.9 (1.03) &  \textit{16.9 (0.48)} \\
         &  KNN &  \textit{12.8 (0.39)} &   8.4 (1.88) &   5.3 (0.86) &    7.7 (1.04) &  \textbf{13.7 (0.73)} \\
FM & linear &  79.3 (1.83) &  75.8 (1.39) &  \textit{80.1 (1.36)} &   62.8 (2.55) &  \textbf{81.9 (0.53)} \\
         &  KNN &  77.0 (1.76) &  \textit{80.5 (0.58)} &  80.4 (1.09) &   78.5 (1.53) &  \textbf{81.3 (0.80)} \\
STL & linear &  \textit{36.0 (2.03)} &  32.4 (0.76) &  33.6 (0.80) &   33.0 (1.57) &  \textbf{40.4 (1.49)} \\
         &  KNN &  \textbf{36.8 (1.85)} &  27.3 (3.33) &  27.5 (0.70) &   29.0 (1.49) &  \textit{35.6 (1.74)} \\
RD & linear &  \textbf{95.9 (0.58)} &  79.7 (5.21) &  38.5 (6.93) &   31.1 (2.44) &  \textit{83.4 (1.77)} \\
         &  KNN &  \textbf{98.8 (0.23)} &  \textit{97.1 (1.87)} &  93.4 (1.03) &  40.8 (28.58) &  88.7 (2.24) \\
\bottomrule
\end{tabular}

}
\centering
\caption{Quality of the learned representations, as assessed by linear and KNN probes. Our method performs comparably to CKM on the vision datasets, and better on RealDISP, fitting with the case made in \citet{pmlr-v189-mahon23a} against autoencoders in HAR clustering.} \label{tab:probe-results}
\end{table*}
%\end{SCtable*}

\subsection{Hard vs.\ Soft Assignment Regularization}
A key element of our method is the regularization of hard assignments, whereas previous methods regularize soft cluster assignments. Ours is a fundamentally different form of regularization, and one which we argue is better able to prevent collapse. For example, assume there are only three clusters and a batch size of four, and consider the following two matrices of assignment probabilities
\[
D_1 = \begin{bmatrix}
.98 & .01 & .01 \\
.98 & .01 & .01 \\
.49 & .50 & .01 \\
.49 & .01 & .50 
\end{bmatrix}
D_2 = \begin{bmatrix}
.34 & .33 & .33 \\
.34 & .33 & .33 \\
.34 & .33 & .33 \\
.34 & .33 & .33 
\end{bmatrix}  \,.
\]
The hard and soft entropy (i.e., the entropy of marginal hard and soft assignments) are $1.5$ and $1.1$ for $D_1$, and $0$ and $1.58$ for $D_2$. That means $D_2$ has higher soft entropy but a much lower hard entropy. Indeed, despite having nearly maximum soft entropy, $D_2$ is collapsed with zero hard entropy. A similar scenario is shown for an idealized batch in Figure \ref{fig:hard-soft-comparison}, again revealing that near-uniform soft assignments is a different objective to, and does not guarantee, avoiding collapse.

\begin{figure}[h]
    \centering
    %\includegraphics[width=\columnwidth]{hard-soft-diagram-wide.png}
    \includegraphics[width=\columnwidth]{hard-soft-diagram.png}
    
    \caption{%Effects of different regularization, with batch of size 4, and 5 clusters, distinguished by colour. 
   Top: with no regularization, the model collapses. %; it places most of the probability mass on the same cluster for each data point, and the argmax is the same for each data point. 
    Middle: soft regularization encourages uniform soft assignments but the argmax is constant. % is the same for every data point.%, so all points are assigned to the same cluster, and the model is also collapsed. 
    Bottom: hard regularization causes the argmax to change, and avoids collapse.}%It allows the marginal soft assignment probabilities to deviate from uniform, and instead encourages uniformity in the number assigned to each cluster. This model avoids collapse.}% In order to make the figure more readable, we cut off the top and middle graphs at 80\% and 40\%, respectively. On full graphs, the hard marginal for cluster 0 would extend up to 100\%.}
    \label{fig:hard-soft-comparison}
\vspace{-1.5ex}
\end{figure}

To investigate whether such differences between hard vs.\ soft regularization manifest in practice, we empirically compare the KL$^*$ for the hard and soft assignments. The results are shown in Table \ref{tab:entropy-comparison}.
The most striking difference between hard and soft entropy is in SS. There, the soft KL$^*$ is often close to the maximum value (equal to the logarithm of the number of clusters), but the hard entropy is consistently close to zero. This shows that this form of regularization produces batch assignment probabilities similar to matrix $D_2$ above, where the probabilities for each data point are squeezed close to one another, without much change in the order of highest to lowest, in particular the argmax.

\begin{table}

\resizebox{\columnwidth}{!}{
\begin{tabular}{@{\,}l*{5}{l}l@{\,}}
\cline{1-7}
    &              &  CA &       SK &      Ent &       SS &     \!\!CKM \\
\cline{1-7}
\multirow{2}{*}{CIFAR 10}
    & hard KL$^*$ &       \textbf{0.04} &     0.56     &     1.05     &     1.66 & 1.13 \\
    & soft KL$^*$ &       0.04 &     \textbf{0.00} &     0.85     &     0.51 & 0.03\\
\cline{1-7}
\multirow{2}{*}{CIFAR 100} 
    & hard KL$^*$ &       \textbf{0.83} &     2.17     &     3.48     &     4.47 & 2.78 \\
    & soft KL$^*$ &       0.13 &     \textbf{0.01} &    1.42     &     0.09 & 0.02\\
\cline{1-7}
\multirow{2}{*}{FMNIST} 
    & hard KL$^*$ &       \textbf{0.04} &     0.47     &     0.99     &     2.30 & 1.32 \\
    & soft KL$^*$ &       0.04 &     \textbf{0.00} &    0.69     &     0.17  & 0.02 \\
\cline{1-7}
\multirow{2}{*}{STL} 
    & hard KL$^*$ &       \textbf{0.09} &     2.31 &     2.25     &     2.25 & 1.17 \\
    & soft KL$^*$ &       0.08 &     \textbf{0.00} &     0.92     &     0.18 & 0.02 \\
\cline{1-7}
\multirow{2}{*}{RealDisp} 
    & hard KL$^*$ &       \textbf{0.05} &     0.66 &     1.69     &   2.05   & 1.65 \\
    & soft KL$^*$ &       0.00 &     \textbf{0.00} &     0.08     &     0.00 & 0.02 \\
\cline{1-7}
\end{tabular}
}
       \caption{Comparison of KL$^*$ for hard and soft assignments. Previous methods regularize soft assignments, but this does not transfer well to hard assignments. Ours (CA) is the only method that closely approximates the true distribution for hard assignments. Lower is better, best results in bold.}
    \label{tab:entropy-comparison}
\vspace{-1.5ex}
\end{table}

A similar discrepancy is found in SK, which produces near-perfect uniformity in the soft assignments, and with maximum entropy (up to rounding) on each dataset. This is because it is a (close approximation to a) hard constraint problem. However,  SK's hard assignments still show significant variability, and markedly lower entropy than the soft assignments, suggesting it also produces batch assignment probabilities similar to $D_2$ above. Our method, on the other hand, explicitly forces the argmax to be more evenly distributed during training and, as Table \ref{tab:entropy-comparison} shows, this transfers to the testing setting as well. (Recall that, during testing, we simply assign each point to the cluster with the nearest centroid.) Our hard entropy is only slightly lower than our soft entropy, and is consistently higher than that of the other three methods. This supports our argument that the mean soft assignments do not contain sufficient information to determine if the clustering model is learning a meaningful partition, and regularizing this quantity is not an optimal way to prevent collapse. The distribution of the argmax is also important and is not captured by mean soft assignment.

Our analysis here of hard vs.\ soft cluster assignments does not contradict \citet{caron2020unsupervised}. They report better results using soft assignments as training labels, while we show the superiority of \emph{regularizing}, i.e., encouraging equal numbers of, hard assignments. The two are different contexts of hard and soft labels. We also explored training SK (the method used by \citeauthor{caron2020unsupervised}) using soft assignments as targets, but the results were slightly worse than using hard targets. %Our own method performs hard batch assignments by definition, so there is no soft training setting to consider.


\subsection{Quality of Learned Representations} \label{subsec:learned-representations}
Although ours is primarily a clustering method, we also examine the quality of the learned representations, using linear-probe protocol \cite{zhang2017split}, which trains a linear model and KNN model ($k=10$) to predict the class labels from encoded feature vectors. The results are shown in Table \ref{tab:probe-results}. It is expected that CKM performs well in this setting, because it uses an autoencoder, which is a widely used method for representation learning. Still, however, our method performs on par with CKM, and outperforms the other methods. Note also CKM's poor performance on RealDisp, suggesting that the effectiveness of a decoder may be specific to simple 
vision~datasets.
%Our method performs on par with, or better than, existing methods for preventing collapse. On FashionMNIST, all methods score highly, and there is little difference between the scores for each method. On the other three datasets, our method outperforms existing methods for preventing collapse (with the exception of KNN of the ``ENT'' method on Cifar 10). This shows that not only does our method for avoiding collapse lead to accurate clustering, but it also produces high-quality representations.


\section{Conclusion} \label{sec:conclusion}
This paper proposed a data-augmentation-free method to prevent collapse in online deep clustering. We frame probabilistically the problem of deciding which clusters to assign a batch of data points to, given the cluster centroids and features of the data points, and hence derive an intuitve optimization objective for making hard cluster assignments. We then described an algorithm to approximately solve this optimization problem and demonstrated empirically on four datasets that this method outperforms existing methods, both in preventing collapse and in the resulting clustering performance. Finally, we analyzed how the cluster distribution is affected by our partition support method and previous comparable methods. The analysis suggests that regularizing the soft assignments, as is done by existing works, is not sufficient to prevent collapse, and that a better approach is to regularize the hard assignments, as is done by our method.

\section*{Acknowledgments}
This work was supported by the
AXA Research Fund.
{\small
%\bibliographystyle{aaai24}
\bibliography{bibliography}
}

\input{technical_appendix}
\end{document}
