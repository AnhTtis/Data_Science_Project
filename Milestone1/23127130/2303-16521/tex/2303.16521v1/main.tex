% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{url}
\usepackage{multirow}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{bbm}
\usepackage{newfloat}
\usepackage{sidecap}


\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\makeatletter
\renewcommand{\Function}[2]{%
  \csname ALG@cmd@\ALG@L @Function\endcsname{#1}{#2}%
  \def\jayden@currentfunction{#1}%
}
\newcommand{\funclabel}[1]{%
  \@bsphack
  \protected@write\@auxout{}{%
    \string\newlabel{#1}{{\jayden@currentfunction}{\thepage}}%
  }%
  \@esphack
}
\makeatother
\renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.8}	% max fraction of floats at bottom
    %   Parameters for TEXT pages (not float pages):
   \DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS

\title{Hard Regularization to Prevent Deep Clustering Collapse without Data Augmentation}
\author{Louis Mahon\inst{1} \and Thomas Lukasiewcz\inst{2}}
\institute{School of Informatics, University of Edinburgh \and School of Informatics, TU Wien}

\begin{document}

\maketitle
%\begin{center}
%  \Large\bfseries\boldmath
%  Hard Regularization to Prevent Collapse in Online Deep Clustering without Data Augmentation
%\end{center}

\begin{abstract}
Online deep clustering refers to the joint use of a feature extraction network and a clustering model to assign cluster labels to each new data point or batch as it is processed. While faster and more versatile than offline methods, online clustering can easily reach the collapsed solution where the encoder maps all inputs to the same point and all are put into a single cluster. Successful existing models have employed various techniques to avoid this problem, most of which require data augmentation or which aim to make the average soft assignment across the dataset the same for each cluster. We propose a method that does not require data augmentation, and that, differently from existing methods, regularizes the hard assignments. Using a Bayesian framework, we derive an intuitive optimization objective that can be straightforwardly included in the training of the encoder network. Tested on four image datasets, it consistently avoids collapse more robustly than other methods and leads to more accurate clustering. We also conduct further experiments and analyses justifying our choice to regularize the hard cluster assignments.
\end{abstract}

\section{Introduction}
%The initial successes of deep learning in computer vision, beginning with AlexNet in the ImageNet Challenge \cite{krizhevsky2012imagenet}, were all in the supervised domain. However, more recently, unsupervised learning is attracting increasing attention, because it avoids the need for manually labelled data, a need that becomes more costly the bigger datasets become, and because it is capable of learning previously unknown features of the data, which were not given in the annotations. Natural language processing (NLP) was greatly advanced by the adoption of unsupervised learning in the form of masked language modelling (e.g., \cite{devlin2018bert}), as this enabled models to be trained on large amounts of unlabelled text data (e.g., \cite{brown2020language}).In computer vision, unsupervised learning has not yet seen the same degree of success as in NLP, though substanstial progress is being made. 

%Unsupervised computer vision models can be broadly divided into clustering models, which aim to place each data point into a cluster, so as to maximize similarity within clusters and minimize it between clusters, and contrastive learning models, which train a model without labels by encouraging representations for certain data points (positive pairs) to be similar, and those for others (negative pairs) to be dissimilar. While some methods propose to leverage contrastive learning for clustering performance \cite{li2020prototypical,cai2021large,caron2020unsupervised}, the two are fundamentally different objectives. The former returns a partition of the dataset and the latter, generally, returns a feature vector for each data point. Contrastive learning requires some prior knowledge about the data in order to be able to select which data points should be similar or different. Often, this prior knowledge comes in the form of data augmentation (DA), where two augmented versions of the same data point serve as positive pairs, and this requires knowing the appropriate semantic symmetries used for augmentation, e.g., reflection or brightness shift. Contrastive learning can also require a large number of negative pairs, if there are a large number of semantically meaningful axes of variation in the data. Clustering, on the other hand, can be performed even on datasets with no prior knowledge, and does not require the generation of negative pairs.

Deep clustering refers to the combination of deep learning and clustering, where the data are first encoded with a deep neural network to a feature space, and then clustering is performed in the feature space. Deep clustering models (and clustering models more generally) can be classified as offline or online. Offline models process the entire dataset, and then assign all cluster labels at once. Online models, in contrast, assign a cluster label to each data point, or each batch, as it is processed. Offline methods can produce more accurate clusterings (e.g., \cite{mahon2021selective,niu2021spice} reaching close to supervised performance), as they can leverage information from later data points when assigning cluster labels to earlier data points. However, they are more expensive to train, as they must alternate between encoding the entire dataset, or training the encoder for some number of epochs, and clustering the encoded data. Online models, on the other hand, can jointly encode and cluster, so are less computationally expensive. They are also more versatile, being applicable in real-world settings where new data is constantly becoming available, as opposed to offline methods, which are limited to having a fixed, pre-defined dataset \cite{silva2013data}.

The disadvantage of online methods is that they are more difficult to train. In particular, they run the risk of producing a degenerate solution where the large majority of data points are concentrated in a small number of clusters. In the extreme case, all points are placed into the same cluster. For example, an intuitive way to formulate a training procedure for online deep clustering is to update both the encoder network and the clustering parameters to make the encoding of each point close to its cluster centroid and far from other cluster centroids. (The clustering parameters are e.g., the centroids in K-means or the first two moments in a Gaussian mixture model.) However, this training objective is trivially minimized by the encoder network mapping all data points to the same point in feature space, where this point is equal to one of the cluster centroids. Techniques for avoiding collapse we refer to as partition support. Several partition support methods have been proposed, but they mostly require data augmentation (DA), and those that do not are often ad hoc and lack a rigorous technical foundation. Additionally, they take the soft assignments as a measure of partition collapse and propose to regularize the soft assignments to make them more uniform. This paper focuses on deep clustering without DA, which is an advantage because relying on DA limits a method to domains in which sufficient prior knowledge to perform class-preserving augmentations is available. Additionally, we argue that soft assignments are not an accurate measure of collapse, and that we should instead focus on hard assignments.

We propose a DA-free partition support by regularizing hard assignments. Specifically, we consider the problem of how to optimally assign each data point in a batch to the most appropriate cluster. We express this problem probabilistically in a Bayesian framework, where the regularizing element is captured by a prior across clusters. In the case of equally-sized clusters, this can be uniform. We then use this expression to derive a precise optimization objective, which we also show to be equivalent, up to a small error term, to the objective of maximizing the mutual information of the cluster assignments and the data index. This objective itself is too slow to solve exactly, but we devise a greedy approximation algorithm that can be implemented straightforwardly and results in an intuitive method for fully online clustering, which we term combination assignment. This method outperforms existing online DA-free clustering methods on four popular image clustering datasets. We also analyze the underlying representations and show them to be of high quality. Finally, we analyze the role of hard vs.\ soft cluster assignments in our partition support method, and in previous methods, and make the case that regularizing hard assignments is a more effective approach. Note that, although existing methods can easily convert soft assignments to hard assignments, this is very different from regularizing the hard assignments, as we propose. While the relation between hard and soft clustering has been studied before \cite{bora2014comparative,kearns1998information}, its study in the context of regularizing online deep clustering collapse is new. 

Our main contributions are briefly summarized below.
\begin{itemize}
    \item We articulate a clear Bayesian framework of the problem of hard assignments in online deep clustering models, which is more theoretically correct than requiring uniformity in each batch, as done by existing methods.
    \item We use this framework to derive an optimization objective and prove that it is approximately equivalent to an information-theoretic framework that maximizes the mutual information of the cluster assignments and the data index.
    %\item We devise a greedy algorithm to approximately solve this optimization objective, and prove equivalence to maximizing mutual information.
    \item We show empirically that the resulting method significantly outperforms existing partition support methods, both in avoiding partition collapse and resulting clustering accuracy.
    \item We conduct further analysis of the performance of different partition support methods, which justifies our choice to focus on hard assignments.
\end{itemize}

The rest of this paper is organized as follows. Section \ref{sec:related-work} gives an overview of related work, while Section \ref{sec:method} lays the theoretical foundations of our method, describes our greedy algorithm for optimizing the resulting objective and proves the equivalence to mutual information maximization. Section \ref{sec:experimental-eval} reports our empirical results and analysis, and finally Section \ref{sec:conclusion} summarizes our findings.

\section{Related Work} \label{sec:related-work}
%The first deep clustering models for images used autoencoders. After training the autoencoder to reconstruct the input, the decoder could be discarded, and the encoder used to extract feature vectors which were fed to a clustering algorithm \cite{yang2017towards,huang2014deep,xie2016unsupervised}. Autoencoders are trained on reconstruction loss in pixel space, and while this can be successful on simple datasets such as MNIST, where images from the same class have significant pixel overlap, it does not transfer to more realistic datasets where, e.g., two different images of a cat could have very different pixel values but we still desire their feature vectors to be similar. Therefore, autoencoder-based clustering is limited for complex images. 

%Offline clustering models, known as pseudo-label training, was proposed by \cite{caron2018deep}. The output of a randomly initialized convolutional encoder is clustered using k-means. Then, the cluster labels are used as training targets to update the encoder and the procedure iterated. Most recent deep clustering methods employ a version of pseudo-label training \cite{wu2019deep,mrabah2019deep,van2020scan,mahon2021selective,niu2021spice}. The strongest performance has been obtained by those methods that carefully select and refine the pseudo-labels they use for training, e.g. \cite{mahon2021selective,niu2021spice}. The disadvantages of pseudo-label training is that it is slow to alternate back and forth between computing pseudo-labels and using them as targets for training. Also, it is almost always required to be offline, because the pseudo-labels are computed using an offline clustering algorithm such as k-means, so cannot operate in a real-world setting in which new data is continuously becoming available. 

A key component of online deep clustering methods is how they avoid the collapsed solution, where (almost) every data point is placed in the same cluster. Methods designed for contrastive learning, and those clustering models that employ it as a part of the training procedure, are more resistant to collapse, because the negative pairs are encouraged to be represented differently. This can mean that they are to be placed in different clusters \cite{huang2021deep}, or just that the encodings should be far apart \cite{zhang2021supporting,cai2021large}. Either approach helps resist all points having the same representation. Even without negative pairs, data augmentation is essential for some methods to avoid collapse. In \cite{grill2020bootstrap}, representation learning is performed without negative pairs, only positive pairs that are encouraged to be similar. Then  an ``online'' network is trained to predict the output of a ``target'' network, where the target network is a slow-moving average of the online network. In \cite{zbontar2021barlow}, data-augmented pairs are used to reduce redundancy in the representations by minimizing off-diagonals in the cross-correlation matrix.

Among online clustering models, several partition support methods have been proposed. The solution in \cite{kulshreshtha2018online} is simply to freeze the encoder during clustering, but this requires pretraining it on a separate task. A different approach is taken by \cite{gao2020deep}, who use an online autoencoder-based (AE) clustering model, where the reconstruction loss helps to avoid partition collapse, but this limits the method to simple datasets, because the AE's reconstruction loss requires that different images from the same class have significant pixel overlap. In \cite{zhan2020online}, the loss function is continually reweighted to encourage the assignment to smaller clusters. Additionally, when clusters decrease below a certain threshold, they are deleted, and the largest cluster is split in two using K-means. Data-augmented pairs are used by \cite{cai2021large}, with the same method as \cite{zbontar2021barlow}, treating soft cluster assignments as representations, and use data-augmented pairs. In \cite{zhong2020deep}, the sum of squares of the probability (i.e., soft assignment) of each cluster is minimized, marginalized over each training batch. Decomposing the expectation of the square, we see that this also involves minimizing the variance. However, the authors also use contrastive learning and DA to avoid collapse, so they do not fully rely on sum-of-squares minimization.

A similar idea, employed by \cite{li2020prototypical,hu2017learning,niu2021spice,niu2020gatcluster,van2020scan}, is to maximize the  entropy of soft assignments, marginalized over each training batch. For an input distribution $X$ with corresponding soft assigned labels $Y$, both approximated over a batch, an extra term $-H(Y)$ is added to the loss function. As the entropy of a multinomial is maximized at the uniform distribution, this encourages more equally sized clusters. Entropy maximization has the advantage of a solid formal interpretation as part of the maximization of the mutual information between data and cluster assignments: by maximizing $H(Y)$ but minimizing $H(Y|X)$ (the latter is minimized explicitly in \cite{hu2017learning} and implicitly via contrastive learning in \cite{li2020prototypical}), we are maximizing 
\[
I(X;Y) = H(Y) - H(Y|X)\,.
\]
However, the marginal entropy term tends to only be partially successful at preventing partition collapse, often the entire dataset is still put into only a small number of clusters. As well as being confirmed in our experiments in Section \ref{sec:experimental-eval}, this empirical weakness of entropy maximization for avoiding partition collapse is reported in \cite{hu2017learning}, and better results are found by explicitly constraining the soft cluster assignments using non-linear programming. The work \cite{li2020prototypical} does not rely entirely on entropy maximization, because they employ contrastive learning, which (as explained above) also helps to avoid partition collapse. Thus, while theoretically sound, entropy maximization is not sufficiently effective empirically.

Another approach is to directly impose a constraint on cluster assignments. Based on earlier work \cite{asano2019self}, \cite{caron2020unsupervised,deshmukh2021representation,kumar2021unsupervised} proposed to constrain the soft cluster assignments to be marginally uniform across each training batch. Though effective in preventing partition collapse, the constraint of exact uniformity across each batch is too strict, as noted in \cite{kumar2021unsupervised}. The ground-truth distribution of classes will almost certainly violate this constraint. %This excessive strictness is unsatisfying theoretically, as well as hurting performance empirically.


\section{Method} \label{sec:method}
\subsection{Problem Formulation} \label{subsec:problem-formulation}
We want to simultaneously (a) train the encoder and (b) make hard assignments to each batch of points at a time, based on the features extracted by that encoder. If the encoder is $f_{\theta_1}: \mathbb{R}^n \rightarrow \mathbb{R}^m$, our clustering model is $g_{\theta_2}:\mathbb{R}^m \rightarrow \{1, \dots, K\}$, parametrized by $\theta_2$, and our batch size is $N$, then we seek a function of the form $\Gamma: \mathbb{R}^{N \times m}\ \rightarrow$ $\{1, \dots, K\}^{N}$. (The difference between $\Gamma$ and $g_{\theta_2}$ is that the former is used during training to assign an entire batch, while the latter is used at inference time and can assign each point individually.) %It is possible to use the clustering model itself to assign batch labels during training, though this is not advisable, as discussed below.
If we have a method for batchwise assignment, then the training objective can be formulated as minimizing some notion of distance of points from the centroids of their assigned clusters, and the encoder and clustering model can be trained as follows:
\begin{equation}
    \argmin_{\theta_1, \theta_2} \sum_{i=1}^N d(f_{\theta_1}(x_i) - \mu_{k_i})\,, \label{eq:online-cluster-obj-euclidean}
\end{equation}
where $x_i$ is the $i$th data point in the batch, $d(\cdot)$ is some distance function, e.g., Euclidean distance, $k_i=\Gamma(f_{\theta_1}(x))_i$ is the cluster label assigned to the encoding of $x_i$ under~$f$, and $\mu_k$ is the $k$th cluster centroid.
%Here we express $\theta_2$ as a matrix whose rows are cluster centroids, and use python-style indexing to indicate the $k$th row of $\theta_2$.

Finding a suitable assignment function $\Gamma$ is non-trivial. It should be more likely to assign points to the clusters whose centroids are closer. However, using only this rule, and assigning every vector to its closest centroid, we allow a collapsed solution where \eqref{eq:online-cluster-obj-euclidean} is minimized by $f_{\theta_1}$ mapping every point to a single centroid: $\forall i, f_{\theta_1}(x_i)=c_k$, for some $k\in [K]$. (In this paper, we use $[n]$ to denote the set of integers from $0$ to $n-1$ inclusive). The heart of our method is the choice of a suitable $\Gamma$, which avoids the collapsed solution. We refer to it as combination assignment, because it uses a prior over the combination of such labels under a multinoulli distribution

\vspace{-1ex}
\subsection{Combination Assignment} \label{subsec:comb-assignment}
Combination assignment is based on a Bayesian formulation of the online clustering problem. Let $\mathcal{D}$ be the data distribution, $X \sim \mathcal{D}$ be a sampled batch of data, $Z$ be the random variable defined by applying the feature extraction to $X$ (i.e., $Z$ is the output of a deep encoder network), and let $Y$ be the assigned cluster labels. Here, we consider the encoder to be fixed, and we are just interested in the best hard assignment of cluster labels, given the extracted features. That is, we want to determine the values of $Y$ with the maximum probability under the a posteriori distribution $p(Y|Z)$. We assume we have some reasonable estimate of the prior distribution over $K$ clusters: $p:[K] \rightarrow [0,1]$.
%This is a weaker and more realistic assumption than enforcing the prior distribution on each batch, as done by existing methods. 
Then, the prior probability of a batch containing exactly $n_k$ labels for each cluster~$k \in\{ 1, \dots, K\}$~is 
\begin{equation} \label{eq:prior}
    \prod_{k=1}^Kp(k)^{n_k} \frac{N!}{\prod_{k=1}^K n_k!}\,,
\end{equation}
where $N=\sum_{k=1}^K n_k$ is the batch size.  It is often suitable to choose a uniform prior over $K$ cluster labels, $p(i) = 1/K$, however we are free to choose any prior we would like. This is an advantage over existing methods that assume equal numbers of each cluster by design. For example, all methods that use k-means as their backbone are implicitly assuming roughly uniform clusters \cite{satapathy2015emerging}. 

We model each cluster as a multivariate normal distribution in feature space, so that the likelihood is given by
\begin{gather}
p(Z=z_1, \dots, z_N|Y=k_1,\dots,k_N) = \\
\prod_{i=1}^N \frac{\exp(-\tfrac{1}{2}(z_i- \mu_{k_i})\Sigma_{k_i}^{-1}(z_i-\mu_{k_i}))}{\sqrt{(2 \pi)^{d}|\Sigma_{k_i}|}}\,,\label{full-likelihood}
\end{gather}
where $d$ is the dimension of the feature space, and $\mu_k$ and $\Sigma_k$ are the centroid and covariance matrix of the $k$th cluster, respectively. 
%If we further assume each cluster is spherical, with the same isotropic variance across all clusters, i.e., $\Sigma_k = \sigma I,\text{ for } k\in\{1,\dots,K\}$, and 
We then maximize the posterior corresponding to the prior in \eqref{eq:prior} and the likelihood in \eqref{full-likelihood}, giving the following optimization problem (more details are given in the appendix):
\begin{align} \label{eq:sum-objective}
    &\argmin_Y \sum_{i=1}^N d(z_i,\mu_i,\Sigma_i) - \log{p(k_i)} + \sum_{k=1}^K\log( n_k!) \\
    \text{where  }& d(z,\mu,\Sigma) =\tfrac{1}{2}(z- \mu)^T\Sigma^{-1}(z-\mu) + \tfrac{1}{2}\log{(2\pi)^d|\Sigma|}\,. \notag
\end{align}
In matrix notation, the objective is 
\begin{gather}
    \argmin_{Q \in \mathcal{B}^{N \times K}} \langle Q, \tilde{Q} \rangle - \mathbbm{1}_N^TQ\log{P} + \log(\mathbbm{1}_N^TQ!) \mathbbm{1}_K \label{eq:matrix-objective} \\
    \text{subject to } Q\mathbbm{1}_K = \mathbbm{1}_N\,, \notag
\end{gather}
where $\tilde{Q}_{i,j} = d(z_i, \mu_j, \Sigma_j)$, $\langle \cdot , \cdot \rangle$ denotes the Frobenius inner product, $\mathbbm{1}_a$ is an $a$-dimensional vector of all ones, $P$ is a $K$-dimensional probability vector specifying the prior, and $\log$ and factorial are applied element-wise to the $K$-dimensional vector $\mathbbm{1}_N^TQ$. The requirement that $Q$ be a Boolean matrix enforces hard assignments. The constraint $Q\mathbbm{1}_K = \mathbbm{1}_N$ specifies the rows of $Q$ to be one-hot vectors, i.e., each latent vector is assigned to exactly one cluster.

\subsection{Solving the Optimization Problem} \label{subsec:greedy-strategy}
Unfortunately, it is too slow to solve \eqref{eq:matrix-objective} exactly (see appendix for details). However, to motivate an approximation, we can consider the simpler problem of assigning the last data point in a batch, given that the rest have already been assigned. That is, we maximize the conditional probability of the $N$th assignment in a batch, conditioned on the $N-1$ previous assignments. This gives the following optimization problem (details in the appendix).
\begin{gather}
%    \argmin_{k_N=1,\dots,K} ||z_N- \mu_{k_N}||^2 - \log{p(k_N)}+ 2\sigma \log( n_{k_N}+1!) - \log( n_{k_N}!) = \notag \\
    \argmin_{k_N=1,\dots,K} d(z_N,\mu_{k_N},\Sigma_{k_N}) - \log{p(k_N)}+ \log( n_{k_N}+1)\,, \label{eq:Nth-point-assignment}
\end{gather}
where $n_k$ is the number of points assigned to cluster $k$ before the $n$th assignment.

To approximately solve \eqref{eq:matrix-objective}, we employ a greedy algorithm that iteratively solves \eqref{eq:Nth-point-assignment} with respect to the most confident assignment possible. That is, at each iteration, select the pair $(i,k)$ (corresponding to $(N,k_N)$ above) for which \eqref{eq:Nth-point-assignment} is smallest, with $i$ ranging over the indices of still-unassigned points, and $k$ ranging over all clusters, and assign the $i$th point to cluster $k$.

\vspace{-1ex}
\subsection{Intuition}
To get an intuition on our method, recall that clustering generally should assign points to the closest centroid, but that we should try to resist assigning to a cluster that already has lots of points assigned to it and also take into account the prior. Even if such a cluster has its centroid closest to the $N$th point, it may be better to instead assign to a smaller, further away cluster, or one with a higher prior probability. This suggests choosing the cluster assignment so as to minimize a combination of the distance to the centroid, the prior and some increasing function of cluster size, which is precisely what \eqref{eq:Nth-point-assignment} expresses. The first term says to pick a nearby cluster, the second term to pick a cluster with a high prior probability, and the third to penalize clusters that already have many points assigned. It would not be obvious, a priori, what the increasing function of cluster-size should be exactly, but the derivation of \eqref{eq:Nth-point-assignment} shows that $\log{(n+1)}$ is an appropriate choice. The fact that we take into account the cluster size when assigning points to clusters is an important difference between our method and existing methods, which generally assign to the cluster with the closest centroid (e.g., k-means) or the cluster which assigns the highest probability to the given data point (e.g., GMM clustering). Another important difference is that the centroids are trainable parameters, and can be updated by gradient descent, rather than being set to the empirical mean of the corresponding cluster.

\subsection{Information-Theoretic Interpretation}
Here, we show that, under a uniform prior, the greedy algorithm that iteratively solves \eqref{eq:Nth-point-assignment} can be interpreted as iteratively making whatever assignment will maximize the mutual information between the batch index $i$ and the cluster labels. First, we show a close equivalence to maximizing the entropy of cluster labels in each batch.
%Specifically, we consider the objective of choosing the assignment that maximizes some combination of the likelihood and the marginal entropy. By the following argument, this can be seen to lead to essentially the same procedure as that described in Section \ref{subsec:greedy-strategy}.

Let $H^{(k)}$ be the marginal hard entropy of cluster labels after a new assignment to cluster $k$:
\begin{gather*}
    H^{(k)} = \frac{x_k + 1}{N+1} \log{\frac{x_k + 1}{N+1}} + \sum_{j=1, j\neq k}^K \frac{x_j}{N+1} \log \frac{x_j}{N+1}\,,
\end{gather*}
and consider the difference $ H^{(k)} - H^{(k')}$ between the entropy after making assignment $k$ vs.\ after making a different assignment $k'$. It can be shown (see appendix) that 

\begin{equation} \label{eq:entropy-assignment-objective}
    H^{(k)} - H^{(k')} \approx  \frac{1}{N+1} \left( \log(x_{k'} + 1) - \log (x_{k} + 1)\right)\,.
\end{equation}
Thus, if we were to make each assignment so as to maximize $\log(\mathcal{L}_k(x)) + \lambda H^{(k)}$,
where $\mathcal{L}(k)$ is the likelihood of the new data point under cluster $k$ and $\lambda$ is some hyperparameter, then, subject to the above approximation, for each $k,k' \in \{1, \dots, K\}$, we would prefer to assign to $k$ iff
\begin{gather}
    \log \mathcal{L}(k) + \lambda H^{(k)} > \log \mathcal{L}(k') + \lambda H^{(k')} \iff \notag \\
%    \log \mathcal{L}(k) - \log \mathcal{L}(k') >  \lambda H^{(k')} - \lambda H^{(k)} \iff \notag \\
    \log \mathcal{L}(k) - \log\mathcal{L}(k') >  \frac{\lambda}{N+1} \left( \log(x_{k'} + 1) - \log (x_{k} + 1)\right)\,. \label{eq:entropy-assignment-objective-1}
\end{gather}
Modelling clusters as multivariate normal distributions (as above), and setting $\lambda = N+1$, \eqref{eq:entropy-assignment-objective-1} becomes equivalent to \eqref{eq:Nth-point-assignment}. (See appendix for full proof.)

Thus, our method closely approximates a maximization of the entropy of cluster labels. There is some similarity to those methods, discussed in Section \ref{sec:related-work}, that use an additional loss term to encourage greater entropy of soft assignments in each batch, but an important difference here is that we are maximizing the entropy of hard assignments. This means that the entropy of cluster labels given batch index is automatically zero. Therefore, using the decomposition $I(X;Y) = H(X) - H(X|Y)$, the mutual information of the batch index $i$ and the cluster labels equals the entropy of cluster labels, and so our method is a close approximation to maximizing this mutual information.


\subsection{Training Procedure}
Our model comprises an encoder network $f_{\theta_1}$ and a set of cluster centroids $\theta_2 = \{\mu_1, \dots, \mu_K\}$. To train on an input batch, we first encode the raw data using $f_{\theta_1}$, then we employ the combination assignment method of Section \ref{subsec:greedy-strategy}, and use these assignments to minimize \eqref{eq:sum-objective} with respect to both $\theta_1$ and $\theta_2$. As the second two terms in \eqref{eq:sum-objective} have no gradient, the updates are made only with respect to the first term, so similarly to \eqref{eq:online-cluster-obj-euclidean}. At inference time, we do not perform combination assignment. Instead, we simply assign each point to the cluster with the nearest centroid, so the resulting clustering model can assign points individually, and is not restricted to assigning cluster labels batch-wise. The full methods for training and inference are described by the functions %\ref{func:train-method} and \ref{func:inference-method}, 
TrainOnBatch and PredictBatch, respectively, in Algorithm~\ref{alg:method}.

\begin{algorithm}[tb]
    \caption{\textbf{(combination assignment):} During training, cluster labels are assigned batchwise, with partition support provided by a uniform prior across clusters. During inference, cluster labels are assigned pointwise, without any explicit partition support.} \label{alg:method}
    \begin{algorithmic}
    \State $f_{\theta_1} \gets$ encoder network; 
    \State $\theta_2 = \mu_1, \dots, \mu_K \gets$ centroids for each of the $K$ clusters; 
    \State $\sigma \gets$ isotropic variance of all clusters; 
    \Function{AssignBatch}{Z}   
        \State $counts \gets$ $K$-dimensional array, initially all $0$s; 
        \State $isAssigned \gets$ $N$-dimensional Boolean array, initially all False; 
        \State $D \gets$ $N \times K$ matrix, where $D_{ij} = ||Z_{i} - \mu_{j} ||^2$; %, $i=1, \dots, N$
        \For{r=1,\dots,N}
            \State $\tilde{D} \gets$ $N \times K$ matrix, where $\tilde{D}_{ij} = D_{ij} + 2\sigma \log(counts[i] + 1)$; 
            \State $(a,k) \gets$ argmin $\{\tilde{D}_{ij} : \neg isAssigned[i]\}$; 
            \State $counts[k] \gets counts[k] + 1$; 
            \State $isAssigned[a] \gets True$; 
            \State $loss \gets loss + D_{a,k}$
        \EndFor 
        \State \Return $loss$
    \EndFunction 
    \Function{TrainOnBatch}{X} \funclabel{func:train-method}
        \State $Z \gets f_{\theta_1}(X)$, encodings for a batch of $N$ data points; 
        \State $loss \gets AssignBatch(Z)$;
        \State take a gradient descent step on $loss$ with respect to $\theta_1, \theta_2$
    \EndFunction 
    \Function{PredictDataPoint}{x} \funclabel{func:inference-method}
        \State $z \gets f_\theta(x)$ encodings for a batch of $N$ data points; 
        \State $assignment = \argmin_{j=1,\dots,K} ||z-\mu_j||^2$; 
        \State \Return assignment
    \EndFunction
    \end{algorithmic}
\end{algorithm}



\section{Experimental Evaluation} \label{sec:experimental-eval}
\begin{table*}[t]
    %\vspace{-10pt}
    \centering
        \caption{\footnotesize Effect, on cluster size and clustering performance, of our method compared to two existing methods of preventing partition collapse. ``CA'' refers to our method of combination assignment, ``SK'' refers to Sinkhorn-Knopp regularization, as proposed by \cite{caron2020unsupervised} and \cite{chen2021exploring}, ``Ent'' refers to entropy maximization, as used by  \cite{hu2017learning}, ``SS'' is the sum of squares minimization proposed in \cite{zhong2020deep} and others, and ``No-Reg'' is the model without any partition support component. Along with standard cluster metrics, we report the KL-divergence from the ground truth distribution across clusters (denoted ``KL$^*$'') to indicate the extent of partition collapse. All figures are the mean of 5 runs, with std dev in parentheses. Best results are in bold.}\label{tab:main-results}
   \resizebox{0.95\textwidth}{!}{\begin{tabular}{*{7}{c}}
\hline
 & & CA & SK & Ent & SS & No Reg \\\hline
	\multirow{4}{*}{Cifar10}  & Acc & \textbf{22.7 (2.07)} & 16.7 (0.36) & 18.6 (1.36) & 11.8 (1.72) & 10.0 (0.00) \\
%\cline{2-7}
	 & NMI & \textbf{10.1 (1.68)} & 3.8 (0.68) & 8.7 (2.63) & 1.1 (1.15) & 0.0 (0.00) \\
%\cline{2-7}
	 & ARI & \textbf{5.8 (0.93)} & 2.7 (0.60) & 5.7 (1.76) & 0.3 (0.38) & 0.0 (0.00) \\
%\cline{2-7}
    & KL$^*$ & \textbf{0.04 (0.01)} & 0.47 (0.31) & 0.71 (0.28) & 1.32 (0.36) & 1.59 (0.00) \\
\hline
	\multirow{4}{*}{Cifar100}  & Acc & \textbf{6.4 (0.22)} & 2.6 (0.21) & 2.4 (0.17) & 1.2 (0.22) & 1.0 (0.00) \\
%\cline{2-7}
	 & NMI & \textbf{13.2 (0.37)} & 5.3 (0.41) & 6.3 (0.73) & 0.6 (1.05) & 0.0 (0.00) \\
%\cline{2-7}
	 & ARI & \textbf{1.7 (0.14)} & 0.3 (0.04) & 0.5 (0.10) & 0.0 (0.04) & 0.0 (0.00) \\
%\cline{2-7}
    & KL$^*$ & \textbf{0.81 (0.07)} & 1.63 (0.47) & 2.98 (0.47) & 3.76 (0.37) & 4.50 (0.12) \\
\hline
	\multirow{4}{*}{FashionMNIST}  & Acc & \textbf{54.5 (6.96)} & 25.1 (2.80) & 25.5 (5.51) & 10.0 (0.04) & 10.0 (0.00) \\
%\cline{2-7}
	 & NMI & \textbf{53.2 (4.23)} & 17.7 (2.08) & 20.9 (10.12) & 0.0 (0.04) & 0.0 (0.00) \\
%\cline{2-7}
	 & ARI & \textbf{39.1 (6.29)} & 9.2 (1.27) & 10.6 (5.37) & 0.0 (0.00) & 0.0 (0.00) \\
%\cline{2-7}
    & KL$^*$ &  \textbf{0.04 (0.00)} & 0.45 (0.27) & 0.62 (0.37) & 1.34 (0.57) & 2.30 (0.01) \\
\hline
	\multirow{4}{*}{STL}  & Acc & \textbf{23.5 (1.42)} & 15.2 (1.03) & 10.9 (1.76) & 10.1 (0.20) & 10.0 (0.00) \\
%\cline{2-7}
	 & NMI & \textbf{13.7 (1.33)} & 2.8 (0.53) & 1.3 (2.56) & 0.0 (0.08) & 0.0 (0.00) \\
%\cline{2-7}
	 & ARI & \textbf{7.1 (0.70)} & 1.1 (0.37) & 0.2 (0.32) & 0.0 (0.00) & 0.0 (0.00) \\
%\cline{2-7}
    & KL$^*$ &  \textbf{0.09 (0.01)} & 0.60 (0.30) & 1.38 (0.79) & 2.23 (0.12) & 2.30 (0.00) \\
\hline
 \end{tabular}} \vspace{-10pt}
 \end{table*}


\subsection{Datasets and Metrics}
We report results on four popular image clustering datasets: CIFAR 10, CIFAR 100, FashionMNIST and STL, with image sizes 32, 32, 28 and 96, respectively. We use the standard clustering metrics of accuracy (ACC), normalized mutual information (NMI), and adjusted Rand index (ARI), defined as, e.g., in \cite{sheng2020unsupervised}. We also report the KL-divergence from the ground truth of the model's empirical distribution over clusters. For a collapsed model whose distribution is skewed towards a small number of clusters, this KL-divergence will be high.

\subsection{Clustering Accuracy and Degree of Collapse} \label{subsec:main-results}
Table \ref{tab:main-results} compares our method with three existing methods: sum of squares minimization, denoted ``SS'' \cite{zhong2020deep}, the Sinkhorn-Knopp algorithm for optimal transport, denoted ``SK'' \cite{caron2020unsupervised,kumar2021unsupervised}, and marginal entropy maximization \cite{li2020prototypical}, denoted ``Ent''. Each is described in Section \ref{sec:related-work}. To make further explicit the phenomenon of partition collapse, we also include a model without any partition support. Our method significantly outperforms others on all datasets and metrics. 

Observe that the unregularized model exhibits total collapse in all experiments, placing all points in the same cluster and consequently achieving a cluster performance no better than random guessing. In many of our experiments, the performance of SS is not much better. By making a slight change to the author's original method, we could actually significantly improve its results (see appendix), but it was still unreliable and less accurate than our method.  The other two existing partition support methods do a reasonable job of avoiding partition collapse. However, entropy maximization occasionally also reaches the state with all points in the same cluster (this is consistent with previous literature, e.g., \cite{hu2017learning}). The Sinkhorn-Knopp method is more reliable, but by far the most uniform cluster sizes are produced by our method. Note that we do not employ our assignment algorithm at inference time, instead we just assign each point to the cluster with the nearest centroid. This shows that our cluster centroids are well-distributed around the data manifold, each capturing a sizeable subset of the data even when the explicit support is removed. Together, these figures show that (a) some form of partition support is necessary to learn anything meaningful, (b) our method of combination assignment is better at avoiding partition collapse than previous methods, and (c) this leads to our model producing a better clustering performance.

To better isolate the effect of our proposed partition support method, we do not perform hyperparameter tuning, and use a relatively simple architecture for all datasets. This is the same for all methods being compared. The network consists of two convolutional layers with filter sizes $6$ and $16$,  and ReLU activations, followed by a fully connected layer to a latent space of dimension $128$. Training uses Adam, with learning rate 1e-3, $\beta_1=0.9$, $\beta_2 = 0.99$, and batch size 256. We follow previous works in setting $K$, the number of clusters, to the number of ground-truth classes. We set $\Sigma=\sigma I$ with $\sigma=1e2$. We observe very similar results for a wide range of values for $\Sigma$, including a setting where we continually estimate from the empirical distributions. 

Although some existing clustering methods produce higher accuracy than that reported in Table \ref{tab:main-results}, this is not a valid comparison because (as discussed above) these higher-scoring methods (a) use data-augmentation and (b) even more significantly, are offline clustering methods. It is considerably easier to score highly in the offline setting. As our method operates in the online setting, it is not comparable.


 \subsection{Imbalanced Data} \label{subsec:imbalanced-data}
 As described in Section \ref{sec:method}, our method is compatible with any prior distribution over clusters, not just a uniform distribution (i.e., equal cluster sizes) as most existing methods assume. To investigate empirically performance under non-uniform priors, we apply our method to cluster imbalanced data. We assume that, in this case, we have an idea of the relative frequency of each class/cluster and that this determines our prior. In order to explore varying levels of class-imbalance, we begin with the balanced datasets from Section \ref{subsec:main-results}, and progressively remove parts of some classes, giving three increasing levels of imbalance. As shown in Table \ref{tab:imbalanced-results}, our method does not degrade in performance over these three settings, showing it to be robust to varying class distributions.

\begin{SCtable}
\caption{\footnotesize Performance of our method on imbalanced data. The three settings, `imb. 1-3', have increasing degrees of class imbalance. Note, there is a slight decrease in difficulty of the task, i.e., random guess will score higher with increasing imbalance. In the first level of imbalance, the ratio in the size of the smallest class to the largest class is $0.8$, in the second level it is $0.55$ and in the third it is $0.1$. See appendix for precise distributions. } \label{tab:imbalanced-results}
\resizebox{0.60\textwidth}{!}{
\begin{tabular}{lllll}
\toprule
    &     & imb.1 & imb.2 & imb.3 \\
\midrule
Cifar10 & acc &  22.2 (1.22) &  22.8 (1.46) &  24.6 (1.26) \\
    & nmi &  10.5 (1.14) &  11.3 (0.73) &  10.5 (0.50) \\
    & ari &   5.9 (0.92) &   6.3 (0.55) &   6.2 (3.22) \\
    & KL* &   0.1 (0.07) &   0.1 (0.04) &   0.0 (0.07) \\
Cifar100 & acc &   6.6 (0.31) &   6.7 (0.62) &   9.0 (0.33) \\
    & nmi &  13.3 (0.62) &  13.7 (0.99) &  15.4 (0.30) \\
    & ari &   1.6 (0.19) &   1.9 (0.36) &   2.7 (0.27) \\
    & KL* &   0.7 (0.10) &   0.8 (0.22) &   0.9 (0.16) \\
FMNIST & acc &  54.6 (6.34) &  52.1 (3.39) &  53.8 (2.95) \\
    & nmi &  52.9 (4.41) &  50.6 (1.50) &  50.0 (2.35) \\
    & ari &  37.7 (5.56) &  36.7 (1.84) &  38.5 (3.08) \\
    & KL* &   0.2 (0.23) &   0.0 (0.06) &   0.2 (0.23) \\
STL & acc &  23.2 (2.07) &  24.6 (1.41) &  25.5 (3.13) \\
    & nmi &  13.3 (1.69) &  14.5 (1.15) &  12.6 (2.44) \\
    & ari &   6.8 (1.13) &   7.8 (0.81) &   6.8 (1.90) \\
    & KL* &   0.3 (0.21) &   0.1 (0.22) &   0.0 (0.06) \\
\bottomrule
\end{tabular}
}
\vspace{-6ex}
\end{SCtable}

\begin{table*}
\centering
\caption{\footnotesize Quality of the learned representations, as assessed by the accuracy of a linear model and K-nearest neighbours model, trained to predict the class labels.} \label{tab:probe-results}
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{llllll}
\toprule
    &      &    CA (ours) &           SK &          ENT &           SS \\
\midrule
Cifar 10 & linear &  \textbf{35.5 (0.73)} &  30.0 (0.66) &  35.4 (1.53) &  26.0 (2.39) \\
    &  KNN &  30.3 (0.87) &  27.1 (1.02) &  \textbf{33.3 (2.29)} &  22.8 (1.74) \\
Cifar 100 & linear &  \textbf{17.1 (0.08)} &   8.9 (1.74) &   6.1 (0.77) &   7.9 (1.03) \\
    &  KNN &  \textbf{12.8 (0.39)} &   8.4 (1.88) &   5.3 (0.86) &   7.7 (1.04) \\
Fashion MNIST & linear &  79.3 (1.83) &  75.8 (1.39) & \textbf{ 80.1 (1.36)} &  62.8 (2.55) \\
    &  KNN &  77.0 (1.76) &  \textbf{80.5 (0.58)} &  80.4 (1.09) &  78.5 (1.53) \\
STL & linear &  \textbf{35.9 (1.51)} &  32.4 (0.76) &  33.6 (0.80) &  33.0 (1.57) \\
    &  KNN &  \textbf{35.3 (1.13)} &  27.3 (3.33) &  27.5 (0.70) &  29.0 (1.49) \\
\bottomrule
\end{tabular}
}
\end{table*}

\subsection{Hard vs.\ Soft Assignment Regularization}
A key element of our method is the regularization of hard assignments, whereas previous methods regularize soft cluster assignments. Ours is a fundamentally different form of regularization, and one which we argue is better able to prevent collapse. For example, assume there are only three clusters and a batch size of four, and consider the following two matrices of assignment probabilities
\[
D_1 = \begin{bmatrix}
.98 & .01 & .01 \\
.98 & .01 & .01 \\
.49 & .50 & .01 \\
.49 & .01 & .50 
\end{bmatrix}
D_2 = \begin{bmatrix}
.34 & .33 & .33 \\
.34 & .33 & .33 \\
.34 & .33 & .33 \\
.34 & .33 & .33 
\end{bmatrix}  \,.
\]
The hard and soft entropy (i.e., the entropy of marginal hard and soft assignments, respectively) for $D_1$ are $1.5$ and $1.1$, respectively, and for $D_2$ they are $0$ and $1.58$, respectively. That means $D_2$ has higher soft entropy than $D_1$ but a much lower hard entropy. Indeed, despite having nearly maximum soft entropy, $D_2$ is collapsed with zero hard entropy. A similar scenario is shown for an idealized batch in Figure \ref{fig:hard-soft-comparison}, again revealing that near-uniform soft assignments is a different objective to, and does not guarantee, avoiding collapse.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{hard-soft-diagram-wide.png}
    
    \caption{\footnotesize Types of assignments encouraged by different types of regularization, for an idealized batch of size 4, with 5 clusters, indicated by different colours.}%\textbf{Top}: with no regularization, the model collapses; it places most of the probability mass on the same cluster for each data point, and the argmax is the same for each data point. \textbf{Middle}: soft regularization forces the soft marginal assignments to be close to uniform, but still the argmax is the same for every data point, so all points are assigned to the same cluster, and the model is also collapsed. \textbf{Bottom}: regularizing the hard assignments causes the argmax to change. It allows the marginal soft assignment probabilities to deviate from uniform, and instead encourages uniformity in the number assigned to each cluster. This model avoids collapse. In order to make the figure more readable, we cut off the top and middle graphs at 80\% and 40\%, respectively. On full graphs, the hard marginal for cluster 0 would extend up to 100\%.}
    \label{fig:hard-soft-comparison}
\end{figure}

To investigate whether such differences between hard vs.\ soft regularization manifest in practice, we empirically measure the variance and entropy of hard and soft assignment probabilities, marginalized across the entire dataset, for each method tested. The results are shown in Table \ref{tab:entropy-comparison}.

\begin{SCtable}

       \caption{\footnotesize Comparison of KL-divergence from the ground-truth cluster distribution, with respect to both hard and soft assignments. Previous methods regularize the soft assignments, but this does not transfer well to the hard assignments. Ours (CA) is the only method that closely approximates the true distribution for the hard assignments. A lower score is better, the best results are in bold.}
    \label{tab:entropy-comparison}
\begin{tabular}{l*{16}{l}}
\cline{1-6}
    &              &  CA &       SK &      Ent &       SS \\
\cline{1-6}
\multirow{2}{*}{Cifar10}
    & hard KL$^*$ &       \textbf{0.04} &     0.56     &     1.05     &     1.66 \\
    & soft KL$^*$ &       0.04 &     \textbf{0.00} &     0.85     &     0.51 \\
\cline{1-6}
\multirow{2}{*}{Cifar100} 
    & hard KL$^*$ &       \textbf{0.83} &     2.17     &     3.48     &     4.47 \\
    & soft KL$^*$ &       0.13 &     \textbf{0.01} &    1.42     &     0.09 \\
\cline{1-6}
\multirow{2}{*}{FMNIST} 
    & hard KL$^*$ &       \textbf{0.04} &     0.47     &     0.99     &     2.30 \\
    & soft KL$^*$ &       0.04 &     \textbf{0.00} &    0.69     &     0.17  \\
\cline{1-6}
\multirow{2}{*}{STL} 
    & hard KL$^*$ &       \textbf{0.09} &     2.31 &     2.25     &     2.25\\
    & soft KL$^*$ &       0.08 &     \textbf{0.00} &     0.92     &     0.18 \\
\cline{1-6}
\end{tabular}
\end{SCtable}

The most striking difference between hard and soft entropy is in SS. There, the soft entropy is often close to the maximum value (equal to the logarithm of the number of clusters), but the hard entropy is consistently close to zero. This shows that this form of regularization produces batch assignment probabilities similar to matrix $D_2$ above, where the probabilities for each data point are squeezed close to one another, without much change in the order of highest to lowest, in particular the argmax.

A similar discrepancy is found in SK, which produces near-perfect uniformity in the soft assignments, with and reaches the maximum possible entropy (up to rounding) on each dataset. This is because it is a (close approximation to a) hard constraint problem. However,  SK's hard assignments still show significant variability, and markedly lower entropy than the soft assignments. This suggests that applying the SK algorithm also produces batch assignment probabilities somewhat similar to $D_2$ above. Our method, on the other hand, explicitly forces the argmax to be more evenly distributed during training and, as Table \ref{tab:entropy-comparison} shows, this transfers to the testing setting as well. (Recall that, during testing, we simply assign each point to the cluster with the nearest centroid.) Our hard entropy is only slightly lower than our soft entropy, and is consistently higher than that of the other three methods. This supports our argument that the mean soft assignments do not contain sufficient information to determine if the clustering model is learning a meaningful partition, and regularizing this quantity is not an optimal way to prevent collapse. The pattern in the argmax is also important and is not captured by mean soft assignment.

Our analysis here of hard vs.\ soft cluster assignments does not contradict \cite{caron2020unsupervised}. They report better results using soft assignments as training labels, while we show the superiority of \emph{regularizing}, i.e., encouraging equal numbers of, hard assignments. The two are different contexts of hard and soft labels. We also explored training SK (the method used by \cite{caron2020unsupervised}) using soft assignments as targets, but the results were slightly worse than using hard targets. %Our own method performs hard batch assignments by definition, so there is no soft training setting to consider.


\subsection{Quality of Learned Representations} \label{subsec:learned-representations}
Although ours is primarily a clustering method, it is also interesting to examine the quality of the learned representations. To do this, we follow the commonly used linear-probe protocol \cite{zhang2017split}, where, after training the main model, we train a simple linear model to predict the ground truth labels from the feature vectors extracted by the encoder. We also train a k-nearest neighbours model, with $k=10$. The results are shown in Table \ref{tab:probe-results}. Our method performs on par with, or better than, existing methods for preventing collapse. On FashionMNIST, all methods score highly, and there is little difference between the scores for each method. On the other three datasets, our method outperforms existing methods for preventing collapse (with the exception of KNN of the ``ENT'' method on Cifar 10). This shows that not only does our method for avoiding collapse lead to accurate clustering, but it also produces high-quality representations.


\section{Conclusion} \label{sec:conclusion}
This paper proposed a data-augmentation-free method to prevent collapse in online deep clustering. We frame probabilistically the problem of deciding which clusters to assign a batch of data points to, given the cluster centroids and features of the data points, and we use this framing to derive a concise optimization objective for making hard cluster assignments. We then described an algorithm to approximately solve this optimization problem and demonstrated empirically on four datasets that this method outperforms existing methods, both in better preventing collapse and in leading to better clustering performance. Finally, we analyzed how the cluster assignment distribution is affected by our partition support method and previous comparable methods. The analysis suggests that regularizing the soft assignments, as is done by existing works, is not sufficient to prevent collapse, and that a better approach is to regularize the hard assignments, as is done by our method.

{\small
\bibliographystyle{splncs04}
\bibliography{bibliography}
}

\clearpage
\input{appendix}

\end{document}