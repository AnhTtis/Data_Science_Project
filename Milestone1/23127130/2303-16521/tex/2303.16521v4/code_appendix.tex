\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage[submission]{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{graphicx}
\usepackage{url}
\usepackage{multirow}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{newfloat}
\usepackage{sidecap}
\usepackage{dsfont}
\usepackage{placeins}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}

\makeatletter
\renewcommand{\Function}[2]{%
  \csname ALG@cmd@\ALG@L @Function\endcsname{#1}{#2}%
  \def\jayden@currentfunction{#1}%
}
\newcommand{\funclabel}[1]{%
  \@bsphack
  \protected@write\@auxout{}{%
    \string\newlabel{#1}{{\jayden@currentfunction}{\thepage}}%
  }%
  \@esphack
}
\makeatother

\catcode`\_=12
% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Hard Regularization to Prevent Deep Online Clustering Collapse without Data Augmentation}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Marc Pujol-Gonzalez\equalcontrib
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    1900 Embarcadero Road, Suite 101\\
    Palo Alto, California 94303-3310 USA\\
    % email address must be in roman text type, not monospace or sans serif
    proceedings-questions@aaai.org
}


\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi



\begin{document}
\section{train.py}
from time import time
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from os.path import join
from dl_utils.label_funcs import label_counts, get_trans_dict, accuracy
from dl_utils.tensor_funcs import cudify, numpyify
from dl_utils.misc import set_experiment_dir, asMinutes, scatter_clusters
from dl_utils.torch_misc import CifarLikeDataset
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.distributions import Categorical
import cl_args
from pdb import set_trace
import numpy as np
import torch
from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score


class ClusterNet(nn.Module):
    def __init__(self,ARGS):
        super().__init__()
        self.bs_train = ARGS.batch_size_train
        self.bs_val = ARGS.batch_size_val
        self.nc = ARGS.nc
        self.nz = ARGS.nz
        #self.sigma = ARGS.sigma
        if ARGS.dataset == 'tweets':
            counts = np.load('datasets/tweets/cluster_label_counts.npy')
            self.log_prior = np.log(counts/counts.sum())
        else:
            self.prior = ARGS.prior
        self.log_prior = np.log(self.prior)

        if ARGS.dataset == 'imt':
            out_conv_shape = 13
        elif ARGS.dataset == 'stl':
            out_conv_shape = 21
        elif ARGS.dataset == 'fashmnist':
            out_conv_shape = 4
        else:
            out_conv_shape = 5
        nc = 1 if ARGS.dataset == 'fashmnist' else 3
        self.conv1 = nn.Conv2d(3, 6, 5)
        if ARGS.arch == 'alex':
            self.net = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=False)
            self.net.classifier = self.net.classifier[:5] # remove final linear and relu
            self.net.classifier[4] = nn.Linear(4096,self.nz,device='cuda')
        if ARGS.arch == 'res':
            self.net = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)
            self.net.fc.weight.data = self.net.fc.weight.data[:self.nz]
            self.net.fc.bias.data = self.net.fc.bias.data[:self.nz]
            self.net.fc.out_features = self.nz
        elif ARGS.arch == 'simp':
            self.net = nn.Sequential(
                nn.Conv2d(nc, 6, 5),
                nn.BatchNorm2d(6),
                nn.ReLU(),
                nn.MaxPool2d(2, 2),
                nn.Conv2d(6, 16, 5),
                nn.BatchNorm2d(16),
                nn.ReLU(),
                nn.MaxPool2d(2, 2),
                nn.Flatten(1),
                nn.Linear(16 * out_conv_shape * out_conv_shape, self.nz),
                )
        elif ARGS.arch == 'fc':
            self.net = nn.Sequential(
                nn.Linear(768,ARGS.hidden_dim),
                nn.ReLU(),
                nn.Linear(ARGS.hidden_dim,self.nz))

        self.opt = optim.Adam(self.net.parameters(),lr=ARGS.lr)

        self.centroids = torch.randn(ARGS.nc,ARGS.nz,requires_grad=True,device='cuda',dtype=torch.double)
        self.inv_covars = (1/ARGS.sigma)*torch.eye(ARGS.nz,requires_grad=ARGS.is_train_covars,device='cuda',dtype=torch.double).repeat(self.nc,1,1)
        self.half_log_det_inv_covars = torch.log(torch.tensor(ARGS.sigma))*self.nz/2
        #self.inv_covars = torch.randn(ARGS.nc,ARGS.nz,ARGS.nz,requires_grad=True,device='cuda',dtype=torch.double)
        self.ng_opt = optim.Adam([{'params':self.centroids}],lr=ARGS.lr)
        if ARGS.is_train_covars:
            self.ng_opt.add_param_group({'params':self.inv_covars})
        self.cluster_log_probs = None
        self.cluster_counts = torch.zeros(ARGS.nc,device='cuda').long()
        self.raw_counts = torch.zeros(ARGS.nc,device='cuda').long()
        self.total_soft_counts = torch.zeros(ARGS.nc,device='cuda')

        self.epoch_num = -1
        self.temp = ARGS.temp

        self.training = True

    def train(self):
        self.training = True
        self.net.train()
        self.centroids.requires_grad = True

    def eval(self):
        self.training = False
        self.net.eval()
        self.centroids.requires_grad = False

    def reset_scores(self):
        self.cluster_counts = torch.zeros(self.nc,device='cuda').long()
        self.raw_counts = torch.zeros(self.nc,device='cuda').long()

    def init_keys_as_dpoints(self,dloader):
        self.eval()
        inp,targets = next(iter(dloader))
        inp = inp[:self.nc]
        while len(inp) < self.nc:
            new_inp,targets = next(iter(dloader))
            inp = torch.cat([inp,new_inp[:self.nc-len(inp)]])
        sample_feature_vecs = self.net(inp.cuda())
        self.centroids = sample_feature_vecs.clone().detach().double().requires_grad_(True)

    def forward(self, inp):
        self.bs = inp.shape[0]
        feature_vecs = self.net(inp)
        cluster_dists = feature_vecs[:,None]-self.centroids
        self.cluster_log_probs = torch.einsum('bcu,czu,bcz->bc',cluster_dists,self.inv_covars,cluster_dists) - self.half_log_det_inv_covars
        #assert torch.allclose(self.cluster_log_probs, cluster_dists.norm(dim=2)**2)
        #assert all([torch.allclose(self.cluster_log_probs[b,c],(cluster_dists[b,c]@self.inv_covars[c]) @cluster_dists[b,c]) for b in range(self.bs) for c in range(self.nc)])
        self.assign_batch()
        return feature_vecs

    def assign_batch(self):
        if ARGS.ng:
            self.cluster_loss,self.batch_assignments = neural_gas_loss(.1*self.cluster_log_probs+(self.cluster_counts+1).log(),self.temp)
        elif ARGS.var:
            self.assign_batch_var()
        elif ARGS.kl:
            self.assign_batch_kl()
        elif ARGS.sinkhorn:
            self.assign_batch_sinkhorn()
        elif ARGS.no_reg:
            min_dists, self.batch_assignments = self.cluster_log_probs.min(axis=1)
            self.cluster_loss = min_dists.mean()
        else:
            self.assign_batch_probabilistic()

        self.raw_counts.index_put_(indices=[self.cluster_log_probs.argmin(axis=1)],values=torch.ones_like(self.batch_assignments),accumulate=True)
        if ARGS.ng or ARGS.sinkhorn or ARGS.kl:
            self.cluster_counts.index_put_(indices=[self.batch_assignments],values=torch.ones_like(self.batch_assignments),accumulate=True)
        if not ARGS.sinkhorn or ARGS.ng:
            self.soft_counts = (-self.cluster_log_probs).softmax(axis=1).sum(axis=0).detach()
        self.total_soft_counts += self.soft_counts

    def assign_batch_sinkhorn(self):
        """Implements the method referred to in the paper as 'SK'"""
        with torch.no_grad():
            #hard_counts = (torch.arange(self.nc).cuda() == self.cluster_log_probs.argmax(axis=1,keepdims=True)).float()
            soft_assignments = sinkhorn(-self.cluster_log_probs,is_hard_reg=ARGS.hard_sinkhorn,eps=.5,niters=15)
        if self.prior != 'uniform' and self.epoch_num>0 and self.batch_assignments = soft_assignments.argmin(axis=1)
        self.soft_counts = soft_assignments.sum(axis=0).detach()
        self.cluster_loss = self.cluster_log_probs[torch.arange(self.bs),self.batch_assignments].mean()

    def assign_batch_var(self):
        """Implements the method referred to in the paper as 'VAR'"""
        self.batch_assignments = self.cluster_log_probs.argmin(axis=1)
        #self.cluster_loss = 10*(self.cluster_log_probs**2).sum()
        if ARGS.var_improved:
            self.cluster_loss = 10*self.cluster_log_probs.mean(axis=0).var()
        else:
            self.cluster_loss = 10*(self.cluster_log_probs.mean(axis=0)**2).mean()
        self.cluster_loss +=.1*self.cluster_log_probs[torch.arange(self.bs),self.batch_assignments].mean()

    def assign_batch_kl(self):
        """Implements the method referred to in the paper as 'ENT'"""
        self.batch_assignments = self.cluster_log_probs.argmin(axis=1)
        self.cluster_loss = 100*-Categorical(self.cluster_log_probs.mean(axis=0)).entropy()
        if ARGS.kl_cent:
            self.cluster_loss +=.1*self.cluster_log_probs[torch.arange(self.bs),self.batch_assignments].mean()
        else:
            self.cluster_loss += .1*Categorical(self.cluster_log_probs).entropy().mean()

    def assign_batch_probabilistic(self):
        """Implements the our method, referred to in the paper as 'CA'"""
        assigned_key_order = []
        cost_table = self.cluster_log_probs.transpose(0,1).flatten(1).transpose(0,1)
        self.batch_assignments = torch.zeros_like(self.cluster_log_probs[:,0]).long()
        unassigned_idxs = torch.ones_like(cost_table[:,0]).bool()
        #cost_table = flat_x/(2*ARGS.sigma)
        if ARGS.imbalance > 0 and self.epoch_num > 0:
            cost_table -= self.translated_log_prior
        had_repeats = False
        if not self.training and not ARGS.constrained_eval:
            self.batch_assignments = self.cluster_log_probs.argmin(axis=1)
            return
        assign_iter = 0
        while unassigned_idxs.any():
            assert (~unassigned_idxs).sum() == assign_iter or had_repeats
            cost = (cost_table[unassigned_idxs]+(self.cluster_counts+1).log()).min()
            nzs = ((cost_table+(self.cluster_counts+1).log() == cost)*unassigned_idxs[:,None]).nonzero()
        """Implements the our method, referred to in the paper as 'CA'"""
            if len(nzs)!=1: had_repeats = True
            new_vec_idx, new_assigned_key = nzs[0]
            assert unassigned_idxs[new_vec_idx]
            unassigned_idxs[new_vec_idx] = False
            assigned_key_order.append(new_vec_idx)
            self.batch_assignments[new_vec_idx] = new_assigned_key
            self.cluster_counts[new_assigned_key] += 1
            #assert cost > 0
            assign_iter += 1
        self.cluster_loss = cost_table[torch.arange(self.bs),self.batch_assignments].mean()

    def train_one_epoch(self,trainloader):
        self.train()
        running_loss = 0.0
        self.reset_scores()
        for i, data in enumerate(trainloader):
            if not ARGS.keep_scores:
                self.reset_scores()
            self.batch_inputs, self.batch_labels = data
            if i==ARGS.db_at: set_trace()
            self(self.batch_inputs.cuda())
            self.cluster_loss.backward()
            self.opt.step()
            self.ng_opt.step()
            self.opt.zero_grad(); self.ng_opt.zero_grad()
            if i \% 10 == 0 and i > 0:
                if ARGS.track_counts:
                    for k,v in enumerate(self.cluster_counts):
                        if (rc := self.raw_counts[k].item()) == 0:
                            continue
                        print(f"{k} constrained: {v.item()}\traw: {self.raw_counts[k].item()}\tsoft: {self.soft_counts[k].item():.3f}")
                if not ARGS.suppress_prints:
                    print(f'batch index: {i}\tloss: {running_loss/10:.3f}')
                running_loss = 0.0
            running_loss += self.cluster_loss.item()
            if (self.centroids==0).all(): set_trace()
            if ARGS.is_test > 0:
                break

    def train_epochs(self,num_epochs,dset,val_too=True):
        trainloader = DataLoader(dset,batch_size=self.bs_train,shuffle=True,num_workers=8)
        testloader = DataLoader(dset,batch_size=self.bs_val,shuffle=False,num_workers=8)
        best_acc = -1
        best_nmi = -1
        best_ari = -1
        best_kl_star = -1
        best_linear_probe_acc = -1
        best_knn_probe_acc = -1
        if ARGS.warm_start:
            self.init_keys_as_dpoints(trainloader)
        for epoch_num in range(num_epochs):
            self.epoch_num = epoch_num
            self.total_soft_counts = torch.zeros_like(self.total_soft_counts)
            self.train_one_epoch(trainloader)
            if val_too:
                self.total_soft_counts = torch.zeros_like(self.total_soft_counts)
                with torch.no_grad():
                    self.test_epoch_unsupervised(testloader)
                model_distribution = self.epoch_hard_counts/self.epoch_hard_counts.sum()
                log_quot = np.log((model_distribution/self.prior)+1e-8)
                self.kl_star = np.dot(model_distribution,log_quot)
                if self.nmi > best_nmi:
                    best_nmi = self.nmi
                    best_acc = self.acc
                    best_ari = self.ari
                    best_kl_star = self.kl_star
                else:
                    with torch.no_grad():
                        self.test_epoch_unsupervised(testloader)
                linear_probe_acc, knn_probe_acc = self.train_test_probes(dset)
                if linear_probe_acc > best_linear_probe_acc:
                    best_linear_probe_acc = linear_probe_acc
                    best_knn_probe_acc = knn_probe_acc
        print(f"Best Acc: {best_acc:.3f}\tBest NMI: {best_nmi:.3f}\tBest ARI: {best_ari:.3f}\tBest KL*:{best_kl_star:.5f}\tBest linear probe acc:{best_linear_probe_acc:.3f}\tBest KNN probe acc:{best_knn_probe_acc:.3f}")
        with open(join(ARGS.exp_dir,'ARGS.txt'),'w') as f:
            f.write(f'Dataset: {ARGS.dataset}\n')
            for a in ['batch_size_train','nz','hidden_dim','lr','sigma']:
                f.write(f'{a}: {getattr(ARGS,a)}\n')
            f.write(f'warm_start: {ARGS.warm_start}\n')

        with open(join(ARGS.exp_dir,'results.txt'),'w') as f:
            f.write(f'ACC: {best_acc:.3f}\nNMI: {best_nmi:.3f}\nARI: {best_ari:.3f}\n')
            f.write(f'KL-star: {best_kl_star:.3f}\nLin-Acc: {best_linear_probe_acc:.3f}\nKNN-Acc: {best_knn_probe_acc:.3f}\n')

    def train_test_probes(self,dset):
        self.eval()
        dloader = DataLoader(dset,batch_size=self.bs_val,shuffle=False,num_workers=8)
        all_encodings = []
        for i,data in enumerate(dloader):
            images, labels = data
            encodings = numpyify(self.net(images.cuda()))
            all_encodings.append(encodings)
        X = np.concatenate(all_encodings)
        y = dset.targets
        X_tr,X_ts,y_tr,y_ts = train_test_split(X,y,test_size=0.33)
        lin_reg = LogisticRegression().fit(X_tr,y_tr)
        lin_test_preds = lin_reg.predict(X_ts)
        lin_test_acc = (lin_test_preds==y_ts).mean()
        knn_reg = KNeighborsClassifier(n_neighbors=ARGS.n_neighbors).fit(X_tr,y_tr)
        knn_test_preds = knn_reg.predict(X_ts)
        knn_test_acc = (knn_test_preds==y_ts).mean()
        return lin_test_acc, knn_test_acc

    def test_epoch_unsupervised(self,testloader):
        self.eval()
        preds = []
        all_feature_vecs = []
        data_for_clusters = [[] for _ in range(self.nc)]
        for images,labels in testloader:
            feature_vecs = self(images.cuda())
            all_feature_vecs.append(numpyify(feature_vecs))
            assignments =self.batch_assignments
            for cluster_idx in range(self.nc):
                data_for_clusters[cluster_idx].append(feature_vecs[assignments==cluster_idx])
            preds.append(assignments.detach().cpu().numpy())
        pred_array = np.concatenate(preds)
        num_of_each_label = label_counts(pred_array)
        self.epoch_hard_counts = np.zeros(self.nc)
        for ass,num in num_of_each_label.items():
            self.epoch_hard_counts[ass] = num
        if ARGS.estimate_covars and len(num_of_each_label) == self.nc: # don't set covars if some dpoints missing
            unnormed_inv_covars = torch.stack([torch.inverse(torch.cat(cd).T.cov()) for cd in data_for_clusters])
            self.inv_covars = (unnormed_inv_covars*self.inv_covars.mean()/unnormed_inv_covars.mean()).double()
            if unnormed_inv_covars.isnan().any():
                breakpoint()
        self.epoch_soft_counts = self.total_soft_counts.detach().cpu().numpy()
        self.gt = testloader.dataset.targets
        self.trans_dict = get_trans_dict(np.array(self.gt),pred_array)
        self.acc = accuracy(pred_array,np.array(self.gt))
        if self.acc == 0:
            breakpoint()
        idx_array = np.array(list(self.trans_dict.keys())[:-1])
        self.translated_prior = self.prior[idx_array]
        self.translated_log_prior = cudify(self.log_prior[idx_array])
        self.nmi = normalized_mutual_info_score(pred_array,np.array(self.gt))
        self.ari = adjusted_rand_score(pred_array,np.array(self.gt))
        self.hcv = self.epoch_hard_counts.var()/self.epoch_hard_counts.mean()
        self.scv = self.epoch_soft_counts.var()/self.epoch_hard_counts.mean()
        if ARGS.viz_clusters:
            feature_vecs_array = np.concatenate(all_feature_vecs)
            import umap
            to_viz = umap.UMAP().fit_transform(feature_vecs_array)
            ax = scatter_clusters(to_viz,testloader.dataset.targets)
            breakpoint()

def neural_gas_loss(v,temp):
    n_instances, n_clusters = v.shape
    weightings = (-torch.arange(n_clusters,device=v.device)/temp).exp()
    sorted_v, assignments_order = torch.sort(v)
    assert (sorted_v**2 * weightings).mean() < ((sorted_v**2).mean() * weightings.mean())
    return (sorted_v**2 * weightings).sum(axis=1), assignments_order[:,0]

def sinkhorn(scores, is_hard_reg=False, eps=0.05, niters=3):
    Q = torch.exp(scores / eps).T
    #Q = torch.softmax(scores / eps,1).T
    Q /= sum(Q)
    eps2 = 0.1
    if is_hard_reg:
        hard_counts = (torch.arange(Q.shape[0]).cuda()[:,None] == Q.argmax(axis=0)).float()
        #Q = torch.cat([Q,1*hard_counts.sum(axis=1,keepdims=True)],axis=1)
        Q = (Q + eps2*hard_counts) / (1+eps2)
    K, B = Q.shape
    r, c = torch.ones(K,device=Q.device) / K, torch.ones(B,device=Q.device) / B
    for _ in range(niters):
        u = torch.sum(Q, dim=1)
        Q *= (r / u).unsqueeze(1)
        Q *= (c / torch.sum(Q, dim=0)).unsqueeze(0)
    if is_hard_reg:
        Q = (Q*2) - hard_counts
        #Q = Q[:,:-1]
    return (Q / torch.sum(Q, dim=0, keepdim=True)).T

if __name__ == '__main__':
    ARGS,dataset = cl_args.get_cl_args_and_dset()
    ARGS.exp_dir = set_experiment_dir(f'experiments/{ARGS.expname}',name_of_trials='experiments/tmp',overwrite=ARGS.overwrite)
    start_time = time()
    cluster_net = ClusterNet(ARGS).cuda()
    cluster_net.train_epochs(ARGS.epochs,dataset,val_too=True)
    print(f'Total time: {asMinutes(time()-start_time)}')

\section{cl_args.py}
import math
import argparse
import torch
from dl_utils.torch_misc import CifarLikeDataset
import numpy as np
import get_datasets
from HAR.make_dsets import StepDataset


RELEVANT_ARGS = []
def get_cl_args():
    parser = argparse.ArgumentParser()
    train_type_group = parser.add_mutually_exclusive_group()
    train_type_group.add_argument('--kl',action='store_true')
    train_type_group.add_argument('--var',action='store_true')
    train_type_group.add_argument('--ng',action='store_true')
    train_type_group.add_argument('--no_reg',action='store_true')
    train_type_group.add_argument('--no_cluster_loss',action='store_true')
    train_type_group.add_argument('--sinkhorn',action='store_true')
    parser.add_argument('--arch',type=str,choices=['alex','res','simp','fc','1dcnn'],default='simp')
    parser.add_argument('--gpu',type=str,default='0')
    parser.add_argument('--batch_size_train',type=int,default=256)
    parser.add_argument('--batch_size_val',type=int,default=1024)
    parser.add_argument('--constrained_eval',action='store_true')
    parser.add_argument('--db_at',type=int,default=-1)
    parser.add_argument('--estimate_covars',action='store_true')
    parser.add_argument('--expname',type=str,default='tmp')
    parser.add_argument('--ckm',action='store_true')
    parser.add_argument('--hard_sinkhorn',action='store_true')
    parser.add_argument('--help_sinkhorn',action='store_true')
    parser.add_argument('--hidden_dim',type=int,default=512)
    parser.add_argument('--imbalance',type=int,default=0)
    parser.add_argument('--is_train_covars',action='store_true')
    parser.add_argument('--keep_scores',action='store_true')
    parser.add_argument('--kl_cent',action='store_true')
    parser.add_argument('--linear_probe',action='store_true')
    parser.add_argument('--lr',type=float,default=1e-3)
    parser.add_argument('--n_neighbors',type=int,default=10)
    parser.add_argument('--nc',type=int,default=10)
    parser.add_argument('--nz',type=int,default=128)
    parser.add_argument('--overwrite',action='store_true')
    parser.add_argument('--pretrain_frac',type=float,default=0.5)
    parser.add_argument('--sigma',type=float,default=100.)
    parser.add_argument('--soft_train',action='store_true')
    parser.add_argument('--suppress_prints',action='store_true')
    parser.add_argument('--temp',type=float,default=1.)
    parser.add_argument('--is_test','-t',action='store_true')
    parser.add_argument('--track_counts',action='store_true')
    parser.add_argument('--var_improved',action='store_true')
    parser.add_argument('--verbose',action='store_true')
    parser.add_argument('--viz_clusters',action='store_true')
    parser.add_argument('--warm_start',action='store_true')
    parser.add_argument('-d','--dataset',type=str,choices=['imt','c10','c100','svhn','stl','fashmnist','tweets','realdisp'],default='c10')
    parser.add_argument('-e','--epochs',type=int,default=1)
    ARGS = parser.parse_args()
    if ARGS.is_test > 0:
        ARGS.expname = 'tmp'
    return ARGS

def make_dset_imbalanced(dset,nc,class_probs):
    imbalanced_data = []
    imbalanced_targets = []
    for i,p in enumerate(class_probs):
        targets = np.array(dset.targets)
        label_mask = targets==i
        rand_mask =np.random.rand(sum(label_mask))<p # select each independently, roughly get 1/p
        new_data = dset.data[label_mask][rand_mask]
        new_targets = targets[label_mask][rand_mask]
        imbalanced_data.append(new_data)
        imbalanced_targets.append(new_targets)
    imbalanced_data_arr = np.concatenate(imbalanced_data)
    imbalanced_targets_arr = np.concatenate(imbalanced_targets)
    assert len(imbalanced_data_arr) == len(imbalanced_targets_arr)
    return CifarLikeDataset(imbalanced_data_arr,imbalanced_targets_arr,transform=dset.transform)

def make_dset_imbalanced_har(dset,nc,class_probs):
    chunked_data = np.stack([dset.data[dset.step_size*i:(dset.step_size*i)+dset.window_size] for i in range(len(dset.targets))])
    chunked_data = np.expand_dims(chunked_data,1)
    chunked_dset = CifarLikeDataset(chunked_data,dset.targets)
    return make_dset_imbalanced(chunked_dset,nc,class_probs)

def get_cl_args_and_dset():
    args = get_cl_args()

    dataset = get_datasets.get_dset(args.dataset,args.is_test)
    n_classes = len(set(dataset.targets))
    if args.imbalance==1:
        n = n_classes//2
        m = n_classes - n
        class_probs=np.concatenate([np.ones(m),1-0.2*np.linspace(0,1,n)])
    elif args.imbalance==2:
        class_probs = 1-0.5*np.linspace(0,1-1/n_classes,n_classes)
    elif args.imbalance==3:
        class_probs = 1-np.linspace(0,1-1/n_classes,n_classes)
    if args.dataset == 'c100':
        args.nc = 100
    elif args.dataset == 'imt':
        if args.imbalance>0:
            class_probs = np.tile(class_probs,20)
        args.nc = 200
    elif args.dataset == 'tweets':
        args.nc = 269
        args.arch = 'fc'
    elif args.dataset == 'realdisp':
        args.nc = 33
        args.nz = 32
        args.arch = '1dcnn'
    else:
        args.nc = 10

    is_har = args.dataset == 'realdisp'
    if args.imbalance > 0:
        imb_dset_func = make_dset_imbalanced_har if is_har else make_dset_imbalanced
        dataset = imb_dset_func(dataset,args.nc,class_probs)
        args.prior = class_probs/class_probs.sum()
    else:
        args.prior = np.ones(args.nc)/args.nc
    return args, dataset

\section{get_datasets.py}
import torch
from os.path import join
import torchvision
from torch.utils import data
from torchvision.transforms import Compose, Normalize, ToTensor
import numpy as np
from functools import partial
from dl_utils.torch_misc import CifarLikeDataset
from dl_utils.tensor_funcs import numpyify
from HAR.make_dsets import make_realdisp_dset
from HAR.project_config import realdisp_info


def get_tweets(is_use_testset):
    X = np.load('datasets/tweets/roberta_doc_vecs.npy')
    y = np.load('datasets/tweets/cluster_labels.npy')
    return CifarLikeDataset(X,y)

def get_cifar10(is_use_testset):
    transform = Compose([ToTensor(),Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])
    data_dir = '~/dataset/cifar10_data'
    dset = torchvision.datasets.CIFAR10(root=data_dir,train=not is_use_testset,transform=transform,download=True)
    return dset.data, dset.targets

def get_cifar100(is_use_testset):
    transform = Compose([ToTensor(),Normalize((0.4914, 0.4822, 0.4465), (0.2675, 0.2565, 0.2761))])
    data_dir = '~/datasets/cifar100_data'
    dset = torchvision.datasets.CIFAR100(root=data_dir,train=not is_use_testset,transform=transform,download=True)
    return dset.data, dset.targets

def get_fashmnist(is_use_testset):
    transform = Compose([ToTensor()])
    data_dir = '~/dataset/fashmnist_data'
    dset = torchvision.datasets.FashionMNIST(root=data_dir,train=not is_use_testset,transform=transform,download=True)
    return dset.data, dset.targets

def get_stl(is_test_run):
    transform = Compose([ToTensor(),Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
    data_dir = './dataset/stl_data'
    def wrapper(dset_func):
        def inner():
            dset = dset_func()
            return CifarLikeDataset(np.transpose(dset.data,(0,3,2,1)),dset.labels,transform)
        return inner
    if is_test_run:
        dset = torchvision.datasets.STL10(root=data_dir,split='train',transform=transform,download=True)
    else:
        dset = torchvision.datasets.STL10(root=data_dir,split='test',transform=transform,download=True)

    return np.transpose(dset.data,(0,3,2,1)), dset.labels

def get_train_or_test_dset(dset_name,is_use_testset):
    if dset_name=='c10':
        X,y = get_cifar10(is_use_testset)
    elif dset_name=='c100':
        X,y = get_cifar100(is_use_testset)
    elif dset_name=='stl':
        X,y = get_stl(True)
    elif dset_name=='fashmnist':
        X,y = get_fashmnist(is_use_testset)
    elif dset_name=='imt':
        X,y = get_imagenet_tiny(is_use_testset)
    else:
        print(f'\nUNRECOGNIZED DATASET: {dset_name}\n')
    X = numpyify(X)
    y = numpyify(y)
    return X, y

def get_dset(dset_name,is_test_run):
    if dset_name=='realdisp':
        subj_ids = realdisp_info().possible_subj_ids
        if is_test_run:
            subj_ids = subj_ids[:1]
        dset,_ = make_realdisp_dset(step_size=5,window_size=512,subj_ids=subj_ids)
        return dset
    else:
        X, y = get_train_or_test_dset(dset_name,True)
    if is_test_run:
        X = X[:1000]
        y = y[:1000]
    elif dset_name!='imt': # no train-test split in im-tiny
        X_tr, y_tr = get_train_or_test_dset(dset_name,False)
        X = np.concatenate([X_tr,X])
        y = np.concatenate([y_tr,y])
    if dset_name == 'fashmnist':
        transform = ToTensor()
    elif dset_name == 'imt':
        transform = None
    else:
        transform = Compose([ToTensor(),Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
    return CifarLikeDataset(X,y,transform=transform)

def get_imagenet_tiny(test):
    data_for_each_class = []
    labels_for_each_class = []
    for class_idx in range(200):
        np_class_data = np.load(join('tiny-imagenet-200/np_data',f'{class_idx}.npy'))
        np_class_labels = np.load(join('tiny-imagenet-200/np_data',f'{class_idx}_labels.npy'))
        if test == 1:
            np_class_data = np_class_data[:50]
            np_class_labels = np_class_labels[:50]
        elif test == 2:
            np_class_data = np_class_data[:20]
            np_class_labels = np_class_labels[:20]
        data_for_each_class.append(np_class_data)
        labels_for_each_class.append(np_class_labels)

    data_as_array = torch.tensor(np.concatenate(data_for_each_class)).transpose(1,3).float()
    labels_as_array = torch.tensor(np.concatenate(labels_for_each_class)).long()

    return data_as_array,labels_as_array

\section{utils.py}
import torch.nn as nn
import math
import torch

class EncByLayer(nn.Module):
    def __init__(self,ncvs,ksizes,strides,paddings,max_pools,show_shapes):
        super(EncByLayer,self).__init__()
        self.show_shapes = show_shapes
        num_layers = len(ksizes)
        assert all([isinstance(x,int) for l in (ncvs,ksizes,strides) for x in l])
        assert all(len(x)==num_layers for x in (ksizes,strides,max_pools))
        conv_layers = []
        for i in range(num_layers):
            if i<num_layers-1:
                conv_layer = nn.Sequential(
                nn.Conv2d(ncvs[i],ncvs[i+1],ksizes[i],strides[i],paddings[i]),
                nn.BatchNorm2d(ncvs[i+1]),
                nn.LeakyReLU(0.3),
                nn.MaxPool2d(max_pools[i])
                )
            else: #No batch norm on the last layer
                conv_layer = nn.Sequential(
                nn.Conv2d(ncvs[i],ncvs[i+1],ksizes[i],strides[i],paddings[i]),
                nn.LeakyReLU(0.3),
                nn.MaxPool2d(max_pools[i])
                )
            conv_layers.append(conv_layer)
        self.conv_layers = nn.ModuleList(conv_layers)

    def forward(self,x):
        if self.show_shapes: print(x.shape)
        for i,conv_layer in enumerate(self.conv_layers):
            x = conv_layer(x)
            # sometimes errors in the batchnorm if size is already down to 1
            if self.show_shapes: print(i,x.shape)
        return x

class DecByLayer(nn.Module):
    def __init__(self,ncvs,ksizes,strides,paddings,show_shapes):
        super(DecByLayer,self).__init__()
        self.show_shapes = show_shapes
        n_layers = len(ksizes)
        assert all([isinstance(x,int) for l in (ncvs,ksizes,strides,paddings) for x in l])
        assert all(len(x)==n_layers for x in (ksizes,strides,paddings) )
        conv_trans_layers = [nn.Sequential(
                nn.ConvTranspose2d(ncvs[i],ncvs[i+1],ksizes[i],strides[i],paddings[i]),
                nn.BatchNorm2d(ncvs[i+1]),
                nn.LeakyReLU(0.3),
                )
            for i in range(n_layers)]
        self.conv_trans_layers = nn.ModuleList(conv_trans_layers)

    def forward(self,x):
        if self.show_shapes: print(x.shape)
        for i,conv_trans_layer in enumerate(self.conv_trans_layers):
            x = conv_trans_layer(x)
            if self.show_shapes: print(i,x.shape)
        return x

def increment_approx_exponentially(insize,outsize,n_increments):
    base = (outsize/insize)**(1/(n_increments-1))
    assert base > 1
    increments = [int(insize*base**(i)) for i in range(n_increments)]
    if increments[-1] != outsize:
        print(f'readjusting output size from {increments[-1]} to {outsize}')
        increments[-1] = outsize
    if increments[0] != insize:
        breakpoint()
    return increments

def build_convt_net(in_chans,in_shape,outsize,n_layers):
    chans = list(reversed(increment_approx_exponentially(in_chans,outsize,n_layers+1)))
    sizes = increment_approx_exponentially(1,in_shape,n_layers+1)
    ksizes,strides,paddings = zip(*[infer_single_layer_shape(sizes[i+1],sizes[i]) for i in range(len(sizes)-1)])
    return DecByLayer(chans,ksizes,strides,paddings,False)

def build_conv_net(in_chans,in_shape,outsize,n_layers):
    chans = increment_approx_exponentially(in_chans,outsize,n_layers+1)
    sizes = increment_approx_exponentially(1,in_shape,n_layers+1) # no flatten
    ksizes,strides,paddings = zip(*[infer_single_layer_shape(sizes[i+1],sizes[i]) for i in reversed(range(len(sizes)-1))])
    max_pools = [1]*len(strides) # no max pool, just strides
    return EncByLayer(chans,ksizes,strides,paddings,max_pools,False)

def infer_single_layer_shape(in_size,out_size):
    stride_size = int(in_size/out_size)
    ksize = in_size - stride_size*(out_size - 1)
    tentative_out_size = (in_size - ksize)/stride_size + 1
    assert tentative_out_size == out_size
    padding = int((out_size - tentative_out_size)/2)
    if ksize<3:
        padding = int(math.ceil((3-ksize)/2))
        ksize += 2*padding
    tentative_out_size = (in_size + 2*padding - ksize)/stride_size + 1
    if not tentative_out_size == out_size:
        breakpoint()
    return ksize,stride_size,padding

if __name__ == '__main__':
    enc = build_conv_net(3,64,256,7)
    import pdb; pdb.set_trace()  # XXX BREAKPOINT
    print(enc(torch.ones(1,3,64,64)).shape)
    dec = build_convt_net(3,64,256,2)
    print(dec(torch.ones(1,256,1,1)).shape)

\end{document}