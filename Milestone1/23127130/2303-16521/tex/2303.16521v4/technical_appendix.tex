
\appendix

\onecolumn
\section{Full Proofs and Derivations}
\subsection{Derivation of Main Optimization Objective}
The prior probability of a batch containing exactly $n_k$ labels for each cluster~$k \in\{ 1, \dots, K\}$ is 
\begin{equation*}
    \prod_{k=1}^Kp(k)^{n_k} \frac{N!}{\prod_{k=1}^K n_k!}\,,
\end{equation*}
where $p(\cdot)$ is the prior. As the distribution within each cluster is modelled as multivariate normal, we can write the likelihood as 
\begin{gather*}
p(Z=z_1, \dots, z_N|Y=k_1,\dots,k_N) = \\ \notag
\prod_{i=1}^N \frac{\exp(-\tfrac{1}{2}(z_i- \mu_{k_i})\Sigma_{k_i}^{-1}(z_i-\mu_{k_i}))}{\sqrt{(2 \pi)^{d}|\Sigma_{k_i}|}}\,,\label{full-likelihood-appendix}
\end{gather*}
where $d$ is the dimension of the feature space, and $\mu_k$ and $\Sigma_k$ are the centroid and covariance matrix of the $k$th cluster, respectively. 

Then the full a posteriori is
\begin{align*}
    p(Y|Z) \propto P(Y)P(Z|Y) =&
    \prod_{k=1}^Kp(k)^{n_k} \frac{N!}{\prod_{k=1}^K n_k!}\prod_{i=1}^N \frac{\exp(-\tfrac{1}{2}(z_i- \mu_{k_i})\Sigma_{k_i}^{-1}(z_i-\mu_{k_i}))}{\sqrt{(2 \pi)^{d}|\Sigma_{k_i}|}}\, \\[8pt] 
    \propto & \prod_{k=1}^Kp(k)^{n_k} n_k!\prod_{i=1}^N \frac{\exp(-\tfrac{1}{2}(z_i- \mu_{k_i})\Sigma_{k_i}^{-1}(z_i-\mu_{k_i}))}{\sqrt{(2 \pi)^{d}|\Sigma_{k_i}|}}\,,
\end{align*}
where we drop the constants that are independent of $Y$. Letting $d(z,\mu,\Sigma) =\tfrac{1}{2}(z- \mu)^T\Sigma^{-1}(z-\mu) + \tfrac{1}{2}\log{|\Sigma|} + \tfrac{d}{2}\log{2\pi}$, then, we obtain an optimization objective by minimizing the corresponding negative log-likelihood as follows
\begin{align*}
\argmax_Y p(Y|Z) &\propto \argmax_Y p(Y)P(Z|Y) = \notag \\
 &=   \argmax_Y \log\left(\prod_{k=1}^Kp(k)^{n_k}\right) +  \log\left(\prod_{i=1}^N  \exp(-d(z_i,\mu_{k_i},\Sigma_{k_i})\right)\log(\prod_{k=1}^K n_k!) = \notag \\
%\frac{\exp(-d(z_i,\mu_{k_i},\Sigma_{k_i})}{\sqrt{(2\pi^d}}\right) - 
 &=   \argmax_Y \sum_{k=1}^Kn_k\log{p(k)}+ \sum_{i=1}^N -d(z_i,\mu_{k_i},\Sigma_{k_i}) - \sum_{k=1}^K\log( n_k!) = \notag \\
 &=   \argmax_Y \sum_{i=1}^N\log{p(k_i)}+ \sum_{i=1}^N -d(z_i,\mu_{k_i},\Sigma_{k_i}) - \sum_{k=1}^K\log( n_k!) = \notag \\
 &=   \argmin_Y \sum_{i=1}^N (d(z_i,\mu_{k_i},\Sigma_{k_i}) - \log{p(k_i)}) +  \sum_{k=1}^K\log( n_k!)\,. 
\end{align*}

\subsection{Computational Complexity}
The main optimization objective is too slow to solve exactly. A common solution would involve interpreting the problem as the rectangular assignment problem, where clusters are workers and data points are jobs. Then take the standard representation of the assignment problem as a flow network. Instead of adding one edge from the source vertex for each worker, add $m$ parallel edges for each worker. For $k \in \{0, \dots, m-1\}$ the , $k$th edge for a worker has capacity 1 and cost $\log{k!} - \log{(k-1)!} = \log{k}$. However, using standard solutions to the assignment problem would then result in complexity cubic in $m$, which is the batch size. This would be prohibitively slow for all but very small batch sizes. 

If $C_{Enc}$ is the cost of a forward and backward pass of the encoder, $N$ is the batch size and $M$ is the latent dimension, then the complexity of our method is $C_{Enc} + \theta(N\log{NM} + NM + N^2)$, because for each of the $NM$ elements in the matrix specifying the cost of assigning each batch element to each cluster, it needs to extract the max and then update the costs with the cluster counts. In comparison, the complexity for SK is $C_{Enc} + \theta{nNM}$, where $n$ is the number of iterations performed in the Sinkhorn Knopp algorithm, CKM is $C_{Enc} + \theta(NM) + C_{Dec}$.

\subsection{Derivation of Greedy Approximation}
We want to maximize the conditional probability of the $N$th assignment in a batch, conditioned on the $N-1$ previous assignments:
\begin{gather}
    \argmax_{k_N=1,\dots,K} p(y_N=k|y_1 = k_1, \dots, y_{N-1}=k_{N-1}; Z) = \notag \\[8pt]
    \argmax_{k_N=1,\dots,K}\frac{p(y_1 = k_1, \dots, y_{N}=k_{N} | Z)}{p(y_1 = k_1, \dots, y_{N-1}=k_{N-1} | Z)} = \notag \\[15pt]
    \argmax_{k_N=1,\dots,K}\log p(y_1 = k_1, \dots, y_{N}=k_{N}| Z) - \notag \\
        - \log p(y_1 = k_1, \dots, y_{N-1}=k_{N-1} | Z) = \notag \\[12pt]
    \argmax_{k_N=1,\dots,K}-\sum_{i=1}^N (d(z_i,\mu_{k_i},\Sigma_{k_i}) + \log{p(k_i)}) + \sum_{k=1}^K\log( n_k'!) + \notag \\
        +\sum_{i=1}^{N-1} (d(z_i,\mu_{k_i},\Sigma_{k_i}) + \log{p(k_i)}) - \sum_{k=1}^K\log( n_k!) = \notag \\[8pt]
    \argmin_{k_N=1,\dots,K} \sum_{i=1}^N (d(z_i,\mu_{k_i},\Sigma_{k_i}) - \log{p(k_i)}) - \sum_{i=1}^{N-1} (d(z_i,\mu_{k_i},\Sigma_{k_i}) - \log{p(k_i)}) + \notag \\
        + (\sum_{k=1}^K\log( n_k'!) - \sum_{k=1}^K\log( n_k!) ) \notag = \\[8pt]
    \argmin_{k_N=1,\dots,K} d(z_N,\mu_{k_N},\Sigma_{k_N}) - \log{p(k_N)} + (\sum_{k=1}^K\log(n_k!) - \sum_{k=1}^K\log( n_k!')) \label{eq:Nth-point-assignment-unsimplified}\,,
\end{gather}
where $n_k$ is the number of points assigned to cluster $k$ before the $N$th assignment, and $n_k'$ is the number assigned to the $k$th cluster after all assignments have been made. This means that
\[
n_{k}' = 
\begin{cases}
    n_{k}+1  & k = k_N \\
    n_{k}  & \text{otherwise}\,. 
\end{cases}
\]
Thus, \eqref{eq:Nth-point-assignment-unsimplified} becomes 
\begin{gather}
    \argmin_{k_N=1,\dots,K} d(z_N,\mu_{k_N},\Sigma_{k_N}) - \log{p(k_N)}+ \log( n_{k_N}+1!) - \log( n_{k_N}!) = \notag \\
    \argmin_{k_N=1,\dots,K} d(z_N,\mu_{k_N},\Sigma_{k_N}) - \log{p(k_N)}+ \log( n_{k_N}+1)\,. %\label{eq:Nth-point-assignment}
\end{gather}

\subsection{Proof of Equivalence to Mutual Information Maximization}
We want to show that, in the case of a uniform prior, the greedy algorithm that iteratively solves \eqref{eq:Nth-point-assignment} can be interpreted as (a close approximation to) iteratively making whatever assignment will maximize the mutual information between the batch index $i$ and the cluster labels. First note that, because the proposed model makes hard assignments, the entropy of cluster labels given the batch index is automatically zero, and so recalling that 
\[
I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\,,
\]
we see that the mutual information of the batch index $i$ and the cluster labels equals the entropy of cluster labels. Below, we show that the proposed method, up to small approximation error, maximizes the  entropy of cluster labels and, hence, the mutual information of cluster labels and the batch indices in each batch.
%Specifically, we consider the objective of choosing the assignment that maximizes some combination of the likelihood and the marginal entropy. By the following argument, this can be seen to lead to essentially the same procedure as that described in Section \ref{sub-sec:greedy-strategy}.


\begin{lemma} \label{lemma:ent-diff}
Let $X \in \mathbb{R}^{N \times K}$ be the matrix of already-made assignments in the current batch, and let $H^{(k)}$ be the marginal entropy after the new hard assignment is made to cluster $k$. Then 
\begin{equation}
    H^{(k)} - H^{(k')} \approx  \frac{1}{N+1} \left( \log(x_{k'} + 1) - \log (x_{k'} + 1)\right)\,.
\end{equation}
\end{lemma}

\begin{proof}
Let $X \in \mathbb{R}^{N \times K}$ be the matrix of already-made assignments in the current batch after $N$ points have been assigned, so that $H$, the current marginal entropy of $X$, is given by:
\begin{gather*}
    H = - \sum_{j=1}^K \left(\frac{1}{N} \sum_{i=1}^N x_{ij}\right) \log\left(\frac{1}{N}\sum_{i=1}^Nx_{ij}\right)\,.
\end{gather*}
To simplify notation, let $x_j = \sum_{i=1}^N x_{ij}$. Then $H^{(k)}$, the marginal entropy after the new hard assignment is made to cluster $k$, is given by
\begin{align*}
    H^{(k)} =& - \frac{x_k + 1}{N+1} \log{\frac{x_k + 1}{N+1}} - \sum_{j=1, j\neq k}^K \frac{x_j}{N+1} \log \frac{x_j}{N+1} \\
    =& \frac{-1}{N+1} \left((x_k+1) \log (x_k + 1) + \sum_{j=1, j\neq k}^K x_j (\log x_j -
    \log(N+1))\right) \\
    =& \frac{-1}{N+1} \left((x_k+1) \log (x_k + 1) + \sum_{j=1, j\neq k}^K x_j \log x_j -\sum_{j=1, j\neq k}^K x_j\log(N+1)\right) \\
    =& \frac{-1}{N+1} \left((x_k+1) \log (x_k + 1) + \sum_{j=1, j\neq k}^K x_j \log x_j -
    N\log(N+1)\right) \\
    =& \frac{-1}{N+1} \left((x_k+1) \log (x_k + 1) + \sum_{j=1, j\neq k}^K x_j \log x_j\right) + \frac{N}{N+1}\log(N+1)  \\
\end{align*}
Now, consider the difference $ H^{(k)} - H^{(k')}$ between the entropy after making assignment $k$ vs.\ after making a different assignment $k'$.

\begin{align*}
    =& \frac{-1}{N+1} \left((x_k+1) \log (x_k + 1) + \sum_{j=1, j\neq k}^K x_j \log x_j \right)- \\
    &\frac{-1}{N+1} \left((x_{k'}+1) \log (x_{k'} + 1) + \sum_{j=1, j\neq k'}^K x_j \log x_j \right)  = \\
    =& \frac{-1}{N+1} \left( (x_k+1) \log (x_k + 1) - (x_{k'} + 1)\log (x_{k'} + 1) \right)+ \\
    &\frac{-1}{N+1} \left( \sum_{j=1, j\neq k}^K x_j \log x_j - \sum_{j=1, j\neq k'}^K x_j \log x_j \right)  = \\
    =& \frac{-1}{N+1} \big( (x_k+1) \log (x_k + 1) - (x_{k'} + 1)\log (x_{k'} + 1) \big)+ 
    \left( x_{k'} \log x_{k'} -  x_k \log x_k \right)  = \\
    =& \frac{-1}{N+1} \left(((x_k + 1)\log(x_k + 1) - x_k \log x_k) - ((x_{k'} + 1)\log(x_{k'} + 1) - x_{k'} \log x_{k'})\right) \\
    \approx& \frac{-1}{N+1} \left( (\log(x_k + 1) + \frac{x_k}{x_k + 1}) - (\log (x_{k'} + 1)  + \frac{x_{k'}}{x_{k'} + 1})\right) \\
    =& \frac{-1}{N+1} \left( \log(x_{k} + 1) - \log (x_{k'} + 1) - \frac{x_{k} - x_{k'}}{(x_k + 1)(x_{k'} + 1)}\right) \\
    =& \frac{1}{N+1} \left( \log(x_{k'} + 1) - \log (x_{k} + 1) - \frac{x_{k'} - x_{k}}{(x_k + 1)(x_{k'} + 1)}\right) \\
    =& \frac{1}{N+1} \left( \log(x_{k'} + 1) - \log (x_{k} + 1)\right) - \frac{1}{N+1}\left(\frac{x_{k'} - x_{k}}{(x_k + 1)(x_{k'} + 1)}\right) \\
\end{align*}
where the fourth last line uses the fact that $\log n \approx H_n$ to make the substitution
\[
x_k\log x_k \approx x_k (\log (x_k + 1) - \frac{x_k}{x_k + 1}) \,,
\]
and similarly for $x_{k'}$. Note that the term $\frac{1}{N+1}\frac{x_k - x_{k'}}{(x_k + 1)(x_{k'} + 1)}$ is $0$ in expectation and has absolute value $\leq \frac{N}{(N+1)^2}$. If we drop this small error term, then we get
\begin{equation} %\label{eq:entropy-assignment-objective}
    H^{(k)} - H^{(k')} \approx  \frac{1}{N+1} \left( \log(x_{k'} + 1) - \log (x_{k} + 1)\right)\,,
\end{equation}
as desired.
\end{proof}

\begin{lemma} \label{lemma:rework-ent-optimization-problem}
Assume that $N$ data points in a batch have already been assigned. Let $\mathcal{L}(k)$ be the posterior probability of the batch of data, under the a $K$-component Gaussian mixture model with isotropic variance $\sigma^2$ (as described in Section \ref{subsec:comb-assignment}), after the $(N+1)$th data point is assigned to cluster $k$. Let $H^k$ be, as above, the entropy of cluster sizes after the $(N+1)$th data point has been assigned to cluster $k$. Then maximizing the objective $\log{\mathcal{L}(k)} + \lambda H^k$ with respect to the $(N+1)$th cluster assignment gives the following optimization problem
\[
\argmin_{k \in \{1,\dots,K\}} d(z_i,\mu_i,\Sigma_i) + \frac{\lambda}{N+1} \log (x_{k} + 1)
\]
\end{lemma}

\begin{proof}
Maximizing $\log{\mathcal{L}(k)} + \lambda H^k$  with respect to the $(N+1)$th cluster assignment means we prefer to assign to cluster $k$ over cluster $k'$ if and only if
\begin{gather}
    \log{\mathcal{L}(k)} + \lambda H^k > \log{\mathcal{L}({k'})} + \lambda H^{k'} \iff \\
    \log{\mathcal{L}(k)} - \log{\mathcal{L}({k'})} > \lambda H^{k'} - \lambda H^{k} \iff \\
    -\log{\mathcal{L}(k)} - (-\log{\mathcal{L}({k'}))} < \lambda H^k - \lambda H^{k'} \iff \\
    \log{\mathcal{L}(k)} - \log{\mathcal{L}({k'})} <  \frac{\lambda}{N+1} \left( \log(x_{k'} + 1) - \log (x_{k} + 1)\right) \,, \label{eq:difference-NLL-ent-objective1}
\end{gather}
where the last line uses lemma \ref{lemma:ent-diff}. Let $z$ be the encoding of the $(N+1)$th point, as in Section \ref{subsec:comb-assignment}. Then
\begin{align*}
-\log{\mathcal{L}(k)} =& -\log{ \left(\frac{\exp(-\tfrac{1}{2}(z_i- \mu_{k_i})\Sigma_k^{-1}(z-\mu_k))}{\sqrt{(2 \pi)^{d}|\Sigma_k|}}\right) } + C \\
        =& d(z,\mu_k,\Sigma_k) + C \,,
\end{align*}
where $C$ is the posterior probability of all previous $N$ assignments.
Subbing this into \eqref{eq:difference-NLL-ent-objective1}, we get
\begin{gather*}
      d(z,\mu_k,\Sigma_k)  -  d(z,\mu_k,\Sigma_k) < \\
      \frac{1}{N+1} \left( \log(x_{k'} + 1) - \log (x_{k} + 1)\right)
      \iff \\
      ( d(z,\mu_{k},\Sigma_{k}) -  d(z,\mu_{k'},\Sigma_{k'})) < \frac{\lambda}{N+1} \left( \log(x_{k'} + 1) - \log (x_{k} + 1)\right) \iff \\
       d(z,\mu_{k},\Sigma_{k}) + \frac{\lambda}{N+1}\log{(x_{k} + 1)} <  d(z,\mu_{k},\Sigma_{k}) + \frac{\lambda}{N+1}\log{(x_{k} + 1)} \iff \\
       d(z,\mu_{k},\Sigma_{k}) + \frac{2\lambda}{N+1}\log{(x_{k} + 1)} <  d(z,\mu_{k'},\Sigma_{k'}) + \frac{2\lambda}{N+1}\log{(x_{k'} + 1)}\,. \label{eq:difference-NLL-ent-objective2}
\end{gather*}
Choosing pairwise between all $k,k'$ as per \eqref{eq:difference-NLL-ent-objective2} is equivalent to choosing $k$ so as to minimize 
\[
 d(z,\mu_{k},\Sigma_{k}) + \frac{2\lambda}{N+1}\log{(x_{k} + 1)} \,.
\]
\end{proof}

\begin{remark}
This shows that our proposed method closely approximates a maximization of the entropy of cluster labels. There is some similarity to those methods, discussed in Section \ref{sec:related-work}, that use an additional loss term to encourage greater entropy of soft assignments in each batch, but the important difference here is that we are maximizing the entropy of hard assignments. 
\end{remark}

\begin{theorem}
Assume that $N$ data points in a batch have already been assigned. Let $\mathcal{L}(k)$ be the batch likelihood, under the a $K$-component Gaussian mixture model, as described in Section \ref{subsec:comb-assignment}, after the $(N+1)$th data point is assigned to cluster $k$. Let $C$ and $B$ be, respectively, random variables indicating the cluster assignments in the batch and the batch indices. Then, the method presented in Section \ref{subsec:greedy-strategy} is equivalent, up to a small error term, to maximizing
\[
\log{\mathcal{L}(k)} + \lambda I(C;B)\,,
\]
for some $\lambda \in \mathbb{R}$ that does not depend on the cluster assignments in the batch.
\end{theorem}
\begin{proof}
By Lemma \ref{lemma:ent-diff}, the method in \eqref{eq:sum-objective} is equivalent to maximizing 
\begin{equation}
    \log{\mathcal{L}(k)} + \lambda H^k \,, \label{eq:equiv-to-NLL-ent-maximization}
\end{equation}
for $\lambda=\frac{1}{N+1}$. The mutual information $I(C;B)$ can be expressed in terms of entropy as 
\[
I(C;B) = H(C) - H(C|B)\,.
\]
Moreoever, we are making hard assignments so, given the cluster index, the distribution over cluster labels has all the probability on one cluster and has zero entropy. This means 
\[
I(C;B) = H(C) - H(C|B) = H(C) - 0 = H(C) \,.
\]
Subbing this into \eqref{eq:equiv-to-NLL-ent-maximization}, the result follows.
\end{proof}

\section{Full Imbalanced Results}

\FloatBarrier

\begin{table}[H]
\caption{Full imbalanced results, mean from 5 runs}
\resizebox{\textwidth}{!}{
\begin{tabular}{lllllllllllllllll}
\toprule
 &  & \multicolumn{3}{r}{ours} & \multicolumn{3}{r}{sk} & \multicolumn{3}{r}{ent} & \multicolumn{3}{r}{ss} & \multicolumn{3}{r}{ckm} \\
 &  & imb1 & imb2 & imb3 & imb1 & imb2 & imb3 & imb1 & imb2 & imb3 & imb1 & imb2 & imb3 & imb1 & imb2 & imb3 \\
\midrule
\multirow[t]{4}{*}{C10} & acc & 0.21 & 0.21 & 0.23 & 0.13 & 0.14 & 0.16 & 0.11 & 0.13 & 0.19 & 0.18 & 0.19 & 0.23 & 0.11 & 0.13 & 0.18 \\
 & nmi & 0.05 & 0.05 & 0.06 & 0.00 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 & 0.05 & 0.04 & 0.04 & 0.00 & 0.00 & 0.00 \\
 & ari & 0.00 & 0.49 & 0.38 & 0.30 & 0.02 & 0.01 & 0.03 & 0.00 & 0.00 & 0.00 & 0.09 & 0.12 & 0.14 & 0.00 & 0.00 \\
 & KL & 0.06 & 0.07 & 0.24 & 0.36 & 0.38 & 0.76 & 2.19 & 2.27 & 1.85 & 1.44 & 1.30 & 2.12 & 2.33 & 2.32 & 2.91 \\
\cline{1-17}
\multirow[t]{4}{*}{C100} & acc & 0.06 & 0.06 & 0.07 & 0.03 & 0.03 & 0.04 & 0.01 & 0.01 & 0.02 & 0.03 & 0.04 & 0.04 & 0.01 & 0.01 & 0.02 \\
 & nmi & 0.02 & 0.01 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
 & ari & 0.05 & 0.05 & 0.06 & 0.00 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 & 0.05 & 0.04 & 0.04 & 0.00 & 0.00 & 0.00 \\
 & KL & 0.77 & 1.26 & 1.74 & 1.66 & 1.69 & 1.70 & 4.61 & 4.65 & 4.93 & 2.70 & 2.41 & 2.57 & 4.57 & 4.65 & 5.40 \\
\cline{1-17}
\multirow[t]{4}{*}{FM} & acc & 0.62 & 0.51 & 0.49 & 0.16 & 0.17 & 0.20 & 0.11 & 0.14 & 0.18 & 0.24 & 0.27 & 0.32 & 0.12 & 0.13 & 0.18 \\
 & nmi & 0.44 & 0.36 & 0.35 & 0.02 & 0.02 & 0.02 & 0.00 & 0.01 & 0.00 & 0.12 & 0.14 & 0.13 & 0.01 & 0.00 & 0.00 \\
 & ari & 0.02 & 0.01 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
 & KL & 0.01 & 0.11 & 0.19 & 0.35 & 0.34 & 0.56 & 2.19 & 2.11 & 2.41 & 1.25 & 1.11 & 0.74 & 2.24 & 2.25 & 2.70 \\
\cline{1-17}
\multirow[t]{4}{*}{STL} & acc & 0.24 & 0.22 & 0.24 & 0.14 & 0.16 & 0.19 & 0.12 & 0.15 & 0.20 & 0.13 & 0.16 & 0.19 & 0.11 & 0.13 & 0.19 \\
 & nmi & 0.07 & 0.05 & 0.05 & 0.01 & 0.01 & 0.02 & 0.00 & 0.00 & 0.01 & 0.01 & 0.01 & 0.01 & 0.00 & 0.00 & 0.00 \\
 & ari & 0.44 & 0.36 & 0.35 & 0.02 & 0.02 & 0.02 & 0.00 & 0.01 & 0.00 & 0.12 & 0.14 & 0.13 & 0.01 & 0.00 & 0.00 \\
 & KL & 0.04 & 0.16 & 0.33 & 0.47 & 0.29 & 0.64 & 1.74 & 1.72 & 1.54 & 1.91 & 1.90 & 2.14 & 2.29 & 2.42 & 2.01 \\
\cline{1-17}
\multirow[t]{4}{*}{RD} & acc & 0.57 & 0.47 & 0.39 & 0.11 & 0.11 & 0.13 & 0.08 & 0.11 & 0.14 & 0.20 & 0.22 & 0.26 & 0.08 & 0.10 & 0.17 \\
 & nmi & 0.49 & 0.38 & 0.30 & 0.02 & 0.01 & 0.03 & 0.00 & 0.00 & 0.00 & 0.09 & 0.12 & 0.14 & 0.00 & 0.00 & 0.02 \\
 & ari & 0.07 & 0.05 & 0.05 & 0.01 & 0.01 & 0.02 & 0.00 & 0.00 & 0.01 & 0.01 & 0.01 & 0.01 & 0.00 & 0.00 & 0.00 \\
 & KL & 0.01 & 0.04 & 0.27 & 0.62 & 0.86 & 0.67 & 3.47 & 3.25 & 3.85 & 2.09 & 1.84 & 2.62 & 3.48 & 3.65 & 3.47 \\
\cline{1-17}
\bottomrule
\end{tabular}
}
\end{table}


\FloatBarrier

\section{Modified Variance Maximization}
As discussed in Section \ref{sec:experimental-eval}, one of the methods we compare to is that proposed by \cite{zhong2020deep}, which minimizes the sum of squares of the marginal soft assignments across a batch. The expectation of the square (and hence the sum of squares) can be decomposed as the square of the expectation plus the variance. Minimizing the sum of squares can then help to combat partition collapse as it involves minimizing the variance. However, empirically we find this method not to perform well, see Table \ref{tab:main-results}. Here, we show that a simply modified version of this method performs better than the original, though still less well than our method. Results are presented in Table \ref{tab:var-improved-comparison}.

The modification is to just minimize variance directly, rather than via sum of squares. Note that this may be equivalent in some formulations, if the probability of membership across clusters for a single data point is normalized to sum to $1$. Then the expectation of the sum of memberships is $1$, because it is $1$ deterministically, so the square of the expectation is also $1$ deterministically and, in particular, is independent of the cluster assignments. This means that minimizing the sum of squares with respect to cluster assignments, is identical to minimizing variance with respect to cluster assignments. In our model this is true. The probability of membership depends only on the distance to the cluster centroids, and is conditionally independent across clusters, given the cluster centroids. Details are not given in \cite{zhong2020deep} as to whether this holds in their method.

\begin{table}
\centering
\begin{tabular}{*{5}{c}}
\hline
 & & CA & Var & VarM \\\hline
	\multirow{4}{*}{Cifar10}  & Acc & \textbf{22.7 (2.07)} & 11.8 (1.72) & 20.8 (0.67) \\
%\cline{2-5}
	 & NMI & 10.1 (1.68) & 1.1 (1.15) & \textbf{11.2 (0.65)} \\
%\cline{2-5}
	 & ARI & 5.8 (0.93) & 0.3 (0.38) & \textbf{7.3 (0.50)} \\
%\cline{2-5}
	 & KL$^*$ & \textbf{0.04 (0.01)} & 1.32 (0.36) & 2.00 (0.09) \\
\hline
	\multirow{4}{*}{Cifar100}  & Acc & \textbf{6.4 (0.22)} & 1.2 (0.22) & 1.0 (0.00) \\
%\cline{2-5}
	 & NMI & \textbf{13.2 (0.37)} & 0.6 (1.05) & 0.0 (0.00) \\
%\cline{2-5}
	 & ARI & \textbf{1.7 (0.14)} & 0.0 (0.04) & 0.0 (0.00) \\
%\cline{2-5}
    & KL$^*$ & \textbf{0.81 (0.07)} & 3.76 (0.37) & 6.64 (0.00) \\
%\cline{2-5}
\hline
	\multirow{4}{*}{FashionMNIST}  & Acc & \textbf{54.5 (6.96)} & 10.0 (0.04) & 37.4 (2.53) \\
%\cline{2-5}
	 & NMI & \textbf{53.2 (4.23)} & 0.0 (0.04) & 42.8 (2.08) \\
%\cline{2-5}
	 & ARI & \textbf{39.1 (6.29)} & 0.0 (0.00) & 27.1 (1.78) \\
%\cline{2-5}
    & KL$^*$ &  \textbf{0.04 (0.00)} & 1.34 (0.57) & 1.70 (0.07) \\
%\cline{2-5}
\hline
	\multirow{4}{*}{STL}  & Acc & \textbf{23.5 (1.42)} & 10.1 (0.20) & 22.6 (1.52) \\
%\cline{2-5}
	 & NMI & \textbf{13.7 (1.33)} & 0.0 (0.08) & 11.4 (1.70) \\
%\cline{2-5}
	 & ARI & 7.1 (0.70) & 0.0 (0.00) & \textbf{7.1 (1.16)} \\
%\cline{2-5}
    & KL$^*$ &  \textbf{0.09 (0.01)} & 2.23 (0.12) & 1.50 (0.13) \\
\hline
\end{tabular}
\caption{Comparison between the modified variance minimization method, denoted `VarM', the original variance minimization method from \cite{zhong2020deep}, denoted `Var', and our method, denoted `CA'.}
\label{tab:var-improved-comparison}
\end{table}

\section{Calcuation of Entropies of Matrices}
Let $h,s: \mathbb{R}^{4 \times 3} \rightarrow \mathbb{R}^3$ be the functions that compute the marginal hard and soft cluster distributions for a given matrix of batch assignment probabilities. Then, for matrices
\[
D_1 = \begin{bmatrix}
.98 & .01 & .01 \\
.98 & .01 & .01 \\
.49 & .50 & .01 \\
.49 & .01 & .50 
\end{bmatrix}
D_2 = \begin{bmatrix}
.34 & .33 & .33 \\
.34 & .33 & .33 \\
.34 & .33 & .33 \\
.34 & .33 & .33 
\end{bmatrix}  \,,
\]
we have
\begin{align*}
    &s(D_1) = \begin{bmatrix}
.74, .13, .13 
\end{bmatrix} &H(h(D_1)) = 1.10 \\
    &h(D_1) = \begin{bmatrix}
.5,.25,.25 
\end{bmatrix} &H(s(D_1)) = 1.50 \\
     &s(D_2) =\begin{bmatrix}
.34.,.33,.33 
\end{bmatrix} &H(s(D_2)) = 1.58 \\
    &h(D_2) = \begin{bmatrix}
1,0,0 
\end{bmatrix} &H(h(D_2)) = 0 \,.
\end{align*}

\section{Explicit Distributions in Imbalanced Experiments}
For the three imbalanced settings, imb. 1, imb. and imb. 3, the relative frequences of clusters are as follows:
imb 1.: $1.0,1.0,1.0,1.0,1.0,1.0,0.95,0.9,0.85,0.8$. \\
imb 2.: $1.0,0.95,0.9,0.85,0.8,0.75,0.7,0.65,0.6,0.55$. \\
imb 3.: $1.0,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1$.
