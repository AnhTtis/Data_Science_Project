

\section{Notation and Setup}
\label{sec:method}


% Let $A: \bm{X}, f, g \longrightarrow \bm{\tilde{X}}$ denote a function that performs \autoref{alg:opt}.

% advarserial attack: fix $f$, $g$ is gradient-based optimization algorithm
% image steganography: special case, millions of attacking target, $f$ can be learned; example: FNNS

% steganoGAN: approximate $A$ with a network and jointly optimize $f$ and $A$
% However ...0000

% \begin{wrapfigure}{r}{0.5\textwidth}
%     \includegraphics[width=\linewidth]{imgs/iso3.pdf}
%     \caption{Pipeline for image steganography.}
%     \label{fig:pipeline}
% \end{wrapfigure}
%\subsection{Task Description}
Let $\bm{X} \in [0,1]^{H\times W\times 3}$ denote an RGB cover image with height $H$ and width $W$.\footnote{In practice, we use PNG-24/JPEG with quantized pixel values.}
The hidden message $\bm{M}$
is a bit string of length $m=H\times W\times B$ that is reshaped to match the cover image's size, i.e. $\bm{M}\in \{0, 1\}^{H \times W \times B}$, where payload $B$ is the number of encoded bits per pixel (bpp).\footnote{If $m$ is not a multiple of $H \times W$, it is padded with zeros.} 
Image steganography aims to conceal $\bm{M}$ in a steganographic image $\bm{\tilde{X}} \in [0,1]^{H\times W\times 3}$ that looks visually identical to $\bm{X}$, such that $\bm{M}$ is transmitted undetected through $\bm{\tilde{X}}$. 
A steganographic encoder \textit{Enc} takes as input $\bm{X}$ and $\bm{M}$ and outputs $\bm{\tilde{X}}$. 
A decoder \textit{Dec} recovers the message, $\bm{M}'={\it{Dec}}(\bm{\tilde{X}})$, with minimal recovery error $\epsilon=\frac{\|\bm{M}'-\bm{M}\|_0}{m}$ (ideally $\epsilon=0$).  
% This process is depicted in \autoref{fig:opt} . 
% In stegoanalysis, $\bm{X}$ is called cover image and $\bm{\tilde{X}}$ is called stego image.
% See \autoref{fig:pipeline} for a schematic illustration. 
Below we describe two recent paradigms for steganography (~\autoref{suppl-sec:steg_comp} summarizes the similarities and differences between the paradigms).


\textbf{Learned Image Steganography.}
\label{sec:encoder_decoder}
Recent deep-learning-based image steganography methods~\citep{zhang2019steganogan,zhu2018hidden,tan2021channel} use straight-forward convolutional neural network architectures for encoders and decoders that maintain the same feature dimensions, $H\!\times\! W$, throughout (i.e. no downsampling). 
SteganoGAN~\citep{zhang2019steganogan} and CHAT-GAN~\citep{tan2021channel} use such an encoder network to generate the steganographic image $\tilde{\bm{X}}$ with a single forward pass and a decoder network to predict the message $\bm{M}'$. The final layer of the decoder usually consists of $H \!\times\! W \!\times\! B$ sigmoidal outputs, producing real values within $[0,1]$ that turn into the binary recovered message $\bm{M}'$  after rounding. These methods also have a $Critic/Discriminator$ to predict whether an image looks natural. The encoder, decoder, and critic are all trained end-to-end. 

\textbf{Optimization-based Image Steganography.}
Given a differentiable decoder with random or learned weights (from a method mentioned in the last paragraph), \citet{kishore2021fixed} express the steganography encoding step as a per sample optimization problem that is inspired by adversarial perturbations~\citep{szegedy2013intriguing}. Their method, Fixed Neural Network Steganography (FNNS), finds a steganographic image by solving the following constrained optimization problem while ensuring that the perturbed image lies within the $[0,1]^{H \times W \times 3}$ hypercube:
\begin{align}
    \min_{\bm{\tilde{X}\in [0,1]^{H\times W\times 3}}} \!\!\!\!L_{\text{acc}}&\left(\textit{Dec}\left(\bm{\tilde{X}}\right), \bm{M}\right) \!\!+\!\! \lambda L_{\text{qua}}\left(\bm{\tilde{X}}, \bm{X}\right)\nonumber \\ 
    L_{\text{acc}}\left(\bm{M}', \bm{M}\right) :=& {\langle \bm{M}, \log{\bm{M}'}\rangle + \langle  \left(1-\bm{M}\right),\log\left(1-\bm{M}'\right)\rangle} % ^{\text{binary cross-entropy loss}} 
    \nonumber \\
    L_{\text{qua}}\left(\bm{\tilde{X}}, \bm{X}\right) :=& {\frac{1}{N}\|\bm{\tilde{X}} - \bm{X}\|_2^2},
    % _{\text{mean-squared error}},
    \label{eq:eq1}
    \end{align} 
where $\langle\cdot\rangle$ denotes the dot product operation, $\lambda$ is a weight factor and $N=H \times W \times 3$. 
Note that only the perturbed image $\tilde{\bm{X}}$ is being optimized and the decoder $\textit{Dec}$ is kept fixed throughout. 
The accuracy loss $L_{\text{acc}}\left(\bm{M}', \bm{M} \right)$ is a binary cross entropy loss that encourages the message recovery error to be low, and the quality loss $L_{\text{qua}}(\bm{X}, \bm{\tilde{X}})$ uses mean squared error to ensure that the steganographic image looks similar to the cover image.
We denote the objective in \autoref{eq:eq1} as $\ell(\bm{\tilde{X}}, \bm{M})$. 
Several solvers can be used to solve \autoref{eq:eq1} as shown in Algorithm \ref{alg:opt}, but the most popular choices are iterative, gradient-based hill-climbing algorithms. 
In the algorithm, $\eta>0$ denotes the step size, and $g(\cdot)$ is an update function defined by the specific optimization method. The perturbation $\bm{\delta}$ is iteratively optimized to minimize the loss $\ell$ within the image pixel constraints.
% The function $g(\cdot)$ can simply be the negative gradient of $\ell$ (clipped if an update exceeds the pixel boundary $[0,1]$), or an approximate second order update as in L-BFGS. Note that in the latter, $g(\cdot)$ maintains an internal (hidden) state to approximate the Hessian of the loss. 
% FNNS~\citep{kishore2021fixed} uses L-BFGS to optimize $\ell$, while contraining the perturbed image within the $[0,1]^{H \times W \times 3}$ hyper-cube. 

\textbf{Drawbacks of Existing Methods} 
% Learned methods that take single forward passes are superior in speed, but they yield high error rates and can hardly achieve 0\% recovery error. 
Existing learned methods are fast because single forward passes with the trained encoder and decoder are all that are needed for encoding and decoding, but they yield high error rates of up to 5-25\% for 4 bpp. 
% Ultimately, it is hard to train a network to hide large messages with no error in a single forward pass.
It is also hard to incorporate additional constraints to these models without retraining. 
On the other hand, optimization-based approaches can result in lower error rates, but they are much slower (requiring thousands of iterations). 
Furthermore, their resulting recovery error depends heavily on the weights of the decoder and the initialization of $\bm{\delta}$~\citep{kishore2021fixed}. Especially with $B\geq 4$, the optimization can get stuck in local minima.
% before reaching zero error. 

% In each iteration, we find a small perturbation $\Delta \bm{X}$ that is added to the image in order to decrease the loss $\ell\left(\bm{\tilde{X}}, \bm{M}\right)$. 

% As is described in \autoref{eq:eq1}, there are two optimization objectives that are jointly optimized:
% $L_{\text{acc}}$, the cross entropy loss between the hidden message $\bm{M}'$ and the decoded message$\bm{M}$, is minimized to enforce that the decoded message $\round{Dec\left(\bm{\tilde{X}}\right)}$\footnote{$\round{\cdot}$ denotes the rounding function.} matches the encoded message $\bm{M}$;
% $L_{qua}$, the image quality loss, contains the mean-squared error penalty that constrains the steganographic image from changing too much and the critic loss that ensures that the steganographic image is realistic. 

% constraints on the perturbation and $l$ is the loss obtained when the message is retrieved from $\bm{\tilde{X}} = \bm{X} + \tilde{\bm{X}}$. Usual constraints are an upper bound on how big the perturbation can be and checks to ensure that after adding the perturbation all image pixels are still within some domain. 

% The optimization in \autoref{eq:eq1} can be directly solved using gradient descent \citep{anonymous2022fixed}. However, gradient descent is slow and not necessarily the most optimal descent direction (given that the problem is non-convex). Even second-order optimization algorithms such as L-BFGS~\citep{liu1989limited} take 30-90 seconds to converge on a single image, while CNN-based encoders (e.g. \citep{zhang2019steganogan}, \citep{baluja2017hiding}) can process a batch of images within a second. 

\section{Learned Steganographic Optimization}
\label{sec:\method{}}

% \vspace{-0.2\baselineskip}
%TODO read this again
Ideally, we would like a method that is both \textbf{fast} and has \textbf{low recovery error}, so we combine ideas from learned methods and iterative optimization methods to propose \emph{\methodlong{} (\method{})}.
\method{} consists of an encoder, a decoder, and a critic network, but the encoder is iterative and it approximates the function $g( \cdot)$ in Algorithm \ref{alg:opt}. 
% This new encoder is iterative and is designed to mimic the steps of a gradient-based optimization algorithm. 
% Similar to SteganoGAN, \method{} consists of a decoder, a critic and an encoder. 
% However, the \method{}'encoder is designed  to be iterative, such that it approximates the function $g( \cdot)$ in Algorithm \ref{alg:opt}. 
The optimization procedure can be viewed as a recurrent neural network with a hidden state that iteratively optimizes the perturbation $\bm{\delta}$ with respect to the loss $\ell$. 
At each iteration, it obtains the previous estimate $\bm{\delta}_{t-1}$ and the gradient of $\ell$ as input and produces a new $\bm{\delta}_{t}$. 
The hidden state allows \method{} to learn what optimization steps are best suited for this task. 
% The hidden state allows \method{} to gain ``momentum'' from past iterations. 
After multiple iterative steps, the encoder outputs the steganographic image. The decoder is a simple feed-forward convolutional neural network trained to retrieve the message from the steganographic image. The critic is a neural classifier (akin to GAN discriminators) trained to discriminate between cover images and steganographic images~\citep{hayes2017generating}. As everything is differentiable, the iterative encoder, the decoder, and the critic can be jointly learned on a dataset of images.
% \setcounter{algorithm}{1}

\begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-3\baselineskip}
    \begin{minipage}{0.5\textwidth}
      \begin{algorithm}[H]
        \caption{Iterative Optimization}
        \label{alg:opt}
        \begin{algorithmic}
          \STATE $\bm{\delta}_0 \leftarrow \bm{0}$\;
          \FOR{t=1 to T}
            \STATE $\bm{\delta}_{t} \leftarrow \bm{\delta}_{t-1} + \newline ~~~~~~~~~~\eta \cdot  g\left(\mathop{\nabla}\limits_{\mathrlap{\bm{\delta}_{t-1}}} \;\;\ell\left(\bm{X}+\bm{\delta}_{t-1}, \bm{M}\right),\bm{X},\bm{\delta}_{t-1}\right)$\;
         \ENDFOR
        \STATE $\bm{\tilde{X}}\leftarrow \bm{X}+\bm{\delta}_T$\;
        % \STATE \textbf{return} $\bm{\tilde{X}}$\;
        \end{algorithmic}
      \end{algorithm}
    \end{minipage}
    
    \vspace{\baselineskip}
    \centering
    \includegraphics[width=\linewidth]{imgs/iso2_new.pdf}
    \caption{Model architecture of \method{}'s iterative optimization network that functions as optimizer $g$ in Algorithm \ref{alg:opt}.}
    %The operations in GRU are explained in \autoref{eq:gru1}. 
    \label{fig:updater}
    \vspace{-0.5\baselineskip}
  \end{wrapfigure}

%where we  with a learned fully convolutional neural network with a hidden recurrent state. 
%The architecture of the optimization network used in \method{} is shown in \autoref{fig:updater}. 
%, where  where  $g()$ is the recurrent block which takes
%obtains gradient information as input in every iteration. 
%which uses an iterative encoder network that is designed to mimic the steps of an optimization algorithm that optimizes \autoref{eq:eq1}. Inspired by RAFT~\citep{teed2020raft}, we created the iterative \method{} encoder with a recurrent gated recurrent unit (GRU). 
%\paragraph{LISO} is a novel encoder network that performs iterative optimization on the perturbation $\bm{\tilde{X}}$.
% \begin{wrapfigure}{r}{0.5\textwidth}
% \end{wrapfigure}
\textbf{\method{} architecture.}
The \method{} decoder and critic have 3 convolutional blocks, each containing a convolutional layer (with $3\!\times\!3$ kernels and stride 1), batch-norm, and leaky-ReLU, similar to~\citet{zhang2019steganogan}.
In the decoder, the convolutional blocks are followed by another convolutional layer to obtain a $H \times W \times B$ dimensional output. In the critic, the convolutional blocks are followed by an adaptive mean pooling layer to obtain a scalar output that denotes a prediction of whether an image has a hidden code. 

The core of \method{} is its encoder, which has a recurrent GRU-based update operator that functions like Algorithm \ref{alg:opt}. The function $g(\cdot)$ in Algorithm \ref{alg:opt} is approximated using a fully convolutional network designed around a gated recurrent unit (GRU)~\citep{cho2014properties}. 
The GRU has an internal state and keeps track of information relevant to the optimization (more details about the GRU cell are in~\autoref{suppl-sec:gru}). 
To avoid information loss, the \method{} networks have no down-sampling/up-sampling layers, which means \method{} can process images of any size. At a high level, the \method{} encoder (illustrated in \autoref{fig:updater}) takes as input the cover image $\bm{X}$, the current perturbation $\bm{\delta}_{t-1}$ and the gradient of the loss with respect to the perturbed image $\nabla_{\bm{\delta}_{t-1}} l(\bm{X}+\bm{\delta}_{t-1}, \bm{M})$ and predicts the perturbation $\bm{\delta}_{t}$ for the next step. The GRU hidden state $\bm{h}_0$ is initialized with a feature extractor (not shown) that is a simple 3-layer fully convolutional network that extracts features from the input image. The input to the GRU in subsequent steps $x_t$ is constructed by concatenating the extracted features from current perturbation $\bm{\delta}_{t-1}$, the gradient $\nabla_{\bm{\delta}_{t-1}} l(\bm{X}+\bm{\delta}_{t-1}, \bm{M})$ and the cover image $\bm{X}$. 
The GRU unit updates its own hidden state $h_{t}$, and the hidden state is processed by additional convolutional layers to produce a gradient-type step update $\bm{g}_t$. 
The final update becomes $\bm{\delta}_{t}=\bm{\delta}_{t-1}+\eta \bm{g_{t}}$,
where $\eta$ is the step size for the new update. 
Perturbation $\bm{\delta}_{t}$ can also be clipped by a truncation factor to limit how much the $\bm{X}$ changes.
The steganographic image, that is the final output of the \method{} encoder, is produced through recurrent applications of the iterative encoder; the final steganographic image becomes 
$\tilde{\bm{X}}=\bm{X}+\eta\sum_{t}\bm{g}_{t}$.

\textbf{Training.} 
The entire \method{} pipeline (the iterative encoder, decoder, and critic) is trained end-to-end on a dataset of images. 
As with GAN training, we optimize the critic and the encoder-decoder networks in alternating steps. During the training, we compute loss for all intermediate updates with exponentially increasing weights ($\gamma^{T-t}$ at step $t$). 
With intermediate predictions denoted as $\bm{\tilde{\bm{X}}}_1, \dots, \bm{\tilde{\bm{X}}}_T$ the loss is:
\begin{align}
    L_{train} = \sum_{t=1}^{T} \gamma^{T-t} \big[L_{acc}\left(\bm{M}, \tilde{\bm{X}_t}\right)
     + \lambda L_{qua}\left(\bm{X}, \tilde{\bm{X}_t}\right)
     +\mu L_{crit}\left(\bm{X},\tilde{\bm{X}_t}\right)\big],
     \label{eq:loss}
\end{align}
%
where $\gamma\in(0,1)$ is a decay factor and $L_{crit}$ denotes the critic loss (with weight $\mu>0$). 
Despite the predictions being sub-optimal from earlier iterations, we still make use of the loss from those steps because they do provide useful update directions for later iterations. Note that this loss is similar to  \autoref{eq:eq1}, but there is an additional critic loss term that ensures that the steganographic image looks like a natural image~\citep{zhang2019steganogan}. As the qualitative and critic losses are computed at all steps, the optimizer is encouraged to stay on or near the manifold of natural images throughout. 

\input{table1}

\textbf{Inference.} The learned \method{} iterative encoder and decoder are used during inference. The iterative encoder performs a gradient-style optimization on the cover image to produce the steganographic image with the desired hidden message. Mathematical optimization algorithms like L-BFGS have strong theoretical convergence guarantees, which assure fast and highly accurate convergence within the vicinity of a global minimum, but for non-convex optimization problems they have a tendency to get stuck in local minima. Although our learned optimizer lacks theoretical convergence guarantees, \method{} is very efficient at finding local vicinity of good minima for natural images---it can reliably find solutions with only a few bits decoded incorrectly (on the order of 0.01\%) as shown in \autoref{tab:results}. FNNS~\citep{kishore2021fixed} works in a complementary manner to trained encoder-decoder networks and optimizes steganographic images using fixed networks and off-the-shelf optimization algorithms. Consequently, we can use \method{} in conjunction with FNNS; we refer to this procedure as \method{}+(PGD) or \method{}+(LBFGS), dependent on which optimization algorithm is used for FNNS optimization. Concretely, we encode a message into a cover image by using the \method{} encoder, and then use L-BFGS/PGD to further reduce the error rate. The error rate after \method{} is already quite low and hence only a few additional L-BFGS iterations are required to reduce the error. 
Compared to FNNS, \method{} can reliably achieve 0\% error for up to 3 bpp, with drastically fewer iterations. 

