\section{Experiments and Discussion}
\label{sec:experiments}
We evaluate our method on three public datasets: 
1) Div2k~\citep{agustsson2017ntire} which is a scenic images dataset, 
2) CelebA~\citep{liu2018large} which consists of facial images of celebrities, and
3) MS COCO~\citep{lin2014microsoft} which contains images of common household objects and scenes. 
For CelebA and MS COCO we use the first 1000 for validation and the following 1,000 for training. 
Note that 1,000 training images are sufficient for \method{} to achieve SOTA performance, and using full datasets will only make training longer. The messages are random binary bit strings sampled from an independent Bernoulli distribution with $p=0.5$, which is close to the distribution of compressed/encrypted messages.

During training, we set the number of encoder iterations $T=15$, the step size $\eta=1$, the decay $\gamma=0.8$ and loss weights $\lambda=\mu=1$. During inference, we use a smaller step size $\eta=0.1$ for a larger number of iterations $T$; we iterate until the error rate converges. We show the average number of iterations required to minimize the recovery error in \autoref{tab:num_k}. The average number is consistently below the number of  iterations used during training $T=15$, although some images require up to 25 iterations. 
These results suggest that in practice LISO just needs a few iterations to achieve low recovery error rates. \autoref{fig:optimizers} shows how the recovery error decreases over time during testing. The monotonicity of the graphs indicates that the optimization loss is well aligned with recovery error and that LISO is successful at learning to find suitable descent directions (more details are in the subsequent optimization analysis). The final steganographic images are stored in PNG format. 

\textbf{Image Steganography Performance.}
Most traditional steganography methods hide $<1$ bpp messages in images, and methods that hide large payloads of information usually hide structured information like images. Very few methods are designed for hiding \textbf{high payload arbitrary bit string messages} in color images. We compare \method{} with three other methods that are designed to do so, SteganoGAN~\citep{zhang2019steganogan}, FNNS~\citep{kishore2021fixed} and CHAT-GAN~\citep{tan2021channel}. Since the source code for CHAT-GAN was not available, we were only able to compare against the numbers provided in the paper on MS-COCO and these results are presented in~\autoref{appendix:chatgan}. In \autoref{tab:results}, we evaluate \method{}'s performance on image steganography in terms of both message recovery accuracy and image quality.
Following previous work~\citep{zhang2019steganogan}, we measure how much the steganographic image changes by computing the structural similarity index (SSIM) and peak signal-to-noise ratio (PSNR)~\citep{wang2004image} between the cover and steganographic images. In~\autoref{appendix:lpips}, we also evaluate with the Learned Perceptual Image Patch Similarity (LPIPS) metric~\citep{zhang2018unreasonable} and find trends similar to that of PSNR and SSIM. 

% \input{table1}
From \autoref{tab:results}, we observe that the error rates of \method{} are an order of magnitude lower when compared to SteganoGAN. Interestingly, 
%while SSIM scores are comparable 
the image quality as measured by PSNR scores is also superior. \method{} is able to achieve an error rate of \textbf{exactly 0\%} for payloads $\leq$ 3 bpp for all images when it is paired with a few steps of direct L-BFGS optimization (the numbers in \autoref{tab:results} are averaged and for many samples 0\% error is obtained just with \method{}).
Using a learned iterative method, we are able to achieve the same performance as that obtained from a direct optimization method (FNNS), but in far fewer optimization steps and time (see \autoref{tab:time}). Essentially LISO's ability to learn from past optimizations and to stay on the image manifold, allows it to achieve low error rates, high image quality, and initializations (for further L-BFGS optimization) near a good minimum. 


 

\autoref{fig:images} shows the steganographic images produced by \method{} under different payloads.
Qualitatively we observe that the image quality is quite good even when a large number of bits per pixel are hidden in the cover images. 
The CelebA example shows a rare example where the hue changes slightly while the image remains un-pixelated and natural; in contrast, the low-quality steganographic images produced by FNNS tend to be pixelated.
Different from previous image steganography methods that produce unnatural Gaussian noise, \method{}'s perturbation is smooth and natural because \method{} implicitly learns the manifold of natural images and produces steganographic images that stay close it.
These color shifts disappear when a larger image quality factor $\lambda$ or a more diverse dataset is used (a similar observation was made by~\citet{upchurch2017deep} for image transformations). 
We can also explicitly control the trade-off between error rate and image quality by changing the image quality factors $\lambda$ and $\mu$ on the loss terms if desired. Additional qualitative images obtained by varying these are provided in~\autoref{suppl-sec:imgs}.

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.90\linewidth]{imgs/images_horizontal.pdf}
% \caption{Random cover images with corresponding steganographic images with different payloads.}
% \label{fig:images}
% \end{figure}

% \begin{table*}[h]
%     \centering
% \resizebox{.97\textwidth}{!}{% 
% \begin{tabular}{c|cccc|cccc|cccc|cccc}
% \hline
% \multirow{3}{*}{ Method } & \multicolumn{4}{c|}{\multirow{2}{*}{Error Rate (\%) $\downarrow$}}  & \multicolumn{4}{c|}{\multirow{2}{*}{PSNR $\uparrow$}} & \multicolumn{8}{c}{Detection Rate (\%) $\downarrow$} \\
% \cline{10-17}
% & & & & & & & & & \multicolumn{4}{c|}{StegExpose} & \multicolumn{4}{c}{SiaSteg} \\
% \cline{2-17}
% & 1 bit & 2 bits & 3 bits & 4 bits & 1 bit & 2 bits & 3 bits & 4 bits & 1 bit & 2 bits & 3 bits & 4 bits & 1 bit & 2 bits & 3 bits & 4 bits\\
% \hline
% w/o Detection Loss & 2E-05 & 4E-04 & 2E-03 & 3E-02 & 31.99 & 33.29 & 30.16 & 27.45 & 23 & 20 & 26 & 30 & 93 & 95 & 97 & 99 \\
% w/ Detection Loss & 2E-03 & 3E-03 & 3E-03 & 3E-02 & 31.57 & 32.00 & 29.43 & 27.23 & 22 & 26 & 26 & 32 & 9 & 1 & 11 & 5 \\
% \hline
% \end{tabular}%
% }
% \caption{Results of using Stegexpose~\citep{boehm2014stegexpose} (a statistical steganalysis tool) and SiaStegNet~\citep{you2020siamese} (a steganlaysis method) to detect whether the images produces by LISO are steganographic images, evaluated on Div2k dataset. For reference, images generated by SteganoGAN are detected by SiaStegNet with 100\% accuracy.}
% \label{tab:neural_steganalysis}
% \end{table*}

\begin{wrapfigure}{r}{0.5\textwidth}
\vspace{-3ex}
    \includegraphics[width=\linewidth]{imgs/errs.pdf}
    \caption{Error-iteration curves for different optimization methods (at 4 bpp). The pretrained network is SteganoGAN.}
    \label{fig:optimizers}
\end{wrapfigure}
\textbf{Optimization with \method{}.}
In \autoref{fig:optimizers}, we compare optimizing with the \method{} encoder against popular numerical optimization algorithms including projected gradient descent~\citep{madry2017towards} and L-BFGS~\citep{liu1989limited} on the steganographic encoding task. Note that \emph{L-BFGS-Random} is equivalent to \emph{FNNS-R} and \emph{L-BFGS-Pretrained} is equivalent to \emph{FNNS-D}~\citep{kishore2021fixed}. \method{} converges faster than any other optimization algorithm and this validates our hypothesis that \method{} can learn a descent direction that is better than the one found by common gradient-based optimization methods; \method{} converges to $100\times$ lower loss and $80 \times$ lower error rate. This is because the \method{} encoder can leverage information learned from optimizing many samples, while off-the-shelf gradient-based methods perform per-sample optimization. Furthermore, we show that recovery error rate from \method{} can further be reduced by taking only a few additional \emph{L-BFGS} steps. This suggests that the \method{} encoder output is quite 
close to the optimum and is a good initialization for using conventional optimization methods like \emph{L-BFGS} or \emph{PGD}. 

% \begin{figure}[t]
%     \centering
%     % \includegraphics[width=0.49\linewidth]{imgs/loss.pdf}
%     \includegraphics[width=0.8\linewidth]{imgs/errs.pdf}
%     \caption{Error-iteration curves for different optimization methods (at 4 bpp). The pretrained network is SteganoGAN.}
%     \label{fig:optimizers}
% \end{figure}


% \begin{figure*}[t]
%     \centerline{
%     \includegraphics[width=0.39\linewidth]{imgs/loss.pdf}
%     \includegraphics[width=0.39\linewidth]{imgs/errs.pdf}}
%     \caption{Loss/error-iteration curves of different optimization methods (for 4 bpp). The pretrained networks are training using StegaanoGAN~\citep{zhang2019steganogan}.}
%     \label{fig:optimizers}
% \end{figure*}

\textbf{Steganography with JPEG Compression.}
JPEG~\citep{wallace1992jpeg} is a commonly used lossy image compression method, which removes high-frequency components for reduced file size. 
JPEG-compression-resistant steganography is hard because steganography and JPEG compression have opposing objectives; steganography attempts to encode information through small imperceptible changes and JPEG compression attempts to eliminate texture details. General-purpose steganography methods have about 50\% recovery error rate when decoding JPEG-compressed images. 
Many methods have been specifically developed to perform JPEG-resistant steganography to hide $<0.5$ bpp messages~\citep{zhang2015jpeg,fan2011robust,zhang2016framework,holub2014universal}. 
Some of these find perturbations specifically in low-frequency space to avoid being removed by compression. 
HiDDeN~\citep{zhu2018hidden} and FNNS~\citep{kishore2021fixed} include differentiable JPEG layers to approximate compression as part of their network architecture. 
% Similarly, we include a differentiable JPEG layer in \method{}.
Following \citep{athalye2018obfuscated}, we adopt an approximate JPEG layer where the forward pass performs normal JPEG compression and the backward pass is an identity function. 
We call this adapted method \method{}-JPEG. \autoref{tab:jpg} compares \method{}-JPEG with existing methods.
% HiDDeN~\citep{zhu2018hidden}, current SOTA JPEG-resistant learned steganography method, is only able to hide 0.2 bpp and has 25\% error rate. 
% FNNS~\citep{kishore2021fixed} can hide 1 bpp but it has high error rate of 32\%.
\method{} is able to find perturbations that won't be lost in JPEG compression, while maintaining low error rate ($<$1E-01). The PSNR values for \method{}-JPEG are low but qualitative 1bpp steganographic image examples from \method{}-JPEG (see \autoref{fig:jpg}) show that there is only a slight pixelation.  
As seen in \autoref{tab:jpg}, the PSNR of FNNS-JPEG and \method{}-JPEG are similar but \method{}-JPEG has a significantly lower error rate. 
% Trade-off between image quality and error rate can be controlled with $L_{qua}$ 
Higher quality steganographic images can be obtained by training a \method{} model with a higher weight on the qualitative loss $L_{qua}$, but this will cause a slight increase in the error rate. 
\begin{figure*}[t]
\centering
\includegraphics[width=0.8\linewidth]{imgs/images.pdf}
\caption{Random images with corresponding steganographic images under different payloads.}
\label{fig:images}
\vspace{-2ex}
\end{figure*}

\begin{table}[h]
\begin{minipage}{0.48\textwidth}
\caption{\method{}-JPEG results compared with baselines. JPEG quality 80 is used.}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c|c}
\hline
Method & bpp & \multicolumn{1}{c|}{Error (\%) $\downarrow$} & \multicolumn{1}{c}{PSNR $\uparrow$}  \\
\hline
% float32 & 1E-04 & 33.01 \\
\method{}-PNG & 1 & 2E-05 & 31.99 \\
FNNS-JPEG & 1 & 32.03 & 22.65 \\
HiDDeN & 0.2 & $\geq$10 & unknown \\
\method{}-JPEG & 1 & 6E-02 & 19.72 \\
\hline
\end{tabular}
}
\label{tab:jpg}
\end{minipage}
\hfill
\begin{minipage}[c]{0.48\textwidth}
\centering
\captionsetup{type=figure} %% tell latex to change to table
\includegraphics[width=\linewidth]{imgs/jpeg_images1.png}
\caption{\method{}-JPEG steganographic images.}
\label{fig:jpg}
\end{minipage}
\end{table}


% Each iteration, on average, takes 29 ms for a $512 \times 512$ RGB image on a Nvidia Titan RTX GPU.
\begin{table}[!t]
\begin{minipage}{0.46\textwidth}
\caption{Number of iterations it takes to reach optimum under payload of 1-4 bits per pixel on Div2k's validation set.}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c}
\hline
Bits per Pixel & Iterations (avg) & Max Iterations \\
\hline
1 & 6.44 & 18 \\
2 & 9.50 & 17 \\
3 & 11.36 & 21 \\
4 & 11.15 & 24 \\
\hline
\end{tabular}
}
\label{tab:num_k}
\end{minipage}
\hfill
\begin{minipage}{0.5\textwidth}
\caption{Average computation time (in seconds). ($*$ indicates the optimization was stopped after zero error was reached.)}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l|c|c|c|c}
\hline
Method & 1 bit & 2 bits & 3 bits & 4 bits \\
\hline
SteganoGAN & 0.09 & 0.09 & 0.08 & 0.09 \\
FNNS-R & 42.18 & 114.44 & 156.91 & 159.19 \\
FNNS-D & 4.95* & 10.53* & 44.39 & 44.29 \\
\method{} & 0.16 & 0.29 & 0.32 & 0.33 \\
\method{}+LBFGS & 1.26* & 2.81* & 6.64* & 28.59 \\
\hline
\end{tabular} 
}
\label{tab:time}
\end{minipage}
\end{table}

\textbf{Computational Time.}
\label{sec:time_cost}
One of the advantages of using a learned method as opposed to an optimization method is that it is much faster. The average inference time required for the different learned and optimization-based methods is presented in \autoref{tab:time}. The fastest method is SteganoGAN because all that is required to use SteganoGAN is single forward passes. The FNNS variants, on the other hand, are much slower. Our proposed method \method{}, is slightly slower than SteganoGAN but not by a significant magnitude. The reported times were the average times on Div2k's validation set and the methods were run on a Nvidia Titan RTX GPU. 

% \begin{table}[ht]
% \begin{minipage}{0.48\textwidth}
% \centering
% {\small
% \begin{tabular}{c|c|c}
% \hline
% Bits per Pixel & Iterations (avg) & Max Iterations \\
% \hline
% 1 & 6.44 & 18 \\
% 2 & 9.50 & 17 \\
% 3 & 11.36 & 21 \\
% 4 & 11.15 & 24 \\
% \hline
% \end{tabular}
% }
% \caption{Number of iterations it takes to reach optimum under payload of 1-4 bits per pixel on Div2k's validation set.}
% \label{tab:num_k}
% \end{minipage}
% \hfill
% \begin{minipage}{0.48\textwidth}
% \centering
% {\small
% \begin{tabular}{l|c|c|c|c}
% \hline
% Method & 1 bit & 2 bits & 3 bits & 4 bits \\
% \hline
% SteganoGAN & 0.09 & 0.09 & 0.08 & 0.09 \\
% FNNS-R & 42.18 & 114.44 & 156.91 & 159.19 \\
% FNNS-D & 4.95* & 10.53* & 44.39 & 44.29 \\
% \method{} & 0.16 & 0.29 & 0.32 & 0.33 \\
% \method{}+LBFGS & 1.26* & 2.81* & 6.64* & 28.59 \\
% \hline
% \end{tabular} 
% }
% \caption{Average computation time (in seconds). ($*$ indicates the optimization was stopped after zero error was reached.)}
% \label{tab:time}
% \end{minipage}
% \end{table}
\begin{table*}[ht!]
\caption{Steganalysis results using images produced by different methods, evaluated on MS-COCO dataset. SG is used to denote SteganoGAN.}
    \centering
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|cccc|cccc|cccc}
\hline
\multirow{2}{*}{ Setting } & \multirow{2}{*}{ Method } & \multicolumn{4}{c|}{Error Rate (\%) $\downarrow$}  & \multicolumn{4}{c|}{PSNR $\uparrow$} & \multicolumn{4}{c}{Detection Accuracy Rate (\%) $\downarrow$} \\
\cline{3-14}
& & 1 bit & 2 bits & 3 bits & 4 bits & 1 bit & 2 bits & 3 bits & 4 bits & 1 bit & 2 bits & 3 bits & 4 bits \\
\hline
% StegExpose &  2E-04 & 9E-04 & 4E-03 & 4E-02 & 33.79 & 32.62 & 27.82 & 25.36 & 19 & 26 & 33 & 36   \\
%SRNet &  2E-04 & 9E-04 & 4E-03 & 4E-02 & 33.79 & 32.62 & 27.82 & 25.36 & 99 & 99 & 99 & 100 \\
\multirow{3}{*}{ w/o defense } & SG & 3.40 & 6.29 & 11.13 & 15.7 & 25.32 & 24.27 & 25.01 & 24.94 & 52 & 57 & 92 & 94 \\
& FNNS-D & 0.00 & 0.00 & 0.01 & 13.65 & 37.94 & 34.51 & 27.77 & 34.78 & 73 & 75 & 99 & 98 \\
& LISO & 1E-04 & 3E-04 & 5E-03 & 4E-02 & 33.83 & 34.15 & 30.08 & 24.58 & 51 & 42 & 98 & 87 \\
\hline
\multirow{2}{*}{ w/ defense } & FNNS-D & 0 & 0.00 & 0.00 & 0.01 & 22.28 & 22.35 & 21.2 & 21.36 & 7 & 12 & 14 & 83 \\
& LISO & 4E-03 & 1E-02 & 8E-03 & 5E-02 & 31.62 & 28.67 & 26.63 & 25.10 & 4 & 8 & 5 & 5 \\
\hline
\end{tabular}%
}
\vspace{-2ex}
\label{tab:neural_steganalysis}
\end{table*} 
\textbf{Steganalysis.}
\label{sec:steganalysis}
%Avoiding detection of steganographic images is important for many steganography applications. 
To investigate the robustness of our method against statistical detection techniques, we use Stegexpose~\citep{boehm2014stegexpose}, a tool devised for steganography detection using 
four 
well-known steganalysis approaches and find that the detection rate is lower than 25\% for images produced by \method{}.
Recently, more powerful neural-based methods are being used to detect steganography (see related work). It is hard for any steganography method, including classical methods like WOW and S-UNIWARD, to evade detection from these neural methods even with low payload messages of about 0.4bpp~\citep{you2020siamese}. 
However, we show that in the following two scenarios we can evade detection by SiaStegNet~\citep{you2020siamese}. 

In the first scenario, ``w/o defense", the steganography model $M$ is trained without any explicit precautions to avoid steganalysis detection. 
% The attacker (i.e. the party applying steganalysis) trains a steganalysis classifier (SRNet) to classify images during test-time.
We assume the attacker (i.e. the party applying steganalysis) knows the model architecture of $M$ but has no access to the actual weights, exact training set, or hyperparameters. 
However, they can train a surrogate model $M'$ in order to create their own training set of steganographic images. To simulate this setting, we used a steganalysis model trained on CelebA to detect steganographic MS COCO images. From \autoref{tab:neural_steganalysis} we see that in the ``w/o defense" scenario, steganographic images from SteganoGAN and FNNS can be detected much more easily when compared to images from \method{}. 

In the second scenario, ``w/ defense", we take advantage of the fact that neural steganalysis methods are fully differentiable, and both \method{} and FNNS involve gradient-based optimization. It is therefore possible to avoid detection by adding an additional loss term from the steganalysis system into the \method{} optimization or the FNNS optimization. Specifically, during evaluation we add the logit value of the steganographic class to the loss if an image is detected as steganographic. Note that we cannot add an additional loss term to SteganoGAN during inference because SteganoGAN just uses forward passes of a trained neural network for inference. We also tried training a SteganoGAN network with an auxiliary steganalysis loss and found that we couldn't reliably avoid detection even when we did so. 
As seen in \autoref{tab:neural_steganalysis}, we obtain single-digit detection accuracy rates for \method{} in the ``w/ defense" scenario for all bit rates.
% but FNNS 4bpp images are detected with high accuracy. 
Compared to FNNS, \method{} is able to avoid detection more effectively.
The image quality of steganographic images produced by \method{} is also better than those produced by FNNS under this scenario. 
We also tested \method{} with two additional steganalysis systems, SRNet~\citep{boroumand2018deep} and XuNet~\citep{xu2016structural}, and the corresponding results are presented in~\autoref{appendix:steganalysis}.

%  We similarly tried adding an additional loss term to SteganoGAN~\citep{zhang2019steganogan} and found that SiaStegNet was able to still detect steganographic images with 100\% accuracy. To the best of our knowledge, all other learned method that hide large amounts of data (1-4 bpp) fail to evade steganalysis.


