\section{Error vs Iteration}
\begin{figure}[t]
\begin{minipage}[c]{0.5\textwidth}
\centering
\captionsetup{type=figure} %% tell latex to change to table
\includegraphics[width=\linewidth]{imgs/update_curve.pdf}
\caption{\method{} recovery error rate with regard to number of iterations $T$ under different bit rates, evaluated on Div2k's validation set. Y-axis is shown in the log scale.}
\label{fig:error_curve}
\end{minipage}
\hfill
\begin{minipage}[c]{0.45\textwidth}
\centering
\captionsetup{type=figure} %% tell latex to change to table
\includegraphics[width=\linewidth]{imgs/loss.pdf}
\caption{Loss-iteration curves for different optimization methods (at 4 bpp). The pre-trained network is SteganoGAN.}
\label{fig:optimizers1}
\end{minipage}
\end{figure}





% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{imgs/update_curve.pdf}
%     \caption{\method{} decoding error rate with regard to number of iterations $T$ under different bit rates, evaluated on Div2k's validation set. Y-axis is shown in log scale.}
%     \label{fig:error_curve}
% \end{figure}

\autoref{fig:error_curve} shows how the recovery error rate decreases every iteration. As we see, the error rate monotonically decreases and this implies that \method{} learns a good descent direction. We also see that 15-20 iterations are enough for the error rate to converge.

\section{Loss for Different Optimization Methods}
\autoref{fig:optimizers1} is analogous to figure 4 in the main paper. However, instead of seeing how the error rate decreases for different optimization methods, we see how the loss decreases. Note that the first data point we plot for each curve is after 1 iteration and we do so to improve the visualization. Similar to figure 4, we see that the loss for \method{} optimization decreases much faster than any other method. 

 
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{imgs/loss.pdf}
%     \caption{Loss-iteration curves for different optimization methods (at 4 bpp). The pretrained network is SteganoGAN.}
%     \label{fig:optimizers1}
% \end{figure}