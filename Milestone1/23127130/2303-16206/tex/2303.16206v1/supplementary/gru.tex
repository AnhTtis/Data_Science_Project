\section{GRU Operations}
\label{suppl-sec:gru}

\begin{align} 
\bm{z}_{t} &= \sigma\left(\text{Conv}_{3\times 3}\left(\left[\bm{h}_{t-1}, \bm{x}_{t}\right], \bm{W}_z\right)\right) \nonumber \\ 
\bm{r}_{t} &= \sigma\left(\text{Conv}_{3\times 3}\left(\left[\bm{h}_{t-1}, \bm{x}_{t}\right], \bm{W}_r\right)\right) \nonumber \\
\bar{\bm{h}}_{t-1} &= \tanh\left(\text{Conv}_{3\times 3}\left(\left[\bm{r}_t \odot \bm{h}_{t-1}, \bm{x}_t\right], \bm{W}_h\right)\right) \nonumber\\
\bm{h}_t &= \left(1-\bm{z}_t\right) \odot \bm{h}_{t-1} + \bm{z}_t \odot \bar{\bm{h}}_{t-1} 
    \label{eq:gru1}
\end{align}

As described in the paper, the Gated Recurrent Unit (GRU) is an integral part of the \method{} encoder.
The operations of the GRU used in \method{} are defined in \autoref{eq:gru1}, where $\bm{x}_t$ is the input to the GRU in $t^{\text{th}}$ iteration, $\bm{h}_{t}$ is the GRU's hidden state, and $\bm{W}_z, \bm{W}_r, \bm{W}_h$ are GRU's weight matrices. For \method{}, $\bm{x}_{t}$ is the concatenation of features extracted from the input image, the gradient from the loss, and the perturbation.