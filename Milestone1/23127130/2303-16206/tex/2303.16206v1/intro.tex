\section{Introduction}
\label{sec:intro}

Image steganography aims to alter a cover image, to imperceptibly hide a (secret) bit string, such that a previously defined decoding method can then extract the message from the altered image. Steganography has been used in many applications such as digital watermarking to establish ownership~\citep{cox2007digital}, copyright certification~\citep{7030779}, anonymized image sharing~\citep{kishore2021fixed}, and for hiding information coupled with images (e.g. patient name and ID in medical CT scans~\citep{srinivasan2004secure}). 
Following the success of neural networks on various tasks, prior work has used end-to-end encoder-decoder networks for steganography~\citep{zhu2018hidden,zhang2019steganogan,NIPS2017_fe2d0103,baluja2017hiding}. 
The encoder takes as input an image $\bm{X}$ and a message $\bm{M}$, and produces a steganographic image $\bm{\tilde{X}}$ that is visually imperceptible from the original image $\bm{X}$. 
The decoder recovers the message $\bm{M}$ from the steganographic image $\bm{\tilde{X}}$. 
Such methods can hide large amounts of data in images with great image quality, due to convolutional networks' ability to generate realistic outputs that are close to the input image along the manifold of natural images~\citep{zhu2016generative,zhang2019steganogan}. 
However, these approaches can only reliably encode 2 bits per pixel. 
At higher bit rates, they suffer from poor recovery error rates for the message $\bm{M}$ (around 5-25\% at 4 bpp). 
Consequently, they cannot be used for some steganography applications that require the message to be recovered with 100.0\% accuracy, for example, if the message is encrypted or hashed. Although error-correcting codes~\citep{crandall1998some,munuera2007steganography} can be used to recover spurious mistakes, their reliance on additional parity bits reduces the payload, negating much of the advantages of neural approaches. %Error-correcting codes might also necessitate  assumptions on the error distribution, e.g. uniform error, and it might be hard to ensure that these assumptions are met. 

% This approach side-steps the optimization process by predicting the altered image directly through a single forward pass in a neural network.
Recent work has abandoned learned encoders altogether and reformulated image steganography as a constrained optimization problem~\citep{kishore2021fixed}, where the steganographic image is optimized with respect to the outputs of a fixed (random or pre-trained) decoder -- by employing a technique based on adversarial image perturbations~\citep{szegedy2013intriguing}. The optimization problem is solved with off-the-shelve gradient-based optimizers, such as projected gradient descent~\citep{carlini2017towards} or L-BFGS~\citep{fletcher2013practical}. Such approaches achieve low error rates with high payloads (2-3\% error at 4 bpp), 
but are slow and prone to getting stuck in local minima. 
Further, each pixel is optimized in isolation, and pixel-level constraints only ensure that the steganographic image stays close to the input image according to an algebraic norm, rather than along the natural image manifold.
Although similar manifold-unaware approaches are successfully deployed for adversarial attacks, steganography aims to precisely control millions of binary decoder outputs, instead of a single class prediction; the resulting optimization problem is thus harder and prone to producing unnatural-looking images. 
%Constraining the optimization problem with respect to the illusive image manifold is not possible in classical optimization theory. 

%Ideally, one would like to constrain the solution to be close to the input image  -- an impossible constraint in classical optimization theory. 

%These  methods iteratively refine a solution to get closer to the optimum solution. Image steganography is also one problem that benefits from iterative refinement


%Recognizing that obtaining an accurate prediction in a single shot is difficult for many complex problems, there has been a push to use iterative methods in place of single-pass neural networks~\citep{teed2020raft,alaluf2021restyle,kishore2021fixed}. 



%Fixed Neural Network Steganography (FNNS) is a recent method that casts image steganography as an optimization problem. Unlike neural methods that encode a message with a single forward pass, FNNS performs multiple steps of gradient based optimization. Although FNNS yield far lower error rates when compared to single-pass neural methods, it has certain drawbacks. Firstly, it is about 1000x slower because FNNS performs a separate per-image level optimization for every image-message input. Secondly, FNNS performs optimization using a neural network with frozen weights and as a result the FNNS optimization process cannot share information across optimizations for different inputs. Thirdly, the constraints imposed during FNNS optimization are only approximate constraints; FNNS ensures that the altered image is not very different by imposing a restriction on how much each pixel can change but there could be unnatural images that satisfy this constraint. Lastly, FNNS's success is highly dependent on the type of decoder and the optimization algorithm that is used; for example, pre-trained decoders yield lower recovery error than decoders with randomly initialized weights and L-BFGS yields faster convergence than PGD. 


%They do not take advantage of the ability of convolutional neural networks to learn the underlying image manifold~\citep{zhu2016generative}, e.g. through a critic classifier that steers clear of perceptible changes. 


\begin{wrapfigure}{rt}{0.5\textwidth}
    \centering
    \vspace{-\baselineskip}
    \includegraphics[width=\linewidth]{imgs/liso_teaser.pdf}
    \caption{\method{} iteratively optimizing a sample image with 3 bits encoded in each pixel. The recovery error goes down drastically to exactly zero in only 8 steps. The perturbation (difference between the steganographic image and the original image) is large in the beginning, but becomes imperperceptible with \method{}'s optimization, leading to a natural-looking steganographic image.}
    %The operations in GRU are explained in \autoref{eq:gru1}. 
    \label{fig:teaser}
    \vspace{-\baselineskip}
\end{wrapfigure}

In this paper we introduce \emph{\methodlong{} (\method{})}, a method that combines the ability of neural networks to learn image manifolds with the rigor of constrained optimization. \method{} is highly efficient in hiding messages inside images, and achieves very low message recovery error. 
%Meanwhile, it also learns the manifold of natural images for image generation.
%    In this paper we introduce \methodlong{} (\method{}), a novel method that subscribes to both paradigms, learning neural encoders to learn the manifold of natural images and obtain faster methods, and optimization to generate the steganographic images with low error rate. 
\method{} achieves these feats with an encoder-decoder-critic architecture. 
The encoder is a \textit{learned} recurrent neural network that iteratively solves~\citep{teed2020raft,alaluf2021restyle} the constraint optimization problem proposed in~\citet{kishore2021fixed} and mimics the steps of a gradient-based optimization method. \autoref{fig:teaser} demonstrates how the steganographic image and error rate change over subsequent iterations.
%The encoder uses a neural network to mimic the update steps of a gradient based optimization method, like PGD, or L-BFGS, to encode the message in the image. 
%Instead of performing a fixed algorithm (like PGD, or L-BFGS), we define a recurrent neural optimizer network~\citep{teed2020raft} that iteratively performs gradient updates on the input image. 
% Similar to traditional optimization algorithms, the iterative encoder takes as input the initial image, the current image, the gradient with respect to the loss, and learns an  update direction. Following other recently published iterative computer vision algorithms~\citep{teed2020raft,alaluf2021restyle} we use a GRU-based recurrent network. 
We also use a critic network~\citep{zhang2019steganogan} to ensure the changes stay imperceptible 
% (in addition to hard constraints) 
and that the steganographic image looks natural. 
% We train the parameters of the iterative encoder and the decoder simultaneously to work together efficiently. 
%In other words, we train an optimizer that is particularly well suited to perform gradient descent style updates on an image steganography problem to change the predictions of the  decoder. 
%Jointly, we also update the weights of the decoder to be particularly amenable to such an optimization. 
Our resulting architecture can be trained end-to-end.%, which results in an optimizer and decoder pair. 
We show that \method{} learns a more efficient descent direction than standard (manifold unaware) optimization algorithms and produces better steganography results with great consistency. 
Finally, the error rate can (almost) always be driven to a flat 0 if the optimization is finished with a few iterations of L-BFGS within the vicinity of LISO's solution (LISO\ +\ L-BFGS). 

%We show that we can learn a descent direction that is better than that of using a fixed algorithm like L-BFGS.


%subscribe to the optimization view of steganography, but \textit{learn} a gradient based optimization algorithm. 
%
%propose an iterative algorithm that sol
%to use a learned iterative neural network for steganography. Since, this method is learned it will be faster. 
%
%Convolutional neural networks are particularly suited at learning the natural image manifold~\citep{zhu2016generative}. We can take advantage of this fact and ensure that the changes we make to the cover image don't move the image of the natural image manifold. In this paper we aim to formalize and solve the problem of \textbf{learning} the decoder and the optimization algorithm jointly.
%We define the decoder as a multi-layer convolutional neural network with $B$ outputs per pixel, similar to prior work~\citep{kishore2021fixed,zhang2019steganogan}. Very different, however, is our optimization procedure, which uses a neural network to mimic the update steps of a gradient based optimization method, to encode the message in the image. Instead of performing a fixed algorithm (like PGD, or L-BFGS), we define a recurrent neural optimizer network~\citep{teed2020raft} that iteratively performs gradient updates on the input image. Similar to traditional optmization algorithms, the optimizer takes as input the loss, the gradient with respect to the loss, and the input image, but it can learn what iterative updates to perform. We train the weights of the decoder and the optimizer simultaneously to work together efficiently. In other words, we train an optimizer that is particularly well suited to perform gradient descent style updates on an image steganography problem to change the predictions of the decoder. Jointly, we also update the weights of the decoder to be particularly amenable to such an optimization. Our resulting network can be trained end-to-end and results in an optimizer and decoder network. We show that we can learn a descent direction that is better than that of using a fixed algorithm like L-BFGS.

We evaluate the efficacy of LISO extensively across multiple datasets. We demonstrate that at test-time, with unseen cover images and random bit strings, the optimizer can reliably circumvent bad local minima and find a low-error solution within only a few iterative steps that already outperforms all previous encoder-decoder-based approaches. If the optimization is followed by a few additional updates of L-BFGS optimization, we can reliably reach 100\% error-free recovery even with 3 bits per pixel (bpp) hidden information across all (thousands of) images we tested on. 


Our contributions are as follows.  
1) We introduce a novel gradient-based neural optimization algorithm, LISO, that learns preferred descent directions and is image manifold aware.
2) We show that its optimization process based on learned descent directions is \emph{orders of magnitudes} faster than classical optimization procedures. 
3) As far as we know, our variant LISO\ +\ L-BFGS is by far the most accurate steganography algorithm in existence, resulting in perfect recovery on \emph{all} images we tried with 3bpp and the vast majority for 4bpp --- implying that it avoids bad local minima and can be deployed without error correcting code.
%\footnote{The popular (7,4)-Hamming Code, 4 bpp result in 1.7 bpp effective payload.} 
% \footnote{Note that the zero error rate can be validated after the encoding process.} 
The code for \method{} is available at \url{https://github.com/cxy1997/LISO}.


