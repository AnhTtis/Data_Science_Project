% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr2023/cvpr}      % To produce the REVIEW version
% \usepackage{cvpr2023/cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr2023/cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{algorithm,algorithmic,bm,color}

\newcommand{\yulei}[1]{\textcolor{red}{\textsuperscript{\textit{Yulei}}\textsf{\textbf{\small[#1]}}}}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{2202} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\usepackage{url}
\usepackage{subfiles}
\usepackage{graphicx}
\usepackage{term_to_use}
\usepackage{boldline}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{bbm}
\usepackage{setspace}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{\approach{}: Discriminative Geometry-Aware Learning for\\ Generalized Few-Shot Object Detection}

\author{Jiawei Ma \quad Yulei Niu$^{*}$ \quad Jincheng Xu \quad Shiyuan Huang \quad Guangxing Han \quad Shih-Fu Chang\\
Columbia University\\
{\tt\small \{jiawei.m, yulei.niu, jx2467, shiyuan.h, gh2561, sc250\}@columbia.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}
\maketitle
\let\thefootnote\relax\footnotetext{$^{*}$Corresponding Author.}

%%%%%%%%% ABSTRACT
\begin{abstract}
    Generalized few-shot object detection aims to achieve precise detection on both base classes with abundant annotations and novel classes with limited training data. Existing approaches enhance few-shot generalization with the sacrifice of base-class performance, or maintain high precision in base-class detection with limited improvement in novel-class adaptation. In this paper, we point out the reason is insufficient
    \underline{Di}scriminative feature learning for all of the classes. As such, we propose a new training framework, DiGeo, to learn \underline{Geo}metry-aware features of inter-class separation and intra-class compactness. To guide the separation of feature clusters, we derive an offline simplex equiangular tight frame (ETF) classifier whose weights serve as class centers and are maximally and equally separated. To tighten the cluster for each class, we include adaptive class-specific margins into the classification loss and encourage the features close to the class centers. Experimental studies on two few-shot benchmark datasets (VOC, COCO) and one long-tail dataset (LVIS) demonstrate that, with a single model, our method can effectively improve generalization on novel classes without hurting the detection of base classes. Our code can be found \href{https://github.com/Phoenix-V/DiGeo}{here}. 
\end{abstract}

    % Due to the extreme imbalance between base and novel sets, the margins are learned through self-distillation from the prior of training instance distribution.
% Detailed ablation studies are provided to justify the selection of each component.

% \begin{abstract}
%     Generalized few-shot object detection aims to achieve precise detection on both \textit{base} classes with abundant annotations and \textit{novel} classes with limited training data. Existing transfer learning based approaches, the regularization such as consistency are studied to maintain high performance in base detection, while the efficiency in few-shot adaptation is limited. In this paper, we point out the reason is insufficient discriminative feature learning for all classes. As such, we propose a new training framework \approach{} to learn a feature distribution of inter-class separation and intra-class compactness. To guide the separation of different feature clusters, we derive an offline classifier whose weights are maximally \& equally separated and are assigned as fixed centers for all classes. Then, we add class-specific margins during training to make the features compact and close to the assigned centers. Due to the extreme imbalance between base and novel sets, the margins are learned through self-distillation from the prior of training instance distribution. Experiments on three benchmark datasets shows consistent performance gain demonstrate the effectiveness of our method. Detailed ablation studies are provided to justify the selection of each component. 
% \end{abstract}
%%%%%%%%% BODY TEXT
\subfile{sections/introduction}
\subfile{sections/relatedwork}
\subfile{sections/approach}
\subfile{sections/experiment}
\subfile{sections/discussion}

\section{Conclusion}
In this paper, we revisit generalized few-shot object detection from a perspective of discriminative feature learning. We further proposed a simple but effective framework, Discriminative Geometry-aware (\approach{}) learning, for inter-class separation and intra-class compactness. Experiments demonstrates that our~\approach{} improves generalization on novel classes without hurting the detection of base classes, and can be extended to long-tail object detection. In the future, we will keep investigating the desired properties of features in object detection and adapted it more realistic scenarios such as domain adaptation.
 

{\small
\begin{spacing}{0.9}
\noindent \textbf{Acknowledgement} This material is based on research sponsored by Air Force Research Laboratory (AFRL) under agreement number FA8750-19-1-1000. 
The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation therein. 
The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of Air Force Laboratory, DARPA or the U.S. Government.
\end{spacing}
}

\subfile{sections/appendix}

% \section{Conclusion}
% In this paper, we revisit (generalized) few-shot object detection from the perspective of learning discriminative features, and propose a simple yet effective training framework, \approach{}. As conventional transfer learning approaches is trained on the union of the whole set of novel classes and a heavily down-sampled subset of base classes, we think the drop of base detection is resulted from the overfitting to base subset and thus choose to train the detector on the full train set. Then, we consider inter-class separation and intra-class compactness to learn discriminative features for all classes. Specifically, we derive an offline classifier whose weights are maximally \& equally separated. We assign the weights as fixed centers for all classes to guide the separation of feature clusters. Then, we add class-specific margins during training to make the features compact and close to the assigned centers. We initialize the margin according to the prior of training instance distribution and adjusted it through self-distillation. We also apply RFS to upsample the novel set to ensure the detectors are sufficiently learned from the novel instances.
% Besides, GFSOD, our approach can be naturally generalized to long-tail object detection. Overall, our approach achieves consistent performance gain on three benchmark dataset, which experimentally demonstrated the effectiveness of our framework. We also provide analysis to validate our design choices.


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{cvpr2023/ieee_fullname}
\bibliography{cvpr2023/egbib}
}

\end{document}

% In this paper, we revisit (generalized) few-shot object detection from the perspective of learning discriminative features, and propose a simple yet effective training framework, \approach{}. As conventional transfer learning approaches is trained on the union of the whole set of novel classes and a heavily down-sampled subset of base classes, we think the drop of base detection is resulted from the overfitting to base subset and thus choose to train the detector on the full train set. Then, we consider inter-class separation and intra-class compactness to learn discriminative features for all classes. Specifically, we derive an offline classifier whose weights are maximally \& equally separated. We assign the weights as fixed centers for all classes to guide the separation of feature clusters. Then, we add class-specific margins during training to make the features compact and close to the assigned centers. We initialize the margin according to the prior of training instance distribution and adjusted it through self-distillation. We also apply RFS to upsample the novel set to ensure the detectors are sufficiently learned from the novel instances.
% Besides, GFSOD, our approach can be naturally generalized to long-tail object detection. Overall, our approach achieves consistent performance gain on three benchmark dataset, which experimentally demonstrated the effectiveness of our framework. We also provide analysis to validate our design choices.
