\section{Experiment}

We mainly conduct experiments on the few-shot object detection (FSOD) benchmark datasets Pascal VOC and MS COCO to validate the effectiveness of our proposed~\approach{}. We further apply~\approach{} on long-tailed object detection and conduct experiments on LVIS to show its generalizability.

%  Given the Base detection as initialization, during adaptation, (b) TFA~\cite{wang2020frustratingly} trains a use the During adaptation, we can use (a) the full set $\baseset{} \cup \novelset{}$, (b) a balanced set $\basesubset{} \cup \novelset{}$ by down-sampling $\baseset{}$ aggressively, or only the \textit{novel} set $\novelset{}$. 


\subfile{../results/table_coco}

\subsection{Datasets \& Training Details}\label{sec:evalsetup}

\bitem{Pascal VOC}~\cite{pascal-voc-2007,pascal-voc-2012} consists of 20 classes where the class split for $\basecls{}$ and $\novelcls{}$ are 15 and 5 separately. 
The train set $\baseset{} \cup \novelset{}$ are from Pascal VOC 07+12 trainval sets~\cite{pascal-voc-2007,pascal-voc-2012} where $\novelset{}$ is randomly sampled with $K$ in $\{1,2,3,5,10\}$.
Following TFA~\cite{wang2020frustratingly}, we conduct experiments on three base-novel class partitions marked as $\{1,2,3\}$. In each partition, for fair comparison, we use the same sampled novel instances and report the detection precision for $\novelcls{}$ (nAP$_{50}$), $\basecls{}$ (bAP$_{50}$) and $\basecls{} \cup \novelcls{}$ (AP$_{50}$) on Pascal VOC 07 test set~\cite{pascal-voc-2007}. 

\bitem{MS COCO}~\cite{lin2014microsoft} is derived from COCO14~\cite{lin2014microsoft} consisting of 80 classes where $|\basecls{}|=60,|\novelcls{}|=20$ and $\novelcls{}$ are in common with Pascal VOC. The $\baseset{}$ and $\novelset{}$ are from train set with $K=\{10,30\}$. The detection precision of $\novelcls{}$ (nAP), $\basecls{}$(bAP) and $\basecls{} \cup \novelcls{}$ (AP) on COCO 14 val set are reported.

\bitem{LVIS}~\cite{gupta2019lvis} is derived from COCO17~\cite{lin2014microsoft} and contains $\sim$0.7M training instances of 1230 classes. 
The classes are divided into three groups w.r.t. the amount of annotation, rare (1-10), common (11-100), and frequent ($>$100).
Following~\cite{wang2020frustratingly}, we report the precision for all classes (AP) and class groups (AP$_r$, AP$_c$, and AP$_f$) on the val set.

\bitem{Implementation Details.} We instanlize our approach on Faster-RCNN~\cite{wang2020frustratingly,ren2015faster} which employs a region proposal network (RPN) to generate region candidates. For fair comparison, we use ResNet-101 with FPN~\cite{lin2017feature} as backbone to extract image feature maps where the Resnet-101 backbone is initialized by ImageNet~\cite{krizhevsky2017imagenet}-pretrained model.
As the outputs of penultimate layer in original classification module are non-negative and does not meet the property of the ETF classifier, we add a linear layer (projector) with the same input and output dimension on top of the penultimate layer. The projector output is then used for classification.
For RFS~\cite{he2009learning}, we set the up-sampling threshold as 0.01 for PASCAL VOC and MS COCO and 0.001 for LVIS.
During distillation, we share and fix the parameters of ResNet101 and FPN and only learn a new detection head.
We follow the setup in TFA~\cite{wang2020frustratingly} baseline such as SGD optimizer~\cite{sutskever2013importance}. More details can be found in Supp.



\subsection{Comparison with FSOD Methods}\label{sec:experiment}

We show the comparisons between our methods and state-of-the-art few-shot object detection approaches on PASCAL VOC and MSCOCO. We follow previous works to conduct experiments on three data splits with different shots of novel classes.
As for the performance AP$_{50}$ over all classes in Table~\ref{tab:voc-all}, our~\approach{} achieves the best performances for 12 out of 15 cases. Compared to the baseline method TFA~\cite{wang2020frustratingly}, our~\approach{} outperformed TFA consistently in all shots \& splits. Compared to the state-of-the-art Retentative RCNN model, our~\approach{} achieves better AP$_{50}$ when the number of shots is larger than 2, and obtains comparable performances for extremely few-shot cases. As for the detailed comparisons over novel classes (nAP$_{50}$) and base classes (bAP$_{50}$) in Table~\ref{tab:voc-all-detail}, our~\approach{} still consistently outperforms the baseline TFA method for all the settings. 

In addition, our~\approach{} achieves a better trade-off between base-class performance and novel-class generalization. On the one hand, although MPSR achieved higher performance for extremely few-shot settings (\eg, 36.2 vs. 31.6 for 1-shot) and competitive performances with $\{3,5,10\}$-shots, its performance drops by large margins (\eg, 68.1 vs. 81.3 for bAP$_{50}$). This observation indicates that MPSR improves the few-shot generalization with the sacrifice of base-class knowledge. On the other hand, as Retentive RCNN~\cite{fan2021generalized} includes the base detector \& RPN through max ensembling at test time, the adapted detector can be trained to specifically detect novel instances, where the bAP$_{50}$ is slightly higher than ours (\ie, 81.6 vs. 81.3). However, its few-shot generalization is not satisfying when the number of shots is larger than 2 (\eg, 6.3 lower than ours with 10 shots). In contrast, our approach only train a single detector and achieves stable and consistent gain. Similarly, for the results on MSCOCO shown in Table~\ref{tab:coco}, our~\approach{} outperforms Retentive RCNN for the novel-class metrics including nAP$_{75}$, nAPs, nAPm and nAPl. These comparison demonstrate that our~\approach{} has a strong few-shot generalization ability without base-class knowledge forgetting.

