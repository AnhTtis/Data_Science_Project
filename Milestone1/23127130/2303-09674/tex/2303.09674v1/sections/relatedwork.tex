\section{Related Work}

\begin{figure*}[]
    \centering
    \includegraphics[width=0.95\linewidth]{results/approach.pdf}
    \caption{Comparison of training frameworks. (a) Conventional approaches first pre-train a base detector among the \textit{base} set $\baseset{}$ and then finetune on the union of novel set $\novelset{}$ and down-sampled subset of base classes $\basesubset{}$. (b) Instead, we choose to up-sample $\novelset{}$ and directly train the detector on the full set. We derive a fixed classifier offline with maximally \& equally separated weights and learn the adaptive margins to tighten the feature clusters. The margins are estimated from priors of instance distribution and learned though self-distillation. The block with shading means training from scratch. We use the same design for localization and omit it for simplicity.}
    \label{fig:appraoch}
\end{figure*}
% To ensure the inter-class separation and intra-class compactness, 

\bitem{Few-shot object detection (FSOD)} aims to detect objects of few-shot (\textit{novel}) classes at instance-level. 
% In addition to recognition with a few training data, the objects should also be properly localized. 
To improve the adaptation efficiency, the approaches based on the meta-learning and the transfer-learning are investigated.
The \emph{meta-learning approaches \cite{fan2020few,han2022meta,Han_2022_CVPR,kang2019few,Han_2021_ICCV,han2022multimodal}} learns a class-agnostic meta-learner to align instances of the same class from different images. 
Under the Faster-RCNN framework, the attention-based meta-RPN~\cite{fan2020few} and meta-detector~\cite{han2022meta} are proposed to generate class-relevant proposals and improve the instance alignment.
% FewX~\cite{fan2020few} designs an attention-based meta-learner to generate class-relevant proposals. 
% Then, Meta Faster-RCNN~\cite{han2022meta} sets a meta-detector to improve the alignment at fine-grained level.
In addition, approaches based on Transformer~\cite{Han_2022_CVPR} and YoLo~\cite{kang2019few} are proposed to extract features jointly and align features at multiple scales. 
% 
The \textit{transfer-learning approaches \cite{wang2020frustratingly,wu2020multi,sun2021fsce,ma2022few}} performs finetuning for few-shot adaptation. 
Specifically, TFA~\cite{wang2020frustratingly} pre-trains an base detector from plenty of \textit{base} samples and finetune it for novel classes. 
To improve the adaptation efficiency, multi-scale feature extraction~\cite{wu2020multi} and regularization such as contrastive loss~\cite{sun2021fsce}, margin equilibrium~\cite{li2021beyond} and transformation invariance~\cite{li2021transformation} are employed.
Recently, DeFRCN~\cite{qiao2021defrcn} adjusts gradients back-propagated from different losses and achieve superior novel detection scores. 

\bitem{Generalized Few-Shot Object Detection.}
For all FSOD approaches mentioned above, the precision on \textit{base} detection is sacrificed after few-shot adaptation. 
This phenomenon has also been observed in various vision tasks where models forget the base knowledge due to domain gap or distribution gap~\cite{tang2020unbiased,niu2021counterfactual,qi2020two,niu2021introspective,zhu2022cross,zhu2022prompt,niu2022respecting}.
As pointed out by Fan \etal~\cite{fan2021generalized}, different from the classification~\cite{finn2017model,snell2017prototypical,tian2020rethinking,ma2021partner,zhao2021domain,liu2020negative,Han_2023_CVPR,huang2022task,ypsilantis2021met},
an image may contain instances from both novel and base classes and base detection is also important.
Then, they propose a consistency regularization few-shot fine-tuning and employ an model ensembling technique
% of the detectors 
to preserve the precision of base detection. However, the few-shot adaptation efficiency is inevitably limited.
In a more general case, \emph{long-tail object detection} (LTOD) has been studied where techniques such as resampling~\cite{zhang2021distribution,ren2020balanced}, decoupling~\cite{li2020overcoming,wang2020devil} and reweighting~\cite{zhang2021distribution,li2022equalized} are studied. Also, ACSL~\cite{wang2021adaptive} revisits LTOD from a statistic-free perspective and propose the adaptive suppression loss.


\bitem{Feature Distribution on a Balanced Set} has been studied in classification. To be specific, the weights in the last linear layer is treated as class centers where the geometry property of feature output by pernuminate layer is analyzed. Recently, Papyan \etal~\cite{papyan2020prevalence} summarized it as neural collapse (NC) and observed that 1) the features in the same class are maximally concentrated towards the class mean and different feature clusters are maximally separated~\cite{yang2022we}. 2) The class means and the class centers converge to each other.  
