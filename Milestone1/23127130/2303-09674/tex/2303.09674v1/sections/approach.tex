\section{Background}

We first introduce the few-shot object detection (FSOD) task, and analyze the limitations of existing FSOD methods.

\subsection{Few-shot Object Detection}\label{sec:task-pre}

In this paper, we focus on the task of few-shot object detection (FSOD). The training data consists of a \textit{base} set $\baseset{}$ and a \textit{novel} set $\novelset{}$, where the \textit{base} classes $\basecls{}$ have plenty of annotated object instances while \textit{novel} classes $\novelcls{}$ has limited annotations.
% (\eg, 5 shots per class). 
In an $N_n$-way $K$-shot FSOD task with $|\novelcls{}|=N_n$, each novel class has $K$ annotated instances.  
Note that an image may contain multiple instances from different classes with associated bounding boxes, which is more challenging than the few-shot classification where each image contains one object to be recognized.
% $\{(c_i,u_i)\}_{i=1}^{N_i}$ where $N_i$ is the number of the instances and $c_i \in \basecls{} \cup \novelcls{}$ for $i \in \{1,...,N_i\}$.
Then, we follow ~\cite{fan2021generalized,wang2020frustratingly} to validate the robustness of detection model under the generalized few-shot object detection (GFSOD) setting, where the test samples come from both base and novel classes, and the models are evaluated on all classes. 



Commonly, object detection models consist of a proposal generation module to generate a set of region candidates, and a detection module to localize \& classify objects on the extracted proposals~\cite{ren2015faster,redmon2016you,carion2020end,zhu2020deformable,sun2021sparse}. 
For the classification part, an additional background class should be considered to recognize the proposal with no foreground objects. We regard the last linear layer as classifier, and its weights $W = \{\mathbf{w}_i\}_{i=1}^{N_b+N_n+1}$ as class centers where $N_b=|\mathcal{C}_b|$. 
% In this way, for each proposal, it is represented by a feature $x \in \mathcal{R}^d$ output by the penultimate layer. 
Without loss of generality, we set $W_b = \{\mathbf{w}_i\}_{i=1}^{N_b}$, $W_n = \{\mathbf{w}_i\}_{i=N_b+1}^{N_b+N_n}$, and $W_- = \{\mathbf{w}_{N+1}\}$ as weights for base classes $\basecls{}$, novel classes $\novelcls{}$, and  background $\negcls{}$.


\subsection{Analysis of Existing Methods}\label{sec:preliminary}

As a representative \textit{transfer-learning} approach shown in Fig.~\ref{fig:appraoch}(a), TFA~\cite{wang2020frustratingly} first trains a \textit{Base} detector on $\baseset{}$ for $\basecls{}$ as initialization. Then, in an $N_n$-way $K$-shot GFSOD task, $K$ instances for each \textit{base} class $c \in \basecls{}$ from $\baseset{}$ are selected to a subset $\basesubset{}$. The detector is fine-tuned on  $\basesubset{} \cup \novelset{}$ with balanced training data distribution over $\basecls{} \cup \novelcls{}$. However, for each class, as the training data is extremely limited, overfitting to $\basesubset{}$ is unignorable and results in the drop of base detection. As such, Retentive RCNN~\cite{fan2021generalized} proposes to ensemble the detector adapted for $\basecls{} \cup \novelcls{}$ and the \textit{Base} detector by combining their outputs as final prediction. However, the novel detection performance on $\novelcls{}$ is limited. 

Nevertheless, training among $\baseset{} \cup \novelset{}$ makes the model favor $\basecls{}$. As shown in Fig.~\ref{fig:step-center-ablation}(a), the novel weights $W_n$ are not well-learned and close to weights of other foreground classes. 
% , the center of class ``bird'' is close to all other few-shot class centers and the center of ``aeroplane''. \mathbf{w}_i 
% 
With such a classifier, the proposal features (\ie, input feature of classifier) cannot be separated. Thus, as shown in Fig.~\ref{fig:appraoch}(b), we obtain a classifier offline with well-separated weights. For each class, the features are trained to be compact and close to the centers using learnable margins.


\section{Approach}

Considering the limitations mentioned above, we aim to achieve the best of both worlds using a single model, \ie, improve the few-shot adaptation performance on novel classes without hurting the precision on base detection. Our motivation is to enhance the discriminative feature learning of detection models, \ie, clear boundaries on the feature space to discriminate all classes. We realize this idea from two aspects, inter-class separation between all classes and intra-class compactness for each class. 

\subsection{Inter-Class Separation}\label{sec:inter}

We realize inter-class separation by maximizing the pair-wise distances between class centers. Specifically, for each $\mathbf{w}_i$, we maximize its minimum 
distance with all other weights $W\setminus\{\mathbf{w}_i\}$:
\begin{equation}
\begin{gathered}
    W^* = \textrm{argmax}_{W}\sum\nolimits^{N_c}_{i=1} \min\nolimits_{j,i \neq j} \|\mathbf{w}_i-\mathbf{w}_j\|_2^2 \\
    s.t.~\|\mathbf{w}_i\|=1,~~\forall \mathbf{w}_i \in W
\end{gathered}\label{eq:etf}
\end{equation}
where $N_c=N_b+N_n+1$ and all weight vectors are of the same norm (\eg, 1). When the feature dimension $d \geq N_c-1$, the distances of all class center pairs in $W^*$ should be the same. Also, the angle between any two of the class centers has the same value given $\|\mathbf{w}_i\|=1$. In this way, we expect the class centers to be evenly distributed in the feature space. In this case, $W^*$ is equivalent to simplex equiangular tight frame (ETF)~\cite{papyan2020prevalence}. Furthermore, we have the following theorem for ETF.

\noindent\textbf{Theorem} \textit{Suppose the vector space is $d$-dimensional and the number of vectors is $N$. When $d \geq N-1$, we can always derive a simplex ETF whose vectors are maximally and equally separated from each other.}

The above theorem guarantees the existence of ETF in application when $d \geq N-1$. For $d < N_c-1$, \eg, the number of classes is large while the feature dimension is compact, we can project the $d$-dim feature to a $d'$-space space with $d' \geq N_c-1$. Then, we can always obtain a Simplex ETF classifier in the mapped feature space.

We have two options to obtain the Simplex ETF classifier. The \emph{online} solution is to use Eq.~\eqref{eq:etf} as a regularization loss to learn the classifier during training. The \emph{offline} solution is to manually set the Simplex ETF classifier for all classes $\mathcal{C} = \basecls{} \cup \novelcls{} \cup \{\negcls{}\}$ and fix it during training. We experimentally find the offline solution is more stable and better than the online solution (details discussed in Sec.~\ref{sec:discussion}) and thus use the offline solution in implementation.

\subsection{Intra-Class Compactness}\label{sec:intra}


We realize intra-class compactness by tightening the clusters of features and push the samples close to the assigned center in $W^{*}$. The challenges are two folds. First, the number of training samples in base and novel classes extremely are imbalanced, which makes it hard to determine the boundaries of novel classes in the feature space. Second, as the number of novel classes is much smaller than that of base classes, \ie, $|\novelset{}| \ll |\baseset{}|$, the network receives less positive gradients for novel classes~\cite{wang2021adaptive}, which makes the features of instances in novel classes farther to the class centers and thus less discriminative. 

Inspired by the success of logit adjustment in long-tailed recognition~\cite{menon2020long}, we apply class-specific margins on logits to modify the classification loss and balance the optimization between base and novel classes. Specifically, we calculate the class-specific margins based on the frequencies of instance (\ie, bounding box annotations) as priors:


\begin{equation}
    m_c=
    \begin{cases}
    -\log(p_c) & \text{, if $c\in\basecls{} \cup \novelcls{}$}\\
    -\log(p_-) & \text{, if $c=\negcls{}$}\\
    \end{cases},
\end{equation}
where $p_c$ is the frequency of bounding box annotations for class $c$, and $p_-$ is an estimated probability of background boxes to train the classifier, and $p_- + \sum_{c\in\basecls{} \cup \novelcls{}}p_c=1$.  Intuitively, the class with fewer data is assigned with a larger margin to guarantee the learning of this class.
% Since most of the candidate proposals are about background, \ie, the number of bounding box for background is an extremely large number, we set $m_{\negcls{}}$ as 0.

Suppose that the logit outputs for sample $x$ are $\mathbf{v} = \{v_c\}_{c \in \mathcal{C}}$, we use the following prior-margin cross-entropy loss by adding the margins to the logits:
\begin{equation}\label{eq:prior}
    \mathcal{L}_\text{prior}(x) = - \sum_{c\in\mathcal{C}}y_c\cdot\log \frac{\exp(v_c - m_c)}{\sum_{c' \in \mathcal{C}}\exp(v_{c'} - m_{c'})}.
\end{equation}
where $y_c$ equals to 1 if $c$ is the ground-truth label, otherwise $y_c=0$. Note that our prior-margin loss reduce to vanilla cross-entropy loss if all margins $m_c$ are set as 0. As the margins are obtained based on prior distribution and fixed during training, we term this baseline as \fixstage{}. 

\subfile{../results/table_voc_all}
\subfile{../results/table_voc_summary}

Though the margin-based loss is calculated over all the proposals, precisely calculating the margins from the proposals is time-consuming. Thus, we obtain the prior margins over all annotated bounding box instances. In this case, there is a misalignment between proposal-based loss and instance-based margin. To mitigate this gap, we proposed to adaptively learn the margins based on the priors. Motivated by the success of self distillation~\cite{tian2020rethinking} in knowledge transfer, we use the detection module learned from $\mathcal{L}_\text{prior}$ in Eq.~\eqref{eq:prior} as teacher model, and distill its knowledge to a student model to adaptively learn and update the margins through soft labels, which has the same architecture as teacher model but different parameters. For sample $x$, the ground-truth label is $y$, the adaptive-margin distillation objective is:
\begin{equation}
    \mathcal{L}_\text{adapt}(x) = - \sum_{c \in \mathcal{C}} \frac{p_c^{t} + y_c}{2} \log \frac{\exp(v^s_c - m_c^{s})}{\sum_{c' \in \mathcal{C}}\exp(v^s_{c'} - m_{c'}^{s})},
\end{equation}
where the predicted probability for class $c$ of the teacher model $p^t_c$ is obtained by $\frac{\exp(v_c - m_c)}{\sum_{c' \in \mathcal{C}}\exp(v_{c'} - m_{c'})}$, $v^s_c$ denotes the logit output for class $c$ of the student model, and $m^s_c$ denotes the adaptive learnable margin for class $c$. The teacher model is fixed during self distillation, and the student detection head uses the same ETF classifier weights $W^{*}$ with other parts in the detection module to be learned. Finally, we use the student model for evaluation.

Even though the margins are added during training, the extreme imbalance between base set and novel set still makes the detector favors more on base set. Considering this limitation and the challenge that the number of novel classes is very limited to provide the gradients for network updating, we proposed to up-sample the images containing annotations of novel classes ($\mathcal{D}^+_n$).  Specifically, we use repeated factor sampling (RFS)~\cite{gupta2019lvis} and the repeating times is set by a hyper-parameter threshold in RFS. We experimentally found that using up-sampling itself can achieve marginal improvement, but can clearly improve the novel detection precision combined with our approach. This observation demonstrates that the up-sampling strategy works closely with our hypothesis rather than just a trivial trick.
