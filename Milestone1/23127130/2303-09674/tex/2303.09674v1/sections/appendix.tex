\section{Approach}


\subsection{Simplex ETF \& Neural Collapse}


The neural collapse (NC) phenomenon is revealed by \cite{papyan2020prevalence} in the fully-supervised learning, \ie, an extremely simple mathematical structure on the last-layer features. 
In particular, when the model is well-trained on a balanced dataset, the $N_f$ features $\{\mathbf{x}_{c,i}\}_{i=1}^{N_f}$ for class $c$ will converge to its class mean $\bar{\mathbf{x}}_c = \frac{1}{N_f} \sum_{i} \mathbf{x}_{c,i}$ where the class means $\{\bar{\mathbf{x}}_c\}_{c \in \mathcal{C}}$ together the class centers $\{\mathbf{w}_c\}_{c \in \mathcal{C}}$ will collapse to the simplex equiangular tight frame (Simplex ETF). Meanwhile, though the optimization objective of class mean and class centers (classifier weights) are not exactly the same, the class mean and class centers will still converge to each other.

\noindent \textbf{\emph{Simplex Equiangular Tight Frame}} denotes a collection of vectors $\mathbf{W}^* = \{\mathbf{w}_i'\}_{i=1}^{N_c} \in \mathcal{R}^{d \times N_c}$ that 
\begin{equation}\label{eq:definition}
    \mathbf{W}^* = \sqrt{\frac{{N_C}}{{N_C}-1}} \mathbf{U} \big( \mathbf{I}_{N_C} - \frac{1}{N_c} \mathbf{1}_{N_C} \mathbf{1}_{N_C}^T \big)
\end{equation}
where each vector $\mathbf{w}_i' \in \mathcal{R}^d$ and $\|\mathbf{w}_i'\|_2 = 1$ for $1 \leq i \leq N_C$, $\mathbf{I}_{N_C} \in \mathcal{R}^{N_C \times N_C}$ and $\mathbf{1}_{N_C} \in \mathcal{R}^{N_C}$ denote the identity matrix and all-ones vector respectively. The rotation matrix $\mathbf{U} \in \mathcal{R}^{d \times N_C}$ satisfies $\mathbf{U}^T \mathbf{U} = \mathbf{I}_{N_C}$ and $d \geq N_C-1$.
In this way, for all vectors in a simplex ETF, their pair-wise angles are identical, \ie,
\begin{equation}\label{eq:equiangular}
    \mathbf{w}_i'^T \mathbf{w}_j' = -\frac{1}{{N_C}-1}, \forall i,j \in [1,{N_C}]~~\text{and}~~i \neq j,
\end{equation}
where the angle $arc cos(-\frac{1}{{N_C}-1})$ shown in \cite{papyan2020prevalence} ans is the maximal equiangular angle of $N_C$ vectors in the feature space. 

\begin{algorithm*}[h]
    \caption{Iterative algorithm for obtaining Simplex ETF)}
    \begin{algorithmic}\label{Alg:ETF-iterative}
        \STATE \textbf{Input:} Number of classes $N_C$, feat. dim $d$ where $d \leq N_C-1$  maximum iterations $T$, stop threshold $\delta$, learning rate $\tau$.
        \STATE 1:~~Initialization: 
        \STATE ~~~~~~~~~Randomly initialize $W$ = np.random.normal(size=($N_C,d$)), 
        \STATE 2:~~for $t=1$ to $T$
        \STATE ~~~~~~~~~$l_2$-normalize the vector $\mathbf{w}$ in each row $W$ = normalize($W$),
        % \STATE ~~~~~~~~~calculate inner product $W_{IP} = W @ W.T$
        \STATE ~~~~~~~~~calculate pair-wise $l_2$ distance $W_{inner} \in \mathcal{R}^{N_C \times N_C}$ and set diagonal value as max infinity
        \STATE ~~~~~~~~~calculate minimum distance for each class (except for the diagonal value) $idx = np.argmin(W_{inner}, axis=1)$
        \STATE ~~~~~~~~~calculate objective funcdtion $obj = np.sum(W_{inner}[range(N_C), idx])$
        \STATE ~~~~~~~~~calculate gradient $grad = (W - W[nn_index,:]) * 2$ 
        \STATE ~~~~~~~~~update the weight $W' = W + grad * \tau$
        \STATE ~~~~~~~~~$l_2$-normalize the vector $\mathbf{w}'$ in each row $W'$ = normalize($W'$),
        \STATE ~~~~~~~~~update the weight $W' = W$,
        \STATE ~~~~~~~~~if $obj < \tau $:
        \STATE ~~~~~~~~~~~~~~~~~~Early stop, set T as t
        \STATE ~~~~~end for
        \STATE \textbf{Output: } $\mathcal{X}^{(T+1)}$ 
    \end{algorithmic}
\end{algorithm*}

Note that the equation in Eq.~\ref{eq:definition} is a \emph{closed-form} for obtaining an ETF but it is only used when $d \leq N_C$. When $d = N_C+1$, we can then use an \emph{iterative algorithm} to obtain the ETF. Specifically, we randomly initialize the values in $W$ and use the Eq.1 in the main paper to update the weight values. We provide the python-stype pseudo-code below. Note, since we want to maximize the objective function, we apply $+$ in the weight updating part.

\section{Experiment}

\subsection{Implementation Details}

The Faster-RCNN system we are using consists of a ResNet-101 feature backbone, a RPN network, and a detection module. The detection module is used to extract features for each region proposal, a linear classifier and a regression for localization. As mentioned in the main paper, 
since penultimate layer in the classification module is followed by a ReLU activation~\cite{goodfellow2017deep}, the proposal features are constrained to have non-negative entries and its distance to weights in $W^{*}$ are lower-bounded, and we thus add a linear layer (projector) on top of the extractor of proposal feature. Meanwhile, as highlighted in Sec. 5.3 in the main paper, we do not need to pretrained the detector on the base set, but directly training everything from scratch, however, we will still use the ImageNet-pretrained model to initialize the feature extractor.

The dimension of proposal feature in Faster-RCNN is $d=1024$ by default. As such, for experiments on MSCOCO and Pascao VOC, we set the projector with the same input and output dimension. However, for experiments on LVIS, since it has 1230 classes in v0.5 and 1203 classes in v1.0, we set the output dimension of projector as 1280.

During distillation, as we mainly focus on the learning of detector. As such, we fix the ResNet-101 feature backbone and the RPN network, and only distill the detection module. Also, during distillation, we do not apply any distillation strategy on the layer for localization. Then, we first use the fixed margin $-\log (p_c)$ in the loss and train the whole network. Then, during distillation, to fasten the training process, we can choose to also initialize the detector module with the pretrained teacher model. 

\paragraph{RFS implementation details.} We directly call the ``RepeatFactorTrainingSample'' as the training sampler function and send rfs parameter (0.01 for VOC \& COOC and 0.001 for LVIS) to the variable ``SAMPLER TRAIN''

\subsection{Full experiment on Pascao VOC}

We summarize the performance of novel detection in Table~\ref{tab:voc-novel}. 
Comparing with baseline TFA, over all 15 experiments on PASCAL VOC, the \fixstage{} baseline has already outperformed TFA by 3.6 gain in nAP$_{50}$ and 1.9 gain in bAP$_{50}$ on average. By performing the self-distillation to adjust margins for all classes $\mathcal{C}$ adaptively, our full approach \approach{} can further improve the detection score, \eg, comparable nAP$_{50}$ with MPSR~\cite{wu2020multi} but maintaining high \textit{base} detection precision (81.3 Vs. 68.1).
As reported in Table 2 in the main paper, comparing with Retentive RCNN~\cite{fan2021generalized}, a state-of-the-art (SOTA) approach in GFSOD, besides maintaining precise base detection, our approach also improves the novel detection score (43.9 Vs. 41.1). Meanwhile, the superior performance by Retentive RCNN on split 1 when $K$ is $\{1,2\}$ cannot be generalized to other splits. However, our approach achieves stable and consistent gain. Meanwhile, when more training data are provided, \ie, $K \geq 3$, the advantage of our \approach{} is better explored and achieve 3.76 nAP$_{50}$ gain on average.

\subfile{../results/table_voc_novel}

\begin{table*}[]
\centering
    \caption{Detailed Performance of MS COCO dataset.}
    \resizebox{\linewidth}{!}
    {\renewcommand{\arraystretch}{1.12}
    \begin{tabular}{l|cccccccc|cccccccc}
    \hlineB{3}
        \multirow{2}{*}{Approach} & \multicolumn{8}{c|}{10-shot} & \multicolumn{8}{c}{30-shot} \\
         & AP   & bAP  & nAP  & nAP$_{50}$ & nAP$_{75}$ & nAPs & nAPm & nAPl & AP   & bAP  & nAP  & nAP$_{50}$ & nAP$_{75}$ & nAPs & nAPm & nAPl \\ \hline
        \fixstage{} & 31.5 & 38.8 & 9.6  & 17.8  & 9.2   & 3.8  & 9.3  & 16.5 & 32.5 & 38.8 & 13.6 & 24.3 & 13.3 & 4.7 & 11.9 & 21.4 \\
        \approach{} & 32.0 & 39.2 & 10.3 & 18.7  & 9.9   & 4.5  & 10.0 & 16.8 & 33.1 & 39.4 & 14.2 & 26.2 & 14.8 & 5.3 & 13.1 & 23.9 \\
    \hlineB{3}
    \end{tabular}
    }\label{tab:coco-detail}
\end{table*}

\subsection{Long-Tail Object Detection}

\bitem{LVIS}~\cite{gupta2019lvis} is derived from COCO17~\cite{lin2014microsoft} and has two versions of annotations. The version v1.0 contains $\sim$1.3M training instances of 1203 classes while the version v0.5 has $\sim$0.7M training instances of 1230 classes. The one reported in the main paper is of v0.5. According to the number of training instances, the classes are divided into three groups, rare (1-10), common (11-100), and frequent ($>$100).
Following~\cite{wang2020frustratingly}, apart from the precision for all classes (AP) on the validation set, we also report the precision for each group, \ie, AP$_r$, AP$_c$, and AP$_f$.
Meanwhile, following a common setup, we try two different backbones ResNet50 and ResNet101.

Here we try two different baseline, TFA and ACSL. We do acknowledge other related research on LVIS such as EFL~\cite{li2022equalized} and LOCE~\cite{feng2021exploring}. However, these approaches are developed on Mask-RCNN framework, \ie, both object detection and object segmentation are trained. Since object segmentation introduces extra supervision signals, while our focus is main on object detection, we thus choose ACSL as the baseline. 

Comparing with ACSL, TFA also focus on object detection only but ACSL 1) applies a two step training strategy and 2) use the model pretrained on MSCOCO as initialization. In contrast, TFA only uses ImageNet-pretrained model to initialize the feature extractor. Meanwhile, it follow the configuration regarding learning rate and training epochs in the 1x Baseline but apply it on base training stage. As such, we consider both of these two setups. As such, we follow the training steps ACSL and use model pre-trained on MS COCO as initialization. From the Table ~\ref{tab:lvis-full}, \approach{} can achieve consistent gain on two cases. 


\begin{table}[!ht]
\centering
    \renewcommand\thetable{M2}
    \caption{Performance comparison of LVIS dataset (Full Table)}
    \resizebox{\linewidth}{!}
    {\renewcommand{\arraystretch}{1.0}
    \begin{tabular}{l|cccccccc}
    \hlineB{3}
        \multirow{2}{*}{Approach} & \multicolumn{4}{c}{ResNet-50} & \multicolumn{4}{c}{ResNet-101} \\
                    & AP    & APr   & APc   & Apf   & AP     & APr   & APc   & Apf   \\ \hline
        \multicolumn{9}{c}{V0.5}                                                     \\ \hline
        1x Baseline & 22.7  & 10.6  & 22.0  & 28.0  & 24.5   & 13.1  & 23.9   & 30.0 \\ 
        TFA w/ fc~\cite{wang2020frustratingly} & 24.1  & 14.9  & 23.9  & 27.9  & -      & -     & -     & -     \\
        TFA w/ cos~\cite{wang2020frustratingly}  & 24.4  & 16.9  & 24.3  & 27.7  & -      & -     & -     & -     \\
        \approach{}        & 24.9  & 17.3  & 24.6  & 28.5  & 26.8   & 18.5  & 26.8  & 30.1  \\
        RFS~\cite{gupta2019lvis}         & 24.9  & 14.4  & 24.5  & 29.5  & -      & -     & -     & -     \\
        Focal Loss~\cite{lin2017focal}  & 22.0  & 10.5  & 22.4  & 25.9  & -      & -     & -     & -     \\
        EQL~\cite{tan2020equalization}         & 25.1  & 11.9  & 26.0  & 29.1  & 26.1   & 11.5  & 27.1  & 30.5  \\
        BAGS~\cite{li2020overcoming}        & 26.0  & 17.7  & 25.8  & 29.5  & 26.4   & 16.8  & 25.8  & 30.9  \\
        ACSL~\cite{wang2021adaptive}        & 26.4  & 18.6  & 26.4  & 29.4  & 27.5   & 19.3  & 27.6  & 30.7  \\
        \approach{} & 26.7  & 18.9  & 27.0  & 29.0  & 27.9   & 19.5  & 28.0  & 31.0  \\ \hline 
        \multicolumn{9}{c}{V1.0}                                                     \\ \hline
        1x Baseline & 19.3  & 6.4  & 17.1   & 27.6  & 21.1   & 10.1  & 21.7  & 25.8  \\
        \approach{} & 22.5  & 12.4 & 20.6   & 26.8  & 24.4   & 16.6  & 22.8  & 28.0  \\
    \hlineB{3}
    \multicolumn{9}{c}{The configuration of 1x Baseline can be found in the TFA official repo.}
    \end{tabular}
    }\label{tab:lvis-full}
\end{table}



\section{Discussion}

\subsection{Decoupling localization from classification.}
Consistent with the observation in~\cite{papyan2020prevalence}, by enhancing inter-class separation and intra-class compactness, the detection scores are improved. However, the features for localization should still be class-independent (\eg, bus and elephant has similar shape). 
% the input features of classifier are converged to the class mean while features of different classes are maximally separated, while 
From the implementation details, a projector is set where its input \& output are used for localization \& classification separately.
Then, sharing the features for localization and classification will lead to slight performance drop (\ie, AP$_{50}$ 74.0, nAP$_{50}$ 55.6).
As such, it is important to decouple the features for localization and classification and employing a simple linear projector has been shown to be userful.


\subsection{Design of Background class}

An object detector should reject the background and not recognize it as any foreground object. As such, a background class $\negcls{}$ is set as a placeholder and is trained to have high similarity with background proposals. Different from foreground objects, as background proposals can be diverse, we considered different strategies in designing the background class center. 

We first choose to separate the design of $W_b \cup W_n$ and $\mathbf{w}_{\negcls{}}$, \ie, deriving fixed offline weights for $\basecls{} \cup \novelcls{}$ only but learn the weight $\mathbf{w}_{\negcls{}}$. Then, we follow the open-set strategy~\cite{zhou2021learning} to set multiple background centers $W_-=\{\mathbf{w}_{\negcls{}}^{(i)}\}_{i=1}^{N_-}$ where $N_-$ is the number of background centers where the maximum logit, \ie, $\text{max}_{1\leq i \leq N_-}(\mathbf{x}^T \mathbf{w}_{\negcls{}}^{(i)})$, is used in classification. As compared in Table~\ref{tab:background}, having more learnable class centers can introduce trivial performance improvement but will drop clearly when $N_-$ is too large. However, when we directly set the classifier for the all classes ,\ie, $W_n \cup W_b \cup W_-$ as ETF , the performance drops when $N_- > 1$.

In practice, we observe all learnable negative weights $W_-$ are trained to separate from the $W_b \cup W_n$ where the weights in $W_-$ are still close to each other such that the diversity of background features are preserved indirectly. Instead, having all negative weights maximally separated from each other assume background features is very diverse and make the model hard to learn. As such, we choose to set $N_-=1$ and adjust margins through self-distillation to maintain the diversity properly.


\begin{table}[]
    \centering
    \renewcommand\thetable{M3}
    \caption{Ablation study of Background Design.}
    % \resizebox{\linewidth}{!}
    {
        \begin{tabular}{ccc|cccc}
        \hlineB{3}
            Idx & $N_{-}$ & Fixed & AP$_{50}$   & bAP$_{50}$  & nAP$_{50}$ \\ \hline
            1 & 1 & \checkmark{}  & 74.9 & 81.0  & 56.4  \\
            2 & 5 & \checkmark{}  & 73.5 & 81.3  & 50.2  \\
            3 & 1 &  & 74.9 & 81.0  & 56.4  \\
            4 & 5 &  & 74.9 & 80.9  & 56.7  \\
            5 & 10&  & 74.9 & 80.9  & 57.0  \\
            6 & 20&  & 74.6 & 81.1  & 55.2  \\
        \hlineB{3}
        \end{tabular}
    }\label{tab:background}
\end{table}

\subsection{More Visualization}

As shown in Fig.~\ref{fig:supp-fig}, we visualize the classifier centers by their pair-wise cosine similarity when they are learned from scratch. Fig.~\ref{fig:supp-fig}(a) is the same as the Fig. 3(b) in the main paper but the background class center is also included (the rightmost and the bottom one). We can then see that when we have both base and novel annotation in the train set, the class centers can be trained to distance from all of the background classes. However, when we only use novel classes during the adaptation stage (Fig.~\ref{fig:supp-fig}(b)), the negative class center can be close to the novel class centers. Meanwhile, when we use the full set for training from scratch, we can see that the applying either RFS or adding margins can help with separating the novel class centers from the background class centers, while adding margins is more important.

The foreground class names (sorted by decreasing order) are person, chair, car, bottle, dog, potted plant, cat, boat, sheep, aeroplane, bicycle, tv monitor, horse, dining table, train, motorbike, cow, bus, bird, sofa.

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{results/supp-vis.pdf}
    \renewcommand\thefigure{M1}
    \caption{Visualization classifier centers.}
    \label{fig:supp-fig}
\end{figure*}
\newpage


\section{Comparison with related work}

In this section, we provide in-detail comparison with a few representation works to highlight our motivation and contribution. All of the approached listed below have been briefly mentioned in the sections of Related Work and Experiment. 


% Beyond Max-Margin: Class Margin Equilibrium for Few-shot Object Detection (CME)
\paragraph{CME}~\cite{li2021beyond} similarly employs a margin equilibrium strategy in the few-shot finetuning. The determination of the margin value is based on the degree of feature disturbance which is measured by the scale of gradient among augmented samples.
Meanwhile, CME is motivated by the trade off between margins of base classes and the variance of novel classes. 

However, we have used the geometric property of Simplex ETF to maximally separate the feature clusters for all classes. In this way, we decouple the learning for inter-class separation and intra-class compactness and only tighten the feature cluster to the corresponding class centers to reach a balanced distribution. As such, we can learn discriminative features for all of the classes even on an extremely imbalanced dataset.

In addition, CME is still trained on the balanced dataset $\basesubset{} \cup \novelset{}$ and the so-called margin equilibrium is realized when the model is trained on a balanced set. Thus, CME may still forget the base knowledge. 
Instead, our margins are for all classes based on the prior of instance distribution and our approach is orthogonal to CME. Furthermore, the margin estimation strategy in CME can be used as an alternative of our self-distillation in margin adjustment.


% Negative margin matters: 
\paragraph{Negative margin on few-shot classification} is studied in \cite{liu2020negative} and reveals the trade-off of classification accuracy between base recognition and novel  recognition. Namely, for a feature extractor pre-trained on \textit{base} classes, if the model achieves better test accuracy on the base classification, the adaptation accuracy towards classification accuracy is then minimized. As such, a comprehensive study is provided in \cite{liu2020negative}.

In contrast, we focus on few-shot object detection and aims to improve the few-shot adaptation efficiency without scarifying the performance of base detection. We always add positive class-specific margins to all classes where the margin values are adaptively learned during network training. 


% Exploring Classification Equilibrium in Long-Tailed Object Detection
\paragraph{LOCE~\cite{feng2021exploring}} is applied on long-tail object detection, which is a more general case of generalized few-shot object detection (\ie, in GFSOD, the imbalance between base set and novel set is more significant and thus more challenging). A common problems discussed in LOCE and our paper is that the instance distribution of classes cannot be directly used to estimate the margins. 

As such, LOCE discards the prior and introduce the Equilibrium loss to use the mean classification score to determine the margin. In addition, they proposed a complex memory-augmented feature sampling to facilitate the network training.
In contrast, we clearly discuss and decouple the training objective for inter-class separation and intra-class compactness. 

We consider the distribution of classifier weights in conventional training and use ETF as a fixed classifier. In this case, we used the assigned weights to guide the separation of feature clusters between different classes, and then apply different margins to push the features to the assigned centers. As we apply margins to facilitate the balanced distribution, we can use the instance distribution as prior and use a simple knowledge distillation to adjust the margins and facilitate training. 


\paragraph{Margin modification techniques} such as BALMS~\cite{ren2020balanced} and Seesaw loss\cite{wang2021seesaw} has been proposed. Specifically, BALMS considers the boundary shifting problem in long-tailed classification/segmentation and present a meta-sampling strategy to re-estiamte the boundary indicated in the Softmax function. Seasaw loss defines a compensation factor in vanilla cross entropy loss to balance the error for different classes. In both case, they in effect count on the real-time (online) distribution of selected samples during the training and then adjust the loss. Instead, we focus on the inter-class separation and intra-class compactness to guide the training of features, \ie, re-arranging the feature distribution from the perspective of feature geometry. In addition, the margin modification techniques can be used as an alternative of our margin adjustment strategy for the intra-class compactness only. 


% data indepedent
% geometric equivalence
% differentive from  - single model, ensemble tricks 
% target & acompolishment

\paragraph{Connection with FSCE} In FSCE~\cite{sun2021fsce}, the authors has provided a strong baseline by adjusting the hyper-parameters in RPN and proposal selection. We have tried to apply it in our framework but the performance drops. As such, we still follow the hyper-paramter setting in TFA. Meanwhile, it also demonstrates that the observation in FSCE is only available in the two-step based training strategy such as TFA, and cannot be generalized to a universal case.

Furthermore, FSCE proposed a contrastive encoding approach and treats the proposals as augmentation of the same instance. However, we have also add the contrastive loss in our approach and observed that it may help improve the nove detection slightly but hurt the base detection significantly. We think the reason is that the data distribution is extremely imbalanced and and the contrastive loss cannot help. 

