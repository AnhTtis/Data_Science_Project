\begin{figure*}[]
    \centering
    \includegraphics[width=0.9\linewidth]{results/step-ablation-4.pdf}
    \caption{Comparison of pair-wise similarities of classifier weights. We follow the TFA~\cite{wang2020frustratingly} baseline but alter the train set at adaptation step between (a) full set $\baseset{} \cup \novelset{}$, (b) a balanced set $\basesubset{} \cup \novelset{}$, or (c) \textit{novel} set $\novelset{}$. For our approach, to maximally separate the classifier weights, we can (d) apply Eq.~\eqref{eq:etf} as a regularization term during training (online) or (e) derive the classifier weights offline and fix them. The weights for all classes are arranged w.r.t a decreasing order of the number of annotations per class, \ie, from top (left) to bottom (right), from many-shot to few-shot.}
    \label{fig:step-center-ablation}
\end{figure*}

\subsection{Analysis of Inter-Class Separation}\label{sec:strategy-generalization}



\noindent \textbf{Revisit the conventional adaptation strategy from the perspective of separation between classes.}
Recall that existing few-shot object detection methods follow in TFA~\cite{wang2020frustratingly} and employs a two-step strategy, \ie, first pre-train on the base train set $\baseset{}$ to learn a \textit{Base} detector, and then fine-tune on the union of the downsampled base set and the novel set, \ie, $\basesubset{} \cup \novelset{}$. We take TFA~\cite{wang2020frustratingly} as the baseline and consider the following settings for the second step: (1) full set $\baseset{} \cup \novelset{}$, (2) balanced set $\basesubset{} \cup \novelset{}$, (3) only novel set  $\novelset{}$.

As shown in Fig.~\ref{fig:step-center-ablation}, we visualize the separation of classifier weights based on their pair-wise cosine similarities.
By comparing Fig.~\ref{fig:step-center-ablation}(a-c), fine-tuning among a balanced set is vital to learn the well-separated classifier weights for all classes $\basecls{} \cup \novelcls{}$.
Instead, using the full set would make the novel classes entangled in Fig.~\ref{fig:step-center-ablation}(a) due to the extremely imbalanced class distribution (\ie, $|\novelset{}| \ll |\baseset{}|$). Although only using $\novelset{}$ can maximally separate the weights of $W_n$, as no training data of $\basecls{}$ is seen, the separation between \textit{base} weights in $W_b$ is hurt and each \textit{novel} class center $\mathbf{w}_i \in W_n$ may still close to some \textit{base} weight, \eg, the similarity between classes ``cat'' and ``cow'' is relatively high in Fig.~\ref{fig:step-center-ablation}(c).

As summarized in Table~\ref{tab:step}, the novel detection fails when the full set is used in fine-tuning (\tablerow{1}). The detection precision on both $\basecls{} \cup \novelcls{}$ is sub-optimal when no annotation of base class is provided (\tablerow{3}) and the detector can easily overfit to the small $\novelset{}$. Then, finetuning on the balanced set (\tablerow{2}) can preserve the base knowledge, maximize the few-shot adaptation effect, and achieve the highest score among the three settings. However, such a balanced set has discarded the diverse training samples of $\basecls{}$ and the performance drop in base detection is inevitable.


\noindent \textbf{Training on the union of whole base set and upsampled novel set.}
In contrast, we propose to train from $\baseset{} \cup \novelsubset{}$ directly. Note that $\novelsubset{}$ is a duplication of $\novelset{}$ with same images but more copies. To properly separate the features of different classes, we use the data-independent optimization target in Eq.~\ref{eq:etf} to derive a ETF classifier weights $W^*$ offline. 
% 
As mentioned in Sec.~\ref{sec:inter}, Eq.~\ref{eq:etf} can still be used as a regularization loss to supervised the learning of the last linear layer during training (online). However, as shown in Fig.~\ref{fig:step-center-ablation}(d), it is still hard to get a perfect ETF classifier shown in Fig.~\ref{fig:step-center-ablation}(e). After all, the update of classifier weights is also impacted by the weight decay regularization and classification loss, and the learning of weights is not stable, in particular, on an extremely imbalanced dataset. 
As the classifier weights are kept being updated, the optimization direction of each feature cluster is not stable, which then impede the adaptation efficiency. As compared in Table.~\ref{tab:online}, the performance by online optimization is slightly worse, in particular when $K=1$.
Though the classifier weights are fixed in ETF, as the pair-wise angles between weights are the same, we can equivalently assign the weights to all classes $\mathcal{C}$.

\subfile{../results/table_step}
\begin{table}[]
\centering
    \caption{Comparison between Offline and Online Classifiers}
    \resizebox{\linewidth}{!}
    {\renewcommand{\arraystretch}{1.0}
    \begin{tabular}{l|ccc|ccc}
    \hlineB{3}
        \multirow{2}{*}{Approach} & \multicolumn{3}{c|}{1-shot} & \multicolumn{3}{c}{5-shot} \\
         & AP$_{50}$ & bAP$_{50}$ & nAP$_{50}$ & AP$_{50}$ & bAP$_{50}$ & nAP$_{50}$ \\ \hline
        Offline & \textbf{68.9} & 79.9 & \textbf{35.8} & \textbf{74.9} & \textbf{81.0} & \textbf{56.4} \\
        Online & 68.4 & \textbf{80.2} & 33.1 & 74.6 & 80.8 & 55.9 \\
    \hlineB{3}
    \end{tabular}
    }\label{tab:online}
\end{table}

Next, as compared in Table~\ref{tab:ablation} \tablerow{1,3,4}, though adding margins or performing RFS may help with inter-class separation and improve nAP$_{50}$ on $\novelcls{}$, since the weights $W_n$ are still not well-learned due to the extreme imbalance between $\baseset{}$ and $\novelset{}$, the performance gain is limited. In contrast, fixing the weights as ETF (\tablerow{(3,5),(4,6)}) can improve the novel detection, in particular, the nAP$_{50}$ is boosted from 12.2 to 35.8 in \tablerow{3,5}, which shows that the inter-class separation is essential for distinguishing objects in GFSOD.

\subfile{../results/table_ablation}
\begin{table}[!ht]
    \centering
    \caption{Comparison of Initialization.}
    \resizebox{\linewidth}{!}
    {
        \begin{tabular}{ll|cccccc}
        \hlineB{3}
            \multirow{2}{*}{Init.} & \multirow{2}{*}{Method} & \multicolumn{3}{c}{VOC 5-shot} & \multicolumn{3}{c}{COCO 10-shot} \\
              &  & AP$_{50}$   & bAP$_{50}$  & nAP$_{50}$  & AP   & bAP  & nAP  \\ \hline
            Base & \fixstage{}  & 74.9 & 80.9 & 56.7 & 31.7 & 39.0 & 9.7\\
            Base & DeFRCN~\cite{qiao2021defrcn} & 74.1 & 77.1 & 65.1 & 30.1 & 34.4 & 17.3\\
            \fixstage{} & DeFRCN~\cite{qiao2021defrcn}  & 74.8 & 78.0 & 65.3 & 30.7 & 35.1 & 17.4 \\
        \hlineB{3}
        \end{tabular}
    }\label{tab:initialization}
\end{table}



Furthermore, being orthogonal to the previous FSOD approaches, our model can be intuitively used as initialization for their adaptation. For the sake of simplicity, we only consider \fixstage{} and use a strong baseline DeFRCN~\cite{qiao2021defrcn} for comparison. The PCB calibration~\cite{qiao2021defrcn} is removed to better demonstrate the effect of \fixstage{}. As reported in Table~\ref{tab:initialization}, though DeFRCN has improved \textit{novel} detection (nAP$_{50}$) significantly, it still sacrifices the performance on base set. Then, comparing with using \textit{Base} detector as initialization, on both datasets, using our \fixstage{} can both help with the adaptation on $\novelcls{}$ and mitigate the drop in $\basecls{}$ (bAP$_{50}$).
Finally, comparing Table~\ref{tab:initialization} \tablerow{1} and \fixstage{} in Table~~\ref{tab:ablation}, adding the step of \textit{Base} detector initialization can only provide marginal improvement for \fixstage{}. As \fixstage{} has already outperformed TFA, we skip pre-training step for simplicity.

\subsection{Analysis of Intra-Class Compactness}\label{sec:discussion}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{results/step-ablation.pdf}
    \caption{Pair-wise cosine similarity between class means and weights. The class mean is obtained by averaging of features for each class. When the detector is trained on full set $\baseset{} \cup \novelset{}$, we first (a) replace the linear classifier with ETF and (b) then use our \fixstage{} baseline. Our \fixstage{} can push the features close to the assigned weights effectively.}
    \label{fig:step-center-mean}
\end{figure}

Even though the classifier weights have been maximally and equally separated in Fig.~\ref{fig:step-center-ablation}(e), as the training data is limited, it is still necessary to effectively push the features towards the assigned weight. As compared in Fig.~\ref{fig:step-center-mean}, when ETF is used, for each $c \in \novelcls{}$, as $|\novelset{}| \ll |\baseset{}|$, the mean of the its features is still distant from the assigned weights. However, our \fixstage{} baseline clearly push the features to the assigned weights to facilitate the novel detection.
Similarly, in Table~\ref{tab:ablation}, only using the ETF classifier can introduce limited gain (\tablerow{1,2}). Though the ETF classifier with dot-regression loss has been used for long-tail classification~\cite{yang2023neural,yang2022we}, we note the efficiency in dealing with hugely imbalanced datasets is limited. By adding margins to tighten each cluster and/or up-sampling novel instances in RFS to ensure that sufficient features of $\novelcls{}$ are used for training, the nAP$_{50}$ can then be improved (\tablerow{2,5,6,7}).


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{results/stat-compare.pdf}
    \caption{With a detector trained on the $\baseset{} \cup \novelset{}$ with vanilla $\mathcal{L}_{CE}$. For each class, we calculate the mean of a) the number of proposals per instance and b) classification confidence score where the number of annotations per class is plotted for reference.}
    \label{fig:stat}
\end{figure}

\bitem{Obtaining effective margins} is essential to train on an extremely imbalanced dataset. As discussed in \cite{menon2020long}, the margins to be added should meet conditions such as \textit{Fisher consistency}~\cite{lin2004note,bartlett2006convexity} to balance the error among different classes.
As $|\novelset{}| \ll |\baseset{}|$, directly learning the margins for each class individually from scratch (Table~\ref{tab:margin} \tablerow{1}) is difficult and may suffer from training instability such as gradient explosion. By sharing margins for classes in the same group, \ie, $\basecls{}$, $\novelcls{}$, and $\negcls{}$, the nAP$_{50}$ can be improved slightly. 



As summarized in Fig.~\ref{fig:stat}, for each class, the number of proposals used to train the detection module ranges from 11 to 17 per instance on average. As such, using the prior of instance distribution $\{p_c\}_{c \in \basecls{} \cup \novelcls{}}$ can help estimate good margins (\fixstage{}). However, as the number of proposals for $\novelcls{}$ (11$\sim$14) is still slightly less than that of $\basecls{}$ (13$\sim$17) and the margin $m_{\negcls{}}$ for $\negcls{}$ is roughly estimated, it is still necessary to learn margins adaptively. As no stronger prior knowledge can be used, directly learning the margins initialized by $\{-\log(p_c)\}$ does not help clearly (\fixstage{}+). However, through self-distillation, the logits output by pretrained \fixstage{} baseline model can be used to indicate the relationship between the proposals features and all class centers, which is then used as supervision signal in our \approach{}.




\subsection{Extension to Long-tailed Object Detection}

\subfile{../results/table_rfs}
\subfile{../results/table_lvis}

As compared in Table~\ref{tab:lvis}, we use TFA~\cite{wang2020frustratingly} and ACSL~\cite{wang2021adaptive} as two baselines. By employing our design, our \approach{} can achieve higher detection precision on both cases. For comparison with ACSL, we follow the training procedure in ACSL and our approach can benefit from the prior of data distribution to learn discriminative features.
More detailed explanation \& results can be found in Supp.

