%%%% 2-RelatedWork.tex starts here %%%%

\section{Related Work}

\paragraph{Image style transfer.}
Traditional style transfer methods such as stroke-based rendering~\cite{Fivser:2016:Stylit} and image filtering~\cite{Wang:2004:EEP} typically use low-level hand-crafted features.
Gatys et al.~\shortcite{Gatys:2016:IST} and the follow-up variants \cite{Gatys:2017:CPF,Kolkin:2019:STR} demonstrate that the statistical distribution of features extracted from pre-trained deep convolutional neural networks can capture style patterns effectively.
Although the results are remarkable, these methods formulate the task as a complex optimization problem, which leads to high computational cost.
Some recent approaches rely on a learnable neural network to match the statistical information in feature space for efficiency.
Per-Style-Per-Model methods~\cite{johnson2016perceptual,gao2020fast, puy2019flexible,Kwon:2022:clipstyler} train a specific network for each individual style.
Multiple-Style-Per-Model methods~\cite{chen2017stylebank,zhang2018multi,dumoulin2016learned, ulyanov2016texture} represent multiple styles using one single model.

Arbitrary style transfer methods~\cite{Liao:2017:VAT,Li:2017:UST,deng2020arbitrary,svoboda2020two,Wu:2021:SF,deng2021stytr2,zhang2020cast} build more flexible feed-forward architectures to handle an arbitrary style using a unified model.
AdaIN~\cite{Huang:2017:AdaIn} and DIN~\cite{jing2020dynamic} directly align the overall statistics of content features with the statistics of style features and adopt conditional instance normalization.
However, dynamic generation of affine parameters in the instance normalization layer may cause distortion artifacts.
Instead, several methods follow the encoder-decoder manner, where feature transformation and/or fusion is introduced into an auto-encoder-based framework. 
For instance, Li et al.~\shortcite{Li:2019:LLT} achieve universal style transfer by developing a cross-domain feature linear transformation matrix (LST) and decoding from the transformed features.
Park et al.~\shortcite{Park:2019:AST} provide a flexible mapping of the semantically nearest style features onto the content features by SANet.
Deng et al.~\shortcite{Deng:2021:MCC} propose MCCNet for efficient video style transfer by fusing input content features and style features via multi-channel correlation. 
Liu et al.~\shortcite{liu2021adaattn} present an adaptive attention normalization module (AdaAttN) to consider both shallow and deep features for attention score calculation.
GAN-based methods~\cite{Zhu:2017:CycleGAN,svoboda2020two, kotovenko2019content_transformation, kotovenko2019content, sanakoyeu2018style} have been successfully used in collection style transfer, which considers style images in a collection as a domain~\cite{chen2021dualast, xu2021drb,Lin:2021:DAM}.
An et al.~\shortcite{An:2021:Artflow} propose reversible neural flows and an unbiased feature transfer module (ArtFlow) to prevent content leak during universal style transfer.
Inspired by the breakthrough of visual transformer (ViT), many researchers have developed ViT for style transfer tasks.
Wu el al.~\shortcite{Wu:2021:SF} propose a feed-forward style transfer method (StyleFormer) which includes a transformer-driven style composition module.
Deng et al.~\shortcite{deng2021stytr2} propose a ViT-based style transfer method (StyTr$^2$) which takes long-range dependencies of input images into account to avoid the biased content representation.
Zhang et al.~\shortcite{Zhang:2022:EFD} performed exact matching of feature distributions and applied this method to arbitrary style transfer.

\paragraph{Contrastive learning.}
Contrastive learning has been used in many applications, such as image dehazing~\cite{Wu:2021:CLC}, context prediction~\cite{Cruz:2019:VPL}, geometric prediction~\cite{Liu:2019:EUD} and image translation.
Contrastive learning is introduced in image translation to preserve the content of the input~\cite{Han:2021:DCL} and reduce mode collapse~\cite{Liu:2021:DivCo,Jeong:2021:Contrad,Kang:2020:ContraGAN}.
CUT~\cite{park2020CUT} proposes patch-wise contrastive learning by cropping input and output images into patches and maximizing the mutual information between patches.
Following CUT, TUNIT~\cite{baek2021tunit} adopts contrastive learning on images with similar semantic structures.
However, the semantic similarity assumption does not hold for arbitrary style transfer tasks, which leads the learned style representations to a significant performance drop.
IEST~\cite{chen2021artistic} applies contrastive learning to image style transfer based on feature statistics (mean and standard deviation) as style priors.
The contrastive loss is calculated only within the generated results.
Contrastive learning in IEST is an auxiliary method to associate stylized images sharing the same style, and the ability comes from the feature statistics from pre-trained VGG.
Differently, we introduce contrastive learning for style representation by proposing a novel framework that uses visual features comprehensively to represent style for the task of arbitrary image style transfer.

Temperature is a critical parameter for the success of a contrastive learning based method.
Wang and Liu~\shortcite{wang2021understanding} showed that the contrastive loss has a hardness-aware property, which makes contrastive learning naturally focusing on difficult negative samples.
Such hardness awareness helps to learn separable and uniformly distributed features but also leads to low tolerance of semantically similar samples.
The extent of penalties on hard negative samples is determined by the temperature $\tau$.
In particular, as the temperature decreases, the relative penalty tends to concentrates more on the high similarity region, whereas as the temperature increases, the relative penalty distribution becomes more uniform, which means that all negative samples are penalized equally.
They built relations between uniformity, tolerance, and temperature.
Zhang et al.~\shortcite{zhang2022does} introduced vector decomposition for analyzing the collapse issue based on gradient analysis of the $l_2$-normalized representation vector and proposed a unified perspective on how negative samples and simple Siamese method alleviate collapse.
Caron et al.~\shortcite{caron2021emerging} investigated dual temperature from the perspective of knowledge distillation and proposed a simple self-supervised method, in which the teacher adopts a lower temperature than the student to help in knowledge distillation.
Zhang et al.~\shortcite{zhang2021temperature} learn temperature as an input-dependent variable.
They consider temperature as a measure of embedding confidence and propose temperature as uncertainty.
Zhang et al.~\shortcite{zhang2022dual} adopt dual temperature in a contrastive InfoNCE for realizing independent control of two hardness-aware sensitiveness.
Previous temperature analysis works mainly focus on the penalty's unevenness of negative samples within an anchor or the sum of penalties of different anchors within a training batch.
Contrarily, we consider the proportion of penalties between the positive sample and negative samples at the same time.
%%%% 2-RelatedWork.tex ends here %%%%
