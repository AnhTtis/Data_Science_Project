%%%% 1-Introduction.tex starts here %%%%

\section{Introduction}
\label{sec:intro}


%%%% Figs/fig_IECAST_compare.tex starts here %%%%
\input{Figs/fig_motivation}
%%%% Figs/fig_IECAST_compare.tex ends here %%%%

If a picture is worth a thousand words, then an artwork may tell the whole story.
The art style depicts the visual appearance of an artwork and characterizes how the artist expresses a theme and shows his/her creativity.
The features that identify an artwork, such as the artist's use of stroke, color and composition, determine the style.
Artistic style transfer, as an efficient way to create a new painting by combining the content of a natural images and the style of an existing painting image, is a major  research topic in computer graphics and computer vision~\cite{Jing:2020:NSTReview}.

The main challenges of arbitrary style transfer are extracting styles from artistic images and mapping a specific realistic image into an artistic one in a controllable way.
The core problem for style extraction is to find an effective representation of styles since it is in general hard to give explicit definitions across different styles. 
To build a reasonable style feature space, it is necessary to explore the relationship and distribution of styles in order to capture both individual and holistic characteristics.
For the mapping process, several generative mechanisms are adopted to address different issues, such as auto-encoder ~\cite{Huang:2017:AdaIn, liu2021adaattn}, neural flow model~\cite{An:2021:Artflow} and visual transformer~\cite{deng2021stytr2}.
In contrast to the goal of those methods, we propose to improve arbitrary style transfer via a unified framework that offers the guidance of proper artistic style representation and works for various generative backbones.


Since Gatys et al.~\shortcite{Gatys:2016:IST} proposed to use Gram matrix as artistic style representation, high-quality visual results are generated by advanced neural style transfer networks.
Despite the remarkable progress made in the field of arbitrary image style transfer, the second-order feature statistics (Gram matrix or mean/variance) style representation has restricted the further development and application.
As shown in Fig.~\ref{fig:teaser}, the appearances of different artwork styles vary considerably in terms of not only the colors and local textures but also the layouts and compositions.
Figs.~\ref{fig:motivation_adattn}-\ref{fig:motivation_stytr2} show the results of three recently proposed state-of-the-art style transfer approaches.
We obverse that aligning the distributions of neural activation between images using second-order statistics results in difficulty to capture the color distribution or the special layouts, or imitate specific detailed brush effects of different styles.

In this paper, we revisit the core problem for neural style transfer, that is, the proper artistic style representation.
The widely used second-order statistics as a global style descriptor can distinguish styles to some extent, but they are not the optimal way to represent styles.
By second-order statistics, arbitrary stylization formulates styles through artificially designed image features and loss functions in a heuristic manner.
In other words, the network learns to fit the second-order statistics of the style image and generated image, instead of the style itself.
Our key insight is that a person without artistic knowledge has difficulty defining the style if only one artistic image is given, but identifying the difference between different styles is relatively easy.
Therefore, exploring the relationship and distribution of styles directly from artistic images instead of using pre-defined style representations is worthwhile. 

We propose to improve arbitrary style transfer with a novel style representation based on contrastive learning.
Specifically, we present a \textbf{U}nified \textbf{C}ontrastive \textbf{A}rbitrary \textbf{S}tyle \textbf{T}ransfer (\textbf{UCAST}) framework for image style representation learning and style transfer, which consists of a generative backbone, a parallel contrastive learning scheme, and a domain enhancement (\textbf{DE}) module.
We introduce contrastive learning to consider the positive and negative relationships between different styles, and we use DE to learn the overall domain distribution of artistic images.
UCAST can be plug-and-play for most arbitrary image style transfer methods to improve their performance.

Since different images may share similar styles, it is necessary to consider similar styles in the style modeling process and the style contrastive learning should be tolerant to highly similar samples.
Besides, compared with Per-Style-Per-Model methods and Multiple-Style-Per-Model tasks, arbitrary image style transfer has the difficulty that when dealing with specific content-style pairs, the content image and style image may not always be compatible with each other.
For instance, when using a realistic image with a large smooth area as the content and an artistic image with rich texture as the style, we may find undesirable artifacts in the stylization output.
Based on these observations, we propose an adaptive contrastive loss that is implemented with a novel dual input-dependent temperature scheme.
Our adaptive contrastive loss takes into account the similarities between the target style image and other artistic images, as well as the similarities between the target style image and the input content image to address the above problems.


Our contribution can be summarized as follows:
\begin{itemize}[leftmargin=*,topsep=1pt]
\setlength\itemsep{0pt}
\item We propose a novel framework called Unified Contrastive Arbitrary Style Transfer (UCAST), which can easily integrate various types of style transfer backbones and lead to improved visual quality in the stylization results.

\item We propose a novel style representation learning method via contrastive learning without employing the commonly used second-order statistics of image features.
We introduce contrastive learning and domain enhancement by considering the relationships between styles as well as the global distribution of styles, which solves the problem that existing style transfer models cannot effectively leverage a large amount of available artistic images. 

\item We propose adaptive contrastive learning for arbitrary style transfer tasks, which allows the model to be tolerant to similar styles and improve the robustness of various content-style inputs.

\end{itemize}



%%%% Figs/fig_framework.tex starts here %%%%

\begin{figure*}%[t]
\centering
\includegraphics[width=0.99\linewidth]{Image/model/UCAST}
%\vspace{-6pt}
\caption{UCAST consists of an generator $\generator$, a parallel contrastive learning scheme relying on a multi-layer style projector (MSP) module, and a domain enhancement module.
The generator is given the content image $\inputcontent$ and the style image $\inputstyle$ and generates images $\image_{cs}$ and $\image_{sc}$.
Then, $\image_{cs}$ and $\inputstyle$ are fed into the MSP module to generate the corresponding style code $\outputlatent$ and $\stylelatent$, which will be used as positive samples in the style contrastive learning process. 
The style codes $\latentcode^{-}$ of other artistic images in the style bank will be used as negative samples.
$\inputcontent$ is fed into the MSP module to generate the corresponding style code $\latentcode^c$.
We design an adaptive temperature module that computes the temperature $\positivetemperature$ of the positive sample and the temperature $\negativetemperature$ of the negative samples on the style codes.
The contrastive style loss $\loss_{contra}^{G}$ is computed on the temperatures and the style codes.
DE module is based on the adversarial loss $\loss_{adv}$ and the cycle consistency loss $\loss_{cyc}$.
}
%\vskip -2.5mm
\label{fig:framework}
\end{figure*}

%%%% Figs/fig_framework.tex ends here %%%%

%%%% 1-Introduction.tex ends here %%%%