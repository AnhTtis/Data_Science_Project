%%%% Tables/tab_quantitative.tex starts here %%%%

\input{Tables/tab_quantitative}

%%%% Tables/tab_quantitative.tex ends here %%%%

%%%% Figs/fig_backbone.tex starts here %%%%

\input{Figs/fig_backbone}

%%%% Figs/fig_backbone.tex ends here %%%%

%%%% 4-Experiments.tex starts here %%%%
\section{Experiments}

We compare UCAST with several state-of-the-art style transfer methods, including AdaIN~\cite{Huang:2017:AdaIn}, ArtFlow~\cite{An:2021:Artflow}, MCCNet~\cite{Deng:2021:MCC},  AdaAttN~\cite{liu2021adaattn}, IEST~\cite{chen2021artistic}, StyleFormer~\cite{Wu:2021:SF}, as well as StyTr$^2$~\cite{deng2021stytr2}.
All the baselines are trained using publicly available implementations with default configurations.
The comparison of inference speed is shown in Table~\ref{tab:quantitative}.
In all our experiments, our results are generated by using AdaIN as backbone, if there is no specific annotation.

\paragraph{Implementation details}
We collect 100,000 artistic images in different styles from WikiArt~\cite{Phillips:2011:wikiart} and randomly sample 20,000 images as our artistic dataset.
We averagely sample 20,000 images from Places365~\cite{Zhou:2018:Places365} as realistic image dataset.
We train and evaluate our framework on those artistic and realistic images.
In the training phase, all images are loaded with $256 \times 256$ resolution.
The number of feature map layers $M$ is set to be 4.
The dimension $K$ of style latent code is set to 512, 512, 512, and 512 for the four different layers, respectively.
We use Adam~\cite{kingma2014adam} as optimizer with $\beta_1=0.5$, $\beta_2=0.999$, and a batch size of $4$.
The initial learning rate is set to $1 \times 10^{-4}$ and linear decayed linear for total $8 \times 10^5$ iterations.
The training process takes about $18$ hours on one NVIDIA GeForce RTX3090.

\subsection{Effectiveness on Various Backbones}
Our UCAST, as an separated network structure can be plug-and-play for most arbitrary image style transfer models.
In our experiments, we adopt UCAST to AdaIN~\cite{Huang:2017:AdaIn}, ArtFlow~\cite{An:2021:Artflow}, and StyTr$^2$~\cite{deng2021stytr2}.
AdaIN~\cite{Huang:2017:AdaIn} is a CNN-based style transfer model that includes a fixed VGG network to encode the content and style images, an adaptive instance normalization layer to align the channel-wise mean and variance of content features to match those of style features, and a CNN decoder to invert the AdaIN output to the image spaces.
ArtFlow~\cite{An:2021:Artflow} is a neural flow-based model which consists of reversible neural flows and an unbiased feature transfer module.
Neural flows are a type of deep generative model that learns the precise likelihood of high-dimensional observations via a series of invertible transformations.
StyTr$^2$~\cite{deng2021stytr2} is a ViT-based model that contains two transformer encoders for the content image and the style reference respectively, a multi-layer transformer decoder for content sequence stylization, and a CNN decoder.

The comparison results are shown in Fig.~\ref{fig:backbone}.
When transferring style image of ink and wash, as shown in the 1\textsuperscript{st} row, the three backbone methods cannot faithfully generate the brush strokes and the empty background.
By training under the UCAST framework, all the enhanced methods can generate high quality ink and wash images with smooth empty background and vivid strokes.
When dealing with watercolor image, as shown in the 2\textsuperscript{nd} row, the backbones cannot capture the feeling of color blooming.
Since the sky in the content image is a large empty area which the style image does not have, the three backbones tend to generate obvious artifacts.
Being trained under UCAST can reduce the artifacts obviously and transfer the unique strokes of watercolor.
As shown in the 3\textsuperscript{rd} and 4\textsuperscript{th} rows, the backbones fail to transfer the sharp lines in the style reference, while UCAST improves the details of the generated images significantly.
UCAST can also help all the backbones to generate vivid brush strokes of oil paintings, as shown in the 5\textsuperscript{th} row.


%%%% Figs/fig_sota_compare.tex starts here %%%%

\input{Figs/fig_sota_compare}

%%%% Figs/fig_sota_compare.tex ends here %%%%

%%%% Figs/fig_interpolate.tex starts here %%%%

\input{Figs/fig_interpolate}

%%%% Figs/fig_interpolate.tex ends here %%%%

\subsection{Qualitative Evaluation}

\subsubsection{Image Style Transfer}

We first present qualitative results of our method against the selected state-of-the-art methods in Fig.~\ref{fig:SOTA}.
The comparison shows the superiority of UCAST in terms of visual quality.
AdaIN often fails to generate sharp details and introduces undesired patterns that do not exist in style images (e.g., the 4\textsuperscript{th}, 6\textsuperscript{th}, 9\textsuperscript{th} and 11\textsuperscript{th} rows).
ArtFlow sometimes generates unexpected colors or patterns in relatively smooth regions in some cases (e.g., the 2\textsuperscript{nd}, 3\textsuperscript{rd} and 8\textsuperscript{th} rows).
MCCNet can effectively preserve the input content but may fail to capture the stroke details and often generates haloing artifacts around object contours (e.g., the 2\textsuperscript{nd}, 5\textsuperscript{th}, 9\textsuperscript{th}rows).
AdaAttN cannot well capture some stroke patterns and fails to transfer important colors of the style references to the results (e.g., the 1\textsuperscript{st},  5\textsuperscript{th} and 6\textsuperscript{th} rows).
Although the generated visual effects of IEST are of high quality, the usage of second-order statistics as style representation causes color distortion (e.g., the 1\textsuperscript{st} and the 4\textsuperscript{th} row) and cannot capture the detailed stylized patterns (e.g., the 5\textsuperscript{th} and 7\textsuperscript{th} rows ).
StyleFormer cannot well capture some stroke patterns and tends to generates artifacts in the results (e.g., the 1\textsuperscript{st},  6\textsuperscript{th} and 8\textsuperscript{th} rows).
StyTr$^2$ cannot well transfer the unique style of the reference images and also tends to generates artifacts(e.g., the 1\textsuperscript{st},  3\textsuperscript{rd} and 4\textsuperscript{th} rows).
In particular, these state-of-the-art methods cannot capture the \emph{leaving blank} characteristic of Chinese painting style in the 1\textsuperscript{st} row of Fig.~\ref{fig:SOTA} and fail to generate results with a clean background.

In comparison, UCAST achieves the best stylization performance that balances characteristics of style patterns and content structures.
Instead of using second-order statistics as a global style descriptor, we use an MSP module for style encoding with the help of a DE module for effective learning of style distribution.
Thus, UCAST can flexibly represent vivid local stroke characteristics and the overall appearance while still preserving the content structure.
For instance, as shown in Figs.~\ref{fig:teaser} and \ref{fig:motivation_ours} (the $1\textsuperscript{st}$ row) and \ref{fig:SOTA} (the $1\textsuperscript{st}$ row), UCAST successfully captures the large portion of empty regions in the style images, and it generates a stylization results which have salient objects in the center and blank space around.
As shown in Fig.~\ref{fig:SOTA}, besides commonly used oil paintings (the $2\textsuperscript{nd}$, $3\textsuperscript{rd}$ and $5\textsuperscript{th}$ rows), UCAST can also generate high quality results of line drawing (the $4\textsuperscript{th}$), cartoon (the $7\textsuperscript{th}$, $8\textsuperscript{th}$ and $9\textsuperscript{th}$ rows) , aquarelle (the $6\textsuperscript{th}$ and $11\textsuperscript{th}$ rows) , crayon drawing
 (the $10\textsuperscript{th}$ row) and color pencil drawing (the $12\textsuperscript{th}$ row).

\subsubsection{Style Interpolation}

We interpolate the feature maps among four style images with equivalent weights.
As shown in Fig.~\ref{fig:interpolate}, we can interpolate among arbitrary styles by providing the decoder with a convex mixture of feature maps converted to various styles.
Smooth intra-domain (vertically) and inter-domain (horizontally) interpolation results are obtained.


\subsection{Quantitative Evaluation}
We use the content loss~\cite{Li:2017:UST}, LPIPS~\cite{chen2021artistic}, and deception rate~\cite{Sanakoyeu:2018:SAC} and conduct two user studies to evaluate our method quantitatively.
The two user studies are online surveys that cover art/computer science students/professors and civil servants.

For content loss and LPIPS, we use a pre-trained VGG-19 and compute the average perceptual distances between the content image and the stylized image. 
The statistics are shown in Table~\ref{tab:quantitative}.
For deception rate, we train a VGG-19 network to classify 10 styles on WikiArt.
Then, the deception rate is calculated as the percentage of stylized images that are predicted by the pre-trained network as the correct target styles.
We report the deception rate for the proposed UCAST and the baseline models in the 2\textsuperscript{nd} column of Table~\ref{tab:quantitative}.
As observed, UCAST achieves the highest accuracy and surpasses other methods by a large margin.
As a reference, the mean accuracy of the network on real images of the artists from WikiArt is $78\%$.


\paragraph{User Study \uppercase\expandafter{\romannumeral1}}
We compare UCAST with seven state-of-the-art style transfer methods to evaluate which method generates results that are most favored by humans.
For each participant, 50 content-style pairs are randomly selected and the stylized results of UCAST and one of the other methods are displayed in a random order.
Then, we ask the participants to choose the image that learns the most characteristics from the style image.
Participants were told that the consistency of content and style was the primary metrics.
The style is subjective and the effectiveness of training also depends on their understanding ability.
Finally, we collect 3,800 votes from 76 participants.
We report the percentage of votes for each method in the 6\textsuperscript{th} column of Table~\ref{tab:quantitative}.
These results demonstrate that UCAST achieves the better style transfer results.
Moreover, from the statistics, we find that UCAST obtains significantly higher preferences in categories of sketch, Chinese painting, and impressionism.

\paragraph{User Study \uppercase\expandafter{\romannumeral2}}
We design a novel user study to evaluate the stylized images quantitatively, which is called the Stylized Authenticity Detection.
For each question, we show participants ten artworks of similar styles, including two to four stylized fake painting and ask them to select the synthetic ones.
Within each single question, the stylized paintings are generated by the same method.
Each participant finished 25 questions.
Finally, we collect 2000 groups of results from 80 participants and use the average precision and recall as the measurement for how likely the results will be recognized as synthetics.
We report the percentage of votes for each method in the 7\textsuperscript{th} column of Table~\ref{tab:quantitative}.
The paintings generated by UCAST have the lowest chance to be decided by people as fake paintings.
We also notice that the precision and recall of UCAST is less than $50\%$, which means that users could not tell the real ones from the fakes and tend to select more real paintings as synthetics when doing the testing.


\subsection{Video Style Transfer}


We compare our method with seven baselines on video style transfer and show the stylization results in Fig.~\ref{fig:video_sota}.
We visualize the heat maps of differences between different frames to assess the stability and consistency of synthesized video clip.
As we can see, our approach outperforms existing style transfer methods in terms of stability and consistency by a significant margin. This can be attributed to three points: 
1) our style representation and domain distribution learning offer proper guidance to prevent the model from distorted texture patterns;
2) the cycle consistency loss enhances the consistency of synthesized video clip;
3) the added patch-wise contrastive losses offer a strong content consistency constrain which motivates the same object in a different frame to have the same stylization results.

\paragraph{Video consistency.}
We employ the widely used temporal loss ~\cite{wang:2020:consistent} to quantitatively analyze the temporal consistency of stylized videos.   
Given two adjacent frames $I_c^{t}$ and $I_c^{t-1}$ in a T-frame input clip and $I_{cs}^t$ and $I_{cs}^{t-1}$ in a T-frame rendered clip, the temporal loss is defined as:
\begin{equation}
{L}_{temporal} =  average(||O \circ (W_{I_{c}^{t-1}\rightarrow I_{c}^{t}} (I_{cs}^{t-1}) - I_{cs}^t) ||),
\end{equation}
where $O$ is an occlusion mask:
\begin{equation}
O = {|W_{I_{c}^{t-1} \rightarrow I_{c}^{t}}(I_{cs}^{t-1}) - I_{cs}^t| > 10}.
\end{equation}
The mask eliminates the negative effects brought by the inaccurate optical flow estimation. 
As shown in Table~\ref{tab:temporal_consistency}, our method achieves the best temporal consistency.


%%%% Figs/fig_video_sota.tex starts here %%%%

\input{Figs/fig_video_sota}

%%%% Figs/fig_video_sota.tex ends here %%%%

%%% Tables/tab_video.tex starts here %%%%

\input{Tables/tab_video}

%%% Tables/tab_video.tex ends here %%%%

% %%%% Figs/fig_temperature.tex starts here %%%%

\input{Figs/fig_temperature}

%%%% Figs/fig_temperature.tex ends here %%%%


\subsection{Ablation Study}

\paragraph{Contrastive style loss.}
We remove the contrastive style loss from Eq.~(\ref{eqn:total_loss}) to train the model.
As shown in Fig.~\ref{fig:ablation_contra_noNCE}, the model without our contrastive style loss cannot capture the color and the stroke characteristics of the style image compared with the full model.
The brushstrokes of water color in the style image almost disappear in the 1\textsuperscript{st} row.
The sharp lines and edges in the 2\textsuperscript{nd} row become smooth and murky.
The brown color of the whole image generated in the 3\textsuperscript{rd} row dose not appear in the style image.

We replace the adaptive temperature from Eq.~(\ref{eqn:loss_G_NCE2}) with constant temperature to train the model.
As shown in Fig.~\ref{fig:ablation_contra_cast}, when dealing with difficult content-style pairs, the model without our adaptive temperature tends to generate artifacts.
For instance, the black artifact appears in the sky of the 1\textsuperscript{st} row and 3\textsuperscript{rd} row.
Meanwhile, by introducing input-dependent temperature, the full UCAST can capture and transfer the unique style of cartoon.
In the 2\textsuperscript{nd} row, the sharp lines and flat color fillings in the style image are faithfully transfer to the results while the simplified model generates result with mixing style.
Meanwhile, the content details of the women's face are well preserved by the full model.
With the contrastive style loss and adaptive temperature, our full model can faithfully transfer the brushstrokes, textures, and colors from the input style image.

%%%% Figs/fig_ablation_study.tex starts here %%%%

\input{Figs/fig_ablation_study}

% %%%% Figs/fig_ablation_study.tex ends here %%%%
\paragraph{Domain enhancement.}
Our full UCAST uses DE for realistic and artistic images separately. 
We train a simplified UCAST model without DE module.
As shown in Figs.~\ref{fig:ablation_study_no_domain} and~\ref{fig:ablation_study_full}, the color of the style images are faithfully transferred, but the generated images do not appear like real paintings.
We train a simplified UCAST model using one discriminator that mixes realistic and artistic images together (mix-DE).
As shown in Fig.~\ref{fig:ablation_study_one_domain}, the results generated by mix-DE model are acceptable, but the stroke details in the generated images are weaker than the ones by the full UCAST model.
This fact is due to the existence of a significant gap between the artistic and realistic image domains. 
We further abandon all images from realistic domain for ablation (one-DE).
As shown in Fig.~\ref{fig:ablation_study_art_domain}, the results generated by one-DE model lack details.

To better evaluate the improvement of the contrastive style loss on the style transfer task, we exclude the latent promotion of cycle consistency loss from network training.
The reason is that the reconstruction process of artistic image may imply style information.
We train UCAST with an asymmetric cycle consistent loss, which only reconstructs the realistic images.
The decoder of the style transfer network is unaffected by the reconstruction of the artistic image.
As shown in Fig.~\ref{fig:ablation_study_cyc}, removing realistic image reconstruction will lead to slightly degraded stylization results.

%%% 4-Experiments.tex ends here %%%%