%%%% 3-Method.tex starts here %%%%

\section{Method}

\subsection{Overview}
Our unified framework for arbitrary image style transfer as a separated network structure can be plug-and-play for most arbitrary image style transfer models.
As shown in Fig.~\ref{fig:framework}, our UCAST consists of three key components:
1) a parallel contrastive learning scheme that is applied to the style representation learning and the style transfer process; 
2) a domain enhancement scheme to further help learn the distribution of the artistic image domain;
and 3) a generator $\generator$ to generate the stylization output.
Both 1) and 2) are used for learning style features to measure the difference between artistic images and realistic images.
The parallel contrastive learning scheme focuses on forcing the specific reference artistic image and the generated result to have the same style, while the domain enhancement scheme pays attention to the holistic difference between the artistic domain and the realistic domain.

The main structure of our parallel contrastive learning scheme is a multi-layer style projector that is trained to project features of artistic images into style codes.
The contrastive losses are introduced to guide parallel optimization processes, including both the training of  multi-layer style projector and the generator.
When training the generator, we introduce adaptive contrastive loss which is implemented with dual input-dependent temperature.
By considering the similarities between the style codes of the reference style image and other artistic images, our adaptive contrastive loss is more tolerant to style consistent samples.
Meanwhile, the input-dependent temperature is also influenced by the similarities between the style codes of the target style image and the input content image, to increase the robustness of various content-style pairs and prevent artifacts.
The domain enhancement scheme is accomplished by two discriminators for the artistic domain and the realistic domain respectively.
The adversarial loss helps the discriminator model the distribution of the corresponding domain and the cycle consistency loss is adopted to maintain the content information.


\input{Figs/fig_MSP}

\subsection{Parallel Contrastive Learning}

\subsubsection{Multi-layer Style Projector}

Our goal is to develop an unified arbitrary style transfer framework that can capture and transfer the local stroke characteristics and overall appearance of an artistic image to a natural image.
A key component is to find a suitable style representation which can be used to distinguish different styles and further guide the generation of style images.
To this end, we design an MSP module, which includes a style feature extractor and a multi-layer projector.
Instead of using features from a specific layer or a fusion of multiple layers, our MSP projects features of different layers into separate latent style spaces to encode local and global style cues.

Specifically, we adopt VGG-19~\cite{simonyan2014very} and fine-tune the VGG-19 model pre-trained on ImageNet with a collection of 18,000 artistic images in 50 categories.
We then select $M$ layers of feature maps in VGG-19 as input to our multi-layer projector (we use layers of ReLU1\_2, ReLU2\_2, ReLU3\_3, and ReLU4\_3 in all experiments).
We use max pooling and average pooling to capture the mean and peak value of features.
The multi-layer projector consists of pooling, convolution, and several multilayer perceptron layers, and it projects the style features into a set of $K$-dimensional latent style code, as shown in Fig.~\ref{fig:msp_overview}.

After training, MSP can encode an artistic image into a set of latent style code $\{ \latentcode_i | i \in [1, M], \latentcode_i \in \mathbb{R}^K\}$, which can be plugged into an existing style transfer network (i.e., replacing the mean and variance in AdaIN~\cite{Huang:2017:AdaIn}) as the guidance for stylization.
Next, we will describe how to jointly train MSP and style transfer networks with a contrastive learning strategy.

\subsubsection{Contrastive Style Representation Learning}

A branch of the parallel contrastive learning scheme is the style representation learning.
The MSP needs to be trained in order to obtain the reasonable style representation which is in the form of the style code $\{ \latentcode_1, \latentcode_2,..., \latentcode_M \}$.
However, we lack the ground-truth style code for supervised training.
Therefore, we adopt contrastive learning and design a new contrastive style  loss as an implicit measurement for the MSP training.

When training the MSP module, an image $\image$ and its augmented version $ \augmentedimage $ (random resizing, cropping, and rotations) are fed into a $M$-layer style feature extractor, which is the pre-trained VGG-19 network.
The extracted style features are then sent to the multi-layer projector, which is an $M$-layer neural network and maps the style features to a set of $K$-dimensional vectors $\{ \latentcode \}$.
The contrastive representation learns the visual styles of images by maximizing the mutual information between $\image$ and $\augmentedimage$ in contrast to other artistic images within the dataset considered as negative samples $\{ \negativeexample \}$.
Specifically, the images $\image$, $\augmentedimage$, and $N$ negative samples are respectively mapped into $M$ groups of $K$-dimensional vectors $\latentcode$, $\latentcode^{+} \in \mathbb{R}^K$ and $\{ \latentcode^{-} \in \mathbb{R}^K \}$. The vectors are normalized to prevent collapsing.
We maintain a large dictionary of 4096 negative examples using a memory bank architecture following MOCO~\cite{He:2019:MOCO}.
The negative examples are sampled from the memory bank.
Following \cite{van2018representation}, we define the contrastive loss function to train our MSP module as:
%
\begin{equation}
\begin{aligned}
\loss_{contra}^{MSP}=-\sum_{i=1}^M{\log \frac{\exp(\latentcode_{i} \cdot {\latentcode_{i}^{+}}/ \tau)}{\exp (\latentcode_{i} \cdot {\latentcode_{i}^{+}} / \tau)+\sum_{j=1}^N{\exp(\latentcode_{i} \cdot {\latentcode_{i_{j}}^{-}} / \tau)}}},
\end{aligned}
\label{eqn:loss_msp}
\end{equation}
%
where $\cdot$ denotes the dot product of two vectors.
It is worth noting that we calculate the contrastive loss between \emph{images}, as opposed to CUT~\cite{park2020CUT} which adopts contrastive learning by cropping images into patches and maximizing the mutual information between \emph{patches}.

%%%% Figs/fig_adaptive_temperature.tex starts here %%%%

\begin{figure*}
\centering
\begin{subfigure}[t]{0.32\linewidth}
\includegraphics[width=1\linewidth]{Image/model/temperature-a}
\caption{Fixed temperature: Case I}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
\includegraphics[width=1\linewidth]{Image/model/temperature-b}
\caption{Fixed temperature: Case II}
\end{subfigure}
\begin{subfigure}[t]{0.32\linewidth}
\includegraphics[width=1\linewidth]{Image/model/temperature-c}
\caption{Adaptive temperature: Case II}
\end{subfigure}
\caption{\revision{
Visualization of the embedding distribution of artistic images and generated results on a hypersphere. 
}}
\label{fig:adaptive_temperature}
\end{figure*}

%%%% Figs/fig_adaptive_temperature.tex ends here %%%%

\subsubsection{Contrastive Style Transfer}

The other branch of the parallel contrastive learning scheme is the style transfer process.
The above contrastive representation provides proper measurement for the generator $\generator$ to transfer styles between images.
We compute the loss using the contrastive representations of the output image $\image_{cs}$ and the reference style image $\inputstyle$, then $\image_{cs}$ will have a style similar to $\inputstyle$:
%
\begin{equation}
\begin{aligned}
\loss_{contra}^{G}=-\sum_{i=1}^M {\log \frac{\exp({\outputlatent}_i \cdot {\stylelatent}_i/ \tau)}{\exp ({\outputlatent}_i \cdot {\stylelatent}_i / \tau)+\sum_{j=1}^N{ \exp({\outputlatent}_i \cdot {\negativelatent} / \tau)}}},
\end{aligned}
\label{eqn:loss_G_NCE}
\end{equation}
%
where $\outputlatent$ and $\stylelatent$ denote the contrastive representation of $\image_{cs}$ and $\inputstyle$, respectively.
Notably, we take the specific generated and reference images as positive examples and utilize contrastive loss as guidance to transfer styles, which is an one-on-one process.
Differently, the contrastive loss in IEST~\cite{chen2021artistic} is calculated only within generated results and it takes a set of images as positive examples, which could reduce the style consistency with the given reference (see Fig.~\ref{fig:SOTA}).


\subsubsection{Adaptive Contrastive Learning}

Since different artworks could have similar styles, the model needs to be tolerant to these style similarities.
Contrastive learning seeks to minimize the distance between positive samples and maximize the distance between negative samples in the representation space.
By gradient analysis, \cite{wang2021understanding} demonstrates that the gradients with regard to negative samples are proportional to the similarity between the particular negative sample and the anchor, proving that the contrastive loss is a hardness-aware loss function.
The temperature $\tau$ controls the distribution of negative gradients. 
Smaller temperatures tend to focus more on the anchor point's nearest neighbors, whereas larger temperatures tend to penalize negative samples equally. 
When the temperature is fixed, the gradient's magnitude with respect to a positive sample is equal to the sum of gradients with respect to all negative samples.
Prior works of temperature analysis mainly focus on the penalty's unevenness of negative samples within an anchor~\cite{wang2021understanding}, or the sum of penalties of different anchors within a training batch~\cite{zhang2022dual}.
Differently, we pay attention to the proportion of penalties between the positive sample and negative samples.

In Fig.~\ref{fig:adaptive_temperature}, we show the embedding distribution with four real paintings and one generated image on a hypersphere. 
As shown in Fig.~\ref{fig:adaptive_temperature}a, when the style of the reference image and the other artistic images served as negative samples vary differently, the punishment of the fixed small temperature may work finely.
Different artistic images may share similar styles.
When dealing with the case that similar style images act as negative samples, as shown in Fig.~\ref{fig:adaptive_temperature}b, the ideal embedding of the generated image is being separated from all the negative samples but closer to the similar negative samples.
However, the contrastive loss with fixed small temperature tends to give strong punishment on similar samples due to the hardness-aware attribute, which means the generated image may be pushed away from the similar negative sample too much which is not a reasonable embedding in the hypersphere.
Our adaptive contrastive style transfer approach is aware of the negative samples which share a similar style with the reference image.
When high similarity negative samples appear, our approach will gain tolerance by increasing the temperature accordingly.
As shown in Fig.~\ref{fig:adaptive_temperature}c, with the help of our adaptive contrastive style transfer approach, the generator will be guided under a reasonable loss and the generated image can get a better embedding.

To further illustrate our adaptive temperature mechanism, we substitute the similarities of the positive sample and the negative samples in Eq.~\ref{eqn:loss_G_NCE} with $\positivepair =  \outputlatent_i \cdot \stylelatent,
\negativepair =  \outputlatent_i \cdot \negativelatent$:
%
\begin{equation}
\begin{aligned}
\loss_{contra}^{G}=-\sum_{i=1}^M {\log \frac{\exp(\positivepair/ \positivetemperature)}{\exp (\positivepair / \positivetemperature)+\sum_{j=1}^N{ \exp(\negativepair / \negativetemperature)}}},
\end{aligned}
\label{eqn:loss_G_NCE2}
\end{equation}
%
where $\positivetemperature$ and $\negativetemperature$ indicate the temperatures of positive samples and negative samples, respectively.
We analyze the gradients with respect to positive samples and different negative samples.
Specifically, the gradients with respect to the positive similarity $\positivepair$ and the negative similarity $\negativepair$ are formulated as:
\begin{equation}
\begin{aligned}
\frac{\partial \loss_{contra}^{G}}{\partial \positivepair} = - \sum_{i=1}^M \frac{1}{\positivetemperature} \cdot \frac{\sum_{j=1}^N{ \exp(\negativepair / \negativetemperature)}}{\exp (\positivepair / \positivetemperature)+\sum_{j=1}^N{ \exp(\negativepair / \negativetemperature)}},\\
\frac{\partial \loss_{contra}^{G}}{\partial \negativepair} = - \sum_{i=1}^M \frac{1}{\negativetemperature} \cdot \frac{\exp(\negativepair / \negativetemperature)}{\exp (\positivepair / \positivetemperature)+\sum_{j=1}^N{ \exp(\negativepair / \negativetemperature)}}.
\end{aligned}
\label{eqn:loss_G_partial}
\end{equation}
%
From Eq.~(\ref{eqn:loss_G_partial}), we can see that the magnitude of the gradient with respect to the positive sample is proportional to the sum of gradients with respect to all the negative samples.
By controlling $\negativetemperature$ and $\positivetemperature$, we can change the strength of penalties on the positive sample and negative samples.

We propose an input-dependent scheme to determine temperature by considering the similarities between the style code of the reference style $\stylelatent$ and the style codes of other artistic images $\negativelatent$.
Specifically, the more highly similar samples the memory bank contains, the larger the temperature is.
% Taking $\mu^-$ and $\sigma^-$ indicate the estimation of the mean and standard deviation of $\sum_{j=1}^N g(\negativepair)$.
Our input-dependent temperature is computed as:
\begin{equation}
\begin{aligned}
\negativetemperature &= t^-_{range} \cdot \frac{1}{1+\exp(-(\sum_{j=1}^N  g(\negativepair)-\mu^-)\cdot \sigma^-)} + t^-_{bound}, \\
g(\negativepair) &= \left\{ \begin{array}{rcl}
\negativepair & \mbox{for} & \negativepair > \mathbf{s^-}\\
0 & \mbox{for} & \negativepair \leq \mathbf{s^-}
\end{array} \right. ,
\end{aligned}
\label{eqn:loss_negative_temperature}
\end{equation}
where $\mu^-$ and $\sigma^-$ indicate the estimation of the mean and standard deviation of $\sum_{j=1}^N  g(\negativepair)$.
$t^-_{range}$ and $t^-_{bound}$ denote the range and lower bound of $\negativetemperature$.

Besides, arbitrary style transfer task often has the problem that the style images may not always be suitable for the content image and thus increase undesired artifacts. 
For example, when transferring a texture-rich style to a smooth content image, the model may produce artifacts and distortion (e.g., the 4\textsuperscript{th} row of Fig.~\ref{fig:SOTA}).
Therefore, it is necessary to adaptively handle various content-style pairs to increase the robustness.
To overcome the aforementioned problem, we propose a suitability-aware scheme to determine temperature based on the similarity between the style code of the reference image $\stylelatent$ and the style code of the content image $\latentcode_i^c$.
When the reference style and the content image are dissimilar, the penalty tends to be assigned more to negative samples to prevent artifacts of being overly stylized:
%
\begin{equation}
\begin{aligned}
\positivetemperature &= \negativetemperature \cdot f(\stylelatent,\latentcode_i^c), \\
f(\stylelatent,\latentcode_i^c) &= t^+_{range} \cdot \frac{1}{1+\exp((\stylelatent \cdot \latentcode_i^c -\mu^+) \cdot \sigma^+)} + t^+_{bound},
\end{aligned}
\label{eqn:loss_positive_temperature}
\end{equation}
%
where $\mu^+$ and $\sigma^+$ indicate the estimation of the mean and standard deviation of $\stylelatent \cdot \latentcode_i^c)$.
$t^+_{range}$ and $t^+_{bound}$ denote the range and lower bound of $\positivetemperature$.


\subsection{Domain Enhancement}
We introduce DE with adversarial loss to enable the network to learn the style distribution.
Recent style transfer models employ GAN~\cite{goodfellow2014generative} to align the distribution of generated images with specific artistic images~\cite{chen2021dualast,Lin:2021:DAM}.
The adversarial loss can enhance the holistic style of the stylization results, while it strongly relies on the distribution of datasets.
Even with the specific artistic style loss, the generation process is often not robust enough to be artifact-free.

Differently from these previous methods, we divide the images in the training set into realistic domain and artistic domain, and we use two discriminators $D_R$ and $D_A$ to enhance them respectively (see Fig.~\ref{fig:framework}).
During the training process, we first randomly select an image from the realistic domain as the content image $\inputcontent$ and another image from the artistic domain as the style image $\inputstyle$.
$\inputcontent$ and $\inputstyle$ are used as the real samples of $D_R$ and $D_A$, respectively.
The generated image $\image_{cs} = \generator(\inputcontent, \inputstyle)$ is used as the fake sample of $D_A$.
We exchange the content and style images to generate an image $\image_{sc} = \generator(\inputstyle, \inputcontent)$ as the fake sample of $D_R$. The adversarial loss is:
%
\begin{equation}
\begin{aligned}
\loss_{adv} =& \mathbb{E}[\log D_R(I_c)]+\mathbb{E}[\log (1-D_R(I_{cs}))]\\
& + \mathbb{E}[\log D_A(I_s)]+\mathbb{E}[\log (1-D_A(I_{sc}))].
\end{aligned}
\end{equation}

To maintain the content information of the content image in the process of style transfer between the two domains, we also add a cycle consistency loss:
%
\begin{equation}
\begin{aligned}
    \loss_{cyc} = \mathbb{E} [\Vert I_c -\generator(I_{cs},I_c)\Vert_1] + \mathbb{E} [\Vert I_s -\generator(I_{sc},I_s)\Vert_1].
\end{aligned}
\end{equation}

\subsection{Video Style Transfer}

To apply our method for video style transfer, we adopt the patch-wise contrastive content loss in~\cite{park2020CUT} to keep the content consistency.
The feature maps of the content image and the stylized result are cut into feature patches.
The patches at the same specific location of the content image and the stylized result are leveraged as positive samples, while the other patches within the input as negatives:
%
\begin{equation}
\begin{aligned}
\loss_{contra}^{c}=-{\log \frac{\exp(v \cdot {v^{+}}/ \tau)}{\exp (v \cdot {v^{+}} / \tau)+\sum_{n=1}^W{\exp(v \cdot {v_n^{-}} / \tau)}}},
\end{aligned}
\label{eqn:loss_cut}
\end{equation}
%
where $v,v^+ \in \mathbb{R}^K$, $v_n^- \in \mathbb{R}^{K \times W}$ denote the content feature of generated image patch, content image patch, and negative image patches, respectively.

\subsection{Network Training}
Our full objective function for training of the generator $G$ and discriminators $D_R$ and $D_A$ is formulated as:
%
\begin{equation}
\begin{aligned}
\loss(G, D_R, D_A) &= \lambda_{1} \loss_{adv}+ \lambda_{2}\loss_{cyc}+ \lambda_{3} \loss^G_{contra}+
\lambda_{4} \loss_{contra}^{c},\\
\end{aligned}
\label{eqn:total_loss}
\end{equation}
where $\lambda_{1}$, $\lambda_{2}$, $\lambda_{3}$, and $\lambda_{4}$ are weights to balance different loss terms. We set $\lambda_{1} = 1.0$, $\lambda_{2} = 2.0$, $\lambda_{3} = 0.2$, and $\lambda_{4} = 1.0$ in our experiments. 




%%%% 3-Method.tex ends here %%%%
