
@misc{ipnn,
title={Indeterminate Probability Theory},
author={Tao Yang and Chuang Liu and Xiaofeng Ma and Weijia Lu and Ning Wu and Bingyang Li and ZHIFEI YANG and Peng Liu and Lin Sun and xiaodong Zhang and Can Zhang},
year={2024},
url={https://openreview.net/forum?id=sSWGqY2qNJ}
}

@misc{cipnn,
title={Continuous Indeterminate Probability Neural Network},
author={Tao Yang},
year={2024},
url={https://openreview.net/forum?id=Rt6btdXS2b}
}

@misc{seqip,
title={Sequential Indeterminate Probability Theory for Multivariate Time Series Forecasting},
author={Tao Yang},
year={2024},
url={https://openreview.net/forum?id=MIKNVIxd2X}
}



@BOOK{VAE_2019,
  author={Kingma, Diederik P. and Welling, Max},
  title={An Introduction to Variational Autoencoders},
  year={2019},
  volume={},
  number={},
  pages={},
  doi={}
}

@inproceedings{VAE2,
  title = 	 {Doubly Stochastic Variational Bayes for non-Conjugate Inference},
  author = 	 {Titsias, Michalis and Lázaro-Gredilla, Miguel},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {1971--1979},
  year = 	 {2014},
  editor = 	 {Xing, Eric P. and Jebara, Tony},
  volume = 	 {32},
  number =    {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/titsias14.pdf},
  url = 	 {https://proceedings.mlr.press/v32/titsias14.html},
  abstract = 	 {We propose a simple and effective variational inference algorithm based on stochastic optimisation   that can be widely applied for Bayesian non-conjugate inference in continuous parameter spaces. This algorithm is based on stochastic approximation and allows for efficient use of gradient information from the model joint density. We demonstrate these properties using illustrative examples as well as in challenging and diverse Bayesian inference   problems such as variable selection in logistic regression and fully   Bayesian inference over kernel hyperparameters in Gaussian process regression.}
}


@inproceedings{VAE3,
author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
title = {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
year = {2014},
publisher = {JMLR.org},
abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic backpropagation - rules for gradient backpropagation through stochastic variables - and derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {II–1278–II–1286},
location = {Beijing, China},
series = {ICML'14}
}

@misc{DLVMs_Tutorial,
Author = {Yoon Kim and Sam Wiseman and Alexander M. Rush},
Title = {A Tutorial on Deep Latent Variable Models of Natural Language},
Year = {2018},
Eprint = {arXiv:1812.06834},
}

@article{variational_method,
author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
title = {An Introduction to Variational Methods for Graphical Models},
year = {1999},
issue_date = {Nov.1.1999},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {37},
number = {2},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1007665907178},
doi = {10.1023/A:1007665907178},
abstract = {This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.},
journal = {Mach. Learn.},
month = {nov},
pages = {183–233},
numpages = {51},
keywords = {mean field methods, graphical models, Boltzmann machines, variational methods, neural networks, approximate inference, Bayesian networks, probabilistic inference, hidden Markov models, belief networks}
}

@article{VAE_binary,
  title={Deep AutoRegressive Networks},
  author={Karol Gregor and Ivo Danihelka and Andriy Mnih and Charles Blundell and Daan Wierstra},
  journal={ArXiv},
  year={2013},
  volume={abs/1310.8499}
}

@article{cifar10,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@InProceedings{stl10,
  title = 	 {An Analysis of Single-Layer Networks in Unsupervised Feature Learning},
  author = 	 {Coates, Adam and Ng, Andrew and Lee, Honglak},
  booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {215--223},
  year = 	 {2011},
  editor = 	 {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
  volume = 	 {15},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Fort Lauderdale, FL, USA},
  month = 	 {11--13 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v15/coates11a/coates11a.pdf},
  url = 	 {https://proceedings.mlr.press/v15/coates11a.html},
  abstract = 	 {A great deal of research has focused on algorithms for learning features from unlabeled data. Indeed, much progress has been made on benchmark datasets like NORB and CIFAR-10 by employing increasingly complex unsupervised learning algorithms and deep models. In this paper, however, we show that several simple factors, such as the number of hidden nodes in the model, may be more important to achieving high performance than the learning algorithm or the depth of the model. Specifically, we will apply several off-the-shelf feature learning algorithms (sparse auto-encoders, sparse RBMs, K-means clustering, and Gaussian mixtures) to CIFAR-10, NORB, and STL datasets using only single-layer networks. We then present a detailed analysis of the effect of changes in the model setup: the receptive field size, number of hidden nodes (features), the step-size (“stride”) between extracted features, and the effect of whitening. Our results show that large numbers of hidden nodes and dense feature extraction are critical to achieving high performance - so critical, in fact, that when these parameters are pushed to their limits, we achieve state-of-the-art performance on both CIFAR-10 and NORB using only a single layer of features. More surprisingly, our best performance is based on K-means clustering, which is extremely fast, has no hyper-parameters to tune beyond the model structure itself, and is very easy to implement. Despite the simplicity of our system, we achieve accuracy beyond all previously published results on the CIFAR-10 and NORB datasets (79.6% and 97.2% respectively).}
}

@misc{fashion_mnist,
Author = {Han Xiao and Kashif Rasul and Roland Vollgraf},
Title = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
Year = {2017},
Eprint = {arXiv:1708.07747},
}

@inproceedings{sparse_PCs,
 author = {Dang, Meihua and Liu, Anji and Van den Broeck, Guy},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {28374--28385},
 publisher = {Curran Associates, Inc.},
 title = {Sparse Probabilistic Circuits via Pruning and Growing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b6089408f4893289296ad0499783b3a6-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@article{PCs_Unifying,
  author    = {Choi, YooJung and Vergari, Antonio and Van den Broeck, Guy},
  title     = {Probabilistic Circuits: A Unifying Framework for Tractable Probabilistic Models},
  month     = {oct},
  year      = {2020},
  url       = "http://starai.cs.ucla.edu/papers/ProbCirc20.pdf",
  keywords  = {techreport}
}

@INPROCEEDINGS{sum_product,
  author={Poon, Hoifung and Domingos, Pedro},
  booktitle={2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)}, 
  title={Sum-product networks: A new deep architecture}, 
  year={2011},
  volume={},
  number={},
  pages={689-690},
  doi={10.1109/ICCVW.2011.6130310}}

@inproceedings{arithmetic_factor_belief,
author = {Darwiche, Adnan},
title = {A Logical Approach to Factoring Belief Networks},
year = {2002},
isbn = {1558605541},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Eights International Conference on Principles of Knowledge Representation and Reasoning},
pages = {409–420},
numpages = {12},
location = {Toulouse, France},
series = {KR'02}
}

@inproceedings{learn_arithmetic,
author = {Lowd, Daniel and Domingos, Pedro},
title = {Learning Arithmetic Circuits},
year = {2008},
isbn = {0974903949},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Graphical models are usually learned without regard to the cost of doing inference with them. As a result, even if a good model is learned, it may perform poorly at prediction, because it requires approximate inference. We propose an alternative: learning models with a score function that directly penalizes the cost of inference. Specifically, we learn arithmetic circuits with a penalty on the number of edges in the circuit (in which the cost of inference is linear). Our algorithm is equivalent to learning a Bayesian network with context-specific independence by greedily splitting conditional distributions, at each step scoring the candidates by compiling the resulting network into an arithmetic circuit, and using its size as the penalty. We show how this can be done efficiently, without compiling a circuit from scratch for each candidate. Experiments on several real-world domains show that our algorithm is able to learn tractable models with very large treewidth, and yields more accurate predictions than a standard context-specific Bayesian network learner, in far less time.},
booktitle = {Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence},
pages = {383–392},
numpages = {10},
location = {Helsinki, Finland},
series = {UAI'08}
}

@inproceedings{cutset,
author = {Rahman, Tahrima and Kothalkar, Prasanna and Gogate, Vibhav},
title = {Cutset Networks: A Simple, Tractable, and Scalable Approach for Improving the Accuracy of Chow-Liu Trees},
year = {2014},
isbn = {978-3-662-44850-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-44851-9_40},
doi = {10.1007/978-3-662-44851-9_40},
abstract = {In this paper, we present cutset networks, a new tractable probabilistic model for representing multi-dimensional discrete distributions. Cutset networks are rooted OR search trees, in which each OR node represents conditioning of a variable in the model, with tree Bayesian networks (Chow-Liu trees) at the leaves. From an inference point of view, cutset networks model the mechanics of Pearl’s cutset conditioning algorithm, a popular exact inference method for probabilistic graphical models. We present efficient algorithms, which leverage and adopt vast amount of research on decision tree induction for learning cutset networks from data. We also present an expectation-maximization (EM) algorithm for learning mixtures of cutset networks. Our experiments on a wide variety of benchmark datasets clearly demonstrate that compared to approaches for learning other tractable models such as thin-junction trees, latent tree models, arithmetic circuits and sum-product networks, our approach is significantly more scalable, and provides similar or better accuracy.},
booktitle = {Machine Learning and Knowledge Discovery in Databases},
pages = {630–645},
numpages = {16},
keywords = {Benchmark Dataset, Probabilistic Inference, Bayesian Network, Time Complexity, Leaf Node},
location = {Nancy
France}
}

@inproceedings{and_or,
author = {Marinescu, Radu and Dechter, Rina},
title = {AND/OR Branch-and-Bound for Graphical Models},
year = {2005},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {The paper presents and evaluates the power of a new framework for optimization in graphical models, based on AND/OR search spaces. The virtue of the AND/OR representation of the search space is that its size may be far smaller than that of a traditional OR representation. We develop our work on Constraint Optimization Problems (COP) and introduce a new generation of depth-first Branch-and-Bound algorithms that explore an AND/OR search space and use static and dynamic mini-bucket heuristics to guide the search. We focus on two optimization problems, solvingWeighted CSPs (WCSP) and finding theMost Probable Explanation (MPE) in belief networks. We show that the new AND/OR approach improves considerably over the classic OR space, on a variety of benchmarks including random and real-world problems. We also demonstrate the impact of different lower bounding heuristics on Branch-and-Bound exploring AND/OR spaces.},
booktitle = {Proceedings of the 19th International Joint Conference on Artificial Intelligence},
pages = {224–229},
numpages = {6},
location = {Edinburgh, Scotland},
series = {IJCAI'05}
}

@inproceedings{decision_diag,
author = {Kisa, Doga and Van den Broeck, Guy and Choi, Arthur and Darwiche, Adnan},
title = {Probabilistic Sentential Decision Diagrams},
year = {2014},
isbn = {1577356578},
publisher = {AAAI Press},
abstract = {We propose the Probabilistic Sentential Decision Diagram (PSDD): A complete and canonical representation of probability distributions defined over the models of a given propositional theory. Each parameter of a PSDD can be viewed as the (conditional) probability of making a decision in a corresponding Sentential Decision Diagram (SDD). The SDD itself is a recently proposed complete and canonical representation of propositional theories. We explore a number of interesting properties of PSDDs, including the independencies that underlie them. We show that the PSDD is a tractable representation. We further show how the parameters of a PSDD can be efficiently estimated, in closed form, from complete data. We empirically evaluate the quality of PS-DDs learned from data, when we have knowledge, a priori, of the domain logical constraints.},
booktitle = {Proceedings of the Fourteenth International Conference on Principles of Knowledge Representation and Reasoning},
pages = {558–567},
numpages = {10},
location = {Vienna, Austria},
series = {KR'14}
}


@INPROCEEDINGS{human_Recognition,  
  author={Biederman, Irving},  
  booktitle={Psychological review},   
  title={Recognition-by-components: a theory of human image understanding.},   
  year={1987}, 
  pages={115-147},  
  doi={10.1037/0033-295X.94.2.115}
}


@INPROCEEDINGS{ZSL,  
  author={Lampert, Christoph H. and Nickisch, Hannes and Harmeling, Stefan},  
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},   
  title={Learning to detect unseen object classes by between-class attribute transfer},   
  year={2009}, 
  pages={951-958},  
  doi={10.1109/CVPR.2009.5206594}
}

@inproceedings{Transformer,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
title = {Attention is All You Need},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000–6010},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@ARTICLE{ZSL_Summary,
  author={Fu, Yanwei and Xiang, Tao and Jiang, Yu-Gang and Xue, Xiangyang and Sigal, Leonid and Gong, Shaogang},
  journal={IEEE Signal Processing Magazine}, 
  title={Recent Advances in Zero-Shot Recognition: Toward Data-Efficient Understanding of Visual Content}, 
  year={2018},
  volume={35},
  number={1},
  pages={112-125},
  doi={10.1109/MSP.2017.2763441}}

@INPROCEEDINGS{resnet,  
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},  
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   
  title={Deep Residual Learning for Image Recognition},   
  year={2016},  
  volume={},  
  number={},  
  pages={770-778},  
  doi={10.1109/CVPR.2016.90}
  }

@article{VAE,
  title={Auto-Encoding Variational Bayes},
  author={Diederik P. Kingma and Max Welling},
  journal={CoRR},
  year={2014},
  volume={abs/1312.6114}
}


@INPROCEEDINGS{softmax,
  author={Mohammed, Abdul Arfat and Umaashankar, Venkatesh},
  booktitle={2018 International Conference on Advances in Computing, Communications and Informatics (ICACCI)}, 
  title={Effectiveness of Hierarchical Softmax in Large Scale Classification Tasks}, 
  year={2018},
  volume={},
  number={},
  pages={1090-1094},
  doi={10.1109/ICACCI.2018.8554637}}


@article{bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={ArXiv},
  year={2019},
  volume={abs/1810.04805}
}


@INPROCEEDINGS{bayes_study,  
  author={Cao, Yonghui},  
  booktitle={2010 International Conference on E-Health Networking Digital Ecosystems and Technologies (EDT)},   
  title={Study of the Bayesian networks},   
  year={2010},  
  volume={1},  
  number={},  
  pages={172-174},  
  doi={10.1109/EDT.2010.5496612}
}


@INPROCEEDINGS{bayes_app1,
  author={Sumanto and Sugiarti, Yuni and Supriyatna, Adi and Carolina, Irmawati and Amin, Ruhul and Yani, Ahmad},
  booktitle={2021 9th International Conference on Cyber and IT Service Management (CITSM)}, 
  title={Model Naïve Bayes Classifiers For Detection Apple Diseases}, 
  year={2021},
  volume={},
  number={},
  pages={1-4},
  doi={10.1109/CITSM52892.2021.9588801}}

@INPROCEEDINGS{bayes_app2,
  author={Khamdamovich, Khamdamov Rustam and Elshod, Haydarov},
  booktitle={2021 International Conference on Information Science and Communications Technologies (ICISCT)}, 
  title={Detecting spam messages using the naive Bayes algorithm of basic machine learning}, 
  year={2021},
  volume={},
  number={},
  pages={1-3},
  doi={10.1109/ICISCT52966.2021.9670243}}

@article{bayes_model,
title = {Improving neural network’s performance using Bayesian inference},
journal = {Neurocomputing},
volume = {461},
pages = {319-326},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.07.054},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221011309},
author = {Jorge Morales and Wen Yu},
keywords = {Performance of neural networks, Bayesian inference, System identification},
abstract = {Neural networks can be used as a data-driven model for system identification. But the probability properties of the training data are not included. The Bayesian approach can model the input and output probability distribution, but it cannot give points estimation. In this paper, we propose a special neural model, which combines the neural model and Bayesian inference. The probability distributions of input and output are obtained by the Bayesian approach. This statistical model changes the neural network’s structure and improve the accuracy of the neural modeling. We also propose a neural network training method using Bayesian inference. The approach capabilities are analyzed. We use three examples to compare our method with the other black box methods. The results show that this new model is much better, when there are large noises, and the dynamics of the system is complex.}
}

@article{bayes_combine_recognition,
  title={A Neural Network-Based Approach for Statistical Probability Distribution Recognition},
  author={Chan Su and Chia-Jen Chou},
  journal={Quality Engineering},
  year={2006},
  volume={18},
  pages={293 - 297}
}

@article{bayes_combine_timeseries,
title = {Nonlinear time series forecasting with Bayesian neural networks},
journal = {Expert Systems with Applications},
volume = {41},
number = {15},
pages = {6596-6610},
year = {2014},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2014.04.035},
url = {https://www.sciencedirect.com/science/article/pii/S0957417414002589},
author = {Ozan Kocadağlı and Barış Aşıkgil},
keywords = {Nonlinear time series, Bayesian neural networks, Gaussian approximation, Recursive hyperparameters, Genetic algorithms, Hybrid Monte Carlo simulations},
abstract = {The Bayesian learning provides a natural way to model the nonlinear structure as the artificial neural networks due to their capability to cope with the model complexity. In this paper, an evolutionary Monte Carlo (MC) algorithm is proposed to train the Bayesian neural networks (BNNs) for the time series forecasting. This approach called as Genetic MC is based on Gaussian approximation with recursive hyperparameter. Genetic MC integrates MC simulations with the genetic algorithms and the fuzzy membership functions. In the implementations, Genetic MC is compared with the traditional neural networks and time series techniques in terms of their forecasting performances over the weekly sales of a Finance Magazine.}
}

@ARTICLE{mnist,  
author={Deng, Li},  
journal={IEEE Signal Processing Magazine},   
title={The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web]},   
year={2012},  
volume={29}, 
number={6},  
pages={141-142},  
doi={10.1109/MSP.2012.2211477}}


@inproceedings{jaccard,
author = {Raff, Edward and Nicholas, Charles},
title = {An Alternative to NCD for Large Sequences, Lempel-Ziv Jaccard Distance},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098111},
doi = {10.1145/3097983.3098111},
abstract = {The Normalized Compression Distance (NCD) has been used in a number of domains to compare objects with varying feature types. This flexibility comes from the use of general purpose compression algorithms as the means of computing distances between byte sequences. Such flexibility makes NCD particularly attractive for cases where the right features to use are not obvious, such as malware classification. However, NCD can be computationally demanding, thereby restricting the scale at which it can be applied. We introduce an alternative metric also inspired by compression, the Lempel-Ziv Jaccard Distance (LZJD). We show that this new distance has desirable theoretical properties, as well as comparable or superior performance for malware classification, while being easy to implement and orders of magnitude faster in practice.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1007–1015},
numpages = {9},
keywords = {normalized compression distance, malware classification, jaccard similarity, cyber security, lempel-ziv},
location = {Halifax, NS, Canada},
series = {KDD '17}
}


@ARTICLE{ZSL_Overview_2022,  
author={Pourpanah, Farhad and Abdar, Moloud and Luo, Yuxuan and Zhou, Xinlei and Wang, Ran and Lim, Chee Peng and Wang, Xi-Zhao and Wu, Q. M. Jonathan},  
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   
title={A Review of Generalized Zero-Shot Learning Methods},   
year={2022},  
volume={},  
number={},  
pages={1-20},  
doi={10.1109/TPAMI.2022.3191696}}

@inproceedings{convergence_paper,
author = {Li, Yuanzhi and Yuan, Yang},
title = {Convergence Analysis of Two-Layer Neural Networks with ReLU Activation},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In recent years, stochastic gradient descent (SGD) based techniques has become the standard tools for training neural networks. However, formal theoretical understanding of why SGD can train neural networks in practice is largely missing. In this paper, we make progress on understanding this mystery by providing a convergence analysis for SGD on a rich subset of two-layer feedforward networks with ReLU activations. This subset is characterized by a special structure called "identity mapping". We prove that, if input follows from Gaussian distribution, with standard O(1/√d) initialization of the weights, SGD converges to the global minimum in polynomial number of steps. Unlike normal vanilla networks, the "identity mapping" makes our network asymmetric and thus the global minimum is unique. To complement our theory, we are also able to show experimentally that multi-layer networks with this mapping have better performance compared with normal vanilla networks.Our convergence theorem differs from traditional non-convex optimization techniques. We show that SGD converges to optimal in "two phases": In phase I, the gradient points to the wrong direction, however, a potential function g gradually decreases. Then in phase II, SGD enters a nice one point convex region and converges. We also show that the identity mapping is necessary for convergence, as it moves the initial point to a better place for optimization. Experiment verifies our claims.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {597–607},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}


@Book{probability_book,
author = {Robert V. Hogg and Elliot A. Tanis and Dale L. Zimmerman},
title = {Probablity and Statistical Inference, 9th Edition},
year = {2015},
publisher = {Pearson},
pages = {3},
address = {Upper Saddle River},
}

@Book{uncertainty_principle,
author = {T. Britannica},
title = {uncertainty principle},
year = {2023},
publisher = {Encyclopedia Britannica},
url = {https://www.britannica.com/science/uncertainty-principle},
}

@book{monte_carlo,
  added-at = {2017-01-09T13:57:26.000+0100},
  author = {Robert, C.P. and Casella, G.},
  biburl = {https://www.bibsonomy.org/bibtex/2c45ba78e01455a35b301daddc182caed/yourwelcome},
  interhash = {faaab58e84fb4966b7d2118bc839c076},
  intrahash = {c45ba78e01455a35b301daddc182caed},
  keywords = {Carlo, Markov Monte Sampler, carlo, chain gibbs, monte simulations},
  publisher = {Springer Verlag},
  timestamp = {2017-01-09T14:01:11.000+0100},
  title = {Monte {Carlo} statistical methods},
  year = 2004
}

@book{kolmogorov_probability,
  added-at = {2025-06-23T14:51:00.000+0800},
  author = {Kolmogorov, A.N.},
  keywords = {probability theory, foundations, Kolmogorov},
  publisher = {Chelsea Publishing Company},
  timestamp = {2025-06-23T14:51:00.000+0800},
  title = {Foundations of the Theory of Probability},
  year = 1933 
}

@book{bernardo_smith_bayesian_theory,
  author    = {José M. Bernardo and Adrian F. M. Smith},
  title     = {Bayesian Theory},
  publisher = {John Wiley \& Sons, Inc.},
  year      = {1994},
  isbn      = {9780471494645},
  doi       = {10.1002/9780470316870},
  series    = {Wiley Series in Probability and Statistics},
  address   = {New York, NY, USA},
  edition   = {1},
}

@book{neal_bayes_network,
author = {Neal, Radford M.},
title = {Bayesian Learning for Neural Networks},
year = {1996},
isbn = {0387947248},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
}

@inproceedings{gal_mc_dropout,
author = {Gal, Yarin and Ghahramani, Zoubin},
title = {Dropout as a Bayesian approximation: representing model uncertainty in deep learning},
year = {2016},
publisher = {JMLR.org},
abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs - extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {1050–1059},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@inproceedings{lakshminarayanan_deep,
author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
title = {Simple and scalable predictive uncertainty estimation using deep ensembles},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6405–6416},
numpages = {12},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@book{koller_pgms,
author = {Koller, Daphne and Friedman, Nir},
title = {Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning},
year = {2009},
isbn = {0262013193},
publisher = {The MIT Press},
abstract = {Most tasks require a person or an automated system to reasonto reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality. Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty. The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss empirical cases related to the approach described in the text, including applications in computer vision, robotics, natural language understanding, and computational biology; and concept boxes, which present significant concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in various combinations, from core topics to more technically advanced material, to suit their particular needs. Adaptive Computation and Machine Learning series}
}

@inproceedings{pearl_bayes_network,
  author    = {Pearl, Judea},
  title     = {Bayesian Networks: A Model of Self-Activated Memory for Evidential Reasoning},
  booktitle = {Proceedings of the Seventh Annual Conference of the Cognitive Science Society},
  address   = {Irvine},
  date      = {1985-04-15/1985-04-17},
  year      = {1985},
  pages     = {329--334},
  url       = {http://ftp.cs.ucla.edu/tech-report/198_-reports/850017.pdf},
  note      = {UCLA Technical Report CSD-850017}
}

@article{Goguen_funzzy_control, 
title={L. A. Zadeh. Fuzzy sets. Information and control, vol. 8 (1965), pp. 338–353. - L. A. Zadeh. Similarity relations and fuzzy orderings. Information sciences, vol. 3 (1971), pp. 177–200.}, 
volume={38},
DOI={10.2307/2272014}, 
number={4}, 
journal={Journal of Symbolic Logic}, 
author={Goguen, J. A.}, 
year={1973}, 
pages={656–657}} <div></div>



@article{VAE_use_summary,
  title={Latent Reconstruction-Aware Variational Autoencoder},
  author={Onur Boyar and Ichiro Takeuchi},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.02399}
}

@inbook{VAE_use_image,
author = {Razavi, Ali and van den Oord, A\"{a}ron and Vinyals, Oriol},
title = {Generating Diverse High-Fidelity Images with VQ-VAE-2},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {1331},
numpages = {11}
}

@inproceedings{VAE_use_anomaly,
author = {Xu, Haowen and Chen, Wenxiao and Zhao, Nengwen and Li, Zeyan and Bu, Jiahao and Li, Zhihan and Liu, Ying and Zhao, Youjian and Pei, Dan and Feng, Yang and Chen, Jie and Wang, Zhaogang and Qiao, Honglin},
title = {Unsupervised Anomaly Detection via Variational Auto-Encoder for Seasonal KPIs in Web Applications},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3185996},
doi = {10.1145/3178876.3185996},
abstract = {To ensure undisrupted business, large Internet companies need to closely monitor various KPIs (e.g., Page Views, number of online users, and number of orders) of its Web applications, to accurately detect anomalies and trigger timely troubleshooting/mitigation. However, anomaly detection for these seasonal KPIs with various patterns and data quality has been a great challenge, especially without labels. In this paper, we proposed Donut, an unsupervised anomaly detection algorithm based on VAE. Thanks to a few of our key techniques, Donut greatly outperforms a state-of-arts supervised ensemble approach and a baseline VAE approach, and its best F-scores range from 0.75 to 0.9 for the studied KPIs from a top global Internet company. We come up with a novel KDE interpretation of reconstruction for Donut, making it the first VAE-based anomaly detection algorithm with solid theoretical explanation.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {187–196},
numpages = {10},
keywords = {anomaly detection, seasonal KPI, variational auto-encoder},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{VAE_use_denoise,
author = {Im, Daniel Jiwoong and Ahn, Sungjin and Memisevic, Roland and Bengio, Yoshua},
title = {Denoising Criterion for Variational Auto-Encoding Framework},
year = {2017},
publisher = {AAAI Press},
abstract = {Denoising autoencoders (DAE) are trained to reconstruct their clean inputs with noise injected at the input level, while variational autoencoders (VAE) are trained with noise injected in their stochastic hidden layer, with a regularizer that encourages this noise injection. In this paper, we show that injecting noise both in input and in the stochastic hidden layer can be advantageous and we propose a modified variational lower bound as an improved objective function in this setup. When input is corrupted, then the standard VAE lower bound involves marginalizing the encoder conditional distribution over the input noise, which makes the training criterion intractable. Instead, we propose a modified training criterion which corresponds to a tractable bound when input is corrupted. Experimentally, we find that the proposed denoising variational autoencoder (DVAE) yields better average log-likelihood than the VAE and the importance weighted autoencoder on the MNIST and Frey Face datasets.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {2059–2065},
numpages = {7},
location = {San Francisco, California, USA},
series = {AAAI'17}
}