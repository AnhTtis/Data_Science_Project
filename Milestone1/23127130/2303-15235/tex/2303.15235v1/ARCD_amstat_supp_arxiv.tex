\documentclass[a4paper,11pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[utf8]{inputenc}
%\usepackage[swedish]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{epsfig}
\usepackage{epic}
\usepackage{setspace}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newenvironment{proof}[1][Proof]{\noindent \textbf{#1.} }{\  \rule{0.5em}{0.5em}}
\newcommand{\dlim}{\xrightarrow{\cal L}}
\newcommand{\plim}{\xrightarrow{p}}

\onehalfspacing

\begin{document}

\title{Confidence distributions for the autoregressive parameter, supplemental material}
%\author{Rolf Larsson\footnote{Dept of Mathematics, Uppsala University, P.O.Box 480, SE-751 06 Uppsala, Sweden, rolf.larsson@math.uu.se.}}
\date{}
\maketitle


\section*{Appendix 1: Simulation setups}

In this Appendix, we shortly describe the numerical calculations and simulations that lie behind the figures of the paper. 

\subsection*{Figures 1-3: Confidence distributions}

The simulation to calculate confidence distributions, that gives Figures 1-3, is as follows.

\begin{enumerate}
\item The observed MLE $\hat\phi_{obs}$ is specified beforehand.
\item Put $h=(1-\phi_{min})/n_\phi$, where $n_\phi$ is some large number. Then, for each $\phi\in\{\phi_{min},\phi_{min}+h,\phi_{min}+2h,...,1\}$, calculate the simulated (empirical) confidence distribution function  $C(\phi)$ as follows:
\begin{enumerate}
\item For each simulation replicate $r=1,...,N$ for some large $N$ do
\begin{enumerate}
\item Given some large $n$, generate independent standard normal random  random variables $\varepsilon_1,...,\varepsilon_n$. 
\item Put $y_1=\varepsilon_1$.
\item For $t=2,...,n$, put $y_t=\phi y_{t-1}+\varepsilon_t$.
\item Calculate the MLE $\hat\phi$ from the generated sample $y_1,...,y_n$.
\item Check if $\hat\phi>\hat\phi_{obs}$.
\end{enumerate}
\item Estimate $C(\phi)$ as the proportion of replicates where $\hat\phi>\hat\phi_{obs}$ (cf (8)).
\item For all $\phi\in\{\phi_{min},\phi_{min}+h,\phi_{min}+2h,...,1\}$, calculate the asymptotic approximations (9) and (10).
\end{enumerate}
\end{enumerate}
To get Figure 3, use $cc(\phi)=|1-2C(\phi)|$, where $C(\phi)$ is the simulated confidence distribution function from above.

\subsection*{Figures 4-6: Confidence densities}

To get the confidence densities in Figure 4, the procedure is as follows.

\begin{enumerate}
\item Fix the `true' parameter value $\phi=\phi_0$. (For Figure 4, we have $\phi_0=0.5$.)
\item Given some large $n$, generate independent standard normal random  random variables $\varepsilon_1,...,\varepsilon_n$. 
\item Put $x_1=\varepsilon_1$.
\item For $t=2,...,n$, put $x_t=\phi_0 x_{t-1}+\varepsilon_t$.
\item Calculate the observed MLE, $\hat\phi_{obs}$, from the generated sample $x_1,...,x_n$.
\item Put $h=(\phi_{max}-\phi_{min})/n_\phi$, where $n_\phi$ is some large number. Then, for each $\phi\in\{\phi_{min},\phi_{min}+h,\phi_{min}+2h,...,\phi_{max}\}$, calculate the simulated (empirical) confidence distribution function  $C(\phi)$ as follows (this is the same as for Figures 1-3):
\begin{enumerate}
\item Given some large $n$, generate independent standard normal random  random variables $\varepsilon_1,...,\varepsilon_n$. 
\item Put $y_1=\varepsilon_1$.
\item For $t=2,...,n$, put $y_t=\phi y_{t-1}+\varepsilon_t$.
\item Calculate the MLE $\hat\phi$ from the generated sample $y_1,...,y_n$.
\item Check if $\hat\phi>\hat\phi_{obs}$ (cf (8)).
\end{enumerate}
\item Estimate $C(\phi)$ as the proportion of replicates where $\hat\phi>\hat\phi_{obs}$.
\item To avoid numerical problems, select all $\phi\in\{\phi_{min},\phi_{min}+h,\phi_{min}+2h,...,\phi_{max}\}$ that are such that $0.01<C(\phi)<0.99$. 
\item For these $\phi$, calculate $\Phi^{-1}\{C(\phi)\}$ and regress on $\phi$ to get the least squares estimates $\hat a$ and $\hat b$ and the `smoothed' estimated confidence distribution function $C_s(\phi)=\Phi(\hat a+\hat b\phi)$. This gives the estimated confidence density as
$c_{emp}(\phi)=C_s'(\phi)=\hat b\varphi(\hat a+\hat b\phi)$.
\item The asymptotic confidence densities are obtained by inserting $\hat\phi_{obs}$ and all selected $\phi$ into (15) and (16).
\end{enumerate}

The confidence densities used for the implied priors of Figures 5 and 6 are obtained in the same way. To get the implied priors, we add calculation of the log of (18) with the generated sample from step 5 above, $x_1,...,x_n$, and $\hat\phi_{obs}$ inserted. For Figure 6, the MLE of $\sigma^2$ from (22) is used.

\subsection*{Figures 8-10:  Bootstrap}

To get the confidence curve in Figure 8, the procedure is as follows.

\begin{enumerate}
\item Input is the observed series $y_1,y_2,...,y_n$, for which we calculate the MLE of the AR coefficient $\hat\phi_{obs}$ from (7).
\item Generate an $n\times N$ matrix of bootstrap indices from the discrete uniform distribution on $1,2,...,n$, where $N$ is the number of bootstrap replicates below. (This is done once and for all, so that for stability, we get the same bootstrap scheme for all $\phi$.)
\item Put $h=(1-\phi_{min})/n_\phi$, where $n_\phi$ is some large number. 
\item For a large number $N$ of bootstrap replicates, and for all $\phi\in\{\phi_{min},\phi_{min}+h,\phi_{min}+2h,...,1\}$:
\begin{enumerate}
\item Resample the `residuals' $e_t=y_t-\phi y_{t-1}$ according to the bootstrap index matrix.
\item Generate a new series $\{y_t^*\}$, where $y_1^*=e_1$, and for $t=2,...,n$, $y_t^*=\phi y_{t-1}^*+e_t$.
\item Calculate the MLE $\hat\phi$ from this series and check if $\hat\phi>\hat\phi_{obs}$.
\end{enumerate}
\item For all $\phi$, estimate $C(\phi)$ as the proportion of bootstrap replicates where $\hat\phi>\hat\phi_{obs}$.
\item Calculate the bootstrap confidence curve through $cc(\phi)=|1-2C(\phi)|$.
\end{enumerate}

For Figure 9, we treat the likelihood, normed by the likelihood at $\phi=1$ as described in section 4, as a confidence distribution function, and then transform it to a confidence curve in the usual way.

The Bayesian confidence curve in Figure 10 is based on the modified confidence distribution function obtained by multiplying the previous Bayesian confidence distribution function with $C(1)$ from Figure 8.



\section*{Appendix 2: Proof of proposition 1}
With $B$ as in (21) in the main text, we will show that $n^{-1}B$ converges to $-\sigma^2$, from which the result easily follows.

By e.g. Shumway and Stoffer (2017), p. 125, we have that as $n\to\infty$, denoting convergence in distribution by $\dlim$,  
$$\sqrt{n}(\hat\phi_{obs}-\phi_0)\dlim U,$$
for a normally distributed variate $U$ with expectation zero and variance $1-\phi_0^2$. We write this as
\begin{equation}
\hat\phi_{obs}=\phi_0+n^{-1/2}U+o_p(n^{-1/2}).\label{phihat}
\end{equation}
Now, (\ref{phihat}) implies
\begin{align}
&(\hat\phi_{obs}-\phi)^2\notag\\
&=\left\{(\hat\phi_{obs}-\phi_0)+(\phi_0-\phi)\right\}^2
=\left\{n^{-1/2}U+o_p(n^{-1/2})+(\phi_0-\phi)\right\}^2\notag\\
&=(\phi_0-\phi)^2+2n^{-1/2}(\phi_0-\phi)U+o_p(n^{-1/2}).
\label{phiobsphi}
\end{align}
Similarly, we have
\begin{align}
&1-2\hat\phi_{obs}\phi+\phi^2\notag\\
&=1-2\left\{(\hat\phi_{obs}-\phi_0)+\phi_0\right\}\phi+\phi^2\notag\\
&=1-2\left(n^{-1/2}U+o_p(n^{-1/2})+\phi_0\right)\phi+\phi^2\notag\\
&=1-2\phi_0\phi+\phi^2-2n^{-1/2}U+o_p(n^{-1/2})\notag\\
&=(\phi_0-\phi)^2+1-\phi_0^2-2n^{-1/2}U+o_p(n^{-1/2}).\label{phiobsphi2}
\end{align}
Moreover, by recursion,
\begin{equation}
y_t=\phi_0 y_{t-1}+\varepsilon_t=\phi_0(\phi_0 y_{t-2}+\varepsilon_{t-1})+\varepsilon_t=...=\sum_{i=0}^{t-1}\phi_0^i\varepsilon_{t-i}.
\label{yt}
\end{equation}
Consequently, for each $t$, $y_t$ is normal with expectation zero and variance $(1-\phi_0^{2t})/(1-\phi_0^2)$. This means that as $n\to\infty$, $(1-\phi_0^2)y_n^2$ is asymptotically $\chi^2$ with one degree of freedom, i.e. $y_n^2=O_p(1)$. 

Furthermore, rewriting (\ref{yt}) as
$$y_t=\sum_{j=1}^t\phi_0^{t-j}\varepsilon_j,$$
we obtain ($i\vee j$ is the maximum of $i$ and $j$)
\begin{align*}
&\sum_{t=1}^n y_{t-1}^2=\sum_{t=0}^{n-1} y_t^2
=\sum_{t=0}^{n-1}\sum_{i=1}^t\sum_{j=1}^t\phi_0^{2t-i-j}
\varepsilon_i\varepsilon_j
=\sum_{i=1}^{n-1}\sum_{j=1}^{n-1}\phi_0^{-i-j}\varepsilon_i\varepsilon_j\sum_{t=i\vee j}^{n-1}\phi_0^{2t}\\
&=
\sum_{i=1}^{n-1}\sum_{j=1}^{n-1}\phi_0^{-i-j}\varepsilon_i\varepsilon_j\frac{\phi_0^{2(i\vee j)}-\phi_0^{2n}}{1-\phi_0^2}\\
&=\frac{1}{1-\phi_0^2}\sum_{i=1}^{n-1}\sum_{j=1}^{n-1}\phi_0^{|i-j|}
\varepsilon_i\varepsilon_j
-\frac{\phi_0^{2n}}{1-\phi_0^2}\sum_{i=1}^{n-1}\sum_{j=1}^{n-1}
\phi_0^{-i-j}\varepsilon_i\varepsilon_j.
\end{align*}
It follows that 
\begin{equation}
E\left(\sum_{t=1}^n y_{t-1}^2\right)
=\frac{1}{1-\phi_0^2}\sum_{i=1}^{n-1}E(\varepsilon_i^2)+O(1)
=\frac{(n-1)\sigma^2}{1-\phi_0^2}+O(1).
\label{Esumy2}
\end{equation}
To find the variance, we similarly get 
\begin{align*}
&E\left\{\left(\sum_{t=1}^n y_{t-1}^2\right)^2\right\}\\
&=\frac{1}{(1-\phi_0^2)^2}
\sum_{i=1}^{n-1}\sum_{j=1}^{n-1}\sum_{k=1}^{n-1}\sum_{l=1}^{n-1}\phi_0^{|i-j|+|k-l|}E(\varepsilon_i\varepsilon_j\varepsilon_k\varepsilon_l)+O(n)\\
&=\frac{1}{(1-\phi_0^2)^2}\left\{
\sum_{i=1}^{n-1}\sum_{k=1}^{n-1}E(\varepsilon_i^2\varepsilon_k^2)
+2\sum_{i=1}^{n-1}\sum_{j=1}^{n-1}\phi_0^{2|i-j|}
E(\varepsilon_i^2\varepsilon_j^2)\right\}+O(n),
\end{align*}
where the second double sum is $O(n)$ while the first one is
$$\sum_{i=1}^{n-1}E(\varepsilon_i^4)+\sum_{i\neq j}E(\varepsilon_i^2)E(\varepsilon_k^2)
=(n-1)\cdot 3\sigma^4+(n-1)(n-2)\sigma^4
=(n-1)(n+1)\sigma^4,
$$
so that
$$E\left\{\left(\sum_{t=1}^n y_{t-1}^2\right)^2\right\}
=\frac{(n-1)(n+1)\sigma^4}{(1-\phi_0^2)^2}+O(n),$$
and via (\ref{Esumy2}), we get the variance
\begin{equation}
V\left(\sum_{t=1}^n y_{t-1}^2\right)=
\frac{2(n-1)\sigma^4}{(1-\phi_0^2)^2}+O(n)=O(n).\label{Vsumy2}
\end{equation}
Now, (\ref{Vsumy2}) implies that the variance of $n^{-1}\sum_{t=1}^n y_{t-1}^2$ tends to zero as $n\to\infty$, so by Chebychev's inequality, $n^{-1}\sum_{t=1}^n y_{t-1}^2$ converges in probability to its asymptotic expectation, which from (\ref{Esumy2}) is $\sigma^2/(1-\phi_0^2)$, cf Gnedenko (1989) p. 248. Indeed, it follows from the proof of that inequality that
$$n^{-1}\sum_{t=1}^n y_{t-1}^2=\frac{\sigma^2}{1-\phi_0^2}+O_p(n^{-1}).$$


Hence, via (21), (\ref{phihat}), (\ref{phiobsphi}), (\ref{phiobsphi2}) and the fact that $y_n=O_p(1)$, Taylor expansion yields
\begin{align*}
\frac{1}{n\sigma^2}B&=\frac{(\phi_0-\phi)^2+2n^{-1/2}(\phi_0-\phi)U+o_p(n^{-1/2})}{1-\left\{\phi_0+n^{-1/2}U+o_p(n^{-1/2})\right\}^2}
+O_p(n^{-1})\\
&-\left\{\frac{1}{1-\phi_0^2}+O_p(n^{-1})\right\}
\left\{(\phi_0-\phi)^2+1-\phi_0^2-2n^{-1/2}U+o_p(n^{-1/2})\right\}\\
&=\frac{(\phi_0-\phi)^2}{1-\phi_0^2}\left\{1+2n^{-1/2}U\left(\frac{1}{\phi_0-\phi}+\frac{\phi_0}{1-\phi_0^2}\right)+o_p(n^{-1/2})\right\}\\
&-\frac{(\phi_0-\phi)^2}{1-\phi_0^2}\left\{1+\frac{1-\phi_0^2}{(\phi_0-\phi)^2}+2n^{-1/2}U\frac{1}{(\phi_0-\phi)^2}+o_p(n^{-1/2})\right\}\\
&=-1-2n^{-1/2}g(\phi,\phi_0)U+o_p(n^{-1/2}),
\end{align*}
where
$$g(\phi,\phi_0)=\frac{1-\phi_0-\phi_0^2+\phi(1+\phi_0^2)-\phi^2\phi_0}{(1-\phi_0^2)^2}.$$
Thus, from (20), noting that $U$ has variance $1-\phi_0^2$, proposition 1 follows.

\newpage

\section*{Appendix 3: Extra figures}

In this appendix, some extra figures are provided. The first one is as Figure 4 but with $\hat\phi_{obs}=0.815$. Just as in Figure 4, we find that the alternative asymptotic approximation comes closest to the empirical confidence density.

Figures 2 and 3 here complement Figures 5 and 6 in the main text. Here, the known parameter is $\phi_0=0.8$. The convergence to the constant asymptotic prior is similar to the figures in the main text.

Finally, Figures 4 and 5 give another perspective, in that it is the true parameter $\phi_0$ that is varied while $n=800$ stays fixed. Observe the more detailed scale on the $y$ axis compared to previous figures. It is seen that the implied priors have a tendency to have a slope close to zero in the near vicinity of the true parameter.


\begin{figure}[htb]
\begin{center}
\includegraphics[width=100mm]{fig1app-eps-converted-to.pdf}
\end{center}
\caption{Log confidence density functions, for $n=100$, $10^5$ replicates.  In this simulation, $\hat\phi_{obs}=0.815$.
}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=100mm]{fig2app-eps-converted-to.pdf}
\end{center}
\caption{Simulated logs of implied priors $c_{emp}(\phi)/L(\phi,\sigma^2)$ for generating parameters $\phi_0=0.8$, $\sigma^2=1$, known $\sigma^2$. We have 1000 replicates for each $\hat\phi_{obs}$, and 50 000 replicates of $\hat\phi_{obs}$.
}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=100mm]{fig3app-eps-converted-to.pdf}
\end{center}
\caption{Simulated logs of implied priors $c_{emp}(\phi)/L(\phi,\sigma^2)$ for generating parameters $\phi_0=0.8$, $\sigma^2=1$, unknown $\sigma^2$. We have 1000 replicates for each $\hat\phi_{obs}$, and 50 000 replicates of $\hat\phi_{obs}$.
}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=100mm]{fig4app-eps-converted-to.pdf}
\end{center}
\caption{Simulated logs of implied priors $c_{emp}(\phi)/L(\phi,\sigma^2)$ for $n=800$, $\sigma^2=1$, known $\sigma^2$. We have 1000 replicates for each $\hat\phi_{obs}$, and 50 000 replicates of $\hat\phi_{obs}$.
}
\end{figure}

\begin{figure}[htb]
\begin{center}
\includegraphics[width=100mm]{fig5app-eps-converted-to.pdf}
\end{center}
\caption{Simulated logs of implied priors $c_{emp}(\phi)/L(\phi,\sigma^2)$ for $n=800$, $\sigma^2=1$, unknown $\sigma^2$. We have 1000 replicates for each $\hat\phi_{obs}$, and 50 000 replicates of $\hat\phi_{obs}$.
}
\end{figure}


\end{document}
%==============================================================
