

We propose panoptic one-click segmentation which is a novel offline system to produce pseudo-labels from minimal user input.
In particular, we use a single click per object to produce pseudo-labels which can then be used to train other systems (e.g. in a semi-supervised learning pipeline).
Compared to traditional one-click segmentation which performs a forward pass for each of the $N$ objects in the image, panoptic one-click segmentation jointly estimates the pixel-wise location of all $N$ objects in a single forward pass.
Jointly processing all $N$ objects in a single pass reduces training and inference time by an order of magnitude.

We initially describe the traditional one-click segmentation approach, including a variant in which we incorporate negative instance clicks.
We then describe our panoptic one-click segmentation approach which jointly estimates the pixel-wise location of all objects in the scene.
Finally, we outline how this panoptic segmentation is able to recover missed or lost clicks during the annotation phase.


\subsection{One-click segmentation baseline} \label{sec:proposed:baseline}

\begin{figure}[t!]
 \vspace{4pt}
 \centering
    \includegraphics[width=.46\textwidth]{data/standard.png}
    \caption{Overview of the baseline one-click semantic segmentation that we use.
    It consists of an encoder-decoder structure (in blue) which outputs all the pixels associated to one object.
    This is based on an input consisting of the RGB image and a click transform map for the corresponding positive click (in green) as a fourth channel.
    Optionally, we extend this system by using all $N$ object clicks, by adding the $N-1$ negative clicks (in red) on other objects within a second click map. 
    Click maps are colored only for visualization purposes.}
     \label{fig:standard}
     \vspace{-16pt}
\end{figure}

The baseline for our one-click object segmentation is based on the concept of~\cite{Xu2016}.
A click, $C_{(h,w)}$, is represented within a map which has the same size as the original image space $H\times{W}$.
Following more recent approaches \cite{Maninis2018,Lin2020}, we replace the originally used Euclidean distance transform and use a 2-D Gaussian instead, here with a standard deviation of 8.
This map is given as an extra input to a standard semantic segmentation network resulting in a four channel input RGB plus click transform  map.

When there are multiple objects (clicks) in the image the above procedure has to be applied iteratively.
At both training and inference time, each of the $N$ objects is treated separately such that $N$ semantic segmentation maps are produced. 
We refer to this approach as the standard one-click baseline.
A disadvantage of this approach is that it greatly increases the computational requirements as the same image is processed $N$ times.

\begin{figure*}[t!]
    \vspace{8pt}
    \centering
    \includegraphics[width=.8\textwidth]{data/scheme.png}
    \caption{Overview of our panoptic one-click segmentation system which is based on panoptic deeplab~\cite{Cheng2020}.
    It takes as input the RGB image (3 channels) and a click transform map (clicks) as the fourth channel.
    Our system consists of two heads, one to estimate the vertical and horizontal offset to the center (offsets) and one for semantic segmentation.
    We disable the center estimation head and use the click centers from the user instead.
    The final output is instance-based object segmentation and is resolved as per~\cite{Cheng2020}.}
    \label{fig:scheme}
    \vspace{-16pt}
\end{figure*}


While this iterative approach is computationally expensive it does include the potential to be enhanced.
As all $N$ objects are annotated using a single click, for each positive click the remaining $N-1$ clicks from the other objects can be considered negative clicks.
These negative clicks are able to provide further information about the scene that a single positive click technique is not able to provide.
We encode all positive and negative clicks into a combined, second click map using the same Gaussian encoding as for the standard one-click approach. 
This full click transform map is then given to the network as the fifth channel in the input.
An overview of this system is provided in \Cref{fig:standard}.


\subsection{Panoptic one-click segmentation} \label{sec:proposed:panoptic}

We propose a panoptic one-click segmentation approach which jointly resolves the location of all objects in a single pass.
Our novel approach is based on Panoptic-Deeplab~\cite{Cheng2020} which we adapt to perform one-click-per-object segmentation for all $N$ objects within an image.
Panoptic-Deeplab produces instance segmentation labels using three output representations, depicted in~\Cref{fig:scheme}: (1) an object center map, (2) a center offset map, and (3) a semantic segmentation map.
The existence of an object (plant) is defined by a valid center location in the object center map.
The offset of every object (plant) pixel to its center location is estimated in the offset map and used to assign each pixel to the closest center location; the details of this post-processing algorithm are described in~\cite{Cheng2020}.
The segmentation map is a pixel-wise map of \textit{things} which in this case are the plants and \textit{stuff} which is everything else (e.g. background). 
Combining all three maps provides the instance-based segmentation mask.


For our panoptic one-click segmentation system only two heads are predicted by the network, these being the semantic segmentation map and the center offsets.
This is because the object centers are supplied by the annotator as a click map and so do not need to be predicted by the network.
Instead, the click map is provided as an extra input channel and is also provided as the object center locations as depicted in~\Cref{fig:scheme}. 


\subsection{Panoptic system to recover from missing clicks} \label{sec:proposed:panoptic_miss} 
A potential advantage of panoptic one-click segmentation is the ability to estimate the location of object centers (e.g. clicks).
This would allow us to deal with potential annotation errors such as missing clicks.
To explore this possibility, we re-introduce the object center location estimation as the third head in the network. 
The input clicks are still used as the fourth channel at the input of the network, however, the object locations are now fully estimated by the network.
This means that we no longer use the clicks from the annotators in the post-processing stage.
A potential downside of this approach is that the center locations given by the user are only used as the input to the network.
