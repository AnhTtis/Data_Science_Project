
\begin{table}[!b]
\vspace{-10pt}
\caption{Object segmentation results (mean object IoU) for the different one-click models trained on only 10\% of the instances in our \textit{SB20} and \textit{CN20} datasets.} \label{tab:results:weakseg}
\centering
\begin{tabular}{lrr}
\toprule
Model variant & \textit{SB20} & \textit{CN20} \\
\midrule
Baseline (standard one-click) & 67.7 & 68.0 \\
Additional negative clicks & \textbf{69.5} & \textbf{71.4} \\
Panoptic system & 68.1 & 68.8 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t!]
    \vspace{8pt}
    \centering
    \includegraphics[width=.48\textwidth]{data/weakseg_examples.png}
    \caption{Selected one-click segmentation predictions from our best traditional model (with additional negative clicks; center) or panoptic model (right). Edge cases where both approaches show errors and considerable differences. Red highlights show misclassification errors resulting in overlaps for the traditional system. Yellow highlights show the non-overlapping errors of our panoptic system in the same regions. Image crops have different zoom factors.}
    \label{fig:results:weakseg}
    \vspace{-12pt}
\end{figure}

We perform three sets of experiments.
First, we quantitatively compare traditional one-click segmentation approaches against our proposed one-click panoptic segmentation approach.
Furthermore, we compare the relative training time, highlighting that our panoptic approach can be trained an order of magnitude faster.

The second set of experiments evaluate the one-click segmentation methods, traditional and panoptic, for the task of semi-supervised learning.
Specifically, the one-click approaches are trained using 10\% of the training data and are then used to generate pseudo-labels for the remaining 90\% of the data.
All of this data, 10\% manual annotations and 90\% pseudo-labels, is then used to train an instance-based segmentation network (Mask R-CNN).

The third set of experiments are an ablation study to evaluate the potential of our panoptic one-click segmentation systems to recover from missing clicks.
We compare recognition quality performance of our panoptic system and different model variants trained with varying amounts of input clicks missing.


\subsection{One-click segmentation}
Here, we present results from our first set of experiments regarding general one-click segmentation performance of our models.


\begin{table}[bp]
\vspace{-10pt}
\caption{Instance segmentation results (mean foreground IoU) of Mask R-CNN when trained using 10\% of the data and 100\% of the data compared to the semi-supervised approaches using standard one-click, one-click with negative clicks, and our proposed one-click panoptic approach.}
\label{table:results:mrcnn}
\centering
\begin{tabular}{lrr}
\toprule
Model variant & \textit{SB20} & \textit{CN20} \\
\midrule
10\% data subset & 28.6 & 31.7 \\
Fully supervised & \textbf{42.7} & \textbf{41.9} \\
\midrule
Semi-supervised (standard one-click) & 37.7 & 38.8 \\
Semi-supervised (with negative clicks) & \textbf{40.9} & 38.5 \\
Semi-supervised (panoptic system) & 38.0 & \textbf{39.6} \\
\bottomrule
\end{tabular}
\end{table}


\subsubsection{Quantitative performance}
In \Cref{tab:results:weakseg}, we report one-click segmentation performance (mIoU) for (1) the \textit{standard one-click} baseline, (2) its extension \textit{with negative clicks} and (3) our novel \textit{panoptic one-click method}.
Including the annotations from remaining objects as negative clicks increases baseline performance by 1.8\% and 3.4\% for \textit{SB20} and \textit{CN20}, respectively.
These results highlight that they should be included whenever available if considering a traditional one-click segmentation system.
Our new method shows similar performance to the stardard one-click approach, increasing 0.4\% on \textit{SB20} and 0.8\% on \textit{CN20}.
The competitive performance of the panoptic one-click system when compared to traditional approaches outlines its suitability for generating psuedo-labels for semi-supervised learning.


\subsubsection{Qualitative comparison}
We demonstrate some qualitative differences between the presented approaches with prediction examples from both datasets in \Cref{fig:results:weakseg}.
We outline figures from the panoptic one-click and traditional negative clicks systems, with particular attention being paid to edge cases which show different types of prediction errors for both systems.
One benefit of the panoptic approach over the traditional approach is that panoptic vision inherently ensures no overlap between objects.
This is not the case for standard methods, which we show for some regions with overlapping misclassification errors highlighted in red.
Non-overlapping errors in the according regions for our panoptic system are further highlighted in yellow.

\subsubsection{Computational performance}
In our setup with a single GPU, we achieved training times of 55 minutes and 38 minutes with our panoptic one-click system on \textit{SB20} and \textit{CN20}, respectively, compared to 674 minutes and 448 minutes with the traditional one-click model with negative clicks.
For both datasets, \textit{SB20} and \textit{CN20}, this is a speed improvement of approximately a factor of 12 which means that our proposed panoptic one-click system is an order of magnitude faster to train.


\subsection{Semi-supervised instance segmentation}

In this experiment, we evaluate the performance of the generated pseudo-labels in a Mask R-CNN based semi-supervised system.
We evaluate this performance on five different systems: (1) using our 10\% data split; (2) fully supervised; (3) semi-supervised standard one-click; (4) semi-supervised one-click with negative clicks; and (5) our novel semi-supervised panoptic one-click.

\Cref{table:results:mrcnn} outlines the instance segmentation performance of these five systems.
We report the instance-wise segmentation performance using mean foreground IoU.
As expected, using just 10\% of the data results in the poorest performance across both datasets, and the fully supervised system performs best.
We use the fully supervised approach as our upper reference score and expect a good performing semi-supervised approach to obtain values commensurate with it.

When considering our semi-supervised approaches we see a consistent improvement over the 10\% data subsets.
On \textit{SB20} the negative clicks system yields close to fully supervised performance (40.9 versus 42.7).
Standard one-click performs slightly lower at 37.7, similar to our panoptic system at 38.0 which is a considerable 9.4 point increase when compared to not using click labels.
On \textit{CN20}, standard one-click and negative clicks models perform on the same level (38.8 and 38.5).
Our panoptic model at 39.6 gets closest to the fully supervised reference model with only a 2.3 point difference while outperforming the 10\% data subset by 7.9 points.

Finally, while the proposed panoptic approach does not consistently perform better than the traditional approach the speed benefits (better than an order of magnitude faster) mean that when rapid prototyping is required this approach is beneficial.
An additional potential benefit of using a panoptic system is that it can recover when clicks are missing, we evaluate this in the following ablation study.


\subsection{Ablation study - Recovering from missing input clicks}


\begin{table}[b!]
\caption{Panoptic segmentation performance with different percentages of missing input clicks of \textit{SB20} data.
First line is our original click segmentation model with input clicks used for post processing panoptic segmentation results.
Remaining models use the original scheme from \cite{Cheng2020} to detect centers using the center head.}
\label{tab:results:missing}
\centering
\begin{tabular}{lrrr}
\toprule
Percentage of Clicks Missing &    PQ &    SQ &    RQ \\
\midrule
0\% [centers provided by user (clicks)] &  75.0 &  85.0 &  86.0 \\
\midrule
0\% [centers predicted by network]  &  74.0 &  84.9 &  84.8 \\
25\%                      &  69.8 &  84.0 &  79.8 \\
50\%                     &  70.1 &  84.4 &  79.8 \\
75\%                     &  69.2 &  83.9 &  78.9 \\
100\%                     &  66.4 &  84.5 &  74.5 \\
\bottomrule
\end{tabular}
\end{table}

In these experiments we evaluate the potential for a panoptic one-click system to recover when clicks are missing.
For these experiments we use the available clicks as an extra input channel but we do not use them as the click locations for the output of the network.
Instead, we train the panoptic network to estimate the click locations and use these to derive the per-object pixel-wise segmentation map.

The results in \Cref{tab:results:missing} highlight that the panoptic one-click system has the potential to recover from missing clicks.
It can be seen, in the top two lines in the table, that using the panoptic one-click network to  predict the object center locations results in only slightly worse performance (in terms of RQ) than using the user input clicks.
The RQ performance drops by only 1.2 points going from 86.0 to 84.8 for user provided center locations and network predicted locations respectively.
When user clicks are removed (or missed) the RQ performance drops to 79.8 and remains at this level until 75\% of the clicks are removed (or missed).
With 75\% of the clicks missing the RQ performance degrades by only 0.9 points and when no clicks are given (100\% missing clicks) the system still has an RQ performance of 74.5.
This indicates that the panoptic system is able to well estimate the object locations, even when large parts (50\% or 75\%) of the input clicks are missing.

