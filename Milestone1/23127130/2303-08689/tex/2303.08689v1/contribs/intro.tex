
Weed control in agricultural settings is an important part of ensuring crop health and subsequently high yield.
For automated systems this is heavily influenced by technological developments from fields such as robotics and computer vision.
Vision based systems in combination with modern deep learning (DL) algorithms have become increasingly popular in this domain~\cite{Hasan2021, Zhang2022}.
Unfortunately, while these techniques can achieve high accuracy in the agricultural domain, they generally require large amounts of labelled data for training.
One approach to alleviate the associated costs with acquiring this labelled information is weak labelling.

\begin{figure}[t!]
\centering
    \includegraphics[width=.40\textwidth]{data/hero_scheme.png}
    \caption{Comparison of a traditional one-click segmentation approach (top) and our novel panoptic one-click segmentation approach (bottom).
    The traditional approach takes a \textit{positive} click and optionally \textit{negative} clicks as input and segments each of the $N$ objects in $N$ separate forward passes.
    By comparison, our approach uses a data representation from \cite{Cheng2020} and jointly resolves all $N$ clicks and objects in 1 forward pass.
    Clicks in RGB images and transform maps are highlighted and colored only for visualization.}
    \vspace{-16pt}
\label{fig:hero}
\end{figure}

Weak labels are considered any label that is less informative than fully annotated labels (i.e. pixel-wise labels for segmentation).
For weak segmentation approaches~\cite{Xu2016,Xu2017,Maninis2018,Zhang2020c,Forte2020}, the primary goal is to obtain fully annotated outputs (or as close to full as possible) from weak priors such as bounding boxes or even single instance clicks.
These methods can be used as a tool to assist interactive segmentation and speed up annotation or rendering tasks~\cite{Xu2016,Forte2020}, or as a base to provide labels for fully supervised systems~\cite{Dai2015,Khoreva2017}.
For these fully supervised systems the generated pseudo-labels can replace expensive manual pixel-wise labels for training segmentation networks, typically with the goal to achieve comparable performance to manual pixel-wise approaches.
Typically, semi-supervised techniques combine both manually annotated ground truth with pseudo-labels from weak learning approaches.
This creates large datasets with minimal expense associated with the labelling (i.e. single click per object compared to pixel-wise segmentation) for training DL algorithms.

In this paper we present an efficient and accurate method for weak labelling enabled by panoptic segmentation.
We introduce panoptic one-click segmentation, a novel offline tool to produce pseudo-labels from a single click (annotation) per object in the image.
This approach greatly simplifies the creation of novel datasets (e.g. for semi-supervised learning).
We compare our panoptic one-click segmentation approach to a traditional click segmentation method~\cite{Xu2016} adapted to this setting.
This traditional approach learns to produce a segmentation mask for an object based on the following input: an RGB image, one click for the object of interest and potentially one click for all of the other objects in the image.
This is a computationally inefficient technique as it processes each of the $N$ objects individually, this leads to long training times.
We overcome this inefficiency by employing panoptic segmentation~\cite{Cheng2020} to jointly resolve the segmentation mask for all $N$ objects simultaneously, as depicted in~\Cref{fig:hero}.

We evaluate our panoptic-based approach on two challenging arable farming (crop/weed) datasets from sugar beet and corn fields (see \Cref{sec:experiments:datasets}). 
Furthermore, we demonstrate the practical applicability of this one-click segmentation approach to train Mask R-CNN using just 10\% of the manually annotated ground truth; the remaining 90\% of the data is generated as pseudo-labels using our approach.
In doing so, we make the following contributions:
\begin{itemize}
    \item we propose panoptic one-click segmentation, a novel offline tool for producing pseudo-labels using just one click per object which yields competitive performance to a traditional approach while being an order of magnitude faster to train; and
    \item we demonstrate the practical applicability of our approach to train Mask R-CNN using only 10\% of the manually annotated ground truth with the remaining 90\% of the data consisting of pseudo-labels generated using our panoptic one-click segmentation approach; and
    \item we demonstrate the potential for our panoptic one-click segmentation system to accurately recover missed clicks from the annotation process.
\end{itemize}

