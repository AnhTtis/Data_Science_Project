We perform all experiments on two challenging agricultural weeding datasets.
Images typically contain multiple, frequently overlapping plant instances from various species and sizes.
Here, we describe both datasets in more detail and further provide information about input clicks, network implementations and evaluation metrics.
We implement our approach using PyTorch and all experiments were conducted on a single A6000 graphical processing unit (GPU).

\subsection{Datasets} \label{sec:experiments:datasets}


For all experiments we use two datasets that we refer to as \textit{SB20}~\cite{Ahmadi2021,Smitt2022,Halstead2021} (sugar beet 2020) and \textit{CN20}~\cite{Ahmadi2022a} (corn 2020).
Both datasets contain instance-level manually annotated data for both training and inference and contain varying weed species, sizes, and densities (see \Cref{fig:dataset_examples}).

These two datasets resemble typical arable farming settings.
The two crops, sugar beet and corn, are common crops (in Europe) and are highly variable in appearance.
Sugar beet has thick leaves and rosette-like shapes while corn consists of thin, long and irregularly twisted leaves.
The weeds present in both datasets (6 species for \textit{SB20} and 7 species for \textit{CN20}) likewise show strong diversity in appearance.
Both datasets are captured on a robot designed for crop monitoring and weed management~\cite{Ahmadi2022a} using a nadir Intel RealSense D435i camera.
\textit{SB20} consists of 71 training ($\sim$50\%), 37 validation ($\sim$26\%) and 35 evaluation ($\sim$25\%) images with a resolution of $640\times480$ and a total of 1424 plant instances in the training set.
\textit{CN20} consists of 150 training ($\sim$64\%), 40 validation (17\%) and 45 evaluation ($\sim$19\%) images with a resolution of $1280\times{720}$ and a total of 1781 plant objects for training.
\textit{CN20} is resized to $704\times416$ similarly to~\cite{Ahmadi2021}.

For our one-click segmentation experiments, we manually sub-sampled images from the training sets to contain roughly 10\% of the original object annotations (157 and 154 for \textit{SB20} and \textit{CN20}, respectively).
For our semi-supervised experiments, we use our one-click segmentation models to create pseudo-labels to replace the original annotations in the remaining 90\% of training data.


\subsection{Click inputs}

In both \textit{SB20} and \textit{CN20} dataset annotations, where visually possible, we also include keypoint/stem locations of the plants, which are used as our click locations.
However, due to the nuance of the datasets a small number of plants (63 and 30 instances for \textit{SB20} and \textit{CN20}, respectively) do not contain this keypoint location.
This is due to a number of complicating factors, such as, being on the border of the image or occlusion by other instances.

If the keypoint is not included we perform a two step pipeline to allocate a center point within the plant's binary mask.
First, we try using the center of mass of the binary mask to locate the click, if this is located outside of the segmentation map we disregard it and move to the second step.
We instead continuously perform binary erosion until complete disappearance of the object and randomly select one of the points remaining within the penultimate iteration.
This process ensures that the click is located within the object region.
Finally, to replicate the uncertainty that a human operator might annotate with, we add random noise of $\pm10$ pixels to the click positions when training models (guaranteeing they remain inside the object).


\subsection{Implementation and metrics}

\subsubsection{Standard one-click segmentation}
For our standard one-click models we train a UNet architecture \cite{Ronneberger2015}, using a batch size of 3 and a fixed learning rate of 0.0001.
We train from scratch for 1500 epochs, which we empirically found was the convergence point.
To tackle class imbalances in our datasets, we use a class weighted cross entropy loss similarly to~\cite{Milioto2020,Smitt2022}, that gives a higher loss to classes with fewer samples.
We determine class weights $w_c$ as,
%
\begin{equation}
\label{eq:loss}
    w_c = 1/\log \left( \frac{a_c}{a_{bg}}+1.02 \right),
\end{equation}
%
where $a_c$ is the area in pixels of class $c$ and $a_{bg}$ is the background area.
For evaluation, we report mean object IoU (mIoU) performance over the evaluation sets.

\subsubsection{Panoptic one-click segmentation}
\label{sec:experiments:panoptic}
For our panoptic system, we replace the original DeepLab architecture from \cite{Cheng2020} and replace it with UNet in order to stay comparable to our previous experiments.
Here, we use a batch size of 1 due to our small dataset sizes, and a fixed learning rate of 0.001.
The network is trained for 500 epochs as we found this to be the convergence point.
For training the semantic segmentation head, we use the same class weighted cross entropy loss as previously described.
Again, we report mean object IoU (mIoU) performance over the evaluation sets.


\subsubsection{Semi-supervised instance segmentation}
In our semi-supervised experiments, we train Mask R-CNN \cite{he2017mask} with a ResNet-50~\cite{He15} backbone to perform instance segmentation of plant objects, in a binary manner (not differentiating species).
We train for 500 epochs using the settings from \cite{Halstead2021}.
For fully supervised reference models, we use the validation set to select the best performing model.
We use IoU filtering with a threshold of 0.4 as seen in \cite{Halstead2021}.
Further, we use NMS filtering with thresholds determined via validation for fully supervised models.
As validation sets are missing in our semi-supervised datasets, we use the optimum value for \textit{SB20} in \textit{CN20} and vice versa.
To evaluate instance segmentation, we report mean fgIoU over the objects in our datasets similarly to~\cite{Halstead2021}.


\subsubsection{Panoptic system to recover from missing clicks}

For our experiments on missing input clicks, we use the same parameters and network structure as our other panoptic experiments (see \Cref{sec:experiments:panoptic}), but with the center head enabled as described in \Cref{sec:proposed:panoptic_miss}.
For evaluation we use the standard metrics for panoptic segmentation~\cite{Kirillov2019} of panoptic quality (PQ), segmentation quality (SQ), and recognition quality (RQ).
For our use case, which is to recover missing clicks, we are predominantly interested in RQ as it describes the system's ability to correctly identify object instances.

\begin{figure}[t!]
\vspace{8pt}
\centering
    \begin{tabular}{cc}
        \includegraphics[width=.23\textwidth]{data/dataset_example_sb20.png} & \includegraphics[width=.23\textwidth]{data/dataset_example_sb20_seg.png} \\
        \includegraphics[width=.23\textwidth]{data/dataset_example_cn20.png} & \includegraphics[width=.23\textwidth]{data/dataset_example_cn20_seg.png}
    \end{tabular}
    \caption{Example data for both datasets \textit{SB20} (top) and \textit{CN20} (bottom). Left are plain RGB images and right additionally shows instance annotations.}
    \label{fig:dataset_examples}
    \vspace{-16pt}
\end{figure}


