{
    "arxiv_id": "2303.11771",
    "paper_title": "Self-Sufficient Framework for Continuous Sign Language Recognition",
    "authors": [
        "Youngjoon Jang",
        "Youngtaek Oh",
        "Jae Won Cho",
        "Myungchul Kim",
        "Dong-Jin Kim",
        "In So Kweon",
        "Joon Son Chung"
    ],
    "submission_date": "2023-03-21",
    "revised_dates": [
        "2023-03-22"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV"
    ],
    "abstract": "The goal of this work is to develop self-sufficient framework for Continuous Sign Language Recognition (CSLR) that addresses key issues of sign language recognition. These include the need for complex multi-scale features such as hands, face, and mouth for understanding, and absence of frame-level annotations. To this end, we propose (1) Divide and Focus Convolution (DFConv) which extracts both manual and non-manual features without the need for additional networks or annotations, and (2) Dense Pseudo-Label Refinement (DPLR) which propagates non-spiky frame-level pseudo-labels by combining the ground truth gloss sequence labels with the predicted sequence. We demonstrate that our model achieves state-of-the-art performance among RGB-based methods on large-scale CSLR benchmarks, PHOENIX-2014 and PHOENIX-2014-T, while showing comparable results with better efficiency when compared to other approaches that use multi-modality or extra annotations.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.11771v1"
    ],
    "publication_venue": null
}