
% \documentclass{article}
% \usepackage{spconf,amsmath,graphicx}
% \usepackage{multirow}
% % \usepackage{subfig}
% \usepackage{ctable}
% \usepackage{subcaption}
% \usepackage{caption}
% \usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
% \usepackage{extra_styles}
% \usepackage[capitalize]{cleveref}
% \crefname{section}{Sec.}{Secs.}  
% \Crefname{section}{Section}{Sections}
% \Crefname{table}{Table}{Tables}
% \crefname{table}{Tab.}{Tabs.}
% \crefname{algorithm}{Alg.}{Algs.}  
% \Crefname{algorithm}{Algorithm}{Algorithms}
% \usepackage{multicol}
% \usepackage{textpos}
% % \onecolumn

% \setcounter{page}{1}

% \title{Self-Sufficient Framework for Continuous Sign Language Recognition}

% \begin{document}

% \maketitle
% \begin{textblock*}{.8\textwidth}[.5,0](0.5\textwidth, -.06\textwidth)
% \centering
% {\small Project page with demo: \url{https://mm.kaist.ac.kr/projects/ssslr}}
% \end{textblock*}
% We thank all the reviewers for their valuable feedback and 
% %appreciate 
% the positive comments such as (1) sufficient experimental validation (\ra, \rb, \rc) and (2) clear presentation (\ra, \rc). We are pleased and thankful that all reviewers gave us positive scores for acceptance. We will apply all the constructive criticisms and feedback in the final version of our paper. 

% \newpara{Loss term formulas (\ra).}
% Due to the space limitations, we were unable to detail all specific losses. We will %be sure to 
% try to include all the details of the losses in our final version.


% \newpara{Citation (\ra).}
% We will include them in the final version. 



% \newpara{Explanation about `non-manual' and `manual' expressions (\rb).} 
% We will be sure to include the following sentences in the beginning portion of our paper for clarity.

% ``\textit{Manual features} are the information related to hands, \ie, hand shape, position, movement, and the orientation of the palm or fingers. 
% On the other hand, \textit{non-manual features} include gaze direction, body orientations, facial expressions as articulating and lip patterns.''




% \newpara{DFConv input output size (\rb).}
% Our DFConv structure is shown in ~\Tref{tab:DFE}. DFConv has 3 separate blocks, meaning that the input size of the image starts out at $224\times 224$ and pools down to $14\times 14$ after all 3 blocks. As Fig. 2. of our main paper is confusing as it uses a real image, we show another representation using a feature map in ~\Fref{fig:ME_layer}. We will change the values to variables for clarity in the final version.
% \begin{table}[!h]
% \begin{center}
%     \resizebox{0.90 \linewidth}{!}{
%         \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
%         \toprule
%         \multicolumn{10}{c}{\textbf{Divide and Focus Convolution Block}}                             \\ \midrule
%         Block                    & Layer  & ${C_{in}}$ & ${C_{out}}$ & ${k}$ & ${p}$ & ${s}$ & ${r}$    & ${N_{n}}$ & ${N_{m}}$ \\ \midrule
%         \multirow{4}{*}{Block 1} & DFConv & 3     & 32     & 3 & 1 & 1 & 1.00 & 1     & 0     \\ \cmidrule{2-10} 
%                                  & \multicolumn{9}{c}{MaxPool, ${k=2}$, ${s=2}$}                     \\ \cmidrule{2-10} 
%                                  & DFConv & 32    & 64     & 3 & 1 & 1 & 1.00 & 1     & 0     \\ \cmidrule{2-10}  
%                                  & \multicolumn{9}{c}{MaxPool, ${k=2}$, ${s=2}$}                     \\ \midrule
%         \multirow{3}{*}{Block 2} & DFConv & 64    & 128    & 3 & 1 & 1 & 0.35 & 1     & 2     \\ \cmidrule{2-10}  
%                                  & DFConv & 128   & 128    & 3 & 1 & 1 & 0.35 & 1     & 2     \\ \cmidrule{2-10} 
%                                  & \multicolumn{9}{c}{MaxPool, ${k=2}$, ${s=2}$}                     \\ \midrule
%         \multirow{3}{*}{Block 3} & DFConv & 128   & 256    & 3 & 1 & 1 & 0.35 & 1     & 4     \\ \cmidrule{2-10} 
%                                  & DFConv & 256   & 256    & 3 & 1 & 1 & 0.35 & 1     & 4     \\ \cmidrule{2-10} 
%                                  & \multicolumn{9}{c}{MaxPool, ${k=2}$, ${s=2}$}                     \\ 
%         \bottomrule
%         \end{tabular}
%     }
% \end{center}
% \vspace{-5mm}
% \caption{\textbf{The structure of Divide and Focus Conv (DFConv) Block.} 
% ${C_{in}}$, ${C_{out}}$, ${k}$, ${p}$ and ${s}$ represent the input channels, output channels, kernel size, padding and stride respectively. The hyper parameters of DFConv ${r}$, ${N_{n}}$ and ${N_{m}}$ indicate the division ratio, the number of non-manual parts and the number of manual parts in DFConv. Note that when $r$ is 1, the DFConv becomes a normal convolutional layer.}
% \vspace{-2mm}
% \label{tab:DFE}
% % \vspace{-10pt}
% \end{table}

% \begin{figure}[!h]
%   \centering
%       \includegraphics[width=0.95\linewidth]{fig/multi_cue_embedding.pdf}
%     \vspace{-2mm}
%   \caption{\textbf{Multi-cue embedding architecture.} Note all convolutions are 2D conv with $C_o$ being the output channel.}
%   \vspace{-2mm}
%   \label{fig:ME_layer}
% \end{figure}

% \newpara{Transformer-based sequence model (\rc).}
% We have previously experimented with Transformer based sequence modeling inspired by SLRT [34]. However, we found that the Transformer based sequence models with DFConv or DPLR do not improve in performance as compared to our LSTM based models.


% % \newpara{Backbone model (\rd).}




% \newpara{Backbone model and computational time (\rd).}
% Although the Swin Transformer might be able to extract more powerful features, we empirically found that it is
% %definitely 
% not computationally efficient. 
% On the same settings, even the \textbf{Swin Transformer Tiny} has a speed of 328ms for only the backbone, which is far slower than ours, where our whole model including sequence model has a speed of 198ms. This shows that using the Swin Transformer is not more efficient than our current method at inference time.

% We also show the computational cost in \Tref{tab:flops}.
% We compare the re-implemented STMC under the same conditions and find that DFConv significantly reduces both FLOPs on inference time and RGB frames. 


% % We compare the computational complexity with the most recent multi-cue based method, STMC in~\Tref{tab:flops}. For the fair comparison, we also reimplement STMC and estimate the inference time under the same conditions in STMC paper. Even though the pose detector of STMC is light-weight, it still induces the bottleneck in the inference time. We highlight that DFConv significantly reduces both FLOPs and inference time by pulling out the pose estimator while extracting multi-cue features from the RGB frames. For reference, in our environment, extracting human keypoints with HRNet~\cite{sun2019deep} from PHOENIX-2014~\cite{koller2015continuous} dataset takes 2-3 GPU days. 
% \begin{table}[!h]
%     \centering
%     \resizebox{.90\linewidth}{!}{
%         \begin{tabular}{lcccc}
%         \toprule
%         &  &  & \multicolumn{2}{c}{WER (\%)} \\
%         \cmidrule{4-5}
%         Method & FLOPs & Time & Dev & Test \\
%         \midrule
%         VGG-11+1D-CNN* & 7.5G  & 264ms & 25.1 & - \\
%         STMC*    & 10.3G & 352ms & 21.1 & 20.7    \\ 
%         \midrule
%         VGG-11+1D-CNN & 16.4G  & 453ms & 24.9 & 25.1 \\
%         STMC & 20.9G & 622ms & 21.0 & \textbf{20.7}    \\ 
%         \midrule
%         \textbf{Ours}       & \textbf{7.4G} & \textbf{198ms} & \textbf{20.9} & 20.8 \\
%         \bottomrule
%         \end{tabular}}
%     \vspace{-2mm}
%     \caption{Comparison of computational cost and inference time with STMC. 
%     (*): we directly take the reported results in the original STMC paper [11].
%     }
%     \label{tab:flops}
% \end{table}
% \end{document}
