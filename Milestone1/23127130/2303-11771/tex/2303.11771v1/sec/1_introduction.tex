\section{Introduction}
\label{sec:intro}

The Continuous Sign Language Recognition (CSLR) task aims to recognise a gloss\footnote{Glosses are the smallest units having independent meaning in sign language.} sequence in a sign language video~\cite{cheng2020fully,huang2018video,koller2019weakly}.
To capture the meaning of the sign expressions from a signer, recent works obtain manual and non-manual expressions by fusing RGB with other modalities such as depth~\cite{molchanov2016online}, infrared maps~\cite{liu2017continuous} and optical flow~\cite{cui2019deep}, 
or by explicitly extracting multi-cue features~\cite{huang2018video,camgoz2017subunets,kim2021dense,kim2021acp++} or human keypoints~\cite{zhou2020spatial} using off-the-shelf detectors.
However, using such extra components introduce bottlenecks in both training and inference processes.
In addition, most CSLR datasets only have sentence-level gloss labels without frame- or gloss- level labels~\cite{huang2018video,cihan2018neural,koller2015continuous}. 
To overcome insufficient annotations, the Connectionist Temporal Classification (CTC)~\cite{graves2006connectionist} loss has been traditionally opted to consider all possible underlying alignments between the input and target sequence.
However, using the CTC loss without true frame-level supervision produces temporally spiky attention which can make the model fail to localise important temporal segments~\cite{min2021visual}.

Accordingly, we develop \emph{self-sufficient} framework for CSLR, which provides meaningful gloss supervision while capturing helpful multi-cue information \emph{without additional modalities} or \emph{annotations.}
To this end, we propose two novel methods: Divide and Focus Convolution (DFConv) and  Dense Pseudo-Label Refinement (DPLR).
DFConv is a task-aware convolutional layer which extracts visual multi-cue features by dividing spatial regions to focus on partially specialised features.
Note that DFConv is designed to leverage prior knowledge about the structure of human bodies without any additional networks or modalities.
In addition, DPLR elaborately refines an initially predicted gloss sequence from the model by referring a ground-truth gloss, and propagates frame-level gloss supervision without additional networks, unlike~\cite{cui2019deep,pu2018dilated}.
We emphasise that DPLR is generally applicable to other CSLR architectures or frameworks~\cite{cheng2020fully,min2021visual} to bring performance gain by reducing missing glosses in predictions.

\begin{figure}[t]
\centering
    \centering
    \includegraphics[width=.85\linewidth]{fig/teaser_2.pdf}
    \vspace{-4mm}
    \caption{Comparison of GradCAM between VGG-11 and DFConv, and an example of the generated pseudo-labels before and after DPLR. DFConv better highlights multiple individual elements (hands, faces) across the entire scene whereas VGG-11 simply highlights a small region (\ie,~the right hand). DPLR corrects the mispredicted gloss with the ground truth gloss (\eg,~red box in Vanilla pseudo-labels) and densifies the pseudo-labels with the nearest glosses, which results in a more informative supervision without external knowledge.
    }
    \vspace{-5mm}
    \label{fig:teaser}
\end{figure}


We extensively validate the effectiveness of DFConv and DPLR. 
We also show that the whole \emph{self-sufficient} counterpart achieves state-of-the-art results among RGB-based methods and is comparable to other methods that use extra knowledge with better efficiency on two publicly available CSLR benchmarks~\cite{cihan2018neural,koller2015continuous}.
To summarise, our main contributions are as follows:

(1) We design a task-specific convolutional layer, named DFConv, that efficiently extracts non-manual and manual features without additional networks or annotations.
(2) We also introduce DPLR, a novel pseudo-label generation method, to propagate frame-level supervision by using the combination of the ground truth gloss sequence and the predicted temporal segmentation information.
(3) We conduct extensive experiments on two publicly available CSLR benchmarks, showing state-of-the-art performance compared to other RGB-based methods, and competitive results compared to other approaches that use multi-modality or additional knowledge {with better efficiency}.
