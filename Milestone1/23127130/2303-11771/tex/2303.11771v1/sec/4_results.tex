\section{Experiments}
\label{sec:exp_results}
\noindent \textbf{Dataset and Evaluation Metric.}
We conduct experiments on two publicly available CSLR benchmarks to validate our \emph{self-sufficient} framework: PHOENIX-2014~\cite{koller2015continuous} and PHOENIX-2014-T~\cite{cihan2018neural}.
We adopt the Word Error Rate (WER)\footnote{WER = ({\#substitutions} + \text{\#deletions} + \text{\#insertions}) / ({\text{\#words in reference}})}~\cite{koller2015continuous} for evaluation. Furthermore, in our project page, we upload a demo video to visually demonstrate the effectiveness of DFConv.
\input{tab/exp_phoenix}

\begin{figure*}[t]
\vspace{-6mm}
   \centering
    \includegraphics[width=0.70\linewidth]{fig/qualitative.pdf}
   \vspace{-3mm}
   \caption{Gloss predictions in a single sentence sign video from different network architectures (D: deletion, I: insertion, S: substitution). In the fourth and sixth rows, we further visualise two cases of Dense Pseudo-Labels (DPL). Applying the DPLR on the prediction greatly reduces the deletion phenomenon. 
   }
   \label{fig:qualitative}
   \vspace{-6mm}
\end{figure*}


\subsection{Experimental Results}
\label{sec:quant_results}
We compare our framework with recent CSLR methods on both PHOENIX-2014~\cite{koller2015continuous} and PHOENIX-2014-T~\cite{cihan2018neural} benchmarks. \Cref{tab:phoenix,tab:phoenix_t} show the WER scores, 
while we specify the type of either extra annotations or modalities used during training for each method. 

\noindent \textbf{PHOENIX-2014.} 
\Tref{tab:phoenix} summarises the results on Dev and Test splits from PHOENIX-2014 for several CSLR baselines. 
First, Ours achieves the state-of-the-art performances on Test split among RGB-based approaches.
In particular, Ours outperforms the recently proposed FCN~\cite{cheng2020fully}, fine-grained labeling~\cite{niu2020stochastic}, VAC~\cite{min2021visual} with alignment supervision to visual features, and CMA~\cite{pu2020boosting} with both gloss and video augmentation.
Moreover, Ours shows superior performance over several recent methods that explicitly require extra annotations for training~\cite{huang2018video,koller2019weakly,cui2019deep}, and comparable performances to SMKD~\cite{hao2021self} with algorithmic gloss segmentations and STMC~\cite{zhou2020spatial} using pose annotations.
Note that the proposed method does not require either extra annotations for acquiring the benefit to detect spatially important regions or additional networks for the refinement of pseudo-labels.

\noindent \textbf{PHOENIX-2014-T.} \Tref{tab:phoenix_t} shows the results on Dev and Test splits of PHOENIX-2014-T. 
Ours surpasses cnn-lstm-hmm~\cite{koller2019weakly} which is trained with both mouth and hand annotations, and even outperforms SLRT~\cite{camgoz2020sign} that jointly learns sign recognition and translation task from both sign glosses and sentences.
Ours also outperforms SMKD~\cite{hao2021self}, a competing baseline using RGB modality, and shows comparable results to STMC~\cite{zhou2020spatial}.

\input{tab/ablation_tables}

\subsection{Ablation Study}
\label{sec:abl_study}

\noindent \textbf{Component Analysis.}
In~\Tref{tab:component}, we ablate each component of our method to investigate its effectiveness. 
In the first row of the table, we show the result of the baseline model with VGG-11~\cite{simonyan2014very} architecture followed by three 1D convolution layers.
All components of our method consistently improve the performance altogether.
In particular, when BTA is combined with DFConv, the performance improvement is marginal, but when combined with DPLR, it shows a large performance improvement. From this, we conclude that DPLR and BTA are complementary modules to each other.
We ablate qualitatively the gloss predictions of each component in ~\Fref{fig:qualitative}. 

\noindent \textbf{Design Choice of DPLR.} 
The baseline in the first row of~\Tref{tab:abl_GRM} is the same baseline in the fourth row in~\Tref{tab:component}, which is the model trained with DFConv and BTA.
`Densify' and `Refine' 
indicate
whether the prediction from the model is filled by the nearest gloss prediction and whether glosses are replaced with ground truth glosses, respectively, as shown in~\Fref{fig:DPLR}.
We show from the second and third row (without `Densify') that directly leveraging the output of the model brings marginal improvements to the baseline.
`Densify,'
which provides direct alignment supervision on the \emph{frame-level} to the latent features is the key component for improving the model performance.
Finally, the proposed Dense Pseudo-Labels (DPL), which is the combination of both `Densify' and `Refine' processes, shows the best performance by the correction mechanism with ground truth labels in `Refine' to reduce the noise in \emph{dense} pseudo-labels.


\begin{figure}[ht]
  \centering
   \includegraphics[width=0.83\linewidth]{fig/transform_v3.pdf}
   \vspace{-4mm}
   \caption{Activation maps from Ours on \emph{test-time} novel transformations.
   }
   \vspace{-2mm}
   \label{fig:transform}
\end{figure}

\input{tab/division_ratio}
\noindent{\textbf{Robustness of DFConv.}}
\emph{Our method is more robust where the signer is not bounded to a specific region at inference time than the state-of-the-art methods in practical cases.}
To simulate such a scenario, we make a set of transformed data from PHOENIX-2014 Dev and Test splits, each of which includes a different degree of vertical translation (T) and scale operation (S).
In \Cref{tab:division_ratio}, we list RGB based state-of-the-art method (VAC), the pose-based method (STMC), our model tested with the original division ratio $(r=0.35)$ (Ours), and our model where the $r$ is changed along the corresponding transformation (Ours$^\dagger$).
Although shifting the $r$ gives the best performance (Ours$^\dagger$), in the real world, where we are not able to adjust on the fly $r$ (Ours), the average performance of Ours still surpasses VAC and STMC. 

We also present the activation maps of DFConv with the static division ratio $(r=0.35)$ in~\Cref{fig:transform}.
DFConv steadily captures non-manual and manual features even when half of the signer's face is out of focus (B, D) or when the signer's hands are partially out (D, F, H) with failure cases of pose-detectors shown in our project page.
This shows that pose-based sign recognition methods are heavily reliant on the performance of the pose-detector.
