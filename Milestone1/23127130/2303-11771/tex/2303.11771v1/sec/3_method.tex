\section{Method}
\label{sec:overview}
CSLR task aims to map a given input video to its corresponding gloss sequence ${g}$ = $\{g_n\}{_{n=1}^N}$ with $N$ glosses.
As shown in \Fref{fig:overall}, 
a sign video is fed into the \emph{spatial modeling} module consisting of several Divide and Focus Convolution (DFConv) layers, and a multi-cue embedding layer to extract manual and non-manual features.
The multi-cue features of all the frames are 
passed through the \emph{temporal modeling} module, that is comprised of Bottleneck-based Temporal Attention (BTA), which captures more important information among adjacent frames,
and Temporal Multi-Cue (TMC) blocks of \cite{zhou2020spatial}.
Then, the output of last TMC block is passed through the \emph{sequence learning} stage, which is composed of a Bi-LSTM layer~\cite{schuster1997bidirectional} and FC layer to predict the gloss sequence from the final model output. 
Finally, the Dense Pseudo-Label Refinement (DPLR) module is introduced to effectively train the latent representations by generating corrected and densified frame-level pseudo-labels.

\subsection{Divide and Focus Convolution}
\label{sec:dfconv}
We observe from 
various
CSLR datasets~\cite{cihan2018neural,koller2015continuous,ham2021ksl} that non-manual expressions occur frequently in the upper region of the image, while manual expressions occur mainly in the lower region.
As shown in~\Fref{fig:teaser}, despite the importance of both non-manual and manual expressions appearing in the entire image area, the conventional 2D convolution layer tends to capture the only one most dominant information (\ie,~right hand) over the whole image.
To address this issue, we propose a novel Divide and Focus Convolution (DFConv) layer designed to independently capture non-manual features and manual features solely from RGB modality. 

The structure of the DFConv is illustrated in~\Fref{fig:overall}.
Inspired by the observation in~\cite{fan2020gaitpart}, DFConv physically limits the receptive field that increases as the network deepens by subdividing an image~\cite{kim2021dense} into upper (for non-manual expressions) and lower regions (for manual expressions) with the division ratio of $r$, where $r$ is the ratio of spatial height of the upper region $h_u$ to the original spatial height $h$ given by $r = \frac{h_u}{h}$.
To precisely capture the dynamic manual expressions, we further subdivide the lower region into $N_m$ groups.
For the upper region, this kind of subdivision is not required since non-manual expressions do not consist of the same amount of dynamics.
We empirically observe that subdividing the upper region reduces the performance as well.
Note that \emph{different convolution weights are used for each upper and lower regions}, and \emph{the weights are shared within the subdivided lower regions}.
This helps the model to focus more on visually meaningful areas that represent complex sign expressions in the segmented image.

Unlike other methods that leverage external knowledge, we only introduce two hyper-parameters $r$ and $N_m$, which make our method significantly more efficient.
By virtue of simply splitting the frames horizontally, DFConv efficiently captures multi-cue features simultaneously without equipping costly human pose estimator like STMC~\cite{zhou2020spatial} that increases model complexity and inference time (See project page).
To further embed the outputs of stage 3 in~\Fref{fig:overall} into the three individual multi-cue vectors (\ie, full-frame, non-manual and manual), a simple and effective Multi-Cue Embedding (ME) layer is employed.
The full-frame features containing global information are passed through two 2D convolution layers, and the remaining features (non-manual and manual) are passed through only single 2D convolution layer. Finally, all these features are vectorised by max pooling with a $2\times 2$ kernel followed by an average pooling layer.

\subsection{Dense Pseudo-Label Refinement}
\label{sec:dense_pl}
Most existing sign language datasets do not have temporally localised gloss labels~\cite{huang2018video,cihan2018neural,koller2015continuous,guo2018hierarchical}.
Due to the characteristics of the CTC loss used in training CSLR models without frame-level labels, the output sequence predictions of models are naturally induced to be sparse.
As a result, it is difficult for CSLR models to receive direct and precise alignment supervision for each gloss token.
In addition, without alignment supervision, CSLR models learn entire sequences as a whole instead of individual gloss words. 
This limits the robustness of models severely as they rely on entire sequences. 
In other words, models can easily confuse similar sequences with slightly different words.
In order to mitigate these drawbacks, we introduce an additional training objective called Dense Pseudo-Label Refinement (DPLR) that uses the alignment information predicted by the model to generate Dense Pseudo-Labels (DPL). Then, the model is further refined with these generated pseudo-labels.

\begin{figure}[!t]
    \centering
    % \vspace{-2mm}
    \includegraphics[width=.70\linewidth]{fig/DPLR.pdf}
    \vspace{-3mm}
    \caption{Dense pseudo-label generation process in DPLR. {\bf (Case 1)}: gloss sequence length predicted by the model and the ground truth are matched. {\bf (Case 2)}: gloss sequence length predicted by the model and the ground truth is off by one word.
    DPLR provides latent features with \emph{frame-level} supervision to compensate for the CTC loss while reducing the noise of pseudo-labels by the correction mechanism of Case 1.
    }
    \vspace{-5mm}
    \label{fig:DPLR}
\end{figure}

In DPLR process, we have two separate cases for generating DPL $\hat{Q}$ as illustrated in~\Fref{fig:DPLR}.
We first compare the sequence length of non-blank predictions of the model with its corresponding ground truth gloss sequence.
If the sequence length is matched, we go to {\bf Case 1}, where we compare the predicted gloss sequence with the ground truth sequence. If a predicted gloss is wrong, we swap in correct gloss from the ground truth to increase the reliability of the pseudo-labels. As mentioned before, the predictions are sparse due to the nature of the CTC loss, and most of the predictions along the temporal axis are blanks.
Here, we create DPL by filling each blank with the nearest predicted glosses.
In the case where the predicted sequence length differs from the ground truth by one gloss length, we go to {\bf Case 2}.
Then, we simply densify the pseudo-label using the nearest gloss without swapping any glosses regardless of the correctness of the glosses.
In the case that the sequence length differs by more than one gloss, we disregard that sequence as this might cause predictions of the model to degrade, so we do not propagate refinement loss $L_{refine}$.

Using pseudo-labels only from {\bf Case 1} and {\bf Case 2},
we refine the model with Cross Entropy (CE) loss on the latent features similar to~\cite{cheng2020fully} as follows:
\begin{equation} \label{eq:refine_loss}
    L_{refine} = \text{CE}(\hat{Q}, \tilde{Q}),
\end{equation}
where $\hat{Q}$ is dense pseudo-labels and, $\tilde{Q}$ is gloss probability acquired from latent features, which is the final output of the inter-cue path (See~\Fref{fig:overall}). 
Note that we demonstrate the efficacy of `Densify' and `Refine' processes in~\Cref{tab:abl_GRM}, and show that DPLR is generalisable to other models in 
our project page.

In addition, the quality of pseudo-labels generated from the model depend heavily on the model's performance. As the CSLR task aims to translate a sign language video into a gloss sequence by mapping several adjacent frames into one gloss, it is important to extract key frames in the video. 
Hence, we design the \textbf{Bottleneck-based Temporal Attention (BTA)} module to attend to the temporally salient frames among adjacent frames.
BTA consists of a temporal-wise attention map using 1D convolution layers and a max pooling layer to capture the temporally salient frames. The CTC loss is then propagated to the bottleneck, after the max pooled features, hence the name is Bottleneck.

With our additional modules, our final loss function is as follows:
\begin{align} \label{eq:total_loss}
    L_{total} = L_{inter} + \lambda_{1} L_{intra} + \lambda_{2} L_{refine} + \lambda_{3} L_{bta},
\end{align}
where, $L_{inter}$, $L_{intra}$ and $L_{bta}$ are all CTC losses. $L_{bta}$ is the average of all the TMC block's CTC losses and $\lambda_{1}$, $\lambda_{2}$, and $\lambda_{3}$ are loss weights.
