\begin{figure*}[t]
\centering
    \includegraphics[width=.58\linewidth]{fig/overall.pdf}
    \vspace{-3mm}
    \caption{\textbf{Overall architecture}. 
    In Spatial Modeling, non-manual and manual features are extracted through DFConv followed by a Multi-Cue embedding layer. In Temporal Modeling, temporal features are extracted for gloss sequence prediction by integrating each element. Finally, the gloss probability vectors are obtained through sequence learning.}
    \label{fig:overall}
    \vspace{-5mm}
\end{figure*}

\section{Related works}
\label{sec:related}
Multi-cue fusion methods for CSLR task can be categorised into \emph{multi-semantic} and \emph{multi-modal} methods. 
Multi-semantic works~\cite{koller2019weakly,koller2015continuous,kim2021dense,kim2021acp++} utilised hand-crafted or weak-labeled features such as detected hands, trajectories of hands, and body parts, then integrate these features into frames to predict the gloss sequences. 
On the other hand, multi-modal works~\cite{liu2017continuous,molchanov2016online} use color, depth, and optical flow to extract orthogonal features. \cite{cui2019deep} proposed a multi-modality integration framework of appearance and motion cues by using both RGB frames and optical flow. Most recently, \cite{zhou2020spatial} fused human body keypoints extracted by an off-the-shelf network~\cite{sun2019deep}.
Unlike the methods listed above, we design DFConv that captures both manual and non-manual expressions from RGB video, without relying on \emph{any additional hand-crafted features} or \emph{multi-modal data}.

In addition, the CSLR task~\cite{huang2018video,camgoz2017subunets,pu2018dilated,koller2016deepA,jang2022signing} naturally corresponds to weakly-supervised learning problem due to
the lack of frame-level gloss annotations.
The challenge lies in the ambiguous semantic boundary of the adjacent glosses from sign videos~\cite{huang2018video,koller2015continuous,duarte2021how2sign}. To address this issue, 
some works in CSLR field generate frame-level pseudo-labels from sparse gloss annotations~\cite{koller2019weakly,koller2017re}, which can be inherently noisy and reliant on the model's performance. Most recently, the CTC loss~\cite{graves2006connectionist} is employed to facilitate end-to-end training of a deep learning model~\cite{cui2017recurrent,niu2020stochastic}, and consider all the feasible underlying alignments between the predictions and labels. However, as observed in~\cite{min2021visual,pu2019iterative}, directly optimising CTC can cause spiky attention in predictions, favoring more blank glosses. Recent works tackle this issue by balancing the blank output and meaningful glosses~\cite{cheng2020fully}, and by directly supervising the visual features via visual alignment constraint~\cite{min2021visual} and mutual knowledge transfer~\cite{hao2021self}.
In contrast, we propose Dense Pseudo-Label Refinement (DPLR) that provides dense and reliable supervision signals obtained by gloss predictions of the model to visual features. 
