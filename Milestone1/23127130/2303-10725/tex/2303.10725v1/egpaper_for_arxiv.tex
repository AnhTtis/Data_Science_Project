\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}

\usepackage{algorithm}
\usepackage{algpseudocode}
% \usepackage{caption}
\usepackage{subcaption}
\usepackage{comment}
\usepackage{enumitem}



% Include other packages here, before hyperref.

% boolean options to add acknowledgements or combine main document with supplemental materials document 
\usepackage{ifthen}
\newboolean{ack}
\newboolean{combined}

 \iccvfinalcopy % *** Uncomment this line for the final submission
\setboolean{ack}{true} 
\setboolean{combined}{true}

% reset counters for pages, figures, and tables in supplemental materials
\newcommand{\beginsupplement}{%
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
    
        \setcounter{equation}{0}
        \renewcommand{\theequation}{S\arabic{equation}}%
        
        \setcounter{section}{0}
        \renewcommand{\thesection}{S\arabic{section}}%
        
        \setcounter{page}{1}
        \renewcommand{\thepage}{S\arabic{page}}%
}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}



\def\iccvPaperID{3969} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\DeclareMathOperator*{\argmaxA}{arg\,max}

\begin{document}

%%%%%%%%% TITLE
\title{SIESTA: Efficient Online Continual Learning with Sleep}


\author{Md~Yousuf~Harun$^{1*}$ \qquad Jhair~Gallardo$^{1}$\thanks{Equal contribution.} \qquad Tyler~L.~Hayes$^1$\thanks{Now at NAVER LABS Europe.} \\ Ronald~Kemker$^{2}$ \qquad Christopher~Kanan$^{3}$\\
Rochester Institute of Technology$^1$, United States Space Force$^2$, University of Rochester$^3$\\
%{\tt\small \{mh1023, gg4099, tlh6792\}@rit.edu, }{\tt\small ronald.kemker@spaceforce.mil, }{\tt\small ckanan@cs.rochester.edu}\\
}


\begin{comment}
\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}
\end{comment}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

\def \hfillx {\hspace*{-\textwidth} \hfill}

%%%%%%%%% ABSTRACT
\begin{abstract}
In supervised continual learning, a deep neural network (DNN) is updated with an ever-growing data stream. Unlike the offline setting where data is shuffled, we cannot make any distributional assumptions about the data stream. Ideally, only one pass through the dataset is needed for computational efficiency. However, existing methods are inadequate and make many assumptions that cannot be made for real-world applications, while simultaneously failing to improve computational efficiency. In this paper, we do not propose a novel method. Instead, we present SIESTA, an incremental improvement to the continual learning algorithm REMIND. Unlike REMIND, SIESTA uses a wake/sleep framework for training, which is well aligned to the needs of on-device learning. SIESTA is far more computationally efficient than existing methods, enabling continual learning on ImageNet-1K in under 3 hours on a single GPU; moreover, in the augmentation-free setting it matches the performance of the offline learner, a milestone critical to driving adoption of continual learning in real-world applications. 
\end{abstract}

%%%%%%%%% BODY TEXT

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

\input{figures/front_image}

Training DNNs is incredibly resource intensive. This is true for both learning in highly resource constrained settings, e.g., on-device learning, and for training large production-level DNNs that can require weeks of expensive cloud compute. Moreover, for real-world applications, the amount of training data typically grows over time. This is often tackled by periodically re-training these production systems from scratch, which requires ever-growing computational resources as the dataset increases in size.  Continual learning algorithms have the ability to learn from ever-growing data streams, and they have been argued as a potential solution for efficient learning for both embedded and large production-level DNN systems, improving the computational efficiency of network training and updating~\cite{parisi2019continual}. However, continual learning is rarely used for real-world applications because these algorithms fail to achieve comparable performance to offline retraining or they make assumptions that do not match real-world applications. In this paper, we describe a resource efficient, continual learning algorithm that rivals an offline learner on supervised tasks, a critical milestone toward enabling the use of continual learning for real-world applications.
 

In conventional offline training of DNNs, training data is shuffled (making it independent and identically distributed (iid), which is required for stochastic gradient descent (SGD) optimization) and repeatedly looped through many training iterations. In contrast, an ideal continual learning algorithm is able to efficiently learn from \emph{potentially} non-iid data streams, where each training sample is only seen by the learner once unless a limited amount of auxiliary storage is used to cache it. Most continual learning algorithms are designed solely to overcome catastrophic forgetting, which occurs when training with non-iid data~\cite{parisi2019continual,kemker2018measuring}. To do this, most models make implicit or explicit assumptions that go beyond the general supervised learning setting, e.g., some methods assume the availability of additional information or assume a specific structure of the data stream. Moreover, existing methods do not match the performance of an offline learner, which is essential for industry to adopt continual learning for updating large DNNs.

 We argue that a continual learning algorithm should have the following properties:
\begin{enumerate}[noitemsep, nolistsep]
    \item It should be capable of online learning and inference in a compute and memory constrained environment,
    \item It should rival (or exceed) an offline learner, regardless of the structure of the training data stream,
    \item It should be significantly more computationally efficient than training from scratch, and
    \item It should make no additional assumptions that constrain the supervised learning task, e.g., using task labels during inference.
\end{enumerate}
These criteria are simple; however, most continual learning algorithms make strong assumptions that do not match real-world systems and are assessed on toy problems that are not appropriate surrogates for real-world problems where continual learning could greatly improve computational efficiency. For example, many works still focus on tasks such as split-CIFAR100~\cite{rajasegaran2019random, zhao2020maintaining, douillard2020podnet, yan2021dynamically, douillard2021dytox}, only work in extreme edge cases like incremental class learning~\cite{castro2018end, chaudhry2018efficient, hou2019learning, rebuffi2017icarl, tao2020topology, wu2019large}, assume the availability of task-labels during inference~\cite{golkar2019continual, fernando2017pathnet, hung2019compacting, serra2018overcoming}, or require large batches to learn~\cite{yan2021dynamically, douillard2021dytox}. For continual learning to have practical utility, we need efficiency and performance that rivals trained from scratch models, as well as robustness to data ordering.

There are two extreme frameworks for continual learning. At one extreme, is incremental batch learning where the agent receives a batch and has as much time as necessary to loop over that batch before proceeding to the next batch. Typically these systems are evaluated with large batches, and many experience dramatic performance decreases when smaller batches are used~\cite{hayes2020REMIND}. At the other extreme is online continual learning, where the agent receives one input at a time. Humans and animals learn in a manner that is a compromise between these two extremes. They acquire new experiences in an online manner and these experiences are consolidated offline during sleep~\cite{mcclelland1996considerations,hayes2021replay}. Sleep plays a role in memory consolidation in all animals studied, including invertebrates, birds, and mammals~\cite{vorster2015sleep}. While animals sleep to consolidate memories, they can use both consolidated (post-sleep) and recent (pre-sleep) experiences to make inferences while awake. 

Although this paradigm is virtually ubiquitous among animals, it has rarely been studied as a paradigm in continual learning. This paradigm matches a real-world need: on-device continual learning and inference. For example, virtual/augmented reality (VR/AR) headsets could use continual learning to establish play boundaries and to identify locations in the physical world to augment with virtual overlays. Home robots, smart appliances, and smart phones need to learn about the environments and the preferences of their owners. In all of these examples, learning online is needed, but there are large periods of time where offline memory consolidation is possible, e.g., while the mobile device is being charged or its owner is asleep. In this paper, we formalize this paradigm for continual learning and we describe an algorithm with these capabilities, which we call SIESTA (\textbf{S}leep \textbf{I}ntegration for \textbf{E}pisodic \textbf{ST}re\textbf{A}ming).

SIESTA is a variant of REMIND~\cite{hayes2020REMIND} that is designed to operate in our online learning with offline consolidation paradigm. REMIND continually trains the upper layers of a deep neural network (DNN) in a pseudo-online manner. It stores quantized mid-level representations of seen inputs in a buffer, which enables it to store a much larger number of samples with a given memory budget compared to methods that store raw images. To do pseudo-online training, REMIND uses rehearsal~\cite{hetherington1989there}, a method for mitigating catastrophic forgetting by mixing new inputs with old inputs. For every new input, REMIND reconstructs a small number of past inputs, mixes the new input with them, and updates the DNN with this mini-batch; however, using rehearsal for every sample to be learned is not ideal. SIESTA addresses this by using rehearsal only during its offline sleep stage. For online learning, SIESTA instead uses lightweight online updates of the DNN's output layer. 

\paragraph{Our major contributions are summarized as:}
\begin{enumerate}
    \item We formalize a framework for online learning with offline memory consolidation, and we describe the SIESTA algorithm that operates in this framework (see Fig.~\ref{fig:siesta-overview}). SIESTA is capable of rapid online learning and inference while awake, but has periods of sleep where it performs offline memory consolidation.

    \item For incremental class learning on ImageNet-1K, SIESTA achieves state-of-the-art performance using far fewer parameters, memory, and computational resources than other methods. Without augmentations, training SIESTA requires only $2.4$ hours on a single NVIDIA A5000 GPU. In contrast, recent methods require orders of magnitude more compute (see Fig.~\ref{fig:summary}). 

    \item SIESTA is the first continual learning algorithm to achieve identical performance to an offline model, when augmentations are not used. It solves \emph{catastrophic forgetting} - the most studied problem in continual learning (see Table~\ref{tab:no_aug}). SIESTA is capable of working with arbitrary orderings, and achieves similar performance in both class incremental and iid settings.
    
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Online Learning with Offline Consolidation}
\label{sec:formal-paradigm}

We formalize the classification problem setting for supervised online continual learning with offline consolidation. The learner alternates between an online phase and an offline phase. For learning, during the $j$'th online phase, the agent receives a sequence of $n$ labeled observations, i.e., $t_{j1},t_{j2}, \ldots, t_{jn}$, where each input observation $x_t$ has label $y_t$. The sequence is not assumed to be stationary, and it can contain examples from classes from an arbitrary label ordering. The agent can cache these labeled pairs, or a subset of them, in memory with storage of size $b$ bits. The agent can be evaluated at any time during the online phase, where it must make inferences using both recent experiences from the $j$'th online phase, as well as past experiences from previous phases. During the subsequent offline consolidation phase, the agent is allowed at most $m$ updates of the network, e.g., gradient descent updates.
We do not assume task labels are available during any phase. In general, iid (shuffled) orderings do not cause catastrophic forgetting; and at the other extreme, an ordering sorted by category causes severe catastrophic forgetting in conventional algorithms~\cite{kemker2018measuring}.  

With some exceptions~\cite{kemker2017fearnet,pham2021dualnet,pham2023learning,arani2022learning}, this paradigm has been little studied and offers several advantages. For example, it allows embedded mobile devices to quickly use new information from users and their environments and then consolidate that learning during a scheduled downtime. It also serves as a setting for studying learning efficiency in continual learning, where different sleep policies can be studied for minimizing the number of updates $m$ during a sleep setting. Lastly, it allows for testing functional hypotheses from neuroscience about sleep. While rehearsal-like mechanisms used in continual learning occur during slow wave sleep, the mechanisms that occur during rapid eye movement (REM) sleep have not yet been explored with DNNs nor has the interplay between slow wave sleep and REM sleep. REM sleep increases abstraction, facilitates pruning of synapses, and is when dreams occur~\cite{smith2003ingestion,djonlagic2009sleep,cai2009rem,Lewis2018How,durrant2015schema,li2017rem}. We juxtapose our training paradigm with alternatives in continual learning in Sec.~\ref{sec:related-work}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:related-work}

We compare SIESTA's online learning with offline consolidation paradigm to alternative paradigms.

\paragraph{Task Incremental Learning with Task Labels.}
Incremental task batch learners~\cite{kirkpatrick2017overcoming, zenke2017continual, aljundi2018memory, chaudhry2018riemannian, chaudhry2018efficient, serra2018overcoming, dhar2019learning}, learn from task batches, where each batch has a distinct task that is often a binary classification problem. These methods assume the task label is available during evaluation so that the correct ``output'' head can be selected, and when this assumption is violated these methods fail~\cite{hayes2020lifelong,hayes2020REMIND}. SIESTA does not require task labels for prediction, which are typically not available in real-world applications.

\paragraph{Incremental Class Batch Learning.} In this paradigm, a dataset is split into multiple batches, where each batch consists of mutually exclusive categories, without any revisiting of categories. An agent is given a batch to learn for as long as it likes and typically can use some auxiliary memory for rehearsal. This paradigm has been studied with rehearsal-based methods~\cite{hayes2021replay, abraham2005memory, belouadah2019il2m, castro2018end, chaudhry2018efficient, french1997pseudo, hayes2019memory, hayes2020REMIND, hou2019learning, rebuffi2017icarl, tao2020topology, wu2019large} that store previously observed data in a memory buffer or reconstruct them to rehearse alongside new data. It has also been studied in regularization based methods ~\cite{chaudhry2018efficient, aljundi2018memory, chaudhry2018riemannian, dhar2019learning, kirkpatrick2017overcoming, fernando2017pathnet, coop2013ensemble, li2017learning, lopez2017gradient, ritter2018online, serra2018overcoming, zenke2017continual} that constrain new weight updates to penalize large deviations from past weights, as well as dynamic methods~\cite{douillard2021dytox, draelos2017neurogenesis, yoon2017lifelong, hou2018lifelong, ostapenko2019learning, rusu2016progressive, yan2021dynamically} that incrementally increase the capacity of a DNN over time. While task labels are not used during prediction, evaluation takes place between batches and many methods require large batches (e.g., thousands of examples) or they fail~\cite{hayes2020REMIND}. Some methods use a large number of parameters for keeping copies of the network in memory for distillation~\cite{castro2018end,kang2022class}. In contrast, SIESTA can perform inference at any time and can operate in scenarios where classes are revisited.

\paragraph{Online Continual Learning.} Unlike batch learning paradigms, in online continual learning, an agent observes data sequentially and learns them instance-by-instance in a single pass through the dataset. To study catastrophic forgetting, examples are typically ordered by class, although alternative orders are sometimes studied~\cite{hayes2020REMIND}. Evaluation can occur at any point during training. This setting eliminates looping over data many times and evaluation between batches, thus making it more efficient in terms of memory and compute time, which is desirable for embedded devices. This paradigm has primarily been studied on smaller datasets (CIFAR-100)~\cite{lopez2017gradient, chaudhry2018efficient, aljundi2019online, wang2021acae}, although some methods have been shown to scale to ImageNet-1K~\cite{hayes2020lifelong, hayes2019memory, hayes2020lifelong, gallardo2021self}. For ImageNet-1K, these methods under-perform incremental batch learning methods. SIESTA's paradigm is a compromise that captures most of the benefits of online continual learning, while enabling increased accuracy.

\input{figures/siesta}

\paragraph{Paradigm Relationships.} The formal online continual learning with offline consolidation setting can be configured to mirror other continual learning settings. For incremental class batch learning, where the learner receives large batches of training examples (e.g., $n>100,000$ for ImageNet-1K~\cite{rebuffi2017icarl,yan2021dynamically}), buffer $b=b_{\textrm{recent}}+b_{\textrm{buffer}}+b_{\textrm{auxiliary}}$ would be sufficiently large to hold all $n$ observations from the $j$'th online phase ($b_{\textrm{recent}}$) as well as past examples used for rehearsal ($b_{\textrm{buffer}}$), and any additional memory needed for other purposes ($b_{\textrm{auxiliary}}$) (e.g., distillation), 
and $m$ is typically very large (e.g., for the state-of-the-art DyTox method, $m > 500n$~\cite{douillard2021dytox}).
A pseudo-online algorithm like REMIND~\cite{hayes2020REMIND} uses a configuration of $n=1$ and $m=51$. For both REMIND and SIESTA, $b$ only acts as a buffer for storing compressed past observations and their labels.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The SIESTA Algorithm}

The SIESTA algorithm (Fig.~\ref{fig:siesta-overview}) alternates between awake and sleep phases. The awake phase involves online learning as well as sample compression, storage, and inference. The sleep phase involves memory consolidation via brief periods of offline learning. SIESTA is designed to handle data streams with arbitrary class orders, ranging from iid to class incremental paradigms. 

SIESTA is a feed-forward DNN defined as $\mathcal{F} \left( \mathcal{G} \left( \mathcal{H} \left( \cdot \right) \right) \right)$, where $\mathcal{H}(\cdot)$ contains the bottom layers, $\mathcal{G}(\cdot)$ contains the top layers prior to the output layer, and $\mathcal{F}(\cdot)$ is the output layer. Specifically, SIESTA takes as input a 3rd-order tensor $\mathbf{X}$. $\mathcal{H}(\cdot)$ produces $\mathbf{Z} = \mathcal{H} \left( \mathbf{X} \right)$, where $\mathbf{Z} \in \mathbb{R}^{r \times s \times d}$, $r$ and $s$ are the tensor spatial dimensions, and $d$ are the tensor channel dimensions. This tensor is then transformed into a vector embedding, i.e., $\mathbf{z} = \mathcal{G} \left( \mathbf{Z}  \right)$. The output layer $\mathcal{F} \left( \cdot \right)$ is then described with cosine softmax, where the score for the $k$'th class is given by:
\begin{equation}
\label{inference_eq}
{p_k} = \frac{{\exp \left( {{a_k }{\tau ^{ - 1}}} \right)}}{{\sum\nolimits_j {\exp \left( {{a_j}{\tau ^{ - 1}}} \right)} }},
\end{equation}
where 
\begin{equation}
{a_k} = \frac{{{\mathbf{f}}_k^T{\mathbf{z}}}}{{{{\left\| {{{\mathbf{f}}_k}} \right\|}_2}{{\left\| {\mathbf{z}} \right\|}_2}}} ,
\end{equation}

$\mathbf{f}_k$ is the weight vector for the $k$'th class,
% \tyler{Could we replace $w_{k}$ with $f_{k}$ since the output layer is called $\mathcal{F}$?} \jhair{done}, 
and $\tau \in \mathbb{R}$ is a learned temperature used during optimization. It has been proven that cosine softmax encourages greater class separation than softmax \cite{kornblith2021better}. In our implementation, $\mathcal{H}$ and $\mathcal{G}$ are both convolutional networks; however, other architectural choices would be suitable.


Following \cite{hayes2020REMIND}, prior to continual learning, the DNN is initialized by pre-training on an initial set of $N$ training samples, e.g., images from the first 100 classes of ImageNet. Tensor features $\mathbf{Z}$ are extracted from each of the $N$ samples and used to fit a Product Quantization (PQ) model~\cite{jegou2010product} to all $rsN$ $d$-dimensional vectors in these tensors. This enables us to efficiently store and reconstruct compressed representations of $\mathbf{Z}$. This approach enables SIESTA to much more efficiently use memory for rehearsal than methods that store raw images. The network $\mathcal{H} \left( \cdot \right)$ is then kept fixed during continual learning.
% , as shown in Fig.~\ref{fig:siesta-overview}. 
While this aspect of SIESTA is the same as REMIND, SIESTA differs significantly in its capabilities and how it is trained. REMIND uses replay during online learning, but SIESTA does not require replay during its online phase. Additionally, SIESTA performs offline learning phases periodically, while REMIND does not.
We next describe how the online and offline learning phases operate.


\subsection{Online Learning while Awake}
\label{sec:siesta_awake}

During the awake phase (see Fig.~\ref{fig:siesta-overview}), only the output layer $\mathcal{F}$ of the DNN is updated. This enables SIESTA to avoid catastrophic forgetting and permits lightweight online updates.  When SIESTA receives an input tensor $\mathbf{X}_t$ at time $t$, $\mathbf{Z}_t$ is then compressed and saved in a limited-sized storage buffer using PQ along with its class label. If the buffer is full, then a randomly selected sample is removed from the class with the most samples. Subsequently, the output layer weights are updated with simple running updates for the appropriate class. The update for the output layer weight vector for class $k$ is given by
\begin{equation}
    {{\mathbf{f}}_k} \leftarrow \frac{{{c_k}{{\mathbf{f}}_k} + {{\mathbf{z}}_t}}}{{{c_k} + 1}},
\end{equation}
where $c_k$ is an integer counter for class $k$.
After updating the weight vector, $c_k$ is incremented, i.e., $c_k \leftarrow c_k + 1$. For inference, the class with the highest score $p_k$ in Eq.~\ref{inference_eq} is selected as the predicted class.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Memory Consolidation During Sleep}
\label{sec:sleep}

During the sleep phase (see Fig.~\ref{fig:siesta-overview}), the output layer $\mathcal{F}$ and the top layers $\mathcal{G}$ are trained using rehearsal, while the bottom layers $\mathcal{H}$ are kept frozen. Rehearsal consists of selecting mini-batches of stored examples in the buffer for reconstruction and then using them to update the network with backpropagation. Following the paradigm in Sec.~\ref{sec:formal-paradigm}, the DNN is allowed at most $m$ gradient descent updates. Given a mini-batch of size $q$, each sleep cycle therefore consists of updating the DNN with $n$ mini-batches, where the total number of updates is $m = q \times n$. At the beginning of a sleep cycle, the samples chosen for reconstruction are governed by a policy. We compare multiple policies for selecting the samples to reconstruct in Sec.~\ref{sec:replay-policy}, but our main results all use balanced uniform sampling, i.e., sampling an equal number from each class, which worked best on class balanced datasets and was competitive on long-tailed ones.

While augmentation is typically applied directly to images, here we apply it to the reconstructed $\mathbf{Z}$ tensors. We use two forms of augmentation: manifold mix-up~\cite{verma2019manifold} and cut-mix~\cite{yun2019cutmix}. Both strategies are used in the standard manner, except instead of producing a weighted combination of two images, we create a weighted combination of tensors.

In our main results, we have the network sleep every $120K$ samples, which in the incremental class learning setting corresponds to training on 100 categories. We study the impact of sleep frequency in Sec.~\ref{sec:sleep-study}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Network Architecture \& Initialization}

While continual learning is starting to use transformers~\cite{douillard2021dytox}, recent work has primarily used ResNet18~\cite{rebuffi2017icarl, wu2019large, castro2018end, wu2019large, hayes2020REMIND, yan2021dynamically}. However, ResNet18 has been shown to perform worse than other similarly sized DNNs~\cite{hayes2022online}. Moreover, given one of the major applications of continual learning is on-device learning, using a DNN designed for embedded devices is ideal. Therefore, in our main results, we use MobileNetV3-L~\cite{howard2019searching}. MobileNetV3-L is lightweight with $2\times$ fewer parameters than ResNet18 and has lower latency. Since PQ encodes features across channels, MobileNetV3-L is more suitable for compressing its features with relatively less reconstruction error than ResNet18.  We compare MobileNetV3-L and ResNet18 in Sec.~\ref{sec:eval_arch}. 

During pre-training (i.e., base initialization), we adopt the approach used in \cite{gallardo2021self}, where the entire network is pre-trained using the self-supervised learning algorithm SwAV~\cite{caron2020unsupervised}. In \cite{caron2020unsupervised}, SwAV worked significantly better than supervised pre-training, as the features transferred more effectively to classes unseen during pre-training. We also use SwAV with DER~\cite{yan2021dynamically} and REMIND~\cite{hayes2020REMIND}. 

Using MobileNetV3-L, we set network $\mathcal{H}$ to be the first 8 layers of the network, consisting of $2.19\%$ of the network parameters. Using images of size $224\times224$ pixels, $\mathcal{H}$ produces a tensor $\mathbf{Z} \in \mathbb{R}^{14 \times 14 \times 80}$. For PQ, we use \emph{Optimized Product Quantization (OPQ)} from FAISS~\cite{johnson2019billion}, which is used to compress and reconstruct the $14^2$ 80-dimensional vectors that make up each tensor. Following REMIND, we exclusively use reconstructed versions of the output of $\mathcal{H}$ during continual learning. The remaining $11$ layers of MobileNetV3-L, consisting of 97.81\% of the network parameters, are trained during continual learning. 
An additional study of the quantization layer for MobileNetV3-L is in Sec.~\ref{mobilenetlayer}.

\input{tables/no_aug_expts}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Setup}
\label{sec:experimental-setup}

\paragraph{Comparison Models.} We compare SIESTA to a variety of baseline and state-of-the-art methods, including online learners REMIND~\cite{hayes2020REMIND}, SLDA~\cite{hayes2020lifelong}, NCM~\cite{mensink2013distance}; incremental batch learners iCaRL~\cite{rebuffi2017icarl}, BiC~\cite{wu2019large}, End-to-End~\cite{castro2018end}, WA~\cite{zhao2020maintaining}, Simple-DER~\cite{li2021preserving}, DER \cite{yan2021dynamically}, DyTox \cite{douillard2021dytox}; and an offline learner. We compare with two variants of DER: DER without pruning (referred to as DER$\dag$) and DER with pruning (referred to as DER$\ast$). These methods have been designed to be effective for incremental class learning on ImageNet-1K, and more details are in Sec.~\ref{comparison_algorithms}.
SIESTA is trained with cross-entropy loss and uses SGD as its optimizer with the OneCycle learning rate (LR) scheduler~\cite{smith2017super} during each sleep phase. It uses a higher initial LR in the last layer to help learn the new tasks and a lower LR in earlier layers to mitigate forgetting of previously learned information. For each sleep cycle, we use a batch size of $64$, momentum $0.9$, weight decay $1e$-$5$, an initial LR of $0.2$ for the last layer, and an initial LR of $0.07$ for the earlier layers. Additional details of SIESTA and implementation details for other methods are provided in Sec.~\ref{section:additional_implementation_details}.

\paragraph{Datasets and Evaluation Criteria.} For our main results, we use ImageNet ILSVRC-2012, which is the standard object recognition benchmark for testing a model's ability to scale~\cite{russakovsky2015imagenet}. It has 1.2 million images uniformly distributed across 1000 categories. We also use a long-tailed version of Places to evaluate rehearsal policies (see Sec.~\ref{sec:replay-policy} for details). We configure the datasets in the continual iid setting and the class incremental learning setting, where data is ordered by class and images are shuffled within each class.

For evaluation, we report the average accuracy $\mu$ over all steps $T$, where $\mu = \frac{1}{T} \sum_{t=1}^{T} \alpha_{t}$, with $\alpha_{t}$ referring to the accuracy at step $t$. We also report the final accuracy $\alpha$. We use top-5 accuracy for  ImageNet-1K and top-1 accuracy for Places. Additionally, we measure the total number of parameters $\mathcal{P}$ (in millions) and the total memory $\mathcal{M}$ (in gigabytes) used by each model. We also measure the total number of updates $\mathcal{U}$, where an update consists of a backward pass on a single input.

\section{Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Comparison with the Offline Learner}
\label{sec:offline-comparison-experiments}

We first compare SIESTA, DER, and REMIND to an offline learner. For many real-world applications, a continual learner that performs significantly worse than an offline learner is unacceptable. Moreover, for these applications we cannot make assumptions about the data stream's class order distribution; so for SIESTA and REMIND, we study both the class incremental and iid continual learning settings, which can be seen as an extreme best case and extreme worst case scenarios. DER, as designed, is only capable of incremental class learning. SIESTA, DER, and REMIND use the same SwAV pre-trained MobileNetV3-L DNN on the first 100 classes of ImageNet-1K. The offline learner is a MobileNetV3-L trained from scratch.

We first show results where all models do not use augmentations, including the offline learner. SIESTA used 1.28M updates per sleep cycle. All continual learners used a similar total number of updates.
Results for incremental class learning are given in Table~\ref{tab:no_aug} and learning curves in Fig.~\ref{fig:learning_curve_der_siesta}. Both DER and REMIND perform over 9\% worse (absolute) than the offline learner for final accuracy. SIESTA had similar final accuracy for both the class incremental (83.59\%) and iid (83.45\%) scenarios, whereas REMIND performed better in the iid setting (79.52\%) than the class incremental setting (74.31\%). Using Chochran's Q test \cite{conover1999practical}, a non-parametric paired statistical test that can be used for comparing multiple classifiers, we found there was no significant difference among SIESTA's final accuracy for the iid and class incremental settings compared to the offline learner ($P = 0.08$). Therefore, SIESTA achieves \emph{``no forgetting''} by matching the performance of the offline MobileNetV3-L across orderings. We conducted a similar analysis when augmentations are used in Sec.~\ref{sec:offline-with-augmentations}.

\input{figures/learning_curve_der_siesta}

\begin{figure}[t]
    \centering     
     \includegraphics[width=.8\linewidth]{images/sleep_plot3_old.png}
     \caption{Learning curves of SIESTA's pre- and post-sleep performance as a function of seen classes.\label{subfig:sleep_plot3}}
\end{figure}


\subsection{Sleep Analyses}
\label{sec:sleep-study}

\begin{figure}[t]
    \centering     
     \includegraphics[width=.8\linewidth]{images/sleep_plot4.png}
     \caption{The impact of sleep length $m$ on SIESTA.\label{subfig:sleep_plot4}}
\end{figure}

We asked the question \emph{``What is the impact of sleep on SIESTA's ability to learn and remember?''} These experiments are for incremental class learning, and data augmentation is not used. First, we analyze the pre-sleep and post-sleep performance of SIESTA on ImageNet-1K in Fig.~\ref{subfig:sleep_plot3} using the same sleep settings as in Sec.~\ref{sec:offline-comparison-experiments}. We see that the performance of SIESTA after sleep is consistently higher than before sleep for all increments, providing a $4.25 \pm 1.38\%$ average absolute increase in accuracy after each sleep cycle. 

Next, we study the impact of sleep frequency. To do this, we trained models with a sleep frequency of 50, 100 (default), or 150 classes seen. For this analysis, we used a fixed sleep length, i.e., a fixed number of updates per sleep ($m=1.28M$). The absolute average increase in post-sleep accuracy was $2.29 \pm 0.86\%$ for 50 classes, $4.25 \pm 1.38\%$ for 100 classes, and $6.18 \pm 2.15\%$ for 150 classes. Despite the 50 class increment being trained with more updates, the 100 class increment achieved better post-sleep performance. We hypothesize that this happens because frequent sleep leads to greater perturbation in the DNN's weights, resulting in gradual forgetting of old memories.

In Fig.~\ref{subfig:sleep_plot4}, we study the impact of sleep length on SIESTA's performance by varying the number of updates ($m$) during each sleep period, where SIESTA slept every 100 classes. We observe that as sleep length increases, SIESTA's performance also increases; however, as the sleep length increases, SIESTA requires more updates and there are diminishing returns in terms of increases in accuracy, so we must strike a balance between accuracy and efficiency.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{State-of-the-Art Comparisons}
\label{sec:sota-experiments}


To put our work in context with respect to existing methods, we compare SIESTA against recent incremental class learning methods on ImageNet-1K. All methods, except for SIESTA (No Aug), use augmentations and have a variety of different DNN architectures. SIESTA (Aug), which uses augmentations, used 6.4 million updates per sleep cycle ($m=6.4M$). With the exception of REMIND and SIESTA, we use published performance numbers for all methods. REMIND and SIESTA both use 2.05GB of memory for rehearsal and SwAV for pre-training. Additional details for the comparison algorithms are provided in Sec.~\ref{comparison_algorithms}.

Overall results are in Table~\ref{tab:aug} and Fig.~\ref{fig:summary}. SIESTA performs best in average accuracy $\mu$ and final accuracy $\alpha$, while having fewer DNN parameters $\mathcal{P}$, lower auxiliary memory usage $\mathcal{M}$, and fewer total updates $\mathcal{U}$. While REMIND uses the same amount of auxiliary memory and a similar number of DNN updates, SIESTA (Aug) outperforms REMIND by $10\%$ (absolute) in final accuracy. SIESTA (Aug) exceeds DyTox, the method with the second highest final accuracy, by $3.08\%$ (absolute), while using 12$\times$ fewer network updates and 11$\times$ less memory.

SIESTA (No Aug) has comparable performance to state of the art batch learners like DER and DyTox, while requiring 18$\times$ fewer updates. Moreover, SIESTA (Aug) can provide even further performance gains (3.41\% absolute improvement in final accuracy) at the cost of 5$\times$ more updates. %\ck{fill in numbers}

\input{tables/aug_expts}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Computational Analysis}

We trained SIESTA on a single NVIDIA A$5000$ GPU.
Excluding pre-training, training SIESTA on ImageNet-1K required only $2.4$ hours using the configurations from Sec.~\ref{sec:offline-comparison-experiments} when augmentations were not used. On the same hardware, training SIESTA is $3.4\times$ faster than REMIND. All methods use a comparable amount of auxiliary memory, where REMIND and SIESTA use it to store compressed tensors and the others use it to store 20,000 images.

For example, many existing models require a large number of parameters ($11.68-116.89$ million), DNN updates ($79-692$ million), and additional memory to store the current batch and/or to keep additional copies of the DNN in memory ($22$ gigabytes~\cite{castro2018end,li2021preserving,rebuffi2017icarl,wu2019large,zhao2020maintaining,yan2021dynamically,douillard2021dytox}). This makes these methods infeasible for on-device learning. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

This paper considers the problem of supervised continual learning, where the learner incrementally learns from a sequence about which it cannot make distributional assumptions. We argue that for real-world applications, continual learning needs to rival an offline learner and be more computationally efficient than periodically re-training from scratch. We also argue that systems need to be designed to handle arbitrary class orderings, where two extremes are iid and class-incremental learning, whereas many continual learning systems are bespoke to the incremental class learning scenario. We demonstrated that SIESTA largely meets these goals, achieving identical performance to the offline learner when augmentations are not used, and outperforming existing continual learning methods in accuracy using less compute when augmentations are used.

Computational efficiency in continual learning has long been a selling point of the research, but it has not been a focus of prior work. Training large DNNs from scratch requires a huge amount of energy that can result in a large amount of greenhouse gas emissions \cite{patterson2021carbon,wu2022sustainable}.
From a financial perspective, training large models often requires a large amount of electricity, expensive hardware, and cloud computing resources. Many are calling for more computationally efficient algorithms to be developed~\cite{strubell2020energy,van2021sustainable}, and we believe that continual learning can help address this problem while also enabling greater functionality provided by continuously updating DNNs with new information.

Because one of the major applications for continual learning is on-device learning, we focused on mobile DNNs, which can enable devices to learn about their users and their environments in a way that provides greater privacy than relying on cloud computing. Learning on-device can also be useful in areas with poor internet connectivity. However, computational efficiency improvements are mostly needed for updating large DNNs, e.g., Foundation Models~\cite{radford2021learning,radford2021learning,devlin2018bert}, that have billions/trillions of parameters and can take months to train using thousands of GPUs.

In this work, we only evaluated CNN architectures with SIESTA; however, the general framework is amenable to other architectures as long as the representations can be quantized. For example, SIESTA could be extended for models like Swin Transformers~\cite{liu2021swin} or even Graph Neural Networks~\cite{zhou2020graph}, where we could quantize graph representations.
Exploring the use of non-CNN architectures is critical for using SIESTA in non-vision modalities, e.g., audio and text data, which would be an exciting area of future work. Another interesting area of research could be incorporating additional data modalities over time to improve existing task performance and enhancing the learning of new tasks. Another direction is to use SIESTA for tasks such as continual learning in object detection, which was previously done using a REMIND-based system~\cite{acharya2020rodeo}.

SIESTA depends on the initial layers of the network, $\mathcal{H}$, being universal features for the domain, since they are not trained after the base initialization phase. Following others \cite{rebuffi2017icarl, belouadah2019il2m, gallardo2021self}, we did base initialization on the first 100 classes of ImageNet-1K. While our model rivaled the offline learner when augmentations were not used, there was a small gap when augmentations were used between SIESTA and the offline learner. We hypothesize that this gap would be closed by improving the features in $\mathcal{H}$. Two potential ways to achieve this goal would be using a superior self-supervised learning algorithm than SwAV or by training on additional data. For continual learning for real applications, it would be prudent to initialize the DNN from a very large unlabeled dataset with self-supervised learning, which would likely work significantly better.

We allowed methods to perform a pre-training phase, which was not included in the total number of updates. We argue that this is viable since pre-training only occurs once during a model's lifetime, and by performing continual learning, a practitioner would not need to periodically re-train a model on new and old (pre-training) data. Future work should investigate the role of more performant self-supervised learning models for producing invariant features. Invariant features could reduce the need for augmentation during continual learning, further improving efficiency.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

We proposed SIESTA, a scalable, faster and lightweight continual learning framework equipped with offline memory consolidation. We reduced computational overhead by making online learning free of rehearsal or expensive parametric updates during the wake phase. This closely aligns with real-time applications such as edge-devices, mobile phones, smart home appliances, robots, and virtual assistants. To effectively learn from a non-stationary data stream, the short-term wake memories were transferred into the DNN for long-term storage during offline sleep periods. Consequently, our model overcame forgetting of past knowledge.
We showed that sleep improved online performance while outperforming state-of-the art methods. SIESTA achieves similar accuracy to DyTox~\cite{douillard2021dytox} using an order of magnitude fewer updates when augmentations are not used, and surpasses DyTox in terms of accuracy when using augmentations. Although we evaluated SIESTA on image classification where augmentations are widely used, SIESTA can also be used for non-vision tasks where augmentations are not widely used.


\ifthenelse{\boolean{ack}}{
\paragraph{Acknowledgements.}
We thank Robik Shrestha for providing comments on the manuscript. This work was supported in part by NSF awards \#1909696 and \#2047556. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies or endorsements of any sponsor.
}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


% Supplemental Materials
\ifthenelse{\boolean{combined}}{
\clearpage
\begin{center}
    {\Large Supplemental Material \normalsize}
\end{center}
\input{supplemental.tex}
}



\end{document}