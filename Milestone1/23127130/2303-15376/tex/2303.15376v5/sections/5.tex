\section{Inference}
\label{Section5}

\subsection{ CPCM algorithm - bivariate case   }
\label{Section_Algorithm}

Our CPCM methodology is based on selecting an appropriate causal model (in our case, the choice of collection $\{F_1, \dots, F_k\}$) and a measure of a model fit. In the following subsections, we measure the model fit by exploiting the principle of independence between the cause and the mechanism. 


We say that a DAG $\mathcal{G}$ is \textbf{plausible} under $CPCM(F)$ model if the joint distribution \textit{can} be generated via $CPCM(F)$ model with graph $\mathcal{G}$. The Algorithm~\ref{Algorithm1} describes the main steps to test the plausibility and estimation of $\mathcal{G}$ in the bivariate case. 

\begin{algorithm}[H]
  \SetAlgoLined
  \KwData{ Random sample $(x_{1,1}, x_{2,1})^\top, \dots, (x_{1,n}, x_{2,n})^\top$}
  \KwResult{Plausibility of $X_1\to X_2$, $X_2\to X_1$ and estimate $\hat{\mathcal{G}}$}
\textbf{Step 0) }Test independence between $X_1$ and $X_2$. 

\textbf{Step 1) } Determine plausibility of   $X_1\to X_2$ using the following: 

  
  \hspace{0.3cm}\textbf{1a)} Estimate $\theta(X_1)$ in $X_2 = F^{-1}(\varepsilon_2; \theta(X_1))$; compute $\hat{\varepsilon}_2 := F(X_2; \hat{\theta}(X_1))$.
  
  \hspace{0.3cm}\textbf{1b)} Test independence between $\hat{\varepsilon}_2$ and $X_1$. If the p-value is larger than $\alpha=0.05$, mark $X_1 \to X_2$ as \textit{plausible}.

  \textbf{Step 2:} Repeat Step 1 for $X_2 \to X_1$.

\textbf{Forced estimate}  (Choose the direction with the higher residual independence):  Return $X_1 \to X_2$ if $\text{p-value}(\hat{\varepsilon}_2, X_1) > \text{p-value}(\hat{\varepsilon}_1, X_2)$, else return $X_2 \to X_1$.

\textbf{Conservative Estimate:}  
\begin{itemize}
 \setlength{\itemsep}{0pt} % Reduce space between items
    \setlength{\parskip}{0pt} % Reduce space between paragraphs
    \item If Step 0 fails to reject independence, return $\hat{\mathcal{G}} = \emptyset$.
    \item If only one direction is plausible, return it as $\hat{\mathcal{G}}$.
    \item If both directions are plausible, return \texttt{"Unidentifiable case"}.
    \item If neither is plausible, return \texttt{"Assumptions not fulfilled"}.
\end{itemize}

  \caption{CPCM(F)}
  \label{Algorithm1}
\end{algorithm}


An estimation of ${\theta}(X_1)$ in the Step 1a) can be done using any machine learning algorithm, such as GAM, GAMLSS, random forest, or neural networks \citep{GAM, GAMLSS}. For the independence test in in  Step 1b), we can use a HSIC test (kernel-based Hilbert-Schmidt independence test, \cite{Kernel_based_tests}) or a copula-based test \citep{copula_based_independence_test}. 

The conservative estimate $\hat{\mathcal{G}}$ helps guard against unfulfilled assumptions or unidentifiable cases. If it returns ``Assumptions not fulfilled,'' it means that we were unable to fit the $CPCM(F)$ model in either direction, suggesting that the variables do not follow the $CPCM(F)$ model.  In this case, one should consider increasing the complexity (i.e., the number of parameters) of $F$. This is discussed further in Section~\ref{Section5Model_choice}. 

If it returns ``Unidentifiable case,'' this means that we were able to fit the $CPCM(F)$ model in both directions. This could indicate that the sample size is too small or that we are in an unfortunate unidentifiable case, such as the one described in Consequence~\ref{consequenceprva}.  In this case, one should consider decreasing the complexity (i.e., the number of parameters) of $F$.

While the warnings from the conservative estimate  $\hat{\mathcal{G}}$  are useful, we often require a single estimate of $\hat{\mathcal{G}}$  for comparison with other benchmark methods. 

\subsubsection{Extension to $CPCM(F_1, \dots, F_k)$}
The following adjustment to Step 1 in Algorithm~\ref{Algorithm1} can be applied to accommodate the \(CPCM(F_1, \dots, F_k)\) model, given a collection \(\{F_1, \dots, F_k\}\).


\begin{algorithm*}[H]
  \SetAlgoLined
$\boldsymbol{CPCM(F_1, \dots, F_k)}$

    \textbf{Step 1) } Determine plausibility of   $X_1\to X_2$ using the following: 

\textbf{$\,\,\,\,\,\,$Step 1a) } Estimate the set $S:=\{j\in \{1, \dots, k\}: supp(F_j) = supp(X_2)\}$. If empty, return ``Inappropriate choice of $F$''. 

\textbf{$\,\,\,\,\,\,$Step 1b) } For all  $j\in \hat{S}$,  estimate $\theta(X_1)$ in a model $X_2 = F_j\big(\varepsilon_2; \theta(X_1)\big)$,  and compute probability transform $\hat{\varepsilon}_2^j := F_j\big(X_2; \hat{\theta}(X_1)\big)$.  

\textbf{$\,\,\,\,\,\,$Step 1c)} Compute the p-value of an independence test between $\hat{\varepsilon}_2^j$ and $X_1$ for all $j\in\hat{S}$. Choose the largest p-value. Direction $X_1\to X_2$ is marked as \textit{plausible} if the p-value is larger than  $\alpha=0.05$. 

%  \caption{Modification of Step 1 in Algorithm~\ref{Algorithm1} accounting for $CPCM(F_1, \dots, F_k)$}
%  \label{Algorithm2}
\end{algorithm*}

By estimating the set $S$ in Step 1a), we can  preliminarily filter out the $F_j$ choices that are evidently unsuitable. We can apply a simple heuristic under the assumption that \( \text{supp}(X_2) \) and \( \text{supp}(F_j) \) for all \( j \) are one of the following: 1) \(\mathbb{R}\) (Gaussian), 2) \(\mathbb{R}^+\) (Gamma), 3) \([0,1]\) (Beta), or 4) \(\mathbb{N}\)(Poisson). The heuristic is as follows: if the number of unique values in \( (x_{2,1}, \dots, x_{2,n}) \) is fewer than \( n/10 \) and the values lie in $\mathbb{N}$, we set \( \text{supp}(X_2) = \mathbb{N} \). Otherwise, if all values lie within \([0,1]\), we set \( \text{supp}(X_2) = [0,1] \). To distinguish between \(\mathbb{R}\) and \(\mathbb{R}^+\), we use the skewness of the distribution: if the skewness is close to 0, the distribution resembles a Gaussian distribution, so we set \( \text{supp}(X_2) = \mathbb{R} \). Otherwise, the distribution resembles a Gamma distribution, so we set \( \text{supp}(X_2) = \mathbb{R}^+ \).





\subsection{Multivariate case: $CPCM(F)$ as a score-based DAG optimization
\label{Section_score_based_algorithm}
}
The forced estimate of $\mathcal{G}$ in Algorithm~\ref{Algorithm1} can be seen as a score-based algorithm; defining the score of a graph as a p-value of the corresponding independence test, we simply compare the scores of graphs $X_1\to X_2$ and $X_2\to X_1$. In the following, we formalize this idea, leading to generalizing the CPCM inference to multivariate case. 

Following the ideas of \cite{Bregmans_information, Score-based_causal_learning, Peters2014}, we use the following penalized independence score: 
\begin{equation}\label{score_definition1}
\hat{\mathcal{G}} =  \argmin_{\mathcal{G}\in DAG(d)}s(\mathcal{G}) = \argmin_{\mathcal{G}\in DAG(d)}\rho (\hat{\varepsilon}_1, \dots, \hat{\varepsilon}_d) + \lambda (\text{Number of edges in }\mathcal{G}),
\end{equation}
where $\rho$ represents some measure of independence, $\lambda$ is a hyperparameter, $DAG(d)$ is the set of all DAGs over $V=\{1, \dots, d\}$ and $\hat{\varepsilon}_1, \dots, \hat{\varepsilon}_d$ are noise estimators obtained by estimating ${\theta}_i(\textbf{X}_{pa_i(\mathcal{G})})$ and putting $\hat{\varepsilon}_i := F\big(X_i; \hat{\theta}_i(\textbf{X}_{pa_i(\mathcal{G})})\big)$ analogously to Algorithm~\ref{Algorithm1}. 

With regard to choice of $\rho$, we use minus the logarithm of the p-value of the copula-based independence test \citep{copula_based_independence_test} and $\lambda = 2$. These choices appear to work well in practice, but we do not provide any theoretical justification of their optimality.

Analogously to the bivariate case, we can define that a DAG ${\mathcal{G}}$ is \textit{plausible} if the p-value of the corresponding independence test between $(\hat{\varepsilon}_1, \dots, \hat{\varepsilon}_d)$ is larger than $\alpha =0.05$. If every ${\mathcal{G}}\in DAG(d)$ is not plausible, it suggests that the variables do not follow the $CPCM(F)$ model.  In this case, one should consider increasing the complexity (i.e., the number of parameters) of $F$. However, on the contrary to the bivariate case, many DAGs can be plausible in an identifiable $CPCM(F)$ model. In particular, if a true causal graph $\mathcal{G}$ is plausible, than any $\tilde{\mathcal{G}}\supseteq \mathcal{G}$ should be plausible. 

 It is important to note that in a bivariate case, score based estimate \eqref{score_definition1} is equal to the forced output of Algorithm~\ref{Algorithm1}, given $\lambda = -\infty$.



\subsubsection{$CPCM(F_1,\dots, F_k)$ extension
}
Similarly to the extension of Algorithm~\ref{Algorithm1}, the following adjustment can be applied to accommodate the \(CPCM(F_1, \dots, F_k)\) model, given a collection \(\{F_1, \dots, F_k\}\):

\begin{equation*}
s(\mathcal{G}) = \min_{j_1\in \hat{S}_1, \dots, j_d\in \hat{S}_d}\rho (\hat{\varepsilon}_1^{j_i}, \dots, \hat{\varepsilon}_d^{j_d}) + \lambda (\text{Number of edges in }\mathcal{G}),
\end{equation*}
where $\hat{S}_i$ are estimates of $S_i:= \{ j\in\{1, \dots, k\}: supp(X_i) = supp(F_j) \}$, and $\hat{\varepsilon}_i^{j_i} := F_{j_i}\big(X_i; \hat{\theta}_i(\textbf{X}_{pa_i(\mathcal{G})})\big)$. If $supp(F_i) \neq supp(F_j)$ for all $i\neq j$, we end up with a single evaluation of the score function for each possible $\mathcal{G}$. 



\subsubsection{Consistency
}

\cite{reviewANMMooij} demonstrates consistency of a causal discovery algorithm in ANMs.  We establish an analogous result for $CPCM(F_1, \dots, F_k)$.


\begin{proposition}
\label{consistency_proposition}
Let $(X_1, X_2)$ follow an identifiable  $CPCM(F_1, \dots, F_k)$ with DAG $\mathcal{G}$. Then, our score based algorithm presented in Section~\ref{Section_score_based_algorithm} is consistent, meaning that
$$\hat{\mathcal{G}} \overset{P}{\to}\mathcal{G}\,\,\,as\,\,n\to\infty,$$
given that we employ a ``suitable'' estimation procedure for the estimator $\hat{\varepsilon}_i$, we use HSIC score as our choice of $\rho$ and consistent estimators $\hat{S}_i$ (for definitions, proof and more details, see  Appendix~\ref{Appendix_consistency}). 
\end{proposition}

\subsubsection{Scalability }
\label{section_scalability}
The main disadvantage of the proposed method is that we have to go through all graphs $\mathcal{G}\in DAG(d)$, which is possible only for very small $d$. However, even though graph learning is typically NP-hard \citep{NP-hard_score_based_causal_learning}, numerous algorithms have been proposed  to speed up the process. Many efficient greedy searches exist, ranging from polynomial time without guarantees to slower algorithms with guarantees \citep{Bregmans_information, Greedy_search, Ramsey2016, Nandy2018, Silander2006}. 


In our implementation, we opt for edge-greedy search algorithm \citep{Greedy_search}. The search starts with an empty DAG. It generates neighboring DAGs by adding or removing a single edge at a time.
 Each candidate DAG is evaluated using the CPCM score function \eqref{score_definition1}.  The DAG with the best (lowest) score is selected. If the best candidate improves upon the current DAG, it replaces it, and the process repeats.
The algorithm stops when no further improvements are possible.
 


While this algorithm is computationally scalable up to approximately $d\approx 20$ (estimating an 18-dimensional graph takes about 72 hours on an Intel Core i5-6300U 2.5 GHz processor with 16.0 GB of RAM when sample size $n=1000$), its statistical scalability presents additional challenges. Conducting independence tests and nonparametric regression with potentially many covariates becomes increasingly difficult, particularly in high-dimensional settings, where these tasks require very large sample sizes. Similar limitations apply to other algorithms, such as RESIT, LOCI, and bQCD. As shown in Appendix~\ref{Appendix_simulations}, Figure~\ref{scalability}, the required sample size needs to systematically grow with dimension $d$ to achieve consistent results.

In practice, a hybrid approach is recommended: using PC or FCI algorithms to estimate the skeleton and applying the CPCM algorithm only to smaller subgroups of unoriented edges. Similar strategy has been discussed, for example, in \citep{Goudet2017}, though a detailed exploration is beyond the scope of this paper.

\subsection{Choice of the collection $\{F_1, \dots, F_k\}$}
\label{Section5Model_choice}

\subsubsection{Why $\{F_1, \dots, F_k\}$ cannot be chosen in a data-driven way }

Selecting the collection \(\{F_1, \dots, F_k\}\) is a crucial step in our approach. Unfortunately, there is no principled, data-driven way to select or adapt this collection without access to alternative data. As discussed in Section~\ref{motivation_section}, alternative methods such as ANM, QVF, bQCD, or IGCI all predefine the notion of a ``simple'' distribution. This predefinition is necessary; otherwise, Occam's razor loses its meaning. The following lemma formalizes this idea.  

\begin{lemma}\label{lemma_o_overparametrizacii}
Suppose that the joint distribution $F_{(X_1,X_2)}$ is generated according to the model $CPCM(F_2)$ with graph $X_1\to X_2$, where $F_2$ is a distribution function with one parameter ($q_2=1$) belonging to the exponential family with the corresponding sufficient statistic $T_2$. 

Then, there exists $F_1$ such that the model $CPCM(F_1)$  with graph $X_2\to X_1$ also generates  $F_{(X_1,X_2)}$. In other words, there exists $F_1$ such that the causal graph in $CPCM(F_1, F_2)$ is not identifiable from the joint distribution. 
\end{lemma}

The proof (provided in \hyperref[Proof of lemma_o_overparametrizacii]{Appendix} \ref{Proof of lemma_o_overparametrizacii}) is based on the specific choice of $F_{1}$, such that its sufficient statistic is equal to the parameter $\theta_2$ from the original model $CPCM(F_{2})$. 

It is important to note that such an $F_1$ often results in a rather non-standard distribution. While the notion of a ``standard'' distribution is somewhat philosophical, some distributions are objectively considered standard due to physical motivations—for example, the Gaussian (by the central limit theorem) or the Poisson (which arises when counting independent events). This motivates the definition of a "standard set of well-known distributions," which we discuss in Section~\ref{Section_practical_choices}.

\subsubsection{Unfair game issue}

Even if the theory suggests that it is not possible to fit a $CPCM(F_1, \dots, F_k)$ in both causal directions, this is only an asymptotic result and may not hold in practice with finite samples. Overparameterized models may overfit and capture spurious patterns, while underparameterized ones may violate key assumptions. To ensure a fair comparison, we recommend selecting all $F_i$ with the same number of parameters ($q_i = q_j$). For example, choosing $F_1$ with one parameter and $F_2$ with five ($q_1 = 1$, $q_2 = 5$) introduces bias, since the more flexible model is more likely to fit the data regardless of its correctness (unless we are in the asymptotic regime). We refer to this issue as an ``unfair game.'' 

\subsubsection{Practical choices of the set of ``standard well-known distributions'' and the choices of tests used in our implementation }
\label{Section_practical_choices}

We define two sets of ``standard set of well-known distributions''; set $\mathscr{S}_1$  containing distributions with one parameter and a larger set $\mathscr{S}_2$  containing distributions with two parameters. This avoids the  ``unfair game'' issue discussed before. Both sets should be rich enough to contain a wide range of distributions with different supports and characteristics, but should not contain many distributions with the same support in order to avoid unidentifiable setups. In practice, we mostly restrict our attention to distributions that are implemented in \texttt{mgcv} package in \texttt{family.mgcv} \citep{Wood}.  


In our implementation, we used the following distributions: Gaussian with fixed variance, Gaussian, Gamma, Pareto, Poisson and Negative binomial distribution. Here, $\mathscr{S}_1$ consists of the distributions with one parameter and  $\mathscr{S}_2$ consists of the distributions with two parameters.  However, we should note that many more choices are appropriate and our collections are not exhaustive and may be improved for specific applications where potentially different distributions may be seen as ``standard''. 


The selection between \(\mathscr{S}_1\) and \(\mathscr{S}_2\) should follow a sequential approach. We first choose \(\mathscr{S}_1\), and if all directions are rejected, we then switch to \(\mathscr{S}_2\). If $\mathscr{S}_2$ also rejects all directions, we may further extend the model class by introducing $\mathscr{S}_3$— the class of standard three-parameter distributions. This hierarchical selection process incrementally increases the complexity of the conditional distributions until at least one direction is not rejected. By sequentially expanding from \(\mathscr{S}_1\) to \(\mathscr{S}_2\), and potentially to \(\mathscr{S}_3\), we ensure that the simplest adequate model is chosen while avoiding unnecessary complexity. As an example, consider that a Gaussian model with fixed variance is fitted in both directions. If this model is rejected in both directions, we expand the model to Gaussian with non-fixed variance. Potentially, if this model is also rejected in both directions, we add a shape parameter and consider Generalized Gaussian model \citep{Nadarajah01092005}.  

In our implementation of Algorithm~\ref{Algorithm1}, we use GAM \citep{Wood2} estimation of ${\theta}(X_i)$. With regard to the independence test, we use Hoeffding D-test \citep{Nonparametric_test_for_independence_review} in the bivariate case, copula-based independence test \citep{Kojadinovic2009} in the multivariate case with $n>1000$, and HSIC \citep{ZhangKernelTest} when $n\leq 1000$ (for a review of different tests and their comparisons, see \cite{copula_based_independence_test}). 



