
%--------------- Personalize your document here ---------------

\author{} % Enter your name
\newcommand{\studentID}{} % enter your student ID
\newcommand{\supervisorone}{} % Enter your supervisor's name
\newcommand{\supervisortwo}{}% Leave it empty or enter your second supervisor's name 
\newcommand{\department}{}
\newcommand{\exam}{}

\title{} %Enter the title of your report 
\date{\today} % insert a specific date	

%--------------------------------------------------------------

% This document was adapted from the
% TEMPLATE FOR PHYS250 WORKSHEET created by Alastair McLean
% URL: https://www.overleaf.com/latex/templates/phys250-worksheet-template/xxftvfhmwqdt

% Jefferson Silveira
% Email: 19jdls1@queensu.ca
% Last update: 09-Jun-2021
% If you have any questions or concerns, do not hesitate to contact me.
%--------------------------------------------------------------

\documentclass[a4paper,11pt]{article}
\usepackage[left=30mm,top=30mm,right=30mm,bottom=30mm]{geometry}
\usepackage{etoolbox} %required for cover page
\usepackage{booktabs}
\usepackage[table,xcdraw]{xcolor}
\usepackage[usestackEOL]{stackengine}
\usepackage[round]{natbib}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{float}
\usepackage{hyperref}
\usepackage[capitalise]{cleveref}
\usepackage{enumitem,kantlipsum}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{amsthm}
\usepackage{bbm}% theorems, definitions, etc.
\usepackage{pifont}

\usepackage[ruled,vlined]{algorithm2e}
\usepackage{listings}
\usepackage{dirtytalk}
\usepackage{graphicx}

\usepackage{chngcntr}
\usepackage{apptools}
\AtAppendix{\counterwithin{lemma}{section}}


%\renewcommand{\listingscaption}{Algorithm}
%\renewcommand{\listoflistingscaption}{List of Algorithms}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\F}{\mathcal{F}}
\newcommand{\M}{ (\mathcal{M}_1, \mathcal{M}_2)}


\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}

\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}
\newtheorem*{assumption*}{Assumption}
\newtheorem{observation}{Observation}
\newtheorem*{terminology}{Terminology}
\newtheorem{consequence}{Consequence}



\newtheorem{innercustomthm}{Theorem}
\newenvironment{customthm}[1]
  {\renewcommand\theinnercustomthm{#1}\innercustomthm}
  {\endinnercustomthm}

\newtheorem{innercustomlem}{Lemma}
\newenvironment{customlem}[1]
  {\renewcommand\theinnercustomlem{#1}\innercustomlem}
  {\endinnercustomlem}


\newtheorem{innercustomprop}{Proposition}
\newenvironment{customprop}[1]
  {\renewcommand\theinnercustomprop{#1}\innercustomprop}
  {\endinnercustomprop}
  
\newtheorem{innercustomconsequence}{Consequence}
\newenvironment{customconsequence}[1]
  {\renewcommand\theinnercustomconsequence{#1}\innercustomconsequence}
  {\endinnercustomconsequence}
  
  \newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{notation}{Notation}

\def\b#1{{\color{red}\bf #1}}%

\def\lm#1{{\textcolor{purple}{LM: \bf #1}}}

\newenvironment{myproof}{
  \par\medskip\noindent
  \textit{Proof}.
}{
\newline
\rightline{$\qedsymbol$}
}

\newenvironment{hproof}{%
  \renewcommand{\proofname}{Idea of the proof}\proof}{\endproof}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\newcommand{\indep}{\perp \!\!\! \perp}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\bibliographystyle{plainnat}

\hypersetup{
    colorlinks,
    linkcolor={black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\linespread{1}

\graphicspath{{figures/}}	

% Keywords command
\providecommand{\keywords}[1]
{
  \small	
  \textbf{\textit{Keywords---}} #1
}
%----------------------------------TITLE PAGE -----------------------------------
\title{Identifiability of causal graphs under nonadditive conditionally parametric causal models} 
\author{Juraj Bodík$^{1 \footnote{ Email of the corresponding author: Juraj.Bodik@unil.ch}}$, Valérie Chavez-Demoulin$^1$}
\date{%
    $^1$ {\small Faculté des Hautes Études Commerciales, Université de Lausanne, Switzerland} \\%
    }

%-------------------------------- END TITLE PAGE ----------------------------------

\begin{document}

\pagenumbering{gobble}% Remove page numbers (and reset to 1)

\maketitle
\begin{abstract}
Causal discovery from observational data is a very challenging, often impossible, task. However, estimating the causal structure is possible under certain assumptions on the data-generating process. Many commonly used methods rely on the additivity of the noise in the structural equation models. Additivity implies that the variance or the tail of the effect, given the causes, is invariant; the cause only affects the mean. In many applications, it is desirable to model the tail or other characteristics of the random variable since they can provide different information about the causal structure. However, 
models for causal inference in such cases have received only very little attention.

It has been shown that the causal graph is identifiable under different models, such as linear non-Gaussian, post-nonlinear, or quadratic variance functional models. We introduce a new class of models called the Conditional Parametric Causal Models (CPCM), where the cause affects the effect in some of the characteristics of interest.
%(often justified by some limit theorem). For example, assuming a cause-effect relation $X_2 = \mu(X_1) + \sigma(X_1)\cdot \varepsilon_2,$ for Gaussian $\varepsilon_2$ and unknown functions $\mu, \sigma$; or $X_2\mid X_1\sim Pareto(\theta(X_1))$ for some function $\theta$. 
We use the concept of sufficient statistics to show the identifiability of the CPCM models, focusing mostly on the exponential family of conditional distributions.
We also propose an algorithm for estimating the causal structure from a random sample under CPCM. Its empirical properties are studied for various data sets, including an application on the expenditure behavior of residents of the Philippines. 
\end{abstract}
%TC:ignore
\keywords{Causal discovery, Structural causal models, Identifiability, Causality from variance and tail, Exponential family}
%TC:endignore
  
\newpage
%\listoffigures
%\newpage
%\listoftables
%\newpage
%\listofalgorithms % List of algorithms in pseudocode format
%\newpage
%\listoflistings % List of algorithms in code format
%\newpage


\pagenumbering{arabic}% Arabic page numbers (and reset to 1)

\section{Introduction}

%Lemma jedna este raz poriadne pochop
Having knowledge about causal relationships rather than statistical associations enables us to predict the effects of actions that perturb the observed system \citep{TheBookOfWhy}. Determining causal structures is a fundamental problem in many scientific fields. However, several data-generating processes can produce the same observational distribution. Observing the system after interventions is a reliable way to find the causal structure, but in many real-life scenario, interventions can be too expensive, unethical \citep{epidem_application} or impossible to observe. Hence, estimating the causal structure from observational data has become an important topic.

In the last years, much effort has been put into developing mathematical background for a "language" of causal inference  \citep{Pearl_book}. The theory stands on the structural causal model (SCM) for random variables $\textbf{X} = (X_1, \dots, X_d)^\top\in\mathbb{R}^d$, with structural equations in the form $X_i=f_i(\textbf{X}_{pa_i}, \varepsilon_i)$, where $f_i\in\mathcal{A}$ are the causal functions belonging to some subclass of real functions $\mathcal{A}$, $pa_i$ are the direct causes of $X_i$, and $(\varepsilon_1, \dots, \varepsilon_d)^\top$ are independent noise variables. SCM is an important mathematical concept, and the usual goal of causal discovery is to estimate the causal structure (causal graph $\mathcal{G}$ associated with SCM) or the direct causes of some variable of interest. However, estimating the causal structure is impossible without strong assumptions on the class $\mathcal{A}$
\citep{Elements_of_Causal_Inference}. 

Many results appear in the literature, with various methods for causal inference under different assumptions on the SCM \citep{ZhangReview}.  If we observe several environments after different interventions, the assumptions can be much less restrictive \citep{Peters_invariance}.  If we aim to discover causal relations based only on an observed random sample, the assumptions are usually more strict; assuming linear structure or additive noise \citep{Peters2014}.

The additivity assumption ($X_i = f_i(\textbf{X}_{pa_i}) \textbf{+} \varepsilon_i$) implies that $\textbf{X}_{pa_i}$ affects only the mean of $X_i$. One well-known approach that allows more general effects of the covariates is PNL (post-nonlinear model \citep{Zhang2010}). This model potentially allows an effect of  $\textbf{X}_{pa_i}$ on the variance of $X_i$, but it does not allow arbitrary relations. QVF DAG models (quadratic variance functions \citep{ParkVariance}) consider the variance to be a quadratic function of the mean, but assume a fixed tail. 

In this paper, we develop a framework where $\textbf{X}_{pa_i}$ can arbitrarily affect the mean, variance, tail or other characteristics of $X_i$. However some cautions have to be taken as if $\textbf{X}_{pa_i}$ affects $X_i$ in a certain way, the causal structure will become unidentifiable (several causal structures can produce the same distribution of $\textbf{X}$). We expose which assumptions are reasonable in order to have an identifiable causal structure.

In some applications, for example, it may be reasonable to assume that $$X_i\mid \textbf{X}_{pa_i}\sim Pareto\big(\theta(\textbf{X}_{pa_i})\big),$$where $\theta$ is the function describing the tail behaviour. Another useful model that allows an arbitrary effect on the variance as well as on the mean is $X_i\mid \textbf{X}_{pa_i}\sim N\big(\mu(\textbf{X}_{pa_i}), \sigma^2(\textbf{X}_{pa_i})\big)$ for some functions $\mu, \sigma$, in which case the structural equation has the form 
\begin{equation}\label{prva_gaussian_equation}
X_i =  \mu(\textbf{X}_{pa_i})+ \sigma(\textbf{X}_{pa_i})\cdot \varepsilon_i, \,\,\,\,\,\,\,\,\,\,\varepsilon_i\text{ is Gaussian.}
\end{equation}
These assumptions are reasonable, especially when our variables arise from a limiting theorem. For example, the central limit theorem can justify (\ref{prva_gaussian_equation}) if $\textbf{X}$ is an appropriate sum of independent events. 

In Section \ref{Section2}, we introduce a causal model (we call it Conditionally Parametric Causal Model or CPCM) where the structural equation has the form 
\begin{equation}\label{11}
X_i=f_i(\textbf{X}_{pa_i}, \varepsilon_i) = F^{-1}(\varepsilon_i; \theta(\textbf{X}_{pa_i})), \,\,\,\,\,\text{ equivalently } X_i\mid \textbf{X}_{pa_i}\sim F\big(\theta(\textbf{X}_{pa_i})\big), 
\end{equation}
where $F$ is a known distribution function with a vector of parameters $\theta(\textbf{X}_{pa_i})$  that depends on the parents of $X_i$ and $\varepsilon_i\sim U(0,1)$. We restrict our focus mostly to the case where $F$ belongs to the exponential family of continuous distributions when sufficient statistics can be used. Section \ref{Section_identifiability} provides the identifiability results of the causal structure in the bivariate case. Assuming (\ref{11}), is it possible to infer the causal structure based only on observational data?
Section \ref{Section4} discusses the multivariate extension. 
%Specific example
In Section \ref{Section5}, we propose an algorithm for estimating the causal graph under assumption (\ref{11}). In Section \ref{simulations_section}, we proceed to a short simulation study and in Section \ref{Section7} we illustrate our methodology on a dataset concerning income and expenditure of residents of Philippines. 

We provide three appendices: Appendix \ref{SectionProofs} contains proofs of the theorems and lemmas presented in the paper. Appendix \ref{appendix} contains detailed definitions and explanations for some concepts that were omitted from the main part of the paper for the sake of clarity. Appendix \ref{Appendix_simulations} concerns the simulation study and the application. 


\subsection{Setup and notation}
\label{Setup}
In this section, we borrow the notations of \cite{PCalgorithm} and introduce the notation of graphical causal models.
A DAG $\mathcal{G}=(V,E)$ (directed acyclic graph) contains a finite set of vertices (nodes) $V$ and a set of directed edges $E$ between distinct vertices. We assume that there exists no directed cycle or multiple edges. 

Let $i,j\in V$ be two distinct nodes of $\mathcal{G}$. We say that $i$ is a parent of $j$ if there exists an edge from $i$ to $j$ in $E$, and we note $i\in  pa_{j}(\mathcal{G})$. Moreover, such $j$ is called a child of $i$, with notation $j\in ch_i(\mathcal{G})$. We say that $i$ is an ancestor of $j$, if there exists a directed path from $i$ to $j$ in $E$, notation $i\in an_{j}(\mathcal{G})$. In the other direction, we say that $j$ is a non-descendant of $i$, if $i\not\in an_{j}(\mathcal{G})$, notation $j\in nd_{i}(\mathcal{G})$. We say that the node $i\in V$ is a source node if $pa_j(\mathcal{G})=\emptyset$, notation $i\in Source(\mathcal{G})$.  We omit the argument $\mathcal{G}$ if evident from the context. 

We use capital $F$ for distributions and small $p$ for densities. Consider a random vector $\textbf{X }= (X_i)_{i\in V}$ over some probability space with distribution $F_\textbf{X}$. With 
a slight abuse of notation, we identify the vertices $j \in V$ with the variables $X_j$.  We denote $\textbf{X}_S = \{X_s: s\in S\}$ for $S\subseteq V$. We focus on the case in which $\textbf{X}$ has a continuous joint density function $p_\textbf{X}$ with respect to the Lebesgue measure. 

The distribution $F_\textbf{X}$ is Markov with respect to $\cal{G}$  if $A\indep_{\cal{G}} B\mid C \implies A\indep B\mid C$ for $A,B,C\subseteq V$ are disjoint subsets of the vertices and  $\indep_{\cal{G}}$ represent a d-separation in $\mathcal{G}$ \citep{Pearl}. On the other hand, if for all $A,B,C\subseteq V$ disjoint subsets of the vertices hold $A\indep_{\cal{G}} B\mid C \impliedby A\indep B\mid C$, we say that the distribution is faithful with respect to $\mathcal{G}$.   A distribution satisfies causal minimality with respect to $\mathcal{G}$ if it is Markov with respect to $\mathcal{G}$, but not to any proper subgraph of $\mathcal{G}$. It can be shown that faithfulness implies causal minimality \citep[Proposition 6.35]{Elements_of_Causal_Inference}. Two graphs $\mathcal{G}_1, \mathcal{G}_2$ are Markov equivalent if the set of distributions that are Markov with respect to $\mathcal{G}_1, \mathcal{G}_2$ is the same. We denote the Markov equivalency class of $\mathcal{G}$ as the set $\{\mathcal{G}':  \mathcal{G}\text{ and } \mathcal{G}' \text{ are Markov equivalent}\}$. 
A set of variables $\textbf{X}$ is said to be causally sufficient if there is no hidden common cause that causes more than one variable in $\textbf{X}$ \citep{Sprites2010}. The main concept in causal inference is the SCM \citep{Pearl}, defined as follows. 


%SCM
\begin{definition}[SCM]\label{def1}
The random vector $\textbf{X}=(X_1, \dots, X_d)$ follows the SCM with DAG $\cal{G}$  if for each $i\leq d$ the variable $X_i$ arises from the structural equation 
$$
S_i:   \,\,\,\, X_i=f_i\big(\textbf{X}_{pa_i(\mathcal{G})}, \varepsilon_i\big),
$$
where $\varepsilon = (\varepsilon_1, \dots, \varepsilon_d)$ is jointly independent and $f_i$ are some measurable functions.
\end{definition}

The SCM uniquely defines a distribution of $\textbf{X}$ that can be decomposed into a product of conditional densities or causal Markov kernels \citep[Definition 6.21]{Elements_of_Causal_Inference}
\begin{equation}\label{def987}
p_\textbf{X}(\textbf{x}) = \prod_{i\in V} p_i(x_i\mid \textbf{x}_{pa_i}),
\end{equation}
where $p_i(\cdot \mid \textbf{x}_{pa_i})$ represents the conditional density function of the random variable $X_i$ conditioned on its parents $\textbf{X}_{pa_j}=\textbf{x}_{pa_j}$. 
Note that every distribution that is Markov with respect to $\mathcal{G}$ can be decomposed to (\ref{def987}).

A core concept in causal inference is the identifiability of the causal structure. It is straightforward to compute $F_\textbf{X}$ if the DAG $\mathcal{G}$ and the Markov kernels (\ref{def987}) are given. However, we deal with the opposite problem, where $F_\textbf{X}$ is given (or a random sample from $F_\textbf{X}$), and we want to infer $\mathcal{G}$. 

Consider the SCM from Definition \ref{def1} with conditional densities satisfying (\ref{def987}). 
Let  $\mathcal{G}\in DAG(d)$ where $DAG(d)$ is the set of all DAGs over $V=\{1, \dots, d\}$ and assume that $p_i\in\mathcal{H}_i$, where $\mathcal{H}_i$ is the subset of all conditional density functions. Let $\mathcal{H} = \mathcal{H}_1\times \dots \times \mathcal{H}_d$. The given pair $(\mathcal{G}, p)\in DAG(d)\times \mathcal{H}$ generates the density $p_p(\textbf{x}) = \prod_{i\in V} p_i(x_i\mid \textbf{x}_{pa_i})$.  

\begin{definition}\label{IdentifiabilityPrvaDefinicia}
We say that the pair $(\mathcal{G}, p)\in DAG(d)\times \mathcal{H}$ is identifiable in $DAG(d)\times \mathcal{H}$ if there does \textit{not} exist a pair $(\mathcal{G}', p')\in DAG(d)\times \mathcal{H}$,  $\mathcal{G}'\neq \mathcal{G}$ such that $p_p= p_{p'}$. We say that the class $\mathcal{H}$ is identifiable over $DAG(d)$ if every  pair  $(\mathcal{G}, p)\in DAG(d)\times \mathcal{H}$ is identifiable in $\mathbb{G}_d\times \mathcal{H}$.  
\end{definition}

Without strong restrictions of the class $\mathcal{H}$, we cannot obtain the identifiability of $\mathcal{G}$. Generally, we can only identify the Markov equivalency class \citep[Proposition 7.1.]{Elements_of_Causal_Inference}. 
In Section 3, we rephrase the definition of identifiability from Definition \ref{IdentifiabilityPrvaDefinicia} for the model (\ref{11}).

%\begin{lemma}(Non-uniqueness of graph structures \label{NonUniqueness}
% Consider a random vector $\textbf{X} = (X_1, \dots, X_d)$ with distribution $P_\textbf{X}$ that has a density with respect to Lebesgue measure and assume it is Markov with respect to $\cal{G}$. Then there exists an SCM with graph $\cal{G}$ that entails the distribution $P_\textbf{X}$.
%\end{lemma}
\subsection{Related work}

Several papers address the problem of the identifiability of the causal structure (for a review see \cite{ZhangReview}). 
\cite{Lingam} show identifiability for the LiNGaM class (Linear Non-Gaussian additive Models $X_i=\beta\textbf{X}_{pa_i} +\varepsilon_i$ for non-Gaussian noise variables $\varepsilon_i$). 
\cite{BuhlmannCAM} explore CAM (causal additive models $X_i = \sum_{j\in pa_i} g_j(X_j) +  \varepsilon_i$ for some smooth functions $g_j$).  
\cite{nonlinearLINGAM} and \cite{Peters2014} develop a framework for ANM (additive noise models where $X_i = g(\textbf{X}_{pa_i}) +  \varepsilon_i$). Under some (not too restrictive) conditions on $g$, they show the identifiability of such model  \citep[Corollary 31]{Peters2014} and propose an algorithm estimating $\mathcal{G}$ (for a review on ANM, see \cite{reviewANMMooij}). 
All these frameworks assume that the variance of $X_i\mid \textbf{X}_{pa_i}$ does not depend on $\textbf{X}_{pa_i}$. This is a crucial part of the identifiability results.

\cite{Zhang2009} consider the PNL model (Post-Nonlinear Causal Model) 
\begin{equation*}
X_i = g_1\big(g_2(\textbf{X}_{pa_i}) +  \varepsilon_i\big)
\end{equation*}
with an invertible link function $g_1$.  Here, potentially $X_i = g_2(\textbf{X}_{pa_i})\cdot \varepsilon$ for the special choice $g_1(x)=\log(x)$.  The PNL causal model has quite a general form (the former two are its special cases). However, it is identifiable under some technical assumptions and with a few exceptions \citep{Zhang2010}. 

 \cite{ParkPoisson, ParkVariance} show identifiability in models where $var[X_i\mid \textbf{X}_{pa_i}]$  is a quadratic function of $\mathbb{E}[X_i\mid \textbf{X}_{pa_i}]$. If $X_i\mid \textbf{X}_{pa_i}$ has a Poisson or Binomial distribution, such a condition is satisfied.  They also provide an algorithm based on comparing dispersions for estimating DAG in polynomial time. Other algorithms have been proposed, with comparable speed and different assumptions on the conditional densities \citep{PolynomialTimeAlgorithmCausalGraphs}. \cite{Galanti} consider the neural SCM with representation $X_i = g_1\big(g_2(\textbf{X}_{pa_i}), \varepsilon_i\big),$ where $g_1, g_2$ are assumed to be neural networks. 

Several different algorithms for estimating causal graphs were proposed, working with different assumptions \citep{IGCI, Score-based_causal_learning, Slope, Natasa_Tagasovska}.  They are often based on Kolmogorov complexity or independence between certain functions in a deterministic scenario. We provide more details about some of these relevant methods in Section~5. 

Some authors assume that causal Markov kernels lie in a parametric family of distributions. \cite{JanzingSecondOrderExponentialModels} consider the case where the density of  $X_i\mid \textbf{X}_{pa_i}$  lies in a second-order exponential family, and the variables are a mixture of discrete and continuous random variables. \cite{ParkGHD} concentrate on a specific subclass of the model (\ref{11}), where $F$ lies in a discrete family of Generalized Hypergeometric Distributions, that is, the family of random variables where the mean and variance have a polynomial relation. To the best of our knowledge, there does not exist any work in the literature, providing identifiability results in the case in which $F$ lies in a general class of continuous exponential family. That is the focus of this paper. 
















































































































% This is how you can organize your document

\section{Definitions of causal models}
\label{Section2}

Recall that the general SCM corresponds to $d$ equations $X_i = f_{i}(\textbf{X}_{pa_i}, \varepsilon_i)$, $i=1, \dots, d$, where $(\varepsilon_1, \dots, \varepsilon_d)^\top$ are jointly independent noise variables. 
 In this section, we discuss appropriate assumptions on the conditional distribution  $F_{effect\mid causes}$, where the causes potentially affect the variance or the tail of the effect. First, we introduce the model in the bivariate case. 


\subsection{Bivariate case of conditionally parametric causal model}

In what follows, we consider that  $F_{effect\mid cause}$ belongs to a known parametric family of distributions, and the cause affects only the parameters of the distribution, not the form itself. 

In (unrestricted) SCM with the causal graph $X_1\to X_2$ and the structural equation $X_2=f_2(X_1, \varepsilon_2), \varepsilon_2\indep X_1$, we can, without loss of generality, assume that $\varepsilon_2$ is uniformly distributed. This notion follows the idea of generating random variables in computer software. As long as we do not restrict $f_2$, we can write  $f_{2}(X_1, \varepsilon_2) = f_{2}\big(X_1, g^{-1}(\varepsilon_U)\big)=\tilde{f_2} (X_1, \varepsilon_U) $, where $\varepsilon_U = g(\varepsilon_2)$ is uniformly distributed and $\tilde{f}_2$ is the new structural equation. Inferring the distribution of $\varepsilon_2$ and the function $g$ is an equivalent task. Therefore, we will assume  $\varepsilon_2\sim U(0,1)$. 

 The following definition describes that, if $X_1\to X_2$ then $X_2\mid X_1$ has the conditional distribution $F$ with parameters $\theta(X_1)\in\mathbb{R}^q$ for some $q\in\mathbb{N}$.  

\begin{definition}
We define the bivariate \textbf{conditionally parametric causal model} (bivariate $CPCM(F)$) with graph $X_1\to X_2$ by two assignments 
\begin{equation}\label{BCPCM}
X_1=\varepsilon_1, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,X_2=  F^{-1}\big(\varepsilon_2;\theta(X_1)\big),\tag{\ding{170}}
\end{equation}
where $\varepsilon_1\indep\varepsilon_2$ are noise variables, $\varepsilon_2$ is uniformly distributed and $F^{-1}$ is the quantile function with $q$ parameters $\theta(X_1)=\big(\theta_1(X_1), \dots, \theta_q(X_1)\big)^\top$. 

We assume that the $\theta_i$ are measurable non-constant \footnote{Hence, we assume causal minimality.} functions on the support of $X_1$.  If $\varepsilon_1$ is continuous, we additionally assume that $\theta$ is continuous on the support of $\varepsilon_1$. 
\end{definition}
We put no restrictions on the marginal distribution of the cause. 

An important special case is when $F$ is Gaussian. 

\begin{example}[Gaussian case]\label{Gaussian case}
Suppose that $(X_1,X_2)$ admits the model ( \ref{BCPCM}) with graph $X_1\to X_2$. Assuming $X_2\mid X_1\sim N\big(\mu(X_1), \sigma^2(X_1)\big)$ for some real functions $\mu, \sigma$ is equivalent to assume that $F$ is Gaussian distribution function in ( \ref{BCPCM}) and $\theta(X_1)=\big(\mu(X_1), \sigma(X_1)\big)^\top$. Equivalently, we write
$$X_2 = \mu(X_1)+\sigma(X_1)\cdot\varepsilon_2, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\varepsilon_2 \text{ is Gaussian.}$$
Assuming $\sigma(X_1)=const.$ brings us back to the Additive Noise Model framework.
\end{example}
The previous example can be understood as a model where the cause affects the mean \textit{and} variance. 
%Another important example comes from finance, where it is well known that many variables follow Pareto distribution. 

\begin{example}[Pareto case]\label{Pareto case}
Suppose that $(X_1,X_2)$ admits the model ( \ref{BCPCM}) with graph $X_1\to X_2$ with $F^{-1}$ being the Pareto quantile function \footnote{The density has the form $p_{X_2\mid X_1=x}(y)=\frac{\theta(x)}{y^{\theta(x)+1}}, \theta(x)>0, y\geq 1$.}. This model corresponds to $$X_2\mid X_1\sim Pareto\big(\theta(X_1)\big).$$ If $\theta(X_1)$ is small, then the tail of $X_2$ is large and extremes occur more frequently. Note that if $\theta(x)<k$  for $k\in\mathbb{N}$,  then the $k-$th moment of $X_2\mid X_1=x$ does not exist. 
\end{example}
Analogously to Example \ref{Gaussian case}, our Example \ref{Pareto case} can be understood as a baseline model for causality in the tail. 


\subsection{Asymmetrical $(\mathcal{M}_{1},\mathcal{M}_{2})$-causal models and $CPCM(F_1, F_2)$}
In the following, we define a class of models, where we put different assumptions on $X_1$ and on $X_2$ and generalize the model from (\ref{BCPCM}).

\subsubsection{Motivation}

In model-based approaches for causal discovery (approaches such as ANM or post-nonlinear models), we \textit{assume} some form of $F_{effect\mid cause}$.  Assume that $F_{effect\mid cause}\in\mathcal{F}$, where $\mathcal{F}$ is a subset of all conditional distributions.
For example, in CPCM, $\mathcal{F}$ is a known parametric family of conditional distributions. In ANM,  $\mathcal{F}$ consists of all conditional distributions that arise as the sum of a function of the cause and a noise. If $\mathcal{F}$ is small enough, we can hope for the identifiability of $\mathcal{G}$. In practice, we somehow measure which conditional distribution is "closer" to $\mathcal{F}$ and make our inference based on this comparison (see Section \ref{Section_Algorithm}).

In what follows, we discuss a different set of assumptions that is more general and can be asymmetrical for $X_1$ and for $X_2$. Instead of restricting $F_{effect\mid cause}\in\mathcal{F}$, we restrict either $F_{X_1\mid X_2}\in \mathcal{F}_1$, or $F_{X_2\mid X_1}\in\mathcal{F}_2$, where $\mathcal{F}_1, \mathcal{F}_2$ are (not necessarily equal) subsets of all conditional distributions (choosing different causal models). We motivate this by the following example. 

Suppose we observe data such as in Figure \ref{Asymetrical_picture}. Here, $X_2$ is non-negative, and $X_1$ has full support. Choosing an appropriate restriction of $F_{effect\mid cause}$ can be tricky, since 
$F_{effect\mid cause}$ needs to be non-negative if $X_1\to X_2$. On the other hand, $F_{effect\mid cause}$ needs to have full support in the case $X_2\to X_1$.  

Instead of restricting $F_{effect\mid cause}$, we divide our assumptions into two cases. If $X_1\to X_2$, then we assume $F_{X_2\mid X_1}\in\mathcal{F}_2$ ; if $X_2\to X_1$, then we assume $F_{X_1\mid X_2}\in\mathcal{F}_1$, where $\mathcal{F}_2$ consists of non-negative distributions and $\mathcal{F}_1$ of distributions with full support. 

Note that this is a generalization of classical model-based approaches since they make the implicit choice $\mathcal{F}_1 = \mathcal{F}_2$.  In the following, we create a framework allowing asymmetrical assumptions. 

\begin{figure}[ht]
\centering
\includegraphics[scale=0.4]{Asymetrical_picture.png}
\caption{A dataset generated as follows: $X_2\sim Pareto(3), X_1=X_2 + X_2\cdot\varepsilon_1$, where $\varepsilon_1\sim N(0, 1)$. That is, we have $X_2\to X_1$ and the model follows $CPCM(F)$ with Gaussian $F$.}
\label{Asymetrical_picture}
\end{figure}

\subsubsection{Definition}

Consider a class of bivariate SCMs, denoted by $\mathcal{M}$. For example, $\mathcal{M}$ can denote all SCM that follow ANM, or $\mathcal{M}$ can denote all SCM that follow (\ref{BCPCM}). 

\begin{definition}\label{Asymmetrical_causal_model}
Let $\mathcal{M}_1, \mathcal{M}_2$ be two classes of bivariate SCMs with non-empty causal graphs. We say that the pair of random variables $(X_1, X_2)$ follows the \textbf{asymmetrical} $(\mathcal{M}_1,\mathcal{M}_2)$\textbf{-causal model}, if one of the following holds: 
\begin{itemize}
\item the pair $(X_1, X_2)$ follows the SCM from $\mathcal{M}_2$ with the causal graph $X_1\to X_2$,
\item the pair $(X_1, X_2)$ follows the SCM from $\mathcal{M}_1$ with the causal graph $X_2\to X_1$.
\end{itemize}
\end{definition}
This definition allows different assumptions on each case of the causal structure. For example, if one variable is discrete and the second is continuous, we can assume ANM in one direction and the discrete QVF DAG model \citep{ParkPoisson} in the other. In the rest of the paper, we focus on the continuous CPCM case. 

Let $F_1, F_2$ be two continuous distribution functions with $q_1, q_2\in\mathbb{N}$ parameters, respectively. Let $\mathcal{M}_{F_1}, \mathcal{M}_{F_2}$ be two classes of SCMs that arise from the CPCM model (\ref{BCPCM}) with $F_1$ and $F_2$, respectively. We rephrase Definition \ref{Asymmetrical_causal_model} for this case.  
A pair of dependent random variables $(X_1, X_2)$ follows the asymmetrical $(\mathcal{M}_{F_1},\mathcal{M}_{F_2})$-causal model (we call it $CPCM(F_1, F_2)$ for short), if either
\begin{equation}\label{asymetrical_F_one_F_two_model}\tag{$\bigstar$}
\begin{split}
 & \,\,\,\,\,\,\,  X_1 = \varepsilon_1, X_2 = F_2^{-1}\big(\varepsilon_2; \theta_2(X_1)\big), \varepsilon_2\sim U(0,1), \varepsilon_1\indep \varepsilon_2,\\&
 \text{or }X_2 = \varepsilon_2, X_1 = F_1^{-1}\big(\varepsilon_1; \theta_1(X_2)\big), \varepsilon_1\sim U(0,1),\varepsilon_1\indep \varepsilon_2,
\end{split}
\end{equation}
where $\theta_1, \theta_2$ are some measurable non-constant continuous functions as in (\ref{BCPCM}). 
Note that the $CPCM(F)$ model is a special case of the asymmetrical $(\mathcal{M}_{1},\mathcal{M}_{2})$-causal model, where  $\mathcal{M}_{1}=\mathcal{M}_{2}=\mathcal{M}_{F}$.  







%Assymetrical CPCM(F1, F2) erase assymetrical
%T(\cdot) erase \cdot a daj pozor na T(x) v proposition 1 and later vymaz x



\section{Identifiability results}
\label{Section_identifiability}
In this section, we are interested in whether it is possible to infer a causal graph from a joint distribution under the assumptions presented in the previous section. First, we rephrase the notion of identifiability from Definition \ref{IdentifiabilityPrvaDefinicia}. 

\begin{definition}[Identifiability]\label{DEFidentifiability}
Let $F_{(X_1, X_2)}$ be a distribution that has been generated according to the asymmetrical ($\mathcal{M}_1, \mathcal{M}_2)$-causal model. We say that the causal graph is identifiable from the joint distribution (equivalently, that the model is identifiable, or that there does not exist a backward model) if exactly one of the bullet points from Definition~\ref{Asymmetrical_causal_model} can generate the distribution $F_{(X_1, X_2)}$.  

Specifically, let $F_{(X_1, X_2)}$ be a distribution that has been generated according to the $CPCM(F)$ model ( \ref{BCPCM}) with graph $X_1\to X_2$. We say that the causal graph is identifiable from the joint distribution, if there does \textit{not} exist 
$\tilde{\theta}$ and a pair of random variables $\tilde{\varepsilon}_2\indep\tilde{\varepsilon}_1$, where $\tilde{\varepsilon}_1$ is uniformly distributed, such that the model $X_2=\tilde{\varepsilon_2},  X_1=  F^{-1}\big(\tilde{\varepsilon}_1;\tilde{\theta}(X_2)\big)$ generates the same distribution $F_{(X_1,X_2)}$. 
\end{definition}

In the rest of the section, we answer the following question: Under what conditions is the causal graph identifiable? We start with the $CPCM(F)$ case. 
\subsection{Identifiability in $CPCM(F)$}

We consider only continuous random variables, but similar results can also be derived for discrete random variables. 

First, we deal with the important Gaussian case. Recall that \cite{hoyer2009} showed that in the additive Gaussian model where $X_2=f(X_1)+\varepsilon_2$, $\varepsilon_2\sim N(0, \sigma^2)$, the identifiability holds if and only if $f$ is non-linear. \cite{Gaussian_SCM_equal_variances_Peters_Buhlmann} explored the multivariate linear Gaussian case assuming equal variance of the noise variables.
\cite{ParkNormalCaseHighDimensional} generalized this result considering less strict assumptions. We provide a different result with both mean \textit{and} variance as functions of the cause. 

\begin{theorem}[Gaussian case]\label{normalidentifiability}
Let $(X_1,X_2)$ admit the model ( \ref{BCPCM}) with graph $X_1\to X_2$ and the Gaussian distribution function $F$ with parameters $\theta(X_1)=\big(\mu(X_1), \sigma(X_1)\big)^\top$.~\footnote{Such as in Example \ref{Gaussian case}, this can be rewritten as $X_2 = \mu(X_1)+\sigma(X_1)\varepsilon_2, \,\varepsilon_2 \text{ is Gaussian}$.}

Let $p_{\varepsilon_1}$ be the density of $\varepsilon_1$ that is absolutely continuous with full support $\mathbb{R}$. Let $\mu(x), \sigma(x)$ be two times differentiable.  Then, the causal graph is identifiable from the joint distribution if and only if there do not exist $a,c,d,e, \alpha, \beta\in\mathbb{R}$,  
$a\geq 0,c>0, \beta>0$, such that
\begin{equation}\label{norm}
\frac{1}{\sigma^2(x)}=ax^2 + c, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \frac{\mu(x)}{\sigma^2(x)}=d+ex,
\end{equation}
for all $x\in\mathbb{R}$ and
\begin{equation}\label{DensityDEF}
p_{\varepsilon_1}(x) \propto \sigma(x)e^{-\frac{1}{2}\big[ \frac{(x-\alpha)^2}{\beta^2}  - \frac{\mu^2(x)}{\sigma^2(x)}\big]},
\end{equation}
where $\propto$  means "is proportional to" (here, $p_{\varepsilon_1} $  is a valid density function if and only if $\frac{1}{\beta^2}\neq  \frac{e^2}{c}\mathbbm{1}[a=0]$). 
Specifically, if $\sigma(x)$ is constant (case $a=0$), then the causal graph is identifiable unless $\mu(x)$ is linear and $p_{\varepsilon_1}$ is the Gaussian density. 
\end{theorem}

The proof is provided in \hyperref[Proof of normalidentifiability]{Appendix} \ref{Proof of normalidentifiability}. Moreover, a distribution of an unidentifiable Gaussian case with $a=c=d=e=\alpha = \beta=1$ can be found in the   \hyperref[Proof of normalidentifiability]{Appendix} \ref{Proof of normalidentifiability}, Figure \ref{GaussianDensity}. 

Theorem \ref{normalidentifiability} shows that the non-identifiability holds only in the "special case", when $\frac{\mu(x)}{\sigma^2(x)}, \frac{-1}{2\sigma^2(x)}$ are linear and quadratic, respectively. Note that $\frac{\mu}{\sigma^2}, \frac{-1}{2\sigma^2}$ are natural parameters of a Gaussian distribution, and sufficient statistics in the Gaussian distribution have a linear and quadratic form (for the definition of the exponential family, natural parameter and sufficient statistic, see \hyperref[appendix]{Appendix} \ref{appendix_exponential_family}). We show that such connections between non-identifiability and sufficient statistics hold in the more general context of the exponential family.  

The following proposition shows a connection between sufficient statistics and the identifiability of the model  (\ref{BCPCM}).

\begin{proposition}[General case, one parameter]\label{Necessary condition for identifiability}
Let $q=1$. Let $(X_1, X_2)$ admit the model ( \ref{BCPCM}) with graph $X_1\to X_2$, where $F$ lies in the exponential family of distributions with a sufficient statistic $T$, where $T$ is continuous. 
If there do not exist $a,b\in\mathbb{R}$, such that
 \begin{equation}\label{eq000}
     \theta(x)=a\cdot T(x)+b,\,\,\,\,\,\,\,\,\,\,\,\,\, \forall x\in supp(X_1),
 \end{equation} 
then the causal graph is identifiable. 

Moreover,  the causal graph is identifiable, if there does not exist $c\in\mathbb{R}$, such that 
\begin{equation}\label{eq007}
p_{X_1}(x)\propto \frac{h_1(x)}{h_2[\theta(x)]}e^{cT(x)}, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \forall x\in supp(X_1),
\end{equation}
where $h_1$ is a base measure of $F$, and $h_2$ is the normalizing function of $F$ defined in \hyperref[appendix]{Appendix} \ref{appendix_exponential_family}. 
\end{proposition}
Note that the causal graph in (\ref{BCPCM}) is trivially identifiable if $X_1$ and $X_2$ have different supports.
\begin{proof}
We first show that if the causal graph is not identifiable, then (\ref{eq000}) holds for some $a,b\in\mathbb{R}$.

If the graph is not identifiable, there exists a function $ \tilde{\theta}$, such that 
causal models $X_1 = \varepsilon_1, X_2 = F^{-1}\big(\varepsilon_2; \theta(X_1)\big)$ and $X_2 = \varepsilon_2, X_1 = F^{-1}\big(\varepsilon_1; \tilde{\theta}(X_2)\big)$ generate the same joint distribution.
 Decompose the joint density
\begin{equation}\label{eq69}
  p_{(X_1, X_2)}(x,y) = p_{X_1}(x)p_{X_2\mid {X_1}}(y\mid x) = p_{X_2}(y)p_{{X_1}\mid {X_2}}(x\mid y),\,\,\,\,\,\,\,\, x,y\in supp(X_1).
 \end{equation}
Since $F$ lies in the exponential family of distributions, we use the notation from \hyperref[appendix]{Appendix} \ref{appendix_exponential_family} and rewrite $$p_{X_2\mid {X_1}}(y\mid x) = h_{1}(y)h_{2}[\theta(x)]\exp[\theta(x)T(y)]$$
and analogously for $p_{{X_1}\mid {X_2}}(x\mid y)$. Then, the second equality in (\ref{eq69}) reads as follows
\begin{equation}\label{eq098}
    \begin{split}
  &   p_{X_1}(x)h_{1}(y)h_{2}[\theta(x)]\exp[\theta(x)T(y)] =p_{X_2}(y) h_{1}(x)h_{2}[\tilde{\theta}(y)]\exp[\tilde{\theta}(y)T(x)],\\&
\underbrace{\log\bigg(  \frac{ p_{X_1}(x)h_{2}[\theta(x)]}{h_{1}(x)}\bigg)}_{f(x)}+ \underbrace{\log\bigg(\frac{ p_{X_2}(y)h_{2}[\tilde{\theta}(y)]}{h_{2}(y)}\bigg)}_{g(y)} = \tilde{\theta}(y)T(x)-\theta(x)T(y),
\end{split}
\end{equation}
where the second equation is the logarithmic transformation of the first equation after dividing both sides by $h_1$ and $h_2$. Since the left side is in the additive form, Lemma \ref{PomocnaLemma1} directly gives us (\ref{eq000}) (to see this without Lemma \ref{PomocnaLemma1}, apply $\frac{d}{dx dy}$  to (\ref{eq098}) and fix $y$ such that $T'(y)\neq 0$. We get 
$\theta'(x) = \frac{\tilde{\theta}'(y)}{T'(y)}T'(x)$. Integrating this equality with respect to $x$ gives (\ref{eq000}). However, using Lemma \ref{PomocnaLemma1}, we do not need to assume the differentiability of $\theta$).

As for equation (\ref{eq007}), the equality (\ref{eq098}) implies 
$f(x) + g(y) = a_1T(x) + a_2T(y) +a_3$ for some constants $a_, a_2, a_3\in\mathbb{R}$. Therefore, fixing $y$ gives $f(x)=\log\bigg\{  \frac{ p_{X_1}(x)h_{2}[\theta(x)]}{h_{1}(x)}\bigg\} = a_1T(x) + const$. Rewriting this gives 
$
p_{X_1}(x)= \frac{h_1(x)}{h_2[\theta(x)]}e^{a_1T(x) + const},
$
which is exactly the form of (\ref{eq007}). 
\end{proof}


Proposition \ref{Necessary condition for identifiability} shows that the only case in which the model is not identifiable is when $\theta(x)$ has a uniquely given functional form (unique up to a linear transformation), and the density of the cause is uniquely given (unique up to one additional parameter $c$).  Note that it is only a sufficient, not a necessary, condition for identifiability since some specific choices of $a,b,c$ can still lead to identifiable cases.  We show  the usage of Proposition \ref{Necessary condition for identifiability} for a Pareto model. 

\begin{consequence}\label{paretoidentifiability}
Let $(X_1,X_2)$ admit the model ( \ref{BCPCM}) with graph $X_1\to X_2$, where $F$ is the Pareto distribution function such as in Example \ref{Pareto case}. Then, the causal graph is \textit{not} identifiable if and only if 
\begin{equation}\label{eq50}
\theta(x) = a\log(x) +b,\,\,\,\,\,\,\,\, p_{X_1}(x) \propto \frac{1}{ [a\log(x)+b] x^{d+1} }, \,\,\,\,\,\,\,\,\forall x\geq 1,
\end{equation}
for some $a,b,d>0$. 
\end{consequence}
The proof is provided in \hyperref[Proof of pareto identifiability]{Appendix} \ref{Proof of pareto identifiability}. Note that if $a=0$, then $X_1\indep X_2$ and the (empty) graph is trivially identifiable. However, we assumed a non-constant $\theta$ , which implies $a\neq 0$.  If $\theta, p_{X_1}$ are such as in (\ref{eq50}), the backward model exists and has the form $\tilde{\theta}(y) = a\log(y)+d$ and $p_{X_2}(y)  \propto \frac{1}{ [a\log(y)+d] x^{b+1} }$, where $X_2\to X_1$. 



\subsection{Identifiability in $CPCM(F_1, F_2)$ models}


Similar sufficient conditions as in Proposition \ref{Necessary condition for identifiability} can be derived for a more general case, when $F$ has several parameters and for the $CPCM(F_1, F_2)$. 
The following theorem shows that the $CPCM(F_1, F_2)$ model is "typically" identifiable.  


\begin{theorem}
\label{thmAssymetricMultivariatesufficient}
Let $(X_1, X_2)$ follow the $CPCM(F_1, F_2)$ defined as in (\ref{asymetrical_F_one_F_two_model}), where $F_1, F_2$ lie in the exponential family of continuous distributions and $T_1 = (T_{1,1}, \dots, T_{1,q_1})^\top$, $T_2 = (T_{2,1}, \dots, T_{2,q_2})^\top$are the corresponding sufficient statistics with a nontrivial intersection of their support $\mathcal{S}:=supp(F_1)\cap supp(F_2)$.

The causal graph is identifiable, if $\theta_2$ is not a linear combination of $T_{1,1}, \dots, T_{1,q_1}$ on $\mathcal{S}$. That is, if $\theta_2$ can not be written as
\begin{equation}\label{eq158}
\theta_{2,i}(x) \overset{}{=} \sum_{j=1}^{q_1}a_{i,j}T_{1,j}(x)+b_i,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \forall x\in \mathcal{S},
\end{equation}
for all $i=1, \dots, q_2,$ and for some constants $a_{i,j},b_i\in\mathbb{R}$, $j=1, \dots, q_1$. 
\end{theorem}

The proof is provided in \hyperref[Proof of thmAssymetricMultivariatesufficient]{Appendix} \ref{Proof of thmAssymetricMultivariatesufficient}. Note that condition (\ref{eq158}) is sufficient, but not necessary for identifiability. Similarly as in the Gaussian case (Theorem \ref{Gaussian case}) or Pareto case (Consequence \ref{Pareto case}), in order to obtain a necessary condition, the distribution of the cause also needs to be restricted.

The following example illustrates the usage of Theorem \ref{thmAssymetricMultivariatesufficient} on Gamma and Beta distributions. 

\begin{consequence}\label{consequenceprva}
\begin{itemize}
\item Let  $(X_1,X_2)$  admit the $CPCM(F)$ model ( \ref{BCPCM}) with graph $X_1\to X_2$, where $F$ is a Gamma distribution with parameters $\theta=(\alpha, \beta)^\top$. If there do not exist constants $a,b,c,d,e,f\in\mathbb{R}$ such that 
\begin{equation*}
\alpha(x) = a\log(x) + bx + c, \,\,\,\,\,\beta(x) = d\log(x)+ex+f,\,\,\,\,\forall x>0,
\end{equation*}
then the causal graph is identifiable.
\item Let $(X_1,X_2)$ admit the $CPCM(F_1, F_2)$ as in (\ref{asymetrical_F_one_F_two_model}), where $F_1$ is the Gamma distribution function with parameters $\theta_1=(\alpha_1, \beta_1)^\top$ and $F_2$ is the Beta distribution function with parameters $\theta_2=(\alpha_2, \beta_2)^\top$. If there do not exist constants $a_i,b_i,c_i,d_i, e_i, f_i\in\mathbb{R}$, $i=1,2$, such that for all $x\in(0,1)$ holds \begin{equation*}\begin{split}
&\alpha_1(x) = a_1\log(x) + b_1x+c_1,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \beta_1(x) = d_1\log(x) +e_1x+f_1,\\&
\alpha_2(x) = a_2\log(x) + b_2\log(1-x)+c_2,\,\,\,\,\,\beta_2(x) = d_2\log(x) +e_2\log(1-x)+f_2,
         \end{split}
         \end{equation*}
then the causal graph is identifiable.
\end{itemize}
\end{consequence}
Details about the Consequence \ref{consequenceprva}, the definitions of the distributions, and their sufficient statistics can be found in \hyperref[consequence]{Appendix} \ref{consequence}. We stress that not all constants $a_i,b_i,c_i,d_i,e_i,f_i, i=1,2,$ give a valid (non-identifiable) model.










%Rozmysli si F_i being CONDITIONAL distirubiton functions
%Zmaz vsetky bold math symboly

\section{Multivariate case}
\label{Section4}

Now, we move the theory to the case with more than two variables $d\geq 2$. We assume causal sufficiency (all relevant variables have been observed) and causal minimality. On the other hand, we do not assume faithfulness. 
It is straightforward to generalize the asymmetric $\M$-causal model to the multivariate case, where we assume that each variable $X_i$ arises under the $\mathcal{M}_i$ model. 

To be more rigorous, we define a multivariate CPCM, as a generalization of (\ref{asymetrical_F_one_F_two_model}). 

\begin{definition}\label{DefinitionCPCM}
Let $F_1, \dots, F_d$ be distribution functions with $q_1, \dots, q_d$ parameters respectively. 
We define an asymmetrical $\mathcal{M}_{F_1}, \dots, \mathcal{M}_{F_d}$-causal model ($CPCM(F_1, \dots, F_d)$ for short) as a collection of equations 
\begin{equation*}
S_j:  X_j = F^{-1}_{j}(\varepsilon_j; \theta_j\big(\textbf{X}_{pa_j})\big), j=1, \dots, d,
\end{equation*}
 and we assume that the corresponding causal graph is acyclic. $(\varepsilon_1, \dots, \varepsilon_d)^\top$ is a collection of jointly independent uniformly distributed random variables. $\theta_j: \mathbb{R}^{|pa_j|}\to \mathbb{R}^{q_j}$ are non-constant functions in any of their arguments and are continuous if $\textbf{X}_{pa_j}$ are continuous. 

If $F_j^{-1} = F^{-1}$  for some quantile function $F^{-1}$ and for all $j\not\in Source(\mathcal{G}_0)$, we call such a model the conditionally parametric causal model ($CPCM(F)$).
\end{definition}
Simply said, we assume that $X_j\mid \textbf{X}_{pa_j}$ is distributed according to the distribution $F_j$ with parameters $\theta_j(\textbf{X}_{pa_j})$. We assume all $F_j$ \textit{are known} besides the source variables. 


The question of the identifiability of $\mathcal{G}$ in the multivariate case is in order. Here, it is not satisfactory to consider the identifiability of each pair of $X_i\to X_j$ separately. Each pair $X_i, X_j$  needs to have an identifiable causal relation \textit{conditioned} on other variables $\textbf{X}_S$, as the following theorem suggests. 


 \begin{theorem}\label{thmMultivairateIdentifiability}
Let $F_{\textbf{X}}$ be generated by the $CPCM(F_1, \dots, F_d)$ with DAG $\mathcal{G}$ and with density $p_{\textbf{X}}$. Let for all $ i,j\in\mathcal{G}, i\in pa_j$ hold the following: $\forall S\subseteq V$ such that  $pa_j\setminus \{i\}\subseteq S \subseteq nd_j\setminus\{i,j\}$ there exist $\textbf{x}_{S}: p_{\textbf{X}_S}(\textbf{x}_S)>0$ satisfying: a bivariate model defined as $X=\tilde{\varepsilon}_X, Y = F^{-1}_j\big(\tilde{\varepsilon}_Y, \tilde{\theta}(X)\big)$ is identifiable (in the sense of Definition \ref{DEFidentifiability}), where  $F_{\tilde{\varepsilon}_X} = F_{X_i\mid \textbf{X}_{S} =\textbf{ x}_S}    $ and $\tilde{\theta}(x) = \theta_j(\textbf{x}_{pa_j\setminus\{i\}}, x)$, where $x\in supp(X)$.

Then,  $\mathcal{G}$ is identifiable from the joint distribution. 
 \end{theorem}
 The proof is provided in \hyperref[Proof of thmMultivairateIdentifiability]{Appendix} \ref{Proof of thmMultivairateIdentifiability}. It is an adaptation of \citep[Theorem 28]{Peters2014}. 
 An important special case arises when we assume (conditional) normality.

\begin{consequence}[Multivariate Gaussian case]\label{ExampleMultivariateGaussiancase}
Suppose that $\textbf{X}=(X_1, \dots, X_d)$ follow $CPCM(F)$ with a Gaussian distribution function $F$. This corresponds to $X_j\mid \textbf{X}_{pa_j}\sim N\big(\mu_j(\textbf{X}_{pa_j}), \sigma_j^2(\textbf{X}_{pa_j})\big)$ for all $j=1, \dots, d$ and for some functions $\mu_j, \sigma_j$. In other words, we assume that the data-generating process has a form 
$$
X_j = \mu_j(\textbf{X}_{pa_j}) + \sigma_j(\textbf{X}_{pa_j})\varepsilon_j, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \varepsilon_j \text{  is Gaussian.}
$$
Potentially, source nodes can have arbitrary distributions. Combining Theorem~\ref{normalidentifiability} and Theorem~\ref{thmMultivairateIdentifiability}, the causal graph $\mathcal{G}$ is identifiable if the following holds: 
functions $\theta_j(\textbf{x}):=\big(\mu_j(\textbf{x}), \sigma_j(\textbf{x})\big)^\top, \textbf{x}\in\mathbb{R}^{|pa_j(\mathcal{G})|}$ , $j=1, \dots, d$, are two times differentiable and they are \textit{not} in the form (\ref{norm}) in any of their arguments. 
\end{consequence}

















\section{Methods for inference}
\label{Section5}

\subsection{Related work}
Many algorithms exist to estimate the causal graph $\mathcal{G}$ from observational data, each requiring different assumptions, guarantees, and outputs. See \cite{ZhangReview}, or Chapter~4 in \cite{Elements_of_Causal_Inference} for a review. In the following three paragraphs, we describe three main approaches in the recent literature. 

Identifiable model-based methods such as LiNGaM \citep{Lingam}, RESIT \citep{Peters2014}, PNL \citep{Zhang2010}, and GHD DAG \citep{ParkGHD,ParkVariance} exploit the fact that we can not fit a certain model in both causal directions. 
This is also the approach of this paper.
For practical evaluation, we need to measure how well the model fits the data. One possibility is to test the independence between the cause and the estimated noise. The second possibility is to use a maximum likelihood approach. In both cases, the choice of the model is important. 
%. However, the choice of model is crucial and should be based on some knowledge about the dataset at hand (or possibly data-driven). Different applications require different models, and a discussion about the choice of the model is as important as in classical statistics. 

Score-based methods \citep{Greedy_search, Score-based_causal_learning} are very popular for distinguishing between the DAGs in the Markov equivalence class. Denote the space of DAGs on $d$ nodes by $DAG(d)$ and let $s: DAG(d)\to \mathbb{R}$ be a score function. Function $s$ assigns each DAG a score that evaluates the overall fit. The goal is then to find  $\min_{\mathcal{G}\in DAG(d)}s(\mathcal{G})$ which is typically an NP-hard problem \citep{NP-hard_score_based_causal_learning}.  However, under appropriately chosen $s$ and additional assumptions, algorithms with polynomial time complexity were proposed \citep{Bregmans_information}. Nevertheless, the choice of $s$ is crucial since the graph which minimizes $s(\mathcal{G})$ can differ from the true data-generating mechanism. 
 
Other methods based on algorithmic mutual information \citep{IGCI,Natasa_Tagasovska} or Information-Geometric approaches \citep{IGCI} are popular, especially in computer science. They are based on a different paradigm of independence (based on Kolmogorov complexity or independence between certain functions in a deterministic scenario).  Often, they are implemented only in the bivariate case. In the simulations in Section \ref{Section_simulations_Gaussian}, we compare our method with several previously mentioned methods. 

\subsection{Algorithm for CPCM using independence testing}
\label{Section_Algorithm}
Our CPCM methodology is based on choosing an appropriate model (in our case, reduced to a choice of $F_1, F_2$) and a measure of a model fit. In what follows, we measure the model fit by exploiting the principle of independence between the cause and the mechanism. First, we provide a more general framework for the model-based methodologies. 

\subsubsection{Invertible functional causal models}

We explain the algorithm in a more general framework. Algorithms such as RESIT \citep{Peters2014}, CAM \citep{BuhlmannCAM} or algorithms based on ICA in the post-nonlinear models \citep{Zhang2009} are special cases or only small modifications of this framework. 

Recall that the general SCM corresponds to $d$ equations $X_i = f_{i}(\textbf{X}_{pa_i}, \varepsilon_i)$, $i=1, \dots, d$, where $\varepsilon=(\varepsilon_1, \dots, \varepsilon_d)^\top$ are jointly independent noise variables. We define the \textbf{invertible functional causal model} \textbf{(IFCM)} as an acyclic SCM, such that there exist functions $f_1^{\leftarrow}, \dots, f_d^{\leftarrow}$ for which $\varepsilon_i  =  f_i^{\leftarrow}(\textbf{X}_{pa_i}, X_i)$, $i=1, \dots, d$.  IFCM is a very general model since it is strictly broader than all previously mentioned models (e.g., the post-nonlinear model where $X_i=f_i(\textbf{X}_{pa_i}, \varepsilon) = g_2\big(g_1(\textbf{X}_{pa_i}) + \varepsilon\big)$ satisfy $\varepsilon = g_2^{-1}(X_i)-g_1(\textbf{X}_{pa_i})$, for $g_2$ invertible). 

\subsubsection{Main steps of the algorithm}

We say that a causal graph is \textbf{plausible} under the causal model $\mathcal{M}$ if the joint distribution \textit{can} be generated under model $\mathcal{M}$ with such a graph. The algorithm in Table \ref{tableDefinitions} describes the main steps to test the plausibility. If one direction is plausible and the other is not, we consider the former as a final estimate. Problems arise if both directions have the same plausibility. If both directions are plausible, it suggests an unidentifiable setup (or we have insufficient data). If both directions are unplausible, it suggests that some assumptions are not fulfilled or that our estimate $\hat{f}^{\leftarrow}$ (resp $\hat{\theta}$) is not appropriate. 
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[htbp]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{General IFCM}} &
  \multicolumn{1}{c|}{$\mathbf{CPCM(F_1, F_2)}$  } \\ \hline
Test for independence between $X_1, X_2$. &
  Test for independence between $X_1, X_2$. \\
If not rejected, return an empty graph. &
  If not rejected, return an empty graph. \\ \hline
In direction  $X_1\to X_2$: &
  In direction  $X_1\to X_2$: \\
\,\,\,\,\,\,\,\,\,\,1) Estimate function $\hat{f}^{\leftarrow}$ &
  \,\,\,\,\,\,\,\,\,\,1) Estimate $\hat{\theta}(X_1)$. \\
\,\,\,\,\,\,\,\,\,\,2) Compute $\hat{\varepsilon}_2 := \hat{f}^{\leftarrow}(X_1, X_2)$. &
  \,\,\,\,\,\,\,\,\,\,2) Use probability transform $\hat{\varepsilon}_2 := F_2\big(X_2; \hat{\theta}(X_1)\big)$ \\
\,\,\,\,\,\,\,\,\,\,3) Test an independence between $\hat{\varepsilon}_2$ and $X_1$. &
  \,\,\,\,\,\,\,\,\,\,3) Test an independence between $\hat{\varepsilon}_2$ and $X_1$. \\
 Direction $X_1\to X_2$ is \textit{plausible} if the test from step 3&
 Direction $X_1\to X_2$ is \textit{plausible}  if the test  from step 3\\
 is not rejected. &
 is not rejected. \\ \hline
Repeat for the other direction $X_2\to X_1$. &
  Repeat for the other direction $X_2\to X_1$ using $F_1$. \\ \hline
\end{tabular}%
}
\caption{Main steps of the algorithm for the estimation of the causal graph under IFCM and CPCM models in a bivariate case. }
\label{tableDefinitions}
\end{table}

Estimation of $\hat{f}^{\leftarrow}$ without additional assumptions is very difficult and has to be based on a more restricted model. Under CPCM, this reduces to an estimation of $\hat{\theta}(X_1)$ in the first step. This can be done using any machine learning algorithm, such as GAM, GAMLSS, random forest, or neural networks. For the third step (test of independence), we can use a Kernel-based HSIC test \citep{Kernel_based_tests} or a copula-based test \citep{copula_based_independence_test}. 

Generalizing this approach for the multivariate case with $d$ variables is straightforward. For each $\mathcal{G}\in DAG(d)$, we estimate  $\hat{\theta}_i(\textbf{X}_{pa_i(\mathcal{G})})$ for all $i = 1, \dots, d$ and compute the probability transform $\hat{\varepsilon}_i := F_i\big(X_i; \hat{\theta}_i(\textbf{X}_{pa_i(\mathcal{G})})\big)$. Then, we can test independence between  $\hat{\varepsilon}_1, \dots, \hat{\varepsilon}_d$ and say that $\mathcal{G}$ is plausible if this test is not rejected. However, in the multivariate case, several graphs can be plausible, which can be inconvenient in practice. The score-based algorithm can overcome this nuisance, as we shortly describe. 

\subsection{Score-based algorithm for CPCM }
\label{Section_score_based_algorithm}
The algorithm for CPCM using the independence testing presented in the previous subsection does not always provide an output. The possibility of rejecting the independence test in all directions can be considered a safety net, shielding us against unfulfilled assumptions or unidentifiable cases. Nevertheless, we may still want to obtain an estimation of the graph. 

Following the ideas from \citep{Score-based_causal_learning} and \citep{Peters2014}, we use the penalized independence score 
\begin{equation*}
\hat{\mathcal{G}} =  \min_{\mathcal{G}\in DAG(d)}s(\mathcal{G}) = \argmin_{\mathcal{G}\in DAG(d)}\rho (\hat{\varepsilon}_1, \dots, \hat{\varepsilon}_d) + \lambda (\text{Number of edges in }\mathcal{G}),
\end{equation*}
where $\rho$ represents some measure of independence, $\hat{\varepsilon}_1, \dots, \hat{\varepsilon}_d$ are noise estimations obtained by estimating $\hat{\theta}_i(\textbf{X}_{pa_i(\mathcal{G})})$ and putting $\hat{\varepsilon}_i := F_i\big(X_i; \hat{\theta}_i(\textbf{X}_{pa_i(\mathcal{G})})\big)$ such as in step 1 and step 2 in the algorithm in Section \ref{Section_Algorithm}. 

As for the choice of $\rho$, we use minus the logarithm of the p-value of the copula-based independence test \citep{copula_based_independence_test} and $\lambda = 2$. These choices seem to work well in practice, but we do not provide any theoretical justification of their optimality. In the bivariate case, we test the independence between $X_1, X_2$ and if rejected, we choose the graph with the lowest score $\rho (\hat{\varepsilon}_1, \hat{\varepsilon}_2)$. Doing so, we do not have to choose an appropriate $\lambda$.  

Another natural choice for the score function $s$ is based on log-likelihood  or AIC (or BIC) as in \cite{Score-based_causal_learning}. We do not comment on this further in this paper. 

The main disadvantage of the proposed method is that we have to go through all graphs $\mathcal{G}\in DAG(d)$, which is possible only for $d\lesssim 5$, since the number of DAGs grows superexponentially with dimension $d$ \citep{NP-hard_score_based_causal_learning}. Several modifications can be used in order to speed up the process. Greedy algorithms have been proposed \citep{Greedy_search, Bregmans_information}. This is beyond the scope of this paper. Our algorithm is relatively standard and is only a slight modification of classical algorithms. Hence, in Section \ref{simulations_section}, we provide only a short simulation study to highlight the results of Section 3. 


\subsection{Model choice, overfitting and problems in practice}
\label{Section5Model_choice}
The choice of the model (choice of $F$ or $F_1, F_2$ if we opt for our CPCM framework) is a crucial step in our approach. The choice of a model is a common problem in classical statistics; however, it is more subtle in causal discovery. 
Here, $F$ should be viewed as a metric for the "complexity" of SCM. We mention some of the problems that arise in practice. We also discuss them in detail in our application. 
\begin{itemize}
\item \textbf{Choosing $F$ with too many parameters ($q$ is large)}: Even if the theory suggests that it is not possible to fit a $CPCM(F)$ in both causal directions, this is only an asymptotic result no longer valid for a finite number of data. If we choose $F$ with many parameters to estimate, our data will be fitted perfectly, and we will not reject the wrong causal graph. However, by choosing an overly simple $F$, our assumptions do not have to be fulfilled, and we may reject the correct causal graph. 

\item \textbf{Several choices of $F$ and multiple testing}: The choice of $F$ should ideally be based on prior knowledge of the data-generating process. Looking at the marginal distributions of $X_1, X_2$ can be useful, although it is questionable how much marginal distributions can help choose an appropriate model for the conditional distribution.  Comparing many different $F$s can lead to multiple testing problems, and we should be careful with data-driven estimation of $F$, as the following lemma suggests:  \begin{lemma}\label{lemma_o_overparametrizacii}
Let $F_2$ be a distribution function with one parameter ($q_2=1$) belonging to the exponential family with the corresponding sufficient statistic $T_2$. 
Suppose that the joint distribution $F_{(X_1,X_2)}$ is generated according to model $CPCM(F_2)$ ( \ref{BCPCM}) with graph $X_1\to X_2$. 

Then, there exists $F_1$ such that the model $CPCM(F_1)$ ( \ref{BCPCM}) with graph $X_2\to X_1$ also generates  $F_{(X_1,X_2)}$. In other words, there exists $F_1$ such that the causal graph in $CPCM(F_1, F_2)$ is not identifiable from the joint distribution. 
\end{lemma}

The proof (provided in \hyperref[appendix]{Appendix} \ref{Proof of lemma_o_overparametrizacii}) is based on the specific choice of $F_{1}$, such that its sufficient statistic $T_1$ is equal to $\theta_2$ (where $\theta_2$ is the parameter from the original model $CPCM(F_{2})$).  Typically, such $F_{1}$ is "ugly" and would lead to a very non-standard distribution. Lemma \ref{lemma_o_overparametrizacii} indicates that a data-driven estimation of $F_{1}$ can be problematic.  

\item \textbf{Different complexity between models}: We recommend choosing the same $F$ in both directions (if reasonable), or at least trying to choose $F_{1}, F_{2}$ with the same number of parameters $q_1 = q_2$. If we choose, say, $F_{1}$ with one parameter and $F_{2}$ with five parameters ($q_1 = 1, q_2 = 5$), it will create a bias towards one direction because the model with more parameters will fit the data more easily. We refer to this as an "unfair game". 
\end{itemize}


In this paper, we opt for the following choices: we choose GAM \citep{Wood2} estimation of $\hat{\theta}(X_i)$. As for the independence test, we choose Hoeffding D-test \citep{Nonparametric_test_for_independence_review} in the bivariate case, copula-based independence test \citep{Kojadinovic2009} in the multivariate case with $n>1000$ and HSIC \citep{ZhangKernelTest} when $n\leq 1000$ (see a review of different tests and their comparisons \cite{copula_based_independence_test}). 















\section{Simulations} \label{simulations_section}
The R code with the implementations of the algorithms presented in the previous section and the code for the simulations and the application can be found in the supplementary package or \url{https://github.com/jurobodik/Causal_CPCM.git}.

In this section we illustrate our methodology under controlled conditions. We first consider the bivariate case where $X_1$ causes $X_2$. Our data-generating process will be a CPCM with an acyclic graph structure in each case.  
In our simulations, we choose different distribution functions $F$ in the CPCM model, different forms of $\theta$, and different distributions of $\varepsilon_1$. We recreate some of the theoretical results presented in Section 2. 


\subsection{Pareto case following Example \ref{Pareto case} and Consequence  \ref{paretoidentifiability} }

Consider the Pareto distribution function $F$, and functions $p_{\varepsilon_1}(x),\theta(x)$ defined similarly as in (\ref{eq50}). Specifically, we choose $p_{\varepsilon_1}(x)\propto \frac{1}{ [\log(x)+1] x^{2} }$ and $\theta(x) = x^\alpha log(x) +1$ for some hyper-parameter $\alpha\in\mathbb{R}$. This $\alpha$ represents the distortion from the unidentifiable case. If $\alpha = 0$, we are in the unidentifiable case described in Consequence \ref{paretoidentifiability}. If $\alpha> 0$, Consequence \ref{paretoidentifiability} suggests that we should be able to distinguish between the cause and the effect. If $\alpha<0$ then $\theta$ is almost constant (function $\frac{log(x)}{x^{-\alpha}}$ is close to zero function on $x\in [1, \infty)$) and $(X_1, X_2)$ are (close to) independent.

For the size of the dataset $n =300$ and  $\alpha \in \{  -2,   0,  2\}$, we simulate data as described above. Using our $CPCM(F)$ algorithm from Section \ref{Section_Algorithm}, we obtain an estimate of the causal graph.  After averaging results from 100 repetitions, we obtain the results described in Figure \ref{Pareto_simulations1}. The resulting numbers are as expected; if $\alpha = 0$, then both directions tend to be plausible. If $\alpha >0$, we tend to estimate the correct direction $X_1\to X_2$, and if $\alpha<0$, then we tend to estimate an empty graph since $X_1, X_2$ are (close to) independent.  


\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{1.pdf}
\caption{Simulations corresponding to the CPCM model with Pareto distribution function $F$. The results represent our estimations of the graph structure with the $CPCM(F)$ algorithm from Section \ref{Section_Algorithm}. The green line represents the case when both directions are plausible (we do not reject the independence test in both directions). The yellow line represents the case when both directions are unplausible (we reject the independence test in both directions. This case did not occur). }
\label{Pareto_simulations1}
\end{figure}



\subsection{Gaussian case and comparison with baseline methods}
\label{Section_simulations_Gaussian}
For simulated data, we use the same datasets as in \cite{Natasa_Tagasovska}. The dataset consists of additive and location-scale Gaussian pairs. We consider the location-scale Gaussian pairs (LS) of the form $X_2= \mu(X_1)+\sigma(X_1)\varepsilon_2$, where $\varepsilon_2\sim N(0, 1)$, $X_1\sim N(0, \sqrt{2})$. In one setup (LSg), we consider $\mu, \sigma$ as nonlinear functions simulated using Gaussian processes with Gaussian kernel with bandwidth one \citep{Gaussian_processes}.  In the second setup (LSs), we consider $\mu, \sigma$ as sigmoids \citep{BuhlmannCAM}. Nonlinear additive noise models (ANM) are generated as LS with $\sigma(X_1)=\sigma \sim U(1/5, \sqrt{2/5})$ and nonlinear multiplicative noise models (MN) are generated as LS with fixed $\mu(X_1)=0$ (only with sigmoid functions for $\sigma$).  For each of the five cases (LSg, LSs, ANMg, ANMs, MNs), we simulate 100 pairs with $n=1000$ datapoints.

We compare our method with RESIT \citep{Peters2014},  bQCD \citep{Natasa_Tagasovska}, IGCI with Gaussian reference measure \citep{IGCI}, and Slope \citep{Slope}. Details can be found in  Appendix \ref{Appendix_simulations}.  As in \cite{reviewANMMooij}, we use the accuracy for forced decisions as our evaluation metric. Results are in Table \ref{Table_Simulated_data_Gaussian}.  We conclude that our estimator performs well on all datasets, and provides comparable results with IGCI although choosing the uniform reference measure would lead to much worse results for IGCI. 

Notice that in the ANM and MN cases, we non-parametrically estimate two parameters while only one is relevant. Therefore, it can happen that we "overfit" (see Section \ref{Section5Model_choice}), and both directions are not rejected. Fixing $\mu$ or $\sigma$ would improve our results (although it is hard to know that in practical applications).

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
         & \multicolumn{1}{l|}{ANMg} & \multicolumn{1}{l|}{ANMs} & \multicolumn{1}{l|}{MNs} & \multicolumn{1}{l|}{LSg} & \multicolumn{1}{l|}{LSs}  \\ \hline
our CPCM & 100                        & 97                        & 96                     & 99                     & 99                                                    \\ \hline
RESIT    & 100                        & 100                        & 39                         & 51                       & 11                                                    \\ \hline
bQCD     & 100                        & 79                        & 99                     & 100                     & 98                                                  \\ \hline
IGCI (Gauss)     & 100                        & 99                        & 99                       & 97                     & 100                                                  \\ \hline
IGCI (Unif)     & 31                        & 35                        & 12                       & 36                     & 28                                                  \\ \hline
Slope    & 22                          & 25                          & 9                       & 12                       & 15                                                    \\ \hline
\end{tabular}
\caption{Accuracy of different estimators on simulated Gaussian datasets. ANM represents additive models, MN is multiplicative, and LS is location-scale models. The difference between ANMg, and ANMs (LSg, LSs) is how the functions $\mu, \sigma$ are generated. Analogous results (without the first row) can also be found in \cite{Natasa_Tagasovska}, Figure 6, with many other estimators from the literature. }
\label{Table_Simulated_data_Gaussian}
\end{table}





\subsection{Misspecified F}
%add alpha definiton
In the following, we want to know how robust our methodology is with respect to the choice of $F$. We consider $X_1\to X_2$, where $X_1\sim N(2,1)^+$, \footnote{$N(2,1)^+$ denotes the truncated Gaussian distribution on $\{x>0\}$. Therefore $X_1>0$ with mean around $2.07$)} and let
\begin{equation}\label{eq9870p}
    X_2\mid X_1\sim Exp\big(\alpha(X_1)\big),
\end{equation}
where $\alpha$ is a non-negative function. In other words, we generate $X_2$ according to (\ref{BCPCM}), with $F$ being an exponential distribution function. Recall that the exponential distribution is a special case of Gamma distribution with a fixed shape parameter. 

The goal of this simulation is to find out how the choice of $F$ affects the resulting estimate of the causal graph. We consider five different choices for $F$: Gamma with fixed scale, Gamma (with two parameters as in Consequence \ref{consequenceprva}), Pareto, Gaussian with fixed variance, and Gaussian (with two parameters as in Example \ref{Gaussian case}). 


We generate $n=500$ variables according to (\ref{eq9870p}) with different functions $\alpha$. Then, we apply the CPCM algorithm with different choices of $F$. Table \ref{table_simulations_about_misspecified_F} presents the percentage of correctly estimated causal graphs (an average out of 100 repetitions). 
We can see that the results remain more or less good for $F$ which are "similar" to the exponential distribution, with respect to the density and support. However, if we choose the Gaussian distribution (a uni-modal distribution with different support), our methodology often provides wrong estimates. 

\begin{table}[tbh]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
  F       & $\alpha(x) = x$ & $\alpha(x) = x^2+1$  & $\alpha(x) = \frac{e^x}{2}$ & Random $\alpha$ \\ \hline
Gamma (fixed scale)   & $93$           & $95$                                & $97$                       & $92$      \\ \hline 
Gamma (two parameters)  & $82$           & $86$                               & $76$                       & $92$           \\ \hline
Pareto    & $99$           & $100$             & $99$                                   & $97$           \\ \hline
Gaussian (fixed variance) & $0$           & $0$                               & $0$                       & $9$           \\ \hline
Gaussian (two parameters) & $16$           & $25$                           & $33$                       & $41$           \\ \hline
\end{tabular}
\caption{Comparison of the accuracy of CPCM estimations for different choices of $F$. Random $\alpha$ represents a function generated using Gaussian processes, similarly as in Simulations \ref{Section_simulations_Gaussian}.}
\label{table_simulations_about_misspecified_F}
\end{table}












\section{Application}
\label{Section7}

We explain our methodology in detail on real-world data describing the expenditure habits of Philippines residents. The Philippine Statistics Authority conducts a nationwide survey of Family Income and Expenditure \citep{psa_fies} every three years. 
The dataset (taken from \cite{FamilyIncomeExpenditure}) contains more than 40,000 observations primarily comprised of the household income and expenditures of each household. To reduce the size and add homogeneity to the data, we consider only families of size $1$ (people living alone) above a poverty line (top 90\%, with income at least $80,000\,\, pesos\approx 4000\,\,dollars $ per year). We end up with $n=1417$ observations. 

We focus on the following variables: Total income ($X_1$), Food expenditure ($X_2$), and Alcohol expenditure ($X_3$). We want to discover causal relations between these variables. Intuition gives us that $Income\to Food$. However, the relation between alcohol and other variables is not trivial. Drinking habits can impact income and food habits, although income can change the quality of the purchased alcohol. Histograms and pairwise plots of the variables can be found in Appendix~\ref{Appendix_simulations}. 

\subsection{Pairwise discovery}

First, we focus on a causal relation between $X_1$ and $X_2$ (between $Income$ and $Food$). We apply our $CPCM(F_1, F_2)$ methodology following the algorithm presented in Section \ref{Section_Algorithm}. As for the choice of $F_1, F_2$, we choose the \textit{Gamma} distribution for both  $F_1,F_2$ (model described in Consequence \ref{consequenceprva}).  We explain our choice: both marginals of $X_1, X_2$ are Gamma-shaped (see histograms in Figure  \ref{Just_histograms}) and the Gamma is a common choice for this type of data. One may argue that the Pareto distribution is more suitable since the tails of $X_1, X_2$ are heavy-tailed and $X_1, X_2$ are dependent in extremes. 
One may compare AIC scores in regression with the Gamma and Pareto distributions. AIC is equal to $31 308$ ($31449$)  for the Gamma (Pareto) distribution in the direction $X_1\to X_2$ and $36089$ ($37171$)  for the Gamma (Pareto) distribution in the direction $X_2\to X_1$. In this case, the Gamma distribution seems a better fit in both directions. However, choosing $F$ based on AIC is not appropriate; we must be careful when comparing several choices of $F$ (see the discussion in Section \ref{Section5Model_choice}). 

Applying the CPCM algorithm gives us an estimation $X_1\to X_2$. P-values of the independence tests were $0.2$ and $0.02$ corresponding to the directions $X_1\to X_2$ and $X_2\to X_1$, respectively.  Choosing the Pareto distribution function or even the Gaussian distribution function gives us similar results. 

Second, we focus on a causal relation between $X_1$ and $X_3$ (between $Income$ and $Alcohol$). As for the choice of $F_3$, the Pareto distribution seems more reasonable since it seems heavy-tailed with a large mass between 0 and 1000 pesos. However, choosing $F_1=Gamma$ and $F_3 = Pareto$ can potentially lead to an "unfair game" bias (see Section \ref{Section5Model_choice}).  

Applying the CPCM algorithm suggests that both directions are unplausible. P-values of the independence tests were $\approx 10^{-9}$ and $0.003$ corresponding to the directions $X_1\to X_3$ and $X_3\to X_1$, respectively. We note that for any pair of Gamma, Gaussian, or Pareto distribution functions, the p-values are always below $0.003$. This suggests that some assumptions are not fulfilled. In this case, we believe that causal sufficiency is violated; there is a strong unobserved common cause between these variables. 
Note that even if both causal graphs were unplausible, direction  $X_3\to X_1$ seemed to be the most probable direction ($0.003>10^{-9}$ for the choice of Gamma distribution). 

Finally, we focus on the causal relation between $X_2$ and $X_3$ (between $Food$ and $Alcohol$). Using the same principle as in the previous pairs, we obtain an estimation $X_3\to X_2$. P-values of the independence tests were $2\cdot10^{-9}$ and $0.46$ corresponding to the directions $X_2\to X_3$ and $X_3\to X_2$, respectively. We note that choosing the Pareto distribution function or the Gaussian distribution function gives us similar results. This result suggests that drinking habits affect food habits. 

\subsection{Multivariate score-based discovery and results}
We apply the score-based algorithm presented in Section \ref{Section_score_based_algorithm} with the Gamma distribution function $F$. The graph with the best score is the one shown in Figure \ref{Just_graph}. However, this graph is not plausible as the test of independence between $\hat{\varepsilon}_1,\hat{\varepsilon}_2, \hat{\varepsilon}_3$ gave a p-value $0.02$. The other two graphs with an added arrow from $alcohol\to income$ or $income\to alcohol$ had slightly worse scores and a p-value equal to $0.03$. 

\begin{figure}[t]
\centering
\includegraphics[scale=0.4]{only_graph.png}
\caption{ Final estimate of the causal graph in the application using a score-based algorithm and pairwise discovery. The relation between $Alcohol$  and $Income$ is inconclusive and differs with different choices of $F$ and used methods. The score-based algorithm with Gamma distribution $F$ suggests no arrow, with Pareto $F$ suggesting an arrow from $Income\to Alcohol$ and the bivariate algorithm suggesting an arrow from $Alcohol\to Income$}. 
\label{Just_graph}
\end{figure}



Possible feedback loops and common causes are the main reason why we do not obtain clear independence between the estimated noise variables. Deviations from the assumed model conditions are often observed in real-world scenarios. Despite this, acceptable estimates of the causal relationships can still be derived if the deviations are not excessive.




























\section{Conclusion and future work}


In this work, we introduced a new family of models for causal inference, called conditionally parametric causal models (CPCM). The main part of the theory consisted in exploring the identifiability of the causal structure under this model. We showed that bivariate CPCM models with distribution function $F$ are typically identifiable, except when the parameters of $F$ are in form of a linear combination of its sufficient statistics. We showed more detailed characterization of the identifiability in the one-parameter, the Gaussian and the Pareto cases.  We briefly explained the multivariate extensions of these results. 

We proposed an algorithm that estimates the causal graph based on the CPCM model. We discussed several possible extensions of the algorithm. A short simulation study suggests that the methodology is comparable with other commonly-used methods in the Gaussian case. However, our methodology is very flexible, varying with the choice of $F$. For specific choices of $F$, our methodology can be adapted for detection of causality-in-variance or causality-in-tail, and can be used to generalize additive models in various frameworks. We applied our methodology on real-world data and discussed some possible problems and results. 

Our methodology is not meant to be a black-box model for causal discovery. Discussing the choice of models and adapting them for different applications can bring a new perspective on the causal discovery, and future research is needed to show how useful our framework is in practice. 
On the other hand, finding an automatic, data-driven choice of $F$ can lead to new and interesting results. 

Our framework can also be useful for different causal inference tasks. For example, the invariant-prediction framework \citep{Peters_invariance} can be adapted for detecting invariance in a non-additive matter, such as invariance in tails or invariance in variance. This can lead to new directions of research with many possible applications. 



\section*{Conflict of interest and data availability}
R code and the data are available in an online \href{https://github.com/jurobodik/Causal\textunderscore CPCM.git}{repository}, or at a request to the author. 

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.


\section*{Acknowledgements}
The work was supported by the Swiss National Science Foundation. 


































































\appendix

\section{Appendix: Proofs}
\label{SectionProofs}

\subsection{}
\begin{customthm}{\ref{normalidentifiability}}
Let $(X_1,X_2)$ admit the model ( \ref{BCPCM}) with graph $X_1\to X_2$ and the Gaussian distribution function $F$ with parameters $\theta(X_1)=\big(\mu(X_1), \sigma(X_1)\big)^\top$. 
Let $p_{\varepsilon_1}$ be the density of $\varepsilon_1$ that is absolutely continuous with full support $\mathbb{R}$. Let $\mu(x), \sigma(x)$ be two times differentiable.  

Then, the causal graph is identifiable from the joint distribution if and only if  there do not exist $a,c ,d,e, \alpha, \beta\in\mathbb{R}$,  
$a\geq 0,c>0, \beta>0$, such that
\begin{equation}\tag{\ref{norm}}
\frac{1}{\sigma^2(x)}=ax^2 + c, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \frac{\mu(x)}{\sigma^2(x)}=d+ex,
\end{equation}
for all $x\in\mathbb{R}$ and
\begin{equation}\tag{\ref{DensityDEF}}
p_{\varepsilon_1}(x) \propto \sigma(x)e^{-\frac{1}{2}\big[ \frac{(x-\alpha)^2}{\beta^2}  - \frac{\mu^2(x)}{\sigma^2(x)}\big]},
\end{equation}
where $\propto$ means "is proportional to" (here, $p_{\varepsilon_1} $  is a valid density function if and only if $\frac{1}{\beta^2}\neq  \frac{e^2}{c}\mathbbm{1}[a=0]$). 
Specifically, if $\sigma(x)$ is constant (case $a=0$), then the causal graph is identifiable unless $\mu(x)$ is linear and $p_{\varepsilon_1}$ is the Gaussian density.
\end{customthm}



\begin{proof}
\label{Proof of normalidentifiability}{}
We opt for proving this theorem from scratch, without using Theorem \ref{thmAssymetricMultivariatesufficient}. The reader can try using Theorem \ref{thmAssymetricMultivariatesufficient}. For clarity in the indexes, we use the notation $X:=X_1, Y:=X_2$. 

First, we show that if the causal graph is not identifiable, then $\mu(x)$ and $\sigma(x)$ have to satisfy (\ref{norm}). Let $p_{(X,Y)}$ be the density function of $(X,Y)$. Since the causal graph is not identifiable, there exist two CPCM models generating $p_{(X,Y)}$; the CPCM model with  $X\to Y$  and a function  $\theta(x)=\big(\mu(x), \sigma^2(x)\big)^\top$ and the CPCM model with  $Y\to X$ and the function $\tilde{\theta}(y)=\big(\tilde{\mu}(y), \tilde{\sigma}^2(y)\big)^\top$. 

We decompose (corresponding to the direction $X\to Y$) 
\begin{equation*}
p_{(X,Y)}(x,y) = p_{X}(x)p_{Y\mid X}(y\mid x) = p_{X}(x) \phi\big(y;\theta(x)\big),
\end{equation*}
where $\phi\big(y;\theta(x)\big)$ is the Gaussian density function with parameters $\theta(x) = \big(\mu(x), \sigma^2(x)\big)^\top$. We rewrite the same in the other direction 
$$
p_{(X,Y)}(x,y) = p_{Y}(y)p_{X\mid Y}(x\mid y) = p_{Y}(y) \phi\big(x;\tilde{\theta}(y)\big).
$$
Take the logarithm of both equations and rewrite 
\begin{equation*}
\log[p_{X}(x)] +  \log\bigg\{\frac{1}{\sqrt{2\pi \sigma^2(x)}}e^{\frac{-[y-\mu(x)]^2}{2\sigma^2(x)}}\bigg\} = \log[p_{Y}(y)] +  \log(\frac{1}{\sqrt{2\pi \tilde{\sigma}^2(y)}}e^{\frac{-(x-\tilde{\mu}(y))^2}{2\tilde{\sigma}^2(y)}}),
\end{equation*}
\begin{equation}\label{eq1}
\log[p_{X}(x)] -\log\sigma(x)-\frac{1}{2}  \frac{[y-\mu(x)]^2}{\sigma^2(x)} = \log[p_{Y}(y)] -\log\tilde{\sigma}(y) -\frac{1}{2}  \frac{[x-\tilde{\mu}(y)]^2}{\tilde{\sigma}^2(y)}. 
\end{equation}
Calculating on both sides $\frac{\partial^4 }{\partial^2 x \partial^2 y }$, we obtain 
$$\frac{\sigma''(x)\sigma(x)-3\sigma'(x)'\sigma(x)}{\sigma^4(x)} =
\frac{\tilde{\sigma}''(y)\tilde{\sigma}(y)-3\tilde{\sigma}'(y)\tilde{\sigma}'(y)}{\tilde{\sigma}^4(y)}. $$Since this has to hold for all $x,y$, both sides need to be constant (let us denote this constant by $a\in\mathbb{R}$).  

Differential equation $\sigma''(x)\sigma(x)-3\sigma'(x)\sigma'(x)=a\cdot\sigma^4(x)$ has solution $\sigma(x) = \frac{1}{\sqrt{a(x+b)^2 + c}}$ for  $x$ such that $a(x+b)^2 + c>0$. 

Plugging this result into (\ref{eq1}) and calculating on both sides $\frac{\partial^3 }{\partial^2 x \partial y }$, we obtain 
\begin{equation}\label{eq2}
\mu''(x) (a(x+b)^2+c) + \mu'(x) (4ax+4ab) + \mu(x) 2a = 2ab.
\end{equation}
Equation (\ref{eq2}) is another differential equation with a solution $\mu(x) = \frac{d+ex}{a(x+b)^2+c} + b$, for some $d,e\in\mathbb{R}$ for all $x:\sigma(x)>0$. 

Now we show that necessary $b=0$. If we show $b=0$, then $\mu(x)$ and $\sigma^2(x)$ are exactly in the form (\ref{norm}). Plugging the previous representations into (\ref{eq1}), where we use $\tilde{a}, \dots, \tilde{e}$ for coefficients on the right side (potentially, they can be different), we obtain
\begin{equation*}
\begin{split}
&\log[p_{X}(x)] +\frac{1}{2}\log[a(x+b)^2 + c]\\&
-\frac{1}{2}  [y^2(ax^2 + 2abx + ab^2+c) + y(2d+2ex) + \frac{1}{a(x+b)^2 +c} ] \\&
= \log[p_{Y}(y)] +\frac{1}{2}\log[\tilde{a}(y+\tilde{b})^2 + \tilde{c}]\\& 
-\frac{1}{2}  [x^2(\tilde{a}y^2 + 2\tilde{a}\tilde{b}y + \tilde{a}\tilde{b}^2+\tilde{c}) + x(2\tilde{d}+2\tilde{e}y) + \frac{1}{\tilde{a}(y+\tilde{b})^2 +\tilde{c}} ] .
\end{split}
\end{equation*}
We can re-write the last expression as
\begin{equation}\label{eq4}
\begin{split}
&h_X(x) + h_Y(y) =\frac{1}{2}  [y^2(ax^2 + 2abx ) + 2yex ] -\frac{1}{2}  [x^2(\tilde{a}y^2 + 2\tilde{a}\tilde{b}y) + 2x\tilde{e}y) ]\\
&=\frac{1}{2}[ x^2y^2(a - \tilde{a})  + xy(2aby - 2\tilde{a}\tilde{b}x +e-\tilde{e}) ],
\end{split}
\end{equation}
where 
\begin{equation*}\label{eqh_x}
h_X(x) = \log[p_{X}(x)]  +  \frac{1}{2}\log[a(x+b)^2 + c] + \frac{1}{2}\frac{1}{a(x+b)^2 +c} -2x\tilde{d} - x^2(\tilde{a}\tilde{b}^2+\tilde{c}), 
\end{equation*}
\begin{equation*}
h_Y(y) = -\log[p_{Y}(y)] -\frac{1}{2}\log[\tilde{a}(y+\tilde{b})^2 + \tilde{c}] +\frac{1}{2}[\frac{1}{\tilde{a}(y+\tilde{b})^2 +\tilde{c}} - 2yd - y^2(ab^2+c)].
\end{equation*}
Since the left hand side of (\ref{eq4}) is in additive form, the right side also needs to have an additive representation. However, that is only possible if $a- \tilde{a}=0$ and $2aby - 2\tilde{a}\tilde{b}x +e-\tilde{e}=0$. Therefore, we necessarily have $a=\tilde{a}$ and either $a=0$ or $b=\tilde{b}=0$. The case $a=0$ corresponds to a constant $\sigma$, and hence also $b=\tilde{b}=0$. We have shown that $\mu(x)$ and $\sigma^2(x)$ have to satisfy (\ref{norm}).

Next, we will show that if the causal graph is not identifiable, then the density of $p_{X}(x)$ has form (\ref{DensityDEF}). Plugging the form of  $\mu(x)$ and $\sigma^2(x)$ into (\ref{eq1}), we get
\begin{equation*}
\begin{split}
\log[p_{X}(x)] -&\log\sigma(x)-\frac{1}{2}[y-  \frac{d+ex}{ax^2+c}    ]^2(ax^2+c) \\&= \log[p_{Y}(y)] -\log\tilde{\sigma}(y) -\frac{1}{2}  (x-  \frac{\tilde{d}+\tilde{e}y}{ay^2+\tilde{c}}   )^2(ay^2 + \tilde{c}).
\end{split}
\end{equation*}
We rewrite
\begin{equation}\label{rftgyh}
\begin{split}
\log[p_{X}(x)] -  &\log\sigma(x)+\frac{1}{2}( \tilde{c}x^2 - 2x\tilde{d} +  \frac{({d}+{e}x)^2}{ax^2+{c}}   ) \\&= \log[p_{Y}(y)] -\log\tilde{\sigma}(y) +\frac{1}{2}[cy^2 - 2yd + \frac{(\tilde{d}+\tilde{e}y)^2}{ay^2+\tilde{c}} ]
. 
\end{split}
\end{equation}
Since this has to hold for all  $x,y\in\mathbb{R}$, both sides of (\ref{rftgyh}) need to be constant and we get that $\log[p_{X}(x))]\propto\log\sigma(x)-\frac{1}{2}  [\tilde{c}x^2 - 2x\tilde{d} +  \frac{({d}+{e}x)^2}{ax^2+{c}}   ] $. Hence 
$$
p_{X}(x) \propto \sigma(x)e^{-\frac{1}{2}  \big[ \tilde{c}x^2 - 2x\tilde{d} +  \frac{({d}+{e}x)^2}{ax^2+{c}}   \big]} =  \sigma(x)e^{-\frac{1}{2}\big[ \frac{(x-\alpha)^2}{\beta^2}  - \frac{\mu^2(x)}{\sigma^2(x)}\big]},
$$
where $\beta = 1/\sqrt{\tilde{c}}$ and $\alpha:=\frac{\tilde{d}}{\tilde{c}}$. The condition $\frac{1}{\beta^2}\neq  \frac{e^2}{c}\mathbbm{1}[a=0]$ comes from the fact that if $a=0$ and$\frac{1}{\beta^2}=  \frac{e^2}{c}$ then $p_X(x)$ is not a density function (it is not integrable since  
$ \sigma(x)e^{-\frac{1}{2}\big[ \frac{(x-\alpha)^2}{\beta^2}  - \frac{\mu^2(x)}{\sigma^2(x)}\big]}= ce^{-\frac{1}{2}\big[ 0x^2 + 2x(\frac{ed}{c} - 2\tilde{d} ) + \frac{d^2}{c}  \big] }\propto e^{-x\cdot const}$ for all $x\in\mathbb{R}$). 

Finally, we deal with the other direction: we show that if $\mu, \sigma$ satisfy (\ref{norm}) and $p_{\varepsilon_X}$ has form (\ref{DensityDEF}), then the causal graph is not identifiable. Assume that $a,c,d,e$ are given. Define $\tilde{a} = a, \tilde{e}=e$ and choose $\tilde{c}, \tilde{d}\in\mathbb{R}$ such that $\tilde{c}>0,\tilde{c}\neq  \frac{e^2}{c}\mathbbm{1}[a=0]$. Define $\frac{1}{\tilde{\sigma}^2(y)} = \tilde{a} y^2 + \tilde{c}, \frac{\tilde{\mu}(y)}{\tilde{\sigma}^2(y)} = \tilde{d}+\tilde{e}y$. Moreover, define 
\begin{align*}
p_X(x) \propto \sigma(x)e^{-\frac{1}{2}\big[ \tilde{c}\big(x-\frac{\tilde{d}}{\tilde{c}}\big)^2  - \frac{\mu^2(x)}{\sigma^2(x)}\big]},\\
p_Y(y) \propto \tilde{\sigma}(y)e^{-\frac{1}{2}\big[c\big(x-\frac{{d}}{{c}}\big)^2  - \frac{\tilde{\mu}^2(y)}{\tilde{\sigma}^2(y)}\big]}.
\end{align*}
Note that regardless of the coefficients, these are valid density functions (with one exception when $\tilde{c}=\frac{e^2}{c}$ and $a=0$, which is why we chose $\tilde{c}\neq  \frac{e^2}{c}\mathbbm{1}[a=0]$). In case of $a=0$, then this is the classical Gaussian distribution density function.

Using these values, we obtain the equality
$$
p_X(x)p_{Y\mid X}(y\mid x) = p_Y(y)p_{X\mid Y}(x\mid y), \forall x,y\in\mathbb{R},
$$
or more precisely, 
$$
\sigma(x)e^{-\frac{1}{2}\big[ \tilde{c}\big(x-\frac{\tilde{d}}{\tilde{c}}\big)^2  - \frac{\mu^2(x)}{\sigma^2(x)}\big]}\frac{1}{\sqrt{2\pi}\sigma(x)}e^{-\frac{1}{2}\frac{[y-\mu(x)]^2}{\sigma^2(x)}} \propto \tilde{\sigma}(y)e^{-\frac{1}{2}\big[c\big(x-\frac{{d}}{{c}}\big)^2  - \frac{\tilde{\mu}^2(y)}{\tilde{\sigma}^2(y)}\big]}\frac{1}{\sqrt{2\pi}\tilde{\sigma}(y)}e^{-\frac{1}{2}\frac{(x-\tilde{\mu}(y))^2}{\tilde{\sigma}^2(y)}}. 
$$
Since this holds for all $x,y\in\mathbb{R}$, we found a valid backward model. The density in (\ref{DensityDEF}) uses the notation $\alpha:=\frac{\tilde{d}}{\tilde{c}}$ and $\beta = 1/\sqrt{\tilde{c}}$.  
\end{proof}

An example of the joint distribution of $X_1, X_2$ with $a=c=d=e=\alpha = \beta=1$ is shown in Figure \ref{GaussianDensity}. 
\begin{figure}[ht]
\centering
\includegraphics[scale=0.35]{Gaussian.pdf}
\caption{Random sample from a joint distribution of $(X_1,X_2)$ where  $X_1$ has the marginal density (\ref{DensityDEF}) and $X_2\mid X_1\sim N\big(\mu(X_1), \sigma^2(X_1)\big)$ with $\mu, \sigma$ defined in (\ref{norm}) with constants   $a=c=d=e=\alpha= \beta=1$. The distribution function is symmetric according to the $x=y$ axis (red line).    }
\label{GaussianDensity}
\end{figure}





\subsection{}


\begin{customconsequence}{\ref{paretoidentifiability}}
Let $(X_1,X_2)$ admit the model ( \ref{BCPCM}) with graph $X_1\to X_2$, where $F$ is the Pareto distribution function such as in Example \ref{Pareto case}. Then, the causal graph is \textit{not} identifiable if and only if   
\begin{equation}\tag{\ref{eq50}}
\theta(x) = a\log(x) +b,\,\,\,\,\,\,\,\, p_{X_1}(x) \propto \frac{1}{ [a\log(x)+b] x^{d+1} }, \,\,\,\,\,\,\,\,\forall x\geq 1,
\end{equation}
for some $a,b,d>0$.
\end{customconsequence}
\begin{proof}
\label{Proof of pareto identifiability}
For a clarity in the indexes, we use the notation $X:=X_1, Y:=X_2$ and we assume $X\to Y$. 

First, we show that if (\ref{eq50}) holds, then there exists a backward model generating the same joint distribution. The joint density can be written as 
\begin{equation*}
\begin{split}
p_{(X,Y)}(x,y) &= p_X(x)p_{Y\mid X}(y\mid x) \propto \frac{1}{[a\log(x) +b]x^{d+1}}\frac{a\log(x) +b}{y^{a\log(x) +b+1}} \\&= \frac{1}{x^{d+1}}\frac{1}{y^{b+1}}e^{-a\log(x)\log(y)},\,\,\,\,\,\,\,\,\,\forall x,y\geq 1.
\end{split}
\end{equation*}



For the backwards model, define $\tilde{\theta}(y) = \alpha\log(y)+\beta$ and $p_Y(y)  \propto \frac{1}{ [\alpha\log(y)+\beta] x^{\delta+1} }$, where $\alpha = a, \beta = d, \delta = b$. Then we can write
\begin{equation*}
\begin{split}
p_{(X,Y)}(x,y)& = p_Y(y)p_{X\mid Y}(x\mid y) \propto \frac{1}{[\alpha\log(y) +\beta]y^{\delta+1}}\frac{\alpha\log(y) +\beta}{x^{\alpha\log(y) +\beta+1}} \\&
=  \frac{1}{x^{\beta+1}}\frac{1}{y^{\delta+1}}e^{-\alpha\log(x)\log(y)},\,\,\,\,\,\,\,\,\,\forall x,y\geq 1.
\end{split}
\end{equation*}
 Therefore, for all $x,y\geq 1$ holds $p_{(X,Y)}(x,y) = p_X(x)p_{Y\mid X}(y\mid x) = p_Y(y)p_{X\mid Y}(x\mid y)$, where both $p_{Y\mid X}(y\mid x), p_{X\mid Y}(x\mid y)$ correspond to the valid model (\ref{BCPCM}), what we wanted to show. 



Second, we will show the other implication. Assume that the causal graph is not identifiable. Therefore, the joint distribution can be decomposed as
 \begin{equation}\label{eq54}
  p_{(X,Y)}(x,y) = p_X(x)p_{Y\mid X}(y\mid x) = p_Y(y)p_{X\mid Y}(x\mid y),
 \end{equation}
where for all $x,y\geq 1$ we have $p_{Y\mid X}(y\mid x) = \frac{\theta(x)}{y^{\theta(x)+1}}$ and  $p_{X\mid Y}(x\mid y) = \frac{\tilde{\theta}(y)}{x^{\tilde{\theta}(y)+1}}$, where $\theta, \tilde{\theta}$ are some non-negative functions and $p_X(x), p_Y(y)$ are some density functions defined on $[1,\infty)$. 
 
 A sufficient statistic for Pareto distribution is $log(x)$. Proposition~\ref{Necessary condition for identifiability} implies that the only form of $\theta, \tilde{\theta}$ can be a linear function of the logarithm. We write $\theta(x) = a\log(x) + b, \tilde{\theta}(y) = c\log(y) + d$, for some $a,b,c,d>0$. 
 
 Take the logarithm of~(\ref{eq54}) and, using the previous notation, we get
\begin{equation*}
 \begin{split}
 \log [p_X(x)] &+   \log\bigg[\frac{a\log(x) + b}{y^{a\log(x) + b+1}}\bigg]
 =  \log[p_Y(y)]+   \log\bigg[\frac{c\log(y) + d}{x^{c\log(y) + d+1}}\bigg].
 \end{split}
 \end{equation*}
By simple algebra we get
 \begin{equation}\label{eq55}
 \begin{split}
 &\log [p_X(x)]+ \log[a\log(x)+b] + (d+1)\log(x)   \\&= \log [p_Y(y)] + \log[c\log(y)+d] + (b+1)\log(y)
   + [(c-a)\log(x)\log(y)].
 \end{split}
 \end{equation}
 We show that the equality (\ref{eq55}) implies $c=a$ and the left side of (\ref{eq55}) is constant. Denote $h_1(x)$ the left side of (\ref{eq55}), and denote $h_2(y) = \log [p_Y(y)] + \log[c\log(y)+d] + (b+1)\log(y)$. Equality (\ref{eq55}) reads as
 \begin{equation}\label{eq1285}
     h_1(x)- h_2(y)= (c-a)\log(x)\log(y), \,\,\,\,\forall x,y\geq 1. 
 \end{equation}
  The only possibility that the right side of (\ref{eq1285}) is additive is when $c=a$, in which case $h_1(x) = h_2(y) = const.$
  Therefore
 $$
 \log\bigg\{ [a\log(x)+b] x^{d+1} p_X(x)\bigg\} = const.
 $$
In other words, $p_X(x) = \frac{const.}{ [a\log(x)+b] x^{d+1} }$, which is what we wanted to show.
\end{proof}


%%%%%%%%%%%%%%%%%%%%% Theorem 2 %%%%%%%%%%%%%%%%%%%%
\subsection{}
Before we prove Theorem \ref{thmAssymetricMultivariatesufficient}, we show the following auxiliary lemma. 
\begin{lemma}\label{PomocnaLemma1}
Let $n\in\mathbb{N}$ and let $\mathcal{S}\subseteq \mathbb{R}$ contain an open interval. Let $f_1, \dots, f_n, g_1, \dots, g_n$ be non-constant continuous real functions on $\mathcal{S}$, such that 
$
f_1(x)g_1(y) + \dots + f_n(x)g_n(y)
$ is additive in $x,y$, that is, there exist functions $f,g$ such that 
$$
f_1(x)g_1(y) + \dots + f_n(x)g_n(y) = f(x) + g(y), \forall x,y\in\mathcal{S}.
$$
Then, there exist (not all zero) constants $a_1, \dots, a_n, c\in\mathbb{R}$ such that 
$\sum_{i=1}^n a_if_i(x) = c$ for all $x\in\mathcal{S}$. Specifically for $n=2$, it holds $f_1(x) = af_2(x)+c$ for some $a,c\in\mathbb{R}$. 

Moreover, assume that for some $q<n$ holds that $g_1, \dots, g_q$ are affinly independent, that is, there exist $y_0, y_1, \dots, y_q\in\mathcal{S}$ such that a matrix 
\begin{equation}\label{matrix243}
M:=\begin{pmatrix}
 g_1(y_1) - g_1(y_0) & \cdots & g_q(y_1) - g_q(y_0) \\
\cdots & \cdots & \cdots \\
g_1(y_q) - g_1(y_0) & \cdots & g_q(y_{q}) - g_q(y_0)
\end{pmatrix} 
\end{equation}
has full rank. Then, for all $i=1, \dots, q$ there exist constants $a_{q+1}, \dots, a_n, c\in\mathbb{R}$ such that $f_i(x)=\sum_{j=q+1}^n a_jf_j(x) +c$ for all $x\in\mathcal{S}$. 
\end{lemma}
\begin{proof}
Fix $y_1, y_2\in\mathcal{S}$ such that $g_1(y_1)\neq g_1(y_2)$. Then, we have for all $x\in\mathcal{S}$
\begin{align*}
&f_1(x)g_1(y_1) + \dots + f_n(x)g_n(y_1) = f(x) + g(y_1),\\&
f_1(x)g_1(y_2) + \dots + f_n(x)g_n(y_2) = f(x) + g(y_2),
\end{align*}
and subtraction of these equalities gives us 
$$
f_1(x)[g_1(y_1)- g_1(y_2)] + \dots + f_n(x)[g_n(y_1)-g_n(y_2)] = g(y_1) - g(y_2).
$$
Defining $a_i = g_i(y_1)- g_i(y_2)$ and $c = g(y_1)- g(y_2)$ gives us the first result (with $a_1\neq 0$).

Now, we prove the "Moreover" part. Consider equalities 
\begin{align*}
&f_1(x)g_1(y_0) + \dots + f_n(x)g_n(y_0) = f(x) + g(y_0),\\&
f_1(x)g_1(y_1) + \dots + f_n(x)g_n(y_1) = f(x) + g(y_1),\\&
\dots\\&
f_1(x)g_1(y_q) + \dots + f_n(x)g_n(y_q) = f(x) + g(y_q),
\end{align*}
where $y_0, \dots, y_q$ are defined such that the matrix (\ref{matrix243}) has full rank. Subtracting from each equality the first equality gives us 
\begin{align*}
&f_1(x)[g_1(y_1)- g_1(y_0)] + \dots + f_n(x)[g_n(y_1)-g_n(y_0)] = g(y_1) - g(y_0)\\&
\dots \\&
f_1(x)[g_1(y_q)- g_1(y_0)] + \dots + f_n(x)[g_n(y_q)-g_n(y_0)] = g(y_q) - g(y_0).
\end{align*}
Using matrix formulation, this can be rewritten as 
\begin{equation}
M\begin{pmatrix}
f_1(x) \\
\cdots \\
f_q(x) 
\end{pmatrix} =
\begin{pmatrix}
g(y_1)-g(y_0) -\sum_{j=q+1}^n f_{j}(x)[g_{j}(y_1) -g_{j}(y_0)] \\
\cdots \\
g(y_q)-g(y_0) -\sum_{j=q+1}^n f_{j}(x)[g_{j}(y_q) -g_{j}(y_0)]
\end{pmatrix} .
\end{equation}
Multiplying both sides by $M^{-1}$ gives us that $f_i(x), i=1, \dots, q$ are nothing else than a linear combination of $f_{q+1}(x), \dots, f_n(x)$, which is what we wanted to show.
\end{proof}


\begin{customthm}{\ref{thmAssymetricMultivariatesufficient}}
Let $(X_1, X_2)$ follow the $CPCM(F_1, F_2)$ defined as in (\ref{asymetrical_F_one_F_two_model}), where $F_1, F_2$ lie in the exponential family of continuous distributions and $T_1 = (T_{1,1}, \dots, T_{1,q_1})^\top$, $T_2 = (T_{2,1}, \dots, T_{2,q_2})^\top$are the corresponding sufficient statistics with a nontrivial intersection of their support $\mathcal{S}:=supp(F_1)\cap supp(F_2)$.

The causal graph is identifiable, if $\theta_2$ is not a linear combination of $T_{1,1}, \dots, T_{1,q_1}$ on $\mathcal{S}$. That is, if $\theta_2$ can not be written as
\begin{equation}\tag{\ref{eq158}}
\theta_{2,i}(x) \overset{}{=} \sum_{j=1}^{q_1}a_{i,j}T_{1,j}(x)+b_i,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \forall x\in \mathcal{S},
\end{equation}
for all $i=1, \dots, q_2,$ and for some constants $a_{i,j},b_i\in\mathbb{R}$, $j=1, \dots, q_1$. 
\end{customthm}



\begin{proof}
\label{Proof of thmAssymetricMultivariatesufficient}{}
If the $CPCM(F_1,F_2)$ is \textit{not} identifiable, then there exist functions $\theta_1, \theta_2$, such that models $X_1 = \varepsilon_1, X_2 = F_2^{-1}(\varepsilon_2, \theta_2(X_1))$ and $X_2 = \varepsilon_2, X_1 = F_1^{-1}(\varepsilon_1, \theta_1(X_2))$ generate the same joint density function. Decompose the joint density as
\begin{equation}\label{eq59}
  p_{(X_1, X_2)}(x,y) = p_{X_1}(x)p_{X_2\mid {X_1}}(y\mid x) = p_{X_2}(y)p_{{X_1}\mid {X_2}}(x\mid y).
 \end{equation}
Since $F_1, F_2$ lie in the exponential family of distributions, we use the notation from \hyperref[appendix]{Appendix} \ref{appendix_exponential_family} and rewrite
\begin{equation*}
    \begin{split}
  &     p_{{X_2}\mid {X_1}}(y\mid x) = h_{1,1}(y)h_{1,2}[\theta_2(x)]\exp[\sum_{i=1}^{q_2}\theta_{2,i}(x)T_{2,i}(y)],\\&
  p_{{X_1}\mid {X_2}}(x\mid y) = h_{2,1}(x)h_{2,2}[{\theta_1}(y)]\exp[\sum_{i=1}^{q_1}{\theta}_{1,i}(y)T_{1,i}(x)].  
    \end{split}
\end{equation*}
After a logarithmic transformation of both sides of (\ref{eq59}), we obtain 
\begin{equation}\label{eq254}
\begin{split}
\log[p_{(X_1,X_2)}(x,y)] &= \log[p_{X_1}(x)] +  \log[h_{1,1}(y)]+\log\{h_{1,2}[\theta_{2}(x)]\} + \sum_{i=1}^{q_2}\theta_{2,i}(x)T_{2,i}(y) \\&
= \log[p_{X_2}(y)] +  \log[h_{2,1}(x)]+\log\{h_{2,2}[\theta_{1}(y)]\} + \sum_{i=1}^{q_1}\theta_{1,i}(y)T_{1,i}(x).
\end{split}
\end{equation}
Define $f(x) = \log[p_{X_1}(x)] +\log\{h_{1,2}[\theta_{2}(x)]\} -\log[h_{2,1}(x)]$ and $g(y) =\log[h_{1,1}(y)] -  \log[p_{X_2}(y)] + \log\{h_{2,2}[\theta_{1}(y)]\}$. Then, equality (\ref{eq254}) reads as 
\begin{equation*}\label{eq9876}
f(x) + g(y) = \sum_{i=1}^{q_1}\theta_{1,i}(y)T_{1,i}(x) - \sum_{i=1}^{q_2}T_{2,i}(y)\theta_{2,i}(x).
\end{equation*}
Finally, we use Lemma \ref{PomocnaLemma1}. We know that functions $T_{2,i}$ are affinly independent in the sense presented in Lemma  \ref{PomocnaLemma1}, see (\ref{eq145151}) in \hyperref[appendix]{Appendix} \ref{appendix_exponential_family}. Therefore, Lemma \ref{PomocnaLemma1} gives us that $\theta_{2,i}, i=1, \dots, q_2$ are only a linear combination of $T_{1, j}, j=1, \dots, q_1$, which is what we wanted to show. 
\end{proof}


\subsection{}\label{consequence}
\textbf{Details corresponding to Consequence \ref{consequenceprva}. }

The density function of the \textbf{ Gamma distribution} with parameters   
$\theta=(\alpha, \beta)^\top$ is in the form $p(x) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha -1}e^{-\beta x}, x>0$. The sufficient statistics are $[T_1(x), T_2(x)] = [\log(x), x]$. 

The density function of the \textbf{Beta distribution} with parameters   $\theta=(\alpha, \beta)^\top$ is in the form $p(x) = \frac{1}{B(\alpha, \beta)}x^{\alpha -1}(1-x)^{\beta -1}$. The sufficient statistics are $[T_1(x), T_2(x)] =[\log(x), \log(1-x)]$. 


Consequence \ref{consequenceprva} follows trivially from Theorem \ref{thmAssymetricMultivariatesufficient}. Theorem \ref{thmAssymetricMultivariatesufficient}  implies that if the causal graph is not identifiable, then $\alpha(x) = a_{1,1}T_1(x) + a_{1,2}T_2(x) + b_1$ and $\beta(x) = a_{2,1}T_1(x) + a_{2,2}T_2(x) + b_2$ for some constants. That are exactly the conditions in the consequence with different notation.




%%%%%%%%%%%%%%%%%%%%% Section 2.4%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{customthm}{\ref{thmMultivairateIdentifiability}}
Let $F_{\textbf{X}}$ be generated by the $CPCM(F_1, \dots, F_d)$ with DAG $\mathcal{G}$ and with density $p_X$. Let for all $ i,j\in\mathcal{G}, i\in pa_j$ hold the following: $\forall S\subseteq V$ such that  $pa_j\setminus \{i\}\subseteq S \subseteq nd_j\setminus\{i,j\}$ there exist $\textbf{x}_{S}: p_S(\textbf{x}_S)>0$ satisfying that a bivariate model defined as $X=\tilde{\varepsilon}_X, Y = F^{-1}_j\big(\tilde{\varepsilon}_Y, \tilde{\theta}(X)\big)$ is identifiable (in the sense of Definition \ref{DEFidentifiability}), where  $F_{\tilde{\varepsilon}_X} = F_{X_i\mid \textbf{X}_{S} =\textbf{ x}_S}    $ and $\tilde{\theta}(x) = \theta_j(\textbf{x}_{pa_j\setminus\{i\}}, x)$,  where $x\in supp(X)$.

Then,  $\mathcal{G}$ is identifiable from the joint distribution. 
 \end{customthm}
 
 \begin{proof}
 \label{Proof of thmMultivairateIdentifiability}
Let there be two $CPCM(F_1, \dots, F_d)$ models with causal graphs $\mathcal{G}\neq \mathcal{G}'$ that both generate $F_\textbf{X}$.   From \citep[Proposition 29]{Peters2014}, there exists variables $L,Y\in \{X_1, \dots, X_d\}$ such that 
\begin{itemize}
\item $Y\to L$ in $\mathcal{G}$ and $L\to Y$ in $\mathcal{G}'$,
\item $S:=\underbrace{\big\{pa_L(\mathcal{G})\setminus\{Y\}\big\}}_\text{\textbf{Q}}\cup\underbrace{\big\{pa_Y(\mathcal{G}')\setminus\{L\}\big\}}_\text{\textbf{R}}\subseteq \big\{nd_L(\mathcal{G}) \cap nd_Y(\mathcal{G}')\setminus\{Y,L\}\big\} $. 
\end{itemize}
For this $S$, choose $x_S$ according to the condition in the theorem. We use the notation $x_S=(x_q, x_r)$ where $q\in \textbf{Q}, r\in \textbf{R}$ and we define $Y^\star := Y\mid \{X_S=x_S\}$ and $L^\star := L\mid \{X_S=x_S\}$.  Now we use Lemma 36 and Lemma 37 from \citep{Peters2014}.  From $Y\to L$ in $\mathcal{G}$, we get $$Y^\star=\tilde{\varepsilon}_{Y^\star},\,\,\,\,\,\,\,\,\,\, L^\star = F^{-1}_{L}\big(\varepsilon_L; \theta_L(Y^\star)\big),$$ 
where $\tilde{\varepsilon}_{Y^\star} = Y\mid \{X_S=x_S\}$ and $\varepsilon_L\indep Y^\star$. We obtained an identifiable bivariate CPCM with $Y^\star\to L^\star$. However, the same holds for the other direction; From $L\to Y$ in $\mathcal{G}'$, we get $$L^\star=\tilde{\varepsilon}_{L^\star},\,\,\,\,\,\,\,\,\,\, Y^\star = F^{-1}_{Y}\big(\varepsilon_Y; \theta_Y(L^\star)\big),$$ 
 where $\tilde{\varepsilon}_{L^\star} =L\mid \{X_S=x_S\}$ and $\varepsilon_Y\indep L^\star$. We obtained an identifiable bivariate CPCM with $L^\star\to Y^\star$, which is a contradiction. Hence,  $\mathcal{G}= \mathcal{G}'$.  
 \end{proof}
 
\subsection{}
 \begin{customlem}{\ref{lemma_o_overparametrizacii}}
Let $F_2$ be a distribution function with one parameter ($q_2=1$) belonging to the exponential family with the corresponding sufficient statistic $T_2$. 
Suppose that the joint distribution $F_{(X_1,X_2)}$ is generated according to model $CPCM(F_2)$ ( \ref{BCPCM}) with graph $X_1\to X_2$. 

Then, there exists $F_1$ such that the model $CPCM(F_1)$ ( \ref{BCPCM}) with graph $X_2\to X_1$ also generates  $F_{(X_1,X_2)}$. In other words, there exists $F_1$ such that the causal graph in $CPCM(F_1, F_2)$ is not identifiable from the joint distribution. 
\end{customlem}
\begin{proof}\label{Proof of lemma_o_overparametrizacii}
\textit{Idea: we choose $F_1$ such that its sufficient statistic is equal to $\theta_2$. } Let us denote the original model as
\begin{equation*}
\begin{split}
 & \,\,\,\,\,\,\,  X_1 = \varepsilon_1, X_2 = F_2^{-1}\big(\varepsilon_2, \theta_2(X_1)\big), \varepsilon_2\sim U(0,1), \varepsilon_1\indep \varepsilon_2,
\end{split}
\end{equation*}
where (using notation from Appendix \ref{appendix_exponential_family}) the conditional density function can be rewritten as 
$$
p_{X_2\mid {X_1}}(y\mid x) =h_{2,1}(y)h_{2,2}[\theta_2(x)]\exp[\theta_{2}(x)T_{2}(y)].
$$
We define $F_1$ from an exponential family using the following: its sufficient statistic $T_1(x) = \theta_2(x)$ for all $x$ in support of $X_1$ and choose $h_{1,1}(x) =  p_{X_1}(x)h_{2,2}[\theta_2(x)]$ and $h_{1,2}(y) = \frac{h_{2,1}(y)}{p_{X_2}(y)}$ for all $y$ in support of $X_2$. Then, a model where 
\begin{equation*}
\begin{split}
 & \,\,\,\,\,\,\,  X_2 = \varepsilon_2, X_1 = F_1^{-1}\big(\varepsilon_1, \theta_1(X_2)\big), \varepsilon_1\sim U(0,1), \varepsilon_1\indep \varepsilon_2,
\end{split}
\end{equation*}
for a specific choice $\theta_1(y) = T_2(y)$ has a conditional density function 
$$
p_{X_1\mid {X_2}}(x\mid y) =h_{1,1}(x)h_{1,2}[\theta_1(y)]\exp[\theta_{1}(y)T_{1}(x)] = \frac{p_{X_1}(x)}{p_{X_2}(y)}h_{2,1}(y)h_{2,2}[\theta_2(x)]\exp[\theta_{2}(x)T_2(y)].
$$
Therefore, the joint distribution is equal in both models, since
\begin{equation*}
\begin{split}
   p_{X_1}(x) h_{2,1}(y)h_{2,2}[\theta_2(x)]\exp[\theta_{2}(x)T_{2}(y)] &=p_{X_2}(y) \frac{p_{X_1}(x)}{p_{X_2}(y)}h_{2,1}(y)h_{2,2}[\theta_2(x)]\exp[\theta_{2}(x)T_2(y)]\\
  p_{X_1}(x)p_{X_2\mid {X_1}}(y\mid x) &= p_{X_2}(y)p_{{X_1}\mid {X_2}}(x\mid y).
\end{split}
 \end{equation*}
We found $CPCM(F_1)$ model with graph $X_2\to X_1$ that generates the same distribution. 
\end{proof}


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
%\input{sections/Auxiliary Results}
\section{Theoretical details and detailed definitions}
\label{appendix}
\subsection{Exponential family}\label{appendix_exponential_family}

The exponential family is a set of probability distributions whose probability density function can be expressed in the form
\begin{equation}\label{Exponential family of distributions}
f(x;\theta) = h_1(x)h_2(\theta)\exp\big[\sum_{i=1}^q\theta_iT_i(x)\big],
\end{equation}
where $h_1, T_i$ are real functions and $h_2:\mathbb{R}^q\to\mathbb{R}^+$ is a vector-valued function. We call $T_i$ a \textit{sufficient} statistic, $h_1$ a base measure and $h_2$ a normalizing (or partition) function.  

Often, form (\ref{Exponential family of distributions}) is called a canonical form, and $$f(x;\theta) = h_1(x)h_2(\theta)\exp\big[\sum_{i=1}^qh_{3,i}(\theta)T_i(x)\big],$$ where  $h_{3,i}:\mathbb{R}^q\to\mathbb{R}, i=1, \dots, q$, is called its \textit{reparametrization} (natural parameters are a specific form of the reparametrization). We always work only with a canonical form (attention for Gaussian distribution, where standard form is not in the canonical form). 

Many important distributions lie in the exponential family of continuous distributions, such as Gaussian, Pareto (with fixed support), log-normal, Gamma or Beta distribution, to name a few. 

Functions in (\ref{Exponential family of distributions}) are \textit{not} uniquely defined. For example, $T_i$ are unique up to a linear transformation. 

The support of $f$ (defined as $supp(f) = \{x\in\mathbb{R}: f(x;\theta)>0\}$) is fixed and does not depend on $\theta$. Potentially, $T_i, h_1$ do not have to be defined outside of this support, however, we will typically neglect this fact (or possibly define $h_1(x) = T_i(x) = 0$ for $x$ where these functions are not defined). If we assume that $f$ is continuous (which is typically the case in this paper), we additionally assume that the support is non trivial in sense that it contains some open interval. 

Without loss of generality, we always assume that $q$ is minimal in sense that we can not write $f(x;\theta)$ using only $q-1$ parameters. Then, $T_1, \dots, T_q$ are linearly independent in the following sense: there exist $x_1, \dots, x_q\in supp(f)$ such that a matrix 
\begin{equation}\label{eq2431087}
\begin{pmatrix}
T_{1}(x_1) & \cdots & T_{{q}}(x_1) \\
\cdots & \cdots & \cdots \\
T_{1}(x_q) & \cdots & T_{{q}}(x_{q}) 
\end{pmatrix} 
\end{equation}
has full rank. Moreover, $T_1, \dots, T_q$ are affinly independent in the following sense: there exist $y_0, y_1, \dots, y_n\in supp(f)$ such that a matrix 
\begin{equation}\label{eq145151}
\begin{pmatrix}
T_{1}(y_1) - T_1(y_0) & \cdots & T_{{q}}(y_1) -T_q(y_0)\\
\cdots & \cdots & \cdots \\
T_{1}(y_q) -T_1(y_0) & \cdots & T_{{q}}(y_{q}) -T_q(y_0)
\end{pmatrix} 
\end{equation}
has full rank. In this paper (in particular in Lemma \ref{PomocnaLemma1}), we assume that  $T_1, \dots, T_q$ are affinly independent (satisfy (\ref{eq145151})). 

Since the notions of  linear and affine independence is nonstandard, consider the following example. Say that $T_1(x) = x, T_2(x) = x^2$ (sufficient statistics in Gaussian distribution). Then, matrices (\ref{eq2431087}) and (\ref{eq145151}) read as
\begin{equation*}
M_1=\begin{pmatrix}
x_1 &  x_1^2\\
x_2  & x_{2}^2
\end{pmatrix} , \,\,\,\,
M_2=\begin{pmatrix}
y_1-y_0 &  y_1^2 -y_0^2\\
y_2 -y_0 &  y_2^2 -y_0^2
\end{pmatrix} ,
\end{equation*}
which are trivially full ranked for a choice $(x_1, x_2) = (1,2)$ and $(y_0, y_1, y_2) = (0,1,2)$, for example. 






\section{Simulations and application details}
\label{Appendix_simulations}

Plots corresponding to the datasets from Simulations 2 and Simulations 3 are drawn in Figure \ref{Simulations2_plots} and Figure \ref{Simulations3_plot}. 
Plots corresponding to the application are drawn in Figure \ref{Just_plots} and Figure \ref{Just_histograms} . 

\subsection{Baselines implementations from Simulations 2}
As mentioned before, the experiments from Simulations 2 were inspired by \cite{Natasa_Tagasovska} and implementations of other baseline methods are also taken from \cite{Natasa_Tagasovska}.

For IGCI, we use the original implementation from \cite{IGCI} with slope-based estimation with Gaussian and uniform reference measures. For RESIT, we use the implementation of \cite{Peters2014} with GP regression and the HSIC independence test with a threshold value $0.05$. For the Slope algorithm, we use the implementation of \cite{Slope}, with the local regression included in the fitting process. If interested for comparisions with other methods such as PNL, GPI-MML, ANM, Sloppy, GR-AN, EMD see Section 3.2 in \cite{Natasa_Tagasovska}. 


\begin{figure}[ht]
\centering
\includegraphics[scale=0.65]{Simulation2_plots.pdf}
\caption{ An example of generated datasets from different generating models in Simulations 2.}
\label{Simulations2_plots}
\end{figure}



\begin{figure}[ht]
\centering
\includegraphics[scale=0.65]{Simulations3_Pareto.pdf}
\caption{Plots corresponding to Simulations 3. An example of a randomly generated function $\theta$ and a generated dataset where $effect\mid cause\sim Pareto(\theta(cause))$. Note that if $\theta(x)$ is small, then $effect\mid cause=x$ will have heavy tails. If $\theta(x)<1$, then the expectation of $effect\mid cause=x$ does not exists.}
\label{Simulations3_plot}
\end{figure}



\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{3ploty.pdf}
\caption{ Total income ($X_1$), Food expenditure ($X_2$) and Alcohol beverages expenditure ($X_3$).  }
\label{Just_plots}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{histograms.pdf}
\caption{ Total income ($X_1$), Food expenditure ($X_2$) and Alcohol expenditure ($X_3$).  }
\label{Just_histograms}
\end{figure}







%\input{sections/appendix}




%If you came here because you want your references in a new page, uncomment the following line

\clearpage % If you want the references in a separate page
\bibliography{bibliography}
%\input{sections/ToErase}

\end{document}
