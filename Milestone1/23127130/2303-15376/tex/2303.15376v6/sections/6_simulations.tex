
\section{Experiments} \label{simulations_section}
The \texttt{R} code for the presented algorithms, simulations, and application is available at \url{https://github.com/jurobodik/Causal_CPCM.git}.

\begin{itemize}
   \item In Section~\ref{Section_simulations_robustness}, we investigate the robustness of our approach against misspecifications of the choice of \(F\) (the data are generated with \(F\) being an exponential distribution but we use different choices for $F$ such as Gaussian or Pareto). 
   \item In Section~\ref{Section_simulations_Pareto}, we empirically validate Consequence~\ref{paretoidentifiability} and Proposition~\ref{consistency_proposition}, and show the distribution of the p-values (scores)  in Algorithm~\ref{Algorithm1} corresponding to the different causal graphs. 
    \item In Section~\ref{Section_simulations_Gaussian} and Section~\ref{Section_simulations_multivariate}, we compare our methodology with state-of-the-art methods using bivariate and multivariate benchmark datasets, respectively. 
    \item In Section~\ref{Section7} we consider a toy example on real-world data. 
    \item In Appendix~\ref{appendix_greedy_definitions}, we evaluate the performance of different greedy algorithms. This simulation study suggests that RESIT-greedy achieves better accuracy than standard greedy or RESIT methods, but at the cost of slightly higher computational complexity, making it a practical choice for moderately sized graphs ($d<10$). 
    \item In Appendix~\ref{Simulations_seq_approach}, we empirically evaluate the Sequential approach, compared to oracle choice of $F$. 
\end{itemize}

\textbf{Implementation details.}  We estimate ${\theta}(X_i)$ using Generalized Additive Models (GAM, \cite{Wood2}). We use the HSIC as an independence test and approximate the null distribution with a gamma distribution
in order to obtain p-values \citep{Kernel_based_tests}.  In Section~\ref{Section_simulations_Pareto}, we use the conservative estimate in Algorithm~\ref{Algorithm1}; elsewhere, for comparability with other methods, we use the Forced/Score-based estimate (recall that the forced estimate is a special case of the score-based estimate when $d=2$ and $\lambda=-\infty$). For the sequential approach algorithm, we apply the exact version when $d\leq 4$, the fast version when using RESIT, and, for greedy algorithms, we assess the plausibility of all candidate DAGs whose scores are evaluated.



\subsection{Robustness against a misspecification of F}
\label{Section_simulations_robustness}

We consider the structural model $X_1 \to X_2$, where $X_1 \sim N(2,1)^+$ (Gaussian truncated to $x > 0$) and
\begin{equation}
    \label{asfdae}
    X_2 \mid X_1 \sim \mathrm{Exponential}\big(\theta(X_1)\big),
\end{equation}
for some non-negative function $\theta$. This corresponds to model~(\ref{BCPCM}) with $F$ equal to the exponential distribution, a special case of the Gamma family with a fixed shape parameter.

The simulation evaluates how misspecifying $F$ affects causal graph estimation. We generate $n = 1000$ samples from~\eqref{asfdae}, apply our framework for several candidate families, and record the proportion of correctly identified directions (using Forced Algorithm~\ref{Algorithm1}), averaged over 100 repetitions. 

Results are shown in Table~\ref{table_simulations_about_misspecified_F}. The method is robust when $F$ resembles the exponential distribution in support and tail behavior, but accuracy drops sharply for e.g. $F = \text{Gaussian}$. Surprisingly, $F = \text{Gamma}$, the correctly specified family, performs relatively poorly due to over-parameterization: the reverse model $X_2 \to X_1$ can often fit well with two parameters. Using three-parameter families typically reduces accuracy to about $50\%$, no better than a coin toss, showing the importance of controlling model complexity in causal discovery.


\begin{table}[ht]
\centering
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{l c S S S S}
\toprule
\textbf{Family $F$} & {\#par} &
{$\theta(x) = \mathrm{random}$} &
{$\theta(x) = x$} &
{$\theta(x) = x^2 + 1$} &
{$\theta(x) = e^x / 2$} \\
\midrule
\rowcolor{RowAlt}
Exponential (oracle)                  & 1 &  0.99 &  0.98 & 0.99 &  1.00 \\
Gamma, fixed scale                    & 1 & 0.96 & 0.96 & 1.00 &  0.99 \\
\rowcolor{RowAlt}
Pareto, shifted support                                 & 1 & 0.99 &  0.99 &  1.00 & 1.00 \\
Gumbel, fixed scale                    & 1 & 0.36 & 0.00 & 0.01 & 0.00 \\
\rowcolor{RowAlt}
Gaussian, fixed $\sigma$               & 1 & 0.01 & 0.00 & 0.01 & 0.00 \\
\midrule
Gamma                                  & 2 & 0.96 & 0.73 & 0.64 & 0.79 \\
\rowcolor{RowAlt}
Gumbel                                 & 2 & 0.68 & 0.87 & 0.91 & 0.97 \\
Gaussian                               & 2 & 0.69 & 0.07 & 0.32 & 0.29 \\
\bottomrule
\end{tabular}
\caption{Accuracy of CPCM estimations for different distribution families $F$, with the Exponential distribution as the ground truth. “Random” $\theta(x)$ is generated via Gaussian processes as in Simulations~\ref{Section_simulations_Gaussian}.}
\label{table_simulations_about_misspecified_F}
\end{table}



























\subsection{Empirical validation of 
 Consequence~\ref{paretoidentifiability}, Proposition~\ref{consistency_proposition} and p-value distribution in Pareto model}
\label{Section_simulations_Pareto}

As in Consequence~\ref{paretoidentifiability}, we consider the CPCM Pareto model $X_1 \to X_2$ with
\begin{equation}\label{fwesef}
    p_{X_1}(x) \propto \frac{1}{[\log(x)+1] x^{2}}, \quad   
X_2 \mid X_1 \sim \mathrm{Pareto}\big(\theta(X_1)\big), \quad  
\theta(x) = x^\gamma \log(x) + 1,
\end{equation}
where $\gamma \in \mathbb{R}$ measures deviation from the unidentifiable case $(\gamma=0)$. For $\gamma > 0$, the causal graph should be identifiable; for $\gamma < 0$, $\theta$ is nearly constant and $X_1, X_2$ are almost independent.

Using $\gamma \in \{-2, -1, 0, 1, 2\}$, we apply the Conservative Algorithm~\ref{Algorithm1} with Pareto $F$ and sample size $n=500$. Note that Algorithm~\ref{Algorithm1} has five possible outcomes: 1) $X_1\indep X_2$, 2) $X_1\to X_2$, 3) $X_2\to X_1$, 4) ``unidentifiable setup'' (both directions appear to be plausible) and 5) ``Assumptions not fulfilled'' (neither direction appears to be plausible). 
 \begin{itemize}
     \item Table~\ref{Pareto_simulations1} in Appendix~\ref{Appendix_Section_simulations_Pareto} shows the results averaged over 100 repetitions. The results align with the theory: if \(\gamma = 0\), we typically estimate both directions to be plausible. If \(\gamma > 0\), we tend to estimate the correct direction \( X_1 \to X_2 \); if \(\gamma < 0\), we tend to estimate an empty graph since \( X_1 \) and \( X_2 \) are (nearly) independent.
     \item Figure~\ref{Pareto_histograms} in Appendix~\ref{Appendix_Section_simulations_Pareto} shows the distributions of the p-values from the independence test in Step 1b) of Algorithm~\ref{Algorithm1}, using data simulated with \(\gamma = 0\) and \(\gamma = 2\). The distribution of the p-values appears roughly uniform on $(0,1)$ in the \(\gamma = 0\) case and in the correct direction \( X \to Y \), while the p-values in the direction \( Y \to X \) are typically very close to $0$ in the identifiable setup  $\gamma=2$.
     \item Figure~\ref{sample_size_pareto} in Appendix~\ref{Appendix_Section_simulations_Pareto} shows an empirical validation of the consistency result from Proposition~\ref{consistency_proposition}. For a range of sample sizes \( n \), we generate the dataset using \(\gamma = 1\) and \(\gamma = 2\) as the hyperparameters, and we compute the percentage (out of 100 repetitions) of correctly estimated causal direction. The algorithm appears to achieve near-perfect performance for large sample sizes. 
 \end{itemize}







\subsection{Comparison with baseline methods: bivariate case}
\label{Section_simulations_Gaussian}
We compare our method with LOCI \citep{immer2022identifiability}, HECI  \citep{xu2022inferring}, RESIT \citep{Peters2014},  bQCD \citep{Natasa_Tagasovska}, IGCI with Gaussian and uniform reference measures \citep{IGCI} and Slope \citep{Slope}. Details can be found in  Appendix \ref{Appendix_simulations}.  As in \cite{reviewANMMooij}, we use the accuracy for forced decisions as our evaluation metric. 


We consider seven benchmark datasets, described below. The first five datasets, taken from \cite{Natasa_Tagasovska}, consist of additive and location-scale Gaussian pairs of the form $X_2 = \mu(X_1) + \sigma(X_1) \varepsilon_2$, where $\varepsilon_2 \sim \mathcal{N}(0, 1)$. In each case, we generate $X_1 \sim N(0, \sqrt{2})$. 

\begin{enumerate}
    \item \textbf{LSg (Location-Scale Gaussian)}: Here, $\mu$ and $\sigma$ are nonlinear functions simulated using Gaussian processes.
    \item \textbf{LSs (Location-Scale Sigmoid)}: In this setup, $\mu$ and $\sigma$ are sigmoid functions.
    \item \textbf{ANMg (Additive Noise Model)}: Nonlinear additive noise models generated similarly to LSg, but with constant $\sigma(X_1) = \sigma \sim U(1/5, \sqrt{2/5})$.
    \item \textbf{ANMs (Additive Noise Model)}: Nonlinear additive noise models generated similarly to LSs, but with constant $\sigma(X_1) = \sigma \sim U(1/5, \sqrt{2/5})$.
    \item \textbf{MNs (Multiplicative Noise)}: Nonlinear multiplicative noise models generated similarly to LSs, but with $\mu(X_1) = 0$.
\end{enumerate}

In addition to the models from \cite{Natasa_Tagasovska}, we consider two more setups:

\begin{enumerate}
    \setcounter{enumi}{5}
    \item \textbf{POISg (Poisson Model)}: $X_2 \sim \text{Pois}\big(\lambda(X_1)\big)$, where $\lambda$ is generated using Gaussian processes similar to $\sigma$ in LSg. Note that $X_2$ is discrete, creating error in some methods. 
    \item \textbf{PARg (Pareto Model)}: $X_2 \sim \text{Pareto}\big(\theta(X_1)\big)$, where $\theta$ is generated using Gaussian processes similar to $\sigma$ in LSg.
\end{enumerate}

For each of the seven setups, we simulate 100 pairs with $n=1000$ data points each. One realization of each model can be found in Figure \ref{Simulations2_plots} in Appendix~\ref{Appendix_simulations}. 

The results are presented in Table \ref{Table_Simulated_data_Gaussian}. We conclude that our estimator performs well across all considered datasets, effectively handling the mix of discrete and continuous variables. Under the Gaussian location-scale setups, it provides comparable results to LOCI and bQCD, which are specifically developed for such cases.
% --- Preamble additions (if not already present) ---
% \usepackage{booktabs}
% \usepackage{siunitx}
% \usepackage{xcolor,colortbl}
% \usepackage{makecell}
% \definecolor{RowAlt}{gray}{0.97}
% \sisetup{
%   table-number-alignment = center,
%   table-format=3.0,
%   detect-weight = true,
%   detect-family = true
% }

% \usepackage{booktabs}
% \usepackage{siunitx}
% \usepackage{xcolor,colortbl}
% \usepackage{makecell}
% \definecolor{RowAlt}{gray}{0.97}
% \sisetup{
%   table-number-alignment=center,
%   table-text-alignment=center,
%   table-format=3.0,
%   detect-weight=true,
%   detect-family=true
% }

\begin{table}[!ht]
\centering
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{l
                S[table-format=3.0]
                S[table-format=3.0]
                S[table-format=3.0]
                S[table-format=3.0]
                S[table-format=3.0]
                S[table-format=3.0]
                S[table-format=3.0]}
\toprule
\textbf{Method} &
\textbf{ANMg} & \textbf{ANMs} & \textbf{MNs} &
\textbf{LSg} & \textbf{LSs} & \textbf{POISg} & \textbf{PARg} \\
\midrule
\rowcolor{RowAlt}
\makecell[l]{\textbf{CPCM} \footnotesize(Seq. choice)\\ \footnotesize Forced Algorithm 1}
  & \bfseries 100 & \bfseries 99 & \bfseries 88 & \bfseries 98 & \bfseries 92 & \bfseries 94 & \bfseries 90 \\
\textbf{LOCI} \footnotesize (NN H)
  & \bfseries 100 & \bfseries 100 & \bfseries 99 & \bfseries 91 & \bfseries 85 & 79 & 0 \\
\rowcolor{RowAlt}
\textbf{HECI}
  & \bfseries 98 & \bfseries 43 & \bfseries 29 & \bfseries 96 & \bfseries 54 & \multicolumn{1}{c}{\textemdash} & 100 \\
\textbf{ANM-RESIT}
  & \bfseries 100 & \bfseries 100 & 39 & 51 & 11 & 0 & 12 \\
\rowcolor{RowAlt}
\textbf{bQCD} \footnotesize(m=3)
  & \bfseries 100 & \bfseries 79 & \bfseries 99 & \bfseries 100 & \bfseries 98 & 97 & 34 \\
\textbf{IGCI} \footnotesize(Gauss)
  & 100 & 99 & 99 & 97 & 100 & 0 & 0 \\
\rowcolor{RowAlt}
\textbf{IGCI} \footnotesize(Unif)
  & 31 & 35 & 12 & 36 & 28 & 0 & 100 \\
\textbf{Slope}
  & 22 & 25 & 9 & 12 & 15 & 0 & 100 \\
\bottomrule
\end{tabular}
\caption{Accuracy (\%) of different estimators on simulated Gaussian datasets. Similar results (excluding the first row and the last two columns) can also be found in \cite{immer2022identifiability}. Bold entries indicate cases where high accuracy is expected because the data-generating mechanism aligns with the method’s modeling assumptions.}
\label{Table_Simulated_data_Gaussian}
\end{table}












\subsection{Comparison with baseline methods: multivariate case}
\label{Section_simulations_multivariate}
We compare our proposed CPCM method with several widely used causal discovery algorithms implemented in \texttt{R}. These include LINGAM \citep{Lingam}, ANM (RESIT) \citep{Peters2014,hoyer2009}, PC (constraint-based method that tests for conditional independence assuming Gaussian noise, \cite{PCalgorithm}), and GES (score-based algorithm that greedily searches over equivalence classes of DAGs using the BIC score under Gaussian assumptions, \cite{Ramsey2016}). We also include a random baseline that selects a DAG uniformly at random from the space of all DAGs on $d$ nodes. Many other methods, such as NOTEARS \citep{zheng2018dags} or DAG-GNN \citep{yu2019dag}, would also be appropriate comparisons. However, we restrict our evaluation to methods directly available in \texttt{R} to ensure consistency and reproducibility within our implementation framework.

We generate data from 4 different scenarios:
\begin{itemize}
    \item \textbf{Linear (non-gaussian)}: $X_{i}=\sum_{k\in pa(i)}X_k + \varepsilon_k$, where $\varepsilon_k\overset{i.i.d}{\sim} Exp(1)$,
    \item \textbf{Nonlinear ANM}: $X_{i}\sim N(\mu(\textbf{X}_{pa_i}), 1)$ where $\mu(\textbf{X}_{pa_i}) = \sum_{k\in pa(i)}sin(X_k) + \frac{1}{2}X_k + \varepsilon_k$, where $\varepsilon_k\overset{i.i.d}{\sim} N(0,1)$
     \item \textbf{CPCM(Exp)}: $X_{i}\sim Exp(\lambda(\textbf{X}_{pa_i}))$, where $\lambda(\textbf{X}_{pa_i}) = \sum_{k\in pa(i)}|X_k|\vee 0.1$,
     \item \textbf{CPCM(Exp, Gauss)}: each node is, with probability $\frac{1}{2}$,  generated either according to the nonlinear ANM case or the $CPCM(Exp)$ case. 
\end{itemize}

The underlying DAG $\mathcal{G}$ is generated uniformly at random by sampling a random ordering of the $d$ variables and including each of the $\tfrac{d(d-1)}{2}$ admissible edges with probability $\tfrac{2}{d-1}$, resulting in an expected total of $d$ edges (following \cite{Peters2014}). We report results for $d=5$; results for $d=10$ are similar and omitted for brevity. For each of the four scenarios, we simulate 50 graphs with $n = 1000$ data points each. We compare the methods using the Structural Intervention Distance (SID, \cite{peters2014structuralinterventiondistancesid}). For fairness, undirected edges in the estimated graph are counted as correct if the true edge exists in either direction.

In the linear case, it performs nearly as well as LINGAM, which is tailored to linear non-Gaussian models. In the nonlinear ANM scenario, it is only slightly less accurate than ANM \citep{Peters2014}. The comparison between RESIT and its greedy variant highlights that in lower dimensions, RESIT-greedy tends to yield better accuracy. Most importantly, in the non-Gaussian $CPCM(F)$ settings, our method clearly outperforms all baselines, underscoring its robustness and suitability for non-additive and heterogeneous functional forms, where standard approaches often fail.




\begin{table}[ht]
\centering
\renewcommand{\arraystretch}{1.15}
\begin{tabular}{>{\raggedright\arraybackslash}p{5.7cm}|
                S[table-format=2.2]
                S[table-format=2.2]
                S[table-format=2.2]
                S[table-format=2.2]}
\toprule
\textbf{Method / Scenario} \footnotesize(Case $d=5$) &
\multicolumn{1}{c}{\makecell{\textbf{Linear}\\\textbf{model}}} &
\multicolumn{1}{c}{\makecell{\textbf{Nonlin.}\\\textbf{ANM}}} &
\multicolumn{1}{c}{\makecell{\boldmath$CPCM(F_1)$\\ \footnotesize$F_1=\mathrm{Exp}$}} &
\multicolumn{1}{c}{\makecell{\boldmath$CPCM(F_1,F_2)$\\ \footnotesize$F_1=\mathrm{Exp},\,F_2=\mathrm{Gauss}$}} \\
\midrule
\rowcolor{RowAlt}
\textbf{CPCM} \footnotesize(Seq. app., RESIT-greedy)                                  & 0.22 & 0.72 & \bfseries 2.16 & \bfseries 0.92 \\
\textbf{LINGAM}                                & \bfseries 0.12 & 4.72 & 4.52 & 5.51 \\
\rowcolor{RowAlt}
\textbf{ANM} \footnotesize(RESIT)              & 1.10 & 1.34 & 12.21 & 6.21 \\
\textbf{ANM} \footnotesize(RESIT-greedy) & \bfseries 0.12 & \bfseries 0.66 & 11.81 & 6.84 \\
\rowcolor{RowAlt}
\textbf{PC} \footnotesize(gaussCItest)         & 1.90 & 2.20 & 4.28 & 3.15 \\
\textbf{GES} \footnotesize(Gaussian score)     & 1.96 & 2.34 & 4.24 & 3.15 \\
\rowcolor{RowAlt}
\textbf{Random}                                & 7.20 & 7.58 & 7.50 & 7.57 \\
\bottomrule
\end{tabular}
\caption{Average SID over 100 repetitions between estimated and true graphs under different methods and scenarios. Bold values are the best (lowest) across methods.}
\label{tab:sid_summary}
\end{table}
