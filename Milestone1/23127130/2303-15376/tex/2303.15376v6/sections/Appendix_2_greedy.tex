\subsection{Exact, Naive-greedy, RESIT, and RESIT-greedy algorithms: definitions and comparison}
\label{appendix_greedy_definitions}
We consider the following algorithms for estimating the underlying causal graph from observational data using the CPCM score function~\eqref{score_definition1}:

\begin{itemize}
    \item \textbf{Exact search:} This algorithm evaluates the CPCM score for all DAGs on $d$ nodes and selects the one with the lowest score. Since the number of DAGs grows super-exponentially with $d$ (e.g., 29,281 DAGs for $d = 5$), exact search is computationally feasible only for very small graphs with $d \leq 4$.
    
    \item \textbf{Naive-edge-greedy:} Starting from an empty DAG, this algorithm iteratively explores neighboring DAGs by adding or removing a single edge. At each step, it selects the neighboring graph with the lowest CPCM score and replaces the current graph if the score improves. The procedure stops when no further improvement is possible. While simple and scalable, this greedy approach lacks theoretical guarantees and may get stuck in local minima, unless we assume some advanced notions of convexity over the space of all DAGs.

    \item \textbf{RESIT (Regression with Subsequent Independence Test):} RESIT first estimates a topological ordering by iteratively selecting the variable whose residual is least dependent on the remaining variables. In the second phase, it removes superfluous edges using conditional independence tests. See Algorithm~\ref{alg:resit} for details. The procedure is computationally efficient and comes with statistical guarantees (see Lemma~\ref{thm:resit_consistency}). However, empirical performance tends to be worse than that of greedy algorithms, particularly due to the accumulation of errors in the ordering phase and false positives (type I errors) in the Phase~2. 

    \item \textbf{RESIT-greedy:} This hybrid algorithm combines the topological ordering phase of RESIT with the edge-pruning phase of naive-greedy search. After estimating the ordering, it starts with a fully connected DAG consistent with the order and iteratively removes edges that lead to the largest improvement in the CPCM score, until no further improvement is possible. See Algorithm~\ref{alg:resit_greedy}.
\end{itemize}
\citet{Peters2014} showed that, in the population case, the RESIT algorithm is consistent under identifiable additive noise models, assuming a consistent nonparametric regression method and a perfect independence oracle. The same reasoning applies directly to CPCM models.


\begin{lemma}[Consistency of RESIT under CPCM]
\label{thm:resit_consistency}
Let $\mathbf{X}$ be generated by a $CPCM(F_1, \dots, F_k)$ model with underlying DAG $\mathcal{G}_0$. Then, the RESIT algorithm, when applied with consistent estimators $\hat{\theta}_k$ and an independence oracle, is guaranteed to recover the true graph $\mathcal{G}_0$ from the distribution of $\mathbf{X}$.
\end{lemma}

\begin{proof}
A direct consequence of Theorem~34 in \citet{Peters2014}. Note that we implicitly assume the causal minimality condition for $CPCM(F_1, \dots, F_k)$ as stated in Definition~\ref{DefinitionCPCM}.
\end{proof}

\begin{algorithm}[]
\caption{Regression with Subsequent Independence Test (RESIT)}
\label{alg:resit}
\KwIn{I.i.d. samples of a $d$-dimensional distribution on $(X_1, \ldots, X_d)$}
$S \gets \{1, \ldots, d\}; \pi \gets [\ ]$\;

\textbf{Phase 1: Determine topological order $\pi$} \;
\While{$S \ne \emptyset$}{
  \ForEach{$k \in S$}{
    Compute residuals $\hat{\varepsilon}_k := F\big(X_k; \hat{\theta}_k(\mathbf{X}_{S \setminus \{k\}})\big)$\;
    Measure dependence between $\hat{\varepsilon}_k$ and $\{X_i\}_{i \in S \setminus \{k\}}$\;
  }
  Let $k^* \gets$ variable with weakest dependence\;
  $S \gets S \setminus \{k^*\}$\;
  $\mathrm{pa}(k^*) \gets S$\;
  Prepend $k^*$ to $\pi$\;
}

\textbf{Phase 2: Remove superfluous edges} \;
\For{$k = 2$ \KwTo $d$}{
  \ForEach{$\ell \in \mathrm{pa}(\pi_k)$}{
    Compute residuals:
    $\hat{\varepsilon}_{\pi_k} := F\big(X_{\pi_k}; \hat{\theta}_{\pi_k}(\mathbf{X}_{\mathrm{pa}(\pi_k) \setminus \{\ell\}})\big)$\;
    \If{$\hat{\varepsilon}_{\pi_k}$ is independent of $\{X_{\pi_1}, \ldots, X_{\pi_{k-1}}\}$}{
      $\mathrm{pa}(\pi_k) \gets \mathrm{pa}(\pi_k) \setminus \{\ell\}$\;
    }
  }
}
\KwOut{$(\mathrm{pa}(1), \ldots, \mathrm{pa}(d))$}
\end{algorithm}





\begin{algorithm}[]
\caption{RESIT-greedy}
\label{alg:resit_greedy}
\KwIn{I.i.d. samples of a $d$-dimensional distribution on $(X_1, \ldots, X_d)$}

\textbf{Phase 1: Determine topological order $\pi$} \;
As in RESIT (Algorithm~\ref{alg:resit})\;

\textbf{Phase 2: Greedy removal of edges using CPCM score} \;
Initialize graph $\mathcal{G}$ with all edges $(j \to i)$ such that $j \in \mathrm{pa}(i)$ and $j$ precedes $i$ in $\pi$\;

\Repeat{\textbf{no score improvement}}{
  $\mathcal{G}_{\text{best}} \gets \mathcal{G}$\;
  $S_{\text{best}} \gets \text{CPCM}(\mathcal{G})$\;

  \ForEach{edge $e = (j \to i)$ in $\mathcal{G}$}{
    $\mathcal{G}' \gets \mathcal{G}$ with $e$ removed\;
    $S' \gets \text{Score of }\mathcal{G} \text{ in }\text{CPCM}$ \eqref{score_definition1}\;

    \If{$S' < S_{\text{best}}$}{
      $\mathcal{G}_{\text{best}} \gets \mathcal{G}'$\;
      $S_{\text{best}} \gets S'$\;
    }
  }

  \If{$\mathcal{G}_{\text{best}} \ne \mathcal{G}$}{
    $\mathcal{G} \gets \mathcal{G}_{\text{best}}$\;
  } \Else{
    \textbf{break}\;
  }
}
\KwOut{$\mathcal{G}$}
\end{algorithm}



\subsubsection{Experiments: comparison of different greedy algorithms}
\label{appendix_greedy}
\textbf{Data-generating process:} We generate  random DAGs uniformly over $d$ nodes with $p$ edges, where $p \sim \mathrm{Exp}(1/d)$ and capped at $\frac{d(d-1)}{2}$. On average, each DAG contains approximately $d$ edges. For a given DAG $\mathcal{G}$, we simulate data from the $CPCM(F)$ model, $F=Exponential$, defined as:
$$
X_i \sim \mathrm{Exp}\left(\lambda(\mathbf{X}_{pa_i(\mathcal{G})})\right), \quad \text{with} \quad \lambda(\mathbf{X}_{pa_i(\mathcal{G})}) = \frac{1}{\sum_{j \in pa_i(\mathcal{G})} |X_j|} = \frac{1}{\mathbb{E}[X_i\mid \textbf{X}_{pa_i}]}.
$$
If $pa_i(\mathcal{G})=\emptyset$, then $X_i\sim N(0,1)$. Using $n = 1000$ samples, we estimate $\mathcal{G}$ using the score-based CPCM estimator defined in Equation~\eqref{score_definition1}, with a fixed function class $\mathscr{S}_1$ (rather than the sequential approach) to allow for a fair comparison in both accuracy and computational cost. We compare the Exact, Naive-greedy, RESIT, and RESIT-greedy methods.  We evaluate their performance using the Structural Intervention Distance (SID, \cite{peters2014structuralinterventiondistancesid}), computing $\mathrm{SID}(\mathcal{G}, \hat{\mathcal{G}})$ for each estimated graph.

\textbf{Results:} Figure~\ref{figure_greedy} presents the average normalized $\frac{SID}{d}$ over 50 repetitions.

\begin{itemize}
    \item The exact method achieves, unsurprisingly, the lowest SID, with the highest computational cost. 
    \item Both RESIT and naive-greedy exhibit similar performance. The greedy approach achieves slightly lower SID on average but requires slightly more computation time. Note that for $d\geq 8$, both methods perform as badly as Trivial algorithm (empty graph).
    \item RESIT-greedy serves as a middle ground: it significantly improves over RESIT/naive-greedy methods in terms of SID while incurring higher computational cost for $d > 5$.
\end{itemize}


These experiments highlight the trade-offs between statistical performance and computational efficiency among the evaluated methods. While the exact method yields the most accurate graph recovery, its scalability is limited. In contrast, RESIT and naive-greedy offer faster but less accurate alternatives, with performance deteriorating as graph complexity increases. RESIT-greedy provides a promising compromise, achieving lower SID than standard greedy methods at a moderate computational cost. \textbf{Overall, RESIT-greedy seems to be a practical choice in graphs with size} $4<d<10$.

\begin{figure}[]
\centering
\includegraphics[scale=0.62]{figures/greedy_comp.pdf}
\caption{Performance of different greedy algorithms from Section~\ref{appendix_greedy}. Here, the trivial algorithm returns an empty graph. Runtime was measured on a machine with an Intel Core i5-6300U 2.5 GHz processor and 16 GB of RAM.}
\label{figure_greedy}
\end{figure}

\subsubsection{Statistical scalability: sample size needs to grow with increasing dimension}
\label{appendix_scalability}
Conducting independence tests and performing nonparametric regression becomes increasingly challenging as the number of covariates grows, particularly in high-dimensional settings, where such tasks demand substantially larger sample sizes. Similar limitations affect several existing algorithms, including ANM-RESIT, LOCI, and bQCD. As demonstrated below, the sample size required for consistent estimation increases systematically with the dimension~$d$.

\textbf{Data-generating process: }For each sample size \( n \) and dimension \( d \), we generated a uniformly random graph with \( d \) nodes and \( d-1 \) edges. The data was then generated according to the structural equation model \( X_i = f_i(\textbf{X}_{pa_i}) + \varepsilon_i \), where \( \varepsilon_i \sim N(0,1) \) and \( f_i(x) = c_i x^{2} \) with \( c_i \sim \text{Unif}(0.5, 1.5) \). 

\textbf{Results:}
We estimate the DAG using the $CPCM(F)$ algorithm, where $F$ is set to a Gaussian distribution with fixed variance, and apply the naive-greedy algorithm for structure learning. The accuracy of the estimated DAG is assessed using the Structural Intervention Distance (SID). This procedure is repeated 100 times, and we report the average SID. Figure~\ref{scalability} displays the ratio of the average SID from the CPCM(F) algorithm to the average SID of a baseline method that generates a random DAG.




\begin{figure}[]
\centering
\includegraphics[scale=0.6]{figures/scalability.pdf}
\caption{Ratio of the computed SID using the $CPCM(F)$ algorithm (edge-greedy version) to the SID of an algorithm producing a random graph, averaged over 100 repetitions for different values of \( n \) and \( d \). Lower SID fraction means better performance of the $CPCM(F)$ algorithm. Values around $SID\,fraction=1$ mean that our algorithm is no better than an algorithm producing a random graph.}
\label{scalability}
\end{figure}
