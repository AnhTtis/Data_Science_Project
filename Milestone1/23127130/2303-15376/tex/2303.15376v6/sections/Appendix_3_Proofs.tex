\renewcommand\thesection{C}
\section{Proofs}
\label{SectionProofs}

\subsection{Proof of Theorem~\ref{normalidentifiability}}
\begin{customthm}{\ref{normalidentifiability}}
Let $(X_1,X_2)$ admit the $CPCM(F)$ model with graph $X_1\to X_2$, where $F$ is the Gaussian distribution function with parameters $\theta(X_1)=\big(\mu(X_1), \sigma(X_1)\big)^\top$. 
Let $p_{\varepsilon_1}$ be the density of $\varepsilon_1$ that is absolutely continuous with full support $\mathbb{R}$. Let $\mu(x), \sigma(x)$ be two times differentiable.  

Then, the causal graph is identifiable from the joint distribution if and only if  there do not exist $a,c ,d,e, \alpha, \beta\in\mathbb{R}$,  
$a\geq 0,c>0, \beta>0$, such that
\begin{equation}\tag{\ref{norm}}
\frac{1}{\sigma^2(x)}=ax^2 + c, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \frac{\mu(x)}{\sigma^2(x)}=d+ex,
\end{equation}
for all $x\in\mathbb{R}$ and
\begin{equation}\tag{\ref{DensityDEF}}
p_{\varepsilon_1}(x) \propto \sigma(x)e^{-\frac{1}{2}\big[ \frac{(x-\alpha)^2}{\beta^2}  - \frac{\mu^2(x)}{\sigma^2(x)}\big]},
\end{equation}
where $\propto$ represents an equality up to a constant (here, $p_{\varepsilon_1} $  is a valid density function if and only if $\frac{1}{\beta^2}\neq  \frac{e^2}{c}\mathbbm{1}[a=0]$). 
Specifically, if $\sigma(x)$ is constant (case $a=0$), then the causal graph is identifiable unless $\mu(x)$ is linear and $p_{\varepsilon_1}$ is the Gaussian density.
\end{customthm}



\begin{proof}
\label{Proof of normalidentifiability}{}
We opt for proving this theorem from scratch, without using Theorem \ref{thmAssymetricMultivariatesufficient}. An interested reader can try to use Theorem \ref{thmAssymetricMultivariatesufficient} instead. For clarity regarding the indexes, we use the notation $X=X_1, Y=X_2$. 

First, we show that if the causal graph is not identifiable, then $\mu(x)$ and $\sigma(x)$ must satisfy (\ref{norm}). Let $p_{(X,Y)}$ be the density function of $(X,Y)$. Since the causal graph is not identifiable, there exist two CPCM models that generate $p_{(X,Y)}$: the CPCM model with  $X\to Y$  and the function  $\theta(x)=\big(\mu(x), \sigma^2(x)\big)^\top$ and the CPCM model with  $Y\to X$ and the function $\tilde{\theta}(y)=\big(\tilde{\mu}(y), \tilde{\sigma}^2(y)\big)^\top$. 

We decompose (corresponding to the direction $X\to Y$) 
\begin{equation*}
p_{(X,Y)}(x,y) = p_{X}(x)p_{Y\mid X}(y\mid x) = p_{X}(x) \phi\big(y;\theta(x)\big),
\end{equation*}
where $\phi\big(y;\theta(x)\big)$ is the Gaussian density function with parameters $\theta(x) = \big(\mu(x), \sigma^2(x)\big)^\top$. We rewrite this in the other direction: 
$$
p_{(X,Y)}(x,y) = p_{Y}(y)p_{X\mid Y}(x\mid y) = p_{Y}(y) \phi\big(x;\tilde{\theta}(y)\big).
$$
We take the logarithm of both equations and rewrite them in the following manner:
\begin{equation*}
\log[p_{X}(x)] +  \log\bigg\{\frac{1}{\sqrt{2\pi \sigma^2(x)}}e^{\frac{-[y-\mu(x)]^2}{2\sigma^2(x)}}\bigg\} = \log[p_{Y}(y)] +  \log\bigg\{\frac{1}{\sqrt{2\pi \tilde{\sigma}^2(y)}}e^{\frac{-(x-\tilde{\mu}(y))^2}{2\tilde{\sigma}^2(y)}}\bigg\} \text{  and}
\end{equation*}
\begin{equation}\label{eq1}
\log[p_{X}(x)] -\log\sigma(x)-\frac{1}{2}  \frac{[y-\mu(x)]^2}{\sigma^2(x)} = \log[p_{Y}(y)] -\log\tilde{\sigma}(y) -\frac{1}{2}  \frac{[x-\tilde{\mu}(y)]^2}{\tilde{\sigma}^2(y)}. 
\end{equation}
Calculating on both sides $\frac{\partial^4 }{\partial^2 x \partial^2 y }$, we obtain 
$$\frac{\sigma''(x)\sigma(x)-3\sigma'(x)'\sigma(x)}{\sigma^4(x)} =
\frac{\tilde{\sigma}''(y)\tilde{\sigma}(y)-3\tilde{\sigma}'(y)\tilde{\sigma}'(y)}{\tilde{\sigma}^4(y)}. $$Since this has to hold for all $x,y$, both sides need to be constant (let us denote this constant by $a\in\mathbb{R}$).  

Differential equation $\sigma''(x)\sigma(x)-3\sigma'(x)\sigma'(x)=a\,\sigma^4(x)$ has solution $\sigma(x) = \frac{1}{\sqrt{a(x+b)^2 + c}}$ for  $x$ , such that $a(x+b)^2 + c>0$. 

Plugging this result into (\ref{eq1}) and calculating on both sides $\frac{\partial^3 }{\partial^2 x \partial y }$, we obtain 
\begin{equation}\label{eq2}
\mu''(x) (a(x+b)^2+c) + \mu'(x) (4ax+4ab) + \mu(x) 2a = 2ab.
\end{equation}
Equation (\ref{eq2}) is another differential equation with a solution $\mu(x) = \frac{d+ex}{a(x+b)^2+c} + b$, for some $d,e\in\mathbb{R}$ for all $x{:}\,\, \sigma(x)>0$. 

Next, we show that it is necessary that $b=0$. If we show $b=0$, then $\mu(x)$ and $\sigma^2(x)$ are exactly in the form (\ref{norm}). We plug the representations  $\mu(x) = \frac{d+ex}{a(x+b)^2+c} + b, \sigma(x) = \frac{1}{\sqrt{a(x+b)^2 + c}}$ and   $\tilde{\mu}(x) = \frac{\tilde{d}+\tilde{e}x}{\tilde{a}(x+\tilde{b})^2+\tilde{c}} + \tilde{b}, \tilde{\sigma}(x) = \frac{1}{\sqrt{\tilde{a}(x+\tilde{b})^2 + \tilde{c}}}$  into (\ref{eq1}). Thus, we obtain
\begin{equation*}
\begin{split}
&\log[p_{X}(x)] +\frac{1}{2}\log[a(x+b)^2 + c]\\&
-\frac{1}{2}  \bigg[y^2(ax^2 + 2abx + ab^2+c) + y(2d+2ex) + \frac{1}{a(x+b)^2 +c} \bigg] \\&
= \log[p_{Y}(y)] +\frac{1}{2}\log[\tilde{a}(y+\tilde{b})^2 + \tilde{c}]\\& 
-\frac{1}{2}  \bigg[x^2(\tilde{a}y^2 + 2\tilde{a}\tilde{b}y + \tilde{a}\tilde{b}^2+\tilde{c}) + x(2\tilde{d}+2\tilde{e}y) + \frac{1}{\tilde{a}(y+\tilde{b})^2 +\tilde{c}} \bigg] .
\end{split}
\end{equation*}
We can re-write the last expression as
\begin{equation}\label{eq4}
\begin{split}
&h_X(x) + h_Y(y) =\frac{1}{2}  [y^2(ax^2 + 2abx ) + 2yex ] -\frac{1}{2}  [x^2(\tilde{a}y^2 + 2\tilde{a}\tilde{b}y) + 2x\tilde{e}y) ]\\
&=\frac{1}{2}[ x^2y^2(a - \tilde{a})  + xy(2aby - 2\tilde{a}\tilde{b}x +e-\tilde{e}) ],
\end{split}
\end{equation}
where 
\begin{equation*}\label{eqh_x}
h_X(x) = \log[p_{X}(x)]  +  \frac{1}{2}\log[a(x+b)^2 + c] + \frac{1}{2}\frac{1}{a(x+b)^2 +c} -2x\tilde{d} - x^2(\tilde{a}\tilde{b}^2+\tilde{c}), 
\end{equation*}
\begin{equation*}
h_Y(y) = -\log[p_{Y}(y)] -\frac{1}{2}\log[\tilde{a}(y+\tilde{b})^2 + \tilde{c}] +\frac{1}{2}[\frac{1}{\tilde{a}(y+\tilde{b})^2 +\tilde{c}} - 2yd - y^2(ab^2+c)].
\end{equation*}
Since the left-hand side of (\ref{eq4}) is in additive form, the right side also needs to have an additive representation. However, that is only possible if $a- \tilde{a}=0$ and $2aby - 2\tilde{a}\tilde{b}x +e-\tilde{e}=0$. Therefore, we necessarily have $a=\tilde{a}$ and either $a=0$ or $b=\tilde{b}=0$. The case $a=0$ corresponds to a constant $\sigma$ and, hence, also $b=\tilde{b}=0$. We have shown that $\mu(x)$ and $\sigma^2(x)$ have to satisfy (\ref{norm}).

Next, we show that if the causal graph is not identifiable, then the density of $p_{X}(x)$ has form (\ref{DensityDEF}). Plugging the form of  $\mu(x)$ and $\sigma^2(x)$ into (\ref{eq1}), we obtain
\begin{equation*}
\begin{split}
\log[p_{X}(x)] -&\log\sigma(x)-\frac{1}{2}\bigg(y-  \frac{d+ex}{ax^2+c}    \bigg)^2(ax^2+c) \\&= \log[p_{Y}(y)] -\log\tilde{\sigma}(y) -\frac{1}{2}  \bigg(x-  \frac{\tilde{d}+\tilde{e}y}{ay^2+\tilde{c}}   \bigg)^2(ay^2 + \tilde{c}).
\end{split}
\end{equation*}
We rewrite
\begin{equation}\label{rftgyh}
\begin{split}
\log[p_{X}(x)] -  &\log\sigma(x)+\frac{1}{2}\bigg[ \tilde{c}x^2 - 2x\tilde{d} +  \frac{({d}+{e}x)^2}{ax^2+{c}}   \bigg] \\&= \log[p_{Y}(y)] -\log\tilde{\sigma}(y) +\frac{1}{2}\bigg[cy^2 - 2yd + \frac{(\tilde{d}+\tilde{e}y)^2}{ay^2+\tilde{c}} \bigg]
. 
\end{split}
\end{equation}
Since this has to hold for all  $x,y\in\mathbb{R}$, both sides of (\ref{rftgyh}) need to be constant and we obtain $\log[p_{X}(x))]\propto\log\sigma(x)-\frac{1}{2}  \big[\tilde{c}x^2 - 2x\tilde{d} +  \frac{({d}+{e}x)^2}{ax^2+{c}}   \big] $. Hence, 
$$
p_{X}(x) \propto \sigma(x)e^{-\frac{1}{2}  \big[ \tilde{c}x^2 - 2x\tilde{d} +  \frac{({d}+{e}x)^2}{ax^2+{c}}   \big]} =  \sigma(x)e^{-\frac{1}{2}\big[ \frac{(x-\alpha)^2}{\beta^2}  - \frac{\mu^2(x)}{\sigma^2(x)}\big]},
$$
where $\beta = 1/\sqrt{\tilde{c}}$ and $\alpha=\frac{\tilde{d}}{\tilde{c}}$. The condition $\frac{1}{\beta^2} > \frac{e^2}{c}\mathbbm{1}[a=0]$ arises from the fact that if $a=0$ and $\frac{1}{\beta^2} \leq \frac{e^2}{c}$, then $p_X(x)$ is not a density function. This is because  
\[ 
\sigma(x)e^{-\frac{1}{2}\left[ \frac{(x-\alpha)^2}{\beta^2} - \frac{\mu^2(x)}{\sigma^2(x)}\right]} \propto e^{-\frac{1}{2}\left[ x^2\left(\frac{1}{\beta^2} - \frac{e^2}{c}\right) + x\left(-2\frac{\alpha}{\beta^2} - \frac{2de}{c}\right) \right] }
\]
for all \(x \in \mathbb{R}\). This expression is integrable is and only if the coefficient at \(x^2\) is positive.

Finally, we deal with the other direction: we show that if $\mu$ and $\sigma$ satisfy (\ref{norm}) and $p_{\varepsilon_X}$ has form (\ref{DensityDEF}), then the causal graph is not identifiable. Assume that $a,c,d,e$ are given. Define $\tilde{a} = a, \tilde{e}=e$ and select $\tilde{c}, \tilde{d}\in\mathbb{R}$, such that $\tilde{c}>0,\tilde{c}\neq  \frac{e^2}{c}\mathbbm{1}[a=0]$. Define $\frac{1}{\tilde{\sigma}^2(y)} = \tilde{a} y^2 + \tilde{c}, \frac{\tilde{\mu}(y)}{\tilde{\sigma}^2(y)} = \tilde{d}+\tilde{e}y$. Moreover, define 
\begin{align*}
p_X(x) &\propto \sigma(x)e^{-\frac{1}{2}\big[ \tilde{c}\big(x-\frac{\tilde{d}}{\tilde{c}}\big)^2  - \frac{\mu^2(x)}{\sigma^2(x)}\big]}\,\,\text{             and}\\
p_Y(y) &\propto \tilde{\sigma}(y)e^{-\frac{1}{2}\big[c\big(x-\frac{{d}}{{c}}\big)^2  - \frac{\tilde{\mu}^2(y)}{\tilde{\sigma}^2(y)}\big]}.
\end{align*}
Note that regardless of the coefficients, these are valid density functions (with one exception when $\tilde{c}=\frac{e^2}{c}$ and $a=0$, which is why we selected $\tilde{c}\neq  \frac{e^2}{c}\mathbbm{1}[a=0]$). In case of $a=0$, this is the classical Gaussian distribution density function.

Using these values, we obtain the equality
$$
p_X(x)p_{Y\mid X}(y\mid x) = p_Y(y)p_{X\mid Y}(x\mid y), \forall x,y\in\mathbb{R},
$$
or more precisely, 
$$
\sigma(x)e^{-\frac{1}{2}\big[ \tilde{c}\big(x-\frac{\tilde{d}}{\tilde{c}}\big)^2  - \frac{\mu^2(x)}{\sigma^2(x)}\big]}\frac{1}{\sqrt{2\pi}\sigma(x)}e^{-\frac{1}{2}\frac{[y-\mu(x)]^2}{\sigma^2(x)}} \propto \tilde{\sigma}(y)e^{-\frac{1}{2}\big[c\big(x-\frac{{d}}{{c}}\big)^2  - \frac{\tilde{\mu}^2(y)}{\tilde{\sigma}^2(y)}\big]}\frac{1}{\sqrt{2\pi}\tilde{\sigma}(y)}e^{-\frac{1}{2}\frac{(x-\tilde{\mu}(y))^2}{\tilde{\sigma}^2(y)}}. 
$$
Since this holds for all $x,y\in\mathbb{R}$, we found a valid backward model. The density in (\ref{DensityDEF}) uses the notation $\alpha=\frac{\tilde{d}}{\tilde{c}}$ and $\beta = 1/\sqrt{\tilde{c}}$.  
\end{proof}

An example of the joint distribution of $X_1, X_2$ with $a=c=d=e=\alpha = \beta=1$ is depicted in Figure \ref{GaussianDensity}. 
\begin{figure}[ht]
\centering
\includegraphics[scale=0.35]{figures/Gaussian2.png}
\caption{Random sample from a joint distribution of $(X_1,X_2)$, where  $X_1$ has the marginal density (\ref{DensityDEF}) and $X_2\mid X_1\sim N\big(\mu(X_1), \sigma^2(X_1)\big)$ with $\mu, \sigma$ defined in (\ref{norm}) with constants   $a=c=d=e=\alpha= \beta=1$. The distribution function is symmetric according to the $x=y$ axis (red line).    }
\label{GaussianDensity}
\end{figure}


\subsection{Proof of Consequence \ref{paretoidentifiability}}


\begin{customconsequence}{\ref{paretoidentifiability}}
\begin{itemize}
   \item Let $(X_1,X_2)$admit the $CPCM(F)$ model with graph $X_1\to X_2$, where $F$ is the (discrete) Poisson distribution function.  Then, the causal graph is \textit{not} identifiable if and only if
\begin{equation}\label{eq505}
     \lambda(x) =e^{ax+b},\,\,\,\,\,\,\,\, P(X_1=x) \propto \frac{e^{e^{ax+b}+cx}}{x! }, \,\,\,\,\,\,\,\,\forall x\in\mathbb{N}_0,
     \end{equation}
for some $a<0,b,c\in\mathbb{R}$.

   \item  Let $(X_1,X_2)$ admit the $CPCM(F)$ model with graph $X_1\to X_2$, where $F$ is the Pareto distribution function. Then, the causal graph is \textit{not} identifiable if and only if   
\begin{equation}\label{eq50}
\theta(x) = a\log(x) +b,\,\,\,\,\,\,\,\, p_{X_1}(x) \propto \frac{1}{ [a\log(x)+b] x^{c+1} }, \,\,\,\,\,\,\,\,\forall x\geq 1,
\end{equation}
for some $a,b,c>0$.

    \item Let $(X_1,X_2)$ admit the $CPCM(F)$ model with graph $X_1\to X_2$, where $F$ is Bernoulli distribution function. Then, the causal graph is identifiable if and only if $supp(X_1) \neq \{0,1\}$.
\end{itemize}


\end{customconsequence}
\begin{proof}
\label{Proof of pareto identifiability}

\textbf{First bullet-point: }Poisson distribution has one parameter and it can be written as $h_1(x) = 1/x!, h_2(x) = e^{-e^x}, T(x) = x$. Note that we do not use classical form of density function but its reparametrisation where $\theta(x) = \log(\lambda(x))$ where $\lambda$ is the classical rate parameter. 

Plugging this into Proposition~\ref{Necessary condition for identifiability}, we directly obtain (\ref{eq505}) with possible $a\in\mathbb{R}$. It is not hard to see that $ \frac{e^{e^{ax+b}+cx}}{x! }$ is integrable if and only if $a<0$. In such a case, the backward model exist and has a form $\tilde{\theta}(y) = a y+c$ and $P(X_2=y) \propto \frac{e^{e^{ay+c}+by}}{y! } $.

\textbf{Second bullet-point:} Pareto distribution has one parameter and it can be written as $h_1(x) = 1, h_2(\theta) = 1/\theta, T(x) = log(x)$. 

Plugging this into Proposition~\ref{Necessary condition for identifiability}, we directly obtain (\ref{eq50}) with possible $a,b,c\in\mathbb{R}$.  It is not hard to see that the density is integrable if and only if $a,c,b>0$, in which case, the backward model exist and has a form $\tilde{\theta}(y) = \tilde{a}\log(y) + \tilde{b}$, $ p_{X_2}(x) \propto \frac{1}{ [\tilde{a}\log(x)+\tilde{b}] x^{\tilde{c}+1} }$ for $\tilde{a}=a,\tilde{b} = c,\tilde{c} = b$. 

\textbf{Third bullet-point:} If $supp(X_1) \neq \{0,1\}$, then the causal graph is identifiable due to the first bullet-point in Proposition~\ref{Necessary condition for identifiability}. Next, consider $supp(X_1) = \{0,1\}$. In this case, we can always write a backward model for the Bernoulli distribution. Let $P(X_1=X_2=0) = p_0$,  $P(X_1=0, X_2 = 1) = p_{0,1}$, $P(X_1=1, X_2 = 0) = p_{1,0}$ and $P(X_1=X_2 = 1) = p_{1}$ for $p_0, p_{0,1}, p_{1,0}, p_1>0$ and $p_0 + p_{0,1} + p_{1,0} + p_1 = 1$. We can define $X_2\mid X_1\sim Bernoulli(\theta(X_1))$ as $\theta(0) = p_{0,1}$ and $\theta(1) = p_1$. On the other hand, we can define $X_1\mid X_2 \sim Bernoulli(\tilde{\theta}(X_2))$ as $\tilde{\theta}(0) = p_{1,0}$ and $\tilde{\theta}(1) = p_{1}$. Since both models produce the same joint distribution, the causal model is not identifiable for any values of $p_0, p_{0,1}, p_{1,0}, p_1$.  
\end{proof}



%%%%%%%%%%%%%%%%%%%%% Theorem 2 %%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem \ref{thmAssymetricMultivariatesufficient}}
Before we prove Theorem \ref{thmAssymetricMultivariatesufficient}, we show the following auxiliary lemma. 
\begin{lemma}\label{PomocnaLemma1}
Let $n\in\mathbb{N}$ and $\mathcal{X,Y}\subseteq \mathbb{R}$. Let $f_1, \dots, f_n, g_1, \dots, g_n$ be non-constant functions on $\mathcal{X,Y}$, respectively, such that 
$
f_1(x)g_1(y) + \dots + f_n(x)g_n(y)
$ is additive in $x,y$---that is, there exist functions $f$ and $g$, such that 
$$
f_1(x)g_1(y) + \dots + f_n(x)g_n(y) = f(x) + g(y), \forall x\in\mathcal{X},y\in\mathcal{Y}.
$$
Then, there exist (not all zero) constants $a_1, \dots, a_n, c\in\mathbb{R}$, such that 
$\sum_{i=1}^n a_if_i(x) = c$ for all $x\in\mathcal{X}$. Specifically for $n=2$, it holds that $f_1(x) = af_2(x)+c$ for some $a,c\in\mathbb{R}$. 

Moreover, assume that for some $q<n$, functions $g_1, \dots, g_q$ are affinly independent---that is, there exist $y_0, y_1, \dots, y_q\in\mathcal{Y}$, such that a matrix 
\begin{equation}\label{matrix243}
M=\begin{pmatrix}
 g_1(y_1) - g_1(y_0) & \cdots & g_q(y_1) - g_q(y_0) \\
\cdots & \ddots & \cdots \\
g_1(y_q) - g_1(y_0) & \cdots & g_q(y_{q}) - g_q(y_0)
\end{pmatrix} 
\end{equation}
has full rank. Then, for all $i=1, \dots, q$ there exist constants $a_{q+1}, \dots, a_n, c\in\mathbb{R}$, such that $f_i(x)=\sum_{j=q+1}^n a_jf_j(x) +c$ for all $x\in\mathcal{X}$. 
\end{lemma}
\begin{proof}
Fix $y_1, y_2\in\mathcal{Y}$, such that $g_1(y_1)\neq g_1(y_2)$. Then, we have for all $x\in\mathcal{X}$
\begin{align*}
&f_1(x)g_1(y_1) + \dots + f_n(x)g_n(y_1) = f(x) + g(y_1),\\&
f_1(x)g_1(y_2) + \dots + f_n(x)g_n(y_2) = f(x) + g(y_2),
\end{align*}
and subtraction of these equalities yields 
$$
f_1(x)[g_1(y_1)- g_1(y_2)] + \dots + f_n(x)[g_n(y_1)-g_n(y_2)] = g(y_1) - g(y_2).
$$
Defining $a_i = g_i(y_1)- g_i(y_2)$ and $c = g(y_1)- g(y_2)$ yields the first result (with $a_1\neq 0$).

Now, we prove the ``Moreover'' part. Consider equalities 
\begin{align*}
f_1(x)g_1(y_0) + &\dots + f_n(x)g_n(y_0) = f(x) + g(y_0),\\
f_1(x)g_1(y_1) + &\dots + f_n(x)g_n(y_1) = f(x) + g(y_1),\\
&\dots\\
f_1(x)g_1(y_q) + &\dots + f_n(x)g_n(y_q) = f(x) + g(y_q),
\end{align*}
where $y_0, \dots, y_q$ are defined, such that matrix (\ref{matrix243}) has full rank. Subtracting from each equality, the first equality yields 
\begin{align*}
f_1(x)[g_1(y_1)- g_1(y_0)] + &\dots + f_n(x)[g_n(y_1)-g_n(y_0)] = g(y_1) - g(y_0)\\
&\dots \\
f_1(x)[g_1(y_q)- g_1(y_0)] + &\dots + f_n(x)[g_n(y_q)-g_n(y_0)] = g(y_q) - g(y_0).
\end{align*}
Using matrix formulation, this can be rewritten as 
\begin{equation}
M\begin{pmatrix}
f_1(x) \\
\cdots \\
f_q(x) 
\end{pmatrix} =
\begin{pmatrix}
g(y_1)-g(y_0) -\sum_{j=q+1}^n f_{j}(x)[g_{j}(y_1) -g_{j}(y_0)] \\
\cdots \\
g(y_q)-g(y_0) -\sum_{j=q+1}^n f_{j}(x)[g_{j}(y_q) -g_{j}(y_0)]
\end{pmatrix} .
\end{equation}
Multiplying both sides by $M^{-1}$ indicates that $f_i(x), i=1, \dots, q$ are nothing else than a linear combination of $f_{q+1}(x), \dots, f_n(x)$, which is what we wanted to show.
\end{proof}



\begin{customthm}{\ref{thmAssymetricMultivariatesufficient}}
Let $(X_1, X_2)$ follow the $CPCM(F_1, \dots, F_k)$ model with graph $X_1\to X_2$, where $F_1, \dots, F_k$ belong to the exponential family of distributions with corresponding sufficient statistics  $T_m= (T_{m,1}, \dots, T_{m,q_m})^\top$, $m=1, \dots, k$.  Following Definition~\ref{CPCM(F1F2)}, let $\tilde{m}\in\{1, \dots, k\}$ be the index such that  $X_2 = F_{\tilde{m}}^{-1}\big(\varepsilon_2; \theta_2(X_1)\big)$. 

The causal graph is identifiable if for all $m\in \{1, \dots, k\}$, at least one of the following holds: 
\begin{itemize}
    \item $ supp(F_m) \neq supp(X_1)$. 
    \item The function \( \theta_2 \) is not a linear combination of the sufficient statistics \( T_{m,1}, \dots, T_{m,q_m} \), i.e., there do not exist coefficients \( a_{i,j}, b_i \in \mathbb{R} \) for \( i = 1, \dots, q_{\tilde{m}} \) and \( j = 1, \dots, q_m \) such that  
   \begin{equation}\tag{\ref{eq158}}
   \theta_{2,i}(x) = \sum_{j=1}^{q_m} a_{i,j} T_{m,j}(x) + b_i, \quad \forall x \in \operatorname{supp}(X_1), \quad \forall i \in \{1, \dots, q_{\tilde{m}}\}.
   \end{equation}  
    \item There do not exist constants \( c_1, \dots, c_{q_m} \in \mathbb{R} \) such that the density of \( X_1 \) satisfies  
   \begin{equation}\tag{\ref{eq007v2}}
   p_{X_1}(x) \propto \frac{h_{m,1}(x)}{h_{\tilde{m},2}[\theta_2(x)]} e^{\sum_{i=1}^{q_m} c_i T_{m,i}(x)}, \quad \forall x \in \operatorname{supp}(X_1),
   \end{equation}  where \( h_{m,1} \) is a base measure associated with \( F_{m} \) and \( h_{\tilde{m},2} \) is the normalizing function of \( F_{\tilde{m}} \), both defined in \hyperref[appendix_exponential_family]{Appendix} \ref{appendix_exponential_family}. 
\end{itemize}

Consequentially, the space of non-identifiable distributions is contained in a $\tilde{d}$-dimensional space, where 
\begin{equation}\tag{\ref{dimension_in_theorem2}}
    \tilde{d} = \sum_{m\in\{1, \dots, k\}:  supp(F_m) = supp(X_1)} (q_m+1)(q_{\tilde{m}}+1) -1 .  
\end{equation}
\end{customthm}



\begin{proof}
\label{Proof of thmAssymetricMultivariatesufficient}{}

If the $CPCM(F_1,\dots, F_k)$ is \textit{not} identifiable, then there exists $m\in\{1, \dots, k\}$ and functions $\theta_1$ and $\theta_2$, such that models 
\begin{equation}
    \label{eq425}
    X_1 = \varepsilon_1, X_2 = F_{\tilde{m}}^{-1}(\varepsilon_2, \theta_2(X_1))\text{, and } X_2 = \varepsilon_2, X_1 = F_m^{-1}(\varepsilon_1, \theta_1(X_2))
\end{equation}generate the same joint density function. For simplifying the notation, let $m=1$ and $\tilde{m}=2$. 

\textbf{1) }Trivially, $X_1$ can not be generated as $X_1 = F_1^{-1}(\varepsilon_1, \theta_1(X_2))$ if $supp(F_1) \neq supp(X_1)$. 

\textbf{2)} For a contradiction, we show that $\theta_2$ is a linear combination of $T_{1,1}, \dots, T_{1,q_m}$. Decompose the joint density as
\begin{equation}\label{eq59}
  p_{(X_1, X_2)}(x,y) = p_{X_1}(x)p_{X_2\mid {X_1}}(y\mid x) = p_{X_2}(y)p_{{X_1}\mid {X_2}}(x\mid y), \,\,\,\,\,\,\,\,x\in supp(X_1), y\in supp(X_2).
 \end{equation}
Since $F_1$ and $F_2$ lie in the exponential family of distributions, we use the notation from \hyperref[appendix_exponential_family]{Appendix} \ref{appendix_exponential_family} and rewrite it as
\begin{equation*}
    \begin{split}
  &     p_{{X_2}\mid {X_1}}(y\mid x) = h_{1,1}(y)h_{1,2}[\theta_2(x)]e^{\sum_{i=1}^{q_2}\theta_{2,i}(x)T_{2,i}(y)},\\&
  p_{{X_1}\mid {X_2}}(x\mid y) = h_{2,1}(x)h_{2,2}[{\theta_1}(y)]e^{\sum_{i=1}^{q_1}{\theta}_{1,i}(y)T_{1,i}(x)}.  
    \end{split}
\end{equation*}
After a logarithmic transformation of both sides of (\ref{eq59}), we obtain 
\begin{equation}\label{eq254}
\begin{split}
\log[p_{(X_1,X_2)}(x,y)] &= \log[p_{X_1}(x)] +  \log[h_{1,1}(y)]+\log\{h_{1,2}[\theta_{2}(x)]\} + \sum_{i=1}^{q_2}\theta_{2,i}(x)T_{2,i}(y) \\&
= \log[p_{X_2}(y)] +  \log[h_{2,1}(x)]+\log\{h_{2,2}[\theta_{1}(y)]\} + \sum_{i=1}^{q_1}\theta_{1,i}(y)T_{1,i}(x).
\end{split}
\end{equation}
Define $f(x) = \log[p_{X_1}(x)] +\log\{h_{1,2}[\theta_{2}(x)]\} -\log[h_{2,1}(x)]$ and $g(y) =\log[h_{1,1}(y)] -  \log[p_{X_2}(y)] + \log\{h_{2,2}[\theta_{1}(y)]\}$. Then, equality (\ref{eq254}) reads as 
\begin{equation}\label{eq9876}
f(x) + g(y) = \sum_{i=1}^{q_1}\theta_{1,i}(y)T_{1,i}(x) - \sum_{i=1}^{q_2}T_{2,i}(y)\theta_{2,i}(x).
\end{equation}
Finally, we use Lemma \ref{PomocnaLemma1}. We know that functions $T_{2,i}$ are affinly independent in the sense presented in Lemma  \ref{PomocnaLemma1} (see (\ref{eq145151}) in \hyperref[appendix_exponential_family]{Appendix} \ref{appendix_exponential_family}). Therefore, Lemma \ref{PomocnaLemma1} gives us that $\theta_{2,i}, i=1, \dots, q_2$ are only a linear combination of $T_{1, j}, j=1, \dots, q_1$, which is what we wanted to show. 

\textbf{3)} For a contradiction, we show that $p_{X_1}$ must have a form \eqref{eq007v2}. Let us rewrite equation~\eqref{eq9876} into 
\begin{equation}\label{eq9876543}
\begin{split}
    \log[p_{X_1}(x)]   =&-\log\{h_{1,2}[\theta_{2}(x)]\} +\log[h_{2,1}(x)]-g(y)\\&  +\sum_{i=1}^{q_1}\theta_{1,i}(y)T_{1,i}(x) - \sum_{i=1}^{q_2}T_{2,i}(y)\theta_{2,i}(x).
\end{split}
\end{equation}
Fix $y\in supp(F_2)$. Using the form of $\theta_{2,i}$ from the previous bullet-point, we can write 
\begin{equation*}
    \begin{split}
      &  \sum_{i=1}^{q_1}\theta_{1,i}(y)T_{1,i}(x) - \sum_{i=1}^{q_2}T_{2,i}(y)\theta_{2,i}(x) = \sum_{i=1}^{q_1}\theta_{1,i}(y)T_{1,i}(x) - \sum_{i=1}^{q_2}T_{2,i}(y)\bigg[ \sum_{j=1}^{q_1}a_{i,j}T_{1,j}(x)+b_i \bigg] \\& = \sum_{i=1}^{q_1}c_iT_{1,i}(x)+d, 
    \end{split}
\end{equation*}
where $c_i = \theta_{1,i}(y) - \sum_{j=1}^{q_2}\sum_{k=1}^{q_1}T_{2,i}(y)a_{i,j}$ and $d =  \sum_{j=1}^{q_2}b_jT_{2,j}(y)$. Therefore, equation~\ref{eq9876543} can be written as 
\begin{equation*}
   \log[p_{X_1}(x)]   =-\log\{h_{1,2}[\theta_{2}(x)]\} +\log[h_{2,1}(x)]  +\sum_{i=1}^{q_1}c_iT_{1,i}(x) +[d-g(y)]  .
\end{equation*}
Applying exponential on both sides, we obtain (\ref{eq007v2}). 

\textbf{Part ''Consequentially'':} We have shown that if \eqref{eq425} holds, then \( \operatorname{supp}(F_m) = \operatorname{supp}(X_1) \), and the joint density \( p_{(X_1, X_2)} \) is uniquely determined by the coefficients \( a_{i,j}, b_i, c_j \in \mathbb{R} \), where \( i = 1, \dots, q_{\tilde{m}} \) and \( j = 1, \dots, q_m \).  

By counting the number of these coefficients, we find that there are \( (q_m+1)(q_{\tilde{m}}+1) -1 \) of them, with the  ``\( -1 \)'' term accounting for the normalization of the density function. Consequently, \eqref{dimension_in_theorem2} follows by summing over all \( m \in \{1, \dots, k\} \).  
\end{proof}



\subsection{Proof of Consequence \ref{consequenceprva}}\label{consequence}
\begin{customconsequence}{\ref{consequenceprva}}
\begin{itemize}
\item Suppose that \( \text{supp}(X_1) = \mathbb{R} \), \(\text{supp}(X_2) = \{0, 1, \dots\}\) such as on Figure~\ref{Asymmetrical_picture}, and let \((X_1, X_2)\) admit the \(CPCM(F_1, F_2)\) model with graph \(X_1 \to X_2\), where \(F_1\) is a Gaussian distribution and \(F_2\) is a Poisson distribution with rate parameter \(\lambda\). The causal graph is identifiable if and only if there do not exist constants \(a_1, a_2, b, c_1, c_2\in \mathbb{R}\), $a_1, c_1<0$, such that for all \(x \in \mathbb{R}\)
\begin{equation*}
\lambda(x) = e^{a_1 x^2 +a_2x + b}, \quad p_{X_1}(x) \propto e^{c_1 x^2 + c_2 x }.
\end{equation*}
\item Let \((X_1, X_2)\) admit the \(CPCM(F)\) model with graph \(X_1 \to X_2\), where \(F\) is a Gamma distribution with parameters \(\theta = (\alpha, \beta)^\top\). If there do not exist constants \(a, b, c, d, e, f \in \mathbb{R}\) such that
\begin{equation*}
\alpha(x) = a\log(x) + bx + c, \quad \beta(x) = d\log(x) + ex + f, \quad \forall x > 0,
\end{equation*}
then the causal graph is identifiable.
\item Let \((X_1, X_2)\) admit the \(CPCM(F_1, F_2)\) model, where \(F_1\) is a Gamma distribution with parameters \(\theta_1 = (\alpha_1, \beta_1)^\top\) and \(F_2\) is a Beta distribution with parameters \(\theta_2 = (\alpha_2, \beta_2)^\top\). If there do not exist constants \(a_i, b_i, c_i, d_i, e_i, f_i \in \mathbb{R}\), \(i = 1, 2\), such that for all \(x \in (0, 1)\)
\begin{equation*}
\begin{split}
\alpha_1(x) &= a_1 \log(x) + b_1 x + c_1, \quad \beta_1(x) = d_1 \log(x) + e_1 x + f_1, \\
\alpha_2(x) &= a_2 \log(x) + b_2 \log(1 - x) + c_2, \quad \beta_2(x) = d_2 \log(x) + e_2 \log(1 - x) + f_2,
\end{split}
\end{equation*}
then the causal graph is identifiable.
\end{itemize}
\end{customconsequence}

\begin{proof}
\label{proof_of_consequence_multi}
Poisson distribution has one parameter and it can be written as $h_1(x) = 1/x!, h_2(x) = e^{-e^x}, T(x) = x$. Note that we do not use classical form of density function but its reparametrisation where $\theta(x) = \log(\lambda(x))$ where $\lambda$ is the classical rate parameter. Theorem \ref{thmAssymetricMultivariatesufficient} gives us that the causal graph is identifiable if there do not exist constants \(a_1, a_2, b, c_1, c_2\in \mathbb{R}\), such that for all \(x \in \mathbb{R}\)
\begin{equation*}
\lambda(x) = e^{a_1 x^2 +a_2x + b}, \quad p_{X_1}(x) \propto e^{c_1 x^2 + c_2 x },
\end{equation*}
then the causal graph is identifiable. It is a simple exercise to prove that the joint distribution is integrable if and only if $a_1, c_1<0$. 

The second and the third bullet-point follow directly from Theorem \ref{thmAssymetricMultivariatesufficient}, noting the following: 
\begin{itemize}
    \item The density function of the \textbf{Gamma distribution} with parameters \(\theta = (\alpha, \beta)^\top\) is given by \(p(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha - 1} e^{-\beta x}\), \(x > 0\). The sufficient statistics are \([T_1(x), T_2(x)] = [\log(x), x]\).
    \item The density function of the \textbf{Beta distribution} with parameters \(\theta = (\alpha, \beta)^\top\) is given by \(p(x) = \frac{1}{B(\alpha, \beta)} x^{\alpha - 1} (1 - x)^{\beta - 1}\). The sufficient statistics are \([T_1(x), T_2(x)] = [\log(x), \log(1 - x)]\).
\end{itemize}



\end{proof}

%%%%%%%%%%%%%%%%%%%%% Section 2.4%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Lemma \ref{thmMultivairateIdentifiability}}
\begin{customlem}{\ref{thmMultivairateIdentifiability}}
Let $F_{\textbf{X}}$ be generated by the $CPCM(F_1, \dots, F_k)$ with DAG $\mathcal{G}$ and with density $p_\textbf{X}$. Assume that for all $ i,j\in\mathcal{G}$, $ S\subseteq V$, such that $i\in pa_j$ and  $pa_j\setminus \{i\}\subseteq S \subseteq nd_j\setminus\{i,j\}$, there exist $\textbf{x}_{S}{:}\,\,  p_S(\textbf{x}_S)>0$, such that a bivariate model defined as $X=\tilde{\varepsilon}_X, Y = F^{-1}_j\big(\tilde{\varepsilon}_Y, \tilde{\theta}(X)\big)$ is identifiable (in the sense of Definition \ref{DEFidentifiability}), where  $F_{\tilde{\varepsilon}_X} = F_{X_i\mid \textbf{X}_{S} =\textbf{ x}_S}    $ and $\tilde{\theta}(x) = \theta_j(\textbf{x}_{pa_j\setminus\{i\}}, x)$,  $x\in supp(X)$.
Then,  $\mathcal{G}$ is identifiable from the joint distribution. 
 \end{customlem}
 
 \begin{proof}
 \label{Proof of thmMultivairateIdentifiability}
Let there be two $CPCM(F_1, \dots, F_k)$ models, with causal graphs $\mathcal{G}\neq \mathcal{G}'$, that both generate $F_\textbf{X}$.   From Proposition 29 in \cite{Peters2014} (recall that we assume causal minimality of $CPCM(F_1, \dots, F_k)$), there exist variables $L,Y\in \{X_1, \dots, X_d\}$, such that 
\begin{itemize}
\item $Y\to L$ in $\mathcal{G}$ and $L\to Y$ in $\mathcal{G}'$,
\item $S:=\underbrace{\big\{pa_L(\mathcal{G})\setminus\{Y\}\big\}}_\text{\textbf{Q}}\cup\underbrace{\big\{pa_Y(\mathcal{G}')\setminus\{L\}\big\}}_\text{\textbf{R}}\subseteq \big\{nd_L(\mathcal{G}) \cap nd_Y(\mathcal{G}')\setminus\{Y,L\}\big\} $. 
\end{itemize}
For this $S$, select $\textbf{x}_S$ in accordance to the condition in the theorem. Below, we use the notation $\textbf{x}_S=(\textbf{x}_q, \textbf{x}_r)$ where $q\in \textbf{Q}, r\in \textbf{R}$. Now, we use Lemma 36 and Lemma 37 from \citep{Peters2014}.  Since $Y\to L$ in $\mathcal{G}$, we define a bivariate SCM as  \footnote{Informally, we consider  $Y^\star := Y\mid \{\textbf{X}_S=\textbf{x}_S\}$ and $L^\star:=L\mid \{\textbf{X}_S=\textbf{x}_S\}$.} $$Y^\star=\tilde{\varepsilon}_{Y^\star},\,\,\,\,\,\,\,\,\,\, L^\star = F^{-1}_{L}\big(\varepsilon_L; \theta_L(Y^\star)\big),$$ 
where $\tilde{\varepsilon}_{Y^\star} \overset{D}{=} Y\mid \{\textbf{X}_S=\textbf{x}_S\}$ and $\varepsilon_L\indep Y^\star, \varepsilon_L\sim U(0,1)$. This is a bivariate CPCM with $Y^\star\to L^\star$. However, the same holds for the other direction: Since $L\to Y$ in $\mathcal{G}'$, we can also define a bivariate SCM in the following manner: $$L^\star=\tilde{\varepsilon}_{L^\star},\,\,\,\,\,\,\,\,\,\, Y^\star = F^{-1}_{Y}\big(\varepsilon_Y; \theta_Y(L^\star)\big),$$ 
 where $\tilde{\varepsilon}_{L^\star} \overset{D}{=}L\mid \{\textbf{X}_S=\textbf{x}_S\}$ and $\varepsilon_Y\indep L^\star, \varepsilon_Y\sim U(0,1)$. We obtained a bivariate CPCM with $L^\star\to Y^\star$, which is a contradiction with the pairwise identifiability. Hence,  $\mathcal{G}= \mathcal{G}'$.  
 \end{proof}
 
\subsection{Proof of Lemma \ref{lemma_o_overparametrizacii}}
 \begin{customlem}{\ref{lemma_o_overparametrizacii}}
Suppose that the joint distribution $F_{(X_1,X_2)}$ is generated according to the model $CPCM(F_2)$ with graph $X_1\to X_2$, where $F_2$ is a distribution function belonging to the exponential family. 

Then, there exists $F_1$ such that the model $CPCM(F_1)$ with graph $X_2\to X_1$ also generates $F_{(X_1,X_2)}$. In other words, there exists $F_1$ such that the causal graph in $CPCM(F_1, F_2)$ is not identifiable from the joint distribution. 
\end{customlem}
\begin{proof}\label{Proof of lemma_o_overparametrizacii}
The idea of the proof is the following: we select $F_1$, such that its sufficient statistic is equal to $\theta_2$. 

Let us denote the original model as
\begin{equation*}
\begin{split}
 & \,\,\,\,\,\,\,  X_1 = \varepsilon_1, X_2 = F_2^{-1}\big(\varepsilon_2, \theta_2(X_1)\big), \varepsilon_2\sim U(0,1), \varepsilon_1\indep \varepsilon_2,
\end{split}
\end{equation*}
where (using notation from Appendix \ref{appendix_exponential_family}) the conditional density function has a form:
$$
p_{X_2\mid {X_1}}(y\mid x) =h_{2,1}(y)h_{2,2}[\theta_2(x)]\exp[\theta_{2}(x)T_{2}(y)].
$$
We define $F_1$ from an exponential family in the following manner: consider the sufficient statistic $T_1(x) = \theta_2(x)$ for all $x$ in support of $X_1$ and choose $h_{1,1}(x) =  p_{X_1}(x)h_{2,2}[\theta_2(x)]$ and $h_{1,2}(y) = \frac{h_{2,1}(y)}{p_{X_2}(y)}$ for all $y$ in support of $X_2$. Then, a model where 
\begin{equation*}
\begin{split}
 & \,\,\,\,\,\,\,  X_2 = \varepsilon_2, X_1 = F_1^{-1}\big(\varepsilon_1, \theta_1(X_2)\big), \varepsilon_1\sim U(0,1), \varepsilon_1\indep \varepsilon_2,
\end{split}
\end{equation*}
for a specific choice $\theta_1(y) = T_2(y)$ has the following conditional density function: 
$$
p_{X_1\mid {X_2}}(x\mid y) =h_{1,1}(x)h_{1,2}[\theta_1(y)]\exp[\theta_{1}(y)T_{1}(x)] = \frac{p_{X_1}(x)}{p_{X_2}(y)}h_{2,1}(y)h_{2,2}[\theta_2(x)]\exp[\theta_{2}(x)T_2(y)].
$$
Therefore, the joint distribution is equal in both models, since
\begin{equation*}
\begin{split}
   p_{X_1}(x) h_{2,1}(y)h_{2,2}[\theta_2(x)]\exp[\theta_{2}(x)T_{2}(y)] &=p_{X_2}(y) \frac{p_{X_1}(x)}{p_{X_2}(y)}h_{2,1}(y)h_{2,2}[\theta_2(x)]\exp[\theta_{2}(x)T_2(y)]\\
  p_{X_1}(x)p_{X_2\mid {X_1}}(y\mid x) &= p_{X_2}(y)p_{{X_1}\mid {X_2}}(x\mid y).
\end{split}
 \end{equation*}
We found $CPCM(F_1)$ model with graph $X_2\to X_1$ that generates the same distribution. 
\end{proof}


 
 
 
 
 
 
 
 
 
 
 