
\section{Identifiability results}
\label{Section_identifiability}
Identifiability is a prerequisite for causal discovery. We now examine the identifiability of the causal graph: can the true causal structure be inferred from the joint distribution under our CPCM model? 
\begin{definition}[Identifiability]\label{DEFidentifiability}
Let $F_{(X_1, X_2)}$ be a distribution that has been generated according to the $CPCM(F_1,\dots, F_k)$ model with graph $X_1\to X_2$. We say that the causal graph is identifiable from the joint distribution (equivalently, that the model is identifiable) if there does \textit{not} exist 
$\tilde{\theta}$ and a pair of random variables $\tilde{\varepsilon}_2\indep\tilde{\varepsilon}_1$, where $\tilde{\varepsilon}_1$ is uniformly distributed, such that the model $X_2=\tilde{\varepsilon}_2,  X_1=  F_i^{-1}\big(\tilde{\varepsilon}_1;\tilde{\theta}(X_2)\big)$ for some $i\in\{1, \dots, k\}$ generates the same distribution $F_{(X_1,X_2)}$. 
\end{definition}

\subsection{Identifiability in $CPCM(F)$}

First, we discuss the Gaussian case. Recall that in the additive Gaussian model, where $X_2=f(X_1)+\varepsilon_2$, $\varepsilon_2\sim N(0, \sigma^2)$, the identifiability holds if and only if $f$ is non-linear \citep{hoyer2009}. We provide a different result with both mean \textit{and} variance as functions of the cause. A similar result is found in \cite[Theorem 1]{Khemakhem_autoregressive_flows} in the context of autoregressive flows and where only a sufficient condition for identifiability is provided. Another similar problem is studied in \cite{immer2022identifiability} and \cite{strobl2022identifying}, both of which discuss identifiability in general location-scale models. 

\begin{theorem}[Gaussian case]\label{normalidentifiability}
Let $(X_1,X_2)$ admit the $CPCM(F)$ model with graph $X_1\to X_2$, where $F$ is the Gaussian distribution function with parameters $\theta(X_1)=\big(\mu(X_1), \sigma(X_1)\big)^\top$.~\footnote{Just like in Example \ref{Gaussian case}, this can be rewritten as $X_2 = \mu(X_1)+\sigma(X_1)\varepsilon_2,  \text{  where }\varepsilon_2 \text{ is Gaussian}$.}

Let $p_{\varepsilon_1}$ be the density of $\varepsilon_1$, which is absolutely continuous with full support $\mathbb{R}$. Let $\mu(x), \sigma(x)$ be two times differentiable.  Then, the causal graph is identifiable from the joint distribution if and only if there do not exist $a,c,d,e, \alpha, \beta\in\mathbb{R}$,  
$a\geq 0,c>0, \beta>0$, such that
\begin{equation}\label{norm}
\frac{1}{\sigma^2(x)}=ax^2 + c, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \frac{\mu(x)}{\sigma^2(x)}=d+ex,
\end{equation}
for all $x\in\mathbb{R}$ and
\begin{equation}\label{DensityDEF}
p_{\varepsilon_1}(x) \propto \sigma(x)e^{-\frac{1}{2}\big[ \frac{(x-\alpha)^2}{\beta^2}  - \frac{\mu^2(x)}{\sigma^2(x)}\big]},
\end{equation}
where $\propto$  represents an equality up to a constant (here, $p_{\varepsilon_1} $  is a valid density function if and only if $\frac{1}{\beta^2}>  \frac{e^2}{c}\mathbbm{1}[a=0]$). 
Specifically, if $\sigma(x)$ is constant (case $a=0$), then the causal graph is identifiable, unless $\mu(x)$ is linear and $p_{\varepsilon_1}$ is the Gaussian density. 
\end{theorem}

The proof is provided in \hyperref[Proof of normalidentifiability]{Appendix} \ref{Proof of normalidentifiability}. Moreover, a visual example of an unidentifiable Gaussian model with $a=c=d=e=\alpha = \beta=1$ can be found in  \hyperref[Proof of normalidentifiability]{Appendix} \ref{Proof of normalidentifiability}, Figure \ref{GaussianDensity}. 

Theorem \ref{normalidentifiability} indicates that the non-identifiability holds only in the ``special case,'' when $\frac{\mu(x)}{\sigma^2(x)}, \frac{-1}{2\sigma^2(x)}$ are linear and quadratic, respectively. Note that natural parameters of a Gaussian distribution are  $\frac{\mu}{\sigma^2}, \frac{-1}{2\sigma^2}$, and sufficient statistics of the Gaussian distribution have a linear and quadratic form (for the definition of the exponential family, natural parameter and sufficient statistic, see \hyperref[appendix_exponential_family]{Appendix} \ref{appendix_exponential_family}). We show that such connections between non-identifiability and sufficient statistics hold in the more general context of the exponential family.  

\begin{proposition}[General case, one parameter]\label{Necessary condition for identifiability}
Let $q=1$. Let $(X_1, X_2)$ admit the $CPCM(F)$ model with graph $X_1\to X_2$, where $F$ lies in the exponential family of distributions with a sufficient statistic $T$. The causal graph is identifiable if at least one of the following conditions is met:
\begin{enumerate}
\item Supports of $X_1$ and $X_2$ differ. 
    \item There do not exist $a,b\in\mathbb{R}$, such that
 \begin{equation}\label{eq000}
     \theta(x)=a\, T(x)+b,\,\,\,\,\,\,\,\,\,\,\,\,\, \forall x\in supp(X_1). 
 \end{equation} 
 \item There does not exist $c\in\mathbb{R}$, such that 
\begin{equation}\label{eq007}
p_{X_1}(x)\propto \frac{h_1(x)}{h_2[\theta(x)]}e^{cT(x)}, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \forall x\in supp(X_1),
\end{equation}
where $h_1$ is a base measure of $F$ and $h_2$ is the normalizing function of $F$ defined in \hyperref[appendix_exponential_family]{Appendix} \ref{appendix_exponential_family}. 
\end{enumerate}
\end{proposition}
\begin{hproof} While this proposition follows as a special case of Theorem~\ref{thmAssymetricMultivariatesufficient}, we outline the key ideas behind the proof.

If the graph is \textit{not} identifiable, there exists a function $ \tilde{\theta}$, such that 
causal models $X_1 = \varepsilon_1, X_2 = F^{-1}\big(\varepsilon_2; \theta(X_1)\big)$, and $X_2 = \varepsilon_2, X_1 = F^{-1}\big(\varepsilon_1; \tilde{\theta}(X_2)\big)$ generate the same joint distribution.

\textbf{1)} If the supports of $X_1$ and $X_2$ differ, then $X_1$ trivially can not be written as $X_1 = F^{-1}\big(\varepsilon_1; \tilde{\theta}(X_2)\big)$  since the support of any distribution in the exponential family is fixed. This immediately rules out non-identifiability in such cases. At the population level, even a difference on a measure-zero set is sufficient to ensure identifiability.

\textbf{2) } Assuming the causal graph is not identifiable, we decompose the joint density:
\begin{equation}\label{eq69}
  p_{(X_1, X_2)}(x,y) = p_{X_1}(x)p_{X_2\mid {X_1}}(y\mid x) = p_{X_2}(y)p_{{X_1}\mid {X_2}}(x\mid y),\,\,\,\,\,\,\,\, x,y\in supp(X_1).
 \end{equation}
Since $F$ belongs to the exponential family, we rewrite the conditional densities using notation from \eqref{Exponential family of distributions}:$$p_{X_2\mid {X_1}}(y\mid x) = h_{1}(y)h_{2}[\theta(x)]\exp[\theta(x)T(y)], \,\,\,\,\,\,p_{{X_1}\mid {X_2}}(x\mid y) = h_{1}(x)h_{2}[\tilde{\theta}(y)]\exp[\tilde{\theta}(y)T(x)].$$
Substituting this into \eqref{eq69} gives:
\begin{equation}\label{eq098}
    \begin{split}
  &   p_{X_1}(x)h_{1}(y)h_{2}[\theta(x)]\exp[\theta(x)T(y)] =p_{X_2}(y) h_{1}(x)h_{2}[\tilde{\theta}(y)]\exp[\tilde{\theta}(y)T(x)],\\&
\underbrace{\log\bigg\{  \frac{ p_{X_1}(x)h_{2}[\theta(x)]}{h_{1}(x)}\bigg\}}_{f(x)}+ \underbrace{\log\bigg\{\frac{h_{1}(y)}{ p_{X_2}(y)h_{2}[\tilde{\theta}(y)]}\bigg\}}_{g(y)} = \tilde{\theta}(y)T(x)-\theta(x)T(y),
\end{split}
\end{equation}
where the second equation follows from taking the logarithm of both sides after dividing by  $h_1$ and $h_2$. Differentiating both sides with respect to $x$ and $y$, and fixing $y$ such that $T'(y)\neq 0$, we obtain $\theta'(x) = \frac{\tilde{\theta}'(y)}{T'(y)}T'(x)$. Integrating this equation with respect to $x$ leads to (\ref{eq000}). Note that we do not need to assume differentiability of $\theta$, and we can get around it by applying Lemma \ref{PomocnaLemma1}.

\textbf{3) } equality (\ref{eq098}) implies 
$f(x) + g(y) = a_1T(x) + a_2T(y) +a_3$ for some constants $a_1, a_2, a_3\in\mathbb{R}$. Therefore, fixing $y$ yields $f(x)=\log\bigg\{  \frac{ p_{X_1}(x)h_{2}[\theta(x)]}{h_{1}(x)}\bigg\} = a_1T(x) + const$. Rewriting this directly yields (\ref{eq007}). 
\end{hproof}

Intuitively, condition~\eqref{eq000} rules out joint distributions that are symmetric with respect to the transformed axis $aT(x)+b$, similar to how symmetry around $y = a+bx$ causes non-identifiability in the Gaussian ANM case \citep{Zhang2009}.
In the $CPCM(F)$ setting, the parameter $\theta(x)$ affects the distribution through the sufficient statistic $T(x)$.
If $\theta(x)$ takes the form $T(x)$ (or $aT(x)+b$, since a sufficient statistic is defined only up to an affine transformation), the joint law has this symmetry, making both causal directions consistent with the data.
By violating \eqref{eq000}, the symmetry is broken and the true causal direction becomes identifiable.


As a consequence of Proposition \ref{Necessary condition for identifiability}, we extend the results demonstrated by \cite{ParkPoisson} and \cite{ParkGHD} for a Poisson DAG model. These authors established the identifiability of a Poisson DAG model, where all variables (including source variables) given their parents follow a Poisson distribution. We present an analogous result, relaxing the restriction on the source variables.

\begin{consequence}\label{paretoidentifiability}
\begin{itemize}
    \item Let $(X_1,X_2)$ admit the $CPCM(F)$ model with graph $X_1\to X_2$, where $F$ is the Poisson distribution function with rate $\lambda$.  Then, the causal graph is \textit{not} identifiable if and only if
\begin{equation*}
 \lambda(x) =e^{ax+b},\,\,\,\,\,\,\,\, P(X_1=x) \propto \frac{e^{ \lambda(x)+cx}}{x! }, \,\,\,\,\,\,\,\,\forall x\in \{0,1,2, \dots \},
 \end{equation*}
for some $a<0,b,c\in\mathbb{R}$.
    \item Let $(X_1,X_2)$ admit the $CPCM(F)$ model with graph $X_1\to X_2$, where $F$ is the Pareto distribution function. Then, the causal graph is \textit{not} identifiable if and only if 
\begin{equation*}
\theta(x) = a\log(x) +b,\,\,\,\,\,\,\,\, p_{X_1}(x) \propto \frac{1}{ [a\log(x)+b] x^{c+1} }, \,\,\,\,\,\,\,\,\forall x\geq 1,
\end{equation*}
for some $a,b,c>0$. 

    \item Let $(X_1,X_2)$ admit the $CPCM(F)$ model with graph $X_1\to X_2$, where $F$ is Bernoulli distribution function. Then, the causal graph is identifiable if and only if $supp(X_1) \neq supp(X_2) $.
\end{itemize}
\end{consequence}
The proof is provided in \hyperref[Proof of pareto identifiability]{Appendix} \ref{Proof of pareto identifiability}, together with definitions of the distribution functions. Note that if $a=0$, then $X_1\indep X_2$ and the (empty) graph is trivially identifiable. 

Note that in the first two bullet points of Consequence~\ref{paretoidentifiability}, we have three free parameters: \(a\), \(b\), and \(c\). The non-identifiability of the graph in a Bernoulli model arises from the fact that the joint distribution of \((X_1, X_2)\) can be fully characterized by only three parameters.

\subsection{Identifiability in $CPCM(F_1, \dots, F_k)$ models}

We generalize Proposition \ref{Necessary condition for identifiability}  to the general case. The following theorem establishes that $CPCM(F_1, \dots, F_k)$ models are ``typically'' identifiable, except for a finite-dimensional set within the space of all possible distributions. 


\begin{theorem}
\label{thmAssymetricMultivariatesufficient}
Let $(X_1, X_2)$ follow the $CPCM(F_1, \dots, F_k)$ model with graph $X_1\to X_2$, where $F_1, \dots, F_k$ belong to the exponential family of distributions with corresponding sufficient statistics  
$T_m= (T_{m,1}, \dots, T_{m,q_m})^\top$, $m=1, \dots, k$.  Following Definition~\ref{CPCM(F1F2)}, 
let $\tilde{m}\in\{1, \dots, k\}$ be the index such that  
$X_2 = F_{\tilde{m}}^{-1}\big(\varepsilon_2; 
\theta_2(X_1)\big)$. 

The causal graph is identifiable if for all $m\in \{1, \dots, k\}$, at least one of the following holds: 
\begin{itemize}
    \item $ supp(F_m) \neq supp(X_1)$. 
    \item The function \( \theta_2 \) is not a linear combination of the sufficient statistics \( T_{m,1}, \dots, T_{m,q_m} \), i.e., there do not exist coefficients \( a_{i,j}, b_i \in \mathbb{R} \) for \( i = 1, \dots, q_{\tilde{m}} \) and \( j = 1, \dots, q_m \) such that  
   \begin{equation}\label{eq158}
   \theta_{2,i}(x) = \sum_{j=1}^{q_m} a_{i,j} T_{m,j}(x) + b_i, \quad \forall x \in \operatorname{supp}(X_1), \quad \forall i \in \{1, \dots, q_{\tilde{m}}\}.
   \end{equation}  
    \item There do not exist constants \( c_1, \dots, c_{q_m} \in \mathbb{R} \) such that the density of \( X_1 \) satisfies  
   \begin{equation}\label{eq007v2}
  p_{X_1}(x) \propto \frac{h_{m,1}(x)}{h_{\tilde{m},2}[\theta_2(x)]} e^{\sum_{i=1}^{q_m} c_i T_{m,i}(x) }, \quad \forall x \in \operatorname{supp}(X_1),
   \end{equation}  where \( h_{m,1} \) is a base measure associated with \( F_{m} \) and \( h_{\tilde{m},2} \) is the normalizing function of \( F_{\tilde{m}} \), both defined in \hyperref[appendix_exponential_family]{Appendix} \ref{appendix_exponential_family}. 
\end{itemize}
Consequentially, the space of non-identifiable distributions is contained in a $\tilde{d}$-dimensional space, where 
\begin{equation}\label{dimension_in_theorem2}
    \tilde{d} \leq \sum_{m\in\{1, \dots, k\}:  supp(F_m) = supp(X_1)} (q_m+1)(q_{\tilde{m}}+1) -1 .  
\end{equation}

\end{theorem}

The proof is provided in \hyperref[Proof of thmAssymetricMultivariatesufficient]{Appendix} \ref{Proof of thmAssymetricMultivariatesufficient}. It is insightful to examine the dimension of the space of unidentifiable distributions for different choices of \( F \).  In one-parameter cases, such as in Consequence~\ref{paretoidentifiability}, the set of all unidentifiable distributions lies within a three-dimensional space, similar to the case for ANM \cite[Proposition 21]{Peters2014}. For the Gaussian \( CPCM(F) \) model, Theorem~\ref{normalidentifiability} shows that this dimension is \( 6 \), despite Theorem~\ref{thmAssymetricMultivariatesufficient} initially suggesting \( (2+1)(2+1) -1 = 8 \). In the proof of Theorem~\ref{normalidentifiability}, we showed that two of these coefficients must be zero, confirming that \eqref{dimension_in_theorem2} provides only an upper boundâ€”the actual dimension can be smaller.  

Since the space of all distributions is infinite-dimensional (assuming infinite support), one can argue that identifiability holds for ``most distributions,'' regardless of the choice of \( F \). However, when \( F \) has many parameters, the model often lies ``close'' to an unidentifiable case, making the finite sample inference significantly more challenging.  

\begin{consequence}\label{consequenceprva}
Suppose that \( \text{supp}(X_1) = \mathbb{R} \), \(\text{supp}(X_2) = \{0, 1, \dots\}\) such as on Figure~\ref{Asymmetrical_picture}, and let \((X_1, X_2)\) admit the \(CPCM(F_1, F_2)\) model with graph \(X_1 \to X_2\), where \(F_1\) is a Gaussian distribution and \(F_2\) is a Poisson distribution with rate parameter \(\lambda\). The causal graph is identifiable if and only if there do not exist constants \(a_1, a_2, b, c_1, c_2\in \mathbb{R}\), $a_1, c_1<0$, such that:
\begin{equation*}
\lambda(x) = e^{a_1 x^2 +a_2x + b}, \quad p_{X_1}(x) \propto e^{c_1 x^2 + c_2 x },\quad\quad \forall x \in \mathbb{R}.
\end{equation*}
Details and more examples are provided in  \hyperref[consequence]{Appendix} \ref{consequence}.  
\end{consequence}







































