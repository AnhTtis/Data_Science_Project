\section{Introduction}
\label{introduction}

Having knowledge regarding causal relationships rather than statistical associations enables us to predict the effects of actions that perturb an observed system \citep{TheBookOfWhy}. Determining causal structures is a fundamental problem in numerous scientific fields \citep{Rubin}. However, different data-generating processes can produce the same observational distribution. Observing the system after interventions is a reliable means to identify the causal structure, but in numerous real-life scenarios, interventions can be too expensive, unethical \citep{epidem_application}, or impossible to observe. Hence, estimating the causal structure from observational data has become an important topic.

In the last few decades, much effort has been put into developing a  mathematical background for a ``language'' of causal inference, centered around the structural causal model (SCM) \citep{Pearl_book}. Given a set of random variables $\textbf{X} = (X_1, \dots, X_d)^\top\in\mathbb{R}^d$, a SCM with an underlying acyclic graph $\mathcal{G}$ represents a data-generating process where the variables arise from structural equations 
\begin{equation*}
    X_i=f_i(\textbf{X}_{pa_i}, \varepsilon_i),\,\,\,\,\,\,\,\,\, \,\,\,\,\,\,\,i=1,\dots, d, 
\end{equation*}
where $f_i$ are the causal (link) functions, $pa_i$ represent the parents (direct causes) of $X_i$ in $\mathcal{G}$, and $\varepsilon_i$ are jointly independent noise variables. The fundamental goal of causal discovery is to estimate the causal structure (causal graph $\mathcal{G}$).  It is straightforward to compute the distribution of $\textbf{X}$ if the graph $\mathcal{G}$ and the conditional densities are given. Here, we deal with the opposite problem, where the distribution of $\textbf{X}$ (or a random sample) is given and we want to infer $\mathcal{G}$. This task is typically impossible without strong assumptions on the SCM \citep{Elements_of_Causal_Inference}. 

The existing literature in the field presents numerous methods and corresponding results for causal discovery under various assumptions on the Structural Causal Model (SCM) \citep{ZhangReview}. When observing multiple environments following different interventions, the assumptions can be significantly less restrictive \citep{Peters_invariance, Multiple_contexts_Mooij}. However, if the goal is to uncover causal relationships based solely on an observed random sample, the assumptions become more strict; typically assuming additive noise \citep{Lingam, Peters2014, reviewANMMooij}. This assumption of additivity $X_i = f_i(\textbf{X}_{pa_i}) + \varepsilon_i$ suggests that $\textbf{X}_{pa_i}$ influences only the mean of $X_i$, while the tail, variance, and higher moments remain fixed. This is a strong assumption, as the tail or other characteristics of the random variable can provide different information about the causal structure.

In this paper, we develop a framework where $\textbf{X}_{pa_i}$ can arbitrarily affect the mean, variance, tail, or other characteristics of $X_i$. However, a caution has to be taken because if the model is too general, the causal structure will become unidentifiable, meaning that multiple causal structures could produce the same distribution of $\textbf{X}$.
\begin{example}\label{Gaussian case}
    A useful model that allows an arbitrary effect on the variance as well as on the mean is $X_i\mid \textbf{X}_{pa_i}\sim N\big(\mu(\textbf{X}_{pa_i}), \sigma^2(\textbf{X}_{pa_i})\big)$ for some functions $\mu, \sigma$, in which case the structural equation has the form 
\begin{equation*}\label{prva_gaussian_equation}
X_i =  \mu(\textbf{X}_{pa_i})+ \sigma(\textbf{X}_{pa_i})\, \varepsilon_i, \,\,\,\,\,\,\,\,\,\,\varepsilon_i\text{ is Gaussian.}
\end{equation*}
\end{example}

\begin{example}\label{example_Poisson}
In certain applications, it may be reasonable to assume
$$X_i\mid \textbf{X}_{pa_i}\sim Poisson\big(\theta(\textbf{X}_{pa_i})\big),$$where $\theta$ is a function describing the rate of certain phenomena. Such a model is common in applications when $X_i$ represents a number of events occurring in a certain time period. 
\end{example}

We introduce a causal model (we call it the conditionally parametric causal model or CPCM) where the structural equation has the following form: 
\begin{equation}\label{11}\begin{split}
&X_i=f_i(\textbf{X}_{pa_i}, \varepsilon_i) = F^{-1}\big(\varepsilon_i; \theta(\textbf{X}_{pa_i})\big),\,\,\,\,\,\,\varepsilon_i\sim U(0,1), \\&   
\,\,\,\,\,\,\,\,\,\,\,\,\text{ or equivalently } X_i\mid \textbf{X}_{pa_i}\sim F\big(\theta(\textbf{X}_{pa_i})\big), \end{split}
\end{equation}
where $F$ is a known distribution function with a vector of parameters $\theta(\textbf{X}_{pa_i})$. 

\subsection{Setup and notation}
\label{Setup}

We adapt the usual notation of graphical models  (e.g., \citealp{PCalgorithm}). We consider a DAG (directed acyclic graph) $\mathcal{G}=(V,E)$ with a finite set of vertices (nodes) $V=\{1, \dots, d\}$ and a set of directed edges $E$, and write $pa_i(\mathcal{G})$, $ch_i(\mathcal{G})$ and $an_i(\mathcal{G})$ for parents, children and ancestors of the node $i$, respectively. In addition, we say that the node $i\in V$ is a source node if $pa_j(\mathcal{G})=\emptyset$, notation $i\in Source(\mathcal{G})$. Given a random vector $\textbf{X }= (X_i)_{i\in V}$ over some probability space with distribution $F_\textbf{X}$, we identify the vertices $j \in V$ with the variables $X_j$.  We omit the argument $\mathcal{G}$ if evident from the context.  


We frequently use the concept of an exponential family, which is a class of probability distributions whose probability density function can be expressed as: 
\begin{equation}\label{Exponential family of distributions}
p(x;\theta) = h_1(x)h_2(\theta)e^{\sum_{i=1}^q\theta_iT_i(x)},
\end{equation}
where $h_1, h_2, T_i$ are measurable functions. We call $T_i$ a \textit{sufficient} statistic, $h_1$ a base measure, and $h_2$ a normalizing function. Note that $T_i$ are only unique up to a linear transformation.   Many well-known distribution families belong to the exponential family, including the Gaussian, Poisson, Binomial, and Gamma distributions. We assume that $q$ is minimal in the sense that we cannot write $p(x;\theta)$ using only $q-1$ parameters; see Appendix~\ref{appendix_exponential_family} that provides more information and detailed description. 

We use capital $F$ for distributions and small $p$ for densities. A random variable $Z$ that is uniformly distributed on $(0,1)$ is denoted as $Z\sim U(0,1)$. Support of a random variable $Z$ is denoted as $supp(Z)$.  We denote a random vector $\textbf{X}_S = \{X_s{:}\,\, s\in S\}$ for $S\subseteq V$. 

%The SCM uniquely defines a distribution of $\textbf{X}$ that can be decomposed into a product of conditional densities or causal Markov kernels \citep[Definition 6.21]{Elements_of_Causal_Inference}
%\begin{equation}%\label{def987}
%p_\textbf{X}(\textbf{x}) = \prod_{i\in V} p_i(x_i\mid \textbf{x}_{pa_i}),
%\end{equation}
%where $p_i(\cdot \mid \textbf{x}_{pa_i})$ represents the conditional density function of the random variable $X_i$ conditioned on its parents $\textbf{X}_{pa_j}=\textbf{x}_{pa_j}$. 
%A core concept in causal inference is the identifiability of the causal structure. It is straightforward to compute $F_{\textbf{X}}$ if the underlying DAG and the Markov kernels \eqref{def987} are given. However, we deal with the opposite problem, where $F_{\textbf{X}}$ is given (or a random sample from $F_{\textbf{X}}$) and we want to infer the DAG. This is generally not possible without further assumptions, as we can only identify the Markov equivalence class \citep[Proposition 7.1]{Elements_of_Causal_Inference}. More formal definition of identifiability is provided in Section 3. 

%Consider a SCM with conditional densities that satisfy (\ref{def987}). 
%Let  $\mathcal{G}\in DAG(d)$, where $DAG(d)$ is the set of all DAGs over $V=\{1, \dots, d\}$ and assume that $p_i\in\mathcal{H}_i$, where $\mathcal{H}_i$ is the subset of all conditional density functions. Let $\mathcal{H} = \mathcal{H}_1\times \dots \times \mathcal{H}_d$. The given pair $(\mathcal{G}, p)\in DAG(d)\times \mathcal{H}$ generates the density $p_p(\textbf{x}) = \prod_{i\in V} p_i(x_i\mid \textbf{x}_{pa_i})$.  

%\begin{definition}\label{IdentifiabilityPrvaDefinicia}
%We say that the pair $(\mathcal{G}, p)$ is identifiable in $DAG(d)\times \mathcal{H}$ if there does \textit{not} exist a pair $(\mathcal{G}', p')\in DAG(d)\times \mathcal{H}$,  $\mathcal{G}'\neq \mathcal{G}$, such that $p_p= p_{p'}$. We say that the class $\mathcal{H}$ is identifiable over $DAG(d)$ if every  pair  $(\mathcal{G}, p)\in DAG(d)\times \mathcal{H}$ is identifiable in $DAG(d)\times \mathcal{H}$.  
%\end{definition}

%Without strong restrictions of class $\mathcal{H}$, we cannot obtain the identifiability of $\mathcal{G}$. 


\subsection{Related work}

Many papers address the problem of the identifiability of the causal structure (for a review, see \cite{ZhangReview}). 
\cite{Lingam} show identifiability for the linear non-Gaussian additive models (LiNGaM), where $X_i=\beta\textbf{X}_{pa_i} +\varepsilon_i$ for non-Gaussian noise variables $\varepsilon_i$. 
\cite{BuhlmannCAM} explore causal additive models (CAM) of the form $X_i = \sum_{j\in pa_i} g_j(X_j) +  \varepsilon_i$ for smooth functions $g_j$.  
\cite{hoyer2009} and \cite{Peters2014} develop a framework for additive noise models (ANM), where $X_i = g(\textbf{X}_{pa_i}) +  \varepsilon_i$. Under certain (not too restrictive) conditions on $g$, the authors show the identifiability of such models  \citep[Corollary 31]{Peters2014} and propose an algorithm estimating $\mathcal{G}$ (for a review on ANM, see \cite{reviewANMMooij}). 
All these frameworks assume that the variance of $X_i\mid \textbf{X}_{pa_i}$ does not depend on $\textbf{X}_{pa_i}$. This is a crucial aspect of the identifiability results.

\cite{Zhang2009} introduce a generalization known as the post-nonlinear model, defined by $
X_i = g_1\big(g_2(\textbf{X}_{pa_i}) +  \varepsilon_i\big),
$
with an invertible link function $g_1$.  
 \cite{ParkPoisson, ParkVariance} reveal identifiability in discrete models in which  $var[X_i\mid \textbf{X}_{pa_i}]$  is a quadratic function of $\mathbb{E}[X_i\mid \textbf{X}_{pa_i}]$. If $X_i\mid \textbf{X}_{pa_i}$ has a Poisson or binomial distribution, such a condition is satisfied.  They also provide an algorithm based on comparing dispersions for estimating a DAG in polynomial time. Other algorithms have also been proposed, with comparable speed and different assumptions on the conditional densities \citep{PolynomialTimeAlgorithmCausalGraphs}. \cite{Galanti} consider the neural SCM with representation $X_i = g_1\big(g_2(\textbf{X}_{pa_i}), \varepsilon_i\big),$ where $g_1$ and $g_2$ are assumed to be neural networks. 

Recently, location-scale models of the form 
\( X_i = g_1(\textbf{X}_{pa_i}) +  g_2(\textbf{X}_{pa_i})\varepsilon_i \)
have garnered attention. \cite{immer2022identifiability} demonstrated that bivariate non-identifiable location-scale models must satisfy a specific differential equation. \cite{strobl2022identifying} explored the problem of estimating patient-specific root causes in location-scale models. Additionally, \cite{Khemakhem_autoregressive_flows} provided more detailed identifiability results under Gaussian noise $\varepsilon_i$ in the bivariate case using autoregressive flows. \cite{xu2022inferring} investigated a more restricted location-scale model, dividing the range of the predictor variable into a finite set of bins and fitting an additive model in each bin.

Further, several different algorithms for estimating causal graphs have been proposed, working with different assumptions \citep{IGCI, Score-based_causal_learning, Slope, Natasa_Tagasovska}.  They are often based on Kolmogorov complexity or independence between certain functions in a deterministic scenario. 

Constraint-based methods, like the PC and FCI algorithms \citep{PCalgorithm, FCI}, are considered a gold standard for causal discovery. They utilize sequential independence testing for causal discovery, consistently estimating the Markov equivalence class. While these methods are powerful, they rely heavily on the accuracy of the conditional independence tests, making them sensitive to statistical errors and often resulting in many edges remaining unoriented.

A few authors assume that causal Markov kernels lie in a parametric family of distributions. \cite{JanzingSecondOrderExponentialModels} consider the case in which the density of  $X_i\mid \textbf{X}_{pa_i}$  lies in a second-order exponential family and the variables are a mixture of discrete and continuous random variables. \cite{ParkGHD} concentrate on a specific subclass of model (\ref{11}), where $F$ lies in a discrete family of generalized hypergeometric distributions---that is, the family of random variables in which the mean and variance have a polynomial relationship. To the best of our knowledge, there does not exist any study in the literature, that provides identifiability results in the case in which $F$ lies in a general class of the exponential family. This is the focus of this paper. 

\textbf{The structure of the paper is as follows.} Section \ref{Section2} introduces the main definitions and motivation in a bivariate case. Section \ref{Section_identifiability} presents identifiability results for the causal structure in the bivariate case, and Section \ref{Section4} discusses the multivariate extension. In Section \ref{Section5}, we propose an algorithm for estimating the causal graph under assumption (\ref{11}). Section \ref{simulations_section} contains an extensive simulation study. We provide three appendices: Appendix \ref{Appendix_A} includes formal definition of Exponential family and some omitted technical content; Appendix \ref{Appendix_simulations} details the experiments and Appendix \ref{SectionProofs} contains all proofs.














































































































