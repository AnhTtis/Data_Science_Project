\section{Bivariate Conditionally Parametric Causal Models}
\label{Section2}

We focus on the bivariate SCM in this section, with multivariate extensions in Section~\ref{Section4}. The following definition describes the restriction on the SCM, assuming \(X_2 \mid X_1\) has the conditional distribution \(F\) with parameters \(\theta(X_1) \in \mathbb{R}^q\) for some \(q \in \mathbb{N}\).



\begin{definition}
We define the bivariate \textbf{conditionally parametric causal model} (bivariate $CPCM(F)$) with graph \(X_1 \to X_2\) by two assignments:
\begin{equation}\label{BCPCM}
X_1 = \varepsilon_1, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,X_2 =  F^{-1}\big(\varepsilon_2; \theta(X_1)\big),
\end{equation}
where \(\varepsilon_1 \indep \varepsilon_2\) are noise variables, \(\varepsilon_2\) is uniformly distributed, and \(F^{-1}\) is the quantile function of a distribution function $F$ with \(q\) parameters \(\theta(X_1) = \big(\theta_1(X_1), \dots, \theta_q(X_1)\big)^\top\).

We assume that \(\theta_i\) represent measurable functions, where at least one of the functions $\theta_1, \dots, \theta_q$ is non-constant on the support of \(X_1\). 
\end{definition}

We impose no restrictions on the marginal distribution of the cause. Note that we implicitly assume causal minimality \citep{zhang2010intervention}, as we assume that \(\theta\) is non-constant.

The Gaussian model introduced in Example~\ref{Gaussian case}, defined by 
\[X_2 = \mu(X_1) + \sigma(X_1) \, \varepsilon_2, \quad \varepsilon_2 \text{ is Gaussian},\]
is equivalent to the $CPCM(F)$ model with \(F\) being the Gaussian distribution and \(\theta(X_1) = (\mu(X_1), \sigma(X_1))^\top\). 

\subsection{\(CPCM(F_1, F_2, \dots, F_k)\) Models}

\subsubsection{Motivation}
\label{motivation_section}
Occam's razor posit that \(F_{\text{effect} \mid \text{cause}}\) should be ``simpler'' than \(F_{\text{cause} \mid \text{effect}}\). In model-based approaches for causal discovery, we define a ``simple distribution'' as one that belongs to a pre-defined class of distributions \(\mathcal{F}\). In ANM, \(\mathcal{F}\) consists of all distributions that can be expressed as the sum of a function of the cause and a noise term. In CPCM, \(\mathcal{F}\) is a given parametric family of  distributions. If \(\mathcal{F}\) is sufficiently small, we achieve identifiability of the causal graph \(\mathcal{G}\) because both \(F_{\text{effect} \mid \text{cause}}\) and \(F_{\text{cause} \mid \text{effect}}\) cannot lie in \(\mathcal{F}\).

However, the choice of \(\mathcal{F}\) is crucial, especially when dealing with mixtures of discrete and continuous distributions. Suppose we observe data as shown in Figure~\ref{Asymmetrical_picture}. To handle such cases, \(\mathcal{F}\) needs to include both continuous and discrete distributions. If we define \(\mathcal{F}\) as a class of \(CPCM(F)\) with continuous \(F\) (e.g., Gaussian), then \(F_{X_2 \mid X_1}\) can never lie in \(\mathcal{F}\). Conversely, choosing discrete \(F\) leads to \(F_{X_1 \mid X_2} \not\in \mathcal{F}\).

To accommodate a wide range of applications with various conditional distributions, we define \(\mathcal{F}\) as the union of \(CPCM(F_1), \dots, CPCM(F_k)\) models. By selecting \(F_1, \dots, F_k\) as a collection of ``standard simple well-known distributions'' (such as Gaussian, Gamma, Poisson, etc., see Section~\ref{Section5Model_choice}), \(\mathcal{F}\) is composed of ``standard simple (conditional) distributions'' with a wide range of possible supports, forms, and properties. We refer to this as the \(CPCM(F_1, F_2, \dots, F_k)\) model.


\begin{figure}[ht]
\centering
\includegraphics[scale=0.7]{figures/Gauss_Poisson.pdf}
\caption{A dataset generated as follows: \(X_1 \sim N(0,1)\), \(X_2 \sim \text{Poisson}(|X_1|)\). In other words, $X_1, X_2$ follow \(CPCM(F)\) with a Poisson \(F\) and DAG  \(X_1 \to X_2\).}
\label{Asymmetrical_picture}
\end{figure}


\subsubsection{Definition}\label{Section2.2.2}

\begin{definition}\label{CPCM(F1F2)}
Let \(F_1, \dots, F_k\) be a collection of distribution functions, each parameterized by a \(q_i\)-dimensional parameter, where \(q_i \in \mathbb{N}\) and \(i = 1, \dots, k\). A pair of dependent random variables \((X_1, X_2)\) follows the \(CPCM(F_1, F_2, \dots, F_k)\) model if there exists an \(i \in \{1, \dots, k\}\) such that \((X_1, X_2)\) follows a \(CPCM(F_i)\) model. Specifically, either:
\begin{equation*}
\begin{split}
 &  X_1 = \varepsilon_1, \quad X_2 = F_i^{-1}\big(\varepsilon_2; \theta_2(X_1)\big), \quad \varepsilon_2 \sim U(0,1), \quad \varepsilon_1 \indep \varepsilon_2, \quad \text{or} \\ 
 &  X_2 = \varepsilon_2, \quad X_1 = F_i^{-1}\big(\varepsilon_1; \theta_1(X_2)\big), \quad \varepsilon_1 \sim U(0,1), \quad \varepsilon_1 \indep \varepsilon_2,
\end{split}
\end{equation*}
for some \(i \in \{1, \dots, k\}\), where \(\theta_1(\cdot)\) and \(\theta_2(\cdot)\) are suitable parameter functions of dimension $q_i$, that are measurable non-constant.
\end{definition}
We can potentially combine other well-known models, such as ANM and discrete QVF models; however, we will focus solely on \(CPCM\) classes for the remainder of this paper. Note the distinction between ANM and CPCM models: the former assumes that \(\theta(X)\) corresponds to the mean, while the latter allows \(\theta(X)\) to represent any distributional characteristic; however, CPCM imposes additional assumptions on the noise.

Extending the definition of \(CPCM(F)\) to allow multiple data-generating mechanisms induces the risk of unidentifiability. If the class \(CPCM(F_1, \dots, F_k)\) is too large, both \(F_{\text{effect} \mid \text{cause}}\) and \(F_{\text{cause} \mid \text{effect}}\) may lie within it. In the following section, we show that this is typically not the case if \(F_1, \dots, F_k\) belong to the exponential family of distributions.