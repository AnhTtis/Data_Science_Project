\renewcommand\thesection{A}
\section{Exponential family and Proposition \ref{consistency_proposition}}
\label{Appendix_A}

\subsection{Exponential family}
\label{appendix_exponential_family}

The exponential family is a set of probability distributions whose probability density function can be expressed in the following form:
\begin{equation}\tag{\ref{Exponential family of distributions}}
f(x;\theta) = h_1(x)h_2(\theta)\exp\big[\sum_{i=1}^q\theta_iT_i(x)\big],
\end{equation}
where $h_1, T_i$ are real functions and $h_2:\mathbb{R}^q\to\mathbb{R}^+$ is a vector-valued function. We call $T_i$ a \textit{sufficient} statistic, $h_1$ a base measure, and $h_2$ a normalizing (or partition) function.  

Often, form (\ref{Exponential family of distributions}) is called a canonical form and $$f(x;\theta) = h_1(x)h_2(\theta)\exp\big[\sum_{i=1}^qh_{3,i}(\theta)T_i(x)\big],$$ where  $h_{3,i}:\mathbb{R}^q\to\mathbb{R}, i=1, \dots, q$, is called its \textit{reparametrization} (natural parameters are a specific form of the reparametrization). We always work only with a canonical form (attention for Gaussian distribution, where the standard form is not in the canonical form). 

Numerous important distributions lie in the exponential family of distributions, such as Gaussian, log-normal, Poisson, Pareto (with fixed support), Weibull, chi-squared, multinomial, Binomial, Gamma, and Beta distributions, to name a few. 

It is important to note that functions in (\ref{Exponential family of distributions}) are \textit{not} uniquely defined. For example, $T_i$ is unique up to a linear transformation. 

The support of $f$ is fixed and does not depend on $\theta$. Potentially, $T_i$ and $h_1$ do not have to be defined outside of this support; however, we typically overlook this fact (or possibly define $h_1(x) = T_i(x) = 0$ for $x$ where these functions are not defined). We additionally assume that the support is nontrivial in the sense that it contains at least two distinct values.





Without loss of generality, we assume that $q$ is minimal in the sense that $f(x; \theta)$ cannot be expressed using only $q - 1$ parameters. The sufficient statistics $T_1, \dots, T_q$ are then linearly independent in the following sense: there exist points $x_1, \dots, x_q \in \operatorname{supp}(f)$ such that the matrix 
\begin{equation}\label{eq2431087}
\begin{pmatrix}
T_{1}(x_1) & \cdots & T_{{q}}(x_1) \\
\cdots & \ddots & \cdots \\
T_{1}(x_q) & \cdots & T_{{q}}(x_{q}) 
\end{pmatrix} 
\end{equation}
has full rank. Moreover, $T_1, \dots, T_q$ are affinly independent in the following sense: there exist $y_0, y_1, \dots, y_q\in supp(f)$, such that a matrix 
\begin{equation}\label{eq145151}
\begin{pmatrix}
T_{1}(y_1) - T_1(y_0) & \cdots & T_{{q}}(y_1) -T_q(y_0)\\
\cdots & \ddots & \cdots \\
T_{1}(y_q) -T_1(y_0) & \cdots & T_{{q}}(y_{q}) -T_q(y_0)
\end{pmatrix} 
\end{equation}
has full rank. In this paper—specifically in Lemma~\ref{PomocnaLemma1}—we assume affine independence of $T_1, \dots, T_q$, i.e., that condition~\eqref{eq145151} holds.

Since the notions of linear and affine independence used here are nonstandard, we illustrate them with a simple example. Let $T_1(x) = x$ and $T_2(x) = x^2$, corresponding to the sufficient statistics of a Gaussian distribution. Then matrices \eqref{eq2431087} and \eqref{eq145151} become: 
\begin{equation*}
M_1=\begin{pmatrix}
x_1 &  x_1^2\\
x_2  & x_{2}^2
\end{pmatrix} , \,\,\,\,
M_2=\begin{pmatrix}
y_1-y_0 &  y_1^2 -y_0^2\\
y_2 -y_0 &  y_2^2 -y_0^2
\end{pmatrix} ,
\end{equation*}
both of which are full-rank for the choices $(x_1, x_2) = (1,2)$ and $(y_0, y_1, y_2) = (0,1,2)$, for instance. 

\subsection{F-suitability and Proposition~\ref{consistency_proposition} }
\label{Appendix_consistency}



\subsubsection{Suitability of an estimator}
In the following, we define an $F$-suitable estimator for a given distribution function $F$ with parameters $\theta$. This is a modification of the concept of a suitable estimator for the conditional expectation discussed in \cite[Appendix A.2]{reviewANMMooij}. In case when $F$ is location-distribution (such as Gaussian distribution with fixed variance), our results fully align with \citep{reviewANMMooij}. 


Let $(X_{i}, Y_{i})_{i=1}^n$ be a random sample from $(X, Y)$. We say that the estimator $\hat{\theta}$ is \textbf{F-suitable for $X \to Y$} if the following conditions are satisfied:

\begin{itemize}
    \item \textbf{(Existence of a point-wise limit)} There exists $\theta$ such that $\hat{\theta}(x) \overset{P}{\to} \theta(x)$ as $n\to\infty$ for all $x\in supp(X)$. Moreover, if it is possible to write  $Y = F^{-1}(\varepsilon_2; \tilde{\theta}(X))$, with $X\indep \varepsilon_2\sim Unif(0,1)$, then this limit is equal to $\theta= \tilde{\theta}$. 
    \item \textbf{(Weak residual consistency)} It holds that
\begin{equation}
    \label{gdert}
    \lim_{n\to\infty}\mathbb{E}_{}\left( \frac{1}{n}\sum_{i=1}^n(\varepsilon_i-\hat{\varepsilon}_i)^2 \right) = 0,
\end{equation}
where $\varepsilon_i := F(Y_{i}; \theta(X_{i}))$ and $\hat{\varepsilon}_i := F(Y_{i}; \hat{\theta}(X_{i})), i=1, \dots, n$ and where the expectation is taken with respect to the distribution of the random sample.
\end{itemize}

We say that the estimator $\hat{\theta}$ is \textbf{F-suitable} if it is F-suitable for both $X \to Y$ and $Y \to X$. We simply write that  $\hat{\theta}$ is ``suitable'' if $F$ is evident from the context. 

\subsubsection{Literature review - which estimators are suitable?}

\textbf{Location family:} when $F$ is location-family distribution, such as a Gaussian distribution with fixed variance, property \eqref{gdert} reduces to classical notion of a weak universal consistency:
\(
\lim_{n\to\infty}\mathbb{E}[||\theta(X) - \hat{\theta}(X)||^2] = 0.
\) Such consistency have been already discussed also in relation to causal discovery \citep{zhang2015estimation,uemura2022multivariate, keropyan2023rank}. 
The weak universal consistency has been established for various estimators under appropriate smoothness assumptions on $\theta$. Examples of such estimators include:
\begin{itemize}
    \item Kernel estimators (see Theorem 5.1 in \cite{Gyorfi2002})
    \item Smoothing spline GAM estimators (see Chapter 14.2 in \cite{Gyorfi2002}, \cite{claeskens2009} or \cite{Wood2})
    \item Neural networks (see Theorem 16.1 in \cite{Gyorfi2002} or \cite{drews2022universalconsistencyoverparametrizeddeep}).
\end{itemize}
Importantly, these consistency results apply regardless of the causal direction. In the anti-causal direction, we have \( X = \theta(Y) + \varepsilon \), where \( \theta(Y) = \mathbb{E}[X \mid Y] \) and \( \varepsilon \not\indep Y \), \( \mathbb{E}[\varepsilon \mid Y] = 0 \). Note that  \( \varepsilon \not\indep Y \) holds if and only if the model is identifiable. For such case, the same form of weak consistency remains valid for the estimators listed above (again, under appropriate smoothness assumptions).

\textbf{Location-scale family:} when $F$ is a location-scale distribution (e.g. Gaussian), several consistency results has also been established for various estimators under appropriate smoothness and regularity assumptions, see e.g. \cite{10.1093/biomet/85.3.645}, or \cite{immer2022identifiability, Siegfried02102023, Le_Smola}. 

\textbf{More general families:} Consistency results for non-Gaussian families are less explored in nonparametric settings, with most existing literature focusing on empirical evidence. GAMLSS \citep{GAMLSS} offers a broad class of estimators, with \cite{GAMLSS_webpage} presenting extensive empirical evidence on simulations and hundreds of real-data examples demonstrating consistency. A few theoretical results are avaliable for GAM estimators; e.g. Theorem 1 in \cite{GAMSamworthConsistency} establishes its almost sure universal consistency for a general one-dimensional exponential family of distributions for \( Y \mid X \), assuming certain smoothness and convexity/monotonicity conditions on \( \theta \). \cite{Wood2} discusses general framework for smoothing parameter estimation for models with regular likelihoods constructed in terms of unknown smooth functions of covariates.   \cite{mammen1997} discusses the consistency under partial linearity assumption.  


\subsubsection{HSIC score}

For the definition and discussion of the HSIC score, see \cite[Appendix A.1]{reviewANMMooij}. We use the same notation and implicitly use bounded non-negative Lipschitz-continuous kernels such that their product is characteristic, as in the "Data recycling" scenario in \cite[Corollary 21]{reviewANMMooij}.

\subsubsection{Proof of Proposition~{\ref{consistency_proposition}}}

\begin{customprop}{\ref{consistency_proposition}}
Let $(X_1, X_2)$ follow an identifiable $CPCM(F_1, \dots, F_k)$ with DAG $\mathcal{G}$. Then, our score-based algorithm presented in Section~\ref{Section_score_based_algorithm} is consistent, meaning that
$$\hat{\mathcal{G}} \overset{P}{\to}\mathcal{G}\,\,\,as\,\,n\to\infty,$$
given that we employ a “suitable” estimation procedure for the estimator  $\hat{\varepsilon}_i$, we use HSIC score as our choice of $\rho$ and consistent estimates of $S_i$. 
\end{customprop}


\begin{proof}
The proof mostly aligns with the proof of Corollary 21 in \cite{reviewANMMooij}. We use the notation $X = X_1$ and $Y = X_2$.

If $X \indep Y$, then $\hat{\mathcal{G}}$ converges to an empty graph, since any other graph has a score of at least $\lambda$ and $\hat{\varepsilon}_1, \dots, \hat{\varepsilon}_d$ are independent by definition. From now on, without loss of generality, let $Y = F_j^{-1}(\varepsilon_2; \theta(X)), \varepsilon_2\indep X$ for some  $j \in \{1, \dots, k\}$. Denote by $\mathbf{x} = (x_1, \dots, x_n)$ and $\mathbf{y} = (y_1, \dots, y_n)$ the observed data. In the following, we compare the asymptotic scores of graphs $X\to Y$ and $Y\to X$. 

\textbf{Graph} $X \to Y$: Define the ``population residual'' \(E_Y := F_j(Y; \theta(X))\) and the ``estimated residual'' $\hat{\varepsilon}_Y^i = (F_i(y_l; \hat{\theta}(x_l)))_{l=1}^n$, where $i=1, \dots, k$. For the true $i = j$, we omit the superscript and write $\hat{\varepsilon}_Y = (F_j(y_l; \hat{\theta}(x_l)))_{l=1}^n$. By construction, \(X \perp\!\!\!\perp E_Y\) which implies \(\text{HSIC}(X, E_Y) = 0\) (due to Lemma 12 in \cite{reviewANMMooij}). 

Now, since the estimator $\hat{\theta}$ satisfies \eqref{gdert}, we can use the argument presented in \cite[Theorem 20]{reviewANMMooij}, and obtain $\widehat{HSIC}(\mathbf{x}, \hat{\varepsilon}_y) \xrightarrow{P} \text{HSIC}(X, E_Y)$. 

Since $\hat{S}_2$ is consistent, we can find $n_0$ such that for all $n\geq n_0$ holds $P(j\in\hat{S}_2)\geq 1-\delta$ for given $\delta>0$. 

Putting everything together, with probability larger than $1-\delta$ holds

\begin{equation*}
    \begin{split}
        s(X \to Y) &= \min_{j_2 \in \hat{S}_2}\rho(\mathbf{x}, \hat{\varepsilon}_y^{j_2}) + \lambda \leq \rho(\mathbf{x}, \hat{\varepsilon}_y) + \lambda \\
        &= \widehat{HSIC}(\mathbf{x}, \hat{\varepsilon}_y) + \lambda \xrightarrow{P} \text{HSIC}(X, E_Y) + \lambda = \lambda.
    \end{split}
\end{equation*}
By sending $\delta\to 0$, we obtain $  s(X \to Y)\xrightarrow{P} \lambda $. 

\textbf{Graph} $Y \to X$: Define the ``population residual'' \(E_X^j := F_j(X; \theta_j(Y))\) and the ``estimated residual'' $\hat{\varepsilon}_X^j = (F_j(x_i; \hat{\theta}_j(y_i)))_{i=1}^n$, where $\theta_j$ is the limit of $\hat{\theta}_j$ defined in the definition of $F_j$-suitability. 

Due to the assumption of identifiability, $Y\not\indep E_X^j$ for all $j\in\{1, \dots, k\}$. Therefore, using Lemma 12 in \cite{reviewANMMooij}, we have \(\text{HSIC}(Y, E_X^j) > 0\). Again, since the estimator $\hat{\theta}$ satisfies \eqref{gdert}, we can use the argument presented in \cite[Theorem 20]{reviewANMMooij}, and obtain $\widehat{HSIC}(\mathbf{y}, \hat{\varepsilon}_X^j) \xrightarrow{P} \text{HSIC}(Y, E_X^j)$. 


Putting everything together
\begin{equation*}
    \begin{split}
        s(Y \to X) &= \min_{j \in \hat{S}_1}\rho(\mathbf{y}, \hat{\varepsilon}_x^j) + \lambda \geq \min_{j \in \{1, \dots, k\}}\widehat{HSIC}(\mathbf{y}, \hat{\varepsilon}_X^j) + \lambda \\
        &  \xrightarrow{P} \min_{j \in \{1, \dots, k\}}\text{HSIC}(Y, E_X^j) + \lambda > \lambda, \,\,\,\,\,\,\,\,\,\,\,\text{as}\,\,n\to\infty.
    \end{split}
\end{equation*}
Therefore, the score for the correct direction is asymptotically smaller than that for the wrong causal direction as $n \to \infty$, hence the procedure is consistent.
\end{proof}


\begin{customconsequence}{\ref{consequence_consistency}}
Let $(X_1, X_2)$ follow an $CPCM(F)$ model with DAG $\mathcal{G}$, for some $F\in\mathscr{S}_1\cup \mathscr{S}_2$. Assume the conditions of Proposition~\ref{consistency_proposition} hold: namely, identifiability of $CPCM(\mathscr{S}_1 \cup \mathscr{S}_2)$, a suitable estimation procedure, and the use of the HSIC score. Then, our score-based algorithm, with the collection $\{F_1, \dots, F_k\}$ chosen via the Sequential approach (Exact or Fast
version), is consistent:
$\hat{\mathcal{G}} \overset{P}{\to}\mathcal{G}$, as $n\to\infty$.
\end{customconsequence}


\begin{proof}

If $F \in \mathscr{S}_1$, the result follows directly from Proposition~\ref{consistency_proposition}. Similarly, if $F \in \mathscr{S}_2$ and the Sequential approach returns $\mathscr{S}_1 \cup \mathscr{S}_2$, the proposition again directly applies.

It remains to consider the case where $F \in \mathscr{S}_2$ but the Sequential approach returns only $\mathscr{S}_1$. We will show that, as $n \to \infty$, this event occurs with probability tending to zero. In other words, when $F \in \mathscr{S}_2$, the Sequential approach will return $\mathscr{S}_1 \cup \mathscr{S}_2$ with probability tending to one.


If $X \indep Y$, then $\hat{\mathcal{G}}$ converges to an empty graph regardless of the choices of $F$. Hence, without loss of generality, assume non-empty causal graph. Consider graph $\mathcal{G} = X_1 \to X_2$, and take some $F_1 \in \mathscr{S}_1$ (that is, wrong distribution $F_1\neq F$). In this case, following the notation of the proof of Proposition~\ref{consistency_proposition}, we have
\begin{equation}
\label{eq2345}
    \widehat{\text{HSIC}}(\mathbf{x}_1, \hat{\varepsilon}_{X_2}^{1}) \xrightarrow{P} \text{HSIC}(X_1, E_{X_2}^{1}) > 0,
\end{equation}
where $\hat{\varepsilon}_{X_2}^{j} := (F_j\big(X_2; \hat{\theta}_j(X_1)\big))_{l=1}^n$ is the vector of residuals obtained by applying a ``suitable'' estimation procedure, and the ``population residual'' \(E_{X_2}^{j} := F_j(X_2; \theta(X_1))\). This convergence follows from the same reasoning as in the `$Y \to X$' case in the proof of Proposition~\ref{consistency_proposition}. 

In particular, \eqref{eq2345} implies that the variance of the empirical HSIC statistic vanishes:
\[
\operatorname{Var} \left( \widehat{\text{HSIC}}(\mathbf{x}_1, \hat{\varepsilon}_{X_2}^{1}) \right) \to 0 \quad \text{as } n \to \infty.
\]
Therefore, according to the construction of confidence intervals for the HSIC test in~\cite[Section~3.2.2]{Kernel_based_tests}, the $(1 - \alpha)$ confidence interval will eventually lie entirely within an interval of the form $[\mathrm{HSIC}(X_1, E_{X_2}^{1}) - \delta, \mathrm{HSIC}(X_1, E_{X_2}^{1}) + \delta]$, for arbitrarily small $\delta > 0$. Since the limiting HSIC value is strictly positive, this interval will exclude $0$ for large enough $n$, yielding p-values smaller than $\alpha$. As a result, the DAG $\mathcal{G}$ will be rejected as plausible.

The same argument applies to the case $\mathcal{G} = X_2 \to X_1$ and for any $F_j \in \mathscr{S}_1$, $j = 1, \dots, k$. Therefore, for sufficiently large $n$, no DAG will be deemed plausible, and the Sequential approach will return the full set $\mathscr{S}_1 \cup \mathscr{S}_2$ with high probability.
\end{proof}


\begin{lemma}[Consistency of Algorithm 1 under unidentifiability]
Let $(X_1, X_2)$ follow an \textit{\textbf{unidentifiable}} model $CPCM(F_1, \dots, F_k)$ with DAG $\mathcal{G}$. Suppose the HSIC independence test is used in step 1b) of Algorithm 1, and assume access to an oracle regression estimator $\hat\theta = \theta$. Then, for sufficiently large $n$, Algorithm 1 outputs “Unidentifiable case” with probability at least $1 - 2\alpha$.
\end{lemma}

\begin{proof}
If the true model is unidentifiable, independence holds in both directions: $X_1 \indep \varepsilon_2$ and $X_2 \indep \varepsilon_1$. Given an oracle regression estimator $\hat\theta = \theta$, we have $\hat\varepsilon_i = \varepsilon_i$ for $i = 1, 2$. Therefore, Algorithm 1 reduces to performing two HSIC independence tests and returning “Unidentifiable case” if both tests fail to reject the null.

The HSIC test has asymptotically correct level under the null hypothesis of independence (\cite{Kernel_based_tests}, Theorem 3.8). Thus, for sufficiently large sample size $n$, the probability that the HSIC test fails to reject the null in each direction is at least $1 - \alpha$. By the union bound, the probability that both tests fail to reject is at least $(1 - \alpha)^2 > 1 - 2\alpha$. Hence, Algorithm 1 returns “Unidentifiable case” with probability at least $1 - 2\alpha$.
\end{proof}
