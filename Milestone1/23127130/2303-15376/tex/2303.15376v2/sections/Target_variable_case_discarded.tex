


\subsection{IFCM and causal minimality}
\label{CausalMinimality}

We start with a definition of an invertible function. This refers to a property of a real function. 

\begin{definition}\label{I}
A function $g:\R^{n}\to \R$  is called \textbf{invertible for the last element}, notation $g\in \mathcal{I}$, if there exists a function $g^{\leftarrow}:\R^{n}\to \R$ that fulfills the following: $\forall \textbf{x}\in\R^{n-1}, \forall y,z\in\R$ such that $y=g(\textbf{x},z)$, then $z=g^{\leftarrow}(\textbf{x},y)$. 

Moreover, denote 
$
\mathcal{I}_{min} = \{g\in\mathcal{I} : \text{ g is not constant in any of its arguments} \}.
$
\end{definition}
The previous definition simply describes, that the last element $z$ in a relation $y=g(\textbf{x},z)$ can be uniquely recovered from $(\textbf{x},y)$. To provide an example, for a function $g(x,z) = x+z$ holds $g^{\leftarrow}(x, y) = y-x$, since $g^{\leftarrow}(x, g(x,z)) = g(x,z) - x = z$. Generally, if $g$ is differentiable and the partial derivative of $g(\textbf{x},z)$ with respect to $z$ is monotonic, then $g\in\mathcal{I}$ (follows from Inverse function theorem (\cite{Inverse_function_theorem})). 

In the context of causal models, we assume that the noise can be fully recovered from the observed covariates. Hence, we define an \textbf{invertible functional causal model} (IFCM) as an acyclic SCM, whose structural equations $f_j\in\mathcal{I}$ for all $j=1, \dots, p$.  Note that $f_j\in\mathcal{I}_{min}$ implies causal minimality of the IFCM model (the index $min$ in $\mathcal{I}_{min}$ represents the word "minimality". Informally, this assumption implies that every arrow in the causal graph is important and can not be omitted. 
\begin{lemma}
Consider a distribution generated by \hyperref[I]{IFCM} with graph $\mathcal{G}_0$ (defined in Section \hyperref[I]{2.1}). Let all structural equations $f_j\in\mathcal{I}_{min}$, $\forall j=1, \dots, p$.  Then, the distribution is causally minimal with respect to  $\mathcal{G}_0$. Conversely, if there exists  $f_j\in\mathcal{I}\setminus\mathcal{I}_{min}$, then the causal minimality is violated. 
\end{lemma}
\begin{proof}
The second claim follows directly from \citep[Proposition 4]{Peters2014}. For the first claim, we use similar approach as in \citep[Proposition 17]{Peters2014}. 

Let $f_j\in\mathcal{I}_{min}$ for all $j=1, \dots, p$ and let the causal minimality be violated, i.e. let $\mathcal{G}`$ be a subgraph of $\mathcal{G}_0$ such that the distribution is Markov wrt. $\mathcal{G}`$. Find $i,j\in\mathcal{G}_0$ such that $i\to j$ in $\mathcal{G}_0$ but $i\not\to j$ in $\mathcal{G}`$. 

In graph $\mathcal{G}_0$ we have a structural equation  $X_j = f_j(\textbf{X}_{pa_j(\mathcal{G}_0)}, \varepsilon_j) = f_j(X_i, \textbf{X}_{pa_j(\mathcal{G}_0)\setminus\{i\}}, \varepsilon_j)$  but in $\mathcal{G}`$ we have $X_j = f^{`}_j(\textbf{X}_{pa_j(\mathcal{G}_0)\setminus\{i\}}, \varepsilon_j)$. Hence, functions $f_j(X_i, \textbf{X}_{pa_j(\mathcal{G}_0)\setminus\{i\}}, \varepsilon_j)$ and $f_j( \textbf{X}_{pa_j(\mathcal{G}_0)\setminus\{i\}}, \varepsilon_j)$ has to be equal almost surely. Condition on $(X_i,\textbf{X}_{pa_j(\mathcal{G}_0)\setminus\{i\}})=(x_i,x)$. Then, in order for  functions $f_j(x_i, x, \varepsilon_j)$ and $f^{`}_j(x,\varepsilon_j)$ to be equal almost surely (stressing out that $\varepsilon_j\indep \textbf{X}_{pa_j}$), $f_j$ can not depend on its first argument. This is a contradiction with $f\in\mathcal{I}_{min}$. 
\end{proof}






\section{Causes of one variable of interest}

Often, we are not interested in estimating full causal graph $\mathcal{G}_0$. Instead, we are only interested in estimating the set of direct causes of $Y$, where $Y$ is a variable of interest and we observe covariates $\textbf{X}\in\mathbb{R}^p$. If we are able to estimate direct causes of $Y$, we can potentially repeat the procedure for other variables, if we have some information about their data-generating process as well. 

This is in particular an interesting case for our CPCM; in many practical applications, it is reasonable to assume a parametric form for $Y\mid \textbf{X}_{pa_Y}$. However, assuming a parametric form for every variable in the model may be too restrictive. 
The structural equation for the variable of interest $Y$ is in the form 
\begin{equation}\label{varOfInterest}
Y=f_Y(\textbf{X}_{pa_Y}, \varepsilon_Y) = F^{-1}(\varepsilon_Y;\theta(\textbf{X}_{pa_Y})), \varepsilon_Y\indep \textbf{X}_{pa_Y}, \varepsilon_Y\sim U(0,1),
\end{equation}
for some function $\theta$. The question remains: assuming \textit{only} (\ref{varOfInterest}) (and putting no restrictions on the covariates), can we derive some identifiability results only from observational data? The general answer is no; since other structural equations are unrestricted, appropriately chosen covariates can create an unidentifiable setup. However, in the following section, we develop a framework where we are (sometimes) able to recover at least a subset of the causal variables only from a random sample. 

Recall $\mathcal{I}_m$ from Definition \ref{I}. We denote  $\mathcal{F}\subseteq\mathcal{I}_m$ a functional space where the structural equation for $Y$ belongs to, i.e we assume that $Y=f_Y(\textbf{X}_{pa_Y}, \varepsilon_Y)$ for some $f_Y\in\mathcal{F}$. Functional space $\mathcal{F}$ corresponds to our assumptions that we are willing to make about the data generating process of $Y$.  To ease the notation and without loss of generality, we assume that $\varepsilon_Y\sim U(0,1)$ \footnote{Similarly as in Section 2. In $f_Y(\textbf{X}_{pa_Y}, \varepsilon) =f_Y(\textbf{X}_{pa_Y}, g^{-1}(\varepsilon_Y))$, where $\varepsilon_Y\sim U(0,1)$, it is equivalent to infer the distribution of $\varepsilon$ and a quantile function $g^{-1}$.}. If we assume linearity, this represents the assumption $f_Y\in\mathcal{F}_L$, where \begin{equation}\label{mathcalF_L}
\mathcal{F}_L = \{f\in\mathcal{I}_m: f(\textbf{x}, \varepsilon) = \beta^T\textbf{x} + q^{-1}(\varepsilon) \text{ for some quantile function }q^{-1}(\cdot)\}.
\end{equation}
On the other hand, if we assume CPCM (\ref{varOfInterest}), then $f_Y\in\mathcal{F}_F$ where 
\begin{equation}\label{mathcalF_F}
\mathcal{F}_F := \{f\in\mathcal{I}_m: f(\textbf{x},\varepsilon) =  F^{-1}(\varepsilon;\theta(\textbf{x})) \text{ for some function } \theta(\cdot)\}.
\end{equation}
Note that $\forall i: \beta_i\neq 0$ in (\ref{mathcalF_L}), and $\theta(x)$ is non-constant in any of its argument in (\ref{mathcalF_F}) (that follows from assuming $f\in\mathcal{I}_m$). 

In the following, we describe which $\textbf{X}_S, S\subseteq\{1, \dots, p\}$ can potentially be parents of $Y$ while assuming $f_Y\in\mathcal{F}$.  
For that, we will assume that $pa_Y\neq\emptyset$ (we discuss and soften this assumption in Section \ref{subsection3.3}). 


\begin{definition}
Let $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow a SCM with DAG $\mathcal{G}_0$ and $pa_Y\neq\emptyset$.  Let $\mathcal{F}\subseteq \mathcal{I}_m$ be a set of possible causal functions corresponding to the structural equation for $Y$. A non-empty set $S\subseteq\{1, \dots, p\}$ is called $\mathcal{F}$-\textbf{plausible} set of parents of $Y$, if 
\begin{equation}\label{Definition_F_plausible}
\exists f\in\mathcal{F} \text{ such that for } \varepsilon_S := f^{\leftarrow}(\textbf{X}_{S}, Y)\text{ holds }\varepsilon_S\indep \textbf{X}_S, \,\,\,  \varepsilon_S\sim U(0,1).
\end{equation}
We define a set of $\mathcal{F}$-\textbf{identifiable} parents of $Y$ as follows:
\begin{equation}
S_\mathcal{F}(Y):= \bigcap_{S\subseteq \{1, \dots, p\}}\{S:   \text{ S is } \mathcal{F} \text{-plausible set of parents of Y } \}.
\end{equation}
\end{definition}
The concept of  $\mathcal{F}$-identifiability provides theoretical boundaries for the causal estimates. If $S_\mathcal{F}(Y) $ contains one element, the best that we can hope for is to find one cause of $Y$, even if we observe infinite number of observations and have an infinite computational power available. 

The principle of an independence of the cause and the mechanism directly implies that a set $S=pa_Y$ is always  $\mathcal{F}$-plausible and hence 
\begin{equation}
S_\mathcal{F}(Y) \subseteq pa_Y.
\end{equation}
However, the equality does not need to hold. Observe that 
$$\text{if }\mathcal{F}_1\subseteq \mathcal{F}_2\subseteq \mathcal{I}_m, \text{ then }S_{\mathcal{F}_1}(Y)\supseteq S_{\mathcal{F}_2}(Y).$$
This is not surprising; more information we have on the data-generating process, the larger set of identifiable parents. 

We focus on the case when $\textbf{X}=(X_1, \dots, X_p)$ are neighbours (either parents or children) of $Y$. Using classical conditional independence approach and d-separation, we can eliminate other variables from being potential parents of $Y$.

In the rest of the section, we describe properties of $S_\mathcal{F}(Y)$ and conditions under which  $S_\mathcal{F}(Y)=pa_Y$. The following example shows an illustration of our results presented in the   rest of the section. 

\begin{example}[Teaser example with Gaussian assumptions]\label{Teaser example with Gaussian assumptions}
Consider a SCM with DAG drawn in Figure \ref{Mixed_Graph}B. Let $(X_1, X_2, X_3)$ be normally distributed, and $X_5$ is non-degenerate. Let $F$ be a Gaussian distribution function such as in Example \ref{ExampleMultivariateGaussiancase} and let 
$$Y = \mu_Y(X_1, X_2, X_3) + \sigma_Y(X_1, X_2, X_3)\varepsilon_Y,\,\,\,  \varepsilon_Y\text{ is Gaussian,}$$
$$X_4 = \mu_4(X_2, Y, X_5) + \sigma_4(X_2, Y, X_5)\varepsilon_4,\,\,\,  \varepsilon_4\text{ is Gaussian,}$$
where $\theta_Y(\cdot) = (\mu_Y(\cdot), \sigma_Y(\cdot))^\top,\theta_4(\cdot) = (\mu_4(\cdot), \sigma_4(\cdot))^\top$ are not in the form (\ref{norm}) in any of their arguments. 


Then, $S_{\mathcal{F}_F}(Y) = pa_Y = \{1,2,3\}$. This follows from Lemma \ref{GaussianSeparabilita} with combination with Lemma \ref{lemma o unseparability=unplausibility} and  Theorem \ref{TheoremFidentifiabilityWithChild} with combination with Theorem \ref{normalidentifiability}, presented in the remaining of the section. 
\end{example}




\begin{table}[]
\begin{tabular}{|l|}
\hline
\multicolumn{1}{|c|}{Definitions of different $\mathcal{F}\subset \mathcal{I}_m$ used in the paper}                                                                                                       \\ \hline
$\mathcal{F}_F := \{f\in\mathcal{I}_m: f(\textbf{x},\varepsilon) =  F^{-1}(\varepsilon;\theta(\textbf{x})) \text{ for some function } \theta(\cdot)\}$                       \\ \hline
$\mathcal{F}_L = \{f\in\mathcal{I}_m: f(\textbf{x}, \varepsilon) = \beta^T\textbf{x} + q^{-1}(\varepsilon) \text{ for some quantile function }q^{-1}(\cdot) \text{ and }\beta\neq 0\}$                                \\ \hline
$\mathcal{F}_A = \{f\in\mathcal{I}_m: f(\textbf{x}, \varepsilon) = \mu(\textbf{x}) + g^{-1}(\varepsilon) \text{ for some }\mu(\cdot)\text{ and quantile function }q^{-1}(\cdot)     \}$        \\ \hline
$\mathcal{F}_{LS} = \{f\in\mathcal{I}_m: f(\textbf{x}, \varepsilon) = \mu(\textbf{x}) + \sigma(\textbf{x}) q^{-1}(\varepsilon) $                                                     \\
\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,$\text{ for some functions  }\mu(\cdot), \sigma(\cdot)>0\,\,\,\,\,\text{ and for some quantile function }q^{-1}(\cdot)\}$ \\ \hline
\end{tabular}
\caption{The table summarizes the definitions of different forms used for the functional space $\mathcal{F}$ representing our assumptions. $\mathcal{F}_F$, $\mathcal{F}_L$, $\mathcal{F}_A$, $\mathcal{F}_{LS}$ correspond to, CPCM assumption, linear assumption, additive assumption and location-scale assumption respectively.   }
\label{tableDefinitions}
\end{table}


\subsection{A few cases when \texorpdfstring{$S_{\mathcal{F}}(Y) = \emptyset$}{Empty set of identifiable parents} }
The following example shows that the concept of  $\mathcal{F}$-identifiable parents is usually not very interesting for linear SCM. On the other hand, we show later that we can indeed profit from the non-linearity. 

%Add how f, f^{-1} looks 
\begin{example}\label{ExampleEasyDAG}
Consider a linear SCM with DAG drawn in Figure \ref{Mixed_Graph}A. Let $\mathcal{F}=\mathcal{F}_L$, and let $Y = \beta_1X_1+\beta_2X_2 + q^{-1}(\varepsilon_Y)$ for some $\beta_1\neq 0, \beta_2\neq 0, \varepsilon_Y\indep (X_1, X_2)$. Then $S_{\mathcal{F}_L}(Y) = \emptyset$. 

The reasoning is as follows: Set $S=\{1\}$  is $\mathcal{F}_L$-plausible, since $Y - \beta_1 X_1 \indep X_1$ and we do not put any restrictions on the noise variable. More precisely, we find $f\in\mathcal{F}_L$ such that (\ref{Definition_F_plausible}) holds. For that, define $f(x,\varepsilon) = \beta_1x + \tilde{q}^{-1}(\varepsilon)\in\mathcal{F}_L$, where $\tilde{q}$ is a distribution function of $[\beta_2X_2 + q^{-1}(\varepsilon)]$. Then, $\varepsilon_{S} = f^{\leftarrow}(X_1, Y) = \tilde{q}(Y - \beta_1X_1) = \tilde{q}[\beta_2X_2 + q^{-1}(\varepsilon)]\indep X_1$  and $\varepsilon_{S}\sim U(0,1)$. Hence, (\ref{Definition_F_plausible}) holds. 

Using the same reasoning,  $S=\{2\}$ is also $\mathcal{F}_L$-plausible. Hence, $S_{\mathcal{F}_L}(Y)\subseteq \{1\}\cap \{2\}=\emptyset$. 
\end{example}

A similar argument can be used in a more general case. The following lemma describes that in the linear structural causal models, $S_{\mathcal{F}_L}(Y)$ is usually empty. Note that even if we assume non-Gaussianity of the noise, the results will remain the same. 

\begin{lemma}\label{LemmaAboutUnidentifiabilityFL}
Let $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow Linear SCM with DAG $\mathcal{G}_0$ and $pa_Y(\mathcal{G}_0)\neq\emptyset$. Then, $|S_{\mathcal{F}_L}(Y)| \leq 1$. Moreover, if there exist $a,b\in an_Y(\mathcal{G}_0)$ that are d-separated in $\mathcal{G}_0$, then $S_{\mathcal{F}_L}(Y) = \emptyset$. 
\end{lemma}

Proof is in the \hyperref[Proof of LemmaAboutUnidentifiabilityFL]{Appendix} \ref{Proof of LemmaAboutUnidentifiabilityFL}. The previous example shows a more general principle; if we can \textit{marginalize} a causal model to a smaller submodel without breaking $f_Y\in\mathcal{F}$, then only the submodel is relevant for an inference about  $S_{\mathcal{F}}(Y)$. We provide more rigorous explanation of this. In the following, we also use the notation $X_0=Y$ for the target variable. 



\begin{definition}
Let $(X_0, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow a SCM with connected DAG $\mathcal{G}_0$. Let $\mathcal{F}\subseteq\mathcal{I}_m$. We say that the SCM follows an $\mathcal{F}$\textbf{-model}, if each structural equation in the SCM satisfy $f_i\in\mathcal{F}, i=0, \dots, p$. For a non-empty set $S\subset \{0, \dots, p\}$, we say that an $\mathcal{F}$-model is \textbf{marginalizable} to $S$ if  $\textbf{X}_{S}$ also follows an $\mathcal{F}$-model. 
%We say that an $\mathcal{F}$-model is \textbf{unmarginalizable} wrt. $X_0$, if for all non-empty sets $ S\subseteq\{1, \dots, p\}$ holds that $\textbf{X}_{(0,S)}$ do not follow an $\mathcal{F}$-model. 
\end{definition}

Although we are only interested in the causes of $Y$, it is natural to consider $f_i\in\mathcal{F}$ for all variables in order to provide some results about $S_{\mathcal{F}}(Y)$. Note that a statement "$(X_0, \textbf{X})$ follows a linear SCM" is equivalent to a statement "$(X_0, \textbf{X})$ follows an $\mathcal{F}_L$-model". We provide some intuition about the marginalizability on a specific example. 

\begin{example}\label{example158}
Consider a simple linear SCM $X_1=\varepsilon_1, X_2 = X_1 + \varepsilon_2,  X_0 = X_1+X_2+\varepsilon_Y$, where the noise variables (here not necessary uniformly distributed) are jointly independent. 
\begin{itemize}
\item If $\varepsilon_1, \varepsilon_2, \varepsilon_Y\overset{iid}{\sim} N(0,\sigma^2)$, then the previous $\mathcal{F}_L$-model is marginalizable to both $S = \{0,1\}$ and $S=\{0,2\}$.
\item If $\varepsilon_1, \varepsilon_2, \varepsilon_Y$ are \textit{not} Gaussian, then the previous $\mathcal{F}_L$-model is marginalizable to $S = \{0,1\}$, but not to $S=\{0,2\}$.
\end{itemize}
We will show later that $\mathcal{F}_F$-models are typically not marginalizable.
\end{example}
\begin{proof}
The marginalizability with respect to $S = \{0,1\}$ follows from the fact that we can rewrite $X_1=\varepsilon_1, X_0=2X_1 + \tilde{\varepsilon}_Y$, where $\tilde{\varepsilon}_Y = \varepsilon_2 + \varepsilon_Y\indep \varepsilon_1$. For  $S=\{0,2\}$ and Gaussian noise, we can rewrite $X_2=\tilde{\varepsilon}_2, X_0= \frac{3}{2}X_2 +\tilde{\varepsilon}_Y$, where $\tilde{\varepsilon}_2 = \varepsilon_1 + \varepsilon_2$ and $ \tilde{\varepsilon}_Y =\frac{1}{2}\varepsilon_1 -\frac{1}{2}\varepsilon_2 + \varepsilon_Y\indep \tilde{\varepsilon}_2$. 

For $S=\{0,2\}$ and non-Gaussian noise, we can never find $X_2=\tilde{\varepsilon}_2, X_0= \beta X_2 +\tilde{\varepsilon}_Y$, where $ \tilde{\varepsilon}_2 \indep \tilde{\varepsilon}_Y$ and $\beta\neq 0$, since for non-Gaussian variables always holds $a\varepsilon_1 + b\varepsilon_2 \not\indep c\varepsilon_1 + d\varepsilon_2$, for $a,b,c,d\in\mathbb{R}\setminus\{0\}$ (Darmois-SkitoviÄ theorem \citep{Skitovic}). 
\end{proof}


The following lemma states that marginalizable models have typically small set of identifiable parents. 

\begin{lemma}\label{lemma158}
 Let $\mathcal{F}\subseteq\mathcal{I}_m$. Let $(X_0, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow an $\mathcal{F}$-model with DAG $\mathcal{G}_0$ and $pa_Y(\mathcal{G}_0)\neq \emptyset$. Let  $S\subseteq \{1, \dots, p\}$. If $(X_0, \textbf{X})$ is marginalizable to $S\cup\{0\}$, then $S_{\mathcal{F}}(X_0)\subseteq S$. 
\end{lemma}
\begin{proof}
 \label{Proof of lemma158}
 Since  $(X_0, \textbf{X}_S)$ is an $\mathcal{F}$-model, there exist $f_0\in\mathcal{F}$, such that $X_0 = f_0(X_{\tilde{S}}, \varepsilon_0)$ for some $\tilde{S}\subseteq S$, $\varepsilon_0\indep X_{\tilde{S}}$, $\varepsilon_0\sim U(0,1)$. 
In other words, $ f_0^{\leftarrow}(X_{\tilde{S}}, X_0)\indep X_{\tilde{S}}$,  $f_0^{\leftarrow}(X_{\tilde{S}}, X_0)\sim U(0,1)$, what is exactly the definition of $\mathcal{F}$-plausibility. Hence, $\tilde{S}$ is $\mathcal{F}$-plausible and consequently  $S_{\mathcal{F}}(X_0)\subseteq \tilde{S}\subseteq S$. 
 \end{proof}
Continuing with Example \ref{example158}, Lemma \ref{lemma158} gives us that $S_{\mathcal{F}_L}(X_0)=\emptyset$ in the Gaussian case, and  $S_{\mathcal{F}}(X_0)=\{1\}$ in the non-Gaussian case. Hence, assuming linearity is not enough to say whether $X_2$ is a parent of $X_0$ or not. 

\subsection{A few cases when \texorpdfstring{$S_{\mathcal{F}}(Y) = pa_Y$}{Complete set of identifiable parents}}\label{Subsection3.2}

Consider a non-empty set $S\subseteq\{1, \dots, p\}$. The question we want to answer is, whether $S$ is $\mathcal{F}$-plausible or not. 
We will focus on two main cases: when $S\cap ch_Y\neq\emptyset$ and when $S\subset pa_Y$. Ideally, we would like to show that sets $S\neq pa_Y$ are not $\mathcal{F}$-plausible.  We only provide some cases when this is true. We start with the case when  $S\cap ch_Y\neq\emptyset$.

\subsubsection{Focusing on \texorpdfstring{$S\cap ch_Y\neq\emptyset$}{Focusing on descendants}}

The following theorem shows that we can use results from Section 2. Specifically, if all variables in the SCM follow identifiable CCPCM model as presented in Theorem \ref{thmMultivairateIdentifiability}, then any $S$ containing a child of $Y$ can not be $\mathcal{F}_F-$plausible. 

\begin{theorem}\label{TheoremFidentifiabilityWithChild}
Let $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow SCM with DAG $\mathcal{G}_0$. Let $S\subseteq\{1, \dots, p\}$ and let $\mathcal{G}=\mathcal{G}_0[S]$ (a projection of $\mathcal{G_0}$ on $S$ defined in (\ref{marginalizationOfGraph})) be a DAG. Let $S$ contain a childless child of $Y$, i.e. $\exists j\in  ch_Y(\mathcal{G})\cap S$ such that $ch_{j}(\mathcal{G})=\emptyset$. Let $F$ be a continuous distribution function with $q$ parameters. Let $(Y, \textbf{X}_S)$ follow an identifiable CCPCM model with graph $\mathcal{G}$, with the assumptions from Theorem \ref{thmMultivairateIdentifiability} fulfilled. Then, $S$ is not $\mathcal{F}_F$-plausible. 
\end{theorem}
Proof is in \hyperref[Proof of TheoremFidentifiabilityWithChild]{Appendix} \ref{Proof of TheoremFidentifiabilityWithChild}. Theorem \ref{TheoremFidentifiabilityWithChild} describes a more general principle that can be used outside of $\mathcal{F}_F$ models. If we have a fully identifiable setup ($\mathcal{G}$ is identifiable), then  $S$ containing a (childless) child of $Y$ can not be a plausible set. 

The following proposition discusses a different case, where $\mathcal{F}$-unplausibility comes from restricting support of $Y$ by conditioning on the child of $Y$. We state it for a general location-scale space of functions, defined as 
\begin{equation*}\label{mathcalF_LS}
\begin{split}
\mathcal{F}_{LS} = \{f\in\mathcal{I}_m: f(\textbf{x}, \varepsilon) = \mu(\textbf{x}) + \sigma(\textbf{x}) &q^{-1}(\varepsilon) \text{ for some functions  }\mu(\cdot), \sigma(\cdot)>0\\&\,\,\,\,\,\text{ and for some quantile function }q^{-1}(\cdot)\}.
\end{split}
\end{equation*}
Note that $\mathcal{F}_L,  \mathcal{F}_A, \mathcal{F}_F\subset  \mathcal{F}_{LS}$ for any Location-Scale distribution $F$  (see Definition \ref{DefLS}). Hence, if a set $S$ is not $\mathcal{F}_{LS}$-plausible, then it is also not $\mathcal{F}_L, \mathcal{F}_A$ or $ \mathcal{F}_F$-plausible. 


\begin{proposition}[Assuming bounded support]
\label{Support_proposition}
Let $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow SCM with DAG $\mathcal{G}_0$. Let $S\subseteq\{1, \dots, p\}$ be a non-empty set. Let  $\underline{\Psi},\overline{\Psi}: \mathbb{R}^{\mid S\mid}\to \mathbb{R}$ be real functions such that
\begin{equation} \label{eq15978}
supp(Y\mid \textbf{X}_S=\textbf{x}) = (\underline{\Psi} (\textbf{x}),\overline{\Psi}(\textbf{x})), \,\,\,\,\,\,\, \forall \textbf{x}\in supp(\textbf{X}_S).
\end{equation}
Moreover, let
\begin{equation} \label{eq9987}
\frac{Y - \underline{\Psi}(\textbf{X}_S)}{\overline{\Psi}(\textbf{X}_S) - \underline{\Psi}(\textbf{X}_S)}\not\indep \textbf{X}_S.
\end{equation}
Then, $S$ is not $\mathcal{F}_{LS}$-plausible. 
\end{proposition}
Proof can be found in \hyperref[Proof of Support_proposition]{Appendix} \ref{Proof of Support_proposition}. Proposition \ref{Support_proposition} can be put in words as follows. If the support of $Y$ given $\textbf{X}_S = \textbf{x}_S$ is bounded, then $S$ can be  $\mathcal{F}_{LS}$-plausible only in a very specific case when (\ref{eq9987}) does not hold. Typically, (\ref{eq9987}) holds if $S$ contains a child of $Y$. 
\begin{example}\label{Example_o_Supporte}
Consider SCM with $Y\to X_1$. Let $X_1 = Y + \varepsilon$, where $Y\indep \varepsilon$. Assume that  $Y, \varepsilon$ are non-negative, i.e. $supp(Y) = supp(\varepsilon) = (0, \infty)$. 
Then, $\underline{\Psi}(x)=0$ and  $\overline{\Psi}({x})=x$, since the support of $[Y\mid Y+\varepsilon=x]$ is $(0,x)$. Hence, (\ref{eq9987}) reduces to $\frac{Y}{X_1} \not\indep X_1$. If $\frac{Y}{X_1} \not\indep X_1$, then $S=\{1\}$ is not $\mathcal{F}_{LS}$-plausible. 
\end{example}

\begin{remark}
One may ask, how strong is the assumption $\frac{Y}{X_1} \not\indep X_1$. We claim that it holds in typical situations. A notable exception when  $\frac{Y}{X_1} \indep X_1$ holds is when $Y, \varepsilon$ have Gamma distributions with equal scales. Details are left for the reader. 
\end{remark}
Proposition \ref{Support_proposition} is useful only when $S$ contains a child of $Y$. Generally if $S\subseteq pa_Y$, then (\ref{eq9987}) typically does not hold, as the following example illustrates.  
\begin{example}
Consider bivariate SCM with $X_1\to Y$. Let $Y = X_1 + \varepsilon$, where $X_1\indep \varepsilon$. Assume that $supp(X) = supp(\varepsilon) = (0, 1)$. Then, $\underline{\Psi}(x)=x$ and  $\overline{\Psi}({x})=1+x$. Hence, (\ref{eq9987}) reduces to $Y-X_1 \not\indep X_1$, which is obviously not satisfied. Hence, Proposition \ref{Support_proposition} is not applicable. 
\end{example}
Similar results as Proposition \ref{Support_proposition} can be also derived for different $\mathcal{F}$ besides $\mathcal{F}_{LS}$. Then, form of (\ref{eq9987}) would change accordingly. Proposition \ref{Support_proposition} can be also stated for a case when  $\overline{\Psi}({x})=\infty$. Then, we require stronger assumptions; specifically, assumption (\ref{eq9987}) changes to $Y - \underline{\Psi}(\textbf{X}_S)\not\indep \textbf{X}_S$ and instead of $\mathcal{F}_{LS}$ we require $\mathcal{F}_A$ (scale is fixed). 

\subsubsection{Focusing on \texorpdfstring{$S\subset pa_Y$}{Focusing on parents}}
In the following, we discuss a case when $S\subset pa_Y$. First, we introduce the notion of unseparability of a real function. We will show that if $f_Y$ is unseparable, then every $S\subsetneq pa_Y$ is not $\mathcal{F}$-plausible. Later, we provide a characterisation of unseparable functions, which leave us with a powerful tool for detecting  $\mathcal{F}$-plausible sets. 

\begin{definition}
Let $\textbf{X}=(X_1, \dots, X_k)$ be random variables and $\mathcal{F}\subseteq\mathcal{I}_m$. A function $f\in\mathcal{I}_m :\mathbb{R}^{k+1}\to\mathbb{R}$ is called $\mathcal{F}-$\textbf{unseparable} \textbf{wrt.} $\textbf{X}$, if for all $S\subsetneq\{1, \dots, k\}$ there exists $z\in (0,1)$ such that 
\begin{equation}\label{DefinitionUnseparability}
\forall g\in\mathcal{F}: g^{\leftarrow}(\textbf{X}_S, f(\textbf{X}, z))\not\indep \textbf{X}_S.
\end{equation}
 \end{definition}
The notion of unseparability of a function simply describes that we are not able to "erase" the effect of $\textbf{X}_S$ on $Y=f(\textbf{X}, z)$ without considering other variables. Note that the notion of $\mathcal{F}$-unseparability is a property of a real function; it does not depend on any causal relations (only on distribution of $\textbf{X}$). 

\begin{example}
Let $k=2$ and $\textbf{X}= (X_1, X_2)$ is continuous with independent components. Consider a function$f(x_1, x_2, z) = x_1x_2z$ and $\mathcal{F}_1, \mathcal{F}_2\subseteq\mathcal{I}_m$ such that $\mathcal{F}_1$ contains only linear functions and $\mathcal{F}_2$ contains only multiplicative functions, i.e. $\mathcal{F}_1 = \mathcal{F}_L$ and 
$$\mathcal{F}_2 = \{f\in\mathcal{I}_m: f(\textbf{x}, \varepsilon) = g_1(x_1)\cdots g_k(x_k)q^{-1}(\varepsilon) \text{ for some functions }g_1(\cdot), \dots, g_k(\cdot), q(\cdot)\}.$$  
Then, $f$ is $\mathcal{F}_1-$unseparable wrt. $\textbf{X}$ but is not $\mathcal{F}_2-$unseparable wrt. $\textbf{X}$.
\end{example}
\begin{proof}
 Intuitively, we can not find $\beta\in\mathbb{R}$ such that $X_1X_2\varepsilon -\beta X_1 \indep X_1$. But, we can find $g_1\in\mathbb{R}^{\mathbb{R}}$ such that $X_1X_2\varepsilon \cdot g_1(X_1)\indep X_1$. We give more rigorous explanation of this. 
 
 First, we show that $f$ is not $\mathcal{F}_2-$unseparable wrt. $\textbf{X}$. Take $S=1$. Choose a function $g\in\mathcal{F}_2$ such that  $g(x, z) = xz$. Its inverse has a form $g^{\leftarrow}(x, xz) = z$. Hence, $g^{\leftarrow}(X_S, f(\textbf{X}, z))=g^{\leftarrow}(X_1, X_1X_2z) = X_2z\indep X_1$. Hence, $f$ is not $\mathcal{F}_2-$unseparable wrt. $\textbf{X}$, since we found $g\in\mathcal{F}_2$ such that (\ref{DefinitionUnseparability}) is violated. 

Second, we  show that $f$ is $\mathcal{F}_1-$unseparable wrt. $\textbf{X}$. Consider $S=1$ and consider any function $g\in\mathcal{F}_1$. Let us write $g$ in a form $g(x,z) = \beta x + {q}^{-1}(z)$ for some quantile function ${q}^{-1}$ and $\beta\neq 0$. Inverse of $g$ satisfy $g^{\leftarrow}(x, xz) ={q}(xz - \beta x)$. In order to show (\ref{DefinitionUnseparability}), we need to show that $q(X_1X_2z - \beta X_1)\not\indep X_1$. In the rest of the chapter, we develop a theory for proving statements like this one. In particular, $X_1X_2z - \beta X_1\not\indep X_1$ holds due to Lemma \ref{CoolLemma} part 2. Hence, for $S=1$ is (\ref{DefinitionUnseparability}) satisfied. For $S=2$ and $S=\emptyset$ the proof follow analogously. 
\end{proof}


The following lemma shows the connection between the  $\mathcal{F}$-unseparability and  $\mathcal{F}$-plausibility. We show that  $\mathcal{F}$-unseparability implies $\mathcal{F}$-unplausibility of all subsets of the parents. 


\begin{lemma}\label{lemma o unseparability=unplausibility}
Let $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow SCM with DAG $\mathcal{G}_0$ and $pa_Y(\mathcal{G}_0)\neq\emptyset$. Let $f_Y\in\mathcal{F}\subseteq\mathcal{I}_m$. If $f_Y$ is  $\mathcal{F}-$unseparable wrt. $\textbf{X}_{pa_Y}$, then each $S\subsetneq pa_Y(\mathcal{G}_0)$ is \textit{not} $\mathcal{F}$-plausible set of parents of $Y$. 
\end{lemma}
\begin{proof}\label{proof of lemma o unseparability=unplausibility}
%"$\implies$": 
For a contradiction, let $S\subsetneq pa_Y(\mathcal{G}_0)$ be $\mathcal{F}$-plausible set of parents of $Y$. Then, $\exists f\in\mathcal{F} \text{ such that } f^{\leftarrow}(\textbf{X}_{S}, Y)\indep \textbf{X}_S.$ Since $Y = f_Y(\textbf{X}_{pa_Y}, \varepsilon_Y)$, we can rewrite $f^{\leftarrow}(\textbf{X}_{S}, f_Y(\textbf{X}_{pa_Y}, \varepsilon_Y))\indep \textbf{X}_S.$ Since $\varepsilon_Y\indep \textbf{X}_{pa_Y}$, conditioning on $[\varepsilon_Y=z]$ for arbitrary $z\in(0,1)$ will give us $f^{\leftarrow}(\textbf{X}_S, f(\textbf{X}_{pa_Y}, z))\indep \textbf{X}_S$, what is a contradiction with unseparability.
%"$\impliedby$" For a contradiction, let $S\subsetneq\{1, \dots, p\}$ and let there exist $g\in \mathcal{F}$ such that $g^{-1}(X_S, f_Y(\textbf{X}_{pa_Y}, z))\indep X_S$ for any $z\in (0,1)$. This implies (law of total probability) that for $\varepsilon_Y\indep X_{pa_Y}$ holds $g^{-1}(X_S, f_Y(\textbf{X}_{pa_Y}, \varepsilon_Y))\indep X_S$. Using $Y= f_Y(\textbf{X}_{pa_Y}, \varepsilon_Y)$ rewrite $g^{-1}(X_S, Y)\indep X_S$. Since $g\in \mathcal{F}\subset\mathcal{I}$, this is a contradiction with $\mathcal{F}$-plausibility. 
\end{proof}
In the following, we move our attention on characterisation of  $\mathcal{F}_F$-unseparability for different $F$. We show that some large classes of $f\in\mathcal{F}_F$ are indeed $\mathcal{F}_F$-unseparable. We will restrict our attention to specific types of $F$, that are additive/multiplicative/location-scale in the following sense. 

 
\begin{definition}\label{DefLS}
Let $F$ be a distribution function with one ($q=1$) parameter $\theta$. We say that the \textbf{parameter acts post-additively} in $F$, if there exist an invertible real function $f_2$ and a function $f_1\in\mathcal{I}_m$ such that for all $\theta_1, \theta_2$ holds \footnote{Notation $F_{\theta_1}(F^{-1}_{\theta_2}(z))$ is equivalent to $F(F^{-1}(z, \theta_2), \theta_1)$. We think that it improves the readability.  } 
\begin{equation}\label{postAdditiveDefinition}
F_{\theta_1}(F^{-1}_{\theta_2}(z)) = f_1(z, f_2(\theta_1) + \theta_2), \,\,\,\forall z\in(0,1). 
\end{equation}

We say that the \textbf{parameter acts post-multiplicatively} in $F$, if there exist an invertible real function $f_2$ and a function $f_1\in\mathcal{I}_m$ such that for all $\theta_1, \theta_2$ holds  \begin{equation}\label{postMultiplDefinition}
F_{\theta_1}(F^{-1}_{\theta_2}(z)) = f_1(z, f_2(\theta_1) \cdot\theta_2), \,\,\,\forall z\in(0,1).
\end{equation}
Let $F$ be a distribution function with two ($q=2$) parameters $\theta = (\mu, \sigma)^\top\in\mathbb{R}\times \mathbb{R}_+$. We say that $F$ is a \textbf{Location-Scale} distribution, if for all $\theta$ holds  
\begin{equation}
F_{\theta}\left( \frac{x-\mu}{\sigma}\right) = F_{\theta_0}(x),\,\,\,\,\forall x\in\mathbb{R},
\end{equation} 
where $F_{\theta_0}$ is called standard distribution and corresponds to a parameter $\theta_0 = (0,1)^\top$.  
\end{definition}
Examples of $F$ whose parameter acts post-additively include a Gaussian distribution with fixed variance, or a Logistic distribution/Gumbel distribution with fixed scales.  Note that typically $f_2(x) = -x$ since $F_{\theta_1}(F^{-1}_{\theta_1}(z))=z$ needs to hold.

Examples of $F$ whose parameter acts post-multiplicatively include a Gaussian distribution with fixed expectation, or a Pareto distribution (where $F_{\theta_1}(F^{-1}_{\theta_2}(z)) = z^{\frac{\theta_1}{\theta_2}}= f_1(f_2(\theta_1) \cdot\theta_2, z)$ for $f_1(x,z) = z^{-1/x}$ and $f_2(x)=-1/x$). Functions $ f_1, f_2$ are not necessary uniquely defined. 

Examples of Location-Scale type of distributions include Gaussian distribution, logistic distribution or Cauchy distribution, among many others.

First, we consider one parameter case ($q=1$). The following two propositions are the main results of this subsection. They characterise $\mathcal{F}_F-$unseparability (under some weak assumptions on the distribution function $F$). Combining these results with Lemma \ref{lemma o unseparability=unplausibility} give us a powerful tool for detecting $\mathcal{F}_F-$plausible sets.  



\begin{proposition}\label{LemmaOParetoUnseparabilite}
Let $F$ be a distribution function whose parameter acts post-multiplicatively. Let  $\textbf{X}=(X_1, \dots, X_k)$ be a continuous random vector with independent components.  
\begin{itemize}
\item Consider $f\in\mathcal{F}_F$ in the form $f(\textbf{x}, \varepsilon)=F^{-1}(\varepsilon, \theta(\textbf{x}))$ with an additive function $\theta(x_1, \dots, x_k) = h_1(x_1)+\dots + h_k(x_k)$, where $h_i$ are continuous non-constant real functions. Then, $f$ is $\mathcal{F}_F-$unseparable wrt $\textbf{X}$. 
\item Consider $f\in\mathcal{F}_F$ in the form $f(\textbf{x}, \varepsilon)=F^{-1}(\varepsilon, \theta(\textbf{x}))$ with an multiplicative function   $\theta(x_1, \dots, x_k) = h_1(\textbf{x}_S)\cdot h_2(\textbf{x}_{\{1, \dots, k\}\setminus S})$ for some $S\subsetneq \{1, \dots, k\}$, where $h_1, h_2$ are continuous non-constant non-zero real functions. Then, $f$ is not $\mathcal{F}_F-$unseparable wrt $\textbf{X}$. 
\end{itemize}
\end{proposition}
\begin{hproof}
Full proof is in \hyperref[Proof of LemmaOParetoUnseparabilite]{Appendix} \ref{Proof of LemmaOParetoUnseparabilite}, here we show the main steps of the first bullet-point in a case when $F$ is Pareto distribution function, $k=2, h_1(x)=h_2(x)=x$.  Take  $S=\{1\}$ (the case $S=\{2\}$ follows from a symmetry). We show that for $S=\{1\}$ and any $z\in(0,1)$ there does not exist $g\in\mathcal{F}_F$ such that $g^{\leftarrow}(X_1, f(\textbf{X}, z))\indep X_1.$ For a contradiction, assume that such $g$ exists and write $g^{\leftarrow}(x, \cdot) = F(\cdot, \theta_g(x))$ for some non-constant function $\theta_g(\cdot)>0$. Rewrite $X_1\indep g^{\leftarrow}(X_1, f(\textbf{X}, z)) = F(F^{-1}(z, \theta(\textbf{X}))\theta_g(X_1)) = z^{-\frac{\theta(\textbf{X})}{\theta_g(X_1)}} = z^{-\frac{(X_1+X_2)}{\theta_g(X_1)}}$. Equivalently, we have that $$X_1\indep \theta_g(X_1)(X_1+X_2).$$ 
We will show that this is not possible. Denote $\xi=\theta_g(X_1)(X_1+X_2)$ and choose \textit{distinct} $a,b,c$ in the support of $X_1$ such that $\theta_g(b)\neq 0$ (since $\theta_g$ is non-constant, this is possible). 

Since $\xi\indep X_1$, then $\xi\mid [X_1=a] \overset{D}{=}\xi\mid [X_1=b] \overset{D}{=}\xi\mid [X_1=c]$. Hence, 
\begin{equation}\label{asdf}
\theta_g(a)(a+X_2)\overset{D}{=}\theta_g(b)(b+X_2)\overset{D}{=}\theta_g(c)(c+X_2).
\end{equation}
By dividing by a nonzero constant $\theta_g(b)$ and subtracting $b$ we get
$$
\{\frac{\theta_g(a)}{\theta_g(b)}a-b\}+\frac{\theta_g(a)}{\theta_g(b)}X_2\overset{D}{=}X_2\overset{D}{=}\{\frac{\theta_g(c)}{\theta_g(b)}c-b\}+\frac{\theta_g(c)}{\theta_g(b)}X_2.
$$
It
holds that (see lemma \ref{distributionalequalitylemma}) if $z_1+z_2X_2\overset{D}{=}X_2$ for some constants $z_1, z_2$, then $z_2=\pm 1$. Hence in our case, it holds that $\frac{\theta_g(a)}{\theta_g(b)}=\pm 1$ and also  $\frac{\theta_g(c)}{\theta_g(b)}=\pm 1$. Therefore, at least two values of $\theta_g(a),\theta_g(b), \theta_g(c)$ has to be equal and neither of them are zero. WLOG $\theta_g(a)= \theta_g(b)$. Plugging this into equation \ref{asdf}, we get $a=b$ which is a contradiction since we chose them distinct. Therefore, non-constant function $\theta_g$ does not exist and hence also $g$ does not exist. 
\end{hproof}


\begin{proposition}\label{LemmaOAdditiveUnseparabilite}
Let $F$ be a distribution function whose parameter acts post-additively. Let  $\textbf{X}=(X_1, \dots, X_k)$ be a continuous random vector with independent components.  
\begin{itemize}
\item Consider $f\in\mathcal{F}_F$ in the form $f(\textbf{x}, \varepsilon)=F^{-1}(\varepsilon, \theta(\textbf{x}))$ with an additive function $\theta(x_1, \dots, x_k) = h_1(\textbf{x}_S) + h_2(\textbf{x}_{\{1, \dots, k\}\setminus S})$ for some $S\subsetneq \{1, \dots, k\}$, where $h_1, h_2$ are continuous non-constant non-zero real functions. Then, $f$ is not $\mathcal{F}_F-$unseparable wrt $\textbf{X}$. 
\item Consider $f\in\mathcal{F}_F$ in the form $f(\textbf{x}, \varepsilon)=F^{-1}(\varepsilon, \theta(\textbf{x}))$ with a multiplicative function   $\theta(x_1, \dots, x_k) = h_1(x_1)\cdot h_2(x_2)\dots h_k(x_k)$ where $h_i$ are continuous non-constant non-zero real functions. Then, $f$ is $\mathcal{F}_F-$unseparable wrt $\textbf{X}$. 
\end{itemize}
\end{proposition}

 Proof follows similar steps as the one of Proposition \ref{LemmaOParetoUnseparabilite} and can be found in \hyperref[Proof of LemmaOAdditiveUnseparabilite]{Appendix} \ref{Proof of LemmaOAdditiveUnseparabilite}.  Proposition \ref{LemmaOParetoUnseparabilite} and Proposition \ref{LemmaOAdditiveUnseparabilite} reveal that the form of the parameter can not match the way how the parameter affects the distribution, if we hope to have $\mathcal{F}_F$-unseparability. If the parameter acts additively (multiplicatively), the parameter can not have an additive (multiplicative) form.  However,  $\mathcal{F}_F$-unseparability typically holds if the forms do not match. 
 
 
Proposition \ref{LemmaOParetoUnseparabilite} and Proposition \ref{LemmaOAdditiveUnseparabilite} are formulated only for one-parameter case (i.e. when $q=1$), where we saw that the role of $\theta$ in $F$ plays a crucial role. The following Proposition discusses a case when $q=2$, where we restrict ourselves to a Location-Scale family of distributions.


\begin{proposition}\label{LemmaOLocationScaleUnseparabilite}
Let $F$ has a Location-Scale type with $q=2$ parameters. Let $\textbf{X}=(X_1, \dots, X_k)$ be a continuous random vector with independent components. Consider $f\in\mathcal{F}_F$ in the form $f(\textbf{x}, \varepsilon)=F^{-1}(\varepsilon, \theta(\textbf{x}))$, where $\theta(\textbf{x}) = (\mu(\textbf{x}), \sigma(\textbf{x}))^\top$ is additive in both components, i.e. 
$\mu(\textbf{x}) = h_{1, \mu}(x_1)+\dots + h_{k, \mu}(x_k)$ and 
$\sigma(\textbf{x}) =h_{1, \sigma}(x_1)+\dots + h_{k, \sigma}(x_k)$ for some continuous non-constant non-zero functions $h_{i,\cdot}$, where moreover we assume $h_{i,\sigma}>0$, $i=1, \dots, k$.  Then, $f$ is $\mathcal{F}_F-$unseparable wrt $\textbf{X}$. 
\end{proposition}

Proof is in \hyperref[Proof of LemmaOLocationScaleUnseparabilite]{Appendix}. The most important assumption lies in the additivity of $\sigma(\textbf{x})$; since this parameter acts multiplicatively in $F$, and has an additive form, similar reasoning as in Proposition  \ref{LemmaOParetoUnseparabilite} can be used (Proposition \ref{LemmaOLocationScaleUnseparabilite} shows that even if another parameter $\mu$ depends on $\textbf{X}$, it does not ruin the results). The Proposition \ref{LemmaOLocationScaleUnseparabilite} can be also reformulated such that both parameters have multiplicative form. 


The biggest drawback of Propositions \ref{LemmaOParetoUnseparabilite}, \ref{LemmaOAdditiveUnseparabilite}, \ref{LemmaOLocationScaleUnseparabilite} lies in the fact that we assumed independent components of $\textbf{X}$, which is rarely the case. This assumption is only a technical assumption that simplifies the proof, and is not necessary. It allows us to explicitly express the conditional distributions $h(X_{i})\mid \textbf{X}_S$, that we used in the proof. However, an explicit form of a conditional distribution can be found also in other cases, such as when $\textbf{X}$ is Gaussian. 

\begin{lemma}\label{GaussianSeparabilita}
Let the assumptions from Proposition \ref{LemmaOLocationScaleUnseparabilite} hold, and let $\textbf{X}=(X_1, \dots, X_k)$ be non-degenerate Gaussian random vector (possibly with dependent components) and $h_{i,\sigma}$ be linear functions. Then, $f$ is $\mathcal{F}_F-$unseparable wrt $\textbf{X}$. 
\end{lemma}
\begin{proof}
The proof follows the same steps as the proof of Proposition \ref{LemmaOLocationScaleUnseparabilite}, with the only difference that we use  Lemma \ref{CoolLemma} part 4 instead of Lemma \ref{CoolLemma} part 3 in the last step. 
\end{proof}

The results of Propositions \ref{LemmaOParetoUnseparabilite}, \ref{LemmaOAdditiveUnseparabilite}, \ref{LemmaOLocationScaleUnseparabilite} are not restricted for assumptions $\mathcal{F}_F$, but can be applied to different $\mathcal{F}$ such as Additive Noise Model assumptions briefly discussed in Example \ref{ANMexample}. 


\subsection{Examples and what if  \texorpdfstring{$pa_Y=\emptyset$?}{What if Y is a source?}}\label{subsection3.3}

The case $pa_Y=\emptyset$ has to be dealt with separately. Many of commonly-used assumptions $\mathcal{F}$ lead to fully unrestricted marginal distribution of $Y$ in case of  $pa_Y=\emptyset$. E.g. assuming $\mathcal{F}=\mathcal{F}_L$ gives us $Y=f_Y(\varepsilon_Y) = g^{-1}(\varepsilon_Y)$ for unrestricted quantile function $g^{-1}$, which fits into every situation. One possibility is to define a subset $\mathcal{P}_{\emptyset} \subset \{P: P \text{ is a distribution function}\}$ and  say that  $pa_Y=\emptyset$ is plausible if and only if $P_Y\in\mathcal{P}_{\emptyset}$. However, in most applications, defining $\mathcal{P}_{\emptyset}$ would be a troublesome step since we usually do not have an information about it. 

Case $\mathcal{F}=\mathcal{F}_F$ gives us an easy option for assessing the validity of  $pa_Y=\emptyset$. If the marginal distribution of $Y$ is $F$ with some (unknown, but constant) parameters, we say that $pa_Y=\emptyset$ is plausible. In other words, assessing  $P_Y\in \mathcal{P}_{F, \emptyset} = \{ F_{\theta}: \theta\in\mathbb{R}^q\}$ which is  equivalent to assessing $f_Y\in \{f\in \mathcal{F}_F: f \text{ is a univariate}\} = \{f : f(\varepsilon) = F^{-1}(\varepsilon, \theta), \text{ for some constant }\theta\}$. 
Even though a marginal assessment $P_Y\in \mathcal{P}_{F, \emptyset}$ correspond nicely to the previous $\mathcal{F}_F$ framework, it does not mean it is reasonable to assume in all applications. 

Sometimes, we have a subset of covariates where we know that one of them is parent (but we do not know which). Or, we have one covariate $X_j$ which we know that it is a parent of $Y$. These cases arise e.g. when we made one do-intervention. Hence, assuming  $pa_Y\neq\emptyset$ can be done often by an expert knowledge about the problem, or by utilizing other causal inference methods. 


In the rest of the section, we provide a few examples and discussion. 

\begin{example}\label{ExampleParetoFinal}
Consider again a SCM with DAG drawn in Figure \ref{Mixed_Graph}A, with $X_1 = \varepsilon_1, X_2 = \varepsilon_2$. Let the structural equation corresponding to $Y$ be in the form (\ref{varOfInterest}) with $F$ being a Pareto distribution such as in Example  \ref{Pareto case} with true $\theta(X_1, X_2) = h_1(X_1)+h_2(X_2)$ for some continuous non-constant functions  $h_1, h_2$. Moreover, let $X_3 = F^{-1}(\varepsilon_3, \theta_3(Y))$, where $\theta_3(x)\neq a\log(x) + b$. Then, $S_{\mathcal{F}_F}(Y) = \{1,2\}=pa_Y$. 

The reason is as follows: if $3\in S$, then $S$ is not $\mathcal{F}_F$-plausible (Theorem \ref{TheoremFidentifiabilityWithChild} with combination with Consequence \ref{paretoidentifiability}). Cases $S=\{1\}$ or $S=\{2\}$ are also  not $\mathcal{F}_F$-plausible, since the function $\theta(x_1, x_2) = h_1(x_1)+h_2(x_2)$ is $\mathcal{F}_F$-unseparable wrt $(X_1, X_2)$ from Proposition \ref{LemmaOParetoUnseparabilite}. Since $S=\{1,2\}$ is trivially $\mathcal{F}_F$-plausible, we get    $S_{\mathcal{F}_F}(Y) = \{1,2\}=pa_Y$.
\end{example}



\begin{example}[Additive noise models]\label{ANMexample}
Consider a SCM with DAG drawn in Figure \ref{Mixed_Graph}B. Let $\mathcal{F}=\mathcal{F}_A$, defined in Table \ref{tableDefinitions},
corresponding to an additive noise assumption. Let
 $$X_1 = \varepsilon_1,\,\,\,X_2 =X_1 + \varepsilon_1, \,\,\,  X_ 3 = \varepsilon_3,\,\,\,Y = h_1(X_1)h_2(X_2) + \varepsilon_Y, X_4 = \mu_1(Y)+ (\mu_2(X_3) + \varepsilon_2)$$for some (here, not necessarily uniformly distributed) independent $\varepsilon_1, \varepsilon_2, \varepsilon_3, \varepsilon_Y$, where  $(X_1, X_2)$ are Gaussian, $h_1, h_2, \mu_1, \mu_2$ are continuous non-zero functions. Let $(Y, X_4)$ follow an identifiable bivariate additive noise model \footnote{\citep[Theorem 20]{Peters2014} showed that there are only a few special cases of $\mu_1$ that lead to an unidentifiable model. Specifically, certain differential equation has to be satisfied. }.


We explain why $S_{\mathcal{F}_A}(Y) = \{1,2\}=pa_Y$. First, $S=\{1,2\}$ is trivially $\mathcal{F}_A$-plausible. We show that other sets are not  $\mathcal{F}_A$-plausible. Sets $S=\{1\},S = \{2\}$ are not $\mathcal{F}_A$-plausible, since $\theta(X_1, X_2):=h_1(X_1)h_2(X_2)$ is in a multiplicative form and we can apply similar results \footnote{We do not provide a rigorous statement and the proof of such a claim for $\mathcal{F} = \mathcal{F}_A$. Nevertheless, following the steps in the \hyperref[Proof of LemmaOParetoUnseparabilite]{proof of proposition \ref{LemmaOParetoUnseparabilite}}, it is not hard to see that we can get to the same contradiction such as in Equation (\ref{ojojoj}) by conditioning on the noise variable from the beginning. } such as in Proposition~\ref{LemmaOAdditiveUnseparabilite}. 


$S=\{4\}$ is not   $\mathcal{F}_A$-plausible since $(Y, {X}_4)$ follow an identifiable (additive) model, and a similar argument as in Theorem \ref{TheoremFidentifiabilityWithChild} can be used \footnote{Again, we do not provide a rigorous proof. Nevertheless, we can come up with a contradiction similar to the one in Theorem \ref{TheoremFidentifiabilityWithChild}. Notice that if there exist 
$\tilde{\mu}(X_4)$ such that $Y -  \tilde{\mu}(X_4)\indep X_4$, this would lead to an unidentifiability of the pair $(Y, X_4)$.}. Set $S = \{3,4\}$ can be dealt with analogously.

$S = \{3\}$ is not  $\mathcal{F}_A$-plausible, since $Y - f(X_3)\indep X_3$ can happen only if $f$ is constant function (and constant functions do not belong to  $\mathcal{I}_m$). We do not have to care about the cases  $S = \{1,2,3\}$ or $S = \{1,2,4\}$ or $S = \{1,2,3,4\}$ since $\{1,2\}\subset S$ and the set of $\mathcal{F}_A$-identifiable parents remain the same regardless of the plausibility of these $S$. Hence,  $S_{\mathcal{F}_A}(Y) = \{1,2\}=pa_Y$. Note that if $X_4 = \mu(Y,X_3)+\varepsilon_2$ for some $\mu(Y,X_3)\neq  \mu_1(Y)+\mu_2(X_3)$ then we can not use the argument for the set $S=\{4\}$ since then $(Y, X_4)$ do not follow an additive bivariate model (marginalizability would not hold). 
\end{example}

Note that in Example \ref{ANMexample}, we considered a variable $X_5$ which is not a neighbour of $Y$. In general, we can directly discard this variable as a potential parent using classical graphical argument of d-separation. However, it turns out that $X_5$ is not $\mathcal{F}_A$-plausible parent anyway. 


TODO more examples?
%\begin{example}
%Consider again a SCM with DAG drawn in Figure \ref{EasyDAG}. Assume that the structural equation corresponding to $Y$ is in the form (\ref{varOfInterest}) with $F$ being a Pareto distribution such as in Example  \ref{Pareto case} with true $\theta(X_1, X_2) = X_1+X_2$. Moreover, let $X_3 = f_3(Y)+\varepsilon_3$ for some $f_3$. Then, $S_\mathcal{F}(Y) = \{1,2\}=pa_Y$. 

%The reasoning is as follows: the quantile function in the structural equation can be rewritten as
%$Y = \varepsilon_Y^{\frac{-1}{\theta(X_1, X_2)}}$. Take $S=\{1\}$. Then, there does not exist a non-zero function $\tilde{\theta}(x)$ such that   $ \varepsilon_Y^{-\frac{\tilde{\theta}(X_1)}{\theta(X_1, X_2)}}\indep X_1$, since always $\frac{\tilde{\theta}(X_1)}{X_1+X_2}\not\indep X_1$. Hence, $\{1\}$ is not a set of plausible parents of $Y$. Similarly for $S=\{2\}$.  However, set $S=\{1,2\}$ is a set of plausible parents since $S=pa_Y$ is always  $\mathcal{F}$-plausible. 
%\end{example}



%\begin{lemma}
%Let $(Y, \textbf{X})\in\mathbb{R}\times \mathbb{R}^p$ follow identifiable CPCM with DAG $\mathcal{G}_0$ and $pa_Y\neq\emptyset$.  Let Condition ??? hold. Then, $S_{\mathcal{F}_F}(Y) = pa_Y$. 
%\end{lemma}

\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{figures/Mixed Graph.png}
\caption{DAGs corresponding to Example \ref{ExampleEasyDAG} (Graph A), Example \ref{Teaser example with Gaussian assumptions} (Graph B), Example \ref{ANMexample}  (Graph B), Example \ref{ExampleParetoFinal} (Graph C) }
\label{Mixed_Graph}
\end{figure}





















%In particular, if $\mathcal{F} = \mathcal{F}_F$ where $F$ is a distribution function with $q$ parameters, then we say that a function $\theta:\mathbb{R}^k\to\mathbb{R}^q$ is  $\mathcal{F}_F-$unseparable wrt. $\textbf{X}$ if for all $S\subsetneq\{1, \dots, k\}$ and for all $z\in(0,1)$ holds 
% $$
% \nexists \tilde{\theta}:\mathbb{R}^{|S|}\to\mathbb{R}^q: F( F^{-1}(z; \theta(\textbf{X}));\tilde{\theta(X_S)})  \indep X_S.
% $$










