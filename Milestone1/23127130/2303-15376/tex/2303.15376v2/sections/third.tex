%Assymetrical CPCM(F1, F2) erase assymetrical
%T(\cdot) erase \cdot a daj pozor na T(x) v proposition 1 and later vymaz x



\section{Identifiability results}
\label{Section_identifiability}
In this section, we are interested in whether it is possible to infer a causal graph from a joint distribution under the assumptions presented in the previous section. First, we rephrase the notion of identifiability from Definition \ref{IdentifiabilityPrvaDefinicia}. 

\begin{definition}[Identifiability]\label{DEFidentifiability}
Let $F_{(X_1, X_2)}$ be a distribution that has been generated according to the asymmetrical ($\mathcal{M}_1, \mathcal{M}_2)$-causal model. We say that the causal graph is identifiable from the joint distribution (equivalently, that the model is identifiable, or that there does not exist a backward model) if exactly one of the bullet points from Definition~\ref{Asymmetrical_causal_model} can generate the distribution $F_{(X_1, X_2)}$.  

Specifically, let $F_{(X_1, X_2)}$ be a distribution that has been generated according to the $CPCM(F)$ model ( \ref{BCPCM}) with graph $X_1\to X_2$. We say that the causal graph is identifiable from the joint distribution, if there does \textit{not} exist 
$\tilde{\theta}$ and a pair of random variables $\tilde{\varepsilon}_2\indep\tilde{\varepsilon}_1$, where $\tilde{\varepsilon}_1$ is uniformly distributed, such that the model $X_2=\tilde{\varepsilon_2},  X_1=  F^{-1}\big(\tilde{\varepsilon}_1;\tilde{\theta}(X_2)\big)$ generates the same distribution $F_{(X_1,X_2)}$. 
\end{definition}

In the rest of the section, we answer the following question: Under what conditions is the causal graph identifiable? We start with the $CPCM(F)$ case. 
\subsection{Identifiability in $CPCM(F)$}

We consider only continuous random variables, but similar results can also be derived for discrete random variables. 

First, we deal with the important Gaussian case. Recall that in the additive Gaussian model where $X_2=f(X_1)+\varepsilon_2$, $\varepsilon_2\sim N(0, \sigma^2)$, the identifiability holds if and only if $f$ is non-linear \citep{hoyer2009}. We provide a different result with both mean \textit{and} variance as functions of the cause. A similar result can be found in \cite[Theorem 1]{Khemakhem_autoregressive_flows} in the context of autoregressive flows and where a sufficient condition for identifiability is provided. Another similar problem is studied in \cite{immer2022identifiability} and \cite{strobl2022identifying}, both showing identifiability in more general location-scale models. 

\begin{theorem}[Gaussian case]\label{normalidentifiability}
Let $(X_1,X_2)$ admit the model ( \ref{BCPCM}) with graph $X_1\to X_2$ and the Gaussian distribution function $F$ with parameters $\theta(X_1)=\big(\mu(X_1), \sigma(X_1)\big)^\top$.~\footnote{Such as in Example \ref{Gaussian case}, this can be rewritten as $X_2 = \mu(X_1)+\sigma(X_1)\varepsilon_2, \,\varepsilon_2 \text{ is Gaussian}$.}

Let $p_{\varepsilon_1}$ be the density of $\varepsilon_1$ that is absolutely continuous with full support $\mathbb{R}$. Let $\mu(x), \sigma(x)$ be two times differentiable.  Then, the causal graph is identifiable from the joint distribution if and only if there do not exist $a,c,d,e, \alpha, \beta\in\mathbb{R}$,  
$a\geq 0,c>0, \beta>0$, such that
\begin{equation}\label{norm}
\frac{1}{\sigma^2(x)}=ax^2 + c, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \frac{\mu(x)}{\sigma^2(x)}=d+ex,
\end{equation}
for all $x\in\mathbb{R}$ and
\begin{equation}\label{DensityDEF}
p_{\varepsilon_1}(x) \propto \sigma(x)e^{-\frac{1}{2}\big[ \frac{(x-\alpha)^2}{\beta^2}  - \frac{\mu^2(x)}{\sigma^2(x)}\big]},
\end{equation}
where $\propto$  means "is proportional to" (here, $p_{\varepsilon_1} $  is a valid density function if and only if $\frac{1}{\beta^2}\neq  \frac{e^2}{c}\mathbbm{1}[a=0]$). 
Specifically, if $\sigma(x)$ is constant (case $a=0$), then the causal graph is identifiable unless $\mu(x)$ is linear and $p_{\varepsilon_1}$ is the Gaussian density. 
\end{theorem}

The proof is provided in \hyperref[Proof of normalidentifiability]{Appendix} \ref{Proof of normalidentifiability}. Moreover, a distribution of an unidentifiable Gaussian case with $a=c=d=e=\alpha = \beta=1$ can be found in the   \hyperref[Proof of normalidentifiability]{Appendix} \ref{Proof of normalidentifiability}, Figure \ref{GaussianDensity}. 

Theorem \ref{normalidentifiability} shows that the non-identifiability holds only in the "special case", when $\frac{\mu(x)}{\sigma^2(x)}, \frac{-1}{2\sigma^2(x)}$ are linear and quadratic, respectively. Note that natural parameters of a Gaussian distribution are  $\frac{\mu}{\sigma^2}, \frac{-1}{2\sigma^2}$, and sufficient statistics of the Gaussian distribution have a linear and quadratic form (for the definition of the exponential family, natural parameter and sufficient statistic, see \hyperref[appendix]{Appendix} \ref{appendix_exponential_family}). We show that such connections between non-identifiability and sufficient statistics hold in the more general context of the exponential family.  

\begin{proposition}[General case, one parameter]\label{Necessary condition for identifiability}
Let $q=1$. Let $(X_1, X_2)$ admit the model ( \ref{BCPCM}) with graph $X_1\to X_2$, where $F$ lies in the exponential family of distributions with a sufficient statistic $T$, where $T$ is continuous. 
If there do not exist $a,b\in\mathbb{R}$, such that
 \begin{equation}\label{eq000}
     \theta(x)=a\cdot T(x)+b,\,\,\,\,\,\,\,\,\,\,\,\,\, \forall x\in supp(X_1),
 \end{equation} 
then the causal graph is identifiable. 

Moreover,  the causal graph is identifiable, if there does not exist $c\in\mathbb{R}$, such that 
\begin{equation}\label{eq007}
p_{X_1}(x)\propto \frac{h_1(x)}{h_2[\theta(x)]}e^{cT(x)}, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \forall x\in supp(X_1),
\end{equation}
where $h_1$ is a base measure of $F$, and $h_2$ is the normalizing function of $F$ defined in \hyperref[appendix]{Appendix} \ref{appendix_exponential_family}. 
\end{proposition}
Note that the causal graph in (\ref{BCPCM}) is trivially identifiable if $X_1$ and $X_2$ have different supports.
\begin{proof}
We first show that if the causal graph is not identifiable, then (\ref{eq000}) holds for some $a,b\in\mathbb{R}$.

If the graph is not identifiable, there exists a function $ \tilde{\theta}$, such that 
causal models $X_1 = \varepsilon_1, X_2 = F^{-1}\big(\varepsilon_2; \theta(X_1)\big)$ and $X_2 = \varepsilon_2, X_1 = F^{-1}\big(\varepsilon_1; \tilde{\theta}(X_2)\big)$ generate the same joint distribution.
 Decompose the joint density
\begin{equation}\label{eq69}
  p_{(X_1, X_2)}(x,y) = p_{X_1}(x)p_{X_2\mid {X_1}}(y\mid x) = p_{X_2}(y)p_{{X_1}\mid {X_2}}(x\mid y),\,\,\,\,\,\,\,\, x,y\in supp(X_1).
 \end{equation}
Since $F$ lies in the exponential family of distributions, we use the notation from \hyperref[appendix]{Appendix} \ref{appendix_exponential_family} and rewrite $$p_{X_2\mid {X_1}}(y\mid x) = h_{1}(y)h_{2}[\theta(x)]\exp[\theta(x)T(y)]$$
and analogously for $p_{{X_1}\mid {X_2}}(x\mid y)$. Then, the second equality in (\ref{eq69}) reads as follows
\begin{equation}\label{eq098}
    \begin{split}
  &   p_{X_1}(x)h_{1}(y)h_{2}[\theta(x)]\exp[\theta(x)T(y)] =p_{X_2}(y) h_{1}(x)h_{2}[\tilde{\theta}(y)]\exp[\tilde{\theta}(y)T(x)],\\&
\underbrace{\log\bigg(  \frac{ p_{X_1}(x)h_{2}[\theta(x)]}{h_{1}(x)}\bigg)}_{f(x)}+ \underbrace{\log\bigg(\frac{ p_{X_2}(y)h_{2}[\tilde{\theta}(y)]}{h_{2}(y)}\bigg)}_{g(y)} = \tilde{\theta}(y)T(x)-\theta(x)T(y),
\end{split}
\end{equation}
where the second equation is the logarithmic transformation of the first equation after dividing both sides by $h_1$ and $h_2$. Since the left side is in the additive form, Lemma \ref{PomocnaLemma1} directly gives us (\ref{eq000}) (to see this without Lemma \ref{PomocnaLemma1}, apply $\frac{d}{dx dy}$  to (\ref{eq098}) and fix $y$ such that $T'(y)\neq 0$. We get 
$\theta'(x) = \frac{\tilde{\theta}'(y)}{T'(y)}T'(x)$. Integrating this equality with respect to $x$ gives (\ref{eq000}). However, using Lemma \ref{PomocnaLemma1}, we do not need to assume the differentiability of $\theta$).

As for equation (\ref{eq007}), the equality (\ref{eq098}) implies 
$f(x) + g(y) = a_1T(x) + a_2T(y) +a_3$ for some constants $a_, a_2, a_3\in\mathbb{R}$. Therefore, fixing $y$ gives $f(x)=\log\bigg\{  \frac{ p_{X_1}(x)h_{2}[\theta(x)]}{h_{1}(x)}\bigg\} = a_1T(x) + const$. Rewriting this gives 
$
p_{X_1}(x)= \frac{h_1(x)}{h_2[\theta(x)]}e^{a_1T(x) + const},
$
which is exactly the form of (\ref{eq007}). 
\end{proof}


Proposition \ref{Necessary condition for identifiability} shows that the only case in which the model is not identifiable is when $\theta(x)$ has a uniquely given functional form (unique up to a linear transformation), and the density of the cause is uniquely given (unique up to one additional parameter $c$).  Note that it is only a sufficient, not a necessary, condition for identifiability since some specific choices of $a,b,c$ can still lead to identifiable cases.  We show  the usage of Proposition \ref{Necessary condition for identifiability} for a Pareto model. 

\begin{consequence}\label{paretoidentifiability}
Let $(X_1,X_2)$ admit the model ( \ref{BCPCM}) with graph $X_1\to X_2$, where $F$ is the Pareto distribution function such as in Example \ref{Pareto case}. Then, the causal graph is \textit{not} identifiable if and only if 
\begin{equation}\label{eq50}
\theta(x) = a\log(x) +b,\,\,\,\,\,\,\,\, p_{X_1}(x) \propto \frac{1}{ [a\log(x)+b] x^{d+1} }, \,\,\,\,\,\,\,\,\forall x\geq 1,
\end{equation}
for some $a,b,d>0$. 
\end{consequence}
The proof is provided in \hyperref[Proof of pareto identifiability]{Appendix} \ref{Proof of pareto identifiability}. Note that if $a=0$, then $X_1\indep X_2$ and the (empty) graph is trivially identifiable. However, we assumed a non-constant $\theta$ , which implies $a\neq 0$.  If $\theta, p_{X_1}$ are such as in (\ref{eq50}), the backward model exists and has the form $\tilde{\theta}(y) = a\log(y)+d$ and $p_{X_2}(y)  \propto \frac{1}{ [a\log(y)+d] x^{b+1} }$, where $X_2\to X_1$. 



\subsection{Identifiability in $CPCM(F_1, F_2)$ models}


Similar sufficient conditions as in Proposition \ref{Necessary condition for identifiability} can be derived for a more general case, when $F$ has several parameters and for the $CPCM(F_1, F_2)$. 
The following theorem shows that the $CPCM(F_1, F_2)$ model is "typically" identifiable.  


\begin{theorem}
\label{thmAssymetricMultivariatesufficient}
Let $(X_1, X_2)$ follow the $CPCM(F_1, F_2)$ defined as in (\ref{asymetrical_F_one_F_two_model}), where $F_1, F_2$ lie in the exponential family of continuous distributions and $T_1 = (T_{1,1}, \dots, T_{1,q_1})^\top$, $T_2 = (T_{2,1}, \dots, T_{2,q_2})^\top$are the corresponding sufficient statistics with a nontrivial intersection of their support $\mathcal{S}:=supp(F_1)\cap supp(F_2)$.

The causal graph is identifiable, if $\theta_2$ is not a linear combination of $T_{1,1}, \dots, T_{1,q_1}$ on $\mathcal{S}$. That is, if $\theta_2$ can not be written as
\begin{equation}\label{eq158}
\theta_{2,i}(x) \overset{}{=} \sum_{j=1}^{q_1}a_{i,j}T_{1,j}(x)+b_i,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \forall x\in \mathcal{S},
\end{equation}
for all $i=1, \dots, q_2,$ and for some constants $a_{i,j},b_i\in\mathbb{R}$, $j=1, \dots, q_1$. 
\end{theorem}

The proof is provided in \hyperref[Proof of thmAssymetricMultivariatesufficient]{Appendix} \ref{Proof of thmAssymetricMultivariatesufficient}. Note that condition (\ref{eq158}) is sufficient, but not necessary for identifiability. Similarly as in the Gaussian case (Theorem \ref{Gaussian case}) or Pareto case (Consequence \ref{Pareto case}), in order to obtain a necessary condition, the distribution of the cause also needs to be restricted.

The following example illustrates the usage of Theorem \ref{thmAssymetricMultivariatesufficient} on Gamma and Beta distributions. 

\begin{consequence}\label{consequenceprva}
\begin{itemize}
\item Let  $(X_1,X_2)$  admit the $CPCM(F)$ model ( \ref{BCPCM}) with graph $X_1\to X_2$, where $F$ is a Gamma distribution with parameters $\theta=(\alpha, \beta)^\top$. If there do not exist constants $a,b,c,d,e,f\in\mathbb{R}$ such that 
\begin{equation*}
\alpha(x) = a\log(x) + bx + c, \,\,\,\,\,\beta(x) = d\log(x)+ex+f,\,\,\,\,\forall x>0,
\end{equation*}
then the causal graph is identifiable.
\item Let $(X_1,X_2)$ admit the $CPCM(F_1, F_2)$ as in (\ref{asymetrical_F_one_F_two_model}), where $F_1$ is the Gamma distribution function with parameters $\theta_1=(\alpha_1, \beta_1)^\top$ and $F_2$ is the Beta distribution function with parameters $\theta_2=(\alpha_2, \beta_2)^\top$. If there do not exist constants $a_i,b_i,c_i,d_i, e_i, f_i\in\mathbb{R}$, $i=1,2$, such that for all $x\in(0,1)$ holds \begin{equation*}\begin{split}
&\alpha_1(x) = a_1\log(x) + b_1x+c_1,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \beta_1(x) = d_1\log(x) +e_1x+f_1,\\&
\alpha_2(x) = a_2\log(x) + b_2\log(1-x)+c_2,\,\,\,\,\,\beta_2(x) = d_2\log(x) +e_2\log(1-x)+f_2,
         \end{split}
         \end{equation*}
then the causal graph is identifiable.
\end{itemize}
\end{consequence}
Details about the Consequence \ref{consequenceprva}, the definitions of the distributions, and their sufficient statistics can be found in \hyperref[consequence]{Appendix} \ref{consequence}. We stress that not all constants $a_i,b_i,c_i,d_i,e_i,f_i, i=1,2,$ give a valid (non-identifiable) model.










