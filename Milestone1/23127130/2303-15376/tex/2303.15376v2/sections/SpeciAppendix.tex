\section{Theoretical details and detailed definitions}
\label{appendix}
\subsection{Exponential family}\label{appendix_exponential_family}

The exponential family is a set of probability distributions whose probability density function can be expressed in the form
\begin{equation}\label{Exponential family of distributions}
f(x;\theta) = h_1(x)h_2(\theta)\exp\big[\sum_{i=1}^q\theta_iT_i(x)\big],
\end{equation}
where $h_1, T_i$ are real functions and $h_2:\mathbb{R}^q\to\mathbb{R}^+$ is a vector-valued function. We call $T_i$ a \textit{sufficient} statistic, $h_1$ a base measure and $h_2$ a normalizing (or partition) function.  

Often, form (\ref{Exponential family of distributions}) is called a canonical form, and $$f(x;\theta) = h_1(x)h_2(\theta)\exp\big[\sum_{i=1}^qh_{3,i}(\theta)T_i(x)\big],$$ where  $h_{3,i}:\mathbb{R}^q\to\mathbb{R}, i=1, \dots, q$, is called its \textit{reparametrization} (natural parameters are a specific form of the reparametrization). We always work only with a canonical form (attention for Gaussian distribution, where standard form is not in the canonical form). 

Many important distributions lie in the exponential family of continuous distributions, such as Gaussian, Pareto (with fixed support), log-normal, Gamma or Beta distribution, to name a few. 

Functions in (\ref{Exponential family of distributions}) are \textit{not} uniquely defined. For example, $T_i$ are unique up to a linear transformation. 

The support of $f$ (defined as $supp(f) = \{x\in\mathbb{R}: f(x;\theta)>0\}$) is fixed and does not depend on $\theta$. Potentially, $T_i, h_1$ do not have to be defined outside of this support, however, we will typically neglect this fact (or possibly define $h_1(x) = T_i(x) = 0$ for $x$ where these functions are not defined). If we assume that $f$ is continuous (which is typically the case in this paper), we additionally assume that the support is non trivial in sense that it contains some open interval. 

Without loss of generality, we always assume that $q$ is minimal in sense that we can not write $f(x;\theta)$ using only $q-1$ parameters. Then, $T_1, \dots, T_q$ are linearly independent in the following sense: there exist $x_1, \dots, x_q\in supp(f)$ such that a matrix 
\begin{equation}\label{eq2431087}
\begin{pmatrix}
T_{1}(x_1) & \cdots & T_{{q}}(x_1) \\
\cdots & \cdots & \cdots \\
T_{1}(x_q) & \cdots & T_{{q}}(x_{q}) 
\end{pmatrix} 
\end{equation}
has full rank. Moreover, $T_1, \dots, T_q$ are affinly independent in the following sense: there exist $y_0, y_1, \dots, y_n\in supp(f)$ such that a matrix 
\begin{equation}\label{eq145151}
\begin{pmatrix}
T_{1}(y_1) - T_1(y_0) & \cdots & T_{{q}}(y_1) -T_q(y_0)\\
\cdots & \cdots & \cdots \\
T_{1}(y_q) -T_1(y_0) & \cdots & T_{{q}}(y_{q}) -T_q(y_0)
\end{pmatrix} 
\end{equation}
has full rank. In this paper (in particular in Lemma \ref{PomocnaLemma1}), we assume that  $T_1, \dots, T_q$ are affinly independent (satisfy (\ref{eq145151})). 

Since the notions of  linear and affine independence is nonstandard, consider the following example. Say that $T_1(x) = x, T_2(x) = x^2$ (sufficient statistics in Gaussian distribution). Then, matrices (\ref{eq2431087}) and (\ref{eq145151}) read as
\begin{equation*}
M_1=\begin{pmatrix}
x_1 &  x_1^2\\
x_2  & x_{2}^2
\end{pmatrix} , \,\,\,\,
M_2=\begin{pmatrix}
y_1-y_0 &  y_1^2 -y_0^2\\
y_2 -y_0 &  y_2^2 -y_0^2
\end{pmatrix} ,
\end{equation*}
which are trivially full ranked for a choice $(x_1, x_2) = (1,2)$ and $(y_0, y_1, y_2) = (0,1,2)$, for example. 






\section{Simulations and application details}
\label{Appendix_simulations}

Plots corresponding to the datasets from Simulations 2 and Simulations 3 are drawn in Figure \ref{Simulations2_plots} and Figure \ref{Simulations3_plot}. 
Plots corresponding to the application are drawn in Figure \ref{Just_plots} and Figure \ref{Just_histograms} . 

\subsection{Baselines implementations from Simulations 2}
As mentioned before, the experiments from Simulations 2 were inspired by \cite{Natasa_Tagasovska} and implementations of other baseline methods are also taken from \cite{Natasa_Tagasovska} and \cite{immer2022identifiability}. 

For LOCI, we use the default format with neural network estimations and subsequent independence testing (also denoted as $NN-LOCI_H$ \cite{immer2022identifiability}).
For IGCI, we use the original implementation from \cite{IGCI} with slope-based estimation with Gaussian and uniform reference measures. For RESIT, we use the implementation of \cite{Peters2014} with GP regression and the HSIC independence test with a threshold value $0.05$. For the Slope algorithm, we use the implementation of \cite{Slope}, with the local regression included in the fitting process. If interested for comparisons with other methods such as PNL, GPI-MML, ANM, Sloppy, GR-AN, EMD, GRCI, see Section 3.2 in \cite{Natasa_Tagasovska} and Section 5 in \cite{immer2022identifiability}. 


\begin{figure}[ht]
\centering
\includegraphics[scale=0.65]{figures/Simulation2_plots.pdf}
\caption{ An example of generated datasets from different generating models in Simulations 2.}
\label{Simulations2_plots}
\end{figure}



\begin{figure}[ht]
\centering
\includegraphics[scale=0.65]{figures/Simulations3_Pareto.pdf}
\caption{Plots corresponding to Simulations 3. An example of a randomly generated function $\theta$ and a generated dataset where $effect\mid cause\sim Pareto(\theta(cause))$. Note that if $\theta(x)$ is small, then $effect\mid cause=x$ will have heavy tails. If $\theta(x)<1$, then the expectation of $effect\mid cause=x$ does not exists.}
\label{Simulations3_plot}
\end{figure}



\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{figures/3ploty.pdf}
\caption{ Total income ($X_1$), Food expenditure ($X_2$) and Alcohol beverages expenditure ($X_3$).  }
\label{Just_plots}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{figures/histograms.pdf}
\caption{ Total income ($X_1$), Food expenditure ($X_2$) and Alcohol expenditure ($X_3$).  }
\label{Just_histograms}
\end{figure}




