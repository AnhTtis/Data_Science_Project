\section{Introduction}

%Lemma jedna este raz poriadne pochop
Having knowledge about causal relationships rather than statistical associations enables us to predict the effects of actions that perturb the observed system \citep{TheBookOfWhy}. Determining causal structures is a fundamental problem in many scientific fields. However, several data-generating processes can produce the same observational distribution. Observing the system after interventions is a reliable way to find the causal structure, but in many real-life scenario, interventions can be too expensive, unethical \citep{epidem_application} or impossible to observe. Hence, estimating the causal structure from observational data has become an important topic.

In the last years, much effort has been put into developing mathematical background for a "language" of causal inference  \citep{Pearl_book}. The theory stands on the structural causal model (SCM) for random variables $\textbf{X} = (X_1, \dots, X_d)^\top\in\mathbb{R}^d$, with structural equations in the form $X_i=f_i(\textbf{X}_{pa_i}, \varepsilon_i)$, where $f_i\in\mathcal{A}$ are the causal functions belonging to some subclass of real functions $\mathcal{A}$, $pa_i$ are the direct causes of $X_i$, and $(\varepsilon_1, \dots, \varepsilon_d)^\top$ are independent noise variables. SCM is an important mathematical concept, and the usual goal of causal discovery is to estimate the causal structure (causal graph $\mathcal{G}$ associated with SCM) or the direct causes of some variable of interest. However, estimating the causal structure is impossible without strong assumptions on the class $\mathcal{A}$
\citep{Elements_of_Causal_Inference}. 

Many results appear in the literature, with various methods for causal inference under different assumptions on the SCM \citep{ZhangReview}.  If we observe several environments after different interventions, the assumptions can be much less restrictive \citep{Peters_invariance}.  If we aim to discover causal relations based only on an observed random sample, the assumptions are usually more strict; assuming linear structure or additive noise \citep{Peters2014}.

The additivity assumption ($X_i = f_i(\textbf{X}_{pa_i}) \textbf{+} \varepsilon_i$) implies that $\textbf{X}_{pa_i}$ affects only the mean of $X_i$. One well-known approach that allows more general effects of the covariates is PNL (post-nonlinear model \citep{Zhang2010}). This model potentially allows an effect of  $\textbf{X}_{pa_i}$ on the variance of $X_i$, but it does not allow arbitrary relations. Quadratic variance functions (QVF) DAG models introduced in \cite{ParkVariance} consider the variance to be a quadratic function of the mean, and location-scale models defined in \cite{immer2022identifiability} allow arbitrary effect on the variance. However, these models assume a fixed tail and fixed higher moments. 

In this paper, we develop a framework where $\textbf{X}_{pa_i}$ can arbitrarily affect the mean, variance, tail or other characteristics of $X_i$. However some cautions have to be taken as if $\textbf{X}_{pa_i}$ affects $X_i$ in a certain way, the causal structure will become unidentifiable (several causal structures can produce the same distribution of $\textbf{X}$). We expose which assumptions are reasonable in order to have an identifiable causal structure.

In some applications, for example, it may be reasonable to assume that $$X_i\mid \textbf{X}_{pa_i}\sim Pareto\big(\theta(\textbf{X}_{pa_i})\big),$$where $\theta$ is the function describing the tail behaviour. Another useful model that allows an arbitrary effect on the variance as well as on the mean is $X_i\mid \textbf{X}_{pa_i}\sim N\big(\mu(\textbf{X}_{pa_i}), \sigma^2(\textbf{X}_{pa_i})\big)$ for some functions $\mu, \sigma$, in which case the structural equation has the form 
\begin{equation}\label{prva_gaussian_equation}
X_i =  \mu(\textbf{X}_{pa_i})+ \sigma(\textbf{X}_{pa_i})\cdot \varepsilon_i, \,\,\,\,\,\,\,\,\,\,\varepsilon_i\text{ is Gaussian.}
\end{equation}
These assumptions are reasonable, especially when our variables arise from a limiting theorem. For example, the central limit theorem can justify (\ref{prva_gaussian_equation}) if $\textbf{X}$ is an appropriate sum of independent events. 

In Section \ref{Section2}, we introduce a causal model (we call it Conditionally Parametric Causal Model or CPCM) where the structural equation has the form 
\begin{equation}\label{11}
X_i=f_i(\textbf{X}_{pa_i}, \varepsilon_i) = F^{-1}(\varepsilon_i; \theta(\textbf{X}_{pa_i})), \,\,\,\,\,\text{ equivalently } X_i\mid \textbf{X}_{pa_i}\sim F\big(\theta(\textbf{X}_{pa_i})\big), 
\end{equation}
where $F$ is a known distribution function with a vector of parameters $\theta(\textbf{X}_{pa_i})$  that depends on the parents of $X_i$ and $\varepsilon_i\sim U(0,1)$. We restrict our focus mostly to the case where $F$ belongs to the exponential family of continuous distributions when sufficient statistics can be used. Section \ref{Section_identifiability} provides the identifiability results of the causal structure in the bivariate case. Assuming (\ref{11}), is it possible to infer the causal structure based only on observational data?
Section \ref{Section4} discusses the multivariate extension. 
%Specific example
In Section \ref{Section5}, we propose an algorithm for estimating the causal graph under assumption (\ref{11}). In Section \ref{simulations_section}, we proceed to a short simulation study and in Section \ref{Section7} we illustrate our methodology on a dataset concerning income and expenditure of residents of Philippines. 

We provide three appendices: Appendix \ref{SectionProofs} contains proofs of the theorems and lemmas presented in the paper. Appendix \ref{appendix} contains detailed definitions and explanations for some concepts that were omitted from the main part of the paper for the sake of clarity. Appendix \ref{Appendix_simulations} concerns the simulation study and the application. 


\subsection{Setup and notation}
\label{Setup}
In this section, we borrow the notations of \cite{PCalgorithm} and introduce the notation of graphical causal models.
A DAG $\mathcal{G}=(V,E)$ (directed acyclic graph) contains a finite set of vertices (nodes) $V$ and a set of directed edges $E$ between distinct vertices. We assume that there exists no directed cycle or multiple edges. 

Let $i,j\in V$ be two distinct nodes of $\mathcal{G}$. We say that $i$ is a parent of $j$ if there exists an edge from $i$ to $j$ in $E$, and we note $i\in  pa_{j}(\mathcal{G})$. Moreover, such $j$ is called a child of $i$, with notation $j\in ch_i(\mathcal{G})$. We say that $i$ is an ancestor of $j$, if there exists a directed path from $i$ to $j$ in $E$, notation $i\in an_{j}(\mathcal{G})$. In the other direction, we say that $j$ is a non-descendant of $i$, if $i\not\in an_{j}(\mathcal{G})$, notation $j\in nd_{i}(\mathcal{G})$. We say that the node $i\in V$ is a source node if $pa_j(\mathcal{G})=\emptyset$, notation $i\in Source(\mathcal{G})$.  We omit the argument $\mathcal{G}$ if evident from the context. 

We use capital $F$ for distributions and small $p$ for densities. Consider a random vector $\textbf{X }= (X_i)_{i\in V}$ over some probability space with distribution $F_\textbf{X}$. With 
a slight abuse of notation, we identify the vertices $j \in V$ with the variables $X_j$.  We denote $\textbf{X}_S = \{X_s: s\in S\}$ for $S\subseteq V$. We focus on the case in which $\textbf{X}$ has a continuous joint density function $p_\textbf{X}$ with respect to the Lebesgue measure. 

The distribution $F_\textbf{X}$ is Markov with respect to $\cal{G}$  if $A\indep_{\cal{G}} B\mid C \implies A\indep B\mid C$ for $A,B,C\subseteq V$ are disjoint subsets of the vertices and  $\indep_{\cal{G}}$ represent a d-separation in $\mathcal{G}$ \citep{Pearl}. On the other hand, if for all $A,B,C\subseteq V$ disjoint subsets of the vertices hold $A\indep_{\cal{G}} B\mid C \impliedby A\indep B\mid C$, we say that the distribution is faithful with respect to $\mathcal{G}$.   A distribution satisfies causal minimality with respect to $\mathcal{G}$ if it is Markov with respect to $\mathcal{G}$, but not to any proper subgraph of $\mathcal{G}$. It can be shown that faithfulness implies causal minimality \citep[Proposition 6.35]{Elements_of_Causal_Inference}. Two graphs $\mathcal{G}_1, \mathcal{G}_2$ are Markov equivalent if the set of distributions that are Markov with respect to $\mathcal{G}_1, \mathcal{G}_2$ is the same. We denote the Markov equivalency class of $\mathcal{G}$ as the set $\{\mathcal{G}':  \mathcal{G}\text{ and } \mathcal{G}' \text{ are Markov equivalent}\}$. 
A set of variables $\textbf{X}$ is said to be causally sufficient if there is no hidden common cause that causes more than one variable in $\textbf{X}$ \citep{Sprites2010}. The main concept in causal inference is the SCM \citep{Pearl}, defined as follows. 


%SCM
\begin{definition}[SCM]\label{def1}
The random vector $\textbf{X}=(X_1, \dots, X_d)$ follows the SCM with DAG $\cal{G}$  if for each $i\leq d$ the variable $X_i$ arises from the structural equation 
$$
S_i:   \,\,\,\, X_i=f_i\big(\textbf{X}_{pa_i(\mathcal{G})}, \varepsilon_i\big),
$$
where $\varepsilon = (\varepsilon_1, \dots, \varepsilon_d)$ is jointly independent and $f_i$ are some measurable functions.
\end{definition}

The SCM uniquely defines a distribution of $\textbf{X}$ that can be decomposed into a product of conditional densities or causal Markov kernels \citep[Definition 6.21]{Elements_of_Causal_Inference}
\begin{equation}\label{def987}
p_\textbf{X}(\textbf{x}) = \prod_{i\in V} p_i(x_i\mid \textbf{x}_{pa_i}),
\end{equation}
where $p_i(\cdot \mid \textbf{x}_{pa_i})$ represents the conditional density function of the random variable $X_i$ conditioned on its parents $\textbf{X}_{pa_j}=\textbf{x}_{pa_j}$. 
Note that every distribution that is Markov with respect to $\mathcal{G}$ can be decomposed to (\ref{def987}).

A core concept in causal inference is the identifiability of the causal structure. It is straightforward to compute $F_\textbf{X}$ if the DAG $\mathcal{G}$ and the Markov kernels (\ref{def987}) are given. However, we deal with the opposite problem, where $F_\textbf{X}$ is given (or a random sample from $F_\textbf{X}$), and we want to infer $\mathcal{G}$. 

Consider the SCM from Definition \ref{def1} with conditional densities satisfying (\ref{def987}). 
Let  $\mathcal{G}\in DAG(d)$ where $DAG(d)$ is the set of all DAGs over $V=\{1, \dots, d\}$ and assume that $p_i\in\mathcal{H}_i$, where $\mathcal{H}_i$ is the subset of all conditional density functions. Let $\mathcal{H} = \mathcal{H}_1\times \dots \times \mathcal{H}_d$. The given pair $(\mathcal{G}, p)\in DAG(d)\times \mathcal{H}$ generates the density $p_p(\textbf{x}) = \prod_{i\in V} p_i(x_i\mid \textbf{x}_{pa_i})$.  

\begin{definition}\label{IdentifiabilityPrvaDefinicia}
We say that the pair $(\mathcal{G}, p)\in DAG(d)\times \mathcal{H}$ is identifiable in $DAG(d)\times \mathcal{H}$ if there does \textit{not} exist a pair $(\mathcal{G}', p')\in DAG(d)\times \mathcal{H}$,  $\mathcal{G}'\neq \mathcal{G}$ such that $p_p= p_{p'}$. We say that the class $\mathcal{H}$ is identifiable over $DAG(d)$ if every  pair  $(\mathcal{G}, p)\in DAG(d)\times \mathcal{H}$ is identifiable in $\mathbb{G}_d\times \mathcal{H}$.  
\end{definition}

Without strong restrictions of the class $\mathcal{H}$, we cannot obtain the identifiability of $\mathcal{G}$. Generally, we can only identify the Markov equivalency class \citep[Proposition 7.1.]{Elements_of_Causal_Inference}. 
In Section 3, we rephrase the definition of identifiability from Definition \ref{IdentifiabilityPrvaDefinicia} for the model (\ref{11}).

%\begin{lemma}(Non-uniqueness of graph structures \label{NonUniqueness}
% Consider a random vector $\textbf{X} = (X_1, \dots, X_d)$ with distribution $P_\textbf{X}$ that has a density with respect to Lebesgue measure and assume it is Markov with respect to $\cal{G}$. Then there exists an SCM with graph $\cal{G}$ that entails the distribution $P_\textbf{X}$.
%\end{lemma}
\subsection{Related work}

Several papers address the problem of the identifiability of the causal structure (for a review see \cite{ZhangReview}). 
\cite{Lingam} show identifiability for the LiNGaM class (Linear Non-Gaussian additive Models $X_i=\beta\textbf{X}_{pa_i} +\varepsilon_i$ for non-Gaussian noise variables $\varepsilon_i$). 
\cite{BuhlmannCAM} explore CAM (causal additive models $X_i = \sum_{j\in pa_i} g_j(X_j) +  \varepsilon_i$ for some smooth functions $g_j$).  
\cite{nonlinearLINGAM} and \cite{Peters2014} develop a framework for ANM (additive noise models where $X_i = g(\textbf{X}_{pa_i}) +  \varepsilon_i$). Under some (not too restrictive) conditions on $g$, they show the identifiability of such model  \citep[Corollary 31]{Peters2014} and propose an algorithm estimating $\mathcal{G}$ (for a review on ANM, see \cite{reviewANMMooij}). 
All these frameworks assume that the variance of $X_i\mid \textbf{X}_{pa_i}$ does not depend on $\textbf{X}_{pa_i}$. This is a crucial part of the identifiability results.

\cite{Zhang2009} consider the PNL model (Post-Nonlinear Causal Model) 
\begin{equation*}
X_i = g_1\big(g_2(\textbf{X}_{pa_i}) +  \varepsilon_i\big)
\end{equation*}
with an invertible link function $g_1$.  Here, potentially $X_i = g_2(\textbf{X}_{pa_i})\cdot \varepsilon$ for the special choice $g_1(x)=\log(x)$.  The PNL causal model has quite a general form (the former two are its special cases). However, it is identifiable under some technical assumptions and with a few exceptions \citep{Zhang2010}. 

 \cite{ParkPoisson, ParkVariance} show identifiability in models where $var[X_i\mid \textbf{X}_{pa_i}]$  is a quadratic function of $\mathbb{E}[X_i\mid \textbf{X}_{pa_i}]$. If $X_i\mid \textbf{X}_{pa_i}$ has a Poisson or Binomial distribution, such a condition is satisfied.  They also provide an algorithm based on comparing dispersions for estimating DAG in polynomial time. Other algorithms have been proposed, with comparable speed and different assumptions on the conditional densities \citep{PolynomialTimeAlgorithmCausalGraphs}. \cite{Galanti} consider the neural SCM with representation $X_i = g_1\big(g_2(\textbf{X}_{pa_i}), \varepsilon_i\big),$ where $g_1, g_2$ are assumed to be neural networks. 

Very recently, location-scale models in the form 
\begin{equation*}
X_i = g_1(\textbf{X}_{pa_i}) +  g_2(\textbf{X}_{pa_i})\varepsilon_i
\end{equation*}
received some attention. \cite{immer2022identifiability} showed that bivariate non-identifiable location-scale models must satisfy a certain differential equation. \cite{strobl2022identifying} considered the multivariate extension and estimating patient-specific root causes. 
\cite{Khemakhem_autoregressive_flows} showed more detailed identifiability results under Gaussian noise $\varepsilon_i$ in the bivariate case using autoregressive flows. \cite{xu2022inferring} considered more restricted location-scale model, dividing the range of the predictor variable into a finite set of bins and then fits an additive model in each bin.

Several different algorithms for estimating causal graphs were proposed, working with different assumptions \citep{IGCI, Score-based_causal_learning, Slope, Natasa_Tagasovska}.  They are often based on Kolmogorov complexity or independence between certain functions in a deterministic scenario. We provide more details about some of these relevant methods in Section~5. 

Some authors assume that causal Markov kernels lie in a parametric family of distributions. \cite{JanzingSecondOrderExponentialModels} consider the case where the density of  $X_i\mid \textbf{X}_{pa_i}$  lies in a second-order exponential family, and the variables are a mixture of discrete and continuous random variables. \cite{ParkGHD} concentrate on a specific subclass of the model (\ref{11}), where $F$ lies in a discrete family of Generalized Hypergeometric Distributions, that is, the family of random variables where the mean and variance have a polynomial relation. To the best of our knowledge, there does not exist any work in the literature, providing identifiability results in the case in which $F$ lies in a general class of continuous exponential family. That is the focus of this paper. 















































































































