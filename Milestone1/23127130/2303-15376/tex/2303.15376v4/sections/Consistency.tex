\subsection{}
\label{Appendix_consistency}

For the definition and discussion of the HSIC score, see \cite[Appendix A.1]{reviewANMMooij}. We use the same notation and implicitly use bounded non-negative Lipschitz-continuous kernels such that their product is characteristic, as in the "Data recycling" scenario in \cite[Corollary 21]{reviewANMMooij}.


In the following, we define an $F$-suitable estimator for a given distribution function $F$ with parameters $\theta$. This is a modification of the concept of a suitable estimator for the conditional expectation discussed in \cite[Appendix A.2]{reviewANMMooij}. In case when $F$ is location-distribution (such as Gaussian distribution with fixed variance), our results fully align with \citep{reviewANMMooij}. 




Let $(X_{i}, Y_{i})_{i=1}^n$ be a random sample from $(X, Y)$. We say that the estimator $\hat{\theta}$ is \textbf{F-suitable for $X \to Y$} if the following conditions are satisfied:

\begin{itemize}
    \item There exists $\theta$ such that $\hat{\theta}(x) \overset{P}{\to} \theta(x)$ as $n\to\infty$ for all $x\in supp(X)$. Moreover, if it is possible to write  $Y = F^{-1}(\varepsilon_2; \tilde{\theta}(X))$, with $X\indep \varepsilon_2\sim Unif(0,1)$, then this limit is equal to $\theta= \tilde{\theta}$,
    \item It holds that
\begin{equation}
    \label{gdert}
    \lim_{n\to\infty}\mathbb{E}_{}\left( \frac{1}{n}\sum_{i=1}^n(\varepsilon_i-\hat{\varepsilon}_i)^2 \right) = 0,
\end{equation}
where $\varepsilon_i := F(Y_{i}; \theta(X_{i}))$ and $\hat{\varepsilon}_i := F(Y_{i}; \hat{\theta}(X_{i})), i=1, \dots, n$ and where the expectation is taken with respect to the distribution of the random sample.
\end{itemize}

We say that the estimator $\hat{\theta}$ is \textbf{F-suitable} if it is F-suitable for both $X \to Y$ and $Y \to X$. We say that the estimator $\hat{\theta}$ is \textbf{$(F_1, \dots, F_k)$-suitable} if it is $F_i$-suitable for all $i = 1, \dots, k$ . We simply write that  $\hat{\theta}$ is ``suitable'' if $(F_1, \dots, F_k)$ are evident from the context. 

\begin{customprop}{\ref{consistency_proposition}}
Let $(X_1, X_2)$ follow an identifiable $CPCM(F_1, \dots, F_k)$ with DAG $\mathcal{G}$. Then, our score-based algorithm presented in Section~\ref{Section_score_based_algorithm} is consistent, meaning that
$$\hat{\mathcal{G}} \overset{P}{\to}\mathcal{G}\,\,\,as\,\,n\to\infty,$$
given that we employ a “suitable” estimation procedure for the estimation of $\hat{\varepsilon}_i$, we use HSIC score as our choice of $\rho$ and consistent estimates of $S_i$. 
\end{customprop}


\begin{proof}
The proof mostly aligns with the proof of Corollary 21 in \cite{reviewANMMooij}. We use the notation $X = X_1$ and $Y = X_2$.

If $X \indep Y$, then $\hat{\mathcal{G}}$ converges to an empty graph, since any other graph has a score of at least $\lambda$ and $\hat{\varepsilon}_1, \dots, \hat{\varepsilon}_d$ are independent by definition. From now on, without loss of generality, let $Y = F_j^{-1}(\varepsilon_2; \theta(X)), \varepsilon_2\indep X$ for some  $j \in \{1, \dots, k\}$. Denote by $\mathbf{x} = (x_1, \dots, x_n)$ and $\mathbf{y} = (y_1, \dots, y_n)$ the observed data. In the following, we compare the asymptotic scores of graphs $X\to Y$ and $Y\to X$. 

\textbf{Graph} $X \to Y$: Define the ``population residual'' \(E_Y := F_j(Y; \theta(X))\) and the ``estimated residual'' $\hat{\varepsilon}_Y^i = (F_i(y_l; \hat{\theta}(x_l)))_{l=1}^n$. For the true $i = j$, we omit the superscript and write $\hat{\varepsilon}_Y = (F_j(y_l; \hat{\theta}(x_l)))_{l=1}^n$. By construction \(X \perp\!\!\!\perp E_Y\) which implies \(\text{HSIC}(X, E_Y) = 0\) (due to Lemma 12 in \cite{reviewANMMooij}). 

Now, since the estimator $\hat{\theta}$ satisfies \eqref{gdert}, we can use the argument presented in \cite[Theorem 20]{reviewANMMooij}, and obtain $\widehat{HSIC}(\mathbf{x}, \hat{\varepsilon}_y) \xrightarrow{P} \text{HSIC}(X, E_Y)$. 

Since $\hat{S}_2$ is consistent, we can find $n_0$ such that for all $n\geq n_0$ holds $P(j\in\hat{S}_2)\geq 1-\delta$ for given $\delta>0$. 

Putting everything together, with probability larger than $1-\delta$ holds

\begin{equation*}
    \begin{split}
        s(X \to Y) &= \min_{j_2 \in \hat{S}_2}\rho(\mathbf{x}, \hat{\varepsilon}_y^{j_2}) + \lambda \leq \rho(\mathbf{x}, \hat{\varepsilon}_y) + \lambda \\
        &= \widehat{HSIC}(\mathbf{x}, \hat{\varepsilon}_y) + \lambda \xrightarrow{P} \text{HSIC}(X, E_Y) + \lambda = \lambda.
    \end{split}
\end{equation*}
By sending $\delta\to 0$, we obtain $  s(X \to Y)\xrightarrow{P} \lambda $. 

\textbf{Graph} $Y \to X$: Define the ``population residual'' \(E_X^j := F_j(X; \theta_j(Y))\) and the ``estimated residual'' $\hat{\varepsilon}_X^j = (F_j(x_i; \hat{\theta}_j(y_i)))_{i=1}^n$, where $\theta_j$ is the limit of $\hat{\theta}_j$ defined in the definition of $F_j$-suitability. 

Due to the assumption of identifiability, $Y\not\indep E_X^j$ for all $j\in\{1, \dots, k\}$. Therefore, using Lemma 12 in \cite{reviewANMMooij}, we have \(\text{HSIC}(Y, E_X^j) > 0\). Again, since the estimator $\hat{\theta}$ satisfies \eqref{gdert}, we can use the argument presented in \cite[Theorem 20]{reviewANMMooij}, and obtain $\widehat{HSIC}(\mathbf{y}, \hat{\varepsilon}_X^j) \xrightarrow{P} \text{HSIC}(Y, E_X^j)$. 


Putting everything together
\begin{equation*}
    \begin{split}
        s(Y \to X) &= \min_{j \in \hat{S}_1}\rho(\mathbf{y}, \hat{\varepsilon}_x^j) + \lambda \geq \min_{j \in \{1, \dots, k\}}\widehat{HSIC}(\mathbf{y}, \hat{\varepsilon}_X^j) + \lambda \\
        &  \xrightarrow{P} \min_{j \in \{1, \dots, k\}}\text{HSIC}(Y, E_X^j) + \lambda > \lambda, \,\,\,\,\,\,\,\,\,\,\,\text{as}\,\,n\to\infty.
    \end{split}
\end{equation*}
Therefore, the score for the correct direction is asymptotically smaller than that for the wrong causal direction as $n \to \infty$, hence the procedure is consistent.
\end{proof}