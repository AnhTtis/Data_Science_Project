
\subsubsection{Consistency}

\cite{reviewANMMooij} demonstrates that if $(X_1, X_2)$ follows a bivariate additive noise model, the ANM algorithm consistently estimates the causal direction between $X_1$ and $X_2$. We establish a similar result for $CPCM(F_1, \dots, F_k)$.

We employ a \textit{suitable} estimation procedure along with the HSIC score; refer to Appendix ??? for detailed explanations.

\begin{proposition}
Let $(X_1, X_2)$ follow an identifiable  $CPCM(F_1, \dots, F_k)$ with DAG $\mathcal{G}$. Then, our score based algorithm presented in Section~\ref{Section_score_based_algorithm} is consistent; that is,
$$\hat{\mathcal{G}} \overset{P}{\to}\mathcal{G}\,\,\,as\,\,n\to\infty,$$
given that we employ a \textit{suitable} estimation procedure for the estimation of $\hat{\varepsilon}_i$ and we use HSIC score as our choice of $\rho$ (for definitions, see  Appendix ???). 
\end{proposition}
For the definition and discussion about the HSIC score, see \cite[Appendix A]{reviewANMMooij}. We will use the same notation and we implicitly use  bounded non-negative Lipschitz-continuous kernels such that their product is characteristic, just as in “Data recycling” scenario in  \cite[Corollary 21]{reviewANMMooij}.  

Let $(X_{i}, Y_{i})_{i=1}^n$ be a random sample from $(X, Y)$ such that  $Y = F^{-1}(\varepsilon_2; \theta(X))$, $\varepsilon_2\indep X$. We say that the estimator $\hat{\theta}$ is \textbf{suitable for $X\to Y$}, if 

\begin{equation}
    \label{gdert}
    \lim_{n\to\infty}\mathbb{E}_{}\bigg( \frac{1}{n}\sum_{i=1}^n(\varepsilon_i-\hat{\varepsilon}_i)^2   \bigg) = 0,
\end{equation}
where the expectation is taken with respect to the distribution of the random sample and where $\varepsilon_i:=F(Y_{i}; \theta(X_{i}))$ and $\hat{\varepsilon}_i:=F(Y_{i}; \hat{\theta}(X_{i})), i=1, \dots, n$. We say that the estimator $\hat{\theta}$ is \textbf{suitable for $Y\to X$} if there exist $\theta$ such that $\hat{\theta}\overset{P}{\to}\theta$ uniformly and \eqref{gdert} holds for $\varepsilon_i:=F(X_{i}; \theta(Y_{i}))$ and $\hat{\varepsilon}_i:=F(X_{i}; \hat{\theta}(Y_{i})), i=1, \dots, n$.  We say that the estimator $\hat{\theta}$ is \textbf{suitable} if it is suitable for both  $X\to Y$ and $Y\to X$. 

\begin{proof}
The proof is analogous to the proof of  \cite[Corollary 21]{reviewANMMooij}.  We use notation $X = X_1$ and $Y = X_2$. 

If $X\indep Y$, then $\hat{\mathcal{G}}$  converges to an empty graph, since any other graph has score at least $\lambda$ and  $\hat{\varepsilon}_1, \dots, \hat{\varepsilon}_d$ are independent by definition. From now on, without loss of generality, let $X\to Y$ and let $Y = F_j^{-1}(\varepsilon_2; \theta(X))$ for some $j\in\{1, \dots, k\}$. Denote by $\textbf{x} = (x_1, \dots, x_n)$ and  $\textbf{y} = (y_1, \dots, y_n)$ the observed data. 

\textbf{Direction} $X\to Y$: Define ``population residual''  \(E_Y :=F_j(Y; \theta(X))\) and ``estimated residual'' $\hat{\varepsilon}_Y^{i} = (F_i(y_l; \hat{\theta}(x_l))_{l=1}^n$ and for true $i=j$ we simply write $\hat{\varepsilon}_Y = (F_j(y_l; \hat{\theta}(x_l))_{l=1}^n$. Trivially  \(X \perp\!\!\!\perp E_Y\) and hence by Lemma 12 in \cite{reviewANMMooij} we have \(\text{HSIC}(X, E_Y) = 0\). 

Now, since the estimator $\hat{\theta}$ is suitable, we can use the same argument as in \cite[Theorem 20]{reviewANMMooij} to obtain $\widehat{HSIC}(\textbf{x}, \hat{\varepsilon}_y ) \xrightarrow{P} \text{HSIC}(X, E_Y)$. Together we have 

\begin{equation*}
    \begin{split}
        s(X\to Y)  &= \min_{j_2\in S_2}\rho (\textbf{x }, \hat{\varepsilon}_y^{j_2}) +\lambda\leq  \rho (\textbf{x }, \hat{\varepsilon}_y) +\lambda\\&= \widehat{HSIC}(\textbf{x}, \hat{\varepsilon}_y )+\lambda \xrightarrow{P} \text{HSIC}(X, E_Y)+\lambda=\lambda. 
    \end{split}
\end{equation*}



\textbf{Direction} $Y\to X$: We will show that $s(Y\to X)$ converges to something strictly larger than $\lambda$. Without loss of generality, let $S_1 = \{1\}$ (if we show that  $s(Y\to X)$ converges to something strictly larger than $\lambda$ for one choice of $F_i$, then the same holds for minimum of finite number of choices). 

Define ``population residual''  \(E_X :=F_1(X; \theta(Y))\) and ``estimated residual'' $\hat{\varepsilon}_X = (F_1(x_i; \hat{\theta}(y_i))_{i=1}^n$. Due to an assumption of identifiability, $E_X\not\indep Y$. Therefore, using Lemma 12 in \cite{reviewANMMooij}, we have \(\text{HSIC}(Y, E_X) > 0\). Again, using the same argument as in \cite[Theorem 20]{reviewANMMooij} we obtain $\widehat{HSIC}(\textbf{y}, \hat{\varepsilon}_X ) \xrightarrow{P} \text{HSIC}(Y, E_X)$. Together


\begin{equation*}
    \begin{split}
        s(Y\to X)  &= \min_{j\in S_1}\rho (\textbf{y }, \hat{\varepsilon}_x^{j}) +\lambda1= \min_{j\in S_1} \widehat{HSIC}(\textbf{y}, \hat{\varepsilon}^j_X )+\lambda \\& = \widehat{HSIC}(\textbf{y}, \hat{\varepsilon}^1_X )+\lambda   \xrightarrow{P} \text{HSIC}(Y, E_X)+\lambda>\lambda. 
    \end{split}
\end{equation*}


Therefore, the score for the correct direction is smaller than in the wrong causal direction (asymptotically for large $n$) and hence procedure is consistent.


    
\end{proof}





