
\section{Identifiability results}
\label{Section_identifiability}
In this section, we are interested in ascertaining whether it is possible to infer a causal graph from a joint distribution under the assumptions presented in the previous section. First, we rephrase the notion of identifiability from Definition \ref{IdentifiabilityPrvaDefinicia}. 

\begin{definition}[Identifiability]\label{DEFidentifiability}
Let $F_{(X_1, X_2)}$ be a distribution that has been generated according to the $CPCM(F_1,\dots, F_k)$ model with graph $X_1\to X_2$. We say that the causal graph is identifiable from the joint distribution (equivalently, that the model is identifiable) if there does \textit{not} exist 
$\tilde{\theta}$ and a pair of random variables $\tilde{\varepsilon}_2\indep\tilde{\varepsilon}_1$, where $\tilde{\varepsilon}_1$ is uniformly distributed, such that the model $X_2=\tilde{\varepsilon}_2,  X_1=  F_i^{-1}\big(\tilde{\varepsilon}_1;\tilde{\theta}(X_2)\big)$ for some $i\in\{1, \dots, k\}$ generates the same distribution $F_{(X_1,X_2)}$. 
\end{definition}

In the remainder of the section, we answer the following question: Under what conditions is the causal graph identifiable? 
\subsection{Identifiability in $CPCM(F)$}

First, we discuss the Gaussian case. Recall that in the additive Gaussian model, where $X_2=f(X_1)+\varepsilon_2$, $\varepsilon_2\sim N(0, \sigma^2)$, the identifiability holds if and only if $f$ is non-linear \citep{hoyer2009}. We provide a different result with both mean \textit{and} variance as functions of the cause. A similar result is found in \cite[Theorem 1]{Khemakhem_autoregressive_flows} in the context of autoregressive flows and where only a sufficient condition for identifiability is provided. Another similar problem is studied in \cite{immer2022identifiability} and \cite{strobl2022identifying}, both of which show identifiability in general location-scale models. 

\begin{theorem}[Gaussian case]\label{normalidentifiability}
Let $(X_1,X_2)$ admit the $CPCM(F)$ model with graph $X_1\to X_2$, where $F$ is the Gaussian distribution function with parameters $\theta(X_1)=\big(\mu(X_1), \sigma(X_1)\big)^\top$.~\footnote{Just like in Example \ref{Gaussian case}, this can be rewritten as $X_2 = \mu(X_1)+\sigma(X_1)\varepsilon_2,  \text{  where }\varepsilon_2 \text{ is Gaussian}$.}

Let $p_{\varepsilon_1}$ be the density of $\varepsilon_1$ that is absolutely continuous with full support $\mathbb{R}$. Let $\mu(x), \sigma(x)$ be two times differentiable.  Then, the causal graph is identifiable from the joint distribution if and only if there do not exist $a,c,d,e, \alpha, \beta\in\mathbb{R}$,  
$a\geq 0,c>0, \beta>0$, such that
\begin{equation}\label{norm}
\frac{1}{\sigma^2(x)}=ax^2 + c, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \frac{\mu(x)}{\sigma^2(x)}=d+ex,
\end{equation}
for all $x\in\mathbb{R}$ and
\begin{equation}\label{DensityDEF}
p_{\varepsilon_1}(x) \propto \sigma(x)e^{-\frac{1}{2}\big[ \frac{(x-\alpha)^2}{\beta^2}  - \frac{\mu^2(x)}{\sigma^2(x)}\big]},
\end{equation}
where $\propto$  represents an equality up to a constant (here, $p_{\varepsilon_1} $  is a valid density function if and only if $\frac{1}{\beta^2}>  \frac{e^2}{c}\mathbbm{1}[a=0]$). 
Specifically, if $\sigma(x)$ is constant (case $a=0$), then the causal graph is identifiable, unless $\mu(x)$ is linear and $p_{\varepsilon_1}$ is the Gaussian density. 
\end{theorem}

The proof is provided in \hyperref[Proof of normalidentifiability]{Appendix} \ref{Proof of normalidentifiability}. Moreover, a visual example of an unidentifiable Gaussian model with $a=c=d=e=\alpha = \beta=1$ can be found in  \hyperref[Proof of normalidentifiability]{Appendix} \ref{Proof of normalidentifiability}, Figure \ref{GaussianDensity}. 

Theorem \ref{normalidentifiability} indicates that the non-identifiability holds only in the ``special case,'' when $\frac{\mu(x)}{\sigma^2(x)}, \frac{-1}{2\sigma^2(x)}$ are linear and quadratic, respectively. Note that natural parameters of a Gaussian distribution are  $\frac{\mu}{\sigma^2}, \frac{-1}{2\sigma^2}$, and sufficient statistics of the Gaussian distribution have a linear and quadratic form (for the definition of the exponential family, natural parameter and sufficient statistic, see \hyperref[appendix_exponential_family]{Appendix} \ref{appendix_exponential_family}). We show that such connections between non-identifiability and sufficient statistics hold in the more general context of the exponential family.  

\begin{proposition}[General case, one parameter]\label{Necessary condition for identifiability}
Let $q=1$. Let $(X_1, X_2)$ admit the $CPCM(F)$ model with graph $X_1\to X_2$, where $F$ lies in the exponential family of distributions with a sufficient statistic $T$. The causal graph is identifiable if at least one of the following conditions holds:
\begin{enumerate}
    \item There do not exist $a,b\in\mathbb{R}$, such that
 \begin{equation}\label{eq000}
     \theta(x)=a\cdot T(x)+b,\,\,\,\,\,\,\,\,\,\,\,\,\, \forall x\in supp(X_1). 
 \end{equation} 
 \item There does not exist $c\in\mathbb{R}$, such that 
\begin{equation}\label{eq007}
p_{X_1}(x)\propto \frac{h_1(x)}{h_2[\theta(x)]}e^{cT(x)}, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \forall x\in supp(X_1),
\end{equation}
where $h_1$ is a base measure of $F$ and $h_2$ is the normalizing function of $F$ defined in \hyperref[appendix_exponential_family]{Appendix} \ref{appendix_exponential_family}. 
\item Supports of $X_1$ and $X_2$ differ. 
\end{enumerate}
\end{proposition}
\begin{proof}
We first show that if the causal graph is not identifiable, then there exist  $a,b\in\mathbb{R}$ such that  (\ref{eq000}) holds. 

If the graph is not identifiable, there exists a function $ \tilde{\theta}$, such that 
causal models $X_1 = \varepsilon_1, X_2 = F^{-1}\big(\varepsilon_2; \theta(X_1)\big)$, and $X_2 = \varepsilon_2, X_1 = F^{-1}\big(\varepsilon_1; \tilde{\theta}(X_2)\big)$ generate the same joint distribution.
 Decomposing the joint density yields
\begin{equation}\label{eq69}
  p_{(X_1, X_2)}(x,y) = p_{X_1}(x)p_{X_2\mid {X_1}}(y\mid x) = p_{X_2}(y)p_{{X_1}\mid {X_2}}(x\mid y),\,\,\,\,\,\,\,\, x,y\in supp(X_1).
 \end{equation}
Since $F$ lies in the exponential family of distributions, we use the notation from \hyperref[appendix_exponential_family]{Appendix} \ref{appendix_exponential_family} and rewrite this as $$p_{X_2\mid {X_1}}(y\mid x) = h_{1}(y)h_{2}[\theta(x)]\exp[\theta(x)T(y)]$$
and analogously for $p_{{X_1}\mid {X_2}}(x\mid y)$. Then, the second equality in (\ref{eq69}) becomes
\begin{equation}\label{eq098}
    \begin{split}
  &   p_{X_1}(x)h_{1}(y)h_{2}[\theta(x)]\exp[\theta(x)T(y)] =p_{X_2}(y) h_{1}(x)h_{2}[\tilde{\theta}(y)]\exp[\tilde{\theta}(y)T(x)],\\&
\underbrace{\log\bigg(  \frac{ p_{X_1}(x)h_{2}[\theta(x)]}{h_{1}(x)}\bigg)}_{f(x)}+ \underbrace{\log\bigg(\frac{h_{1}(y)}{ p_{X_2}(y)h_{2}[\tilde{\theta}(y)]}\bigg)}_{g(y)} = \tilde{\theta}(y)T(x)-\theta(x)T(y),
\end{split}
\end{equation}
where the second equation is the logarithmic transformation of the first equation after dividing both sides by $h_1$ and $h_2$. Since the left side is in the additive form, Lemma \ref{PomocnaLemma1} directly yields (\ref{eq000}) (to see this without Lemma \ref{PomocnaLemma1}, apply $\frac{d}{dx dy}$  to (\ref{eq098}) and fix $y$ such that $T'(y)\neq 0$. We get 
$\theta'(x) = \frac{\tilde{\theta}'(y)}{T'(y)}T'(x)$. Integrating this equality with respect to $x$ yields (\ref{eq000}). However, using Lemma \ref{PomocnaLemma1}, we do not need to assume the differentiability of $\theta$).

With regard to equation (\ref{eq007}), equality (\ref{eq098}) implies 
$f(x) + g(y) = a_1T(x) + a_2T(y) +a_3$ for some constants $a_1, a_2, a_3\in\mathbb{R}$. Therefore, fixing $y$ yields $f(x)=\log\bigg\{  \frac{ p_{X_1}(x)h_{2}[\theta(x)]}{h_{1}(x)}\bigg\} = a_1T(x) + const$. Rewriting this yields 
$
p_{X_1}(x)= \frac{h_1(x)}{h_2[\theta(x)]}e^{a_1T(x) + const},
$
which is exactly the form of (\ref{eq007}). 

Part 3) is a direct consequence of the fact that the support of any distribution in the exponential family is fixed and does not depend on $\theta$. 
\end{proof}


Proposition \ref{Necessary condition for identifiability} indicates that the only case in which the model is not identifiable is when $\theta(x)$ has a uniquely given functional form (unique up to a linear transformation) and the density of the cause is uniquely given (unique up to one additional parameter $c$).  Note that it is only a sufficient, not a necessary, condition for identifiability since some specific choices of $a,b,$ and $c$ can still lead to identifiable cases.  


As a consequence of Proposition \ref{Necessary condition for identifiability}, we extend the results demonstrated by \cite{ParkPoisson} and \cite{ParkGHD} for a Poisson DAG model. These authors established the identifiability of a Poisson DAG model, where all variables (including source variables) given their parents follow a Poisson distribution. We present an analogous result, relaxing the restriction on the source variables.

\begin{consequence}\label{paretoidentifiability}
\begin{itemize}
    \item Let $(X_1,X_2)$ admit the $CPCM(F)$ model with graph $X_1\to X_2$, where $F$ is the Poisson distribution function with rate $\lambda$.  Then, the causal graph is \textit{not} identifiable if and only if
\begin{equation*}
 \lambda(x) =e^{ax+b},\,\,\,\,\,\,\,\, P(X_1=x) \propto \frac{e^{e^{ax+b}+cx}}{x! }, \,\,\,\,\,\,\,\,\forall x\in \{0,1,2, \dots \},
 \end{equation*}
for some $a<0,b,c\in\mathbb{R}$.
    \item Let $(X_1,X_2)$ admit the $CPCM(F)$ model with graph $X_1\to X_2$, where $F$ is the Pareto distribution function. Then, the causal graph is \textit{not} identifiable if and only if 
\begin{equation*}
\theta(x) = a\log(x) +b,\,\,\,\,\,\,\,\, p_{X_1}(x) \propto \frac{1}{ [a\log(x)+b] x^{c+1} }, \,\,\,\,\,\,\,\,\forall x\geq 1,
\end{equation*}
for some $a,b,c>0$. 

    \item Let $(X_1,X_2)$ admit the $CPCM(F)$ model with graph $X_1\to X_2$, where $F$ is Bernoulli distribution function and $supp(X_1) = supp(X_2) = \{0,1\}$.  Then, the causal graph is \textit{not} identifiable. 
\end{itemize}
\end{consequence}
The proof is provided in \hyperref[Proof of pareto identifiability]{Appendix} \ref{Proof of pareto identifiability}, together with definitions of the distribution functions. Note that if $a=0$, then $X_1\indep X_2$ and the (empty) graph is trivially identifiable. 

Note that in the first two bullet points of Consequence~\ref{paretoidentifiability}, we have three free parameters: \(a\), \(b\), and \(c\). The non-identifiability of the graph in a Bernoulli model arises from the fact that the joint distribution of \((X_1, X_2)\) can be fully characterized by only three parameters.

\subsection{Identifiability in $CPCM(F_1, F_2)$ models}


Similar sufficient conditions as in Proposition \ref{Necessary condition for identifiability} can be derived for a more general case, when $F$ has several parameters and for the $CPCM(F_1, F_2)$. 


\begin{theorem}
\label{thmAssymetricMultivariatesufficient}
Let $(X_1, X_2)$ follow the $CPCM(F_1, F_2)$ model with graph $X_1\to X_2$, where $F_1, F_2$ lie in the exponential family of distributions and $T_1 = (T_{1,1}, \dots, T_{1,q_1})^\top$, $T_2 = (T_{2,1}, \dots, T_{2,q_2})^\top$are the corresponding sufficient statistics.  Assume either $F_1 = F_2$ (that is, we assume $CPCM(F_1)$ model) or $supp(F_1)\neq supp(F_2)$ and define $\mathcal{S}=supp(F_1)\cap supp(F_2)$. 

The causal graph is identifiable if $\theta_2$ is not a linear combination of $T_{1,1}, \dots, T_{1,q_1}$ on $\mathcal{S}$---that is, if $\theta_2$ cannot be written as
\begin{equation}\label{eq158}
\theta_{2,i}(x) \overset{}{=} \sum_{j=1}^{q_1}a_{i,j}T_{1,j}(x)+b_i,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \forall x\in \mathcal{S},
\end{equation}
for all $i=1, \dots, q_2,$ and for some constants $a_{i,j},b_i\in\mathbb{R}$, $j=1, \dots, q_1$. 
\end{theorem}

The proof is provided in \hyperref[Proof of thmAssymetricMultivariatesufficient]{Appendix} \ref{Proof of thmAssymetricMultivariatesufficient}. Note that condition (\ref{eq158}) is sufficient, but not necessary for identifiability. Similar to that in the Gaussian  (Theorem \ref{normalidentifiability}) or Pareto cases (Consequence \ref{paretoidentifiability}), in order to obtain a necessary condition, the distribution of the cause also needs to be restricted.

Note that Theorem~\ref{thmAssymetricMultivariatesufficient} can be iteratively applied to obtain identifiability of $CPCM(F_1, \dots, F_k)$ models. 

\begin{consequence}\label{consequenceprva}
\begin{itemize}
\item Let \((X_1, X_2)\) admit the \(CPCM(F)\) model (\ref{BCPCM}) with graph \(X_1 \to X_2\), where \(F\) is a Gamma distribution with parameters \(\theta = (\alpha, \beta)^\top\). If there do not exist constants \(a, b, c, d, e, f \in \mathbb{R}\) such that
\begin{equation*}
\alpha(x) = a\log(x) + bx + c, \quad \beta(x) = d\log(x) + ex + f, \quad \forall x > 0,
\end{equation*}
then the causal graph is identifiable.

\item Suppose that \( \text{supp}(X_1) = \mathbb{R} \), \(\text{supp}(X_2) = \{0, 1, \dots\}\) such as on Figure~\ref{Asymmetrical_picture}, and let \((X_1, X_2)\) admit the \(CPCM(F_1, F_2)\) model with graph \(X_1 \to X_2\), where \(F_1\) is a Gaussian distribution with mean \(\mu\) and fixed variance, and \(F_2\) is a Poisson distribution with rate parameter \(\lambda\). If there do not exist constants \(a_i, b_i, c_i, d_i \in \mathbb{R}\), \(i = 1, 2\), such that for all \(x \in \{0, 1, \dots\}\)
\begin{equation*}
\lambda(x) = e^{a_1 x + b_1}, \quad p_{X_1}(x) \propto e^{\lambda(x) + c_1 x^2 + d_1 x},\,\,\,\,\,\,\,\, \quad \mu(x) = a_2 + b_2 x, \quad p_{X_2}(x) \propto \frac{1}{x!} e^{c_2 x^2 + d_2 x},
\end{equation*}
then the causal graph is identifiable.
\end{itemize}
\end{consequence}
Details regarding Consequence \ref{consequenceprva}, the definitions of the distributions, their sufficient statistics and more examples are provided in  \hyperref[consequence]{Appendix} \ref{consequence}. We emphasize that not all constants and combinations of parameters yield a non-identifiable model.

It is interesting to observe the dimension of the space of unidentifiable distributions for different choices of \(F\). In the one-parameter cases, such as in Consequence~\ref{paretoidentifiability}, the set of all unidentifiable distributions is contained in a three-dimensional space, similar to the case for ANM \cite[Proposition 21]{Peters2014}. Not surprisingly, if \(F\) has more parameters, this dimension increases, but it remains finite. Since the space of all distributions is infinite-dimensional (assuming infinite support), one can argue that regardless of the choice of $F$, identifiability holds for ``most distributions''. However, if \(F\) has many parameters, we often find ourselves ``close'' to an unidentifiable case, making inference much more challenging.





































