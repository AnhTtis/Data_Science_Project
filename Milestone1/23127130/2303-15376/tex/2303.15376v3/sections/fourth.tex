%Rozmysli si F_i being CONDITIONAL distirubiton functions
%Zmaz vsetky bold math symboly

\section{Multivariate case}
\label{Section4}

We move the theory to the case with more than two variables $d\geq 2$. We assume causal sufficiency (all relevant variables have been observed) and causal minimality. However, we do not assume faithfulness. 
It is straightforward to generalize the asymmetric $\M$ causal model to the multivariate case, in which we assume that each variable $X_i$ arises under the $\mathcal{M}_i$ model. 

To be more rigorous, we define a multivariate CPCM as a generalization of (\ref{asymetrical_F_one_F_two_model}). 

\begin{definition}\label{DefinitionCPCM}
Let $F_1, \dots, F_d$ be distribution functions with $q_1, \dots, q_d$ parameters, respectively. 
We define an asymmetrical $\mathcal{M}_{F_1}, \dots, \mathcal{M}_{F_d}$ causal model ($CPCM(F_1, \dots, F_d)$ for short) as a collection of equations: 
\begin{equation*}
S_j{:}\,\,\,\,  X_j = F^{-1}_{j}(\varepsilon_j; \theta_j\big(\textbf{X}_{pa_j})\big), \,\,\,\, j=1, \dots, d;
\end{equation*}
we assume that the corresponding causal graph is acyclic. $(\varepsilon_1, \dots, \varepsilon_d)^\top$ is a collection of jointly independent uniformly distributed random variables. $\theta_j{:}\,\,  \mathbb{R}^{|pa_j|}\to \mathbb{R}^{q_j}$ are continuous non-constant functions in any of their arguments.

If $F_j^{-1} = F^{-1}$  for some quantile function $F^{-1}$ and for all $j\not\in Source(\mathcal{G}_0)$, we call such a model $CPCM(F)$.
\end{definition}
Simply said, we assume that $X_j\mid \textbf{X}_{pa_j}$ is distributed according to distribution $F_j$ with parameters $\theta_j(\textbf{X}_{pa_j})$. We assume all $F_j$ \textit{are known}, apart from the source variables. Note that  $CPCM(F_1, F_2)$ defined in Section~\ref{Section2.2.2} is a special case of Definition~\ref{DefinitionCPCM}, since we allow the source variable to be arbitrarily distributed. 


The question of the identifiability of $\mathcal{G}$ in the multivariate case is in order. Here, it is not satisfactory to consider the identifiability of each pair of $X_i\to X_j$ separately. Each pair $X_i, X_j$  needs to have an identifiable causal relation \textit{conditioned} on other variables $\textbf{X}_S$. Such an observation was first made by \cite{Peters2014} in the context of additive noise models. We now provide a more precise statement in the context of  $CPCM(F_1, \dots, F_d)$. 

\begin{definition} 
We say that the $CPCM(F_1, \dots, F_d)$ is \textit{pairwise identifiable}, if for all $ i,j\in\mathcal{G}$, $ S\subseteq V$, such that $i\in pa_j$ and  $pa_j\setminus \{i\}\subseteq S \subseteq nd_j\setminus\{i,j\}$, there exists  $\textbf{x}_{S}{:}\,\,  p_S(\textbf{x}_S)>0$, which satisfies that a bivariate model defined as $X=\tilde{\varepsilon}_X, Y = F^{-1}_j\big(\tilde{\varepsilon}_Y, \tilde{\theta}(X)\big)$ is identifiable (in the sense of Definition \ref{DEFidentifiability}), where  $F_{\tilde{\varepsilon}_X} = F_{X_i\mid \textbf{X}_{S} =\textbf{ x}_S}    $ and $\tilde{\theta}(x) = \theta_j(\textbf{x}_{pa_j\setminus\{i\}}, x)$,  $x\in supp(X)$.
\end{definition}

 \begin{lemma}\label{thmMultivairateIdentifiability}
Let $F_{\textbf{X}}$ be generated by the pairwise identifiable $CPCM(F_1, \dots, F_d)$ with DAG $\mathcal{G}$. Then,  $\mathcal{G}$ is identifiable from the joint distribution. 
 \end{lemma}
 The proof is provided in \hyperref[Proof of thmMultivairateIdentifiability]{Appendix} \ref{Proof of thmMultivairateIdentifiability}. It is an application of Theorem 28 in  \cite{Peters2014}. 
 An important special case arises when we assume (conditional) normality.

\begin{consequence}[Multivariate Gaussian case]\label{ExampleMultivariateGaussiancase}
Suppose that $\textbf{X}=(X_1, \dots, X_d)$ follow $CPCM(F)$ with a Gaussian distribution function $F$. This corresponds to $X_j\mid \textbf{X}_{pa_j}\sim N\big(\mu_j(\textbf{X}_{pa_j}), \sigma_j^2(\textbf{X}_{pa_j})\big)$ for all $j=1, \dots, d$ and for some functions $\mu_j, \sigma_j$. In other words, we assume that the data-generation process has the following form: 
$$
X_j = \mu_j(\textbf{X}_{pa_j}) + \sigma_j(\textbf{X}_{pa_j})\varepsilon_j, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \text{where  } \varepsilon_j \text{  is Gaussian.}
$$
Potentially, source nodes can have arbitrary distributions. Combining Theorem~\ref{normalidentifiability} and Lemma~\ref{thmMultivairateIdentifiability}, the causal graph $\mathcal{G}$ is identifiable if the 
functions $\theta_j(\textbf{x}):=\big(\mu_j(\textbf{x}), \sigma_j(\textbf{x})\big)^\top, \textbf{x}\in\mathbb{R}^{|pa_j(\mathcal{G})|}$ , $j=1, \dots, d$, are \textit{not} in the form (\ref{norm}) in any of their arguments. 
\end{consequence}


















