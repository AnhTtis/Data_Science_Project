\section{Introduction}

%Lemma jedna este raz poriadne pochop
%example najprv az potom 

Having knowledge regarding causal relationships rather than statistical associations enables us to predict the effects of actions that perturb an observed system \citep{TheBookOfWhy}. Determining causal structures is a fundamental problem in numerous scientific fields. However, several data-generating processes can produce the same observational distribution. Observing the system after interventions is a reliable means to identify the causal structure, but in numerous real-life scenarios, interventions can be too expensive, unethical \citep{epidem_application}, or impossible to observe. Hence, estimating the causal structure from observational data has become an important topic in recent years.

In the last few years, much effort has been put into developing a  mathematical background for a ``language'' of causal inference  \citep{Pearl_book}. The theory stands on the structural causal model (SCM) for random variables $\textbf{X} = (X_1, \dots, X_d)^\top\in\mathbb{R}^d$, with structural equations in the form $X_i=f_i(\textbf{X}_{pa_i}, \varepsilon_i)$, where $f_i\in\mathcal{A}$ are the causal functions belonging to a certain subclass of real functions $\mathcal{A}$, $pa_i$ represent the direct causes of $X_i$, and $(\varepsilon_1, \dots, \varepsilon_d)^\top$ are independent noise variables. The SCM is an important mathematical concept, and the fundamental goal of causal discovery is to estimate the causal structure (causal graph $\mathcal{G}$ associated with SCM) or the direct causes of a certain variable of interest. However, estimating the causal structure is impossible without strong assumptions on the class $\mathcal{A}$
\citep{Elements_of_Causal_Inference}. 

The extant literature in the field presents numerous methods and consequent results for causal inference under different assumptions on the SCM \citep{ZhangReview}.  If we observe several environments after different interventions, the assumptions can be much less restrictive \citep{Peters_invariance}.  If we aim to discover causal relations based only on an observed random sample, the assumptions are usually more strict---assuming linear structure or additive noise \citep{Peters2014}.

The additivity assumption ($X_i = f_i(\textbf{X}_{pa_i}) \textbf{+} \varepsilon_i$) implies that $\textbf{X}_{pa_i}$ affects only the mean of $X_i$. One well-known approach that enables more general effects of the covariates is post-nonlinear model (PNL;  \citealp{Zhang2010}). This model potentially allows an effect of  $\textbf{X}_{pa_i}$ on the variance of $X_i$, but it does not allow arbitrary relations. Quadratic variance function (QVF) models introduced in \cite{ParkVariance} consider the variance to be a quadratic function of the mean, and location-scale models defined in \cite{immer2022identifiability} allow arbitrary effect on the variance. However, these models assume a fixed tail and fixed higher moments. 

In this paper, we develop a framework where $\textbf{X}_{pa_i}$ can arbitrarily affect the mean, variance, tail, or other characteristics of $X_i$. However, a caution have to be taken because if $\textbf{X}_{pa_i}$ affects $X_i$ in a certain manner, the causal structure will become unidentifiable (several causal structures can produce the same distribution of $\textbf{X}$). We expose the assumptions that are reasonable in order to obtain an identifiable causal structure.
\begin{example}\label{Gaussian case}
    A useful model that allows an arbitrary effect on the variance as well as on the mean is $X_i\mid \textbf{X}_{pa_i}\sim N\big(\mu(\textbf{X}_{pa_i}), \sigma^2(\textbf{X}_{pa_i})\big)$ for some functions $\mu, \sigma$, in which case the structural equation has the form 
\begin{equation}\label{prva_gaussian_equation}
X_i =  \mu(\textbf{X}_{pa_i})+ \sigma(\textbf{X}_{pa_i})\cdot \varepsilon_i, \,\,\,\,\,\,\,\,\,\,\varepsilon_i\text{ is Gaussian.}
\end{equation}
The central limit theorem can justify (\ref{prva_gaussian_equation}) if $\textbf{X}$ is an appropriate sum of independent events. 
\end{example}

\begin{example}\label{example_Pareto}
In certain applications, it may be reasonable to assume
$$X_i\mid \textbf{X}_{pa_i}\sim Pareto\big(\theta(\textbf{X}_{pa_i})\big),$$where $\theta$ is the function that describes the tail behaviour. Such a model is common when $X_i$ represents extreme events \citep{Coles}. 
\end{example}

In Section \ref{Section2}, we introduce a causal model (we call it the conditionally parametric causal model or CPCM) where the structural equation has the following form: 
\begin{equation}\label{11}\begin{split}
&X_i=f_i(\textbf{X}_{pa_i}, \varepsilon_i) = F^{-1}(\varepsilon_i; \theta(\textbf{X}_{pa_i})),\,\,\,\,\,\,\varepsilon_i\sim U(0,1), \\&   
\,\,\,\,\,\,\,\,\,\,\,\,\text{ or equivalently } X_i\mid \textbf{X}_{pa_i}\sim F\big(\theta(\textbf{X}_{pa_i})\big), \end{split}
\end{equation}
where $F$ is a known distribution function with a vector of parameters $\theta(\textbf{X}_{pa_i})$. We restrict our focus mostly to the case in which $F$ belongs to the exponential family of continuous distributions when sufficient statistics can be used. Section \ref{Section_identifiability} provides the identifiability results of the causal structure in the bivariate case. Assuming (\ref{11}), is it possible to infer the causal structure based only on observational data?
In Section \ref{Section4}, we discuss the multivariate extension. 
In Section \ref{Section5}, we propose an algorithm for estimating the causal graph under assumption (\ref{11}). In Section \ref{simulations_section}, we proceed to a short simulation study, and in Section \ref{Section7} we illustrate our methodology on a dataset concerning income and expenditure of residents of Philippines. 

We provide three appendices: Appendix \ref{SectionProofs} contains proofs of the theorems and lemmas presented in the paper. Appendix \ref{appendix} contains detailed definitions and explanations for a few concepts that were omitted from the main part of the paper for the sake of clarity. Appendix \ref{Appendix_simulations} presents some details about the simulation study and the application. 


\subsection{Setup and notation}
\label{Setup}
We borrow the notations of \cite{PCalgorithm} and introduce the notation of graphical causal models.
A directed acyclic graph (DAG) $\mathcal{G}=(V,E)$ contains a finite set of vertices (nodes) $V$ and a set of directed edges $E$ between distinct vertices. We assume that there exists no directed cycle or multiple edges. 

Let $i,j\in V$ be two distinct nodes of $\mathcal{G}$. We say that $i$ is a parent of $j$ if there exists an edge from $i$ to $j$ in $E$, and we note $i\in  pa_{j}(\mathcal{G})$. Moreover, such $j$ is called a child of $i$, with notation $j\in ch_i(\mathcal{G})$. We say that $i$ is an ancestor of $j$, if there exists a directed path from $i$ to $j$ in $E$, notation $i\in an_{j}(\mathcal{G})$. In the other direction, we say that $j$ is a non-descendant of $i$, if $i\not\in an_{j}(\mathcal{G})$, notation $j\in nd_{i}(\mathcal{G})$. In addition, we say that the node $i\in V$ is a source node if $pa_j(\mathcal{G})=\emptyset$, notation $i\in Source(\mathcal{G})$.  We omit the argument $\mathcal{G}$ if evident from the context. 

We use capital $F$ for distributions and small $p$ for densities. Consider a random vector $\textbf{X }= (X_i)_{i\in V}$ over some probability space with distribution $F_\textbf{X}$. With 
a slight abuse of notation, we identify the vertices $j \in V$ with the variables $X_j$.  We denote $\textbf{X}_S = \{X_s{:}\,\, s\in S\}$ for $S\subseteq V$. We focus on the case in which $\textbf{X}$ has a continuous joint density function $p_\textbf{X}$ with respect to the Lebesgue measure. 

The distribution $F_\textbf{X}$ is Markov with respect to $\cal{G}$  if $A\indep_{\cal{G}} B\mid C \implies A\indep B\mid C$ for $A,B,C\subseteq V$ are disjoint subsets of the vertices and  $\indep_{\cal{G}}$ represents a d-separation in $\mathcal{G}$ \citep{Pearl}. On the other hand, if for all $A,B,C\subseteq V$ disjoint subsets of the vertices hold $A\indep_{\cal{G}} B\mid C \impliedby A\indep B\mid C$, we say that the distribution is faithful with respect to $\mathcal{G}$.   A distribution satisfies causal minimality with respect to $\mathcal{G}$ if it is Markov with respect to $\mathcal{G}$, but not to any proper subgraph of $\mathcal{G}$. Moreover, it can be shown that faithfulness implies causal minimality \citep[Proposition 6.35]{Elements_of_Causal_Inference}. Two graphs $\mathcal{G}_1, \mathcal{G}_2$ are Markov equivalent if the set of distributions that are Markov with respect to $\mathcal{G}_1, \mathcal{G}_2$ is the same. We denote the Markov equivalency class of $\mathcal{G}$ as the following set $\{\mathcal{G}'{:}\,\,  \mathcal{G}\text{ and } \mathcal{G}' \text{ are Markov equivalent}\}$. 
A set of variables $\textbf{X}$ is said to be causally sufficient if there is no hidden common cause that causes more than one variable in $\textbf{X}$ \citep{Sprites2010}. The main concept in causal inference is the SCM \citep{Pearl}, defined in the following manner. 


%SCM
\begin{definition}[SCM]\label{def1}
The random vector $\textbf{X}=(X_1, \dots, X_d)$ follows the SCM with DAG $\cal{G}$  if for each $i\leq d$ the variable $X_i$ arises from the following structural equation: 
$$
S_i{:}   \,\,\,\, X_i=f_i\big(\textbf{X}_{pa_i(\mathcal{G})}, \varepsilon_i\big),
$$
where $\varepsilon = (\varepsilon_1, \dots, \varepsilon_d)$ is jointly independent and $f_i$ are measurable functions.
\end{definition}

The SCM uniquely defines a distribution of $\textbf{X}$ that can be decomposed into a product of conditional densities or causal Markov kernels \citep[Definition 6.21]{Elements_of_Causal_Inference}
\begin{equation}\label{def987}
p_\textbf{X}(\textbf{x}) = \prod_{i\in V} p_i(x_i\mid \textbf{x}_{pa_i}),
\end{equation}
where $p_i(\cdot \mid \textbf{x}_{pa_i})$ represents the conditional density function of the random variable $X_i$ conditioned on its parents $\textbf{X}_{pa_j}=\textbf{x}_{pa_j}$. 
Note that every distribution that is Markov with respect to $\mathcal{G}$ can be decomposed into (\ref{def987}).

A core concept in causal inference is the identifiability of the causal structure. It is straightforward to compute $F_\textbf{X}$ if the DAG $\mathcal{G}$ and the Markov kernels (\ref{def987}) are given. However, we deal with the opposite problem, where $F_\textbf{X}$ is given (or a random sample from $F_\textbf{X}$) and we want to infer $\mathcal{G}$. 

Consider the SCM from Definition \ref{def1} with conditional densities that satisfy (\ref{def987}). 
Let  $\mathcal{G}\in DAG(d)$, where $DAG(d)$ is the set of all DAGs over $V=\{1, \dots, d\}$ and assume that $p_i\in\mathcal{H}_i$, where $\mathcal{H}_i$ is the subset of all conditional density functions. Let $\mathcal{H} = \mathcal{H}_1\times \dots \times \mathcal{H}_d$. The given pair $(\mathcal{G}, p)\in DAG(d)\times \mathcal{H}$ generates the density $p_p(\textbf{x}) = \prod_{i\in V} p_i(x_i\mid \textbf{x}_{pa_i})$.  

\begin{definition}\label{IdentifiabilityPrvaDefinicia}
We say that the pair $(\mathcal{G}, p)\in DAG(d)\times \mathcal{H}$ is identifiable in $DAG(d)\times \mathcal{H}$ if there does \textit{not} exist a pair $(\mathcal{G}', p')\in DAG(d)\times \mathcal{H}$,  $\mathcal{G}'\neq \mathcal{G}$, such that $p_p= p_{p'}$. We say that the class $\mathcal{H}$ is identifiable over $DAG(d)$ if every  pair  $(\mathcal{G}, p)\in DAG(d)\times \mathcal{H}$ is identifiable in $\mathbb{G}_d\times \mathcal{H}$.  
\end{definition}

Without strong restrictions of class $\mathcal{H}$, we cannot obtain the identifiability of $\mathcal{G}$. Generally, we can only identify the Markov equivalency class \citep[Proposition 7.1.]{Elements_of_Causal_Inference}. 
In Section 3, we rephrase the definition of identifiability from Definition \ref{IdentifiabilityPrvaDefinicia} for model (\ref{11}).

%\begin{lemma}(Non-uniqueness of graph structures \label{NonUniqueness}
% Consider a random vector $\textbf{X} = (X_1, \dots, X_d)$ with distribution $P_\textbf{X}$ that has a density with respect to Lebesgue measure and assume it is Markov with respect to $\cal{G}$. Then there exists an SCM with graph $\cal{G}$ that entails the distribution $P_\textbf{X}$.
%\end{lemma}
\subsection{Related work}

Several papers address the problem of the identifiability of the causal structure (for a review, see \cite{ZhangReview}). 
\cite{Lingam} show identifiability for the linear non-Gaussian additive models (LiNGaM), where $X_i=\beta\textbf{X}_{pa_i} +\varepsilon_i$ for non-Gaussian noise variables $\varepsilon_i$. 
\cite{BuhlmannCAM} explore causal additive models (CAM) of the form $X_i = \sum_{j\in pa_i} g_j(X_j) +  \varepsilon_i$ for smooth functions $g_j$.  
\cite{hoyer2009} and \cite{Peters2014} develop a framework for additive noise models (ANM), where $X_i = g(\textbf{X}_{pa_i}) +  \varepsilon_i$. Under certain (not too restrictive) conditions on $g$, the authors show the identifiability of such models  \citep[Corollary 31]{Peters2014} and propose an algorithm estimating $\mathcal{G}$ (for a review on ANM, see \cite{reviewANMMooij}). 
All these frameworks assume that the variance of $X_i\mid \textbf{X}_{pa_i}$ does not depend on $\textbf{X}_{pa_i}$. This is a crucial aspect of the identifiability results.

\cite{Zhang2009} consider the following PNL model
\begin{equation*}
X_i = g_1\big(g_2(\textbf{X}_{pa_i}) +  \varepsilon_i\big)
\end{equation*}
with an invertible link function $g_1$.  Here, potentially $X_i = g_2(\textbf{X}_{pa_i})\cdot \varepsilon$ for the special choice $g_1(x)=\log(x)$.  The PNL causal model a rather general form (the former two are its special cases). However, it is identifiable under certain technical assumptions and with a few exceptions \citep{Zhang2010}. 

 \cite{ParkPoisson, ParkVariance} reveal identifiability in models in which  $var[X_i\mid \textbf{X}_{pa_i}]$  is a quadratic function of $\mathbb{E}[X_i\mid \textbf{X}_{pa_i}]$. If $X_i\mid \textbf{X}_{pa_i}$ has a Poisson or binomial distribution, such a condition is satisfied.  They also provide an algorithm based on comparing dispersions for estimating a DAG in polynomial time. Other algorithms have also been proposed, with comparable speed and different assumptions on the conditional densities \citep{PolynomialTimeAlgorithmCausalGraphs}. \cite{Galanti} consider the neural SCM with representation $X_i = g_1\big(g_2(\textbf{X}_{pa_i}), \varepsilon_i\big),$ where $g_1$ and $g_2$ are assumed to be neural networks. 

Very recently, location-scale models of the form 
$X_i = g_1(\textbf{X}_{pa_i}) +  g_2(\textbf{X}_{pa_i})\varepsilon_i$
have received some attention. \cite{immer2022identifiability} revealed that bivariate non-identifiable location-scale models must satisfy a certain differential equation. \cite{strobl2022identifying} considered the multivariate extension and estimating patient-specific root causes. 
\cite{Khemakhem_autoregressive_flows} revealed more detailed identifiability results under Gaussian noise $\varepsilon_i$ in the bivariate case using autoregressive flows. \cite{xu2022inferring} considered a more restricted location-scale model that divides the range of the predictor variable into a finite set of bins and then fitting an additive model in each bin.

Further, several different algorithms for estimating causal graphs have been proposed, working with different assumptions \citep{IGCI, Score-based_causal_learning, Slope, Natasa_Tagasovska}.  They are often based on Kolmogorov complexity or independence between certain functions in a deterministic scenario. We provide more details about a few  of these relevant methods in Section~5. 

A few authors assume that causal Markov kernels lie in a parametric family of distributions. \cite{JanzingSecondOrderExponentialModels} consider the case in which the density of  $X_i\mid \textbf{X}_{pa_i}$  lies in a second-order exponential family and the variables are a mixture of discrete and continuous random variables. \cite{ParkGHD} concentrate on a specific subclass of model (\ref{11}), where $F$ lies in a discrete family of generalized hypergeometric distributions---that is, the family of random variables in which the mean and variance have a polynomial relationship. To the best of our knowledge, there does not exist any study in the literature, that provides identifiability results in the case in which $F$ lies in a general class of the continuous exponential family. This is the focus of this paper. 















































































































