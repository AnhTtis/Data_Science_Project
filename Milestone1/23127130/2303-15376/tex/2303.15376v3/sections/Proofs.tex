
\section{Appendix: Proofs}
\label{SectionProofs}

\subsection{}
\begin{customthm}{\ref{normalidentifiability}}
Let $(X_1,X_2)$ admit the model ( \ref{BCPCM}) with graph $X_1\to X_2$ and the Gaussian distribution function $F$ with parameters $\theta(X_1)=\big(\mu(X_1), \sigma(X_1)\big)^\top$. 
Let $p_{\varepsilon_1}$ be the density of $\varepsilon_1$ that is absolutely continuous with full support $\mathbb{R}$. Let $\mu(x), \sigma(x)$ be two times differentiable.  

Then, the causal graph is identifiable from the joint distribution if and only if  there do not exist $a,c ,d,e, \alpha, \beta\in\mathbb{R}$,  
$a\geq 0,c>0, \beta>0$, such that
\begin{equation}\tag{\ref{norm}}
\frac{1}{\sigma^2(x)}=ax^2 + c, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \frac{\mu(x)}{\sigma^2(x)}=d+ex,
\end{equation}
for all $x\in\mathbb{R}$ and
\begin{equation}\tag{\ref{DensityDEF}}
p_{\varepsilon_1}(x) \propto \sigma(x)e^{-\frac{1}{2}\big[ \frac{(x-\alpha)^2}{\beta^2}  - \frac{\mu^2(x)}{\sigma^2(x)}\big]},
\end{equation}
where $\propto$ represents an equality up to a constant (here, $p_{\varepsilon_1} $  is a valid density function if and only if $\frac{1}{\beta^2}\neq  \frac{e^2}{c}\mathbbm{1}[a=0]$). 
Specifically, if $\sigma(x)$ is constant (case $a=0$), then the causal graph is identifiable unless $\mu(x)$ is linear and $p_{\varepsilon_1}$ is the Gaussian density.
\end{customthm}



\begin{proof}
\label{Proof of normalidentifiability}{}
We opt for proving this theorem from scratch, without using Theorem \ref{thmAssymetricMultivariatesufficient}. The reader attempt to use Theorem \ref{thmAssymetricMultivariatesufficient}. For clarity regarding the indexes, we use the notation $X=X_1, Y=X_2$. 

First, we show that if the causal graph is not identifiable, then $\mu(x)$ and $\sigma(x)$ must satisfy (\ref{norm}). Let $p_{(X,Y)}$ be the density function of $(X,Y)$. Since the causal graph is not identifiable, there exist two CPCM models that generate $p_{(X,Y)}$: the CPCM model with  $X\to Y$  and the function  $\theta(x)=\big(\mu(x), \sigma^2(x)\big)^\top$ and the CPCM model with  $Y\to X$ and the function $\tilde{\theta}(y)=\big(\tilde{\mu}(y), \tilde{\sigma}^2(y)\big)^\top$. 

We decompose (corresponding to the direction $X\to Y$) 
\begin{equation*}
p_{(X,Y)}(x,y) = p_{X}(x)p_{Y\mid X}(y\mid x) = p_{X}(x) \phi\big(y;\theta(x)\big),
\end{equation*}
where $\phi\big(y;\theta(x)\big)$ is the Gaussian density function with parameters $\theta(x) = \big(\mu(x), \sigma^2(x)\big)^\top$. We rewrite this in the other direction: 
$$
p_{(X,Y)}(x,y) = p_{Y}(y)p_{X\mid Y}(x\mid y) = p_{Y}(y) \phi\big(x;\tilde{\theta}(y)\big).
$$
We take the logarithm of both equations and rewrite them in the following manner:
\begin{equation*}
\log[p_{X}(x)] +  \log\bigg\{\frac{1}{\sqrt{2\pi \sigma^2(x)}}e^{\frac{-[y-\mu(x)]^2}{2\sigma^2(x)}}\bigg\} = \log[p_{Y}(y)] +  \log\bigg\{\frac{1}{\sqrt{2\pi \tilde{\sigma}^2(y)}}e^{\frac{-(x-\tilde{\mu}(y))^2}{2\tilde{\sigma}^2(y)}}\bigg\} \text{  and}
\end{equation*}
\begin{equation}\label{eq1}
\log[p_{X}(x)] -\log\sigma(x)-\frac{1}{2}  \frac{[y-\mu(x)]^2}{\sigma^2(x)} = \log[p_{Y}(y)] -\log\tilde{\sigma}(y) -\frac{1}{2}  \frac{[x-\tilde{\mu}(y)]^2}{\tilde{\sigma}^2(y)}. 
\end{equation}
Calculating on both sides $\frac{\partial^4 }{\partial^2 x \partial^2 y }$, we obtain 
$$\frac{\sigma''(x)\sigma(x)-3\sigma'(x)'\sigma(x)}{\sigma^4(x)} =
\frac{\tilde{\sigma}''(y)\tilde{\sigma}(y)-3\tilde{\sigma}'(y)\tilde{\sigma}'(y)}{\tilde{\sigma}^4(y)}. $$Since this has to hold for all $x,y$, both sides need to be constant (let us denote this constant by $a\in\mathbb{R}$).  

Differential equation $\sigma''(x)\sigma(x)-3\sigma'(x)\sigma'(x)=a\cdot\sigma^4(x)$ has solution $\sigma(x) = \frac{1}{\sqrt{a(x+b)^2 + c}}$ for  $x$ , such that $a(x+b)^2 + c>0$. 

Plugging this result into (\ref{eq1}) and calculating on both sides $\frac{\partial^3 }{\partial^2 x \partial y }$, we obtain 
\begin{equation}\label{eq2}
\mu''(x) (a(x+b)^2+c) + \mu'(x) (4ax+4ab) + \mu(x) 2a = 2ab.
\end{equation}
Equation (\ref{eq2}) is another differential equation with a solution $\mu(x) = \frac{d+ex}{a(x+b)^2+c} + b$, for some $d,e\in\mathbb{R}$ for all $x{:}\,\, \sigma(x)>0$. 

Next, we show that it is necessary that $b=0$. If we show $b=0$, then $\mu(x)$ and $\sigma^2(x)$ are exactly in the form (\ref{norm}). We plug the representations  $\mu(x) = \frac{d+ex}{a(x+b)^2+c} + b, \sigma(x) = \frac{1}{\sqrt{a(x+b)^2 + c}}$ and   $\tilde{\mu}(x) = \frac{\tilde{d}+\tilde{e}x}{\tilde{a}(x+\tilde{b})^2+\tilde{c}} + \tilde{b}, \tilde{\sigma}(x) = \frac{1}{\sqrt{\tilde{a}(x+\tilde{b})^2 + \tilde{c}}}$  into (\ref{eq1}). Thus, we obtain
\begin{equation*}
\begin{split}
&\log[p_{X}(x)] +\frac{1}{2}\log[a(x+b)^2 + c]\\&
-\frac{1}{2}  [y^2(ax^2 + 2abx + ab^2+c) + y(2d+2ex) + \frac{1}{a(x+b)^2 +c} ] \\&
= \log[p_{Y}(y)] +\frac{1}{2}\log[\tilde{a}(y+\tilde{b})^2 + \tilde{c}]\\& 
-\frac{1}{2}  [x^2(\tilde{a}y^2 + 2\tilde{a}\tilde{b}y + \tilde{a}\tilde{b}^2+\tilde{c}) + x(2\tilde{d}+2\tilde{e}y) + \frac{1}{\tilde{a}(y+\tilde{b})^2 +\tilde{c}} ] .
\end{split}
\end{equation*}
We can re-write the last expression as
\begin{equation}\label{eq4}
\begin{split}
&h_X(x) + h_Y(y) =\frac{1}{2}  [y^2(ax^2 + 2abx ) + 2yex ] -\frac{1}{2}  [x^2(\tilde{a}y^2 + 2\tilde{a}\tilde{b}y) + 2x\tilde{e}y) ]\\
&=\frac{1}{2}[ x^2y^2(a - \tilde{a})  + xy(2aby - 2\tilde{a}\tilde{b}x +e-\tilde{e}) ],
\end{split}
\end{equation}
where 
\begin{equation*}\label{eqh_x}
h_X(x) = \log[p_{X}(x)]  +  \frac{1}{2}\log[a(x+b)^2 + c] + \frac{1}{2}\frac{1}{a(x+b)^2 +c} -2x\tilde{d} - x^2(\tilde{a}\tilde{b}^2+\tilde{c}), 
\end{equation*}
\begin{equation*}
h_Y(y) = -\log[p_{Y}(y)] -\frac{1}{2}\log[\tilde{a}(y+\tilde{b})^2 + \tilde{c}] +\frac{1}{2}[\frac{1}{\tilde{a}(y+\tilde{b})^2 +\tilde{c}} - 2yd - y^2(ab^2+c)].
\end{equation*}
Since the left-hand side of (\ref{eq4}) is in additive form, the right side also needs to have an additive representation. However, that is only possible if $a- \tilde{a}=0$ and $2aby - 2\tilde{a}\tilde{b}x +e-\tilde{e}=0$. Therefore, we necessarily have $a=\tilde{a}$ and either $a=0$ or $b=\tilde{b}=0$. The case $a=0$ corresponds to a constant $\sigma$ and, hence, also $b=\tilde{b}=0$. We have shown that $\mu(x)$ and $\sigma^2(x)$ have to satisfy (\ref{norm}).

Next, we show that if the causal graph is not identifiable, then the density of $p_{X}(x)$ has form (\ref{DensityDEF}). Plugging the form of  $\mu(x)$ and $\sigma^2(x)$ into (\ref{eq1}), we obtain
\begin{equation*}
\begin{split}
\log[p_{X}(x)] -&\log\sigma(x)-\frac{1}{2}[y-  \frac{d+ex}{ax^2+c}    ]^2(ax^2+c) \\&= \log[p_{Y}(y)] -\log\tilde{\sigma}(y) -\frac{1}{2}  (x-  \frac{\tilde{d}+\tilde{e}y}{ay^2+\tilde{c}}   )^2(ay^2 + \tilde{c}).
\end{split}
\end{equation*}
We rewrite
\begin{equation}\label{rftgyh}
\begin{split}
\log[p_{X}(x)] -  &\log\sigma(x)+\frac{1}{2}( \tilde{c}x^2 - 2x\tilde{d} +  \frac{({d}+{e}x)^2}{ax^2+{c}}   ) \\&= \log[p_{Y}(y)] -\log\tilde{\sigma}(y) +\frac{1}{2}[cy^2 - 2yd + \frac{(\tilde{d}+\tilde{e}y)^2}{ay^2+\tilde{c}} ]
. 
\end{split}
\end{equation}
Since this has to hold for all  $x,y\in\mathbb{R}$, both sides of (\ref{rftgyh}) need to be constant and we obtain $\log[p_{X}(x))]\propto\log\sigma(x)-\frac{1}{2}  [\tilde{c}x^2 - 2x\tilde{d} +  \frac{({d}+{e}x)^2}{ax^2+{c}}   ] $. Hence, 
$$
p_{X}(x) \propto \sigma(x)e^{-\frac{1}{2}  \big[ \tilde{c}x^2 - 2x\tilde{d} +  \frac{({d}+{e}x)^2}{ax^2+{c}}   \big]} =  \sigma(x)e^{-\frac{1}{2}\big[ \frac{(x-\alpha)^2}{\beta^2}  - \frac{\mu^2(x)}{\sigma^2(x)}\big]},
$$
where $\beta = 1/\sqrt{\tilde{c}}$ and $\alpha=\frac{\tilde{d}}{\tilde{c}}$. The condition $\frac{1}{\beta^2}\neq  \frac{e^2}{c}\mathbbm{1}[a=0]$ comes from the fact that if $a=0$ and$\frac{1}{\beta^2}=  \frac{e^2}{c}$ then $p_X(x)$ is not a density function (it is not integrable since  
$ \sigma(x)e^{-\frac{1}{2}\big[ \frac{(x-\alpha)^2}{\beta^2}  - \frac{\mu^2(x)}{\sigma^2(x)}\big]}= ce^{-\frac{1}{2}\big[ 0x^2 + 2x(\frac{ed}{c} - 2\tilde{d} ) + \frac{d^2}{c}  \big] }\propto e^{-x\cdot const}$ for all $x\in\mathbb{R}$). 

Finally, we deal with the other direction: we show that if $\mu$ and $\sigma$ satisfy (\ref{norm}) and $p_{\varepsilon_X}$ has form (\ref{DensityDEF}), then the causal graph is not identifiable. Assume that $a,c,d,e$ are given. Define $\tilde{a} = a, \tilde{e}=e$ and select $\tilde{c}, \tilde{d}\in\mathbb{R}$, such that $\tilde{c}>0,\tilde{c}\neq  \frac{e^2}{c}\mathbbm{1}[a=0]$. Define $\frac{1}{\tilde{\sigma}^2(y)} = \tilde{a} y^2 + \tilde{c}, \frac{\tilde{\mu}(y)}{\tilde{\sigma}^2(y)} = \tilde{d}+\tilde{e}y$. Moreover, define 
\begin{align*}
p_X(x) &\propto \sigma(x)e^{-\frac{1}{2}\big[ \tilde{c}\big(x-\frac{\tilde{d}}{\tilde{c}}\big)^2  - \frac{\mu^2(x)}{\sigma^2(x)}\big]}\,\,\text{             and}\\
p_Y(y) &\propto \tilde{\sigma}(y)e^{-\frac{1}{2}\big[c\big(x-\frac{{d}}{{c}}\big)^2  - \frac{\tilde{\mu}^2(y)}{\tilde{\sigma}^2(y)}\big]}.
\end{align*}
Note that regardless of the coefficients, these are valid density functions (with one exception when $\tilde{c}=\frac{e^2}{c}$ and $a=0$, which is why we selected $\tilde{c}\neq  \frac{e^2}{c}\mathbbm{1}[a=0]$). In case of $a=0$, this is the classical Gaussian distribution density function.

Using these values, we obtain the equality
$$
p_X(x)p_{Y\mid X}(y\mid x) = p_Y(y)p_{X\mid Y}(x\mid y), \forall x,y\in\mathbb{R},
$$
or more precisely, 
$$
\sigma(x)e^{-\frac{1}{2}\big[ \tilde{c}\big(x-\frac{\tilde{d}}{\tilde{c}}\big)^2  - \frac{\mu^2(x)}{\sigma^2(x)}\big]}\frac{1}{\sqrt{2\pi}\sigma(x)}e^{-\frac{1}{2}\frac{[y-\mu(x)]^2}{\sigma^2(x)}} \propto \tilde{\sigma}(y)e^{-\frac{1}{2}\big[c\big(x-\frac{{d}}{{c}}\big)^2  - \frac{\tilde{\mu}^2(y)}{\tilde{\sigma}^2(y)}\big]}\frac{1}{\sqrt{2\pi}\tilde{\sigma}(y)}e^{-\frac{1}{2}\frac{(x-\tilde{\mu}(y))^2}{\tilde{\sigma}^2(y)}}. 
$$
Since this holds for all $x,y\in\mathbb{R}$, we found a valid backward model. The density in (\ref{DensityDEF}) uses the notation $\alpha=\frac{\tilde{d}}{\tilde{c}}$ and $\beta = 1/\sqrt{\tilde{c}}$.  
\end{proof}

An example of the joint distribution of $X_1, X_2$ with $a=c=d=e=\alpha = \beta=1$ is depicted in Figure \ref{GaussianDensity}. 
\begin{figure}[ht]
\centering
\includegraphics[scale=0.35]{figures/Gaussian.pdf}
\caption{Random sample from a joint distribution of $(X_1,X_2)$, where  $X_1$ has the marginal density (\ref{DensityDEF}) and $X_2\mid X_1\sim N\big(\mu(X_1), \sigma^2(X_1)\big)$ with $\mu, \sigma$ defined in (\ref{norm}) with constants   $a=c=d=e=\alpha= \beta=1$. The distribution function is symmetric according to the $x=y$ axis (red line).    }
\label{GaussianDensity}
\end{figure}





\subsection{}


\begin{customconsequence}{\ref{paretoidentifiability}}
Let $(X_1,X_2)$ admit the model ( \ref{BCPCM}) with graph $X_1\to X_2$, where $F$ is the Pareto distribution function, such as in Example \ref{example_Pareto}. Then, the causal graph is \textit{not} identifiable if and only if   
\begin{equation}\tag{\ref{eq50}}
\theta(x) = a\log(x) +b,\,\,\,\,\,\,\,\, p_{X_1}(x) \propto \frac{1}{ [a\log(x)+b] x^{d+1} }, \,\,\,\,\,\,\,\,\forall x\geq 1,
\end{equation}
for some $a,b,d>0$.
\end{customconsequence}
\begin{proof}
\label{Proof of pareto identifiability}
For a clarity in the indexes, we use the notation $X=X_1, Y=X_2$ and we assume $X\to Y$. 

First, we show that if (\ref{eq50}) holds, then there exists a backward model that generates the same joint distribution. The joint density can be written as 
\begin{equation*}
\begin{split}
p_{(X,Y)}(x,y) &= p_X(x)p_{Y\mid X}(y\mid x) \propto \frac{1}{[a\log(x) +b]x^{d+1}}\frac{a\log(x) +b}{y^{a\log(x) +b+1}} \\&= \frac{1}{x^{d+1}}\frac{1}{y^{b+1}}e^{-a\log(x)\log(y)},\,\,\,\,\,\,\,\,\,\forall x,y\geq 1.
\end{split}
\end{equation*}



For the backwards model, define $\tilde{\theta}(y) = \alpha\log(y)+\beta$ and $p_Y(y)  \propto \frac{1}{ [\alpha\log(y)+\beta] x^{\delta+1} }$, where $\alpha = a, \beta = d, \delta = b$. Then, we can write
\begin{equation*}
\begin{split}
p_{(X,Y)}(x,y)& = p_Y(y)p_{X\mid Y}(x\mid y) \propto \frac{1}{[\alpha\log(y) +\beta]y^{\delta+1}}\frac{\alpha\log(y) +\beta}{x^{\alpha\log(y) +\beta+1}} \\&
=  \frac{1}{x^{\beta+1}}\frac{1}{y^{\delta+1}}e^{-\alpha\log(x)\log(y)},\,\,\,\,\,\,\,\,\,\forall x,y\geq 1.
\end{split}
\end{equation*}
 Therefore, for all $x,y\geq 1$ holds $p_{(X,Y)}(x,y) = p_X(x)p_{Y\mid X}(y\mid x) = p_Y(y)p_{X\mid Y}(x\mid y)$, where both $p_{Y\mid X}(y\mid x), p_{X\mid Y}(x\mid y)$ correspond to the valid model (\ref{BCPCM}), which is what we wanted to prove. 



Second, we show the other implication. Assume that the causal graph is not identifiable. Therefore, the joint distribution can be decomposed as
 \begin{equation}\label{eq54}
  p_{(X,Y)}(x,y) = p_X(x)p_{Y\mid X}(y\mid x) = p_Y(y)p_{X\mid Y}(x\mid y),
 \end{equation}
where for all $x,y\geq 1$, we have $p_{Y\mid X}(y\mid x) = \frac{\theta(x)}{y^{\theta(x)+1}}$ and  $p_{X\mid Y}(x\mid y) = \frac{\tilde{\theta}(y)}{x^{\tilde{\theta}(y)+1}}$, where $\theta, \tilde{\theta}$ are some non-negative functions and $p_X(x), p_Y(y)$ are some density functions defined on $[1,\infty)$. 
 
 A sufficient statistic for Pareto distribution is $log(x)$. Proposition~\ref{Necessary condition for identifiability} implies that the only form of $\theta, \tilde{\theta}$ can be a linear function of the logarithm. Thus., we write $\theta(x) = a\log(x) + b, \tilde{\theta}(y) = c\log(y) + d$, for some $a,b,c,d>0$. 
 
 Take the logarithm of~(\ref{eq54}) and, using the previous notation, we obtain
\begin{equation*}
 \begin{split}
 \log [p_X(x)] &+   \log\bigg[\frac{a\log(x) + b}{y^{a\log(x) + b+1}}\bigg]
 =  \log[p_Y(y)]+   \log\bigg[\frac{c\log(y) + d}{x^{c\log(y) + d+1}}\bigg].
 \end{split}
 \end{equation*}
By simple algebra, we obtain
 \begin{equation}\label{eq55}
 \begin{split}
 &\log [p_X(x)]+ \log[a\log(x)+b] + (d+1)\log(x)   \\&= \log [p_Y(y)] + \log[c\log(y)+d] + (b+1)\log(y)
   + [(c-a)\log(x)\log(y)].
 \end{split}
 \end{equation}
 We show that equality (\ref{eq55}) implies $c=a$ and the left side of (\ref{eq55}) is constant. Denote the left side of (\ref{eq55}) as  $h_1(x)$, and denote $h_2(y) = \log [p_Y(y)] + \log[c\log(y)+d] + (b+1)\log(y)$. Equality (\ref{eq55}) reads as
 \begin{equation}\label{eq1285}
     h_1(x)- h_2(y)= (c-a)\log(x)\log(y), \,\,\,\,\forall x,y\geq 1. 
 \end{equation}
  The only possibility that the right side of (\ref{eq1285}) is additive is when $c=a$, in which case $h_1(x) = h_2(y) = const.$
  Therefore,
 $$
 \log\bigg\{ [a\log(x)+b] x^{d+1} p_X(x)\bigg\} = const.
 $$
In other words, $p_X(x) = \frac{const.}{ [a\log(x)+b] x^{d+1} }$, which is what we wanted to prove.
\end{proof}


%%%%%%%%%%%%%%%%%%%%% Theorem 2 %%%%%%%%%%%%%%%%%%%%
\subsection{}
Before we prove Theorem \ref{thmAssymetricMultivariatesufficient}, we show the following auxiliary lemma. 
\begin{lemma}\label{PomocnaLemma1}
Let $n\in\mathbb{N}$ and let $\mathcal{S}\subseteq \mathbb{R}$ contain an open interval. Let $f_1, \dots, f_n, g_1, \dots, g_n$ be non-constant continuous real functions on $\mathcal{S}$, such that 
$
f_1(x)g_1(y) + \dots + f_n(x)g_n(y)
$ is additive in $x,y$---that is, there exist functions $f$ and $g$, such that 
$$
f_1(x)g_1(y) + \dots + f_n(x)g_n(y) = f(x) + g(y), \forall x,y\in\mathcal{S}.
$$
Then, there exist (not all zero) constants $a_1, \dots, a_n, c\in\mathbb{R}$, such that 
$\sum_{i=1}^n a_if_i(x) = c$ for all $x\in\mathcal{S}$. Specifically for $n=2$, it holds that $f_1(x) = af_2(x)+c$ for some $a,c\in\mathbb{R}$. 

Moreover, assume that for some $q<n$, functions $g_1, \dots, g_q$ are affinly independent---that is, there exist $y_0, y_1, \dots, y_q\in\mathcal{S}$, such that a matrix 
\begin{equation}\label{matrix243}
M=\begin{pmatrix}
 g_1(y_1) - g_1(y_0) & \cdots & g_q(y_1) - g_q(y_0) \\
\cdots & \cdots & \cdots \\
g_1(y_q) - g_1(y_0) & \cdots & g_q(y_{q}) - g_q(y_0)
\end{pmatrix} 
\end{equation}
has full rank. Then, for all $i=1, \dots, q$ there exist constants $a_{q+1}, \dots, a_n, c\in\mathbb{R}$, such that $f_i(x)=\sum_{j=q+1}^n a_jf_j(x) +c$ for all $x\in\mathcal{S}$. 
\end{lemma}
\begin{proof}
Fix $y_1, y_2\in\mathcal{S}$, such that $g_1(y_1)\neq g_1(y_2)$. Then, we have for all $x\in\mathcal{S}$
\begin{align*}
&f_1(x)g_1(y_1) + \dots + f_n(x)g_n(y_1) = f(x) + g(y_1),\\&
f_1(x)g_1(y_2) + \dots + f_n(x)g_n(y_2) = f(x) + g(y_2),
\end{align*}
and subtraction of these equalities yields 
$$
f_1(x)[g_1(y_1)- g_1(y_2)] + \dots + f_n(x)[g_n(y_1)-g_n(y_2)] = g(y_1) - g(y_2).
$$
Defining $a_i = g_i(y_1)- g_i(y_2)$ and $c = g(y_1)- g(y_2)$ yields the first result (with $a_1\neq 0$).

Now, we prove the ``Moreover'' part. Consider equalities 
\begin{align*}
&f_1(x)g_1(y_0) + \dots + f_n(x)g_n(y_0) = f(x) + g(y_0),\\&
f_1(x)g_1(y_1) + \dots + f_n(x)g_n(y_1) = f(x) + g(y_1),\\&
\dots\\&
f_1(x)g_1(y_q) + \dots + f_n(x)g_n(y_q) = f(x) + g(y_q),
\end{align*}
where $y_0, \dots, y_q$ are defined, such that matrix (\ref{matrix243}) has full rank. Subtracting from each equality, the first equality yields 
\begin{align*}
&f_1(x)[g_1(y_1)- g_1(y_0)] + \dots + f_n(x)[g_n(y_1)-g_n(y_0)] = g(y_1) - g(y_0)\\&
\dots \\&
f_1(x)[g_1(y_q)- g_1(y_0)] + \dots + f_n(x)[g_n(y_q)-g_n(y_0)] = g(y_q) - g(y_0).
\end{align*}
Using matrix formulation, this can be rewritten as 
\begin{equation}
M\begin{pmatrix}
f_1(x) \\
\cdots \\
f_q(x) 
\end{pmatrix} =
\begin{pmatrix}
g(y_1)-g(y_0) -\sum_{j=q+1}^n f_{j}(x)[g_{j}(y_1) -g_{j}(y_0)] \\
\cdots \\
g(y_q)-g(y_0) -\sum_{j=q+1}^n f_{j}(x)[g_{j}(y_q) -g_{j}(y_0)]
\end{pmatrix} .
\end{equation}
Multiplying both sides by $M^{-1}$ indicates that $f_i(x), i=1, \dots, q$ are nothing else than a linear combination of $f_{q+1}(x), \dots, f_n(x)$, which is what we wanted to show.
\end{proof}


\begin{customthm}{\ref{thmAssymetricMultivariatesufficient}}
Let $(X_1, X_2)$ follow the $CPCM(F_1, F_2)$ defined as in (\ref{asymetrical_F_one_F_two_model}), where $F_1$ and $F_2$ lie in the exponential family of continuous distributions and $T_1 = (T_{1,1}, \dots, T_{1,q_1})^\top$, $T_2 = (T_{2,1}, \dots, T_{2,q_2})^\top$are the corresponding sufficient statistics with a nontrivial intersection of their support $\mathcal{S}=supp(F_1)\cap supp(F_2)$.

The causal graph is identifiable if $\theta_2$ is not a linear combination of $T_{1,1}, \dots, T_{1,q_1}$ on $\mathcal{S}$---that is, if $\theta_2$ cannot be written as
\begin{equation}\tag{\ref{eq158}}
\theta_{2,i}(x) \overset{}{=} \sum_{j=1}^{q_1}a_{i,j}T_{1,j}(x)+b_i,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \forall x\in \mathcal{S},
\end{equation}
for all $i=1, \dots, q_2,$ and for some constants $a_{i,j},b_i\in\mathbb{R}$, $j=1, \dots, q_1$. 
\end{customthm}



\begin{proof}
\label{Proof of thmAssymetricMultivariatesufficient}{}
If the $CPCM(F_1,F_2)$ is \textit{not} identifiable, then there exist functions $\theta_1$ and $\theta_2$, such that models $X_1 = \varepsilon_1, X_2 = F_2^{-1}(\varepsilon_2, \theta_2(X_1))$, and $X_2 = \varepsilon_2, X_1 = F_1^{-1}(\varepsilon_1, \theta_1(X_2))$ generate the same joint density function. Decompose the joint density as
\begin{equation}\label{eq59}
  p_{(X_1, X_2)}(x,y) = p_{X_1}(x)p_{X_2\mid {X_1}}(y\mid x) = p_{X_2}(y)p_{{X_1}\mid {X_2}}(x\mid y).
 \end{equation}
Since $F_1$ and $F_2$ lie in the exponential family of distributions, we use the notation from \hyperref[appendix]{Appendix} \ref{appendix_exponential_family} and rewrite it as
\begin{equation*}
    \begin{split}
  &     p_{{X_2}\mid {X_1}}(y\mid x) = h_{1,1}(y)h_{1,2}[\theta_2(x)]\exp[\sum_{i=1}^{q_2}\theta_{2,i}(x)T_{2,i}(y)],\\&
  p_{{X_1}\mid {X_2}}(x\mid y) = h_{2,1}(x)h_{2,2}[{\theta_1}(y)]\exp[\sum_{i=1}^{q_1}{\theta}_{1,i}(y)T_{1,i}(x)].  
    \end{split}
\end{equation*}
After a logarithmic transformation of both sides of (\ref{eq59}), we obtain 
\begin{equation}\label{eq254}
\begin{split}
\log[p_{(X_1,X_2)}(x,y)] &= \log[p_{X_1}(x)] +  \log[h_{1,1}(y)]+\log\{h_{1,2}[\theta_{2}(x)]\} + \sum_{i=1}^{q_2}\theta_{2,i}(x)T_{2,i}(y) \\&
= \log[p_{X_2}(y)] +  \log[h_{2,1}(x)]+\log\{h_{2,2}[\theta_{1}(y)]\} + \sum_{i=1}^{q_1}\theta_{1,i}(y)T_{1,i}(x).
\end{split}
\end{equation}
Define $f(x) = \log[p_{X_1}(x)] +\log\{h_{1,2}[\theta_{2}(x)]\} -\log[h_{2,1}(x)]$ and $g(y) =\log[h_{1,1}(y)] -  \log[p_{X_2}(y)] + \log\{h_{2,2}[\theta_{1}(y)]\}$. Then, equality (\ref{eq254}) reads as 
\begin{equation*}\label{eq9876}
f(x) + g(y) = \sum_{i=1}^{q_1}\theta_{1,i}(y)T_{1,i}(x) - \sum_{i=1}^{q_2}T_{2,i}(y)\theta_{2,i}(x).
\end{equation*}
Finally, we use Lemma \ref{PomocnaLemma1}. We know that functions $T_{2,i}$ are affinly independent in the sense presented in Lemma  \ref{PomocnaLemma1} (see (\ref{eq145151}) in \hyperref[appendix]{Appendix} \ref{appendix_exponential_family}). Therefore, Lemma \ref{PomocnaLemma1} gives us that $\theta_{2,i}, i=1, \dots, q_2$ are only a linear combination of $T_{1, j}, j=1, \dots, q_1$, which is what we wanted to show. 
\end{proof}


\subsection{}\label{consequence}
\textbf{Details corresponding to Consequence \ref{consequenceprva}. }

The density function of the \textbf{ Gamma distribution} with parameters   
$\theta=(\alpha, \beta)^\top$ is in the form $p(x) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha -1}e^{-\beta x}, x>0$. The sufficient statistics are $[T_1(x), T_2(x)] = [\log(x), x]$. 

The density function of the \textbf{Beta distribution} with parameters   $\theta=(\alpha, \beta)^\top$ is in the form $p(x) = \frac{1}{B(\alpha, \beta)}x^{\alpha -1}(1-x)^{\beta -1}$. The sufficient statistics are $[T_1(x), T_2(x)] =[\log(x), \log(1-x)]$. 


Consequence \ref{consequenceprva} follows trivially from Theorem \ref{thmAssymetricMultivariatesufficient}. Theorem \ref{thmAssymetricMultivariatesufficient}  implies that if the causal graph is not identifiable, then $\alpha(x) = a_{1,1}T_1(x) + a_{1,2}T_2(x) + b_1$ and $\beta(x) = a_{2,1}T_1(x) + a_{2,2}T_2(x) + b_2$ for certain constants. These are exactly the conditions in the consequence with a different notation.




%%%%%%%%%%%%%%%%%%%%% Section 2.4%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
\begin{customlem}{\ref{thmMultivairateIdentifiability}}
Let $F_{\textbf{X}}$ be generated by the $CPCM(F_1, \dots, F_d)$ with DAG $\mathcal{G}$ and with density $p_\textbf{X}$. Assume that for all $ i,j\in\mathcal{G}$, $ S\subseteq V$, such that $i\in pa_j$ and  $pa_j\setminus \{i\}\subseteq S \subseteq nd_j\setminus\{i,j\}$, there exist $\textbf{x}_{S}{:}\,\,  p_S(\textbf{x}_S)>0$, such that a bivariate model defined as $X=\tilde{\varepsilon}_X, Y = F^{-1}_j\big(\tilde{\varepsilon}_Y, \tilde{\theta}(X)\big)$ is identifiable (in the sense of Definition \ref{DEFidentifiability}), where  $F_{\tilde{\varepsilon}_X} = F_{X_i\mid \textbf{X}_{S} =\textbf{ x}_S}    $ and $\tilde{\theta}(x) = \theta_j(\textbf{x}_{pa_j\setminus\{i\}}, x)$,  $x\in supp(X)$.

Then,  $\mathcal{G}$ is identifiable from the joint distribution. 
 \end{customlem}
 
 \begin{proof}
 \label{Proof of thmMultivairateIdentifiability}
Let there be two $CPCM(F_1, \dots, F_d)$ models, with causal graphs $\mathcal{G}\neq \mathcal{G}'$, that both generate $F_\textbf{X}$.   From Proposition 29 in \cite{Peters2014} (recall that we assume causal minimality of $CPCM(F_1, \dots, F_d)$), there exist variables $L,Y\in \{X_1, \dots, X_d\}$, such that 
\begin{itemize}
\item $Y\to L$ in $\mathcal{G}$ and $L\to Y$ in $\mathcal{G}'$,
\item $S:=\underbrace{\big\{pa_L(\mathcal{G})\setminus\{Y\}\big\}}_\text{\textbf{Q}}\cup\underbrace{\big\{pa_Y(\mathcal{G}')\setminus\{L\}\big\}}_\text{\textbf{R}}\subseteq \big\{nd_L(\mathcal{G}) \cap nd_Y(\mathcal{G}')\setminus\{Y,L\}\big\} $. 
\end{itemize}
For this $S$, select $\textbf{x}_S$ in accordance to the condition in the theorem. Below, we use the notation $\textbf{x}_S=(\textbf{x}_q, \textbf{x}_r)$ where $q\in \textbf{Q}, r\in \textbf{R}$. Now, we use Lemma 36 and Lemma 37 from \citep{Peters2014}.  Since $Y\to L$ in $\mathcal{G}$, we define a bivariate SCM as  \footnote{Informally, we consider  $Y^\star := Y\mid \{\textbf{X}_S=\textbf{x}_S\}$ and $L^\star:=L\mid \{\textbf{X}_S=\textbf{x}_S\}$.} $$Y^\star=\tilde{\varepsilon}_{Y^\star},\,\,\,\,\,\,\,\,\,\, L^\star = F^{-1}_{L}\big(\varepsilon_L; \theta_L(Y^\star)\big),$$ 
where $\tilde{\varepsilon}_{Y^\star} \overset{D}{=} Y\mid \{\textbf{X}_S=\textbf{x}_S\}$ and $\varepsilon_L\indep Y^\star, \varepsilon_L\sim U(0,1)$. This is a bivariate CPCM with $Y^\star\to L^\star$. However, the same holds for the other direction: Since $L\to Y$ in $\mathcal{G}'$, we can also define a bivariate SCM in the following manner: $$L^\star=\tilde{\varepsilon}_{L^\star},\,\,\,\,\,\,\,\,\,\, Y^\star = F^{-1}_{Y}\big(\varepsilon_Y; \theta_Y(L^\star)\big),$$ 
 where $\tilde{\varepsilon}_{L^\star} \overset{D}{=}L\mid \{\textbf{X}_S=\textbf{x}_S\}$ and $\varepsilon_Y\indep L^\star, \varepsilon_Y\sim U(0,1)$. We obtained a bivariate CPCM with $L^\star\to Y^\star$, which is a contradiction with the pairwise identifiability. Hence,  $\mathcal{G}= \mathcal{G}'$.  
 \end{proof}
 
\subsection{}
 \begin{customlem}{\ref{lemma_o_overparametrizacii}}
Let $F_2$ be a distribution function with one parameter ($q_2=1$) belonging to the exponential family with the corresponding sufficient statistic $T_2$. 
Suppose that the joint distribution $F_{(X_1,X_2)}$ is generated in accordance with model $CPCM(F_2)$ ( \ref{BCPCM}) with graph $X_1\to X_2$. 

Then, there exists $F_1$, such that the model $CPCM(F_1)$ ( \ref{BCPCM}) with graph $X_2\to X_1$ also generates  $F_{(X_1,X_2)}$. In other words, there exists $F_1$, such that the causal graph in $CPCM(F_1, F_2)$ is not identifiable from the joint distribution. 
\end{customlem}
\begin{proof}\label{Proof of lemma_o_overparametrizacii}
The idea of the proof is the following: we select $F_1$, such that its sufficient statistic is equal to $\theta_2$.  Let us denote the original model as
\begin{equation*}
\begin{split}
 & \,\,\,\,\,\,\,  X_1 = \varepsilon_1, X_2 = F_2^{-1}\big(\varepsilon_2, \theta_2(X_1)\big), \varepsilon_2\sim U(0,1), \varepsilon_1\indep \varepsilon_2,
\end{split}
\end{equation*}
where (using notation from Appendix \ref{appendix_exponential_family}) the conditional density function can be rewritten as 
$$
p_{X_2\mid {X_1}}(y\mid x) =h_{2,1}(y)h_{2,2}[\theta_2(x)]\exp[\theta_{2}(x)T_{2}(y)].
$$
We define $F_1$ from an exponential family in the following manner: consider the sufficient statistic $T_1(x) = \theta_2(x)$ for all $x$ in support of $X_1$ and choose $h_{1,1}(x) =  p_{X_1}(x)h_{2,2}[\theta_2(x)]$ and $h_{1,2}(y) = \frac{h_{2,1}(y)}{p_{X_2}(y)}$ for all $y$ in support of $X_2$. Then, a model where 
\begin{equation*}
\begin{split}
 & \,\,\,\,\,\,\,  X_2 = \varepsilon_2, X_1 = F_1^{-1}\big(\varepsilon_1, \theta_1(X_2)\big), \varepsilon_1\sim U(0,1), \varepsilon_1\indep \varepsilon_2,
\end{split}
\end{equation*}
for a specific choice $\theta_1(y) = T_2(y)$ has the following conditional density function: 
$$
p_{X_1\mid {X_2}}(x\mid y) =h_{1,1}(x)h_{1,2}[\theta_1(y)]\exp[\theta_{1}(y)T_{1}(x)] = \frac{p_{X_1}(x)}{p_{X_2}(y)}h_{2,1}(y)h_{2,2}[\theta_2(x)]\exp[\theta_{2}(x)T_2(y)].
$$
Therefore, the joint distribution is equal in both models, since
\begin{equation*}
\begin{split}
   p_{X_1}(x) h_{2,1}(y)h_{2,2}[\theta_2(x)]\exp[\theta_{2}(x)T_{2}(y)] &=p_{X_2}(y) \frac{p_{X_1}(x)}{p_{X_2}(y)}h_{2,1}(y)h_{2,2}[\theta_2(x)]\exp[\theta_{2}(x)T_2(y)]\\
  p_{X_1}(x)p_{X_2\mid {X_1}}(y\mid x) &= p_{X_2}(y)p_{{X_1}\mid {X_2}}(x\mid y).
\end{split}
 \end{equation*}
We found $CPCM(F_1)$ model with graph $X_2\to X_1$ that generates the same distribution. 
\end{proof}


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 