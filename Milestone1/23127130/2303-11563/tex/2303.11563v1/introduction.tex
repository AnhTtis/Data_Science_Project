%


The availability of large-scale, high-resolution hospital operations data has opened up many avenues for the use of predictive modeling to improve patient care. Questions such as ``How likely is a patient at risk for an adverse event or complication that might require transfer into a Medical Intensive Care Unit (MICU) from another unit?'', ``Is a particular patient at risk of developing a \textit{healthcare-associated infection} (HAI) during their visit?'', ``Does a patient have a high risk of mortality?''. These are just a few examples of key predictive tasks which could help healthcare practitioners provide informed and personalized patient care. 

Recent advances in machine learning have enabled predictions in numerous domains. However, machine learning models such as recurrent neural networks, graph convolution networks, and transformers, often leveraged for predictive tasks, assume that the input data is structured. Unfortunately, hospital operations data consists of diverse data types, including architectural diagrams, admission-discharge-transfer logs, inpatient-doctor interactions, room visits, prescriptions, clinical notes, etc. These data tend to be unstructured and high-dimensional. Hence, learning structured representations of entities such as patients, doctors, rooms, and medications from hospital operations data is a key step in enabling the use of off-the-shelf machine learning algorithms for predictive tasks in healthcare settings.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{plots/Overall_framework_batches.PNG}
    \caption{Our overall framework. 
    Our approach \ourmethod learns dynamic embeddings of healthcare entities from heterogeneous interaction log and other hospital operations data.
    For an interaction between a patient $p$ and an entity $e$ at time $t_2$,
    $p$'s dynamic embedding at time $t_1$ (time of $p$'s previous interaction) is projected to time $t_2^-$ (right before $t_2$).
    Then, the static and dynamic embeddings of $p$ and $e$ at time $t_2^-$ and $p$'s static features are used to reconstruct static and dynamic embeddings of entity $e$.
    Finally, dynamic embeddings of $p$ and $e$ get updated to time $t_2$ via update modules.
    We train \ourmethod in batches of interactions in parallel, while maintaining the temporal order of interactions across batches.
    % and $p$'s previous interaction occurred at time $t_1$ such that $t_1 < t_2$
    % These embeddings are then leveraged for different applications in patient care and for case studies.
    }
    \label{fig:overall_framework}
\end{figure*}

There has been some recent interest in learning embeddings in a healthcare setting, primarily focusing on patient embeddings, such as MiME \cite{choi2018mime}. 
% Examples include med2vec \cite{choi2016multi}, GRAM \cite{choi2017gram}, and MiME \cite{choi2018mime}. 
% A key drawback of these approaches is that they all learn \textit{static} embeddings. In a healthcare setting, events that occur over time (e.g., prescription of an antibiotic, transfer to the MICU, exposure to infection, etc.) play a key role and 
%there are multiple confounding factors including medications, exposure to infections, and so on are at play over time and 
A major drawback of these approaches is that they all learn \textit{static} embeddings. In a healthcare setting, events that occur over time (e.g., prescription of an antibiotic, transfer to the MICU, and exposure to infection) play a crucial role, and a single static embedding is not representative enough. For example, a patient with a low risk of HAI at admission time could be considered high risk at a later day if there are patients with HAI in her unit. However, she could be considered low risk again in the future if she is treated with appropriate medications and shows no symptoms. Thus, learning a single embedding for the entire duration of the patient's stay does not capture the evolving nature of the risks involved and the care patient has received. 

Furthermore, none of the existing research on patient embeddings takes interactions between patients and other healthcare entities (e.g., physicians, hospital rooms, and medications) into account. However, capturing these interactions in learned embeddings is critical for numerous healthcare-associated predictive tasks. As an example, consider two interactions that happen in quick succession: (i) a patient $p_1$ in hospital room $r_1$ is prescribed the antibiotic vancomycin, which is commonly used as a treatment for suspected CDI\footnote{
CDI is shorthand for \emph{Clostridioides difficile infection}, a  healthcare-associated infection that affects gastro-intestinal regions leading to inflammation of the colon and severe diarrhea due to disruption of normal healthy bacteria in the colon \cite{CDCCDI}.}, (ii) a patient $p_2$ transfers into hospital room $r_2$ that is in the same unit as room $r_1$. Together, these interactions indicate an elevated risk of C.~diff infection for patient $p_2$, and we want the embeddings we learn to capture this.
%mainly due to the following reasons: (1) Inpatients with a similar diagnosis are likely to receive procedures from physicians with similar specialties and expertise. (2) There is an overlap of the nurses among the inpatients in nearby rooms because nurses tend to provide care to patients in the vicinity. 

There is also a separate thread of research on learning general-purpose dynamic embeddings (e.g., \cite{dai2016deep,kumar2019predicting}). 
%Dai et al.~proposed DeepCoevolve~\cite{dai2016deep} to learn user and item embeddings from their dynamic interactions. Kumar et al~\cite{kumar2019predicting} improved upon DeepCoevolve by adding an embedding projection layer to infer embeddings even when an interaction is not observed and batched training technique for large datasets. 
However, these approaches learn embeddings from \textit{homogeneous} interactions between a user and a predefined item type. Such an approach is not readily applicable in a healthcare setting, where a patient may interact with heterogeneous entities, including physicians, medications, and rooms, which have different impacts on patients. Hence lumping all of these together into a single entity type will limit the discriminative power of the learned embeddings. Another line of related approaches includes dynamic network embeddings
% \cite{zhang2017learning, zhou2018dynamic}.
\cite{zhou2018dynamic}.
However, these approaches are not readily applicable in our setting as they usually require a coarse snapshot representation of the network. 

To address the gap between existing approaches and the requirements of healthcare predictive modeling, we propose \ourmethod to learn dynamic embedding of entities associated with healthcare based on heterogeneous interactions.
\ourmethod learns general-purpose embedding in an unsupervised way which can be used in various predictive modeling tasks that can not be obtained via supervised training.
Specifically, \ourmethod jointly learns dynamic embeddings of patients, doctors, medications, and rooms while preserving hierarchical relationships between medications, doctor specializations, and physical proximity between rooms. 
In order to do so, \ourmethod maintains and updates weights for each specific interaction type and minimizes intra-entity similarity loss 
while performing an auto-encoder training scheme so that the embeddings of patient-entity interactions can re-construct their original features.
% while ensuring the embeddings are predictive of future interactions. 
Our overall framework is presented in Figure \ref{fig:overall_framework}. Our contributions in this paper are as follows: 

\begin{itemize}
    \item We propose \ourmethod, a novel approach for learning dynamic embeddings of healthcare entities from heterogeneous interactions.
    \item \ourmethod enables several healthcare predictive modeling applications, including adverse event prediction, such as transfer to MICU, case severity and mortality prediction.
    \item We perform extensive experiments for evaluating patient embeddings. Our results show that \ourmethod outperforms state-of-the-art baselines in all the healthcare predictive modeling tasks we consider. Moreover, our embeddings are interpretable and meaningful.
    % captures prescriptions, room visits, and doctor encounters over time such that
    % \item Enable novel applications in healthcare predictive modeling
    % \item Extensive experiments
\end{itemize}

%Patient EHR data is rich. Can be exploited for many different tasks. Machine learning can be helpful here, but data is unstructed. Hence learning low-dimensional structured embeddings is helpful.

%Previous medical embedding studies do not work for this task. Static. Fail to capture interactions,]. 

%Jodie/Deepco-evolve also do not work. Work with Homogeneous interactions only. Temporal network interactions also do not work here.

%We overcome all the shorcomings of the previous work by 1) 2) 3) and so on. Our contributions:

% \begin{comment}
% The availability of patient information in EHR and the recent advancement in machine learning enables us to solve healthcare problems in a data-driven way.
% For instance, we can use EHR data to predict the onset of a certain disease of patients.
% To do this, we may need to represent a patient or the patient visit as a vector.
% Traditionally, this is done by carefully designing features based on medical knowledge.
% Recently, there have been developments in learning representations of visits, patients, and even doctors.
% % without in-depth medical knowledge.

% Many existing works on learning patient embeddings are similar in the sense that diagnosis, prescription, and procedure information is captured.
% These works differ from each other depending on what information they try to preserve:
% med2vec \cite{choi2016multi} preserves the similarity of the medical codes in nearby visits by the same patient; GRAM \cite{choi2017gram} extends med2vec by utilizing hierarchy relationship in the medications using medical ontologies; MiME \cite{choi2018mime} preserves relationship among medical codes; Doctor2Vec \cite{biswal2020doctor2vec} utilizes patient embedding to learn doctor embeddings; and inpatient2vec \cite{wang2019inpatient2vec} utilizes temporal aspect in the care by learning daily embeddings of patients.

% % However, to the best of our knowledge none of the existing works utilize the hospital graph structure or physician information in learning patient embeddings.

% However, to the best of our knowledge, none of the existing works capture extrinsic factors such as physicians or hospitalized rooms of patients.
% Here are our observations on why we need to preserve physician or room information to learn patient embeddings:
% \begin{enumerate}
%     \item Inpatients with a similar diagnosis are likely to receive procedures from physicians with similar specialties and expertise.
%     \item There is an overlap of the nurses among the inpatients in nearby rooms because nurses tend to provide care to patients in the vicinity.
% \end{enumerate}
% % These observations and the limitations of existing works on learning patient embedding motivated us to design a framework that learns patient embeddings that capture the observations above.

% Many aspects change dynamically, such as prescriptions and procedures to patients, physicians' medical experience, and patients' history that resided in each room.
% To take these into account, we need to learn patient embedding that changes dynamically, and also, the representation of rooms and physicians should change dynamically.
% Since we can relate physicians to patient visits via procedure and rooms to patient visits via the rooms that patients reside in, we can utilize it to design a framework that learns patient embedding dynamically.
% % We propose heterogeneous interactions dynamic embedding of patients (HIDEP), which is a framework that learns patient embedding dynamically from various sources of interaction history of the patient.
% \end{comment}



