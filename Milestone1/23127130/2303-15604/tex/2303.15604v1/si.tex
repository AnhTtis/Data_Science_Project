%  from xiaohua/felices paper with a target with 4k atoms and 40k molecues, total of 2.628e+6 seconds to screen (~1 month) which comes out to (2.628e+6/ 40000) = 65.7 seconds latency per molecule. Our HDC inference can be done on average on the same DUD-E dataset in approximately .0003 seconds per molecule


\begin{table}[]
    \centering
    \begin{tabular}{c|c}
        Name & Param. Dist. \\
    \hline
    early stopping & [True] \\
    Validation Fraction & [0.1, 0.2] \\
    number of iterations with no change & [2] \\
    alpha & [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2] \\ 
    solver& Adam \\ 
    batch size & np.linspace(8, 128, dtype=int) \\ 
    learning rate & [constant, invscaling, adaptive] \\ 
    activation & [tanh, relu] \\
    hidden layer sizes & [(512, 256, 128), (256, 128, 64), (128, 64, 32)]
    \end{tabular}
    \caption{MLP Hyperparameter Distributions}
    \label{tab:mlp_training_hyperparams}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{c|c}
    Name & Param. Dist. \\ 
    \hline
criterion & [gini, entropy] \\
max depth & [np.linspace(2,np.log2($N_{\text{train samples}}$), 10)] \\
min samples per leaf & [1, 2, 5, 10] \\
bootstrap & [True] \\
oob score & [True] \\
max samples & [np.linspace(10, $N_{\text{train samples}}$, 10)]
                    
    \end{tabular}
    \caption{Random Forest Hyperparameter Distributions}
    \label{tab:random_forest_training_hyperparams}
\end{table}