While drug discovery has traditionally encompassed a long and expensive development process, machine learning has rapidly come of age in recent years with the promise to improve efficiency\cite{Schneider2020-wz}. Modern virtual screening pipelines incorporate multiple computational tools such as molecular docking and molecular dynamics simulations\cite{Lau2021-dr} to progressively filter large collections of molecules to find ``hits'' which exhibit some detectable activity with a target above a threshold. Further stages then distinguish ``leads'' which are those that have the best activity among all available candidates. Leads can then form the basis of proprietary chemical libraries that are used for future development efforts. 


Evidence suggests that the paradigm of focusing design efforts heavily on existing lead molecules is a factor in the expensive and time consuming drug discovery process\cite{Gorgulla2020-xn}. With greater swathes of chemical space becoming feasible to search due to advancements in chemical synthesis such as ``click-chemistry'' numerous works have appeared with progressively more ambitious virtual screening efforts\cite{enamine_real,Lyu2019-qr, Gorgulla2021-it, Lau2021-dr, Stein2020-rd}. Current work suggests that simply expanding the number of compounds that are able to be screened improves the quality of the hit molecules in a screening campaign\cite{Lyu2019-qr}. Thus, a critically important area of current machine learning research is in the investigation of methods to enable larger collections of molecules to be screened\cite{Gentile2020-tj, Clyde2023-vk}. 


A simple criteria to screen molecules is known as the Lipinski rule of 5\cite{Lipinski2004-mc}.


Previous works have investigated machine learning models as surrogates for expensive steps in the virtual screening process. For example, machine learning models can be trained using simpler representations of the drug candidate on a subset of the compounds to be screened (or from an existing supervised labeled data resource) then deployed to screen far more molecules than would other wise be possible using a traditional molecular docking approach\cite{Gentile2020-tj}. Thus as the virtual screening pipeline is highly modular, subsequent methods that are believed to be more accurate but time consuming can progressively be applied. 

An important line of thought to consider however is the achievable scale by machine learning methods available today. In other words, what is the largest collection of molecules one could train on as well perform inference on? As more methods come available, the relative efficiency will also need to be compared 




A fundamental task in molecular modeling is the design of a representation of a molecular structure. As molecules themselves range widely in terms of their size and properties, today there are numerous possible representations to choose from. Constraining the scope of this problem to drug-like molecules (i.e. ligands) and protein structures does focus our discussion on a tractable set of methods. One of the most recognizable representations of a ligand is known as the \textbf{S}implified \textbf{M}olecular-\textbf{I}nput \textbf{L}ine-\textbf{E}ntry \textbf{S}ystem or ``SMILES''. The SMILES embeds the atoms and their covalent bonds for a given molecule using a sequence of character values (e.g. c, C, o, =, ...) that represents a depth-first search (DFS) traversal of the molecule. Thus SMILES can provide a compact yet powerful representation, however they do include limitations. Notably, a molecule may have numerous unique yet equivalent SMILES. Some of these issues can be alleviated to some degree by standardizing the representations through ``canonicalization'', however the representation still lacks finely detailed information regarding the spatial conformation of the molecule though it is possible to represent stereoisomerism. Additionally, the string-based representation must be translated into a numerical representation. This is typically done by mapping the individual possible characters as a one-hot encoding, or by using groups of characters in $n$-gram embedding. Advances in representations for Natural Language Processing (NLP) are generally readily applicable to the SMILES domain and have shown promising results\cite{Chithrananda2020-el}. There are alternatives to SMILES that have been developed in recent history to aid in the development of deep learning based approaches for a range of molecular structure modeling tasks\cite{Krenn2019-rr, OBoyle_undated-nz}.

The ``Extended-Connectivity FingerPrint'' or ECFP is a popular and somewhat orthogonal approach to represent molecular structures as numerical vectors\cite{Rogers2010-xp}. ECFP has been a useful representation for tasks such as substructure searching (original application domain), similarity searching, clustering, as well as modeling Quantitative Structure-Activity Relationships (i.e. QSAR) that arise in the drug discovery domain\cite{Rogers2010-xp}. Often SMILES serves as the raw text-based representation which is then converted into the ECFP or other formats. ECFP more deliberately captures the structural motifs (e.g. R-groups) in a given molecule by producing a count-vector of such motifs that is often converted to binary\cite{Rogers2010-xp}. There are numerous variations to the ECFP algorithm that can be specified at run-time that determine the resolution of the structural motifs embedded in the representation as well as the size of the fingerprint itself, which is commonly chosen as 1024 or 2048 bits (i.e. binary elements in the vector)\cite{Rogers2010-xp}. 

Recent advances in molecular representations have benefited tremendously from the explosion of interest in deep learning (DL). One of the most well-known contributions is known as the Neural FingerPrint (NFP), which proposes a differentiable analog of the ECFP algorithm\cite{Duvenaud2015-kl}. This work demonstrated the utility of the randomly-initialized or ``un-trained'' NFP representation as producing similar results in terms of similarity of arbitrary molecules in terms of Pearson as well as Root Mean Squared Error (RMSE) as input to a linear regression to predict solubility\cite{Duvenaud2015-kl}. Later work progressed by unified work in Graph Convolution Neural Networks (GCNNs) and Message-Passing Neural Networks (MPNNs) to model additional types of differentiable molecular representations, validating the work with predictive tasks in Quantum Chemistry with the QM9 dataset\cite{Gilmer2017-ul, quantum_machine}. 


Today there are numerous approaches \ref{tab:representation_table} for modeling molecular structure in cheminformatics applications in general as well as specifically for applications relevant to drug discovery and design\cite{Sun2019-fa, Chithrananda2020-el}. While it is in general unclear which approaches are best for specific situations, it is still relevant as well as desirable to focus on further improving the computational efficiency of learning and reasoning with such representations. It is the case that drug-like chemical space is enormous and artificial intelligence, machine learning and deep learning will continue to play a crucial role in enabling more effective traversal of the space, however the processing efficiency of these techniques will ultimately limit their effectiveness\cite{Schneider2020-wz}.


Hyperdimensional Computing or HDC, is family of light-weight processing techniques for automated intelligent reasoning that is especially suited for the increasingly large amounts of data encountered in the drug-discovery setting\cite{Thomas2021-uo, Kanerva2009-sz}. HDC uses a high-dimensional yet low-precision binary independent and identically distributed (i.i.d) representation of the data (i.e. encoding) in combination with a similarity metric defined in the domain of the binary representation. In contrast to many machine learning and deep learning approaches, backpropagation is not required to train the method and inference simply consists of comparing a query ``hypervector'' (HV) to a limited set of canonical class HVs using the applicable similarity metric. Due to the algorithmic simplicity, HDC is readily able to accelerated at the hardware level in chips such as Field-programmable Gate Arrays (FPGAs) and Application-Specific Integrated Circuits (ASICs)\cite{Karunaratne2019-zu}, making the potential application to drug-discovery related cheminformatics tasks attractive as a strategy to process the increasingly large amounts of molecular data. While there has been previous work on the development of HDC methods to process SMILES-based representations on a set of molecular property classification tasks\cite{Ma2021-xu}, our work expands upon this to investigate the feasibility of HDC methods under a variety of potential base feature representations and encoding methods. Further, our work is the first to study the method in the context of QSAR to screen potential ``hit'' molecules from a large collection of data.