\subsection{Comparing to previous work through MoleculeNet}
To sanity-check our methods as well as facilitate comparison to other previously published as well as future works, we consider a broad set of classification tasks from MoleculeNet that have been previously studied in the context of HDC and the SMILES-based encoding methods described in Section~\ref{sec:smiles_encoding}\cite{Wu2018-or, Ma2021-xu}:

To facilitate comparison between our proposed methods, we follow the evaluation protocol previously proposed in MoleHD\cite{Ma2021-xu}. As in the case of MoleHD\cite{Ma2021-xu}, we consider various splitting strategies that account for potential structural biases between the training and testing sets. This has become a standard practice after several studies pointed out the need to account for properties such as ligand structural similarity between training and testing sets that tends to inflate the values of relevant performance metrics~\cite{Ellingson2020-mg, Jones2021-al, Feinberg2018-eg, Wu2018-or}. Unless otherwise stated, all algorithms described use the extended connectivity fingerprint extracted using the RDKit computational chemistry toolkit. We use the default parameters with 1024 bits and a radius of 2. The benchmark random forest (RF) and multilayer-perceptron (MLP) are implemented using the scikit-learn python machine learning toolkit. All sklearn-models have hyperparameter optimization using 10 samples of random search with $k=5$ cross validation to choose optimal model hyperparameters. The best model is then instantiated and trained on the full training set. HDC models are trained using $D$=10,000 dimensional hypervector space with a maximum of 10 epochs for perceptron-style retraining. No further optimizations are performed. All HDC and scikit-learn (with optimal parameters) models are trained using 10 random seeds and all performance results are averaged over these seeds, error bars denote standard deviation over these runs. 


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/bbbp_roc.png}
    \caption{Receiver Operation Curve for Blood-brain-barrier permeability evaluation. Each bar represents mean ROC-AUC score over 10 random seeds and the black vertical lines indicate the variance over the 10 seeds. All methods significantly outperform random. Our proposed ECFP based methods (HDBind-ECFP and HDBind-RPFP) strictly outperform the SMILES and SELFIES based HDC models and achieve competitive performance with the RF and MLP baselines on a random split of the data. The trend is also observed in the case of scaffold split where ligand similarity between the training and test sets is accounted for and minimized to some degree, suggesting the structural information present in the ECFP-based representations provides a representation the the HDC model is better able to generalize to new chemical space with.}
    \label{fig:bbbp_roc}
\end{figure}



In Figure \ref{fig:bbbp_roc} we give results for BBBP in conjunction with the random forest and multilayer perceptron (MLP) benchmark algorithms. In the case of randomly splitting the data into training and testing sets, our result in this case matches similar performance to previous state of the art results\cite{Wu2018-or} and is better than random in terms of the receiver operating characteristic - area under the curve (ROC-AUC) score for all models considered. Similarly for the case of the chemical structure-informed scaffold splitting strategy, all models considered perform better than random by a significant margin. For random split, the worst HDC method still performs reasonably well considering the performance of the best model (RF) in terms of the ROC-AUC metric. Similarly, while all models perform worse on the scaffold split as expected, the gap in performance between the worst HDC model and the best baseline is not particularly large. 


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/clintox_roc.png}
    \caption{Barplots of ROC-AUC scores for the ClinTox dataset classification task of identification of FDA approved drugs. The MoleHD approaches in this case are strictly superior to all other HDC and baseline ML methods as well as the HDBind SELFIES encoding method. SMILES string based representations were previously demonstrated to achieve high performance on this task, thus this provides an additional sanity check on our approaches with a consistent observation in our specific context \cite{Wu2018-or}.}
    \label{fig:clintox}
\end{figure}


In Figure \ref{fig:clintox} we give results for the ClinTox dataset\cite{Wu2018-or} using both random and scaffold splitting strategies. Models are trained according the previously described protocol. In the case of both train-test splitting strategies (random and scaffold), the SMILES string-based atomwise encoding method described by\cite{Ma2021-xu} outperforms all other methods. It is notable that the best previously reported SOA on this task (random split) was a text-based convolutional neural network (CNN). Across the board, our performance metrics in terms of the random split are similar to the performance previously reported\cite{Wu2018-or}. Our metrics for the scaffold splitting strategy are similar (nearly consistently lower) however the MLP takes a dramatic hit in performance. This behavior is not surprising given the nature of MLP having significantly higher degrees of freedom than HDC-based approaches, thus making them prone to overfit on smaller datasets. 


\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/sider_roc.png}
    \caption{Distributions of ROC-AUC scores over the 27 SIDER binary classification tasks. On random split and scaffold split, all models follow roughly the same order in terms of relative performance with slight increases in variance over the 27 tasks in the case of scaffold split. Our HDBind HDC approaches as well as the ECFP fingerprint trained baseline ML models consistently outperform the SELFIES and MoleHD SMILES based approaches.}
    \label{fig:sider}
\end{figure}


Lastly for our MoleculeNet evaluation, we consider the SIDER dataset that includes 27 distinct binary classification tasks. We train each model separately for each task using the previously described protocol for hyperparameter optimization. Additionally we consider random and chemical structure-informed scaffold splitting strategies. In Figure \ref{fig:sider} we show the performance of each method using the ROC-AUC metric to allow for comparison to previously reported state of the art results on these tasks. In each boxplot, we give the distribution of the ROC-AUC across the 27 distinct classification tasks for each method. In the case of both splitting strategies, our results are similar to those that have been previously reported with more advanced techniques\cite{Wu2018-or}. Additionally, our novel ECFP-based encoding methods demonstrate a significant improve across both splits as compared to the SMILES-based methods previously reported in MoleHD\cite{Ma2021-xu}.


\subsection{DUD-E classification of Active and Inactive Molecules}

Subsequently, we assess the viability of the HDC approaches on the task of distinguishing active versus decoy (inactive) ligands for a set of 38 protein targets collected from the Directory of Useful Decoys-Extended (DUD-E)\cite{Mysinger2012-hn}. This task requires one to develop a method that is able to identify ``hit'' molecules from a potentially large collection of candidates. While it has been shown that DUD-E exhibits a high degree of structural bias among the active and decoy molecules for each protein \cite{Ellingson2020-mg, Jones2018-ci}, we consider this task as an additional sanity check for our approaches as it is well studied. DUD-E is also well known to be highly imbalanced, with the approximate ratio of actives to decoys being $1:50$\cite{Mysinger2012-hn, Jones2018-ci}. 

We focus our analysis on alternative metrics that are more informative for the case of identifying hit compounds that may potentially be verified experimentally after using a computational protocol to select the most promising candidates from a large collection of molecules. In Figure \ref{fig:dude_enrich} we give enrichment factor distributions over the 38 DUD-E human protein targets in our dataset. For each target a separate collection of active and decoy molecules are provided. We directly compare to previous work using the well known Vina scoring function within the LLNL-developed ConveyorLC HPC docking toolkit\cite{Zhang2014-iq}. 


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/dude_enrich.png}
    \caption{ Distributions of the enrichment metric are reported across all 38 DUD-E human protein targets. Results are reported as the average over 10 random seeds. (a) Enrichment at top 1\% of screened library for DUD-E dataset. Nearly all models achieve the same median enrichment factor (approx. 46-48) and improve greatly upon the Vina molecular docking scoring function (3.4) (b) Enrichment at top 10\% of screened library. Similarly, all models outperform Vina by a significant margin, demonstrating a consistent pattern of improvement. Vina achieves a median enrichment factor of 2.2 while the best HDC method achieves 9.9, competitive with the RF (9.9) and MLP (9.9) baselines. HDBind-ECFP and HDBind-RPFP are notably the better models in the case of (b) with considerably lower variance as compared to the MoleHD SMILES string based HDC methods.}
    \label{fig:dude_enrich}
\end{figure}


\subsection{LIT-PCBA Challenge Dataset}

In the practical setting, the number of actives for a given target will be significantly outnumbered by inactive molecules. A challenge with datasets such as DUD-E\cite{Mysinger2012-hn} is that while traditionally the set has served a role as the gold-standard benchmark for docking algorithms and other virtual screening methods including AI and deep learning-based, significant biases that have been demonstrated to over-inflate the performance of models have been identified\cite{Chaput2016-qr, Wallach2018-lf, Chen2019-gd, Sieg2019-gy}. Thus it has been the subject of much recent research in development of more challenging benchmark datasets to aid in the development of more robust models\cite{Wallach2018-lf}. The recently proposed LIT-PCBA\cite{Tran-Nguyen2020-xq} benchmark provides a rigorous test set that is more reflective of the true expected performance of the various predictive modeling techniques considered in virtual screening research. Subsequent studies of various techniques have been reported in the literature that clearly illustrates the challenges posed by this particular dataset\cite{Jiang2021-wr, Tran-Nguyen2021-jw}. Our intent in reporting on this dataset is to test the limits of our various approaches relative to each other as well as to what has been previously reported. We do not expect to outperform more sophisticated techniques as it is clear our motivation is in leveraging the clear computational efficiency possessed by our approaches with the goal being to achieve better than random performance. As this particular dataset is highly imbalanced, we elect not to report accuracy as it is highly uninformative in this setting. Rather we choose to report the \textit{Reciever operating characteristic} or ROC which gives the true positive rate (TPR) as a function of the false positive rate (FPR), where the Area under the curve of a perfect classifier is the metric (ROC-AUC). 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/lit_pcba_enrich.png}
    \caption{Distributions are defined the enrichment metric are reported across all 15 protein targets and include agonists, inhibitors, and antagonists. Results are reported as the average over 10 random seeds. (a) Enrichment at top 1\% of screened library for LIT-PCBA dataset. Both the RF and MLP strictly dominate the HDC-based methods in terms of enrichment factor metric (33.6 and 22.8 respectively) versus the best HDC method, HDBind-ECFP (6.1). (b) Enrichment at top 10\% of screened library for LIT-PCBA dataset. The gap between the median enrichment factor between RF and MLP (5.4 and 4.3 respectively) is considerably smaller with the best HDC approach, HDBind-ECFP (3.0). In both cases, the best HDC approach is based upon our ECFP encoding methods.}
    \label{fig:dude_enrich_10}
\end{figure}
