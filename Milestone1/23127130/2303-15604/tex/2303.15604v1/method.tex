\subsection{Hyperdimensional Computing (HDC)}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/HD_Diagram.png}
    \caption{Description of the HD computing paradigm. (a) The full set of components is given by some encoder that maps the raw input data to a high-dimensional space, the resulting set of hypervectors, and the corresponding labels. (b) Training or ``building'' the associative memory is formed for each of the $k$ classes by bundling the hypervectors belonging to a given class into a canonical representation. (c) Inference is done by evaluating a similarity metric between a query hypervector and each of the canonical class representations stored in the associative memory, then assigning the class label that maximizes similarity to the query.}
    \label{fig:hd_fig}
\end{figure} 

We briefly describe the HDC workflow that is depicted in Figure~\ref{fig:hd_fig}. At a high level, HDC works by relating an input ``data-space'' $\mathcal{X}$ with an encoding dimension $\mathcal{H}$ by way of a function $\phi: \mathcal{X} \rightarrow \mathcal{H}$. Formally, $\mathcal{H}$ is defined as a real-valued inner-product space however in practice $\mathcal{H}$ is usually restricted to be defined over integers in the range $[-b, b]$ where $b=1$. Restricting to integers facilitates the acceleration of the method in hardware by using simpler arithmetic representations. Considering high-dimensional vector spaces for $\mathcal{H}$ (e.g. $\{-1,1\}^{10,000}$) results in useful properties gleaned from the ``curse of dimensionality``. More specifically, in higher dimensional spaces, nearly all hypervectors are unrelated and can be considered as \textit{quasi-orthogonal}, thus it is possible to assign semantically meaningful objects to unique hypervectors\cite{Thomas2021-uo, Yu2022-no}. 
Given the encodings of the data $h \in \mathcal{H}$ generated by $\phi$, the $h$ can then be manipulated using simple element-wise operators, ``binding'', ``bundling'',  and permutation. 
The bundling operator is defined as $\oplus: \mathcal{H} \times \mathcal{H} \rightarrow \mathcal{H}$, which maps a pair of hypervectors $(h_i, h_j)$ to a new hypervector $h_k$ that is similar to both, in our case we implement this as the element wise sum of the $\phi(x_i)$ in our dataset: 
\begin{equation}
    \label{eq:bundling}
    h = \phi(x) = \bigoplus_{i}^{n} \phi(x_i)
\end{equation}
Thus, bundling allows for the composition of information from disparate sources into a single $h$. 

The binding operator is used to create ordered tuples of points in $\mathcal{H}$ and is defined as $\otimes: \mathcal{H} \times \mathcal{H} \rightarrow \mathcal{H}$. Binding is used to relate features to values and we specify it as:
\begin{equation}
\label{eq:binding}
    h = \phi(\mathrm{x}) = \bigoplus_{i=1}^{n} \psi(f_i) \otimes \phi(x_i)
\end{equation} 
Thus, binding can be used to build data structures such as the \textit{item memory} (IM) wich maps the possible alphabet symbols $a \in \mathcal{A}$ to unique hypervectors. Encoding then proceeds by mapping the input data point to a sequence of its constituent symbol hypervectors $h_a$, then aggregating these using a specific user-defined \textit{encoding} protocol. We discuss the specific implementations of $\phi$ in Section \ref{sec:hdc_molecule_encoders}.


\subsection{Learning in HDC}
\label{subsec:learning_in_hdc}
 
The initial epoch of \textbf{training} or building the \textit{Associative Memory} (i.e. AM) proceeds by \textit{bundling} the hypervectors for each class $k$ into $h_{k}$, producing a canonical representative vector 
(eq. \ref{eq:bundling}) with a single pass of the training set. The subsequent \textbf{re-training} epochs refine the model by testing the prediction obtained for a given sample and updating $h^k$ when the prediction is incorrect, analogous to the perceptron algorithm \cite{Thomas2021-uo}. To perform \textbf{inference} on a query data point $x^q$ we compute:
\begin{equation}
\label{eq:hdc_inference}
    \hat{y} = \underset{k \in 1,\ldots,K}{\text{argmax }} \langle h^k, \phi(x^q) \rangle
\end{equation}
where $\langle ., .\rangle$ denotes a user specified similarity metric \cite{Thomas2021-uo}. We opt to use the cosine similarity in order to compare $h^q$ and $h^k$. 



\subsection{Encoding molecular data for HDC}
\label{sec:hdc_molecule_encoders}

\subsubsection{MoleHD SMILES encoding}
\label{sec:smiles_encoding}

There are three approaches that we explore in context of the previous work MoleHD\cite{Ma2021-xu}; $n$-gram, atomwise, and SMILES-Pair Encoding\cite{Ma2021-xu, Li2021-pf}. 
$n$-gram encoding assumes the existence of structure in the data where consecutive windows of the input string carry information regarding context in a sentence. The simplest form ($n=1$ or uni-gram) treats each possible valid SMILES symbol as an individual element in the encoding. Thus one smiles string of length $n$ may at most have $n$ unique symbols that represent it assuming no duplicates. However a limitation of this approach is exemplified by the fact ``Cl'' would be decomposed into ``C'' and ``l'', resulting in an (incorrect) additional carbon atom and an invalid symbol in the representation. It it also possible that for arbitrary choices of $n$, additional artifacts may be introduced into the representation. We consider the cases of uni-gram($n=1$), bi-gram ($n=2$), and tri-gram ($n=3$). A simple improvement to the uni-gram algorithm instead treats atoms coherently as single symbols (e.g. ``Cl'' is treated as one symbol) can be called the \textit{atom-level} encoding. 


Recently, the SMILES Pair Encoding (SPE) was introduced to address limitations of the aforementioned encoding strategies. The SPE method is based on the ``byte-pair encoding'' (i.e. BPE)\cite{Sennrich2016-ns} that has become widely used in the NLP community, resulting in massive successes such as Dall-E 2\cite{Ramesh2022-lg}. SPE works by identifying high-frequency SMILES sub-strings in the atomwise representation (i.e. keep atoms coherent in tokens) and iteratively merging them in a bottom-up approach. SPE ensures that the most common sub-strings are assigned to unique tokens. Benefits of SPE are that it provides some ability to encode meaningful higher-level molecular substructures such as functional groups while also providing a compact representation that improves the computational efficiency of learning. 

We study each of these sequence representations of smiles (char-level, atom-level, SPE) and use uni-gram ($n=1$), bi-gram ($n=2$), and tri-gram ($n=3$) to further encode the strings in to an alphabet of symbols $\mathcal{A}$. Thus, for each input smiles string $x_i$, we iterate over each symbol and retrieve its $h_a$, then we shift the representation elements to the right by $j$ position to encode the order information, then we add the result to the $0$-initialized $h_i$ describing $x_i$. The representation is then converted to a bipolarized binary representation where positive entries are mapped to 1 and entries less than or equal to 0 are mapped to -1. Learning then proceeds as described in Section\ref{subsec:learning_in_hdc}. We implement our methods using the SmilesPE python package\cite{Li2021-pf}. We found that the atomwise and SPE tokenizers consistently led to the best SMILES-based HDC models. 


\subsubsection{HDBind ECFP encoding}

The ECFP representation of a molecule is widely used in computational chemistry for tasks such as similarity search in chemical libraries and in machine learning as features for prediction of a range of molecular-structure based predictive tasks. ECFP is based on the \textit{Morgan algorithm}\cite{Morgan1965-zp} (i.e. \textit{MorganFP}) which was proposed to solve the molecular isomorphism problem. The MorganFP\cite{Morgan1965-zp} algorithm represents the molecular structure explicitly as a graph where atoms are treated as nodes and covalent bonds are treated as edges. The ECFP algorithm makes changes to MorganFP that improve efficiency, such as using a user-defined iteration limit, using a cache to store intermediate atom identifiers between iterations, and a hashing scheme to record resulting representations encountered. Thus, ECFP effectively uses a bottom-up approach to collect progressively larger molecular substructures that are guaranteed to preserve the graph structure and coherency. ECFP allows for a user to specify the number of bits in a representation, commonly chosen as 1024 or 2048. Further, a maximum radius size (i.e. number of bond ``hops'' from a root atom) for collecting substructure-graphs is specified. Thus, ECFP can be tweaked to allow for varying degree of resolution in the resultant representation.

In the case of ECFP representation, we take a similar approach to the SMILES based representation. The ECFP is made up of $n$-bits, with each value indicating the presence (bit=1) or lack thereof (bit=0) for a chemical substructure. While the SMILES representation allows for the different tokens to be specified in arbitrary order, the ECFP has a user-specified fixed number of bits. To compute the item memory (IM), we proceed by drawing random $(h_I, h_N) \in \{0,1\}^D$ to represent the presence of a substructure $h_I$ versus lack thereof $h_N$. We then draw random $h_p ~ \{0,1\}^D$ to represent the $p$-th position of the substructure. We then encode the $x_i$ iterating over the bits indexed by $j$, selecting the value hypervector for position $j$ and binding it with the position hypervector at index $j$. Our method can readily generalize to encode a larger dynamic range of values beyond the binary case, thus supporting ECFP count vectors, provided one is able to \textit{apriori} specify the maximum count value.


\subsubsection{HDBind Random projection fingerprint (RPFP) encoding}
A simple encoding method is to use a random linear projection of the data into the HDC space:
\begin{equation}
    h = \text{sign}(W^{\top}X)
\end{equation}

where the weight matrix $W$ is initialized according to a bernoulli distribution $p^{k}(1-p)^{1-k}$, where $p=0.5$ and possible values lie within the set $\{0,1\}$. Subsequently, the sample generated from the bernoulli distribution is transformed to a value in the set $\{-1,1\}$ by multiplication by 2, then subtraction by 1. The bias term $b$ is omitted (i.e. zero-valued vector). This encoding method allows for the mapping of a larger range of representation types as it lacks the restriction of a problem specific encoding alphabet. The viability of this approach remains yet unknown for encoding molecular representations.



\subsubsection{HDBind SELFIES encoding}

The SELFIES (SELF-referencIng Embedded Strings) representation was recently proposed by \cite{Krenn2020-zn} in order to address issues in chemical generative modeling tasks. SELFIES are defined by a formal grammar, a concept in theoretical computer science. A valid SELFIES corresponds to a valid molecule, a guarantee not provided by the SMILES representation. Additionally, SELFIES are able to handle supporting numerous constraints that are subject to ongoing research\cite{Krenn2022-un, Lo2023-mp}. However, it is not clear what representational advantages SELFIES possesses as compared to the SMILES representation beyond chemical validity properties previously mentioned. We propose two straightforward encoding schemes for SELFIES that are somewhat analogous to the SMILES encoding presented in Section \ref{sec:smiles_encoding}.

The first method simply tokenizes a SELFIES string into the indvidual characters present, mapping each of the unique characters to a corresponding randomly drawn binary hypervector of dimension $D$. Next, we bundle the character hypervectors present in a given molecules SELFIES to form the molecule hypervector. The second method instead only tokenizes the unique SELFIES terminals present in a given string. Similarly to the previous method, the unique terminals are mapped to corresponding randomly drawn binary hypervectors. Subsequently we bundle the together the terminal hypervectors present in a given SELFIES to form the molecule hypervector.



\subsection{Metrics}
We employ a variety of metrics to assess the classification performance of our HD-based methods that go beyond simply measuring accuracy. This is particularly important in the domain of virtual screening as the class distributions are significantly different in terms of size. To facilitate comparison with previous work on the MoleculeNet suite of molecular machine learning benchmarking datasets\cite{Wu2018-or} which uses the Reciever Operating Characteristic - Area Under the Curve (ROC-AUC) to measure performance between different models. ROC-AUC is defined by computing the True Positive Rate (TPR) as a function of the False Positive Rate (FPR), defined in equations \ref{eq:FPR} and \ref{eq:TPR}, at different thresholds of a classifiers score (e.g. probability of being positive class).  



\begin{align}
\label{eq:TPR}
    \text{TPR} = \frac{TP}{TP + FN}
\end{align}

\begin{align}
\label{eq:FPR}
    \text{FPR} = \frac{FP}{FP + TN}
\end{align}


In the domain of drug discovery, it is also common to encounter the Enrichment Factor (EF) metric\cite{Bender2005-vc, Gentile2020-ku}, which attempts to measure how well a screening method may be able to improve the density of actives in a large database of molecular candidates. While this metric does not require a screening method to necessarily be as accurate as other more expensive methods, it does favor methods that are able to reliably \textit{rank} molecules in such a way that it becomes more likely to draw a successful candidate after a set has been filtered. Being able to improve the speed at which this can be done also invites the possibility of pushing the boundaries in terms of number of initial candidates that can be considered. Following the work of \cite{Zhang2014-iq}, we define the EF as: 

\begin{align}
    \text{EF}^{\%} = \frac{\text{actives}_{\text{sampled}}}{\text{actives}_{\text{total}}} \frac{\text{N}_{\text{total}}}{\text{N}_{\text{sampled}}} 
\end{align}
where $\text{actives}_{\text{sampled}}$ is the number of actives in a sample at $X\%$ of the database, $\text{actives}_{\text{total}}$ is the total number of actives in the database, $\text{N}_{\text{total}}$ is the total size of the database, and $\text{N}_{\text{sampled}}$f is the size of the sample at $X\%$ of the database.


%\subsection{HD Confidence Estimation}

We use the equation introduced in MoleHD\cite{Ma2021-xu} to measure the similarity of a query hypervector $h_q$ to the active class hypervector $h_a$ in order to compute a measure that can be used to sort compounds to approximate likelihood of binding. 

\begin{equation}
    \eta = \frac{1}{2} + \frac{1}{4} (\delta(h_{q}, h_{a}) - \delta(h_q, h_i))
\end{equation}



\subsection{HDC acceleration}

We compare the speed of performing docking on the DUD-E dataset\cite{Zhang2014-iq}, in terms of a single compound, and compare to performing inference for binding activity using HDC. According to previous work on the same set of 38 protein targets, performing a screen for a target with approximately 4,000 atoms against a library of 40,000 molecules would require approximately 1 month or a total of $2.628 \times 10^6$ seconds to screen (approximately 65.7 seconds per molecule) on a single CPU and approximately 1 hour (3600 seconds / 40000 molecules = .09 seconds per molecule) on 700 CPUs \cite{Zhang2014-iq}. 

\begin{equation}
\label{eq:speedup}
    % t_{s} = \frac{t_{\text{model}}}{t_{\text{vina}}}
    t_{s} = \frac{t_{\text{baseline}}}{t_{\text{model}}}
\end{equation}

In table \ref{tab:dude_speedup} we give the latency (in seconds) to compute inference on the average molecule from the DUD-E dataset we consider in this work. Specifically, the average test time for each model is divided by the average number of molecules in each of the 38 target test sets to arrive at the latency per molecule estimate. We compute the speedup according to equation \ref{eq:speedup}.


\begin{table}[]
    \centering
    \begin{tabular}{c|c|c}
    \textbf{Method} & \textbf{Median Latency (per molecule (s))} & \textbf{Speedup} \\
      % Vina (1-CPU)   &  $65.7$\cite{Zhang2014-iq} & -\\
      % Vina (700-CPU) & $9 \times 10^{-2}$\cite{Zhang2014-iq} & 730\\
      RF & $2.78 \times 10^{-5}$ & - \\
      % AutoDockVina\cite{Eberhardt2021-ql} & 22.3 & - \\
      MLP & $1.04 \times 10^{-5}$ & 2.68 \\ 
      MoleHD-Atomwise & $3.79 \times 10^{-7}$ & 73.39 \\ 
      MoleHD-BPE & $4.10 \times 10^{-7}$ & 67.89 \\
      HDBind-Selfies & $3.07 \times 10^{-7}$ & 90.57 \\ 
      HDBind-ECFP & $3.80 \times 10^{-7}$ & 73.18 \\
      HDBind-RPFP & $3.50 \times 10^{-7}$ & 79.37 \\ 
      
    \end{tabular}
    \caption{Inference comparison on DUD-E benchmark dataset. Measured latency for AutoDock Vina is 22.3 seconds per molecule, demonstrating the dramatic efficiency improvement for each ML and HDC method considered in our work. Speedup in this case is measured according to the slowest ML model, the Random Forest (RF). All HDC methods (MoleHD and HDBind) significantly outperform the baseline ML methods.}
    \label{tab:dude_speedup}
\end{table}
%TODO: do we need to incorporate training timing in the speedup? what does this mean and how does this change the world and why ? Can we screen 30 billion on a cluster right now? what can we do now that we couldn't do before?
% is there a way to calculate out

% can process 1 billion molecules with RF of ER of 5 vs process 10 billion molecules with HDC with ER of 3? does it makes sense 

% chain together the HDC and MLP, compare with HDC and RF, 
\subsection{Computer Hardware Specifications}

All models are run on in the same hardware environemnt, namely the Pascal GPU cluster at Lawrence Livermore National Laboratory. All models were run on a single node of Pascal. A Pascal node features 2x NVIDIA P100 GPUs, an Intel Xeon E5-2695v4 CPUs with 36 physical cores, and 256GB of main memory.


\subsection{Sci-kit Learn Training and Parameter Selection}

Sci-kit learn was used for the implementation of the Multilayer Perceptron (MLP) and Random Forest baseline approaches. Our goal was not to exhaustively search parameter space to find an optimal model, rather to use a lightweight routine to find a simple yet reasonable model. We use random search for hyperparameter optimzation, using 10 samples of hyperparameters along with 5 fold cross validation for a total of 50 fits per model per classification task. 

