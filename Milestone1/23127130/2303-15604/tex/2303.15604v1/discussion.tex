We have demonstrated a range of methods for encoding molecular data for their use hardware-accelerated HDC-based classification methods. We demonstrated their relative performance on a number of representative classification tasks in the domain of molecular design and optimization of drug-like molecules. Future work will leverage the progress made in acceleration of HDC methods in hardware, leading to enormous gains in efficiency over state of the art neural network based approaches which require gradient-based training methods. The ability to rapidly train models with lightweight HDC-based approaches can enable more rapid exploration of the enormous drug-like chemical space. Further, the techniques proposed here can be adapted to other molecular optimization domains such as materials design. Our work can also extend to arbitrary molecular representations that can be learned with a neural network, thus allowing for HDC to co-exist as an efficient tool for subsequent training and inference stages. With more efficient use of available compute resources, the ecological impact of moving towards training and inference with lightweight methods can provide significant advantages over continued reliance on more expensive techniques. 

Our results for HDC-based classification here are largely built upon the SMILES and ECFP representations, thus there may be great potential in leveraging ongoing research in molecular representation learning, notably in the context of Large Language Models (LLMs) such as ChemBERTa\cite{Chithrananda2020-el} and GPT-4\cite{OpenAI2023-lz}. Future work will investigate emerging representations that can potentially improve upon our predictive performance. Ideas from current research in attention mechanisms and transformer based architectures. Leveraging ongoing research in unsupervised learning and using these representations as the basis of our hypervectors. specifically learning these quanitized representations in a deep learning architecture trained on large collections of molecules.  