%%The template is adapted from IEEE Computer Society format.

%% bare_jrnl_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% Biometrics Council journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Biometrics Council needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
  \usepackage{graphicx}
  \usepackage{amsmath}
  \usepackage{amssymb}
  \usepackage{booktabs}
  \usepackage{algorithm}
  \usepackage{algorithmic}
  \usepackage{multirow}
  \usepackage[dvipsnames,table,xcdraw]{xcolor}
% \usepackage{colortbl}
% \usepackage{color}
  \usepackage{tikz}
  \usepackage{booktabs}
  \usepackage{threeparttable}
  \usepackage{graphicx}                                                           
  \usepackage{float} 
  \usepackage{times}
  \usepackage{epsfig}
  \usepackage{subfigure}
  \else
  % normal IEEE
  \usepackage{cite}
 
\fi
%\usepackage[pagebackref,breaklinks,colorlinks,citecolor=red]{hyperref}
% % \usepackage[dvipsnames,table,xcdraw]{xcolor}
% \usepackage[usenames,dvipsnames,svgnames,table,x11names]{xcolor}
% \usepackage[pagebackref=false,breaklinks,colorlinks,citecolor=RoyalBlue,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}

% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Biometrics Council  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Biometrics Council format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Unsupervised Gait Recognition with \\ Selective Fusion}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Biometrics Council journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc 
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Xuqian Ren,
        Saihui Hou,
        Chunshui Cao,
        Xu Liu and
        Yongzhen Huang
        % <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem Xuqian Ren is with the Computer Science Unit, Faculty of Information Technology and Communication Sciences, Tampere Universities, Tampere 33720, Finland. This work is finished when she was an intern at Watrix Technology Limited Co. Ltd before becoming a Ph.D. candidate at Tampere Universities.
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
(E-mail: xuqian.ren@tuni.fi)

\IEEEcompsocthanksitem Saihui Hou and Yongzhen Huang are with School of Artificial Intelligence,
Beijing Normal University, Beijing 100875, China and also with Watrix Technology Limited Co. Ltd, Beijing 100088, China. (E-mail: housai hui@bnu.edu.cn, huangyongzhen@bnu.edu.cn). They are the corresponding authors of this paper.

\IEEEcompsocthanksitem Chunshui Cao and Xu Liu are with Watrix Technology Limited Co. Ltd, Beijing 100088, China.}% <-this % stops an unwanted space
% \thanks{Manuscript received April 19, 2005; revised August 26, 2015.}
\protect
}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Biometrics Council Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Biometrics Council journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% or like this to get the Biometrics Council new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2015 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Biometrics Council\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Biometrics Council jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Biometrics Council papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
%Previous deep gait recognition methods primarily trained with supervised learning require painful label effort.
%
%When adopting a pre-trained model to a new dataset without fine-tuning, the performance degrades much.
%
%And if the new dataset does not have any label, the model needs to be trained in an unsupervised way.
%
%So to make the pre-trained gait recognition model able to be fine-tuned on unlabeled datasets, we propose a new task: Unsupervised Gait Recognition (UGR). 
%
%This technique aims to improve the accuracy of training gait recognition models with an utterly unlabeled training set.
%
%We propose a new cluster-based baseline to solve UGR with cluster-level contrastive learning.
%
%With the baseline, the UGR task can be partly solved, but we further find more challenges this task meets.
%
%The first one is that sequences in different clothes of one person tend to gather into different clusters since their appearance changes a lot when the clothes change.
%
%The second one is that sequences in around $0^{\circ}$ and $180^{\circ}$ do not cluster with sequences in other views because they lack walking postures.
%
%To solve these two challenges, we propose a Selective Fusion method, which includes Selective Cluster Fusion (SCF) and Selective Sample Fusion (SSF).
%
%With SCF, we can pull matched clusters in different clothes of the same person closer by ranking potential cluster pairs in a support set and updating the cluster-level memory bank used in our baseline with a multi-cluster update strategy.
%
%And in SSF, we merge sequences around front/back views with sequences in other views together gradually with curriculum learning.
%
%Extensive experiments show the effectiveness of our method in improving the rank-1 accuracy in walking with different coats (CL) condition and front/back views conditions.

Previous gait recognition methods primarily trained on labeled datasets, which require painful labeling effort.
%
However, using a pre-trained model on a new dataset without fine-tuning can lead to significant performance degradation.
%
So to make the pre-trained gait recognition model able to be fine-tuned on unlabeled datasets, we propose a new task: Unsupervised Gait Recognition (UGR).
%
We introduce a new cluster-based baseline to solve UGR with cluster-level contrastive learning.
%
But we further find more challenges this task meets.
%
First, sequences of the same person in different clothes tend to cluster separately due to the significant appearance changes.
%
Second, sequences taken from $0^{\circ}$ and $180^{\circ}$ views lack walking postures and do not cluster with sequences taken from other views.
%
To address these challenges, we propose a Selective Fusion method, which includes Selective Cluster Fusion (SCF) and Selective Sample Fusion (SSF).
%
With SCF, we merge matched clusters of the same person wearing different clothes by updating the cluster-level memory bank with a multi-cluster update strategy.
%
And in SSF, we merge sequences taken from front/back views gradually with curriculum learning. Extensive experiments show the effectiveness of our method in improving the rank-1 accuracy in walking with different coats condition and front/back views conditions.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Gait Recognition, Unsupervised Learning, Contrastive Learning, Curriculum Learning.
\end{IEEEkeywords}}


% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc 
% or transmag modes are not selected <OR> if conference mode is selected 
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
% Biometrics Council journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.




% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 介绍步态识别，引出主要无监督步态识别问题
\IEEEPARstart{W}{ith} the growing intelligent security and safety camera systems, gait recognition has gradually gained more attention and exploration due to its non-contact, long-term, and long-distance recognition properties. 
%
Several works~\cite{chao2019gaitset,fan2020gaitpart,lin2021gait} attempt to solve gait recognition tasks and have reached significant progress in a laboratory environment. 
%
However, gait recognition in a realistic situation will be affected by many factors such as occlusion, dirty label, or labeling, and more.
%
Among them, the problem of labeling is one of the biggest challenge because it demands intensive manual effort to label pairwise data. 
%
Moreover, deploying pre-trained models in new test environments without any adaptation often suffers from severe performance deterioration due to the domain gap across different datasets.
%
So it is necessary to train gait recognition models on unlabeled datasets, which saves a lot of human and financial resources.


%----------------------------------------
To realize gait recognition trained with an unlabeled dataset, we introduce a new task called \textbf{Unsupervised Gait Recognition} (abbreviated as UGR) to facilitate the research on training gait recognition models with new unlabeled datasets.
%
And here we focus on using silhouettes as input to conduct gait recognition.
%
When only using silhouettes of human walking sequences as input, due to lacking enough information, we observe two main challenges in UGR, as shown in Figure~\ref{fig:Introduction}.
%
First, due to the large change in appearance, sequences in different clothes of a subject are hard to gather into one cluster without any label supervision.
%
Second, sequences captured from front/back views, such as views in $0^{\circ}/018^{\circ}/162^{\circ}/180^{\circ}$ in CASIA-B~\cite{yu2006framework}, are challenging to gather with sequences taken from other views of the same person because they lack vital information, such as walking postures.
%
Furthermore, these sequences tend to cluster into small groups based on their views or get mixed with sequences of the same perspective from other subjects.
%
So in this paper, we provide methods to overcome them accordingly.

\begin{figure}[t]
	\centering	 
	\includegraphics[width=\linewidth]{figures/Introduction.pdf}	 	
	\caption{Two main challenges in UGR. A kind of style in different color denotes a subject in different clothes, which are usually erroneously assigned with different pseudo labels (\textit{e.g.}, `001', `002'). Also, sequences taken from front/back views of different subjects tend to mix together (\textit{e.g.}, `003').}
	\label{fig:Introduction}
 % \vspace{-0.6cm}
\end{figure}


Currently, some Person Re-identification (Re-ID) works~\cite{lin2019bottom,ge2020self,lin2020unsupervised,wang2020unsupervised,dai2021cluster,9978648} have already touched the field of identifying person in an unsupervised manner.
%
We follow the state-of-the-art pattern in~\cite{dai2021cluster}, a cluster-based framework with contrastive learning, as a baseline to realize UGR.
%
However, when directly adopting this framework, we find the performance still needs to be improved, especially in the \textbf{walking with different clothes (CL)} condition.
%
To address the two challenges, we propose a new method called \textbf{Selective Fusion}, which comprises two techniques: Selective Cluster Fusion (SCF), which selects candidate clusters of the same subject wearing different clothes and pulls them closer; and Selective Sample Fusion (SSF), which handles front/back view samples of each subject.
%

To be specific, first, in SCF, we use a support set selection module to generate a support set for each cluster and design a multi-cluster update strategy to help update the cluster centroid of each pseudo cluster in the memory bank.
%
By utilizing this approach, we not only improve the tightness of the clusters themselves but also encourage clusters of the same individual in different clothing to be influenced by the current clustered groups and pulled closer towards them.
%
Second, we design the SSF to deal with samples taken from front/back views. 
%
In SSF, we utilize a view classifier to identify sequences captured from front/back views. We then employ curriculum learning to gradually incorporate these sequences with those captured from other views.
%
Namely, the sequences are absorbed at a dynamic rate, relaxing the aggregate requirement for each cluster.
%
This approach enables us to re-assign pseudo labels for sequences captured from front/back views, thereby encouraging them to cluster with sequences captured from other views.
%
With our method, we gain a large recognition accuracy improvement comparing to the baseline (with GaitSet~\cite{chao2019gaitset} backbone: NM + 3.1\%, BG + 8.6\%, CL + 9.7\%; with GaitGL~\cite{lin2021gait} backbone: BG + 1.1\%, GL + 17.2\% on CASIA-BN dataset~\cite{yu2006framework}\footnote{NM: normal walking condition, BG: carrying
bags when walking, CL: walking with different coats})
To sum up, our contributions mainly lie in three folds:

\begin{itemize}
    \item[$\bullet$] We introduce a new task Unsupervised Gait Recognition (UGR) for performing gait recognition using unsupervised learning, which is a practical approach but requires careful consideration. To address this task, we establish a baseline that employs cluster-level contrastive learning.
    
    \item[$\bullet$]  We deeply explore the characteristic of UGR, finding the two most critical challenges we need to overcome: gathering sequences with different clothes and with front/back views. We propose a \textbf{Selective Fusion} method to tackle the two main problems by select potential matched cluster/sample pairs to help them fuse gradually.
    
    \item[$\bullet$] Extensive experiments on two popular gait recognition benchmarks have shown that our method can bring consistent improvement over baseline, especially in the walking in different coats condition. 
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}\label{sec:relatedwork}

Most existing gait recognition works are trained in a supervised manner, in which cross-cloth and cross-view labeled sequence pairs have been provided.
%
They mainly focus on learning more discriminative features~\cite{liao2020model,li2020jointsgait,chao2019gaitset,fan2020gaitpart,lin2021gait,9916067,9229117,9913216,10042966} or developing gait recognition applications in natural scenes~\cite{hou2022gait,das2023gait,9870842,9928336}.
%
Despite the abundance of research in gait recognition, there is still a need for further exploration of its practical applications.
%
Here we consider one practical setting and work as one of the first attempts to realize gait recognition without labeling training datasets.

\subsection{Supervised Gait Recognition}

\noindent\textbf{Model-based method:} This kind of method encodes poses or skeletons into discriminative features to classify identities.
%
For example, PoseGait~\cite{liao2020model} extracts handcrafted features from 3D poses based on human prior knowledge. 
%
JointsGait~\cite{li2020jointsgait} extracts spatio-temporal features from 2D joints by GCN~\cite{yan2018spatial}, then map them into discriminative space according to the human body structure and walking pattern.
%
GaitGraph~\cite{gaitgraph} use human pose estimation to extract robust pose from RGB images, and encode the keypoint as node and joints as skeleons in the Graph Convolutional Network to extract gait information.

\noindent\textbf{Appearance-based method:} This series of methods mostly input silhouettes, extracting identity information from the shape and walking postures.
%
GaitSet~\cite{chao2019gaitset} first extracts frame-level and set-level features from an unordered silhouette set, promoting the set-based method's development.
%
GaitPart~\cite{fan2020gaitpart} further includes part-level pieces of information, mining details from silhouettes.
%
In contrast, the video-based method GaitGL~\cite{lin2021gait} employs 3D CNN for feature extraction based on temporal knowledge.
%
Our method can be used in appearance-based unsupervised gait recognition.
%
Since our method mainly focuses on realistic gait recognition with a large amount of data, we choose silhouettes as input for its computation saving and robustness.
%
In our framework, we adopt both the set-based method and the video-based method as the backbones to illustrate the generalization of our framework.


\subsection{Unsupervised Person Re-identification}
\textbf{Short-term Unsupervised Re-ID:}
Most fully-unsupervised Re-ID methods estimate pseudo labels for sequences, which can be roughly categorized into the clustering-based and non-clustering-based methods. 
%----------------------------------------
Clustering-based methods~\cite{zeng2020hierarchical,wang2021camera,chen2021ice,xuan2021intra} first estimate a pseudo label for each sequence and train the network with sequence similarity.
%
In contrast, non-clustering-based methods~\cite{lin2020unsupervised,wang2020unsupervised} regard each image as a a class and use a non-parametric classifier to push each similar image closer and pull all other images further.
%
In total, the accuracy of most non-cluster-based methods do not exceed the latest cluster-based methods, so we use the latter to solve UGR.
%

At present, there are some typical algorithms in clustering-based methods.
%
BUC~\cite{lin2019bottom} utilizes a bottom-up clustering method, gradually clustering samples into a fixed number of clusters. Though there is a need for more flexibility, it is a good starting point. 
%
HCT~\cite{zeng2020hierarchical} adopts triplet loss to BUC to help learn hard samples. 
%
SpCL~\cite{ge2020self} introduces a self-paced learning strategy and memory bank, gradually making generated sample features closer to reliable cluster centroids. 
%
To alleviate the high intra-class variance inside a cluster caused by camera styles, CAP~\cite{wang2021camera} proposes cross-camera proxy contrastive loss to pull instances near their own camera centroids in a cluster. 
%
ICE~\cite{chen2021ice} further explores inter-instance relationships instead of using camera labels to compact the clusters with hard contrastive loss and soft instance consistency loss. 
%
IICS~\cite{xuan2021intra} also considers the difference caused by cameras, decomposing the training pipeline into two phases. First, considering each camera as a domain, IICS categorizes features within every single camera and generates intra-camera labels. Second, according to sample similarity across cameras, inter-camera pseudo labels will be generated based on all instances. These two stages train CNN alternately to optimize features.
%
Cluster-contrast~\cite{dai2021cluster} improves SpCL by establishing cluster-level memory dictionary, optimizing and updating both CNN and memory bank at the cluster level.
%

Inspired by the simple but elegant structure, we start from the Cluster-contrast framework to solve UGR.
%
However, owing to the cloth-changing complexity and lack of posture in the front/back view, directly applying Cluster-contrast will lead to sequences in different clothes or with front/back views separate into different clusters. 
%
As a solution, we design specific modules to gradually merge samples from different clothes and views.
%----------------------------------------

On the contrary, the non-clustering-based methods mainly realize fully-unsupervised Re-ID with similarity-based methods. 
%
SSL~\cite{lin2020unsupervised} predicts a soft label for each sample and trains the classification model with softened label distribution. 
%
MMCL~\cite{wang2020unsupervised} formulates FUL Re-ID as a multi-label classification task and classifies each sample into multiple classes by considering their self-similarity and neighbor similarity.
 

\noindent\textbf{Long-term Unsupervised Re-ID:}
%
Most FUL Re-ID methods are based on short-term Re-ID datasets, however, gait recognition is a long-term task with cloth-changing, so long-term FUL Re-ID is more similar to UGR.
%
CPC~\cite{li2022unsupervised} use curriculum learning~\cite{bengio2009curriculum} strategy to incorporate easy and hard samples and gradually relax the clustering criterion.
%
We do not use the same method in SCF, since each cluster mainly contains sequences with one cloth type, and it is better to pull clusters as a whole.
%
In contrast, we use curriculum learning in SSF to distinguishingly deal with sequences in front/back views and re-assign pseudo labels for sequences in these views progressively.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Our Method}
In this work, we propose a new task called Unsupervised Gait Recognition (UGR), which is practical when dealing with realistic unlabeled gait datasets.
%
In this section, we first formally define our technique.
%
Next, we will show our baseline based on Cluster-contrast~\cite{dai2021cluster}, trying to solve UGR with an unlabeled trainset.
%
Then, we deeply research the problems faced by UGR and find two challenges to improve the accuracy: sequences in different clothes of the same person tend to form different clusters, and the sequences captured from front/back views are difficult to gather with other views.
%
Based on the two problems, we propose Selective Fusion to gradually merge cross-cloth clusters and sequences taken from front/back views to make samples of each subject gathered tighter.
%
To clarify the narrate, we summarize the symbols we use in Table~\ref{tab:symbol}.

% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[]
\centering
\caption{The definition of important symbols}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|c}
\toprule
Symbol & Definition \\ \midrule
$\mathcal{X}_u/\mathcal{X}_t$ & \begin{tabular}[c]{@{}c@{}}Unlabeled training dataset/\\ Labeled testing dataset\end{tabular} \\ \hline
$\mathcal{Y}_u/\mathcal{Y}'_u$ & The true/ pseudo label for training dataset \\ \hline
$\mathcal{Y}_t$ & The true label for testing dataset \\ \hline
$f_\theta$ & Gait Recognition backbone \\ \hline
$f_n/f_{CA}$ & Feature of original/cloth augmented sequence \\ \hline
$\mathcal{C}_k$ & $k$-th cluster center \\ \hline
$M$ & neighbors of each sequence search \\ \hline
$s_{up}$ & clustering threshold \\ \hline
$\tau$ & The temperature hyper-parameter \\ \hline
$m$ & The momentum hyper-parameter \\ \hline
$\mathcal{L}_{CNCE}$ & ClusterNCE Loss \\ \hline
$\mathcal{C}_{+}/\mathcal{C}_{q}$ & The positive/ $q$-th cluster \\ \hline
$\mathcal{C}_{low}$ & The lower bound to judge FVC \\ \hline
$\mathcal{S}_{c}/\mathcal{S}_{i}$ & \begin{tabular}[c]{@{}c@{}}The current/initial similarity bound \\ when re-assign pseudo labels for sequences in FVC  \end{tabular}  \\ \hline
$\lambda/\lambda_{base}$ & \begin{tabular}[c]{@{}c@{}}The ratio/base ratio to merge \\ extreme view sequences in each epoch\end{tabular} \\ \hline
$\mathcal{C}_n/\mathcal{C}_o$ & The number of new or old clusters in each epoch \\ \hline
$q[:]$ & The support set for $q$-th cluster \\ \hline
$k$ & The number of candidate clusters \\ \bottomrule
\end{tabular}%
}

\label{tab:symbol}
\end{table}


%----------------------------------------
\subsection{Problem Formulation}
We first define an unlabeled training dataset with cloth-changing as $\mathcal{X}_u=\{x_{1}, x_2, ..., x_N\}$, where $N$ is the total sequence number.
%
Then, we assume its ground truth label is set as $\mathcal{Y}_u$, but we cannot obtain it during training.
%
We want to train a gait recognition backbone $f_\theta$ to classify these sequences according to their similarity.
%
During evaluation, $f_\theta$ will extract features from a labeled test dataset $\{\mathcal{X}_t, \mathcal{Y}_t\}$ and the gallery will rank according to their similarity with the probe, then we gain the rank-1 accuracy for each condition and each view.
%
We aim to train $f_\theta$ and gain the best performance on $\mathcal{X}_t$.


%----------------------------------------
\subsection{Proposed Baseline}
Without any supervision, it is not easy to train $f_\theta$. 
%
Since there is no pre-trained model in the Gait Recognition field like in Re-ID, we choose OUMVLP~\cite{takemura2018multi} to train a pre-trained model and gain cross-view prior knowledge, Since OUMVLP has a large number of subjects and various views, which is an ideal dataset to obtain identity classification ability.
%
Then we modify Cluster-contrast~\cite{dai2021cluster} to build our baseline framework to transfer the knowledge learned from OUMVLP to other datasets. 
%
The training pipeline can be summarized as followings:
%

1) At the beginning of each epoch, we first use $f_\theta$ to extract features from each sequence in parts, which has been sliced by Horizontal Pyramid Matching (HPM)~\cite{chao2019gaitset} horizontally and equally. 
%
Then we concatenate all the parts to an embedding and regard it as a sequence feature to participate in the following process, denoted as $f_n,\ n \in \{1,2,...,N\}$. 
%
In this way, we can consider features in parts and details.
%

2) We adopt KNN~\cite{fix1989discriminatory} to search $M$ neighbors for each sequence in feature space and calculate the similarity distances between each other. 
%
Then InfoMap~\cite{rosvall2008maps} is used to cluster $f_n$ with a similarity threshold $s_{up}$, and predict a pseudo label $\mathcal{Y}'_u= \{y'_{1},y'_{2},...,y'_{N} \}$ for each sequence.
%
When mapping features with a pre-trained model, each sequence tends to be mapped closer and not well separated.
%
So we tighten $s_{up}$, aiming to separate each subject into single cluster.
%

3) With the pseudo labels, we compute the centroids of each cluster and then initialize a memory bank at the cluster-level to store these centers.
%

4) During each iteration, a mini-batch will be randomly selected from pseudo clusters, and during gradient propagation, we update the backbone with a ClusterNCE loss~\cite{dai2021cluster}.
\begin{equation}
\label{ClusterNCE}
    \mathcal{L}_{CNCE}=-\log \frac{\exp \left( f_n\cdot \mathcal{C}_+/\tau \right)}{\sum_{q=0}^Q{\exp}\left( f_n\cdot \mathcal{C}_q/\tau \right)}
\end{equation}
where $Q$ is the cluster number gained in the current epoch, $\mathcal{C}_+$ is the positive cluster the feature belonging to, $\mathcal{C}_q$ is the $q$-th clusters, $\tau$ is the temperature hyper-parameter. Here we use all the positive features in the mini-batch to calculate $\mathcal{L}_{CNCE}$.
%

5) Also, we update the memory bank at the cluster-level.
\begin{equation}
    \mathcal{C}_q\gets m\mathcal{C}_q+(1-m)f_n
\end{equation}
$m$ is the momentum hyper-parameter used to update the centroids impacted by the batch.

%----------------------------------------
With this pipeline, we can initially realize UGR.
%
However, some defects still prevent further improvement in both cross-cloth and cross-view situations.
%
First, each cloth condition of a subject has been separated away from each other, making it hard to group them together into a single class.
%
This is because of the large intra-class diversity within each subject when the identity changes cloth and subtle inter-class variance between different persons when cloth types of different subjects are similar.
%
For example, NM and CL of one person are less similar in appearance to NM of other persons, leading to the intra-similarity being smaller than the inter-similarity.
%
Second, some sequences in front/back views (such as $0^{\circ}/018^{\circ}/162^{\circ}/180^{\circ}$) cannot correctly gather with sequences in other views, but tend to be confused with front/back views sequences of other subjects.
%
This is because sequences in these views lack enough walking patterns, so the model can only use the shape information to classify these sequences. With more similarity in appearance, sequences of different subjects in these views tend to be classified together.
%
Necessary solutions need to be considered to help the framework solve the two problems.
%----------------------------------------

\subsection{Proposed Method}

To tackle the problems we pointed out for UGR, we develop Selective Fusion, containing Selective Cluster Fusion (SCF) and Selective Sample Fusion (SSF) to solve the two drawbacks separately.
%
The framework of our method is in Figure~\ref{fig:structure}, and the pseudo-code is shown in Algorithm~\ref{our}.
%
\begin{figure*}[t]
	\centering	 
	\includegraphics[width=\linewidth]{figures/structure.pdf}
	\caption{Overview of the framework with Selective Fusion. The upper two branches generate pseudo labels and initialize a memory bank at the start of each epoch. The lower branch accepts mini-batch extracted from pseudo clusters and calculates ClusterNCE Loss with Memory Bank to update it and the backbone in each iteration. CA is the \textit{Cloth Augmentation} method. InfoMap is employed in the Cluster module. SCF means \textit{Selective Cluster Fusion}. SSF represents \textit{Selective Sample Fusion}. In the Support set, the darker the color, the higher the similarity with the target cluster (Best viewed in color).}
	\label{fig:structure}
\end{figure*}


\begin{algorithm}[t]
\caption{The training procedure of Selective Fusion}
\label{alg:algorithm}
\textbf{Input}:  $\mathcal{X}_u$;  $f_\theta$
\begin{algorithmic}[1]  %1表示每隔一行编号
\REQUIRE Epoch number; iteration number; batch size; \\
hyper-parameters: $M/s_{up}/\tau/m/\mathcal{C}_{low}/\lambda_{base}/k/\mathcal{S}_{i}$
\FOR {epoch in range(0, epoch number+1) }
    \STATE {Apply CA to $\mathcal{X}_u$, get the augmented $\mathcal{X}_u$;}
    \STATE {Extract $f_n$ from $\mathcal{X}_u$ by $f_\theta$;}
    \STATE {Extract $f_{CA}$ from augmented $\mathcal{X}_u$ by $f_\theta$;}
    \STATE {Generate $\mathcal{Y}'_u$ for $f_n$}
    \STATE {Calculate the pseudo centroids with $\mathcal{Y}'_u$ and $f_n$;}
    \STATE {Generate adjusted $\mathcal{Y}'_u$ in SSF;}
    \STATE {Generate adjusted centroids with adjusted $\mathcal{Y}'_u$ and $f_n$;}
    \STATE {Initialize Memory Bank with adjusted centroids;}   
    \STATE {Generate support set in Support Set Selection Module with the centroids of $f_{CA}$ and $f_n$;}
    \FOR {iter in range(0, iteration number+1) }
    \STATE {Extract mini-batch from pseudo clusters;}
    \STATE {Extract sequences feature from batch;}
    \STATE {Calculate ClusterNCE loss with Memory Bank according to Eq.\textcolor{red}{~\ref{ClusterNCE}};}
    \STATE {Update the backbone;}
    \STATE {Update the Memory Bank according to Eq.\textcolor{red}{~\ref{momentum}};}
    \ENDFOR
\ENDFOR \\
\textbf{Output}: $f_\theta$
\end{algorithmic}
\label{our}
\end{algorithm}

 
In our method, at the beginning of each epoch, we use a Cloth Augmentation (CA) method to randomly generate an augmented variant for each sequence in the training dataset, then put them into the same backbone to extract features, named $f_{n}$ and $f_{CA}$.
%
Second, after getting the original pseudo labels generated by InfoMap, we use SSF to adjust the pseudo labels and then apply them to $f_{n}$ and $f_{CA}$ to get their adjusted centroids.
%
The centroids of $f_{n}$ are used to initialize the memory bank.
%
Third, we use a support set selection module to generate a support set for each cluster, which will be used in the multi-cluster update strategy during backpropagation to help update the memory bank.
%
The support set selection module and multi-cluster update strategy are the two components of our SCF.
%

Next, we will introduce how we implement our Cloth Augmentation, SCF, and SSF method.
% After using InfoMap in the Cluster module, we can get pseudo labels for each original feature, then apply them to $f_{n}$ and calculate cluster centroids.
% %
% However, since the pseudo label has many mistakes, we first put centroids and $f_n$ into SSF to re-assign pseudo labels for each feature, assigning the label of other views to the sequence in front/back views and get the adjusted pseudo labels for each sequence. 
% %
% With the new adjusted pseudo labels, we assign them to $f_{n}$ and gain the final adjusted centroids.
% %
% Also, the adjusted pseudo labels can be used to classify $f_{DA}$, and the centroids of $f_{n}$ and $f_{DA}$ will be put into Support Set Selection, generating the support set for each cluster.
% %
% The adjusted centroids of $f_{n}$ will initialize Memory Bank, storing the centers.
% %
% During each iteration, a batch of sequences will be selected according to the adjusted pseudo labels, then input backbones to extract features in the batch.
% %
% Next, all the batch features will be used when calculating ClusterNCE loss with Memory Bank.
% %
% When conducting gradient backpropagation, we update the backbone as well as use a Multi-cluster Update strategy to update the Memory Bank.
% %
% The Suppor Set Selection Module and Multi-cluster Update Strategy are the two components of our SCF.
% %
% Next, I will introduce how we implement our Data Augmentation, SSF, and SCF.





%----------------------------------------
\subsubsection{Cloth Augmentation}

The cloth augmentation is conducted for each sequence in training set to explicitly get a fuse direction for each cluster, which simulates the potential clusters in other conditions belonging to the same person.
%
Currently the cloth augmentation methods we use are target for silhouettes datasets, which are the majority algorithms work on.
%
We randomly dilate or erode the upper/bottom/whole body in the whole sequence with a probability of 0.5\footnote{The kernel size for upper part: $5 \times 5$, lower part: $2 \times 2$}, forming six cloth augmentation types.
%
Also, the upper/middle/bottom has a dynamic edited boundary\footnote{The boundary selected from upper bound: [14, 18], middle bound: [38, 42], bottom bound:[60, 64] for $64 \times 64$ silhouettes}, adding more variance to the augmentation results.
%
Here we visualize some cloth augmented results in Figure~\ref{fig:DA}. When dilating NM, the sequence can simulate its corresponding CL condition, and when eroding CL, the appearance of subjects can be regarded as in NM condition.
\begin{figure}[ht]
	\centering	 
	\includegraphics[width=\linewidth]{figures/DA.pdf}	 	
	\caption{The visualization of data augmentation on NM and CL conditions. Cloth Augmentation can simulate the potential appearance in different conditions of the same person.}
	\label{fig:DA}
\end{figure}
It is indeed that in the real-world cloth-changing situations, the clothes have more diversity, only dilating or erode cannot fully simulate all the situations.
%
Currently we first consider the simple cloth-changing situation that walking with or without coats, which is also the cloth-changine method in CASIA-B~\cite{yu2006framework} and Outdoor-Gait~\cite{song2019gaitnet} dataset, to prove our method is valid.
%
When the situations become complex, other cloth augmentation method can be employed, such as sheer the bottom part of the silhouettes to simulate wearing a dress, adding oval above the head to simulate wearing a hat.
%
But since such real conditions don't appear in these common datasets, adding more cloth augmentation method will hinder we verify the effectiveness of our method.
%
%----------------------------------------
\subsubsection{Selective Cluster Fusion}
SCF aims to pull clusters in different clothes belonging to the same subject closer.
%
It comprises two parts, a support set selection, which is used to generate a support set for each cluster, aiming to find potential candidate clusters in different clothes, and a multi-cluster update strategy, aiming to decrease the distance between candidate clusters in the support set.
%

\textbf{Support Set:} The input of the support set selection module is the adjusted centroids of $f_{n}$ and $f_{CA}$.
%
By calculating the similarity between the CA centroids of each cluster with the original centroids of $f_n$, we can get a rank list with these pseudo labels, ranging from highest to lowest according to their similarity distances.
%
We select the top $k$ ids in each rank list, and the first id we set is the cluster itself, formulating the support set.
%
With the support set, we can concretize the optimization direction when pulling NM and CL together because $f_{CA}$ can be seen as the cross-cloth sequences in reality to some extent.
%
With the explicit regulation, we will not blindly pull a cluster close to any near neighbor.

\textbf{Multi-cluster update strategy:} When updating the memory bank during backpropagating, we use the support set in the multi-cluster update strategy.
%
Knowing which clusters are the potential conditions of one person, the new strategy can be formulated as follows:
\begin{equation}
\label{momentum}
    \mathcal{C}_{q[:k]} \leftarrow m \mathcal{C}_{q[:k]}+(1-m) f_{n}
\end{equation}
All the candidate clusters in $q[:k]$ need to participate in updating the memory bank.
%
By forcing the potential conditions to fuse, we can make the mini-batch influence clusters and clusters in the support set, compressing the distance between cross-cloth pairs.



%----------------------------------------
\subsubsection{Selective Sample Fusion}

Seeing that walking postures are absent in front/back views, sequences taken from these views have less feature similarity with features extracted from other views.
%
So they cannot be appropriately gathered into their clusters like other views, and tend to mix up with sequences of other identities captured from front/back views.
%
If we pull all the clusters towards their candidate clusters in the support set, those clusters mainly composed of sequences taken from front/back views will further gather with clusters with the same view condition, making the situation worse. 
%
To deal with clusters in this condition, we design SSF, in which we use curriculum learning to gradually re-assign pseudo labels to sequences in front/back views, forcing them to fuse with samples taken from other views progressively before conducting the SCF method.
%

\textbf{View classifer:} Specifically, we first train a view classifier on OUMVLP, classifying whether the sequence is in the front/back view.
%
The view classifier structure we set is as same as the GaitSet structure we used, and we add the BNNeck behind. 
%
We assign the label 1 for sequences in $0^{\circ}/180^{\circ}$ and assign the label 0 for sequences in other views to train the view classifier. 
%
We train the view classifer on OUMVLP to gain view knowledge, and when we have prior knowledge on sequences' view, we can quickly identify which clusters generated by our framework are composed of sequences taken from front/back views.
%

Indeed, it is true that when adapting the view classifier to other datasets, there may be appearance domain gaps and view classification gaps between different datasets, as other datasets may not label views in the same way as OUVMLP, and some datasets may not provide view labels at all.
%
But from our observations, we found the view classification task is not a hard task, the view classifier can already identify most sequences with less walking postures.
%
And to further solve the problem that some sequences haven't appropriately assigned the right view label, we set a threshold to relax the requirement when clustering.
%
The criterion is that only when the number of sequences in front/back view in one cluster is larger than the threshold $\mathcal{C}_{low}$, we consider the cluster as Front/Back View Clusters (FVC).
%
%train:0.8865567216391804
%test:0.8850931677018633
By dissolving FVC, we calculate the similarity between each sequence in it with other centroids of non-FVC, and if the similarity is larger than $\mathcal{S}_{c}$, we re-assign the nearest non-FVC pseudo label for the sequence.
%
So it does not matter whether we identify all the sequences with front/back views. We only need to push most of these sequences closer to other views.
%
And with the training process of curriculum learning, the clusters are tighter. The sequences identified with front/back views will also push the sequences that were misidentified closer to their cluster center automatically since they have appearance similarity. 
%
We will further propose more intelligent methods to reduce the dependence of the model on the prior information.

\textbf{Curriculum Learning:} However, we do not incorporate all the sequences in FVC at the same time, instead, we utilize Curriculum Learning~\cite{bengio2009curriculum} to fuse them progressively by enlarging the $\mathcal{S}_{c}$ during each epoch:
%
\begin{equation}
    \mathcal{S}_{c} = \mathcal{S}_{i} - \lambda \times epoch\_number
\end{equation}
%
In the first epoch, $\mathcal{S}_{i}$ is higher, and only the similarity between sequences in FVC and centroids of other non-FVC higher than $\mathcal{S}_{i}$, can these sequences be assigned new pseudo labels. 
%
Otherwise, they will be seen as outliers and cannot participate in training.
%
During training, the criterion gradually increases with a speed of $\lambda$, which allows for the gradual incorporation of new knowledge.
%
% To tighten the clusters and make more samples can be fused, we adopt a Center Loss:
% \begin{equation}
%     \mathcal{L} _C=\sum_{i=1}^{batch}{ \| f_{i}-c_{y'_i} \| _{2}^{2}}
% \end{equation}
% $f_i$ is the sequence feature in batch, $c_{y'_i}$ is the centroids of its corresponding pseudo cluster.
%
In response to this, we propose to set $\lambda$ adaptively~\cite{hou2019learning}:
\begin{equation}
\lambda=\lambda_{base} \left| \frac{\mathcal{C} _n}{\mathcal{C} _o} \right|
\end{equation}
where $\mathcal{C}_n$ and $\mathcal{C}_o$ is the number of new and old clusters in each epoch, $\lambda_{base}$ is a fixed constant for each dataset. 
%
Since more clusters fused in each phase, $\lambda$ increases as the ratio of the number of new clusters to that of old clusters increases.
%


%----------------------------------------
\subsection{Training Strategy}
Overall, Selective Fusion can make separated conditions and scattered sequences taken from front/back views fuse tighter.
%
Here we show our training strategy and represent the feature distribution of each training phase in Figure~\ref{fig:stage}.
%
Our training strategy encompasses three stages.
%
At first, the features extracted by the pre-trained model have a tendency to cluster together, making it hard to distinguish between them.
%
We adopt our baseline to separate these sequences with a strict criterion, making each cluster gathered according to their similarity.
%
Second, we apply Selective Fusion to fuse different conditions of the same person and gradually merge sequences in front/back views with sequences in other views.
%
Finally, we get the clusters with all the cloth conditions and views.
%
% The total loss is summarized as:
% \begin{equation}
%     \mathcal{L} = \mathcal{L}_{CNCE} + \lambda_c \mathcal{L}_{C}
% \end{equation}
% $\lambda_c$ is used to control the weight of $\mathcal{L}_{C}$.

\begin{figure}[htbp]
	\centering	 
	\includegraphics[width=\linewidth]{figures/multi-stage.pdf}	 	
	\caption{The three stages in our training strategy. First, with narrowed features extracted by pre-trained model, we first adopt our baseline to separate them further. Then Selective Fusion is used to fuse matched clusters and samples together. Finally, we gain clusters with different clothes and views. Base is the \textit{Baseline}. Each type in different color indicates \textit{each subject in different cloth condition.} }
	\label{fig:stage}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
%
Our methods can be employed in appearance-based methods. 
%
For simplicity, we take silhouettes sequences as input since they are more robust when datasets are collected in the wild.
%
To demonstrate the effectiveness of our framework, we apply our methods to two existed backbones: GaitSet~\cite{chao2019gaitset} and GaitGL~\cite{lin2021gait} to help them train with unlabeled datasets. 
%
We also compare our method with upper bound which is trained with ground truth label, and with the baseline, which is also trained without supervision.
%
All methods are implemented with PyTorch~\cite{paszke2019pytorch} and trained on TITAN-XP GPUs.
%In this section, 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Datasets} 
Here we first pre-train backbones on a large gait recognition dataset OUMVLP~\cite{takemura2018multi}. 
%
Most of the previous unsupervised ReID methods use ResNet-50~\cite{he2016deep} pre-trained on ImageNet~\cite{deng2009imagenet} as the pre-train model, while in gait recognition, there does not exist any pre-train model with strong generalization.
%
Fortunately, OUMVLP contains a large number of subjects with 14 views, which is an ideal dataset to pre-train models.
%
We can train backbones on it to gain preliminary information to classify subjects, and with the large dataset volume, the model can be more generalized when adapted to other datasets. 
%
However, without cross-cloth pairs in OUMVLP, the model could only gain cross-view ability.
%
So, we need to develop methods to help the model recognize cross-cloth pairs.
%
We load the pre-trained model and evaluate the performance of our method on two popular datasets, CASIA-BN~\cite{yu2006framework} and Outdoor-Gait~\cite{song2019gaitnet}.

\subsubsection{OUMVLP}
OUMVLP~\cite{takemura2018multi} contains 10,307 subjects. 
%
The sequences of each subject distributed in 14 views between $[0^{\circ},90^{\circ}]$ and $[180^{\circ},270^{\circ}]$ but only have normal walking conditions (00, 01). 
%
We used 5153 subjects to pre-train backbones.
%
Especially, 00 is used as the gallery, and 01 is taken as the probe.
%
Without walking with different clothes sequences, the model trained on OUMVLP does not have much cross-cloth ability, so we need to develop specific methods to fuse cloth-changing pairs.



\subsubsection{CASIA-BN} 
%
The original CASIA-B is a useful dataset with both cross-view and cross-cloth sequence pairs. 
%
It consists of 124 subjects, having three walking conditions: \textbf{normal walking} (NM\#01-NM\#06), \textbf{carrying bags} (BG\#01-BG\#02), and \textbf{walking with different coats} (CL\#01-CL\#02).
%
Each walking condition contains 11 views distributed in $[0^{\circ},180^{\circ}]$. We employ the protocol in the previous research~\cite{chao2019gaitset,fan2020gaitpart}, taking 74 subjects as the training dataset and 50 subjects as the testing dataset. 
%
During the evaluation, NM\#01-NM\#04 are taken as the gallery set, NM\#05-NM\#06, BG\#01-BG\#02, CL\#01-CL\#02 used as the probe.
%
However, since the segmentation of CASIA-B is relatively rough, we collect some pedestrian images and trained a new segmentation model to re-segment CASIA-B, and gain CASIA-BN.

\subsubsection{Outdoor-Gait}
This dataset only has cross-cloth sequence pairs.
%
With 138 subjects, Outdoor-Gait contains three walking conditions: normal walking (NM\#01-NM\#04), carrying bags (BG\#01-BG\#04), and walking with different coats (CL\#01-CL\#04). 
%
There are three capture scenes (Scene\#01-Scene\#03), however, each person only has one view ($90^{\circ}$).
%
69 subjects are used for training and the last 69 subjects for tests.
%
During the test, we use NM\#01-NM\#04 in Scene\#03 as gallery and all the sequences in Scene\#01-Scene\#02 as probe in different conditions.

Noted that during training, we do not use the true label in the training set of both CASIA-BN and Outdoor-Gait.

%----------------------------------------
\subsection{Implementation Details} 

\subsubsection{Structure and Optimization Details}
Here we show the structure and optimization setting we used in pre-training in Table~\ref{oumvlp_param} and used in unsupervised learning in Table~\ref{unsupervise_param}.
%
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\setlength{\tabcolsep}{15pt}
\begin{table}[t]
\centering
\caption{The Parameters used to train a pre-train model on OUMVLP dataset.}
\begin{tabular}{ccc}
\hline
Param                       & Backbone & OUMVLP                 \\ \hline
\multirow{2}{*}{Batch Size} & GaitSet  & (32, 16)               \\
                            & GaitGL   & (32, 8)                \\
Start LR                    & Both         & 1e-1                   \\
\multirow{2}{*}{MileStones} & GaitSet  & {[}50k, 100k, 125k{]}  \\
                            & GaitGL   & {[}150k, 200k, 210k{]} \\ \hline
\end{tabular}
\label{oumvlp_param}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\setlength{\tabcolsep}{1pt}
\begin{table}[t]
\caption{The parameters used in unsupervised learning on CASIA-BN and Outdoor-Gait dataset.}
%\resizebox{\columnwidth}{!}{%
\begin{tabular}{cccc}

\hline
Param                                   & Backbone & CASIA-BN                 & Outdoor-Gait             \\ \hline
\multirow{2}{*}{Model Channel}         & GaitSet  & (32, 64, 128)(128, 256) & (32, 64, 256)(128, 256)  \\
                                        & GaitGL   & (32, 64, 128)(128, 128) & -                        \\
Batch Size                              & Both         & (8, 16)                  & (8, 8)                   \\
Weight Decay                            & Both         & 5e-4                     & 5e-4                     \\
Start LR                                & Both         & 1e-4                     & 1e-4                     \\
MileStones                              & Both         & {[}3.5k, 8.5k{]}         & {[}3.5k, 8.5k{]}         \\
Epoch                                   & Both         & Baseline: 50, SF: 50     & Baseline: 50, SF: 50     \\
Iteration                               & Both         & Baseline: 50, SF: 100    & Baseline: 50, SF: 100    \\
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Upper bound\\ Milestones\end{tabular}} & GaitSet  & {[}20k, 40k, 60k, 80k{]} & {[}10k, 20k, 30k, 35k{]} \\
                                        & GaitGL   & {[}70k, 80k{]}           & -                        \\ \hline
\end{tabular}%
%}
\label{unsupervise_param}
\end{table}

The pretrained backbone we use is same for both CASIA-BN and Outdoor-Gait.
%
For the GaitSet backbone, we adopt the GaitSet baseline in OpenGait~\cite{fan2022opengait}, and for the GaitGL backbone, we also use the GaitGL implementation in OpenGait.
%
When training the view classifier, we adopt the same GaitSet baseline in OpenGait.
%
The input and output channel dimension of each module in GaitSet and GaitGL backbone can be in the first row of Table~\ref{unsupervise_param}.


\subsubsection{Hyper-Parameters Setting} 
During training, we select 30 frames, and use all the frames for evaluation. 
%
For the hyper-parameters, we set $s_{up}=0.7$ for baseline, and $s_{up}=0.3$ for Selective Fusion to enlarge the boundary, others are set as $M=40$, $\tau=0.05$, $m=0.2$, $k=2$, $\lambda_{base}=0.005$, $\mathcal{C}_{low} = 0.8$, $\mathcal{S}_{i} = 0.7$. 
%
The experiments conducted for choosing these hyper-parameters can be seen in Section ~\ref{hyper}.
%
Since the sequences in Outdoor-Gait is fewer, we change $M$ to 20 to avoid overfitting.
%
Each frame has been normalized to $64\times44$. 

\subsubsection{Benchmark settings} To show the effectiveness, we define several benchmarks:

(1) \textit{Upper}. Upper bound reports the performance of each backbone trained with ground-truth labels.

(2) \textit{Pre-train}. The effect when directly applying the pre-trained model to the target dataset without fine-tuning.

(3) \textit{Base (CC)}. Fine-tuning the pre-trained model with our baseline framework implemented by Cluster-contrast.

(4) \textit{Ours (SF)}. The results with our proposed method.


%----------------------------------------


\subsection{Performance Comparison}
Before training and testing on CASIA-BN or Outdoor-Gait, we first pre-train backbones on OUMVLP and then load it when training on the unlabeled dataset, CASIA-BN and Outdoor-Gait.
%
The effect of the pre-trained model on the OUMVLP test dataset with GaitSet~\cite{chao2019gaitset} backbone is 77.9, and with GaitGL~\cite{lin2021gait} backbone is 79.1 on rank-1 accuracy of the NM condition. 
%
Training on OUMVLP can make the model gain cross-view knowledge but cannot achieve cross-cloth information.
%
So the comparison results can show that our method boosts the cross-view performance further and produces decent cross-cloth results.


%----------------------------------------
\subsubsection{CASIA-BN}
The performance comparison on CASIA-BN is shown in Table~\ref{tab:dataset_casiabn}. 
%
We evaluate the probe in three walking conditions separately. 
%
Since our method aims to improve the rank-1 accuracy of CL and sequences in front/back views, \textbf{ we take the accuracy for CL as the main criteria}.
%
From the results, we can see that our method outperforms the baseline in CL condition by a remarkable margin (GaitSet: CL + 9.7\%; GaitGL: CL + 17.2\%).
%
It indicates that our Selective Cluster Fusion method can properly identify the potential clusters of the same person with different cloth conditions and pull them together.
%
Moreover, sequences in $0^{\circ}/018^{\circ}/162^{\circ}/180^{\circ}$ also gained large improvement in both cloth conditions.
%
Selective Sample Fusion can gradually gather individual front/back samples that were excluded initially, by assigning them the same pseudo labels as the sequences in other views.
%
Although lacking walking postures, the sequences with front/back views can still provide useful information for identifying a particular person. 
%
It should be noted that the hyper-parameters used for GaitGL are as same as GaitSet and without specific adjustment, which shows the generalization of our method when applying to different backbones. 
%
Both cues indicate that our method is effective when dealing with cloth-changing and front/back views.

%
\input{CASIA-BN}
%----------------------------------------
\subsubsection{Outdoor-Gait}
Although Outdoor-Gait does not consider cross-view data pairs, we can still verify the SCF method on this dataset and show the result with the GaitSet backbone in Table~\ref{tab:outdoor_gait}.
%
% Please add the following requiBrickRed packages to your document preamble:
% \usepackage{multirow}
% Please add the following requiBrickRed packages to your document preamble:
% \usepackage{multirow}
\setlength{\tabcolsep}{12pt}
\begin{table}[ht]
\centering
\caption{The Rank-1 accuracy (\%) on Outdoor-Gait. When evaluation, we take NM\#1-NM\#4 in Scene\#3 as gallery and others as probe.}
\begin{tabular}{ccccc}
\hline
Backbone & Method & NM & BG & CL \\ \hline
\multirow{4}{*}{GaitSet} & Upper & \textcolor{BrickRed}{97.6} & \textcolor{BrickRed}{90.9} & \textcolor{BrickRed}{90.4} \\
 & Pretrain & 45.8 &  46.4 & 43.3 \\
 & Base (CC) & 84.8 & 66.5 & 62.9 \\
 & Ours (SCF) & \textbf{89.1} & \textbf{73.6} & \textbf{71.9} \\ \hline
\end{tabular}
\label{tab:outdoor_gait}
\end{table}

We can see that SCF surpasses the baseline on both conditions (NM + 4.3\%; BG + 7.1\%; CL + 9.0\%).
%
With SCF method, not only the accuracy of CL condition improved, but also the NM, BG's accuracy, which means features from CL sequences can also provide useful information when recognizing person, and they can not be neglected.
%
If the features before and after changing clothes are not correctly associated, the gait recognition model will miss important information, leading to insufficient learning and difficulty in distinguishing between different individuals.
%
However, due to the small dataset volume and lack of views in Outdoor-Gait, the upper bound with GaitGL backbone overfit\footnote{NM: 95.5, BG: 91.3, CL: 86.2 }, so we do not show the results with it.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\subsection{Ablation Study}
%----------------------------------------
\subsubsection{Effects of different parameters in baseline}
\label{hyper}
In our first training stage, we load the pre-train model with the baseline framework.
%
Here we research how hyper-parameters $s_{up}$, $M$, $\tau$, $m$ affect the results of baselines. 
%
We adjust one parameter at the one time and keep other hyper-parameters unchanged. 
%
$s_{up}$ regulates the boundary of how far the features can be gathered into one cluster. The smaller $s_{up}$ it is, the tighter the boundary.
%
$M$ is the number of neighbors KNN searched for each sequence.
%
$\tau$ is the temperature parameter in ClusterNCE loss, indicating the entropy of the distribution.
%
$m$, the momentum value, controls the update speed of centroids stored in the Memory Bank.
%
From results, we can see that when $s_{up}=0.7$, $M=40$, $\tau=0.05$, $m=2$ we have the overall best results for NM, BG and CL.
%
When these parameters deviate too much from the current setting, the performance represents sub-optimal.
%
Here we show the accuracy of NM, BG and CL when adopting different parameters in baseline framework in Figure~\ref{fig:hyperparameter}.

\begin{figure*}[ht]
	\centering	 
	\includegraphics[width=\linewidth]{figures/hyper-parameter.pdf}	 	
	\caption{The effect of hyper-parameters $s_{up}/M/\tau/m$ on baselines. In there we choose a set of hyper-paramters that have the best result in our experiments. Other hyper-parameters don't change the result a lot, just lead to sub-optimal.}
	\label{fig:hyperparameter}
\end{figure*}

%----------------------------------------
\subsubsection{Impact of each component in our Selective Fusion}
This section, we demonstrate that both SCF and SSF are indispensable in our Selective Fusion method.
%
In Table~\ref{tab:each} we show the results only using SCF or SSF.
%
Only with SSF, the rank-1 accuracy for each condition in front/back view slightly improved, but still had a poor performance on the cross-cloth problem.
%
When directly applying SCF, clusters mainly composed of sequences in front/back views will also pull closer to other clusters in the same condition, which should be forbidden.
%
Pulling more FVCs closer will further make these sequences harder to merge into their actual clusters with other views, degrading the performance in recognizing sequences with front/back views.
%
Therefore, the best effect can be achieved only when these two methods are effectively combined.
%
Next we will show the effect of parameters used in each method.
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}

\begin{table}[t]
\centering
\setlength{\tabcolsep}{12pt}
\caption{The effectiveness of each components in our framework. Combining both methods can maximum their effect.}
\begin{tabular}{cccc}
\hline
Settings & NM & BG & CL \\ \hline
Base & 87.2 & 74.3 & 31.1 \\
Base + SSF & 87.5 & 75.8 & 32.8 \\
Base + SCF & 83.3 & 75.9 & 39.9 \\
Ours & \textbf{90.3} & \textbf{82.9} & \textbf{40.8} \\ \hline
\end{tabular}%
\label{tab:each}
\end{table}




%----------------------------------------
\subsubsection{Impact of candidate number in support set}
Here we discuss the effect of a parameter in SCF, the candidate number $k$, in support set.
%
In Table~\ref{tab:SC-Fusion}, we can see that when $k=2$, we have the best performance, which is in line with the fact that the features of NM and BG are easily projected together in the feature space since they have larger similarity and we should drag the features of NM with CL specifically in CASIA-BN.


\begin{table}[t]
\centering
\caption{The effect of cluster candidates number $k$ in support set.}
\setlength{\tabcolsep}{12pt}
\begin{tabular}{cccc}
\hline
Settings     & NM & BG & CL \\ \hline
Ours ($k=4$) & 89.2 & 81.2 & 35.1 \\
Ours ($k=3$) & 89.7 & 81.1 & 37.9   \\
Ours ($k=2$) & \textbf{90.3} & \textbf{82.9} & \textbf{40.8}  \\ \hline
\end{tabular}%
\label{tab:SC-Fusion}
\end{table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Impact of rate of curriculum learning in SSF}

We test the effect of SSF with a dynamic or constant rate when conducting curriculum learning. 
%
Without curriculum learning, linearly clustering the front/back view sequences with sequences in other views will degrade the performance.
%
With a dynamic pulling rate, we can relax the requirement when training model, which can make the model learn from easy to hard better. 

\begin{table}[t]
\centering
\caption{The performance of different kinds of rates when conducting curriculum learning.}
\setlength{\tabcolsep}{12pt}
\begin{tabular}{cccc}
\toprule
Settings & NM & BG & CL \\ \midrule
Ours ($\lambda_{base}$) & 90.3  & 82.7 & 40.7 \\
Ours ($\lambda$) & \textbf{90.3} & \textbf{82.9} & \textbf{40.8} \\ \bottomrule
\end{tabular}%
\label{tab:SS-Fusion}

\end{table}


%----------------------------------------

%----------------------------------------
\subsubsection{Visulization of Selective Fusion}
The visualization effect of Selective Fusion shows in Figure~\ref{fig:tsne}.
%

%
We select a subject in CASIA-BN, finding that in baseline, BG and CL have a different pseudo label with NM. 
%
At the same time, some sequences in front/back views of NM/BG/CL are also assigned with different pseudo labels with sequences in other views.
%
With Selective Fusion, most sequences in various views and different conditions are assigned the same ID.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Discussion and future work}

Our method can be employed with off-the-shelf backbones to train a new unlabeled dataset without a label. 
%
However, there are still some limitations. 
%

First, we utilize prior knowledge about the views to identify the challenging samples with front/back views. This is because without such knowledge, it is difficult to specifically group these sequences with other views.
%
More automatic methods can be further developed to reduce the dependence on prior knowledge.
%

Second, our method uses data augmentation to simulate cross-cloth samples for gait recognition, which makes the model's recognition performance dependend on the effect of data augmentation to some extent. Therefore, more data augmentation methods can be developed to simulate the real-world cloth changing situations.
%
Also, the intrinsic principles of cross-cloth recognition need further study.
%

Third, the accuracy of unsupervised learning relies much on the pre-trained model's accuracy, so a higher precision model is necessary when conducting unsupervised learning on larger datasets, especially on real-world dataset.
%
\begin{figure}[ht]
	\centering
        \includegraphics[width=0.6\linewidth]{figures/tsne.pdf}	      	    
	\caption{The TSNE images of baseline/Ours. The text above each feature point shows the pseudo label. Blue color in text represents \textit{the sequence is in front/back view}. A type of color in feature point indicates a type of condition. SF is the Selective Fusion method we proposed. Please scroll in to see the details.}
	\label{fig:tsne}
\end{figure}

Since OUMVLP is a dataset that captured in the lab environment, it provides limited knowledge when realizing cross-view and cross-cloth on real world data.
%
So further efforts need to be spent on training more robust and high percision pre-train model to make the unsupervised learning method better adapted to real world. 

Our work conducts gait recognition with unsupervised learning, which alleviate the human labor requirement in the data collection process, making training gait recognition models with large dataset become possible and economical. This is crucial because currently previous datasets are collected with manual label, which can be a time-consuming and costly process, especially when large datasets are involved. By reducing the amount of human labor required, unsupervised gait recognition make training gait recognition models with large datasets more feasible and cost-effective. Training with larger dataset can ultimately lead to more accurate and robust gait recognition models, which can have a wide range of applications in fields such as security, healthcare, and sports analysis. 

In a nutshell, for future work, more intelligent methods can be developed to identify sequences taken from front/back views and and incorporate them with other views correctly. And more robust architecture can be developed to make the gait recognition method stable when adapting to real-world datasets.
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% grew 预训练模型效果86.025


\section{Conclusion}
In this work, we propose a new task, Unsupervised Gait Recognition. 
%
We first design a new baseline with cluster-level contrastive learning.
%
Then, we find two problems Unsupervised Gait Recognition faced: sequences in different clothes cannot be gathered into a single cluster, and sequences captured from front/back views are hard to be collected with sequences in other views.
%
To solve the two problems, we propose a Selective Fusion method with Selective Cluster Fusion and Selective Sample Fusion to gradually fuse clusters and sample pairs.
%
Selective Cluster Fusion helps clusters in different clothes of the same person gather as close as possible, and Selective Sample Fusion gradually merges sequences taken from front/back views with sequences in other views.
%
Experiments show the effectiveness and excellent performance in improving the accuracy of CL and front/back view conditions.



% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
% However, the Biometrics Council has been known to put floats at the bottom.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Biometrics Council conferences (but
% not Biometrics Council journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.







% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%




% % use section* for acknowledgment
% \ifCLASSOPTIONcompsoc
%   % The Biometrics Council usually uses the plural form
%   \section*{Acknowledgments}
% \else
%   % regular IEEE prefers the singular form
%   \section*{Acknowledgment}
% \fi


% The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,egbib.bib}




% \begin{thebibliography}{1}

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\section{Biography Section}

\begin{IEEEbiographynophoto}{Xuqian Ren}
received the B.E. degree from University of Science and Technology Beijing in 2019, received the M.S. degree from Beijing Institute of Technology in 2022. She is currently a Ph.D. candidate with Computer Science Unit, Faculty of Information Technology and Communication Sciences, Tampere Universities, Finland. Her current research interests include contrastive learning, image generation and 3d reconstruction.
\end{IEEEbiographynophoto}

\vspace{-5mm}
% if you will not have a photo at all:
\begin{IEEEbiographynophoto}{Saihui Hou}
received the B.E. and Ph.D. degrees from University of Science and Technology of China in 2014 and 2019, respectively. He is currently an Assistant Professor with School of Artificial Intelligence, Beijing Normal University, and works in cooperation with Watrix Technology Limited Co. Ltd. His research interests include computer vision and machine learning. He recently focuses on gait recognition which aims to identify different people according to the walking patterns.
\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage
\vspace{-5mm}
\begin{IEEEbiographynophoto}{Chunshui Cao}
received the B.E. and Ph.D. degrees from University of Science and Technology of China in 2013 and 2018, respectively. During his Ph.D. study, he joined Center for Research on Intelligent Perception and Computing, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences. From 2018 to 2020, he worked as a Postdoctoral Fellow with PBC School of Finance, Tsinghua University. He is currently a Research Scientist with Watrix Technology Limited Co. Ltd. His research interests include pattern recognition, computer vision and machine learning.
\end{IEEEbiographynophoto}
\vspace{-5mm}
\begin{IEEEbiographynophoto}{Xu Liu}
received the B.E. and Ph.D. degrees from University of Science and Technology of China in 2013 and 2018, respectively. He is currently a Research Scientist with Watrix Technology Limited Co. Ltd. His research interests include gait recognition, object detection and image segmentation.
\end{IEEEbiographynophoto}
\vspace{-5mm}
\begin{IEEEbiographynophoto}{Yongzhen Huang}
received the B.E. degree from Huazhong University of Science and Technology in 2006, and the Ph.D. degree from Institute of Automation, Chinese Academy of Sciences in 2011. He is currently an Associate Professor with School of Artificial Intelligence, Beijing Normal University, and works in cooperation with Watrix Technology Limited Co. Ltd. He has published one book and more than 80 papers at international journals and conferences such as TPAMI, IJCV, TIP, TSMCB, TMM, TCSVT, CVPR, ICCV, ECCV, NIPS, AAAI. His research interests include pattern recognition, computer vision and machine learning.
\end{IEEEbiographynophoto}



% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


