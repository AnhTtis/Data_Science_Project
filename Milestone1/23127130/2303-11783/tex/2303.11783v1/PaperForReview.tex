% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% bink added
\usepackage{multicol}
\usepackage{multirow}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Lightweight Contrastive Protein Structure-Sequence Transformation}

\author{\textbf{Jiangbin Zheng}\textsuperscript{1,2}, \textbf{Ge Wang}\textsuperscript{1,2}, \textbf{Bozhen Hu}\textsuperscript{1,2}, \textbf{Yufei Huang}\textsuperscript{1,2}, \\  \textbf{Siyuan Li}\textsuperscript{1,2}, \textbf{Cheng Tan}\textsuperscript{1,2}, \textbf{Xinwen Fan}\textsuperscript{3,4}, \textbf{Stan Z. Li}\textsuperscript{2,*} \\
 \textsuperscript{1}Zhejiang University\\
  \textsuperscript{2}AI Division, School of Engineering, Westlake University\\
  \textsuperscript{3}School of Life Sciences, Westlake University\\
  \textsuperscript{4}Institute of Basic Medical Sciences, Westlake Institute for Advanced Study\\
  \texttt{\{zhengjiangbin,wangge,hubozhen,huangyufei,lisiyuan,}\\
  \texttt{tancheng,fanxinwen,Stan.ZQ.Li\}@westlake.edu.cn}
  }
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Pretrained protein structure models without labels are crucial foundations for the majority of protein downstream applications. The conventional structure pretraining methods follow the mature natural language pretraining methods such as denoised reconstruction and masked language modeling but usually destroy the real representation of spatial structures. The other common pretraining methods might predict a fixed set of predetermined object categories, where a restricted supervised manner limits their generality and usability as additional labeled data is required to specify any other protein concepts. In this work, we introduce a novel unsupervised protein structure representation pretraining with a robust protein language model. In particular, we first propose to leverage an existing pretrained language model to guide structure model learning through an unsupervised contrastive alignment. In addition, a self-supervised structure constraint is proposed to further learn the intrinsic information about the structures. With only light training data, the pretrained structure model can obtain better generalization ability. To quantitatively evaluate the proposed structure models, we design a series of rational evaluation methods, including internal tasks (e.g., contact map prediction, distribution alignment quality) and external/downstream tasks (e.g., protein design). The extensive experimental results conducted on multiple tasks and specific datasets demonstrate the superiority of the proposed sequence-structure transformation framework.
\end{abstract}


\begin{figure*}[htp]
  \centering
   \includegraphics[width=1.0\linewidth]{pics/1.png}   
   \caption{The proposed sequence-structure transformation framework. We use a pretrained protein language model to guide the training of the protein structure model with contrast alignment loss. To facilitate internal evaluation and to strengthen the information constraints on the structure, we propose to predict contact maps based on \text{C$_\beta$} as a self-supervised loss, where virtual \text{C$_\beta$} atoms can be computed from three atoms of N, \text{C$_\alpha$}, and O.}
   \label{fig:1}
\end{figure*}

\section{Introduction}
Deep learning has witnessed the power of protein-related fields, especially for the protein structure prediction task (represented by AlphaFold2 \cite{jumper2021highly}) and protein design task (represented by ESM-IF \cite{hsu2022learning}, ProteinMPNN \cite{hsu2022learning}). The target of protein structure prediction is to generate the 3D structures from the corresponding protein sequences (i.e., amino acid sequences), while the protein design is an almost inverted task.

Generally, deep protein tasks involve protein sequence and protein structure. Thus protein language model and protein structure model become the most two fundamental components for downstream tasks. For the protein language modeling, there are a variety of available models \cite{bepler2019learning,rao2020transformer,rives2021biological,rao2021msa,lin2022language} due to mature pretraining techniques and more simple textual representation \cite{tong2020document,zheng2021enhancing,wu2020fuzzy}. Among them, denoised reconstruction and masked language modeling (MLM) based protein language models \cite{devlin2018bert} have become the most commonly-used pretraining modes. However, there are few universal pretrained protein structure models. Since the complex and continuous spatial structures of proteins, applying similar operations of protein language model pretraining such as adding noise and masking coordinate sequences for structure model pretraining might damage the original real structural space and even result in radically different semantic representation. Therefore, the existing most effective structure models are almost task-specific in a supervised manner and not suitable for large-scale unsupervised pretraining.

Rethinking the current advanced protein structure prediction models (i.e., sequence-to-structure models) which reach even atom-level high-accuracy transformation performances approaching the experimental level, this observation clearly indicates that protein sequence representations naturally contain abundant structural information. Therefore, \textit{\textbf{is it possible to enhance the training of protein structure models guided by reliable and robust pretrained protein language models?}} Fortunately, emerging pretraining methods in the typical text-image cross-modal domain represented by CLIP \cite{radford2021learning,ramesh2022hierarchical} bring us direct inspiration, where large-scale textual pretrained language models are used to pretrain visual models for intermediate feature alignment. The success of CLIP thus also implies that we can treat protein structure and sequence as two different modalities and then adopt a similar cross-modal alignment pretraining strategy, which makes the existing large-scale protein language modalities potential to be used for training protein structure modalities.

In this work, we propose a novel contrastive protein sequence-structure transformation for protein structure model pretraining. Specifically, we utilize an unsupervised contrastive alignment between the protein sequence representation and protein structure representation. To pretrain the model, random structure-sequence parallel pairs are selected for building mini-batches, which not only establish the cross-modal alignment relationships but also obtain negative samples from other pairs naturally for contrastive learning. Furthermore, to strengthen the constraint of structural representation, we propose a self-supervised contact map constraint based on intermediate features from the structure encoder.

To evaluate the proposed sequence-structure transformation framework, quantitative and qualitative experiments should be conducted. However, as a brand-new protein pretraining paradigm, there are no complete reference evaluation strategies. Hence, we additionally introduce a series of evaluation experiments to comprehensively prove the robustness of our trained structure model. In particular, internal tasks (e.g., contact map prediction evaluation, and distribution alignment quality evaluation) are proposed to show the internal contrastive alignment ability and distillation performances, while external/downstream tasks (e.g., protein design task) are proposed to show the generalization ability of pretrained protein structure model. In the proposed evaluation system, the sequence-structure transformation framework is experimentally validated, which fully shows the robustness of the pretraining performance. For example, the downstream protein design task achieves excellent performance on lower perplexity and higher sequence recovery.

The contributions are summarized as follows:
\begin{itemize}
\item The proposed unsupervised protein sequence-structure transformation framework establishes a novel deep alignment relationship between protein sequences and protein structures.

\item Based on the proposed contrastive transformation framework, we first pretrain the protein structure model under the guidance of abundant prior language knowledge from the pretrained protein sequence model.

\item To evaluate the pretrained structure model, we propose a complete evaluation system, where evaluation metrics and evaluation tasks including internal and downstream tasks are introduced, providing baselines for the protein structure research community.

\item The proposed protein structure model pretraining achieves competitive performances among various evaluation tasks, such as a superior performance in the protein design task. 
\end{itemize}


\section{Related Work}
\textbf{Protein Language Models}.
Inspired by recent breakthroughs in natural language processing and similarities between human languages and protein sequences, protein language modeling has emerged as an active and promising direction for unsupervised learning of protein primary structures~\cite{EhsaneddinAsgari2015ContinuousDR, EthanCAlley2019UnifiedRP, ZacharyWu2021ProteinSD, BozhenHu2022ProteinLM,huang2023data}. UniRep~\cite{EthanCAlley2019UnifiedRP}, UDSMProt~\cite{Strodthoff:2019universal} and SeqVec~\cite{MichaelHeinzinger2019ModelingTL} use LSTM or its variants to learn sequence representations and long-range dependencies. TAPE~\cite{RoshanRao2019EvaluatingPT} benchmarks a group of protein models through various tasks, concluding the effectiveness of the self-supervised pretraining methods. Much work has improved the model scale and architecture to learn more protein semantics. Elnaggar et al.~\cite{AhmedElnaggar2021ProtTransTC} have trained several successful transformer variants on billions of amino acid sequences. Similarly, ESM-1b~\cite{AlexanderRives2019BiologicalSA} employs a Transformer architecture and a masked language modelling strategy to train robust representations based on a large-scale dataset. And ESM-2~\cite{ZemingLin2022LanguageMO} then extends ESM-1b with larger-scale parameters (15 billion), which achieves more superior results compared with other smaller ESM models.

% Other than the MLM training objective, alternatives have been also explored, such as pairwise MLM~\cite{LiangHe2021PretrainingCP}, span MLM~\cite{RuidongWu2022HighresolutionDN}, conditional generation~\cite{AliMadani2020ProGenLM} and contrastive learning~\cite{AmyXLu2020SelfSupervisedCL}. Besides, Sturmfels et al.~\cite{PascalSturmfels2020ProfilePA} and Sercu et al.~\cite{TomSercu2021NeuralPM} use sets of sequences for supervision. Furthermore, multiple sequence alignment (MSA)-based methods leverage the sequences within a protein family to seize the conserved regions of homologous sequences~\cite{SurojitBiswas2020LowNPE, JoshuaMeier2021LanguageME}, e.g., the most famous MSA Transformer~\cite{RoshanRao2021MSAT} tackles sets of sequences as input by alternating attention over rows and columns with far greater parameter efficiency that prior protein language models without performance degradation.

\textbf{Protein Structure Models}. 
As the function of a protein is usually determined by its structure, structure-based protein models are also the crucial solution for learning a representative protein embedding. Thanks to the advances in highly accurate deep learning-based protein structure prediction methods~\cite{MinkyungBaek2021AccuratePO, JohnMJumper2021HighlyAP}, AlphaFoldDB~\cite{KathrynTunyasuvunakool2021HighlyAP} has provided over 200 million protein structure predictions to accelerate scientific research. Based on this development, more and more protein structure models have appeared, which commonly seek to encode the spatial information of protein structures by convolutional neural networks (CNNs) ~\cite{GeorgyDerevyanko2018DeepCN} or graph neural networks (GNNs)~\cite{VladimirGligorijevi2020StructureBasedPF, FedericoBaldassarre2020GraphQAPM, BowenJing2020LearningFP}. Among these methods, SPROF~\cite{ShengChen2020ToIP} employs distance maps to predict protein sequence profiles. And IEConv~\cite{PedroHermosilla2020IntrinsicExtrinsicCA} introduces a convolution operator to capture all relevant structural levels of a protein. GearNet~\cite{ZuobaiZhang2022ProteinRL} encodes the spatial information by adding different types of sequential or structural edges and then performed relational message passing on protein residue graphs. GVP-GNN~\cite{BowenJing2020LearningFP} designed the geometric vector perceptrons (GVP) to learn both scalar and vector features in an equivariant and invariant manner, while Guo et al.~\cite{YuzhiGuo2022SelfsupervisedPF} adopted SE(3)-invariant features as the model inputs and reconstructed gradients over 3D coordinates to avoid the usage of complicated SE(3)-equivariant models.

% Besides, Mansoor et al.~\cite{SanaaMansoor2021TowardMG} encoded both sequence and structure information through joint training in a semi-supervised manner. While ProSE~\cite{TristanBepler2021LearningTP} carried out multi-task with MLM and structural supervision to enrich protein embeddings. STEPS~\cite{CanChen2022StructureawarePS} correlated the representations learned from language models and structure graph models by pseudo-bi-level optimization. Moreover, the contrastive representation learning method~\cite{hermosilla2022contrastive} has also been applied for 3D protein structures. 

% \textbf{Protein-related Tasks}
% protein prediction and protein design


\begin{figure*}[htp]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=1.0\linewidth]{pics/4.png}
   \caption{Schematic diagram of the GVP module. Protein backbone atoms (C, C$_\alpha$ and N) are used to generate graphs with node and edge features based on k-nearest neighbor relationships. The graphs are fed into the vector channel and scalar channel of the GVP module to generate vector and scalar features, respectively. These two primary features are then combined with additional spatial features such as rotation frame, sidechain, orientation, and dihedral features to form the spatial structure features.
}
   \label{fig:2}
\end{figure*}

\section{Methods}

In this section, we describe the proposed sequence-structure transformation framework in detail. As shown in Figure~\ref{fig:1}, to train the protein structure model, a pretrained protein language model is used as the teacher model to guide the pretraining of the protein structure model. The contrastive alignment constraint learns prior pretrained language information, and the internal contact map reconstruction task assists structure learning in an unsupervised manner.

\subsection{Pretrained Sequence Module}
Pretrained protein sequence/language models have been widely used in deep protein downstream tasks, which are typically trained on large-scale datasets. Since sequence models implicitly include abundant structural information learned from existing sequence data, a robust protein language model can serve as a teacher to guide the structure model learning. We adopt the ESM-2 \cite{lin2022language} as the primary teacher among the existing language models. Note that in this work ESM-2 base is utilized since this version has fewer parameters without a significant loss of precision.

The ESM-2 language model is a high-performance protein language model. Compared to previous generation models such as ESM-1b \cite{rao2020transformer}, ESM-2 has improved model architecture, training techniques, and increased computational resources and data. The addition of relative positional embeddings in ESM-2 makes generalization to arbitrary length sequences possible. These modifications resulted in a significantly better model.


\subsection{Equivariant Structure Module}
For downstream protein structure tasks such as inverse folding, the predicted sequence should be independent of the reference frame of the structural coordinates. In other words, for any rotation and translation $\mathcal{F}$ of the input coordinates $X$, the output distribution $Y$ of the model is expected to be invariant under these transformations, i.e., $p(Y |X) = p(Y |\mathcal{F}(X))$. To satisfy this condition, equivariant network models are widely used in protein 3D structure modeling, where geometric vector perceptron (GVP)-based models such as GVP-GNN\cite{jing2020learning} and GVP-Transformer\cite{hsu2022learning} have achieved impressive performance in protein design tasks. 
This suggests that the GVP module plays an important role in representing structural features to maintain equivariant properties. Furthermore, the GVP components are simple and lightweight, thus the GVP architecture is adapted as the structure module, as shown in Figure~\ref{fig:2}.

We first use the GVP-GNN encoder layers to extract spatial geometric features, and then a generic Transformer based self-attention blocks\cite{vaswani2017attention} to extract temporal features. In GVP-GNN, the input features are translation-invariant and each layer is rotation-equivariant. We perform a change of basis on the vector features from GVP-GNN into local reference frames defined for each amino acid to derive rotation-invariant features\cite{hsu2022learning}. Besides, different from the generic Transformer, the learned positional embeddings are used instead of sinusoidal positional embeddings.


% 训练技巧：先用esm-inversefold的预训练参数初始化，但是在描述的时候，直接写“使用esm-inverse fold的encoder部分，不强调参数的初始化”
\subsection{Cross-modal Pretraining Objectives}
\subsubsection{Contrastive Alignment Loss}
Aligning both structural and textual modalities can improve multi-modality framework \cite{min2021visual}. To enforce cross-modality alignment constraints, we propose a contrastive alignment loss for protein language and structure models.

Previous cross-modality alignment mostly focuses on positive samples only \cite{zheng2023cvt}. Inspired by contrastive learning \cite{zheng2022using}, we construct both positive and negative samples in the same mini-batch and implement a contrastive cross-modal alignment method to ensure that similar features are closer while different features are farther apart. Given that the normalized structural features from GVP as $\mathcal{S}_{\text{logits}} \in \mathbb{R}^{B \times T \times d}$, and the normalized temporal features from self-attention layers as $\mathcal{V}_{\text{logits}} \in \mathbb{R}^{B \times T \times d}$,  where $B$ is the number of samples, $T$ is the maximum length of sequences and $d$ is the dimension of features. Then the pair matrices are computed firstly as:
\begin{equation}
\mathcal{S}2\mathcal{V}_{\text{pair}} =\mathcal{S}_{\text{logits}} \times \mathcal{V}_{\text{logits}}^T,
\end{equation}
% or
\begin{equation}
\mathcal{V}2\mathcal{S}_{\text{pair}} =\mathcal{V}_{\text{logits}} \times \mathcal{S}_{\text{logits}}^T ,
\end{equation}

Taking $\mathcal{S}2\mathcal{V}_{\text{pair}} \in \mathbb{R}^{B \times B}$ pair matrix (denoted as $\mathcal{P}$) as an example, $\mathcal{P}[i,j] (0 \leq i,j < B)$ represents the similarity value between the structural feature of the $i$-th batch and the textual feature of $j$-th batch. The structural and textual features from the same input instances are positive samples, and the features from other different input instances are negative samples. Hence, in the matrix $\mathcal{P}$, the diagonal similarity values are from positive sample pairs, and the rest values are from negative sample pairs. To make the loss computation differentiable and easy, we convert the computation of positive and negative pairs into a binary classification task\cite{radford2021learning}. The value $i$ corresponding to the $i$-th row in the matrix $\mathcal{P}$ is a positive label. Therefore, the corresponding labels of $\mathcal{P}$ are $\text{Labels}=\{0, 1, \cdots, i, \cdots, B\}$, then the contrastive alignment loss related to $\mathcal{S}2\mathcal{V}_{\text{pair}}$ as:
\begin{equation}
\mathcal{L}_{\text{align}_\mathcal{S}} = \text{CrossEntropy}(\text{Softmax}(\mathcal{S}2\mathcal{V}{\text{pair}}), \text{Labels}).
\end{equation}
Similarly, the temporal-to-spatial alignment loss related to $\mathcal{V}2\mathcal{S}_{\text{pair}}$ as:
\begin{equation}
\mathcal{L}_{\text{align}_\mathcal{V}} = \text{CrossEntropy}(\text{Softmax}(\mathcal{V}2\mathcal{S}_{\text{pair}}), \text{Labels}).
\end{equation}
Therefore, the complete contrastive alignment loss as:
\begin{equation}
\mathcal{L}_{\text{align}} = \frac{1}{2}(\mathcal{L}_{\text{align}_\mathcal{S}} + \mathcal{L}_{\text{align}_\mathcal{V}}) .
\end{equation}


% To enhance the alignment ability, we proposed a extra KL-divergence loss to align multi-modality features directly.
% \begin{equation}
% \mathcal{L}_{\text{KL}} = \text{KL}(\mathcal{S}_{\text{logits}}, \mathcal{S}_{\text{logits}})
% \end{equation}


\subsubsection{Contrastive Alignment Level}
\begin{figure}[h]
  \centering
   \includegraphics[width=1.0\linewidth]{pics/3.png}
   \caption{Schematic diagram of different alignment levels. (a) Residue-level alignment is the comparison of each pair of structure-sequence features residue by residue. (b) Protein-level alignment is the comparison of each pair of structure-sequence features protein by protein, where the features of each protein are combined from all the residue features contained in that protein. The same colors indicate a sequence-structure pair from the same protein.}
   \label{:align_level}
\end{figure}

As shown in Figure \ref{:align_level}, we propose to compute the alignment loss at the residue level and protein level, respectively. For residue-level alignment, we compare the encoded sequence and structure features residue by residue. While for protein-level alignment, we compare the encoded features at a coarser fine-grained level. Intuitively, alignment at different fine-grained levels leads to different capabilities. Finer-grained comparisons such as residue-level alignment require more complex computations, but may have better performance. Coarse-grained comparison alignments such as protein-level alignment may perform worse, but are an option worth considering due to their lighter computation. See our experimental section for a detailed analysis. It is worth mentioning that no matter which level is used for calculation, the samples in the mini-batch will be randomly disrupted, and each pairing is different, which is also an implicit data augmentation.


\subsection{Structural Reconstruction Loss}
As mentioned above, the GVP layers encode N, C\text{$_\alpha$} and O atoms as the core structural feature representation. To strengthen the structural constraints, we propose a self-supervised structure reconstruction task. For constructing a self-supervised structural reconstruction method, predicting the coordinates of virtual C\text{$_\beta$} atom given the structural features of N, C\text{$_\alpha$} and O atoms is the most direct way, but in practice, it is difficult to predict the coordinates accurately. Therefore, we simplify the difficulty by converting the coordinate prediction to a contact map prediction, as shown in Figure \ref{:contact}. Specifically, we take the intermediate features (i.e., attention maps) provided by the self-attention blocks shown in Figure \ref{fig:2} to predict the \text{C$_\beta$} related contact maps \cite{rao2020transformer}. In addition to the constraint of structural information during training, the introduction of a contact map generation can be also used as a metric for the internal performance evaluation of the pretrained model.


\textbf{Virtual \text{C$_{\beta}$} Atom}.
The \text{C$_\beta$} is a virtual coordinate that might not appear real in every residue. But by calculating the spatial relationship between N, \text{C$_\alpha$}, and O in the protein backbone, we can obtain the position of the virtual \text{C$_\beta$} atom as a reference coordinate, calculated as:
\begin{equation}
    \begin{split}
        D_0 & = \text{P}_{C_{\alpha}} - \text{P}_{N}, \\
        D_1 & = \text{P}_{C} - \text{P}_{C_{\alpha}}, \\
        D_2 & = D_0 \otimes D_1 ,\\
        \text{P}_{C_{\beta}} & =  C_0 \times D_0 + C_1 \times D_1 +  C_2\times D_2 + \text{P}_{C_{\alpha}},
    \end{split}
\end{equation}
%           b = X[:,:,1,:] - X[:,:,0,:]
%         c = X[:,:,2,:] - X[:,:,1,:]
%         a = torch.cross(b, c, dim=-1)
%         Cb = -0.58273431*a + 0.56802827*b - 0.54067466*c + X[:,:,1,:]
%         Ca = X[:,:,1,:]
%         N = X[:,:,0,:]
%         C = X[:,:,2,:]
%         O = X[:,:,3,:
where $\otimes$ denotes the cross product of vectors and \text{P$_{(*)}$} denotes the coordinates of the corresponding atom. And $C_0=0.56802827$, $C_1=-0.54067466$ and $C_2=-0.58273431$ are constant coefficients.


\begin{figure}[h]
  \centering
   \includegraphics[width=0.95\linewidth]{pics/9.png}
   \caption{Pipeline of contact map reconstruction based on C$_\beta$ atoms with length $L$. Attention maps are extracted from each layer of the self-attention blocks, and then an $L \times L$ coupling matrix is generated by a symmetrization operation and average product correction (APC) along the amino acid dimensions. Then a regression layer is applied to this coupling matrix to determine the final contact map predictions.
}
   \label{:contact}
\end{figure}


\begin{table*}[htp]
\centering
\scalebox{0.75}{
\begin{tabular}{lcccccccccccccccccc} 
    \toprule
    
    \textbf{Group} & \textbf{Level} & \multicolumn{3}{c}{\textbf{CATH Test Set}} & \multicolumn{3}{c}{\textbf{trRosetta Set}} & \multicolumn{3}{c}{\textbf{TS500 Set}} & \multicolumn{3}{c}{\textbf{Ts50 Set}} & \multicolumn{3}{c}{\textbf{CASP14 Set}} \\ 

    \midrule
    & & \textbf{P@L} & \textbf{P@L2}& \textbf{P@L5}& \textbf{P@L} & \textbf{P@L2}& \textbf{P@L5}& \textbf{P@L} & \textbf{P@L2}& \textbf{P@L5}& \textbf{P@L} & \textbf{P@L2}& \textbf{P@L5} & \textbf{P@L} & \textbf{P@L2}& \textbf{P@L5}\\
        
    \cmidrule(lr){3-17}
        
    \multirow{2}{*}{1}        
    & residue & 78.05& 90.97 & 96.12 & 87.69	 & 94.94	 & 96.89	 & 90.31	 & 96.67	 & 98.10	 & 91.36	 & 98.66	 & 100.00 & 74.9 & 91.49 & 95.34 \\
     & protein & 72.21	& 88.03	& 98.31	& 81.30	& 92.42	& 96.18	& 83.78	& 94.14	& 97.44	& 85.18	& 96.32	& 99.53 & 68.24 & 87.23 & 93.96 \\

    \midrule

     & & \textbf{acc$_1$} & \textbf{acc$_2$}& \textbf{KL}& \textbf{acc$_1$} & \textbf{acc$_2$}& \textbf{KL}& \textbf{acc$_1$} & \textbf{acc$_2$}& \textbf{KL}& \textbf{acc$_1$} & \textbf{acc$_2$}& \textbf{KL}& \textbf{acc$_1$} & \textbf{acc$_2$}& \textbf{KL}\\

     \cmidrule(lr){3-17}
     
    \multirow{2}{*}{2} 
    & residue & 87.71 & 87.65 & 0.004 & 96.13 & 96.23 & 0.0001 & 94.69 & 95.02 & 0.0001 & 98.29	 & 98.41 & 0.0001 & 30.87 & 30.45 & 0.0001  \\
     & protein & 100.00 & 100.00 & 0.00 & 100.00 & 100.00 & 0.00 & 100.00 & 100.00 & 0.00 & 100.00 & 100.00 & 0.00 & 100.00 & 100.00 & 0.00 \\
	 \bottomrule
\end{tabular}
}
\caption{\label{tab:1} Internal evaluation on different test sets. Group 1 shows the performance of contact map predictions on P@L, P@L2 and P@L5 scores with the residue-level and protein-level pretrained models, respectively. Group 2 shows the performance of retrieval alignment evaluations on alignment accuracy and KL distance with residue-level and protein-level pretrained models, respectively. The acc$_1$ and acc$_2$ respectively represent the accuracy of structure-to-sequence and sequence-to-structure alignment scores in the contrastive algorithm.}
\end{table*}


\textbf{Contact Map Predictor}. As shown in Figure \ref{:contact}, we show the pipeline diagram of the contact map predictor. The self-attention maps extracted from the self-attention blocks are symmetrized and average product corrected to obtain the final contact maps. The target of the self-supervised structural loss aims to constrain the structural feature representation, and the loss is calculated based on the cross-entropy as:

%reference ESM-1b & ESM-2 : conatact map predictor architecture and process of N,C,CA=>Cb 

% 自监督训练的损失
% 参考的contact map为0-1矩阵，而预测得到的contact map为0-1之间的概率矩阵，一般的，我们认为概率大于0.5的为contact，即赋值为1。为此，我们将contact map的损失用cross entropy loss作为约束：
\begin{equation}
\mathcal{L}_{contact} = \text{CrossEntropy}(M_{ref}, M_{pred}),
\end{equation}
where $M_{ref}$ denotes the ground-truth contact maps and $M_{pred}$ denotes the predicted contact maps. 

Overall, the objective of pretraining is jointly optimized contrastive alignment loss $\mathcal{L}_{\text{align}}$, and contact map loss $\mathcal{L}_{\text{contact}}$ as:
\begin{equation}
   \mathcal{L}_{\text{pretrain}} = \lambda_1 \times \mathcal{L}_{\text{align}} + \lambda_2 \times \mathcal{L}_{\text{contact}},
\end{equation}
where $\lambda_1$ and $\lambda_2$ are the weighted values.



\section{Experiments}


% 实验大纲：
% 1.内部任务： 
% ①contact Map prediction， trrosetta， R@L， 可以可视化， 比较对象：有无预训练的strucEnc比较， =》证明学到了预训练的strucEnc
% ②Seq和Struc输出的对齐比较或者说是检索对齐， cath test或ts50等， KL/acc 等， 可以可视化， 比较对象：有无预训练的比较 =》证明学到了预训练的strucEnc
% 2.外部任务：
% ① Protein design， cath数据集 ，比较对象: 自身有无预训练比较+和ESM-inverse、ProteinMPNN比较 =》证明学到了预训练的strucEnc

% ③有分类任务？
% 可视化：
% 1.SeqStruc-similarity matrix： 参考ICCV2022 SLR =》证明对齐的效果
% 2.Contact Map 可视化，包括attention Map，contact map的直接可可视化 =》 证明学到了有效的注意力
% 3.聚类分析，（如果有一个基于蛋白质的分类任务的话？） =》证明struc的重要性

\subsection{Settings}
\subsubsection{Evaluation Tasks}

Evaluating pretrained protein structure model is intractable, especially for a brand new training framework. Therefore, we build a new evaluation system with multiple internal and external/downstream tasks to demonstrate the learned representation ability, alignment ability, downstream generalization ability, etc. 

% Contact Map Predictor => P@L， Self-similarity evaluation， retrieval alignments evaluation=>ACC/R@L； 评估两者的分布，用KL散度等 => 对齐评估； Protein MAQ TASK
\textbf{Internal Evaluation}. For internal tasks, we introduce a contact map prediction task and a self-similarity evaluation task. 1) The contact map prediction task uses the top-L long-range precision (P@L) metric to evaluate the quality of the generated contact maps. 2) The self-similarity alignment tasks uses the accuracy and KL divergence metrics to reflect the alignment ability between the language model and the structure model.


% Protein Design
\textbf{External Evaluation}. External tasks refer to downstream tasks specifically here. We introduce the protein sequence design task (also known as protein inverse folding), which predicts the corresponding protein sequences given the protein backbone atomic coordinates. The mostly used metrics for protein design is perplexity and accuracy of recovery.

% We introduce a protein design task and a protein classification task for external tasks. 1) Protein design aims to predict protein sequences based on the corresponding protein backbones (metric: P@L); 2) protein classification task (TODO: specify a task)


\subsubsection{Datasets}
% 数据集介绍：% CATH4.2：% Ts50：% TrRosetta：

% \begin{table*}[htp]
% \centering
% \scalebox{1}{
% \begin{tabular}{lcccc} 
%     \toprule

%     \textbf{DataName} & \textbf{\#Struc-Seq Pairs} & \textbf{Avg Length} & \textbf{Internal} & \textbf{External} \\ 
%     \midrule

%     CATH Testing & \\
%     trRosetta & \\
%     ts500 & \\
%     ts50 & \\
%     CASP14 & \\
    
 
% 	 \bottomrule
% \end{tabular}
% }
% \caption{\label{tab:contact_score} Internal Contact Map Prediction task.}
% \end{table*}


\textbf{Pretraining Dataset}. We leverage a larger-scale protein dataset PDB70 with corresponding protein sequence-structure pairs as the pretraining dataset. To prevent label leakage, we remove all existing data that also appear in evaluation datasets.

\textbf{Evaluation Datasets}. CATH 4.2 dataset \cite{ingraham2019generative} is widely used in protein-related work, and its training, validation, and test splits consist of 18204, 608, and 1120 protein data, respectively. In downstream protein design tasks, the training set of CATH is used to fine-tune the pretrained structure model, and the test set of CATH is used for evaluation. We also report results on TS50 \& TS500 \cite{li2014direct}, where Ts50 contains only 50 instances and Ts500 contains 500 instances. Besides, the trRosetta set is often used for contact map prediction tasks. It includes about 15k instances, each of which includes structure, sequence, MSAs and etc. The CASP14 set is known as AlphaFold2 \cite{jumper2021highly}. Although CASP14 is small in number, it is closer to the practice environment of blind tests and competitions. The full CASP14 targets can be found in Appendix Section \ref{sec:casp14_targets}. In short, the trRosetta set, CATH testing set, Ts50/Ts500, and CASP14 set are all used for internal evaluation. 


% \textbf{Classfication dataset}.
% TODO
% 训练StrucEmb： Training set: CATH4.2; 验证集、测试集： CATH 4.2的test set 
% 外部任务：
% 训练Protein Design： 训练集CATH和和训练strucEmb一致；


\begin{figure*}[htp]
  \centering
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   \includegraphics[width=0.9\linewidth]{pics/2.png}
   \caption{Illustration of the protein design model. We use a GVP-based structure model as an encoder, which captures protein structure features in a latent space. The latent features are then fed into the MLP layers to decode and generate protein sequences. The pretrained structure module is finetuned in the protein design task.}
   \label{:design}
\end{figure*}

%%%% Table1
\begin{table*}[htp]
\centering
\scalebox{0.98}{
\begin{tabular}{clcccccc} 
    \toprule

    \multirow{2}{*}{\textbf{Group}} & \multirow{2}{*}{\textbf{Models}} & \multicolumn{3}{c}{\textbf{Perplexity}} & \multicolumn{3}{c}{\textbf{Recovery (\%)}} \\ 

    \cmidrule(lr){3-5}\cmidrule(lr){6-8}
    
    &  & \textbf{CATH} & \textbf{Ts50} & \textbf{Ts500} & \textbf{CATH} & \textbf{Ts50} & \textbf{Ts500} \\ 
    \midrule
 
    \multirow{12}{*}{1} 
    & *Natural frequencies \cite{hsu2022learning}  & 17.97 & - & -  & 9.5 & - & -\\
    & SPIN \cite{li2014direct}  & - & - & -  & - & 30.3 & 30.3 \\
    & SPIN2 \cite{o2018spin2}  & - & - & -  &- & 33.6 &36.6 \\
    & Structured Transformer \cite{ingraham2019generative}  & 6.85 & 5.60 & 5.16  & 36.4 & 42.40 &44.66 \\
    & Structured GNN \cite{jing2020learning}   & 6.55 & 5.40 & 4.98  & 37.3 & 43.89 & 45.69\\
    & *GVP-Transformer \cite{hsu2022learning}   & 6.44 & - & - & 38.3 & - &-\\
    & AlphaDesign \cite{gao2022alphadesign}  & 6.30 & 5.25 & 4.93  & 41.31 & 48.36 & 49.23\\
    & *GVP-GNN-large \cite{jing2020learning}  & 6.17 & - & - & 39.2 & - &-\\
    & GCV \cite{tan2022generative} & 6.05 & 5.09 & 4.72  & 37.64 & 47.02 & 47.74\\
    & GVP-GNN \cite{jing2020learning}   & 5.29 & 4.71 & 4.20  & 40.2 & 44.14 & 49.14 \\
    & ProteinMPNN \cite{dauparas2022robust}   & 4.61 & 3.93 & 3.53  & 45.96 & 54.43 & 58.08\\
    
    % & PiFold \cite{zhangyang} & 6.04 & 6.31  & 4.55 & 3.86 & 3.44 & 39.84 & 38.53 & 51.66 & 58.72 & 60.42\\
    \midrule
    
    % \multirow{2}{*}{2}
    % & *GVP-GNN-large \cite{jing2020learning} &  6.11	 & 4.09  & 4.08 & - & -& 38.3 & 50.8 & 50.8 & - &-\\
    % & GVP-Transformer \cite{hsu2022learning} &  6.30	 & 6.29  & \underline{3.99} & \underline{3.43} & \underline{3.34} & 34.74 & 34.25 & \underline{52.51} & \underline{56.66} & \underline{60.85}\\
    % \midrule
    
    % \multirow{2}{*}{3}
    % & $\text{Ours-VACSLR}$   & \textbf{3.71} & \textbf{3.32} & \textbf{3.04}   & \textbf{53.80} & \textbf{56.76} & \textbf{62.07}\\
    % & $\text{Ours-CLIP}$  & \textbf{3.74} & \textbf{3.24} & \textbf{2.98}  & \textbf{53.99} & \textbf{56.70} & \textbf{62.82}\\
    % \midrule
    
    \multirow{3}{*}{2}
    & $\text{Design}_{r}$ (\textit{w/} \text{Pretraining})  & \textbf{4.48} & \textbf{3.76}  & \textbf{3.28} & \textbf{50.8} & \textbf{55.8} & \textbf{60.3}\\
    & $\text{Design}_{p}$ (\textit{w/} \text{Pretraining})  & \underline{4.51} & \underline{3.82} & \underline{3.35} & \underline{50.1} & \underline{55.7} & \underline{59.5}\\ 
	& $\text{Design}$ (\textit{w/o} \text{Pretraining})	& 6.27	& 5.05 & 4.87 & 39.62 & 49.21 & 50.01 \\
 
	 \bottomrule
  
\end{tabular}
}
\caption{\label{tab:design} Comparison of perplexity and recovery among the proposed protein design model (Group 2) and other baselines (Group 1) on the CATH, Ts50 and Ts500 datasets. The best result is \textbf{bolded}, followed by \underline{underlined}. All models here are trained/finetuned on the CATH training set. Design$_p$: protein-level pretrained model; Design$_r$: residue-level pretrained model; *: evaluated on the CATH 4.3 test set; Others: evaluated on the CATH 4.2 test set. (Note: Most of the experiments were evaluated on the CATH 4.2 dataset, but part of the work used CATH 4.3 due to version issues. The distribution of data sampled by the two versions is similar, so the trends and conclusions of the results are similar.)}
\end{table*}


\subsubsection{Implementation Details}
During pre-training, our structure-to-sequence transformation network is pretrained using the AdamW optimizer with a batch size of 8 and an initial learning rate of 1e-3. The teacher protein language model we use defaults to the ESM-2 base version, and we fix the parameters in the training pipeline. GVP module is set to 4 layers. 0.1 dropout, top-k neighbors 30, node hidden dimension of scalar features 1024, node hidden dimension of vector features 256. And the self-attention block that follow the GVP includes 4 self-attention layers, 8 attention heads, embedded dimension 512, and an attention dropout of 0.1. Note that we pretrain the model for residue-level alignment and protein-level alignment independently. 


\subsection{Internal Evaluation}
\textbf{Contact Map Prediction}.
Contact map predictions reflect the structural representation capabilities. Since we use the gradient backpropagation mechanism in the pretraining stage, the contact map predictor can generate contact maps directly without finetuning.
The contact map prediction scores are shown in Group 1 of Table ~\ref{tab:1}, which are evaluated on CATH, trRosetta, Ts50/Ts500 and CASP14 test sets.
Both the residue-level and protein-level pretrained models achieve high P@L accuracy in predicting contact maps for all datasets, implying that the pretrained structure module has learned rich structural representations. Within Group 1, the residue-level evaluation has better performance, probably due to its fine-graininess.


% Table 2 展示了模型预测contact map的精确度，评估集为trRosetta数据集。从表格中，通过对比#2和#1#3#4，我们可以预训练的结构模型可以明显地提升contact map的预测能力，这直接表明了我们预训练的模型学到了较好的结构表征能力。
% 通过对比#1 和# 2#3， 我们可以看出模型的语言监督信号了结构的自监督信号，对模型分别起到了不同的程度的促进作用，而语言模型的信号约束大于自监督的信号约束。换言之，语言模型为结构模型的学习提供了更多的语义信息。

% \subsubsection{Retrieval Alignment Evaluation}
% ACC/R@L ； KL散度等 => 对齐评估

% Table 2： 基于CATH 测试集，我们计算了struc和Seq模型输出特征的KL散度、R@L等指标，来衡量两个模型特征的对齐分布情况。（似乎可以和contact map score的表格结合在一起看）

% 由于我们使用的是基于对比学习的对齐方法来预训练模型，因此，2个模型的对齐能力，也是评估一个模型训练程度的重要指标。为了评估这样的能力，我们使用cath-test数据集，计算了struc encoder和seq encoder输出的特征的KL分布和R@L等指标。
% 与contact map的结果一致，预训练模型的的对齐效果，明显优于未预训练的模型。预训练的模型中，contact loss 和distill loss都帮助了模型学习。这与上树的结论不谋而合。


% Table 1. 基于ESM-2（seq Encoder），在不同数据集和Struc Encoder的表现。
\textbf{Retrieval Alignment Evaluation}.
In the proposed pretraining framework, a strong assumption is that protein language models and protein structure models are equally good at representing features, albeit in different modalities. Therefore, we propose to quantify the sequence-structure retrieval power to reflect the alignment power of the pretrained model. Looking back at the contrastive alignment loss we used in pretraining, the loss actually includes the structure-to-sequence and sequence-to-structure alignment calculations. The intermediate state score calculations can directly help evaluate the multi-modality alignment level.
The retrieval alignment evaluation on CATH, trRosetta, Ts50/Ts500 and CASP14 test sets are shown in Group 2 of Table~\ref{tab:1}. Similarly, We also report residue-level and protein-level results, respectively. The protein-level pretrained model is easier to align the sequences and structures with higher accuracy and lower KL distance, which satisfies the intuition. Overall, the pretrained structure module shows a strong alignment level implying that the proposed framework works well.



\subsection{Downstream Evaluation}
% External evaluation: Downstream tasks
% 外部任务：下游任务; 评估模型的泛化能力

% \subsubsection{Protein Sequence Design Task}
% This is one of the most important tasks for verifying the preformance of prerained protein structure model.

Computational protein design is the conceptual inverse of protein structure prediction, aiming to infer amino acid sequences given the corresponding atomic coordinates of the protein structure backbones. Figure \ref{:design} shows the proposed protein design model for our downstream evaluation, which directly transfers the pretrained structure model as the backbone, followed by a non-autoregressive decoder with linear MLPs. The CATH training set is used to train the protein design framework and finetune the structure model. And the CATH testing set and Ts50/Ts500 are used to score the primary results.

As shown in the Table ~\ref{tab:design}, we have selected several most typical baselines of different types. Among the methods with advanced performance, ProteinMPNN \cite{dauparas2022robust} and GVP-transformer \cite{hsu2022learning} are particularly prominent, which have more advantages in sequence recovery and model perplexity. The lightweight GVP-GNN\cite{jing2020learning} is still competitive, which is reflected in its relatively good performance and speed.

Compared to the baselines, our protein design models perform better on both perplexity and recovery metrics. It is worth mentioning that the non-autoregressive decoder in our module brings faster sampling speed.

In terms of different levels (Design$_p$ vs. Design$_r$), there is no significant difference in the residue-level and protein-level pretrained modules in downstream tasks, although Design$_r$ slightly exceeds Design$_p$ overall. This may be due to the fact that the small gap between the two levels of pretrained models is further narrowed during the fine-tuning process.

In addition, within Group 2, we use the pretrained model and the non-trained model as the backbone for comparison. It can be found that the pretrained model significantly improves the performance in terms of perplexity and recovery, which further shows that the pretrained structure model has a stronger generalization ability in downstream tasks.


% 我们的GVP Enc和ESM-invser的不同是，我们的模型，只有2层的Enc layers。

% 注意：与专门做蛋白质设计的工作不同：1. 不一定要跟SOTA比较，主要是跟自身比较；2. 模型结构和训练方法可以简单原始一点，不用像专门的design工作那样复杂（e.g., 不需要预训练SeqAE，不需要复杂的DECODER，不需要3-body等）。

% Model 1和 model2比较，内部比较，说明预训练的重要性；model3作为最强baseline的参考，和model2作用一样，但是更具说服力，说明我们的模型很强。

% 1.Struc Emb related tasks
% (1)蛋白质设计（可以直接跟自己比较）: 其他任务，例如MAQ等；
% a)struc Emb被训练过和没被驯过的性能比较
% b)Optional：跟其他流行的Design算法正面比较
% c)struc Emb训练过和没训练过的模型的收敛速度（收敛曲线）等进行比较



% \subsubsection{Protein Classification Task}
% Based on classification, we can visualize clustering. 
% https://github.com/DeepGraphLearning/GearNet


% TODO
% \textbf{loss curver}


% \subsection{Ablation Study}
% \textbf{protein-level vs. residue-level}.
% As mentioned repeatedly above, the comparison of protein-level and residue-level is a major ablation study. As shown in Table ~\ref{tab:contact_score}, Table ~\ref{tab:alignment_score} and Table~\ref{tab:design_score}. Overall, different fine-grained training exhibited different performance capabilities. For example, for alignment retrieval capability, protein-level training exhibits superior performance, while for contact map prediction, residue-level pre-training demonstrates a slight advantage. In the downstream task, after fine-tuning, the difference between the two levels becomes small. However, as far as training is concerned, we believe that protein-level training is less difficult and less complex.


% \textbf{Pretraining ablation}.
% In all evaluation tasks in Table ~\ref{tab:contact_score}, Table ~\ref{tab:alignment_score} and Table~\ref{tab:design_score}, we show comparisons between all structure models with pretraining and without pretraining. The results are obvious that the pretrained versions show better performances.



% % 内部评估指标即可

% % \textbf{Layer Ablation}
% % . 结构模型的消融，使用不同的层数
% % 比较不同层数的模型对任务的影响力，以design为主要评价任务

% % 后面在做 
% % 5.对于design任务消融，比较使用预训练的Enc模型的重要性
% % =》训练曲线等分析，分析收敛的速度等 =》证明具有加快收敛和提升效果的作用


% % 2. 消融学习率、bt等基础设置

% % \textbf{LMs Ablation}
% % % 3. 消融SeqEmb模型 => 使用不同的SeqEmb
% % % Table 2. 基于GVP-Transformer（Struc Encoder），在不同数据集和Seq Encoder的表现。


% % \begin{table*}[htp]
% % \centering
% % \scalebox{0.75}{
% % \begin{tabular}{lccccccccccccccc} 
% %     \toprule

% %     \multirow{2}{*}{\textbf{Group}} & \multirow{2}{*}{\textbf{Models}} & \multicolumn{3}{c}{\textbf{CATH testing}} & \multicolumn{3}{c}{\textbf{trRosetta Set}} & \multicolumn{3}{c}{\textbf{TS500 Set}} & \multicolumn{3}{c}{\textbf{Ts50 Set}}\\ 

% %     \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14}
    
% %     & & \textbf{P@L} & \textbf{P@L2}& \textbf{P@L5}& \textbf{P@L} & \textbf{P@L2}& \textbf{P@L5}& \textbf{P@L} & \textbf{P@L2}& \textbf{P@L5}& \textbf{P@L} & \textbf{P@L2}& \textbf{P@L5} \\ 
% %     \midrule
    
% %     \multirow{2}{*}{1} 
% %     & \text{ESM-2} & 78.05& 90.97 & 96.12 & 87.69	 & 94.94	 & 96.89	 & 90.31	 & 96.67	 & 98.1	 & 91.36	 & 98.66	 & 100
% %  \\
% % 	& \text{ESM-1b}	& xx& xx & xx \\
% %     \midrule
    
% %     \multirow{2}{*}{2} 
% %     & \text{ESM-2-base}	 & 72.21	& 88.03	& 98.31	& 81.3	& 92.42	& 96.18	& 83.78	& 94.14	& 97.44	& 85.18	& 96.32	& 99.53
% %  \\
% % 	& \text{ESM-1-base}		& xx& xx & xx \\
 
% % 	 \bottomrule
% % \end{tabular}
% % }
% % \caption{\label{tab:contact_score} Internal Tasks - Contact Map Prediction task.}
% % \end{table*}


% \textbf{Loss Ablation}
% % 4. 消融训练的loss
% % 仅去掉 contact loss = 》contact 的辅助重要性； 仅去掉 clip loss =》 语言信息的重要性；保留两者 =》 对比

% We perform an ablation study over seven model hyperparameters, using  contact prediction on the CATH and trRosetta validation set for evaluation. 

% The ablation studies show the use of combined training losses plays a
% critical role in model performance.



% %%%% Table2
% \begin{table}
% \centering
% \scalebox{1.0}{
% \begin{tabular}{ccccc}\toprule

% \multirow{2}{*}{\textbf{\#}} & \multirow{2}{*}{\textbf{PVM}} & \multirow{2}{*}{\textbf{PLM}} & \multicolumn{2}{c}{\textbf{WER}} \\ 
% \cmidrule(lr){4-5}
% &  &  & \textbf{Dev(\%)} & \textbf{Test(\%)}  \\ 
% \midrule

% 	1 & \xmark & \xmark & 89.2	& 88.5 \\
% 	2 & \xmark & \cmark & 30.8  & 31.4 \\
% 	3 & \cmark & \xmark	& 20.4 	&  20.7 \\
% 	4 & \cmark & \cmark	& \textbf{19.8} & \textbf{20.1} \\
% \bottomrule
  
% \end{tabular}
% }
% \caption{\label{table_3} Ablation comparison of pretrained modules on PHOENIX-14 (PVM: pretrained visual module, PLM: pretrained language module). The best results are bolded.}
% \end{table}


% %%%% Table4
% \begin{table}
% \centering
% \scalebox{1.0}{
% \begin{tabular}{ccccc}\toprule

%  \multirow{2}{*}{\textbf{\#}} & \multirow{2}{*}{\textbf{CTC Weight}} & \multirow{2}{*}{\textbf{Align Weight}} & \multicolumn{2}{c}{\textbf{WER}} \\ 
% \cmidrule(lr){4-5}
% &  &  & \textbf{Dev(\%)} & \textbf{Test(\%)}  \\ 
% \midrule
% 	1 & 1.0	& 0 & 20.2 & 20.3 \\
% 	2 & 1.0 & 1.0  &  20.0 & 20.2 \\
% 	3 & 1.0 & 10.0 &  \textbf{19.8}	&  \textbf{20.1} \\
% 	4 & 1.0 & 100.0 & 20.1  &  20.6 \\
% 	5 & 1.0 & 1000.0	&  20.5 & 20.9 \\
% \bottomrule
  
% \end{tabular}
% }
% \caption{\label{table_4} Ablation study of varied weights of CTC loss and proposed contrastive alignment loss on PHOENIX-14. The best results are bolded.}
% \end{table}

% \subsection{Visualization Analysis}

% \subsubsection{Sequence-Structure Alignment}
% % Protein level（N个proteins的话 就是N*N的矩阵，横轴为某个protein的struc feat，纵轴为某个protein的seq feat，每个矩阵位点表示其相似度）和residue level（m个proteins有M个氨基酸的的话 就是M*M的矩阵），理想的情况下，第（i,i）个位点的相似度最高，也就是斜对角的相似度最高

% We show protein-level and residue-level alignment performances. 

% \begin{figure}[h]
%   \centering
%   % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    \includegraphics[width=0.9\linewidth]{pics/6.png}
%    \caption{Sequence-Structure Alignment Visualization}
%    \label{alignment_visualization}
% \end{figure}



% \subsubsection{Contact Map Instances}
% % 不同训练步数 contact map的变化之 某个实例

% \begin{figure*}[htp]
%   \centering
%   % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    \includegraphics[width=1.0\linewidth]{pics/8.png}
%    \caption{Sequence-Structure Alignment Visualization}
%    \label{alignment_visualization}
% \end{figure*}


% % \subsubsection{Clustering Visualization}
% % % 结合分类任务; 可视化聚类（找永杰、老王）
% % % 分析一下Struc feat （Seq feat）的聚类情况，使用DMT，PAC，KNN等，验证一下检索的情况，看看聚在一起的都是些什么东西。（聚类可视化+聚类实例分析，证明训练的encoder有较好的特征捕捉能力）
% % Clustering visualization can show the performance of classification task, which can also show the effectiveness of Pretrained structure model.

% % \begin{figure}[h]
% %   \centering
% %   % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
% %    \includegraphics[width=0.9\linewidth]{pics/5.png}

% %    \caption{Clustering Visualization}
% %    \label{clustering}
% % \end{figure}


\section{Conclusion}
% 在这个工作中，我们首次提出了使用蛋白质语言模型来训练结构模型的训练方法。这不仅 可以充分利用现有的大规模数据训练得到的powerful蛋白质语言模型，还可以学到具有语言信息的蛋白质结构模型。此外，我们还使用了自监督的contact map 预测的方法，辅助模型学到结构信息。contact 为了能够定量评估模型，我们分别提出了内部评估方法，e.g., contact map prediction， 。。，外部评估方法，e.g., protein design，protein 。。。定量的实验，充分证明了我们的方法可以学到有效有效的struc emb。And 定性的分析，帮助我们进一步理解模型的有效性。
% 尽管，大规模语言模型可以帮助学习结构信息，但是这个探索还刚起步。未来，我们认为使用更多有效的struc structure和训练约束，可以帮助结构模型进一步改进。

In this work, we first propose the use of pretrained protein language models to train protein structure models, which exploit sufficient prior knowledge from robust protein language models. Furthermore, we use self-supervised contact map prediction as an auxiliary structural constraint. A series of internal tasks (e.g., contact map prediction and retrieval alignment evaluation), and external tasks (e.g., protein design) are proposed to evaluate the performance.
The quantitative experimental analysis demonstrates the superiority of our pretrained framework.
Although large-scale language models do help to learn effective structural information from language knowledge, related exploration is still in its infancy. In future work, we will adopt more powerful structural architecture and training constraints for the sequence-structure translation.



% \section*{Acknowledgements}
% This work was supported by National Key R\&D Program of China (No. 2022ZD0115100), National Natural Science Foundation of China Project (No. U21A20427), and Project (No. WU2022A0XX) from the Center of Synthetic Biology and Integrated Bioengineering of Westlake University.


\section*{Appendix}
\label{sec:appendix}

\subsection*{Full CASP14 Targets}
\label{sec:casp14_targets}
The full CASP14 target list we used in this work is:

T1024, T1025, T1026, T1027, T1029, T1030, T1031, T1032, T1033, T1035, T1036s1, T1037, T1038, T1039, T1040, T1041, T1042, T1043, T1044, T1046s1, T1046s2, T1049, T1050, T1054, T1056, T1064, T1067, T1073, T1074, T1079, T1080, T1082, T1090, T1099. 

These are all publicly available CASP14 targets totaling 34, and we have filtered the data for which no PDB files are available. 


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
