%% Created for Prasanta Ghosh on -01-20

@misc{profanity,
  title = {Building a Better Profanity Detection Library with scikit-learn},
  howpublished = {\url{https://victorzhou.com/blog/better-profanity-detection-with-scikit-learn/}}
}

@article{paszke2017automatic,
  title={Automatic differentiation in pytorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}

@article{o2007empathy,
  title={Empathy and depression: the moral system on overdrive},
  author={O’Connor, Lynn E and Berry, Jack W and Lewis, Thomas and Mulherin, Kathleen and Crisostomo, Patrice S},
  journal={Empathy in mental illness},
  year={2007},
  publisher={Cambridge University Press Cambridge}
}


@article{vingerhoets2013swearing,
  title={Swearing: A biopsychosocial perspective},
  author={Vingerhoets, Ad JJM and Bylsma, Lauren M and De Vlam, Cornelis},
  journal={Psihologijske teme},
  year={2013},
  publisher={Filozofski fakultet u Rijeci}
}

@article{rude2004language,
  title={Language use of depressed and depression-vulnerable college students},
  author={Rude, Stephanie and Gortner, Eva-Maria and Pennebaker, James},
  journal={Cognition \& Emotion},
  year={2004},
  publisher={Taylor \& Francis}
}

@book{martin2006morality,
  title={From morality to mental health: Virtue and vice in a therapeutic culture},
  author={Martin, Mike W},
  year={2006},
  publisher={Oxford University Press}
}

@article{vos2016global,
  title={Global, regional, and national incidence, prevalence, and years lived with disability for 310 diseases and injuries, 1990--2015: a systematic analysis for the Global Burden of Disease Study 2015},
  author={Vos, Theo and Allen, Christine and Arora, Megha and Barber, Ryan M and Bhutta, Zulfiqar A and Brown, Alexandria and Carter, Austin and Casey, Daniel C and Charlson, Fiona J and Chen, Alan Z and others},
  journal={The lancet},
  year={2016},
  
  publisher={Elsevier}
}

@inproceedings{coppersmith2015adhd,
  title={From ADHD to SAD: Analyzing the language of mental health on Twitter through self-reported diagnoses},
  author={Coppersmith, Glen and Dredze, Mark and Harman, Craig and Hollingshead, Kristy},
  booktitle={Proc. 2nd workshop on CLPsych},
  year={2015}
}

@article{graham2009liberals,
  title={Liberals and conservatives rely on different sets of moral foundations.},
  author={Graham, Jesse and Haidt, Jonathan and Nosek, Brian A},
  journal={Journal of personality and social psychology},
  year={2009},
  publisher={American Psychological Association}
}

@article{lyons2018mental,
  title={Mental distress and language use: Linguistic analysis of discussion forum posts},
  author={Lyons, Minna and Aksayli, Nazli Deniz and Brewer, Gayle},
  journal={Computers in Human Behavior},
  year={2018},
  publisher={Elsevier}
}

@article{hamilton1983cognitive,
  title={Cognitive patterns and major depressive disorder: a longitudinal study in a hospital setting.},
  author={Hamilton, Eleanor W and Abramson, Lyn Y},
  journal={Journal of Abnormal Psychology},
  year={1983},
  publisher={American Psychological Association}
}

@article{hollon1986specificity,
  title={Specificity of depressotypic cognitions in clinical depression.},
  author={Hollon, Steven D and Kendall, Philip C and Lumry, Ann},
  journal={Journal of Abnormal Psychology},
  volume={95},
  number={1},
  pages={52},
  year={1986},
  publisher={American Psychological Association}
}

@article{Davis80-COP,
	Author = {Steven B. Davis and Paul Mermelstein},
	Journal = {IEEE Transactions on Acoustics, Speech and Signal Processing},
	Number = {4},
	Pages = {357--366},
	Title = {Comparison of Parametric Representation for Monosyllabic Word Recognition in Continuously Spoken Sentences},
	Volume = {28},
  Month = aug,
	Year = {1980}}

@article{Rabiner89-ATO,
	Author = {Lawrence R. Rabiner},
	Journal = {Proceedings of the IEEE},
	Number = {2},
	Pages = {257--286},
	Title = {A Tutorial on Hidden {Markov} Models and Selected Applications in Speech Recognition},
	Volume = {77},
  Month = feb,
	Year = {1989}}

@book{Hastie09-TEO,
	Address = {New York},
	Author = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
	Publisher = {Springer},
	Title = {The Elements of Statistical Learning -- Data Mining, Inference, and Prediction},
	Year = {2009}}

@inproceedings{YourName21-XXX,
	Author = {Firstname1 Lastname1 and Firstname2 Lastname2 and Firstname3 Lastname3},
	Crossref = {INTERSPEECH21},
	Pages = {100--104},
	Title = {Title of your {INTERSPEECH} 2021 publication}}

@proceedings{INTERSPEECH22,
	Booktitle = {Proceedings {INTERSPEECH} 2022 -- 23\textsuperscript{rd} Annual Conference of the International Speech Communication Association},
	Date-Modified = {2022-01-18},
	Key = {INTERSPEECH},
	Title = {Proceedings {INTERSPEECH} 2022 -- 23\textsuperscript{rd} Annual Conference of the International Speech Communication Association},
  Address = {{Incheon, Korea}},
  Month = {{Sep.}},
	Year = {2022}}
	% Άρθρα σε διεθνή επιστημονικά περιοδικά
@article{mitchell1997machine,
  title={Machine learning},
  author={Mitchell, Tom},
  year={1997},
  publisher={McGraw hill Burr Ridge}
}

@article{bishop2006pattern,
  title={Pattern recognition},
  author={Bishop, Christopher M},
  journal={Machine learning},
  volume={128},
  number={9},
  year={2006}
}

@article{noble2006support,
  title={What is a support vector machine?},
  author={Noble, William S},
  journal={Nature biotechnology},
  volume={24},
  number={12},
  pages={1565--1567},
  year={2006},
  publisher={Nature Publishing Group}
}

@inproceedings{rish2001empirical,
  title={An empirical study of the naive Bayes classifier},
  author={Rish, Irina and others},
  booktitle={IJCAI 2001 workshop on empirical methods in artificial intelligence},
  volume={3},
  number={22},
  pages={41--46},
  year={2001}
}

@article{DBLP:journals/corr/MohammadT13,
  author    = {Saif Mohammad and
               Peter D. Turney},
  title     = {Crowdsourcing a Word-Emotion Association Lexicon},
  journal   = {CoRR},
  volume    = {abs/1308.6297},
  year      = {2013},
  url       = {http://arxiv.org/abs/1308.6297},
  eprinttype = {arXiv},
  eprint    = {1308.6297},
  timestamp = {Mon, 13 Aug 2018 16:48:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MohammadT13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wilson2005opinionfinder,
  title={OpinionFinder: A system for subjectivity analysis},
  author={Wilson, Theresa and Hoffmann, Paul and Somasundaran, Swapna and Kessler, Jason and Wiebe, Janyce and Choi, Yejin and Cardie, Claire and Riloff, Ellen and Patwardhan, Siddharth},
  booktitle={Proceedings of HLT/EMNLP 2005 Interactive Demonstrations},
  pages={34--35},
  year={2005}
}
@inproceedings{amir2019mental,
  title={Mental health surveillance over social media with digital cohorts},
  author={Amir, Silvio and Dredze, Mark and Ayers, John W},
  booktitle={Proc. 6th Workshop on CLPsych},
  year={2019}
}

@article{zetsche2012depression,
  title={Depression and rumination: Relation to components of inhibition},
  author={Zetsche, Ulrike and D'Avanzato, Catherine and Joormann, Jutta},
  journal={Cognition \& emotion},
  volume={26},
  number={4},
  pages={758--767},
  year={2012},
  publisher={Taylor \& Francis}
}

@article{conneely2021medicalising,
  title={Medicalising the moral: the case of depression as revealed in internet blogs},
  author={Conneely, Maev and Higgs, Paul and Moncrieff, Joanna},
  journal={Social Theory \& Health},
  volume={19},
  number={4},
  pages={380--398},
  year={2021},
  publisher={Springer}
}

@book{american2013diagnostic,
  title={Diagnostic and statistical manual of mental disorders (DSM-5{\textregistered})},
  author={American Psychiatric Association and others},
  year={2013},
  publisher={American Psychiatric Pub}
}

@inproceedings{pirina2018identifying,
  title={Identifying depression on reddit: The effect of training data},
  author={Pirina, Inna and {\c{C}}{\"o}ltekin, {\c{C}}a{\u{g}}r{\i}},
  booktitle={Proc. EMNLP Workshop SMM4H: The 3rd Social Media Mining for Health Applications Workshop \& Shared Task},
  year={2018}
}

@article{ren2021depression,
  title={Depression detection on reddit with an emotion-based attention network: algorithm development and validation},
  author={Ren, Lu and Lin, Hongfei and Xu, Bo and Zhang, Shaowu and Yang, Liang and Sun, Shichang and others},
  journal={JMIR Medical Informatics},
  year={2021},
  publisher={JMIR Publications Inc., Toronto, Canada}
}

@article{tadesse2019detection,
  title={Detection of depression-related posts in reddit social media forum},
  author={Tadesse, Michael M and Lin, Hongfei and Xu, Bo and Yang, Liang},
  journal={IEEE Access},
  year={2019},
  publisher={IEEE}
}
@article{demszky2020goemotions,
  title={GoEmotions: A dataset of fine-grained emotions},
  author={Demszky, Dorottya and Movshovitz-Attias, Dana and Ko, Jeongwoo and Cowen, Alan and Nemade, Gaurav and Ravi, Sujith},
  journal={arXiv preprint arXiv:2005.00547},
  year={2020}
}

@article{turcan2019dreaddit,
  title={Dreaddit: A Reddit dataset for stress analysis in social media},
  author={Turcan, Elsbeth and McKeown, Kathleen},
  journal={arXiv preprint arXiv:1911.00133},
  year={2019}
}

@article{ekman1992there,
  title={Are there basic emotions?},
  author={Ekman, Paul},
  year={1992},
  publisher={American Psychological Association}
}
@inproceedings{turcan2021emotion,
  title={Emotion-infused models for explainable psychological stress detection},
  author={Turcan, Elsbeth and Muresan, Smaranda and McKeown, Kathleen},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2895--2909},
  year={2021}
}

@article{ji2021mentalbert,
  title={MentalBERT: Publicly Available Pretrained Language Models for Mental Healthcare},
  author={Ji, Shaoxiong and Zhang, Tianlin and Ansari, Luna and Fu, Jie and Tiwari, Prayag and Cambria, Erik},
  journal={arXiv preprint arXiv:2110.15621},
  year={2021}
}
@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@techreport{rumelhart1985learning,
  title={Learning internal representations by error propagation},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  year={1985},
  institution={California Univ San Diego La Jolla Inst for Cognitive Science}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@article{gers2000learning,
  title={Learning to forget: Continual prediction with LSTM},
  author={Gers, Felix A and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  journal={Neural computation},
  volume={12},
  number={10},
  pages={2451--2471},
  year={2000},
  publisher={MIT Press}
}

@misc{daniel2000speech,
  title={Speech and Language Processing},
  author={Daniel, Jurafsky and James, H Martin},
  year={2000},
  publisher={Prentice-Hall}
}

@article{dumoulin2018feature-wise,
  author = {Dumoulin, Vincent and Perez, Ethan and Schucher, Nathan and Strub, Florian and Vries, Harm de and Courville, Aaron and Bengio, Yoshua},
  title = {Feature-wise transformations},
  journal = {Distill},
  year = {2018},
  note = {https://distill.pub/2018/feature-wise-transformations},
  doi = {10.23915/distill.00011}
}

@article{feldman2013techniques,
  title={Techniques and applications for sentiment analysis},
  author={Feldman, Ronen},
  journal={Communications of the ACM},
  volume={56},
  number={4},
  pages={82--89},
  year={2013},
  publisher={ACM New York, NY, USA}
}

@article{yaghoobian2021sarcasm,
  title={Sarcasm Detection: A Comparative Study},
  author={Yaghoobian, Hamed and Arabnia, Hamid R and Rasheed, Khaled},
  journal={arXiv preprint arXiv:2107.02276},
  year={2021}
}

@article{majumder2017deep,
  title={Deep learning-based document modeling for personality detection from text},
  author={Majumder, Navonil and Poria, Soujanya and Gelbukh, Alexander and Cambria, Erik},
  journal={IEEE Intelligent Systems},
  volume={32},
  number={2},
  pages={74--79},
  year={2017},
  publisher={IEEE}
}

@article{joulin2016bag,
  title={Bag of tricks for efficient text classification},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1607.01759},
  year={2016}
}

@article{yates2017depression,
  title={Depression and self-harm risk assessment in online forums},
  author={Yates, Andrew and Cohan, Arman and Goharian, Nazli},
  journal={arXiv preprint arXiv:1709.01848},
  year={2017}
}

@inproceedings{wang2012baselines,
  title={Baselines and bigrams: Simple, good sentiment and topic classification},
  author={Wang, Sida I and Manning, Christopher D},
  booktitle={Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={90--94},
  year={2012}
}

@techreport{pennebaker2015development,
  title={The development and psychometric properties of LIWC2015},
  author={Pennebaker, James W and Boyd, Ryan L and Jordan, Kayla and Blackburn, Kate},
  year={2015}
}
@article{staiano2014depechemood,
  title={Depechemood: a lexicon for emotion analysis from crowd-annotated news},
  author={Staiano, Jacopo and Guerini, Marco},
  journal={arXiv preprint arXiv:1405.1605},
  year={2014}
}

@article{chung2014empirical,
  title={Empirical evaluation of gated recurrent neural networks on sequence modeling},
  author={Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.3555},
  year={2014}
}

@article{tai2015improved,
  title={Improved semantic representations from tree-structured long short-term memory networks},
  author={Tai, Kai Sheng and Socher, Richard and Manning, Christopher D},
  journal={arXiv preprint arXiv:1503.00075},
  year={2015}
}

@article{rao2020mgl,
  title={MGL-CNN: A hierarchical posts representations model for identifying depressed individuals in online forums},
  author={Rao, Guozheng and Zhang, Yue and Zhang, Li and Cong, Qing and Feng, Zhiyong},
  journal={IEEE Access},
  year={2020},
  publisher={IEEE}
}

@inproceedings{rao2020knowledge,
  title={A Knowledge Enhanced Ensemble Learning Model for Mental Disorder Detection on Social Media},
  author={Rao, Guozheng and Peng, Chengxia and Zhang, Li and Wang, Xin and Feng, Zhiyong},
  booktitle={KSEM},
  year={2020},
  organization={Springer}
}

@article{yadav2020assessing,
  title={Assessing the severity of health states based on social media posts},
  author={Yadav, Shweta and Sain, Joy Prakash and Sheth, Amit and Ekbal, Asif and Saha, Sriparna and Bhattacharyya, Pushpak},
  journal={arXiv preprint arXiv:2009.09600},
  year={2020}
}

@article{yadav2020identifying,
  title={Identifying Depressive Symptoms from Tweets: Figurative Language Enabled Multitask Learning Framework},
  author={Yadav, Shweta and Chauhan, Jainish and Sain, Joy Prakash and Thirunarayan, Krishnaprasad and Sheth, Amit and Schumm, Jeremiah},
  journal={arXiv preprint arXiv:2011.06149},
  year={2020}
}

@article{zhang2019feature,
  title={Feature Fusion Text Classification Model Combining CNN and BiGRU with Multi-Attention Mechanism},
  author={Zhang, Jingren and Liu, Fang’ai and Xu, Weizhi and Yu, Hui and others},
  journal={Future Internet},
  volume={11},
  number={11},
  pages={237},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@inproceedings{buechel2017emobank,
  title={Emobank: Studying the impact of annotation perspective and representation format on dimensional emotion analysis},
  author={Buechel, Sven and Hahn, Udo},
  booktitle={Proc. 15th ACL},
  year={2017}
}

@mastersthesis{chen2015convolutional,
  title={Convolutional neural network for sentence classification},
  author={Chen, Yahui},
  year={2015},
  school={University of Waterloo}
}

@article{rezaeinia2019sentiment,
  title={Sentiment analysis based on improved pre-trained word embeddings},
  author={Rezaeinia, Seyed Mahdi and Rahmani, Rouhollah and Ghodsi, Ali and Veisi, Hadi},
  journal={Expert Systems with Applications},
  volume={117},
  pages={139--147},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{ghosh2017magnets,
  title={Magnets for sarcasm: Making sarcasm detection timely, contextual and very personal},
  author={Ghosh, Aniruddha and Veale, Tony},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={482--491},
  year={2017}
}

@article{bosco2013developing,
  title={Developing corpora for sentiment analysis: The case of irony and senti-tut},
  author={Bosco, Cristina and Patti, Viviana and Bolioli, Andrea},
  journal={IEEE intelligent systems},
  volume={28},
  number={2},
  pages={55--63},
  year={2013},
  publisher={IEEE}
}
@article{pennebaker2001linguistic,
  title={Linguistic inquiry and word count: LIWC 2001},
  author={Pennebaker, James W and Francis, Martha E and Booth, Roger J},
  journal={Mahway: Lawrence Erlbaum Associates},
  year={2001}
}

@inproceedings{jiang2020automatic,
  title={Automatic Text-Based Personality Recognition on Monologues and Multiparty Dialogues Using Attentive Networks and Contextual Embeddings (Student Abstract)},
  author={Jiang, Hang and Zhang, Xianzhe and Choi, Jinho D},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={10},
  pages={13821--13822},
  year={2020}
}
@misc{sentimentanalysishugging,
  title = {pipelines in huggingface},
  howpublished = {\url{https://huggingface.co/docs/transformers/quicktour}}
}


@inproceedings{bucur-etal-2021-exploratory,
    title = "An Exploratory Analysis of the Relation between Offensive Language and Mental Health",
    author = "Bucur, Ana-Maria  and
      Zampieri, Marcos  and
      Dinu, Liviu P.",
    booktitle = "Findings ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    doi = "10.18653/v1/2021.findings-acl.315",
    pages = "3600--3606",
}
@inproceedings{yadav-etal-2020-identifying,
    title = "Identifying Depressive Symptoms from Tweets: Figurative Language Enabled Multitask Learning Framework",
    author = "Yadav, Shweta  and
      Chauhan, Jainish  and
      Sain, Joy Prakash  and
      Thirunarayan, Krishnaprasad  and
      Sheth, Amit  and
      Schumm, Jeremiah",
    booktitle = "Proc. 28th COLING",
    month = dec,
    year = "2020",
    publisher = "International Committee on Computational Linguistics",
    doi = "10.18653/v1/2020.coling-main.61",
    pages = "696--709",
    abstract = "Existing studies on using social media for deriving mental health status of users focus on the depression detection task. However, for case management and referral to psychiatrists, health-care workers require practical and scalable depressive disorder screening and triage system. This study aims to design and evaluate a decision support system (DSS) to reliably determine the depressive triage level by capturing fine-grained depressive symptoms expressed in user tweets through the emulation of the Patient Health Questionnaire-9 (PHQ-9) that is routinely used in clinical practice. The reliable detection of depressive symptoms from tweets is challenging because the 280-character limit on tweets incentivizes the use of creative artifacts in the utterances and figurative usage contributes to effective expression. We propose a novel BERT based robust multi-task learning framework to accurately identify the depressive symptoms using the auxiliary task of figurative usage detection. Specifically, our proposed novel task sharing mechanism,co-task aware attention, enables automatic selection of optimal information across the BERT lay-ers and tasks by soft-sharing of parameters. Our results show that modeling figurative usage can demonstrably improve the model{'}s robustness and reliability for distinguishing the depression symptoms.",
}

@inproceedings{yates-etal-2017-depression,
    title = "Depression and Self-Harm Risk Assessment in Online Forums",
    author = "Yates, Andrew  and
      Cohan, Arman  and
      Goharian, Nazli",
    booktitle = "Proc. EMNLP",
    month = sep,
    year = "2017",

    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/D17-1322",
    abstract = "Users suffering from mental health conditions often turn to online resources for support, including specialized online support communities or general communities such as Twitter and Reddit. In this work, we present a framework for supporting and studying users in both types of communities. We propose methods for identifying posts in support communities that may indicate a risk of self-harm, and demonstrate that our approach outperforms strong previously proposed methods for identifying such posts. Self-harm is closely related to depression, which makes identifying depressed users on general forums a crucial related task. We introduce a large-scale general forum dataset consisting of users with self-reported depression diagnoses matched with control users. We show how our method can be applied to effectively identify depressed users from their use of language alone. We demonstrate that our method outperforms strong baselines on this general forum dataset.",
}

@inproceedings{margatina-etal-2019-attention,
    title = "Attention-based Conditioning Methods for External Knowledge Integration",
    author = "Margatina, Katerina  and
      Baziotis, Christos  and
      Potamianos, Alexandros",
    booktitle = "Proc. 57th ACL",
    month = jul,
    year = "2019",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/P19-1385",
    abstract = "In this paper, we present a novel approach for incorporating external knowledge in Recurrent Neural Networks (RNNs). We propose the integration of lexicon features into the self-attention mechanism of RNN-based architectures. This form of conditioning on the attention distribution, enforces the contribution of the most salient words for the task at hand. We introduce three methods, namely attentional concatenation, feature-based gating and affine transformation. Experiments on six benchmark datasets show the effectiveness of our methods. Attentional feature-based gating yields consistent performance improvement across tasks. Our approach is implemented as a simple add-on module for RNN-based models with minimal computational overhead and can be adapted to any deep neural architecture.",
}

@inproceedings{cho-etal-2014-learning,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proc EMNLP",
    month = oct,
    year = "2014",
    publisher = "Association for Computational Linguistics",
    doi = "10.3115/v1/D14-1179",
}


@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proc. NAACL",
    month = jun,
    year = "2019",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/N19-1423",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@inproceedings{inproceedings,
author = {Yadav, Shweta and Sain, Joy Prakash and Sheth, Amit and Ekbal, Asif and Saha, Sriparna and Bhattacharyya, Pushpak},
year = {2021},
month = {01},
title = {Assessing the Severity of Health States based on Social Media Posts},
doi = {10.1109/ICPR48806.2021.9411980}
}

@article{zhang2016dependency,
  title={Dependency sensitive convolutional neural networks for modeling sentences and documents},
  author={Zhang, Rui and Lee, Honglak and Radev, Dragomir},
  journal={arXiv preprint arXiv:1611.02361},
  year={2016}
}

@article{zhang2018combination,
  title={A combination of RNN and CNN for attention-based relation classification},
  author={Zhang, Xiaobin and Chen, Fucai and Huang, Ruiyang},
  journal={Procedia computer science},
  volume={131},
  pages={911--917},
  year={2018},
  publisher={Elsevier}
}


@article{margatina2019attention,
  title={Attention-based conditioning methods for external knowledge integration},
  author={Margatina, Katerina and Baziotis, Christos and Potamianos, Alexandros},
  journal={arXiv preprint arXiv:1906.03674},
  year={2019}
}

@article{yates2017depression,
  title={Depression and self-harm risk assessment in online forums},
  author={Yates, Andrew and Cohan, Arman and Goharian, Nazli},
  journal={arXiv preprint arXiv:1709.01848},
  year={2017}
}

@article{cohan2018smhd,
  title={SMHD: a large-scale resource for exploring online language usage for multiple mental health conditions},
  author={Cohan, Arman and Desmet, Bart and Yates, Andrew and Soldaini, Luca and MacAvaney, Sean and Goharian, Nazli},
  journal={arXiv preprint arXiv:1806.05258},
  year={2018}
}

@inproceedings{gratch2014distress,
  title={The distress analysis interview corpus of human and computer interviews.},
  author={Gratch, Jonathan and Artstein, Ron and Lucas, Gale M and Stratou, Giota and Scherer, Stefan and Nazarian, Angela and Wood, Rachel and Boberg, Jill and DeVault, David and Marsella, Stacy and others},
  booktitle={LREC},
  pages={3123--3128},
  year={2014}
}


@article{xezonaki2020affective,
  title={Affective conditioning on hierarchical attention networks applied to depression detection from transcribed clinical interviews},
  author={Xezonaki, Danai and Paraskevopoulos, Georgios and Potamianos, Alexandros and Narayanan, Shrikanth},
  journal={Interspeech 2020},
  year={2020}
}

@inproceedings{xu2020inferring,
  title={Inferring Social Media Users’ Mental Health Status from Multimodal Information},
  author={Xu, Zhentao and P{\'e}rez-Rosas, Ver{\'o}nica and Mihalcea, Rada},
  booktitle={Proceedings of The 12th Language Resources and Evaluation Conference},
  pages={6292--6299},
  year={2020}
}

@article{yadav2020assessing,
  title={Assessing the severity of health states based on social media posts},
  author={Yadav, Shweta and Sain, Joy Prakash and Sheth, Amit and Ekbal, Asif and Saha, Sriparna and Bhattacharyya, Pushpak},
  journal={arXiv preprint arXiv:2009.09600},
  year={2020}
}

@article{yadav2020identifying,
  title={Identifying Depressive Symptoms from Tweets: Figurative Language Enabled Multitask Learning Framework},
  author={Yadav, Shweta and Chauhan, Jainish and Sain, Joy Prakash and Thirunarayan, Krishnaprasad and Sheth, Amit and Schumm, Jeremiah},
  journal={arXiv preprint arXiv:2011.06149},
  year={2020}
}

@article{zhang2019feature,
  title={Feature Fusion Text Classification Model Combining CNN and BiGRU with Multi-Attention Mechanism},
  author={Zhang, Jingren and Liu, Fang’ai and Xu, Weizhi and Yu, Hui and others},
  journal={Future Internet},
  volume={11},
  number={11},
  pages={237},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{trotzek2018utilizing,
  title={Utilizing neural networks and linguistic metadata for early detection of depression indications in text sequences},
  author={Trotzek, Marcel and Koitka, Sven and Friedrich, Christoph M},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={32},
  number={3},
  pages={588--601},
  year={2018},
  publisher={IEEE}
}

@inproceedings{ye2018encoding,
  title={Encoding sentiment information into word vectors for sentiment analysis},
  author={Ye, Zhe and Li, Fang and Baldwin, Timothy},
  booktitle={Proceedings of the 27th International Conference on Computational Linguistics},
  pages={997--1007},
  year={2018}
}

@inproceedings{de2013social,
  title={Social media as a measurement tool of depression in populations},
  author={De Choudhury, Munmun and Counts, Scott and Horvitz, Eric},
  booktitle={Proceedings of the 5th annual ACM web science conference},
  pages={47--56},
  year={2013}
}

@techreport{world2017depression,
  title={Depression and other common mental disorders: global health estimates},
  author={World Health Organization and others},
  year={2017},
  institution={World Health Organization}
}

@article{lin2016association,
  title={Association between social media use and depression among US young adults},
  author={Lin, Liu Yi and Sidani, Jaime E and Shensa, Ariel and Radovic, Ana and Miller, Elizabeth and Colditz, Jason B and Hoffman, Beth L and Giles, Leila M and Primack, Brian A},
  journal={Depression and anxiety},
  year={2016},
  publisher={Wiley Online Library}
}

@inproceedings{de2013predicting,
  title={Predicting depression via social media},
  author={De Choudhury, Munmun and Gamon, Michael and Counts, Scott and Horvitz, Eric},
  booktitle={Proc. AAAI},
  year={2013}
}

@article{keles2020systematic,
  title={A systematic review: the influence of social media on depression, anxiety and psychological distress in adolescents},
  author={Keles, Betul and McCrae, Niall and Grealish, Annmarie},
  journal={International Journal of Adolescence and Youth},
  year={2020},
  publisher={Taylor \& Francis}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{ghosh2017magnets,
  title={Magnets for sarcasm: Making sarcasm detection timely, contextual and very personal},
  author={Ghosh, Aniruddha and Veale, Tony},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={482--491},
  year={2017}
}

@misc{kim5882convolutionalneuralnetworksforsentence,
  title={ConvolutionalNeuralNetworksforSentence Classification},
  author={Kim, Yoon},
  journal={2014r08r25). https://arx iv. org/abs/1408.5882}
}



@article{yang2021psycholinguistic,
  title={Psycholinguistic Tripartite Graph Network for Personality Detection},
  author={Yang, Tao and Yang, Feifan and Ouyang, Haolan and Quan, Xiaojun},
  journal={arXiv preprint arXiv:2106.04963},
  year={2021}
}


@article{wallace2019universal,
  title={Universal adversarial triggers for attacking and analyzing NLP},
  author={Wallace, Eric and Feng, Shi and Kandpal, Nikhil and Gardner, Matt and Singh, Sameer},
  journal={arXiv preprint arXiv:1908.07125},
  year={2019}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}



@article{margatina2019attention,
  title={Attention-based conditioning methods for external knowledge integration},
  author={Margatina, Katerina and Baziotis, Christos and Potamianos, Alexandros},
  journal={arXiv preprint arXiv:1906.03674},
  year={2019}
}

@article{araque2020moralstrength,
  title={MoralStrength: Exploiting a moral lexicon and embedding similarity for moral foundations prediction},
  author={Araque, Oscar and Gatti, Lorenzo and Kalimeri, Kyriaki},
  journal={Knowledge-based systems},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{georgiou2019deep,
  title={Deep Hierarchical Fusion with Application in Sentiment Analysis.},
  author={Georgiou, Efthymios and Papaioannou, Charilaos and Potamianos, Alexandros},
  booktitle={INTERSPEECH},
  pages={1646--1650},
  year={2019}
}

@article{georgiou2021m3,
  title={M3: MultiModal Masking applied to sentiment analysis},
  author={Georgiou, Efthymios and Paraskevopoulos, Georgios and Potamianos, Alexandros},
  journal={Proc. Interspeech 2021},
  pages={2876--2880},
  year={2021}
}
@article{pestian2012sentiment,
  title={Sentiment analysis of suicide notes: A shared task},
  author={Pestian, John P and Matykiewicz, Pawel and Linn-Gust, Michelle and South, Brett and Uzuner, Ozlem and Wiebe, Jan and Cohen, K Bretonnel and Hurdle, John and Brew, Christopher},
  journal={Biomedical informatics insights},
  volume={5},
  pages={BII--S9042},
  year={2012},
  publisher={SAGE Publications Sage UK: London, England}
}
@article{10.1136/amiajnl-2014-002733,
    author = {Huang, Sandy H and LePendu, Paea and Iyer, Srinivasan V and Tai-Seale, Ming and Carrell, David and Shah, Nigam H},
    title = "{Toward personalizing treatment for depression: predicting diagnosis and severity}",
    journal = {Journal of the American Medical Informatics Association},
    volume = {21},
    number = {6},
    pages = {1069-1075},
    year = {2014},
    month = {07},
    abstract = "{Objective Depression is a prevalent disorder difficult to diagnose and treat. In particular, depressed patients exhibit largely unpredictable responses to treatment. Toward the goal of personalizing treatment for depression, we develop and evaluate computational models that use electronic health record (EHR) data for predicting the diagnosis and severity of depression, and response to treatment.Materials and methods We develop regression-based models for predicting depression, its severity, and response to treatment from EHR data, using structured diagnosis and medication codes as well as free-text clinical reports. We used two datasets: 35 000 patients (5000 depressed) from the Palo Alto Medical Foundation and 5651 patients treated for depression from the Group Health Research Institute.Results Our models are able to predict a future diagnosis of depression up to 12 months in advance (area under the receiver operating characteristic curve (AUC) 0.70–0.80). We can differentiate patients with severe baseline depression from those with minimal or mild baseline depression (AUC 0.72). Baseline depression severity was the strongest predictor of treatment response for medication and psychotherapy.Conclusions It is possible to use EHR data to predict a diagnosis of depression up to 12 months in advance and to differentiate between extreme baseline levels of depression. The models use commonly available data on diagnosis, medication, and clinical progress notes, making them easily portable. The ability to automatically determine severity can facilitate assembly of large patient cohorts with similar severity from multiple sites, which may enable elucidation of the moderators of treatment response in the future.}",
    issn = {1067-5027},
    doi = {10.1136/amiajnl-2014-002733},
    url = {https://doi.org/10.1136/amiajnl-2014-002733},
    eprint = {https://academic.oup.com/jamia/article-pdf/21/6/1069/17376116/21-6-1069.pdf},
}


@article{poulin2014predicting,
  title={Predicting the risk of suicide by analyzing the text of clinical notes},
  author={Poulin, Chris and Shiner, Brian and Thompson, Paul and Vepstas, Linas and Young-Xu, Yinong and Goertzel, Benjamin and Watts, Bradley and Flashman, Laura and McAllister, Thomas},
  journal={PloS one},
  volume={9},
  number={1},
  pages={e85733},
  year={2014},
  publisher={Public Library of Science San Francisco, USA}
}

@misc{hirst2016method,
  title={Method and system of longitudinal detection of dementia through lexical and syntactic changes in writing},
  author={Hirst, Graeme John and Jokel, Regina and Lancashire, Dauphin Ian and Le, Xuan D},
  year={2016},
  month=dec # "~6",
  publisher={Google Patents},
  note={US Patent 9,514,281}
}

@inproceedings{coppersmith2015clpsych,
  title={CLPsych 2015 shared task: Depression and PTSD on Twitter},
  author={Coppersmith, Glen and Dredze, Mark and Harman, Craig and Hollingshead, Kristy and Mitchell, Margaret},
  booktitle={Proc. 2nd Workshop on CLPsych},
  year={2015}
}

@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{peng2018large,
  title={Large-scale hierarchical text classification with recursively regularized deep graph-cnn},
  author={Peng, Hao and Li, Jianxin and He, Yu and Liu, Yaopeng and Bao, Mengjiao and Wang, Lihong and Song, Yangqiu and Yang, Qiang},
  booktitle={Proceedings of the 2018 world wide web conference},
  pages={1063--1072},
  year={2018}
}

@article{zhou2015c,
  title={A C-LSTM neural network for text classification},
  author={Zhou, Chunting and Sun, Chonglin and Liu, Zhiyuan and Lau, Francis},
  journal={arXiv preprint arXiv:1511.08630},
  year={2015}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{moreno2011feeling,
  title={Feeling bad on Facebook: Depression disclosures by college students on a social networking site},
  author={Moreno, Megan A and Jelenchick, Lauren A and Egan, Katie G and Cox, Elizabeth and Young, Henry and Gannon, Kerry E and Becker, Tara},
  journal={Depression and anxiety},
  year={2011},
  publisher={Wiley Online Library}
}

@article{li2015attitudes,
  title={Attitudes towards suicide attempts broadcast on social media: an exploratory study of Chinese microblogs},
  author={Li, Ang and Huang, Xiaoxiao and Hao, Bibo and O’Dea, Bridianne and Christensen, Helen and Zhu, Tingshao},
  journal={PeerJ},
  year={2015},
  publisher={PeerJ Inc.}
}

@article{lytle2018suicidal,
  title={Suicidal and help-seeking behaviors among youth in an online lesbian, gay, bisexual, transgender, queer, and questioning social network},
  author={Lytle, Megan C and Silenzio, Vincent MB and Homan, Christopher M and Schneider, Phoenix and Caine, Eric D},
  journal={Journal of homosexuality},
  volume={65},
  number={13},
  pages={1916--1933},
  year={2018},
  publisher={Taylor \& Francis}
}

@inproceedings{park2013perception,
  title={Perception differences between the depressed and non-depressed users in twitter},
  author={Park, Minsu and McDonald, David W and Cha, Meeyoung},
  booktitle={Seventh International AAAI Conference on Weblogs and Social Media},
  year={2013}
}

﻿@Article{info:doi/10.2196/jmir.6895,
author="Mowery, Danielle
and Smith, Hilary
and Cheney, Tyler
and Stoddard, Greg
and Coppersmith, Glen
and Bryan, Craig
and Conway, Mike",
title="Understanding Depressive Symptoms and Psychosocial Stressors on Twitter: A Corpus-Based Study",
journal="J Med Internet Res",
year="2017",
month="Feb",
day="28",
volume="19",
number="2",
pages="e48",
keywords="social media; Twitter messaging; natural language processing; major depressive disorder; data annotation; machine learning",
abstract="Background: With a lifetime prevalence of 16.2{\%}, major depressive disorder is the fifth biggest contributor to the disease burden in the United States. Objective: The aim of this study, building on previous work qualitatively analyzing depression-related Twitter data, was to describe the development of a comprehensive annotation scheme (ie, coding scheme) for manually annotating Twitter data with Diagnostic and Statistical Manual of Mental Disorders, Edition 5 (DSM 5) major depressive symptoms (eg, depressed mood, weight change, psychomotor agitation, or retardation) and Diagnostic and Statistical Manual of Mental Disorders, Edition IV (DSM-IV) psychosocial stressors (eg, educational problems, problems with primary support group, housing problems). Methods: Using this annotation scheme, we developed an annotated corpus, Depressive Symptom and Psychosocial Stressors Acquired Depression, the SAD corpus, consisting of 9300 tweets randomly sampled from the Twitter application programming interface (API) using depression-related keywords (eg, depressed, gloomy, grief). An analysis of our annotated corpus yielded several key results. Results: First, 72.09{\%} (6829/9473) of tweets containing relevant keywords were nonindicative of depressive symptoms (eg, ``we're in for a new economic depression''). Second, the most prevalent symptoms in our dataset were depressed mood and fatigue or loss of energy. Third, less than 2{\%} of tweets contained more than one depression related category (eg, diminished ability to think or concentrate, depressed mood). Finally, we found very high positive correlations between some depression-related symptoms in our annotated dataset (eg, fatigue or loss of energy and educational problems; educational problems and diminished ability to think). Conclusions: We successfully developed an annotation scheme and an annotated corpus, the SAD corpus, consisting of 9300 tweets randomly-selected from the Twitter application programming interface using depression-related keywords. Our analyses suggest that keyword queries alone might not be suitable for public health monitoring because context can change the meaning of keyword in a statement. However, postprocessing approaches could be useful for reducing the noise and improving the signal needed to detect depression symptoms using social media. ",
issn="1438-8871",
doi="10.2196/jmir.6895",
url="http://www.jmir.org/2017/2/e48/",
url="https://doi.org/10.2196/jmir.6895",
url="http://www.ncbi.nlm.nih.gov/pubmed/28246066"
}


@article{li2021relevance,
  title={Relevance-Aware Anomalous Users Detection in Social Network via Graph Neural Network},
  author={Li, Yangyang and Ji, Yipeng and Li, Shaoning and He, Shulong and Cao, Yinhao and Li, Xiong and Shi, Jun and Yang, Yangchao and Liu, Yifeng},
  journal={arXiv preprint arXiv:2104.06095},
  year={2021}
}

@article{yang2021psycholinguistic,
  title={Psycholinguistic Tripartite Graph Network for Personality Detection},
  author={Yang, Tao and Yang, Feifan and Ouyang, Haolan and Quan, Xiaojun},
  journal={arXiv preprint arXiv:2106.04963},
  year={2021}
}


@article{nguyen2014affective,
  title={Affective and content analysis of online depression communities},
  author={Nguyen, Thin and Phung, Dinh and Dao, Bo and Venkatesh, Svetha and Berk, Michael},
  journal={IEEE Transactions on Affective Computing},
  year={2014},
  publisher={IEEE}
}

@misc{gururangan2020dont,
      title={Don't Stop Pretraining: Adapt Language Models to Domains and Tasks}, 
      author={Suchin Gururangan and Ana Marasović and Swabha Swayamdipta and Kyle Lo and Iz Beltagy and Doug Downey and Noah A. Smith},
      year={2020},
      eprint={2004.10964},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{LiArTs13,
  author = {J. Liaperdos and A. Arapoyanni and Y. Tsiatouhas},
  year = 2013,
  month={sep},
  title = {Adjustable {RF} Mixers' Alternate Test Efficiency Optimization by
          the Reduction of Test Observables},
  journal = {{IEEE} Transactions on Computer-Aided Design of Integrated
            Circuits and Systems},
  volume = 32,
  number = 9,
  pages = {1383-1394}
}

@article{Le_2020,
   title={Deep Learning for Source Code Modeling and Generation},
   volume={53},
   ISSN={1557-7341},
   url={http://dx.doi.org/10.1145/3383458},
   DOI={10.1145/3383458},
   number={3},
   journal={ACM Computing Surveys},
   publisher={Association for Computing Machinery (ACM)},
   author={Le, Triet H. M. and Chen, Hao and Babar, Muhammad Ali},
   year={2020},
   month={Jul},
   pages={1–38}
}

@misc{vaswani2017attention,
    title={Attention Is All You Need},
    author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
    year={2017},
    eprint={1706.03762},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@misc{frankle2018lottery,
    title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
    author={Jonathan Frankle and Michael Carbin},
    year={2018},
    eprint={1803.03635},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{rogers2020primer,
      title={A Primer in BERTology: What we know about how BERT works}, 
      author={Anna Rogers and Olga Kovaleva and Anna Rumshisky},
      year={2020},
      eprint={2002.12327},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{liu_inoculation_2019,
	title = {Inoculation by {Fine}-{Tuning}: {A} {Method} for {Analyzing} {Challenge} {Datasets}},
	shorttitle = {Inoculation by {Fine}-{Tuning}},
	url = {http://arxiv.org/abs/1904.02668},
	abstract = {Several datasets have recently been constructed to expose brittleness in models trained on existing benchmarks. While model performance on these challenge datasets is signiﬁcantly lower compared to the original benchmark, it is unclear what particular weaknesses they reveal. For example, a challenge dataset may be difﬁcult because it targets phenomena that current models cannot capture, or because it simply exploits blind spots in a model’s speciﬁc training set. We introduce inoculation by ﬁne-tuning, a new analysis method for studying challenge datasets by exposing models (the metaphorical patient) to a small amount of data from the challenge dataset (a metaphorical pathogen) and assessing how well they can adapt. We apply our method to analyze the NLI “stress tests” (Naik et al., 2018) and the Adversarial SQuAD dataset (Jia and Liang, 2017). We show that after slight exposure, some of these datasets are no longer challenging, while others remain difﬁcult. Our results indicate that failures on challenge datasets may lead to very different conclusions about models, training datasets, and the challenge datasets themselves.},
	language = {en},
	urldate = {2021-04-19},
	journal = {arXiv:1904.02668 [cs]},
	author = {Liu, Nelson F. and Schwartz, Roy and Smith, Noah A.},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.02668},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 9 pages, 4 figures; to appear at NAACL 2019},
	file = {Liu et al. - 2019 - Inoculation by Fine-Tuning A Method for Analyzing.pdf:/Users/StefanosAchlatis/Zotero/storage/UJPCNAS6/Liu et al. - 2019 - Inoculation by Fine-Tuning A Method for Analyzing.pdf:application/pdf},
}

@article{Turing_NLG,
	title = {Turing-NLG: A 17-billion-parameter language model by Microsoft},
	url = {https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/},
	author = {Corby Rosset. et al.},
	year = {2020},

	keywords = {Computer Science - Computation and Language},
}

@article{brown2020language,
    title={Language Models are Few-Shot Learners},
    author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    year={2020},
    eprint={2005.14165},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@misc{strubell2019energy,
      title={Energy and Policy Considerations for Deep Learning in NLP}, 
      author={Emma Strubell and Ananya Ganesh and Andrew McCallum},
      year={2019},
      eprint={1906.02243},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{schwartz2019green,
      title={Green AI}, 
      author={Roy Schwartz and Jesse Dodge and Noah A. Smith and Oren Etzioni},
      year={2019},
      eprint={1907.10597},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}
@misc{Matthias2020,
      title={On the comparability of Pre-trained Language Models}, 
      author={Matthias Aßenmacher and Christian Heumann},
      year={2020},
      eprint={2001.00781},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kao2021berts,
      title={BERT's output layer recognizes all hidden layers? Some Intriguing Phenomena and a simple way to boost BERT}, 
      author={Wei-Tsung Kao and Tsung-Han Wu and Po-Han Chi and Chun-Cheng Hsieh and Hung-Yi Lee},
      year={2021},
      eprint={2001.09309},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{tenney2019bert,
      title={BERT Rediscovers the Classical NLP Pipeline}, 
      author={Ian Tenney and Dipanjan Das and Ellie Pavlick},
      year={2019},
      eprint={1905.05950},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{gordon2020compressing,
    title={Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning},
    author={Mitchell A. Gordon and Kevin Duh and Nicholas Andrews},
    year={2020},
    eprint={2002.08307},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{liu2019linguistic,
    title={Linguistic Knowledge and Transferability of Contextual Representations},
    author={Nelson F. Liu and Matt Gardner and Yonatan Belinkov and Matthew E. Peters and Noah A. Smith},
    year={2019},
    eprint={1903.08855},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@misc{roberts2020knowledge,
    title={How Much Knowledge Can You Pack Into the Parameters of a Language Model?},
    author={Adam Roberts and Colin Raffel and Noam Shazeer},
    year={2020},
    eprint={2002.08910},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{goldberg2019assessing,
      title={Assessing BERT's Syntactic Abilities}, 
      author={Yoav Goldberg},
      year={2019},
      eprint={1901.05287},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{baan2019understanding,
      title={Understanding Multi-Head Attention in Abstractive Summarization}, 
      author={Joris Baan and Maartje ter Hoeve and Marlies van der Wees and Anne Schuth and Maarten de Rijke},
      year={2019},
      eprint={1911.03898},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{farzindar2015natural,
  title={Natural language processing for social media},
  author={Farzindar, Atefeh and Inkpen, Diana},
  journal={Synthesis Lectures on Human Language Technologies},
  volume={8},
  number={2},
  pages={1--166},
  year={2015},
  publisher={Morgan \& Claypool Publishers}
}

@misc{lin2019open,
      title={Open Sesame: Getting Inside BERT's Linguistic Knowledge}, 
      author={Yongjie Lin and Yi Chern Tan and Robert Frank},
      year={2019},
      eprint={1906.01698},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{rogers_primer_2020,
	title = {A {Primer} in {BERTology}: {What} {We} {Know} {About} {How} {BERT} {Works}},
	volume = {8},
	issn = {2307-387X},
	shorttitle = {A {Primer} in {BERTology}},
	url = {https://direct.mit.edu/tacl/article/96482},
	doi = {10.1162/tacl_a_00349},
	abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.},
	language = {en},
	urldate = {2021-04-19},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	month = dec,
	year = {2020},
	pages = {842--866},
	file = {Rogers et al. - 2020 - A Primer in BERTology What We Know About How BERT.pdf:/Users/StefanosAchlatis/Zotero/storage/KNIB52JT/Rogers et al. - 2020 - A Primer in BERTology What We Know About How BERT.pdf:application/pdf},
}

@inproceedings{hao_visualizing_2019,
	address = {Hong Kong, China},
	title = {Visualizing and {Understanding} the {Effectiveness} of {BERT}},
	url = {https://www.aclweb.org/anthology/D19-1424},
	doi = {10.18653/v1/D19-1424},
	abstract = {Language model pre-training, such as BERT, has achieved remarkable results in many NLP tasks. However, it is unclear why the pretraining-then-ﬁne-tuning paradigm can improve performance and generalization capability across different tasks. In this paper, we propose to visualize loss landscapes and optimization trajectories of ﬁne-tuning BERT on speciﬁc datasets. First, we ﬁnd that pre-training reaches a good initial point across downstream tasks, which leads to wider optima and easier optimization compared with training from scratch. We also demonstrate that the ﬁnetuning procedure is robust to overﬁtting, even though BERT is highly over-parameterized for downstream tasks. Second, the visualization results indicate that ﬁne-tuning BERT tends to generalize better because of the ﬂat and wide optima, and the consistency between the training loss surface and the generalization error surface. Third, the lower layers of BERT are more invariant during ﬁne-tuning, which suggests that the layers that are close to input learn more transferable representations of language.},
	language = {en},
	urldate = {2021-04-19},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Hao, Yaru and Dong, Li and Wei, Furu and Xu, Ke},
	year = {2019},
	pages = {4141--4150},
	file = {Hao et al. - 2019 - Visualizing and Understanding the Effectiveness of.pdf:/Users/StefanosAchlatis/Zotero/storage/VVXK7CPS/Hao et al. - 2019 - Visualizing and Understanding the Effectiveness of.pdf:application/pdf},
}

@article{htut_attention_2019,
	title = {Do {Attention} {Heads} in {BERT} {Track} {Syntactic} {Dependencies}?},
	url = {http://arxiv.org/abs/1911.12246},
	abstract = {We investigate the extent to which individual attention heads in pretrained transformer language models, such as BERT and RoBERTa, implicitly capture syntactic dependency relations. We employ two methods—taking the maximum attention weight and computing the maximum spanning tree—to extract implicit dependency relations from the attention weights of each layer/head, and compare them to the ground-truth Universal Dependency (UD) trees. We show that, for some UD relation types, there exist heads that can recover the dependency type signiﬁcantly better than baselines on parsed English text, suggesting that some self-attention heads act as a proxy for syntactic structure. We also analyze BERT ﬁne-tuned on two datasets—the syntaxoriented CoLA and the semantics-oriented MNLI—to investigate whether ﬁne-tuning affects the patterns of their self-attention, but we do not observe substantial differences in the overall dependency relations extracted using our methods. Our results suggest that these models have some specialist attention heads that track individual dependency types, but no generalist head that performs holistic parsing signiﬁcantly better than a trivial baseline, and that analyzing attention weights directly may not reveal much of the syntactic knowledge that BERT-style models are known to learn.},
	language = {en},
	urldate = {2021-04-19},
	journal = {arXiv:1911.12246 [cs]},
	author = {Htut, Phu Mon and Phang, Jason and Bordia, Shikha and Bowman, Samuel R.},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.12246},
	keywords = {Computer Science - Computation and Language},
	file = {Htut et al. - 2019 - Do Attention Heads in BERT Track Syntactic Depende.pdf:/Users/StefanosAchlatis/Zotero/storage/ZZ77AYNN/Htut et al. - 2019 - Do Attention Heads in BERT Track Syntactic Depende.pdf:application/pdf},
}

@article{voita_analyzing_2019,
	title = {Analyzing {Multi}-{Head} {Self}-{Attention}: {Specialized} {Heads} {Do} the {Heavy} {Lifting}, the {Rest} {Can} {Be} {Pruned}},
	shorttitle = {Analyzing {Multi}-{Head} {Self}-{Attention}},
	url = {http://arxiv.org/abs/1905.09418},
	abstract = {Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads in the encoder to the overall performance of the model and analyze the roles played by them. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.},
	language = {en},
	urldate = {2021-04-19},
	journal = {arXiv:1905.09418 [cs]},
	author = {Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
	month = jun,
	year = {2019},
	note = {arXiv: 1905.09418},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ACL 2019 (camera-ready)},
	file = {Voita et al. - 2019 - Analyzing Multi-Head Self-Attention Specialized H.pdf:/Users/StefanosAchlatis/Zotero/storage/WSPDXG5R/Voita et al. - 2019 - Analyzing Multi-Head Self-Attention Specialized H.pdf:application/pdf},
}

@article{fournier_practical_2021,
	title = {A {Practical} {Survey} on {Faster} and {Lighter} {Transformers}},
	url = {http://arxiv.org/abs/2103.14636},
	abstract = {Recurrent neural networks are effective models to process sequences. However, they are unable to learn long-term dependencies because of their inherent sequential nature. As a solution, Vaswani et al. introduced the Transformer, a model solely based on the attention mechanism that is able to relate any two positions of the input sequence, hence modelling arbitrary long dependencies. The Transformer has improved the stateof-the-art across numerous sequence modelling tasks. However, its effectiveness comes at the expense of a quadratic computational and memory complexity with respect to the sequence length, hindering its adoption. Fortunately, the deep learning community has always been interested in improving the models’ efﬁciency, leading to a plethora of solutions such as parameter sharing, pruning, mixed-precision, and knowledge distillation. Recently, researchers have directly addressed the Transformer’s limitation by designing lower-complexity alternatives such as the Longformer, Reformer, Linformer, and Performer. However, due to the wide range of solutions, it has become challenging for the deep learning community to determine which methods to apply in practice to meet the desired trade-off between capacity, computation, and memory. This survey addresses this issue by investigating popular approaches to make the Transformer faster and lighter and by providing a comprehensive explanation of the methods’ strengths, limitations, and underlying assumptions.},
	language = {en},
	urldate = {2021-04-19},
	journal = {arXiv:2103.14636 [cs]},
	author = {Fournier, Quentin and Caron, Gaétan Marceau and Aloise, Daniel},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.14636},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 20 pages, 17 figures},
	file = {Fournier et al. - 2021 - A Practical Survey on Faster and Lighter Transform.pdf:/Users/StefanosAchlatis/Zotero/storage/FRMRA4WD/Fournier et al. - 2021 - A Practical Survey on Faster and Lighter Transform.pdf:application/pdf},
}

@article{fan_reducing_2019,
	title = {Reducing {Transformer} {Depth} on {Demand} with {Structured} {Dropout}},
	url = {http://arxiv.org/abs/1909.11556},
	abstract = {Overparameterized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and question answering. These models contain hundreds of millions of parameters, necessitating a large amount of computation and making them prone to overﬁtting. In this work, we explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efﬁcient pruning at inference time. In particular, we show that it is possible to select sub-networks of any depth from one large network without having to ﬁnetune them and with limited impact on performance. We demonstrate the effectiveness of our approach by improving the state of the art on machine translation, language modeling, summarization, question answering, and language understanding benchmarks. Moreover, we show that our approach leads to small BERT-like models of higher quality compared to training from scratch or using distillation.},
	language = {en},
	urldate = {2021-04-19},
	journal = {arXiv:1909.11556 [cs, stat]},
	author = {Fan, Angela and Grave, Edouard and Joulin, Armand},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.11556},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Fan et al. - 2019 - Reducing Transformer Depth on Demand with Structur.pdf:/Users/StefanosAchlatis/Zotero/storage/SZV287QY/Fan et al. - 2019 - Reducing Transformer Depth on Demand with Structur.pdf:application/pdf},
}

@article{nakkiran_deep_2021,
	title = {The {Deep} {Bootstrap} {Framework}: {Good} {Online} {Learners} are {Good} {Offline} {Generalizers}},
	shorttitle = {The {Deep} {Bootstrap} {Framework}},
	url = {http://arxiv.org/abs/2010.08127},
	abstract = {We propose a new framework for reasoning about generalization in deep learning. The core idea is to couple the Real World, where optimizers take stochastic gradient steps on the empirical loss, to an Ideal World, where optimizers take steps on the population loss. This leads to an alternate decomposition of test error into: (1) the Ideal World test error plus (2) the gap between the two worlds. If the gap (2) is universally small, this reduces the problem of generalization in ofﬂine learning to the problem of optimization in online learning. We then give empirical evidence that this gap between worlds can be small in realistic deep learning settings, in particular supervised image classiﬁcation. For example, CNNs generalize better than MLPs on image distributions in the Real World, but this is “because” they optimize faster on the population loss in the Ideal World. This suggests our framework is a useful tool for understanding generalization in deep learning, and lays a foundation for future research in the area.},
	language = {en},
	urldate = {2021-04-19},
	journal = {arXiv:2010.08127 [cs, math, stat]},
	author = {Nakkiran, Preetum and Neyshabur, Behnam and Sedghi, Hanie},
	month = feb,
	year = {2021},
	note = {arXiv: 2010.08127},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Mathematics - Statistics Theory},
	annote = {Comment: Accepted to ICLR 2021},
	file = {Nakkiran et al. - 2021 - The Deep Bootstrap Framework Good Online Learners.pdf:/Users/StefanosAchlatis/Zotero/storage/WEBJBYTE/Nakkiran et al. - 2021 - The Deep Bootstrap Framework Good Online Learners.pdf:application/pdf},
}

@article{dhamdhere_how_2018,
	title = {How {Important} {Is} a {Neuron}?},
	url = {http://arxiv.org/abs/1805.12233},
	abstract = {The problem of attributing a deep network’s prediction to its input/base features is well-studied (cf. [1]). We introduce the notion of conductance to extend the notion of attribution to the understanding the importance of hidden units.},
	language = {en},
	urldate = {2021-04-19},
	journal = {arXiv:1805.12233 [cs, stat]},
	author = {Dhamdhere, Kedar and Sundararajan, Mukund and Yan, Qiqi},
	month = may,
	year = {2018},
	note = {arXiv: 1805.12233},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: under submission},
	file = {Dhamdhere et al. - 2018 - How Important Is a Neuron.pdf:/Users/StefanosAchlatis/Zotero/storage/JXBADS9Z/Dhamdhere et al. - 2018 - How Important Is a Neuron.pdf:application/pdf},
}

@article{authors_y-drop_nodate,
	title = {Y-{Drop}: {A} {Conductance} based {Dropout} for fully connected layers},
	language = {en},
	author = {Authors, Anonymous},
	pages = {10},
	file = {Authors - Y-Drop A Conductance based Dropout for fully conn.pdf:/Users/StefanosAchlatis/Zotero/storage/BIWB5I6D/Authors - Y-Drop A Conductance based Dropout for fully conn.pdf:application/pdf},
}
@misc{morcos2019ticket,
    title={One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers},
    author={Ari S. Morcos and Haonan Yu and Michela Paganini and Yuandong Tian},
    year={2019},
    eprint={1906.02773},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}
@misc{yu2020playing,
      title={Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP}, 
      author={Haonan Yu and Sergey Edunov and Yuandong Tian and Ari S. Morcos},
      year={2020},
      eprint={1906.02768},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{zhou_deconstructing_2020,
	title = {Deconstructing {Lottery} {Tickets}: {Zeros}, {Signs}, and the {Supermask}},
	shorttitle = {Deconstructing {Lottery} {Tickets}},
	url = {http://arxiv.org/abs/1905.01067},
	abstract = {The recent “Lottery Ticket Hypothesis” paper by Frankle \& Carbin showed that a simple approach to creating sparse networks (keeping the large weights) results in models that are trainable from scratch, but only when starting from the same initial weights. The performance of these networks often exceeds the performance of the non-sparse base model, but for reasons that were not well understood. In this paper we study the three critical components of the Lottery Ticket (LT) algorithm, showing that each may be varied signiﬁcantly without impacting the overall results. Ablating these factors leads to new insights for why LT networks perform as well as they do. We show why setting weights to zero is important, how signs are all you need to make the reinitialized network train, and why masking behaves like training. Finally, we discover the existence of Supermasks, masks that can be applied to an untrained, randomly initialized network to produce a model with performance far better than chance (86\% on MNIST, 41\% on CIFAR-10).},
	language = {en},
	urldate = {2021-04-19},
	journal = {arXiv:1905.01067 [cs, stat]},
	author = {Zhou, Hattie and Lan, Janice and Liu, Rosanne and Yosinski, Jason},
	month = mar,
	year = {2020},
	note = {arXiv: 1905.01067},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	annote = {Comment: NeurIPS 2019 camera ready version},
	file = {Zhou et al. - 2020 - Deconstructing Lottery Tickets Zeros, Signs, and .pdf:/Users/StefanosAchlatis/Zotero/storage/CV6BA2AF/Zhou et al. - 2020 - Deconstructing Lottery Tickets Zeros, Signs, and .pdf:application/pdf},
}

@article{ramanujan_whats_2020,
	title = {What's {Hidden} in a {Randomly} {Weighted} {Neural} {Network}?},
	url = {http://arxiv.org/abs/1911.13299},
	abstract = {Training a neural network is synonymous with learning the values of the weights. In contrast, we demonstrate that randomly weighted neural networks contain subnetworks which achieve impressive performance without ever modifying the weight values. Hidden in a randomly weighted Wide ResNet-50 [32] we ﬁnd a subnetwork (with random weights) that is smaller than, but matches the performance of a ResNet-34 [9] trained on ImageNet [4]. Not only do these “untrained subnetworks” exist, but we provide an algorithm to effectively ﬁnd them. We empirically show that as randomly weighted neural networks with ﬁxed weights grow wider and deeper, an “untrained subnetwork” approaches a network with learned weights in accuracy. Our code and pretrained models are available at: https://github.com/allenai/hidden-networks.},
	language = {en},
	urldate = {2021-04-19},
	journal = {arXiv:1911.13299 [cs]},
	author = {Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
	month = mar,
	year = {2020},
	note = {arXiv: 1911.13299},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to CVPR 2020},
	file = {Ramanujan et al. - 2020 - What's Hidden in a Randomly Weighted Neural Networ.pdf:/Users/StefanosAchlatis/Zotero/storage/7V6AVGDR/Ramanujan et al. - 2020 - What's Hidden in a Randomly Weighted Neural Networ.pdf:application/pdf},
}

@article{zhao_masking_2020,
	title = {Masking as an {Efficient} {Alternative} to {Finetuning} for {Pretrained} {Language} {Models}},
	url = {http://arxiv.org/abs/2004.12406},
	abstract = {We present an efﬁcient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights in lieu of modifying them through ﬁnetuning. Extensive evaluations of masking BERT, RoBERTa, and DistilBERT on eleven diverse NLP tasks show that our masking scheme yields performance comparable to ﬁnetuning, yet has a much smaller memory footprint when several tasks need to be inferred. Intrinsic evaluations show that representations computed by our binary masked language models encode information necessary for solving downstream tasks. Analyzing the loss landscape, we show that masking and ﬁnetuning produce models that reside in minima that can be connected by a line segment with nearly constant test accuracy. This conﬁrms that masking can be utilized as an efﬁcient alternative to ﬁnetuning.},
	language = {en},
	urldate = {2021-04-19},
	journal = {arXiv:2004.12406 [cs]},
	author = {Zhao, Mengjie and Lin, Tao and Mi, Fei and Jaggi, Martin and Schütze, Hinrich},
	month = oct,
	year = {2020},
	note = {arXiv: 2004.12406},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: EMNLP 2020; MZ and TL contribute equally},
	file = {Zhao et al. - 2020 - Masking as an Efficient Alternative to Finetuning .pdf:/Users/StefanosAchlatis/Zotero/storage/LCYEWFCS/Zhao et al. - 2020 - Masking as an Efficient Alternative to Finetuning .pdf:application/pdf},
}
@misc{mallya2018packnet,
      title={PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning}, 
      author={Arun Mallya and Svetlana Lazebnik},
      year={2018},
      eprint={1711.05769},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{mallya2018piggyback,
    title={Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights},
    author={Arun Mallya and Dillon Davis and Svetlana Lazebnik},
    year={2018},
    eprint={1801.06519},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
@inproceedings{kovaleva_revealing_2019,
	address = {Hong Kong, China},
	title = {Revealing the {Dark} {Secrets} of {BERT}},
	url = {https://www.aclweb.org/anthology/D19-1445},
	doi = {10.18653/v1/D19-1445},
	abstract = {BERT-based architectures currently give stateof-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success. In the current work, we focus on the interpretation of selfattention, which is one of the fundamental underlying components of BERT. Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT’s heads. Our ﬁndings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization. While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks. We show that manually disabling attention in certain heads leads to a performance improvement over the regular ﬁne-tuned BERT models.},
	language = {en},
	urldate = {2021-04-19},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and Rumshisky, Anna},
	year = {2019},
	pages = {4364--4373},
	file = {Kovaleva et al. - 2019 - Revealing the Dark Secrets of BERT.pdf:/Users/StefanosAchlatis/Zotero/storage/AK9Z3BIU/Kovaleva et al. - 2019 - Revealing the Dark Secrets of BERT.pdf:application/pdf},
}

@misc{baevski2020wav2vec,
      title={wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations}, 
      author={Alexei Baevski and Henry Zhou and Abdelrahman Mohamed and Michael Auli},
      year={2020},
      eprint={2006.11477},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{michel_are_2019,
	title = {Are {Sixteen} {Heads} {Really} {Better} than {One}?},
	url = {http://arxiv.org/abs/1905.10650},
	abstract = {Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art natural language processing (NLP) models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention “head” potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without signiﬁcantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efﬁciency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention1.},
	language = {en},
	urldate = {2021-04-19},
	journal = {arXiv:1905.10650 [cs]},
	author = {Michel, Paul and Levy, Omer and Neubig, Graham},
	month = nov,
	year = {2019},
	note = {arXiv: 1905.10650},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: NeurIPS 2019},
	file = {Michel et al. - 2019 - Are Sixteen Heads Really Better than One.pdf:/Users/StefanosAchlatis/Zotero/storage/DH8EKXK2/Michel et al. - 2019 - Are Sixteen Heads Really Better than One.pdf:application/pdf},
}

@article{chen_lottery_2020,
	title = {The {Lottery} {Ticket} {Hypothesis} for {Pre}-trained {BERT} {Networks}},
	url = {http://arxiv.org/abs/2007.12223},
	abstract = {In natural language processing (NLP), enormous pre-trained models like BERT have become the standard starting point for training on a range of downstream tasks, and similar trends are emerging in other areas of deep learning. In parallel, work on the lottery ticket hypothesis has shown that models for NLP and computer vision contain smaller matching subnetworks capable of training in isolation to full accuracy and transferring to other tasks. In this work, we combine these observations to assess whether such trainable, transferrable subnetworks exist in pre-trained BERT models. For a range of downstream tasks, we indeed ﬁnd matching subnetworks at 40\% to 90\% sparsity. We ﬁnd these subnetworks at (pre-trained) initialization, a deviation from prior NLP research where they emerge only after some amount of training. Subnetworks found on the masked language modeling task (the same task used to pre-train the model) transfer universally; those found on other tasks transfer in a limited fashion if at all. As large-scale pre-training becomes an increasingly central paradigm in deep learning, our results demonstrate that the main lottery ticket observations remain relevant in this context. Codes available at https://github.com/VITA-Group/BERT-Tickets.},
	language = {en},
	urldate = {2021-04-19},
	journal = {arXiv:2007.12223 [cs, stat]},
	author = {Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Wang, Zhangyang and Carbin, Michael},
	month = oct,
	year = {2020},
	note = {arXiv: 2007.12223},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: NeurIPS 2020},
	file = {Chen et al. - 2020 - The Lottery Ticket Hypothesis for Pre-trained BERT.pdf:/Users/StefanosAchlatis/Zotero/storage/ZU5BGDJ9/Chen et al. - 2020 - The Lottery Ticket Hypothesis for Pre-trained BERT.pdf:application/pdf},
}

@article{lee_snip_2019,
	title = {{SNIP}: {Single}-shot {Network} {Pruning} based on {Connection} {Sensitivity}},
	shorttitle = {{SNIP}},
	url = {http://arxiv.org/abs/1810.02340},
	abstract = {Pruning large neural networks while maintaining their performance is often desirable due to the reduced space and time complexity. In existing methods, pruning is done within an iterative optimization procedure with either heuristically designed pruning schedules or additional hyperparameters, undermining their utility. In this work, we present a new approach that prunes a given network once at initialization prior to training. To achieve this, we introduce a saliency criterion based on connection sensitivity that identiﬁes structurally important connections in the network for the given task. This eliminates the need for both pretraining and the complex pruning schedule while making it robust to architecture variations. After pruning, the sparse network is trained in the standard way. Our method obtains extremely sparse networks with virtually the same accuracy as the reference network on the MNIST, CIFAR-10, and Tiny-ImageNet classiﬁcation tasks and is broadly applicable to various architectures including convolutional, residual and recurrent networks. Unlike existing methods, our approach enables us to demonstrate that the retained connections are indeed relevant to the given task.},
	language = {en},
	urldate = {2021-04-19},
	journal = {arXiv:1810.02340 [cs]},
	author = {Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip H. S.},
	month = feb,
	year = {2019},
	note = {arXiv: 1810.02340},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICLR 2019},
	file = {Lee et al. - 2019 - SNIP Single-shot Network Pruning based on Connect.pdf:/Users/StefanosAchlatis/Zotero/storage/5F2FCMKW/Lee et al. - 2019 - SNIP Single-shot Network Pruning based on Connect.pdf:application/pdf},
}

@misc{gordon2020compressing,
      title={Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning}, 
      author={Mitchell A. Gordon and Kevin Duh and Nicholas Andrews},
      year={2020},
      eprint={2002.08307},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{prasanna_when_2020,
	title = {When {BERT} {Plays} the {Lottery}, {All} {Tickets} {Are} {Winning}},
	url = {http://arxiv.org/abs/2005.00561},
	abstract = {Large Transformer-based models were shown to be reducible to a smaller number of selfattention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis, using both structured and magnitude pruning. For ﬁne-tuned BERT, we show that (a) it is possible to ﬁnd subnetworks achieving performance that is comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse. Strikingly, with structured pruning even the worst possible subnetworks remain highly trainable, indicating that most pre-trained BERT weights are potentially useful. We also study the “good” subnetworks to see if their success can be attributed to superior linguistic knowledge, but ﬁnd them unstable, and not explained by meaningful self-attention patterns.},
	language = {en},
	urldate = {2021-04-19},
	journal = {arXiv:2005.00561 [cs]},
	author = {Prasanna, Sai and Rogers, Anna and Rumshisky, Anna},
	month = oct,
	year = {2020},
	note = {arXiv: 2005.00561},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: EMNLP 2020 camera-ready},
	file = {Prasanna et al. - 2020 - When BERT Plays the Lottery, All Tickets Are Winni.pdf:/Users/StefanosAchlatis/Zotero/storage/5P5VF5E4/Prasanna et al. - 2020 - When BERT Plays the Lottery, All Tickets Are Winni.pdf:application/pdf},
}

@article{serra_overcoming_nodate,
	title = {Overcoming {Catastrophic} {Forgetting} with {Hard} {Attention} to the {Task}},
	abstract = {Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artiﬁcial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks’ information without affecting the current task’s learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting, cutting current rates by 45 to 80\%. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications.},
	language = {en},
	author = {Serrà, Joan and Surís, Dídac and Miron, Marius and Karatzoglou, Alexandros},
	pages = {10},
	file = {Serrà et al. - Overcoming Catastrophic Forgetting with Hard Atten.pdf:/Users/StefanosAchlatis/Zotero/storage/ZRDGDNT4/Serrà et al. - Overcoming Catastrophic Forgetting with Hard Atten.pdf:application/pdf},
}

@misc{rogers2020primer,
      title={A Primer in BERTology: What we know about how BERT works}, 
      author={Anna Rogers and Olga Kovaleva and Anna Rumshisky},
      year={2020},
      eprint={2002.12327},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{louizos2018learning,
      title={Learning Sparse Neural Networks through $L_0$ Regularization}, 
      author={Christos Louizos and Max Welling and Diederik P. Kingma},
      year={2018},
      eprint={1712.01312},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{ramasesh_anatomy_nodate,
	title = {Anatomy of {Catastrophic} {Forgetting}: {Hidden} {Representations} and {Task} {Semantics}},
	abstract = {We investigate how catastrophic forgetting affects internal representations of neural networks and how this interacts with semantics. We ﬁnd that deeper layers are disproportionately the source of forgetting, and relatedly a study of methods to mitigate forgetting illustrates that they act to stabilize deeper representations. These insights enable the development of an analytic argument and empirical picture relating the degree of forgetting to semantic similarity between tasks. Consistent with this picture, we observe maximal forgetting occurs for task sequences with intermediate similarity. Along with experiments in the standard split CIFAR-10 setup, we introduce a novel CIFAR-100 based task approximating realistic input distribution shift.},
	language = {en},
	author = {Ramasesh, Vinay and Dyer, Ethan and Raghu, Maithra},
	pages = {21},
	file = {Ramasesh et al. - Anatomy of Catastrophic Forgetting Hidden Represe.pdf:/Users/StefanosAchlatis/Zotero/storage/ZUH9TI3M/Ramasesh et al. - Anatomy of Catastrophic Forgetting Hidden Represe.pdf:application/pdf},
}

@inbook{7fc5dea30a6e42b2a9e1be2a8fbff85d,
title = "Layer-Wise Relevance Propagation: An Overview",
abstract = "For a machine learning model to generalize well, one needs to ensure that its decisions are supported by meaningful patterns in the input data. A prerequisite is however for the model to be able to explain itself, e.g. by highlighting which input features it uses to support its prediction. Layer-wise Relevance Propagation (LRP) is a technique that brings such explainability and scales to potentially highly complex deep neural networks. It operates by propagating the prediction backward in the neural network, using a set of purposely designed propagation rules. In this chapter, we give a concise introduction to LRP with a discussion of (1) how to implement propagation rules easily and efficiently, (2) how the propagation procedure can be theoretically justified as a {\textquoteleft}deep Taylor decomposition{\textquoteright}, (3) how to choose the propagation rules at each layer to deliver high explanation quality, and (4) how LRP can be extended to handle a variety of machine learning scenarios beyond deep neural networks.",
keywords = "Deep Neural Networks, Deep Taylor Decomposition, Explanations, Layer-wise Relevance Propagation",
author = "Gr{\'e}goire Montavon and Alexander Binder and Sebastian Lapuschkin and Wojciech Samek and M{\"u}ller, {Klaus Robert}",
year = "2019",
doi = "10.1007/978-3-030-28954-6_10",
language = "English",
series = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
publisher = "Springer Verlag",
pages = "193--209",
booktitle = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
}

@inproceedings{clark_bam_2019,
	address = {Florence, Italy},
	title = {{BAM}! {Born}-{Again} {Multi}-{Task} {Networks} for {Natural} {Language} {Understanding}},
	url = {https://www.aclweb.org/anthology/P19-1595},
	doi = {10.18653/v1/P19-1595},
	abstract = {It can be challenging to train multi-task neural networks that outperform or even match their single-task counterparts. To help address this, we propose using knowledge distillation where single-task models teach a multi-task model. We enhance this training with teacher annealing, a novel method that gradually transitions the model from distillation to supervised learning, helping the multi-task model surpass its single-task teachers. We evaluate our approach by multi-task ﬁne-tuning BERT on the GLUE benchmark. Our method consistently improves over standard single-task and multi-task training.},
	language = {en},
	urldate = {2021-04-19},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Clark, Kevin and Luong, Minh-Thang and Khandelwal, Urvashi and Manning, Christopher D. and Le, Quoc V.},
	year = {2019},
	pages = {5931--5937},
	file = {Clark et al. - 2019 - BAM! Born-Again Multi-Task Networks for Natural La.pdf:/Users/StefanosAchlatis/Zotero/storage/LA9NWZTK/Clark et al. - 2019 - BAM! Born-Again Multi-Task Networks for Natural La.pdf:application/pdf},
}

@article{mensink_factors_2021,
	title = {Factors of {Influence} for {Transfer} {Learning} across {Diverse} {Appearance} {Domains} and {Task} {Types}},
	url = {http://arxiv.org/abs/2103.13318},
	abstract = {Transfer learning enables to re-use knowledge learned on a source task to help learning a target task. A simple form of transfer learning is common in current state-of-the-art computer vision models, i.e. pre-training a model for image classiﬁcation on the ILSVRC dataset, and then ﬁne-tune on any target task. However, previous systematic studies of transfer learning have been limited and the circumstances in which it is expected to work are not fully understood. In this paper we carry out an extensive experimental exploration of transfer learning across vastly different image domains (consumer photos, autonomous driving, aerial imagery, underwater, indoor scenes, synthetic, close-ups) and task types (semantic segmentation, object detection, depth estimation, keypoint detection). Importantly, these are all complex, structured output tasks types relevant to modern computer vision applications. In total we carry out over 1200 transfer experiments, including many where the source and target come from different image domains, task types, or both. We systematically analyze these experiments to understand the impact of image domain, task type, and dataset size on transfer learning performance. Our study leads to several insights and concrete recommendations for practitioners.},
	language = {en},
	urldate = {2021-04-19},
	journal = {arXiv:2103.13318 [cs]},
	author = {Mensink, Thomas and Uijlings, Jasper and Kuznetsova, Alina and Gygli, Michael and Ferrari, Vittorio},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.13318},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: submitted to TPAMI},
	file = {Mensink et al. - 2021 - Factors of Influence for Transfer Learning across .pdf:/Users/StefanosAchlatis/Zotero/storage/HBJJQ89V/Mensink et al. - 2021 - Factors of Influence for Transfer Learning across .pdf:application/pdf},
}

@article{naacl-hlt_udalm_nodate,
	title = {{UDALM}: {Unsupervised} {Domain} {Adaptation} through {Language} {Modeling}},
	language = {en},
	author = {Naacl-Hlt, Anonymous},
	pages = {11},
	file = {Naacl-Hlt - UDALM Unsupervised Domain Adaptation through Lang.pdf:/Users/StefanosAchlatis/Zotero/storage/KYA4YX3N/Naacl-Hlt - UDALM Unsupervised Domain Adaptation through Lang.pdf:application/pdf},
}

@inproceedings{voita_analyzing_2019-1,
	address = {Florence, Italy},
	title = {Analyzing {Multi}-{Head} {Self}-{Attention}: {Specialized} {Heads} {Do} the {Heavy} {Lifting}, the {Rest} {Can} {Be} {Pruned}},
	shorttitle = {Analyzing {Multi}-{Head} {Self}-{Attention}},
	url = {https://www.aclweb.org/anthology/P19-1580},
	doi = {10.18653/v1/P19-1580},
	language = {en},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
	year = {2019},
	pages = {5797--5808},
	file = {Voita et al. - 2019 - Analyzing Multi-Head Self-Attention Specialized H.pdf:/Users/StefanosAchlatis/Zotero/storage/VW24ZDS3/Voita et al. - 2019 - Analyzing Multi-Head Self-Attention Specialized H.pdf:application/pdf},
}

@article{mccarley_structured_2021,
	title = {Structured {Pruning} of a {BERT}-based {Question} {Answering} {Model}},
	url = {http://arxiv.org/abs/1910.06360},
	abstract = {The recent trend in industry-setting Natural Language Processing (NLP) research has been to operate large pretrained language models like BERT under strict computational limits. While most model compression work has focused on “distilling" a general-purpose language representation using expensive pretraining distillation, less attention has been paid to creating smaller task-speciﬁc language representations which, arguably, are more useful in an industry setting. In this paper, we investigate compressing BERT- and RoBERTabased question answering systems by structured pruning of parameters from the underlying transformer model. We ﬁnd that an inexpensive combination of task-speciﬁc structured pruning and task-speciﬁc distillation, without the expense of pretraining distillation, yields highly-performing models across a range of speed/accuracy tradeoff operating points. We start from existing full-size models trained for SQuAD 2.0 or Natural Questions and introduce gates that allow selected parts of transformers to be individually eliminated. Speciﬁcally, we investigate (1) structured pruning to reduce the number of parameters in each transformer layer, (2) applicability to both BERT- and RoBERTa-based models, (3) applicability to both SQuAD 2.0 and Natural Questions, and (4) combining structured pruning with distillation. We achieve a neardoubling of inference speed with less than a 0.5 F1-point loss in short answer accuracy on Natural Questions.},
	language = {en},
	urldate = {2021-04-20},
	journal = {arXiv:1910.06360 [cs]},
	author = {McCarley, J. S. and Chakravarti, Rishav and Sil, Avirup},
	month = apr,
	year = {2021},
	note = {arXiv: 1910.06360},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {McCarley et al. - 2021 - Structured Pruning of a BERT-based Question Answer.pdf:/Users/StefanosAchlatis/Zotero/storage/YHJCMG3I/McCarley et al. - 2021 - Structured Pruning of a BERT-based Question Answer.pdf:application/pdf},
}

@article{lan_albert_2020,
	title = {{ALBERT}: {A} {LITE} {BERT} {FOR} {SELF}-{SUPERVISED} {LEARNING} {OF} {LANGUAGE} {REPRESENTATIONS}},
	abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameterreduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
	language = {en},
	author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	year = {2020},
	pages = {17},
	file = {Lan et al. - 2020 - ALBERT A LITE BERT FOR SELF-SUPERVISED LEARNING O.pdf:/Users/StefanosAchlatis/Zotero/storage/SX4SXL9U/Lan et al. - 2020 - ALBERT A LITE BERT FOR SELF-SUPERVISED LEARNING O.pdf:application/pdf},
}

@inproceedings{jawahar_what_2019,
	address = {Florence, Italy},
	title = {What {Does} {BERT} {Learn} about the {Structure} of {Language}?},
	url = {https://www.aclweb.org/anthology/P19-1356},
	doi = {10.18653/v1/P19-1356},
	abstract = {BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. We ﬁrst show that BERT’s phrasal representation captures phrase-level information in the lower layers. We also show that BERT’s intermediate layers encode a rich hierarchy of linguistic information, with surface features at the bottom, syntactic features in the middle and semantic features at the top. BERT turns out to require deeper layers when long-distance dependency information is required, e.g. to track subjectverb agreement. Finally, we show that BERT representations capture linguistic information in a compositional way that mimics classical, tree-like structures.},
	language = {en},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Jawahar, Ganesh and Sagot, Benoît and Seddah, Djamé},
	year = {2019},
	pages = {3651--3657},
	file = {Jawahar et al. - 2019 - What Does BERT Learn about the Structure of Langua.pdf:/Users/StefanosAchlatis/Zotero/storage/2JF2ZP4P/Jawahar et al. - 2019 - What Does BERT Learn about the Structure of Langua.pdf:application/pdf},
}

@misc{ribeiro2016why,
      title={"Why Should I Trust You?": Explaining the Predictions of Any Classifier}, 
      author={Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
      year={2016},
      eprint={1602.04938},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{ribeiro_beyond_2020,
	address = {Online},
	title = {Beyond {Accuracy}: {Behavioral} {Testing} of {NLP} {Models} with {CheckList}},
	shorttitle = {Beyond {Accuracy}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.442},
	doi = {10.18653/v1/2020.acl-main.442},
	abstract = {Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on speciﬁc behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a taskagnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.},
	language = {en},
	urldate = {2021-04-20},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
	year = {2020},
	pages = {4902--4912},
	file = {Ribeiro et al. - 2020 - Beyond Accuracy Behavioral Testing of NLP Models .pdf:/Users/StefanosAchlatis/Zotero/storage/6LGFQEB3/Ribeiro et al. - 2020 - Beyond Accuracy Behavioral Testing of NLP Models .pdf:application/pdf},
}

@article{mallya_piggyback_2018,
	title = {Piggyback: {Adapting} a {Single} {Network} to {Multiple} {Tasks} by {Learning} to {Mask} {Weights}},
	shorttitle = {Piggyback},
	url = {http://arxiv.org/abs/1801.06519},
	abstract = {This work presents a method for adapting a single, ﬁxed deep neural network to multiple tasks without aﬀecting performance on already learned tasks. By building upon ideas from network quantization and pruning, we learn binary masks that “piggyback” on an existing network, or are applied to unmodiﬁed weights of that network to provide good performance on a new task. These masks are learned in an end-toend diﬀerentiable fashion, and incur a low overhead of 1 bit per network parameter, per task. Even though the underlying network is ﬁxed, the ability to mask individual weights allows for the learning of a large number of ﬁlters. We show performance comparable to dedicated ﬁne-tuned networks for a variety of classiﬁcation tasks, including those with large domain shifts from the initial task (ImageNet), and a variety of network architectures. Unlike prior work, we do not suﬀer from catastrophic forgetting or competition between tasks, and our performance is agnostic to task ordering.},
	language = {en},
	urldate = {2021-04-21},
	journal = {arXiv:1801.06519 [cs]},
	author = {Mallya, Arun and Davis, Dillon and Lazebnik, Svetlana},
	month = mar,
	year = {2018},
	note = {arXiv: 1801.06519},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Mallya et al. - 2018 - Piggyback Adapting a Single Network to Multiple T.pdf:/Users/StefanosAchlatis/Zotero/storage/3AE9BD3P/Mallya et al. - 2018 - Piggyback Adapting a Single Network to Multiple T.pdf:application/pdf},
}

@article{mallya_packnet_nodate,
	title = {{PackNet}: {Adding} {Multiple} {Tasks} to a {Single} {Network} by {Iterative} {Pruning}},
	abstract = {This paper presents a method for adding multiple tasks to a single deep neural network while avoiding catastrophic forgetting. Inspired by network pruning techniques, we exploit redundancies in large deep networks to free up parameters that can then be employed to learn new tasks. By performing iterative pruning and network re-training, we are able to sequentially “pack” multiple tasks into a single network while ensuring minimal drop in performance and minimal storage overhead. Unlike prior work that uses proxy losses to maintain accuracy on older tasks, we always optimize for the task at hand. We perform extensive experiments on a variety of network architectures and large-scale datasets, and observe much better robustness against catastrophic forgetting than prior work. In particular, we are able to add three ﬁne-grained classiﬁcation tasks to a single ImageNettrained VGG-16 network and achieve accuracies close to those of separately trained networks for each task.},
	language = {en},
	author = {Mallya, Arun and Lazebnik, Svetlana},
	pages = {9},
	file = {Mallya and Lazebnik - PackNet Adding Multiple Tasks to a Single Network.pdf:/Users/StefanosAchlatis/Zotero/storage/A6YWHY68/Mallya and Lazebnik - PackNet Adding Multiple Tasks to a Single Network.pdf:application/pdf},
}

@book{interpreting_explainable_2019,
	title = {Explainable {AI}: {Interpreting}, {Explaining} and {Visualizing} {Deep} {Learning}},
	isbn = {978-3-030-28953-9},
	shorttitle = {Explainable {AI}},
	abstract = {The development of "intelligent" systems that can take decisions and perform autonomously might lead to faster and more consistent decisions. A limiting factor for a broader adoption of AI technology is the inherent risks that come with giving up human control and oversight to "intelligent" machines. Forsensitive tasks involving critical infrastructures and affecting human well-being or health, it is crucial to limit the possibility of improper, non-robust and unsafe decisions and actions. Before deploying an AI system, we see a strong need to validate its behavior, and thus establish guarantees that it will continue to perform as expected when deployed in a real-world environment. In pursuit of that objective, ways for humans to verify the agreement between the AI decision structure and their own ground-truth knowledge have been explored. Explainable AI (XAI) has developed as a subfield of AI, focused on exposing complex AI models to humans in a systematic and interpretable manner. The 22 chapters included in this book provide a timely snapshot of algorithms, theory, and applications of interpretable and explainable AI and AI techniques that have been proposed recently reflecting the current discourse in this field and providing directions of future development. The book is organized in six parts: towards AI transparency; methods for interpreting AI systems; explaining the decisions of AI systems; evaluating interpretability and explanations; applications of explainable AI; and software for explainable AI. --.},
	language = {en},
	editor = {Interpreting, Explaining {and} Visualizing Deep Learning (Workshop) and Samek, Wojciech and {NIPS (Conference)}},
	year = {2019},
	note = {OCLC: 1121483582},
	file = {Interpreting et al. - 2019 - Explainable AI Interpreting, Explaining and Visua.pdf:/Users/StefanosAchlatis/Zotero/storage/GTSVDWN8/Interpreting et al. - 2019 - Explainable AI Interpreting, Explaining and Visua.pdf:application/pdf},
}

@article{wu_explaining_2021,
	title = {On {Explaining} {Your} {Explanations} of {BERT}: {An} {Empirical} {Study} with {Sequence} {Classification}},
	shorttitle = {On {Explaining} {Your} {Explanations} of {BERT}},
	url = {http://arxiv.org/abs/2101.00196},
	abstract = {BERT, as one of the pretrianed language models, attracts the most attention in recent years for creating new benchmarks across GLUE tasks via ﬁne-tuning. One pressing issue is to open up the blackbox and explain the decision makings of BERT. A number of attribution techniques have been proposed to explain BERT models, but are often limited to sequence to sequence tasks. In this paper, we adapt existing attribution methods on explaining decision makings of BERT in sequence classiﬁcation tasks. We conduct extensive analyses of four existing attribution methods by applying them to four different datasets in sentiment analysis. We compare the reliability and robustness of each method via various ablation studies. Furthermore, we test whether attribution methods explain generalized semantics across semantically similar tasks. Our work provides solid guidance for using attribution methods to explain decision makings of BERT for downstream classiﬁcation tasks.},
	language = {en},
	urldate = {2021-04-22},
	journal = {arXiv:2101.00196 [cs]},
	author = {Wu, Zhengxuan and Ong, Desmond C.},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.00196},
	keywords = {Computer Science - Computation and Language},
	file = {Wu and Ong - 2021 - On Explaining Your Explanations of BERT An Empiri.pdf:/Users/StefanosAchlatis/Zotero/storage/CT94CHUW/Wu and Ong - 2021 - On Explaining Your Explanations of BERT An Empiri.pdf:application/pdf},
}

@inproceedings{ding_visualizing_2017,
	address = {Vancouver, Canada},
	title = {Visualizing and {Understanding} {Neural} {Machine} {Translation}},
	url = {http://aclweb.org/anthology/P17-1106},
	doi = {10.18653/v1/P17-1106},
	abstract = {While neural machine translation (NMT) has made remarkable progress in recent years, it is hard to interpret its internal workings due to the continuous representations and non-linearity of neural networks. In this work, we propose to use layer-wise relevance propagation (LRP) to compute the contribution of each contextual word to arbitrary hidden states in the attention-based encoderdecoder framework. We show that visualization with LRP helps to interpret the internal workings of NMT and analyze translation errors.},
	language = {en},
	urldate = {2021-04-22},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for           {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ding, Yanzhuo and Liu, Yang and Luan, Huanbo and Sun, Maosong},
	year = {2017},
	pages = {1150--1159},
	file = {Ding et al. - 2017 - Visualizing and Understanding Neural Machine Trans.pdf:/Users/StefanosAchlatis/Zotero/storage/VFVP8DN6/Ding et al. - 2017 - Visualizing and Understanding Neural Machine Trans.pdf:application/pdf},
}

@misc{simonyan2015deep,
      title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
      author={Karen Simonyan and Andrew Zisserman},
      year={2015},
      eprint={1409.1556},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{bach_pixel-wise_2015,
	title = {On {Pixel}-{Wise} {Explanations} for {Non}-{Linear} {Classifier} {Decisions} by {Layer}-{Wise} {Relevance} {Propagation}},
	volume = {10},
	issn = {1932-6203},
	url = {https://dx.plos.org/10.1371/journal.pone.0130140},
	doi = {10.1371/journal.pone.0130140},
	abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
	language = {en},
	number = {7},
	urldate = {2021-04-22},
	journal = {PLOS ONE},
	author = {Bach, Sebastian and Binder, Alexander and Montavon, Grégoire and Klauschen, Frederick and Müller, Klaus-Robert and Samek, Wojciech},
	editor = {Suarez, Oscar Deniz},
	month = jul,
	year = {2015},
	pages = {e0130140},
	file = {Bach et al. - 2015 - On Pixel-Wise Explanations for Non-Linear Classifi.pdf:/Users/StefanosAchlatis/Zotero/storage/4GSK3ELW/Bach et al. - 2015 - On Pixel-Wise Explanations for Non-Linear Classifi.pdf:application/pdf},
}

@article{htut_attention_2019-1,
	title = {Do {Attention} {Heads} in {BERT} {Track} {Syntactic} {Dependencies}?},
	url = {http://arxiv.org/abs/1911.12246},
	abstract = {We investigate the extent to which individual attention heads in pretrained transformer language models, such as BERT and RoBERTa, implicitly capture syntactic dependency relations. We employ two methods—taking the maximum attention weight and computing the maximum spanning tree—to extract implicit dependency relations from the attention weights of each layer/head, and compare them to the ground-truth Universal Dependency (UD) trees. We show that, for some UD relation types, there exist heads that can recover the dependency type signiﬁcantly better than baselines on parsed English text, suggesting that some self-attention heads act as a proxy for syntactic structure. We also analyze BERT ﬁne-tuned on two datasets—the syntaxoriented CoLA and the semantics-oriented MNLI—to investigate whether ﬁne-tuning affects the patterns of their self-attention, but we do not observe substantial differences in the overall dependency relations extracted using our methods. Our results suggest that these models have some specialist attention heads that track individual dependency types, but no generalist head that performs holistic parsing signiﬁcantly better than a trivial baseline, and that analyzing attention weights directly may not reveal much of the syntactic knowledge that BERT-style models are known to learn.},
	language = {en},
	urldate = {2021-04-23},
	journal = {arXiv:1911.12246 [cs]},
	author = {Htut, Phu Mon and Phang, Jason and Bordia, Shikha and Bowman, Samuel R.},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.12246},
	keywords = {Computer Science - Computation and Language},
	file = {Htut et al. - 2019 - Do Attention Heads in BERT Track Syntactic Depende.pdf:/Users/StefanosAchlatis/Zotero/storage/6WJS7TF4/Htut et al. - 2019 - Do Attention Heads in BERT Track Syntactic Depende.pdf:application/pdf},
}

@article{wu_explaining_2021-1,
	title = {On {Explaining} {Your} {Explanations} of {BERT}: {An} {Empirical} {Study} with {Sequence} {Classification}},
	shorttitle = {On {Explaining} {Your} {Explanations} of {BERT}},
	url = {http://arxiv.org/abs/2101.00196},
	abstract = {BERT, as one of the pretrianed language models, attracts the most attention in recent years for creating new benchmarks across GLUE tasks via ﬁne-tuning. One pressing issue is to open up the blackbox and explain the decision makings of BERT. A number of attribution techniques have been proposed to explain BERT models, but are often limited to sequence to sequence tasks. In this paper, we adapt existing attribution methods on explaining decision makings of BERT in sequence classiﬁcation tasks. We conduct extensive analyses of four existing attribution methods by applying them to four different datasets in sentiment analysis. We compare the reliability and robustness of each method via various ablation studies. Furthermore, we test whether attribution methods explain generalized semantics across semantically similar tasks. Our work provides solid guidance for using attribution methods to explain decision makings of BERT for downstream classiﬁcation tasks.},
	language = {en},
	urldate = {2021-04-23},
	journal = {arXiv:2101.00196 [cs]},
	author = {Wu, Zhengxuan and Ong, Desmond C.},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.00196},
	keywords = {Computer Science - Computation and Language},
	file = {Wu and Ong - 2021 - On Explaining Your Explanations of BERT An Empiri.pdf:/Users/StefanosAchlatis/Zotero/storage/5TLUKIMS/Wu and Ong - 2021 - On Explaining Your Explanations of BERT An Empiri.pdf:application/pdf},
}

@inproceedings{masana_domain-adaptive_2017,
	address = {Venice},
	title = {Domain-{Adaptive} {Deep} {Network} {Compression}},
	isbn = {978-1-5386-1032-9},
	url = {https://ieeexplore.ieee.org/document/8237722/},
	doi = {10.1109/ICCV.2017.460},
	abstract = {Deep Neural Networks trained on large datasets can be easily transferred to new domains with far fewer labeled examples by a process called ﬁne-tuning. This has the advantage that representations learned in the large source domain can be exploited on smaller target domains. However, networks designed to be optimal for the source task are often prohibitively large for the target task. In this work we address the compression of networks after domain transfer. We focus on compression algorithms based on low-rank matrix decomposition. Existing methods base compression solely on learned network weights and ignore the statistics of network activations. We show that domain transfer leads to large shifts in network activations and that it is desirable to take this into account when compressing. We demonstrate that considering activation statistics when compressing weights leads to a rank-constrained regression problem with a closed-form solution. Because our method takes into account the target domain, it can more optimally remove the redundancy in the weights. Experiments show that our Domain Adaptive Low Rank (DALR) method signiﬁcantly outperforms existing low-rank compression techniques. With our approach, the fc6 layer of VGG19 can be compressed more than 4x more than using truncated SVD alone – with only a minor or no loss in accuracy. When applied to domain-transferred networks it allows for compression down to only 5-20\% of the original number of parameters with only a minor drop in performance.},
	language = {en},
	urldate = {2021-04-23},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Masana, Marc and van de Weijer, Joost and Herranz, Luis and Bagdanov, Andrew D. and Alvarez, Jose M.},
	month = oct,
	year = {2017},
	pages = {4299--4307},
	file = {Masana et al. - 2017 - Domain-Adaptive Deep Network Compression.pdf:/Users/StefanosAchlatis/Zotero/storage/X3LKRVGU/Masana et al. - 2017 - Domain-Adaptive Deep Network Compression.pdf:application/pdf},
}

@article{clark_what_2019,
	title = {What {Does} {BERT} {Look} {At}? {An} {Analysis} of {BERT}'s {Attention}},
	shorttitle = {What {Does} {BERT} {Look} {At}?},
	url = {http://arxiv.org/abs/1906.04341},
	abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classiﬁers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT’s attention heads exhibit patterns such as attending to delimiter tokens, speciﬁc positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we ﬁnd heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classiﬁer and use it to further demonstrate that substantial syntactic information is captured in BERT’s attention.},
	language = {en},
	urldate = {2021-04-29},
	journal = {arXiv:1906.04341 [cs]},
	author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.04341},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: BlackBoxNLP 2019},
	file = {Clark et al. - 2019 - What Does BERT Look At An Analysis of BERT's Atte.pdf:/Users/StefanosAchlatis/Zotero/storage/7Z6XIGGY/Clark et al. - 2019 - What Does BERT Look At An Analysis of BERT's Atte.pdf:application/pdf},
}

@article{sanh_movement_2020,
	title = {Movement {Pruning}: {Adaptive} {Sparsity} by {Fine}-{Tuning}},
	shorttitle = {Movement {Pruning}},
	url = {http://arxiv.org/abs/2005.07683},
	abstract = {Magnitude pruning is a widely used strategy for reducing model size in pure supervised learning; however, it is less effective in the transfer learning regime that has become standard for state-of-the-art natural language processing applications. We propose the use of movement pruning, a simple, deterministic ﬁrst-order weight pruning method that is more adaptive to pretrained model ﬁne-tuning. We give mathematical foundations to the method and compare it to existing zeroth- and ﬁrst-order pruning methods. Experiments show that when pruning large pretrained language models, movement pruning shows signiﬁcant improvements in highsparsity regimes. When combined with distillation, the approach achieves minimal accuracy loss with down to only 3\% of the model parameters.},
	language = {en},
	urldate = {2021-05-01},
	journal = {arXiv:2005.07683 [cs]},
	author = {Sanh, Victor and Wolf, Thomas and Rush, Alexander M.},
	month = oct,
	year = {2020},
	note = {arXiv: 2005.07683},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 14 pages, 6 figures, 3 tables. Published at NeurIPS2020. Code: {\textbackslash}url\{huggingface.co/mvp\}},
	file = {Sanh et al. - 2020 - Movement Pruning Adaptive Sparsity by Fine-Tuning.pdf:/Users/StefanosAchlatis/Zotero/storage/SBG93NKC/Sanh et al. - 2020 - Movement Pruning Adaptive Sparsity by Fine-Tuning.pdf:application/pdf},
}

@article{zhang_know_2020,
	title = {Know {What} {You} {Don}'t {Need}: {Single}-{Shot} {Meta}-{Pruning} for {Attention} {Heads}},
	shorttitle = {Know {What} {You} {Don}'t {Need}},
	url = {http://arxiv.org/abs/2011.03770},
	abstract = {Deep pre-trained Transformer models have achieved state-of-the-art results over a variety of natural language processing (NLP) tasks. By learning rich language knowledge with millions of parameters, these models are usually overparameterized and signiﬁcantly increase the computational overhead in applications. It is intuitive to address this issue by model compression. In this work, we propose a method, called Single-Shot Meta-Pruning, to compress deep pre-trained Transformers before ﬁne-tuning. Speciﬁcally, we focus on pruning unnecessary attention heads adaptively for different downstream tasks. To measure the informativeness of attention heads, we train our Single-Shot Meta-Pruner (SMP) with a meta-learning paradigm aiming to maintain the distribution of text representations after pruning. Compared with existing compression methods for pre-trained models, our method can reduce the overhead of both ﬁne-tuning and inference. Experimental results show that our pruner can selectively prune 50\% of attention heads with little impact on the performance on downstream tasks and even provide better text representations. The source code will be released in the future.},
	language = {en},
	urldate = {2021-05-01},
	journal = {arXiv:2011.03770 [cs]},
	author = {Zhang, Zhengyan and Qi, Fanchao and Liu, Zhiyuan and Liu, Qun and Sun, Maosong},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.03770},
	keywords = {Computer Science - Computation and Language},
	file = {Zhang et al. - 2020 - Know What You Don't Need Single-Shot Meta-Pruning.pdf:/Users/StefanosAchlatis/Zotero/storage/7CXAW4NK/Zhang et al. - 2020 - Know What You Don't Need Single-Shot Meta-Pruning.pdf:application/pdf},
}

@article{lundberg_unified_2017,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	url = {http://arxiv.org/abs/1705.07874},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction’s accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a uniﬁed framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identiﬁcation of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class uniﬁes six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this uniﬁcation, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	language = {en},
	urldate = {2021-05-02},
	journal = {arXiv:1705.07874 [cs, stat]},
	author = {Lundberg, Scott and Lee, Su-In},
	month = nov,
	year = {2017},
	note = {arXiv: 1705.07874},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	annote = {Comment: To appear in NIPS 2017},
	file = {Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf:/Users/StefanosAchlatis/Zotero/storage/2RJPWWXM/Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf:application/pdf},
}

@article{kobayashi_attention_2020,
	title = {Attention is {Not} {Only} a {Weight}: {Analyzing} {Transformers} with {Vector} {Norms}},
	shorttitle = {Attention is {Not} {Only} a {Weight}},
	url = {http://arxiv.org/abs/2004.10102},
	abstract = {Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and speciﬁc linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The ﬁndings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These ﬁndings provide insights into the inner workings of Transformers.},
	language = {en},
	urldate = {2021-05-03},
	journal = {arXiv:2004.10102 [cs]},
	author = {Kobayashi, Goro and Kuribayashi, Tatsuki and Yokoi, Sho and Inui, Kentaro},
	month = oct,
	year = {2020},
	note = {arXiv: 2004.10102},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 19 pages, accepted by EMNLP 2020},
	file = {Kobayashi et al. - 2020 - Attention is Not Only a Weight Analyzing Transfor.pdf:/Users/StefanosAchlatis/Zotero/storage/FQ8JP3LU/Kobayashi et al. - 2020 - Attention is Not Only a Weight Analyzing Transfor.pdf:application/pdf},
}

@article{wortsman_supermasks_2020,
	title = {Supermasks in {Superposition}},
	url = {http://arxiv.org/abs/2006.14769},
	abstract = {We present the Supermasks in Superposition (SupSup) model, capable of sequentially learning thousands of tasks without catastrophic forgetting. Our approach uses a randomly initialized, ﬁxed base network and for each task ﬁnds a subnetwork (supermask) that achieves good performance. If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to ﬁnd a linear superposition of learned supermasks which minimizes the output entropy. In practice we ﬁnd that a single gradient step is often sufﬁcient to identify the correct mask, even among 2500 tasks. We also showcase two promising extensions. First, SupSup models can be trained entirely without task identity information, as they may detect when they are uncertain about new data and allocate an additional supermask for the new training distribution. Finally the entire, growing set of supermasks can be stored in a constant-sized reservoir by implicitly storing them as attractors in a ﬁxed-sized Hopﬁeld network.},
	language = {en},
	urldate = {2021-05-03},
	journal = {arXiv:2006.14769 [cs, stat]},
	author = {Wortsman, Mitchell and Ramanujan, Vivek and Liu, Rosanne and Kembhavi, Aniruddha and Rastegari, Mohammad and Yosinski, Jason and Farhadi, Ali},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.14769},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	annote = {Comment: NeurIPS 2020 Camera Ready},
	file = {Wortsman et al. - 2020 - Supermasks in Superposition.pdf:/Users/StefanosAchlatis/Zotero/storage/487D3NZQ/Wortsman et al. - 2020 - Supermasks in Superposition.pdf:application/pdf},
}

@article{tay_are_2021,
	title = {Are {Pre}-trained {Convolutions} {Better} than {Pre}-trained {Transformers}?},
	url = {http://arxiv.org/abs/2105.03322},
	abstract = {In the era of pre-trained language models, Transformers are the de facto choice of model architectures. While recent research has shown promise in entirely convolutional, or CNN, architectures, they have not been explored using the pre-train-ﬁne-tune paradigm. In the context of language models, are convolutional models competitive to Transformers when pre-trained? This paper investigates this research question and presents several interesting ﬁndings. Across an extensive set of experiments on 8 datasets/tasks, we ﬁnd that CNN-based pre-trained models are competitive and outperform their Transformer counterpart in certain scenarios, albeit with caveats. Overall, the ﬁndings outlined in this paper suggest that conﬂating pre-training and architectural advances is misguided and that both advances should be considered independently. We believe our research paves the way for a healthy amount of optimism in alternative architectures.},
	language = {en},
	urldate = {2021-05-10},
	journal = {arXiv:2105.03322 [cs]},
	author = {Tay, Yi and Dehghani, Mostafa and Gupta, Jai and Bahri, Dara and Aribandi, Vamsi and Qin, Zhen and Metzler, Donald},
	month = may,
	year = {2021},
	note = {arXiv: 2105.03322},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted to ACL 2021},
	file = {Tay et al. - 2021 - Are Pre-trained Convolutions Better than Pre-train.pdf:/Users/StefanosAchlatis/Zotero/storage/2TR8Y334/Tay et al. - 2021 - Are Pre-trained Convolutions Better than Pre-train.pdf:application/pdf},
}

@article{feng_survey_2021,
	title = {A {Survey} of {Data} {Augmentation} {Approaches} for {NLP}},
	url = {http://arxiv.org/abs/2105.03075},
	abstract = {Data augmentation has recently seen increased interest in NLP due to more work in lowresource domains, new tasks, and the popularity of large-scale neural networks that require large amounts of training data. Despite this recent upsurge, this area is still relatively underexplored, perhaps due to the challenges posed by the discrete nature of language data. In this paper, we present a comprehensive and unifying survey of data augmentation for NLP by summarizing the literature in a structured manner. We ﬁrst introduce and motivate data augmentation for NLP, and then discuss major methodologically representative approaches. Next, we highlight techniques that are used for popular NLP applications and tasks. We conclude by outlining current challenges and directions for future research. Overall, our paper aims to clarify the landscape of existing literature in data augmentation for NLP and motivate additional work in this area.},
	language = {en},
	urldate = {2021-05-10},
	journal = {arXiv:2105.03075 [cs]},
	author = {Feng, Steven Y. and Gangal, Varun and Wei, Jason and Chandar, Sarath and Vosoughi, Soroush and Mitamura, Teruko and Hovy, Eduard},
	month = may,
	year = {2021},
	note = {arXiv: 2105.03075},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted to ACL 2021 Findings},
	file = {Feng et al. - 2021 - A Survey of Data Augmentation Approaches for NLP.pdf:/Users/StefanosAchlatis/Zotero/storage/4BYFAHMY/Feng et al. - 2021 - A Survey of Data Augmentation Approaches for NLP.pdf:application/pdf},
}

@article{warstadt_neural_2019,
	title = {Neural {Network} {Acceptability} {Judgments}},
	url = {http://arxiv.org/abs/1805.12471},
	abstract = {This paper investigates the ability of artiﬁcial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. As baselines, we train several recurrent neural network models on acceptability classiﬁcation, and ﬁnd that our models outperform unsupervised models by Lau et al. (2016) on CoLA. Error-analysis on speciﬁc grammatical phenomena reveals that both Lau et al.’s models and ours learn systematic generalizations like subject-verb-object order. However, all models we test perform far below human level on a wide range of grammatical constructions.},
	language = {en},
	urldate = {2021-05-13},
	journal = {arXiv:1805.12471 [cs]},
	author = {Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R.},
	month = oct,
	year = {2019},
	note = {arXiv: 1805.12471},
	keywords = {Computer Science - Computation and Language},
	file = {Warstadt et al. - 2019 - Neural Network Acceptability Judgments.pdf:/Users/StefanosAchlatis/Zotero/storage/3LZC4DIK/Warstadt et al. - 2019 - Neural Network Acceptability Judgments.pdf:application/pdf},
}

@article{wang_glue_2019,
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	shorttitle = {{GLUE}},
	url = {http://arxiv.org/abs/1804.07461},
	abstract = {For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and ﬁnd that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.},
	language = {en},
	urldate = {2021-05-13},
	journal = {arXiv:1804.07461 [cs]},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2019},
	note = {arXiv: 1804.07461},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ICLR 2019; https://gluebenchmark.com/},
	file = {Wang et al. - 2019 - GLUE A Multi-Task Benchmark and Analysis Platform.pdf:/Users/StefanosAchlatis/Zotero/storage/LSMRXWQ8/Wang et al. - 2019 - GLUE A Multi-Task Benchmark and Analysis Platform.pdf:application/pdf},
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2021-05-13},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {Vaswani et al. - 2017 - Attention Is All You Need.pdf:/Users/StefanosAchlatis/Zotero/storage/PU6KPTHT/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf},
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2021-05-13},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:/Users/StefanosAchlatis/Zotero/storage/P7B7XLBD/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@article{wu_group_2018,
	title = {Group {Normalization}},
	url = {http://arxiv.org/abs/1803.08494},
	abstract = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems — BN’s error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN’s usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN’s computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6\% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to ﬁne-tuning. GN can outperform its BNbased counterparts for object detection and segmentation in COCO,1 and for video classiﬁcation in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
	language = {en},
	urldate = {2021-05-13},
	journal = {arXiv:1803.08494 [cs]},
	author = {Wu, Yuxin and He, Kaiming},
	month = jun,
	year = {2018},
	note = {arXiv: 1803.08494},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: v3: Update trained-from-scratch results in COCO to 41.0AP. Code and models at https://github.com/facebookresearch/Detectron/blob/master/projects/GN},
	file = {Wu and He - 2018 - Group Normalization.pdf:/Users/StefanosAchlatis/Zotero/storage/EZBF5UST/Wu and He - 2018 - Group Normalization.pdf:application/pdf},
}

@article{shen_powernorm_2020,
	title = {{PowerNorm}: {Rethinking} {Batch} {Normalization} in {Transformers}},
	shorttitle = {{PowerNorm}},
	url = {http://arxiv.org/abs/2003.07845},
	abstract = {The standard normalization method for neural network (NN) models used in Natural Language Processing (NLP) is layer normalization (LN). This is different than batch normalization (BN), which is widely-adopted in Computer Vision. The preferred use of LN in NLP is principally due to the empirical observation that a (naive/vanilla) use of BN leads to signiﬁcant performance degradation for NLP tasks; however, a thorough understanding of the underlying reasons for this is not always evident. In this paper, we perform a systematic study of NLP transformer models to understand why BN has a poor performance, as compared to LN. We ﬁnd that the statistics of NLP data across the batch dimension exhibit large ﬂuctuations throughout training. This results in instability, if BN is naively implemented. To address this, we propose Power Normalization (PN), a novel normalization scheme that resolves this issue by (i) relaxing zero-mean normalization in BN, (ii) incorporating a running quadratic mean instead of per batch statistics to stabilize ﬂuctuations, and (iii) using an approximate backpropagation for incorporating the running statistics in the forward pass. We show theoretically, under mild assumptions, that PN leads to a smaller Lipschitz constant for the loss, compared with BN. Furthermore, we prove that the approximate backpropagation scheme leads to bounded gradients. We extensively test PN for transformers on a range of NLP tasks, and we show that it signiﬁcantly outperforms both LN and BN. In particular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0 PPL on PTB/WikiText-103. We make our code publicly available at https://github.com/ sIncerass/powernorm.},
	language = {en},
	urldate = {2021-05-14},
	journal = {arXiv:2003.07845 [cs]},
	author = {Shen, Sheng and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W. and Keutzer, Kurt},
	month = jun,
	year = {2020},
	note = {arXiv: 2003.07845},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Shen et al. - 2020 - PowerNorm Rethinking Batch Normalization in Trans.pdf:/Users/StefanosAchlatis/Zotero/storage/3SAFCKPT/Shen et al. - 2020 - PowerNorm Rethinking Batch Normalization in Trans.pdf:application/pdf},
}

@article{wu_group_2018-1,
	title = {Group {Normalization}},
	url = {http://arxiv.org/abs/1803.08494},
	abstract = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems — BN’s error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN’s usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN’s computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6\% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to ﬁne-tuning. GN can outperform its BNbased counterparts for object detection and segmentation in COCO,1 and for video classiﬁcation in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
	language = {en},
	urldate = {2021-05-14},
	journal = {arXiv:1803.08494 [cs]},
	author = {Wu, Yuxin and He, Kaiming},
	month = jun,
	year = {2018},
	note = {arXiv: 1803.08494},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: v3: Update trained-from-scratch results in COCO to 41.0AP. Code and models at https://github.com/facebookresearch/Detectron/blob/master/projects/GN},
	file = {Wu and He - 2018 - Group Normalization.pdf:/Users/StefanosAchlatis/Zotero/storage/9C9VU7DA/Wu and He - 2018 - Group Normalization.pdf:application/pdf},
}

@inproceedings{tenney_bert_2019,
	address = {Florence, Italy},
	title = {{BERT} {Rediscovers} the {Classical} {NLP} {Pipeline}},
	url = {https://www.aclweb.org/anthology/P19-1452},
	doi = {10.18653/v1/P19-1452},
	abstract = {Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We ﬁnd that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lowerlevel decisions on the basis of disambiguating information from higher-level representations.},
	language = {en},
	urldate = {2021-05-15},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
	year = {2019},
	pages = {4593--4601},
	file = {Tenney et al. - 2019 - BERT Rediscovers the Classical NLP Pipeline.pdf:/Users/StefanosAchlatis/Zotero/storage/KN8XHNBA/Tenney et al. - 2019 - BERT Rediscovers the Classical NLP Pipeline.pdf:application/pdf},
}

@article{lee-thorp_fnet_2021,
	title = {{FNet}: {Mixing} {Tokens} with {Fourier} {Transforms}},
	shorttitle = {{FNet}},
	url = {http://arxiv.org/abs/2105.03824},
	abstract = {We show that Transformer encoder architectures can be massively sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that “mix” input tokens. These linear transformations, along with simple nonlinearities in feed-forward layers, are sufﬁcient to model semantic relationships in several text classiﬁcation tasks. Perhaps most surprisingly, we ﬁnd that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92\% of the accuracy of BERT on the GLUE benchmark, but pre-trains and runs up to seven times faster on GPUs and twice as fast on TPUs. The resulting model, which we name FNet, scales very efﬁciently to long inputs, matching the accuracy of the most accurate “efﬁcient” Transformers on the Long Range Arena benchmark, but training and running faster across all sequence lengths on GPUs and relatively shorter sequence lengths on TPUs. Finally, FNet has a light memory footprint and is particularly efﬁcient at smaller model sizes: for a ﬁxed speed and accuracy budget, small FNet models outperform Transformer counterparts.},
	language = {en},
	urldate = {2021-05-15},
	journal = {arXiv:2105.03824 [cs]},
	author = {Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
	month = may,
	year = {2021},
	note = {arXiv: 2105.03824},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Lee-Thorp et al. - 2021 - FNet Mixing Tokens with Fourier Transforms.pdf:/Users/StefanosAchlatis/Zotero/storage/6D9IRY6T/Lee-Thorp et al. - 2021 - FNet Mixing Tokens with Fourier Transforms.pdf:application/pdf},
}

@inproceedings{yenicelik_how_2020,
	address = {Online},
	title = {How does {BERT} capture semantics? {A} closer look at polysemous words},
	shorttitle = {How does {BERT} capture semantics?},
	url = {https://www.aclweb.org/anthology/2020.blackboxnlp-1.15},
	doi = {10.18653/v1/2020.blackboxnlp-1.15},
	abstract = {The recent paradigm shift to contextual word embeddings has seen tremendous success across a wide range of down-stream tasks. However, little is known on how the emergent relation of context and semantics manifests geometrically. We investigate polysemous words as one particularly prominent instance of semantic organization. Our rigorous quantitative analysis of linear separability and cluster organization in embedding vectors produced by BERT shows that semantics do not surface as isolated clusters but form seamless structures, tightly coupled with sentiment and syntax.},
	language = {en},
	urldate = {2021-05-16},
	booktitle = {Proceedings of the {Third} {BlackboxNLP} {Workshop} on {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Yenicelik, David and Schmidt, Florian and Kilcher, Yannic},
	year = {2020},
	pages = {156--162},
	file = {Yenicelik et al. - 2020 - How does BERT capture semantics A closer look at .pdf:/Users/StefanosAchlatis/Zotero/storage/JC3YACMP/Yenicelik et al. - 2020 - How does BERT capture semantics A closer look at .pdf:application/pdf},
}

@article{kou_improving_2020,
	title = {Improving {BERT} with {Self}-{Supervised} {Attention}},
	url = {http://arxiv.org/abs/2004.03808},
	abstract = {One of the most popular paradigms of applying large, pre-trained NLP models such as BERT is to ﬁne-tune it on a smaller dataset. However, one challenge remains as the ﬁnetuned model often overﬁts on smaller datasets. A symptom of this phenomenon is that irrelevant words in the sentences, even when they are obvious to humans, can substantially degrade the performance of these ﬁne-tuned BERT models. In this paper, we propose a novel technique, called Self-Supervised Attention (SSA) to help facilitate this generalization challenge. Speciﬁcally, SSA automatically generates weak, token-level attention labels iteratively by “probing” the ﬁne-tuned model from the previous iteration. We investigate two different ways of integrating SSA into BERT and propose a hybrid approach to combine their beneﬁts. Empirically, on a variety of public datasets, we illustrate signiﬁcant performance improvement using our SSA-enhanced BERT model.},
	language = {en},
	urldate = {2021-05-16},
	journal = {arXiv:2004.03808 [cs]},
	author = {Kou, Xiaoyu and Yang, Yaming and Wang, Yujing and Zhang, Ce and Chen, Yiren and Tong, Yunhai and Zhang, Yan and Bai, Jing},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.03808},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Kou et al. - 2020 - Improving BERT with Self-Supervised Attention.pdf:/Users/StefanosAchlatis/Zotero/storage/X2VE884Q/Kou et al. - 2020 - Improving BERT with Self-Supervised Attention.pdf:application/pdf},
}

@article{wortsman_supermasks_2020-1,
	title = {Supermasks in {Superposition}},
	url = {http://arxiv.org/abs/2006.14769},
	abstract = {We present the Supermasks in Superposition (SupSup) model, capable of sequentially learning thousands of tasks without catastrophic forgetting. Our approach uses a randomly initialized, ﬁxed base network and for each task ﬁnds a subnetwork (supermask) that achieves good performance. If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to ﬁnd a linear superposition of learned supermasks which minimizes the output entropy. In practice we ﬁnd that a single gradient step is often sufﬁcient to identify the correct mask, even among 2500 tasks. We also showcase two promising extensions. First, SupSup models can be trained entirely without task identity information, as they may detect when they are uncertain about new data and allocate an additional supermask for the new training distribution. Finally the entire, growing set of supermasks can be stored in a constant-sized reservoir by implicitly storing them as attractors in a ﬁxed-sized Hopﬁeld network.},
	language = {en},
	urldate = {2021-05-16},
	journal = {arXiv:2006.14769 [cs, stat]},
	author = {Wortsman, Mitchell and Ramanujan, Vivek and Liu, Rosanne and Kembhavi, Aniruddha and Rastegari, Mohammad and Yosinski, Jason and Farhadi, Ali},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.14769},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	annote = {Comment: NeurIPS 2020 Camera Ready},
	file = {Wortsman et al. - 2020 - Supermasks in Superposition.pdf:/Users/StefanosAchlatis/Zotero/storage/IJZRIUKF/Wortsman et al. - 2020 - Supermasks in Superposition.pdf:application/pdf},
}

@article{cui_fine-tune_nodate,
	title = {Fine-tune {BERT} with {Sparse} {Self}-{Attention} {Mechanism}},
	abstract = {In this paper, we develop a novel Sparse Self-Attention Fine-tuning model (referred as SSAF) which integrates sparsity into selfattention mechanism to enhance the ﬁnetuning performance of BERT. In particular, sparsity is introduced into the self-attention by replacing softmax function with a controllable sparse transformation when ﬁne-tuning with BERT. It enables us to learn a structurally sparse attention distribution, which leads to a more interpretable representation for the whole input. The proposed model is evaluated on sentiment analysis, question answering, and natural language inference tasks. The extensive experimental results across multiple datasets demonstrate its effectiveness and superiority to the baseline methods.},
	language = {en},
	author = {Cui, Baiyun and Li, Yingming and Chen, Ming and Zhang, Zhongfei},
	pages = {6},
	file = {Cui et al. - Fine-tune BERT with Sparse Self-Attention Mechanis.pdf:/Users/StefanosAchlatis/Zotero/storage/XR2P7T59/Cui et al. - Fine-tune BERT with Sparse Self-Attention Mechanis.pdf:application/pdf},
}

@article{xia_using_2021,
	title = {Using {Prior} {Knowledge} to {Guide} {BERT}'s {Attention} in {Semantic} {Textual} {Matching} {Tasks}},
	url = {http://arxiv.org/abs/2102.10934},
	doi = {10.1145/3442381.3449988},
	abstract = {We study the problem of incorporating prior knowledge into a deep Transformer-based model, i.e., Bidirectional Encoder Representations from Transformers (BERT), to enhance its performance on semantic textual matching tasks. By probing and analyzing what BERT has already known when solving this task, we obtain better understanding of what task-specific knowledge BERT needs the most and where it is most needed. The analysis further motivates us to take a different approach than most existing works. Instead of using prior knowledge to create a new training task for fine-tuning BERT, we directly inject knowledge into BERT’s multi-head attention mechanism. This leads us to a simple yet effective approach that enjoys fast training stage as it saves the model from training on additional data or tasks other than the main task. Extensive experiments demonstrate that the proposed knowledge-enhanced BERT is able to consistently improve semantic textual matching performance over the original BERT model, and the performance benefit is most salient when training data is scarce.},
	language = {en},
	urldate = {2021-05-16},
	journal = {arXiv:2102.10934 [cs]},
	author = {Xia, Tingyu and Wang, Yue and Tian, Yuan and Chang, Yi},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.10934},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: 10 pages, WWW'21, April19-23, 2021, Ljubljana, Slovenia
 
external knowledge},
	file = {Xia et al. - 2021 - Using Prior Knowledge to Guide BERT's Attention in.pdf:/Users/StefanosAchlatis/Zotero/storage/P84X2U7V/Xia et al. - 2021 - Using Prior Knowledge to Guide BERT's Attention in.pdf:application/pdf},
}

@article{rogers_primer_2020-1,
	title = {A {Primer} in {BERTology}: {What} we know about how {BERT} works},
	shorttitle = {A {Primer} in {BERTology}},
	url = {http://arxiv.org/abs/2002.12327},
	abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the ﬁrst survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modiﬁcations to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
	language = {en},
	urldate = {2021-05-16},
	journal = {arXiv:2002.12327 [cs]},
	author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	month = nov,
	year = {2020},
	note = {arXiv: 2002.12327},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted to TACL. Please note that the multilingual BERT section is only available in version 1},
	file = {Rogers et al. - 2020 - A Primer in BERTology What we know about how BERT.pdf:/Users/StefanosAchlatis/Zotero/storage/2A69Z8PQ/Rogers et al. - 2020 - A Primer in BERTology What we know about how BERT.pdf:application/pdf},
}

@article{hewitt_structural_nodate,
	title = {A {Structural} {Probe} for {Finding} {Syntax} in {Word} {Representations}},
	abstract = {Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network’s word representation space. The probe identiﬁes a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models’ vector geometry.},
	language = {en},
	author = {Hewitt, John and Manning, Christopher D},
	pages = {10},
	file = {Hewitt and Manning - A Structural Probe for Finding Syntax in Word Repr.pdf:/Users/StefanosAchlatis/Zotero/storage/T6DKERPC/Hewitt and Manning - A Structural Probe for Finding Syntax in Word Repr.pdf:application/pdf},
}

@inproceedings{department_of_linguistics_and_philology_uppsala_university_sweden_understanding_2019,
	title = {Understanding {Neural} {Machine} {Translation} by {Simplification}: {The} {Case} of {Encoder}-free {Models}},
	isbn = {978-954-452-056-4},
	shorttitle = {Understanding {Neural} {Machine} {Translation} by {Simplification}},
	url = {https://acl-bg.org/proceedings/2019/RANLP 2019/pdf/RANLP136.pdf},
	doi = {10.26615/978-954-452-056-4_136},
	abstract = {In this paper, we try to understand neural machine translation (NMT) via simplifying NMT architectures and training encoder-free NMT models. In an encoderfree model, the sums of word embeddings and positional embeddings represent the source. The decoder is a standard Transformer or recurrent neural network that directly attends to embeddings via attention mechanisms. Experimental results show (1) that the attention mechanism in encoder-free models acts as a strong feature extractor, (2) that the word embeddings in encoder-free models are competitive to those in conventional models, (3) that non-contextualized source representations lead to a big performance drop, and (4) that encoder-free models have different effects on alignment quality for German→English and Chinese→English.},
	language = {en},
	urldate = {2021-05-20},
	booktitle = {Proceedings - {Natural} {Language} {Processing} in a {Deep} {Learning} {World}},
	publisher = {Incoma Ltd., Shoumen, Bulgaria},
	author = {{Department of Linguistics and Philology, Uppsala University, Sweden} and Tang, Gongbo and {Rico Sennrich} and {School of Informatics, University of Edinburgh Edinburgh, UK} and {Institute of Computational Linguistics, University of Zurich, Switzerland} and Nivre, Joakim and {Department of Linguistics and Philology, Uppsala University, Sweden}},
	month = oct,
	year = {2019},
	keywords = {entropy},
	pages = {1186--1193},
	file = {Department of Linguistics and Philology, Uppsala University, Sweden et al. - 2019 - Understanding Neural Machine Translation by Simpli.pdf:/Users/StefanosAchlatis/Zotero/storage/9YVUWX3T/Department of Linguistics and Philology, Uppsala University, Sweden et al. - 2019 - Understanding Neural Machine Translation by Simpli.pdf:application/pdf},
}

@article{baevski_unsupervised_nodate,
	title = {Unsupervised {Speech} {Recognition}},
	language = {en},
	author = {Baevski, Alexei and Hsu, Wei-Ning and Conneau, Alexis and Auli, Michael},
	pages = {35},
	file = {Baevski et al. - Unsupervised Speech Recognition.pdf:/Users/StefanosAchlatis/Zotero/storage/58DTVHP2/Baevski et al. - Unsupervised Speech Recognition.pdf:application/pdf},
}

@inproceedings{das_introduction_2020,
	address = {Rome, Italy},
	title = {Introduction of {Wireless} {Services} and {Devices} in a {Hospital} {Environment} {Following} a {Risk}-based {EMC} {Approach}},
	isbn = {978-1-72815-579-1},
	url = {https://ieeexplore.ieee.org/document/9245878/},
	doi = {10.1109/EMCEUROPE48519.2020.9245878},
	abstract = {This paper emphasizes the need for a risk-based EMC approach in the hospital environment. A modern hospital with various kinds of wireless medical electronic equipment especially in the intensive care, operation theatre, neonatology, etc., pollutes the environment by creating electromagnetic interference with other equipment in the vicinity or even implanted equipment inside the patient. Even following the IEC 60601-1-2 product standard for medical equipment which is based on a rule-based approach, suppressing electromagnetic interference effectively is an arduous task. In the upcoming years, the number of wireless devices in a modern hospital will significantly increase and might cause EMC instability in this highly complex system. Hence, an urgent requirement for an advanced and smarter approach leads to the use of a risk-based EMC approach. The detrimental effect of intentional sources causing electromagnetic interference in the hospital environment is briefly discussed. The risk-based EMC approach has been applied in the Medisch Spectrum Twente hospital, where it was discovered that wireless devices such as digital mobile radio, cell phone, and radio frequency identification equipment critically affect equipment pumps and patient monitors.},
	language = {en},
	urldate = {2021-05-24},
	booktitle = {2020 {International} {Symposium} on {Electromagnetic} {Compatibility} - {EMC} {EUROPE}},
	publisher = {IEEE},
	author = {Das, Mumpy and Jeunink, Silvo and Vogt-Ardatjew, Robert and van den Berg, Barbel and Leferink, Frank},
	month = sep,
	year = {2020},
	pages = {1--6},
	file = {Das et al. - 2020 - Introduction of Wireless Services and Devices in a.pdf:/Users/StefanosAchlatis/Zotero/storage/4TSB8Y2Q/Das et al. - 2020 - Introduction of Wireless Services and Devices in a.pdf:application/pdf},
}

@article{hoefler_sparsity_2021,
	title = {Sparsity in {Deep} {Learning}: {Pruning} and growth for efficient inference and training in neural networks},
	shorttitle = {Sparsity in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2102.00554},
	abstract = {The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, if not better than, the original dense networks. Sparsity can reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field.},
	language = {en},
	urldate = {2021-05-27},
	journal = {arXiv:2102.00554 [cs]},
	author = {Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
	month = jan,
	year = {2021},
	note = {arXiv: 2102.00554},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Computer Science - Hardware Architecture},
	annote = {Comment: 90 pages, 26 figures},
	file = {Hoefler et al. - 2021 - Sparsity in Deep Learning Pruning and growth for .pdf:/Users/StefanosAchlatis/Zotero/storage/F6S58GA2/Hoefler et al. - 2021 - Sparsity in Deep Learning Pruning and growth for .pdf:application/pdf},
}

@inproceedings{jawahar_what_2019-1,
	address = {Florence, Italy},
	title = {What {Does} {BERT} {Learn} about the {Structure} of {Language}?},
	url = {https://www.aclweb.org/anthology/P19-1356},
	doi = {10.18653/v1/P19-1356},
	abstract = {BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. We ﬁrst show that BERT’s phrasal representation captures phrase-level information in the lower layers. We also show that BERT’s intermediate layers encode a rich hierarchy of linguistic information, with surface features at the bottom, syntactic features in the middle and semantic features at the top. BERT turns out to require deeper layers when long-distance dependency information is required, e.g. to track subjectverb agreement. Finally, we show that BERT representations capture linguistic information in a compositional way that mimics classical, tree-like structures.},
	language = {en},
	urldate = {2021-06-01},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Jawahar, Ganesh and Sagot, Benoît and Seddah, Djamé},
	year = {2019},
	pages = {3651--3657},
	file = {Jawahar et al. - 2019 - What Does BERT Learn about the Structure of Langua.pdf:/Users/StefanosAchlatis/Zotero/storage/2IXAEW2N/Jawahar et al. - 2019 - What Does BERT Learn about the Structure of Langua.pdf:application/pdf},
}

@article{wang_structured_2020,
	title = {Structured {Pruning} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/1910.04732},
	doi = {10.18653/v1/2020.emnlp-main.496},
	abstract = {Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a generic, structured pruning approach by parameterizing each weight matrix using its low-rank factorization, and adaptively removing rank-1 components during training. On language modeling tasks, our structured approach outperforms other unstructured and block-structured pruning baselines at various compression levels, while achieving significant speedups during both training and inference. We also demonstrate that our method can be applied to pruning adaptive word embeddings in large language models, and to pruning the BERT model on several downstream fine-tuning classification benchmarks.},
	language = {en},
	urldate = {2021-06-06},
	journal = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	author = {Wang, Ziheng and Wohlwend, Jeremy and Lei, Tao},
	year = {2020},
	note = {arXiv: 1910.04732},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {6151--6162},
	file = {Wang et al. - 2020 - Structured Pruning of Large Language Models.pdf:/Users/StefanosAchlatis/Zotero/storage/7PK6ZU2K/Wang et al. - 2020 - Structured Pruning of Large Language Models.pdf:application/pdf},
}

@article{louizos_learning_2018,
	title = {Learning {Sparse} {Neural} {Networks} through \${L}\_0\$ {Regularization}},
	url = {http://arxiv.org/abs/1712.01312},
	abstract = {We propose a practical method for L0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of L0 regularization. However, since the L0 norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected L0 regularized objective is differentiable with respect to the distribution parameters. We further propose the hard concrete distribution for the gates, which is obtained by “stretching” a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efﬁcient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.},
	language = {en},
	urldate = {2021-06-06},
	journal = {arXiv:1712.01312 [cs, stat]},
	author = {Louizos, Christos and Welling, Max and Kingma, Diederik P.},
	month = jun,
	year = {2018},
	note = {arXiv: 1712.01312},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published as a conference paper at the International Conference on Learning Representations (ICLR) 2018},
	file = {Louizos et al. - 2018 - Learning Sparse Neural Networks through \$L_0\$ Regu.pdf:/Users/StefanosAchlatis/Zotero/storage/37DDWPTM/Louizos et al. - 2018 - Learning Sparse Neural Networks through \$L_0\$ Regu.pdf:application/pdf},
}

@article{zhang_know_2020-1,
	title = {Know {What} {You} {Don}'t {Need}: {Single}-{Shot} {Meta}-{Pruning} for {Attention} {Heads}},
	shorttitle = {Know {What} {You} {Don}'t {Need}},
	url = {http://arxiv.org/abs/2011.03770},
	abstract = {Deep pre-trained Transformer models have achieved state-of-the-art results over a variety of natural language processing (NLP) tasks. By learning rich language knowledge with millions of parameters, these models are usually overparameterized and signiﬁcantly increase the computational overhead in applications. It is intuitive to address this issue by model compression. In this work, we propose a method, called Single-Shot Meta-Pruning, to compress deep pre-trained Transformers before ﬁne-tuning. Speciﬁcally, we focus on pruning unnecessary attention heads adaptively for different downstream tasks. To measure the informativeness of attention heads, we train our Single-Shot Meta-Pruner (SMP) with a meta-learning paradigm aiming to maintain the distribution of text representations after pruning. Compared with existing compression methods for pre-trained models, our method can reduce the overhead of both ﬁne-tuning and inference. Experimental results show that our pruner can selectively prune 50\% of attention heads with little impact on the performance on downstream tasks and even provide better text representations. The source code will be released in the future.},
	language = {en},
	urldate = {2021-06-07},
	journal = {arXiv:2011.03770 [cs]},
	author = {Zhang, Zhengyan and Qi, Fanchao and Liu, Zhiyuan and Liu, Qun and Sun, Maosong},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.03770},
	keywords = {Computer Science - Computation and Language},
	file = {Zhang et al. - 2020 - Know What You Don't Need Single-Shot Meta-Pruning.pdf:/Users/StefanosAchlatis/Zotero/storage/N2MKE8PP/Zhang et al. - 2020 - Know What You Don't Need Single-Shot Meta-Pruning.pdf:application/pdf},
}

@article{hua_noise_nodate,
	title = {Noise {Stability} {Regularization} for {Improving} {BERT} {Fine}-tuning},
	abstract = {Fine-tuning pre-trained language models such as BERT has become a common practice dominating leaderboards across various NLP tasks. Despite its recent success and wide adoption, this process is unstable when there are only a small number of training samples available. The brittleness of this process is often reﬂected by the sensitivity to random seeds. In this paper, we propose to tackle this problem based on the noise stability property of deep nets, which is investigated in recent literature (Arora et al., 2018; Sanyal et al., 2020). Speciﬁcally, we introduce a novel and effective regularization method to improve ﬁne-tuning on NLP tasks, referred to as Layer-wise Noise Stability Regularization (LNSR). We extend the theories about adding noise to the input and prove that our method gives a stabler regularization effect. We provide supportive evidence by experimentally conﬁrming that well-performing models show a low sensitivity to noise and ﬁne-tuning with LNSR exhibits clearly higher generalizability and stability. Furthermore, our method also demonstrates advantages over other state-of-the-art algorithms including L2SP (Li et al., 2018), Mixout (Lee et al., 2020) and SMART (Jiang et al., 2020).},
	language = {en},
	author = {Hua, Hang and Li, Xingjian and Dou, Dejing and Xu, Chengzhong and Luo, Jiebo},
	pages = {13},
	file = {Hua et al. - Noise Stability Regularization for Improving BERT .pdf:/Users/StefanosAchlatis/Zotero/storage/3GNQP8FC/Hua et al. - Noise Stability Regularization for Improving BERT .pdf:application/pdf},
}

@article{xu_rethinking_nodate,
	title = {Rethinking {Network} {Pruning} – under the {Pre}-train and {Fine}-tune {Paradigm}},
	language = {en},
	author = {Xu, Dongkuan and Yen, Ian En-Hsu and Zhao, Jinxi and Xiao, Zhibin},
	pages = {7},
	file = {Xu et al. - Rethinking Network Pruning – under the Pre-train a.pdf:/Users/StefanosAchlatis/Zotero/storage/DZ942QWW/Xu et al. - Rethinking Network Pruning – under the Pre-train a.pdf:application/pdf},
}

@article{sanh_low-complexity_nodate,
	title = {Low-{Complexity} {Probing} via {Finding} {Subnetworks}},
	language = {en},
	author = {Sanh, Victor and Rush, Alexander},
	pages = {7},
	file = {Sanh and Rush - Low-Complexity Probing via Finding Subnetworks.pdf:/Users/StefanosAchlatis/Zotero/storage/QP2PPFXN/Sanh and Rush - Low-Complexity Probing via Finding Subnetworks.pdf:application/pdf},
}

@article{bian_attention_nodate,
	title = {On {Attention} {Redundancy}: {A} {Comprehensive} {Study}},
	language = {en},
	author = {Bian, Yuchen and Huang, Jiaji and Cai, Xingyu and Yuan, Jiahong and Church, Kenneth},
	pages = {16},
	file = {Bian et al. - On Attention Redundancy A Comprehensive Study.pdf:/Users/StefanosAchlatis/Zotero/storage/R57CCW74/Bian et al. - On Attention Redundancy A Comprehensive Study.pdf:application/pdf},
}

@article{devlin_bert_2019-1,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2021-06-11},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:/Users/StefanosAchlatis/Zotero/storage/ZCPMWV8H/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@article{zaranis_empathetic_2020,
	title = {Empathetic {Dialogue} {Generation} using generation-based models},
	language = {en},
	author = {Zaranis, Emmanouil},
	year = {2020},
	pages = {148},
	file = {Zaranis - 2020 - Empathetic Dialogue Generation using generation-ba.pdf:/Users/StefanosAchlatis/Zotero/storage/AZ7QH2GD/Zaranis - 2020 - Empathetic Dialogue Generation using generation-ba.pdf:application/pdf},
}

@misc{wang2019glue,
      title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}, 
      author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
      year={2019},
      eprint={1804.07461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{warstadt2018neural,
    title={Neural Network Acceptability Judgments},
    author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
    journal={arXiv preprint arXiv:1805.12471},
    year={2018}
}

@inproceedings{socher-etal-2013-recursive,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D13-1170",
    pages = "1631--1642",
}
@inproceedings{dolan-brockett-2005-automatically,
    title = "Automatically Constructing a Corpus of Sentential Paraphrases",
    author = "Dolan, William B.  and
      Brockett, Chris",
    booktitle = "Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)",
    year = "2005",
    url = "https://www.aclweb.org/anthology/I05-5002",
}

@inproceedings{cer-etal-2017-semeval,
    title = "{S}em{E}val-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation",
    author = "Cer, Daniel  and
      Diab, Mona  and
      Agirre, Eneko  and
      Lopez-Gazpio, I{\~n}igo  and
      Specia, Lucia",
    booktitle = "Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/S17-2001",
    doi = "10.18653/v1/S17-2001",
    pages = "1--14",
}

@misc{peters2018deep,
      title={Deep contextualized word representations}, 
      author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
      year={2018},
      eprint={1802.05365},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@InProceedings{N18-1101,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for 
           Sentence Understanding through Inference",
  booktitle = "Proceedings of the 2018 Conference of 
               the North American Chapter of the 
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
  url = "http://aclweb.org/anthology/N18-1101"
}

@inproceedings{rajpurkar-etal-2016-squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}

@article{Levesque_2013,
    title={The Winograd schema challenge. In
AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning},
    author={Hector J Levesque, Ernest Davis, and Leora Morgenstern. },
    year={2011}
}

@inproceedings{ghader-monz-2017-attention,
    title = "What does Attention in Neural Machine Translation Pay Attention to?",
    author = "Ghader, Hamidreza  and
      Monz, Christof",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://www.aclweb.org/anthology/I17-1004",
    pages = "30--39",
    abstract = "Attention in neural machine translation provides the possibility to encode relevant parts of the source sentence at each translation step. As a result, attention is considered to be an alignment model as well. However, there is no work that specifically studies attention and provides analysis of what is being learned by attention models. Thus, the question still remains that how attention is similar or different from the traditional alignment. In this paper, we provide detailed analysis of attention and compare it to traditional alignment. We answer the question of whether attention is only capable of modelling translational equivalent or it captures more information. We show that attention is different from alignment in some cases and is capturing useful information other than alignments.",
}

@inproceedings{beltagy-etal-2019-scibert,
    title = "SciBERT: A Pretrained Language Model for Scientific Text",
    author = "Beltagy, Iz  and Lo, Kyle  and Cohan, Arman",
    booktitle = "EMNLP",
    year = "2019",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1371"
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}

@article{Yao_2019,
   title={Balanced Sparsity for Efficient DNN Inference on GPU},
   volume={33},
   ISSN={2159-5399},
   url={http://dx.doi.org/10.1609/aaai.v33i01.33015676},
   DOI={10.1609/aaai.v33i01.33015676},
   journal={Proceedings of the AAAI Conference on Artificial Intelligence},
   publisher={Association for the Advancement of Artificial Intelligence (AAAI)},
   author={Yao, Zhuliang and Cao, Shijie and Xiao, Wencong and Zhang, Chen and Nie, Lanshun},
   year={2019},
   month={Jul},
   pages={5676–5683}
}

@article{gpu_1,
   url={https://dl.acm.org/doi/10.1145/3289602.3293898},
}

@article{gpu_2,
   url={https://dl.acm.org/doi/10.1145/3293883.3295701},
}


@misc{mikolov2013efficient,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{pennington-etal-2014-glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}
@inproceedings{baker-etal-1998-berkeley-framenet,
    title = "The {B}erkeley {F}rame{N}et Project",
    author = "Baker, Collin F.  and
      Fillmore, Charles J.  and
      Lowe, John B.",
    booktitle = "36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",
    month = aug,
    year = "1998",
    address = "Montreal, Quebec, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P98-1013",
    doi = "10.3115/980845.980860",
    pages = "86--90",
}

@article{Bengio_anguage_model,
  author={Y. Bengio, R. Ducharme, P. Vincent},
  title={A neural probabilistic language model. Journal of Machine Learning Research}, 
  year={2003}}


@article{Mikolov_thesis,
  author={T. Mikolov},
  title={Language Modeling for Speech Recognition in Czech}, 
  year={2007}}
  
@inproceedings{4960686,
  author={Mikolov, Tomas and Kopecky, Jiri and Burget, Lukas and Glembek, Ondrej and ?Cernocky, Jan},
  booktitle={2009 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
  title={Neural network based language models for highly inflective languages}, 
  year={2009},
  volume={},
  number={},
  pages={4725-4728},
  doi={10.1109/ICASSP.2009.4960686}}
  
 @inproceedings{Mikolov:2010wx,
  added-at = {2019-05-21T10:10:49.000+0200},
  author = {Mikolov, Tomas and Karafi{\'a}t, Martin and Burget, Luk{\'a}s and Cernock{\'{y}}, Jan and Khudanpur, Sanjeev},
  biburl = {https://www.bibsonomy.org/bibtex/278d2617b9b9829b5476450bd4d1200b1/sxkdz},
  booktitle = {Proceedings of the 11th Annual Conference of the International Speech Communication Association},
  interhash = {245d1279b03856348523ebefda4cac10},
  intrahash = {78d2617b9b9829b5476450bd4d1200b1},
  keywords = {imported},
  location = {Makuhari, Chiba, Japan},
  pages = {1045--1048},
  publisher = {ISCA},
  series = {INTERSPEECH 2010},
  timestamp = {2019-05-21T10:10:49.000+0200},
  title = {{Recurrent Neural Network Based Language Model}},
  url = {http://www.isca-speech.org/archive/interspeech_2010/i10_1045.html},
  year = 2010
}


@article{726791,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  doi={10.1109/5.726791}}
  
 
@inproceedings{ding-etal-2017-visualizing,
    title = "Visualizing and Understanding Neural Machine Translation",
    author = "Ding, Yanzhuo  and
      Liu, Yang  and
      Luan, Huanbo  and
      Sun, Maosong",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1106",
    doi = "10.18653/v1/P17-1106",
    pages = "1150--1159",
    abstract = "While neural machine translation (NMT) has made remarkable progress in recent years, it is hard to interpret its internal workings due to the continuous representations and non-linearity of neural networks. In this work, we propose to use layer-wise relevance propagation (LRP) to compute the contribution of each contextual word to arbitrary hidden states in the attention-based encoder-decoder framework. We show that visualization with LRP helps to interpret the internal workings of NMT and analyze translation errors.",
}
  
@misc{chen2021gans,
    title={GANs Can Play Lottery Tickets Too},
    author={Xuxi Chen and Zhenyu Zhang and Yongduo Sui and Tianlong Chen},
    year={2021},
    eprint={2106.00134},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{zhu2015aligning,
    title={Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books},
    author={Yukun Zhu and Ryan Kiros and Richard Zemel and Ruslan Salakhutdinov and Raquel Urtasun and Antonio Torralba and Sanja Fidler},
    year={2015},
    eprint={1506.06724},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{han2015learning,
      title={Learning both Weights and Connections for Efficient Neural Networks}, 
      author={Song Han and Jeff Pool and John Tran and William J. Dally},
      year={2015},
      eprint={1506.02626},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
@misc{huang2018learning,
      title={Learning to Prune Filters in Convolutional Neural Networks}, 
      author={Qiangui Huang and Kevin Zhou and Suya You and Ulrich Neumann},
      year={2018},
      eprint={1801.07365},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{church-hanks-1989-word,
    title = "Word Association Norms, Mutual Information, and Lexicography",
    author = "Church, Kenneth Ward  and
      Hanks, Patrick",
    booktitle = "27th Annual Meeting of the Association for Computational Linguistics",
    month = jun,
    year = "1989",
    address = "Vancouver, British Columbia, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P89-1010",
    doi = "10.3115/981623.981633",
    pages = "76--83",
}

@misc{luhn_tf,
      title={A Statistical Approach to Mechanized Encoding and Searching of Literary Information}, 
      author={Luhn, H. P. },
      year={1957},
      booktitle = {IBM Journal of Research and Development, 1, 309-317},
      howpublished = {\url{https://doi.org/10.1147/rd.14.0309}},
}
@ARTICLE{Jones72astatistical,
    author = {Karen Spärck Jones},
    title = {A statistical interpretation of term specificity and its application in retrieval},
    journal = {Journal of Documentation},
    year = {1972},
    volume = {28},
    pages = {11--21}
}
@article{PhysRev.124.1866,
  title = {Effects of Configuration Interaction on Intensities and Phase Shifts},
  author = {Fano, U.},
  journal = {Phys. Rev.},
  volume = {124},
  issue = {6},
  pages = {1866--1878},
  numpages = {0},
  year = {1961},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.124.1866},
  url = {https://link.aps.org/doi/10.1103/PhysRev.124.1866}
}

% Άρθρα σε διεθνή επιστημονικά συνέδρια
@inproceedings{dcis2011,
  author = {I. Liaperdos and L. Dermentzoglou and A. Arapoyanni and Y. Tsiatouhas},
  title = {Fault Detection in {RF} Mixers Combining Defect-Oriented and Alternate Test Strategies},
  booktitle = {26th Conference on Design of Circuits and Integrated Systems (DCIS)},
  year = {2011},
  address = {San Sebastian, Spain},
  month = {nov}
}

@InProceedings{luan2018multitask,
     author = {Luan, Yi and He, Luheng and Ostendorf, Mari and Hajishirzi, Hannaneh},
     title = {Multi-Task Identification of Entities, Relations, and Coreferencefor Scientific Knowledge Graph Construction},
     booktitle = {Proc.\ Conf. Empirical Methods Natural Language Process. (EMNLP)},
     year = {2018},
}

@inproceedings{dernoncourt-lee-2017-pubmed,
    title = "{P}ub{M}ed 200k {RCT}: a Dataset for Sequential Sentence Classification in Medical Abstracts",
    author = "Dernoncourt, Franck  and
      Lee, Ji Young",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/I17-2052",
    pages = "308--313",
    abstract = "We present PubMed 200k RCT, a new dataset based on PubMed for sequential sentence classification. The dataset consists of approximately 200,000 abstracts of randomized controlled trials, totaling 2.3 million sentences. Each sentence of each abstract is labeled with their role in the abstract using one of the following classes: background, objective, method, result, or conclusion. The purpose of releasing this dataset is twofold. First, the majority of datasets for sequential short-text classification (i.e., classification of short texts that appear in sequences) are small: we hope that releasing a new large dataset will help develop more accurate algorithms for this task. Second, from an application perspective, researchers need better tools to efficiently skim through the literature. Automatically classifying each sentence in an abstract would help researchers read abstracts more efficiently, especially in fields where abstracts may be long, such as the medical field.",
}

@misc{yuan2020selfsupervised,
      title={Self-supervised Point Set Local Descriptors for Point Cloud Registration}, 
      author={Yijun Yuan and Jiawei Hou and Andreas Nüchter and Sören Schwertfeger},
      year={2020},
      eprint={2003.05199},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@misc{zhuang2020comprehensive,
      title={A Comprehensive Survey on Transfer Learning}, 
      author={Fuzhen Zhuang and Zhiyuan Qi and Keyu Duan and Dongbo Xi and Yongchun Zhu and Hengshu Zhu and Hui Xiong and Qing He},
      year={2020},
      eprint={1911.02685},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}


@Article{HochSchm97,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  number      = {8},
  pages       = {1735--1780},
  volume      = {9},
  optabstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  optdoi      = {10.1162/neco.1997.9.8.1735},
  opteprint   = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  opturl      = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
}

@misc{bahdanau2016neural,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xu2016show,
      title={Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}, 
      author={Kelvin Xu and Jimmy Ba and Ryan Kiros and Kyunghyun Cho and Aaron Courville and Ruslan Salakhutdinov and Richard Zemel and Yoshua Bengio},
      year={2016},
      eprint={1502.03044},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Nadaraya1964OnER,
  title={On Estimating Regression},
  author={Elizbar Nadaraya},
  journal={Theory of Probability and Its Applications},
  year={1964},
  volume={9},
  pages={141-142}
}

@misc{cho2014learning,
      title={Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}, 
      author={Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
      year={2014},
      eprint={1406.1078},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{chaudhari2021attentive,
      title={An Attentive Survey of Attention Models}, 
      author={Sneha Chaudhari and Varun Mithal and Gungor Polatkan and Rohan Ramanath},
      year={2021},
      eprint={1904.02874},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{luong2015effective,
    title={Effective Approaches to Attention-based Neural Machine Translation},
    author={Minh-Thang Luong and Hieu Pham and Christopher D. Manning},
    year={2015},
    eprint={1508.04025},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@misc{lin2021survey,
      title={A Survey of Transformers}, 
      author={Tianyang Lin and Yuxin Wang and Xiangyang Liu and Xipeng Qiu},
      year={2021},
      eprint={2106.04554},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{peters2018deep,
    title={Deep contextualized word representations},
    author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
    year={2018},
    eprint={1802.05365},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@misc{ba2016layer,
    title={Layer Normalization},
    author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
    year={2016},
    eprint={1607.06450},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{he2015deep,
    title={Deep Residual Learning for Image Recognition},
    author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    year={2015},
    eprint={1512.03385},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@book{DudaHartStork01,
  abstract = {The first edition, published in 1973, has become a classic reference in the field. Now with the second edition, readers will find information on key new topics such as neural networks and statistical pattern recognition, the theory of machine learning, and the theory of invariances. Also included are worked examples, comparisons between different methods, extensive graphics, expanded exercises and computer project topics.},
  added-at = {2016-05-28T21:24:35.000+0200},
  address = {New York},
  author = {Duda, Richard O. and Hart, Peter E. and Stork, David G.},
  biburl = {https://www.bibsonomy.org/bibtex/25ef4fe4778daaf4b4e56c0d66161e048/flint63},
  edition = 2,
  file = {eBook:2000-04/DudaHartStork01.pdf:PDF;Wiley Product page:http\://eu.wiley.com/WileyCDA/WileyTitle/productCd-0471056693.html:URL;Amazon Search inside:http\://www.amazon.de/gp/reader/0471056693/:URL},
  groups = {public},
  interhash = {5af620770e95f6b9ccffca6b3638a8ae},
  intrahash = {5ef4fe4778daaf4b4e56c0d66161e048},
  isbn = {978-0-471-05669-0},
  keywords = {01801 105 book shelf ai data pattern recognition analysis},
  publisher = {Wiley},
  timestamp = {2018-04-17T11:20:04.000+0200},
  title = {Pattern Classification},
  username = {flint63},
  year = 2001
}

@misc{baevski2020wav2vec,
      title={wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations}, 
      author={Alexei Baevski and Henry Zhou and Abdelrahman Mohamed and Michael Auli},
      year={2020},
      eprint={2006.11477},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{timit,
  added-at = {2008-02-26T11:58:58.000+0100},
  author = {Garofolo, J. S. and Lamel, L. F. and Fisher, W. M. and Fiscus, J. G. and Pallett, D. S. and Dahlgren, N. L.},
  biburl = {https://www.bibsonomy.org/bibtex/2f03c554589b58a88b798d35211645b6e/schaul},
  citeulike-article-id = {2382101},
  description = {idsia},
  interhash = {2aeb4ac73650922f64feb31c9e2b35f3},
  intrahash = {f03c554589b58a88b798d35211645b6e},
  keywords = {juergen},
  priority = {2},
  publisher = {NIST},
  timestamp = {2008-02-26T11:59:12.000+0100},
  title = {DARPA TIMIT Acoustic Phonetic Continuous Speech Corpus CDROM},
  year = 1993
}

@misc{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@inproceedings{NIPS1989_6c9882bb,
 author = {LeCun, Yann and Denker, John and Solla, Sara},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Brain Damage},
 url = {https://proceedings.neurips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf},
 volume = {2},
 year = {1990}
}

@misc{nagel2021white,
      title={A White Paper on Neural Network Quantization}, 
      author={Markus Nagel and Marios Fournarakis and Rana Ali Amjad and Yelysei Bondarenko and Mart van Baalen and Tijmen Blankevoort},
      year={2021},
      eprint={2106.08295},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
% Ιστοσελίδες
@misc{LaTeXProject,
  title = {{LaTeX} Project},
  howpublished = {\url{http://www.latex-project.org}},
  note = {\textgreek{Ημερομηνία πρόσβασης}: 13-11-2014}
}

% Πτυχιακές Εργασίες
@ptychionthesis{elli05,
  author =       "\textgreek{Ε. Ανδρουλάκη}",
  title =        "\textgreek{Υλοποίηση Ενεργού Μηχανισμού σε Σύστημα Ομότιμων Βάσεων}",
  school =  "KDBS Lab, \textgreek{Εθνικό Μετσόβιο Πολυτεχνείο}",
  year =         "2005",
  month =        "jul"
}

% Διπλωματικές Εργασίες
@diplomathesis{zoi04,
  author =       "\textgreek{Ζ. Καούδη}",
  title =        "\textgreek{Πρότυπο Σύστημα Αποθήκευσης και Διαχείρισης Σχημάτων \en{RDFS}}",
  school =  "\textgreek{Εθνικό Μετσόβιο Πολυτεχνείο}",
  year =         "2004",
  month =        "jul"
}

% Μεταπτυχιακές Διπλωματικές Εργασίες
@mastersthesis{master04,
  author =       "\textgreek{Ζ. Λάσκαρη}",
  title =        "\textgreek{Κοινωνική Ανάλυση των Ταινιών της \en{Finos Films}}",
  school =  "\textgreek{Εθνικό Μετσόβιο Πολυτεχνείο}",
  year =         "2012",
  month =        "aug"
}

% Διδακτορικές Διατριβές
@phdthesis{phd045,
  author =       "\textgreek{Ζ. Κουρούκλη}",
  title =        "\textgreek{Κατανεμημένα Συστήματα}",
  school =  "\textgreek{ΤΕΙ Πελοποννήσου}",
  year =         "2013",
  month =        "dec"
}

% Τεχνικές Αναφορές
@techreport{MSU-CSE-05-29,
  author =        {J. Gao, H. Cheng and P.-N. Tan},
  title =         {A Framework for Incorporating Labeled Examples into Anomaly Detection},
  number =        {\textlatin{MSU-CSE-05-29}},
  institution =   {Department of Computer Science, Michigan State University},
  address=       {East Lansing, Michigan},
  year  =         {2005}
}

% Διπλώματα Ευρεσιτεχνίας (πατέντες)
@patent{viswanathan2014convenient,
  author={Viswanathan, P. and Winner, G. and Vyas, P.},
  title={Convenient Provisioning of Embedded Devices with WiFi Capability},
  number =        {\textlatin{US Patent 8,665,744}},
  year  =         {2014}
}

