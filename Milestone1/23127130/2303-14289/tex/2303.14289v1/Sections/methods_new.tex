\section{Gradient Tracking Algorithmic Framework}\label{sec.methods}
In this section, we describe our algorithmic framework (\texttt{GTA}) that unifies gradient tracking methods. We then extend the framework to allow for flexibility in the number of communication and  computation steps performed at every iteration. Finally, we make remarks about the algorithmic framework and implementation, and then discuss popular gradient tracking methods as special cases of our proposed framework.
%In this section, we first describe our framework to unify gradient tracking methods. We then develop an algorithm over the framework that incorporates the flexibility of performing multiple communication and multiple computation steps. 

The %\rb{Our} 
iterate update form for the decision variable $\xmbf \in \mathbb{R}^{nd}$ and the auxiliary variable $\ymbf \in \mathbb{R}^{nd}$ that we propose to unify gradient tracking methods is  (for all $k\geq0$)
\begin{equation}\label{eq: general_form}
\begin{aligned}
    \xmbf_{k+1, 1} & = \Zmbf_1 \xmbf_{k, 1} - \alpha \Zmbf_2 \ymbf_{k,1} \\ 
    \ymbf_{k+1, 1} & = \Zmbf_3 \ymbf_{k, 1} + \Zmbf_4 (\nabla \fmbf(\xmbf_{k+1, 1}) - \nabla \fmbf(\xmbf_{k, 1})), 
\end{aligned}
\end{equation}
where $\alpha>0$ is the constant step size, $\Zmbf_i = \Wmbf_i \otimes I_d \in \mathbb{R}^{nd \times nd}$, for $i = 1, 2, 3, 4$ and $\Wmbf_i \in \mathbb{R}^{n \times n}$ are communication matrices. A communication matrix, $\Umbf \in \mathbb{R}^{n \times n}$ is a symmetric, doubly stochastic matrix that respects the connectivity of the network, i.e., $u_{ii} > 0$ and $u_{ij}  \geq  0$ ($i \neq j$) if $(i,j) \in \mathcal{E}$ and $u_{ij} = 0$ ($i \neq j$) if $(i,j) \notin \mathcal{E}$. %\asb{Thus, a communication matrix represents a network topology consisting of all nodes and \rb{(a subset of) }the edges present in the network}. The communication matrices, $\Wmbf_i$ for $i = 1, 2, 3, 4$, represent four (possibly different) network topologies over which the corresponding vectors are communicated. 
The communication matrices, $\Wmbf_i$ for $i = 1, 2, 3, 4$, represent four (possibly different) network topologies consisting of all the nodes and (possibly different) subsets of the edges of the network over which the corresponding vectors are communicated.
% $\Wmbf_1$, $\Wmbf_2$, $\Wmbf_3$ and $\Wmbf_4$ are communication matrices and $\Zmbf_i = \Wmbf_i \otimes I_d$, for  %\,\, \forall \,\, 
% $i = 1, 2, 3, 4$. A communication matrix, $\Umbf \in \mathbb{R}^{n \times n}$, is a symmetric, doubly stochastic matrix that respects the connectivity of the network, \asb{i.e.,} $u_{ii} > 0$ and $u_{ij} = 0$ if $(i,j) \notin \mathcal{E}$. Thus, a communication matrix represents a network topology consisting of all nodes and a subset of the edges (or all edges) present in the network. The four communication matrices, $\Wmbf_1$, $\Wmbf_2$, $\Wmbf_3$ and $\Wmbf_4$, represent four (possibly different) network topologies over which the corresponding vectors are communicated. 
% This allows one to represent all ATC and CTA based communication strategies seen in literature using \eqref{eq: general_form}. Some of these cases have been shown in \cref{tab: Algorithm Def} where $\Wmbf$ is the mixing matrix for the connected network.
The update form given in \eqref{eq: general_form} generalizes many popular gradient tracking methods for different choices of the communication matrices; see \cref{tab: Algorithm Def}. In \eqref{eq: general_form} one communication and one computation step is performed at every iteration and so the inner iteration index is always $1$. %For consistency with the presentation of the algorithm and analysis with multiple communication and computation steps we include this subscript.}\rb{(How about.)
We include this subscript for consistency with the presentation of the algorithm and analysis with multiple communication and computation steps.
% \sg{In \cref{sec.theory}, we show that ensuring $\Wmbf_1$ and $\Wmbf_3$ to represent a connected (possibly different) network is sufficient for convergence to the solution with respect to the communication strategy. The update form \eqref{eq: general_form} can represent all communication strategies for gradient tracking methods seen in literature. Some examples have been shown in \cref{tab: Algorithm Def} where $\Wmbf$ is the mixing matrix of a connected network.
% As the update form \eqref{eq: general_form} only performs one computation step, the inner iteration index stays at one. We use the inner iteration index later in the section to represent multiple computation steps.}

\begin{table}[]\centering
\caption{Special cases of Gradient Tracking Algorithm (\texttt{GTA}).  %$\Wmbf$ is a mixing matrix.}
% Special cases of Gradient Tracking Algorithm (\texttt{GTA}).
}\label{tab: Algorithm Def}
\begin{tabular}{l*{4}{>{\centering\arraybackslash}p{0.8cm}}c}\toprule
\multirow{2}{*}{Method} &\multicolumn{4}{c}{Communication Matrices} & Algorithms in literature\\\cmidrule{2-5}
&$\Wmbf_1$&$\Wmbf_2$&$\Wmbf_3$&$\Wmbf_4$& $(n_c = n_g = 1)$\\\midrule
\texttt{GTA-1} &$\Wmbf$ &$I_n$ &$\Wmbf$ &$I_n$& DIGing \cite{nedic2017achieving}, EXTRA\footnotemark[1] \cite{shi2015extra},  \\\hdashline
\texttt{GTA-2} &$\Wmbf$ &$\Wmbf$ &$\Wmbf$ &$I_n$ & SONATA \cite{sun2022distributed}, NEXT \cite{di2016next,pu2020push} \\\hdashline
\texttt{GTA-3} &$\Wmbf$ &$\Wmbf$ &$\Wmbf$ &$\Wmbf$ & Aug-DGM \cite{xu2015augmented}, ATC-DIGing \cite{nedic2017geometrically}\\ 
\bottomrule
\end{tabular}
Note: $\Wmbf$ is a mixing matrix.
\end{table}

% \footnotetext[1]{EXTRA is a special case of \texttt{GTA-1} with $n_c = n_g = 1$ and $\Wmbf_1 = 2\Wmbf - I_n$ and $\Wmbf_3 = \Wmbf^2$.}



% \texttt{GTA-1} with $n_c = n_g = 1$ is a special case of EXTRA where $$

% mixing matrix in EXTRA is set to $(2\Wmbf - I_n)$ and the second is set to $\Wmbf^2$.}

We incorporate multiple communications in \eqref{eq: general_form} by replacing $\Zmbf_i$ with $\Zmbf_i^{n_c} = \Wmbf_i^{n_c} \otimes I_d$ for $i=1, 2, 3, 4$, where $n_c \geq 1$ is the number of communication steps at each iteration  \cite{berahas2018balancing}. Taking the communication matrices to the $n_c$ power represents performing $n_c$ communication (consensus) steps at every iteration. We further extend \eqref{eq: general_form} to incorporate multiple computation steps at each iteration. That is, the algorithm performs multiple local updates before communicating information with local neighbors. Our full algorithmic framework with flexibility in the number of communication and computation steps, i.e., $n_c \geq 1$ and $n_g \geq 1$, is given in \cref{alg : Deterministic}. A balance between the number of communication and computation steps is required to achieve overall efficiency for different applications, and \texttt{GTA} allows for such flexibility in these steps via the parameters $n_g$ and $n_c$.




% old version
% We incorporate multiple communications in \eqref{eq: general_form} by replacing $\Zmbf_i$ with $\Zmbf_i^{n_c} = \Wmbf_i^{n_c} \otimes I_d$ for all $i=1, 2, 3, 4$, where $n_c \geq 1$ is the number of communication steps each iteration, \cite{berahas2018balancing}. Taking power $n_c$ of the communication matrices represents performing $n_c$ consensus steps every iteration. This improves the consensus of the communicated vector across the network. We further extend \eqref{eq: general_form} to incorporate multiple computation steps and present \texttt{GTA} (\cref{alg : Deterministic}). 
% \sg{The algorithm performs $n_g \geq 1$ computation steps each iteration by performing multiple updates using only local information. If the local auxiliary variable is a good estimate of the global objective gradient, performing multiple computation steps using only local information will drive the local iterate closer to the global optimal solution without distorting the consensus error too much.}
% A balance between the number of communication and computation steps is required to achieve overall efficiency and \texttt{GTA} allows for such flexibility in these steps using the parameters $n_g$ and $n_c$.

\begin{algorithm}[H]
    \caption{\texttt{GTA}: Gradient Tracking Algorithm}
    \textbf{Inputs:} initial point $\xmbf_{0, 1} \in \R{nd}$, step size $\alpha$, computations $n_g$, communications $n_c$
    \begin{algorithmic}[1]
        \State $\textbf{y}_{0, 1} \gets \nabla \textbf{f}(\textbf{x}_{0, 1})$
        \For{$k \gets 0, 1, 2$ ... }    
            \If{$n_g > 1$}
                \For{$j \gets 1, 2$ ... $, n_g-1$}
                    \State $\textbf{x}_{k, j+1} \gets \textbf{x}_{k, j} - \alpha \,\textbf{y}_{k, j}$
                    \State $\textbf{y}_{k, j+1} \gets \textbf{y}_{k, j} + \nabla \textbf{f}(\textbf{x}_{k, j+1})  - \nabla \textbf{f}(\textbf{x}_{k, j})$
                \EndFor
            \EndIf
            
            \State $\textbf{x}_{k+1, 1} \gets \textbf{Z}_1^{n_c} \textbf{x}_{k, n_g} - \alpha \, \textbf{Z}_2^{n_c} \textbf{y}_{k, n_g}$
            \State $\textbf{y}_{k+1, 1} \gets \textbf{Z}_3^{n_c} \textbf{y}_{k, n_g} + \textbf{Z}_4^{n_c}(\nabla \textbf{f}(\textbf{x}_{k+1, 1})  - \nabla \textbf{f}(\textbf{x}_{k, n_g}))$
        \EndFor
    \end{algorithmic}
    \label{alg : Deterministic}
\end{algorithm}
\bremark 
We make the following remarks about \cref{alg : Deterministic}. 
\begin{itemize}
    \item \textbf{Communications and Computations:} The number of communication and computation steps are dictated by $n_c$ and $n_g$, respectively. Suppose $n_g =n_c = 1$, in this setting, at every outer iteration, each node computes a local gradient and communicates across the network. By performing multiple communication steps, the goal is to improve consensus across the local decision variables. By performing multiple computation steps, the goal is for individual nodes to make more progress on their local objective functions. %\rb{If the local auxiliary variable (\textbf{y}) is a good estimate of the global objective gradient, performing multiple computation steps using only local information will drive the local iterate closer to the global optimal solution without distorting the consensus error too much.}
    \item \textbf{Inner and Outer Loops:} Lines 2--8 form the outer loop and Lines 4--6 form the inner loop. The algorithm performs $n_c$ communication steps each outer iteration (Lines 7 and 8). The algorithm performs $n_g$ local (gradient) computations at each outer iteration; $n_g-1$ computations in the inner loop (Line 6, $\nabla \fmbf(\xmbf_{k, j+1})$) and one computation in the outer loop (Line 8, $\nabla \fmbf(\xmbf_{k+1, 1})$). 
    %\rb{$n_c$ communication steps are performed each outer iteration (Lines 7 and 8) and $n_g$ local (gradient) computations at each outer iteration; $n_g-1$ computations in the inner loop (Line 6, $\nabla \fmbf(\xmbf_{k, j+1})$) and one computation in the outer loop (Line 8, $\nabla \fmbf(\xmbf_{k+1, 1})$) }
    The inner loop is only executed if more than one computation is to be performed every outer iteration (Line 3). By default, we refer to outer iterations when we say iterations unless otherwise specified.
    % - Lines 2--8 form the outer loop. The algorithm performs $n_c$ communication steps each outer iteration in lines 7--8.  
    % The algorithm performs $n_g$ local gradient computations each outer iteration with one computation in line 8 $(\nabla \fmbf(\xmbf_{k+1, 1}))$ and $n_g - 1$ computations in the inner loop (lines 4-6, $(\nabla \fmbf(\xmbf_{k, j+1}))$). The inner loop is only executed if more than one computation is to be performed every outer iteration (line 3).  \sg{By default, we refer to outer iterations when we say iterations unless specified.}
    \item \textbf{Step size ($\alpha>0$):} The algorithm employs a constant step size that depends on the problem parameters, the choices of $n_c$ and $n_g$, and the communication strategy, i.e., $\Wmbf_i$ for $i = 1, 2, 3, 4$. 
    % Increase in communication steps per iteration ($n_c$) allows employing a larger stepsize. Increase in the computation steps per iteration ($n_g$) requires the stepsize to be reduced at a polynomial rate.
    % \item \textbf{Communication Strategy} - The communication matrices employed in lines 7--8 allow for various communication strategies to be employed. If $\Wmbf_1$ and $\Wmbf_3$ represent connected networks, the algorithm can show linear rate of convergence for any composition of communication and computation steps.
\end{itemize} 
\eremark

We analyze \texttt{GTA} and provide results for several popular communication strategies as special cases; summarized in \cref{tab: Algorithm Def}. The choice of the communication matrices ($\Wmbf_i$ for $i = 1, 2, 3, 4$), or equivalently the communication strategy, impact both the convergence of the algorithm and practical implementation. Notice that all methods in \cref{tab: Algorithm Def} require that $\Wmbf_1$ and $\Wmbf_3$ are mixing matrices. Our theoretical results recover this for the general framework. Consider \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3} defined in \cref{tab: Algorithm Def} with $n_g=1$. In \texttt{GTA-1} and \texttt{GTA-2}, computing local gradients and communications can be performed in parallel because the local gradients need not be communicated ($\Wmbf_4 = I_n$). On the other hand, in \texttt{GTA-3}, these steps need to be performed sequentially. Such trade-offs can create significant impact depending on the problem setting and system.
% Consider instances \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3} from \cref{tab: Algorithm Def} when $n_g=1$. Computing local gradients and communications can be performed in parallel in \texttt{GTA-1} and \texttt{GTA-2}. \texttt{GTA-3} requires performing communication and computation sequentially. 
% Depending on properties of the problem and system, such trade-offs can create significant impacts. 

% The iterate update form \eqref{eq: general_form} also represents \emph{heterogeneous} communication strategies; sharing different pieces of information across a different subset of network edges. Heterogeneous strategies can be useful in applying gradient tracking methods to decentralized settings in networks with bandwidth limitation such as cyberphysical systems with battery powered wireless sensors (\cite{magnusson2017bandwidth}). These sensors communicate in short packets for efficiency to extend lifetime. Such strategies can be represented by appropriate communication matrices using \eqref{eq: general_form}. In \cref{sec.theory}, we establish conditions on the communication matrices to ensure linear convergence of the iterates generated by unifying framework. Specifically, ensuring $\Wmbf_1$ and $\Wmbf_3$ to represent a connected (possibly different) network is sufficient to guarantee linear convergence to the solution.



% \texttt{GTA} converges to the solution at a linear rate employing a constant stepsize given $\Wmbf_1$ and $\Wmbf_3$ represent connected networks for any composition of communication and computation steps at each iteration. Increasing $n_c$ (communication steps) allows a larger stepsize to be employed providing accelerated convergence. Increase in $n_g$ (computation steps) in \texttt{GTA} requires the stepsize to be reduced at a polynomial rate. While the effect of $n_g$ on convergence rate is not clear, we empirically find it brings acceleration to convergence. 

As mentioned above, the communication matrices ($\Wmbf_i$ for $i = 1, 2, 3, 4$) in \texttt{GTA} need not be the same. That is,  different information can be exchanged on subsets of the edges of the network. This allows for a flexibility in the communication strategy that current gradient tracking methodologies do not possess. 
%The algorithmic framework \texttt{GTA} can also incorporate \rb{\emph{heterogeneous}} communication strategies, i.e., asymmetric sharing of information across different subsets of network edges. 
Such strategies can be useful in applying gradient tracking methods to decentralized settings with networks with bandwidth limitations, e.g., optimization problems in cyberphysical systems with battery powered wireless sensors \cite{magnusson2017bandwidth}. 

% Old version
% \sg{The iterate update form \eqref{eq: general_form} and hence \texttt{GTA} can also represents \emph{heterogeneous} communication strategies; sharing different pieces of information across a different subset of network edges. Such strategies can be represented using appropriate communication matrices as they can represent different network topologies. Heterogeneous strategies can be useful in applying gradient tracking methods to decentralized settings in networks with bandwidth limitation. For example, optimization problems in cyberphysical systems with battery powered wireless sensors (\cite{magnusson2017bandwidth}) as these sensors communicate in short packets for efficiency to extend lifetime.}

% using \eqref{eq: general_form}. In \cref{sec.theory}, we establish conditions on the communication matrices to ensure linear convergence of the iterates generated by unifying framework.}

