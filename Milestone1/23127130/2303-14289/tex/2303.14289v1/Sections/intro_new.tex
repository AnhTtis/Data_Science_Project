\section{Introduction} \label{sec.intro}

We consider the problem of minimizing a function over a network. In this setting, each node of the network has a portion of the global objective function and the edges represent neighbor nodes that can exchange information, i.e., communicate. The goal is to collectively minimize a finite sum of functions where each component is only known to one of the $n$ nodes (or agents) of the network. Such problems arise in many application areas such as machine learning \cite{forero2010consensus,tsianos2012consensus}, sensor networks \cite{baingana2014proximal, predd2007distributed}, multi-agent coordination \cite{cao2012overview, zhou2011multirobot} and signal processing \cite{combettes2011proximal}. The problem, known as a \emph{decentralized optimization} problem, can be represented as follows:
% We consider the problem of minimizing a function over a network. Each node of the network has a portion of the global objective function and the edges represent \asb{neighbor} nodes that can \asb{exchange information, i.e., communicate}. Such problems arise in many application areas such as machine learning \cite{forero2010consensus,tsianos2012consensus}, sensor networks \cite{baingana2014proximal, predd2007distributed}, multi-agent coordination \cite{cao2012overview, zhou2011multirobot} and signal processing \cite{combettes2011proximal}. The goal is to collectively minimize a finite sum of functions where each component is only known to one of the $n$ nodes (or agents) of the network. This problem, known as a \emph{decentralized optimization} problem, can be represented as follows:
\begin{align}		\label{eq:prob}
	\min_{x\in \mathbb{R}^d}\quad f(x) = \frac{1}{n} \sum_{i=1}^n f_i(x),
\end{align}
where $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is the global objective function, $f_i: \mathbb{R}^d \rightarrow \mathbb{R}$ for each $i\in \{1,2,...,n \}$ is the local objective function known only to node $i$ and $x\in \mathbb{R}^d$ is the decision variable.

To decouple the computation across different nodes, \eqref{eq:prob} is often reformulated as (see e.g., \cite{bertsekas2015parallel,nedic2009distributed})
\begin{equation}\label{eq:cons_prob}
\begin{aligned}	
	\min_{x_i \in \mathbb{R}^d}&\quad \frac{1}{n} \sum_{i=1}^n f_i(x_i)\\
	 \text{s.t.} &\quad  x_i = x_j, \quad \forall \,\, (i, j) \in \mathcal{E},
\end{aligned}
\end{equation}
where $x_i \in \mathbb{R}^d$ for each node $i\in \{1,2,...,n \}$ is a local copy of the decision variable, and  $\mathcal{E}$ denotes the set of edges of the network. If the underlying network is connected, the \emph{consensus} constraint ensures that all local copies are equal, and, thus, problems \eqref{eq:prob} and \eqref{eq:cons_prob} are equivalent. For compactness, we express problem \eqref{eq:cons_prob} as
\begin{equation}\label{eq:cons_prob1}
\begin{aligned}		
	\min_{x_i \in \mathbb{R}^d}&\quad \textbf{f} (\textbf{x}) = \frac{1}{n} \sum_{i=1}^n f_i(x_i)\\
	\text{s.t.} & \quad (\textbf{W}\otimes I_d)\textbf{x} = \textbf{x}, 
\end{aligned}
\end{equation}
where $\textbf{x} \in \mathbb{R}^{nd}$ is a concatenation of local copies $x_i$, $\textbf{W} \in \mathbb{R}^{n \times n}$ is a matrix that captures the connectivity of the underlying network, $I_d \in \mathbb{R}^{d \times d}$ is the identity matrix of dimension $d$, and the operator $\otimes$ denotes the Kronecker product,  $\textbf{W}\otimes I_d \in \mathbb{R}^{nd \times nd}$. The matrix $\textbf{W}$, known as the \emph{mixing} matrix, is a symmetric, doubly-stochastic matrix with $w_{ii}>0$ and $w_{ij}>0$ ($i\neq j$) if and only if $(i, j) \in \mathcal{E}$ in the underlying network. This matrix ensures that $(\textbf{W}\otimes I_d) \textbf{x}=\textbf{x}$ if and only if $x_i=x_j \,\, \forall \,\, (i, j) \in \mathcal{E}$ in the connected network, thus, % making 
\eqref{eq:cons_prob} and \eqref{eq:cons_prob1} are equivalent.% problems.

In this paper, we focus on gradient tracking methods. These first-order methods update and communicate the local decision variables, and also maintain, update and communicate an additional auxiliary variable that estimates (tracks) the gradient of the global objective function.
%\rb{These first-order methods update and communicate the local decision variables, and an additional auxiliary variable that estimates (tracks) the gradient of the global objective function.  }
%average gradient across all the nodes.} 
%These methods maintain an auxiliary variable ($y_i$) that estimates the average gradient across the network in addition to the decision variable ($x_i$) at each agent (node) $i$. 
We refer to the information shared by the methods as the communication strategy. When applied to the same decentralized setting, the theoretical convergence guarantees and practical implementations of gradient tracking methods with different communication strategies can vary significantly. %When applied to the same decentralized setting, the theoretical convergence guarantees and practical implementations of the methods vary with respect to the communication strategy. 
We propose an algorithmic framework that unifies communication strategies in gradient tracking methods and that allows for a direct theoretical and empirical comparison. The framework recovers popular gradient tracking methods as special cases.

The update form of gradient tracking methods can be generalized and decomposed as: $(1)$ one \emph{computation step} of calculating the local gradients, and $(2)$ one \emph{communication step} of sharing information based on the communication strategy. In practice, the complexity of these two steps can vary significantly across applications. For example, a large-scale machine learning problem solved on a cluster of computers with shared memory access has a higher cost of computation than communication \cite{tsianos2012consensus}. On the other hand, optimally allocating channels over a wireless sensor network requires economic usage of communications due to limited battery power \cite{magnusson2017bandwidth}.
% The same being solved for data stored over distant data servers would have a higher cost of communication than computation. 
The subject of developing algorithms (and convergence guarantees) that balance these costs has received significant attention in recent years; see e.g.,~\cite{chen2012fast,berahas2018balancing,9479747,berahas2019nested,sayed2014diffusion,zhang2018communication} and the references therein. In this paper, we follow the approach used in~\cite{berahas2018balancing} and %. That is, we 
explicitly decompose the two steps. %, and so o
As a result, our algorithms are endowed with flexibility in terms of the number of communication and computation steps performed at each iteration. We show the benefits of this flexibility theoretically and empirically.

\subsection{Literature Review} \label{sec.lit}

Decentralized Gradient Descent (DGD) \cite{bertsekas2015parallel, nedic2009distributed}, a primal first-order method, is considered the prototypical method for solving~\eqref{eq:prob}. %The DGD method 
At each iteration nodes perform local computations and communicate local decision variable to neighbors. 
%that employs a CTA \sg{(Combine-then-Adapt \cite{sayed2014diffusion})} communication strategy. 
% Under reasonable assumptions, with a constant step size, it converges to a neighbourhood of the solution \cite{yuan2016convergence}. 
Gradient tracking methods, e.g., EXTRA \cite{shi2015extra}, SONATA \cite{sun2022distributed}, NEXT \cite{di2016next}, DIGing \cite{nedic2017achieving}, Aug-DGM \cite{xu2015augmented}, have emerged as popular alternatives due to their superior theoretical guarantees and empirical performance. %as they converge to the solution with a constant step size.}
% have emerged as popular alternatives as they avoid the aforementioned shortcomings. 
They maintain, update and communicate an additional auxiliary variable that tracks the average gradient (additional communication cost compared to DGD). These methods are usually applied to smooth convex functions over undirected networks; however, they are also applicable to various other settings such as time varying networks \cite{nedic2017achieving}, uncoordinated stepsizes \cite{nedic2017geometrically, xu2015augmented}, directed networks \cite{nedic2017achieving, pu2020push}, nonconvex functions \cite{di2016next, sun2022distributed} %, sun2016distributed} 
and stochastic gradients \cite{pu2021distributed}. Our algorithmic framework generalizes and extends current gradient tracking methodologies, allowing for a unified analysis and direct comparison of popular methods. In \cite{sundararajan2017robust,zhang2019computational}, semi-definite programming is used to unify communication strategies in gradient tracking methods. 
% old version
% the authors presented a method to unify CTA and ATC \sg{(Adapt-then-Combine \cite{sayed2014diffusion})} communication strategies in gradient tracking \sg{methods} using semi-definite programming. 
Our framework is simpler and allows for more general communication strategies than those in \cite{sundararajan2017robust,zhang2019computational}.

% also encompassesheterogeneous communication strategies \rb{which allow for asymmetric sharing of information across different subsets of the edges in the network.}

Another class of popular methods is %that \sg{can converge to the solution with a constant stepsize are}
% mitigate the shortcomings of DGD are 
primal-dual methods \cite{arjevani2020ideal, jakovetic2014linear, ling2015dlm, shi2014linear, wei20131, mansoori2021flexpd, mancino2021decentralized}. Of these methods, Flex-PD \cite{mansoori2021flexpd} and ADAPD \cite{mancino2021decentralized} allow for flexibility with respect to the number of communication and computation steps. That said, Flex-PD \cite{mansoori2021flexpd} does not show improved performance with the employment of the flexibility and ADAPD \cite{mancino2021decentralized} does not allow for a balance between communication and computation. 
%, however, \rb{these methods either fail to show improved performance with the employment of the flexibility or do not provide the precision to specify the exact balance between communication and computation as does our framework.}
% these methods fail to simultaneously provide improvement with the employment of the flexibility and the exact specification of communication and computation budget balance in iterations like our algorithm. 
Finally, algorithms that consider the consensus constraint as a proximal operator have been proposed. These algorithms aim to reduce communication load on distributed systems via a randomization scheme but are primarily designed for fully connected networks (all pairs of nodes are connected). %networks where all pairs of nodes are connected (fully connected networks). 
Examples of such methods include, but are not limited to, Scaffnew \cite{mishchenko2022proxskip}, FedAvg \cite{li2019convergence}, Scaffold \cite{karimireddy2020scaffold}, Local-SGD \cite{gorbunov2021local} and FedLin \cite{mitra2021linear}.


% Old version
% Another class of methods that achieve linear convergence to the solution are \asb{primal-dual} methods that work with the Lagrangian function \cite{arjevani2020ideal, jakovetic2014linear, ling2015dlm, shi2014linear, wei20131, mansoori2021flexpd, mancino2021decentralized}. Flex-PD \cite{mansoori2021flexpd} and ADAPD \cite{mancino2021decentralized}  build upon the idea of augmented lagrangian and decentralised ADMM to provide the flexibility in number of communication and computation steps. These methods fail to simultaneously provide improvement with the employment of the flexibility and the exact specification of communication and computation budget balance in iterations like our algorithm. 

% Another approach to this problem is looking at the consensus constraint as a proximal operator. These algorithms aim to reduce communication load on distributed systems but are designed for networks where all pairs of nodes are connected. Algorithms such as Scaffnew \cite{mishchenko2022proxskip}, FedAvg \cite{li2019convergence}, Scaffold \cite{karimireddy2020scaffold}, Local-SGD \cite{gorbunov2021local}, FedLin \cite{mitra2021linear} achieve this by performing communications less often decided via a randomized scheme while continuously performing local updates.

% This paper is part of the growing literature in decentralized optimization. One of the first algorithms to solve problem \eqref{eq:cons_prob1} is Distributed Gradient Descent (DGD,  \cite{nedic2009distributed, yuan2016convergence}), a first order primal iterative method with a CTA communication structure. It has been shown to converge to a neighbourhood of the solution under a constant stepsize when the problem is smooth and convex. In \cite{berahas2018balancing} and \cite{9479747}, the authors introduced variants of DGD with an ATC communication strategy. These variants show convergence to the solution every iteration under a constant step size when communications are increased every iteration, not possible with DGD. Algorithms with similar neighbourhood convergence results can be found in \cite{mokhtari2016network, sayed2014diffusion}.

% Gradient tracking methods such as EXTRA \cite{shi2015extra}, SONATA \cite{sun2022distributed}, NEXT \cite{di2016next}, DIGing \cite{nedic2017achieving}, Aug-DGM \cite{xu2015augmented} also collectively referred to as push-pull algorithms \cite{pu2020push} are first order primal methods. They use an auxiliary variable to track the average gradient across the network. These algorithms achieve a linear rate of convergence to the solution with a constant stepsize. Usually applied to smooth convex functions over undirected networks, they are also applicable to various other settings like time varying networks \cite{nedic2017achieving}, uncoordinated stepsizes \cite{nedic2017geometrically, pu2020push, xu2015augmented}, directed networks \cite{nedic2017achieving, pu2020push}, non convex functions \cite{di2016next, sun2022distributed, sun2016distributed} and stochastic gradients \cite{pu2021distributed} and randomized communication \cite{pu2021distributed}. In this paper we unify and further generalise the communication strategies of gradient tracking algorithms present in literature while restricting ourselves to smooth convex functions over undirected graphs with deterministic gradients. We also introduce flexibility in terms of number of computation and communication steps in iterations in the general framework. In \cite{sundararajan2017robust} and \cite{zhang2019computational}, the authors presented a method to unify all CTA and ATC diffusion strategies in gradient tracking algorithms using semi-definite programming. Our framework is simpler and also able to encompass heterogeneous communication strategies where all information is not communicated among the same neighbors.

% Another class of methods that achieve linear convergence to the solution are primal dual methods based on the idea of lagrangian dualitly and alternating method of multipliers (ADMM) \cite{arjevani2020ideal, jakovetic2014linear, ling2015dlm, shi2014linear, wei20131}. Algorithms such as Flex-PD \cite{mansoori2021flexpd} and ADAPD \cite{mancino2021decentralized}  have built upon the idea of augmented lagrangian and decentralised ADMM to provide the flexibility in number of communication and computation steps. 

% While Flex-PD works for smooth strongly convex functions, it does not provide the flexibility to manipulate both communications and computations at the same time while our framework does. Flex-PD shows an exponential decrease in stepsize requirement with increase in both communications and computation in an iteration. Our algorithm shows an improved stepsize and convergence rate with increase in communication. Our algorithm shows a polynomial decrease in the stepsize with increased computation. ADAPD is a framework for non convex problems. It shows improvement in performance with multiple communications. Multiple computations are performed within ADAPD in terms of solving a local problem to a certain accuracy. Our method allows to specify the exact number of computations being performed at each node. This allows greater control over the balance between computation and communication to the user based on the systems properties. ADAPD framework also does not empirically show any improvements in terms of iterations when multiple computations are performed while ours does.

% Another approach to this problem is looking at the consensus constraint as a proximal operator. These algorithms aim to reduce communication load on distributed systems but are designed for fully connected networks where all pair of nodes are connected. Algorithms such as Scaffnew \cite{mishchenko2022proxskip}, FedAvg \cite{li2019convergence}, Scaffold \cite{karimireddy2020scaffold}, Local-SGD \cite{gorbunov2021local}, FedLin \cite{mitra2021linear} achieve this by performing communications less often while continuously performing local updates. The decision to whether or not communicate in an iteration is determined using either a deterministic or randomized sequence. The iterate update is usually accompanied by a correction term to correct for the bias from local updates. In this paper, we propose a deterministic method to introduce flexibility in number of communication and computation steps for any connectivity of the network.

\subsection{Contributions} \label{sec.contri}
We summarize our main contributions as follows:
\begin{enumerate}
	\item We propose a gradient tracking algorithmic framework (\texttt{GTA}) that unifies communication strategies in gradient tracking methods and provides flexibility in the number of communication and computation steps performed at each iteration. The framework recovers as special cases popular gradient tracking methods, i.e., ~\texttt{GTA-1} \cite{shi2015extra, nedic2017achieving}, \texttt{GTA-2} \cite{di2016next, sun2022distributed} and \texttt{GTA-3} \cite{nedic2017geometrically, xu2015augmented}; see \cref{tab: Algorithm Def}.
 % The framework recovers as special cases popular methods, i.e., ~\texttt{GTA-1} \cite[EXTRA]{shi2015extra}, \cite[DIGing]{nedic2017achieving}, \texttt{GTA-2} \cite[NEXT]{di2016next}, \cite[SONATA]{sun2022distributed} and \texttt{GTA-3} \cite[ATC-Digging]{nedic2017geometrically}, \cite[Aug-DGM]{xu2015augmented}; see \cref{tab: Algorithm Def}.
    \item We establish the conditions required, on the communication strategy and the step size parameter, that %to
    guarantee a global linear rate of convergence for \texttt{GTA} with multiple communication and multiple computation steps. 
    %multiple communication and single computation steps (\cref{th. general g=1 step cond}), and multiple communication and multiple computation steps (\cref{th. alpha bound g > 1}). 
    We also compare the relative performance of the special case gradient tracking algorithms, and illustrate the theoretical advantages of \texttt{GTA-3} over \texttt{GTA-2} (and \texttt{GTA-2} over \texttt{GTA-1}), a direct comparison not established in prior literature. %show that the rate constant of \texttt{GTA-3} is better than \texttt{GTA-2} which is better than \texttt{GTA-1}, a comparison that has not been established in prior literature.
    \item We show that the rate of convergence improves %rate constants improve 
    with increasing the number of communication steps, and the extent of improvement depends on the communication strategy. % (\cref{th.incr rates g > 1}). 
    %We show that t
    The improvements are much more profound in \texttt{GTA-3} as compared to \texttt{GTA-2} and \texttt{GTA-1}. %(Corollary~\ref{col. g=1 rate bound}) in explicit form when a single computation step is performed in each iteration.
    \item We illustrate the empirical performance of the proposed \texttt{GTA} framework on quadratic and binary classification logistic regression problems. We show the effect and benefits of %performing 
    multiple communication and/or computation steps per iteration on the performance of the special case algorithms.
\end{enumerate}

\subsection{Notation} \label{sec.notation}
Our proposed algorithmic framework is iterative and works with inner and outer loops. The variables $x_{i, k, j} \in \mathbb{R}^d$ and $y_{i, k, j} \in \mathbb{R}^d$ denote the local copies %copy 
of the decision variable and the auxiliary variable, respectively, of node $i$, in outer iteration $k$ and inner iteration $j$. The average of all local decision variables and local auxiliary variables are denoted by $\bar{x}_{k, j} = \frac{1}{n} \sum_{i=1}^n x_{i, k, j}$ and $\bar{y}_{k, j} = \frac{1}{n} \sum_{i=1}^n y_{i, k, j}$, respectively. Boldface lowercase letters represent concatenated vectors of local copies
\begin{align*}
    \xmbf_{k, j} = 
    \begin{bmatrix}
        x_{1, k, j}\\
        x_{2, k, j}\\
        \vdots \\
        x_{n, k, j}
    \end{bmatrix} \in \mathbb{R}^{nd}\mbox{,} \quad
    \ymbf_{k, j} = 
    \begin{bmatrix}
        y_{1, k, j}\\
        y_{2, k, j}\\
        \vdots \\
        y_{n, k, j}
    \end{bmatrix} \in \mathbb{R}^{nd}
    \mbox{,} \quad
      \nabla \fmbf(\xmbf_{k, j}) = 
    \begin{bmatrix}
        \nabla f_1(x_{1, k, j})\\
        \nabla f_2(x_{2, k, j})\\
        \vdots \\
        \nabla f_n(x_{n, k, j})
    \end{bmatrix} \in \mathbb{R}^{nd}.%,
\end{align*}
%in outer iteration $k$ and inner iteration $j$. 
The concatenated vector of the average of decision variables ($\Bar{x}_{k, j}$) and auxiliary variables ($\Bar{y}_{k, j}$) repeated $n$ times is denoted by $\xbb_{k, j}$ and $\ybb_{k, j}$, respectively. 
% The concatenated vector of the average of decision variables and auxiliary variables in outer iteration $k$ and inner iteration $j$ repeated $n$ times is denoted by $\xbb_{k, j}$ and $\ybb_{k, j}$ respectively. 
The $n$ dimensional vector of all ones is denoted by $1_n$ and the identity matrix of dimension $n$ is denoted by $I_n$. The spectral radius of square matrix $A$ is $\rho(A)$. Matrix inequalities are defined component wise. %, i.e., $A \geq B$ implies every element in matrix A is greater than or equal to the corresponding element in matrix $B$. 
The Kronecker product of any two matrices $A \in \mathbb{R}^{n \times n}$ and $B \in \mathbb{R}^{d \times d}$ is represented using the operator $\otimes$ and denoted as $A \otimes B \in \mathbb{R}^{nd \times nd}$.

\subsection{Paper Organization} In \cref{sec.methods}, we describe our proposed gradient tracking algorithmic framework (\texttt{GTA}). In \cref{sec.theory}, we provide theoretical convergence guarantees for the proposed algorithmic framework for multiple communication steps and a single computation step at each iteration (\cref{sec.mult comms}) and multiple communication and computation steps at each iteration (\cref{sec.mult grads}). In \cref{sec.full graph res}, we consider the special case %(not captured in \cref{sec.mult comms,sec.mult grads}) 
of fully connected networks. Numerical experiments on quadratic and binary classification logistic regression problems %that illustrate the advantages of the flexibility 
are presented in \cref{sec.num_exp}. Finally, we provide concluding remarks in \cref{sec.conc}.

% Old version
%In \cref{sec.methods}, we describe our proposed algorithm to unify communication strategies in gradient tracking methods with the flexibility in number of communication and computation steps. In \cref{sec.theory}, we provide theoretical convergence guarantees for the proposed algorithm. We first analyse the algorithm with multiple communication steps and a single computation step being performed in each iteration in \cref{sec.mult comms}. We extend the analysis to multiple communication and computation steps in \cref{sec.mult grads}. In \cref{sec.full graph res}, we analyse the algorithm under a fully connected network as it is a special case not covered by \cref{sec.mult comms} and \cref{sec.mult grads}. In \cref{sec.num_exp}, we illustrate the performance of the algorithm over  quadratic functions and binary logistic regression. Finally, we provide concluding remarks and future directions in \cref{sec.conc}.