\subsection{\texttt{GTA} with multiple communication \texorpdfstring{($n_c \geq 1, n_g=1$)}{Lg}} \label{sec.mult comms} In this section, we analyse \texttt{GTA} when only one computation step is performed per iteration. % i.e., $n_g = 1$. We use this special case to introduce the analysis as it is simpler to follow, and extend it to the case where $n_g \geq 1$ in \cref{sec.mult grads}. 
In this setting ($n_g=1$), the inner loop (Lines 4--6 in \cref{alg : Deterministic}) is never executed. Thus, the inner iteration counter in \texttt{GTA} can be ignored and the iteration simplifies to 
\begin{equation}   \label{eq : g=1 general form}
\begin{aligned}
    \xmbf_{k+1} & = \Zmbf_1^{n_c} \xmbf_k - \alpha \Zmbf_2^{n_c} \ymbf_k,   \\ 
    \ymbf_{k+1} & = \Zmbf_3^{n_c} \ymbf_k + \Zmbf_4^{n_c} (\nabla \fmbf(\xmbf_{k+1}) - \nabla \fmbf(\xmbf_{k})).
\end{aligned}
\end{equation}
We note that throughout this subsection we drop the subscript related to the inner iteration $j$, i.e., $x_{i,k,j}$ is denoted as $x_{i,k}$ (since $j=1$), and similar with other quantities. 
We first establish the progression of the error vector $r_k$ as a linear system for \eqref{eq : g=1 general form}. Then, we provide the step size conditions and convergence rates for \eqref{eq : g=1 general form} and the instances in \cref{tab: Algorithm Def} when $n_g = 1$.

\begin{lemma}\label{lem:lyapunov g = 1}
    Suppose \cref{asum.convex and smooth} holds and the number of gradient steps in each outer iteration of \cref{alg : Deterministic} is set to one (i.e., $n_g=1$). If $\alpha \leq \tfrac{1}{L}$, then for all $k\geq 0$,
    \begin{align*}
      r_{k+1} \leq A(n_c) r_k, 
    \end{align*}
    % where
    \begin{align} \label{eq : g = 1 general A}
    \mbox{where} \quad A(n_c) = \begin{bmatrix}
        1 - \alpha \mu & \tfrac{\alpha L}{\sqrt{n}} & 0\\
        0 & \beta_1^{n_c} & \alpha\beta_2^{n_c}\\
        \sqrt{n}\alpha \beta_4^{n_c} L^2 & \beta_4^{n_c}L(\|\Zmbf_1^{n_c}-I_{nd}\|_2 + \alpha L) & \beta_3^{n_c} + \alpha \beta_4^{n_c} L\\
        \end{bmatrix}.
    \end{align}
\end{lemma}
\begin{proof}
If $n_g=1$, using \eqref{eq : g=1 general form}, the average iterates can be expressed as
\begin{align*}
    \Bar{x}_{k+1} & = \Bar{x}_k - \alpha \Bar{y}_k,   \\
    \Bar{y}_{k+1} & = \Bar{y}_k + h_{k+1} - h_{k},
\end{align*} 
where $h_k$ is defined in~\eqref{eq : derivative terms define}.
Taking the telescopic sum of $\Bar{y}_{i+1}$ from $i=0$ to $k-1$ with $\bar{y}_0 = h_0$, it follows that
\begin{align*}
    \Bar{y}_{k} & = h_k. \numberthis \label{eq:y_bar_telescope}
\end{align*}

We first consider the optimization error on the average iterates. That is, 
%Let's first consider the optimization error on the average iterates. That is, 
\begin{align*}
    \|\Bar{x}_{k+1} - x^*\|_2 & = \left\|\Bar{x}_k - \alpha \Bar{y}_k + \alpha \hbar_k - \alpha\hbar_k - x^*\right\|_2  \\
    & \leq \left\|\Bar{x}_k- \alpha\hbar_k - x^*\right\|_2 + \alpha \left\|\Bar{y}_k - \hbar_k\right\|_2  \\
    % & \leq (1-\alpha \mu) \|\Bar{x}_k - x^*\|_2 + \alpha \left\| \Bar{y}_k - \hbar_k\right\|_2   \\
    &\leq  (1-\alpha \mu) \|\Bar{x}_k - x^*\|_2 + \alpha \left\|h_k - \hbar_k\right\|_2 \\
    &= (1-\alpha \mu) \|\Bar{x}_k - x^*\|_2 + \tfrac{\alpha}{n}\left\|\sum_{i=1}^n \nabla f_i(x_{i, k}) - \nabla f_i(\Bar{x}_{k})\right\|_2 \\
    & \leq (1-\alpha \mu) \|\Bar{x}_k - x^*\|_2 + \tfrac{\alpha L}{n} \sum_{i=1}^n  \| x_{i, k} - \Bar{x}_{k}\|_2    \\
    & \leq (1-\alpha \mu) \|\Bar{x}_k - x^*\|_2 + \tfrac{\alpha L}{\sqrt{n}}  \| \xmbf_{k} - \Bar{\xmbf}_{k}\|_2    \numberthis \label{eq : g = 1 opt bound}
\end{align*}
where the first inequality is due to the triangle inequality, the second inequality is obtained by performing one gradient descent iteration on function $f$ under \cref{asum.convex and smooth} at the average iterate $\bar{x}_k$ with $\alpha \leq \tfrac{1}{L}$ \cite[Theorem 2.1.14]{nesterov1998introductory} and substituting using \eqref{eq:y_bar_telescope},  the equality is due to \eqref{eq : derivative terms define}, the second to last inequality follows by \cref{asum.convex and smooth}, and the last inequality is due to  $\sum_{i=1}^n \|x_{i, k} - \xbar_k\|_2 \leq \sqrt{n}\|\xmbf_{k} - \Bar{\xmbf}_{k}\|_2$.
% where in the first inequality is using triangle inequality. In the second inequality, we used the bound obtained by performing one gradient descent iteration on function $f$ under \cref{asum.convex and smooth} at the average iterate $\bar{x}_k$ with $\alpha \leq \frac{1}{L}$ \cite[Theorem 2.1.14]{nesterov1998introductory}. Then we subsitutue $\ybar_k$ using \eqref{eq:y_bar_telescope}. The second last inequality is from \cref{asum.convex and smooth} and the last inequality is using $\sum_{i=1}^n \|x_{i, k} - \xbar_k\|_2 \leq \sqrt{n}\|\xmbf_{k} - \Bar{\xmbf}_{k}\|_2$.  

Next, we consider the consensus error in $\xmbf_k$,
%consider the consensus error in $\xmbf_k$.
\begin{align*}
    \xmbf_{k+1} - \Bar{\xmbf}_{k+1} &= \Zmbf_1^{n_c}\xmbf_k - \Bar{\xmbf}_{k} - \alpha \Zmbf_2^{n_c}\ymbf_k + \alpha \Bar{\ymbf}_k \\
    & = \Zmbf_1^{n_c}\xmbf_k - \Zmbf_1^{n_c}\Bar{\xmbf}_{k} - \alpha \Zmbf_2^{n_c}\ymbf_k + \alpha \Zmbf_2^{n_c}\Bar{\ymbf}_k  - \Imbf (\xmbf_k - \Bar{\xmbf}_k) + \Imbf (\ymbf_k - \Bar{\ymbf}_k)\\
    & = \left(\Zmbf_1^{n_c} - \Imbf\right)(\xmbf_k - \Bar{\xmbf}_k) - \alpha\left(\Zmbf_2^{n_c} - \Imbf\right)(\ymbf_k - \Bar{\ymbf}_k). 
\end{align*}
where the second equality follows from adding $- \Imbf (\xmbf_k - \Bar{\xmbf}_k) = 0$ and $\Imbf (\ymbf_k - \Bar{\ymbf}_k) = 0$. By the triangle inequality and~\eqref{eq : beta and Z}, 
% Therefore using triangle inequality, 
\begin{equation}\label{eq : g = 1 x con error}
\begin{aligned}
    \|\xmbf_{k+1} - \Bar{\xmbf}_{k+1}\|_2 & \leq \left\|\Zmbf_1^{n_c} - \Imbf\right\|_2 \|\xmbf_k - \Bar{\xmbf}_k\|_2 + \alpha \left\|\Zmbf_2^{n_c} - \Imbf\right\|_2 \|\ymbf_k - \Bar{\ymbf}_k\|_2   \\ 
    & = \beta_1^{n_c} \|\xmbf_k - \Bar{\xmbf}_k\|_2 + \alpha \beta_2^{n_c} \|\ymbf_k - \Bar{\ymbf}_k\|_2.  
\end{aligned}
\end{equation}

Finally, we consider the consensus error in $\ymbf_k$. By the triangle inequality and~\eqref{eq : beta and Z},
\begin{equation}\label{eq: y_bar_bnd_twoterm}
\begin{aligned}
    &\left\|\ymbf_{k+1} - \Bar{\ymbf}_{k+1}\right\|_2 \\
    =& \left\|\Zmbf_3^{n_c}\ymbf_k - \Bar{\ymbf}_{k} + \Zmbf_4^{n_c} (\nabla \fmbf(\xmbf_{k+1}) - \nabla \fmbf(\xmbf_{k})) - \Imbf (\nabla \fmbf(\xmbf_{k+1}) - \nabla \fmbf(\xmbf_{k}))\right\|_2 \\
     \leq& \left\|\left(\Zmbf_3^{n_c} - \Imbf\right)(\ymbf_k - \Bar{\ymbf}_k)\right\|_2 + \left\|\left(\Zmbf_4^{n_c} - \Imbf\right)(\nabla \fmbf(\xmbf_{k+1}) - \nabla \fmbf(\xmbf_{k}))\right\|_2 \\
    \leq& \beta_3^{n_c}\|\ymbf_k - \Bar{\ymbf}_k\|_2 + \beta_4^{n_c} \left\|\nabla \fmbf(\xmbf_{k+1}) - \nabla \fmbf(\xmbf_{k}) \right\|_2.
\end{aligned}
\end{equation}
%where the first inequality is using triangle inequality and we substitute \eqref{eq : beta and Z} at the end. 
The last term in \eqref{eq: y_bar_bnd_twoterm} can be bounded as follows,
\begin{align*}
\left\|\nabla \fmbf(\xmbf_{k+1}) - \nabla \fmbf(\xmbf_{k}) \right\|_2 
&\leq L\|\xmbf_{k+1} - \xmbf_{k}\|_2 \\
&= L\|\Zmbf_1^{n_c}\xmbf_{k} - \alpha \Zmbf_2^{n_c}\ymbf_k - \xmbf_{k}\|_2   \\
% &= L\|(\Zmbf_1^{n_c}-I_{nd})\xmbf_{k} - (\Zmbf_1^{n_c} - I_{nd})\Bar{\xmbf}_k  - \alpha \Zmbf_2^{n_c}\ymbf_k\|_2 \\
&= L\|(\Zmbf_1^{n_c}-I_{nd})(\xmbf_{k} - \Bar{\xmbf}_k) - \alpha \Zmbf_2^{n_c}\ymbf_k\|_2 \\
&\leq L\|\Zmbf_1^{n_c}-I_{nd}\|_2\|\xmbf_{k} - \Bar{\xmbf}_k\|_2 + \alpha L\|\Zmbf_2^{n_c}\|_2\|\ymbf_k + \Bar{\ymbf}_k - \Bar{\ymbf}_k\|_2   \\
&\leq L\|\Zmbf_1^{n_c}-I_{nd}\|_2\|\xmbf_{k} - \Bar{\xmbf}_k\|_2 + \alpha L\|\ymbf_k - \Bar{\ymbf}_k\|_2 + \alpha L\left\|\Bar{\ymbf}_k\right\|_2,\numberthis \label{eq: y_bar_bnd_twoterm33}
\end{align*}
where the first inequality is due to \cref{asum.convex and smooth}, the first equality is due to iterate update form \eqref{eq : g=1 general form}, the second equality is by adding $-(\Zmbf_1^{n_c} - I_{nd})\xbb_{k} = 0$ and the last two inequalities are applications of the triangle inequality.
Next we bound the term $\left\|\Bar{\ymbf}_k\right\|_2$. By \eqref{eq:y_bar_telescope}, \cref{asum.convex and smooth} and $\sum_{i=1}^n \|x_{i, k} - \xbar_k\|_2 \leq \sqrt{n}\|\xmbf_{k} - \Bar{\xmbf}_{k}\|_2$, %it follows that,
\begin{align*}
\left\|\Bar{\ymbf}_k\right\|_2 & \leq \sqrt{n} \|\bar{y}_k\|_2 \\
&= \sqrt{n} \|h_k\|_2 \\
&\leq \sqrt{n}\left\|\tfrac{1}{n} \sum_{i = 1}^n \nabla f_i(x_{i, k}) -  \tfrac{1}{n} \sum_{i = 1}^n \nabla f_i(\bar{x}_{k})\right\|_2 + \sqrt{n}\left\|\tfrac{1}{n} \sum_{i = 1}^n \nabla f_i(\bar{x}_{k})\right\|_2  \\
&= \tfrac{1}{\sqrt{n}}\left\| \sum_{i = 1}^n \nabla f_i(x_{i, k}) -  \sum_{i = 1}^n \nabla f_i(\bar{x}_{k})\right\|_2 + \tfrac{1}{\sqrt{n}} \left\| \sum_{i = 1}^n \nabla f_i(\bar{x}_{k}) - \sum_{i = 1}^n \nabla f_i(x^*)\right\|_2 \\
% &\leq \frac{1}{\sqrt{n}}\left\|\nabla f(x_{k}) - \nabla f(\bar{x}_k)\right\|_2 + \frac{1}{\sqrt{n}}\left\|\nabla f(\bar{x}_k) - \nabla f((x^*)^T)\right\|_2 \\
&\leq L\left\|\xmbf_k - \bar{\xmbf}_k\right\|_2 + \sqrt{n} L \| \Bar{x}_{k} - x^*\|_2. \numberthis \label{eq : y_bar_bound}
\end{align*}
Thus, by \eqref{eq: y_bar_bnd_twoterm}, \eqref{eq: y_bar_bnd_twoterm33} and \eqref{eq : y_bar_bound}, it follows that
\begin{equation}\label{eq.y_result}
\begin{aligned}
    \left\|\ymbf_{k+1} - \Bar{\ymbf}_{k+1}\right\|_2 
    % &\leq \beta_3^{n_c}\|\ymbf_k - \Bar{\ymbf}_k\|_2 + \beta_4^{n_c} \left\|\nabla \fmbf(\xmbf_{k+1}) - \nabla \fmbf(\xmbf_{k}) \right\|_2 \\
    % &\leq \beta_3^{n_c}\|\ymbf_k - \Bar{\ymbf}_k\|_2 \\
    % & \quad + \beta_4^{n_c} \left( L\|\Zmbf_1^{n_c}-I_{nd}\|_2\|\xmbf_{k} - \Bar{\xmbf}_k\|_2 + \alpha L\|\ymbf_k - \Bar{\ymbf}_k\|_2 + \alpha L\left\|\Bar{\ymbf}_k\right\|_2\right)\\
    % &\leq \beta_3^{n_c}\|\ymbf_k - \Bar{\ymbf}_k\|_2 \\
    % & \quad + \beta_4^{n_c} \left( L\|\Zmbf_1^{n_c}-I_{nd}\|_2\|\xmbf_{k} - \Bar{\xmbf}_k\|_2 + \alpha L\|\ymbf_k - \Bar{\ymbf}_k\|_2 + \alpha L\left( L\left\|\xmbf_k - \bar{\xmbf}_k\right\|_2 + L\sqrt{n} \| \Bar{x}_{k} - x^*\|_2\right) \right)\\
    &\leq \beta_4^{n_c}\sqrt{n}\alpha L^2 \| \Bar{x}_{k} - x^*\|_2 + \beta_4^{n_c}L\left( \|\Zmbf_1^{n_c}-I_{nd}\|_2 + \alpha L\right) \|\xmbf_{k} - \Bar{\xmbf}_k\|_2\\
    % & \quad + \beta_4^{n_c}L\left( \|\Zmbf_1^{n_c}-I_{nd}\|_2 + \alpha L\right) \|\xmbf_{k} - \Bar{\xmbf}_k\|_2 \\
    & \qquad +  \left( \beta_3^{n_c} + \beta_4^{n_c}\alpha L\right) \|\ymbf_k - \Bar{\ymbf}_k\|_2.
\end{aligned}
\end{equation}
Combining \eqref{eq : g = 1 opt bound}, \eqref{eq : g = 1 x con error} and \eqref{eq.y_result} concludes the proof.
% Substituting the above bound in \eqref{eq: y_bar_bnd_twoterm}, we get, 
% \begin{align*}
%     \left\|\ymbf_{k+1} - \Bar{\ymbf}_{k+1}\right\| &\leq  (\beta_3^{n_c} + \alpha \beta_4^{n_c} L) \|\ymbf_k - \Bar{\ymbf}_k\|_2 + \beta_4^{n_c}L\|\Zmbf_1^{n_c}-I_{nd}\|_2\|\xmbf_{k} - \Bar{\xmbf}_k\|_2 + \alpha L \beta_4^{n_c}\left\|\Bar{\ymbf}_k\right\|_2 \numberthis \label{eq:y_bar_bnd_threeterm}
% \end{align*}
% Now, let's analyze the last term on the right hand side in the above inequality.
% \begin{align*}
% \left\|\Bar{\ymbf}_k\right\|_2 & \leq \sqrt{n} \|\bar{y}_k\|_2 = \sqrt{n} \|h_k\|_2 \\
% &\leq \sqrt{n}\left\|\frac{1}{n} \sum_{i = 1}^n \nabla f_i(x_{i, k}) -  \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\bar{x}_{k})\right\|_2 + \sqrt{n}\left\|\frac{1}{n} \sum_{i = 1}^n \nabla f_i(\bar{x}_{k})\right\|_2  \\
% &\leq \frac{1}{\sqrt{n}}\left\| \sum_{i = 1}^n \nabla f_i(x_{i, k}) -  \sum_{i = 1}^n \nabla f_i(\bar{x}_{k})\right\|_2 + \frac{1}{\sqrt{n}} \left\| \sum_{i = 1}^n \nabla f_i(\bar{x}_{k}) - \sum_{i = 1}^n \nabla f_i(x^*)\right\|_2 \\
% % &\leq \frac{1}{\sqrt{n}}\left\|\nabla f(x_{k}) - \nabla f(\bar{x}_k)\right\|_2 + \frac{1}{\sqrt{n}}\left\|\nabla f(\bar{x}_k) - \nabla f((x^*)^T)\right\|_2 \\
% &\leq L\left\|\xmbf_k - \bar{\xmbf}_k\right\|_2 + L\sqrt{n} \| \Bar{x}_{k} - x^*\|_2 \numberthis \label{eq : y_bar_bound}
% \end{align*}
% The first equality is using \eqref{eq:y_bar_telescope}. The last inequality is due to \cref{asum.convex and smooth} and $\sum_{i=1}^n \|x_{i, k} - \xbar_k\|_2 \leq \sqrt{n}\|\xmbf_{k} - \Bar{\xmbf}_{k}\|_2$.
% Substituting the above bound into \eqref{eq:y_bar_bnd_threeterm} gives the desired bound on the consensus error in $\ymbf_k$. Combining \eqref{eq : g = 1 opt bound}, \eqref{eq : g = 1 x con error} and \eqref{eq:y_bar_bnd_threeterm} gives the desired form in \eqref{eq : g = 1 general A}
\end{proof}

Using \cref{lem:lyapunov g = 1}, we now provide the explicit form for $A(n_c)$ in order to establish the progression of the error vector $r_k$ for the special cases defined in \cref{tab: Algorithm Def}.
% to establish the progression of error vector $r_k$ for the special cases defined in \cref{tab: Algorithm Def} defined over the mixing matrix $\Wmbf$.

\begin{corollary} \label{col. A special cases}
Suppose the conditions of \cref{lem:lyapunov g = 1} are satisfied. Then, the matrices $A(n_c)$ for the methods described in \cref{tab: Algorithm Def} are defined as:%$A(n_c)$ for the methods given in \cref{tab: Algorithm Def} is as follows: 
\begin{align*}
    \mbox{\texttt{GTA-1}:} & \quad A_1({n_c}) = \begin{bmatrix}
            1 - \alpha \mu & \tfrac{\alpha L}{\sqrt{n}} & 0\\
            0 & \beta^{n_c} & \alpha\\
            \sqrt{n}\alpha L^2 & L(2 + \alpha L) & \beta^{n_c} + \alpha L\\
        \end{bmatrix},\\
    \mbox{\texttt{GTA-2}:} & \quad A_2({n_c})  = \begin{bmatrix}
            1 - \alpha \mu & \tfrac{\alpha L}{\sqrt{n}} & 0\\
            0 & \beta^{n_c} & \alpha \beta^{n_c}\\
            \sqrt{n}\alpha L^2 & L(2 + \alpha L) & \beta^{n_c} + \alpha L\\
        \end{bmatrix},   \numberthis \label{eq : g = 1 algos A}\\
    \mbox{\texttt{GTA-3}:} & \quad A_3({n_c}) = \begin{bmatrix}
            1 - \alpha \mu & \tfrac{\alpha L}{\sqrt{n}} & 0\\
            0 & \beta^{n_c} & \alpha \beta^{n_c}\\
            \beta^{n_c} \sqrt{n}\alpha L^2 & \beta^{n_c}L(2 + \alpha L) & \beta^{n_c}(1 + \alpha L)\\
        \end{bmatrix}.
\end{align*}
\end{corollary}
\begin{proof}
Substituting the matrix values for each method in \eqref{eq : g = 1 general A} and using $\|\Zmbf_1^{n_c} - I_{nd}\|_2 \leq 2$ gives the desired result. 
\end{proof}
The convergence properties of \texttt{GTA} when $n_g=1$ can be analyzed using the spectral radius of the matrix $A({n_c})$. We now qualitatively establish the effect of $n_c$ on $\rho(A({n_c}))$, the spectral radius of the matrix $A({n_c})$, and the relative ordering between $\rho(A_1({n_c}))$,  $ \rho(A_2({n_c}))$ and $ \rho(A_3({n_c}))$.

\btheorem   \label{th.incr rates}
Suppose \cref{asum.convex and smooth} holds and the number of gradient steps in each outer iteration of  \cref{alg : Deterministic} is set to one (i.e., $n_g=1$). If $\alpha \leq \tfrac{1}{L}$, then as ${n_c}$ increases, $\rho(A({n_c}))$ decreases where $A(n_c)$ is defined in \eqref{eq : g = 1 general A}. Thus, as ${n_c}$ increases, $\rho(A_i({n_c}))$ decreases, for $i=1,2,3$  defined in \eqref{eq : g = 1 algos A}. Moreover, if all three methods described in \cref{tab: Algorithm Def} ({\texttt{GTA-1}}, \texttt{GTA-2} and \texttt{GTA-3}) employ the same step size, 
\begin{align*}
\rho(A_1({n_c})) \geq \rho(A_2({n_c})) \geq \rho(A_3({n_c})),
\end{align*}
where the matrices $A_1({n_c})$, $A_2({n_c})$ and $A_3({n_c})$ are defined in \eqref{eq : g = 1 algos A}.
% Moreover, as ${n_c}$ increases, $\rho(A_i({n_c}))$ decreases, for $i=1,2,3$. 
\etheorem
\bproof
Note that $A(n_c) \geq 0$ and $A(n_c) \geq A(n_c + 1) \geq 0$.  By \cite[Corollary 8.1.19]{horn2012matrix}, it follows that $\rho(A(n_c)) \geq \rho(A(n_c + 1))$. The same argument is applicable for $A_1({n_c})$, $A_2({n_c})$ and $A_3({n_c})$. Now, observe that $A_1({n_c}) \geq A_2({n_c}) \geq A_3({n_c}) \geq 0$ when the same step size is employed. Thus, again by \cite[Corollary 8.1.19]{horn2012matrix}, it follows that $\rho(A_1({n_c})) \geq \rho(A_2({n_c})) \geq \rho(A_3({n_c}))$.
\eproof

% step size conditions g = 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We now derive  conditions for establishing a  linear rate of convergence to the solution for \cref{alg : Deterministic} when $n_g = 1$ in terms of network parameters ($\beta_1, \beta_2, \beta_3, \beta_4$) and objective function parameters ($L$, $\mu$, $\kappa = \tfrac{L}{\mu}$).

\btheorem \label{th. general g=1 step cond}
 Suppose \cref{asum.convex and smooth} holds and the number of gradient steps at each outer iteration of \cref{alg : Deterministic} is set to one (i.e., $n_g=1$). If the matrix $A({n_c})$ is irreducible, $\beta_1, \beta_3 < 1$ and 
 \begin{align} \label{eq : g = 1 gen step cond}
    \alpha < \min \left\{\tfrac{1}{L}, \tfrac{1 - \beta_3^{n_c}}{L\beta_4^{n_c}} , \tfrac{(1 - \beta_1^{n_c} + 2\beta_2^{n_c})}{2\beta_2^{n_c}\kappa(L + \mu)} \left(\sqrt{1 + \tfrac{4(1-\beta_1^{n_c})(1-\beta_3^{n_c})\beta_2^{n_c}(\kappa+1)}{\beta_4^{n_c}(1 - \beta_1^{n_c} + 2\beta_2^{n_c})^2}} - 1\right)\right\},
 \end{align}
%  and $\beta_1, \beta_3 < 1$, then there exists a sequence $\epsilon_k\geq 0$ such that,
% \begin{align*}
%     \|r_{k}\|_2 \leq (\rho(A({n_c})) + \epsilon_k)^k \|r_0\|_2 \quad \mbox{and} \quad \lim_{k \rightarrow \infty} \epsilon_k = 0
% \end{align*}
% for all $k\geq 0$, where $\rho(A({n_c})) < 1$.
then, for all $\epsilon > 0$ there exists a constant $C_\epsilon>0$ such that, for all $k\geq 0$,
\begin{align*}
    \|r_{k}\|_2 \leq C_\epsilon(\rho(A({n_c})) + \epsilon)^k \|r_0\|_2, \quad \text{where } \; \rho(A({n_c})) < 1.
\end{align*}
%where $\rho(A({n_c})) < 1$.
\etheorem

\bproof
% Using \cite[Lemma 5]{pu2021distributed}, 
Following \cite[Lemma 5]{pu2021distributed}, derived from the Perron-Forbenius Theorem \cite[Theorem 8.4.4]{horn2012matrix} for a $3\times3$ matrix, when the matrix $A({n_c})$ is non-negative and irreducible, it is sufficient to show that the diagonal elements of $A({n_c})$ are less than one and that $\det(I_3 - A({n_c})) > 0$ in order to guarantee $\rho(A({n_c})) < 1$. We upper bound $\|\Zmbf_1^{n_c} - I_{nd}\|_2 \leq 2$ in $A(n_c)$ for the results.

Let us first consider the diagonal elements of the matrix $A({n_c})$. The first element is, $1 - \alpha \mu \leq 1 -\tfrac{\mu}{L} < 1$ by \eqref{eq : g = 1 gen step cond}. The second element is $\beta_1^{n_c} < 1$ as $\beta_1 < 1$. Finally, the third element is $\beta_3^{n_c} + \alpha\beta_4^{n_c}L < \beta_3^{n_c} + \tfrac{1 - \beta_3^{n_c}}{\beta_4^{n_c}L}\beta_4^{n_c}L = 1$ due to \eqref{eq : g = 1 gen step cond} and $\beta_3 < 1$.

Next, let us consider
\begin{align*}
&\det(I_3 - A({n_c}))\\
= &-\alpha(\alpha^2L^2\beta_2^{n_c}\beta_4^{n_c}\left(L + \mu \right) + \alpha\mu L\beta_4^{n_c}\left(1 - \beta_1^{n_c} + 2\beta_2^{n_c}\right) - \mu\left(1 - \beta_1^{n_c}\right)\left(1 - \beta_3^{n_c}\right)) \\
=&-L^2\beta_2^{n_c}\beta_4^{n_c}(L + \mu)\alpha(\alpha - \alpha_l)(\alpha - \alpha_u), 
\end{align*}
% where $\alpha_l = \alpha_1 + \alpha_2$, $\alpha_u = \alpha_1 - \alpha_2$, and 
% \begin{align*}
% \alpha_1 = \tfrac{-(1 - \beta_1^{n_c} + 2\beta_2^{n_c})}{2\beta_2^{n_c}\kappa(L + \mu)} \quad \text{and} \quad 
% \alpha_2 = -\alpha_1\sqrt{1 + \tfrac{4(1-\beta_1^{n_c})(1-\beta_3^{n_c})\beta_2^{n_c}(\kappa+1)}{\beta_4^{n_c}(1 - \beta_1^{n_c} + 2\beta_2^{n_c})^2}}. 
% \end{align*}
% Observe that $\alpha_l < 0 < \alpha_u$ since $\alpha_1 < 0$ and $\alpha_2 > |\alpha_1|$. From \eqref{eq : g = 1 gen step cond}, we have $0<\alpha < \alpha_2$. Therefore, $\det(I_3 - A({n_c})) > 0$, which combined with the fact that the diagonal elements of the matrix are less than 1, implies $\rho(A(n_c)) < 1$.
where $\alpha_l = \alpha_1 - \alpha_2$, $\alpha_u = \alpha_1 + \alpha_2$, and 
\begin{align*}
\alpha_1 = \tfrac{-(1 - \beta_1^{n_c} + 2\beta_2^{n_c})}{2\beta_2^{n_c}\kappa(L + \mu)} \quad \text{and} \quad 
\alpha_2 = -\alpha_1\sqrt{1 + \tfrac{4(1-\beta_1^{n_c})(1-\beta_3^{n_c})\beta_2^{n_c}(\kappa+1)}{\beta_4^{n_c}(1 - \beta_1^{n_c} + 2\beta_2^{n_c})^2}}. 
\end{align*}
Observe that $\alpha_l < 0 < \alpha_u$ and $\alpha_2 > |\alpha_1|$.
%Observe that $\alpha_l < 0 < \alpha_u$ since $\alpha_1 < 0$ and $\alpha_2 > |\alpha_1|$. 
From \eqref{eq : g = 1 gen step cond}, we have $0<\alpha < \alpha_u$. Therefore, $\det(I_3 - A({n_c})) > 0$, which combined with the fact that the diagonal elements of the matrix are less than 1, implies $\rho(A(n_c)) < 1$.

% \vrb{Now, let us consider 
% \begin{align*}
% &
% \det(I_3 - A({n_c}))\\
% &~~= -\alpha(\alpha^2L^2\beta_2^{n_c}\beta_4^{n_c}\left(L + \mu \right) + \alpha\mu L\beta_4^{n_c}\left(1 - \beta_1^{n_c} + 2\beta_2^{n_c}\right) - \mu\left(1 - \beta_1^{n_c}\right)\left(1 - \beta_3^{n_c}\right)) \\
% &~~=-\alpha(\alpha - \alpha_l)(\alpha - \alpha_u) 
% \end{align*}
% where $\alpha_l = \alpha_1 + \alpha_2$, $\alpha_u = \alpha_1 - \alpha_2$, and 
% \begin{align*}
% \alpha_1 &= \tfrac{-(1 - \beta_1^{n_c} + 2\beta_2^{n_c})}{2\beta_2^{n_c}\kappa(L + \mu)} \\
% \alpha_2 &= -\alpha_1\sqrt{1 + \tfrac{4(1-\beta_1^{n_c})(1-\beta_3^{n_c})\beta_2^{n_c}(\kappa+1)}{\beta_4^{n_c}(1 - \beta_1^{n_c} + 2\beta_2^{n_c})^2}}. 
% \end{align*}

% Observe that $\alpha_l < 0 < \alpha_u$ since $\alpha_1 < 0$ and $\alpha_2 > |\alpha_1|$. From \eqref{eq : g = 1 gen step cond}, we have $0<\alpha < \alpha_2$. Therefore, $\det(I_3 - A({n_c})) > 0$ which implies $\rho(A(n_c)) < 1$. 
% }

% Now, let us consider the condition $\det(I_3 - A({n_c})) > 0$.
% \begin{align*}
% % &\det(I_3 - A({n_c})) \\
% &\alpha \mu \left((1-\beta_1^{n_c})(1-\beta_3^{n_c} - \alpha L\beta_4^{n_c}) - \alpha \beta_2^{n_c}\beta_4^{n_c}L(2 + \alpha L)\right) + \frac{\alpha L}{\sqrt{n}}\left(-\alpha^2 L^2\beta_2^{n_c}\beta_4^{n_c} \sqrt{n}\right) > 0\\
% &-\alpha \left(\alpha^2\left(L^3\beta_2^{n_c}\beta_4^{n_c} + \mu L^2\beta_2^{n_c}\beta_4^{n_c}\right) + \alpha\left(L\mu\beta_4^{n_c}(1- \beta_1^{n_c}) + 2L\mu\beta_2^{n_c}\beta_4^{n_c}\right) \right. > 0\\
% & \quad \left.- \mu\left(1 - \beta_1^{n_c}\right)\left(1 - \beta_3^{n_c}\right)\right)
% \end{align*}
% % Setting $\det(I_3 - A({n_c})) > 0$ yields the following quadratic inequality in $\alpha$
% As $\alpha > 0$, the condition reduces to the following quadratic inequality
% \begin{align*}
% \alpha^2L^2\beta_2^{n_c}\beta_4^{n_c}\left(L + \mu \right) + \alpha\mu L\beta_4^{n_c}\left(1 - \beta_1^{n_c} + 2\beta_2^{n_c}\right) - \mu\left(1 - \beta_1^{n_c}\right)\left(1 - \beta_3^{n_c}\right) < 0,
% \end{align*}
% % where the roots, after minor simplification and defining $\kappa = \frac{L}{\mu}$, are given as, 
% % \begin{align*}
% %    \frac{-(1 - \beta_1^{n_c} + 2\beta_2^{n_c})}{2\beta_2^{n_c}\kappa(L + \mu)} \pm \frac{\sqrt{(1-\beta_1^{n_c} + 2\beta_2^{n_c})^2 + 4(\kappa + 1)\left(1 - \beta_1^{n_c}\right)\left(1 - \beta_3^{n_c}\right)\beta_2^{n_c}\beta_4^{-{n_c}}}}{2\beta_2^{n_c}\kappa(L + \mu)}.
% % \end{align*}
% % Therefore, both the roots are real as all the terms inside the square root are positive. Thus, the quadratic inequality with $\alpha > 0$ is satisfied when
% with roots $\alpha_1$ and $\alpha_2$ such that $\alpha_1 < 0 < \alpha_2$. Thus, the condition requires $\alpha < \alpha_2$ which is satisfied by \eqref{eq : g = 1 gen step cond} as
% \begin{align*}
% \alpha < \frac{-(1 - \beta_1^{n_c} + 2\beta_2^{n_c})}{2\beta_2^{n_c}\kappa(L + \mu)} + \frac{(1 - \beta_1^{n_c} + 2\beta_2^{n_c})}{2\beta_2^{n_c}\kappa(L + \mu)}\sqrt{1 + \frac{4(1-\beta_1^{n_c})(1-\beta_3^{n_c})\beta_2^{n_c}(\kappa+1)}{\beta_4^{n_c}(1 - \beta_1^{n_c} + 2\beta_2^{n_c})^2}}.
% \end{align*}
% % as this upper bound is the larger positive root of the quadratic. Observe that the bounding quantity is positive, thus there exists a positive step size that can satisfy \eqref{eq : g = 1 gen step cond}. 
% % Now, we observe that,
% % \begin{align*}
% %     \|r_{k}\|_2 &\leq \|A({n_c})^k\|_2\|r_0\|_2,
% % \end{align*}
% % and using \cite[Theorem 5.6.12]{horn2012matrix}, we can bound the matrix norm with a spectral norm using some sequence $\epsilon_k \geq 0$ with $\lim_{k \rightarrow \infty} \epsilon_k = 0$.
% Thus given \eqref{eq : g = 1 gen step cond}, $\rho(A(n_c)) < 1$. 

% Finally, we bound the norm of error vector $\|r_k\|_2$ by telescoping $r_{i+1} \leq A(n_c) r_{i}$ from $i = 0$ to $k-1$ and triangle inequality as
% \begin{align*}
%     \|r_{k}\|_2 &\leq \|A({n_c})^k\|_2\|r_0\|_2.
% \end{align*}
% From \cite[Corollary 5.6.13]{horn2012matrix}, we can bound $\|A({n_c})^k\|_2 \leq C(\rho(A(n_c)) + \epsilon)^k$ where $\epsilon > 0$ and $C$ is a constant matrix depending on $A(n_c)$ and $\epsilon$. Using, \cite[Theorem 5.6.12]{horn2012matrix}, $\lim_{k \rightarrow \infty} A(n_c)^k = 0$ as $\rho(A(n_c)) < 1$. Thus, we can bound the matrix norm with a spectral radius using some sequence $\epsilon_k \geq 0$ with $\lim_{k \rightarrow \infty} \epsilon_k = 0$ and obtain the desired result.

Finally, we bound the norm of error vector $\|r_k\|_2$ by telescoping $r_{i+1} \leq A(n_c) r_{i}$ from $i = 0$ to $k-1$ and triangle inequality as
\begin{align*}
    \|r_{k}\|_2 &\leq \|A({n_c})^k\|_2\|r_0\|_2.
\end{align*}
From \cite[Corollary 5.6.13]{horn2012matrix}, we can bound $\|A({n_c})^k\|_2 \leq C_{\epsilon}(\rho(A(n_c)) + \epsilon)^k$ where $\epsilon > 0$ and $C_{\epsilon}$ is a positive constant that depends on $A(n_c)$ and $\epsilon$.
\eproof

The only constraint \cref{th. general g=1 step cond} imposes on the system (network) is $\beta_1, \beta_3 < 1$. This implies that the communication matrices $\Wmbf_1$ and $\Wmbf_3$ must represent connected networks (not necessarily the same network). Properties of $\Wmbf_2$ and $\Wmbf_4$ change the step size requirement but are not part of the sufficient conditions for convergence. \cref{th. general g=1 step cond} also does not require any relation among $\Wmbf_1$, $\Wmbf_2$, $\Wmbf_3$ and $\Wmbf_4$. This allows for more flexibility than the structures considered in the literature. %This shows a greater generalization than ATC and CTA structures discussed in literature. 
The variables can be communicated along different connections within the network. %One drawback of %gradient tracking methods is the requirement \rb{to %communicate two} %of communication of 2 
%vectors instead of one, resulting in higher communication bandwidth. This limitation can be overcome using our general structure by limiting communication of variables on certain edges as long as \cref{th. general g=1 step cond} conditions are met. 
We note that if $A(n_c)$ is a reducible matrix, the analysis for the progression of $r_k$ can be further simplified from \cref{lem:lyapunov g = 1}. For example, when 
% \asb{In this case,}
$\Wmbf = \tfrac{1_n1_n^T}{n}$, i.e., $\beta = 0$, in \texttt{GTA-2} and \texttt{GTA-3}. 
The analysis for these cases is presented in \cref{sec.full graph res}. 

The next result establishes step size conditions that guarantee a linear rate of convergence for the three special cases (\texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3}).
% We next establish the step size conditions for linear rate of convergence for our special cases \texttt{GTA-1}, \texttt{GTA-2} amd \texttt{GTA-3}.

\bcorollary \label{col. g=1 step cond}
Suppose \cref{asum.convex and smooth} holds, $\Wmbf \neq \tfrac{1_n1_n^T}{n}$, and the number of gradient steps at each outer iteration of \cref{alg : Deterministic} is set to one (i.e., $n_g=1$). If the following step size conditions hold for the methods described in \cref{tab: Algorithm Def},
\begin{align*}
    \mbox{\texttt{GTA-1}:} & \quad \alpha < \min \left\{ \tfrac{1 - \beta^{n_c}}{L} , \tfrac{(3 - \beta^{n_c})}{2\kappa(L + \mu)}\left(\sqrt{1 + 4(\kappa + 1)\left( \tfrac{1 - \beta^{n_c}}{3 - \beta^{n_c}} \right)^2} - 1\right) \right\},\\
    \mbox{\texttt{GTA-2}:} & \quad \alpha < \min \left\{\tfrac{1 - \beta^{n_c}}{L} , \tfrac{(1+\beta^{n_c})}{2\kappa(L + \mu)\beta^{n_c}} \left( \sqrt{1 + 4(\kappa + 1)\beta^{n_c}\left(\tfrac{1 - \beta^{n_c}}{1+\beta^{n_c}}\right)^2} - 1\right)  \right\}, \\
    \mbox{\texttt{GTA-3}:} & \quad \alpha < \min \left\{\tfrac{1}{L}, \tfrac{1 - \beta^{n_c}}{L\beta^{n_c}} , \tfrac{(1 + \beta^{n_c})}{2\kappa(L + \mu)\beta^{n_c}} \left(\sqrt{1 + 4(\kappa + 1)\left(\tfrac{1 - \beta^{n_c}}{1 + \beta^{n_c}}\right)^2} - 1\right)\right\},
\end{align*}
% \begin{align*}
%     \intertext{For \texttt{GTA-1}}       
%     &\alpha < \min \left\{ \tfrac{1 - \beta^{n_c}}{L} , \tfrac{(3 - \beta^{n_c})}{2\kappa(L + \mu)}\left[\sqrt{1 + 4(\kappa + 1)\left[ \tfrac{1 - \beta^{n_c}}{3 - \beta^{n_c}} \right]^2} - 1\right] \right\} 
%     \intertext{For \texttt{GTA-2}}   
%     &\alpha < \min \left\{\tfrac{1 - \beta^{n_c}}{L} , \tfrac{(1+\beta^{n_c})}{2\kappa(L + \mu)\beta^{n_c}} \left[ \sqrt{1 + 4(\kappa + 1)\beta^{n_c}\left[\tfrac{1 - \beta^{n_c}}{1+\beta^{n_c}}\right]^2} - 1\right]  \right\}  %\numberthis \label{eq : g = 1 step cond}
%     \intertext{For \texttt{GTA-3}}    
%     &\alpha < \min \left\{\tfrac{1}{L}, \tfrac{1 - \beta^{n_c}}{L\beta^{n_c}} , \tfrac{(1 + \beta^{n_c})}{2\kappa(L + \mu)\beta^{n_c}} \left[\sqrt{1 + 4(\kappa + 1)\left[\tfrac{1 - \beta^{n_c}}{1 + \beta^{n_c}}\right]^2} - 1\right]\right\},   
% \end{align*}
% then there exists a sequence $\epsilon_k\geq 0$ such that,
% \begin{align*}
%     \|r_{k}\|_2 \leq (\rho(A_i({n_c})) + \epsilon_k)^k \|r_0\|_2 \quad \mbox{and} \quad \lim_{k \rightarrow \infty} \epsilon_k = 0
% \end{align*}
% for all $k\geq 0$ where $\rho(A_i({n_c})) < 1 \,\, \forall \,\, i = 1, 2, 3$.
then, for all $\epsilon > 0$ there exist constants $C_{i,\epsilon} > 0$ such that, for all $k\geq 0$,
\begin{align*}
    \|r_{k}\|_2 \leq C_{i,\epsilon}(\rho(A_i({n_c})) + \epsilon)^k \|r_0\|_2, \; \text{where } \; \rho(A_i({n_c})) < 1, \text{ for } \;  i=1, 2, 3.
\end{align*}
 %where $\rho(A_i({n_c})) < 1$ for all $i=1, 2, 3$.}
\ecorollary
\bproof
The conditions given in \cref{th. general g=1 step cond} are satisfied for all three methods. That is, the matrices are irreducible as $\Wmbf \neq \tfrac{1_n1_n^T}{n}$, i.e., $\beta > 0$ and $\beta_1, \beta_3 < 1$ in all the three methods as $\beta < 1$ because $\Wmbf$ is mixing matrix of a connected network. Thus, we can use \eqref{eq : g = 1 gen step cond} to derive the conditions on the step size for each of the methods. Substituting the values for $\beta_1,\beta_2,\beta_3$ and $\beta_4$ for each method yields the desired result. We should note that in \texttt{GTA-1} and  \texttt{GTA-2}, we  ignore the term $\tfrac{1}{L}$ since $\tfrac{1}{L} > \tfrac{1-\beta^{n_c}}{L}$. 
\eproof

\cref{col. g=1 step cond} shows how the communication strategy affects the step size when $n_g = 1$. Among the three methods, \texttt{GTA-3} allows for the largest step size, even having the possibility to use the step size $\tfrac{1}{L}$ if sufficiently large number of communications are performed (high $n_c$) and depending on $\beta$. Among \texttt{GTA-1} and \texttt{GTA-2}, \texttt{GTA-2} allows for a larger step size. While these share the same first term in the $\min$ bound, the presence of the $\beta^{n_c}$ factor in the denominator of the second term in \texttt{GTA-2} makes the bound larger than \texttt{GTA-1} and possibly allowing for a larger step size.

% While $\beta_1 = \beta_3 = \beta$, as $\beta_2$ is decreased from $1$ to $\beta$ from \texttt{GTA-1} to \texttt{GTA-2}, the step size condition improves. As $\beta_4$ is now changed from $1$ to $\beta$ from \texttt{GTA-2} to \texttt{GTA-3} , the condition improves even more, now allowing the possibility of $\tfrac{1}{L}$ gradient descent step size condition if $\beta$ is small enough or enough communications are performed ($n_c$ increased). Among


\cref{th. general g=1 step cond} states that there exists a step size such that \texttt{GTA} converges at a linear rate when $n_g = 1$. We now proceed to analyze the convergence rate \texttt{GTA} when $n_g = 1$. Before that, we provide a technical lemma that shows that the largest eigenvalue of the matrix $A(n_c)$ is a positive real number. 

\begin{lemma}\label{lem. g=1 spec norm}
Suppose \cref{asum.convex and smooth} holds, the number of gradient steps at each outer iteration of \cref{alg : Deterministic} is set to one (i.e., $n_g=1$) and $\alpha \leq \tfrac{1}{L}$. If the matrix $A({n_c})$ defined in  \eqref{eq : g = 1 general A} is irreducible, then, the spectral radius of $A({n_c})$ is the largest eigenvalue of $A({n_c})$ and is a positive real number. Consequently, if $\Wmbf \neq \tfrac{1_n1_n^T}{n}$, the spectral radius of matrices $A_1({n_c})$, $A_2({n_c})$, $A_3({n_c})$ defined in \eqref{eq : g = 1 algos A} are also positive real numbers and equal to their largest eigenvalues, respectively.
\end{lemma}

\begin{proof}
The statement about the matrix $A({n_c})$ follows from the Perron-Forbenius Theorem \cite[Theorem 8.4.4]{horn2012matrix}, and the fact that the matrix is %observing that the matrix $A({n_c})$ is 
non-negative and irreducible. Using similar arguments, the statement about the matrices $A_1({n_c})$, $A_2({n_c})$ and $A_3({n_c})$ follows as these matrices are irreducible when $\Wmbf \neq \tfrac{1_n1_n^T}{n}$, i.e., $\beta > 0$.
\end{proof}

% rate bounds g = 0
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The next theorem provides an upper bound on the convergence rate of \texttt{GTA} for sufficiently small constant step sizes.%$ $\alpha$. %To make the bounds simpler, we will employ $\beta_1=\beta_3$.
\btheorem  \label{th. general g=1 rate bound} 
    Suppose \cref{asum.convex and smooth} holds and the number of gradient steps at each outer iteration of \cref{alg : Deterministic} is set to one (i.e., $n_g=1$). If the matrix $A({n_c})$ is irreducible and $\alpha \leq \tfrac{1}{L}$, then, %and $\beta_1=\beta_3 = \beta$,  then, 
    \begin{align*}
        \rho(A({n_c})) & \leq \,\lambda_u = \max\left\{1 - \tfrac{\alpha\mu}{2}, \hat{\lambda} + \sqrt{2\alpha L \kappa \beta_2^{n_c}\beta_4^{n_c}}\right\}, \numberthis \label{eq : general g = 1 rate}
    \end{align*}
    where $\hat{\lambda} = \tfrac{\beta_1^{n_c} + \beta_3^{n_c} + L\alpha\beta_4^{n_c}  + \sqrt{\left(\beta_1^{n_c} - \beta_3^{n_c} - L\alpha\beta_4^{n_c}\right)^2 + 4\beta_2^{n_c}\beta_4^{n_c} L^2\alpha^2 + 8L\alpha\beta_2^{n_c}\beta_4^{n_c}}}{2}$.
    % FINAL VERSION OF EQUATION
    % \begin{align*}
    % \hat{\lambda} = \tfrac{\beta_1^{n_c} + \beta_3^{n_c} + L\alpha\beta_4^{n_c}  + \sqrt{\left(\beta_1^{n_c} - \beta_3^{n_c} - L\alpha\beta_4^{n_c}\right)^2 + 4\beta_2^{n_c}\beta_4^{n_c} L^2\alpha^2 + 8L\alpha\beta_2^{n_c}\beta_4^{n_c}}}{2}.
    % \end{align*}
% 
    % \begin{align*}
    % \hat{\lambda} = \tfrac{\beta_1^{n_c} + \beta_3^{n_c} + L\alpha\beta_4^{n_c}  + \sqrt{\left(\beta_1^{n_c} - \beta_3^{n_c}\right)^2 + L^2\alpha^2(\beta_4^{n_c})^2 - 2L\alpha\beta_4^{n_c} (\beta_1^{n_c} - \beta_3^{n_c}) + 4\beta_2^{n_c}\beta_4^{n_c} L^2\alpha^2 + 8L\alpha\beta_2^{n_c}\beta_4^{n_c}}}{2}.
    % \end{align*}
    % \begin{align*}
    % \hat{\lambda} = \tfrac{\beta_1^{n_c} + \beta_3^{n_c} + L\alpha\beta_4^{n_c}  + \sqrt{\left(\beta_1^{n_c} - \beta_3^{n_c}\right)^2 + L^2\alpha^2\left( (\beta_4^{n_c})^2 + 4\beta_2^{n_c}\beta_4^{n_c}\right) + L\alpha \left( 8\beta_2^{n_c}\beta_4^{n_c}  - 2\beta_4^{n_c} (\beta_1^{n_c} - \beta_3^{n_c}) \right)  }}{2}.
    % \end{align*}
\etheorem

\bproof
%Since the matrix $A({n_c})$ is non-negative and irreducible, using \cref{lem. g=1 spec norm}, we know that the spectral norm is equal to the largest eigenvalue which is a positive real number. 
Using \cref{lem. g=1 spec norm}, we know that the spectral radius of $A(n_c)$ is equal to the largest eigenvalue which is a positive real number.
Following a similar approach to \cite{qu2017harnessing}, we prove $\lambda_u$ is an upper bound on the largest eigenvalue by showing the characteristic equation is non-negative at $\lambda_u$ and strictly increasing for all values greater than $\lambda_u$. Consider 
 \begin{align*}
 g(\lambda) 
= &\det(\lambda I_3 - A({n_c})) \\
= &(\lambda - 1 + \alpha\mu)\left((\lambda - \beta_1^{n_c})(\lambda - \beta_3^{n_c} - \alpha L\beta_4^{n_c}) - \alpha L(2 + \alpha L)\beta_2^{n_c}\beta_4^{n_c}\right) 
\\
& \quad - \alpha^3 L^3 \beta_2^{n_c}\beta_4^{n_c} \\
=& (\lambda - 1 + \alpha\mu) q(\lambda) - \alpha^3 L^3 \beta_2^{n_c}\beta_4^{n_c},
% =& (\lambda - 1 + \alpha\mu) \left(\lambda^2 -\lambda(\beta_1^{n_c} + \beta_3^{n_c} + L\alpha\beta_4^{n_c})  + \beta_1^{n_c}\beta_3^{n_c} + L\alpha\beta_4^{n_c}(\beta_1^{n_c} - 2\beta_2^{n_c} - L\alpha\beta_2^{n_c})\right) \\
% &\qquad - \alpha^3 L^3 \beta_2^{n_c}\beta_4^{n_c}
 \end{align*}
where $q(\lambda) = \lambda^2 -\lambda(\beta_1^{n_c} + \beta_3^{n_c} + L\alpha\beta_4^{n_c})  + \beta_1^{n_c}\beta_3^{n_c} + L\alpha\beta_4^{n_c}(\beta_1^{n_c} - 2\beta_2^{n_c} - L\alpha\beta_2^{n_c})$.
Let the roots of the quadratic function $q(\lambda)$ be denoted as $\lambda_1$ and $\lambda_2$. Then, we have,  %can be upper bounded as follows.
\begin{align*}
\max\{\lambda_1, \lambda_2\} 
=& \tfrac{\beta_1^{n_c} + \beta_3^{n_c} + L\alpha\beta_4^{n_c}  + \sqrt{\left(\beta_1^{n_c} + \beta_3^{n_c} + L\alpha\beta_4^{n_c}\right)^2 - 4\left(\beta_1^{n_c}\beta_3^{n_c} + L\alpha\beta_4^{n_c}(\beta_1^{n_c} - 2\beta_2^{n_c} - L\alpha\beta_2^{n_c})\right)}}{2}\\
% =& \frac{\beta_1^{n_c} + \beta_3^{n_c} + L\alpha\beta_4^{n_c}  + \sqrt{\left(\beta_1^{n_c} - \beta_3^{n_c}\right)^2 + L^2\alpha^2(\beta_4^{n_c})^2 - 2L\alpha\beta_4^{n_c} (\beta_1^{n_c} - \beta_3^{n_c}) + 4\beta_2^{n_c}\beta_4^{n_c} L^2\alpha^2 + 8L\alpha\beta_2^{n_c}\beta_4^{n_c}}}{2} \\
=& \tfrac{\beta_1^{n_c} + \beta_3^{n_c} + L\alpha\beta_4^{n_c}  + \sqrt{\left(\beta_1^{n_c} - \beta_3^{n_c} - L\alpha\beta_4^{n_c}\right)^2 + 4\beta_2^{n_c}\beta_4^{n_c} L^2\alpha^2 + 8L\alpha\beta_2^{n_c}\beta_4^{n_c}}}{2} 
\end{align*}
Thus, for any 
$\lambda \geq \max\left\{1 - \alpha \mu, \hat{\lambda}\right\}$, the function $g(\lambda)$ is increasing and is lower bounded by $(\lambda - 1 + \alpha\mu)(\lambda - \hat{\lambda})^2 - \alpha^3 L^3 \beta_2^{n_c}\beta_4^{n_c}$. 
% Now, let
% \begin{align*}
% \lambda_u =  \max\left\{1 - \frac{\alpha\mu}{2}, \hat{\lambda} + \sqrt{\frac{2\alpha L^2 \beta_2^{n_c}\beta_4^{n_c}}{\mu}}\right\}.
% \end{align*}
By $\lambda_u \geq \max\left\{1 - \alpha \mu, \hat{\lambda}\right\}$, 
\begin{align*}
g(\lambda_u) &\geq (\lambda - 1 + \alpha\mu)(\lambda - \hat{\lambda})^2 - \alpha^3 L^3 \beta_2^{n_c}\beta_4^{n_c} \\
&\geq \left(1 - \tfrac{\alpha\mu}{2} -1 + \alpha\mu\right)(\lambda - \hat{\lambda})^2 - \alpha^3 L^3 \beta_2^{n_c}\beta_4^{n_c} \\
&\geq \tfrac{\alpha \mu}{2}\left(\tfrac{2\alpha L^2\beta_2^{n_c}\beta_4^{n_c}}{\mu}\right)  - \alpha^3 L^3 \beta_2^{n_c}\beta_4^{n_c} \\
&=\alpha^2L^2\beta_2^{n_c}\beta_4^{n_c}(1 - \alpha L) \geq 0,
\end{align*}
% where the second and third inequalities are due to definition of $\lambda_u$ and the last inequality is due to $\alpha L \leq 1$. 
where the second and third inequalities are due to the definition of $\lambda_u$ and the final quantity is non-negative since $\alpha \leq \tfrac{1}{L}$. Therefore, by the above arguments, we conclude that $\rho(A({n_c})) \leq \lambda_u$ which completes the proof. 
\eproof

\cref{th. general g=1 rate bound} is derived independent of the conditions in \cref{th. general g=1 step cond}. 
%It places the same conditions on the system as \cref{th. general g=1 step cond}. 
When $\rho(A(n_c)) < 1$ is imposed using \cref{th. general g=1 rate bound}, $\beta_1,\beta_3 < 1$ is %recognized as 
a necessary condition for convergence. We show this by constructing a lower bound on $\lambda_u$, $\lambda_u \geq \hat{\lambda} \geq \tfrac{\beta_1^{n_c} + \beta_3^{n_c}+ \left|\beta_1^{n_c} - \beta_3^{n_c}\right|}{2}$.
% \begin{align*}
%     \lambda_u & \geq \hat{\lambda} 
%     % \\
%     % & \geq \tfrac{\beta_1^{n_c} + \beta_3^{n_c} + L\alpha\beta_4^{n_c}  + \sqrt{\left(\beta_1^{n_c} - \beta_3^{n_c} - L\alpha\beta_4^{n_c}\right)^2 + 4\beta_2^{n_c}\beta_4^{n_c} L^2\alpha^2 + 8L\alpha\beta_2^{n_c}\beta_4^{n_c}}}{2} \\
%     % & \geq \tfrac{\beta_1^{n_c} + \beta_3^{n_c} + L\alpha\beta_4^{n_c}  + \sqrt{\left(\beta_1^{n_c} - \beta_3^{n_c} - L\alpha\beta_4^{n_c}\right)^2}}{2} \\ 
%     % & \geq \tfrac{\beta_1^{n_c} + \beta_3^{n_c} + L\alpha\beta_4^{n_c}  + \left|\beta_1^{n_c} - \beta_3^{n_c} - L\alpha\beta_4^{n_c}\right|}{2} \\
%     % & 
%     \geq \tfrac{\beta_1^{n_c} + \beta_3^{n_c}+ \left|\beta_1^{n_c} - \beta_3^{n_c}\right|}{2}.
% \end{align*}
% where the second inequality is obtained by removing the last two positive terms in the sqaure root and the last inequality is due to traingle inequality.
For convergence we require $\lambda_u < 1$, i.e., $\tfrac{\beta_1 + \beta_3 + |\beta_1 - \beta_3|}{2} < 1$, which implies $\beta_1,\beta_3 < 1$ as $\beta_1, \beta_3 \in [0,1]$. Thus, again we require $\Wmbf_1$ and $\Wmbf_3$ to represent a connected network. The step size condition in \cref{th. general g=1 step cond} is $\mathcal{O}(L^{-1}\kappa^{-0.5})$ while \cref{th. general g=1 rate bound} requires $\mathcal{O}(L^{-1}\kappa^{-1})$, which is more pessimistic. That  said,  the precise and interpretable characterization of the convergence rate in \cref{th. general g=1 rate bound}  allows us to better differentiate amongst the communication strategies and the effect of $n_c$. 
% But \cref{th. general g=1 rate bound} gives more interpretable results for the convergence rate and allows us to differentiate better among communication strategies and effect of $n_c$.

\bcorollary  \label{col. g=1 rate bound}
Suppose \cref{asum.convex and smooth} holds, $\Wmbf \neq \tfrac{1_n1_n^T}{n}$, and the number of gradient steps at each outer iteration of \cref{alg : Deterministic} is set to one (i.e., $n_g=1$). If $\alpha \leq \tfrac{1}{L}$, then, the spectral radii for the methods described in \cref{tab: Algorithm Def} satisfy
\begin{align*}
    \mbox{\texttt{GTA-1}:} & \quad \rho(A_1({n_c}))   \leq \max\left\{1 - \tfrac{\alpha\mu}{2}, \beta^{n_c} + \sqrt{\alpha L} \left(2.5 + \sqrt{\kappa}\right)\right\},\\
    \mbox{\texttt{GTA-2}:} & \quad \rho(A_2({n_c}))  \leq \max\left\{1 - \tfrac{\alpha\mu}{2}, \beta^{n_c} + \sqrt{\alpha L} \left(2.5 + \sqrt{\kappa \beta^{n_c}}\right)\right\}, \\%\numberthis \label{eq : g = 1 rate} \\
    \mbox{\texttt{GTA-3}:} & \quad \rho(A_3({n_c}))  \leq \max\left\{1 - \tfrac{\alpha\mu}{2}, \beta^{n_c}\left(1 + \sqrt{\alpha L} \left(2.5 + \sqrt{\kappa}\right)\right)\right\}.
\end{align*}
    % \begin{align*}
    %     \intertext{For \texttt{GTA-1}}       
    %         \rho(A_1({n_c}))  & < \max\left\{1 - \frac{\alpha\mu}{2}, \beta^{n_c} + \sqrt{\alpha L} \left(2.5 + \sqrt{\kappa}\right)\right\}
    %     \intertext{For \texttt{GTA-2}}   
    %         \rho(A_2({n_c})) & < \max\left\{1 - \frac{\alpha\mu}{2}, \beta^{n_c} + \sqrt{\alpha L} \left(2.5 + \sqrt{\kappa \beta^{n_c}}\right)\right\} \numberthis \label{eq : g = 1 rate}
    %     \intertext{For \texttt{GTA-3}}    
    %         \rho(A_3({n_c})) & < \max\left\{1 - \frac{\alpha\mu}{2}, \beta^{n_c}\left(1 + \sqrt{\alpha L} \left(2.5 + \sqrt{\kappa}\right)\right)\right\} 
    % \end{align*}
\ecorollary
\bproof
The conditions in \cref{th. general g=1 rate bound} are satisfied due to \cref{lem. g=1 spec norm}. Thus, we can plug in the values for $\beta_i$ ($i=1,2,3,4$) for each method to get an upper bound on the spectral radii. The upper bound $\lambda_u$ for \texttt{GTA-1} can be simplified as
\begin{align*}
\hat{\lambda} + \sqrt{\tfrac{2\alpha L^2 \beta_2^{n_c}\beta_4^{n_c}}{\mu}} &= \tfrac{2\beta^{n_c} + L\alpha  + \sqrt{5L^2\alpha^2 + 8L\alpha}}{2} + \sqrt{\tfrac{2\alpha L^2}{\mu}} \\
&= \beta^{n_c} + \tfrac{\sqrt{\alpha L}}{2}\left(\sqrt{\alpha L} + 2\sqrt{\kappa} + \sqrt{8 + 5L\alpha} \right) \\
&\leq \beta^{n_c} + \sqrt{\alpha L} \left(2.5 + \sqrt{\kappa}\right)
\end{align*}
where the last inequality is due to $\alpha \leq \tfrac{1}{L}$. Following the same approach, $\lambda_u$ for \texttt{GTA-2} can be simplified as
\begin{align*}
\hat{\lambda} + \sqrt{\tfrac{2\alpha L^2 \beta_2^{n_c}\beta_4^{n_c}}{\mu}} &= \tfrac{2\beta^{n_c} + L\alpha  + \sqrt{L^2\alpha^2 + 4L^2\alpha^2\beta^{n_c} + 8L\alpha\beta^{n_c}}}{2} + \sqrt{\tfrac{2\alpha L^2\beta^{n_c}}{\mu}} \\
&= \beta^{n_c} + \tfrac{\sqrt{\alpha L}}{2}\left(\sqrt{\alpha L} + 2\sqrt{\kappa \beta^{n_c}} + \sqrt{8\beta^{n_c} + 4L\alpha\beta^{n_c} + L\alpha} \right) \\
&\leq \beta^{n_c} + \sqrt{\alpha L} \left(2.5 + \sqrt{\kappa \beta^{n_c}}\right)
\end{align*}
where the last inequality uses $\alpha \leq \tfrac{1}{L}$ and $\beta < 1$. Finally, the upper bound $\lambda_u$ for \texttt{GTA-3} is
\begin{align*}
\hat{\lambda} + \sqrt{\tfrac{2\alpha L^2 \beta_2^{n_c}\beta_4^{n_c}}{\mu}} &= \tfrac{2\beta^{n_c} + L\alpha\beta^{n_c}  + \sqrt{ 5L^2\alpha^2(\beta^{n_c})^2 + 8L\alpha(\beta^{n_c})^2}}{2} + \sqrt{\tfrac{2\alpha L^2(\beta^{n_c})^2}{\mu}} \\
&= \beta^{n_c}\left(1  + \tfrac{\sqrt{\alpha L}}{2}\left(\sqrt{\alpha L} + 2\sqrt{\kappa} + \sqrt{8 + 5L\alpha} \right)\right) \\
&\leq \beta^{n_c}\left(1 + \sqrt{\alpha L} \left(2.5 + \sqrt{\kappa}\right)\right)
\end{align*}
where the last inequality is due to $\alpha \leq \tfrac{1}{L}$ and $\beta < 1$. 

\eproof

%Using \cref{col. g=1 rate bound}, we can conclude that in \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3}, the convergence rate improves with increased communication ($n_c$) when $n_g = 1$. The effect is more profound from \texttt{GTA-1} to \texttt{GTA-2}. In \texttt{GTA-1}, increasing $n_c$ only effects the constant term while $\sqrt{L\kappa}$ still dominates. In \texttt{GTA-2}, the dominating term is also impacted. From \texttt{GTA-2} to \texttt{GTA-3}, a similar difference is observed. In \texttt{GTA-3}, increasing $n_c$ impacts the entire convergence rate expression. Thus, a sufficient increase in $n_c$ can achieve gradient descent performance in \texttt{GTA-3} when $n_g = 1$. Based on the properties of the objective, an even higher increase in $n_c$ when $n_g = 1$ can achieve gradient descent performance in \texttt{GTA-2} as well while it does not seem possible in \texttt{GTA-1}.

\cref{col. g=1 rate bound} characterizes the effect of  multiple communication steps (when $n_g = 1$) on the convergence rates of \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3}.
%shows how performing multiple communications (when $n_g = 1$) impacts the convergence rate differently across the communication strategies in \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3}. 
First, the convergence rate improves with increased communications (increase in $n_c$) when $n_g = 1$ for all methods. The improvement is strongest in \texttt{GTA-3} as increasing $n_c$ drives the second term in the $\max$ bound to zero. Thus, if a sufficient number of communication steps are performed in \texttt{GTA-3}, the method can achieve convergence rates similar to those of gradient descent, i.e., $(1 - \tfrac{\alpha\mu}{2})$. %achieves gradient descent performance with convergence rate $(1 - \tfrac{\alpha\mu}{2})$. 
The improvement is less apparent in \texttt{GTA-2} and the weakest in \texttt{GTA-1}. With an increase in $n_c$, the dominating term in the max bound, i.e., $\sqrt{\alpha L\kappa}$, remains unchanged in \texttt{GTA-1} and changes to $\sqrt{\alpha L\kappa \beta^{n_c}}$ in \texttt{GTA-2} which is affected by the number of communication steps $n_c$. %That dominating term changes to $\sqrt{\alpha L\kappa \beta^{n_c}}$ in \texttt{GTA-2}, now reducing with increase in $n_c$.}