\subsection{Fully connected network} \label{sec.full graph res}

In this section, we analyse the methods defined in \cref{tab: Algorithm Def} under a fully connected network. While showing linear convergence of \texttt{GTA} in \cref{th. alpha bound g > 1}, we assume $B(n_c, n_g)$ is an irreducible matrix. When the network is fully connected, i.e., $\Wmbf = \tfrac{1_n1_n^T}{n}$ and $\beta = 0$, the assumption doesn't hold for \texttt{GTA-2} and \texttt{GTA-3} as the matrices  $B_2(n_c, n_g)$ and $B_3(n_c, n_g)$ defined by \cref{col. B special cases} are reducible. For \texttt{GTA-1}, such an issue does not arise as $A_1(n_c, n_g)$ defined in \cref{col. B special cases} is irreducible for all $\beta \in [0,1]$. Thus, we now present sufficient conditions for linear rate of convergence and the convergence rate for \texttt{GTA-2} and \texttt{GTA-3} under a fully connected network.

% \asb{Discuss GTA-1}

\btheorem \label{th. fully connected}
Suppose \cref{asum.convex and smooth} holds, $\Wmbf = \frac{1_n1_n^T}{n}$ and a finite number of computation steps are performed each outer iteration of \texttt{GTA-3} defined in \cref{tab: Algorithm Def} (i.e., $1 \leq n_g < \infty$). If $\alpha < \min \left\{\frac{\mu}{(2L^2 + \mu^2)(n_g-1)}, \frac{1}{L n_g}\right\}$, then for all $k \geq 0$,
\begin{align*}
    \|\xbar_{k+1, 1} - x^*\|_2 & \leq \left(
    (1 - \alpha \mu)^{n_g} + \alpha^2 L^2 n_g(n_g - 1) \right)  \|\xbar_{k, 1} - x^*\|_2.
\end{align*}
%where $(1 - \alpha \mu)^{n_g} + \alpha^2 L^2 n_g(n_g - 1) < 1$. 
Moreover, suppose the number of computation steps performed each outer iteration of \texttt{GTA-2} and \texttt{GTA-3} defined in \cref{tab: Algorithm Def} is set to one (i.e., $n_g =1$). If $\alpha \leq \frac{1}{L}$, then for both the methods, for all $k \geq 0$,
\begin{align*}
    \|\xbar_{k+1, 1} - x^*\|_2 & \leq 
    (1 - \alpha \mu) \|\xbar_{k, 1} - x^*\|_2.
\end{align*}
% where $1 - \alpha\mu < 1$.
\etheorem
\bproof
When we substitute $\beta = 0$ in \cref{col. B special cases} as $\alpha < \frac{1}{n_g L}$, the matrices $B_2(n_c, n_g)$ and $B_3(n_c, n_g)$ now have rows of zeros that make them reducible. Thus, we reduce these matrices by ignoring the error terms corresponding to the row of zeros. This yields the following systems for the progression of errors in these methods,
\begin{align}
    &\mbox{\texttt{GTA-2: }}
     \Tilde{r}_{k+1} \label{eq : g > 1 reduced 2}    
    \leq \begin{bmatrix}
    (1 - \alpha \mu)^{n_g} + \alpha^2 L^2 n_g(n_g - 1) & \frac{\alpha^2 L n_g(n_g - 1)}{\sqrt{n}}\\
    \sqrt{n}\alpha L^2 \Tilde{\delta}(n_c, n_g) & \alpha L\Tilde{\delta}(n_c, n_g)\\
    \end{bmatrix} \Tilde{r}_{k}, \\
    &\mbox{\texttt{GTA-3: }}
         \|\xbar_{k+1, 1} - x^*\|_2  \leq \left(
    (1 - \alpha \mu)^{n_g} + \alpha^2 L^2 n_g(n_g - 1) \right)
        \|\xbar_{k, 1} - x^*\|_2, \label{eq : g > 1 reduced 3} 
\end{align}
where $\Tilde{\delta}(n_c, n_g) = 1 + 2(n_g - 1)\left(2 + \tfrac{1}{n_g}\right)$ and $\Tilde{r}_{k} = \begin{bmatrix}
        \|\xbar_{k, 1} - x^*\|_2\\
        \|\ymbf_{k, 1} - \Bar{\ymbf}_{k, 1}\|_2\\
    \end{bmatrix}$. \\
    By $\alpha < \frac{\mu}{(2L^2 + \mu^2)(n_g-1)}$ and $(1-\alpha\mu)^{n_g} \leq 1 - \alpha\mu n_g + \alpha^2\mu^2 \tfrac{n_g(n_g-1)}{2}$ from \cref{th. alpha bound g > 1},
\begin{align*}
    (1 - \alpha \mu)^{n_g} + \alpha^2 L^2 n_g(n_g - 1) \leq 1 - \alpha \mu n_g + \alpha^2 \left(L^2 + \tfrac{\mu^2}{2}\right) n_g(n_g - 1) < 1,
\end{align*}
and thus the result for \texttt{GTA-3} follows. When the number of computation steps performed each outer iteration is set to one, i.e., $n_g = 1$, 
the result for \texttt{GTA-3} follows by substituting $n_g=1$ in \eqref{eq : g > 1 reduced 3}, where $1 - \alpha\mu < 1$ as $\alpha \leq \frac{1}{L}$. Substituting $n_g=1$ in \eqref{eq : g > 1 reduced 2} for \texttt{GTA-2} yields,  $\Tilde{r}_{k+1} \leq \begin{bmatrix}
    1 - \alpha \mu & 0\\
    \sqrt{n}\alpha L^2 & \alpha L\\
    \end{bmatrix}
        \Tilde{r}_{k}$, 
% \begin{align*}
%     &\mbox{\texttt{GTA-2:}}
%     &\Tilde{r}_{k+1} &\leq \begin{bmatrix}
%     1 - \alpha \mu & 0\\
%     \sqrt{n}\alpha L^2 & \alpha L\\
%     \end{bmatrix}
%         \Tilde{r}_{k}, %\label{eq : g = 1 reduced 2} 
% \end{align*}
where the bound on optimization error is independent of the consensus error in $\ymbf_{k, 1}$.
Thus, we obtain $\|\xbar_{k+1, 1} - x^*\|_2 \leq \left(1 - \alpha \mu \right) \|\xbar_{k, 1} - x^*\|_2$ for \texttt{GTA-2}.
\eproof

By \cref{th. fully connected}  if the network is fully connected and a single computation step is performed, i.e., $n_g = 1$, \texttt{GTA-2} and \texttt{GTA-3} display gradient descent performance. For \texttt{GTA-2}, when the network is fully connected and $n_g > 1$, the convergence rate can be expressed as the spectral radius of the $2\times2$ matrix in \eqref{eq : g > 1 reduced 2}. %It is computable but results in a convoluted and non interpretable expression, thus we omit it.