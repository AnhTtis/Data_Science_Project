\subsection{\texttt{GTA} with multiple communication and computation \texorpdfstring{($n_c \geq 1, n_g\geq 1$)}{Lg}} \label{sec.mult grads}
In this section, we analyze \texttt{GTA} when multiple computation and communication steps are performed every iteration. We extend the analysis from \cref{sec.mult comms}; the case $n_g = 1$ is a special case of the analysis in this section. The subscript for the inner iteration counter is re-introduced in this section as we consider cases with $n_g > 1$ and the inner loop (Lines 4--6 in \cref{alg : Deterministic}) is executed. We first provide a technical lemma that bounds the errors 
due to the execution of the inner loop. We use this result to extend \cref{lem:lyapunov g = 1} and establish the progression of the error vector $r_k$ when multiple communication and computation steps are performed. Finally, we provide the conditions for linear convergence of \cref{alg : Deterministic} with any composition of communication and computation steps.

\begin{lemma} \label{lem : inner loop deviations}
Suppose \cref{asum.convex and smooth} holds and $\alpha \leq \frac{1}{n_gL}$ in \cref{alg : Deterministic}. Then, for all $k\geq0$ and $1 \leq j \leq n_g$
\begin{align}%   \label{eq : iterate deviation bound}
    \|\Bar{\ymbf}_{k,1}\|_2 &\leq \|\ymbf_{k, 1} -  \bar{\ymbf}_{k, 1}\|_2 + L\left\|\xmbf_{k,1} - \bar{\xmbf}_{k,1}\right\|_2 + L\sqrt{n} \| \Bar{x}_{k,1} - x^*\|_2\label{eq:y_bound} \\
    \|\xmbf_{k, j} - \xmbf_{k, 1}\|_2 &\leq 2\alpha (j - 1 ) \|\ymbf_{k, 1}\|_2, \label{eq:iterate_deviation}\\
    \|\xmbf_{k, j} - \xbb_{k, j}\|_2 &\leq 2 \alpha (j - 1)\|\ymbf_{k, 1}\|_2 + \|\xmbf_{k, 1} - \xbb_{k, 1}\|_2, \label{eq:consensus_error_deviation}
\end{align}
% \begin{align*}%   \label{eq : iterate deviation bound}
%     \|\xmbf_{k, j} - \xmbf_{k, 1}\|_2 &\leq 2\alpha (j - 1 )\left[ \|\ymbf_{k, 1} - \bar{\ymbf}_{k, 1}\|_2 + L\|\xmbf_{k, 1} - \bar{\xmbf}_{k, 1}\|_2 + \sqrt{n}L\|\Bar{x}_{k, 1} - x^*\|_2 \right]\\
%     \|\xmbf_{k, j} - \xbb_{k, j}\|_2 &\leq 2 \alpha (j - 1)\left[ \|\ymbf_{k, 1} - \ybb_{k, 1}\|_2 + \sqrt{n}L \|\bar{x}_{k, 1} - x^*\|_2\right] \\
%     & \quad + (2\alpha (j - 1)L + 1)\|\xmbf_{k, 1} - \xbb_{k, 1}\|_2. 
% \end{align*}
\end{lemma}
\bproof
% \asb{Summing $\ymbf_{k, i}$~\eqref{} from $i=2$ to $j$, 
Taking a telescopic sum of $\ymbf_{k, i+1} = \ymbf_{k, i} + \nabla \fmbf(\xmbf_{k, i+1}) - \nabla \fmbf(\xmbf_{k, i})$, the inner loop update, from $i=1$ to $j-1$ we get
\begin{align}
    \ymbf_{k, j} & = \ymbf_{k, 1} + \nabla \fmbf(\xmbf_{k, j}) - \nabla \fmbf(\xmbf_{k, 1}). \label{eq: v telescope}
\end{align}
Using~\eqref{eq: v telescope},  $\ymbf_{k+1, 1}$ can be expressed as
\begin{equation}\label{eq : y general telescope sum}
\begin{aligned}
    \ymbf_{k+1, 1} &= \Zmbf_3^{n_c}\left(\ymbf_{k, 1} + \nabla \fmbf(\xmbf_{k, n_g}) - \nabla \fmbf(\xmbf_{k, 1})\right) + \Zmbf_4^{n_c}\left(\nabla \fmbf(\xmbf_{k+1, 1}) - \nabla \fmbf(\xmbf_{k, n_g})\right)   \\  
    &= \Zmbf_3^{n_c}\ymbf_{k, 1} + \Zmbf_4^{n_c}\nabla \fmbf(\xmbf_{k+1, 1}) - \Zmbf_3^{n_c}\nabla \fmbf(\xmbf_{k, 1}) + \Zmbf_3^{n_c}\nabla \fmbf(\xmbf_{k, n_g}) - \Zmbf_4^{n_c} \nabla \fmbf(\xmbf_{k, n_g})
\end{aligned}
\end{equation}
Taking the component-wise average across all nodes in \eqref{eq: v telescope} and \eqref{eq : y general telescope sum} and using~\eqref{eq : derivative terms define}, it follows that
\begin{align}
    \Bar{y}_{k, j} & = \Bar{y}_{k, 1} + h_{k, j} - h_{k, 1}, \label{eq: g > 1 y_j_telescope}\\
    \Bar{y}_{k+1, 1} & = \Bar{y}_{k, 1} + h_{k+1, 1} - h_{k, 1}.  \label{eq : g > 1 y bar telescope}
\end{align}
% Following similar logic as that used to derive~\eqref{eq:y_bar_telescope}, $\Bar{y}_{k, 1} = h_{k, 1}$, from which it follows by~\eqref{eq: g > 1 y_j_telescope} that 
% \begin{align}   \label{eq: v_bar_telescope}
%     \bar{y}_{k, j} &= \bar{y}_{k, 1} + h_{k, j} - h_{k, 1} = h_{k, j}.
% \end{align}
Performing a similar telescopic sum as \eqref{eq:y_bar_telescope} with \eqref{eq : g > 1 y bar telescope}, we obtain $\Bar{y}_{k, 1} = h_{k, 1}$. Thus, substituting $\Bar{y}_{k, 1} = h_{k, 1}$ in \eqref{eq: g > 1 y_j_telescope} yields 
%a generalization of \eqref{eq:y_bar_telescope}, i.e., 
\begin{align}   \label{eq: v_bar_telescope}
    \bar{y}_{k, j} &= \bar{y}_{k, 1} + h_{k, j} - h_{k, 1} = h_{k, j}.
\end{align}
By the triangle inequality, $\|\ymbf_{k, 1}\|_2 \leq \|\ymbf_{k, 1} -  \bar{\ymbf}_{k, 1}\|_2   + \| \bar{\ymbf}_{k, 1} \|_2$, where $\|\bar{\ymbf}_{k, 1}\|_2$ can be bounded by a similar procedure to \eqref{eq : y_bar_bound} due to $\Bar{y}_{k, 1} = h_{k, 1}$ to yield \eqref{eq:y_bound}.
% By $\Bar{y}_{k, 1} = h_{k, 1}$, we can also perform a procedure similar to \eqref{eq : y_bar_bound} to obtain a bound on $\|\ybb_{k,1}\|_2$ as
% \begin{align}   \label{eq : y_bar_bnd g > 1}
% \left\|\Bar{\ymbf}_{k,1}\right\|_2 &\leq L\left\|\xmbf_{k,1} - \bar{\xmbf}_{k,1}\right\|_2 + L\sqrt{n} \| \Bar{x}_{k,1} - x^*\|_2.
% \end{align}
% Now, by triangle inequality and \eqref{eq : y_bar_bnd g > 1},
% \begin{equation} \label{eq: g>1_y_bound}
%     \begin{aligned}
%         \|\ymbf_{k, 1}\|_2 &\leq \|\ymbf_{k, 1} -  \bar{\ymbf}_{k, 1}\|_2   + \| \bar{\ymbf}_{k, 1} \|_2 \\
%         &\leq \|\ymbf_{k, 1} -  \bar{\ymbf}_{k, 1}\|_2 + L\left\|\xmbf_{k,1} - \bar{\xmbf}_{k,1}\right\|_2 + L\sqrt{n} \| \Bar{x}_{k,1} - x^*\|_2.
%     \end{aligned}
% \end{equation}
% Taking the telescopic sum of $\ymbf_{k, i}$ from $i=2$ to $j$, it follows that
% \begin{align}
%     \ymbf_{k, j} & = \ymbf_{k, 1} + \nabla \fmbf(\xmbf_{k, j}) - \nabla \fmbf(\xmbf_{j, 1}). \label{eq: v telescope}
% \end{align}
% Taking the component wise average across all nodes in \eqref{eq: v telescope} and using \eqref{eq : derivative terms define}, we get 
% \begin{align}
%     \Bar{y}_{k, j} & = \Bar{y}_{k, 1} + h_{k, j} - h_{k, 1}.  \label{eq: g > 1 y_j_telescope}
% \end{align}
% Using \eqref{eq: v telescope}, $\ymbf_{k+1, 1}$ can be expanded as
% \begin{align*}
%     \ymbf_{k+1, 1} &= \Zmbf_3^{n_c}\left(\ymbf_{k, 1} + \nabla \fmbf(\xmbf_{k, n_g}) - \nabla \fmbf(\xmbf_{k, 1})\right) + \Zmbf_4^{n_c}\left(\nabla \fmbf(\xmbf_{k+1, 1}) - \nabla \fmbf(\xmbf_{k, n_g})\right)   \numberthis \label{eq : y general telescope sum}\\  
%     &= \Zmbf_3^{n_c}\ymbf_{k, 1} + \Zmbf_4^{n_c}\nabla \fmbf(\xmbf_{k+1, 1}) - \Zmbf_3^{n_c}\nabla \fmbf(\xmbf_{k, 1}) + \Zmbf_3^{n_c}\nabla \fmbf(\xmbf_{k, n_g}) - \Zmbf_4^{n_c} \nabla \fmbf(\xmbf_{k, n_g})
% \end{align*}
% Taking the component wise average across all nodes in \eqref{eq : y general telescope sum} and using \eqref{eq : derivative terms define} yields
% \begin{align}
%     \Bar{y}_{k+1, 1} & = \Bar{y}_{k, 1} + h_{k+1, 1} - h_{k, 1}  \label{eq : g > 1 y bar telescope}.
% \end{align}
% Thus, performing a similar telescopic sum as
% \eqref{eq:y_bar_telescope} using \eqref{eq : g > 1 y bar telescope}, we obtain $\Bar{y}_{k, 1} = h_{k, 1}$. Using this in \eqref{eq: g > 1 y_j_telescope} yields a generalization of \eqref{eq:y_bar_telescope}, i.e.
% % Thus, the telescopic sum of $\Bar{y}_{k, i}$ from $i = 2$ to $j$ yields a generalization of \eqref{eq:y_bar_telescope}, i.e.
% \begin{align}   \label{eq: v_bar_telescope}
%     \bar{y}_{k, j} &= \bar{y}_{k, 1} + h_{k, j} - h_{k, 1} = h_{k, j}.
% \end{align}
% So following a procedure similar to \eqref{eq : y_bar_bound}, we can produce a bound for $\|\ybb_{k,1}\|_2$ as 
% \begin{align}   \label{eq : y_bar_bnd g > 1}
% \left\|\Bar{\ymbf}_{k,1}\right\|_2 &\leq L\left\|\xmbf_{k,1} - \bar{\xmbf}_{k,1}\right\|_2 + L\sqrt{n} \| \Bar{x}_{k,1} - x^*\|_2
% \end{align}

Now, taking the telescopic sum of the inner loop update $\xmbf_{k, i} = \xmbf_{k, i-1} - \alpha \ymbf_{k, i-1}$ from $i=2$ to $j$ yields $\xmbf_{k, j} = \xmbf_{k, 1} - \alpha \sum_{i=1}^{j-1}\ymbf_{k, i}$. The sum $\sum_{i = 1}^{j-1} \ymbf_{k, i}$ is evaluated using \eqref{eq: v telescope} as
\begin{equation}\label{eq: sum_y_k_j}
\begin{aligned}   
    \sum_{i = 1}^{j -1} \ymbf_{k, j} & = \ymbf_{k, 1} + \sum_{i = 2}^{j-1} \ymbf_{k, i} + \nabla \fmbf(\xmbf_{k, i}) - \nabla \fmbf(\xmbf_{k, 1}) \\
    &= (j - 1) \ymbf_{k, 1} + \sum_{i = 2}^{j - 1} \nabla \fmbf(\xmbf_{k, i}) - \nabla \fmbf(\xmbf_{k, 1}).
\end{aligned}
\end{equation}
By the triangle inequality and \cref{asum.convex and smooth}, it follows that
% \begin{align*}
% \xmbf_{k, j} &= \xmbf_{k, 1} - \alpha \sum_{i=1}^{j-1}\ymbf_{k, i} = \xmbf_{k, 1} - \alpha \sum_{i=1}^{j-1}\left[ \ymbf_{k, 1} + \nabla \fmbf(\xmbf_{k, i}) - \nabla \fmbf(\xmbf_{k, 1}) \right]\\
% &= \xmbf_{k, 1} - \alpha(j-1)\ymbf_{k, 1}  - \alpha  \sum_{i=1}^{j-1} \left[ \nabla \fmbf(\xmbf_{k, i}) - \nabla \fmbf(\xmbf_{k, 1}) \right].
% \end{align*}
% Therefore, for any $j \geq 2$, we have, 
\begin{align*}
\|\xmbf_{k, j} - \xmbf_{k, 1}  \|_2 &\leq \alpha(j-1)\|\ymbf_{k, 1}\|_2 + \alpha  \sum_{i=1}^{j-1}\| \nabla \fmbf(\xmbf_{k, i}) - \nabla \fmbf(\xmbf_{k, 1})\|_2 \\
&\leq  \alpha(j-1)\|\ymbf_{k, 1}\|_2 + \alpha L \sum_{i=1}^{j-1}\| \xmbf_{k, i} - \xmbf_{k, 1}\|_2.
\end{align*}
Now we apply induction to show \eqref{eq:iterate_deviation} using the above inequality.
% Now we apply induction to show $\|\xmbf_{k, j} - \xmbf_{k, 1}  \|_2 \leq 2 \alpha (j-1) \|\ymbf_{k, 1}\|_2$ for all $j=1,\dots,n_g$ using the above inequality.
\begin{align*}
    \mbox{For $j = 1$,} \qquad \|\xmbf_{k, 1} - \xmbf_{k, 1}\|_2 &= 0 =  2 \alpha (1 - 1) \|\ymbf_{k, 1}\|_2. \\
    \mbox{For $j \geq 2$,} \qquad \|\xmbf_{k, j} - \xmbf_{k, 1}\|_2 &\leq \alpha(j-1)\|\ymbf_{k, 1}\|_2 + \alpha L \sum_{i=1}^{j-1}\| \xmbf_{k, i} - \xmbf_{k, 1}\|_2 \\
    &\leq \alpha(j-1)\|\ymbf_{k, 1}\|_2 + 2 \alpha^2 L  \sum_{i=1}^{j-1} (i-1) \|\ymbf_{k, 1}\|_2 \\
    &=  \alpha(j-1)\|\ymbf_{k, 1}\|_2 + 2 \alpha^2 L  \|\ymbf_{k, 1}\|_2 \tfrac{(j-2)(j-1)}{2} \\
    &= \alpha(j-1) \left(1 + \alpha L (j-2) \right)\|\ymbf_{k, 1}\|_2  \\
    %&\leq \alpha(j-1) \left(1 + \tfrac{(n_g-2)}{n_g} \right)\|\ymbf_{k, 1}\|_2  \\
    %& = \alpha(j-1)\|\ymbf_{k, 1}\|_2 \left(1 + \frac{n_g-2}{n_g + 2} \right) \\
    &\leq 2 \alpha(j-1)\|\ymbf_{k, 1}\|_2,
\end{align*}
where the first equality uses the sum of $j-1$ natural numbers and the second to last inequality is due to $\alpha L \leq \frac{1}{n_g}$ and $j \leq n_g$. 
% \begin{align*}
% \|\xmbf_{k, j} - \xmbf_{k, 1}\|_2 &\leq 2 \alpha (j-1) \|\ymbf_{k, 1}\|_2 \\
% \|\xmbf_{k, j} - \xmbf_{k, 1}\|_2 &\leq 2 \alpha (j-1) \left(\|\ymbf_{k, 1} -  \bar{\ymbf}_{k, 1}\|_2   + \| \bar{\ymbf}_{k, 1} \|_2\right).
% \end{align*}
% The first result follows by substituting \eqref{eq : y_bar_bnd g > 1} in the above.
% The first result follows by substituting \eqref{eq: g>1_y_bound} in the induction result.

By \eqref{eq:iterate_deviation}, the triangle inequality and $\|I_{nd} - \Imbf\|_2 = 1$, it follows that
\begin{align*}
    \|\xmbf_{k, j} - \xbb_{k, j}\|_2 &\leq \|\xmbf_{k, j} - \xmbf_{k, 1} + \xbb_{k,1} -  \xbb_{k, j}\|_2 + \|\xmbf_{k, 1} - \xbb_{k, 1}\|_2    \\
    &\leq \|(I_{nd} - \Imbf)(\xmbf_{k, j} - \xmbf_{k, 1})\|_2 + \|\xmbf_{k, 1} - \xbb_{k, 1}\|_2    \\
    &\leq \|\xmbf_{k, j} - \xmbf_{k, 1}\|_2 + \|\xmbf_{k, 1} - \xbb_{k, 1}\|_2.    
\end{align*}
% where the last inequality is due to $\|I_{nd} - \Imbf\|_2 = 1$.
\eproof% and use \eqref{eq : iterate deviation bound} in the end.
% \eproof
% \asb{The first result XXX}

The two bounds, \eqref{eq:iterate_deviation} and \eqref{eq:consensus_error_deviation}, in \cref{lem : inner loop deviations} bound the deviation of the local decision variables from the start of the outer iteration, $\|\xmbf_{k, j} - \xmbf_{k, 1}\|_2$, and the consensus error, $\|\xmbf_{k, j} - \xbb_{k, j}\|_2$,  in inner iteration $j$, respectively. Combined with \eqref{eq:y_bound}, these quantities are bounded as an $\mathcal{O}(\alpha j)$ multiple of the components of the error vector $r_k$. This property has two implications; $(1)$ if one performs more inner iterations, i.e., increases $n_g$, the constant step size $\alpha$ needs to be reduced to reduce these quantities, $(2)$ if an outer iteration is the optimal solution, the inner loop does not introduce any deviations in the iterates and maintains optimality.

We now establish the progression of error vector $r_k$ under multiple communication and computation steps being performed every iteration in \cref{alg : Deterministic}.

\begin{lemma}\label{lem:lyapunov g > 1}
Suppose \cref{asum.convex and smooth} holds and $\alpha \leq \frac{1}{n_g L}$ in \cref{alg : Deterministic}. Then, for all $k\geq 0$,
\begin{align*}
  r_{k+1} \leq B(n_c, n_g) r_k, \quad \text{where $\; B(n_c, n_g) = A(n_c, n_g) + \alpha L (n_g - 1) E(n_c, n_g)$}, \quad     
\end{align*}
% where $B(n_c, n_g) = A(n_c, n_g) + \alpha L (n_g - 1) E(n_c, n_g)$,
\begin{equation}\label{eq : g > 1 general A}
\begin{aligned}
A(n_c, n_g) &= \begin{bmatrix}
    (1 - \alpha \mu)^{n_g}& \frac{\kappa}{\sqrt{n}}(1 - (1 - \alpha\mu)^{n_g}) & 0\\
    0 & \beta_1^{n_c} & \alpha\left((n_g-1)\beta_1^{n_c} + \beta_2^{n_c}\right)\\
    \sqrt{n}\alpha \beta_4^{n_c} L^2 & \beta_4^{n_c}L(\|\Zmbf_1^{n_c}-I_{nd}\|_2 + \alpha L) & \beta_3^{n_c} + \alpha \beta_4^{n_c} L
    \end{bmatrix} ,\\
E(n_c, n_g) &= \begin{bmatrix}
    \alpha L n_g & \frac{\alpha L n_g}{\sqrt{n}} & \frac{\alpha n_g}{\sqrt{n}}\\
    \sqrt{n}\alpha L \delta_1(n_c, n_g) & \alpha L \delta_1(n_c, n_g) & \alpha \delta_1(n_c, n_g)\\
    \sqrt{n} L\delta_2(n_c, n_g) & L \delta_2(n_c, n_g)& \delta_2(n_c, n_g)
    \end{bmatrix},
\end{aligned}
\end{equation}
and
\begin{equation}\label{eq:delta_errors}
\begin{aligned}
    \delta_1(n_c, n_g) &= 2\beta_2^{n_c} + \beta_1^{n_c}(n_g - 2) ,\\
    \delta_2(n_c, n_g) &= 2 \left( \beta_4^{n_c} \|\Zmbf_1^{n_c} - I_{nd}\|_2 + \tfrac{\beta_4^{n_c}}{n_g} + \beta_3^{n_c} \right).
\end{aligned}
\end{equation}
\end{lemma}
\bproof We first consider the optimization error of the average iterates $\xbar_{k, 1}$. Similar to \eqref{eq : g = 1 opt bound}, we  bound the  optimization error as %for $\xbar_{k, j}$, the average iterate in the inner iterations as
\begin{align*}
    % \|\Bar{x}_{k, j+1} - x^*\|_2 & = \left\|\Bar{x}_{k, j} - \alpha \Bar{y}_{k, j} + \alpha \hbar_{k, j} - \alpha \hbar_{k, j} - x^*\right\|_2  \\
    % & \leq (1-\alpha \mu) \|\Bar{x}_{k, j} - x^*\|_2 + \alpha \left\| \Bar{y}_{k, j} - \hbar_{k, j}\right\|_2   \\
    % &=  (1-\alpha \mu) \|\Bar{x}_{k, j} - x^*\|_2 + \alpha \left\|h_{k, j} - \hbar_{k, j}\right\|_2 \\
    % &\leq (1-\alpha \mu) \|\Bar{u}_j - x^*\|_2 + \alpha \frac{\|1\|}{n}\left\|\nabla f(u_{j}) - \nabla f(1\Bar{u}_{j}^T)\right\|_2 \\
    \|\Bar{x}_{k, j+1} - x^*\|_2 & \leq (1-\alpha \mu) \|\Bar{x}_{k, j} - x^*\|_2 + \tfrac{\alpha L}{\sqrt{n}} \| \xmbf_{k, j} - \xbb_{k, j}\|_2 \quad  \forall \,\, 1 \leq j \leq n_g - 1,
\end{align*}
where the above holds by using \eqref{eq: v_bar_telescope} (the generalization of \eqref{eq:y_bar_telescope}) and the error bound of gradient descent from \cite[Theorem 2.1.14]{nesterov1998introductory}.  %due to \eqref{eq: v_bar_telescope} as a generalization of \eqref{eq:y_bar_telescope} and the error bound of gradient descent from \cite[Theorem 2.1.14]{nesterov1998introductory}. 
Next, we bound the optimization error in $\xbar_{k+1, 1}$ with respect to $\xmbf_{k, n_g}$ in a similar manner as, %The optimization error for $\xbar_{k+1, 1}$ can be bound  w.r.t. $\xmbf_{k, n_g}$, the last inner iterate, in a similar way as
\begin{align*}
    \|\Bar{x}_{k+1, 1} - x^*\|_2 & \leq (1-\alpha \mu) \|\Bar{x}_{k, n_g} - x^*\|_2 + \tfrac{\alpha L}{\sqrt{n}} \| \xmbf_{k, n_g} - \xbb_{k, n_g}\|_2.
\end{align*}
% where in the first inequality, we used the bound that is obtained by performing one gradient descent iteration on the strongly convex function $f$ (\cref{asum.convex and smooth}) at the average iterate $\bar{x}_{k, j}$ with $\alpha < \frac{1}{L}$ \cite[Theorem 2.1.14]{nesterov1998introductory}, the second equality is due to \eqref{eq: v_bar_telescope}, and the last inequality is due to \cref{asum.convex and smooth}. 
% using the error bound of gradient descent from \cite[Theorem 2.1.14]{nesterov1998introductory} and due to \eqref{eq: v_bar_telescope}.
Recursively applying the above two bounds, by \eqref{eq:consensus_error_deviation} it follows that,
% We join the two bounds above by telescoping as follows,
% \begin{align*}
%     \|\Bar{x}_{k, n_g} - x^*\|_2 & \leq (1-\alpha \mu)^{n_g-1} \|\Bar{x}_{k, 1} - x^*\|_2 + \frac{\alpha L}{\sqrt{n}} \sum_{j = 1}^{n_g-1}(1 - \alpha\mu)^{n_g - j - 1}\| \xmbf_{k, j} - \xbb_{k, j}\|_2.
% \end{align*}
% We obtain the optimization error at $x_{k+1}$ the same way as
% \begin{align*}
%     \|\Bar{x}_{k+1, 1} - x^*\|_2 & \leq (1-\alpha \mu) \|\Bar{x}_{k, n_g} - x^*\|_2 + \frac{\alpha L}{\sqrt{n}} \| \xmbf_{k, n_g} - \xbb_{k, n_g}\|_2.
% \end{align*}
% We combine the above 2 equations to get
\begin{align*}
    \|\Bar{x}_{k+1, 1} - x^*\|_2 & 
    % \leq (1-\alpha \mu)^2 \|\Bar{x}_{k, n_g - 1} - x^*\|_2 + \tfrac{(1-\alpha \mu) \alpha L}{\sqrt{n}} \| \xmbf_{k, n_g - 1} - \xbb_{k, n_g - 1}\|_2 \\
    % & \quad+ \tfrac{\alpha L}{\sqrt{n}} \| \xmbf_{k, n_g} - \xbb_{k, n_g}\|_2 \\
    % & \leq (1-\alpha \mu)^{n_g} \|\Bar{x}_{k, 1} - x^*\|_2 + \tfrac{\alpha L}{\sqrt{n}} \sum_{j = 1}^{n_g}(1 - \alpha\mu)^{n_g - j}\| \xmbf_{k, j} - \xbb_{k, j}\|_2 \\
    \leq (1-\alpha \mu)^{n_g} \|\Bar{x}_{k, 1} - x^*\|_2  + \tfrac{\alpha L}{\sqrt{n}} \sum_{j = 1}^{n_g}(1 - \alpha\mu)^{n_g - j} \|\xmbf_{k,1} - \xbb_{k, 1}\|_2 \\
    & \quad + \tfrac{2\alpha^2 L}{\sqrt{n}} \sum_{j = 1}^{n_g}(1 - \alpha\mu)^{n_g - j} (j-1) \|\ymbf_{k, 1}\|_2\\
    & \leq (1-\alpha \mu)^{n_g} \|\Bar{x}_{k, 1} - x^*\|_2  + \tfrac{\kappa}{\sqrt{n}} \left[1 - (1 - \alpha\mu)^{n_g}\right] \|\xmbf_{k, 1} - \xbb_{k, 1}\|_2 \\
    & \quad + \tfrac{\alpha^2 L}{\sqrt{n}} n_g(n_g - 1) \|\ymbf_{k, 1}\|_2,
\end{align*}
% Old full version
% \begin{align*}
%     \|\Bar{x}_{k+1, 1} - x^*\|_2 & \leq (1-\alpha \mu)^2 \|\Bar{x}_{k, n_g - 1} - x^*\|_2 + \tfrac{(1-\alpha \mu) \alpha L}{\sqrt{n}} \| \xmbf_{k, n_g - 1} - \xbb_{k, n_g - 1}\|_2 \\
%     & \quad+ \tfrac{\alpha L}{\sqrt{n}} \| \xmbf_{k, n_g} - \xbb_{k, n_g}\|_2 \\
%     & \leq (1-\alpha \mu)^{n_g} \|\Bar{x}_{k, 1} - x^*\|_2 + \tfrac{\alpha L}{\sqrt{n}} \sum_{j = 1}^{n_g}(1 - \alpha\mu)^{n_g - j}\| \xmbf_{k, j} - \xbb_{k, j}\|_2 \\
%     & \leq (1-\alpha \mu)^{n_g} \|\Bar{x}_{k, 1} - x^*\|_2  + \tfrac{\alpha L}{\sqrt{n}} \sum_{j = 1}^{n_g}(1 - \alpha\mu)^{n_g - j} \|\xmbf_{k,1} - \xbb_{k, 1}\|_2 \\
%     & \quad + \tfrac{2\alpha^2 L}{\sqrt{n}} \sum_{j = 1}^{n_g}(1 - \alpha\mu)^{n_g - j} (j-1) \|\ymbf_{k, 1}\|_2. 
% \end{align*}
%where the last inequality follows from \eqref{eq:consensus_error_deviation}. 
where the last inequality is due to the fact that $(1 - \alpha\mu)^{n_g - j} \leq 1 \,\forall j = 1, 2, ..., n_g$ due to $\alpha \leq \frac{1}{L n_g}$, the coefficient of the second term is the sum of a geometric progression, and the coefficient of the third term is the sum of the first $n_g-1$ natural numbers. 
% In the above bound, the second term coefficient is the sum of a geometric progression. In the third term coefficient, we bound $(1 - \alpha\mu)^{n_g - j} \leq 1 \,\forall j = 1, 2, ..., n_g$ due to $\alpha \leq \frac{1}{L n_g}$ and calculate the resulting coefficient as the sum of the first $n_g - 1$ natural numbers. This yields
% \begin{align*}
    % \|\Bar{x}_{k+1, 1} - x^*\|_2 
    % & \leq (1-\alpha \mu)^{n_g} \|\Bar{x}_{k, 1} - x^*\|_2  + \frac{\alpha L}{\sqrt{n}} \sum_{j = 1}^{n_g}(1 - \alpha\mu)^{n_g - j} \|\xmbf_{k,1} - \xbb_{k, 1}\|_2 \\
    % & \quad + \frac{\alpha L}{\sqrt{n}} \sum_{j = 2}^{n_g}(1 - \alpha\mu)^{n_g - j}\|\xmbf_{k, j} - \xmbf_{k, 1}\|_2\ \\
    % & \leq (1-\alpha \mu)^{n_g} \|\Bar{x}_{k, 1} - x^*\|_2  + \frac{\alpha L}{\sqrt{n}} \left[\frac{1 - (1 - \alpha\mu)^{n_g}}{1 - (1 - \alpha\mu)}\right] \|\xmbf_{k, 1} - \xbb_{k, 1}\|_2 \\
    % & \quad + \frac{2\alpha^2 L}{\sqrt{n}} \sum_{j = 1}^{n_g}(1 - \alpha\mu)^{n_g - j} (j-1) \|\ymbf_{k, 1}\|_2.
% \end{align*}
% We bound $(1 - \alpha\mu)^{n_g - j} \leq 1 \,\,\forall j = 1, 2, ..., n_g$ as $\alpha \leq \frac{1}{L}$ to get
% \begin{align*}
%     \|\Bar{x}_{k+1, 1} - x^*\|_2 
%     & \leq (1-\alpha \mu)^{n_g} \|\Bar{x}_{k, 1} - x^*\|_2  + \tfrac{\kappa}{\sqrt{n}} \left[1 - (1 - \alpha\mu)^{n_g}\right] \|\xmbf_{k, 1} - \xbb_{k, 1}\|_2 \\
%     & \quad + \tfrac{\alpha^2 L}{\sqrt{n}} n_g(n_g - 1) \|\ymbf_{k, 1}\|_2
% \end{align*} 
By \eqref{eq:y_bound}, we obtain the desired bound on the optimization error.

Next, we consider the consensus error in $\xmbf_{k, 1}$,
\begin{align*}
     &\xmbf_{k + 1, 1} - \xbb_{k+1, 1} \\
    =& \left(I_{nd} - \Imbf\right)\xmbf_{k+1, 1} = \left(I_{nd} - \Imbf \right)(\Zmbf_1^{n_c} \xmbf_{k, n_g} - \alpha \Zmbf_2^{n_c}\ymbf_{k, n_g})    \\
    =& \left(I_{nd} - \Imbf \right)\left(\Zmbf_1^{n_c}\left(\xmbf_{k, 1} - \alpha\sum_{j = 1}^{n_g - 1}\ymbf_{k, j}\right) - \alpha \Zmbf_2^{n_c}\ymbf_{k, n_g}\right) \\
    =& \left(I_{nd} - \Imbf\right)\left(\Zmbf_1^{n_c}\xmbf_{k, 1} - \alpha \Zmbf_1^{n_c}\left((n_g-1) \ymbf_{k, 1} + \sum_{j = 2}^{n_g-1} \nabla \fmbf(\xmbf_{k, j}) - \nabla \fmbf(\xmbf_{k, 1})\right)\right) \\
    & \; - \alpha \left(I_{nd} - \Imbf\right) \left(\Zmbf_2^{n_c}( \ymbf_{k, 1} + \nabla \fmbf(\xmbf_{k, n_g}) - \nabla \fmbf(\xmbf_{k, 1}))\right)\\
    =& \left(\Zmbf_1^{n_c} - \Imbf\right)(\xmbf_{k, 1} - \xbb_{k, 1}) -\alpha \left((n_g-1)\left(\Zmbf_1^{n_c} - \Imbf\right) + \left(\Zmbf_2^{n_c} - \Imbf\right)\right) (\ymbf_{k, 1} - \ybb_{k, 1})  \\
    & \; - \alpha\left(\Zmbf_2^{n_c} - \Imbf\right)\left(\nabla \fmbf(\xmbf_{k, n_g}) - \nabla \fmbf(\xmbf_{k, 1})\right) - \alpha\left(\Zmbf_1^{n_c} - \Imbf\right)\left(\sum_{j = 2}^{n_g-1} \nabla \fmbf(\xmbf_{k, j}) - \nabla \fmbf(\xmbf_{k, 1})\right) 
\end{align*}
where the second equality is a telescopic sum of the inner loop update ($\xmbf_{k, i} = \xmbf_{k, i-1} - \alpha \ymbf_{k, i-1}$) from $i=2$ to $n_g$ and the third equality is due to \eqref{eq: sum_y_k_j}. By the triangle inequality, \cref{asum.convex and smooth} and \eqref{eq : beta and Z},
\begin{align*}
    \|\xmbf_{k + 1, 1} - \xbb_{k+1, 1}\|_2 &\leq \beta_1^{n_c}\|\xmbf_{k, 1} - \xbb_{k, 1}\|_2 + \alpha \left((n_g-1)\beta_1^{n_c} + \beta_2^{n_c}\right) \|\ymbf_{k, 1} - \ybb_{k, 1}\|_2 \\
    & \quad + \alpha\beta_2^{n_c} L\|\xmbf_{k, n_g} - \xmbf_{k, 1}\|_2 + \alpha\beta_1^{n_c} L \sum_{j = 2}^{n_g-1} \|\xmbf_{k, j} - \xmbf_{k, 1}\|_2.
\end{align*}
Adding $\alpha\beta_1^{n_c} L \|\xmbf_{k, 1} - \xmbf_{k, 1}\|_2 = 0$ to the right hand side and \eqref{eq:iterate_deviation}, it follows,
\begin{align*}
    \|\xmbf_{k + 1, 1} - \xbb_{k+1, 1}\|_2 &\leq \beta_1^{n_c}\|\xmbf_{k, 1} - \xbb_{k, 1}\|_2 + \alpha \left((n_g-1)\beta_1^{n_c} + \beta_2^{n_c}\right) \|\ymbf_{k, 1} - \ybb_{k, 1}\|_2 \\
    & \quad + \alpha^2\beta_2^{n_c} L (2(n_g - 1)) \|y_{k, 1}\|_2 + \alpha^2\beta_1^{n_c} L \sum_{j = 1}^{n_g-1} 2(j - 1) \|y_{k, 1}\|_2 \\
    &= \beta_1^{n_c}\|\xmbf_{k, 1} - \xbb_{k, 1}\|_2 + \alpha \left((n_g-1)\beta_1^{n_c} + \beta_2^{n_c}\right) \|\ymbf_{k, 1} - \ybb_{k, 1}\|_2 \\
    & \quad + 2\alpha^2 L (n_g - 1) \left( \beta_2^{n_c} + \beta_1^{n_c}\tfrac{(n_g - 2)}{2}  \right)\|\ymbf_{k, 1}\|_2.
\end{align*}
The desired bound for the consensus error in $\xmbf_{k, 1}$ follows by using \eqref{eq:y_bound}.
% Substituting \eqref{eq:y_bound} in the above gives the desired bound for consensus error in $\xmbf_{k, 1}$. 

Finally, we consider the consensus error in $\ymbf_{k, 1}$. By \eqref{eq : y general telescope sum},
\begin{align*}
    \ymbf_{k + 1, 1} - \ybb_{k+1, 1} &= \left(I_{nd} - \Imbf\right)\ymbf_{k+1, 1} \\
    &= \left(I_{nd} - \Imbf\right)(\Zmbf_3^{n_c}\ymbf_{k, 1} + \Zmbf_4^{n_c}\nabla \fmbf(\xmbf_{k+1, 1}) - \Zmbf_3^{n_c}\nabla \fmbf(\xmbf_{k, 1})) \\
    &\quad + \left(I_{nd} - \Imbf\right)(\Zmbf_3^{n_c}\nabla \fmbf(\xmbf_{k, n_g}) - \Zmbf_4^{n_c} \nabla \fmbf(\xmbf_{k, n_g}))\\
    & = \left(\Zmbf_3^{n_c} - \Imbf\right)(\ymbf_{k, 1} - \ybb_{k, 1}) + \left(\Zmbf_4^{n_c} - \Imbf\right)(\nabla \fmbf(\xmbf_{k+1, 1}) - \nabla \fmbf(\xmbf_{k, n_g})) \\
    & \quad + \left(\Zmbf_3^{n_c} - \Imbf\right)(\nabla \fmbf(\xmbf_{k, n_g}) - \nabla \fmbf(\xmbf_{k, 1}) )
\end{align*}
By \cref{asum.convex and smooth} and \eqref{eq : beta and Z},
\begin{align*}
    &\|\ymbf_{k + 1, 1} - \ybb_{k+1, 1}\|_2 \\
    &\leq \beta_3^{n_c}\|\ymbf_{k, 1} - \ybb_{k, 1}\|_2 + \beta_4^{n_c} L\|\xmbf_{k+1, 1} - \xmbf_{k, n_g}\|_2 + \beta_3^{n_c} L\|\xmbf_{k, n_g} - \xmbf_{k, 1}\|_2 \\
    &= \beta_3^{n_c}\|\ymbf_{k, 1} - \ybb_{k, 1}\|_2 + \beta_4^{n_c} L\|(\Zmbf_1^{n_c} - I_{nd})(\xmbf_{k, n_g} - \xbb_{k, n_g}) - \alpha \Zmbf_2^{n_c} \ymbf_{k, n_g}\|_2   \\
    & \quad+ \beta_3^{n_c} L\|\xmbf_{k, n_g} - \xmbf_{k, 1}\|_2\\
    &\leq \beta_3^{n_c}\|\ymbf_{k, 1} - \ybb_{k, 1}\|_2 + \beta_4^{n_c} L\|\Zmbf_1^{n_c} - I_{nd}\|_2\|\xmbf_{k, n_g} - \xbb_{k, n_g}\|_2 + \alpha \beta_4^{n_c} L \|\Zmbf_2^{n_c}\|_2\| \ymbf_{k, n_g}\|_2 \\
    & \quad  + \beta_3^{n_c} L\|\xmbf_{k, n_g} - \xmbf_{k, 1}\|_2 \\
    &= \beta_3^{n_c}\|\ymbf_{k, 1} - \ybb_{k, 1}\|_2 + \beta_4^{n_c} L\|\Zmbf_1^{n_c} - I_{nd}\|_2\|\xmbf_{k, n_g} - \xbb_{k, n_g}\|_2 \\
    & \quad + \alpha \beta_4^{n_c} L \| \ymbf_{k, 1} + \nabla \fmbf(\xmbf_{k, n_g}) - \nabla \fmbf(\xmbf_{k, 1})\|_2 + \beta_3^{n_c} L\|\xmbf_{k, n_g} - \xmbf_{k, 1}\|_2 \\
    &\leq \beta_3^{n_c}\|\ymbf_{k, 1} - \ybb_{k, 1}\|_2 + \beta_4^{n_c} L\|\Zmbf_1^{n_c} - I_{nd}\|_2\|\xmbf_{k, n_g} - \xbb_{k, n_g}\|_2 \\
    & \quad + \alpha \beta_4^{n_c} L \| \ymbf_{k, 1}\|_2 + \alpha \beta_4^{n_c} L^2 \|\xmbf_{k, n_g} - \xmbf_{k, 1}\|_2 + \beta_3^{n_c} L\|\xmbf_{k, n_g} - \xmbf_{k, 1}\|_2
\end{align*}
where the first equality follows from $\xmbf_{k+1, 1} = \Zmbf_1^{n_c} \xmbf_{k, n_g} - \alpha\Zmbf_2^{n_c}\ymbf_{k, n_g}$ and $- (\Zmbf_1^{n_c} - I_{nd})\xbb_{k, n_g} = 0$, the second inequality is by the triangle inequality, the second equality follows by %substitutes $\ymbf_{k, n_g}$ by 
\eqref{eq: v telescope}, and the last inequality is an application of triangle inequality and \cref{asum.convex and smooth}. By \eqref{eq:iterate_deviation}, \eqref{eq:consensus_error_deviation} and $\alpha L \leq \frac{1}{n_g}$, it follows,
\begin{align*}
    &\|\ymbf_{k + 1, 1} - \ybb_{k+1, 1}\|_2 \\
    \leq &\beta_3^{n_c}\|\ymbf_{k, 1} - \ybb_{k, 1}\|_2 + \beta_4^{n_c} L\|\Zmbf_1^{n_c} - I_{nd}\|_2 \|\xmbf_{k, 1} - \xbb_{k, 1}\|_2 \\
    &  + \left( \alpha \beta_4^{n_c} L + 2\alpha (n_g-1) L \left( \beta_4^{n_c} \|\Zmbf_1^{n_c} - I_{nd}\|_2 + \tfrac{\beta_4^{n_c}}{n_g}  + \beta_3^{n_c} \right) \right) \|\ymbf_{k, 1}\|_2.
\end{align*}
Substituting \eqref{eq:y_bound} yields the desired bound for the consensus error in $\ymbf_{k,1}$.
\eproof

\cref{lem:lyapunov g > 1} quantifies the progression of error vector $r_k$ using the matrix $B(n_c, n_g)$, similar to \cref{lem:lyapunov g = 1} but now allowing for multiple computation steps. Notice that when $n_g = 1$, \cref{lem:lyapunov g > 1} reduces to \cref{lem:lyapunov g = 1}, making it a special case of this analysis. We split the matrix $B(n_c, n_g)$ into the matrices $A(n_c, n_g)$ and $E(n_c, n_g)$. The latter matrix is characterized by the terms $\delta_1(n_c, n_g)$ and $\delta_2(n_c, n_g)$. 
We now define the explicit form of $B(n_c, n_g)$ %in terms of the above mentioned components 
for the methods defined in \cref{tab: Algorithm Def}.

\bcorollary \label{col. B special cases}
Suppose the conditions of \cref{lem:lyapunov g > 1} are satisfied. Then, the matrices $A(n_c, n_g)$ for the methods described in \cref{tab: Algorithm Def} are defined as:
\begin{equation} \label{eq : g > 1 algos A}
    \begin{aligned}
        \mbox{\texttt{GTA-1}:} & \quad A_1({n_c, n_g}) = \begin{bmatrix}
        (1 - \alpha \mu)^{n_g}& \frac{\kappa}{\sqrt{n}}(1 - (1 - \alpha\mu)^{n_g}) & 0\\
        0 & \beta^{n_c} & \alpha\left((n_g-1)\beta^{n_c} + 1\right)\\
        \sqrt{n}\alpha L^2 & L(2 + \alpha L) & \beta^{n_c} + \alpha L
    \end{bmatrix},\\
    \mbox{\texttt{GTA-2}:} & \quad A_2({n_c, n_g})  = \begin{bmatrix}
        (1 - \alpha \mu)^{n_g}& \frac{\kappa}{\sqrt{n}}(1 - (1 - \alpha\mu)^{n_g}) & 0\\
        0 & \beta^{n_c} & \alpha\beta^{n_c}n_g\\
        \sqrt{n}\alpha L^2 & L(2 + \alpha L) & \beta^{n_c} + \alpha  L
    \end{bmatrix}, \\
    \mbox{\texttt{GTA-3}:} & \quad A_3({n_c, n_g}) = \begin{bmatrix}
        (1 - \alpha \mu)^{n_g}& \frac{\kappa}{\sqrt{n}}(1 - (1 - \alpha\mu)^{n_g}) & 0\\
        0 & \beta^{n_c} & \alpha\beta^{n_c}n_g\\
        \sqrt{n}\alpha \beta^{n_c} L^2 & \beta^{n_c}L(2 + \alpha L) & \beta^{n_c}(1 + \alpha L)
    \end{bmatrix}.
    \end{aligned}
\end{equation}
The matrix $E(n_c, n_g)$ for the methods described in \cref{tab: Algorithm Def} is defined using the error terms $(\delta_1(n_c, n_g)$ and $\delta_2(n_c, n_g))$. 
The error terms for the methods described in \cref{tab: Algorithm Def} are defined in \cref{tab: n_g > 1 error terms special cases}.
\begin{table}[H]\centering
\caption{Error terms ($\delta_1(n_c, n_g)$ and $\delta_2(n_c, n_g)$) for \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3}}\label{tab: n_g > 1 error terms special cases}
\begin{tabular}{lcc}\toprule
Method & $\delta_1(n_c, n_g)$ & $\delta_2(n_c, n_g)$ \vspace{2pt} \\ \hline \\[-8pt]
\texttt{GTA-1} & $2 + \beta^{n_c}(n_g - 2)$ & $2 \left( 2 + \tfrac{1}{n_g} + \beta^{n_c} \right)$ \vspace{2pt} \\ \hdashline \\[-10pt]
\texttt{GTA-2} & $n_g\beta^{n_c}$ & $2 \left( 2 + \tfrac{1}{n_g} + \beta^{n_c} \right)$ \vspace{2pt} \\ \hdashline \\[-10pt]
\texttt{GTA-3} & $ n_g\beta^{n_c} $ & $2 \beta^{n_c} \left( 3 + \tfrac{1}{n_g} \right)$ \vspace{2pt} \\
\bottomrule
\end{tabular}
\end{table}
\ecorollary
\bproof
Substituting the matrix values for each method in \eqref{eq : g > 1 general A} and bounding $\|\Zmbf_1^{n_c} - I_{nd}\|_2 \leq 2$ gives the desired result.
\eproof

\cref{col. B special cases} presents the explicit form of the matrices $B_i(n_c, n_g) = A_i(n_c, n_g) + \alpha L (n_g - 1) E_i(n_c, n_g)$ for $i = 1, 2, 3$, for each of the methods in \cref{tab: Algorithm Def}. The convergence properties of \texttt{GTA} can be analysed using the spectral radius of $B(n_c, n_g)$. We now qualitatively establish the effect of the number of communication steps $n_c$ on $\rho(B(n_c, n_g))$ and a relative ordering for $\rho(B_1(n_c, n_g))$, $\rho(B_2(n_c, n_g))$ and $\rho(B_3(n_c, n_g))$.

% old version
%By \cref{tab: n_g > 1 error terms special cases}, we define matrices $E_1(n_c, n_g), E_2(n_c, n_g), E_3(n_c, n_g)$ for \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3} respectively. Correspondingly we define $B_i(n_c, n_g) = A_i(n_c, n_g) + \alpha L (n_g - 1) E(n_c, n_g)$ for $i = 1, 2, 3$. The convergence properties of \texttt{GTA} can be analysed using the spectral radius of $B(n_c, n_g)$. We now qualitatively establish the effect of $n_c$ on $\rho(B(n_c, n_g))$ and a relative ordering for $\rho(B_1(n_c, n_g))$, $\rho(B_2(n_c, n_g))$ and $\rho(B_3(n_c, n_g))$.
% \begin{lemma}\label{lem. g>1 spec norm}
% Suppose \cref{asum.convex and smooth} holds, and $\alpha < \frac{1}{n_g L}$. If the matrix $B({n_c, n_g})$ defined in  \cref{lem:lyapunov g > 1} is irreducible then the spectral norm of $B({n_c, n_g})$ is the largest eigen value of $B({n_c, n_g})$ and is a positive real number. Consequently, if $\Wmbf \neq \frac{1_n1_n^T}{n}$, i.e., the network is not fully connected, the spectral norm of matrices $B_1({n_c, n_g})$, $B_2({n_c, n_g})$, $B_3({n_c, n_g})$ defined using \cref{col. B special cases} are also positive real numbers and equal to their largest eigenvalues respectively.
% \end{lemma}

% \begin{proof}
% The statement about general matrix $B({n_c, n_g})$ directly follows from the Perron-Forbenius Theorem (\cite[Theorem 8.4.4]{horn2012matrix}), and observing that the matrix $B({n_c, n_g})$ is nonnegative and irreducible. Consequently, the statement about $B_1({n_c, n_g})$, $B_2({n_c, n_g})$, $B_3({n_c, n_g})$ also follows as $A_1({n_c, n_g})$, $A_2({n_c, n_g})$, $A_3({n_c, n_g})$ matrices are irreducible when $\Wmbf \neq \frac{1_n1_n^T}{n}$, i.e., $\beta > 0$.
% \end{proof}

\btheorem   \label{th.incr rates g > 1}
Suppose \cref{asum.convex and smooth} holds. If $\alpha \leq \frac{1}{L n_g}$ in \cref{alg : Deterministic}, then
as $n_c$ increases, $\rho(B(n_c, n_g))$ decreases where $B(n_c, n_g)$ is defined in \cref{lem:lyapunov g > 1}. Thus, as $n_c$ increases, $\rho(B_i(n_c, n_g))$ decreases for all $i =1, 2, 3$ defined in \cref{col. B special cases}. Moreover, if all three methods defined in \cref{tab: Algorithm Def} (\texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3}) employ the same stepsize,
\begin{align*}
\rho(B_1({n_c, n_g})) \geq \rho(B_2({n_c, n_g})) \geq \rho(B_3({n_c, n_g})).
\end{align*}
\etheorem

\bproof
Note that $A(n_c, n_g) \geq 0$ and $E(n_c, n_g) \geq 0$, thus $B(n_c, n_g) \geq 0$. Also, $A(n_c, n_g) \geq A(n_c + 1, n_g) $, $\delta_1(n_c, n_g) \geq \delta_1(n_c + 1, n_g)$, $\delta_2(n_c, n_g) \geq \delta_2(n_c + 1, n_g)$, thus $E(n_c, n_g) \geq E(n_c + 1, n_g)$ and $B(n_c, n_g) \geq B(n_c + 1, n_g)$.  By \cite[Corollary 8.1.19]{horn2012matrix}, it follows that $\rho(A(n_c, n_g)) \geq \rho(A(n_c + 1, n_g))$, $\rho(E(n_c, n_g)) \geq \rho(E(n_c + 1, n_g))$ and $\rho(B(n_c, n_g)) \geq \rho(B(n_c + 1, n_g))$. The same argument is applicable for $B_1({n_c, n_g})$, $B_2({n_c, n_g})$ and $B_3({n_c, n_g})$. Now, observe that $B_1({n_c, n_g}) \geq B_2({n_c, n_g}) \geq B_3({n_c, n_g}) \geq 0$ when the same step size is employed. Thus, again by \cite[Corollary 8.1.19]{horn2012matrix}, it follows that $\rho(B_1({n_c, n_g})) \geq \rho(B_2({n_c, n_g})) \geq \rho(B_3({n_c, n_g}))$.
\eproof

The effect of the number of computation steps $n_g$ on $\rho(B({n_c, n_g}))$ is not clear as the effect of the number of communication steps $n_c$. Increasing $n_g$ increases all elements of the matrix $\alpha L (n_g - 1)E(n_c, n_g)$, while $(1 - \alpha \mu)^{n_g}$ in the matrix $A(n_c, n_g)$ decreases since $\alpha \leq \frac{1}{Ln_g}$. Thus, the effect of $n_g$ on $\rho(B(n_c, n_g))$ is not monotonic. 

We now derive conditions for establishing a 
linear rate of convergence for \cref{alg : Deterministic} with multiple communication and computation steps every iteration in terms of network parameters ($\beta_1, \beta_2, \beta_3, \beta_4$) and objective function parameters ($L, \mu, \kappa = \frac{L}{\mu}$).

\btheorem  \label{th. alpha bound g > 1}
Suppose \cref{asum.convex and smooth} holds and a finite number of computation steps are performed at each outer iteration of \cref{alg : Deterministic} (i.e., $1 \leq n_g < \infty$). If the matrix $B(n_c, n_g)$ is irreducible, $\beta_1, \beta_3 < 1$ and
\begin{align} \label{eq : alpha mult grads gen}
    \alpha < \min \left\{\tfrac{1}{n_g L}, \tfrac{\mu}{(2L^2 + \mu^2)(n_g - 1)}, \tfrac{1}{2L} \sqrt{\tfrac{3(1 - \beta_1^{n_c})}{\delta_1(n_c, n_g) (n_g - 1)}}, \tfrac{3(1 - \beta_3^{n_c})}{4L(\beta_4^{n_c} + \delta_2(n_c, n_g)(n_g - 1))}, \tfrac{ - b_2 + \sqrt{b_2^2 + 4b_1b_3}}{2b_1}\right\}
 \end{align}
where
\begin{align*}
    % b_1 &= \tfrac{\mu L^2 n_g}{2} \left((n_g-1) \left( \beta_1^{n_c} + \delta_1(n_c, n_g)\right) + \beta_2^{n_c} \right)\left(\beta_4^{n_c} +  (n_g - 1)\delta_2(n_c, n_g)\right) \\
    % &+L^2 (n_g - 1)\delta_1(n_c, n_g) \left( L n_g +  (n_g - 1)\right) \left(\tfrac{1 - \beta_3^{n_c}}{4}\right) \\
    % &+ L^3 (n_g - 1)^2 \delta_1(n_c, n_g)\left(3\beta_4^{n_c} + (n_g - 1)\delta_2(n_c, n_g)\right) \\
    % &+ L^2( \beta_4^{n_c} + (n_g - 1)\delta_2(n_c, n_g)) \left(L n_g + (n_g - 1)\right) \left((n_g-1) \left( \beta_1^{n_c} + \delta_1(n_c, n_g)\right) + \beta_2^{n_c} \right) \\
    % & +L^3 n_g (n_g - 1) (\beta_4^{n_c} + (n_g - 1)\delta_2(n_c, n_g)) \left(\tfrac{1 - \beta_1^{n_c}}{4}\right) \\
    b_1 =& \tfrac{\mu L^2 n_g}{2} \left[(n_g-1) \left( \beta_1^{n_c} + \delta_1(n_c, n_g)\right) + \beta_2^{n_c} \right]\left[\beta_4^{n_c} +  (n_g - 1)\delta_2(n_c, n_g)\right] \\
    &+L^3 n_g (n_g - 1) \left[ \delta_1(n_c, n_g)  \left(\tfrac{1 - \beta_3^{n_c}}{4}\right) + (\beta_4^{n_c} + (n_g - 1)\delta_2(n_c, n_g)) \left(\tfrac{1 - \beta_1^{n_c}}{4}\right) \right]\\
    &+ L^2 (n_g - 1)^2 \left[ L\delta_1(n_c, n_g)\left(3\beta_4^{n_c} + (n_g - 1)\delta_2(n_c, n_g)\right) +  \delta_1(n_c, n_g) \left(\tfrac{1 - \beta_3^{n_c}}{4}\right)\right] \\
    &+ L^2[ \beta_4^{n_c} + (n_g - 1)\delta_2(n_c, n_g)] \left[L n_g + (n_g - 1)\right] \left[(n_g-1) \left( \beta_1^{n_c} + \delta_1(n_c, n_g)\right) + \beta_2^{n_c} \right] \\
    % b_1 =& \sg{\tfrac{\mu L^2 n_g}{2} \left[(n_g-1) \left( \beta_1^{n_c} + \delta_1(n_c, n_g)\right) + \beta_2^{n_c} \right]\left[\beta_4^{n_c} +  (n_g - 1)\delta_2(n_c, n_g)\right]} \\
    % &\sg{+L^3 n_g (n_g - 1) \left[ \delta_1(n_c, n_g)  \left(\tfrac{1 - \beta_3^{n_c}}{4}\right) + \beta_4^{n_c}  \left(\tfrac{1 - \beta_1^{n_c}}{4}\right) \right] }\\
    % &\sg{+ L^2 (n_g - 1)^2 \left[ L\delta_1(n_c, n_g)\left(3\beta_4^{n_c} + (n_g - 1)\delta_2(n_c, n_g)\right) +  \delta_1(n_c, n_g) \left(\tfrac{1 - \beta_3^{n_c}}{4}\right)  +L n_g \delta_2(n_c, n_g) \left(\tfrac{1 - \beta_1^{n_c}}{4}\right)\right]} \\
    % &\sg{+ L^2[ \beta_4^{n_c} + (n_g - 1)\delta_2(n_c, n_g)] \left[L n_g + (n_g - 1)\right] \left[(n_g-1) \left( \beta_1^{n_c} + \delta_1(n_c, n_g)\right) + \beta_2^{n_c} \right]} \\
    % & +L^3 n_g (n_g+L^3 n_g (n_g - 1) (\beta_4^{n_c} + (n_g - 1)\delta_2(n_c, n_g)) \left(\tfrac{1 - \beta_1^{n_c}}{4}\right) - 1) (\beta_4^{n_c} + (n_g - 1)\delta_2(n_c, n_g)) \left(\tfrac{1 - \beta_1^{n_c}}{4}\right) \\
    b_2  = & \mu n_g \beta_4^{n_c}L \left((n_g-1)(\beta_1^{n_c} + \delta_1(n_c, n_g)) + \beta_2^{n_c} \right), \text{ and }\; b_3 = \tfrac{\mu n_g}{2} \left( \tfrac{1 - \beta_1^{n_c}}{4}\right) \left(\tfrac{1 - \beta_3^{n_c}}{4}\right)
    % b_2 & =  \mu n_g \beta_4^{n_c}L \left((n_g-1)\beta_1^{n_c} + \beta_2^{n_c} + (n_g - 1) \delta_1(n_c, n_g)\right) \\
    % b_3 & = \tfrac{\mu n_g}{2} \left( \tfrac{1 - \beta_1^{n_c}}{4}\right) \left(\tfrac{1 - \beta_3^{n_c}}{4}\right)
\end{align*}
and $\delta_1(n_c, n_g)$ and $\delta_2(n_c, n_g)$ are defined in \eqref{eq:delta_errors}, 
% then there exists a sequence $\epsilon_k\geq 0$ such that,
% \begin{align*}
%     \|r_{k}\|_2 \leq (\rho(B(n_c, n_g)) + \epsilon_k)^k \|r_0\|_2 \quad \mbox{and} \quad \lim_{k \rightarrow \infty} \epsilon_k = 0
% \end{align*}
% for all $k \geq 0$, where $\rho(B({n_c, n_g})) < 1$.
then, for all $\epsilon > 0$ there exists a constant $C_{\epsilon} > 0$ such that, for all $k\geq 0$,
\begin{align*}
    \|r_{k}\|_2 \leq C_{\epsilon}(\rho(B({n_c, n_g})) + \epsilon)^k \|r_0\|_2, \quad \text{where } \; \rho(B({n_c, n_g})) < 1.
\end{align*} 
\etheorem
\bproof
By the binomial expansion of $(1-\alpha\mu)^{n_g}$ and the condition that $\alpha \leq \tfrac{1}{L n_g}$, it follows that $1 - \alpha \mu n_g \leq (1 - \alpha \mu)^{n_g} \leq 1 - \alpha \mu n_g + \alpha^2 \mu^2 \tfrac{n_g (n_g-1)}{2} $. Following a similar approach to \cite[Theorem 2]{pu2020push}, since the step size satisfies \eqref{eq : alpha mult grads gen}, the first, second and third diagonal terms of $B(n_c, n_g)$
% \sg{Upon taylor expanding $(1-\alpha\mu)^{n_g}$, under $\alpha \leq \tfrac{1}{L n_g}$, the series is decreasing in magnitude with alternating signs. Thus, ignoring higher-order terms we can bound $1 - \alpha \mu n_g \leq (1 - \alpha \mu)^{n_g} \leq 1 - \alpha \mu n_g + \alpha^2 \mu^2 \tfrac{n_g (n_g-1)}{2} $. Following a similar approach to \cite[Theorem 2]{pu2020push}, since the step size satisfies \eqref{eq : alpha mult grads gen}, the first}, second and third diagonal terms of $B(n_c, n_g)$
% Following an approach similar to \cite{pu2020push}, as $\alpha$ satisfies the inequalities
% \begin{align*}
%     \alpha < \tfrac{\mu}{2L^2(n_g - 1)}, \quad \alpha < \tfrac{1}{2L} \sqrt{\frac{3(1 - \beta_1^{n_c})}{\delta_1(n_c, n_g) (n_g - 1)}}, \quad \alpha <  \tfrac{3(1 - \beta_3^{n_c})}{4L(\beta_4^{n_c} + \delta_2(n_c, n_g)(n_g - 1))},
% \end{align*}
% the first, second and third diagonal terms of $B(n_c, n_g)$ 
can be upper bounded as
\begin{align*}
    % (1 - \alpha \mu)^{n_g} + \alpha^2 L^2 n_g (n_g - 1)  \asb{\approx} 1 - \alpha \mu n_g + \alpha^2 L^2 n_g (n_g - 1)& < 1 - \tfrac{\alpha \mu n_g}{2}, \\ 
    (1 - \alpha \mu)^{n_g} + \alpha^2 L^2 n_g (n_g - 1)  \leq 1 - \alpha \mu n_g + \alpha^2 (L^2 + \tfrac{\mu^2}{2}) n_g (n_g - 1)& < 1 - \tfrac{\alpha \mu n_g}{2}, \\ 
    \beta_1^{n_c} + \alpha^2 L^2 (n_g - 1) \delta_1(n_c, n_g) & < \tfrac{3 + \beta_1^{n_c}}{4}, \\
    \beta_3^{n_c} + \alpha \beta_4^{n_c} L + \alpha L (n_g - 1) \delta_2(n_c, n_g) & < \tfrac{3 + \beta_3^{n_c}}{4}.
\end{align*}
With the above bounds, $(1-\alpha \mu)^{n_g} \geq 1 - \alpha\mu n_g$ and $\|\Zmbf_1^{n_c} - I_{nd}\| \leq 2$, we construct the $3\times 3$ matrix $\Tilde{B}(n_c, n_g)$ that has entries $\tilde{b}_{ij}$ defined as follows:
% $B(n_c, n_g) = A(n_c, n_g) + \alpha L (n_g - 1) E(n_c, n_g)$
% \begin{align*}
% A(n_c, n_g) &= \begin{bmatrix}
%     (1 - \alpha \mu)^{n_g} + & \frac{\kappa}{\sqrt{n}}(1 - (1 - \alpha\mu)^{n_g}) & 0\\
%     0 & \beta_1^{n_c} & \alpha\left((n_g-1)\beta_1^{n_c} + \beta_2^{n_c}\right)\\
%     \sqrt{n}\alpha \beta_4^{n_c} L^2 & \beta_4^{n_c}L(\|\Zmbf_1^{n_c}-I_{nd}\|_2 + \alpha L) & \beta_3^{n_c} + \alpha \beta_4^{n_c} L
%     \end{bmatrix},\\
% E(n_c, n_g) &= \begin{bmatrix}
%     \alpha L \gamma(n_g)& \frac{\alpha L}{\sqrt{n}} \gamma(n_g) & \frac{\alpha}{\sqrt{n}}\gamma(n_g)\\
%     \sqrt{n}\alpha L \phi(n_c, n_g) & \alpha L \phi(n_c, n_g) & \alpha \phi(n_c, n_g)\\
%     \sqrt{n} L\delta(n_c, n_g) & L \delta(n_c, n_g)& \delta(n_c, n_g)
%     \end{bmatrix}
% \end{align*}
% and
% \begin{align*}
%     &\gamma(n_g) =  n_g, \\
%     &\phi(n_c, n_g) = 2\beta_2^{n_c} + \beta_1^{n_c}(n_g - 2) ,\\
%     &\delta(n_c, n_g) = 2 \left[ \beta_4^{n_c} \|\Zmbf_1^{n_c} - I_{nd}\|_2 + \frac{\beta_4^{n_c}}{n_g} + \beta_3^{n_c} \right].
% \end{align*}
% \begin{align*}
% \Tilde{B}(n_c, n_g) = 
% \begin{bmatrix}
%     \Tilde{b}_{11} & \Tilde{b}_{12} & \Tilde{b}_{13}\\
%     \Tilde{b}_{21} & \Tilde{b}_{22} & \Tilde{b}_{23}\\
%     \Tilde{b}_{31} & \Tilde{b}_{32} & \Tilde{b}_{33}
%     \end{bmatrix}
% \end{align*}
% where 
% \begin{align*}
% &\Tilde{B}(n_c, n_g) = \\
% &\begin{bmatrix}
%     (1 - \frac{\alpha \mu n_g}{2})& \frac{\alpha L n_g}{\sqrt{n}} + \frac{\alpha^2 L^2 n_g(n_g - 1)}{\sqrt{n}} & \frac{\alpha^2 L n_g (n_g - 1)}{\sqrt{n}}\\
%     \sqrt{n}\alpha^2 L^2 (n_g - 1) \delta_1(n_c, n_g) & \frac{3 + \beta_1^{n_c}}{4} & \alpha\left((n_g-1)\beta_1^{n_c} + \beta_2^{n_c}\right) + \alpha^2 L (n_g - 1) \delta_1(n_c, n_g)\\
%     \sqrt{n}\alpha \beta_4^{n_c} L^2 + \sqrt{n}\alpha L^2 (n_g - 1)\delta_2(n_c, n_g) & \beta_4^{n_c}L(2 + \alpha L) + \alpha L^2 (n_g - 1) \delta_2(n_c, n_g) & \frac{3 + \beta_3^{n_c}}{4}
%     \end{bmatrix}
% \end{align*}
% \begin{align*}
%     &\Tilde{b}_{11} = 1 - \tfrac{\alpha \mu n_g}{2}, \quad \Tilde{b}_{12} = \tfrac{\alpha L n_g}{\sqrt{n}} + \tfrac{\alpha^2 L^2 n_g(n_g - 1)}{\sqrt{n}},\quad \Tilde{b}_{13} = \tfrac{\alpha^2 L n_g (n_g - 1)}{\sqrt{n}}\\
%     &\Tilde{b}_{21} = \sqrt{n}\alpha^2 L^2 (n_g - 1) \delta_1(n_c, n_g), \quad \Tilde{b}_{22} = \tfrac{3 + \beta_1^{n_c}}{4},\\
%     & \Tilde{b}_{23} = \alpha\left((n_g-1)\beta_1^{n_c} + \beta_2^{n_c}\right) + \alpha^2 L (n_g - 1) \delta_1(n_c, n_g),\\
%     &\Tilde{b}_{31} = \sqrt{n}\alpha \beta_4^{n_c} L^2 + \sqrt{n}\alpha L^2 (n_g - 1)\delta_2(n_c, n_g), \\
%     &\Tilde{b}_{32} = \beta_4^{n_c}L(2 + \alpha L) + \alpha L^2 (n_g - 1) \delta_2(n_c, n_g), \quad \Tilde{b}_{33} = \tfrac{3 + \beta_3^{n_c}}{4},
% \end{align*}
\begin{align*}
    &\Tilde{b}_{11} = 1 - \tfrac{\alpha \mu n_g}{2}, \quad \Tilde{b}_{12} = \tfrac{\alpha L n_g}{\sqrt{n}} \left(1 + \alpha L (n_g - 1)\right),\quad \Tilde{b}_{13} = \tfrac{\alpha^2 L n_g (n_g - 1)}{\sqrt{n}}\\
    &\Tilde{b}_{21} = \sqrt{n}\alpha^2 L^2 (n_g - 1) \delta_1(n_c, n_g), \quad \Tilde{b}_{22} = \tfrac{3 + \beta_1^{n_c}}{4},\\
    & \Tilde{b}_{23} = \alpha\left((n_g-1) (\beta_1^{n_c}  + \alpha L \delta_1(n_c, n_g)) + \beta_2^{n_c}\right), \\
    &\Tilde{b}_{31} = \sqrt{n}\alpha L^2 \left(\beta_4^{n_c} + (n_g - 1)\delta_2(n_c, n_g)\right), \\
    &\Tilde{b}_{32} = \beta_4^{n_c}L(2 + \alpha L) + \alpha L^2 (n_g - 1) \delta_2(n_c, n_g), \quad \Tilde{b}_{33} = \tfrac{3 + \beta_3^{n_c}}{4},
\end{align*}
such that $0 \leq B(n_c, n_g) \leq \Tilde{B}(n_c, n_g)$ and by \cite[Corollary 8.1.19]{horn2012matrix}, $\rho(B(n_c, n_g)) \leq \rho(\Tilde{B}(n_c, n_g))$. Following \cite[Lemma 5]{pu2021distributed} derived from the Perron-Forbenius Theorem \cite[Theorem 8.4.4]{horn2012matrix} for a $3\times3$ matrix, when the matrix $\Tilde{B}(n_c, n_g)$ is nonnegative and irreducible, it is sufficient to show that the diagonal elements of $\Tilde{B}(n_c, n_g)$ are less than one and $\det(I_3 - \Tilde{B}(n_c, n_g)) > 0$ in order to guarantee $\rho(\Tilde{B}(n_c, n_g)) < 1$ which suffices to show $\rho(B(n_c, n_g)) < 1$. 

% \pagebreak
Consider the diagonal elements of the matrix $\Tilde{B}(n_c, n_g)$. The first element is $1 - \frac{\alpha \mu n_g}{2} \leq 1 - \frac{\mu}{2L} < 1$ by \eqref{eq : alpha mult grads gen}. The second element is $\frac{3 + \beta_1^{n_c}}{4} < 1$ as $\beta_1 < 1$. Finally the third element is $\frac{3 + \beta_3^{n_c}}{4} < 1$ as $\beta_3 < 1$. Next, let us consider,
\begin{align*}
    &\det(I_3 - \Tilde{B}(n_c, n_g)) \\
    =& \tfrac{\alpha \mu n_g}{2} \left( \tfrac{1 - \beta_1^{n_c}}{4}\right) \left(\tfrac{1 - \beta_3^{n_c}}{4}\right) - \alpha^3 L^3 n_g (n_g - 1)\left[\beta_4^{n_c}  \left(\tfrac{1 - \beta_1^{n_c}}{4}\right) + \delta_1(n_c, n_g)  \left(\tfrac{1 - \beta_3^{n_c}}{4}\right)\right]\\
    &- \tfrac{\alpha^2 \mu L n_g}{2} \left[(n_g-1)(\beta_1^{n_c}+ \alpha L \delta_1(n_c, n_g)) + \beta_2^{n_c} \right]\left[\beta_4^{n_c}(2 + \alpha L) + \alpha L (n_g - 1)\delta_2(n_c, n_g)\right] \\
    &-\alpha^4 L^4 n_g (n_g - 1)^2\delta_1(n_c, n_g) \left[2\beta_4^{n_c} + \alpha L \left( \beta_4^{n_c} + (n_g - 1)\delta_2(n_c, n_g)\right) \right] \\
    & - \alpha^3 L^3 n_g\left(1 + \alpha (n_g - 1)\right)\left[ \beta_4^{n_c}  +(n_g - 1)\delta_2(n_c, n_g)\right] \left[(n_g-1) (\beta_1^{n_c} + \alpha L \delta_1(n_c, n_g)) + \beta_2^{n_c} \right] \\
    & - \alpha^3 L^3 n_g (n_g - 1)^2\left[ \delta_2(n_c, n_g)  \left(\tfrac{1 - \beta_1^{n_c}}{4}\right) + \alpha \delta_1(n_c, n_g)  \left(\tfrac{1 - \beta_3^{n_c}}{4}\right)\right]\\  
    % &= \tfrac{\alpha \mu n_g}{2} \left( \tfrac{1 - \beta_1^{n_c}}{4}\right) \left(\tfrac{1 - \beta_3^{n_c}}{4}\right) \\
    % &- \tfrac{\alpha \mu n_g}{2} \left(\alpha\left((n_g-1)\beta_1^{n_c} + \beta_2^{n_c}\right) + \alpha^2 L (n_g - 1) \delta_1(n_c, n_g)\right)\left(\beta_4^{n_c}L(2 + \alpha L) + \alpha L^2 (n_g - 1)\delta_2(n_c, n_g)\right) \\
    % &-\alpha^2 L^2 (n_g - 1)\delta_1(n_c, n_g) \left(\alpha L n_g + \alpha^2 L n_g (n_g - 1)\right) \left(\tfrac{1 - \beta_3^{n_c}}{4}\right) \\
    % &- \alpha^2 L^2 (n_g - 1)\delta_1(n_c, n_g)   \left(\alpha^2 L n_g(n_g - 1)\right)\left(\beta_4^{n_c}L(2 + \alpha L) + \alpha L^2 (n_g - 1)\delta_2(n_c, n_g)\right) \\
    % & - (\alpha \beta_4^{n_c} L^2 +\alpha L^2(n_g - 1)\delta_2(n_c, n_g)) \left(\alpha L n_g + \alpha^2 L n_g (n_g - 1)\right) \left(\alpha\left((n_g-1)\beta_1^{n_c} + \beta_2^{n_c}\right) + \alpha^2 L (n_g - 1) \delta_1(n_c, n_g)\right) \\
    % & - (\alpha \beta_4^{n_c} L^2 + \alpha L^2(n_g - 1)\delta_2(n_c, n_g))  \left(\alpha^2 L n_g (n_g - 1) \right)\left(\tfrac{1 - \beta_1^{n_c}}{4}\right)\\  
    \geq& \alpha(- b_1 \alpha^2 - b_2 \alpha + b_3) = -b_1 \alpha(\alpha - \alpha_l)(\alpha - \alpha_u)
\end{align*}
% \begin{align*}
%     &\det(I_3 - \Tilde{B}(n_c, n_g))\\
%     &= \tfrac{\alpha \mu n_g}{2} \left(1 - \tfrac{3 + \beta_1^{n_c}}{4}\right) \left(1 - \tfrac{3 + \beta_3^{n_c}}{4}\right) \\
%     &- \tfrac{\alpha \mu n_g}{2} \left(\alpha\left((n_g-1)\beta_1^{n_c} + \beta_2^{n_c}\right) + \alpha^2 L (n_g - 1) \delta_1(n_c, n_g)\right)\left(\beta_4^{n_c}L(2 + \alpha L) + \alpha L^2 (n_g - 1)\delta_2(n_c, n_g)\right) \\
%     &- \sqrt{n}\alpha^2 L^2 (n_g - 1)\delta_1(n_c, n_g) \left(\tfrac{\alpha L n_g}{\sqrt{n}} + \tfrac{\alpha^2 L n_g (n_g - 1)}{\sqrt{n}}\right) \left(1 - \tfrac{3 + \beta_3^{n_c}}{4}\right) \\
%     &- \sqrt{n}\alpha^2 L^2 (n_g - 1)\delta_1(n_c, n_g)   \left(\tfrac{\alpha^2 L n_g(n_g - 1)}{\sqrt{n}}\right)\left(\beta_4^{n_c}L(2 + \alpha L) + \alpha L^2 (n_g - 1)\delta_2(n_c, n_g)\right) \\
%     & - (\sqrt{n}\alpha \beta_4^{n_c} L^2 +\sqrt{n}\alpha L^2(n_g - 1)\delta_2(n_c, n_g)) \left(\tfrac{\alpha L n_g}{\sqrt{n}} + \tfrac{\alpha^2 L n_g (n_g - 1)}{\sqrt{n}}\right) \left(\alpha\left((n_g-1)\beta_1^{n_c} + \beta_2^{n_c}\right) + \alpha^2 L (n_g - 1) \delta_1(n_c, n_g)\right) \\
%     & - (\sqrt{n}\alpha \beta_4^{n_c} L^2 +\sqrt{n}\alpha L^2(n_g - 1)\delta_2(n_c, n_g))  \left(\tfrac{\alpha^2 L n_g (n_g - 1)}{\sqrt{n}}\right)\left(1 - \tfrac{3 + \beta_1^{n_c}}{4}\right) \\
%     \\ 
%     &= \tfrac{\alpha \mu n_g}{2} \left( \tfrac{1 - \beta_1^{n_c}}{4}\right) \left(\tfrac{1 - \beta_3^{n_c}}{4}\right) \\
%     &- \tfrac{\alpha \mu n_g}{2} \left(\alpha\left((n_g-1)\beta_1^{n_c} + \beta_2^{n_c}\right) + \alpha^2 L (n_g - 1) \delta_1(n_c, n_g)\right)\left(\beta_4^{n_c}L(2 + \alpha L) + \alpha L^2 (n_g - 1)\delta_2(n_c, n_g)\right) \\
%     &-\alpha^2 L^2 (n_g - 1)\delta_1(n_c, n_g) \left(\alpha L n_g + \alpha^2 L n_g (n_g - 1)\right) \left(\tfrac{1 - \beta_3^{n_c}}{4}\right) \\
%     &- \alpha^2 L^2 (n_g - 1)\delta_1(n_c, n_g)   \left(\alpha^2 L n_g(n_g - 1)\right)\left(\beta_4^{n_c}L(2 + \alpha L) + \alpha L^2 (n_g - 1)\delta_2(n_c, n_g)\right) \\
%     & - (\alpha \beta_4^{n_c} L^2 +\alpha L^2(n_g - 1)\delta_2(n_c, n_g)) \left(\alpha L n_g + \alpha^2 L n_g (n_g - 1)\right) \left(\alpha\left((n_g-1)\beta_1^{n_c} + \beta_2^{n_c}\right) + \alpha^2 L (n_g - 1) \delta_1(n_c, n_g)\right) \\
%     & - (\alpha \beta_4^{n_c} L^2 + \alpha L^2(n_g - 1)\delta_2(n_c, n_g))  \left(\alpha^2 L n_g (n_g - 1) \right)\left(\tfrac{1 - \beta_1^{n_c}}{4}\right)\\  
%     \\ 
%     &\geq \tfrac{\alpha \mu n_g}{2} \left( \tfrac{1 - \beta_1^{n_c}}{4}\right) \left(\tfrac{1 - \beta_3^{n_c}}{4}\right) \\
%     &- \tfrac{\alpha^2 \mu n_g}{2} \left((n_g-1)\beta_1^{n_c} + \beta_2^{n_c} + (n_g - 1) \delta_1(n_c, n_g)\right)\left(\beta_4^{n_c}L(2 + \alpha L) + \alpha L^2 (n_g - 1)\delta_2(n_c, n_g)\right) \\
%     &-\alpha^2 L^2 (n_g - 1)\delta_1(n_c, n_g) \left(\alpha L n_g + \alpha (n_g - 1)\right) \left(\tfrac{1 - \beta_3^{n_c}}{4}\right) \\
%     &- \alpha^2 L^2 (n_g - 1)\delta_1(n_c, n_g)   \left(\alpha(n_g - 1)\right)\left(3\beta_4^{n_c}L + L (n_g - 1)\delta_2(n_c, n_g)\right) \\
%     & - \alpha^2L^2( \beta_4^{n_c} + (n_g - 1)\delta_2(n_c, n_g)) \left(\alpha L n_g + \alpha (n_g - 1)\right) \left((n_g-1)\beta_1^{n_c} + \beta_2^{n_c} +  (n_g - 1) \delta_1(n_c, n_g)\right) \\
%     & -\alpha L^2 (\beta_4^{n_c} + (n_g - 1)\delta_2(n_c, n_g))  \left(\alpha^2 L n_g (n_g - 1) \right)\left(\tfrac{1 - \beta_1^{n_c}}{4}\right) \\
%     \\
%     &= \alpha(- b_1 \alpha^2 - b_2 \alpha + b_3) = - \alpha(\alpha - \alpha_l)(\alpha - \alpha_u)
% \end{align*}
where the inequality is due to $\alpha L n_g \leq 1$ and thus $\alpha L \leq 1$ as $n_g \geq 1$, and 
\begin{align*}
\alpha_l = \tfrac{-b_2 - \sqrt{b_2^2 + 4b_1 b_3}}{2b_1} \quad \text{and} \quad 
\alpha_u = \tfrac{-b_2 + \sqrt{b_2^2 + 4b_1 b_3}}{2b_1}. 
\end{align*}
Observe that $\alpha_l < 0 < \alpha_u$ since $b_1, b_2, b_3 \geq 0$. From \eqref{eq : alpha mult grads gen}, we have $0<\alpha < \alpha_u$. Therefore, $\det(I_3 - \Tilde{B}({n_c, n_g})) > 0$, which combined with the fact that the diagonal elements of the matrix are less than 1, implies $\rho(B(n_c, n_g)) \leq \rho(\Tilde{B}(n_c, n_g)) < 1$.

% Now, we observe that,
% \begin{align*}
%     \|r_{k}\|_2 &\leq \|B({n_c, n_g})^k\|_2\|r_0\|_2,
% \end{align*}
% and using \cite[Theorem 5.6.12]{horn2012matrix}, we can bound the matrix norm with a spectral radius using some sequence $\epsilon_k \geq 0$ with $\lim_{k \rightarrow \infty} \epsilon_k = 0$.
Finally, we bound the norm of error vector $\|r_k\|_2$ by telescoping $r_{i+1} \leq B(n_c, n_g) r_{i}$ from $i = 0$ to $k-1$ and triangle inequality as
\begin{align*}
    \|r_{k}\|_2 &\leq \|B(n_c, n_g)^k\|_2\|r_0\|_2.
\end{align*}
From \cite[Corollary 5.6.13]{horn2012matrix}, we can bound $\|B(n_c, n_g)^k\|_2 \leq C_{\epsilon}(\rho(B(n_c, n_g)) + \epsilon)^k$ where $\epsilon > 0$ and $C_{\epsilon}$ is a positive constant depending on $B(n_c, n_g)$ and $\epsilon$.
\eproof

Similar to \cref{th. general g=1 rate bound}, the only constraint \cref{th. alpha bound g > 1} imposes on the system is $\beta_1, \beta_3 < 1$. This implies the communication matrices $\Wmbf_1$ and $\Wmbf_3$ must represent connected networks (not necessarily the same network) even when multiple communication and multiple computation steps are performed. \cref{th. alpha bound g > 1} does not impose any restrictions on the relation among $\Wmbf_1$, $\Wmbf_2$, $\Wmbf_3$ and $\Wmbf_4$. Thus, it allows for more flexibility than the structures considered in the literature even when multiple communication and multiple computation steps are performed. %Variables can be communicated along different connections within the network while the agents perform multiple computation steps using only local information. 
% One drawback of gradient tracking methods is the requirement of communication of 2 vectors instead of one requiring higher communication bandwidth. This limitation can be overcome using our general structure by limiting communication of variables on certain edges as long as \cref{th. alpha bound g > 1} conditions are met. 
\cref{th. alpha bound g > 1} uses a relaxation of the original matrix $B(n_c, n_g)$ to provide a more pessimistic step size condition than required. But observe, when $n_g = 1$, \eqref{eq : alpha mult grads gen} recovers the $\mathcal{O}(L^{-1}\kappa^{-0.5})$ step size condition of \cref{th. general g=1 step cond}, suggesting it might not be very pessimistic.

Based on \cref{th. alpha bound g > 1}, the step size conditions for methods described in \cref{tab: Algorithm Def} can be derived. We omit these conditions as they are complex and do not offer any additional insights. %These conditions have been omitted as they are highly complex and do not offer any \asb{additional} insights. 
We also omit the counterpart to \cref{th. general g=1 rate bound} as the matrix $B(n_c, n_g)$ is now a dense matrix, thus any such bounds are again highly complex and do not offer strong insights into the effects of communication and computation on the convergence rate. If $B(n_c, n_g)$ is a reducible matrix, the analysis for the progression of $r_k$ can be further simplified from \cref{lem:lyapunov g > 1}. The analysis for this case is presented in \cref{sec.full graph res} with the example of \texttt{GTA-2} and \texttt{GTA-3} when $\Wmbf = \frac{1_n1_n^T}{n}$, i.e., $\beta= 0$. 

