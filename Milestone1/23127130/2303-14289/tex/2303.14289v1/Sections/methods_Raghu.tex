\section{Gradient Tracking Algorithmic Framework}\label{sec.methods}
\rb{In this section, we first describe our framework to unify gradient tracking algorithms and to incorporate flexibility in the number of communication steps. We then develop an algorithm that incorporates flexibility in the number of computation steps based on this framework.
The iterate update form of gradient tracking methods with multiple communication steps is given by  
\begin{equation}\label{eq : general form}
\begin{aligned}
    \xmbf_{k+1} & = \Zmbf_1^{n_c} \xmbf_k - \alpha \Zmbf_2^{n_c} \ymbf_k \\ 
    \ymbf_{k+1} & = \Zmbf_3^{n_c} \ymbf_k + \Zmbf_4^{n_c} (\nabla \fmbf(\xmbf_{k+1}) - \nabla \fmbf(\xmbf_{k})), 
\end{aligned}
\end{equation}
where $\Wmbf_1$, $\Wmbf_2$, $\Wmbf_3$ and $\Wmbf_4$ are communication matrices, $\Zmbf_i = \Wmbf_i \otimes I_d, \,\, \forall \,\, i = 1, 2, 3, 4$, and $n_c$ denote number of communication steps. A communication matrix, $\Umbf \in \mathbb{R}^{n \times n}$, is a symmetric, doubly stochastic matrix that respects 
the connectivity of the network, i.e. $u_{ii} > 0$ and $u_{ij} = 0$ if $(i,j) \notin \mathcal{E}$. Thus, \rb{a communication matrix} represents a network topology consisting of all nodes and a subset of the edges \rb{(or all edges)} present in the network. %(possibly all edges). 
Moreover, performing multiple communication steps ($n_c$) improves the consensus of the communicated vector across the network and represented in a compact form by taking power $n_c$ of $\Zmbf_i \,\, \forall \,\, i = 1, 2, 3, 4$ as in \cite{berahas2018balancing}.   
%Also, performing multiple communication steps ($n_c$), which improves consensus across the network, is compactly represented by taking power $n_c$ of $\Zmbf_i \,\, \forall \,\, i = 1, 2, 3, 4$ as in \cite{berahas2018balancing}.
%Performing multiple communication steps can improve the consensus across the network and 


The four communication matrices, $\Wmbf_1$, $\Wmbf_2$, $\Wmbf_3$ and $\Wmbf_4$, represent four (possibly different) network topologies over which the corresponding vectors are communicated. Due to these communication matrices, the iterate update form given in \eqref{eq : general form} can represent all ATC and CTA communication strategies based algorithms.% seen in literature using \eqref{eq : general form}. 
Some of these algorithms have been shown in \cref{tab: Algorithm Def} where $\Wmbf$ is the mixing matrix for the connected network.

%This allows one to represent all ATC and CTA based communication strategies seen in literature using \eqref{eq : general form}. Some of these cases have been shown in \cref{tab: Algorithm Def} where $\Wmbf$ is the mixing matrix for the connected network. The iterate update form \eqref{eq : general form} also represents \emph{heterogeneous} communication strategies, where different pieces of information are shared across a different subset of network edges. Such strategies are useful in several decentralized settings such as networks with bandwidth limitations where the amount of data transfer on each edge is limited XXX. Such network settings can be represented using appropriate choice of communication matrices used in \eqref{eq : general form}. In \cref{sec.theory} we establish conditions on the communication matrices to ensure linear convergence of the iterates generated by GTA framework. Specifically,} ensuring $\Wmbf_1$ and $\Wmbf_3$ to represent a connected (possibly different) network is sufficient to guarantee linear convergence to the solution.

The iterate update form \eqref{eq : general form} also represents \emph{heterogeneous} communication strategies, where different pieces of information are shared across a different subset of network edges. These communication strategies are useful in several decentralized settings such as networks with bandwidth limitations where the amount of data transfer on each edge is limited XXX. Such network settings can be represented using appropriate choice of communication matrices used in \eqref{eq : general form}. In \cref{sec.theory} we establish conditions on the communication matrices to ensure linear convergence of the iterates generated by GTA framework. Specifically, ensuring $\Wmbf_1$ and $\Wmbf_3$ to represent a connected (possibly different) network is sufficient to guarantee linear convergence to the solution.

We can extend \eqref{eq : general form} to incorporate multiple computation steps. We present \texttt{GTA} (\cref{alg : Deterministic}), a deterministic algorithm that performs $n_c \geq 1$ communication steps and $n_g \geq 1$ computation steps each iteration. If $\ymbf_k$ is a good estimate of the average gradient across the network, performing multiple computation steps using only local information will drive the local iterate closer to the global optimal solution without distorting the consensus error too much.
A balance between the number of communication and computation steps is required to achieve overall efficiency and \cref{alg : Deterministic} allows for such flexibility in these steps using the parameters $n_g$ and $n_c$.

\bremark 
We make the following remarks about \cref{alg : Deterministic}. 

\begin{itemize}
    \item \textbf{Inner and Outer Loops} - Lines 2--8 form the outer loop. The algorithm performs $n_c$ communication steps each outer iteration in lines 7--8. These multiple communications are theoretically represented by taking powers of the communication matrices. The algorithm performs $n_g$ local gradient computations each outer iteration with one computation in line 8 $(\nabla \fmbf(\xmbf_{k+1, 1}))$ and $n_g - 1$ computations in the inner loop (lines 4-6, $(\nabla \fmbf(\xmbf_{k, j+1}))$). The inner loop is \rb{only} executed if more than one computation is to be performed every outer iteration (line 3).
    
    \item \textbf{Step size} ($\alpha$) - The algorithm employs a constant step size \rb{that depends on the choice of $n_g$ and $n_c$}. Increase in communication steps per iteration ($n_c$) allows employing a larger stepsize. Increase in the computation steps per iteration ($n_g$) requires the stepsize to be reduced at a polynomial rate.
    
    \item \textbf{Communication Strategy} - The communication matrices employed in lines 7--8 allow for various communication strategies to be employed. If $\Wmbf_1$ and $\Wmbf_2$ represent connected networks, the algorithm can show linear rate of convergence for any composition of communication and computation steps.
\end{itemize} 
\eremark
}



\section{Gradient Tracking Algorithmic Framework}\label{sec.methods}
\rb{In this section, we first describe our framework to unify gradient tracking algorithms. We then develop an algorithm that incorporates flexibility in the number of communication and computation steps based on this framework. }
%\rb{In this section, we describe our framework to unify gradient tracking algorithms and to incorporate flexibility in the number of communication and computation steps.} 
%In this section, we first describe our framework to unify gradient tracking algorithms. We then describe our algorithm for flexibility in number of communication and computation steps. 
\rb{The iterate update form given in \eqref{eq : general form} unifies different communication strategies used in gradient tracking algorithms,} 
%The iterate form we use to unify different communication strategies in gradient tracking algorithms is given in \eqref{eq : general form}, 
\begin{equation}\label{eq : general form}
\begin{aligned}
    \xmbf_{k+1} & = \Zmbf_1 \xmbf_k - \alpha \Zmbf_2 \ymbf_k \\ 
    \ymbf_{k+1} & = \Zmbf_3 \ymbf_k + \Zmbf_4 (\nabla \fmbf(\xmbf_{k+1}) - \nabla \fmbf(\xmbf_{k})), 
\end{aligned}
\end{equation}
% \begin{align}\label{eq : general form}
%     \xmbf_{k+1} & = \Zmbf_1 \xmbf_k - \alpha \Zmbf_2 \ymbf_k \\ 
%     \ymbf_{k+1} & = \Zmbf_3 \ymbf_k + \Zmbf_4 (\nabla \fmbf(\xmbf_{k+1}) - \nabla \fmbf(\xmbf_{k})), \nonumber
% \end{align}
where $\Wmbf_1$, $\Wmbf_2$, $\Wmbf_3$ and $\Wmbf_4$ are communication matrices, and $\Zmbf_i = \Wmbf_i \otimes I_d, \,\, \forall \,\, i = 1, 2, 3, 4$. 
A communication matrix, $\Umbf \in \mathbb{R}^{n \times n}$, is a symmetric, doubly stochastic matrix that respects 
the connectivity of the network, i.e. $u_{ii} > 0$ and $u_{ij} = 0$ if $(i,j) \notin \mathcal{E}$. Thus, \rb{a communication matrix} represents a network topology consisting of all nodes and a subset of the edges \rb{(or all edges)} present in the network. %(possibly all edges). 
The four communication matrices, $\Wmbf_1$, $\Wmbf_2$, $\Wmbf_3$ and $\Wmbf_4$, represent four (possibly different) network topologies over which the corresponding vectors are communicated. 
This allows one to represent all ATC and CTA based communication strategies seen in literature using \eqref{eq : general form}. Some of these cases have been shown in \cref{tab: Algorithm Def} where $\Wmbf$ is the mixing matrix for the connected network.

\rb{The iterate update form \eqref{eq : general form} also represents \emph{heterogeneous} communication strategies, where different pieces of information are shared across a different subset of network edges. Such strategies are useful in several decentralized settings such as networks with bandwidth limitations where the amount of data transfer on each edge is limited XXX. Such network settings can be represented using appropriate choice of communication matrices used in \eqref{eq : general form}. In \cref{sec.theory} we establish conditions on the communication matrices to ensure linear convergence of the iterates generated by GTA framework. Specifically,} ensuring $\Wmbf_1$ and $\Wmbf_3$ to represent a connected (possibly different) network is sufficient to guarantee linear convergence to the solution.
%Our generalization can also represent heterogeneous communication strategies. In this case, different pieces of information are shared across a different subset of network edges. 
%Such a system can be represented using the communication matrices. This might be useful in decentralised systems with bandwidth limitations to reduce load on some network edges. We find that ensuring $\Wmbf_1$ and $\Wmbf_3$ represent a connected (possibly different) network is sufficient to ensure linear convergence to the solution.

\begin{algorithm}[H]
    \caption{\texttt{GTA}: Gradient Tracking Algorithm}
    \textbf{Inputs:} initial point $\xmbf_{0, 1} \in \R{nd}$, step size $\alpha$, computations $n_g$, communications $n_c$
    \begin{algorithmic}[1]
        \State $\textbf{y}_{0, 1} \gets \nabla \textbf{f}(\textbf{x}_{0, 1})$
        \For{$k \gets 0, 1, 2$ ... }    
            \If{$n_g > 1$}
                \For{$j \gets 1, 2$ ... $, n_g-1$}
                    \State $\textbf{x}_{k, j+1} \gets \textbf{x}_{k, j} - \alpha \,\textbf{y}_{k, j}$
                    \State $\textbf{y}_{k, j+1} \gets \textbf{y}_{k, j} + \nabla \textbf{f}(\textbf{x}_{k, j+1})  - \nabla \textbf{f}(\textbf{x}_{k, j})$
                \EndFor
            \EndIf
            
            \State $\textbf{x}_{k+1, 1} \gets \textbf{Z}_1^{n_c} \textbf{x}_{k, n_g} - \alpha \, \textbf{Z}_2^{n_c} \textbf{y}_{k, n_g}$
            \State $\textbf{y}_{k+1, 1} \gets \textbf{Z}_3^{n_c} \textbf{y}_{k, n_g} + \textbf{Z}_4^{n_c}(\nabla \textbf{f}(\textbf{x}_{k+1, 1})  - \nabla \textbf{f}(\textbf{x}_{k, n_g}))$
        \EndFor
    \end{algorithmic}
    \label{alg : Deterministic}
\end{algorithm}

Based on \eqref{eq : general form}, we present \texttt{GTA} (\cref{alg : Deterministic}), a deterministic algorithm that performs $n_c \geq 1$ communication steps and $n_g \geq 1$ computation steps each iteration. 
% It first performs $n_g - 1$ computation steps done in an inner loop without communication. Then it performs one computation with $n_c$ communication steps. The multiple communications are represented by taking power $n_c$ of $\Zmbf_i \,\, \forall \,\, i = 1, 2, 3, 4$ as in \cite{berahas2018balancing}.
\rb{Performing multiple communication steps improves consensus across the network. Also, if $\ymbf_k$ is a good estimate of the average gradient across the network, performing multiple computation steps using only local information will drive the local iterate closer to the global optimal solution without distorting the consensus error too much. A balance between these two steps is required to achieve overall efficiency and \cref{alg : Deterministic} allows for such flexibility in these steps by choosing $n_g$ and $n_c$ appropriately.}
%The intuition behind the method is that the multiple communication steps would provide improved consensus across the network. Also, if $\ymbf_k$ is a good estimate of the average gradient across the network, performing multiple computation steps updating it with only local information will drive the local iterate closer to the global optimal solution without distorting the consensus error too much.

\begin{table}\centering
\caption{Special cases of Gradient Tracking Algorithms (GTA).}\label{tab: Algorithm Def}
\begin{tabular}{l*{4}{>{\centering\arraybackslash}p{0.8cm}}c}\toprule
\multirow{2}{*}{Method} &\multicolumn{4}{c}{Communication Matrices} & Algorithms in literature\\\cmidrule{2-5}
&$\Wmbf_1$&$\Wmbf_2$&$\Wmbf_3$&$\Wmbf_4$& $(n_c = n_g = 1)$\\\midrule
\texttt{GTA-1} &$\Wmbf$ &$I_n$ &$\Wmbf$ &$I_n$& EXTRA\footnotemark[4] \cite{shi2015extra}, DIGing \cite{nedic2017achieving} \\\hdashline
\texttt{GTA-2} &$\Wmbf$ &$\Wmbf$ &$\Wmbf$ &$I_n$ & SONATA \cite{sun2022distributed}, NEXT \cite{di2016next}, \cite{pu2020push} \\\hdashline
\texttt{GTA-3} &$\Wmbf$ &$\Wmbf$ &$\Wmbf$ &$\Wmbf$ & Aug-DGM \cite{xu2015augmented}, ATC-DIGing \cite{nedic2017geometrically}\\ 
\bottomrule
\end{tabular}
\end{table}

\bremark 
We make the following remarks about \cref{alg : Deterministic}. 

\begin{itemize}
    \item \textbf{Inner and Outer Loops} - Lines 2--8 form the outer loop. The algorithm performs $n_c$ communication steps each outer iteration in lines 7--8. These multiple communications are theoretically represented by taking powers of the communication matrices. The algorithm performs $n_g$ local gradient computations each outer iteration with one computation in line 8 $(\nabla \fmbf(\xmbf_{k+1, 1}))$ and $n_g - 1$ computations in the inner loop (lines 4-6, $(\nabla \fmbf(\xmbf_{k, j+1}))$). The inner loop is \rb{only} executed if more than one computation is to be performed every outer iteration (line 3).
    
    \item \textbf{Step size} ($\alpha$) - The algorithm employs a constant step size \rb{that depends on the choice of $n_g$ and $n_c$}. Increase in communication steps per iteration ($n_c$) allows employing a larger stepsize. Increase in the computation steps per iteration ($n_g$) requires the stepsize to be reduced at a polynomial rate.
    
    \item \textbf{Communication Strategy} - The communication matrices employed in lines 7--8 allow for various communication strategies to be employed. If $\Wmbf_1$ and $\Wmbf_2$ represent connected networks, the algorithm can show linear rate of convergence for any composition of communication and computation steps.
\end{itemize} 
\eremark

% \texttt{GTA} converges to the solution at a linear rate employing a constant stepsize given $\Wmbf_1$ and $\Wmbf_3$ represent connected networks. Increasing $n_c$ (communication steps) allows a larger stepsize to be employed providing accelerated convergence. Increase in $n_g$ (computation steps) in \texttt{GTA} requires the stepsize to be reduced at a polynomial rate. While the effect of $n_g$ on convergence rate is not clear, we empirically find it brings acceleration to convergence. 

We analyse \texttt{GTA} and provide results for some popular communication strategies as special cases summarised in \cref{tab: Algorithm Def}. The choice of the communication matrices, $\textbf{W}_1, \textbf{W}_2, \textbf{W}_3$ and $\textbf{W}_4$, hence the communication strategy, impacts both convergence of algorithm (discussed in \cref{sec.theory}) and practical implementation. Consider instances \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3} from \cref{tab: Algorithm Def} when $n_g=1$. Computing local gradients and communications can be performed in parallel in \texttt{GTA-1} and \texttt{GTA-2}. \texttt{GTA-3} requires performing communication and computation sequentially. Depending on properties of the problem and system, such trade-offs can create significant impacts. 


\sg{Removed empirical experiments to explain how ATC and CTA differ as there is already so much theory and literature on it in the paper, felt redundant, let me know? The figures are available in Figures/Method Explanation}

\begin{comment}
To provide intuition on how these instances differ, we run an experiment of minimizing quadratic functions over a 16 node network where each node is initialized at the global minimizer and the gradient track $\textbf{y}_0$ is initialized as the local gradient at each node. We run this experiment over two networks, a fully connected network, where all nodes are connected with equal weights and a cyclic network where each node is connected to exactly 2 nodes with 50 communication and 5 computation steps. We measure the optimization error ($\|\Bar{x}_k - x^*\|$) and the consensus error ($\|x_k - 1\Bar{x}_k^T\|$) summarised in figure~\cref{fig : Method Differences Display}. In the fully connected network, no agent moves away from the global optimizer in DIGing\_2K and DIGing\_3K while they do so in DIGing\_K to find their way back. In the cyclic network, due to lack of communication the iterates will move away but we can see how DIGing\_3K performs better in both optimization and consensus error than DIGing\_2K and DIGing\_K.

The above discussion highly favours DIGing\_3K but the choice of communication structure also impacts the implementation of these algorithms. The communication and first local gradient calculation can be parallilized in DIGing\_K and DIGing\_2K while are required to be performed sequentially for DIGing\_3K which has the potential to highly impact practical performance of certain distributed system problems. Thus the mixing matrices should be chosen to reflect the specifics of the task and the system being used.


\begin{figure}[H]
\centering
    
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Method Explanation/Full1_error_opt_cost_iter.png}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Method Explanation/Full1_error_con_cost_iter.png}
    \end{subfigure}
    
\caption{Optimization and Consensus error for instances of Algorithm~\cref{alg : Deterministic} when the initial point for every agent is the global optimizer}
\label{fig : Method Differences Display}
\end{figure}
\end{comment}

\footnotetext[4]{\texttt{GTA-1} with $n_c = n_g = 1$ is a special case of EXTRA when the first mixing matrix in EXTRA is set to $(2\Wmbf - I_n)$ and the second is set to $\Wmbf^2$.}