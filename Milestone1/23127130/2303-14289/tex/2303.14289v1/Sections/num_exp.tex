\section{Numerical Experiments}\label{sec.num_exp}

\setcounter{footnote}{1} 
In this section, we %investigate 
illustrate the empirical performance of the methods defined in \cref{tab: Algorithm Def} using Python implementations\footnote{Our code will be made publicly available upon publication of the manuscript. Github repository: \url{https://github.com/Shagun-G/Gradient-Tracking-Algorithmic-Framework}.}. %We study the effect of communication strategy and composition of communication and computation steps as proposed in \cref{alg : Deterministic}. %We aim to 
The aim of this section is to show, over multiple problems, that different communication strategies and the balance between communication and computation steps %\vrb{in \cref{alg : Deterministic}} 
can substantially effect the algorithm's performance. Specifically, we establish the relative performance of the methods defined in \cref{tab: Algorithm Def} and illustrate the benefits of flexibility in terms of communication and computation steps.

We present results on two problems: $(1)$ a synthetic strongly convex quadratic problem (\cref{sec : quads}); and, $(2)$ binary classification logistic regression problems over the mushroom and australian datasets \cite{Dua:2019} (\cref{sec : logistic}). We %invesigate
investigated 
two %various 
network structures (different mixing matrix $\Wmbf$) with $n = 16$ nodes: %. 
$(1)$ a connected cyclic network ($\beta = 0.992$) where all nodes have two neighbours; and, $(2)$ a connected star network ($\beta = 0.95$) where all nodes are connected to a single central node. %All chosen 
Both networks have low connectivity (i.e., high $\beta$). We should note that the performance of \cref{alg : Deterministic} with multiple communication steps is equivalent to the performance over a network with higher connectivity (i.e., lower $\beta$).
% The performance of \cref{alg : Deterministic} with multiple communication steps is equivalent to the performance over a network with high connectivity (i.e. lower $\beta$).

%We denote the 
The methods defined in \cref{tab: Algorithm Def} 
are denoted 
as \texttt{GTA}$-i(n_c, n_g)$, $i = 1, 2, 3$, where $n_c$ and $n_g$ are the number of communication and computation steps, respectively. We %test them over 
tested 5 values of $n_c$ and $n_g$ for each of the methods; %. That is,  
$n_c \in \{1, 5, 10, 50, 100\}$ and $n_g \in \{ 1, 5, 20, 50, 100\}$. We compared the  performance with popular gradient tracking methods, which are special cases of our generalized framework. %, as shown in \cref{tab: Algorithm Def}}.  
The step sizes were tuned over the set $\{2^{-t} | t = 0, 1, 2, .., 20\}$ for all algorithms and problems, and the initial iterates for all algorithms, problems and nodes were set to the zero vector (i.e., $\xmbf_k = \mathbf{0}$). 
%All nodes are initialized zero. 
The performance of the methods was measured %using
in terms of the optimization error ($\|\bar{x}_k - x^*\|_2$) and the consensus error ($\|\xmbf_k - \xbb_k\|_2$). We do not track the consensus error in auxiliary variable $\ymbf_k$ ($\|\ymbf_k - \ybb_k\|_2$) as this measure does not provide any significant additional insights about the performance of the algorithms.  
% \footnote{\vrb{We do not track the consensus error ($\|\ymbf_k - \ybb_k\|_2$) in auxiliary variable $\ymbf_k$ as this measure will not provide significant additional insights about the performance of the algorithms.}}. 
The optimal solution $x^*$ for quadratic problem was obtained analytically and for the logistic regression problems was obtained by
%For logistic regression, it is obtained by 
running gradient descent in the centralized setting to high accuracy, i.e., $\|\nabla f(x^*)\|_2 \leq 10^{-12}$. %over the centralised problem up to \vrb{$\|\nabla f(x^*)\|_2 \leq 10^{-12}$.}
%We do not track the consensus error ($\|\ymbf_k - \ybb_k\|_2$) in auxiliary variable $\ymbf_k$ as is not a useful output of the algorithm.  We track these errors across number of outer iterations, number of communications and number of gradients computed. 
%The performance of gradient tracking methods is well established in \vrb{the} literature, thus we focus on the effect of using the proposed flexibility and comparison between communication strategies.

\subsection{Quadratic Problems}\label{sec : quads}

%In this section we consider
We first consider quadratic problems
\begin{align*} %\label{eq : quad problem}
    f(x) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2}x^TQ_ix + b_i^Tx,
\end{align*}
where $Q_i \in \mathbb{R}^{10 \times 10}$, $Q_i \succ 0$ and $b_i \in \mathbb{R}^{10}$ is the local information at each node $i \in \{1, 2, .., n\}$, and $n=16$. Each local problem is strongly convex and was generated using the procedure described in \cite{mokhtari2016network}, with global condition number, $\kappa \approx 10^4$. 

\cref{fig : Quadratic cyclic,fig : Quadratic Star} show the performance of \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3} over a cyclic network and a star network, respectively. Our first observation, from the iteration plots in both the figures, is that the optimization error and consensus error converge at a linear rate for all methods, matching the theoretical results of \cref{sec.theory}. Moreover, improvements in the rates of convergence of all methods are observed as a result of the flexibility in terms of the number of communication and computation steps. Specifically, the consensus error is improved (and on par optimization error) when multiple communication steps with single computation step are performed (see \texttt{GTA-i}($1$, 1) vs. \texttt{GTA-i}($n_c$, 1) lines), and the optimization error is improved (and on par consensus error) when multiple computation steps with same number of communication steps are performed (see \texttt{GTA-i}($n_c$, 1) vs. \texttt{GTA-i}($n_c$, $n_g$) lines). % These observations are not surprising due to the theoretical results that show improvements with multiple communication and communication steps. Moreoevr, as communication increase consesnus error gets improved and as communications increase the optimality error gets improved. 
These observations match the theory presented in Section~\ref{sec.mult grads}. 
That being said, %However 
these improvements come at a higher cost in terms of total communication or computation steps, respectively, and an optimal choice of $(n_c, n_g)$ depends on the exact cost structure that combines the complexity of both these steps (see \cite{berahas2018balancing}). Finally, we also observe that \texttt{GTA-2} and \texttt{GTA-3} outperform \texttt{GTA-1} in terms of optimization error and achieve similar consensus error. The performance of \texttt{GTA-2} and \texttt{GTA-3} is very similar for this problem, we suspect the reason for this behavior is due to the large $\beta$ and the high condition number ($\kappa \approx 10^4$) that dominate the rate constant; see \cref{col. g=1 rate bound}. 
  



%Within the cases of single computation, when compared with respect to iterations, performing multiple communications does bring an overall improvement mainly in form the improved consensus error. The effect is more prominent in \texttt{GTA-2} and \texttt{GTA-3} than \texttt{GTA-1} for both cyclic and star networks when optimization error is also considered. This does come at a cost that becomes evident when we compare with respect to communications. When multiple computations are performed along with the multiple communications, with respect to iterations we see improvement over the other cases in the form of improved optimization error. Again this comes at the cost high number of gradient evaluations in both cyclic and star networks. We believe we don't see a significant difference in the performance of \texttt{GTA-2} and \texttt{GTA-3} as the problem is highly ill conditioned ($\kappa \approx 10^4$).



\begin{figure}[]
\centering
\includegraphics[width=\textwidth]{Figures/Quad_cyclic.pdf}  
\caption{Optimization Error ($\|\xbar_k - x^*\|_2$) and Consensus Error ($\|\xmbf_k - \xbb_k\|_2$) of \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3} with respect to number of iterations, communications and gradient evaluations for a synthetic quadratic problem ($n = 16$, $d = 10$, $\kappa = 10^4$) over a cyclic network ($\beta =  0.992$).}
\label{fig : Quadratic cyclic}
\end{figure}

\begin{figure}[]
\centering
\includegraphics[width=\textwidth]{Figures/Quad_star.pdf}    
\caption{Optimization Error ($\|\xbar_k - x^*\|_2$) and Consensus Error ($\|\xmbf_k - \xbb_k\|_2$) of \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3} with respect to number of iterations, communications and gradient evaluations for a synthetic quadratic problem ($n = 16$, $d = 10$, $\kappa = 10^4$) over star network($\beta =  0.95$).}
\label{fig : Quadratic Star}
\end{figure}



\subsection{Binary Classification Logistic Regression}\label{sec : logistic}
%In this section 
Next, we consider $\ell_2$-regularized binary classification logistic regression problems of the form
\begin{align*} %\label{eq : logistic problem}
    f(x) &= \frac{1}{n} \sum_{i=1}^n \frac{1}{n_i}\log(1 + e^{-b_i^TA_ix}) + \frac{1}{n_i}\|x\|_2^2, 
\end{align*}
%which is negative log likelihood for binary classification using logistic regression 
%\vrb{$\ell_2$ regularized binary classification logistic regression problems} 
%where 
where each node $i \in \{1, 2, .., n\}$ has a portion of data samples $A_i \in \mathbb{R}^{n_i \times d}$ and corresponding labels $b_i \in \{0, 1\}^{n_i}$. Experiments were performed over the mushroom dataset ($n = 16$, $d = 117$, $\sum_{i=1}^n n_i = 8124$) and the australian dataset ($n = 16$, $d = 41$, $\sum_{i=1}^n n_i = 690$) \cite{Dua:2019}. 
%with $d = 117$ and 8124 sample datapoints and the Australian dataset \cite{Dua:2019} with $d = 14$ and 690 sample datapoints. 

\cref{fig : Mushroom Cyclic,fig : Austrailian Star} show the performance of \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3} over a cyclic network ($\beta =  0.992$) for the mushroom dataset and a star network for the australian dataset ($\beta =  0.95$), respectively. Similar observations  to those made for the quadratic problem with respect to the effect of performing multiple communication and computation steps can also be made for these problems. %The comparisons made in the previous section with respect to quadratic problems among the methods and improvement with multiple communications and computations are still observed in both the figures here. 
Additionally, we observe that \texttt{GTA-3} outperforms \texttt{GTA-2} on these problems. 
%We also additionally observe improvements when we compare \texttt{GTA-2} and \texttt{GTA-3}. 
We should note that although \texttt{GTA-3} performs the best within these experiments, it also brings certain implementation constraints; see \cref{sec.methods}.

\begin{figure}[]
\centering
\includegraphics[width=\textwidth]{Figures/Mushroom_cyclic.pdf} 
\caption{Optimization Error ($\|\xbar_k - x^*\|_2$) and Consensus Error ($\|\xmbf_k - \xbb_k\|_2$) of \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3} with respect to number of iterations, communications and gradient evaluations for binary logistic regression on Mushroom dataset ($n = 16$, $d = 117$, $\sum_{i=1}^n n_i = 8124$) over cyclic network ($\beta =  0.992$).}
\label{fig : Mushroom Cyclic}
\end{figure}

\begin{figure}[]
\centering
\includegraphics[width=\textwidth]{Figures/Austrailian_star.pdf}  
\caption{Optimization Error ($\|\xbar_k - x^*\|_2$) and Consensus Error ($\|\xmbf_k - \xbb_k\|_2$) of \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3} with respect to number of iterations, communications and gradient evaluations for binary logistic regression on Australian dataset ($n = 16$, $d = 41$, $\sum_{i=1}^n n_i = 690$) over star network ($\beta = 0.95$).
% Performance of GTA-1, GTA-2 and GTA-3 measured in terms of optimization error ($\|\xbar_k - x^*\|_2$) and consensus error ($\|\xmbf_k - \xbb_k\|_2$) with respect to number of iterations, communications and gradient evaluations for binary logistic regression on Australian dataset ($n = 16$, $d = 14$, $\sum_{i=1}^n n_i = 690$) over \vrb{s}tar \vrb{n}etwork ($\beta = 0.95$)
}
\label{fig : Austrailian Star}
\end{figure}

% We %display
% report the performance of the methods with respect to communication and computation (gradient) steps but the true cost is a combination of both based on the complexity of the steps in your application \cite{berahas2018balancing}. The methods converge for any composition of communication and computation. We observe from our experiments that increased number of computation ($n_g$) brings improved performance when the network has a good amount of connectivity. If the network has low connectivity, we can increase number of communications ($n_c$) to improve connectivity.

% \sg{We also restate that although \texttt{GTA-3} performs the best within these experiments, it also brings in certain implementation constraints; see \cref{sec.methods}. Thus, selecting the best communication strategy takes such constraints, the problem and system parameters into account.}