
\section{Numerical Experiments}\label{sec.num_exp}

\setcounter{footnote}{1} 
In this section, we 
illustrate the empirical performance of the methods defined in \cref{tab: Algorithm Def} using Python implementations\footnote{Our code will be made publicly available upon publication of the manuscript. Github repository: \url{https://github.com/Shagun-G/Gradient-Tracking-Algorithmic-Framework}. Moreover, additional extensive numerical results can be found in the same repository.}. 
The aim of this section is to show, over multiple problems, that different communication strategies and the balance between communication and computation steps 
can substantially effect the algorithm's performance. Specifically, we establish the relative performance of the methods defined in \cref{tab: Algorithm Def} and illustrate the benefits of the flexibility in terms of communication and computation steps.

We present results on two problems: $(1)$ a synthetic strongly convex quadratic problem (\cref{sec : quads}); and, $(2)$ binary classification logistic regression problems over the mushroom and australian datasets \cite{Dua:2019} (\cref{sec : logistic}). We 
investigated 
two  
network structures (different mixing matrix $\Wmbf$) with $n = 16$ nodes: %. 
$(1)$ a connected cyclic network ($\beta = 0.992$) where all nodes have two neighbours; and, $(2)$ a connected star network ($\beta = 0.95$) where all nodes are connected to a single central node. %All chosen 
Both networks have low connectivity (i.e., high $\beta$). We should note that the performance of \cref{alg : Deterministic} with multiple communication steps is equivalent to the performance over a network with higher connectivity (i.e., lower $\beta$).

The methods defined in \cref{tab: Algorithm Def} 
are denoted 
as \texttt{GTA}$-i(n_c, n_g)$, $i = 1, 2, 3$, where $n_c$ and $n_g$ are the number of communication and computation steps, respectively. We %test them over 
tested 5 values of $n_c$ and $n_g$ for each of the methods; %. That is,  
$n_c \in \{1, 5, 10, 50, 100\}$ and $n_g \in \{ 1, 5, 20, 50, 100\}$. We compared the  performance of %with 
popular gradient tracking methods, which are special cases of our generalized framework. %, as shown in \cref{tab: Algorithm Def}}.  
The step sizes were tuned over the set $\{2^{-t} | t = 0, 1, 2, .., 20\}$ for all algorithms and problems, and the initial iterates for all algorithms, problems and nodes were set to the zero vector (i.e., $\xmbf_k = \mathbf{0}$). 
The performance of the methods was measured %using
in terms of the optimization error ($\|\bar{x}_k - x^*\|_2$) and the consensus error ($\|\xmbf_k - \xbb_k\|_2$). We do not report %track 
the consensus error in the auxiliary variable $\ymbf_k$ ($\|\ymbf_k - \ybb_k\|_2$) as this measure does not provide any significant additional insights about the performance of the algorithms.  
The optimal solution $x^*$ for quadratic problem was obtained analytically and for the logistic regression problems was obtained by
running gradient descent in the centralized setting to high accuracy, i.e., $\|\nabla f(x^*)\|_2 \leq 10^{-12}$. 

\subsection{Quadratic Problems}\label{sec : quads}

We first consider quadratic problems
\begin{align*} 
    f(x) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2}x^TQ_ix + b_i^Tx,
\end{align*}
where $Q_i \in \mathbb{R}^{10 \times 10}$, $Q_i \succ 0$ and $b_i \in \mathbb{R}^{10}$ is the local information at each node $i \in \{1, 2, .., n\}$, and $n=16$. Each local problem is strongly convex and was generated using the procedure described in \cite{mokhtari2016network}, with global condition number $\kappa \approx 10^4$. 

\cref{fig : Quadratic cyclic,fig : Quadratic Star} show the performance of \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3} over a cyclic network and a star network, respectively. Our first observation, from the iteration plots in both the figures, is that the optimization error and consensus error converge at a linear rate for all methods, matching the theoretical results of \cref{sec.theory}. Moreover, improvements in the rates of convergence of all methods are observed as a result of the flexibility in terms of the number of communication and computation steps. Specifically, the consensus error is improved (and on par optimization error) when multiple communication steps with single computation step are performed (see \texttt{GTA-i}($1$, 1) vs. \texttt{GTA-i}($n_c$, 1) lines), and the optimization error is improved (and on par consensus error) when multiple computation steps with same number of communication steps are performed (see \texttt{GTA-i}($n_c$, 1) vs. \texttt{GTA-i}($n_c$, $n_g$) lines). 
These observations match the theory presented in Section~\ref{sec.mult grads}. 
That being said, %However 
these improvements come at a higher cost in terms of total communication or computation steps, respectively, and an optimal choice of $(n_c, n_g)$ depends on the exact cost structure that combines the complexity of both these steps; see e.g., \cite{berahas2018balancing}. Finally, we also observe that \texttt{GTA-2} and \texttt{GTA-3} outperform \texttt{GTA-1} in terms of optimization error and achieve similar consensus error. The performance of \texttt{GTA-2} and \texttt{GTA-3} is very similar for this problem, we suspect the reason for this behavior is due to the large $\beta$ and the high condition number ($\kappa \approx 10^4$) that dominate the rate constant; see \cref{col. g=1 rate bound}. 

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{Figures/Quad_cyclic.pdf}  
\caption{Optimization Error ($\|\xbar_k - x^*\|_2$) and Consensus Error ($\|\xmbf_k - \xbb_k\|_2$) of \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3} with respect to number of iterations, communications and gradient evaluations for a synthetic quadratic problem ($n = 16$, $d = 10$, $\kappa = 10^4$) over a cyclic network ($\beta =  0.992$).}
\label{fig : Quadratic cyclic}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{Figures/Quad_star.pdf}    
\caption{Optimization Error ($\|\xbar_k - x^*\|_2$) and Consensus Error ($\|\xmbf_k - \xbb_k\|_2$) of \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3} with respect to number of iterations, communications and gradient evaluations for a synthetic quadratic problem ($n = 16$, $d = 10$, $\kappa = 10^4$) over star network($\beta =  0.95$).}
\label{fig : Quadratic Star}
\end{figure}

\subsection{Binary Classification Logistic Regression}\label{sec : logistic}
Next, we consider $\ell_2$-regularized binary classification logistic regression problems of the form
\begin{align*} %\label{eq : logistic problem}
    f(x) &= \frac{1}{n} \sum_{i=1}^n \frac{1}{n_i}\log(1 + e^{-b_i^TA_ix}) + \frac{1}{n_i}\|x\|_2^2, 
\end{align*}
where each node $i \in \{1, 2, .., n\}$ has a portion of data samples $A_i \in \mathbb{R}^{n_i \times d}$ and corresponding labels $b_i \in \{0, 1\}^{n_i}$. Experiments were performed over the mushroom dataset ($n = 16$, $d = 117$, $\sum_{i=1}^n n_i = 8124$) and the australian dataset ($n = 16$, $d = 41$, $\sum_{i=1}^n n_i = 690$) \cite{Dua:2019}. 

\cref{fig : Mushroom Cyclic,fig : Austrailian Star} show the performance of \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3} over a cyclic network ($\beta =  0.992$) for the mushroom dataset and a star network for the australian dataset ($\beta =  0.95$), respectively. Similar observations  to those made for the quadratic problem with respect to the effect of performing multiple communication and computation steps can also be made for these problems. 
Additionally, we observe that \texttt{GTA-3} outperforms \texttt{GTA-2} on these problems. 
We should note that although \texttt{GTA-3} performs the best within these experiments, it also brings certain implementation constraints; see \cref{sec.methods}.

\begin{figure}[]
\centering
\includegraphics[width=\textwidth]{Figures/Mushroom_cyclic.pdf} 
\caption{Optimization Error ($\|\xbar_k - x^*\|_2$) and Consensus Error ($\|\xmbf_k - \xbb_k\|_2$) of \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3} with respect to number of iterations, communications and gradient evaluations for binary logistic regression on Mushroom dataset ($n = 16$, $d = 117$, $\sum_{i=1}^n n_i = 8124$) over cyclic network ($\beta =  0.992$).}
\label{fig : Mushroom Cyclic}
\end{figure}

\begin{figure}[]
\centering
\includegraphics[width=\textwidth]{Figures/Austrailian_star.pdf}  
\caption{Optimization Error ($\|\xbar_k - x^*\|_2$) and Consensus Error ($\|\xmbf_k - \xbb_k\|_2$) of \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3} with respect to number of iterations, communications and gradient evaluations for binary logistic regression on Australian dataset ($n = 16$, $d = 41$, $\sum_{i=1}^n n_i = 690$) over star network ($\beta = 0.95$).
}
\label{fig : Austrailian Star}
\end{figure}