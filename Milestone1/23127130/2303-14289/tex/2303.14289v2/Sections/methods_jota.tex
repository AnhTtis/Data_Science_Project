\section{Gradient Tracking Algorithmic Framework}\label{sec.methods}
In this section, we describe our algorithmic framework (\texttt{GTA}) that unifies gradient tracking methods. We then extend the framework to allow for flexibility in the number of communication and  computation steps performed at every iteration. Finally, we make remarks about the algorithmic framework and implementation, and then discuss popular gradient tracking methods as special cases of our proposed framework.

The 
iterate update form (for all $k\geq0$) for the decision variable $\xmbf \in \mathbb{R}^{nd}$ and the auxiliary variable $\ymbf \in \mathbb{R}^{nd}$ that we propose to unify gradient tracking methods is  
\begin{equation}\label{eq: general_form}
\begin{aligned}
    \xmbf_{k+1, 1} & = \Zmbf_1 \xmbf_{k, 1} - \alpha \Zmbf_2 \ymbf_{k,1} \\ 
    \ymbf_{k+1, 1} & = \Zmbf_3 \ymbf_{k, 1} + \Zmbf_4 (\nabla \fmbf(\xmbf_{k+1, 1}) - \nabla \fmbf(\xmbf_{k, 1})), 
\end{aligned}
\end{equation}
where $\alpha>0$ is the constant step size, $\Zmbf_i = \Wmbf_i \otimes I_d \in \mathbb{R}^{nd \times nd}$ for $i = 1, 2, 3, 4$ and $\Wmbf_i \in \mathbb{R}^{n \times n}$ are communication matrices. A communication matrix $\Umbf \in \mathbb{R}^{n \times n}$ is a symmetric, doubly stochastic matrix that respects the connectivity of the network, i.e., $u_{ii} > 0$ and $u_{ij}  \geq  0$ ($i \neq j$) if $(i,j) \in \mathcal{E}$ and $u_{ij} = 0$ ($i \neq j$) if $(i,j) \notin \mathcal{E}$. 
The communication matrices, $\Wmbf_i$ for $i = 1, 2, 3, 4$, represent four (possibly different) network topologies consisting of all the nodes and (possibly different) subsets of the edges of the network over which the corresponding vectors are communicated.
The update form given in \eqref{eq: general_form} generalizes many popular gradient tracking methods for different choices of the communication matrices; see \cref{tab: Algorithm Def}. 
While the  methodology has %shares 
similarities to \cite{xu2021distributed, alghunaim2020decentralized}, our framework allows for the exact specification of the communication quantities within the network and does not %while not imposing 
impose any interdependent conditions among the communication matrices $\Wmbf_i$ for $i = 1, 2, 3, 4$.
In \eqref{eq: general_form} one communication and one computation step is performed at every iteration and so the inner iteration index is always $1$. 
We include this subscript for consistency with the presentation of the algorithm and analysis with multiple communication and computation steps.

\begin{table}[H]\centering
\caption{Special cases of Gradient Tracking Algorithm (\texttt{GTA}).  
}\label{tab: Algorithm Def}
\begin{tabular}{l*{4}{>{\centering\arraybackslash}p{0.8cm}}c}\toprule
\multirow{2}{*}{Method} &\multicolumn{4}{c}{Communication Matrices} & Algorithms in literature\\\cmidrule{2-5}
&$\Wmbf_1$&$\Wmbf_2$&$\Wmbf_3$&$\Wmbf_4$& $(n_c = n_g = 1)$\\\midrule
\texttt{GTA-1} &$\Wmbf$ &$I_n$ &$\Wmbf$ &$I_n$& DIGing \cite{nedic2017achieving}, EXTRA  
\cite{shi2015extra},  \\\hdashline
\texttt{GTA-2} &$\Wmbf$ &$\Wmbf$ &$\Wmbf$ &$I_n$ & SONATA \cite{sun2022distributed}, NEXT \cite{di2016next,pu2020push} \\\hdashline
\texttt{GTA-3} &$\Wmbf$ &$\Wmbf$ &$\Wmbf$ &$\Wmbf$ & Aug-DGM \cite{xu2015augmented}, ATC-DIGing \cite{nedic2017geometrically}\\ 
\bottomrule
\end{tabular}

Note: $\Wmbf$ is a mixing matrix.
\end{table}

We incorporate multiple communications in \eqref{eq: general_form} by replacing $\Zmbf_i$ with $\Zmbf_i^{n_c} = \Wmbf_i^{n_c} \otimes I_d$ for $i=1, 2, 3, 4$, where $n_c \geq 1$ is the number of communication steps at each iteration. 
Taking the communication matrices to the $n_c$ power represents performing $n_c$ communication (consensus) steps at every iteration. We further extend \eqref{eq: general_form} to incorporate multiple computation steps at each iteration. That is, the algorithm performs multiple local updates before communicating information with local neighbors. Our full algorithmic framework with flexibility in the number of communication and computation steps, i.e., $n_c \geq 1$ and $n_g \geq 1$, is given in \cref{alg : Deterministic}. A balance between the number of communication and computation steps is required to achieve overall efficiency for different applications, and \texttt{GTA} allows for such flexibility in these steps via the parameters $n_g$ and $n_c$.

\begin{algorithm}[H]
    \caption{\texttt{GTA}: Gradient Tracking Algorithm}
    \textbf{Inputs:} initial point $\xmbf_{0, 1} \in \R{nd}$, step size $\alpha >0$, computations $n_g \geq 1$, 
    
    communications $n_c \geq 1$.
    \begin{algorithmic}[1]
        \State $\textbf{y}_{0, 1} \gets \nabla \textbf{f}(\textbf{x}_{0, 1})$
        \For{$k \gets 0, 1, 2$ ... }    
            \If{$n_g > 1$}
                \For{$j \gets 1, 2$ ... $, n_g-1$}
                    \State $\textbf{x}_{k, j+1} \gets \textbf{x}_{k, j} - \alpha \,\textbf{y}_{k, j}$
                    \State $\textbf{y}_{k, j+1} \gets \textbf{y}_{k, j} + \nabla \textbf{f}(\textbf{x}_{k, j+1})  - \nabla \textbf{f}(\textbf{x}_{k, j})$
                \EndFor
            \EndIf
            
            \State $\textbf{x}_{k+1, 1} \gets \textbf{Z}_1^{n_c} \textbf{x}_{k, n_g} - \alpha \, \textbf{Z}_2^{n_c} \textbf{y}_{k, n_g}$
            \State $\textbf{y}_{k+1, 1} \gets \textbf{Z}_3^{n_c} \textbf{y}_{k, n_g} + \textbf{Z}_4^{n_c}(\nabla \textbf{f}(\textbf{x}_{k+1, 1})  - \nabla \textbf{f}(\textbf{x}_{k, n_g}))$
        \EndFor
    \end{algorithmic}
    \label{alg : Deterministic}
\end{algorithm}
\bremark 
We make the following remarks about \cref{alg : Deterministic}. 
\begin{itemize}
    \item \textbf{Communications and Computations:} The number of communication and computation steps are dictated by $n_c$ and $n_g$, respectively. 
    By performing multiple communication steps, the goal is to improve consensus across the local decision variables. By performing multiple computation steps, the goal is for individual nodes to make more progress on their local objective functions. 
    \item \textbf{Inner and Outer Loops:} Lines 2--8 form the outer loop and Lines 4--6 form the inner loop. The algorithm performs $n_c$ communication steps each outer iteration (Lines 7 and 8). The algorithm performs $n_g$ local (gradient) computations at each outer iteration; $n_g-1$ computations in the inner loop (Line 6, $\nabla \fmbf(\xmbf_{k, j+1})$) and one computation in the outer loop (Line 8, $\nabla \fmbf(\xmbf_{k+1, 1})$). 
    The inner loop is only executed if more than one computation, i.e., $n_g>1$,  is to be performed every outer iteration (Line 3). By default, we refer to outer iterations when we say iterations unless otherwise specified.
    \item \textbf{Step size ($\alpha>0$):} The algorithm employs a constant step size that depends on the problem parameters, the choices of $n_c$ and $n_g$, and the communication strategy, i.e., $\Wmbf_i$ for $i = 1, 2, 3, 4$. 
\end{itemize} 
\eremark

We analyze \texttt{GTA} and provide results for several popular communication strategies as special cases; summarized in \cref{tab: Algorithm Def}. The choice of the communication matrices ($\Wmbf_i$ for $i = 1, 2, 3, 4$), or equivalently the communication strategy, impact both the convergence of the algorithm and practical implementation. Notice that all methods in \cref{tab: Algorithm Def} require that $\Wmbf_1$ and $\Wmbf_3$ are mixing matrices. Our theoretical results recover this for the general framework. Consider \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3} defined in \cref{tab: Algorithm Def} with $n_g=1$. In \texttt{GTA-1} and \texttt{GTA-2}, computing local gradients and communications can be performed in parallel because the local gradients need not be communicated ($\Wmbf_4 = I_n$). On the other hand, in \texttt{GTA-3}, these steps need to be performed sequentially. Such trade-offs can create significant impact depending on the problem setting and system.

As mentioned above, the communication matrices ($\Wmbf_i$ for $i = 1, 2, 3, 4$) in \texttt{GTA} need not be the same. That is,  different information can be exchanged on subsets of the edges of the network. This allows for a flexibility in the communication strategy that current gradient tracking methodologies do not possess. 
Such strategies can be useful in applying gradient tracking methods to decentralized settings with networks with bandwidth limitations, e.g., optimization problems in cyberphysical systems with battery powered wireless sensors \cite{magnusson2017bandwidth}. 

