
\section{Convergence Analysis}\label{sec.theory}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Prequals to the theory analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section, we present %prove 
theoretical convergence guarantees for our proposed algorithmic framework (\texttt{GTA}). 
The analysis is divided into three subsections. 
In \cref{sec.mult comms}, we analyze the effect of multiple communications, i.e., $n_c \geq 1$ (and $n_g = 1$), on \texttt{GTA} and the three special cases \texttt{GTA-1}, \texttt{GTA-2} and \texttt{GTA-3}. 
While these results are a special case of the results presented in \cref{sec.mult grads}, we present these results first as they are simpler to derive, easier to follow and allow us to gain intuition about the effect of the number of communications. 
We then look at the effect of multiple computations in conjunction with multiple communications, i.e., $n_c \geq 1$ and $n_g \geq 1$, in \cref{sec.mult grads} by extending the analysis from \cref{sec.mult comms}. In \cref{sec.full graph res}, we analyze \texttt{GTA-2} and \texttt{GTA-3} for fully connected networks; this special case is not captured by the analysis in the previous subsections. 

We make the following assumption on the functions. 

\bassumption    \label{asum.convex and smooth}
    The global objective function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is $\mu$-strongly convex. Each component function $f_i: \mathbb{R}^d \rightarrow \mathbb{R}$ $($for $i \in \{ 1,2,\dots,n\}$$)$ has L-Lipschitz continuous gradients. That is, for all $z, z' \in \mathbb{R}^d$
    \begin{align*}
        &f(z')  \geq f(z) +  \langle \nabla f(z), z' - z \rangle + \tfrac{\mu}{2} \|z' - z\|_2^2, \\
        &\|\nabla f_i(z) - \nabla f_i(z')\|_2 \leq L\|z - z'\|_2,    \qquad \qquad \quad \forall \; i = 1, \dots, n.
    \end{align*}
\eassumption
By \cref{asum.convex and smooth}, the global minimizer of~\eqref{eq:prob} is unique, and we denote it by $x^*$.

For notational convenience, we define
\begin{align*}
   \beta^{n_c} = \left\|\Wmbf^{n_c} - \tfrac{1_n1_n^T}{n}\right\|_2, \quad \beta_i^{n_c} = \left\|\Wmbf_i^{n_c} - \tfrac{1_n1_n^T}{n}\right\|_2, \qquad \forall \; i = 1, 2, 3, 4, 
\end{align*}
% \begin{align*}
%    \left\|\Wmbf^{n_c} - \tfrac{1_n1_n^T}{n}\right\|_2 = \beta^{n_c}, \quad \left\|\Wmbf_i^{n_c} - \tfrac{1_n1_n^T}{n}\right\|_2 = \beta_i^{n_c}, \qquad \forall \,\,\, i = 1, 2, 3, 4,    
% \end{align*}
where $\beta \in [0, 1)$ because $\Wmbf$ is a mixing matrix for a connected network and $\beta_i \in [0,1]$ because $\Wmbf_i$ for $ i = 1, 2, 3, 4$ are symmetric, doubly stochastic matrices. Using the definitions of $\Zmbf^{n_c} = \Wmbf^{n_c}\otimes I_{d}$ and $\Zmbf_i^{n_c} = \Wmbf_i^{n_c}\otimes I_{d}$ for $ i = 1, 2, 3, 4$, it follows that
\begin{align}   \label{eq : beta and Z}
    \|\Zmbf^{n_c} - \Imbf\|_2 = \beta^{n_c}, \quad \|\Zmbf_i^{n_c} - \Imbf\|_2 = \beta_i^{n_c},  \qquad \forall \,\, i = 1, 2, 3, 4.
\end{align}
We also define,
\begin{align}\label{eq : derivative terms define}
   h_{k, j} = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(x_{i, k, j}), \quad \hbar_{k, j} = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\xbar_{k, j}), \quad \mbox{and} \quad \Imbf = \frac{1_n1_n^T}{n} \otimes I_d .
\end{align}
where $x_{i,k,j}$, denotes the local copy of the $i$th node, at outer iteration $k$ and inner iteration $j$. 
In the analysis, for all $k\geq 0$, we consider the following error vector
\begin{align*} %\label{eq : error vector def}
    r_k = \begin{bmatrix}
        \|\xbar_{k,1} - x^*\|_2\\
        \|\xmbf_{k,1} - \Bar{\xmbf}_{k,1}\|_2\\
        \|\ymbf_{k,1} - \Bar{\ymbf}_{k,1}\|_2\\
    \end{bmatrix}. %\quad \forall \,\, k \geq 0,
\end{align*}
The error vector $r_k$ combines the optimization error, $\|\xbar_{k,1} - x^*\|_2$, and consensus errors, $\|\xmbf_{k,1} - \Bar{\xmbf}_{k,1}\|_2$ and $\|\ymbf_{k,1} - \Bar{\ymbf}_{k,1}\|_2$ where $\xmbf_{k, 1}$ and $\ymbf_{k, 1}$ are the first iterates of outer iteration $k$. We establish general technical lemmas that quantify the relation between $r_{k+1}$ and $r_k$ for each case of the presented algorithm.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Deterministic Analysis for g = 0
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{Sections/Proof_subsections_jota/mult_comms.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Deterministic Analysis for g > 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{Sections/Proof_subsections_jota/mult_grads.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Fully connected Graph Analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{Sections/Proof_subsections_jota/fully_connected.tex}
