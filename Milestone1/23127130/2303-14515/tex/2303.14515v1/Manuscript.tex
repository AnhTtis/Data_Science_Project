%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
\usepackage{natbib}
\setcitestyle{aysep={}}
\newcommand\citen[1]{\citeauthor*{#1}}						% for author
\newcommand\citens[1]{\citeauthor*{#1}'s}					% for author's work
\newcommand\cites[1]{\citeauthor*{#1}'s\ (\citeyear{#1})}	% for author's (YEAR) work
\newcommand\citenss[1]{\citeauthor*{#1}'}					% for authors' work
\newcommand\citess[1]{\citeauthor*{#1}'\ (\citeyear{#1})}	% for authors' (YEAR) work
\usepackage{xcolor}		% for text color
\usepackage{amssymb}	% for \mathbb{.} 
\usepackage{amsmath}	% for boldsymbol with \pmb{X}
\usepackage{multirow}
%\usepackage{tabu}
\usepackage{tabularx}
\usepackage{enumitem} 	% for style of the counters
\usepackage{pdflscape}
\usepackage{afterpage}
%\usepackage[toc,page]{appendix}	% for appendix ref
%\usepackage{array}		% for fixed width table columns p{10mm}
%\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%

\begin{document}

\title{Specific investments under negotiated transfer pricing: effects of different surplus sharing parameters on managerial performance%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
\subtitle{An agent-based simulation with fuzzy Q-learning agents}

\titlerunning{Specific investments under negotiated transfer pricing}        % if too long for running head

\author{Christian Mitsch
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{Christian Mitsch (0000-0002-8587-4887) \at
              Department of Management Control and Strategic Management, University of Klagenfurt \\
              Universit\"atsstra\ss{}e 65-67, 9020 Klagenfurt, Austria \\
              %Tel.: +123-45-678910\\
              %Fax: +123-45-678910\\
              \email{christian.mitsch@aau.at}           %  \\
              %\emph{Present address:} of F. Author  %  if needed 
}

%\vspace{-20mm}
%\date{Received: date / Accepted: date}
\date{}
% The correct dates will be entered by the editor

%\maketitle

\vspace*{3mm}
\begin{center}
 	\begin{Large}
 		\textbf{Specific investments under negotiated transfer pricing: effects of different surplus sharing parameters on managerial performance} \\
 	\end{Large}
 	\vspace{3mm}
 	\begin{large}
 		An agent-based simulation with fuzzy Q-learning agents \\
 	\end{large}
 	\vspace{10mm}
 	\begin{normalsize}
 		$\text{Christian Mitsch}^{[0000-0002-8587-4887]}$ \\
 		\vspace{3mm}
 		Department of Management Control and Strategic Management \\
 		University of Klagenfurt, 9020, Austria \\
 		\texttt{christian.mitsch@aau.at} 
              
 	\end{normalsize}
\end{center}

\vspace{10mm}

\begin{abstract}


	%In this paper, we focus on negotiated transfer pricing, as is a common way of setting the transfer price in multidivisional firms. %MM
	%This paper focuses on negotiated transfer pricing, as is a common way of setting the transfer price in multidivisional firms. %MM
	%Since, in a negotiated transfer pricing model, the headquarters delegates operating decisions to its divisions, the headquarters has to provide an incentive compatibility to ensure that the divisions act in the headquarters' interest.
	%A common choice 
	%\textcolor{blue}{It is not uncommon for the headquarters to use a surplus-sharing rule as an incentive.} %MM 
	%The concept of subgame perfection equilibrium  is often applied to establish adequate incentive compatibility constraints. %MM
	%A surplus sharing rule is a common choice for this.
	%However, in order to achieve the desired effect, it is necessary that, for example, the divisions are fully individual rational utility maximizers and, in particular, have common knowledge of the opponent's utility function. %L320pg.324
	%Such assumptions are rather heroic and how divisions process information under uncertainty, does not perfectly match with human decision-making behavior. %L331pg22 and L331pg.2
	%Therefore, this paper relaxes some key assumptions and studies whether cognitively bounded agents achieve the same results as fully rational utility maximizers. 
	%For this purpose, we apply fuzzy Q-learning, which is a reinforcement learning method, and provide an agentized version of the negotiated transfer pricing model. %L331pg22
	%For this purpose, an agentized version of the negotiated transfer pricing model with fuzzy Q-learning agents is provided. %L331pg22
	
	This paper focuses on a decentralized profit-center firm that uses negotiated transfer pricing as an instrument to coordinate the production process. % and sales decisions. %MM and L274pg3
	%The firm's headquarters delegates operating decisions, such as quantity decisions, to its divisions and it is assumed that each division can make an upfront investment decision that enhances the value of internal trade. %MM
	Moreover, the firm's headquarters gives its divisions full authority over operating decisions and it is assumed that each division can additionally make an upfront investment decision that enhances the value of internal trade. %MM
	%On early works, the paper expands the number of divisions by one downstream division. % and assumes that all divisions are subject to cognitive limitations. %MM
	%Furthermore, the study relaxes basic assumptions, such as the assumption of common knowledge of rationality for the divisions, since, in the economic transfer pricing literature, the solutions of transfer pricing problems are often based on game theory approaches which require demanding assumptions. %MM
	On early works, the paper expands the number of divisions by one downstream division and relaxes basic assumptions, such as the assumption of common knowledge of rationality.
	Based on an agent-based simulation, it is examined whether cognitively bounded individuals modeled by fuzzy Q-learning achieve the same results as fully rational utility maximizers. %Paper1
	In addition, the paper investigates different constellations of bargaining power to see whether a deviation from the recommended optimal bargaining power leads to a higher managerial performance. %MM
	The simulation results show that fuzzy Q-learning agents perform at least as well or better than fully individual rational utility maximizers. %MM
	The study also indicates that, in scenarios with different marginal costs of divisions, a deviation from the recommended optimal distribution ratio of the bargaining power of divisions can lead to higher investment levels and, thus, to an increase in the headquarters' profit. %Paper1
	
	%The paper examines whether cognitively bounded individuals (modeled with fuzzy Q-learning agents) achieve the same results as fully rational utility maximizers. %Paper1
	%In addition, different constellations of bargaining power are examined to see whether a deviation from the recommended optimal bargaining power leads to a higher divisional performance. %MM
	
	%This paper investigates the impact of surplus sharing on the outcomes of specific investments in a profit-center organization using negotiated transfer pricing. %Internet and Paper1-Titel and MM
	%The analysis focuses on a decentralized firm where divisions can make an upfront investment decision that enhances the value of internal trade.
	%In addition, the divisional investment costs have a quadratic cost structure and it is assumed that divisions differ in their marginal cost parameters.
	%In order to ensure that the divisions act in the headquarters' interest, the headquarters applies a linear surplus sharing rule. %Paper2
	%On early works, this paper expands the number of divisions by one downstream division and assumes that the divisions differ in their marginal costs. %MM
	%In this framework, closed-form expressions are derived to calculate the subgame perfect equilibrium for the investment hold-up problem. %
	
	%Based on an agent-based simulation, it is shown that fuzzy Q-learning agents perform better than fully individual rational utility maximizers.
	%Based on an agent-based simulation, it is shown that fuzzy Q-learning agents perform at least as well or better than fully individual rational utility maximizers. %MM
	%The simulation study also indicates that, in scenarios with different marginal costs of divisions, a deviation from the recommended optimal distribution ratio of the bargaining power of divisions can lead to higher investment levels and, thus, to an increase in the headquarters' profit. %Paper1
	
	
	
	
	
	
	%However, the negotiated transfer pricing model is based on assumptions which are rather heroic. %L331
	%The common knowledge assumption or assumptions of the rational behavior of economic agents, which means that each agent will behave optimally at each decision stage, do not perfectly match with human decision-making behavior.
	%Such models no longer prove to be adequate. %L338pg.21
	
	 %consisting of one supplying division and two buying divisions which differ in their marginal costs. %MM
	%Furthermore, this paper provides closed-form expressions to calculate the subgame perfect equilibrium for the investment hold-up problem. %
	
	
	%However, if cognition and perception are very different between the decision-maker and its environment, such models no longer prove to be adequate. %L338pg.21
	

	%\textcolor{blue}{Neuer Text: Therefore, the assumptions that are made in negotiated transfer pricing, for example, the common knowledge assumption, i.e., everyone knows everyons'....., or the assumption of rational behavior of economic agents, which means that each division will behave optimally at each decision stage, are rather heroic.} %MM
	
	%Against this background, this paper relaxes key assumptions and studies whether cognitively bounded agents achieve the same results as fully rational utility maximizers and, in particular, whether the recommendations on managerial-compensation arrangements and bargaining infrastructures are designed to maximize headquarters' profit in the setting described above. %L331pg1
	
	
	
	
	
	

	% Sätze für mein PAPER!!!
	%The role of a transfer price can be seen as an important instrument in the transfer pricing literature as it provides incentives for specific investment decisions at the divisional level. %L263pg.2

	%In addition, our analysis assumes that the headquarters cannot obtain any information required to play an active role in determining the transfer price. %L157pg.3 (stimmt bei meinem Model nicht ganz, weil Gamma von HQ festgelegt wird)
	%The headquarters only delegates certain rights and obligations to its divisions. %L157pg.3


	%In this paper, we investigate ... a decentralized firm consisting of two divisions, followers and pioneers, and design their decision-making behavior by using fuzzy Q-learning. %


	%However, if cognition and perception are very different between the decision-maker and its environment, such models no longer prove to be adequate. %L338pg.21

	%This study is necessary to give guidance on how to assign the bargaining power between the divisions. %L332pg19
	%These findings suggest that sharing the bargaining power in equal shares could increase headquarters' profit. %L308pg11


	
	

\vspace{5mm}

\keywords{Agent-based simulation \and Fuzzy Q-learning \and Hold-up problem \and Negotiated transfer pricing \and Specific investments}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}



\section{Introduction}
\label{sec:Introduction}

%\subsection{Problem formulation}
%\label{ss:Problem_formulation}

		%In decentralized firms, solving the divisions' coordination problem of manufacturing intermediate products is one of the most challenging tasks in modern business practice. %L172pg1
		In decentralized firms, the headquarters gives its divisions more authority in making decisions. %L172pg1 and https://smallbusiness.chron.com/advantages-disadvantages-companies-decentralize-11938.html
		Transfer pricing and managerial performance evaluation are two key instruments for managing potential conflicts between divisions and the headquarters and guiding the internal trade between divisions \citep[e.g.,][]{anctil1999negotiated, baldenius1999negotiated}. %MM instrument for coordinating operational decisions (Internet) und L191pg.2 und L190pg.1 the internal trade of intermediate products 
		Internal transfers, especially in multi-stage production processes, are often performed under conditions of asymmetric information \citep{baldenius2000intrafirm}. %L157pg.1 and Paper1
		To induce effort and reduce the risk of moral hazard, divisional profit-based compensation schemes are often applied \citep[e.g.,][]{edlin1995specific, vaysman1998model}. %L170pg3 and MM
		
		The divisions' coordination problem increases when divisions can additionally make specific investments that enhance the value of internal trade. %Paper1
		The main problem with specific investments is that they are irreversible and are of little or no value in the divisions' external lines of business \citep{edlin1995specific}. 	%L170pg.3 und L178pg.23 and, especially, do not have an outside value.
		Since those investments are usually made under uncertainty, each division tends to underinvest. %L178pg5 and L190pg.1 
		%Due to the fact that those investments are sunk at the later date, when divisions make decisions over transfer price and quantity, each division tends to underinvest in the first place. %L170pg3 and L190pg.1 
		%Due to the fact that the divisions bear the full cost of their own investments, but are evaluated according to their profits, each division is prone to underinvestment. %L170pg3 und L190pg.1 is prone to underinvestment
		This problem is also known as an investment ``hold-up'' problem \citep{schmalenbach1908verrechnungspreise, williamson1979transaction, williamson1985economic}. %L191pg3
	
		While various methods of transfer pricing to mitigate the hold-up problem have been investigated extensively in the economic transfer pricing literature \citep[e.g.,][]{baldenius1999negotiated, edlin1995specific, hofmann2006verfugungsrechte, lengsfeld2004transfer, pfeiffer2011cost, wagner2008corporate}, it seems less clear how well the literature's recommendations actually work in practice.  %L190pg1
		Moreover, the solutions of transfer pricing problems are often based on game theory approaches, such as the concept of subgame perfect equilibrium. %Paper1
		Such concepts require demanding assumptions, e.g., of rational decision-making behavior or common knowledge, and, in practice, these assumptions are often not met \citep{axtell2007economic, simon1979rational}. %Paper1
		Assumptions on rationality are questionable in models that reflect human behavior and, therefore, most of such models quickly lose practical relevance \citep{young2001individual}. %Paper1
	
		Against this background, this paper focuses on negotiated transfer pricing and, specifically, addresses and extends the simulation model introduced in \cite{mitsch2023impact}. %L331pg2
		In particular, the simulation study analyzes a decentralized firm in which divisions produce highly specialized intermediate products. %MM
		 %a decentralized firm consisting of one headquarters and two divisions which are organized as profit centers. %MM and L178pg5
		%In particular, the study analyzes a decentralized firm consisting of one supplying division and two buying divisions which have private information about the commodity market and the outlet market, respectively. %MM and L267pg3
		The firm's headquarters applies the concept of profit centers to determine the profit for internal divisions of responsibility. %Paper1
		In addition, divisions can make upfront specific investments and have private information regarding their area of responsibility. %MM L178pg4  L178pg5 L267pg3  have private information about their markets.
		%The firm's headquarters cannot observe the 
		A linear divisional profit-based compensation scheme is applied in order to guarantee that the divisions act in the headquarters' interest. %Paper2
		%The findings of \textcolor{red}{Mitsch's} simulation study show that cognitively bounded individuals (modeled with fuzzy Q-learning agents) do not achieve the same results as fully rational utility maximizers do. %MM Paper1
		Furthermore, the study relaxes basic assumptions, e.g., the assumption of common knowledge of rationality for the divisions, and examines whether cognitively bounded individuals (modeled with fuzzy Q-learning agents) achieve the same results as fully rational utility maximizers. %Paper1
		%In addition, it is examined whether the division's performance can be improved in scenarios with non-symmetrical investment costs, when the headquarters assigns equal bargaining power to both divisions. %MM
		In addition, different constellations of bargaining power are examined to see whether a deviation from the recommended optimal bargaining power leads to a higher divisional performance. %MM
		The findings of \cites{mitsch2023impact} simulation study show that cognitively bounded individuals can achieve the same results as fully rational utility maximizers, but, in certain cases where divisional investment costs differ widely from each other, cognitively bounded individuals can generate even higher profits, if they obtain approximately the same level of bargaining power from the headquarters for their negotiation process. %MM Paper1
		%if the headquarters assigns them approximately the same bargaining power. %MM Paper1
		
		In this study, the number of divisions is increased by one. %MM
		%In this study, the number of divisions is increased by one, which increases the internal competitive pressure within the firm. %MM
		Concretely, there is one supplying division and two buying divisions. %MM
		As a consequence, two negotiations over transfer prices and quantities have to be carried out at the same time. %MM
		%Since the upfront investment decisions depend on the anticipated outcome of the negotiations, it more difficult for the headquarters to solve the divisions' coordination problem in a decentralized manner. %MM
		Since the upfront investment decisions depend on the anticipated outcome of the negotiations, it is more difficult for the headquarters to determine the optimal bargaining power for each division. %MM	
		%Since the upfront investment decisions depend on the anticipated outcome of the negotiations, it more difficult for the headquarters to calculate the subgame perfect equilibrium for the investment hold-up problem described above. %Internet and MM	
		%non-separability in the objective function; trifft vielleicht bei mir zu, aber da würde ich mich angreifbar machen.
		%Simultaneously, the investments made by the divisions cannot be assigned to the respective divisions and, therefore, have to be coordinated for all divisions.
		%Simultaneously, the investments made by the divisions influence the divisions' decision-making process. %MM
		Simultaneously, the investments made by the divisions cannot be individually separated out due to their nature. %Internet
		Hence, the management decision problem to be solved cannot be divided into two or more disjoint parts. %Internet
		
		This paper distinguishes between an ``all-knowing'' headquarters (serve as a benchmark for optimal decision-making behavior) and a headquarters that does not have all the information to find the optimal bargaining power for its divisions and, therefore, relies on reference values or simple rules to determine the bargaining power of each division. %MM
		%closed-form expressions are provided to determine the optimal bargaining power for each division. %MM
		%In addition, the study seeks to examine, under what circumstances would a firm achieve higher profits by not following the recommendations based on concepts of subgame perfect equilibria? %L331pg3 and L190pg1
		%The study also seeks to examine whether cognitively bounded agents achieve the same results as fully rational utility maximizers and, especially, whether the recommendations on managerial compensation arrangements and bargaining infrastructures are designed to maximize headquarters' profit. %Paper1
		%The study also seeks to examine whether cognitively bounded individuals achieve the same results as fully rational utility maximizers and, especially, whether the recommendations on managerial compensation arrangements and bargaining infrastructures maximize headquarters' profits under more realistic assumptions. %Paper1
		The first research question in this contribution is to examine whether cognitively bounded individuals achieve the same results as fully rational utility maximizers on the investment hold-up problem described above.
		%In addition, the study also seeks to examine whether the profit of a headquarters that does not know the exact optimal bargaining power is higher than the profit of a headquarters that applies the optimal bargaining power, if divisions have different investment costs. %MM
		In addition, the study also seeks to examine the impact of bargaining power on managerial performance. %MM
		
		For this purpose, this paper conducts an agent-based computer simulation with individual learning agents which are modeled by fuzzy Q-learning. %L311pg175
		The use of fuzzy Q-learning offers many advantages, including a high degree of heterogeneity with respect to the structure and the dynamic interactions between agents and their environment and, in particular, it is a feasible way to deal with the divisions' bounded rationality and cognitive limitations. %Paper1 and MM
		Moreover, an agent-based simulation also allows to observe the agents' behavior as well as the system's behavior on the macro-level in time, which otherwise cannot be derived in relation to a ``functional relationship'' from the individual behaviors of those agents \citep[e.g.,][]{epstein2006generative, wall2016agent}. %Paper1

		The remainder of this paper is organized as follows: In Sec. \ref{sec:The_Model}, the extended negotiated transfer pricing model is introduced and the resulting solutions are discussed. %Paper1
		Section \ref{sec:agent_based_simulation} introduces the simulation model with fuzzy Q-learning agents. %Paper1
		The parameter settings for the agent-based simulation are explained in Sec. \ref{sec:Parameter_Settings_and_Simulation_Setup} and, in Sec.  \ref{sec:Results_and_Discussion}, the simulation results are presented and discussed. %Paper1 
		Section \ref{sec:Conclusion} contains concluding remarks and suggests possible directions for future research. %Paper1



\newpage
\section{Negotiated transfer pricing model}
\label{sec:The_Model}

\subsection{Specification of the firm} %Organizational setting
\label{ssec:Organizational_Setting}

	In this paper, a decentralized firm is examined that consists of one headquarters and three divisions - one supplying division (the ``upstream'' division) and two buying divisions (the ``downstream'' divisions). %L170pg.4 two-divisional firm: L191pg.5  supplying division and the buying division: L190pg.4
	%It is assumed that both divisions are organized as profit centers, i.e., each division is evaluated on its own divisional profit, and that the headquarters delegates operating decisions to its divisions. %SS1191: are treated as profit centers  L200pg.3   are organized as profit centers L178pg.7
	Suppose that the supplying division purchases raw materials from a commodity market in order to produce a highly specialized intermediate product that, in particular, cannot be bought from an external market \citep[e.g.,][]{anctil1999negotiated, edlin1995specific, wagner2008corporate}.
	%Suppose that the supplying division purchases raw materials from a commodity market in order to manufacture an intermediate product which is transferred to the buying division.\footnote{
	%In negotiated transfer pricing models, it is commonly assumed that the intermediate product is highly specialized and, therefore, there is no external market for the intermediate product \citep[e.g.,][]{anctil1999negotiated, edlin1995specific, wagner2008corporate}. For the sake of simplicity, it is assumed that one unit of raw material is required to manufacture one unit of the intermediate product.} %L170pg.4 und L200pg.4 MM: raw materials from a commodity market Footnote: one unit L263pg.5
	On the downside, each buying division refines the intermediate products independently of each other in order to improve their quality.
	Lastly, the buying divisions sell the refined products on an outlet market separately. %L157pg.3 
	%Figure \ref{fig:Schematic_Representation} summarizes the essential components of our model. %MM
	Beyond that, all three divisions can additionally make specific investments that increase the value of internal trade. 
	However, these investment decisions have to be made before the negotiations over transfer prices and transfer quantities take place. 
	Figure \ref{fig:Schematic_Representation} schematically represents the negotiated transfer pricing model investigated here. %L331pg.6
	
	%As in the study of 
	%In line with the previous literature
	Corresponding to the well-known negotiated transfer pricing models by \cite{eccles1988price}, \cite{edlin1995specific}, \cite{gox2006economic}, \cite{pfeiffer2007rekonstruktion}, \cite{vaysman1998model}, and \cite{wagner2008corporate}, it is assumed that all three divisions are treated as profit centers. %Paper1 and MM
	Therefore, divisional performance evaluations are based on profits and each division is allowed to determine the amount of specific investments, the level of the transfer price, and the amount of intermediate products autonomously. %L324pg12 and Paper1
	 %is evaluated on its own divisional profit, and that the headquarters delegates to its divisions operating decisions like the amount of specific investment, the amount of intermediate products, and the level of the transfer price. % \citep[e.g.,][]{vaysman1998model, wagner2008corporate}. %MM	
	 %SS1191: are treated as profit centers  L200pg.3   are organized as profit centers L178pg.7
	%Fig. \ref{fig:Schematic_Representation} schematically represents the negotiated transfer pricing model investigated here. %L331pg.6
	%Please be aware that there is no standardized model in the transfer pricing literature. %MM
	%Therefore, the standard negotiated transfer pricing model investigated here and its solutions is based on the XXX, as well as an extensive comparison of the most common transfer pricing methods, are described in \cites{wagner2008corporate} PhD thesis in more detail.} % and there is also an extensive comparison to other transfer pricing methods. %MM
	
	\vspace{0mm}
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.9\textwidth]{Schematic_Representation.png}
		\caption{Schematic representation of the decentralized firm. In addition, the sequence of events within the negotiated transfer pricing model is illustrated. Source: Based on \cite{wagner2008corporate} and, further, modified here for the case of two buying divisions.}
		\label{fig:Schematic_Representation}
	\end{figure}
	%\vspace{-3mm}

	In the following, the supplying division, the $1$st buying division, the $2$nd buying division, and the headquarters are abbreviated to $S$, $1$, $2$, and $HQ$, respectively. %MM
	%Suppose that $q \in \mathbb{R}^+$ units of the intermediate product are transferred, %L170pg.4
	To keep the analysis simple and, especially to ensure, that the negotiated transfer pricing model has a unique subgame perfect equilibrium, it is assumed that the supplying division's costs of manufacturing $q_j \in \mathbb{R}^+$, $j \in \{ 1,2 \}$, units of the intermediate product are given by %the following expression %L157pg.4
	\begin{equation}
		C_S(q_1,q_2,\theta_S,I_S) = (\theta_S - I_S ) \cdot (q_1 + q_2) \;,  \label{eq:supplying_division_costs}	
	\end{equation}
	where $\theta_S \in \mathbb{R}^+$ is a state variable which reflects the purchase price of raw materials and $I_S \in \mathbb{R}^+$ stands for the amount of specific investment carried by the supplying division. %L170pg.4  borne: L191pg.5
	In contrast, each buying division's, $j \in \{ 1,2 \}$, net revenue is %L190pg.5   Similarly,
	\begin{equation}
		R_j(q_j,\theta_j,I_j) = (\theta_j - \frac{1}{2} \; b \; q_j + I_j) \; q_j \; , \label{eq:buying_division_net_revenue}
	\end{equation}  
	where $\theta_j \in \mathbb{R}^+$ is a state variable which represents the constant term in the inverse demand function for the selling product and $I_j \in \mathbb{R}^+$ denotes the amount of specific investment carried by the $j$th buying division. %\footnote{Eq. \ref{eq:supplying_division_costs} and \ref{eq:buying_division_net_revenue} are used in order to get a unique solution for investments and quantity.} %L170pg.4  borne: L191pg.5   Footnote: MM
	For the sake of simplicity, it is assumed that $b_1 = b_2 = b \in \mathbb{R}^+$, where $b$ describes the slope of the inverse demand function.
	%To keep the analysis simple and, especially, that the negotiated transfer pricing model has a unique subgame perfect equilibrium, it is assumed that the inverse demand function and the costs of production are linear and, for linguistic reasons, the term ``investment decision'' or just ``investment'' is used for $I_j$, $j \in  \{ S,B \}$, instead of ``amount of specific investment''. %MM
	
	%In this paper, we assume that the supplying division and the buying division have private information about their costs and revenues, respectively, which are embodied in the state variables $\theta_S$ and $\theta_B$.
	In the decentralized setting examined here, it is assumed that the supplying division has private information about purchase prices of raw materials on the commodity market, while the buying divisions have private information about selling prices of the refined products on the outlet market. %MM  Footnote: L170pg.4 
	Therefore, the supplying division and the $j$th buying division know the distribution of the state variable $\theta_S$ and $\theta_j$, respectively.
	In order to solve the transfer price problem analytically, it is required that all divisions have at least access to information about the expected values of the markets.
	For the sake of simplicity, it is assumed that the state variables $\theta_S$, $\theta_1$, and $\theta_2$ are stochastically independent random variables. %MM
	%In addition, our analysis assumes that the headquarters cannot obtain any information required to play an active role in determining the transfer price. %L157pg.3
	Apart from that, the analysis assumes that the headquarters cannot observe the state variables nor the undertaken investments; %%L190pg.4
	%it only delegates certain rights and obligations to its divisions. %L157pg.3
	%In addition, it is assumed that the headquarters cannot observe the state variables nor the undertaken investments.\footnote{Our analysis assumes that the headquarters cannot obtain any information required to play an active role in determining the transfer price. The headquarters only delegates certain rights and obligations to its divisions.} %L190pg.5 footenote: L157pg.3
	the headquarters only knows the expected values of the state variables.
	Furthermore, the headquarters' accounting system receives costs and revenues after the negotiation phase and, subsequently, the headquarters' profit is calculated by %L200pg.200 und MM
	\begin{equation}
		\Pi_{HQ}(q_1,q_2,\theta_S,\theta_1,\theta_2,I_S,I_1,I_2) = R_1 + R_2 - C_S - w_S - w_1 - w_2 \; .  \label{eq:headquarters_profit}
	\end{equation}
	%Investments by the supplying division and by the buying division cause divisional fixed costs $w_S(I_S)$ and $w_B(I_B)$, respectively, which have quadratic cost structures %L157pg.3
	The supplying division's investments as well as each buying division's investments cause divisional investment costs (or divisional capital expenditure) $w_S(I_S)$ and $w_j(I_j)$, respectively.
	As in the study of \cite{baldenius1999negotiated}, \cite{edlin1995specific}, \cite{hofmann2006verfugungsrechte}, \cite{pfeiffer2007rekonstruktion}, and \cite{wagner2008corporate}, the divisional investment costs have the following quadratic cost structure. %L157pg.3 und L264pg441 dort steht zwar nur w'_A=I_A, aber das bedeutet, dass w_A=1/2*I_A^2 sein muss (also quadratische Fixkosten)
	\begin{equation}
		w_j(I_j) = \frac{1}{2} \; \lambda_j \; I_j^2 \; \; \; \text{for } j \in \{ S,1,2\} \label{eq:divisional_investment_costs}
	\end{equation}
	The marginal cost parameter is denoted by $\lambda_j \in \mathbb{R}^+$ % of divisional fixed costs. %MM
	and the analysis assumes that the parameters $w_S$, $w_1$, $w_2$, $\lambda_S$, $\lambda_1$, $\lambda_2$, and $b$ are known to the headquarters and each division. %MM
	%For linguistic reasons, we use the term ``marginal costs parameter'' for $\lambda_j$. %MM 
	The term, one half, is only for reasons of expediency. %MM
	%Please notice that the supplying division's net revenue generated by the internal quantity transfer corresponds to the buying division's transfer costs, i.e., $R_S=C_B=TP \cdot q$, and, hence, they do not appear in the headquarters' profit $\Pi_{HQ}$. 
	%In of almost all transfer pricing models, the formulas are represented by the transfer quantity $q$; the transfer price $TP$ does not appear in any formula, but $TP$ can be calculated explicitly by using substitution. 	%Footnote: MM
	%In the transfer pricing model without negotiation over transfer price and transfer quantity, the sequence of events involves four dates (see Figure \ref{fig:Sequence_Of_Events_01}). %L157pg.3 und MM
	%In a nutshell, 
	
	The sequence of events within the negotiated transfer pricing model (see Fig. \ref{fig:Schematic_Representation}) can be summarized as follows: %L157pg.3 und MM
	at date one, all three divisions have to make an investment decision independently of each other. %MM
	Subsequently, the supplying division and each buying division independently observe the state variables $\theta_S$ and $\theta_j$, respectively. %MM L170pg.5
	%At date three, the amount of intermediate products is determined by the divisions and, finally, the headquarters' profit as well as the divisions' profits are realized. %MM
	At date three, the amount of intermediate products is determined by the divisions and, finally, profits are realized. %MM

	\subsection{First-best solution}
	\label{ss:First_Best_Solution}

		In order to provide a benchmark solution (first-best solution or ex post efficient solution), Eq. \ref{eq:headquarters_profit} is maximized with respect to investment and quantity. %L267pg9
		%From the view of the headquarters, the decisions over investments and quantity are taken in such a way that Eq. \ref{eq:headquarters_profit} is maximized. %MM
		Since investment and quantity are determined at different times, the analysis starts by backward induction on date three, on which the quantity is set.\footnote{ %L178pg.42
		%The solution of the two-stage decision problem is received by backward induction starting with the quantity decision on date three.\footnote{ %L178pg.42
		A step-by-step solution of the two-stage decision problem for $j=1$ (only one buying division) is presented, e.g., in \cites{wagner2008corporate} PhD thesis.} %Note that the common knowledge assumption is necessary to use backward induction.
		\begin{equation}
			(q_1^*,q_2^*) \in \underset{(q_1,q_2) \in \mathbb{R}_+^2}{\mathrm{arg\,max}} \; \Pi_{HQ}(q_1,q_2,\theta_S,\theta_1,\theta_2,I_S,I_1,I_2) %q^*(\theta_S,\theta_B,I_S,I_B) 
		\end{equation}
		In this paper, first-best solutions are indexed by a superscript $^*$. %L320pg.360
		%and, in order to guarantee that the first-best solutions are reached, divisions must be fully rational at each stage of the decision problem, i.e., at each stage, the action with the highest outcome must be selected. %MM
		%Since each division has access to all relevant information, backward induction can be applied to calculate the subgame perfect equilibrium. %https://en.wikipedia.org/wiki/Backward_induction
		On date three, the first order condition for maximizing the headquarters' profit $\Pi_{HQ}$ with respect to $q_j$ is equal to %L178pg.28
		\begin{equation}
			\frac{\partial \Pi_{HQ}}{\partial q_j} = \theta_j - b \; q_j + I_j - \theta_S + I_S = 0 
		\end{equation}
		and, hence, the profit maximizing quantity is given by
		\begin{equation}
			q_j^* = \frac{\theta_j - \theta_S + I_S + I_j}{b} \; , \text{ for } j \in \{ 1,2 \} \; . \label{eq:backward_q}
		\end{equation}
		%On date one, the investment decisions are simultaneously taken and, due to the fact, that these decisions are made when 
		On date one, the investment decisions are made under uncertainty and, therefore, the expected headquarters' profit $E[\Pi_{HQ}]$ is maximized with respect to $I_S$, $I_1$, and $I_2$. %MM
		\begin{equation}
			(I_S^*,I_1^*,I_2^*) \in \underset{(I_S,I_1,I_2) \in \mathbb{R}_+^3}{\mathrm{arg\,max}} \; E[\Pi_{HQ}(q_1,q_2,\theta_S,\theta_1,\theta_2,I_S,I_1,I_2)] 	\label{eq:backward_I}
		\end{equation}
		Differentiating the expected headquarters' profit with respect to investments yields %the optimal investments %L178pg.43
		\begin{eqnarray}
			I_S^* & = & \frac{E[q_1^*+q_2^*]}{\lambda_S} \; , \label{eq:backward_I_S} \\
			I_j^* & = & \frac{E[q_j^*]}{\lambda_j} \; , \text{ for } j \in \{ 1,2 \} \; . \label{eq:backward_I_j} %\label{eq:optimal_I_j}
		\end{eqnarray}
		Now, substituting Eq. \ref{eq:backward_q} into Eq. \ref{eq:backward_I_S} and \ref{eq:backward_I_j}, one gets the following first-best expected investments. %L191pg.17
		\begin{eqnarray}
			I_S^* & = & \frac{E[\theta_1 - \theta_S] \cdot (b \; \lambda_1 \; \lambda_2 - \lambda_1) + E[\theta_2 - \theta_S] \cdot (b \; \lambda_1 \; \lambda_2 - \lambda_2)}{b^2 \; \lambda_1 \; \lambda_2 \; \lambda_S - b \; ( \lambda_1 \; \lambda_S + \lambda_2 \; \lambda_S +2 \; \lambda_1 \; \lambda_2) + \lambda_1 + \lambda_2 + \lambda_S} \label{eq:optimal_I_S} \\
			I_1^* & = & \frac{E[\theta_1 - \theta_S] \cdot (b \; \lambda_2 \; \lambda_S - \lambda_2 - \lambda_S) + E[\theta_2 - \theta_S] \cdot \lambda_2}{b^2 \; \lambda_1 \; \lambda_2 \; \lambda_S - b \; ( \lambda_1 \; \lambda_S + \lambda_2 \; \lambda_S +2 \; \lambda_1 \; \lambda_2) + \lambda_1 + \lambda_2 + \lambda_S} \label{eq:optimal_I_1} \\
			I_2^* & = & \frac{E[\theta_2 - \theta_S] \cdot (b \; \lambda_1 \; \lambda_S - \lambda_1 - \lambda_S) + E[\theta_1 - \theta_S] \cdot \lambda_1}{b^2 \; \lambda_1 \; \lambda_2 \; \lambda_S - b \; ( \lambda_1 \; \lambda_S + \lambda_2 \; \lambda_S +2 \; \lambda_1 \; \lambda_2) + \lambda_1 + \lambda_2 + \lambda_S} \label{eq:optimal_I_2}
		\end{eqnarray}
		Finally, substituting Eq. \ref{eq:optimal_I_S} - \ref{eq:optimal_I_2} into Eq. \ref{eq:backward_q} implies the first-best expected quantities. %L191pg.17
		\begin{small}
		\begin{eqnarray}
			q_1^* & = & \frac{\theta_1-\theta_S}{b} + \frac{E[\theta_1-\theta_S] (b \lambda_1 \lambda_2 + b \lambda_2 \lambda_S - \lambda_1 - \lambda_2 - \lambda_S) + E[\theta_2-\theta_S] \; b \lambda_1 \lambda_2}{b (b^2 \; \lambda_1 \; \lambda_2 \; \lambda_S - b (\lambda_1 \; \lambda_S + \lambda_2 \; \lambda_S +2 \; \lambda_1 \; \lambda_2) + \lambda_1 + \lambda_2 + \lambda_S)} \label{eq:optimal_q_1} \\
			q_2^* & = & \frac{\theta_2-\theta_S}{b} + \frac{E[\theta_2-\theta_S] (b \lambda_1 \lambda_2 + b \lambda_1 \lambda_S - \lambda_1 - \lambda_2 - \lambda_S) + E[\theta_1-\theta_S] \; b \lambda_1 \lambda_2}{b (b^2 \; \lambda_1 \; \lambda_2 \; \lambda_S - b (\lambda_1 \; \lambda_S + \lambda_2 \; \lambda_S +2 \; \lambda_1 \; \lambda_2) + \lambda_1 + \lambda_2 + \lambda_S)} \label{eq:optimal_q_2}
		\end{eqnarray}
		\end{small}
		\hspace{-2.5mm} With the first-best solutions of the two-stage decision problem, the first-best expected profit is given by the following expression. %MM
		\begin{small}
		\begin{equation}
			\Pi_{HQ}^* \hspace{-0.5mm}=\hspace{-0.7mm} \frac{E[\theta_1 \hspace{-1mm}-\hspace{-0.7mm} \theta_S]^2 (b \; \lambda_1 \lambda_2 \lambda_S \hspace{-1mm}-\hspace{-0.7mm} \lambda_1 \lambda_S) \hspace{-0.5mm}+\hspace{-0.5mm} E[\theta_2\hspace{-1mm}-\hspace{-0.7mm}\theta_S]^2 (b \; \lambda_1 \lambda_2 \lambda_S \hspace{-1mm}-\hspace{-0.7mm} \lambda_2 \lambda_S) \hspace{-1mm}-\hspace{-0.7mm} E[\theta_1\hspace{-1mm}-\hspace{-0.7mm}\theta_2]^2 \lambda_1 \lambda_2}{2 \; (b^2 \; \lambda_1 \; \lambda_2 \; \lambda_S - b (\lambda_1 \; \lambda_S + \lambda_2 \; \lambda_S +2 \; \lambda_1 \; \lambda_2) + \lambda_1 + \lambda_2 + \lambda_S)} \label{eq:optimal_Profit_HQ}
			%\Pi_{HQ}^* = \frac{E[\theta_1-\theta_S]^2 (b \; \lambda_1 \lambda_2 \lambda_S - \lambda_1 \lambda_S) + E[\theta_2-\theta_S]^2 (b \; \lambda_1 \lambda_2 \lambda_S - \lambda_2 \lambda_S) - E[\theta_1-\theta_2]^2 \lambda_1 \lambda_2}{2 \; (b^2 \; \lambda_1 \; \lambda_2 \; \lambda_S - b (\lambda_1 \; \lambda_S + \lambda_2 \; \lambda_S +2 \; \lambda_1 \; \lambda_2) + \lambda_1 + \lambda_2 + \lambda_S)} \label{eq:optimal_Profit_HQ}
		\end{equation}
		\end{small}
		\hspace{-2.5mm} The headquarters' profit in Eq. \ref{eq:optimal_Profit_HQ} can be seen as a benchmark for the highest feasible profit that can be achieved, if investments and quantities are set according to Eq. \ref{eq:optimal_I_S} - \ref{eq:optimal_I_2} and Eq. \ref{eq:optimal_q_1} - \ref{eq:optimal_q_2}, respectively. %L178pg.8


	
	%\subsection{Negotiated Transfer Pricing}
	\subsection{Second-best solution}
	\label{sec:Second_Best_Solution}

		Assuming that each division acts in its own interest, the first-best solutions are usually not achieved. %MM
		However, the headquarters could provide the following linear divisional profit-based compensation schemes to ensure that the negotiations over transfer prices and quantities result in ex post efficient transfer prices and quantities.\footnote{ %L200pg12
		%In negotiated transfer pricing, the following linear divisional profit-based compensation schemes are often specified in order to ensure that the negotiation over transfer price and quantity leads to an ex post efficient transfer price and quantity. %L200pg12
		%In negotiated transfer pricing models, both divisions simultaneously undertake investments before the quantity negotiation takes place. %MM
		%Due to the fact that each division is free to decide any amount of investment and quantity, the headquarters has to provide an incentive compatibility for each division. %L191pg.7 und MM the headquarters is faced with a divisional incentive.
		%In order to ensure that both divisions report their state variables truthfully and the negotiation leads to an ex post efficient quantity, the following division incentives are commonly used.\footnote{
		In general, incentives for efficient investments do not necessarily provide incentives for an efficient quantity and verse versa. For instance, in full-cost transfer pricing models, both divisions are rewarded for efficient investments, but the negotiation does not lead to an ex post efficient quantity \citep[][]{baldenius1999negotiated}.} %L178pg.17 und MM Footnote: L178pg.5 und L178pg.23 aber ich zitiere L190pg.13
		%\vspace{-10mm}	
		\begin{eqnarray}
			\Pi_S(q_1,q_2,\theta_S,\theta_1,\theta_2,I_S,I_1,I_2,\Gamma_1,\Gamma_2) & = & \Gamma_1 \; M_1 + \Gamma_2 \; M_2 - w_S \label{eq:Profit_S} \\
			\Pi_1(q_1,\theta_S,\theta_1,I_S,I_1,\Gamma_1) & = & (1-\Gamma_1) \; M_1 - w_1 \label{eq:Profit_1} \\
			\Pi_2(q_2,\theta_S,\theta_2,I_S,I_2,\Gamma_2) & = & (1-\Gamma_2) \; M_2 - w_2 \label{eq:Profit_2} 
		\end{eqnarray}	
		$\Pi_S$, $\Pi_1$, and $\Pi_2$ denote the profit of the supplying division, the profit of the $1$st buying division, and the profit of the $2$nd buying division, respectively. %MM
		$M_j$ stands for the headquarters' contribution margin with regard to the internal trade between supplying division and the $j$th buying division, i.e., $M_j=(\theta_j -\frac{1}{2} \hspace{0.5mm} b \hspace{0.5mm} q_j + I_j) \hspace{0.5mm} q_j - (\theta_S-I_S) \hspace{0.5mm} q_j$, $j \in \{ 1,2 \}$. %L190pg.5 
		$\Gamma_j \in [0,1]$ represents the share of the contribution margin achieved between the supplying division and the $j$th buying division (also known as the $\Gamma$-surplus sharing rule \citep{edlin1995specific}, the supplying division's bargaining power \citep{baldenius1999negotiated}, or the surplus sharing parameter \citep{wielenberg2000negotiated}). %L190pg.5 
		%In almost all negotiated transfer pricing models, the negotiation process is modeled by a simple linear surplus sharing rule, i.e., the supplying division receives a share $\Gamma$ and the buying division a fraction $(1-\Gamma)$ of the headquarters' contribution margin. %L190pg.5 und L178pg.27
		%$\Gamma_S \in [0,1]$ and $\Gamma_B \in [0,1]$ represent the share of the achievable contribution margin for the supplying division and buying division, respectively. %L190pg.5
		%The parameter $\Gamma$ can be considered as the supplying division's bargaining power and, for $\Gamma=0.5$, the negotiation result is equivalent to the Nash bargaining solution of the standard Rubinstein bargaining game \citep[e.g.,][]{baldenius1999negotiated, rubinstein1982perfect}. %L190pg.6 und L178pg.27

		As in the preceding section, the headquarters can apply the concept of subgame perfect equilibrium. %MM
		By doing so, the analysis of negotiated transfer pricing starts by backward induction on date three. %L178pg.27 und MM
		%Since the headquarters delegates operation decisions to its divisions, the supplying division and the buying division seek to maximize their own divisional profits which are given by Eq. \ref{eq:Profit_S} and Eq. \ref{eq:Profit_B}, respectively. %MM
		Starting with the quantity decision on date three, one gets
		\begin{equation}
			q_j^{sb} = \frac{\theta_j - \theta_S + I_S + I_j}{b} \; , \text{ for } j \in \{ 1,2 \} \; . \label{eq:backward_q_sb}
		\end{equation}
		Note that differentiating the divisional profits with respect to $q_j$, $j \in \{ 1,2 \}$, leads to the same profit maximizing quantity given by Eq. \ref{eq:backward_q}, but the first order conditions for maximizing the expected divisional profits $E[\Pi_j]$ with respect to $I_j$, $j \in \{ S,1,2 \}$, lead to second-best investments (labelled by the superscript $sb$). %L178pg.28 und MM
		\begin{eqnarray}
			I_S^{sb} & = & \frac{\Gamma_1 \; E[q_1^{sb}] + \Gamma_2 \; E[q_2^{sb}]}{\lambda_S} \label{eq:second_best_I_j} \\
			I_j^{sb} & = & \frac{(1-\Gamma_j) \; E[q_j^{sb}]}{\lambda_j} \; , \text{ for } j \in \{ 1,2 \}
		\end{eqnarray}
		Since the headquarters has to specify how the contribution margins are shared among the three divisions, the headquarters has to solve the following additional decision problem before the decision-making process for the divisions takes place. %MM is confronted
		\begin{equation}
			(\Gamma_1^{sb},\Gamma_2^{sb}) \in \underset{(\Gamma_1,\Gamma_2) \in [0,1]^2}{\mathrm{arg\,max}} \; E[\Pi_{HQ}(q_1,q_2,\theta_S,\theta_1,\theta_2,I_S,I_1,I_2,\Gamma_1,\Gamma_2)] %q^*(\theta_S,\theta_B,I_S,I_B) 
		\end{equation}
		

		\afterpage{
		%\clearpage
		\begin{landscape}
		\hspace{-5.3mm} Differentiating the expected headquarters' profit with respect to the surplus sharing parameter $\Gamma_j$, for $j \in \{ 1,2 \}$, %yields the optimal investments %L178pg.43
		%\begin{equation}
		%	\frac{\partial E[\Pi_{HQ}]}{\partial \Gamma} = \Bigg[ \big( E[\theta_B]-\frac{1}{2} \; b \; q^{sb} + I^{sb} \big) \; q^{sb} - \big( E[\theta_S] - I^{sb} \big) \; q^{sb} -\frac{1}{2} \; \lambda_S \; {I_S^{sb}}^2 -\frac{1}{2} \; \lambda_B \; {I_B^{sb}}^2 \Bigg]_{\Gamma} = 0
		%\end{equation}
		\begin{small}
		\begin{equation}
			\frac{\partial}{\partial \Gamma_j} \Big( \big( E[\theta_1]-\frac{1}{2} b q_1^{sb} + I_1^{sb} \big) q_1^{sb} - \big( E[\theta_S] - I_S^{sb} \big) q_1^{sb}  + \big( E[\theta_2]-\frac{1}{2} b q_2^{sb} + I_2^{sb} \big) q_2^{sb} - \big( E[\theta_S] - I_S^{sb} \big) q_2^{sb}  -  \frac{1}{2} \lambda_S {I_S^{sb}}^2 - \frac{1}{2} \lambda_1 {I_1^{sb}}^2 - \frac{1}{2} \lambda_2 {I_2^{sb}}^2 \Big)
		\end{equation}
		\end{small}
		and setting that equation to zero, yields to the following two expressions.
		\begin{small}		
		\begin{eqnarray}
			\Gamma_1^{sb} = \frac{E[\theta_1\hspace{-1mm}-\hspace{-0.7mm}\theta_S] \lambda_S(b \lambda_1 (\lambda_1+\lambda_2-b \lambda_1 \lambda_2) - \lambda_1)   +   E[\theta_2\hspace{-1mm}-\hspace{-0.7mm}\theta_S] \lambda_S(b \lambda_1 \lambda_2 (2-b \lambda_1)-\lambda_2)}{E[\theta_1\hspace{-1mm}-\hspace{-0.7mm}\theta_S]\lambda_1 \lambda_S (b(\lambda_1\hspace{-1mm}+\hspace{-0.7mm}4\lambda_2\hspace{-1mm}+\hspace{-0.7mm}\lambda_S) \hspace{-1mm}-\hspace{-0.7mm} b^2 \lambda_2(\lambda_1\hspace{-1mm}+\hspace{-0.7mm}\lambda_2\hspace{-1mm}+\hspace{-0.7mm}\lambda_S)\hspace{-1mm}-\hspace{-0.7mm}3)   +   E[\theta_2\hspace{-1mm}-\hspace{-0.7mm}\theta_S] \lambda_2 \lambda_S(b \lambda_1\hspace{-1mm}-\hspace{-0.7mm}1)   +   E[\theta_1\hspace{-1mm}-\hspace{-0.7mm}\theta_2] \lambda_1 \lambda_2(b \lambda_1 \hspace{-1mm}+\hspace{-0.7mm} b \lambda_2 \hspace{-1mm}-\hspace{-0.7mm}2)} \label{eq:Gamma_1_sb} \\
			\Gamma_2^{sb} = \frac{E[\theta_2\hspace{-1mm}-\hspace{-0.7mm}\theta_S] \lambda_S(b \lambda_2 (\lambda_1+\lambda_2-b \lambda_1 \lambda_2) - \lambda_2)   +   E[\theta_1\hspace{-1mm}-\hspace{-0.7mm}\theta_S] \lambda_S(b \lambda_1 \lambda_2 (2-b \lambda_2)-\lambda_1)}{E[\theta_2\hspace{-1mm}-\hspace{-0.7mm}\theta_S]\lambda_2 \lambda_S (b(\lambda_2\hspace{-1mm}+\hspace{-0.7mm}4\lambda_1\hspace{-1mm}+\hspace{-0.7mm}\lambda_S) \hspace{-1mm}-\hspace{-0.7mm} b^2 \lambda_1(\lambda_1\hspace{-1mm}+\hspace{-0.7mm}\lambda_2\hspace{-1mm}+\hspace{-0.7mm}\lambda_S)\hspace{-1mm}-\hspace{-0.7mm}3)   +   E[\theta_1\hspace{-1mm}-\hspace{-0.7mm}\theta_S] \lambda_1 \lambda_S(b \lambda_2\hspace{-1mm}-\hspace{-0.7mm}1)   +   E[\theta_2\hspace{-1mm}-\hspace{-0.7mm}\theta_1] \lambda_1 \lambda_2(b \lambda_1 \hspace{-1mm}+\hspace{-0.7mm} b \lambda_2 \hspace{-1mm}-\hspace{-0.7mm}2)} \label{eq:Gamma_2_sb}
		\end{eqnarray}
		\end{small}
		By using substitution as in Sec. \ref{ss:First_Best_Solution}, one obtains the following second-best solutions.  %MM D2pg.325
		%\footnote{$\Gamma$ can be expressed as $(b \; \lambda_B -1) /(b \; (\lambda_S+\lambda_B)-2)$ L264pg.435!!!.} %MM D2pg.325
		\begin{small}		
		\begin{eqnarray}
			I_S^{sb} & = & \frac{(\Gamma_1 E[\theta_1-\theta_S] + \Gamma_2 E[\theta_2-\theta_S]) b \lambda_1 \lambda_2   -   E[\theta_1-\theta_S] \Gamma_1 (1-\Gamma_2) \lambda_1   -   E[\theta_2-\theta_S] (1-\Gamma_1)\Gamma_2 \lambda_2}{b^2 \lambda_1 \lambda_2 \lambda_S - b ((1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1) \lambda_2 \lambda_S + (1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2) \lambda_1 \lambda_S + (\Gamma_1\hspace{-1mm}+\hspace{-0.7mm}\Gamma_2) \lambda_1 \lambda_2)   +   (1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1)(1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2) \lambda_S + \Gamma_1(1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2) \lambda_1   +   (1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1)\Gamma_2 \lambda_2} \label{eq:second_best_I_S} \\
			I_1^{sb} & = & \frac{E[\theta_1-\theta_S] (1-\Gamma_1) b \lambda_2 \lambda_S   -   E[\theta_1-\theta_S](1-\Gamma_1)(1-\Gamma_2) \lambda_S   -   E[\theta_1-\theta_2] (1-\Gamma_1)\Gamma_2 \lambda_2}{b^2 \lambda_1 \lambda_2 \lambda_S - b ((1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1) \lambda_2 \lambda_S + (1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2) \lambda_1 \lambda_S + (\Gamma_1\hspace{-1mm}+\hspace{-0.7mm}\Gamma_2) \lambda_1 \lambda_2)   +   (1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1)(1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2) \lambda_S + \Gamma_1(1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2) \lambda_1   +   (1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1)\Gamma_2 \lambda_2} \label{eq:second_best_I_B1} \\
			I_2^{sb} & = & \frac{E[\theta_2-\theta_S] (1-\Gamma_2) b \lambda_1 \lambda_S   -   E[\theta_2-\theta_S](1-\Gamma_1)(1-\Gamma_2) \lambda_S   -   E[\theta_2-\theta_1] \Gamma_1 (1-\Gamma_2) \lambda_1}{b^2 \lambda_1 \lambda_2 \lambda_S - b ((1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1) \lambda_2 \lambda_S + (1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2) \lambda_1 \lambda_S + (\Gamma_1\hspace{-1mm}+\hspace{-0.7mm}\Gamma_2) \lambda_1 \lambda_2)   +   (1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1)(1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2) \lambda_S + \Gamma_1(1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2) \lambda_1   +   (1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1)\Gamma_2 \lambda_2} \label{eq:second_best_I_B2} \\
			q_1^{sb} & = & \frac{\theta_1\hspace{-1mm}-\hspace{-0.7mm}\theta_S}{b} \hspace{-1mm}+\hspace{-0.7mm} \frac{E[\theta_1\hspace{-1mm}-\hspace{-0.7mm}\theta_S] \big( b \lambda_2 ((1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1)\lambda_S \hspace{-1mm}+\hspace{-0.7mm} \Gamma_1 \lambda_1) \hspace{-1mm}-\hspace{-0.7mm} \Gamma_1 (1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2) \lambda_1 \hspace{-1mm}-\hspace{-0.7mm}(1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1)\Gamma_2 \lambda_2 \hspace{-1mm}-\hspace{-0.7mm}(1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1)(1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2) \lambda_S \big)   \hspace{-1mm}+\hspace{-0.7mm}   E[\theta_2\hspace{-1mm}-\hspace{-0.7mm}\theta_S] \Gamma_2 b \lambda_1 \lambda_2}{b \big( b^2 \lambda_1 \lambda_2 \lambda_S \hspace{-1mm}-\hspace{-0.7mm} b ((1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1) \lambda_2 \lambda_S \hspace{-1mm}+\hspace{-0.7mm} (1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2) \lambda_1 \lambda_S \hspace{-1mm}+\hspace{-0.7mm} (\Gamma_1\hspace{-1mm}+\hspace{-0.7mm}\Gamma_2) \lambda_1 \lambda_2)   \hspace{-1mm}+\hspace{-0.7mm}   (1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1)(1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2) \lambda_S \hspace{-1mm}+\hspace{-0.7mm} \Gamma_1(1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2) \lambda_1   \hspace{-1mm}+\hspace{-0.7mm}   (1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1)\Gamma_2 \lambda_2 \big)} \label{eq:second_best_q_1} \\
			q_2^{sb} & = & \frac{\theta_2\hspace{-1mm}-\hspace{-0.7mm}\theta_S}{b} \hspace{-1mm}+\hspace{-0.7mm} \frac{E[\theta_2\hspace{-1mm}-\hspace{-0.7mm}\theta_S] \big( b \lambda_1 ((1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2)\lambda_S \hspace{-1mm}+\hspace{-0.7mm} \Gamma_2 \lambda_2) \hspace{-1mm}-\hspace{-0.7mm} (1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1) \Gamma_2 \lambda_2 \hspace{-1mm}-\hspace{-0.7mm}(1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2)\Gamma_1 \lambda_1 \hspace{-1mm}-\hspace{-0.7mm}(1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1)(1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2) \lambda_S \big)   \hspace{-1mm}+\hspace{-0.7mm}   E[\theta_1\hspace{-1mm}-\hspace{-0.7mm}\theta_S] \Gamma_1 b \lambda_1 \lambda_2}{b \big( b^2 \lambda_1 \lambda_2 \lambda_S \hspace{-1mm}-\hspace{-0.7mm} b ((1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1) \lambda_2 \lambda_S \hspace{-1mm}+\hspace{-0.7mm} (1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2) \lambda_1 \lambda_S \hspace{-1mm}+\hspace{-0.7mm} (\Gamma_1\hspace{-1mm}+\hspace{-0.7mm}\Gamma_2) \lambda_1 \lambda_2)   \hspace{-1mm}+\hspace{-0.7mm}   (1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1)(1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2) \lambda_S \hspace{-1mm}+\hspace{-0.7mm} \Gamma_1(1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_2) \lambda_1   \hspace{-1mm}+\hspace{-0.7mm}   (1\hspace{-1mm}-\hspace{-0.7mm}\Gamma_1)\Gamma_2 \lambda_2 \big)} \label{eq:second_best_q_2} 
			%\Pi_{HQ}^{sb} & = & \frac{1}{2} \; \frac{\lambda_S \; \lambda_B \; (b \; ( \lambda_S + \lambda_B)-1) \; E[\theta_B - \theta_S]^2}{(b \; \lambda_S -1) \; (b \; \lambda_B -1) \; (\lambda_S + \lambda_B)} + \frac{Var[\theta_B - \theta_S]}{2b} \label{eq:second_best_HQ}	
		\end{eqnarray}
		\end{small}
		\begin{small}		
		\begin{eqnarray}
			\Pi_{HQ}^{sb} & = & E[\theta_1-\theta_S]^2 \big( b^2(\lambda_1 \lambda_2 \lambda_S^2 + \lambda_1 \lambda_2^2 \lambda_S + \lambda_1^2 \lambda_2 \lambda_S) - b(\lambda_1 \lambda_S^2 + 3 \lambda_1 \lambda_2 \lambda_S + \lambda_1^2 \lambda_S) + 2 \lambda_1 \lambda_S) \big)   +  \nonumber \\
			& & E[\theta_2-\theta_S]^2 \big( b^2(\lambda_1 \lambda_2 \lambda_S^2 + \lambda_1 \lambda_2^2 \lambda_S + \lambda_1^2 \lambda_2 \lambda_S) - b(\lambda_2 \lambda_S^2 + 3 \lambda_1 \lambda_2 \lambda_S + \lambda_2^2 \lambda_S) + 2 \lambda_2 \lambda_S) \big)   -  \nonumber \\ \label{eq:second_best_HQ}
			& & E[\theta_1-\theta_2]^2 \big( b(\lambda_1 \lambda_2^2 + \lambda_1^2 \lambda_2) - 2 \lambda_1 \lambda_2 \big) / \\
			& & 2\big( b^3 (\lambda_1 \lambda_2 \lambda_S^2 + \lambda_1 \lambda_2^2 \lambda_S + \lambda_1^2 \lambda_2 \lambda_S) -b^2(\lambda_1 + \lambda_2 + \lambda_S)(\lambda_1 \lambda_S + 2\lambda_1 \lambda_2 +\lambda_2 \lambda_S) +  \nonumber \\
			& & b((\lambda_1 + \lambda_2 + \lambda_S)^2 +4 \lambda_1 \lambda_2 +\lambda_1 \lambda_S + \lambda_2 \lambda_S) -2(\lambda_1 + \lambda_2 + \lambda_S) \big) \nonumber
		\end{eqnarray}
		\end{small}
		
		\end{landscape}
		}



\newpage		
		%According to Eq. \ref{eq:Gamma_sb}, the headquarters assigns higher bargaining power to the division that has a lower marginal cost parameter. % of divisional fixed costs. %MM
		\hspace{-6.5mm} Note, if $E[\theta_1]=E[\theta_2]$, $\lambda_S=1$, and $\lambda_1=\lambda_2=0.5$, then each surplus sharing parameter $\Gamma_j$ is one half, which means that the contribution margin $M_j$ achieved is divided between the supplying division and the $j$th buying division in equal shares. %MM
		But independently of how the headquarters determines $\Gamma_j$, each division tends to underinvest, since each division has to bear the investment costs on its own. %L178pg.28 and MM
		As a consequence, the first-best quantities resulting from the negotiation process cannot be reached either, which implies that the resulting headquarters' profit $\Pi_{HQ}^{sb}$ is smaller than the headquarters' profit in the first-best case $\Pi_{HQ}^*$. %L178pg.2
		%Regardless of the headquarters' decision on $\Gamma$, Eq. \ref{eq:second_best_I_S} -  \ref{eq:second_best_I_B2} indicate that both divisions are prone to underinvesting and, hence, the first-best quantity resulting from the negotiation process cannot be reached either. %L178pg.28 und MM
		%Fehler!!! Diese Aussage stimmt leider nicht!!! Comparing Eq. \ref{eq:optimal_I_j} with Eq. \ref{eq:second_best_I_j}, the supplying division chooses the first-best investment, if and only if the surplus parameter $\Gamma$ is one, while the buying division chooses the first-best investment, if and only if $\Gamma$ is zero. %L178pg.28 und MM
		%Baldenius 2000 recommends that the division that has ``more'' private information ...
		%Nach Ergebnis: L157pg.16: we find that bargaining power should be conferred to the division that has more private information in order to reduce trade distortions.
		%As outlined in the first-best case, the headquarters' profit depends not only on $\lambda_j$, $j \in \{ S,B \}$, but also on the expected values of the state variables and benefits, in particular, from the volatility of the markets.%\footnote{
		%Please be aware that the variance of the difference between two stochastically independent random variables is given by the sum of their variances. In general it holds: $Var[\theta_B \pm \theta_S] = Var[\theta_B] + Var[\theta_S] \pm 2Cov[\theta_B,\theta_S]$.} %MM and https://stats.stackexchange.com/questions/142745/what-is-the-demonstration-of-the-variance-of-the-difference-of-two-dependent-var
		
		A headquarters that has all the information required to solve the $\Gamma$-choice problem and knows exactly, how its divisions will operate, would choose $\Gamma_1^{sb}$ and $\Gamma_2^{sb}$ according to Eq. \ref{eq:Gamma_1_sb} and Eq. \ref{eq:Gamma_2_sb}, respectively. %MM
		But how could a headquarters anticipate how its divisions will respond in the future.
		%A rule of thumb or alternative approaches, that are easier to implement for a headquarters with a lack of knowledge, are therefore needed to determine the share of the contribution margin. %MM
		A headquarters with a lack of knowledge therefore needs a rule of thumb or alternative approaches that are easier to implement in order to determine the optimal share of the contribution margin achieved between divisions. %MM		
		
	%Maximizing $\Pi_{HQ}$ with respect to $q$ yields %L178pg.37

	%L178pg.31: the seller's and the buyer's investment decisions maximize the following objective functions

	%L170pg.5: At date $3$, the two parties have complete information.... 

	%\begin{figure}[h!]
	%\includegraphics[width=1\textwidth]{Sequence_Of_Events_02.png}
	%\vspace{-6mm}
	%\caption{Sequence of events.} \label{fig:Sequence_Of_Events_02}
	%\end{figure}

	%L191pg.8: At that point....

	%L157pg.5 The seller should optimally untertake....

	%From the perspective of the supplying division, ... %L178pg.12

	%Due to the fact that analytical results are nearly impossible to obtain, an agent-based simulation is applied. %L58 .pg.3276
	%In this paper, the learning agents act in an environment which is an ``agentized version" of the negotiated transfer pricing model.\footnote{Agentized means that certain assumptions of the negotiated transfer pricing model are relaxed, e.g., the assumption that all agents are informed about competitors' actions (e.g., \citep{Guerrero_Axtell_2011,Leitner_Behrens_2015}).} %PP

	%Einbauen hier: individual learning behavior

	%Even though incentive theory has been developed under the standard assumption that all players are rational, it can take into account whatever bounded rationality assumption one may wish to choose.



\section{Simulation model with fuzzy Q-learning agents}
\label{sec:agent_based_simulation} %Bezeichnung noch im Text ändern

	\subsection{Overview of the simulation model}
	\label{ssec:Relaxed_Assumptions}

		The negotiated transfer pricing model described in the preceding section expects that all three divisions make decisions by means of their anticipated information. %MM
		However, in decentralized business organizations, it is common that not all parties have access to information which are required to make optimal decisions. %MM
		Therefore the common knowledge assumption is widely relaxed in the simulation model (see Tab. \ref{tab:Overview} for the most important adaptations regarding the common knowledge assumption and, correspondingly, how the decision variables are determined). %MM how the decision-making behavior of the agents changes
		Since the headquarters applies incentive compatibilities for ex post efficient quantities (cf. Eq. \ref{eq:Profit_S} - \ref{eq:Profit_2}), all three divisions have an incentive to truthfully share their private information with each other during the negotiation process. %MM
		In the agentized version of the negotiated transfer pricing model, the headquarters' profit function $\Pi_{HQ}$ is known to each division, but the division's profit function $\Pi_j$ as well as the expected value of the state variable $E[\theta_j]$, for $j \in \{ S,1,2 \}$, remain private for each division. % for the entire duration of the simulation. %MM
		
		\afterpage{
		%\clearpage
		\begin{landscape}
		\begin{table}[h!]
		\linespread{1.1}\selectfont
		\centering
		%\caption{Overview of }
		\caption{Comparison between negotiated transfer pricing with fully rational agents and the agent-based variant with cognitively bounded agents.}
		\label{tab:Overview}
		\begin{tabularx}{186mm}{|c|l|l|l|}
		%\begin{tabular}{19cm}{|c|l|l|l|}
			%\hline	
			%\multicolumn{9}{|l|}{\textbf{First-best solutions}} \\
			\hline
			\multirow{2}{*}{Parameter} & \multirow{2}{*}{Description} & Negotiated transfer pricing  & Agent-based variant of negotiated transfer pricing \\
			& & with fully rational agents (see Sec. \ref{sec:The_Model}) & with cognitively bounded agents (see Sec. \ref{sec:agent_based_simulation}) \\
			\hline
			\rule{0pt}{9pt} $\Pi_{HQ}$ & headquarters' profit & common knowledge & common knowledge \\
			\rule{0pt}{12pt} $\Pi_j$ & division's profit & common knowledge & private information for entire duration \\
			\rule{0pt}{12pt} $E[\theta_j]$ & expected value of state variable & common knowledge & private information for entire duration \\
			%$C_j$, $R_j$, $w_j$ & \multirow{2}{*}{common knowledge} & \multirow{2}{*}{private information until the negotiation} \\ %are known to everyone
			%$\lambda_j$, $b$ & & \\
			\rule{0pt}{12pt} $C_j$ & division's costs & common knowledge & private information until negotiation \\
			\rule{0pt}{12pt} $R_j$ & division's net revenue & common knowledge & private information until negotiation \\
			\rule{0pt}{12pt} $\lambda_j$ & division's marginal cost & common knowledge & private information until negotiation \\
			\rule{0pt}{12pt} $w_j$ & division's investment costs & common knowledge & private information until negotiation \\
			\rule{0pt}{12pt} $b$ & slope of the inv. demand func. & common knowledge & private information until negotiation \\
			\hline
			\rule{0pt}{9pt} $\Gamma_j$ & surplus sharing parameter & is set optimally & is a scenario-based exogenous parameter, which \\
			& & & varies in small steps \\
			\rule{0pt}{12pt} $I_j$ & amount of specific investment & is set optimally given $\Gamma_j$ & is chosen by an exploration policy, which mainly \\
			& & & depends on the learned Q-function \\
			\rule{0pt}{12pt} $\theta_j$ & state variable & private information until negotiation & private information until negotiation \\
			\rule{0pt}{12pt} $q_j$ & quantity & is set optimally given $I_j$ and $\theta_j$ & is set optimally given $I_j$ and $\theta_j$ \\
			\hline
		%\end{tabular}
		\end{tabularx}
		\linespread{1}\selectfont
		\end{table}
		\end{landscape}
		}
		
		In the agent-based simulation, the choice of different $\Gamma_j$ values, $j \in \{ 1,2\}$, is examined. %MM
		$\Gamma_j$ is a scenario-based exogenous parameter, which is varied in small steps (see Tab. \ref{tab:Parameter_settings_and_scenario_overview}), in order to investigate whether $\Gamma_j^{sb}$ resulting from a headquarters which has access to all required information leads to higher profit-effectiveness than other $\Gamma_j$ constellations taken from a headquarters which cannot solve the three-stage decision problem analytically due to a lack of knowledge and uses, e.g, a fifty-fifty surplus sharing rule. %MM
		In addition, the study assumes that each division has no beliefs about the rationality of other divisions nor how other divisions face their maximization problem. %MM
		Therefore, the divisions cannot apply the concept of subgame perfect equilibrium as in Sec. \ref{sec:The_Model} to solve the multi-stage decision-making process and, in particular, determine the optimal values for $I_j$ according to Eq. \ref{eq:second_best_I_S} - \ref{eq:second_best_I_B2}.  %MM
		%Thus, the agents have first to learn which amount of investment leads to which consequence and, by doing so, the amount of specific investment $I_j$ is chosen by an exploration policy, which is discussed in Sec. XX in more detail. %MM
		Thus, each division has first to learn which level of investment leads to which consequence and, in doing so, the study assumes that the divisions behave like fuzzy Q-learning agents. %MM the amount of specific investment $I_j$ is chosen by an exploration policy, which is discussed in Sec. XX in more detail. %MM
		Given that the divisions no longer instantaneously see the consequences of their actions, an exploration policy for the amount of specific investment $I_j$ has to be chosen, which is discussed in Sec. \ref{ss:Exploration_Polic} in more detail. %L331pg6 and MM
		%Given that the divisions no longer instantaneously see the consequences of their actions, the amount of specific investment $I_j$ is chosen by an exploration policy, which is discussed in Sec. XX in more detail. %L331pg6 and MM
		
		%The use of fuzzy Q-learning leads to a situation where 
		 
		Since the simulation study assumes that all divisions have no prior knowledge about the consequences of their decisions, the sequence of events within the negotiated transfer pricing model is run through several times. %MM nor about opponent's decision-making behavior
		This additional time dimension is characterized by a time subscript $t \in \mathbb{N}$, which describes the ``inner loop'' of the simulation.
		A flow diagram of the agent-based simulation is given in Fig. \ref{fig:Procedure} in Appendix A. %MM 
		
		%Recall that, in the organizational setting investigated here,
		
		%Recall that, in the organizational setting investigated here, the headquarters determines the incentives for the divisions in such a way that the negotiation over transfer price and quantity at date three leads to an ex post efficient quantity (cf. Eq. \ref{eq:backward_q_sb}). %MM
		%Consequently, the two divisions are only faced with the investment decision at date one. % (cf. Fig. \ref{fig:Schematic_Representation}). %MM
		%Therefore, the agents have to learn which amount of investment leads to which consequence. %MM
		%Since the simulation study assumes that the agents have no prior knowledge about their consequences nor about opponent's decision-making behavior, the sequence of events within the negotiated transfer pricing model is run through several times. %MM
		%In order to indicate this additional time dimension, a time subscript $t \in \mathbb{N}$ for the ``inner loop'' of the simulation is added to the model (a flow diagram of the agent-based simulation is given in Sec. \ref{sec:Parameter_Settings_and_Simulation_Setup}, Fig. \ref{fig:Procedure}). %MM 
		
		%Hence, the common knowledge assumption is required to determine the optimal values of the decision variables $\Gamma$, $I_j$, and $q$, $j \in \{ S,B\}$. %MM
		%In the agentized version of the negotiated transfer pricing model, the divisions know neither the profit function $\Pi_j$ nor the expected value of the state variable $E[\theta_j]$ of the other division. %MM
		%In addition, both divisions only share those parameters that are necessary for an efficient quantity decision in their negotiation phase at date three. %MM
		%Table \ref{tab:Overview} shows the parameters of negotiated transfer pricing presented in Sec. \ref{sec:The_Model} and summarizes the most important changes associated with the agent-based variant of negotiated transfer pricing with cognitively bounded agents.
		%It should be emphasized once again that, in this paper, cognitively bounded agents are modeled by fuzzy Q-learning agents, which are introduced in Sec. \ref{sec:The_learning_method}. %MM
		
		
		
		
		
		%Recall that the negotiated transfer pricing model \citep[e.g.,][]{edlin1995specific, fudenberg1991perfect, vaysman1998model} requires fully individual rational utility maximizers. %MM
		%As mentioned in the introduction, such assumptions are rather heroic. %L331pg.2
		%Therefore, this paper investigates negotiated transfer pricing with divisions that are cognitively bounded, e.g., in terms of their rationality. %MM
		%Therefore, this paper investigates negotiated transfer pricing with divisions which (1) have no beliefs about the rationality of other divisions nor how other divisions face their maximization problem, (2) have no common knowledge of the opponent's utility function, and (3) are subject to cognitive limitations such as limits on the capacity of human memory, restrictions on the speed of learning, and constraints in foresight. %L170pg.2-3 L308pg.2: Against this background (1) XXX (2) https://policonomics.com/lp-game-theory1-common-knowledge/ (3) MM
		%In order to implement these relaxations of key assumptions, an agent-based simulation is set up. %MM realize oder implement 
		
		%The agentized version of the negotiated transfer pricing model presented in the previous section assumes that the divisions can make investment decisions by means of their learned information and, after the negotiation phase, they observe the outcome of their actions. %MM
		%The negotiated transfer pricing model presented in the previous section assumes that the divisions make decisions by means of their anticipated information. %MM
		%Hence, the common knowledge assumption is required to determine the optimal values of the decision variables $\Gamma$, $I_j$, and $q$, $j \in \{ S,B\}$. %MM
		%In the agentized version of the negotiated transfer pricing model, the divisions know neither the profit function $\Pi_j$ nor the expected value of the state variable $E[\theta_j]$ of the other division. %MM
		%In addition, both divisions only share those parameters that are necessary for an efficient quantity decision in their negotiation phase at date three. %MM
		%Table \ref{tab:Overview} shows the parameters of negotiated transfer pricing presented in Sec. \ref{sec:The_Model} and summarizes the most important changes associated with the agent-based variant of negotiated transfer pricing with cognitively bounded agents.
		%It should be emphasized once again that, in this paper, cognitively bounded agents are modeled by fuzzy Q-learning agents, which are introduced in Sec. \ref{sec:The_learning_method}. %MM
		
		
		
		%\subsection{What the agents have to learn}		
		
		%In the agent-based variant, both divisions have first to learn how their environment works in which they are and, in order to do so, they have the ability to store their individually learned information. %L331pg6
		%In the agent-based simulation variant, the divisions have first to learn about their environment in which they are located and, in order to do so, they have the ability to store their individually learned information. %L331pg6
		%For this purpose, the divisions are able to process a manageable number of states (concretely, $25$ states are used in the simulation study to describe the agent's memory; the parameter settings for the simulations are discussed in Sec. \ref{sec:Parameter_Settings_and_Simulation_Setup} in more detail). %MM
		%Moreover, the divisions have a learning rate which indicates how much new information overrides old information and both divisions have a discount factor which represents the agent's foresight, i.e., how important future rewards are to the agent. %MM
		%While the learning rate is fixed in the simulation study, the discount factor is a scenario-based exogenous parameter which varies in small steps. %MM
		%In addition to varying the discount factor, three different exploration policies are examined, as the agents no longer instantaneously see the consequences of their actions. %L331pg6
		
		%Recall that, in the organizational setting investigated here, the headquarters determines the incentives for the divisions in such a way that the negotiation over transfer price and quantity at date three leads to an ex post efficient quantity (cf. Eq. \ref{eq:backward_q_sb}). %MM
		%Consequently, the two divisions are only faced with the investment decision at date one. % (cf. Fig. \ref{fig:Schematic_Representation}). %MM
		%Therefore, the agents have to learn which amount of investment leads to which consequence. %MM
		%Since the simulation study assumes that the agents have no prior knowledge about their consequences nor about opponent's decision-making behavior, the sequence of events within the negotiated transfer pricing model is run through several times. %MM
		%In order to indicate this additional time dimension, a time subscript $t \in \mathbb{N}$ for the ``inner loop'' of the simulation is added to the model (a flow diagram of the agent-based simulation is given in Sec. \ref{sec:Parameter_Settings_and_Simulation_Setup}, Fig. \ref{fig:Procedure}). %MM 
		
		%, which means that the agents just have to search for an optimal amount of specific investment. %MM
		%In the following section, a brief introduction to reinforcement learning is given and the fuzzy Q-learning method is formalized. %MM
		%Since the simulation study assumes that the agents have no prior knowledge about opponent's decision-making behavior, the agents have to learn which investment leads to which consequence. %MM
		%For this purpose, the sequence of events within the negotiated transfer pricing model is run through several times and, in order to indicate this additional time dimension, a time subscript $t \in \mathbb{N}$ for the ``inner loop'' of the simulation is added to the model. %MM 
		
		

%\subsection{The learning method}
%\label{sec:The_learning_method}

	%Models of the learning behavior of economic agents are often investigated in game theory, in which the analytic methodology prevails (e.g., \citep{Fudenberg_Levine_1998}), as well as in agent-based computer economics, where the methodology of computer simulation is usually applied (e.g., \citep{Tesfatsion_2003,Tesfatsion_2006}). %L58 pg.2
	%Models of the learning behavior of economic agents are studied both in agent-based computational economics (e.g., [4,19]) and in game theory (e.g., [7,16]). %L58 UMFORMULIEREN
	%In agent-based computer economics, the methodology of computer simulation is typically used, while in game theory the analytic methodology prevails. %L58 noch mehr UMFORMULEIREN
	%In agent-based computational economics the methodology of computer simulation is typically adopted, whereas in game theory the analytical methodology is predominant [6]. %L58 UMFORMULEIREN
	%With an agent-based simulation the researcher gains both a dynamic modeling approach and a high degree of heterogeneity in terms of structure and dynamic interactions. %L117 pg.3 UMFORMULIEREN
	%A common approach to describe individual learning behavior is reinforcement learning \citep{sandholm1996multiagent}. %L58 UMFORMULIEREN
	%Belief-based learning is studied in, e.g., [17,18], in which agents have the ability to observe its opponents' actions and reward functions. %L58 und UMFORMULIEREN
	%Considering belief-based learning, the game converges to the Nash equilibrium. %L58 und UMFORMULIEREN KRISTIANT
	%This approach is applicable in cases where the learning individuals (agents) are in an unknown environment and choose actions for which they receive rewards, but they do not have the ability to directly observe their opponents' actions and, especially, they do not have beliefs about the likely play of others \citep{feltovich2000reinforcement}. %L133 pg.2, SS148
	%In reinforcement learning, a learning agent is able to perceive its environment, can choose actions that can change its environmental state, and, of course, the agent pursues a goal. %L139 pg.2
	%Moreover, an agent uses its past experiences to find an optimal policy to maximize its utility function.\footnote{ % to reach its objective. %L136 pg.2
	%Note, a policy is a mapping from environmental states to agent's actions and determines the learning agent's way of behaving \citep{sutton2018reinforcement}.}
	%In the following, a brief introduction to Q-learning and its fuzzy version is provided. %MM
	%A policy is a mapping... L139 pg.5
	%A policy is a plan of the course of play. %L118 pg.4
	%One important step forward in understanding how reinforcement learning works in multi-agent systems was made in \citep{Tuyls_Hoen_Vanschoenwinkel_2006}. %L121 pg.22
	%Many reinforcement learning approaches are based on estimating state-action value functions (often abbreviated to Q) that ultimately give the expected utility of a given action in a given state. %, and following an optimal policy thereafter. %https://www.researchgate.net/publication/50247491_An_Analysis_of_Q-Learning_Algorithms_with_Strategies_of_Reward_Function

	%Reinforcement learning is a type of machine learning and depends on a straightforward principle, that...
	%The idea of reinforcement learning is that the tendency to produce an action should be strengthened, if it produces favorable results, and weakened, if it produces unfavorable results \citep{Sandholm_Crites_1995}. %L133 pg.3 
	%: search for the action that brings the highest reward. %Wiki + Internet L58 pg.3 und MM
	%Reinforcement learning is also widely used because it provides a theoretical framework for investigating the principles of agents learning to act \citep{Kaelbling_Littman_Moore_1996}. %L134 pg.34

	%In reinforcement learning, a learning agent is able to perceive its environment, can choose actions that can change its environmental state, and of course the agent pursues a goal. %L139 pg.2
	%Moreover, an agent uses its past experiences to learn an optimal policy to reach its objective. %L136 pg.2
	%A policy is a plan of the course of play. %L118 pg.4
	%A Markov decision process includes these three aspects and is a common choice to describe such learning processes \citep{Sutton_Barto_2017}. %L139 pg.2 passende Referenz erfunden

	%In general, reinforcement learning is a solution to a Markov chain or more precisely to a Markov decision process, whenever the environment fulfills the Markovian property (e.g., \citep{Bellman_1957,Howard_1960}). %L128 pg.4 könnte in dieser ersten Referen stehen, da er als erstes MDP zusammengefasst hat
	%A decision process has the Markovian property if, at any time, the future state of the process is conditionally independent of the sequence of past states, given the current state, or in other words, the next state depends only on the previous state. %Internet könnte dort so stehen
	%A decision process has the Markovian property if the conditional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it. %Wiki UMFORMULEIREN



%\subsection{Markov Decision Process}

%A Markov decision process is an abstract framework which can describe decision making in a discrete time stochastic control process and consists of a four-tuple $(S,A,r,p)$ (e.g., \citep{Bellman_1957,Puterman_1994}). 		%L121 pg.39, Wiki
%As usual, $S$ and $A$ are the state and action spaces, respectively, $r:S \times A \to \mathbb{R}$ is the reward function of the agent, and $p: S \times A \to \Delta(S)$ is the transition function, where $\Delta(S)$ denotes the set of probability distributions over the state space $S$. %L131 pg.2, L118 pg.4
%The concept of the Markov decision process is that an agent stays in some state $s \in S$, chooses some action $a \in A$, and receives a reward $r$ from its environment. %L137 pg.12 und MM
%To make a good decision in each state, the agent needs a policy function $\Pi$. %L137
%A policy is a plan of the course of play. %L118 pg.4
%Especially, $\boldsymbol{\Pi} =(\Pi_0,...,\Pi_t,...)$ is specified over the total game with decision function $\Pi_t: H_t \to \Delta(A)$, where $H_t=(s_0,a_0,...,s_{t-1},a_{t-1},s_t)$ is the space of histories at time $t \in \mathbb{N}$ and $\Delta(A)$ denotes the space of probability distributions over the agent's actions \citep{Hu_Wellman_2003}. %L118 pg.4 und MM over=across

%In a Markov decision process, an agent's goal is to find a policy $\boldsymbol{\Pi}$ which maximizes its state value function $V(s, \boldsymbol{\Pi})$, which is nothing more than the sum of discounted expected rewards for some state $s \in S$, %L118 pg.4, L137 pg.14
%\begin{equation}
%	V(s,\boldsymbol{\Pi}) = \sum\limits_{t=0}^{\infty} \gamma^t E[r_t | s_0=s, \boldsymbol{\Pi}] \; , \label{eq:State_Value_Function} %L118 pg.4
%\end{equation}
%where $\gamma \in [0,1)$ is the discount factor, $s_0$ denotes the initial state of the game and $r_t$ denotes the reward value at time $t$. %L118 pg.4
%To solve Eq. (\ref{eq:State_Value_Function}), the Bellman equation is typically used, which is a dynamic programming method \citep{Puterman_1994}. %L118 pg.4
%Hence, the Bellman equation is given by
%\begin{equation}
%	V(s,\boldsymbol{\Pi}^*) = \max\limits_{a} \Big( r(s,a) + \gamma \sum\limits_{s'}P(s'|s,a)V(s',\boldsymbol{\Pi}^*) \Big) \; , \label{eq:Bellman_Equation}	%L118 pg.4
%\end{equation}
%where $s'$ is the next state and $P(s'|s,a)$ is the probability of transiting to state $s'$ given action $a$ and state $s$. %L118 pg.4
%Therefore, an optimal policy can be calculated.
%If an agent is not informed about its reward function or about the state transition function, Eq. (\ref{eq:Bellman_Equation}) cannot be solved this way (for traditional learning methods for $r$ and $p$ see, e.g., \citep{Sata_Abe_Takeda_1988}). %L118 pg.4, L138 pg.2 und MM

%In this paper, agents cannot directly observe their competitors' actions and have no detailed information about their environment. %MM have no beliefs about the state transition function
%Based on these assumptions, temporal difference learning, a class of model-free reinforcement learning approach, is applied \citep{sutton2018reinforcement}. %L118 pg.4 und MM
%In temporal difference learning, the state value function $V$ can be comprehended by an action value function $Q: S \times A \to \mathbb{R}$ (for Q-learning), which is given by %MM
%\begin{equation}
%	Q^*(s,a) = r(s,a) + \gamma \sum\limits_{s'}P(s'|s,a)V(s',\boldsymbol{\Pi}^*) \; . \label{eq:State_Value_Function}
%\end{equation}
%According to Eq. (\ref{eq:State_Value_Function}), $Q^*(s,a)$ is nothing more than the expected discounted reward for executing action $a$ in state $s$ and thereafter following an optimal policy. %L118 pg.4, L138 pg.2 und MM
%The relationship between (\ref{eq:Bellman_Equation}) and (\ref{eq:State_Value_Function}) is as follows: %L118 pg.4 und MM
%\begin{equation}
%	V(s,\boldsymbol{\Pi}^*) = \max\limits_a Q^*(s,a) \; .	%L118 pg.4
%\end{equation}
%Hence, if an agent knows its $Q^*(s,a)$, then an optimal policy $\boldsymbol{\Pi}^*$ for state $s$ can be played by %L118 pg.4 und MM
%\begin{equation}
%	\boldsymbol{\Pi}^* = \arg\max\limits_a Q^*(s,a) \; . %L128 pg.5
%\end{equation}
%So, the learning problem is shifted to finding $Q^*(s,a)$ instead of $V(s,\boldsymbol{\Pi}^*)$. %L118 pg.4
%If an agent knows its optimal Q-function, then the optimal choice of policy is given by the action which yields the highest Q-value. %L136 pg.2



		%\subsubsection{Q-learning}
		%\label{sssec:Q_Learning_Agents}
	
			%Two popular reinforcement learning approaches to multi-agent problems are Q- and SARSA-learning \citep{Watkins_Dayan_1992,Sutton_Barto_2017} which are applied in this paper. %SS94, L58 pg.2#
			%Q-learning is a popular reinforcement learning approach to multi-agent problems. %SS94, L58 pg.2#
			%For an overview of reinforcement learning techniques for learning behavior in multi-agent economic models see, e.g., \citep{Hu_Wellman_1998,Lauer_Riedmiller_2000,Littman_1994,Littman_2001}. %L133 pg.12
			%The learning method by \cite{watkins1989learning} is based on estimating the state-action value function (or Q-function) that ultimately gives the expected utility of a given action in a given state.
			%In Q-learning, an agent learns its Q-values $\in \mathbb{R}$ by a simple iteration update which is given by
			%\begin{equation}
			%	Q_{t+1}[\pmb{s}_t,a_{t}] = (1-\alpha) \; Q_{t}[\pmb{s}_t,a_{t}] + \alpha \; \Big( r_t(\pmb{s}_t,a_t) + \gamma \; \max\limits_{a \in \mathcal{A}} Q_t[\pmb{s}_{t+1},a] \Big) \; , \label{eq:Q_Update_Rule} 	%L118 pg.4
			%\end{equation}
			%where $\pmb{s}_t = (s_{t,S},s_{t,B}) \in \mathcal{S} \subset \mathbb{N}^2, a_t \in \mathcal{A} \subset \mathbb{N}$, and $r_t \in \mathbb{R}$ stand for state vector, action, and reward  at time $t \in \mathbb{N}$, respectively. 
			%For the sake of readability, the subscript $j \in \{ S,B\}$ is omitted from all variables in the formulas. %MM
			%In line with the negotiated transfer pricing model in Sec. \ref{sec:The_Model}, action $a_t$ taken by the supplying division (buying division) corresponds to the investment decision $I_S$ ($I_B$), the state $s_{t,S}$ ($s_{t,B}$) represents the last action $a_{t-1}$ chosen by the supplying division (buying division), and reward $r_t$ that the supplying division (buying division) receives is equal to $\Pi_S$ ($\Pi_B$). %MM
			%(which is the supplying division's profit $\Pi_S$ and the buying division's profit $\Pi_B$ based on Eq. \ref{eq:Profit_S} and Eq. \ref{eq:Profit_B}, respectively).
			
			%$\mathcal{S}$ and $\mathcal{A}$ describe the state space and the action space, respectively. %, and	$n \in \mathbb{N}$ denotes the number of Q-learning agents.
			%Further, $\gamma \in [0,1)$ describes the discount factor and $\alpha \in (0,1]$ is the learning rate.\footnote{
			%Technical note: if the discount factor is one, then the sum of the expected future rewards becomes infinite, which implies that the Q-values keep growing as long as the simulation is running. Hence, the Q-function does not converge.} 
			%Q-learning can be seen as a weighted average of old stored information and new received information from the agent's environment. %L126 pg.8, Wiki und MM
			%Note that the values of the state-action value function are stored in a lookup table (labelled with square brackets) initialized with zero for all states and actions, i.e., there is no information about the agent's environment when the simulation is started. %MM
			%Moreover, it is assumed that the state space $\mathcal{S}$ and the action space $\mathcal{A}$ are finite sets. %L58 und MM discrete and
			%The new Q-value is performed as a weighted average of the old stored information and the new received information from the agent's environment. %L126 pg.8, Wiki und MM
			%The above update rule converges to $Q^*(s,a)$ for any finite Markov decision process, for all $s$ and $a$, under the assumption that the agent plays every action infinitely often in every state and the learning rate fulfills certain constraints \citep{Watkins_Dayan_1992}.
			%When an agent uses Q-learning to find an optimal policy, an off-policy is followed \citep{sutton1988learning}, which means that the agent pursues a behavioral policy and simultaneously learns its optimal Q-function. %L136 pg.2
	
			%In temporal difference learning, there is another common approach to deal with optimal policies. %L122 pg.2 und MM to learn an optimal policy.
			%The state-action-reward-state-action learning method \citep{rummery1994line} (also known as SARSA) estimates $Q(s,a)$ for the current learned policy for all states and actions. %L139 pg.105
			%Basically, the SARSA-algorithm is similar to the Q-learning method. %L139 pg.105 und MM
			%The SARSA update is given by
			%\begin{equation}
			%	Q_{t+1}(s_t,a_t) = (1-\alpha)Q_t(s_t,a_t) + \alpha \Big( r_t(s_t,a_t) + \gamma Q_t(s_{t+1},a_{t+1}) \Big) \; , \label{eq:SARSA_Update_Rule} %L127 pg.4
			%\end{equation}
			%where $s_{t+1}$ is the next state and $a_{t+1}$ is the next action which is selected by the learned policy (the  choice of action is based on the Boltzmann exploration strategy which is explained in the next subsection). %L122 pg.2 und MM
			%The only difference between Eq. (\ref{eq:Q_Update_Rule}) and Eq. (\ref{eq:SARSA_Update_Rule}) is that an agent in off-policy learning always selects the action with the highest Q-value, while a SARSA-learning agent learns its Q-function with its current policy which has been learned in the current state (on-policy learning). %L122 pg.2 und MM



\subsection{Fuzzy Q-learning}
\label{sec:The_learning_method}

		%\subsubsection{Fuzzy Q-learning}
		%\label{sssec:Fuzzy_Q_Learning_Agents}		
			
			%In the conventional Q-learning approach, the Q-values are recorded in a look-up table. %L283pg.659
			%If the number of states and actions increases, this method is highly impractical and, especially, it is not applicable to cases of continuous states or actions. %L283pg.659 und L286pg.2051
			%Different authors have also reported poor performance due to lack of convergence property in multi-agent settings \citep[e.g.,][]{agogino2005quicker, georgila2014single}. %L286pg.2051 und MM mit multi-agent systems
			
			%In line with decision-making in fuzzy environments \citep[e.g.,][]{glorennec1994fuzzy, zimmermann2011fuzzy}, this paper studies negotiated transfer pricing with fuzzy Q-learning agents.\footnote{ %L331pg8 divisions which are characterized by fuzzy Q-learning agents.
			%Following \cites{zimmermann2011fuzzy} work, the divisions in the negotiated transfer pricing model are characterized by fuzzy Q-learning agents who have to learn what amount of specific investment should they invest.\footnote{
			%While Q-learning is a widely used reinforcement learning method to study human behavior in decision-making situations, fuzzy Q-learning is less common. 
			%For agent-based simulations with Q-learning agents acting in an economic context see, e.g., \cite{dearden1998bayesian, tesauro2002pricing, waltman2008learning} and with fuzzy Q-learning agents see, e.g., \cite{kofinas2018fuzzy, rahimiyan2006modeling}.} %L331pg.9   L331pg.3 in managerial science
			%Recall that the negotiation over transfer price and quantity leads to an ex post efficient quantity 
			%Fuzzy set theory is often applied to relax or generalize classical methods from a dichotomous to a gradual character \citep{zimmermann2011fuzzy}. %L287pg31 
			%In addition, it can also reduce the complexity of the problem and also model the uncertainty in the data. %L287pg31
			%For example, an agent does not know exactly which state is currently active, or there are simply too many states and the agent cannot precisely identify the desired state for storing the information it has learned. %MM
			%For example, an agent cannot precisely locate the desired state (in its memory) for storing the information it has learned, if there are too many or too similar states. %MM
			%Moreover, the underlying fuzzy rule based system can be easily interpreted and can also be combined well with the traditional Q-learning approach, which makes fuzzy Q-learning the learning method of choice. %SSC-Paper
			%Furthermore, the fuzzy Q-learning approach offers better properties than the conventional Q-learning method. %MM
			%Due to linguistic reasons, we sometimes use the term agent for division. %MM
			%In the context of reinforcement learning, it is common to refer to agents than to divisions and, hence, we  %MM
			
			In this paper, fuzzy Q-learning proposed by \cite{glorennec1994fuzzy} is applied to describe the before-mentioned divisions, hereinafter referred to as agents.\footnote{
			Readers unfamiliar with reinforcement learning methods should refer to \cite{sutton2018reinforcement}. %L331pg.3  L139pg.59: Q-learning which is based on dynamic programming ideas
			For agent-based simulations with fuzzy Q-learning agents acting in an economic context see, e.g., \cite{kofinas2018fuzzy, rahimiyan2006modeling}.} %L331pg.9   L331pg.3 in managerial science	
			%and which is ``technically'' based on is applied to
			%In this paper, an efficient adaptation of Watkins' Q-learning method, called fuzzy Q-learning, is applied to facilitate a continuous state space and action space. %L283pg.662 und Internet
			%Fuzzy Q-learning proposed by \cite{glorennec1994fuzzy} is an universal approximator of the Q-function defined in Eq. \ref{eq:Q_Update_Rule} %L285pg.475 und Internet
			%and combines Q-learning and a fuzzy rule-based system. %, which usually has the following form:\footnote{
			Note that fuzzy logic is based on the premise that the key elements in human thinking are not numbers, but labels of fuzzy sets \citep{zadeh1973outline}. %L359pg1
			Since human thinking is tolerant of imprecision, fuzzy logic is suitable for representing human decision-making in a natural way \citep[e.g.,][]{zadeh1973outline, zimmermann2011fuzzy}. %L342pg3 and MM
			%Human thinking is tolerant of imprecision and the real world is too complicated to be described precisely. %L342pg3		
			By doing so, the fuzzy conditional statements are expressions of the form, IF $A$ THEN $B$, where $A$ and $B$ have fuzzy meaning. %L359pg1
			The fuzzy rule-based system in this simulation study has the following form, which can be regarded as a zero order Takagi-Sugeno fuzzy system. % with $N$ fuzzy rules.} %  Footnote: L299pg.5 therefore, the denominator in Eq. () and Eq. () is one
				%\begin{equation}
				%	\text{rule }i \hspace{-1mm}: \text{if } s_1 \text{ is } L_1^i \text{ and ... and } s_n\text{ is } L_n^i \text{ then } Q(\pmb{s}, \pmb{a}) = \sum\limits_{i=1}^N \alpha_i(\pmb{s}) q[i,a_i] \label{eq:fuzzy_rule_based_system} %L284pg.13 and L284pg.15
				%\end{equation}
			For each fuzzy rule $i$:
			\begin{align}
				\text{IF } s_{S,t} \text{ IS } L_S^i & \text{ AND } s_{1,t}\text{ IS } L_1^i \text{ AND } s_{2,t}\text{ IS } L_2^i \text{ THEN } \label{eq:fuzzy_rule_based_system} \\ %L284pg.13 and L284pg.15
				 & Q_{j,t}(\pmb{s}_{t}, \pmb{a}_{j,t}) = \sum\limits_{i=1}^N \alpha_i(\pmb{s}_t) \; q_{j,t}[i,a_{j,t,i}] \label{eq:Q_values} \\ %L284pg.13 and L284pg.15
				& A_{j,t}(\pmb{s}_{t}, \pmb{a}_{j,t}) = \sum\limits_{i=1}^N \alpha_i(\pmb{s}_t) \; a_{j,t}[i,a_{j,t,i}] \label{eq:A_values}
			\end{align}
			where the parameter $i \in \{ 1,...,N \}$ describes the index of fuzzy rules, $N \in \mathbb{N}$ is the number of fuzzy rules, $\pmb{s}_t = (s_{S,t},s_{1,t},s_{2,t}) \in \mathcal{S} \subset \mathbb{R}^3$ represents the state vector, $\pmb{a}_{j,t}=(a_{j,t,1},...,a_{j,t,N}) \in \{ 1,...,K \}^N$ denotes the index vector of stored actions, $a_{j,t}[i, a_{j,t,i}] \in \mathcal{A} \subset \mathbb{R}$ are the stored actions, $K \in \mathbb{N}$ is the number of stored actions in each fuzzy rule, $A_{j,t}(\pmb{s}_t, \pmb{a}_{j,t}) \in \mathbb{R}$ refers to the inferred action, and $q_{j,t}[i,a_{j,t,i}] \in \mathbb{R}$ denotes the stored q-values, whereby $a_{j,t}[., \hspace{-0.6mm} .]$ and $q_{j,t}[., \hspace{-0.6mm} .]$ are stored in look-up tables (indicated by square brackets).
			%As with the conventional Q-learning approach, the q-values are stored in a look-up table, but fuzzy Q-learning requires much less memory storage, since the table size depends on the number of fuzzy rules $N$ and on the number of stored actions $K$ in each fuzzy rule. 
			The number of fuzzy rules $N$ and the number of stored actions $K$ rely on the fuzzy partition of the state space $\mathcal{S}$ and on the discretization of the action space $\mathcal{A}$, respectively. 
			For the sake of simplicity, it is assumed that each fuzzy rule has exactly $K$ possible actions. %MM footnote: L286pg.2052 und MM
			%While in the conventional Q-learning, the state space and the action space are discrete, they are continuous. 
			%and, for the sake of readability, the subscript $j \in \{ S,1,2\}$ is omitted from all variables in the formulas.
			
			Note, in fuzzy Q-learning, $\mathcal{S}$ and $\mathcal{A}$ are continuous spaces and the inferred action $A_{j,t}(\pmb{s}_t, \pmb{a}_{j,t})$ of agent $j$ corresponds to the investment decision $I_{j,t}$, for $j \in \{ S,1,2 \}$. %, for $j \in \{ S,B\}$. %MM	
			Further, the fuzzy sets $L_j^i$ are characterized by linguistic labels and the function $\alpha_i(\pmb{s}_t)$ denotes the truth value of rule $i$ given the state vector $\pmb{s}_t$. %L285pg.475 and L283pg.659
			Q-values $Q_{j,t}(\pmb{s}_t, \pmb{a}_{j,t})$ and actions $A_{j,t}(\pmb{s}_t, \pmb{a}_{j,t})$ are inferred from Eq. \ref{eq:Q_values} and Eq. \ref{eq:A_values}, respectively. %Internet
	
			%In fuzzy Q-learning, the Q-function is no longer a look-up table, since the Q-values are estimated by a linear parameterized approximation of a few stored q-values, where the ``weights" $\alpha_i(\pmb{s}_t)$ are commonly generated by the T-norm product %L299pg.5 
			The fuzzy rule-based system in this paper assumes that the ``weights" $\alpha_i(\pmb{s}_t)$ are generated by the T-norm product %L299pg.5 
			\begin{equation}
				\alpha_i(\pmb{s}_t) = \mu_{L_S^i}(s_{S,t}) \cdot \mu_{L_1^i}(s_{1,t}) \cdot \mu_{L_2^i}(s_{2,t}) \;, \label{eq:truth_value} 
			\end{equation}
			where $\mu_{L_j^i}(s_{j,t}) \in [0,1]$ denotes the membership function (or membership grade) of rule $i$ in state $s_{j,t}$, $j \in \{ S,1,2 \}$. %L299pg.5
			%\footnote{Usually, the membership function (or membership grade) is defined on the interval $[0,1]$. If an object has a membership grade of one (zero) in a fuzzy set, then the object is absolutely (not) in that fuzzy set.} %Footnote: SS1025
			The T-norm (or triangular norm, see, e.g., \cite{zimmermann2011fuzzy}) is a type of binary operation that is often used in fuzzy logic to model the AND operator in Eq. \ref{eq:fuzzy_rule_based_system}. %Footnote: https://en.wikipedia.org/wiki/T-norm		
			The membership functions considered here are defined on the interval $[0,1]$, which implies that if an object has a membership grade of one (zero) in a fuzzy set, then the object is absolutely (not) in that fuzzy set. %L299pg.5 Footnote: SS1025		
			In addition, the membership functions are set in such a way that the strong fuzzy partition is fulfilled, i.e., $\sum_{i=1}^N \alpha_i(\pmb{s}_t) =1$ for each $\pmb{s}_t \in \mathcal{S}$. %
			
			%Similar to the conventional Q-learning method, 
			Finally, the stored q-values are updated by %L295pg.5
			\begin{equation}
				q_{j,t+1}[i,a_{j,t,i}] = q_{j,t}[i,a_{j,t,i}] + \alpha_i(\pmb{s}_t) \; \Delta Q_{j,t}(\pmb{s}_t, \pmb{a}_{j,t}) \;,	\label{eq:Fuzzy_Q_Update_Rule} 	%284 pg.17
			\end{equation}
			where $\Delta Q_{j,t}(\pmb{s}_t, \pmb{a}_{j,t})$ is the temporal difference error which is given by %L284pg.16
			%Q_{t+1}(\pmb{s}_t, \pmb{a}_t) = (1-\alpha)Q_t(\pmb{s}_t, \pmb{a}_t) + \alpha \Big( r_t(\pmb{s}_t,A(\pmb{s}_t, \pmb{a}_t)) + \gamma \max\limits_{a_i \in \{ 1,...,K \}} \sum\limits_{i=1}^N \alpha_i(\pmb{s}_{t+1}) q_t[i,a_i] \Big) \; , \label{eq:Fuzzy_Q_Update_Rule} 	%L118 pg.4
			\begin{equation}
				%\Delta Q_{j,t}(\pmb{s}_t, \pmb{a}_{j,t}) = \nonumber \\
				\alpha \Big( r_{j,t}(\pmb{s}_t,A_{j,t}(\pmb{s}_t, \pmb{a}_{j,t})) + \gamma \sum\limits_{i=1}^N \alpha_i(\pmb{s}_{t+1}) \hspace{-1mm} \max\limits_{k \in \{ 1,...,K\}} \hspace{-1mm} q_{j,t}[i,k] - Q_{j,t}(\pmb{s}_t, \pmb{a}_{j,t}) \Big) \; . \label{eq:Fuzzy_Q_Error} 	%L118 pg.4
			\end{equation}
			%According to Eq. \ref{eq:Fuzzy_Q_Update_Rule}, up to $N$ q-values per time step can be adjusted and, therefore, the Q-function converges more quickly in fuzzy Q-learning.\footnote{ %L286pg.2054
			%Roughly speaking, the Q-function converges to its optimum, when a sufficient number of time steps has been performed and the stored q-values change only slightly, or if the temporal difference error $\Delta Q_t(\pmb{s}_t, \pmb{a}_t)$ goes to zero, i.e., $\Delta Q_t(\pmb{s}_t, \pmb{a}_t) \to 0$ for $t \to \infty$. % after $T_L$ time steps.
			%For the precise Q-learning conditions which ensure that the Q-function converges (from any initial state) see, e.g., \cite{watkins1992q}.
			%According to the parameter settings in Sec. \ref{sec:Parameter_Settings_and_Simulation_Setup}, up to $4$ q-values are updated simultaneously per time step.} % L345pg.2 and MM		
			%Moreover, fuzzy Q-learning is simple, has low computational demands per iteration, and, with relatively few environmental interactions, good results can be achieved. %L139 pg.134
			%Nevertheless, there are more sophisticated learning policies and efficient learning techniques in reinforcement learning (for an overview see, e.g., \citep{sutton2018reinforcement}). %L126 pg.9
			%Nevertheless, there are more sophisticated learning techniques in reinforcement learning (for an overview see, e.g., \cite{sutton2018reinforcement}). %L126 pg.9
			In Eq. \ref{eq:Fuzzy_Q_Error}, $r_{j,t} \in \mathbb{R}$ describes the reward of agent $j$ which is the division's profit $\Pi_j$ at time $t \in \mathbb{N}$, $j \in \{ S,1,2 \}$. %MM
			$\alpha \in (0,1]$ and $\gamma \in [0,1)$ denote the learning rate and the discount factor, respectively. %MM
			For the sake of simplicity, it is assumed that all three agents have the same learning rate as well as the same discount factor. %MM
			
			

	\subsection{Exploration policy} %L283pg.660
	\label{ss:Exploration_Polic}
	
		When using a reinforcement learning method, a trade-off is required between choosing the currently optimal action and choosing a varied action with the prospect of a higher reward in the future \citep{sutton2018reinforcement}. %L136 pg.2 und MM
		Thus, the fuzzy Q-learning agents, which are situated in a non-stationary environment, need an exploration policy which ensures that all actions are performed frequently enough so that the fuzzy Q-function converges to an optimum. %L121 pg. 6 und MM
		In this simulation study, three commonly used exploration policies are applied to verify whether the results are robust to changes in the exploration policy. %MM


		%Especially, in non-stationary environments, the selection of actions must occur randomly \citep{Abdallah_Kaisers_2016}. %L133 pg.16
		%Therefore, an exploration strategy is applied, where the agent has enough time to exploit good strategies, but has also time for exploitation.

		%Since an agent does not always choose the action he currently finds best, an exploration strategy is applied. %MM
		%An agent should not always choose the action it currently finds the best. %L133 pg.16

		%Contrary, the agent should also have enough time to exploit good strategies. %MM

		%A very key part in reinforcement learning is the trade-off between exploration and exploitation. %MM Youtube
		%In order to select ``good'' actions, an agent has to explore its environment and, in particular, has to learn which action provides which reward. %L133 pg.16 und MM
		%As with other reinforcement learning approaches, a balance between exploration and exploitation has to be found. %L132 pg.2 und MM
		%Therefore, an agent is confronted with the trade-off between choosing the currently optimal action (exploitation) and choosing a varied action with the prospect of a higher reward in the future (exploration) \citep{sutton2018reinforcement}. %L136 pg.2 und MM
		%Nevertheless, the agent needs the possibility to explore all states and all actions frequently enough to ensure the convergence of the Q-function to an optimum. %, respectively. %L121 pg. 6 und MM
		%In the following, three exploration policies, which are applied in the simulation study, are explained. %MM


		\subsubsection{Boltzmann exploration policy}
		\label{sssec:Boltzmann_Exploration_Policy}
		
			The Boltzmann, Gibbs, or softmax exploration policy \citep[][]{cesa2017boltzmann} is a classic strategy for sequential decision-making under uncertainty. %L328pg1
			The probability of choosing an action is proportional to an exponential function of the empirical mean of the reward of that action.
			The Boltzmann exploration policy is given by the probability mass function %MM
			\begin{equation}
				P_{j,t,i,k}(a_t[i,k]) = \frac{exp\big( {q_{j,t}[i,k]/\beta_j(t)} \big)}{\sum_{l=1}^{K} exp\big( {q_{j,t}[i,l]/\beta_j(t)} \big)} \;, \text{ for } k \in \{ 1,...,K\} \;, \label{eq:Boltzmann_Probability} %L132 pg.2
			\end{equation}
			where $\beta_j(t) \in \mathbb{R}^+$, $j \in \{ S,1,2\}$, controls the degree of randomness for exploration \citep{powell2012optimal}. %MM
			%So, for each action $a_t[i,k]$, $i \in \{ 1,...,N\}$ and $k \in \{ 1,...,K\}$, the Boltzmann probability mass function... %MM
			Note that, for each agent $j$, for each time step $t$, and for each fuzzy rule $i$, it holds $\sum_{k=1}^{K} P_{j,t,i,k} = 1$ and, moreover, that actions with higher q-values are more likely to be selected than actions with lower q-values. %L124 pg.12
			After all probabilities have been calculated according to Eq. \ref{eq:Boltzmann_Probability}, the index of each stored action is drawn from
			\begin{equation}
				a_{j,t,i} \thicksim \bigg( \genfrac{}{}{0pt}{0}{1}{P_{j,t,i,1}},...,\genfrac{}{}{0pt}{0}{K}{P_{j,t,i,K}} \bigg) \;. 
			\end{equation}
			%for each fuzzy rule $i$.
			
			
			 
			%A commonly used and very sophisticated exploration policy is the Boltzmann exploration policy \citep[][]{cesa2017boltzmann}.\footnote{
			%For other exploration policies see, e.g., \cite{thrun1992efficient}, and, by the way, the Boltzmann exploration policy is sometimes referred to as Gibbs or softmax exploration policy \citep{sutton2018reinforcement}.} %L132 pg. 2 and L328pg1
			%For each action $a_t[i,k]$, $i \in \{ 1,...,N\}$ and $k \in \{ 1,...,K\}$, the Boltzmann probability mass function is given by %L58 pg. 4 und MM
			%\begin{equation}
			%	P_{t,i,k}(a_t[i,k]) = \frac{exp\big( {q_t[i,k]/\beta(t)} \big)}{\sum_{l=1}^{K} exp\big( {q_t[i,l]/\beta(t)} \big)} \;, \label{eq:Boltzmann_Probability} %L132 pg.2
				%P_{t,i,k}(a_t[i,a_{t,k}]|s) = \frac{exp\big( {q_t[i,a_{t,k}]/\beta_t} \big)}{\sum_{l=1}^{K} exp\big( {q_t[i,l]/\beta_t} \big)} \;, %L132 pg.2
			%\end{equation}
			%where $\beta(t) \in \mathbb{R}^+$ denotes the experimentation tendency \citep{waltman2008learning}. % \citep{Waltman_Kaymak_2008}. %L58 pg. 4
			%According to Eq. \ref{eq:Boltzmann_Probability}, it holds $\sum_{k=1}^{K} P_{t,i,k} = 1$ for each fuzzy rule $i$ and time step $t$. %MM
			%The Boltzmann distribution states that actions with higher q-values are more likely to be selected than actions with lower q-values. %L124 pg.12
			%The degree of randomness for exploration is controlled by the experimentation tendency \citep{powell2012optimal} and, therefore, $\beta(t)$ should change slowly over time. %L121 pg.6, L133 pg.4 und MM
			%In particular, if $\beta(t)$ tends to infinity, the shape of the Boltzmann distribution converges to the discrete uniform distribution, which means that all actions have the same probability to being chosen (pure exploration). %L135 pg.4
			%Contrary, if $\beta(t)$ decreases toward zero, only the action with the highest Q-value is selected (pure exploitation). %L135 pg.4 und MM
			%However, the Boltzmann exploration works well, when the Q-value of the best action is well separated from the others \citep{Kaelbling_Littman_Moore_1996}. %L134 pg.10
			
			%For the long-term behavior of self-learning agents, the experimentation tendency should slowly decrease over time in order to reduce exploration \citep{dearden1998bayesian}. %L132 pg.2 slowly goes to zero. %L58
			%Hence, the experimentation tendency is calculated by a simple rational function	%MM
			%\begin{equation}
			%	\beta(t) = \frac{\beta_1}{\beta_2+t} \; , \label{eq:experimentation_tendency}
			%\end{equation}
			%whereby the experimentation tendency parameters $(\beta_1,\beta_2) \in \mathbb{R}^2$ should be set in such a way that the learning agents have a long time to explore for good policies, but also time to exploit them.\footnote{
			%Rational functions, like Eq. \ref{eq:experimentation_tendency}, have the advantage over linear functions that the experimentation tendency decreases faster at the beginning of a simulation run, but it never reaches zero. 
			%Technical note: the experimentation tendency $\beta(t)$ should not become too small during a simulation run, otherwise the expression $exp(q_t[i,k]/\beta(t))$ will be too large and this could endanger the numerical stability of the simulation.} %MM} %PP and MM
			
			%This exploration procedure is typical in reinforcement learning (see, e.g., \citep{alos2004cournot}). %L58
			
			%Note, after all probabilities have been calculated according to Eq. \ref{eq:Boltzmann_Probability}, the index of each stored action is drawn from
			%\begin{equation}
			%	a_{t,i} \thicksim \bigg( {1 \atop P_{t,i,1}},...,{K \atop P_{t,i,K}} \bigg) 
			%\end{equation}
			%for each fuzzy rule $i$.



		\subsubsection{$\epsilon$-greedy exploration policy}
		\label{sssec:Epsilon_Greedy_Exploration_Policy}
		
			A very easy and commonly used exploration policy in reinforcement learning is the $\epsilon$-greedy exploration policy.
			It chooses the action with the highest q-value in the current state with probability $1- \epsilon$ and a random action otherwise \citep{tijsma2016comparing}. %L136pg2
			The $\epsilon$-greedy exploration policy is given by
			\begin{equation}
				a_{j,t,i}= \begin{cases}
				\vspace{3mm} \underset{k \in \{ 1,...,K \}}{\mathrm{arg\,max}} q_{j,t}[i,k] & \text{with probability $1-\epsilon(t)$} \\
				\thicksim Unif \Big( \{ 1 ,...,K \} \Big) & \text{with probability $\epsilon(t)$}
				\end{cases}
			\end{equation}	
			where $\epsilon(t) \in [0,1]$ determines the degree of randomness for exploration which decreases over time. %MM
			Note that the $\epsilon$-greedy exploration policy can be seen as a benchmark for other more sophisticated exploration policies \citep{tijsma2016comparing}. %Paper1pg35
			
			
		
			%Another common and fairly easy exploration policy is the so-called $\epsilon$-greedy exploration policy, which means that with probability $1-\epsilon(t)$ the action with the highest q-value is chosen, while with probability $\epsilon(t) \in [0,1]$ a random action is selected \citep[][]{sutton2018reinforcement}. %MM
			%For each fuzzy rule $i \in \{ 1,...,N \}$, the index of the stored action is calculated by the following rule: %MM
			%\begin{equation}
			%	a_{t,i}= \begin{cases}
			%	\vspace{3mm} \underset{k \in \{ 1,...,K \}}{\mathrm{arg\,max}} q_t[i,k] & \text{with probability $1-\epsilon(t)$} \\
				%\thicksim Unif(\{ 1,...,K \}) & \text{with probability $\epsilon_t$}
				%\thicksim \Big( {1 \atop 1/K},...,{K \atop 1/K} \Big) & \text{with probability $\epsilon(t)$}
			%	\thicksim Unif \Big( \{ 1 ,...,K \} \Big) & \text{with probability $\epsilon(t)$}
			%	\end{cases}
			%\end{equation}		
			%In the simulation study, $\epsilon(t)$ is a decreasing linear function of the time
			%\begin{equation}
			%	\epsilon(t) = \begin{cases}
			%	\vspace{3mm}  \epsilon_1 - \epsilon_2 \; t & \text{for $t \leq T_L$} \\
			%	0 & \text{for $t > T_L$}
			%	\end{cases}
			%\end{equation}	
			%\begin{equation}
			%	\epsilon(t) = \epsilon_1 - \epsilon_2 \cdot t
			%\end{equation}	
			%with the two properties that, at the beginning of a simulation run, $\epsilon(1)$ is one, which indicates that the action-selection is total random (pure exploration) and, in the end, $\epsilon(T_L)$ is zero (pure exploitation). %MM
			%The parameter $T_L$ is discussed in Sec. \ref{sec:Parameter_Settings_and_Simulation_Setup}. %MM			
			%After the learning phase, the index of the stored action with the highest q-value is always selected with probability one. %MM 
			%The associated $\epsilon$-greedy exploration parameters $(\epsilon_1,\epsilon_2) \in \mathbb{R}^2$ are given in Table \ref{tab:Parameter_Setting}. %MM
	 
		
		\subsubsection{Upper confidence bound exploration policy}
		\label{sssec:Upper_Confidence_Bound_Exploration_Policy}

			The third and last exploration policy is the so-called upper confidence bound exploration policy (hereafter abbreviated to UCB policy). %MM
			The idea of the UCB policy is that the square-root expression is a measure of the uncertainty in the estimate of the action in the current state \citep{sutton2018reinforcement}. %L139pg28 and MM
			After choosing an action, the uncertainty is reduced by increasing the number of actions by one.
			For each agent $j \in \{ S,1,2 \}$, the UCB policy is given by 
			\begin{equation}
				a_{j,t,i}= \begin{cases}
				\vspace{3mm} \underset{k \in \{ 1,...,K \}}{\mathrm{arg\,max}} q_{j,t}[i,k] + c_1 \sqrt{\frac{ln(t)}{N_{j,t}[i,k]}} & \text{if $\forall k \; N_{j,t}[i,k] > 0$} \\ %\textcolor{red}{Formel in L139pg27 aber mit MINUS!!!!}
				\thicksim Unif \Big( \{ k \in \{ 1,...,K \} \; | \; N_{j,t}[i,k]=0 \} \Big) & \text{else} %\text{if $\exists N_{j,t}[i,k] = 0$}
				\end{cases}
			\end{equation}
			where $c_1 \in \mathbb{R}$ controls the degree of randomness for exploration and $N_{j,t}[i,k] \in \mathbb{N}$ is the number of times that the index of the stored action $k$ in fuzzy rule $i$ has been chosen prior to time $t$ \citep[][]{sutton2018reinforcement}.  %MM
			

			%For the third and last exploration policy, the so-called upper confidence bound exploration policy (often abbreviated as the UCB policy) is adapted, which was also proposed for multi-armed bandit problems \citep{auer2002finite}. %L136pg.3
			%The idea behind this exploration policy is that not only the current q-values are used for the action-selection, but also the uncertainty that comes along with that selection. %MM
			%One way of doing this is to select an action according to the following rule \citep[][]{sutton2018reinforcement}:
			%\begin{equation}
			%	a_{t,i}= \begin{cases}
			%	\vspace{3mm} \underset{k \in \{ 1,...,K \}}{\mathrm{arg\,max}} q_t[i,k] + c_1 \sqrt{\frac{ln(t)}{N_t[i,k]}} & \text{if $\forall N_t[i,k] > 0$} \\ %\textcolor{red}{Formel in L139pg27 aber mit MINUS!!!!}
			%	\thicksim Unif \Big( \{ k \in \{ 1,...,K \} \; | \; N_t[i,k]=0 \} \Big) & \text{else} %\text{if $\exists N_t[i,k] = 0$}
			%	\end{cases}
			%\end{equation}	
			%The parameter $c_1 \in \mathbb{R}$ describes the degree of exploration and $N_t[i,k] \in \mathbb{N}$ is the number of times that the index of the stored action $k$ in fuzzy rule $i$ has been chosen prior to time $t$. %L139pg28
			%At the beginning of a simulation run, all $N_t[i,k]$ are initialized with zero and, each time $a_{t,i}$ is determined, $N_t[i,a_{t,i}]$ is incremented by one, and, hence, the ``uncertainty term'' is reduced. %L139pg28
			
			%Note that actions that have never been chosen are given preference over actions that have already been chosen. %MM
			%In addition, $c_1$ controls the degree of exploration and the use of the natural logarithm implies that the uncertainty increases slightly over time. % \citep{sutton2018reinforcement}. %L139pg28 and MM
			%If $c_1$ is set too high, then the distribution of the UCB policy converges to the discrete uniform distribution. %MM
			%If $c_1$ is set to zero, then the highest q-value determines which action is chosen. %MM
			%According to the pre-generated simulations, a value of about $30$ is a good choice for $c_1$. %MM
			%Nevertheless, balancing the parameters of an exploration policy is one of the most challenging tasks in reinforcement learning \citep{tijsma2016comparing}. %L136pg1 and MM


\afterpage{
		%\clearpage
		\begin{landscape}
		\begin{table}[h!]
		\linespread{1.0}\selectfont
		\centering
		\caption{Parameter settings and scenario overview}
		%\caption{Comparison between negotiated transfer pricing with fully rational agents and the agent-based variant with cognitively bounded agents.}
		\label{tab:Parameter_settings_and_scenario_overview}
		\begin{tabularx}{179mm}{|l|cccc|}
			\hline
			\textbf{Fixed exogenous parameters} & \multicolumn{4}{c|}{\textbf{Values}} \\
			\hline
			Number of simulation runs & \multicolumn{4}{l|}{$10,\hspace{-0.6mm}000$} \\
			Number of times steps per simulation run & \multicolumn{4}{l|}{$T=2,\hspace{-0.6mm}100$} \\
			Time steps to learn the Q-function & \multicolumn{4}{l|}{$T_L=2,\hspace{-0.6mm}000$} \\
			Time steps to evaluate the outcome & \multicolumn{4}{l|}{$T_E=100$} \\
			Slope of the inverse demand function & \multicolumn{4}{l|}{$b=12$} \\
			Expected values of the state variables & \multicolumn{4}{l|}{$E[\theta_S]=60$,  $E[\theta_1]=E[\theta_2]=E[\theta_B]=100$} \\
			Action space and state space & \multicolumn{4}{l|}{$\mathcal{A}=\{ 0, 5,..., 50 \}$ and $\mathcal{S}= \{ 0, 12.5,..., 50 \}$} \\
			Number of fuzzy rules & \multicolumn{4}{l|}{$N=125$} \\
			Number of stored actions in each fuzzy rule & \multicolumn{4}{l|}{$K=11$} \\
			Learning rate & \multicolumn{4}{l|}{$\alpha=0.5$} \\
			Boltzmann exploration policy & \multicolumn{4}{l|}{$\beta_{S,1}=49975$, $\beta_{1,1}=\beta_{2,1} = 24987.5$, $\beta_{S,2}=\beta_{1,2}=\beta_{2,2}=498.75$}  \\
			$\epsilon$-greedy and UCB exploration policy & \multicolumn{4}{l|}{$\epsilon_1=1+1/1999$, $\epsilon_2=-1/1999$, and $c_S=60$, $c_1=c_2=30$} \\
			\hline
			\hline
			\multirow{2}{*}{\textbf{Scenario-based exogenous parameters}} & \multicolumn{4}{c|}{\textbf{Symmetric scenarios}} \\
			 & \textbf{I} & \textbf{II} & \textbf{III} & \textbf{IV} \\
			\hline
			$\lambda_S$ marginal cost of the supplying division & 1 & 1 & 1 & 1 \\
			$\lambda_1$ marginal cost of the 1st buying division & \multirow{2}{*}{0.5} & \multirow{2}{*}{0.222} & \multirow{2}{*}{(0.5, 0.222)} & \multirow{2}{*}{(0.222, 0.262, 0.307, 0.361, 0.424, 0.5)} \\
			$\lambda_2$ marginal cost of the 2nd buying division & & & & \\
			$\Gamma_1$ surplus sharing parameter between ``S and 1'' & \multirow{2}{*}{0.5} & \multirow{2}{*}{0.25} & \multirow{2}{*}{(0.25, 0.30,..., 0.55)} & \multirow{2}{*}{(0.25, 0.30,..., 0.55)} \\
			$\Gamma_2$ surplus sharing parameter between ``S and 2'' & & & & \\
			$\gamma$ discount factor & (0, 0.9) & (0, 0.9) & (0, 0.1,..., 0.9) & (0, 0.1,..., 0.9) \\
			$\sigma$ standard deviation of the state variables & 0 & 0 & 0 & (0, 5, 10) \\
			Exploration policy & BM & BM & BM & (Greedy, UCB, BM) \\
			\hline
			\hline
			\multirow{2}{*}{\textbf{Scenario-based exogenous parameters}} & \multicolumn{4}{c|}{\textbf{Non-symmetric scenarios}} \\
			 & \multicolumn{2}{c}{\textbf{V}} & \multicolumn{2}{c|}{\textbf{VI}} \\
			\hline
			$\lambda_S$ marginal cost of the supplying division & \multicolumn{2}{c}{1} & \multicolumn{2}{c|}{1} \\
			$\lambda_1$ marginal cost of the 1st buying division & \multicolumn{2}{c}{(0.534, 0.621)} & \multicolumn{2}{c|}{(0.5, 0.534, 0.564, 0.590, 0.609, 0.621)} \\
			$\lambda_2$ marginal cost of the 2nd buying division & \multicolumn{2}{c}{(0.463, 0.301)} & \multicolumn{2}{c|}{(0.5, 0.463, 0.425, 0.385, 0.343, 0.301)} \\
			$\Gamma_1$ surplus sharing parameter between ``S and 1'' & \multicolumn{2}{c}{(0.5, 0.55,..., 0.75)} & \multicolumn{2}{c|}{(0.5, 0.550, 0.600, 0.650, 0.700, 0.750)} \\
			$\Gamma_2$ surplus sharing parameter between ``S and 2'' & \multicolumn{2}{c}{(0.5, 0.45,..., 0.25)} & \multicolumn{2}{c|}{(0.5, 0.450, 0.400, 0.350, 0.300, 0.250)} \\
			$\gamma$ discount factor & \multicolumn{2}{c}{(0, 0.1,..., 0.9)} & \multicolumn{2}{c|}{(0, 0.1,..., 0.9)} \\
			$\sigma$ standard deviation of the state variables & \multicolumn{2}{c}{0} & \multicolumn{2}{c|}{(0, 5, 10)} \\
			Exploration policy & \multicolumn{2}{c}{BM} & \multicolumn{2}{c|}{(Greedy, UCB, BM)} \\
			\hline
		%\end{tabular}
		\end{tabularx}
		\linespread{1}\selectfont
		\end{table}
		\end{landscape}
	}
	


%\section{Parameter Settings and Simulation Experiments}
\section{Parameter settings}
\label{sec:Parameter_Settings_and_Simulation_Setup}

	This simulation study focuses on settings in which the learning behavior of the three agents is modeled by fuzzy Q-learning. %L58pg.3281
	%This simulation study focuses on settings in which three fuzzy Q-learning agents play a repeated negotiated transfer pricing game. %L58pg.3281
	%It is assumed that each division bears the full cost of its own investments. %MM
	It is assumed that each division has an individual constant marginal cost parameter $\lambda_j$, $j \in \{ S,1,2\}$. %, where $\lambda_S$ is set to one without loss of generality. %MM  
	%Since there are infinitely many combinations that can be examined, the simulation experiments only focus on the following convex combination, namely, $\lambda_S+\lambda_B=1$ with $\lambda_j \geq 0$. %MM
	For reasons of simplicity, $\lambda_S$ is set to one in all examined scenarios, while $\lambda_1$ and $\lambda_2$ are varied in small steps (the corresponding $\lambda_j$ values are listed in Tab. \ref{tab:Parameter_settings_and_scenario_overview}). %MM
	Besides $\lambda_j$, also the surplus sharing parameter $\Gamma_j$, $j \in \{ 1,2\}$ is a scenario-based exogenous parameter which is set in small increments. %MM
	In addition, the discount factor $\gamma$ is varied between $0$ and $0.9$ in steps of $0.1$. 
	Note that a discount factor of zero indicates that the agent is very myopic, which implies that the agent cannot observe future rewards, while, if $\gamma$ goes to one, the agent becomes more non-myopic, which means that the agent will place more emphasis on future rewards. %MM
	%and the standard deviation of the state variables $\sigma$  %MM
	To describe the turbulence of the market environment in which the agents operate, the standard deviations of the state variables $\sigma$ are set to $0$, $5$, and $10$, which can be interpreted as a deterministic market environment, a market environment with minor stochastic fluctuations, and an environment with considerable volatility on the markets. %MM	
	Additionally, the state variables $\theta_S$ and $\theta_j$, $j \in \{ 1,2\}$, are captured in a normally distributed random variable with mean $60$ and $100$, respectively. %MM
	Finally, three different exploration policies are applied to check the robustness of the simulation results.
	In the following, the parameter settings in Tab. \ref{tab:Parameter_settings_and_scenario_overview} are discussed in more detail. %MM
	
	In Eq. \ref{eq:buying_division_net_revenue}, the slope of the inverse demand function $b$ is set to $12$, as this results in that, in the first scenario examined, the first- and second-best solutions have integer solutions (see Tab. \ref{tab:Firstbest_Secondbest_Solutions} in Appendix A). %MM
	The choice of $\mathcal{A}$ and $\mathcal{S}$ is set at least in such a way that the agents can always find the first- and second-best solution in all studied scenarios. %MM 
	In this sense, the stored actions $a_{j,t}[i,a_{j,t,i}] \in \mathcal{A}$ vary between $0$ and $50$ in steps of $5$, while the states $\pmb{s}_t \in \mathcal{S}$ range from $0$ to $50$ in steps of $12.5$. %MM
	Consequently, there are $11$ stored actions for each fuzzy rule $i \in \{ 1,...,N \}$, $5$ states, and the number of fuzzy rules $N$ is $5^3=125$ (the fuzzy partition of the state space raised to the power of the number of agents). %MM
	%Recall that, according to Eq. \ref{eq:A_values}, regardless of whether the stored actions have integer values or not, the inferred action $A_t$ (which corresponds to the agent's investment decision) is a real number. %MM	
	%Also for the state space $\mathcal{S}$, it is supposed that the number of fuzzy rules should be small. %MM
	%In order to deal with a maximum of $25$ fuzzy rules (concretely, $5$ membership functions in each state space dimension), the values of the state space range from $0$ to $50$ by a step size of $12.5$. %MM
	%In order to deal with $5$ membership functions in each state space dimension, the values of the state space range from $0$ to $50$ by a step size of $12.5$; thus, there are $25$ fuzzy rules in total. %MM  
	
	\begin{figure}[b!]
		\centering
		\includegraphics[width=0.85\textwidth]{Triangular_Membership_Functions.png}
		\caption{The fuzzy sets considered here consist of $5$ triangular membership functions in each state space dimension $j \in \{S,1,2\}$, which can be characterized by linguistic labels ranging from ``Very low'' to ``Very high''.}
		\label{fig:Triangular_Membership_Functions}
	\end{figure}	
	
	According to \cites{zimmermann2011fuzzy} suggestions, triangular membership functions are applied (cf. Fig. \ref{fig:Triangular_Membership_Functions}) because they are simple, have low computing power per iteration, and are easy to interpret. %%https://www.sciencedirect.com/science/article/pii/S095741740900044X   because... L288pg.3
	For example, if the state of the supplying division $s_{t,S}$ is $10$, then the membership grade $\mu_{L_S^1}$ of fuzzy rule $1$ is $0.2$, whereas the membership grade $\mu_{L_S^2}$ of fuzzy rule $2$ is $0.8$, and all other membership grades $\mu_{L_S^3}$, $\mu_{L_S^4}$, and $\mu_{L_S^5}$ are zero, or in other words, the supplying division is in the ``very low investment range'' with a fraction of $20\%$ and in the ``low investment range'' with a share of $80\%$. %MM
	%The example shows one of the main strengths of the fuzzy sets, namely, that a fuzzy Q-learning agent does not always belong to exactly one ``investment range'', but often to two. %MM
	%This is particularly useful, if the agent is unsure of its current ``investment state''. %MM
	Incidentally, increasing the number of fuzzy rules means that the agents need more time to learn their environment.
	With only $5$ membership functions, the decision-making behavior can be exhibited quite well with the given parameter settings. %MM
	%Recall, the state $s_{t,S}$ represents the last investment decision made by the supplying division and the state $s_{t,B}$ denotes the buying division's investment from the previous time step, i.e., $s_{t,S} = A_{t-1}(\pmb{s}_{t-1}, \pmb{a}_{t-1})$ and $s_{t,B} = A_{t-1}(\pmb{s}_{t-1}, \pmb{a}_{t-1})$. %L58 as well as its competitors' joint quantities from the previous time step.
	
	%Note, according to the triangular membership functions in Fig. \ref{fig:Triangular_Membership_Functions}, up to $2^3=8$ q-values can be updated simultaneously per time step ($2$ raised to the power of $dim(\mathcal{S})$). %MM 	
	
	Furthermore, the learning rate $\alpha$ is set to $0.5$, because it can be assumed that, for long-term decisions, old stored information is just as important as new received information.  %MM
	Regardless of the selection of the learning rate, the Q-function converges (sometimes faster and sometimes slower) to an optimum with the parameter settings giving in Tab. \ref{tab:Parameter_settings_and_scenario_overview}. %MM  local optimum 
	%Note, if the learning rate is set too high, the learned information will be regularly overwritten, which is usually not recommended as this requires more time steps for convergence and, in particular, stochastic fluctuations during learning make convergence difficult.
	%On the other hand, if the learning rate is chosen too small, many more time steps are required for convergence. %MM
	
	%In our agent-based simulation, the number of time steps to learn the Q-function $T_L \in \mathbb{N}$ is relating to $\alpha$, $|\mathcal{A}|$, $|\mathcal{S}|$, the degree of exploration, and the extent of stochastic fluctuations. %MM 
	The number of time steps to learn the Q-function $T_L$ is set to $2,\hspace{-0.6mm}000$ as this is a sufficient number to guarantee that the Q-function converges in all investigated scenarios. %MM
	Note that this simulation study assumes that all agents have no prior knowledge of the consequences of their actions (all q-values are initialized to zero). %MM
	After determining the number of time steps required for learning the Q-function, further $100$ time steps are simulated to evaluate the agents' outputs, because, if $\sigma$ is not zero, the stochastic fluctuations of the markets affect the divisions' profits.
	Consequently, the number of time steps per simulation run $T$ is set to $2,\hspace{-0.6mm}100$.

	Since the choice of action is based on stochastic exploration policies, each scenario is carried out $10,\hspace{-0.6mm}000$ times and, due to the coefficient of variation (the ratio of the standard deviation to the mean), $10,\hspace{-0.6mm}000$ simulation runs are sufficient to express the precision and repeatability of the simulation study. %L58
	
	
	%The number of time steps to learn the Q-function $T_L \in \mathbb{N}$ is mainly based on the cardinalities of $\mathcal{A}$ and $\mathcal{S}$; other factors like $\alpha$, the degree of exploration, and the extent of stochastic fluctuations also influence the learning time, but to take them into account explicitly would be too time-consuming. %MM 
	%Since each agent has $275$ ($=|\mathcal{A}| \cdot |\mathcal{S}|^2=11 \cdot 5^2$) different q-values and the maximum number of q-values that are updated per time step is $4$, %average visits to each state-action pair should be at least about $4$ times , %the action selection of the investments is determined by the Boltzmann exploration policy,
	% the number of time steps to learn the Q-function is set to $1,\hspace{-0.6mm}000$ so that a sufficient number of state-action pairs are visited. %, which implies that the number of average visits to each state-action pair is approximately $4$ times, if actions are uniformly distributed. %MM
	%The pre-generated simulations also suggest that a learning time of $1,\hspace{-0.6mm}000$ is sufficient to guarantee convergence in all examined scenarios. %MM
	%After the complexity of learning the Q-function has been determined, further time steps 
	%After determining the number of time steps required for learning the Q-function, further $100$ time steps are simulated to evaluate the agents' outputs, hence $T_E=100$ and, consequently, the number of time steps per simulation run $T$ is set to $1,\hspace{-0.6mm}100$.\footnote{ %MM
	%Please be aware that even if the Q-function converges, the stochastic fluctuations of the environment affect the divisions' profits. In order to take the volatility of the markets into account, an observation period of $100$ time steps is chosen to evaluate the simulation outcomes.} %MM	
	
	%It should be emphasized once again that this study aims to analyze negotiated transfer pricing with cognitively bounded agents. %MM
	%This means that not only the number of actions or the number of fuzzy rules is limited, but also the possibility of observing all state-action pairs infinitely often. %MM 
	%Therefore, the simulation ends after $1,\hspace{-0.6mm}100$ time steps, even if not all state-action pairs have been visited. %MM
	
	%Besides the surplus sharing parameter $\Gamma$, also the discount factor $\gamma$ varies from $0$ to $0.9$ in steps of $0.1$. %MM
	%The discount factor is set exogenously at the very beginning of a simulation run and represents the agent's foresight. %MM
	%A discount factor of zero indicates that the agent is very myopic, which implies that the agent cannot observe future rewards. %MM
	%Conversely, if $\gamma$ goes to one, the agent becomes more non-myopic, so the agent will place more emphasis on future rewards. %MM
	%Recall, when $\gamma$ is set to one, then the Q-function does not converge. %MM
	
	%Finally, the state variables $\theta_S$ and $\theta_B$ are captured in a normally distributed random variable with mean $60$ and $100$, respectively. %MM
	%To describe the turbulence of the environment in which the agents operate, the standard deviations of the state variables are set to $0$, $5$, and $10$, which can be interpreted as a deterministic environment, an environment with minor stochastic fluctuations, and an environment with considerable volatility on the markets. %MM

	Finally, the used exploration policies are discussed. %MM
	%In addition to varying the state variables, the sensitivity of the results is checked in regard to three different exploration policies. %MM
	Inspired by the work of \cite{tijsma2016comparing}, this simulation study focuses on the Boltzmann, $\epsilon$-greedy, and upper confidence bound exploration policy. %L178pg11 and MM
	%These exploration policies for fuzzy Q-learning are explained in more detail below. %MM
	For the introduced Boltzmann exploration policy in Sec. \ref{sssec:Boltzmann_Exploration_Policy}, $\beta_j(t)$ should slowly decrease over time in order to reduce exploration \citep{dearden1998bayesian}. %L132 pg.2 slowly goes to zero. %L58
	In this sense, $\beta_j(t)$ is calculated by a simple rational function	%MM
	\begin{equation}
		\beta_j(t) = \frac{\beta_{j,1}}{\beta_{j,2}+t} \; , \text{ for } j \in \{ S,1,2\}, \label{eq:experimentation_tendency}
	\end{equation}
	where $(\beta_{j,1},\beta_{j,2}) \in \mathbb{R}^2$ are set in such a way that the learning agents have a long time to explore for good policies, but also time to exploit them.\footnote{
	Technical note: The experimentation tendency $\beta_j(t)$ should not become too small during a simulation run, otherwise the expression $exp(q_{j,t}[i,a_{j,t,i}]/\beta_j(t))$ will be too large and this could endanger the numerical stability of the simulation.} %MM} %PP and MM	
	Note that rational functions, like Eq. \ref{eq:experimentation_tendency}, have the advantage over linear functions that the degree of randomness for exploration decreases faster at the beginning of a simulation run, but it never reaches zero. 
	According to pre-generated simulations, $\beta_S(t)$ should be about $100$ for the supplying division at the very beginning of a simulation run, while, at the end, a value of $20$ is sufficient for the convergence of the Q-function. %MM
	Conversely, for each buying division $j \in \{ 1,2\}$, $\beta_j(t)$ only needs to be half as large. %MM 
	Solving Eq. \ref{eq:experimentation_tendency} with the two associated boundary conditions ($\beta_S(1)=100$ and $\beta_S(T_L)=20$ for the supplying division or rather $\beta_j(1)=50$ and $\beta_j(T_L)=10$ for each buying division $j \in \{ 1,2\}$) leads to the Boltzmann exploration parameters given in Tab. \ref{tab:Parameter_settings_and_scenario_overview}. %MM
	It should be mentioned that balancing the parameters of an exploration policy is one of the most challenging tasks in reinforcement learning \citep{tijsma2016comparing}. %L136pg1 and MM
	
	For the $\epsilon$-greedy exploration policy (see Sec. \ref{sssec:Epsilon_Greedy_Exploration_Policy}), $\epsilon(t)$ is a simple decreasing linear function of the time
	\begin{equation}
		\epsilon(t) = \begin{cases}
		\vspace{3mm}  \epsilon_1 - \epsilon_2 \; t & \text{for $t \leq T_L$} \\
		0 & \text{for $t > T_L$}
		\end{cases}
	\end{equation}		
	with the two properties that, at the beginning of a simulation run, $\epsilon(1)$ is one, which indicates that the action-selection is total random (pure exploration) and, at the end, $\epsilon(T_L)$ is zero (pure exploitation). %MM
	The associated $\epsilon$-greedy exploration parameters $\epsilon_1$ and $\epsilon_2$ are reported in Tab. \ref{tab:Parameter_settings_and_scenario_overview}. %MM
	Finally, for the UCB exploration policy in Sec. \ref{sssec:Upper_Confidence_Bound_Exploration_Policy}, the pre-generated simulations reveal that a value of about $60$ is a good choice for $c_S$, while $c_1$ and $c_2$ only need to be half as large. %MM
	
	
	
	
	
	
	%In order to analyze scenarios with surplus sharing parameters $\Gamma$ ranging from $0.1$ to $0.9$ in steps of $0.1$ (and in order that the first-best and the second-best case have integer solutions for $\Gamma=0.5$), the slope of the inverse demand function $b$ is set to $12$ as well as the expected values of the state variables $\theta_S$ and $\theta_B$ is set to $60$ and $100$, respectively. %MM
	%The corresponding $\lambda_j$ values are given in Table \ref{tab:Parameter_Setting}, whereas the solutions of subgame perfect equilibrium are listed in Table \ref{tab:Firstbest_Secondbest_Solutions}. %MM
	%By the way, scenarios where $\Gamma$ is one (zero) lead, according to Eq. \ref{eq:optimal_I_j} and Eq. \ref{eq:second_best_I_j}, to the result that the supplying division (buying division) chooses first-best investments, but the other division has no incentive to invest as any investment cause divisional investment costs and, at the same time, it does not benefit from the resulting profit. % that are not refundable. %MM
	%Hence, such scenarios are less interesting to analyze %L58pg.3281
	%In such a setting, a fuzzy Q-learning agent with a surplus sharing parameter of zero receives only negative rewards except in the case that it does not make any investments. %MM
	%and, due to the fact that both divisions have the same quadratic cost structure (cf. Eq. \ref{eq:divisional_investment_costs}) and that $\lambda_B=1-\lambda_S$, it is sufficient to study scenarios for $\lambda_S$ ranging from $0.5$ to $0.83$. %MM

	


	%For simulating a negotiated transfer pricing game, %L308pg8
	%This simulation study investigates four scenarios: (1) 
	
	

	%The agent-based simulation is conducted in four main steps: first, the scenario in which both divisions have the same marginal cost parameter is studied, i.e., $\lambda_S=\lambda_B$ and, according to Eq. \ref{eq:Gamma_sb}, this yields to a surplus sharing parameter $\Gamma$ of $0.5$, which means that each division receives the same fraction of the contribution margin achieved. %MM
	%In the next step, the case where the divisional investment costs are not symmetrical is examined, which means that the contribution margin is not divided symmetrically either. %MM
	%A third analysis studies whether the divisions make higher investment decisions, if the surplus sharing parameter $\Gamma$ has different values than the theory of subgame perfection equilibrium recommends. %MM
	%In the first three investigations, the standard deviations of the state variables is set to zero, which implies that there are no stochastic fluctuations that affect the decision-making process, and, in addition, it is supposed that the divisions explore their environment using the Boltzmann exploration policy. %MM
	%In the fourth and final step, a sensitivity analysis with regard to the standard deviations of the state variables and the exploration policies is carried out in order to check whether the observed results also occur in stochastic environments with different exploration settings.\footnote{ %MM
	%A simulation study should include a systematic  sensitivity analysis to verify the stability of the simulation outcomes \citep[e.g.][]{davis2007developing, guerrero2011using}. %L154pg13 
	%}
	%For a better understanding, the parameter settings are successively explained in more detail and  the procedure of the agent-based simulation is illustrated (the exogenous parameters are summarized in Table \ref{tab:Parameter_Setting}, while the procedure of the agent-based simulation is depicted in Fig. \ref{fig:Procedure}). %MM
	
	

	
	%Next, the choice of the action space $\mathcal{A}$ is discussed. %MM 
	%If the number of possible investment decisions is too small, the learning behavior of both divisions cannot be thoroughly investigated, and, on the other hand, if the size of the action space is chosen too large, then learning is slowed. %MM
	%In order to ensure that the first-best investment decisions are in the action space in all examined scenarios, the action space must have at least a minimum size of $50$. %MM	
	%Since it can be guessed that a human decision-maker can only differentiate between a limited number of actions, the stored actions $a_t[i,k] \in \mathcal{A}$ vary between $0$ and $50$ in steps of $5$. %MM
	%Consequently, there are $11$ stored actions for each fuzzy rule $i \in \{ 1,...,N \}$. %MM
	%Recall that, according to Eq. \ref{eq:A_values}, regardless of whether the stored actions have integer values or not, the inferred action $A_t$ (which corresponds to the agent's investment decision) is a real number. %MM

	%\begin{figure}[h!]
	%	\centering
	%	\includegraphics[width=1\textwidth]{Firstbest_Secondbest_Solutions.png}
	%	\caption{First-best and second-best solutions resulting from the concept of subgame perfect equilibrium for $b=12$, $E[\theta_S]=60$, and $E[\theta_B]=100$.} %MM
	%	\label{fig:Firstbest_Secondbest_Solutions}
	%\end{figure}	
	
	%\begin{table}[h!]
	%\tiny
	%\centering
	%\caption{First-best and second-best solutions resulting from the concept of subgame perfect equilibrium for $b=12$, $E[\theta_S]=60$, and $E[\theta_B]=100$.}
	%\label{tab:Firstbest_Secondbest_Solutions}
	%\begin{tabular}{|ccc|cccccc|cccccc|}
	%	\hline	
	%	$\Gamma^*$ & $\lambda_S$ & $\lambda_B$ & $I_S^*$ & $I_B^*$ & $q^*$ & $\Pi_S$ & $\Pi_B$ & $\Pi_{HQ}$ & $I_S^{sb}$ & $I_B^{sb}$ & $q^{sb}$ & $\Pi_S$ & $\Pi_B$ & $\Pi_{HQ}$ \\
	%	\hline
	%	$0.1$ & $0.83$ & $0.17$ & $10$ & $50$ & $8.33$ & $0$ & $166.67$ & $166.67$ & $0.74$ & $33.33$ & $6.17$ & $22.63$ & $113.17$ & $135.80$ \\
	%	$0.2$ & $0.75$ & $0.25$ & $8$ & $24$ & $6$ & $19.2$ & $100.8$ & $120$ & $1.25$ & $15$ & $4.69$ & $25.78$ & $77.34$ & $103.13$ \\
	%	$0.3$ & $0.67$ & $0.33$ & $8$ & $16$ & $5.33$ & $29.87$ & $76.8$ & $106.67$ & $1.90$ & $8.89$ & $4.23$ & $31.04$ & $62.08$ & $93.12$ \\
	%	$0.4$ & $0.58$ & $0.42$ & $8.70$ & $12.17$ & $5.07$ & $39.70$ & $61.75$ & $101.45$ & $2.78$ & $5.83$ & $4.05$ & $37.13$ & $51.99$ & $89.12$ \\
	%	$0.5$ & $0.5$ & $0.5$ & $10$ & $10$ & $5$ & $50$ & $50$ & $100$ & $4$ & $4$ & $4$ & $44$ & $44$ & $88$ \\
	%	\hline
	%	$0.6$ & $0.42$ & $0.58$ & $12.17$ & $8.70$ & $5.07$ & $61.75$ & $39.70$ & $101.45$ & $5.83$ & $2.78$ & $4.05$ & $51.99$ & $37.13$ & $89.12$ \\
	%	$0.7$ & $0.33$ & $0.67$ & $16$ & $8$ & $5.33$ & $76.8$ & $29.87$ & $106.67$ & $8.89$ & $1.90$ & $4.23$ & $62.08$ & $31.04$ & $93.12$ \\
	%	$0.8$ & $0.25$ & $0.75$ & $24$ & $8$ & $6$ & $100.8$ & $19.2$ & $120$ & $15$ & $1.25$ & $4.69$ & $77.34$ & $25.78$ & $103.13$ \\
	%	$0.9$ & $0.17$ & $0.83$ & $50$ & $10$ & $8.33$ & $166.67$ & $0$ & $166.67$ & $33.33$ & $0.74$ & $6.17$ & $113.17$ & $22.63$ & $135.80$ \\
	%	\hline
	%\end{tabular}
	%\end{table}

	

	%Also for the state space $\mathcal{S}$, it is supposed that the number of fuzzy rules should be small. %MM
	%In order to deal with a maximum of $25$ fuzzy rules (concretely, $5$ membership functions in each state space dimension), the values of the state space range from $0$ to $50$ by a step size of $12.5$. %MM
	%In order to deal with $5$ membership functions in each state space dimension, the values of the state space range from $0$ to $50$ by a step size of $12.5$; thus, there are $25$ fuzzy rules in total. %MM  
	%According to \cites{zimmermann2011fuzzy} suggestions, triangular membership functions are applied (cf. Fig. \ref{fig:Triangular_Membership_Functions}) because they are simple, have low computing power per iteration, and are easy to interpret. %%https://www.sciencedirect.com/science/article/pii/S095741740900044X   because... L288pg.3
	%For example, if the state of the supplying division $s_{t,S}$ is $10$, then the membership grade $\mu_{L_S^1}$ of fuzzy rule $1$ is $0.2$, whereas the membership grade $\mu_{L_S^2}$ of fuzzy rule $2$ is $0.8$, and all other membership grades $\mu_{L_S^3}$, $\mu_{L_S^4}$, and $\mu_{L_S^5}$ are zero, or in other words, the supplying division is in the ``very low investment range'' with a fraction of $20\%$ and in the ``low investment range'' with a share of $80\%$. %MM
	%The example shows one of the main strengths of the fuzzy sets, namely, that a fuzzy Q-learning agent does not always belong to exactly one ``investment range'', but often to two. %MM
	%This is particularly useful, if the agent is unsure of its current ``investment state''. %MM
	%Incidentally, increasing the number of fuzzy rules in the parameter settings just means that the agents need more time to learn their environment; their decision-making behavior can be exhibited quite well with only $5$ membership functions. %MM
	%Recall, the state $s_{t,S}$ represents the last investment decision made by the supplying division and the state $s_{t,B}$ denotes the buying division's investment from the previous time step, i.e., $s_{t,S} = A_{t-1}(\pmb{s}_{t-1}, \pmb{a}_{t-1})$ and $s_{t,B} = A_{t-1}(\pmb{s}_{t-1}, \pmb{a}_{t-1})$. %L58 as well as its competitors' joint quantities from the previous time step.
	%Note, according to the triangular membership functions in Fig. \ref{fig:Triangular_Membership_Functions}, up to $4$ q-values can be updated simultaneously per time step ($2$ raised to the power of $dim(\mathcal{S})$). %MM 	
	
	
	%\begin{figure}[h!]
	%	\centering
	%	\includegraphics[width=0.85\textwidth]{Triangular_Membership_Functions.png}
	%	\caption{The fuzzy sets considered here consist of $5$ triangular membership functions in each state space dimension $j \in \{S,B\}$, which can be characterized by linguistic labels ranging from ``Very low'' to ``Very high''.}
	%	\label{fig:Triangular_Membership_Functions}
	%\end{figure}
	
	
	

	
	
	%\vspace{10mm}
	%Due to the fact that the divisions have a memory of the played history, the state space $\mathcal{S}$ of each division consists of its last investment decision as well as its competitors' investment from the previous time step, %L58 as well as its competitors' joint quantities from the previous time step.
	%thus, $S=A^2$. % has the same value range as $A$. %MM
	%As a consequence, there are five fuzzy rules in each dimension, which means that each fuzzy Q-learning agent has to store $275$ $(=|A|*|S|^2=11*5^2)$ q-values. %MM
	%Since each agent has $275$ different q-values and the action selection of the investments is widely determined randomly (Boltzmann distribution), the number of time steps is set to $1,\hspace{-0.6mm}000$ in order that the number of average visits to each state-action pair is approximately $4$ times, if actions are uniformly distributed. %MM
	%\footnote{On average, all agents observe each state-action pair approximately $5$ times, taking into account that actions are approximately uniformly distributed in the first third of the game.} %Wall und MM
	%The learning behavior of agents is modeled by Q-learning (for off-policy learning) and SARSA-learning (for on-policy learning). %L58
	%Finally, the state variable $\theta_S$ ($\theta_B$) is captured in a normally distributed random variable with mean $60$ ($100$) and standard deviation $1$ ($1$). %MM
	



	%\vspace{10mm}
	%Besides the fixed exogenous parameters, we vary the learning rate $\alpha$ and the discount factor $\gamma$ which are also set exogenously at the very beginning of a simulation run. %MM
	%In particular, we model the learning rate $\alpha$ between $0.1$ and $1$ in steps of $0.1$ and we vary the discount factor $\gamma$ from $0$ to $0.9$ in steps of $0.1$ in order to examine the long-term behavior of our reinforcement learning agents.



	%As the long-term behavior of reinforcement learning agents is examined ($\beta \to 0$), the learning rate $\alpha$ is set to $0.1$ to get a good balance between taking new information and overwriting old information. %MM
	%A value of zero indicates that the agent cannot learn new information, while a value of one causes the agent to consider only the latest information \citep{sutton2018reinforcement}. %Wiki
	%Incidentally, a learning rate of one is only optimal in deterministic environments. %Wiki
	%Bowling and Veloso \citep{Bowling_Veloso_2002} use a variable learning rate to make rational learning algorithms in general stochastic games convergent. %and show empirical results of their algorithm. %L120 pg.1

	%The discount factor $\gamma$ determines the importance of future rewards and is set to zero for myopic agents which means that the agents cannot observe future rewards. %PP
	%Contrary, for non-myopic agents, the discount factor is set to $0.8$ which implies that the agents strive for a long-term high reward. %Wiki
	
	%New: The first parameters are control variables and, hence, they are fix during the simulation runs. %L300pg.29 und MM



\newpage
\section{Results and discussion}
\label{sec:Results_and_Discussion}

	The analysis of results is structured in two parts: %L308pg10
	(1) results for symmetric marginal cost parameters $\lambda_1=\lambda_2$ (called symmetric scenarios) and (2) results for non-symmetric marginal cost parameters (called non-symmetric scenarios). %MM
	For a better overview, symmetric and non-symmetric scenarios are additionally divided into four and two sub-scenarios, respectively. %MM  
	An overview of the examined scenarios is given in Tab. \ref{tab:Parameter_settings_and_scenario_overview}. %MM
	In the following, the supplying division, the 1st buying division, and the 2nd buying division are referred to as seller, first buyer, and second buyer, respectively. %MM
	In addition, this simulation study distinguishes between an all-knowing headquarters which sets the surplus sharing parameters according to Eq. \ref{eq:Gamma_1_sb} and Eq. \ref{eq:Gamma_2_sb}, and a headquarters which cannot solve the $\Gamma$-choice problem analytically and, therefore, chooses $\Gamma_j$, $j \in \{ 1,2 \}$, according to Tab. \ref{tab:Parameter_settings_and_scenario_overview}. %MM
	


	\subsection{Results for symmetric marginal cost parameters}

		In scenarios I-IV, $\lambda_S=1$ and $\lambda_1=\lambda_2$ which yields to $\Gamma_1=\Gamma_2$. %MM
		%In order to compare the simulation results with \textcolor{red}{meinem Paper 1} where only one buyer is involved, $\lambda_S$ has to set to one. %MM
		In scenarios I-III, the exploration policy is the Boltzmann exploration policy and the underlying market environment is deterministic. %MM
		Finally, scenario IV performs an extensive sensitivity analysis in order to verify whether the results for symmetric marginal cost parameters are robust to changes in volatility of the markets and exploration policy. %Paper1pg.25



	\subsubsection{Scenario I}
	
		In the first scenario, high investment costs are assumed on the downstream divisions. %MM
		Concretely, $\lambda_1$ and $\lambda_2$ are set to $0.5$ so that the simulation results can also be compared to \cites{mitsch2023impact} previous simulation study where only one buyer is involved. %MM
		%Since $\lambda_S$ is one, the simulation results can compare to \textcolor{red}{meinem Paper 1} where only one buyer is involved. %MM
		If $\lambda_1=\lambda_2=0.5$, then an all-knowing headquarters would set $\Gamma_1$ and $\Gamma_2$ to $0.5$, according to Eq. \ref{eq:Gamma_1_sb} and Eq. \ref{eq:Gamma_2_sb}. %MM
		%According to Eq. \ref{eq:Gamma_1_sb} and \ref{eq:Gamma_2_sb}, $\Gamma_1$ and $\Gamma_2$ are also $0.5$. %MM	
		To get an understanding of the decision-making behavior of fuzzy Q-learning agents, the simulation model studies investment decisions with two extreme levels of no ($\gamma=0$) and high ($\gamma=0.9$) foresight of future rewards. %foresight or prospect. %MM and L331pg.15
		
		\begin{figure}[t!]
			\centering
			\includegraphics[width=0.942\textwidth]{Plot_01a.png} \\ %\includegraphics[width=0.434\textwidth]{Plot_02a.png} \\
			%\vspace{1.5mm}
			\includegraphics[width=0.5\textwidth]{Plot_01b.png} \includegraphics[width=0.434\textwidth]{Plot_02b.png} \\
			%\vspace{1.5mm}
			\includegraphics[width=0.5\textwidth]{Plot_01c.png} \includegraphics[width=0.434\textwidth]{Plot_02c.png} \\
			%\vspace{1.5mm}
			\includegraphics[width=0.5\textwidth]{Plot_01d.png} \includegraphics[width=0.434\textwidth]{Plot_02d.png}
			\caption{Modified boxplots of $I_S$, $I_1$, $I_2$, and $\Pi_{HQ}$ for myopic ($\gamma=0$) and non-myopic ($\gamma=0.9$) fuzzy Q-learning agents with $\lambda_S=1$, $\lambda_1=\lambda_2=0.5$, $\Gamma_1=\Gamma_2=0.5$, $\sigma=0$, and exploration policy is Boltzmann.}
			\label{fig:Plot_01}
		\end{figure}
		
		The simulation results of investment decisions and, what profits the headquarters can expect, are presented with modified boxplots (see Fig. \ref{fig:Plot_01}). %L331pg.15
		In addition, first- and second-best solutions are also displayed. %MM
		Be aware that the thick black line in the modified boxplots represents the median of $10,\hspace{-0.6mm}000$ observations per time step and, due to the fact that the mean value is not robust against outliers, the mean value of the headquarters' profit is below the median value in case of a negative skewness (roughly spoken, the mass of the distribution is concentrated on the ``right side''). %MM 
		Additionally, note that, in the modified boxplots used here, the distance between the 25th and 75th percentile is visualized in gray, the outer black lines denote the whiskers, and, for an improved readability, outliers beyond the wiskers are not depicted. %Internet and MM	
	
		For myopic fuzzy Q-learning agents (see left plots in Fig. \ref{fig:Plot_01}), the simulation results indicate that the investment decisions of all divisions converge towards the second-best solution. %MM and L331pg.15
		In the case where all divisions seek for high future rewards (see right plots in Fig. \ref{fig:Plot_01}), the investment decisions converge towards the first-best solution. %MM
		Similarly, the headquarters' profit approaches the second-best solution and the first-best solution for myopic and non-myopic agents, respectively. %MM 
		%Note that there is no stochastic influence on the markets, so the headquarters' profit depends solely on the divisions' investment decisions. %MM
		
		On looking more closely at the last $100$ investment decisions over $10,\hspace{-0.6mm}000$ simulation runs, the investment decisions of myopic (non-myopic) agents are normally distributed with mean $5.13$ ($10.02$) and standard deviation $0.44$ ($2.62$). 
		Moreover, the headquarters' profit for myopic (non-myopic) agents is nearly normally distributed with mean $177.9$ ($189.8$) and standard deviation $2.12$ ($6.37$), whereby the distribution of the headquarters' profit has a slightly negative skewness of $-0.21$ ($-0.98$) for myopic (non-myopic) agents. %MM
		
		The findings of scenario I show that myopic fuzzy Q-learning agents invest about as much as fully individual rational utility maximizers in the classic hold-up problem (cf. Tab. \ref{tab:Firstbest_Secondbest_Solutions} in Appendix A), whereas non-myopic agents invest optimally. %MM
		%Since there is no stochastic influence in this scenario setting, the headquarters' profit depends solely on the divisions' investment decisions.} %MM
		%Concretely, both myopic divisions choose an investment of $5.04$ on average which yields a headquarters' profit of $87.9$ on average. %MM		
		%Having a high level of foresight to strive for high future rewards increases divisions' investment decisions and, in turn, rises the headquarters' profit. %%L308pg11 and MM
		With a high level of foresight to strive for high future rewards, the investment level of each division increases on the one hand and the headquarters' profit on the other. %%L308pg11 and MM



	\subsubsection{Scenario II}
	
		In the second scenario, the marginal cost parameters $\lambda_j$, $j \in  \{ 1,2\}$, are set to $0.222$. % which yields to $\Gamma_1 = \Gamma_2 = 0.25$. %MM
		In the case that both buying divisions have low investment costs, an all-knowing headquarters would choose $\Gamma_1 = \Gamma_2 = 0.25$. %MM
		The simulation results are depicted in the left plot for myopic agents and in the right plot for non-myopic agents in Fig. \ref{fig:Plot_03}. %MM
		As the output shows, myopic fuzzy Q-learning agents achieve second-best results, while non-myopic agents realize better results, but the first-best solution is not reached. %MM
		
		A closer analysis reveals that the investment decisions of myopic (non-myopic) sellers are nearly normally distributed with mean $4.27$ ($6.82$) and standard deviation $0.5$ ($1.78$) with a positive (negative) skewness of $0.48$ ($-0.25$). %Paper1pg28
		On the downside, the investment decisions of myopic (non-myopic) buyers are normally distributed with mean $18.41$ ($25.24$) and standard deviation $1.53$ ($5.02$). %Paper1pg28
		In the case of myopic (non-myopic) agents, the headquarters' profit is (almost) normally distributed with mean $233.7$ ($258.5$) and standard deviation $3.58$ ($11.5$) with a negative skewness of $-0.02$ ($-0.63$). %Paper1pg28		
		
		The findings of scenario II indicate that divisions' investment decisions of myopic fuzzy Q-learning agents are close to the second-best solutions (cf. Tab. \ref{tab:Firstbest_Secondbest_Solutions} in Appendix A), but compared to scenario I, non-myopic agents generate lower profits for themselves as well as for their headquarters. %MM 
		A high degree of foresight favors divisions' investment decisions, however, on average, first-best investment decisions are not made. %MM 

		\begin{figure}[t!]
			\centering
			\includegraphics[width=0.942\textwidth]{Plot_03a.png} \\ %\includegraphics[width=0.434\textwidth]{Plot_04a.png} \\
			%\vspace{1.5mm}
			\includegraphics[width=0.5\textwidth]{Plot_03b.png} \includegraphics[width=0.434\textwidth]{Plot_04b.png} \\
			%\vspace{1.5mm}
			\includegraphics[width=0.5\textwidth]{Plot_03c.png} \includegraphics[width=0.434\textwidth]{Plot_04c.png} \\
			%\vspace{1.5mm}
			\includegraphics[width=0.5\textwidth]{Plot_03d.png} \includegraphics[width=0.434\textwidth]{Plot_04d.png}
			\caption{Modified boxplots of $I_S$, $I_1$, $I_2$, and $\Pi_{HQ}$ for myopic ($\gamma=0$) and non-myopic ($\gamma=0.9$) fuzzy Q-learning agents with $\lambda_S=1$, $\lambda_1=\lambda_2=0.222$, $\Gamma_1=\Gamma_2=0.25$, $\sigma=0$, and exploration policy is Boltzmann.}
			\label{fig:Plot_03}
		\end{figure}	
			

	
	\subsubsection{Scenario III}
	
		In order to get a general understanding of the effects of different values of surplus sharing parameters and discount factors, $\Gamma_j$ and $\gamma_j$, $j \in \{ 1,2\}$, are varied from $0.25$ to $0.55$ in $0.05$ increments and from $0$ to $0.9$ in $0.1$ increments, respectively.   	%L308pg10	 
		Note that, in the symmetric case, it is sufficient to vary $\Gamma_j, j \in \{ 1,2\}$, from $0.25$ to $0.55$ to adequately study the divisions' decision-making behavior. %MM		
		%Note that due to symmetry considerations it is sufficient to present the results for the parameter in the range 0.55 to 0.25. %MM		
		The simulation results are summarized in Fig. \ref{fig:Plot_05} and Fig. \ref{fig:Plot_06} with four subplots for $\lambda_1=\lambda_2=0.5$ (high investment costs) and for $\lambda_1=\lambda_2=0.222$ (low investment costs), respectively. %MM
		In the following, the meaning of the subplots is explained. %MM
	
		The top left subplot displays contours representing the headquarters' profits resulting from the simulation. %L331pg19
		The bottom left subplot depicts the ``first-best performance indicator'' for fuzzy Q-learning agents, which is defined by $\Pi_{HQ} / \Pi_{HQ}^*$, i.e., the headquarters' profit obtained from the simulation is normalized by the highest feasible profit that can be achieved. %L331pg15
		The first-best performance indicator can serve as a relative indicator for the profit-effectiveness of the decision-making behavior of fuzzy Q-learning agents; the higher the value, the better the profitability for the headquarters. %L308pg11
		%For example, if the first-best performance indicator reaches one, then divisions make optimal investment decisions and, in this way, the divisions as well as the headquarters achieve the maximum profit. %MM
		%Another performance indicator is visualized in the bottom right subplot. %MM 
		The bottom right subplot shows the ``second-best performance indicator'', which reflects how much better fuzzy Q-learning agents perform than fully rational utility maximizers. %MM
		This second-best performance indicator is formalized by the relative change between $\Pi_{HQ}$ and $\Pi_{HQ}^{sb}$ dividing by $\Pi_{HQ}^{sb}$; the higher the relative change, the higher the performance of fuzzy Q-learning agents. %MM
		
		Lastly, and more importantly, the so-called ``baseline performance indicator'', which is displayed in the top right subplot. 
		The baseline performance indicator is based on the relative change between $\Pi_{HQ}^{\Gamma_j}$ and $\Pi_{HQ}^{baseline}$ dividing by $\Pi_{HQ}^{baseline}$, where $\Pi_{HQ}^{\Gamma_j}$ denotes the headquarters' profit resulting from scenario where the headquarters uses $\Gamma_j$, while $\Pi_{HQ}^{baseline}$ describes the headquarters' profit resulting from scenario where the headquarters applies $\Gamma_j^{sb}$. %MM
		The scenario in which the headquarters uses the optimal surplus sharing parameter $\Gamma_j^{sb}$ is called the ``baseline scenario''. %MM
		Note that in a baseline scenario, the headquarters knows all information from its divisions to determine the optimal level of bargaining power and, therefore, the headquarters can set the bargaining power $\Gamma_1$ and $\Gamma_2$ according to Eq. \ref{eq:Gamma_1_sb} and \ref{eq:Gamma_2_sb}, respectively. %MM
		So, this indicator provides information about how much better fuzzy Q-learning agents given $\Gamma_j$ perform than fuzzy Q-learning agents given $\Gamma_j^{sb}$ or, in other words, whether the profit of a headquarters that does not know the exact optimal surplus sharing parameters is higher than the profit of a headquarters that applies the optimal surplus sharing parameters. %MM



\newpage
		In order to find out whether the baseline performance indicator is significant (i.e., if there is a significant difference between $\Pi_{HQ}^{\Gamma_j}$ and $\Pi_{HQ}^{baseline}$), Welch's t-tests and Wilcoxon rank-sum tests are applied.\footnote{
		\cites{welch1947generalization} t-test and Wilcoxon rank-sum test \citep{mann1947test} are widely common statistical hypothesis tests.
		In the default case, the first one tests the equality of the means for two independent normally distributed samples with unequal and unknown variances, while the second one, a nonparametric test, tests the null hypothesis that the distributions of two independent samples differ by a location shift of zero.} % or, roughly spoken, that they have the same distribution.} %P2
		Since the p-values of hypothesis tests go quickly to zero when very large samples ($10,\hspace{-0.6mm}000$ observations and more) are evaluated, the null hypotheses become statistically significant because the standard errors become extremely small \citep{lin2013research}. 
		To mitigate this phenomenon, one-tailed Welch's t-tests and Wilcoxon rank-sum tests are used with a positive hypothesized mean difference (hereafter abbreviated to $d_H$). %MM
		The null hypotheses to be tested are given by $mean(\Pi_{HQ}^{\Gamma_j})-mean(\Pi_{HQ}^{baseline}) \geq d_H$.
		%\footnote{
		%If necessary, the two samples are simply swapped in their order to ensure an absolute difference in means because both tests are designed for null hypotheses of the form: $mean(Bi\_0)-mean(Bj\_0) \leq d_H$ for $i\neq j$.} %MM
		Be aware that the Welch's t-test is designed for normally distributed samples but, due to the central limit theorem, it can be assumed that this also holds for the headquarters' profits which are nearly normally distributed. %MM

	\begin{figure}[t!]
		\centering
		\includegraphics[width=1\textwidth]{Plot_05}
		\caption{Results for symmetric marginal cost parameters with $\sigma=0$ and exploration policy is Boltzmann: (a) absolute performance of the headquarters' profit resulting from the simulation, (b) relative performance change and statistical hypothesis testing of $\Pi_{HQ}$ compared to the simulation output from the baseline scenario with $\Gamma_1^{sb}=0.5$, (c) relative performance of $\Pi_{HQ}$ compared to $\Pi_{HQ}^*$, and (d) relative performance change of $\Pi_{HQ}$ compared to $\Pi_{HQ}^{sb}$.} 
		\label{fig:Plot_05}
	\end{figure}	

		In order to find out, if the headquarters' profits differ significantly in terms of their means (see Welch's t-tests) or their distributions (see Wilcoxon rank-sum tests), the hypothesized mean difference $d_H$ is set to $\Pi_{HQ}^{baseline} / 100$, which can be interpreted as $1\%$ of the headquarters' profit achieved in the baseline scenario. %MM
		Therefore, whenever the t-test result is statistically significant, fuzzy Q-learning agents perform at least $1\%$ better in scenario with $\Gamma_j$ than fuzzy Q-learning agents in the baseline scenario with $\Gamma_j^{sb}$ given a standard significance level of $0.05$. %MM
		To better distinguish the test results graphically, the baseline performance indicator is colored as follows: If the p-value of the t-test (rank-sum test) is greater than $0.05$, the cell turns yellow (blue). %MM
		If both tests are statistically significant, the cell is green; otherwise the cell is white. %MM 
		
		The simulation results on the performance of fuzzy Q-learning agents with $\lambda_1=\lambda_2=0.5$ and $\lambda_1=\lambda_2=0.222$ are presented in Fig. \ref{fig:Plot_05} and Fig. \ref{fig:Plot_06}, respectively. %L331pg15
		According to subplot (a) in Fig. \ref{fig:Plot_05}, non-myopic fuzzy Q-learning agents tend to generate higher profits for their headquarters compared to myopic fuzzy Q-learning agents. %MM
		In cases where seller and buyer do not have equal bargaining power, i.e., $\Gamma_j \neq 0.5$, $j \in \{ 1,2 \}$, the headquarters' profit decreases. %Paper1pg
		In the extreme case with a non-symmetrical $\Gamma$-surplus sharing rule, $\Gamma_j=0.25$, $j \in \{ 1,2 \}$, non-myopic agents perform worse than myopic agents. %under a non-symmetrical $\Gamma$-surplus sharing rule. %Paper1pg
		
	\begin{figure}[b!]
		\centering
		\includegraphics[width=1\textwidth]{Plot_06.png}
		\caption{Results for symmetric marginal cost parameters with $\sigma=0$ and exploration policy is Boltzmann: (a) absolute performance of the headquarters' profit resulting from the simulation, (b) relative performance change and statistical hypothesis testing of $\Pi_{HQ}$ compared to the simulation output from the baseline scenario with $\Gamma_1^{sb} \hspace{-0.75mm} =0.25$, (c) relative performance of $\Pi_{HQ}$ compared to $\Pi_{HQ}^*$, and (d) relative performance change of $\Pi_{HQ}$ compared to $\Pi_{HQ}^{sb}$.}
		\label{fig:Plot_06}
	\end{figure}
	
		The efficiency of fuzzy Q-learning agents can be seen in the bottom contour plots (c) and (d). %Paper1pg
		According to subplot (c) in Fig. \ref{fig:Plot_05}, the first-best performance indicator shows that the profit-effectiveness of the headquarters is relatively high, especially for non-myopic agents. %Paper1pg
		%The second-best performance indicator on the other side points out that fuzzy Q-learning agents outperform fully rational utility maximizers in almost all scenarios with a non-symmetrical bargaining power. % between seller and buyer. %Paper1pg
		The second-best performance indicator on the other side points out that fuzzy Q-learning agents slightly outperform fully rational utility maximizers in all investigated scenarios. % with a non-symmetrical bargaining power. % between seller and buyer. %Paper1pg
		%This results from the agents' learning phase in which the agents consider not only the current investments and profits, but also the future ones. %MM
		%\textcolor{red}{HIER!}
		%This results from the agents' learning phase in which the agents start without any information about...%
		This results from the agents' learning phase in which learning begins without prior knowledge of the consequences of agents' actions (all q-values are initialized to zero). %MM and L139pg27
		The timelines in all modified boxplots show the same picture: Agents start with a moderate level of investment, which slowly decreases to its new equilibrium level (sometimes faster and sometimes slower). %MM
		When learning the Q-values, the agents will get higher rewards as they make higher investments. %MM
		But on the other hand, if too high investments are made, profits will decrease. %MM
		%Also, if an agent chooses a high investment level and another chooses a low one, the profit of the first agent is reduced while the profit of th.
		%Also, if only one-sided investments are made, the investor's profit is reduced while the other agents benefit from these investments. %MM
		Also, if only one-sided investments are made, the agent's own profit is reduced while the other agents benefit from these investments. %MM
		Overall, fuzzy Q-learning agents tend to invest more than the benchmark of the second-best solution indicates. %  the theory of subgame perfection equilibrium predicts. %MM 
		Note that pre-generated simulations reveal that, regardless of the starting level of investments, the Q-functions converge to their final values which are slightly higher than the expected values resulting from fully rational utility maximizers. %MM
		
		Finally, to test whether the headquarters' profit is significantly different for fuzzy Q-learning agents with $\Gamma_j$ or $\Gamma_j^{sb}$, the baseline performance indicator is calculated. %MM
		Based on the baseline performance indicator (see subplot (b) in Fig. \ref{fig:Plot_05}), it may be inferred that, for symmetric surplus sharing parameters, $\Gamma_j=0.5$, $j \in \{ 1,2 \}$, a deviation from the recommended optimal solutions of subgame perfection equilibrium leads to worse results. %Paper1pg 
		Therefore, the headquarters should follow the theory of subgame perfection equilibrium and should give all divisions the same bargaining power, or in other words, the headquarters should use a fifty-fifty surplus sharing rule. %set/distribute the bargaining power to equal shares. %MM 
		%Therefore, the headquarters should follow the recommendations based on the transfer pricing literature, e.g., \cite{ewert2014interne}, \cite{gox2006economic}, and \cite{wagner2008corporate}, and should give all divisions equal bargaining power. %set/distribute the bargaining power to equal shares. %MM 
		
		The simulation results for $\lambda_1=\lambda_2=0.222$ (low investment costs) show a similar picture. %MM
		The absolute performance of the headquarters' profit rises with increasing degree of agents' foresight (cf. subplot (a) in Fig. \ref{fig:Plot_06}). %MM
		First- and second-best performance indicators indicate that fuzzy Q-learning agents generate relatively high profits, although, again, a high level of foresight favors agents' investment decisions. %MM
		According to subplot (b) in Fig. \ref{fig:Plot_06}, the results indicate that, for agents with a low discount factor $\gamma$ between $0$ and $0.4$, the headquarters should set $\Gamma_j$, $j \in \{ 1,2 \}$, to $\Gamma_j^{sb}$ for maximum profits. %Paper1pg
		However, for agents with a high foresight greater than $0.4$, the headquarters should empower the seller with greater bargaining power in order to achieve higher profits. %MM
		A closer look reveals that, in $19$ cases, Welch's t-test and Wilcoxon rank-sum test show a significant difference between $\Pi_{HQ}^{\Gamma_j}$ and $\Pi_{HQ}^{baseline}$, when the seller obtains a bargaining power which is greater than the recommended optimal surplus sharing rule resulting from solving the subgame perfect equilibrium. %MM
		
		In order to get a rule of thumb for determining the ideal surplus sharing rule, the weighted arithmetic mean of $\Gamma_j$ may be used. %Paper1pg
		For $j \in \{ 1,2 \}$, the weighted arithmetic mean of $\Gamma_j$ is given by the double series over all pairs of $\Gamma_j$ and $\gamma$, where the baseline performance indicator $BPI$ is significant: $\sum_{\Gamma_j=0.25}^{0.55} \sum_{\gamma=0}^{0.9} \Gamma_j \cdot BPI[\Gamma_j,\gamma] / \sum_{\Gamma_j=0.25}^{0.55} \sum_{\gamma=0}^{0.9} BPI[\Gamma_j,\gamma]$. %MM
 		According to subplot (b) in Fig. \ref{fig:Plot_06}, the weighted arithmetic mean of $\Gamma_j$ is $0.386$, which can be seen as a good choice for agents with a high degree of foresight between $0.5$ and $0.9$. %MM
 		%Be aware that, in general, the headquarters cannot optimally set the bargaining power according to Eq. \ref{eq:Gamma_1_sb} and \ref{eq:Gamma_2_sb} due to a lack of information. %L308pg17	
		%This rule of thumb can be particularly relevant because, in general, the headquarters cannot optimally determine the bargaining power, as, e.g., information required to solve the three-stage decision problem is often missing. %L308pg17	
		%Therefore, simple methods for determining reference values for the distribution ratio of the bargaining power between seller and buyer are desirable. %MM
		%The results are that... in most parameter constellations, the decision-making behavior of the fuzzy Q-learning agents is much better than the predicted behavior of fully rational utility maximizers. %Paper1pg
		An $1:2$ surplus sharing rule for seller and buyer may therefore be a good choice for non-myopic agents when the headquarters cannot determine $\Gamma_j$ analytically. %MM
			
			
			
	\subsubsection{Scenario IV}
	
		To verify whether the simulation results are robust in terms of volatility on the markets and the implementation of other exploration policies, an extensive sensitivity analysis is performed (see figures in Appendix B). %Paper1pg35 \ref{appendix_B:Sensitivity_analysis}
		To evaluate the robustness of stochastic fluctuations on the markets, the standard deviations of the state variables $\sigma$ vary from $0$ to $10$ in $5$ increments. %MM
		In addition to the Boltzmann exploration policy, the $\epsilon$-greedy and
the upper confidence bound exploration policy, which are introduced in Sec.
\ref{ss:Exploration_Polic}, are applied. %Paper1pg35

	In the case of symmetric marginal cost parameters, the learning behavior of fuzzy Q-learning agents seems to be robust against volatilities on the markets. %Paper1pg36
	Also, all three exploration policies show a similar picture, whereby fuzzy Q-learning agents using the $\epsilon$-greedy exploration policy performing worst while agents applying Boltzmann performing best. %Paper1pg36 
	In scenarios where $\lambda_1=\lambda_2=0.5$ (see Fig. \ref{fig:Plot_08}), a symmetric $\Gamma$-surplus sharing rule leads to high profits, a deviation from it results in lower profits and should therefore not be pursued.  %MM 
	%If the marginal cost parameters $\lambda_j$, $j \in \{ S,1,2\}$, differ only slightly (see Fig. \ref{fig:Plot_16}), then 
	More importantly, the simulation results for $\lambda_1=\lambda_2<0.361$ (low investment costs) suggest that the seller should have a greater bargaining power in order to increase the headquarters' profit, especially when agents have a high foresight of future rewards. %MM 
	%In such cases, the headquarters should allocate the distribution ratio of the bargaining power based on the calculation of the weighted arithmetic mean. %Paper1pg36
	In such cases, the headquarters should allocate the distribution ratio of the bargaining power at a ratio of $1:2$ for seller and buyer. %Paper1pg36 and MM
	

	\subsection{Results for non-symmetric marginal cost parameters}
	
		Again, $\lambda_S$ is set to one, the exploration policy is the Boltzmann exploration policy, and the underlying market environment is deterministic. %as in scenarios I-IV. %MM
		%In scenarios with non-symmetric marginal cost parameters, $\lambda_1$ is systematically increased from $0.534$ to $0.621$ in small increments, while $\lambda_2$ is decreased from $0.463$ to $0.301$. %MM
		In scenario V, the marginal cost parameter $\lambda_1$ is systematically increased from $0.534$ to $0.621$ in small increments, while $\lambda_2$ is decreased from $0.463$ to $0.301$ (cf. Tab. \ref{tab:Parameter_settings_and_scenario_overview}). %MM
		Also the surplus sharing parameters $\Gamma_1$ and $\Gamma_2$ differ accordingly. %MM
		Conclusively, scenario VI conducts an extensive sensitivity analysis in order to verify whether the results for non-symmetric marginal cost parameters are robust to changes in volatility of the markets and exploration policy. %Paper1pg.25
	
		\vspace{-2mm}
		\begin{figure}[b!]
			\centering
			\includegraphics[width=1\textwidth]{Plot_35}
			\caption{Results for non-symmetric marginal cost parameters with $\sigma=0$ and exploration policy is BM: (a) absolute performance of the headquarters' profit resulting from the simulation, (b) relative performance change and statistical hypothesis testing of $\Pi_{HQ}$ compared to the simulation output from the baseline scenario with $\Gamma_1^{sb} \hspace{-0.75mm} =0.55$, (c) relative performance of $\Pi_{HQ}$ compared to $\Pi_{HQ}^*$, and (d) relative performance change of $\Pi_{HQ}$ compared to $\Pi_{HQ}^{sb}$.} 
			\label{fig:Plot_35}
		\end{figure}
		
		
	
		\subsubsection{Scenario V}
		
			%In order to study the divisions' decision-making behavior in case of non-symmetric marginal cost parameters, it is sufficient to vary $\Gamma_1$ from $0.5$ to $0.75$. %MM
		
			Due to symmetry considerations, it is sufficient to vary $\Gamma_1$ from $0.5$ to $0.75$ in order to study the agents' decision-making behavior in case of non-symmetric marginal cost parameters (cf. Tab. \ref{tab:Parameter_settings_and_scenario_overview}). %MM
			%Note that, due to symmetry considerations, it is sufficient to present the results for the parameter in the range 0.55 to 0.25. %MM
			%Please notice that, in non-symmetric scenarios, $\Gamma_2=1-\Gamma_1$.
			Note that, in scenarios I-IV, $\Gamma_1=\Gamma_2$, while, in scenarios V-VI, $\Gamma_2=1-\Gamma_1$. %MM
			%For an improved readability of the results, only 2-dimensional plots are used, where $\Gamma_1$ being shown on the Y-axis. %L331pg13 and MM
			%For the sake of simplicity, $\Gamma_2=1-\Gamma_1$ in all investigated scenarios and, for an improved readability of the plots, only $\Gamma_1$ is depicted on the Y-axis. %L331pg13 and MM
			For $\lambda_1=0.534$ and $\lambda_2=0.463$ (small difference between investment costs), the simulation results on the performance of fuzzy Q-learning agents are summarized in Fig. \ref{fig:Plot_35}. 
			For $\lambda_1=0.621$ and $\lambda_2=0.301$ (large difference between investment costs), the simulation outputs are depicted in Fig. \ref{fig:Plot_36}. %L331pg21 and MM
			
		\begin{figure}[b!]
			\centering
			\includegraphics[width=1\textwidth]{Plot_36.png}
			\caption{Results for non-symmetric marginal cost parameters with $\sigma=0$ and exploration policy is BM: (a) absolute performance of the headquarters' profit resulting from the simulation, (b) relative performance change and statistical hypothesis testing of $\Pi_{HQ}$ compared to the simulation output from the baseline scenario with $\Gamma_1^{sb} \hspace{-0.75mm} =0.75$, (c) relative performance of $\Pi_{HQ}$ compared to $\Pi_{HQ}^*$, and (d) relative performance change of $\Pi_{HQ}$ compared to $\Pi_{HQ}^{sb}$.}
			\label{fig:Plot_36}
		\end{figure}
		
			If the marginal cost parameters differ only slightly (cf. Fig. \ref{fig:Plot_35}), a deviation from the theory of subgame perfection equilibrium leads to a lower investment level and, thus, lower profits are made. %Paper2
			As discussed in the symmetric case, the headquarters should set the surplus sharing parameters $\Gamma_1$ and $\Gamma_2$ according to Eq. \ref{eq:Gamma_1_sb} and Eq. \ref{eq:Gamma_2_sb}, respectively, in order to maximize profits. %L331pg21
			Furthermore, the simulation results show that the higher the agents' level of foresight, the higher the generated profits. %MM
			Moreover, the headquarters' profit as well as the divisions' investment decisions are almost normally distributed in all parameter constellations investigated. %MM
			
			According to Fig. \ref{fig:Plot_36}, a deviation from $\Gamma_1^{sb}$ can actually lead to higher profits, especially, if divisions have a high degree of foresight. %MM
			The simulation results provide support the intuition that all divisions should receive approximately the same share of the contribution margin. %L331pg17
			In particular, in situations in which the headquarters only have poor information, e.g., about the costs of manufacturing, the headquarters relies on simple methods for determining decision problems in transfer pricing. %L331pg22			
			%Here, too, the weighted arithmetic mean of $\Gamma_j$ may be a good choice for determining the surplus sharing parameter when divisions' marginal cost parameters differ. %MM
			According to the weighted arithmetic mean of $\Gamma_j$ ($\Gamma_1=0.586$ and $\Gamma_2=0.414$), a $3:2$ surplus sharing rule for seller and first buyer and a distribution ratio of the bargaining power at a ratio of $2:3$ for seller and second buyer may be good choices for determining the surplus sharing parameters when divisions' marginal cost parameters differ widely. %MM
		
		

		\subsubsection{Scenario VI}
		
			As in scenario IV, an extensive sensitivity analysis is conducted to check whether the simulation results for non-symmetric marginal cost parameters are robust in terms of volatility on the markets and the implementation of other exploration policies. %Paper2
			According to figures in Appendix C, the findings appear to be robust against the turbulence of the market environment and different exploration policies.  %L261pg173
			
			The simulation results of the baseline performance indicator in Fig. \ref{fig:Plot_20}-\ref{fig:Plot_30} suggest that, for non-myopic fuzzy Q-learning agents, there is an advantage in allocating equal bargaining power to all three divisions when marginal costs differ.  %L261pg170
			Adjusting the distribution ratio of bargaining power in direction of an equal level leads to higher investments and, thus, to a significant improvement in agents' performance.  %L331pg22
		
		
	
%\newpage
		%\begin{figure}[h!]
			%\centering
			%\includegraphics[width=0.5\textwidth]{Plot_31a.png} \includegraphics[width=0.434\textwidth]{Plot_32a.png} \\
			%\vspace{1.5mm}
			%\includegraphics[width=0.5\textwidth]{Plot_31b.png} \includegraphics[width=0.434\textwidth]{Plot_32b.png} \\
			%\vspace{1.5mm}
			%\includegraphics[width=0.5\textwidth]{Plot_31c.png} \includegraphics[width=0.434\textwidth]{Plot_32c.png} \\
			%\vspace{1.5mm}
			%\includegraphics[width=0.5\textwidth]{Plot_31d.png} \includegraphics[width=0.434\textwidth]{Plot_32d.png}
			%\caption{Modified boxplots of seller's investment $I_S$, buyer's investment $I_B$, and headquarters' profit $\Pi_{HQ}$ for myopic fuzzy Q-learning agents in the scenario $1$, i.e., $\lambda_S=\lambda_B=0.5$, $\Gamma=0.5$, $\gamma=0$, $\sigma[\theta_S]=\sigma[\theta_B]=0$, and exploration policy is Boltzmann.}
			%\label{fig:Plot_31}
		%\end{figure}
	
	
	
%\newpage
		%\begin{figure}[h!]
			%\centering
			%\includegraphics[width=0.5\textwidth]{Plot_33a.png} \includegraphics[width=0.434\textwidth]{Plot_34a.png} \\
			%\vspace{1.5mm}
			%\includegraphics[width=0.5\textwidth]{Plot_33b.png} \includegraphics[width=0.434\textwidth]{Plot_34b.png} \\
			%\vspace{1.5mm}
			%\includegraphics[width=0.5\textwidth]{Plot_33c.png} \includegraphics[width=0.434\textwidth]{Plot_34c.png} \\
			%\vspace{1.5mm}
			%\includegraphics[width=0.5\textwidth]{Plot_33d.png} \includegraphics[width=0.434\textwidth]{Plot_34d.png}
			%\caption{Modified boxplots of seller's investment $I_S$, buyer's investment $I_B$, and headquarters' profit $\Pi_{HQ}$ for myopic fuzzy Q-learning agents in the scenario $1$, i.e., $\lambda_S=\lambda_B=0.5$, $\Gamma=0.5$, $\gamma=0$, $\sigma[\theta_S]=\sigma[\theta_B]=0$, and exploration policy is Boltzmann.}
			%\label{fig:Plot_33}
		%\end{figure}		
		

	
\section{Summary and conclusive remarks}
\label{sec:Conclusion}

	%\textcolor{blue}{Be aware that, in general, the headquarters cannot optimally set the bargaining power according to Eq. \ref{eq:Gamma_1_sb} and \ref{eq:Gamma_2_sb} due to a lack of information. %L308pg17	 
	%Therefore, simple methods for determining reference values for the distribution ratio of the bargaining power between seller and buyer are desirable.} %MM

	The starting point of the simulation study is the well-known negotiated transfer pricing model by \cite{edlin1995specific}. %Paper1
	The authors extend the neoclassical model of \cite{schmalenbach1908verrechnungspreise} and \cite{williamson1985economic} by assuming that each division can simultaneously make an upfront specific investment that enhances the value of internal trade. %Paper1
	However, in negotiated transfer pricing models with specific investments \citep[e.g.,][]{eccles1988price, edlin1995specific, vaysman1998model}, it is generally supposed that divisions operate like fully individual rational utility maximizers. %Paper1
	But in reality, such assumptions are quite demanding, because, e.g., how could a division expect that the other division will behave optimally at a later point in time as required by (Nash) equilibrium concept. %L331pg23 und Paper1
	For this purpose, an agent-based variant of negotiated transfer pricing with individual utility maximizers who are subject to limitations is examined. %Paper1

	%To deal with the divisions' bounded rationality, asymmetric information, and cognitive limitations, fuzzy Q-learning, which is technically based on dynamic programming and Monte Carlo methods, is apply. %Paper1
	To deal with the divisions' bounded rationality, asymmetric information, and cognitive limitations, fuzzy Q-learning is apply. %Paper1
	%The divisions modeled by fuzzy Q-learning agents have only limited information about their environment. %L331pg22
	%It is assumed that the divisions have only limited information about their environment. %L331pg22
	Moreover, divisions are not able to find the best investment decision instantaneously, instead they have the ability to explore their decision space stepwise in order to find good policies. %MM
	%have no beliefs about the rationality of other divisions and, especially, are not able to find the global optimum of the solution space instantaneously; therefore, they discover the solution space stepwise. %L331pg23
	In addition, the study widely relaxes the common knowledge assumption and also increases the complexity of the divisions' coordination problem by increasing the number of downstream divisions by one. %MM
		
	%This study enriches the transfer pricing literature by an agent-based simulation with fuzzy Q-learning agents. %Paper1
	%First, this paper provides closed-form expressions to calculate the subgame perfect equilibrium for the investment hold-up problem with one supplying division and two buying divisions. %MM
	The paper makes two main contributions: on the one hand, the paper derives closed-form expressions for the subgame perfect equilibrium of the investment hold-up problem with one supplying division and two buying divisions and, on the other hand, the computer simulation provides some important insights into the dynamics of negotiated transfer pricing. %L331pg23
	According to the simulation outputs, fuzzy Q-learning agents perform at least as well or better than fully individual rational utility maximizers. % in all investigated scenarios. %MM
	In addition, the simulation results show that, if the headquarters applies the concept of subgame perfect equilibrium and allocates the distribution ratio of the bargaining power according to optimal (Nash) solutions, then fuzzy Q-learning agents generate profits that are at least in the range of the second-best solution or even higher (however, first-best solutions are generally not achieved). %significantly. %MM und Paper1
	
	In cases where both buying divisions have marginal cost parameters that are only half as large as the supplying division, the headquarters is well advised to give all of its divisions the same bargaining power. %MM
	But if the marginal cost parameters of both buying divisions are less than half, then the headquarters should assign the supplying division a greater bargaining power than the theory of subgame perfection equilibrium recommends. %MM
	In such a case, an $1:2$ surplus sharing rule for seller and buyer may be a good choice for non-myopic divisions. %MM
	This finding is especially important when the headquarters cannot optimally set the bargaining power due to a lack of information. %MM
	On the other hand, if the marginal cost parameters of the buying divisions differ widely from each other, then $2:3$ surplus sharing rules may favor divisions' investment decisions and, thus, can lead to significantly higher profits.
	%But if the headquarters assigns the supplying division a greater bargaining power, then, in some scenarios, the headquarters' profit increases significantly. %MM
	%The effects of different surplus sharing parameters seem to depend crucially on the level of foresight. 
	The findings also show that the performance of fuzzy Q-learning agents depends crucially on the level of foresight. %MM	
	According to an extensive sensitivity analysis, the simulation outputs appear to be robust against the turbulence of the market environment and different exploration policies. %MM	
	%the headquarters is well advised to give all of its divisions the same bargaining power, wobei Divisionen mit niederigeren XXX auch eine niedriger Verhaldungmacht erhalten sollten. %MM
	%The simulation results show that scenarios in which divisions are designed so that they have symmetric marginal cost parameters ... lead to a significantly higher performance than scenarios in which the headquarters follows the rec %L333pg11
	%The results suggest that the choice of XXX is only of crucial relevance in case of changing XXX %L333pg11

	Future research could address the following points: (1) The simulation study investigated here focuses on fuzzy Q-learning. %MM 
	It would be interesting to know to what extent the simulation results change, when other reinforcement learning approaches are applied instead. %Paper1 and MM
	%(2) The expansion to two buying divisions is obviously not limited. %L331pg23
	(2) The expansion is not limited to two buying divisions. %L331pg23
	Besides, it is also interesting to examine other firm settings, e.g, a supply chain in which each division can improve the quality of the intermediate product by upfront investments. %L181pg3 and MM
	(3) Future research could also study negotiated transfer pricing under capacity constraints. %MM
	This is especially important when the number of divisions increases. %MM
	For example, \cite{gavious1999transfer} and \cite{wielenberg2000negotiated} examine a two-divisional firm where the term capacity describes the level of production that is achievable with the resources currently available and could be raised by investing specific resources such as labor, machinery, or simply by overtime. %L195pg3
	Model extensions with several consecutive downstream divisions would certainly be interesting, especially how well cognitively bounded agents perform under these terms. %MM
	%The term capacity does not necessarily represent an upper bound on the level of production. %L195pg3
	%It could describe the level of production that is achievable with the resources currently available and could be raised by investing specific resources such as labor, machinery, or overtime. %L195pg3	
	%This is especially important when several divisions are related in some way....
	%(4) whether the simulation results are robust in terms of stochastic fluctuations on investments was not investigated. %MM
	(4) More research work could also consider changes in the timeline. %Paper1
	For example, the assumption that the state variables are realized before the negotiation process takes place is only to simplify the backward induction. %MM
	In reality, stochastic influences also take place after the negotiation phase and should therefore be taken into consideration. %MM
	%For example, that the state variables are not observed until after the negotiation is only . %Paper1
	%there are other points which play an essential roll in the divisions' decisions, such as the quality
	%It would be interesting to know to what extent the simulation results change, if, for example, the state variables are not observed until after the negotiation, or that the investments are also subject to stochastic fluctuations.
	
	
	%In particular, when some divisions, which may be in different countries, operate in different tax jurisdictions. %L178pg2
	%Hence, closed-form expressions are usually not available for such settings.
	%However, it is also interesting to investigate other decentralized firm settings with more than two divisions, since analytical results are generally not available for such settings. %L58pg.3282
	%(2) the use of more complicated membership functions, such as those implemented with Gaussian or S-shapes. %L
	%Such are very popular in the fuzzy logic literature and have properties, such as being differentiable everywhere, but they are more computationally expensive than triangular membership functions.
	%(3) future research could also examine a different sequence of events. %MM
	%It would be interesting to know to what extent the simulation results change, if, for example, the state variables are not observed until after the negotiation, or that the investments are also subject to stochastic fluctuations.
	
	%It would be interesting to know to what extent the simulation results change, when more competing divisions appear. %Paper1 and MM
	



	
	%\textcolor{blue}{WICHTIG: L324 in den Schluss, da L324 nicht nur specific investments untersucht sondern gleichzeitig zu den Investitionen auch capacity choice untersucht. Das sollte ich umbedingt erwähnen; außerdem verwendet L324 als einzige Literatur die Bezeichnung surplus sharing parameter! Deshalb möchte ich sie erwähnen! Der Satz könnte so heißen: Wenn mehrere Buyers involviert sind, dann würde es Sinn machen, wenn es diverse Kapazitätsgrenzen in der Produktion gibt. In L324 werden der Fall von Kapazitäten behandelt, allerdings nur für den Fall 1 Seller und 1 Buyer. Hier könnte man eine Modellerweiterung in Erwägung ziehen.}






%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}


% Authors must disclose all relationships or interests that 
% could have direct or potential influence or impart bias on 
% the work: 
%
%\section*{Conflict of interest}
%
%The authors declare that they have no conflict of interest.

%\newpage
	%\vspace{5mm}
%	\textbf{Declarations}
	
%	\vspace{5mm}
%	\textbf{Funding}
	
%	Not applicable
	
%	\vspace{5mm}
%	\textbf{Conflict of interest}
	
%	The authors declare that they have no conflict of interest.
	
%	\vspace{5mm}
%	\textbf{Availability of data and material}
	
%	Not applicable
	
%	\vspace{5mm}
%	\textbf{Code availability}
	
%	Not applicable
	
%	\vspace{5mm}
%	\textbf{Authors' contributions}
	
%	Not applicable
	
	%\vspace{5mm}
	%\textbf{Ethics declarations}	
	
	%\vspace{5mm}
	%\textbf{Ethical approval}
	
	%This article does not contain any studies with human participants or animals performed by any of the authors.

	%\vspace{5mm}
	%\textbf{Informed consent}

	%None.
	
	

\newpage
%\appendix
\section*{Appendix A: Flow diagram of the agent-based simulation }
\label{appendix_A:Flow_diagram}	
	\vspace{-1mm}
	\begin{figure}[b!]
		\centering
		\includegraphics[width=0.95\textwidth]{Procedure.png}
		\caption{Flow diagram of the agent-based simulation using the Boltzmann exploration policy. For an improved readability, most for-loops are omitted. } %MM
		\label{fig:Procedure}
	\end{figure}	



	\afterpage{
		%\clearpage
		\begin{landscape}
		\begin{table}[h!]
		\linespread{1.0}\selectfont
		\centering
		\caption{First-best and second-best solutions resulting from the concept of subgame perfect equilibrium for $b=12$, $E[\theta_S]=60$, and $E[\theta_B]=100$.}
		\label{tab:Firstbest_Secondbest_Solutions}
		\begin{tabularx}{151.5mm}{|ccccc|ccccccccc|}
			\hline
			\multicolumn{14}{|l|}{\textbf{First-best solutions for symmetric scenarios I-IV}} \\
			\hline
			$\Gamma_1$ & $\Gamma_2$ & $\lambda_S$ & $\lambda_1$ & $\lambda_2$ & $I_S^*$ & $I_1^*$ & $I_2^*$ & $q_1^*$ & $q_2^*$ & $\Pi_S^*$ & $\Pi_1^*$ & $\Pi_2^*$ & $\Pi_{HQ}^*$ \\
			\hline			
			0.5 & 0.5 & 1 & 0.5 & 0.5 & 10 & 10 & 10 & 5 & 5 & 100 & 50 & 50 & 200 \\
			0.45 & 0.45 & 1 & 0.424 & 0.424 & 10.47 & 12.35 & 12.35 & 5.23 & 5.23 & 93.10 & 58.14 & 58.14 & 209.38 \\
			0.40 & 0.40 & 1 & 0.361 & 0.361 & 11.07 & 15.33 & 15.33 & 5.53 & 5.53 & 85.67 & 67.82 & 67.82 & 221.30 \\
			0.35 & 0.35 & 1 & 0.307 & 0.307 & 11.86 & 19.32 & 19.32 & 5.93 & 5.93 & 77.13 & 80.08 & 80.08 & 237.29 \\
			0.30 & 0.30 & 1 & 0.262 & 0.262 & 12.94 & 24.69 & 24.69 & 6.47 & 6.47 & 67.02 & 95.87 & 95.87 & 258.77 \\
			0.25 & 0.25 & 1 & 0.222 & 0.222 & 14.56 & 32.79 & 32.79 & 7.28 & 7.28 & 52.79 & 119.18 & 119.18 & 291.15 \\
			%\hline
			%\noalign{\vskip 0mm}
			\hline
			\multicolumn{14}{|l|}{\textbf{Second-best solutions for symmetric scenarios I-IV}} \\
			\hline
			$\Gamma_1$ & $\Gamma_2$ & $\lambda_S$ & $\lambda_1$ & $\lambda_2$ & $I_S^{sb}$ & $I_1^{sb}$ & $I_2^{sb}$ & $q_1^{sb}$ & $q_2^{sb}$ & $\Pi_S^{sb}$ & $\Pi_1^{sb}$ & $\Pi_2^{sb}$ & $\Pi_{HQ}^{sb}$ \\
			\hline			
			0.5 & 0.5 & 1 & 0.5 & 0.5 & 4 & 4 & 4 & 4 & 4 & 88 & 44 & 44 & 176 \\
			0.45 & 0.45 & 1 & 0.424 & 0.424 & 3.67 & 5.29 & 5.29 & 4.08 & 4.08 & 83.14 & 49.02 & 49.02 & 181.18 \\
			0.40 & 0.40 & 1 & 0.361 & 0.361 & 3.35 & 6.97 & 6.97 & 4.19 & 4.19 & 78.78 & 54.55 & 54.55 & 187.89 \\
			0.35 & 0.35 & 1 & 0.307 & 0.307 & 3.04 & 9.23 & 9.23 & 4.36 & 4.36 & 74.92 & 61.01 & 61.01 & 196.93 \\
			0.30 & 0.30 & 1 & 0.262 & 0.262 & 2.75 & 12.24 & 12.24 & 4.58 & 4.58 & 71.85 & 68.56 & 68.56 & 208.97 \\
			0.25 & 0.25 & 1 & 0.222 & 0.222 & 2.46 & 16.65 & 16.65 & 4.93 & 4.93 & 69.67 & 78.46 & 78.46 & 226.59 \\ 
			\hline
			\noalign{\vskip 1.8mm}   
			\hline 
			\multicolumn{14}{|l|}{\textbf{First-best solutions for non-symmetric scenarios V-VI}} \\
			\hline
			$\Gamma_1$ & $\Gamma_2$ & $\lambda_S$ & $\lambda_1$ & $\lambda_2$ & $I_S^*$ & $I_1^*$ & $I_2^*$ & $q_1^*$ & $q_2^*$ & $\Pi_S^*$ & $\Pi_1^*$ & $\Pi_2^*$ & $\Pi_{HQ}^*$ \\
			\hline			
			0.5 & 0.5 & 1 & 0.5 & 0.5 & 10 & 10 & 10 & 5 & 5 & 100 & 50 & 50 & 200 \\
			0.55 & 0.45 & 1 & 0.534 & 0.463 & 10.02 & 9.25 & 10.98 & 4.94 & 5.08 & 100.02 & 42.96 & 57.48 & 200.46 \\
			0.60 & 0.40 & 1 & 0.564 & 0.425 & 10.09 & 8.68 & 12.22 & 4.90 & 5.19 & 100.12 & 36.32 & 65.36 & 201.80 \\
			0.65 & 0.35 & 1 & 0.590 & 0.385 & 10.21 & 8.26 & 13.87 & 4.87 & 5.34 & 100.32 & 29.73 & 74.21 & 204.26 \\
			0.70 & 0.30 & 1 & 0.609 & 0.343 & 10.42 & 7.99 & 16.18 & 4.87 & 5.55 & 100.56 & 23.18 & 84.61 & 208.35 \\
			0.75 & 0.25 & 1 & 0.621 & 0.301 & 10.73 & 7.86 & 19.42 & 4.88 & 5.85 & 101.05 & 16.36 & 97.16 & 214.57 \\
			%\hline
			%\noalign{\vskip 0mm}
			\hline
			\multicolumn{14}{|l|}{\textbf{Second-best solutions for non-symmetric scenarios V-VI}} \\
			\hline
			$\Gamma_1$ & $\Gamma_2$ & $\lambda_S$ & $\lambda_1$ & $\lambda_2$ & $I_S^{sb}$ & $I_1^{sb}$ & $I_2^{sb}$ & $q_1^{sb}$ & $q_2^{sb}$ & $\Pi_S^{sb}$ & $\Pi_1^{sb}$ & $\Pi_2^{sb}$ & $\Pi_{HQ}^{sb}$ \\
			\hline			
			0.5 & 0.5 & 1 & 0.5 & 0.5 & 4 & 4 & 4 & 4 & 4 & 88 & 44 & 44 & 176 \\
			0.55 & 0.45 & 1 & 0.534 & 0.463 & 4.00 & 3.32 & 4.84 & 3.94 & 4.07 & 88.02 & 39.00 & 49.31 & 176.32 \\
			0.60 & 0.40 & 1 & 0.564 & 0.425 & 4.00 & 2.76 & 5.87 & 3.90 & 4.16 & 88.10 & 34.30 & 54.86 & 177.26 \\
			0.65 & 0.35 & 1 & 0.590 & 0.385 & 4.00 & 2.29 & 7.21 & 3.86 & 4.27 & 88.26 & 29.70 & 61.03 & 178.99 \\
			0.70 & 0.30 & 1 & 0.609 & 0.343 & 4.00 & 1.88 & 9.03 & 3.82 & 4.42 & 88.47 & 25.23 & 68.13 & 181.82 \\
			0.75 & 0.25 & 1 & 0.621 & 0.301 & 4.00 & 1.52 & 11.55 & 3.79 & 4.63 & 88.92 & 20.75 & 76.46 & 186.13 \\
			\hline
		%\end{tabular}
		\end{tabularx}
		\linespread{1}\selectfont
		\end{table}
		\end{landscape}
	}
		
	

\newpage
%\appendix
\section*{Appendix B: Sensitivity analysis conducted according to scenario IV}
\label{appendix_B:Sensitivity_analysis}
%\newpage	
	\vspace{-5mm}
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.99\textwidth]{Plot_07.png}
		\caption{Abs. performance of the headquarters' profit for $\Gamma_1^{sb}=0.5$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$ and $\lambda_1=\lambda_2=0.5$.}
		\label{fig:Plot_07}
	\end{figure}
	\vspace{-5mm}
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.99\textwidth]{Plot_09.png}
		\caption{Abs. performance of the headquarters' profit for $\Gamma_1^{sb}=0.45$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$ and $\lambda_1=\lambda_2=0.424$.}
		\label{fig:Plot_09}
	\end{figure}
	
	
	
\newpage	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.99\textwidth]{Plot_11.png}
		\caption{Abs. performance of the headquarters' profit for $\Gamma_1^{sb}=0.4$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$ and $\lambda_1=\lambda_2=0.361$.}
		\label{fig:Plot_11}
	\end{figure}
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.99\textwidth]{Plot_13.png}
		\caption{Abs. performance of the headquarters' profit for $\Gamma_1^{sb}=0.35$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$ and $\lambda_1=\lambda_2=0.307$.}
		\label{fig:Plot_13}
	\end{figure}
	
	
	
\newpage
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.99\textwidth]{Plot_15.png}
		\caption{Abs. performance of the headquarters' profit for $\Gamma_1^{sb}=0.3$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$ and $\lambda_1=\lambda_2=0.262$.}
		\label{fig:Plot_15}
	\end{figure}
	%\vspace{-10mm}
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.95\textwidth]{Plot_17.png}
		\caption{Abs. performance of the headquarters' profit for $\Gamma_1^{sb}=0.25$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$ and $\lambda_1=\lambda_2=0.222$.}
		%\caption{Relative performance change and statistical hypothesis testing of the headquarters' profit compared the baseline scenario with $\Gamma^{sb}=0.5$ of different exploration policies and standard deviations of state variables given $\lambda_S=\lambda_B=0.5$.}
		\label{fig:Plot_17}
	\end{figure}
	
	
	
\newpage	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.96\textwidth]{Plot_08.png}
		\caption{Results of the baseline performance indicator for $\Gamma_j^{sb}=0.5$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$ and $\lambda_1=\lambda_2=0.5$.}
		\label{fig:Plot_08}
	\end{figure}	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.96\textwidth]{Plot_10.png}
		\caption{Results of the baseline performance indicator for $\Gamma_j^{sb}=0.45$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$ and $\lambda_1=\lambda_2=0.424$.}
		\label{fig:Plot_10}
	\end{figure}
	
	
	
\newpage	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.96\textwidth]{Plot_12.png}
		\caption{Results of the baseline performance indicator for $\Gamma_j^{sb}=0.4$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$ and $\lambda_1=\lambda_2=0.361$.}
		\label{fig:Plot_12}
	\end{figure}	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.96\textwidth]{Plot_14.png}
		\caption{Results of the baseline performance indicator for $\Gamma_j^{sb}=0.35$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$ and $\lambda_1=\lambda_2=0.307$.}
		\label{fig:Plot_14}
	\end{figure}



\newpage	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.96\textwidth]{Plot_16.png}
		\caption{Results of the baseline performance indicator for $\Gamma_j^{sb}=0.3$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$ and $\lambda_1=\lambda_2=0.262$.}
		\label{fig:Plot_16}
	\end{figure}	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.96\textwidth]{Plot_18.png}
		\caption{Results of the baseline performance indicator for $\Gamma_j^{sb}=0.25$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$ and $\lambda_1=\lambda_2=0.222$.}
		\label{fig:Plot_18}
	\end{figure}



\newpage
%\appendix
\section*{Appendix C: Sensitivity analysis conducted according to scenario VI}
\label{appendix_C:Sensitivity_analysis}
%\newpage	
	\vspace{-5mm}
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.99\textwidth]{Plot_19.png}
		\caption{Abs. performance of the headquarters' profit for $\Gamma_1^{sb}=0.5$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$ and $\lambda_1=\lambda_2=0.5$.}
		\label{fig:Plot_19}
	\end{figure}
	\vspace{-5mm}
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.99\textwidth]{Plot_21.png}
		\caption{Abs. performance of the headquarters' profit for $\Gamma_1^{sb}=0.55$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$, $\lambda_1=0.534$, and $\lambda_2=0.463$.}
		\label{fig:Plot_21}
	\end{figure}
	
	
	
\newpage	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.99\textwidth]{Plot_23.png}
		\caption{Abs. performance of the headquarters' profit for $\Gamma_1^{sb}=0.6$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$, $\lambda_1=0.564$, and $\lambda_2=0.425$.}
		\label{fig:Plot_23}
	\end{figure}
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.99\textwidth]{Plot_25.png}
		\caption{Abs. performance of the headquarters' profit for $\Gamma_1^{sb}=0.65$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$, $\lambda_1=0.59$, and $\lambda_2=0.385$.}
		\label{fig:Plot_25}
	\end{figure}
	


\newpage	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.99\textwidth]{Plot_27.png}
		\caption{Abs. performance of the headquarters' profit for $\Gamma_1^{sb}=0.7$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$, $\lambda_1=0.609$, and $\lambda_2=0.343$.}
		\label{fig:Plot_27}
	\end{figure}
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.99\textwidth]{Plot_29.png}
		\caption{Abs. performance of the headquarters' profit for $\Gamma_1^{sb}=0.75$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$, $\lambda_1=0.621$, and $\lambda_2=0.301$.}
		\label{fig:Plot_29}
	\end{figure}	
	


\newpage	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.96\textwidth]{Plot_20.png}
		\caption{Results of the baseline performance indicator for $\Gamma_1^{sb}=0.5$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$,  $\lambda_1=0.5$, and $\lambda_2=0.5$.}
		\label{fig:Plot_20}
	\end{figure}	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.96\textwidth]{Plot_22.png}
		\caption{Results of the baseline performance indicator for $\Gamma_1^{sb}=0.55$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$,  $\lambda_1=0.534$, and $\lambda_2=0.463$.}
		\label{fig:Plot_22}
	\end{figure}	
	
	
	
\newpage	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.96\textwidth]{Plot_24.png}
		\caption{Results of the baseline performance indicator for $\Gamma_1^{sb}=0.6$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$,  $\lambda_1=0.564$, and $\lambda_2=0.425$.}
		\label{fig:Plot_24}
	\end{figure}	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.96\textwidth]{Plot_26.png}
		\caption{Results of the baseline performance indicator for $\Gamma_1^{sb}=0.65$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$,  $\lambda_1=0.59$, and $\lambda_2=0.385$.}
		\label{fig:Plot_26}
	\end{figure}		
	
	

\newpage	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.96\textwidth]{Plot_28.png}
		\caption{Results of the baseline performance indicator for $\Gamma_1^{sb}=0.7$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$,  $\lambda_1=0.609$, and $\lambda_2=0.343$.}
		\label{fig:Plot_28}
	\end{figure}	
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.96\textwidth]{Plot_30.png}
		\caption{Results of the baseline performance indicator for $\Gamma_1^{sb}=0.75$ of different exploration policies and standard deviations of state variables given $\lambda_S=1$,  $\lambda_1=0.621$, and $\lambda_2=0.301$.}
		\label{fig:Plot_30}
	\end{figure}		



\newpage
% BibTeX users please use one of
\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{biblio}   % name your BibTeX data base


% Non-BibTeX users please use
%\begin{thebibliography}{}
%
% and use \bibitem to create references. Consult the Instructions
% for authors for reference list style.
%
%\bibitem{RefJ}
% Format for Journal Reference
%Author, Article title, Journal, Volume, page numbers (year)
% Format for books
%\bibitem{RefB}
%Author, Book title, page numbers. Publisher, place (year)
% etc
%\end{thebibliography}

\end{document}
% end of file template.tex