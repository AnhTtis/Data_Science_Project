%!TEX root = main.tex
\section{Experimental evaluation}\label{sec:exp}

We evaluate an implementation of $\textsc{explore-ce}$ and $\textsc{explore-ce}^*$ in the context of the Java Pathfinder (JPF)~\cite{DBLP:conf/issta/VisserPK04} model
checker for Java concurrent programs. As benchmark, we use bounded-size client programs of a number of database-backed applications drawn from the literature. % of distributed systems and databases
The experiments were performed on an Apple M1 with $8$ cores and $16$ GB of RAM.

%%Each client is a different program (following the terminology in Section 2). 5 clients x 3 sessions means 5 independent programs where each program executes 3 sessions. Those clients are studied in isolation, not considering any interleaving between them. We will clarify.

\subsection{Implementation}

We implemented our algorithms as an extension of the \texttt{DFSearch} class in JPF. For performance reasons, we implemented an iterative version of these algorithms where roughly, inputs to recursive calls are maintained as a collection of histories instead of relying on the call stack. For checking consistency of a history with a given isolation level, we implemented the algorithms proposed by \citet{DBLP:journals/pacmpl/BiswasE19}. We plan to make our implementation publicly available.

%$\textsc{explore-ce}$ and $\textsc{explore-ce}^*$ on top of JPF and an implementation of the algorithms proposed by \citet{DBLP:journals/pacmpl/BiswasE19} for checking consistency of a history with a given isolation level. For performance issues, we implemented an iterative version of these algorithms 

%\ref{algorithm:algo-class} for reducing the actual memory consumption. We employ an annotated stack of database states that is modified in each $\textsc{explore-ce}$ call; and we consider the algorithm works with list instead of simply sets.
%At every step where we execute a $\iread$, we annotate which $\iwrite$ event is reading from and  when the state pops from the stack, we change the $\wro$-dependency. Analogously, at every step we produce a swap, we annotate which tuple given by $\compute$ we used in the swap for selecting a different tuple when the state is popped out.
Our tool takes as input a Java program and isolation levels as parameters. We assume that the program uses a fixed API for interacting with the database, similar to a key-value store interface. This API consists of specific methods for starting/ending a transaction, and reading/writing a global variable. The fixed API is required for being able to maintain the database state separately from the JVM state (the state of the Java program) and update the current history in each database access. This relies on a mechanism for ``transferring'' values read from the database state to the JVM state. 
% a verification tool for Java concurrent programs. We admit as input a Java program that (1) can be parsed and executed by JPF and (2) employs the API according to the specifications. Our tool is parametric in the isolation levels employed.

%The API provided allow $\ibegin, \iwrite, \iread$ and $\icommit$ operations. Thanks to this API, we can maintain the database state separately from JPF's internal state and update the current history in each database call. We do not rely on any concrete database but store the information as a collection of String, one per write API call executed; along with the $\so$, $\wro$-dependencies between the events associated to them. Any time there is a need to connect data from the local state to the database one or viceversa we copy and store the concrete String value from one state to the other. %Altogether, we are able to simulate from simple variables to actual tables provided the client has proper methods to transform from one to the other representative.

%Our implementation is compatible with JPF's standards, as both algorithm are instances of DFSearch; a JPF for DFS exploring algorithms. Moreover, we built consistency checkers based on the algorithm's presented in \textcolor{red}{Ranadeep and Enea's paper} for different isolation levels to ensure database's soundness. 

%However, for performance issues, we developed an iterative version of the algorithm \ref{algorithm:algo-class} for reducing the actual memory consumption. We employ an annotated stack of database states that is modified in each $\textsc{explore-ce}$ call; and we consider the algorithm works with list instead of simply sets.
%At every step where we execute a $\iread$, we annotate which $\iwrite$ event is reading from and  when the state pops from the stack, we change the $\wro$-dependency. Analogously, at every step we produce a swap, we annotate which tuple given by $\compute$ we used in the swap for selecting a different tuple when the state is popped out. % In addition, as JPF is not designed for supporting database operations, we developed an API that simulates every database instruction. operations, but they have been proved expressive enough both during our experiments and in real time applications. We admit any Java program that (1) can be parsed and executed by JPF and (2) has an equivalent translation into a program written with the syntax defined in Figure~\ref{fig:syntax}.

\vspace{-1mm}
\subsection{Benchmark}

We consider a set of benchmarks inspired by real-world applications and evaluate them under different types of client programs and isolation levels.
%both implemented following the aforementioned specifications. We provide also examples of different behaviors depending on the isolation level for each applications.

%\vspace{1mm}
%\noindent
\textit{Shopping Cart~\cite{sivaramakrishnan2015declarative5}} allows users to add, get and remove items from their shopping cart and modify the quantities of the items present in the cart. 
%We employ only one table, $\texttt{cart}$, in this application. Given a program that add an item in one section and deletes it in another one, we may observe, depending on the isolation level, that at the end of the execution the cart contains either zero, one or two items.

%\vspace{1mm}
%\noindent
\textit{Twitter~\cite{difallah2013oltp}} allows users to follow other users, publish tweets and get their followers, tweets and tweets published by other followers. 
%We model twitter with four tables: $\texttt{users}, \texttt{tweets}, \texttt{followed}, \texttt{followers}$. Under weak isolation levels, it is possible that one user can publish a tweet and not being able to obtain it from a different session. We can also detect other behaviors as users following another users in one session but the latter not being able to find the former as a follower.

%\vspace{1mm}
%\noindent
\textit{Courseware~\cite{DBLP:conf/esop/NairP020}} manages the enrollment of students in courses in an institution. It allows to open, close and delete courses, enroll students and get all enrollments. One student can only enroll to a course if it is open and its capacity has not reached a fixed limit. 
%It employs three tables, $\texttt{student}, \texttt{course}$ and $\texttt{enrollments}$. Under weak isolation levels, two students in different sessions may enroll to a course with only one free place or being able to enroll into a course that has been deleted in another session.

%\vspace{1mm}
%\noindent
\textit{Wikipedia~\cite{difallah2013oltp}} allows users to get the content of a page (registered or not), add or remove pages to their watching list and update pages. 
%It employs ten tables, but the vast majority of procedures only access to a small subset of them. Under weak isolation levels, one change in a page may be overwritten by another one done in a different session as well as the watching list may contain a variable number of pages if they are added/deleted from different sessions.

%\vspace{1mm}
%\noindent
\textit{TPC-C~\cite{TPCC}} models an online shopping application with five types of transactions: reading the stock of a product, creating a new order, getting its status, paying it and delivering it. 
%TPC-C employs nine tables and all its procedures read and write several variables. Under weak isolation level two orders may be created from different sessions or the account balance may be inconsistent if some order is payed twice.

\nver{SQL tables are modeled using a ``set'' global variable whose content is the set of ids (primary keys) of the rows present in the table, and a set of global variables, one variable for each row in the table (the name of the variable is the primary key of that row). SQL statements such as INSERT and DELETE statements are modeled as writes on that ``set'' variable while SQL statements with a WHERE clause (SELECT, JOIN, UPDATE) are compiled to a read of the table's set variable followed by reads or writes of variables that represent rows in the table (similarly to~\cite{DBLP:journals/pacmpl/RahmaniNDJ19,DBLP:journals/pacmpl/BiswasKVEL21}).} % following a starting read of the table's set variable.} (see~\cite{DBLP:journals/pacmpl/RahmaniNDJ19,DBLP:journals/pacmpl/BiswasKVEL21}).
% \nver{All SQL instructions are translated to key-value paradigm by adding a ``set'' variable per table whose content is the set of  ids (primary keys) of the rows present in the table. The translation of every SQL instruction with a WHERE clause starts reading this variable while INSERT and DELETE are modelled as writes on it.}

\vspace{-1mm}
\subsection{Experimental Results}

We designed three experiments where we compare the performance of a baseline model checking algorithm, $\textsc{explore-ce}$ and $\textsc{explore-ce}^*$ for different (combinations of) isolation levels, and we explore the scalability of $\textsc{explore-ce}$ when increasing the number of sessions and transactions per session, respectively.
\appendixver{All data is exhibited in Appendix~\ref{sec:appendix:experiments} in the supplementary material \cite{bouajjani2023dynamic}. }For each experiment we report running time, memory consumption, and the number of end states, i.e., histories of complete executions and in the case of $\textsc{explore-ce}^*$, before applying the $\genericEvaluate$ filter. As the number of end states for a program on a certain isolation level increases, the running time of our algorithms naturally increases as well. 

%We have run three type of experiments to determine the algorithm's scalability. The more transactions that write into the database, the more potential behaviors allowed. Therefore, in each case we design the experiments with this guideline.

\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{0.322\linewidth}
		\centering
		\includegraphics[width=\textwidth]{figures/app-scala-Time.eps}
		\vspace{-5mm}
		\caption{Running time.}%$10$ $11$ and $18$ benchmarks timeout for $\langle \RC, \CC \rangle$, $\langle \texttt{true}, \CC \rangle$ and $\textsc{DFS}(\CC)$ respectively.
		%\caption{Execution time. For the last program, the running time is $25' 17''$ under $\RA$ and $33' 3''$ under $\langle\RA, \CC\rangle$ (out of the figure).}
		\label{fig:results-ra-cc}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.322\linewidth}
		\centering
		\includegraphics[width=\textwidth]{figures/app-scala-Mem.eps}
		\vspace{-5mm}
		\caption{Memory consumption.}
		\label{fig:results-memory}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.322\linewidth}
		\centering
		\includegraphics[width=\textwidth]{figures/app-scala-histories.eps}
		\vspace{-5mm}
		\caption{End states.}
		\label{fig:results-histories}
	\end{subfigure}
\vspace{-3.5mm}
	\caption[where every history is consistent]{Cactus plots comparing different algorithms in terms of time, memory, and end states. For readability, we use $\CC$ to denote $\textsc{explore-ce}$ under $\CC$, $I_1 + I_2$ stands for $\textsc{explore-ce}^*(I_1, I_2)$, and $\texttt{true}$ is the trivial isolation level where every history is consistent. \nver{Differences between $\CC$, $\CC + \SI$ and $\CC + \SER$ are very small and their graphics overlap.} Moreover, $\textsc{DFS}(\CC)$ denotes a standard DFS traversal of the semantics defined in Section~\ref{ssec:semantics}. These plots exclude benchmarks that timeout ($30$ mins): $11$ $12$ and $20$ benchmarks timeout for $\langle \RC, \CC \rangle$, $\langle \texttt{true}, \CC \rangle$ and $\textsc{DFS}(\CC)$ respectively.}%In Figure~\ref{fig:results-memory} is deduced the minimum JPF's memory consumption ($256$ MB).
	%\caption{Comparing $\textsc{explore-ce}$ under $\CC$ and $\RA$ and $\textsc{explore-ce}^*$ under $\langle\RA, \CC\rangle, \langle\CC, \SI\rangle$ and $\langle\CC, \SER\rangle$. Client programs are ordered according to running time under $\RA$. The execution time of $\textsc{explore-ce}$ under $\CC$ is indistinguishable from the execution time of $\textsc{explore-ce}^*$ under $\langle\CC, \SI\rangle$ and $\langle\CC, \SER\rangle$.}
	\label{fig:results1}
\vspace{-5mm}
\end{figure}

%\footnotetext{\nver{ $\CC$, $\CC + \SI$ and $\CC + \SER$ graphics overlap.}}

%Each client is a different program (following the terminology in Section 2). 5 clients x 3 sessions means 5 independent programs where each program executes 3 sessions. Those clients are studied in isolation, not considering any interleaving between them. We will clarify.

The first experiment  compares the performance of our algorithms for different combinations of isolation levels and a baseline model checking algorithm that performs no partial order reduction. We consider as benchmark five \nver{(independent)} client programs\footnote{For an application that defines a number of transactions, a client program consists of a number of sessions, each session containing a sequence of transactions defined by the application.} for each application described above ($25$ in total), each \nver{program} with three sessions and three transactions per session. The running time, memory consumption, and number of end states are reported in Figure~\ref{fig:results1} as \nver{cactus plots~\cite{DBLP:conf/issac/BrainDG17}.} %added the citation of cactus plot.

%These are cactus plots. We start with a list of benchmark programs sorted by the running time (of our tool), and then plot the accumulated time for our tool on the first $i$-benchmarks in this list, for every $i$.

%One client has only one writing transaction per session while the second one has two writing transactions per session. 
 %for each application we have designed two programs with three threads each and three transactions per thread; where one contains only one transaction that writes per thread instead of the two writing transactions the second program have (in the case of TPC-C we discriminate transactions depending on the number of variables they write). 
To justify the benefits of partial order reduction, we implement a baseline model checking algorithm $\textsc{DFS}(\CC)$ that performs a standard DFS traversal of the execution tree w.r.t. the formal semantics defined in Section~\ref{ssec:semantics} for $\CC$ (for fairness, we restrict interleavings so at most one transaction is pending at a time). This baseline algorithm may explore the same history multiple times since it includes no partial order reduction mechanism. In terms of time, $\textsc{DFS}(\CC)$ behaves poorly: it timeouts for $20$ out of the $25$ programs and it is less efficient even when it terminates. We consider a timeout of $30$ mins. In comparison the strongly optimal algorithm $\textsc{explore-ce}(\CC)$ (under $\CC$) finishes in $17$ seconds in average. $\textsc{DFS}(\CC)$ is also worse in terms of memory consumption. The memory consumption of $\textsc{DFS}(\CC)$ is $441$MB in average, compared to $317$MB for $\textsc{explore-ce}(\CC)$ (JPF forces a minimum consumption of $256$MB).

To argue about the benefits of \emph{strong} optimality, we compare $\textsc{explore-ce}(\CC)$ which is strongly optimal with ``plain'' optimal algorithms $\textsc{explore-ce}^*(I_0,\CC)$ for different levels $I_0$. As shown in Figure~\ref{fig:results1}(a), in terms of time, $\textsc{explore-ce}(\CC)$ is more efficient than every ``plain'' optimal algorithm, and the difference in performance grows as $I_0$ becomes weaker. In the limit, when $I_0$ is the trivial isolation level $\texttt{true}$ where every history is consistent, $\textsc{explore-ce}^*(\texttt{true},\CC)$ timeouts for $12$ out of the $25$ programs. The average speedup (average of individual speedups) of $\textsc{explore-ce}(\CC)$ w.r.t. $\textsc{explore-ce}^*(\RA,\CC)$, $\textsc{explore-ce}^*(\RC,\CC)$ and $\textsc{explore-ce}^*(\texttt{true},\CC)$ is $2$, $31$, and $54$ respectively. In terms of memory, all algorithms consume around 300 MB in average.


%\begin{table}[!ht]
%	\resizebox{\textwidth}{!}{
%    \centering
%
%	\begin{tabular}{c|c|c|c|c|c|c|c|}
%	\cline{2-8}
%	 & \CC     & \SI + \CC & \SER + \CC & \RA + \CC & \RC+ \CC   & \texttt{true} + \CC & \textsc{DFS}$(\CC)$  \\ \hline
%	\multicolumn{1}{|c|}{Avg. speedup}                        & 1      & 1     & 1      & 2       & 31       & 54        & 297      \\ \hline
%	\multicolumn{1}{|c|}{Avg. time (s)}    & 17     & 18      & 17       & 79      & 184      & 302       & 953      \\ \hline
%	\multicolumn{1}{|c|}{Avg. memory (MB)} & 317  & 335 & 332  & 339 & 332 & 341 & 441 \\ \hline
%	\multicolumn{1}{|c|}{Avg. histories ($\times 10^3$)} & 421.28 & 31.00   & 9.64     & 1746.52 & 25976.29 & 55789.54  & 68106.00 \\ \hline
%	\end{tabular}
%	}
%	\caption{Average speedup, time, memory and histories in the first experiment after excluding benchmarks that timeout.}
%\end{table}

For the $\SI$ and $\SER$ isolation levels that admit no strongly optimal $\textsc{explore}$ algorithm, we observe that the overhead of $\textsc{explore-ce}^*(\CC,\SI)$ or $\textsc{explore-ce}^*(\CC,\SER)$ relative to $\textsc{explore-ce}(\CC)$ is negligible (the corresponding lines in Figure~\ref{fig:results1} are essentially overlapping). This is due to the fact that the consistency checking algorithms of \citet{DBLP:journals/pacmpl/BiswasE19} are polynomial time when the number of sessions is fixed, which makes them fast at least on histories with few sessions.
 
%We test effective running time comparing $\textsc{explore-ce}$ under $\CC$ and $\textsc{explore-ce}^*$ under the pairs of isolation levels $\langle\CC, \SI\rangle$ and $\langle\CC, \SER\rangle$. In addition, we compare strong-optimality versus optimality running $\textsc{explore-ce}$ under $\CC$ and $\textsc{explore-ce}^*$ under the pairs of isolation levels $\langle\RA, \CC\rangle$, $\langle\RC, \CC\rangle$ and $\langle \texttt{true}, \CC\rangle$ (where $\texttt{true}$ is the isolation level under which every history is consistent). We compare both algorithms with a naive DFS algorithm that explores all possible transactions' interleavings under $\Rightarrow_I$ semantics, $\textsc{DFS}(\CC)$. It only allows a pending transaction at a time, all $\textsc{DFS}(\CC)$'s executions are $\CC$-consistent and there has not any incomplete execution (Theorem~\ref{theorem:causalExtensibleModels-CC-RA-RC}).
%The running times and number of histories are reported in Figure~\ref{fig:results1}. On one hand, the time complexity of $\textsc{explore-ce}^*$ is essentially determined by the underlying isolation level ($\CC$). On the other hand, the weaker the underlying with respect the tested isolation level, the worse the running time. In particular, $\textsc{explore-ce}$ under $\CC$ beats every competitor and no $\textsc{explore-ce}^*$ algorithm is beat by $\textsc{DFS}(\CC)$.

%As probably expected, $\textsc{explore-ce}^*$ under $(\RA, \CC)$ has a worse performance than $\textsc{explore-ce}$ under $\CC$ because of the time spent in enumerating histories which are $\RA$ but not $\CC$. As it can be seen in Figure~\ref{fig:results-histories} there is large gap between the number of $\RA$ and $\CC$ histories. 
%The time difference between $\textsc{explore-ce}(I_0)$ and $\textsc{explore-ce}^*(I_0,I)$ is insignificant. This is due to the fact that the consistency checking algorithms of \citet{DBLP:journals/pacmpl/BiswasE19} are polynomial time when the number of sessions is fixed, which makes them fast at least on histories with few sessions.

%We run $\textsc{explore-ce}$ for these programs under $\CC$, and $\textsc{explore-ce}^*$ under the pairs of isolation levels $\langle\CC, \SI\rangle$ and $\langle\CC, \SER\rangle$, $\langle\RA, \CC\rangle$, $\langle\RC, \CC\rangle$, $\langle \texttt{true}, \CC\rangle$ and DFS$(\CC)$. The running times and number of histories are reported in Figure~\ref{fig:results1}. As probably expected, $\textsc{explore-ce}^*$ under $(\RA, \CC)$ has a worse performance than $\textsc{explore-ce}$ under $\CC$ because of the time spent in enumerating histories which are $\RA$ but not $\CC$. As it can be seen in Figure~\ref{fig:results-histories} there is large gap between the number of $\RA$ and $\CC$ histories. The time difference between $\textsc{explore-ce}(I_0)$ and $\textsc{explore-ce}^*(I_0,I)$ is insignificant. This is due to the fact that the consistency checking algorithms of \citet{DBLP:journals/pacmpl/BiswasE19} are polynomial time when the number of sessions is fixed, which makes them fast at least on histories with few sessions.

%However, this is not anymore true for $\textsc{explore-ce}^*$ under $\langle\CC, \SI\rangle$ and $\langle\CC, \SER\rangle$. The difference w.r.t. $\textsc{explore-ce}$ under $\CC$ is insignificant. A possible explanation is that the difference between the number of $\RA$ histories and $\CC$ histories is significantly bigger than the difference between the number of $\CC$ histories and $\SI$/$\SER$ histories.

%number of $\CC$ histories is significantly smaller than $\RA$ histories, the cost of checking consistency with $\SI$ and $\SER$ in $\textsc{explore-ce}^*$ with $\langle\CC, \SI\rangle$ and $\langle\CC, \SER\rangle$. As $\CC$ is a causal-extensible model and it had a reasonable performance under this experiment, in the following we will work only with this isolation level.


%The results obtained in Figure~\ref{fig:results-ra-cc} show that an increment on the number of history a program may have under a concrete isolation level, obliges an increasing execution time. As expected, $\textsc{explore-ce}^*$ under $(\RA, \CC)$ have a worse performance than $\textsc{explore-ce}$ under $\CC$ by the cost of checking if a history satisfies $\CC$. However, as the number of $\CC$ histories is significantly smaller than $\RA$ histories, the cost of $\SI$ and $\SER$ checks is diluted. As $\CC$ is a causal-extensible model and it had a reasonable performance under this experiment, in the following we will work only with this isolation level.
% Moreover, it can be even worse than simply $\textsc{explore-ce}$ under $I_0$ if the $\evaluate$'s cost for isolation level $I$ is higher than the cost of checking if $h$ satisfies $I_0$. Moreover, as seen in Figure~\ref{fig:results-histories}, we notice that even for isolation levels with thousands of histories, the algorithm terminates in a reasonable time (no longer than one hour). As $\CC$ have a reasonable performance under this experiment, in the following we will work only with this isolation level.


% measures the algorithm's performance under $\CC$. We design three types of performance situations depending the number of shared variables a pair of transactions update; being ``Light'' a situation where every update is potentially read by only one thread, ``Heavy'' one where every update writes a variable every transaction reads and ``Medium'' something in between. For each of them, we describe five TPC-C programs, with a different number of threads each, between $1$ and $5$, with only one transaction per thread.

%writing transactions, and make them the same


%THREAD SCALABILITY

\begin{figure}[t]
	\centering
	\begin{subfigure}[b]{0.322\linewidth}
		\centering
		\includegraphics[width=\textwidth]{figures/th-scala.eps}
		\vspace{-5mm}
		\caption{Increasing sessions.}
		%\caption{Execution time. For the last ``heavy'' client with 5 sessions, the running time  $20' 40''$ (out of the figure).}
		\label{fig:results-threads}
	\end{subfigure}
	\hspace{1cm}
	%\hfill
	\begin{subfigure}[b]{0.422\linewidth}
		\centering
		\includegraphics[width=0.76\textwidth]{figures/tr-scala.eps}
		\vspace{-1mm}
		\caption{Increasing transactions per session.}
		%\caption{Number of histories. For the last ``heavy'' client with 5 sessions, the number of histories $1296$.}
		\label{fig:results-transactions}
	\end{subfigure}
\vspace{-3.5mm}
	\caption{Evaluating the scalability of $\textsc{explore-ce}(\CC)$ for TPC-C and Wikipedia client programs when increasing their size. These plots include benchmarks that timeout ($30$ mins): $6$ and $10$ for $4$ and $5$ sessions respectively in Figure~\ref{fig:results-threads}, and $2$ and $5$ for $4$ and $5$ transactions per sessions respectively in Figure~\ref{fig:results-transactions}.}
	\label{fig:results2}
\vspace{-3mm}
\end{figure}


In our second experiment, we investigate the scalability of $\textsc{explore-ce}$ when increasing the number of sessions. For each $i\in [1,5]$, we consider five \nver{(independent)} client programs for TPC-C and five for Wikipedia\oldver{\footnote{We focus on these two applications because they are of similar complexity.}}
(10 in total) with $i$ sessions, each session containing three transactions\footnote{We consider 10 client programs with 5 sessions, and remove sessions one by one to obtain client programs with a smaller number of sessions.}. We take $\CC$ as isolation level. 
%We take as benchmark five client programs for TPC-C and Wikipedia (10 in total) and $\CC$ as isolation level. 
%For each $i\in [1,5]$, we consider three clients per application with $i$ sessions, each containing three transactions. Starting with a client with $5$ sessions, we remove $5-i$ to obtain a client with $i$ sessions.
%Clients with less than five sessions are obtained by removing several sessions from a client with five sessions.  %Every client with $i$ sessions is obtained removing $5-i$ sessions satisfying every client with $i < 5$ sessions is a sub-client of a $i+1$ session client in our benchmark.
%of increasing levels of difficulty that we call light, medium, and heavy. For light clients, each session contains a different type of transaction, so the number of read/write interactions on the same variable is small. All sessions of a heavy client contain the same type of transaction in all sessions, and medium difficulty is an average of these two. 
The plot in Figure~\ref{fig:results-threads} shows average running time and memory consumption for each number $i\in [1,5]$ of sessions. 
As expected, increasing the number of sessions is a bottleneck running time wise because the number of histories/executions increases significantly as well. However, memory consumption does not grow with the same trend, as expected from the polynomial space complexity bound.
%As expected, at least for ``heavy'' clients, increasing the number of sessions remains a bottleneck. This is expected also because the size of the output (number of histories) increases significantly as well.
%However, under the small-scope hypothesis, the systematic enumeration of \emph{all} executions of small clients enabled by our algorithms should suffice to uncover all potential bugs in an application. 

%In Figure~\ref{fig:results2} we can observe that, in general, the running time of this algorithm does not exceed the couple of minutes in almost every situation. Moreover, we point that the cost of swapping two events has a sensible impact on the overall performance.

For our third experiment, we evaluate the scalability of $\textsc{explore-ce}(\CC)$ when increasing the number of transactions per session. We consider five \nver{(independent)} TPC-C client programs and five \nver{(independent)} Wikipedia client programs with $3$ sessions and $i$ transactions per session, for every $i\in [1,5]$. 
%As before, we remove $5-i$ transactions in each session of a client with $5$ transaction per thread to obtain our benchmarks. 
Figure~\ref{fig:results-transactions} shows average running time and memory consumption for each number $i\in [1,5]$ of transactions per session. Increasing the number of transactions per session is also a bottleneck for the same reasons as before.
% consists on, fixed the number of threads per program, studying how the algorithm performance when those programs have different number of transactions. Analogously to the previous situation, we design three types of performance situations, ``Heavy'', ``Medium'' and ``Light''; and for each of them, we analyze five TPC-C programs with different number of transactions, from $1$ to $5$.

%The memory consumption in all these experiments fluctuates around $500$MB, taking into account that JPF forces a minimum consumption of $256$MB. There are no remarkable memory consumption variations between different isolation levels. There are slight grows in memory consumption when the number of histories increases. The particular implementation of each benchmark represent the most important source of memory consumption's variability.

%The results in Figure~\ref{fig:results3} supports our claims as the bigger the total space to explore, the greater the time it has to be consumed in the exploration.