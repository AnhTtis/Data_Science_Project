%!TEX root = main.tex
\section{Experiments}
The effectiveness of our model is evaluated on two datasets: 1) ImageNet \cite{deng2009imagenet} and 2) FFHQ \cite{karras2019style}. The ImageNet dataset consists of $1.2M$ images over $1K$ classes, whereas the FFHQ dataset contains $\sim 70K$ images of curated human faces. All our models have $64$ dimensional latent space sampled from a normal distribution with mean $0$ and standard deviation $1$. The affine parameters space $\mathbf{W}$ of the mapping network is $512$ dimensions, and the synthesis network consists of $10$ levels with feature dimension $n=1024$ for the ImageNet and $n=512$ for FFHQ. We follow the training scheme of the StyleGAN-XL method \cite{sauer2022stylegan} and use a projected discriminator based on the pre-trained classifiers (DeiT \cite{touvron2021training} and EfficientNet \cite{tan2019efficientnet}) with an additional classifier guidance loss \cite{dhariwal2021diffusion}.

% In our case, it is more practical to train the model progressively with increasing resolution since it generates each pixel independently. A low-resolution image is computationally less costly to generate than a high resolution; hence the model can be trained for a larger number of iterations at low resolution. 
We train our model progressively with increasing resolution, i.e., we start by training at low resolution and continue training with higher resolutions as training progresses. Since the computational cost is less at low resolution, the model is trained for large number of iterations, followed by training for high resolution. Since the model is already trained at low resolution, fewer iterations are needed for convergence at high resolution. 
However, unlike StyleGAN-XL, which freezes the previously trained layers and introduces new layers for higher resolution, Poly-INR uses a fixed number of layers and trains all the parameters at every resolution. 
% The model is first trained on $32\times32$ for $200$M images (number of images shown to the discriminator), followed by  $64\times 64$ for $72$M images, $128\times 128$ for $21$M images, $256\times256$ for $10$M images,  and $512\times 512$ for $2$M images. 
%More details of the training procedure are presented in the \textit{ supplementary material.}
% learning rate of 1e-4 and 2e-4, batch size 2024, 512, 128



\subsection{Quantitative results}
%mterics and models used for comparision, results
We compare our model against CNN-based GANs (BigGAN \cite{brock2018large} and StyleGAN-XL \cite{sauer2022stylegan}) and diffusion models (CDM \cite{ho2022cascaded}, ADM, ADM-G \cite{dhariwal2021diffusion}, and DiT-XL \cite{peebles2022scalable}) on the ImageNet dataset. We also report results on the FFHQ dataset for INR-based GANs (CIPS \cite{anokhin2021image} and INR-GAN \cite{skorokhodov2021adversarial}) as they do not train models on ImageNet. 
% We also compare our model against the INR-based GANs (CIPS \cite{anokhin2021image} and INR-GAN \cite{skorokhodov2021adversarial}) for FFHQ dataset. 

\noindent \textbf{Quantitative metrics:} We use Inception Score (IS) \cite{salimans2016improved}, Frechet Inception Distance (FID) \cite{heusel2017gans}, Spatial Frechet Inception Distance (sFID) \cite{nash2021generating}, random-FID (rFID) \cite{sauer2022stylegan}, precision (Pr), and recall (Rec) \cite{kynkaanniemi2019improved}. IS (higher the better) quantifies the quality and diversity of the generated samples based on the predicted label distribution by the Inception network but does not compare the distribution of the generated samples with the real distribution. The FID score  (lower the better) overcomes this drawback by measuring the Frechet distance between the generated and real distribution in the Inception feature space. Further, sFID uses higher spatial features from the Inception network to account for the spatial structure of the generated image. Like StyleGAN-XL, we also use the rFID score to ensure that the network is not just optimizing for IS and FID scores. We use the same randomly initialized Inception network provided by \cite{sauer2022stylegan}. In addition, we also compare our model on the precision and recall metric (higher the better) that measures how likely the generated sample is from the real distribution.

Table \ref{table:quantitative} summarizes the results on the ImageNet dataset at different resolutions. The results for existing methods are quoted from the StyleGAN-XL paper. We observe that the performance of the proposed model is third best after DiT-XL and StyleGAN-XL on the FID and IS metrics. The proposed model outperforms the ADM and BigGAN models at all resolutions and performs comparably to the StyleGAN-XL at $128\times128$ and $256\times256$. We also observe that with the increase in image size, the FID score for Poly-INR drops much more than StyleGAN-XL. The FID score drops more because our model does not add any additional layers with the increase in image size. For example, the StyleGAN-XL uses $134.4$M parameters at $64\times 64$ and $168.4$M at $512\times512$, whereas Poly-INR uses only $46.0$M parameters at every resolution, as reported in Table \ref{table:params}(d). The table shows that our model performs comparably to the state-of-the-art CNN-based generative models, even with significantly fewer parameters. On precision metric, the Poly-INR method performs comparably to other methods; however, the recall value is slightly lower compared to StyleGAN-XL and diffusion models at higher resolution. Again, this is due to the small model size, limiting the model's capacity to represent much finer details at a higher resolution. 

We also compare the proposed method with other INR-based GANs: CIPS and INR-GAN on the FFHQ dataset. Table \ref{table:ffhq} shows that the proposed model significantly outperforms these models, even with a small generator model. Interestingly the Poly-INR method outperforms the StyleGAN-2 and performs comparable to StyleGAN-XL, using significantly fewer parameters. Table \ref{table:ffhq} also reports the inference speed of these models on a Nvidia-RTX-6000 GPU. StyleGANs and INR-GAN use a multi-scale architecture, resulting in faster inference. In contrast, CIPS and Poly-INR models perform all computations at the same resolution as the output image, increasing the inference time. 
%Our Poly-INR model offers faster inference speeds than CIPS due to its parameter efficiency.
 \begin{figure}[]
     \centering
     %\vspace{-0.2in}
         \includegraphics[width=0.95\columnwidth]{cvpr2023-author_kit-v1_1-1/latex/figures/poly-inr-lvl-vis.pdf}
         %\vspace{-0.1in}
        \caption{Heat-map visualization at different levels of the synthesis network. At initial levels, the model captures the basic shape of the object, and at higher levels, the image's finer details are captured.}
        \label{fig:heatmap}
        %\vspace{-0.10in}
\end{figure}

\begin{figure}[]
     \centering
     %\vspace{-0.1in}
         \includegraphics[width=0.95\columnwidth]{cvpr2023-author_kit-v1_1-1/latex/figures/extrapolate.pdf}
        %\vspace{-0.1in}
        \caption{Few example images showing extrapolation outside the image boundary (yellow square). The Poly-INR model is trained to generate images on the coordinate grid $[0,1]^2$. For extrapolation, we use the grid size $[-0.25, 1.25]^2$. Our model generates continuous image outside the conventional boundary.}
        \label{fig:extrapolation}
        %\vspace{-0.2in}
\end{figure}


\begin{figure*}[]
     \centering
     \vspace{-0.2in}
         \includegraphics[width=0.95\textwidth]{cvpr2023-author_kit-v1_1-1/latex/figures/polyinr-interpolate.pdf}
        \vspace{-0.1in}
        \caption{Linear interpolation between two random points. The first two rows represent interpolation in the latent space, while in the last two, we directly interpolate between the affine parameters. Poly-INR provides smooth interpolation even in a high dimension of affine parameters. Our model generates high-fidelity images similar to state-of-the-art models like StyleGAN-XL but without the need for convolution or self-attention mechanism. Comparisons with existing methods are present in the supplementary material. }
        \label{fig:interpolation}
        %\vspace{-0.15in}
\end{figure*}

\begin{figure*}[]
     \centering
     %\vspace{-0.1in}
         \includegraphics[width=0.75\textwidth]{cvpr2023-author_kit-v1_1-1/latex/figures/style-mixing.pdf}
        \vspace{-0.1in}
        \caption{Source A and B images are generated corresponding to random latent codes, and the rest of the images are generated by copying the affine parameters of source A to source B at different levels. Copying the higher levels' ($8$ and $9$) affine parameters leads to finer style changes, whereas copying the middle levels' ($7$, $6$, and $5$) leads to coarse style changes. }
        \label{fig:stylemixing}
        %\vspace{-0.15in}
\end{figure*}

\subsection{Qualitative results}
%fig1, diversity, sampling at different resolution
Fig. \ref{fig:teaser} shows images sampled at different resolutions by the Poly-INR model trained on $512 \times 512$. We observe that our model generates diverse images with very high fidelity. Even though the model does not use convolution or self-attention layers, it generates realistic images over datasets like ImageNet. In addition, the model provides flexibility to generate images at different scales by changing the size of the coordinate grid, making the model efficient if low-resolution images are needed for a downstream task. In contrast, CNN-based models generate images only at the training resolution due to the non-equivariant nature of the convolution kernels to image scale. 
%Additional generated images and qualitative comparisons with the StyleGAN-XL model are provided in the \textit{supplementary}.

\noindent\textbf{Heat-map visualization: }Fig. \ref{fig:heatmap} visualizes the heat-map at different levels of our synthesis network. To visualize a feature as a heat-map, we first compute the mean along the spatial dimension of the feature and use it as a weight to sum the feature along the channel dimension. In the figure, we observe that in the initial levels ($0$-$3$), the model forms the basic structure of the object. Meanwhile, in the middle levels ($4$-$6$), it captures the object's overall shape, and in the higher levels ($7$-$9$), it adds finer details about the object. Furthermore, we can interpret this observation in terms of polynomial order. Initially, it only approximates low-order polynomials and represents only basic shapes. However, at higher levels, it approximates higher-order polynomials representing finer details of the image.




\noindent\textbf{Extrapolation:}
The INR model is a continuous function of the coordinate location; hence we extrapolate the image by feeding the pixel location outside the conventional image boundary. Our Poly-INR model is trained to generate images on the coordinate grid defined by $[0,1]^2$. We feed the grid size $[-0.25, 1.25]^2$ to the synthesis network to generate the extrapolated images. Fig. \ref{fig:extrapolation} shows a few examples of extrapolated images. In the figure, the region within the yellow square represents the conventional coordinate grid $[0,1]^2$. The figure shows that our INR model not only generates a continuous image outside the boundary but also preserves the geometry of the object present within the yellow square. However, in some cases, the model generates a black or white image border, resulting from the image border present in some real images of the training set. 


\begin{figure*}[t!]
     \centering
     %\vspace{-0.15in}
         \includegraphics[width=0.95\textwidth]{cvpr2023-author_kit-v1_1-1/latex/figures/ood-interpolation1.pdf}
        \vspace{-0.1in}
        \caption{The Poly-INR model generates smooth interpolation with embedded images in affine parameters space. The leftmost image (first row) is from the ImageNet validation set, and the last two (rightmost) are the OOD images.}
        \label{fig:oodinter}
        \vspace{-0.2in}
\end{figure*}
\begin{figure}[ht!]
     \centering
     %\vspace{-0.15in}
         \includegraphics[width=0.95\columnwidth]{cvpr2023-author_kit-v1_1-1/latex/figures/embedd_style_mix.pdf}
        %\vspace{-0.1in}
        \caption{Style-mixing with embedded images in affine parameters space. Source B is the embedded image from the ImageNet validation
set, mixed with the style of randomly sampled source A image.}
        \label{fig:oodstylemix}
        %\vspace{-0.2in}
\end{figure}
\begin{table}[ht!]
%\vspace{-0.1in}
\caption{FID score (lower the better) evaluated at $512 \times 512$ for models trained at a lower resolution and compared against classical interpolation-based upsampling.}
%\vspace{-0.1in}
\label{table:highres_sampling}
\centering
\begin{small}
\begin{tabular}{ccccc}
%\centering
\toprule
\multicolumn{1}{c}{\textbf{Training}} & \multicolumn{1}{c}{\textbf{Nearest}}& 
\multicolumn{1}{c}{\textbf{Bilinear}}&
\multicolumn{1}{c}{\textbf{Bicubic}}&
\multicolumn{1}{c}{\textbf{Poly-INR}}
\\
\textbf{Resolution} & \textbf{Neighbour}& & &\\
\midrule
32$\times$32 & 184.39& 112.28& 73.86 &65.15\\
64$\times$64 & 89.24& 72.41 & 42.97 &36.30\\
%128x128 & 12.26 & 11.13& 12.02 &14.88\\
 \bottomrule
\end{tabular}
\end{small}
%\vspace{-0.15in}
\end{table}

\noindent\textbf{Sampling at higher-resolution:}
Another advantage of using our model is the flexibility to generate images at any resolution, even if the model is trained on a lower resolution. We generate a higher-resolution image by sampling a dense coordinate grid within the $[0,1]^2$ range. Table \ref{table:highres_sampling} shows the FID score evaluated at $512\times512$ for models trained on the lower-resolution ImageNet dataset. We compare the quality of upsampled images generated by our model against the classical interpolation-based upsampling methods. The table shows that our model generates crisper upsampled images, achieving a significantly better FID score than the classical interpolation-based upsampling method. However, we do not observe significant FID score improvement for our Poly-INR model trained on $128\times128$ or higher resolution against the classical interpolation techniques. This could be due to the limitations of the ImageNet dataset, which primarily consists of lower-resolution images than the $512\times512$. We used bilinear interpolation to prepare the training dataset at $512\times512$. As per our knowledge, there are currently no large and diverse datasets like ImageNet with high-resolution images. We believe this performance can be improved when the model has access to higher-resolution images for training. We also compare the upsampling performance with other INR-based GANs by reporting the FID scores at $1024\times1024$ for models trained on FFHQ-$256\times256$ as follows: \textbf{Poly-INR:13.69, INR-GAN: 18.51, CIPS:29.59}. Our Poly-INR model provides better high-resolution sampling than the other two INR-based generators.
% An additional advantage of using our model is the flexibility to generate images at any resolution, even if the model is trained on a lower resolution. We generate a higher-resolution image by sampling a dense coordinate grid within the $[0,1]^2$ range. Table \ref{table:highres_sampling} shows the FID score evaluated at $512\times512$ for models trained on the lower-resolution ImageNet dataset. We cannot compare our performance with existing INR-based generative models like CIPS and INR-GAN as they are limited to smaller datasets like FFHQ. However, we compare the quality of upsampled images generated by our model against the classical interpolation-based upsampling methods. The table shows that our model generates crisper upsampled images, achieving a significantly better FID score than the classical interpolation-based upsampling method. However, we do not observe significant FID score improvement for our Poly-INR model trained on $128\times128$ or higher resolution against the classical interpolation techniques. This could be due to the limitations of the ImageNet dataset, which primarily consists of lower-resolution images than the $512\times512$. We used bilinear interpolation to prepare the training dataset at $512\times512$. As per our knowledge, there are currently no large and diverse datasets like ImageNet with high-resolution images. We believe this performance can be improved when the model has access to higher-resolution images for training. 

\noindent\textbf{Interpolation:}
Fig. \ref{fig:interpolation} shows that our model generates smooth interpolation between two randomly sampled images. In the first two rows of the figure, we interpolate in the latent space, and in the last two rows, we directly interpolate between the affine parameters. In our synthesis network, only the affine parameters depend on the image, and other parameters are fixed for every image. Hence interpolating in affine parameters space means interpolation in INR space. Our model provides smoother interpolation even in the affine parameters space and interpolates with the geometrically coherent movement of different object parts. For example, in the first row, the eyes, nose, and mouth move systematically with the whole face. 

\noindent\textbf{Style-mixing:} Similar to StyleGANs, our Poly-INR model transfers the style of one image to another. Our model generates smooth style mixing even though we do not use any style-mixing regularization during the training. Fig. \ref{fig:stylemixing} shows examples of style-mixing from source A to source B images. For style mixing, we first obtain the affine parameters corresponding to the source A and B images and then copy the affine parameters of A to B at various levels of the synthesis network. Copying affine parameters to higher levels ($8$ and $9$) leads to finer changes in the style, while copying to middle levels ($7$, $6$, and $5$ ) leads to the coarse style change. Mixing the affine parameters at initial levels changes the shape of the generated object. In the figure, we observe that our model provides smooth style mixing while preserving the original shape of the source B object.







\noindent\textbf{Inversion:}
%inverted in affine parameters, psnr comparision, provide smooth interpolation however less fidelity
Embedding a given image into the latent space of the GAN is an essential step for image manipulation. In our Poly-INR model, for inversion, we optimize the affine parameters to minimize the reconstruction loss, keeping the synthesis network's parameters fixed. We use VGG feature-based perceptual loss for optimization. We embed the ImageNet validation set in the affine parameters space for the quantitative evaluation. Our Poly-INR method effectively embeds images with high PSNR scores (\textbf{PSNR:}$\mathbf{26.52}$ and \textbf{SSIM:}$\mathbf{0.76}$), better than StyleGAN-XL (PSNR:$13.5$ and SSIM:$0.33$). However, our affine parameters dimension is much larger than the StyleGAN-XL's latent space. Even though the dimension of the affine parameters is much higher, the Poly-INR model provides smooth interpolation for the embedded image. Fig. \ref{fig:oodinter} shows examples of interpolation with embedded images. In the figure, the first row (leftmost) is the embedded image from Val set, and the last two rows (rightmost) are the out-of-distribution images. Surprisingly, our model provides smooth interpolation for OOD images. In addition, Fig. \ref{fig:oodstylemix} shows smooth style-mixing with the embedded images. In some cases, we observe that the fidelity of the interpolated or style-mixed image with the embedded image is slightly less compared to samples from the training distribution. This is due to the large dimension of the embedding space, which sometimes makes the embedded point farther from the training distribution. It is possible to improve interpolation quality further by using the recently proposed pivotal tuning inversion method \cite{roich2022pivotal}, which finetunes the generator's parameters around the embedded point.


\subsection{Discussion}
The proposed Poly-INR model performs comparably to state-of-the-art generative models on large ImageNet datasets without using convolution or self-attention layers. In addition to smooth interpolation and style-mixing, the Poly-INR model provides attractive flexibilities like image extrapolation and high-resolution sampling. In this work, while we use our INR model for $2$D image datasets, it can be extended to other modalities like $3$D datasets.

\noindent\textbf{Challenges:} One of the challenges in our INR method is the higher computation cost compared to the CNN-based generator model for high-resolution image synthesis. The INR method generates each pixel independently; hence all the computation takes place at the same resolution. In contrast, a CNN-based generator uses a multi-scale generation pipeline, making the model computationally efficient. In addition, we observe common GAN artifacts in some generated images. For example, in some cases, it generates multiple heads and limbs, missing limbs, or the objectâ€™s geometry is not correctly synthesized. We suspect that the CNN-based discriminator only discriminates based on the object's parts and fails to incorporate the entire shape.



% INR-based GAN models are also extended to 3D datasets \cite{dupont2021generative} or $3$D-aware image synthesis \cite{chan2022efficient,zhou2021cips, kwak2022injecting,xu20223d}. 

% \begin{table}[ht!]
% %\vspace{-0.1in}
% \caption{Comparison with StyleGAN-XL}
% \label{table:sgxl_compare}
% \centering
% \begin{small}
% \begin{tabular}{ccccccc}
% %\centering
% \toprule
% \multicolumn{1}{c}{Resolution} & \multicolumn{3}{c}{StyleGAN-XL}&  \multicolumn{3}{c}{Poly-INR} \\
% & params & Imgs  & FID $\downarrow$& params & Imgs  & FID $\downarrow$\\


% \midrule
% 16$\times$16 & 18.0M & 300M& 0.73& 46.0M & - & -\\
% 32$\times$32 & 84.6M & 175M&1.10 & 46.0M & 200M & 1.24\\
% 64$\times$64 & 134.4M & 95M&1.51 & 46.0M & 72M & 2.09\\
% 128$\times$128 & 158.7M &57M &1.81 & 46.0M & 21M & 2.08\\
% 256$\times$256 & 166.3M &11M &2.30 & 46.0M & 10M & 2.86\\
% 512$\times$512 & 168.4M &2M &2.41 & 46.0M & 2M & 3.84\\
%  \bottomrule
% \end{tabular}
% \end{small}
% \end{table}







