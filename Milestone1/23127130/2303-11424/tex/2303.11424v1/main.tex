% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{url}            % simple URL typesetting
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{paralist}
\usepackage{caption}
\usepackage{subcaption}
%\usepackage{subfigure}
\usepackage{etoolbox}
\usepackage{float}
\usepackage{tabularx}
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{5938} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Polynomial Implicit Neural Representations For Large Diverse Datasets}

\author{Rajhans Singh  \qquad Ankita Shukla \qquad Pavan Turaga\\
 \qquad Geometric Media Lab, Arizona State University\\
{\tt\small \{rsingh70, ashukl20, pavan.turaga\}@asu.edu}
}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}

% \and Geometric Media Lab\\
% Arizona State University\\
% {\tt\small rsingh70@asu.edu}
% }

% \twocolumn[{%
% \renewcommand\twocolumn[1][]{#1}%
% \maketitle
%   \centering
%   %\vspace{-0.3in}

%   \begin{subfigure}{0.99\textwidth}\centering
%      \centering
%         % \vspace{-0.15in}
%          \includegraphics[ width=0.99\textwidth]{cvpr2023-author_kit-v1_1-1/latex/figures/poly-teaser.pdf}
%         %\vspace{-0.1in}
% \end{subfigure}

% \captionof{figure}{Samples generated by our Poly-INR model on the ImageNet dataset at various resolutions. Our model generates images with high fidelity without using convolution, upsample, or self-attention layers, i.e., no interaction between the pixels.}
% \label{fig:teaser}

%   \vspace{3mm}
% }]
\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=.99\textwidth]{cvpr2023-author_kit-v1_1-1/latex/figures/poly-teaser.pdf}
    \captionof{figure}{Samples generated by our Poly-INR model on the ImageNet dataset at various resolutions. Our model generates images with high fidelity without using convolution, upsample, or self-attention layers, i.e., no interaction between the pixels.}
    \label{fig:teaser}
\end{center}%
}]

\maketitle

%%%%%%%%% ABSTRACT
% Recently, there has been a significant interest in representing an image as a continuous function of its coordinate location and approximating that function with a neural network. Such representation of an image is called
% Implicit Neural Representations (INR) have gained significant popularity for image representation as a continuous function of the pixel location. 
\begin{abstract}
\vspace{-3mm}
Implicit neural representations (INR) have gained significant popularity for signal and image representation for many end-tasks, such as superresolution, 3D modeling, and more. Most INR architectures rely on sinusoidal positional encoding, which accounts for high-frequency information in data. However, the finite encoding size restricts the model's representational power. Higher representational power is needed to go from representing a single given image to representing large and diverse datasets. Our approach addresses this gap by representing an image with a polynomial function and eliminates the need for positional encodings. Therefore, to achieve a progressively higher degree of polynomial representation, we use element-wise multiplications between features and affine-transformed coordinate locations after every ReLU layer. The proposed method is evaluated qualitatively and quantitatively on large datasets like ImageNet. The proposed Poly-INR model performs comparably to state-of-the-art generative models without any convolution, normalization, or self-attention layers, and with far fewer trainable parameters. With much fewer training parameters and higher representative power, our approach paves the way for broader adoption of INR models for generative modeling tasks in complex domains. The code is available at \url{https://github.com/Rajhans0/Poly_INR}
\end{abstract}
\vspace{-4mm}
%%%%%%%%% BODY TEXT
\input{intorduction}
\input{related_work}
\input{method}
\input{experiments}

\section{Conclusion}
In this work, we propose polynomial function based implicit neural representations for large image datasets while only using Linear and ReLU layers. Our Poly-INR model captures high-frequency information and performs comparably to the state-of-the-art CNN-based generative models without using convolution, normalization, upsampling, or self-attention layers. The Poly-INR model outperforms previously proposed positional embedding-based INR GAN models. We demonstrate the effectiveness of the proposed model for various tasks like interpolation, style-mixing, extrapolation, high-resolution sampling, and image inversion. Additionally, it would be an exciting avenue for future work to extend our Poly-INR method for $3$D-aware image synthesis on large datasets like ImageNet.   

\section*{Acknowledgements}
This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Agreement No. HR00112290073. Approved for public release; distribution is unlimited.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}
\input{supplementary}
\end{document}
