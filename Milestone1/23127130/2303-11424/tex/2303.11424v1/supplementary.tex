% \documentclass[10pt,twocolumn,letterpaper]{article}
% %\usepackage[rebuttal]{cvpr}
% \usepackage{cvpr}   
% % Include other packages here, before hyperref.
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{booktabs}
% \usepackage[accsupp]{axessibility} 

% % If you comment hyperref and then uncomment it, you should delete
% % egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% % run, let it finish, and you should be clear).
% \usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% % Support for easy cross-referencing
% \usepackage[capitalize]{cleveref}
% \crefname{section}{Sec.}{Secs.}
% \Crefname{section}{Section}{Sections}
% \Crefname{table}{Table}{Tables}
% \crefname{table}{Tab.}{Tabs.}

% % If you wish to avoid re-using figure, table, and equation numbers from
% % the main paper, please uncomment the following and change the numbers
% % appropriately.
% %\setcounter{figure}{2}
% %\setcounter{table}{1}
% %\setcounter{equation}{2}

% % If you wish to avoid re-using reference numbers from the main paper,
% % please uncomment the following and change the counter for `enumiv' to
% % the number of references you have in the main paper (here, 6).
% %\let\oldthebibliography=\thebibliography
% %\let\oldendthebibliography=\endthebibliography
% %\renewenvironment{thebibliography}[1]{%
% %     \oldthebibliography{#1}%
% %     \setcounter{enumiv}{6}%
% %}{\oldendthebibliography}


% %%%%%%%%% PAPER ID  - PLEASE UPDATE
% \def\cvprPaperID{5938} % *** Enter the CVPR Paper ID here
% \def\confName{CVPR}
% \def\confYear{2023}

% \begin{document}
% \onecolumn
% %%%%%%%%% TITLE - PLEASE UPDATE
% \title{Supplementary Material For \\ Polynomial Implicit Neural Representations For Large Diverse Datasets}  % **** Enter the paper title here

% \maketitle
% \thispagestyle{empty}
%!TEX root = main.tex
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
\section{Training details}
%training batch, learning rates, and other details
\textbf{ImageNet:} We train the Poly-INR model progressively with increasing resolution. The Poly-INR model is first trained on $200M$ images at $32\times32$ with $2048$ batch size, followed by $72M$ images at $64\times64$ with $512$ batch size, $21M$ images at $128\times128$ with $256$ batch size, $10M$ images at $256\times256$ with $128$ batch size and $2M$ images at $512\times512$ with $128$ batch size. We use learning rate of $1e^{-4}$ for the generator and $2e-{4}$ for the discriminator. We use Adam optimizer for both the generator and discriminator with $beta=(0.0, .99)$ and $eps=1e^{-8}$ and the classifier guidance loss weight is set to 8.0 starting at $128\times128$ and higher resolution. We do not use style mixing regularization and path length regularization.

\textbf{FFHQ:} We also train the Poly-INR model progressively with increasing resolution on the FFHQ dataset. We first train our model with $64\times64$ on $60M$ images using a batch size of $2048$, followed by $15M$ images at $128\times128$ with $256$ batch size and $15M$ images at $256\times256$ with $256$ batch size. The other training hyperparameters are same as the ImageNet experiments described above.

\begin{table}[h]
\centering
%\vspace{-0.02in}
%\begin{small}
\caption{Poly-INR performance on FFHQ-32x32 across various levels (lvl) and model size (number of parameters in Million).}
%\vspace{-0.12in}
\label{table:performance_levels}


\resizebox{0.50\textwidth}{!}{
\begin{tabular}{ccccccccc}
%\centering
\toprule
\multicolumn{1}{c}{} & \multicolumn{1}{c}{\textbf{Lvl-2}}& 
\multicolumn{1}{c}{\textbf{Lvl-4}}&
\multicolumn{1}{c}{\textbf{Lvl-7}}&
\multicolumn{1}{c}{\textbf{Lvl-10}}&
\multicolumn{1}{c}{\textbf{Lvl-14}}&
\multicolumn{1}{c}{\textbf{Lvl-14}}
\\
%#\textbf{Resolution} & \textbf{Neighbour}& & &\\
\midrule
Feat. Dim. &512 &512&512&512&512&1024\\
Params (M) & 2.98& 5.62& 9.57 &13.52&18.79&64.74\\
FID $\downarrow$& 27.01& 3.46 & 1.92 &1.83& 1.52& 1.12\\
Precision $\uparrow$&0.85&0.68&0.67&0.68& 0.68& 0.70\\
Recall$\uparrow$&0.01&0.41&0.56&0.57&0.59&0.63\\
%128x128 & 12.26 & 11.13& 12.02 &14.88\\
 \bottomrule
\end{tabular}}
%\end{small}
%\vspace{-0.20in}
\end{table}
%\noindent \textbf{[\textcolor{blue}{R2}]}
\section{Ablation study on the number of levels and feature dimension}
We present an ablation study in Table \ref{table:performance_levels}, demonstrating the Poly-INR performance on the FFHQ-32x32 dataset as levels increase. We observe that with increasing levels, the model's performance improves. We utilize $10$ levels in our experiments because of training stability and also achieve comparable performance compared to CNN-based models. In case of training with more than $10$ levels, we can incrementally increase the number of levels by first training the model on a lower number, such as $10$, and gradually add more levels as training progresses.

In Table \ref{table:performance_levels}, we increase the model capacity either by adding more levels (layers) or increasing the feature dimension on FFHQ-32x32. We observe that when the model capacity is very small, the recall score is also very poor, but as we increase the model parameters, the recall score gets much better. 




\section{Inference speeds across resolutions for our ImageNet model}
\begin{table}[h]
\centering
%\vspace{-0.02in}
%\begin{small}
\caption{Inference speed (sec-per-image) of Poly-INR model trained on the ImageNet dataset across various resolutions.}
%\vspace{-0.12in}
\label{table:speed_resolution}
%\resizebox{0.70\textwidth}{!}{
\begin{tabular}{ccccc}
%\centering
\toprule
 \multicolumn{1}{c}{\boldmath{$32^2$}}& 
\multicolumn{1}{c}{\boldmath{$64^2$}}&
\multicolumn{1}{c}{\boldmath{$128^2$}}&
\multicolumn{1}{c}{\boldmath{$256^2$}}&
\multicolumn{1}{c}{\boldmath{$512^2$}}
\\
%#\textbf{Resolution} & \textbf{Neighbour}& & &\\
\midrule
0.007 & 0.013 & 0.047 & 0.179 & 0.720\\
%128x128 & 12.26 & 11.13& 12.02 &14.88\\
 \bottomrule
\end{tabular}
%\end{small}
%\vspace{-0.20in}
\end{table}

Table \ref{table:speed_resolution} shows the inference speed (sec-per-image) across various resolutions of the Poly-INR model trained on the ImageNet dataset on a Nvidia-RTX-6000 GPU. The Poly-INR model synthesizes each pixel independently and performs all computations at the same resolution, resulting in slower inference time at higher resolutions.

\section{Affine parameters mixing}
An advantage of representing an image in the polynomial form is that it inherently breaks the image into shape and style. For example, the lower polynomial orders represent the object's shape, whereas the higher orders represent finer details like the style of the image. In our Poly-INR model, manipulating the lower levels' affine parameters changes the object's shape, and manipulating higher levels' affine parameters changes the style. Fig. \ref{fig:suppli_stylemixing} shows examples of style mixing from source A to source B images. In the figure, copying the affine parameters of source A to source B at higher levels ($8$ and $9$) brings fine change in the style, whereas middle levels ($5$, $6$, and $7$) bring coarse style change. Fig. \ref{fig:suppli_shapemixing} shows affine parameters mixing at initial levels ($0$-$5$). In the figure, we observe that copying the affine parameters at these levels changes the shape of the source B image to the source A image. 
\begin{figure*}[]
     \centering
     \vspace{-0.2in}
         \includegraphics[width=0.84\textwidth]{cvpr2023-author_kit-v1_1-1/latex/suppli_figures/suppli_style-mix.pdf}
        \vspace{-0.1in}
        \caption{Source A and B images  generated from random latent codes, and the remaining images are generated by copying the affine parameters of source A to source B at different levels. Copying the higher levels' ($8$ and $9$) affine parameters leads to finer style changes, whereas copying the middle levels' ($7$, $6$, and $5$) leads to coarse style changes. }
        \label{fig:suppli_stylemixing}
        %\vspace{-0.15in}
\end{figure*}
\begin{figure*}[]
     \centering
     \vspace{-0.3in}
         \includegraphics[width=0.84\textwidth]{cvpr2023-author_kit-v1_1-1/latex/suppli_figures/shape-mixing.pdf}
        \vspace{-0.1in}
        \caption{Source A and B images are generated from random latent codes, and remaining images are generated by copying the affine parameters of source A to source B at different levels. Copying the initial levels' ($0$, $1$, and $2$) affine parameters leads to finer shape changes, whereas copying slightly higher levels' ($3$, $4$, and $5$) leads to coarse shape changes. }
        \label{fig:suppli_shapemixing}
        %\vspace{-0.15in}
\end{figure*}
\section{Interpolation}
Fig. \ref{fig:suppli_interpolation} shows linear interpolation between samples of different classes in the affine parameters space. The Poly-INR model provides smooth interpolation between different classes. \textit{Please see the attached video for better interpolation visualization.}

\begin{figure*}[]
     \centering
     \vspace{-0.3in}
         \includegraphics[width=0.99\textwidth]{cvpr2023-author_kit-v1_1-1/latex/suppli_figures/suppli_interpolation.pdf}
        \vspace{-0.1in}
        \caption{The poly-INR model generates smooth interpolations between samples of different classes. }
        \label{fig:suppli_interpolation}
        %\vspace{-0.15in}
\end{figure*}


\section{ Qualitative comparison with StyleGAN-XL:}
We also compare the quality of images generated by Poly-INR model against state-of-the-art CNN-based StyleGAN-XL model for different classes. Fig. \ref{fig:suppli_sgxl_comp1}, \ref{fig:suppli_sgxl_comp2}, and \ref{fig:suppli_sgxl_comp3} show examples of images generated from different classes for the models trained on ImageNet at $256\times256$. The Poly-INR generates samples qualitatively similar to the StyleGAN-XL model but without using any convolution or self-attention layers.

\begin{figure*}[]
     \centering
     \vspace{-0.3in}
         \includegraphics[width=0.99\textwidth]{cvpr2023-author_kit-v1_1-1/latex/suppli_figures/suppli-classes1.pdf}
        \vspace{-0.1in}
        \caption{Qualitative comparison between StyleGAN-XL (left column) and Poly-INR (right column). Classes from top to bottom: agaric, daisy, volcano, seashore, cup, and beer glass. }
        \label{fig:suppli_sgxl_comp1}
        %\vspace{-0.15in}
\end{figure*}

\begin{figure*}[]
     \centering
     \vspace{-0.3in}
         \includegraphics[width=0.99\textwidth]{cvpr2023-author_kit-v1_1-1/latex/suppli_figures/suppli_class2.pdf}
        \vspace{-0.1in}
        \caption{Qualitative comparison between StyleGAN-XL (left column) and Poly-INR (right column). Classes from top to bottom: type writer, valley, pizza, wardrobe, spider web, barn spider. }
        \label{fig:suppli_sgxl_comp2}
        %\vspace{-0.15in}
\end{figure*}
\begin{figure*}[]
     \centering
     \vspace{-0.3in}
         \includegraphics[width=0.99\textwidth]{cvpr2023-author_kit-v1_1-1/latex/suppli_figures/suppli_class3.pdf}
        \vspace{-0.1in}
        \caption{Qualitative comparison between StyleGAN-XL (left column) and Poly-INR (right column). Classes from top to bottom: maltese dog, german shepherd, persian cat, bulbul, robin, american coot. }
        \label{fig:suppli_sgxl_comp3}
        %\vspace{-0.15in}
\end{figure*}
\section{Qualitative comparison with CIPS and INR-GAN on FFHQ dataset}
We also provide qualitative comparison of Poly-INR model against previously proposed INR-based generative models like CIPS and INR-GAN. Fig. \ref{fig:suppli_ffhq} shows samples generated by the three models trained on the FFHQ dataset at $256\times256$. Our Poly-INR model generates qualitatively better samples than the CIPS and INR-GAN using significantly fewer parameters.
\begin{figure*}[]
     \centering
     \vspace{-0.3in}
         \includegraphics[width=0.99\textwidth]{cvpr2023-author_kit-v1_1-1/latex/suppli_figures/suppli_ffhq.pdf}
        \vspace{-0.1in}
        \caption{Qualitative comparison between INR-GAN (left column), CIPS (middle column), and Poly-INR (right column) on FFHQ dataset at $256\times256$. }
        \label{fig:suppli_ffhq}
        %\vspace{-0.15in}
\end{figure*}

%%%%%%%%% REFERENCES
% {\small
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

% \end{document}
