\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{url}            % simple URL typesetting
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{paralist}
\usepackage{caption}
\usepackage{subcaption}
%\usepackage{subfigure}
\usepackage{etoolbox}
\usepackage{float}
\usepackage{tabularx}
%\usepackage{savetrees}
\usepackage{enumitem}
\setlist{leftmargin=.5mm}
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{5938} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{\vspace{-0.10in}Polynomial Implicit Neural Representations For Large Diverse Datasets\vspace{-0.10in}}  % **** Enter the paper title here

\maketitle

\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW

%comments of reviewer 9Nyt (R1):
%%%%%%%%%%%%%%%%%%%%%%Strength%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The paper is easy to follow.
% The use of polynomial functions for coordinate-based generative models is an interesting and novel approach, to the best of my knowledge.
% Their evaluation on standard benchmark datasets, both large-scale and small-scale, demonstrates that their approach achieves significantly fewer trainable parameters compared to existing convolution-based and coordinate-based generative models. This is useful in applications where the amount of computational power and memory is constrained. Additionally, they demonstrate the effectiveness of the proposed framework for other tasks such as interpolation, style-mixing and extrapolation.
%%%%%%%%%%%%%%%%Weekness%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Since the Poly-INR has significantly fewer trainable parameters compared to existing coordinate-based generative models, such as CIPS and INR-GAN, on the FFHQ dataset, it would be beneficial for the authors to include an evaluation of computational efficiency in their study. This would provide a more comprehensive understanding of how the proposed approach compares to existing methods, not only in terms of performance but also in terms of computational resources required. I expect to see that the Poly-INR model can be trained at a faster speed as a result of its (significantly) reduced number of parameters.
% Apart from Sine-based MLP (described on L114-124), non-traditional activations such as Gaussian activation [R1,R2] have been proposed for achieving high-fidelity implicit neural representations without the need for positional embeddings. It would be beneficial for the authors to also include of discussion of these other non-traditional activations in the introduction and related work section of the paper.
% It is not clear from the experiments on lines 430-462 why a different number of images is required for training the Poly-INR model at different resolutions. The authors should provide more information to explain this discrepancy.
% References
% [R1] Ramasinghe, Sameera, and Simon Lucey. "Beyond periodicity: towards a unifying framework for activations in coordinate-MLPs." European Conference on Computer Vision. Springer, Cham, 2022.
% [R2] Chng, Shin-Fang, et al. "Gaussian activated neural radiance fields for high fidelity reconstruction and pose estimation." European Conference on Computer Vision. Springer, Cham, 2022.


%comments of reviewer zLkW (R2):
%%%%%%%%%%%%%%%%%%%%%%Strength%%%%%%%%%%%%%%%%%%%%%%%%%%%
% to my knowledge, this is the first INR-based method that demonstrates comparable performance to CNN-based methods on large-scale datasets such as ImageNet
% The proposed method is parameter efficient and achieves comparable results to existing traditional CNN-based models without any convolution layers and with significantly fewer parameters in terms of image quality.
% The experiments are extensive and show the effectiveness of the proposed model for diverse tasks like interpolation, extrapolation, style-mixing, etc.
%%%%%%%%%%%%%%%%Weekness%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Since the proposed model synthesizes each pixel independently, the authors stated there is a concern regarding the inference speed/time. Could authors provide analysis on the computational efficiency of their model (e.g inference speed, FLOPs) and compare it to both CNN- and INR- based counterparts?
% There is no comparison to existing INR-based methods in terms of sampling at high resolution. Authors show that their INR-based model upsampling ability (trained on low resolution) outperforms traditional upsampling techniques (e.g. Bilinear, Bicubic) in terms of FID score. However, I cannot see any comparison to other INR-based methods. For example, INR-GAN [1] showed similar out-of-the-box superresolution properties. The authors stated that existing methods are limited to only small datasets. But, the comparison can be done on smaller datasets such as FFHQ for existing INR-based methods (at least INR-GAN)? Does the proposed method do well?
% [1]https://openaccess.thecvf.com/content/CVPR2021/papers/Skorokhodov_Adversarial_Generation_of_Continuous_Images_CVPR_2021_paper.pdf
%%%%%%%%%%%%%%%%%%%%%additional%%%%%%%%%%%%%%%%
% Figure 2 is a little confusing. At first glance, it gives the impression that Latent vector z + Class c suggests that they are combined together before going to the class embedding layer as an input. However, in the manuscript, it is stated class label is embedded by a pre-trained class embedding layer, then concatenated with the latent code z before feeding to the mapping network.
% Synthesis network consists of 10 layers of Linear + LeakyReLU. Why the number of layers is set to 10? Any insights? It would be good to provide some ablations on how the number of layers influences the performance of the method.

%comments of reviewer 2kxX(R3):
%%%%%%%%%%%%%%%%%%%%%%Strength%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The proposed Poly-INR performas comparable performance to CNN-based GAN model while enjoys a much smaller network architecture.
% The paper is well-structured. I think the related work section is very well-written in particular.
% The paper includes sufficient amount of code and instruction to reproduce experiments in the article.
% I appreciate the simplicity in this work, given that it does not rely on many training tricks such as weight modulation or normalizing. I have some further questions in the clarification section. Please refer to their for some further discussions.
% The model is very competitve on tasks like extrapolation and interpolation
%%%%%%%%%%%%%%%%Weekness%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Some may argue that this paper lacks comparison with some of the SOTA generative models such as the line of works around diffusion models. Personally, I think the comparisons and tables in this work already verify the authors main points. Of course, I think adding SOTA methods will make this piece more complete and even more convincing, but I think it is not a big problem.
%%%%%%%%%%%%%%%%%%%%%%%%%%Suggestions For Rebuttal%%%%%%%%%%%%%%
% As discusses in the paper, it seems that simplicity is one of the advantage of this work. At the same time, it is widely perceived that generative models such as GANs can be very difficult to train. I'm wondering if the proposed methods are easier to train (For example, more tolerance towards dataset, batchsize, or even random seeds?)
% As discussed in section 4.1 about the precision and recall issue. The authors conclude that lower precision and recall could be attributed to smaller network size. I'm wondering is their additional evidences to support this claim? Rather than the method itself being biased. I understand that it might be difficult to scale up due to it's a new method. So, under fair comparison, does GAN models such as BIG-GAN and Style-GAN also suffer from lower precision and recall. (I assume that ckpt might be available online so no additional experiment will be conducted. But of course, given short amount of rebuttal time, some supportive evidences are enough)




%%%%common points about strength to mention%%%%
%$ R1, R2 easy to follow and well written$
% R1 novelity
% R1,R2 simplicity in this work, not relying on lot of tricks to improve the performance
% R2, R3 first inr model perform comparable to CNN on large dataset like imagenet
% R1, R2, R3 parameter efficient
%R1, R2, R3 experiments on diverse and large datasets and different tasks

%%%questions to address%%%%%%%

%R1, R2 computational efficiency: Could authors provide analysis on the computational efficiency of their model (e.g inference speed, FLOPs) and compare it to both CNN- and INR- based counterparts?
%R1 why a different number of images is required for training the Poly-INR model at different resolutions?
%R1 discussion of non-traditional activations like periodic and gaussian functions in the introduction and related work section of the paper

% R2 comparison to existing INR-based methods in terms of sampling at high resolution on FFHD dataset
%R2:  extrapolation properties comparison
%

We would like to express our gratitude to the reviewers \textbf{[R1 (9Nyt), R2 (zLkW), R3 (2kxX)]} for their valuable feedback and suggestions. We are pleased that the reviewers find our approach novel and value our method's simplicity. Additionally, \textbf{[R2, R3]} noted that our proposed approach is the first INR model that performs similarly to CNN-based models on large datasets like ImageNet. Furthermore, \textbf{[R1, R2, R3]} find our method to be parameter efficient. They also find our paper easy to understand and well-written. We also appreciate that \textbf{[R1, R2, R3]} find our experiments to be extensive across multiple datasets and tasks. We appreciate  \textbf{R3's} score being between \textit{weak-accept and accept}. Below, we address the concerns raised by the reviewers:
%.
%\noindent \textbf{[\textcolor{brown}{R1}, \textcolor{blue}{R2}]}
\paragraph{\textcolor{blue}{Response to R1 (9Nyt):}}
\vspace{-0.13in}
\begin{itemize}
\setlength\itemsep{-0.3em}
\vspace{-0.07in}
\item \textbf{Comparison of computational efficiency:}
We evaluate all models trained on FFHQ-256x256 and report the inference speeds (in Sec-per-image) on a single Nvidia-RTX-6000 GPU as follows: \textit{StyleGAN-XL: 0.047s, INR-GAN: 0.024s, CIPS: 0.067s, Poly-INR: 0.054s}. StyleGAN-XL (CNN-based) and INR-GAN (no CNNs) employ a multi-scale architecture, where most computation occurs at low spatial resolution, making the models computationally very efficient. However, technically the multi-scale architecture of INR-GAN does not synthesize each pixel independently. The CIPS and our Poly-INR models synthesize each pixel independently and perform all computations at the same resolution. Our Poly-INR model offers faster inference speeds than CIPS due to its parameter efficiency. If accepted, we will provide inference speeds across resolutions in the final.
%\noindent \textbf{[\textcolor{brown}{R1}]}
\item \textbf{Why does the Poly-INR model require a different number of images for training at different resolutions?:} 
% Here, we would first like to clarify that `number of images' refer to the total number of training samples fed to the discriminator.
% It is a common notation generally used in GAN literature to indicate the training progress (equivalent to training iteration X $batch_size$). 
We train our model progressively with increasing resolution, i.e., we start by training at low resolution and continue training with higher resolutions as training progresses. Since the computational cost is less at low resolution, the model is trained for large number of iterations (i.e. large number of training images are seen by the discriminator), followed by training for high resolution. Since the model is already trained at low resolution, so fewer iterations are needed for convergence at high resolution, i.e., fewer images are seen by the discriminator. We realize that mentioning `number of images' (lines: 459-463) used for training is confusing. We will add this information in the paper as well for clarity.  

%\noindent \textbf{[\textcolor{brown}{R1}]}
\item \textbf{Include a discussion of other non-traditional activations in the paper's introduction and related work section:} \textit{Yes}, we will definitely include recommended citations and discuss non-traditional activation functions.
\end{itemize}
\paragraph{\textcolor{blue}{Response to R2 (zLkW):}}
\vspace{-0.13in}
\begin{itemize}
\setlength\itemsep{-0.3em}
\vspace{-0.07in}
% \noindent \textbf{[\textcolor{blue}{R2}]}
\item \textbf{Computational efficiency:} see response to R1 above.
\item \textbf{Comparison to existing INR-based methods in terms of sampling at high resolution:}
The FID scores (lower is better) at 1024x1024 for models trained on FFHQ-256x256 are as follows: \textit{Poly-INR:13.69, INR-GAN: 18.51, CIPS:29.59}. Our Poly-INR model provides better high-resolution sampling than the other two INR-based generators.
\begin{table}[t!]
\vspace{-0.02in}
\begin{small}
\caption{Poly-INR performance on FFHQ-32x32 across various levels (lvl) and model size (number of parameters in Million).}
\vspace{-0.12in}
\label{table:performance_levels}
\centering

\resizebox{0.45\textwidth}{!}{
\begin{tabular}{ccccccccc}
%\centering
\toprule
\multicolumn{1}{c}{} & \multicolumn{1}{c}{\textbf{Lvl-2}}& 
\multicolumn{1}{c}{\textbf{Lvl-4}}&
\multicolumn{1}{c}{\textbf{Lvl-7}}&
\multicolumn{1}{c}{\textbf{Lvl-10}}&
\multicolumn{1}{c}{\textbf{Lvl-14}}&
\multicolumn{1}{c}{\textbf{Lvl-14}}
\\
%#\textbf{Resolution} & \textbf{Neighbour}& & &\\
\midrule
Feat. Dim. &512 &512&512&512&512&1024\\
Params (M) & 2.98& 5.62& 9.57 &13.52&18.79&64.74\\
FID $\downarrow$& 27.01& 3.46 & 1.92 &1.83& 1.52& 1.12\\
Precision $\uparrow$&0.85&0.68&0.67&0.68& 0.68& 0.70\\
Recall$\uparrow$&0.01&0.41&0.56&0.57&0.59&0.63\\
%128x128 & 12.26 & 11.13& 12.02 &14.88\\
 \bottomrule
\end{tabular}}
\end{small}
\vspace{-0.20in}
\end{table}
%\noindent \textbf{[\textcolor{blue}{R2}]}
\item \textbf{Why the number of levels (layers) is set to 10? Ablation study on the number of levels:}
As requested, we present an ablation study in Table \ref{table:performance_levels} (above), demonstrating the Poly-INR performance on the FFHQ-32x32 dataset as levels increase. We observe that with increasing levels, the model's performance improves. 
We utilize $10$ levels in our experiments because of training stability and also achieve comparable performance compared to CNN-based models. In case of training with more than $10$ levels, we can incrementally increase the number of levels by first training the model on a lower number, such as $10$, and gradually add more levels as training progresses. %in terms of FID, precision and recall value.
%\noindent \textbf{[\textcolor{blue}{R2}]}
\item \textbf{Figure 2 is a little confusing:} Thanks for pointing that out. We will improve the clarity of the figure.
\end{itemize}

\paragraph{\textcolor{blue}{Response to R3 (2kxX):}}
\vspace{-0.13in}
\begin{itemize}
\setlength\itemsep{-0.3em}
\vspace{-0.07in}
%\noindent \textbf{[\textcolor{teal}{R3}]}
\item \textbf{Precision and recall score:}
We agree with the reviewer that the GAN models' precision (Pr) and recall (Rec) scores are generally less than the diffusion models. In Table 1 of the paper, we compare our Poly-INR against the CNN-based GAN models like BigGAN and StyleGAN-XL in terms of precision and recall. Our model has a slightly inferior recall value than StyleGAN-XL but better than BigGAN models across all resolutions. However, we can improve the precision-recall score of our Poly-INR model further by increasing the model capacity. In Table \ref{table:performance_levels} (above), we increase the model capacity either by adding more levels (layers) or increasing the feature dimension on FFHQ-32x32. We observe that when the model capacity is very small, the recall score is also very poor, but as we increase the model parameters, the recall score gets much better. 

%\noindent \textbf{[\textcolor{teal}{R3}]}
\item \textbf{Comparison with recent SOTA generative models around diffusion models:}
Recent diffusion models perform better than ADM-G (Dhariwal et al. 2021), as reported in Table 1 of the paper. However, our model's performance is comparable to these recent methods. We will report recent SOTA diffusion models in the paper, if accepted.
%\cite{peebles2022scalable,zheng2022entropy}

%\noindent\textbf{[\textcolor{teal}{R3}]}
\item \textbf{Is the proposed method easier to train?}
Our model is equivariant to image size, allowing for easy training with progressive resolution. Training with smaller images is faster and more stable, and the parameters can be used for higher-resolution training. We did not experience instability during our experiments.
% however, further extensive experiments regarding random seeds, batch size, etc., are needed to make a claim regarding stability. 
\end{itemize}










%%%%%%%%% REFERENCES
% {\scriptsize
% \bibliographystyle{ieee_fullname}
% \bibliography{egbib}
% }

\end{document}
