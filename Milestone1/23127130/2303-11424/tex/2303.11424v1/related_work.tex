%!TEX root = main.tex
\section{Related work}
%inr awesome, applications, Fourier relation 
\noindent\textbf{Implicit neural representations:}
% INRs are an effective and efficient way of representing a signal in the form of a continuous function with the help of neural networks. 
INRs have been widely adopted for $3$D scene representation and synthesis \cite{sitzmann2019scene, mescheder2019occupancy, mildenhall2021nerf}. Following the success of NeRF \cite{mildenhall2021nerf}, there has been a large volume of work on $3$D scene representation from $2$D images \cite{yariv2020multiview, yu2021pixelnerf, sitzmann2021light, martin2021nerf, pumarola2021d, jain2021putting, barron2022mip}. They have also been used for semantic segmentation \cite{fu2022panoptic}, video \cite{park2021nerfies, xian2021space, gao2021dynamic}, audio \cite{gao2021dynamic}, and time-series modeling \cite{fons2022hypertime}. INRs have also been used as a prior for inverse problems \cite{sitzmann2020implicit, reed2021dynamic}. However, most INR approaches either use a sinusoidal positional encoding \cite{mildenhall2021nerf, tancik2020fourier} or a sinusoidal activation function \cite{sitzmann2020implicit}, which limits the model capacity for large dataset representation. In our work, we represent our Poly-INR model as a polynomial function without using any positional encoding.

\begin{figure*}[ht!]
     \centering
     %\vspace{-0.25in}
         \includegraphics[width=0.95\textwidth]{cvpr2023-author_kit-v1_1-1/latex/figures/polyinr.png}
         %\vspace{-0.1in}
        \caption{Overview of our proposed Polynomial Implicit Neural Representation (Poly-INR) based generator architecture. Our model consists of two networks: $1)$ Mapping network, which generates the affine parameters from the latent code $z$, and $2)$ Synthesis network, which synthesizes the RGB value for the given pixel location. Our Poly-INR model is defined using only Linear and ReLU layers end-to-end.}
        \label{fig:arch}
        %\vspace{-0.2in}
\end{figure*}

% consist of a generator and a discriminator. The discriminator takes samples from the real distribution and the generated, and tries to distinguish which are real or fake. At the same time, the generator takes a low-dimension latent code $z$ and maps it to an image, and is trained to generate realistic images to fool the discriminator. 

% have been made by introducing new loss function \cite{arjovsky2017wasserstein}, regularization \cite{gulrajani2017improved}, normalization \cite{miyato2018spectral} and architecture \cite{radford2015unsupervised, karras2019style}.
%def of gan, style gan, applications
\noindent\textbf{GANs:} have been widely used for image generation and synthesis tasks \cite{goodfellow2020generative}. In recent work, several improvements have been proposed\cite{radford2015unsupervised, karras2019style,miyato2018spectral,arjovsky2017wasserstein,gulrajani2017improved} over the original architecture. For example, the popularly used StyleGAN \cite{karras2019style} model uses a mapping network to generate style codes which are then used to modulate the weights of the Conv layers. StyleGAN improves image fidelity, as well as enhances inversion \cite{tov2021designing} and image editing capabilities \cite{harkonen2020ganspace}. StyleGAN has been scaled to large datasets like ImageNet \cite{sauer2022stylegan}, using a discriminator which uses projected features from a pre-trained classifier \cite{sauer2021projected}. More recently, transformer-based models have also been used as generators \cite{zhao2021improved, lee2021vitgan}; however, the self-attention mechanism is computationally costly for achieving higher resolution. Unlike these methods, our generator is free of convolution, normalization, and self-attention mechanisms and only uses ReLU and Linear layers to achieve competitive results, but with far fewer parameters.

%inr gans, inr+conv, 3d nerf gan
\noindent\textbf{GANs + coordinates:}
INRs have also been implemented within generative models. For example,  CIPS \cite{anokhin2021image} uses Fourier features and learnable vectors for each spatial location as positional encoding and uses StyleGAN-like weight modulation for layers in the MLP. Similarly, INR-GAN \cite{skorokhodov2021adversarial} proposes a multi-scale generator model where a hyper-network determines the parameters of the MLP. INR-GAN has been further extended to generate an `infinite'-size continuous image using anchors \cite{skorokhodov2021aligning}. However, these INR-based models have only shown promising results on smaller datasets. Our work scales easily to large datasets like ImageNet owing to the significantly fewer parameters. 

Other approaches have combined CNN with coordinate-based features. For example, the Local Implicit Image Function (LIIF) \cite{chen2021learning} and Spherical Local Implicit Image Function (SLIIF) \cite{yoon2022spheresr} use a CNN-based backbone to generate feature vectors corresponding to each coordinate location. Arbitrary-scale image synthesis \cite{ntavelis2022arbitrary} uses a multi-scale convolution-based generator model with scale-aware position embedding to generate scale-consistent images. StyleGAN model, further extended by \cite{karras2021alias} (StyleGAN-3) to use coordinate location-based Fourier features. In addition, StyleGAN-3 uses filter kernels equivariant to the coordinate grid's translation and rotation. However, the rotation equivariant version of the StyleGAN-3 model fails to scale to ImageNet dataset, as reported in \cite{sauer2022stylegan}. Instead of using convolution layers, the Poly-INR only uses linear and ReLU layers. 



%Moment work


%relation to moment matching
\noindent\textbf{Relation to classical geometric moment:} Polynomial functions have been explored earlier in the form of geometric moments for image reconstruction \cite{hu1962visual,teague1980image, honarvar2014image, flusser2009moments}. Unlike the Fourier transform, which uses the sinusoidal functions as the basis, the geometric moment method projects the $2$D image on a polynomial basis of the form $x^py^q$ to compute the moment of order $p+q$. The moment matching method \cite{teague1980image} is generally used for image reconstruction from given finite moments. In moment matching, the image is assumed to be a polynomial function, and the coefficients of the polynomial are defined to match the given finite moments. Similar to geometric moments, we also represent images on a polynomial basis; however, our polynomial coefficients are learned end-to-end and defined by a deep neural network. 
% Nonetheless, expressing INR as a polynomial function has widespread applications, irrespective of the underlying nature of the basis function (sinusoidal, cosine, or exponential), each of which can be approximated as a polynomial function using Taylor series expansion. 

% It is known that these coefficients can be estimated by solving a system of linear equations \cite{teague1980image}. Our approach also assumes that the image is a polynomial function. However, we differ from the moment matching as our polynomial coefficients are non-linear functions of some latent code and these coefficients are learnt end-to-end. 
