%!TEX root = main.tex
\section{Method}
%position encoding and inr
%mathematical formulation
%our model overview
%relation to style
%relation to moments
%mathematical formulation
We are interested in a class of functions that represent an image in the form:
\begin{equation}
\label{eq:poly_form}
\begin{aligned}
G(x,y) =  g_{00}+g_{10}x+g_{01}y+...+g_{pq}x^{p}y^{q},
\end{aligned}
%\vspace{-0.1cm}
\end{equation}

where, $(x,y)$ is the normalized pixel location sampled from a coordinate grid of size $(H\times W)$, while the coefficients of the polynomial $(g_{pq})$ are parameterized by a latent vector $z$ sampled from a known distribution and are independent of the pixel location. Therefore, to form an image, we evaluate the generator $G$ for all pixel locations $(x,y)$ for a given fixed $z$:
\begin{equation}
\label{eq:image_from_G}
\begin{aligned}
I =  \{G(x,y;z)\,|\,(x,y)\in CoordinateGrid(H,W)\},
\end{aligned}
%\vspace{-0.1cm}
\end{equation}
where, $CoordinateGrid(H,W)=\{(\frac{x}{W-1},\frac{y}{H-1})\,|\, 0\leq x<W, 0\leq y<H \}$. By sampling different latent vectors $z$, we generate different polynomials and represent images over a distribution of real images.

%mlp low order and motivation for archi 
Our goal is to learn the polynomial defined by Eq. \ref{eq:poly_form} using only Linear and ReLU layers. However, the conventional definition of MLP usually takes the coordinate location as input, processed by a few Linear and ReLU layers. This definition of INR can only approximate low-order polynomials and hence only generates low-frequency information. Although, one can use a positional embedding consisting of polynomials of the form $x^py^q$ to approximate a higher-order polynomial. However, this definition of INR is limiting since a fixed-size embedding space can contain only a small combination of polynomial orders. Furthermore, we do not know which polynomial order is essential to generate the image beforehand. Hence, we progressively increase the polynomial order in the network and let it learn the required orders. We implement this by using element-wise multiplication with the affine-transformed coordinate location at different levels, shown in Fig \ref{fig:arch}. Our model consists of two parts: 1) \textbf{Mapping network}, which takes the latent code $z$ and maps it to affine parameters space $\mathbf{W}$, and 2) \textbf{Synthesis network}, which takes the pixel location and generates the corresponding RGB value.


%mapping network, affine paramters
\noindent\textbf{Mapping Network:}
The mapping network takes the latent code $z \in \mathbb{R}^{64}$ and maps it to the space $\mathbf{W} \in \mathbb{R}^{512}$. Our model adopts the mapping network used in \cite{sauer2022stylegan}. It consists of a pre-trained class embedding, which embeds the one hot class label into a $512$ dimension vector and concatenates it with the latent code $z$. Then the mapping network consists of an MLP with two layers, which maps it to the space $\mathbf{W}$. We use this $\mathbf{W}$ to generate affine parameters by using additional linear layers; hence we call $\mathbf{W}$ as affine parameters space.

%levels , 
\noindent\textbf{Synthesis network:}
The synthesis network generates the RGB $(\mathbb{R}^{3})$ value for the given pixel location $(x,y)$. As shown in Fig. \ref{fig:arch}, the synthesis network consists of multiple levels; at each level, it receives the affine transformation parameters from the mapping network and the pixel coordinate location. At \textit{level-$0$}, we affine transform the coordinate grid and feed it to a Linear layer followed by a Leaky-ReLU layer with $negative\_slope=0.2$. At later levels, we do element-wise multiplication between the feature from the previous level and the affine-transformed coordinate grid, and then feed it to Linear and Leaky-ReLU layers. With the element-wise multiplication at each level, the network has the flexibility to increase the order for $x$ or $y$ coordinate position, or not to increase the order by keeping the affine transformation coefficient $a_j=b_j=0$. In our model, we use $10$ levels, which is sufficient to generate large datasets like ImageNet. Mathematically, the synthesis network can be expressed as follows:
% \begin{equation}
\label{eq:synth}
\begin{align}
G_{syn} = \hdots\sigma(W_2((A_2X)\odot \sigma(W_1((A_1X)\odot\\\nonumber \sigma(W_0(A_0X)))))),
\end{align}
%\vspace{-0.1cm}
% \end{equation}
where $X\in \mathbb{R}^{3\times HW}$ is the coordinate grid of size $H\times W$ with an additional dimension for the bias, $A_i \in \mathbb{R}^{n\times 3}$ is the affine transformation matrix from the mapping network for \textit{level-i}, $W_i \in \mathbb{R}^{n\times n}$ is the weight of the linear layer at \textit{level-i}, $\sigma$ is the Leaky-ReLU layer and $\odot$ is element-wise multiplication. Here $n$ is the dimension of the feature channel in the synthesis network, which is the same for all levels. For large datasets like ImageNet, we choose the channel dimension $n=1024$, and for smaller datasets like FFHQ, we choose $n=512$. Note that with this definition, our model only uses Linear and ReLU layers end-to-end and synthesizes each pixel independently.


\begin{table*}[ht!]
%\vspace{-0.1in}
\caption{Quantitative comparison of Poly-INR method with CNN-based generative models on ImageNet datasets. (d) compares the number of parameters used in all models at various resolutions. The results for existing methods are quoted from the StyleGAN-XL paper.}
\label{table:quantitative}
%\vspace{-0.1in}
\parbox{.5\textwidth}{
\centering
\label{table:imagenet128}
\subcaption{ImageNet $128\times 128$} 
\begin{small}
\resizebox{0.49\textwidth}{!}{\begin{tabular}{ccccccc}
%\centering
\toprule
\multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{FID $\downarrow$}} & \multicolumn{1}{c}{\textbf{sFID $\downarrow$} }& \multicolumn{1}{c}{\textbf{rFID $\downarrow$}} & \multicolumn{1}{c}{\textbf{IS $\uparrow$}} & \multicolumn{1}{c}{\textbf{Pr $\uparrow$}} & \multicolumn{1}{c}{\textbf{Rec $\uparrow$}}\\

\midrule
BigGAN &  6.02 &7.18 & 6.09 & 145.83 & 0.86 & 0.35\\
CDM &  3.52 &-&-& 128.80&-&-\\
ADM &  5.91 &5.09 &13.29 &93.31 &0.70 &0.65\\
ADM-G  & 2.97 &5.09& 3.80& 141.37& 0.78& 0.59\\
StyleGAN-XL  &1.81 &3.82 &1.82 &200.55& 0.77 &0.55\\
\hline
\textbf{Poly-INR} & 2.08 & 3.93 & 2.76 & 179.64 & 0.70 & 0.45\\
 \bottomrule

\end{tabular}
}
\end{small}
}
\hfill
\parbox{.5\textwidth}{
\centering
\label{table:imagenet256}
\subcaption{ImageNet $256\times 256$} 
\begin{small}
\resizebox{0.49\textwidth}{!}{\begin{tabular}{ccccccc}
%\centering
\toprule
\multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{FID $\downarrow$}} & \multicolumn{1}{c}{\textbf{sFID $\downarrow$} }& \multicolumn{1}{c}{\textbf{rFID $\downarrow$}} & \multicolumn{1}{c}{\textbf{IS $\uparrow$}} & \multicolumn{1}{c}{\textbf{Pr $\uparrow$}} & \multicolumn{1}{c}{\textbf{Rec $\uparrow$}}\\

\midrule
BigGAN &  6.95 &7.36 &75.24 &202.65 &0.87 &0.28\\
%CDM &   4.88 & - & -&158.70&-&-\\
ADM &  10.94& 6.02& 125.78& 100.98& 0.69& 0.63\\
ADM-G  & 3.94& 6.14 &11.86 &215.84 &0.83 &0.53\\
DiT-XL/2-G& 2.27 & 4.60& -&278.54& 0.83 & 0.57\\
StyleGAN-XL  &2.30 &4.02 &7.06 &265.12 &0.78& 0.53\\
\hline
\textbf{Poly-INR}  & 2.86 & 4.37 & 7.79 & 241.43& 0.71& 0.39\\
 \bottomrule
\end{tabular}}
\end{small}}
\vspace{0.1in}
\vfill
\parbox{.5\textwidth}{
\centering
\label{table:imagenet512}
\subcaption{ImageNet $512\times 512$} 
\begin{small}
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{ccccccc}
%\centering
\toprule
\multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{FID $\downarrow$}} & \multicolumn{1}{c}{\textbf{sFID $\downarrow$} }& \multicolumn{1}{c}{\textbf{rFID $\downarrow$}} & \multicolumn{1}{c}{\textbf{IS $\uparrow$}} & \multicolumn{1}{c}{\textbf{Pr $\uparrow$}} & \multicolumn{1}{c}{\textbf{Rec $\uparrow$}}\\
%158.3, 559, 168.4
\midrule
BigGAN &  8.43 &8.13 &312.00 &177.90 &0.88 &0.29\\
ADM & 23.24& 10.19& 561.32& 58.06& 0.73& 0.60\\
ADM-G &  3.85 &5.86 &210.83 &221.72 &0.84 &0.53\\
DiT-XL/2-G&3.04 & 5.04& -&240.82& 0.84 & 0.54\\
StyleGAN-XL & 2.41 &4.06 &51.54& 267.75& 0.77& 0.52\\
\hline
\textbf{Poly-INR} &  3.81 & 5.06 & 54.31 &267.44 & 0.70& 0.34\\
 \bottomrule
\end{tabular}}
\end{small}}
\hfill
\parbox{.5\textwidth}{
\centering
\label{table:params}
\subcaption{Number of parameters in millions (M)} 
\begin{small}
\begin{tabular}{ccccc}
%\centering
\toprule
\multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\boldmath{$64^2$}}& \multicolumn{1}{c}{\boldmath{$128^2$}}&  \multicolumn{1}{c}{\boldmath{$256^2$}} & \multicolumn{1}{c}{\boldmath{$512^2$}}\\

\midrule
BigGAN & - &141.0& 164.3& 164.7\\
ADM  & 296.0& 422.0& 554.0&559.0\\
DiT-XL  & - & - & 675.0&675.0\\
StyleGAN-XL &134.4 & 158.7& 166.3&168.4\\
\hline
\textbf{Poly-INR} &46.0 &46.0& 46.0 &46.0\\
 \bottomrule
\end{tabular}

\end{small}}
%\vspace{-0.2in}
\end{table*}

\begin{table}[ht!]
%\vspace{-0.1in}
\caption{Quantitative comparison of Poly-INR method with CNN and INR-based generative models on FFHQ dataset at $256\times256$.}
\label{table:ffhq}
%\vspace{-0.1in}
\centering
\begin{small}
\begin{tabular}{cccc}
%\centering
\toprule
\multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{\textbf{params (M)}}&  \multicolumn{1}{c}{\textbf{FID $\downarrow$}} &  \multicolumn{1}{c}{\textbf{Inference Time}}\\
& & &\textbf{(sec/img)}\\
\midrule
StyleGAN2 & 30.0 & 3.83 & 0.016\\
StyleGAN-XL  & 67.9 &2.19& 0.047\\
CIPS & 45.9 &4.38& 0.067\\
INR-GAN & 72.4& 4.95& 0.024\\
\hline
\textbf{Poly-INR} & 13.6 & 2.72& 0.054\\
 \bottomrule
\end{tabular}
\end{small}
%\vspace{-0.2in}
\end{table}

\noindent\textbf{Relation to StyleGAN:} StyleGANs \cite{karras2019style,karras2020analyzing, karras2021alias} can be seen as a special case of our formulation. By keeping the coefficients ($a_j$,$b_j$) in the affine transformation matrix of $x$ and $y$ coordinate location equal to zero, the bias term $c_j$ would act as a style code. However, our affine transformation adds location bias to the style code, rather than just using the same style code for all locations in StyleGAN models. This location bias makes the model very flexible in applying a style code only to a specific image region, making it more expressive. In addition, our model differs from the StyleGANs in many aspects. First, our method does not use weight modulation/demodulation or normalizing \cite{karras2020analyzing} tricks. Second, our model does not employ low-pass filters or convolutional layers. Finally, we do not inject any spatial noise into our synthesis network. We can also use these tricks to improve the model's performance further. However, our model's definition is straightforward compared to other GAN models.

