\section{Introduction}
Autonomous driving technology has emerged as a promising solution for enhancing road safety, reducing traffic congestion, and improving the overall driving experience. As the deployment of autonomous vehicles becomes more widespread, understanding the interactions between humans and these vehicles becomes increasingly critical \cite{Dey2018}. 

Previous research has focused on employing diverse strategies to effectively communicate the current state or intention of autonomous vehicles to vulnerable road users (VRUs), such as pedestrians or cyclists \cite{Mirnig2018}. Relying on the information provided can help users make better decisions, for example, when crossing the road. The design of external Human Machine Interfaces (eHMIs) that enable communication can include external displays, light or sound signals \cite{Ranasinghe2020}.

Pedestrians evaluate multiple factors to determine the safety of road crossing, such as vehicle speeds and movements, safety gap sizes or location familiarity. While implicit interaction, such as perceived speed or gap size, may be the primary basis for crossing decisions, as suggested by some studies \cite{Clamann2017, Zimmermann2017}, some experiments indicate that explicit communication may also play a role, depending on the distance between the pedestrian and the vehicle \cite{Dey2019}.

Conversely, the effectiveness of virtual reality (VR) in conducting human factors research within the realm of interface design has been widely acknowledged, due to issues of safety and control of the experimental setting for each trial \cite{Nascimento2019}. A pedestrian VR simulator can generate the possibility of participants being hit by the vehicle and create the real sense of fear in them \cite{Deb2018}. The ability of the simulator to make pedestrians exhibit very realistic behavior (i.e., to minimize the \emph{behavioral gap} with respect to a real situation) justifies the use of VR in interaction research. In addition, VR platform provides an efficient method for measuring pedestrian travel behavior via headset tracking data and synchronized video recordings \cite{Sween2016}.

\begin{figure}
\centerline{\includegraphics[width=0.92\columnwidth]{figures/introduccion_virtual_real.png}}
\caption{Digital twin for human-vehicle interaction in autonomous driving. Virtual (above) and real (bottom) setting.}
\label{fig:schematic1}
\end{figure}

In our previous work \cite{CarlaCHIRA}, we presented a framework to enable real-time interaction between pedestrians and CARLA simulator using immersive virtual reality and human motion capture systems. In the following, we present the application of this framework to develop a digital twin of a real scenario, and replicate the field experiments carried out in real-world driving conditions \cite{Izquierdo2023} (see Fig. \ref{fig:schematic1}). The experiments are focused in studying human-vehicle interaction in crosswalks through the use of external HMIs and implicit communication based on the motion of the vehicle. We evaluate the effectiveness of different forms of communication between the autonomous vehicle and real pedestrians immersed in a virtual crosswalk scenario, which will then be used in a future work to establish a comparison with the same study at a real twin crosswalk and provide a reliable measure of the \emph{behavioral gap}. 


\section{Related work}
VR environments are frequently used to investigate pedestrian crossing actions, providing a safer alternative to real-world experiments while also enabling greater flexibility and replicability in constructing experimental scenarios \cite{Nascimento2019}. We summarize the works that have employed VR to simulate an immersive virtual pedestrian crossing experience.

Most of the approaches focus on studying the communication between autonomous vehicles and pedestrians based on the design of different eHMI systems, such as eyes added to the car that look at the pedestrian to indicate their intention to stop \cite{Chang2017}, or a human-like visual embodiment in the driver's seat \cite{Furuya2021}. \cite{Shuchisnigdha2020-2} shows greater acceptance of written messages (e.g., "walk") than conveyed by images such as a walking silhouette or a raised hand. Malfunctioning external displays of the car lead pedestrians to ignore it and aggravate the effects of overtrust in the autonomous vehicle, so the design of external communication must avoid misleading information \cite{Hollander2019}, \cite{Shuchisnigdha2020}. Certain studies explore ways in which pedestrians can communicate their intentions to autonomous vehicles (e.g. through gestures \cite{Gruenefeld2019}) while the vehicle displays its locomotion and reacts to the pedestrian's language. A novel application of eHMI incorporates vehicle directional information in scenarios where a pedestrian is in close proximity to a collision and increases pedestrians' self-reported understanding of the car's intent \cite{Bazilinskyy2022}.

To our knowledge, all previous work has used VR scenarios developed with Unity, and has not operated in a simulator specifically dedicated to autonomous driving. In this paper, the chosen proposal enables a VR environment in the open-source CARLA simulator with an interactive pedestrian interface \cite{CarlaCHIRA}. In addition, experiments are conducted on the digital twin of a pre-existing crosswalk with a virtual replica of an autonomous vehicle, exploiting all the advantages of working with digital twins and virtual reality \cite{Hoffmann2022, Jagannath2022}. Another remarkable aspect is that body movements are not only recorded on video, but also registered with a motion capture system to collect accurate data on pedestrian responses to the variables under study. 