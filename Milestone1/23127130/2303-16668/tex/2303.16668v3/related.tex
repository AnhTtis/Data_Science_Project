\section{Related Work}
\label{sec:related}
%\dimitri{Feel free to fix the Related Work Section according to your needs} 
%Below, we discuss the most relevant contributions to this work, which fall into two categories: {\em(i) Defense against Byzantine model poisoning attacks to FL} and {\em(ii) Multidimensional time-series analysis}.

%\smallskip
%\noindent{{\bf Defense against Byzantine attacks on FL}}
%\newline
%\subsection{Defenses against Model Poisoning Attacks}
% OLD, LONG VERSION
% Defensive (or mitigating) strategies for Byzantine model poisoning attacks on FL systems range from bland aggregation rules based on descriptive statistics~\cite{yin2018icml,pillutla2019robust}, to more complex solutions based on outlier detection techniques~\cite{blanchard2017nips,mhamdi2018pmlr, shen2016acm,fung2018arxiv}.
%Without the ambition of being exhaustive, 
%In the following, we describe the most popular Byzantine-resilient aggregation methods, which will also be used as the baselines for our comparison.
Below, we describe the most popular defenses against model poisoning attacks on FL systems, which will serve as the baselines for our comparison.
A more comprehensive discussion is in~\cite{hu2021arxiv,barroso2023if}.
% We suggest referring to~\cite{hu2021arxiv} for an exhaustive overview of robust FL aggregation methods and to~\cite{barroso2023if} for a general survey on FL security threats.
%
%\noindent{{\bf {\em Trimmed Mean}}~\cite{yin2018icml}{\bf.}}
%%% TODO: Modify this description to comply with the real definition of beta given in the experimental sec
%This rule aggregates each model parameter independently. Specifically, for each $j$-th model parameter, the central server sorts the $j$-th parameters of the $m$ local models received at the generic FL round ($1\leq m \leq K$), removes the largest and smallest $\beta$ of them, and computes the average of the remaining $m-\beta$ parameters as the $j$-th parameter of the global model.
%Suppose at most $b$ clients are compromised. This aggregation rule achieves order-optimal error rate when $b \leq \beta < m/2$ and the objective function to be minimized is strongly convex.

\noindent{{\bf {\em FedMedian}}~\cite{yin2018icml}{\bf.}}
The central server sorts the $j$-th parameters received from all the $m$ local models and takes the median of those as the value of the $j$-th parameter of the global model. This process is applied for all the model parameters, i.e., $\forall j\in \{1,\ldots,d\}$.
%Note that when $m$ is an even number, the median is simply the average of the middle two parameters. 
%Like Trimmed Mean, this aggregation rule also achieves an order-optimal error rate when the objective function is strongly convex.
\\
\noindent{{\bf {\em Trimmed Mean}}~\cite{xie2018corr}{\bf.}}
%Trimmed Mean is a family of aggregation rules that contains multiple variants, but the general idea remains the same.
This rule computes a model as FedMedian does, and then it averages the $k$ nearest parameters to the median.
If $b$ clients are compromised at most, this aggregation rule achieves an order-optimal error rate when $b \leq \frac{m}{2} - 1$.
\\
\noindent{{\bf {\em Krum}}~\cite{blanchard2017nips}{\bf.}}
It selects one of the $m$ local models received from the clients, which is most similar to \emph{all} other models, as the global model. The rationale behind this approach is that even if the chosen local model is poisoned, its influence would be restricted because it resembles other local models, potentially from benign clients.
%Again, suppose at most $b$ clients are compromised. 
%The server computes the sum of the Euclidean distances between each local model and its closest $m-b-2$ local models. 
%Hence, Krum selects the local model with the smallest sum of distance as the global model. 
%When $b < (m-2)/2$, Krum has theoretical guarantees for the convergence for certain objective functions.
A variant called Multi-Krum mixes Krum with standard FedAvg. 
%Notice that this method needs to know or estimate the number of corrupted clients $b$ in advance.
%
%con multikrum
%\noindent{{\bf {\em Krum/Multi-Krum}}~\cite{blanchard2017nips}{\bf.}}
%Krum selects one of the $m$ local models received from the clients that is most similar to {\em all} other models as the global model.
%The intuition is that, even if the selected local model is from a malicious client, its impact would be limited since it is similar to other local models, possibly from benign clients. 
%Again, suppose at most $b$ clients are compromised. 
%The server computes the sum of the Euclidean distances between each local model and its closest $m-b-2$ local models. 
%Hence, Krum selects the local model with the smallest sum of distance as the global model. 
%When $b < (m-2)/2$, Krum has theoretical guarantees for the convergence for certain objective functions.
%A variant of Krum, called Multi-Krum, interpolates between Krum and averaging, thereby mixing the robustness of Krum with the convergence speed of FedAvg. 
%Notice that this method needs to know or estimate the number of corrupted clients $b$ in advance.
\\
\noindent{{\bf {\em Bulyan}}~\cite{mhamdi2018pmlr}{\bf.}}
%\gabri{@Edoardo: I have added this robust aggregation rule, just in case it is already implemented in Flower (this is a low-priority task to do!)}
Since a single component can largely impact the Euclidean distance between two high-dimensional vectors, Krum may lose effectiveness in complex, high-dimensional parameter spaces due to the influence of a few abnormal local model weights. Bulyan iteratively applies Krum to select $\alpha$ local models and aggregates them with a Trimmed Mean variant to mitigate this issue.
\\
\noindent{{\bf {\em DnC}}~\cite{shejwalkar2021ndss}{\bf.}}
This robust aggregation %is inspired by existing defenses against data poisoning attacks on centralized machine learning, which 
uses spectral analysis to detect and filter outliers as proposed by~\cite{diakonikolas2017icml}. 
Similarly, DnC computes the principal component of the set of local updates sent by clients. Then, it projects each local update onto this principal component. Finally, it removes a constant fraction of the submitted model updates with the largest projections.
\\
\noindent{{\bf {\em FLDetector}}~\cite{zhang2022fldetector}{\bf.}}
%This method filters out malicious clients by using the history of global models to predict the next one. 
This method is the closest to our approach. It filters out malicious clients by measuring their consistency across $N$ rounds using an approximation of the integrated Hessian to compute the \textit{suspicious scores} for all clients. However, unlike our method, FLDetector struggles with large numbers of malicious clients, and cannot work with highly heterogeneous data.

%\gabri{We should add new baselines here! DeFL, FLDetector, etc.}

% REMOVED FOR UAI
%\\
%\noindent{{\bf {\em FLTrust}} (\cite{cao2020fltrust}){\bf.}}
%Different from outlier detection heuristics, 
%This approach assumes the server acts itself as a client, i.e., it collects a {\em trustworthy} small dataset (called {\em root dataset}) on which it trains its own local model (called {\em server model}) for comparison against all the local models from other clients.

%\smallskip
%\gabri{This one below must be fixed}
%\noindent{{\bf Multidimensional time-series analysis}}
%\newline
% REMOVED FOR UAI 2024
% \subsection{Multidimensional Time Series Analysis}
% Multidimensional time series analysis aims to identify patterns in datasets with more than one time-dependent variable.
% Two primary goals of it are {\em forecasting}~\cite{mahalakshmi2016icctide,basu2015annalsstats} and {\em anomaly detection}~\cite{chandola2009acm,chalapathy2019arxiv,schmidl2022pvld}.
% %The former tries to predict the future values of a time series from past observations~\cite{mahalakshmi2016icctide,basu2015annalsstats}; the latter, instead, aims to detect observations that do not conform to the ``expected'' behavior, i.e., anomalies~\cite{chandola2009acm,chalapathy2019arxiv,schmidl2022pvld}.
% %For both goals, many methods have been investigated, spanning from classic statistical learning approaches~\cite{li2017multivariate,rousseeuw2018anomaly} to standard ML techniques up to the most recent deep learning solutions~\cite{liu2018time,kieu2018mdm,zhao2020multivariate,lim2021time}.
% %Below, we review one of the latest outlier detection methods for multivariate time series proposed in the literature based on deep learning.
% \\
% \noindent{{\bf {\em MSCRED}}~\cite{zhang2019aaai}{\bf.}}
% % In addition to the well-known heuristics above, we consider another data-driven approach for computing the anomaly score from the time series of observed local model parameters received from FL clients. 
% This method performs anomaly detection in multivariate time series using a Multi-Scale Convolutional Recurrent Encoder-Decoder (MSCRED) network. In detail, MSCRED builds multi-scale signature matrices to characterize multiple temporal dependencies. %levels of the system statuses in different time steps.
% Then, it uses a convolutional encoder to represent the inter-time series correlations and trains an attention-based convolutional LSTM network to capture temporal patterns. Anomalies correspond to residual errors computed as the difference between the input and reconstructed signature matrices.

% Can be reintroduced
%MSCRED or related approaches~\cite{kieu2018mdm} are conceived for vector-based time series. 
%To the best of our knowledge, no outlier detection method proposed in the literature is designed to work with matrix-based time series data. This capability, instead, would be required in the case of FL (i.e., to handle the sequence of multiple high-dimensional local model parameters observed over several FL rounds).
%However, we may observe that any time series forecasting model can also be used to define anomaly scores, which serve as the basis for outlier detection.
%Therefore, we leverage an existing matrix autoregressive time series forecasting model proposed by Chen et al.~(\citeyear{chen2021je}) to detect anomalous matrix observations.
%The rationale of the proposed solution is detailed in the following sections.


% Besides, time series forecasting models can also be used to define anomaly scores, which serve as the basis for outlier detection. 

% As detailed in Section~\ref{sec:problem}, we frame our problem as an anomaly detection task on matrix-based time series data (i.e., the sequence of high-dimensional local model parameters observed over several FL rounds). 
% To the best of our knowledge, though, existing outlier detection methods are designed to work only on vector-based time series data.
% On the other hand, 

% To the best of our knowledge, in literature there is no study that applies a matrix-based approach to anomaly detection in FL scenarios. For the case, we suggest the autoregressive model introduced in \cite{chen2021je} to fill the gap. The rationale of the proposed solution is detailed in the following sections.



%%%%%%%% OLD %%%%%%%%
% Defensive (or mitigating) strategies for Byzantine model poisoning attacks range from bland aggregation rules based on descriptive statistics like trimmed mean~\cite{yin2018icml}, FedAvg~\cite{mcmahan2016arxiv,mcmahan2017aistats} and median~\cite{pillutla2019robust}, to more robust solutions that consider distance measures for outlier detection~\cite{blanchard2017nips, mhamdi2018pmlr, shen2016acm,fung2018arxiv}. Among the most reputable, Krum~\cite{blanchard2017nips} is an outlier detection strategy that leverages the Euclidean distance to select the closest gradient among the local ones and mitigate, in this way, the effect of corrupted clients. The advantage of Krum with respect to descriptive statistical approaches stands on its Byzantine-resilience skill. Briefly, Krum combines a majority-based approach, which considers just the subset of local updates with the smallest diameter, and the euclidean distance as square root-based method to choose in the subset of vectors the closest one to its $n-f$ neighbors. However, the narrowness of Krum stands on the adopted distance measure, which can be easily affected by some abnormal parameter. A variant of Krum, Bulyan ~\cite{mhamdi2018pmlr}, to overcome the issue considers a trimmed mean like strategy to select (some) local gradients to be aggregated for updating the global model. The Krum-variant is effective in terms of reduction of leeway of malicious workers and bounded to $O({1}/{\sqrt{d}})$, where $d$ is the data dimensionality. Yet, being Krum-dependent, Bulyan suffers the same scalabilty problems as its predecessor. Other strategies consider different measures to mitigate the effect of malicious clients. For instance, Shen et al.~\cite{shen2016acm} leverage the K-means algorithm to cluster local updates and detect outliers. Fung et al.~\cite{fung2018arxiv} take advantage of the cosine similarity to discern the good local gradients from the bad ones in non-i.i.d. to mitigate the effect of sybil-based poisoning attacks. Cao et al., instead, propose FLTrust~\cite{cao2020fltrust}, an approach in which the service provider collects a small root dataset without anomalies for training operations and the server acts like the clients, maintaining the model for the root dataset, except then streamlining the global model in function of local model updates of both server and clients. Panda et al., in the end, propose SparseFed~\cite{panda2022pmlr}, an approach that considers global top-k sparsification and local gradient clipping to reduce the effects of model poisoning attacks. In a few words, SparseFed computes and clips the gradients locally, and aggregates updates in the cloud; then extracts the top-k values and broadcasts them as updates to local devices for the next rounds. However, as the models become more complex, the effectiveness of the above defenses comes down. The challenge here is to design a more robust aggregation mechanism to mitigate the effect of poisoning attacks.

% \noindent{{\bf {\em Multidimensional time-series analysis}}}
% \newline
% Multidimensional time series analysis aims to identify, in datasets that have more than a time-dependent variable, patters that do not conform to a normal behavior, i.e., anomalies~\cite{chandola2009acm,chalapathy2019arxiv}. To date, many methods have been investigated for different research areas. The main families span from classic ML techniques to Deep Learning~\cite{liu2018time,zhao2020multivariate}, from stochastic learning to statistical regression~\cite{li2017multivariate,rousseeuw2018anomaly}, and so forth~\cite{schmidl2022pvld}.
% [...]
% Forecasting models are mainly used for the definition of anomaly scores [...]
%Of particular interest is the problem of Anomaly Detection in Multivariate Time Series (MVTS) based on vectors [...]

% To the best of our knowledge, in literature there is no study that applies a matrix-based approach to anomaly detection in FL scenarios. For the case, we suggest the autoregressive model introduced in \cite{chen2021je} to fill the gap. The rationale of the proposed solution is detailed in the following sections.
%In FL, the identification of multiple malicious clients through the recognition of anomalous data can be formulated as an anomaly detection problem~\cite{suyi2019corr}.
