\section{Background and Preliminaries}
\label{sec:background}
\newcommand\addtag{\refstepcounter{equation}\tag{\theequation}}
\subsection{Federated Learning}
\label{subsec:fl}
We consider a typical supervised learning task under a standard FL setting, which consists of a central server $S$ and a set of distributed clients $\mathcal{C}$, such that $|\mathcal{C}|=K$.
Each client $c\in \mathcal{C}$ has its own private training set $\dataset_c$, namely the set of its $n_c$ local labeled examples, i.e., $\dataset_c = \{\bm{x}_{c,i}, y_{c,i}\}_{i=1}^{n_c}$.
\\
The goal of FL is to train a global predictive model whose architecture and parameters $\params^*\in \R^d$ are shared across all clients by solving 
%\begin{equation}
$\params^* = \text{argmin}_{\params} \Loss(\params) = \text{argmin}_{\params} \sum_{c=1}^K p_c \Loss_c(\params;\dataset_c),$
%\label{eq:erm}
%\end{equation}
where $\Loss_c$ is the local objective function for client $c$. Usually, this is defined as the empirical risk calculated over the training set $\dataset_c$ sampled from the client's local data distribution:
%\begin{equation}
$\Loss_c(\params;\dataset_c) = \frac{1}{n_c}\sum_{i=1}^{n_c} \loss(\params;(\insta_{c,i}, y_{c,i})),$
%\label{eq:client-loss}
%\end{equation}
where $\loss$ is an instance-level loss, e.g., cross-entropy (classification) or squared error (regression). 
Each $p_c \geq 0$ specifies the relative contribution of each client. 
Since it must hold that $\sum_{c=1}^{K}p_c = 1$, two possible settings are: $p_c = 1/K$ or $p_c = n_c/n$, where $n = \sum_{c=1}^K n_c$. 

The generic federated round at each time $t$ is decomposed into the following steps and iteratively performed until convergence, i.e., for each $t\in \{1,2,\ldots,T\}$:
%\\
%{\em(i)} $S$ randomly selects a subset of clients ${\mathcal{C}}^{(t)}\subseteq \mathcal{C}$, so that $1 \leq |{\mathcal{C}}^{(t)}| \leq K$, and sends them the current, global model $\params^{(t)}$. At $t=1$, $\params^{(1)}$ is randomly initalized.
%In the following, we assume that the number of clients picked at each round is constant and fixed, i.e., $|\mathcal{C}^{(t)}| = m,~\forall t\in \{1,2,\ldots, T\}$.
%\\
%{\em(ii)} Each selected client $c\in {\mathcal{C}}^{(t)}$ trains its local model $\params_c^{(t)}$ on its own private data $\dataset_c$ by optimizing the following objective, starting from $\params^{(t)}$: $\params_c^{(t)} = \text{argmin}_{\params^{(t)}}\Loss_c(\params^{(t)}; \dataset_c)$.
%The value $\params_c^{(t)}$ is computed via gradient-based methods like stochastic gradient descent and sent to $S$. 
%\\
%{\em(iii)} $S$ computes $\params^{(t+1)} = \phi(\{\params_c^{(t)}~|~c\in \mathcal{C}^{(t)}\})$ as the updated global model, where $\phi: \R^{d^m} \mapsto \R^d $ is an \emph{aggregation function}; for example, $\phi = \frac{1}{m}\sum_{c\in \mathcal{C}^{(t)}} \params_c^{(t)}$, i.e., FedAvg or alike~\cite{lu2020spml}.

 \begin{enumerate}
     \item[{\em(i)}] $S$ randomly selects a subset of clients ${\mathcal{C}}^{(t)}\subseteq \mathcal{C}$, so that $1 \leq |{\mathcal{C}}^{(t)}| \leq K$, and sends them the current, global model $\params^{(t)}$. At $t=1$, $\params^{(1)}$ is randomly initalized.
     \item[{\em(ii)}] Each selected client $c\in {\mathcal{C}}^{(t)}$ trains its local model $\params_c^{(t)}$ on its own private data $\dataset_c$ by optimizing the following objective, starting from $\params^{(t)}$:
     \[
     \params_c^{(t)} = \text{argmin}_{\params^{(t)}}\Loss_c(\params^{(t)}; \dataset_c). \addtag
     \]
     The value of $\params_c^{(t)}$ is computed via gradient-based methods (e.g., stochastic gradient descent) and sent to $S$.
     \item[{\em(iii)}] $S$ computes $\params^{(t+1)} = \phi(\{\params_c^{(t)}~|~c\in \mathcal{C}^{(t)}\})$ as the updated global model, where $\phi: \R^{d^m} \mapsto \R^d $ is an \emph{aggregation function}; for example, $\phi = \frac{1}{m}\sum_{c\in \mathcal{C}^{(t)}} \params_c^{(t)}$, i.e., FedAvg or alike~\cite{lu2020spml}.
 \end{enumerate}

% REMOVED FOR AISTATS24
%A few alternatives to the scheme above are possible. For example, instead of sending the vector of parameters $\params_c^{(t)}$, each selected client may transmit to the server its displacement vector compared to the global model received at the beginning of the round, i.e., $\bm{u}_c^{(t)} = \params_c^{(t)} - \params^{(t)}$. 
%This way, the server computes the new global model as $\params^{(t+1)} = \params^{(t)} + \phi(\{\bm{u}_c^{(t)}~|~c\in \mathcal{C}^{(t)}\})$. %, namely the aggregation function is calculated on the local update vectors rather than the raw local models.
%Similarly, clients can send ``raw'' gradients $\nabla \Loss_c^{(t)}$ to the server; if so, $S$ aggregates the gradients and updates the global model accordingly, i.e., $\params^{(t+1)} = \params^{(t)} - \eta \phi(\{\nabla \Loss_c^{(t)}~|~c\in \mathcal{C}^{(t)}\})$, where $\eta$ is the learning rate.

% It is worth noticing that %, for the aggregation functions we consider in this paper, 
% sending local model updates $\params_c^{(t)}$ is equivalent to sending ``raw'' gradients $\nabla \Loss_c^{(t)}$ to the central server; in the latter case, $S$ simply aggregates the gradients and uses them to update the global model, i.e., $\params^{(t+1)} = \params^{(t)} - \eta \phi(\{\nabla \Loss_c^{(t)}~|~c\in \mathcal{C}^{(t)}\})$, where $\eta$ is the learning rate.

\subsection{The Attack Model: Federated Aggregation under Model Poisoning}
\label{subsec:byzantine}
The most straightforward aggregation function $\phi$ the server can implement is FedAvg, which computes the global model as the average of the local model weights received from clients. 
% FedAvg is widely used under non-ad\-ver\-sar\-ial settings~\cite{dean2012nips,konecny2016nipsws,mcmahan2017aistats}, i.e., when all the FL participants are supposed to be honest.
FedAvg is effective when all the FL participants behave honestly~\cite{dean2012nips,konecny2016nipsws,mcmahan2017aistats}.
Instead, this work assumes that an attacker controls a fraction $b=\lceil r*K \rceil, r\in [0,1]$ of the $K$ clients, i.e., $0\leq b\leq K$, known as malicious. This is in contrast to previous works, where $r < 0.5$.
Below, we describe our attack model.
%Moreover, each malicious client can send a perturbed vector of parameters to the server that \textit{arbitrarily} deviates from the vector it would have sent if it had acted correctly. 
%In doing so, the attacker can induce the jointly learned global model to make prediction errors indiscriminately on \textit{any} test example.

\noindent{{\bf {\em Attacker's Goal.}}}
Inspired by many studies on poisoning attacks against ML~\cite{rubinstein2009imc,biggio2012icml,biggio2013icb,xiao2015icml,li2016nips,yang2017ndss,jagielski2018sp}, we consider the attacker's goal is to jeopardize the jointly learned global model \textit{indiscriminately} at inference time with \textit{any} test example. 
Such attacks are known as {\em untargeted}~\cite{fang2020usenix}, as opposed to {\em targeted} poisoning attacks, where instead, the goal is to induce prediction errors only for some specific test inputs (e.g., via so-called \textit{backdoor triggers} as shown by~\cite{bagdasaryan2020pmlr}).
%\\

\noindent{{\bf {\em Attacker's Capability.}}}
%To achieve the goal above, we assume the attacker controls a fraction $b=\lceil r*m \rceil, r\in [0,1]$ of malicious clients, i.e., $0\leq b\leq m$. 
Like Sybil attacks to distributed systems~\cite{douceur2002iptps}, the attacker can inject $b$ fake clients into the FL system or compromise $b$ honest clients.
The attacker can arbitrarily manipulate the local models sent by malicious clients to the server $S$.
%REMOVED FOR AISTATS
%More formally, let $x \in \mathcal{C}$ be one of the $b$ corrupted clients selected by the server on the generic $t$-th FL round; $x$ will send to $S$ a malicious local model $\check{\params}_x^{(t)}$, rather than the legitimate $\params_x^{(t)}$, via \textit{model} poisoning.
%REMOVED FOR AISTATS
%The malicious client $x$ can compute $\check{\params}_x^{(t)}$ in two ways: via \textit{data} or \textit{model} poisoning, i.e., before or after local training. 
%As per how $x$ computes $\widetilde{\params}_x^{(t)}$, this can be done at least in two ways: via \textit{data} or \textit{model} poisoning, i.e., before or after local training. 
%REMOVED FOR AISTATS
%In the former case, $x$ finds $\check{\params}_x^{(t)}$ as the result of training its local model on a perturbed version $\check{\dataset}_{x}$ of its private data $\dataset_{x}$ (e.g., via label flipping~\cite{xiao2015icml}), i.e., $\check{\params}_x^{(t)} = \text{argmin}_{\params^{(t)}}\Loss_x(\params^{(t)}; \check{\dataset}_{x})$.
%REMOVED FOR AISTATS
%More formally, $x$ first computes its legitimate local model $\params_x^{(t)}$ using~(\ref{eq:client-opt}) without modifying its private data $\dataset_x$; then it finds $\check{\params}_x^{(t)}$ by applying a {\em post hoc} perturbation $\bm{\varepsilon} \in \R^d$ to $\params_x^{(t)}$. For example, $\check{\params}_x^{(t)} = \params_x^{(t)} + \bm{\varepsilon}$, where $\bm{\varepsilon}\sim \mathcal{N}(\bm{\mu}, \bm{\Sigma})$ is a Gaussian noise vector. 
%SHORT VERSION FOR AISTATS
More formally, let $x \in \mathcal{C}$ be one of the $b$ corrupted clients selected by the server on the generic $t$-th FL round; it first computes its legitimate local model $\params_x^{(t)}$ without modifying its private data $\dataset_x$; then it finds $\check{\params}_x^{(t)}$ by applying a {\em post hoc} perturbation $\bm{\varepsilon} \in \R^d$ to $\params_x^{(t)}$. For example, $\check{\params}_x^{(t)} = \params_x^{(t)} + \bm{\varepsilon}$, where $\bm{\varepsilon}\sim \mathcal{N}(\bm{\mu}, \bm{\Sigma})$ is a Gaussian noise vector. 
More advanced attack strategies have been designed, as discussed in Section~\ref{subsec:attacks}.
%\\
%REMOVED FOR AISTATS
%Notice that any data poisoning attack can be seen as a special case of a local model poisoning attack.
%REMOVED FOR AISTATS
%However, recent studies~\cite{bhagoji2019pmlr,fang2020usenix} show that {\em direct} local model poisoning attacks are more effective than {\em indirect} data poisoning attacks against FL. 
%REMOVED FOR AISTATS
%Therefore, we focus on the former in this work. 

%\\
\noindent{{\bf {\em Attacker's Knowledge.}}}
We assume the attacker knows its controlled clients' code, local training datasets, and local models. 
Moreover, %since this work is concerned with the robustness of the aggregation function, 
% REMOVED FOR UAI
% we consider the worst-case scenario (from an honest FL system's perspective), where the attacker also knows the aggregation function used by the central server.
we consider the worst-case scenario, where the attacker is \textit{omniscient}, i.e., it has full knowledge of the parameters sent by honest parties. This allows the attacker to choose the best parameters to inject into the FL protocol, e.g., by crafting malicious local models that are close to legitimate ones, thus increasing the probability of being chosen by the server for the aggregation.
% REMOVED FOR UAI
% Based on this knowledge, the attacker may design more disruptive poisoning strategies (\cite{fang2020usenix}).
% REMOVED FOR AISTATS
%Besides, this assumption is realistic as the FL service provider may want to publicly disclose the aggregation rule used to promote transparency and trust in the FL system~\cite{mcmahan2017aistats}.

%\\
\noindent{{\bf {\em Attacker's Behavior.}}}
In each FL round, the attacker, similar to the server, randomly selects $b$ clients to corrupt out of the $K$ available. Any of these $b$ malicious clients that happen to be among those selected by the server will poison their local models using one of the strategies outlined in Section~\ref{subsec:attacks}. 
Note that, unlike earlier studies, we do not assume that malicious clients are chosen in the initial round and remain constant throughout the training. We argue that this approach is more realistic, as legitimate clients can be compromised at any point in actual deployments. Therefore, an effective defense in FL should ideally exclude these clients as soon as they submit their first malicious model.
% At each FL round, the attacker can instruct any of its $b$ controlled clients selected by the server to poison their local models using one of the strategies outlined in Section~\ref{subsec:attacks}.
% % NOT SURE WHETHER TO KEEP THE FOLLOWING (WE MAY NEED TO SHOW WHAT HAPPENS UNDER "DISCONTINUOUS" ATTACKS)
% Moreover, similarly to what the server does, the attacker randomly selects \textit{b} corrupted clients from $\mathcal{C}$ at each round.
%any corrupted client alternates legitimate and malicious behavior between FL rounds to increase stealthiness. % once the attack starts.

This category of {\em untargeted model poisoning} attacks has been extensively explored in previous works~\cite{blanchard2017nips,bhagoji2019pmlr,fang2020usenix}.
It has been shown that including updates even from a single malicious client can wildly disrupt the global model if the server runs standard FedAvg~\cite{blanchard2017nips,yin2018icml}. 
%Thus, several aggregation rules robust to such types of model poisoning attacks have been developed.
%\\
% \noindent \textbf{A note on the terminology used in this paper.} The type of model poisoning attacks we consider here are often referred to as \textit{Byzantine} attacks in the literature~\cite{blanchard2017nips,fang2020usenix,barroso2023if}. 
% Although, in this work, we adhere to the taxonomy proposed by \citeauthor{barroso2023if}~\shortcite{barroso2023if}, the research community has yet to reach a unanimous consensus on the terminology. 
% In fact, some authors use the word ``Byzantine'' as an umbrella term to broadly indicate \textit{any} attack involving malicious clients (e.g., targeted data poisoning like backdoor attacks \textit{and} untargeted model poisoning~\cite{hu2021arxiv}).
% Therefore, to avoid confusion and hurting the feelings of some readers who have already debated on that and found the term inappropriate or disrespectful,\footnote{\small{\url{https://openreview.net/forum?id=pfuqQQCB34&noteId=5KAMwoI2cC}}} we have decided \textit{not} to use the word ``Byzantine'' to refer to our attack model.