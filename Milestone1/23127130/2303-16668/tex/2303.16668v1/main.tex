%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\pdfoutput=1
%\documentclass[nohyperref]{article}
\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
%\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2022} with \usepackage[nohyperref]{icml2022} above.
\usepackage[hyphens]{url}
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[arxiv]{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2022}

% For theorems and such


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{bm}
\usepackage{bbm}
\setlength {\marginparwidth }{2cm} % fix todonotes warning
\usepackage{caption}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{soul}


\renewcommand\theadalign{cc}
%\renewcommand\theadfont{\bfseries}
\renewcommand\theadfont{\normalsize}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argsup}{arg\,sup}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{FLANDERS}

\begin{document}

\twocolumn[
\icmltitle{A Byzantine-Resilient Aggregation Scheme for Federated Learning via Matrix Autoregression on Client Updates}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2022
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Gabriele Tolomei}{sapienza}
\icmlauthor{Edoardo Gabrielli}{sapienza}
\icmlauthor{Dimitri Belli}{isti}
\icmlauthor{Vittorio Miori}{isti}
\end{icmlauthorlist}

\icmlaffiliation{sapienza}{Department of Computer Science, Sapienza University of Rome, Italy}
\icmlaffiliation{isti}{ISTI-CNR, Pisa, Italy}

\icmlcorrespondingauthor{Gabriele Tolomei}{tolomei@di.uniroma1.it}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Federated Learning, Robust Aggregation, Byzantine Model Poisoning}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
In this work, we propose FLANDERS, a novel federated learning (FL) aggregation scheme robust to Byzantine attacks.
FLANDERS considers the local model updates sent by clients at each FL round as a matrix-valued time series. Then, it identifies malicious clients as outliers of this time series by comparing actual observations with those estimated by a matrix autoregressive forecasting model. 
Experiments conducted on several datasets under different FL settings demonstrate that FLANDERS matches the robustness of the most powerful baselines against Byzantine clients. 
Furthermore, FLANDERS remains highly effective even under extremely severe attack scenarios, as opposed to existing defense strategies.
\end{abstract}

\input{macros}

% INTRODUCTION
\section{Introduction}
\label{sec:introduction}

% Diffusion of FL
Recently, {\em federated learning} (FL)~\cite{mcmahan2017googleai,mcmahan2017aistats} has emerged as the leading paradigm for training distributed, large-scale, and privacy-preserving machine learning (ML) systems. 
The core idea of FL is to allow multiple edge clients to collaboratively train a shared, global model without disclosing their local private training data.
\\
Specifically, an FL system consists of a central server and many edge clients; a typical FL round involves the following steps: {\em(i)} the server sends the current, global model to the clients and appoints some of them for training; {\em(ii)} each selected client locally trains its copy of the global model with its own private data; then, it sends the resulting local model back to the server;\footnote{Whenever we refer to global/local model, we mean global/local model {\em parameters}.} {\em(iii)} the server updates the global model by computing an \emph{aggregation function} on the local models received from clients (by default, the average, also referred to as FedAvg).
The process above continues until the global model converges (e.g., after a certain number of rounds or other similar stopping criteria).

The advantages of FL over the traditional, centralized learning paradigm are undoubtedly clear in terms of flexibility/scalability (clients can join/disconnect from the FL network dynamically), network communications (only model weights\footnote{We will use \textit{parameters} and \textit{weights} interchangeably.} are exchanged between clients and server), and privacy (each client's private training data is kept local at the client's end and is not uploaded to the centralized server).
\\
% Security threats to FL
However, the growing adoption of FL also raises security concerns~\cite{costa2022covert}, particularly about its confidentiality, integrity, and availability.
Here, we focus on \emph{Byzantine model poisoning} attacks, where an adversary attempts to tweak the global model weights~\cite{bhagoji2019pmlr} by directly perturbing the local model's parameters of some infected FL clients before these are sent to the central server for aggregation.
It turns out that Byzantine model poisoning attacks severely impact standard FedAvg; therefore, more robust aggregation functions must be designed to make FL systems secure.

% Current countermeasures and their limitations
Several countermeasures have been proposed in the literature to combat Byzantine model poisoning attacks on FL systems, i.e., to discard possible malicious local updates from the aggregation performed at the server's end. 
These techniques range from simple statistics more robust than plain average (e.g., Trimmed Mean and FedMedian~\cite{yin2018icml}) to outlier detection heuristics (e.g., Krum/Multi-Krum~\cite{blanchard2017nips} and Bulyan~\cite{mhamdi2018pmlr}) or data-driven approaches (e.g., via K-means clustering~\cite{shen2016acm}), or methods based on ``source of trust'' (e.g., FLTrust~\cite{cao2020fltrust}).
\\
% Limitations of existing Byzantine-resilient strategies
Unfortunately, existing Byzantine-resilient strategies are either too simple, or rely on strong and unrealistic assumptions to work effectively (e.g., knowing in advance the number of malicious clients in the FL system, as for Krum and alike).
Furthermore, outlier detection methods, such as K-means clustering, do not consider the temporal evolution of local model updates received. 
Finally, strategies like FLTrust require the server to collect its own dataset and act as a proper client, thereby altering the standard FL protocol.

% Description of the proposed method
In this work, we introduce a novel aggregation function robust to Byzantine model poisoning attacks based on a data-driven strategy that incorporates time dependency. 
Specifically, we formulate the problem of identifying Byzantine client updates as a multidimensional (i.e., matrix-valued) time series anomaly detection task. 
Inspired by the matrix autoregressive (MAR) framework for multidimensional time series forecasting~\cite{chen2021je}, we present a method to actually implement this new Byzantine-resilient aggregation function, which we call FLANDERS ({\em \textbf{F}ederated \textbf{L}earning leveraging \textbf{AN}omaly \textbf{DE}tection for \textbf{R}obust and \textbf{S}ecure}) aggregation.
The advantage of FLANDERS over existing approaches is its increased flexibility in quickly recognizing local model drifts caused by malicious clients.

% Summary of the main contribution of this work
Below, we summarize the main contributions of this work:
\\
{\em(i)} We propose FLANDERS, which, to the best of our knowledge, is the first Byzantine-resilient aggregation for FL based on multidimensional time series anomaly detection.
\\
{\em(ii)} We integrate FLANDERS into Flower,\footnote{\scriptsize{\url{https://flower.dev/}}} a popular FL simulation framework.
\\
{\em(iii)} We compare the robustness of FLANDERS against that of the most popular baselines under multiple settings: different datasets (tabular vs. non-tabular), tasks (classification vs. regression), models (standard vs. deep learning), data distribution (i.i.d. vs. non-i.i.d.), and attack scenarios.
\\
{\em(iv)} We publicly release all the implementation code of FLANDERS along with our experiments.\footnote{\scriptsize{\url{https://anonymous.4open.science/r/flanders}}}

% Paper's structure and organization
The remainder of the paper is structured as follows.
Section~\ref{sec:background} covers background and preliminaries. 
In Section~\ref{sec:related}, we discuss related work.
Section~\ref{sec:problem} and Section~\ref{sec:method} describe the problem formulation and the method proposed. % to tackle it. 
Section~\ref{sec:experiments} gathers experimental results. 
Finally, Section~\ref{sec:conclusion} discusses the limitations of this work and draws future research directions.

% BACKGROUND & NOTATION
\section{Background and Preliminaries}
\label{sec:background}
\newcommand\addtag{\refstepcounter{equation}\tag{\theequation}}
\subsection{Federated Learning}
\label{subsec:fl}
We consider a typical supervised learning task under a standard FL setting, which consists of a central server $S$ and a set of distributed clients $\mathcal{C}$, such that $|\mathcal{C}|=K$.
Each client $c\in \mathcal{C}$ has access to its own private training set $\dataset_c$, namely the set of its $n_c$ local labeled examples, i.e., $\dataset_c = \{\bm{x}_{c,i}, y_{c,i}\}_{i=1}^{n_c}$.

The goal of FL is to train a global predictive model whose architecture and parameters $\params^*\in \R^d$ are shared amongst all the clients and found solving the following objective:
\[
\params^* = \text{argmin}_{\params} \Loss(\params) = \text{argmin}_{\params} \sum_{c=1}^K p_c \Loss_c(\params;\dataset_c), \addtag \label{eq:erm}
\]
where $\Loss_c$ is the local objective function for client $c$. Usually, this is defined as the empirical risk calculated over the training set $\dataset_c$ sampled from the client's local data distribution:
\[
\Loss_c(\params;\dataset_c) = \frac{1}{n_c}\sum_{i=1}^{n_c} \loss(\params;(\insta_{c,i}, y_{c,i})), \addtag
\]
where $\loss$ is an instance-level loss (e.g., cross-entropy loss or squared error in the case of classification or regression tasks, respectively). 
Furthermore, each $p_c \geq 0$ specifies the relative contribution of each client. 
Since it must hold that $\sum_{c=1}^{K}p_c = 1$, two possible settings for it are: $p_c = 1/K$ or $p_c = n_c/n$, where $n = \sum_{c=1}^K n_c$. 
 
The generic federated round at each time $t$ is decomposed into the following steps and iteratively repeated until convergence, i.e., for each $t=1,2,\ldots,T$:

{\em(i)} $S$ sends the current, global model $\params^{(t)}$ to every client and selects a subset ${\mathcal{C}}^{(t)}\subseteq \mathcal{C}$, so that $1 \leq |{\mathcal{C}}^{(t)}| \leq K$. 
To ease of presentation, in the following, we assume that the number of clients picked at each round is constant and fixed, i.e., $|\mathcal{C}^{(t)}| = m,~\forall t\in \{1,2,\ldots, T\}$.
\\
{\em(ii)} Each selected client $c\in {\mathcal{C}}^{(t)}$ trains its local model $\params_c^{(t)}$ on its own private data $\dataset_c$ by optimizing the following objective, starting from $\params^{(t)}$:
\begin{equation}
    \params_c^{(t)} = \text{argmin}_{\params^{(t)}}\Loss_c(\params^{(t)}; \dataset_c).
\label{eq:client-opt}
\end{equation}
The value of $\params_c^{(t)}$ is computed via gradient-based methods (e.g., stochastic gradient descent) and sent to $S$. 
\\
{\em(iii)} $S$ computes $\params^{(t+1)} = \phi(\{\params_c^{(t)}~|~c\in \mathcal{C}^{(t)}\})$ as the updated global model, where $\phi$ is an \emph{aggregation function} (e.g., FedAvg or one of its variants~\cite{lu2020spml}).

It is worth noticing that, for the aggregation functions we consider in this paper, sending local model updates $\params_c^{(t)}$ is equivalent to sending ``raw'' gradients $\nabla \Loss_c^{(t)}$ to the central server; in the latter case, $S$ simply aggregates the gradients and uses them to update the global model, i.e., $\params^{(t+1)} = \params^{(t)} - \eta \phi(\{\nabla \Loss_c^{(t)}~|~c\in \mathcal{C}^{(t)}\})$, where $\eta$ is the learning rate.

\subsection{Model Aggregation Under Byzantine Attacks}
\label{subsec:byzantine}
The most straightforward aggregation function $\phi$ the server can implement is FedAvg, which computes the global model weights as the average of the local model weights received from clients. 
This aggregation rule is widely used under non-ad\-ver\-sar\-ial settings~\cite{dean2012nips,konecny2016nipsws,mcmahan2017aistats}, i.e., when all the clients participating in the FL network are supposed to behave honestly.
\\
Instead, this work assumes that an attacker controls a fraction $b=\lceil r*K \rceil, r\in [0,1]$ of the $K$ clients, i.e., $0\leq b\leq K$. 
Moreover, each of these malicious clients can send a perturbed vector of parameters to the server that {\em arbitrarily} deviates from the vector it would send if it acted correctly.
Note that this type of {\em poisoning attack}, also known as \textit{Byzantine}, has been extensively investigated in previous work~\cite{blanchard2017nips,fang2020usenix}.
We formalize the Byzantine attack model in Section~\ref{subsec:attack-model}. 
\\
It has been shown that including updates even from a single Byzantine client can wildly disrupt the global model if the server performs FedAvg aggregation~\cite{blanchard2017nips,yin2018icml}. 
Therefore, several aggregation rules robust to Byzantine model poisoning attacks have been developed.

% RELATED WORK
\section{Related Work}
\label{sec:related}

\subsection{Defenses against Byzantine Attacks on FL}
In the following, we describe the most popular aggregation strategies for countering Byzantine model poisoning attacks on FL systems, which will also be used as the baselines for our comparison.
We invite the reader to refer to~\cite{hu2021arxiv} for a comprehensive overview of Byzantine-robust FL aggregation methods and to~\cite{barroso2023if} for a survey on FL security threats in general.

\noindent{{\bf {\em FedMedian}}~\cite{yin2018icml}{\bf.}}
With FedMedian, the central server sorts the $j$-th parameters received from all the $m$ local models and takes the median of those as the value of the $j$-th parameter of the global model. This process is applied for all the model parameters, i.e., $\forall j\in \{1,\ldots,d\}$.

\noindent{{\bf {\em Trimmed Mean}}~\cite{xie2018corr}{\bf.}}
Trimmed Mean is a family of aggregation rules that contains multiple variants, but the general idea remains the same.
This rule computes a model as FedMedian does, then it averages the $k$ nearest parameters to the median.
Suppose at most $b$ clients are compromised, this aggregation rule achieves order-optimal error rate when $b \leq \frac{m}{2} - 1$.

\noindent{{\bf {\em Krum}}~\cite{blanchard2017nips}{\bf.}}
Krum selects one of the $m$ local models received from the clients that is most similar to {\em all} other models as the global model.
The intuition is that, even if the selected local model is from a malicious client, its impact would be limited since it is similar to other local models, possibly from benign clients.

\noindent{{\bf {\em Bulyan}}~\cite{mhamdi2018pmlr}{\bf.}}
Since the Euclidean distance between two high-dimensional vectors may be substantially affected by a single component, Krum could be influenced by a few abnormal local model weights and become ineffective for complex, high-dimensional parameter spaces. To address this issue, Bulyan combines Krum and a variant of Trimmed Mean. Specifically, it first iteratively applies Krum to select $\alpha$ local models, and then it uses a Trimmed Mean variant for aggregation.

\noindent{{\bf {\em FLTrust}}~\cite{cao2020fltrust}{\bf.}}
This approach assumes the server acts itself as a client, i.e., it collects a {\em trustworthy} small dataset (called {\em root dataset}) on which it trains its own local model (called {\em server model}) for comparison against all the local models from other clients.


\subsection{Multidimensional Time Series Analysis}
Multidimensional time series analysis aims to identify patterns in datasets with more than one time-dependent variable.
Two primary goals of (multidimensional) time series analysis are {\em forecasting}~\cite{mahalakshmi2016icctide,basu2015annalsstats} and {\em anomaly detection}~\cite{chandola2009acm,chalapathy2019arxiv,schmidl2022pvld}.

\noindent{{\bf {\em MSCRED}}~\cite{zhang2019aaai}{\bf.}}
This method performs anomaly detection in multivariate time series using a Multi-Scale Convolutional Recurrent Encoder-Decoder (MSCRED) network. In detail, MSCRED builds multi-scale signature matrices to characterize multiple levels of the system statuses in different time steps. Then, it uses a convolutional encoder to represent the inter-time series correlations and trains an attention-based convolutional Long-Short Term Memory network to capture temporal patterns. Anomalies correspond to residual errors computed as the difference between the input and reconstructed signature matrices.

% PROBLEM FORMULATION
\section{Problem Formulation}
\label{sec:problem}

\subsection{Local Model Updates as Multidimensional Time Series Data}
At the end of each FL round $t$, the central server $S$ collects the model updates $\{\params_c^{(t)}\}_{c\in \mathcal{C}^{(t)}}$ sent by a subset of selected clients.\footnote{A similar reasoning would apply if clients sent their local gradients rather than local model parameters.}
Without loss of generality, we assumed that the number of clients picked at each round is constant and fixed, i.e., $|\mathcal{C}^{(t)}| = m,~\forall t\in \{1,2,\ldots, T\}$.
We can arrange the model updates received at round $t$ by clients $\mathcal{C}^{(t)}$ into an $d\times m$ matrix $\Params_{t}$, whose $c$-th column corresponds to the $d$-dimensional vector of parameters $\params_c^{(t)}$ sent by client $c$, i.e., $\Params_{t} = \Big[\params_1^{(t)}, \ldots, \params_c^{(t)}, \ldots, \params_m^{(t)}\Big]$. 
We consider the multidimensional time series represented by the $d\times\ m \times T$ tensor $\Params_{1:T} = \Big[\Params_{1}, \Params_{2}, \ldots, \Params_{T} \Big]$, as depicted in Fig.~\ref{fig:tensor}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{./img/tensor}
    \caption{The $d$-dimensional vector of parameters $\params_c^{(t)}$ sent by client $c$ at round $t$ (left). The $d\times m$ matrix $\Params_t$ of {\em all} the $m$ client updates sent at round $t$ (middle). The $d\times m\times T$ tensor $\Params_{1:T}$ of all the client updates after $T$ rounds (right).}
    \label{fig:tensor}
    \vspace{-4mm}
\end{figure}

\subsection{Byzantine Attack Model}
\label{subsec:attack-model}
\noindent{{\bf {\em Attacker's goal.}}}
Like many studies on poisoning attacks~\cite{rubinstein2009imc,biggio2012icml,biggio2013icb,xiao2015icml,li2016nips,yang2017ndss,jagielski2018sp,fang2020usenix}, we consider the attacker's goal is to manipulate the jointly learned global model with a high error rate indiscriminately for test examples. Such attacks are known as {\em untargeted}, as opposed to {\em targeted} poisoning attacks, where instead, the goal is to induce prediction errors only for some specific test instances.

\noindent{{\bf {\em Attacker's capability.}}}
To achieve the goal above, we assume the attacker controls a fraction $b=\lceil r*K \rceil, r\in [0,1]$ of Byzantine clients, i.e., $0\leq b\leq K$. 
Like Sybil attacks~\cite{douceur2002iptps} to distributed systems, the attacker could inject $b$ fake clients into the FL system or compromise $b$ benign clients.
The attacker can arbitrarily manipulate the local
models sent by these malicious clients to the central server.
More formally, let $x \in \mathcal{C}$ be one of the $b$ corrupted clients. 
On the generic $t$-th FL round, $x$ will send to $S$ its malicious local model update $\widetilde{\params}_x^{(t)}$ rather than the legitimate $\params_x^{(t)}$.
As per how $x$ computes $\widetilde{\params}_x^{(t)}$, this can be done at least in two ways: via data or model poisoning, i.e., before or after local training. 
In the former case, $x$ finds $\widetilde{\params}_x^{(t)}$ as the result of performing its local training on a perturbed version $\widetilde{\dataset}_{x}$ of its private data $\dataset_{x}$ (e.g., via label flipping~\cite{xiao2015icml}), i.e., $\widetilde{\params}_x^{(t)} = \text{argmin}_{\params^{(t)}}\Loss_x(\params^{(t)}; \widetilde{\dataset}_{x})$.
In the latter case, $x$ first legitimately computes $\params_x^{(t)}$ using Eq.~(\ref{eq:client-opt}) without modifying its local data $\dataset_x$; then it obtains $\widetilde{\params}_x^{(t)}$ by applying a {\em post hoc} perturbation $\bm{\varepsilon} \in \R^d$ to the correct local model update. For example, $\widetilde{\params}_x^{(t)} = \params_x^{(t)} + \bm{\varepsilon}$, where $\bm{\varepsilon}\sim \mathcal{N}(\bm{\mu}, \bm{\Sigma})$ is a random Gaussian noise vector.
\\
Notice that any data poisoning attack can be seen as a special case of a local model poisoning attack.
However, recent studies~\cite{bhagoji2019pmlr,fang2020usenix} show that {\em direct} local model poisoning attacks are more effective than {\em indirect} data poisoning attacks against FL. 
Therefore, we focus on the former in this work. 

\noindent{{\bf {\em Attacker's knowledge.}}}
We suppose the attacker knows the code, local training datasets, and local models on its controlled clients. 
Moreover, since this work is concerned with the robustness of the aggregation function, we consider the worst-case scenario (from an honest FL system's perspective), where the attacker also knows the aggregation function used by the central server.
Based on this knowledge, the attacker may design more effective (i.e., disruptive) poisoning strategies~\cite{fang2020usenix}.
Besides, this assumption is realistic as the FL service provider may want to publicly disclose the aggregation rule used to promote transparency and trust in the FL system~\cite{mcmahan2017aistats}.

\noindent{{\bf {\em Attacker's behavior.}}}
We suppose there is no anomaly in the client updates observed in the first $L<T$ rounds, i.e., either no malicious clients have joined the FL network yet ($b=r=0$) or, even if they have, they have been acting legitimately (e.g., to achieve stealthiness and elude possible detection mechanisms).
Then, assuming the Byzantine attack starts after $L$ rounds, our goal is to detect anomalous model updates sent afterwards (i.e., at any round $L+j$, where $j > 0$), and discard them from the input to the aggregation function (e.g., FedAvg). 

\subsection{Byzantine Model Updates as Matrix-Based Time Series Outliers}
\label{subsec:ts-outliers}
Our problem can be therefore formulated as a multidimensional time series anomaly detection task.
At a high level, we want to equip the central server $S$ with an anomaly scoring function that, based on the historical observations of model updates seen so far from {\em all} the clients, estimates the degree of each client being malicious, i.e., its {\em anomaly score}.
Such a score can in turn be used to restrict the set of candidate clients that are trustworthy for the aggregation.
\\
More formally, at the generic $t$-th FL round, the central server $S$ must compute the anomaly score $s_{c}^{(t)} \in \R$ of every selected client $c\in \mathcal{C}^{(t)}$. 
To achieve that, we assume $S$ can leverage the past $w$ observations of model updates received, i.e., $\Params_{t-w:t-1}$, where $1\leq w \leq t-1$.
The server will thus rank all the $m$ selected clients according to their anomaly scores (e.g., from the lowest to the highest).
Hence, several strategies can be adopted to choose which model updates should be aggregated in preparation for the next round, i.e., to restrict from the initial set $\mathcal{C}^{(t)}$ to another (possibly smaller) set $\mathcal{C}_{*}^{(t)}\subseteq \mathcal{C}^{(t)}$ of trusted clients. 
For example, $S$ may consider only the model updates received from the top-$k$ clients ($1 \leq k \leq m$) with the smallest anomaly score, i.e., $\mathcal{C}_*^{(t)} = \{c\in \mathcal{C}^{(t)}~|~s_{c}^{(t)} \leq s_k^{(t)}\}$, where $s_k^{(t)}$ indicates the $k$-th smallest anomaly score at round $t$. 
Alternatively, the raw anomaly scores computed by the server can be converted into well-calibrated probability estimates. 
Here, the server sets a threshold $\rho \in [0,1]$ and aggregates the weights only of those clients whose anomaly score is below $\rho$, i.e., $\mathcal{C}_*^{(t)} = \{c\in \mathcal{C}^{(t)}~|~s_c^{(t)}\leq \rho\}$. 
In the former case, the number of considered clients ($k$) is bound apriori,\footnote{This may not be true if anomaly scores are not unique; in that case, we can simply enforce $|\mathcal{C}_*^{(t)}| = k$.} whereas the latter does not put any constraint on the size of final candidates $|\mathcal{C}_*^{(t)}|$.
Eventually, $S$ will compute the updated global model $\params^{(t+1)} = \phi(\{\params_c^{(t)}~|~c\in \mathcal{C}_*^{(t)}\})$, where $\phi$ is any well-known aggregation function, e.g., FedAvg.
\\
Several techniques can be used to compute the anomaly scoring function. 
In this work, we leverage the matrix autoregressive (MAR) framework for multidimensional time series forecasting proposed by Chen et al.~(\citeyear{chen2021je}).

% PROPOSED METHOD
\section{Proposed Method: FLANDERS}
\label{sec:method}

\subsection{Matrix Autoregressive Model (MAR)}
\label{subsec:mar}
Many time series data involve observations that appear in a matrix form.
For example, a set of atmospheric measurements like temperature, pressure, humidity, etc. (rows) are regularly reported by different weather stations spread across a given region (columns) every hour (time).
In cases like this, the usual approach is to flatten matrix observations into long vectors, then use standard vector autoregressive models (VAR) for time series analysis.
However, VAR has two main drawbacks: {\em (i)} it looses the original matrix structure as it mixes rows and columns, which instead may capture different relationships; {\em (ii)} it significantly increases the model complexity to a large number of parameters.
\\
To overcome the two issues above, Chen et al.~(\citeyear{chen2021je}) introduce the matrix autoregressive model (MAR).
In its most generic form, MAR($p$) is a $p$-order autoregressive model:
\[
    {\bm X}_t = {\bm A}_1 {\bm X}_{t-1} {\bm B}_1 + \cdots + {\bm A}_p {\bm X}_{t-p} {\bm B}_p + {\bm E}_t,
    \addtag \label{eq:mar-p}
\]
where ${\bm X}_t$ is a $d\times m$ matrix of observations at time $t$, ${\bm A}_i$ and ${\bm B}_i$ ($i\in \{1,\ldots, p\})$ are $d\times d$ and $m\times m$ autoregressive coefficient matrices, and ${\bm E}_t$ is a $d\times m$ white noise matrix.
In this work, we consider the simplest MAR($1$)\footnote{Unless otherwise specified, whenever we refer to MAR, we assume MAR($1$).} forecasting model, i.e., ${\bm X}_t = {\bm A} {\bm X}_{t-1} {\bm B} + {\bm E}_t$, where the matrix of observations at time $t$ depends only on the previously observed matrix at time step $t-1$.
\\
Let $\hat{\bm{X}}_t = \hat{{\bm A}} {\bm{X}}_{t-1}\hat{{\bm B}}$ be the {\em estimated} matrix of observations at time $t$, according to MAR for some coefficient matrices $\hat{{\bm A}}$ and $\hat{{\bm B}}$.
Also, suppose we have access to $w > 0$ historical matrix observations. 
Thus, we compute the best coefficients ${\bm A}$ and ${\bm B}$ by solving the following objective: 
\[
    {\bm A}, {\bm B} = \argmin_{\hat{{\bm A}}, \hat{{\bm B}}}\Big\{\sum_{j=0}^{w-1} ||{\bm X}_{t-j} - \hat{{\bm A}}{\bm X}_{t-j-1}\hat{{\bm B}}||^2_{\text F} \Big\},
    \addtag \label{eq:mar-opt}
\]
where $||\cdot||_{\text F}$ indicates the Frobenius norm of a matrix.
The coefficients ${\bm A}$ and ${\bm B}$ can thus be estimated via alternating least squares (ALS) optimization~\cite{koren2009ieeecomp}.
We provide further details on MAR in Appendix~\ref{app:mar}.

\subsection{MAR-based Anomaly Score}
So far, we have described an existing method for multidimensional time series forecasting, i.e., MAR. But how could MAR be helpful for detecting, and therefore discarding, local model sent by malicious FL clients?
Indeed, MAR can be used to compute the anomaly score and thus identify possible outliers, as indicated in Section~\ref{subsec:ts-outliers}.
\\
First of all, notice that the $d\times m$ matrix of $d$-dimensional model updates $\Params_t$ received by the central server from the $m$ clients at time $t$ nicely fits with the hypothesis under which MAR is supposed to work. 
We can set an analogy between FL training and our example in Section~\ref{subsec:mar}. 
Specifically, the matrix of observations of atmospheric measurements collected from distributed weather stations at a given time step resembles the matrix of local model weights sent by edge clients to the server at a given FL round.
Therefore:
\begin{equation}
\label{eq:mar-fl}
\Params_t = {\bm A}\Params_{t-1}{\bm B} + {\bm E}_t,
\end{equation}
To estimate ${\bm A}$ and ${\bm B}$ we consider two separate cases: before malicious clients start sending corrupted updates (i.e., until the $L$-th time step), and from the $(L+1)$-th time step onward.
We start focusing on the first $L$ matrices of observed weights, i.e., $\Params_1, \Params_2, \ldots, \Params_{L}$, and we set a fixed window size of observations $w$, which we will use for training.
Then, following~(\ref{eq:mar-opt}), we compute ${\bm A}$ and ${\bm B}$ as follows:
\begin{equation}
{\bm A}, {\bm B} = \argmin_{\hat{{\bm A}}, \hat{{\bm B}}}\Big\{ \sum_{j=0}^{w-1} ||{\Params}_{L-j} - \hat{{\Params}}_{L-j}||^2_{\text F}\Big\},
\end{equation}
where $\hat{{\Params}}_{L-j} = \hat{{\bm A}}\Params_{L-1-j}\hat{{\bm B}}$ is the predicted weight matrix.
Notice that $1\leq w\leq (L-1)$. 
In particular, if $w=1$ then ${\bm A}$ and ${\bm B}$ are computed using only $\Params_{L-1}$ and $\Params_{L}$.
On the other side of the spectrum, if $w=L-1$ then ${\bm A}$ and ${\bm B}$ are estimated using {\em all} the $L$ previous observations: $\Params_{1}, \Params_{2},\ldots, \Params_{L}$.
This last setting is a natural choice for building a consistent MAR forecasting model under no attack. 
However, in the real world, the FL server does not know in advance if and when malicious clients kick in, i.e., the number $L$ of legitimate FL rounds is generally unknown apriori.
Still, one could imagine that the initial parameters of a ``clean'' MAR model can be estimated during a bootstrapping stage in a ``controlled'' FL environment, where only well-known trusted clients are allowed to participate.
Anyway, no matter how the MAR forecasting model is initially trained on clean observations, we use it to compute the anomaly score as follows.
Consider the matrix of observed weights $\Params_{t}$ at the generic FL round $t > L$. 
We know that this matrix may contain one or more corrupted local model updates sent by malicious clients.
Then, we compute the $m$-dimensional anomaly score vector ${\bm s}^{(t)}$ as follows:
\begin{equation}
\label{eq:as-vector}
    {\bm s}^{(t)}[c] = s_c^{(t)} = \delta(\Params_{t}[:,c], \hat{\Params}_t[:,c]),
\end{equation}
where: $c$ is one of the $m$ clients; $\Params_{t}[:,c]$ indicates the $c$-th column of the observed $\Params_{t}$ and contains the $d$-dimensional vector of weights sent by client $c$; $\hat{\Params}_{t} = {\bm A}\Params_{t-1}{\bm B}$ is the matrix {\em predicted} by MAR at time $t$, using the previous observation $\Params_{t-1}$ and the coefficients ${\bm A}$ and ${\bm B}$ learned so far; 
and $\delta(\cdot)$ is a function measuring the distance between the observed vector of weights sent to the server and the predicted vector of weights output by MAR. 
In this work, we set $\delta = ||\Params_{t}[:,c] - \hat{\Params}_t[:,c]||_2^2$, where $||\cdot||_2^2$ is the squared $L^2$-norm. 
Other functions can be used (e.g., {\em cosine distance}), especially in high dimensional spaces, where $L^p$-norm with $p\in (0,1)$ has proven effective~\cite{aggarwal2001icdt}. 
However, choosing the best $\delta$ is outside the scope of this work, and we leave it to future study.
\begin{figure}[htb!]
    \centering
    \includegraphics[width=\columnwidth]{./img/flanders}
    \caption{Overview of FLANDERS.}
    \label{fig:flanders}
\end{figure}

According to one of the filtering strategies discussed in Section~\ref{subsec:ts-outliers}, we retain only the $k$ clients with the smallest anomaly scores. The remaining $m-k$ clients will be discarded and not contribute to the aggregation performed by the server (e.g., FedAvg).
\\
At the next round $t+1$, to compute the updated anomaly score vector as in Eq.~(\ref{eq:as-vector}), we need to refresh our MAR model, i.e., to update the coefficient matrices ${\bm A}$ and ${\bm B}$, based on the latest observed $\Params_{t}$, along with the other $w-1$ previous matrices of local updates.
Since the initially observed matrix $\Params_{t}$ contains $m-k$ potentially malicious clients, we cannot use it as-is to update our MAR model. Otherwise, we may alter the estimated coefficients ${\bm A}$ and ${\bm B}$ by considering possibly corrupted matrix columns.
The solution we propose to overcome this problem is to replace the original $\Params_{t}$ with $\Params'_{t}$ {\em before} feeding it to train the new MAR model. 
Specifically, $\Params'_{t}$ is obtained from $\Params_{t}$ by substituting the $m-k$ anomalous columns with the parameter vectors from the same clients observed at time $t-1$, which are supposed to be still legitimate.
The advantage of this solution is twofold. 
On the one hand, a client labeled as malicious at FL round $t$ would likely still be considered so at $t+1$ if it keeps perturbing its local weights, thus improving robustness.
On the other hand, our solution allows malicious clients to alternate legitimate behaviors without being banned, hence speeding up model convergence.
Notice that the two considerations above might not be valid if we updated our MAR model using the original, partially corrupted $\Params_{t}$. 
Indeed, in the first case, the distance between two successive corrupted vectors of weights by the same client would realistically be small. So the client's anomaly score will likely drop to non-alarming values, thereby increasing the number of false negatives.
In the second case, the distance between a corrupted vector of weights and a legitimate one would likely be considerable. 
Therefore, a malicious client will maintain its anomaly score high even if it decides to act normally, thus impacting the number of false positives.
\\
The overview of FLANDERS is depicted in Fig.~\ref{fig:flanders}. 

% EXPERIMENTS
\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}
\noindent{{\bf {\em Datasets, Tasks, and FL Models.}}}
We consider a mix of four tabular and non-tabular (i.e., image) public datasets used for classification ({\em Income}, {\em MNIST}, and {\em CIFAR-10}) and regression ({\em California Housing}) tasks. 
Except for the {\em MNIST} and {\em CIFAR-10} datasets that already come split into a training and a test portion, the remaining datasets are randomly shuffled and partitioned into two disjoint sets: 80\% reserved for training and 20\% for testing.
\\
Eventually, for each dataset/task combination, we choose a dedicated FL model amongst the following: Logistic Regression (LogReg), Multilayer Perceptron (MLP), Convolutional Neural Network (CNN), and Linear Regression (LinReg).
All models are trained by minimizing cross-entropy loss (in the case of classification tasks) or elastic net regularized mean squared error (in the case of regression). 
At each FL round, every client performs one iteration of cyclic coordinate descent (LogReg and LinReg) or one epoch of stochastic gradient descent (MLP and CNN).
\\
The full details are shown in Table~\ref{tab:setup} of Appendix~\ref{app:setup}.

\noindent{{\bf {\em FL simulation environment.}}}
To simulate a realistic FL environment, we integrate FLANDERS into Flower.\footnote{\scriptsize{\url{https://flower.dev/}}} Other valid FL frameworks are available (e.g., TensorFlow Federated\footnote{\scriptsize{\url{https://www.tensorflow.org/federated}}} and PySyft\footnote{\scriptsize{\url{https://github.com/OpenMined/PySyft}}}), but Flower was the most appropriate choice for our scope.
Flower allows us to specify the number $K$ of FL clients, which in this work we set to $100$. 
Moreover, we assume that the server selects {\em all} these clients in every FL round; in other words, $K = m$. For the first $L=30$ rounds, no attack occurs; then, we monitor the performance of the global model for additional $20$ rounds, i.e., until $T=50$, while malicious clients possibly enter the FL system.
\\
A complete description of our FL simulation environment is provided in Appendix~\ref{app:fl-env}.

We run our experiments on a 4-node cluster; each node has a 48-core 64-bit AMD CPU running at 3.6 GHz with 512 GB RAM and an NVIDIA A100 GPU with 40 GB VRAM.

\subsection{Attacks}
\label{subsec:attacks}
We assess the robustness of FLANDERS under the following well-known attacks.
Moreover, for each type of attack, we vary the number $b = \lceil r * m\rceil, r\in [0,1]$ of Byzantine clients by experimenting with several values of $r = \{0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95\}$.

\noindent{{\bf {\em Gaussian Noise Attack.}}} 
\noindent This attack randomly crafts the local models on the compromised clients. 
Specifically, the attacker samples a random value from a Gaussian distribution $\varepsilon \sim \mathcal{N}(0,\sigma^2)$ and sums it to the all the $d$ learned parameters. 
We refer to this attack as GAUSS.

\noindent{{\bf {\em ``A Little Is Enough'' Attack}}~\cite{baruch2019neurips}{\bf.}}
This attack lets the defender remove the non-Byzantine clients and shift the aggregation gradient by carefully crafting malicious values that deviate from the correct ones as far as possible. 
We call this attack LIE.

\noindent{{\bf {\em Optimization-based Attack}}~\cite{fang2020usenix}{\bf.}}
The authors formulate this attack as an optimization task, aiming to maximize the distance between the poisoned aggregated gradient and the aggregated gradient under no attack. 
By leveraging halving search, they obtain a crafted malicious gradient. 
We refer to this attack as OPT.

\noindent{{\bf {\em AGR Attack Series}}~\cite{shejwalkar2021ndss}{\bf.}}
This improves the optimization program above by introducing perturbation vectors and scaling factors. Then, three instances are proposed: AGR-tailored, AGR-agnostic Min-Max, and Min-Sum, which maximize the deviation between benign and malicious gradients.
In this work, we experiment with AGR Min-Max, which we call AGR-MM.

We detail the parameters of the attacks above in Appendix~\ref{app:attacks}.

\subsection{Evaluation}
\label{subsec:eval}

%%% UPDATED FROM THE VERSION ABOVE
We compare the robustness of FLANDERS against plain ``vanilla'' FedAvg and all the baselines already described in Section~\ref{sec:related}: Trimmed Mean, FedMedian, Krum, Bulyan, FLTrust, and MSCRED. 
For a fair comparison, we set the number of trusted clients to keep at each round to $k = 1$.
We discuss all the other critical parameters of these defense strategies in Appendix~\ref{app:defenses}.
\\
For each classification task, we measure the best global model's accuracy obtained with all the FL aggregation strategies after the $L$-th round (i.e., after the attack).
We operate similarly for the regression task associated with the {\em California Housing} dataset, measuring the global model's one minus mean absolute percentage error (1-MAPE).

\noindent{{\bf {\em Robustness.}}} We show the result of our comparison in Fig.~\ref{fig:robustness}. 
Each row on this figure represents a dataset/task, whereas each column corresponds to a specific attack. 
Moreover, each plot depicts how the maximum accuracy (1-MAPE for the regression task) of the global model changes as the power of the attack (i.e., the percentage of Byzantine clients in the FL system) increases.
\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{./img/robustness}
    \caption{Comparing the robustness of FLANDERS against all the baselines on four datasets/tasks (rows) under four attacks (columns).}
    \label{fig:robustness}
\end{figure*}
To comment on this result, we can distinguish between two cases: when the attack strength is low/moderate or when this is severe.
In the former case, i.e., from $10\%$ up to $40\%$ of malicious clients, FLANDERS behaves similarly to its best-performing competitors. 
In fact, except for {\em CIFAR-10}, where FLANDERS performs slightly worse than other robust baselines, for all the other datasets/tasks, FLANDERS is either on par with (see {\em Income} and {\em MNIST}) or even constantly better (see {\em California Housing}) than baseline approaches.
Instead, when the ratio of Byzantine clients gets above $40\%$, FLANDERS outperforms all the competitors. 
More generally, the robustness of FLANDERS is way more steady than that of other methods across the whole spectrum of the attack power. 
In addition, it is worth remarking that some approaches, like Krum and Bulyan, are not even designed to work under such severe attack scenarios (i.e., with more than $50\%$ and $25\%$ malicious clients, respectively).
We further discuss the stability of the performance exhibited by FLANDERS in Appendix~\ref{app:stability}.

\noindent{{\bf {\em Robustness vs. Cost.}}} Evaluating the robustness is undoubtedly pivotal. 
However, any robust FL aggregation strategy may sacrifice the global model's performance when no attack occurs. In other words, there is a price to pay in exchange for the level of security guaranteed. 
Thus, we introduce another evaluation metric that captures the robustness vs. cost trade-off. 
Our experiments show that FLANDERS achieves the best balance between robustness and cost under every attack when the ratio of corrupted clients is extremely severe (i.e., $80\%$).
Due to space limitations, we report the full results of this analysis in Appendix~\ref{app:rob-vs-cost}.

\subsection{FLANDERS with Non-IID Local Training Data}
A peculiarity of FL is that the local training datasets on different clients may not be independent and identically distributed (non-i.i.d.)~\cite{mcmahan2017aistats}).
Hence, we also validate FLANDERS in this setting.
Specifically, we simulate non-i.i.d. local training
data of the {\em CIFAR-10} dataset using the built-in capabilities provided by Flower.
Indeed, Flower includes a dataset partitioning method based on Dirichlet sampling to simulate non-i.i.d. label distribution across federated clients as proposed by~\cite{hsu2019LDA}.
This technique uses a hyperparameter $\alpha_D$ to control client label identicalness. 
In this work, we set $\alpha_D = 0.5$ to simulate a strong class label imbalance amongst clients.
\\
We analyze the impact of non-i.i.d. training data on FLANDERS by measuring how the global model's accuracy changes with the number of FL rounds, both under attack and with no malicious clients. In the first case, we consider all the attacks described in Section~\ref{subsec:attacks} with $20\%$ of Byzantine clients, and we vary the number of clients deemed to be legitimate at each round as $k\in \{1, 5, 20\}$.
Due to space limitations, results are presented in Appendix~\ref{app:non-iid}.

% CONCLUSION & FUTURE WORK
\section{Conclusion and Future Work}
\label{sec:conclusion}
We have introduced FLANDERS, a novel federated learning (FL) aggregation scheme robust to Byzantine attacks. 
FLANDERS treats the local model updates sent by clients at each FL round as a matrix-valued time series. 
Then, it identifies malicious clients as outliers of this time series by computing an anomaly score that compares actual observations with those estimated by a matrix autoregressive forecasting model. 
Experiments run on several datasets under different FL settings have demonstrated that FLANDERS matches the robustness of the most powerful baselines against Byzantine clients. 
Furthermore, FLANDERS remains highly effective even under extremely severe attacks (i.e., beyond $50\%$ malicious clients), as opposed to existing defense strategies that either fail or are not even designed to handle such settings as Krum and Bulyan.
\\
In the future, we will address some limitations of this work. 
First, we expect to reduce the computational cost of FLANDERS further, especially for complex models with millions of parameters, leveraging the implementation tricks discussed in Appendix~\ref{app:defenses}. 
Then, we plan to extend FLANDERS using more expressive MAR($p$) models ($p > 1$) and conduct a deeper analysis of its key parameters (e.g., $w$, $k$, and $\delta$).

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% probably should) include acknowledgements. In this case, please
% place such acknowledgements in an unnumbered section at the
% end of the paper. Typically, this will include thanks to reviewers
% who gave useful comments, to colleagues who contributed to the ideas,
% and to funding agencies and corporate sponsors that provided financial
% support.


%
% ---- Bibliography ----
%
%%
% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
%\balance
\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[inc(1996)]{income-ds}
{Adult Income Dataset}.
\newblock [Online]. Available from:
  \url{https://archive.ics.uci.edu/ml/datasets/adult}, 1996.

\bibitem[cal(1997)]{california-housing-ds}
{California Housing Prices Dataset}.
\newblock [Online]. Available from:
  \url{https://www.kaggle.com/datasets/camnugent/california-housing-prices},
  1997.

\bibitem[mni(1998)]{mnist-ds}
{MNIST Dataset}.
\newblock [Online]. Available from: \url{http://yann.lecun.com/exdb/mnist/},
  1998.

\bibitem[cif(2009)]{cifar10-ds}
{CIFAR-10 Dataset}.
\newblock [Online]. Available from:
  \url{https://www.cs.toronto.edu/~kriz/cifar.html}, 2009.

\bibitem[Aggarwal et~al.(2001)Aggarwal, Hinneburg, and Keim]{aggarwal2001icdt}
Aggarwal, C.~C., Hinneburg, A., and Keim, D.~A.
\newblock {On the Surprising Behavior of Distance Metrics in High Dimensional
  Space}.
\newblock In \emph{Proc. of ICDT'01}, pp.\  420--434. Springer, 2001.

\bibitem[Baruch et~al.(2019)Baruch, Baruch, and Goldberg]{baruch2019neurips}
Baruch, G., Baruch, M., and Goldberg, Y.
\newblock {A Little Is Enough: Circumventing Defenses For Distributed
  Learning}.
\newblock In \emph{NeurIPS'19}, pp.\  8632--8642, 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/hash/ec1c59141046cd1866bbbcdfb6ae31d4-Abstract.html}.

\bibitem[Basu \& Michailidis(2015)Basu and Michailidis]{basu2015annalsstats}
Basu, S. and Michailidis, G.
\newblock {Regularized Estimation in Sparse High-Dimensional Time Series
  Models}.
\newblock \emph{The Annals of Statistics}, 43\penalty0 (4):\penalty0
  1535--1567, 2015.
\newblock URL \url{https://doi.org/10.1214/15-AOS1315}.

\bibitem[Bhagoji et~al.(2019)Bhagoji, Chakraborty, Mittal, and
  Calo]{bhagoji2019pmlr}
Bhagoji, A.~N., Chakraborty, S., Mittal, P., and Calo, S.
\newblock {Analyzing Federated Learning through an Adversarial Lens}.
\newblock In \emph{ICML'19}, pp.\  634--643. PMLR Press, 2019.

\bibitem[Biggio et~al.(2012)Biggio, Nelson, and Laskov]{biggio2012icml}
Biggio, B., Nelson, B., and Laskov, P.
\newblock {Poisoning Attacks against Support Vector Machines}.
\newblock In \emph{ICML'12}, pp.\  1467--1474. Omnipress, 2012.
\newblock URL \url{http://icml.cc/2012/papers/880.pdf}.

\bibitem[Biggio et~al.(2013)Biggio, Didaci, Fumera, and Roli]{biggio2013icb}
Biggio, B., Didaci, L., Fumera, G., and Roli, F.
\newblock {Poisoning Attacks to Compromise Face Templates}.
\newblock In \emph{ICB'13}, pp.\  1--7. {IEEE}, 2013.
\newblock URL \url{https://doi.org/10.1109/ICB.2013.6613006}.

\bibitem[Blanchard et~al.(2017)Blanchard, El~Mhamdi, Guerraoui, and
  Stainer]{blanchard2017nips}
Blanchard, P., El~Mhamdi, E.~M., Guerraoui, R., and Stainer, J.
\newblock {Machine Learning with Adversaries: Byzantine Tolerant Gradient
  Descent}.
\newblock In \emph{NeurIPS'17}, pp.\  118--128. Curran Associates Inc., 2017.

\bibitem[Cao et~al.(2020)Cao, Fang, Liu, and Gong]{cao2020fltrust}
Cao, X., Fang, M., Liu, J., and Gong, N.~Z.
\newblock {FLTrust: Byzantine-Robust Federated Learning via Trust
  Bootstrapping}.
\newblock \emph{arXiv preprint arXiv:2012.13995}, abs/2012.13995, 2020.

\bibitem[Chalapathy \& Chawla(2019)Chalapathy and Chawla]{chalapathy2019arxiv}
Chalapathy, R. and Chawla, S.
\newblock {Deep Learning for Anomaly Detection: {A} Survey}.
\newblock \emph{CoRR}, abs/1901.03407, 2019.

\bibitem[Chandola et~al.(2009)Chandola, Banerjee, and Kumar]{chandola2009acm}
Chandola, V., Banerjee, A., and Kumar, V.
\newblock {Anomaly Detection: A Survey}.
\newblock \emph{ACM computing surveys (CSUR)}, 41\penalty0 (3):\penalty0 1--58,
  2009.

\bibitem[Chen et~al.(2021)Chen, Xiao, and Yang]{chen2021je}
Chen, R., Xiao, H., and Yang, D.
\newblock {Autoregressive Models for Matrix-Valued Time Series}.
\newblock \emph{Journal of Econometrics}, 222\penalty0 (1):\penalty0 539--560,
  2021.

\bibitem[Costa et~al.(2022)Costa, Pinelli, Soderi, and
  Tolomei]{costa2022covert}
Costa, G., Pinelli, F., Soderi, S., and Tolomei, G.
\newblock {Turning Federated Learning Systems into Covert Channels}.
\newblock \emph{IEEE Access}, 10:\penalty0 130642--130656, 2022.
\newblock \doi{10.1109/ACCESS.2022.3229124}.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Ranzato,
  Senior, Tucker, Yang, et~al.]{dean2012nips}
Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Ranzato, M.,
  Senior, A., Tucker, P., Yang, K., et~al.
\newblock {Large Scale Distributed Deep Networks}.
\newblock In \emph{NeurIPS'12}, pp.\  1223--1231, 2012.

\bibitem[Douceur(2002)]{douceur2002iptps}
Douceur, J.~R.
\newblock {The Sybil Attack}.
\newblock In \emph{IPTPS'02}, volume 2429 of \emph{LNCS}, pp.\  251--260.
  Springer, 2002.
\newblock URL \url{https://doi.org/10.1007/3-540-45748-8\_24}.

\bibitem[Fang et~al.(2020)Fang, Cao, Jia, and Gong]{fang2020usenix}
Fang, M., Cao, X., Jia, J., and Gong, N.
\newblock {Local Model Poisoning Attacks to Byzantine-Robust Federated
  Learning}.
\newblock In \emph{USENIX'20}, pp.\  1605--1622. USENIX Association, 2020.

\bibitem[Hsu et~al.(2019)Hsu, Qi, and Brown]{hsu2019LDA}
Hsu, T.-M.~H., Qi, H., and Brown, M.
\newblock {Measuring the Effects of Non-Identical Data Distribution for
  Federated Visual Classification}, 2019.
\newblock URL \url{https://arxiv.org/abs/1909.06335}.

\bibitem[Hu et~al.(2021)Hu, Lu, Wan, and Zhang]{hu2021arxiv}
Hu, S., Lu, J., Wan, W., and Zhang, L.~Y.
\newblock {Challenges and Approaches for Mitigating Byzantine Attacks in
  Federated Learning}.
\newblock \emph{CoRR}, abs/2112.14468, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.14468}.

\bibitem[Jagielski et~al.(2018)Jagielski, Oprea, Biggio, Liu, Nita-Rotaru, and
  Li]{jagielski2018sp}
Jagielski, M., Oprea, A., Biggio, B., Liu, C., Nita-Rotaru, C., and Li, B.
\newblock {Manipulating Machine Learning: Poisoning Attacks and Countermeasures
  for Regression Learning}.
\newblock In \emph{IEEE SP'18}, pp.\  19--35. IEEE, 2018.
\newblock \doi{10.1109/SP.2018.00057}.

\bibitem[Kone{\v{c}}n{\'y} et~al.(2016)Kone{\v{c}}n{\'y}, McMahan, Yu,
  Richt{\'{a}}rik, Suresh, and Bacon]{konecny2016nipsws}
Kone{\v{c}}n{\'y}, J., McMahan, H.~B., Yu, F.~X., Richt{\'{a}}rik, P., Suresh,
  A.~T., and Bacon, D.
\newblock {Federated Learning: Strategies for Improving Communication
  Efficiency}.
\newblock \emph{CoRR}, abs/1610.05492, 2016.
\newblock URL \url{http://arxiv.org/abs/1610.05492}.

\bibitem[Koren et~al.(2009)Koren, Bell, and Volinsky]{koren2009ieeecomp}
Koren, Y., Bell, R., and Volinsky, C.
\newblock {Matrix Factorization Techniques for Recommender Systems}.
\newblock \emph{Computer}, 42\penalty0 (8):\penalty0 30--37, Aug 2009.
\newblock URL \url{https://doi.org/10.1109/MC.2009.263}.

\bibitem[Li et~al.(2016)Li, Wang, Singh, and Vorobeychik]{li2016nips}
Li, B., Wang, Y., Singh, A., and Vorobeychik, Y.
\newblock {Data Poisoning Attacks on Factorization-Based Collaborative
  Filtering}.
\newblock In \emph{NeurIPS'16}, pp.\  1885--1893. Curran Associates, Inc.,
  2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2016/hash/83fa5a432ae55c253d0e60dbfa716723-Abstract.html}.

\bibitem[Lu \& Fan(2020)Lu and Fan]{lu2020spml}
Lu, Y. and Fan, L.
\newblock {An Efficient and Robust Aggregation Algorithm for Learning Federated
  CNN}.
\newblock In \emph{SPML'20}, SPML 2020, pp.\  1--7. ACM, 2020.

\bibitem[Mahalakshmi et~al.(2016)Mahalakshmi, Sridevi, and
  Rajaram]{mahalakshmi2016icctide}
Mahalakshmi, G., Sridevi, S., and Rajaram, S.
\newblock {A Survey on Forecasting of Time Series Data}.
\newblock In \emph{ICCTIDE'16}, pp.\  1--8, 2016.
\newblock \doi{10.1109/ICCTIDE.2016.7725358}.

\bibitem[McMahan \& Ramage(2017)McMahan and Ramage]{mcmahan2017googleai}
McMahan, B. and Ramage, D.
\newblock {Federated Learning: Collaborative Machine Learning without
  Centralized Training Data}.
\newblock \emph{Google Research Blog}, 3, 2017.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  Arcas]{mcmahan2017aistats}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and Arcas, B. A.~y.
\newblock {Communication-Efficient Learning of Deep Networks from Decentralized
  Data}.
\newblock In \emph{AISTATS'17}, volume~54, pp.\  1273--1282. PMLR, 2017.

\bibitem[Mhamdi et~al.(2018)Mhamdi, Guerraoui, and Rouault]{mhamdi2018pmlr}
Mhamdi, E. M.~E., Guerraoui, R., and Rouault, S.
\newblock {The Hidden Vulnerability of Distributed Learning in Byzantium}.
\newblock In Dy, J.~G. and Krause, A. (eds.), \emph{ICML'18}, volume~80, pp.\
  3518--3527. {PMLR}, 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/mhamdi18a.html}.

\bibitem[Reddi et~al.(2021)Reddi, Charles, Zaheer, Garrett, Rush,
  Kone{\v{c}}n{\'y}, Kumar, and McMahan]{reddi2021iclr}
Reddi, S.~J., Charles, Z., Zaheer, M., Garrett, Z., Rush, K.,
  Kone{\v{c}}n{\'y}, J., Kumar, S., and McMahan, H.~B.
\newblock {Adaptive Federated Optimization}.
\newblock In \emph{{ICLR}'21}. OpenReview.net, 2021.

\bibitem[Rodrguez-Barroso et~al.(2023)Rodrguez-Barroso, Jimnez-Lpez,
  Luzn, Herrera, and Martnez-Cmara]{barroso2023if}
Rodrguez-Barroso, N., Jimnez-Lpez, D., Luzn, M.~V., Herrera, F., and
  Martnez-Cmara, E.
\newblock {Survey on Federated Learning Threats: Concepts, Taxonomy on Attacks
  and Defences, Experimental Study and Challenges}.
\newblock \emph{Information Fusion}, 90:\penalty0 148--173, 2023.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S1566253522001439}.

\bibitem[Rubinstein et~al.(2009)Rubinstein, Nelson, Huang, Joseph, Lau, Rao,
  Taft, and Tygar]{rubinstein2009imc}
Rubinstein, B. I.~P., Nelson, B., Huang, L., Joseph, A.~D., Lau, S., Rao, S.,
  Taft, N., and Tygar, J.~D.
\newblock {ANTIDOTE: Understanding and Defending against Poisoning of Anomaly
  Detectors}.
\newblock In \emph{IMC'09}, pp.\  1--14. {ACM}, 2009.
\newblock URL \url{https://doi.org/10.1145/1644893.1644895}.

\bibitem[Schmidl et~al.(2022)Schmidl, Wenig, and Papenbrock]{schmidl2022pvld}
Schmidl, S., Wenig, P., and Papenbrock, T.
\newblock {Anomaly Detection in Time Series: A Comprehensive Evaluation}.
\newblock \emph{Proceedings of the VLDB Endowment}, 15\penalty0 (9):\penalty0
  1779--1797, Jul 2022.
\newblock URL \url{https://doi.org/10.14778/3538598.3538602}.

\bibitem[Shejwalkar \& Houmansadr(2021)Shejwalkar and
  Houmansadr]{shejwalkar2021ndss}
Shejwalkar, V. and Houmansadr, A.
\newblock {Manipulating the Byzantine: Optimizing Model Poisoning Attacks and
  Defenses for Federated Learning}.
\newblock In \emph{NDSS'21}, 2021.

\bibitem[Shen et~al.(2016)Shen, Tople, and Saxena]{shen2016acm}
Shen, S., Tople, S., and Saxena, P.
\newblock {Auror: Defending Against Poisoning Attacks in Collaborative Deep
  Learning Systems}.
\newblock In \emph{ACSAC'16}, pp.\  508--519. ACM, 2016.

\bibitem[Wang et~al.(2020)Wang, Liu, Liang, Joshi, and Poor]{wang2020neurips}
Wang, J., Liu, Q., Liang, H., Joshi, G., and Poor, H.~V.
\newblock Tackling the objective inconsistency problem in heterogeneous
  federated optimization.
\newblock In \emph{NeurIPS'20}, pp.\  7611--7623. Curran Associates Inc., 2020.
\newblock ISBN 9781713829546.

\bibitem[Xiao et~al.(2015)Xiao, Biggio, Brown, Fumera, Eckert, and
  Roli]{xiao2015icml}
Xiao, H., Biggio, B., Brown, G., Fumera, G., Eckert, C., and Roli, F.
\newblock {Is Feature Selection Secure against Training Data Poisoning?}
\newblock In \emph{ICML'15}, volume~37 of \emph{{JMLR} Workshop and Conference
  Proceedings}, pp.\  1689--1698. JMLR.org, 2015.
\newblock URL \url{http://proceedings.mlr.press/v37/xiao15.html}.

\bibitem[Xie et~al.(2018)Xie, Koyejo, and Gupta]{xie2018corr}
Xie, C., Koyejo, O., and Gupta, I.
\newblock {Generalized Byzantine-Tolerant SGD}.
\newblock \emph{CoRR}, abs/1802.10116, 2018.
\newblock URL \url{http://arxiv.org/abs/1802.10116}.

\bibitem[Yang et~al.(2017)Yang, Gong, and Cai]{yang2017ndss}
Yang, G., Gong, N.~Z., and Cai, Y.
\newblock {Fake Co-visitation Injection Attacks to Recommender Systems}.
\newblock In \emph{NDSS'17}. The Internet Society, 2017.
\newblock URL
  \url{https://www.ndss-symposium.org/ndss2017/ndss-2017-programme/fake-co-visitation-injection-attacks-recommender-systems/}.

\bibitem[Yin et~al.(2018)Yin, Chen, Kannan, and Bartlett]{yin2018icml}
Yin, D., Chen, Y., Kannan, R., and Bartlett, P.
\newblock {Byzantine-Robust Distributed Learning: Towards Optimal Statistical
  Rates}.
\newblock In \emph{ICML'18}, volume~80, pp.\  5650--5659. PMLR, 10--15 Jul
  2018.
\newblock URL \url{https://proceedings.mlr.press/v80/yin18a.html}.

\bibitem[Zhang et~al.(2019)Zhang, Song, Chen, Feng, Lumezanu, Cheng, Ni, Zong,
  Chen, and Chawla]{zhang2019aaai}
Zhang, C., Song, D., Chen, Y., Feng, X., Lumezanu, C., Cheng, W., Ni, J., Zong,
  B., Chen, H., and Chawla, N.~V.
\newblock {A Deep Neural Network for Unsupervised Anomaly Detection and
  Diagnosis in Multivariate Time Series Data}.
\newblock In \emph{AAAI'19}, volume~33, pp.\  1409--1416. AAAI Press, 2019.

\end{thebibliography}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
%%% IMPACT OF BYZANTINE CLIENTS
\section{The Impact of Byzantine Clients at each FL Round}
Under our assumptions, the FL system contains a total of $K$ clients, where $b$ of those are Byzantine controlled by an attacker ($0\leq b\leq K$).
In addition, at each FL round, $m$ clients ($1 \leq m \leq K$) are selected, and thus, some of the $m$ model updates received by the central server may be corrupted.
The probability of this event can actually be computed by noticing that the outcome of the client selection at each round can be represented by a random variable $X\sim \text{Hypergeometric}(K, b, m)$, whose probability mass function is:
\[p_X(x) = \Pr(X=x) = \frac{\binom{b}{x} \binom{K-b}{m-x}}{\binom{K}{m}}.
\]
The chance that, at a single round, {\em at least one} of the $b$ malicious clients ends up in the list of $m$ clients equals:
\[\Pr(X\geq 1) = 1 - \Pr(X=0) = 1 - \frac{\binom{b}{0}\binom{K-b}{m-0}}{\binom{K}{m}}= 1 - \frac{ \binom{K-b}{m}}{\binom{K}{m}}.
\]
For example, if the total number of clients is $K=100$, $b=5$ of them are malicious, and $m=20$ must be drawn at each round then $\Pr(X\geq 1) \approx 68\%$. 
In other words, there are about two out of three chances that at least one malicious client is picked at {\em every} FL round.

%%% MAR
\section{Matrix Autoregressive Model (MAR)}
\label{app:mar}
In this work, we consider the simplest MAR($1$) forecasting model, which exhibits a bilinear structure as follows:
\[
{\bm X}_{t} = {\bm A}{\bm X}_{t-1}{\bm B} + {\bm E}_t.
\]
Thus, the key question here is how to estimate the best coefficient matrices ${\bm A}$ and ${\bm B}$.
To achieve this goal, we first let $\hat{\bm{X}}_t = \hat{{\bm A}} {\bm{X}}_{t-1}\hat{{\bm B}}$ be the {\em estimated} matrix of observations at time $t$, according to MAR for some coefficient matrices $\hat{{\bm A}}$ and $\hat{{\bm B}}$.
Hence, we can define an instance-level loss function as follows:
\begin{equation}
\loss(\hat{{\bm A}},\hat{{\bm B}}; {\bm X}_{t}) = ||{\bm X}_{t} - \hat{{\bm X}}_{t}||^2_{\text F} = ||{\bm X}_{t} - \hat{{\bm A}}{\bm X}_{t-1}\hat{{\bm B}}||^2_{\text F},
\label{eq:app-inst-loss}
\end{equation}
where $||\cdot||_{\text F}$ indicates the Frobenius norm of a matrix.
More generally, if we have access to $w > 0$ historical matrix observations, we can compute the overall loss function below:
\begin{equation}
\Loss(\hat{{\bm A}},\hat{{\bm B}}; {\bm X}_{t},w) = \sum_{j=0}^{w-1} \loss(\hat{{\bm A}},\hat{{\bm B}}; {\bm X}_{t-j}).
\label{eq:app-loss}
\end{equation}
As we assume ${\bm E}_t$ is a white noise matrix, its entries are i.i.d. normal with zero-mean and constant variance.
Eventually, ${\bm A}$ and ${\bm B}$ can be found as the solutions of the following objective:
\[
    {\bm A}, {\bm B} = \argmin_{\hat{{\bm A}}, \hat{{\bm B}}}\Big\{\Loss(\hat{{\bm A}},\hat{{\bm B}}; {\bm X}_{t},w) \Big\} = \argmin_{\hat{{\bm A}}, \hat{{\bm B}}}\Big\{\sum_{j=0}^{w-1} ||{\bm X}_{t-j} - \hat{{\bm A}}{\bm X}_{t-j-1}\hat{{\bm B}}||^2_{\text F} \Big\}.
    \addtag \label{eq:app-mar-opt}
\]
To solve the optimization task defined in Eq.~(\ref{eq:app-mar-opt}) above, with a slight abuse of notation, we may observe the following.
A closed-form solution to find ${\bm A}$ can be computed by taking the partial derivative of the loss w.r.t. ${\bm A}$, setting it to $0$, and solving it for ${\bm A}$. In other words, we search for ${\bm A}$ such that:
\[
\frac{\partial{\Loss({\bm A},{\bm B}; {\bm X}_{t},w)}}{\partial{{\bm A}}} = 0.
\addtag \label{eq:app-partial-a}
\]
Using Eq.~(\ref{eq:app-loss}) and Eq.~(\ref{eq:app-inst-loss}), the left-hand side of Eq.~(\ref{eq:app-partial-a}) can be rewritten as follows:
\begin{align}
\frac{\partial{\Loss({\bm A},{\bm B}; {\bm X}_{t},w)}}{\partial{{\bm A}}} & = \frac{\partial{\sum_{j=0}^{w-1} ||{\bm X}_{t-j} - {\bm A}{\bm X}_{t-j-1}{\bm B}||^2_{\text F}}}{\partial{{\bm A}}}=\nonumber\\
& = -2 \sum_{j=0}^{w-1}({\bm X}_{t-j} - {\bm A}{\bm X}_{t-j-1}{\bm B}^T){\bm B}{\bm X}^T_{t-1}=\nonumber\\
& = -2 \Bigg[\Bigg(\sum_{j=0}^{w-1} {\bm X}_{t-j}{\bm B}{\bm X}^T_{t-j-1}\Bigg) - {\bm A}\Bigg(\sum_{j=0}^{w-1}{\bm X}_{t-j-1}{\bm B}^T{\bm B}{\bm X}^T_{t-j-1}\Bigg)\Bigg].
\label{eq:app-partial}
\end{align}
If we set Eq.~(\ref{eq:app-partial}) to $0$ and solve it for ${\bm A}$, we find:
\[
-2 \Bigg[\Bigg(\sum_{j=0}^{w-1} {\bm X}_{t-j}{\bm B}{\bm X}^T_{t-j-1}\Bigg) - {\bm A}\Bigg(\sum_{j=0}^{w-1}{\bm X}_{t-j-1}{\bm B}^T{\bm B}{\bm X}^T_{t-j-1}\Bigg)\Bigg] = 0 \iff
\]
\[
\Bigg(\sum_{j=0}^{w-1} {\bm X}_{t-j}{\bm B}{\bm X}^T_{t-j-1}\Bigg) - {\bm A}\Bigg(\sum_{j=0}^{w-1}{\bm X}_{t-j-1}{\bm B}^T{\bm B}{\bm X}^T_{t-j-1}\Bigg) = 0.
\]
Hence:
\begin{align}
\Bigg(\sum_{j=0}^{w-1} {\bm X}_{t-j}{\bm B}{\bm X}^T_{t-j-1}\Bigg) - {\bm A}\Bigg(\sum_{j=0}^{w-1}{\bm X}_{t-j-1}{\bm B}^T{\bm B}{\bm X}^T_{t-j-1}\Bigg) &= 0\\
\Bigg(\sum_{j=0}^{w-1} {\bm X}_{t-j}{\bm B}{\bm X}^T_{t-j-1}\Bigg) = {\bm A}\Bigg(\sum_{j=0}^{w-1}{\bm X}_{t-j-1}{\bm B}^T{\bm B}{\bm X}^T_{t-j-1}\Bigg) \label{eq:app-A-imp}\\
{\bm A} = \Bigg(\sum_{j=0}^{w-1} {\bm X}_{t-j}{\bm B}{\bm X}^T_{t-j-1}\Bigg)\Bigg(\sum_{j=0}^{w-1}{\bm X}_{t-j-1}{\bm B}^T{\bm B}{\bm X}^T_{t-j-1}\Bigg)^{-1}.
\label{eq:app-A}
\end{align}
Notice that Eq.~(\ref{eq:app-A}) is obtained by multiplying both sides of Eq.~(\ref{eq:app-A-imp}) by $\Big(\sum_{j=0}^{w-1}{\bm X}_{t-j-1}{\bm B}^T{\bm B}{\bm X}^T_{t-j-1}\Big)^{-1}$.

If we apply the same reasoning, we can also find a closed-form solution to compute ${\bm B}$. That is, we take the partial derivative of the loss w.r.t. ${\bm B}$, set it to $0$, and solve it for ${\bm B}$:
\[
\frac{\partial{\Loss({\bm A},{\bm B}; {\bm X}_{t},w)}}{\partial{{\bm B}}} = 0.
\addtag \label{eq:app-partial-b}
\]
Eventually, we obtain the following:
\[
{\bm B} = \Bigg(\sum_{j=0}^{w-1} {\bm X}^T_{t-j}{\bm A}{\bm X}_{t-j-1}\Bigg)\Bigg(\sum_{j=0}^{w-1}{\bm X}^T_{t-j-1}{\bm A}^T{\bm A}{\bm X}_{t-j-1}\Bigg)^{-1}.
\addtag \label{eq:app-B}
\]
We now have two closed-form solutions; one for ${\bm A}$ (see Eq.~(\ref{eq:app-A})) and one for ${\bm B}$ (see Eq.~(\ref{eq:app-B})).
However, the solution to ${\bm A}$ involves ${\bm B}$, and the solution to ${\bm B}$ involves ${\bm A}$. In other words, we must know ${\bm B}$ to compute ${\bm A}$ and vice versa.

We can use the standard Alternating Least Squares (ALS) algorithm~\cite{koren2009ieeecomp} to solve such a problem. 
The fundamental idea of ALS is to iteratively update the least squares closed-form solution of each variable alternately, keeping fixed the other. At the generic $i$-th iteration, we compute:
\begin{align}
{\bm A}^{(i+1)} = \Bigg(\sum_{j=0}^{w-1} {\bm X}_{t-j}{\bm B}^{(i)}{\bm X}^T_{t-j-1}\Bigg)\Bigg(\sum_{j=0}^{w-1}{\bm X}_{t-j-1}({\bm B}^{(i)})^T{\bm B}^{(i)}{\bm X}^T_{t-j-1}\Bigg)^{-1};\nonumber\\
{\bm B}^{(i+1)} = \Bigg(\sum_{j=0}^{w-1} {\bm X}^T_{t-j}{\bm A}^{(i+1)}{\bm X}_{t-j-1}\Bigg)\Bigg(\sum_{j=0}^{w-1}{\bm X}^T_{t-j-1}({\bm A}^{(i+1)})^T{\bm A}^{(i+1)}{\bm X}_{t-j-1}\Bigg)^{-1};\nonumber
\end{align}
ALS repeats the two steps above until some convergence criterion is met, e.g., after a specific number of iterations $N$ or when the distance between the values of the variables computed in two consecutive iterations is smaller than a given positive threshold, i.e., $d({\bm A}^{(i+1)}-{\bm A}^{(i)}) < \varepsilon$ and $d({\bm B}^{(i+1)}-{\bm B}^{(i)}) < \varepsilon$, where $d(\cdot)$ is any suitable matrix distance function and $\varepsilon \in \R_{>0}$.

%%% SETUP
\section{Datasets, Tasks, and FL Models}
\label{app:setup}

In Table~\ref{tab:setup}, we report the full details of our experimental setup concerning the datasets used, their associated tasks, and the models (along with their hyperparameters) trained on the simulated FL environment. 
The description of the hyperparameters is shown in Table~\ref{tab:hyp}.

\begin{table*}[ht]
\centering
\caption{\label{tab:setup}Experimental setup: datasets, tasks, and FL models considered.
}
\vspace{2mm}
\scalebox{0.92}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Dataset}        & \thead{\textbf{N. of Instances}\\\textbf{(training/test)}} & \textbf{N. of Features} & \textbf{Task} & \textbf{FL Model} & \textbf{Hyperparameters}\\ \hline
\thead{\textit{Income}\\(\citeyear{income-ds})}  & $39,074$/$9,768$                      & \thead{$14$\\(numerical+categorical)}          & \thead{binary\\classification} & LogReg & \{$\lambda_1$=$1.0$; $\lambda_2$=$0.0$; {\tt opt}=CCD\} \\ \hline 
\thead{\textit{MNIST}\\(\citeyear{mnist-ds})}       & $60,000$/$10,000$                      & \thead{$28$x$28$\\(numerical)}           & \thead{multiclass\\classification} & MLP & \thead{\{{\tt batch}=$32$; {\tt layers}=$3$;\\{\tt opt}=Adam; {\tt dropout}=$0.2$; \\$\eta$=$10^{-3}$\}}\\ \hline
\thead{\textit{CIFAR-10}\\(\citeyear{cifar10-ds})}          & $50,000$/$10,000$                      & \thead{$32$x$32$x$3$\\(numerical)}          & \thead{multiclass\\classification} & CNN & \thead{\{{\tt batch}=$32$; {\tt layers}=$6$;\\{\tt opt}=SGD; $\eta$=$10^{-2}$\}}\\ \hline
\thead{\textit{California Housing}\\(\citeyear{california-housing-ds})} & $15,740$/$3,935$                      & \thead{$18$\\(numerical)}              & regression & LinReg & \{$\lambda_1$=$0.5$; $\lambda_2$=$0.5$; {\tt opt}=CCD\}\\ \hline
\end{tabular}
}
\end{table*}

\begin{table}[ht!]
\centering
\caption{\label{tab:hyp}Description of hyperparameters.
}
\vspace{2mm}
\begin{tabular}{|c|c|}
\hline
\textbf{Hyperparameter}        & \textbf{Description} \\ \hline
$\lambda_1$ & $L^1$-regularization term (LASSO)\\ 
\hline
$\lambda_2$ & $L^2$-regularization term (Ridge)\\
\hline
$\eta$ & learning rate\\
\hline
{\tt opt} & optimizer: cyclic coordinate descent (CCD); stochastic gradient descent (SGD); Adam\\
\hline
{\tt batch} & batch size\\
\hline
{\tt layers} & number of neural network layers\\
\hline
{\tt dropout} & neural network node's dropout probability\\
\hline
\end{tabular}
\end{table}

LogReg and LinReg are optimized via cyclic coordinate descent (CCD) by minimizing $L^1$-regularized binary cross-entropy loss and $L^1$/$L^2$-regularized mean squared error, respectively.
MLP is a $3$-layer fully-connected feed-forward neural network trained by minimizing multiclass cross-entropy loss using Adam optimizer with batch size equal to $32$.
CNN is a $6$-layer convolutional neural network trained by minimizing multiclass cross-entropy loss via stochastic gradient descent (SGD) with batch size equal to $32$.

For LogReg and LinReg, at each FL round, every client performs one iteration of CCD (i.e., each model parameter is updated independently from the others).
Instead, for MLP and CNN, at each FL round, every client performs one training epoch of Adam/SGD, which corresponds to the number of iterations needed to ``see'' all the training instances once, when divided into batches of size $32$.
In any case, the updated local model is sent to the central server for aggregation.

We run our experiments on a 4-node cluster; each node has a 48-core 64-bit AMD CPU running at 3.6 GHz with 512 GB RAM and an NVIDIA A100 GPU with 40 GB VRAM.

%%% FL SIMULATION ENVIRONMENT
\section{FL Simulation Environment}
\label{app:fl-env}
\begin{table}[ht!]
\centering
\caption{\label{tab:fl-env}Main properties of our FL environment simulated on Flower.
}
\vspace{2mm}
\begin{tabular}{|l|c|}
\hline
\textbf{Total N. of Clients}        & $K = 100$ \\ \hline
\textbf{N. of Selected Clients (at each round)} & $m = K = 100$\\ \hline
\textbf{Ratio of Byzantine Clients} & $r=\{0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95\}$ \\ \hline
\textbf{Total N. of FL Rounds} & $T=50$ \\ \hline
\textbf{N. of Legitimate FL Rounds (with no attacks)} & $L=30$ \\ \hline
\textbf{Historical Window Size of Past FL Rounds} & $w=L=30$ \\ \hline
\textbf{Dataset Distribution across Clients} & i.i.d. for all datasets (in addition, non-i.i.d. only for {\em CIFAR-10})  \\ \hline
\end{tabular}
\end{table}

Each training portion of the datasets listed in Table~\ref{tab:setup} is distributed across the FL clients so that every client observes the same local data distribution. 
For example, in the case of {\em MNIST}, we let each client observe the same proportion of digit samples.
However, this setting -- known as i.i.d. -- may be an unrealistic scenario in FL systems, and, most likely, local training datasets on different clients follow different distributions, i.e., they are {\em non}-i.i.d.
Notice that there are many types of ``non-i.i.d.-ness'' that may involve: {\em(i) labels}, {\em (ii) number of instances}, and {\em (iii) features}. 
Concerning {\em(i)}, this occurs when the distribution of labels is very unbalanced across clients (i.e., each client observes only a subset of the possible outputs). 
As for {\em (ii)}, this happens when some clients have more data than others. 
Finally, {\em (iii)} represents a situation where similar feature patterns are grouped and shared amongst a few clients.
In this work, we experiment with the first type of non-i.i.d.-ness limitedly to the {\em CIFAR-10} dataset, which is already implemented in Flower (see Appendix~\ref{app:non-iid}).

We report the main properties of our FL environment simulated on Flower in Table~\ref{tab:fl-env}. 

%%% ATTACK SETTINGS
\section{Attack Settings}
\label{app:attacks}
Below, we describe the critical parameters for
each attack considered, which are also summarized in Table~\ref{tab:attacks}.

\noindent{{\bf {\em GAUSS}}{\bf.}} This attack has only one parameter: the magnitude $\sigma$ of the perturbation to apply. We set $\sigma=10$ for all the experiments.

\noindent{{\bf {\em LIE}}{\bf.}} This method has no parameters to set.

\noindent{{\bf {\em OPT}}{\bf.}} The parameter $\tau$ represents the minimum value that $\lambda$ can assume. Below this threshold, the halving search stops. As suggested by the authors, we set $\tau=10^{-5}$.

\noindent{{\bf {\em AGR-MM}}{\bf.}} In addition to the threshold $\tau$, AGR-MM uses the so-called perturbation vectors ($\nabla^p$) in combination with the scaling coefficient $\gamma$ to optimize. We set $\tau=10^{-5}$ and $\nabla^p = \{std\}$, which is the vector obtained by computing the parameters' inverse of the standard deviation. For Krum, we set $\nabla^p = \{uv\}$, which is the inverse unit vector perturbation.
\begin{table}[ht]
\centering
\caption{\label{tab:attacks}Key parameter settings for each attack strategy considered.
}
\vspace{2mm}
\begin{tabular}{|c|c|}
\hline
\textbf{Attack}        & \textbf{Parameters} \\ \hline
{\bf {\em GAUSS}} & $\sigma=10$\\ \hline
{\bf {\em LIE}} & N/A\\ \hline
{\bf {\em OPT}} & $\tau=10^{-5}$\\ \hline
{\bf {\em AGR-MM}} & $\tau=10^{-5}; \nabla^p = \{uv, std\}; \gamma=5$\\ \hline
\end{tabular}
\end{table}

%%% DEFENSE SETTINGS
\section{Defense Settings}
\label{app:defenses}
Below, we describe the critical parameters for
each baseline considered.

\noindent{{\bf {\em Trimmed Mean}}{\bf.}} 
The key parameter of this defense strategy is the number $k$, which is used to select the closest parameters to the median. In this work, we set $k = 1$ to be homogeneous with other baselines.

\noindent{{\bf {\em FedMedian}}{\bf.}}
This method has no parameters to set.

\noindent{{\bf {\em Krum}}{\bf.}} 
We set the number of Byzantine clients to $b = \lceil r' * m \rceil$, where $r'\subset r = \{0.05, 0.1, 0.2, 0.3, 0.4\}$, i.e., $b=\{5,10,20,30,40\}$.

\noindent{{\bf {\em Bulyan}}{\bf.}}
The two crucial parameters of this hybrid robust aggregation rule are $\alpha$ and $\beta$. The former determines the number of times Krum is applied to generate $\alpha$ local models; the latter is used to determine the number of parameters to select closer to the median.
In this work, we set $\alpha = m - 2*b, \beta = \alpha - 2*b$, where $b=\{5,10,20\}$.

\noindent{{\bf {\em FLTrust}}{\bf.}}
A key parameter is the so-called root dataset. In this work, we assume the server samples the root dataset from the union of the clients' clean local training data uniformly at random as in~\cite{cao2020fltrust}. We set the global learning rate to $\alpha=2\times10^{-3}$ for \emph{MNIST} and \emph{CIFAR-10} and $\alpha=1.0$ for \emph{Income} and \emph{California Housing}. Furthermore, the number of training epochs performed by the server is the same as that performed by the clients, i.e., $R_l=1$.

\noindent{{\bf {\em MSCRED}}{\bf.}}
Since MSCRED is designed to work with vector-valued rather than  matrix-valued time series, we need to adapt it to work in our setting. 
Specifically, we transform (i.e., flatten) our matrix observations into vectors and apply MSCRED by setting $w=1$. Note, however, that $w$ is the size of the window used to compute the signature matrices, meaning that the generic signature matrix is dependent on the previous one. In FLANDERS, instead, it indicates the number of previous steps to consider for estimating the coefficient matrices ${\bm A}$ and ${\bm B}$.

\noindent{{\bf {\em FLANDERS}}{\bf.}}
We set the window size $w=30$ and the number of clients to keep at every round $k=1$ for a fair comparison with other baselines. Furthermore, we use a sampling value $\widetilde{d}\leq d$ that indicates how many parameters we store in the history, the number $N$ of ALS iterations, and $\alpha$ and $\beta$ which are regularization factors. Random sampling is used to select the subset of parameters considered; this step is necessary when the model to train is too big, otherwise leading to extremely large tensors that would exhaust the server memory. On the other hand, the regularization factors are needed when the model parameters, the number of clients selected, or $\widetilde{d}$ cause numerical problems in the ALS algorithm, whose number of iterations is set to $N=100$. We always set $\alpha=1$ and $\beta=1$, meaning that there is no regularization since, in our experience, it reduces the capability of predicting the right model. For this reason, we set $\widetilde{d}=500$ when we train \textit{CIFAR-10} and \textit{MNIST}.
Finally, we set the distance function $\delta$ used to measure the difference between the observed vector of weights sent to the server and the predicted vector of weights output by MAR to squared $L^2$-norm.

Table~\ref{tab:defenses} summarizes the values of the key parameters discussed above and those characterizing our method FLANDERS.
\vspace{-8mm}
\begin{table}[h]
\centering
\caption{\label{tab:defenses}Key parameter settings for each defense strategy considered.
}
\vspace{2mm}
\begin{tabular}{|c|c|}
\hline
\textbf{Defense}        & \textbf{Parameters} \\ \hline
{\bf {\em FedAvg}} & N/A\\ \hline
{\bf {\em Trimmed Mean}} & $\beta=k=1$\\ \hline
{\bf {\em FedMedian}} & N/A\\ \hline
{\bf {\em Krum}} & $b=\{5,10,20,30,40\}$\\ \hline
{\bf {\em Bulyan}} & $b=\{5,10,20\}; \alpha = m - 2*b; \beta = \alpha - 2*b$\\ \hline
{\bf {\em FLTrust}} & $\alpha = \{1.0, 2 \times 10^{-3}\}; R_l=1;$ {\em root dataset} randomly sampled from $\bigcup_{c=1}^K \dataset_c$\\ \hline 
{\bf {\em MSCRED}} & $w=1$\\ \hline
{\bf {\em FLANDERS}} & $w=30$; $k=1$; $\widetilde{d}=\{0, 500\}$; $N=100$; $\alpha=1$; $\beta=1$; $\delta = \text{squared } L^2$-norm  \\ \hline
\end{tabular}
\end{table}

\section{Additional Experimental Results}
\label{app:extra-eval}

\subsection{Performance Stability}
\label{app:stability}
In this section, we further show how stable is the robustness of FLANDERS even under highly severe attack scenarios.
Specifically, for each dataset/task, we plot the distribution of the global model's accuracy obtained with all the FL aggregation rules considered in two attack settings: when the ratio of Byzantine clients is {\em below} $50\%$ (see Fig.~\ref{fig:boxplot-below}) or {\em equal to or above} $50\%$ (see Fig.~\ref{fig:boxplot-above}). 
We may notice that FLANDERS exhibits a low variance across the two settings, thus indicating high stability (i.e., more predictable behavior) compared to other baselines.
\begin{figure*}[htb!]
    \centering
    \includegraphics[width=\textwidth]{./img/boxplot_below}
    \caption{Distribution of the global model's performance as measured with all the FL aggregation strategies under every attack when the ratio of Byzantine client is {\em below} $50\%$.}
    \label{fig:boxplot-below}
\end{figure*}
\begin{figure*}[htb!]
    \centering
    \includegraphics[width=\textwidth]{./img/boxplot_above}
    \caption{Distribution of the global model's performance as measured with all the FL aggregation strategies under every attack when the ratio of Byzantine client is {\em equal to or above} $50\%$.}
    \label{fig:boxplot-above}
\end{figure*}

\subsection{Robustness vs. Cost}
\label{app:rob-vs-cost}
Showing the global model's performance drift before and after the attack helps understand the robustness of a specific FL aggregation scheme.
However, this may not consider a possible cost due to the degradation introduced by the secure FL aggregation when no attack occurs. 
In other words, suppose that the best global model's accuracy without attack is obtained with plain FedAvg, and let this value be $0.92$. Then, suppose that the accuracy drops to $0.65$ when Byzantine clients enter the FedAvg-based FL system. Undoubtedly, this is a remarkable accuracy loss.
Now, consider some robust FL aggregation that achieves an accuracy of $0.71$ when no attack occurs and $0.66$ under attack.
Clearly, this is far more robust than FedAvg as the accuracy values measured before and after the attack are closer than those obtained with FedAvg. In fact, we would draw a similar conclusion even if the accuracy under attack was $0.63$ instead of $0.66$ (i.e., if it was lower than the value observed with FedAvg). 
Still, we should also consider the accuracy drop observed with no attack (from $0.92$ to $0.71$).

Inspired by~\cite{shejwalkar2021ndss}, we design a unified metric that combines both the {\em robustness} and {\em cost} of an FL aggregation strategy.
Specifically, let $v^*_{\text{pre}}$ be the overall best global model's performance as measured within the $L$-th round (i.e., before {\em any} attack) for the generic task.
Then, we compute $v^{\phi}_{\text{pre}}$ and $v^{\phi}_{\text{post}}$, namely the global model's performance observed before and after a specific attack, using a fixed aggregation strategy $\phi$, where $\phi$ = \{FedAvg, FedMedian, Trimmed Mean, Krum, Bulyan, FLTrust, MSCRED, FLANDERS\}.

Thus, we define $R^{\phi}$ and $C^{\phi}$ respectively as the robustness and the cost of the aggregation strategy $\phi$, as follows:\footnote{We assume $v^{\phi}_{\text{pre}} \neq 0$.}
\[
R^{\phi} = \frac{v^{\phi}_{\text{post}}}{v^{\phi}_{\text{pre}}};~~~C^{\phi} = \frac{v^*_{\text{pre}}}{v^{\phi}_{\text{pre}}} - 1.
\]
The former captures the resiliency of an aggregation strategy to a given attack; the latter quantifies the drop w.r.t. the optimal model's performance under no attack.  
Notice that $R^{\phi} = 1$ iff the performance of the global model is unaffected by the attack, i.e., it is the same before and after the attack. Otherwise, $R^{\phi} < 1$ indicates the defense mechanism struggles to compensate for the attack, and if $R^{\phi} > 1$, the aggregation is so robust that the model keeps improving its performance even under attack.
On the other hand, assuming $v^*_{\text{pre}}\geq v^{\phi}_{\text{pre}}$ for any $\phi$, $C^{\phi} \geq 0$ and $C^{\phi} = 0$ iff the aggregation strategy achieves optimal performance under no attack.
Eventually, we can calculate the net gain $G^{\phi}$ of an aggregation strategy as:
\[
G^{\phi} = R^{\phi} - C^{\phi}.
\]
Obviously, the larger $G^{\phi}$ the better. 

It is worth remarking that, for each classification task, we measure the global model's accuracy, whereas, for the regression task associated with the {\em California Housing} dataset, we compute the global model's one minus mean absolute percentage error (1-MAPE). 
We report the result of this analysis in Table~\ref{tab:gain}, focusing on two attack settings: when the ratio of Byzantine clients is $20\%$ and $80\%$, i.e., when $b=20$ and $b=80$.

\begin{table*}[htb!]
\centering
\caption{\label{tab:gain}Evaluation of all the FL aggregation strategies on every dataset/task under four types of attack and two Byzantine settings: $20\%$ and $80\%$ malicious clients. The value in each cell is computed as the difference between the {\em robustness} and the {\em cost} of a specific defense mechanism as defined above, or N/A if that defense does not support the attack. Higher figures correspond to better performance.}
\vspace{2mm}
\scalebox{0.93}{
    \begin{tabular}{ |c|c|c||c|c||c|c||c|c||c|c| } 
    \hline
    \multirow{2}{*}{{\bf Dataset}} & \multirow{2}{*}{{\bf FL Aggregation}} & \multirow{2}{*}{{\bf No Attack}} & \multicolumn{2}{c||}{\textbf{GAUSS}} & \multicolumn{2}{c||}{\textbf{LIE}} & \multicolumn{2}{c||}{\textbf{OPT}} & \multicolumn{2}{c|}{\textbf{AGR-MM}} \\\cline{4-11}
    &  &  & $b=20$ & $b=80$ & $b=20$ & $b=80$ & $b=20$ & $b=80$ & $b=20$ & $b=80$ \\
    \hline
    \multirow{8}{*}{\textit{Income}} 
    & FedAvg & ${\bf 1.00}$ & $0.94$ & $0.94$ & $0.94$ & $0.94$ & $0.94$ & $0.29$ & $0.95$ & $0.94$ \\
    & FedMedian & $0.98$ & $0.98$ & $0.99$ & $0.99$ & $0.94$ & $0.98$ & $0.93$ & $0.99$ & $0.94$ \\
    & Trimmed Mean & $0.99$ & ${\bf 1.00}$ & $0.99$ & ${\bf 1.00}$ & $0.94$ & $0.99$ & $0.29$ & $0.99$ & $0.94$ \\
    & Krum & $0.99$ & $0.99$ & N/A & $0.99$ & N/A & $0.99$ & N/A & $0.99$ & N/A \\
    & Bulyan & ${\bf 1.00}$ & ${\bf 1.00}$ & N/A & ${\bf 1.00}$ & N/A & $0.99$ & N/A & ${\bf 1.00}$ & N/A \\
    & FLTrust & ${\bf 1.00}$ & $0.99$ & $0.96$ & ${\bf 1.00}$ & $0.99$ & $0.99$ & $0.95$ & ${\bf 1.00}$ & ${\bf 1.00}$ \\
    & MSCRED & ${\bf 1.00}$ & $0.99$ & $0.98$ & ${\bf 1.00}$ & $0.94$ & ${\bf 1.00}$ & ${\bf 1.00}$ & $0.99$ & $0.94$ \\
    & FLANDERS & ${\bf 1.00}$ & ${\bf 1.00}$ & ${\bf 1.00}$ & $0.99$ & ${\bf 1.00}$ & $0.99$ & $0.99$ & $0.99$ & ${\bf 1.00}$ \\
    \hline\hline
    \multirow{8}{*}{\textit{MNIST}} 
    & FedAvg & $1.01$ & $0.11$ & $0.10$ & $1.00$ & $0.12$ & $0.00$ & $0.07$ & $0.97$ & $0.10$ \\
    & FedMedian & ${\bf 1.02}$ & ${\bf 1.01}$ & $0.97$ & ${\bf 1.01}$ & $0.10$ & $1.01$ & $0.02$ & ${\bf 1.01}$ & $0.11$ \\
    & Trimmed Mean & $1.01$ & $0.98$ & ${\bf 1.00}$ & $1.00$ & $0.09$ & $0.99$ & $-0.01$ & $0.99$ & $0.08$ \\
    & Krum & $1.00$ & $0.99$ & N/A & $1.00$ & N/A & $0.99$ & N/A & $0.97$ & N/A \\
    & Bulyan & $1.01$ & ${\bf 1.01}$ & N/A & ${\bf 1.01}$ & N/A & ${\bf 1.01}$ & N/A & $1.00$ & N/A \\
    & FLTrust & $0.61$ & $0.42$ & $0.54$ & $0.53$ & $0.44$ & $0.69$ & $0.60$ & $0.47$ & $0.51$ \\
    & MSCRED & $1.00$ & $0.11$ & $0.10$ & $0.11$ & $0.10$ & $1.00$ & $0.97$ & $0.10$ & $0.10$ \\
    & FLANDERS & $0.96$ & $0.91$ & $0.96$ & $0.99$ & ${\bf 0.96}$ & $0.95$ & ${\bf 0.98}$ & $0.99$ & ${\bf 0.97}$ \\
    \hline\hline
    \multirow{8}{*}{\textit{CIFAR-10}} 
    & FedAvg & ${\bf 1.14}$ & $0.20$ & $0.20$ & $0.94$ & $0.21$ & $0.24$ & $0.21$ & ${\bf 1.10}$ & ${\bf 0.65}$ \\
    & FedMedian & $1.12$ & ${\bf 1.16}$ & ${\bf 0.96}$ & ${\bf 1.10}$ & $0.16$ & $1.08$ & $0.21$ & $1.08$ & $0.37$ \\
    & Trimmed Mean & $1.00$ & $1.02$ & $0.95$ & $0.98$ & $0.10$ & $0.99$ & $0.09$ & $0.97$ & $0.17$ \\
    & Krum & $0.98$ & $0.99$ & N/A & $0.99$ & N/A & $0.92$ & N/A & $1.00$ & N/A \\
    & Bulyan & ${\bf 1.14}$ & $1.14$ & N/A & $1.14$ & N/A & ${\bf 1.12}$ & N/A & $1.07$ & N/A \\
    & FLTrust & $-2.16$ & $-1.96$ & $-2.42$ & $-2.08$ & $-2.40$ & $-2.21$ & $-2.40$ & $-2.21$ & $-2.42$ \\
    & MSCRED & $0.89$ & $0.28$ & $0.18$ & $0.26$ & $0.23$ & $0.28$ & $0.77$ & $0.43$ & $0.37$ \\
    & FLANDERS & $0.83$ & $0.92$ & $0.86$ & $0.81$ & ${\bf 0.80}$ & $0.58$ & ${\bf 0.79}$ & $0.79$ & $0.25$ \\
    \hline\hline
    \multirow{8}{*}{\shortstack[l]{{\em California}\\{\em Housing}}} 
    & FedAvg & $0.90$ & $0.90$ & $0.90$ & $0.91$ & $0.91$ & $-0.09$ & $-0.09$ & $0.96$ & $0.96$ \\
    & FedMedian & $0.90$ & $0.90$ & $0.90$ & $0.90$ & $0.90$ & $0.91$ & $-0.07$ & $0.93$ & $0.96$ \\
    & Trimmed Mean & $0.91$ & $0.91$ & $0.91$ & $0.90$ & $0.91$ & $0.93$ & $-0.09$ & $0.93$ & $0.96$ \\
    & Krum & $0.91$ & $0.91$ & N/A & $0.90$ & N/A & $0.91$ & N/A & $0.91$ & N/A \\
    & Bulyan & $0.91$ & $0.91$ & N/A & $0.91$ & N/A & $0.91$ & N/A & $0.93$ & N/A \\
    & FLTrust & N/A & N/A & N/A & N/A & N/A & N/A & N/A & N/A & N/A \\
    & MSCRED & $0.94$ & $0.94$ & $0.93$ & $0.99$ & ${\bf 1.00}$ & $0.94$ & $0.90$ & $0.96$ & $0.96$ \\
    & FLANDERS & ${\bf 1.00}$ & ${\bf 1.00}$ & ${\bf 1.00}$ & ${\bf 1.00}$ & ${\bf 1.00}$ & ${\bf 1.01}$ & ${\bf 0.99}$ & ${\bf 1.00}$ & ${\bf 1.00}$ \\
    \hline
    \end{tabular}
    }
    \vspace{-2mm}
\end{table*}

\subsection{FLANDERS with Non-IID Local Training Data}
\label{app:non-iid}
We simulate non-i.i.d. local training
data distributions of the {\em CIFAR-10} dataset using the built-in capabilities of Flower.
Specifically, Flower includes a dataset partitioning method based on Dirichlet sampling to simulate non-i.i.d. label distribution across federated clients as proposed by~\cite{hsu2019LDA}.
Note that this is a common practice in FL research to obtain synthetic federated datasets~\cite{wang2020neurips, reddi2021iclr}.

We assume every client training example is drawn independently with class labels following a categorical distribution over $N$ classes parameterized by a vector $\bm{q}$ such that $q_i \geq 0, i \in [1,\ldots,N]$ and $||\bm{q}||_1 = 1$.
\begin{figure*}[htb!]
    \centering
    \includegraphics[width=\textwidth]{./img/non-iid-20-all}
    \caption{Global model's accuracy on {\em CIFAR-10} dataset with $20\%$ Byzantine clients and non-i.i.d. local training data using FLANDERS when $k=1$ (left), $k=5$ (middle), and $k=20$ (right).}
    \label{fig:non-iid-20-all}
\end{figure*}
To synthesize a population of non-identical clients, we draw $\bm{q} \sim \text{Dir}(\alpha_D, \bm{p})$ from a Dirichlet distribution, where $\bm{p}$ characterizes a prior class distribution over $N$ classes, and $\alpha_D > 0$ is a {\em concentration} parameter controlling the identicalness amongst clients. 
With $\alpha_D \rightarrow \infty$, all clients have identical label distributions; on the other extreme, with $\alpha_D \rightarrow 0$, each client holds examples from only one class chosen at random. 
In our experiment, we set $\alpha_D = 0.5$ to simulate a strong class label imbalance amongst clients.
Specifically, the \textit{CIFAR-10} dataset contains $60,000$ images ($50,000$ for training and $10,000$ for testing) from $N=10$ classes. 
We start from balanced populations consisting of $K=100$ clients, each holding $500$ images. Then, we set the prior distribution $\bm{p}$ to be uniform across $10$ classes. 
For every client, we sample $\bm{q} \sim \text{Dir}(0.5, \bm{p})$ and assign the client with the corresponding number of images from $10$ classes.

We analyze the impact of non-i.i.d. training data on FLANDERS by measuring how the global model's accuracy changes with the number of FL rounds, both under attack and with no malicious clients. In the first case, we consider all the attacks described in Section~\ref{subsec:attacks} with $20\%$ of Byzantine clients (i.e., $b=20$), and we vary the number of clients deemed to be legitimate at each round as $k\in \{1, 5, 20\}$.
The result of this study is shown in Fig.~\ref{fig:non-iid-20-all}.
We may observe that model training under a highly non-i.i.d. setting results in low and oscillating accuracy, disregarding the type of attack when the number of clients deemed to be legitimate is small (i.e., when $k=1$). 
The negative impact of non-i.i.d. is mitigated when we increase the number of legitimate clients $k$ included in the aggregation, except for the OPT attack, which preserves its detrimental effect on the global model's accuracy. 
This result can be explained as follows. 
Intuitively, the more (legitimate) client updates we consider for aggregation, the better we cope with possible parameter variation due to local non-i.i.d. training.  


% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one, even using the one-column format.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022. 
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
