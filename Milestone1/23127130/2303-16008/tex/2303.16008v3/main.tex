\documentclass[12pt,a4paper]{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}
%\usepackage[normalem]{ulem}
\usepackage{dsfont}

%\usepackage[dvipsnames]{xcolor}


\setlength\parindent{0pt} % noindent for whole document -

\usepackage{amsmath,amsfonts,amssymb, amsthm} % beautiful math
\theoremstyle{plain}
\newtheorem{definition}{Definition}
%\newtheorem*{proof}{Proof}
\newtheorem{assumption}{Assumption}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
%\newtheorem{corollary}{Corollary}

\newcommand{\modif}[1]{{\color{black} #1}}
\newcommand{\modifbis}[1]{{\color{black} #1}}
%\newcommand{\sout}[1]{}

%contribution box
\newcommand{\mybox}[4]{
    \begin{figure}[h]
        \centering
    \begin{tikzpicture}
        \node[anchor=text,text width=\columnwidth-1.2cm, draw, rounded corners, line width=1pt, fill=#3, inner sep=5mm] (big) {\\#4};
        \node[draw, rounded corners, line width=.5pt, fill=#2, anchor=west, xshift=5mm] (small) at (big.north west) {#1};
    \end{tikzpicture}
    \end{figure}
}

	
\usepackage{pdflscape} % landscape

\usepackage[%
    font={small,sf},
    labelfont=bf,
    format=hang,    
    format=plain,
    margin=0pt,
]{caption}

\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{bbm} % indicatrix
\usepackage[table,xcdraw, dvipsnames]{xcolor}

\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }
 
 % Fancy boxes 
 \usepackage[framemethod=TikZ]{mdframed}
\newcounter{takeaway}[section]\setcounter{takeaway}{0}
\renewcommand{\thetakeaway}{\arabic{section}.\arabic{takeaway}}
\newenvironment{takeaway}[2][]{%
    \refstepcounter{takeaway}
 
    % Code for box design goes here.
 
\begin{mdframed}[]\relax}{%
\end{mdframed}}
\ifstrempty{#1}%
% if condition (without title)
{\mdfsetup{%
    frametitle={%
        \tikz[baseline=(current bounding box.east),outer sep=0pt]
        \node[anchor=east,rectangle,fill=blue!20]
        {\strut Takeaways};}
    }%
% else condition (with title)
}{\mdfsetup{%
    frametitle={%
        \tikz[baseline=(current bounding box.east),outer sep=0pt]
        \node[anchor=east,rectangle,fill=blue!20]
        {\strut Takeaways};}%
    }%
}%
% Both conditions
\mdfsetup{%
    innertopmargin=10pt,linecolor=blue!20,%
    linewidth=2pt,topline=true,%
    frametitleaboveskip=\dimexpr-\ht\strutbox\relax%
}

 % Fancy boxes ENDS



\usepackage[list=true]{subcaption}


% TODO commands
\usepackage{snaptodo}
\setlength{\marginparwidth}{2cm}
\setlength{\marginparsep}{.02cm}
\snaptodoset{block rise=2em}
\snaptodoset{margin block/.style={font=\tiny}}

\newcommand{\gv}[1]{\snaptodo[margin block/.append style=green!50!black]{\sloppy\textbf{GV}: #1}}
\newcommand{\jj}[1]{\snaptodo[margin block/.append style=orange]{\sloppy\textbf{JJ}: #1}}
\newcommand{\bc}[1]{\snaptodo[margin block/.append style=yellow!50!black]{\sloppy\textbf{BC}: #1}}
\newcommand{\es}[1]{\snaptodo[margin block/.append style=magenta]{\sloppy\textbf{ES}: #1}}

\newcommand{\sout}[1]{}
% Uncomment below to erase all comments
%\renewcommand{\snaptodo}[2][1]{\relax}

%\usepackage[colorinlistoftodos,textwidth=2.1cm]{todonotes}
%\newcommand{\jj}[1]{\todo[color=orange, size=\tiny]{JJ: #1}}
%\newcommand{\bc}[1]{\todo[color=yellow, size=\tiny]{BC: #1}}
%\newcommand{\gv}[1]{\todo[color=green, size=\tiny]{GV: #1}}
%\newcommand{\es}[1]{\todo[color=magenta, size=\tiny]{ES: #1}}

\newcommand{\indep}{\perp \!\!\! \perp}

\usepackage{comment}

\usepackage{natbib}
\usepackage{here}

\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{cleveref}






%%%%%%% for fancy formula


\usepackage{tikz}
\usetikzlibrary{backgrounds}
\usetikzlibrary{arrows,shapes}
\usetikzlibrary{tikzmark}
\usetikzlibrary{calc}


\usepackage{amssymb}
\usepackage{mathtools, nccmath}

% for custom commands
\usepackage{xspace}

% table alignment
\usepackage{array}
\usepackage{ragged2e}
\newcolumntype{P}[1]{>{\RaggedRight\hspace{0pt}}p{#1}}
\newcolumntype{X}[1]{>{\RaggedRight\hspace*{0pt}}p{#1}}

% color box
\usepackage{tcolorbox}


% for tikz
%\usetikzlibrary{trees}
\usetikzlibrary{arrows,shapes,positioning,shadows,trees,mindmap}
% \usepackage{forest}
\usepackage[edges]{forest}
\usetikzlibrary{arrows.meta}
\colorlet{linecol}{black!75}
\usepackage{xkcdcolors} % xkcd colors


% for colorful equation
\usepackage{tikz}
\usetikzlibrary{backgrounds}
\usetikzlibrary{arrows,shapes}
\usetikzlibrary{tikzmark}
\usetikzlibrary{calc}
% Commands for Highlighting text -- non tikz method
\newcommand{\highlight}[2]{\colorbox{#1!17}{$\displaystyle #2$}}
%\newcommand{\highlight}[2]{\colorbox{#1!17}{$#2$}}
\newcommand{\highlightdark}[2]{\colorbox{#1!47}{$\displaystyle #2$}}

% my custom colors for shading
\colorlet{mhpurple}{Plum!80}


% Commands for Highlighting text -- non tikz method
\renewcommand{\highlight}[2]{\colorbox{#1!17}{#2}}
\renewcommand{\highlightdark}[2]{\colorbox{#1!47}{#2}}
\renewcommand{\eqref}[1]{eq.\,\ref{#1}}

\title{Risk Ratio, odds ratio, risk difference...  Which causal measure is easier to generalize?} %What are the safest causal effect metrics to use when trial and target population differs?
\author{B\'{e}n\'{e}dicte Colnet \thanks{Soda project-team,  Premedical project-team, INRIA (email: benedicte.colnet@inria.fr).}
  \and
Julie Josse\thanks{Premedical project team, INRIA Sophia-Antipolis, Montpellier, France.}
  \and
Ga\"{e}l Varoquaux\thanks{Soda project-team, INRIA Saclay, France.}
     \and 
Erwan Scornet \thanks{LPSM and SCAI, Sorbonne Universit\'{e}, Paris, France (email: erwan.scornet@polytechnique.edu).}
}



\date{\today}


\begin{document}


\maketitle
%\gv{Proposition de titre: "Summary measures of causal effects are not equal when facing population shifts", pour éviter le mot "metrics" qui a beaucoup de sens différents. Ou, pour un titre plus different "Causal effects with population shifts: summary measures are not equal", "Risk Ratio, odds ratio, risk difference... Which summary measures of causal effects when facing population shifts?" pour un titre plus catchy, voir même "Risk Ratio, odds ratio, risk difference... Which summary measures of causal effects for generalization?"}

\begin{abstract}
    There are many measures to report so-called treatment or causal effects: absolute difference, ratio, odds ratio, number needed to treat, and so on. The choice of a measure, \emph{eg} absolute versus relative, is often debated because it leads to different \modif{impressions of the benefit or risk of a treatment.} \modif{Besides, different causal measures may lead to various  treatment effect heterogeneity: some input variables may have an influence on some causal measures and no effect at all on others.} In addition some measures -- but not all -- have appealing properties such as collapsibility, matching the intuition of a population summary. 
    In this paper, we \modif{first} review common causal measures and their pros and cons typically brought forward. Doing so, we clarify the notions of collapsibility and treatment effect heterogeneity, unifying existing definitions. %\sout{Our main contribution is to propose to reverse the thinking: rather than starting from the measure, we start from a non-parametric generative model of the outcome. Depending on the nature of the outcome, some causal measures disentangle treatment modulations from baseline risk.}
    %\modif{By considering different non-%parametric generative models of the outcome, we prove that some causal measures disentangle treatment modulations from baseline risk.}
    \modif{Then, we show that for any causal measures there exists a generative model such that the conditional average treatment effect (CATE) captures the treatment effect.
    %and disentangles it from the baseline risk. 
    However, only the risk difference can disentangle the treatment effect  from the baseline at both population and strata levels, regardless of the outcome type (continuous or binary).} 
    %and this is the case even for binary outcome. }
    %\sout{Therefore, our analysis outlines an understanding of what heterogeneity and homogeneity of treatment effect mean, not through the lens of the measure, but through the lens of the covariates.}
    \modif{As our primary } goal is the generalization of causal measures, we show that different sets of covariates are needed to generalize an effect to a  target population depending on \textit{(i)} the causal measure of interest,
%\sout{\textit{(ii)} the nature of the outcome} 
and \textit{(ii)} the identification method chosen, that is generalizing either conditional outcome or local effects. %\es{changement ici} \jj{pas compris erwan?} \bc{j'ai enlevé 'aim' pour 'identification method'. En tout cas j'ai l'impression qu'ici on parle de la méthode ou de l'approche. Est-ce plus clair ?}
%We show that some measures require less covariates for adjustment. %We provide all generalization identification formulae.
    %Our results are general as they build on non-parametric models.

       \vspace{12pt}
\textit{Keywords:} Standardization;  
Transportability; 
Collapsibility; 
Treatment effect modifier; 
Clinical trials.
\end{abstract}

%\tableofcontents

\section{The age-old question of how to report effects}


From the physician to the patient, the term \textit{effect} of a drug on an outcome usually appears very spontaneously, within a casual discussion or in scientific documents. % Désolée j'inaiste sur cette premiere phrase :)
Overall, everyone agrees that \modif{for a binary treatment} an effect is a comparison between two states: treated or not.
%\es{rev2 : Well, not all exposures are binary, and not all binary comparisons are causal effects.}.
But there are various ways to report the \modif{average}\sout{main} effect 
%\es{rev2: meaning? ES: average effect?} 
of a treatment.
%But, when looking for more details, and in particular at the quantitative ways of defining a so-called treatment effect, this notion is far from being a universal concept.
%Leaving aside the analysis of side effects, the main effect of a treatment can take several quantitative definitions, and as a consequence can be reported differently depending on the context or habits.
For example, the scale \modif{on which we choose to quantify the effect of a treatment} may be absolute  \citep[e.g. the number of migraine days per month is expected to diminishes by 0.8 taking Rimegepant, see][]{Edvinsson2021Migraine} or relative (e.g. the probability of having a thrombosis is expected to be multiplied by 3.8 when taking oral contraceptives \citep{Vandenbroucke1994thrombosis}).
Choosing one measure or the other has several consequences. 
First, it conveys a different impression of the same data to an external reader. \modif{\cite{naylor1992measured, Forrow1992HowResultsAreSummarized} both showed that physicians's likelihood to treat patient -- following their impression of therapeutic effect -- is impacted by the scale chosen to report clinical effect.} %\citep{Forrow1992HowResultsAreSummarized, Cook2014UserGuide, xiao2022IsORPortable}
%\es{rev2 : Roughly, what do these papers say about the impressions different measures convey?}\jj{Pour Béné}\bc{Du coup j'ai préféré choisir deux papiers qui font des expériences. Ca vous va ?}. 
Such subjective impressions may be even more prominent in newspapers, where most effects are presented in relative rather than absolute terms, creating a heightened sense of sensationalism \citep{moynihan2000coverage}. 
Second, the heterogeneity of the treatment effect  -- i.e. \modif{how the treatment effect changes from one  sub-population to another } -- 
%\es{rev2 : You are using the word effect here to mean basically a contrast or comparison. The word effect sounds more mechanistic, though, and presumably does not vary at all if it arises from physical laws?} \jj{??}\bc{Je suis un peu perdue aussi, j'ai proposé un changement en espérant que ce soit plus clair} \es{j'ai un peu modifié}
depends on the chosen causal measure \citep[see p.199 in][]{Rothman2011bookEpidemiologyIntrod}. %\jj{reference oun phrase en dessous que je garderais, ou mettre voir annexe ou section }\bc{Aie je ne comprends pas la question ?}
%For instance, an effect presented as the absolute difference can be varying on two subgroups, while the ratio is not (as illustrated in Figure~\ref{fig:toyexamplesummary}).
%\gv{Referencer l'appendice aussi tôt dans l'introduction n'est pas très élègant. Une version super simple de cette figure ici?}.
The choice of the measure to report an effect is still actively discussed \citep{Spiegelman2017Modeling, Spiegelman2017letSubject, baker2018new, Changyong2019RelationsAmongThreePop, Doi2020callToChangePractice, doi2022TimeToDoAway, xiao2021odds, xiao2022IsORPortable, Huitfeldt2021ShallWe,Lapointe2022FromMathToMeaning, liu2022rejoinder}. 
Publications on the topic come with many diverging opinions and guidelines (see Appendix~\ref{appendix:different-point-of-views} for quotes).
Yet, the question of the measure (or metric) of interest is not new. For example, as \cite{Sheps1958ShallWe} wrote in the \textit{New England Journal of Medicine} \citep[see also][]{Huitfeldt2021ShallWe}:
\begin{quote}
    `` We wish to decide whether we shall count the failures or the successes and whether we shall make relative or absolute comparisons ".
\end{quote}
%Note that this quote from Mindel C. Sheps does not only highlight the importance of the chosen measure, but also how we encode the outcome for binary outcomes of interest\footnote{We detail in this work how the choice of encoding can be seen as choosing one of the possible relative measures.}.
% \begin{wrapfigure}{r}{0.32\textwidth}
%  \begin{center}
%    \includegraphics[width=0.32\textwidth]{fig/toyexamplesummary.png}
%  \end{center}
%  \caption{Sub-groups' effects can vary or not depending on the chosen scale.}
%  \label{fig:toyexamplesummary}
%\end{wrapfigure} 

Beyond \modif{conveyed impressions and captured heterogeneity}, different causal measures lead to different \modif{generalization properties} towards populations \citep{huitfeldt2018choice}.
The problem of generalizability (or portability) encompasses a range of different scenarios, and refers to the ability of \modif{carrying over findings to a broader population, beyond the study sample}. 
%
%It can also encompass issues related to the transferability of results from one research domain to another, or from research settings to real-world clinical practice. 
Generalizability of trials' findings is crucial as, most often, clinicians use causal effects from published trials to estimate the expected response to treatments for a specific patient based on his/her baseline risks, and therefore to choose the best treatment. 
In this work, we show that some effect measures are less sensitive than others to population's \sout{shift}\modif{differences}
%\es{Perhaps just 'differences'?}\bc{ou distributional differences? pour préciser} \es{ça me va de juste mettre différences} 
between the study sample and the target population.\\
%\es{à moduler en fonction de nos modifications dans le corps de l'article}\bc{?}

Section~\ref{sec:formalization-and-key-contributions} starts with a didactic clinical example to introduce the questions, \modif{the concepts,} the notations, and our main results. 
\sout{Reading it suffices for an executive summary of the paper.}
%This part can be read as a whole to grasp the key results. 
Our four contributions are detailed in Section~\ref{sec:formalization-and-key-contributions} and summarized below. 
\sout{For the mathematical underpinnings, each of the following sections (Section \ref{section:causal-metrics-properties}-\ref{sec:generalize}) is dedicated to one of the three contributions exposed in  %intermediate steps backing the main results exposed in
Section~\ref{sec:formalization-and-key-contributions}.
%, each section being linked to one contribution.}
}
In Section~\ref{section:causal-metrics-properties}, we \modif{review,} clarify, and demonstrate typical properties of causal measures, such as treatment effect homogeneity, heterogeneity, and collapsibility.\sout{and then explicitly links collapsibility with generalizability (\textit{i.e.} re-weighting of local effects to get the population effect).}
%In this section we both clarify and enrich the different definitions available in the literature. 
%Doing so, Section~\ref{section:causal-metrics-properties} adopts the general approach taken in the literature while completing it.  
%Our findings are allowed thanks to 
In Section~\ref{section:generative-models}, \modif{we show that for any causal measures there exists a generative model such that the conditional average treatment effect captures the treatment effect. We also show that among collapsible measures, only the Risk Difference can disentangle these effects at both the conditional and unconditional level, for a variety of settings. This result also holds for bounded outcome as binary ones, but only for some values of the baseline risk. 
We exhibit specific settings in which some causal measures are able to disentangle the treatment effect from the baseline. More precisely, we study a model for binary outcome inspired by the example of the Russian Roulette, in which the Risk Difference depends on the baseline, but the Survival Ratio is constant.}\sout{we reverse the thinking: we propose to start from a non-parametric generative model of the outcome, and then observe what each measure captures}\sout{ This approach leads to a new view on treatment effect heterogeneity: accounting for covariates that may act as treatment effect modulators, as opposed to those that only affect baseline levels.}
%This enables another definition of heterogeneity, and in particular a new definition of heterogeneity. 
%Primarily, we show how the non-parametric generative models introduced help to have another definition of the heterogeneity of a treatment effect (some covariates acting only on the baseline level while others being also treatment effect modulators). 
%Our primary aim is to demonstrate how non-parametric generative models can provide an alternative approach to defining treatment effect heterogeneity. 
Section~\ref{sec:generalize} presents the consequences on the generalizability of causal measures. We show that the Risk Difference is easier to generalize, in the sense that it requires adjustment only on the shifted treatment effect modulators introduced in Section~\ref{section:generative-models}, and not on all shifted prognostic covariates, \modif{i.e.\ variables both predictive of the outcome and with a different distribution between sources}. 
%There is exactly one measure per situation: nature of the outcome and direction of the effect. 
Other causal measures can be generalized in some very specific settings (e.g., homogeneous treatment effect). 
Section~\ref{sec:simulations} illustrates the takeaways through simulations. \\ %These simulations are inspired from a major public health issue: the effect of oral contraceptives and thrombosis.
% ask for less covariates.\bc{Julie dit "	in particular showing which variables are required for each measure to be generalisable by standardization? "} %required for each measure to be generalisable by standardization, while using properties detailed in Section~\ref{section:causal-metrics-properties}, and in particular collapsibility. %We show how this new definition of hetereogeneity can lead to the universal generalizability of one of the causal metric for each outcome. We also show how some metrics are easier to generalize to other population, in the sense that they require less covariate to generalize well.
%\jj{je pense qu'on aurait envie d'être plus précis ici pour donner envie au lecteur du genre risk difference generalise bien, ou quelque chose comme ça}\bc{Ce serait en partie 2, ça irait?}

%\tableofcontents


\modif{As this paper builds on a prolific and diverse literature, we differentiate our original contributions from previously-known results. For this purpose, all definitions, assumptions, and lemmas from prior work contain an explicit reference in their title, while those without are original contributions.}


\section{Problem setting and key results}\label{sec:formalization-and-key-contributions}


\subsection{Causal effects in the potential outcomes framework}

%The current standard to report causal effect of a policy or treatment on a certain outcome can be denoted within the so-called potential outcome framework. 
\modif{Among the various frameworks for causal reasoning such as \cite{Pearl2000Book}, \cite{dawid2000causal}, or \cite{hernan2020whatifbook}}, we use the \emph{potential outcome} framework to characterize treatment (or causal) effects.
This framework has been proposed by Neyman in 1923 \citep[English translation in][]{SplawaNeyman1990Translation}, and popularized by Donald Rubin in the 70s \citep{imbens2015causal, hernan2020whatifbook}.
It formalizes the concept of an intervention by studying two possible values $Y^{(1)}_i$ and $Y^{(0)}_i$ for the outcome of interest (say the pain level of headache) for the two different situations where the individual $i$ has been exposed to the treatment ($A_i = 1$) or not ($A_i=0$). 
%\es{it is probably worth at least mentioning that there are other ways to view causal effects, i.e. counterfactuals (Robins) or counterfactual-free (Dawid). Interestingly I think in your generative setup the three approaches come closest to some form of agreement in principle, as well as in practice.}
We will only consider binary exposure.
The treatment has a causal effect if the potential outcomes are different, that is testing the assumption:
%
\begin{equation}
\label{eq:is_equal_indivi_treat_effect}
Y^{(1)}_i \stackrel{?}{=} Y^{(0)}_i.   
\end{equation}
Unfortunately, one cannot observe the two worlds for a single individual. Statistically, it can still be possible to compare the \emph{expected} values of each potential outcome $Y^{(a)}$ but it requires a population-level approach, broadening from a specific individual.
The paradigmatic example is a randomized experiment (called Randomized Controlled Trial --RCT-- in clinical research or A/B test in marketing): randomly assigning the treatment to half of the individuals enables the average comparison of the two situations.
Doing so, the previous question of interest amounts to \textit{comparing} or \textit{contrasting} two expectations:
%
\begin{equation}
\label{eq:is_equal_potential_expectation}
%Y^{(1)}_i \stackrel{?}{=} Y^{(0)}_i \quad 	\longrightarrow  \quad 
\mathbb{E}\left[Y^{(1)}\right] \stackrel{?}{=} \mathbb{E}\left[Y^{(0)}\right],
\end{equation}
%
where $\mathbb{E}[Y^{(a)}]$ is the expected counterfactual outcome had all individuals in the \textit{population} received the treatment level $a$. 
This quantity is defined with respect to a population: statistically, the expectation is taken on a distribution, which we denote $P_{\text{\tiny S}}$ (reflecting the \textbf{s}ource or \textbf{s}tudy sample from which evidence comes, for example a RCT). Many methodological efforts have focused on estimating the two expectations (namely $\hat{\mathbb{E}}\left[Y^{(1)}\right]$ and $\hat{\mathbb{E}}\left[Y^{(0)}\right]$). Our focus is different: we propose theoretical guidance for choosing among different \modif{real-valued} measures that allow us to compare
%\es{rev2 : I think the point is that you want to make a *scalar* (i.e. one-dimensional) comparison.}
%\modif{that make one-dimensional comparison between} \sout{to \textit{compare}} 
those two expectations at the population level, e.g. ratio, difference, or odds. What are the properties of these measures? 
How do they impact the conclusions of a study? 
%\jj{je dirais we do not focus on estimation moi je dirais même si on dit un mot à la fin car trop important pour ne pas en parler du tout}

%as if we had access to the true expectations (i.e. having access to an very large randomized controlled trial). 
%\es{pas clair clair, redire qu'on cherche à évaluer l'impact des différentes mesures}\bc{est-ce mieux?}

%\jj{section 2.1 pourrait être raccourcie mais c'est un détail}\bc{je suis d'accord. mais il me semble que pour des médecins c'est pas une section évidente. Or je suis restée dans l'optique qu'on vise un stat in medecine ou assimilé... Je me dis que cette demi page est le minimum pour mettre à bord une personne qui connait pas les PO, et que plus court on rate tout ce public. Qu'en penses tu ? }
\subsection{Comparing two averaged situations: different treatment effect measures}\label{subsec:causal-measures-presentation}

%To quantify how different the expected potential outcomes values from \eqref{eq:is_equal_potential_expectation} are, one can find several practices.
We focus on two types of outcomes: continuous  (e.g. headache pain level) and binary (e.g. death). 
%Usually within the medical field an event such as death is usually encoded $Y=1$ (and $Y=0$ for no event). 
%Later in the paper we will explain why encoding has an importance.
Binary outcomes are frequent in medical questions, often related to the occurrence of an event.
%Binary outcomes are often encountered within the medical domain, as they are well-suited to describe outcome such as remission or death. 
%Note that such outcome is always defined with respect to a duration, for example the death within 28 days, of the remission within 6 months \citep{Greenland1987Interpretation, Rothman2000ModernEpidemiology}. 
%\es{Considering directly the time of survival as the outcome of interest requires resorting to survival analysis, which is beyond the scope of this work. } 
%Considering directly the time of survival as the outcome of interest requires resorting to survival analysis, which is beyond the scope of this work.
%Willing to remove the time period into the binary outcome leads to a different notation and concept usually called survival. This work does not consider such outcomes. 

\paragraph{Continuous outcome}
 For continuous outcomes,  a common measure is the absolute difference, which  \modif{corresponds to the difference of means} (\sout{usually referred to}\modif{for homogeneity of notations with the binary outcome, we denote it as the} the Risk Difference - RD): 
 %\es{Is this really referred to as a "risk difference" when the response is continuous? Surely it would be more appropriate to call it just the "mean difference" or, perhaps better, "difference of means"? There is no "risk" to difference here. The same point applies to "Risk Ratio" (I suggest "ratio of means") and "excess Risk Ratio" (which is just a kind of relative change, so I suggest "relative difference of means").} \jj{Béné tu sais?, j'ai changé mais à voir}\bc{Je suis d'accord avec le reviewer, et je trouve plus simple d'avoir une seule notation. Ca me va parfaitement ce changement !}: 
\begin{equation*}
    \tau_{\text{\tiny RD}} := \mathbb{E}\left[Y^{(1)}\right] -  \mathbb{E}\left[Y^{(0)}\right].
\end{equation*}
A null effect corresponds to $\tau_{\text{\tiny RD}}=0$. If the outcomes are of constant sign and different from $0$, one can also consider relative measures\footnote{Allowing situations where the outcomes can be null or change sign is at risk of having undefined ratio due to $\mathbb{E}\left[Y^{(0)}\right]=0$. This is why, when considering relative measure we assume that the continuous outcome is of constant sign. Note that this is often the case in medicine. For example with blood glucose level, systolic blood pressure, etc.} such as the ratio \modif{of means} (\modif{also called} Risk Ratio - RR), or \modif{relative difference of means}  (\modif{also called} Excess Risk Ratio - ERR):
\begin{equation*}
   \tau_{\text{\tiny RR}} :=  \frac{\mathbb{E}\left[Y^{(1)}\right]}{\mathbb{E}\left[Y^{(0)}\right]}, \quad\qquad  \tau_{\text{\tiny ERR}} :=  \frac{\mathbb{E}\left[Y^{(1)}\right]-\mathbb{E}\left[Y^{(0)}\right]}{\mathbb{E}\left[Y^{(0)}\right]} = \tau_{\text{\tiny RR}} -1.
\end{equation*}
%\es{It may be worth pointing out that (unlike for the difference of means) this ratio of means is *not* the same as the mean of the ratios.}
\modif{Contrary to the difference of means which equals the mean of the differences, the ratio of means $\tau_{\text{\tiny RR}}$ is not equal to the mean of the ratios.}
A null effect now corresponds to $\tau_{\text{\tiny RR}} =1$ or $\tau_{\text{\tiny ERR}}=0$. 
Note that the ranges of the three metrics are \sout{very} different, e.g. if $\mathbb{E}\left[Y^{(1)}\right] =200$ and $\mathbb{E}\left[Y^{(0)}\right]  = 100$, then $ \tau_{\text{\tiny RD}} = 100$, while $ \tau_{\text{\tiny RR}} = 2$ and $ \tau_{\text{\tiny ERR}} = 1$. %In particular $\tau_{\text{\tiny RR}}$ and $\tau_{\text{\tiny ERR}}$ are dimensionless quantities. Also note that the relatives measures  $\tau_{\text{\tiny RR}}$ and $\tau_{\text{\tiny ERR}}$ make more sens if the outcome

\paragraph{Binary outcome}
Due to the binary nature of the outcome, the two expectations of \eqref{eq:is_equal_potential_expectation} can now also be understood as the probability of the event to occur $ \mathbb{E}\left[Y^{(a)}\right] = \mathbb{P}\left[Y^{(a)} = 1\right]$.
%, and as a consequence $\mathbb{P}\left[Y^{(a)} = 0\right] = 1-\mathbb{E}\left[Y^{(a)}\right]$.
As long as the phenomenon is non-deterministic in the sense that  $\mathbb{P}\left[Y^{(0)} = 1\right] \neq 0$, previous relative measures $\tau_{\text{\tiny RR}}$ and $\tau_{\text{\tiny ERR}}$ can be used for binary outcomes. Other measures, such as the Survival Ratio (SR) can be considered if $\mathbb{P}\left[Y^{(1)} = 1\right] \neq 1$: SR is nothing but a \textit{reversed} Risk Ratio (RR) where null events are counted instead of positive events. \modif{Doing so, one could also define a reversed Excess Risk Ratio (ERR), which we denote Risk Susceptibility.}
%But other measures exist when the outcome is binary. For example the Risk Ratio (RR) can be \textit{reversed}, rather counting the null events 
%\es{rev2 : I suppose also the ERR can also be reversed, in the sense of replacing Y0 with Y1 in the denominator.} \jj{pour moi on peut reverse mais je ne sais pas si ce qu'on trouve à un nom, sinon on peut aussi choisir une autre base pour le dénominateur mais je ne sais pas si ça a un nom}\bc{Sur ce commentaire précisément je ne suis pas du même avis que le reviewer, mais ça nous dit qu'il faut préciser.} \es{si on considère $0$ comme l'événement d'intérêt au lieu de 1, il me semble que le ERR se transforme en la risk susceptibility (cf annexe C)}. 
%\bc{oui, j'ai proposé une phrase}
%It is called the Survival Ratio (SR). 
The Odds Ratio (OR) is another very common measure, as it serves as a link between follow-up studies and case-control studies \citep{Greenland1987Interpretation, king2002estimating}.
Another measure called the Number Needed to Treat (NNT) has been proposed more recently \citep{Laupacis1988AnAssessmentOfClinically}: it helps the interpretation of the Risk Difference by counting how many individuals should be treated to observe one individual answering positively to the treatment. Depending on the direction of the effect, NNT can also be called Number Needed to Harm (NNH) when the events are side effects or Number of Prevented Events (NPE) when it comes to prevention. \modif{One unappealing aspect of NNT, NNH and NPE is that the null effect corresponds to an infinite value of these measures which implies that when the difference between the two treatments is not statistically significant, the confidence interval for the number needed to treat is difficult to describe \citep{altman1998confidence}.}
%\es{rev2 : One unappealing aspect of NNT and NNH is that the null effect is infinite. Discuss?} \jj{que peut t'on dire?} \es{que c'est plus difficile de tester si une quantité est infinie que le fait qu'elle soit égale à 0?}\bc{Vous avez toujours de trop bonnes idées, j'ai donc été chercher un article qui parle des CIs pour le NNT} 
For simplicity of the exposition, in this work, we only consider NNT. The exact expression of the above measures are given here:  
\begin{equation*}
   \tau_{\text{\tiny SR}} :=  \frac{\mathbb{P}\left[Y^{(1)} = 0\right]}{\mathbb{P}\left[Y^{(0)} = 0\right]}, \qquad \tau_{\text{\tiny OR}}  := \frac{\mathbb{P}[Y^{(1)} = 1]}{\mathbb{P}[Y^{(1)} = 0]}\, \modif{\Bigg/} \left(  \frac{\mathbb{P}[Y^{(0)} = 1]}{\mathbb{P}[Y^{(0)} = 0]}\right), \qquad \tau_{\text{\tiny NNT}}  := \tau_{\text{\tiny RD}}^{-1}.
\end{equation*}
%\es{For purely aesthetic reasons I think I would prefer to see this as (a/b) / (c/d)}
Other measures can be found in the literature, such as the log Odds Ratio (log-OR). We recall each measure in Appendix~\ref{appendix:list-of-measures}, where Figure~\ref{fig:big-plot-with-all-metrics} illustrates the differences between measures, for different values of the expected outcomes of controls and treated. We also compute all these measures on a clinical example in Section~\ref{subsec:illustrative-example-and-key-results}.
%\jj{dire qu'en dessous tu as un exemple où tu calcules pour un même cas toutes ces mesures?}
\paragraph{Treatment effects on subgroups}
%For now, we proposed marginal effect, that is effect at the population level.
Treatment effects can also be reported within subgroups of a population (i.e. stratified risks) to show how sub-populations react to the treatment. 
Therefore, one could also define each of the previously introduced measures on sub-populations.
For the rest of the work, we denote by $X$ a set of covariates\footnote{Those covariates are baseline or pre-treatment covariates. See \cite{VanderWeele2007FourTypes} for a detailed explanation.}. We denote by $\tau(x)$ the treatment effect on the subpopulation $X=x$ for any causal measure. 
For example $\tau_{\text{\tiny RD}}(x)$ denotes the Risk Difference on the subgroup for which $X=x$. The quantity $\tau(x)$ is often referred to as the Conditional Average Treatment Effect (CATE). \\

\modif{\paragraph{Assumptions} Throughout this paper, and for the Average Treatment Effect and the Conditional Average Treatment Effect to be well-defined, we assume that $\mathds{E}[|Y^{(0)} ], \mathds{E}[|Y^{(1)}|] < \infty $ and $\mathds{E}[|Y^{(0)}| |X], \mathds{E}[|Y^{(1)}| |X]< \infty $. Such assumptions are satisfied as soon as the response variable is bounded.} 
%Most of the time, the consideration of heterogeneity in applied clinical work is made ``\textit{one-variable-at-a-time}", for e.g. for male and female (in such a situation, $X$ is univariate and binary), even if some literature propose to change this practice to observe hetereogeneity along several covariates at a time \citep{Kent2019PATH1}. 

\subsection{Key messages: from effect measures to generalization}\label{subsec:illustrative-example-and-key-results}
\subsubsection{An illustrative example}
We consider clinical data assessing the benefit of antihyperintensive therapy ($A$) against stroke ($Y$) \citep{macmahon1990blood, Cook1995NNT}. We denote by $Y=1$ a stroke, and $Y=0$ no stroke. Individuals can be categorized into two groups depending on their diastolic blood pressure: either \modif{$X=0$} \modif{corresponding to a mild baseline risk of stroke}\sout{moderate} \modif{or $X=1$ corresponding to a moderate baseline risk of stroke:} \sout{mild}  %\es{rev2: I assume these are moderate or mild *risk of stroke* (rather than moderate or mild blood pressure). Is there a particular reason you have coded X this way around?}
\sout{. Moderate patients have a higher baseline risk of stroke than mild patients, which corresponds to} $\mathbb{P}\left[ Y^{(0)} = 1 \mid X = 0\right]\leq\mathbb{P}\left[ Y^{(0)} =1\mid X =1\right]$. In this example, $X=1$ (resp. $X=0$) corresponds to a baseline risk of 2 events for 10 individuals (resp. 15  events for 1,000 individuals). All the measures previously introduced are computed from values reported in the original articles and presented in Table~\ref{tab:introduction-diastolic}. \\
%\es{je pense que le reviewer nous suggère d'intervertir $X=0$ et $X=1$} bc : ok \\

%$\mathbb{P}\left[Y^{(0)} \mid X =0 \right] = 0.2$, $\mathbb{P}\left[Y^{(1)} \mid X =0 \right] = 0.12$, $\mathbb{P}\left[Y^{(0)} \mid X = 1 \right] = 0.015$, $\mathbb{P}\left[Y^{(1)} \mid X =1 \right] = 0.009$$
\begin{figure}[!h]
	\begin{minipage}{.4\linewidth}
     \captionof{table}{\textbf{Different treatment measures give different impressions of the phenomenon}: The outcome is stroke in 5 years ($Y=1$ denoting stroke and $Y=0$ no stroke) and stratification is done along a binary covariate $X$ (moderate $X=1$ or mild $X=0$). Each measure are computed from aggregated data taken from \cite{macmahon1990blood, Cook1995NNT}. No confidence intervals are represented as our focus is the interpretation of the measure and not statistical significance.}
      \label{tab:introduction-diastolic}
    \end{minipage}
    \hspace{0.3cm}
    \begin{minipage}{.55\linewidth}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
      & $\tau_{\text{\tiny RD}}$ & $\tau_{\text{\tiny RR}}$ &  $\tau_{\text{\tiny SR}}$ &  $\tau_{\text{\tiny NNT}}$ & $\tau_{\text{\tiny OR}}$  & $\tau_{\text{\tiny ERR}}$ \\ \hline \hline
\cellcolor[HTML]{CBCEFB}\textbf{All ($P_\text{\tiny S}$)} &   $-0.045$ &   \textbf{0.6} &  $1.05$  &  $22$   &  $0.57$  &  \textbf{-0.4}  \\ \hline \hline
\cellcolor[HTML]{ECF4FF}\textbf{X = 0} &   $-0.006$ &   \textbf{0.6} &  $1.01$  &  $167$   &  $0.60$ &  \textbf{-0.4}   \\ \hline 
\cellcolor[HTML]{ECF4FF}\textbf{X = 1} &  $-0.080$  &  \textbf{0.6}  &  $1.10$  &  $13$   &  $0.55$ &  \textbf{-0.4}  \\ \hline 
\end{tabular}
\end{center}
	\end{minipage}
\end{figure}
%\es{on peut mettre le même nombre de chiffres significatifs par colonne ? Je trouve ça plus facile à lire}bc:oui bien sur! 
A Risk Ratio below 1 means that there is an inverse association, that is a decreased risk of stroke in the treated group compared with the treated group. 
More precisely, the treated group has 0.6 times the risk of having a stroke outcome when compared with the non-treated group.
On this example, one can also recover that the Odds Ratio approximates the Risk Ratio in a stratum where prevalence of the outcome is low (\modif{$X=0$}), but not if the prevalence is higher (\modif{$X=1$}) (derivations recalled in Appendix~\ref{appendix:list-of-measures}).
The survival ratio of 1.05 captures that there is an increased chance of not having a stroke when treated compared to the control by a factor 1.05.
Note that the Survival Ratio takes really different values than the Risk Ratio: it corresponds to the Risk Ratio where labels $Y$ are swapped for occurrences and non-occurrences, illustrating that Risk Ratio is not symmetric to the choice of outcome 0 and 1 --e.g. counting the living or the dead \citep{Sheps1958ShallWe}. This lack of symmetry is usually considered as a drawback of the survival ratio and Risk Ratio compared to the odds ratio.
%\es{And do you agree with those who consider this a drawback? (I do not.)}\bc{I live very well with that, I am more concerned by the non collapsibility and complex interpretability of OR :) You would like to remove this sentence?}. 
Indeed, the odds ratio is robust to a change of labels: swapping labels leads to changing the odds ratio $\tau_{\text{\tiny OR}}$ by its inverse $\tau_{\text{\tiny OR}}^{-1}$ (see Appendix~\ref{appendix:list-of-measures}). \sout{There is no such formula to understand the effect of a change of labels on the Risk Ratio or the survival ratio.}

Finally, the Risk Difference translates the effect on a absolute scale: treatment reduces by $0.045$ the probability to suffer from a stroke when treated\footnote{When it comes to binary outcomes, such absolute effects are rather presented as reducing by 45 events over $1,000$ individuals.}.
The NNT is the number of patients you need to treat to prevent one additional bad outcome. Here the NNT is 22, meaning that on average, one has to treat 22 people with the drug to prevent one additional stroke.
\sout{The Number Needed to Treat represents the same Risk Difference on a way bigger amplitude} 
%\es{The word "way" is too informal, but more importantly, what do you mean by "amplitude"?} 
\sout{than the Risk Difference, especially when looking at the effect on subgroups.
This is one of the interest of the NNT, making the measure more explicit}
%\es{I don't think it is more explicit, but I think it is more human, in the sense that you can quickly do a rough calculation like "drug costs €1K/dose, and NNT is 30, so am I willing to spend €30K to save one life?"} \sout{than a difference of probabilities.} 
\modif{NNT may seem simpler to interpret than a difference in probability and it enables us to quickly assess the cost (e.g., in terms of money) of a positive outcome.} %\jj{Je ferais des phrases avec les chiffres pour expliquer comment tu les lis, hyper important je pense}
%Both RD and NNT are highly dependent on the baseline, but not if the prevalence is getting higher ($X=0$). Finally, note that the Survival Ratio takes really different values than the Risk Ratio. As the Survival Ratio corresponds to the Risk Ratio where labels $Y$ are swapped for occurrences and non-occurrences, this illustrates the non-symmetry of the Risk Ratio. The Odds Ratio or the Risk Difference does not suffer from non-symmetry usually presented as an important drawback by practioners. 

\subsubsection{Contributions: how to choose a causal measure?}

\modif{\textit{This section intends to present key results in an intuitive manner. Complete mathematical definitions are given in Sections~\ref{section:causal-metrics-properties}, \ref{section:generative-models}} and \ref{sec:generalize}}.

\paragraph{Contribution 1: Properties of causal measures [Sections~\ref{section:causal-metrics-properties}]}
\modif{Different causal measures can have different properties (homogeneous/heterogeneous treatment, logic-respecting, collapsibility), which may in turn impact their interpretation. We give precise definitions of all these properties and establish relations between them. For instance, to understand the importance of collapsibility, let us dive into the following example.} 
If we were only provided with subgroup effects, and not the population effect ($P_\text{\tiny S}$ or \textbf{All} on Figure~\ref{tab:introduction-diastolic}), an intuitive procedure to obtain the population effect from local effects would be to average subgroups effects. More explicitly,
\begin{flalign}\label{eq:toy-example-collapsibility}
    \text{(Collapsibility)}
    &&
    \tau_{\text{\tiny RD}} =  \tikzmarknode{amp}{\highlight{ForestGreen}{\color{black} $p_\text{\tiny S}(X=1)$ }}\cdot\tau_{\text{\tiny RD}}(X=1) +  \tikzmarknode{amp}{\highlight{ForestGreen}{\color{black} $p_\text{\tiny S}(X=0)$  }}\cdot\tau_{\text{\tiny RD}}(X=0),&&
\end{flalign}%
\begin{tikzpicture}[overlay,remember picture,>=stealth,nodes={align=left,inner ysep=1pt},<-]
% For "t_{j+1}"
%\path (tj1.north) ++ (-3.85,-1.8em) node[anchor=north west,color=NavyBlue!85] (tj1text){\textsf{\footnotesize property of (j+1)\textsuperscript{th} item}};
\path (amp.north) ++ (-7,-1.8em) node[anchor=north west,color=ForestGreen] (sotext){\textsf{\footnotesize \% individuals with $X=1$} in $P_{\text{\tiny S}}$};
\path (amp.north) ++ (-1.5,-1.8em) node[anchor=north west,color=ForestGreen] (sotext){\textsf{\footnotesize \% individuals with  $X=0$} in $P_{\text{\tiny S}}$};
\end{tikzpicture}

where $P_{\text{\tiny S}}$ is the \textbf{s}ource population from which the study was sampled, and $p_\text{\tiny S}(X=x)$ is the proportion of individual with $X=x$ in this population. In our example study above,  $p_\text{\tiny S}(X=0) = 0.53$ \citep{Cook1995NNT}, thus for the risk difference, the formula retrieves the population effect from the sub-group effects:
\begin{align*}
-0.47\cdot0.08 - 0.53\cdot 0.006 = 0.0452 =    \tau_{\text{\tiny RD}}. 
\end{align*}
When a population-effect measure can be written as a weighted average of subgroup effects \modif{with positive weights and summing to 1}, it is said to be \textit{collapsible} \modif{\citep[Definition \ref{def:indirect-collapsibility}, based on][]{Huitfeldt2019collapsible}}, or \textit{directly collapsible} \modif{\citep[Definition \ref{def:direct-collapsibility}, based on][]{pearl1999collapsibility, Pearl2000Book}} if the weights are simply  equal to the population's proportions.
%\es{rev2 : Is this your definition of collapsibility, or someone else's? Is direct collapsibility your term, or someone else's?} \jj{j'ai rajouté les ref}. 
While the Risk Difference is directly collapsible, this is not true for all measures (e.g. the Number Needed to Treat is such that $0.47 \cdot 167 + 0.53 \cdot 13 = 85 \neq 22$). We precisely define collapsibility and which measures are collapsible (or not) in Section~\ref{subsec:collapsibility}, and summarized the results in Table~\ref{tab:small-summary-measures}.
%Intuitively, one would wish that the procedure from \eqref{eq:toy-example-collapsibility} remains valid, only changing the weights for example swapping $p_\text{\tiny S}(X=0)$ for $p_\text{\tiny S}(X=1)$ (resp. $p_\text{\tiny T}(X=0)$ for $p_\text{\tiny T}(X=1)$). More explicitly, 
%\es{I think you need to make clear what kind of equality this is. Are you making a definition, or does the left-hand side have another definition and (under some circumstances) equal the right-hand side? For myself, I have found it very helpful to think in terms of the two measures $p_S$ and $p_T$ (in your notation). I would suggest that the left-hand side here has a definition in terms of $p_T$ only, and reduces to the right-hand side under certain assumptions} \jj{pas compris?}
%where $ \tau_{\text{\tiny RD}}^{\text{\tiny T}}$ is the RD on the target population $P_{\text{\tiny T}}$ and $\tau_{\text{\tiny RD}}^{\text{\tiny S}}(x)$ are local effects in the source population $P_{\text{\tiny S}}$.

\paragraph{Contribution 2: A measure can disentangle treatment effect from baseline risk [Section~\ref{section:generative-models}]}

%In this work, we will present that, if the treatment effect diminishes the number of occurences, it is expected that the RR depends less on the baseline. The results comes from \textit{(i)} a re-writing of the outcome as a non-parametric generative model from baseline covariates and treatment \jj{redonne la section}, which \textit{(ii)} enables to express what each treatment effect measures grasps of it \jj{pas compris la fin de la phrase}. %As non-parametric models used depends on the nature of the outcome (continuous or binary), the measures's properties depend on the nature of the outcome too. 
%As a consequence the nature of the outcome and the direction of the effect dictate one measure as being a measure expressing only the modulation of the treatment effect without including the baseline's level. 
%A by-product is another way to define what treatment effect modulators are, with respect to the mechanism that generates $Y$ and not with respect to the measure.
Table~\ref{tab:introduction-diastolic} shows that the choice of the measure gives different impressions of the heterogeneity of the effect, i.e. how much the effects measures change on different subgroups. 
Such differences can be due to different baseline risks. For example, it seems that a higher number needed to treat on the subgroup with low prevalence (\modif{$X=0$}) is expected as, even without the treatment, individuals already have a low risk of stroke.
Is it possible to disentangle the baseline variation with the treatment effect in itself? 
Surprisingly, in this example, one measure is constant (or \textit{homogeneous}) over the strata $X$: the Risk Ratio. 
\modif{We will show that among collapsible measures, only the Risk Difference can disentangle in all generality the baseline risk with the treatment effect at both the levels of strata and population. Other causal measures are able to do so only in specific settings (e.g., homogeneous treatment effect). 
%We also show that for a collapsible measure constant on subgroups, one can express the average treatment effect as independent of the baseline risk. 
This is the case in 
the example given in Table~\ref{tab:introduction-diastolic} for the Risk Ratio.%\bc{"Therefore the Table~\ref{tab:introduction-diastolic} example gives the average treatment effect for the Risk Ratio." je ne comprends pas bien cette phrase}\gv{J'ai changé la phrase}
%Other measures can do it only for the conditional average treatment effect. 
For binary outcomes, we exhibit a specific model (inspired from the Russian Roulette) in which natural causal measures to consider are $(i)$ the Conditional Risk Ratio when the effect is beneficial or $(ii)$ the Conditional Survival  Ratio when the effect is detrimental. }
 \sout{show that this measure was the only one expected to behave like this.
More precisely, we will show that depending on the outcome nature and the direction of treatment effect (harmful or beneficial), there \sout{exists one}\modif{can exist} treatment effect measure 
%\es{and only one? This seems a surprisingly strong claim, if so.} 
capable of disentangling the baseline level with the treatment effect itself: the RD for continuous outcome; either the RR or the SR (depending on the direction of the treatment) for binary outcome. 
%If the outcome is continuous, then the RD is the measure enabling such disconnection. 
All other common measures entangle baseline level and treatment effect. 
A by-product of this definition is a non-parametric way to define covariates being treatment effect modulators or only prognostic covariates.}
%This contribution is detailed in Section~\ref{section:generative-models}.%\jj{paragraphe assez dur à comprendre... Si tu dis juste la dernière partie à partir de for each outcome's nature and direction j'ai l'impression que ça suffit, le reste est dur à lire sans le détails???}
%This leads to several contributions.
%In this work, we will show that even if the Risk Ratio has no reasons to be constant when stratified on subgroups, this measure is more likely to behave so if the outcome is binary and if the treatment effect is constantly beneficial (i.e. towards the reduction of occurrences of events).\bc{We show that depending on the nature of the outcome (binary or continuous), each  metric expresses something different. A non-intuitive but important result being this sensitivity to population's shift depends on the nature of the considered outcome.}

%\mybox{Contribution 1: Disentangling the influence of treatment effect modulators from baseline risk}{blue!20}{blue!10}{ 
%For each outcome's nature and direction of treatment effect (harmful or beneficial), it is is possible to prove that one treatment effect measure is able to disentangle the baseline level with the treatment effect itself.  When the outcome is binary, this is either the RR or the SR depending on the direction of the treatment.  If the outcome considered is continuous, then the RD is the measure enabling such disconnection.  All other common measures intricate baseline level and treatment effect.}
%\bc{Learning causal effect in one environment has interest when findings can be generalized to another population, in particular because most studies are conducted with the intention of applying the results elsewhere. \textit{Generalization} can also be found under the name \textit{Transportability}, \textit{Portability}, or \textit{Recoverability}, and refers to the external validity of a study. }\bc{This is particularly important as conclusions that are remembered and shared from clinical trials are causal, and therefore somehow considered as an invariant ground truth\footnote{We refer the reader to \cite{Pearl2000Book} (p. 182), talking about this practice: ``\textit{Once people interpret proportions as causal relations, they continue to process those relations by causal calculus and not by the calculus of proportions}".}. }
%Also, not all measures are collapsibles, and therefore procedure from \eqref{eq:toy-example-collapsibility} is not always valid. 

%To summarize: Aggregating subgroups effects to retrieve a population-effect is a property called collapsibility. Not all measures are collapsible. Collapsibility is necessary for generalization by standardization.

%\mybox{Contribution 2: Collapsibility is necessary for generalization by standardization}{blue!20}{blue!10}{Aggregating subgroups effects to retrieve a population-effect is a property called collapsibility. Not all measures are collapsible. Collapsibility is necessary for generalization by standardization.}

%\paragraph{A collapsible measure is needed to generalize local effects [Section~\ref{sec:generalize}]

%This practice of extending findings from a source population $P_{\text{\tiny S}}$ to a target population $P_{\text{\tiny T}}$ is found under the names \textit{generalization}, \textit{transportability}, \textit{portability}, or \textit{recoverability}. \jj{Il te manque les références}
%Here, one must account for distributional shift across the two populations in  covariates that are prognostic of the outcome. 



\paragraph{Contribution 3: There exist two generalization strategies, via potential outcomes or local effects [Section~\ref{subsection:two_generalization_strategies}]}
%Causal effect measures are not equal when facing population's shift}

\modif{Collapsibility may come into play when one is interested in the population effect on a target population $P_{\text{\tiny T}}$ different from the original source population $P_{\text{\tiny S}}$, \emph{e.g.} with a different proportion of individuals with diastolic pressure ($\forall x \in \{0, 1\},\, p_\text{\tiny S}(x) \neq p_\text{\tiny T}(x) $). 

In Section~\ref{sec:generalize}, we provide two different strategies to generalize causal measures via the generalization of conditional outcomes or local effects. The first approach is valid for any causal measures, whereas the second one may require fewer variables, but can be applied to collapsible measures only (see Contribution 4 below). The second strategy works as follows.} 
\sout{Collapsibility is needed when generalizing local effects to another target population using \eqref{eq:toy-example-standardization}. But which covariates $X$ must be accounted for in \eqref{eq:toy-example-standardization} for the procedure to be valid?}
\modif{Considering the Risk Difference, the average treatment effect $\tau_{\text{\tiny RD}}^{\text{\tiny T}}$ on the target population is given by 
\begin{align}
 \tau_{\text{\tiny RD}}^{P_{\text{\tiny T}}} =   \tikzmarknode{amp}{\highlight{Bittersweet}{ $p_\text{\tiny T}(X=1)$ }}\cdot \tau_{\text{\tiny RD}}^{P_{\text{\tiny T}}} (X=1) +  \tikzmarknode{amp}{\highlight{Bittersweet}{ $ p_\text{\tiny T}(X=0) $}}\cdot\tau_{\text{\tiny RD}}^{P_{\text{\tiny T}}}(X=0),
\end{align}
\begin{tikzpicture}[overlay,remember picture,>=stealth,nodes={align=left,inner ysep=1pt},<-]
\path (amp.north) ++ (-7,-1.8em) node[anchor=north west,color=Bittersweet] (sotext){\textsf{\footnotesize \% individuals with $X=1$} in $P_{\text{\tiny T}}$};
\path (amp.north) ++ (-1.4,-1.8em) node[anchor=north west,color=Bittersweet] (sotext){\textsf{\footnotesize \% individuals with  $X=0$} in $P_{\text{\tiny T}}$};
\end{tikzpicture}\\[.1em]
where $\tau_{\text{\tiny RD}}^{P_{\text{\tiny S}}}(x)$ are local effects in the target population $P_{\text{\tiny T}}$. If we assume that the CATE on the source $\tau_{\text{\tiny RD}}^{P_{\text{\tiny S}}}(x)$ and target population $\tau_{\text{\tiny RD}}^{P_{\text{\tiny T}}}(x)$ are the same, we can swap them into the above equation, giving:}
%\newpage
\begin{flalign}\label{eq:toy-example-standardization}
    \text{Effect on target population}
    &&
    \tau_{\text{\tiny RD}}^{P_{\text{\tiny T}}} =  \tikzmarknode{amp}{\highlight{Bittersweet}{\color{black} $p_\text{\tiny T}(X=1)$ }}\cdot\tau_{\text{\tiny RD}}^{P_{\text{\tiny S}}} (X=1) +  \tikzmarknode{amp}{\highlight{Bittersweet}{\color{black} $p_\text{\tiny T}(X=0)$  }}\cdot\tau_{\text{\tiny RD}}^{P_{\text{\tiny S}}}(X=0).
\end{flalign}
\begin{tikzpicture}[overlay,remember picture,>=stealth,nodes={align=left,inner ysep=1pt},<-]
\path (amp.north) ++ (-7,-1.8em) node[anchor=north west,color=Bittersweet] (sotext){\textsf{\footnotesize \% individuals with $X=1$} in $P_{\text{\tiny T}}$};
\path (amp.north) ++ (-1.4,-1.8em) node[anchor=north west,color=Bittersweet] (sotext){\textsf{\footnotesize \% individuals with  $X=0$} in $P_{\text{\tiny T}}$};
\end{tikzpicture}
\vspace{0.15cm}

\modif{Therefore, a natural procedure to generalize a collapsible causal measure to a target population is to replace the proportions $p_\text{\tiny S}(X=0)$ (resp. $p_\text{\tiny S}(X=1)$) in \eqref{eq:toy-example-collapsibility} by their counterpart $p_\text{\tiny T}(X=0)$ (resp. $p_\text{\tiny T}(X=1)$) computed on the target population. This procedure can be found under various names: \textit{standardization}, \textit{re-weighting}, \textit{recalibration} \citep{miettinen1972standardization, Rothman2000ModernEpidemiology, Bareinboim2014ExternalValidity}. We will call it \textit{generalization}, as it  \sout{related}\modif{follows} the work initiated by \cite{stuart2011use}. \modif{\cite{stuart2011use} explicitly tackles the generalization of a trial with a sample of a target population.} 
We show below that procedure from \eqref{eq:toy-example-standardization} is \modif{theoretically grounded,} \sout{accurate} for collapsible causal measures.}

\paragraph{Contribution 4: All causal measures are not equal when facing a population shift [Section~\ref{sec:generalization}]}
Current line of works usually advocate to adjust on all prognostic covariates being shifted between the two populations. 
\modif{Using Contribution 2 and 3, we will show that the Risk Difference is likely to be more easily generalizable than other causal measures, as it requires less covariates to adjust on (only the shifted treatment effect modulators, and not all shifted prognostic covariates). Other causal measures can be generalized using an extended set of variables via generalization of the conditional outcomes. In some specific settings, e.g. when the treatment effect is homogeneous, some measures can be easily generalized as the Risk Ratio
%\gv{There is an apparent contradiction between the fact that a few sentences before we said that the Risk Difference was more likely to be easily generalizable, and here we show an example with the Risk Ratio where it is easy} 
in Table~\ref{tab:introduction-diastolic}.} 
\sout{Illustration from Table~\ref{tab:introduction-diastolic} taken from a real clinical example is in perfect agreement with our findings.} 
\sout{In this example, the RR is directly generalizable from $P_{\text{\tiny R}}$ to $P_{\text{\tiny T}}$, while the risk difference would need to be adjusted on \modif{all shifted prognostic variables} }
%\sout{$X$}\es{What exactly is X here? You have just said that they are (all?) covariates.}.
%\jj{Plus formule pour RR pour généraliser}
% $X$ is the only prognostic covariate to be shifted between the two populations (called transportability assumption) are both licensing the following generalization procedure:
%where $ \tau_{\text{\tiny RD}}^{\text{\tiny T}}$ is the RD on the target population $P_{\text{\tiny T}}$ and $\tau_{\text{\tiny RD}}^{\text{\tiny S}}(x)$ are local effects in the source population $P_{\text{\tiny S}}$.
%Contribution~2 implies that not all measure are collapsible. 
%Hidden in the transportability assumption, is the fact that $X$ (here the diastolic blood pressure level) captures all the covariates modulating treatment effect that are shifted between the two population. Recall that contribution~1 details that some measure are able to disentangle baseline level from treatment effect itself, when some measure are not.
%Combining these two contributions leads to our key contribution: some measures are likely to be more easily generalizable than others, and in particular are likely to require less covariates for generalization by standardization.  In this illustration, this can be seen as the fact that the RR is directly generalizable from $P_{\text{\tiny R}}$ to $P_{\text{\tiny T}}$. \jj{Euh... là j'ai perdu le raisonnement comme tu nous parlais de RD avant?}%: that some treatment effect measure require less covariates to be generalized, and in particular only the covariates modulating the treatment effect and not the one only involved in the baseline risk.

%\mybox{Contribution 3: Causal effect measures are not equal when facing population's shift}{blue!20}{blue!10}{Some %measures are likely to be more easily generalizable than others, and in particular are likely to require less covariates for generalization by standardization.}



\subsection{Related work: many different viewpoints on effect measures}

\paragraph{The choice of measure, a long debate}
%Medicine and epidemiology lectures discuss at length the different treatment-effect measures \citep{Rothman2000ModernEpidemiology, Cook2014UserGuide}.
The question of which treatment-effect measure 
%\es{It may be helpful to try to define the class of all possible effect measures.} 
is most appropriate (RR, SR, RD, OR, NNT, log-OR, etc) is age-old \citep{Sheps1958ShallWe, Greenland1987Interpretation,Laupacis1988AnAssessmentOfClinically,  Cook1995NNT, Sackett1996DownWO, davies1998can, king2002estimating, Schwartz2006ratio, Cummings2009RelativeMeritsRRAndOR}.
Health authorities advise to report both absolute and relative causal effect \citep[item 17b]{schulz2010consort}\modif{, but in practice public health publications mostly report relative risk \citep{king2012use}}. 
And yet, the question is still a heated debate: in the last 5 years, numerous publications have advocated different practices \citep[see Appendix~\ref{appendix:different-point-of-views} for details]{Spiegelman2017Modeling, Spiegelman2017letSubject, lesko2018considerations, baker2018new, Changyong2019RelationsAmongThreePop, George2020WhatsTR, Doi2020callToChangePractice, doi2022TimeToDoAway, xiao2021odds, xiao2022IsORPortable, Huitfeldt2021ShallWe,Lapointe2022FromMathToMeaning}. 
Most of these works focus on the interpretation of the metrics and simple properties such as symmetry \citep{Cummings2009RelativeMeritsRRAndOR}, heterogeneity of effects \citep{Rothman2011bookEpidemiologyIntrod, VanderWeele2007FourTypes, lesko2018considerations}, or collapsibility \citep{simpson1951interpretation, Whittemore1978Collapsibility, Miettinen1981Essence, Greenland1987Interpretation, pearl1999collapsibility, Cummings2009RelativeMeritsRRAndOR, Greenland2011adjustments, Hernan2011unraveled, Sjolander2016NoteOnNoncollapsibility, Huitfeldt2019collapsible, Daniel2020MakingApple, liu2022rejoinder,Didelez2021collapsibility} --some works discuss the paradoxes induced by a lack of collapsibility without using this exact term, e.g. in oncology \citep{ding2016subgroup, liu2022correct}. We shed new light on this debate with a framing on generalization and non-parametric generative models of the outcome (Section~\ref{section:generative-models}).


\paragraph{Connecting to the generalization literature}

The problem of external validity is a growing concern in clinical research \citep{rothwell2005external,Rothman1013WhyRepresentativeness,Berkowitz2018GeneralizingBlood, Deeks2022IssuesInSelection}, related to
various methodological questions \citep{cook2002experimental, pearl2011transportability}. We focus on external validity concerns due to shifted covariates between the trial's population and the target population, following the line of work initiated in \cite{imai2008misunderstandings} (see their definition of \textit{sample} effect versus \textit{population} effect), or Corollary 1 of \cite{Bareinboim2014ExternalValidity}).
%In other words, our work only considers generalization \underline{by standardization}, i.e. by re-weighting local treatment effect such as in \jj{tu dis ce qu'on considère mais ici on ne sait pas du coup ce qu'on ne considère pas ou ce qu'il y aurait d'autre } \eqref{eq:toy-example-collapsibility}. This formulae is also called the transport formulae or weighting \jj{je ne mettrais pas que pearl ici en référence si?} \citep{Bareinboim2014ExternalValidity}. 
Generalization by standardization 
%\es{This is neither more nor less than a change of measure, where the Radon-Nikodym derivative fully characterizes the reweighting.} 
(\eqref{eq:toy-example-standardization}, \textit{i.e.} re-weighting\footnote{\modif{It can also be seen as a change of measure, where the Radon-Nikodym derivative fully characterizes the reweighting.}}  local effects) has been proposed before  in epidemiology \citep{Rothman2000ModernEpidemiology}, and in an even older line of work in the demography literature \citep{yule1934some}. 
Note that \eqref{eq:toy-example-standardization} is very close to procedure from \eqref{eq:toy-example-collapsibility} which can be linked to post-stratification \citep{imbens2011experimental, miratrix2013adjusting}. Post-stratification is used to lower variance on a randomized controlled trial and therefore has no explicit link with generalization, despite using a similar statistical procedure.
Today, almost all statistical papers dealing with generalization focus on the estimation procedures that generalizes the risk difference $\tau_{\text{\tiny RD}}$ \citep{stuart2011use, tipton2013improving, Muircheartaigh2014GeneralizingApproach, kern2016assessing, lesko2017generalizing, nguyen2018sensitivitybis, Stuart2017ChapterBook, buchanan2018generalizing, dahabreh2020extending, ackerman2020generalization} (reviewed in \cite{Degtiar2021Generalizability, colnet2021causal}), seldom mentioning other measures. 
Other works focus on the generalization of the distribution of the treated outcome $\mathbb{E}\left[ Y^{(1)}\right]$ \citep{pearl2011transportability, Bareinboim2014ExternalValidity, CinelliGeneralizing2019}. A notable exception, \cite{huitfeldt2018choice}, details which choice of variables enables the standardization procedure for binary outcomes.
%While these authors exactly tackle the same question as we do, their conclusions only hold for binary outcomes. Our approach tackles \textit{(i)} a larger scope, while also \textit{(ii)} proposing different notations.
%We extensively generalize the thought process of \cite{huitfeldt2018choice} \jj{Toujours vrai?}. 
\sout{revealing the interplay with choice of measure and role of covariates in heterogeneous settings, as well as continuous outcomes.} %\es{je trouve cette phrase compliquée}bc : ok
%Our conclusions are totally consistent when the scope is that of \cite{huitfeldt2018choice} (i.e. binary outcomes and monotonous effect)\gv{Can we say that, in a sense, our work extends \cite{huitfeldt2018choice}: role of covariates, continuous outcomes}.


\paragraph{Building up on causal research}
By writing the outcomes as generated by a non-parametric process disentangling the baseline from the treatment effect (in the spirit of \cite{robinson1988semiparam, nie2020quasioracle, Gao2021DINA}), we extend the usual assumptions for generalization.
In particular, \cite{Bareinboim2014ExternalValidity} state that their assumptions for generalization are
``\textit{the worst case analysis where every variable may potentially be an effect-modifier}''. Our work proposes more optimistic situations, by introducing a notion of effect-modifier without parametric assumptions. This enables the description of situations where fewer covariates are required for the generalization of certain measures.
%While other works such as \cite{nguyen2018sensitivitybis} highlights that less covariates are necessary when considering the risk difference, this
%\footnote{See complete quote of \cite{Bareinboim2014ExternalValidity} ``\textit{The inferences licensed by Theorem 2 and 3 represent worst case analysis, since we have assumed, in the tradition of nonparametric modeling, that every variable may potentially be an effect-modifier (or moderator). If one is willing to assume that certain relationships are noninteractive, or monotonic as is the case in additive models, then additional transport licenses may be issued}".}. \jj{Alors là je ne comprends rien de ce paragraphe...Trop de Pearl c'est pour ça...} \bc{hihi ! J'ai supprimé !} 
\cite{CinelliGeneralizing2019} have proposed similar ideas, assuming monotonicity of the effect (i.e. the effect being either harmful or beneficial for everyone) \underline{and} the absence of shifted treatment effect modifiers, in order to generalize $\mathbb{E}\left[Y^{(1)} \right]$. More precisely they assume that what they call \textit{probabilities of causation} $\mathbb{P}\left[Y^{(1)} = 0 \mid Y^{(0)} = 1\right]$ are invariant across populations. We relax this assumption to allow more general situations. Doing so, we also extend work from \cite{huitfeldt2018choice, Huitfeldt2019EffectHeterogeneity}, showing how those probabilities are linked with the causal measures of interest. %\jj{peut être être plus précise dans ce paragraphe sur ce que tu veux dire et en quoi tu étends ces travaux?}\bc{oui, je vais finir de rédiger la dernière partie, je laisse ton commentaire}
Interestingly, all our derivations retrieves \cite{Sheps1958ShallWe} intuition and results when the outcome is binary (which was the only situation described by Sheps). Our work also proposes conclusions for a continuous outcome which was not treated by \cite{Sheps1958ShallWe, CinelliGeneralizing2019, Huitfeldt2021ShallWe}.
%Shep's work was advocating different measure depending on the direction of the effect (harmful or beneficial). 
%Our results about which measure is easier to generalize (and therefore less sensitive to population's shift) is in complete accordance with Shep's work.
%Note that Mindel C. Sheps' work has been recently brought back at the front of the stage by \cite{Huitfeldt2021ShallWe}.
%Our work also extends \cite{CinelliGeneralizing2019}'s ``\textit{monotonicity}" assumption allowing transport and in particularly bounds on the transported distribution $\mathbb{E}\left[ Y^{(1)}\right]$ with extended assumptions from \citep{pearl2011transportability, Bareinboim2014ExternalValidity} (in particular using selection diagram). Our work also extends on the approach of \cite{huitfeldt2018choice, Huitfeldt2019EffectHeterogeneity}.
%And note that \cite{Bareinboim2014ExternalValidity, CinelliGeneralizing2019} focus on the estimation of the expectation of the treated potential outcome on the target population, and therefore do not explictly consider the generalization of one causal measure.
%For example, let's consider that a clinician has in mind the result from a trial conducted on a population with distribution $P_1$ stating that the efficacy of the drug on relative scale is $\hat \tau_{\text{\tiny Rel}} = 3$. But the clinician's typical patients are rather following a different distribution $P_2$.
%A intuitive practice is to multiply the baseline of a patient by this relative measure to obtain the estimated outcome under treatment for this patient. But if the clinician was rather provided a study with an absolute measure $\hat \tau_{\text{\tiny Abs}} = 3$, the process would be to add this effect to the baseline. %Generalization is what physician typically do, as they extend what they have learned through previous trials to a new population or a new patient.
%With respect to this practice, it is of major importance that the effect transposed to new situations has grasped something truly causal and somehow invariant. 
%\jj{pas si facile ici de suivre ton cheminement }
%In this work, we will establish that not all causal metrics ensure this practice to be correct, when adjustment has to be made, and when generalization is impossible. The major outcome of this work is to prove that some causal metrics convey a true causal invariant, while some others have to be standardized or when transport is not possible.\bc{phrase pas belle mais idée forte à mettre ici.}

%\subsection{Toy example}
%We show that depending on the nature of the outcome (binary or continuous), each  metric expresses something different.
%A non-intuitive but important result being this sensitivity to population's shift depends on the nature of the considered outcome.

%\begin{wrapfigure}{l}{0.38\textwidth}
%  \begin{center}
%    \includegraphics[width=0.35\textwidth]{fig/toyexamplesummary.png}
%  \end{center}
%  \caption{Synthetic results of a trial from a source population (left) and a target population (right). Different causal effect measures are reported: the Risk Ratio $\tau_{\text{\tiny RR}} = 32/45 = 21/30$ and the risk difference $\tau_{\text{\tiny RD}}$.}
%  \label{fig:toyexamplesummary}
%\end{wrapfigure} A clinical trial (or Randomized Controlled Trial - RCT) was conducted in a source population $P_{\text{\tiny S}}$ to assess the efficacy of a drug preventing death from a head trauma. 
%In particular investigators found 45\% deaths 28 days after admission to hospital in the placebo group and 32\% in the treated group.
%When reporting the so-called treatment effect, several scales (or measures) can be used: for e.g the ratio of the two risks ($\tau_{\text{\tiny RR}} = 0.7$) or the risk difference ($\tau_{\text{\tiny RD}} = -13\%$). 
%This trial soon became the international reference to advocate the treatment's usage, and many hospitals and countries started to use it as a standard-of-care. Years later, another trial was conducted in another population $P_{\text{\tiny T}}$ (for \textbf{t}arget), now revealing $\tau_{\text{\tiny RR}} = 0.7$ and $\tau_{\text{\tiny RD}} = -9\%$ (see Figure~\ref{fig:toyexamplesummary}).
%\textit{What could be the source of such a difference?}
%Because both trials did compare exact same treatment and outcome, investigators suspect the population to be different. Looking at the so-called Table~1, they observe that $P_{\text{\tiny T}}$ contains $20\%$ women (resp. $80\%$ men), while in $P_{\text{\tiny S}}$ is composed of  $50\%$ women and men. To confirm the assumption that the difference comes from the population's composition, investigators estimated the effect of the treatment on the subgroups. They found a constant Risk Ratio of $\tau_{\text{\tiny RR}}(M) = \tau_{\text{\tiny RR}}(F) = 0.7$ on each subgroups but $\tau_{\text{\tiny RD}}(F) = -22$ and $\tau_{\text{\tiny RD}}(M) = -6$ (F for \textbf{f}emale and M for \textbf{m}ale). 
%Note that as soon as the gender is the only baseline predictor accounting for the modulation of the response on the risk difference scale, it would be possible to generalize this measure by standardization (i.e. weighted average of $\tau_{\text{\tiny RD}}(F)$ and $\tau_{\text{\tiny RD}}(M)$ on the population of interest): Such a process is (surprisingly) not possible with all typical measures used by clinicians, such as the odds ratio or the number needed to treat. Why was the Risk Ratio more generalizable in this example? Is this property always expected? In this work we aim to answer such questions. Generalizability (or external validity) of a causal effect is a major property as most of the time clinicians use causal effects from previous trials \textit{(i)} to estimate the expected response to treatment for a specific patient based on his/her baseline risks, \textit{(ii)} to choose which treatment is the best. Therefore, it is of major importance that RCTs convey the most portable measure possible, or at least a measure that can be generalized to another population after standardization such as in \eqref{eq:toy.example.generalization}. This work aims at detailing which measures of effect are easier to generalize to another population.
%\es{je trouve cette partie super et très bien écrite!}\bc{*Ronronnements*}


\section{Causal metrics and their properties}\label{section:causal-metrics-properties}

\textit{This section uses notations introduced in Section~\ref{sec:formalization-and-key-contributions}, in particular the potential outcomes $Y^{(0)}$, $Y^{(1)}$ (which can be either binary or continuous), the binary treatment $A$, and the covariates $X$.} \\
%\textit{By default, all expectations are assumed to refer to a source population $P_\text{\tiny S}$. Only when generalizing (see Section~\ref{sec:generalize}), we also consider a target population $P_\text{\tiny T}$. For example $\tau^{\text{\tiny T}}_{\text{\tiny RR}}$ denotes the RR on the target population.}\\

%To illustrate this arguments, Figure~\ref{fig:big-plot-with-all-metrics} represents the ranges all the metrics from Table~\ref{tab:list-measures} can take when considering a binary outcome. \es{je mettrais la référence à la figure plus tard car elle ne concerne que l'hétérogénéité}
%Beyond the impression, the choice of the measure impacts how we perceive the heterogeneity of the effect. Another important property, the collapsibility, strongly depends on the measure considered. 
%Besides, important properties  while also having different properties such as collapsibility. 
In this section, we ground formally concepts such as homogeneity and heterogeneity of treatment effect, but also collapsibility, and its link to generalization. Those concepts are already described in the literature, via numerous and slightly different definitions  (see   Appendix~\ref{appendix:other-formal-definitions}).  
We unify existing definitions. For clarity, all definitions, assumptions, and lemmas that do not contain an explicit reference in the title are original. 

%\jj{Remodifier l'annexe mais je pense qu'un des reviewers veut une def d'une mesure causale} BC : Ok, merci pour l'ajout.
\modif{\begin{definition}[Causal effect measures -- \cite{Pearl2000Book}]\label{def:causal-measure}
Assuming a certain joint distribution of potential outcomes $P(Y^{(0)}, Y^{(1)})$, which implies that a certain treatment $A$ of interest is considered, we denote by $\tau$ any functional of the joint distribution of potential outcomes. More precisely,

\begin{align}
\mathcal{P}_{(Y^{(0)}, Y^{(1)})} \quad & \rightarrow \mathbb{R} \nonumber \\
\tau : P(Y^{(0)}, Y^{(1)}) & \mapsto \tau^{P},
\end{align}
where $\mathcal{P}_{(Y^{(0)}, Y^{(1)})}$ is the set of all joint distributions of $(Y^{(0)}, Y^{(1)})$.
\end{definition}
This definition is also valid for any subpopulation: for any covariate $X$, the conditional causal effect measure $\tau^{P}(X)$ is defined as a functional of $P(Y^{(0)}, Y^{(1)}\mid X)$.
This definition highlights the fact that a so-called treatment or causal effect naturally depends on \emph{the population considered}.
The notation $\tau^{P}$ highlights this dependency. Note also that this definition could admit many more causal measures than the ones presented in this work.
} 

%\es{exemple ?}
%\jj{aussi le papier \cite{lin2023causal}, desirata d'une causal measure, à voir chercher d'uatres defs}


\subsection{Treatment effect heterogeneity depends on the measure chosen}\label{subsec:homogeneity}

Homogeneity or heterogeneity is linked to how the effects change on population subgroups. %For example, computing the average effect on women and men separately. This is also called stratification. 
If the effect amplitude or direction is different in some subgroups (not due to sampling noise as we only consider the true population's values), the treatment effect is said to be heterogeneous. 
In the literature, one can find several informal definitions of heterogeneity of a treatment effect % \citep[ such as the quote from  recalled in Section~\ref{appendix:other-formal-definitions} ]{Rothman2011bookEpidemiologyIntrod}, 
but formal definitions are scarce. From now on, we let $\mathbb{X}$ be the covariate space.  
%Note that this question of treatment-effect heterogeneity is of major interest when it comes to personalized medicine or when looking for optimal individual treatment regimes.
 %\jj{J'aurais gardé ici, ça dépend de ton positionnement mais si ton papier veut parler à des médecins tu as envie de la garder ++}
%For the rest of the work, we denote $X$ the set of baseline covariates\footnote{We insist on the fact that those covariates are baseline or pre-treatment covariates. See \cite{VanderWeele2007FourTypes} for a detailed explanation.}, and $\tau(x)$ the treatment effect on the subpopulation $X=x$ for any causal measure. Most of the time, the consideration of heterogeneity in applied clinical work is made ``\textit{one-variable-at-a-time}", for e.g. for male and female (in such a situation, $X$ is univariate and binary), even if some literature propose to change this practice to observe hetereogeneity along several covariates at a time \citep{Kent2019PATH1}. Our formalism allows both point of views. First, we define homogeneity as the absence of effect modification on the scale chosen.
%\es{la différence entre treatment effect et causal effect measure n'est pas très claire dans la définition}
%\gv{Derrière ces notions il y a la question de savoir si on peut raisonner à l'échelle individuelle}
%\bc{Gael: je compte mettre dans la partie generalization}
\begin{definition}[Treatment effect homogeneity]\label{def:homogeneity}
A causal effect measure $\tau$ is said to be homogeneous \modif{with respect to the covariate space $\mathds{X}$}, if
\begin{equation*}
   \forall x_1, x_2 \in \mathds{X},\quad \tau^P(x_1) = \tau^P(x_2) = \tau^P.
\end{equation*}
\end{definition}

\begin{definition}[Treatment effect heterogeneity - \cite{VanderWeele2007FourTypes}] \label{def:heterogeneity}
\modif{A causal effect measure $\tau$ is said to be heterogeneous with respect to any set of covariates $X$ if there exist $x_1,x_2$ such that} 
%Assuming a causal measure $\tau$ and a baseline covariate $X$, a treatment effect is said to be heterogeneous with respect to $X$ if,
\modif{
\begin{equation*}
\tau^P(X=x_1) \neq \tau^P(X=x_2) .
\end{equation*}}
\end{definition}
Heterogeneity and homogeneity are properties defined with respect to \textit{(i)}  covariates $X$ and \textit{(ii)} a measure. Claiming \textit{hetereogeneity or homogeneity of a treatment effect} should always be completed by the information about the considered covariates and the  measure under study. For instance in the illustrative example from Table~\ref{tab:introduction-diastolic}, the treatment effect on the Risk Difference scale is heterogeneous with respect to the baseline diastolic blood pressure level $X$, while the treatment effect on the Risk Ratio scale is homogeneous with respect to $X$. 
%\modif{In the recent statistical literature in causal inference, heterogeneity of treatment effect is mostly considered along the Risk Difference scale \citep{wager2018estimation}, even though causal forests have recently been extended to this case \citep{shirvaikar2023targeting}.}
%To be convinced of the latter phenomenon, we refer the reader to Figure 11–1 of \cite{Rothman2011bookEpidemiologyIntrod} (we reproduced a similar plot in Appendix, see Figure~\ref{fig:hetero-schematic}). %\jj{Ici reprendre ton exemple de figure 1 pour dire RR est ici homogène et les autres hétérogènes} ok ! C'est mieux oui.
%\jj{ce qui pourrait manquer avec ça, c'est qu'on a envie que tu dises laquelle fait plus de sens à interpréter et pourquoi  pour le médecin tu lui dis dans ton cas précis toi de regarder quelle mesure et pourquoi? }\bc{Ah par contre là je ne vois pas ce que tu voudrais que j'ajoute}
%\jj{Ou en fait bizarreemnt ce paragraphe n'est pas assez lié à la suite du travail autant collapsibilité tu le dis clairement autant hétérogénéité c'est moins clair}\bc{oui, je vais amélirer cela.}\bc{}
\modif{In Section~\ref{sec:generalization}, we will show that, under some proper assumptions, a homogeneous treatment effect is easily generalizable (Theorem~\ref{thm_homogeneous_independence_generalization}).} 
\sout{Below we link homogeneity of treatment effect with the generalizability of a causal measure.}
%\es{j'ai modifié} \jj{assez loin, à préciser}
%Assessing the heterogeneity of an effect can be done via testing for interaction between covariates and treatment in a model. The link function of statistical model encodes the scale on which the treatment effect heterogeneity is tested \citep{lesko2018considerations}.\gv{Ces deux dernières phrases me paraissent pas coeurs au raisonnement que nous menont, et donc je le retirerais}
%Finally, note that testing for heterogeneity or homogeneity of an effect can be linked with testing for interaction when relying on a linear generative model.%R\bc{Arg Julie? J'ai oublié de mentionner ça. Qu'en dis tu ?}
\subsection{Not all measures are collapsible}\label{subsec:collapsibility}

\paragraph{Intuition}
Collapsibility is intuitively linked to heterogeneity.
Indeed, to investigate for heterogeneity, one looks up the treatment effect on subgroups of the population. 
Collapsibility is the opposite process, where local information is aggregated to obtain a global information (i.e. on a population). 
%\es{This is an elegant way to put it.} 
One might expect the global effect on a population to be an average of the subgroups effects, with weights corresponding to proportions of each subgroup in the target population of interest as in \eqref{eq:toy-example-collapsibility}.
Counter-intuitively, this procedure is valid only for certain causal effect measures.
For example, if the treatment effect is reported as an Odds Ratio, it is possible to find bewildering situations, such as that of the synthetic example detailed on Table~\ref{tab:odds-ratio-simpson}. 
%We propose a synthetic example in Table~\ref{tab:odds-ratio-simpson}. 
In this example, the Odds Ratio is measured on the overall population (Table~\ref{tab:odds-ratio-simpson} (a)) and on the two subpopulations if female ($X=1$) or not ($X=0$) (Table~\ref{tab:odds-ratio-simpson} (b)).
Here, the drug's effect (on the OR scale) is found almost equal on both males (0.166) and females (0.167); however the average effect on the overall population appears \sout{much more efficient} \modif{weaker} (0.26). %\es{Isn't this a *smaller* (i.e. closer to null) odds ratio?}.BC :ok
The Odds Ratio value in the overall population is not even \emph{between} Odds Ratios of sub-populations. The situation mimics a randomized controlled trial conducted with exact population proportions and with $X$ being a covariate, so the phenomenon observed is not an effect of counfounding.\\

\begin{table}[H]
    \centering
    \caption{\textbf{Non-collapsibility of the odds ratio on a toy example}: The tables below represent the exact proportion of an hypothetical population, considering two treatment level $A \in \{0, 1\}$ and a binary outcome. The proportion are as if a randomized controlled trial was conducted on this population. This population can be stratified in two strata: \modif{woman ($X=1$) or not ($X=0$)} \sout{$F \in \{0, 1\}$}. The odds ratio can be measured on (a) the overall population, or on (b) each of the sub-population, namely \sout{$F=0$} \modif{$X=0$} or \modif{$X=1$} \sout{$F=1$}. Surprisingly, on each sub-population the odds ratios are similar, but on the overall population the odds ratio is almost two times bigger than on each sub-population. This example is largely inspired from \cite{Greenland1987Interpretation}, but several similar examples can be found elsewhere, for example in \cite{hernan2020whatifbook} (see their Fine point 4.3) or in \cite{pearl1999collapsibility} (see their Table 1). Another didactic example is provided in \cite{Daniel2020MakingApple} (see their Figure 1), with a geometrical argument.}
    \begin{subtable}{.3\linewidth}
        \centering
        \caption{Overall population, $\tau_{\text{\tiny OR}}  \approx 0.26$}

        \begin{tabular}{c|cc|}
        \cline{2-3}
                          & \multicolumn{1}{c|}{Y=0} & Y=1 \\ \hline
\multicolumn{1}{|c|}{A=1} & 1005                      & 95 \\ \cline{1-1}
\multicolumn{1}{|c|}{A=0} & 1074                       & 26  \\ \hline
        \end{tabular}

    \end{subtable}%
    \begin{subtable}{.6\linewidth}
        \caption{$\tau_{\text{\tiny OR}\mid X=1} \approx 0.167$ and $\tau_{\text{\tiny OR}\mid X=0} \approx 0.166$}
        \centering
\begin{tabular}{c|cc|lc|cc|}
\cline{2-3} \cline{6-7}
\textbf{X= 1}             & \multicolumn{1}{c|}{Y=0} & Y=1 &                       & \textbf{X=0} & \multicolumn{1}{c|}{Y=0} & Y=1 \\ \cline{1-3} \cline{5-7} 
\multicolumn{1}{|c|}{A=1} &              40          & 60  & \multicolumn{1}{l|}{} & A=1          & 965                       &  35 \\ \cline{1-1} \cline{5-5}
\multicolumn{1}{|c|}{A=0} & 80                       & 20  & \multicolumn{1}{l|}{} & A=0          & 994                      & 6  \\ \cline{1-3} \cline{5-7} 
\end{tabular}

    \end{subtable}% 
    \label{tab:odds-ratio-simpson}
\end{table}
%\es{Why $F$?}
This apparent paradox is due to what is called the non-collapsibility\footnote{This definition and phenomenon has been observed long ago by Simpson. See also the \cite{Hernan2011unraveled} for a discussion of Simpson's original paper with modern statistical framework. Note that \cite{Pearl2000Book} (page 176) mentions that collapsibility has been discussed earlier, for example by Pearson in 1899.} of the Odds Ratio. The fact that the average effect on a population could not be written as a weighted sum of effects on sub-populations is somehow going against the ``\textit{implicit assumptions that drive our causal intuitions}'' (\cite{Pearl2000Book}, page 180).
%\es{I agree there is something surprising here, but I don't think it is to do with causal intuitions. Instead, I think it is to do with the fact that we don't always recognise nonlinearity when we see it. We know that E(f(X)) is not necessarily f(E(X)), but the relevance of this is not obvious unless the function f is made explicit.}. 
\modif{Non-collapsibility can also be understood through the non-linearity of a function linking the baseline (control) and response functions, see \ref{sec:appendixORnoncollapsible}.}
On the contrary, an effect measure is said to be collapsible when the population effect measure can be expressed as a weighted average of the stratum-specific measures.
%\citep{simpson1951interpretation}, and described multiple times in articles \citep{Whittemore1978Collapsibility, Miettinen1981Essence, Greenland1987Interpretation, pearl1999collapsibility, Cummings2009RelativeMeritsRRAndOR, Greenland2011adjustments, Hernan2011unraveled, Sjolander2016NoteOnNoncollapsibility, Huitfeldt2019collapsible, Daniel2020MakingApple, Didelez2021collapsibility}, and also outside of the field of causality as this phenomenon can also be encountered for regression models.
Note that non-collapsibility and confounding are two different concepts, as explained in several papers e.g. in \cite{pearl1999collapsibility}\footnote{``\textit{
%Nonetheless, 
the two concepts are distinct: confounding may occur with or without noncollapsibility and noncollapsibility may occur with or without confounding.}"}.
%or more recently in \cite{Daniel2020MakingApple}\footnote{``\textit{We aim to provide an educational summary of issues surrounding (non)collapsibility from a causal inference perspective and to promote the idea that the words `conditional' and `adjusted' (likewise `marginal' and `unadjusted') should not be used inter- changeably.}"}.
%, even if without any context about the covariates status and the treatment assignment regime, collapsibility and confounding can not be dissociated from the association tables. \es{on peut être plus affirmatif ici, si on est vraiment dans une assignation complètement aléatoire non ? }
%In this article, we propose two formal definitions of collapsibility. 
%Those two definitions are streamlining the different formalizations we found in the literature, and aim at unifying them. Indeed, 
\paragraph{Formalizing the problem} In various formal definitions found in the literature (see Section~\ref{appendix:other-formal-definitions}), collapsibility relates to the possibility of writing the marginal effect as a weighted sum of conditional effects on each subgroups. Yet two definitions coexist, depending on whether weights are forced to be equal to the proportion of individuals in each subgroup or not. We outline various definitions and their links below.

\begin{definition}[Direct collapsibility - adapted from \cite{pearl1999collapsibility, Pearl2000Book, liu2022correct, Didelez2021collapsibility}] \label{def:direct-collapsibility}
Let $\tau$ be a measure of effect (see Definition~\ref{def:causal-measure}).
%\es{I do think you need to define this class of objects. For example, in this definition you make it a function of the covariates X, and I suppose it is also a function of the joint distribution P. This also raises the important question of whether an effect measure might always be directly collapsible, or if it depends criticially on the joint distribution P.} \jj{erwan?}
\modif{The measure $\tau$ is said to be directly collapsible with respect to a set of  covariates $X$ if, for all joint distribution $P(Y^{(0)}, Y^{(1)}, X)$, we have}
\begin{equation*}
   \mathbb{E}\left[  \tau^P(X)\right] = \tau^P.
\end{equation*}
\end{definition}


\modif{This definition can be found written sligthly differently in literature, see Definition~\ref{def:strict-collaps-pearl-greenland} in Appendix~\ref{appendix-def_collapsibility}}.
%\sout{, which corresponds to our definition of homogeneity (Definition~\ref{def:homogeneity}) \citep{pearl1999collapsibility, liu2022correct, Didelez2021collapsibility}. A homogeneous treatment effect with respect to $X$ has indeed its marginal effect equal to all subgroups effects. }
%We do not consider such definition as the property of collapsibility, as it rather refers to both a measure property and a peculiar data generative process at hand (where homogeneity is observed along a certain baseline covariate like in Table~\ref{tab:introduction-diastolic}). \es{je ne comprends pas la phrase d'avant}\bc{Je suis d'accord, je fais référence à des travaux qui ont défini collapsibilité autrement. On peut enlever je pense} 
%Our proposed definition of collapsibility rather refers to a measure property only.
%Our direct collapsibility definition encompasses such phenomena.

%\es{On met une référence dans le lemme, si le résultat a déjà été montré?}
\begin{lemma}[Direct collapsibility of the risk difference (RD) -- \citep{pearl1999collapsibility}]\label{lemma:direct-collapsibility-RD}
The Risk Difference $\tau_{\text{\tiny RD}}$ is directly collapsible.
\end{lemma}
%The direct collapsbility is the most intuitive apprach to aggregate local effects to a population effect. 
This result \sout{has been much discussed; it} grounds \eqref{eq:toy-example-collapsibility} in the illustrative example.
In the literature, more flexible definitions of collapsibility can be found, keeping the intuition of the population effect being a weighted sum of effects on subpopulations, with certain constraints on the weights: weights must be positive and \modif{sum to one}. %\es{Positivity seems natural, but what do you mean by normalization?}
\begin{definition}[Collapsibility - adapted from \cite{Huitfeldt2019collapsible}] \label{def:indirect-collapsibility}
\modif{Let $\tau$ be a measure of effect and $\mathds{X}$ the covariate space. \sout{and $X$ a set of covariates.} Let $\mathcal{P}_{(X, Y^{(0)})}$ be the set of all joint distributions of $(X, Y^{(0)})$. The measure $\tau$ is said to be collapsible with respect to the covariate space $\mathds{X}$ if there exists a positive weight function
\begin{align}
\mathds{X} \times \mathcal{P}_{(X, Y^{(0)})} \quad & \rightarrow \mathbb{R}_+ \nonumber \\
w : (X, P(X, Y^{(0)})) & \mapsto w(X, P(X, Y^{(0)})),
\end{align}
satisfying $\mathbb{E}\left[  w(X, P(X,Y^{(0)}))\right] = 1,$
such that, for all joint distributions $P(X, Y^{(0)}, Y^{(1)})$, we have
\begin{equation*}
\mathbb{E}\left[ w(X, P(X,Y^{(0)}))\, \tau^P(X) \right] = \tau^P.
\end{equation*}}
\end{definition}
%\bc{est-ce qu'on garde $g$ pour les poids ? comme apres on a $g_b^{-1}$ ?}
%\es{I think by this quantity you really mean the measure $P$ restricted to the sigma-algebra generated by $X$ and $Y^(0)$, rather than the evaluated value of $P$ at some random variables.} \jj{erwan?}
%In other words, the Definition~\ref{def:indirect-collapsibility} of collapsibility means that $\tau$ is collapsible if it can be written as a weighted average of the conditional associational measures on variable $X$. Note that an alternative writting could have been considered with,
%\begin{equation*}
%   \int_{\mathds{X}} \tau \left(P(Y^{(0)}, Y^{(1)}\mid X\right) w(x)\,dx = \tau \left(P(Y^{(0)}, Y^{(1)})\right). 
%\end{equation*}
%Note that would Definition~\ref{def:indirect-collapsibility} allows for the weights to depend on $Y^{(1)}$, then all measures would be collapsible with weights also depending on the joint distribution $P(X, Y^{(0)}, Y^{(1)})$. The weights would corresponds to $\tau/\tau(X)$. This is why, one has to put constraints on the weights in Definition~\ref{def:indirect-collapsibility} to respect the idea of a re-standardization of the local effect to the global effect, where 
Note that here weights depends on the density of $X$ and the distribution of controls $P(X, Y^{(0)})$. 
%\es{Why is this important? Why should it not also depend on the joint distribution with $Y^{(1)}$?} BC: reponse dessous
%\jj{On a envie de savoir pourquoi tu dis ça}\bc{je ne suis pas moi même convaincue de mon explication... qu'en dis tu ?}
The direct collapsibility is therefore a specific case of the more general version of collapsibility from Definition~\ref{def:indirect-collapsibility}, where $ w(X, P(X,Y^{(0)}))$ corresponds to $1$. 
\sout{This definition enables more treatment effect measures to be collapsible.} 
%\es{But I don't think this in itself is a good indicator of whether the definition is appropriate: it is more important that collapsibilty is somehow an important or interesting property than that it is shared by many effect measures.} 
\modif{Allowing the weights to depend on the joint distribution of the covariates and the two potential outcomes would lead to all measures being collapsible. Besides, if one had access to the joint distribution of $(X, Y^{(0)}, Y^{(1)})$, one could generate and generalize any causal measure. We choose to consider weights that depend on $Y^{(0)}$ (instead of $Y^{(1)}$) as accessing the distribution of $Y^{(0)}$ (control cases) may be easier in practice.}
\begin{lemma}[Collapsibility of the Risk Ratio and survival ratio - extending \cite{ding2016subgroup, Huitfeldt2019collapsible, Didelez2021collapsibility}]\label{lemma:collapsibility-of-RR-SR}
The Risk Ratio and survival ratio are collapsible measures. In particular, \modif{for any set of covariates $X$}, assume that, almost surely, $0 < \mathbb{E}[Y^{(0)}|X] < 1$. Then, the conditional Risk Ratio and conditional survival ratio \modif{exist} and satisfy %\jj{est-ce qu'on les numeroterait pas? tu ne les utilises pas après?}\bc{je peux dire "formula of Lemme 2" ?}
\begin{equation*}
    \mathbb{E}\left[ \tau_\text{\tiny RR}^P(X) \frac{\mathbb{E}\left[Y^{(0)} \mid X \right]}{\mathbb{E}\left[Y^{(0)}\right]}\right] = \tau_\text{\tiny RR}^P \qquad \textrm{and} \qquad \mathbb{E}\left[\tau_\text{\tiny SR}^P(X) \frac{1-\mathbb{E}\left[ Y^{(0)} \mid X \right]}{1-\mathbb{E}\left[ Y^{(0)}\right] }\right] = \tau_\text{\tiny SR}^P.
\end{equation*}
%\es{first equation. This is an interesting result, and elegant in its simplicity. But do you consider it important to know the actual form of the weights? Or is it really their existence that is essential? When and how would you use the weights in practice?} \es{est-ce qu'on ne renverrait pas à la section sur la généralisation ?}BC : oui ! :)
\end{lemma}
%\bc{Au début de cette partie on dit qu'on ne marque pas la dépendance à une distribution, et ici on laisse $P$. On est bien d'accord qu'on le laisse ? Parce que je trouve presque que ça peut rendre la lecture difficile car les espérances de $Y^{(0)}$ sont aussi sur $P$.}
\modif{Knowing the actual form of the weights will be very helpful when coming to generalization in Section~\ref{sec:generalization}.}
The proof of collapsibility for the Risk Ratio, the Survival Ratio and other causal measures are established in Appendix~\ref{proof:collapsibilty}. 
%Note that  introduced in Appendix (see Section~XX) follows directly from the collapsibility of $ \tau_\text{\tiny RR}$ and $ \tau_\text{\tiny SR}$.
Note that results of Lemma~\ref{lemma:collapsibility-of-RR-SR} are already presented in \cite{Huitfeldt2019collapsible} or \cite{ding2016subgroup} (see their Equation 2.3) 
%also recall these results 
but only for a binary outcome and categorical  covariate\footnote{Note that this result can be found under slightly different forms such as in \cite{huitfeldt2018choice, Didelez2021collapsibility}, with a categorical $X$ and using Bayes formula, $\tau_\text{\tiny RR} = \sum_x \tau_\text{\tiny RR}(x)\, \mathbb{E}\left[X=x \mid Y^{(0)} = 1 \right]$.}.   
Thus, Lemma~\ref{lemma:collapsibility-of-RR-SR} extends their results \modif{for any set of covariates $X$ (including categorical and continuous variables) and any type of outcome $Y$ (continuous or binary).}
%Lemma~\ref{lemma:collapsibility-of-RR-SR} is consistent with the illustrative example in Table~\ref{tab:introduction-diastolic} where   
%\begin{equation*}
%   \tau_{\text{\tiny RR}}^P = \mathbb{E}\left[ \tau_\text{\tiny RR}^P(X) \frac{\mathbb{E}\left[Y^{(0)} \mid X \right]}{\mathbb{E}\left[Y^{(0)}\right]}\right]  = \mathbb{E}\left[ 0.6\,\frac{\mathbb{E}\left[Y^{(0)} \mid X \right]}{\mathbb{E}\left[Y^{(0)}\right]}\right] =  0.6\cdot \underbrace{\mathbb{E}\left[\frac{\mathbb{E}\left[Y^{(0)} \mid X \right]}{\mathbb{E}\left[Y^{(0)}\right]}\right]}_{=1} = 0.6.
%\end{equation*}


\begin{lemma}[Non-collapsibility of the OR, log-OR, and NNT, based on \cite{Daniel2020MakingApple}]\label{lemma:non-collapsibility}    
The odds ratio $\tau_{\text{\tiny OR}}$,  log odds ratio $\tau_{\text{\tiny log-OR}}$, and Number Needed to Treat $\tau_{\text{\tiny NNT}}$ are non-collapsible measures.
\end{lemma}
%\es{mentionner ici que la preuve pour OR et log OR n'est pas nouvelle, avec la référence}
 The proof is in Appendix~\ref{proof:collapsibilty}. 
  \sout{Note that the proof for the Odds Ratio and the log Odds Ratio are not new (in particular we recall the one from \cite{Daniel2020MakingApple}).}
  While the non-collapsibility of the odds ratio and the log Odds Ratio have been reported multiple times \citep[see, e.g.,][and the example from Table~\ref{tab:odds-ratio-simpson}]{Daniel2020MakingApple}, we have not found references stating results about the NNT. 
  
  When considering the OR, the marginal effect $\tau$ can be smaller or bigger than the range of local effects $\tau(x)$. 
 \modif{Accordingly, \cite{liu2022correct} introduces the term \textit{logic} for such characteristic.}

\begin{definition}[Logic-respecting measure -- \cite{liu2022correct}]\label{def:logic-respecting}
A measure $\tau$ is said to be logic-respecting if, \modif{for any set of  covariates $X$ and any distribution $P(X, Y^{(0)}, Y^{(1)})$,} %\es{il ne faut pas rajouter ``with respect to a set of baseline covariates''?}
\modif{
\begin{equation*}
    \tau^P \in \left[\min_{x}(\tau^P(x)), \;\; \max_{x}(\tau^P(x)) \right].
\end{equation*}}
%\es{I quite like the feel of this term, but wonder if "logic" is the right word. What about "boundary-respecting"? The reason I say this is that, of course, you have demonstrating (through use of logic) that many effect measures are not "logic-respecting".}
%\jj{We kept logic-respecting as it was already used in some works.}
%BC: j'ai ajoute une phrase
\end{definition}

\begin{lemma}[All collapsible measures are logic-respecting, but not the opposite]\label{lemma:logic-respecting-measures} Several properties can be noted:
\begin{itemize}
    \item \textit{(i)} All collapsible measures are logic-respecting measures.
    \item \textit{(ii)} The Number Needed to Treat is a logic-respecting measure.
    \item \textit{(iii)} The OR and the log-OR are not logic-repecting measures.
\end{itemize}
%\es{vérifier que c'est bien cohérent par rapprot aux définitions $X$ ou sans $X$}
\end{lemma}

The proof is in Appendix~\ref{proof:logic-respecting}. While the NNT is not collapsible, this measure does not show the same paradoxical behavior as the OR  (see Table~\ref{tab:odds-ratio-simpson}). \modif{This is due to the fact that the NNT results from a monotonic transformation of the RD, which is collapsible (see Lemma \ref{lemma:annexe_monotinic_causal_measure} in Appendix~\ref{app:supplementary_lemma}).} 
%\jj{là j'enlève tes interprétations de c'est peut être parce que c'est une mesure récente, du coup tu peux dire quelque chose sur NNT?. }
%What can be observed on the OR can be seen as against \textit{logic}. 
The numerous mentions of paradoxes with the OR are probably more driven by its non logic-respecting property than by its non-collapsibility. %\es{Is it the fact that weights are restricted to depend only on the joint distribution of exposure and control that makes these two non-equivalent? Put another way, if you allowed the weights to be arbitrary, then would collapsibility and logic-respecting be the same thing? This relates to my earlier question about whether knowing the weights is important.} \jj{J'ai pas l'impression} \es{absolument pas, car dans ce cas, toutes les mesures sont collapsibles, mais certaines ne sont pas logic-respecting} BC : oui, je vous propose de ne rien changer dans le texte mais de lui  répondre directement
This also probably explains why some definitions of collapsibility proposed in the literature do not explicitly separate the notions of collapsibility and logic-respecting  as they do not detail how weights are defined (see for example Definitions~\ref{def:collapsibility-huitfeldt} or \ref{def:collapsibility-didelez} in Appendix). All properties of this section are summarized in Table \ref{tab:small-summary-measures}.

\begin{figure}[!h]
	\begin{minipage}{.30\linewidth}
    \captionof{table}{\textbf{Causal measures and their properties}: highlighting the properties of collapsibility (Definition~\ref{def:indirect-collapsibility}) and logic respecting (Definition~\ref{def:logic-respecting}). An exhaustive table is available in Appendix (see Table~\ref{tab:list-measures-with-all-properties}).}
  \label{tab:small-summary-measures}
    \end{minipage}
    \hspace{0.3cm}
    \begin{minipage}{.65\linewidth}
 \begin{center}
 {\footnotesize 
\begin{tabular}{|
>{\columncolor[HTML]{ECF4FF}}c |
>{\columncolor[HTML]{E0FAE0}}c |
>{\columncolor[HTML]{E0FAE0}}c |}
\hline
\cellcolor[HTML]{CBCEFB}\textbf{Measure} & \cellcolor[HTML]{CBCEFB}\textbf{Collapsible} & \cellcolor[HTML]{CBCEFB}\textbf{Logic-respecting} \\ \hline
Risk Difference (RD)                                       & Yes                                          & Yes                                               \\
Number Neeeded to Treat (NNT)                                      & \cellcolor[HTML]{FCF1F1}No                                           & Yes                                               \\
Risk Ratio (RR)                                       & Yes                                          & Yes                                               \\
Survival Ratio (SR)                                       & Yes                                          & Yes                                               \\
Odds Ratio (OR)                                       & \cellcolor[HTML]{FCF1F1}No                   & \cellcolor[HTML]{FCF1F1}No                        \\ \hline
\end{tabular}
}
\end{center}

	\end{minipage}
\end{figure}



\modif{
\section{Disentangling the treatment effect from the baseline}\label{section:generative-models}}

%Without any assumption on the generative model, except a zero-mean error model, one can propose to consider the outcome $Y$ as being generated from a very general working model. 
We now propose to reverse the thinking: rather than starting from a given metric, we propose to reason from generic \modif{non-parametric} generative models (for continuous and binary outcomes). Such models allow us to disentangle covariates that affect only baseline level from those that modulate treatment effects. %\bc{je couperais la dernière phrase} Consequently, we express each causal measure as a function of the baseline and the treatment effect modulation, showing that some measures depend on the treatment effect only (and not on the baseline). 
%and observe what each metric expresses. Doing so, we are able to 
%
%The goal of this section is to introduce another deﬁnition of heterogeneity of effects, proposing to distentangle covariates affecting only baseline level and covariates modulating treatment effects. 
This disentangling phenomenon
%(that we call disentanglement) 
will be used later on in Section~\ref{sec:generalize}  to determine which measures are easier to generalize.
%\es{j'ai modifié ici}\bc{ok! merci !}
%observe which transportability assumptions is more likely to hold.
\sout{Note that our generative models are very general, since no parametric assumptions are made. }%Consequently, the conclusions drawn from our analysis are thought to be valid for a broad class of applications. 
\sout{As the models depend on the nature of the outcome considered, this section is organized accordingly.}


%This will allow to explain how the subgroup effect from the illustrative example in Table~\ref{tab:introduction-diastolic} moves with baseline level.
%This enables to observe how causal measures are dependent (or not) with the baseline level. %distentangle covariates affecting only baseline level and covariates modulating treatment effects. %Doing so, we link the heterogeneity of effect (Definition~\ref{def:heterogeneity}) %: showing that some effects are heterogeneous when conditoned on any prognostic covariates, while some subgroups effects are only modulated along covariates modulators of the treatment effect.

%Finally, not that our generative models are very general, since no parametric assumptions are made. %Consequently, the conclusions drawn from our analysis are thought to be valid for a broad class of applications. %\jj{Il faut dire quand même ici encore à quoi ça te servira en particulier dans une section 5}
%As the models depends on the nature of the outcome considered, this section is organized accordingly.
%\jj{Ici dire le but de cette section déméler les effets} ok phrase ajoutée.


\modif{\subsection{One generative model per causal measure}}

\modif{
%\subsubsection{Main results}

In this section, we consider causal measures $\tau$ such that there exists a function $f: D_f \to \mathds{R}$ defined on $D_f \subset \mathds{R}^2$, verifying, for all distributions $(Y^{(0)}, Y^{(1)}) | X$, and for all $x \in \mathds{X}$ such that $\left(\mathds{E}[Y^{(0)} \mid X=x ] , \mathds{E}[Y^{(1)} \mid X=x ] \right) \in D_f$, 
\begin{align}
\tau^P (x) = f \left(\mathds{E}[Y^{(0)} \mid X=x ] , \mathds{E}[Y^{(1)} \mid X=x ] \right). \label{eq_causal_measure} 
\end{align}
All causal measures presented in Section~\ref{subsec:causal-measures-presentation} satisfy \eqref{eq_causal_measure}. For example, the function $f$ associated to the risk difference is simply $f: (z,z') \mapsto z'-z$ with $D_f = \mathds{R}^2$. 
%\bc{les variables sont muettes, cependant avec b(x) après j'aurais eu tendance à mettre $z_1$ et $z_2$, ok pour vous ?}
If $\left(\mathds{E}[Y^{(0)}] , \mathds{E}[Y^{(1)} ] \right) \in D_f$, the average treatment effect for the measure defined in \eqref{eq_causal_measure} is simply given by 
\begin{align}
\tau^P = f \left(\mathds{E}[Y^{(0)}] , \mathds{E}[Y^{(1)} ] \right). \label{eq_causal_measure_average}  
\end{align}


\begin{assumption}[Injectivity]
\label{ass:injection_def_domain}
Let $\tau$ be a causal measure defined in Equation~\eqref{eq_causal_measure}. Let, for all $z \in D_f^{(1)}$, 
\begin{align}
\begin{array}{cclc}
     g_z :  & D_f^{(2)}(z) & \to & \mathds{R} \\
     &  z' & \mapsto & f(z,z'),
\end{array}
\end{align} 
where $D_f^{(1)} = \{z_1, \exists z' \in \mathds{R} \textrm{ such that } (z_1,z') \in D_f\}$ and $D_f^{(2)}(z) = \{z', (z,z') \in D_f\}$. Assume that, for all $z \in D_f^{(1)}$, $g_z$ is an injection. 
\end{assumption}

Such an assumption, stating that $g_z$ is an injection, is mild: if this was not the case, two different values of $\mathds{E}[Y^{(1)}|X]$ would lead to the same CATE for a given baseline $\mathds{E}[Y^{(0)}|X].$ 

Using the binary nature of $A$, it is possible to decompose the response $Y$ in two parts: baseline level and modification induced by the treatment. Such decomposition is generic and does not rely on any parametric assumptions.

\begin{lemma}
\label{lem_generative_models}
Let $\tau$ be a causal measure defined in Equation~\eqref{eq_causal_measure} satisfying Assumption~\ref{ass:injection_def_domain}. Then, for all distributions $(Y^{(0)}, Y^{(1)}) | X$, there exist two unique functions $b,m : \mathds{X} \to \mathds{R}$ such that, for all $x \in \mathds{X}$ such that 
\begin{align}
\label{eq_def_domain_lemma}
\left(\mathds{E}[Y^{(0)} \mid X=x ] , \mathds{E}[Y^{(1)} \mid X=x ] \right) \in D_f,
\end{align}
we have
\begin{align}
    \mathds{E}[Y^{(0)} | X =x ] = b(x) \quad \textrm{and} \quad \mathds{E}[Y^{(1)} | X=x ] = g_{b(x)}^{-1} (m(x)). \label{eq_gen_model_lemma}
\end{align}
Under the model defined in~\eqref{eq_gen_model_lemma}, for all $x$ satisfying \eqref{eq_def_domain_lemma}, 
\begin{align}
    \tau^P(x) = m(x).
\end{align}
\end{lemma}
The proof can be found in Appendix~\ref{app_subsection_proof_Lemma1_genmodels}. Lemma~\ref{lem_generative_models} shows that for any causal measure, there exists an appropriate generative model such that, under this model, the conditional causal measure captures the treatment effect $m(x)$.
Applying Lemma~\ref{lem_generative_models} to the Risk Difference leads to the following Corollary.} 

\modif{
\begin{corollary}
\label{lemma:working-model-continuous-Y}
Consider the Risk Difference. In the framework of Lemma~\ref{lem_generative_models}, we have $b(X) = \mathds{E}[Y^{(0)}|X]$. Besides, $g_z(z') = z' - z$ and $g_z^{-1}(z') = z' + z,$ which leads to
\begin{align}
   \mathds{E}[Y^{(1)}|X] = m(X) + b(X).
\end{align}
Such a model can also be written as, for all $a\in \{0,1\}$, 
\begin{align}
    \mathds{E}[Y^{(a)}|X]  = b(X) + a m(X). \label{corollary_decomposition_linear}
\end{align}
%that $\mathbb{E}\bigl[|Y^{(1)}| \,\bigl| X\bigr] < \infty$ and $\mathbb{E}\bigl[|Y^{(0)}|\, \bigl| X\bigr] < \infty$, there exist two functions $b, m:\mathcal{X} \to \mathbb{R}$ such that
% \begin{align*}
% Y^{(a)} = b(X) + a\,m(X) + \varepsilon_a,
% \end{align*}
% where $ b(X) :=\mathbb{E}[Y^{(0)} | X]$, $ m(X) :=\mathbb{E}[Y^{(1)}-Y^{(0)} | X]$ and a \modif{residual} \sout{noise} $\varepsilon_A$ %\es{I would prefer the term 'residual'.}  satisfying $\mathbb{E}\left[ \varepsilon_A \mid X \right] = 0$ almost surely. 
Besides, we have $\tau_{\text{\tiny RD}}^P(X) = m(X)$, 
 \begin{equation*}
     \tau_{\text{\tiny RD}}^P= \mathbb{E}\left[ m(X)\right],\qquad 
     \tau_{\text{\tiny RR}}^P = 1 + \frac{\mathbb{E}\left[m(X) \right]}{\mathbb{E}\left[b(X) \right]},\qquad \text{and}\quad 
     \tau_{\text{\tiny ERR}}^P = \frac{\mathbb{E}\left[m(X) \right]}{\mathbb{E}\left[b(X) \right]}.
 \end{equation*}
\end{corollary}}
%\es{For a positive-valued $Y$, I think there exists an equally universal *multiplicative* decomposition of $Y^(a)$. In this case the (multiplicative) residuals would have mean 1. In such a scenario it would be the RR and ERR that would be homogeneous, and the RD would not be. I support what you are trying to do here (thinking generatively) but I don't see that your decomposition is in any sense unique.} \jj{à rediscuter tous ensemble}
%\bc{volontiers !}

\modif{
The formula in \eqref{corollary_decomposition_linear} is related to the Robinson \citep{robinson1988semiparam}  decomposition  \modif{\citep[see also][for a completely linear model]{angrist2008mostlyharmless}}.
This model allows to interpret the difference between the distributions of treated and control groups as the alteration $m(X)$ of a generative model $b(X)$  by the treatment. The function $b$ corresponds to the \textbf{b}aseline, and $m$ to the \textbf{m}odifying function due to treatment. Figure~\ref{fig:alteration.png} gives the intuition backing \Cref{lemma:working-model-continuous-Y}.



\begin{figure}[!h]
    \begin{minipage}{.34\linewidth}
	\caption{\textbf{Intuition behind \eqref{corollary_decomposition_linear}}: This illustration highlights that, for a given set of covariates $X$, one can assume that there exist two functions accounting for the expected outcome value for any individual with baseline characteristics $X$. Then, it is possible to denote $m(X)$ as the alteration or \textbf{m}odification of the \textbf{b}aseline $b(X):=  \mathbb{E}[Y^{(0)} \mid X]$ response.}
	\label{fig:alteration.png}
    \end{minipage}%
    \hfill%
    \begin{minipage}{.65\linewidth}
    \includegraphics[width=0.8\textwidth]{fig/alteration.png}
    \end{minipage}
\end{figure}



%From this non-parametric model \jj{phrase vague?}, one can observe what each causal metric captures each of these functions.
%\begin{lemma}[Expression of the causal measures]\label{lemma:expression-of-causal-quantities-under-generative-model-continuous-outcome}
%Under the assumptions of  Lemma~\ref{lemma:working-model-continuous-Y}, 
%\end{lemma}
%Proof is in appendix~\ref{proof:lemma-causal-quantities-continuous-outcome}. 
Corollary~\ref{lemma:working-model-continuous-Y} also illustrates how the relative measures $ \tau_{\text{\tiny RR}}$ and $ \tau_{\text{\tiny ERR}}$ depend on both the effect $m(X)$ \textbf{and}  the baseline $b(X)$. On the contrary, $ \tau_{\text{\tiny RD}}^P$ and $ \tau_{\text{\tiny RD}}^P(X)$ are independent of the baseline $b(X)$. \\

Based on \Cref{lem_generative_models}, 
one can make explicit the generative model associated to any causal measure. In particular, the Conditional Odds Ratio equals  the treatment effect in the logistic model (see Section~\ref{appendix:usual-point-of-view}). 
Below, we detail the generative model associated to the Risk Ratio. }

\modif{
\begin{corollary}
\label{lemma:working-model-continuous-Y-RR}
Consider the Risk Ratio. In the framework of Lemma~\ref{lem_generative_models}, we have $b(X) = \mathds{E}[Y^{(0)}|X]$. Besides, $g_z(z') = z'/z$ and $g_z^{-1}(z') = z z',$ which leads to
\begin{align}
   \mathds{E}[Y^{(1)}|X] = b(X) m(X).
\end{align}
Such a model can also be written as, for all $a\in \{0,1\}$, 
\begin{align}
    \mathds{E}[Y^{(a)}|X]  = b(X) (m(X))^a.
\end{align}
Consequently, we have
 \begin{equation}
     \tau_{\text{\tiny RR}}^P(X) = m(X) \quad \textrm{and} \quad \tau_{\text{\tiny RR}}^P= \frac{\mathbb{E}[Y^{(1)}]}{\mathbb{E}[Y^{(0)}]} = \frac{\mathbb{E}\left[  b(X) m(X)\right]}{\mathbb{E}\left[  b(X)\right]}.
     %,\qquad 
     %\tau_{\text{\tiny RD}} = \mathbb{E}[b(X) (m(X)-1)],\qquad \text{and}\quad 
     %\tau_{\text{\tiny ERR}} = \frac{\mathbb{E}\left[  b(X) (m(X)-1) \right]}{\mathbb{E}\left[  b(X)\right]}.
 \end{equation}
\end{corollary}

Under the generative model associated with the Risk Ratio (as stated in Corrolary~\ref{lemma:working-model-continuous-Y-RR}), the conditional Risk Ratio captures the treatment effect, but the Risk Ratio computed on the overall population depends on the baseline: the Risk Ratio is unable to disentangle the treatment effect from the baseline both at a strata level and at the population level. Finally Appendix~\ref{appendix:usual-point-of-view} proposes a comment about the logistic regression model which is usually used in applied statistics.
}


\modif{
\subsection{Only the Risk Difference can disentangle the treatment effect from the baseline}



Lemma~\ref{lem_generative_models} seems to suggest that all causal measures are equivalent, in the sense that they can all capture the conditional treatment effect modification, provided a suitable generative model. 

While any CATE $\tau(x)$ is able to disentangle the treatment effect from the baseline provided a suitable generative model, it is not the case for the ATE. In fact, among all collapsible measures, only linear causal measures are able to disentangle the baseline from the treatment effect modification both at a conditional level (CATE) \textit{and} for the overall population (ATE).

\begin{theorem}
\label{th_onlyRD_separates_baseline_tteffect}
Let $\tau$ be a collapsible (see Definition~\ref{def:indirect-collapsibility}) causal measure defined in Equation~\eqref{eq_causal_measure} satisfying Assumption~\ref{ass:injection_def_domain}.
%
Assume that for all distributions $P(X)$ of $X$, and for all functions $h: \mathds{X} \to f(D_f)$, there exists $C_{P(X), h} \in \mathds{R}$ such that, for all distributions $Y^{(0)}|X$ satisfying $\forall x \in \mathds{X}, \mathds{E}[Y^{(0)} | X = x] \in D_f^{(1)}$, there exists a distribution $Y^{(1)}|X$ such that 
\begin{itemize}
    \item for all $x \in \mathds{X}, \tau^P(x) = h(x)$
    \item $\tau^P = C_{P(X), h}$.
\end{itemize}
%both the conditional causal measure $\tau(X)$ and the average causal measure $\tau$ are independent of the conditional baseline distribution $Y^{(0)}|X$. 
Then, there exist $a,b,c \in \mathds{R}$ such that, for all distributions $P(X, Y^{(0)}, Y^{(1)})$,
\begin{align}
    \tau^P(X) = a \mathds{E}[Y^{(1)}|X] + b \mathds{E}[Y^{(0)}|X] + c.
\end{align}
\end{theorem}
%\es{modifier ici et sous le théorème 2. Est-ce clair ? }  bc : ok pour moi, merci !
The proof is postponed to Appendix~\ref{app_subsection_proof_thm1_genmodels}. Theorem~\ref{th_onlyRD_separates_baseline_tteffect} shows that up to renormalization, the Risk Difference is the only causal measure capable of separating the treatment effect from the baseline, both at a strata and population level.  
Indeed, the assumption inside Theorem~\ref{th_onlyRD_separates_baseline_tteffect} states that both the CATE and the ATE of the causal measure $\tau$ are independent of the baseline. Under this assumption, Theorem~\ref{th_onlyRD_separates_baseline_tteffect} establishes that $\tau$ is the Risk Difference (up to constant factors). Thus the Risk Difference  is the only causal measure able to capture the treatment effect modification, regardless of the baseline value. This holds for \textit{any} form of the treatment effect (the function $h$ can be arbitrary chosen) independent of the baseline.  While the assumption inside Theorem~\ref{th_onlyRD_separates_baseline_tteffect} may appear to be strong, it is valid in the generative models presented in Corollary~\ref{lemma:working-model-continuous-Y}, if the choice of the function $m$ does not depend on the function $b$. }
%Such a result is particularly interesting if the treatment effects (both CATE and ATE) are independent of the baseline distribution $Y^{(0)}|X$, that is the treatment has the same effect across populations with different baseline distributions. In this case, the Risk Difference  is the only causal measure able to capture the treatment effect modification, regardless of the baseline value.}





%\paragraph{Comment on the non-parametric model for a continuous outcome}

%\paragraph{Comment: Linear generative model} %\jj{je dirais plus un c as particulier est un modèle paramétriuque complètement linéaire et dans ce cas on retrouve}
%    A decomposition such as in Lemma~\ref{proof:lemma:working-model-continuous-Y} is often used in the literature. For example, many applied works or introductory books \citep{angrist2008mostlyharmless} propose completely linear models such as \es{Yes, but equally, many authors argue that linear models are very restrictive!}
%\begin{equation}\label{eq:typical-model-used-continuous-Y}
%    \mathbb{E}[ Y \mid X, A] =  \beta_0 + \langle \boldsymbol{\beta}, \boldsymbol{X} \rangle  + A\,m,
%\end{equation}
%where $m(X) := m $ is a constant and $b(X)$ a linear model of the covariates. Assuming this model as the true generative model, Lemma~\ref{lemma:expression-of-causal-quantities-under-generative-model-continuous-outcome} leads to

% \begin{equation*}
%     \tau_{\text{\tiny RD}}= m,\qquad 
%     \tau_{\text{\tiny RR}} = 1 + \frac{m}{\beta_0 + \langle\boldsymbol{\beta}, \mathbb{E}\left[ X\right] \rangle},\qquad \text{and}\quad 
%     \tau_{\text{\tiny ERR}} = \frac{m}{\beta_0 + \langle\boldsymbol{\beta}, \mathbb{E}\left[ X\right]\rangle}.
% \end{equation*}
%This illustrates the Lemma~\ref{lemma:expression-of-causal-quantities-under-generative-model-continuous-outcome} on a typical example.
%As expected, one can recover that under such model, $  \tau_{\text{\tiny RD}}$ is homogeneous according to Definition~\ref{def:homogeneity}, while $ \tau_{\text{\tiny RR}}$ and $ \tau_{\text{\tiny ERR}}$ are not.
%This explain why, under appropriate causal assumptions and parametric (here linearity) assumption, the parameter $m$ is sometimes said to be causal.\\


%This highlights why \eqref{eq:typical-model-used-continuous-Y} ensures that under the RD measure the effect is homogeneous as it does not depend on $X$ anymore. \jj{là je te suis pas} \bc{oui pas clair}\bc{Marine dit qu'ici on a envie d'en savoir plus, sur comment "attraper" le m(X) quelque soit sa forme. "Regression adjustment"}
%Note that  Here we only highlight that very light assumptions are required. Also note that a usual assumption made is the one of treatment effect homogeneity along with a linear generative model, corresponding to
%\es{Peut-être écrire les mesures plus haut dans le cas de ce modèle linéaire, pour bien montrer que seul $\tau_{RD}$ est indépendant de $b$?}
%\jj{Ce qui manque c'est OK tu ecris RD, RR, ERR mais tu commentes pas, il faut des exemples, ça implique quoi en pratique selon les valeurs de baseline?} 

\modif{

%Determining if the treatment effect $m$ can be considered as independent of the baseline $b$ is application-specific. However, we believe it is important to make explicit the behaviors of the different causal measures in this context, which we did in the previous section by showing that the Risk Difference is the only collapsible measure able to disentangle the treatment effect from the baseline at an individual level (CATE) and population level (ATE). \jj{c'est dur de voir ton cheminement}

\paragraph{Case of bounded outcomes}
Let us consider a specific application in which the outcome and the potential outcomes are bounded.
In this setting, the Risk Difference is not able to disentangle the treatment effect from all values of the baseline. More precisely, we assume that, for all $x \in \mathcal{X}$, 
\begin{align}
& c_1(x) = \min \left(\mathds{E}\left[Y^{(0)}|X=x\right], \mathds{E}\left[Y^{(1)}|X=x \right]\right) > 0 \\
\textrm{and } & c_2(x) = \max \left(\mathds{E}\left[Y^{(0)}|X=x\right], \mathds{E}\left[Y^{(1)}|X=x \right] \right) < \infty.   
\end{align} 
%Hence, almost surely, $c_1(X) < \mathds{E}\left[Y^{(0)}|X\right], \mathds{E}\left[Y^{(1)}|X\right] < c_2.$ 
Since $\tau_{\text{\tiny RD}}^P(X) = \mathds{E}\left[Y^{(1)}|X\right] - \mathds{E}\left[Y^{(0)}|X\right]$, we must have
\begin{align}
c_1(X) - \mathds{E}\left[Y^{(0)}|X\right]    \leq \tau_{\text{\tiny RD}}^P(X) \leq c_2(X) - \mathds{E}\left[Y^{(0)}|X\right]. \label{eq_baseline_condition}
\end{align}
%
%In this context, under the generative model associated to the Risk Difference, we have for all $a \in \{0,1\}$,
%\begin{align}
%    \mathds{E}[Y^{(a)} | X] = b(X) + a m(X)
%\end{align}
%where $b(X) = \mathds{E}[Y^{(0)}|X].$ Thus, for any value of the treatment effect $m(X)$, one must have
%\begin{align}
%\label{eq_baseline_condition}
%    \max(c_1, c_1-m(X)) < b(X) < \min(c_2, c_2-m(X)).
%\end{align}
Thus, assuming that $\tau_{\text{\tiny RD}}^P(X)$ is given, it may be possible to disentangle the treatment effect $\tau_{\text{\tiny RD}}^P(X)$ from the baseline distribution $Y^{(0)}|X$, but only for baseline distribution satisfying \eqref{eq_baseline_condition}. This condition is more stringent as the CATE  is important (i.e., close to $c_1(x) - c_2(x)$ or $c_2(x) - c_1(x)$), which requires the baseline to be close to $c_1(x)$ or $c_2(x)$. 

\begin{definition}
\label{definition_domain_potential_outcomes}
We say that $\mathcal{A}: x \mapsto \mathcal{A}(x) \subset \mathds{R}$ is an admissible set of values for the potential outcomes if, for each $x \in \mathds{X}$,   
\begin{align}
    \mathds{E}[Y^{(0)}| X = x], \mathds{E}[Y^{(1)}| X = x] \in \mathcal{A}(x).
\end{align}
Given a causal measure $\tau$, and a CATE $\tau(\cdot)$, we let $\mathcal{A}_{B, \tau(\cdot)}(x)$ be the admissible set of values for the baseline, defined as
 \begin{align}
    \mathcal{A}_{B, \tau(\cdot)}(x) = \{ u \in \mathcal{A}(x), \exists v \in \mathcal{A}(x) \textrm{ such that } f(u,v) = h(x) \}.
\end{align}
\end{definition}


\begin{theorem}
\label{th_onlyRD_separates_baseline_tteffect_bounded_outcome}
Let $\mathcal{A}$ be an admissible set of values for the potential outcomes and assume that there exist $\alpha_1<\alpha_2$ such that, for all $x \in \mathds{X}$,  $(\alpha_1,\alpha_2) \subset \mathcal{A}(x)$.  Let $\tau$ be a collapsible (see Definition~\ref{def:indirect-collapsibility}) causal measure defined in Equation~\eqref{eq_causal_measure} satisfying Assumption~\ref{ass:injection_def_domain} and such that $\mathcal{A}(x)\times \mathcal{A}(x) \subset D_f$.  
%

Assume that for all distributions $P(X)$ of $X$, and for all functions $h: \mathds{X} \to f(D_f)$ such that $h(x) \in f(\mathcal{A}(x)\times \mathcal{A}(x))$, there exists $C_{P(X), h} \in \mathds{R}$ such that, for all distributions $Y^{(0)}|X$ satisfying 
$$\forall x \in \mathds{X}, ~\mathds{E}[Y^{(0)} | X = x] \in \mathcal{A}_{B, h}(x),$$
there exists a distribution $Y^{(1)}|X$ such that $\forall x \in \mathds{X}, \mathds{E}[Y^{(1)} | X = x] \in \mathcal{A}(x) $ and
\begin{itemize}
    \item for all $x \in \mathds{X}, \tau^P(x) = h(x)$
    \item $\tau^P = C_{P(X), h}$.
\end{itemize}
%Let the CATE  $\tau: \mathcal{X} \to \mathds{R}$ be any function such that, for all $x \in\mathcal{X}$, 
%\begin{align}
%f(c_2(x), c_1(x)) < \tau(x) < f(c_1(x), c_2(x)). 
%\end{align}
%Now, assume that the ATE $\tau$ does not depend on the choice of the baseline distribution $Y^{(0)}|X$ provided that, almost surely, 
%\begin{align}
%f(\mathds{E}[Y^{(0)}|X],c_2(X)) > \tau(X) \quad \textrm{and} \quad f(\mathds{E}[Y^{(0)}|X], c_1(X)) < \tau(X).
%\end{align}
Then, there exist $a,b,c \in \mathds{R}$ such that, for all distributions $P(X, Y^{(0)}, Y^{(1)})$ satisfying for all $x \in \mathds{X}$,  $\mathds{E}[Y^{(0)}| X = x], \mathds{E}[Y^{(1)}| X = x] \in \mathcal{A}(x)$, we have
\begin{align}
    \tau^P(X) = a \mathds{E}[Y^{(1)}|X] + b \mathds{E}[Y^{(0)}|X] + c.
\end{align}
\end{theorem}

The proof can be found in Appendix~\ref{app_subsection_proof_thm2_genmodels}. Theorem~\ref{th_onlyRD_separates_baseline_tteffect_bounded_outcome} is the equivalent of Theorem~\ref{th_onlyRD_separates_baseline_tteffect} when the potential outcomes are bounded from above and below. Theorem~\ref{th_onlyRD_separates_baseline_tteffect_bounded_outcome} states that the Risk Difference is the only causal measure capable of disentangling the treatment effect from the baseline, for any form of the treatment effect (function $h$) and any acceptable value of the baseline in $\mathcal{A}_{B, h}(x)$.
%eb to be constant across different acceptable values of the baseline. 
In a binary setting, potential outcomes naturally belong to $[0,1]$ and the expected potential outcomes turn into 
\begin{align}
    \mathds{E}[Y^{(0)} | X] = \mathds{P}[Y^{(0)} = 1 | X] \quad \textrm{and} \quad \mathds{E}[Y^{(1)} | X] = \mathds{P}[Y^{(1)} = 1 | X].
\end{align}
In this context, Theorem~\ref{th_onlyRD_separates_baseline_tteffect_bounded_outcome} proves that the only causal measure able to disentangle the treatment effect from the baseline at both a strata and global level for all admissible values of the baseline is the Risk Difference. However, in specific binary settings, other causal measures may allow us to retrieve information on the underlying causal process. This is the subject of the next section.
}

\modif{\subsection{A specific binary outcome model: the Russian Roulette}}






\sout{With binary outcomes, one cannot simply write the outcome model as a function of the baseline plus the treatment alteration,
%such as in Lemma~\ref{lemma:simplified-nonparametric-causal-model}
due to the fact that the  probability of an event ($Y=0$ or $Y=1$) is bounded by zero and one. }
%Or at least, this could be done, but the range of values of the modifying function $m(X)$ would depend on the baseline value $b(X)$. }
\sout{Figure~\ref{fig:alteration-binary.png} illustrates the situation.}
\sout{As a consequence, another non-parametric generative model than Lemma~\ref{lemma:working-model-continuous-Y} is needed to disentangle baseline risk with treatment effect.} 
%Indeed, when considering a binary outcome and in practice, one can intuitively think of a treatment effect as the probability that the treatment can save (or kill) the patient or not.  %Can we propose a model that makes sense of such a principle? \jj{tu peux enlever la question? }
%\es{Il faudrait plutôt remplacer le "there exists a function" par for all $x$, for all $a \in \{0,1\},$ $0 < \mathbb{P}(Y^{(a)} = 1 \mid X=x) < 1$} 


%% Removing figure with binary outcome following R1 remarks
%\begin{figure}[H]
 %   \begin{minipage}{.35\linewidth}
%	\caption{\textbf{Intuition for a binary outcome}: Symmetric illustration than the one proposed in Figure~\ref{fig:alteration.png}, but highlighting that for a binary outcome the quantity to consider is rather the conditional probabilities of the counterfactual events $\mathbb{P}[Y^{(a)}=1 \mid X] $. The two probabilities are bounded by $0$ and $1$.}
%	\label{fig:alteration-binary.png}
 %   \end{minipage}%
  %  \hspace{2cm}
   % \begin{minipage}{.6\linewidth}
   % \includegraphics[width=0.85\textwidth]{fig/alteration-binary.png}
   % \end{minipage}
%\end{figure}
%}

\subsubsection{Intuition of the entanglement model}
\label{sec:binary-outcomes-intrication}
%\gv{On devrait pas dire "entanglement" ici, plutôt que "intrication", sachant que "intrication", c'est un mot Français qui veut dire "entanglement"}
%Such models allow to disentangle the treatment effect from the baseline.
%This enables to analyze what each metric expresses of the effect, and in particular this puts in evidence that some measures grasp both the baseline and the effect, while only one measure only catch the effect of the treatment itself.
%Because the non-parametric model allowing to disentangle treatment effect from the baseline depends on the nature of the outcome, the generalizability properties one can not conclude on the properties of the measure only, but in the context of a certain outcome.
%Note that when using the term \textit{generative models}, we want to insist on the fact that those models do not rely on parametric assumption, and are thus valid in all generality.
%A by-product of our analysis is another way of understanding what heterogeneity means, not through the %lens of the metric, but through the lens of the working model.

 
\sout{To illustrate the \modif{characteristics of a desirable} binary-outcome model \sout{that disentangles the baseline risk with the effect of a treatment}, }
We borrow the intuitive example of the Russian Roulette from \cite{Huitfeldt2019LessWrong}, further used by \cite{CinelliGeneralizing2019}.
When playing the Russian Roulette, everyone has the same probability of $1/6$ to die each time they play.
We know this because of the intrinsic mechanism of the Russian Roulette. 
Now, assume that we have not access to this information. 
\sout{In biology, medicine, or economy this is often the case, as the systems under study are too complex. 
Therefore, one has to empirically estimate this effect. }
%Somehow, we have the intuition that the treatment effect is \textit{homogeneous}, and that ideally some treatment effect measure should convey this phenonemon.
Consider a hypothetical randomized trial to estimate the effect of the Russian Roulette: a random set of individuals is forced to play Russian Roulette, and the others just wait.
For logistic reasons, the experiment is done on a certain time frame, \modif{i.e. we collect the outcome 28 days after the ``treatment'' administration, mimicking a typical clinical outcome defined as mortality after 28 days of hospitalization}.
During this time frame, individuals can die from other reasons, such as diseases or poor health conditions.
For an individual with characteristics $x$, denoting $b(x)$ his/her probability to die without the Russian Roulette, and counting a death as $Y=1$ and survival $Y=0$, one has:
\begin{align}\label{eq:intuition-of-intrication}
    \mathbb{P}\left[ Y^{(a)} = 1 \mid X = x\right] &= b(x) + a\,\underbrace{\textcolor{RoyalBlue}{\left(1-b\left(x \right)\right)}}_\textrm{Entanglement}\, \frac{1}{6}.
\end{align}
This equation 
%\footnote{This equation comes from $ \mathbb{P}\left[ Y^{(a)} = 1 \mid X = x\right] = \mathbb{P}\left[ Y^{(0)} = 1 \mid X = x\right] + \mathbb{P}\left[ Y^{(1)} = 1 \mid X = x\right] =  b(X) + 1/6 - (1/6)\cdot b(X)$.} 
simply states the fact that each individual $X=x$ has a certain probability to die $b(x)$ by default. When getting treatment, an individual can also die from Russian Roulette if affected in the treated group $a=1$, but only if not dead otherwise.
%\sout{(see the multiplication by $\left(1-b(x)\right)$, while in the continuous outcome this was just the sum of the two effects)}
In this equation, one can explicitly observe that the effect%\es{modifié ici}ok 
\modif{(measured via the Risk Difference)} is \modif{naturally} \textit{entangled} with the baseline. %, due to the binary nature of the outcome. 
\modif{As a consequence, the treatment effect $m$ in the generative model associated to the Risk Difference cannot be assumed to be independent of the baseline $b(x)$, as $m(x) = (1-b(x))/6$. In particular, }\sout{risk difference no longer captures the modification as it was the case for a continuous outcome (see Lemma~\ref{lemma:expression-of-causal-quantities-under-generative-model-continuous-outcome}), but rather}:
\begin{equation*}
    \tau_{\text{\tiny RD}}^P = \frac{1}{6}\left(1 - \mathbb{E}\left[b(x) \right]  \right), \quad \text{and in particular}\quad  \lim\limits_{\mathbb{E}[b(x)] \rightarrow 1} \tau_{\text{\tiny RD}} = 0.
\end{equation*}
%\es{est-ce clair?} oui :) !!
\modif{In this situation, the first assumption (independence between the CATE and the baseline) of Theorem~\ref{th_onlyRD_separates_baseline_tteffect_bounded_outcome} is not satisfied. Thus Theorem ~\ref{th_onlyRD_separates_baseline_tteffect_bounded_outcome} does not apply. Indeed, both the CATE and the ATE of  the Risk Difference depends on the baseline. This is  illustrated in Figure \ref{fig:RouletteRusse}}.  In a population with a high baseline, the measured effect vanishes along the risk difference scale.

\begin{figure}[H]
    \begin{minipage}{.35\linewidth}
	\caption{\modif{\textbf{Illustration of the properties of the Risk Difference} under the russian roulette example. With low baseline risk, the Risk Difference can capture the effect of the roulette 1/6 whereas with high baseline the effects tend to 0.}}
	\label{fig:RouletteRusse}
    \end{minipage}%
    \hspace{2cm}
    \begin{minipage}{.6\linewidth}
    \includegraphics[width=0.85\textwidth]{fig/RouletteRusse.png}
    \end{minipage}
\end{figure}


In other words, when considering the RD, the effect of the treatment can only be observed on people that would not have died otherwise. 
This could seem a bit odd, as the Russian Roulette example contains the idea of an \textit{homogeneous} treatment effect, that should not vary over different populations. Still, one measure, the survival ratio, shows an interesting property,
\begin{equation*}
     \tau_{\text{\tiny SR}}^P = 1 - \frac{\mathbb{E}\left[\left( 1- b\left( X\right) \right) \frac{1}{6} \right]}{\mathbb{E}\left[\left( 1- b\left( X\right) \right) \right]} = \frac{5}{6}.
\end{equation*}
The Survival Ratio thus captures the idea of homogeneity: no matter the baseline risk, the Russian Roulette acts in the same way for everyone, as noticed by \cite{Huitfeldt2019LessWrong}. Appendix~\ref{appendix:more-details-on-the-intrication-model} \modif{gives more details about the origin of this example}.
%This property is interesting as one can \textit{remove} the baseline and only capture the effect of the Russian Roulette. %The consequence of SR is that it is making sense of the idea of homogeneity, while allowing to carry a truly causal effect (as a parameter) and not population's effect (in this example health conditions of individuals).
%Below, we propose a formalization of this intuition, into a model capturing all situations, especially both deleterious and positive effects.

\subsubsection{Formal analysis}
%Based on the previous intuition, we propose a non-parametric model that allows to model baseline risk and homogeneity (or not) of treatment effect. 
\sout{The intuitive model presented in \eqref{eq:intuition-of-intrication} does not allow catching all phenomena. In particular,} Equation~\ref{eq:intuition-of-intrication} only describes harmful situations while we may be interested in modelling positive or deleterious effects of the treatment. %But in some real-world situations, treatment effect can go in different directions and be benefical.% For example the seat belts could be protective for taller individuals (mostly men) but could be deleterious for smaller individuals (mostly women). 
In addition, we want a model able to encode situations where there is heterogeneity of the treatment effect (e.g. Russian Roulette can have a higher impact on stressed out people  because the prospect of playing would create cardiac arrests). Or on a more concrete example: the seat belts could be protective for taller individuals but less protective (or even deleterious) for smaller individuals because of the design. 

\begin{lemma}[Entanglement Model]\label{lemma:intrication_model}%\gv{AMHA on retire "intrication" :)}
Considering a binary outcome $Y$, assume that 
\begin{equation*}
     \forall x \in \mathds{X},\, \forall a \in \{0,1\},\quad 0 < p_a(x) < 1,\quad \text{where } p_a(x)  :=  \mathbb{P}\left[Y^{(a)} = 1 \mid X=x\right].
 \end{equation*}
Introducing
\begin{equation*}
    m_g(x):= \mathbb{P}\left[ Y^{(1)} = 0 \mid Y^{(0)} = 1, X = x\right] \quad \text{and} \quad m_b(x):= \mathbb{P}\left[ Y^{(1)} = 1 \mid Y^{(0)} = 0, X = x\right],
\end{equation*}
allows to have
\begin{align*}
     \mathbb{P}\left[ Y^{(a)} = 1 \mid X = x\right]  &= b(x)+  a\, \big( \left( 1-b\left(x\right) \right) m_b\left(x\right) -  b\left(x\right)m_g\left(x\right) \big),\quad \text{where } b(x):=p_0(x).
\end{align*}
\end{lemma}
Proof is available in Appendix~\ref{proof:intrication_model}.
Usually $Y=1$ denotes death or deleterious events, therefore the subscripts $b$ (resp. $g$) stands for \textit{bad} (resp. \textit{good}) events. $m_b$ (resp. $m_g$) corresponds to  the probability that a person who was previously not destined (resp. destined) to experience the outcome, does (resp. does not) experience the outcome in response to treatment. They represent the outcome switch depending on the position at baseline\footnote{Such parameters can be found to be close to the “\textit{counterfactual outcome state transition}” (COST) in \cite{huitfeldt2018choice}. For example $m_b$ would correspond to the quantity denoted by $1-H$. Also note that the intrication model also allows to apprehend what has been done by \cite{CinelliGeneralizing2019}, where the quantity they introduce being $PS_{01} := \mathbb{P}\left[ Y^{(1)} = 1 \mid Y^{(0)} = 0\right]$ corresponds to $m_b$. While their work mostly rely on the formalism of selection diagram, they define $PS_{01}$ (and therefore $m_b$) as the probability of fatal treatment among those who would survive had they not been assigned to for treatment. And conversely, $PS_{10} := \mathbb{P}\left[ Y^{(1)} = 0 \mid Y^{(0)} = 1\right]$ (corresponding to $m_g$) stands for the probability that the treatment is sufficient to save a person who would die if defined. As far as we understand, in both of these works these probabilities are not taken conditionally to $X$.}.
%It is possible to propose an equivalent of Lemma~\ref{lemma:expression-of-causal-quantities-under-generative-model-continuous-outcome}.
%\es{lemme avec les expressions des mesures causales passé en annexe}
%
\modif{The expressions of classical causal measures are established in Lemma~\ref{lemma:expression-of-causal-quantities-under-generative-model-binary-outcome-intrication-model} (see Appendix~\ref{proof:expression-of-causal-quantities-under-generative-model-binary-outcome-intrication-model}). Such expressions are difficult to interpret in the general case where both $m_b$ and $m_g$ are non-zero. In fact, in such a situation $m_b(X)$ and $m_g(X)$ are not identifiable 
%\es{But in any case we would only be interested in their "total" effect, right?} 
\citep{Pearl2000Book, huitfeldt2018choice}. \modif{However, since we are mainly interested in the total effect, one could consider the generative model of the risk difference, }
\begin{equation*}
    \mathbb{P}\left[ Y^{(a)} = 1 \mid X = x \right] = b(x)\,+  a\, \tau(x), \quad \text{where } \tau(x):= \left( 1-b(x) \right) m_b(x)- b(x)m_g(x),
\end{equation*}
with the limitations described in Section~\ref{sec:binary-outcomes-intrication}. 
%As a consequence, the interesting model to consider would be
%\begin{equation*}
%    \mathbb{P}\left[ Y^{(a)} = 1 \mid X = x \right] = b(x)\,+  a\, \tau(x), \quad \text{where } \tau(x):= \left( 1-b(x) \right) m_b(x)- b(x)m_g(x).
%\end{equation*}
%This expression is close to the generative model for a continuous outcomes (Lemma~\ref{lemma:working-model-continuous-Y}). Still, $\tau(x)$ now contains covariates linked to both baseline level and the treatment effect modulators. In such a situation, all measures now depend on the baseline level, and it is no longer possible to find a measure that \modif{disentangles} \sout{decomposes} baseline from the effect. 
Thus, we consider the case of monotonous effects.}
%\jj{Là on ne sait plus où on va et ce qu'on doit retenir intérpréter?}
\sout{At first sight  appears to be very complex. Still, this allows to recover some intuitions. For example, if the treatment is always beneficial ($m_b(x)=0$), and assuming that $m_g(x)$ is lower bounded by a positive constant (i.e there is always a positive effect, even if small), then
\begin{equation*}
     \tau_{\text{\tiny NNT}} = \frac{1}{-  \mathbb{E}\left[ b\left(X\right)m_g\left(X\right) \right] }, \qquad \text{such that, } \lim\limits_{\mathbb{E}[b(X)] \rightarrow 0}  | \tau_{\text{\tiny NNT}} |  = \infty.
\end{equation*}}
\sout{Recall that in the illustrative example of Table~\ref{tab:introduction-diastolic}, the Number Needed to Treat is considerably higher on the population with a low baseline. If the population is at low risk, the effect of a beneficial treatment is indeed perceived as very small\footnote{We recall that a high NNT corresponds to a small effect.} because individuals have already no reason to suffer from the outcome.}
%does not appear to be much more simpler than the results under the logistic model (see Lemma~\ref{lemma:expression-of-causal-quantities-under-generative-model-binary-outcome}) \jj{c'est pour ça qu'il faut le mettre}. 
\sout{Lemma~\ref{lemma:expression-of-causal-quantities-under-generative-model-binary-outcome-intrication-model} can be simplified under some situations, in particular when only  monotonous effects are involved.}


\subsubsection{Notion of monotonous effect}
We introduce the assumption of monotonous effects, where either $\forall x, m_b(x) = 0$ or $\forall x, m_g(x) = 0$ \citep{huitfeldt2018choice, CinelliGeneralizing2019}, corresponding to scenarios where the treatment is only beneficial or deleterious\footnote{In particular, the Russian Roulette corresponds to a situation where $\forall x, m_g(x) = 0$ (Russian Roulette makes no good).}, but cannot be both. If the treatment is always beneficial (i.e. $\forall x, m_b(x) = 0$) then the probability $p_1(x)$ (see Lemma~\ref{lemma:intrication_model}) is lower than the baseline. Respectively, if the treatment is always deleterious  (i.e. $\forall x, m_g(x) = 0$) then the probability $p_1(x)$ is higher than the baseline.
This can be summarized as follows, 

\begin{align}
    \mathbb{P}\left[ Y^{(a)} = 1 \mid X = x\right] &= b(x)\, \underbrace{+ \, a\,  \left( 1-b\left(x\right) \right) m_b\left(x\right)}_{\nearrow} \, \underbrace{- \,a\, b\left(x\right)m_g\left(x\right) }_{\searrow} , \label{eq_monotonous_effect_deletere_beneficial}
\end{align}
where arrows indicate whether each term of the equation is increasing or decreasing the probability of occurrences.
Equation~\eqref{eq_monotonous_effect_deletere_beneficial} highlights that the entanglement is not the same depending on the nature of the treatment (deleterious or not). A beneficial effect ($m_b(x)=0$) is more visible on a high baseline population ($b(x)$ close to 1). On the opposite, a deleterious effect ($m_g(x)=0$) is visible only on the population with low baseline  ($1-b(x)$ close to 1). %All this reasoning can be made considering the situation as increasing occurences or not. 
In other words, an effect increasing the probability of occurences acts only on individuals on which occurences has not already happened yet. %\jj{un peu dure la phrase}\bc{mieux ? sinon je supprime. Moi j'aime bien cette lecture, mais c'est clairement pas necessaire}
%\es{Je suis d'accord avec le modèle :). Il faudrait rajouter une phrase pour faire le lien avec la roulette russe plus haut (justifier $m_g=0$) et mettre les calculs que tu as fait ci-dessous dans un joli Lemme.}

\begin{lemma}[Risk Ratio and Survival Ratio under a monotonous effect]\label{lemma:monotonous-effect}
Ensuring conditions of Lemma~\ref{lemma:intrication_model}, 



\begin{itemize}
    \item Assuming that the treatment is beneficial (i.e. $\forall x, m_b(x) = 0$), then

    \begin{align}
    \tau_{\text{\tiny RR}}^P(X)  = 1  -  m_g(X)
    \quad \textrm{and} \quad 
     \tau_{\text{\tiny RR}}^P = 1  - \frac{\mathbb{E}\left[  b(X) m_g(X)\right]}{\mathbb{E}\left[ b(X)\right]}.
 \end{align}

 \item  Assuming that the treatment is harmful (i.e. $\forall x, m_g(x) = 0$), then


 \begin{align}
 \tau_{\text{\tiny SR}}^P(X) = 1  -   m_b(X)  \quad \textrm{and} \quad 
     \tau_{\text{\tiny SR}}^P = 1  - \frac{\mathbb{E}\left[ \left( 1-b(X) \right) m_b(X)\right]}{\mathbb{E}\left[ 1-b(X)\right]}.
 \end{align}
\end{itemize}


 

\end{lemma}


%For example, considering a beneficial (resp. harmful) effect, and if there exist a subset of $X$, denoted $X_b$ only implied in the baseline risk $b(X_b)$, then $m_g(x)$ (resp. $m_b(x)$) is a constant (i.e. no modulation of the treatment effect) on the space $X_b$. 
%In other words, Lemma~\ref{lemma:monotonous-effect} confirms that Risk Ratio (resp. SR) is constant on these subgroups even if the baseline risk moves.
 %If the population concerned are such that $m_b(X)$ or $m_g(X)$ are constant (i.e. no modulation of the treatment effect), then either the RR or the SR takes a very simple value. For e.g. in the Russian Roulette case, the treatment effect increases the number of events, and is such that $m_g(X)=0$ and $m_b(X)$ is constant, such that the SR is an homogeneous measure (Definition~\ref{def:homogeneity}).
 
 These results formalize what has been proposed several times in the literature, for example by \cite{Sheps1958ShallWe}, and later by \cite{huitfeldt2018choice, Huitfeldt2021ShallWe}, or with what has been called the \textit{Generalised Relative Risk Reduction} \citep{baker2018new}. In particular, Sheps finishes her paper with the following quote
 \begin{quote}
   ``A beneficial or harmful effect may be estimated from the proportions of persons affected. The absolute measure does not provide a measure of this sort. The choice of an appropriate measure resolves itself largely into the choice of an appropriate base or denominator for a relative comparisons. [$\dots$] the appropriate denominator consists of the number of persons who could have been affected by the factor in question".
 \end{quote}
This recommendation is consistent with Lemma~\ref{lemma:monotonous-effect}. In other words,\modif{sign} of the effect dictates on which labels the relative comparison should be made \modif{i.e., dividing by $\mathbb{P}\left[Y^{(0)}=1\right]$ or $\mathbb{P}\left[Y^{(0)}=0\right]$}, to obtain a \modif{CATE disentangled from the baseline}\sout{treatment effect measure as less as possible entangled by the baseline}: \modif{SR should be used when the effect is harmful (like the Russian Roulette), while RR should be used when the effect is beneficial.} \modif{While the CATE of SR (resp. RR) is interpretable, as it allows us to retrieve a deleterious (resp. beneficial) local effect, the corresponding ATE is not able to disentangle the baseline from the causal effect. This is consistent with the result of Theorem~\ref{th_onlyRD_separates_baseline_tteffect_bounded_outcome} that states that only the Risk Difference can disantangle baseline and treatment effetcs at both CATE and ATE levels.}


\sout{The comment of Sheps about absolute measure holds for binary outcome, but we showed that RD has good properties when considering a continuous outcome. Doing so, we justified and extended the scope of her conclusions. In other words, depending on the direction of the effect (harmful or beneficial) it is possible to define an equivalent of the Risk Difference for the continuous outcome, but in the world of binary outcome \jj{attention ici}. Such definitions also enable a meaningful interpretation of what constitutes a homogeneous treatment effect when dealing with binary outcomes..}
%Indeed, the well-known
%\jj{Il me faudrait une phrase soit avec un exemple, soit avec ce que tu me conseilles de faire en pratique}\bc{d'accord. Je t'avoue que je ne sais pas trop quoi faire de cette partie. Je la garde pour notre point. je me demande si il ne faut pas raccourcir cette partie pour faire une concusion générale apres}


\sout{Lemma~\ref{lemma:monotonous-effect} is still disappointing as it does not allow to disentangle baseline risk and treatment effect ($m_b(.)$ or $m_g(.)$) such as for the continuous outcome situation.
%Indeed, such result further highlights that for a population with similar baseline risks, then either the SR or the RR are taking simpler expressions.
Still, looking at local effect $x$, for example on the Risk Ratio scale, one has
\begin{equation*}
    \tau_{\text{\tiny RR}}(x) = 1-\frac{b(x)m_g(x)}{b(x)} = 1-m_g(x).
\end{equation*}
Therefore, if willing to compute subgroup effects on covariates $X$ affecting only the baseline level $b(.)$, one would observe a constant effect. 
This explains the illustrative example from Table~\ref{tab:introduction-diastolic}, where the Risk Ratio is constant over strata with varying baseline (here diastolic blood pressure).}%, which also suggest a constant probability $m_g(x)$ over the baseline covariate of diastolic blood pressure.
%\es{je ne suis pas sûr de comprendre l'exemple}

%\subsubsection{What about non-monotonous effect?}

%While the situation of monotonous effect can lead to simpler expression of RR or SR, the situation remains complex when it comes to treatment being both beneficial and harmful (such as the seat belt example, and depending on the individuals). 






\section{Generalization}
\label{sec:generalize}

%\subsection{Generalizability or portability of a causal measure}\label{subsec:first-time-generalization}

As highlighted in Section~\ref{sec:formalization-and-key-contributions}, an RCT conducted in a population $P_{\text{\tiny S}}$  allows for the estimation of a treatment effect $\tau^{P_{\text{\tiny S}}}$ on this population. What would the result be if the individuals in the trial were rather sampled from a population $P_{\text{\tiny T}}$ with different covariates distribution? 
%
This question is linked to external validity, and more precisely to a sub-problem of external validity being \textit{generalizability} or \textit{transportability}. We say that findings from a trial sampled from $P_{\text{\tiny S}}$ can be generalized to $P_{\text{\tiny T}}$ when $\tau^{P_{\text{\tiny T}}}$ can be estimated without running a trial on $P_{\text{\tiny T}}$, but only using data from the RCT and baseline information on the target population $P_{\text{\tiny T}}$ (the covariates $X$, and sometimes also the control outcome $Y^{(0)}$), as summarized on Figure~\ref{fig:observed-data}. 

\begin{figure}[!h]
	\begin{minipage}{.28\linewidth}
    \caption{\textbf{Generalization in practice}: We typically consider a situation where the treatment effect is estimated from a Randomized Controlled Trial (RCT) where individuals are sampled from a population $P_{\text{\tiny S}}$. When willing to extend these findings to $P_{\text{\tiny T}}$, we assume to have access to a representative sample of the patients of interest, with information on their covariates $P_{\text{\tiny T}}(X)$, and also \underline{maybe} the outcome under no treatment $P_{\text{\tiny T}}(X, Y^{(0)})$.}
    \label{fig:observed-data}
    \end{minipage}
    \hspace{0.8cm}
    \begin{minipage}{.70\linewidth}
    \centering
    \includegraphics[width= 0.98\textwidth]{fig/observed-data.png}
    \end{minipage}
\end{figure}




\subsection{Two different strategies for generalizability}
\label{subsection:two_generalization_strategies}


There exist two identification strategies, generalizing \textit{(i)} the conditional outcomes or \textit{(ii)} the local effect measure itself, leading to different assumptions required for generalizing.
For both strategies, we consider the settings where information gathered on the source population covers at least the support of the target population. 

\begin{assumption}[Overlap or positivity]\label{a:overlap}
The support of the target population is included in the source population:
$\operatorname{supp}(P_{\text{\tiny T}}) \subset \operatorname{supp}(P_{\text{\tiny S}})$.
\end{assumption}
%\es{Can you express this more succinctly simply in terms of absolute continuity of measures? I think you need $P_T << P_S$.} \jj{OK avec son écriture mais est-ce qu'on fait ça?} \es{je dirais non, l'inclusion des support est assez claire comme ça} bc : ok on enleve et j'ai ajouté dans les réponses pourquoi on ne met pas.

\modif{This assumption\footnote{\modif{Note that Assumption~\ref{a:overlap} can be phrased as ``the measure $P_{\text{\tiny T}}$ is absolutely continuous with respect to $P_{\text{\tiny S}}$''.}} is the counterpart of the the so-called positivity or overlap assumption in observational studies. It means that all members of the target population have positive probability of being selected into the source population. In the specific case of generalization, such a common assumption is violated when the source population is a randomized controlled trial conducted on a restricted population (for e.g. because of strict eligibility criteria for safety reasons) compared to the target/whole population. Still, in practice, it is possible to restrict the support of the target population to the source population. This would allow generalizing an effect from the source population to the target population, answering the following question “what would the effect be on the target population if the same eligibility criteria were used?”. \\}


The first approach aims at generalizing conditional expectations $\mathbb{E}_{\text{\tiny S}}\left[ Y^{(a)} \mid X\right]$ of the potential outcomes to the target population. Such a strategy is valid only under the following assumption.


\begin{assumption}[Transportability or S-ignorability or Exchangeability between populations]\label{a:transportability-wide}
For all $x \in \operatorname{supp}(P_{\text{\tiny T}}) \, \cap \, \operatorname{supp}(P_{\text{\tiny S}})$, for all $a \in \{0,1 \}$, $$\mathbb{E}_{\text{\tiny S}}\left[ Y^{(a)} \mid X = x \right] = \mathbb{E}_{\text{\tiny T}}\left[ Y^{(a)} \mid X = x\right].$$
\end{assumption}
%\es{j'ai enlevé les a.s.: soit tu écris la formule avec des conditionnements par rapport à des variables aléatoires, auquel cas tu peux laisser a.s., soit tu écris pour tout $x$ et tu n'as pas besoin de a.s.}\bc{d'accord, merci Erwan !!}
This assumption\footnote{This assumption is also commonly found expressed as $Y^{(0)}, Y^{(1)} \indep  I \mid X$, where $I$ is an indicator of the population membership \citep{stuart2011use, pearl2015findings, lesko2017generalizing}. Such assumptions can also be expressed using selection diagram \citep{pearl2011transportability}. 
%\es{I am impressed at how widely-read and non-dogmatic you are. It is very helpful to readers to have the same ideas expressed in multiple ways.}
} boils down to: $X$ contains all \sout{the baseline} covariates that are \textit{both} shifted between the two populations $P_{\text{\tiny S}}$ and $P_{\text{\tiny T}}$ \textit{and}  prognostic of the outcome.  
Assumption~\ref{a:transportability-wide} enables the identification of $\tau^{P_{\text{\tiny T}}}$ using information from \modif{$P_{\text{\tiny S}}(X,Y^{(0)}, Y^{(1)})$} and only the covariate distribution $P_{\text{\tiny T}}(X)$ in the target population, as shown in the following Proposition (see Appendix~\ref{proof:generalizability-section-3} for the proof).

\begin{proposition}[Generalizing conditional outcomes]\label{proposition:generalization-density}
Consider two distributions $P_{\text{\tiny S}}$ and $P_{\text{\tiny T}}$ satisfying Assumptions~\ref{a:overlap} and \ref{a:transportability-wide}. Then, the conditional outcomes are generalizable:
\begin{align*}
    \forall a \in  \{0,1\}\, \quad \mathbb{E}_{\text{\tiny T}}\left[ Y^{(a)}\right] &=  \mathbb{E}_{\text{\tiny T}}\left[ \mathbb{E}_{\text{\tiny S}}\left[ Y^{(a)} \mid X\right]  \right] && \text{G-formula} \\
    &= \mathbb{E}_{\text{\tiny S}}\left[ \frac{p_{\text{\tiny T}}(X)}{p_{\text{\tiny S}}(X)}\mathbb{E}_{\text{\tiny S}}\left[ Y^{(a)} \mid X\right]  \right] %&& \text{Re-weighting.}
\end{align*}
where $\frac{p_{\text{\tiny T}}(X)}{p_{\text{\tiny S}}(X)}$ corresponds \modif{to the ratio of covariate densities in the }%\es{Which one exactly? I think it should be the ratio of conditional densities of $Y^(a)$ given $X$?}
\sout{between  the source and target} \modif{source and target} populations.
\end{proposition}
%\es{est-ce qu'on ne sortirait pas le doing so... de la proposition pour expliquer clairement la stratégie d'estimation ?}

\modif{The first formula in Proposition~\ref{proposition:generalization-density} suggests a strategy to generalize the potential outcomes: first, one can compute of $\mu_{a, \text{\tiny S}}(x) = \mathbb{E}_{\text{\tiny S}}\left[ Y^{(a)} \mid X =x \right]$ using the source distribution $P_{\text{\tiny S}}(X,Y^{(0)}, Y^{(1)})$, then one can compute $\mathds{E}_{\text{\tiny T}} [\mu_{a, \text{\tiny S}}(X)]$ using the covariate target distribution $P_{\text{\tiny T}}(X)$. Any causal measure $\tau$ satisfying Equation~\eqref{eq_causal_measure} can be generalized on the target distribution using this strategy.}  
\sout{be built using the source data set only
Doing so, any causal measure $\tau^{P_{\text{\tiny T}}}$ can be identified from \modif{$P_{\text{\tiny S}}(X,Y^{(0)}, Y^{(1)})$} and $P_{\text{\tiny T}}(X)$ as any causal measure on the target population can be computed from the generalized outcomes $   \mathbb{E}_{\text{\tiny T}}\left[Y^{(0)} \right] $ and $   \mathbb{E}_{\text{\tiny T}}\left[Y^{(1)} \right] $.}%\jj{je ne comprends pas ce qu'on dit là}


%\sout{The first formula of Proposition~\ref{proposition:generalization-density} connects to a classic estimation strategy, plug-in g-formula \citep[see][for a review on the Risk Difference]{colnet2021causal}. Yet under these assumptions, estimation can also be performed by re-weighting the observations \citep{stuart2011use}. }
%(this is also highlighted when doing meta-analysis \citep{vo2019novel}).
%One can also find this method under the name Subgroup Mixable Estimation (SME) \citep{ding2016subgroup}.
%\jj{On va vérifier toutes les deux mais je pense que la deuxième c'est pas IPSW, ça serait la 2 dans l'approche 2}

%\subsubsection{Generalizing a collapsible measure via local effects}\label{subsection:generalizing-local-effects}

When the causal measure is collapsible, rather than using a conditional outcome model, the second approach relies on the local effects $\tau^{P_{\text{\tiny S}}}(x)$ to get the target population's effect $\tau^{P_{\text{\tiny T}}}$, such as in Equation~\ref{eq:toy-example-standardization}. Importantly,  Assumption~\ref{a:transportability-wide} can then be relaxed into a new, less restrictive, Assumption~\ref{a:transportability}.
%We call this procedure \underline{generalization by standardization} to refer to the principle of \cite{Rothman2000ModernEpidemiology}. 
%\subsubsection{What do we mean by generalizability?}
%Learning causal effect in one environment has interest when findings can be generalized to another population, in particular because most studies are conducted with the intention of applying the results elsewhere.
%\textit{Generalization} can also be found under the name \textit{Transportability}, \textit{Portability}, \textit{Recoverability}, and refers to the external validity of a study. 
%Generalization is what physician typically do, as they extend what they have learned through previous trials to a new population or a new patient.
%Finally, one concerns about causal effect measures is linked to their generalizability.  In other words, how far can an investigator extends his/her learning from a trial to another population of interest? Are there measures expected to more generalizable than others? This is particularly important as conclusions that are remembered and shared from clinical trial are causal, and therefore somehow considered as an invariant ground truth\footnote{We refer the reader to \cite{Pearl2000Book} (p. 182), talking about this practice: ``\textit{Once people interpret proportions as causal relations, they continue to process those relations by causal calculus and not by the calculus of proportions}".}. For example, let's consider that  a clinician has in mind the result from a trial conducted on a population with distribution $P_1$ stating that the efficacy of the drug if $\hat \tau_{\text{\tiny RR}} = 3$. But the clinician has patients that are following a different distribution $P_2$. A intuitive practice is to multiply the baseline of the patient $i$ by the Risk Ratio $\hat \tau_{\text{\tiny RR}}$ to obtain the estimated outcome under treatment for this patient. This process can be found under the name \textit{effect function} in \cite{Huitfeldt2021ShallWe}. Namely,\bc{Discussion avec Erwan, est-ce qu'on met du conditionnel?}
%\begin{equation*}
%    \mathbb{E}\left[ Y_i^{(1)} \right] =   \hat \tau_{\text{\tiny RR}}\,  \mathbb{E}\left[ Y_i^{(0)} \right].
%\end{equation*}
%If the clinician was rather provided a study with a risk difference, the process would be,
%\begin{equation*}
%    \mathbb{E}\left[ Y_i^{(1)} \right] =   \hat \tau_{\text{\tiny RD}} +   \mathbb{E}\left[ Y_i^{(0)} \right].
%\end{equation*}
%But, to me more or less accurate, this process require an homogeneity assumption:  \textit{(i)} Either the causal effect obtained from the trial $\hat \tau$ is assumed to be the same for every individual. In such situation, this process is accurate for every patient the clinician face, and also true for any population. \textit{(ii)}  Or, if the effect obtained from the trial $\hat \tau$ is expected to be true on average on $P_2$. Then, this process is accurate on average.
%While \textit{(i)} will guide us toward a strict version of generalizability, \textit{(ii)}  will require some additional assumptions. 
%In this Section we show how results from Section~\ref{section:generative-models} should encourage the usage of certain metrics rather than other to hope that such practice is not completely misleading.



%\begin{definition}[Generalizability by standardization]
%Let $\tau$ be a measure of effect and $X$ a set of baseline covariates. A treatment effect $\tau$ is said to be generalizable from a source population $P_{\text{\tiny S}}$ to a target population $P_{\text{\tiny T}}$ with respect to causal effect measure $\tau$, if there exists positive weights $w(X, P_{\text{\tiny S}}(X), P_{\text{\tiny T}}(X, Y^{(0)}))$ such that for all source distribution $P_{\text{\tiny S}}$ and target distribution $P_{\text{\tiny T}}$, 
%\begin{align*}
%\tau^{\text{\tiny T}} = \mathbb{E}_{\text{\tiny S}} \left[\tau^{\text{\tiny S}}(X)\, w(X, P_{\text{\tiny S}}(X), P_{\text{\tiny T}}(X, Y^{(0)})) \right].
%\end{align*}

%\end{definition}

\begin{assumption}[Transportability of the treatment effect]\label{a:transportability}
For all \modif{$x \in \operatorname{supp}(P_{\text{\tiny T}}) \cap \operatorname{supp}(P_{\text{\tiny S}})$}, $$\quad \tau^{P_{\text{\tiny S}}}(x) = \tau^{P_{\text{\tiny T}}}(x).$$
\end{assumption}
Here, the transportability assumption\footnote{This assumption is also commonly found expressed as $Y^{(0)}- Y^{(1)} \indep I \mid X$ when it comes to the generalization of the risk difference ($I$ being an indicator of the population membership). 
%\es{I like the way you express things mathematically and then interpret them qualitatively. Both are very valuable to readers!}
Note that the transportability assumptions conveys the idea of some homogeneity assumption (close to the spirit of Definition~\ref{def:homogeneity}). This is highlighted by \cite{huitfeldt2018choice} who refer to Assumptions~\ref{a:transportability-wide} and \ref{a:transportability} as ``\textit{different homogeneity conditions to operationalize standardization}".} can be phrased as: $X$ contains all \sout{the baseline} covariates that are \textit{both} shifted between the two populations $P_{\text{\tiny R}}$ and $P_{\text{\tiny T}}$ \textit{and}  treatment effect modulators. %\es{jai repris la formulation d'au-dessus pour le lecteur, ça vous va? }\bc{parfait pour moi!}
%all treatment effect modulators of the treatment effect that are shifted between the two data sources are measured and used for adjustment.
  %In other words, it is possible to transport some information from the source population to the target because locally this information is the same. 
%\jj{c'est pas un théoreme la proposition 1?}
%\begin{proposition}[Non-collapsibility prevents generalizability]\label{prop:noncollaps-prevents-standardization}
   % Consider two population $P_{\text{\tiny S}}$ and $P_{\text{\tiny T}}$, and any causal measure $\tau$. If $\tau$ is not collapsible, then $\tau$ is not generalizable by standardization, even if Assumptions~\ref{a:transportability} and \ref{a:overlap} are granted.
%\end{proposition}\bc{Problème de preuve}
%But when collapsibility is granted, Assumption~\ref{a:transportability} and Assumption~\ref{a:overlap} license \jj{encore license?} generalizability of $\tau$ from $P_{\text{\tiny S}}$ to $P_{\text{\tiny T}}$.


\begin{proposition}[Generalizing local effects]\label{prop:generalization-of-local-effects}
Consider two distributions $P_{\text{\tiny S}}$ and $P_{\text{\tiny T}}$ and a collapsible causal measure $\tau$ satisfying Assumptions~\ref{a:overlap} and \ref{a:transportability}. Then, $\tau$ is generalizable to the target population vie the formula 
\modif{\begin{align*}
    \tau^{P_{\text{\tiny T}}}  &= \mathbb{E}_{\text{\tiny T}}\left[ w(X, P_{\text{\tiny T}}(X, Y^{(0)})) \tau^{P_{\text{\tiny S}}}(X)  \right] \\%&& \text{G-formula}\\
    &= \mathbb{E}_{\text{\tiny S}}\left[ \frac{p_{\text{\tiny T}}(X)}{p_{\text{\tiny S}}(X)} \, w(X, P_{\text{\tiny T}}(X, Y^{(0)})) \, \tau^{P_{\text{\tiny S}}}(X)  \right] && \text{Re-weighting.}
\end{align*}}
where $\frac{p_{\text{\tiny T}}(X)}{p_{\text{\tiny S}}(X)}$ corresponds to the ratio of covariate densities in the source and target populations, and $w(X, P_{\text{\tiny T}}(X, Y^{(0)}))$ corresponds to the collapsibility weights (see Definition~\ref{def:indirect-collapsibility}). 
\sout{of $\tau$ on the target population.}
\end{proposition}
%
The proof is postponed to Appendix~\ref{proof:generalizability-section-3}. \modif{The first formula in Proposition~\ref{prop:generalization-of-local-effects} leads to the following generalization strategy: the quantity $\tau^{P_{\text{\tiny S}}}(X) = \mathds{E}_{\text{\tiny S}}[Y^{(1)} - Y^{(0)} | X]$ can be computed using the source distribution $P_{\text{\tiny S}}(X,Y^{(0)}, Y^{(1)})$ and both the collapsibility weights and the expectation in the first formula of Proposition~\ref{prop:generalization-of-local-effects} can be computed using the target distribution $P_{\text{\tiny T}}(X,Y^{(0)})$.} \sout{Doing so, any collapsible causal measure can be identified from $P_{\text{\tiny S}}(X,A,Y)$ and $P_{\text{\tiny T}}(X, Y^{(0)})$ (if not directly collapsible).}
Note that the \modif{second} formula suggests the classical re-weighting estimation strategy also called IPSW (Inverse Propensity of Sampling Weighting, see  \cite{colnet2021causal} for a review on the Risk Difference).

%\subsubsection{One assumption needs \sout{less} \modif{fewer} covariates than the other}
%\es{On the one hand, I would be happy to stop making the distinction between less and fewer, since we never make the same distinction between more and greater. On the other hand, not all readers will feel this way, so perhaps you should write 'fewer'?}

\sout{The two above sections (\ref{subsection:generalizing-conditional-expectation} and \ref{subsection:generalizing-local-effects}) mirror each other with two different strategies relying on two different assumptions.}
The two above strategies rely on two different assumptions. However,
it is very important to note that Assumption~\ref{a:transportability} is lighter than Assumption~\ref{a:transportability-wide} as highlighted in \cite{nguyen2018sensitivitybis, huitfeldt2018re, colnet2022sensitivity}.
As a consequence,
using local effects -- Proposition~\ref{prop:generalization-of-local-effects} -- as opposed to conditionnal-outcomes --Proposition~\ref{proposition:generalization-density} -- may allow generalizing \modif{collapsible causal measure \emph{with fewer covariates}, as detailled in the next section.} \sout{ (only shifted treatment effect modulators), provided that the collapsibility weights do not depend on other variables. This is particularly the case for the Risk Difference, whose collapsibility weights are equal to one. }
\sout{This is at the cost of generalizing only collapsible measures (RD, RR, SR -- Table~\ref{tab:small-summary-measures}), and having access to $Y^{(0)}$ in the target population if the measure is not directly collapsible such as the RR and SR.}
%In the worst case, covariates $X$ needed to ensure Assumption~\ref{a:transportability} are the same as Assumption~\ref{a:transportability-wide} (all shifted prognostic covariates being also modulator in the treatment effect scale chosen). %\bc{Still, moving from generalizing the densities to generalizing by standardization is not for free, as this also require that the treatment effect measure of interest has to be collapsible. }
%\jj{à la fin ici on a envie de redire quelles mesures est généralisable par standardizatiuon}
%Even if a strong generalizability is not possible, solutions can be proposed. When facing a distribution shift, a solution is to standardize the trial's findings toward a new population of interest. In particular, the intuition is that there exists a weight function $w$ such that $\tau_T = \mathbb{E}[\tau_R(X) w(X)]$, then, under the assumption that $\tau_R(x) = \tau_T(x)$ for all $x$, we obtain that $\tau_T =  \mathbb{E}[\tau_T(X) w(X)]$ is collapsible. Note that Using a ratio of proportion as weights for generalizability is only valid if the causal measure is directly collapsible. Otherwise, the weights to use should probably be a ratio involving elements that appear in the collapsible definition of the measure (see formula in Lemma~\ref{lemma:collapsibility-of-RR-SR}). \bc{J'ai laissé les remarques de Erwan ici.}
\sout{\paragraph{Final comment} %\es{on peut peut-être ajouter ici que chaque équation mène à des procédures d'estimations différentes et les détailler pour une mesure non collnon collapsible (en annexe). Je trouve toujours les calculs en note de bas de page difficile à lire}\bc{ok, je laisse pour qu'on en parle, je voulais mettre cela dans la partie simulation ?}
The two procedures are equivalent when it comes to the Risk Difference, thanks to the direct generalization of this measure and linearity of expectation,
\begin{equation}\label{eq:equivalence-risk-difference}
    \tau^{\text{\tiny T}}_{\text{\tiny RD}}  =\mathbb{E}_{\text{\tiny T}}\left[ g_{\text{\tiny T}}(Y^{(0)}, X) \tau^{\text{\tiny S}}_{\text{\tiny RD}}(X)  \right]= \mathbb{E}_{\text{\tiny T}}\left[ 1 \cdot \tau^{\text{\tiny S}}_{\text{\tiny RD}}(X)  \right] = \mathbb{E}_{\text{\tiny T}}\left[  \mathbb{E}_{\text{\tiny S}}\left[Y^{(1)} \mid X = x \right] \right] - \mathbb{E}_{\text{\tiny T}}\left[  \mathbb{E}_{\text{\tiny S}}\left[Y^{(0)} \mid X = x \right] \right].
\end{equation}}
%\es{je ne sais pas si je laisserais ce calcul car le point de la discussion précédente est de dire que les \textit{hypothèses} des deux stratégies ne sont pas les mêmes}
\sout{\modif{While this is not the main purpose of this work,} we discuss how to transform identification into estimation in the simulations section (see Section~\ref{sec:simulations}).}
%\es{renvoyer plutôt à la section simulation du corps de l'article ?}

\subsection{Are some measures easier to generalize than others?}
\label{sec:generalization}

Section~\ref{subsection:two_generalization_strategies} exposes two transportability assumptions depending on which conditional quantity from the source population is generalized: the conditional outcome (Assumption~\ref{a:transportability-wide}) or the local effect (Assumption~\ref{a:transportability}).
\modif{While Assumption~\ref{a:transportability-wide} requires that all covariates being prognostic and shifted in the two populations have been observed, Assumption~\ref{a:transportability} involves all covariates modulating treatment effect and shifted. In this section, we analyze precisely which strategy can be used for a given causal measure, and what is the required set of covariates for such a strategy. }
%
\sout{The first approach assumes that all covariates being prognostic and shifted in the two populations have been observed, while the second approach only requires to adjust on all covariates modulating treatment effect and shifted. \jj{est-ce que c'est vrai car on a les poids de collapsibilité qui dépendent de Y0}
%shifted covariates modulating the treatment effect on the chosen scale.
By disentangling the baseline level and the treatment effect, Section~\ref{section:generative-models} paves the way toward establishing which transportability assumption is more likely to hold, depending on the outcome nature \jj{phrase à changer} and the causal measure considered. 
Recall that, for any given causal measure $\tau$, Assumption~\ref{a:transportability} states that
\begin{equation*}
   \forall x \in \operatorname{supp}(P_{\text{\tiny T}}) \, \cap \, \operatorname{supp}(P_{\text{\tiny S}}), \quad   \tau^{\text{\tiny R}}(x) =  \tau^{\text{\tiny T}}(x).
\end{equation*}
Considering the generative model associated to $\tau$ (see Lemma~\ref{lem_generative_models}), we know that $m(x) = \tau(x)$. 
%If the outcome is continuous, Lemma~\ref{lemma:expression-of-causal-quantities-under-generative-model-continuous-outcome} ensures $\tau_{\text{\tiny RD}}(x) = m(x)$. 
Therefore, \sout{if considering a continuous outcome, }Assumption~\ref{a:transportability} is satisfied as soon as we adjust on the shifted covariates implied in $m(.)$ (regardless of the covariates implied in the baseline level $b(.)$).}
%
We start by specifying which variables are shifted between the two populations. %Following Section~\ref{section:generative-models}, we now understand the set of prognostic covariates $X$ 

\begin{assumption}[Shifted covariates set]\label{def:shidted-covariates}
We assume that only some components of $X$ are shifted between the source and the target population. More precisely, we denote by $\textrm{Sh} \subset \{1, \hdots, d \}$ \modif{the set of indices} corresponding to the components of $X$ that are shifted between the source and the target population, that is, 
%for all $j \in \textrm{Shift}$, there exists $x_j \in \mathds{R}$ such that 
for all integrable functions $f: \mathds{X} \to \mathbb{R}$, for all $x \in Supp(P_{\text{\tiny T}})$, 
\begin{align*}
    \mathbb{E}_{\text{\tiny S}}[ f(X) | X_{Sh} = x_{Sh}] =  \mathbb{E}_{\text{\tiny T}}[ f(X) | X_{Sh} = x_{Sh}],
\end{align*}
and the complementary set of covariates $X_{Sh^c}$ is independent of $X_{Sh}$.
\end{assumption}

To formalize which covariates are implied in local treatment effect, we introduce notations to distinguish covariates status, either intervening on the baseline level or modulating the effect. 

%ici que les variables qui interviennent dans B et M. 
\begin{assumption}[Two types of covariates]\label{def:two-kind-covariates}
Let $\tau$ be a causal measure and let $b: \mathds{X} \to \mathbb{R}$ and $m: \mathds{X} \to \mathbb{R}$ be the function describing the associated model (see Lemma~\ref{lem_generative_models}). \sout{Recall that $b: \mathds{X} \to \mathbb{R}$ and $m:  \mathds{X} \to \mathbb{R}$ are defined in Lemma~\ref{lemma:working-model-continuous-Y} for a continuous outcome or in Lemma~\ref{lemma:intrication_model} (here, $m$ referring for $m_b$ and $m_g$). }
For all $J \subset \{1, \hdots, d\}$, we let $X_J$ the subvector of $X$ composed of components of $X$ indexed by $J$. We assume that the function $b$ depends only on $X_B$, a subset of covariates indexed by $B \subset \{1, \hdots, d \}$. Similarly, we assume that the function $m$ depends only on $X_M$, a subset of covariates indexed by $M \subset \{1, \hdots, d \}$.
%$ \subset Accordingly, we let $X_B$ (resp. $X_M$) be \sout{the} \modif{a} minimal set of variables 
%\es{no, indeed, the has been changed into a minimal set} involved in the function $b$ (resp. the function $m$), such that, for all $x \in \mathcal{X}$,
%\begin{align*}
%\mathbb{E}\left[b\left(X\right) | X = x \right] = \mathbb{E}\left[b\left(X\right) | X_B = x_B\right] \quad \textrm{and} \quad  \mathbb{E}\left[m\left(X\right) | X = x\right] =  \mathbb{E}\left[m(X) | X_M = x_M\right].
%\end{align*}
\end{assumption}
%Then, within the set of baseline covariate $X$, some covariates may be shifted between the two populations or not. 
%\es{ici la définition est celle d'un shift univarié, on peut imaginer des cas où la structure de dépendance varie mais pas les distributions univariées}


\modif{The baseline $b$ and the treatment effect $m$ are assumed to depend on certain sets of variables (Assumption~\ref{def:two-kind-covariates}). Determining such sets is an active area of research \citep{hines2022variable, benard2023variable} and falls beyond the scope of this paper. Instead, we fix the sets $X_B$ and $X_M$, and the set of shifted covariates between the source and target population, and analyze which covariates are required for generalization, depending on the considered strategy (generalizing potential outcomes or local effects)}
\footnote{Note that the size of $X_B$, $X_M$ and $X_{Sh}$ completely depends on the data distribution and the causal measure: if the causal measure does not allow disentangling the treatment effect from the baseline at a strata level then $X_M = X_B$, whereas if all variables are shifted then $X_{Sh} = X$.}



Generalizing conditional outcomes requires to have access to all shifted prognostic covariates.

\begin{theorem}\label{theorem:all-covariates}
    Consider an injective causal measure (Assumption~\ref{ass:injection_def_domain}). For all distributions $P_{\text{\tiny S}}(X, Y^{(0)}, Y^{(1)})$ and $P_{\text{\tiny T}}(X, Y^{(0)}, Y^{(1)})$ satisfying Assumption~\ref{a:overlap} (overlap assumption) and Assumption~\ref{a:transportability-wide}, 
    %Under the assumption of Lemma~\ref{lem_generative_models}, \sout{of Lemma~\ref{lemma:working-model-continuous-Y} or \ref{lemma:intrication_model},} for any causal measure, 
    generalization of the conditional outcomes is possible if one has access to all shifted covariates involved in the baseline and the treatment effect, that is $X_{(B\cup M)\cap Sh}$.
\end{theorem}

The proof can be found in Appendix~\ref{subsubsec:proof_generalization_conditional_outcomes}. To illustrate what are the different covariate sets, we introduce the data generative model of the simulations (see Section~\ref{sec:simulations}), where we assume that six covariates are prognostic and that data are generated as
\begin{equation}\label{eq:simulation-continuous-generative-model}
    Y=b\left(X_1, X_2, X_3, X_4, X_5, X_6\right) + A\, m\left(X_1, X_2, X_5\right) + \varepsilon.
\end{equation}
 Doing so, $B=(1,2,3,4,5,6)$, and $M = (1,2,5)$. In addition, the two populations are constructed such that $X_1, \dots X_4$ are shifted covariates, but not $X_5, X_6$. Figure~\ref{fig:illustration-shifted} illustrates what shifted and non-shifted means.  Theorem~\ref{theorem:all-covariates} states that generalization of the conditional outcomes is possible when observing  $X_1, \dots X_4$.
 
 \begin{wrapfigure}{r}{0.38\textwidth}
\centering
    \includegraphics[width=0.35\textwidth]{fig/illustration-shifted.png}
    \caption{$2 \in \textrm{Shift}$, and $6 \not\in \textrm{Shift}$.}
    \label{fig:illustration-shifted}
\end{wrapfigure}
%\jj{Yes, je mettrais ça comme un théoreme!!!}hihi!
%Such approach does not put constraints on the properties of the causal measure such as collapsibility.

Having access to all shifted prognostic covariates in the \underline{two} data samples seems challenging (and maybe too optimistic). This situation could explain all the numerous recent research works about sensitivity analysis when necessary covariates are not observed or partially observed when generalizing \citep{nguyen2018sensitivitybis, nie2021covariate, colnet2022sensitivity}. In such a context, \modif{generalizing local effects (instead of conditional outcomes) is a promising strategy, as it may require less covariates, as shown in Theorem~\ref{theorem:restricted-set-for-Y-continuous-RD} below. }
\sout{Assumption~\ref{a:transportability} is appealing as it potentially reduces the needed covariates.} 
\sout{In fact, not all measures can be easier to generalize than others. }
%\es{Rephrase for clarity.}
%To keep general results forces to distinguish causal measures that are easier to generalize than others.
%But is is most likely that all baseline covariates between populations are shifted, and that many covariates are prognostic \jj{la phrase but me perturbe? tu veux dire quoi?}.
%We propose to define such situations in which less covariates are required for generalization. 

%Building on Section~\ref{section:generative-models}, we decompose the set of baseline covariates to distinguish the set of covariate $X_{\text{b}}$ being prognostic of the outcome, and  $X_{\text{\tiny m}}$ being covariates modulating treatment effect. More precisely, we say that  $b(X) = b(X_{\text{b}})$ and $m(X) = m(X_{\text{m}})$ for $b(.)$ and $m(.)$ being the non-parametric functions proposed in Lemma~\ref{lemma:expression-of-causal-quantities-under-generative-model-continuous-outcome} (continuous $Y$) or Lemma~\ref{lemma:intrication_model} (binary outcome). By definition $X = X_{\text{b}} \cup X_{\text{m}}$. While the most optimistic scenarii are that $X_{\text{m}} \subsetneq X$,  it is also possible that $X_{\text{m}} = X$.
%being coll why some measures can not be generalized by standardization due to their non-collapsibility (for e.g. Odds Ratio or Number Needed to Treat). Section~\ref{section:generative-models} shows us how one can think of a non-parametric model disentangling the baseline level and the treatment effect on the outcome. This puts in evidence that each of the treatment effect measures catches something different from the model. In particular, each of the measures contains the modifying part of the generative model $m(X)$, but some contains both parts $b(X)$ and $m(X)$. \jj{doit être dit avant le début de la section 4 sinon on ne sait pas où on va ni pourquoi} \\
\begin{theorem}
\label{theorem:restricted-set-for-Y-continuous-RD}
 Consider the Risk Difference $\tau_{RD}$. For all distributions $P_{\text{\tiny S}}(X, Y^{(0)}, Y^{(1)})$ and $P_{\text{\tiny T}}(X, Y^{(0)}, Y^{(1)})$ satisfying Assumption~\ref{a:overlap} (overlap assumption) and Assumption~\ref{a:transportability},
\sout{Assume that the generative model of Corollary \ref{lemma:working-model-continuous-Y} holds in both population (target and source) population. Then, provided Assumption~\ref{a:overlap} (overlap assumption)}
observing  all shifted \modif{treatment effect modifiers $X_{M \cap Sh}$} is sufficient for generalizing $\tau_{\text{\tiny RD}}$.
\end{theorem}
The proof can be found in Appendix~\ref{sec:proof_theorem_generalizing_local_effects}. 
Theorem~\ref{theorem:restricted-set-for-Y-continuous-RD} shows that the Risk Difference can be generalized via the local effect strategy with fewer covariates and under a weaker assumption compared to Theorem~\ref{theorem:all-covariates}. \sout{reveals that Assumption~\ref{a:transportability}, \modif{(transportability of the treatment effect)},  %\es{I wonder if you could refer to these assumptions by their informal names, like transportable treatment in this case.}
is expected to be more likely to hold for the Risk Difference only. }
Back to \eqref{eq:simulation-continuous-generative-model}, one would require only $X_1$ and $X_2$ to generalize the Risk Difference with the local effects strategy, as $X_5$ is not shifted. 
\sout{Wanting to generalize Risk Ratio or Excess Risk Ratio would still require $X_1, X_2, X_3, X_4$ for identification, no matter the approach (generalizing conditional outcomes or local effects). }%, while all other measures would require adjustment on all shifted covariates of $X_{B\cup M}$ (here $X_1, X_2, X_3, X_4$) no matter the identification strategies (generalizing conditional outcomes or local effects).

\modif{
There are two specific situations in which all collapsible causal measures can be generalized: in presence of a homogeneous effect or when the baseline and the treatment effect are independent. 
\begin{theorem}
\label{thm_homogeneous_independence_generalization}
Consider a collapsible causal measure $\tau$. 
\begin{itemize}
    \item (homogeneous treatment effect) For all distributions $P_{\text{\tiny S}}(X, Y^{(0)}, Y^{(1)})$ and $P_{\text{\tiny T}}(X, Y^{(0)}, Y^{(1)})$ satisfying Assumption~\ref{a:overlap} (overlap assumption), Assumption~\ref{a:transportability} and such that there exists $C \in \mathds{R}$ satisfying, for all $x \in  Supp(P_{\text{\tiny T}})$, $\tau^{P_{\text{\tiny S}}}(x) = C$, we have
    \begin{align}
    \tau^{P_{\text{\tiny T}}} = \tau^{P_{\text{\tiny S}}} = C. 
    \end{align}

    \item (independence between treatment effect and collapsibility weights) For all distributions $P_{\text{\tiny S}}(X, Y^{(0)}, Y^{(1)})$ and $P_{\text{\tiny T}}(X, Y^{(0)}, Y^{(1)})$ satisfying Assumption~\ref{a:overlap} (overlap assumption), Assumption~\ref{a:transportability} and such that $\tau(X)$ is independent of the collapsibility weights $w(X, P(X, Y^{(0)}))$ (Definition~\ref{def:indirect-collapsibility}), we have, 
    \begin{align}
        \tau^{P_{\text{\tiny T}}} =  \tau^{P_{\text{\tiny S}}} = \mathds{E}\left[ \tau^{P_{\text{\tiny S}}}(X) \right].
    \end{align}
\end{itemize}
\end{theorem}
}




\sout{
\begin{theorem}\label{theorem:restricted-set-for-Y-binary-SR-RR} 
Consider a binary outcome $Y$. Under Assumptions of Lemma~\ref{lemma:intrication_model}, if the effect is beneficial (resp. harmful), having access to all covariates $X_{M \cap \textrm{Sh}}$ that are shifted and  treatment effect modifiers and to the distribution $\mathbb{E}^{\text{\tiny T}}\left[ Y^{(0)} \mid X_{M \cap \textrm{Sh}} \right]$ in the target population is sufficient  for generalizing $\tau_{\text{\tiny RR}}$ (resp. $\tau_{\text{\tiny SR}}$), provided such a set of covariates satisfies the overlap assumption (Assumption~\ref{a:overlap}).
\end{theorem} 
}


 

%\jj{est-ce qu'il faudrait pas un exemple pour voir qui est XM et XB dans le cas binaire comme l 'equation 8?}\bc{oui ! je peux remonter le coup du stress et la roulette russe :)}
% We think that this should be further investigated, to confirm if what theory suggests is indeed recovered in nature.

%These lemmas are closed in the spirit to lemmas from \cite{huitfeldt2018choice} (and by the way, to obtain the lemma similar derivations as us are made, see Appendix 2 were we can find the intrication working model as an intermediate tool but without the X). Big difference is that we had the conditioning on $X$. A difference is that we claim that there is more chance that RR and SR are portable (equal in a different setting) because due to the intrication model and the Lemma, we need less covariates for standardization. We do not base this conclusion on the fact that intrication model contains a biological mechanism, even if we recognize that this model encode how a treatment effect on a binary outcome is perceived by a clinician better than logistic model family. Take-away: some homogeneity conditions can be proposed on the working model. If so, then it is possible to obtain one measure being strictly generalizable. The homogeneity conditions proposed in Section~\ref{sec:strict-generalizability} allows to link the homogeneity from Definition~\ref{def:homogeneity}, and with collapsibility to obtain one invariant causal measure. Interestingly, this can be seen on the working model.

\section{Illustration through simulations}\label{sec:simulations}

We use synthetic simulations to illustrate Theorems~\ref{theorem:all-covariates} and \ref{theorem:restricted-set-for-Y-continuous-RD}, that is different covariates sets are required to \sout{retrieve}\modif{identify} the target population effect depending on \textit{(i)} the causal measure of interest   and \textit{(ii)} the generalization method.
All implementations details, as well as the estimation strategies are provided in Appendix~\ref{appendix:additional_simulations}. Other experiments studying the impact of missing covariates or misspecified models are also presented in Appendix~\ref{appendix:additional_simulations} (see Figure~\ref{fig:simulations-continuous-Y-missing-X1} and Figure~\ref{fig:simulations-continuous-Y-mispe}). 
%In appendix we propose one additional simulation set-up, close to a stunning clinical questions from the 2000's: the effect of oral contraceptive on thrombosis (see Section~\ref{appendix:pill-scare-simulation}). 
\sout{Appendix~\ref{appendix:comments-on-estimation} \sout{gives comments}\modif{proposes suggestions} on how to transform identification formula (see Propositions~\ref{proposition:generalization-density} and \ref{prop:generalization-of-local-effects}) into estimation. \modif{In addition, each step of estimation is detailed in Sections~\ref{appendix:continuous-estimation-steps} and \ref{appendix:simulation-binary-estimation-steps}.}}
The code to reproduce the simulations is available on \href{https://github.com/BenedicteColnet/ratio-versus-difference}{github} (see repository \texttt{BenedicteColnet/ratio-versus-difference}).
 \begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{fig/simulations-continuous-April-2023.png}
    \caption{\textbf{Results of the simulations for a continuous outcomes}: where the generative model corresponds to \eqref{eq:simulation-continuous-generative-model}\modif{, and where $b(.)$ and $m(.)$ are linear functions (more details are given in Appendix, see \eqref{eq:Ymodel-simulation-continuous})}. Column 1 corresponds to generalizing conditional outcome \modif{(Proposition~\ref{proposition:generalization-density})}, column 2 corresponds to generalizing local effect \modif{(Proposition~\ref{prop:generalization-of-local-effects})}. For these two approaches we use different covariates set, with \textcolor{Goldenrod}{\textbf{shifted treatment effect modulators}} ($X_1$, $X_2$), \textcolor{RedOrange}{\textbf{shifted prognostic covariates}} ($X_1$, $X_2$, $X_3$, and $X_4$), and all \textcolor{Mahogany}{\textbf{prognostic covariates}}  ($X_1$, $X_2$, $X_3$, $X_4$, $X_4$ and $X_6$). According to Theorems~\ref{theorem:all-covariates} and \ref{theorem:restricted-set-for-Y-continuous-RD}, only the Risk Difference can be generalized with a restricted covariates set. Simulations are performed with $1000$ repetitions, a source sample size of $500$ and target sample size of $1,000$. Estimation is performed with plug-in g-formula modeling all responses with an OLS approach \modif{as detailed in Section~\ref{appendix:continuous-estimation-steps}}.}
    \label{fig:simulations-continuous-Y}
\end{figure}
%\es{ces mêmes ensembles sont utilisés pour tout ? Estimation des poids de collapsibilité, des fonctions $\mu$ ?}\bc{aie je n'ai pas compris ta question :(}



\subsection{Continuous outcome}
We propose a situation where the continuous outcome is generated from six prognostic covariates $X_1, \dots X_6$ as detailed in \eqref{eq:simulation-continuous-generative-model}. More precisely, $B=\{1,2,3,4,5,6\}$, and $M = \{1,2,5\}$, while only covariates $X_1, X_2, X_3, X_4$ are shifted between $P_{\text{\tiny S}}$ and $P_{\text{\tiny T}}$. \modif{For this simulation, }both $b(.)$ and $m(.)$ are linear functions of the covariates \modif{(see Section~\ref{appendix:continuous-generative-model}), so that estimation with an OLS procedure is well-specified}. 
Figure~\ref{fig:simulations-continuous-Y} presents results, where the \textcolor{magenta}{\textbf{pink}} dashed line represents the source causal effect and the \textcolor{blue}{\textbf{blue}} dashed line represents the target causal effect. %\sout{As the outcome $Y$ is continuous, }
%\es{I'm not sure this is the reason: surely it is more the fact that you have used a linear model suited to the risk difference in order to generate your data? I think I would be more interested to see simulations where your generative model was *not* linear in this way. I conjecture that all covariates could easily become entangled.} \jj{à faire Béné?}\bc{je suis d'accord avec le reviewer, ce n'est pas la bonne raison } 
\modif{As expected for the outcome generalization strategy (Theorem~\ref{theorem:all-covariates}), all causal measure can be generalized using all prognostic and shifted covariates (orange boxplots).}
Note that adding all prognostic covariates (red boxplots) leads to more precision, in accordance with what is proposed in \cite{colnet2022reweighting} for the risk difference\footnote{\modif{This is similar to adding an instrument or an outcome-related covariate in an adjustement set when estimating causal effect from a single observational data set \citep{brookhart2006variable}.}}.
According to Theorem~\ref{theorem:restricted-set-for-Y-continuous-RD}, the Risk Difference $\tau_{\text{\tiny RD}}$ can be generalized \modif{via the local effect strategy using  less covariates, namely} the shifted treatment effect modulators, $X_1$ and $X_2$ (yellow boxplot). \sout{For this, both procedures (generalizing the conditional outcomes or the local effects) are equivalent due to the linearity of the expectation (the second row of Figure~\ref{fig:simulations-continuous-Y} is identical across procedures). We indeed observe that only the RD can be recovered with the smaller covariates set composed of shifted treatment effect modulators .} \modif{We observe that all other causal measures require access to all shifted prognostic covariates in both strategies in order to retrieve the target effect.}  %We further added a third method\gv{Retrospectivement, je ne suis pas certain que cela soit utile. J'ai peur que cela dilue le message.} corresponding to a generalization of local effects with $g(X, P(X,Y^{(0)}))=1$. This is equivalent to a naive approach where the local effects are averaged without the proper collapsibility weights (third column of Figure~\ref{fig:simulations-continuous-Y}). This confirms that naively aggregating ratio can lead to a bias. 
 \\

%\modif{Finally, other situations are not discussed in this article such as the situation where :

%\begin{itemize}
%    \item \textit{one or more shifted treatment effect modifier are not observed}. In such a situation the target effect can not be identified not matter the method and the causal effect considered. To illustrate this phenomenon, additional simulations are proposed in Appendix (see Figure~\ref{fig:simulations-continuous-Y-missing-X1} where the exact same simulation than Figure~\ref{fig:simulations-continuous-Y} is performed but omitting $X_1$).
%    \item  \textit{the estimation methods for nuisance parameters estimation are mispecified}. In such situation, the estimator can be biased. As possible solution is to rely on non parametric estimators of nuisance parameters. This phenomenon is illustrated in Appendix, see Figure~\ref{fig:simulations-continuous-Y-mispe}. 
%\end{itemize}



% } %\es{Can you offer the intuition for why this should be the case?} \jj{Béné, dire que comme instrument?} bc : footnote ajoutée
%\jj{la figure n'est pas commentée}
%\jj{Tu ne dis pas ce que c'est le truc naive}


\subsection{Binary outcome}

We enrich the example of the Russian Roulette assuming that the effect of the Russian Roulette itself is modulated by covariates.
\modif{This gloomy example is, of course, completely fictitious and is used for better understanding.}
We adapt the generative model of \eqref{eq:intuition-of-intrication} into
\begin{equation}\label{eq:simulation-binary-generative-model}
    \mathbb{P}\left[ Y^{(a)} = 1 \mid X = x \right] = b(X_1, X_2, X_3) + a\,\left(1-b\left(X_1, X_2, X_3\right)\right)\,m_b(X_2, X_3),
\end{equation}
where $X_1=\texttt{lifestyle}$, $X_2=\texttt{stress}$, and $X_3=\texttt{gender}$, a situation where individuals' baseline risk of death depends on their lifestyle, stress, and gender. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.90\textwidth]{fig/simulations-binary-April-2023.png}
    \caption{\textbf{Simulation with binary outcome $Y$}: for a monotonous and deleterious effect. %Therefore conditions of Lemma~\ref{lemma:monotonous-effect} are satisfied, allowing to generalize the Survival Ratio with fewer covariates, and in particular only \textcolor{Goldenrod}{\textbf{shifted treatment effect modulators}}, here \texttt{stress}. 
    Adjusting on \textcolor{RedOrange}{\textbf{shifted prognostic covariates}} (\texttt{stress} and \texttt{lifestyle}), or with all \textcolor{Mahogany}{\textbf{prognostic covariates}} (\texttt{stress, lifestyle,} and \texttt{gender}) enables generalization of all causal measures by generalization of the conditional outcome or re-weighting of local effect if possible (only for collapsible measures, namely RR, SR, and RD). %Most of the time, naively re-weighting local effects does not allow to retrieve the causal effect, see for example in the third column the NNT, OR, or RR. 
    On this simulation, estimation is done with IPSW estimator, source (resp. target) sample being of size $n=5\,000$ (resp. $m=20,000$), with $1000$ repetitions.} 
    \label{fig:simulations-binary-Y-russian-roulette}
\end{figure}
We assume that the effect of the Russian Roulette can be modulated by stress (imagine individuals having a heart attack as soon as the gun is approaching their head) and gender (the executioner being more merciful when facing a women). 
We further assume that \texttt{gender} is the only covariate with no shift between the two populations. In particular, we suppose that $P_{\text{\tiny S}}$ is composed of more people with a good lifestyle but are very stressed, while in $P_{\text{\tiny T}}$ individuals have a poor lifestyle but a low stress. %\sout{Therefore Theorem~\ref{theorem:restricted-set-for-Y-binary-SR-RR} tells us that the Survival Ratio can be generalized to another target population having at hand only \texttt{stress}, without adjusting on \texttt{lifestyle} and \texttt{gender}. \es{This is practical and interesting, and reminiscent of the kind of thought processes we use when try to imagine what might make a sufficient set of covariates to adjust for confounding.} Wanting to generalize all other measures (no matter the method) would require \texttt{lifestyle} \underline{and} \texttt{stress}.}  
%We therefore expect the effect \modif{measured on the risk difference scale} of the Russian Roulette to be higher in the source population than in the target population due to both different baseline level and heterogeneity of treatment effect.
%
%The  model is described in \eqref{eq:simulation-binary-generative-model} with three prognostic covariates: \texttt{lifestyle}, \texttt{gender}, and \texttt{stress}. 
\modif{Details on the generative model are provided in Appendix (see Section~\ref{appendix:simulation-binary-data-generative-model})}. 
%We assume that the source population $P_{\text{\tiny S}}$ contains the same proportion of men and women as in the target population $P_{\text{\tiny T}}$, but that the two other covariates (\texttt{lifestyle} and \texttt{stress}) are shifted.  
%\sout{\sout{Doing so, }\modif{Because in this set up the only shifted treatment effect modifier is \texttt{stress}, }Theorem~\ref{theorem:restricted-set-for-Y-binary-SR-RR}  states that the Survival Ratio is identifiable having at hand only the covariate \texttt{stress} when generalizing local effects, while all other causal measures require to have access to all shifted prognostic covariates (\texttt{stress} and \texttt{lifestyle}). }



\sout{Simulations \sout{confirm}\modif{illustrates} the model and} Results are shown in Figure~\ref{fig:simulations-binary-Y-russian-roulette}. \modif{Note that on the simulation both the NNT and the OR cannot be generalized via the local effect strategy, as these measures are not collapsible.}
\modif{As expected for the outcome generalization strategy (Theorem~\ref{theorem:all-covariates}), all causal measure can be generalized using all prognostic and shifted covariates (orange boxplots). Note that this appears to hold also for the local effect strategy.}
%Adding all shifted prognostic covariates allows to retrieve all effect measures (\textcolor{RedOrange}{orange} boxplots), in particular generalizing conditional outcomes.
\sout{Generalizing local effects work only for collapsible measure, information on $Y^{(0)}$ and with the appropriate weights (see RR, SR, and RD).} 


%As for the continuous outcomes we added a third column for a naive generalization of local effects, showing that most of the time this approach leads to biased estimates (see for example NNT or RR). 


%\jj{Il faudra mettre une footnote qui pointe vers ton papier d'avant, pour dire attention quand on estimera par la suite, si on met plus de variables ça peut desfois améliorer la variance? }\bc{oui ! tout à fait. Je pensais dans la partie simulation. je déplace ton commentaire pour ne pas oublier.}

\section{Conclusion}

\modif{
The choice of a population-level measure of treatment effect has been much debated. Indeed, all causal measures do not share the same properties, which may lead to different interpretation of the treatment effect. In particular, we show that collapsibility is a very important property, as it allows computing the average treatment effect via a reweighting of local effects on substrata. 
Among such collapsible measures, only the Risk Difference is able to disentangle the treatment effect from the baseline at both a strata (CATE) and population (ATE) level (see Theorem~\ref{th_onlyRD_separates_baseline_tteffect}). This generic result holds for both continuous and binary outcomes, but only for a restricted range of baseline functions in the case of bounded outcomes (see Theorem~\ref{th_onlyRD_separates_baseline_tteffect_bounded_outcome}).   
Our analysis of the different properties of causal measures leads us to establish two different strategies for generalization, based on the potential outcomes (Proposition~\ref{proposition:generalization-density}) or the local effect (Proposition~\ref{prop:generalization-of-local-effects}). The first approach can be applied to any causal measure but requires a stronger assumption. The local effect strategy can be applied to collapsible causal measures only but requires a less stringent assumption, and potentially fewer covariates than the first approach. In particular, we show that all shifted prognostic variables are needed to generalize the potential outcomes (Theorem~\ref{theorem:all-covariates}), while only shifted treatment effect modifiers are needed to generalize the Risk Difference via the local effect procedure (Theorem~\ref{theorem:restricted-set-for-Y-continuous-RD}). Regardless of the outcome type (continuous or binary), the Risk Difference may require fewer variables than other causal measures to be generalized. Note that this is not always the case: if the treatment effect of the Risk Difference is entangled with the baseline (as in the Russian Roulette example), then generalizing the Risk Difference via local effect would require all shifted prognostic variables. 
Note that all other causal measures are able to separate the treatment effect from the baseline in very specific settings (e.g., homogeneous treatment effect), and are thus easily generalizable in these contexts (see Theorem~\ref{thm_homogeneous_independence_generalization}).} 
%This is the case of the Survival Ratio in the Russian Roulette example

 
\sout{We bring a new argument: a well-chosen measure is easier to generalize to a population different from that of the initial study or to sub-populations, crucial to make decisions based on this measure. Indeed, as the probability of different outcomes often varies across individuals, the average treatment effect typically depends on the population considered. A \emph{collapsible} measure --such as the risk difference, the Risk Ratio or the survival ratio but not the odds ratio-- can be computed from local effects, on strata of the population. Reweighting these strata then adapts the measure to a new population. Stratification must be done along the individual's characteristics that modulate the probability of outcomes: treatment-effect modifying covariates. But less stratification is needed for a good choice of population-level measure, one that is not affected by covariates that modulated only the baseline risk, common to treated and non-treated individuals. We showed that \sout{if the outcome is continuous, then} the Risk Difference is the only causal measure to disentangle the treatment effect modification from the baseline (\Cref{th_onlyRD_separates_baseline_tteffect}). Thus, this measure is easier to generalize, as it only requires variables that are shifted treatment effect modifiers (and, in particular, does not require baseline information,  see \Cref{theorem:restricted-set-for-Y-continuous-RD}). In the absence of the control outcome in the target population, any measure of treatment effect can be generalized through the conditional outcome, which however typically requires more covariates (Theorem~\ref{theorem:all-covariates}). 

For binary outcomes, understanding the conditional mechanism can be done via other causal measures: 
\sout{depends on the expectation of the modification, while the Risk Ratio or Excess Risk Ratio \sout{highly} depends on the baseline (Lemma~\ref{lemma:expression-of-causal-quantities-under-generative-model-continuous-outcome} and Theorem~\ref{theorem:restricted-set-for-Y-continuous-RD}).} \sout{But if the outcome is binary, relative measures such as the Risk Ratio or the Survival Ratio can remove the baseline level (Lemma~\ref{lemma:monotonous-effect} and Theorem~\ref{theorem:restricted-set-for-Y-binary-SR-RR}).} according to Lemma~\ref{lemma:monotonous-effect}, the Risk Ratio is the appropriate measure when the effect is beneficial (i.e. reduces events), while the Survival Ratio has better properties when the effect is harmful (i.e. increases events). \sout{If the treatment is beneficial and harmful at the same time, none of the measures can remove the baseline level. }}
 %\gv{Todo: rajouter les ref des lemmes et equation}

\sout{Finally, note that if some measures are easier to generalize (i.e. needs fewer covariates to adjust on), then a by-product of this result is that they should be less sensitive to a population's shift. \cite{Spiegelman2017Modeling} does mention that empirically the Risk Ratio seems to be more constant accross populations. The illustrative example (Table~\ref{tab:introduction-diastolic}) in introduction perfectly illustrates this phenomenon too.
}

\sout{So, which is the best measure to summarize a causal effect across a population but facilitate reasoning at the individual level? It depends. For continuous outcomes, the Risk Difference is not modulated by a varying baseline risk. For binary outcomes, prefer a Risk Ratio for beneficial effects and a Survival Ratio for harmful ones.}
%\es{I understand the desire to sum up, but this seems too much of a simplification of your work. Omit?} \bc{agree}

%Bullet points of what come into my mind into take home message and contribution and futur works, to reorganize, and need to add the story of deletarious or not effect
%\begin{itemize}
%\item With continuous outcome, RD could be seen as the measure to recommand as enjoying many nice property, directly collapsible, easily generalizable (without knowledge of Y(0)) using either only treatment effect modifiers that are shifted, or if we don't know which are the treatment effect modifier using all prognostic variables shifted (Note that it may impact the variance in estimation, cf Béné). The baseline and modulation effetcs are not mixed which may facilitate the interpretation of the causal measure. 
%\item With binary outcome, RR and SR can be seen has having more interesting properties: collapsible, the baseline and modulation of the effect can be disantangled \jj{En fait pas certaine...}, can be generalized by reweighting the effect if Y(0) is known and using less variables only treatment effect modifiers shifted or if Y(0) unknown can be generalized by condition outcome formulae with more variables. 
%\item As the litterature has focused on generalization of RD only, here we generalize all the other measures
%\item Coming back at table 1, our alternative way of thinking about heterogeneity can be understood as follow, RR manage to disantangle the effect between baseline and modifier of the effect and here is homogénéous, whereas RD just simply does not manage to disantangle the effetcs. 
%\item From an individual point of view, this mixing of baseline and modifier is not necessarely a bad thing
%\item Futur works involve looking at estimation, in particular for generalization of RR
%\item If the effect is no longer monotone, we are a bit lost....
%\item Contribution: unifying a large part of the litterature on the different metrics with the  suggestion of  a new model + linking collapsibility and a certain notion of generalization + discussing other generalization
%\item g-formula plus simple à la fin? pas de choix de variable à faire, moins difficile? car dure de savoir si modifiers ou non?, marche qu'on soit collapsible ou non
%\item je fais quoi quand je sais pas si l'effet est délétaire ou beneficique?
%\end{itemize}\bc{autre bullet point: extension à la survie ?}\bc{need for empirical assessment of these results looking at large trials. Luckyly, this assumptions could be tested.}
%In particular, we wonder whether some treatment effect measures are more or less sensitive to a change of distribution. In other words: What measure is equal in a different setting? The absolute augmentation of the deaths? Or its tripling? Or the division of survival?




\section*{Acknowledgments} 
First of all we would like to thank clinician François-Xavier \textsc{Ageron} who first raised our interest on generalizability and the choice of measure. %This highlights how interdisciplinarity research work can lead to original and fundamental research questions.
We also would like to thank fruitful discussions with our clinicians collaborators, in particular with François-Camille \textsc{Grolleau} and Raphaël \textsc{Porcher}.
We thank Maxime \textsc{Fosset} and Marine \textsc{Le Morvan} who gave precious comments on the manuscript. Thank you also to Wouter \textsc{van Amsterdam} for careful proofreading.
Finally, we would like to thank Anders \textsc{Huitfieldt}: his research papers (and blog articles) have been precious sources of inspiration for our work.



\section*{Fundings}
Authors are all funded by their respective employer (Inria or École polytechnique). 

This work/project was partially and publicly funded through ANR (the French National Research Agency) under the ``\textit{Investissements d’avenir}" program with the reference \texttt{ANR-16-IDEX-0006}.

GV acknowledges funding from Intercept-T2D (\texttt{HORIZON-HLTH-2022-STAYHLTH-02-01})



\section*{Data availability statement} The simulations that support the findings of this study are openly available on github at \texttt{BenedicteColnet/ratio-versus-difference}.

\section*{Conflict of interest} Authors state no conflict of interest.

\bibliographystyle{chicago}
\bibliography{references}

\appendix


\newpage

\begin{center}
    {\Large \textbf{Appendix}}
\end{center}

\section{Treatment effect measures}\label{appendix:list-of-measures}

\textit{This section completes Section~\ref{sec:formalization-and-key-contributions} (and more specially Section~\ref{subsec:causal-measures-presentation}) by exposing the different treatment (or causal) effect measures.}

\subsection{About the definition of causal measures}

%In this section we recall the definition of all measures used in this paper or that can be found in applied medical work.
%As all of these measures correspond to a combination of the two potential outcomes expectations, such that the concept of causal effect measures could be written in a general way. 
Recall that all causal measures used in this paper or present in applied medical work can be defined as follows. 
\begin{definition}[Causal effect measures -- \cite{Pearl2000Book}]
Assuming a certain joint distribution of potential outcomes $P(Y^{(0)}, Y^{(1)})$, which implies that a certain treatment $A$ of interest is considered, we denote by $\tau^P$ any functional of the joint distribution of potential outcomes. More precisely,

\begin{align}
\mathcal{P} \quad & \rightarrow \mathbb{R} \\
P(Y^{(0)}, Y^{(1)}) & \mapsto \tau^{P}
\end{align}
\end{definition}

This definition is also valid for any subpopulation, as for any covariate $X$, $\tau^{P}(X)$ is defined as a functional of $P(Y^{(0)}, Y^{(1)}\mid X)$. \modif{This definition is the one used in this article.}
%Note that this definition could admit many more causal measures than the one presented in this work.
%Introducing a definition is meant for \textit{(i)} generality of the definition and \textit{(ii)} to highlight what kind of mathematical object is a causal measure. For instance, this definition highlights the fact that a so-called treatment or causal effect naturally depends on \emph{the population considered}.



\textit{Why do we say that those measures are causal?} Note that the same definition could have been made on the distribution $P(A,Y)$, comparing expectation on two distributions: $P(Y \mid A=1)$ and $P(Y \mid A=0)$. For example, within the statistical community, the odds ratio is often known as the strength of the association between two events, $A=1$ and $A=0$ and therefore defined as:

\begin{align*}
    OR := \frac{P(Y = 1 \mid A=1)}{P(Y = 0 \mid A=1)} \cdot \frac{P(Y = 0 \mid A=0)}{P(Y = 1 \mid A=0)}.
\end{align*}

In such a situation, the OR measure would be an associational measure and not a causal measure, except if there is no confounding in the distribution considered (for e.g. in the case of a Randomized Controlled Trial). To avoid discussion about confounding, in this paper we never consider distribution such as $Y \mid A, X$ or $Y \mid A$. We rather consider $Y^{(a)}  \mid X$. For any new reader discovering the potential outcomes framework, we refer to the first chapters of \cite{imbens2015causal} for a clear and complete exposition of this notations inherited from Neyman. Note that \cite{Didelez2021collapsibility} make the same distinction when discussing collapsibility questions.

\begin{figure}[!htbp]
\centering
\begin{subfigure}{.45\linewidth}
\includegraphics[width=\linewidth]{fig/RD-plot.png}
\caption{\textbf{Risk Difference (RD)}}\label{fig:RD}
\end{subfigure}%
\hspace{1cm}
\begin{subfigure}{.45\textwidth}% glorified minipage
\includegraphics[width=\linewidth]{fig/NNT-plot.png}
\caption{\textbf{Number Needed to Treat (NNT)}}\label{fig:NNT}
\end{subfigure}%
\\
\vspace{0.5cm}
\begin{subfigure}{.45\linewidth}
\includegraphics[width=\linewidth]{fig/RR.png}
\caption{\textbf{Risk Ratio (RR)}}\label{fig:RR}
\end{subfigure}%
\hspace{1cm}
\begin{subfigure}{.45\linewidth}% glorified minipage
\includegraphics[width=\linewidth]{fig/SR.png}
\caption{\textbf{Survival Ratio (SR)}}\label{fig:SR}
\end{subfigure}%
\\
\vspace{0.5cm}
\begin{subfigure}{.45\linewidth}
\includegraphics[width=\linewidth]{fig/odds-plot.png}
\caption{\textbf{Odds Ratio (OR)}}\label{fig:OR}
\end{subfigure}%
\hspace{1cm}
\begin{subfigure}{.45\linewidth}% glorified minipage
\includegraphics[width=\linewidth]{fig/log-odds-plot.png}
\caption{\textbf{Log Odds Ratio (log-OR)}}\label{fig:logOR}
\end{subfigure}%
\\
\vspace{1cm}
\begin{minipage}{0.49\textwidth}

\caption{\textbf{Plots of the ranges of the different metrics as a function of the proportion of events in control group}, namely $\mathbb{E}[ Y^{(0)}]$ (x-axis), and of the proportion of events in treated group, namely $\mathbb{E}[ Y^{(1)}]$ (y-axis). See Subfigure~\ref{fig:how-to-read}. As both the colors and the different scale illustrate, the ranges of the effect considerably differ with the metric chosen. Similar plots can be found under the name "\textit{L'Abbé plots}" \citep{Labbe1987MetaAnalysis, Jimnez1997GraphicalDisplayUseful,Deeks2022IssuesInSelection} in research works related to meta-analysis.} %Note that for the NNT (Figure~\ref{fig:NNT}) we only represented the quarter of the plot when an event encodes death as usually done in the medical field, with threshold at 20 for readability of the scale. 
\label{fig:big-plot-with-all-metrics}
\end{minipage}
\begin{minipage}{0.49\textwidth}
  \begin{flushright}
    \begin{subfigure}{1.1\linewidth}
\centering
\includegraphics[width=\textwidth]{fig/how-to-read.png}
\caption{\textbf{Legend}}\label{fig:how-to-read}
\end{subfigure}%
  \end{flushright}
\end{minipage}
\end{figure}



\subsection{Common treatment effect measures}

As highlighted by Definition~\ref{def:causal-measure}, many measures could be proposed. Here we detail common measures found in applied works and propose an illustration for the case of binary outcomes (Figure~\ref{fig:big-plot-with-all-metrics}). 
Most of the time, the distinction is made on whether or not the measure is an absolute or a relative effect.

\subsubsection{Absolute measures}

\begin{definition}[Risk Difference (RD)]
The risk difference is a causal effect measure defined as the difference of the expectations (also called risks),

\begin{equation*}
    \tau_{\text{\tiny RD}} = \mathbb{E}[Y^{(1)}] - \mathbb{E}[Y^{(0)}].
\end{equation*}

%Note that the risk difference can also be written, $\tau_{\text{\tiny RD}} = \mathbb{E}[ Y^{(1)} - Y^{(0)}] = \mathbb{P}[Y^{(1)} = 1] - \mathbb{P}[Y^{(0)} = 1]$.
\end{definition}
RD is also named Absolute Risk Reduction (ARR), Absolute Effect (AE), Absolute Difference (AD), or Excess Risk (ER).



\begin{definition}[Number Needed to Treat (NNT)]\label{def:nnt}

The number needed to treat (NNT) is a causal effect measure defined as the average number of individuals or observations who need to be treated to prevent one additional outcome,

\begin{equation*}
  \tau_{\text{\tiny NNT}}  = \frac{1}{\mathbb{E}[Y^{(1)}= 1] - \mathbb{E}[Y^{(0)}=1]}
\end{equation*}

\end{definition}
 The Number Needed to Treat (NNT) has been proposed as a measure rather recently \citep{Laupacis1988AnAssessmentOfClinically}. A harmful treatment is usually called the Number Needed to Harm (NNH) and made positive.

\subsubsection{Relative measures}

\begin{definition}[Risk Ratio]
The Risk Ratio is a causal effect measure defined as the ratio of the expectations,

\begin{equation*}
     \tau_{\text{\tiny RR}} = \frac{\mathbb{E}[Y^{(1)}]}{\mathbb{E}[Y^{(0)}]} 
\end{equation*}

\end{definition}

The Risk Ratio (RR) is also named Relative Risk (RR), Relative Response (RR), or Incidence Proportion Ratio (IPR)

\begin{definition}[Survival Ratio]
The survival ratio is a causal effect measure defined as the Risk Ratio were labels are swapped,

\begin{equation*}
     \tau_{\text{\tiny SR}} = \frac{1-\mathbb{E}[Y^{(1)}]}{1-\mathbb{E}[Y^{(0)}]} 
\end{equation*}

\end{definition}

It is possible to introduce a measure that captures both the Risk Difference, but normalized by the baseline.


\begin{definition}[Excess relative risk (ERR)]

\begin{equation*}
  \tau_{\text{\tiny ERR}}  = \frac{\mathbb{E}[Y^{(1)}] - \mathbb{E}[Y^{(0)}]}{\mathbb{E}[Y^{(0)}]}
\end{equation*}

\end{definition}

The Excess relative risk (ERR) has been proposed by \cite{cole1971attributable}. Note that,

\begin{equation*}
    \tau_{\text{\tiny ERR}}   = \tau_{\text{\tiny RR}} - 1 .
\end{equation*}


\begin{definition}[Relative Susceptibility (RS)]
\begin{equation*}
    \tau_{\text{\tiny RS}}  := \frac{\mathbb{E}[Y^{(1)}] - \mathbb{E}[Y^{(0)}]}{1-\mathbb{E}\left[Y^{(0)}\right]}.
\end{equation*}
\end{definition}

Note that,

\begin{equation*}
      \tau_{\text{\tiny RS}} =  1 - \tau_{\text{\tiny SR}}.
\end{equation*}

Finally, another measure is often used based on odds. Odds are a way of representing probability in particular for betting. For example a throw with a die will produce a one with odds 1:5. The odds is the ratio of the probability that the event occurs to the probability it does not. 


\begin{definition}[Odds Ratio (OR)]
The odds ratio is a causal effect measure defined as the ratio of the odds of the treated and control groups,
\begin{equation*}
     \tau_{\text{\tiny OR}} := \frac{\mathbb{P}[Y^{(1)} = 1]}{1-\mathbb{P}[Y^{(1)} = 1]}\, \left(  \frac{\mathbb{P}[Y^{(0)} = 1]}{1-\mathbb{P}[Y^{(0)} = 1]}\right)^{-1}.
\end{equation*}

\end{definition}
Odds Ratio (OR) is sometimes named Marginal Causal Odds Ratio (MCOR). This is by opposition to a conditional Odds Ratio, being defined as,


\begin{equation*}
     \tau_{\text{\tiny OR}}(X) := \frac{\mathbb{E}[Y^{(1)} = 1 \mid X = x]}{1-\mathbb{E}[Y^{(1)} = 1\mid X = x]}\, \left(  \frac{\mathbb{E}[Y^{(0)} = 1\mid X = x]}{1- \mathbb{E}[Y^{(0)} = 1\mid X = x]}\right)^{-1},
\end{equation*}

often used due to its homogeneity when considering a logistic generative model of the outcome (see Section~\ref{proof:non-collapsibility-OR} for a detailed proof).
The OR is known to approximate the RR at low baseline (see for example the illustrative example of Table~\ref{tab:introduction-diastolic}).


\begin{proof}
{\footnotesize {\color{Blue} 
    $
   \mathbb{P}[Y^{(1)} = 1] \le \mathbb{P}[Y^{(0)} = 1] \ll 1 \implies \tau_{\text{\tiny OR}}  = \frac{\mathbb{P}[Y^{(1)} = 1]}{1-\mathbb{P}[Y^{(1)} = 1]}\cdot  \frac{1-\mathbb{P}[Y^{(0)} = 1]}{\mathbb{P}[Y^{(0)} = 1]} \approx\frac{\mathbb{P}[Y^{(1)} = 1]}{1}\cdot \frac{1}{\mathbb{P}[Y^{(0)} = 1]} =  \tau_{\text{\tiny RR}}$. }}
\end{proof}

These derivations can be found as late as in the 50's in case-control studies about lung cancer \citep{cornfield1951method}.
Also note that,


\begin{equation*}
    \tau_{\text{\tiny OR}} = \tau_{\text{\tiny RR}} \cdot \tau_{\text{\tiny SR}}^{-1} .
\end{equation*}


\begin{proof}

{\footnotesize {\color{Blue} 
    \begin{align*}
           \tau_{\text{\tiny OR}}  &= \frac{\mathbb{P}[Y^{(1)} = 1]}{1-\mathbb{P}[Y^{(1)} = 1]}\, \left(  \frac{\mathbb{P}[Y^{(0)} = 1]}{1-\mathbb{P}[Y^{(0)} = 1]}\right)^{-1} \\
           &= \frac{\mathbb{P}[Y^{(1)} = 1]}{\mathbb{P}[Y^{(1)} = 0]}\, \left(  \frac{\mathbb{P}[Y^{(0)} = 1]}{\mathbb{P}[Y^{(0)} = 0]}\right)^{-1}\\
           &=  \frac{\mathbb{P}[Y^{(1)} = 1]}{\mathbb{P}[Y^{(1)} = 0]}\,  \frac{\mathbb{P}[Y^{(0)} = 0]}{\mathbb{P}[Y^{(0)} = 1]}\\
           &= \frac{\mathbb{P}[Y^{(1)} = 1]}{\mathbb{P}[Y^{(0)} = 1]}\,  \frac{\mathbb{P}[Y^{(0)} = 0]}{\mathbb{P}[Y^{(1)} = 0]} \\
           &=  \tau_{\text{\tiny RR}} \cdot \tau_{\text{\tiny SR}}^{-1} 
    \end{align*}
    }}
\end{proof}

One can observe on Figure~\ref{fig:big-plot-with-all-metrics} (see subplots Figures~\ref{fig:OR} and \ref{fig:logOR}) the range on which the OR varies depends on the direction of the effect. Therefore, the OR is often presented encapsulated in a logarithm.
\begin{definition}[Log Odds Ratio (log-OR)]
    \begin{equation*}
        \tau_{\text{\tiny log-OR}}  := \operatorname{log}\left( \frac{\mathbb{P}[Y^{(1)} = 1]}{\mathbb{P}[Y^{(1)} = 0]}\right) - \operatorname{log}\left( \frac{\mathbb{P}[Y^{(0)} = 1]}{\mathbb{P}[Y^{(0)} = 0]}\right)
    \end{equation*}
\end{definition}





\section{Definitions found in the literature}\label{appendix:other-formal-definitions}

This section completes Section~\ref{section:causal-metrics-properties} with formalization of homogeneity of effects, heterogeneity of effects, and collapsibility we have found in the literature. Doing so, we highlight that definitions can be more or less formal, and therefore can lead to different apprehension of phenomenons, in particular collapsibility. 
%The purpose of the definitions given in Section~\ref{section:causal-metrics-properties} is to provide  intuitions behind all previously proposed definitions, while uniformizing them.


\subsection{Effect modification}

\textit{This section supports definitions proposed in Section~\ref{subsec:homogeneity}.}

Note that effect modification or heterogeneity is mentioned in many places, but not always clearly defined. This is highlighted by the following quote:

\begin{quote}
    We searched the National Library of Medicine Books, National Library of Medicine Catalog, Current Index to Statistics database, ISI web of science, and websites of 25 major regulatory agencies and organizations for papers and guidelines on study design, analysis and interpretation of treatment effect heterogeneity. Because there is not standard terminology for this topic, a structured search strategy was not sensitive nor specific and we found many resources through “snowball” searching, that is, reviewing citations in, and citations of, key methodological and policy papers. -- \citep{lesko2018considerations}
\end{quote}

\subsubsection{Definitions of heterogeneity of effect or effect modification found in the literature}

\begin{definition}[\cite{Rothman2011bookEpidemiologyIntrod}, page 51]\label{def-heterogeneity-Greenland}
    Suppose we divide our cohort into two or more distinct categories, or strata. In each stratum, we can construct an effect measure of our choosing. These stratum-specific effect measures may or may not equal on another. Rarely would we have any reason to suppose that they do equal one another.
    If indeed they are not equal, we say that the effect measure is \textit{heterogeneous} or \textit{modified} across strata.
    If they are equal we say that the measure is \textit{homogeneous}, \textit{constant}, or \textit{uniform} across strata.
    A major point about effect-measure modification is that, if effects are present, it will usually be the case that only one or none of the effect measures will be uniform across strata.
\end{definition}


\begin{definition}[\cite{VanderWeele2007FourTypes}]\label{def-VanderWeele2007FourTypes}
We say that a variable $Q$ is a treatment effect modifier for the causal risk difference of $A$ on $Y$ if $Q$ is not affected by $A$ and if there exist two levels of $A$, $a_0$ and $a_1$, such that $\mathbb{E}\left[ Y^{(a_1)}   \mid Q = q\right]-\mathbb{E}\left[ Y^{(a_0)}   \mid Q = q\right]$ is not constant in $q$.
\end{definition}


\subsubsection{Effect heterogeneity depends on the chosen scale: an illustration}

A treatment effect heterogeneity depends on the causal measure $\tau$ chosen (the scale). 
This idea is well-known in epidemiology \citep{Rothman2011bookEpidemiologyIntrod, lesko2018considerations}.
To be convinced by such phenomenon, the drawing in Figure~\ref{fig:hetero-schematic} illustrates what could be two data generative models leading to two different homogeneity and heterogeneity patterns.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/hetero-schematic.png}
    \caption{\textbf{Heterogeneity of a treatment effect depends on the scale}: Illustrative schematics where the data generative model on the left leads to a constant treatment effect on the absolute scale (RD) when conditioning on $X$, while on the data generative model on the right leads to an homogeneous treatment effect on the relative scale (RR). In both of the situations, homogeneity of treatment effect of one scale (RR or RD) leads to heterogeneity on the other scale. Note that a similar schematic is presented in \cite{Rothman2011bookEpidemiologyIntrod} (see their Figure 11–1, p. 199)}
    \label{fig:hetero-schematic}
\end{figure}


\subsection{Different definitions of collapsibility in the literature}
\label{appendix-def_collapsibility}

\textit{This section supports definitions proposed in Section~\ref{subsec:collapsibility}.}
\subsubsection{Unformal definitions}
We have found many unformal definitions in the literature, such as:

\begin{quote}
    In a single study with a non-confounding stratification variable, if the stratum-specific effects are homogenous, then they are expected to be the same as the crude effect, a desirable property known as collapsibility of an effect measure.  -- \citep{xiao2022IsORPortable}
\end{quote}


\begin{quote}
    RR but, not OR, have a mathematical property called collapsibility; this means the size of the Risk Ratio will not change if adjustment is made for a variable that is not a confounder. -- \citep{Cummings2009RelativeMeritsRRAndOR}
\end{quote}


and

\begin{quote}
    Collapsibility means that in the absence of confounding, a weighted average of stratum-specific ratios (e.g., using Mantel-Haenszel methods) will equal the ratio from a single 2 by 2 table of the pooled (collapsed) counts from the stratum-specific tables. This means that a crude (unadjusted) ratio will not change if we adjust for a variable that is not a confounder.  -- \citep{Cummings2009RelativeMeritsRRAndOR}
\end{quote}


\subsubsection{Formal definitions}
\begin{definition}[Strict collapsibility \cite{pearl1999collapsibility}]\label{def:strict-collaps-pearl-greenland}
We say a measure of association between $ Y^{(0)}$ and $ Y^{(1)}$ is strictly collapsible accross $X$ if it is constant accross the strata (subtables) and this constant value equals the value obtained from the marginal table.
\end{definition}
Similar definition as Definition~\ref{def:strict-collaps-pearl-greenland} have been proposed in \cite{liu2022correct, Didelez2021collapsibility}.
\begin{definition}[\cite{Pearl2000Book}]\label{def:collapsibility-pearl}
Let $\tau \left(P\left( Y^{(0)}, Y^{(1)} \right) \right)$ be any functional that measures the association between $Y^{(0)}$ and $Y^{(1)}$ in the joint distribution  $P\left( Y^{(0)}, Y^{(1)} \right)$. We say that $\tau$ is collapsible on a variable $V$ if

\begin{align*}
    \mathbb{E}\left[ \tau \left(P\left( Y^{(0)}, Y^{(1)} \mid V \right)  \right) \right] = \tau \left(P\left( Y^{(0)}, Y^{(1)} \right) \right)
\end{align*}
\end{definition}

Note that in his book, Judea Pearl rather present the definition of collapsibility with respect two any two covariates, not necessarily potential outcomes. Indeed, collapsibility is a statistical concept at first. As in this work we are explicitely concerned with causal metrics, this definition has been written here with potential outcomes.

\begin{definition}
[\cite{Huitfeldt2019collapsible}]\label{def:collapsibility-huitfeldt}
Let $\tau \left(P\left( Y^{(0)}, Y^{(1)} \right) \right)$ be any function of the parameters $Y^{(0)}$ and $Y^{(1)}$ in the joint distribution $P\left( Y^{(0)}, Y^{(1)} \right)$. We say that $\tau$ is collapsible on a variable $V$ with weights $w_v$ if,

\begin{align*}
    \frac{\sum_v w_v \tau \left(P\left( Y^{(0)}, Y^{(1)} \right) \mid V = v \right)}{\sum_v w_v} &= \tau \left(P\left( Y^{(0)}, Y^{(1)} \right) \right)
\end{align*}
 \end{definition}



\begin{definition}[\cite{Didelez2021collapsibility}]\label{def:collapsibility-didelez}
Let $\tau=\tau \left(P\left( Y^{(0)}, Y^{(1)} \right) \right)$ be a measure of association between $Y^{(0)}$ and $Y^{(1)}$; that is, $\tau$ is a functional of the joint distribution $P\left( Y^{(0)}, Y^{(1)} \right)$. Let $\tau_x= \tau(Y,A\mid X=x)$  be a measure of conditional association between $Y$ and $A$ given $X=x$; that is, $\tau_x$ is a functional of the conditional distribution $P(Y,A \mid X = x)$. The measure $\tau$ is called \textit{collapsible over $X$}, if $\tau$ is a weighted average of $\tau_x$ for $x \in \mathds{X}$. Strict collapsibility demands that $\tau=\tau_x$.
\end{definition}
%Note that under this definition, collapsibility seems to be defined over a certain covariate $X$. Doing the odds-ratio could be defined as a strictly collapsible as soon as the outcome is generated from a parametric logistic model such as in \eqref{eq:typical-model-used-binary-Y} (this is explicitely given as an example in their work). Note that in our work we define collapsibility differently, such that OR can not be considered as a collapsible measure.



\section{Proofs}

In this section we detail all the derivations needed to understand the results of this article. 

\subsection{Collapsibility}\label{proof:collapsibilty}

\textit{Note that not all proofs are novel work. Collapsibility results have been reported multiple times and in multiple ways as explained in the main paper. For clarity we still recall them. We indicate when the proofs are not novel or when similar proofs exist elsewhere. When we indicate nothing, this means that we have not found those results in other published work.}

\subsubsection{Proof of Lemma~\ref{lemma:direct-collapsibility-RD}}

\textit{N.B: The proof for the direct collapsibility of the RD \underline{is not} a novel contribution.}

\begin{proof}

\begin{align*}
    \tau_{\text{\tiny RD}}^P&= \mathbb{E}\left[Y^{(1)} - Y^{(0)}  \right] && \text{By definition} \\
&= \mathbb{E}\left[  \mathbb{E}\left[Y^{(1)} - Y^{(0)} \mid X  \right]  \right] && \text{Law of total expectation} \\
&= \mathbb{E}\left[ \tau_{\text{\tiny RD}}^P\left(X \right) \right].
\end{align*}


\end{proof}


\paragraph{Remark} To observe the phenomenon as weighting, one can also write this last quantity as an integral.


\begin{align*}
   \mathbb{E}\left[  \mathbb{E}\left[Y^{(1)} - Y^{(0)} \mid X  \right]  \right] 
&=  \int_{\mathds{X}}  \mathbb{E}\left[Y^{(1)} - Y^{(0)} \mid X  \right] f(x)\,dx  &&\text{Re-writing} \\
&= \int_{\mathds{X}}  \tau_{\text{\tiny RD}}^P\left( x \right)f(x)\,dx.
\end{align*}

Here, one can observe that weights are the density of $x$ in the population. Most of the time \citep{pearl2011transportability, huitfeldt2018choice, Didelez2021collapsibility} express such quantity on categorical covariates $X$, therefore using a sum.

\subsubsection{Proof of Lemma~\ref{lemma:collapsibility-of-RR-SR}}

\textit{N.B: The proof for the collapsibility of the RR and SR are extensions of \cite{Huitfeldt2019collapsible}.}


\paragraph{General comment}
In this subsection we detail the proof for collapsibility of the RR, and SR. 
Before detailing the proof, we want to highlight why the RR (and SR) is not directly collapsible.


\begin{align*}
    \tau_{\text{\tiny RR}}^P &= \frac{\mathbb{E}\left[ Y^{(1)}\right]}{\mathbb{E}\left[ Y^{(0)}\right]}  \\
    &= \frac{\mathbb{E}\left[ \mathbb{E}\left[ Y^{(1)} \mid X  \right]\right]}{\mathbb{E}\left[ \mathbb{E}\left[ Y^{(0)} \mid X  \right]\right]}  \\
    & \neq \mathbb{E}\left[ \frac{\mathbb{E}\left[ Y^{(1)} \mid X\right]}{\mathbb{E}\left[ Y^{(0)} \mid X\right]}\right],
\end{align*}
%\es{ici, on a envie d'être un peu plus précis : donner des cas dans lesquels ce n'est pas vrai}
in all generality. For example, assuming that $\mathbb{E}\left[ Y^{(0)} \mid X\right]$ and $\mathbb{E}\left[ Y^{(1)} \mid X\right]$ are independent, we have 
\begin{align*}
\mathbb{E}\left[ \frac{\mathbb{E}\left[ Y^{(1)} \mid X\right]}{\mathbb{E}\left[ Y^{(0)} \mid X\right]}\right] & = \mathbb{E}[Y^{(1)}]\mathbb{E}\left[ \frac{1}{\mathbb{E}\left[ Y^{(0)} \mid X\right]}\right] > \frac{\mathbb{E}[Y^{(1)}]}{\mathbb{E}[Y^{(0)}]} = \tau_{\text{\tiny RR}}^P,
\end{align*}
by Jensen inequality, assuming additionally that $\mathbb{E}\left[ Y^{(0)} \mid X\right] > 0$. 

\paragraph{Risk Ratio (RR)}

\begin{proof}


\begin{align*}
    \tau_{\text{\tiny RR}}^P &= \frac{\mathbb{E}\left[ Y^{(1)}\right]}{\mathbb{E}\left[ Y^{(0)}\right]}  && \text{By definition of the RR}\\
    &= \frac{\mathbb{E}\left[ \mathbb{E}\left[ Y^{(1)} \mid X \right]\right]}{\mathbb{E}\left[ Y^{(0)}\right]}  && \text{Law of total expectation used on $\mathbb{E}\left[ Y^{(1)}\right]$} \\
    &= \frac{\mathbb{E}\left[\frac{ \mathbb{E}\left[ Y^{(1)} \mid X \right]}{ \mathbb{E}\left[ Y^{(0)} \mid X \right]} \mathbb{E}\left[ Y^{(0)} \mid X \right]\right]}{\mathbb{E}\left[ Y^{(0)}\right]}  && \text{$\mathbb{E}\left[ Y^{(0)} \mid X \right] \neq 0$ ~almost surely} \\
    &= \mathbb{E}\left[\frac{ \mathbb{E}\left[ Y^{(1)} \mid X \right]}{ \mathbb{E}\left[ Y^{(0)} \mid X \right]} \frac{\mathbb{E}\left[ Y^{(0)} \mid X \right]}{\mathbb{E}\left[ Y^{(0)}\right] }\right] && \text{$\mathbb{E}\left[ Y^{(0)}\right]$ is a constant} \\
    &= \mathbb{E}\left[\tau_\text{\tiny RR}^P(X) \frac{\mathbb{E}\left[ Y^{(0)} \mid X \right]}{\mathbb{E}\left[ Y^{(0)}\right] }\right]. && \text{$\frac{ \mathbb{E}\left[ Y^{(1)} \mid X \right]}{ \mathbb{E}\left[ Y^{(0)} \mid X \right]}:= \tau_\text{\tiny RR}^P(X)$ }
\end{align*}





\end{proof}

\paragraph{Survival Ratio (SR)}

\begin{proof}
\begin{align*}
    \tau_{\text{\tiny SR}}^P &= \frac{1-\mathbb{E}\left[ Y^{(1)}\right]}{1-\mathbb{E}\left[ Y^{(0)}\right]}  && \text{By definition of the SR}\\
    &= \frac{1-\mathbb{E}\left[ \mathbb{E}\left[ Y^{(1)} \mid X \right]\right]}{1-\mathbb{E}\left[ Y^{(0)}\right]}  && \text{Law of total expectation} \\
    &= \frac{\mathbb{E}\left[\frac{1- \mathbb{E}\left[ Y^{(1)} \mid X \right]}{ 1-\mathbb{E}\left[ Y^{(0)} \mid X \right]} \left( 1-\mathbb{E}\left[ Y^{(0)} \mid X \right] \right)\right]}{1-\mathbb{E}\left[ Y^{(0)}\right]}  && \text{$1-\mathbb{E}\left[ Y^{(0)} \mid X \right] \neq 0$~ almost surely} \\
    &= \mathbb{E}\left[\tau_\text{\tiny SR}^P(X) \frac{1-\mathbb{E}\left[ Y^{(0)} \mid X \right]}{1-\mathbb{E}\left[ Y^{(0)}\right] }\right] && \text{$1-\mathbb{E}\left[ Y^{(0)}\right]$ is a constant}
    \end{align*}


\end{proof}


The Excess Risk Ratio (ERR) (resp. Risk Susceptibility) collapsibility are proven using the same derivations than RR (resp. SR).

\paragraph{Excess Risk Ratio (ERR)}

\begin{proof}
\begin{align*}
    \tau_{\text{\tiny ERR}}^P & =\frac{ \mathbb{E}\left[ Y^{(1)} - Y^{(0)} \right]}{ \mathbb{E}\left[  Y^{(0)} \right]} \\
    &= \frac{ \mathbb{E}\left[  \mathbb{E}\left[  Y^{(1)} - Y^{(0)} \mid X  \right]\right]}{ \mathbb{E}\left[  Y^{(0)} \right]} \\
    &=  \mathbb{E}\left[  \frac{\mathbb{E}\left[  Y^{(1)} - Y^{(0)} \mid X  \right]}{ \mathbb{E}\left[  Y^{(0)} \right]}\right]\\
    &= \mathbb{E}\left[  \frac{\mathbb{E}\left[  Y^{(1)} - Y^{(0)} \mid X  \right]}{ \mathbb{E}\left[  Y^{(0)} \right]} \frac{\mathbb{E}\left[  Y^{(0)} \mid X \right]}{\mathbb{E}\left[  Y^{(0)} \mid X \right]}  \right]\\
    &= \mathbb{E}\left[   \tau_{\text{\tiny ERR}}^P(X) \frac{\mathbb{E}\left[  Y^{(0)} \mid X \right]}{ \mathbb{E}\left[  Y^{(0)} \right]} \right]
\end{align*}
\end{proof}

\paragraph{Risk Susceptibility (RS)}

\begin{proof}
\begin{align*}
    \tau_{\text{\tiny RS}}^P & = \frac{ \mathbb{E}\left[ Y^{(1)} - Y^{(0)} \right]}{ 1- \mathbb{E}\left[  Y^{(0)} \right]} \\
    &=  \frac{ \mathbb{E}\left[\mathbb{E}\left[ Y^{(1)} - Y^{(0)} \mid X \right] \right]}{ 1- \mathbb{E}\left[  Y^{(0)} \right]} \\
    &=  \mathbb{E}\left[ \frac{ \mathbb{E}\left[ Y^{(1)} - Y^{(0)} \mid X \right]}{1- \mathbb{E}\left[  Y^{(0)} \right]}  \right] \\
    &=  \mathbb{E}\left[ \frac{ \mathbb{E}\left[ Y^{(1)} - Y^{(0)} \mid X \right]}{1- \mathbb{E}\left[  Y^{(0)} \right]} \frac{1- \mathbb{E}\left[  Y^{(0)} \mid X \right]}{1-\mathbb{E}\left[  Y^{(0)} \mid X \right]}  \right] \\
    &= \mathbb{E}\left[\tau_{\text{\tiny RS}}^P(X) \frac{1- \mathbb{E}\left[  Y^{(0)} \mid X \right]}{1- \mathbb{E}\left[  Y^{(0)}\right]} \right]
\end{align*}
\end{proof}


\subsubsection{Proof of Lemma~\ref{lemma:non-collapsibility}: Non-collapsibility of the OR, log-OR, and NNT}\label{proof:non-collapsibility-OR}



\textbf{Odds Ratio (OR).}
According to the first point of Lemma~\ref{lemma:logic-respecting-measures}, all collapsible measure are logic-respecting. However, according to the third point of Lemma~\ref{lemma:logic-respecting-measures}, OR is not logic-respecting. Therefore OR is not collapsible. 

\medskip 

\textbf{Log Odds Ratio (log-OR).}
The same reasoning as above holds for the log Odds Ratio. 

\medskip 
%\textit{N.B: the proof for the non collapsibility of the OR and log-OR \underline{is not} a novel contribution}\\

%See proof showing that OR is not logic-respecting in Section~\ref{proof:logic-respecting}, which leads to non-collapsibility of the OR. If a measure is not logic-respecting, then the population's effect can not be written as a weighted sum of local effects with positive weights. \\


\textbf{Number Needed to Treat (NNT).} \\

\begin{proof}
Recall that 
\begin{align}
\tau_{\text{\tiny  NNT}}^P = \frac{1}{\mathbb{E}[Y^{(1)}] - \mathbb{E}[Y^{(0)}]} \quad \textrm{and} \quad \tau_{\text{\tiny  NNT}}^P(X) = \frac{1}{\mathbb{E}[Y^{(1)}| X ] - \mathbb{E}[Y^{(0)} | X]}.
\end{align}

Assume that the NNT causal measure is collapsible, that is there exist weights $w(X, P(X,Y^{(0)}))$ such that for all distributions $P(X, Y^{(0)}, Y^{(1)})$ we have
\begin{align}
\mathbb{E}\left[ w(X, P(X,Y^{(0)}))\, \tau_{\text{\tiny  NNT}}^P(X) \right] = \tau_{\text{\tiny  NNT}}^P,\qquad \text{with }   w \ge 0,\, \text{and} \quad  \mathbb{E}\left[  w(X, P(X,Y^{(0)}))\right] = 1. \label{eq_proof_NNT_def_collapsible}
\end{align}

Note that 
\begin{align}
\tau_{\text{\tiny  NNT}}^P = \frac{1}{\mathbb{E}\left[ \frac{1}{\tau_{\text{\tiny  NNT}}^P(X)}\right]},
\end{align}
which, combined with the previous equation, leads to 
\begin{align}
\mathbb{E}\left[ w(X, P(X,Y^{(0)}))\, \tau_{\text{\tiny  NNT}}^P(X) \right] = \frac{1}{\mathbb{E}\left[ \frac{1}{\tau_{\text{\tiny  NNT}}^P(X)}\right]}.
\end{align}
Assuming that $\tau_{\text{\tiny  NNT}}^P(X) \geq 0$, by Jensen inequality, we have 
\begin{align}
\mathbb{E}\left[ w(X, P(X,Y^{(0)}))\, \tau_{\text{\tiny  NNT}}^P(X) \right] & \leq \mathbb{E}\left[ \tau_{\text{\tiny  NNT}}^P(X)\right]\\
\mathbb{E}\left[ \left( w(X, P(X,Y^{(0)})) - 1 \right) \, \tau_{\text{\tiny  NNT}}^P(X) \right] & \leq 0. \label{eq_proof_NNT1}
\end{align}
Fix $\varepsilon >0$. Assume now that there exists a measurable set $B \subset \mathcal{X}$ with positive measure, such that for all $x \in B$, $w(X, P(X,Y^{(0)})) > 1+ \varepsilon$. By choosing the distribution of $Y^{(1)}$ such that $\mathbb{E}[Y^{(1)} | X]$ is arbitrary close to $\mathbb{E}[Y^{(0)} | X]$ on $B$, one has that $\tau_{\text{\tiny  NNT}}^P(X)$ is arbitrary large, so that $\left( w(X, P(X,Y^{(0)})) - 1 \right) \, \tau_{\text{\tiny  NNT}}^P(X) $ is arbitrary large on $B$, which contradicts  \eqref{eq_proof_NNT1}. This proves that $w(X, P(X,Y^{(0)})) \leq 1$ almost surely. Since $\mathbb{E}[w(X, P(X,Y^{(0)}))] =1$, this implies that almost surely $w(X, P(X,Y^{(0)})) = 1$. Thus, one should have 
\begin{align}
\mathbb{E}\left[ \tau_{\text{\tiny  NNT}}^P(X) \right] = \frac{1}{\mathbb{E}\left[ \frac{1}{\tau_{\text{\tiny  NNT}}^P(X)}\right]}, 
\end{align}
which, according to Jensen inequality, holds only if $\tau_{\text{\tiny  NNT}}^P(X)$ is constant. Thus the Number Needed to Treat satisfies the collapsibility equation \eqref{eq_proof_NNT_def_collapsible} only in the specific case of homogeneous treatment effect. \\
This proves that the NNT is not collapsible. 

%\begin{align*}
%      \tau_{\text{\tiny NNT}}&= \frac{1}{ \mathbb{E}\left[ Y^{(1)} - Y^{(0)} \right]}  \\
%      &=  \frac{1}{ \mathbb{E}\left[ \mathbb{E}\left[ Y^{(1)} - Y^{(0)} \mid X \right] \right]}\\ 
 %     &= \mathbb{E}\left[ \frac{1}{\tau_{\text{\tiny NNT}}(X)}   \right]^{-1}.
%\end{align*}
 
\end{proof}

\subsection{Proof of Lemma~\ref{lemma:annexe_monotinic_causal_measure} }\label{app:supplementary_lemma}

\begin{lemma}
\label{lemma:annexe_monotinic_causal_measure}
Let $\tau_1 $ be any collapsible causal measure defined by Equation~\eqref{eq_causal_measure}, that is 
\begin{align}
\tau_1^P (x) = f \left(\mathds{E}[Y^{(0)} \mid X=x ] , \mathds{E}[Y^{(1)} \mid X=x ] \right),
\end{align}
and 
\begin{align}
\tau_1^P = f \left(\mathds{E}[Y^{(0)}  ] , \mathds{E}[Y^{(1)} ] \right),
\end{align}
Consider $\tau_2$ another causal measure, such that, there exists $h$ satisfying 
\begin{align}
\tau_2^P(x) = h(\tau_1(x)) \textrm{ and } \tau_2 = h(\tau_1). 
\end{align}
If $h$ is bijective and monotonic, then $\tau_2$ is logic-respecting. 
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lemma:annexe_monotinic_causal_measure}]

Since $\tau_1$ is collapsible, we know that, for all distributions of $(X, Y^{(0)}, Y^{(1)})$, 
\begin{align}
\tau_1^P = \mathds{E}[\tau_1^P(X) w(X, P(X,Y^{(0)}))].
\end{align}
Since $\tau_2 = h(\tau_1)$, we obtain
\begin{align}
\tau_2^P = h \left( \mathds{E}[h^{-1} (\tau_2^P(X)) w(X, P(X,Y^{(0)}))] \right).
\end{align}
Assume that $h$ is increasing, then $h^{-1}$ is increasing and 
\begin{align}
   h^{-1} \left( \min_x \tau_2^P(x) \right) \leq h^{-1} (\tau_2^P(X)) \leq h^{-1} \left( \max_x \tau_2^P(x) \right),
\end{align}
which implies, since $h$ is increasing, 
\begin{align}
    \min_x \tau_2^P(x) \leq h \left( \mathds{E}[h^{-1} (\tau_2^P(X)) w(X, P(X,Y^{(0)}))] \right) \leq \max_x \tau_2^P(x),
\end{align}
and thus, 
\begin{align}
\min_x \tau_2^P(x) \leq  \tau_2^P \leq \max_x \tau_2^P(x).   
\end{align}
Consequently, the causal measure $\tau_2$ is logic-respecting. The same reasoning holds for a decreasing function $h$. 

\end{proof}


\subsection{Proof of Lemma~\ref{lemma:logic-respecting-measures} (about logic-respecting measures)}\label{proof:logic-respecting}


\subsubsection{All collapsible measures are logic respecting}
\begin{proof}
    We recall from Definition~\ref{def:indirect-collapsibility} that a measure $\tau$ is said to be collapsible (directly or not), if there exist positive weights $w(X, P(X,Y^{(0)}))$ verifying  $\mathbb{E}\left[w(X, P(X,Y^{(0)}))\right] = 1$, such that
   
    \begin{equation*}
        \tau^P = \mathbb{E}\left[ w(X, P(X,Y^{(0)}))\tau^P(X)  \right].
    \end{equation*}


    Then,

    \begin{align*}
         \tau^P &\le  \mathbb{E}\left[w(X, P(X,Y^{(0)})) \max_x \left( \tau^P(X) \right)  \right]\\
         \tau^P &\le  \mathbb{E}\left[w(X, P(X,Y^{(0)})) \right] \max_x \left( \tau^P(x) \right)  \\
         \tau^P &\le\max_x \left( \tau^P(x) \right)
    \end{align*}

    using the properties of the weights. Similarly, one can show that,

  \begin{align*}
        \mathbb{E}\left[w(X, P(X,Y^{(0)})) \min_x \left( \tau^P(x) \right)  \right] &\le  \tau^P.
    \end{align*}

    This proves that $\tau$ is logic-respecting, according to Definition~\ref{def:logic-respecting}.
    
\end{proof}

\subsubsection{Number Needed to Treat is a logic-respecting measure}


\begin{proof}

   First, note that,
\begin{align*}
      \tau_{\text{\tiny NNT}}^P&= \frac{1}{ \mathbb{E}\left[ Y^{(1)} - Y^{(0)} \right]}  \\
      &=  \frac{1}{ \mathbb{E}\left[ \mathbb{E}\left[ Y^{(1)} - Y^{(0)} \mid X \right] \right]} && \text{Law of total expectation}\\ 
      &= \mathbb{E}\left[ \frac{1}{\tau_{\text{\tiny NNT}}^P(X)}   \right]^{-1} . && \text{$\tau_{\text{\tiny NNT}}^P(X):= 1/\mathbb{E}\left[ Y^{(1)} - Y^{(0)} \mid X \right]$}
\end{align*}


By definition, $\min_x \left(\tau_{\text{\tiny NNT}}^P(x) \right) \le \tau_{\text{\tiny NNT}}^P(X)$ almost surely, such that taking the inverse and the expectation leads to

\begin{equation*}
  \mathbb{E}\left[\frac{1}{\tau_{\text{\tiny NNT}}^P(X) }  \right] \le  \mathbb{E}\left[\frac{1}{\min_x \left(\tau_{\text{\tiny NNT}}^P(x) \right)} \right] = \frac{1}{\min_x \left(\tau_{\text{\tiny NNT}}^P(x) \right)}, 
\end{equation*}
%as $\min_x \left(\tau_{\text{\tiny NNT}}(X) \right)$ is a constant. 
which implies
%Taking again the inverse of $ \mathbb{E}\left[\frac{1}{\tau_{\text{\tiny NNT}}(X) }  \right] $ allows to recover the marginal $\tau_{\text{\tiny NNT}}$, so that
\begin{equation*}
  \min_x \left(\tau_{\text{\tiny NNT}}^P(x) \right)\le  \tau_{\text{\tiny NNT}}^P.
\end{equation*}
The exact same reasoning leads to 
\begin{equation*}
  \tau_{\text{\tiny NNT}}^P \le \max_x \left(\tau_{\text{\tiny NNT}}^P(x) \right).
\end{equation*}
Consequently, 
\begin{equation*}
  \min_x \left(\tau_{\text{\tiny NNT}}^P(x) \right)\le  \tau_{\text{\tiny NNT}}^P \le \max_x \left(\tau_{\text{\tiny NNT}}^P(x) \right),
\end{equation*}
which concludes the proof.
\end{proof}


\subsubsection{OR and log-OR are not logic-respecting}
\label{sec:appendixORnoncollapsible}
%\es{je mettrais la preuve ici et ferai référence au Lemme 4 et à ce résultat dans la partie précédente}


Proving that the OR is not logic-respecting can be done with a counter-example as in Table~\ref{tab:odds-ratio-simpson}. 
 Previous works propose to understand non-collapsibility through the non-linearity of a function linking the baseline (control) and response functions. This link function is named the \textit{characteristic collapsibility function} (CCF) and have been proposed by \cite{Neuhaus1993GeometricApproach} and is nicely recalled in \cite{Daniel2020MakingApple} (see their Appendix 1A). This proof relies on Jensen inequality. The proof we recall here is largely inspired from these works, but written within the formalism of our paper.

 
%On the contrary, some measures are not collapsible. In particular for the odds ratio, this can be viewed through the example from Table~\ref{tab:odds-ratio-simpson}. It is possible to understand non-collapsibility through the non-linearity of a function linking the baseline (control) and response functions. This link function is named the \textit{characteristic collapsibility function} (CCF) and have been proposed by \cite{Neuhaus1993GeometricApproach} and is nicely recalled in \cite[Appendix 1A]{Daniel2020MakingApple}. \es{of what ?}\bc{Erwan: j'ai mis ici}

\begin{proof}
    
Assume a generative model such as

\begin{equation}\label{eq:proof-gen-model-non-collapsible}
    \operatorname{logit}\left( \mathbb{P}(Y^{(a)} =1 \mid X, A=a) \right) = b(X) + a\,m,
\end{equation}

where $b(X)$ can be any function of the vector $X$ to $\mathbb{R}$, and where $m$ is a non-null constant. Without loss of generality, one can further assume that $m>0$. Under such model, on has a property on the conditional log-OR or OR, being that:

\begin{equation}\label{eq_proof_local_ORx}
    \tau_{\text{\tiny log-OR}}^P(X) =  \operatorname{log}\left( \frac{\mathbb{P}(Y^{(1)} =1 \mid X)}{1- \mathbb{P}(Y^{(1)} =1 \mid X)} \cdot \left ( \frac{\mathbb{P}(Y^{(0)} =1 \mid X)}{1- \mathbb{P}(Y^{(0)} =1 \mid X)} \right)^{-1} \right) =  b(X) + m - b(X) =  m,
\end{equation}

or similarly that 

\begin{equation*}
     \tau_{\text{\tiny OR}}^P(X) = e^{b(X) + m}\cdot e^{-b(X)} =  e^{m}.
\end{equation*}

In other words, for any $x$ the OR $\tau_{\text{\tiny OR}}^P(x)$ (resp. log-OR) is the same and equal to $e^{m}$ (resp. $m$).\\

Now, we propose to go from this conditional causal measure to the marginal measure. When looking for the marginal OR, one can first estimate $\mathbb{P}(Y^{(1)}=1)$ and $\mathbb{P}(Y^{(0)}=1)$, and then compute the OR. To do so, we propose to rewrite $ \mathbb{P}(Y^{(1)}=1 \mid X)$ as a function of $ \mathbb{P}(Y^{(0)}=1 \mid X)$. From \eqref{eq:proof-gen-model-non-collapsible} one has,

\begin{equation*}
   \operatorname{logit}\left( \mathbb{P}(Y^{(0)}=1 \mid X) \right) =  b(X) ,
\end{equation*}
so that 
\begin{align}\label{eq:proof-non-collapsible}
 \operatorname{logit}\left( \mathbb{P}(Y^{(1)}=1 \mid X) \right)  &=  \operatorname{logit}\left( \mathbb{P}(Y^{(0)}=1 \mid X) \right) + m ,
\end{align}
which is equivalent to
\begin{align}\label{eq:proof-non-collapsible}
  \mathbb{P}(Y^{(1)}=1 \mid X) &=\operatorname{expit}\left(  \operatorname{logit}\left(   \mathbb{P}(Y^{(0)}=1 \mid X) \right) + m \right).
\end{align}

Letting, for all $z \in [0,1]$, 
\begin{align}\label{eq:proof-non-collapsible}
  f(z) &=\operatorname{expit}\left(  \operatorname{logit}\left(   z \right) + m \right),
\end{align}
we have
\begin{align}\label{eq:proof-non-collapsible}
  \mathbb{P}(Y^{(1)}=1 \mid X) &=f \left(  \mathbb{P}(Y^{(0)}=1 \mid X) \right) .
\end{align}
Note that the function $f$ is concave for positive $m$ (it is possible to derive it, but we propose an illustration on Figure~\ref{fig:schema-proof-non-collapsible} to help to be convinced). Then, using Jensen inequality, we obtain,



\begin{align*}
 \mathbb{P}(Y^{(1)}=1) 
 &=   \mathbb{E}\left[   \mathbb{P}(Y^{(1)}=1 \mid X )\right]\\
 &= \mathbb{E}\left[ f \left(  \mathbb{P}(Y^{(0)}=1 \mid X) \right) \right] \\
 %& = \mathbb{E}\left[   f(X) \right]\\
 & < f(\mathbb{E}\left[   \mathbb{P}(Y^{(0)}=1 \mid X) \right]\\
   %&= \mathbb{E}\left[  \operatorname{expit}\left(  \operatorname{logit}\left(   \mathbb{P}(Y^{(0)}=1 \mid X) \right) + m \right) \right]\\
     &= \operatorname{expit}\left(  \operatorname{logit}\left(  \mathbb{E}\left[  \mathbb{P}(Y^{(0)}=1 \mid X) \right]\right) + m \right) && \text{Jensen and $m >0 $} \\
     &= \operatorname{expit}\left(  \operatorname{logit}\left( \mathbb{P}(Y^{(0)}=1 )\right) + m \right),
\end{align*}

and because the $ \operatorname{logit}$ is a monotonous function, then,

\begin{align*}
     \operatorname{logit}\left( \mathbb{P}(Y^{(1)}=1)\right) &<  \operatorname{logit}\left( \mathbb{P}(Y^{(0)}=1 )\right) + m,
\end{align*}


so that 

\begin{align*}
     \operatorname{logit}\left( \mathbb{P}(Y^{(1)}=1)\right) - \operatorname{logit}\left( \mathbb{P}(Y^{(0)}=1 )\right) = \tau_{\text{\tiny log-OR}}^P  &< m,
\end{align*}
where $m=\tau_{\text{\tiny log-OR}}^P(x)$ (see \eqref{eq_proof_local_ORx}).
This allows to conclude that there exist a data generative process for which the odds ratio at the population level can not be written as a positively weighted sum of conditional odds ratio. 
%Would $m$ be strictly negative, then the function would be convex and the inequality would be in the other direction. \es{on peut enlever cette remarque je pense}

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{fig/schema-proof-non-collapsible.png}
    \caption{Implementation of the formulae from \eqref{eq:proof-non-collapsible} for different values of $m$. This illustrates the concavity of the function linking $\mathbb{P}(Y^{(0)}=1 \mid X) $ to $\mathbb{P}(Y^{(1)}=1 \mid X)$ when assuming the generative model of \eqref{eq:proof-gen-model-non-collapsible}. }
    \label{fig:schema-proof-non-collapsible}
\end{figure}


Note that the example provided in Table~\ref{tab:odds-ratio-simpson} is for a negative $m$, showing constant effect on the two substrata and a higher effect on the marginal population.
\end{proof}


\subsection{Proofs related to generalizability}\label{proof:generalizability-section-3}


\subsubsection{Proof of Proposition~\ref{proposition:generalization-density}}



\begin{proof}

   Consider $a\in \{0,1 \}$, then
\begin{align*}
   \mathbb{E}_{\text{\tiny T}}\left[Y^{(a)} \right] &=   \mathbb{E}_{\text{\tiny T}}\left[\mathbb{E}_{\text{\tiny T}}\left[Y^{(a)}\mid X \right]\right] && \text{Total expectation}\\
   &=  \mathbb{E}_{\text{\tiny T}}\left[\mathbb{E}_{\text{\tiny S}}\left[Y^{(a)}\mid X \right]\right] && \text{\modif{Transportability} -- Assumptions~\ref{a:transportability-wide}}\\
   &= \mathbb{E}_{\text{\tiny S}}\left[ \frac{p_{\text{\tiny T}}(X)}{p_{\text{\tiny S}}(X)}  \mathbb{E}_{\text{\tiny S}}\left[Y^{(a)}\mid X \right]\right] && \text{\modif{Overlap} -- Assumptions~\ref{a:overlap}} \\
\end{align*} 


\end{proof}


\modif{The last step can also be written as follow:

\begin{align*}
    \mathbb{E}_{\text{\tiny T}}\left[\mathbb{E}_{\text{\tiny S}}\left[Y^{(a)}\mid X \right]\right]  &= \int \mathbb{E}_{\text{\tiny S}}\left[Y^{(a)}\mid X=x \right] p_{\text{\tiny T}}(x)\,dx && \text{By definition}\\
    &=  \int \mathbb{E}_{\text{\tiny S}}\left[Y^{(a)}\mid X=x \right]p_{\text{\tiny T}}(x) \frac{p_{\text{\tiny S}}(x)}{p_{\text{\tiny S}}(x)}\,dx && \text{Assumption~\ref{a:overlap}: $\frac{p_{\text{\tiny T}}}{p_{\text{\tiny S}}}(x)$ is defined} \\
   &= \int\mathbb{E}_{\text{\tiny S}}\left[Y^{(a)}\mid X =x\right]p_{\text{\tiny S}}(x) \frac{p_{\text{\tiny T}}(x)}{p_{\text{\tiny S}}(x)}\,dx && \text{Re-arrangement} \\
   &=\mathbb{E}_{\text{\tiny S}}\left[ \frac{p_{\text{\tiny T}}(X)}{p_{\text{\tiny S}}(X)}\mathbb{E}_{\text{\tiny S}}\left[Y^{(a)}\mid X \right]\right]
\end{align*}
}
\subsubsection{Proof of Proposition~\ref{prop:generalization-of-local-effects}}

\begin{proof}

If $\tau$ is collapsible, then there exists weights $w(X, P_{\text{\tiny T}}(X,Y^{(0)}))$  such that

\begin{align*}
   \tau^{P_{\text{\tiny T}}}&= \mathbb{E}_{\text{\tiny T}}\left[ w(X, P_{\text{\tiny T}}(X,Y^{(0)})) \tau^{P_{\text{\tiny T}}} (X)\right] && \text{Collapsibility}\\
   &= \mathbb{E}_{\text{\tiny T}}\left[ w(X, P_{\text{\tiny T}}(X,Y^{(0)})) \tau^{P_{\text{\tiny S}}} (X)\right] && \text{Transportability -- Assumption~\ref{a:transportability}}\\
   &= \mathbb{E}_{\text{\tiny S}}\left[ \frac{p_{\text{\tiny T}}(X)}{p_{\text{\tiny S}}(X)} w(X, P_{\text{\tiny T}}(X,Y^{(0)})) \tau^{P_{\text{\tiny S}}} (X)\right] && \text{Overlap -- Assumption~\ref{a:overlap}}.
\end{align*}
\end{proof}




\subsection{Proofs related to non-parametric generative models (Section~\ref{section:generative-models})}


%\textit{As we have not found elsewhere the approach of writing non-parametric models and to relate them to measures of effect,  to the best of our knowledge, all proofs in this subsection are novel.}
Proofs of Corollary~\ref{lemma:working-model-continuous-Y} and \ref{lemma:working-model-continuous-Y-RR} are straightforward and left to the reader. 


\subsubsection{Proof of Lemma~\ref{lem_generative_models}}
\label{app_subsection_proof_Lemma1_genmodels}

By assumption, throughout the paper, $\mathds{E}[Y^{(0)}|X] < \infty$ and $\mathds{E}[Y^{(1)}|X] < \infty$. Thus, one can set 
\begin{align}
    b(X) = \mathds{E}[Y^{(0)}|X], \quad \textrm{and} \quad m(X) = g_{b(X)} \left( \mathds{E}[Y^{(1)}|X]\right).
\end{align}
Since, for all $b \in \mathds{R}$, the function $g_b$ is a bijection on its domain, we have 
\begin{align}
\mathds{E}[Y^{(1)}|X] = g_{b(X)}^{-1}(m(X)). 
\end{align}
With these notations, 
\begin{align}
    \tau^P(X) & = f(\mathds{E}[Y^{(0)}|X], \mathds{E}[Y^{(1)}|X]) \\
    & = g_{b(X)} (\mathds{E}[Y^{(1)}|X]) \\
    & = m(X).
\end{align}

\subsubsection{Proof of Theorem~\ref{th_onlyRD_separates_baseline_tteffect}}
\label{app_subsection_proof_thm1_genmodels}

Recall that the conditional causal measure $\tau$ can be written as 
\begin{align}
    \tau^P (x) = f \left( \mathds{E}[Y^{(0)}|X=x] , \mathds{E}[Y^{(1)}|X=x] \right),
\end{align}
if $\left( \mathds{E}[Y^{(0)}|X=x] , \mathds{E}[Y^{(1)}|X=x] \right) \in D_f$. 
Besides, if $\left( \mathds{E}[Y^{(0)} ] , \mathds{E}[Y^{(1)}] \right) \in D_f$,
\begin{align}
    \tau^P = f \left( \mathds{E}[Y^{(0)} ] , \mathds{E}[Y^{(1)}] \right).
\end{align}
As the causal measure $\tau$ is assumed to be collapsible, there exist non-negative weights $w(X,$ $P(X,Y^{(0)}))$ verifying, for all distribution $(X, Y^{(0)}, Y^{(1)})$,  $\mathds{E}[w(X, P(X,Y^{(0)}))] = 1$ and 
\begin{equation}
\tau^P = \mathbb{E}\left[ w(X, P(X,Y^{(0)})) \tau^P(X)  \right].
\end{equation}
By assumption, for all distributions $P(X)$ of $X$, and for all functions $h: \mathds{X} \to f(D_f)$, there exists $C_{P(X), h} \in \mathds{R}$ such that, for all distributions $Y^{(0)}|X$ satisfying $\forall x \in \mathds{X}, \mathds{E}[Y^{(0)} | X = x] \in D_f^{(1)}$, there exists a distribution $Y^{(1)}|X$ such that 
\begin{itemize}
    \item for all $x \in \mathds{X}, \tau^P(x) = h(x)$
    \item $\tau^P = C_{P(X), h}$.
\end{itemize}
Thus, for all distributions $P(X)$ of $X$, and for all functions $h: \mathds{X} \to f(D_f)$, there exists $C_{P(X), h} \in \mathds{R}$ such that, for all distributions $Y^{(0)}|X$ satisfying $\forall x \in \mathds{X}, \mathds{E}[Y^{(0)} | X = x] \in D_f^{(1)}$, 
\begin{align}
    C_{P(X), h}  = \mathbb{E}\left[ h(X)  w(X, P(X,Y^{(0)})) \right].
\end{align}
Note that the joint distribution $P(X,Y^{(0)})$ can be written as $P(Y^{(0)}|X) P(X)$. Since $h$ can be arbitrary chosen, and since the left-hand term does not depend on the distribution $Y^{(0)}|X$, one must have
\begin{align}
 w(X, P(X,Y^{(0)})) = w(X, P(X)).   
\end{align}
Therefore, 
\begin{equation}
\tau^P = \mathbb{E}\left[ \tau^P(X)  w(X, P(X)) \right], 
\end{equation}
For simplicity, we denote $w(X,P(X))$ by $w(X)$. Thus, for all distributions $(X, Y^{(0)}, Y^{(1)})$, 
\begin{equation}
\tau^P = \mathbb{E}\left[ \tau^P(X)  w(X) \right], 
\end{equation}
which is equivalent to 
\begin{align}
f \left( \mathds{E}[Y^{(0)} ] , \mathds{E}[Y^{(1)}] \right) = \mathds{E} \left[ f \left( \mathds{E}[Y^{(0)}|X] , \mathds{E}[Y^{(1)}|X] \right) w(X)\right]. \label{eq_proof_th_models2}
\end{align}
Now, assume that $\mathds{E}[Y^{(0)} | X] = C$ (constant baseline), and let $h(X) = \mathds{E}[Y^{(1)}|X].$ We have
\begin{align}
f \left( C , \mathds{E}[h(X)] \right) = \mathds{E} \left[ f \left( C , h(X)  \right) w(X)\right].
\end{align}

\paragraph{First case} Assume that $f(C,C) = 0$. Let $B \subset \mathds{X}$ a borelian. Set 
\begin{align}
    h_B(x) = \left\lbrace 
    \begin{array}{cc}
       0  &  \textrm{if } x \in B^c\\
       C  & \textrm{if } x \in B
    \end{array}
    \right.
\end{align}
We have
\begin{align}
    \mathds{E}[h_B(X)] = C \mu_X(B),
\end{align}
and
\begin{align}
    \mathds{E}[f \left( C , h_B(X)  \right) w(X)] = \mathds{E}\left[ f \left( C , 0  \right) \mathds{1}_{X \in B^c} w(X) \right].
\end{align}
Hence, 
\begin{align}
f \left( C , C \mu_X(B) \right) = f(C,0) \mathds{E}\left[ w(X)  \mathds{1}_{X \in B^c} \right].
\end{align}
Since $x \mapsto f(C,x)$ is an injection, $f(C,0) \neq f(C,C) = 0$. Thus, for all borelian $B_1, B_2 \subset \mathds{X}$ such that $\mu_X(B_1) = \mu_X(B_2)$, 
\begin{align}
\frac{1}{\mu_X(B_1)} \mathds{E}\left[ w(X)  \mathds{1}_{X \in B_1} \right] = \frac{1}{\mu_X(B_2)}  \mathds{E}\left[ w(X)  \mathds{1}_{X \in B_2} \right]. \label{eq_proof_th_model1}    
\end{align}
Let $x_1, x_2 \in \mathds{X}$ and $(B_{1,n}), (B_{2,n})$ two sequences of decreasing open balls centered respectively at $x_1$ and $x_2$ such that, for all $n$,  $\mu_X(B_{1,n}) = \mu_X(B_{2,n})$. 
Letting $f$ the density of $X$, we have
\begin{align}
\frac{1}{\mu_X(B_1)} \mathds{E}\left[ w(X)  \mathds{1}_{X \in B_1} \right] 
&=     \frac{\mu(B_{1,n})}{\mu_X(B_{1,n})} \frac{1}{\mu(B_{1,n})} \int_{B_{1,n}} w(x)f(x) dx.
\end{align}
According to the Lebesgue density theorem, we have 
\begin{align}
\frac{\mu_X(B_{1,n})}{\mu(B_{1,n})} &= \frac{1}{\mu(B_{1,n})} \int_{B_{1,n}} f(x) dx \to f(x_1),
\end{align}
and
\begin{align}
\frac{1}{\mu(B_{1,n})} \int_{B_{1,n}} w(x)f(x) dx \to w(x_1) f(x_1).
\end{align}
Thus, 
\begin{align}
\frac{1}{\mu_X(B_{1,n})} \mathds{E}\left[ w(X)  \mathds{1}_{X \in B_{1,n}} \right] \to w(x_1)
\end{align}
and similarly, 
\begin{align}
\frac{1}{\mu_X(B_{2,n})} \mathds{E}\left[ w(X)  \mathds{1}_{X \in B_{2,n}} \right] \to w(x_2), 
\end{align}
which implies, according to equation~\eqref{eq_proof_th_model1}, $w(x_1) = w(x_2)$. Since $\mathds{E}[w(X)] = 1$, we obtain $w(x) = 1$ for all $x \in \mathds{X}$. 

\paragraph{Second case} Assume that $f(C,0) = 0$. For all Borelian $B \subset \mathds{X}$,  
\begin{align}
    h_B(x) = \left\lbrace 
    \begin{array}{cc}
       0  &  \textrm{if } x \in B\\
       1  & \textrm{if } x \in B^c
    \end{array}
    \right.
\end{align}
We have
\begin{align}
    \mathds{E}[h_B(X)] =  \mu_X(B^c),
\end{align}
and
\begin{align}
    \mathds{E}[f \left( C , h_B(X)  \right) w(X)] = \mathds{E}\left[ f \left( C , 1  \right) \mathds{1}_{X \in B^c} w(X) \right].
\end{align}
Hence, 
\begin{align}
f \left( C ,  \mu_X(B^c) \right) = f(C,1) \mathds{E}\left[ w(X)  \mathds{1}_{X \in B^c} \right],
\end{align}
and the same reasoning as above applies. Since for all $x$, $w(x) = 1$, according to Equation~\eqref{eq_proof_th_models2}, we have
\begin{align}
f \left( \mathds{E}[Y^{(0)} ] , \mathds{E}[Y^{(1)}] \right) = \mathds{E} \left[ f \left( \mathds{E}[Y^{(0)}|X] , \mathds{E}[Y^{(1)}|X] \right) \right]. \label{eq_proof_theorem_models3}
\end{align}
Again, assume that $\mathds{E}[Y^{(0)}|X] = C$ and set $h(X) = \mathds{E}[Y^{(1)}|X]$. For any $a,b \in \mathds{R}$, and any $p \in [0,1]$, set 
\begin{align}
    h_{a,b,p}(x) = \left\lbrace 
    \begin{array}{cc}
       a  &  \textrm{with probability } p\\
       b  &  \textrm{with probability } 1-p\\
    \end{array}
    \right.
\end{align}
Hence, 
\begin{align}
f(C, ap + b(1-p)) = f(C,a) p + f(C,b) (1-p).
\end{align}
Thus, the function $x \mapsto f(C,x)$ is convex. By Jensen inequality, \eqref{eq_proof_theorem_models3} holds if and only if $x \mapsto f(C,x)$ is linear or $h(X)$ is degenerate. Since 
\eqref{eq_proof_theorem_models3} must hold for every distribution of $h(X)$, we deduce that, for all $C$, $x \mapsto f(C,x)$ is linear. The same reasoning can be applied by considering $x \mapsto f(x,C)$. Thus,  $x \mapsto f(x,C)$ is also linear for all $C$ and we obtain that there exist $a,b,c,d \in \mathds{R}$ such that
\begin{align}
    f \left (\mathds{E}[Y^{(0)}|X] , \mathds{E}[Y^{(1)}|X] \right) = a \mathds{E}[Y^{(1)}|X] \mathds{E}[Y^{(0)}|X] + b \mathds{E}[Y^{(1)}|X] + c \mathds{E}[Y^{(0)}|X] + d.
\end{align}
Considering $  \mathds{E}[Y^{(0)}|X] = \mathds{E}[Y^{(1)}|X] = h(X)$, we have
\begin{align}
f \left(\mathds{E}[h(X)], \mathds{E}[h(X)] \right) = a (\mathds{E}[h(X)])^2 + (b+c) \mathds{E}[h(X)] + d,
\end{align}
and
\begin{align}
\mathds{E} \left[ f \left( h(X) , h(X) \right) \right] = a \mathds{E}[h(X)^2] + (b+c) \mathds{E}[h(X)] + d,
\end{align}
which leads to, according to Equation~\eqref{eq_proof_theorem_models3}, 
\begin{align}
 a \mathds{V}[h(X)] = 0   
\end{align}
Since this must hold for every distribution of $h(X)$, we deduce that $a=0$. Finally, there exist $a,b,c \in \mathds{R}$ such that 
\begin{align}
    \tau^P (X) & = f( \mathds{E}[Y^{(0)}|X] , \mathds{E}[Y^{(1)} | X] ) \\
    & = a  \mathds{E}[Y^{(1)}|X] + b \mathds{E}[Y^{(0)}|X] + c. 
\end{align}


\subsubsection{Proof of Theorem~\ref{th_onlyRD_separates_baseline_tteffect_bounded_outcome}}
\label{app_subsection_proof_thm2_genmodels}

Recall that, by assumption, there exist $\alpha_1, \alpha_2$ such that for all $x \in \mathds{X}$, $(\alpha_1,\alpha_2) \subset \mathcal{A}(x)$. Recall that the conditional causal measure $\tau$ can be written as 
\begin{align}
    \tau^P (x) = f \left( \mathds{E}[Y^{(0)}|X=x] , \mathds{E}[Y^{(1)}|X=x] \right),
\end{align}
if $\left( \mathds{E}[Y^{(0)}|X=x] , \mathds{E}[Y^{(1)}|X=x] \right) \in D_f$. 
Besides, if $\left( \mathds{E}[Y^{(0)} ] , \mathds{E}[Y^{(1)}] \right) \in D_f$,
\begin{align}
    \tau^P = f \left( \mathds{E}[Y^{(0)} ] , \mathds{E}[Y^{(1)}] \right).
\end{align}
As the causal measure $\tau$ is assumed to be collapsible, there exist non-negative weights $w(X,$ $P(X,Y^{(0)}))$ verifying, for all distribution $(X, Y^{(0)}, Y^{(1)})$,  $\mathds{E}[w(X, P(X,Y^{(0)}))] = 1$ and 
\begin{equation}
\tau^P = \mathbb{E}\left[ \tau^P(X)  w(X, P(X,Y^{(0)})) \right].
\end{equation}
By assumption, for all distributions $P(X)$ of $X$, and for all functions $h: \mathds{X} \to f(D_f)$ such that $h(x) \in f(\mathcal{A}(x)\times \mathcal{A}(x))$, there exists $C_{P(X), h} \in \mathds{R}$ such that, for all distributions $Y^{(0)}|X$ satisfying 
$$\forall x \in \mathds{X}, ~\mathds{E}[Y^{(0)} | X = x] \in \mathcal{A}_{B, h}(x),$$
there exists a distribution $Y^{(1)}|X$ such that $\forall x \in \mathds{X}, \mathds{E}[Y^{(1)} | X = x] \in \mathcal{A}(x) $ and
\begin{itemize}
    \item for all $x \in \mathds{X}, \tau^P(x) = h(x)$
    \item $\tau^P = C_{P(X), h}$.
\end{itemize}
Consequently, since $\tau$ is collapsible, 
\begin{align}
    C_{P(X), h}  = \mathbb{E}\left[ h(X)  w(X, P(X,Y^{(0)})) \right].
\end{align}
Assume that one can find two distributions $P_1$ and $P_2$ of $Y^{(0)}|X$ such that $w(X, P(X), P_1)$ and $w(X, P(X), P_2)$ differ, that is there exists a ball $B \subset \mathds{X}$ such that 
\begin{align}
\mathds{E}[w(X, P(X), P_1) \mathds{1}_{X \in B}] \neq \mathds{E}[w(X, P(X), P_2) \mathds{1}_{X \in B}].
\end{align}
Let $h(x) = (f(\alpha_1,\alpha_2) - f(\alpha_1, \alpha_1)) \mathds{1}_{x \in B} + f(\alpha_1, \alpha_1) \in f(\mathcal{A}(x) \times \mathcal{A}(x))$, we have
\begin{align}
 & \mathbb{E}\left[ h(X)  w(X,  P(X), P_1) \right] = \mathbb{E}\left[ h(X)  w(X,P(X), P_2) \right]\\
 \Leftrightarrow ~~& \mathbb{E}\left[ (f(\alpha_1,\alpha_2) - f(\alpha_1, \alpha_1)) \mathds{1}_{x \in B}  w(X,  P(X), P_1) \right] \\
 & \qquad \qquad \qquad = \mathbb{E}\left[ (f(\alpha_1,\alpha_2) - f(\alpha_1, \alpha_1)) \mathds{1}_{x \in B}  w(X,P(X), P_2) \right]\\
 \Leftrightarrow ~~ & \mathbb{E}\left[  \mathds{1}_{x \in B}  w(X,  P(X), P_1) \right] = \mathbb{E}\left[  \mathds{1}_{x \in B}  w(X,P(X), P_2) \right],
\end{align}
since $\mathds{E}[w(X,P(X), P_1)] = \mathds{E}[w(X,P(X), P_2)] = 1$ and by injectivity of $x \mapsto f(\alpha_1,x)$. Therefore, $w(X, P(X,Y^{(0)}))$ does not depend on the distribution $Y^{(0)}|X$ and one can write, for all distributions $(X, Y^{(0)}, Y^{(1)})$, 
\begin{equation}
\tau^P = \mathbb{E}\left[ \tau^P(X)  w(X) \right], 
\end{equation}
where $w(X) = w(X,P(X))$. Thus,  
\begin{align}
f \left( \mathds{E}[Y^{(0)} ] , \mathds{E}[Y^{(1)}] \right) = \mathds{E} \left[ f \left( \mathds{E}[Y^{(0)}|X] , \mathds{E}[Y^{(1)}|X] \right) w(X)\right]. \label{eq_proof_th_models2}
\end{align}
Now, assume that $\mathds{E}[Y^{(0)} | X] = \alpha_1$ (constant baseline), and let $h(X) = \mathds{E}[Y^{(1)}|X].$ We have
\begin{align}
f \left( \alpha_1 , \mathds{E}[h(X)] \right) = \mathds{E} \left[ f \left( \alpha_1 , h(X)  \right) w(X)\right].
\end{align}
Let $B \subset \mathds{X}$ a borelian. Set 
\begin{align}
    h_B(x) = \left\lbrace 
    \begin{array}{cc}
       \alpha_1  &  \textrm{if } x \in B \\
       \alpha_2  & \textrm{if } x \in B^c
    \end{array}
    \right.
\end{align}
We have
\begin{align}
    \mathds{E}[h_B(X)] & = \alpha_1 \mu_X(B) + \alpha_2 \mu_X(B^c)\\
    & = \alpha_2 + (\alpha_1 - \alpha_2) \mu_X(B),
\end{align}
and
\begin{align}
    & \mathds{E}[f \left( \alpha_1 , h_B(X)  \right) w(X)] \\
     = & \mathds{E}\left[ f \left( \alpha_1 , \alpha_1  \right) \mathds{1}_{X \in B } w(X) \right] 
     + \mathds{E}\left[ f \left( \alpha_1 , \alpha_2  \right) \mathds{1}_{X \in B^c } w(X) \right]\\
     = & f(\alpha_1,\alpha_2) + \left( f(\alpha_1,\alpha_1) - f(\alpha_1,\alpha_2) \right) \mathds{E}[\mathds{1}_{X \in B} w(X)].
\end{align}
Hence, 
\begin{align}
f \left( \alpha_1 , \alpha_2 + (\alpha_1 - \alpha_2) \mu_X(B) \right) = f(\alpha_1,\alpha_2) + \left( f(\alpha_1,\alpha_1) - f(\alpha_1,\alpha_2) \right) \mathds{E}[\mathds{1}_{X \in B} w(X)].
\end{align}
Since $x \mapsto f(\alpha_1,x)$ is an injection, $f(\alpha_1,\alpha_1) \neq f(\alpha_1,\alpha_2)$. Thus, the right-hand side in
\begin{align}
\mathds{E}[\mathds{1}_{X \in B} w(X)] = \frac{f \left( \alpha_1 , \alpha_2 + (\alpha_1 - \alpha_2) \mu_X(B) \right) - f(\alpha_1,\alpha_2)}{f(\alpha_1,\alpha_1) - f(\alpha_1,\alpha_2)}
\end{align}
depends only on $\mu_X(B)$. Hence, for all borelian $B_1, B_2 \subset \mathds{X}$ such that $\mu_X(B_1) = \mu_X(B_2)$, 
\begin{align}
\frac{1}{\mu_X(B_1)} \mathds{E}\left[ w(X)  \mathds{1}_{X \in B_1} \right] = \frac{1}{\mu_X(B_2)}  \mathds{E}\left[ w(X)  \mathds{1}_{X \in B_2} \right]. \label{eq_proof_th_model1}    
\end{align}
Let $x_1, x_2 \in \mathds{X}$ and $(B_{1,n}), (B_{2,n})$ two sequences of decreasing open balls centered respectively at $x_1$ and $x_2$ such that, for all $n$,  $\mu_X(B_{1,n}) = \mu_X(B_{2,n})$. 
Letting $f$ the density of $X$, we have
\begin{align}
\frac{1}{\mu_X(B_1)} \mathds{E}\left[ w(X)  \mathds{1}_{X \in B_1} \right] 
&=     \frac{\mu(B_{1,n})}{\mu_X(B_{1,n})} \frac{1}{\mu(B_{1,n})} \int_{B_{1,n}} w(x)f(x) dx.
\end{align}
According to the Lebesgue density theorem, we have 
\begin{align}
\frac{\mu_X(B_{1,n})}{\mu(B_{1,n})} &= \frac{1}{\mu(B_{1,n})} \int_{B_{1,n}} f(x) dx \to f(x_1),
\end{align}
and
\begin{align}
\frac{1}{\mu(B_{1,n})} \int_{B_{1,n}} w(x)f(x) dx \to w(x_1) f(x_1).
\end{align}
Thus, 
\begin{align}
\frac{1}{\mu_X(B_{1,n})} \mathds{E}\left[ w(X)  \mathds{1}_{X \in B_{1,n}} \right] \to w(x_1)
\end{align}
and similarly, 
\begin{align}
\frac{1}{\mu_X(B_{2,n})} \mathds{E}\left[ w(X)  \mathds{1}_{X \in B_{2,n}} \right] \to w(x_2), 
\end{align}
which implies, according to equation~\eqref{eq_proof_th_model1}, $w(x_1) = w(x_2)$. Since $\mathds{E}[w(X)] = 1$, we obtain $w(x) = 1$ for all $x \in \mathds{X}$. 
Since for all $x$, $w(x) = 1$, according to Equation~\eqref{eq_proof_th_models2}, we have
\begin{align}
f \left( \mathds{E}[Y^{(0)} ] , \mathds{E}[Y^{(1)}] \right) = \mathds{E} \left[ f \left( \mathds{E}[Y^{(0)}|X] , \mathds{E}[Y^{(1)}|X] \right) \right]. \label{eq_proof_theorem_models3}
\end{align}
Let $x \in \mathds{X}$. Let $a,b \in \mathcal{A}(x)$. Set $\mathds{E}[Y^{(0)}|X] = a$ and, for any   $p \in [0,1]$, 
\begin{align}
    \mathds{E}[Y^{(1)}|X=x] = \left\lbrace 
    \begin{array}{cc}
       a  &  \textrm{with probability } p\\
       b  &  \textrm{with probability } 1-p\\
    \end{array}
    \right.
\end{align}
Hence, 
\begin{align}
f(a, ap + b(1-p)) = f(a,a) p + f(a,b) (1-p).
\end{align}
Thus, the function $z' \mapsto f(a,z')$ is convex. By Jensen inequality, \eqref{eq_proof_theorem_models3} holds if and only if $z' \mapsto f(a,z')$ is linear or $h(X)$ is degenerate. Since 
\eqref{eq_proof_theorem_models3} must hold for every distribution of $h(X)$, we deduce that, for all $u \in \mathcal{A}(x)$, $z' \mapsto f(u,z')$ is linear. The same reasoning can be applied by considering $z \mapsto f(z,u)$. Thus,  $z \mapsto f(z,u)$ is also linear for all $u \in \mathcal{A}(x)$ and we obtain that there exist $\beta_1, \beta_2, \beta_3, \beta_4 \in \mathds{R}$ such that
\begin{align}
    f \left (\mathds{E}[Y^{(0)}|X] , \mathds{E}[Y^{(1)}|X] \right) = \beta_1 \mathds{E}[Y^{(1)}|X] \mathds{E}[Y^{(0)}|X] + \beta_2 \mathds{E}[Y^{(1)}|X] + \beta_3 \mathds{E}[Y^{(0)}|X] + \beta_4.
\end{align}
Considering $  \mathds{E}[Y^{(0)}|X] = \mathds{E}[Y^{(1)}|X] = h(X)$, we have
\begin{align}
f \left(\mathds{E}[h(X)], \mathds{E}[h(X)] \right) = \beta_1 (\mathds{E}[h(X)])^2 + (\beta_2+\beta_3) \mathds{E}[h(X)] + \beta_4,
\end{align}
and
\begin{align}
\mathds{E} \left[ f \left( h(X) , h(X) \right) \right] = \beta_1 \mathds{E}[h(X)^2] + (\beta_2+\beta_3) \mathds{E}[h(X)] + \beta_4,
\end{align}
which leads to, according to Equation~\eqref{eq_proof_theorem_models3}, 
\begin{align}
 \beta_1 \mathds{V}[h(X)] = 0   
\end{align}
Since this must hold for every distribution of $h(X)$, we deduce that $\beta_1=0$. Finally, there exist $a,b,c \in \mathds{R}$ such that 
\begin{align}
    \tau^P (X) & = f( \mathds{E}[Y^{(0)}|X] , \mathds{E}[Y^{(1)} | X] ) \\
    & = a  \mathds{E}[Y^{(1)}|X] + b \mathds{E}[Y^{(0)}|X] + c. 
\end{align}




%\subsubsection{Proof of Lemma~\ref{lemma:working-model-continuous-Y} (continuous outcomes)}\label{proof:lemma:working-model-continuous-Y}


%\begin{proof}
%By assumption, we know that $\mathbb{E}\left[\left|Y^{(1)}\right| \mid X\right] < \infty$. Therefore, one can write
%\begin{equation*}
%    Y^{(0)} = f(0,X) + \varepsilon_0, 
%\end{equation*}
%where $ f(0,X) = \mathbb{E}\left[Y^{(0)} \mid X\right]$ and $\mathbb{E}%\left[\varepsilon_0 \mid X\right]=0$ almost surely. In the exact same way, we have 
%\begin{equation*}
%    Y^{(1)} = f(1,X) + \varepsilon_1, 
%\end{equation*}
%where $ f(1,X) = \mathbb{E}\left[Y^{(1)} \mid X\right]$ and $\mathbb{E}\left[\varepsilon_1 \mid X\right]=0$ almost surely. Note that the two previous equations are equivalent  to 
%\begin{equation*}
%    Y^{(A)} = \underbrace{f(0,X)}_{:=b(X)} + A\,\underbrace{\left(  f(1,X) - f(0,X)\right) }_{:=m(X)}  
% +\, \underbrace{A \varepsilon_1 + (1-A) \varepsilon_0}_{:=\varepsilon_A}.
%\end{equation*}
%Note that 
%\begin{align*}
%\mathbb{E}\left[\varepsilon_A \mid X\right] & = \mathbb{E}\left[A \varepsilon_1 + (1-A) \varepsilon_0 \mid X\right] \\
%& =  \mathbb{E}\left[ A \mathbb{E}\left[ \varepsilon_1 \mid A, X\right] \mid X \right] +  \mathbb{E}\left[ (1-A) \mathbb{E}\left[ \varepsilon_0 \mid A, X \right] \mid X \right] \\
%& =  \mathbb{E}\left[ A \mathbb{E}\left[ \varepsilon_1 \mid  X\right] \mid X \right] +  \mathbb{E}\left[ (1-A) \mathbb{E}\left[ \varepsilon_0 \mid X \right] \mid X \right] \\
%& =  \mathbb{E}\left[ A  \mid X \right] \mathbb{E}\left[ \varepsilon_1 \mid  X\right] +  \mathbb{E}\left[ (1-A)  \mid X \right] \mathbb{E}\left[ \varepsilon_0 \mid X \right]\\
%& = 0,
%\end{align*}
%and
%\begin{align*}
%\mathbb{E}\left[\varepsilon_A \mid X, A\right] & = \mathbb{E}\left[A \varepsilon_1 + (1-A) \varepsilon_0 \mid X, A\right] \\
%& =  A \mathbb{E}\left[ \varepsilon_1 \mid A, X\right]  +  (1-A)  \mathbb{E}\left[ \varepsilon_0 \mid A, X \right]  \\
%& = 0.
%\end{align*}
%Consequently, we have 
%\begin{equation*}
%    Y^{(A)} = b(X) + A\,m(X)
% +\, \varepsilon_A, 
%\end{equation*}
%with $\mathbb{E}\left[\varepsilon_A \mid X, A\right] =0$ almost surely and 
%\begin{align*}
%    b(X) &=   \mathbb{E}\left[ Y^{(0)} \mid X \right],\\
 %   m(X) &=  \mathbb{E}\left[ Y^{(1)} \mid X \right] -  \mathbb{E}\left[ Y^{(0)} \mid X \right] =  \mathbb{E}\left[ Y^{(1)} -  Y^{(0)}  \mid X \right].
%\end{align*}
%In this context, the Risk Difference is given by 
%\begin{align*}
%\tau_{\text{\tiny RD}}&= \mathbb{E} [Y^{(1)} - Y^{(0)}   ] && \text{By definition} \\
%&=    \mathbb{E}\left[ b(X) + m(X) +  \varepsilon_1 - b(X) -  \varepsilon_0\right]  \\
%&=  \mathbb{E}\left[ m(X)\right] +  \mathbb{E}\left[ \varepsilon_1\right] -  \mathbb{E}\left[ \varepsilon_0\right] \\
%&=  \mathbb{E}\left[ m(X)\right],
%\end{align*}
%since $ \mathbb{E}\left[ \varepsilon_0\right] = \mathbb{E}\left[ \varepsilon_1\right]0$. The Risk Ratio satisfies 
%\begin{align*}    \tau_{\text{\tiny RR}}&= \frac{\mathbb{E}\left[ Y^{(1)}\right]}{\mathbb{E}\left[ Y^{(0)}\right]}&& \text{By definition}  \\
%    &= 1 + \frac{\mathbb{E}\left[m(X) \right]}{\mathbb{E}\left[b(X) \right]},
%\end{align*}
%whereas the Excess Risk Ratio can be expressed as  
%\begin{align*}
%       \tau_{\text{\tiny ERR}}&= \frac{ \mathbb{E}\left[ m(X)\right] }{ \mathbb{E}\left[ b(X)\right] }.
%\end{align*}


%The assumption stating that $\mathbb{E}\left[\left|Y^{(1)}\right| \mid X\right] < \infty$ and $\mathbb{E}\left[\left|Y^{(0)}\right| \mid X\right] < \infty$ in Lemma~\ref{lemma:working-model-continuous-Y} can also be seen as the assumption of a zero-mean additive-error representations \es{je ne comprends pas}. This implies that one can consider that the potential outcomes are generated according to:\begin{equation}\label{eq_proof_working_model_continuous_Y}
%    Y^{(A)} = f(A,X) + \varepsilon_A, 
%\end{equation}
%for some function $f \in \mathbf{L}^1\left(\{0,1\} \times \mathcal{X} \to \mathbb{R}\right)$ and a noise $\varepsilon_A$ satisfying $\mathbb{E}\left[ \varepsilon_A \mid X \right] = 0$ almost surely.\\


%Thanks to the binary nature of $A$, one has
%\begin{equation*}
%    Y^{(A)} = \underbrace{f(0,X)}_{:=b(X)} + A\,\underbrace{\left(  f(1,X) - f(0,X)\right) }_{:=m(X)}   +\,\varepsilon_A.
%\end{equation*}
%Noting that, taking the expectation around \eqref{eq_proof_working_model_continuous_Y}
%\begin{equation*}
%    Y^{(0)} = f(0,X) + \varepsilon_0, 
%\end{equation*}

%so that 
%\begin{equation*}
%    \mathbb{E}\left[ Y^{(0)} \mid X \right] = f(0,X),
%\end{equation*}
%and 

%\begin{equation*}
%   Y^{(1)} = f(1,X) + \varepsilon_1, 
%\end{equation*}

%so that
%\begin{equation*}
%    \mathbb{E}\left[ Y^{(1)} \mid X \right] = f(1,X),
%\end{equation*}

%it is possible to observe that $b(X)$ and $m(X)$ correspond to

%\begin{align*}
 %   b(X) &=   \mathbb{E}\left[ Y^{(0)} \mid X \right],\\
 %   m(X) &=  \mathbb{E}\left[ Y^{(1)} \mid X \right] -  \mathbb{E}\left[ Y^{(0)} \mid X \right] =  \mathbb{E}\left[ Y^{(1)} -  Y^{(0)}  \mid X \right].
%\end{align*}
%\end{proof}



%\subsubsection{Proof of \Cref{lemma:working-model-continuous-Y-RR}}






\subsubsection{Proof of Lemma~\ref{lemma:intrication_model} (binary outcomes)}\label{proof:intrication_model}


\begin{proof}
Consider a binary outcome $Y$. We further assume that,
 \begin{equation*}
     \forall x \in \mathds{X},\, \forall a \in \{0,1\},\quad 0 < p_a(x) < 1,\quad \text{where } p_a(x)  := \mathbb{P}\left[Y^{(a)} = 1 \mid X=x\right],
 \end{equation*}
which means that the outcome is non-deterministic. Using the law of total expectation, one has


\begin{align*}
 p_1(x) &= \mathbb{P}\left[Y^{(1)} = 1 \mid X=x\right]\\
  & = \mathbb{P}\left[ Y^{(1)} = 1 \mid Y^{(0)} = 0, X = x\right] \mathbb{P}\left[ Y^{(0)} = 0\mid X = x\right] \\
  & \quad \quad + \mathbb{P}\left[ Y^{(1)} = 1 \mid Y^{(0)} = 1, X = x\right] \mathbb{P}\left[ Y^{(0)} = 1\mid X = x\right] \\
  &= \mathbb{P}\left[ Y^{(1)} = 1 \mid Y^{(0)} = 0, X = x\right] (1- p_0(x) )+ \mathbb{P}\left[ Y^{(1)} = 1 \mid Y^{(0)} = 1, X = x\right]  p_0(x) .
\end{align*}

Denoting
\begin{equation*}
    m_g(x):= \mathbb{P}\left[ Y^{(1)} = 0 \mid Y^{(0)} = 1, X = x\right] \quad \text{and} \quad m_b(x):= \mathbb{P}\left[ Y^{(1)} = 1 \mid Y^{(0)} = 0, X = x\right],
\end{equation*}
we finally obtain
\begin{align*}
 p_1(x) &=m_b(x) (1- p_0(x) ) + (1-  m_g(x))  p_0(x) \\
 &= p_0(x) +  m_b(x) (1- p_0(x) )  - p_0(x)m_g(x).
\end{align*}
Therefore, for all $a\in \{0,1\}$, 
\begin{align*}
 p_a(x)  &= p_0(x) +  a \left( m_b(x) (1- p_0(x) )  - p_0(x)m_g(x) \right).
\end{align*}
\end{proof}



\modif{Note that the rational of the proof can be captured with a probability tree. Below on Figure~\ref{fig:tree} illustrates the problem with the Russian roulette example.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/tree.png}
    \caption{Illustration of the Russian Roulette problem with a probability tree}
    \label{fig:tree}
\end{figure}}

\subsubsection{Proof of Lemma~\ref{lemma:expression-of-causal-quantities-under-generative-model-binary-outcome-intrication-model}}\label{proof:expression-of-causal-quantities-under-generative-model-binary-outcome-intrication-model}

\begin{lemma}[Expression of the causal measures]\label{lemma:expression-of-causal-quantities-under-generative-model-binary-outcome-intrication-model}
Ensuring conditions of Lemma~\ref{lemma:intrication_model} leads to,
\begin{align*}
  \tau_{\text{\tiny RD}}^P & = \mathbb{E}\left[ \left( 1-b\left(X\right) \right) m_b\left(X\right)\right] -  \mathbb{E}\left[ b\left(X\right)m_g\left(X\right) \right]\\
    \tau_{\text{\tiny NNT}}^P & =  \frac{1}{\mathbb{E}\left[ \left( 1-b\left(X\right) \right) m_b\left(X\right)\right] -  \mathbb{E}\left[ b\left(X\right)m_g\left(X\right) \right] }
\end{align*}
 \begin{align*}
     \tau_{\text{\tiny RR}}^P & = 1  + \frac{ \mathbb{E}\left[  \left( 1-b\left(X\right) \right) m_b\left(X\right) \right] }{ \mathbb{E}\left[ b(X)\right]} - \frac{\mathbb{E}\left[ b(X) m_g\left(X\right)\right]}{\mathbb{E}\left[ b(X)\right]}\\
         \tau_{\text{\tiny SR}}^P & = 1 - \frac{\mathbb{E}\left[ \left( 1-b\left(X\right) \right) m_b\left(X\right) \right]}{\mathbb{E}\left[ 1- b(X)\right]} + \frac{\mathbb{E}\left[ b\left(X\right)m_g\left(X\right) \right]}{\mathbb{E}\left[ 1- b(X)\right]},
 \end{align*}

 \begin{equation*}
     \tau_{\text{\tiny OR}}^P =\frac{\mathbb{E}\left[ b(X) \right]+  \mathbb{E}\left[\left( \left( 1-b\left(X\right) \right) m_b\left(X\right) \right]-  \mathbb{E}\left[b\left(X\right)m_g\left(X\right) \right)\right] }{\mathbb{E}\left[ 1- b(X) \right] - \mathbb{E}\left[ \left( 1-b\left(X\right) \right) m_b\left(X\right)  \right]+  \mathbb{E}\left[b\left(X\right)m_g\left(X\right) \right] }  \frac{\mathbb{E}\left[1-b(X)\right]}{ \mathbb{E}\left[b(X)\right]} .
 \end{equation*}

\end{lemma}%\bc{C'est moche non ?}


\begin{proof}


Consider a binary outcome $Y$. Under the assumptions of  Lemma~\ref{lemma:intrication_model}, there exist probabilities $b(x)$, $ m_g(x)$, and $m_b(x)$ such that 
\begin{align*}
     \mathbb{P}\left[ Y^{(a)} = 1 \mid X = x\right]  &= b(x) +  a\, \left( \left( 1-b\left(x\right) \right) m_b\left(x\right) -  b\left(x\right)m_g\left(x\right) \right).
\end{align*}

Using such a decomposition, one has

    \begin{align*}
        \tau_{\text{\tiny RD}}^P &=  \mathbb{E}\left[ b(X)+  \left( \left( 1-b\left(X\right) \right) m_b\left(X\right) -  b\left(x\right)m_g\left(X\right) \right)\right] -  \mathbb{E}\left[ b(X)\right]\\
        &=  \mathbb{E}\left[ \left( 1-b\left(X\right) \right) m_b\left(X\right)\right] -  \mathbb{E}\left[ b\left(X\right)m_g\left(X\right) \right], \\
        \tau_{\text{\tiny NNT}}^P &=  \frac{1}{\mathbb{E}\left[ \left( 1-b\left(X\right) \right) m_b\left(X\right)\right] -  \mathbb{E}\left[ b\left(X\right)m_g\left(X\right) \right] },\\
         \tau_{\text{\tiny RR}}^P &= \frac{ \mathbb{E}\left[ b(X)+  \left( \left( 1-b\left(X\right) \right) m_b\left(X\right) -  b\left(X\right)m_g\left(X\right) \right)\right] }{ \mathbb{E}\left[ b(X)\right]} \\
         &= 1  + \frac{ \mathbb{E}\left[  \left( 1-b\left(X\right) \right) m_b\left(X\right) \right] }{ \mathbb{E}\left[ b(X)\right]} - \frac{\mathbb{E}\left[ b(X) m_g\left(X\right)\right]}{\mathbb{E}\left[ b(X)\right]}, \\
         \tau_{\text{\tiny SR}}^P &= \frac{ 1- \mathbb{E}\left[ b(X)+  \left( \left( 1-b\left(X\right) \right) m_b\left(X\right) -  b\left(X\right)m_g\left(X\right) \right)\right] }{ 1- \mathbb{E}\left[ b(X)\right]} \\
         &= \frac{\mathbb{E}\left[ 1 - b(X) -  \left( \left( 1-b\left(X\right) \right) m_b\left(X\right) +  b\left(X\right)m_g\left(X\right) \right)\right] }{ \mathbb{E}\left[ 1- b(X)\right]} \\
         &= 1 - \frac{\mathbb{E}\left[ \left( 1-b\left(X\right) \right) m_b\left(X\right) \right]}{\mathbb{E}\left[ 1- b(X)\right]} + \frac{\mathbb{E}\left[ b\left(X\right)m_g\left(X\right) \right]}{\mathbb{E}\left[ 1- b(X)\right]}, \\
    \tau_{\text{\tiny OR}}^P  &= \frac{\mathbb{P}[Y^{(1)} = 1]}{\mathbb{P}[Y^{(1)} = 0]}\, \left(  \frac{\mathbb{P}[Y^{(0)} = 1]}{\mathbb{P}[Y^{(0)} = 0]}\right)^{-1} \\
    &= \frac{\mathbb{E}\left[ b(X)+  \left( \left( 1-b\left(X\right) \right) m_b\left(X\right) -  b\left(X\right)m_g\left(X\right) \right)\right] }{1- \mathbb{E}\left[ b(X)+  \left( \left( 1-b\left(X\right) \right) m_b\left(X\right) -  b\left(X\right)m_g\left(X\right) \right)\right] }  \left(  \frac{\mathbb{E}\left[b(X)\right]}{1- \mathbb{E}\left[b(X)\right]}\right)^{-1} \\
    &=     \frac{\mathbb{E}\left[ b(X)+  \left( \left( 1-b\left(X\right) \right) m_b\left(X\right) -  b\left(X\right)m_g\left(X\right) \right)\right] }{\mathbb{E}\left[ 1- b(X) -  \left( \left( 1-b\left(X\right) \right) m_b\left(X\right) +  b\left(X\right)m_g\left(X\right) \right)\right] }  \frac{\mathbb{E}\left[1-b(X)\right]}{ \mathbb{E}\left[b(X)\right]} \\
    &=     \frac{\mathbb{E}\left[ b(X) \right]+  \mathbb{E}\left[\left( \left( 1-b\left(X\right) \right) m_b\left(X\right) \right]-  \mathbb{E}\left[b\left(X\right)m_g\left(X\right) \right)\right] }{\mathbb{E}\left[ 1- b(X) \right] - \mathbb{E}\left[ \left( 1-b\left(X\right) \right) m_b\left(X\right)  \right]+  \mathbb{E}\left[b\left(X\right)m_g\left(X\right) \right] }  \frac{\mathbb{E}\left[1-b(X)\right]}{ \mathbb{E}\left[b(X)\right]} \\
    &= \left( 1  + \frac{ \mathbb{E}\left[  \left( 1-b\left(X\right) \right) m_b\left(X\right) \right] }{ \mathbb{E}\left[ b(X)\right]} - \frac{\mathbb{E}\left[ b(X) m_g\left(X\right)\right]}{\mathbb{E}\left[ b(X)\right]} \right) \\ 
    & \quad \quad \cdot \left( 1 - \frac{\mathbb{E}\left[ \left( 1-b\left(X\right) \right) m_b\left(X\right) \right]}{\mathbb{E}\left[ 1- b(X)\right]} + \frac{\mathbb{E}\left[ b\left(X\right)m_g\left(X\right) \right]}{\mathbb{E}\left[ 1- b(X)\right]}\right)^{-1}.
\end{align*}
\end{proof}




\subsection{Proofs of Section~\ref{sec:generalization}}


\subsubsection{Proof of Theorem~\ref{theorem:all-covariates}}
\label{subsubsec:proof_generalization_conditional_outcomes}

\begin{proof}
Let $\tau$ be a causal measure defined as 
\begin{align}
    \tau^P = f (\mathds{E}[Y^{(0)}], \mathds{E}[Y^{(1)}] ).
\end{align}
Let $P_{\text{\tiny S}}(X, Y^{(0)}, Y^{(1)})$ and $P_{\text{\tiny T}}(X, Y^{(0)}, Y^{(1)})$ satisfying Assumption~\ref{a:overlap} (overlap assumption) and Assumption~\ref{a:transportability-wide}. By Lemma~\ref{lem_generative_models}, on the source population, for all $x \in \operatorname{supp}(P_{\text{\tiny S}})$, we have
\begin{align}
    \mathds{E}_{\text{\tiny S}}[Y^{(0)} | X =x ] = b(x) \quad \textrm{and} \quad \mathds{E}_{\text{\tiny S}}[Y^{(1)} | X =x ] = g_{b(x)}^{-1} (m(x)), 
\end{align}
where $g_z : z' \mapsto f(z,z')$. According to Assumption~\ref{a:transportability-wide}, for all $x \in \operatorname{supp}(P_{\text{\tiny T}}) \, \cap \, \operatorname{supp}(P_{\text{\tiny S}}) = \operatorname{supp}(P_{\text{\tiny T}})$  (by Assumption~\ref{a:overlap}),
\begin{align}
    &  \mathds{E}_{\text{\tiny T}}[Y^{(0)} | X = x] = \mathds{E}_{\text{\tiny S}}[Y^{(0)} | X = x ] \\
    \textrm{and} ~~ &  \mathds{E}_{\text{\tiny T}}[Y^{(1)} | X = x] = \mathds{E}_{\text{\tiny S}}[Y^{(1)} | X = x ].
\end{align}
Thus, 
\begin{align}
    \mathds{E}_{\text{\tiny T}}[Y^{(0)} | X ] = b(X) \quad \textrm{and} \quad \mathds{E}_{\text{\tiny T}}[Y^{(1)} | X ] = g_{b(X)}^{-1} (m(X)). 
\end{align}
We are interested in estimating the average treatment effect on the target population, that is 
\begin{align}
    \tau^{P_\text{\tiny T}} = f (\mathds{E}_{\text{\tiny T}}[Y^{(0)}], \mathds{E}_{\text{\tiny T}}[Y^{(1)}] ).
\end{align}
According to Definitions~\ref{def:two-kind-covariates} and \ref{def:shidted-covariates}, we have 
\begin{align}
     \mathbb{E}_{\text{\tiny T}}\left[ Y^{(0)} \right] & = \mathbb{E}_{\text{\tiny T}}\left[  b(X) \right] \\
     &=  \mathbb{E}_{\text{\tiny T}}\left[   \mathbb{E}_{\text{\tiny T}}\left[  b(X) \mid X_{\textrm{Sh}} \right] \right] \\
     & = \mathbb{E}_{\text{\tiny T}}\left[   \mathbb{E}_{\text{\tiny S}}\left[  b(X) \mid X_{\textrm{Sh}} \right] \right] \\
     & = \mathbb{E}_{\text{\tiny T}}\left[   \mathbb{E}_{\text{\tiny S}}\left[  b(X) \mid X_{B \cap \textrm{Sh}} \right] \right], \label{proof_th1_eq1}
\end{align}
where the third line comes from Assumption~\ref{a:overlap} and the definition of $X_{Sh}$. Similarly, 
\begin{align}
     \mathbb{E}_{\text{\tiny T}}\left[  Y^{(1)} \right] &= \mathbb{E}_{\text{\tiny T}}\left[ g_{b(X)}^{-1} (m(X)) \right] \\
     &=  \mathbb{E}_{\text{\tiny T}}\left[   \mathbb{E}^{\text{\tiny T}}\left[  g_{b(X)}^{-1} (m(X)) \mid X_{\textrm{Sh}} \right] \right] \\
     & = \mathbb{E}_{\text{\tiny T}}\left[   \mathbb{E}_{\text{\tiny S}}\left[  g_{b(X)}^{-1} (m(X)) \mid X_{\textrm{Sh}} \right] \right] \\
     & = \mathbb{E}_{\text{\tiny T}}\left[   \mathbb{E}_{\text{\tiny S}}\left[  g_{b(X)}^{-1} (m(X)) \mid X_{(M \cup B) \cap \textrm{Sh}} \right] \right]. \label{proof_th1_eq2}
\end{align}
Consequently, one can generalize $\tau$ to the target population by using the formula
\begin{align}
    \tau^{P_\text{\tiny T}} = f \left(\mathbb{E}_{\text{\tiny T}}\left[   \mathbb{E}_{\text{\tiny S}}\left[  b(X) \mid X_{B \cap \textrm{Sh}} \right] \right], \mathbb{E}_{\text{\tiny T}}\left[   \mathbb{E}_{\text{\tiny S}}\left[  g_{b(X)}^{-1} (m(X)) \mid X_{(M \cup B) \cap \textrm{Sh}} \right] \right] \right).
\end{align}

%\textbf{Continuous outcome}. Consider a continuous outcome $Y$. Under assumptions of Lemma~\ref{lemma:working-model-continuous-Y}, 
% \begin{align*}
% Y^{(a)} = b(X) + a\,m(X) + \varepsilon_a,
% \end{align*}
% where $ b(X) :=\mathbb{E}[Y^{(0)} \mid X]$, $ m(X) :=\mathbb{E}[Y^{(1)}-Y^{(0)} \mid X]$ and $\varepsilon_A$ satisfies $\mathbb{E}\left[ \varepsilon_A \mid X \right] = 0$.\\



%Further assume Assumptions~\ref{a:transportability-wide} and \ref{a:overlap}. 

%Let $a \in \{0, 1\}$, %and denote $Z$ any set of baseline covariates,
%\begin{align*}
%    \mathbb{E}_{\text{\tiny T}}\left[Y^{(a)} \right] &=   \mathbb{E}_{\text{\tiny T}}\left[  Y^{(a)} \right]\\
%     &=     \mathbb{E}_{\text{\tiny T}}\left[  b(X) + a\,m(X) + \varepsilon_a \right]&& \text{Lemma~\ref{lemma:working-model-continuous-Y}} \\
%    &=    \mathbb{E}_{\text{\tiny T}}\left[  b(X)  \right]+ a\,   \mathbb{E}_{\text{\tiny T}}\left[  m(X) \right].
%\end{align*}
%Generalizing conditional outcomes to the target population means that we can compute  $\mathbb{E}_{\text{\tiny T}} [Y^{(a)} ]$ for $a \in \{0,1\}$, which is equivalent to computing 
%\begin{align}
%     \mathbb{E}_{\text{\tiny T}}\left[  b(X) \right],  \quad \text{and }\quad \mathbb{E}_{\text{\tiny T}}\left[  m(X) \right],
%\end{align}
%that is generalizing $b(X)$ and $m(X)$. 
%Having access to $X_{(M \cup B) \cap \textrm{Sh}}$ that is all shifted covariates that are treatment effect modifiers or related to the baseline risk is then sufficient to generalize $b$ and $m$ and thus to generalize $ \mathbb{E}_{\text{\tiny T}}\left[Y^{(a)} \right]$ for all $a\in \{0,1\}$.


%The covariate set $Z$ satisfying 
%\begin{equation*}
%     \mathbb{E}^{\text{\tiny T}}\left[  b(X) \mid Z\right] =     \mathbb{E}^{\text{\tiny S}}\left[  b(X) \mid Z \right],  \quad \text{and }\quad \mathbb{E}^{\text{\tiny T}}\left[  m(X) \mid Z\right] =     \mathbb{E}^{\text{\tiny S}}\left[  m(X) \mid Z \right],
%\end{equation*}
%is thus the minimal set required to generalize the conditional outcomes. 

%\bigskip 
%According to Definitions~\ref{def:two-kind-covariates} and \ref{def:shidted-covariates}, we introduce the sets of indices $M, B, \textrm{Sh} \subset \{1, \hdots, d \}$ such that $X_B$ and $X_M$ are the minimal sets of variables satisfying    
%\begin{align*}
%\mathbb{E}\left[b\left(X\right) | X = x \right] = \mathbb{E}\left[b\left(X\right) | X_B = x_B\right] \quad \textrm{and} \quad  \mathbb{E}\left[m\left(X\right) | X = x\right] =  \mathbb{E}\left[m(X) | X_M = x_M\right].
%\end{align*}
%ensuring that any causal quantity can be generalized by generalizing conditional outcomes with $Z$ as the conditional set.
%But any smaller set would prevent to generalize either $b(X)$ and / or $m(X)$:

%\begin{equation*}
%     \mathbb{E}^{\text{\tiny T}}\left[  b(X) \mid Z\right] \neq     \mathbb{E}^{\text{\tiny S}}\left[  b(X) \mid Z \right],  \quad \text{and / or } \mathbb{E}^{\text{\tiny T}}\left[  m(X) \mid Z\right] \neq     \mathbb{E}^{\text{\tiny S}}\left[  m(X) \mid Z \right],
%\end{equation*}
%and therefore prevents the generalization of $\mathbb{E}^{\text{\tiny T}}\left[  Y^{(a)} \mid Z \right]$.\\

%\bigskip 

%\textbf{Binary outcome.} Consider a binary outcome $Y \in \{0,1\}$, and let
%\begin{equation*}
%    m_g(x):= \mathbb{P}\left[ Y^{(1)} = 0 \mid Y^{(0)} = 1, X = x\right] \quad \text{and} \quad m_b(x):= \mathbb{P}\left[ Y^{(1)} = 1 \mid Y^{(0)} = 0, X = x\right].
%\end{equation*}
%Under Assumptions of Lemma~\ref{lemma:intrication_model}, we have, for all $a \in \{0,1\},$
%\begin{align*}
%     \mathbb{P}\left[ Y^{(a)} = 1 \mid X = x\right]  &= b(x)+  a\, m(x), 
%\end{align*}
%with $b(x)=p_0(x) $ and 
%\begin{align*}
%   m(x) =    \left( 1-b\left(x\right) \right) m_b\left(x\right) -  b\left(x\right)m_g\left(x\right).
%\end{align*}
%For all  $a \in \{0, 1\}$, we have, as above, 
%\begin{align*}
%    \mathbb{E}_{\text{\tiny T}}\left[Y^{(a)} \right] &= \mathbb{E}_{\text{\tiny T}}\left[  \mathbb{P}\left[ Y^{(a)} = 1 \mid X \right]  \right]\\
 %    &=  \mathbb{E}_{\text{\tiny T}}\left[    b(X)+  a\, m(X), \right] \\
%     &=  \mathbb{E}_{\text{\tiny T}}\left[    b(X) \right] +  a\,\mathbb{E}_{\text{\tiny T}}\left[ m(X), \right]. 
    %&=  \mathbb{E}^{\text{\tiny T}}\left[  \mathbb{E}^{\text{\tiny T}}\left[  b(X) \mid Z\right] \right]+ a\, \mathbb{E}^{\text{\tiny T}}\left[  \mathbb{E}^{\text{\tiny T}}\left[ \left( 1-b\left(X\right) \right) m_b\left(X\right) \mid  Z\right]\right] - a\, \mathbb{E}^{\text{\tiny T}}\left[  \mathbb{E}^{\text{\tiny T}}\left[ b\left(X\right)m_g\left(X\right) \mid Z \right] \right].
%\end{align*}
%Having access to $X_{(M \cup B) \cap \textrm{Sh}}$ that is all shifted covariates that are treatment effect modifiers or related to the baseline risk is then sufficient to generalize $b$ and $m$ and thus to generalize $ \mathbb{E}_{\text{\tiny T}}\left[Y^{(a)} \right]$ for all $a\in \{0,1\}$.

%As soon as the set $Z = (X_B \cup X_M ) \cap \textrm{Shift}$, then one has,
%\begin{equation}\label{eq_proof_1}
%     \mathbb{E}^{\text{\tiny T}}\left[  b(X) \mid Z\right] =     \mathbb{E}^{\text{\tiny S}}\left[  b(X) \mid Z \right],
%\end{equation}
%and
%\begin{equation}\label{eq_proof_2}
%   \quad \mathbb{E}^{\text{\tiny T}}\left[  \left( 1-b\left(X\right) \right) m_b\left(X\right) \mid Z\right] =     \mathbb{E}^{\text{\tiny S}}\left[ \left( 1-b\left(X\right) \right) m_b\left(X\right) \mid Z \right],
%\end{equation}
%along with
%\begin{equation}\label{eq_proof_3}
%   \quad \mathbb{E}^{\text{\tiny T}}\left[ b\left(X\right)m_g\left(X\right) \mid Z\right] =     \mathbb{E}^{\text{\tiny S}}\left[b\left(X\right)m_g\left(X\right) \mid Z \right].
%\end{equation}

%This ensures to have,

%\begin{align*}
%    \mathbb{E}^{\text{\tiny T}}\left[Y^{(a)} \right]
%    &=  \mathbb{E}^{\text{\tiny T}}\left[  \mathbb{E}^{\text{\tiny S}}\left[  b(X) \mid Z\right] \right]+ a\, \mathbb{E}^{\text{\tiny T}}\left[  \mathbb{E}^{\text{\tiny S}}\left[ \left( 1-b\left(X\right) \right) m_b\left(X\right) \mid  Z\right]\right] - a\, \mathbb{E}^{\text{\tiny T}}\left[  \mathbb{E}^{\text{\tiny S}}\left[ b\left(X\right)m_g\left(X\right) \mid Z \right] \right]\\
%    &= \mathbb{E}^{\text{\tiny T}}\left[   \mathbb{E}^{\text{\tiny S}}\left[ b(X)+  a\, \left( \left( 1-b\left(X\right) \right) m_b\left(X\right) -  b\left(X\right)m_g\left(X\right) \right)  \mid Z \right] \right]\\
%    &= \mathbb{E}^{\text{\tiny T}}\left[   \mathbb{E}^{\text{\tiny S}}\left[  Y^{(a)} \mid Z \right] \right].
%\end{align*}


%Any smaller set than $Z$ would prevent at least \eqref{eq_proof_1}, \eqref{eq_proof_2}, or \eqref{eq_proof_3} to hold, and therefore would not enable the transport formula to hold without further assumptions.
\end{proof}


\subsubsection{Proof of Theorem~\ref{theorem:restricted-set-for-Y-continuous-RD}}
\label{sec:proof_theorem_generalizing_local_effects}


\begin{proof}
Consider the Risk Difference $\tau_{\text{\tiny RD}}$. Let   $P_{\text{\tiny S}}(X, Y^{(0)}, Y^{(1)})$ and $P_{\text{\tiny T}}(X, Y^{(0)}, Y^{(1)})$ satisfying Assumption~\ref{a:overlap} (overlap assumption) and Assumption~\ref{a:transportability}. 
%
Since $\tau_{\text{\tiny RD}}$ satisfies Assumption~\ref{ass:injection_def_domain}, Corollary~\ref{lemma:working-model-continuous-Y} can be applied on the source population, that is, for all $x \in \operatorname{supp}(P_{\text{\tiny S}})$, we have
\begin{align}
    \mathds{E}_{\text{\tiny S}}[Y^{(0)} | X =x ] = b(x) \quad \textrm{and} \quad \mathds{E}_{\text{\tiny S}}[Y^{(1)} | X =x ] = b(x) + m(x). 
\end{align}
Thus, for all $x \in \operatorname{supp}(P_{\text{\tiny S}})$,
\begin{align}
m(x) = \mathds{E}_{\text{\tiny S}}[Y^{(1)} - Y^{(0)} | X =x ].
\end{align}
According to Assumption~\ref{a:transportability}, for all $ x \in \operatorname{supp}(P_{\text{\tiny T}}) \cap \operatorname{supp}(P_{\text{\tiny S}}) = \operatorname{supp}(P_{\text{\tiny T}})$ (Assumption~\ref{a:overlap}),
\begin{align}
m(x) = \mathds{E}_{\text{\tiny T}}[Y^{(1)} - Y^{(0)} | X =x ].
\end{align}

%We start by proving that Assumption~\ref{a:transportability} is satisfied for $\tau_{\text{\tiny RD}}(X_{M \cap \textrm{Sh}})$. 
Since $\tau_{\text{\tiny RD}}$ is directly collapsible, we have
\begin{align}
\tau_{\text{\tiny RD}}^{P_{\text{\tiny T}}} & = \mathds{E}_{\text{\tiny T}}[m(X)] \\
& = \mathds{E}_{\text{\tiny T}} \left[ \mathds{E}_{\text{\tiny T}}\left[    m(X) \mid X_{M \cap \textrm{Sh}} \right] \right],
\end{align}
where
\begin{align*}
%\tau^{\text{\tiny T}}_{\text{\tiny RD}}(X_{M \cap \textrm{Sh}}) & = \mathbb{E}_{\text{\tiny T}}\left[ Y^{(1)} - Y^{(0)} \mid X_{M \cap \textrm{Sh}} \right] && \\
%& = 
\mathbb{E}_{\text{\tiny T}}\left[    m(X) \mid X_{M \cap \textrm{Sh}} \right]  %\text{Lemma~\ref{lemma:working-model-continuous-Y}} \\
& = \mathbb{E}_{\text{\tiny T}}\left[    m(X) \mid X_{\textrm{Sh}} \right] && \text{Definition~\ref{def:two-kind-covariates}} \\
& = \mathbb{E}_{\text{\tiny S}}\left[    m(X) \mid X_{\textrm{Sh}} \right] && \text{Definition~\ref{def:shidted-covariates}} \\
& = \mathbb{E}_{\text{\tiny S}}\left[    m(X) \mid X_{M \cap \textrm{Sh}} \right] && \text{Definition~\ref{def:two-kind-covariates}}\\
& = \tau^{P_\text{\tiny S}}_{\text{\tiny RD}}(X_{M \cap \textrm{Sh}}) .
 \end{align*}
 Consequently, 
\begin{align}
\tau_{\text{\tiny RD}}^{P_{\text{\tiny T}}}  
& = \mathds{E}_{\text{\tiny T}} \left[  \tau^{P_\text{\tiny S}}_{\text{\tiny RD}}(X_{M \cap \textrm{Sh}}) \right],
\end{align}
%Thus, Assumption~\ref{a:transportability} is verified for $\tau_{\text{\tiny RD}}$ with covariates $X_{M \cap \textrm{Sh}}$. Furthermore, by assumption in Theorem~\ref{theorem:restricted-set-for-Y-continuous-RD}, Assumption~\ref{a:overlap} is verified with  covariates $X_{M \cap \textrm{Sh}}$. Thus, by Proposition~\ref{prop:generalization-of-local-effects}, 
and $\tau_{\text{\tiny RD}}$ is generalizable with covariates $X_{M \cap \textrm{Sh}}$.

%\bigskip 

%Below, we give insights explaining why the Risk Ratio does not satisfy a similar property. Indeed, in the same manner as above, the Risk Ratio satisfies

 
 %\begin{align*}
%\tau^{\text{\tiny S}}_{\text{\tiny RR}}(X_{M \cap \textrm{Sh}})  &= \frac{\mathbb{E}_{\text{\tiny S}}\left[ Y^{(1)} \mid X_{M \cap \textrm{Sh}} \right]}{\mathbb{E}_{\text{\tiny S}}\left[ Y^{(0)} \mid X_{M \cap \textrm{Sh}}\right]} \\
%&= \frac{\mathbb{E}_{\text{\tiny S}}\left[ b(X) + m(X) \mid X_{M \cap \textrm{Sh}} \right]}{\mathbb{E}_{\text{\tiny S}}\left[  b(X) \mid X_{M \cap \textrm{Sh}} \right]} && \text{Lemma~\ref{lemma:working-model-continuous-Y}}\\
%&= \frac{\mathbb{E}^{\text{\tiny S}}\left[ \mathbb{E}^{\text{\tiny S}}\left[b(X) + m(X) \mid X \right] \mid X_M \cap \textrm{Shift} \right]}{\mathbb{E}^{\text{\tiny S}}\left[  \mathbb{E}^{\text{\tiny S}}\left[b(X) \mid X \right] \mid X_M \cap \textrm{Shift} \right]} && \\
%&= 1 + \frac{ \mathbb{E}_{\text{\tiny T}}\left[ m(X) \mid X_{M \cap \textrm{Sh}} \right]}{\mathbb{E}_{\text{\tiny S}}\left[b(X) \mid X_{M \cap \textrm{Sh}} \right]}. && \text{ Definitions~\ref{def:shidted-covariates}} \\
%&= 1 + \frac{ \mathbb{E}^{\text{\tiny T}}\left[ m(X) \mid X_{M \cap \textrm{Sh}} \right]}{\mathbb{E}^{\text{\tiny S}}\left[b(X) \mid X_{M \cap \textrm{Sh}} \right]},
% \end{align*}
%For Assumption~\ref{a:overlap} to hold, we need $\tau^{\text{\tiny S}}_{\text{\tiny RR}}(X_{M \cap \textrm{Sh}}) = \tau^{\text{\tiny T}}_{\text{\tiny RR}}(X_{M \cap \textrm{Sh}})$, which, given the previous calculation, is equivalent to 
%\begin{align*}
%\mathbb{E}_{\text{\tiny S}}\left[b(X) \mid X_{M \cap \textrm{Sh}} \right] = \mathbb{E}_{\text{\tiny T}}\left[b(X) \mid X_{M \cap \textrm{Sh}} \right],    
%\end{align*}
%which has no reason to be valid in general, since in all generality, $X_{M \cap \textrm{Sh}}  \not\subset X_{B \cap \textrm{Sh}}$. A similar reasoning holds for the Excess Risk Ratio (ERR), as it is defined as a function of $\tau_{\text{\tiny RR}}$.
%,  requires that 
% but there is no reason that $\mathbb{E}^{\text{\tiny S}}\left[b(X) \mid X_M \cap \textrm{Shift} \right] = \mathbb{E}^{\text{\tiny T}}\left[b(X) \mid X_M \cap \textrm{Shift} \right]$, preventing Assumption~\ref{a:transportability} to hold. 


\end{proof}

\subsubsection{Proof of Theorem~\ref{thm_homogeneous_independence_generalization}}

The proof is straightforward by recalling that any collapsible causal measure satisfies Definition~\ref{def:indirect-collapsibility}.



\sout{
\begin{proof}
Recall that we consider a  binary output $Y \in \{0,1\}$. Recall that, by Lemma~\ref{lemma:intrication_model}, we have, for all $a \in \{0,1\},$
\begin{align*}
     \mathbb{P}\left[ Y^{(a)} = 1 \mid X = x\right]  &= b(x)+  a\, \left(   \left( 1-b\left(x\right) \right) m_b\left(x\right) -  b\left(x\right)m_g\left(x\right) \right), 
\end{align*}
with $b(x)=p_0(x)$. The proof of Lemma~\ref{lemma:intrication_model} can be adapted for any subset of covariates of $X$, so that 
\begin{align*}
     \mathbb{E}\left[ Y^{(a)}  \mid X_{M \cap \textrm{Sh}}\right]  &= b(X_{M \cap \textrm{Sh}})+  a\, \left(   \left( 1-b\left(X_{M \cap \textrm{Sh}}\right) \right) m_b\left(X_{M \cap \textrm{Sh}}\right) -  b\left(X_{M \cap \textrm{Sh}}\right)m_g\left(X_{M \cap \textrm{Sh}}\right) \right), 
\end{align*}
with 
\begin{align*}
b(X_{M \cap \textrm{Sh}}) & = \mathbb{P}\left[ Y^{(0)} = 1 \mid X_{M \cap \textrm{Sh}}\right]\\
m_g\left(X_{M \cap \textrm{Sh}}\right) & = \mathbb{P}\left[ Y^{(1)} = 0 \mid Y^{(0)} = 1, X_{M \cap \textrm{Sh}}\right] \\
 m_b\left(X_{M \cap \textrm{Sh}}\right) & = \mathbb{P}\left[ Y^{(1)} = 1 \mid Y^{(0)} = 0, X_{M \cap \textrm{Sh}}\right].    
\end{align*}


\textbf{First case.} Assume that, for all $x$, $m_b(x)=0$. According to the calculation above, we have

\begin{align}
    \tau^{\text{\tiny S}}_{\text{\tiny RR}}(X_{M \cap \textrm{Sh}})
    &= \frac{\mathbb{E}_{\text{\tiny S}}\left[ Y^{(1)} | X_{M \cap \textrm{Sh}} \right] }{\mathbb{E}_{\text{\tiny S}}\left[ Y^{(0)} | X_{M \cap \textrm{Sh}} \right]} \\
    %&= \frac{\mathbb{E}^{\text{\tiny S}}\left[ \mathbb{E}^{\text{\tiny S}}\left[ Y^{(1)} | X \right] | X_{M \cap \textrm{Sh}} \right] }{\mathbb{E}^{\text{\tiny S}}\left[  \mathbb{E}^{\text{\tiny S}}\left[ Y^{(0)} | X \right] | X_{M \cap \textrm{Sh}} \right]} \\
    & = \frac{b(X_{M \cap \textrm{Sh}}) - b(X_{M \cap \textrm{Sh}}) m_g\left(X_{M \cap \textrm{Sh}}\right)  }{b(X_{M \cap \textrm{Sh}})} && \text{Lemma~\ref{lemma:intrication_model}}\\
    & = 1 - m_g\left(X_{M \cap \textrm{Sh}}\right)\\
    & = 1 - \mathbb{P}_{\text{\tiny S}}\left[ Y^{(1)} = 0 \mid Y^{(0)} = 1, X_{M \cap \textrm{Sh}}\right] \\
    & = 1 - \mathbb{P}_{\text{\tiny S}}\left[ Y^{(1)} = 0 \mid Y^{(0)} = 1, X_{\textrm{Sh}}\right] && \text{Definition~\ref{def:two-kind-covariates}}\\
    & = 1 - \mathbb{P}_{\text{\tiny T}}\left[ Y^{(1)} = 0 \mid Y^{(0)} = 1, X_{\textrm{Sh}}\right] && \text{Definition~\ref{def:shidted-covariates}}\\
    & = 1 - \mathbb{P}_{\text{\tiny T}}\left[ Y^{(1)} = 0 \mid Y^{(0)} = 1, X_{M \cap \textrm{Sh}} \right] && \text{Definition~\ref{def:two-kind-covariates}}\\
    & = \tau^{\text{\tiny T}}_{\text{\tiny RR}}(X_{M \cap \textrm{Sh}}). \label{eq1_proof_RR_generalization_section5}
\end{align}
Following the proof of Lemma~\ref{lemma:collapsibility-of-RR-SR}, we have 
\begin{align*}
    \tau^{\text{\tiny T}}_{\text{\tiny RR}} &= \frac{\mathbb{E}_{\text{\tiny T}}\left[ Y^{(1)}\right]}{\mathbb{E}_{\text{\tiny T}}\left[ Y^{(0)}\right]}  && \text{By definition of the RR}\\
    &= \frac{\mathbb{E}_{\text{\tiny T}}\left[ \mathbb{E}_{\text{\tiny T}}\left[ Y^{(1)} \mid X_{M \cap \textrm{Sh}} \right]\right]}{\mathbb{E}_{\text{\tiny T}}\left[ Y^{(0)}\right]}  && \text{Law of total expectation used on $\mathbb{E}_{\text{\tiny T}}\left[ Y^{(1)}\right]$} \\
    &= \frac{\mathbb{E}_{\text{\tiny T}}\left[\frac{ \mathbb{E}_{\text{\tiny T}}\left[ Y^{(1)} \mid X_{M \cap \textrm{Sh}} \right]}{ \mathbb{E}_{\text{\tiny T}}\left[ Y^{(0)} \mid X_{M \cap \textrm{Sh}} \right]} \mathbb{E}_{\text{\tiny T}}\left[ Y^{(0)} \mid X_{M \cap \textrm{Sh}} \right]\right]}{\mathbb{E}_{\text{\tiny T}}\left[ Y^{(0)}\right]}  && \text{$\mathbb{E}_{\text{\tiny T}}\left[ Y^{(0)} \mid X_{M \cap \textrm{Sh}} \right] \neq 0$ ~almost surely} \\
    %&= \mathbb{E}\left[\frac{ \mathbb{E}\left[ Y^{(1)} \mid X \right]}{ \mathbb{E}\left[ Y^{(0)} \mid X \right]} \frac{\mathbb{E}\left[ Y^{(0)} \mid X \right]}{\mathbb{E}\left[ Y^{(0)}\right] }\right] && \text{$\mathbb{E}\left[ Y^{(0)}\right]$ is a constant} \\
    &= \mathbb{E}_{\text{\tiny T}}\left[\tau^{\text{\tiny T}}_\text{\tiny RR}(X_{M \cap \textrm{Sh}}) \frac{\mathbb{E}_{\text{\tiny T}}\left[ Y^{(0)} \mid X_{M \cap \textrm{Sh}} \right]}{\mathbb{E}_{\text{\tiny T}}\left[ Y^{(0)}\right] }\right]\\
    &= \mathbb{E}_{\text{\tiny T}}\left[\tau^{\text{\tiny S}}_\text{\tiny RR}(X_{M \cap \textrm{Sh}}) \frac{\mathbb{E}_{\text{\tiny T}}\left[ Y^{(0)} \mid X_{M \cap \textrm{Sh}} \right]}{\mathbb{E}_{\text{\tiny T}}\left[ Y^{(0)}\right] }\right] && \text{By \eqref{eq1_proof_RR_generalization_section5}}\\
    &= \mathbb{E}_{\text{\tiny S}}\left[\tau^{\text{\tiny S}}_\text{\tiny RR}(X_{M \cap \textrm{Sh}}) \frac{\mathbb{E}_{\text{\tiny T}}\left[ Y^{(0)} \mid X_{M \cap \textrm{Sh}} \right]}{\mathbb{E}_{\text{\tiny T}}\left[ Y^{(0)}\right] } \frac{p_{\text{\tiny T}}(X_{M \cap \textrm{Sh}})}{p_{\text{\tiny S}}(X_{M \cap \textrm{Sh}})} \right] && \text{ Since Assumption~\ref{a:overlap} holds}\\
\end{align*}
 Thus, $\tau_{\text{\tiny RR}}$ is generalizable with covariates $X_{M \cap \textrm{Sh}}$ when $m_b=0$, if one has access to $\mathbb{E}_{\text{\tiny T}}\left[ Y^{(0)} \mid X_{M \cap \textrm{Sh}} \right]$.

\bigskip 

\textbf{Second case.}
Assume that, for all $x$, $m_g(x)=0$. As above, we have
\begin{align*}
    \tau^{\text{\tiny S}}_{\text{\tiny SR}}(X_{M \cap \textrm{Sh}})
    &= \frac{1 - \mathbb{E}_{\text{\tiny S}}\left[ Y^{(1)} | X_{M \cap \textrm{Sh}} \right] }{1 - \mathbb{E}_{\text{\tiny S}}\left[ Y^{(0)} | X_{M \cap \textrm{Sh}} \right]} \\
    & = \frac{1 - b(X_{M \cap \textrm{Sh}}) - (1 - b(X_{M \cap \textrm{Sh}})) m_b\left(X_{M \cap \textrm{Sh}}\right)  }{1 - b(X_{M \cap \textrm{Sh}})} && \text{Lemma~\ref{lemma:intrication_model}}\\
    & = 1 - m_b\left(X_{M \cap \textrm{Sh}}\right)\\
    & = 1 - \mathbb{P}_{\text{\tiny S}}\left[ Y^{(1)} = 1 \mid Y^{(0)} = 0, X_{M \cap \textrm{Sh}}\right] \\
    & = 1 - \mathbb{P}_{\text{\tiny S}}\left[ Y^{(1)} = 1 \mid Y^{(0)} = 0, X_{\textrm{Sh}}\right] && \text{Definition~\ref{def:two-kind-covariates}}\\
    & = 1 - \mathbb{P}_{\text{\tiny T}}\left[ Y^{(1)} = 1 \mid Y^{(0)} = 0, X_{\textrm{Sh}}\right] && \text{Definition~\ref{def:shidted-covariates}}\\
    & = 1 - \mathbb{P}_{\text{\tiny T}}\left[ Y^{(1)} = 1 \mid Y^{(0)} = 0, X_{M \cap \textrm{Sh}} \right] && \text{Definition~\ref{def:two-kind-covariates}}\\
    & = \tau^{\text{\tiny T}}_{\text{\tiny RR}}(X_{M \cap \textrm{Sh}}).
\end{align*}
As above, one can use the arguments in the proof of Lemma~\ref{lemma:collapsibility-of-RR-SR} to show that 
\begin{align*}
    \tau^{\text{\tiny T}}_{\text{\tiny SR}} &=\mathbb{E}_{\text{\tiny S}}\left[\tau^{\text{\tiny S}}_\text{\tiny SR}(X_{M \cap \textrm{Sh}}) \frac{1-\mathbb{E}_{\text{\tiny T}}\left[ Y^{(0)} \mid X_{M \cap \textrm{Sh}} \right]}{1-\mathbb{E}_{\text{\tiny T}}\left[ Y^{(0)}\right] } \frac{p_{\text{\tiny T}}(X_{M \cap \textrm{Sh}})}{p_{\text{\tiny S}}(X_{M \cap \textrm{Sh}})}\right],
    \end{align*}
which proves that $\tau^{\text{\tiny T}}_{\text{\tiny SR}}$ is generalizable with covariates $X_{M \cap \textrm{Sh}}$ when $m_g=0$, if one has access to $\mathbb{E}_{\text{\tiny T}}\left[ Y^{(0)} \mid X_{M \cap \textrm{Sh}} \right]$.


%All others causal measure would not allow such equalities due to their intrication with the baseline. For example,
%\begin{align*}
 %   \tau^{\text{\tiny S}}_{\text{\tiny RD}}(X_M \cap \textrm{Shift})
%    &= \mathbb{E}\left[ \left( 1-b\left(X\right) \right) m_b\left(X\right)\right] -  \mathbb{E}\left[ b\left(X\right)m_g\left(X\right) \right]&& \text{Lemma~\ref{lemma:expression-of-causal-quantities-under-generative-model-binary-outcome}} \\
%    &= 1-\mathbb{E}^{\text{\tiny S}}\left[ Y^{(1)}=1 \mid Y^{(0)}=0, X \cap \textrm{Shift} \right] && \text{Definition~\ref{def:two-kind-covariates}} \\
%    &= 1-\mathbb{E}^{\text{\tiny T}}\left[ Y^{(1)}=1 \mid Y^{(0)}=0, X \cap \textrm{Shift} \right] \\
 %   &=  \tau^{\text{\tiny T}}_{\text{\tiny SR}}(X_M \cap \textrm{Shift}).
%\end{align*}


\end{proof}}

\section{Comments on logistic regression}\label{appendix:usual-point-of-view}

%What we highlight in this paper is that the interpretation of all other metrics than the conditional odds ratio is clearly not obvious (see Lemma~\ref{lemma:expression-of-causal-quantities-under-generative-model-binary-outcome} in Section~\ref{appendix:usual-point-of-view}). In other words, it seems that such logistic models are rather forcing all covariates and treatment to interact through the link function, preventing any simple interpretation of the causal measure (except conditional OR). This is even more problematic as the odds ratio is a non logic-respecting measure.


A common practice \modif{in applied statistics} is to adopt a logistic regression model (or any model encapsulating a function taking values in $\mathbb{R}$), for example assuming that the following logistic model holds:
\begin{equation}\label{eq:typical-model-used-binary-Y-main}
   \operatorname{log}\left( \frac{\mathbb{P}(Y^{(a)} = 1 \mid X)}{\mathbb{P}(Y^{(a)} = 0 \mid X) } \right) = \beta_0 + \langle \boldsymbol{\beta}, \boldsymbol{X} \rangle + A\,m,
\end{equation}
%\es{à relire}\bc{partiellement relu et modifié suite à la lecture}
where $\beta_0, \boldsymbol{\beta}$ and $m$ are the coefficients of a linear model (see for example \cite{Daniel2020MakingApple}). When the generative model from Equation~\ref{eq:typical-model-used-binary-Y-main} holds, some nice properties arise.
Notably, one can show that this implies constant conditional odds ratio $\tau_{\text{\tiny log-OR}} (x) = m$ and $\tau_{\text{\tiny OR}} (x) = e^{m}$. The derivations are detailed below:

\begin{align*}
    \tau_{\text{\tiny OR}}(X) &:= \frac{\mathbb{P}(Y^{(1)} = 1 \mid X)}{\mathbb{P}(Y^{(1)} = 0 \mid X) } \cdot \left( \frac{\mathbb{P}(Y^{(0)} = 1 \mid X)}{\mathbb{P}(Y^{(0)} = 0 \mid X) } \right)^{-1}\\
    & = e^{\beta_0 + \langle \boldsymbol{\beta}, \boldsymbol{X} \rangle + m} \cdot e^{-\beta_0 - \langle \boldsymbol{\beta}, \boldsymbol{X} \rangle} \\
    &= e^{m}.
\end{align*}



Beyond \eqref{eq:typical-model-used-binary-Y-main} it is possible to encapsulate non-parametric functions in the logit. Such decomposition is present in the literature \citep{Gao2021DINA} (and see Section~\ref{appendix:usual-point-of-view}, and in particular Lemma~\ref{lem:conditional-odds-ratio} for details).

\begin{lemma}[Logit generative model for a binary outcome]\label{lemma:generative-model-binary-Y}
 Considering a binary outcome $Y$, assume that 
 
 \begin{equation*}
     \forall x \in \mathds{X},\, \forall a \in \{0,1\},\quad 0 < p_a(x) < 1,\quad \text{where } p_a(x)  = \mathbb{P}(Y^{(a)} = 1 \mid X=x).
 \end{equation*}

 Then, there exist two functions $b, m:\mathcal{X} \to \mathbb{R}$ such that

\begin{equation*}
\operatorname{ln}\left( \frac{\mathbb{P}(Y^{(a)} = 1 \mid X)}{\mathbb{P}(Y^{(a)} = 0 \mid X) } \right) = b(X) + a\, m(X).
\end{equation*}
\end{lemma}


%Similarly than for the continuous outcomes, the assumption allowing the existence of such functions is very weak, only asking for the counterfactual probabilities to be distinct from $0$ and $1$. The notations have been chosen to reflect the previous idea of a \textit{baseline} $b(x)$ and the \textit{modification} $m(x)$ induced by the treatment $A$. Still, we point out that in this model, $p_0(x) \neq  b(x)$, and that due to the link function, $b(x)$ and $m(x)$ can not be disentangled.


\begin{proof}
{\footnotesize {\color{Blue} Consider $a \in \{0,1\}$, and assume that their exists a function $p_a:\mathbb{R}^d \to ]0,1[$ such that,

\begin{equation*}
    \mathbb{P}(Y^{(a)} = 1 \mid X) = p_a(X).
\end{equation*}


Because $p_a$ takes values in  $]0,1[$ the odds can be considered, so that,

\begin{equation*}
    \operatorname{ln}\left(\frac{\mathbb{P}(Y^{(a)} = 1 \mid X)}{\mathbb{P}(Y^{(a)} = 0 \mid X) }\right) =   \operatorname{ln}\left(\frac{p_a(X)}{1-p_a(X)}\right).
\end{equation*}

Denoting, 

\begin{equation*}
    b(X) := \operatorname{ln}\left(\frac{p_0(X)}{1-p_0(X)}\right),
\end{equation*}

and

\begin{equation*}
   m(X) \ := \operatorname{ln}\left(\frac{p_1(X)}{1-p_1(X)}\right) - \operatorname{ln}\left(\frac{p_0(X)}{1-p_0(X)}\right) = \operatorname{ln}\left(\frac{p_1(X)}{1-p_1(X)}\, \frac{1-p_0(X)}{p_0(X)} \right),
\end{equation*}

one can write the log-odds as

\begin{equation*}
    \operatorname{ln}\left(\frac{\mathbb{P}(Y^{(a)} = 1 \mid X)}{\mathbb{P}(Y^{(a)} = 0 \mid X) }\right) =   b(X) + A\, m(X).
\end{equation*}

Note that another link function could have been chosen, which impacts how $b(x)$ and $m(x)$ are defined.}}
\end{proof}

\begin{comment}
    \paragraph{Comment on the usual practice} In many papers it is possible to find this very common assumption
\begin{equation}\label{eq:typical-model-used-binary-Y}
   \operatorname{ln}\left( \frac{\mathbb{P}(Y^{(a)} = 1 \mid X)}{\mathbb{P}(Y^{(a)} = 0 \mid X) } \right) = \beta_0 + \langle \boldsymbol{\beta}, \boldsymbol{X} \rangle + A\,m,
\end{equation}
which corresponds to a linear function $b(X)$ and a constant function $m(X)$ \citep{Daniel2020MakingApple}. In particular, it is easy to derive from \eqref{eq:typical-model-used-binary-Y} that for any $X\in \mathds{X}$, one has $\tau_{\text{\tiny log-OR}} (x) = m$ and $\tau_{\text{\tiny OR}} (x) = e^{m}$. And more generally,
\end{comment}

\begin{lemma}[Conditional log odds ratio]\label{lem:conditional-odds-ratio}
Ensuring conditions of Lemma~\ref{lemma:generative-model-binary-Y} leads to,

\begin{equation*}
     \mathbb{E}\left[ \tau_{\text{\tiny log-OR}} (X) \right]:= \mathbb{E}\left[\operatorname{ln}\left( \frac{\mathbb{P}(Y^{(1)} = 1 \mid X)}{\mathbb{P}(Y^{(1)} = 0 \mid X) } \, \left( \frac{\mathbb{P}(Y^{(0)} = 1 \mid X)}{\mathbb{P}(Y^{(0)} = 0 \mid X) }  \right)^{-1} \right) \right]=  \mathbb{E}\left[m(X)\right].
\end{equation*}


\end{lemma}

This result is apparently satisfying, where $\mathbb{E}\left[ \tau_{\text{\tiny log-OR}} (X) \right]$ somehow only grasps the modification function. Still, note that due to non-collapsibility of the odds ratio, this \underline{does not imply} that $\tau_{\text{\tiny log-OR}} = \tau$ (i.e. $\tau_{\text{\tiny OR}}= e^{\tau}$) because $ \mathbb{E}\left[ \tau_{\text{\tiny log-OR}} (X) \right] \neq  \tau_{\text{\tiny log-OR}}$ (except if treatment effect is null or if the outcome does not depend on $X$, that is $b(X)$ and $m(X)$ are both scalars). As an intermediary conclusion, the working model from Lemma~\ref{lemma:generative-model-binary-Y} leads to complex expression of causal measures, except for $\mathbb{E}\left[ \tau_{\text{\tiny log-OR}} (X) \right]$, but with the default that this measure shows bad property of non-collapsibility.


For example, a working model such that $m(x) = m$ is a constant don't lead to any measures to be constant.

\begin{lemma}\label{lemma:expression-of-causal-quantities-under-generative-model-binary-outcome}
Ensuring conditions of Lemma~\ref{lemma:generative-model-binary-Y} leads to,
\begin{align}
     \tau_{\text{\tiny RD}} &= \mathbb{E}\left[ \frac{e^{b(X) + m(X)}}{1+e^{b(X) + m(X)}}  \right] - \mathbb{E}\left[ \frac{e^{b(X)}}{1+e^{b(X)}}  \right]  \\
       \tau_{\text{\tiny ERR}} &= \mathbb{E}\left[ \frac{e^{b(X) + m(X)}}{1+e^{b(X) + m(X)}}  \right] \left(\mathbb{E}\left[ \frac{e^{b(X)}}{1+e^{b(X)}}  \right]  \right)^{-1} -1  \\ 
      \tau_{\text{\tiny NNT}} &= \left( \mathbb{E}\left[ \frac{e^{b(X) + m(X)}}{1+e^{b(X) + m(X)}}  \right] - \mathbb{E}\left[ \frac{e^{b(X)}}{1+e^{b(X)}}  \right] \right)^{-1}\\   
     \tau_{\text{\tiny RR}} &= \mathbb{E}\left[ \frac{e^{b(X)+m(X)}}{1+e^{b(X)+m(X)}}\right] \left( \mathbb{E}\left[ \frac{e^{b(X)}}{1+e^{b(X)}}\right] \right)^{-1} \\
     \tau_{\text{\tiny SR}} &= \mathbb{E}\left[ \left( 1 + e^{b(X)+m(X)} \right)^{-1}  \right] \left(\mathbb{E}\left[ \left(1 + e^{b(X)}  \right)^{-1} \right]\right)^{-1} \\
     \tau_{\text{\tiny OR}} &= \frac{\mathbb{E}\left[ \frac{e^{b(X)+m(X)}}{1 +e^{b(X)+m(X)} }\right]}{\mathbb{E}\left[ \frac{1}{1+e^{b(X)+m(X)}}\right]} \frac{\mathbb{E}\left[ \frac{1}{1 + e^{b(X)} }\right]}{\mathbb{E}\left[ \frac{e^{b(X)}}{1+e^{b(X)}}\right]}. 
\end{align}
\end{lemma}

%For example the rather simple expression of RD for the continuous outcome now shows bewildering and complex forms when having a binary outcome.

All expressions from Lemma~\ref{lemma:expression-of-causal-quantities-under-generative-model-binary-outcome} now involve both $b(.)$ and $m(x)$. All other metrics show complex relation between the two functions. 


Finally, note that the logistic model is unable to easily describe accurately the Russian Roulette thought being simple. Note that
\begin{equation*}
   \operatorname{log}\left( \frac{\mathbb{P}(Y^{(a)} = 1 \mid X = x)}{\mathbb{P}(Y^{(a)} = 0 \mid X =x) } \right) = \operatorname{log}\left( \frac{b(x)}{1-b(x)}\right)+ A\, \operatorname{log}\left( \frac{\left(\frac{1}{6} + b(x) \right)}{1-\left(\frac{1}{6} + b(x))(1-b(x))\right)} \cdot \frac{1-b(x)}{b(x)} \right),
\end{equation*}
is the equivalent to Equation~\ref{eq:intuition-of-intrication}.


\begin{comment}
    Denoting $b_1(X)$ and $m_1(X)$ the functions for the intrication model, and $b_2(X)$ and $m_2(X)$ for the logistic model, one has:


\begin{equation*}
   b_2(X) = \operatorname{ln}\left(\frac{b_1(X)}{1-b_1(X)}\right)
\end{equation*}

and 


\begin{align*}
    m_2(X) &= \operatorname{ln}\left( \frac{\left(m_1(X) + b_1(X))(1-b_1(X))\right)}{1-\left(m_1(X) + b_1(X))(1-b_1(X))\right)}\right)  -   \operatorname{ln}\left(\frac{b_1(X)}{1-b_1(X)}\right)
\end{align*}


Taking the case of the Russian Roulette, one has

\begin{equation*}
    b_1(X) := p_0(X), \quad m_1(X) = \frac{1}{6}
\end{equation*}

so that 
\begin{equation*}
     b_2(X) := \operatorname{ln}\left(\frac{X}{1-X}\right)
\end{equation*}

and

\begin{equation*}
     m_2(X) := \operatorname{ln}\left( \frac{\left(\frac{1}{6} + p_0(X) \right)}{1-\left(\frac{1}{6} + p_0(X))(1-p_0(X))\right)}\right)  -   \operatorname{ln}\left(\frac{p_0(X)}{1-p_0(X)}\right).
\end{equation*}

Despite a rather simple example, it is non-intuitive to encode it into the logistic model due to the link function.

\end{comment}





\section{More details about the Russian Roulette example}\label{appendix:more-details-on-the-intrication-model}

\textit{We provide more details on how the Russian Roulette is stated in \cite{CinelliGeneralizing2019}. Note that the first reference we have found of this problem is in \cite{Huitfeldt2019LessWrong}. This section is just meant to recall how the problem was initially introduced by \cite{Huitfeldt2019LessWrong}.}\\


Suppose the city of Los Angeles decides to run a randomized control trial.  Running the experiment, the mayor of Los Angeles discovers that “Russian Roulette” is harmful: among those assigned to play Russian Roulette, 17.5\% of the people died, as compared to only 1\% among those who were not assigned to play the game (people can die due to other causes during the trial, for example, prior poor health conditions). This example is a good toy example as the mechanism is well-known, with a chance of one over six to die when playing. Even if it seems counter-intuitive, we consider the treatment as being forced to play to the russian roulette (we consider the player plays only one time). 
We denote by $\Pi$ the population from Los Angeles. In that case, we can already note that the RR is $17.5$ and the ATE is $0.165$ (outcome being $Y$ equals to 1 if death before the end of the period). With this notation $\mathrm{E}[Y^{(0)} | pop = \Pi] = 0.01$ and $\mathrm{E}[Y^{(1)} | pop = \Pi] = 0.175$\\

After hearing the news about the Los Angeles experiment, the mayor of New York City (a dictator, and we propose to denote the population of New York City by $\Pi^*$) wonders what the overall mortality rate would be if the city forced everyone to play Russian Roulette. Currently, the practice of Russian Roulette is forbidden in New York, and its mortality rate is at 5\% (4\% higher than LA, being $\mathrm{E}[Y^{(0)} | pop = \Pi^*] = 0.05$). The mayor thus asks the city’s statistician to decide whether and how one could use the data from from Los Angeles to predict the mortality rate in New York, once the new policy is implemented. But in fact, knowing the mechanism of the russian roulette we can already compute the value of interest being $\mathrm{E}[Y^{(1)} | pop = \Pi^*]$. Results are presented in Table~\ref{tab:summmary_russian_roulette}. Here we used the fact that mortality is a consequence of two “independent” processes (the game of Russian Roulette and prior health conditions of the individual), and while the first factor remains unaltered across cities, the second intensifies by a known amount (5\% vs 1\%).  Moreover, we can safely assume that the two processes interact disjunctively, namely, that death occurs if and only if at least one of the two processes takes effect. We can also - within the two cities - compute the associated RR, ATE and survival ratio (SR). We can observe they are not the same, but only the survival ratio comparing how many people dies with treatment on how many people would have died without treatement, transport the \textit{mechanism} of the Russian Roulette (note that $\frac{5}{6} \sim 0.83$).


\begin{table}[!h]
\begin{center}
\begin{tabular}{l|l|l}
\hline
Population &  Los Angeles ($\Pi$) & New York city ($\Pi^*$)  \\ \hline \hline
$\mathrm{E}\left[Y^{(0)}\right]$ & 0.01 & 0.05 \\ \hline
$\mathrm{E}\left[Y^{(1)}\right]$ & $\frac{1}{6}0.99 + 0.01 = 0.175$ & $\frac{1}{6}0.95 + 0.05 = 0.208$ \\ \hline
RR & 17.5  & 4.16 \\ \hline
ATE & 0.165 & 0.158 \\ \hline
SR & 0.83 & 0.83 \\
\end{tabular}
\caption{Summary of the different values. Note that none of the transport equation is applied, everything is computed within each population taking into account a distinct mechanism between the two reasons to die. SR corresponds to the survival ratio.}
\label{tab:summmary_russian_roulette}
\end{center}
\end{table}


\begin{comment}
\textbf{A limit case, when $b(x) \ll m(x)$}\\

We recall the intuitive model we have proposed in \eqref{eq:intuition-of-intrication}. 

\begin{align*}
    \mathbb{P}\left[ Y^{(a)} = 1 \mid X = x\right] &= b(x) + a\,\underbrace{\textcolor{RoyalBlue}{\left(1-b\left(x \right)\right)}}_\textrm{Intrication}\, \frac{1}{6}.
\end{align*}


Comparing the intrication model from \eqref{eq:intuition-of-intrication}, one can observe that, compared to the working model of the continuous outcome (Lemma~\ref{lemma:working-model-continuous-Y}) the baseline and the effect of the treatment are entangled. Interestingly, if  $b(x) \ll m(x)$ (in particular, the baseline is close to $0$) then it is possible to have:
\begin{equation*}
    \text{If,  } p_0(x) \ll 1, \quad \text{then,} \quad \mathbb{P}\left[ Y^{(a)} \mid X = x\right] \approx b(x) + a\,m(x),
\end{equation*}
such that we retrieved the intuition of the continuous outcome model, and remove the entanglement as expected.


Application of Lemma~\ref{lemma:expression-of-causal-quantities-under-generative-model-binary-outcome-intrication-model} for the Russian Roulette example gives:
\begin{equation*}
\begin{aligned}
    &\tau_{\text{\tiny RD}} = \frac{1}{6}\left(1 - \mathbb{E}\left[b(x) \right]  \right),\quad   \tau_{\text{\tiny NNT}} = \frac{6}{1-  \mathbb{E}\left[b(x) \right] }  ,\quad 
    \tau_{\text{\tiny RR}} = \frac{5}{6} + \frac{1}{ \mathbb{E}\left[b(x) \right] } \frac{1}{6},\quad \tau_{\text{\tiny SR}} =\frac{5}{6}, \\ 
    &\text{and}\quad \tau_{\text{\tiny OR}} = \left(1 + \frac{\mathbb{E}\left[(1-b(X)) \right]}{\mathbb{E}\left[ b(X)\right]} \frac{1}{6} \right) \frac{\mathbb{E}\left[1-b(X) \right]}{\mathbb{E}\left[(1-b(X)) \right]}\frac{6}{7}.
    \end{aligned}
\end{equation*}
\end{comment}

\section{Different points of view}\label{appendix:different-point-of-views}

\textit{This section gathers quotes from research papers or books. The aim is to illustrate how diverse opinions are.}

\paragraph{General remarks about the choice of measure}
\begin{quote}
    Physicians, consumers, and third-party payers may
be more enthusiastic about long-term preventive treatments when benefits are stated as relative, rather than absolute, reductions in the risk of adverse events.
Medical-journal editors have said that reporting only relative reductions in risk is usually inadequate in scientific articles and have urged the news media to consider the importance of discussing both absolute and relative risks. For example, a story reporting that
in patients with myocardial infarction, a new drug
reduces the mortality rate at two years from 10 percent to 7 percent may help patients weigh both the
3 percent absolute and the 30 percent relative reduction in risk against the costs of the drug and its side
effects. -- \citep{moynihan2000coverage}
\end{quote}

\begin{quote}
    In general, giving only the absolute or only the relative benefits does not tell the full story; it is more
informative if both researchers and the media make
data available in both absolute and relative terms.  -- \citep{moynihan2000coverage}
\end{quote}

\begin{quote}
    The promotion of a measure often reflects personal preferences – those who are keen to promote the use of research in practice emphasize issues of interpretability of Risk Ratios and risk differences, those who are keen to ensure mathematical rules are always obeyed emphasize the limitations and inadequacies of the same measures. -- \citep{Deeks2022IssuesInSelection}
\end{quote}

\begin{quote}
    Failing to report NNT may influence the interpretation of study results. For example reporting RR alone may lead a reader to believe that a treatment effect is larger than it really is. -- \citep{Nuovo2002ReportingNNT}
\end{quote}

\begin{quote}
    As \textit{evidence-based practitioners}, we must decide which measure of association deserves our focus. Does it matter? The answer is yes. The same results, when presented in different ways, may lead to different treatment decisions. -- \citep{Cook2014UserGuide}
\end{quote}

\begin{quote}
    You must, however, distinguish between the RR and the RD. The reason is that the RR is generally far larger than the RD, and presentations of results in the form of RR (or RRR) can convey a misleading message. -- (focusing on binary outcome) \citep{Cook2014UserGuide}
\end{quote}

\begin{quote}
    Standard measures of effect, including the Risk Ratio, the odds ratio, and the risk difference, are associated with a number of well-described shortcomings, and no consensus exists about the conditions under which investigators should choose one effect measure over another. -- \citep{huitfeldt2018choice}
\end{quote}

\begin{quote}
    Additive treatment effect heterogeneity is also most informative for guiding public health policy that aims to maximize the benefit or minimize the harm of an exposure by targeting subgroups. The relative scale (Risk Ratios or odds ratios) can tend to overstate treatment benefits or harms. -- \citep{lesko2018considerations}
\end{quote}

\begin{quote}
   The way to express and measure risk may appear to be a pure technicality. In fact, it is a crucial element of the risk-benefit balance that underlies the dominant medical discourse on contraception. Its influence on the perception and communication of risk is decisive, especially among people without a solid statistical education, like most patients and doctors who prescribe the pill (mostly generalists and gynaecologists). The dispute over \textit{Non-rare thrombophilia} (NRT) screening sets an important difference between the absolute risk, the number of events occurring per time unit and the relative risk, which is the ratio between two absolute risks. Practically, whereas the relative risk may sound alarming, the absolute risk looks more reassuring. -- \citep{Bourgain2021Appraising}
\end{quote}

\begin{quote}
    We believe if an efficacy measure is
    \begin{itemize}
        \item well defined,
        \item understandable by human,
        \item desired by patients and clinicians,
        \item proven to be logic-respecting\footnote{see Definition~\ref{def:logic-respecting}.},
        \item readily implementable computationally,
    \end{itemize}

    them it is worthy of consideration. -- \citep{liu2022rejoinder}
\end{quote}


\paragraph{The odds ratio as a complex measure to interpret}


\begin{quote}
    Odds ratios and parameters of multivariate models will often be useful in serving as or in constructing the estimates, but should not be treated as the end product of a statistical analysis of epidemiologic data or as summaries of effect in themselves. -- \citep{Greenland1987Interpretation}
\end{quote}


\begin{quote}
   The concept of the odds ratio is now well-established in epidemiology, largely because it serves as a link between results obtainable from follow-up studies and those obtainable from case-control studies. [$\dots$] This ubiquity, along with certain technical considerations, has led some authors to treat the odds ratio as perhaps a ``universal" measure of epidemiologic effect, in that they would estimate odds ratios in follow-up studies as well as case-control studies; others have expressed reservations about the utility of the odds ratio as something other than an estimate of an incidence ratio. I believe that such controversy as exists regarding the use of the odds ratio arises from its inherent disadvantages compared with the other measures for biological inference, and its inherent advantages for statistical inference.  -- \citep{Greenland1987Interpretation}
\end{quote}


\begin{quote}
   There is a problem with odds: unlike risks, they are difficult to understand.  -- \citep{davies1998can}
\end{quote}


\begin{quote}
    Another measure often used to summarise effects of treatment is the odds ratio. This is defined as the odds of an event in the active treatment group divided by the odds of an event in the control group. Though this measure has several statistical advantages and is used extensively in epidemiology, we will not pursue it here as it is not helpful in clinical decision making. -- \citep{Cook1995NNT}
\end{quote}

\begin{quote}
    In logit and other multiplicative intercept models (but not generally), OR also has the attractive feature of being invariant with respect to the values at which control variables are held constant. The disadvantage of OR is understanding what it means, and when OR is not the quantity of interest then its ‘advantages’ are not suficient to recommend its use. Some statisticians seem comfortable with OR as their ultimate quantity of interest, but this is not common. Even more unusual is to find anyone who feels more comfortable with OR than the other quantities defined above; we have found no author who claims to be more comfortable communicating with the general public using an odds ratio. -- \citep{king2002estimating}
\end{quote}


\begin{quote}
    The OR lacks any interpretation as an average. -- \citep{Cummings2009RelativeMeritsRRAndOR}
\end{quote}
\begin{quote}
    As is well established, the odds ratio is not a parameter of interest in public health research. -- \citep{Spiegelman2017Modeling}
\end{quote}

\begin{quote}
    Because of the exaggeration present, it is important to avoid representing ORs as RRs, and similarly, it is important to recognize that a reported OR rarely provides a good approximation of relative risks but rather simply provides a measure of correlation. -- \citep{George2020WhatsTR}
\end{quote}


\begin{quote}
    We agree with Liu et al. (2020) that (causal) odds ratios and hazard ratios are problematic as causal contrasts. The non-collapsibility of these parameters is a mathematical property which makes their interpretation awkward, and this is amplified for hazard by their conditioning on survival. Thus they are also unsuitable measures for transportability between different populations (Martinussen \& Vansteelandt, 2013). It is particularly concerning that meta-analyses pool odds ratios or hazard ratios from different studies each possibly using different variables for adjustment where the issue of non-collapsibility is typically ignored. -- \citep{Didelez2021collapsibility}
\end{quote}


\begin{quote}
    ORs are notoriously difficult to interpret. When people hear “odds” they think of “risks” and this leads to the common misinterpretation of the OR as a RR by scientists and the public, which is a serious concern. For example, an OR of 2 is not generally a doubling of risk (if the risk in the control group is 20\% and the OR is 2, then the risk in the treated group is 33.3\% not 40\%). In contrast, the RD and RR offer clearer interpretations. -- \citep{xiao2022IsORPortable}
\end{quote}

\begin{quote}
    The admitted mathematical niceties of the OR are not reason enough to accept such a confusing state of affairs. Of course, when the outcome is rare, the OR approximates the RR and is, therefore, approximately collapsible.-- \citep{xiao2022IsORPortable}
\end{quote}

\begin{quote}
    Because of the interpretability issues and lack of collapsibility, we urge researchers to avoid ORs when either the RD or RR is available. -- \citep{xiao2022IsORPortable} 
\end{quote}

\begin{quote}
    Odds ratios provoke similar discomfort—only 19\% of learners and 25\% of speakers at an annual meeting of the Canadian Society of Internal
Medicine (CSIM) understood odds ratios well enough to explain them to others. -- \citep{Lapointe2022FromMathToMeaning}
\end{quote}

\paragraph{The OR is a better metric to use than RR}

\begin{quote}
    The results demonstrate the need to a) end the primary use of the RR in clinical trials and meta-analyses as its direct interpretation is not meaningful; b) replace the RR by the OR; and c) only use the post-intervention risk recalculated from the OR for any expected level of baseline risk in absolute terms for purposes of interpretation such as the number needed to treat. -- \citep{Doi2020callToChangePractice}
\end{quote}

\begin{quote}
    We can no longer accept the commonly argued for view that the relative risk is easier to understand. Once we realize that the RR depends more on prevalence than the exposure-outcome association, its interpretation becomes much more difficult to comprehend than the odds ratio. It is well known that, for common events, large values of the Risk Ratio are impossible and this should have rung the alarm bells much earlier regarding whether the RR is more a measure of prevalence than a measure of effect. However this was not the main focus of the derivation outlined previously and the latter was aimed at demonstrating why the OR is a true measure of effect against which the RR can be compared. -- \citep{Doi2020callToChangePractice}
\end{quote}


\begin{quote}
    Our response to this is that, although this is certainly a problem, there is an even bigger problem – \textit{the RR is not a portable measure of effect}. By "portable" we mean a numerical value that is not dependent on baseline risk and not transportability in causal inference. --- \citep{doi2022TimeToDoAway}
\end{quote}



\paragraph{Relative versus absolute measures}
\begin{quote}
    In reviewing the different ways that benefit and harm can be expressed, we conclude that the RD is superior to the RR because it incorporates both the baseline risk and the magnitude of the risk reduction. -- \citep{Laupacis1988AnAssessmentOfClinically}
\end{quote}


\begin{quote}
    For clinical decision making, however, it is more meaningful to use the measure “number needed to treat.” This measure is calculated on the inverse of the absolute risk reduction. It has the advantage that it conveys both statistical and clinical significance to the doctor. Furthermore, it can be used to extrapolate published findings to a patient at an arbitrary specified baseline risk when the relative risk reduction associated with treatment is constant for all levels of risk. -- \citep{Cook1995NNT}
\end{quote}



\begin{quote}
    Medical journals need to be conscious that they will contribute to scaremongering newspaper headlines if they do not request authors to quantify Adverse Drug Reactions (ADR) into best estimates of absolute numbers. -- \citep{Mills1999PillScare}
\end{quote}


\begin{quote}
    As a relative measure of effect, the RR is most directly estimated by the multiplicative model when it fits the data. The risk difference is an absolute measure of effect, most directly estimated by the additive model when it fits the data. -- \citep{Spiegelman2017Modeling}
\end{quote}

\paragraph{About portability or generalizability of causal effects}



\begin{quote}
    The numbers needed to treat method still presents a
problem when applying the results of a published
randomised trial in patients at one baseline risk to a
particular patient at a different risk. -- \citep{Cook1995NNT}
\end{quote}
\begin{quote}
    Some authors prefer odds ratios because they believe a constant (homogeneous) odds ratio may be more plausible than a constant Risk Ratio when outcomes are common. -- \citep{Cummings2009RelativeMeritsRRAndOR}
\end{quote} 

\begin{quote}
    All of this assumes a constant RR across risk groups; fortunately, a more or less constant RR is usually the case, and we suggest you make that assumption unless there is evidence that suggests it is incorrect. -- \citep{Cook2014UserGuide}
\end{quote}



\begin{quote}
    Although further and more formal quantitative work evaluating the relative degree of heterogeneity for Risk Ratio versus risk differences may be important, the previously mentioned considerations do seem to provide some indication that, for whatever reason, Risk Ratio modification is uncommon. -- \citep{Spiegelman2017Modeling}
\end{quote}

\begin{quote}
    It is commonly believed that the Risk Ratio is a more homogeneous effect measure than the risk difference, but recent methodological discussion has questioned the evidence for the conventional wisdom. -- \citep{huitfeldt2018choice}
\end{quote}

\begin{quote}
    In the real world of clinical medicine, doctors are usually given information about the effects of a drug on the Risk Ratio scale (the probability of the outcome if treated, divided by the probability of the outcome if untreated). With information on the Risk Ratio, a doctor may make a prediction for what will happen to the patient if treated, by multiplying the Risk Ratio and patient's risk if untreated (which is predicted informally based on observable markers for the patient's condition).  -- \citep{Huitfeldt2019LessWrong}
\end{quote}


\begin{quote}
    In this article we will show that the RR is not a measure of the magnitude of the intervention-outcome association alone because it as stronger relationship with prevalence and therefore is not generalizable beyond the baseline risk of the population in which it is computed. -- \citep{Doi2020callToChangePractice}
\end{quote}


\begin{quote}
    It is possible that no effect measure is “portable” in a meta-analysis. In cases where portability of the effect measure is challenging to satisfy, we suggest presenting the conditional effect based on the baseline risk using a bivariate generalized linear mixed model. The bivariate generalized linear mixed model can be used to account for correlation between the effect measure and baseline disease risk. Furthermore, in addition to the overall (or marginal) effect, we recommend that investigators also report the effects conditioning on the baseline risk. -- \citep{xiao2022IsORPortable}
\end{quote}

\begin{quote}
    Despite some concerns, the RR has been widely used because it is considered a measure with “portability” across varying outcome prevalence, especially when the outcome is rare. -- \citep{Suhail2022CallForChange}
\end{quote}


\section{Comments and answers to related articles}

As highlighted by the length of the references or even by Section~\ref{appendix:different-point-of-views}: the literature on the choice of causal measures is prolific. In this Section, we propose comments or answers to previous articles in order to show how our contributions either complete what was said or shed lights on a different apprehension of the problem.

\begin{comment}
\subsection{Comments of \cite{Cook1995NNT}}

 \cite{Cook1995NNT}'s widely cited paper promotes the usage of absolute measure versus relative measure for clinical decision. In particular they advocate the NNT as it is easier to interpret than a difference of probabilities (RD). For example, we quote such a section:

 \begin{quote}
    \textit{ For example, an estimated relative risk reduction of 50\% might be statistically significant and clinically important for patients at moderate to high risk of a particular adverse event. However, for patients with a low probability of an event the risk reduction might not be sufficient to warrant the toxicity and cost of active treatment. This is the main criticism of relative measures of treatment effect for the purposes of clinical decision making.}
 \end{quote}


We agree on the fact that for a binary outcome an absolute measure better (such as the NNT) may be more informative for a patient. In this article authors use clinical data on which the Risk Ratio is constant across subgroups, and the treatment effect is beneficial. Interestingly, this what we show with the intrication model, that if one measure is more likely to be constant across different populations or subgroup: this is the RR (or the SR depending on the direction of the effect). These qualitative observations are completely coherent with Lemmas~\ref{lemma:expression-of-causal-quantities-under-generative-model-binary-outcome-intrication-model} and \ref{lemma:monotonous-effect}.

\end{comment}

\subsection{Comments of \cite{Cummings2009RelativeMeritsRRAndOR}}

\cite{Cummings2009RelativeMeritsRRAndOR} propose a review of how the OR and the RR differ. In particular, they review typical arguments for pro and cons, while providing examples. In this section, we want to comment how the entanglement model (Lemma~\ref{lemma:intrication_model}) allows to formalize many of their arguments and examples.


\begin{quote}
    \textit{Some authors prefer odds ratios because they believe a constant (homogeneous) odds ratio may be more plausible than a constant Risk Ratio when outcomes are common. Risk range from 0 to 1. Risk Ratios greater than 1 have an upper limit constrained by the risk when not exposed. For example the risk when not exposed is $0.5$, the Risk Ratio when exposed cannot exceed $2: 5\cdot 2 = 1$. In a population with an average Risk Ratio of $2$ for outcome $Y$ among those exposed to $X$, assuming that the risk for $Y$ if not exposed to $X$ varies from .1 to .9, the average Risk Ratio must be less than $2$ for those with risks greater than  $0.5$ when not exposed. Because the average Risk Ratio for the entire population is $2$, the average Risk Ratio must be more than $2$ for those with risks less than $.5$ when not exposed. Therefore, a Risk Ratio of $2$ cannot be constant (homogeneous) for all individuals in a population if risk when not exposed is sometimes greater than $.5$. More generally, if the average Risk Ratio is greater than $1$ in a population, the individual Risk Ratios cannot be constant (homogeneous) for all persons if any of them have risks when not exposed that exceed 1/average Risk Ratio. }
\end{quote}

The authors claim that if $\tau_{\text{\tiny RR}} > 1$, then

\begin{itemize}
    \item The RR has an upper limit linked to the risk of the unexposed ($p_0(x) = b(x)$),
    \item Or, the RR cannot be constant on every individuals if their risk is above a certain threshold being equal to 1/average Risk Ratio.
\end{itemize}

The entanglement model perfectly describes such a situation, and we propose to illustrate why. As authors consider that $\tau_{\text{\tiny RR}} > 1$, then we use Lemma~\ref{lemma:intrication_model} with $\forall x,\,  m_g(x)=0$. More specifically, the authors mention that for $\tau_{\text{\tiny RR}} > 1$ (that we rather model as $\forall x,\,  m_g(x)=0$), it is not possible to have a constant RR on each subgroup. We recall that, 

\begin{align}\label{eq:cumming1}
 \forall x, \,   \tau_{\text{\tiny RR}}(x) =  1 + \frac{1-b(x)}{b(x)}m_b(x)
\end{align}
If $\tau_{\text{\tiny RR}}(x)$ is assumed constant, one can plot the probability $m_b(x)$ as a function of $b(x)$ and observe that indeed this quantity is bounded and/or that $m_b(x)$ can not exist for all baseline $b(x)$. We illustrate this equation on Figure~\ref{fig:cumming1}.


\begin{figure}[H]
    \begin{minipage}{.35\linewidth}
	\caption{\textbf{Illustration of the impossibility of having a constant $\tau_{\text{\tiny RR}}(x) > 1$ if allowing all ranges for baseline risks $p_0(x)$}: This plot illustrates \eqref{eq:cumming1} for several constant values of $\tau_{\text{\tiny RR}}(x)$ (from $1.2$ to $4$), showing how the baseline risk $p_0(x)$ implies different values of $m_b(x)$. If the baseline risk is too high, then there is no plausible $m_b(x)$ (the upper limit is highlighted with the dashed red line). The dark vertical dashed line illustrate the precise example of \cite{Cummings2009RelativeMeritsRRAndOR} with $\tau_{\text{\tiny RR}}(x)=2$.}
	\label{fig:cumming1}
    \end{minipage}%
    \hfill%
    \begin{minipage}{.62\linewidth}
     \includegraphics[width = 0.8\textwidth]{fig/cumming1.png}
    \end{minipage}
\end{figure}


We want to add that, as the treatment effect is assumed to increase the occurence of the event, then a better measure to use (at least if willing to maximise the chance to have a constant value for each individuals as claimed by the author) is the survival ratio. In particular, the Figure~\ref{fig:cumming1} can be adapted when considering a constant SR (see Figure~\ref{fig:cumming2}). One can observe that all ranges of the baseline risks are allowed.

\begin{figure}[H]
    \begin{minipage}{.35\linewidth}
	\caption{\textbf{Illustration of the possibility to have a constant $\tau_{\text{\tiny SR}}(x) < 1$ when allowing all ranges for baseline risks $p_0(x)$}: This plot illustrates how several constant values of $\tau_{\text{\tiny SR}}(x)$ (from $0.2$ to $0.9$) is allowed for any baseline values $p(x)$. Note that this implies a constant $m_b(x)$.}
	\label{fig:cumming2}
    \end{minipage}%
    \hfill%
    \begin{minipage}{.62\linewidth}
     \includegraphics[width = 0.8\textwidth]{fig/cumming2.png}
    \end{minipage}
\end{figure}


Then, authors add the following comment.

\begin{quote}
   \textit{ Odds range from $0$ to infinity. Odds ratios greater than $1$ have no upper limit, regardless of the outcome odds for persons not exposed. If we multiply any unexposed outcome odds by an exposure odds ratio greater than 1 and convert the resulting odds when exposed to a risk, that risk will fall between $0$ and $1$. Thus, it is always hypothetically possible for an odds ratio to be constant for all individuals in a population.}
\end{quote}

We agree that it is always hypothetically possible for an odds ratio to be constant for all individuals (this corresponds to Lemma~\ref{lem:conditional-odds-ratio}, and $m(x)=m$ in the logistic working models). But note that this does not mean that the odds ratio at the individual level is then the same for the population level due to non-collapsibility. 


\begin{quote}
    \textit{\textbf{Possibility of Constancy for Risk Ratios Less Than 1}. For both risk and odds, the lower limit is 0. For any level of risk or odds under no exposure, multiplication by a risk or odds ratio less than 1 will produce a risk or odds given exposure that is possible: 0 to 1 for risks and 0 to infinity for odds. Thus, a constant risk or odds ratio is possible for ratios less than 1. If the Risk Ratio comparing exposed persons with those not exposed is greater than 1, the ratio can be inverted to be less than 1 by comparing persons not exposed with those exposed. Therefore, a constant Risk Ratio less than 1 is hypothetically possible. This argument has been used to rebut the criticism of the Risk Ratio in the previous argument.}
\end{quote}

To us, this argument is a consequence of Lemma~\ref{lemma:monotonous-effect} accounting for the fact that a RR less than $1$ is comparable to $m_b(x) = 0$.



\begin{comment}
    \subsection{Link with model from \cite{Daniel2020MakingApple}}

In \cite{Daniel2020MakingApple} collapsibility is characterized differently, and this appendix aims at binding the two points of view. This follows what was also described in \cite{Neuhaus1993GeometricApproach} to assess bias due to omitted covariates in generalized linear models. Indeed, collapsibility also concerns coefficients of a model. Keeping the notation of the paper, the idea of \cite{Daniel2020MakingApple} is to write the outcome model if treated as a function of the outcome if control. More formally, for a binary outcome, recall that one can have several working model. For example taking the logistic one from Lemma~\ref{lemma:generative-model-binary-Y}, it is possible to note that there exist a function

The odds ratio is non-collapsible, and therefore the marginal effect can not be written as a weighted sum of the conditional effect. If willing to go from conditional effect to marginal effect, other methods should be used, such as fitting a logistic regression with covariates \citep{Daniel2020MakingApple}, which comes with parametric assumptions on the generative model and necessity to observe all covariates affecting the outcome.


\end{comment}




\subsection{Comment on Appendix 3 of \cite{huitfeldt2018choice}}

Many of our insights can be found in \cite{huitfeldt2018choice} (and in particular in their Appendix). %Differences come from the way the model is introduced, along with the dependency in $X_B$ and $X_M$ we highlight in the entanglement model (and with the fact that we also deal with continuous outcomes). In this section, we transpose their example from Appendix 3.
What we want to highlight is that our notations and framework enable another view of the problem. First, we quote the authors.

\begin{quote}
    For illustration, we will consider an example concerning the effect of treatment with antibiotics ($A$), on mortality ($Y$). We will suppose that response to treatment is fully determined by bacterial susceptibility to that antibiotic ($X$). In the following, we will suppose that attribute $X$ has the same prevalence in populations s and t (for example because the two populations share the same bacterial gene pool) and that treatment with $A$ has no effect in the absence of $X$. Further, suppose that this attribute is independent of the baseline risk of the outcome (for example, old people at high risk of death may have the same strains of the bacteria as young people at low risk).
\end{quote}



Within the entanglement model, and denoting $X=0$ the absence of the mutation, this means that:
\begin{itemize}
    \item ``attribute $X$ has the same prevalence in populations s and t" which corresponds to Definition~\ref{def:shidted-covariates};
    \item``treatment with $A$ has no effect in the absence of $X$" $m_b(X=0) = m_g(X=0) = 0$,
    \item ``Further, suppose that this attribute is independent of the baseline risk of the outcome" Here, we think that this assumption could be easily transposed in our intrication model, clearly decomposing $X_B$ and $X_M$.
\end{itemize}




\subsection{Comment on the research work from Cinelli \& Pearl}

The way \cite{CinelliGeneralizing2019} deals with the problem is to encode the assumption of the problem with selection diagrams. In particular selection diagrams are an extension of DAGs with selection nodes, those nodes are used by the
analyst to indicate which local mechanisms are suspected to differ between two environments (in the Russian roulette example, the prevalence risk is suspected to differ between Los Angeles and New York, but not the mechanism).\\

A first difference to our work is that authors rather whant to predict in a target population $\mathbb{E}_{\text{\tiny T}}\left[Y^{(1)} \right]$ from $\mathbb{E}_{\text{\tiny T}}\left[Y^{(0)} \right]$ and $ \text{PS}_{01}$ and $ \text{PS}_{10}$ detailed below, while we focus on causal effects $\tau$. Another difference is that authors mostly reason marginally, while in our work we link subpopulations with larger populations relying on collapsibility.

Cinelli and Pearl introduce the following quantities:


\begin{equation*}
    \text{PS}_{01} := \mathbb{P}\left[ Y^{(1)} = 1 \mid Y^{(0)} = 0 \right],\quad \text{and}\quad   \text{PS}_{10} := \mathbb{P}\left[ Y^{(1)} = 0 \mid Y^{(0)} = 1 \right].
\end{equation*}


Those quantity corresponds to $\mathbb{E}\left[m_b(X)\right]$ and $\mathbb{E}\left[m_g(X)\right]$ defined in Lemma~\ref{lemma:intrication_model}. \modif{In their work, \cite{CinelliGeneralizing2019} assumes that $\mathbb{E}_\text{\tiny T}\left[m_b(X)\right] = \mathbb{E}_\text{\tiny S}\left[m_b(X)\right]$ and $\mathbb{E}_\text{\tiny T}\left[m_g(X)\right] = \mathbb{E}_\text{\tiny S}\left[m_g(X)\right]$. Therefore, their equation,}

\begin{equation*}
    \mathbb{P}^{\Pi^*}\left[ Y^{(1)} = 1\right] = (1-\text{PS}_{10})  \mathbb{P}^{\Pi^*}\left[ Y^{(0)} = 1\right] + \text{PS}_{01}(1-\mathbb{P}^{\Pi^*}\left[ Y^{(0)} = 1\right]),
\end{equation*}

is completely equivalent to the entanglement model. Note that they do consider that $\mathbb{P}^{\Pi^*}\left[ Y^{(0)} = 1\right] $ ( which corresponds to $\mathbb{E}_{\text{\tiny T}}\left[ b(x) \right]$) varies when marginalized in another population. The entanglement model rather highlight the dependencies to covariates (i.e. chracteristics), while their equation rather models the fact that only the baseline risk is necessary to be known if 

\begin{equation*}
    Y^{(1)} \indep I \mid Y^{(0)},
\end{equation*}
where $I$ is the indicator of population's membership and if effect is monotonous (and they denote $Y^{(1)} \le Y^{(0)}$ or conversely depending on the direction assumed).

In our work, such assumption is equivalent with assuming monotonicity (either $m_b(x)=0$ or $m_g(x)=0$) and that all treatment effect modifiers are not shifted. Authors then propose to soften their assumptions deriving bounds on the target quantity $ \mathbb{P}^{\Pi^*}\left[ Y^{(1)} = 1\right] $. Our work rather keeps on targeting causal measure themselves, and  assume that we have access to the shifted covariates of $X_M$. We think this could be stated as,


\begin{equation*}
    Y^{(1)} \indep I \mid Y^{(0)}, X_M,
\end{equation*}
along with the monotonicity assumption.
\modif{Linking selection diagrams assumptions with results from Theorems~\ref{th_onlyRD_separates_baseline_tteffect} and  \ref{th_onlyRD_separates_baseline_tteffect_bounded_outcome} is an open work.}



\section{Details about the simulations}
\label{appendix:additional_simulations}

\subsection{Comments on estimation}\label{appendix:comments-on-estimation}
In this paper, we have been focusing on identification rather than estimation. In this simulation, we illustrate the two approaches that can be taken when transforming identification formula (see Propositions~\ref{proposition:generalization-density} and \ref{prop:generalization-of-local-effects}) into estimation: Plug-in g-formula or Inverse Propensity Sampling Weighting (IPSW).
Existing consistency results of these approaches for the Risk Difference are reviewed in \cite{colnet2021causal}.
We assume that the data sampled from $P_{\text{\tiny S}}$ is a randomized trial $\mathcal{R}$ of size $n$ and the data sampled from $P_{\text{\tiny T}}$ is a cohort $\mathcal{T}$ of size $m$ which contains covariates information $X$ and possibly $Y^{(0)}$.

\subsubsection{Plug-in formula}

When considering \textit{generalization of the conditional outcome}, the plug-in g-formula consists in estimating the two surface responses $\mathbb{E}\left[ Y^{(a)} \mid X\right]$ using the RCT data from $P_{\text{\tiny T}}$. We denote by $\hat \mu_{a, n}(X)$ the estimates ($n$ is added to indicate that estimation is performed on the trial). Any approach can be proposed, for e.g. OLS or non-parametric learners. These models are then used on the target sample to estimate the averaged expected responses,

\begin{equation}\label{estimator-cond-outcome-g-formula}
    \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(a)} \right] = \frac{1}{m} \sum_{i \in \mathcal{T}} \hat \mu_{a,n}(X),
\end{equation}
where $m$ denotes the target sample size. Doing so this estimate depends on the two sample sizes, $n$ and $m$. Finally, $ \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(0)} \right]$ and $ \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(1)} \right]$ are then used to estimate any causal measures on the target population: RD, RR, OR, and so on. Consistency of procedure \eqref{estimator-cond-outcome-g-formula} has been proven for any consistent estimator $\hat \mu_a$ of $\mathbb{E}\left[ Y^{(a)} \mid X\right]$ in \cite{colnet2022sensitivity}.\\

\textit{Generalizing local effects} using a plug-in formula suggests to estimate the local treatment effect (or CATE) $\hat \tau_n(x)$ using $\mathcal{S}$. This can be done using the previously introduced $\hat \mu_{a}(X)$ too (this is called T-learner), and then making a difference or a ratio of the two depending on the causal measure someone wants to generalize. Then, one has to estimate $\hat  g_m(X, P(X,Y^{(0)}))$ using $\mathcal{T}$, for exemple using a linear model (or any other model). Finally, one can obtain the target treatment effect with


\begin{equation}\label{estimtator-local-effect-g-formula}
    \hat \tau = \frac{1}{m}\sum_{i \in \mathcal{T}} \hat g_m(X_i, P(X_i,Y_i^{(0)})) \hat \tau_n(X_i),
\end{equation}

where $m$ denotes the target sample size. Note that \eqref{estimtator-local-effect-g-formula} relies on the estimation of $\tau(X)$ directly.
While the estimation of the conditional risk difference is well described in the literature \citep{wager2018estimation, nie2020quasioracle} (to name a few), estimation of conditional ratios is way less described. We have found only one recent work dealing with such questions \citep{yadlowsky2021estimation}. Consistency of such procedure for another metric than the Risk Difference is an open research question.


\subsubsection{Inverse Propensity Sampling Weighting (IPSW)}
IPSW uses the ratio of densities to re-weight individual observation in the trial. Denoting $r(X):=\frac{p_{\text{\tiny T}}(X)}{p_{\text{\tiny S}}(X)}$ the density ratio, one has first to learn this ratio $\hat r_{n,m}(X)$ using both data set $\mathcal{S}$ and $\mathcal{T}$. 
One can \textit{generalize conditional outcomes} doing:

\begin{equation*}
    \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(a)} \right] = \frac{1}{n} \sum_{i \in \mathcal{S}} \hat{r}_{n,m}(X_i)A_iY_i.
\end{equation*}
Those estimates ($\hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(0)} \right]$ and $\hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(a1} \right]$) are then used to estimate any causal measures on the target population. 



Now, considering \textit{generalizing local effects} using a re-weighing approach rather suggest to also estimate $\hat  g_m(X, P(X,Y^{(0)}))$ using $\mathcal{T}$ (for example using a linear model). Then, for e.g when considering the Risk Difference, this consists in doing

\begin{equation*}
\hat \tau_{\text{\tiny RD}} = \frac{1}{n} \sum_{i \in \mathcal{S}}\hat{r}(X_i) \left( A_iY_i - (1-A_i)Y_i\right),
\end{equation*}

or when considering the Risk Ratio, a procedure could be

\begin{equation*}
\operatorname{ln}\left(\hat  \tau_{\text{\tiny RR}} \right) =  \frac{1}{n} \sum_{i \in \mathcal{S}}\hat{r}(X_i) \left( \operatorname{ln}\left(A_iY_i\right) - \operatorname{ln}\left((1-A_i)Y_i\right) \right) \hat  w_m(X_i, P(X_i,Y_i^{(0)})).
\end{equation*}
 We use these weighting approaches for the simulation with a binary outcomes. As the purpose is not estimation, we propose a simulation with categorical covariates only, in particular to propose an estimation of $\hat r_{n,m}(X)$ as in \cite{colnet2022reweighting}. $\hat  w_m(X, P(X,Y^{(0)}))$ is estimating by computing the empirical mean of $\mathbb{E}\left[ Y^{(0)} \mid X\right]$ in each category. %Note that if covariates are categorical, \cite{colnet2022reweighting} proposes a finite and large sample characterization of IPSW when it comes to risk difference. Other works have been proposed.




\subsection{Continuous outcomes}\label{appendix:continuous}

\subsubsection{Data generative process}\label{appendix:continuous-generative-model}
We assume that the outcome is generated linearly from six covariates in the two populations
\begin{equation}
\label{eq:Ymodel-simulation-continuous}
    Y(a) = 0.05 X_1 +  0.04 X_2 + 2 X_3 + X_4 + 2 X_5 - 2 X_6 + a\cdot \left(1.5 X_1 + 2 X_2 + X_5 \right) +\epsilon \mbox{ with } \epsilon \sim \mathcal{N}(0,2).
\end{equation}

The two data samples are directly sampled from two different baseline distributions.

Covariates $X_1, X_2, X_3$ are generated from 

\begin{equation*}
    \mathcal{N}\left(\left[\begin{array}{l}
6 \\
5 \\
8
\end{array}\right],\left[\begin{array}{lll}
1 & 0  & 0.5 \\
0 & 1 & 0.2 \\
0.5 & 0.2 & 1 
\end{array}\right]\right)
\end{equation*}

in $P_\text{\tiny S}$, and in 


\begin{equation*}
    \mathcal{N}\left(\left[\begin{array}{l}
15 \\
7 \\
10
\end{array}\right],\left[\begin{array}{lll}
1 & 0  & 0.5 \\
0 & 1 & 0.2 \\
0.5 & 0.2 & 1 
\end{array}\right]\right)
\end{equation*}

for $P_\text{\tiny T}$. $X_4$ is such that $X_4 \sim \mathcal{B}(1, 0.8)$ in $P_\text{\tiny S}$ and $X_4 \sim \mathcal{B}(1, 0.3)$ in $P_\text{\tiny T}$. Then, $X_5$ and $X_6$ are non-shifted covariates, where $X_5 \sim \mathcal{B}(1, 0.8)$ and $X_6 \sim \mathcal{N}(4, 1)$ in both populations.

Within the trial sample of size $n$ we generate the treatment according to a Bernoulli distribution with probability equals to $0.5$.

\paragraph{Estimation}\label{appendix:continuous-estimation-steps}
For this simulation we applied a plug-in g-formula approach, using Ordinary Least Squares (OLS) to estimate $\hat \mu_{a,n}$ and $\hat  g_m(X, P(X,Y^{(0)}))$. $\hat \tau_n$ is estimated combining $\hat \mu_{a,n}$ as a difference or ratio or else (T-learner). \modif{More precisely, in this simulation the different steps when generalizing the conditional outcomes are the following : 
\begin{itemize}
    \item Fit an OLS estimator on the subset of treated individuals ($A=1$) in the trial sample to obtain $\hat \mu_{1,n}(X)$,
    \item Fit an OLS estimator on the subset of control individuals ($A=0$) in the trial sample to obtain $\hat \mu_{0,n}(X)$,
    \item Estimate the expected outcome if treated and control on the target population using the following formulae 
    $$\hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(a)} \right] = \frac{1}{m} \sum_{i \in \mathcal{T}} \hat \mu_{a,n}(X), $$
    \item Use the two previous quantities to estimate 
    \begin{itemize}
        \item The risk difference $\hat \tau_{\text{\tiny RD}} = \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(1)} \right]-\hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(0)} \right]$,
        \item The Risk Ratio $\hat \tau_{\text{\tiny RR}} = \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(1)} \right]/\hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(0)} \right]$,
        \item The excess Risk Ratio $\hat \tau_{\text{\tiny ERR}} = \left( \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(1)} \right]-\hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(0)} \right]\right) /\hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(0)} \right]$.
    \end{itemize}
\end{itemize}



To generalize the local effects, we perform the following list of steps :

\begin{itemize}
    \item Rely on the first two steps performed to generalize the conditional outcomes, namely fit an OLS estimator on the trial sample to obtain $\hat \mu_{1,n}(X)$ and $\hat \mu_{0,n}(X)$,
    \item The risk difference is estimated with the following formula\footnote{Note that by linearity we retrieve that for the risk difference and for a continuous outcome, generalizing conditional outcomes or local effects is strictly equivalent.} $$ \hat \tau_{\text{\tiny RD}}  = \frac{1}{m} \sum_{i \in \mathcal{T}} \hat \mu_{1,n}(X) - \hat \mu_{0,n}(X),$$
    \item The Risk Ratio is obtained by
    \begin{itemize}
        \item Fitting an OLS estimator on the target sample to obtain $\hat \mu_{0,m}(X)$,
        \item To finally compute 
        $$\hat \tau_{\text{\tiny RR}} = \frac{1}{m} \sum_{i \in \mathcal{T}} \frac{\hat \mu_{1,n}(X_i)}{\hat \mu_{0,n}(X_i)} \underbrace{\frac{\hat \mu_{0,m}(X_i)}{\frac{1}{m} \sum_{j \in \mathcal{T}} \hat \mu_{0,m}(X_j)}}_\text{weights estimated on $\mathcal{T}$}.$$
    \end{itemize}
\end{itemize}


}

\modif{
\paragraph{What if a shifted treatment effect modifier is missing?} This situation leads to a biased estimate \citep{nguyen2018sensitivitybis, colnet2022sensitivity}. To illustrate such situation we replicated simulations presented in Figure~\ref{fig:simulations-continuous-Y} but without covariate $X_1$. Results are presented on Figure~\ref{fig:simulations-continuous-Y-missing-X1}}


 \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig/simulations-continuous-Y-missing-X1.png}
    \caption{\textbf{Results of the simulations for a continuous outcomes without observing $X_1$}: where the generative model corresponds to \eqref{eq:simulation-continuous-generative-model}. Column 1 corresponds to generalizing conditional outcome, column 2 corresponds to generalizing local effect with the proper collapsibility weights. For these two approaches we use different covariates set, with $X_2$, $X_{2 \dots 4}$, and $X_{2 \dots 6}$. According to Theorem~\ref{theorem:all-covariates} and \ref{theorem:restricted-set-for-Y-continuous-RD}, the target treatment effect can not be identified when a shifted treatment effect modifier is unobserved. Simulations are performed following the exact same procedure than Figure~\ref{fig:simulations-continuous-Y}, with $1000$ repetitions, a source sample size of $500$ and target sample size of $1,000$.}
    \label{fig:simulations-continuous-Y-missing-X1}
\end{figure}


\modif{
\paragraph{What if mispecification occurs?} This situation leads to a biased estimate. To illustrate such situation we introduced a different generative model for the outcome such that eq.~\ref{eq:Ymodel-simulation-continuous} becomes

\begin{equation}
\label{eq:Ymodel-simulation-continuous-mispe}
    Y(a) = 0.05 X_1^2 +  0.04 X_2 + 2 X_3 + X_4 + 2 X_5 - 2 X_6 + a\cdot \left(1.5 X_1^2 + 2 X_2 + X_5 \right) +\epsilon \mbox{ with } \epsilon \sim \mathcal{N}(0,2).
\end{equation}

Then, and using a simple OLS estimator without squared terms such as described in Section~\ref{appendix:continuous-estimation-steps} and presented in Figure~\ref{fig:simulations-continuous-Y}, leads to a biased estimate in all situations (generalizing local effects or conditional outcomes, and with any of the covariates subsets). Results are presented on Figure~\ref{fig:simulations-continuous-Y-mispe}.
}


 \begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{fig/simulations-continuous-Y-mispecified.png}
    \caption{\textbf{Results of the simulations for a continuous outcomes with mispecification}: where the generative model corresponds to \eqref{eq:Ymodel-simulation-continuous-mispe} rather than \eqref{eq:Ymodel-simulation-continuous}. Column 1 corresponds to generalizing conditional outcome, column 2 corresponds to generalizing local effect with the proper collapsibility weights. For these two approaches we use different covariates set, with $X_2$, $X_{2 \dots 4}$, and $X_{2 \dots 6}$. According to Theorem~\ref{theorem:all-covariates} and \ref{theorem:restricted-set-for-Y-continuous-RD}, the target treatment effect can not be identified when a shifted treatment effect modifier is unobserved. Simulations are performed following the exact same procedure than Figure~\ref{fig:simulations-continuous-Y}, with $1000$ repetitions, a source sample size of $500$ and target sample size of $1,000$. Estimation is performed with plug-in g-formula modeling all responses with an OLS approach as detailed in Section~\ref{appendix:continuous-estimation-steps}.}
    \label{fig:simulations-continuous-Y-mispe}
\end{figure}



\subsection{Binary outcomes}\label{appendix:simulation-binary}

\subsubsection{Data generative process}\label{appendix:simulation-binary-data-generative-model}
For this simulation the covariates are categorical to ease the estimation strategy\modif{, and as the purpose of this work is not on estimation}. The data generative model is build on top of \eqref{eq:intuition-of-intrication}, and adapted to give,
\begin{equation*}
    \mathbb{P}\left[ Y^{(a)} = 1 \mid X = x \right] = b(X_1, X_2, X_3) + a\,\left(1-b\left(X_1, X_2, X_3\right)\right)\,m_b(X_2, X_3),
\end{equation*}
where $X_1=\texttt{lifestyle}$, $X_2=\texttt{stress}$, and $X_3=\texttt{gender}$.

Each of the three covariates are sampled following a Bernoulli distribution. 
In $P_\text{\tiny S}$, one has $X_1 \sim \mathcal{B}(1, 0.4)$, $X_2 \sim \mathcal{B}(1, 0.8)$, and $X_3 \sim \mathcal{B}(1, 0.5)$. In $P_\text{\tiny T}$, one has $X_1 \sim \mathcal{B}(1, 0.6)$, $X_2 \sim \mathcal{B}(1, 0.2)$, and $X_3 \sim \mathcal{B}(1, 0.5)$.

The outcome is defined such as,

  \begin{equation*}
    b(X) =  \operatorname{ifelse}(X_1 = 1, 0.2, 0.05) \cdot  \operatorname{ifelse}(X_2  = 1, 2, 1) \cdot  \operatorname{ifelse}(X_3 = 1, 0.5, 1), 
  \end{equation*}

where $ \operatorname{ifelse}$ corresponds to the function with the same name in \texttt{R}. And,

  \begin{equation*}
    m_b(X) = \operatorname{ifelse}(X_2 = 1, 1/4, \operatorname{ifelse}(X_3 = 1, 1/10, 1/6)).
  \end{equation*}

Within the trial sample of size $n$ we generate the treatment according to a Bernoulli distribution with probability equals to $0.5$. 

\subsubsection{Estimation}\label{appendix:simulation-binary-estimation-steps}

First we estimate $\mu_{a}(.)$ on the trial sample. As covariates are categorical this corresponds to computing average values of $Y$ in each bin, namely 


\begin{equation*}
   \forall x \in \mathcal{X},\;\; \hat \mu_{1,n} (x):=  \frac{\sum_{i \in \mathcal{S}; X_i = x} Y_i A_i}{\sum_{i \in \mathcal{S}} \mathbbm{1}_{A_i = 1} \mathbbm{1}_{X_i = x}}\;\; \text{ and,    }\,\hat \mu_{0,n} (x):=  \frac{\sum_{i \in \mathcal{S}; X_i = x} Y_i (1-A_i)}{\sum_{i \in \mathcal{S}} \mathbbm{1}_{A_i = 0} \mathbbm{1}_{X_i = x}}.
\end{equation*}

\modif{This allows to estimate $ \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(0)} \right]$ and $ \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(1)} \right] $ with 

\begin{equation*}
    \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(1)} \right] = \frac{\sum_{i \in \mathcal{T}} \hat \mu_{1,n}(X_i)}{m}  \, \text{ and,    }\,  \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(0)} \right] = \frac{\sum_{i \in \mathcal{T}} \hat \mu_{0,n}(X_i)}{m} .
\end{equation*}
}

\modif{Then, these two quantities are used to estimate :
\begin{itemize}
    \item The risk difference $\hat \tau_{\text{\tiny RD}} = \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(1)} \right] - \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(0)} \right] $,

    \item The number needed to treat $\hat \tau_{\text{\tiny NNT}} = \tau_{\text{\tiny RD}}^{-1}$,

    \item The Risk Ratio $\hat \tau_{\text{\tiny RR}} = \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(1)} \right] / \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(0)} \right] $,


     \item The survival ratio $\hat \tau_{\text{\tiny SR}} = \left( 1-\hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(1)} \right]\right) / \left( 1- \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(0)} \right] \right) $,

\item The odds ratio $\hat \tau_{\text{\tiny OR}} =  \left( \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(1)} \right] /  \left( 1- \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(1)} \right] \right)\right)  \cdot \left( \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(0)} \right] /  \left( 1- \hat{\mathbb{E}}_{\text{\tiny T}}\left[ Y^{(0)} \right] \right)\right)^{-1} $.


     
\end{itemize}


This procedure corresponds to the generalization of  the outcome.
}


\modif{On the other hand, for each categories in the trial sample, each causal measure $\tau$ is estimated on each strata to obtain $\tau(x)$, using the fitted outcome models $\hat \mu_{1,n}(.)$ and $\hat \mu_{0,n}(.)$. This corresponds to local effects. For example for the Risk Ratio, one has :

\begin{equation*}
    \forall x \in \mathcal{X},\, \hat \tau_{\text{\tiny RR},n}(x) :=  \frac{\hat  \mu_{1,n}(X_i)}{\hat \mu_{0,n}(X_i)}.
\end{equation*}


Then collapsibility weights are estimated as followed :

\begin{enumerate}
    \item Estimate the outcome model on the target population 

$$ \hat \mu_{0,m} (x):=  \frac{\sum_{i \in \mathcal{T}; X_i = x} Y_i }{\sum_{i \in \mathcal{S}} \mathbbm{1}_{X_i = x}}.$$

\item So that 

$$\hat w_{\text{\tiny T},m}(x) :=  \frac{\hat \mu_{0,m} (x)}{\frac{\sum_{i \in \mathcal{T}}Y_i}{m}}. $$
    
\end{enumerate}


Finally and for each causal measure $\tau$, the target effect is obtained computing :

\begin{equation*}
  \hat \tau_{\text{\tiny T},n,m} :=  \frac{1}{m}\sum_{i \in \mathcal{T}} \hat w_{\text{\tiny T},m}(X_i)  \hat \tau_n(X_i).
\end{equation*}

}

\begin{comment}

\subsection{Another motivating example: the pill scare of the 2000's}\label{appendix:pill-scare-simulation}
In the early 90's the rise of genetic methods leaded to the identification of several genetic mutations associated to a higher prevalence of certain diseases. 
A well-known example is the mutation called \textit{Factor V Leiden} that was associated to the resistance to a certain protein (named APC resistance), leading to an enhanced susceptibility to thrombosis (a disease that can be followed by death). 
This APC resistance was found to be rather common, for e.g. a prevalence 3 to 5 \% was found in the general Netherlands population \citep{bertina1994mutation}, but population's proportions varies across the world and can be almost absent in some populations but can reach 12\% \citep{Rees1995WorldLeiden}.
This enhanced risk of thrombosis was shown to be even greater within young women using oral contraceptives (case-control study by \cite{Vandenbroucke1994thrombosis}).
This discovery leaded to one of the most prescribed genetic test called \textit{Non-rare thrombophilia} (NRT) \citep{Bourgain2021Appraising}, even if initial investigators did not support universal screening before oral contraceptives prescription \citep{vandenbroucke1996factor}. The reasons invoked where due to the low prevalence of the mutation along with low susceptibility to thrombosis even as a carrier of the mutation. But the RR exposed in \cite{Vandenbroucke1994thrombosis} are pretty alarming, with $\tau_{\text{\tiny RR}} = 5$ for women taking oral contraceptives and carrier of the mutation.\\
Our results rather advocatethe use of $\tau_{\text{\tiny SR}}$ for those typology of outcomes and effect. This measure was not computed at the time, and we propose to shed light on this paper with other measures than the $\tau_{\text{\tiny RR}}$ initially reported\footnote{The original paper is a case-control study \citep{Vandenbroucke1994thrombosis}. Usually absolute risks and other quantities cannot be computed in a case-control study due to the lack of knowledge of the population incidence fraction, even if modern methods have been proposed to circumvent the problem \citep{king2002estimating}. Looking back to the original publication, authors indeed computed a OR which was reported as an RR. The low prevalence of thrombosis within young women, ($2.1$ for $10,000$ per year) enabling the low prevalence assumption $\tau_{\text{\tiny OR}} \sim \tau_{\text{\tiny RR}}$. Luckily, authors also computed the population's incidence thrombosis with all the combinations (oral contraceptive and/or factor V mutation), which enables us to re-compute other measures. In this illustration we do not provide confidence intervals as we aim at illustrating how the measure's choice leads to amplitude differences.}.


\begin{table}[]
\begin{center}
\begin{tabular}{c|cc|cc|}
\cline{2-5}
                            & \multicolumn{2}{c|}{X = 1} & \multicolumn{2}{c|}{X = 0} \\ \cline{2-5} 
                            & Y = 1       & Y = 0        & Y = 1       & Y = 0        \\ \hline
\multicolumn{1}{|c|}{A = 1} & 25          & 8,732        & 84          & 275,774      \\ \cline{1-1}
\multicolumn{1}{|c|}{A = 0} & 10          & 17,505       & 36          & 437,834      \\ \hline
\end{tabular}
\end{center}
\end{table}



Follow-up works rather claimed that the actual absolute risk was rather low, and that the widespread use of the NRT rather leaded to what is called a \textit{pill scare}, i.e. women stopping oral contraception \citep{Mills1999PillScare}. 
This example of clinical discoveries can not be reduced to the way the treatment effect is reported. It also involves socio-culturel consequences \citep{Bourgain2021Appraising}, or cost-benefit analysis related to the generalization of tests \citep{Creinin1999Cost}.


We show that - still being non-parametric - one can propose heterogeneity. 


\end{comment}


\newpage

\begin{landscape}

\bgroup
\def\arraystretch{2}%  1 is the default, change whatever you need

 \begin{table}
 \begin{center}
 {\scriptsize
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline

 \multicolumn{1}{|c|}{\textbf{Name}} &  \multicolumn{1}{|c|}{\textbf{Outcome type}} & \multicolumn{1}{c|}{\textbf{Definition}} & \multicolumn{1}{c|}{\textbf{Collapsibility}} &  \multicolumn{1}{c|}{\textbf{Logic respecting}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Invariant\\ to encoding\end{tabular}}} & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Covariate set\\ for generalization\end{tabular}}} \\ \hline \hline
 Risk Difference  (RD)       & Continuous           &     $\tau_{\text{\tiny RD}} := \mathbb{E}\left[Y^{(1)}\right] - \mathbb{E}\left[Y^{(0)}\right]$        &                                                       Directly collapsible  & Logic-respecting                      &  Not applicable      &  $X_{M \cap \textrm{Sh}}$                                                                                                    \\ \hline
Risk Ratio (RR)            & Continuous                 &       $\tau_{\text{\tiny RR}} := \mathbb{E}\left[Y^{(1)}\right] / \mathbb{E}\left[Y^{(0)}\right]$                                                                   & Collapsible         & Logic-respecting                      &   Not applicable                   &  $X_{(M \cup B) \cap \textrm{Sh}}$                                                                                                   \\ \hline
 Excess Risk Ratio (ERR)       & Continuous               &    $\tau_{\text{\tiny ERR}}  := \tau_{\text{\tiny RD}}/\mathbb{E}\left[Y^{(0)}\right] = \tau_{\text{\tiny RR}} - 1 $                                                                    & Collapsible     & Logic-respecting                         &        Not applicable                & $X_{(M \cup B) \cap \textrm{Sh}}$                                                                                                  \\ \hline \hline  \hline 
Risk Difference (RD)       & Binary                 &               $\tau_{\text{\tiny RD}} := \mathbb{P}\left[Y^{(1)} = 1\right] - \mathbb{P}\left[Y^{(0)} = 1\right]$                                            &  Directly collapsible    & Logic-respecting                       & Multiplied by $-1$ &        $X_{(M \cup B) \cap \textrm{Sh}}$                                                                                                                                     \\ \hline
 Number Needed to Treat (NNT)    & Binary         &  $\tau_{\text{\tiny RD}} := 1 / \left( \mathbb{P}\left[Y^{(1)} = 1\right] - \mathbb{P}\left[Y^{(0)} = 1\right] \right)$   & \textbf{Not} collapsible  & Logic-respecting                   &  Multiplied by $-1$    &     $X_{(M \cup B) \cap \textrm{Sh}}$                                                                                                                                         \\ \hline
Risk Ratio (RR)        & Binary              &  $\tau_{\text{\tiny RR}} := \mathbb{P}\left[Y^{(1)} = 1\right] / \mathbb{P}\left[Y^{(0)} = 1\right]$   &      Collapsible   & Logic-respecting                    &   $= \tau_{\text{\tiny SR}}$    &     If $m_b(x)=0$, $X_{M \cap \textrm{Sh}}$                                                                                             \\ \hline
Survival Ratio (SR)      & Binary              &   $\tau_{\text{\tiny SR}} := \mathbb{P}\left[Y^{(1)} = 0\right] / \mathbb{P}\left[Y^{(0)} = 0\right]$  &  Collapsible   & Logic-respecting                   &      $= \tau_{\text{\tiny RR}}$      &                If $m_g(x)=0$, $X_{M \cap \textrm{Sh}}$                                                                                                                                \\ \hline
Excess Risk Ratio (ERR)    & Binary                 &    $\tau_{\text{\tiny ERR}}  := \tau_{\text{\tiny RD}}/\mathbb{P}\left[Y^{(0)} = 1\right] = \tau_{\text{\tiny RR}} - 1 $                                                                    & Collapsible     & Logic-respecting                        &        $= \tau_{\text{\tiny SR}} - 1$                   & If $m_b(x)=0$, $X_{M \cap \textrm{Sh}}$                                                                                                    \\ \hline 
Relative Susceptibility (RS)      & Binary             &    $\tau_{\text{\tiny RS}}  := \tau_{\text{\tiny RD}}/\mathbb{P}\left[Y^{(0)} = 0\right] = 1 - \tau_{\text{\tiny SR}}$                                                                    & Collapsible     & Logic-respecting                        &        $= 1-\tau_{\text{\tiny RR}}$                   & If $m_g(x)=0$, $X_{M \cap \textrm{Sh}}$                                                                                                     \\ \hline 
Odds Ratio (OR)  & Binary   &$\tau_{\text{\tiny OR}}  := \frac{\mathbb{P}[Y^{(1)} = 1]}{\mathbb{P}[Y^{(1)} = 0]}\, \left(  \frac{\mathbb{P}[Y^{(0)} = 1]}{\mathbb{P}[Y^{(0)} = 0]}\right)^{-1}  = \tau_{\text{\tiny RR}} \cdot \tau_{\text{\tiny SR}}^{-1}$&  \textbf{Not} collapsible   & \textbf{Not} logic-respecting                  &  Reciprocal    &     $X_{(M \cup B) \cap \textrm{Sh}}$                                                                                                                                   \\ \hline
Log Odds Ratio (log-OR)     & Binary           & $\tau_{\text{\tiny log-OR}}  := \operatorname{log}\left( \frac{\mathbb{P}[Y^{(1)} = 1]}{\mathbb{P}[Y^{(1)} = 0]}\right) - \operatorname{log}\left( \frac{\mathbb{P}[Y^{(0)} = 1]}{\mathbb{P}[Y^{(0)} = 0]}\right)$ &  \textbf{Not} collapsible   & \textbf{Not} logic-respecting                   &  Multiplied by $-1$    &  $X_{(M \cup B) \cap \textrm{Sh}}$                                                                                                                                                                                                                                   \\ \hline
\end{tabular}}
\caption{\textbf{Typical causal measures reported in clinical practice}: The upper part of the Table mentions the three typical measures found when the outcome is ordinal or continuous, and the lower part mentions measures for binary outcomes. For each measure we provide the explicit formulae, and propoerties such as collapsibility (see Definitions~\ref{def:direct-collapsibility} and \ref{def:indirect-collapsibility}), invariance to encoding (also called symetry in the literature), and whether the covariate set for generalization by standardization is extended or not. All these properties are defined in this article, and prooved for each of the measures.}
\label{tab:list-measures-with-all-properties}
\end{center}
\end{table}   


\egroup
\end{landscape}
\end{document}
