% !TEX root=main.tex
\section{Proposed Method}
\label{sec:proposal}

This section provides details on how to transform any detector such that it can efficiently process high-resolution images with many small target objects, leveraging density crops. Our proposed cascaded zoom-in (CZ) object detector allows to re-purpose the detector to extract density crops along with base class objects.  Let us consider the \emph{original image}, which is kept at its high resolution, the \emph{down-sampled image}, which is an image containing the same view of the original, but down-scaled to the detector input resolution, and the \emph{cropped images}, which are the selected regions of the image that are up-scaled to the detector resolution. The first component of our pipeline is the density crop labeling algorithm that labels the crowded object regions as "density crops" and augments the training data by adding up-scaled versions of those regions. Then, the density crops are also added to the original image as a new class to be detected. Thus, the augmented training set will include high-resolution versions of the regions containing crowded small objects, allowing the detector to see those small objects in high resolution. Once the model is trained, the inference consists of the detection of the base classes and "density crop" class on the original image, and then a second detection on the predicted density crops after up-scaling. Finally, the detection from the original image and density crops are combined to produce the output detection.


%%%%%%%%
%\subsection{Annotations}
%\subsection{Density Crop Labeling Algorithm}
\subsection{Training with Density Crops}
In order to use a standard detector for our approach, we need to add a new class that we call "density crop" to the training annotations. In this way, our approach is detector agnostic(as we don't change the internals of the detector, we just add one additional class to the list of target classes) and does not require any additional component than the detector itself. The density crop class should label those parts of the image that contains many small objects and include them in a bounding box. This will allow training and inference to focus on those parts by analyzing them in higher resolution. Several different ways could be considered for defining the density crop. The quality constraints we used to define density crops are: (i) they should enclose groups of small target objects,  (ii) they are easy to localize at inference time, and (iii) they are optimal in number to reduce the computational cost.
 

%Two parameters should be tuned
%\subsection{Density Crop Labeling Algorithm}
Algorithm \ref{alg:crop_discovery} describes the procedure we used for discovering and labeling density crops from the groundtruth(GT) annotations. In summary, we perform an iterative merging of the GT boxes to discover the density crops. In the first step, all GT boxes $\mathcal{B}$ are scaled by expanding the min and max coordinates of the boxes by  $\sigma$ pixels (\verb|scale|($\mathcal{B},\sigma$)). Then we calculate the pairwise Intersection over Union (IoU) between the scaled boxes (\verb|pairwise_IoU|($\mathcal{D}$)) in $O$ as a $|\mathcal{D}| \times|\mathcal{D}|$ matrix. Connections are labeled in $C$ by assigning one to all overlap values above a threshold $\theta$ in the pairwise IoU matrix $O$. Then we select in $C$ the row $m^*$ with the maximum number of connections. %and find all of its connected boxes to form the crop members. 
An enclosing box is computed (\verb|enclosing_box|($C_{m^{*}}$)) by finding the min and max coordinates of all crop members connected to $m^*$. The newly obtained crop box is added to the list of crops and the row $C_{m^*}$ is set to zeros. Subsequently, the crops that are bigger than a maximum threshold $\pi$ are removed from the list $\mathcal{D}$ (\verb|filter_size|($\mathcal{D},\pi$)). This procedure of iterative merging is performed $N$ times. The crop size threshold $\pi$ used here is the ratio of  the area of the crop to that of the image.

%(we used $N=2$ by visually inspecting the quality of the crops; more details are provided in the supplementary material).

% consistent signal as a crop, unlike other methods.

\begin{comment}
%old verison
\begin{algorithm}[t]
    \KwIn{GT boxes in an image: $(\mathcal{B})$}
    \Parameter{$N$: no. of merging steps, $\theta$: overlap threshold}
    \KwOut{Crop boxes}
    1. $\mathcal{B}$'  $\leftarrow$ scale($\mathcal{B}$,{\color{red}S})\;
    2. \For{$i\gets1$ \KwTo $N$ }{
        a) overlaps = pairwise\_IOU($\mathcal{B}$')\\
        b) connections = (overlaps $>$ {\color{red} $\theta$})\\
        c) max\_connected = $\argmax$ rowsum (connections)\\
        d) crop\_members = connections[max\_connected]\\
        e) crop\_box = enclosing\_box(crop\_members)\\
        f) prune crop\_members and update connections\\
        g) $\mathcal{B}' \leftarrow$ filter\-size$(\mathcal{B}',{\color{red}P})$
    }
    \caption{Density Crop Labeling Algorithm.}
\label{alg:cluster_discovery}
\end{algorithm}
\end{comment}

\begin{algorithm}[t]
    \KwIn{$\mathcal{B}$: GT boxes in an image}
    \KwOut{$\mathcal{D}$: Density crops}
    \Parameter{$N$: no. of merging steps, \\${\sigma}$: expansion pixels, \\${\theta}$: overlap threshold, \\${\pi}$: maximum crop size}
    1. $\mathcal{D}  \leftarrow$ {\texttt{scale}}($\mathcal{B},{\sigma}$)\;
    2. \For{$i\gets1$ \KwTo $N$ }{
        %a) $O$ = pairwise\_IOU($\mathcal{B}^*$) \#overlap matrix \\%($|\mathcal{C}| \times |\mathcal{C}|$)\\
        %b) $C$ = $O$ $>$ {\color{red} $\theta$} \ \ \#connection matrix \\%$(K \times K)$ \\
        %a) $O$ = pairwise\_connection($\mathcal{B}^*,\theta$) \\%\#connection matrix \\%
        a) $O$ = {\texttt{pairwise\_IoU}}($\mathcal{D}$)\\
        b) $C = O > \theta$ \\
        c) $\mathcal{D} \leftarrow \emptyset$\\ 
        \While{$|C|>0$}{
        i) $m^* = \argmax_{m} \left( \sum_i C_{m,i} \right)$ \\ %\#max connections \\
        ii) $d$ = {\texttt{enclosing\_box}}($C_{m^*}$) \\%\#new density crop \\        
        %iii) $\mathcal{B}^* \leftarrow$ $\mathcal{B}^*-link(C_m)$\\
        %iv) $C \leftarrow$ $C - link(C_m)$\\
        iii) $\mathcal{D} \leftarrow$ $\mathcal{D}+ d $ \\
        iv) $C_{m^*}$ = 0 \\%\vec{0}$\\
        }
        d) $\mathcal{D} \leftarrow$ {\texttt{filter\_size}}$(\mathcal{D},\pi)$\\
    }
    \caption{Density Crop Labeling Algorithm.}
\label{alg:crop_discovery}
\end{algorithm}

The quality of the crops is important for our method. It is in fact the iterative merging that brings out the best quality crops. Naive scaling and merging to find the maximum enclosing boxes based on pairwise IoU results in either bad crops or too many small crops (with fewer objects in them) depending on the value of the scaling factor. Iterative merging produces good-quality crops enclosing groups of small objects respecting the quality constraints. In section \ref{sec:experiments}, we present the ablation studies validating the effectiveness of our density crop labeling algorithm. We also show that hyperparameters of the algorithm can b easily set. Note that by labeling the density crops apriori,  we are giving the detector a consistent signal of what constitutes a "density crop" throughout the training period, unlike other methods where density crops are also evolving while the training is progressing\cite{clusnet-Yang-2019, dmap-Li-2020, dmap-Duan-2021}. This is consistent with the observations in \cite{detic-zhou-2022} that simple heuristic methods that can give a consistent groundtruth label during training are superior to complex prediction-based groundtruth labels that are continuously evolving.

%\subsection{Training}
With the newly obtained crop labels, we can also augment the training set with additional image crops. The original image and its annotations $\mathcal{B}$ are down-scaled using the maximum training resolution $W \times H$. Note that it is expected the detector will not detect many small objects in the down-scaled image. But the augmented up-scaled version of the density crop $d \in \mathcal{D}$ of a given image will have those small objects that fall inside the crop in a higher resolution. This will reduce the extreme scale variation at training time. The crop labeling can be performed as a pre-processing step. The up-scaled version  augmentation of density crops is simply a data augmentation process. Thus our method is not introducing any change in the standard training pipeline of a detector, except the new class "density crop" is added. In this regard, it is practically easy to use like uniform cropping.

%%%%
\subsection{Multi-stage Inference}
As the detector is trained to recognize density crops, at inference, we can get the density crop from its prediction itself. Figure~\ref{fig:base_daigram} bottom explains our inference process in detail. It consists of two stages. In stage one, it predicts the base class objects and density crops on the input image. Then we select the high-quality density crops based on their confidence score. In stage two, the upscaled density crops are passed through the same detector again, producing small object detection on the density crops. Finally, we re-project the detections on the crops to the original image and concatenate them with the detections on the original image. 
Let $c \in \mathcal{C}$ be an up-scaled crop image of size $(I^W_c,I^H_c)$ defined by its bounding box coordinates $(c_{x1}, c_{y1}, c_{x2}, c_{y2})$ in the original image. Given the scaling factors $(S^W_c,S^H_c)=(\frac{c_{x2}-c_{x1}}{I^W_c},\frac{c_{y2}-c_{y1}}{I^H_c})$, the re-projection box $p_i$ scales down and shifts the detection boxes  $(x_{1,i},y_{1,i},x_{2,i},y_{2,i}) \in \mathcal{B}^c$ in the crop $c$ as:
\begin{align}
    %& p_i = \frac{y^c_2-y^c_1}{I_H}b_i^c + (x^c_1, y^c_1, x^c_1, y^c_1) 
    p_i = & ({S^W}x_{1,i},{S^H}y_{1,i},{S^W}x_{2,i},{S^H}y_{2,i}) \notag \\
    & + (c_{x1}, c_{y1}, c_{x1}, c_{y1}) 
    %& with \hspace{3pt} b_i^c \in \mathcal{B}^c.
    \label{equ:reproj}
\end{align}
\begin{comment}
Let $c \in \mathcal{C}$ be an up-scaled crop with height $c^U_H$, whose height  and position in the input image $I$ are $c^I_H$ and $[x^c_1, y^c_1, x^c_2, y^c_2]$ respectively. The re-projection operation simply scales down and shifts the detection boxes $\mathcal{B}^c$ in $c$ as shown in equation \ref{equ:reproj}:
\begin{equation}
    \mathcal{B}^c_{proj} = \frac{c^I_H}{c^U_H} \mathcal{B}^c \oplus [x^c_1, y^c_1, x^c_1, y^c_1].
    \label{equ:reproj}
\end{equation}
where $\oplus$ is an element-wise addition operation. 
\end{comment}
The Non-Maximal Suppression(NMS) is then applied to remove duplicate detections.

While other methods need complex post-processing to filter the noisy crops\cite{clusnet-Yang-2019}, we can simply use the confidence score of the density crops to do the same. Stage one of the inference is the standard inference procedure in any detector. The filtering of the noisy crops can be easily performed with the confidence scores given by the detector. The second stage of the inference is performed with the same detector, but a different input (the up-scaled density crops). So, we are simply repeating the standard inference procedure of a detector one more time. All of these operations can be easily wrapped on top of the inference procedure of any detector, thus keeping the simplicity of the uniform cropping approach at inference too.
