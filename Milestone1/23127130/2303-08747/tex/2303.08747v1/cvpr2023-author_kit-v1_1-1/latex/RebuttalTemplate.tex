\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref,breaklinks,colorlinks,bookmarks=false]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{7334} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Cascaded Zoom-in Detector for High Resolution Aerial Images}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW

We thank the reviewers for their comments \& insights. The common issues raised are addressed below first \& then the independent ones by each reviewer are addressed.

\noindent \textbf{Novelty}: We accept the reviewers views on the novelty and agree to it; we are not proposing a completely novel idea, the concept of 2-stage detection is known for some time. Even the majority of the aerial image detection papers we cited are also based on the same. The existing papers use additional learnable modules to get density crops. However, the resulting complexity with more parameters to learn \& difficulty to train (additional loss functions, multi-stage training, hyper-parameter tuning) makes the practitioners in this community favor more towards uniform cropping (also called "tiling"). Please see the following references from the practitioners community 
\cite{small_obj_detection, tiling_obj_detection}. Uniform cropping is simple to use for training and inference, it has no other component than the detector itself. We attempted to bring the same simplicity by repurposing the detector itself to produce density crops thus reducing the gap between the research and practice in this domain. The results is a much simpler model than previous approaches, with better results.

\noindent \textbf{Why Faster RCNN?}: We used Faster RCNN mainly to comply with the comparison to the existing approaches. From the detector's point of view, it is just detecting another new class, which made us believe that the scale of improvement will be similar regardless of the detector used.\\
\textbf{1. Reviewer e18F}\\
\noindent(1) Difference in $\textrm{AP}_l$ between DOTA \& VisDrone: For DOTA, the images are very high-resolution, so we processed it as overlapping sliding windows; so the large objects might have fragmented. Also, this is happening only when P2 features are used.
\noindent(2) Dependence on Crop Discovery: Yes, the improvement is coming from the detection of the density crops, and they are typical in aerial images. Thus we have a collection of literature based on the density crop idea in this domain[5]. But without the density crops, the performance won't degrade as we also do detection on the original image. Fig 4, position (2, 3) we think the objects are relatively high scale(the visualization made it look small) at the bottom right, and the baseline detector detects most of them as well, probably which is why no crops there.\\
\textbf{2. Reviewer CGfh}\\
\noindent(1) Fusion of the prediction: The fusion process is simple, we detect objects on the original image as well as the crops. Then the detection from the crops is projected to the image by rescaling \& shifting. Finally, NMS is performed by taking all detection into account. We will add this in the paper. \noindent(2) The literature we cited uses DOTA in different ways \& the metric is also different across the papers. Methods like ClusterNet[35] assumed only movable object classes of the DOTA dataset appear crowded, so they evaluated only the 5 movable classes out of the 15 present in the dataset. Since DOTA is using Pascal VOC 2007 style mAP in its toolkit, many authors reported only that metric (mAP@50\% in VOC style). To assess the boost in small object detection ($\textrm{AP}_{s}$), we used COCO style metrics. Also, the immovable objects are small as well hence we kept all classes (we see improvements on them too in table \ref{table:per_cat_ap_improvement}). These differences made a direct comparison impossible across the literature.\\
\textbf{3. Reviewer M6JR}\\
\noindent(1) Effect on large datasets: As discussed in section 1, aerial images typically have hundreds of (small) objects per image, so annotating is difficult hence the community is yet to see large-scale datasets like COCO or OpenImages. In terms of annotated object instances, VisDrone(540k) and DOTA(280k) are comparable to MS-COCO(200k+). Moreover, the feature responses of small objects may be less if not processed at high resolution[5] highlighting the need for density crops regardless of the dataset size. \noindent(2) Effectiveness on other datasets: The method is effective if the crops are discovered around small objects regardless of the dataset. We can see improvement in the detection of (small) immovable object categories of the DOTA dataset too (table \ref{table:per_cat_ap_improvement}). These categories are not from street-view images.
\setlength{\textfloatsep}{0.2cm}
\begin{table}
    \tiny
    \centering
    \resizebox{.48\textwidth}{!}{% <------ Don't forget this %
      \begin{tabular}{r|r|r}
    \textbf{Category} & \textbf{Baseline AP} &  \textbf{CZ Detector AP} \\  
    \hline    
    basketball-court & 39.21 & 40.22 \\ 
    %roundabout & 31.65 & 34.74\\
    harbor & 34.92 & 35.86\\
    storage-tank & 22.56 & 25.09\\
    tennis-court & 76.57 & 78.70\\
    \end{tabular}% <------ Don't forget this %
    }
    \caption{AP improvement of immovable objs in the DOTA dataset.}
    \label{table:per_cat_ap_improvement}
\end{table}\\
\textbf{4. Reviewer iwpx}\\
\noindent(1) Complexity: We request the reviewer to see the gap between research and practice in this domain to clarify our "complex" argument.
\noindent(2, 3) The crops are augmented during training in all methods, in that sense, our training time is not different from existing density crop-based methods. The augmentation is also key to improving the performance; our ablation showed it more clearly, while in the literature it is not explicit. \noindent(4) Improvement on large objects: We focused mostly on improving the detection of small objects in high-res images. As we also detect on the original image (along with density crops), the detection on large objects will never go substantially low. Methods like SNIP, and TridentNet though normalize the scale, we think for images with 2k \& 4k resolutions they will struggle when giving equal importance to all image regions. They tested it on MS-COCO which has much lower resolution than DOTA or VisDrone.

%%%%%%%%% REFERENCES
{\scriptsize
\bibliographystyle{ieee_fullname}
\vspace{-1em}
\bibliography{egbib-rebuttal}
}

\end{document}
