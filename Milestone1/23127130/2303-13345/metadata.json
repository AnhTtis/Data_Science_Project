{
    "arxiv_id": "2303.13345",
    "paper_title": "A new subspace minimization conjugate gradient method for unconstrained minimization",
    "authors": [
        "Zexian Liu",
        "Yan Ni",
        "Hongwei Liu",
        "Wumei Sun"
    ],
    "submission_date": "2023-03-23",
    "revised_dates": [
        "2023-03-24"
    ],
    "latest_version": 1,
    "categories": [
        "math.OC"
    ],
    "abstract": "Subspace minimization conjugate gradient (SMCG) methods have become a class of quite efficient iterative methods for unconstrained optimization and have attracted extensive attention recently. Usually, the search directions of SMCG methods are generated by minimizing approximate models with the approximation matrix $ B_k $ of the objective function at the current iterate over the subspace spanned by the current gradient $ g_k $ and the latest search direction. The $ g_k^TB_kg_k $ must be estimated properly in the calculation of the search directions, which is crucial to the theoretical properties and the numerical performance of SMCG methods. It is a great challenge to estimate it properly. The projection technique has been used successfully to generate conjugate gradient directions such as Dai-Kou conjugate gradient direction. Motivated by the above two observations, in the paper we present a new subspace minimization conjugate gradient methods by using a projection technique based on the memoryless quasi-Newton method. More specially, we project the search direction of the memoryless quasi-Newton method into the subspace spanned by the current gradient and the latest search direction and drive a new search direction, which is proved to be descent. Remarkably, the proposed method without any line search enjoys the finite termination property for two dimensional convex quadratic functions, which is helpful for designing algorithm. An adaptive scaling factor in the search direction is given based on the above finite termination property. The proposed method does not need to determine the parameter $ œÅ_k $ and can be regarded as an extension of Dai-Kou conjugate gradient method. The global convergence of the proposed method is analyzed. Numerical comparisons indicate the proposed method is very promising.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.13345v1"
    ],
    "publication_venue": null
}