% Currently, very few research have explored video retrieval for generative tasks, e.g., video captioning. 

Retrieving video snippets for generation is used primarily in two tasks: video-grounded dialogue and video captioning. Recently, augmenting LLMs with video retrieval also demonstrates good performances, especially in few-shot settings.

\noindent\textbf{Video-grounded dialogue}~~~
Given video contexts, the model learns to engage in a relevant dialogue. \citet{pasunuru2018game} introduces a video-context, many-speaker dialogue dataset, which challenges researchers to develop visually-grounded dialogue models that generate relevant responses from live videos. Similarly, \citet{lei-etal-2020-tvqa} proposes TVQA+, a dataset that requires retrieving relevant video moments to answer textual questions about videos. Then, it proposes a unified framework that encodes video segments into representations, uses an attention mechanism to locate relevant information, and produces textual answers. To better perform visually-grounded dialogue tasks, \citet{le-etal-2020-bist} retrieves visual cues from prior user queries. The cues are then used as contextual information to construct relevant responses. On video QA, it substantially outperforms prior approaches. Recently, \citet{le2022vgnmn} extracts visual cues from the video to augment video-grounded dialogues. The video retrieval is performed with neural module networks, which are instantiated with entities and actions in previous dialogues. 

\noindent\textbf{Video captioning}~~~
{Sharing a similar motivation to RAG, }\citet{long2018video} first proposes to use attention layers to automatically select the most salient visual or semantic features and use them to augment caption generation. As a result, it stably outperforms previous methods. \citep{whitehead-etal-2018-incorporating} then develops a retrieval-based approach for video description generation. For news videos, it retrieves topically related news documents and then generates a description using a knowledge-aware video description network.

\noindent\textbf{LLM augmentation}~~
% With the emergence of general-purpose LLMs, 
\citet{wang2022language} attempts to augment an LLM to generalize to various video-to-text tasks from a few examples. As the LLMs cannot accept video inputs, it first translates video contents into attributes using image-language models and then prompts the retrieved content to instruct the LLM. It demonstrates good few-shot performances on a wide range of video-language tasks.

% REPRESENTATION GAP
Currently, the video-text research bottleneck mainly lies in the representation gap between different modalities. Research has been attempting to learn a better mapping between video-text via joint learning \citep{xu2015jointly, sun2019videobert}. Recent studies on dense video representation learning can also be useful for future video RAG. Besides, some papers~\citep{yang23vid2seq,wang2021t2vlad} try to introduce fine-grained interaction between different modalities {to learn better aligned representations}. \citet{zeng2022socratic} encourages multiple pretrained models in different modalities to exchange information with each other in a zero-shot manner. Most recently, \citet{zhang2023video} trains Video-Llama to better align pretrained video and audio encoders with LLM's embedding space.
% % TEXT-VIDEO RETRIEVAL
% \citet{bogolin2022querybank} proposes a query bank normalization method for cross-modal text-video retrieval. To enhance text-video retrieval pre-training, Cap4Video \citep{bogolin2022cap4video} and CLIP-ViP \citep{xue2022clip-vip} utilize the web-scale pre-trained knowledge to perform data augmentation.
% However, these methods still own a significant gap to be the foundation of retrieval-augmented generation models due to the cost of building a video index for knowledge search.