% To increase factual grounding and reduce hallucinations for PLMs, many research papers have attempted to augment generation by incorporating more structured knowledge, such as knowledge graphs, tables, and databases.

An open challenge in generative models is hallucination, where the model is likely to output false information.
% seemingly plausible sentences that do not conform to the ground-truth facts. 
% Researchers have denoted that language models, while only relying on internal knowledge (pre-trained weights), fail to recall accurate details when functioning as a knowledge base in question-answering tasks \citep{ye2022unreliability,creswell2022selection}. 
Thus, A potential solution is to ground generation with retrieved structured knowledge, such as knowledge graphs, tables, and databases.
% , often represents how knowledge from different domains is integrated. They could function as a reliant source of truth to enhance factuality. 

% QA task
\noindent\textbf{Question Answering (QA)}~~~
A natural setting to use knowledge is QA. To augment \emph{Knowledge Base (KB) QA} by extracting the most relevant knowledge, \citet{hu-etal-2022-logical} uses dense retrieval and \citet{liu-etal-2022-uni} uses a cross-encoder ranker. \citet{shu-etal-2022-tiara} employs multi-grained retrieval to extract relevant KB context and uses constrained decoding to control the output space. In \emph{table QA}, \citet{nan-etal-2022-fetaqa} proposes a dataset that requires retrieving relevant tables for answer generation. \citet{pan-etal-2021-cltr} then proposes a method that uses a transformer-based system to retrieve the most relevant tables and locate the correct cells. Moreover, to improve \emph{Video QA}, \citet{hu2023reveal} retrieves from Knowledge Graph (KG) encodings stored in the memory. The most prominent RAG usage remains in \emph{open-domain QA}, where multiple datasets are proposed \citep{lin-etal-2023-fvqa, ramnath-etal-2021-worldly}. For retrieval, \citet{ma-etal-2022-open-domain} verbalizes the KG and then uses dense passage retrieval. \citet{fan2019using, gupta-etal-2018-retrieve} encodes KG information into dense representations. \citet{pramanik2021uniqorn, jin2022heterformer} builds graph embeddings to retrieve question-relevant evidence. \citet{xu-etal-2021-fusing, baek2023knowledge} use semantic similarity and text-matching methods.
{Synthesis can occur at different stages. At the input stage,} \citet{xu-etal-2021-fusing, baek2023knowledge} feed in the retrieved contexts as additional inputs or prompts to the PLM. \citep{ma-etal-2022-open-domain, fan2019using} adapt the generator to accept the context representations as inputs.  {At model inference stage, }an interesting work is \citet{hu-etal-2022-empowering}, which inserts an interaction layer into PLMs to guide an external KG reasoning module.

\noindent\textbf{General text generation}~~~
External knowledge retrieval can improve general text generation to be more factually grounded. \citet{liu-etal-2022-relational} presents a memory-augmented approach to condition an autoregressive language model on a knowledge graph (KG). {During inference, }\citet{tan-etal-2022-tegtok} selects knowledge entries through dense retrieval and then injects them into the input encoding and output decoding stages in pretrained language models (PLMs). For \emph{domain-specific text generation}, \citet{frisoni-etal-2022-bioreader, yang-etal-2021-writing, li2019knowledgedriven} retrieve medical report chunks or report templates {to augment input prompts. Then, they} use self-devised decoders or graph transformers to generate grounded reports. To improve interpretability, RAG could be used to select facts as interpretable reasoning paths \citep{aggarwal-etal-2021-explanations, jansen-ustalov-2019-textgraphs}. 
% For image captioning, \citet{lu-etal-2018-entity} retrieves named entities via hashtags to provide richer contextual information. 
Moreover, RAG is especially useful for low-resource generation tasks, such as question generation \citep{yu-jiang-2021-expanding, xin-etal-2021-enhancing, gu-etal-2019-extract}, document-to-slide \citep{sun-etal-2021-d2s}, table-to-text \citep{su-etal-2021-shot-table}, counterargument generation \citep{jo-etal-2021-knowledge-enhanced}, entity description generation \citep{cheng-etal-2020-ent} and text-based games \citep{murugesan-etal-2021-efficient}.

% To aid LLMs, some papers design task-specific queries to retrieve structured knowledge by fine-tuning. 
Recent research has attempted to reduce hallucinations in LLMs by leveraging external structured knowledge. For example, during fine-tuning, LaMDA \citep{thoppilan2022lamda} learns to consult external knowledge sources before responding to the user, including an information retrieval system that can retrieve knowledge triplets and web URLs. Some papers treat the generative model (often large language models) as black-box and retrieve structured information without fine-tuning. For example, BINDER \citep{cheng2022binding} uses in-context learning to output designed API calls that retrieve question-relevant columns from tables.

\noindent\textbf{Reasoning with knowledge}~~~
By selecting knowledge, reasoning tasks can be solved in a more grounded and interpretable way. 
% \citet{moon-etal-2019-opendialkg} studies the Open-ended Dialog task parallel with a KG corpus. It proposes a model that learns the symbolic transitions of dialog contexts as structured traversals over KG, and predicts natural entities to introduce given previous dialog contexts via a graph path decoder. 
To generate an entailment tree explanation for a given hypothesis, \citet{neves-ribeiro-etal-2022-entailment} retrieves from textual premises iteratively and combines them with generation. \citet{yang-etal-2022-logicsolver} proposes a math reasoner that first retrieves highly-correlated algebraic knowledge and then passes them as prompts to improve the semantic representations for the generation task. With the recent advances in LLMs, \citet{he2022rethinking, li2023chain} retrieve from KG and KB, such as Wikidata, based on reasoning steps obtained from the chain-of-thought (CoT) prompting \citep{wei2022chain}.

\noindent\textbf{Knowledge-grounded dialogue}~~~
Dialogue generation based on relevant tables and knowledge bases has been a practical research application \citep{wu-etal-2020-diverse, li2022opera, nakamura-etal-2022-hybridialogue, gao-etal-2022-comfact, lu-etal-2023-statcan}. To tackle the challenge, \citet{li-etal-2022-knowledge} and \citet{galetzka-etal-2021-space} retrieve relevant knowledge, process it into a dense representation and incorporate it into dialogue generation. On top of dense representations, \citet{gu-etal-2020-filtering} and \citet{jung-etal-2020-attnio} leverage attention mechanisms to flexibly adjust which knowledge to depend on during generation. 
% \citet{} learns a fact-linking model to improve dialogue generation. 
Some methods~\citep{zhang-etal-2021-kers-knowledge, dziri-etal-2021-neural, chen-etal-2020-airconcierge} first generate subgoals or responses and then use them to retrieve relevant knowledge. The retrieved knowledge then helps amend previous responses. 
Besides knowledge, \citet{cai-etal-2019-retrieval} and \citet{wu-etal-2020-improving-knowledge} improve dialogue response generation by retrieving templates or prototype dialogues {to augment inputs}. Recently, \citet{kang2023knowledge} retrieves relevant subgraphs from KGs, and then utilizes contrastive learning to ensure that the generated texts have high similarity to the subgraphs.
% \citet{li2022opera} propose a unified dialog model that learns to query pre-defined databases with belief states, which is a list of triplets.

% As the format of structured knowledge departs from the natural texts seen by LLMs during pre-training, how to effectively retrieve and synthesize it for generation has been an open challenge. \citet{xie-etal-2022-unifiedskg} represent an early attempt, where all formats of knowledge, including tables, triplets, and ontology, are linearized into text format and fed into the LLM without retrieval. Such methods, however, are limited to the acceptable context length of the PLM and are often computationally expensive.


By retrieving from relevant sources, RAG not only improves factuality but also provides the grounding contexts while generating, thus addressing interpretability and robustness concerns. With the potential to handle more information types with recent advances in LLMs \citep{gpt4}, RAG with structured knowledge could be further enhanced. There are still challenges to be addressed. For example, there could be new designs for better retrieval systems that could promote efficient interactions suitable for diverse knowledge bases. Synthesizing this information correctly is also an open challenge, where it is hard to decide which parts need augmenting in the textual outputs.