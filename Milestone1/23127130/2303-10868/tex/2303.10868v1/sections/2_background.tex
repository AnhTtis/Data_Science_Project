\subsection{Multimodal Learning} % Xiaobao

% Multimodal learning involves utilizing multiple data modes, such as text, images, audio, and video, to learn and extract knowledge. Multimodal learning aims to create models that can leverage and capture complementary information for different tasks by learning from and integrating multiple modalities of data~\cite{baltruvsaitis2018multimodal, gao2020survey}. 
Multimodal learning focuses on learning a unified representation for data from different modalities, e.g., text, images, audio, and video. Multimodal learning aims at extracting complementary information to facilitate compositional tasks~\cite{baltruvsaitis2018multimodal, gao2020survey}.
With the fruitful progress made in computer vision~\cite{dosovitskiy2020vit, liu2021swin}, natural language processing~\cite{lanalbert, lewis2020retrieval}, and speech recognition~\cite{baevski2020wav2vec,hsu2021hubert}, 
% the development of multimodal models that are capable of processing and integrating different modalities of data has also been achieved. 
multimodal models that are capable of processing and integrating data from different modalities have been greatly improved.

Multimodal learning has numerous applications. For instance, multimodal learning can improve image recognition accuracy by analyzing images and videos in conjunction with textual descriptions in computer vision~\cite{ju2022prompting, alayracflamingo, jia2021scaling, radford2021learning}. Multimodal models can incorporate visual information from images or videos to enhance language understanding and generation~\cite{zhou2020unified, lei2021less}. It also has the potential to significantly enhance the performance of machine learning systems in different domains by allowing them to learn from and integrate multiple sources of information~\cite{tsai2019multimodal, acosta2022multimodal, nagrani2021attention}.

With the increasing availability of large-scale multimodal datasets~\cite{elliott2016multi30k, sheng2016dataset, duarte2021how2sign}, multimodal pre-trained models have been developed and showed promising results in various applications~\cite{gan2022vision, uppal2022multimodal}. Using the successful Transformer-based architecture, large multimodal pre-trained models, such as VL-Bert~\cite{citation-0}, SimVLM~\cite{wangsimvlm}, ALBEF~\cite{li2021align}, and CLIP~\cite{radford2021learning} are highly effective at learning complex patterns and relationships in multimodal data. These large models can then be transferred to different downstream tasks including VQA, image captioning, and object detection. 

Additionally, there has been growing interest in developing models that can generate output that incorporates multiple modalities of data. For example, DALL-E~\cite{ramesh2021zero} is fed with pairs of textual descriptions and corresponding images to learn the joint representations. It can generate highly creative and diverse images from even very complex textual descriptions. Similarly, VQGAN-CLIP~\cite{crowson2022vqgan} can generate new images based on textual prompts, where the textual description is used to guide the generation of the image. It combines the CLIP model for image-text understanding with the VQGAN model for image generation. There is also potential to improve the performance of natural language processing models by incorporating visual information in language generation tasks \citep{RA-VQA, chen-etal-2022-murag}.

Multimodal generative models have a wide range of applications, such as text-image generation, creative writing generation, and multilingual translation. They can also be used to produce new product designs or textual content including website content and documents. However, there remain challenges for multimodal generation models, such as access to a large amount of multimodal data, the network design that produces semantically meaningful outputs, the interpretability of the models, and related ethical issues. It is critical to address these challenges to realize the full potential of multimodal generative models and ensure the proper use of these models.
 


\subsection{Retrieval-Augmented Generation}
The idea of retrieval-augmented generation is popular nowadays in natural language processing (NLP), which has been a longstanding challenge in the field of artificial intelligence (AI). In the past, the primary research focus was on developing specialized frameworks for specific tasks \cite{chiu-nichols-2016-named,liu2016recurrent,ding-etal-2020-daga,qin-joty-2022-continual}. In recent years, there has been a significant shift in approach towards utilizing powerful, general-purpose language models that can be fine-tuned or prompt-tuned for a wide range of applications \cite{devlin-etal-2019-bert,Yang2019XLNetGA, Raffel2019ExploringTL,lewis2019bart,brown2020language,liu2021pre,qin2022lfpt,ding2022gpt,qin2023learning}. Through pre-training on a large-scale unlabeled corpus, pre-trained language models have shown significant improvement in a wide range of NLP tasks \cite{he-etal-2021-effectiveness,liu-etal-2021-mulda,ding-etal-2022-globalwoz,qin2023chatgpt,zhou2023comprehensive}. While this approach showed great potential, it is mainly applied to simple tasks such as sentiment analysis, which humans can easily accomplish without requiring additional knowledge or expertise \cite{lewis2020retrieval}. 

In order to address the difficulties associated with resolving knowledge-intensive NLP tasks, there exist primarily two approaches. The first approach involves pre-training on a knowledge base and storing the acquired knowledge within a PLM \cite{zhang-etal-2019-ernie,liu2020k,wang-etal-2021-k,liu2022enhancing,zhou-etal-2022-prix,jiang2022xlm}. The benefit of this approach is that it leverages a single model. However, it has two significant disadvantages: Firstly, it is difficult to control what knowledge has been learned by the models; Secondly, parameter updates are required when new knowledge comes in. The second approach is to develop retrieval-augmented generation methods \cite{gu2018search,weston-etal-2018-retrieve,cai-etal-2019-retrieval,lewis2020retrieval} by combining a retrieval-based component and a generative component (e.g. PLM, LLM, etc.).  Specifically, we denote the generative model by $f$ and input text by $x$. Traditional generative models focus on predicting output $y$ by $y = f(x)$. Denote the retriever module by $g$, and we could retrieve segments of information $c^r$ based on (parts of) the input $x^r \in x$. Thus, the retriever can predict $c^r = g(x^r)$. Then, the retrieval-augmented generation can be formulated as: $y = f(x, c)$, where $c = \{\langle x^r,c^r \rangle\}$ is a set of relevant instances retrieved from either the original training set or external datasets to improve response generation. The primary concept behind this approach is that $c^r$ can aid in generating a better response if it is similar or relevant to the input $x^r$. The retrieval memory can be obtained from three sources: the training corpus, external datasets, and large-scale unsupervised corpus \cite{li2022survey}.
% , where $x^r = \emptyset$ for unsupervised retrieval sources \cite{li2022survey}. }

Retrieval-augmented generation has been applied to a wide range of downstream NLP tasks, including machine translation~\cite{gu2018search, zhang-etal-2018-guiding, xu2020boosting, he2021fast}, dialogue generation \cite{weston-etal-2018-retrieve, wu2019response,cai-etal-2019-skeleton}, abstractive summarization \cite{peng-etal-2019-text}, knowledge-intensive generation \cite{lewis2020retrieval,izacard-grave-2021-leveraging}, etc. For text retrieval, there exist two types of retrievers that can be used to augment an LM: dense and sparse \cite{mialon2023augmented}. Sparse retrievers \cite{robertson2009probabilistic} use sparse bag-of-words representations, while dense neural retrievers \cite{asai2022task} use dense query and document vectors. Both types assess document relevance to a query, with sparse retrievers excelling at precise term overlap and dense retrievers being better at computing semantic similarity \cite{luan2021sparse}. Various works have proposed methods to jointly train a retrieval system with an encoder or sequence-to-sequence LM, achieving comparable performance to larger LMs that use significantly more parameters. These models include REALM~\cite{guu2020retrieval}, RAG~\cite{lewis2020retrieval}, and RETRO~\cite{borgeaud2022improving}, which integrate retrieval into existing pre-trained LMs, and Atlas~\cite{izacard2022atlas}, which obtains a strong few-shot learning capability despite being much smaller than other large LMs. Recent works propose combining a retriever with chain-of-thought (CoT) prompting for reasoning to augment language models \cite{he2022rethinking,trivedi2022interleaving}. For example, \citet{anonymous2023verify-and-edit:} verifies the validity of CoT reasoning steps and retrieves relevant contexts to augment the generation of the uncertain ones. \citet{he2022rethinking} generate reasoning paths using CoT prompts and retrieve knowledge to support the explanations and predictions. \citet{trivedi2022interleaving} propose an information retrieval CoT approach for multi-step question answering, where retrieval guides CoT reasoning and vice versa.




% The next part:
% 1. retriever (dense / sparse)
% 2. generation (PLM / LLM)
% 3. CoT + RAG (Lecun's survey, emerging trend)
% 4. Applications on Downstream tasks (li's survey)



% where $z = \{\langle x^r,y^r \rangle\}$ is a set of relevant instances retrieved from the original training set or external datasets. The main idea of this paradigm is that $y^r$ may benefit the response generation, if $x^r$ (or $y^r$) is similar (or relevant) to the input $x$. It is worth noting that $x^r = \emptyset$ when unsupervised retrieval sources are used. In general, the retrieval memory can be retrieved from three kinds of sources: the training corpus, external datasets in the same format with the training corpus, and large-scale unsupervised corpus.

% The problem has been redefined in terms of retrieving the appropriate information from knowledge base, by utilizing the non-parametric index as a means of organizing knowledge.
% The retrieval-augmented generation
% can be further formulated as:
% y = f(x, z) (1)
% where z = {hx
% r
% , y
% r
% i} is a set of relevant instances
% retrieved from the original training set or external
% datasets.