\subsection{Retrieval Augmented Multimodal Reasoning}
%including both cot and other formats of reasoning (kosmos-1)

\begin{quote}
\small 
\emph{The words of the language, as they are written or spoken, do not seem to play any role in my mechanism of thought. The psychical entities which seem to serve as elements in thought are certain signs and more or less clear images which can be "voluntarily" reproduced and combined. --- Albert Einstein}
\end{quote}

One potential application of multimodal information retrieval is multimodal reasoning. \citet{lu2022learn} first introduce ScienceQA, a large-scale multimodal science question dataset annotated with lectures and explanations. Based on this benchmark, \citet{zhang2023multimodal} propose Multimodal Chain-of-Thought (Multimodal-CoT) which incorporates language and vision modalities into a two-stage (rationale generation and answer inference) framework, surpassing GPT-3.5 by a large margin with a much smaller fine-tuned model. Similar to \citet{zhang2023multimodal}, kosmos-1 \citep{huang2023language} breaks down multimodal reasoning into two steps. It first generates intermediate content as the rationale based on visual information, and then uses the generated rationale to induce the result. However, both methods may have difficulties in understanding certain types of images (e.g., maps), which could be mitigated by retrieving relevant informative image-text pairs. We hope that future work can pay more attention to how to effectively and efficiently combine multimodal reasoning with multimodal retrieval.
% \xl{$citet$ is often counted as plural (as it means many authors) according to my understanding. Please adjust the verbs if needed.}

\subsection{Building a Multimodal Knowledge Index}
% Assigned with Fangkai

In order to facilitate retrieval augmented generation, one of the most fundamental aspects is the building of a multimodal knowledge index. The goal of building a knowledge index is twofold: Firstly, the dense representation should support low storage, dynamic updating of the knowledge base, and accurate search. Secondly, it could enable faster search speed with the help of local sensitive hashing~\citep{data-mining}, which combats scaling and robustness concerns when the knowledge base is scaled up extremely.

Currently, the dense representation for text snippets has been widely studied for documents~\cite{karpukhin-etal-2020-dense,gao-callan-2021-condenser,gao-etal-2021-simcse}, entities~\citep{sciavolino-etal-2021-simple,lee-etal-2021-learning-dense}, and images~\cite{clip}. Besides, there are also many studies optimizing dense representations in an end-to-end manner~\cite{lewis2020retrieval}.
Nevertheless, few works~\citep{chen-etal-2022-murag} have explored building 
a multimodal index at the same time for downstream generation, and are also limited in text and image. How to map a multimodal knowledge index into a unified space is still a long-term challenge.

\subsection{Pre-training combined with multimodal retrieval}

With the goal of better aligning the abilities to handle different modalities in a pre-trained model, there could be future work built on employing retrieval-based approaches during pre-training. Currently, there have been many methods that fine-tune the pre-trained generative model for retrieval. For example, LaMDA \citep{thoppilan2022lamda} can call an external toolset for fine-tuning, including an information retrieval system, a calendar, and a calculator. Similarly, during fine-tuning, Toolformer \citep{schick2023toolformer} augments models with API calls to tools including a question-answering system and a Wikipedia search engine. 

During pretraining, if similar retrieval abilities are leveraged, the generative model would be able to interact with retrieval tools better. Thus, it could output more grounded information, provide relevant contexts to users, and update their information accordingly. When new information comes in, the generative model would be able to effectively retrieve from an up-to-date external base instead of relying solely on pre-trained weights. This advantage also expands to handling robustness in out-of-domain questions.

To incorporate retrieval with pre-training, there remains the challenge of developing appropriate datasets labeled with retrieval-based API calls. To tackle this challenge, LaMDA \citep{thoppilan2022lamda} uses labels developed by human annotators, which could be expensive to collect. Toolformer \citep{schick2023toolformer} uses a sampling and filtering approach for automatic labeling, which is inexpensive but could induce noise and bias. A potential solution is to use a neuro-symbolic approach such as \citet{davoudi2021toward}, which use prototype learning and deep-KNN to find nearest neighbors during training.