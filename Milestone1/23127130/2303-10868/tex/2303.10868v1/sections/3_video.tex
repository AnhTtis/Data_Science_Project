Currently, very few works have explored video retrieval for generative tasks, e.g., video captioning. However, the recent studies on dense video representation learning can be useful when developing video knowledge-enhanced generative approaches in the future. \citet{bogolin2022querybank} propose a query bank normalization method for cross-modal text-video retrieval. Besides, Cap4Video \cite{bogolin2022cap4video} and CLIP-ViP \citep{xue2022clip-vip} are data augmentation frameworks that utilize the web-scale pre-trained knowledge to enhance text-video retrieval pre-training. Besides, some works also try to introduce fine-grained interaction between different modalities~\citep{yang23vid2seq,wang2021t2vlad}. However, these methods still own a significant gap to be the foundation of retrieval-augmented generation models due to the cost of building a video index for knowledge search.