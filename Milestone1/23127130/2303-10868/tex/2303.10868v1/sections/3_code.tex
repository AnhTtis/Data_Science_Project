Software developers attempt to search for relevant information to improve their productivity from large amounts of available resources such as explanations for unknown terminologies, reusable code patches, and solutions to common programming bugs~\cite{dsf}. Inspired by the progress of deep learning in NLP, a general retrieval-augmented generation paradigm has benefited a wide range of code intelligent tasks including code completion \cite{lu-etal-2022-reacc}, code generation \cite{zhou2022docprompting}, and automatic program repair (APR) \cite{nashidretrieval}. However, these approaches often treat programming languages and natural languages as equivalent sequences of tokens and ignore the rich semantics inherent to source code. To address these limitations, recent research work has focused on improving code generalization performance via multimodal learning, which incorporates additional modalities such as code comments, identifier tags, and abstract syntax trees (AST) into code pretrained models~\cite{codet5,unixcoder,codereviewer}. To this end, multimodal retrieval-augmented generation approach has demonstrated its feasibility in a variety of code-specific tasks, including:

\paragraph{\textbf{Text-to-Code Generation}} 
%ws edit
Numerous research studies have investigated the utilization of relevant codes and associated documents to benefit code generation models. A prominent example is REDCODER \cite{parvez-etal-2021-retrieval-augmented}, which retrieves the top-ranked code snippets or summaries from an existing codebase, and aggregates them with source code sequences to enhance the generation or summarization capabilities. As another such approach, DocPrompting \citep{zhou2022docprompting} uses a set of relevant documentation as in-context prompts to generate corresponding code via retrieval. In addition to these lexical modalities, RECODE~\cite{DBLP:conf/emnlp/HayatiOAYTN18} proposes a syntax-based code generation approach to reference existing subtree from the AST as templates to direct code generation explicitly.
%Original: Numerous research studies have investigated how to retrieve relevant codes and documents to enhance (both end-to-end and syntax-based) code generation models. A prominent example is REDCODER \cite{parvez-etal-2021-retrieval-augmented} which augments the source input code sequence with relevant code snippets or summaries retrieved from a codebase. DocPrompting \citet{zhou2022docprompting} is another such example that retrieves the relevant documentation pieces given a natural language query and generates code based on the query and the retrieved documentation. In addition, RECODE~\cite{DBLP:conf/emnlp/HayatiOAYTN18} is an approach that explores retrieving codes to support syntax-based code generation models. Specifically, it explicitly references existing subtrees from the Abstract Syntax Tree (AST) of codes as templates to guide the code generation models.

%\ws{For your reference, how chatgpt refines this paragraph: Numerous research studies have investigated the utilization of relevant codes and documents to enhance code generation models. A prominent example is REDCODER \cite{parvez-etal-2021-retrieval-augmented}, which entails augmenting the input sequence to the code generation model with relevant code snippets or summaries retrieved from a database. DocPrompting \cite{zhou2022docprompting} is a code generation approach that goes beyond code retrieval to include (1) retrieving relevant documentation pieces based on a natural language query, and (2) generating code based on the query and the retrieved documentation. Additionally, the RECODE approach \cite{DBLP:conf/emnlp/HayatiOAYTN18} is a syntax-based code generation technique that uses existing subtrees from the AST as templates to direct code generation.}

\paragraph{\textbf{Code-to-Text Generation}} 
%ws edit
Retrieval-based code summarization methods have been studied extensively. For example, RACE~\cite{DBLP:conf/emnlp/ShiW0DZHZ022} leverages relevant code differences and their associated commit message to enhance commit message generation. Besides, RACE calculates the semantic similarity between source code differences and retrieved ones to weigh the importance of different input modalities. Another retrieval-based neural approach is Rencos~\cite{10.1145/3377811.3380383}, which retrieves two similar code snippets based on the aspects of syntactic-level similarity and semantic-level similarity of a given query code. 
These similar contexts are then incorporated into the summarization model during the decoding phase.
This idea is further explored by~\citet{liu2021retrievalaugmented}, where retrieved code-summary pairs are used to augment the original code property graph~\cite{DBLP:conf/sp/YamaguchiGAR14} of source code via local attention mechanism. To capture the global semantics for better code structural learning, a global structure-aware self-attention mechanism~\cite{DBLP:conf/emnlp/ZhuLZQZZ19} is further employed.

\paragraph{\textbf{Code Completion}}
%\rc{``Methods that improve'' instead of ``benefiting''?} 
%Methods that improve code completion tasks 
%\rc{tasks?} 
%\cite{mcconnell2004code} by retrieval have 
%\rc{have?} 
%have gained increasing attention in recent years. 
%\citet{hashimoto2018retrieve} introduce the very first trainable approach to retrieve a training example based on the input and then edit 
%\rc{edit?} 
%it for code auto-completion. 
%ReACC \cite{lu-etal-2022-reacc} is another approach that employs both lexical copying and referring to code with semantic similarity to the input to enhance it for the code completion generator. More recently, \citet{ding2022cocomic} introduce CoCoMIC that targets at 
%ws edit
Recent advances in retrieval-based code completion tasks~\cite{mcconnell2004code} have gained increasing attention. Notably,~\citet{hashimoto2018retrieve} adapt the retrieve-and-edit framework to improve the model's performance in code auto-completion tasks. To address practical code completion scenarios, ReACC~\cite{lu-etal-2022-reacc} takes both lexical and semantic information of the unfinished code snippet into account, utilizing a hybrid technique to combine a lexical-based sparse retriever and a semantic-based dense retriever. First, the hybrid retriever searches for a relevant code from the codebase based on the given incomplete code. Then, the unfinished code is concatenated with the retrieval, and an auto-regressive code completion generator will generate the completed code based on them. In order to address project relations, CoCoMIC~\cite{ding2022cocomic} decomposes a code file into four components: \emph{files}, \emph{global variables}, \emph{classes}, and \emph{functions}. It constructs an in-file context graph based on the hierarchical relations among all associated code components, forming a project-level context graph by considering both in-file and cross-file dependencies. Given an incomplete program, CoCoMIC retrieves the most relevant cross-file entities from its project-level context graph and jointly learns the incomplete program and the retrieved cross-file context for code completion.   

%Recently, \citet{ding2022cocomic} introduce CoCoMIC that targets at 
%\rc{to -> at} 
%effectively locating and retrieving 
%\rc{locating and retrieving} 
%relevant cross-file codes 
%\rc{use ``cross-file codes'' instead of ``codes across files''? could be clearer} 
%with the given context, and incorporate cross-file context to jointly learn the in-file and cross-file contexts for code completion.
% \rc{should change the ``context'' to ``contexts''?} for code completion.
%\ws{For your reference, how chatgpt refines this paragraph: Retrieval-based methods for improving code completion tasks \cite{mcconnell2004code} have gained increasing attention in recent years. Notably, Hashimoto et al. \cite{hashimoto2018retrieve} introduced the first trainable approach that retrieves a training example based on the input and then modifies it for code auto-completion. ReACC \cite{lu-etal-2022-reacc} is another approach that employs both lexical copying and retrieval of code with similar semantics to enhance the input for code completion generation. More recently, Ding et al. \cite{ding2022cocomic} proposed CoCoMIC, a method that effectively locates and retrieves relevant cross-file codes given a context and jointly learns the in-file and cross-file contexts for code completion.}

\paragraph{\textbf{Automatic Program Repair (APR)}}
Inspired by the nature that a remarkable portion of commits is comprised of existing code commits~\cite{Martinez2014DoTF}, APR is typically treated as a search problem by traversing the search space of repair ingredients to identify a correct fix~\cite{DBLP:conf/icse/QiMLDW14}, based on a redundancy assumption~\cite{White2019SortingAT} that the target fix can often be reconstructed in the search space. Recent studies have shown that mining relevant bug-fix patterns from existing search space~\cite{simfix} and external repair templates from StackOverflow~\cite{DBLP:conf/wcre/LiuZ18} can significantly benefit APR models.~\citet{joshi2022repair} intuitively rank a collection of bug-fix pairs based on the similarity of error messages to develop few-shot prompts. They incorporate compiler error messages into a large programming language model Codex~\cite{codex} for multilingual APR. CEDAR~\cite{nashidretrieval} further extends this idea to retrieval-based prompts design using relevant code demonstrations, comprising more modalities such as unit test, error type, and error information. Additionally, ~\citet{jin2023inferfix} leverage a static analyzer Infer to extract error type, error location, and syntax hierarchies~\cite{clement2021long} to prioritize the focal context. Then, they retrieve semantically-similar fixes from an existing bug-fix codebase and concatenate the retrieved fixes and focal context to form the instruction prompts for program repair.

\paragraph{\textbf{Reasoning over Codes as Intermediate Steps}} 
%\ws{I have refined this part, please go through it and feel free to correct where I might misunderstand your intention, thanks Long.} \xl{I think they look great. Thanks Weishi!}
While large language models (LLMs) have recently demonstrated their impressive capability to perform reasoning tasks, they are still prone to logical and arithmetic errors~\cite{gao2022pal, chen2022program, madaan2022language}. To mitigate this issue, emerging research works have focused on using LLMs of code (e.g., Codex \cite{codex}) to generate the code commands for solving logical and arithmetic tasks and calling external interpreters to execute the commands to obtain the results. Notably, \citet{gao2022pal} propose to generate Python programs as intermediate reasoning steps and offload the solution step to a Python interpreter. Additionally, \citet{chen2022program} explore generating chain-of-thought (CoT) \cite{wei2022chain} for not only text but also programming language statements as reasoning steps to solve the problem. During the inference phase, answers are obtained via an external interpreter. Similarly, \citet{lyu2023faithful} propose Faithful CoT that first translates the natural language query to a symbolic reasoning chain and then solves the reasoning chain by calling external executors to derive the answer. Another example is \citet{ye2023large}, which utilizes LLMs to decompose table-based reasoning tasks into subtasks, decouples logic and numerical computations in each step through SQL queries by Codex, and calls SQL interpreters to solve them (a process called "parsing-execution-filling").

LLMs of code are also known as good-structured commonsense reasoners, and even better-structured reasoners than LLMs \cite{madaan2022language}. As a result, prior studies have also investigated the idea of transforming structured commonsense generation tasks into code generation problems and employing LLMs of code as the solvers. One such work is CoCoGen \cite{madaan2022language} which converts each training sample (consisting of textual input and the output structure) into a Tree class in Python. The LLMs of code then perform few-shot reasoning over the textual input to generate Python code, and the Python code is then converted back to the original structure for evaluation. Besides, the success of LLMs of code such as Codex in synthesizing computer code also makes them suitable for generating formal codes. Motivated by this, \citet{wu2022autoformalization} propose to prove mathematical theorems by adopting Codex to generate formalized theorems from natural language mathematics for the interactive theorem prover Isabelle \cite{WenzelPN-TPHOLs08}.

%Prior works have attempted to address these reasoning tasks by converting them into code generation problems, and utilizing large language models of code (e.g., Codex \cite{codex}) to derive answers. For instance, CoCoGen \cite{madaan2022language} solves the structured commonsense generation tasks by converting the structure (typically a graph) into semantically equivalent code programs and employing Codex as the task solver. Besides, 

%Recent research has focused on using LLMs of code to output the code commands for solving logical and arithmetic tasks then calling external interpreters to execute the commands and derive the results. Notably, \citet{gao2022pal} propose to generate Python programs as intermediate reasoning steps and offload the solution step to a Python interpreter, while \citet{chen2022program} explore generating chain-of-thought \cite{wei2022chain}(CoT) for both text and programming language statements. During the inference phase, answers are obtained via an external interpreter. Similarly, \citet{lyu2023faithful} propose Faithful CoT, which decomposes reasoning tasks into two stages: (1) translating natural language query to symbolic reasoning chain; (2) solving reasoning chains by calling external executors. Another example is \citet{ye2023large}, which leverages LLMs to decompose table-based reasoning tasks into subtasks, decouples logic and numerical computations in each step through SQL queries by Codex, and calls SQL interpreters to solve them (a process called "parsing-execution-filling"). 
%






