Incorporating image data with text information has long been a crucial research topic, as a considerable amount of world knowledge is stored exclusively in images. % todo: to cite prev research, and stats on visual information

% todo: make a table for easier comparison
% FIBER, Blip-2
Recent advances on pretrained models shed light on general image-text multi-modal models:
 Flamingo \cite{flamingo} can generate comprehensive captions from input images. FIBER \citep{dou2022coarse} proposes a two-stage vision-language (VL) pre-training strategy benefiting different levels of VL tasks. DALL-E \cite{DALL-E} and Parti \cite{parti} can generate images based on given text instructions. CM3 \cite{CM3} models both text and image for its input and output. Blip-2 \citep{li2023blip} bootstraps language-image pre-training from off-the-shelf frozen vision and language models. 
 
 However, these models require huge computational resources for pre-training and large amounts of model parameters --- as they need to memorize vast world knowledge, such as what chinchillas look like and where they commonly habitat. More critically, such models cannot efficiently deal with new or out-of-domain knowledge. To this end, multiple retrieval-augmented works have been proposed to better incorporate external knowledge from images and text documents. 

%PICa, Re-imagen
 Towards open-domain visual question answering (VQA), RA-VQA \cite{RA-VQA} jointly trains the document retriever and answer generation module by approximately marginalizing predictions over retrieved documents. It first uses existing tools of object detection, image captioning, and optical character recognition (OCR) to convert target images to textual data. Then, it performs dense passage retrieval (DPR)~\citep{dpr} to fetch text documents relevant to target image in the database. Finally, each retrieved document is concatenated with the initial question to generate the final prediction, similar to RAG \cite{lewis2020retrieval}. Besides external documents, PICa \citep{yang2022empirical} and KAT \cite{KAT} also consider LLMs as implicit knowledge bases and extract relevant implicit information from GPT-3. Plug-and-Play \cite{Plug-and-Play} retrieves relevant image patches by using GradCAM \cite{GradCAM} to localize relevant parts based on the initial question. It then performs image captioning on retrieved patches to acquire augmented context. Beyond text-only augmented context, MuRAG \cite{chen-etal-2022-murag} retrieves both text and image data and incorporates images as visual tokens. RAMM \cite{RAMM} retrieves similar biomedical images and captions, then encodes two modalities through different networks. 

 Apart from VQA, RA-transformer \cite{RA-transformer} and Re-ViLM \cite{Re-ViLM} generate more factual captions by retrieving relevant captions. Beyond retrieving images and text documents before generating text, Re-Imagen \citep{chen2022re} leverages a  multi-modal knowledge base to retrieve image-text pairs to facilitate image generation. RA-CM3 \cite{RA-CM3} can generate mixtures of images and text. It shows that retrieval-augmented image generation performs much better on knowledge-intensive generation tasks and opens up new capabilities such as multimodal in-context learning. % todo: RA-CM3 is a great work, should extend it a bit
 