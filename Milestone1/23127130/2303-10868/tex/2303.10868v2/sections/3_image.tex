% Incorporating image data with text information has long been a crucial research topic, as a considerable amount of world knowledge is stored exclusively in images. % todo: to cite prev research, and stats on visual information
% Comment from fangkai: we may need to reorganize this section to highlight generation, instead of multimodal learning/pretraining.

% todo: make a table for easier comparison
% FIBER, Blip-2
% Recent advances in pretrained models shed light on general image-text multi-modal models: Flamingo \cite{flamingo} can generate comprehensive captions from input images. FIBER \citep{dou2022coarse} proposes a two-stage vision-language (VL) pre-training strategy benefiting different levels of VL tasks. DALL-E \cite{DALL-E} and Parti \cite{parti} can generate images based on given text instructions. CM3 \cite{CM3} models both text and image for its input and output. Blip-2 \citep{li2023blip} bootstraps language-image pre-training from off-the-shelf frozen vision and language models. 
 
Recent advances on pretrained models shed light on general image-text multi-modal models \citep{DALL-E, flamingo, CM3, parti, dou2022coarse, li2023blip}. However, these models require huge computational resources for pretraining and large amounts of model parameters --- as they need to memorize vast world knowledge.
% , such as what chinchillas look like and where they commonly habitat. 
More critically, they cannot efficiently deal with new or out-of-domain knowledge. To this end, multiple retrieval-augmented methods have been proposed to better incorporate external knowledge from images and text documents. 

%PICa, Re-imagen
\noindent\textbf{Visual question answering (VQA)}~~~
To tackle open-domain VQA, RA-VQA \cite{lin-byrne-2022-retrieval} jointly trains the document retriever and answer generation module by approximately marginalizing predictions over retrieved documents. 
It first uses existing tools of object detection, image captioning, and optical character recognition (OCR) to convert target images to textual data. Then, it performs dense passage retrieval (DPR)~\citep{dpr} to fetch text documents relevant to the target image in the database. Finally, each retrieved document is concatenated with the initial question to generate the final prediction, similar to RAG \cite{lewis2020retrieval}. Besides external documents, PICa \citep{yang2022empirical} and KAT \cite{gui-etal-2022-kat} also consider LLMs as implicit knowledge bases and extract relevant implicit information from GPT-3. Plug-and-Play \cite{tiong-etal-2022-plug} retrieves relevant image patches by using GradCAM \cite{GradCAM} to localize relevant parts based on the initial question. It then performs image captioning on retrieved patches to acquire augmented context. Beyond text-only augmented context, MuRAG \cite{chen-etal-2022-murag} retrieves both text and image data and incorporates images as visual tokens. RAMM \cite{RAMM} retrieves similar biomedical images and captions and encodes them through different networks. 

\noindent\textbf{Image captioning}~~~
% Apart from VQA, 
To generate multi-style captions, \citet{zhou-long-2023-style} uses a style-aware visual encoder to retrieve image contents before generating captions. Beyond simply encoding visual information, \citet{cho-etal-2022-fine} further uses the multimodal similarity between  image-text pairs as a reward function to train a more fine-grained captioning model. Beyond retrieving image elements, \citet{RA-transformer, shi-etal-2021-retrieval-analogy,ramos-etal-2023-retrieval,Re-ViLM} retrieves relevant captions to the inputs. \citet{zhou-etal-2022-focus} tackles news image captioning by retrieving visually grounded entities in news articles.

\noindent\textbf{Visually grounded dialogue}~~~
This task \citep{lee-etal-2021-constructing} requires retrieving visual information to produce relevant dialog responses. \citet{fan-etal-2021-augmenting} augments generative models with KNN-based Information Fetching (KIF) modules that retrieve images and wiki knowledge. \citet{liang-etal-2021-maria} retrieves a correlated image to the dialog from an image index to ground the response generator.

\noindent\textbf{Text generation}~~~
For general text generation tasks, image retrieval can also help expand contexts. \citet{yang-etal-2022-z} augments a text model's ``imagination '' by retrieving existing images and synthesizing newly generated images. As a result, fueling language models with imagination can improve performances in many downstream natural language tasks. \citet{fang-feng-2022-neural} shows that machine translation can be significantly improved by retrieving visual information at the phrase level, especially when the textual context is limited. Image RAG can also help low-resource tasks such as medical report generation \citep{wu-etal-2022-deltanet} and architectural description generation \citep{mille-etal-2020-case}.

Beyond retrieving images before generating text, Re-Imagen \citep{chen2022re} leverages a  multi-modal knowledge base to retrieve image-text pairs to facilitate image generation. RA-CM3 \cite{RA-CM3} can generate mixtures of images and text. It shows that retrieval-augmented image generation performs much better on knowledge-intensive generation tasks and opens up new capabilities such as multimodal in-context learning. % todo: RA-CM3 is a great work, should extend it a bit
 