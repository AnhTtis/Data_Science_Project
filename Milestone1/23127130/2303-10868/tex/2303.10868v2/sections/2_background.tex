
To better understand the state and advancements that inspired multimodal retrieval augmentation, we first define and discuss the background of two key concepts: multimodal learning and retrieval-augmented generation (RAG).

\vspace{-1ex}
\subsection{Multimodal Learning} % Xiaobao
% \vspace{-1ex}

% DEFINITION
Multimodal learning refers to learning a unified representation of data from different modalities. It aims at extracting complementary information to facilitate compositional tasks~\cite{baltruvsaitis2018multimodal, gao2020survey}.
% With the fruitful progress made in computer vision~\cite{dosovitskiy2020vit, liu2021swin}, natural language processing~\cite{lanalbert, lewis2020retrieval}, and speech recognition~\cite{baevski2020wav2vec,hsu2021hubert},  unified models capable of processing and integrating data from different modalities are greatly improved.

% APPLICATION
Multimodal generative models have a wide range of applications, such as text-image generation, creative writing generation, and multilingual translation. For instance, the image recognition task can benefit from analyzing images and videos in conjunction with textual descriptions~\cite{ju2022prompting, alayracflamingo, jia2021scaling, radford2021learning}. Conversely, incorporating visual information
% from images and/or videos 
also aids language understanding and generation~\citep{zhou2020unified, lei2021less}. Moreover, they have the potential to significantly improve machine learning systems across various domains by enabling models to learn from and integrate multiple sources of information~\cite{tsai2019multimodal, acosta2022multimodal, nagrani2021attention}.
% PROGRESS: GENERATIVE AI
Additionally, there has been growing interest in developing generative models that can output multiple modalities of data \citep{ramesh2021zero, crowson2022vqgan, RA-VQA, chen2022murag}. However, there remain challenges for multimodal generative models, such as gaining access to a large amount of multimodal data and designing a network that produces semantically meaningful outputs.
% , \red{enhancing} interpretability, and \red{addressing potential} ethical issues. 
% It is critical to address these challenges to realize the full potential of multimodal generative models.


% PROGRESS: PRETRAINED
% With the increasing availability of large-scale multimodal datasets~\cite{elliott2016multi30k, sheng2016dataset, duarte2021how2sign}, multimodal pre-trained models have shown promising results in various applications~\cite{gan2022vision, uppal2022multimodal}. Using the successful Transformer-based architecture, large multimodal pre-trained models, such as VL-Bert~\cite{citation-0}, SimVLM~\cite{wangsimvlm}, ALBEF~\cite{li2021align}, and CLIP~\cite{radford2021learning} are highly effective at learning complex patterns and relationships in multimodal data. These large models can then be transferred to different downstream tasks including VQA, image captioning, and object detection. 


% Additionally, there has been growing interest in developing \red{generative} models that can output multiple modalities of data. For example, DALL-E~\cite{ramesh2021zero} is fed with pairs of textual descriptions and corresponding images to learn the joint representations. It can generate highly creative and diverse images from even very complex textual descriptions. Similarly, VQGAN-CLIP~\cite{crowson2022vqgan} can generate new images based on textual prompts, where the textual description is used to guide the generation of the image. It combines the CLIP model for image-text understanding with the VQGAN model for image generation. There is also potential to improve the performance of natural language processing models by incorporating visual information in language generation tasks \citep{RA-VQA, chen2022murag}. 

\vspace{-1ex}
\subsection{Retrieval-Augmented Generation (RAG)}

% DEFINITION
RAG 
% refers to retrieving external information and leveraging it for generation. It 
typically consists of two phases: 
retrieving contextually relevant information, and guiding the generation process using the retrieved knowledge.
% Firstly, a retriever is usually employed to select relevant information. Then, the information is synthesized into language models to augment their generated outputs.

% DEVELOPMENT
Recently, RAG has gained popularity in Natural Language Processing (NLP) due to the rise of general-purpose Large Language Models (LLMs) \cite{chowdhery2022palm, gpt4}, which have boosted performances in a wide range of NLP tasks. However, there are two primary challenges: Firstly, because generative models rely on the inner knowledge (weights), they result in a high amount of hallucinations \cite{zhao2023chatgptlike}.
% Despite studies showing that weights can store an amount of internal knowledge~\citep{petroni2019language, roberts2020knowledge}, the outputs are often found to be false or outdated, thus resulting in a high amount of hallucinations \cite{zhao2023chatgptlike}. 
Secondly, due to their large parameter sizes and the high costs of updating, the traditional pretraining and finetuning approaches have become infeasible. As a solution, RAG methods  \cite{gu2018search,weston-etal-2018-retrieve,cai-etal-2019-retrieval,lewis2020retrieval} offer a promising solution for LLMs to effectively interact with the external world.
% By retrieving external knowledge, they can provide grounding contexts that effectively mitigate hallucinations. It is also possible to synthesize the retrieved information without updating the entire LLM. Thus, RAG offers a powerful toolset for LLMs to effectively interact with the external world.}
% The idea of retrieval-augmented generation is popular nowadays in Natural Language Processing (NLP). In the past, \red{research focused on} developing specialized frameworks for \red{individual} tasks \cite{chiu-nichols-2016-named,liu2016recurrent,ding-etal-2020-daga,qin-joty-2022-continual}. In recent years, \red{however,} 
% there has been a significant shift in approach towards utilizing powerful, general-purpose language models that can be fine-tuned or prompt-tuned for a wide range of applications \cite{devlin-etal-2019-bert,Yang2019XLNetGA, Raffel2019ExploringTL,lewis2019bart,brown2020language,liu2021pre,qin2022lfpt,ding2022gpt,qin2023learning}. 
% Through pre-training on a large-scale unlabeled corpus, pre-trained language models have shown significant improvement in a wide range of NLP tasks \cite{he-etal-2021-effectiveness,liu-etal-2021-mulda,ding-etal-2022-globalwoz,qin2023chatgpt,zhou2023comprehensive}. 
% While this approach showed great potential, it is mainly applied to simple tasks such as sentiment analysis, which humans can easily accomplish without requiring additional knowledge or expertise \cite{lewis2020retrieval}. 

% \red{To tackle the problem of hallucination,}
% In order to address the difficulties associated with resolving knowledge-intensive NLP tasks, 
% there exist primarily two approaches. The first approach \red{pretrains} on a knowledge base and \red{stores} the acquired knowledge within a PLM \cite{zhang-etal-2019-ernie,liu2020k,wang-etal-2021-k,liu2022enhancing,zhou-etal-2022-prix,jiang2022xlm}. The benefit of this approach is that it leverages a single model. However, it has two significant disadvantages: Firstly, it is difficult to control what knowledge has been learned by the models; Secondly, parameter updates are required when new knowledge comes in. The second approach is to develop retrieval-augmented generation methods \cite{gu2018search,weston-etal-2018-retrieve,cai-etal-2019-retrieval,lewis2020retrieval} by combining a retrieval-based component and a generative component (e.g. PLM, LLM, etc.).  
% Specifically, we denote the generative model by $f$ and input text by $x$. Traditional generative models focus on predicting output $y$ by $y = f(x)$. Denote the retriever module by $g$, and we could retrieve segments of information $c^r$ based on (parts of) the input $x^r \in x$. Thus, the retriever can predict $c^r = g(x^r)$. Then, the retrieval-augmented generation can be formulated as: $y = f(x, c)$, where $c = \{\langle x^r,c^r \rangle\}$ is a set of relevant instances retrieved from either the original training set or external datasets to improve response generation. The primary concept behind this approach is that $c^r$ can aid in generating a better response if it is similar or relevant to the input $x^r$. The retrieval memory can be obtained from three sources: the training corpus, external datasets, and large-scale unsupervised corpus \cite{li2022survey}.
% , where $x^r = \emptyset$ for unsupervised retrieval sources \cite{li2022survey}. }

RAG is applied to a wide range of downstream NLP tasks, including machine translation~\cite{gu2018search, zhang-etal-2018-guiding, xu2020boosting, he2021fast}, dialogue generation \cite{weston-etal-2018-retrieve, wu2019response,cai-etal-2019-skeleton}, abstractive summarization \cite{peng-etal-2019-text}, and knowledge-intensive generation \cite{lewis2020retrieval,izacard-grave-2021-leveraging}. Among them, most methods focus on retrieving textual information.
% where there exist two \red{primary} types of retrievers : dense and sparse \cite{mialon2023augmented}. Sparse retrievers \cite{robertson2009probabilistic} use sparse bag-of-words representations, while dense neural retrievers \cite{asai2022task} use dense query and document vectors. Besides them, 
For example, \citet{guu2020retrieval, lewis2020retrieval, borgeaud2022improving, izacard2022atlas} jointly train a retrieval system with an encoder or sequence-to-sequence LM, achieving comparable performance to larger LMs that use significantly more parameters. 
% These models include REALM~\cite{guu2020retrieval}, RAG~\cite{lewis2020retrieval}, and RETRO~\cite{borgeaud2022improving}, which integrate retrieval into existing pre-trained LMs, and Atlas~\cite{izacard2022atlas}, which obtains strong few-shot learning capabilities despite being much smaller than other large LMs. 
Recent research also proposes combining a retriever with chain-of-thought (CoT) prompting for reasoning to augment language models \cite{he2022rethinking,trivedi2022interleaving,zhao2023verifyandedit}. 
% For example, \citet{zhao2023verifyandedit} verifies the validity of CoT reasoning steps and retrieves relevant contexts to augment the generation of the uncertain ones. \citet{he2022rethinking} generate reasoning paths using CoT prompts and retrieve knowledge to support the explanations and predictions. \citet{trivedi2022interleaving} propose an information retrieval CoT approach for multi-step question answering, where retrieval guides CoT reasoning and vice versa.




% The next part:
% 1. retriever (dense / sparse)
% 2. generation (PLM / LLM)
% 3. CoT + RAG (Lecun's survey, emerging trend)
% 4. Applications on Downstream tasks (li's survey)



% where $z = \{\langle x^r,y^r \rangle\}$ is a set of relevant instances retrieved from the original training set or external datasets. The main idea of this paradigm is that $y^r$ may benefit the response generation, if $x^r$ (or $y^r$) is similar (or relevant) to the input $x$. It is worth noting that $x^r = \emptyset$ when unsupervised retrieval sources are used. In general, the retrieval memory can be retrieved from three kinds of sources: the training corpus, external datasets in the same format with the training corpus, and large-scale unsupervised corpus.

% The problem has been redefined in terms of retrieving the appropriate information from knowledge base, by utilizing the non-parametric index as a means of organizing knowledge.
% The retrieval-augmented generation
% can be further formulated as:
% y = f(x, z) (1)
% where z = {hx
% r
% , y
% r
% i} is a set of relevant instances
% retrieved from the original training set or external
% datasets.