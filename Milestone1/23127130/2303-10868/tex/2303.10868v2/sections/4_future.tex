
% \vspace{-1ex}
\subsection{Retrieval Augmented Multimodal Reasoning}
\vspace{-1ex}
%including both cot and other formats of reasoning (kosmos-1)

% \begin{quote}
% \small 
% \emph{The words of the language, as they are written or spoken, do not seem to play any role in my mechanism of thought. The psychical entities which seem to serve as elements in thought are certain signs and more or less clear images which can be "voluntarily" reproduced and combined. --- Albert Einstein}
% \end{quote}

One potential application of multimodal RAG
% information retrieval 
is multimodal reasoning. \citet{lu2022learn} first introduces ScienceQA, a large-scale multimodal science question dataset annotated with lectures and explanations. Then, \citet{zhang2023multimodal} proposes Multimodal Chain-of-Thought (Multimodal-CoT) which incorporates language and vision modalities into a two-stage (rationale generation and answer inference) framework, surpassing GPT-3.5 by a large margin with a much smaller fine-tuned model. Similar to \citet{zhang2023multimodal}, kosmos-1 \citep{huang2023language} breaks down multimodal reasoning into two steps. It first generates intermediate content as the rationale based on visual information and then uses the generated rationale to induce the result. However, both methods may have difficulties understanding certain types of images (e.g., maps), which could be mitigated by retrieving informative image-text pairs. 
% \red{With the advances in multimodal retrieval, we believe that utilizing RAG for multimodal reasoning would be a promising research direction.}
% We hope that future work can pay more attention to how to effectively and efficiently combine multimodal reasoning with multimodal retrieval.

\vspace{-1ex}
\subsection{Building a Multimodal Knowledge Index}
% \vspace{-1ex}
% Assigned with Fangkai

In order to facilitate multimodal RAG, one of the most fundamental aspects is building a multimodal knowledge index. The goal is twofold: Firstly, dense representations should support low storage, dynamic updating of the knowledge base, and accurate search. Secondly, it could enable faster search speed with the help of local sensitive hashing~\citep{data-mining}, which combats scaling and robustness concerns when the knowledge base is scaled up extremely.

Currently, the dense representations for text snippets are widely studied for documents~\cite{karpukhin-etal-2020-dense,gao-callan-2021-condenser,gao-etal-2021-simcse}, entities~\citep{sciavolino-etal-2021-simple,lee-etal-2021-learning-dense}, and images~\cite{clip}. Besides, there are studies optimizing dense representations in an end-to-end manner~\cite{lewis2020retrieval}.
Nevertheless, few papers~\citep{chen2022murag} have explored building 
a multimodal index at the same time for downstream generation tasks.
% , and are also limited in text and image.
How to map a multimodal knowledge index into a unified space remains a long-term challenge.

\vspace{-1ex}
\subsection{Pretraining with Multimodal Retrieval}

To better align the abilities to handle different modalities in a pre-trained model, future work could be built on employing retrieval-based approaches during pre-training. Currently, some methods fine-tune the pre-trained generative model to learn to retrieve from different modalities.
% for retrieval. 
For example, LaMDA~\citep{thoppilan2022lamda} calls an external toolset for fine-tuning, including an information retrieval system.
% , a calendar, and a calculator. 
Similarly, during fine-tuning, Toolformer \citep{schick2023toolformer} augments models with API calls to tools including a QA system and a Wikipedia search engine. 

When similar retrieval abilities are leveraged during pretraining, the generative models can interact with retrieval tools much better. Then, instead of relying solely on internal weights, they could effectively use an external base to output more grounded information, provide relevant contexts to users, and update their information accordingly. Such pretraining techniques would also greatly improve robustness for out-of-domain tasks. As an example, \citet{guu2020realm} augments pretraining with an external knowledge retriever, which outperforms previous methods.
% When new information comes in, the generative model would be able to effectively retrieve from an up-to-date external base instead of relying solely on pre-trained weights. 
% This advantage also expands to handling robustness in out-of-domain questions.

To incorporate retrieval with pretraining, there remains the challenge of developing appropriate datasets labeled with retrieval API calls. To tackle this challenge, LaMDA \citep{thoppilan2022lamda} uses labels developed by human annotators, which could be expensive to collect. Toolformer \citep{schick2023toolformer} uses a sampling and filtering approach for automatic labeling, which is inexpensive but could induce bias. A potential solution is to use a neuro-symbolic approach~\citep{davoudi2021toward}, which uses prototype learning and deep-KNN to find nearest neighbors during training.