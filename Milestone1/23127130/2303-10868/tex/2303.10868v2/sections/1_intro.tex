% one- Motivation

% WHY MULTI-MODAL?
% LLM generation ability & becoming more multi-modal
Generative Artificial Intelligence (GAI) has demonstrated impressive performances in tasks such as text generation \citep{ouyang2022training, chowdhery2022palm, brown2020language} and text-to-image generation \citep{DALL-E, poole2022dreamfusion}. The recent advancements in Multimodal Large Language Models (MLLMs) \citep{driess2023palm, gpt4, huang2023language} have further improved the models' capabilities to handle multi-format information, opening up possibilities for developing general-purpose learners. 

% WHY RETRIEVAL?
% limitations of generative models
% how it could be addressed by retrieval-based
% However, generative models suffer from inevitable limitations, such as hallucinations \citep{ye2022unreliability}, arithmetic difficulties \citep{math}, lack of interpretability \fk{outdated knowledge}. Thus, a promising solution for generative models is learning to interact with the external world and retrieve knowledge in different formats, thus augmenting their generation abilities \citep{mialon2023augmented}. 
Nevertheless, generative models are not exempt from inherent limitations, including the tendency for generating hallucinations~\citep{ye2022unreliability}, struggling with arithmetic tasks~\citep{math}, and lacking interpretability
% , and relying on outdated knowledge
. Consequently, a promising solution for enhancing their capabilities lies in enabling them to interact with the external world and acquire knowledge in diverse formats and modalities, thereby improving the factuality and rationality of the generated content.
Recently, there have been emerging studies focusing on retrieval-augmented approaches \citep{mialon2023augmented}, which aim to provide information that is more grounded and factually dependent. Among them, most \citep{nakano2021webgpt, guu2020retrieval} retrieves textual information, 
% retrieved from the web or textual corpora. Although the textual format 
which matches the data format used during pre-training and offers a natural medium for interaction. However, there is more world knowledge stored in different structures and modalities, such as images and videos, which is often inaccessible, unavailable, or not describable in traditional textual corpora.

% WHY DO A SURVEY
% thus retrieval multi-modal generation models
% \red{The popularity of LLMs has also introduced challenges: As their parameter size is often infeasible for tuning, researchers are now attempting to interact with them in a retrieval-based manner.} 
Therefore, there arises an important research intersection that retrieves multimodal knowledge to augment generative models. It offers a promising solution to current challenges such as factuality, reasoning, interpretability, and robustness. As this field is very recent, there lacks a unified understanding of recognizing these methods as a specific group, visualizing their innate connections, connecting their methodologies, and outlining their applications.

% two- our contribution
% classify and discuss in different modalities

Therefore, we survey recent advancements in multimodal retrieval-augmented generation (RAG). Specifically, we discuss current research by grouping them into different modalities, including image, code, structured knowledge, audio, and video.
% For each modality, there are often differences in retrieval and synthesis procedures, goals, and targeted tasks.
% a brief introduction of each modality
% \red{Therefore, we individually discuss} the previous researc, the current states, and future challenges. 
% We  collect xx papers for detailed analysis. 
% As this field is very recent, many researc remain unpublished in mainstream conferences but are already gaining popularity. Therefore, 
For each modality, we systematically search the ACL Anthology and Google Scholar with relevant keywords and perform manual filtering to determine their relevance to the survey. As a result, we collect 146 papers for detailed analysis. 
% Details on the search can be found 
In \Cref{subsec:search}, we include search details, statistics, and a trend analysis figure, which shows that multimodal RAG papers have indeed developed very fastly since the emergence of large-scale general-purpose models. Within each modality, we discuss relevant papers by grouping them under different applications. By providing an in-depth survey, we hope to help researchers recognize the importance of incorporating knowledge in different formats and encourage adaptation and advancements on existing techniques to the fast-growing field of LLMs.
% For each paper, we give an informative summary with main focus along xx important research questions: xx. To analyze the trend, we provide a timetable of the methods. We also include a high-level classification of these methods in a detailed table.


In summary, our contributions are as follows:

\vspace{-0.5ex}
\begin{itemize}[leftmargin=*,topsep=2pt,itemsep=2pt,parsep=0pt]
\item We establish retrieval augmented generation with multi-modality as an important group of methods that emerges with the recent advances in LLMs.
\item For common modalities, we provide an in-depth review of research papers by contextualizing their innate connections and shared challenges.
\item We provide an informative analysis of future directions, which could contain promising solutions to many current challenges.
\end{itemize}
\vspace{-0.5ex}

% For example, in the image domain, retrieval-augmented methods have been used to better ground visual question-answering (VQA) tasks \citep{chen-etal-2022-murag, Plug-and-Play} and generate more factual captions \citep{Re-ViLM, RA-CM3}. In the code domain, retrieval-based research decouple logic and textual information, which results in more faithful and factual outputs \citep{lyu2023faithful, chen2022program}. To enhance factuality, some methods \citep{thoppilan2022lamda, cheng2022binding} also retrieve grounding contexts from structured knowledge, such as tables and knowledge graphs. Moreover, there are emerging research in combining audio and video retrieval in generative models \citep{he2022recap, bogolin2022querybank}.

% % future directions discussion 
% We believe that the emergence of multimodal retrieval-augmented generation contains the solution to many current challenges. To encourage more future research in this domain, we analyze several promising future directions, including retrieval-augmented multimodal reasoning, building a multimodal knowledge index, and combining retrieval with pre-training.

% % continue adding and perfecting the survey
% As the direction of multimodal retrieval-augmented generation is emerging, we will continue to add new research and expand the scope of our current survey.