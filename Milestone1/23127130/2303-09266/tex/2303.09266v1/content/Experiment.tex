\begin{table*}[th]
\scalebox{0.68}{
\begin{tabular}{c|cc|cc|cc|cc|cc|cc|cc|cc|cc}
\toprule
\makecell{Dataset/\\ Model}      & \multicolumn{2}{c|}{MRPC}  & \multicolumn{2}{c|}{SST-2}  & \multicolumn{2}{c|}{QNLI}     
                      & \multicolumn{2}{c|}{QQP}   & \multicolumn{2}{c|}{MNLI}   & \multicolumn{2}{c|}{MNLI-mm}       
                      & \multicolumn{2}{c|}{CoLA}  & \multicolumn{2}{c|}{RTE}    & \multicolumn{2}{c}{WNLI}    
\\ \midrule
& F1            & \begin{tabular}[c]{@{}c@{}}FLOPs\\ (cost)\end{tabular}           
& Acc           & \begin{tabular}[c]{@{}c@{}}FLOPs\\ (cost)\end{tabular}           
& Acc           & \begin{tabular}[c]{@{}c@{}}FLOPs\\ (cost)\end{tabular}           
& F1            & \begin{tabular}[c]{@{}c@{}}FLOPs\\ (cost)\end{tabular}           
& Acc           & \begin{tabular}[c]{@{}c@{}}FLOPs\\ (cost)\end{tabular}           
& Acc           & \begin{tabular}[c]{@{}c@{}}FLOPs\\ (cost)\end{tabular}           
& Mcc           & \begin{tabular}[c]{@{}c@{}}FLOPs\\ (cost)\end{tabular}           
& Acc           & \begin{tabular}[c]{@{}c@{}}FLOPs\\ (cost)\end{tabular}           
& Acc           & \begin{tabular}[c]{@{}c@{}}FLOPs\\ (cost)\end{tabular}           
\\ \midrule
BERT                                                       & 88.5          & \begin{tabular}[c]{@{}c@{}}21744M\\ (100\%)\end{tabular}        & \textbf{93.2}
& \begin{tabular}[c]{@{}c@{}}21744M\\ (100\%)\end{tabular}        & \textbf{88.1} & \begin{tabular}[c]{@{}c@{}}21744M\\ (100\%)\end{tabular}        & \textbf{84.4} & \begin{tabular}[c]{@{}c@{}}21744M\\ (100\%)\end{tabular}        & \textbf{84.6} & \begin{tabular}[c]{@{}c@{}}21744M\\ (100\%)\end{tabular}        & \textbf{84.8} & \begin{tabular}[c]{@{}c@{}}21744M\\ (100\%)\end{tabular}        & 50.8          & \begin{tabular}[c]{@{}c@{}}21744M\\ (100\%)\end{tabular}        & \textbf{61.0} & \begin{tabular}[c]{@{}c@{}}21744M\\ (100\%)\end{tabular}        & 56.3 & \begin{tabular}[c]{@{}c@{}}21744M\\ (100\%)\end{tabular}        
\\ \midrule
DistillBERT                                                       & 87.0          & \textbf{\begin{tabular}[c]{@{}c@{}}10872M\\ (50\%)\end{tabular} }       & 91.5 & \begin{tabular}[c]{@{}c@{}}10872M\\ (50\%)\end{tabular}        & 86.8 & \begin{tabular}[c]{@{}c@{}}10872M\\ (50\%)\end{tabular}        & 82.4 & \begin{tabular}[c]{@{}c@{}}10872M\\ (50\%)\end{tabular}        & 81.9 & \begin{tabular}[c]{@{}c@{}}10872M\\ (50\%)\end{tabular}        & 81.8 & \begin{tabular}[c]{@{}c@{}}10872M\\ (50\%)\end{tabular}        & 47.5          & \begin{tabular}[c]{@{}c@{}}10872M\\ (50\%)\end{tabular}        & 59.2 & \textbf{\begin{tabular}[c]{@{}c@{}}10872M\\ (50\%)\end{tabular} }       & 55.3 & \begin{tabular}[c]{@{}c@{}}10872M\\ (50\%)\end{tabular}        
\\ \midrule
\begin{tabular}[c]{@{}c@{}}DeeBERT\\ (S=0.1)\end{tabular}  
& 88.4          & \begin{tabular}[c]{@{}c@{}}21744M\\ (100\%)\end{tabular}        
& 92.4          & \begin{tabular}[c]{@{}c@{}}13900M\\ (63\%)\end{tabular}          
& 87.5          & \begin{tabular}[c]{@{}c@{}}20008M\\ (92\%)\end{tabular}         
& 84.2          & \begin{tabular}[c]{@{}c@{}}17578M\\ (80\%)\end{tabular}          
& 84.2          & \begin{tabular}[c]{@{}c@{}}20796M\\ (95\%)\end{tabular}         
& 84.3          & \begin{tabular}[c]{@{}c@{}}20786M\\ (95\%)\end{tabular}         
& 49.9          & \begin{tabular}[c]{@{}c@{}}21840M\\ (100\%)\end{tabular}         
& 59.9          & \begin{tabular}[c]{@{}c@{}}21758M\\ (100\%)\end{tabular}       
& 56.3          & \begin{tabular}[c]{@{}c@{}}21758M\\ (100\%)\end{tabular}        \\
\begin{tabular}[c]{@{}c@{}}DeeBERT\\ (S=0.3)\end{tabular}  
& 88.2          & \begin{tabular}[c]{@{}c@{}}19726M\\ (90\%)\end{tabular}          
& 90.3          & \begin{tabular}[c]{@{}c@{}}10874M\\ (50\%)\end{tabular}          
& 87.1          & \begin{tabular}[c]{@{}c@{}}15770M\\ (72\%)\end{tabular}          
& 83.1          & \begin{tabular}[c]{@{}c@{}}14750M\\ (67\%)\end{tabular}          
& 84.2          & \begin{tabular}[c]{@{}c@{}}19698M\\ (90\%)\end{tabular}          
& 84.1          & \begin{tabular}[c]{@{}c@{}}18148M\\ (83\%)\end{tabular}          
& 49.9          & \begin{tabular}[c]{@{}c@{}}16850M\\ (77\%)\end{tabular}         
& 59.9          & \begin{tabular}[c]{@{}c@{}}21718M\\ (99\%)\end{tabular}         
& 56.3          & \begin{tabular}[c]{@{}c@{}}21758M\\ (100\%)\end{tabular}        \\
\begin{tabular}[c]{@{}c@{}}DeeBERT\\ (S=0.5)\end{tabular}  
& 87.2          & \begin{tabular}[c]{@{}c@{}}13540M\\ (62\%)\end{tabular} 
& 85.2          & \begin{tabular}[c]{@{}c@{}}7468M\\ (34\%)\end{tabular}          
& 84.9          & \begin{tabular}[c]{@{}c@{}}10502M\\ (48\%)\end{tabular}          
& 74.5          & \begin{tabular}[c]{@{}c@{}}8744M\\ (40\%)\end{tabular}          
& 83.9          & \begin{tabular}[c]{@{}c@{}}18728M\\ (86\%)\end{tabular}          
& 83.1          & \begin{tabular}[c]{@{}c@{}}16362M\\ (75\%)\end{tabular}         
& 49.9          & \begin{tabular}[c]{@{}c@{}}15130M\\ (70\%)\end{tabular}         
& 59.9          & \begin{tabular}[c]{@{}c@{}}21332M\\ (98\%)\end{tabular}         
& 56.3          & \begin{tabular}[c]{@{}c@{}}21758M\\ (100\%)\end{tabular}        
\\ \midrule
\begin{tabular}[c]{@{}c@{}}FastBERT\\ (S=0.1)\end{tabular} 
& 88.5          & \begin{tabular}[c]{@{}c@{}}22196M\\ (102\%)\end{tabular}        
& 92.4          & \begin{tabular}[c]{@{}c@{}}11094M\\ (51\%)\end{tabular}          
& 87.5          & \begin{tabular}[c]{@{}c@{}}19684M\\ (90\%)\end{tabular}          
& 83.9          & \begin{tabular}[c]{@{}c@{}}15808M\\ (72\%)\end{tabular}          
& 83.4          & \begin{tabular}[c]{@{}c@{}}19980M\\ (91\%)\end{tabular}         
& 84.0          & \begin{tabular}[c]{@{}c@{}}19116M\\ (87\%)\end{tabular}       
& 49.8          & \begin{tabular}[c]{@{}c@{}}20846M\\ (95\%)\end{tabular}        
& 59.4          & \begin{tabular}[c]{@{}c@{}}22196M\\ (102\%)\end{tabular}        
& 56.3          & \begin{tabular}[c]{@{}c@{}}22196M\\ (102\%)\end{tabular}        \\
\begin{tabular}[c]{@{}c@{}}FastBERT\\ (S=0.3)\end{tabular} 
& 88.5          & \begin{tabular}[c]{@{}c@{}}20278M\\ (93\%)\end{tabular}         
& 90.7          & \begin{tabular}[c]{@{}c@{}}6704M\\ (30\%)\end{tabular}          
& 86.8          & \begin{tabular}[c]{@{}c@{}}13096M\\ (60\%)\end{tabular}          
& 82.9          & \begin{tabular}[c]{@{}c@{}}10586M\\ (48\%)\end{tabular}          
& 83.1          & \begin{tabular}[c]{@{}c@{}}15468M\\ (71\%)\end{tabular}          
& 83.6          & \begin{tabular}[c]{@{}c@{}}15358M\\ (70\%)\end{tabular}         
& 49.5          & \begin{tabular}[c]{@{}c@{}}15262M\\ (70\%)\end{tabular}         
& 59.4          & \begin{tabular}[c]{@{}c@{}}22196M\\ (102\%)\end{tabular}       
& 56.3          & \begin{tabular}[c]{@{}c@{}}22196M\\ (102\%)\end{tabular}        \\
\begin{tabular}[c]{@{}c@{}}FastBERT\\ (S=0.5)\end{tabular} 
& 88.3          & \begin{tabular}[c]{@{}c@{}}16478M\\ (75\%)\end{tabular}         
& 86.8          & \begin{tabular}[c]{@{}c@{}}4076M\\ (18\%)\end{tabular}          
& 84.5          & \begin{tabular}[c]{@{}c@{}}7964M\\ (36\%)\end{tabular}          
& 78.2          & \begin{tabular}[c]{@{}c@{}}6064M\\ (27\%)\end{tabular}          
& 81.3          & \begin{tabular}[c]{@{}c@{}}11958M\\ (54\%)\end{tabular}          
& 81.3          & \begin{tabular}[c]{@{}c@{}}11020M\\ (50\%)\end{tabular}         
& 44.4          & \begin{tabular}[c]{@{}c@{}}12700M\\ (58\%)\end{tabular}         
& 59.4          & \begin{tabular}[c]{@{}c@{}}22196M\\ (102\%)\end{tabular}        
& 56.3          & \begin{tabular}[c]{@{}c@{}}22196M\\ (102\%)\end{tabular}        
\\ \midrule
\begin{tabular}[c]{@{}c@{}}Ours\\ (S=0.1)\end{tabular}     
& \textbf{89.8} & \begin{tabular}[c]{@{}c@{}}21390M\\ (98\%)\end{tabular}         
& 93.1          & \begin{tabular}[c]{@{}c@{}}9040M\\ (41\%)\end{tabular}          
& 87.9          & \begin{tabular}[c]{@{}c@{}}16512M\\ (75\%)\end{tabular}          
& 84.1          & \begin{tabular}[c]{@{}c@{}}13128M\\ (60\%)\end{tabular}          
& 84.4          & \begin{tabular}[c]{@{}c@{}}18436M\\ (84\%)\end{tabular}          
& 84.6          & \begin{tabular}[c]{@{}c@{}}18196M\\ (83\%)\end{tabular}          
& \textbf{51.5} & \begin{tabular}[c]{@{}c@{}}20568M\\ (94\%)\end{tabular}         
& 60.6          & \begin{tabular}[c]{@{}c@{}}12684M\\ (58\%)\end{tabular} 
& 56.3  & \textbf{\begin{tabular}[c]{@{}c@{}}6768M\\ (31\%)\end{tabular}} \\
\begin{tabular}[c]{@{}c@{}}Ours\\ (S=0.3)\end{tabular}     
& 89.6          & \begin{tabular}[c]{@{}c@{}}17884M\\ (82\%)\end{tabular}         
& 91.9          & \begin{tabular}[c]{@{}c@{}}5706M\\ (26\%)\end{tabular}          
& 87.0          & \begin{tabular}[c]{@{}c@{}}10878M\\ (50\%)\end{tabular}          
& 82.5          & \begin{tabular}[c]{@{}c@{}}8610M\\ (39\%)\end{tabular}          
& 83.6          & \begin{tabular}[c]{@{}c@{}}14442M\\ (66\%)\end{tabular}          
& 83.9          & \begin{tabular}[c]{@{}c@{}}13936M\\ (64\%)\end{tabular}          
& 51.3          & \begin{tabular}[c]{@{}c@{}}15072M\\ (69\%)\end{tabular}         
& 60.6          & \begin{tabular}[c]{@{}c@{}}12684M\\ (58\%)\end{tabular} 
& 56.3 & \textbf{\begin{tabular}[c]{@{}c@{}}6768M\\ (31\%)\end{tabular}} \\
\begin{tabular}[c]{@{}c@{}}Ours\\ (S=0.5)\end{tabular}     
& 88.9          & \begin{tabular}[c]{@{}c@{}}14348M\\ (65\%)\end{tabular}          
& 87.7          & \textbf{\begin{tabular}[c]{@{}c@{}}3608M\\ (16\%)\end{tabular}}
& 84.9          & \textbf{\begin{tabular}[c]{@{}c@{}}7020M\\ (32\%)\end{tabular}}
& 78.0          & \textbf{\begin{tabular}[c]{@{}c@{}}5130M\\ (23\%)\end{tabular}} 
& 82.4          & \textbf{\begin{tabular}[c]{@{}c@{}}10450M\\ (49\%)\end{tabular}}
& 82.0          & \textbf{\begin{tabular}[c]{@{}c@{}}10570M\\ (48\%)\end{tabular}}
& 49.9          & \textbf{\begin{tabular}[c]{@{}c@{}}10468M\\ (49\%)\end{tabular}}
& 60.6          & \begin{tabular}[c]{@{}c@{}}12684M\\ (58\%)\end{tabular} 
& 56.3 & \textbf{\begin{tabular}[c]{@{}c@{}}6768M\\ (31\%)\end{tabular}} \\ 
\bottomrule
\end{tabular}}
\caption{Comparison between baselines(BERT,DistillBERT,FastBERT,DeeBERT) and \our \space on the GLUE benchmark. \emph{MACs} are multiply–accumulate operations which represent computational complexity. \emph{S} represents the entropy threshold, and \emph{cost} is the computational cost.}
\label{tab:tabel2}
\end{table*}

\begin{figure*}[htpb]
    \centering
    \includegraphics[scale=0.65]{picture/comparison.eps}
    \caption{Comparison between baselines and \our \space on SST-2, QNLI, QQP, MNLI, CoLA, MRPC, showing the influence of computational cost on performance(Accuracy/F1 score/MCC).}
    \label{fig:comparsion}
\end{figure*}


\section{Experiments}


\subsection{Baselines}
We compare our method with three baselines:
\begin{itemize}
    \item \textbf{BERT.} A large-scale pre-trained language model based on Transformer, which we use as the backbone of our methods. In experiments, we only use the BERT-base model which is pre-trained by Google. \cite{devlin-etal-2019-bert}
    
    \item \textbf{DistillBERT.} It is a smaller transformer-based model by distilling the BERT. \cite{sanh2019distilbert}
    
    \item \textbf{Early exiting model.} The dynamic early exiting method for BERT is an effective method to accelerate BERT inference. We choose some classical methods like \textbf{DeeBERT} \cite{xin2020deebert} and \textbf{FastBERT} \cite{liu2020fastbert} as our baselines, which use the early exiting mechanism based on entropy.



\end{itemize}

\subsection{Datasets}
To verify the effectiveness of our methods, We conduct experiments on eight classification datasets of the GLUE benchmark\cite{wang2018glue}, including SST-2 \cite{socher2013recursive}, CoLA \cite{warstadt2019neural}, MRPC \cite{dolan2005automatically}, MNLI \cite{williams2017broad}, QQP \cite{chen2018quora}, QNLI \cite{rajpurkar2016squad}, RTE \cite{bentivogli2009fifth}, and WNLI \cite{levesque2012winograd}.

\subsection{Experimental Setup}
The experiments are done on an NVIDIA 2080Ti GPU. We adopt the same parameters for BERT, DeeBERT, FastBERT, and \our. In experiment, these models use the pre-trained parameters(\textbf{bert-base-uncased}) released by the Hugging Face Transformer Library \cite{wolf2019huggingface}. In the pre-trained parameters, the number of transformer layers, the dimension of hidden states, and the max length of the input sentence are set to 12, 768, and 128. We use AdamW \cite{kingma2014adam} to train these models with a default batch size of 32. For each task, we select the best fine-tuning learning rate(among 1e-5,2e-5,5e-5).

In the first stage of \our, we train the model with 5 epochs and select one with the best accuracy for the second stage. In the second stage, we adopt cross-layer contrastive learning to train the classifiers and train these classifiers for 4 epochs. We slightly tune the hyper-parameters across the different tasks.

Following prior work, our batch size of inference is set to 1 in the inference phase. 

In inference, We used MACs(Multiply–Accumulate Operations) as an indicator to evaluate the computational cost. In FastBERT, the author use the FLOPs(floating-point Operations) as the indicator, which is twice as large as MACs. Generally speaking, the size of the MACs reflects the inference speed of the model, and the smaller the MACs of model is, the shorter the inference time will be. Table \ref{tab:table1} presents the computational cost of each operation within the \our, which shows that the computational cost of the Skipping Gate and Classifier is much lower than the Transformer Layer. 

\begin{table}[b]
\centering
\begin{tabular}{clllc}
\toprule
\multicolumn{2}{c}{Operation}              &&& FLOPs  \\ 
\midrule
\multicolumn{2}{c}{Each Transformer Layer} &&& 1811.8M \\
\midrule
\multicolumn{2}{c}{Each Classifier}        &&& 37.6M  \\
\midrule
\multicolumn{2}{c}{Each Skipping Gate}     &&& 37.4M \\
\bottomrule
\end{tabular}
\caption{This experiment evaluates each operation in the \our, using MACs as the indicator.}
\label{tab:table1}
\end{table}



\subsection{Main Results}
We evaluate these models in eight classification datasets of the GLUE benchmark and select different entropy thresholds to test the performance and sample-averaged MACs for DeeBERT, FastBERT and \our. We set three different entropy thresholds for early exiting models and compare the results with other baselines in Table \ref{tab:tabel2}. The results of the experiment show that our model achieves 2-3× computation reduction with minimal accuracy drops and even has a better performance than BERT. In the results, due to DistillBert cannot adaptively change the model architecture according to the sample complexity, it has a fixed computational complexity. In other words, the computational cost of DistillBert is independent of sample difficulty. In fact, the difficulty of each dataset is different, and in more datasets, our method has a better performance than DistillBert.



In Figure \ref{fig:comparsion}, we set several entropy thresholds and compare different models' tradeoffs in accuracy and computational cost on some GLUE datasets. From Figure \ref{fig:comparsion}, we conclude that: \begin{enumerate}
    \item  Compared with FastBERT and DeeBERT, \our\space have a better performance at the same computational cost, which proves our model is more effective than other approaches.
    \item In all datasets, the scores(accuracy/F1 score/MCC) of these models start to decline when the computational costs reach certain values which verify the redundant computation in original BERT, and \our\space dominates the performance under different computation costs comparing with FastBERT and DeeBERT.
\end{enumerate}





\subsection{Effectiveness of Skipping Mechanism}
The early exiting mechanism is based on the entropy of the corresponding classifier output. However, previous methods hardly reduce computation cost when samples are complex and difficult enough. Because the entropy of classifier output will be extremely high that leads to most exiting classifiers do not exit early. In Table \ref{tab:tabel2}, we can see that even if the threshold \emph{S} is set as 0.5, the DeeBERT and FastBERT still have an extremely high computational cost for RTE and WNLI dataset, which means early exiting mechanism is invalid on such situations. But \our \space achieves a relatively large computation reduction with minimal accuracy drop, which shows the skipping mechanism is effective. 

To further prove the effectiveness of the layer skipping mechanism, we disable the early exiting mechanism and only use the layer skipping mechanism during the inference phase. The empirical results are shown in Table \ref{tab:table3}, which proves that the skipping mechanism is able to reduce the computational cost compared with BERT. Moreover, combined Table \ref{tab:tabel2} and Table \ref{tab:table3}, MACs and Accuracy are hardly changed on the RTE and WNLI datasets, which further proves that the early exiting mechanism is invalid in difficult datasets and layer skipping mechanism plays an important role in our method on such situations. 


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \begin{table}[]
% \begin{tabular}{c|cc|cc|cc}
% \toprule
% \multirow{2}{*}{Dataset/\\ Model} & \multicolumn{2}{c|}{\multirow{2}{*}{SST-2}}                                      & \multicolumn{2}{c|}{\multirow{2}{*}{RTE}}                                        & \multicolumn{2}{c}{\multirow{2}{*}{WNLI}}                                       \\
% \midrule                                                                          
% & Acc  & \begin{tabular}[c]{@{}c@{}}FLOPs\\ (cost)\end{tabular}   
% & Acc  & \begin{tabular}[c]{@{}c@{}}FLOPs\\ (cost)\end{tabular}   
% & Acc  & \begin{tabular}[c]{@{}c@{}}FLOPs\\ (cost)\end{tabular}   \\
% \midrule 
% BERT                                                                      
% & 93.2 & \begin{tabular}[c]{@{}c@{}}21744\\ (100\%)\end{tabular} 
% & 61.0 & \begin{tabular}[c]{@{}c@{}}21744\\ (100\%)\end{tabular} 
% & 56.3 & \begin{tabular}[c]{@{}c@{}}21744\\ (100\%)\end{tabular} \\
% \midrule
% Skip                                                                      
% & 93.1 & \begin{tabular}[c]{@{}c@{}}14500\\ (66\%)\end{tabular}   
% & 60.6 & \begin{tabular}[c]{@{}c@{}}12190\\ (56\%)\end{tabular}   
% & 56.3 & \begin{tabular}[c]{@{}c@{}}6360\\(29\%)\end{tabular}\\  
% \bottomrule
% \end{tabular}
% \caption{\emph{Skip} represents our methods only use the skipping mechanism and disable the early exiting mechanism. This experiment is conducted on the SST-2, RTE and WNLI datasets.}
% \label{tab:table3}
% \end{table}

\begin{table}[]
\scalebox{0.9}{
\begin{tabular}{c|cc|cc|cc}
\toprule
\makecell{Dataset/\\ Model} & \multicolumn{2}{c|}{SST-2}    & \multicolumn{2}{c|}{RTE}      & \multicolumn{2}{c}{WNLI}                                     \\
\midrule                                                                          
& Acc  & \makecell{FLOPs\\ (cost)}
& Acc  & \makecell{FLOPs\\ (cost)}   
& Acc  & \makecell{FLOPs\\ (cost)}   \\
\midrule 
BERT                                                                      
& 93.2 & \makecell{21744\\ (100\%)} 
& 61.0 & \makecell{21744\\ (100\%)} 
& 56.3 & \makecell{21744\\ (100\%)} \\
\midrule
Skip                                                                      
& 93.1 & \makecell{14500\\ (66\%)}   
& 60.6 & \makecell{12190\\ (56\%)}   
& 56.3 & \makecell{6360\\(29\%)}\\  
\bottomrule
\end{tabular}}
\caption{\emph{Skip} represents our methods only use the skipping mechanism and disable the early exiting mechanism. This experiment is conducted on the SST-2, RTE and WNLI datasets.}
\label{tab:table3}
\end{table}


\subsection{Effectiveness of The Cross-Layer Contrastive Learning}
We have introduced the cross-layer contrastive learning, which takes the same tokens across consecutive layers as positive pairs and the different tokens across consecutive layers as negative pairs. To further evaluate the effectiveness of contrastive learning, we compare the two cases of using contrastive learning and not using contrastive learning. Empirical results are shown in Figure \ref{fig:contrast}, which shows cross-layer contrastive learning is effective for further reducing the computation.

\begin{figure}[htbp]
    \centering
    \includegraphics[scale = 0.30]{picture/contrast.eps}
    \caption{\emph{contrast} represents using the cross-layer contrastive learning, \emph{w/o contrast} represents not using the cross-layer contrastive learning.}
    \label{fig:contrast}
\end{figure}

\subsection{Effectiveness of The Special Training Way}
In the previous section, we introduce the soft weight mechanism and hard weight mechanism. Meanwhile, we propose a special training way from soft weight mechanism to hard weight mechanism. In the experiment, we compare three different training strategies: (1) the soft weight mechanism: using the continuous probability value of skipping gates for training but using the discrete value of skipping gates for inference, (2) the hard weight mechanism: using the discrete value of skipping gates for training and inference and (3) the special training way: firstly using continuous probability value for training, and then using the discrete value for training. 

We compare these methods on the SST-2 and QNLI datasets, and the results are shown in Figure \ref{fig:gate}. The results prove that the special training way is effective for balancing the inconsistent usage of skipping gates between training and inference phase.

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.30]{picture/hard_mechanism.eps}
    \caption{\emph{Hard} represents the hard weight mechanism, \emph{Soft} represents the soft weight mechanism, and \emph{SP} uses the special training way from soft mechanism to hard mechanism.}
    \label{fig:gate}
\end{figure}

