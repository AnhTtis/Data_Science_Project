\section{Conclusion}
In this paper, we propose a novel dynamic early exiting combined with layer skipping for BERT inference. To address the inconsistent usage of skipping gates in the inference and training phase, we propose hard weight mechanism and a special training way. In addition, we propose cross-layer contrastive learning and combine it into our training phases to boost the intermediate layers and classifiers which can further reduce computation cost. We evaluated our model on eight classification datasets of the GLUE benchmark. Empirical results show that \our \space could achieve performance comparable to BERT while significantly reducing computational cost. Compared to other dynamic early exiting models, \our \space obtain better accuracy with lower computation. Moreover, we conduct a series of ablation studies to demonstrate that each component is beneficial. In the future, we will adopt our methods to other pre-trained language models. We also will further study the combination of other acceleration methods and layer skipping mechanism.