\section{Methodology}

In this section, we first introduce the architecture of \our, which combines the early exiting mechanism and layer skipping mechanism. Secondly, we will illustrate how to combine cross-layer contrastive learning into the training phase to obtain powerful classifiers. Lastly, we introduce a hard weight mechanism and a special training way for skipping gates, further keeping the consistent usage of skipping gates between the training and inference phases.
\subsection{Preliminaries}
Given an input sentence $\mathbf{S}=\left\{\mathbf{w}_{1}, \mathbf{w}_{2}, \ldots, \mathbf{w}_{N}\right\} \in \mathbb{R}^{N \times W}\ $, $N$ and $L$ stand for the number of tokens and encoder layers respectively. $\mathbf{X}^{i} \in \mathbb{R}^{N \times D}$ represents the output of the $i^{\text{th}}$ encoder layer $\mathcal{E}^{i}(\cdot)$. $\mathbf{X}^{0}$ stands for the output of embedding layer. $\mathcal{G}^{i}(\cdot)$ and $\mathcal{C}^{i}(\cdot)$ represent the skipping gate and classifier, respectively, and the superscript means they are inserted in $i^{\text{th}}$ layer.
\subsection{Model Design}
Different to \cite{liu2020fastbert,xin2020deebert} whose each layer will introduce one exiting classifier, we additionally plug a skipping gate into each layer. In order to understand \our \space better, we will first introduce the skipping gates and then introduce exiting classifiers. 

The skipping gate aims to decide whether to execute or bypass the current layer like Figure \ref{fig:pic2}. 

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.6]{picture/gate.eps}
    \caption{The skipping gate has $p(\text{skip})$ probability to skip the current transformer layer and has $p(\text{not skip})$ probability not to skip. In the inference phase, when $p(\text{skip})$ is greater than $p(\text{not skip})$, the gate will choose to skip, otherwise not to skip.}
    \label{fig:pic2}
\end{figure}
With the skipping gate $\mathcal{G}^{i}(\cdot)$, the output of $i^{th}$ encoder block can be defined:
\begin{equation}
 \mathbf{X}^i = \mathcal{G}^{i}(\mathbf{X}^{i-1})\mathbf{X}^{i-1} + (1 - \mathcal{G}^{i}(\mathbf{X}^{i-1}))\mathcal{E}^{i}(\mathbf{X}^{i-1}), 
\label{eq2}
\end{equation}
where $\mathcal{G}^{i}(\mathbf{X}^{i-1})\subseteq[0,1]$ is the output of the $i^{th}$ skipping gate which represents the probability of skipping the current layer. In the training phase, $\mathcal{G}^{i}(\mathbf{X}^{i-1})$ is a continuous value in the range 0 to 1. However, in the inference phase, $\mathcal{G}^{i}(\mathbf{X}^{i-1})$ is a discrete value that is 0 or 1, and the inconsistent usage of skipping gates between the training and inference phase will be discussed in Section~\ref{hard_weight}. 

Next, we will introduce early exiting classifiers. All early exiting classifiers use the early exiting mechanism based on the entropy, and we define the output of classifiers as:

\begin{equation}
 \mathbf{z}^i = \mathcal{C}^{i}(\mathbf{X}^i), 
\label{eq3}
\end{equation}
where $\mathbf{z}^i \in \mathbb{R}^{1 \times C}$ is a vector that represents the probability of each category.

\subsection{Training}
\our \space is composed of pre-trained BERT, skipping gates, and early exiting classifiers. For stability, we use two stages training strategy for the model, and different stages will train different parts of the model. In this section, we will illustrate two stages separately. 


\subsubsection{First Training Stage}
In the first stage, we train skipping gates and fine-tune pre-trained BERT in a joint way. Other components will be fixed except the last classifier, as shown in Figure \ref{fig:pic3}a. The total loss of the first stage consists of three main parts: classification error, the sloth of skipping, and cross-layer contrastive loss. It can be formalized as
\begin{equation}
\mathcal{L}_{\text {first}} = \mathcal{L}_{\text{CE}}(\mathbf{z}^L,\mathbf{y}) + \frac{\lambda}{\sum\limits^n_{i=1}{\mathcal{G}^i(\mathbf{X}^i)}} + \eta\mathcal{L}_{\text{contra\_1}}
\label{eq4}
\end{equation}
where the first term represents the classification error(e.g., cross-entropy loss) like Equation \ref{eq3}. The second term indicates the sloth of skipping. In order to encourage the gates to skip to some extent rather than only considering the performance, we introduce the second term as a regular term

to enlarge the output of skipping gates. And $\lambda,\eta \in[0,1]$ are scaling factors that are used to balance the influence of the regular terms on the loss. The third term represents a \emph{cross-layer} contrastive loss, which mainly aims to obtain a more powerful BERT. 

In cross-layer contrastive learning, we assume that each token should have similar semantics across the consecutive layers because their representations do not change drastically. The same tokens will be considered as positive pairs across consecutive layers. Otherwise, they shall be deemed to be negative pairs. The cross-layer contrastive loss in the first training stage can be formulated as:
\begin{equation}
\begin{split}
    &\mathcal{L}_{\text{contra\_1}} = \frac{1}{N}\frac{1}{L-1}\sum_{i=1}^{L-1}\sum_{m=1}^{N}\\
    &-\log (\frac{e^{\textnormal{s}({\mathbf{p}}^{i}_m, {\mathbf{p}}^{i+1}_m))/ \tau}}{\sum_{k=1}^{N}e^{\textnormal{s}({\mathbf{p}}^{i}_m, {\mathbf{p}}^{i+1}_k))/ \tau} +  \sum_{k=m}^{N} \mathbbm{1}_{[k \neq i]} e^{\textnormal{s}({\mathbf{p}}^{i}_m, {\mathbf{p}}^{i}_k))/ \tau} }),
\end{split}
\label{contra1}
\end{equation}
where $\mathbf{p}_m^i=f(\boldsymbol{x}_m^i)$, $\boldsymbol{x}_m^i$ represents the $m^{\text{th}}$ word representation in the $i^{\text{th}}$ layer. $f(\cdot)$ is a projection function that maps representations to another latent space where the contrastive loss is calculated. $\textnormal{s}(x,y)$ represents a score function (e.g. cosine similarity), $\mathbbm{1}$ is an indicator function which returns $1$ if $i \neq k$ otherwise returns $0$, and $\tau$ is temperature parameter. In this training stage, we only boost the transformer layer by the cross-layer contrastive loss. 




\begin{figure}[t]
    \centering
    \includegraphics[scale=0.5]{picture/model.eps}
    \caption{The processes of two-stage training stages and inference phase. (a) and (b) show the first and second stages of training, respectively. Note that different components will be trained in different stages. (c) shows the fast inference process. In each layer, the skipping will decide whether to skip the current layer. If it chooses to skip, then the input directly enters the next gate. Otherwise, the input will be fed into the current layer, and the corresponding classifier will decide whether to exit.}
    \label{fig:pic3}
\end{figure}

\subsubsection{Second Training Stage} \label{second}
In the second stage, the trained parameters of the first stage will be frozen, and all classifiers except the last one will be trained as shown in Figure~\ref{fig:pic3}b. In order to obtain more powerful classifiers, we adopt \emph{cross-layer} contrastive learning into the second training phase.
Specifically, the token representations obtained by frozen transformer layers will be fed into classifiers firstly. Secondly, we extract the hidden states that only go through the first layer~(\ie, self-attention layer) of classifiers, and these hidden states will be fed into the projection head to map them into the space where contrastive loss is occupied. Lastly, the contrastive loss~(\ie, InfoNCE loss) is employed. Figure~\ref{fig:contra} shows the overall pipeline. The cross-layer contrastive loss in the second training stage can be formalized as:
\begin{equation}
\begin{split}
    &\ell(\hat{\mathbf{h}}^{i}_m, \hat{\mathbf{h}}^{i+1}_m) = \\
    &-\log (\frac{e^{\textnormal{s}(\hat{\mathbf{h}}^{i}_m, \hat{\mathbf{h}}^{i+1}_m))/ \tau}}{\sum_{k=1}^{N}e^{\textnormal{s}(\hat{\mathbf{h}}^{i}_m, \hat{\mathbf{h}}^{i+1}_k))/ \tau} +  \sum_{k=m}^{N} \mathbbm{1}_{[k \neq i]} e^{\textnormal{s}(\hat{\mathbf{h}}^{i}_m, \hat{\mathbf{h}}^{i}_k))/ \tau} }),
\end{split}
\label{contra}
\end{equation}
where $\hat{\mathbf{h}}^{i}_m=f(\mathbf{h}^{i}_m), \mathbf{h}^{i}_m=\text{Self-Attention}(\mathbf{X}^i)[m]$, $\text{Self-Attention}(\cdot)$ represents a self-attention function and $[m]$ means we only select $m$-th token. Other notations have the same meanings in Equation~\ref{contra1}.
Enumerating all tokens in all layers, the overall loss is:

\begin{equation}
    \mathcal{L}_{\text{contra\_2}}=\frac{1}{N}\frac{1}{L-1}\sum_{i=1}^{L-1}\sum_{m=1}^{N}\ell(\hat{\mathbf{h}}^{i}_m, \hat{\mathbf{h}}^{i+1}_m).
\end{equation}
In this way, the exiting classifiers can help each other in the consecutive layers, which will assist our model early exiting. 
In detail, our classifier is similar to \cite{liu2020fastbert}, which consists of one attention layer and one linear layer. Each token in the $i$-th layer will acquire its high-level representation $\mathbf{H}^i$ through the attention layer. Through Equation~\ref{contra}, the self-attention layer will be boosted so that we can empower the classifier to produce more confident results which are helpful for early exiting based on entropy.
\begin{figure}
    \centering
    \includegraphics[scale=0.6]{picture/contra.eps}
    \caption{The pipeline of cross-layer contrastive learning in the second stage. 
    The green line~(\ie, the same token in the consecutive layers) indicates positive pairs, and red lines~(\ie, otherwise) represent negative pairs.}
    \label{fig:contra}
\end{figure}


For keeping consistent with the inference phase, \our \space uses the output of skipping gates to update the hidden states of the current layer like Equation \ref{eq2}. Next, we define the loss of each classifier as:
\begin{equation}
\mathcal{L}_{\text{CE}}^{i} = \mathcal{L}_{\text{CE}}(\mathbf{z}^i,\mathbf{y}).
\label{eq5}
\end{equation}
In the second stage, the loss function is the sum of the cross-entropy loss of these classifiers with cross-layer contrastive loss:
\begin{equation}
\mathcal{L}_{\text{second}} = \sum\limits^{L-1}_{i=1}\mathcal{L}_{\text{CE}}^{i}+\eta\mathcal{L}_{\text{contra\_2}}.
\label{eq6}
\end{equation}


\subsection{Inference}
After finishing the two-stage training, \our\space can take advantage of skipping gates and early exiting to accelerate inference considering the complexity of samples. 
Only when the output of skipping gates is greater than or equal to 0.5, the model will make a skipping decision. The specific formula follows:

\begin{equation}
\mathbf{X}^i = 
\left\{
             \begin{array}{lr}
             \mathbf{X}^{i-1}&,\quad \text{if}~ \mathcal{G}^{i}(\mathbf{X}^{i-1}) \geq 0.5 \\
             \\
             \mathcal{E}^i(\mathbf{X}^{i-1})&,\quad \text{if}~ \mathcal{G}^{i}(\mathbf{X}^{i-1}) < 0.5
             \end{array}
\label{eq7}
\right.
\end{equation}
When skipping gates choose not to skip, the current early exiting classifier will be used, and all classifiers use an early exiting mechanism based on the entropy:
\begin{equation}
\left\{
             \begin{array}{cl}
             \text{Exit}&,\quad \text{if}~\operatorname{Entropy}(\mathbf{z}^i) < S \\
             \\
             \text{Continue}&,\quad \text{otherwise}
             \end{array}
\label{eq8}
\right.
\end{equation}
where $\mathbf{z}^i$ represents a categorical distribution of the $i^{th}$ classifier, and $S$ is the entropy threshold for early exiting, which is set manually. 




\subsection{Hard Weight Mechanism} \label{hard_weight}
\label{sec:3.4}
In the previous sections, we have introduced \our \space training and inference. However, we find the inconsistent usage of skipping gates between the training and inference phase. Specifically, in the training phase, the output of skipping gates is a continuous probability value between 0 and 1 like Equation~\ref{eq2}, and in the inference phase, we will discretize the output of skipping gates like Equation \ref{eq7} for acceleration. To reduce the impact of inconsistent usage of skipping gates, we introduce a hard-weight mechanism for skipping gates in the training phase. And the hard weight mechanism is differentiable, we use the trick:
\begin{equation}
G_{\text{hard}} = \mathbbm{1}_{G_{\text{soft}\geq 0.5}} - G_{\text{soft}}.\text{detach()} + G_{\text{soft}},
\end{equation}
where $\mathbbm{1}_{G_{\text{soft}\geq 0.5}}$ is an indicator function which returns 1 if $G_{\text{soft}\geq 0.5}$ otherwise 0. With this trick, we achieve two things: (1) makes the output value exactly one-hot (since we add then subtract soft value), (2) makes the gradient equal to the soft gradient (since we strip all other gradients). In the training phase, we define the output of skipping gates as the soft weight $G_{\textrm{soft}}$. To keep the consistent usage of skipping gates between training and inference phases, we turn the soft weight $G_{\textrm{soft}}$ into discrete values~(0 or 1), and keep the gradient equal to Equation \ref{eq9}. In this way, the output of the skipping gate becomes a discrete skipping decision in the training phase, which is consistent with the inference phase.
\begin{equation}
G_{\textrm{hard}} = 
\left\{
             \begin{array}{cl}
             1&, \quad \text{fp}, G_{\textrm{soft}}\geq 0.5  \\
             0&,\quad \text{fp}, G_{\textrm{soft}}< 0.5 \\
             G_{\textrm{soft}}&,\quad \text{backward pass}
             \end{array}
\label{eq9}
\right.
\end{equation}
where fp represents forward pass, $G_{\textrm{soft}}$ is the soft weight and $G_{\textrm{hard}}$ is the hard weight.
\subsection{A Special Training Way for Skipping Gates}
Although the hard weight mechanism can reduce the impact of inconsistent usage of skipping gates, directly using the hard weight mechanism  will lead to some layers can not be well fine-tuned because they may be skipped by skipping gates. However, these layers may be used in the inference phase, which will result in unsatisfactory performance. To ensure all layers can be trained and keep the consistent usage of skipping gates in the training and inference phase, we introduce a special training way from soft weight mechanism to hard weight mechanism for skipping gates.

We use the transition from the soft weight mechanism to the hard weight mechanism to address the above problems. First, we use the soft weight mechanism to warm up all skipping gates and transformer layers and then use the hard weight mechanism to train them. Moreover, the soft weight mechanism aims to fine-tune these layers, and the hard weight mechanism is used for keeping the consistent usage of skipping gates between training and inference phases.

