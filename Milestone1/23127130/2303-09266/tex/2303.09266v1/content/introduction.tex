\section{Introduction}
In recent years, large-scale pre-trained language models(PLMs) such as BERT \cite{devlin-etal-2019-bert}, GPT \cite{radford2018improving}, ALBERT \cite{lan2019albert}, and XLNET \cite{yang2019xlnet} RoBERTa \cite{liu2019roberta} have made significant progress in the field of natural language processing(NLP). However, these models usually require large computation resources and take a long inference time. It is difficult to deploy these models in the case of limited resources. To improve the inference speed and reduce redundant computation, various approaches have been proposed, including network pruning \cite{li2016pruning,he2017channel}, weight quantization \cite{jacob2018quantization}, knowledge distillation \cite{hinton2015distilling,sanh2019distilbert,jiao2019tinybert} and dynamic early exiting \cite{xin2020deebert,liu2020fastbert,schwartz2020right}.

In this work, we mainly focus on dynamic early exiting methods. Such methods do not change the original network structure, and they only add some light plugins in the original network to decide whether early exit or not in each layer which can reduce plenty of computation while keeping comparable performance.

Although current dynamic early exiting technology has shown excellent characteristics, samples must go through all consecutive layers before early exiting, and complex samples have to go through nearly all layers. That is, complex samples have more redundant computation. In the field of computer vision, the Highway Network \cite{srivastava2015highway} and the SkipNet \cite{wang2018skipnet} have proved that redundant computations exist in neural networks and some blocks(layers) can be skipped directly. Motivated by these works, we assume that some layers also can be skipped directly before early exiting for BERT to further reduce computation. 

In this paper, firstly, we introduce a novel dynamic early exiting combined with layer skipping technology for BERT inference named \our, which plug a skipping gate and an exiting operator into each layer of BERT and perform adaptive inference based on the principle of the higher priority of skipping than exiting. We use the widely used PLM BERT as the backbone, and this method can be extended to other PLMs. 

Secondly, in order to address the inconsistent usage of skipping gates between the train and inference stage, we design a hard weight mechanism. For adopting the transformation from soft to hard, we proffer a special training way.
Thirdly, we propose cross-layer contrastive learning and combine it into our training phases to boost the intermediate layers and classifiers to achieve more efficient computation.
Moreover, We conducted experiments on eight classification datasets of the GLUE benchmark\cite{wang2018glue}, which shows that \our \space achieves 2-3Ã— computation reduction with minimal accuracy drops compared with BERT and have a better performance than other dynamic early exiting models at the same computational cost. We also prove skipping mechanism is highly effective in some complex datasets like RTE and WNLI.

The main contributions of this paper can be summarized
as follows: 
\begin{itemize}
\item We propose a novel dynamic early exiting method combined with layer skipping for BERT inference. And we design a hard weight mechanism and a special training way for consistent usage and better training respectively.
\item We propose cross-layer contrastive learning into the training phase to boost the intermediate layers and classifiers, which is beneficial for early exiting.
\item Our method can achieve 2-3x computation reduction with comparable performance and outperforms previous methods in both efficiency and accuracy. In some complex datasets like RTE and WNLI, our method can save the quantity of computation time (2-4x) compared to current early existing methods while achieving better performance. 
\end{itemize}