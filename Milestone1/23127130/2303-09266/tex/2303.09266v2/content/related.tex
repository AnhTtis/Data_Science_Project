\section{Related work}
\subsection{Model Compression}
Many pre-trained language models~(PLMs) have emerged in the last few years. Although these PLMs have achieved high scores in many NLP tasks, their inference time is slow, and the cost of calculation is expensive. One of the most representative models is BERT \cite{devlin-etal-2019-bert}, which has made remarkable improvements in many NLP tasks. However, the inference speed of BERT is criticized. Therefore, a series of methods of model compression have been proposed to solve the above problems. Knowledge distillation \cite{hinton2015distilling} aims to transfer knowledge from the teacher model to the student model, which is applied in DistillBERT \cite{sanh2019distilbert} and TinyBERT \cite{jiao2019tinybert}. ALBERT \cite{lan2019albert} uses sharing parameters to greatly reduce the number of parameters and memory consumption. Q8BERT \cite{zafrir2019q8bert} use symmetric linear quantization \cite{jacob2018quantization} to reduce the number of bits about the parameters of BERT. For Pruning, \cite{DBLP:journals/corr/abs-2002-08307} mainly remove the unimportant part based on gradients of weights. Although these methods can improve the inference time of BERT, they can not adaptively change the architecture of the model according to the complexity of each sample. For example, simple samples must go through all layers for these methods. However, these samples may only go through early layers in early exiting models. 

\subsection{Adaptive Inference}
Adaptive inference can adaptively change the architecture of the model according to the complexity of samples. DeeBERT \cite{xin2020deebert} chooses whether to early exit according to the entropy of the output distribution on each layer. FastBERT divides the early exiting classifiers into student classifiers and the teacher classifier, and then trains all student classifiers through self-distillation. FastBERT also uses the early exit based on entropy. FastBERT and DeeBERT can adaptively adjust the model size according to different samples. However, samples need to go through all layers before early exiting, and if they are complex enough, they need to go through the whole model. Hence, we introduce cross-layer contrastive learning to obtain more powerful classifiers and use layer skipping mechanism to reduce redundancy further.


\subsection{Contrastive Learning}
Recently, contrastive learning (CL) has made significant progress in various domains. Especially, unsupervised CL can exploit a bulk of unlabelled data to train a model with generalization which can even surpass the model trained under supervised training in some situations. The aim of CL is to maximize the agreement between positive views which are jointly sampled and disparting negative views in the representation space. Pioneering works were mainly proposed in the CV domain~\cite{he2020momentum,chen2020simple}. Then in the graph domain, there are plentiful follow-up works \cite{you2020graph,zhu2022rosa} which borrow ideas from pioneering works. However, in the field of natural language processing, there are few inventive works \cite{gao2021simcse} that have shown up in recent years. The main difficulty in NLP domain is that augmentation is hard to design. Besides, previous works mainly focus on the quality of the final representations which may neglect the intermediate layers. But in our work, we hope each layer can be competent for good classification to early exit. So, we propose \emph{cross-layer} contrastive learning in our method for both the first stage and the second training stage. In the \emph{cross-layer} contrastive learning, we do not rely on data augmentation and assume that each token should have similar semantics across the consecutive layers because their representations do not change drastically. The same tokens will be treated as positive pairs in the consecutive layers, otherwise they shall be negative pairs.