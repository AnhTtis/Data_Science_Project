{
    "arxiv_id": "2303.11228",
    "paper_title": "Bimodal SegNet: Instance Segmentation Fusing Events and RGB Frames for Robotic Grasping",
    "authors": [
        "Sanket Kachole",
        "Xiaoqian Huang",
        "Fariborz Baghaei Naeini",
        "Rajkumar Muthusamy",
        "Dimitrios Makris",
        "Yahya Zweiri"
    ],
    "submission_date": "2023-03-20",
    "revised_dates": [
        "2023-03-21"
    ],
    "latest_version": 1,
    "categories": [
        "cs.CV"
    ],
    "abstract": "Object segmentation for robotic grasping under dynamic conditions often faces challenges such as occlusion, low light conditions, motion blur and object size variance. To address these challenges, we propose a Deep Learning network that fuses two types of visual signals, event-based data and RGB frame data. The proposed Bimodal SegNet network has two distinct encoders, one for each signal input and a spatial pyramidal pooling with atrous convolutions. Encoders capture rich contextual information by pooling the concatenated features at different resolutions while the decoder obtains sharp object boundaries. The evaluation of the proposed method undertakes five unique image degradation challenges including occlusion, blur, brightness, trajectory and scale variance on the Event-based Segmentation (ESD) Dataset. The evaluation results show a 6-10\\% segmentation accuracy improvement over state-of-the-art methods in terms of mean intersection over the union and pixel accuracy. The model code is available at https://github.com/sanket0707/Bimodal-SegNet.git",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.11228v1"
    ],
    "publication_venue": "8 Pages"
}