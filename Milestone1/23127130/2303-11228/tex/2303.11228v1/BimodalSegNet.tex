\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{Bimodal SegNet: Instance Segmentation Fusing Events and RGB Frames for Robotic Grasping}





\author{Sanket Kachole$^{1}$, Xiaoqian Huang$^{2}$, Fariborz Baghaei Naeini$^{1}$, Rajkumar Muthusamy$^{3}$, \\ Dimitrios Makris$^{1}$ and Yahya Zweiri$^{2}$% <-this % stops a space
\thanks{*This work was supported by Kingston University, the Advanced Research and Innovation Center (ARIC), and Khalifa University of Science and Technology, Abu Dhabi, UAE.}% <-this % stops a space
\thanks{$^{1}$Sanket Kachole, Fariborz Baghaei Naeini, and Dimitrios Makris are with Faculty of Science, Engineering and Computing, Kingston University, London, KT1 2EE.
        {\tt\small Corresponding author: K1742163@Kingston.ac.uk}}%
\thanks{$^{2}$Xiaoqian Huang and Yahya Zweiri are with the Aerospace Engineering Department, Khalifa University of Science and Technology, Abu Dhabi, UAE.
        {\tt\small yahya.zweiri@ku.ac.ae}}%
\thanks{$^{3}$Rajkumar Muthusamy is with Dubai Future Labs - RLab, Dubai, UAE
        {\tt\small rajkumar.muthusamy@dubaifuture.gov.ae}}%
}









% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
%{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

%\IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}

%Robotic grasping under dynamic conditions is a challenging task as it requires the sequential execution of object segmentation, identification of grasp orientation, determining the geometry of the object and estimation of the force. In robotics, cameras are often placed on top of the robotic arm and the conditions in the factories are mostly unstable. Thus, the impact of dynamic conditions on image quality is vital. 

%Robotic grasping under dynamic conditions often faces challenges in achieving the optimum accuracy of object segmentation. 
Object segmentation for robotic grasping under dynamic conditions often faces challenges such as occlusion, low light conditions, motion blur and object size variance. To address these challenges, we propose a Deep Learning network that fuses two types of visual signals, event-based data and RGB  frame data. The proposed Bimodal SegNet network has two distinct encoders,  one for each signal input and a spatial pyramidal pooling with atrous convolutions. Encoders capture rich contextual information by pooling the concatenated features at different resolutions while the decoder obtains sharp object boundaries. The evaluation of the proposed method undertakes five unique image degradation challenges including occlusion, blur, brightness, trajectory and scale variance on the Event-based Segmentation (ESD) Dataset. The evaluation results show a 6-10\% segmentation accuracy improvement over state-of-the-art methods in terms of mean intersection over the union and pixel accuracy. The model code is available at  https://github.com/sanket0707/Bimodal-SegNet.git

 
 
\end{abstract}


\begin{IEEEkeywords}
Bimodal, Fusion, Segmentation, Robotics, Contours, Occlusion
\end{IEEEkeywords}

\section{Introduction}



Robotic grasping is the ability of the automated system to hold and manipulate target objects using the arm and end effectors. The grasping mechanism is mainly driven using sensors to locate the position and properties of the object for deploying advanced algorithms to control the movement of the robot's joints and fingers for a firm grasp \cite{Dinakaran2023AGrasping}. It has wider industrial applications such as manufacturing plants, assembly lines, machining tasks and warehouses. An essential requirement to attempt a successful grasp include sensing, planning, control, adaptability and robustness. Vision-based robotic grasping methods including object detection, tracking, monitoring, inspection and gripping have delivered convincing results. These methods require a perception of the environment for the robot to execute the actions. Thus, the quality of the collected visual information is key to improving the performance in grasping applications \cite{Du2021Vision-basedReview}. 

%yet, limitation in detecting the accurate object contours impacts the grasp quality. 
%Robotic grasping can be divided into series of tasks including image capturing, object segmentation, determining the contours of the target object.
%In addition, deep learning-based instance segmentation distinguishes the two similar types of objects and thus it has huge scope in robotics grasping applications.
%The object segmentation method has relatively higher significance in comparison to other vision-based methods as it extracts the information of object boundaries with as low as pixel-level details

%\begin{figure}[h]
%      \centering
%\includegraphics[width=1\linewidth]{Images/front_page_pic.jpg}
%\caption{Proposed approach and network for robust object instance segmentation under clutter and occlusions. Exemplary scenario: Eye-in-hand Robotic pick and place of cluttered objects in modern %industries requiring quick operations under varying lighting conditions.}
%\label{fig: Overview BimodalNet}
%\end{figure}

\begin{figure}[h]
      \centering
\includegraphics[width=1\linewidth]{Images/OverviewofArchitecture.png}
\caption{Overview of the proposed Bimodal SegNet for robust object instance segmentation. The encoder-decoder-based architecture employs two separate encoders to handle the input from the RGB camera and the event camera.  It further fuses the features using skip connections at multiple resolutions. }
\label{fig: Overview BimodalNet}
\end{figure}

Although extensive efforts have been taken to deploy human-level grasping skills into robots \cite{sanket20163Tracker}, robotic grasps are still inferior to human ones. While grasping systems may operate under strict conditions (e.g. fix lighting, object type/size/orientation, etc), generalising their operation to a wider set of environmental settings is benefited by using visual sensors which provide richer and more detailed information regarding the object to be grasped, especially when combined with machine learning methods. 
%The industrial robotic grasping environments are often dynamic where visual information offers flexibility to build the machine learning algorithms in order to achieve higher results.
In particular, object segmentation is relevant to robotic grasping, as they aim to extract the object boundaries at pixel level. 
Since visual information collected by cameras in the industrial environment may be affected by low light conditions, motion blur, object size variance, occlusion, etc \cite{Bader2018ChallengesManufacturing}, object segmentation methods should address such challenges. %These challenges are required to be tackled %using various techniques yet, 
%using the object segmentation method which is highly relevant in robotic grasping in comparison to other vision-based methods as it extracts the information of object boundaries with as low as pixel-level details.
  
%Various approaches have been used to segment the objects in an image. The traditional thresholding \cite{N2016ImageImages} algorithms are a fast and simple technique to apply on segmentation applications however, it shows high sensitivity to illumination changes and limited adaptability to the changes within image structure. In addition, it requires a fixed threshold and provides binary output, which makes it unsuitable for images with multiple objects and multiple intensity levels. These challenges were overcome using clustering \cite{Thilagamani2011ObjectClustering} which allowed adaptability to the variable structure and successfully handled the multi-object and multi-class segmentation challenges. However, the clustering technique showed sensitivity to the initial conditions and may lead to over-segmentation of objects into various small parts. Further, conditional random fields \cite{Arnab2018ConditionalSegmentation} (CRFs) handled the drawbacks of thresholding and clustering  by successfully handling contextual information within an image. 

%Various approaches have been used to segment the objects in an image. The deep learning techniques such as Fully convolutional network \cite{Long2014FullySegmentation} are designed to produce dense predictions by upsampling the feature maps from the encoder part of the network using transposed convolution layers. Although, FCN improved the results over traditional machine learning techniques yet, FCN often smoothens detailed structures and ignored small objects. The U-Net \cite{Ronneberger2015U-Net:Segmentation} employed encoder-decoder architecture, where the encoder part of the network compresses the input features, and the decoder part expands them to produce a dense prediction. In order to avoid the excessive compression expansion the DeepLab \cite{Chen2016DeepLab:CRFs} skipped the image resolution expansion of the middle stages in decoder and added spatial pyramidal pooling block with sparse kernels. All the above methods are applied on RGB images and their performance may be affected in the challenging conditions occur in robotic grasping scenarios. 

%The Mask R-CNN \cite{He2017MaskR-CNN}  combined object detection and semantic segmentation tasks into a single network. The architecture ignores the difference in spatial information between receptive fields of different sizes. Lastly, to improve the segmentation accuracy CRFs are effectively used by incorporating prior knowledge extracted by Deep Learning techniques. 
%Moreover, considering the challenges that occur in the industry, the existing methods are required to be customized according to the unique problems of the robotics field. Most current methods are dedicated to specific problems and they lack in delivering a generalized solution that can tackle the most common challenges through deep learning-based artificial neural networks. 


%3. Proposed approach (multiple sensors), 
Standard RGB cameras capture a full projection of the scene in sequences of frames. On the other side, neuromorphic cameras, a recent advance in visual sensors,  record changes in pixel brightness as a stream of “events" \cite{Liao2021NeuromorphicPerspectives}. Their advantage over standard RGB cameras is their high temporal resolution, low motion blur, low transmission bandwidth, high dynamic range and low power. However, since events only record changes, their representative power is higher in textured regions  and boundaries, but lower in uniform regions. \cite{Huang2020NeuromorphicApplications}, \cite{Muthusamy2021NeuromorphicServoing}. In addition, RGB cameras can capture contextual information and perform better when stationary and worse in high-speed due to motion blurring, while neuromorphic cameras are benefited by the camera motion that causes events to be generated around the object boundaries \cite{Muthusamy2020NeuromorphicManipulation}. Both sensors are thus complementary and fusing their signals within a deep learning model may improve the accuracy of instance segmentation and address multiple challenges such as occlusion, blur, brightness and scale variance \cite{Gehrig2021CombiningPrediction}.

For this purpose, we introduce the Bimodal SegNet, a segmentation deep learning network that generalizes the traditional U-Net architecture \cite{Ronneberger2015U-Net:Segmentation} to handle asynchronous and irregular data from multiple sensors. The proposed network has two distinct encoders, one for each signal type, and a spatial pyramidal pooling with atrous convolutions\cite{Chen2018Encoder-DecoderSegmentation} that capture rich contextual information by pooling the concatenated features at different resolutions while the decoder can obtain sharp object boundaries. 




The main contribution of this paper are as follows:
\begin{enumerate}
    \item We propose an encoder-decoder-based architecture that employs two separate encoders to handle the multimodal input by fusing the features with skip connections at multiple resolutions.
    \item  We demonstrate how to use atrous spatial pyramidal pooling within the context of bimodal encoder-decoder-based architecture in instance segmentation applications. 
    \item  Our proposed model attains new state-of-the-art performance on ESD dataset \cite{Huang2023AEnvironment} in terms of mean Intersection Over Union (mIoU) and Pixel Accuracy. Especially, it substantially outperforms the state-of-the-art methods under dark light conditions.    
\end{enumerate} 


%5. Paper outline.

The rest of this paper is organized as follows. Section II reviews related works. The proposed architecture is described in detail in Section III. Section IV provides experimental results and an ablation study. Finally, Section V explains the conclusion and scope for further research. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Literature Review %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{{Related Work}}


Instance segmentation aims to assign a separate class label to each object in an image \cite{Xie2020UnseenEnvironments}. 
%Conventional methods including thresholding, clustering etc. utilize low-level features such as edges or blobs to discover the object contours \cite{Canny1986ADetection}. Graphical or holistic methods including Markov Random Fields and Conditional Random Fields generate the inference by identifying the dependencies between neighboring pixels. Although the conventional methods show poor performance while tackling the occlusion between similar objects yet, Conditional Random Fields (CRF) are still widely used as post-processing layers to uplift the segmentation results \cite{Ulku2022AImages}. 
%In early deep learning approaches to the segmentation problems, several methods attempted to convert classification networks, such as VGG \cite{Simonyan2014VeryRecognition}, and Alex-Net \cite{KrizhevskyImageNetNetworks}, to segmentation networks by fine-tuning the fully connected layers. In addition, \cite{Garriga-Alonso2018DeepProcesses} employed the shallow CNN which showed limitations in abstract features generation. 
Fully convolutional networks \cite{Long2015FullySegmentation} introduced the idea of separating deep convolutional layers that perform feature extraction from the fully connected layers at the end of the architecture. The purpose of isolating the feature extraction blocks was to reuse classification networks like GoogleNet \cite{Szegedy2014GoingConvolutions} and AlexNet \cite{Krizhevsky2012ImageNetNetworks} for other applications, using transfer learning. %A skip connection in the deep neural network architectures links non-adjacent layers of the DCNNs. 
The loss of information due to the use of max pooling or dropout layers can be bypassed using skip connection by simply summing or concatenating the outputs of non-adjacent layers to avoid vanishing gradients. The concept of skip connections was applied on the U-Net architecture, which is based on encoder-decoder structure and applied in instance segmentation \cite{Ronneberger2015U-Net:Segmentation}. 

Robotic applications require techniques that achieve fine-grained pixel-level localization of class labels, especially in critical applications such as robotic surgeries where few pixel errors could lead to undesired consequences \cite{Ronneberger2015U-Net:Segmentation} \cite{Nathoo2005InFuture} \cite{Lin2020Global-and-localImages}. %The convolution principle lacks the ability to model the global context information of the image. In addition, max-pooling layers create a hierarchy of features and tend to lose partial localization . The encoder-decoder architecture has two main types of blocks. In encoder blocks, features are gradually reduced to capture the longer-range instance information. Then, the decoder block gradually increase the spatial dimensions to recover the fine object details. %U-Net is a popular example where an encoder-decoder architecture with a skip connection has been applied. 
The dilated convolution \cite{Chen2017RethinkingSegmentation} also known as atrous convolution is a compelling convolutional tool that takes the features computed from the deep convolutional neural network and captures the context at various coarser resolutions by increasing the receptive fields convolution kernel. In other words, it is a method that expands the kernel size by inserting zeroes between its consecutive elements \cite{Yu2015Multi-ScaleConvolutions}\cite{Chen2014SemanticCRFs} \cite{Chen2018DeepLab:CRFs}.  It also preserves detailed feature map resolutions without pooling or sub-sampling. Various models have been explored using  atrous convolutions where experiments were performed by modifying atrous rates of capturing long-range information. Similarly, the application of combined U-Net with atrous convolution has been successfully implemented in \cite{Safarov2021A-denseunet:Convolution}\cite{Lv2020AttentionSegmentation}. A convMixer-based UNet model is used in vision-based robotic grasping applications \cite{Huang2022CM-UNet:Scenes} which enhances the segmentation mask and boundary of unknown objects appearing in cluttered scenes. 

%where the segmentation results achieved 90\% Dice coefficient score. The majority of application uses the Intersection over Union (IoU) as a metric to compare the results while Accuracy, Recall, Precision, confusion matrix and F1 score are  other metrics used sparingly.


Vision-based robotic grasping systems can be divided into mainly three types, depending on the sensors used: RGB-based,  event-based and RGB-event fusion based. Traditional cameras capture visual information in the form of RGB frames continuously which leads to high power consumption. In addition, image quality may suffer e.g. blurring due to motion or low contrast in low lighting conditions which impact the segmentation and consequently the grasping quality \cite{Brooks2019LearningBlur}\cite{LiyuanPan2019BringingCamera}. A recent study showed that traditional cameras are incompetent to capture blur-free movement on industrial conveyor belts due to a relatively low sampling rate, compared to event-based vision systems. Besides, the robotic gripper’s actuating speed in industrial applications is a minimum of 100 ms \cite{Muthusamy2020NeuromorphicManipulation} which also contributes to motion blur. Although high-speed frame-based cameras with a frame rate of more than 100 FPS are available, they demand higher power and storage consumption. Reducing latency is a significant consideration in robotic grasping applications. Complex algorithms in computer vision and image processing consume additional computing time which eventually impacts on overall grasping process \cite{Huang2020NeuromorphicApplications}. This demands faster data acquirement and processing for efficient grasping. Lastly, improvement in data capturing processing and computation can reserve additional time for the gripper’s actuation. 
Exploiting the advantage of high temporal resolution of events, \cite{Gehrig2018AsynchronousFrames} used the Xception encoder, supplied with 6 channels of image representation. 
%The initial two channels comprised a histogram of positive and negative events. The last 4 channels consist of the event information during a certain time period. 
The EV-SegNet \cite{Alonso2018EV-SegNet:Cameras} also used an Xception-based CNN to handle event data for semantic segmentation applications.


Various algorithms fuse modalities to take advantage of their complementarity. DAVIS produces events and frames simultaneously, making it a suitable visual sensor.
CMX \cite{Liu2022CMX:Transformers} employs a transformer-based architecture with a feature rectification module for semantic segmentation of cross-modality data.
%This study used the CMX model trained on the ESD dataset \cite{Huang2023AEnvironment} using default parameters for feature fusion of multiple modalities described in \cite{Liu2022CMX:Transformers}. CMX employs a transformer-based architecture with a feature rectification module, but qualitative results shows that it struggles with spatially variant features and modeling low-level visual features.
The recurrent architecture \cite{Gehrig2021CombiningPrediction} has been explored to fuse the events and frames for depth prediction.  The fusion of events and RGB frames has been successfully applied in simultaneous localization and mapping \cite{Zhu2018UnsupervisedEgomotion}, high-dynamic range intensity reconstruction \cite{Tomy2017FusingConditions}, feature tracking \cite{Gehrig2018AsynchronousFrames}, and image deblurring \cite{LiyuanPan2019BringingCamera}. However, none of them is applicable to the application of instance segmentation. Especially, in robotic grasping applications where a dynamic environment increases the complexity to identify the accurate object contours and segment them to a sub-class level. In addition, existing methods lack showing generality to tackle frequently occurring issues in the factories for robotic grasping applications. 












%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Methodology %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{{Bimodal SegNet}}


In this section, the proposed Bimodal SegNet architecture for robust instance segmentation in robotic grasping applications is presented in details. Firstly, the conversion of asynchronous events into event frames is explained. Then, the concatenated skip connections between encoders and decoders are discussed. Next, the atrous convolution, which is the core element of the Spatial Pyramid Pooling is explained. Finally, the overall network architecture for the Bimodal SegNet is shown. 

%We briefly review encoder – decoder block of the segmentation models which is used as the main structure of the proposed network. We then discuss the event encoder module and then present the modified Atrous Convolutions. 


\subsection{\textbf{Event representation}}

The event stream generated from a dynamic vision sensor (DVS) is characterized by a position $x_e$ and $y_e$, timestamp $t_e$ and polarity $p_e$ where e is the counter of events. The polarity is binary and thus either 0 for negative or 1 for positive polarity. In \cite{Naeini2020Dynamic-vision-basedNetworks}, the events were accumulated over a time window $T$ irrespective of position information. In order to convert the asynchronous events into sequential frames, this paper uses the event accumulation method. 

The configuration of the time window $T$ hyperparameter should consider parameters such as the DVS threshold, application speed and noise, but most importantly the RGB frame rate. The event-to-frame conversion process can be formulated as: 


\begin{equation}
    E_t(x,y,p) = \sum_{\forall e} rect( \frac{t_e}{T} - 0.5 - t) \delta_{xx_e} \delta_{yy_e} \delta_{pp_e}
    \label{eqn: event}
\end{equation}

where $E_t$  represents a frame of accumulated events for different polarities $p\in \{0,  1\}$ at timestamp \textit{t}. The Kronecker delta function and rectangle function are denoted as $\delta$ and \textit{rect} respectively. The $t_e$ represents the timestamp of each event and $e$ indicates the event number.







\subsection{\textbf{Concatenated Skip Connections}}


The concatenated skip connections maintain the information throughout the network as it allows connected layers to reuse the feature representation resulting in a performance improvement. As shown in Figure \ref{fig:BimodalNet}, the event encoder layers are from $E_0$ to $E_3$. The standard encoder layers are from $S_0$ to $S_3$. The decoder layers are $D_0$ to $D_3$. Feature map from event encoders $E_n$ is concatenated with feature map of standard encoder $S_n$ as

\begin{equation}
    X_{n} = ([E_{n},S_{n}])
\end{equation} 
where $ X_{n}$ represents the concatenated features with $n$ ranges from 0 to 3. Further, the decoder layers receive the feature maps from the same level as the encoder and the previous decoder layer. It can be represented as, 

\begin{equation}
    D_{n} = ([X_{n},D_{n+1}])
\end{equation}


The features extracted at various resolutions in the event encoders are bypassed to the same resolution of the standard encoder. The enriched features are then passed to the same resolution of the decoder. 

\subsection{\textbf{Atrous convolution}}



Atrous convolution also called dilated convolution can be mathematically expressed as follows:

\begin{equation}
    y[i] = \sum_1^K x[i+r.k] w[k]
    \label{eqn: atrs}
\end{equation}


where $y$ is the output feature map and $x$ is the input feature map. $i$ represents the location in the output feature map $y[i]$ in 2-D image. The convolution filter is stated by $w$ and the atrous stride rate is determined by $r$. The atrous rate $r$ increases the size of the kernel by inserting $r-1$ zeros along every spatial dimension. In the case of standard convolution, the value of rate is $r = 1$. The filter’s receptive field is changed by modifying the stride rate \cite{Chen2018Encoder-decoderSegmentationb}. 


\subsection{\textbf{Network Architecture for Bimodal SegNet}}
\label{subsection :Network Architecture for Bimodal SegNet }





\begin{figure*}[t!]
\resizebox{\hsize}{!}{\includegraphics[clip=true]{Images/Architecture.png}}
\caption{\footnotesize
{ Bimodal SegNet Architecture    }
}
\label{fig:BimodalNet}
\end{figure*}




The architecture of the Bimodal SegNet Network is shown in Figure \ref{fig:BimodalNet}. %Where we consider the task of instance segmentation of the objects in the scene by utilizing the inputs in form of event frames and RGB frames. 
Event-based vision sensors such as DAVIS346 can produce both continuous asynchronous events (with temporal resolution of few microseconds) and RGB frames (with frame rates between 25-50 Hz) simultaneously. The RGB frames are passed into RGB encoders $S_n$ and the event frames are passed into event encoders $E_n$. Both types of encoders are built with convolutional blocks for downscaling the input frame by 0.5x for $N+1$ consecutive times to learn or infer the feature maps. Event-based features mainly represent the object contours while RGB-based features provide rich contextual information.

 
At each of the $N$ feature fusion stages in the encoder, a combination of convolution, dropout, convolution and MaxPooling layer is used. The output tensor of the convolution is passed into the dropout layer. It is further passed into the convolution layer and lastly into the MaxPooling layer to downsample the input along its spatial dimensions. The feature maps from the encoders are passed into multiple parallel atrous convolutional blocks with different sampling rates. The features extracted for each sampling rate within the spatial pyramidal pooling block are fused at the output of the block. The fused results are then passed into the decoder block where the image is upscaled by 2x for $N$ times consecutively. At each of the $N$  feature fusion stages in the decoder, a combination of deconvolution, concatenation, convolution, dropout and convolution layer is used. The deconvolution layer assists in retrieving the original spatial dimension of the input image. Further, the concatenation layer takes the concatenated input tensor of the event encoder and RGB encoder i.e. $X_n$ and output of the previous decoder layer i.e. $D_{(n+1)}$ to provide a single fused tensor. The concatenated tensor is then passed into the dropout layer to reduce the overfitting in the network and convolution. This allows the network to learn features at multiple scales, which is important to produce a scale-invariant model. Thus, the features from the event and RGB encoders are fused at each downscaling block and passed to the corresponding level of upscaling to the decoder block using a skip connection. The purpose of the concatenation in the decoder is to enrich the large-level features learned by the decoder block. Altogether, the network enriches the RGB frames at the decoder block by fusing the events frames and RGB frames at $N$ separate feature resolutions and in the atrous spatial pyramid pooling block. 








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Experiments and Discussion %%%%%%%%%%%%%%%%%%%%


\section{{Experimentation}}


\subsection{\textbf{Dataset}}

ESD \cite{Huang2023AEnvironment} is one of the largest datasets which focuses on instance understanding of robotic grasping scenes. The dataset was captured using the Davies346 sensor mounted on a robotic arm and consists of both conventional RGB frames and corresponding asynchronous events. In addition, the dataset has instance-wise dense pixel and events annotations for 15 classes grouped into 6 categories (bottle, box, pouch, book, mouse, platform). The dataset consists of 17186 annotated images and 177 labeled event streams. The captured dataset considers several variations such as in the camera’s direction of motion, arm speed, lighting conditions and the number of objects in clutter. Motion variations are linear, rotational and partial-rotational motion. Arm speed variants are 0.15 m/s, 0.3 m/s and 1 m/s. Lighting conditions include normal light and low light.  
The number of objects in the clutter varies as 2,4,6,8,10 objects as shown in Table \ref{tab: data_visualization_ESD1}. There are 13984 images in the training dataset and 3202 images in the testing dataset. The five objects in the validation dataset are different from the testing dataset.

The RGB framerate is 40 Hz, therefore a temporal window of T=25 ms is used for framing the events to ensure temporal alignment between the two types of frames.
The RGB and event frame resolution is 346$\times$260, which differs from the expected input of the network (256$\times$256). Therefore, the popular approach of patchifying the input images is employed and the final  segmentation map is reconstructed by unpatching the outputs.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
Raw Image & Ground Truth & Annotated Events     
\\

    \hline
    \rotatebox{90}{2 objects}
    \subfloat{\includegraphics[width = 0.8 in,height=0.6in]{Images/2objRGB.png}}  
    & \subfloat{\includegraphics[width = 0.8 in,height=0.6in]{Images/2objRGBGT.png}}
    & \subfloat{\includegraphics[width = 0.8 in,height=0.6in]{Images/2objeventGT.png}}
 \\
    \hline    
   \rotatebox{90}{4 objects}
    \subfloat{\includegraphics[width = 0.8 in,height=0.6in]{Images/4objRGB.png}}
   & \subfloat{\includegraphics[width = 0.8 in,height=0.6in]{Images/4objRGBGT.png}}
   & \subfloat{\includegraphics[width = 0.8 in,height=0.6in]{Images/4objeventGT.png}}
\\
    \hline
    \rotatebox{90}{6 objects}
    \subfloat{\includegraphics[width = 0.8 in,height=0.6in]{Images/6objRGB.png}}
    &\subfloat{\includegraphics[width = 0.8 in,height=0.6in]{Images/6objRGBGT.png}}
    &\subfloat{\includegraphics[width = 0.8 in,height=0.6in]{Images/6objeventGT.png}}
\\
    \hline
    \rotatebox{90}{8 objects}
    \subfloat{\includegraphics[width = 0.8 in,height=0.6in]{Images/8objRGB.png}}
    &\subfloat{\includegraphics[width = 0.8 in,height=0.6in]{Images/8objRGBGT.png}}
    &\subfloat{\includegraphics[width = 0.8 in,height=0.6in]{Images/8objeventGT.png}}
    \\
    \hline
    \rotatebox{90}{10 objects}
    \subfloat{\includegraphics[width = 0.8 in,height=0.6in]{Images/10objRGB.png}} 
    &\subfloat{\includegraphics[width = 0.8 in,height=0.6in]{Images/10objRGBGT.png}}
    &\subfloat{\includegraphics[width = 0.8 in,height=0.6in]{Images/10objeventGT.png}}
\\
\hline
    \end{tabular}
    \caption{Example of the ESD dataset in terms of the number of objects attributes, under the condition of 0.15 moving speed, normal light condition, linear movement, and 0.82 height.}
    \label{tab: data_visualization_ESD1}
 \end{table}






\subsection{\textbf{Evaluation Metrics}}

\subsubsection{\textbf{Pixel accuracy}}

Pixel accuracy is a metric to calculate the percentage of pixels in the image that are classified correctly as given by Equation (\ref{eqn: Pixel Accuracy}) 

\begin{equation}
    Acc(p,p') = \frac{1}{N}\sum_{1}^N \delta (p_i,p'_i)
    \label{eqn: Pixel Accuracy}
\end{equation}

where $p$, $p'$, $N$ and $\delta$ represent the ground truth image, the predicted image, the total number of pixels and Kronecker delta function respectively. However, its descriptive power is limited in cases with a significant imbalance between foreground and background pixels. 


\subsubsection{\textbf{Mean Intersection over Union (mIoU)}}

The performance of the network for instance segmentation is evaluated using the mean Intersection over Union also known as the Jaccard Index. mIoU handles imbalanced binary and multi-class segmentation. It is calculated across classes according to Equation (\ref{eqn: mIoU}):

\begin{equation}
    mIoU = \frac{1}{C}\sum_{i}^C \frac {\sum_{i}^N \delta (p_{i,c},1) \delta (p_{i,c},p'_{i,c})}{max (1,\delta (p_{i,c},1) + \delta (p'_{i,c},1) }
    \label{eqn: mIoU}
\end{equation}
 
Thus, the mIoU measure is more informative than the pixel accuracy as it considers false positives.




\subsection{\textbf{Training}}

The RestNet-101 pretrained on ImageNet is the backbone of the model that is treated as an encoder to extract features. The last layer of the pretrained ResNet-101 is replaced with 11 instance classes (10 objects and 1 background ). The whole dataset is divided into training/testing/validation with a ratio of 70:20:10.
%In the pre-processing phase, in order to avoid resizing of the standard images and resultant loss of information due to resizing, the patches of the images are cropped prior to training. The cropped patches of the image are then repatched post-training. The event frames have the majority of a pixel as background class because the object’s contours are only captured by the event camera and therefore the frames generated from the events has the majority of pixels near zero value.
The number of filters in the event/RGB encoders at $E_0$/$S_0$, $E_1$/$S_1$, $E_2$/$S_2$, $E_3$/$S_3$ are 16, 32, 64, 128 respectively. The kernel size of the convolution layer is 3. The output tensor convolution is passed into the dropout layer where the input units are randomly set to 0 with a frequency of 0.2 in order to avoid overfitting. The MaxPooling layer is 2$\times$2 in size to take the maximum value over each channel of the input window. Using empirical results \cite{Chen2018Encoder-DecoderSegmentation}, typically 6, 12, 18, 24 atrous rate for RGB and 6 atrous rate for event frames are used. Since the event frames have mainly edge-level information, the use of atrous rate $r$ = 6 assists in preventing information leak at this stage.  A 3$\times$3 kernel size for convolution is empirically selected as it is small enough to capture local spatial information. Softmax as an activation function and Adam as an optimizer are used while compiling the model. Categorical cross-entropy is used as a loss function. %Jaccard coefficient or the mean Intersection of union and the pixel accuracy is the accuracy metric. 
The fine-tuned model has 0.001 learning rate, 30 batch size, 100 epochs. The training was performed on a processor 11th Gen Intel(R) Core(TM) i7-1165G7, 1.69 GHz and 16 GB RAM. 








\subsection{\textbf{Alternative Architectures for Ablation studies}}
\label{subsection : Ablation_Study}

To demonstrate the validity of the proposed architecture, we investigate  alternative approaches that vary at the stage where the fusion of RGB and events frames occurs and its impact on segmentation accuracy. The following two  architectures are considered:


\subsubsection{\textbf{\textit{ Pre-Encoder Fusion}}}
\label{subsubsection : ablation_pre-Encoder}


The pre-encoder fusion architecture is derived from U-Net architecture where event and RGB frames are concatenated and passed into the single encoder and its  output is passed into the decoder, without any skip connections or atrous convolutions (Figure \ref{fig:ablation_pre-Encoder}). 

\begin{figure}[h!]
      \centering
\includegraphics[width=1\linewidth]{Images/ablationpreencoderfusion.png}
\caption{Pre-Encoder Fusion Architecture: RGB and event frames are fused before encoding. The architecture lacks any skip connections and atrous convolutions  }
\label{fig:ablation_pre-Encoder}
\end{figure}


\subsubsection{\textbf{\textit{Pre-Decoder Fusion}}}
\label{subsubsection : Pre-Decoder Enrichment}

In the Pre-Decoder fusion approach, event and RGB frames are encoded separately similar to the proposed model, except the skip connections between encoders and the decoder as shown in Figure \ref{fig:BimodalNet}. Instead, the low-dimension outputs of the two encoders are concatenated and fed into a single decoder. The Spatial Pyramid Pooling is also omitted (Figure \ref{fig:ablation_pre-decoder}).  
%Event information ie. object contours are supplied directly to the decoder in order to enrich the low-level information learned using an encoder.


\begin{figure}[h!]
      \centering
\includegraphics[width=1\linewidth]{Images/ablationpredecoderfusion.png}
\caption{Pre-Decoder Fusion Architecture: RGB and event frames are encoded seperately and fused before decoding. The architecture lacks any skip connections and atrous convolutions }
\label{fig:ablation_pre-decoder}
\end{figure}



%\subsubsection{\textbf{\textit{Fusion SegNet}}}
%\label{subsubsection : Encoder-Encoder Enrichment}

%\begin{figure}[h!]
%      \centering
%\includegraphics[width=1\linewidth]{Images/ablation_within_encoder_fusion_detailed.png}
%\caption{Fusion SegNet Architecture includes skip connections between the encoders and the decoder but lacks the atrous convolutions}
%\label{fig:ablation_Encoder-Encoder}
%\end{figure}

%The method shown in Figure \ref{fig:ablation_Encoder-Encoder}, employs two separate encoders for event frames and RGB frames. The learned features are concatenated within several convolutional stages of the encoder similar to the Bimodal SegNet.  However, this architecture lacks the spatial pyramidal pooling with atrous convolutions and instead passes the fused output directly into the decoder to learn the low-dimenstional features.  









\subsection{\textbf{Results}}
\label{section:Results}
%In order to evaluate the robustness of the model, various experiments were conducted by varying the number of objects in the clutter, lighting conditions, the distance between the camera and the table, the trajectory of the camera and the moving speed of the camera. The proposed model was also compared with the state-of-the-art models including RGB-based FCN\cite{Long2015FullySegmentation}, U-Net\cite{Ronneberger2015U-Net:Segmentation}, DeepLab\cite{Chen2018Encoder-DecoderSegmentation}, RGB-event fusion CMX\cite{Liu2022CMX:Transformers} and the models considered in the ablation experiment in subsection \ref{subsection : Ablation_Study}. If not explicitly specified, the following default conditions will be considered to define the testing subsets for the experiments: 2 objects, normal light, rotational arm motion, 0.15 m/s speed and 62 cm platform to camera distance. The evaluation metrics, ablation study and results are discussed further.

To assess model robustness, experiments were conducted, varying object clutter, lighting, camera distance, trajectory, and speed. The proposed model was compared to state-of-the-art models, including RGB-based FCN\cite{Long2015FullySegmentation}, U-Net\cite{Ronneberger2015U-Net:Segmentation}, DeepLab\cite{Chen2018Encoder-DecoderSegmentation}, RGB-event fusion CMX\cite{Liu2022CMX:Transformers} and models in the ablation study in subsection \ref{subsection : Ablation_Study}. In addition, the CMX model used in this study is trained on the ESD dataset \cite{Huang2023AEnvironment} using default parameters for feature fusion of multiple modalities described in \cite{Liu2022CMX:Transformers}. If not explicitly specified, the following default conditions will be considered to define the testing subsets for the experiments: 2 objects, normal light, rotational arm motion, 0.15 m/s speed and 62 cm platform to camera distance. Evaluation metrics, ablation study, and results are discussed below.



\subsubsection{\textbf{\textit{Varying objects/occlusion using pre-trained ResNet-101 encoder}}}
\label{subsubsection : pre-trained occlusion results}

The first experiment was conducted on a subset of the testing dataset where the number of objects in the clutter varied between 2,4,6,8,10. The scenario of two objects indicates no occlusion, while in scenarios of more than 2 objects occlusions are present. Moreover, the higher the number of objects, the more occlusion and the higher complexity in the scenario. This is also evident in the segmentation accuracy results of all models, as shown in Figure \ref{fig:Pre_trained cluttered objects}, where the mIoU score of all models is decreasing with the increasing number of objects. The mIoU score for all the state-of-the-art models for 2 objects ranges from 54.65\% to 92.46\% and it drops for 10 objects (38.42\% to 85.42\%). whereas, the proposed Bimodal SegNet model’s performance drops only by 7.97\% and maintains better performance than the other methods, including state-of-the-art models such as FCN, U-Net and DeepLab, pre-trained on ImageNet for any number of scene objects.




\begin{figure}[h]
      \centering
\includegraphics[width=1\linewidth]{Images/ResultsPretrainedclusters.png}
\caption{Segmentaion accuracy of the pre-trained encoder in varying clutter objects scene}
\label{fig:Pre_trained cluttered objects}
\end{figure}

\subsubsection{\textbf{\textit{Varying objects/occlusion using re-trained encoders}}} 
\label{subsubsection : retrained occlusion results}

In order to tackle the challenge of substantial loss in accuracy for 10 objects in clutter, transfer learning is employed by unfreezing the last 4 convolution layers of the encoder, the whole decoder module, and the classifier of the ResNet-101 on the similar data used in experiment \ref{subsubsection : pre-trained occlusion results}. Figure \ref{fig:retrained cluttered object} shows that this certainly improves the overall accuracy of all the models. As far as FCN, U-Net and DeepLab are concerned, the average accuracy increased by about 7\% for all scenarios. The proposed Bimodal SegNet model still maintains the highest mIoU score, e.g 95.46\% for the 2-object scenario and with a drop of only 7.97\% for the 10-object scenario.


\begin{figure}[h]
      \centering
\includegraphics[width=1\linewidth]{Images/Resultsretrainedclusters.png}
\caption{Segmentation accuracy of re-trained encoder in varying clutter objects scene}
\label{fig:retrained cluttered object}
\end{figure}

\subsubsection{\textbf{\textit{Varying lighting conditions using re-trained encoder}}} 
\label{subsubsection : light conditions results}

State-of-the-art RGB-based methods struggle in low light conditions, as objects are not clearly visible. This is reflected on the huge drop in their segmentation accuracy between bright and low conditions, as shown in Figure \ref{fig:results_lighting}: 34.9$\%$, 32.08$\%$ and 31.55$\%$ for FCN, U-Net and DeepLab respectively. Methods that fuse RGB and event frames demonstrate some resilience in low light conditions, with smaller drops in segmentation accuracy: 5.85$\%$, 7.04$\%$, and 2.13$\%$ for the Pre-Encoder Architecture, the Pre-Decoder Architecture and the CMX respectively. The proposed Bimodal SegNet seems to be robust against varying lighting conditions with the lowest drop of only 1.60$\%$ in segmentation accuracy.
%The pixel intensity is directly correlated to the accuracy of the model. All the objects are clearly visible in the bright light conditions thus, the mIoU score in figure (\ref{fig:results_lighting}) shows the difference between the Bimodal SegNet model and DeepLab is 3.68\%. The state-of-the-art models do not perform well in dark light conditions. In dark lighting conditions, the average performance difference between the state-of-the-art model and the Bimodal SegNet is 37.26\%. Clearly, the impact of event-based vision features played a key role in dark light conditions. In addition, the Fusion SegNet fusion method \ref{subsubsection : Encoder-Encoder Enrichment} has employed feature fusion over various resolutions of the event encoder and standard encoder, which shows the drop in the accuracy for the is only 12.86\%. It demonstrate the significance of spatial pyramidal pooling block in the architecture. The Bimodal SegNet model maintains the accuracy of segmentation. 



\begin{figure}[h]
      \centering
\includegraphics[width=1\linewidth]{Images/resultslighting.png}
\caption{Segmentation accuracy in varying lighting conditions scene}
\label{fig:results_lighting}
\end{figure}

\subsubsection{\textbf{\textit{Varying moving trajectories using re-trained encoder}}} 
\label{subsubsection : Trajecotries results}


The subsequent experiment was conducted on a subset of the testing dataset, where the robotic arm movement direction was varied as linear, rotational, or partial rotational. In event-based vision sensors, the direction of the robotic arm movement plays a crucial role as perpendicular edges generate more informative event sets compared to parallel edges. However, linear motion is marked by abrupt directional changes while rotational motion involves smoother curves, causing higher jerk and vibrations in linear motion, leading to more blurred images than in rotational and partial rotational motion. The impact of the phenomenon is demonstrated in Figure \ref{fig:results_trajectory}, where FCN, U-Net, and DeepLab models were evaluated on RGB images. Their average mIoU score is highest in rotational motion 67.74\%, decreasing by 4.65\% in partial rotational and 10.95\% in linear motion. Methods that combine RGB and event frames are proved more robust with  smaller drops. 
%The next experiment was conducted on a subset of the testing dataset where the type of robotic arm movement (i.e. direction of motion of the camera) was varied. There are three directions robotic arm moves i.e. linear, rotational and partial rotational. In the case of an event-based vision sensor, the direction is an important factor as object edges perpendicular to the motion direction lead to more informative sets of events than the parallel edges do. On the other %hand, robot creates a more jerk in linear motion than a rotational motion which causes more vibration in linear motion resultantly produces more blurred RGB images in linear motion. 
%The impact of the phenomenon can be clearly seen in Figure \ref{fig:results_trajectory} in terms of the segmentation accuracy. 
%FCN, U-Net, DeepLab are trained and evaluated on the RGB images. FCN, U-Net and DeepLab shows the lowest accuracy in linear motion 54.15\%, 57.79\%, 58.41\% than in partial-rotational motion 60.70\%, 63.62\%,64.95\% and highest in rotational motion 63.39\%,68.64\%, 71.18\% respectively. Whereas, after fusing the events and RGB frames in the CMX and Bimodal SegNet the segmentation accuracy is highest in rotational motion 88.82\%, 89.27\% than in linear motion 85.05\%, 86.82\% and lowest in partial-rotational motion 84.40\%,84.93\% respectively. 
%The segmentation results are varied minimally in pre-encoder and pre-decoder architectures ie. in linear 52.40\%, 53.95\%, in partial rotational 52.09\%, 53.63\% and in rotational 51.08\%, 52.32\% respectively. In short, linear motion showed slightly higher results than partial rotational motion but lower than rotational motion. 
The proposed Bimodal Segnet maintains its superiority in segmentation accuracy, but also demonstrates the lowest variation across different motion directions.



\begin{figure}[h]
      \centering
\includegraphics[width=1\linewidth]{Images/resultsdirectionofmotion.png}
\caption{Segmentation accuracy in varying directions of robotic arm motion scene}
\label{fig:results_trajectory}
\end{figure}


\subsubsection{\textbf{\textit{Moving speed of camera using re-trained encoder}}}
\label{subsubsection : speed results}

The camera is placed at the end of the robotic arm thus the camera speed and resultant blur in images is an important factor while evaluating the robustness of the model. This experiment was conducted on a subset of the training dataset where the speed of the end effector was varied. State-of-the-art models have an mIoU score ranging from 64.89\%  to 86.58\% for 0.15 m/s which then drops to a range of 46.45\% to 84.90\% for 1m/s (Figure \ref{fig:results_arm_speed}). Whereas, the proposed Bimodal SegNet model has the highest accuracy of 89.31\% for 0.15m/s and it drops to 86.72\% for 1m/s i.e. by 2.59\%. The impact of event-based vision in high-speed conditions is clear as it assists in the accurate estimation of contours. 



\begin{figure}[h]
      \centering
\includegraphics[width=1\linewidth]{Images/resultsspeed.png}
\caption{Segmentation accuracy in varying speed of robotic arm}
\label{fig:results_arm_speed}
\end{figure}


\subsubsection{\textbf{\textit{Object size variance using re-trained encoder}}} 
\label{subsubsection : size variance results}


In order to understand the scale invariance of the model the experiment was conducted on a subset of the training dataset where the distance between the platform and the camera was varied to 62 cm and 82 cm. As per the results illustrated in figure \ref{fig:results_distance_betwn_ob_n_camera} there is a minimal impact of the camera and object distance on the accuracy of all the models, yet, a drop in performance of the Bimodal SegNet is only 0.37\% ie. from 86.53\% to 86.16\% as compared to the state of the art models i.e. 3.17\% ie. 71.24\% to 68.07\%.



\begin{figure}[h]
      \centering
\includegraphics[width=1\linewidth]{Images/resultsdistancebetwnobncamera.png}
\caption{Segmentaion accuracy in varying distance between object and camera scene}
\label{fig:results_distance_betwn_ob_n_camera}
\end{figure}


\subsubsection{\textbf{\textit{Quantitative Comparison}}} 
\label{subsubsection: Cumulated results}


\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
\textbf{Method} & \textbf{mIoU} & \textbf{Pixel Accuracy}\\
    \hline
    FCN \cite{Long2015FullySegmentation} & 59.6 & 70.48
 \\
    \hline    
    U-Net \cite{Ronneberger2015U-Net:Segmentation} & 64.7  & 75.2  \\
    \hline
    DeepLab \cite{Chen2018Encoder-DecoderSegmentation} & 71.05 & 81.06 \\
    \hline
    Pre-Encoder [\ref{subsubsection : ablation_pre-Encoder}]  & 51.3 & 72.2 \\
    \hline
    Pre-Decoder [\ref{subsubsection : Pre-Decoder Enrichment}] & 52.7 & 73.1 \\
    \hline    
    CMX \cite{Liu2022CMX:Transformers} & 85.81 & 94.58 \\
    \hline
    Bimodal SegNet [ours] & \textbf{87.05} & \textbf{96.83}  \\
    \hline
    \end{tabular}
    \caption{Quantitative comparison of Bimodal SegNet against other RGB-based and RGB-event fusion methods on the whole ESD dataset. }
    \label{tab: Segmentation On Whole Dataset}
 \end{table}


 Segmentation results of the FCN, U-Net, DeepLab, Pre-Encoder, Pre-Decoder, CMX and Bimodal SegNet on the whole testing part of the ESD dataset, i.e. considering all possible variations of object numbers, lighting conditions, motion type and speed, as well as distance from the camera,
%ie. the cumulation of image degradation scenarios used in (\ref{subsubsection : pre-trained occlusion results}, \ref{subsubsection : retrained occlusion results}, \ref{subsubsection : light conditions results}, \ref{subsubsection : Trajecotries results}, \ref{subsubsection : speed results}, \ref{subsubsection : size variance results}) were used 
are shown in the Table \ref{tab: Segmentation On Whole Dataset} using  the pixel accuracy and the mIoU metrics. 
The proposed Bimodal SegNet outperforms the traditional RGB-based methods demonstrating the value of fusing RGB and event frames. In addition, it outperforms all other architectures that fuse RGB and event frames, including the state-of-the-art CMX \cite{Liu2022CMX:Transformers}, illustrating the superiority of the proposed architecture. The CMX architecture achieves 85.81\% mIoU and 94.58\% pixel accuracy, still these scores are  1.24\% and 2.25\% respectively lower than what is achieved by the proposed Bimodal SegNet model.




\section{{CONCLUSIONS}}

In this paper, a novel architecture for instance segmentation in robotic grasping using a dynamic vision sensor is presented. The architecture fuses the events frames and RGB frames at various resolutions of the event encoder and RGB encoder, which further fuses the feature maps in the decoder. The feature maps from both encoders are passed into the spatial pyramidal block where atrous convolutions are employed.
%The proposed architecture can handle multiple challenges in segmentation for robotic grasping, such as 


Our proposed model achieves state-of-the-art performance on the ESD dataset in terms of mIoU and pixel accuracy, as demonstrated in multiple scenarios with variations in object types, trajectory, camera speed, apparent size, and lighting conditions. The fusion approach we propose achieves high robustness against various challenges, such as occlusions, low lighting, small objects, high speed, and linear motion. Bimodal SegNet, leveraging the complementarity of events and frames, outperforms unimodal encoder-decoder approaches. The experimental comparisons to alternative fusion strategies, including the state-of-the-art transformer-based CMX, demonstrate the superiority of our proposed architecture that fuses RGB and event frames at multiple resolutions, with skip connections and atrous convolutions.
 
%Our proposed model achieves state-of-the-art performance on the ESD dataset in terms of mIoU and pixel accuracy. The model was evaluated across multiple scenarios that consider variations on a number of objects, trajectory, camera moving speed, apparent size variance and lighting conditions. It was demonstrated that our fusion approach achieves the highest robustness against a variety of challenges such as occlusions, low lighting conditions, small-size objects, high speed and linear motion. Bimodal SegNet outperforms unimodal approaches based on the encoder-decoder architecture by leveraging the complementarity of events and frames. In addition, the superiority of the proposed fusion architecture that fuses the RGB and event frames at multiple resolutions, and utilises skip connections and atrous convolutions are demonstrated by experimental comparisons to other alternative fusion strategies, including the state-of-the-art transformer-based CMX.


Our future objective is to improve the current method's temporal limitations caused by event framing by incorporating an asynchronous event-based approach to effectively leverage the event camera's high temporal resolution for unknown object segmentation.
 



\bibliographystyle{IEEEtran}

\bibliography{references.bib}


%  Xiaoqian Huang, Kachole Sanket, Abdulla Ayyad, Fariborz Baghaei Naeini, Dimitrios Makris, and Yahya Zweiri "A Neuromorphic Dataset 
%  for Object Segmentation in Indoor Cluttered Environment" https://kuacae-my.sharepoint.com/:f:/g/personal/100049863_ku_ac_ae/263 
% Elpr7P2iqPxJurmIo6ssGQMBnBcNSwKolYDrIR8MHbntfw?e=v7K7fj", 2023.




















\end{document}


