\section{Introduction}

% \todo{an eye-catching awe-inspiring breathtaking introduction}

Understanding an in-hand object's pose and shape is a fundamental component to many robotic manipulation tasks, such as grasping \cite{ferrari1992planning}, assembly \cite{zhao2020towards}, and tool using \cite{fang2020learning}. 
While pose estimation and shape reconstruction through vision have been active areas of research in the computer vision domain \cite{wang2019densefusion,xiang2017posecnn,hodan2018bop}, physical interactions through touch provide humans with an effective yet often underestimated way to understand the scene \cite{klatzky1985identifying}. 
With the recent advance in high-resolution tactile sensing \cite{wang2021gelsight,kappassov2015tactile}, researchers finally have the means to obtain detailed contact information such as the shape and force in a real-time manner. 
Enabled by these newly developed sensors, several works focus on scene understanding relying solely on high definition tactile sensing \cite{yuan2015measurement,zhang2018fingervision,bauza2019tactile,bauza2022tac2pose,suresh2021tactile,suresh2022shapemap,sodhi2022patchgraph}.

However, humans use both vision and tactile during shape inference and localization \cite{klatzky1987there}.
Vision provides a global understanding of the scene, while tactile provides a detailed view for the contact geometry.
Both modalities are needed for precise localization and reconstruction.
In this work, we tackle the problem of localizing and reconstructing an unknown in-hand object by combining the two sensing modalities.

To solve this problem, we need to answer the following question: How can we calculate an optimal pose given multiple sensory estimates that do not always agree with each other?
Simultaneous Localization And Mapping (SLAM) is a well-studied problem in the field robotics domain \cite{durrant2006simultaneous}.
Central to SLAM is the task of jointly optimizing an objective, often a multi-dimensional pose, by leveraging multiple sensory measurements and producing a more accurate and tractable solution.
One popular choice of such optimization frameworks is the factor graph, which models different sensory measurements and pose priors as factors in a bipartite graph.
Due to its flexibility and computational efficiency, we chose it as the optimization tool for our visuo-tactile pose estimation and shape reconstruction task.

Our main contributions are:
\begin{itemize}
    \item A factor graph-optimized 6-DoF pose estimator that combines global vision and local tactile tracking.
    \item A loop closure matching process that effectively reduces accumulated errors.
    % \item A shape reconstruction pipeline that produces realistic reconstructions 
    % \item Quantitative and qualitative evaluations on both artificial and real-world objects for localization and reconstruction.
\end{itemize}

We train FingerSLAM with 20 real-world objects, and evaluate it on 6 objects that are unseen during training.
FingerSLAM achieves a pose tracking accuracy of $2.8mm / 2.4deg$, and produces realistic reconstructed point clouds.
% It also produces realistic reconstructions of the in-hand object without 