\section{Related Works}

\begin{figure*}[!ht]
\centering
\includegraphics[width=\linewidth]{body/figures/data_collection2.png}
\caption{\textbf{Hardware Setup.} We use a GelSight Wedge sensor for tactile sensing, an Intel ReslSense D405 camera mounted on the side for RGB vision sensing, and an OptiTrack setup for motion capture. \textbf{Data Collection.} The tactile finger and the camera are fixed to the table at all times. A human operator moves a test object and presses it against the finger. We show sampled tactile and RGB images as well as a reconstructed local tactile depth map on the right.}
\label{datacollection}
\end{figure*}

% \textbf{Tactile sensors}:
% Over the years, researchers have developed tactile sensors working on different sensing principles, such as resistance, capacitance, magnetic, barometric, and optic.
% We refer readers to \cite{kappassov2015tactile} for an in-depth review of different types of tactile sensors and their applications.
% Compared to other sensing principles, GelSight tactile sensors have the advantage of providing high-resolution geometrical information of the contact surface.
% They are usually constructed with an elastic silicone gel, directional colored LEDs, and a camera pointing at the gel.
% The gels are usually coated with reflective paint with printed dots.
% When in contact, the gel deforms and takes the shape of the contact surface.
% Shear force can be retrieved by tracking dot movements.
% Furthermore, the color value of a pixel is correlated with the gradient of the height of the contact surface at the specific location. 
% With a pre-calibrated color table, a depth map can be reconstructed from the color image.
% This type of tactile sensor is selected for our work for its rich output and ease to use.

Researchers of the robotics community have put forward a wide range of tactile sensing solutions.
Sensors working on different sensing principles have been adopted to solve a large set of manipulation tasks.
Among different types of tactile sensors, vision-based ones such as GelSight \cite{yuan2017gelsight} and GelSlim \cite{donlon2018gelslim} stand out for their rich output, ease to use, and affordability.
While we focus on the pose estimation and shape reconstruction task using vision-based tactile sensors, we refer readers to \cite{kappassov2015tactile} for an in-depth review of different types of tactile sensing and their applications.
In this section, we review works on three typical tasks that are most relevant to our solution: slip detection, object property inference, and SLAM.

% Researchers have found that using this class of vision-based tactile sensors can greatly increase the accuracy when reasoning about the contact surface, compared to traditional tactile sensors that are constructed with normal direction force sensors [\todo{add citation}].

\textbf{Slip detection and estimation}:
Using a similar sensor to ours, Yuan \etal compared and analyzed a GelSight tactile sensor's images collected at different stages of slip in \cite{yuan2015measurement} and showed this type of sensors' capability in detecting micro scale movements.
Li \etal and Zhang \etal trained recurrent neural networks on tactile images to detect slip between multiple time steps in a manipulation sequence \cite{li2018slip, zhang2018fingervision}.
Built on their binary slip detection model in \cite{li2018slip}, Li further added rotational slip direction prediction in \cite{li2019rotational}.
Calandra \etal improved a grasp planner for the classic robot bin-picking problem by incorporating slip detection and achieved a higher grasp success rate \cite{calandra2017feeling}. 
However, those methods only detect slip without localizing the object after the slip.
In many precision manipulation tasks we are also interested in the amount of the displacement.

\textbf{Object property inference and localization}:
% With detailed information on the contact surface provided by high-resolution tactile sensors, 
Many works have focused on inferring properties of the in-contact object, such as shape \cite{strub2014using, luo2015tactile, luo2019iclap}, texture \cite{luo2018vitac, yuan2017connecting}, and material \cite{yuan2017connecting, kroemer2011learning, kerr2018material}.
Those learned object properties can be further used for localization.
In order to localize current grasps, Bauza \etal proposed to match new tactile imprints with previously collected tactile imprints \cite{bauza2019tactile}, while Luo \etal learned to match tactile imprints directly to visual images of the whole object \cite{luo2015localizing}.
Assuming known CAD models, Bauza \etal proposed to localize by comparing contact masks generated from tactile images with a large bank of random projections of the CAD model \cite{bauza2022tac2pose}.
To solve the reverse problem, i.e. what a tactile image looks like given an object and a pose, several tactile simulators have been built to automatically generate tactile images given an object's CAD model and a finger pose \cite{si2022taxim, wang2022tacto}.
One major limitation for this category of works is that they all require a known calibrated geometry of the object: a pre-collected tactile map \cite{bauza2019tactile}, a model of the object \cite{bauza2022tac2pose}, or a global image with known geometry \cite{luo2015localizing}.
This requirement can be hard to meet in less constraint environments.

\textbf{Tactile SLAM}:
Recent studies have shown interests in working with unknown objects by leveraging methods from the SLAM problem.
With a focus on 2D shapes, Suresh \etal parameterized shapes as Gaussian Process Implicit Surfaces (GPIS), and learned its parameters from tactile signals collected during pushing \cite{suresh2021tactile}.
Assuming known contact poses, authors of \cite{suresh2022shapemap} first learned a noisy mapping from known surface geometries to corresponding tactile images, then reconstructed an object by combining many noisy local tactile measurements into an optimized global shape using factor graph optimization.
The closest prior work to ours is \cite{sodhi2022patchgraph}, where the authors learned to estimate 6D poses and 3D shapes simultaneously for unknown objects. 
They constructed a pose estimator based on tactile sensing, and a shape reconstruction pipeline that added in new tactile point clouds incrementally on the run.
However, this approach heavily relies on the performance of the tactile pose estimator, which lacks a global understanding of the object and can suffer from repeated patterns or smooth surfaces.
In contrast, our work combines vision and tactile sensing which provides us with both global and local understandings of the scene without requiring any other domain knowledge.
Furthermore, we designed a loop closure mechanism that periodically matches current tactile and vision images to stored key-frames, which significantly reduced accumulated errors.
With this, FingerSLAM is able to produce realistic reconstructions even in long sequences. 