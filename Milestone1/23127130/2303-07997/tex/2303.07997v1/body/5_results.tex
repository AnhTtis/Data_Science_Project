\section{Experiments and Evaluation}
\label{sec:eval}

\subsection{Experiment Setup}
We collected real world data $(D_t, I_t, P_t)$'s for both training and evaluation. 
In this section we discuss our data collection setup, with the procedure shown in Fig. \ref{datacollection}.

% \begin{figure}[!ht]
% \centering
% \includegraphics[width=0.9\linewidth]{body/figures/hardware.png}
% \caption{Hardware setup. We used: (1) a GelSight Wedge sensor for tactile sensing, (2) an Intel ReslSense D405 camera mounted on the side for RGB vision sensing, and (3) a 4-camera OptiTrack setup for motion capture.}
% \label{hardware}
% \end{figure}

\textbf{Sensors}:
We used a GelSight Wedge finger for tactile sensing and an Intel RealSense\textsuperscript{TM} D405 camera mounted on the side of the GelSight sensor to mimic the view of a wrist-mounted camera.
Note that even though the D405 camera is a near-range RGB-D camera, we found its depth sensing to be rather noisy in our settings, thus only RGB information was used.

\textbf{Motion Capture}:
We used 4 OptiTrack Flex-3\textsuperscript{TM} cameras to capture the ground truth movement of the object.
We attached a number of reflective markers to the base of the finger and the base of the object.
During experiments, the finger and the RGB camera were always fixed on the desk. 
A human operator held the object and pressed the object against the finger tip at different locations.
Images from both sensors $(D_t, I_t)$ as well as the ground truth pose of the object in the finger's coordinate frame $P_t$ were recorded at each time step.

\textbf{Objects}:
We used 20 3D-printed objects for training, as well as 4 3D-printed and 2 real-world objects for evaluation.
The training objects are shown in Fig. \ref{trainobj}.
Each individual evaluated object will be discussed in Sec. \ref{sec:evalres}.
All objects were selected from the YCB object dataset \cite{calli2015ycb} or the ABC object dataset \cite{Koch_2019_CVPR}.
The two real-world objects are identical glass jars, except one was painted red and the other one was clear.
The two jars were used to test the generalizability of our pose estimator.
We collected about 350 data points on each object, which gave us roughly 7,000 raw training data and 2,500 raw evaluation data.
Because of the difficulty in collecting a larger amount of data in the real world, we augmented the training data in two ways:
\begin{itemize}
    \item For the tactile pose estimator, we artificially translate and rotate the depth images randomly. 
    We first generated multiple random $SE(2)$ poses, $\hat{p}$'s, for each $(P_{t-1}^t, D_{t-1}, D_{t})$ pair, and we applied $\hat{p}$'s to $P_{t-1}^t$ and $D_{t-1}$.
    The generated $(\hat{P}_{t-1}^t, \hat{D}_{t-1}, D_{t})$'s were used as augmented training data.
    \item For both tactile and vision, instead of only using data points collected at consecutive time steps, we also used data points collected at further away time steps as long as the relative displacement between them was smaller than $12mm$.
\end{itemize}
The final size of our training dataset was 40,000 for tactile and 25,000 for vision.
No augmentation was done to the evaluation data.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.75\linewidth]{body/figures/trainobj.png}
\caption{Objects used during training. We selected 20 objects from the YCB and the ABC dataset.}
\label{trainobj}
\end{figure}

\subsection{Pose Tracking Performance}
\label{sec:evalres}
In this section two evaluation methods and their results are discussed: 
(1) an ablation study of the pose estimator's per-component performance, and (2) a comparison between the proposed pose estimator's performance and the size of the raw movement.

\textbf{Ablation study}:
Detailed numerical results of the proposed system's per-component performance on each object are shown in Tab. \ref{tab:ablation}. 
The \textit{Raw} column shows the median of the raw movement between two time steps.
This was calculated from the poses collected from the motion capture system.
The rest of the four columns shows the median error obtained from each corresponding pose estimator.

\textit{Tactile v. Vision}:
Comparing the top three objects against the bottom three, we see a clear trend that the tactile estimator performed better when there were unique and fine textures on the object.
We found the tactile estimator to perform the worst on the \textit{Mini Bleach} object, likely due to its mostly smooth surface.
The vision estimator struggled on the two jars, which we believe was attributed to their significantly larger sizes than all the training objects.
Comparing the two jars, tactile performed almost equally, while vision performed worse on the transparent one.
This shows that estimating with tactile has the advantage of generalizability across apparent properties.

\textit{Factor graph optimization}: 
The \textit{Combined} column shows prediction errors of the optimized estimator which combined tactile and vision, without loop closure detection.
For the bottom 4 objects, the \textit{Combined} method achieved a performance in between the performances of tactile standalone and vision standalone.
% In those cases, we believe the factor graph combined the two measurements and produced an averaged estimation.
For the top 2 objects, \textit{Combined} outperformed both standalone methods.
In those cases, the factor graph was able to retrieve good individual channels and compose a 6-DoF prediction that had a higher accuracy.
Moreover, the combined method produced very close accuracy on the two jars, despite the vision estimator's much poorer performance on the transparent jar.
This further confirms that the combined method was able to pick more valuable channels between the estimators.

\textit{Closed-loop factor graph}:
The last column shows the performance of FingerSLAM, the factor graph-optimized estimator with loop closure checking.
We did parameter tuning on the loop closure checking threshold.
The best result was obtained with $a = 0.3, b = 0.7, c = 0.9$ in Eq. \ref{eq:lc}.
Vision was assigned a higher weight for this loop closure process, because tactile suffered more from repeated patterns.
Looking at the \textit{w/ Loop Closure} column, FingerSLAM consistently outperformed the \textit{Combined} method for all objects.
In examinations of per-step errors within an episode, we found that loop closure significantly helped cases when a newly visited spot was close to a previously visited spot.
% At those steps, the \textit{Combined} method's error was usually much higher than \textit{w/ Loop Closure} due to error accumulating. 

\begin{table*}[!htb]
    % \hspace{0.0\linewidth}
    % \begin{minipage}{.45\linewidth}
    %\strut\vspace*{-4.1\baselineskip}% \newline
      \centering
        \caption{P.E. error per component per object}
        \label{tab:ablation}
        \begin{tabular}{c|c|P{5em}|P{5em}|P{5em}|P{5em}|P{6.9em}}
    \hline
    Name & Picture & Raw\newline mm : deg & Tactile only \newline  mm : deg & Vision only \newline  mm : deg & Combined \newline  mm : deg & w/ Loop Closure \newline  mm : deg \\
        \hline
    Letter Cube 
    & \parbox[c]{10em}{\includegraphics[width=9em]{body/figures/MIT_letter.png}} &
    6.18  : 6.34 & 3.10  : 2.79   & 2.99  : 2.61   & 2.87  : 2.29   & 2.27  : 2.18  \\
    Graphic
    & \parbox[c]{10em}{\includegraphics[width=9em]{body/figures/graphic1.png}} &
    5.93  : 5.59 & 2.45  : 2.61   & \cellcolor{green!20} 2.86  : 2.11   & \cellcolor{green!20}2.28  : 2.13   & \cellcolor{green!20}2.13  : 2.07  \\
    % Graphic 2
    % & \parbox[c]{15em}{\includegraphics[width=15em]{body/figures/bird.png}} &
    % & 3.18 : 1.90   & 3.59 : 2.16   & 3.04 : 1.58   & 2.99 : 1.60  \\
    Rubik's Cube
    & \parbox[c]{10em}{\includegraphics[width=9em]{body/figures/rubbik.png}} &
    5.88  : 5.66   & \cellcolor{green!20} 2.33  : 2.80   & 2.97  : 2.37   &  2.50  : 2.43   & 2.30  : 2.41  \\
    Mini Bleach
    & \parbox[c]{10em}{\includegraphics[width=9em]{body/figures/bleach.png}} &
    5.95  : 5.74   & \cellcolor{red!20}4.52  : 3.77   & 3.25  : 2.54   & \cellcolor{red!20}4.06  : 2.91   & \cellcolor{red!20} 3.46  : 2.50  \\
    Painted Glass Jar 
    & \parbox[c]{10em}{\includegraphics[width=9em]{body/figures/paintedjar.png}}  & 
    6.40  : 6.21   & 3.50  : 2.70   & 4.12  : 2.94   & 3.63  : 2.63   & 3.19  : 2.82  \\
    Glass Jar 
    & \parbox[c]{10em}{\includegraphics[width=9em]{body/figures/glassjar.png}} &
    6.46  : 6.02   & 3.58  : 2.47   & \cellcolor{red!20}4.79  : 4.21   & 3.79  : 2.27   & 3.38  : 2.27  \\
     \hline
  \end{tabular}
    % \end{minipage} 
\end{table*}

\textbf{Performance v. step size}:
A pose estimator's error hugely depends on the size of the original movement.
We grouped all our evaluation data points in three groups according to the size of their original movement: (a) data pairs that has a movement of less than $5mm$, (b) data pairs of movement between $5mm$ and $10mm$, and (c) data pairs with more than $10mm$ movement.
Detailed per-component performance for each movement group is shown in Tab. \ref{tab:stepsize}.
When step sizes are small (the $<5mm$ group), tactile sensing provides a more accurate estimation compared to vision, which we believe attributed to the fact that tactile has a higher resolution for local movements.
The situation reversed when the step size was large (the $>10mm$ group), likely due to the increased difficulty in finding correlations from two tactile images when the overlapping area is small.
Averaging all our evaluation data (the $all$ row), the tactile estimator performed better than the vision estimator, and our proposed closed-loop combined method achieved the best overall pose estimation result.

\vspace{-1em}

\begin{table}[!htb]
\renewcommand*{\arraystretch}{1.4}
    % \caption{Global caption} 
    % \begin{minipage}{.5\linewidth}
      \caption{P.E. error at each raw movement size group}
      \label{tab:stepsize}
      \centering
        \begin{tabular}{c|c|c|c|c|c}
    \hline
Raw & \scriptsize Raw Median \par& Tactile & Vision & Combined & w/ LC \\
Group& mm:deg& mm:deg& mm:deg& mm:deg& mm:deg\\
\hline
\scriptsize$<$5mm \par& 3.1 : 3.8 & 1.0 : 2.3  & 2.6 : 2.7  & 1.8 : 2.3  & 1.4 : 2.3  \\
\scriptsize5-10mm \par& 7.1 : 5.7  & 3.3 : 2.9  & 3.9 : 3.0  & 3.8 : 2.8  & 2.8 : 2.8  \\
\scriptsize$>$10mm \par& 11.7 : 5.9  & 6.8 : 3.1  & 6.1 : 3.2  & 6.3 : 3.1  & 6.0 : 3.1  \\
\hline
all & 6.7 : 5.6  & 3.0 : 2.7  & 3.5 : 2.9  & 3.2 : 2.4  & 2.8 : 2.4  \\
 \hline
  \end{tabular}
    % \end{minipage}%
\end{table}

\vspace{-1em}

\subsection{Reconstruction Performance}
Qualitative reconstruction results of two objects are shown in Fig. \ref{reconstruction}.
Each sequence used for reconstruction consists of 21 time steps, with every 7 time steps as an episode.
After each episode, the factor graph was reset and a new pose from the motion capture system was used as the initial pose prior for the next episode.
Comparing shapes reconstructed from the tactile pose estimator only (middle column) and the proposed FingerSLAM pose estimator (right column), FingerSLAM wins clearly with much less mis-alignments.
Comparing shapes reconstructed from OptiTrack poses (left column) and FingerSLAM (right column), we see that the two columns share similar overall shape.
The right column shapes are easily recognizable, and appear consistent with the left column despite small mis-alignments in certain areas.

\begin{figure}[ht]
\includegraphics[width=\linewidth]{body/figures/recon.png}
\caption{Shaped reconstructed from 21 local tactile patches, with poses produced by the motion capture system (left), the tactile pose estimator only (middle), and the proposed estimator (right).}
\label{reconstruction}
\centering
\end{figure}