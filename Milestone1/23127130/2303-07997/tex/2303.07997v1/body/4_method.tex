\section{Localization and Reconstruction}

\begin{figure*}[ht]
\centering
\includegraphics[width=0.8\linewidth]{body/figures/factorgraph.png}
\caption{Factor graph formulation. $O_t$ denotes the observation (tactile and vision images) at each time step, while $P_t$ denotes the estimated $SE(3)$ pose at each time step. Each factor is denoted as color dots.}
\label{factorgraph}
\end{figure*}

\label{sec:approach}
% This section discusses our proposed framework for the localization and reconstruction problem.
We begin with pre-processing steps for tactile images, follow with our factor graph formulation of the localization problem, end with our reconstruction approach.

\subsection{Local Surface Reconstruction From Tactile Images}
\label{sec:preprocess}
Local surface depth maps are reconstructed with photometric stereo for the GelSight Wedge \cite{wang2021gelsight} sensor that we used in this project.
On the outside of a GelSight Wedge sensor is a thin layer of elastic silicone gel, which is coated with reflective paint.
Once pressed, the gel conforms to the contact surface, and its inner side is illuminated by light sources placed at three sides of the gel with three different colors.
A color image is taken, and the color value at each pixel is correlated to the gradients at two directions $\nabla_x, \nabla_y$ at that location. 
During post-processing, such images are first unwarped to remove optical distortion.
Then the color image is converted to a gradient map using a pre-calibrated look-up table.
At each pixel, the gradients $\nabla_x, \nabla_y$ are recovered. 
A Poisson solver is used to reconstruct a depth map from this gradient map. 
An example raw tactile image and its corresponding reconstruction can be found in Fig. \ref{datacollection}.

% \begin{figure}[ht]
% \centering
% \includegraphics[width=\linewidth]{body/figures/data_collection.png}
% \caption{(a) A GelSight Wedge sensor was used for tactile sensing. (b-left) A sample RGB image collected from a wrist-mounted camera. (b-right) A raw tactile image. (c) Reconstructed point cloud of the contact, processed from the tactile image.}
% \label{datacollection}
% \end{figure}

\subsection{Pose Estimation with Factor Graph}
\label{sec:poseestimate}
Following works in the tactile SLAM domain \cite{suresh2021tactile, sodhi2022patchgraph}, we choose factor graph to synthesize pose measurements from multiple learned estimators. 
A factor graph is a bipartite graph, with factors $F$ as constraints and variables $P$ as optimization targets. 
In our case, $P$ is the object's 6-DoF pose at all time steps, and $F$ consists of a prior $F_{pri}$ and three measurements: tactile odometry $F_{tac}$, vision odometry $F_{vis}$, and loop closure $F_{lc}$.
The optimization problem solves for the best $\bm{P}^* = P_{t=0:N}^*$ such that its posterior probability is maximized (MAP). 
Measurement noise for each factor is modeled as a zero-mean Gaussian distribution, with a covariance matrix $\Sigma$.

\begin{align*} 
\bm{P}^* = \argmax_{\bm{P}} & \Biggl[ ||F_{pri}(P_0)||_{\Sigma_p}^2 + \sum_{t=1}^N \Bigl( ||F_{tac}(P_{t-1}, P_t)||_{\Sigma_t}^2+\Bigr.\Biggr.\\
&\Biggl.\Bigl.||F_{vis}(P_{t-1}, P_t)||_{\Sigma_v}^2 + ||F_{lc}(P_t)||_{\Sigma_l}^2\Bigr) \Biggr]
\end{align*}

Our factor graph formulation is illustrated in Fig. \ref{factorgraph}.
We used the GTSAM C++ library \cite{dellaert2012factor} for factor graph optimization.
Next, we discuss details for each factor.

\textbf{Prior}: 
A unary prior is added to the factor graph only at the initial time step of an episode. 
This prior anchors the object's estimated pose sequence at a given initial pose.
Covariance for the noise model $\Sigma_p$ is zero.

\textbf{Tactile odometry with multiple-step refinement}:
Given tactile depth maps collected at two consecutive time steps that sufficiently overlaps with each other, $D_{t-1}$ and $D_t$, it it possible to tell the relative movement of the object $P_{t-1}^t$ between the two time steps.
We learn the mapping $P_{t-1}^t = f_{tacNN}(D_{t-1}, D_t)$ using convolutional neural networks. 
The architecture of $f_{tacNN}$ is illustrated on the left of Fig. \ref{nn}.

Inspired by \cite{wang2019densefusion}, we propose a multi-pass refinement process which runs $f_{tacNN}$ multiple times until convergence.
This process is detailed in Alg. \ref{alg:tacref}.
A sequence of resulted intermediate $\Bar{D}_{t-1}^i$ as well as $D_t$ are shown in Fig. \ref{tactilestage}.
As $i$ (the number of passes) goes larger, $\Bar{D}_{t-1}^i$ appears closer to $D_t$, which means the estimated pose improves over time.

\begin{algorithm}
\caption{Tactile pose estimator with multi-step refinement}\label{alg:tacref}
\KwData{$D_{t-1}, D_t$}
\KwResult{$P_{t-1}^t$}
$P_{t-1}^t \gets \begin{bmatrix}
    I_{3\times 3} & [0]_{3\times 1}\\
    0 & 1
\end{bmatrix}$\;
$\Bar{D}_{t-1}^0 \gets D_{t-1}$\;
$i \gets 1$\;
$\sigma \gets$ a convergence threshold chosen empirically\;
\While{$i \leq 10$}{
  % \Return{a}
  $\Delta P \gets f_{tacNN}(\Bar{D}_{t-1}^{i-1}, D_t)$\;
  \Comment{Run tacNN to obtain a delta estimate}
  
  $\Bar{D}_{t-1}^i \gets warp(\Bar{D}_{t-1}^{i-1}, \Delta P)$\;
  \Comment{Warp $\Bar{D}_{t-1}^{i-1}$ with the delta estimate}
  
  $P_{t-1}^t \gets \Delta P \cdot P_{t-1}^t$\;
  \Comment{Apply the delta estimate to the result}
  
  \If{$||\Delta P|| < \sigma$}{
    \Comment{Break early if converged}
    
    \Return{$P_{t-1}^t$}\;
  }
  $i \gets i + 1$\;
}
\Return{$P_{t-1}^t$}\;
\end{algorithm}

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{body/figures/nn.png}
\caption{Neural network architecture for tactile odometry and vision odometry. (left) The tactile odometry network is composed of two convolutional branch models followed by three fully connected (FC) layers. (right) The vision odometry is constructed similarly to the tactile odometry, with the branch model being replaced with convolutional layers extracted from a pre-trained network, MobileNetv3. In both networks, the two branch models share weights.}
\label{nn}
\end{figure}

In experiments we found that the neural network $f_{tacNN}$ learned better when only predicting $SE(2)$ poses, $\{ x, y, \gamma_{yaw} \}$, likely due to the fact that the tactile sensor has a flat surface and a relative thin gel, which means the sensor is only sensitive to tangential movements.
As such, we only predict $\{ x, y, \gamma_{yaw} \}$ with this odometry, and populate $\{ z, \alpha_{roll}, \beta_{pitch} \}$ as all zero, and assign large values to the corresponding cells in the covariance matrix $\Sigma_t$ for the noise model.
Values in other cells of $\Sigma_t$ are chosen empirically.
% should I explain this a bit more?

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{body/figures/tactilestage.png}
\caption{A sample sequence of intermediate and goal tactile images from multi-step refined pose estimation. $D_{t-1}$ and $D_t$ are the raw tactile images collected at time step $t-1$ and $t$. Intermediate $\Bar{D}_{t-1}^i$'s are warped versions of $D_{t-1}$ according to the estimated pose at each step. After the last step $i=3$ in the refinement process, the error was reduced to $0.6mm$, compared to the original movement of $5.5mm$. Visually, $\Bar{D}_{t-1}^i$ gets closer to $D_t$ as $i$ increases.}
\label{tactilestage}
\end{figure}

\textbf{Vision odometry}:
Taken input of RGB images captured from a wrist-mounted camera at two consecutive time steps, $I_{t-1}$ and $I_t$, another CNN $f_{visNN}$ is trained to learn the mapping $P_{t-1}^t = f_{visNN}(I_{t-1}, I_t)$.
The architecture is illustrated on the right of Fig. \ref{nn}.
Unlike $f_{tacNN}$, $f_{visNN}$ predicts the full $SE(3)$ pose because visual images capture global information.
We run it for only one pass at each time step.
Following examples of many works on visual perception, we bootstrap $f_{visNN}$ with feature layers of models that are pre-trained on a larger image dataset.
MobileNetV3 \cite{howard2017mobilenets} is chosen for this task, due to its compact size.
We extract the convolutional layers from the pre-trained model and follow them with four fully connected layers.
The pre-trained layers were frozen at the beginning of training, then they were un-frozen to allow for fine-tuning.
Noise covariance $\Sigma_v$ is chosen empirically.

\textbf{Loop closure}:
% In any robust navigation system, checking for loop closures, i.e. recognizing previously visited locations, is essential as it 
Starting from the $3^{rd}$ time step, loop closure is checked between each new time step and all previous time steps that are at least 3 steps earlier, i.e. between the two time steps $t_{i, i>2}$ and $t_{j, j=\forall [1, i-2]}$ we check $LC(i, j) = \{ True, False\}$.
Two neural networks are trained for this process, $G_{tac}(D_{i}, D_{j})$ and $G_{vis}(I_i, I_j)$.
Both output a scalar value between $0$ and $1$ which represents the likelihood that there is a loop closure between $t_i$ and $t_j$.
$G_{tac}(D_{i}, D_{j})$ takes tactile depth maps as inputs and $G_{vis}(I_i, I_j)$ takes RGB images from the wrist-mounted camera as inputs. 
Both neural networks share the same architecture as the odometry networks but with a different output layer, whose size is equal to one.
Then, we let 
\begin{equation}
    \label{eq:lc}
    LC(i, j) = a \times G_{tac}(D_{i}, D_{j}) + b \times G_{vis}(I_i, I_j) \overset{?}{>}  c
\end{equation}
with $a, b$ as weights and $c$ as a threshold.
All three values are chosen empirically.

If any pair of two time steps is considered as a match, we run both the vision odometry and the multi-pass tactile odometry between this pair, and add the measurement to the factor graph.

\subsection{Shape Reconstruction}
\label{sec:reconstruction}
With tactile depth maps $D_t$ and the optimized object's pose $P_t$ at each time step, we reconstruct the object's shape by stitching all $D_i$'s together.
Since the reconstructed shape is not used for any further prediction, the reconstruction is done trivially by scaling $D_t$ to its true size, converting it to a point cloud, anchoring the point cloud to pose $P_t$, and finally reducing the whole point cloud with voxel down-sampling.
Voxel size is chosen empirically.