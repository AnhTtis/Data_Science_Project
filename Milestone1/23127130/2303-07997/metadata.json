{
    "arxiv_id": "2303.07997",
    "paper_title": "FingerSLAM: Closed-loop Unknown Object Localization and Reconstruction from Visuo-tactile Feedback",
    "authors": [
        "Jialiang Zhao",
        "Maria Bauza",
        "Edward H. Adelson"
    ],
    "submission_date": "2023-03-14",
    "revised_dates": [
        "2023-03-15"
    ],
    "latest_version": 1,
    "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV"
    ],
    "abstract": "In this paper, we address the problem of using visuo-tactile feedback for 6-DoF localization and 3D reconstruction of unknown in-hand objects. We propose FingerSLAM, a closed-loop factor graph-based pose estimator that combines local tactile sensing at finger-tip and global vision sensing from a wrist-mount camera. FingerSLAM is constructed with two constituent pose estimators: a multi-pass refined tactile-based pose estimator that captures movements from detailed local textures, and a single-pass vision-based pose estimator that predicts from a global view of the object. We also design a loop closure mechanism that actively matches current vision and tactile images to previously stored key-frames to reduce accumulated error. FingerSLAM incorporates the two sensing modalities of tactile and vision, as well as the loop closure mechanism with a factor graph-based optimization framework. Such a framework produces an optimized pose estimation solution that is more accurate than the standalone estimators. The estimated poses are then used to reconstruct the shape of the unknown object incrementally by stitching the local point clouds recovered from tactile images. We train our system on real-world data collected with 20 objects. We demonstrate reliable visuo-tactile pose estimation and shape reconstruction through quantitative and qualitative real-world evaluations on 6 objects that are unseen during training.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.07997v1"
    ],
    "publication_venue": "Submitted and accepted to 2023 IEEE International Conference on Robotics and Automation (ICRA 2023)"
}