@inproceedings{han2023medgen3d,
  title={Medgen3d: A deep generative framework for paired 3d image and mask generation},
  author={Han, Kun and Xiong, Yifeng and You, Chenyu and Khosravi, Pooya and Sun, Shanlin and Yan, Xiangyi and Duncan, James S and Xie, Xiaohui},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={759--769},
  year={2023},
  organization={Springer}
}

@article{you2024rethinking,
  title={Rethinking semi-supervised medical image segmentation: A variance-reduction perspective},
  author={You, Chenyu and Dai, Weicheng and Min, Yifei and Liu, Fenglin and Clifton, David and Zhou, S Kevin and Staib, Lawrence and Duncan, James},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{mei2022radimagenet,
  title={RadImageNet: an open radiologic deep learning research dataset for effective transfer learning},
  author={Mei, Xueyan and Liu, Zelong and Robson, Philip M and Marinelli, Brett and Huang, Mingqian and Doshi, Amish and Jacobi, Adam and Cao, Chendi and Link, Katherine E and Yang, Thomas and others},
  journal={Radiology: Artificial Intelligence},
  volume={4},
  number={5},
  pages={e210315},
  year={2022},
  publisher={Radiological Society of North America}
}

@inproceedings{Karras2021,
  author = {Tero Karras and Miika Aittala and Samuli Laine and Erik H\"ark\"onen and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  title = {Alias-Free Generative Adversarial Networks},
  booktitle = {Proc. NeurIPS},
  year = {2021}
}

@misc{akroutDiffusionbasedDataAugmentation2023,
  title = {Diffusion-Based {{Data Augmentation}} for {{Skin Disease Classification}}: {{Impact Across Original Medical Datasets}} to {{Fully Synthetic Images}}},
  shorttitle = {Diffusion-Based {{Data Augmentation}} for {{Skin Disease Classification}}},
  author = {Akrout, Mohamed and Gyepesi, B{\'a}lint and Holl{\'o}, P{\'e}ter and Po{\'o}r, Adrienn and Kincs{\H o}, Bl{\'a}ga and Solis, Stephen and Cirone, Katrina and Kawahara, Jeremy and Slade, Dekker and Abid, Latif and Kov{\'a}cs, M{\'a}t{\'e} and Fazekas, Istv{\'a}n},
  year = {2023},
  month = jan,
  number = {arXiv:2301.04802},
  eprint = {arXiv:2301.04802},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.04802},
  abstract = {Despite continued advancement in recent years, deep neural networks still rely on large amounts of training data to avoid overfitting. However, labeled training data for real-world applications such as healthcare is limited and difficult to access given longstanding privacy, and strict data sharing policies. By manipulating image datasets in the pixel or feature space, existing data augmentation techniques represent one of the effective ways to improve the quantity and diversity of training data. Here, we look to advance augmentation techniques by building upon the emerging success of text-to-image diffusion probabilistic models in augmenting the training samples of our macroscopic skin disease dataset. We do so by enabling fine-grained control of the image generation process via input text prompts. We demonstrate that this generative data augmentation approach successfully maintains a similar classification accuracy of the visual classifier even when trained on a fully synthetic skin disease dataset. Similar to recent applications of generative models, our study suggests that diffusion models are indeed effective in generating high-quality skin images that do not sacrifice the classifier performance, and can improve the augmentation of training datasets after curation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/bram/Zotero/storage/SL36S6RK/Akrout et al. - 2023 - Diffusion-based Data Augmentation for Skin Disease.pdf;/home/bram/Zotero/storage/A7ELXNCV/2301.html}
}

@software{AUTOMATIC1111_Stable_Diffusion_Web_2022,
author = {AUTOMATIC1111},
month = aug,
title = {{Stable Diffusion Web UI}},
url = {https://github.com/AUTOMATIC1111/stable-diffusion-webui},
year = {2022}
}

@inproceedings{bermudezLearningImplicitBrain2018,
  title = {Learning Implicit Brain {{MRI}} Manifolds with Deep Learning},
  booktitle = {Medical {{Imaging}} 2018: {{Image Processing}}},
  author = {Bermudez, Camilo and Plassard, Andrew J. and Davis, Larry T. and Newton, Allen T. and Resnick, Susan M. and Landman, Bennett A.},
  year = {2018},
  month = mar,
  volume = {10574},
  pages = {408--414},
  publisher = {{SPIE}},
  doi = {10.1117/12.2293515},
  abstract = {An important task in image processing and neuroimaging is to extract quantitative information from the acquired images in order to make observations about the presence of disease or markers of development in populations. Having a low-dimensional manifold of an image allows for easier statistical comparisons between groups and the synthesis of group representatives. Previous studies have sought to identify the best mapping of brain MRI to a low-dimensional manifold, but have been limited by assumptions of explicit similarity measures. In this work, we use deep learning techniques to investigate implicit manifolds of normal brains and generate new, high-quality images. We explore implicit manifolds by addressing the problems of image synthesis and image denoising as important tools in manifold learning. First, we propose the unsupervised synthesis of T1-weighted brain MRI using a Generative Adversarial Network (GAN) by learning from 528 examples of 2D axial slices of brain MRI. Synthesized images were first shown to be unique by performing a cross-correlation with the training set. Real and synthesized images were then assessed in a blinded manner by two imaging experts providing an image quality score of 1-5. The quality score of the synthetic image showed substantial overlap with that of the real images. Moreover, we use an autoencoder with skip connections for image denoising, showing that the proposed method results in higher PSNR than FSL SUSAN after denoising. This work shows the power of artificial networks to synthesize realistic imaging data, which can be used to improve image processing techniques and provide a quantitative framework to structural changes in the brain.},
  file = {/home/bram/Zotero/storage/Z7AI7NPS/Bermudez et al. - 2018 - Learning implicit brain MRI manifolds with deep le.pdf}
}

@article{bovenbergHowFixGDPR2020,
  title = {How to Fix the {{GDPR}}'s Frustration of Global Biomedical Research},
  author = {Bovenberg, Jasper and Peloquin, David and Bierer, Barbara and Barnes, Mark and Knoppers, Bartha Maria},
  year = {2020},
  month = oct,
  journal = {Science},
  volume = {370},
  number = {6512},
  pages = {40--42},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.abd2499},
  file = {/home/bram/Zotero/storage/BPJVR5Y4/Bovenberg et al. - 2020 - How to fix the GDPR's frustration of global biomed.pdf}
}

@misc{chambonAdaptingPretrainedVisionLanguage2022,
  title = {Adapting {{Pretrained Vision-Language Foundational Models}} to {{Medical Imaging Domains}}},
  author = {Chambon, Pierre and Bluethgen, Christian and Langlotz, Curtis P. and Chaudhari, Akshay},
  year = {2022},
  month = oct,
  number = {arXiv:2210.04133},
  eprint = {arXiv:2210.04133},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.04133},
  abstract = {Multi-modal foundation models are typically trained on millions of pairs of natural images and text captions, frequently obtained through web-crawling approaches. Although such models depict excellent generative capabilities, they do not typically generalize well to specific domains such as medical images that have fundamentally shifted distributions compared to natural images. Building generative models for medical images that faithfully depict clinical context may help alleviate the paucity of healthcare datasets. Thus, in this study, we seek to research and expand the representational capabilities of large pretrained foundation models to medical concepts, specifically for leveraging the Stable Diffusion model to generate domain specific images found in medical imaging. We explore the sub-components of the Stable Diffusion pipeline (the variational autoencoder, the U-Net and the text-encoder) to fine-tune the model to generate medical images. We benchmark the efficacy of these efforts using quantitative image quality metrics and qualitative radiologist-driven evaluations that accurately represent the clinical content of conditional text prompts. Our best-performing model improves upon the stable diffusion baseline and can be conditioned to insert a realistic-looking abnormality on a synthetic radiology image, while maintaining a 95\% accuracy on a classifier trained to detect the abnormality.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/bram/Zotero/storage/EAZMZCMW/Chambon et al. - 2022 - Adapting Pretrained Vision-Language Foundational M.pdf;/home/bram/Zotero/storage/Y77K7HJT/2210.html}
}

@misc{chambonRoentGenVisionLanguageFoundation2022,
  title = {{{RoentGen}}: {{Vision-Language Foundation Model}} for {{Chest X-ray Generation}}},
  shorttitle = {{{RoentGen}}},
  author = {Chambon, Pierre and Bluethgen, Christian and Delbrouck, Jean-Benoit and {Van der Sluijs}, Rogier and Po{\l}acin, Ma{\l}gorzata and Chaves, Juan Manuel Zambrano and Abraham, Tanishq Mathew and Purohit, Shivanshu and Langlotz, Curtis P. and Chaudhari, Akshay},
  year = {2022},
  month = nov,
  number = {arXiv:2211.12737},
  eprint = {arXiv:2211.12737},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.12737},
  abstract = {Multimodal models trained on large natural image-text pair datasets have exhibited astounding abilities in generating high-quality images. Medical imaging data is fundamentally different to natural images, and the language used to succinctly capture relevant details in medical data uses a different, narrow but semantically rich, domain-specific vocabulary. Not surprisingly, multi-modal models trained on natural image-text pairs do not tend to generalize well to the medical domain. Developing generative imaging models faithfully representing medical concepts while providing compositional diversity could mitigate the existing paucity of high-quality, annotated medical imaging datasets. In this work, we develop a strategy to overcome the large natural-medical distributional shift by adapting a pre-trained latent diffusion model on a corpus of publicly available chest x-rays (CXR) and their corresponding radiology (text) reports. We investigate the model's ability to generate high-fidelity, diverse synthetic CXR conditioned on text prompts. We assess the model outputs quantitatively using image quality metrics, and evaluate image quality and text-image alignment by human domain experts. We present evidence that the resulting model (RoentGen) is able to create visually convincing, diverse synthetic CXR images, and that the output can be controlled to a new extent by using free-form text prompts including radiology-specific language. Fine-tuning this model on a fixed training set and using it as a data augmentation method, we measure a 5\% improvement of a classifier trained jointly on synthetic and real images, and a 3\% improvement when trained on a larger but purely synthetic training set. Finally, we observe that this fine-tuning distills in-domain knowledge in the text-encoder and can improve its representation capabilities of certain diseases like pneumothorax by 25\%.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/bram/Zotero/storage/UMBEUX4V/Chambon et al. - 2022 - RoentGen Vision-Language Foundation Model for Che.pdf;/home/bram/Zotero/storage/GIK4E8PT/2211.html}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = {2009},
  month = jun,
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500\textendash 1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  keywords = {Explosions,Image databases,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine},
  file = {/home/bram/Zotero/storage/7ZCT84NY/Deng et al. - 2009 - ImageNet A large-scale hierarchical image databas.pdf;/home/bram/Zotero/storage/N2JH2Q9Q/5206848.html}
}

@misc{dhariwalDiffusionModelsBeat2021,
  title = {Diffusion {{Models Beat GANs}} on {{Image Synthesis}}},
  author = {Dhariwal, Prafulla and Nichol, Alex},
  year = {2021},
  month = jun,
  number = {arXiv:2105.05233},
  eprint = {arXiv:2105.05233},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.05233},
  abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\$\textbackslash times\$128, 4.59 on ImageNet 256\$\textbackslash times\$256, and 7.72 on ImageNet 512\$\textbackslash times\$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\$\textbackslash times\$256 and 3.85 on ImageNet 512\$\textbackslash times\$512. We release our code at https://github.com/openai/guided-diffusion},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/bram/Zotero/storage/IH4T7JC5/Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf;/home/bram/Zotero/storage/X5B3ULKL/2105.html}
}

@article{ehteshamibejnordiDiagnosticAssessmentDeep2017,
  title = {Diagnostic {{Assessment}} of {{Deep Learning Algorithms}} for {{Detection}} of {{Lymph Node Metastases}} in {{Women With Breast Cancer}}},
  author = {Ehteshami Bejnordi, Babak and Veta, Mitko and {Johannes van Diest}, Paul and {van Ginneken}, Bram and Karssemeijer, Nico and Litjens, Geert and {van der Laak}, Jeroen A. W. M. and {and the CAMELYON16 Consortium}},
  year = {2017},
  month = dec,
  journal = {JAMA},
  volume = {318},
  number = {22},
  pages = {2199--2210},
  issn = {0098-7484},
  doi = {10.1001/jama.2017.14585},
  abstract = {Application of deep learning algorithms to whole-slide pathology images can potentially improve diagnostic accuracy and efficiency.Assess the performance of automated deep learning algorithms at detecting metastases in hematoxylin and eosin\textendash stained tissue sections of lymph nodes of women with breast cancer and compare it with pathologists' diagnoses in a diagnostic setting.Researcher challenge competition (CAMELYON16) to develop automated solutions for detecting lymph node metastases (November 2015-November 2016). A training data set of whole-slide images from 2 centers in the Netherlands with (n\,=\,110) and without (n\,=\,160) nodal metastases verified by immunohistochemical staining were provided to challenge participants to build algorithms. Algorithm performance was evaluated in an independent test set of 129 whole-slide images (49 with and 80 without metastases). The same test set of corresponding glass slides was also evaluated by a panel of 11 pathologists with time constraint (WTC) from the Netherlands to ascertain likelihood of nodal metastases for each slide in a flexible 2-hour session, simulating routine pathology workflow, and by 1 pathologist without time constraint (WOTC).Deep learning algorithms submitted as part of a challenge competition or pathologist interpretation.The presence of specific metastatic foci and the absence vs presence of lymph node metastasis in a slide or image using receiver operating characteristic curve analysis. The 11 pathologists participating in the simulation exercise rated their diagnostic confidence as definitely normal, probably normal, equivocal, probably tumor, or definitely tumor.The area under the receiver operating characteristic curve (AUC) for the algorithms ranged from 0.556 to 0.994. The top-performing algorithm achieved a lesion-level, true-positive fraction comparable with that of the pathologist WOTC (72.4\% [95\% CI, 64.3\%-80.4\%]) at a mean of 0.0125 false-positives per normal whole-slide image. For the whole-slide image classification task, the best algorithm (AUC, 0.994 [95\% CI, 0.983-0.999]) performed significantly better than the pathologists WTC in a diagnostic simulation (mean AUC, 0.810 [range, 0.738-0.884]; P\,\&lt;\,.001). The top 5 algorithms had a mean AUC that was comparable with the pathologist interpreting the slides in the absence of time constraints (mean AUC, 0.960 [range, 0.923-0.994] for the top 5 algorithms vs 0.966 [95\% CI, 0.927-0.998] for the pathologist WOTC).In the setting of a challenge competition, some deep learning algorithms achieved better diagnostic performance than a panel of 11 pathologists participating in a simulation exercise designed to mimic routine pathology workflow; algorithm performance was comparable with an expert pathologist interpreting whole-slide images without time constraints. Whether this approach has clinical utility will require evaluation in a clinical setting.},
  file = {/home/bram/Zotero/storage/35B4UXJQ/Ehteshami Bejnordi et al. - 2017 - Diagnostic Assessment of Deep Learning Algorithms .pdf;/home/bram/Zotero/storage/5789FAET/2665774.html}
}

@misc{galImageWorthOne2022,
  title = {An {{Image}} Is {{Worth One Word}}: {{Personalizing Text-to-Image Generation}} Using {{Textual Inversion}}},
  shorttitle = {An {{Image}} Is {{Worth One Word}}},
  author = {Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit H. and Chechik, Gal and {Cohen-Or}, Daniel},
  year = {2022},
  month = aug,
  number = {arXiv:2208.01618},
  eprint = {arXiv:2208.01618},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.01618},
  abstract = {Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new "words" in the embedding space of a frozen text-to-image model. These "words" can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks. Our code, data and new words will be available at: https://textual-inversion.github.io},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {/home/bram/Zotero/storage/TDRVA33V/Gal et al. - 2022 - An Image is Worth One Word Personalizing Text-to-.pdf;/home/bram/Zotero/storage/6Q4K95WS/2208.html}
}

@misc{HaveBeenTrained,
  title = {Have {{I Been Trained}}?},
  howpublished = {https://haveibeentrained.com/},
  file = {/home/bram/Zotero/storage/AI5JUB85/haveibeentrained.com.html}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\textemdash 8\texttimes{} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {/home/bram/Zotero/storage/X42NEIBY/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf}
}

@misc{hoClassifierFreeDiffusionGuidance2022,
  title = {Classifier-{{Free Diffusion Guidance}}},
  author = {Ho, Jonathan and Salimans, Tim},
  year = {2022},
  month = jul,
  number = {arXiv:2207.12598},
  eprint = {arXiv:2207.12598},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.12598},
  abstract = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/bram/Zotero/storage/Y6DHGXSY/Ho and Salimans - 2022 - Classifier-Free Diffusion Guidance.pdf;/home/bram/Zotero/storage/MAUI8X2Q/2207.html}
}

@inproceedings{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  volume = {33},
  pages = {6840--6851},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
  file = {/home/bram/Zotero/storage/7THQ7MRX/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}

@misc{huLoRALowRankAdaptation2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and {Allen-Zhu}, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year = {2021},
  month = oct,
  number = {arXiv:2106.09685},
  eprint = {arXiv:2106.09685},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.09685},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/bram/Zotero/storage/A5ZI9SP9/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf;/home/bram/Zotero/storage/XJNA57AB/2106.html}
}

@misc{irvinCheXpertLargeChest2019,
  title = {{{CheXpert}}: {{A Large Chest Radiograph Dataset}} with {{Uncertainty Labels}} and {{Expert Comparison}}},
  shorttitle = {{{CheXpert}}},
  author = {Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and {Ciurea-Ilcus}, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and Seekins, Jayne and Mong, David A. and Halabi, Safwan S. and Sandberg, Jesse K. and Jones, Ricky and Larson, David B. and Langlotz, Curtis P. and Patel, Bhavik N. and Lungren, Matthew P. and Ng, Andrew Y.},
  year = {2019},
  month = jan,
  number = {arXiv:1901.07031},
  eprint = {arXiv:1901.07031},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1901.07031},
  abstract = {Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models. The dataset is freely available at https://stanfordmlgroup.github.io/competitions/chexpert .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/bram/Zotero/storage/U48DV8WB/Irvin et al. - 2019 - CheXpert A Large Chest Radiograph Dataset with Un.pdf;/home/bram/Zotero/storage/V8VVJWZM/1901.html}
}

@article{johnsonMIMICCXRDeidentifiedPublicly2019,
  title = {{{MIMIC-CXR}}, a de-Identified Publicly Available Database of Chest Radiographs with Free-Text Reports},
  author = {Johnson, Alistair E. W. and Pollard, Tom J. and Berkowitz, Seth J. and Greenbaum, Nathaniel R. and Lungren, Matthew P. and Deng, Chih-ying and Mark, Roger G. and Horng, Steven},
  year = {2019},
  month = dec,
  journal = {Scientific Data},
  volume = {6},
  number = {1},
  pages = {317},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/s41597-019-0322-0},
  abstract = {Chest radiography is an extremely powerful imaging modality, allowing for a detailed inspection of a patient's chest, but requires specialized training for proper interpretation. With the advent of high performance general purpose computer vision algorithms, the accurate automated analysis of chest radiographs is becoming increasingly of interest to researchers. Here we describe MIMIC-CXR, a large dataset of 227,835 imaging studies for 65,379 patients presenting to the Beth Israel Deaconess Medical Center Emergency Department between 2011\textendash 2016. Each imaging study can contain one or more images, usually a frontal view and a lateral view. A total of 377,110 images are available in the dataset. Studies are made available with a semi-structured free-text radiology report that describes the radiological findings of the images, written by a practicing radiologist contemporaneously during routine clinical care. All images and reports have been de-identified to protect patient privacy. The dataset is made freely available to facilitate and encourage a wide range of research in computer vision, natural language processing, and clinical data mining.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {Radiography,Translational research},
  file = {/home/bram/Zotero/storage/7U2UKQ94/Johnson et al. - 2019 - MIMIC-CXR, a de-identified publicly available data.pdf}
}

@misc{JournalMedicalInternet,
  title = {Journal of {{Medical Internet Research}} - {{Revolutionizing Medical Data Sharing Using Advanced Privacy-Enhancing Technologies}}: {{Technical}}, {{Legal}}, and {{Ethical Synthesis}}},
  howpublished = {https://www.jmir.org/2021/2/e25120/},
  file = {/home/bram/Zotero/storage/B5ZLK5DN/e25120.html}
}

@misc{karrasElucidatingDesignSpace2022,
  title = {Elucidating the {{Design Space}} of {{Diffusion-Based Generative Models}}},
  author = {Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
  year = {2022},
  month = oct,
  number = {arXiv:2206.00364},
  eprint = {arXiv:2206.00364},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.00364},
  abstract = {We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/bram/Zotero/storage/JJIH5394/Karras et al. - 2022 - Elucidating the Design Space of Diffusion-Based Ge.pdf;/home/bram/Zotero/storage/IZZWFBZT/2206.html}
}

@misc{kazerouniDiffusionModelsMedical2022,
  title = {Diffusion {{Models}} for {{Medical Image Analysis}}: {{A Comprehensive Survey}}},
  shorttitle = {Diffusion {{Models}} for {{Medical Image Analysis}}},
  author = {Kazerouni, Amirhossein and Aghdam, Ehsan Khodapanah and Heidari, Moein and Azad, Reza and Fayyaz, Mohsen and Hacihaliloglu, Ilker and Merhof, Dorit},
  year = {2022},
  month = nov,
  number = {arXiv:2211.07804},
  eprint = {arXiv:2211.07804},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.07804},
  abstract = {Denoising diffusion models, a class of generative models, have garnered immense interest lately in various deep-learning problems. A diffusion probabilistic model defines a forward diffusion stage where the input data is gradually perturbed over several steps by adding Gaussian noise and then learns to reverse the diffusion process to retrieve the desired noise-free data from noisy data samples. Diffusion models are widely appreciated for their strong mode coverage and quality of the generated samples despite their known computational burdens. Capitalizing on the advances in computer vision, the field of medical imaging has also observed a growing interest in diffusion models. To help the researcher navigate this profusion, this survey intends to provide a comprehensive overview of diffusion models in the discipline of medical image analysis. Specifically, we introduce the solid theoretical foundation and fundamental concepts behind diffusion models and the three generic diffusion modelling frameworks: diffusion probabilistic models, noise-conditioned score networks, and stochastic differential equations. Then, we provide a systematic taxonomy of diffusion models in the medical domain and propose a multi-perspective categorization based on their application, imaging modality, organ of interest, and algorithms. To this end, we cover extensive applications of diffusion models in the medical domain. Furthermore, we emphasize the practical use case of some selected approaches, and then we discuss the limitations of the diffusion models in the medical domain and propose several directions to fulfill the demands of this field. Finally, we gather the overviewed studies with their available open-source implementations at https://github.com/amirhossein-kz/Awesome-Diffusion-Models-in-Medical-Imaging.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/bram/Zotero/storage/NEU8FDIR/Kazerouni et al. - 2022 - Diffusion Models for Medical Image Analysis A Com.pdf;/home/bram/Zotero/storage/WRW2U5DH/2211.html}
}

@misc{khaderMedicalDiffusionDenoising2023,
  title = {Medical {{Diffusion}}: {{Denoising Diffusion Probabilistic Models}} for {{3D Medical Image Generation}}},
  shorttitle = {Medical {{Diffusion}}},
  author = {Khader, Firas and {Mueller-Franzes}, Gustav and Arasteh, Soroosh Tayebi and Han, Tianyu and Haarburger, Christoph and {Schulze-Hagen}, Maximilian and Schad, Philipp and Engelhardt, Sandy and Baessler, Bettina and Foersch, Sebastian and Stegmaier, Johannes and Kuhl, Christiane and Nebelung, Sven and Kather, Jakob Nikolas and Truhn, Daniel},
  year = {2023},
  month = jan,
  number = {arXiv:2211.03364},
  eprint = {arXiv:2211.03364},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.03364},
  abstract = {Recent advances in computer vision have shown promising results in image generation. Diffusion probabilistic models in particular have generated realistic images from textual input, as demonstrated by DALL-E 2, Imagen and Stable Diffusion. However, their use in medicine, where image data typically comprises three-dimensional volumes, has not been systematically evaluated. Synthetic images may play a crucial role in privacy preserving artificial intelligence and can also be used to augment small datasets. Here we show that diffusion probabilistic models can synthesize high quality medical imaging data, which we show for Magnetic Resonance Images (MRI) and Computed Tomography (CT) images. We provide quantitative measurements of their performance through a reader study with two medical experts who rated the quality of the synthesized images in three categories: Realistic image appearance, anatomical correctness and consistency between slices. Furthermore, we demonstrate that synthetic images can be used in a self-supervised pre-training and improve the performance of breast segmentation models when data is scarce (dice score 0.91 vs. 0.95 without vs. with synthetic data). The code is publicly available on GitHub: https://github.com/FirasGit/medicaldiffusion.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/bram/Zotero/storage/8V9XDF2E/Khader et al. - 2023 - Medical Diffusion Denoising Diffusion Probabilist.pdf;/home/bram/Zotero/storage/3ZYNXVFZ/2211.html}
}

@misc{kimDiffusionDeformableModel2022,
  title = {Diffusion {{Deformable Model}} for {{4D Temporal Medical Image Generation}}},
  author = {Kim, Boah and Ye, Jong Chul},
  year = {2022},
  month = jun,
  number = {arXiv:2206.13295},
  eprint = {arXiv:2206.13295},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.13295},
  abstract = {Temporal volume images with 3D+t (4D) information are often used in medical imaging to statistically analyze temporal dynamics or capture disease progression. Although deep-learning-based generative models for natural images have been extensively studied, approaches for temporal medical image generation such as 4D cardiac volume data are limited. In this work, we present a novel deep learning model that generates intermediate temporal volumes between source and target volumes. Specifically, we propose a diffusion deformable model (DDM) by adapting the denoising diffusion probabilistic model that has recently been widely investigated for realistic image generation. Our proposed DDM is composed of the diffusion and the deformation modules so that DDM can learn spatial deformation information between the source and target volumes and provide a latent code for generating intermediate frames along a geodesic path. Once our model is trained, the latent code estimated from the diffusion module is simply interpolated and fed into the deformation module, which enables DDM to generate temporal frames along the continuous trajectory while preserving the topology of the source image. We demonstrate the proposed method with the 4D cardiac MR image generation between the diastolic and systolic phases for each subject. Compared to the existing deformation methods, our DDM achieves high performance on temporal volume generation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/bram/Zotero/storage/2MGDGLRV/Kim and Ye - 2022 - Diffusion Deformable Model for 4D Temporal Medical.pdf;/home/bram/Zotero/storage/86BI2UGZ/2206.html}
}

@misc{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {arXiv:1412.6980},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1412.6980},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/bram/Zotero/storage/ZZA4DDGH/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/bram/Zotero/storage/CDSYM93S/1412.html}
}

@misc{LearningImplicitBrain,
  title = {Learning Implicit Brain {{MRI}} Manifolds with Deep Learning},
  howpublished = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10574/105741L/Learning-implicit-brain-MRI-manifolds-with-deep-learning/10.1117/12.2293515.short?SSO=1},
  file = {/home/bram/Zotero/storage/SNT553S9/12.2293515.html}
}

@misc{liuCompositionalVisualGeneration2023,
  title = {Compositional {{Visual Generation}} with {{Composable Diffusion Models}}},
  author = {Liu, Nan and Li, Shuang and Du, Yilun and Torralba, Antonio and Tenenbaum, Joshua B.},
  year = {2023},
  month = jan,
  number = {arXiv:2206.01714},
  eprint = {arXiv:2206.01714},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.01714},
  abstract = {Large text-guided diffusion models, such as DALLE-2, are able to generate stunning photorealistic images given natural language descriptions. While such models are highly flexible, they struggle to understand the composition of certain concepts, such as confusing the attributes of different objects or relations between objects. In this paper, we propose an alternative structured approach for compositional generation using diffusion models. An image is generated by composing a set of diffusion models, with each of them modeling a certain component of the image. To do this, we interpret diffusion models as energy-based models in which the data distributions defined by the energy functions may be explicitly combined. The proposed method can generate scenes at test time that are substantially more complex than those seen in training, composing sentence descriptions, object relations, human facial attributes, and even generalizing to new combinations that are rarely seen in the real world. We further illustrate how our approach may be used to compose pre-trained text-guided diffusion models and generate photorealistic images containing all the details described in the input descriptions, including the binding of certain object attributes that have been shown difficult for DALLE-2. These results point to the effectiveness of the proposed method in promoting structured generalization for visual generation. Project page: https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/bram/Zotero/storage/AVK8PZUW/Liu et al. - 2023 - Compositional Visual Generation with Composable Di.pdf;/home/bram/Zotero/storage/BYP5X34Q/2206.html}
}

@misc{muller-franzesDiffusionProbabilisticModels2022,
  title = {Diffusion {{Probabilistic Models}} Beat {{GANs}} on {{Medical Images}}},
  author = {{M{\"u}ller-Franzes}, Gustav and Niehues, Jan Moritz and Khader, Firas and Arasteh, Soroosh Tayebi and Haarburger, Christoph and Kuhl, Christiane and Wang, Tianci and Han, Tianyu and Nebelung, Sven and Kather, Jakob Nikolas and Truhn, Daniel},
  year = {2022},
  month = dec,
  number = {arXiv:2212.07501},
  eprint = {arXiv:2212.07501},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.07501},
  abstract = {The success of Deep Learning applications critically depends on the quality and scale of the underlying training data. Generative adversarial networks (GANs) can generate arbitrary large datasets, but diversity and fidelity are limited, which has recently been addressed by denoising diffusion probabilistic models (DDPMs) whose superiority has been demonstrated on natural images. In this study, we propose Medfusion, a conditional latent DDPM for medical images. We compare our DDPM-based model against GAN-based models, which constitute the current state-of-the-art in the medical domain. Medfusion was trained and compared with (i) StyleGan-3 on n=101,442 images from the AIROGS challenge dataset to generate fundoscopies with and without glaucoma, (ii) ProGAN on n=191,027 from the CheXpert dataset to generate radiographs with and without cardiomegaly and (iii) wGAN on n=19,557 images from the CRCMS dataset to generate histopathological images with and without microsatellite stability. In the AIROGS, CRMCS, and CheXpert datasets, Medfusion achieved lower (=better) FID than the GANs (11.63 versus 20.43, 30.03 versus 49.26, and 17.28 versus 84.31). Also, fidelity (precision) and diversity (recall) were higher (=better) for Medfusion in all three datasets. Our study shows that DDPM are a superior alternative to GANs for image synthesis in the medical domain.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/bram/Zotero/storage/UPQ27U7L/MÃ¼ller-Franzes et al. - 2022 - Diffusion Probabilistic Models beat GANs on Medica.pdf;/home/bram/Zotero/storage/2VS4I7E8/2212.html}
}

@misc{packhauserGenerationAnonymousChest2022,
  title = {Generation of {{Anonymous Chest Radiographs Using Latent Diffusion Models}} for {{Training Thoracic Abnormality Classification Systems}}},
  author = {Packh{\"a}user, Kai and Folle, Lukas and Thamm, Florian and Maier, Andreas},
  year = {2022},
  month = nov,
  number = {arXiv:2211.01323},
  eprint = {arXiv:2211.01323},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.01323},
  abstract = {The availability of large-scale chest X-ray datasets is a requirement for developing well-performing deep learning-based algorithms in thoracic abnormality detection and classification. However, biometric identifiers in chest radiographs hinder the public sharing of such data for research purposes due to the risk of patient re-identification. To counteract this issue, synthetic data generation offers a solution for anonymizing medical images. This work employs a latent diffusion model to synthesize an anonymous chest X-ray dataset of high-quality class-conditional images. We propose a privacy-enhancing sampling strategy to ensure the non-transference of biometric information during the image generation process. The quality of the generated images and the feasibility of serving as exclusive training data are evaluated on a thoracic abnormality classification task. Compared to a real classifier, we achieve competitive results with a performance gap of only 3.5\% in the area under the receiver operating characteristic curve.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/bram/Zotero/storage/ZWCTYUR3/PackhÃ¤user et al. - 2022 - Generation of Anonymous Chest Radiographs Using La.pdf;/home/bram/Zotero/storage/VMKVELB3/2211.html}
}

@misc{pinayaBrainImagingGeneration2022a,
  title = {Brain {{Imaging Generation}} with {{Latent Diffusion Models}}},
  author = {Pinaya, Walter H. L. and Tudosiu, Petru-Daniel and Dafflon, Jessica and {da Costa}, Pedro F. and Fernandez, Virginia and Nachev, Parashkev and Ourselin, Sebastien and Cardoso, M. Jorge},
  year = {2022},
  month = sep,
  number = {arXiv:2209.07162},
  eprint = {arXiv:2209.07162},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.07162},
  abstract = {Deep neural networks have brought remarkable breakthroughs in medical image analysis. However, due to their data-hungry nature, the modest dataset sizes in medical imaging projects might be hindering their full potential. Generating synthetic data provides a promising alternative, allowing to complement training datasets and conducting medical image research at a larger scale. Diffusion models recently have caught the attention of the computer vision community by producing photorealistic synthetic images. In this study, we explore using Latent Diffusion Models to generate synthetic images from high-resolution 3D brain images. We used T1w MRI images from the UK Biobank dataset (N=31,740) to train our models to learn about the probabilistic distribution of brain images, conditioned on covariables, such as age, sex, and brain structure volumes. We found that our models created realistic data, and we could use the conditioning variables to control the data generation effectively. Besides that, we created a synthetic dataset with 100,000 brain images and made it openly available to the scientific community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing,Quantitative Biology - Quantitative Methods},
  file = {/home/bram/Zotero/storage/VQZ5USLX/Pinaya et al. - 2022 - Brain Imaging Generation with Latent Diffusion Mod.pdf;/home/bram/Zotero/storage/N2LELEMU/2209.html}
}

@misc{rameshHierarchicalTextConditionalImage2022,
  title = {Hierarchical {{Text-Conditional Image Generation}} with {{CLIP Latents}}},
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  year = {2022},
  month = apr,
  number = {arXiv:2204.06125},
  eprint = {arXiv:2204.06125},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.06125},
  abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/bram/Zotero/storage/EBB8R876/Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf;/home/bram/Zotero/storage/HSL3BN6C/2204.html}
}

@misc{rameshZeroShotTexttoImageGeneration2021,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2102.12092},
  eprint = {arXiv:2102.12092},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.12092},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/bram/Zotero/storage/24PX9V33/Ramesh et al. - 2021 - Zero-Shot Text-to-Image Generation.pdf;/home/bram/Zotero/storage/SGQSJL63/2102.html}
}

@misc{roichPivotalTuningLatentbased2021,
  title = {Pivotal {{Tuning}} for {{Latent-based Editing}} of {{Real Images}}},
  author = {Roich, Daniel and Mokady, Ron and Bermano, Amit H. and {Cohen-Or}, Daniel},
  year = {2021},
  month = jun,
  number = {arXiv:2106.05744},
  eprint = {arXiv:2106.05744},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.05744},
  abstract = {Recently, a surge of advanced facial editing techniques have been proposed that leverage the generative power of a pre-trained StyleGAN. To successfully edit an image this way, one must first project (or invert) the image into the pre-trained generator's domain. As it turns out, however, StyleGAN's latent space induces an inherent tradeoff between distortion and editability, i.e. between maintaining the original appearance and convincingly altering some of its attributes. Practically, this means it is still challenging to apply ID-preserving facial latent-space editing to faces which are out of the generator's domain. In this paper, we present an approach to bridge this gap. Our technique slightly alters the generator, so that an out-of-domain image is faithfully mapped into an in-domain latent code. The key idea is pivotal tuning - a brief training process that preserves the editing quality of an in-domain latent region, while changing its portrayed identity and appearance. In Pivotal Tuning Inversion (PTI), an initial inverted latent code serves as a pivot, around which the generator is fined-tuned. At the same time, a regularization term keeps nearby identities intact, to locally contain the effect. This surgical training process ends up altering appearance features that represent mostly identity, without affecting editing capabilities. We validate our technique through inversion and editing metrics, and show preferable scores to state-of-the-art methods. We further qualitatively demonstrate our technique by applying advanced edits (such as pose, age, or expression) to numerous images of well-known and recognizable identities. Finally, we demonstrate resilience to harder cases, including heavy make-up, elaborate hairstyles and/or headwear, which otherwise could not have been successfully inverted and edited by state-of-the-art methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/bram/Zotero/storage/KA32327E/Roich et al. - 2021 - Pivotal Tuning for Latent-based Editing of Real Im.pdf;/home/bram/Zotero/storage/94IVM9BT/2106.html}
}

@misc{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2022},
  month = apr,
  number = {arXiv:2112.10752},
  eprint = {arXiv:2112.10752},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.10752},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/bram/Zotero/storage/LGQH9FYN/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf;/home/bram/Zotero/storage/8BHL7WNP/2112.html}
}

@misc{ruizDreamBoothFineTuning2022,
  title = {{{DreamBooth}}: {{Fine Tuning Text-to-Image Diffusion Models}} for {{Subject-Driven Generation}}},
  shorttitle = {{{DreamBooth}}},
  author = {Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  year = {2022},
  month = aug,
  number = {arXiv:2208.12242},
  eprint = {arXiv:2208.12242},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.12242},
  abstract = {Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for "personalization" of text-to-image diffusion models (specializing them to users' needs). Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model (Imagen, although our method is not limited to a specific model) such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can then be used to synthesize fully-novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views, and lighting conditions that do not appear in the reference images. We apply our technique to several previously-unassailable tasks, including subject recontextualization, text-guided view synthesis, appearance modification, and artistic rendering (all while preserving the subject's key features). Project page: https://dreambooth.github.io/},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {/home/bram/Zotero/storage/DMWFQAHL/Ruiz et al. - 2022 - DreamBooth Fine Tuning Text-to-Image Diffusion Mo.pdf;/home/bram/Zotero/storage/QAF5IQ74/2208.html}
}

@misc{sagersImprovingDermatologyClassifiers2022,
  title = {Improving Dermatology Classifiers across Populations Using Images Generated by Large Diffusion Models},
  author = {Sagers, Luke W. and Diao, James A. and Groh, Matthew and Rajpurkar, Pranav and Adamson, Adewole S. and Manrai, Arjun K.},
  year = {2022},
  month = nov,
  number = {arXiv:2211.13352},
  eprint = {arXiv:2211.13352},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.13352},
  abstract = {Dermatological classification algorithms developed without sufficiently diverse training data may generalize poorly across populations. While intentional data collection and annotation offer the best means for improving representation, new computational approaches for generating training data may also aid in mitigating the effects of sampling bias. In this paper, we show that DALL\$\textbackslash cdot\$E 2, a large-scale text-to-image diffusion model, can produce photorealistic images of skin disease across skin types. Using the Fitzpatrick 17k dataset as a benchmark, we demonstrate that augmenting training data with DALL\$\textbackslash cdot\$E 2-generated synthetic images improves classification of skin disease overall and especially for underrepresented groups.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/bram/Zotero/storage/2VCSTSZ2/Sagers et al. - 2022 - Improving dermatology classifiers across populatio.pdf;/home/bram/Zotero/storage/BML9PFA9/2211.html}
}

@techreport{sahaArtificialIntelligenceRadiologists2022,
  title = {Artificial {{Intelligence}} and {{Radiologists}} at {{Prostate Cancer Detection}} in {{MRI}}: {{The PI-CAI Challenge}} ({{Study Protocol}})},
  shorttitle = {Artificial {{Intelligence}} and {{Radiologists}} at {{Prostate Cancer Detection}} in {{MRI}}},
  author = {Saha, Anindo and Twilt, Jasper Jonathan and Bosma, Joeran Sander and {van Ginneken}, Bram and Yakar, Derya and Elschot, Mattijs and Veltman, Jeroen and F{\"u}tterer, Jurgen and {de Rooij}, Maarten and Huisman, Henkjan},
  year = {2022},
  month = jun,
  institution = {{Zenodo}},
  doi = {10.5281/zenodo.6667655},
  abstract = {This document represents the preregistration of the PI-CAI challenge study design, in compliance with MICCAI-BIAS reporting guidelines. The PI-CAI challenge is an all-new grand challenge that aims to validate the diagnostic performance of artificial intelligence and radiologists at clinically significant prostate cancer (csPCa) detection/diagnosis in MRI, with histopathology and follow-up ({$\geq$} 3 years) as the reference standard, in a retrospective setting. The study hypothesizes that state-of-the-art AI algorithms, trained using thousands of patient exams, are non-inferior to radiologists reading bpMRI. Key aspects of the PI-CAI study design have been established in conjunction with an international scientific advisory board of 16 experts in prostate AI, radiology and urology \textemdash to unify and standardize present-day guidelines, and to ensure meaningful validation of prostate AI towards clinical translation (Reinke et al., 2021).},
  langid = {english},
  keywords = {artificial intelligence,computer-aided detection and diagnosis,magnetic resonance imaging,prostate cancer,radiologists},
  file = {/home/bram/Zotero/storage/J2V5XR5Z/Saha et al. - 2022 - Artificial Intelligence and Radiologists at Prosta.pdf}
}

@misc{sahariaPhotorealisticTexttoImageDiffusion2022,
  title = {Photorealistic {{Text-to-Image Diffusion Models}} with {{Deep Language Understanding}}},
  author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J. and Norouzi, Mohammad},
  year = {2022},
  month = may,
  number = {arXiv:2205.11487},
  eprint = {arXiv:2205.11487},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.11487},
  abstract = {We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/bram/Zotero/storage/SYMX8Y6S/Saharia et al. - 2022 - Photorealistic Text-to-Image Diffusion Models with.pdf;/home/bram/Zotero/storage/C83XQN7A/2205.html}
}

@article{scheibnerRevolutionizingMedicalData2021,
  title = {Revolutionizing {{Medical Data Sharing Using Advanced Privacy-Enhancing Technologies}}: {{Technical}}, {{Legal}}, and {{Ethical Synthesis}}},
  shorttitle = {Revolutionizing {{Medical Data Sharing Using Advanced Privacy-Enhancing Technologies}}},
  author = {Scheibner, James and Raisaro, Jean Louis and {Troncoso-Pastoriza}, Juan Ram{\'o}n and Ienca, Marcello and Fellay, Jacques and Vayena, Effy and Hubaux, Jean-Pierre},
  year = {2021},
  month = feb,
  journal = {Journal of Medical Internet Research},
  volume = {23},
  number = {2},
  pages = {e25120},
  publisher = {{JMIR Publications Inc., Toronto, Canada}},
  doi = {10.2196/25120},
  abstract = {Multisite medical data sharing is critical in modern clinical practice and medical research. The challenge is to conduct data sharing that preserves individual privacy and data utility. The shortcomings of traditional privacy-enhancing technologies mean that institutions rely upon bespoke data sharing contracts. The lengthy process and administration induced by these contracts increases the inefficiency of data sharing and may disincentivize important clinical treatment and medical research.  This paper provides a synthesis between 2 novel advanced privacy-enhancing technologies\textemdash homomorphic encryption and secure multiparty computation (defined together as multiparty homomorphic encryption). These privacy-enhancing technologies provide a mathematical guarantee of privacy, with multiparty homomorphic encryption providing a performance advantage over separately using homomorphic encryption or secure multiparty computation. We argue multiparty homomorphic encryption fulfills legal requirements for medical data sharing under the European Union's General Data Protection Regulation which has set a global benchmark for data protection. Specifically, the data processed and shared using multiparty homomorphic encryption can be considered anonymized data. We explain how multiparty homomorphic encryption can reduce the reliance upon customized contractual measures between institutions. The proposed approach can accelerate the pace of medical research while offering additional incentives for health care and research institutes to employ common data interoperability standards.},
  langid = {english},
  file = {/home/bram/Zotero/storage/4KILB6VL/Scheibner et al. - 2021 - Revolutionizing Medical Data Sharing Using Advance.pdf;/home/bram/Zotero/storage/R7KW6LYK/e25120.html}
}

@misc{schuhmannLAION5BOpenLargescale2022,
  title = {{{LAION-5B}}: {{An}} Open Large-Scale Dataset for Training next Generation Image-Text Models},
  shorttitle = {{{LAION-5B}}},
  author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and Schramowski, Patrick and Kundurthy, Srivatsa and Crowson, Katherine and Schmidt, Ludwig and Kaczmarczyk, Robert and Jitsev, Jenia},
  year = {2022},
  month = oct,
  number = {arXiv:2210.08402},
  eprint = {arXiv:2210.08402},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.08402},
  abstract = {Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the training and capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B - a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection. Announcement page https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/bram/Zotero/storage/HWVG6TLQ/Schuhmann et al. - 2022 - LAION-5B An open large-scale dataset for training.pdf;/home/bram/Zotero/storage/HRFDY2JT/2210.html}
}

@misc{shiConversionMayoLDCT2023,
  title = {Conversion of the {{Mayo LDCT Data}} to {{Synthetic Equivalent}} through the {{Diffusion Model}} for {{Training Denoising Networks}} with a {{Theoretically Perfect Privacy}}},
  author = {Shi, Yongyi and Wang, Ge},
  year = {2023},
  month = jan,
  number = {arXiv:2301.06604},
  eprint = {arXiv:2301.06604},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.06604},
  abstract = {Deep learning techniques are widely used in the medical imaging field; for example, low-dose CT denoising. However, all these methods usually require a large number of data samples, which are at risk of privacy leaking, expensive, and time-consuming. Because privacy and other concerns create challenges to data sharing, publicly available CT datasets are up to only a few thousand cases. Generating synthetic data provides a promising alternative to complement or replace training datasets without patient-specific information. Recently, diffusion models have gained popularity in the computer vision community with a solid theoretical foundation. In this paper, we employ latent diffusion models to generate synthetic images from a publicly available CT dataset-the Mayo Low-dose CT Challenge dataset. Then, an equivalent synthetic dataset was created. Furthermore, we use both the original Mayo CT dataset and the synthetic dataset to train the RED-CNN model respectively. The results show that the RED-CNN model achieved similar performance in the two cases, which suggests the feasibility of using synthetic data to conduct the low-dose CT research. Additionally, we use the latent diffusion model to augment the Mayo dataset. The results on the augmented dataset demonstrate an improved denoising performance.},
  archiveprefix = {arxiv},
  keywords = {Physics - Medical Physics},
  file = {/home/bram/Zotero/storage/KNIDSHDH/Shi and Wang - 2023 - Conversion of the Mayo LDCT Data to Synthetic Equi.pdf;/home/bram/Zotero/storage/ANV5DMLC/2301.html}
}

@misc{skandaraniGANsMedicalImage2021,
  title = {{{GANs}} for {{Medical Image Synthesis}}: {{An Empirical Study}}},
  shorttitle = {{{GANs}} for {{Medical Image Synthesis}}},
  author = {Skandarani, Youssef and Jodoin, Pierre-Marc and Lalande, Alain},
  year = {2021},
  month = jul,
  number = {arXiv:2105.05318},
  eprint = {arXiv:2105.05318},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.05318},
  abstract = {Generative Adversarial Networks (GANs) have become increasingly powerful, generating mind-blowing photorealistic images that mimic the content of datasets they were trained to replicate. One recurrent theme in medical imaging is whether GANs can also be effective at generating workable medical data as they are for generating realistic RGB images. In this paper, we perform a multi-GAN and multi-application study to gauge the benefits of GANs in medical imaging. We tested various GAN architectures from basic DCGAN to more sophisticated style-based GANs on three medical imaging modalities and organs namely : cardiac cine-MRI, liver CT and RGB retina images. GANs were trained on well-known and widely utilized datasets from which their FID score were computed to measure the visual acuity of their generated images. We further tested their usefulness by measuring the segmentation accuracy of a U-Net trained on these generated images. Results reveal that GANs are far from being equal as some are ill-suited for medical imaging applications while others are much better off. The top-performing GANs are capable of generating realistic-looking medical images by FID standards that can fool trained experts in a visual Turing test and comply to some metrics. However, segmentation results suggests that no GAN is capable of reproducing the full richness of a medical datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/bram/Zotero/storage/K7VRQYHK/Skandarani et al. - 2021 - GANs for Medical Image Synthesis An Empirical Stu.pdf;/home/bram/Zotero/storage/XFF9MQRS/2105.html}
}

@misc{szegedyRethinkingInceptionArchitecture2015,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
  year = {2015},
  month = dec,
  number = {arXiv:1512.00567},
  eprint = {arXiv:1512.00567},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1512.00567},
  abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error on the validation set (3.6\% error on the test set) and 17.3\% top-1 error on the validation set.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/bram/Zotero/storage/3EST4ZID/Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer.pdf;/home/bram/Zotero/storage/KAXF6YIK/1512.html}
}

@misc{UsingLoRAEfficient,
  title = {Using {{LoRA}} for {{Efficient Stable Diffusion Fine-Tuning}}},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/blog/lora},
  file = {/home/bram/Zotero/storage/N8NQYTH9/lora.html}
}

@misc{veelingRotationEquivariantCNNs2018,
  title = {Rotation {{Equivariant CNNs}} for {{Digital Pathology}}},
  author = {Veeling, Bastiaan S. and Linmans, Jasper and Winkens, Jim and Cohen, Taco and Welling, Max},
  year = {2018},
  month = jun,
  number = {arXiv:1806.03962},
  eprint = {arXiv:1806.03962},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1806.03962},
  abstract = {We propose a new model for digital pathology segmentation, based on the observation that histopathology images are inherently symmetric under rotation and reflection. Utilizing recent findings on rotation equivariant CNNs, the proposed model leverages these symmetries in a principled manner. We present a visual analysis showing improved stability on predictions, and demonstrate that exploiting rotation equivariance significantly improves tumor detection performance on a challenging lymph node metastases dataset. We further present a novel derived dataset to enable principled comparison of machine learning models, in combination with an initial benchmark. Through this dataset, the task of histopathology diagnosis becomes accessible as a challenging benchmark for fundamental machine learning research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/bram/Zotero/storage/AXS69NDP/Veeling et al. - 2018 - Rotation Equivariant CNNs for Digital Pathology.pdf;/home/bram/Zotero/storage/8CMGX4L4/1806.html}
}

@misc{vincentGettyImagesSuing2023,
  title = {Getty {{Images}} Is Suing the Creators of {{AI}} Art Tool {{Stable Diffusion}} for Scraping Its Content},
  author = {Vincent, James},
  year = {2023},
  month = jan,
  journal = {The Verge},
  abstract = {Getty Images claims Stability AI `unlawfully' scraped millions of images from its site. It's a significant escalation in the developing legal battles between generative AI firms and content creators.},
  howpublished = {https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit},
  langid = {american},
  file = {/home/bram/Zotero/storage/NFL6DP5D/ai-art-copyright-stable-diffusion-getty-images-lawsuit.html}
}

@misc{vonplatenDiffusersStateoftheartDiffusion2023,
  title = {Diffusers: {{State-of-the-art}} Diffusion Models},
  shorttitle = {Diffusers},
  author = {{von Platen}, Patrick and Patil, Suraj and Lozhkov, Anton and Cuenca, Pedro and Lambert, Nathan and Rasul, Kashif and Davaadorj, Mishig and Wolf, Thomas},
  year = {2023},
  month = mar,
  abstract = {ð¤ Diffusers: State-of-the-art diffusion models for image and audio generation in PyTorch},
  copyright = {Apache-2.0}
}

@article{yiGenerativeAdversarialNetwork2019,
  title = {Generative Adversarial Network in Medical Imaging: {{A}} Review},
  shorttitle = {Generative Adversarial Network in Medical Imaging},
  author = {Yi, Xin and Walia, Ekta and Babyn, Paul},
  year = {2019},
  month = dec,
  journal = {Medical Image Analysis},
  volume = {58},
  pages = {101552},
  issn = {1361-8415},
  doi = {10.1016/j.media.2019.101552},
  abstract = {Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross-modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique.},
  langid = {english},
  keywords = {Deep learning,Generative adversarial network,Generative model,Medical imaging,Review},
  file = {/home/bram/Zotero/storage/4E8V5PGT/Yi et al. - 2019 - Generative adversarial network in medical imaging.pdf;/home/bram/Zotero/storage/HNEERX8Z/S1361841518308430.html}
}
