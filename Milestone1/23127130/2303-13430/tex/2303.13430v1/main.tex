\documentclass[10pt,twocolumn,letterpaper]{article}

\makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{todonotes}
\usepackage{caption}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{12562} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Medical diffusion on a budget: textual inversion for medical image generation}

\author{Bram de Wilde* \and Anindo Saha \and Richard P. G. ten Broek \and Henkjan Huisman\\
Radboud University Medical Center\\
{\tt\small *contact@bramdewilde.com}
}



%\maketitle

\twocolumn[{
\maketitle
\begin{center}
    \captionsetup{type=figure}
    \includegraphics[width=0.9\linewidth]{figures/general_overview.pdf}
    \captionof{figure}{The textual inversion fine-tuning process for diffusion models trains a text conditioning embedding for a new token (e.g. Prostate MRI) using a small set of example images, while keeping the rest of the architecture frozen. In this work we show that this allows adaption of latent diffusion models to a variety of medical imaging modalities, using only 100 examples and a single consumer-grade GPU.}
    \label{fig:overview}
\end{center}
}]
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

% \begin{figure*}[h!]
% \begin{center}
%    \includegraphics[width=0.8\linewidth]{figures/general_overview.pdf}
% \end{center}
%    \caption{Visual examples illustrating the effect of varying inference and training settings for T2-weighted prostate MRI, all generated using the same random seed. Columns with a bold title indicate optimal values. Row labels indicate the parameter that changes along the column, with bold values set for the other parameters. For example: in the top row the number of steps changes, but CFG scale, embedding size and training cases are 2, 64 and 100, respectively.}
% \label{fig:overview}
% \end{figure*}

%%%%%%%%% ABSTRACT
\begin{abstract}
Diffusion-based models for text-to-image generation have gained immense popularity due to recent advancements in efficiency, accessibility, and quality. Although it is becoming increasingly feasible to perform inference with these systems using consumer-grade GPUs, training them from scratch still requires access to large datasets containing captioned data and significant computational resources. In the case of medical image generation, the availability of large, publicly accessible datasets that include text reports is limited due to legal and ethical concerns. While training a diffusion model on a private dataset may address this issue, it is not always feasible for institutions lacking the necessary computational resources. This work demonstrates that pre-trained Stable Diffusion models, originally trained on natural images, can be adapted to various medical imaging modalities by training text embeddings with textual inversion.
In this study, we conducted experiments using small medical datasets comprising only 100 samples from three medical modalities. The embeddings were trained in a matter of hours, while still retaining diagnostic relevance in image generation. Our experiments were designed to achieve several objectives. Firstly, we aimed to fine-tune the training and inference processes of textual inversion, which revealed that larger embeddings and more examples are required for the medical domain.  Secondly, we validated our approach by demonstrating a 2\% increase in the diagnostic accuracy (AUC) for detecting prostate cancer on MRI, which is a challenging multi-modal imaging modality, from 0.78 to 0.80. Thirdly, we performed simulations by interpolating between healthy and diseased states, combining multiple pathologies, and inpainting to show the flexibility of the trained embeddings and to demonstrate potential for fine control of disease appearance. Finally, the embeddings trained in this study are small (less than 1 MB), which facilitates easy sharing of medical data with reduced privacy concerns. The code and all embeddings trained in this work will be made available online for future research.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Image generation has increasingly captured the attention
of many researchers, spurring an impressive progression in text-to-image
generation.
In particular, diffusion models have gained enormous popularity
through their ability to generate high-quality and diverse images, conditioned
on a text prompt.\cite{hoDenoisingDiffusionProbabilistic2020, dhariwalDiffusionModelsBeat2021, rameshZeroShotTexttoImageGeneration2021, rameshHierarchicalTextConditionalImage2022, sahariaPhotorealisticTexttoImageDiffusion2022}
Of all text-to-image model implementations, Stable Diffusion has by far generated
the biggest impact in terms of users, owing to the fact that it is both
released under a permissive license and operable using a single GPU.\cite{rombachHighResolutionImageSynthesis2022}
The unprecedented availability and performance of Stable Diffusion is perhaps
best demonstrated by artists ringing the alarm bell,
for fear of AI systems replicating their work or style without their consent, 
seemingly even leading to a lawsuit against the Stable Diffusion team.\cite{vincentGettyImagesSuing2023}
Whatever your stance in this debate, it shows that humans are starting to
have difficulty distinguishing AI-generated art from human art.

Generating art or even photorealistic images, however, has some room for error.
Even if generated images are not completely physically realistic, they may
still be appealing or impressive.
The medical imaging field, on the other hand, places a higher bar
on generation quality.\cite{yiGenerativeAdversarialNetwork2019,skandaraniGANsMedicalImage2021}
Images need not only be anatomically correct, but diagnostically as well.
In theory, there is no reason why a system like Stable Diffusion for
medical imaging is impossible.
You simply need a large, varied and ideally public dataset of images
with captions.\cite{schuhmannLAION5BOpenLargescale2022}
In practice, however, ethical and legal issues often get in the way of
sharing medical data.\cite{scheibnerRevolutionizingMedicalData2021,bovenbergHowFixGDPR2020}
This issue is especially difficult to tackle for radiology reports due to their 
unstructured nature, whereas they
are the natural source for high quality captions.
For one of the few public datasets of this caliber that exists, MIMIC-CXR,
Chambon et al. have indeed demonstrated that it is possible to train a
latent diffusion model capable of generating chest x-ray images with
high fidelity and diversity through free text prompts.\cite{johnsonMIMICCXRDeidentifiedPublicly2019, chambonRoentGenVisionLanguageFoundation2022}
They trained the system using up to 170,000 images on 64 A100 GPUs.

On top of data sharing issues, some modalities and pathologies are
inherently scarce: certain types of scans can be expensive or experimental and
some diseases are rare or tied to specific demographics.
For these reasons, especially in the medical domain, it is essential to
have computationally feasible methods which can fine-tune existing models
towards a smaller set of a specific modality or disease.
In this paper, we pick one such method, Textual Inversion, and rigorously
explore its capacities for adapting Stable Diffusion to medical imaging.\cite{galImageWorthOne2022}
All experiments are done using a single RTX2070 GPU and all training scripts
and embeddings are shared online.
We summarize our contributions as follows:

\begin{enumerate}
    \item We show, through careful tuning and experiments, that a diffusion
model trained on natural images can be adapted to produce a wide variety
of realistic medical images.
    \item We demonstrate the practical value of our approach, by improving cancer
classification models in the low-data regime, using synthetic data.
    \item We demonstrate that the trained embeddings are highly flexible, by showing (1) interpolation between healthy and diseased state, (2) inpainting for fine control of disease appearance and (3) that multiple embeddings can be combined to generate images with multiple pathologies.
\end{enumerate}

\section{Related work}

Since our study explores a specific fine-tuning method for diffusion models applied to medical image generation, we briefly review popular fine-tuning methods for diffusion in general and cover studies applying diffusion models in the medical domain.

\subsection{Fine-tuning diffusion models}

Various methods have different computational requirements and output files of different sizes. We review three popular methods in decreasing order of compute and output size.

In \cite{ruizDreamBoothFineTuning2022} Ruiz et al. fine-tune the denoising U-net component of a diffusion model, using a handful of images for introducing a new concept.
This method is typically employed on 24 GB GPUs and results in checkpoints of several GB when fine-tuning Stable Diffusion models.

LoRA was introduced as a method to fine-tune large language models, which freezes original model weights and introduces rank decomposition matrices into the Transformer architecture.\cite{huLoRALowRankAdaptation2021}
Recently, this fine-tuning method was incorporated into the populare \textit{diffusers} library. 
It can be deployed on a 11 GB GPU and results in shareable files of under 5 MB for a fully fine-tuned latent diffusion model.\cite{vonplatenDiffusersStateoftheartDiffusion2023, UsingLoRAEfficient}

In \cite{galImageWorthOne2022}, Gal et al. introduce textual inversion, which finetunes a diffusion model by finding a new word embedding for newly introduced concepts, resulting in extremely small files of under 1 MB. Like LoRA, this method is deployable on a 11 GB GPU.
In this study, we adopt textual inversion as fine-tuning method, because it has low computational requirements and the smallest file output, but we have no reason to expect our results to be limited to this fine-tuning method.

\subsection{Medical image generation}

Several papers have applied diffusion to medical imaging, with a wide range of applications including anomly detection, segmentation, registration and modality transfer with image-to-image translation.\cite{kazerouniDiffusionModelsMedical2022}
Specifically for medical image generation, several recent works have trained diffusion models for image generation.
Pre-trained models are often trained on 2D RGB datasets, but many medical imaging modalities are 3D.
Recently, studies such as \cite{khaderMedicalDiffusionDenoising2023} and \cite{pinayaBrainImagingGeneration2022a} have trained diffusion models from scratch on 3D data, or even on 4D data.\cite{kimDiffusionDeformableModel2022}
Several other works studied text-to-image latent diffusion models for medical imaging.\cite{chambonRoentGenVisionLanguageFoundation2022, akroutDiffusionbasedDataAugmentation2023}

Closest to our work is \cite{chambonAdaptingPretrainedVisionLanguage2022}, where Chambon et al. explore various methods to adapt a pre-trained Stable Diffusion model to chest X-ray generation.
They performed experiments with both textual inversion and fine-tuning the U-net component of Stable Diffusion, similar to \cite{ruizDreamBoothFineTuning2022}.
They find that textual inversion works, but that fine-tuning the U-net is more effective, especially with more complex prompts.
They fine-tune using 5 examples per class.

Our work extends on this by exploring textual inversion more deeply, by training with more examples and bigger embeddings.
Additionally, we demonstrate the flexibility of the approach through example applications and by adapting to multiple and more complex modalities beyond chest X-ray.
In contrast to most other studies, we intentionally do not train from scratch and use small datasets to explore the feasibility of diffusion in low-data and low-compute environments.

\section{Methods}



\subsection{Image generation}

All images are generated with Stable Diffusion v2.0, using an interactive open-source web interface.\cite{rombachHighResolutionImageSynthesis2022, automatic1111StableDiffusionWeb2023}
Images are sampled using the ancestral Euler scheduler.\cite{karrasElucidatingDesignSpace2022}
The main inference parameters which influence image generation quality are 
(1) the number of steps for the sampling scheduler and 
(2) the classifier-free guidance (CFG) scale.\cite{hoClassifierFreeDiffusionGuidance2022}
Using more steps for sampling typically leads to better image quality, but increases the inference time.
The CFG scale can be used to set the trade-off between sample quality and sample diversity.
Practically, a high CFG scale means images will follow the text prompt more closely at the expense of diversity.
Conversely, a low CFG scale results in images that deviate more from the prompt and consequently have lower fidelity, but higher diversity.

To introduce a medical modality as a new concept to a pre-trained diffusion model, we use the textual inversion process.\cite{galImageWorthOne2022}
Put simply, this process finds a vector in the text embedding space that optimally represents the concept.
Practically, this is done by freezing the entire architecture apart from this embedding vector and doing backpropagation with a similarity loss, as illustrated in Figure \ref{fig:overview}.
We train embeddings with a constant learning rate of $0.005$ for 50,000 steps with a batch size of 1.
In the work of Gal et al., during training prompts are generated from a list of templates, for instance: "\textit{a photo of a $<$embedding$>$}" or "\textit{a rendering of a $<$embedding$>$}".
Since these templates do not necessarily make sense in a medical imaging context, we simplify this by always prompting the model only with the embedding name during training.
We experiment with the amount of images used to train an embedding and the vector size of the embedding.
To evaluate the impact of inference and training parameters on generation quality, we compute the Fr\'echet Inception Distance using 100 generated samples compared to 100 real examples for each parameter setting.\cite{szegedyRethinkingInceptionArchitecture2015}

To investigate the usability of the trained embeddings, we also experiment with combining multiple trained embeddings using composable diffusion.\cite{liuCompositionalVisualGeneration2023}
This method allows prompting with a combination of embeddings using an AND operator in the prompt, 
e.g. "\textit{$<$cardiomegaly$>$ AND $<$pleural effusion$>$}" to generate an image with both cardiomegaly and pleural effusion present.
Additionally, this methods allows a weight to be given to each embedding, to tune the strength of each embedding separately.
In this study, we use this to experiment with interpolating between healthy and diseased states and to generate images with multiple diseases present.

\subsection{Classification}

For classification experiments, we train ResNet-18 models, pre-trained on ImageNet.\cite{heDeepResidualLearning2016, dengImageNetLargescaleHierarchical2009}
Models are trained with a fixed learning rate of $10^{-4}$ with the Adam optimizer for 6250 batches of 32 images.\cite{kingmaAdamMethodStochastic2017}
This corresponds to 100 epochs for the biggest synthesized dataset (2000 synthesized cases).
AUC is evaluated on a separate validation set during training and performance of the best validation checkpoint 
on a separate test set is reported.
We apply random horizontal flipping, gaussian noise, intensity transformations, channel dropout, translation, 
scaling and rotation as data augmentation.
Detailed training scripts will be shared online.

\subsection{Datasets}

To showcase the wide applicability of textual inversion for adaptation to medical imaging, 
we demonstrate results on three different types of data: multi-modal MRI, chest X-ray and histopathology.

\subsubsection{Multi-modal MRI - PI-CAI}

The main dataset used in this work is a recently released public dataset of 1500 prostate MRI cases.
This dataset was released as part of the PI-CAI (Prostate Imaging: Cancer AI) challenge, where the task 
is to detect clinically significant prostate cancer.\cite{sahaArtificialIntelligenceRadiologists2022}
Each case is a 3D MRI scan featuring three modalities: T2-weighted imaging (T2W), apparent diffusion coefficient maps (ADC) and diffusion-weighted imaging (DWI).
Since this work adapts a pre-trained 2D diffusion model, we extract one 2D axial slice per case.
Each case is first resampled to a resolution of $3 \times 0.5 \times 0.5$ mm and then center-cropped to
a $90 \times 150 \times 150$ mm ($30 \times 300 \times 300$ px) region.
For negative cases, we select the median prostate slice using provided full prostate segmentations.
For positive cases, we select the slice with maximum tumor area, according to the provided tumor segmentation maps.
Each slice is finally upsampled to $512 \times 512$ px.
Each modality is encoded as one of the RGB channels when training multi-modal embeddings.
The training, validation and test set or the classification experiments each consist of 100 randomly sampled negative slices and 100 randomly sampled positive slices.
The embeddings are trained on the training set.

Prostate MRI is special in that it provides multiple images of the pelvic region that each depict a unique pathophysiologically relevant aspect for the specific disease purpose. This multi-modal dataset deviates from the natural image distribution seen during pre-training, and thus makes it highly challenging for image generation.
To be relevant, the generated medical images need to be diagnostically consistent across the modalities. For example, prostate cancer should appear dark on ADC, bright on DWI, and show a blurry structure in T2W. 

\subsubsection{Chest X-ray - CheXpert}

CheXpert is a large public dataset of 224,316 chest radiographs, with corresponding labels for 14 different observations.\cite{irvinCheXpertLargeChest2019}
In principle, this is a multi-label classification task, but since we explicitly investigate compositional prompting
with the learned embeddings, we only sample images with a single class.
Specifically, we sample 100 AP-view radiographs to train embeddings for the following four observations: 
No Finding (healthy), Cardiomegaly, Pleural Effusion and Pneumonia.
Each radiograph is first cropped to non-zero borders. Then the longest edge is resized to $512$ px, while
keeping the aspect ratio fixed.
Finally, the image is zero-padded to a square resolution of $512 \times 512$ px.

\subsubsection{Histopathology - Patch Camelyon}

Patch Camelyon is a public dataset of 327,680 $96 \times 96$ px patches extracted from histopathology 
whole-slide images of lymph node sections, originally released as part of the Camelyon16 challenge.\cite{veelingRotationEquivariantCNNs2018, ehteshamibejnordiDiagnosticAssessmentDeep2017}
Each patch has a corresponding binary label indicating the presence of metastatic tissue. We randomly selected
100 negative and 100 positive patches, upsampled them to $512 \times 512$ px and used them to train embeddings.

\section{Experiments}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.8\linewidth]{figures/ti_settings_overview.pdf}
\end{center}
   \caption{Visual examples illustrating the effect of varying inference and training settings for T2-weighted prostate MRI, all generated using the same random seed. Columns with a bold title indicate optimal values. Row labels indicate the parameter that changes along the column, with bold values set for the other parameters. For example: in the top row the number of steps changes, but CFG scale, embedding size and training cases are 2, 64 and 100, respectively.}
\label{fig:ti_settings}
\end{figure*}

\subsection{Adapting TI settings to medical imaging}\label{sec: ti_settings}

We start by tuning the settings of the Textual Inversion training process,
using the PI-CAI prostate cancer dataset.
All embeddings in this section were trained using 2D T2-weighted
healthy prostate slices, meaning they feature only one modality of the
three modalities present for each 2D slice.
The T2-weighted image clearly shows the anatomy and is therefore easiest
to judge qualitatively.

Firstly, we determine the optimal inference
parameters for an embedding with vector size 64, trained with 100 cases.
In particular, we tune the number of sampling steps and the classifier-free
guidance (CFG) scale. The full results are
shown in Table \ref{tab:inference_settings}. Following this table, in the remainder of the
paper we do inference with 100 steps and a CFG scale of 2, unless specified otherwise.
Even though a CFG scale of 1 gives slightly lower FID, images are much less
accurate, as will be discussed later in this section.

Secondly, we study the impact of the vector size of the trained embeddings,
varying it from 8 to 64, again using 100 cases during training.
The limit of the CLIP encoder used for Stable Difffusion is 75, so a bigger
embedding size would need adaptation of the framework.
The results are shown in Table \ref{tab:n_vectors} and show that using
a larger embedding size is better.

Thirdly, we study the impact of the number of cases used during training,
using an embedding size of 64. We vary the number of cases from 5, as
proposed in \cite{galImageWorthOne2022}, to 100. The results are shown in
Table \ref{tab:n_cases} and show that using more cases leads to better
embeddings.

% Inference parameter table
\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
Steps & CFG scale & FID $\downarrow$\\
\hline\hline
25 & 2 & 194 \\
50 & 2 & 184 \\
75 & 2 & 176 \\
100 & 2 & \textbf{171} \\
\hline
\hline
100 & 1 & \textbf{168} \\
100 & 2 & 171 \\
100 & 3 & 202 \\
100 & 4 & 211  \\
100 & 5 & 222  \\
\hline
\end{tabular}
\end{center}
\caption{FID score for embeddings generated with varying number of sampling steps and CFG scale.}
\label{tab:inference_settings}
\end{table}

% Embedding size table
\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Embedding size & FID $\downarrow$\\
\hline\hline
8 & 177\\
16 & 184\\
32 & 218 \\
64 & \textbf{171} \\
\hline
\end{tabular}
\end{center}
\caption{FID score for embeddings trained with varying size (vectors per token).}
\label{tab:n_vectors}
\end{table}

% Number of cases table
\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Training cases & FID $\downarrow$\\
\hline\hline
5 & 203\\
10 & 181\\
50 & 181\\
100 & \textbf{171}\\
\hline
\end{tabular}
\end{center}
\caption{FID score for embeddings trained with varying number of training cases.}
\label{tab:n_cases}
\end{table}

Figure \ref{fig:ti_settings} shows the effect of the parameters studied in 
this section visually on a single random seed. A high number of sampling steps
improves generation quality, with the generations for 25 and 50 steps showing
incorrect anatomy for the bladder.
Although a CFG scale of 1 results in the lowest FID score in Table \ref{tab:inference_settings},
visually the results are much worse, featuring inaccurate general anatomy.
A high CFG scale (e.g. 5 in the Figure) also leads to bad results, showcased here by
simplified structure inside the prostate and a curious fractured pelvic bone.
The difference between CFG scale 2 and 3 is not that large, but upon manual inspection we find
that a CFG scale of 2 gives better generations overall, as seems to be confirmed by the lower FID
score in Table \ref{tab:inference_settings}.
The embedding size is clearly optimally chosen to be large, with sizes 8 and 16 showing
inaccurate generation, particularly of the bladder.
Although size 32 looks better, the structure of the prostate itself is not nearly as
good as generated by the size 64 embedding.
Finally, the impact of the amount of training cases seems to trump all other settings,
where 5 and 10 cases produce very unrealistic images. The embedding trained
with 100 cases generates images with the most realistic prostate structure.
In general we found that the FID score, although it identifies general trends, is
not the most suitable metric to judge generation quality. We chose optimal parameters
by inspecting generation results visually as well.

Following the results presented in this section, all embeddings in the
remainder of this paper were trained with a size of 64 vectors per token,
using 100 cases per class. For inference, we sample with 100 steps
and a CFG scale of 2, unless specified otherwise.

\subsection{Improving classification with synthetic data}

In this section, we experiment with using synthetic data to train prostate cancer classification models on multi-modal prostate MRI.
Embeddings are trained on two sets of 100 cases, with only negative or only positive cases.
With these embeddings, we generate up to 1000 cases for each class and use various combinations of real and synthetic data to train classification models.
Results are shown in Table \ref{tab:auc_binary}, with the primary result being that augmenting the 200-case training set with 2000 synthesized cases leads to a 2\% improvement in AUC, from 0.78 to 0.80.
Note that these 2000 synthesized cases are based on embeddings trained with the same 200-case set used to train the classification models.
This shows that the generated cases actually add non-trivial variation to the data distribution and that the embedding does not simply reproduce training cases.
Furthermore, models trained with only synthetic cases do not see a large drop in performance, indicating that the synthetic cases are diagnostically accurate.
Finally, to confirm visual results from section \ref{sec: ti_settings}, classification models trained with synthetic cases generated with embeddings trained on 10 cases instead of 100 show a dramatic drop in performance.
This confirms our finding in section \ref{sec: ti_settings} that more cases are needed for textual inversion for medical data.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Real cases & Synthetic cases & AUC $\uparrow$ \\
\hline\hline
200 & 0 & 0.780 $\pm$ 0.017 \\
0 & 200 & 0.737 $\pm$ 0.019 \\
0 & 2000 & 0.766 $\pm$ 0.020 \\
200 & 200 & 0.773 $\pm$ 0.015 \\
200 & 2000 & \textbf{0.803 $\pm$ 0.009} \\
\hline
0 & 2000* & 0.562 $\pm$ 0.036\\
200 & 2000* & 0.745 $\pm$ 0.012 \\
\hline
\end{tabular}
\end{center}
\caption{AUC for binary prostate cancer classifiers, trained with real or synthetic data, or a combination of both. Synthetic cases marked with an asterisk (*) were generated with an embbeding trained on only 10 cases, instead of 100. The mean test AUC over 10 training runs is shown, together with the standard deviation.}
\label{tab:auc_binary}
\end{table}

\subsection{Composability of embeddings}\label{sec:composing}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.9\linewidth]{figures/disease_interpolation.pdf}
\end{center}
   \caption{Visual examples illustrating interpolation between healthy and diseased states for multi-modal Prostate MRI, various pathologies on Chest X-Ray and lymph node metastasis in histopathology. The column titles show the trade-off between healthy and diseased. The Chest X-Ray examples are all generated using the same random seed.  The prostate images are cropped to the prostate region for visibility.}
\label{fig:disease_interpolation}
\end{figure*}

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.8\linewidth]{figures/chexpert_composition.pdf}
\end{center}
   \caption{Visual example illustrating that multiple embeddings can be composed, to show multiple pathologies in a single image. From left to right, pleural effusion, pneumonia and cardiomegaly are progressively added to a healthy generated example.}
\label{fig:chexpert_composition}
\end{figure*}

One particularly attractive property of textual inversion is that you can use your newly introduced concept in text prompts, even by combining multiple trained embeddings. In this section we give visual evidence that this works, to an extent, for medical data as well.
Firstly, in Figure \ref{fig:disease_interpolation}, disease state is gradually increased from healthy to diseased, using composable diffusion.
For instance, the cardiomegaly radiograph in the second column (25\% diseased) is generated with a prompt like "\textit{0.25*$<$healthy$>$ AND 0.75*$<$cardiomegaly$>$}".
This seems to work well across the modalities studied in this paper: the tumors in the prostate example become gradually more prominent (darker on ADC, brighter on DWI); the heart in the cardiomegaly example appears to grow from left to right; the tissue in the lymph node metastasis example becomes gradually more abnormal.

Even combining multiple conditions in a single image appears to work, as illustrated by Figure \ref{fig:chexpert_composition}.
Here we start with a healthy image and consequently add pleural effusion, pneumonia and cardiomegaly to the prompt for a single random seed.
For the image with all three diseases, we gave each embedding a strength of 0.5 and found that it works better to increase the CFG scale to 3.

These examples show that the trained embeddings are flexible and go beyond fine-tuning towards a single new concept.
The fact that rendering of disease progression or accurate depiction of multiple conditions is possible, while the embeddings have only been trained on cases with a single condition present is a promising result.
Practically, this could be useful to generate cases with rare combinations of conditions in a single image, or to simulate disease progression for medical surveillance settings.

\subsection{Controlling disease appearance with inpainting}

\begin{figure}[t]
\begin{center}
   \includegraphics[width=\linewidth]{figures/chexpert_inpainting.pdf}
\end{center}
   \caption{Inpainting of pleural effusion and pneumonia on the same healthy generated Chest X-ray example.}
\label{fig:chexpert_inpainting}
\end{figure}

\begin{figure}[t]
\begin{center}
   \includegraphics[width=\linewidth]{figures/prostate_inpainting.pdf}
\end{center}
   \caption{Inpainting of prostate cancer in different locations on the same healthy generated Prostate MRI example. The top row shows the original healthy case, with the bottom rows showing inpainting in different locations with varying mask size.}
\label{fig:prostate_inpainting}
\end{figure}

In this section, we demonstrate the potential of inpainting to precisely control where disease shows in an image.
Starting with a generated healthy example, a portion of the image is masked.
The diffusion model then denoises the masked part of the image, while conditioned on a specific disease embedding.
For example, for the top row in Figure \ref{fig:chexpert_inpainting}, the bottom of the left lung of the healthy image is masked.
The diffusion model inpaints it, while it is conditioned with the pleural effusion embedding.
This results in pleural effusion appearing in the masked region.
In Figure \ref{fig:chexpert_inpainting}, it is used to force pleural effusion or pneumonia appears at the left lung.
Similarly, in Figure \ref{fig:prostate_inpainting}, the same healthy prostate example is masked in two different locations, with a different mask size.
When inpainting conditioned on the positive embedding, this generates tumors at those locations of corresponding sizes.
Similar to the examples in section \ref{sec:composing}, this allows engineering of examples with specific disease appearance and could for instance be useful to generate more cases with rare tumor locations.

\section{Conclusion}

In this paper, we show that pre-trained latent diffusion models can be adapted to a variety of modalities in the medical domain, using textual inversion.
High quality images can be generated with embeddings trained on 100 examples on a single consumer-grade GPU.
We showcased various possible applications: improvement of diagnostic models in the low-data regime by adding synthetic cases during training, simulation of disease progression and generation of images with specific disease appearance.
Although a dedicated diffusion model trained on a large captioned medical dataset would likely generate better images, our results are promising for institutions with limited computational resources.
Especially for situations where collecting a large dataset is not feasible, such as rare diseases, this approach is suitable and would also be compatible with a medically pre-trained model.
Finally, since the trained embeddings are extremely small files, they may facilitate sharing of medical information with reduced privacy concerns.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{iccv_references}
}

\end{document}
