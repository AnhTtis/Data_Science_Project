\section{Experimental Results}\label{sec:results}

We evaluate the effectiveness of \SystemName on seven datasets, comparing it to five state-of-the-art uncertainty modeling methods and two classical methods based on historical data methods.  We investigate the sensitivity of two hyperparameters: confidence level and the split between training and calibration data in \cref{subsec:Pre-processing}. Additionally, we conduct a detailed analysis to evaluate the impact of training data splitting ratio (\cref{sub:dsr}) and justify the calibration table construction method (\cref{sub:gvq}).

Highlights of our evaluation are: 
\begin{itemize}
\item \SystemName consistently delivers the best overall performance than the baseline uncertainty methods across base DNN architectures and datasets (\cref{sec:overall});
\item The adaptive calibration scheme introduced \SystemName gives a better trade-off between  the coverage and PI width for individual locations and sensor nodes over CQR (\cref{sec:api});
\item \SystemName gives more robust performance over CQR and DQR at different desired coverage rates (\cref{sub_different_confidence_level});
\item We showcase how \SystemName can be used to enhance the evaluation of traffic forecasting models (\cref{sec:petf}). 
\end{itemize}

\subsection{Overall Results\label{sec:overall}} 
\begin{table*}
\small
\centering
\caption {Performance Evaluation for Traffic Methods with Different Uncertainty Qualification Methods on Traffic Speed Datasets. \\($\mathrm{PICP}$ (\%) $\uparrow$ / $\mathrm{MPIW}$ $\downarrow$ )}
\begin{tabular}{lllllllccccccc}
\multicolumn{7}{l}{(a) METR-LA, Hist-D: $+$3.6\% / 34.2, Hist-W: $-$9.2\% / 28.2}\\ %, Mean / Std: 53.8 / 20.3}  \\
\toprule
Model Name& MC dropout & Bayesian & Conformal & DQR & CQR & \SystemName \\
\cmidrule(lr){1-7}
STGCN	&	$-$62.4\% /	3.0 	&	$-$44.9\% /	7.1 	&	$-$5.2\% /	13.7 	&	$+$0.5\% /	30.1 	&	$+$0.6\% /	20.6 	&	\textbf{$+$1.1\% /	20.5} 	\\
GWNet	&	$-$47.9\% /	3.5 	&	$-$38.9\% /	11.7 	&	$-$4.5\% /	11.9 	&	$-$0.3\% /	19.7 	&	$+$1.2\% /	15.6 	&	\textbf{$+$1.8\% /	15.1}	\\
MTGNN	&	$-$61.6\% /	2.4 	&	$-$63.0\% /	2.7 	&	$-$4.3\% /	11.8 	&	$-$0.1\% /	19.3 	&	$-$0.4\% /	14.7 	&	\textbf{$+$0.1\% /	14.6} 	\\
GMAN	&	$-$68.4\% /	1.7 	&	$-$11.9\% /	16.1 	&	$-$1.2\% /	18.1 	&	$+$0.9\% /	27.4 	&	$+$1.3\% /	20.4 	&	\textbf{$+$1.6\% /	20.1} 	\\
LSTNet	&	$-$54.7\% /	3.6 	&	$-$66.0\% /	3.0 	&	$-$2.7\% /	24.1 	&	$-$0.1\% /	29.0 	&	$+$0.3\% /	24.7 	&	\textbf{$+$0.4\% /	24.0} 	\\
\textbf{AVERAGE}	&	$-$59.00\% / 2.84 	&	$-$44.94\% /	8.12  	&	$-$3.58\% /	15.92  	&	$+$0.18	\% /	25.10 	&	$+$0.60\% /	19.20 	&	\textbf{$+$1.00\% /	18.86 } 	\\


\bottomrule

\multicolumn{7}{l}{} \\
\multicolumn{7}{l}{(b) PEMS-BAY, Hist-D: $-$13.8\% / 9.5, Hist-W: $-$32.7\% / 6.2}  \\ %, Mean / Std: 62.6 / 9.6} \\
\toprule
  Model Name          & MC dropout & Bayesian & Conformal & DQR & CQR & \SystemName \\
\cmidrule(lr){1-7}
STGCN	&	$-$58.8\% /	1.6 	&	$-$38.1\% /	3.7 	&	$-$3.3\% /	6.8 	&	$-$2.4\% /	9.8 	&	$-$0.1\% /	8.2 	&	\textbf{$+$0.0\% /	8.1} 	\\
GWNet	&	$-$47.3\% /	2.9 	&	$-$32.9\% /	3.9 	&	$-$1.8\% /	5.9 	&	$-$0.9\% /	7.8 	&	$+$0.1\% /	6.3 	&	\textbf{$+$0.6\% /	6.2} 	\\
MTGNN	&	$-$66.4\% /	1.2 	&	$-$67.5\% /	1.2 	&	$-$1.6\% /	5.8 	&	$-$0.6\% /	7.8 	&	$-$0.2\% /	6.2 	&	\textbf{$+$0.2\% /	6.1} 	\\
GMAN	&	$-$64.8\% /	1.4 	&	$-$3.3\% /	7.3 	&	$-$5.5\% /	5.7 	&	$+$0.5\% /	9.7 	&	$-$0.2\% /	7.6 	&	\textbf{$+$0.5\% /	7.6} 	\\
LSTNet	&	$-$31.9\% /	29.2 	&	$-$84.0\% /	0.3 	&	$-$0.6\% /	12.3 	&	$-$5.1\% /	10.5 	&	$-$1.5\% /	10.3 	&	\textbf{$-$0.9\% /	9.9} 	\\
\textbf{AVERAGE}	&	$-$53.84\% / 7.26  	&	$-$45.16\% /	3.28 	&	$-$2.56\% /	7.30	&	$-$1.70\% /	9.12 	&	$-$0.38\% /	7.72	&	\textbf{$+$0.08\% /	7.58} 	\\
\bottomrule

\multicolumn{7}{l}{} \\
\multicolumn{7}{l}{(c) PEMSD7M, Hist-D: $-$15.3\% / 10.7, Hist-W: $-$35.0\% / 7.9}  \\ %, Mean / Std: 58.9 / 13.5} \\

\toprule
  Model Name          & MC dropout & Bayesian & Conformal & DQR & CQR & \SystemName  \\
\cmidrule(lr){1-7}
STGCN	&	$-$56.0\% /	2.8 	&	$-$36.7\% /	5.8 	&	$-$7.1\% /	12.2 	&	$-$2.4\% /	17.0 	&	+0.4\% /	17.0 	&	\textbf{$+$1.1\% /	16.9} 	\\
GWNet	&	$-$40.1\% /	3.6 	&	$-$37.2\% /	6.6 	&	$-$4.6\% /	9.6 	&	$-$0.5\% /	14.4 	&	$-$0.2\% /	13.0 	&	\textbf{$+$0.1\% /	12.9} 	\\
MTGNN	&	$-$37.8\% /	4.6 	&	$-$53.2\% /	2.8 	&	$-$2.2\% /	11.6 	&	$-$1.0\% /	13.0 	&	$-$0.2\% /	13.0 	&	\textbf{$+$0.2\% /	12.9} 	\\
GMAN	&	$-$51.8\% /	2.7 	&	$-$2.1\% /	14.4 	&	$-$3.7\% /	12.2 	&	$-$0.7\% /	17.0 	&	$+$0.7\% /	15.4 	&	\textbf{$+$0.8\% /	15.1} 	\\
LSTNet	&	$-$33.0\% /	5.9 	&	$-$16.2\% /	9.6 	&	$-$3.1\% /	17.7 	&	$-$4.8\% /	16.7 	&	$-$0.4\% /	17.6 	& \textbf{$+$0.0\% /	17.6} 	\\
\textbf{AVERAGE}	&	$-$43.74\% / 3.92  	&	$-$29.08\% /	7.84  	&	$-$4.14\% /	12.66  	&	$+$1.88\% /	15.62 	&	$+$0.06\% /	15.20 	&	\textbf{$+$0.44\% /	15.08 } 	\\

\bottomrule
\label{table_res_speed}
\end{tabular}
\end{table*}

\begin{table*}
\small
\centering
\caption{Performance Evaluation for Traffic Methods with Different Uncertainty Qualification Methods on Traffic Flow Datasets\\ ($\mathrm{PICP}$ (\%) $\uparrow$ / $\mathrm{MPIW}$ $\downarrow$)}
\begin{tabular}{lllllllccccccc}

\multicolumn{7}{l}{(a) PEMS03, Hist-D: $-$27.4\% / 67.6, Hist-W: $-$56.9\% / 37.2 }  \\ %, Mean / Std: 179.3 / 143.7} \\
\toprule
  Model Name          & MC dropout & Bayesian & Conformal & DQR & CQR & \SystemName  \\
\cmidrule(lr){1-7}
STGCN	&	$-$54.9\% /	18.0 	&	$-$18.1\% /	57.9 	&	$-$3.8\% /	75.8 	&	$-$10.8\% /	61.8 	&	$-$2.1\% /	81.6 	&	\textbf{$-$0.5\% /	78.7} 	\\
GWNet	&	$-$36.0\% /	30.4 	&	$-$8.2\% /	79.6 	&	$-$1.5\% /	64.3 	&	$-$4.5\% /	55.3 	&	$+$1.5\% /	66.2 	&	\textbf{$+$1.2\% /	66.1} 	\\
MTGNN	&	$-$27.3\% /	40.9 	&	$-$49.9\% /	18.8 	&	$-$0.8\% /	65.7 	&	$-$2.0\% /	58.6 	&	$-$0.5\% /	60.6 	&	\textbf{$-$0.4\% /	61.6} 	\\
GMAN	&	$-$41.8\% /	29.8 	&	$+$5.7\% /	127.1 	&	$-$1.5\% /	64.3 	&	$-$8.5\% /	67.8 	&	$-$1.2\% /	85.5 	&	\textbf{$-$1.3\% /	85.9} 	\\
LSTNet	&	$-$31.9\% /	29.2 	&	$-$57.8\% /	40.4 	&	$-$2.3\% /	73.2 	&	$-$4.0\% /	63.3 	&	$+$0.4\% /	70.8 	&	\textbf{$+$0.4\% /	69.1} 	\\
\textbf{AVERAGE}	&	$-$38.38\% /	29.66  	&	$-$25.66\% /	64.76  	&	$-$1.98\% /	68.66 	&	$-$5.96\% /	61.36  	&	$-$0.38\% /	72.94  	&	\textbf{$-$0.12\% /	72.28 } 	\\

\bottomrule

\multicolumn{7}{l}{} \\
\multicolumn{7}{l}{(b) PEMS04, Hist-D: $-$21.8\% / 92.2, Hist-W: $-$58.2\% / 52.1}  \\ %, Mean / Std: 211.7 / 158.1} \\
\toprule
  Model Name          & MC dropout & Bayesian & Conformal & DQR & CQR & \SystemName  \\
\cmidrule(lr){1-7}
STGCN	&	$-$51.5\% /	22.6 	&	$-$19.4\% /	64.1 	&	$+$0.9\% /	108.7 	&	$-$4.1\% /	86.7 	&	$-$0.3\% /	93.5 	&	\textbf{$+$0.2\% /	94.7} 	\\
GWNet	&	$-$39.0\% /	37.5 	&	$-$3.1\% /	115.6 	&	$+$0.5\% /	92.5 	&	$-$1.3\% /	84.4 	&	$+$0.9\% /	85.5 	&	\textbf{$+$1.1\% /	85.3} 	\\
MTGNN	&	$-$28.1\% /	55.0 	&	$-$46.8\% /	25.6 	&	$+$0.5\% /	91.3 	&	$-$1.4\% /	81.2 	&	$+$0.5\% /	81.2 	&	\textbf{$+$0.8\% /	82.4} 	\\
GMAN	&	$-$48.6\% /	32.0 	&	$-$0.2\% /	116.4 	&	$+$2.0\% /	117.3 	&	$+$5.6\% /	132.7 	&	$+$0.2\% /	97.9 	&	\textbf{$+$0.9\% /	100.9} 	\\
LSTNet	&	$-$29.9\% /	39.4 	&	$-$52.6\% /	25.4 	&	$+$0.8\% /	102.8 	&	$-$0.3\% /	92.8 	&	$+$1.3\% /	93.1 	&	\textbf{$+$1.4\% /	92.3} 	\\
\textbf{AVERAGE}	&	$-$39.42\% /	37.30  	&	$-$24.42\% /	69.42  	&	$+$0.94\% /	102.52  	&	$-$0.30\% /	95.56  	&	$+$0.52\% /	90.24  	&	\textbf{$+$0.88\% /	91.12 } 	\\
\bottomrule

\multicolumn{7}{l}{} \\
\multicolumn{7}{l}{(c) PEMS07, Hist-D: $-$20.7\% / 110.2, Hist-W: $-$21.2\% / 59.6}  \\ %, Mean / Std: 308.5 / 188.2} \\
\toprule
  Model Name          & MC dropout & Bayesian & Conformal & DQR & CQR & \SystemName  \\
\cmidrule(lr){1-7}
STGCN	&	$-$51.9\% /	23.9 	&	$-$18.4\% /	88.4 	&	$-$1.3\% /	116.3 	&	$-$8.4\% /	94.9 	&	$-$0.5\% /	112.2 	&	\textbf{$+$0.0\% /	108.7} 	\\
GWNet	&	$-$80.6\% /	5.9 	&	$-$6.8\% /	117.9 	&	$-$1.3\% /	93.5 	&	$-$1.1\% /	92.9 	&	$+$0.4\% /	94.9 	&	\textbf{$+$0.4\% /	92.5} 	\\
MTGNN	&	$-$52.6\% /	24.8 	&	$-$51.8\% /	23.2 	&	$-$2.6\% /	88.3 	&	$-$1.7\% /	88.1 	&	$-$0.3\% /	90.7 	&	\textbf{$-$0.1\% /	89.1} 	\\
GMAN	&	$-$45.2\% /	35.9 	&	$+$0.7\% /	145.6 	&	$-$6.5\% /	137.1 	&	$+$4.5\% /	154.7 	&	$-$1.8\% /	117.4 	&	\textbf{$-$1.8\% /	116.8}	\\
LSTNet	&	$-$52.6\% /	24.8 	&	$-$53.1\% /	30.3 	&	$-$2.2\% /	107.8 	&	$-$1.4\% /	107.9 	&	$-$0.3\% /	109.2 	&	\textbf{$+$0.0\% /	107.1} 	\\
\textbf{AVERAGE}	&	$-$56.58\% / 23.06  	&	$-$25.88\% /	81.08  	&	$-$2.78\% /	108.60 	&	$-$1.62\% /	107.70 	&	$-$0.50\% /	104.88  	&	\textbf{$-$0.30\% /	102.84 } 	\\

\bottomrule

\multicolumn{7}{l}{} \\
\multicolumn{7}{l}{(d) PEMS08, Hist-D: $-$36.4\% / 80.2, Hist-W: $-$29.8\% / 41.9}  \\ %, Mean / Std: 230.7 / 146.2} \\
\toprule
  Model Name          & MC dropout & Bayesian & Conformal & DQR & CQR & \SystemName \\
\cmidrule(lr){1-7}
STGCN	&	$-$50.8\% /	18.8 	&	$-$15.8\% /	65.4 	&	$-$0.4\% /	90.6 	&	$-$5.9\% /	74.4 	&	$-$0.2\% /	89.0 	&	\textbf{$+$0.1\% /	83.4} 	\\
GWNet	&	$-$37.1\% /	36.3 	&	$-$1.3\% /	99.8 	&	$-$0.7\% /	70.6 	&	$+$0.6\% /	74.8 	&	$+$0.1\% /	75.1 	&	\textbf{$+$0.1\% /	72.0} 	\\
MTGNN	&	$-$26.9	\% /	47.4 	&	$-$35.3\% /	27.8 	&	$-$1.3\% /	69.1 	&	$-$1.2\% /	68.5 	&	$+$1.4\% /	73.1 	&	\textbf{$+$1.2\% /	71.1} 	\\
GMAN	&	$-$39.3	\% /	31.0 	&	$+$0.9\% /	104.7 	&	$-$2.1\% /	118.7 	&	$+$1.2\% /	92.6 	&	$+$3.0\% /	93.0 	&	\textbf{$+$3.4\% /	92.6} 	\\
LSTNet	&	$-$36.1	\% /	28.2 	&	$+$1.0\% /	145.0 	&	$+$0.5\% /	85.1 	&	$-$2.0\% /	77.3 	&	$+$0.7\% /	80.3 	&	\textbf{$+$0.8\% /	78.3} 	\\
\textbf{AVERAGE}	&	$-$38.04\% /	32.34  	&	$-$10.10\% /	88.54 	&	$-$0.80\% /	86.82  	&	$-$1.46\% /	77.52  	&	$+$1.00\% /	82.10  	&	\textbf{$+$1.12\% /	79.48 } 	\\

\bottomrule
\label{table_res_flow}
\end{tabular}
\end{table*}

In this experiment, we set an expectation of a 90\% coverage rate, and we evaluate if the quantify method can reach or even exceed the 90\% coverage rate across test samples using PICP.  Later in \cref{sub_different_confidence_level}, we compare the performance under different coverage rate expectations. 

Tables \ref{table_res_speed} and \ref{table_res_flow} present the results obtained when applying each evaluated uncertainty method to different DNN models and datasets. The best-performing results are highlighted in bold for each base DNN model. In the corresponding column of each uncertainty method, we first show the improvement of PICP (as a percentage with respect to a 90\% coverage rate), followed by the MPIW that measures the width of the PI (\emph{lower-is-better}). In this context, a positive PICP improvement means that the PI's coverage exceeds the expectation of a 90\% coverage rate, while a negative PCIP improvement ($-$) means that the PI's coverage falls below the expectation.  For example, in Table \ref{table_res_speed}(a), applying \SystemName to STGCN yields $+$1.1\% / 20.5. This should read as that \SystemName achieves a 91.1\% coverage rate (above the coverage expectation) with 20.5 in MPIW (i.e., the difference between the upper and the lower bound of the PI is 20.5). For each dataset, we report the average PCIP and MPIW across test samples. 

As expected, the MPIW given by all methods depends on the base DNN model's capability, because a larger MPIW (or a wider PI) is needed to ensure the coverage for a less accurate DNN model. However, we observe that \SystemName achieves or exceeds the desired 90\% coverage rate for most test cases. For a handful of cases where the \SystemName PICP falls below 90\%, the resulting coverage rate remains close to the target of 90\%, with a coverage rate of at least 88\% (i.e., $\geqslant -1.8\%$ in the tables). In addition to achieving a good coverage rate, \SystemName produces small MPIW, thus providing a meaningful uncertainty measure for decision-making. 


Compared to \SystemName, non-frequentist baselines such as MC dropout, Bayesian, and Conformal methods provide a narrow PI, resulting in a small MPIW. However, they struggle to meet the coverage expectation, where the true value of the test sample often falls outside the PI. This suggests that the PI given by non-frequentist baselines can mislead decision-making by being too optimistic or over-conservative for traffic forecasting. The performance of non-frequentist baselines is also not consistent, showing significant variance depending on the test datasets and DNN models. For instance, while Bayesian gives a coverage rate of 86.89\% on PEMS-BAY when using GMAN as the base DNN, its coverage rate is only 22.52\% on the same dataset when using MTGNN as the base model. 

By utilizing previously collected data, Hist-D gives good coverage but with a large MPIW, leading to over-conservative forecasts. In contrast, quantile-based methods such as DQR and CQR significantly outperform others by providing a better coverage guarantee. By incorporating an adaptive scheme to adjust the initial PI based on the test node and prediction window, \SystemName improves upon CQR and DQR with higher PCIP and smaller MPIW for all test cases. \SystemName also delivers consistent good performance across datasets and base DNN models, demonstrating the robustness of our approach. This further highlights the advantage of quantile-based methods over non-frequentist baselines, which exhibit inconsistent and varying performance depending on the test datasets and base DNN models. 

\subsection{Adaptive PI Adjustments\label{sec:api}}
\begin{figure}
  % correct this 
  \centering
  \includegraphics[width=0.95\linewidth]{fig/nodeqca_3.pdf}
  \caption{Violin diagrams show PICP and MPIW by applying CQR and \SystemName to GWNet model tested on the PEMS04.  The thick black line shows where 50\% of the data locates.}
  \label{fig:nodeqca}
\end{figure}
\SystemName advances CQR by adaptively adjusting the initial PI through a dedicated calibration table that uses different adjustments rather than a constant global value adopted by standard CQR. To demonstrate the benefit of \SystemName over CQR, we closely examine the test samples of the PEMS04 dataset when using GWNet  as the base DNN for traffic forecasting. The results are given in the violin diagram of \cref{fig:nodeqca}. In the diagram, the width of the violin indicates the density of test data points with a given PICP, and the center of the plot is a box plot with the median and quartiles the test samples fall into a specific PICP (or coverage rate). 

For this test scenario, the averaged PICP given by CQR and \SystemName is comparable at 90.9\% and 91.1\%, respectively. However, the effectiveness of the PI can vary significantly for individual nodes, highlighting the importance of having an adaptive scheme. Upon closer inspection of \cref{fig:nodeqca}, we see that the coverage rates provided by \SystemName are more uniformly located around the desired coverage rate of 90\% across nodes. In contrast, the coverage rate provided by CQR is highly diverse, ranging from 45\% to 100\%. In other words, CQR's performance is less consistent and less robust than \SystemName, as CQR can lead to a poor coverage guarantee for individual nodes under a given prediction window.


\subsection{Case Study of Selected Test Samples}
\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/interval.pdf}
    \caption{
      Coverage and prediction intervals for different uncertainty quanlification models of PEMS04 from 2012-02-18 18:00 to 2012-02-19 18:00. }
      \label{fig_interval}
\end{figure*}
In \cref{fig_interval}, we closely examine the PI generated by different uncertainty methods for traffic flow prediction performed on PEMS04 on the date from 2012-02-18 18:00 to 2012-02-19 18:00. In the diagram, the PI is represented as a grey area, while the ground truth of a test sample is represented by a point. If a point falls within the grey belt, the generated PI covers the true value, otherwise, it fails to cover the true value. The grey area of a good strategy should cover as many points as possible while being as narrow as possible.

Methods like MC drop and Hist-W result in a small grey area with a small MPIW, but their PIs fail to cover the ground truth for a large number of test samples, leading to a poor PICP and insufficient coverage. Quantile methods like DQR and CQR perform significantly better than MC dropout and Bayesian. Compared to DQR and CQR, \SystemName produces PIs that cover the ground truth of more test samples (i.e., a higher PICP) with a smaller MPIW (i.e., a narrower grey area in the diagram). This example demonstrates the effectiveness of our adaptive scheme.


\subsection{Impact of Desired Coverage Rates}\label{sub_different_confidence_level}



\begin{table}[t]
  \centering
  \caption{Performance Comparison of Quantile Methods under Different Coverage Levels for $\mathrm{PICP}$ (\%) $\uparrow$ / $\mathrm{MPIW}$ $\downarrow$.}
  \begin{tabular}{rllll}
    \toprule
    \begin{tabular}[x]{@{}c@{}} Expected \\ Coverage \end{tabular} &  DQR  & CQR & \SystemName\\
    \cmidrule(lr){1-4}
      %0.5	& -50.00\% / 0.00 & -0.74\% / 2.85 &	\textbf{-0.72\%	/ 2.76} \\
      0.6	& $-$41.1\% / 1.0 & $-$0.1\% / 4.1 &	\textbf{$+$0.1\%	/ 4.1} \\
      0.7	& $-$33.0\% / 2.0 & $-$1.5\% / 5.3 &	\textbf{$-$1.1\%	/ 5.2} \\
      0.8	& $-$21.2\% / 4.3 & $-$1.7\% / 7.8 &	\textbf{$-$1.4\%	/ 7.6} \\
      0.95	& $-$5.2\%	/ 21.5 &	$-$8.5\% / 20.9 &	\textbf{$-$2.5\% / 20.2} \\
      \textbf{Avg.}	& $-$30.09\%	/ 5.76	& $-$2.50\%	/ 8.15 &	\textbf{$-$1.11\%	/ 7.97}\\
    \bottomrule
    \label{tab: different quantile}
  \end{tabular}
\end{table}
So far, our evaluation set 90\% as the targeting coverage rate. In this section, we investigate the impact of the desired coverage rate on the performance of uncertainty methods. Previous evaluations have shown that DQR, CQR and \SystemName are the best-performing methods. Therefore, we focus on quantile-based methods in this experiment. We apply the quantile methods to the METR-LA dataset using GWNet as the base DNN model.  We vary the expected coverage level, $1-\alpha$, from 0.6 to 0.95, corresponding to a target coverage rate of 60\% to 95\%, respectively, to evaluate the usefulness of traffic uncertainty models in different practical scenarios.  

\cref{tab: different quantile} compares the PCIP/MPIW given by each quantile method averaged across the test samples for a target coverage level. As expected, as we increase the coverage level to ensure a coverage rate, a larger MPIW (and a wider PI) is required. Once again, \SystemName outperforms the other methods, providing the best coverage rate across settings. While DQR has the smallest MPIW in most cases, it gives a poor coverage rate, which can be up to 41\% below the expected value. This suggests that the PI provided by DQR is too narrow (and hence has a small MPIW) to cover the true value of the prediction. CQR addresses the issues of DQR by using a constant adjustment value. However, it can still provide poor coverage or a PI that is too wide for some individual nodes. By dynamically adjusting the PI based on individual nodes, \SystemName provides the best overall performance. This further demonstrates the effectiveness and flexibility of our approach in adapting to different sensor nodes or locations. 

\subsection{Impact of Training-Calibration Split Ratio}\label{sub:dsr}
\begin{table}
  \centering
  \caption{Performance Comparison of Different Training-calibration Ratio $\beta$ on Traffic Flow Dataset PEMS08 ($\mathrm{PICP}$ (\%) $\uparrow$ / $\mathrm{MPIW}$ $\downarrow$)} % train:cal
  \begin{tabular}{ccc}
    \toprule
    $\beta$ & CQR & \SystemName \\
    \cmidrule(lr){1-3}
    1:6 & $+$2.4\% / \textbf{88.7} & \textbf{$+$3.0\%} / 88.9 \\
    2:5 & $+$0.5\% / 76.2 & \textbf{$+$0.8\% / 76.1} \\
    3:4 & $+$0.1\% / 72.5 & \textbf{$+$0.1\% / 72.3} \\
    1:1 & $+$0.3\% / 74.6 & \textbf{$+$0.5\% / 73.6} \\
    4:3 & $+$2.6\% / 73.9 & \textbf{$+$2.6\% / 71.7} \\
    5:2 & $+$1.4\% / 73.6 & \textbf{$+$1.6\% / 73.4} \\
    6:1 & $+$2.1\% / \textbf{74.3} & \textbf{$+$2.4\%} / 75.1 \\
    \textbf{Avg.} & $+$1.35\% / 76.25 & \textbf{$+$1.56\% / 75.88}\\ 
    \bottomrule
  \end{tabular}
  \label{sub_different_splitting_rate}
\end{table}
To build the calibration component, \SystemName and CQR require setting aside some data from the training dataset as the calibration data. Our experiments described so far leave 40\% of the original training data as the calibration dataset. In this experiment, we evaluate how the training-calibration dataset ratio affects the performance on \SystemName and CQR. Specifically, We vary the ratio ($\beta$) between the DNN model training data and the calibration data to examine the impact on performance. Specifically, we evaluate the ratios of 6:1, 5:2, 4:3, 1:1, 3:4, 2:5, and 1:6. We then apply \SystemName and CQR to the PEMS08 dataset, using GWNet as the base DNN model, with a target coverage rate of 90\%.

The result is given in \cref{sub_different_splitting_rate}. Leaving too few samples to train the base DNN model may lead to decreased model accuracy, resulting in a wider PI (and larger MPIW) required to ensure sufficient coverage. For instance, when the training-calibration data ratio $\beta$ is set to 1:6 or 2:5, both CQR and \SystemName produce a wide PI. However, insufficient calibration data can also impact the accuracy of the uncertainty model, given that \SystemName relies on partitioning the calibration dataset to construct the calibration table. As a result, a smaller calibration dataset (e.g., when $\beta$ is 6:1) can affect the performance of the model. Nevertheless, for most experimental settings, \SystemName outperforms CQR in terms of both the PICP and MPIW metrics.





\subsection{Quantile Search for Calibration Table Construction} \label{sub:gvq}
\begin{table}[t!]
  \centering
  \caption{Performance Comparison of Grid Search and \SystemName \\ ($\mathrm{PICP}$ (\%) $\uparrow$ / $\mathrm{MPIW}$ $\downarrow$)}
  \begin{tabular}{rrr}
    \toprule
    Frequency & Grid search	&	\SystemName \\
    \midrule
    10 &	$-$49.1\%	/ 31.5 &	\textbf{$+$0.9\%	/	73.1 }\\
    20 & $-$23.8\%	/	44.7 & \textbf{$+$0.7\%	/	72.0} \\
    40 & $-$9.9\%	/	56.3 & \textbf{$+$0.5\%	/	71.4} \\
    60 & $-$5.6\%	/	61.9 & \textbf{$+$0.4\%	/	71.1} \\
    80 & $-$3.7\%	/	64.5 & \textbf{$+$0.4\%	/	70.9} \\
    100 &	$-$2.7\%	/	66.3 & \textbf{$+$0.4\%	/	70.9} \\
    \bottomrule
  \end{tabular}
  \label{sub_grid_vs_quantile}
\end{table}

As described in Section \ref{subsec: component}, to construct the calibration table, we first apply the trained DNN and quantile function to the calibration dataset to obtain the prediction residuals. We then partition the residuals into continuous intervals with equal percentiles rather than predefined equal-sized intervals. In this experiment, we compare our quantile-based approach against a grid method for projecting the residuals onto intervals. The results were obtained on the PEMS08 dataset using GWNet as the base DNN model and a target coverage rate of 90\%. The results are given in \cref{sub_grid_vs_quantile}, which demonstrates the effectiveness of quantile search over the grid search method.

Using quantiles over a specific range can be advantageous in situations where traffic data is partially missing, and the range of possible values for residuals is not well-defined. Quantile search reduces the need for dense interval steps by focusing the search on the most promising regions in the parameter space, thereby increasing the efficiency of the search process. It can also help avoid overfitting and increase robustness to outliers. On the other hand, in grid search, missing traffic data can cause a narrow range of possible values, requiring more interval steps to capture better adjustments.


\subsection{Performance Evaluation of Traffic Forecasting Models\label{sec:petf}}
\begin{table}[t!]
  \centering
  \caption{Performance Evaluation Based on Point and Uncertainty Metrics}
  \label{tab: point vs uncer}
  \scriptsize
  \begin{tabular}{clrrrrr}
  \toprule
  \multirow{2}*[\multirowoffset]{Data} & \multirow{2}*[\multirowoffset]{Method} & \multicolumn{3}{c}{Point estimation} & \multicolumn{2}{c}{Uncertainty estimation} \\
  \cmidrule(lr){3-5}\cmidrule(lr){6-7}
  & & MAPE $\downarrow$ & RMSE $\downarrow$ & MAE $\downarrow$ & PICP $\uparrow$ & MPIW $\downarrow$ \\
  \midrule
  \multirow{5}*{\rotatebox{90}{METR-LA}}
  & STGCN  & 13.09\% & 11.81 & 5.08 &	90.13\% & 19.52 \\
  & GWNet  & \textbf{11.93\%} & \textbf{11.49} & \textbf{4.85} & 90.14\% & 15.22 \\
  & MTGNN  & 12.17\% & 11.62 & 4.87 &	\textbf{90.24\%} & \textbf{14.79} \\
  & GMAN   & 14.10\% & 11.91 & 5.48 &	89.36\%	& 17.27 \\
  & LSTNet & 15.38\% & 11.89 & 6.34 &	89.83\%	& 23.28 \\
  \cmidrule(lr){1-7}
  \multirow{5}*{\rotatebox{90}{PEMS08}}
  & STGCN  & 11.15\% & 18.01 & 28.11 & 90.06\% & 83.44 \\
  & GWNet  & \textbf{9.59\%} & \textbf{15.05} & \textbf{23.81} & 90.05\% & 72.01  \\
  & MTGNN  & 9.69\% & 15.15 & 23.98 & \textbf{91.16\%} & \textbf{71.13} \\
  & GMAN   & 12.99\% & 18.29 & 28.06 & 90.20\% & 80.77 \\
  & LSTNet & 10.86\% & 17.07 & 26.83 & 90.78\% &  78.31 \\
  \bottomrule
  \end{tabular}
\end{table}

The ability to model prediction uncertainty can also be useful in evaluating the creditability of a traffic forecasting mode. In this experiment, we extend our evaluation to explore the trade-off between accuracy and confidence in traffic forecasting tasks. There is increasing research effort in discovering the best-scoring traffic predictor in specific or ideal scenarios \cite{Vlahogianni2021}. Meanwhile, the reliability of the experimental evaluation is often neglected. For example, a traffic forecasting model could perform well on average in point estimation matrices but be less accurate during peak hours than at night. In this evaluation, we apply \SystemName to METR-LA and PEMS08 and compare the prediction accuracy measured by commonly used loss functions, Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Mean Absolute Percentage Error (MAPE) and the corresponding coverage and discrimination on test data measured in PICP and MPIW. 

As can be seen from the \cref{tab: point vs uncer}, GWNet would be regarded as the most accurate model using traditional point-based evaluation metrics, but it is less reliable in coverage and discrimination metrics. While evaluating model credibility is not the focus of this work, our approach can provide a new measure for the performance evaluation of traffic forecasting models. 

\section{Discussion}

Naturally, there is room for future work and improvement. We discuss a few points here.

\cparagraph{Alternative quantile functions.} In the quantile DNN model training (see \cref{subsec:Quantile}), we attach a linear layer  to the last layer of the DNN and use the linear layer as the quantile function. An alternative could be using a dedicated network to generate two separate predictions. In this way, for each input, the DNN model would produce two separate outputs from the modified last layer, one for the lower bound and the other for the upper bound. The pinball loss assigns orthogonal weights to both predictions and uses them to calculate the loss. We leave this as our future work.

\cparagraph{Calibration component.} In this paper, we use a dedicated calibration table to provide customized PI adjustments for individual nodes. Another way of doing this is to train a calibration function using, e.g., linear regression or a neural network. Doing so would require having sufficient training data to learn an accurate calibration function. 

\cparagraph{Coverage goal.} \SystemName aims to produce a PI to meet the user-defined coverage level. In general, a higher coverage level guarantees stronger prediction accuracy  In practise usage, a high coverage level can be employed to ensure, for instance, the worst-case arrival time. In contrast, a lower coverage level may suffice if the user is willing to tolerate a certain level of prediction error in traffic information, for example, if the consequence of missing an event is insignificant. As such, techniques for learning and modeling user needs \cite{Webb2001} are complementary to \SystemName. 

\cparagraph{Other calibration techniques.}
\SystemName employs a straightforward yet efficient method of utilizing a calibration table to adjust the PI. Additionally, other post-processing techniques can be used to enhance initial predictions. For instance, ensemble methods \cite{nips_Lakshminarayanan17} can leverage multiple prediction models, and data augmentation techniques \cite{thulasidasan2019mixup} involve applying various data augmentation methods to the test input to obtain multiple predictions to form the PI. Our future work will investigate the use of these techniques. 

\cparagraph{Data distribution drifts.} CQR leverages conformal prediction for uncertainty measurement, which is based on the fundamental assumption of data exchangeability \cite{Jing2018}. Exchangeability refers to the statistical properties of the data remaining unchanged regardless of the order in which it is presented. However, in many real-world applications, particularly in traffic forecasting, the data distribution may be shifted due to various factors such as changes in road infrastructure (e.g., temporary road closures), weather patterns, and traffic patterns (e.g., traffic accidents), leading to a shift in the test data distribution with respect to the training data. Our future work will investigate how online  adaptation techniques can be employed to detect data drift and mitigate the issue of data drift through (e.g., continuous learning \cite{9349197}).

\cparagraph{Calibration data size.}
\SystemName and CQR require setting aside some data from the model training dataset as the calibration data.  If the calibration set is not representative of the test set, the performance of CQR and \SystemName may suffer, as shown in \cref{sub_different_confidence_level}. As such, techniques for data augmentation \cite{ijcai2021p631} like  basic data augmentation methods (e.g., window cropping \cite{le2016data}), Deep Generative Models \cite{8903012} and data selection \cite{eetemadi2015survey} like active learning  and scoring functions \cite{eck2005low} are orthogonal to our approach. 



