\section{Preliminaries}\label{sec:problem}

\subsection{Problem Definition}\label{sec:problem/define}

A single-point traffic forecasting model attempts to predict future traffic information based on the past. Examples of traffic information include flow, speed, and density. Given a $D$-dimensional multivariate time series $X = \begin{Bmatrix} x_{t-(m-1)},\hdots, x_{t-1} ,x_{t} \end{Bmatrix} \in \mathbb{R}^{ N \times D}$ collected at $t$ time with past $m$ steps from $N$ data sources (roads or senors), a point forecasting model $f(\cdot)$ attempts to estimate the multivariate time series $Y = \begin{Bmatrix} y_{t}, \hdots, y_{t+h-1}, y_{t+h} \end{Bmatrix} \in \mathbb{R}^{ N \times D}$ in the next prediciton window (e.g., $h$ steps), : $X \overset{f(\cdot)}{\rightarrow} Y$.

Probabilistic traffic forecasting involves predicting the likelihood of various potential outcomes, rather than simply estimating the most likely outcome. Given historical data $X$, a probability predictor $F(\cdot)$ can estimate the uncertainty of future traffic conditions by producing a set of PIs, which contain the real data with a certain level of confidence.

Specifically, a probabilistic forecasting model attempts to estimate PIs for a set of future values denoted as $\hat{Y} = \{\hat{y}_{t}, \ldots, \hat{y}_{t+h}\} \in \mathbb{R}^{2 \times N \times D}$, where $t$ is the current time, $h$ is the prediction window. The PI for $\hat{y}_{t+h}$ is defined as ${\hat{y}_{t+h}^{l} \leq \hat{y}_{t+h} \leq \hat{y}_{t+h}^{u}}$, where $\hat{y}_{t+h}^{l}$ and $\hat{y}_{t+h}^{u}$ are the lower and upper bounds of the PI, respectively. By specifying a confidence level $\alpha$, we can say that there is a probability of $(1-\alpha)$  that the true value $Y_{t+h}$ falls within the PI $\hat{Y}_{t+h}$. The probabilistic forecasting process can be described as $X \overset{F}{\rightarrow} \hat{Y}$, where $F$ is the probability predictor that maps historical data $X$ to the predicted PIs for $\hat{Y}$.

This paper aims to develop a generic framework to extend a single-point predictor $f$ to model the
uncertainties in traffic modeling tasks.  We target DNN-based traffic forecasting models as they are the core of
state-of-the-art traffic forecasting methods~\cite{jiang2021dl}. It is worth nothing that our approach does not change
the underlying DNN model structure and can be integrated with other classical supervised learning methods, including
support vector machines~\cite{708428}, linear~\cite{PAPADOPOULOS2011842} and non-linear regressors~\cite{Random2001}.

\subsection{Optimization Goals\label{sec:goal}}
Our goal is to generate PIs that are both narrow and accurate, meaning that they are just wide enough to cover the true value (e.g., the actual traffic speed) with high probability. To achieve this, we need the PI to be \emph{discriminative} so that it is narrow for single-point predictions with high confidence, but wide enough to ensure good coverage overall \cite{NIPS2021_00225}.

We also require the uncertainty method to perform well across test samples and have good coverage, which means that we need the results to be \emph{valid}~\cite{NIPS2021_00225}. Specifically, for a given probability or coverage rate of $x$ (defined as $1 - \alpha$), we expect the PI to encompass the true value at least $x$ of the time.

\subsection{Quantified Metrics\label{sec:qm}}

We adopt the established practices of prior uncertainty modeling methods in related domains~\cite{NIPS2021_00225,pmlr-v80-pearce18a} and use two metrics to evaluate the efficacy of the PI: \textbf{validity} and \textbf{discrimination}~\cite{NIPS2021_00225}. Specifically, we use \emph{mean prediction interval width (MPIW)} and \emph{prediction interval coverage probability (PICP)} to measure these requirements. Our goal is to obtain a small MPIW with adequate coverage across test samples. While a large MPIW can ensure good coverage, it may frequently underestimate or overestimate traffic information, even when the model is relatively confident with the prediction. On the other hand, a small MPIW can miss the true value, leading to incorrect decisions such as selecting a slower route due to incorrect predictions. 

\cparagraph{Mean prediction interval width} We compute the MPIW across $n$ test samples as:
\begin{equation}
    \label{eq:PI}
        \mathrm{MPIW} = \frac{1}{n} \sum_{i=1}^{n} \begin{vmatrix}y_i^{u} - y_i^{l}\end{vmatrix}.
    \end{equation}
The MPIW is a \emph{smaller-is-better} metric as we anticipate the prediction width to be as narrow as possible.

\cparagraph{Prediction interval coverage probability} In addition to MPIW, we also consider the frequency PI covers the real value over the complete $n$ test samples. The observed coverage $\hat{C}_{\alpha}$ is computed as:
\begin{equation}
    \label{eq:coverage}
        \begin{matrix}
            \hat{C}_{\alpha} = \frac{1}{n}\sum_{i=1}^{n}c_{i},
    &
    c_{i} = \left\{\begin{matrix}
     1, & y_{i} \in [y_i^{l},y_i^{u}]\\
        0, & y_{i} \notin  [y_i^{l},y_i^{u}]
        \end{matrix}. \right.
    \end{matrix}
    \end{equation}
Here, $c_i$ is a binary value counting if the true value $y_i$, is within the PI $\{y_i^{l},y_i^{u}\}$ for test sample $i$.

Given a target probability level $1 - \alpha$, the \emph{expected coverage} is $1 - \alpha$ for $n$ test samples. $\alpha$ is defined as mis-coverage rate. For this case, an ideal uncertainty predictor produces $n$ PIs for all test samples to cover the real value at least $1-\alpha$
time, rendering the \emph{observed coverage} $\hat{C}_{\alpha}$, to be $1 - \alpha$ as well.
If $\hat{C}_{\alpha}$ is less than $1 - \alpha$, the PI is likely undercovered.

Undercoverage is generally bad since it limits the PI to meet the requirement of retaining the true values of important test samples.

\subsection{Conformalized Quantile Regression}
\label{subsec: CQR}

Our approach is inspired by the recently proposed CQR
algorithm~\cite{romano2019conformalized}. CQR combines conformal prediction \cite{ Vladimir2005}
and classical quantile regression for uncertainty qualification. It uses quantile regression to generate an initial PI
and then uses conformal prediction to adjust the PI if required.

\subsubsection{Working mechanism of CQR}
In layperson's terms, a quantile regressor is a tool for quantifying the chance of having a specific point in a range of possible outcomes. For instance, if we want to predict the 90th percentile or quantile for traffic speed, it means there's a 5\% chance that the actual speed will be lower than our prediction, and an 5\% chance it will be higher. By fitting two quantile functions - one at the 5th percentile and another at the 95th percentile - we can create a prediction interval (PI) with a 90\% coverage. This is where CQR comes in handy for modeling traffic uncertainty, as it allows us to estimate these quantiles. However, no previous work has used CQR for traffic uncertainty modeling, making our work the first attempt to fill this research gap.

In CQR, the training data is divided into two distinct sets: a training set and a calibration set. The training set is used to develop the traffic forecasting model using standard supervised learning techniques. Then, two quantile regressors are trained on the training set to generate initial estimates of the upper and lower bounds of the PI. One quantile regressor is used for estimating the upper bound quantile, and the other for the lower bound. After obtaining the initial estimates, a conformal step is performed on the calibration to learn a \emph{single} adjustment (e.g., increasing or decreasing the PI by x\%) to be used to adjust the PI for all test samples. 

\subsubsection{Limitations of CQR\label{sec:lmcqr}} 
A drawback of using CQR for traffic forecasting is that it only produces a single, global quantile adjustment for all test samples. However, a global quantile adjustment is unlikely to be effective, as
prediction difficulties can vary significantly across domains
or roads. To illustrate this point, consider once again Routes 1 and 2 given in our motivation example (\cref{sec:motivation}). Suppose Route 1 is more complex than Route 2, which is smoother. Applying a global adjustment, such as increasing or decreasing the model prediction by a certain percentage, to both Routes 1 and 2 is unlikely to be effective. It could result in a PI that is too narrow for the complex Route 1 or too wide for the smooth Route 2.  A better approach, and is the one adopted in this paper, is to generate different adjustment ranges for each route, resulting in more accurate and reliable PIs. Doing so  enhances the robustness of traffic uncertainty modeling by considering the specific prediction difficulties of each location.
