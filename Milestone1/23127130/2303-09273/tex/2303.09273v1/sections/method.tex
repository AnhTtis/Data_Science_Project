\section{Our Approach}
\label{sec:method}

\begin{figure*}
    \centering
    \includegraphics[width=0.80\textwidth]{fig/deployment_overview}
    \caption{\textbf{Modeling forecasting uncertainties during deployment.} \SystemName extends the standard DNN-based traffic forecasting model to quantify prediction uncertainties. For a given a test input, a trained DNN generates a single-point prediction akin to traditional DNN models. This single-point prediction is subsequently processed by a quantile function, which produces an initial prediction interval (PI) for the input. The initial PI, along with the test input, is then passed through the calibration component, to produce a final PI. }
    \label{fig:deployment}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{fig/training_overview}
    \caption{\textbf{Training workflow of our approach.}
    We first partition the DNN model training datasets into disjoint parts (a). We use the first part to train the DNN forecasting model and a quantile function that can produce an initial PI during deployment (b). We then apply the trained DNN model and quantile function to the set-aside calibration dataset to build the calibration component (c). }
\label{fig:training}
\end{figure*}

\SystemName is a generic framework for quantifying the prediction uncertainties of DNN-based traffic forecasting models.  Given a certain level of confidence, the \SystemName framework computes a PI that defines a range of possible values where the real data (e.g., traffic time, flow or speed) is likely to fall within.

\SystemName extends upon the existing CQR framework but provides a significant advantage by using a dedicated calibration component to dynamically adjust the width of the PI  based on locations and prediction windows (e.g., traffic conditions in the next $n$ minutes, also known as \emph{prediction horizon}), leading to improved accuracy. Another important feature of \SystemName is that it does not require any modifications to the underlying DNN model structure, making it  easy to implement. Instead, \SystemName uses a dedicated loss function to train the DNN model and a quantile function to predict an initial PI. This allows \SystemName to be seamlessly integrated with any DNN-based traffic predictor, providing an efficient and effective solution for improved traffic forecasting.

This section begins by providing an overview of \SystemName and illustrating its design principle. It then discusses how to train and construct the key components that drive the DNN model to achieve better coverage and more accurate PIs and finally elaborates on how  uncertainty qualification is achieved with test samples during deployment.

\subsection{Overview of \SystemName}
Figure \ref{fig:deployment} demonstrates how \SystemName enhances a standard DNN-based traffic forecasting model to quantify uncertainties for traffic forecasting during deployment. For a test input, the trained DNN model first produces a single-point estimation for, e.g., speeds or travel time, just like any existing traffic forecasting model. This single-point prediction is then passed to a quantile function to produce an initial PI. This PI is a 2-dimensional numerical vector, $\{l, u\}$, where $l$ is the lower bound and $u$ is the upper bound value.


In contrast to standard CQR that applies a global adjustment to the initial PI, our approach utilizes the test input's sensor node id and prediction window to locate a residual value, $\delta_i$, from a calibration table. This value is specific to the given node id, $i$, and the target prediction window, which is likely to provide a good trade-off between coverage and discrimination (see Section \ref{sec:qm}). After retrieving $\delta_i$, we then apply it to adjust the width of the initial PI, as $\{l-\delta_i, u+\delta_i\}$, to obtain the final PI. Through this process, the PI can be more accurately adjusted to reflect the specific context of the prediction, leading to enhanced accuracy and better coverage.

As shown in Figure \ref {fig:training}, our approach adapts the standard DNN model training workflow for uncertainty modeling. Our training process starts by partitioning the training data into two \emph{disjoint}, namely \emph{training} and \emph{calibration} sets respectively, to train the DNN model and build the calibration table separately. This process is described in Section \ref{subsec:Pre-processing}. Specifically, the first traing set is used to train both the DNN-based traffic forecasting model and a quantile function, using a pinball function to compute the loss during training (Section \ref{subsec:Quantile}). Once the DNN model and quantile function have been trained, we then move on to use the calibration set to construct the calibration table (Section \ref{subsec: component}).

\subsection{Pre-processing of Training Data}\label{subsec:Pre-processing}
As shown in \cref{fig:training} (a), as a \emph{one-off} preprocessing step, the training data is split into a standard model training dataset and a calibration dataset.
Specifically, in this paper, we set aside 40\% of the training data for calibration purposes, but this ratio can be adjusted by the user. The calibration dataset is created using random sampling on nodes, ensuring that it contains data for each node (or sensor), if possible. During the preprocessing stage, the quantile function (e.g., a linear function) is also added to the last layer of the DNN model.

\subsection{Training DNN Model and Quantile Function\label{subsec:Quantile}}

As depicted in \cref{fig:training} (b), the underlying DNN model and the attached quantile function are trained as a single network using the DNN training data. Then, during the back-propagation, the pinball loss is used to figure out the loss of the network. Specifically, the native DNN model makes a single-point estimate for each input, which is then passed to the quantile layer to determine the upper and lower boundaries of the prediction interval. Then the loss of the network is computed as follows:

\begin{equation}
	\label{eq: pinball loss}
    L_{\alpha} =\left\{\begin{matrix}
\alpha_{\textnormal{tra}}(y-\hat y) & y-\hat y> 0 \\
(1 - \alpha_{\textnormal{tra}})(\hat y - y) & \textup{otherwise}
\end{matrix}\right.,
\end{equation}
where $\alpha_{\textnormal{tra}}$ is a pre-defined mis-coverage rate (see Section \ref {sec:qm}) used at the training phase, $y$ is the ground truth, and $\hat{y}$ is the single-point prediction given by the base DNN model.

Essentially, the pinball loss function applies different weights to positive and negative residuals based on a known level of confidence ($1-\alpha$) \cite{Koenker2001Quantile}. A smaller value of $\alpha$ results in greater penalty for samples with values smaller than the predicted value, and vice versa \cite{RePEc:spr:aodasc:v:9:y:2022:i:2:d:10.1007_s40745-020-00253-5}. When $\alpha_{\textnormal{tra}}$ is set to 0.5, \cref{eq: pinball loss} degenerates to the yielding the median value of the corresponding dataset. Practically, setting $\alpha_{\textnormal{tra}}$ to $0.1$ expects that $90\%$ of ground truths would fall within the generated PI.

\subsection{Building Calibration Table}\label{subsec: component}

In Figure \ref{fig:training} (c), we outline the process of constructing the calibration table using the left-over calibration dataset.

\subsubsection{Calibration table} The proposed calibration table is a matrix designed to record the residual errors of a sensor node (or location) for a given prediction window. Each row of the table stores the residuals of a specific sensor node for various prediction windows, while each column records the residuals of a particular prediction window for different sensor nodes.

\subsubsection{Data preprocessing} Similar to the standard DNN training procedure, where the training data is partitioned into training and validation sets, we split the calibration data into two sets: $\chi_{1}$ for constructing the potential adjustments set for the initial PI and $\chi_{2}$ for decision-making to determine the best adjustment. In the first set $\chi_{1}$, the trained quantile DNN predicts the traffic information for each test node in a given prediction window. Then the prediction errors or residuals is calculated by measuring how far the PI boundaries deviate from the ground truth. In the second validation set $\chi_{2}$, the residual candidates obtained in $\chi_{1}$ are used to determine the optimal values. This optimal values is stored in the corresponding cells of the calibration table for a given pair of node id and prediction window. We gain this optimal values by minimizing a carefully designed objective function. In this paper, we leave 50\% of the calibration data as the $\chi_{2}$.

\subsubsection{Compute residual percentiles}
To compute the residuals for populating the calibration table, we apply the trained quantile DNN to each test sample (from the first calibration dataset $\chi_{1}$) of a given node $i$ and prediction window to produce a PI, denoted as $\{\hat{y}_{i,j}^{l},\hat{y}_{i,j}^{u}\}$, where $\hat{y}_{i,j}^{l}$ and $\hat{y}_{i,j}^{u}$ are the lower and the upper bound of
the PI of node $i$ in the prediction window $j$,  respectively. We then compute the residuals of the PI as $R_{i,j} = \left| \hat{y}_{i,j}^{u} - y_{i,j} \right|\cap \left| \hat{y}_{i,j}^{l} - y_{i,j} \right|$, where $y_{i,j}$ is the true value of node $i$ in the given prediction
window $j$. Note that there may be multiple test samples for a given node-prediction window pair, resulting in $n$ residuals denoted as $R^1_{i,j}, R^2_{i,j}, \dots, R^n_{i,j}$. 

Next, the percentiles of $n$ residuals for each node-prediction-window combination could be constructed. Specifically, the residual candidates $R^1_{i,j}, R^2_{i,j}, \dots, R^n_{i,j}$ are first sorted by value and subsequently partitioned into $m$ equal-size groups, commonly referred to as percentiles, based on their relative position within the distribution. It is worth noting that the number of quantiles $m$ is a user-defined parameter that balances calibration accuracy and computational complexity. For our study, we set $m=100$. The 0th percentile corresponds to the lowest value of residuals, while the 100th percentile corresponds to the maximum value in the residual datasetCompared to histogram-based approaches, residual quantiles demonstrate more robustness to outliers (as demonstrated in \cref{sub:gvq}). Finally the residual percentiles are denoted as $Q^1_{i,j}, Q^2_{i,j}, \dots, Q^m_{i,j}$.

\subsubsection{Choosing the best quantile}\label{subsec: calibration function}
After calculating the residual percentiles, the next step is to choose a specific quantile from the distribution to be saved in the matching cell of a node-prediction-window pair. To this end, we apply the trained quantile DNN to test samples from the second validation set $\chi_{2}$ to produce an initial PI for each test sample. Next, the residual quantiles, $Q^1_{i,j}, Q^2_{i,j}, \dots, Q^m_{i,j}$, are applied to the initial PI to obtain an adjusted PI one by one. By comparing the coverage and effective width of each resulting PI for each node-prediction-window combination against the true value of the validation set, we determine which residual provides the best performance. The optimal residual for the node-prediction-window combination is then recorded in the corresponding cell of the calibration table.

Specifically, our objective for selecting the residual to be recorded in the calibration table is to minimize: 
\begin{equation}
L = \mathop{\arg\min}\limits_{Q^m} \sum_{0}^{i} \sum_{0}^{j}  - \lambda \mathcal{C}{\textnormal{cov}} (Q^m_{i,j},\chi_{2}) + (1 - \lambda) \textnormal{PI} (Q^m_{i,j},\chi_{2}),
\label{eq:calibrations quantile loss}
\end{equation}
where $\lambda \in [0,1]$ is a weight to control the balance of $\textnormal{PI}$ and the coverage $\mathcal{C}{\textnormal{cov}}$ for a given node-prediction-window combination. Finally, the calibration table records the residual value where the index is the calibration quantile that gives the minimum $L$. 


\subsection{Using the Trained DNN and Calibration Component}
Once we have trained the DNN model and the quantile function and constructed the calibration table as described above, we can then apply them to unseen test samples. This process is illustrated in Figure~\ref{fig:deployment}.

For a given test sample, the trained DNN generates a single-point prediction that is then passed to the quantile function to create the initial PI, $\{l, u\}$. Using the node id of the test sample and the prediction window, we search the calibration table for the corresponding residual value $\delta$. This value is used to amend the initial PI to $\{l-\delta, u+\delta\}$. A positive $\delta$ results in a wider PI, while a negative $\delta$ narrows it. It is possible that no residual value is available for a given combination of node id and prediction window, in which case the initial PI remains unchanged.

Residual errors of missing combinations of node ids and prediction windows can be later inserted into the calibration table. This can be done by using the true value measured after the prediction window to compute the residual errors of the node-prediction-window pair. Similarly, during deployment or every time the DNN model is re-trained, the calibration table can be also updated with the recorded predictions and true values. This way, the calibration table and quantile function can evolve over time to adapt to changes in the deployment environment.
