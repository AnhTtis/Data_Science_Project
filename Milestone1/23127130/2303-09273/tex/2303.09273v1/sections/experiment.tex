\section{Evaluation Setup}
We evaluate \SystemName on real-world datasets. In particular, we apply \SystemName to representative DNN architectures for traffic forecasting and compare it with a wide range of baseline methods for uncertainty modeling. This section provides a detailed description of the datasets used in our evaluation, the baseline quantile methods that we compare \SystemName against, as well as the hardware and software platforms used for the experiments.

\subsection{Datasets}
\begin{table}[t!]
\centering
\scriptsize
\caption{Description of the Datasets Used.}
\begin{tabular}{ccccc}
\toprule
\textbf{Type}  & \textbf{Dataset}  & \textbf{No. sensors} & \textbf{Time period} &\textbf{Mean / Std} \\   
\midrule
\multirow{3}{*}{\rotatebox{90}{Speed}}
& METR-LA   & 207     & 03/01/2012 - 06/30/2012  & 53.71 / 20.26\\
& PeMS-BAY  & 325     & 01/01/2017 - 05/31/2017  & 62.61 / 9.59 \\
& PeMSD7(M) & 228     & 05/01/2012 - 06/30/2012  & 58.89 / 13.48\\

\cmidrule(lr){1-5}
\multirow{4}{*}{\rotatebox{90}{Flow}}
& PEMS03    & 358     & 09/01/2018 - 11/30/2018  & 179.3 / 143.7\\
& PEMS04    & 307     & 01/01/2018 - 02/28/2018  & 211.7 / 158.1\\
& PEMS07    & 883     & 05/01/2017 - 08/31/2017  & 308.5 / 188.2\\
& PEMS08    & 170     & 07/01/2016 - 08/31/2016  & 230.7 / 146.2\\

\bottomrule
\end{tabular}
\label{tab:dataset}
\end{table}

In our evaluation, we used seven public traffic datasets that are widely adopted in the literature. \cref{tab:dataset} provides the basics of each dataset, including the type of traffic data collected (e.g., speed or flow), the number of sensors used to collect the data, and the time period covered by each dataset. The raw data from these datasets are aggregated into 5-minute intervals, in align with previous literature, e.g., \cite{9346058}. 

\subsection{Base Traffic Forecasting Models}\label{subsec: base model}

To demonstrate the applicability of \SystemName over a variety of DNN-based traffic predictors, we adopt six representative models, namely, Spatio-Temporal Graph Convolutional Network (STGCN) \cite{Yu2018spatio}, Graph Wavenet (GWNet) \cite{ijcai_WuPLJZ19}, Graph Multi-Attention Network (GMAN) \cite{aaai_ZhengFW020}, Long-and Short-term Time-series Network (LSTNet) \cite{sigir_LaiCYL18}, and Multivariate Graph Neural Network (MTGNN) \cite{wu2020connect}. These models cover different variants of the graph neural network and  the long short-term memory (LSTM) architecture. 
They  have been widely used in the literature and represent 
different classes of deep learning-based methods for traffic forecasting. Note that our goal is to demonstrate the generalization ability of \SystemName to different models, rather than to compare the relative performance of the base DNN architectures. Therefor, our evaluation is designed to compare the performance of various quantile methods in uncertainty modeling, all within the same base DNN architecture.

\subsection{Competing Baselines}\label{subsec: competing baseline}
We compare \SystemName against five representative uncertainty quantification methods: 

\cparagraph{Historical Data-Based Methods} assume that traffic data, such as speeds or flow distribution, exhibit strong repeat patterns during the same period (e.g., the same day across weeks or the same hour across days). These methods predict current travel speed and flow distribution by utilizing the distribution of the same period from prior data, such as the distribution of the same day from the previous weeks. In our experiments, we consider two baseline methods that rely on historical data: \emph{Hist-D}, which uses data from the previous days in the training dataset for predictions, and \emph{Hist-W}, which uses the distribution of the same period from the previous weeks for prediction. For example, if we were to predict the PI for node $i$ at 8 a.m. on Monday, \emph{Hist-D} would calculate the mean $\mu_{i}^{t}$ and variance $\sigma_{i}^{t}$ across all previously seen samples of node $i$ at 8 a.m. of all Mondays. Once the mean and variance for each time slot and node of interest are calculated, these two values can be used to construct the prediction interval for that slot and node with $\{\mu_{i}^{t}$-$\sigma_{i}^{t}$, $\mu_{i}^{t}$+$\sigma_{i}^{t}\}$. Similarly, \emph{Hist-W} would compute the weekly mean and variances across four weeks of a month to compute the PI. 

\cparagraph{Bayesian uncertainty quantification} models the uncertainty in the model parameters using a likelihood function constructed by Bayesian modeling \cite{Blundell2015weight}. It also computes the data uncertainty by approximating the probability distribution over the model outputs through sampling and averaging over the resulting models. In our work, we use a Gaussian prior distribution with zero mean and unit variance \cite{Blundell2015weight}, and the MC sampling number of 50.

\cparagraph{Monte Carlo dropout} \cite{pmlr-v48-gal16} models predictive distributions by randomly switching off neurons in DNNs during testing. This generates different model outputs that can be interpreted as samples from a probabilistic distribution, allowing MC dropout to estimate the prediction probability. In our work, we added a dropout layer with a rate of 0.3 after the last hidden layer of the base traffic forecasting model and used a sample number of 50.

\cparagraph {Deep Quantile Regression (DQR)} can also estimate PIs \cite{Roger1978, nips_ChungNCS21}. Unlike conventional methods that minimize the averaged residual errors, DQR calculates the prediction errors at a specific quantile of the distribution. This method requires the use of differential models or tailored loss objectives such as NLL-based or pinball losses to generate the boundaries of the PI. In our quantile regression method, we use the 5th and 95th percentile estimates, following the configuration outlined in \cite{romano2019conformalized}.

\cparagraph{Conformal} prediction \cite{Mach2008Tutorial} is a post-processing method for quantifying prediction uncertainties. The main idea behind conformal prediction is to use a nonconformity measure to evaluate the similarity between new input data and the training data. Given a certain confidence level, conformal prediction constructs prediction regions that contain a certain fraction of predictions with the same nonconformity measure. In this study, we use inductive conformal prediction \cite{Papadopoulos08} due to its simplicity, which requires splitting the training data into two subsets. We use the same training dataset splitting ratio for conformal prediction and our approach.

\subsection{Experimental Setting}

\subsubsection{Hardware and software}
We implemented \SystemName as a Python package using PyTorch version 1.8.0. We conduct our model training and evaluation on a multi-core machine  equipped with a dual-socket 20-core, 2.50 GHz Xeon(R) Silver 4210 CPU, 256GB of DDR4 RAM, and 2x NVIDIA GeForce RTX 2080 Ti GPUs. Our system is running Ubuntu 18.04.5 with Linux kernel 4.15.0 and we execute the GPU code using CUDA version 11.1. 

\subsubsection{Training setup}\label{subsec: training}
For consistency with prior work \cite{DL2021Jiang}, we split the data into training, validation, and test sets at a ratio of 7:1:2. To model uncertainty, we further reserve 40\% of the training data for calibration (see \cref{subsec:Pre-processing}). We use the Kullback--Leibler divergence loss function to train Bayesian models \cite{Blundell2015weight}, and pinball loss for DQR, CQR, and the proposed \SystemName. For all other models, we use mean absolute error (MAE) as the loss function. We train all models using the Adam optimizer \cite{LeCun2015Adam} with a learning rate of 0.001, and a batch size of 64 (except for GMAN, where we use a batch size of 16 due to GPU memory constraints). The training process stops either after 200 training epochs or when the validation loss remaining unchanged for ten consistent epochs, following the standard practice in prior work \cite{DL2021Jiang}. 
 
To ensure a fair comparison, the hyperparameters of quantile-based methods are identical to those of CQR methods. For the additional hyperparameter in CQR, i.e., calibration miscoverage rate $\alpha_{\textnormal{cal}} $ controlling the movement and direction of the upper and lower marginal bounds, we use the calibration set to pick the best value.


\subsection{Evaluation Methodology}
We vary the observation step from 1 to 12, corresponding to historical data in the last 5 to 60 minutes, as each step contains sensor data over 5 minutes. We also set the prediction windows to $1, 2, \dots, 12$ steps by asking the based DNN model to predict traffic information in the next $5, 10, \dots, 60$ minutes. In other words, the base DNN model takes as input sensor data of the last $n$ minutes to predict information in the next $n$ minutes. 


We evaluate the coverage and discrimination guarantees when applying an uncertainty method to each tested DNN model. As explained in Section \ref{sec:qm}, we use MPIW (where smaller values are better) to evaluate the discriminative performance and PICP (where larger values are better) to quantify the coverage of the generated PIs for each test sample. We compute PICP and MPIW by averaging the results across nodes the prediction window settings.
