% Chapter 2

\chapter{Introduction} % Main chapter title

\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

This chapter outlines the background knowledge and core concepts that are required in the subsequent chapters. The first section commences by deriving the core problem of computer graphics starting at the root. Afterwards, we show how the global illumination problem is tackled specifically by radiosity and raytracing.

\section{The Speed - Realism Dichotomy}

A classical image-synthetization process computes how light scattered into an environment translates into pixel colors on a retina.
The color an object should adopt on a virtual sensor can be traced back to the wavelengths absorbed by its surface in relation to the light incident on it, which is turn affected by the light that is reflected, refracted or emitted by other surfaces around it.

Combine this endless recursion with the vast amount of intrinsics this process is subject to, such as physical properties or geometric arrangements, and it quickly becomes clear that a complete, physically accurate light simulation is an unfeasible computational task that needs to be approximated.

Indeed, even highly photorealistic, computationally heavy methods employ a significant amount of approximation and reductionism. The question therefore becomes which simplifications one is willing to make and what their payoff is in computational expense.

The balance of speed vs. realism that underpins this challenge divides it into two distinct problem domains: Whilst some industries, like CG film-making and SFX, are more geared towards realism, other areas have driven an increased demand of faster, more responsive graphics, known under the umbrella term of \textit{real-time rendering}.

\section{Rendering Optics}

The essence of generating images from abstract descriptions can be narrowed down to the simulation of a real-world camera in a virtual environment. As such, we commence by examining the characteristics of virtual cameras as well as the related concepts from \textit{radiometry} that help us model light propagation.

\subsection{Camera Optics}

Most genuine cameras have a series of common denominators arranged in a similar construction:

An aperture allows light to enter through a convex lens, which casts an image onto a light-capturing sensor.
The convexity of the lens ensures that the direction in which light hits the sensor is restricted, thus focusing the image with a limited depth of field, determined by the focal length.

This directional limitation of incident light can also be accomplished without a lens, by severely limiting the size of the camera's aperture, which is the principle of the \textit{pinhole camera} \cite{pinhole_virtual_camera} (as seen in fig. \ref{lens_vs_pinhole}).

\begin{figure}[th]
\centering
\includegraphics[scale=0.3]{Figures/lens_camera.jpg}
\decoRule
\caption[]{Comparison of the optics behind a lens-based and pinhole camera.}
\label{lens_vs_pinhole}
\end{figure}

Pinhole cameras have nearly infinite depth of field and, unlike lens-based cameras, do not suffer from lens distortion (see fig. \ref{pinhole_vs_lens_photo}). However, their minuscule apertures require proportionally lengthy exposure times to produce serviceable photographs \cite{pinhole_virtual_camera}, for which reason pinhole cameras tend to find little to no use in real-life photography \cite{PinholeOptics}.

%NOTE: This image below is from here: https://www.sciencesource.com/archive/Normal-vs--Pinhole-Lens-SS2836731.html
% I've paid the 50$ and have a license/invoice.

\begin{figure}[h]
\centering
\includegraphics[scale=1.15]{Figures/pinhole_vs_lens.jpg}
\decoRule
\caption[]{Normal lens (left) vs. Pinhole lens (right). The pinhole has greater depth of field, but the image sharpness decreases with pinhole size. Image credited to Leonard Lessin/Science Source \cite{ScienceSource}.}
\label{pinhole_vs_lens_photo}
\end{figure}

However, virtual environments are not subject to the same physical constraints, as any numerical value for light can arbitrarily be multiplied by some factor to control for brightness or exposure. As such, the notion of an \textit{exposure time} is not a valid one within a virtual context.

\subsection{Virtual Camera}

A common observation one can make in computer-generated images is that they tend to have unlimited depth of field. This is, indeed, because virtual cameras strongly mimic the simplified optics of a pinhole camera \cite{pinhole_virtual_camera}. 

In reality, increasing the distance between a pinhole and its sensor would produce a weaker image due to inverse-square attenuation. But since brightness factors and exposure times are irrelevant in a virtual context, the pinhole-sensor distance can be entirely discarded.
As such, the sensor can be regarded as being a virtual screen \textit{in front} of the camera \cite{pinhole_virtual_camera}, where light enters through grates corresponding to pixels on the final image. This arrangement is depicted in fig. \ref{virtual_camera}.

By designating the location of the pinhole as the camera's position, we are left with a location vector, a view direction and two \textit{field-of-view} (FOV) angles that are proportional to the height and width of the resulting image respectively \footnote{Most computer graphics domains expand this definition by also including a near and far \textit{clipping plane}, thus forming a \textit{view frustum} \cite{pinhole_virtual_camera}.}.

\begin{equation}
Camera = \{C, \harpoon x, \harpoon y, \harpoon z, \measuredangle_{x}, \measuredangle_{y}\}
\end{equation}

where $C$ is the location of the camera, $\harpoon x$, $\harpoon y$ and $\harpoon z$ are the camera's right, upward and forward directions respectively and $\measuredangle_{x}$, $\measuredangle_{y}$ are the FOV angles for the $x$ and $y$ directions.

In most practical cases, the given directions form an orthogonal coordinate system with $\hat{z} = \hat{x} \times \hat{y}$.

\begin{figure}[th]
\centering
\includegraphics[scale=0.33]{Figures/virtual_camera_optics.jpg}
\decoRule
\caption[]{Optics of a virtual camera. The dimensions of the virtual screen are directly tied to the angles $\measuredangle_{x}$, $\measuredangle_{y}$ making them independent of their distance from $C$.}
\label{virtual_camera}
\end{figure}

\section{Radiometric Quantities}

The process of rendering a virtual object from the perspective of a virtual camera now comes down to measuring the light that the object emits or reflects towards the location of the virtual pinhole $C$.

The direction of incident photons from a given surface cannot be described by a simple 3D vector, as surfaces subtend an infinite amount of directions towards a single point. Thus, it is useful to define a measurement of \textit{solid angle} for the total \textit{field of view} an object occupies towards an observer.

\subsection{Solid Angle}

Regular 2D angles can be adequately represented by the length of the arc they cover on a unit circle. Analogously, solid angles are proportional to the area a surface projects onto a unit sphere around the point of origin \cite{radiosity, fu_cg}.
Solid angles are measured in \textit{steradians} and limited by the total surface area $4\pi$ of a unit sphere.

Let $dA$ be a differential, arbitrarily rotated surface area at point $x'$ with the normal vector $\harpoon n$. The solid angle that $dA$ occupies at another point $x$ can be calculated using two separate operations \cite{radiosity}, both of which are illustrated in fig. \ref{solid_angle_adv}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.355]{Figures/solid_angle_end.jpg}
\decoRule
\caption[Solid Angle]{A surface projected onto a plane results in a surface area of $dA\cos \theta$. Projecting a perpendicular surface towards the center of a sphere results in a projected surface area on the sphere of $dA / \lVert x-x'\rVert^2$. Image from work by Benjamin Kahl \cite{VXCT}.}
\label{solid_angle_adv}
\end{figure}

\begin{itemize}
    \item The surface area $dA$ projects onto a plane perpendicular to $x'-x$ is equal to $dA \cos(\theta)$, where $\theta$ is the angle between $\harpoon n$ and $x'-x$ \cite{radiosity}. This operation accounts for the \textit{rotation} of $dA$ and gives us the surface area of $dA$ that is perpendicular to $x$.
    \item Projecting a perpendicular, differential surface onto a unit sphere around $x$ is simply given by the inverse square of their distance \cite{radiosity}. This operation is given by the \textit{inverse square law} and accounts for the distance between $x$ and $x'$.
\end{itemize}

Combining both these operations into a single formula, will take both rotation and distance into consideration \cite{radiosity, c3}:

\begin{equation}\label{SolidAngleProjectionRelation}
\omega = \frac{\cos(\theta) dA}{\lVert  x'-x \rVert ^2}
\end{equation}

The field-of-view that an object occupies on the sensor of a virtual camera can be quantified through a solid angle. What color the corresponding pixels should adopt now depends on the light that the object emits, reflects or refracts towards the camera, which is given by its \textit{radiance}.

\subsection{Radiance}

\textit{Radiance} describes the radiant energy propagating in a given direction $\harpoon\omega$ at a given location $x$.

Let $p(x, \harpoon\omega, \lambda)$ be the volume density of photons of wavelength $\lambda$ at position $x$ that are travelling in direction $\harpoon\omega$, then the corresponding radiance $L(x, \harpoon \omega)$ equates to the product of said photon density and the energy of a single photon $\frac{hc}{\lambda}$, integrated over all wavelengths \cite{radiosity}:

\begin{equation}
L(x, \harpoon \omega) = \int_{\lambda } p(x, \harpoon \omega, \lambda) \frac{hc}{\lambda}
\label{eqn:RadiancePhotons}
\end{equation}

Since the retinas in human eyes consist of three different types of photoreceptor cones (red, green and blue respectively), pixel colors are usually modelled as 3D vectors with each dimension corresponding to a respective color component. A commonly employed format is the RGB 24-bit color depth format, which assigns each component 8 bits of depth, with another optional 8 bits for transparency in an \textit{alpha channel}.

The overall magnitude of a color-vector is a measure of its overall energy, which corresponds to the radiometric quantity of \textit{flux}.

\subsection{Radiant Flux}

In computer graphics the quantum nature of light (photon density) tends to be discarded in favour of \textit{radiant flux} $\Phi$, which amounts to a general measure of radiant energy per unit time \cite{radiosity, c3}:

\begin{equation}
\Phi = \frac{\partial Q}{\partial t} [W]
\end{equation}

where $Q$ is the energy emitted, transmitted or reflected.

The total flux a surface $A$ emanates is equal to the total radiance all the points on this surface emit in all directions \cite{radiosity}:

\begin{equation}
\Phi_o = \int_{x\in A} \int_{\omega} L(x, \harpoon \omega)
\end{equation}

Respectively, the radiance exiting a surface in a particular direction is the total flux the surface emits per unit solid angle per unit projected surface area \cite{VXCT}:

\begin{equation}
L = \frac{d\Phi}{d\omega dA_{\perp}} = \frac{d\Phi}{d\omega dA\cos \theta}
\label{eqn:Radiance}
\end{equation}

This formulation of radiance is of particular importance, as it acts as a measure of how bright a surface would appear to a camera in direction $\harpoon \omega$ \cite{radiosity}.

Henceforth we will refer to radiance \textit{exiting} a surface point in a certain direction as $L_o(x, \harpoon \omega)$ and radiance \textit{incident} on that point from $\harpoon \omega$ as $L_i(x, \harpoon \omega)$.

Under the assumption that a surface does not emit any light of its own, we can discern that the radiance $L_o$ emitted towards a camera would have to be less or equal to the flux incident on the surface, as per conservation of energy.
The total flux incident on a given surface can be quantified by the value of \textit{irradiance}.

\subsection{Irradiance and Radiant Exitance}

The total flux incident on a surface per unit surface area is termed the \textit{irradiance} $E$ of that surface (sometimes known as \textit{illuminance}) \cite{c3, radiosity}:

\begin{equation}
E = \frac{d\Phi_i}{dA} [W/m^2]
\label{eqn:Irradiance}
\end{equation}

It is equivalent to the radiance incident from all directions in a hemisphere $\Omega$ above the surface \cite{c3, radiosity}:

\begin{equation}
E = \frac{d\Phi_i}{dA} = \int_{\Omega} L_i(\omega) \cos \theta d\omega
\label{eqn:IrradianceExpanded}
\end{equation}

\begin{figure}[h]
\centering
\includegraphics[scale=1.0]{Figures/radio_quant.png}
\decoRule
\caption[]{Visualization of flux, radiance, irradiance and radiant exitance (a-d). Images based on depictions by Jarosz et al. \cite{jarosz12theory}.}
\end{figure}

The opposite of irradiance is the \textit{radiant exitance} $B$, which is defined as the flux per surface
area leaving or being emanated from a surface \cite{Radiometry_Book}:

\begin{equation}
B = \frac{d\Phi_o}{dA} = \int_{\Omega} L_o(\omega) \cos \theta d\omega
\end{equation}

The first law of thermodynamics dictates that energy is neither created nor destroyed. In the context of computer graphics it implies that the radiance reflected by a non-emissive surface must be less or equal to the radiance it receives.

Put differently, the radiance of a point $x$ must be proportional to its irradiance under a coefficient of one or less \cite{radiosity}:

\begin{equation}
dL_o(\harpoon \omega_o) \propto dE(\harpoon \omega_i)
\end{equation}

The coefficient of proportionality that ordains this relationship is given by the \textit{bidirectional reflectance distribution function} of $x$.

\subsection{BRDF and the Reflectance Equation}

For any given pair of differential solid angles (i.e. directions) $\harpoon \omega_i$ and $\harpoon \omega_o$, a material's {\it{bidirectional reflectance distribution function}} (BRDF) defines the ratio of flux concentration per steradian incident from $\harpoon \omega_i$ that is reflected into $\harpoon \omega_o$ \cite{radiosity, fu_cg}:

\begin{equation}
f_r(\harpoon\omega_i \rightarrow \harpoon\omega_o) = \frac{L_o(\harpoon\omega_o)}{E(\harpoon\omega_i)} = \frac{L_o(\harpoon\omega_o)}{L_i(\harpoon\omega_i) \cos \theta d\omega_i}
\end{equation}

If we solve this equation for $L_o$ and perform the same hemispherical integral over the set of all incident directions $\Omega$ as in (\ref{eqn:IrradianceExpanded}), we arrive at the total amount of light reflected by a surface in a specified direction, also known as the {\it{reflectance equation}} \cite{radiosity}:

\begin{equation}\label{eqn:ReflectanceEquation}
\begin{aligned}
& f_r(\harpoon \omega_i \rightarrow \harpoon \omega_o) = \frac{L_o(\harpoon\omega_o)}{E(\harpoon\omega_i)} \\
\Leftrightarrow   & L_o(\harpoon\omega_o) = \int_{\Omega} f_r(\harpoon\omega_i \rightarrow \harpoon\omega_o) L_i(\harpoon \omega_i) \cos \theta_i d\omega_i
\end{aligned}
\end{equation}

Put simply, the reflectance equation prescribes that the radiance $L_o$ a surface reflects in a particular direction $\harpoon\omega_o$ equates to its BRDF weighted irradiance.

\section{The Rendering Equation}

The reflectance equation yields the light a surface point reflects towards a camera. Including a term $L_e(x, \harpoon\omega)$ for emission (light the point $x$ emits itself) provides the total radiance the surface emanates in a direction $\harpoon \omega$ \cite{radiosity} (see fig. \ref{RendEquBase}).
This sum constitutes the \textit{Rendering Equation}, as originally formulated by Kayija et al. in 1986 \cite{Kajiya, radiosity}:

\begin{equation}\label{RenderingEquation_general}
\begin{aligned}
L_o(x, \harpoon\omega) & = L_e(x, \harpoon\omega) + \int_{\Omega} f_r(\harpoon\omega_i, \harpoon\omega, x) L_i(x, \harpoon \omega_i) (\harpoon \omega_i \cdot \harpoon {n_x}) d\omega_i
\end{aligned}
\end{equation}

The rendering equation states that "the transport intensity of light from one surface point to another is simply the sum of emitted light and the total light intensity which is scattered toward $x$ form all other surface points" \cite{Kajiya}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.35]{Figures/rend_eq_base.jpg}
\decoRule
\caption[]{The fundamentals of the rendering equation: $L$ is a weighted integral over all incident directions.}
\label{RendEquBase}
\end{figure}

An alternative formulation of this equation can be derived by replacing the hemispheric integral of directions by an integral over all other surface points \cite{Kajiya, radiosity}.

Let $S$ be the set of all surfaces in the scene, then the irradiance $E$ incident on a given surface point $x$ is the integral over all light leaving any other surface point towards $x$, so long as $x$ and the other point $x'$ are mutually visible. The term of visibility - or occlusion - is given by a function $V(x, x')$ which is equal to 1 if $x$ and $x'$ are mutually visible, 0 if not:

\begin{equation}
E = \int_{S} L_o(x', \harpoon \omega') V(x, x') \frac{\cos \theta \cos \theta'}{\lVert x-x' \rVert^2}dA
\end{equation}

The multiplication of the two cosines $\cos{\theta}$ and $\cos{\theta'}$ accounts for the mutually projected surface area of the two locations and the division by the square of their distance stems from the inverse-square law.

Applying this equation for irradiance to the reflectance equation in (\ref{eqn:ReflectanceEquation}) and adding the emissive component $L_e$, leaves us with the following variant of the rendering equation \cite{Kajiya, radiosity}:

\begin{equation}\label{RenderingEquation_surface}
L_o(x, \harpoon\omega) = L_e(x, \harpoon\omega) + \int_{S} f_r(\harpoon\omega', \harpoon\omega, x) L_o(x', \harpoon \omega') V(x, x') \frac{\cos \theta \cos \theta'}{\lVert x-x' \rVert ^2}dA
\end{equation}

For the sake of simplicity, we will henceforth refer to these individual variants as the \textit{hemispheric-} and \textit{surface-based} rendering equation respectively.

\section{Specular and Diffuse BRDFs}

Materials we encounter in reality tend to be highly granular and possess immense detail on a microscopic scale, which leads to light being reflected in complex distributions of outgoing directions that our simple, computational models cannot fully replicate.

The tendency in computer graphics is to differentiate between three distinct reflectance components: diffuse, specular and glossy \cite{radiosity, VXCT}, as portrayed in fig. \ref{ReflectanceComponents} and fig. \ref{ReflectanceComponent_Render}. These components are weighed in various proportions to one another, depending on the underlying material's physical properties and parameters.

\begin{figure}[th]
\centering
\includegraphics[scale=0.7]{Figures/Ref_components.png}
\decoRule
\caption[]{Reflectance components as part of a BRDF for a given angle of incidence (in red).}
\label{ReflectanceComponents}
\end{figure}

\subsection{Specular Reflections}\label{SpecularReflection}

In a \textit{specular} reflection the incident lights' trajectory is perfectly mirrored across the surface's normal vector.
We can model this behaviour by applying a \textit{dirac delta function} to the angle around the normal \cite{radiosity}:

\begin{equation}
f_r(\harpoon \omega_i \rightarrow \harpoon \omega_o) = \frac{\delta(\cos \theta_i - \cos \theta_o)}{\cos \theta_i} \delta(\phi_i - (\phi_o \pm \pi))
\end{equation}

The values of $\theta$ and $\phi$ correspond to the angles with and around the upward axis respectively. The delta function $\delta$ yields zero for any non-zero parameter.

\subsection{Glossy Reflections}\label{GlossyReflection}

Glossy reflections typically describe specular reflections with a small to moderate amount of scattering and variation. A material's \textit{glossiness} determines the degree of diffusion that occurs.

Mathematically, a broader version of the delta function can be modelled by taking the dot product of two normalized vectors and raising it to some high exponent \cite{learnopengl, VXCT}.

Let $\harpoon R$ be the vector of a perfectly specular reflection and ${\harpoon\omega_o}$ be a vector that points from a surface towards the camera, then the glossy radiance equates to the following \cite{learnopengl, phong, VXCT}:

\begin{equation}
L_o(\harpoon\omega_o) = L_{i}d\omega_{i}(\max(\harpoon R\cdot\harpoon\omega_o), 0)^{\alpha}
\end{equation}

The value of $\max(\harpoon R\cdot\harpoon\omega_o), 0)$ is zero for angles larger than 90° and increases if $\harpoon R$ and ${\harpoon\omega_o}$ are of similar angle to the surface normal. The glossiness $\alpha$ dictates the narrowness of the underlying pseudo-delta function.

\begin{figure}[th]
\centering
\includegraphics[scale=1.27]{Figures/reflectance_components_blender.png}
\decoRule
\caption[]{Reflectance components on a sphere rendered in the Blender 2.8 EEVEE engine.}
\label{ReflectanceComponent_Render}
\end{figure}

\subsection{Diffuse Reflections}\label{DiffuseReflection}

As depicted in fig. \ref{ReflectanceComponents}, \textit{labertian diffuse} reflections are defined as completely isotropic, where the apparent brightness and surface color remain equal for all angles of observation \cite{radiosity}:

\begin{equation}
\forall \alpha, \beta \in \Omega: \quad f_r(x \rightarrow \alpha) = f_r(x \rightarrow \beta)
\end{equation}

The corresponding BRDF $f_r$ thus acts as a constant, since it retains the same value for all parameters.

Replacing the BRDF in the reflectance equation (\ref{eqn:ReflectanceEquation}) by a constant allows for it to be separated from the integrand, providing us with a constant fraction of the irradiance $E$ \cite{radiosity}:

\begin{equation}
\begin{aligned}
L_o(\harpoon\omega_o) & = \int_{\Omega} f_r L_i(\harpoon \omega_i) \cos \theta_i d\omega_i \\
& = f_r \int_{\Omega} L_i(\harpoon \omega_i) \cos \theta_i d\omega_i \\
& = f_r E
\end{aligned}
\end{equation}

Intuitively, this equation describes that if the light incident on a surface gets scattered evenly, then the same fraction of the irradiance is reflected in all directions.

Since genuine surfaces typically absorb a portion of the light incident on them, it is useful to define a measure of \textit{reflectivity} $\rho$ that defines what percentage of irradiance is reflected into radiant exitance \cite{radiosity}:

\begin{equation}
\rho = \frac{B}{E} = \frac{\int_{\Omega_o} L_o(\harpoon\omega_o)\cos \theta_o d\omega_o}{E}
\end{equation}

$L_o$ is constant for all directions and $\Omega$ constitutes a hemisphere, as such, $\rho$ constitutes the BRDF constant multiplied by $\pi$ \cite{radiosity}: 

\begin{equation}
\begin{aligned}
\rho & = \frac{\int_{\Omega_o} L_o(\harpoon\omega_o)\cos \theta_o d\omega_o}{E} \\
& = \frac{L_o \int_{\Omega_o} \cos \theta_o d\omega_o}{E} \\
& = \frac{L_o \pi }{E} \\
& = \pi f_r
\end{aligned}
\end{equation}

Which, in turn, implies that the BRDF for a lambertian diffuse reflection is equal to a multiplication by $\frac{\rho}{\pi}$ \cite{radiosity}:

\begin{equation}\label{equn:diffuse_brdf}
    f_r = \frac{\rho}{\pi}
\end{equation}

Diffuse reflections are the most difficult type of reflections to model accurately, as they require knowing the total light incident for \textit{all} directions.
 
\section{Rasterization}

The rendering equation provides a mathematical model of what brightness and color a given surface adopts for a given observer. To compile this information into an actual image, we require the set of \textit{pixels} on the image that a surface occupies.

In practice, three-dimensional scenes are usually described by a series of simple polygons - known as \textit{primitives} - that span areas between 3D points known as \textit{vertices}.
The conversion process of a primitive into a corresponding set of pixels that it occupies on a screen is known as \textit{rasterization} and forms a vital step in the majority of rendering pipelines. A more detailed account of this process is given in section \ref{rast_pipeline}.

The most common rasterization methods triangulate higher degree polygons and proceed to only rasterize pixels if their center lies completely inside a triangle. \textit{Conservative rasterization} can add some certainty to pixel rendering, as all pixels that are at least partially covered by a rendered primitive are rasterized \cite{directx12_docs} (see fig. \ref{def_vs_const_rast}).

\begin{figure}[H]
\centering
\includegraphics[scale=0.19]{Figures/Def_vs_Cons_Rast.jpg}
\decoRule
\caption[Rasterization]{Default and conservative rasterization in comparison: Default rasterization only rasterizes fragments if their center is covered by the triangle. Conservative rasterization additionally includes all fragments partially covered by the triangle.}
\label{def_vs_const_rast}
\end{figure}

Once the pixels a surface occupies have been determined, a plethora of different lighting models can be applied to compute the color for each pixel individually, such as the \textit{Phong local illumination model} \cite{phong}.
So called \textit{global illumination} models will include indirect light, i.e. light that bounces more than once before reaching the camera (see fig. \ref{gi_vs_li}).

All illumination models are, in essence, an exercise in solving the rendering equation through approximation.

\begin{figure}[H]
\centering
\includegraphics[scale=0.48]{Figures/ill_models_top.jpg}
\decoRule
\caption[Rasterization]{Local illumination (left) vs. global illumination (right) in the Unity 2020.2 Engine. Note in particular the indirect, green light on the sphere and angled surfaces.}
\label{gi_vs_li}
\end{figure}

\section{Raytracing}\label{Raytracing}

The infinite recursion and integration of the rendering equation might appear to have one simple solution: Apply a maximum recursion-depth limit and approximate the hemispheric integral as a finite sum of directions. This exact thought-process lies behind \textit{raytracing}, which has long stood as one the simplest and most intuitive approaches of numerically solving the rendering equation.

The core concept is simple and consists of tracing rays starting from the camera position through each pixel of the virtual screen and then setting the pixel colors based on the surfaces the respective rays collide with \cite{rtx_gems, fu_cg}.
Shooting rays directly from the camera makes a discrete rasterization unnecessary.

Computing a surface's lit color usually involves launching further sets of rays, which depend on the surface's material properties as well as the specific raytracing variant that is being utilized \cite{rtx_gems}.
For instance, if the intersection surface is smooth, a specular ray is cast in the reflection direction and whichever surface this \textit{specular ray} collides with will be mirror-reflected by the former surface. Rough surfaces, on the other hand, require sampling a myriad of rays to account for diffuse light (see fig. \ref{raytracing_example}).

This process occurs recursively, with each intersection shooting its own set of rays until either a light source is hit or a maximum amount of bounces is reached. Whereupon the tree of rays is evaluated bottom-up until, at the root, the pixel's final color can be calculated \cite{rtx_gems}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{Figures/Raytracing_example.jpg}
\decoRule
\caption[Raytracing]{Illustration of the raytracing process with the resulting image. Specular rays are marked in red, diffuse rays in green and initial rays in blue.}
\label{raytracing_example}
\end{figure}

This process is, in essence, a brute-force approach at computing the Riemann sum of the hemispheric rendering equation, as defined in (\ref{RenderingEquation_general}):

\begin{equation}
L_o(x, \harpoon\omega) = L_e(x, \harpoon\omega) + \int_{\Omega} f_r(\harpoon\omega_i, \harpoon\omega, x) L_i(\harpoon \omega_i, x) (\harpoon \omega_i \cdot \harpoon {n_x}) d\omega_i
\end{equation}

Let $I(x, \harpoon \omega)$ be a function that yields the closest surface intersected by a ray along $\harpoon\omega$ starting from $x$. The radiance incident at a surface point $x$ from a given direction $\harpoon\omega$ must be equal to the light exiting the closest surface visible from $\harpoon\omega$ in the reverse direction taken to the inverse square of their distance:

\begin{equation}
    L_i(\harpoon \omega, x) = \frac{L_o(I(x, \harpoon \omega), -\harpoon \omega)}{\lVert x - I(x, \harpoon \omega) \rVert^2}
\end{equation}

Based on this concept, raytracing collapses the hemispherical integral of the rendering equation into a finite sum of directions that are randomly and isotropically selected from the hemisphere:

\begin{equation}\label{RenderingEquation_Raytracing}
\begin{aligned}
L_o(x, \harpoon\omega)
= L_e(x, \harpoon\omega) + \frac{1}{|\Omega|}\sum_{\harpoon\omega_i \in \Omega} f_r(\harpoon\omega_i, \harpoon\omega, x)  L_o(I(x, \harpoon \omega_i), -\harpoon \omega) \frac{\harpoon \omega_i \cdot \harpoon {n_x}}{\lVert x - I(x, \harpoon \omega_i) \rVert^2}
\end{aligned}
\end{equation}

where $\Omega$ is such a finite set of directions. 

Despite being fundamentally simple, raytracing can produce highly photorealistic images because physical effects like refractions or caustics can easily be replicated. However, it does come at an equally high computational cost, as the recursive process can create a colossal amount of rays, all of which have to undergo a vast amount of intersection tests with the scene's geometry.

\subsection{Ray Definition}
\label{RayDefinition}

Rays follow the mathematical description of a three-dimensional half-line constituted by an origin $o$ and a direction $\harpoon d$ \cite{rtx_gems}. In its parametric form, a ray can be described as:

\begin{equation}\label{eqn:ray_common}
    R(t) = o + t\harpoon d \;\;\;\;\text{for } 0\leq t < \infty
\end{equation}

In practice it can be useful to compute cosines between vectors via dot-products, as is done in glossy reflections (see section \ref{GlossyReflection}). To facilitate this process, direction vectors are often restricted to be normalized unit vectors $\hat{d}$ \cite{rtx_gems}.

This restriction also implies that the distance from the origin is directly represented by $t$.
More generally, the difference in $t$ value is equal to the distance between the respective points \cite{rtx_gems}:

\begin{equation}
    \lVert R(t_1) - R(t_2) \rVert = | t_1 - t_2 |
\end{equation}

\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{Figures/Ray_definition_gems.jpg}
\decoRule
\caption[Raytracing]{A ray launched from $o$ to $P$. To avoid precision problems the interval is offset by $\epsilon$. Of the three intersections the first is reported as the \textit{closest hit}. Image based on a depiction from \textit{Raytracing Gems} \cite{rtx_gems}.}
\label{ray_def}
\end{figure}

In raytracing, rays are intersected with a finite amount of geometry to determine collision points. As such, the semi-infinite description of a ray is not practical. Instead, rays frequently are defined with an additional interval that constitutes the range of t-values for which an intersection is useful: $t \in [t_{min}, t_{max}]$. Intersections that lie outside this interval are not reported (see fig. \ref{ray_def}).

The utility of this interval comes in the form of several advantages: Firstly, the minimum value can help prevent self-intersections with the geometry itself that can arise from floating-point inaccuracies \cite{rtx_gems}. Secondly, the maximum value can speed up intersection calculations when hits beyond a certain point do not matter, like with a shadow ray that is shot towards a light-source \cite{rtx_gems}.

\subsection{Ray-triangle Intersection}

Ray-triangle intersection tests are at the heart of the raytracing algorithm. Given a triangle $T$ with the three vertices $V_1$, $V_2$ and $V_3$ as well as a ray $R$ with origin $o$ and direction $\harpoon d$, we seek the intersection point where $R$ is equal to $T$.

One of the most commonly employed intersection algorithms in computer graphics is the \textit{Möller-Trumbore} algorithm \cite{moeller_trumbore}, which functions with minimal memory requirements by defining triangles in their parametric form of their \textit{barycentric coordinates}:

\begin{equation}\label{eqn:Triangle_bary}
    T(u,v) = (1-u-v)V_1 + uV_2 + vV_3
\end{equation}

where the barycentric coordinates $u$ and $v$ must fulfill the conditions $u\geq0$, $v\geq0$ and $u+v\leq1$.

\begin{figure}[th]
\centering
\includegraphics[scale=0.7]{Figures/Moeller_Trumbore.jpg}
\decoRule
\caption[Raytracing]{The system of linear equations produced in Möller-Trumbore algorithm effectively expresses the point of intersection in $t$, $u$, $v$ space. The unit triangle spanned by $u$ and $v$ corresponds to $T$. Image based on depiction by Scratchapixel \cite{moeller_trumbore_image}.}
\label{moeller-trumbore}
\end{figure}

Equating $T$ with the definition for a ray put forth in (\ref{eqn:ray_common}) yields the following expression \cite{moeller_trumbore}:

\begin{equation}
\begin{aligned}
R(t) & = T(u,v) \\
\Longleftrightarrow o + t\harpoon d & = (1-u-v)V_1 + uV_2 + vV_3 \\
\Longleftrightarrow o + t\harpoon d & = V_1 + u(V_2 - V_1) + v(V_3 - V_1) \\
\Longleftrightarrow o - V_1 & = -t\harpoon d + u(V_2 - V_1) + v(V_3 - V_1)
\end{aligned}
\end{equation}

which can be written as a linear system of equations \cite{moeller_trumbore}:

\begin{equation}
    o - V_1 = \begin{bmatrix}-D & (V_2 - V_1) & (V_3 - V_1)\end{bmatrix} \begin{bmatrix}t\\u\\v\end{bmatrix}
\end{equation}

This process is equivalent with expressing the point of intersection in a coordinate system spanned by the $t$, $u$ and $v$ axes, as depicted in fig. \ref{moeller-trumbore}.

There are several methods for solving linear equation systems, but the one employed by Möller and Trumbore is \textit{Cramer's Rule} \cite{moeller_trumbore}.

The resulting $t$, $u$ and $v$ values give both the distance from the ray origin as well as the barycentric triangle coordinates of the intersection. If the conditions from (\ref{eqn:Triangle_bary}) are not met or $t$ lies outside of its defined interval, then no intersection exists \cite{moeller_trumbore}.

Running this algorithm for every triangle in a scene whilst minimizing for $t$ gives us the first - or closest - point that a ray encounters, which corresponds to the function $I$ defined above:

\begin{equation}
\begin{aligned}
I(o,\harpoon d) = o + t\harpoon d \;\;\; & \text{for $t$ being the minimal value in $[t_{min}, t_{max}]$ that satisfies} \\
& \text{$R(o, \harpoon d) = T_i(u,v)$ with} \\
& \text{$u\geq0$, $v\geq0$ and $u+v\leq1$} \\
& \text{for any triangle $T_i$ in the scene}
\end{aligned}
\end{equation}

\begin{figure}[th]
\centering
\includegraphics[scale=0.33]{Figures/BVH_example_rework.jpg}
\decoRule
\caption[BVH]{Simplified illustration of a BVH of a simple 3D scene.}
\label{bvh_example}
\end{figure}

\subsection{Bounding Volume Hierarchies}\label{BVH_theory}

The intersection tests required for tracing a ray can constitute a huge computational endeavour that \textit{acceleration data structures} aim to ameliorate.

Primitives can be grouped into bounding boxes that can in turn be grouped as well, thereby forming a hierarchy \cite{rtx_gems}, as depicted in fig. \ref{bvh_example}. Naturally, if a ray does not intersect a bounding volume it cannot intersect any of the contained primitives.

A \textit{bounding volume hierarchy} (BVH) stores the triangles in the leaves of a tree structure with each node corresponding to a bounding volume. Ray traversal can then commence at the root and progresses into the child nodes, in practice, reducing the time complexity to logarithmic in the number of primitives \cite{rtx_gems, RTX_visibility_koch}.

Other acceleration structures like \textit{binary space partitions} can also be used, but today's consensus is that BVHs are generally the best suited for raytracing, as they guarantee maximum memory usage threshold \cite{rtx_gems}.

\section{Radiosity}

In computer graphics, the term \textit{radiosity} refers to a finite-element approximation of the rendering equation for diffuse reflections. This method has its roots in heat transfer models used in thermal engineering \cite{radiosity_og} and, in contrast to raytracing, is far more adequate for real-time rendering purposes.

In (\ref{equn:diffuse_brdf}) we defined diffuse BRDFs as being constant and irrespective of the angle of observation. This means that, within a static environment, every surface retains its diffuse color no matter what perspective it is being rendered from.
Radiosity makes use of this fact by offloading the computation costs required for global illumination to a phase of pre-computation. Pre-computed values are stored in a so-called \textit{lightmap}, then simply retrieved on demand at minimal cost.

This method can only account for diffuse reflections on static (non-moving) geometry. Any displaced object will require the entire generation process to be repeated, while specular reflections have to be deferred to a different method, such as raytracing \cite{wallace}.

The mathematical basis behind radiosity can be derived from the surface-based rendering equation defined in (\ref{RenderingEquation_surface}):

\begin{equation}
L_o(x, \harpoon\omega) = L_e(x, \harpoon\omega) + \int_{S} f_r(\harpoon\omega', \harpoon\omega, x) L_o(x, \harpoon \omega') V(x, x') \frac{\cos \theta \cos \theta'}{\lVert x-x' \rVert ^2}dA
\end{equation}

Since only diffuse reflections are accounted for, the BRDF $f_r$ can entirely be replaced by the constant $\frac{\rho}{\pi}$ defined in section \ref{DiffuseReflection}. Furthermore, any directional parameters such as $\harpoon\omega$ can be dropped, as lambertian diffuse reflections are equal in all directions \cite{radiosity}:

\begin{equation}
L_o(x) = L_e(x) + \frac{\rho(x)}{\pi} \int_{S} L_o(x) V(x, x') \frac{\cos \theta \cos \theta'}{\lVert x-x' \rVert^2}dA
\end{equation}

Similarly to raytracing, radiosity makes use of the {\it{finite element method}} by subdividing the environment's geometry into a series of small patches, thus collapsing the area-integral into a finite sum \cite{radiosity}. Fig. \ref{raytracing_vs_radiosity} illustrates the two Riemann sums these approaches correspond to.

\begin{figure}[H]
\centering
\includegraphics[scale=0.9]{Figures/Raytracing_vs_Radiosity.png}
\decoRule
\caption[]{Illustration of raytracing (left) and radiosity (right), as depicted by Benjamin Kahl \cite{VXCT}.}
\label{raytracing_vs_radiosity}
\end{figure}

The term $V(x, x') \frac{\cos \theta \cos \theta'}{\lVert x-x' \rVert^2}dA$ is separated into a \textit{view factor} $F_{i,j}$ that describes the fraction of the energy leaving patch $i$ that arrives at $j$. Let $n$ be the amount of surfaces - e.g. patches - in our scene. Then the diffuse radiance of a patch $i$ can be approximated by the following equation \cite{radiosity_og, radiosity}:

\begin{equation}\label{eqn:RadiosityEquation}
L_o(i) = L_e(i) + \frac{\rho(i)}{\pi} \sum_{j=1}^n L_o(j) F_{i,j}
\end{equation}

which constitutes the \textit{Radiosity Equation} as it was formulated in 1984 by Goral et al. \cite{radiosity_og}. The individual components behind this mathematical transformation is illustrated in fig. \ref{raytracing_radiosity_eqn_derivation}.

\begin{figure}[h]
\centering
\includegraphics[scale=0.44]{Figures/add1.jpg}
\decoRule
\caption[]{Approximating the rendering equation through radiosity and raytracing. The inverse square factor is contained within the definition of $L_i$. Image based on a depiction by Benjamin Kahl \cite{VXCT}.}
\label{raytracing_radiosity_eqn_derivation}
\end{figure}

\subsection{View Factor}

Also known as \textit{form factor}, a view factor $F_{i,j}$ describes how well two surfaces $i$ and $j$ are visible to one another and consists purely of geometric parameters.

In the engineering field of \textit{heat transfer}, view factors are calculated through the same geometric term we abstracted from the rendering equation above, integrated for all point-pairs on the two surfaces \cite{FormFactorLecture}:

\begin{equation}\label{eqn:ViewFactorHeatTransfer}
    F_{i,j} = \frac{1}{A_i} \int_{A_i}\int_{A_j} \frac{\cos \theta_i \cos \theta_j}{\lVert x_i-x_j \rVert^2}dA_idA_j
\end{equation}

The division over the surface area $A_i$ results in the \textit{reciprocity rule}, which states that if $A_i$ and $A_j$ are of equal size, then $F_{i,j}$ is equal to $F_{j,i}$ \cite{FormFactorLecture}:

\begin{equation}\label{eqn:ReciprocityRule}
    F_{i,j}A_i = F_{j,i}A_j
\end{equation}

This implies that if both $A_i$ and $A_j$ are known quantities, only half of the form factors need to computed or stored, as each respective mirror pair follows from $F_{i,j} = F_{j,i}\frac{A_j}{A_i}$.

\subsection{The Nusselt Analog}

The numerical computation of form factors is not a wholly simple task, as differential surfaces are difficult to establish. An analog to differential form factors developed by Wilhem Nusselt can provide some useful intuition for the algorithms that follow \cite{radiosity}.

The Nusselt analog corresponds to the same procedures outlined in the section on solid angles (see fig. \ref{solid_angle_adv}), where a patch $A_j$ is projected onto an imaginary unit hemisphere centered at $A_i$ and then orthogonally down onto the base of the hemisphere \cite{radiosity, hemicube}. 
Thus, the view factor equates to the area projected onto the base divided by the area of the base itself.

\subsubsection{The Hemicube Approximation}\label{ViewFactor_Hemicube}

Nusselt's analog illustrates how two differential surfaces that occupy the same solid angle must have the same form factor. Likewise, if a surface is projected radially onto an intermediate surface, such as a hemicube, the form factor of the projection will be the same as that of the original element \cite{radiosity} (see fig. \ref{hemicube_approx}).

This is the justification behind the \textit{hemicube approximation}, devised by Cohen et al. in 1985 \cite{hemicube}.
It approximates the hemisphere with a hemicube, the faces of which are subdivided into small cells.
Once one establishes how many of these cells are occupied by a patch projected onto the hemicube, this results in an amount proportional to the patches' view factor.

\begin{figure}[H]
\centering
\includegraphics[scale=0.28]{Figures/hemicube.jpg}
\decoRule
\caption[]{The justification behind using a hemicube: Patches $A$, $B$ and $C$ have the same view factor, with $D$ corresponding to the Nusselt analog. The size of $B$ can be approximated by the amount of cells it occupies on the hemicube. Depiction based on an image by Watt et al. \cite{wattwatt}.}
\label{hemicube_approx}
\label{hemicube}
\end{figure}

In graphics card programs the cells of a hemicube can easily be encoded as pixels on a \textit{cubemap}. Figuring out which cells a surface occupies then simply amounts to rasterizing said surface into a cubemap from the perspective of the patch (see fig. \ref{hemicube_elias}). Rasterizing all other surfaces alongside it lets us make a full accounting of which surface has what contribution (view-factor) from the current patch, including the visibility term $V$. This process is closely related to \textit{Z-Buffering}, which is described in further detail in section \ref{zbuffering}.

\begin{figure}[ht]
\centering
\includegraphics[scale=0.39]{Figures/Elias_Hemicube.png}
\decoRule
\caption[]{Rendering a scene through a hemicube to determine visible patches and their view factors, giving an overall approximation of irradiance. Image adapted from work by Hugo Elias \cite{HugoElias_Radiosity}.}
\label{hemicube_elias}
\end{figure}

\subsubsection{Monte-Carlo Integration}\label{ViewFactor_MonteCarlo}

An alternative method of determining a view factor is to simply use a randomized sample set of pairs that are uniformly distributed points from each surface. Let $x_{ki}$ and $x_{kj}$ be the $k$'th random pair of  points for the two surfaces $A_i$ and $A_j$ respectively. Then the form factor between $A_i$ and $A_j$ can be computed through a Monte-Carlo approximation of $K$ samples \cite{HierarchalMonteCarloRadiosity} (see fig. \ref{MC_fiewfactor}): 

\begin{equation}\label{eqn:ViewFactor_MonteCarlo}
    F_{i,j} = \frac{1}{K} \sum_{k=1}^{K} A_j \frac{\cos \theta_{ki} \cos \theta_{kj}}{\lVert x_{ki}-x_{kj} \rVert^2} V(x_{ki},x_{kj})
\end{equation}

\begin{figure}[H]
\centering
\includegraphics[scale=0.23]{Figures/Monte_Carlo_ViewFactor.png}
\decoRule
\caption[]{Form factor calculation requires solving a double integral over surfaces of patches. We can instead randomly sample the 4D space that each patch-pair lies in to get an accurate estimate.}
\label{MC_fiewfactor}
\end{figure}

\subsection{Classical Radiosity}

Recall the radiosity equation as defined in (\ref{eqn:RadiosityEquation}):

\begin{equation}
L_o(i) = L_e(i) + \frac{\rho(i)}{\pi} \sum_{j=1}^N L_o(j) F_{i,j}
\end{equation}

Now, where the view factors $F_{i,j}$ have been defined, the radiance values $L_o$ can be formulated as a solution vector, which allows the problem to be entirely expressed as a matrix equation \cite{radiosity, FormFactorLecture}:

\begin{equation}
    \begin{bmatrix}L_o(1) \\ L_o(2) \\ ... \\ L_o(n) \end{bmatrix} = 
    \begin{bmatrix}L_e(1) \\ L_e(2) \\ ... \\ L_e(n) \end{bmatrix} +
    \begin{bmatrix}\rho_1 & 0 & ... & 0 \\ 0 & \rho_2 & ... & 0 \\ ... & ... & ... & ... \\ 0 & 0 & ... & \rho_n \end{bmatrix}
    \begin{bmatrix}F_{11} & F_{12} & ... & F_{1n} \\ F_{21} & F_{22} & ... & F_{2n} \\ ... & ... & ... & ... \\ F_{n1} & F_{n2} & ... & F_{nn} \end{bmatrix}
    \begin{bmatrix}L_o(1) \\ L_o(2) \\ ... \\ L_o(n) \end{bmatrix}
\end{equation}

Through some algebraic transformation (see Cohen et al. \cite{radiosity}), this can formally be written as:

\begin{equation}
    L_o = (I - \rho F)^{-1} L_e
\end{equation}

where $I$ is an identity matrix of size $n\times n$.

Solving this system yields the complete solution to the radiosity equation directly, but requires the entire computational cost to be paid upfront, which becomes prohibitive for larger values of $n$. Instead, it is common practice to solve the equation progressively, with each bounce of light performed separately \cite{Weimar_radiosity}.

\subsection{Progressive Radiosity}\label{ProgressiveRadiosity}

The nature of diffuse reflections combined with the inverse square law implies that lighting values converge rather quickly, which can be leveraged in iterative solutions where each iteration applies the calculations for a single bounce of light. The amount of iterations - e.g. passes - will determine the brightness and fidelity of the scene but also linearly impact the required computation time. Since each subsequent bounce has a lesser impact on the resulting image, fewer than 16 iterations tend to be sufficient in most cases, after which the difference in radiance tends to become non-tangible.

\begin{figure}[h]
\centering
\includegraphics[scale=0.3]{Figures/radiosity.jpg}
\decoRule
\caption[]{Progressive radiosity after a specified amount of additional light bounces, as done by the 2014 VRAD tool and rendered by the 2009 Source Engine. Image from work by Benjamin Kahl \cite{VXCT}.}
\label{radiosity_bounces}
\end{figure}

The prevalence of algorithms such as \textit{instant radiosity} \cite{instant_radiosity} and \textit{voxel cone tracing} \cite{Crassin} imply that, frequently, a mere two bounces tend to be sufficient for an adequate approximation of indirect light (see fig. \ref{radiosity_bounces}). Progressive radiosity can be halted after any iteration, once a desired solution has been reached. 

Standard iterative methods for solving matrix equations include the \textit{Jacobi iteration} and the \textit{Gauss-Seidel method} \cite{FormFactorLecture}. The solution can also be configured as a \textit{shooting} or \textit{gathering} variant, depending on which patches are processed in the algorithms outermost for-each loop \cite{gathering_shooting}.

A generalized pseudo-code of progressive radiosity (adapted from Wüthrich \cite{Weimar_radiosity}) is listed below:

\begin{algorithm}[H]
    \caption{Progressive Radiosity}
    \begin{algorithmic}[1]
      \For{each iteration}
        \For{$A_i \in S$}
            \For{$A_j \in S$}
                \State Calculate or retrieve $F_{i,j}$
                \State Update radiosity of $A_j$
                \State Update emission of $A_j$
            \EndFor
            \State Set emission of $A_i$ to 0
        \EndFor
    \EndFor
    \end{algorithmic}
\end{algorithm}

Henceforth, we will refer to this algorithm as \textit{pure} progressive radiosity, to differentiate it from its variants that employ additional enhancements to improve performance.

\subsection{Progressive Refinement Radiosity}
\label{ProgressiveRefinementRadiosity}

An extension to pure radiosity, \textit{progressive refinement radiosity}, was first introduced by Cohen et al. in 1988 \cite{progressive_refinement}. This reformulation of the original algorithm eliminates the memory requirements for view factors entirely by computing them on-the-fly. Patches are processed in sorted order according to their energy contribution to the environment and then updated simultaneously after each pass.

More importantly is the use of \textit{refinement} through adaptive subdivision, which had already been introduced by Cohen et al. in 1986 \cite{adaptive_subdivision}. This process dynamically sub-divides or merges individual radiosity patches depending on the gradient across them. The resulting \textit{quad-tree} will have more leaves in places of relevance, such as the boundary of a shadow, whilst treating, flat mono-colored surfaces as single patches (as depicted in fig. \ref{adaptive_subdivision_scene}).

\begin{figure}[th]
\centering
\includegraphics[scale=1.3]{Figures/AS_Example.png}
\decoRule
\caption[]{Radiosity with adaptive subdivision. More patches are allocated to areas with a high gradient, such as shadow boundaries. Image based on a concept by Coombe et al. \cite{RadiosityOnGPUs_Coombe}.}
\label{adaptive_subdivision_scene}
\end{figure}

Refinement in general is not exclusive to progressive radiosity and can be performed in a number of varieties. Most techniques operate \textit{a posteriori}, meaning they adjust the amount of patches based on the output of each iteration. These are commonly categorized as follows (as done by Slusallek et al. \cite{FormFactorLecture}):

\begin{itemize}
    \item \textit{r-refinement:} Repositions vertices of a mesh based on the lighting gradient.
    \item \textit{h-refinement:} Stores lighting data in a quad-tree and subdivide each node depending on a gradient threshold.
    \item \textit{p-refinement:} Increases polynomial order of patches depending on a gradient threshold.
    \item \textit{remeshing:} Re-computes an entirely new mesh, with edges and vertices aligned along shadow boundaries.
\end{itemize}

The primary goal behind refinement is to drive down the need for large patch amounts, thus improving performance.

Some solutions (see Coombe et al. \cite{gpu_gems_2005}) model the patches that are sampled (i.e shot towards) as separate sub-set of the set of all patches in scene. This allows one to maintain a large set of evenly distributed patches, whilst only sampling the most important ones, in accordance to a subdivided quad-tree.

\subsection{Instant Radiosity and Sampling Approaches}\label{instant_radiosity_section}

To complement radiosity's slow rate of convergence and static geometry constraints, \textit{Instant Radiosity} was introduced by Keller in 1997 \cite{instant_radiosity}. It approximates global illumination effects by creating additional, \textit{virtual light-sources} on inter-reflecting surfaces, making it perfectly fit to be used within real-time requirements \cite{instant_radiosity_2}.

\begin{figure}[th]
\centering
\includegraphics[scale=0.25]{Figures/InstantRadiosity.jpg}
\decoRule
\caption[]{Concept behind instant radiosity. Rays are scattered from a light-source. On their points of collision we place a new, virtual point-light. The entire scene (including virtual point-lights) can then be rendered using a model like phong.}
\label{instant_radiosity_concept}
\end{figure}

Rays are shot in random directions from light-sources. Then, at their intersection locations with other geometry, \textit{virtual point lights} (VPLs) are created that emit light corresponding with the underlying surface's color and brightness (see fig. \ref{instant_radiosity_concept}). The process can be repeated for each of these light-sources recursively, with the amount of virtual point-lights ultimately determining the quality of global illumination.

Instant radiosity does not require costly pre-computations and can accommodate dynamic scene changes on demand, but generally does not produce the same lighting and shadow quality as regular radiosity, and requires significant amount of GPU power to run in real-time. \textit{Incremental} instant radiosity \cite{instant_radiosity_aalto} allows incrementally adding new VPLs over time, whilst maintaining an even distribution of VPLs across the scene.

Instant radiosity can loosely be classified as a \textit{directional sampling} approach. In regular radiosity, visibility is determined separately for all possible patch-pairs, which results in a complexity of at least $O(n^2)$ in the number of patches. In directional sampling approaches, we isotropically scatter a set of rays 
for each patch, then calculate lighting contribution for each patch hit by a ray. In theory this drops the complexity to $O(n*m)$, where $m$ is the maximum amount of samples taken for each patch.

\section{Visibility Determination}

The surface-based rendering equation in (\ref{RenderingEquation_surface}) defines a function $V(x, x')$ that is equal to 1 if no other geometry lies between the two points $x$ and $x'$, otherwise 0.

Determination of visibility has been a cornerstone problem in computer graphics from its very beginning.
Conundrums such as the \textit{art gallery problem}, \textit{watchman route problem} or graph visibility provide great insight into the theoretical perspective in that the underlying issue is NP-hard \cite{Watchman_Route}.

A common solution in computer graphics is to leverage the celerity of rasterization pipelines to calculate approximate visibility through a Z-Buffer. This method is frequently employed for the calculation of shadows, whilst a binary-space partition (BSP)\footnote{Sometimes referred to as Portal-Engine / Portal rendering} restricts the selection to only relevant geometry \cite{BittnerVisibility}.

\begin{figure}[th]
\centering
\includegraphics[scale=0.75]{Figures/zBuffer.png}
\decoRule
\caption[]{Rasterization of basic shapes (left) and respective z-buffer (right).}
\label{zbuffer}
\end{figure}

\subsection{Z-Buffering}\label{zbuffering}

A z-buffer (or depth buffer) consists of the clip-space z-coordinates for every pixel of a rendered primitive. These values correspond to the "image depth" or the distance of the painted geometry to the camera \cite{fu_cg} (see \ref{zbuffer} for an example). Z-buffers are regular by-products of the rasterization process.

To determine whether an object is visible to a certain location, a scene can be rasterized from the perspective of the given point, as described by the hemicube approximation (see \ref{ViewFactor_Hemicube}). Instead of the object's colors, it is sufficient to simply store a unique ID for each object into the pixels of the raster image. The result will contain the IDs of all visible objects, barring translucent or small surfaces that occupy less than a pixel \cite{ProducerScroungers, fu_cg}. 

This method is frequently used to generate shadows in real-time, although a low render resolution can impair the quality  and can lead to shadow pixelation (see fig. \ref{pixelated_shadows}).

\begin{figure}[th]
\centering
\includegraphics[scale=0.55]{Figures/pixelatedShadows.jpg}
\decoRule
\caption[]{Red cuboid with a pixelated shadow as a result of z-buffering (Unity Engine 2021.3.21f).}
\label{pixelated_shadows}
\end{figure}

\subsection{Raytracing for Visibility}\label{RaytracingForVisibility}

Instead of translating a 3D location to a screen position, we can translate a screen position to a 3D location by tracing a ray through the corresponding pixel in the image plane. Likewise, two locations $x_1$ and $x_2$ are mutually visible if a ray launched from $x_1$ can reach $x_2$ unimpeded:

\begin{equation}
    V(x_1, x_2) \leftrightarrow I(x_1, x_2 - x_1) = x_2
\end{equation}

Whilst z-Buffering can be very quick in computing the visibility of several objects from a single location (like with shadows of a point-light), it is not as well-suited for the visibility computation of large sets of arbitrary point-pairs, as is required by radiosity.

