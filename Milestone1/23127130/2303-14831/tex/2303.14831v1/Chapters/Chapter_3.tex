% Chapter 1

\chapter{The Turing Architecture and DXR} % Main chapter title

\label{Chapter3} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------

In August 2018 the multinational tech company \textit{Nvidia} introduced the first consumer products capable of genuine real-time raytracing in the form of the \textit{GeForce 20 series} of GPUs.

Built on a newly developed \textit{Turing microarchitecture}, these chips subsume several different types of specialized processor cores to accelerate their respective tasks considerably \cite{turing_whitepaper}. The so-called \textit{RT core} is designed specifically to process the traversal of bounding volume hierarchies and triangle intersection tests, thereby enabling Turing GPUs to execute rudimentary raytracing algorithms at a rate of several billions of rays per second \cite{rtx_gems, turing_whitepaper}.

In this chapter we will review the exact makeup of this architecture and what its key enablers are for the significant boost in graphics performance. Additionally we will outline its embedded solution for real-time raytracing and how it can be used through the DirectX 12 raytracing pipeline.

\section{GPUs and Parallelism}

GPUs (graphics processing units) can be generally defined as specialized processing units designed for the quick computation and management of visual data in a frame buffer.

In essence, their primary intended task is to continuously compute 2D matrices of color values that represent pixels on a screen.
For most intents and purposes (excluding post-processing effects such as anti-aliasing, bloom or blurring), the color of any one individual pixel is independent of its predecessor and neighbours. As such, this task is highly adequate for parallelization.

\begin{figure}[th]
\centering
\includegraphics[scale=0.5]{Figures/gpu-devotes-more-transistors-to-data-processing.png}
\decoRule
\caption[]{General architecture of a CPU (left) vs. a GPU (right). Image adapted from the Nvidia CUDA programming guide 2022 \cite{cuda_guide}.}
\label{fig:cpuvgpu}
\end{figure}

Fig. \ref{fig:cpuvgpu} shows the generalized differences in architecture between CPUs and GPUs: The latter contain hundreds to thousands of cores/ALUs (arithmetic logical units) that can run a large set of lightweight threads simultaneously, whilst CPUs are geared towards a small number of highly efficient, general-purpose cores and fast access to system memory.

Due to their immense power in parallel computing, GPUs have found use in many other areas than graphics, such as machine learning and cryptography.

\section{The Turing Architecture}

The Turing GPU microarchitecture (named after \textit{Alan Turing}) was introduced in August 2018 \cite{turing_whitepaper} and is the architecture that Nvidia's newest range of consumer-grade graphics processors are built upon. 

Turing cards inherit large parts of their design from earlier microarchitectures, namely the 2010 \textit{Fermi} \cite{FermiArchitecture} architecture and its successors (\textit{Kepler (2012)}, \textit{Maxwell (2014)} and \textit{Pascal (2016)}).
Turing's key enabler for its new features and improved performance is the overhauled GPU processor which accommodates "improved shader execution efficiency, and a new memory system architecture that includes support for the latest GDDR6 memory technology"\cite{turing_whitepaper}.

\subsection{TU102 GPU Structure}

The TU102 is a a high-end GPU of the GeForce 20 series and is divided into six Graphics Processing Clusters (GPCs) each of which contains a hybrid makeup of computational units \cite{turing_whitepaper}. Below we provide a description of its primary components in line with the \textit{Turing architecture whitepaper} \cite{turing_whitepaper}.

The GPU receives its instructions and data through a PCIe 3.0 interface, which connects it to the rest of the computer and acts as the main access point to the systems main memory \cite{turing_whitepaper}.
Each GPC comes equipped with its own L1 cache, whilst all six GPCs share twelve memory controllers and an additional 512kb of L2 cache \cite{turing_whitepaper}.

An onboard, chip-level \textit{GigaThread Engine} receives command queues from the host via the PCIe bus and executes these by setting up shaders on available hardware \cite{FermiArchitecture}. This is the central command processor that manages the entire chip, including context switches, scheduling and power management \cite{FermiArchitecture}.

\subsection{GPC}

Each GPC, as depicted in fig. \ref{GPC_overview}, houses six \textit{Texture Processor Clusters} (TPCs), which are nested sub-clusters in of themselves, as well as a dedicated raster engine \cite{turing_whitepaper}. The GPC/TPC subdivision is useful for distributing and allocating of the computational workload evenly, as they can be managed as a single unit \cite{turing_whitepaper}.

Each TPC consists of two Streaming Multiprocessors (SM), which the parent TPC can wake and sleep based on how heavy the incoming workload is \cite{turing_whitepaper}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.42]{Figures/Turing_GPC.png}
\decoRule
\caption[]{Architecture of the TU102: 6 GPCs, each consisting of 6 TPCs that contain 2 SMs each. Each SM contains a number of ALUs for integer, floating point and matrix operations as well as a single RT core. Each RT core has a separate core for BVH traversal and triangle intersections respectively. Image adapted from the Turing architecture whitepaper \cite{turing_whitepaper}.}
\label{GPC_overview}
\end{figure}

\subsection{SM - Streaming Multiprocessor}

The streaming multiprocessor houses all the primary computation units of the GPU itself.
Each one contains its own onboard memory in the form of a 256KB register file, four texture units and 89 KB of L1 shared memory which can be dynamically allocated depending on the workload \cite{turing_whitepaper}.

Each SM contains three different types of processing cores that represent the GPU's entire computational capacity \cite{turing_whitepaper}:

\begin{itemize}
    \item 64 CUDA cores (4608 total)
    \item 2 Tensor cores (576 total)
    \item 1 RT core (72 total)
\end{itemize}

All of these components can be identified on the GPU die itself, as shown in fig. \ref{turing_die}.

\begin{figure}[th]
\centering
\includegraphics[scale=0.3]{Figures/Turing_dye.jpg}
\decoRule
\caption[]{Turing GPU die. Image adapted from the Turing architecture whitepaper \cite{turing_whitepaper}.}
\label{turing_die}
\end{figure}

\subsubsection{CUDA Core}

CUDA cores are basic computational units that perform common integer- or floating point operations \cite{cuda_guide, turing_whitepaper}. These can be used both for rendering but also other parallel computing purposes through the CUDA platform\footnote{Note that we are using the term "CUDA core" as an umbrella term for a GPUs general purpose ALUs. Whilst CUDA is technically an Nvidia-exclusive platform, all GPUs are equipped with equivalent computational units.}.

Unlike its predecessors, the Turing architecture provides separate data-paths for integer and floating-point operations. In previous generations the execution of an integer-based instruction would have blocked floating-point instructions from issuing \cite{turing_whitepaper}.

\subsubsection{Tensor Core}

Tensor cores are specialized execution units that are built specifically and purely to accelerate the process of matrix (or tensor) multiplication \cite{TensorCores}. These find heavy usage in vertex transformation operations of common rendering applications but also for deep learning (neural network) purposes.

Turing's tensor cores have been enhanced for inferencing and equipped with additional integer precision modes \cite{turing_whitepaper}.

\subsubsection{RT Core}

One of the biggest innovations of the Turing architecture is the introduction of \textit{RT cores}, a specialized processing core that exclusively performs BVH traversal and ray-triangle intersections to facilitate real-time raytracing \cite{turing_whitepaper}. The underlying acceleration concepts are not new; parallelization and BVHs have been common practice in raytracing for some time.

But unlike previous raytracing solutions built on the GPU, RT cores can run autonomously, in parallel to the CUDA cores and thus offload the SM \cite{turing_whitepaper}. This means that when a shader dispatches a ray, it can continue performing other calculations whilst the ray is being traced in parallel. Similarly, once a ray intersects geometry, the RT core can directly move on to tracing the next ray, whilst the CUDA and tensor cores take care of shading and lighting. The horizontal parallelism this concept provides is demonstrated in fig. \ref{pascal_vs_turing}.

RT cores consist of two different units: one for bounding box testing and a second for ray-triangle intersections \cite{turing_whitepaper}. This distinction allows for further horizontal parallelism, as the first unit can move on to the next triangle/input whilst the second unit still finishes the previous input. 

\begin{figure}[th]
\centering
\includegraphics[scale=0.31]{Figures/TuringvsPascal_RT.jpg}
\decoRule
\caption[]{Software emulated raytracing on Pascal GPUs (left) vs. hardware accelerated raytracing on Turing GPUs (right). Image in line with the depictions in the Turing whitepaper \cite{turing_whitepaper}.}\label{TuringVsPascalRT}
\label{pascal_vs_turing}
\end{figure}

Pascal GPUs have retroactively been made compatible with RTX through the DirectX raytracing API. But since no RT cores are available on these, a software approximation that runs on CUDA cores is employed instead (see fig. \ref{TuringVsPascalRT}). This results in significant performance degradation. Turing GPUs can process 10+ Giga-Rays per second, whilst high end Pascal GPUs only reach approximately 1.1 Giga-Rays per second \cite{turing_whitepaper}.

The general umbrella term for Nvidias real-time raytracing technology is \textit{RTX}, which is an abbreviation for \textit{Ray Tracing Texel eXtreme}.

\section{DirectX}

Rendering applications typically interact with GPU hardware through graphics APIs that span across multiple programming languages and platforms. These describe an abstract programming layer that specifies exactly what result, input and output of each function ought to be.
Minor variations in how computation is performed on a hardware-level, can produce divergent results even when a program is executed on the same API\footnote{For instance, \textit{Qualcomm Adreno 2xx} processors use 24 bit floating precision in fragment shaders, whilst the Nvidia X1 uses the more common 32 bit precision. The OpenGL 4.6 specification states that it "does not guarantee an exact match between images produced"\cite{opengl_46_spec}.}.

The most notable APIs are \textit{OpenGL}, \textit{Vulkan} and \textit{DirectX}, which have all seen industry-wide adoptions across most common graphics cards and applications \cite{ocg_api_comparison}.

Upon their release, Turing's raytracing features were only exposed through DirectX 12, later becoming available on Vulkan and (partially) OpenGL as well \cite{rtx_vulkan_opengl}. Since the compatibility has most matured on DirectX, the remainder of this thesis will focus primarily on DirectX 12.

\subsection{DirectX Rasterization Pipeline}\label{rast_pipeline}

When a scene is rendered into a frame output buffer using rasterization, DirectX employs a programmable graphics pipeline, which sequentially executes a series of highly specialized, fixed-function steps that allow the system to efficiently draw abstract 3D geometry in a given perspective. Each of these steps executes a concrete task with the output of the previous step as its input parameters. Given the consecutive nature of the pipeline, the individual steps can easily be executed in parallel for successive frames to be rendered.

The GPU programs that can be inserted in-between these steps are commonly termed \textit{shaders} and let developers configure custom pipeline behaviour. In a DirectX context, shaders are written in a \textit{high-level shader language} (HLSL), which strongly resembles the syntax used in C-based languages.

Fig. \ref{directx_rast_pipeline} shows a simplified overview of the individual stages that form the DirectX rasterization pipeline (as described in the DirectX 12 documentation \cite{directx12_docs}).

\begin{figure}[th]
\centering
\includegraphics[scale=0.285]{Figures/DirectX_Pipeline.jpg}
\decoRule
\caption[]{Stages of the DirectX 12 rasterization pipeline. Programmable shaders are marked in green. Image based on information from the DirectX12 documentation \cite{directx12_docs}.}
\label{directx_rast_pipeline}
\end{figure}

\subsubsection{Input Assembler Stage}

The Input Assembler Stage reads geometry data from the allocated buffers and assembles it into primitives (usually triangles) that are usable by the other pipeline stages \cite{directx12_docs}. This stage will also attach system-generated values, such as primitive IDs, instance IDs or a vertex IDs, so that subsequent shader stages can reduce processing to only instances, primitives or vertices that have not already undergone processing \cite{directx12_docs}.

The primitives generated by the input assembler stage are subsequently transferred to the vertex shader.

\subsubsection{Vertex Shader}

The vertex shader is the first and arguably most important geometry processing step in any graphics pipeline.
It is an input-output program that is executed on every vertex individually and lets the user transform, modify or otherwise set up vertex-specific data for later pipeline stages \cite{learnopengl}.

The most common operations performed in the vertex shader are the applications of the model-, view- and projection- matrices \cite{fu_cg, learnopengl}. To conserve memory, 3D model data is stored in local coordinates so each instance of any 3D model in a scene comes with a model matrix that describes the object's position, rotation and scale. The model matrix is a 4x4 affine transformation matrix that converts vertex positions from local space into world space.

\begin{figure}[th]
\centering
\includegraphics[scale=0.5]{Figures/coordinate_systems.png}
\decoRule
\caption[]{Coordinate system transformations in the vertex shader, as pictured by Joey de Vries \cite{learnopengl}.}
\label{coord_systems}
\end{figure}

The view matrix will thereafter transform the coordinates into view space and the projection matrix applies the FOV-based distortion of non-orthographic cameras to each vertex. The final vertices find themselves in a $[-1, 1]$-ranged clip space coordinate-system, where vertices outside this range lie outside the camera's view frustum. The entire transformation process is depicted in fig. \ref{coord_systems}.

Furthermore, additional data-points such as a vertex's texture coordinates and normal vector are likewise set up in the vertex shader. By the end of this stage, each vertex will possess its own set of data known as \textit{attributes}, which subsequent pipeline stages linearly interpolate on for respective values anywhere on the spanned primitive.

\subsubsection{Tessellation Stage}

The tessellation stage is an optional stage added in DirectX 12, which allows the user to generate additional vertices directly on the GPU \cite{directx12_docs}. The utility of this stage plays no importance in remainder of this thesis.

\subsubsection{Geometry Shader}\label{GeometryShader}

Another optional step that allows for additional processing is the geometry shader, which is executed on a per-primitive basis.
For triangles, each set of three vertices relayed by the vertex shader are passed into the geometry shader as a triplet, which lets the user remove, subdivide or otherwise transform them in a manifold of ways \cite{directx12_docs, learnopengl}.

This stage is frequently used for triangle-based effects like enlargements or shrinking (see fig. \ref{geom_example}) as well as a vital part for GPU-based voxelization \cite{VXCT, crassin_voxelization1} (see \ref{Voxelization}).

\begin{figure}[th]
\centering
\includegraphics[scale=0.4]{Figures/geom_shad.jpg}
\decoRule
\caption[]{Example: Shrinking triangles in the geometry shader. Image by Bailey \cite{geom}.}
\label{geom_example}
\end{figure}

\subsubsection{Rasterizer Stage}

As implied by its name, the rasterizer stage converts each primitive into the set of pixels that it occupies on-screen, whilst interpolating the per-vertex attributes across it, so that each pixel has a corresponding set of values for normal vectors, positions, texture coordinates etc.

If the pipeline is set to utilize multi-sampling (compute several color values per pixel), the individual sub-samples are likewise arranged here \cite{directx12_docs}.
This stage additionally discards any pixels that do not face the camera (face culling), are occluded by other objects (depth clip) or are outside the viewport (scissor clip) \cite{directx12_docs}. This limits the amount of pixels - i.e. fragments - that need to be processed in the next stage.

\subsubsection{Pixel Shader}

The pixel shader (known as \textit{fragment shader} in OpenGL) is executed once for each pixel provided by the rasterization step \cite{directx12_docs, learnopengl}. This is typically the most performance-intensive stage, as it is executed most frequently and includes the vital lighting calculations that produce the final color of each pixel.

Mathematical models like \textit{Phong} \cite{phong} can be used to compute the pixel brightness and color composition equating to the radiance emanated from the fragment position towards the camera. The input data for these will be a position-based linear interpolation between the vertex attributes of each triangle. The factors of this interpolation can be equated with the fragment's barycentric coordinates on the triangle.

\subsubsection{Output Merger Stage}

The final stage of the pipeline utilizes any present depth/stencil buffers to perform depth-testing and blending of transparent objects (alpha testing) with the colors provided by the pixel shader to generate the final image \cite{directx12_docs}.

If the image is being rendered into multiple render targets (DirectX 12 supports up to 8 \cite{directx12_docs}), the writing process is likewise handled by the output merger stage.

\section{DirectX Raytracing}

Since the raytracing procedure fundamentally differs from classic rasterization, DirectX introduces an entirely new graphics pipeline with a whole new set of programmable shaders to accommodate it. 

\textit{DirectX Raytracing}, or simply \textit{DXR}, shares several similarities with its rasterization counterpart, as it aims to strike a balance between fixed-function and programmable stages to maximize both execution efficiency and potential for customization \cite{rtx_spec_ms}.

\subsection{DXR Pipeline}

The system is intended to process rays independently and in parallel. Once a ray hits or misses, it can create further sub-rays, but generated rays that are in-flight can never be dependent on each other \cite{rtx_spec_ms}.

\begin{figure}[th]
\centering
\includegraphics[scale=0.75]{Figures/rtx_pipeline_simple.jpg}
\decoRule
\caption[]{Simplified overview of RTX pipeline stages. Depiction based on an image from the Microsoft DXR Specification \cite{rtx_spec_ms}.}
\label{rtx_pipeline_simple}
\end{figure}

On a surface level, the raytracing pipeline can be divided into three core components: Ray generation, BVH traversal and shading (see fig. \ref{rtx_pipeline_simple}).
Scheduling functionalities of which ray or shader is processed in which order is an opaque, hardware-bound process and cannot be altered \cite{rtx_spec_ms}. Likewise, the BVH traversal is treated as a single, fixed-function step that is offloaded to the RT cores. In their examination of RTX performance, Sanzharov et al. \cite{RTX_examination} concluded that RTX does some ray grouping and sorting during the GPU work creation process, in order to speed up bundles of rays traversing through the same BVH leaves.

\subsubsection{Rays in DXR}\label{RaysInDXR}

The data structure that represents a ray in DXR closely follows the definition put forth in section \ref{RayDefinition}, consisting of an origin, direction and a min-max distance interval \cite{rtx_gems}. Each of these values needs to be initialized before the ray can be traced:

\begin{lstlisting}
struct RayDesc
{
    float3 Origin;
    float3 Direction;
    float TMin;
    float TMax;
};
\end{lstlisting}

Furthermore, rays in DXR can carry a \textit{payload}, which is a custom data-structure of limited memory that can be accessed on a per-ray basis by any of the programmable shaders \cite{rtx_gems}. An example of how this payload may be utilized is given in section \ref{RTX_code_example}.

\subsubsection{Programmable Shaders}

The DirectX raytracing pipeline introduces a total of five new programmable shader types that are invoked during the raytracing process based on the flow diagram depicted in fig. \ref{rtx_pipeline_detailed}.

\begin{figure}[th]
\centering
\includegraphics[scale=0.38]{Figures/DXR_RT_Pipeline.jpg}
\decoRule
\caption[]{Overview of the DXR pipeline as depicted in the \textit{Raytracing Gems} book \cite{rtx_gems}. It includes the five programmable shader types marked in orange. BVH traversal is marked by a grey outline.}
\label{rtx_pipeline_detailed}
\end{figure}

Below we list a brief description for each one in line with how their documentation in \textit{Ray Tracing Gems} \cite{rtx_gems}.

\begin{itemize}
    \item The \textit{ray generation} shader is launched once for every instance of some enumerable index (1D, 2D or 3D grid) and handles the initial ray launches \cite{rtx_gems}. In a traditional raytracing application, this shader would launch the initial rays from the camera through each virtual pixel. An HLSL \verb|TraceRay(...)| function is available for this purpose.
    
    \item \textit{Intersection shaders} define how intersections are calculated with arbitrary primitives \cite{rtx_gems}. By using intrinsic functions like \verb|ReportHit()| or \verb|AcceptHitAndEndSearch()| the user can define which intersections are counted as hits or not. If no intersection shader is provided, the pipeline employs a high-performance default that uses triangles \cite{rtx_gems}. Utilizing intersection shaders instead of the build-in ray-triangle intersection is less efficient but offers far more flexibility \cite{rtx_spec_ms, rtx_gems}.
    
    \item As the intersection shader defines which hits to report and which not to, \textit{any-hit shaders} are executed for all reported hits \cite{rtx_gems}. In addition to running regular HLSL code (like writing data into textures on hit) any hit shaders allow otherwise valid intersections to be discarded, such as transparent surfaces.
    
    \item As the name implies, \textit{closest-hit shaders} are executed at the closest intersection for each ray \cite{rtx_gems}. In a traditional raytracing context, this would recursively launch additional rays to compute the color the hit location has.
    
    \item If no hit is registered for a ray, the \textit{miss shader} is executed \cite{rtx_gems}. This can be used, for instance, to display a background color for pixels not occupied by geometry.
\end{itemize}

This pipeline shares many components and concepts with regular raytracers. Algorithm \ref{DXR_Raytracing} illustrates how a standard DXR pipeline would function if it were executed in sequence.

\begin{algorithm}
    \caption{DXR Raytracing Process (Adapted from Raytracing Gems \cite{rtx_gems})}\label{DXR_Raytracing}
    \begin{algorithmic}[1]
      \For{$x, y \in image.dimensions()$}
        \State $ray \gets createRay(x,y)$\Comment{Ray from $C$ through pixel $(x, y)$}
        \State $closestHit \gets null$
        \While{$leaf \gets findBvhLeadNode(ray, scene)$}\Comment{BVH traversal}
            \State $hit \gets intersectGeometry(ray, leaf)$\Comment{\textit{intersection} shader}
            \If{$isCloser(hit, closestHit) \And isOpaque(hit)$}
                \State $closestHit \gets hit$
            \EndIf
        \EndWhile
        \If{closestHit}
            \State $image(x,y) \gets shade(ray, closestHit)$\Comment{\textit{closest-hit} shader}
        \Else
            \State $image(x,y) \gets miss(ray)$\Comment{\textit{miss} shader}
        \EndIf
      \EndFor
    \end{algorithmic}
\end{algorithm}

\subsection{TraceRay Function}

The DXR-intrinsic \verb|TraceRay(...)| function can be used in programmable shaders to commence a raytracing process on an RT core. It operates under the following parameters \cite{rtx_spec_ms}:

\begin{lstlisting}
TraceRay(
    RaytracingAccelerationStructure AccelerationStructure,
    uint RayFlags,
    uint InstanceInclusionMask,
    uint RayContributionToHitGroupIndex,
    uint MultiplierForGeometryContributionToHitGroupIndex,
    uint MissShaderIndex,
    RayDesc Ray,
    inout payload_t Payload
);
\end{lstlisting}

\begin{itemize}
    \item The first parameter selects the BVH containing the geometry that needs to be traced. In most instances only one BVH of a scene exists, but multiple ones can be defined to trace on different scenes from a single shader.
    
    \item The second parameter is an integer where each bit represents a certain flag that affects ray behaviour. Some notable flags are \begin{spverbatim}RAY_FLAG_CULL_BACK_FACING_TRIANGLES\end{spverbatim} to disregard intersections on triangles not facing the ray, or \begin{spverbatim}RAY_FLAG_ACCEPT_FIRST_HIT_AND_END_SEARCH\end{spverbatim} to terminate the BVH traversal immediately when any geometry is hit.
    
    \item The third parameter is an instance mask that allows skipping geometry on a per-instance basis. Passing a value of \verb|0xFF| (in hexadecimal) would cause all geometry to be tested for intersections, whilst \verb|0x00| would test none.
    
    \item The fourth and fifth parameters define the hit-group for this ray. A hit group consists of an intersection, closest-hit and anyhit shader. Typically each ray-type is separated into a respective hit-group so that the respective shader code is executed. (For instance, shadow-rays, diffuse reflections and specular reflections would each possess their own hit groups)
    
    \item The sixth parameter lets us configure which miss shader to use, irrespective of the hit group.
    
    \item The seventh parameter is the ray description, as defined in \ref{RaysInDXR}, and
    
    \item the eighth parameter is the payload the ray carries over its lifetime.
\end{itemize}

An alternative \verb|TraceRayInline(...)| function exists, that does not use separate shaders, and defers all shading to the caller \cite{directx12_docs}.

\subsection{Code Example}\label{RTX_code_example}

The code below serves as a simple example for the respective shaders a simple DXR program for shadow rays would utilize. The respective anyhit and miss shaders are marked by \verb|[shader("anyhit")]| and \verb|[shader("miss")]| attributes respectively:

\begin{lstlisting}
// Custom ray-payload datatype
struct ShadowPayload {
    bool isVisible;
};

[shader("miss")]
void ShadowMiss(inout ShadowPayload payload) {
    payload.isVisible = false;
}

[shader("anyhit")]
void ShadowAnyHit(intout ShadowPayload payload, BuiltInTriangleIntersectionAttributes attrib)
{
    // Ignore transparent surfaces
    if( isTransparent(attrib, PrimitiveIndex() ) ) {
        IgnoreHit();
    }
}

[shader("raygeneration")]
void rayGen()
{
    float3 origin = CameraPosition();
    
    float3 dir = PixelPosition(DispatchRaysIndex().xy) - origin;

    RayDesc ray = {
        Origin = origin,
        Direction = dir,
        TMin = 0.001f,
        TMax = 1000f
    };
    
    ShadowPayload payload = { true };
    
    TraceRay( scene,
              RAY_FLAG_SKIP_CLOSEST_HIT_SHADER,
              0xFF, 0, 1, 0, ray, payload );
    
    if( payload.isVisible ) {
        OutputTexture[DispatchRaysIndex().xy] = RED;
    }
    else {
        OutputTexture[DispatchRaysIndex().xy] = BLACK;
    }
}
\end{lstlisting}

This example colors every pixel that is occupied by geometry in red. It uses the ray payload to mark whether a pixel is occupied by any non-transparent geometry.
The \verb|isVisible| component is initialized as \verb|true|, but is set to false in the miss shader. The anyhit shader discards intersections with transparent geometry
\footnote{Some functionality given in the example, such as the functions CameraPosition(), isTransparent() and PixelPosition() are not part of DXR and have been abstracted for the sake of simplicity.}.

\subsection{Host Initialization}

The foregoing sections provide a comprehensive overview of the DXR raytracing pipeline and its GPU-side functions and capabilities. However, as with any graphics API, the global pipeline state and execution is managed by the device host (CPU side) through a series of API function calls.

Low level DirectX code is typically highly verbose, with even simple projects requiring hundreds to thousands of lines in C++ code. For the sake of brevity, this thesis will only provide superficial examination of the key functions required for raytracing.

Initialization of a DXR raytracer typically follow these common steps \cite{rtx_gems}:

\begin{itemize}
    \item Initializing the DirectX device (GPU) and verifying that it supports raytracing.
    \item Loading scene geometry and generating a BVH acceleration structure from it.
    \item Loading and compiling the respective HLSL shaders, defining root signatures and shader tables.
    \item Defining a DirectX pipeline state object.
    \item Dispatching a workload to the pipeline.
\end{itemize}

\subsection{Acceleration Structure}

As described in section \ref{BVH_theory}, utilizing a hierarchical acceleration structure can reduce the complexity per ray from linear to logarithmic in the number of triangles.
There are a variety of acceleration structure types available and DirectX does not mandate the use of any particular one, though bounding volume hierarchies (BVHs) are generally best suited \cite{rtx_gems}.

The construction process and data structure of DirectX BVHs is entirely opaque, as they are built and maintained by the device driver on the GPU \cite{rtx_gems}. Different graphics card vendors may choose alternative structures, but DirectX operates on a given set of structural principles \cite{rtx_gems, rtx_spec_ms}:

The acceleration structure consists of two levels: a bottom-level acceleration structure (BLAS), which contains geometric primitives, and a top-level acceleration structure (TLAS), that contains one or more bottom-level structures (see fig. \ref{blas_tlas_table}).

\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{Figures/blas_tlas_table.jpg}
\decoRule
\caption[]{Key components of an RTX pipeline: TLAS, BLAS and shader binding table. Image from the Microsoft DXR specification \cite{rtx_spec_ms}.}
\label{blas_tlas_table}
\end{figure}

\subsubsection{BLAS}

A bottom-level acceleration structure is usually a BVH in of itself that represents a single geometry type. Ray-triangle intersection tests are performed on BLAS data \cite{rtx_gems}.

If the geometry topology remains fixed, BLAS structures can be refit with scene changes, which is an order of magnitude faster than complete rebuilds. However, repeatedly performing refit operations may degrade the quality and performance of the acceleration structure over time. It is generally recommended to use an appropriate combination of refits and complete rebuilds \cite{rtx_gems}. 

\subsubsection{TLAS}

Analogously to how single 3D models can be instantiated multiple times with individual model matrices, BLAS instances within a TLAS are referenced with memory pointers alongside a transformation matrix \cite{rtx_spec_ms} (see fig. \ref{BlasTlasSimpl}).
Whilst the reuse of geometry has great benefits to memory requirements, its overuse can impact performance, as the individual BLAS instances ought to overlap as little as possible \cite{rtx_gems}.

A TLAS is, in essence, an acceleration structure of acceleration structures. Pointers to BLAS structures already living in GPU memory are contained alongside an instance matrix as well as other data like shader-index, flags etc.

\begin{figure}[H]
\centering
\includegraphics[scale=0.35]{Figures/blas_tlas_simplifier.jpg}
\decoRule
\caption[]{Simplified illustration of how the individual components of a scene relate to BLAS-TLAS components.}
\label{BlasTlasSimpl}
\end{figure}

\subsubsection{Shader Table}

The whens and hows of tracing rays in DXR are not as strictly mandated by a sequential pipeline as in rasterization. As such, all resource bindings and shaders must be simultaneously available during the entire execution time. The selection of which shader to run is treated as any other resource binding and kept as a set of shader records in a contiguous region of memory known as the \textit{shader table}.

The shader binding table is, in essence, a region of 64-bit aligned GPU memory that is owned and managed by the application \cite{rtx_spec_ms}. It links the acceleration structures, hit groups and shader functions together by indicating what programs are executed for which geometry and which resources are associated with it \cite{rtx_spec_ms}.

\section{Status Quo of RTX}

In their examination of RTX technology, Sanzharov et al. conclude that whilst RTX on Turing GPUs performs well, its software-emulated variant that runs without RT-cores is an inefficient and expensive process that "essentially loses to simple and straightforward open source ray tracing" \cite{RTX_examination}, implying that "'the golden age of software' has ended and that 'the golden age of compilers and HW/SW projects' has started" \cite{RTX_examination}.

The visual fidelity that hardware-accelerated RTX can provide has indeed found favour with developers, as implied by its widespread adoption across many products and rendering engines such as software by \textit{Adobe}, \textit{Unity} and a vast catalogue of video games \cite{rtx_catalogue}. Even more predicating is the fact that one of Nvidia's primary competitors on GPU market, \textit{AMD} released their newest line of products, the \textit{Radeon RX 6000 Series}, with an RT-core equivalent component, designed specifically for hardware-accelerated raytracing \cite{RDNA2_explained}.

Despite these successes, lower \textit{frames-per-second}, in addition to price and compatibility constraints, show that RTX remains an \textit{enhancement}, not a replacement, to classical rasterization.
The underlying concept of raytracing remains just as computationally expensive as it has been since its inception in the late 1970s.

In the same time, rasterization-based techniques have come a long way in finding ideal mathematical approximations, shortcuts and simplifications that maximize photorealism. Combining the newest kinds of these techniques can produce images of similar quality to raytracing, albeit at far greater speeds.

\begin{figure}[th]
\centering
\includegraphics[scale=0.2]{Figures/rtx_vs_screenspaceref.jpg}
\decoRule
\caption[]{Screen-space reflections (left) vs ray-traced reflections (right). Screen-space reflections are generally less resource-intensive, but can only reflect geometry that is rendered to the screen itself. Images taken from the PC game \textit{Hellblade: Senua's Sacrifice} by \textit{Ninja Theory}.}
\label{rtx_vs_ssr}
\end{figure}

A great example of how clever and sophisticated these techniques have become can be observed in the form of \textit{screen-space reflections} (fig. \ref{rtx_vs_ssr}). This is a fast method of creating realistic looking reflections by simply taking respective pixel values already rendered into the FBO \cite{SSR}. At minimal performance impact, this requires no triangle-intersections or BVH traversals, but only geometry that is visible on screen can be reflected \cite{SSR}.

Raytracing, on the other hand, provides a more powerful, brute-force approach that costs exponentially more. Consumers may prefer incurring the small cost in photorealism provided by raytracing in return for a more responsive application running at a higher frame-rate.

It is generally recommended by Nvidia themselves, to opt for a hybrid approach by using rasterization as a base and complement it with raytracing where it provides the most benefits (such as specular reflections, refractions and shadows) \cite{turing_whitepaper, Hybrid_RTX_Mader}.

In chapter \ref{Chapter2} we demonstrated that radiosity and raytracing share many fundamental aspects by deriving both from the rendering equation. The underlying implication, which we will examine in the subsequent chapters, is that the performance increase RTX provides for raytracing ought to applicable to radiosity as well.