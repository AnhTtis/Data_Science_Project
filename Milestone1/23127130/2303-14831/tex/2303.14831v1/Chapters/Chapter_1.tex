% Chapter 1

\chapter{Preface} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------

The computational synthesis of photorealistic images has been a quintessential challenge in the computer graphics domain since its inception. Growing industries such as video games, virtual reality and visual effects have induced a veritable explosion in demand for increased realism over the last few decades. A major step towards this goal was taken by James Kajiya in 1989 when he formulated the \textit{rendering equation} \cite{Kajiya}, which provides a general mathematical description of how light propagates through a 3D environment.

Unfortunately, the rendering equation proved far too complex to solve linearly. Every surface can receive light from infinitely many directions and then scatter it diffusely, effectively qualifying the surface as a separate light-source itself.

Computer graphics researchers have spent a large part of their endeavour grappling with the conundrum that is finding an ideal, numerically solvable model to this infinitely recursive complexity. Indeed, rendering algorithms we see employed today can all be regarded as approximations, shortcuts or simplifications of the rendering equation.

With regard to \textit{global illumination}, two of these have stood the test of time: \textit{ray tracing} for the generation of individual, highly realistic images and \textit{radiosity} for real-time use cases that continuously render the same, static geometry from a large set of camera angles.

\section{Nvidia RTX}

Over the last decades raytracing has generally found its place as a crude and expensive approach that nevertheless provides a very high degree of photorealism, albeit at a proportionally high cost in required computation time.

Yet in 2018, fifty years after the first computer-based ray-tracer was created \cite{first_raytracer}, the American tech company \textit{Nvidia} unveiled their \textit{GeForce RTX} series of graphics cards. Uniquely, these contain specialized computation units that can speed up raytracing-related operations to such a degree that it propels this blunt, brute-force approach into the domain of real-time \cite{turing_whitepaper}. 

The mathematical challenges faced by raytracing and radiosity are fundamentally identical and thus inextricably linked. In this thesis we argue that the considerable performance increase enabled by the RTX platform ought to be reflected in radiosity to the same degree it is seen in raytracing.

\section{Motivation}

Global illumination solutions based on radiosity typically generate lighting information and then export it into a texture, which can be rendered a-posteriori within consumer applications at virtually no cost at all. Despite great rendering performance, the process of generating these textures remains a computationally expensive process that can severely hamstring the development and design process of complex 3D environments.

Radiosity's performance bottleneck unequivocally lies with the vast amount of visibility calculations required \cite{radiosity}.
Although this problem is, in theory, highly parallelizable \cite{RadiosityParallelization}, implementations of the radiosity model seem to generally favour multi-core CPUs (approx. 4-16 high performance cores) over GPUs (approx. 1-10 thousand low performance cores) \cite{ray_engine}, because GPU variants rely on hemicuboid z-Buffering for visibility determination \cite{RadiosityOnGPUs_Coombe}.

In this thesis we investigate if an RTX-based visibility solution provides a performance improvement sufficient enough to fully advance radiosity into the realm of GPUs and parallel computing.

Not only is raytracing a highly adequate solution for visibility, but RTX GPUs also perform their operations on dedicated hardware in the form of a moderate amount (30 to 80) of highly specialized \textit{RT cores} \cite{turing_whitepaper}. This intermediate solution between the parallelization levels of a CPU and a GPU may prove ideal for the acceleration of the radiosity algorithm.

\section{Objective}

The intended goal behind this thesis is to further the acceleration of radiosity computations for developers and designers of 3D environments working on machines compatible with RTX. Once lighting textures have been generated, they can in turn be rendered on almost any graphics hardware, regardless of RTX compatibility.

To accomplish this, we grapple a common variant of radiosity found on GPUs, known as \textit{progressive refinement radiosity}, and substitute its z-Buffering components with an RTX-based approach. We will also investigate and examine potential performance improvements in addition to how well this approach compares to already existing solutions.

\section{Thesis Structure}

The next chapter will cover the theoretical knowledge required for the remainder of the thesis by deriving the rendering equation and providing mathematical models for several global illumination solutions.

Afterwards, chapter \ref{Chapter3} takes a deep dive into RTX technology by examining and reviewing the underlying \textit{Turing architecture} and \textit{DirectX} raytracing pipeline.

Chapters \ref{Chapter4} and \ref{Chapter5} will present \textit{RTRad}, our RTX-accelerated progressive refinement radiosity implementation as well as any related tweaks and potential performance improvements.

Lastly, in chapter \ref{Chapter6}, we compare and analyze the performance of this implementation upon which we draw our conclusions in chapter \ref{Chapter7}.

