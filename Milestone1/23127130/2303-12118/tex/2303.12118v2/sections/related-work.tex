To situate our study and motivate our methods, we review prior literature in credibility signals, visual fact-checking, and media provenance. Unlike previous work, this paper contributes an empirical study on how users' perceptions of media change upon exposure to provenance information in a social media setting, along with how design choices in provenance-bearing UIs may influence those perceptions.


\subsection{Credibility on Online Platforms}
How do users determine credibility online in the first place? Previous work in online news, social media, and e-commerce agree that credibility is relational \cite{chowdhury2020joint, alsmadi2016interaction, luazuaroiu2020consumers, kim2013effects, brown2007word}---i.e. an individual's judgement is heavily influenced by relationships with other users and online entities---, longitudinal \cite{filieri2018makes, metzger2010social, alrubaian2018credibility}---i.e. built up over time through repeated interactions---, and experiential \cite{morris2012tweeting, metzger2013credibility, metzger2010social, zhang2014examining}---users develop their own evaluation heuristics developed through the aforementioned repeated interactions. Specifically on social media, some of these heuristics include reputation, endorsement, consistency, and self-confirmation, obtained from information such as the post author, content source, and number of comments or reviews \cite{metzger2013credibility}. The magnitude of effects from these heuristics, however, can vary based on evaluation task. Shen et al. \cite{shen2019fake} conducted a study with purely fake images and found that participants' past experiences with photo-editing and social media were significant predictors of image credibility evaluation, while most social and heuristic indicators (e.g. source trustworthiness) had no significant impact. However, even if users wanted to leverage information such as content source to make a credibility judgement, they are often obscured or simply not present in the interface \cite{dejavu, liang2018tracing}. 

% These works add to psychology literature suggesting that emotional state is an important factor in determining mode of information processing, which in turn influences ratings of veracity in a given moment \cite{tiedens2001judgment}. 

%Why do people deceive others online in the first place? From a content creation perspective, literature in online deception suggests that this may be due to personal privacy \cite{hancock2009butler, xiao2016disclosure, ho2018psychological}, insecurity \cite{hancock2007digital, hancock2007digital, litt2014awkward, toma2008separating, toma2010looks}, and entertainment \cite{lu2008sensation, chadwick2019news}. On the receiving end, to investigate social media users' interpretations of veracity, Jahanbakhsh et al. \cite{jahanbakhsh2021nudges} developed a taxonomy of reasons why one may or may not accept a news claim as true. The taxonomy showed that while many participants relied on evidence they already had knowledge of and trusted, many (dis)believed claims out of a random guess or because they wanted it to be true or false. Shen et al. \cite{shen2019fake} conducted a study with purely fake images and found that participants' past experiences with photo-editing experience, social media, and the internet as a whole were significant predictors of image credibility evaluation, while most social and heuristic indicators (e.g. source trustworthiness) had no significant impact. These works add to psychology literature suggesting that emotional state is an important factor in determining mode of information processing, which in turn influences ratings of veracity in a given moment \cite{tiedens2001judgment}. As such, ``truth'' can be a rather an abstract concept that people do not converge on. Indeed, even experts' evaluations of credibility may differ depending on the discipline of their expertise \cite{bhuiyan2020experts}, casting multifaceted challenges upon creating credibility indicators \cite{zhang2018structured}. 

These challenges have given rise to a variety of mechanisms and designs for communicating media credibility \cite{zhang2018structured}. Some actively dissuade users from interacting with content though making an editorial judgement \cite{ozturk2015combating, yaqub2020credibility, sherman2021designing, twitter-manip-label, im2020signals}. However, this approach is not advisable for platforms that attempt to appear impartial \cite{wsj-impartial}. Thus, significant interest lies in content-neutral techniques that nudge users towards relevant articles, useful resources, fact-checking tools, and more \cite{dias2020emphasizing, jahanbakhsh2021nudges, morris2012tweeting, lazer2018science, fourcorners}. %Many platforms implement the latter approach in an attempt to appear impartial \cite{wsj-impartial}. 
In a hybrid approach, some platforms have also attempted to crowdsource editorial judgements without casting any liabilities on themselves \cite{birdwatch, mashable-crowd}. For example, Twitter's Birdwatch program \cite{birdwatch} crowdsources ``notes'' from members to identify problematic tweets, but Twitter explicitly states that the notes ``do not represent Twitter's viewpoint and cannot be edited or modified by [Twitter's] teams.'' 

Regardless of approach, literature has shown that effective credibility indicators should be immediate (e.g. shown at first exposure), in context (e.g. in close visual proximity to the content), and specific (e.g. show exactly what makes the content problematic) \cite{sherman2021designing}, leading to calls for interoperable standards for content credibility \cite{zhang2018structured}. Some of these techniques may seem successful at first glance in reducing the sharing of unfounded rumours \cite{lazer2018science} and false content \cite{jahanbakhsh2021nudges}, as well as improving the accuracy of user interpretations \cite{bode2018see, seo2019trust}. These techniques can also have the reverse effect, however, making viewers more ignorant of factual information \cite{jahanbakhsh2021nudges, dias2020emphasizing, garrett2013corrections}. Other factors, such as display mode, can impact media perception as well---literature in end-to-end encrypted messaging found that icon-based disclosures of encryption actually impacted trust negatively, while text-based ones were effective in gaining trust \cite{stransky2021encryption}. Additionally, Yaqub et al. \cite{yaqub2020credibility} have shown that the success of credibility indicators on social media can vary based on demographics and personal characteristics. Besides relying on UI-based interventions alone, researchers have also underscored the importance of educational programs to increase awareness of and provide tools to combat misinformation online \cite{karduni2019, lindsay2019literacy}.

% One commonly explored area in prior work is the use of warnings and nudges to dissuade users from interacting with dubious content \cite{dias2020emphasizing, jahanbakhsh2021nudges, morris2012tweeting, lazer2018science, ozturk2015combating, yaqub2020credibility}. [stuff about neutral and not neutral here]. Specifically, Morris et al. \cite{morris2012tweeting} surveyed early Twitter users on factors used to assess credibility of tweets and suggested UI changes, including author credentials accessible at a glance and cluster visualization to show similar tweets, to better convey credibility. Indeed, some works increased visibility of certain areas of content that may be relevant to forming a truth judgement, such as publishers \cite{dias2020emphasizing} and headline in the case of news articles \cite{yaqub2020credibility}. The literature has shown that effective indicators should be immediate (e.g. shown at first exposure), in context (e.g. in close visual proximity to the content), and specific (e.g. show exactly what makes the content problematic) \cite{sherman2021designing}, leading to calls for interoperable standards for content credibility \cite{zhang2018structured}. Some of these techniques may seem successful at first glance in reducing the sharing of unfounded rumours \cite{lazer2018science} and false content \cite{jahanbakhsh2021nudges}, as well as improving the accuracy of user interpretations \cite{bode2018see, seo2019trust}. These techniques can also have the reverse effect, however, making viewers more ignorant of factual information \cite{jahanbakhsh2021nudges, dias2020emphasizing, garrett2013corrections}. Other factors, such as display mode, can impact media perception as well---literature in the adjacent field of end-to-end encrypted messaging found that icon-based disclosures of encryption actually impacted trust negatively, while text-based ones were effective in gaining trust \cite{stransky2021encryption}. Additionally, Yaqub et al. \cite{yaqub2020credibility} have shown that the success of credibility indicators on social media can vary based on demographics and personal characteristics. Besides relying on UI-based interventions alone, researchers have also underscored the importance of educational programs to increase awareness of and provide tools to combat misinformation online \cite{karduni2019, lindsay2019literacy}.

So far, work on credibility signals has mainly focused on supporting the assessment of truthfulness at a particular point in time. Our work extends current literature into the realm of provenance-enabled media, where key information also lies along a temporal axis. 

\subsection{Visual Fact-Checking Techniques}
Fact-checking for information accuracy has been a common practice in news journalism for decades. However, the proliferation of visual mis/disinformation presents many novel challenges. First, images and videos are powerful in their ability to convince: participants who see fictional news accompanied by even a tangentially related photo are more likely to say they remember the event \cite{Strange2011-ye} and share the content on social media \cite{Fenn2019-px} than those who did not see a visual. Furthermore, sophisticated image and video editing techniques, such as ``deepfaking'' \cite{bode2021deepfaking}, or even lightweight modifications via ``cheapfaking'' \cite{la2022cheapfakes}, make it difficult for the naked eye and even automated algorithms to detect manipulations. This calls for efforts to rethink how traditional text-based fact-checking techniques may be expanded to visual media. 

The computer vision community has dedicated significant attention to the area of ``image forensics''---manipulation detection through hardware and software fingerprinting \cite{fridrich, phan, caldelli} as well as comparisons with similar images \cite{cozz-cnn, moreira-cnn, wu-cnn, li-fast, zhou2018learning}. Modern techniques such as feature learning and convolutional neural networks (CNNs) were used to identify images that underwent copy-move and splicing transformations \cite{cozz-cnn, moreira-cnn, wu-cnn, zhou2018learning}, along with datasets and frameworks in this area \cite{wen-ds, nguyen-ds, hu2020span, He-ds}. More classical techniques have also been able to quickly and effectively identify some manipulation procedures. Li et al. were able to effectively detect copy-move forgeries with hierarchical feature point matching \cite{li-fast}. Other work builds off the observation that even though a manipulation is not always recognizable or visible, the statistical properties of an image or video may change upon being edited via JPEG compression \cite{popescu2004statistical, fu2007generalized, fan2003identification}, Colour Filter Array (CFA) interpolation \cite{popescu2005exposing, goljan2015cfa}, contrast and lighting \cite{stamm2010forensic, johnson2007exposing}, and noise \cite{pan2012exposing}. Additionally, web scraping and reverse image search services such as TinEye \cite{tineye} can help reveal similar or exact images that have previously been posted elsewhere on the internet. These techniques can all be used to supplement, or replace, investigators' manual research to create warning labels for the media \cite{twitter-manip-label}.

\remove{Identifying visual misinformation is no easy task, and researchers have recognized the value of collaboration in this process. Matatov et al. present DejaVu \cite{dejavu} to assist journalists in collaboratively addressing visual misinformation. The system looks for near-identical image matches across the web and extends the matches by crawling and indexing rogue social media sites such as 4chan. Journalists can then collaboratively flag results and highlight flags for other journalists. The Eunoma Project \cite{eunomia} aims to approach fact-checking from a different angle; instead of relying on users to fact-check information on their own, the platform provides information cascade visualizations, context checking features, and provenance identification tools for users to determine whether the information they see can be trusted. Users then indicate their trustworthiness, which then turns into crowdsourced trust indicators for other users on the platform.} %Twitter's Birdwatch program crowdsources ``notes'' from members to identify problematic tweets \cite{birdwatch}. 
While significant progress has been made in visual fact-checking, development of checker-evading techniques have progressed right alongside it \cite{gragnaniello2018analysis, hussain2021adversarial, rozsa2020adversarial}. Provenance standards circumvent this by inserting a signed (often cryptographic) signature into media metadata as the media is created or edited. Additionally, offering provenance information may be more comprehensible to users than explaining complex algorithmic or crowdsourced approaches. 

%there are still limitations to depicting media as a true-false binary \cite{margolin2018political}. Our work leverages provenance information of digital media to provide users with resources to make an informed decision about a piece of content rather than explicitly asserting a truthfulness claim.

x

\subsection{Surfacing Provenance in Media}
The benefits of surfacing provenance are several. Exposing a piece of media's prior history of creation, editing, and sharing can empower viewers to \textit{proactively} make a truth judgement for themselves instead of \textit{passively} consuming a misinformation label \add{\cite{c2pa-explainer, emily2022usable}}. Moreover, provenance equips content creators and media forensics experts with more tools to identify fake or illegitimately copied media \cite{dejavu}. Over time, provenance-enabled systems can build more trust in media sources by increasing transparency and accountability through structural mechanisms \cite{gundecha2013tool}. 

%Standard fact-checking approaches attempt to mitigate misinformation by arbitrating ``truthfulness.'' The viewer plays a \textit{passive} role in this process by consuming a pre-prepared truth judgement. Provenance presents a different approach---by exposing a piece of media's prior history of creation, editing, and sharing, viewers are empowered to \textit{proactively} make a truth judgement for themselves. Over time, provenance-enabled systems can build more trust in media sources by increasing transparency and accountability through structural mechanisms \cite{gundecha2013tool}. 

One major challenge associated with attaching provenance to media is verifying the credibility of the provenance information itself \cite{c2pa-explainer}. As a result, many researchers have turned to cryptographic methods to establish a single source of truth for provenance. Sidnam-Mauch et al. \cite{emily2022usable} argue that cryptographic provenance systems should provide 4 key assurances: authenticating provenance, verifying content is (un)altered, ensuring users are viewing the same content, and creating unalterable records of changes. Several provenance systems and standards based on cryptographic methods have been developed in the last few years and are summarized in Table \ref{t:prov-sys}. %\remove{For example, the News Provenance Project \cite{npp} is a simulated news feed demonstrating a vision for news publishers to display provenance information on news photos. The images' metadata is saved through publication to the Hyperledger Fabric Blockchain \cite{hyperledger}. While scrolling through the simulated feed, users can see an overview of a particular photo—including details on who took it, at what time, and where, as well as photo history—which news sources published it and when, along with caption comparisons between sources. Other examples \cite{aythora2020multi, arweave, cai,starling, project-origin, fourcorners} are shown in Table \ref{t:prov-sys}.}

\begin{table}
\small
\centering
    \begin{tabular}[ht]{p{4.5cm} p{6.5cm} p{2cm}}
    \toprule
    Project & Description & Year Founded\\
    \midrule 
    Arweave \cite{arweave} & Protocol that allows for decentralized saving of media and  websites to create a permanent record & 2017\\
    \midrule
    Four Corners Project \cite{fourcorners} & Open source tool that lets creators add authorship, media backstory, related imagery, and relevant links in an interactive layer on top of their images. & 2018\\
    \midrule
    Content Authenticity Initiative (CAI) \cite{cai} & Community of organizations developing off-the-shelf, open-source tools to integrate content credentials into media. & 2019 \\
    \midrule
    News Provenance Project (NPP) \cite{npp} & Prototype news feed that explores relationship between blockchain-based provenance and user trust in news images. & 2019 \\
    \midrule
    Project Origin \cite{project-origin, aythora2020multi} & Alliance of organizations to create a process where provenance and technical integrity of content can be confirmed. & 2019 \\
    \midrule
    Coalition for Content Provenance and Authenticity (C2PA) \cite{c2pa} & Open technical standard for media authoring tools to embed cryptographic signatures into media metadata. A merger of CAI and Project Origin. & 2020\\
    \midrule
    Starling Labs \cite{starling} & Academic partnership between USC and Stanford that aims to capture, store and verify digital content through cryptographic methods and decentralized web protocols. Incorporates open source tools from C2PA. & 2021\\
    \bottomrule
    \end{tabular}
    \caption{Table of cryptographic provenance initiatives, partially adapted from \cite{emily2022usable}.}
    \label{t:prov-sys}
\end{table}

Of course, cryptography is not the only path to uncovering provenance. Non-cryptographic methods have also been explored. This includes the web-based W3C PROV data model \cite{w3c-prov}, as well as efforts to identify provenance of social media content using formal methods in network theory \cite{barbier2013provenance}, network information and URLs on Twitter \cite{ranganath2013tool}, and user profile information \cite{gundecha2013tool}. The information security community has also developed a number of technical protocols out of interest for globally consistent logs that records change events emitted by various authorities \cite{melara2015coniks, laurie2014ct, ryan2013email, fahl2014stay, nikitin2017chainiac, yu2015detect}. Laurie \cite{laurie2014ct} introduced Certificate Transparency, a certificate distribution mechanism that records Transport Layer Security (TLS) certificates in a public, append-only log for open monitoring. Systems for verifying activities such as software distribution \cite{nikitin2017chainiac, fahl2014stay}, cryptographic key usage \cite{yu2015detect}, and encrypted email \cite{ryan2013email} have also been created, along with the proposition of general-purpose transparency protocols \cite{chen2020reducing, tyagi2021client}. Although not specifically designed with internet media in mind, such protocols may be adapted to any system that contains regular publishing and updating of uniquely identifiable information units.

Researchers have also recognized importance questions surrounding the usability of provenance systems. Sidnam-Mauch et al. \cite{emily2022usable} state that ``usability challenges pose the greatest barriers to implementing provenance tools,'' and that there may be a mismatch between the ideal degree of transparency to news consumers and news publishers. Prior work has also leveraged data visualization to more effectively communicate provenance information, specifically in scientific workflows \cite{yazici2018data, yazici2021usability}. Sherman et al. \cite{sherman2021designing} found that provenance was a key heuristic used to evaluate the authenticity of text, image, and video media when provided. They went on to conduct a large-scale quantitative evaluation of different indicator designs for video provenance and concluded that future designs should be concise, easy to interpret, passive rather than interstitial, and should be highly specific about details such as source and edits. \remove{In our study, we incorporate}\add{Our study incorporates} these principles into our social media prototype\add{, but differs from the work of Sherman et al. in three key ways. First, we explore provenance indicators for both images and videos, whereas Sherman et al. focused exclusively on videos. Second, our inclusion of the C2PA standard allows users to view a \textit{chain} of provenance information associated with a piece of media, rather than one source as per Sherman et al.'s designs. Finally, Sherman et al.'s survey study captured one variable--interpretability of the indicators---whereas we also are concerned with how the indicators change users' credibility judgements of media}. 

Despite prior technical and empirical work on digital media provenance, it is still unclear how end users' perceptions of media are affected by the introduction of provenance information. This may be due to the current lack of systems that implement provenance in-the-wild. In our work, we integrate provenance into an interactive social media feed and study how media perception changes between a regular feed and our provenance-enabled feed.
\add{\subsection{Measurements of Media Perception Online}}
\label{s:rw-measures}
\add{Given the findings surfaced by prior work, it is important to understand common measurements used by researchers to evaluate user perception of online content. One popular measure is the \textit{perceived accuracy} of a text-based headline or claim \cite{clayton2019partisan, clayton2020real, kirchner2020countering, kuru2017motivated, pennycook2017falls, pennycook2018prior, pennycook2019lazy, pennycook2020implied, pennycook2020fighting, jahanbakhsh2021nudges}. Participants are typically shown a piece of text, sometimes with accompanying information such as an image, and are asked rate how accurate they believe it is on a binary true-false scale \cite{jahanbakhsh2021nudges, pennycook2017falls, pennycook2018prior, pennycook2019lazy, pennycook2020implied}, or a 4- or 5-point Likert scale that accounts for more granularity such as ``somewhat accurate'' and ``very accurate'' \cite{clayton2020real, kirchner2020countering}. From accuracy measurements, Pennycook and Rand derived \textit{discernment}, defined as the difference in perceived accuracy or sharing intentions between true and false headlines \cite{pennycook2019lazy}. They then found that simple nudges to think about accuracy can nearly triple discernment levels in participants' sharing intentions, motivating the use of accuracy reminders to combat misinformation \cite{pennycook2020fighting}.}

\add{Another popular measure is \textit{trust} \cite{fisher2016trouble, heuer2018trust, huang2019trust, sherchan2013trust, pennycook2019fighting, epstein2020distrusted}. Sherchan et al. \cite{sherchan2013trust} conducted a review of literature in social and computer sciences to form a definition for social trust in the context of social networks: trust derived from social capital based on interactions between members of a network. Eslami et al. \cite{eslami2017careful} additionally found that user-detected algorithmic bias on social platforms can lead to a breakdown of user trust. The core concept of trust, however, is still not well-defined. Indeed, Fisher noted that despite evolution of literature in measuring trust in news media over a span of 80+ years, there does not exist an agreed-upon definition or measure of trust \cite{fisher2016trouble}. As a result, many studies measure trust in a similar way as accuracy: participants self-report trust on a Likert scale \cite{heuer2018trust, pennycook2019fighting, epstein2020distrusted}.}

\add{Besides perceived accuracy and trust, measures of \textit{believability} \cite{kim2019combating, moravec2018fake}, \textit{credibility} \cite{moravec2018fake, bhuiyan2020experts, hilligoss2008developing},  and \textit{truthfulness} \cite{saeed2022crowdsourced, soprano2021many} have also appeared in prior work. In Kim et al.'s study of fake news articles \cite{kim2013effects}, believability was a composite measure consisting of three 7-point Likert scale ratings of belief in, trust in, and credibility of an article, respectively. Moravec et al. \cite{moravec2018fake} used the same three 7-point ratings but called their composite measurement credibility rather than believability. We note that credibility has also been used interchangeably with trust \cite{huang2019trust}, but prior work has agreed on the use of credibility as a high-level, multi-dimensional construct consisting of believability, fairness, trust, accuracy, reliability, and ``dozens of other concepts and definitions thereof'' \cite{bhuiyan2020experts, hilligoss2008developing}. Bhuiyan et al. \cite{bhuiyan2020experts} specifically included a blend of subjective (e.g. trust) and objective (e.g. accuracy) measurements in their approach to credibility. We take a similar approach in our work. Overall, we observe significant overlaps in prior work's terminology definitions.}

\add{In this study, we draw from past study designs to measure shifts in credibility upon introducing provenance information in social media feeds. Because the multifaceted dimensions of credibility are too numerous to capture in one study, we both subjective measurements (e.g. perceived accuracy of claim) and objective measurements (e.g. actual accuracy of claim) to compute our variables representing credibility.}\\
