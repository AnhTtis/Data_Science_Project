Our CLMM models for estimating effects of \textsc{trust} and \textsc{perceived accuracy}, as outlined in Section \ref{s:quant-analysis}, were used to identify significant effects and interaction effects to further explore using hypothesis tests. Both models revealed \textsc{edit status}, \textsc{claim agreement}, \textsc{media topic}, and \textsc{provenance state} as significant main effects. We then examined interactions between the independent variables by crossing them in pairwise combinations and incorporating them into the models. In both models, we found interactions between \textsc{claim agreement} and \textsc{provenance state}, as well as \textsc{edit status} and \textsc{media topic}, to be significant. The coefficients of fixed effects from the two models, along with their standard errors and significance, are shown in Table \ref{t:models}. Note that the independent variables are split up into their levels due to their categorical nature.

\begin{table}[h]
\small
\centering
    \begin{tabular}[h]{p{4cm} p{4cm} p{5cm}}
    \toprule
    Variable Level & Trust Coefficient (Std. Error) & Agreement Coefficient (Std. Error)\\
    \midrule 
    Composited & -0.49 (0.22)* & -0.56 (0.22)*\\
    Disagree & -0.96 (0.10)*** & -0.96 (0.10)***\\
    National & 0.37 (0.10)*** & 0.36 (0.10)***\\
    News & 1.03 (0.09)*** & 0.78 (0.09)***\\
    Invalid & -1.42 (0.15)*** & -1.49 (0.15)***\\
    Normal & 0.45 (0.12)*** & 0.19 (0.15)\\
    Composited * National & -0.91 (0.18)*** & -0.95 (0.18)***\\
    Composited * Normal & -0.26 (0.20) & -0.13 (0.20)\\
    Disagree * National & 0.07 (0.16) & 0.04 (0.16)\\
    Disagree * Invalid & 0.58 (0.19)** & 0.68 (0.19)***\\
    \bottomrule
    \end{tabular}
    \caption{Coefficients, standard errors, and significance from the trust and perceived accuracy CLMMs. $p<0.05$*, $p<0.01$**, $p<0.001$***}
    \label{t:models}
\end{table}

Additionally, past work on credibility indicators and sharing behaviour \cite{yaqub2020credibility} signalled the possibility of significant differences in our responses based on demographic characteristics, which would in turn affect our data aggregation methods during analysis and reporting. We performed hypothesis tests (Mann-Whitney U Test for 2 groups or the Kruskal-Wallis test for 3 or more groups) on data across levels within collected demographic information (gender, age range, country, household income, race, and education attainment). We did not find evidence of significant differences in both trust and perceived accuracy across any demographic levels. %[need table [x] in Appendix?]

\subsection{RQ1: Differences in \textsc{trust} and \textsc{perceived accuracy} in no-provenance and provenance-enabled feeds}
\label{s:rq1}
RQ.1 was concerned with whether provenance information even made a significant difference in participants' ratings of trust and perceived accuracy. To evaluate this, we used the Wilcoxon signed-rank test to compare ratings from the no-provenance and provenance-enabled rounds. To capture the direction and approximate magnitude of change, we computed the difference in means between the two rounds, subtracting the no-provenance rating from the provenance-enabled rating. That is, we computed $\overline{t_e}-\overline{t_c}$ and $\overline{\alpha_e}-\overline{\alpha_c}$, where $\overline{x} = \sum_{i=0}^{n} x_i / n$. Overall, we mostly found significant differences, with trust and perceived accuracy decreasing in most cases. Taking into consideration interaction effects between the variables, we stratified our analysis by \textsc{edit status} and \textsc{media topic}, as well as by \textsc{claim agreement} and \textsc{provenance state}. The results are shown in Tables \ref{t:edit-topic} and \ref{t:agreement-state}, respectively. %A high-level visual summary with \textsc{provenance state} and \textsc{edit status} is also shown in Fig. \ref{fig:visual-summary}.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=1\textwidth]{nick-summary.png}
%     \caption{Difference in means of \textsc{trust} and \textsc{agreement} ratings, broken down by \textsc{provenance state} and \textsc{edit status}. All values shown are statistically significant ($p < 0.05$).}
%     \label{fig:visual-summary}
% \end{figure}

As Table \ref{t:edit-topic} shows, composited content had a larger negative change in trust and perceived accuracy than non-composited content, indicating that participants were informed about the edit through exposure to provenance and changed their judgements accordingly. The differences in $\Delta t$ and $\Delta \alpha$ between non-composited and composited media were statistically significant: ($U = 1741294.0, p < 0.001$) for $\Delta t$, ($U = 1620590.5, p < 0.001$) for $\Delta \alpha$. We did not observe any cases where the direction of change in trust and perceived accuracy were different. We also did not find evidence for significant differences in trust and perceived accuracy in non-composited national content. %The significant changes in non-composited content may be partially due to state differences: for example, 1 piece of non-composited news content bore the incomplete state while no non-composited lifestyle content did. The effects of state are further clarified in Table \ref{t:agreement-state}.

About half of participants (52\%) saw a no-state feed in which there was no difference in provenance state between feed media and the other 48\% saw mixed-state feeds in which feed media were assigned normal, incomplete, and invalid provenance states. The assignment was fixed, meaning that a piece of media's state in a mixed-state feed remained consistent across all mixed-state feeds. There was evidence for significant differences in all groups, except for no-state media that agreed with their claims. For media with an invalid state and agreed with their claims, trust and perceived accuracy decreased noticeably more sharply than media with the same claim agreement but an incomplete state. This pattern was not observed with content that disagreed with their claims. The only instances of significant positive change in trust and perceived accuracy came from normal-state content that agreed with their claims. Overall, we found significant differences in $\Delta t$ and $\Delta \alpha$ between content that agreed with their claims and ones that did not: ($U = 3109723.5, p < 0.001$) for $\Delta t$, ($U = 3012445.0, p < 0.001$) for $\Delta \alpha$.

From Table \ref{t:agreement-state}, we noticed differences between ratings of the no-state media (which still bear the normal state provenance indicator by default) and normal state media in the mixed-state feeds. This led us to run a Mann-Whitney U Test to check for significant differences between the normal state media in the mixed-state feeds and those same pieces of media in the no-state feeds. Indeed, rating differences across the two were significant. Compared to the no-state media, the mixed-state media's trust was 0.08 points higher on average ($U = 1056801.5$, $p < 0.05$) and its perceived accuracy was 0.09 points higher on average ($U = 1058681.5$, $p < 0.05$). This suggests that the visual representation of the normal state may be perceived as more reliable in the presence of incomplete and invalid states compared to simply on its own.

\subsection{RQ2: Differences in \textsc{correction} upon consumption of provenance information}
\label{s:rq2}
Differences in perceived accuracy ratings pre- and post-provenance show that provenance information has shifted judgement, but for better or for worse? This is what RQ.2 explores. The \textsc{correction} variable, $\alpha_{corr}$, shown in Tables \ref{t:edit-topic} and \ref{t:agreement-state} can offer us some insight by measuring how much closer the participant got to the ground truth rating for each piece of content. We note here that $\alpha_{corr} = \Delta \alpha$ if $\alpha_c - A < 0$ and $\alpha_e - A < 0$. We also include an additional measurement in our tables, remaining distance, that measures how far (in absolute distance) participants' average perceived accuracy ratings (via claim agreement) were from the ground truth agreement rating for each post. We do this to better contextualize $\alpha_{corr}$: even if $\alpha_{corr}$ is positive, its effectiveness may be diminished if the remaining distance is still large. We denote remaining distance with $d_{rem}$.

\begin{table}[h]
\small
\centering
    \begin{tabular}[h]{p{1.9cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm} p{1.1cm}}
    \toprule
    & \multicolumn{4}{c}{Non-composited} & \multicolumn{4}{c}{Composited}\\
    \cmidrule(lr){2-5}
    \cmidrule(lr){6-9}
    Media Topic & $\Delta t$ & $\Delta \alpha$ & $\alpha_{corr}$ & $d_{rem}$ & $\Delta t$ & $\Delta \alpha$ & $\alpha_{corr}$ & $d_{rem}$ \\
    \midrule 
    News & -0.28*** (0.03) & -0.28*** (0.04) & -0.28*** (0.04) & 1.44 (0.03) & N/A & N/A & N/A & N/A\\
    Lifestyle & 0.22*** (0.05) & 0.21*** (0.05) & 0 \hspace{0.2cm} (0.05) & 1.70 (0.04) & -0.75*** (0.08) & -0.92*** (0.08) & 0.92*** (0.08) & 1.17 (0.06) \\
    National & 0.05 (0.05) & 0.01 (0.05) & -0.06 (0.05) & 1.89 (0.04) & -0.65*** (0.06)  & -0.82*** (0.07) & 0.82*** (0.07) & 0.79 (0.05) \\
    \bottomrule
    \end{tabular}
    \caption{Difference in means of \textsc{trust}, \textsc{perceived accuracy}, \textsc{correction}, and remaining distance from pre- to post-provenance, broken down by \textsc{edit status} and \textsc{media topic}. Note that no news content was composited in our study. $p<0.05$*, $p<0.01$**, $p<0.001$***. \add{Standard errors are shown in in parentheses.}}
    \label{t:edit-topic}
\end{table}

\begin{table}[h]
\small
\centering
    \begin{tabular}[h]{p{2.3cm} p{1.1cm} p{1.1cm} p{1.1cm} p{0.9cm} p{1.1cm} p{1.1cm} p{1.1cm} p{0.9cm}}
    \toprule
    & \multicolumn{4}{c}{Agree With Claim} & \multicolumn{4}{c}{Disagree With Claim}\\
    \cmidrule(lr){2-5}
    \cmidrule(lr){6-9}
    Provenance & $\Delta t$ & $\Delta \alpha$ & $\alpha_{corr}$ & $d_{rem}$ & $\Delta t$ & $\Delta \alpha$ & $\alpha_{corr}$ & $d_{rem}$\\
    \midrule 
    No state (no-state feed) & 0.05 (0.04) & 0.02 (0.04) & 0.02 (0.04) & 1.40 (0.03) & -0.24*** (0.05) & -0.35*** (0.05) & 0.30*** (0.05) & 1.23 (0.04) \\
    Normal state & 0.19*** (0.06) & 0.20*** (0.06) & 0.20*** (0.06) & 1.53 (0.04) & -0.15** (0.08) & -0.25*** (0.08) & 0.16* (0.07) & 1.17 (0.06) \\
    Incomplete state & -0.53*** (0.08) & -0.45*** (0.08) & -0.45*** (0.08) & 1.34 (0.06) & -0.66*** (0.09) & -0.86*** (0.11) & 0.86*** (0.11) & 0.76 (0.06) \\
    Invalid state & -1.14*** (0.09) & -1.24*** (0.09) & -1.24*** (0.09) & 2.43 (0.07) & -0.68*** (0.09) & -0.57*** (0.10) & 0.57*** (0.10) & 1.12 (0.07) \\
    \bottomrule
    \end{tabular}
    \caption{Difference in means of \textsc{trust}, \textsc{perceived accuracy}, \textsc{correction}, and remaining distance from pre- to post-provenance, broken down by \textsc{claim agreement} and \textsc{provenance state}. $p<0.05$*, $p<0.01$**, $p<0.001$***. \add{Standard errors are shown in parentheses.}}
    \label{t:agreement-state}
\end{table}

For non-composited content, the only significant \textsc{correction} identified was in news content. However, this may be partially due to state differences: in mixed-state feeds, each of the 3 pieces of news content took on a different state. The effects of state are further clarified in Table \ref{t:agreement-state}. The significant, positive \textsc{correction} in composited content indicate that provenance was helpful in correcting participants' perceived accuracy by shifting it towards the ground truth rating post-provenance. Lifestyle content had a slightly higher average \textsc{correction} than national content, possibly due to higher skepticism around national content even before provenance was introduced. Indeed, composited lifestyle content had an average pre-provenance perceived accuracy rating of \remove{2.17 }\add{3.09 (std. err = 0.06)} while the same metric for composited national content was \remove{1.79}\add{2.60 (std. err = 0.06)}. 

In no-state feeds, we did not find evidence of significant \textsc{correction} in media that agreed with their claims but did so in content that disagreed. In mixed-state feeds, we found significant and positive \textsc{correction} in all normal-state media, regardless of claim agreement. Incomplete- and invalid-state media had significant and negative \textsc{correction} when media agreed with their claims, but significant and positive \textsc{correction} otherwise. Running a Kruskal-Wallis test and Bonferroni-corrected Dunn posthoc tests revealed that the differences in changes \textit{between} states were significant for all variables discussed. In media that agreed with their claims, incomplete and invalid states actually pushed participants \textit{further away} from the ground truth rating. This is especially noticeable in the invalid state, and is a phenomenon not present in normal-state media nor media that disagreed with their claims. As such, incompleteness and invalidity in provenance may hinder provenance's corrective abilities in truthful content. However, they can aid correction in scenarios with more dubious content. 

To what extent were corrections effective? In general, larger \textsc{correction} corresponded to smaller $d_{rem}$. This was not the case with lifestyle and national content that had been composited: even a larger \textsc{correction} was not enough to close $d_{rem}$ more so for the former over the latter. Additionally, although provenance was sometimes helpful in correcting perceived accuracy, the corrections were relatively slight (usually within one Likert point). Participants also appeared to typically end up around one point away from the ground truth rating, suggesting that they were not hopelessly far off from the truth to begin with.


\subsection{RQ3: Differences in change in \textsc{trust}, \textsc{perceived accuracy}, and \textsc{correction} upon introduction of incomplete and invalid provenance chains}
To further investigate the effects of the incomplete and invalid states on perception (RQ.3), we measured significant differences in $\Delta t$ and $\Delta \alpha$ between posts in mixed-state feeds and those same posts in no-state feeds. Just like previous analyses, we included $\alpha_{corr}$ as a way to measure whether a participant's change in perceived accuracy was helping them towards or pushing them away from the ground truth rating. This is distinct from our analysis in Sections \ref{s:rq1} and \ref{s:rq2} as we are now measuring differences in responses \textit{across} participants rather than pre- and post-provenance responses \textit{within} participants. We used the Mann-Whitney U Test and its $n$-group generalization, the Kruskal-Wallis test, and found that both incompleteness and invalidity generally lowered trust and perceived accuracy compared to media that did not exhibit those states. We describe observations from each state in more detail.

\subsubsection{Incompleteness}
Out of the two posts on mixed-state feeds that were designated as incomplete, one was a non-composited news article and the other was a composited national image. 

For the news article, we observed significant differences in $\Delta t$ ($U = 31637.5, p < 0.001$) and $\Delta \alpha$ ($U = 33765.5, p < 0.001$) between no-state and incomplete variants of the post. The difference in means from the no-state post to the incomplete post was -0.48 \add{(std. err = 0.07) }and -0.43 \add{(std. err = 0.08) }for $\Delta t$ and $\Delta \alpha = \alpha_{corr}$, respectively. This means that participants who saw the incomplete variant generally trusted the media less and actually moved further away from the truth rating than those who saw the no-state counterpart. Ratings before provenance and after provenance (with and without state) are shown in Fig. \ref{fig:incomplete-vis}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/incomplete-vis.pdf}
    \caption{Trust and perceived accuracy ratings for the non-composited news article. Ratings pre-provenance, post-provenance (no state) and post-provenance (incomplete) are shown. Correction, equivalent to the change in perceived accuracy, is not shown explicitly.}
    \label{fig:incomplete-vis}
\end{figure}

For the national image, we found no evidence of significant differences in responses between the no-state post and its incomplete variant. 

\subsubsection{Invalidity}
Similar to incompleteness, two posts on mixed-state feeds were designated as invalid. One was a non-composited news article and the other was a non-composited national video. 

We observed significant differences between no-state and invalid variants of both posts. Here, $\Delta \alpha = \alpha_{corr}$ in both cases. For the news article, the results were ($U = 21544.0, p < 0.001$) and ($U = 22288.5, p < 0.001$) for $\Delta t$ and $\Delta \alpha = \alpha_{corr}$, respectively. For the video, they were ($U = 25858.0, p < 0.001$) and ($U = 31677.5, p < 0.001$), respectively. For the news article, the difference in means between the no-state variant and invalid variant were -1.13 \add{(std. err = 0.09) }and -1.16 \add{(std. err = 0.10) }for $\Delta t$ and $\Delta \alpha = \alpha_{corr}$, respectively, a considerably sharper drop than the incomplete state. For the video, they were -0.81 \add{(std. err = 0.08) }and -0.54\add{ (std. err = 0.09)}, respectively. The invalid state amplifies some of the patterns observed in the incomplete state, particularly in the news article: trust and perceived accuracy decreased noticeably in the invalid variant, even when the content aligns with its claim and is non-composited. Ratings before provenance and after provenance (with and without state) for both pieces of media are shown in Fig. \ref{fig:invalid-vis}.

\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/invalid-vis-1.pdf}
        \caption{}
        \label{fig:first}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/invalid-vis-2.pdf}
        \caption{}
        \label{fig:second}
    \end{subfigure}
        
\caption{Trust and perceived accuracy ratings for the non-composited news article (a) and non-composited national video (b). Ratings pre-provenance, post-provenance (no state) and post-provenance (invalid) are shown. Correction, equivalent to the change in perceived accuracy, is not shown explicitly.}
\label{fig:invalid-vis}
\end{figure}

\subsection{RQ4: Quantitative and qualitative findings across indicator designs and terminology variations}
\subsubsection{Quantitative findings}
RQ.4 explored the possibility of significant differences in dependent variables across indicator designs. Provenance states were represented with some stylistic variations across different indicator designs (shown row-wise in Fig. \ref{fig:design-spread}): some designs simply have a colour-changing icon, while others explicitly show the state name. Using a Kruskal-Wallis test, we found no evidence of significant differences in $\Delta t$ ($H(2) = 0.34, p = 0.84$), $\Delta \alpha$ ($H(2) = 0.80, p = 0.67$), $\alpha_{corr}$ ($H(2) = 1.98, p = 0.37$), and \textsc{comprehension} ($H(2) = 5.91, p = 0.05$) across any of the designs. While we did not expect $\Delta t$, $\Delta \alpha$, and $\alpha_{corr}$ to capture the differences in language variations, we failed to find evidence of significant differences in \textsc{comprehension} as well ($H(3) = 1.81, p = 0.61$). 

Overall, the provenance UIs were well-understood by participants\add{, suggesting that the novelty of the provenance interface was not a barrier in this study}. The mean comprehension rating was 4.11 (std. = 0.90) on a 5-point Likert scale, where 1 was ``Very unclear'' and 5 was ``Very clear''. Fig. \ref{f:comp} shows histograms of the comprehension ratings and its breakdowns by indicator design and terminology variation.

\begin{figure}[h!]
    \centering
    % \begin{subfigure}{0.33\textwidth}
    %     \includegraphics[width=\textwidth]{figures/comp-all.pdf}
    %     \caption{}
    %     \label{fig:first}
    % \end{subfigure}
    % \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/comp-design.pdf}
        \caption{}
        \label{fig:second}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/comp-language.pdf}
        \caption{}
        \label{fig:third}
\end{subfigure}
        
\caption{Histograms of participants' comprehension ratings by indicator design (a), and by terminology variation (b). Note that some participants saw only an provenance icon and no text descriptor.}
\label{f:comp}
\end{figure}

\subsubsection{Qualitative findings}
\label{s:qual}
Although we did not observe evidence for significant differences across indicator design and terminology variations, it does not imply total impartiality in those areas. Qualitative feedback revealed preferences and design shortcomings not captured quantitatively using our dependent variables. We received qualitative feedback through our free response question, and analyzing it provided takeaways and desiderata for improving the usability of provenance UIs. 223 out of the 595 participants provided meaningful feedback of more than 5 words. We identified four themes in their responses: \textsc{explainability}, \textsc{interactivity}, \textsc{visibility}, and \textsc{iconography/terminology}. A summary of the insights can be found in Table \ref{t:quotes}.


\textbf{\textsc{Explainability}}: participants generally wanted more explanations in the UIs to clarify terminology around both state (e.g. ``invalid'', ``incomplete'') as well as descriptions about the nature of the edit (e.g. ``size and position adjustments''). P136 echoed P152's sentiment in Table \ref{t:quotes}: \textit{``I wasn't sure of the validity when some items were marked "Incomplete." Defining that would help me better determine the validity of the source.''} Other participants suggested \textit{``Something such as `Likely invalid because of \_\_\_\_, or \_\_\_\_ indicates this as likely to be true.'''} and to \textit{``Change `incomplete credentials' to something more condemning of the information, so `unreliable source' or something similar''}, which is shows a slight misinterpretation of state, as potentially unreliable provenance information may not necessarily correspond to an unreliable source. More explanations around how state was determined may help dissipate confusion and reinforce state as an indicator of \textit{provenance} credibility over \textit{media content} credibility. 

Regarding text-based descriptions about the edits, participants wanted more clarity on whether they should still trust content with those edits: \textit{``Better descriptions, instead of "combined media" perhaps "uses elements from several sources" to be more clear that the content is manipulated and not single source.''} Multiple other participants mentioned that they wanted to see more detailed descriptions about the edits, as it was not always clear what the change was. Additionally, participants wanted a stronger link between the signer shown in the provenance details panel affects reliability of the piece of media. P37 stated \textit{``I just wasn't sure what some of the sources indicated. E.g. if Getty Images meant it was a stock photo. Or what it meant for a picture to come from Adobe but without obvious edits.''} This implies that users may automatically associate some signers with particular types of edits, which may or may not be justified. This association is made even more clear by P159: \textit{``I think I understood the `Adobe' information to mean that the image was manipulated but that was not totally clear.''} Some explanations to educate users on the role of the signer may help users avoid unsupported signer assumptions. This is especially important when a signer's reputation may be used a signal of reliability: \textit{``showing that pictures were created by reputable news organizations greatly helped me determine if they were real''} (P216).

\begin{table}[h]
\small
\renewcommand{\arraystretch}{1.5}
\centering
    \begin{tabular}[h]{p{0.5cm} | p{5cm} p{7cm}}
    \hline
     & Desiderata & Example Quote\\
    \hline
    \multirow{3}{=}[-25pt]{\begin{sideways}Explainability\end{sideways}} & Explain terminology in UI & ``I wasnâ€™t sure if invalid meant it failed a check or the check produced an invalid result.'' (P152)\\
     & More assertive explanations and terms & ``Change `incomplete credentials' to something more condemning of the information, so `unreliable source' or something similar.'' (P253)\\
     & More explanation of signer-edit relationship & ``I think I understood the `Adobe' information to mean that the image was manipulated but that was not totally clear.'' (P159)\\
     \hline
    \multirow{2}{=}[-11pt]{\begin{sideways}Interactivity\end{sideways}} & Reveal explanations on-demand through interaction & ``I think a tooltip explaining what the tampering means and what all of the other messages mean when you look at the history of the image would be nice'' (P6)\\
     & Enable a separate view with more details & ``A zoom feature that pops out an image in another window / fills screen.'' (P423)\\
    \hline
    \multirow{2}{=}[-25pt]{\begin{sideways}Visibility\end{sideways}} & Make indicator more noticeable & ``I think the icons could be a little bit larger to make them stand out slightly more.'' (P475)\\
     & Avoid blocking content with indicator & ``Less blocking of content would be nice. Maybe more in the corner or more transparent.'' (P9)\\
     & Use colour to attract attention to suspicious content & ``Better icons would be great; green for trustworthy, red for untrustworthy, grey for unsure.'' (P311)\\
     \hline
     \multirow{2}{=}[-22pt]{\begin{sideways}Icons/Terms\end{sideways}} & More specific icon on indicator & ``I would suggest using a magnifying glass icon because this tool allows us to examine the media in more detail.'' (P491)\\
     & Indicator icon should change according to edits & ``Maybe different labels and maybe more clearer icons, especially for importing and major image editing'' (P96)\\
     & Avoid or clearly define jargon & ``Maybe a definition of provenance because I didn't think about what it meant on the first one.'' (P14)\\
     \hline
    \end{tabular}
    \caption{Summary of themes, desiderata, and example quotes from participants' free responses.}
    \label{t:quotes}
\end{table}


\textbf{\textsc{Interactivity}}: to address some of the explainability issues, participants have recommended UI interactions that reveal supplementary information on-demand. A few participants suggested tooltips that provides definitions of terms when one hovers over them\add{: ``I think a tooltip explaining what the tampering means and what all of the other messages mean [...] would be nice'' (P6). Similarly, P327 desired ``clickable explanation[s] of the definition of terms, e.g. `incomplete.'''} Many indicated interest in accessing more provenance information besides those displayed in the details panel and clicked on the ``View More'' button, but the button was not a functional one in this study. Many also had ideas for ways to further explore the media in intermediate steps of the provenance chain: the most commonly-mentioned features were zooming further into the content to inspect it, and an expanded view that open the details panel up into a full screen and allow direct comparisons with other versions. \add{For instance, P423 suggested ``a zoom feature that pops out an image in another
window/fills screen,'' while P328 added that such UIs should contain ``clickable links to the original documents/sources.''} 

Conveniently, the expanded view participants described resembles C2PA's provenance UI \cite{c2pa-ux} at one level of detail higher than the provenance details panel that appeared in our feeds under the indicator. This view was not implemented in our study but can be accessed from the ``View More'' button in deployment-ready versions of the UI. While we did not study this expanded view, we can confirm many design choices and features with our qualitative data. 

\textbf{\textsc{Visibility}}: Some participants mentioned that the provenance indicator was not noticeable enough. A couple mentioned they would not have noticed the indicator if it wasn't for the pictorial overview of the UIs before the second round of media evaluations. P475 suggested larger icons: \textit{``I think the icons could be a little bit larger to make them stand out slightly more,''} which was also echoed by several other participants. Since the indicator was overlaid on top of the media, poor background contrast also presented visibility concerns: \textit{``I guess it could use an icon that contrasts more with the background. I didn't notice it immediately''} (P543). 

Despite some indicator designs having a smaller surface area and potentially having less visibility than others (e.g. the singular icons in Fig. \ref{fig:design-spread}, there was no clear correlation between the design assigned to the participant and them mentioning poor visibility. On the other hand, some participants also thought the indicators were too obstructive. Participants thought the indicator should be \textit{``maybe placed differently/not covering image''} and also suggested increasing opacity to interfere less with the underlying content. This reveals a fine (and potentially subjective) line between acceptable visibility and acceptable obtrusiveness. 

In addition to indicator visibility, many participants also brought up the use of colour and how it can help increase the visibility of edits. P181 wanted edits to be \textit{``color [assigned] to indicate more obviously if things seem suspicious.''} Icon visibility can also be aided by using brighter colour to \textit{``attract attention''} and \textit{``make it stand out more''}. In general, it seems like participants wanted a colour to provide a quick and easy way to tell whether they should trust a piece of media. P340 suggested to use \textit{``a more simple colour coded green, amber, red system of authenticity.''} P311 expressed similar thoughts in Table \ref{t:quotes}. P225 wanted the signal to be even more explicit: \textit{``Manipulated in red letters would be very helpful.''} While clarity is important, we are concerned that using colour this way could misrepresent the relationship between provenance and content. The confusion can be seen in P97's response: \textit{``I like the color change that indicated there could be a problem with the image.''} Here, colour change was indicating a potential issue with  provenance data, not necessarily in the image itself. More user education may be needed before using colour in the way participants suggested.

\textbf{\textsc{Iconography/Terminology}}: Participants noticed the the icon in the provenance indicators were somewhat generic (P93 called it \textit{``just a random one it seems''}) and had suggestions for alternative designs. P301 suggested a question mark, but did not experience the mixed-state feed and was not aware that the question mark was already used in the incomplete state. P491 suggested a magnifying glass, but also acknowledges that \textit{``this may confuse people into thinking it would enlarge the image.''} Dynamic icons were also proposed by P96 in Table \ref{t:quotes}. P267 was more imaginative: \textit{``the symbol could maybe be a small plant growing?''} At least a couple participants initially confused the icon with the three dots ``...'' on the side of the post, which was a non-functional button in our study. Thus, we can see how using recognizable UI visual language in the indicator may be a natural choice but presents challenges when the language overlaps with UI on current platforms. Participants also had suggestions for terminology used on the indicators, particularly those who saw the term ``provenance.'' P184 made a point about how \textit{``Some people might not be familiar with the word `provenance.' `Origin' might be more familiar,''} while P14 wanted \textit{``a definition of provenance because I didn't think about what it meant on the first one.''} This feedback calls for terminology that is simple yet precise, in line with previous findings on provenance indicator designs for videos \cite{sherman2021designing}.