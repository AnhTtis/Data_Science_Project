We explore how perceptions of images and videos on social media change upon the introduction of media provenance information. To do this, we deployed an online experiment with a repeated measures design featuring two social media feed prototypes---a regular, no-provenance one and a provenance-enabled one---with participants in the US and UK. Unlike prior work, which provides empirical and theoretical evidence as to why provenance may be helpful to users \cite{emily2022usable, sherman2021designing, gundecha2013tool, yazici2018data, yazici2021usability}, we integrate provenance into an interactive social media environment and directly observe its effects. 

We developed an interface combining a survey and social media prototype for our experiment. Following some preliminary questions about social media use, participants completed two rounds of media evaluations using a mock Twitter-like feed. The first round used a regular feed, while the second used a provenance-enabled feed (see Fig. \ref{fig:l1-l2-b}) with different UI variants (see Fig. \ref{fig:design-spread} in Appendix \ref{a:design-spread}) randomly assigned to participants. The feed automatically scrolled to and highlighted the post associated with the survey question (see Fig. \ref{fig:system}). After both rounds, participants completed an exit survey.

\begin{figure}[t]
    \centering
    \begin{subfigure}[h]{0.3\textheight}
        \centering
        \includegraphics[height=\textwidth]{l1.png}
        \caption{}
        \label{fig:l1-l2-a}
    \end{subfigure}
    \hspace{.1cm}
    \begin{subfigure}[h]{0.3\textheight}
        \centering
        \includegraphics[height=\textwidth]{l2-open.png}
        \caption{}
        \label{fig:l1-l2-b}
    \end{subfigure}
    \caption{A provenance indicator in a normal state is displayed in \ref{fig:l1-l2-a}. Once the user clicks on it, a provenance details panel will open up, as shown in \ref{fig:l1-l2-b}.}
    \label{fig:l1-l2}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{system}
    \caption{The experiment interface in a side-by-side view during a media evaluation question. The participant responds to the survey on the left while the prototype highlights the relevant post in the feed on the right. Although only one post is highlighted, the participant can still freely interact with the rest of the prototype.}
    \label{fig:system}
\end{figure}

%We built an interface for this experiment that combined a survey and social media prototype into one cohesive user experience for the participant. Participants were first onboarded to the study and answered a few preliminary survey questions about social media usage patterns. Then, each participant completed two rounds of media evaluations where a mock Twitter-like feed appeared next to survey questions about the perceived credibility of pieces of media on the feed. The feed automatically scrolled to and highlighted the post associated with the question the participant is currently viewing (see Fig. \ref{fig:system}). The first round of evaluations was completed with a regular feed, whereas the second was completed with a provenance-enabled feed with special UIs that displayed media provenance information (see Fig. \ref{fig:l1-l2-b}). Because we tested different variations of UIs in the provenance-enabled feed (see Fig. \ref{fig:design-spread}), our system randomly assigned participants to one of the variants in the second round. After the both rounds were completed, the prototype disappeared from view and participants completed an exit survey. 

We further describe our study artifacts and experimental design below.

\subsection{Experiment Interface}
Our experiment interface consisted of a Qualtrics survey, a prototype of a Twitter-like feed, and an overarching system that orchestrates communication between the two.

\subsubsection{Survey}
We collected our data through a Qualtrics survey. The survey began by asking participants about their use of social media platforms, including which platform(s) they use, how frequently they use them, and characteristics that prompt them to question the trustworthiness of a piece of media. 

Afterwards, participants entered two rounds of media evaluation. The first round was the control round of our study, where participants responded to two 5-point Likert scale questions for each post they saw on a no-provenance Twitter feed, similar to a feed they would see on the platform at the time of writing. The two questions measured the two variables (one variable each) used to represent participants' perceptions of media credibility: \textsc{perceived accuracy} and \textsc{trust}. The variables and their corresponding questions are as follows:
\begin{enumerate}
    \item \textsc{perceived accuracy}: \textit{``The following claim may be associated with the post's [image/video]: [claim]. Indicate your agreement with this claim.''}
    \item \textsc{trust}:\textit{``Consider the image in the post by [account name] ([handle]). Indicate how much you trust this image to give you reliable information.''}
\end{enumerate}

Before entering the experimental round, participants were shown a pictorial overview of the provenance UIs we implemented, including their interactive features. In this round, they responded to the same scalar rating questions and saw the same posts as the control round, but did so in a provenance-enabled feed featuring the UIs they were debriefed on earlier. \add{Using the same media across both rounds allowed us to attribute users' changes in perception to the introduction of provenance, eliminating perceptual variations due to differing media as a confounding variable.}

Once they finished, they were taken to an exit survey where they rated their overall understanding of the provenance UIs' functionalities on a 5-point Likert scale, wrote any feedback they had for improving the UIs in a free response question, and completed demographics questions. Select screenshots of the survey can be found in Appendix \ref{a:survey}.

\subsubsection{Social Media Prototype}
\label{s:prototype}
Our social media prototype allowed participants to view and interact with media-embedded posts in a Twitter-like setting. The prototype was a web app built using Svelte\footnote{https://svelte.dev/}. It replicated the appearance of a light-theme Twitter feed and was fully responsive. The number of likes, retweets, and comments were randomized for each post. The order of posts was randomized \textit{between} participants, but were preserved \textit{within} each participant. Besides 3 news posts which we set to be authored by the real Twitter accounts of international news outlets, we generated usernames and handles for the posting accounts using a random name generator, and performed post-hoc editing as needed to ensure there was variance in username styles and genders. We also generated a range of profile photos, selecting those with human faces as well as images and graphics without people. None of the accounts on the feed were verified except for the news outlets. Unlike Twitter, participants could not click into the profile of the posting accounts, nor could they hover over their names and see a profile preview with user statistics such as the number of followers.

Our prototype also integrated analytics tracking. Some of its tracking capabilities included session duration, clicks for various parts of the UI (including open and close clicks where applicable), and interaction timestamps. This allowed us to ensure the participant successfully loaded and interacted with the prototype during the media evaluation rounds. 


\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{l2-anatomy}
    \caption{The media journey and the corresponding provenance details panel, including a breakdown of the panel anatomy.}
    \label{fig:l2-anatomy}
\end{figure}

\subsubsection{Provenance UIs}
The feed for the control round did not contain any provenance information, much like a conventional Twitter feed. The provenance-enabled feed in the experimental round, however,  prototypes an implementation of the C2PA standard \cite{c2pa}, an open technical standard that allows media authoring tools to embed cryptographic signatures into the media's metadata. The data can be parsed by the client and displayed to the user through a UI. We implemented the UI in accordance with C2PA's UX guidelines \cite{c2pa-ux}: the user can click on an icon that floats on the media's top right corner---a \textbf{provenance indicator} (see Fig. \ref{fig:l1-l2-a})---to open up a panel displaying a chain of provenance information---a \textbf{provenance details panel} (see Fig. \ref{fig:l1-l2-b}). The anatomy of an example provenance details panel can be seen in Fig. \ref{fig:l2-anatomy}. In our study, we use \textbf{provenance UIs} to refer to the combination of the indicator and details panel. Before entering the experimental round, participants were given a simplified, graphical overview showing that the details panel can be accessed by clicking on the indicator, but were not primed on any specifics within the UIs.

The provenance information can take on three states, as defined by the C2PA standard: \textbf{normal}, \textbf{incomplete}, and \textbf{invalid}. \add{A \textbf{normal} state indicates that there are no abnormalities with the provenance chain. The data carries an \textbf{incomplete} state if the chain was not properly updated after an edit (e.g. an edit was made in a non-C2PA compliant tool). The data is \textbf{invalid} if the chain has been intentionally tampered with, and any portion of the chain before the tampering occurred should be disregarded.} We did not educate participants on the meaning of these states prior to the experimental round to better simulate initial exposure to real-world deployment \addd{through a specific UI representation, to better capture the usability of such a representation}. For example, Twitter did not provide a tutorial for users when introducing new terminology to its warning labels \cite{ap-twitter-labels}, nor did Facebook for its misinformation alerts \cite{fb-alerts}. 

Fig. \ref{fig:l2-anatomy} shows the anatomy of a provenance details panel alongside the corresponding media journey (how the media was created, edited, and published). A details panel consists of a series of manifests, where each manifest consists of a signer name, a signer date, edit details and/or production details. Content that has been composited also has a content gallery showing the original media. According to the C2PA UX guidelines \cite{c2pa-ux}, the Expand button leads to an even more detailed view of the provenance information, but we did not implement that functionality in our prototype. Instead, we tracked its clicks to gauge user interest in further interaction. To craft the manifests within each panel, we used the backstory that Snopes uncovered for each piece of media and fabricated likely signers and details that would fit with the story. For the 3 pieces of news media not from Snopes, we followed the same process but used the image credits in the article instead. We limited the number of manifests in a panel to 2–3, following feedback from our pilot studies that any more would be too confusing.


\subsubsection{Overarching System}
The primary motivation for creating the overarching system was to ensure that participants were actually looking at the posts referred to in the survey as they answered the questions. We implemented the system such that it hosted the survey and prototype simultaneously, and allowed the survey to communicate a question ID to the prototype so the prototype can scroll to and highlight the associated post. %embedded JavaScript snippets into survey questions that referenced media, using the \texttt{postMessage()} method to allow them to broadcast the question ID out to any parent window. We implemented a functionality in the prototype that allowed it to receive a question ID and then scroll to and highlight a specific post in the feed. We then created an overarching system that embeds both survey and prototype as \texttt{iframes}, takes messages broadcasted from the survey, and forwards them along to the prototype. 
The system also helped with study mechanics and improving the overall study-taking experience. It randomly assigned participants to one of the many design variants in the experimental round and linked survey response IDs to prototype sessions. %We tested different variations of indicators in the provenance-enabled feed, and the system randomly assigned participant to one of those conditions after the first round of media evaluations. In addition to question IDs, the system also passed along the Qualtrics response ID so we could link Qualtrics responses to prototype sessions. 
It full-screened the survey when the prototype was not needed, and put the survey and prototype side-by-side in a 50\% split-screen view when it was. 

% The resulting user experience for the study was the following. Participants were onboarded to the study and answered survey questions about social media use in a full-screen view. Then, the participant completed two rounds of media evaluation where a mock Twitter-like feed appeared next to their questions in each round, scrolling to and highlighting the post associated with survey question the participant is currently viewing (see Fig. \ref{fig:system}). Finally, the prototype disappeared and participants completed the exit survey in full-screen mode. 





\subsection{Media Selection}
\label{s:media-selection}
Rather than fabricating our own images and videos to use in our study, we followed examples from prior works \cite{fan2020juries, pan2022comparing} and used media from real social media posts. Our aim in doing this was to reduce biases we hold as a result of deriving our research questions and hypotheses, which may differ from real-world decisions \cite{kuhberger2002framing} and therefore impose unwarranted effects on the appearance of the media. We drew most of our media from the Snopes fauxtography archives\footnote{https://www.snopes.com/fact-check/category/photos/}, which contains fact-checking investigations of images and videos that have previously been posted on social media. Because the investigations uncover some backstory of the media---for example, what the original image for an edited version looks like---it provided us with reliable sources from which we craft provenance information for that piece of media. Snopes also provided a claim with every entry in the archive (e.g. ``A photograph shows a meteor falling into a volcano''), which we used in our study in one of our measures of media credibility perception. 

We varied the topics of media throughout our feed to minimize potential perceptive bias one may hold towards content of a particular topic; we did this following previous work that suggests the presence of this bias in social media users \cite{flintham2018falling}. We organized our media into three non-overlapping categories of \textsc{media topics} commonly seen in social media feeds: \textbf{news}, \textbf{lifestyle}, and \textbf{national}. \textbf{News} content comprised of mainstream news articles from well-known international news outlets, all published about two months before the deployment of the study. Their header images, titles, and subtitles were displayed in our prototype in the same style as Twitter's Summary Card UI \cite{summary-cards}, while their subtitles were repeated as the posts' caption. \textbf{Lifestyle} content was content intended to trigger intrigue from the viewer, existing primarily for entertainment. The image or video was displayed in the post, along with an accompanying caption that has been posted with the media before elsewhere on the internet. \textbf{National} content was specific to a certain country and included political content as well as content meant to invoke nationalist sentiments. The images and videos, along with the post captions, were displayed in the same way as lifestyle content. The Snopes archive boasted an abundance of lifestyle and national content, but lacked mainstream news articles, so we selected our news media separately. 

Our media also each took on one of two \textsc{edit statuses}: \textbf{non-composited} and \textbf{composited}. \textbf{Non-composited} media did not undergo major graphical modifications---they were either an untouched original copy or had been edited in a way that did not change their visual contents, such as resizing. \textbf{Composited} media had been edited significantly from the original and combined two or more pieces of media together. Additionally, our media were assigned one of two \textsc{claim agreements}, based on their relationship with their Snopes claim: \textbf{agree} and \textbf{disagree}. If a piece of media's claim states what it authentically shows, then the two are considered to agree, and disagree otherwise. For the 3 news articles not from the Snopes archive, we constructed claims for them by objectively stating the content in the articles' header images; thus all news article agreed with their claims. \add{While we discussed \textit{perceived} accuracy in Section \ref{s:rw-measures}, \textsc{claim agreement} here can be thought of as \textit{ground truth} accuracy---the truthful rating one would select if they evaluated the claim while having access to the media's entire backstory. For example, an image that disagrees with its claim is is also inaccurately represented by it\footnote{Consider a composited image depicting a mountain against a blue sky with the claim ``This is an authentic image showing a mountain against a blue sky.'' The image was in fact originally one of a mountain against a gray sky. The claim therefore inaccurately describes the image's authenticity. Someone who has access to this information via provenance should truthfully select 1 (Definitely disagree) while evaluating the claim for the image.}. We use this objective measure to ground our subjective measures, as prior work has done when evaluating content credibility \cite{bhuiyan2020experts}.}

Each participant saw 9 pieces of media. 7 were images and 2 were videos. Every participant saw the same 6 pieces of news and lifestyle media, while US and UK participants saw 3 pieces of American and British national media, respectively. 7 pieces of media were non-composited, while 2 were composited. 5 pieces of media agreed with their claims, while 4 did not. Both pieces of composited media did not agree with their claims. \add{That said, a piece of media does not have to be composited to disagree with its claim\footnote{Consider the following example from our media collection: an authentic photo of a meteor flying over a volcano is taken at an angle such that it looked like the meteor was heading into the volcano itself. The image was reposted with a caption claiming that a meteor has flown into a volcano, but scientific calculations showed that the meteor likely fell about 10km to the north. This information was corroborated by the original photographer, who said he did not hear any crashing noises indicative of the meteor actually striking the volcano \cite{snopes-volcano}.}---we used 2 pieces of such media in our study.} Descriptions of all media, along with their media properties and additional independent variables, are displayed in Appendix \ref{a:all-media} Table \ref{t:media-table}.


\subsection{Experimental Design}

We conducted a repeated measures study using our experiment interface to examine the differences in media credibility perception between no-provenance and provenance-enabled Twitter-like feeds. Since we also wanted to see whether varying the provenance indicator designs had a significant effect on perception, participants were randomly assigned to a design variation in the provenance-enabled feed. We compensated participants at a rate of \$14.49 USD per hour, in line with the state minimum wage. The average completion time for our study was around 15 minutes. Our study design was reviewed and approved by our institution’s Institutional Review Board (IRB) under protocol ID 00014901.

\subsubsection{Data Validation}

We placed two attention check questions in our survey, immediately before participants started each round of media evaluation. The questions referenced prototype content and instructions, serving as an indicator that the prototype successfully loaded for participants in addition to observing attention. We eliminated responses from participants who answered both attention check questions incorrectly. We also eliminated responses from participants who did not leave any traces of activity or open any sessions on our prototype (as per prototype analytics). Finally, since most participants took more than 10 minutes to complete the study, we eliminated responses that used less than 5 minutes. 

\subsubsection{Participants} We recruited participants from the US and UK between the ages of 18 and 65 using Prolific\footnote{https://www.prolific.co}, which offered more rigorous social media pre-screening criteria than Amazon Mechanical Turk. We recruited 5 participants from each region as a pilot to test out the platform and our interface. We modified the wording of our attention check questions upon feedback. For the main study, we recruited a total of 690 participants (360 from the US, 330 from the UK), resulting in a total of 595 valid responses (298 from the US, 297 from the UK). This exceeded our target of 540 valid responses based on power analysis aiming to detect a power of 0.7 with a significance criterion of $\alpha = 0.05$. We recruited in 8 batches for both regions to ameliorate demographic skews: after each batch, we ran a demographics analysis on our participant pool and compared it against the most recent official census data for each country. We did this to figure out which pre-screening criteria we should adjust to better match the census demographics. We used the 2020 Census for the US \cite{us-census} and the 2011 Census for the UK \cite{uk-census}\add{, as they were the most recent census datasets available for the respective countries at the time of our study}. 

All participants used one or more of Facebook, YouTube, Twitter, Instagram, and Reddit at least once a month, with 87\% reporting daily use. 52\% identified as female, 46\% as male, 1\% as non-binary, and the rest did not disclose. 14\% of participants were between 18 and 24, 46\% between 25 and 44, and 40\% between 45 and 65. 86\% reported as White, 6\% as Black, 5\% as Asian, and 3\% as Mixed Race. Racial breakdown by region is available in Tables \ref{t:demo-us} and \ref{t:demo-uk} in Appendix \ref{a:demo-region}, where region-specific demographics may also be found. All participants except for 1 completed at least high school: 36\% reported their highest educational attainment was a high school degree, 16\% had an associate's degree or equivalent vocational training, 33\% had a Bachelor's degree, 13\% had a Master's degree, and 2\% had a Ph.D..

\subsubsection{Manipulation of Independent Variables}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{stateful-l2}
    \caption{The provenance UIs for various states. From left to right: normal, incomplete, invalid. Note the colour change in the indicator and the addition of incomplete or invalid descriptors in the details panel.}
    \label{fig:stateful-l2}
\end{figure}

As mentioned in Section \ref{s:prototype}, we included 3 \textsc{provenance states} in our study: normal, incomplete, and invalid. State impacted the appearance of the provenance indicator and the contents of the provenance details panel, as shown in Fig. \ref{fig:stateful-l2}. We had specific media we would assign as incomplete or invalid for consistency across feeds. However, since we were interested in isolating the effect of incomplete and invalid states on media credibility perception, not all feeds displayed the full spectrum of states. About half of the feeds saw the presence of all 3 states, while the other half did not have variable states and defaulted to the normal state indicator across all posts. We call the former \textbf{mixed-state feeds} and the latter \textbf{no-state feeds}. 



Additionally, we deployed different variations of \textsc{indicator designs} and \textsc{indicator terminology} in our provenance indicators. We used 3 design variations: one was a simple circular icon with an ``i'' in it, one was the icon with a descriptive string (e.g. content provenance), and one was the icon and the descriptive string plus a secondary string. The secondary string bears the name of the most recent signer in the normal state, and the name of the state (e.g. Invalid) otherwise. We used 4 terminology variations in describing provenance in the UIs: ``Content Credentials'', ``Content Attribution'', ``Content History'', and ``Content Provenance.'' These variations appeared in the provenance indicators as well as the panel title in the provenance details panel (see Fig. \ref{fig:l2-anatomy}). In total, we had 18 different variations of indicators in the provenance-enabled feed, shown in Appendix \ref{a:design-spread} Fig. \ref{fig:design-spread}. Our experiment system randomly assigned participants to one of those variations after the control round.  

We also have 3 variables from Section \ref{s:media-selection} that we can treat as independent variables: \textsc{media topic category}, \textsc{edit status}, and \textsc{claim agreement}. All of our independent variables discussed are summarized in Table \ref{t:ivs}. The first 4 variables are considered properties of the media and their assignments to each piece of media in the feeds can be seen in Appendix \ref{a:all-media} Table \ref{t:media-table}.

\begin{table}
\small
\centering
    \begin{tabular}[h]{p{3cm} p{10cm}}
    \toprule
    Independent Variable & Levels\\
    \midrule 
    Media Topic & News, lifestyle, national \\
    Provenance State* & Normal, incomplete, invalid \\
    Edit Status & Non-composited, composited \\
    Claim Agreement & Agree, disagree \\
    Indicator Design & Icon only, icon + string, icon + string + secondary string  \\
    Indicator terminology & Content Credentials, Content Attribution, Content History, Content Provenance \\
    \bottomrule
    \end{tabular}
    \caption{Independent variables and their levels. *Only applies to mixed-state feeds.}
    \label{t:ivs}
\end{table}

\subsubsection{Measurements and Dependent Variables}
We quantitatively measured two ordinal variables that represent \add{credibility perception}: \add{\textsc{perceived accuracy}} and \textsc{trust}. \add{\textsc{Perceived accuracy}} is a participant's response to the question \textit{``The following claim may be associated with the post's [image/video]: [claim]. Indicate your agreement with this claim.''} The claim referred to here is the same claim described in \ref{s:media-selection}---we used the claim provided by Snopes where applicable. The response is in the form of a numerical rating on a 5-point Likert scale, where 1 is \textit{Definitely disagree} and 5 is \textit{Definitely agree}. \add{While we could have asked for accuracy ratings directly, as prior work has done \cite{jahanbakhsh2021nudges, pennycook2019lazy, pennycook2020implied}, we were concerned that doing so would shift the evaluation of accuracy more towards the \textit{factual contents of the claim itself} (which has little relevance to provenance) rather than the \textit{relationship of the claim with the media}. Participants may then be more likely to perform web searches to verify information in the claims, which we wanted to avoid as the claims are published by Snopes and the media-claim relationships are readily available on the internet. Prior work in psychology has used participants' agreement with a piece of information as a reliable proxy for their perceived accuracy \cite{berry2018believability, riggio1987social}. We take inspiration from this and ask for claim agreement instead of accuracy directly to better capture participants' impressions of accuracy via the claims' alignment with their media. }The \textsc{trust} is a participant's response to the question \textit{``Consider the image in the post by [account name] ([handle]). Indicate how much you trust this image to give you reliable information.''} Just like \textsc{perceived accuracy}, the response is a numerical rating on a 5-point Likert scale, where 1 is \textit{Definitely not trustworthy} and 5 is \textit{Definitely trustworthy}. These variables were collected for both the control and experimental rounds. Let us denote \textsc{perceived accuracy} and \textsc{trust} in the control round as $\alpha_c$ and $t_c$, respectively, and in the experiment as $\alpha_e$ and $t_e$.

These measured variables are useful when we check for \textit{whether there exists significant differences} in responses between the two rounds, but not so much in \textit{whether and how those differences change between treatment conditions} should they exist. We process the measured variables into 3 dependent variables we can use for this. 

\begin{enumerate}
    \item \textsc{Change in trust}, denoted by $\Delta t$, is calculated by $t_e - t_c$. 
    \item \textsc{Change in perceived accuracy}, denoted by $\Delta \alpha$, is calculated by $\alpha_e - \alpha_c$.
    \item \textsc{Correction}, denoted as $\alpha_{corr}$, is $\Delta \alpha$ relative to a post's \textit{true agreement/accuracy} ( denoted $A$). Every piece of media has a ground truth \textsc{claim agreement} associated with it (see Section \ref{s:media-selection}). Correction is calculated by $|\alpha_c - A| - |\alpha_e - A|$. Note that we did not set a ground truth for trust due to its subjectivity.
\end{enumerate}


\begin{table}
\small
\centering
    \begin{tabular}[h]{p{5cm} p{4cm} p{3.7cm}}
    \toprule
    Dependent Variable & Formula (If Applicable) & Used In\\
    \midrule 
    Perceived accuracy ($\alpha_c$ and $\alpha_e$) & N/A & All\\
    Trust ($t_c$ and $t_e$) & N/A & All\\
    Change in perceived accuracy ($\Delta \alpha$) & $\alpha_e - \alpha_c$ & RQ.3, RQ.4\\
    Change in trust ($\Delta t$) & $t_e - t_c$ & RQ.3, RQ.4\\
    Correction ($\alpha_{corr}$) & $|\alpha_c - A| - |\alpha_e - A|$ & RQ.2, RQ.3, RQ.4 \\
    Comprehension & N/A & RQ.4 \\
    \bottomrule
    \end{tabular}
    \caption{Dependent variables with their formulas and where it was used. A N/A in the Formula column means the variable was measured directly.}
    \label{t:dvs}
\end{table}

Additionally, it was important for us to know whether the provenance UIs were even comprehensible to participants. We asked the following question in our survey after both media evaluation rounds: \textit{``For the interactive media UIs that were only in second social media feed, did you have a clear idea of what the UIs did? Rate your understanding of their functionality from a scale of Very unclear (1) to Very clear (5).''} RQ.4 probed participant comprehension, which we could not capture well with the other dependent variables, so we captured the Likert scale ratings for the aforementioned question in a variable called \textsc{comprehension} and used it as a dependent variable.


Our dependent variables, along with which research questions they were used for, are summarized in Table \ref{t:dvs}.



\subsubsection{Quantitative Analysis}
\label{s:quant-analysis}
First, we wanted to find out which independent variables were most important in contributing to the participant's rating of \textsc{perceived accuracy} and \textsc{trust}, as well as identify any interaction effects between the variables. To this end, we used the independent variables to create two cumulative link mixed models (CLMMs) with \textsc{perceived accuracy} and \textsc{trust} as the respective response variables, setting \textsc{media topic}, \textsc{provenance state}, \textsc{edit status}, \textsc{claim agreement} as fixed effects, and \textsc{indicator design} and \textsc{indicator terminology} as random effects. We chose to use CLMMs due to the ordinal nature of our measured variables and random effects that may be present in \textsc{Indicator design} and \textsc{Indicator terminology} due to random assignment of participants to indicator variations in the experimental round. 

%We then checked for significant differences in \textsc{agreement} and \textsc{trust} between the control and experiment rounds to find out if introducing provenance had any effect at all. We used a Wilcoxon signed-rank test to do this. We ran the test aggregating across all media, as well as grouping by media characteristics such as edit status. If results were significant, we identified direction and magnitude of change by computing a difference in means across the two groups. We also checked for significant differences in $\Delta a$, $\Delta t$, and $a_{imp}$ across independent variable levels. We used the Mann-Whitney U Test (computing difference in means where significant) when there were two groups, and the Kruskal-Wallis test (with Dunn's posthoc test and Bonferroni correction for pairwise analysis where significant) when there were more than two. Overall, we chose non-parametric tests for our analyses due to the ordinal nature of our dependent variables.

We then checked for significant differences in \textsc{trust}, \textsc{perceived accuracy}, and \textsc{correction} between the control and experiment rounds to find out if introducing provenance had any significant effects at all. That is, we checked for evidence of non-negligible $\Delta \alpha$, $\Delta t$, and $a_{imp}$. We used a Wilcoxon signed-rank test to do this, as the pre- and post-provenance ratings are always matching samples. If results were significant, we identified the direction and magnitude of change by computing a difference in means across the two groups. We structured our reporting based on the significant interaction effects revealed by our CLMMs: if interactions existed between two variables, we separated the results into the variables' levels to avoid aggregating across those effects. When we check for significant differences in $\Delta \alpha$, $\Delta t$, and $a_{imp}$ \textit{across} independent variable levels, we used the Mann-Whitney U Test for independent groups (computing difference in means where significant) when there were two groups, and the Kruskal-Wallis test (with Dunn's posthoc test and Bonferroni correction for pairwise analysis where significant) when there were more than two. Overall, we chose non-parametric tests for our analyses due to the ordinal nature of our dependent variables.

\subsubsection{Qualitative Analysis}

Our survey included the following free response question as part of the exit survey: \textit{``Do you have any feedback to improve the interactive media UIs that appeared in the second social media feed? Examples include suggestions of a better icon, different name, etc.''} 223 out of the 595 participants gave meaningful responses longer than 5 words. To extract insights from the responses, one author took two passes through the data, first performing an open coding procedure to capture meaningful comments and suggestions, and then organizing the results into broader themes through axial coding. The result was the identification of 4 categories of desiderata for future provenance UIs: \textsc{explainability}, \textsc{interactivity}, \textsc{visibility}, and \textsc{iconography/terminology}.
