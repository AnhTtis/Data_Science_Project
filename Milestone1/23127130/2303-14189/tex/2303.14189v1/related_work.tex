\section{Related Work}
\label{sec:related_work}

For the past decade, convolutional neural networks have been the standard architecture for vision models~\cite{resnet, RegNet, ResNeXt, EfficientNet, efficientnet_v2_quoc, MobileNet_v1, MobileNet_v2, Mobilenet_v3, ConvNext, Ding_2021_repvgg, mobileone}. More recently, transformers have shown great success on computer vision tasks~\cite{ViT, vitc_arxiv, deit, Swin, PVT_v1, cait, litv1, litv2}. Unlike convolutional layers, the self-attention layers in vision transformers provide a global context by modeling long-range dependencies. Unfortunately, this global scope often comes at a high computational price~\cite{marin2021token}. Works like~\cite{litv2, linformer, reformer, marin2021token} address ways to alleviate computational costs associated with self-attention layers. In our work, we explore an efficient alternative to self-attention layers for lower latency.

\vspace{-3.5mm}
\paragraph{Hybrid Vision Transformers} In order to design efficient networks while maintaining accuracy,
recent works introduce hybrid architectures that combine convolutional and transformer design to effectively capture local and global information.
Some designs replace the patchify stem~\cite{ViT} with convolutional layers~\cite{vitc_arxiv}, introduce early convolutional stages~\cite{CoAtNet,litv2} or implicitly hybridize through windowed attention~\cite{Swin, Twins}. More recent works build explicit hybrid structures for better exchange of information between tokens (or patches)~\cite{Cmt, d2021convit, cvt}. In majority of the hybrid architectures, token mixers are predominantly self-attention based. Recently, MetaFormer~\cite{metaformer} introduced Pooling, a simple and efficient candidate for token mixing.

\vspace{-3.5mm}
\paragraph{Structural Reparameterization} Recent work~\cite{Ding_2021_repvgg, mobileone} shows the benefits of reparameterizing skip connections to lower memory access cost. In our model, we introduce a novel architectural component \textit{RepMixer}, that is reparameterizable at inference. For better efficiency, works like~\cite{MobileNet_v1, MobileNet_v2, Mobilenet_v3, ShuffleNet_v2, ShuffleNet} introduce factorized k$\times$k convolutions using depthwise or grouped convolutions followed by 1$\times$1 pointwise convolutions. While this approach is highly effective in improving the overall efficiency of the model, the lower parameter count can lead to reduced capacity. Recently, linear train-time over overparameterization was introduced in~\cite{mobileone, Ding_2019_ICCV, ding2021diverse} to improve capacity of such models. We use factorized k$\times$k convolutions in our model and boost the capacity of these layers using linear train-time overparameterization. To the best of our knowledge, structural reparameterization to remove skip connections and linear overparameterization has not been attempted in any prior hybrid transformer architecture.

