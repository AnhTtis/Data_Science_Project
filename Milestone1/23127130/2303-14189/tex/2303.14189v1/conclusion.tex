\vspace{-2mm}
\section{Conclusion}
We have proposed a general purpose hybrid vision transformer that is highly efficient on multiple compute fabrics: mobile devices and desktop grade GPUs. 
Through structural reparameterization, our model incurs reduced memory access cost. This leads to significant improvements in runtime especially at higher resolutions. In addition, we propose further architectural changes that boosts performance on ImageNet classification task and other downstream tasks like object detection, semantic segmentation and 3D hand mesh estimation. We empirically show that our backbone is highly robust to out-of-distribution samples, while being significantly faster than competing robust models.

\label{sec:conclusion}
