\section{Experiments}
\label{sec:experiments}

\subsection{Image Classification}\label{sec:imagenet_exps}

We report results on the the widely used ImageNet-1K dataset~\cite{ImageNet-1K} which contains $\sim$1.3M training images and 50K validation images. We follow the training recipe prescribed in ~\cite{deit, metaformer}, i.e. the models are trained for 300 epochs using AdamW optimizer with weight decay of 0.05 and peak learning rate $10^{-3}$ for a total batch size of 1024. The number of warmup epochs is set to 5 and cosine schedule is used to decay the learning rate. Our implementation uses timm library~\cite{timm} and all the models were trained on 8 NVIDIA A100 GPUs. See supplementary materials for details on hyper parameters used for all the variants. For 384$\times$384 input size, we fine-tune the models for 30 epochs with weight decay of $10^{-8}$ and learning rate of $5\times10^{-5}$ and batch size of 512 following~\cite{ConvNext}. To measure latency, we use the input sizes corresponding to the respective methods. For iPhone latency measurements, we export the models using Core ML Tools (v6.0) and run it on iPhone12 Pro Max with iOS 16 and batch size is set to 1 for all the models. We follow the same protocol as~\cite{mobileone}. For GPU latency measurements, we export the traced model to TensorRT (v8.0.1.6) format and run it on NVIDIA RTX-2080Ti with batch size of 8. We report the median latency estimate from 100 runs.


\begin{table}[]
    \centering

    \scalebox{0.72}{
    \begin{tabular}{l| @{\hspace{0.5\tabcolsep}} c @{\hspace{1.0\tabcolsep}} c @{\hspace{1.0\tabcolsep}}c @{\hspace{1.0\tabcolsep}}c @{\hspace{1.0\tabcolsep}}c| @{\hspace{0.5\tabcolsep}}c}
    \toprule
    \multirow{2}{*}{Model}        & Eval    & Param     & FLOPs    & GPU  & Mobile       & Top-1  \\ 
    % \cmidrule{5-6}
                                    & Image &        &       & Latency     & Latency  &  Acc.      \\   
                    & Size & (M) & (G) & (ms)  & (ms) & (\%) \\
                                    
                                    \midrule

    MobileOne-S0\cite{mobileone} & 224 & 2.1 & 0.3 & \textbf{1.0} & \textbf{0.8} & 71.4 \\ 
    \textbf{FastViT-T8} & 256 & 3.6 & 0.7 & 1.7 & \textbf{0.8} & \textbf{75.6} \\ 
    \midrule
    MobileOne-S3\cite{mobileone} & 224 & 10.1 & 1.8 & \textbf{2.1} & 1.5 & 78.1 \\ 
    
    CMT-$\text{T}^*$\cite{Cmt}  & 160 & 9.5 & 0.6 & 4.5 & 3.8 & \textbf{79.1} \\
    EfficientNet-B1\cite{EfficientNet} & 256 & 7.8 & 0.7 & 3.2 & 2.5 & \textbf{79.1} \\
    \textbf{FastViT-T12} & 256 & 6.8 & 1.4 & \textbf{2.1} & \textbf{1.2} & \textbf{79.1} \\

    \midrule
    MobileOne-S4\cite{mobileone} & 224 & 14.8 & 2.9 & 2.6 & 1.9 & 79.4 \\
    RSB-ResNet50\cite{rsbresnets} & 224 & 25.6 & 4.1 & 2.3 & 2.6 & \textbf{79.8} \\
    DeiT-S\cite{deit}  & 224 & 22.0 & 4.6 & 5.2 & 5.3 & \textbf{79.8} \\
    \textbf{FastViT-S12} & 256 & 8.8 & 1.8 & \textbf{2.2} & \textbf{1.4} & \textbf{79.8} \\
    
    \midrule
    
    RegNetY-8G\cite{RegNet} & 224 & 39.2 & 8.0 & 5.8 & 3.8 & 79.9 \\
    EfficientNet-B2\cite{EfficientNet} & 288 & 9.2 & 1.0  & 4.1 & 3.5 & 80.1 \\
    PoolFormer-S24\cite{metaformer} & 224  & 21.0 & 3.4 & 15.7 & 2.5 & 80.3 \\
    RegNetY-16G \cite{RegNet}  & 224 & 83.6 & 16.0 & 8.1 & 7.5 & 80.4 \\
    \textbf{FastViT-SA12} & 256 & 10.9 & 1.9 & \textbf{2.5} & \textbf{1.6} & \textbf{80.6} \\
    
    \midrule
    
    ResNeSt50\cite{zhang2022resnest} & 224 & 27.5 & 5.4 & 51.5 & 29.9 & 81.1 \\
    Swin-T\cite{Swin} & 224 & 29.0 & 4.5 & -  & 9.3 & 81.3\\
    PoolFormer-S36\cite{metaformer}  & 224 & 31.0 & 5.0 & 23.7 & 3.5 & 81.4 \\
    EfficientNet-B3\cite{EfficientNet} & 300 & 12.0 & 1.8 & 5.3 & 7.1 & 81.6 \\
    CvT-13\cite{cvt} & 224 & 20.1 & 4.5 & 18.1 & 62.6 & 81.6 \\
    
    CMT-$\text{XS}^*$\cite{Cmt} & 192 & 15.2 & 1.5 & 8.3 & 6.3 & 81.8 \\
    DeiT-B \cite{deit} & 224 & 86.0 & 17.5 & 11.6 & 11.6 & 81.8 \\
    RSB-ResNet152\cite{rsbresnets} & 224 & 60.2 & 11.6 & 5.4 & 4.7 & 81.8 \\
	LITv2-S\cite{litv2} & 224 & 28.0 & 3.7 & - & - & 82.0 \\
	ConvNeXt-T\textsuperscript{\textdagger}\cite{ConvNext} & 224 & 29.0 & 4.5 & 6.3 & 3.7 & 82.1 \\ 
    PoolFormer-M48\cite{metaformer} & 224 & 73.0 & 11.6 & 30.2 & 7.6 & 82.5 \\
    \textbf{FastViT-SA24} & 256 & 20.6 & 3.8 & \textbf{3.8} & \textbf{2.6} & \textbf{82.6} \\
    \midrule
        
    ResNeSt101\cite{zhang2022resnest} & 224 & 48.2 & 10.2 & 75.5 & 37.7 & 83.0 \\
    Swin-S\cite{Swin}   & 224  & 50.0  & 8.7 & -  & 13.0 & 83.0        \\
    ConvNeXt-S\textsuperscript{\textdagger}\cite{ConvNext}  & 224  & 50.0 & 8.7 & 10.3 & 5.4 & 83.1 \\
    Swin-B \cite{Swin}  & 224 & 88.0 & 15.4 & - & 18.5 & 83.5  \\
    CMT-$\text{S}^*$\cite{Cmt} & 224 & 25.1 & 4.0 & 13.8 & 15.6 & 83.5 \\
    LITv2-B\cite{litv2} & 224 & 87.0 & 13.2 & - & - & \textbf{83.6} \\
    EfficientNet-B5\cite{EfficientNet}  & 456 & 30.0 & 9.9 & 24.2  & 22.1  & \textbf{83.6}  \\
    NFNet-F0\cite{nfnets} & 256 & 71.5 & 12.4 & 6.2 & 6.4 & \textbf{83.6} \\
    \textbf{FastViT-SA36} & 256 & 30.4 & 5.6 & \textbf{5.2} & \textbf{3.5} & \textbf{83.6} \\
    \midrule

    
    ConvNeXt-B\textsuperscript{\textdagger}\cite{ConvNext}             &224        & 89.0      & 15.4       &  13.6     & 8.4    & 83.8\\
    EfficientNetV2-S\cite{efficientnet_v2_quoc} & 384 & 22.0 & 8.8 & 8.2 & 7.1 & \textbf{83.9}\\
     \textbf{FastViT-MA36} & 256 & 42.7 & 7.9 & \textbf{6.7} & \textbf{4.5} & \textbf{83.9} \\
     

    \midrule
    

    EfficientNet-B6\cite{EfficientNet}  & 528 & 43.0 & 19.0 & 44.3 & 51.7  & 84.0  \\
    EfficientNet-B7\cite{EfficientNet}  & 600 & 66.0 & 37.0 & 72.6 & 85.5 & 84.3  \\
    CMT-$\text{B}^*$\cite{Cmt} & 256 & 45.7 & 9.3 & 26.9 & 20.6 & \textbf{84.5} \\
    Swin-B\cite{Swin}   & 384  & 88.0  & 47.0 & -  & 57.2 & \textbf{84.5}        \\
    \textbf{FastViT-SA36} & 384 & 30.4 & 12.6 & \textbf{12.8} & \textbf{7.4} & \textbf{84.5} \\
    
    \midrule
    
    NFNet-F1\cite{nfnets} & 320 & 132.6 & 35.5 & 16.9 & 16.1 & 84.7 \\
    LITv2-B\cite{litv2} & 384 & 87.0 & 39.7 & - & - & 84.7 \\
    \textbf{FastViT-MA36} & 384 & 42.7 & 17.7 & \textbf{16.5} & \textbf{9.2} & \textbf{84.9} \\
    
    \bottomrule
    \end{tabular}
    
    }
        \caption{Comparison of different state-of-the-art methods on ImageNet-1k classification. HardSwish is not well supported by Core ML, $*$ denotes we replace it with GELU for fair comparison. ``$\dagger$" denotes that model has been modified from original implementation for efficient deployment.  
        Models which could not be reliably exported either by TensorRT or Core ML Tools are annotated by ``-".}
    \label{tab:ImageNet_1K}
\end{table}


\textbf{Comparison with SOTA Models}
In Table~\ref{tab:ImageNet_1K}, we compare our models against recent state-of-the-art models on ImageNet-1k dataset. 
For a fair comparison, we modify ConvNeXt~\cite{ConvNext} from official implementation by avoiding costly reshape operations, see supplementary materials for more details. 
We were unable to reliably export LITv2~\cite{litv2} due to poor support for deformable convolutions in either library. Our model obtains the best accuracy-latency trade-off when compared to recent state-of-the-art models on two different compute fabrics, i.e. desktop-grade GPU and mobile device. Our models improve over LITv2~\cite{litv2} on both parameter count and FLOPs, at Top-1 accuracy of 84.9\%, FastViT-MA36 is 49.3\% smaller and consumes 55.4\% less FLOPs than LITv2-B.  FastViT-S12 is 26.3\% faster than MobileOne-S4~\cite{mobileone} on iPhone 12 Pro and 26.9\% faster on GPU. At Top-1 accuracy of 83.9\%, FastViT-MA36 is 1.9$\times$ faster than an optimized ConvNeXt-B model on iPhone 12 Pro and 2.0$\times$ faster on GPU. At Top-1 accuracy of 84.9\%, FastViT-MA36 is just as fast as NFNet-F1~\cite{nfnets} on GPU while being 66.7\% smaller and using 50.1\% less FLOPs and 42.8\% faster on mobile device.  

\textbf{Knowledge distillation}
We report performance of our models when trained with distillation objective in Table~\ref{tab:ImageNet_1K_distill}. We follow the setting described in DeiT~\cite{deit}, with RegNet-16GF~\cite{RegNet} as the teacher model. Following DeiT~\cite{deit}, we use hard distillation where hard decision of the teacher is set as true label. Our models are trained for 300 epochs. Unlike~\cite{deit, cait, li2022efficientformer}, we do not introduce an additional classification head for distillation. FastViT outperforms recent state-of-the-art model EfficientFormer~\cite{li2022efficientformer}. FastViT-SA24 attains similar performance as EfficientFormer-L7 while having 3.8$\times$ less parameters, 2.7$\times$ less FLOPs and 2.7$\times$ lower latency.

\begin{table}
    \centering
    \scalebox{0.74}{
    \begin{tabular}{l| @{\hspace{0.5\tabcolsep}} c @{\hspace{1.0\tabcolsep}} c @{\hspace{1.0\tabcolsep}}c @{\hspace{1.0\tabcolsep}}c @{\hspace{1.0\tabcolsep}}c| @{\hspace{0.5\tabcolsep}}c}
    \toprule
    \multirow{2}{*}{Model}        & Eval    & Param     & FLOPs    & GPU  & Mobile       & Top-1  \\ 
    
                                    & Image &        &       & Latency     & Latency  &  Acc.      \\   
                    & Size & (M) & (G) & (ms)  & (ms) & (\%)      \\
                    \midrule
    % \midrule

    \textbf{FastViT-T8} & 256 & 3.6 & 0.7 & 1.7 & \textbf{0.8} & \textbf{76.7} \\
    \midrule
    \textbf{FastViT-T12} & 256 & 6.8 & 1.4 & 2.1 & \textbf{1.2} & \textbf{80.3} \\
    \midrule
    
    CaiT XXS-36\cite{cait} & 224 & 17.3 & 3.8 & 15.8 & - & 79.7 \\
    \textbf{FastViT-S12} & 256 & 8.8 & 1.8 & \textbf{2.2} & \textbf{1.4} & \textbf{80.9} \\
    \midrule
    
    DeiT-S\cite{deit} & 224 & 22 & 4.6 & 5.2 & 6.7 & 81.2 \\
    \textbf{FastViT-SA12}  & 256 & 10.9 & 1.9 & \textbf{2.5}  & \textbf{1.6} & \textbf{81.9} \\
    
    \midrule
    
    EfficientFormer-L3\cite{li2022efficientformer}  & 224 & 31.3 & 3.9 & \textbf{3.6} & 3.0 & 82.4 \\
    EfficientFormer-L7\cite{li2022efficientformer}  & 224 & 82.1 & 10.2 & 7.5 & 7.0 & 83.3 \\
    DeiT-B\cite{deit} & 224 & 87 & 17.6 & 11.6 & 11.6 & \textbf{83.4} \\
    \textbf{FastViT-SA24}  & 256 & 20.6 & 3.8 & 3.8 & \textbf{2.6} & \textbf{83.4} \\
    
    \midrule
    
    CaiT S-24\cite{cait} & 224 & 46.9 & 9.4 & 19.1 & - & 83.5 \\
    CaiT XS-24\cite{cait} & 384 & 26.6 & 19.3 & 77.7 & - & 84.1 \\
    \textbf{FastViT-SA36}  & 256 & 30.4 & 5.6 & \textbf{5.2} & \textbf{3.5} & \textbf{84.2} \\
    \midrule
    
    \textbf{FastViT-MA36}  & 256 & 42.7 & 7.9 & \textbf{6.7} & \textbf{4.5} & \textbf{84.6} \\
    
    \bottomrule
    \end{tabular}
    
    }
        \caption{Comparison of different state-of-the-art methods on ImageNet-1k classification when trained using distillation objective specified by~\cite{deit}. Models which could not be reliably exported either by TensorRT or Core ML Tools are annotated by ``-".}
    \label{tab:ImageNet_1K_distill}
    \vspace{-.2cm}
\end{table}


\subsection{Robustness Evaluation}
We evaluate our models for out-of-distribution robustness on the following benchmarks -- (i) ImageNet-A~\cite{imageneta}, a dataset that contains naturally occurring examples that are misclassified by ResNets; (ii) ImageNet-R~\cite{imagenetr}, a dataset that contains natural renditions of ImageNet object classes with different textures and local image statistics; (iii) ImageNet-Sketch~\cite{imagenetsketch}, a dataset that contains black and white sketches of all ImageNet classes, obtained using google image queries; and (iv) ImageNet-C~\cite{imagenetc}, a dataset that consists of algorithmically generated corruptions (blur, noise) applied to the ImageNet test-set. We evaluate our models using the implementation provided by~\cite{mao2022robust}. Our models are more robust than recent vision transformers while being faster than pure-CNN based models which exhibit low robustness as seen Table~\ref{tab:robustness}. Architectural choices like using a large kernel convolutions in FFN and patch-embedding layers in combination with self-attention layers helps in improving the robustness of our model as discussed in Section~\ref{sec:convnext_ffn}. All the models compared in Table~\ref{tab:robustness} are trained only on ImageNet-1k dataset using similar training recipes. From Table~\ref{tab:robustness}, our model is highly competitive to RVT and ConvNeXt, in fact FastViT-M36 has better clean accuracy, better robustness to corruptions and similar out-of-distribution robustness as ConvNeXt-S which has 6.1M more parameters and has 10\% more FLOPs than our model.

\begin{table}[t]
\centering
\scalebox{0.73}{
\begin{tabular}{l@{\hspace{0.95\tabcolsep}}c@{\hspace{0.95\tabcolsep}}c@{\hspace{0.95\tabcolsep}}c@{\hspace{0.95\tabcolsep}}c@{\hspace{0.95\tabcolsep}}c@{\hspace{0.95\tabcolsep}}c@{\hspace{0.95\tabcolsep}}c}
\toprule
Model   & \#Params & \#FLOPs & Clean & IN-C ($\downarrow$) & IN-A & IN-R & IN-SK \\
\midrule
PVT-Tiny & 13.2 & 1.9 & 75.0 & 79.6 & 7.9 & 33.9 & 21.5 \\
RVT-Ti & 8.6 & 1.3 & \underline{78.4} & \textbf{58.2} & \underline{13.3} & \textbf{43.7} & \textbf{30.0} \\
\textbf{FastViT-SA12} & 10.9 & 1.9 &\textbf{80.6} & \underline{62.2} & \textbf{17.2} & \underline{42.6} & \underline{29.7} \\
\midrule
PVT-Small    & 24.5 & 3.8 & \underline{79.9} & 66.9 & \underline{18.0} & 40.1 & 27.2   \\
ResNet-50*  & 25.6 & 4.1 & 79.0 & 65.5 & 5.9 & \underline{42.5} & \underline{31.5}      \\
ResNeXt50-32x4d & 25.0 & 4.3 & 79.8 & \underline{64.7} & 10.7 & 41.5 & 29.3 \\
\textbf{FastViT-SA24}  & 20.6 & 3.8 & \textbf{82.6} & \textbf{55.3} & \textbf{26.0} & \textbf{46.5} & \textbf{34.0} \\
\midrule
Swin-T     & 28.3 & 4.5 & 81.2 & 62.0 & 21.6 & 41.3 & 29.1    \\ 
ConViT-S   & 27.8 & 5.4 & 81.5 & \textbf{49.8} & 24.5 & 45.4 & 33.1    \\
RVT-S & 22.1 & 4.7 & 81.7 & \underline{50.1} & 24.1 & 46.9 & 35.0  \\ 
ConvNeXt-T & 28.6 & 4.5 & 82.1 & 53.2 & 24.2 & \underline{47.2} & 33.8  \\ 
EfficientNet-B4 & 19.3 & 4.2 & \underline{83.0} & 71.1 & \underline{26.3} & 47.1 & \underline{34.1} \\
\textbf{FastViT-SA36} & 30.4 & 5.6 & \textbf{83.6} & 51.8 & \textbf{32.3} & \textbf{48.1} & \textbf{35.8} \\
\midrule
ConvNeXt-S & 50.0 & 8.7 & 83.1 & 51.2 & 31.2 & \textbf{49.5} & \textbf{37.1} \\ 
\textbf{FastViT-MA36} & 42.7 & 7.9 & \textbf{83.9} & \textbf{49.9} & \textbf{34.6} & \textbf{49.5} & 36.6  \\ 
\bottomrule

\end{tabular}
}
\caption{Results on robustness benchmark datasets. Models are grouped based on FLOPs. Performance of competing models is reported by~\cite{mao2022robust} and \cite{ConvNext}. *ResNet-50 model is trained with AugMix to improve robustness as reported in ~\cite{mao2022robust}. For ImageNet-C mean corruption error is reported (lower is better) and for other datasets Top-1 accuracy is reported (higher is better). The best results are in bold and second best results are underlined.}
\label{tab:robustness}
\vspace{-0.3cm}
\end{table}


\subsection{3D Hand mesh estimation}
Recent works on real-time 3D hand mesh estimation introduce complex mesh regression layers over CNN based backbones. The backbones usually belong to ResNet or MobileNet family of architectures with the exception of METRO and MeshGraphormer which use HRNets~\cite{hrnets} for feature extraction. While most hardware devices are highly optimized for feature extraction from 2D CNNs, the same is not true for the complex mesh regression heads used in these methods. In our method, we replace the complex mesh regression head with a simple regression module which regresses weak perspective camera, pose and shape parameters of the MANO model~\cite{manomodel}. We argue that using a feature extraction backbone that learns a good representation for underlying images can alleviate learning challenges in mesh regression. While other real-time methods compensate for weak feature extraction backbone with complex mesh regression layers, we use a better feature extraction backbone with a simple mesh regression layer. We compare our approach with other published methods on the FreiHand~\cite{freihand} dataset. For a fair comparison, we cite results of methods that only used FreiHand dataset for training as some methods either pre-train, train, or fine-tune with additional pose datasets. We only use ImageNet-1k dataset for pre-training and then train exclusively on the FreiHand dataset using the experimental setup described in ~\cite{metro_hand}. For more details, please see supplementary materials. From Table~\ref{tab:freihand}, amongst real-time methods, our method outperforms other methods on all joint and vertex error related metrics while being 1.9$\times$ faster than MobileHand~\cite{mobilehand} and 2.8$\times$ faster than recent state-of-art MobRecon.


\begin{table}[t]
\centering
\scalebox{0.77}{
\begin{tabular}{l @{\hspace{0.5\tabcolsep}} c @{\hspace{0.5\tabcolsep}} | c @{\hspace{0.5\tabcolsep}} c @{\hspace{0.5\tabcolsep}} c @{\hspace{0.5\tabcolsep}} c @{\hspace{0.5\tabcolsep}} c}
\toprule
Method  &  Backbone  & PJ $\downarrow$ & PV $\downarrow$ & F@5 $\uparrow$ & F@15 $\uparrow$ & FPS $\uparrow$ \\
\midrule
\multicolumn{7}{c}{Non Real-Time methods}\\
\midrule
HIU-DMTL \cite{hiu_dtml}           & Customized & 7.1 & 7.3   & 0.699 & 0.974 & 5  \\
METRO \cite{metro_hand}  & HRNet & 6.3 & 6.5 & 0.731 & 0.984 & 4.2 \\
MeshGraphormer \cite{meshgraphormer}  & HRNet & 5.9  & 6.0 & 0.764 & 0.986 & 2.6 \\
\midrule
\multicolumn{7}{c}{Real-Time methods}\\
\midrule
Hasson \etal \cite{obman} & ResNet18 & - & 13.2 & 0.436 & 0.908 & 20 \\
MobileHand \cite{mobilehand}   & MobileNetV3 & $-$ & 13.1 & 0.439 & 0.902 & 110  \\
YoutubeHand \cite{youtube_hand} & ResNet50  & 8.4  & 8.6  & 0.614 & 0.966 & 60  \\
I2L-MeshNet \cite{i2l_meshnet}        & ResNet50 & 7.4 & 7.6   & 0.681 & 0.973 & 33.3 \\
Pose2Mesh \cite{pose2mesh}    & Customized & 7.4 & 7.6   & 0.683 & 0.973 & 20  \\
I2UV-HandNet \cite{i2uv_handnet}      & ResNet50  & 7.2  & 7.4 & 0.682 & 0.973 & - \\ 

Tang \etal \cite{handar}      & ResNet50 & 7.1  & 7.1 & 0.706 & 0.977 & 39.1 \\   
MobRecon\cite{mobrecon}     & DenseStack & 6.9  & 7.2 & 0.694 & 0.979 & 77 \\
CMR \cite{cmr_hand}                & ResNet50$^*$ & 6.9  & 7.0 & 0.715 & 0.977 & - \\ 
\textbf{Ours} & \textbf{FastViT-MA36} & \textbf{6.6} & \textbf{6.7} & \textbf{0.722} & \textbf{0.981} & \textbf{218} \\
\bottomrule

\end{tabular}
}
% \vspace{-1.5mm}
\caption{Results on the FreiHAND test dataset. FPS is computed on NVIDIA RTX-2080Ti under similar setting as MobRecon~\cite{mobrecon}. 
Performance of competing methods obtained from training exclusively on FreiHand dataset.
}
\label{tab:freihand}
\vspace{-0.3cm}
\end{table}



\subsection{Semantic Segmentation and Object Detection}
For semantic segmentation, we validate the performance of our models on ADE20k~\cite{ade20k}. The dataset contains 20K training images and 2K validation images with 150 semantic categories. We train semantic segmentation models with Semantic FPN~\cite{Semantic_FPN} decoder. Models with Semantic FPN head use the same settings as~\cite{metaformer}. All models are initialized with pretrained weights from their corresponding image classification models. FLOPs and backbone latency are estimated on image crops of 512$\times$512. Due to higher resolution of input image, GPU latencies are estimated over a batch size of 2 in both Table~\ref{tab:segmentation} and Table~\ref{tab:detection}. In Table~\ref{tab:segmentation}, we compare our models with recent works. FastViT-MA36 model obtains 5.2\% higher mIoU than PoolFormer-M36 which has higher FLOPs, parameter count and latency on both desktop GPU and mobile device.

We train object detection on the MS-COCO~\cite{COCO} dataset with 80 classes containing 118K training and 5K validation images. In Table~\ref{tab:detection}, we compare our models with recent works. All the models are trained with 1x schedule following~\cite{metaformer} using a Mask-RCNN~\cite{Mask_RCNN} head. All models are initialized with pretrained weights from their corresponding image classification models. We show that our models achieve state-of-the-art performance under multiple latency regimes. FastViT-MA36 model has similar performance as CMT-S, while being 2.4$\times$ and 4.3$\times$ faster on desktop GPU and mobile device respectively.


\begin{table}[]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l@{\hspace{0.5\tabcolsep}}|c@{\hspace{0.5\tabcolsep}}c|@{\hspace{0.5\tabcolsep}}c@{\hspace{0.5\tabcolsep}}c|c}
    \toprule
    \multirow{2.5}{*}{Backbone}    & GPU & Mobile    & \multicolumn{3}{c}{Semantic FPN 80k}  \\
    \cmidrule{4-6}
    & Latency  & Latency    & Param(M)   & FLOPs(G)   & mIoU(\%)  \\
    \midrule
    ResNet-50    & 2.7 &  8.7  & 28.5      &    46     & 36.7\\
    PoolFormer-S12\cite{metaformer}      &  9.7  &   6.2     & 15.7   & 31  & 37.2            \\
    \textbf{FastViT-SA12} & \textbf{2.5} & \textbf{5.6} &  14.1 & 29 & \textbf{38.0} \\
    \midrule
    ResNet-101 & 4.6 & 12.4 & 47.5 & 65 & 38.8 \\
    PVT-Small\cite{PVT_v1}   & - & - & 28.2 & 41 & 39.8 \\
    PoolFormer-S24\cite{metaformer}     & 18.9 &   10.7     & 23.2   & 48     & 40.3 \\
    \textbf{FastViT-SA24} & \textbf{4.4} & \textbf{9.3} & 23.8 & 37 & \textbf{41.0} \\
    \midrule
    PoolFormer-S36\cite{metaformer}     & 28.0 &   17.0     & 34.6   & 48     & 42.0   \\
    \textbf{FastViT-SA36}          & \textbf{6.1} &  \textbf{12.9} & 33.6      &   44      & \textbf{42.9}      \\ 
    \midrule
    
    PVT-Medium\cite{PVT_v1}   & - & - & 48.0 & 55 & 41.6 \\
    PoolFormer-M36\cite{metaformer} & 41.4 & 24.8 & 59.8 & 68 & 42.4 \\ 
    \textbf{FastViT-MA36}          & \textbf{8.2} & \textbf{16.3} & 45.7 & 53 & \textbf{44.6}      \\ 
    \bottomrule
    \end{tabular}}
    % \vspace{-1.5mm}
    \caption{Performance of different backbones on ADE20K semantic segmentation task. Following common convention, FLOPs and backbone latencies are measured on crops of 512$\times$512. 
    }\label{tab:segmentation}
    \vspace{-2mm}
\end{table}



\begin{table}[t]
\scalebox{0.79}{
% 	\setlength\tabcolsep{4.0pt}
% 	\vspace{-0.3cm}
	\label{tab:detection}
	\centering
	\small 
	\begin{tabular}{l@{\hspace{0.3\tabcolsep}}|c@{\hspace{0.6\tabcolsep}}c@{\hspace{0.6\tabcolsep}}|c@{\hspace{0.6\tabcolsep}}c@{\hspace{0.6\tabcolsep}}c@{\hspace{0.6\tabcolsep}}c@{\hspace{0.6\tabcolsep}}c@{\hspace{0.6\tabcolsep}}c}
		\toprule
		\multirow{2}{*}{Backbone} & GPU & Mobile & \multirow{2}{*}{AP$^{\rm b}$} & \multirow{2}{*}{AP$_{50}^{\rm b}$} & \multirow{2}{*}{AP$_{75}^{\rm b}$} & \multirow{2}{*}{AP$^{\rm m}$} & \multirow{2}{*}{AP$_{50}^{\rm m}$} & \multirow{2}{*}{AP$_{75}^{\rm m}$} \\
		& Latency & Latency & & & & \\
		\midrule
		Poolfomer-S12\cite{metaformer} & 9.7 & 6.2 & 37.3 & 59.0 & 40.1 & 34.6 & 55.8 & 36.9 \\
		ResNet-50~\cite{resnet} & 2.7 & 8.7 & 38.0 & 58.6 & 41.4 & 34.4 & 55.1 & 36.7 \\
		\textbf{FastViT-SA12} & \textbf{2.5} & \textbf{5.6} & \textbf{38.9} & \textbf{60.5} & \textbf{42.2} &	\textbf{35.9} & \textbf{57.6} & \textbf{38.1} \\
		\midrule
		ResNet-101~\cite{resnet} & 4.6 & 12.4 & 40.0 & 60.5 & 44.0 & 36.1 & 57.5 & 38.6 \\
		Poolfomer-S24\cite{metaformer} & 18.9 & 10.7 & 40.1 & 62.2 & 43.4 & 37.0 & 59.1 & 39.6 \\
		PVT-S~\cite{PVT_v1} & - & - & 40.4 & 62.9 & 43.8 & 37.8 & 60.1 & 40.3 \\
		\textbf{FastViT-SA24} & \textbf{4.4} & \textbf{9.3} & \textbf{42.0} & \textbf{63.5} & \textbf{45.8} & \textbf{38.0} & \textbf{60.5} & \textbf{40.5} \\
		\midrule
		Swin-T~\cite{Swin} & - & - & 42.2 & 64.6 & 46.2 & 39.1 & 61.6 & 42.0 \\
		Twins-SVT-S~\cite{Twins} & - & - & 42.7 & 65.6 & 46.7 & 39.6 & 62.5 & 42.6 \\
		Twins-PCPVT-S~\cite{Twins} & - & 52.1 & 42.9 & 65.8 & 47.1 & 40.0 & 62.7 & 42.9 \\
		\textbf{FastViT-SA36} & \textbf{6.1} & \textbf{12.9} & \textbf{43.8} & \textbf{65.1} & \textbf{47.9} & \textbf{39.4} & \textbf{62.0} & \textbf{42.3} \\
		\midrule
		
		CMT-S~\cite{Cmt}  & 19.9 & 70.9 & 44.6 & \bf 66.8 & 48.9 & \bf 40.7 & \bf 63.9 & \bf 43.4 \\
        Poolfomer-S36\cite{metaformer} & 28.0 & 17.0 & 41.0 & 63.1 & 44.8 & 37.7 & 60.1 & 40.0 \\
        % \hline 
        \textbf{FastViT-MA36} & \textbf{8.2} & \textbf{16.3} & \textbf{45.1} & \textbf{66.8} & \textbf{49.5} & 40.5 & 63.8 & \textbf{43.4}  \\
		\bottomrule
	\end{tabular}
	}
% 	\vspace{-1.5mm}
    \caption{Results for object detection and instance segmentation on MS-COCO \texttt{val2017} split using Mask-RCNN~\cite{Mask_RCNN} framework using 1x training schedule, i.e. 12 epochs used for training the models. Backbone latencies are measured on crops of 512$\times$512. 
    }\label{tab:detection}
	\vspace{-0.4cm}
\end{table}








