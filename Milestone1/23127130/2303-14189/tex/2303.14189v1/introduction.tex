\section{Introduction}
\label{sec:introduction}



\begin{figure*}[t]
  \centering
  \begin{subfigure}{.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{images/acc_vs_latency.pdf}
    \caption{}
  \end{subfigure}%
  \hfill
  \begin{subfigure}{.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{images/gpu_acc_vs_latency.pdf}
    \caption{}
  \end{subfigure}%
    \vspace{-2mm}
  \caption{(a) Accuracy vs. Mobile latency scaling curves of recent methods. The models are benchmarked on an iPhone 12 Pro following ~\cite{mobileone}. (b) Accuracy vs. GPU latency scaling curves of recent methods. For better readability only models with Top-1 accuracy better than 79\% are plotted. See supplementary materials for more plots.
  Across both compute fabrics, our model has the best accuracy-latency tradeoff.}
  \label{fig:teaser}
  \vspace{-4mm}
\end{figure*}

Vision Transformers~\cite{ViT} have achieved state-of-the-art performance on several tasks such as image classification, detection and segmentation~\cite{Swin}. However, these models have traditionally been computationally expensive. Recent works ~\cite{zhai2021aft, marin2021token, litv2, linformer, reformer} have proposed methods to lower the compute and memory requirements of vision transformers. Recent hybrid architectures~\cite{litv2, Cmt, d2021convit, cvt} effectively combine the strengths of convolutional architectures and transformers to build  architectures that are highly competitive on a wide range of computer vision tasks. Our goal is to build a model that achieves state-of-the-art latency-accuracy trade-off.

Recent vision and hybrid transformer models~\cite{deit, Cmt, litv1, litv2} follow the Metaformer~\cite{metaformer} architecture, which consists of a token mixer with a skip connection followed by Feed Forward Network (FFN) with another skip connection. These skip connections account for a significant overhead in latency due to increased memory access cost~\cite{Ding_2021_repvgg, mobileone}.
To address this latency overhead, we introduce ~\textit{RepMixer}, a fully reparameterizable token mixer that uses structural reparameterization to remove the skip-connections.
The RepMixer block also uses depthwise convolutions for spatial mixing of information similar to ConvMixer~\cite{trockman2022convmixer}. However, the key difference is that our module can be reparameterized at inference to remove any branches. 

To further improve on latency, FLOPs and parameter count, we replace all dense k$\times$k convolutions with their factorized version, i.e. depthwise followed by pointwise convolutions. This is a common approach used by efficient architectures~\cite{MobileNet_v1, MobileNet_v2, Mobilenet_v3} to improve on efficiency metrics, but, naively using this approach hurts performance as seen in Table~\ref{tab:ablation_arch_choices}. In order to increase capacity of the these layers, we use linear train-time overparameterization as introduced in~\cite{Ding_2021_repvgg, Ding_2019_ICCV, ding2021diverse, mobileone, NEURIPS2020_expandnets}. These additional branches are only introduced during training and are reparameterized at inference.  

In addition, we use large kernel convolutions 
in our network. This is because, although self-attention based token mixing is highly effective to attain competitive accuracy, they are inefficient in terms of latency~\cite{marin2021token}.
Therefore, we incorporate large kernel convolutions in Feed Forward Network (FFN)~\cite{ViT} layer and patch embedding layers. These changes have minimal impact on overall latency of the model while improving performance.  

Thus, we introduce \emph{FastViT} that is based on three key design principles-- i) use of RepMixer block to remove skip connections, ii) use of linear train-time overparameterization to improve accuracy, iii) use of large convolutional kernels to substitute self-attention layers in early stages.


{FastViT} achieves significant improvements in latency compared to other hybrid vision transformer architectures while maintaining accuracy on several tasks like -- image classification, object detection, semantic segmentation and 3d hand mesh estimation. We perform a comprehensive analysis by deploying recent state-of-the-art architectures on an iPhone 12 Pro device and an NVIDIA RTX-2080Ti desktop GPU.

In Figure~\ref{fig:teaser}, we show that, at ImageNet Top-1 accuracy of 83.9\%, our model is 4.9$\times$ faster than EfficientNet-B5~\cite{EfficientNet}, 1.6$\times$ faster than EfficientNetV2-S~\cite{efficientnet_v2_quoc}, 3.5$\times$ faster than CMT-S~\cite{Cmt} and 1.9$\times$ faster than ConvNeXt-B~\cite{ConvNext} on an iPhone 12 Pro mobile device. At ImageNet Top-1 accuracy of 84.9\% our model is just as fast as NFNet-F1~\cite{nfnets} on a desktop GPU while being 66.7\% smaller, using 50.1\% less FLOPs and 42.8\% faster on mobile device.
At latency of 0.8ms on an iPhone 12 Pro mobile device, our model obtains 4.2\% better Top-1 accuracy on ImageNet than MobileOne-S0. 
For object detection and instance segmentation on MS COCO using Mask-RCNN~\cite{Mask_RCNN} head, our model attains comparable performance to CMT-S~\cite{Cmt} while incurring 4.3$\times$ lower backbone latency. For semantic segmentation on ADE20K, our model improves over PoolFormer-M36~\cite{metaformer} by 5.2\%, while incurring a 1.5$\times$ lower backbone latency on an iPhone 12 Pro mobile device. On 3D hand mesh estimation task, our model is 1.9$\times$ faster than MobileHand~\cite{mobilehand} and 2.8$\times$ faster than recent state-of-the-art MobRecon~\cite{mobrecon} when benchmarked on GPU. 

% \\


In addition to accuracy metrics, we also study the robustness of our models to corruption and out-of-distribution samples which does not always correlate well with accuracy.
For example, PVT~\cite{PVT_v1} achieves highly competitive performance on ImageNet dataset, but has very poor robustness to corruption and out-of-distribution samples as reported in Mao et al.~\cite{mao2022robust}.
In real world applications, 
using a robust model in such a scenario can significantly improve user experience.   
We demonstrate the robustness of our architecture on popular benchmarks and show that our models are highly robust to corruption and out-of-distribution samples while being significantly faster than competing robust models. 
% \vspace{-0.2cm}
In summary, our contributions are as follows:
\begin{itemize}[leftmargin=*]
\setlength\itemsep{-.1em}
    \item We introduce \textit{FastViT}, a hybrid vision transformer that uses structural reparameterization to obtain lower memory access cost and increased capacity, achieving state-of-the-art accuracy-latency trade-off.

    \item We show that our models are the fastest in terms of latency on two widely used platforms -- mobile device and desktop GPU.
    \item We show that our models generalize to many tasks -- image classification, object detection, semantics segmentation, and 3D hand mesh regression.
    \item We show that our models are robust to corruption and out-of-distribution samples and significantly faster than competing robust models.
\end{itemize}
