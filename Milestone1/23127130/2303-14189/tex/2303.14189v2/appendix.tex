\appendix
\section{Ablations}
\subsection{Architectural Choices}
The primary motivation behind the design choices made for FastViT is efficient mobile deployment. The cost of self-attention blocks is very high, especially when there are many tokens. In early stages (when the number of tokens are high), self attention can be replaced with efficient alternatives %(we use RepMixer in our design)
with small accuracy degradation but significantly lower latency. In Table 4 of main paper, we analyzed this for last two stages. In Table~\ref{tab:arch_choices_abl}, we present the full analysis for S12 architecture.

\begin{table}[]
\centering
\scalebox{0.7}{
\begin{tabular}{@{\hspace{0.5\tabcolsep}}l| @{\hspace{0.5\tabcolsep}}l| @{\hspace{0.5\tabcolsep}}c @{\hspace{1.0\tabcolsep}}c}
\toprule
\multirow{2}{*}{Ablation} & \multirow{2}{*}{Description} & Top-1 & Mobile       \\
                          & & Acc. & Latency (ms)  \\
\midrule
 - & Baseline & 79.8 & 1.4 \\
\midrule
 Normalization & BatchNorm $\rightarrow$ LayerNorm & 79.7 & 1.7 \\
\midrule
\multirow{2}{*}{Activation} & GELU $\rightarrow$ ReLU & 79.4 & 1.3 \\
                            & GELU $\rightarrow$ SiLU & 79.7 & 1.4 \\
\midrule
\multirow{4}{*}{Hybrid Stages} & [RepMix, RepMix, RepMix, SelfAttn.] & 80.6 & 1.6 \\
                               & [RepMix, RepMix, SelfAttn., SelfAttn.] & 81.2 & 3.7 \\
                              & [RepMix, SelfAttn., SelfAttn., SelfAttn.] & 81.5 & 5.0 \\
                              & [SelfAttn., SelfAttn., SelfAttn., SelfAttn.] & 82.0 & 11.7 \\
                            
\bottomrule
\end{tabular}
}
\caption{Ablation for FastViT-S12 on ImageNet-1K. All models are trained and benchmarked using the same settings described in main paper.}
\label{tab:arch_choices_abl}
\end{table}

\subsection{Large Kernel Convolutions}
In Table~\ref{tab:large_kernel_abl}, we ablate over various kernel sizes in FFN and patch embedding layers and report their Top-1 accuracy on ImageNet-1k along with mobile latency on iPhone 12 Pro. We observe that performance of the model stagnates beyond the kernel size of 7$\times$7, while the overall FLOPs, latency and parameter count increases. Hence, we use 7$\times$7 kernel size in our network architecture. 

\begin{table}[!htb]
\centering
\scalebox{0.90}{
\begin{tabular}{c|c|c|c|c}
\toprule
\multirow{2}{*}{Kernel Size}   & Params & FLOPs & Mobile & Top-1 \\
& (M) & (G) & Latency(ms) & (\%) \\
\midrule

3$\times$3   & 8.7 & 1.7 & 1.3 & 78.9 \\
5$\times$5   & 8.7 & 1.8 & 1.4 & 79.5 \\
7$\times$7   & 8.8 & 1.8 & 1.4 & 79.8 \\
9$\times$9   & 8.8 & 1.8 & 1.5 & 79.6 \\
11$\times$11 & 8.8 & 1.9 & 1.5 & 79.8 \\

\bottomrule
\end{tabular}
}
\caption{Top-1 accuracy on ImageNet-1k dataset for FastViT-S12 model with varied kernel sizes in FFN and patch embedding layers.}
\label{tab:large_kernel_abl}
\end{table}

\subsection{Training Time}
We provide a coarse ablation of this in Table 3 of main paper. In our model, we do not overparameterize every element of the architecture, especially the parameter dense blocks like ConvFFN. In fact, we do not obtain any improvement by overparameterizing ConvFFN layers, verified empirically in Table~\ref{tab:ttop_abl}. Since our model is partially overparameterized (only convolutional stem and patch embedding layers) during training, we do not see a significant degradation in train time as opposed to other train-time overparameterized models in literature which overparameterize all layers in a network. Note, all models were trained using the same hardware as described in Section 4.1 of main paper. 

\begin{table}[]
\centering
\scalebox{0.70}{
\begin{tabular}{@{\hspace{0.5\tabcolsep}}l| @{\hspace{0.5\tabcolsep}}l| @{\hspace{0.5\tabcolsep}}c @{\hspace{1.0\tabcolsep}}c}
\toprule
\multirow{2}{*}{Ablation} & \multirow{2}{*}{Description} & Top-1 & Train       \\
                                                      &  & Acc. & Time (hrs)  \\
\midrule
 No OverParam. & -                                                   & 80.0 & 31.3 \\
\midrule
\multirow{2}{*}{Train-Time OverParam.} & Stem + Patch Emb.           & 80.6 & 33.4 \\
                                       & Stem + Patch Emb. + ConvFFN & 80.6 & 40.1  \\

                            
\bottomrule
\end{tabular}
}
\caption{Train-time overparameterization ablation for FastViT-SA12 on ImageNet-1K. Extension to Table 3 in main paper. Train time is wall clock time elapsed at the end of a training run.}
\label{tab:ttop_abl}

\end{table}


\section{Experiments}
\subsection{Benchmarking}
We follow the same protocol prescribed in~\cite{mobileone} while benchmarking the model on an iPhone 12 Pro mobile device. To benchmark the models on desktop-grade GPU NVIDIA RTX-2080Ti, we first warmup the device by running the forward pass of TensorRT model for 100 iterations and then benchmark the model over a 100 iterations. We report the median latency value from 100 estimates. For image classification models we use a batchsize of 8 (similar approach was adopted in~\cite{ShuffleNet_v2}) and a batchsize of 2 (due to GPU memory limits) for semantic segmentation and object detection models. Both mobile and GPU latency estimates can have a standard deviation of $\pm$0.1ms.

While benchmarking ConvNeXt~\cite{ConvNext} models on mobile device, we noticed the inefficiencies introduced by reshape ops causing increase in latency. While majority of hybrid models that use self-attention based token mixers require explicit reshaping of tensors. We can avoid this operation in ConvNeXt basic block by simply using a channel first implementation of LayerNorm and replacing \texttt{nn.Linear} layers with 1$\times$1 convolutions. This simple change results in significant improvement in runtime as shown in Table~\ref{tab:convnext_mod}.

\begin{table}[!htb]
\centering
\scalebox{0.8}{
\begin{tabular}{l|c|c}
\toprule
\multirow{2}{*}{Model}   & \multicolumn{2}{c}{Mobile Latency (ms)} \\
\cmidrule{2-3}
 & Before & After \\
\midrule
ConvNeXt-T & 33.5 & 3.7 \\
ConvNeXt-S & 66.4 & 5.4 \\
ConvNeXt-B & 89.1 & 8.4 \\
\bottomrule
\end{tabular}
}
\caption{Benchmarking ConvNeXt before and after modifications.}
\label{tab:convnext_mod}
\end{table}

\begin{table}[!htb]
\centering
\scalebox{0.68}{
\begin{tabular}{l|c|c}
\toprule
\multirow{2}{*}{Hyperparameter}   & Training & Fine-tuning \\
 & T8, T12, S12, SA12, SA24, SA36, MA36 & SA36, MA36 \\
\midrule
Stochastic depth rate & [0.0, 0.0, 0.0, 0.1, 0.1, 0.2, 0.35] & [0.2, 0.4] \\
Input resolution & 256$\times$256 & 384$\times$384 \\
Data augmentation & RandAugment & RandAugment \\
Mixup $\alpha$ & 0.8 & 0.8\\
CutMix $\alpha$ & 1.0 & 1.0 \\
Random erase prob. & 0.25 & 0.25 \\
Label smoothing & 0.1 & 0.1 \\
Train epochs & 300 & 30 \\
Warmup epochs & 5 & None \\
Batch size & 1024 & 1024 \\
Optimizer & AdamW & AdamW \\
Peak learning rate & 1e-3 & 5e-6 \\
LR. decay schedule & cosine & None \\
Weight decay rate & 0.05 & 1e-8 \\
Gradient clipping & None & None \\
EMA decay rate & 0.9995 & 0.9995 \\


\bottomrule

\end{tabular}
}
\caption{Training hyperparameters for ImageNet-1k experiments.}
\label{tab:hyperparams_inet1k}
\end{table}


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{images/acc_vs_latency_mobile.pdf}
    \caption{Accuracy vs. Mobile latency scaling curves of recent state-of-the-art Mobile Architectures and FastViT variants. The models are benchmarked on iPhone 12 Pro using the appropriate image sizes described in Table~\ref{tab:ImageNet_1K_mobile}. }
    \label{fig:mob_arch_plot}
\end{figure*}

\begin{table}[]
    \centering

    \scalebox{0.82}{
    \begin{tabular}{l| @{\hspace{0.5\tabcolsep}} c @{\hspace{1.0\tabcolsep}} c @{\hspace{1.0\tabcolsep}}c @{\hspace{1.0\tabcolsep}}c| @{\hspace{0.5\tabcolsep}}c}
    \toprule
    \multirow{2}{*}{Model}        & Eval    & Param     & FLOPs    & Mobile       & Top-1  \\ 
    % \cmidrule{5-6}
                                    & Image &      &       & Latency  &  Acc.      \\   
                    & Size & (M) & (G) & (ms) & (\%) \\
                                    
                                    \midrule

    MobileNetV3-S*\cite{Mobilenet_v3} & 224 & 2.5 & 0.06 & \textbf{0.8} & 67.4 \\
    MobileOne-S0\cite{mobileone} & 224 & 2.1 & 0.3 & \textbf{0.8} & 71.4 \\ 
    MobileNetV2-x1.0\cite{MobileNet_v2} & 224 & 3.4 & 0.3 & 0.9 & 72.0 \\
    DeiT-Ti\cite{deit}  & 224 & 5.7 & 1.3 & 4.8 & 72.2 \\
    MobileNeXt-x1.0\cite{mobilenext_eccv2020} & 224 & 3.4 & 0.3 & 0.9 & 74.0 \\
    EdgeViT-XXS\cite{edgevit} & 224 & 4.1 & 0.6 & 1.4 & 74.4 \\
    MNASNet-A1\cite{mnasnet_cvpr} & 224 & 3.9 & 0.3 & 1.0 & 75.2 \\
    \textbf{FastViT-T8} & 256 & 3.6 & 0.7 & \textbf{0.8} & \textbf{75.6} \\ 

    \midrule

    MobileNetV2-x1.4\cite{MobileNet_v2} & 224 & 6.9 & 0.6 & 1.4 & 74.7 \\
    MobileNetV3-L\cite{Mobilenet_v3} & 224 & 5.4 & 0.2 & 1.1 & 75.2 \\
    MobileNeXt-x1.4\cite{mobilenext_eccv2020}  & 224 & 6.1 & 0.6 & 1.3 & 76.1 \\
    EfficientNet-B0\cite{EfficientNet} & 224 & 5.3 & 0.4 & 1.7 & 77.1 \\
    EdgeViT-XS\cite{edgevit} & 224 & 6.7 & 1.1 & 3.0 & 77.5 \\
    MobileOne-S3\cite{mobileone} & 224 & 10.1 & 1.8  & 1.5 & 78.1 \\ 
    CMT-$\text{T}^*$\cite{Cmt}  & 160 & 9.5 & 0.6  & 3.8 & \textbf{79.1} \\
    EfficientNet-B1\cite{EfficientNet} & 256 & 7.8 & 0.7 & 2.5 & \textbf{79.1} \\
    \textbf{FastViT-T12} & 256 & 6.8 & 1.4 &  \textbf{1.2} & \textbf{79.1} \\

    \midrule
    MobileOne-S4\cite{mobileone} & 224 & 14.8 & 2.9  & 1.9 & 79.4 \\
    DeiT-S\cite{deit}  & 224 & 22.0 & 4.6 & 5.3 & \textbf{79.8} \\
    \textbf{FastViT-S12} & 256 & 8.8 & 1.8 &  \textbf{1.4} & \textbf{79.8} \\
 
    \bottomrule
    \end{tabular}
    
    }
        \caption{Comparison of different state-of-the-art Mobile architectures on ImageNet-1k classification. HardSwish is not well supported by Core ML, $*$ denotes we replace it with GELU for fair comparison.}
    \label{tab:ImageNet_1K_mobile}
\end{table}


\subsection{Image Classification}
We provide hyperparameters used for training models on ImageNet-1k dataset reported in Table 5 in main paper. Models are trained at resolution 256$\times$256 and fine-tuned for 384$\times$384. 
We follow the same training setup as~\cite{metaformer, deit}. The hyperparameters used for all FastViT variants are listed in Table~\ref{tab:hyperparams_inet1k}. For distillation experiments, we use RegNetY-16GF~\cite{RegNet} as the teacher model similar to~\cite{deit}. Additional hyperparameters are same as our image classification training procedure and are listed in Table~\ref{tab:hyperparams_inet1k}. When trained using different seeds, results are within $\pm$0.2\% in Top-1 accuracy.


\subsection{Comparison with Mobile Architectures}
We compare our model against highly efficient mobile architectures in Table~\ref{tab:ImageNet_1K_mobile} and in Figure~\ref{fig:mob_arch_plot}. Our model outperforms recent state-of-the-art MobileOne~\cite{mobileone} architecture which is purely convolutional. Our model also outperforms EdgeViT~\cite{edgevit}, which is a recent state-of-the-art light-weight ViT architecture. 


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{images/hand_mesh_arch.pdf}
    \caption{Overview of 3d hand mesh estimation framework.}
    \label{fig:hand_mesh_arch}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{images/mesh_results.pdf}
    \caption{Qualitative results from our framework on FreiHand test set. 3D predictions are projected on to the image using weak perspective camera model, parameters for this camera model is also predicted by the model.}
    \label{fig:hand_mesh_results}
\end{figure*}


\subsection{Model Scaling} 
In this work, we sample architectures that are smaller than 50M parameters for efficient deployment. Similar to the Swin-T~\cite{Swin} variant, we use a stage compute ratio of 1:1:3:1 most of our variants and for the smallest variant we use 1:1:2:1. For our models, width doubles at each new stage. We use configurations of [48, 96, 192, 384], [64, 128, 256, 512] and [76, 152, 304, 608] in this paper. The tiny(``T") variants use MLP expansion ratio of 3. Rest of the variants use an MLP expansion ratio of 4.


\subsection{3D Hand mesh estimation}

\paragraph{Architecture}
As shown in Figure~\ref{fig:hand_mesh_arch}, our model uses simple regression layers to regress weak perspective camera, pose and shape parameters of the MANO model~\cite{manomodel}. These layers are single fully connected layers unlike deep regression layers used in~\cite{mobilehand}. We regress 6D rotations~\cite{6drot} for all joints in MANO model. There are 3 losses to minimize in our framework, $L_{V3D}$, 3D vertex loss between predicted mesh and ground truth mesh. $L_{J3D}$, 3D joint loss between predicted 3D joints and ground truth 3D joints. $L_{J2D}$, 2D joint loss between projected 3D joints and ground 2D keypoints.

\vspace{-3mm}
\paragraph{Setup}
We train our model on FreiHand~\cite{freihand} dataset, that contains 130,240 training images and 3,960 test images. Following METRO~\cite{metro_hand}, we train our model on 224$\times$224 images from the dataset using Adam optimizer. The models are trained for 200 epochs, with an initial learning rate of 1e-4 and decayed by a factor of 10 after 100 epochs. We initialize the backbone with weights obtained by pretraining on ImageNet-1k dataset. The weighting for all the losses in our framework is set to 1.0.

\paragraph{Results}
Figure~\ref{fig:hand_mesh_results} shows qualitative results from our framework on FreiHand test set. Even though our model is simple, it can model complicated gestures. Our model predicts reliable poses even in the presence of occlusion from hand-held objects. 
