\section{Architecture} \label{sec:arch}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.97\linewidth]{images/fastvit_arch.pdf}
    \caption{(a) Overview of FastViT architecture which decouples train-time and inference-time architecture. Stages 1, 2, and 3 have the same architecture and uses RepMixer for token mixing. In stage 4, self attention layers are used for token mixing. (b) Architecture of the convolutional stem. (c) Architecture of convolutional-FFN (d) Overview of \textit{RepMixer} block, which reparameterizes a skip connection at inference. }
    \label{fig:arch}
    \vspace{-4mm}
\end{figure*}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{images/benefits_of_reparam.pdf}
    \caption{  
    Latency comparison of a MetaFormer (S12) architecture with Pooling and RepMixer as a choice for token mixing; measured on iPhone 12 Pro for various image resolutions. Both models have $\sim$1.8G FLOPs. Absence of a skip connection in RepMixer lowers the overall memory access cost leading to lower latency. }
    \label{fig:benefits_reparam}
    \vspace{-0.5cm}
\end{figure}





\subsection{Overview} 
\label{sec:overview}
FastViT is a hybrid transformer and has four distinct stages which operate at different scales as shown in Figure~\ref{fig:arch}. We detail all the FastViT variants in Table~\ref{tab:model_arch}.


FastViT uses \textit{RepMixer}, a token mixer that reparameterizes a skip connection, which helps in alleviating memory access cost (see Figure~\ref{fig:arch}d).
To further improve efficiency and performance, we replace dense k$\times$k convolutions commonly found in stem and patch embedding layers with its factorized version that uses train-time overparameterization (see Figure~\ref{fig:arch}a). 


Self-attention~\cite{ViT} token mixers are computationally expensive, especially at higher resolutions~\cite{litv1, marin2021token}. While efficient versions of self-attention layers are explored in~\cite{Cmt, litv2}, we use large kernel convolutions as an efficient alternative to improve receptive field in early stages of the network architecture (see Figure~\ref{fig:arch}c). 

We analyze various architectural choices made in designing FastViT from a PoolFormer~\cite{metaformer} baseline in Table~\ref{tab:ablation_arch_choices} and detail our approach below.

\begin{table}
    \centering
    \scalebox{0.80}{
    \begin{tabular}{l@{\hspace{0.5\tabcolsep}}c@{\hspace{0.5\tabcolsep}}c@{\hspace{0.5\tabcolsep}}c@{\hspace{0.5\tabcolsep}}c@{\hspace{0.5\tabcolsep}}}
    \toprule
    \multirow{2}{*}{Architectural Choices} & Params & FLOPs & Mobile & Top-1 \\
     & (M) & (G) & Latency (ms) & (\%) \\ 
     \midrule
    \multicolumn{1}{l}{PoolFormer-S12 (Baseline)} & 11.9 & 1.8 & 1.50 & 77.2  \\
    \multicolumn{1}{l}{\quad + 224 $\rightarrow$ 256} & 11.9 & 2.4 & 2.25 & 77.6 \\
    \midrule
    Section \ref{sec:reparam_skip} \\
    \multicolumn{1}{l}{\quad + Pooling $\rightarrow$ RepMixer} & 11.9 & 2.4 & 1.58 & 78.5  \\
    \midrule
    Section \ref{sec:train_overparam} \\
    \multicolumn{1}{l}{\quad + Factorized dense conv.} & 8.7 & 1.7 & 1.26 & 78.0 \\
    \multicolumn{1}{l}{\quad + Train-Time Overparam.} & 8.7 & 1.7 & 1.26 & 78.9 \\
    \midrule
    Section \ref{sec:convnext_ffn} \\
    \multicolumn{1}{l}{\quad + LK. conv. FFN } & 8.7 & 1.8 & 1.33 & 79.4 \\
    \multicolumn{1}{l}{\quad + LK. conv. Patch Emb. } & 8.8 & 1.8 & 1.40 & 79.8 \\
    \bottomrule
    \end{tabular}
    
    }
        \caption{Analysis of architectural choices made to obtain FastViT-S12 variant, starting from PoolFormer-S12. ``LK." stands for Large Kernel.}
    \label{tab:ablation_arch_choices}
\end{table}

\begin{table}
\footnotesize
\centering
\setlength{\tabcolsep}{0.8pt}
\input{tables/models}
\caption{\textbf{Architecture details of FastViT variants.} Models with smaller embedding dimensions, i.e. [64, 128, 256, 512] are prefixed with ``S" and models that contain Self-Attention layers are prefixed with ``SA". Models with bigger embedding dimensions, i.e. [76, 152, 304, 608] are prefixed with ``M". Models with MLP expansion ratio less than 4 are prefixed with ``T". The number in the notation denotes total number of FastViT blocks. FLOP count is computed by \texttt{fvcore}\cite{fvcore} library.
}
\label{tab:model_arch}
\vspace{-4mm}
\end{table}


\subsection{FastViT}\label{sec:FastViT_arch}

\subsubsection{Reparameterizing Skip Connections}\label{sec:reparam_skip}

\paragraph{RepMixer} 
Convolutional mixing was first introduced in ConvMixer\cite{trockman2022convmixer}. For an input tensor $X$, the mixing block in the layer was implemented as,
\begin{equation}
Y = \texttt{BN(}\sigma\texttt{(DWConv(}X\texttt{)))} + X
\end{equation}
where $\sigma$ is a non-linear activation function and \texttt{BN} is Batch Normalization~\cite{Batch_Norm} layer and \texttt{DWConv} is depthwise convolutional layer. While this block was shown to be effective, in \textit{RepMixer}, we simply rearrange the operations and remove the non-linear activation function as shown below,
\begin{equation}
Y = \texttt{DWConv(BN(}X\texttt{)}+ X
\end{equation}\label{equation:repmix_train}
The main benefit of our design is that it can be reparameterized at inference time to a single depthwise convolutional layer as shown below and in Figure~\ref{fig:arch}d.
\begin{equation}
Y = \texttt{DWConv(}X\texttt{)} 
\end{equation}\label{equation:repmix_infer}

\vspace{-5mm}
\paragraph{Positional Encodings} 
We use conditional positional encodings~\cite{Twins, CPE} that is dynamically generated and conditioned on the local neighborhood of the input tokens. These encodings are generated as a result of a depth-wise convolution operator and are added to the patch embeddings. Note the lack of non-linearities in this group of operations, hence this block is reparameterized as shown in Figure~\ref{fig:arch}a.

\paragraph{Empirical Analysis} In order to verify the benefits of reparameterizing skip connections, we ablate over the using one of the most efficient (in terms of latency) token mixer, i.e. Pooling and RepMixer in a MetaFormer S12 architecture. Both the models being ablated have $\sim$1.8G FLOPs. We time the models for various input resolutions starting from 224$\times$224 to 1024$\times$1024 on an iPhone 12 Pro mobile device. From Figure~\ref{fig:benefits_reparam}, we see that RepMixer significantly improves over Pooling, especially at higher resolutions. At 384$\times$384, using RepMixer will lower the latency by 25.1\% and at larger resolutions like 1024$\times$1024, latency is lowered significantly by 43.9\%. 


\vspace{-3.5mm}
\subsubsection{Linear Train-time Overparameterization}\label{sec:train_overparam}

In order to further improve efficiency (parameter count, FLOPs, and latency), we replace all dense k$\times$k convolutions with its factorized version, i.e. k$\times$k depthwise followed by 1$\times$1 pointwise convolutions. However, the lower parameter count from factorization can diminish the capacity of the model. In order to increase capacity of the factorized layers, we perform linear train-time overparameterization as described in MobileOne~\cite{mobileone}.
MobileOne-style overparameterization in stem, patch embedding, and projection layers help in boosting performance. From Table~\ref{tab:ttop_ablation}, we note that train-time overparameterization improves Top-1 accuracy on ImageNet by 0.6\% on FastViT-SA12 model. On a smaller FastViT-S12 variant, Top-1 accuracy improves by 0.9\% as shown in Table~\ref{tab:ablation_arch_choices}.

However, train-time overparameterization results in increased training time due to computational overhead from the added branches. In our architecture, we only overparameterize those layers that replace dense k$\times$k convolutions with its factorized form as described above. These layers are found in the convolutional stem, patch embedding and projection layers. The computational cost incurred in these layers are lower than the rest of the network, hence overparameterizing these layers do not result in significant increases to train time. For example, FastViT-SA12 takes 6.7\% longer and FastViT-SA36 takes 4.4\% longer to train with train-time overparameterization as opposed to training those variants without it under the same settings described in Section~\ref{sec:imagenet_exps}.

\vspace{-3.5mm}
\subsubsection{Large Kernel Convolutions}\label{sec:convnext_ffn}
The receptive field of RepMixer is local compared to self-attention token mixers. However, self-attention based token mixers are computationally expensive. A computationally efficient approach to improve the receptive field of early stages that do not use self-attention is by incorporating depthwise large kernel convolutions. We introduce depthwise large kernel convolutions in FFN and patch embedding layers. 
From Table~\ref{tab:large_kernel_ablations}, we note that variants using depthwise large kernel convolutions can be highly competitive to variants using self-attention layers while incurring a modest increase in latency. When we compare V5 with V3, model size increases by 11.2\%, and latency increases by a factor of 2.3$\times$ for a relatively small gain of 0.4\% in Top-1 accuracy. V2 is larger than V4 by 20\% and has 7.1\% higher latency than V4 while attaining similar Top-1 accuracy on ImageNet. Further ablations on kernel sizes and latency is provided in the supplementary materials. In Table~\ref{tab:ablation_arch_choices}, we ablate over large kernel convolutions in FFN and patch embedding layers. Overall, large kernel convolutions provide 0.9\% improvement in Top-1 accuracy on FastViT-S12. 

The architecture of our FFN and patch embedding layer is shown in Figure~\ref{fig:arch}. Our FFN block has a structure similar to ConvNeXt~\cite{ConvNext} block with a few key differences, see Figure~\ref{fig:arch}c. We use Batch Normalization as opposed to Layer Normalization, as it can be fused with the preceding layer at inference. Also, it does not require additional reshape operations to obtain appropriate tensor layout for LayerNorm as done in the original implementation of ConvNeXt block. 

Along with increased receptive field, large kernel convolutions help in improving model robustness as observed in~\cite{wang2022robustcnn} and convolutional-FFN blocks generally tend to be more robust than vanilla-FFN blocks as observed in~\cite{mao2022robust}. Hence, incorporating large kernel convolutions is an efficient way to improve model performance and robustness. 

\begin{table}
    \centering
    \scalebox{0.68}{
    \begin{tabular}{l|c|c|c}
    \toprule
    Model   & Ablation  & Top-1 (\%) & Train Time (hrs)  \\ 
    \midrule
    \multirow{2}{*}{FastViT-SA12} & Without Train-Time Overparam. & 80.0 & 31.3 \\
                                & With Train-Time Overparam. & \textbf{80.6} & 33.4 \\
    \midrule                                
    \multirow{2}{*}{FastViT-SA36} & Without Train-Time Overparam. & 83.3 & 73.5 \\
                                & With Train-Time Overparam. & \textbf{83.6} & 76.7 \\                                
    \bottomrule
    \end{tabular}
    
    }
        \caption{Comparison of FastViT variants with and without linear train-time overparameterization when trained on ImageNet-1k dataset. Train time is wall clock time elapsed at the end of a training run.}
    \label{tab:ttop_ablation}
    \vspace{-0.4cm}
\end{table}

\begin{table}
    \centering
    \scalebox{0.78}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
    \toprule
    \multirow{2}{*}{Variant} & \multicolumn{4}{c|}{Stages}  & Params & Top-1 & Mobile \\
    \cmidrule{2-5}
    & 1 & 2 & 3 & 4 & (M) & (\%) & Latency (ms) \\ 
    \midrule
    \multicolumn{8}{c}{Standard Setting} \\
    \midrule
    V1 & RM & RM & RM & RM & 8.7  & 78.9 & 1.3 \\
    V2 & RM & RM & RM & SA & 10.8 & 79.9 & 1.5 \\
    V3 & RM & RM & SA & SA & 12.4 & 81.0 & 3.7 \\
    \midrule
    \multicolumn{8}{c}{Large Kernel Convolutions (7$\times$7)} \\
    \midrule
    V4 & RM & RM & RM & RM & 8.8  & 79.8  & 1.4 \\
    V5 & RM & RM & RM & SA & 10.9 & 80.6  & 1.6 \\
    \bottomrule
    \end{tabular}
    
    }
        \caption{Ablation on using large kernel convolutions as a substitute for self-attention layers. ``RM" indicates [RepMixer-FFN] block is used in the stage. ``SA" indicates [Self Attention-FFN] block is used in the stage. Standard setting uses 3x3 factorized convolutions in patch embedding and stem layers and 1$\times$1 convolutions for FFN. In variants V4 and V5, large kernel convolutions (7$\times$7) are used in patch embedding and FFN layers.}
    \label{tab:large_kernel_ablations}
\end{table}