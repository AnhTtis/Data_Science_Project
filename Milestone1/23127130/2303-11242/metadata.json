{
    "arxiv_id": "2303.11242",
    "paper_title": "Make Landscape Flatter in Differentially Private Federated Learning",
    "authors": [
        "Yifan Shi",
        "Yingqi Liu",
        "Kang Wei",
        "Li Shen",
        "Xueqian Wang",
        "Dacheng Tao"
    ],
    "submission_date": "2023-03-20",
    "revised_dates": [
        "2023-03-21"
    ],
    "latest_version": 1,
    "categories": [
        "cs.LG",
        "cs.CR",
        "cs.CV"
    ],
    "abstract": "To defend the inference attacks and mitigate the sensitive information leakages in Federated Learning (FL), client-level Differentially Private FL (DPFL) is the de-facto standard for privacy protection by clipping local updates and adding random noise. However, existing DPFL methods tend to make a sharper loss landscape and have poorer weight perturbation robustness, resulting in severe performance degradation. To alleviate these issues, we propose a novel DPFL algorithm named DP-FedSAM, which leverages gradient perturbation to mitigate the negative impact of DP. Specifically, DP-FedSAM integrates Sharpness Aware Minimization (SAM) optimizer to generate local flatness models with better stability and weight perturbation robustness, which results in the small norm of local updates and robustness to DP noise, thereby improving the performance. From the theoretical perspective, we analyze in detail how DP-FedSAM mitigates the performance degradation induced by DP. Meanwhile, we give rigorous privacy guarantees with RÃ©nyi DP and present the sensitivity analysis of local updates. At last, we empirically confirm that our algorithm achieves state-of-the-art (SOTA) performance compared with existing SOTA baselines in DPFL.",
    "pdf_urls": [
        "http://arxiv.org/pdf/2303.11242v1"
    ],
    "publication_venue": "CVPR2023, 18 pages"
}