\section{Experiments}
In this section, we conduct extensive experiments to verify the effectiveness of DP-FedSAM. 

\subsection{Experiment Setup}

\noindent
\textbf{Dataset and Data Partition.}\  
The efficacy of DP-FedSAM is evaluated on three datasets, including \textbf{EMNIST} \cite{cohen2017emnist}, \textbf{CIFAR-10} and \textbf{CIFAR-100} \cite{krizhevsky2009learning}, in both IID and Non-IID settings. Specifically, Dir Partition \cite{hsu2019measuring} is used for simulating Non-IID across federated clients, where the local data of each client is partitioned by splitting the total dataset through sampling the label ratios from the Dirichlet distribution Dir($\alpha$) with parameters $\alpha=0.3$ and $\alpha=0.6$. 


\noindent
\textbf{Baselines.} We focus on DPFL methods that ensure client-level DP. Thus we compare DP-FedSAM with existing DPFL baselines: \textbf{DP-FedAvg} \cite{McMahan2018learning} ensures client-level DP guarantee by directly employing Gaussian mechanism to the local updates. \textbf{DP-FedAvg-blur} \cite{cheng2022differentially} adds regularization method (BLUR) based on DP-FedAvg. \textbf{DP-FedAvg-blurs} \cite{cheng2022differentially} uses local update sparsification (LUS) and BLUR for improving the performance of DP-FedAvg.
\textbf{Fed-SMP-$\randk_k$} and \textbf{Fed-SMP-$\topk_k$} \cite{hu2022federated} leverage random sparsification and $\topk_k$ sparsification technique for reducing the impact of DP noise on model accuracy, respectively.

\noindent
\textbf{Configuration.}
For EMNIST, we use a simple CNN model and train $200$ communication round. For CIFAR-10 and CIFAR-100 datasets, we use the ResNet-18 \cite{he2016deep} backbone and train $300$ communication round. 
For all experiments, we set the number of clients $M$ to $500$. The default sample ratio $q$ of the client is $0.1$. The local learning rate $\eta$ is set to 0.1 with a decay rate $0.005$ and momentum $0.5$, and the number of training epochs is $30$. For privacy parameters, noise multiplier $\sigma$ is set to $0.95$ and  the privacy failure probability $\delta=\frac{1}{M}$. The clipping threshold $C$ is selected by grid search from set $\{0.1,0.2,0.4,0.6,0.8\}$, and we find that the gradient explosion phenomenon will occur when $C \ge 0.6$ on EMNIST and $C=0.2$ performs better on three datasets.  The weight perturbation ratio is set to $\rho = 0.5$. We run each experiment $3$ trials and report the best-averaged testing accuracy in each experiment.

The results on EMNIST and CIFAR-10 datasets are placed in the main paper, while the results on  CIFAR-100 are placed in \textbf{Appendix} \ref{imp_detail}.
For a more comprehensive and fair comparison, we integrate DP-FedSAM with $\topk_k$ sparsification technique \cite{hu2022federated, cheng2022differentially}, named DP-FedSAM-$\topk_k$ (More discussion in \textbf{Appendix} \ref{DP-sam-topk}), to compare with baselines that also use local sparsification technique, such as Fed-SMP-$\randk_k$, Fed-SMP-$\topk_k$, and DP-FedAvg-$\blurs$.


\begin{figure*}[t]
\centering
    \begin{subfigure}{1\linewidth}
    \centering
        \includegraphics[width=0.95\textwidth]{figures/emnist.pdf}
        \caption{EMNIST}
        \label{fig:emnist}
    \end{subfigure}
    % \hfill
    % \quad
    \begin{subfigure}{1\linewidth}
    \centering
        \includegraphics[width=0.95\textwidth]{figures/cifar10.pdf}
        \caption{CIFAR-10}
        \label{fig:cifar10}
    \end{subfigure}
    \vspace{-0.55cm}
    \caption{\small The averaged testing accuracy on \textit{EMNIST} and \textit{CIFAR-10} dataset under symmetric noise for all compared methods. }
 \label{fig:all}
\vspace{-0.6cm}
\end{figure*}
% FedAvg, DP-FedAvg, DP-FedAvg-$\blurs$, Fed-SMP-$\topk_k$ and Fed-SMP-$\randk_k$.
% DP-FedSAM

\subsection{Experiment Evaluation}\label{eva}
% \begin{figure*}[t]
%     \centering
%         \includegraphics[width=1\textwidth]{figures/emnist.pdf}
%     \caption{Averaged testing accuracy on \textit{EMNIST} dataset under symmetric noise for all compared methods. }
%     \label{emnist}
% \end{figure*}



% \usepackage{multirow}
% \usepackage{booktabs}







\noindent
\textbf{Performance with compared baselines.}
In Table \ref{table:all_baselines} and Figure \ref{fig:all}, we evaluate DP-FedSAM and DP-FedSAM-$\topk_k$ on EMNIST and CIFAR-10 datasets in both settings compared with all baselines from DP-FedAvg to DP-FedAvg-$\blurs$. The baseline methods seem to be overfitting in Table \ref{table:all_baselines}, especially on more complex data (e.g., CIFAR-100 in Table \ref{table:cifar100_all_baselines}). The main reasons are the random noise introduced by DP and the over-fitting and inconsistency of the local models. From all these results, it is seen that our proposed algorithms outperform other baselines under symmetric noise both on accuracy and generalization perspectives.
It means that we significantly improve the performance and generate a better trade-off between performance and privacy in DPFL.
% Specifically, in Table 1, we can see that the performance improvement is more obvious than all other baselines on EMNIST and CIFAR-10 with the same communication round.  
For instance, the averaged testing accuracy is $85.90\%$ in DP-FedSAM and $87.70\%$ in DP-FedSAM-$\topk_k$ on EMNIST in the IID setting, which are better than other baselines. Meanwhile, the difference between training accuracy and test accuracy is $9.71\%$ in DP-FedSAM and $8.40\%$ in DP-FedSAM-$\topk_k$, while that is $17.74\%$ in DP-FedAvg and $16.41\%$ in Fed-SMP-$\topk_k$, respectively. That means our algorithms significantly mitigate the performance degradation issue caused by DP.


\noindent
\textbf{Impact of Non-IID levels.}
In the experiments under different participation cases as shown in Table \ref{table:all_baselines}, we further prove the robust generalization of the proposed algorithms. Heterogeneous data distribution of local clients is set to various participation levels from IID, Dirichlet 0.6, and Dirichlet 0.3, which makes the training of the global model more difficult. On EMNIST, as the Non-IID level decreases, DP-FedSAM achieves better generalization than DP-FedAvg, and the difference between training accuracy and test accuracy in DP-FedSAM $\{19.47\%, 10.75\%, 9.71\%\}$ are lower than that in DP-FedAvg $\{26.18\%, 17.35\%, 17.74\%\}$. Similarly, the difference values in DP-FedSAM-$\topk_k$ $\{17.50\%, 11.07\%, 8.40\%\}$ are also lower than that in Fed-SMP-$\topk_k$ $\{23.56\%, 16.31\%, 16.41\%\}$. These observations confirm that our algorithms are more robust than baselines in various degrees of heterogeneous data.
\subsection{Discussion for DP with SAM in FL}\label{exper_DP}
In this subsection, we empirically discuss how SAM mitigates the negative impacts of DP from the aspects of the norm of local update and the visualization of loss landscape and contour. Meanwhile, we also observe the training performance with SAM under different privacy budgets $\epsilon$ compared with all baselines. These experiments are conducted on CIFAR-10 with ResNet-18 \cite{he2016deep} and Dirichlet $\alpha=0.6$.
% And we conduct experiments on CIFAR-10 with ResNet-18 \cite{he2016deep} and Dirichlet $\alpha=0.6$.

\begin{figure}
\centering
\includegraphics[width=0.48\textwidth]{figures/norm1.pdf}
\vspace{-0.4cm}
\caption{\small Norm distribution and average norm of local updates.
}
\vspace{-0.4cm}
\label{fig:norm}
\end{figure}
\begin{figure*}[ht]
\centering
\includegraphics[width=0.95\textwidth]{figures/abla.pdf}
\vspace{-0.35cm}
\caption{\small Impact of hyper-parameters: perturbation radius $\rho$, local iteration steps $K$, total clients size $M$.}
\vspace{-0.35cm}
\label{fig:abla}
\end{figure*}
% \begin{figure}
% \centering
% \begin{subfigure}{0.57\linewidth}
% \centering
% \includegraphics[width=1\textwidth]{figures/sam_landscape.pdf}
% \caption{\small Loss landscape}
% \end{subfigure}
% % \quad
% \hfill
% \begin{subfigure}{0.41\linewidth}
% \centering
% \includegraphics[width=1\textwidth]{figures/contour_c_comparsion.pdf}
% \caption{\small Loss surface contour}
% \end{subfigure}
% % \begin{subfigure}{0.45\linewidth}
% % \centering
% % \includegraphics[width=1\textwidth]{figures/norm1.pdf}
% % \caption{\small Norm distribution and average norm of local updates.
% % }
% % \end{subfigure}
% \vspace{-0.2cm}
% \caption{\small Loss landscape and surface contour of DP-FedSAM. Compared with DP-FedAvg in the left of Figure \ref{landscape_fedavg_dpfedavg} (a) and (b) with the same setting, DP-FedSAM has a flatter landscape with both better generalization ability (flat minima, see Figure \ref{sam_land} (a)) and weight perturbation robustness (see Figure \ref{sam_land} (b)).
% }
% \vspace{-0.2cm}
% \label{sam_land}
% \end{figure}


\noindent
\textbf{The norm of local update.}
To validate the theoretical results for mitigating the adverse impacts of the norm of local updates, we conduct experiments on DP-FedSAM and DP-FedAvg with clipping threshold $C=0.2$ as shown in Figure \ref{fig:norm}. We show the norm $\Delta_i^t$ distribution and average norm $\overline{\Delta}^t$ of local updates before clipping during the communication rounds. In contrast to DP-FedAvg, most of the norm is distributed at the smaller value in our scheme in Figure \ref{fig:norm} (a), which means that the clipping operation drops less information. Meanwhile, the on-average norm $\overline{\Delta}^t$ is smaller than DP-FedAvg as shown in Figure \ref{fig:norm} (b). These observations are also consistent with our theoretical results in Section \ref{th}.




\begin{table}
\centering
\caption{Performance comparison under different privacy budgets.}
\vspace{-0.2cm}
\label{privacy}
\small 
\renewcommand{\arraystretch}{0.9}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc} 
\toprule
\multirow{2}{*}{Algorithm} & \multicolumn{4}{c}{Averaged test accuracy (\%) under different privacy budgets $\epsilon$}                                                                                                            \\ 
\cmidrule{2-5}
                           &  $\epsilon$ = 4                      & $\epsilon$ = 6                                  &$\epsilon$ =  8                                  & $\epsilon$ = 10                                  \\ 
\midrule
DP-FedAvg                  & 38.23 $\pm$
  0.15 & 43.87 $\pm$ 0.62 & 46.74 $\pm$ 0.03 & 49.06 $\pm$ 0.49  \\
Fed-SMP-$\randk_k$              & 33.78 $\pm$ 0.92    & 42.21 $\pm$ 0.21 & 48.20 $\pm$ 0.05 & 50.62 $\pm$ 0.14  \\
Fed-SMP-$\topk_k$               & 38.99 $\pm$0.50     & 46.24 $\pm$ 0.80 & 49.78 $\pm$ 0.78 & 52.51 $\pm$ 0.83  \\
DP-FedAvg-$\blur $            & 38.23 $\pm$ 0.70    & 43.93 $\pm$ 0.48 & 46.74 $\pm$ 0.92 & 49.06 $\pm$ 0.13  \\
DP-FedAvg-$\blurs $             & 39.39 $\pm$0.43     & 46.64 $\pm$ 0.36  & 50.18 $\pm$ 0.27 & 52.91 $\pm$ 0.57  \\
DP-FedSAM                  & \textbf{39.89$\pm$ 0.17}   & 47.92 $\pm$ 0.23 & 51.30 $\pm$ 0.95 & 53.18 $\pm$ 0.40  \\
DP-FedSAM-$\topk_k$              & 38.96 $\pm$ 0.61    & \textbf{49.17 $\pm$ 0.15} & \textbf{53.64 $\pm$ 0.12} & \textbf{56.36 $\pm$ 0.36}  \\
\bottomrule
\end{tabular}
}
\vspace{-0.45cm}
\end{table}

\noindent
\textbf{Performance under different privacy budgets $\epsilon$.}
Table \ref{privacy} shows the test accuracies for the different level privacy guarantees. Note that $\epsilon$ is not a hyper-parameter, but can be obtained by the privacy design in each round such as Eq. (\ref{eq:accumulative_eps}).
Our methods consistently outperform the previous SOTA methods under various privacy budgets $\epsilon$.
Specifically, DP-FedSAM and DP-FedSAM-$\topk_k$ significantly improve the accuracy of DP-FedAvg and Fed-SMP-$\topk_k$ by $1\% \sim 4\%$ and $3\% \sim 4\%$ under the same $\epsilon$, respectively. 
Furthermore, the test accuracy can improve as the privacy budget $\epsilon$ increases, which suggests us a better balance is necessary between training performance and privacy. There is a better trade-off when achieving better accuracy under the same $\epsilon$ or a smaller $\epsilon$ under approximately the same accuracy.


\noindent
\textbf{Loss landscape and contour.}
The discussion and visualizing results are placed in \textbf{Appendix} \ref{exper_DP_appendix} due to limited space.
% To visualize the sharpness of the flat minima and observe robustness to DP noise obtained by DP-FedSAM, we show the loss landscape and surface contour following by the plotting method \cite{li2018visualizing} in Figure \ref{sam_land}. It is clear that DP-FedSAM has flatter minima and better robustness to DP noise than DP-FedAvg in the left of Figure \ref{landscape_fedavg_dpfedavg} (a) and (b), respectively. It indicates that our proposed algorithm achieves better generalization and makes the training process more adaptive to the DPFL setting.
% thereby getting a better balance between performance and privacy.

\subsection{Ablation Study}
We verify the influence of hyper-parameters in DP-FedSAM on EMNIST with Dirichlet partition  $\alpha=0.6$. 

\noindent
\textbf{Perturbation weight $\rho$.}
Perturbation weight $\rho$ has an impact on performance as the adding perturbation is accumulated when the communication round $T$ increases. 
% It is a trade-off between performance and generalization. 
To select a proper value for our algorithms, we conduct some experiments on various perturbation radius from the set $\{ 0.01, 0.1, 0.3, 0.5, 0.7, 1.0\}$ in Figure \ref{fig:abla} (a). As $\rho = 0.5$, we achieve better convergence and performance. 
% Meanwhile, $\rho = \mathcal{O}(\frac{1}{\sqrt{T}})$ can make a speedup on convergence.

\noindent
\textbf{Local iteration steps $K$.}
Large local iteration steps $K$ can help the convergence in previous DPFL work \cite{cheng2022differentially} with the theoretical guarantees. To investigate the acceleration on $T$ by adopting a larger $K$, we fix the total batchsize and change local training epochs. In Figure \ref{fig:abla} (b), our algorithm can accelerate the convergence in Theorem \ref{th:conver} as a larger $K$ is adopted, that is, use a larger epoch value. However, the adverse impact of clipping on training increases as $K$ is too large, e.g., epoch 
= $40$. Thus we choose $30$ local epoch.

\noindent
\textbf{Client size $M$.}
We compare the performance between different numbers of client participation $m=\{ 100, 200, 300, 500, 700\}$ with the same hyper-parameters in Figure \ref{fig:abla} (c). Compared with larger $m=500$, the smaller $m$ may have worse performance due to large variance $\sigma^2 C^2/m$ in DP noise with the same setting. Meanwhile, when $m$ is too large such as $M=700$, the performance may degrade as the local data size decreases. 
\begin{table}
\caption{The averaged training accuracy and testing accuracy.}
\vspace{-0.2cm}
\label{ab:sam}
\small 
\centering
\renewcommand{\arraystretch}{0.9}
\resizebox{\linewidth}{!}{
\begin{tabular}{cccc}
\toprule
Algorithm  & Train  (\%)& Validation (\%) & Differential value (\%)  \\
\midrule
DP-FedAvg  &    99.55$\pm$0.02   &       82.20$\pm$ 0.35        &   17.35$\pm$0.32              \\
DP-FedSAM  &    95.07$\pm$0.45   &    84.32$\pm$0.19 $\uparrow$        &     10.75$\pm$0.26              $\downarrow$  \\
Fed-SMP-$\topk_k$   &   99.72$\pm$0.02    &     83.41  $\pm$ 0.91     &        16.31$\pm$ 0.89            \\
DP-FedSAM-$\topk_k$ & 95.87$\pm$0.52        &     84.80$\pm$0.60  $\uparrow$      & 11.07$\pm$0.08  $\downarrow$  \\   
\bottomrule
\end{tabular}}
\vspace{-0.4cm}
\end{table}

\noindent
\textbf{Effect of SAM.}
% 
As shown in Table \ref{ab:sam}, it is seen that DP-FedSAM and DP-FedSAM-$\topk_k$ can achieve performance improvement and better generalization compared with DP-FedAvg and Fed-SMP-$\topk_k$ as SAM optimizer is adopted with the same setting, respectively.

% \vspace{-0.25cm}